<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [math.LO](#math.LO) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 介绍了一个针对德语开发者社区的5,949条情感标注数据集，填补了德语情感分析工具的空白，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决德语软件工程领域中情感分析工具的缺乏，提供适用于德语开发者社区的专用数据集。

Method: 从德国开发者论坛提取语句，由四名德语学生标注六种基本情感，评估标注一致性和可靠性。

Result: 数据集标注一致性和可靠性高，适用于德语软件工程社区，现有德语工具未满足领域需求。

Conclusion: 该数据集填补了德语情感分析领域的空白，并提出了优化标注和扩展应用的方向。

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [2] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: 提出了一种自动化工具方法，从用户反馈中提取解释性需求并生成对应解释，通过工业合作验证其效果，展示AI生成的解释在清晰度上的优势但仍需人工验证。


<details>
  <summary>Details</summary>
Motivation: 提升软件系统的透明度和用户信任度是解释性需求的初衷，但目前缺乏从用户反馈中系统化提取需求并生成解释的方法。

Method: 开发工具支持的方法，自动从用户评论中提取解释性需求并生成解释，通过工业合作构建标注数据集进行评估。

Result: AI生成的解释在清晰度和风格上更受欢迎，但需求的相关性和正确性上不如人工生成，强调人工验证的必要性。

Conclusion: 该方法推动了从用户反馈中自动化生成解释性需求的研究，并公开数据集支持未来工作，同时指出AI生成结果仍需人工验证以确保正确性。

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [3] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: 论文探讨了如何将工业4.0技术与工程工作流结合，利用资产管理外壳（AAS）和业务流程模型与符号（BPMN）实现自动化，提出了分布式AAS写入时复制基础设施和工作流管理原型。


<details>
  <summary>Details</summary>
Motivation: 通过整合工业4.0技术优化和自动化工程流程，解决数据交换和跨组织协作的难题。

Method: 结合AAS与BPMN，设计分布式AAS写入时复制基础设施，并开发工作流管理原型。

Result: 提高了工程工作流的效率和可追溯性，同时增强了安全性和扩展性。

Conclusion: AAS与BPMN的结合为工程流程的自动化与优化提供了可行解决方案。

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [4] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 论文探讨了生成式大语言模型（LLM）在代码生成中对传统软件工程的影响，发现现有需求文档对LLM输入过于抽象，需先分解为编程任务并补充设计信息，强调了需求工程的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代码生成能力是否终结传统软件工程，并探索开发人员在代码生成中如何利用需求文档等信息。

Method: 访谈了14家公司的18名从业者，分析他们在LLM代码生成中对需求和设计文档的利用方式。

Result: 需求文档对LLM输入过于抽象，需手动分解为编程任务并补充设计决策和架构约束，才能有效用于生成代码。

Conclusion: 即使使用LLM生成代码，需求工程仍然是必要的，研究为自动化需求导向的软件工程任务提供了理论支持。

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [5] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: 本文提出了一种系统性的文献综述方法，为需求工程（RE）中的提示工程（PE4RE）提供了分类和路线图，以解决当前LLM在RE中缺乏可控性和指导的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在需求工程（RE）任务中的应用缺乏明确指导和可控性，阻碍了其可信赖的实施。

Method: 基于Kitchenham和Petersen的二次研究协议，对867条记录进行筛选，分析35项主要研究，提出了一种混合分类法，链接技术导向模式与任务导向RE角色。

Result: 研究发现当前PE4RE的任务分布、LLM家族使用情况以及提示类型，并揭示了局限性和研究空白。

Conclusion: 提出逐步路线图，指导从临时原型演变为可重复、面向实践者的工作流程。

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [6] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: 本文探讨了利用检索增强生成（RAG）模型在航天领域半自动化生成需求的潜力，提出了基于AI的模块化方法，并通过实际案例验证了其可行性和初步成效。


<details>
  <summary>Details</summary>
Motivation: 航天领域的需求工程复杂且要求高精度，小型机构和新参与者难以从大量非结构化文档中提取可操作需求。

Method: 采用AI驱动的方法，预处理航天任务文档，分类内容，检索相关标准，并利用大语言模型合成需求草案。

Result: 初步结果显示，该方法能减少人工努力、提高需求覆盖范围，并支持轻量级合规性对齐。

Conclusion: 研究为AI在需求工程中的更广泛应用铺路，帮助小型机构参与安全关键任务。

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: 该论文讨论了命题程序等价性的最新进展，特别是在保护Kleene代数与测试（GKAT）的背景下，表明在忽略语句语义的情况下，程序等价性是可判定的。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了在抽象语句语义的情况下，解决程序等价性这一通常不可判定的问题，探讨命题等价性的可行性。

Method: 研究方法基于保护Kleene代数与测试（GKAT），通过理论分析探讨程序命题等价性。

Result: 结果表明，通过抽象语句语义，程序命题等价性不仅可判定，而且在实践中可行。

Conclusion: 论文结论指出，命题等价性在GKAT框架下具有实际应用潜力，为进一步研究提供了方向。

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [8] [Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning](https://arxiv.org/abs/2507.07118)
*Zelin Zhu,Kai Yang,Rui Zhang*

Main category: cs.NI

TL;DR: 该论文提出了一种基于随机近端梯度的混合整数双层优化方法（SPG-MIBO），用于联合建模和优化无线定位与传感任务，以利用其潜在协同效应，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代无线网络中，高精度定位与传感技术对智能应用至关重要。尽管已有研究利用MIMO-OFDM系统的空间多样性和频率粒度提升性能，但联合建模仍不足。

Method: 提出了SPG-MIBO算法，将定位与传感建模为混合整数双层深度学习问题，并采用小批量训练提高计算和内存效率。

Result: 算法具有理论收敛保证，且在多数据集实验中表现出性能提升。

Conclusion: 联合优化定位与传感能显著提升系统性能，SPG-MIBO是一种高效且可扩展的解决方案。

Abstract: Wireless localization and sensing technologies are essential in modern
wireless networks, supporting applications in smart cities, the Internet of
Things (IoT), and autonomous systems. High-performance localization and sensing
systems are critical for both network efficiency and emerging intelligent
applications. Integrating channel state information (CSI) with deep learning
has recently emerged as a promising solution. Recent works have leveraged the
spatial diversity of multiple input multiple output (MIMO) systems and the
frequency granularity of orthogonal frequency division multiplexing (OFDM)
waveforms to improve spatial resolution. Nevertheless, the joint modeling of
localization and sensing under the high-dimensional CSI characteristics of
MIMO-OFDM systems remains insufficiently investigated. This work aims to
jointly model and optimize localization and sensing tasks to harness their
potential synergy. We first formulate localization and sensing as a
mixed-integer bilevel deep learning problem and then propose a novel stochastic
proximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)
algorithm. SPG-MIBO is well-suited for high-dimensional and large-scale
datasets, leveraging mini-batch training at each step for computational and
memory efficiency. The algorithm is also supported by theoretical convergence
guarantees. Extensive experiments on multiple datasets validate its
effectiveness and highlight the performance gains from joint localization and
sensing optimization.

</details>


### [9] [DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training](https://arxiv.org/abs/2507.07149)
*Renyuan Liu,Yuyang Leng,Kaiyan Liu,Shaohan Hu,Chun-Fu,Chen,Peijun Zhao,Heechul Yun,Shuochao Yao*

Main category: cs.NI

TL;DR: 动态激活框架（DAF）通过系统级优化实现了高效设备端训练，显著减少内存使用并提升速度。


<details>
  <summary>Details</summary>
Motivation: 解决设备端训练中激活内存占用高及现有动态量化方法的系统级挑战。

Method: DAF采用混合缩减操作、CPU-GPU协同位打包和重要性感知分页内存管理。

Result: 内存使用减少22.9倍，速度提升3.2倍，不影响训练精度。

Conclusion: DAF为资源受限环境提供了可扩展的实用解决方案。

Abstract: Recent advancements in on-device training for deep neural networks have
underscored the critical need for efficient activation compression to overcome
the memory constraints of mobile and edge devices. As activations dominate
memory usage during training and are essential for gradient computation,
compressing them without compromising accuracy remains a key research
challenge. While existing methods for dynamic activation quantization promise
theoretical memory savings, their practical deployment is impeded by
system-level challenges such as computational overhead and memory
fragmentation.
  To address these challenges, we introduce DAF, a Dynamic Activation Framework
that enables scalable and efficient on-device training through system-level
optimizations. DAF achieves both memory- and time-efficient dynamic
quantization training by addressing key system bottlenecks. It develops hybrid
reduction operations tailored to the memory hierarchies of mobile and edge
SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic
quantization, and implements an importance-aware paging memory management
scheme to reduce fragmentation and support dynamic memory adjustments.
  These optimizations collectively enable DAF to achieve substantial memory
savings and speedup without compromising model training accuracy. Evaluations
on various deep learning models across embedded and mobile platforms
demonstrate up to a $22.9\times$ reduction in memory usage and a $3.2\times$
speedup, making DAF a scalable and practical solution for resource-constrained
environments.

</details>


### [10] [PHandover: Parallel Handover in Mobile Satellite Network](https://arxiv.org/abs/2507.07437)
*Jiasheng Wu,Shaojie Su,Wenjun Zhu,Xiong Wang,Jingjing Zhang,Xingqiu He,Yue Gao*

Main category: cs.NI

TL;DR: 本文提出了一种并行切换机制，通过基于计划的切换和卫星同步功能（SSF）显著降低低轨卫星网络的切换延迟，减少了21倍的延迟并提升了网络稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于低轨卫星高速移动导致地面终端频繁高延迟切换，影响延迟敏感应用性能。

Method: 采用基于计划的切换替代基于测量的切换，引入卫星同步功能（SSF）和机器学习信号强度预测模型。

Result: 实验显示切换延迟降低21倍，网络稳定性和用户性能显著提升。

Conclusion: 提出的并行切换机制高效解决了低轨卫星网络切换延迟问题。

Abstract: The construction of Low Earth Orbit (LEO) satellite constellations has
recently attracted tremendous attention from both academia and industry. The 5G
and 6G standards have identified LEO satellite networks as a key component of
future mobile networks. However, due to the high-speed movement of satellites,
ground terminals often experience frequent and high-latency handovers, which
significantly deteriorate the performance of latency-sensitive applications. To
address this challenge, we propose a parallel handover mechanism for mobile
satellite networks that can considerably reduce handover latency. The main idea
is to employ plan-based handovers instead of measurement-based handovers to
avoid interactions between the access and core networks, thereby eliminating
the significant time overhead associated with traditional handover procedures.
Specifically, we introduce a novel network function named the Satellite
Synchronized Function (SSF), which is designed to be fully compliant with the
standard 5G core network. In addition, we propose a machine learning model for
signal strength prediction, coupled with an efficient handover scheduling
algorithm. We have conducted extensive experiments, and the results demonstrate
that our proposed handover scheme can reduce handover latency by 21\times
compared to the standard NTN handover scheme and two other existing handover
approaches, along with significant improvements in network stability and
user-level performance.

</details>


### [11] [Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks](https://arxiv.org/abs/2507.07481)
*Wen Zhang,Aimin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Weijie Yuan,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出了无人机辅助的无电池传感器网络数据收集和无线能量传输框架，通过联合优化功率分配和飞行轨迹，提升公平数据收集量并减少能耗，采用了改进的SAC-PPV算法。


<details>
  <summary>Details</summary>
Motivation: 在高温等难访问环境中，传统固定无线能量传输和电池传感器存在安装困难和寿命短的问题，需要创新的解决方案。

Method: 利用无人机进行能量传输和数据收集，提出了多目标优化问题，并通过改进的SAC-PPV算法解决非凸和动态特性问题。

Result: 仿真结果表明，所提算法在各种网络配置下均优于基准算法。

Conclusion: 无人机辅助的无电池传感器网络解决方案在极端环境中具有可行性，改进的SAC-PPV算法有效提升了效率和稳定性。

Abstract: The integration of wireless power transfer (WPT) with Internet of Things
(IoT) offers promising solutions for sensing applications, but faces
significant challenges when deployed in hard-to-access areas such as
high-temperature environments. In such extreme conditions, traditional fixed
WPT infrastructure cannot be safely installed, and batteries rapidly degrade
due to hardware failures. In this paper, we propose an uncrewed aerial vehicle
(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)
networks deployed in these challenging environments. Specifically, we consider
a practical scenario where a UAV first transfers energy to BLS nodes via WPT,
enabling these nodes to subsequently transmit their collected data to the UAV
through orthogonal frequency-division multiple access (OFDMA). Then, we
formulate a multi-objective optimization problem that aims to maximize the fair
data collection volume while minimizing the UAV energy consumption through
joint optimization of transmit power allocation and flight trajectory planning.
Due to the non-convex nature and dynamic characteristics of this problem,
conventional optimization methods prove inadequate. To address these
challenges, we propose an enhanced soft actor-critic algorithm with
parameter-free attention, prioritized experience replay, and value-based reward
centering (SAC-PPV), thereby improving the exploration efficiency and learning
stability of the algorithm in complex WPT scenarios. Simulation results
demonstrate that the proposed approach consistently outperforms benchmark
algorithms under various network configurations.

</details>


### [12] [A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks](https://arxiv.org/abs/2507.07535)
*Jingzhao Xie,Zhenglian Li,Gang Sun,Long Luo,Hongfang Yu,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文研究了计算能力网络（CPN）中服务映射问题，提出了一种名为ABS的模块化框架，显著提升了资源利用率和服务接受率。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能充分利用网络协调来整合计算资源，且服务映射问题在CPN中仍然具有挑战性。

Method: 提出ABS框架，包括基于图分区的重新定义、双层优化架构和碎片感知评估，并采用分布式粒子群优化实现。

Result: 在复杂场景中，ABS比最优基线方法提升了73.2%的计算资源利用率和60.2%的服务接受率。

Conclusion: ABS在CPN中有效地解决了服务映射问题，显著提升了资源效率和服务满意度。

Abstract: Computing Power Network (CPN) unifies wide-area computing resources through
coordinated network control, while cloud-native abstractions enable flexible
resource orchestration and on-demand service provisioning atop the elastic
infrastructure CPN provides. However, current approaches fall short of fully
integrating computing resources via network-enabled coordination as envisioned
by CPN. In particular, optimally mapping services to an underlying
infrastructure to maximize resource efficiency and service satisfaction remains
challenging. To overcome this challenge, we formally define the service mapping
problem in CPN, establish its theoretical intractability, and identify key
challenges in practical optimization. We propose Adaptive Bilevel Search (ABS),
a modular framework featuring (1) graph partitioning-based reformulation to
capture variable coupling, (2) a bilevel optimization architecture for
efficient global exploration with local optimality guarantees, and (3)
fragmentation-aware evaluation for global performance guidance. Implemented
using distributed particle swarm optimization, ABS is extensively evaluated
across diverse CPN scenarios, consistently outperforming existing approaches.
Notably, in complex scenarios, ABS achieves up to 73.2% higher computing
resource utilization and a 60.2% higher service acceptance ratio compared to
the best-performing baseline.

</details>


### [13] [Can cloud-based VR streaming handle Wi-Fi OBSS contention?](https://arxiv.org/abs/2507.07677)
*Miguel Casasnovas,Marc Carrascosa-Zamacois,Boris Bellalta*

Main category: cs.NI

TL;DR: 论文通过实验分析了邻居Wi-Fi网络在重叠信道上的竞争对VR流媒体的负面影响，发现不同重叠情况下的性能差异，并验证了NeSt-VR算法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究邻居Wi-Fi网络在重叠信道上的竞争如何影响VR流媒体的性能，以优化VR流媒体在复杂无线环境中的表现。

Method: 通过实验分析不同信道重叠情况（部分和完全重叠）下VR流媒体的性能，并评估了NeSt-VR算法的效果。

Result: 发现完全重叠下两个40 MHz竞争者的影响小于部分重叠下单个高负载竞争者，且NeSt-VR能有效减轻性能下降。

Conclusion: NeSt-VR算法在实际环境中表现良好，能够支持更高负载的VR流媒体。

Abstract: This paper experimentally analyzes the negative impact of contention caused
by neighboring Wi-Fi networks operating on overlapping channels on Virtual
Reality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full
channel overlap within an 80 MHz channel. Our results show that (i) increasing
the number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies
contention and degrades VR streaming performance; (ii) OBSS activity on the
secondary-sided 40 MHz portion degrades performance more than activity on the
primary-sided 40 MHz portion; (iii) for the same aggregate load, full channel
overlap with two 40 MHz OBSS contenders is less detrimental than partial
overlap with a single high-load 40 MHz contender, but more disruptive than full
overlap with two 80 MHz contenders; and (iv) full channel overlap with two 40
MHz OBSS contenders has a smaller impact on VR streaming under symmetric
traffic loads than under asymmetric loads. Moreover, our results demonstrate
that our previously proposed Network-aware Step-wise adaptive bitrate algorithm
for VR streaming (NeSt-VR) effectively mitigates performance degradation in
OBSS environments, enabling VR streaming under heavier OBSS traffic conditions.

</details>


### [14] [HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN](https://arxiv.org/abs/2507.07841)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.NI

TL;DR: HaLert是一个基于Wi-Fi HaLow IEEE 802.11s网状网络的智能城市韧性架构，用于灾难期间的应急通信。


<details>
  <summary>Details</summary>
Motivation: 灾难事件通常不可预测，利用现有基础设施开发应急通信系统至关重要。智能城市的密集物联网网络为此提供了潜力。

Method: 采用SDN（软件定义网络）范式，结合LoRa控制的洪水网状网络，构建了基于Wi-Fi HaLow的通信架构。

Result: 测试显示，Wi-Fi HaLow网络在复杂环境下保持稳定，LoRa网络平均消息成功率达94.96%。

Conclusion: HaLert架构在灾难场景下展现出韧性和实用性，能够支持多类型通信需求。

Abstract: Events such as catastrophes and disasters are, in most cases, unpredictable.
Consequently, reusing existing infrastructures to develop alternative
communication strategies after disasters is essential to minimise the impact of
these events on the population's ability to communicate and promptly receive
alerts from authorities. In this context, the emergence of smart cities,
characterised by dense and geographically distributed IoT networks, presents
significant potential for such reuse. This work proposes HaLert, a resilient
architecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,
whose resources can be readily reallocated to support a emergency communication
system to exchange messages (including text, location, image, audio, and video)
between citizens, authorities, and between both parties. To facilitate remote
monitoring and configuration of the network, the architecture incorporates the
SDN (Software-Defined Networking) paradigm, supported by a LoRa controlled
flooding mesh network. A prototype was developed based on this architecture and
tested in a real urban scenario comprising both indoor and outdoor
environments. The results demonstrated that, despite the significant impact of
obstacles, lack of line-of-sight, and terrain slopes on the latency (average
latency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and
726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow
network, it remained stable and resilient, successfully providing all
functionalities associated with the HaLert architecture. The tests conducted on
the LoRa network revealed a high average message success rate of 94.96%.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech Processing](https://arxiv.org/abs/2507.07396)
*Zeyang Song,Shimin Zhang,Yuhong Chou,Jibin Wu,Haizhou Li*

Main category: cs.MM

TL;DR: 论文提出了一种名为IML-Spikeformer的脉冲神经网络架构，旨在解决大规模语音处理任务中SNN的性能和能耗问题，通过输入感知的多级脉冲机制和HD-RepSSA模块，实现了与ANN相当的性能和更高的能效。


<details>
  <summary>Details</summary>
Motivation: 由于SNN在大规模语音处理任务中面临高计算开销和缺乏专用架构的问题，研究提出IML-Spikeformer以提升性能和能效。

Method: 采用输入感知的多级脉冲机制（IMLS）和HD-RepSSA模块（包含RepSSA和HDM），优化脉冲发放和注意力映射。

Result: IML-Spikeformer在AiShell-1和Librispeech-960上分别实现了6.0%和3.4%的词错误率，能耗降低4倍以上。

Conclusion: IML-Spikeformer在大规模语音处理中展现了SNN的可扩展性和高能效，性能与ANN相当。

Abstract: Spiking Neural Networks (SNNs), inspired by biological neural mechanisms,
represent a promising neuromorphic computing paradigm that offers
energy-efficient alternatives to traditional Artificial Neural Networks (ANNs).
Despite proven effectiveness, SNN architectures have struggled to achieve
competitive performance on large-scale speech processing task. Two key
challenges hinder progress: (1) the high computational overhead during training
caused by multi-timestep spike firing, and (2) the absence of large-scale SNN
architectures tailored to speech processing tasks. To overcome the issues, we
introduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking
Transformer architecture specifically designed for large-scale speech
processing. Central to our design is the Input-aware Multi-Level Spike (IMLS)
mechanism, which simulate multi-timestep spike firing within a single timestep
using an adaptive, input-aware thresholding scheme. IML-Spikeformer further
integrates a Reparameterized Spiking Self-Attention (RepSSA) module with a
Hierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module
enhances the precision of attention maps and enables modeling of multi-scale
temporal dependencies in speech signals. Experiments demonstrate that
IML-Spikeformer achieves word error rates of 6.0\% on AiShell-1 and 3.4\% on
Librispeech-960, comparable to conventional ANN transformers while reducing
theoretical inference energy consumption by 4.64$\times$ and 4.32$\times$
respectively. IML-Spikeformer marks an advance of scalable SNN architectures
for large-scale speech processing in both task performance and energy
efficiency.

</details>


### [16] [The Potential of Olfactory Stimuli in Stress Reduction through Virtual Reality](https://arxiv.org/abs/2507.07911)
*Yasmin Elsaddik Valdivieso,Mohd Faisal,Karim Alghoul,Monireh,Vahdati,Kamran Gholizadeh Hamlabadi,Fedwa Laamarti,Hussein Al Osman,Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: 研究表明，嗅觉刺激可以潜在地增强虚拟现实中的放松效果，虽自我报告无显著差异，但生理指标显示压力显著降低。


<details>
  <summary>Details</summary>
Motivation: 探讨嗅觉刺激是否能在虚拟现实中增强放松效果，弥补传统视觉和听觉刺激的不足。

Method: 采用随机组内设计，30名参与者体验含或不含海滩香气的VR场景，通过问卷和心电图测量评估效果。

Result: 自我报告无显著差异，但HRV显示含香气的VR显著降低压力（p=0.002）。

Conclusion: 嗅觉刺激可能通过潜意识增强放松效果，未来可研究个性化香气和长期影响。

Abstract: Immersive virtual reality (VR) is a promising tool for stress reduction and
relaxation, traditionally relying on visual and auditory stimuli. This study
examines the role of olfactory stimuli in enhancing these effects, using a
randomized within-subject design. Thirty participants aged 18-60 experienced VR
scenarios simulating a calming seaside environment, with sessions lasting 45
minutes, in two conditions: with and without a "Beach" essential oil scent
(Yankee Candle) administered via diffuser. Stress and relaxation were assessed
through self-reported surveys and physiological measures, specifically
ECG-based heart rate variability (HRV). Results showed no significant
difference in self-reported relaxation scores (p=0.371) between conditions, but
HRV analysis revealed a significant stress reduction (p=0.002) with olfactory
input, with HF increasing 108% from the Math Stress Test to the scented
relaxation condition, compared to 44% without scent. Additionally, 71.4% of
participants expressed willingness to use olfactory-enhanced VR for relaxation,
suggesting practical appeal. These findings indicate that olfactory stimuli may
enhance relaxation subconsciously, underscoring the importance of multisensory
integration in VR. Future work could explore personalized scents and long-term
effects to optimize VR- based interventions for emotional and physical
well-being.

</details>


### [17] [Multimodal Framework for Explainable Autonomous Driving: Integrating Video, Sensor, and Textual Data for Enhanced Decision-Making and Transparency](https://arxiv.org/abs/2507.07938)
*Abolfazl Zarghani,Amirhossein Ebrahimi,Amir Malekesfandiari*

Main category: cs.MM

TL;DR: 本文提出了一种多模态框架，结合视频、传感器和文本数据来预测自动驾驶行为并生成易理解的解释，提高了决策的透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的成功依赖于对复杂动态环境的理解，但目前多模态输入的集成和AI决策的透明度仍然是重大挑战。

Method: 结合VideoMAE进行视频时空分析、定制传感器融合模块处理实时数据，以及BERT用于文本理解，实现了鲁棒决策和可解释输出。

Result: 在BDD-X和nuScenes数据集上，模型训练损失从5.7231降至0.0187，动作预测准确率达92.5%，解释质量BLEU-4得分为0.75，优于现有方法。

Conclusion: 多模态集成和可解释性为构建安全、透明的自动驾驶系统提供了重要进展，推动了自动驾驶技术的广泛应用。

Abstract: Autonomous vehicles (AVs) are poised to redefine transportation by enhancing
road safety, minimizing human error, and optimizing traffic efficiency. The
success of AVs depends on their ability to interpret complex, dynamic
environments through diverse data sources, including video streams, sensor
measurements, and contextual textual information. However, seamlessly
integrating these multimodal inputs and ensuring transparency in AI-driven
decisions remain formidable challenges. This study introduces a novel
multimodal framework that synergistically combines video, sensor, and textual
data to predict driving actions while generating human-readable explanations,
fostering trust and regulatory compliance. By leveraging VideoMAE for
spatiotemporal video analysis, a custom sensor fusion module for real-time data
processing, and BERT for textual comprehension, our approach achieves robust
decision-making and interpretable outputs. Evaluated on the BDD-X (21113
samples) and nuScenes (1000 scenes) datasets, our model reduces training loss
from 5.7231 to 0.0187 over five epochs, attaining an action prediction accuracy
of 92.5% and a BLEU-4 score of 0.75 for explanation quality, outperforming
state-of-the-art methods. Ablation studies confirm the critical role of each
modality, while qualitative analyses and human evaluations highlight the
model's ability to produce contextually rich, user-friendly explanations. These
advancements underscore the transformative potential of multimodal integration
and explainability in building safe, transparent, and trustworthy AV systems,
paving the way for broader societal adoption of autonomous driving
technologies.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [18] [Dirty Data in the Newsroom: Comparing Data Preparation in Journalism and Data Science](https://arxiv.org/abs/2507.07238)
*Stephen Kasica,Charles Berret,Tamara Munzner*

Main category: cs.HC

TL;DR: 该研究填补了数据新闻中数据准备研究的空白，通过混合主题分析方法扩展了数据科学工作模型，并提出了新的脏数据分类法和记者面临的四大挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决数据新闻中数据准备研究的不足，扩展现有数据科学工作流程模型。

Method: 采用混合主题分析方法，结合演绎代码（来自数据科学工作流程）和归纳代码（来自对36名职业数据记者的访谈）。

Result: 扩展了数据科学工作模型，综合了60种脏数据问题并提出了新的分类法，同时识别了记者面临的四大挑战。

Conclusion: 研究结论是数据新闻中的数据准备问题可以通过新的分类法和模型更好地理解和解决。

Abstract: The work involved in gathering, wrangling, cleaning, and otherwise preparing
data for analysis is often the most time consuming and tedious aspect of data
work. Although many studies describe data preparation within the context of
data science workflows, there has been little research on data preparation in
data journalism. We address this gap with a hybrid form of thematic analysis
that combines deductive codes derived from existing accounts of data science
workflows and inductive codes arising from an interview study with 36
professional data journalists. We extend a previous model of data science work
to incorporate detailed activities of data preparation. We synthesize 60 dirty
data issues from 16 taxonomies on dirty data and our interview data, and we
provide a novel taxonomy to characterize these dirty data issues as
discrepancies between mental models. We also identify four challenges faced by
journalists: diachronic, regional, fragmented, and disparate data sources.

</details>


### [19] [FLoRA: An Advanced AI-Powered Engine to Facilitate Hybrid Human-AI Regulated Learning](https://arxiv.org/abs/2507.07362)
*Xinyu Li,Tongguang Li,Lixiang Yan,Yuheng Li,Linxuan Zhao,Mladen Raković,Inge Molenaar,Dragan Gašević,Yizhou Fan*

Main category: cs.HC

TL;DR: SRL（自我调节学习）对学术成就至关重要，AI技术可影响SRL。论文提出HHAIRL平衡方法，并介绍改进的\flora Engine工具，结合GenAI和学习分析，支持动态调节学习。研究表明其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有数字工具在支持SRL和HHAIRL方面不足，缺乏适应性和全面支持，需要改进工具以促进人机交互和学习效果。

Method: 开发改进的\flora Engine，融合GenAI和学习分析，提供协作写作、多代理聊天机器人等工具，实现动态适应性支持。

Result: 研究验证了\flora Engine在支持SRL和HHAIRL方面的有效性，并为实际教育场景提供了理论依据和实践方案。

Conclusion: \flora Engine为AI增强学习提供了理论和实践支持，未来可进一步优化工具以适应更广泛的教育需求。

Abstract: SRL, defined as learners' ability to systematically plan, monitor, and
regulate their learning activities, is crucial for sustained academic
achievement and lifelong learning competencies. Emerging Artificial
Intelligence (AI) developments profoundly influence SRL interactions by
potentially either diminishing or strengthening learners' opportunities to
exercise their own regulatory skills. Recent literature emphasizes a balanced
approach termed Hybrid Human-AI Regulated Learning (HHAIRL), in which AI
provides targeted, timely scaffolding while preserving the learners' role as
active decision-makers and reflective monitors of their learning process.
Nevertheless, existing digital tools frequently fall short, lacking
adaptability, focusing narrowly on isolated SRL phases, and insufficiently
support meaningful human-AI interactions. In response, this paper introduces
the enhanced \flora Engine, which incorporates advanced Generative Artificial
Intelligence (GenAI) features and state-of-the-art learning analytics,
explicitly grounded in SRL and HHAIRL theories. The \flora Engine offers
instrumentation tools such as collaborative writing, multi-agents chatbot, and
detailed learning trace logging to support dynamic, adaptive scaffolding
tailored to individual needs in real time. We further present a summary of
several research studies that provide the validations for and illustrate how
these instrumentation tools can be utilized in real-world educational and
experimental contexts. These studies demonstrate the effectiveness of \flora
Engine in fostering SRL and HHAIRL, providing both theoretical insights and
practical solutions for the future of AI-enhanced learning context.

</details>


### [20] [Pluri-perspectivism in Human-robot Co-creativity with Older Adults](https://arxiv.org/abs/2507.07550)
*Marianne Bossema,Rob Saunders,Aske Plaat,Somaya Ben Allouch*

Main category: cs.HC

TL;DR: 这篇观点论文探讨了多视角主义作为人类创意体验的核心元素及其在人机共创中的重要性，提出了一个五维分层模型，用于指导共创行为设计和互动动力学分析。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨多视角主义如何支持创意实践，并通过适应性行为提升人类创造力。

Method: 基于文献和与10名视觉艺术家及8名艺术教育者的访谈研究，提出了五维模型。

Result: 研究揭示了机器人如何通过适应性行为增强人类创造力，展示了多视角主义的潜力。

Conclusion: 论文展望了将多视角主义与视觉语言模型结合以提升共创机器人情境敏感性的未来方向。

Abstract: This position paper explores pluriperspectivism as a core element of human
creative experience and its relevance to humanrobot cocreativity We propose a
layered fivedimensional model to guide the design of cocreative behaviors and
the analysis of interaction dynamics This model is based on literature and
results from an interview study we conducted with 10 visual artists and 8 arts
educators examining how pluriperspectivism supports creative practice The
findings of this study provide insight in how robots could enhance human
creativity through adaptive contextsensitive behavior demonstrating the
potential of pluriperspectivism This paper outlines future directions for
integrating pluriperspectivism with visionlanguage models VLMs to support
context sensitivity in cocreative robots

</details>


### [21] [ArchiveGPT: A human-centered evaluation of using a vision language model for image cataloguing](https://arxiv.org/abs/2507.07551)
*Line Abele,Gerrit Anders,Tolgahan Aydın,Jürgen Buder,Helen Fischer,Dominik Kimmel,Markus Huff*

Main category: cs.HC

TL;DR: 研究探讨了AI生成的照片目录描述能否接近人类编写的质量，以及如何将生成式AI融入档案和博物馆的编目流程，通过实验发现人类审查仍是必要的，尤其是在考古等专业领域。


<details>
  <summary>Details</summary>
Motivation: 照片收藏的快速增长超过了手动编目的能力，促使研究者利用视觉语言模型（VLMs）来自动生成元数据。

Method: 使用InternVL2模型生成照片目录描述，由专家和非专家评估分类、质量和信任度。

Result: AI生成的描述被误判为人类编写的频率较高，OCR错误和幻觉限制了质量，专家对AI工具的信任度较低。

Conclusion: 建议采用协作模式，AI生成草稿后由人类验证，需通过透明可解释的AI流程建立专业人士的信任。

Abstract: The accelerating growth of photographic collections has outpaced manual
cataloguing, motivating the use of vision language models (VLMs) to automate
metadata generation. This study examines whether Al-generated catalogue
descriptions can approximate human-written quality and how generative Al might
integrate into cataloguing workflows in archival and museum collections. A VLM
(InternVL2) generated catalogue descriptions for photographic prints on
labelled cardboard mounts with archaeological content, evaluated by archive and
archaeology experts and non-experts in a human-centered, experimental
framework. Participants classified descriptions as AI-generated or
expert-written, rated quality, and reported willingness to use and trust in AI
tools. Classification performance was above chance level, with both groups
underestimating their ability to detect Al-generated descriptions. OCR errors
and hallucinations limited perceived quality, yet descriptions rated higher in
accuracy and usefulness were harder to classify, suggesting that human review
is necessary to ensure the accuracy and quality of catalogue descriptions
generated by the out-of-the-box model, particularly in specialized domains like
archaeological cataloguing. Experts showed lower willingness to adopt AI tools,
emphasizing concerns on preservation responsibility over technical performance.
These findings advocate for a collaborative approach where AI supports draft
generation but remains subordinate to human verification, ensuring alignment
with curatorial values (e.g., provenance, transparency). The successful
integration of this approach depends not only on technical advancements, such
as domain-specific fine-tuning, but even more on establishing trust among
professionals, which could both be fostered through a transparent and
explainable AI pipeline.

</details>


### [22] [Conjugated Capabilities: Interrelations of Elementary Human Capabilities and Their Implication on Human-Machine Task Allocation and Capability Testing Procedures](https://arxiv.org/abs/2507.07560)
*Nils Mandischer,Larissa Füller,Torsten Alles,Frank Flemisch,Lars Mikelsons*

Main category: cs.HC

TL;DR: 该论文提出‘共轭能力’概念，探讨人类与自动化系统中相互依赖的能力如何通过重新分配努力来克服人类限制，并在康复患者数据中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 研究人类与自动化系统交互中，如何通过共轭能力优化任务分配，克服人类能力限制。

Method: 分析IMBA标准中基础能力的相互关系，构建共轭能力网络图，并在制造应用中验证其用途。

Result: 通过图表优化IMBA测试设计，加速数据记录，并探讨共轭能力对任务分配的潜在影响。

Conclusion: 共轭能力为人类与自动化系统交互提供了新的优化途径，尤其在任务分配和能力互补方面具有重要价值。

Abstract: Human and automation capabilities are the foundation of every human-autonomy
interaction and interaction pattern. Therefore, machines need to understand the
capacity and performance of human doing, and adapt their own behavior,
accordingly. In this work, we address the concept of conjugated capabilities,
i.e. capabilities that are dependent or interrelated and between which effort
can be distributed. These may be used to overcome human limitations, by
shifting effort from a deficient to a conjugated capability with performative
resources. For example: A limited arm's reach may be compensated by tilting the
torso forward. We analyze the interrelation between elementary capabilities
within the IMBA standard to uncover potential conjugation, and show evidence in
data of post-rehabilitation patients. From the conjugated capabilities, within
the example application of stationary manufacturing, we create a network of
interrelations. With this graph, a manifold of potential uses is enabled. We
showcase the graph's usage in optimizing IMBA test design to accelerate data
recordings, and discuss implications of conjugated capabilities on task
allocation between the human and an autonomy.

</details>


### [23] [Probing Experts' Perspectives on AI-Assisted Public Speaking Training](https://arxiv.org/abs/2507.07930)
*Nesrine Fourati,Alisa Barkar,Marion Dragée,Liv Danthon-Lefebvre,Mathieu Chollet*

Main category: cs.HC

TL;DR: 研究探讨专家对商用AI公开演讲训练工具的看法，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 公开演讲是重要技能，但传统培训依赖专家指导，商用AI工具的研究不足。

Method: 通过16次半结构化访谈和2次焦点小组，收集专家对当前工具的意见。

Result: 专家认可AI工具在技术性培训中的作用，但指出需改进个性化和反馈设计。

Conclusion: 支持AI与传统培训结合的混合模式。

Abstract: Background: Public speaking is a vital professional skill, yet it remains a
source of significant anxiety for many individuals. Traditional training relies
heavily on expert coaching, but recent advances in AI has led to novel types of
commercial automated public speaking feedback tools. However, most research has
focused on prototypes rather than commercial applications, and little is known
about how public speaking experts perceive these tools.
  Objectives: This study aims to evaluate expert opinions on the efficacy and
design of commercial AI-based public speaking training tools and to propose
guidelines for their improvement.
  Methods: The research involved 16 semi-structured interviews and 2 focus
groups with public speaking experts. Participants discussed their views on
current commercial tools, their potential integration into traditional
coaching, and suggestions for enhancing these systems.
  Results and Conclusions: Experts acknowledged the value of AI tools in
handling repetitive, technical aspects of training, allowing coaches to focus
on higher-level skills. However they found key issues in current tools,
emphasising the need for personalised, understandable, carefully selected
feedback and clear instructional design. Overall, they supported a hybrid model
combining traditional coaching with AI-supported exercises.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [24] [Generative Panoramic Image Stitching](https://arxiv.org/abs/2507.07133)
*Mathieu Tuli,Kaveh Kamali,David B. Lindell*

Main category: cs.GR

TL;DR: 提出了一种基于扩散模型的生成式全景图像拼接方法，解决传统方法和现有生成模型在大规模、一致性全景合成中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接方法在视差和光线变化等复杂场景中效果不佳，而现有生成模型难以保证大规模全景的一致性和连贯性。

Method: 通过微调基于扩散的修复模型，使其能够根据多张参考图像保留场景内容和布局，从而生成无缝的全景图像。

Result: 该方法在图像质量和场景布局一致性方面显著优于基线方法。

Conclusion: 研究为复杂场景下的全景图像生成提供了一种有效的解决方案。

Abstract: We introduce the task of generative panoramic image stitching, which aims to
synthesize seamless panoramas that are faithful to the content of multiple
reference images containing parallax effects and strong variations in lighting,
camera capture settings, or style. In this challenging setting, traditional
image stitching pipelines fail, producing outputs with ghosting and other
artifacts. While recent generative models are capable of outpainting content
consistent with multiple reference images, they fail when tasked with
synthesizing large, coherent regions of a panorama. To address these
limitations, we propose a method that fine-tunes a diffusion-based inpainting
model to preserve a scene's content and layout based on multiple reference
images. Once fine-tuned, the model outpaints a full panorama from a single
reference image, producing a seamless and visually coherent result that
faithfully integrates content from all reference images. Our approach
significantly outperforms baselines for this task in terms of image quality and
the consistency of image structure and scene layout when evaluated on captured
datasets.

</details>


### [25] [LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS](https://arxiv.org/abs/2507.07136)
*Wanhua Li,Yujie Zhao,Minghan Qin,Yang Liu,Yuanhao Cai,Chuang Gan,Hanspeter Pfister*

Main category: cs.GR

TL;DR: LangSplatV2通过高维特征溅射和3D开放词汇文本查询实现了显著的速度提升和查询精度改进，解决了LangSplat因重型解码器导致的实时性不足问题。


<details>
  <summary>Details</summary>
Motivation: LangSplat在复杂场景的语言交互应用中有潜力，但其实时性不足限制了广泛应用。因此，LangSplatV2旨在通过消除重型解码器来提升性能。

Method: LangSplatV2假设每个高斯在全局字典中充当稀疏编码，学习3D稀疏系数场，并结合CUDA优化的稀疏系数溅射方法。

Result: LangSplatV2在476.2 FPS和384.6 FPS的速度下运行，分别比LangSplat快42倍和47倍，同时提升了查询精度。

Conclusion: LangSplatV2通过稀疏编码和优化溅射方法，显著提高了速度和精度，为3D语言交互应用提供了更高效的解决方案。

Abstract: In this paper, we introduce LangSplatV2, which achieves high-dimensional
feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6
FPS for high-resolution images, providing a 42 $\times$ speedup and a 47
$\times$ boost over LangSplat respectively, along with improved query accuracy.
LangSplat employs Gaussian Splatting to embed 2D CLIP language features into
3D, significantly enhancing speed and learning a precise 3D language field with
SAM semantics. Such advancements in 3D language fields are crucial for
applications that require language interaction within complex scenes. However,
LangSplat does not yet achieve real-time inference performance (8.2 FPS), even
with advanced A100 GPUs, severely limiting its broader application. In this
paper, we first conduct a detailed time analysis of LangSplat, identifying the
heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2
assumes that each Gaussian acts as a sparse code within a global dictionary,
leading to the learning of a 3D sparse coefficient field that entirely
eliminates the need for a heavyweight decoder. By leveraging this sparsity, we
further propose an efficient sparse coefficient splatting method with CUDA
optimization, rendering high-dimensional feature maps at high quality while
incurring only the time cost of splatting an ultra-low-dimensional feature. Our
experimental results demonstrate that LangSplatV2 not only achieves better or
competitive query accuracy but is also significantly faster. Codes and demos
are available at our project page: https://langsplat-v2.github.io.

</details>


### [26] [Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation](https://arxiv.org/abs/2507.07387)
*Chengan He,Jorge Alejandro Amador Herrera,Zhixin Shu,Xin Sun,Yao Feng,Sören Pirk,Dominik L. Michels,Meng Zhang,Tuanfeng Y. Wang,Julie Dorsey,Holly Rushmeier,Yi Zhou*

Main category: cs.GR

TL;DR: Digital Salon是一个支持实时3D头发生成、模拟与渲染的全方位头发创作系统，通过自然语言交互降低技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于3D头发建模的孤立部分，计算量大或需要网络训练，Digital Salon旨在提供一体化的交互式解决方案。

Method: 系统通过四个关键阶段实现：文本引导的头发检索、实时头发模拟、交互式头发优化及头发条件图像生成。

Result: 用户研究表明，该系统在快速原型设计上优于传统头发建模工作流程。

Conclusion: 该系统为数字媒体中的头发建模提供了直观、多功能且高效的解决方案，并展示了在真实沙龙环境中应用的潜力。

Abstract: We introduce Digital Salon, a comprehensive hair authoring system that
supports real-time 3D hair generation, simulation, and rendering. Unlike
existing methods that focus on isolated parts of 3D hair modeling and involve a
heavy computation process or network training, Digital Salon offers a holistic
and interactive system that lowers the technical barriers of 3D hair modeling
through natural language-based interaction. The system guides users through
four key stages: text-guided hair retrieval, real-time hair simulation,
interactive hair refinement, and hair-conditioned image generation. This
cohesive workflow makes advanced hair design accessible to users of varying
skill levels and dramatically streamlines the creative process in digital media
with an intuitive, versatile, and efficient solution for hair modeling. User
studies show that our system can outperform traditional hair modeling workflows
for rapid prototyping. Furthermore, we provide insights into the benefits of
our system with future potential of deploying our system in real salon
environments. More details can be found on our project page:
https://digital-salon.github.io/.

</details>


### [27] [Self-supervised Learning of Latent Space Dynamics](https://arxiv.org/abs/2507.07440)
*Yue Li,Gene Wei-Chin Lin,Egor Larionov,Aljaz Bozic,Doug Roble,Ladislav Kavan,Stelian Coros,Bernhard Thomaszewski,Tuur Stuyck,Hsiao-yu Chen*

Main category: cs.GR

TL;DR: 提出了一种基于神经潜空间积分器的子空间仿真框架，适用于便携设备，实现高效且稳定的动态形变模拟。


<details>
  <summary>Details</summary>
Motivation: 传统模拟方法计算成本高，子空间方法虽能提升性能，但仍无法满足便携设备的严格性能需求。

Method: 利用自监督学习训练的神经潜空间积分器，完全在潜空间操作，无需全空间计算。

Result: 在涉及棒、壳和固体的复杂示例中验证了方法的有效性和泛化能力。

Conclusion: 该方法高效、稳定，适用于便携设备的广泛部署。

Abstract: Modeling the dynamic behavior of deformable objects is crucial for creating
realistic digital worlds. While conventional simulations produce high-quality
motions, their computational costs are often prohibitive. Subspace simulation
techniques address this challenge by restricting deformations to a
lower-dimensional space, improving performance while maintaining visually
compelling results. However, even subspace methods struggle to meet the
stringent performance demands of portable devices such as virtual reality
headsets and mobile platforms. To overcome this limitation, we introduce a
novel subspace simulation framework powered by a neural latent-space
integrator. Our approach leverages self-supervised learning to enhance
inference stability and generalization. By operating entirely within latent
space, our method eliminates the need for full-space computations, resulting in
a highly efficient method well-suited for deployment on portable devices. We
demonstrate the effectiveness of our approach on challenging examples involving
rods, shells, and solids, showcasing its versatility and potential for
widespread adoption.

</details>


### [28] [SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction](https://arxiv.org/abs/2507.07465)
*Wei Yao,Shuzhao Xie,Letian Li,Weixiang Zhang,Zhixin Lai,Shiqi Dai,Ke Zhang,Zhi Wang*

Main category: cs.GR

TL;DR: SD-GS是一个高效紧凑的动态高斯框架，用于复杂动态场景重建，通过可变形锚点网格和变形感知的密度策略，显著提升了存储效率和计算性能。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯框架在动态场景重构中存在存储成本与复杂运动建模能力的固有权衡，限制了其实际应用。

Method: 引入可变形锚点网格作为高效场景表示，并提出变形感知的密度策略来自适应调整锚点分布。

Result: 实验表明，SD-GS在模型大小上平均减少60%，FPS提升100%，同时保持或超过视觉质量。

Conclusion: SD-GS在存储和计算效率上显著改进，适用于复杂动态场景重建。

Abstract: Current 4D Gaussian frameworks for dynamic scene reconstruction deliver
impressive visual fidelity and rendering speed, however, the inherent trade-off
between storage costs and the ability to characterize complex physical motions
significantly limits the practical application of these methods. To tackle
these problems, we propose SD-GS, a compact and efficient dynamic Gaussian
splatting framework for complex dynamic scene reconstruction, featuring two key
contributions. First, we introduce a deformable anchor grid, a hierarchical and
memory-efficient scene representation where each anchor point derives multiple
3D Gaussians in its local spatiotemporal region and serves as the geometric
backbone of the 3D scene. Second, to enhance modeling capability for complex
motions, we present a deformation-aware densification strategy that adaptively
grows anchors in under-reconstructed high-dynamic regions while reducing
redundancy in static areas, achieving superior visual quality with fewer
anchors. Experimental results demonstrate that, compared to state-of-the-art
methods, SD-GS achieves an average of 60\% reduction in model size and an
average of 100\% improvement in FPS, significantly enhancing computational
efficiency while maintaining or even surpassing visual quality.

</details>


### [29] [Capture Stage Environments: A Guide to Better Matting](https://arxiv.org/abs/2507.07623)
*Hannah Dröge,Janelle Pfeifer,Saskia Rabich,Markus Plack,Reinhard Klein,Matthias B. Hullin*

Main category: cs.GR

TL;DR: 本文探讨了高精度捕获阶段图像中的抠图挑战，并提出了一种无需大量标注的适应先进算法的高效流程。


<details>
  <summary>Details</summary>
Motivation: 现有抠图算法在捕获阶段内容中表现不佳，作者旨在分析这些挑战并提出改进方案。

Method: 提出了一个高效流程，将先进算法适配到定制设置中，无需大量标注，并提供一种基于扩散模型的验证方法。

Result: 提出的方法在捕获阶段内容的抠图中表现更好，并通过验证模型展示了其优势。

Conclusion: 本文为实践者提供了改进的工作流程指南，并展示了如何高效适配现有算法以解决捕获阶段的抠图问题。

Abstract: Capture stages are high-end sources of state-of-the-art recordings for
downstream applications in movies, games, and other media. One crucial step in
almost all pipelines is the matting of images to isolate the captured
performances from the background. While common matting algorithms deliver
remarkable performance in other applications like teleconferencing and mobile
entertainment, we found that they struggle significantly with the peculiarities
of capture stage content. The goal of our work is to share insights into those
challenges as a curated list of those characteristics along with a constructive
discussion for proactive intervention and present a guideline to practitioners
for an improved workflow to mitigate unresolved challenges. To this end, we
also demonstrate an efficient pipeline to adapt state-of-the-art approaches to
such custom setups without the need of extensive annotations, both offline and
real-time. For an objective evaluation, we propose a validation methodology
based on a leading diffusion model that highlights the benefits of our
approach.

</details>


### [30] [RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection](https://arxiv.org/abs/2507.07733)
*Yongyang Zhou,Fang-Lue Zhang,Zichen Wang,Lei Zhang*

Main category: cs.GR

TL;DR: RTR-GS是一个新框架，用于处理反射物体的逆渲染和重光照问题，结合前向渲染和延迟渲染，有效分解BRDF和光照。


<details>
  <summary>Details</summary>
Motivation: 3DGS在合成新视角时表现优异，但对反射物体的渲染仍有挑战，尤其是在逆渲染和重光照方面。

Method: 采用混合渲染模型，结合前向渲染和延迟渲染，分离高频和低频外观，并通过物理延迟渲染分支优化BRDF和光照分解。

Result: 实验表明，该方法提升了新视角合成、法线估计、分解和重光照效果，同时保持高效训练推理。

Conclusion: RTR-GS成功解决了反射物体渲染的难题，为逆渲染领域提供了有效解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in
novel view synthesis. However, rendering reflective objects remains a
significant challenge, particularly in inverse rendering and relighting. We
introduce RTR-GS, a novel inverse rendering framework capable of robustly
rendering objects with arbitrary reflectance properties, decomposing BRDF and
lighting, and delivering credible relighting results. Given a collection of
multi-view images, our method effectively recovers geometric structure through
a hybrid rendering model that combines forward rendering for radiance transfer
with deferred rendering for reflections. This approach successfully separates
high-frequency and low-frequency appearances, mitigating floating artifacts
caused by spherical harmonic overfitting when handling high-frequency details.
We further refine BRDF and lighting decomposition using an additional
physically-based deferred rendering branch. Experimental results show that our
method enhances novel view synthesis, normal estimation, decomposition, and
relighting while maintaining efficient training inference process.

</details>


### [31] [Hi-d maps: An interactive visualization technique for multi-dimensional categorical data](https://arxiv.org/abs/2507.07890)
*Radi Muhammad Reza,Benjamin A Watson*

Main category: cs.GR

TL;DR: 提出了一种名为Hi-D maps的新方法，用于可视化多维度分类数据，通过2D多边形区域有效展示多维度信息，并支持交互式浏览。


<details>
  <summary>Details</summary>
Motivation: 解决多维度数据可视化技术稀缺的问题，提供一种高效且节省空间的展示方法。

Method: 将多维度数据映射到2D多边形区域，通过用户控制的顺序切割多边形边，结合多种视觉线索展示跨维度信息。

Result: Hi-D maps能够直观展示多维度数据，支持交互和层次化浏览，尽管在维度过多时效果受限。

Conclusion: Hi-D maps是一种有效的多维度数据可视化工具，适用于中等维度的数据集，并具有扩展性。

Abstract: In this paper, we present Hi-D maps, a novel method for the visualization of
multi-dimensional categorical data. Our work addresses the scarcity of
techniques for visualizing a large number of data-dimensions in an effective
and space-efficient manner. We have mapped the full data-space onto a 2D
regular polygonal region. The polygon is cut hierarchically with lines parallel
to a user-controlled, ordered sequence of sides, each representing a dimension.
We have used multiple visual cues such as orientation, thickness, color,
countable glyphs, and text to depict cross-dimensional information. We have
added interactivity and hierarchical browsing to facilitate flexible
exploration of the display: small areas can be scrutinized for details. Thus,
our method is also easily extendable to visualize hierarchical information. Our
glyph animations add an engaging aesthetic during interaction. Like many
visualizations, Hi-D maps become less effective when a large number of
dimensions stresses perceptual limits, but Hi-D maps may add clarity before
those limits are reached.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [Distributed Training under Packet Loss](https://arxiv.org/abs/2507.07114)
*Erez Weintraub,Ron Banner,Ariel Orda*

Main category: cs.DC

TL;DR: 提出了一种新的分布式训练框架，能在不可靠的网络连接下保持模型准确性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有分布式框架依赖可靠连接，导致延迟和可扩展性受限，而不可靠连接可能影响模型精度。

Method: 采用两阶段防御机制：无偏梯度聚合和有界参数广播，保障梯度估计正确性和参数一致性。

Result: 在LLAMA2 7B模型上，容忍10%丢包率时困惑度变化不超过0.8%。

Conclusion: 该框架填补了高效通信协议与大规模模型训练需求之间的空白。

Abstract: State-of-the-art language and vision models are routinely trained across
thousands of GPUs, often spanning multiple data-centers, yet today's
distributed frameworks still assume reliable connections (e.g., InfiniBand or
RoCE). The resulting acknowledgment traffic and retransmissions inflate tail
latencies and limit scalability. Leveraging unreliable connections will reduce
latency but may sacrifice model accuracy and convergence once packets are
dropped. A principled, end-to-end solution that preserves accuracy and
convergence guarantees under genuine packet loss has previously been missing.
We address this critical gap by introducing a novel distributed training
framework capable of operating over unreliable connections, offering unbiased
gradient aggregation and bounded parameter drift without modifying model code
or optimizers. The key insight is a two-stage defense against missing messages:
(i) Unbiased gradient aggregation: each worker reconstructs a consistent
gradient estimate from whatever packets arrive, guaranteeing expectation-level
correctness; and (ii) Bounded-drift parameter broadcasts: we prove the
inter-worker model discrepancy remains O(1) even after arbitrarily many
iterations, preventing the unbounded divergence typical of asynchronous setups.
Analytical bounds are matched by experiments on the LLAMA2 7B model with 64
GPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.
This work bridges the gap between communication-efficient datacenter protocols
and the accuracy and generalization guarantees demanded by modern large-model
training, enabling robust, high-throughput learning on commodity or wide-area
networks.

</details>


### [33] [Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces](https://arxiv.org/abs/2507.07116)
*Juan Cano-Benito,Andrea Cimmino,Sven Hertling,Heiko Paulheim,Raúl García-Castro*

Main category: cs.DC

TL;DR: 本文评估了在不同类型DLT上存储语义数据的效率，发现私有DLT效率最高，混合DLT在审计与效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 实现数据空间的语义互操作性，填补DLT在语义数据高效存储上的空白。

Method: 系统评估公共、私有和混合DLT的性能、存储效率、资源消耗及语义数据更新查询能力。

Result: 私有DLT最有效，混合DLT在审计与效率间平衡。

Conclusion: 需根据数据主权需求选择适合的DLT基础设施。

Abstract: Data spaces are emerging as decentralised infrastructures that enable
sovereign, secure, and trustworthy data exchange among multiple participants.
To achieve semantic interoperability within these environments, the use of
semantic web technologies and knowledge graphs has been proposed. Although
distributed ledger technologies (DLT) fit as the underlying infrastructure for
data spaces, there remains a significant gap in terms of the efficient storage
of semantic data on these platforms. This paper presents a systematic
evaluation of semantic data storage across different types of DLT (public,
private, and hybrid), using a real-world knowledge graph as an experimental
basis. The study compares performance, storage efficiency, resource
consumption, and the capabilities to update and query semantic data. The
results show that private DLTs are the most efficient for storing and managing
semantic content, while hybrid DLTs offer a balanced trade-off between public
auditability and operational efficiency. This research leads to a discussion on
the selection of the most appropriate DLT infrastructure based on the data
sovereignty requirements of decentralised data ecosystems.

</details>


### [34] [Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](https://arxiv.org/abs/2507.07223)
*Myoungsoo Jung*

Main category: cs.DC

TL;DR: 论文提出了一种基于CXL的模块化数据中心架构，以解决现代AI工作负载对内存、带宽和资源灵活性的高需求，通过优化互连和分层内存模型提升可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载（如大语言模型和检索增强生成）对内存、带宽和资源灵活性提出了极高要求，传统GPU架构因通信开销难以扩展。

Method: 提出基于CXL的模块化数据中心架构，结合XLink优化的互连设计，分层内存模型，并评估轻量级CXL实现和硅光子技术。

Result: 评估显示该架构显著提升了AI基础设施的可扩展性、吞吐量和灵活性。

Conclusion: 提出的架构为大规模AI工作负载提供了高效的解决方案，解决了传统GPU架构的瓶颈问题。

Abstract: Modern AI workloads such as large language models (LLMs) and
retrieval-augmented generation (RAG) impose severe demands on memory,
communication bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU communication
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in LLMs. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.

</details>


### [35] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: 分析机器学习任务中分布式高性能系统的集体通信行为，探讨其对网络性能的影响，并建议优化集体通信框架和网络拓扑。


<details>
  <summary>Details</summary>
Motivation: 集体通信操作可能导致高带宽和突发流量，引发网络拥塞和数据包丢失，影响机器学习任务性能。因此需要分析这些模式以优化网络资源分配。

Method: 通过Nvidia集体通信库记录多种模型（如DeepSeek、GPT、Llama等）的通信行为，并调整并行度、节点数和模型类型等参数。

Result: 展示了DeepSeek V3推理模型的集体通信行为数据，包括操作类型、传输大小和请求分布，发现需重新设计集体通信框架和网络拓扑。

Conclusion: 当前集体通信框架和网络拓扑需优化，以适应机器学习工作负载对网络异常的影响。

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>


### [36] [Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](https://arxiv.org/abs/2507.07120)
*Nidhi Bhatia,Ankit More,Ritika Borkar,Tiyasa Mitra,Ramon Matas,Ritchie Zhao,Maximilian Golub,Dheevatsa Mudigere,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: 为了解决大规模LLMs在实时解码中的计算瓶颈，作者提出了Helix并行性，通过混合执行策略优化KV缓存和FFN计算，显著提升了吞吐量和延迟。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模的扩大，实时解码在紧密的Token-to-Token延迟约束下面临KV缓存读取和FFN权重访问的瓶颈，亟需新的并行策略。

Method: 采用Helix并行性，结合KV并行和TP/EP并行，通过轻量级通信保留精确注意力行为，并通过批量重叠优化通信开销。

Result: Helix在固定批量大小下将TTL降低1.5倍，并支持更大批量，提升了DeepSeek-R1的吞吐-延迟帕累托前沿。

Conclusion: Helix并行性解决了超长序列实时推理的挑战，为大模型的高效计算提供了实用方案。

Abstract: As LLMs scale to multi-million-token KV histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long KV caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of KV heads, it leads to inefficient KV
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long KV histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies KV
parallelism during attention to shard KV caches across GPUs, then reuses the
same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
communication step. To minimize the exposed communication cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through
batchwise overlap, preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.

</details>


### [37] [Ampere: Communication-Efficient and High-Accuracy Split Federated Learning](https://arxiv.org/abs/2507.07130)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: Ampere是一种新的协作训练系统，通过本地损失训练和减少通信开销，显著提升了非独立同分布数据下的模型准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统的分割联邦学习（SFL）在减轻设备计算负担时引入了通信开销和模型准确性下降的问题，特别是在非独立同分布数据（non-IID）场景下。

Method: Ampere采用单向区块间训练和轻量辅助网络生成方法，减少了设备与服务器之间的交互，并通过整合设备生成的激活数据来缓解数据异构性的影响。

Result: 相比SFL，Ampere在多项指标上表现优异，包括提升模型准确性达13.26%，减少通信开销达99.1%，降低标准偏差53.39%。

Conclusion: Ampere在非独立同分布数据下的训练中表现出色，显著提升了效率与准确性，是SFL的有效替代方案。

Abstract: A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large communication overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server communication while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the communication overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and transformers show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server communication overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.

</details>


### [38] [M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure](https://arxiv.org/abs/2507.07144)
*Hongyi Xie,Min Zhou,Qiao Yu,Jialiang Yu,Zhenli Sheng,Hong Xie,Defu Lian*

Main category: cs.DC

TL;DR: 论文提出M²-MFP框架，通过多尺度分层方法预测内存故障，显著提升云基础设施的可靠性和可用性。


<details>
  <summary>Details</summary>
Motivation: 内存故障对系统稳定性构成威胁，现有预测方法泛化性和召回率不足，需改进。

Method: M²-MFP将可纠正错误转换为多级二进制矩阵，利用BSFE提取高低阶特征，结合双路径时间建模架构。

Result: 实验证明M²-MFP在基准数据集和实际部署中优于现有方法。

Conclusion: M²-MFP有效解决内存故障预测问题，为云基础设施提供可靠保障。

Abstract: As cloud services become increasingly integral to modern IT infrastructure,
ensuring hardware reliability is essential to sustain high-quality service.
Memory failures pose a significant threat to overall system stability, making
accurate failure prediction through the analysis of memory error logs (i.e.,
Correctable Errors) imperative. Existing memory failure prediction approaches
have notable limitations: rule-based expert models suffer from limited
generalizability and low recall rates, while automated feature extraction
methods exhibit suboptimal performance. To address these limitations, we
propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction
framework designed to enhance the reliability and availability of cloud
infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level
binary matrix representations and introduces a Binary Spatial Feature Extractor
(BSFE) to automatically extract high-order features at both DIMM-level and
bit-level. Building upon the BSFE outputs, we develop a dual-path temporal
modeling architecture: 1) a time-patch module that aggregates multi-level
features within observation windows, and 2) a time-point module that employs
interpretable rule-generation trees trained on bit-level patterns. Experiments
on both benchmark datasets and real-world deployment show the superiority of
M$^2$-MFP as it outperforms existing state-of-the-art methods by significant
margins. Code and data are available at this repository:
https://github.com/hwcloud-RAS/M2-MFP.

</details>


### [39] [Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience](https://arxiv.org/abs/2507.07352)
*Loïc Pottier,Konstantia Georgouli,Timothy S. Carpenter,Fikret Aydin,Jeremy O. B. Tempkin,Dwight V. Nissley,Frederick H. Streitz,Thomas R. W. Scogland,Peer-Timo Bremer,Felice C. Lightstone,Helgi I. Ingólfsson*

Main category: cs.DC

TL;DR: 摘要介绍了多尺度机器学习建模基础设施MuMMI及其轻量版mini-MuMMI，用于协调不同时间尺度的大规模分子动力学模拟。


<details>
  <summary>Details</summary>
Motivation: 多尺度建模在复杂现象（如生物分子相互作用）中需要巨大的计算资源和高效的工作流管理，传统方法难以跨越不同尺度，机器学习为此提供了新思路。

Method: 提出了MuMMI及其轻量版mini-MuMMI，前者用于大规模HPC系统，后者适用于小型HPC或笔记本电脑，能够协调数千个不同时间尺度的分子动力学模拟。

Result: 通过RAS-RAF膜相互作用的案例展示了mini-MuMMI的实用性，并讨论了多尺度工作流通用化的挑战及其潜在应用扩展。

Conclusion: mini-MuMMI为多尺度建模提供了更灵活的工具，有望扩展到分子动力学以外的更广泛领域。

Abstract: Computational models have become one of the prevalent methods to model
complex phenomena. To accurately model complex interactions, such as detailed
biomolecular interactions, scientists often rely on multiscale models comprised
of several internal models operating at difference scales, ranging from
microscopic to macroscopic length and time scales. Bridging the gap between
different time and length scales has historically been challenging but the
advent of newer machine learning (ML) approaches has shown promise for tackling
that task. Multiscale models require massive amounts of computational power and
a powerful workflow management system. Orchestrating ML-driven multiscale
studies on parallel systems with thousands of nodes is challenging, the
workflow must schedule, allocate and control thousands of simulations operating
at different scales. Here, we discuss the massively parallel Multiscale
Machine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow
management infrastructure, that can orchestrate thousands of molecular dynamics
(MD) simulations operating at different timescales, spanning from millisecond
to nanosecond. More specifically, we introduce a novel version of MuMMI called
"mini-MuMMI". Mini-MuMMI is a curated version of MuMMI designed to run on
modest HPC systems or even laptops whereas MuMMI requires larger HPC systems.
We demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions
and discuss the different challenges behind the generalization of multiscale
workflows and how mini-MuMMI can be leveraged to target a broader range of
applications outside of MD and RAS-RAF interactions.

</details>


### [40] [KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](https://arxiv.org/abs/2507.07400)
*Zaifeng Pan,Ajjkumar Patel,Zhengding Hu,Yipeng Shen,Yue Guan,Wan-Lu Li,Lianhui Qin,Yida Wang,Yufei Ding*

Main category: cs.DC

TL;DR: KVFlow是一种面向代理工作负载的KV缓存管理框架，通过优化缓存策略和预取机制，显著提升了LLM代理工作流的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM系统在代理工作流中使用KV缓存时，由于采用了LRU策略导致缓存命中率低、计算开销大。KVFlow旨在解决这一问题。

Method: KVFlow引入Agent Step Graph抽象代理执行计划，并通过步骤到执行估计值优化缓存策略，同时提出完全重叠的KV预取机制。

Result: KVFlow相比现有系统（如SGLang），在单工作流和多并发工作流场景下分别实现了1.83倍和2.19倍的加速。

Conclusion: KVFlow通过智能缓存管理和预取技术，显著提升了LLM代理工作流的效率，适用于大规模和并发场景。

Abstract: Large language model (LLM) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing LLM systems employ prefix caching to
reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict KV caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards KV caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present KVFlow, a workflow-aware KV
cache management framework tailored for agentic workloads. KVFlow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the KV node
level, allowing KVFlow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a
fully overlapped KV prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.

</details>


### [41] [Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems](https://arxiv.org/abs/2507.07671)
*Jovan Prodanov,Blaž Bertalanič,Carolina Fortuna,Shih-Kai Chou,Matjaž Branko Jurič,Ramon Sanchez-Iborra,Jernej Hribar*

Main category: cs.DC

TL;DR: 论文提出了一种基于多智能体强化学习的动态资源扩展引擎（MARLISE），通过结合DQN和PPO算法优化边缘-云计算中的资源管理，提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统静态阈值的资源扩展方法在动态工作负载下效率不足，无法满足边缘-云基础设施的性能需求。

Method: 使用DQN和PPO两种深度强化学习算法开发MARLISE，支持动态、反应式资源扩展。

Result: 实验表明，MARLISE在动态工作负载下优于启发式方法，能降低微服务响应时间并提高资源效率。

Conclusion: MARLISE为边缘-云系统的高效资源管理提供了新解决方案。

Abstract: Modern edge-cloud systems face challenges in efficiently scaling resources to
handle dynamic and unpredictable workloads. Traditional scaling approaches
typically rely on static thresholds and predefined rules, which are often
inadequate for optimizing resource utilization and maintaining performance in
distributed and dynamic environments. This inefficiency hinders the
adaptability and performance required in edge-cloud infrastructures, which can
only be achieved through the newly proposed in-place scaling. To address this
problem, we propose the Multi-Agent Reinforcement Learning-based In-place
Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with
in-place resource scaling. We develop our solution using two Deep Reinforcement
Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization
(PPO). We analyze each version of the proposed MARLISE solution using dynamic
workloads, demonstrating their ability to ensure low response times of
microservices and scalability. Our results show that MARLISE-based approaches
outperform heuristic method in managing resource elasticity while maintaining
microservice response times and achieving higher resource efficiency.

</details>


### [42] [KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling](https://arxiv.org/abs/2507.07932)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Qiang Guan,Hailong Jiang*

Main category: cs.DC

TL;DR: KIS-S框架结合了GPU感知的Kubernetes推理模拟器KISim和基于PPO的自动缩放器KIScaler，显著提升了GPU推理工作负载的动态缩放能力。


<details>
  <summary>Details</summary>
Motivation: 现有Kubernetes的自动缩放机制（如HPA）对动态流量模式适应性差，且缺乏GPU指标集成，需要更智能的解决方案。

Method: KIS-S通过KISim模拟GPU环境，KIScaler基于PPO算法在模拟中学习缩放策略，无需重新训练即可部署。

Result: 实验显示KIScaler在四种流量模式下平均奖励提升75.2%，P95延迟降低6.7倍，且具有泛化能力。

Conclusion: KIS-S填补了反应式自动缩放与智能编排之间的鸿沟，适用于GPU加速环境。

Abstract: Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [43] [Algorithmic Complexity Attacks on All Learned Cardinality Estimators: A Data-centric Approach](https://arxiv.org/abs/2507.07438)
*Yingze Li,Xianglong Liu,Dong Wang,Zixuan Wang,Hongzhi Wang,Kaixing Zhang,Yiming Guan*

Main category: cs.DB

TL;DR: 该论文首次从理论上研究了数据漂移如何最大程度降低学习型基数估计器的准确性，提出了数据中心的算法复杂性攻击，并设计了一种多项式时间近似算法。实验表明，攻击仅需修改少量训练数据即可显著降低估计器性能。同时，论文还提出了两种防御措施，以提高学习型数据库优化器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 学习型基数估计器在查询基数预测中表现出色，但对训练数据漂移的脆弱性限制了其在实际部署中的可靠性。论文旨在揭示这种脆弱性，并提供理论分析和防御措施。

Method: 提出了一种针对学习型基数估计器的数据中心算法复杂性攻击，证明了最优攻击策略的NP难问题，并设计了一种多项式时间的近似算法。

Result: 实验结果表明，攻击仅需修改0.8%的训练数据即可显著降低估计器性能，90%分位数的Qerror增加了三个数量级，端到端处理时间增加了高达20倍。

Conclusion: 论文揭示了学习型基数估计器的关键漏洞，并首次提供了其数据漂移下的最坏情况理论分析。此外，提出了两种防御措施，为开发鲁棒的学习型数据库优化器提供了启示。

Abstract: Learned cardinality estimators show promise in query cardinality prediction,
yet they universally exhibit fragility to training data drifts, posing risks
for real-world deployment. This work is the first to theoretical investigate
how minimal data-level drifts can maximally degrade the accuracy of learned
estimators. We propose data-centric algorithmic complexity attacks against
learned estimators in a black-box setting, proving that finding the optimal
attack strategy is NP-Hard. To address this, we design a polynomial-time
approximation algorithm with a $(1-\kappa)$ approximation ratio. Extensive
experiments demonstrate our attack's effectiveness: on STATS-CEB and IMDB-JOB
benchmarks, modifying just 0.8\% of training tuples increases the 90th
percentile Qerror by three orders of magnitude and raises end-to-end processing
time by up to 20$\times$. Our work not only reveals critical vulnerabilities in
deployed learned estimators but also provides the first unified worst-case
theoretical analysis of their fragility under data updates. Additionally, we
identify two countermeasures to mitigate such black-box attacks, offering
insights for developing robust learned database optimizers.

</details>


### [44] [JOB-Complex: A Challenging Benchmark for Traditional & Learned Query Optimization](https://arxiv.org/abs/2507.07471)
*Johannes Wehrstein,Timo Eckmann,Roman Heinrich,Carsten Binnig*

Main category: cs.DB

TL;DR: 该论文指出现有查询优化器基准测试的局限性，并提出新基准JOB-Complex以更真实地评估优化器性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能反映真实世界查询优化的复杂性，导致高估传统和学习的优化器性能。

Method: 引入JOB-Complex基准，包含30个查询和约6000个执行计划，挑战优化器的性能。

Result: 评估显示传统和学习型优化器在JOB-Complex上性能显著下降，比最优计划慢11倍。

Conclusion: JOB-Complex为优化器和成本模型的真实性能评估提供了更有效的工具。

Abstract: Query optimization is a fundamental task in database systems that is crucial
to providing high performance. To evaluate learned and traditional optimizer's
performance, several benchmarks, such as the widely used JOB benchmark, are
used. However, in this paper, we argue that existing benchmarks are inherently
limited, as they do not reflect many real-world properties of query
optimization, thus overstating the performance of both traditional and learned
optimizers. In fact, simple but realistic properties, such as joins over string
columns or complex filter predicates, can drastically reduce the performance of
existing query optimizers. Thus, we introduce JOB-Complex, a new benchmark
designed to challenge traditional and learned query optimizers by reflecting
real-world complexity. Overall, JOB-Complex contains 30 SQL queries and comes
together with a plan-selection benchmark containing nearly 6000 execution
plans, making it a valuable resource to evaluate the performance of query
optimizers and cost models in real-world scenarios. In our evaluation, we show
that traditional and learned cost models struggle to achieve high performance
on JOB-Complex, providing a runtime of up to 11x slower compared to the optimal
plans.

</details>


### [45] [A Service Architecture for Dataspaces](https://arxiv.org/abs/2507.07979)
*Benedikt T. Arnold,Christoph Lange,Christina Gillmann,Stefan Decker*

Main category: cs.DB

TL;DR: 文章提出了在数据空间中添加一个抽象层，以支持通用服务，并在EDC Connector中实现了这一架构，展示了其实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管数据空间目前主要关注数据资产的交换，但实际需求和对概念的研究表明，数据空间还需要支持服务。

Method: 提出一个抽象层，使开发者能够轻松开发与现有数据空间技术无缝集成的服务，并在EDC Connector中实现了这一架构。

Result: 实现了服务架构的原型，并证明了其在实践中的可行性。

Conclusion: 该抽象层为数据空间中的服务开发提供了便捷途径，扩展了数据空间的功能。

Abstract: Dataspaces are designed to support sovereign, trusted and decentralized data
exchange between participants forming an ecosystem. They are standardized by
initiatives such as the International Data Spaces Association or Gaia-X and
have gained adoption in several domains such as mobility, manufacturing,
tourism or culture. In dataspaces, participants use connectors to communicate
peer-to-peer. The Eclipse Dataspace Components (EDC) Connector is a broadly
adopted, open-source implementation that adheres to the standards and is
supported by a large community. As dataspaces in general, it focuses on the
exchange of data assets with associated usage policies and does not support
services. In practice, however, there is demand for dataspace-based services
and conceptual arguments support their inclusion in dataspaces. In this paper,
we propose an abstraction layer for providing generic services within
dataspaces. Adopters can use this layer to easily develop own services,
seamlessly integrated with the existing dataspace technology. Besides, we
present an initial implementation of this service architecture for the EDC
Connector and demonstrate its practical applicability.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [46] [Accelerating Transposed Convolutions on FPGA-based Edge Devices](https://arxiv.org/abs/2507.07683)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 论文提出MM2IM加速器，通过结合矩阵乘法和col2IM技术，优化边缘设备上的转置卷积（TCONV）计算，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现有输入导向映射（IOM）方法在转置卷积中存在效率低下问题，限制了生成模型在资源受限边缘设备的性能。

Method: 提出硬件-软件协同设计的MM2IM加速器，结合矩阵乘法与col2IM技术，并利用SECDA-TFLite工具包实现和评估。

Result: 在261种TCONV配置中平均加速1.9倍，知名生成模型上最高加速4.2倍，能效提升显著。

Conclusion: MM2IM在边缘设备上高效处理TCONV层，显著优于CPU基线和同类加速器。

Abstract: Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [47] [Discrete Beamforming Optimization for RISs with a Limited Phase Range and Amplitude Attenuation](https://arxiv.org/abs/2507.07342)
*Dogan Kutay Pekcan,Hongyi Liao,Ender Ayanoglu*

Main category: eess.SP

TL;DR: 论文提出了一种在有限相位范围内通过可重构智能表面（RIS）最大化用户设备接收功率的方法，并通过离散相位和振幅依赖特性优化解决方案，比穷举搜索更高效。


<details>
  <summary>Details</summary>
Motivation: 研究RIS在离散相位和振幅依赖特性下的最优功率接收问题，以克服传统方法的计算复杂度高和性能受限的挑战。

Method: 提出了一种线性时间收敛的最优搜索算法，并引入了APQ和EAPQ量化框架，利用几何投影优化配置。

Result: 分析指出，当离散相位数超过4时增益有限，且性能对相位范围和衰减敏感。所提算法为振幅受限RIS离散波束成形提供了通用性能上限。

Conclusion: 论文提出的算法和框架显著提升了RIS配置效率，为相关研究提供了理论基准和实践工具。

Abstract: This paper addresses the problem of maximizing the received power at a user
equipment via reconfigurable intelligent surface (RIS) characterized by
phase-dependent amplitude (PDA) and discrete phase shifts over a limited phase
range. Given complex RIS coefficients, that is, discrete phase shifts and PDAs,
we derive the necessary and sufficient conditions to achieve the optimal
solution. To this end, we propose an optimal search algorithm that is proven to
converge in linear time within at most NK steps, significantly outperforming
the exhaustive search approach that would otherwise be needed for RISs with
amplitude attenuation. Furthermore, we introduce a practical quantization
framework for PDA-introduced RISs termed amplitude-introduced polar
quantization (APQ), and extend it to a novel algorithm named extended
amplitude-introduced polar quantization (EAPQ) that works with geometric
projections. We derive closed-form expressions to assess how closely the
performance of the proposed RIS configuration can approximate the ideal case
with continuous phases and no attenuation. Our analysis reveals that increasing
the number of discrete phases beyond K = 4 yields only marginal gains,
regardless of attenuation levels, provided the RIS has a sufficiently wide
phase range R. Furthermore, we also show and quantify that when the phase range
R is limited, the performance is sensitive to attenuation for larger R, and
sensitive to R when there is less attenuation. Finally, the proposed optimal
algorithm provides a generic upper bound that could serve as a benchmark for
discrete beamforming in RISs with amplitude constraints.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [48] [Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task](https://arxiv.org/abs/2507.07327)
*Brian B. Vuong,Josie Davidson,Sangheui Cheon,Kyujin Cho,Allison M. Okamura*

Main category: cs.RO

TL;DR: 研究提出了一种将触觉反馈从手部移至手腕的可穿戴设备，以解决手术机器人中触觉反馈与操作手柄的冲突问题，实验表明腕部触觉反馈能显著提高力的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统手部触觉反馈在远程手术机器人中与操作手柄的冲突问题，同时验证腕部触觉反馈的有效性。

Method: 通过软气压手腕触觉设备，将组织-工具交互力传递至手腕，参与者在使用和不使用腕部触觉反馈的情况下进行组织按压任务。

Result: 实验结果显示，提供腕部触觉反馈时，参与者的施力误差显著降低，但操作时间延长。

Conclusion: 腕部触觉反馈能有效提高远程手术中的施力准确性，但可能影响操作速度。

Abstract: Previous work has shown that the addition of haptic feedback to the hands can
improve awareness of tool-tissue interactions and enhance performance of
teleoperated tasks in robot-assisted minimally invasive surgery. However,
hand-based haptic feedback occludes direct interaction with the manipulanda of
surgeon console in teleoperated surgical robots. We propose relocating haptic
feedback to the wrist using a wearable haptic device so that haptic feedback
mechanisms do not need to be integrated into the manipulanda. However, it is
unknown if such feedback will be effective, given that it is not co-located
with the finger movements used for manipulation. To test if relocated haptic
feedback improves force application during teleoperated tasks using da Vinci
Research Kit (dVRK) surgical robot, participants learned to palpate a phantom
tissue to desired forces. A soft pneumatic wrist-worn haptic device with an
anchoring system renders tool-tissue interaction forces to the wrist of the
user. Participants performed the palpation task with and without wrist-worn
haptic feedback and were evaluated for the accuracy of applied forces.
Participants demonstrated statistically significant lower force error when
wrist-worn haptic feedback was provided. Participants also performed the
palpation task with longer movement times when provided wrist-worn haptic
feedback, indicating that the haptic feedback may have caused participants to
operate at a different point in the speed-accuracy tradeoff curve.

</details>


### [49] [FiDTouch: A 3D Wearable Haptic Display for the Finger Pad](https://arxiv.org/abs/2507.07661)
*Daria Trinitatova,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: FiDTouch是一种新型3D穿戴式触觉设备，通过微型倒Delta机器人提供精确触觉反馈，提升人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 指尖触觉设备在虚拟现实、医疗培训和远程机器人操作等领域潜力巨大，但需更精确的触觉反馈技术。

Method: 设计采用微型倒Delta机器人，提供接触、压力、皮肤拉伸等动态触觉刺激，并通过用户研究评估其性能。

Result: 用户研究验证了设备在静态空间接触和皮肤拉伸刺激中的有效性，能增强沉浸感和交互效率。

Conclusion: FiDTouch通过精确触觉反馈，为人机交互提供了新的技术解决方案。

Abstract: The applications of fingertip haptic devices have spread to various fields
from revolutionizing virtual reality and medical training simulations to
facilitating remote robotic operations, proposing great potential for enhancing
user experiences, improving training outcomes, and new forms of interaction. In
this work, we present FiDTouch, a 3D wearable haptic device that delivers
cutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin
stretch, and vibrotactile feedback. The application of a tiny inverted Delta
robot in the mechanism design allows providing accurate contact and fast
changing dynamic stimuli to the finger pad surface. The performance of the
developed display was evaluated in a two-stage user study of the perception of
static spatial contact stimuli and skin stretch stimuli generated on the finger
pad. The proposed display, by providing users with precise touch and force
stimuli, can enhance user immersion and efficiency in the fields of
human-computer and human-robot interactions.

</details>


### [50] [A Graph Isomorphism-based Decentralized Algorithm for Modular Robot Configuration Formation](https://arxiv.org/abs/1602.03104)
*Ayan Dutta,Prithviraj Dasgupta,Carl Nelson*

Main category: cs.RO

TL;DR: 提出了一种基于图同构的模块化机器人系统配置形成新算法，减少了时间和能量消耗。


<details>
  <summary>Details</summary>
Motivation: 解决模块化机器人系统中模块从不同初始配置和位置移动到目标配置的问题。

Method: 采用基于效用的图同构算法，最大化保留原始配置。

Result: 算法具有完整性和帕累托最优性，规划时间短（100模块仅需毫秒级）。

Conclusion: 提出的算法在市场基准测试中表现优于其他算法，时间和通信开销更低。

Abstract: We consider the problem of configuration formation in modular robot systems
where a set of modules that are initially in different configurations and
located at different locations are required to assume appropriate positions so
that they can get into a new, user-specified, target configuration. We propose
a novel algorithm based on graph isomorphism, where the modules select
locations or spots in the target configuration using a utility-based framework,
while retaining their original configuration to the greatest extent possible,
to reduce the time and energy required by the modules to assume the target
configuration. We have shown analytically that our proposed algorithm is
complete and guarantees a Pareto-optimal allocation. Experimental simulations
of our algorithm with different number of modules in different initial
configurations and located initially at different locations, show that the
planning time of our algorithm is nominal (order of msec. for 100 modules). We
have also compared our algorithm against a market-based allocation algorithm
and shown that our proposed algorithm performs better in terms of time and
number of messages exchanged.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Bias-Aware Mislabeling Detection via Decoupled Confident Learning](https://arxiv.org/abs/2507.07216)
*Yunyi Li,Maria De-Arteaga,Maytal Saar-Tsechansky*

Main category: cs.LG

TL;DR: 论文提出了DeCoLe方法，用于检测标签偏差导致的错误标注，提升数据可靠性。


<details>
  <summary>Details</summary>
Motivation: 标签偏差是数据完整性的重要挑战，尤其在关键领域（如仇恨言论检测）中亟需解决方法。

Method: 提出Decoupled Confident Learning（DeCoLe）框架，用于检测标签偏差下的错误标注。

Result: DeCoLe在仇恨言论检测中表现优异，优于现有标签错误检测方法。

Conclusion: DeCoLe为解决标签偏差感知的标注错误提供了有效工具，可增强数据可靠性。

Abstract: Reliable data is a cornerstone of modern organizational systems. A notable
data integrity challenge stems from label bias, which refers to systematic
errors in a label, a covariate that is central to a quantitative analysis, such
that its quality differs across social groups. This type of bias has been
conceptually and empirically explored and is widely recognized as a pressing
issue across critical domains. However, effective methodologies for addressing
it remain scarce. In this work, we propose Decoupled Confident Learning
(DeCoLe), a principled machine learning based framework specifically designed
to detect mislabeled instances in datasets affected by label bias, enabling
bias aware mislabelling detection and facilitating data quality improvement. We
theoretically justify the effectiveness of DeCoLe and evaluate its performance
in the impactful context of hate speech detection, a domain where label bias is
a well documented challenge. Empirical results demonstrate that DeCoLe excels
at bias aware mislabeling detection, consistently outperforming alternative
approaches for label error detection. Our work identifies and addresses the
challenge of bias aware mislabeling detection and offers guidance on how DeCoLe
can be integrated into organizational data management practices as a powerful
tool to enhance data reliability.

</details>


### [52] [CHOMET: Conditional Handovers via Meta-Learning](https://arxiv.org/abs/2507.07581)
*Michail Kalntis,Fernando A. Kuipers,George Iosifidis*

Main category: cs.LG

TL;DR: 传统切换在现代蜂窝网络中面临延迟和失败问题，3GPP引入条件切换（CHO）解决，但CHO仍面临资源分配和信令开销挑战。本文提出基于O-RAN和新学习优化的框架，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着移动网络复杂性增加，传统切换方法难以满足需求，CHO虽能提升成功率但引入新问题，需优化。

Method: 基于O-RAN范式，提出利用元学习优化CHO的框架。

Result: 在信号波动条件下，性能比3GPP基准提升至少180%。

Conclusion: 该框架有效解决了CHO的资源分配和信令开销问题，显著提升切换性能。

Abstract: Handovers (HOs) are the cornerstone of modern cellular networks for enabling
seamless connectivity to a vast and diverse number of mobile users. However, as
mobile networks become more complex with more diverse users and smaller cells,
traditional HOs face significant challenges, such as prolonged delays and
increased failures. To mitigate these issues, 3GPP introduced conditional
handovers (CHOs), a new type of HO that enables the preparation (i.e., resource
allocation) of multiple cells for a single user to increase the chance of HO
success and decrease the delays in the procedure. Despite its advantages, CHO
introduces new challenges that must be addressed, including efficient resource
allocation and managing signaling/communication overhead from frequent cell
preparations and releases. This paper presents a novel framework aligned with
the O-RAN paradigm that leverages meta-learning for CHO optimization, providing
robust dynamic regret guarantees and demonstrating at least 180% superior
performance than other 3GPP benchmarks in volatile signal conditions.

</details>


### [53] [Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data](https://arxiv.org/abs/2507.07589)
*Arpana Sinhal,Anay Sinhal,Amit Sinhal*

Main category: cs.LG

TL;DR: 研究通过多模态数据集和机器学习模型改进医护人员压力监测系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究在数据集和分析框架上的不足，特别是在COVID-19期间医护人员的压力监测问题。

Method: 使用多模态数据集（生理信号等），结合SMOTE处理数据不平衡，采用随机森林、XGBoost和MLP模型，并通过堆叠分类器优化性能。

Result: 提出了可部署的压力监测系统，并通过公开数据集和可复现分析流程验证了有效性。

Conclusion: 研究为医护人员心理健康保护提供了实用工具，未来可扩展人口多样性和边缘计算应用。

Abstract: Healthcare professionals, particularly nurses, face elevated occupational
stress, a concern amplified during the COVID-19 pandemic. While wearable
sensors offer promising avenues for real-time stress monitoring, existing
studies often lack comprehensive datasets and robust analytical frameworks.
This study addresses these gaps by introducing a multimodal dataset comprising
physiological signals, electrodermal activity, heart rate and skin temperature.
A systematic literature review identified limitations in prior stress-detection
methodologies, particularly in handling class imbalance and optimizing model
generalizability. To overcome these challenges, the dataset underwent
preprocessing with the Synthetic Minority Over sampling Technique (SMOTE),
ensuring balanced representation of stress states. Advanced machine learning
models including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were
evaluated and combined into a Stacking Classifier to leverage their collective
predictive strengths. By using a publicly accessible dataset and a reproducible
analytical pipeline, this work advances the development of deployable
stress-monitoring systems, offering practical implications for safeguarding
healthcare workers' mental health. Future research directions include expanding
demographic diversity and exploring edge-computing implementations for low
latency stress alerts.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 该论文探讨了如何在数据稀疏且不可靠的供应链中利用神经符号方法和大型语言模型（LLM）自动检测非法活动。


<details>
  <summary>Details</summary>
Motivation: 供应链中的非法活动（如强迫劳动）数据稀疏且易被故意损坏，传统机器学习方法难以处理。需要新的方法来自动识别这类活动。

Method: 使用神经符号方法和LLM的问题树方法，从新闻文章中自动提取特征并量化其相关性。

Result: 论文比较了人工和机器对新闻文章分类的效果，提出了系统性评估方法。

Conclusion: 该方法能有效处理稀疏数据，为供应链中非法活动的检测提供了新思路。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [55] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 该论文探讨了机器学习中解释预测的重要性，尤其是在高风险领域，并分析了规则型模型中的负面问题及其影响。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，机器学习模型的解释必须严谨，错误的解释会误导决策者。因此，规则型模型（如决策树、规则集等）被广泛使用，但它们存在负面问题。

Method: 论文开发了算法来分析规则型系统中的负面问题，如负重叠和冗余。

Result: 研究发现，广泛使用的规则型模型学习工具会生成具有一个或多个负面问题的规则集。

Conclusion: 规则型模型的负面问题不可忽视，需要在模型设计和解释中加以关注。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [56] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

TL;DR: 本文提出了一种多模态实体链接（MEL）的新模型MMoE，通过多级专家混合机制解决现有方法中语义模糊和模态动态选择的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有MEL方法未能解决两个关键问题：mention的语义模糊性和动态选择模态信息的重要性，因此需要新模型来填补这一空白。

Method: 提出MMoE模型，包括描述感知的mention增强模块、多模态特征提取模块和两级专家混合模块，动态选择相关特征。

Result: 实验表明MMoE在性能上优于现有技术。

Conclusion: MMoE通过动态选择多模态信息，有效解决了MEL中的关键问题，具有实际应用价值。

Abstract: Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [57] [T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates](https://arxiv.org/abs/2507.07633)
*Zhitao Wang,Hengyu Man,Wenrui Li,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: T-GVC提出了一种轨迹引导的生成视频编码框架，通过语义感知的稀疏运动采样和轨迹对齐损失约束，解决了现有方法在ULB场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成视频编码方法在领域特异性和过度依赖高级文本引导方面的问题，以实现更逼真的视频重建。

Method: 采用语义感知的稀疏运动采样管道，并结合轨迹对齐的损失约束，以减少比特率并保留关键语义信息。

Result: 实验表明，T-GVC在ULB条件下优于传统编解码器和现有端到端视频压缩方法，并实现了更精确的运动控制。

Conclusion: T-GVC为基于几何运动建模的生成视频编码开辟了新方向。

Abstract: Recent advances in video generation techniques have given rise to an emerging
paradigm of generative video coding, aiming to achieve semantically accurate
reconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong
generative priors. However, most existing methods are limited by domain
specificity (e.g., facial or human videos) or an excessive dependence on
high-level text guidance, which often fails to capture motion details and
results in unrealistic reconstructions. To address these challenges, we propose
a Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC
employs a semantic-aware sparse motion sampling pipeline to effectively bridge
low-level motion tracking with high-level semantic understanding by extracting
pixel-wise motion as sparse trajectory points based on their semantic
importance, not only significantly reducing the bitrate but also preserving
critical temporal semantic information. In addition, by incorporating
trajectory-aligned loss constraints into diffusion processes, we introduce a
training-free latent space guidance mechanism to ensure physically plausible
motion patterns without sacrificing the inherent capabilities of generative
models. Experimental results demonstrate that our framework outperforms both
traditional codecs and state-of-the-art end-to-end video compression methods
under ULB conditions. Furthermore, additional experiments confirm that our
approach achieves more precise motion control than existing text-guided
methods, paving the way for a novel direction of generative video coding guided
by geometric motion modeling.

</details>


### [58] [SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs](https://arxiv.org/abs/2507.07610)
*Siting Wang,Luoyang Sun,Cheng Deng,Kun Shao,Minnan Pei,Zheng Tian,Haifeng Zhang,Jun Wang*

Main category: cs.CV

TL;DR: 论文提出了一个名为SpatialViz-Bench的全面多模态基准测试，用于评估大型语言模型（MLLMs）在空间可视化方面的能力。该基准包含12项任务和1180个自动生成的问题，测试了33个最先进的MLLMs，揭示了它们在空间任务上的表现差异和缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估往往依赖与训练数据重叠的IQ测试或数学竞赛，无法可靠评估空间可视化能力。因此，需要更专门的基准来填补这一空白。

Method: 开发了一个名为SpatialViz-Bench的多模态基准，包含12项任务和1180个自动生成的问题，对33个MLLMs进行了评估。

Result: 评估显示MLLMs在空间可视化任务中存在广泛性能差异和缺陷，如感知困难与人类直觉不符、2D到3D性能显著下降、偏好公式推导而非可视化。

Conclusion: SpatialViz-Bench证实了当前MLLMs在空间可视化任务中的不足，为领域提供了重要评估工具，并公开可用。

Abstract: Humans can directly imagine and manipulate visual images in their minds, a
capability known as spatial visualization. While multi-modal Large Language
Models (MLLMs) support imagination-based reasoning, spatial visualization
remains insufficiently evaluated, typically embedded within broader
mathematical and logical assessments. Existing evaluations often rely on IQ
tests or math competitions that may overlap with training data, compromising
assessment reliability. To this end, we introduce SpatialViz-Bench, a
comprehensive multi-modal benchmark for spatial visualization with 12 tasks
across 4 sub-abilities, comprising 1,180 automatically generated problems. Our
evaluation of 33 state-of-the-art MLLMs not only reveals wide performance
variations and demonstrates the benchmark's strong discriminative power, but
also uncovers counter-intuitive findings: models exhibit unexpected behaviors
by showing difficulty perception that misaligns with human intuition,
displaying dramatic 2D-to-3D performance cliffs, and defaulting to formula
derivation despite spatial tasks requiring visualization alone. SpatialVizBench
empirically demonstrates that state-of-the-art MLLMs continue to exhibit
deficiencies in spatial visualization tasks, thereby addressing a significant
lacuna in the field. The benchmark is publicly available.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [59] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj,Brindha Raghuraman,Nagarani Gopalakrishnan,Yazan Otoum*

Main category: cs.CR

TL;DR: 提出了一种混合AI驱动的网络安全框架，以增强关键基础设施的实时漏洞检测、威胁建模和自动修复能力。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施系统的互联性增加了其面临的网络威胁，如勒索软件、DoS攻击和APT攻击。

Method: 研究采用混合AI驱动的网络安全框架，结合漏洞检测、威胁建模和自动修复技术。

Result: 提供了可操作的见解，以增强关键基础设施系统对新兴网络威胁的安全性和韧性。

Conclusion: AI在提升关键基础设施网络安全方面具有潜力，但仍需解决对抗性AI和监管合规等挑战。

Abstract: Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [60] [Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling](https://arxiv.org/abs/2507.07250)
*Jordi Serra-Ruiz,David Megías*

Main category: cs.CR

TL;DR: 提出了一种用于多波段图像的半脆弱水印方案，通过树结构矢量量化方法在像素签名中嵌入标记，以检测原始图像的重大修改。


<details>
  <summary>Details</summary>
Motivation: 为了保护遥感图像免受篡改，同时能够检测到显著修改和伪造块的位置。

Method: 将图像分割为三维块，为每个块构建树结构矢量量化器，并通过迭代算法操作这些树，直到满足嵌入标记的标准。

Result: 该方法在无损压缩（超过给定阈值）下能保留水印，同时能检测伪造块及其位置。

Conclusion: 该方案有效实现了遥感图像的半脆弱水印嵌入和篡改检测。

Abstract: A semi-fragile watermarking scheme for multiple band images is presented in
this article. We propose to embed a mark into remote sensing images applying a
tree-structured vector quantization approach to the pixel signatures instead of
processing each band separately. The signature of the multispectral or
hyperspectral image is used to embed the mark in it order to detect any
significant modification of the original image. The image is segmented into
three-dimensional blocks, and a tree-structured vector quantizer is built for
each block. These trees are manipulated using an iterative algorithm until the
resulting block satisfies a required criterion, which establishes the embedded
mark. The method is shown to be able to preserve the mark under lossy
compression (above a given threshold) but, at the same time, it detects
possibly forged blocks and their position in the whole image.

</details>


### [61] [Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations](https://arxiv.org/abs/2507.07916)
*Federico Maria Cau,Giuseppe Desolda,Francesco Greco,Lucio Davide Spano,Luca Viganò*

Main category: cs.CR

TL;DR: 研究表明，大型语言模型（LLMs）生成的钓鱼警告解释在效果上可媲美或超越人工编写，Claude 3.5尤其表现优异。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击利用人类行为绕过技术防御，现有警告对话因内容静态且解释不清效果有限，需探索LLMs生成警告的潜力。

Method: 通过大规模用户研究（N=750），比较人工与两种LLMs（Claude 3.5和Llama 3.3）生成解释的效果，分析两种解释风格（基于特征和反事实）对行为与感知的影响。

Result: LLMs生成的解释可有效降低钓鱼点击率，Claude表现最佳；基于特征的风格对真实钓鱼更有效，反事实风格减少误报。性别、工作量等因素显著影响效果。

Conclusion: LLMs能自动生成高效、可扩展且人性化的钓鱼警告解释，未来可进一步优化适用性。

Abstract: Phishing has become a prominent risk in modern cybersecurity, often used to
bypass technological defences by exploiting predictable human behaviour.
Warning dialogues are a standard mitigation measure, but the lack of
explanatory clarity and static content limits their effectiveness. In this
paper, we report on our research to assess the capacity of Large Language
Models (LLMs) to generate clear, concise, and scalable explanations for
phishing warnings. We carried out a large-scale between-subjects user study (N
= 750) to compare the influence of warning dialogues supplemented with manually
generated explanations against those generated by two LLMs, Claude 3.5 Sonnet
and Llama 3.3 70B. We investigated two explanatory styles (feature-based and
counterfactual) for their effects on behavioural metrics (click-through rate)
and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that
well-constructed LLM-generated explanations can equal or surpass manually
crafted explanations in reducing susceptibility to phishing; Claude-generated
warnings exhibited particularly robust performance. Feature-based explanations
were more effective for genuine phishing attempts, whereas counterfactual
explanations diminished false-positive rates. Other variables such as workload,
gender, and prior familiarity with warning dialogues significantly moderated
warning effectiveness. These results indicate that LLMs can be used to
automatically build explanations for warning users against phishing, and that
such solutions are scalable, adaptive, and consistent with human-centred
values.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [62] [Audio-Visual Speech Separation via Bottleneck Iterative Network](https://arxiv.org/abs/2507.07270)
*Sidong Zhang,Shiv Shankar,Trang Nguyen,Andrea Fanelli,Madalina Fiterau*

Main category: cs.SD

TL;DR: 本文提出了一种名为Bottleneck Iterative Network (BIN)的轻量级迭代表征细化方法，通过在融合过程中使用瓶颈令牌，提升模型性能的同时控制模型规模和计算成本。在噪声音频-视觉语音分离任务中，BIN优于现有基准模型，并显著减少了训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度模态特定网络的语音分离模型在性能与计算成本之间难以平衡，BIN旨在通过轻量级迭代融合提升模型容量，同时避免模型规模的显著增加。

Method: 采用BIN方法，通过轻量级融合块和瓶颈令牌迭代优化融合表征，从而提升模型性能。

Result: 在NTCD-TIMIT和LRS3+WHAM!数据集上，BIN在SI-SDRi指标上优于现有基准模型，且训练和GPU推理时间减少50%以上。

Conclusion: BIN在提升语音分离性能的同时，有效控制了计算成本，为多模态信息融合提供了一种高效解决方案。

Abstract: Integration of information from non-auditory cues can significantly improve
the performance of speech-separation models. Often such models use deep
modality-specific networks to obtain unimodal features, and risk being too
costly or lightweight but lacking capacity. In this work, we present an
iterative representation refinement approach called Bottleneck Iterative
Network (BIN), a technique that repeatedly progresses through a lightweight
fusion block, while bottlenecking fusion representations by fusion tokens. This
helps improve the capacity of the model, while avoiding major increase in model
size and balancing between the model performance and training cost. We test BIN
on challenging noisy audio-visual speech separation tasks, and show that our
approach consistently outperforms state-of-the-art benchmark models with
respect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously
achieving a reduction of more than 50% in training and GPU inference time
across nearly all settings.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [63] [The Richness of CSP Non-redundancy](https://arxiv.org/abs/2507.07942)
*Joshua Brakensiek,Venkatesan Guruswami,Bart M. P. Jansen,Victor Lagerkvist,Magnus Wahlström*

Main category: cs.DM

TL;DR: 论文研究了约束满足问题（CSP）中的非冗余性（NRD），展示了非冗余性与多个重要问题的关联，并提出了一种代数理论来分类条件非冗余性。


<details>
  <summary>Details</summary>
Motivation: 非冗余性与计算机科学和数学中的许多重要问题（如稀疏化、核化、查询复杂度和极值组合学）密切相关，本文旨在深入理解非冗余性。

Method: 通过分析CSP谓词的非冗余性，提出了一种代数理论来分类条件非冗余性，并探讨了Mal'tsev嵌入的应用。

Result: 主要结果为：1. 对每个有理数r≥1，存在CSP谓词P使得其非冗余性为Θ(n^r)；2. 完全分类了二元谓词的条件非冗余性。

Conclusion: 非冗余性是研究CSP和其他领域问题的重要工具，开发的代数理论和应用为理解其结构提供了新视角。

Abstract: In the field of constraint satisfaction problems (CSP), a clause is called
redundant if its satisfaction is implied by satisfying all other clauses. An
instance of CSP$(P)$ is called non-redundant if it does not contain any
redundant clause. The non-redundancy (NRD) of a predicate $P$ is the maximum
number of clauses in a non-redundant instance of CSP$(P)$, as a function of the
number of variables $n$. Recent progress has shown that non-redundancy is
crucially linked to many other important questions in computer science and
mathematics including sparsification, kernelization, query complexity,
universal algebra, and extremal combinatorics. Given that non-redundancy is a
nexus for many of these important problems, the central goal of this paper is
to more deeply understand non-redundancy.
  Our first main result shows that for every rational number $r \ge 1$, there
exists a finite CSP predicate $P$ such that the non-redundancy of $P$ is
$\Theta(n^r)$. Our second main result explores the concept of conditional
non-redundancy first coined by Brakensiek and Guruswami [STOC 2025]. We
completely classify the conditional non-redundancy of all binary predicates
(i.e., constraints on two variables) by connecting these non-redundancy
problems to the structure of high-girth graphs in extremal combinatorics.
  Inspired by these concrete results, we build off the work of Carbonnel [CP
2022] to develop an algebraic theory of conditional non-redundancy. As an
application of this algebraic theory, we revisit the notion of Mal'tsev
embeddings, which is the most general technique known to date for establishing
that a predicate has linear non-redundancy. For example, we provide the first
example of predicate with a Mal'tsev embedding that cannot be attributed to the
structure of an Abelian group, but rather to the structure of the quantum Pauli
group.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [64] [Toolchain for Faster Iterations in Quantum Software Development](https://arxiv.org/abs/2507.07448)
*Otso Kinanen,Andrés D. Muñoz-Moller,Vlad Stirbu,Tommi Mikkonen*

Main category: quant-ph

TL;DR: 量子计算具有革命性潜力，但软件开发面临硬件限制和高计算需求等挑战。本文提出利用远程计算能力优化量子软件开发流程，通过简化本地与远程硬件的切换，实现了高达5倍的执行速度提升。


<details>
  <summary>Details</summary>
Motivation: 量子计算虽然前景广阔，但由于硬件资源有限、模拟计算需求高和技术栈复杂，开发者难以构建高效的量子软件开发流程。本文旨在通过远程计算能力解决这些问题，降低开发门槛。

Method: 利用远程计算能力优化量子软件开发流程，提出一种高效方法，简化本地与远程硬件切换，并在Jupyter笔记本中实现即插即用的内核支持。

Result: 实验中实现了高达5倍的电路执行速度提升，并支持21至29量子位的范围扩展，显著提升了开发效率。

Conclusion: 通过远程计算能力的优化，本文方法显著改善了量子软件开发流程，为开发者提供了更高效的迭代开发环境和更大的电路设计空间。

Abstract: Quantum computing proposes a revolutionary paradigm that can radically
transform numerous scientific and industrial application domains. To realize
this promise, these new capabilities need software solutions that are able to
effectively harness its power. However, developers may face significant
challenges when developing and executing quantum software due to the limited
availability of quantum computer hardware, high computational demands of
simulating quantum computers on classical systems, and complicated technology
stack to enable currently available accelerators into development environments.
These limitations make it difficult for the developer to create an efficient
workflow for quantum software development. In this paper, we investigate the
potential of using remote computational capabilities in an efficient manner to
improve the workflow of quantum software developers, by lowering the barrier of
moving between local execution and computationally more efficient remote
hardware and offering speedup in execution with simulator surroundings. The
goal is to allow the development of more complex circuits and to support an
iterative software development approach. In our experiment, with the solution
presented in this paper, we have obtained up to 5 times faster circuit
execution runtime, and enabled qubit ranges from 21 to 29 qubits with a simple
plug-and-play kernel for the Jupyter notebook.

</details>


### [65] [Quantum Executor: A Unified Interface for Quantum Computing](https://arxiv.org/abs/2507.07597)
*Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: quant-ph

TL;DR: 论文介绍了Quantum Executor，一种与后端无关的执行引擎，用于协调异构平台的量子实验，提供声明式和模块化接口，支持异步和分布式执行，并通过实际案例展示了其应用。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算从理论走向实践，亟需强大、可移植且可扩展的量子软件实验工具，以满足跨平台实验需求。

Method: 开发了Quantum Executor，该引擎通过声明式和模块化接口解耦实验设计与后端执行，支持异步和分布式执行，并提供了统一的API。

Result: 通过自动化基准测试和混合验证等案例展示了Quantum Executor的高效性和互操作性，能够简化量子开发流程。

Conclusion: 论文总结了Quantum Executor的优势，同时指出了当前限制，并提出了未来改进的路线图。

Abstract: As quantum computing evolves from theoretical promise to practical
deployment, the demand for robust, portable, and scalable tools for quantum
software experimentation is growing. This paper introduces Quantum Executor, a
backend-agnostic execution engine designed to orchestrate quantum experiments
across heterogeneous platforms. Quantum Executor provides a declarative and
modular interface that decouples experiment design from backend execution,
enabling seamless interoperability and code reuse across diverse quantum and
classical resources. Key features include support for asynchronous and
distributed execution, customizable execution strategies and a unified API for
managing quantum experiments. We illustrate its applicability through two
life-like usage scenarios such as automated benchmarking and hybrid validation,
discussing its capacity to streamline quantum development. We conclude by
discussing current limitations and outlining a roadmap for future enhancements.

</details>


### [66] [ProvideQ: A Quantum Optimization Toolbox](https://arxiv.org/abs/2507.07649)
*Domenik Eichhorn,Nick Poser,Maximilian Schweikart,Ina Schaefer*

Main category: quant-ph

TL;DR: 该论文介绍了ProvideQ工具箱，旨在解决混合求解器在实际应用中整合量子与经典计算的难题，通过Meta-Solver策略实现问题分解。


<details>
  <summary>Details</summary>
Motivation: 混合求解器虽然在理论上具有优势，但实际应用面临技术栈不兼容的挑战。ProvideQ工具箱的开发旨在实现量子与经典计算的无缝集成。

Method: 提供Meta-Solver策略和配置工具，通过分解技术将问题拆分为经典与量子子程序，支持多后端执行。

Result: 概念验证表明Meta-Solver策略已可实现量子子程序应用，但硬件性能仍需提升以增强竞争力。

Conclusion: ProvideQ工具箱为混合求解器提供了实用基础，未来需硬件改进以充分发挥潜力。

Abstract: Hybrid solvers for combinatorial optimization problems combine the advantages
of classical and quantum computing to overcome difficult computational
challenges. Although their theoretical performance seems promising, their
practical applicability is challenging due to the lack of a technological stack
that can seamlessly integrate quantum solutions with existing classical
optimization frameworks. We tackle this challenge by introducing the ProvideQ
toolbox, a software tool that enables users to easily adapt and configure
hybrid solvers via Meta-Solver strategies. A Meta-Solver strategy implements
decomposition techniques, which splits problems into classical and quantum
subroutines. The ProvideQ toolbox enables the interactive creation of such
decompositions via a Meta-Solver configuration tool. It combines
well-established classical optimization techniques with quantum circuits that
are seamlessly executable on multiple backends. This paper introduces the
technical details of the ProvideQ toolbox, explains its architecture, and
demonstrates possible applications for several real-world use cases. Our proof
of concept shows that Meta-Solver strategies already enable the application of
quantum subroutines today, however, more sophisticated hardware is required to
make their performance competitive.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [67] [Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance](https://arxiv.org/abs/2507.07881)
*Roberto Ulloa,Juhi Kulshrestha,Celina Kacperski*

Main category: cs.CY

TL;DR: 研究发现，教育水平较低的人更倾向于回避对话式AI，这可能加剧数字不平等。


<details>
  <summary>Details</summary>
Motivation: 探讨教育水平差异是否与对话式AI的回避行为相关，以揭示数字不平等问题。

Method: 通过在线实验（N=1,636）和结构方程模型分析，结合理论框架UTAUT2和LASSO回归。

Result: 对话式AI组的回避率（51%）显著高于传统搜索组（30.9%）和对照组（16.8%），教育水平较低者回避率最高（74.4%）。

Conclusion: 教育是影响AI采用的关键因素，需通过包容性设计确保新兴技术的公平使用。

Abstract: The rise of conversational AI (CAI), powered by large language models, is
transforming how individuals access and interact with digital information.
However, these tools may inadvertently amplify existing digital inequalities.
This study investigates whether differences in formal education are associated
with CAI avoidance, leveraging behavioral data from an online experiment (N =
1,636). Participants were randomly assigned to a control or an
information-seeking task, either a traditional online search or a CAI
(Perplexity AI). Task avoidance (operationalized as survey abandonment or
providing unrelated responses during task assignment) was significantly higher
in the CAI group (51%) compared to the search (30.9%) and control (16.8%)
groups, with the highest CAI avoidance among participants with lower education
levels (~74.4%). Structural equation modeling based on the theoretical
framework UTAUT2 and LASSO regressions reveal that education is strongly
associated with CAI avoidance, even after accounting for various cognitive and
affective predictors of technology adoption. These findings underscore
education's central role in shaping AI adoption and the role of self-selection
biases in AI-related research, stressing the need for inclusive design to
ensure equitable access to emerging technologies.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [68] [A 2-categorical approach to the semantics of dependent type theory with computation axioms](https://arxiv.org/abs/2507.07208)
*Matteo Spadetto*

Main category: math.LO

TL;DR: 该论文从高阶范畴论的视角研究了公理类型理论的语义，通过2维范畴论数据编码公理类型构造器，提出了显示映射2范畴模型，并证明了其在公理类型理论中的良好定义和完备性。


<details>
  <summary>Details</summary>
Motivation: 研究公理类型理论的语义，以解决在高阶范畴框架下编码类型构造器的挑战，尤其是公理与内涵类型理论的差异。

Method: 采用Richard Garner的2维范畴方法，将公理类型构造器编码为自然2维范畴论数据，构建显示映射2范畴模型。

Result: 证明了公理类型理论在2范畴模型中的语义是良好定义且完备的，并通过语义证明了内涵恒等类型的计算规则在公理理论中不可接受。

Conclusion: 公理类型理论的语义可以通过2范畴模型有效描述，且该模型能够区分公理与内涵类型理论的语义差异。

Abstract: Axiomatic type theory is a dependent type theory without computation rules.
The term equality judgements that usually characterise these rules are replaced
by computation axioms, i.e., additional term judgements that are typed by
identity types. This paper is devoted to providing an effective description of
its semantics, from a higher categorical perspective: given the challenge of
encoding intensional type formers into 1-dimensional categorical terms and
properties, a challenge that persists even for axiomatic type formers, we adopt
Richard Garner's approach in the 2-dimensional study of dependent types. We
prove that the type formers of axiomatic theories can be encoded into natural
2-dimensional category theoretic data, obtaining a presentation of the
semantics of axiomatic type theory via 2-categorical models called display map
2-categories. In the axiomatic case, the 2-categorical requirements identified
by Garner for interpreting intensional type formers are relaxed. Therefore, we
obtain a presentation of the semantics of the axiomatic theory that generalises
Garner's one for the intensional case. Our main result states that the
interpretation of axiomatic theories within display map 2-categories is
well-defined and enjoys the soundness property. We use this fact to provide a
semantic proof that the computation rule of intensional identity types is not
admissible in axiomatic type theory. This is achieved via a revisitation of
Hofmann and Streicher's groupoid model that believes axiomatic identity types
but does not believe intensional ones.

</details>
