<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 2]
- [cs.NI](#cs.NI) [Total: 23]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 12]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.AI](#cs.AI) [Total: 3]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Teaching Code Refactoring Using LLMs](https://arxiv.org/abs/2508.09332)
*Anshul Khairnar,Aarya Rajoju,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 提出利用大语言模型（LLMs）为软件工程课程的代码重构教学提供实时情境反馈，以解决传统方法反馈有限的问题。


<details>
  <summary>Details</summary>
Motivation: 代码重构教学面临复杂代码库的挑战，传统方法如代码审查和静态分析工具反馈有限且不一致。

Method: 通过在课程项目中集成LLM辅助重构，使用结构化提示帮助学生识别和处理代码异味（如长方法和低内聚）。

Result: 初步结果表明，LLMs能弥合理论与实践的鸿沟，帮助学生更深入理解可维护性和重构原则。

Conclusion: LLMs在代码重构教学中具有潜力，能显著提升学生学习效果和代码质量。

Abstract: This Innovative Practice full paper explores how Large Language Models (LLMs)
can enhance the teaching of code refactoring in software engineering courses
through real-time, context-aware feedback. Refactoring improves code quality
but is difficult to teach, especially with complex, real-world codebases.
Traditional methods like code reviews and static analysis tools offer limited,
inconsistent feedback. Our approach integrates LLM-assisted refactoring into a
course project using structured prompts to help students identify and address
code smells such as long methods and low cohesion. Implemented in Spring 2025
in a long-lived OSS project, the intervention is evaluated through student
feedback and planned analysis of code quality improvements. Findings suggest
that LLMs can bridge theoretical and practical learning, supporting a deeper
understanding of maintainability and refactoring principles.

</details>


### [2] [Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser](https://arxiv.org/abs/2508.09366)
*Qiaolin Qin,Xingfang Wu,Heng Li,Ettore Merlo*

Main category: cs.SE

TL;DR: 本文介绍了一种新型的统计型日志解析器PIPLUP，挑战了统计型解析器不如语义型解析器的普遍观点，展示了其准确性、通用性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有统计型日志解析器虽效率高且隐私保护性好，但在准确性和通用性上不足。本文挑战了这一现状，提出PIPLUP以提升性能。

Method: PIPLUP摒弃了固定位置的假设，采用数据不敏感参数实现"即插即用"，并通过实验验证其性能。

Result: PIPLUP在大型开源日志数据集上表现优异，优于Drain及其变体，与LUNAR竞争，且无需GPU加速或外部API。

Conclusion: PIPLUP是一种简单、高效且实用的日志解析方法，尤其适用于成本与隐私敏感的场景。

Abstract: Log parsing is an essential task in log analysis, and many tools have been
designed to accomplish it. Existing log parsers can be categorized into
statistic-based and semantic-based approaches. In comparison to semantic-based
parsers, existing statistic-based parsers tend to be more efficient, require
lower computational costs, and be more privacy-preserving thanks to on-premise
deployment, but often fall short in their accuracy (e.g., grouping or parsing
accuracy) and generalizability. Therefore, it became a common belief that
statistic-based parsers cannot be as effective as semantic-based parsers since
the latter could take advantage of external knowledge supported by pretrained
language models. Our work, however, challenges this belief with a novel
statistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the
position of constant tokens for log grouping and relies on data-insensitive
parameters to overcome the generalizability challenge, allowing "plug and play"
on given log files. According to our experiments on an open-sourced large log
dataset, PIPLUP shows promising accuracy and generalizability with the
data-insensitive default parameter set. PIPLUP not only outperforms the
state-of-the-art statistic-based log parsers, Drain and its variants, but also
obtains a competitive performance compared to the best unsupervised
semantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time
consumption without GPU acceleration and external API usage; our simple,
efficient, and effective approach makes it more practical in real-world
adoptions, especially when costs and privacy are of major concerns.

</details>


### [3] [Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion](https://arxiv.org/abs/2508.09537)
*Yanzhou Li,Tianlin Li,Yiran Zhang,Shangqing Liu,Aishan Liu,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出了一种三阶段的方法，通过意图推断和交互式细化，提高LLM在缺乏注释的情况下生成代码的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实代码库中常缺乏显式注释（如文档字符串），导致LLM性能大幅下降，需要一种方法在缺乏注释时仍能准确生成代码。

Method: 方法分为三阶段：1) 意图推断，通过分析目标函数前的代码提取功能线索；2) 交互式细化，通过候选意图选择或编辑进一步校准意图；3) 基于最终意图生成代码。

Result: 实验表明，该方法在多类LLM上均显著提升性能，相对改进超过20%，交互式细化阶段能进一步优化结果。

Conclusion: 三阶段方法有效解决了缺乏注释时的代码生成问题，结合交互式细化显著提高了准确性和实用性。

Abstract: Large Language Models (LLMs) are increasingly used for function completion in
repository-scale codebases. Prior studies demonstrate that when explicit
instructions--such as docstrings--are provided, these models can generate
highly accurate implementations. However, in real-world repositories, such
annotations are frequently absent, and performance drops substantially without
them. To address this gap, we frame the task as a three-stage process. The
first stage focuses on intent inference, where the model analyzes the code
preceding the target function to uncover cues about the desired functionality.
Such preceding context often encodes subtle but critical information, and we
design a reasoning-based prompting framework to guide the LLM through
step-by-step extraction and synthesis of these signals before any code is
generated. The second stage introduces an optional interactive refinement
mechanism to handle cases where preceding context alone is insufficient for
intent recovery. In this stage, the model proposes a small set of candidate
intentions, enabling the developer to select or edit them so that the inferred
intent closely matches the actual requirement. Finally, in the third stage, the
LLM generates the target function conditioned on the finalized intent. To
support this pipeline, we curate a dataset of 40,000 examples annotated with
intermediate reasoning traces and corresponding docstrings. Extensive
experiments on DevEval and ComplexCodeEval show that our approach consistently
boosts multiple LLMs, achieving over 20\% relative gains in both
reference-based and execution-based metrics, with the interactive refinement
stage delivering additional improvements beyond these gains.

</details>


### [4] [ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation](https://arxiv.org/abs/2508.09648)
*Taohong Zhu,Lucas C. Cordeiro,Youcheng Sun*

Main category: cs.SE

TL;DR: ReqInOne是一种基于LLM的代理，通过模块化方法将自然语言转换为结构化SRS，相比现有方法生成更准确和结构化的文档。


<details>
  <summary>Details</summary>
Motivation: 手动编写软件需求规格说明书（SRS）耗时且易产生歧义，现有自动化方法依赖人工分析，而基于LLM的方法存在幻觉和可控性不足的问题。

Method: ReqInOne将SRS生成分解为摘要、需求提取和需求分类三个任务，采用模块化架构，并为每个任务设计定制提示模板。

Result: 实验表明，ReqInOne生成的SRS文档比基于GPT-4的整体方法和新手工程师更准确和结构良好，其需求分类组件性能媲美或优于现有最先进模型。

Conclusion: ReqInOne的模块化设计有效提升了LLM在SRS生成中的表现，为自动化需求工程提供了新思路。

Abstract: Software Requirements Specification (SRS) is one of the most important
documents in software projects, but writing it manually is time-consuming and
often leads to ambiguity. Existing automated methods rely heavily on manual
analysis, while recent Large Language Model (LLM)-based approaches suffer from
hallucinations and limited controllability. In this paper, we propose ReqInOne,
an LLM-based agent that follows the common steps taken by human requirements
engineers when writing an SRS to convert natural language into a structured
SRS. ReqInOne adopts a modular architecture by decomposing SRS generation into
three tasks: summary, requirement extraction, and requirement classification,
each supported by tailored prompt templates to improve the quality and
consistency of LLM outputs.
  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the
generated SRSs against those produced by the holistic GPT-4-based method from
prior work as well as by entry-level requirements engineers. Expert evaluations
show that ReqInOne produces more accurate and well-structured SRS documents.
The performance advantage of ReqInOne benefits from its modular design, and
experimental results further demonstrate that its requirement classification
component achieves comparable or even better results than the state-of-the-art
requirement classification model.

</details>


### [5] [DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity](https://arxiv.org/abs/2508.09676)
*Vishal Khare,Vijay Saini,Deepak Sharma,Anand Kumar,Ankit Rana,Anshul Yadav*

Main category: cs.SE

TL;DR: DeputyDev是一款AI代码审查助手，旨在解决软件开发中低效的代码审查问题，通过自动化审查显著减少审查时间和提高效率。


<details>
  <summary>Details</summary>
Motivation: 代码审查过程存在低效问题（如时间长、反馈不一致、质量参差不齐），影响开发流程和代码质量。

Method: 开发DeputyDev的自动化代码审查功能，并通过包含200多名工程师的双控A/B实验验证其效果。

Result: DeputyDev显著减少了每PR（23.09%）和每行代码（40.13%）的审查时间，并成功在企业内部和外部推广。

Conclusion: AI辅助代码审查能有效改进开发工作流程和代码质量。

Abstract: This study investigates the implementation and efficacy of DeputyDev, an
AI-powered code review assistant developed to address inefficiencies in the
software development process. The process of code review is highly inefficient
for several reasons, such as it being a time-consuming process, inconsistent
feedback, and review quality not being at par most of the time. Using our
telemetry data, we observed that at TATA 1mg, pull request (PR) processing
exhibits significant inefficiencies, with average pick-up and review times of
73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review
cycle was marked by prolonged iterative communication between the reviewing and
submitting parties. Research from the University of California, Irvine
indicates that interruptions can lead to an average of 23 minutes of lost
focus, critically affecting code quality and timely delivery. To address these
challenges, we developed DeputyDev's PR review capabilities by providing
automated, contextual code reviews. We conducted a rigorous double-controlled
A/B experiment involving over 200 engineers to evaluate DeputyDev's impact on
review times. The results demonstrated a statistically significant reduction in
both average per PR (23.09%) and average per-line-of-code (40.13%) review
durations. After implementing safeguards to exclude outliers, DeputyDev has
been effectively rolled out across the entire organisation. Additionally, it
has been made available to external companies as a Software-as-a-Service (SaaS)
solution, currently supporting the daily work of numerous engineering
professionals. This study explores the implementation and effectiveness of
AI-assisted code reviews in improving development workflow timelines and code.

</details>


### [6] [Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering](https://arxiv.org/abs/2508.09680)
*Orvila Sarker,Mona Jamshaid,M. Ali Babar*

Main category: cs.SE

TL;DR: 总结了自闭症个体在ICT领域（尤其是软件工程）的潜力与障碍，并提出了促进其教育和职业包容的成功因素。


<details>
  <summary>Details</summary>
Motivation: 自闭症个体在软件工程中具有独特优势，但面临多重障碍。研究旨在通过系统性综述填补从教育到职场包容的路径知识空白。

Method: 对30项研究进行系统性综述，归纳出18个成功因素，分为四类：软件工程教育、职业培训、工作环境及工具技术。

Result: 提出基于证据的建议，如包容性会议实践、结构化工作环境、明确职责及个性化工作调整等。

Conclusion: 为教育机构、雇主和工具开发者提供了促进自闭症个体在软件工程领域包容性的具体策略。

Abstract: Research has highlighted the valuable contributions of autistic individuals
in the Information and Communication Technology (ICT) sector, particularly in
areas such as software development, testing, and cybersecurity. Their strengths
in information processing, attention to detail, innovative thinking, and
commitment to high-quality outcomes in the ICT domain are well-documented.
However, despite their potential, autistic individuals often face barriers in
Software Engineering (SE) roles due to a lack of personalised tools, complex
work environments, non-inclusive recruitment practices, limited co-worker
support, challenging social dynamics and so on. Motivated by the ethical
framework of the neurodiversity movement and the success of pioneering
initiatives like the Dandelion program, corporate Diversity, Equity, and
Inclusion (DEI) in the ICT sector has increasingly focused on autistic talent.
This movement fundamentally reframes challenges not as individual deficits but
as failures of environments designed for a neurotypical majority. Despite this
progress, there is no synthesis of knowledge reporting the full pathway from
software engineering education through to sustainable workplace inclusion. To
address this, we conducted a Systematic Review of 30 studies and identified 18
success factors grouped into four thematic categories: (1) Software Engineering
Education, (2) Career and Employment Training, (3) Work Environment, and (4)
Tools and Assistive Technologies. Our findings offer evidence-based
recommendations for educational institutions, employers, organisations, and
tool developers to enhance the inclusion of autistic individuals in SE. These
include strategies for inclusive meeting and collaboration practices,
accessible and structured work environments, clear role and responsibility
definitions, and the provision of tailored workplace accommodations.

</details>


### [7] [LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations](https://arxiv.org/abs/2508.09791)
*Junxiao Han,Yarong Wang,Xiaodong Gu,Cuiyun Gao,Yao Wan,Song Han,David Lo,Shuiguang Deng*

Main category: cs.SE

TL;DR: LibRec是一个结合LLMs和RAG技术的新框架，用于自动化推荐替代库，并通过上下文学习从提交消息中提取迁移意图以提高推荐准确性。作者还提出了LibEval基准来评估性能。


<details>
  <summary>Details</summary>
Motivation: 解决库迁移推荐中的自动化问题，并提高推荐的准确性。

Method: 结合LLMs和RAG技术，利用上下文学习提取迁移意图，并引入LibEval基准进行评估。

Result: 评估了十种流行LLM的性能，进行了消融实验、提示策略分析和失败案例研究。

Conclusion: LibRec在库迁移推荐任务中表现出色，LibEval为未来研究提供了有效基准。

Abstract: In this paper, we propose LibRec, a novel framework that integrates the
capabilities of LLMs with retrieval-augmented generation(RAG) techniques to
automate the recommendation of alternative libraries. The framework further
employs in-context learning to extract migration intents from commit messages
to enhance the accuracy of its recommendations. To evaluate the effectiveness
of LibRec, we introduce LibEval, a benchmark designed to assess the performance
in the library migration recommendation task. LibEval comprises 2,888 migration
records associated with 2,368 libraries extracted from 2,324 Python
repositories. Each migration record captures source-target library pairs, along
with their corresponding migration intents and intent types. Based on LibEval,
we evaluated the effectiveness of ten popular LLMs within our framework,
conducted an ablation study to examine the contributions of key components
within our framework, explored the impact of various prompt strategies on the
framework's performance, assessed its effectiveness across various intent
types, and performed detailed failure case analyses.

</details>


### [8] [Fast and Accurate Heuristics for Bus-Factor Estimation](https://arxiv.org/abs/2508.09828)
*Sebastiano Antonio Piccolo*

Main category: cs.SE

TL;DR: 提出两种基于图剥离的启发式方法（最小覆盖和最大覆盖），用于估算软件项目的“bus-factor”，并在大规模图上验证其性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前计算“bus-factor”的复杂度高且不准确，难以扩展至大规模软件系统，因此需要更高效的近似方法。

Method: 将软件项目建模为开发者和任务的双边图，提出两种基于迭代图剥离的启发式算法（最小覆盖和最大覆盖）。

Result: 提出的方法在性能上优于现有方法，能够在分钟内处理百万级节点图，并提供更准确的估算结果。

Conclusion: 新启发式方法不仅更准确，且对图结构变化具有鲁棒性，已开源实现以支持进一步研究和实际应用。

Abstract: The bus-factor is a critical risk indicator that quantifies how many key
contributors a project can afford to lose before core knowledge or
functionality is compromised. Despite its practical importance, accurately
computing the bus-factor is NP-Hard under established formalizations, making
scalable analysis infeasible for large software systems.
  In this paper, we model software projects as bipartite graphs of developers
and tasks and propose two novel approximation heuristics, Minimum Coverage and
Maximum Coverage, based on iterative graph peeling, for two influential
bus-factor formalizations. Our methods significantly outperform the widely
adopted degree-based heuristic, which we show can yield severely inflated
estimates.
  We conduct a comprehensive empirical evaluation on over $1\,000$ synthetic
power-law graphs and demonstrate that our heuristics provide tighter estimates
while scaling to graphs with millions of nodes and edges in minutes. Our
results reveal that the proposed heuristics are not only more accurate but also
robust to structural variations in developer-task assignment graph. We release
our implementation as open-source software to support future research and
practical adoption.

</details>


### [9] [Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification](https://arxiv.org/abs/2508.09832)
*Linh Nguyen,Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 本文探讨了使用大型语言模型（LLMs）分类代码审查评论的潜力，发现其性能优于现有的深度学习方法，尤其是在低训练样本类别中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统的代码审查评论分类方法依赖监督学习，需要大量人工标注；而LLMs可能提供更高效的解决方案。

Method: 利用LLMs对17类代码审查评论进行分类，并与现有基于深度学习的先进方法进行比较。

Result: LLMs在分类评论时表现优于现有方法，尤其是在低频类别中表现更佳，且能提供更均衡的分类性能。

Conclusion: LLMs可以作为代码审查分析的规模化解决方案，提升代码审查的效率。

Abstract: Code review is a crucial practice in software development. As code review
nowadays is lightweight, various issues can be identified, and sometimes, they
can be trivial. Research has investigated automated approaches to classify
review comments to gauge the effectiveness of code reviews. However, previous
studies have primarily relied on supervised machine learning, which requires
extensive manual annotation to train the models effectively. To address this
limitation, we explore the potential of using Large Language Models (LLMs) to
classify code review comments. We assess the performance of LLMs to classify 17
categories of code review comments. Our results show that LLMs can classify
code review comments, outperforming the state-of-the-art approach using a
trained deep learning model. In particular, LLMs achieve better accuracy in
classifying the five most useful categories, which the state-of-the-art
approach struggles with due to low training examples. Rather than relying
solely on a specific small training data distribution, our results show that
LLMs provide balanced performance across high- and low-frequency categories.
These results suggest that the LLMs could offer a scalable solution for code
review analytics to improve the effectiveness of the code review process.

</details>


### [10] [An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues](https://arxiv.org/abs/2508.09875)
*Jinbao Chen,Boyao Ding,Yu Zhang,Qingwei Li,Fugen Tang*

Main category: cs.SE

TL;DR: 论文研究了Go语言中CGO的使用情况，揭示了其分布、模式、目的和关键问题，并提出了临时解决方案和改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管CGO提升了效率，但也带来独特风险，现有研究却忽视了这一领域。

Method: 通过对920个开源Go项目的实证研究，开发了CGOAnalyzer工具进行分析。

Result: 发现11.3%的项目使用CGO，存在19类问题，包括可能导致崩溃的关键问题，并提出了临时解决方案。

Conclusion: 研究为开发者和Go团队提供了宝贵见解，有助于提升开发效率和工具链的稳健性。

Abstract: Multilingual software development integrates multiple languages into a single
application, with the Foreign Function Interface (FFI) enabling seamless
interaction. While FFI boosts efficiency and extensibility, it also introduces
risks. Existing studies focus on FFIs in languages like Python and Java,
neglecting CGO, the emerging FFI in Go, which poses unique risks.
  To address these concerns, we conduct an empirical study of CGO usage across
920 open-source Go projects. Our study aims to reveal the distribution,
patterns, purposes, and critical issues associated with CGO, offering insights
for developers and the Go team. We develop CGOAnalyzer, a tool to efficiently
identify and quantify CGO-related features. Our findings reveal that: (1) 11.3%
of analyzed Go projects utilize CGO, with usage concentrated in a subset of
projects; (2) CGO serves 4 primary purposes, including system-level
interactions and performance optimizations, with 15 distinct usage patterns
observed; (3) 19 types of CGO-related issues exist, including one critical
issue involving unnecessary pointer checks that pose risks of runtime crashes
due to limitations in the current Go compilation toolchain; (4) a temporary
solution reduces unnecessary pointer checks, mitigating crash risks, and (5) we
submitted a proposal to improve the Go toolchain for a permanent fix, which has
been grouped within an accepted proposal for future resolution. Our findings
provide valuable insights for developers and the Go team, enhancing development
efficiency and reliability while improving the robustness of the Go toolchain.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Invertible Syntax without the Tuples (Functional Pearl)](https://arxiv.org/abs/2508.09856)
*Mathieu Boespflug,Arnaud Spiwack*

Main category: cs.PL

TL;DR: 论文提出利用延续传递风格（CPS）作为替代依赖类型和单子聚合的方法，重新审视Danvy的原始方法在通用场景中的适用性。


<details>
  <summary>Details</summary>
Motivation: 探讨延续传递风格在解析和打印结构化数据中的作用，弥补当前研究中对CPS的忽视，重新强调其重要性。

Method: 提出三种基于延续传递风格的解决方案，逐步提升表达能力，用于解析和打印结构化数据。

Result: 证明了CPS在通用场景下依然有效，能够替代依赖类型和单子聚合方法。

Conclusion: 延续传递风格在结构化数据处理中仍具重要意义，适用于更广泛的场景。

Abstract: In the seminal paper Functional unparsing, Olivier Danvy used continuation
passing to reanalyse printf-like format strings as combinators. In the
intervening decades, the conversation shifted towards a concurrent line of work
-- applicative, monadic or arrow-based combinator libraries -- in an effort to
find combinators for invertible syntax descriptions that simultaneously
determine a parser as well as a printer, and with more expressive power, able
to handle inductive structures such as lists and trees. Along the way,
continuation passing got lost. This paper argues that Danvy's insight remains
as relevant to the general setting as it was to the restricted setting of his
original paper. Like him, we present three solutions that exploit
continuation-passing style as an alternative to both dependent types and
monoidal aggregation via nested pairs, in our case to parse and print
structured data with increasing expressive power.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [12] [A Limits Study of Memory-side Tiering Telemetry](https://arxiv.org/abs/2508.09351)
*Vinicius Petrucci,Felippe Zacarias,David Roberts*

Main category: cs.OS

TL;DR: 本文介绍了一种基于CXL的实验性内存请求记录器，通过实时内存访问模式分析提出Hotness Monitoring Unit (HMU)改进内存分层方案。


<details>
  <summary>Details</summary>
Motivation: 需要提升计算系统中多种内存和存储层的管理效率。

Method: 结合数据地址监测（反应式）、主动数据迁移和编译器提示，开发HMU优化内存分层。

Result: 在DLRM测试中，对比Linux NUMA平衡分层提速1.94倍，90%页面迁移到CXL内存时仅慢3%。

Conclusion: 现有分层策略覆盖性和准确性不足，设备级遥测是可扩展的高效解决方案。

Abstract: Increasing workload demands and emerging technologies necessitate the use of
various memory and storage tiers in computing systems. This paper presents
results from a CXL-based Experimental Memory Request Logger that reveals
precise memory access patterns at runtime without interfering with the running
workloads. We use it for software emulation of future memory telemetry
hardware. By combining reactive placement based on data address monitoring,
proactive data movement, and compiler hints, a Hotness Monitoring Unit (HMU)
within memory modules can greatly improve memory tiering solutions. Analysis of
page placement using profiled access counts on a Deep Learning Recommendation
Model (DLRM) indicates a potential 1.94x speedup over Linux NUMA balancing
tiering, and only a 3% slowdown compared to Host-DRAM allocation while
offloading over 90% of pages to CXL memory. The study underscores the
limitations of existing tiering strategies in terms of coverage and accuracy,
and makes a strong case for programmable, device-level telemetry as a scalable
and efficient solution for future memory systems.

</details>


### [13] [Holistic Heterogeneous Scheduling for Autonomous Applications using Fine-grained, Multi-XPU Abstraction](https://arxiv.org/abs/2508.09503)
*Mingcong Han,Weihang Shen,Rong Chen,Binyu Zang,Haibo Chen*

Main category: cs.OS

TL;DR: 论文提出XAUTO，一种针对多异构处理器（XPUs）的运行时系统，通过细粒度任务管理优化延迟敏感型自主应用的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 现有运行时系统（如ROS）仅支持模块级任务管理，无法充分利用多XPUs的细粒度计算资源，导致延迟较高。

Method: XAUTO引入细粒度多XPU编程抽象XNODE，与任务阶段对齐，并通过全局调度优化XPU分配与执行顺序。

Result: 实验表明，XAUTO将自动驾驶感知管道的端到端延迟降低了1.61倍，优于ROS2。

Conclusion: XAUTO通过细粒度任务管理和多XPU协同，显著提升了延迟敏感型自主应用的性能。

Abstract: Modern autonomous applications are increasingly utilizing multiple
heterogeneous processors (XPUs) to accelerate different stages of algorithm
modules. However, existing runtime systems for these applications, such as ROS,
can only perform module-level task management, lacking awareness of the
fine-grained usage of multiple XPUs. This paper presents XAUTO, a runtime
system designed to cooperatively manage XPUs for latency-sensitive autonomous
applications. The key idea is a fine-grained, multi-XPU programming abstraction
-- XNODE, which aligns with the stage-level task granularity and can
accommodate multiple XPU implementations. XAUTO holistically assigns XPUs to
XNODEs and schedules their execution to minimize end-to-end latency.
Experimental results show that XAUTO can reduce the end-to-end latency of a
perception pipeline for autonomous driving by 1.61x compared to a
state-of-the-art module-level scheduling system (ROS2).

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [14] [Agentic TinyML for Intent-aware Handover in 6G Wireless Networks](https://arxiv.org/abs/2508.09147)
*Alaa Saleh,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.NI

TL;DR: WAAN是一个跨层框架，通过嵌入轻量级TinyML代理实现意图感知和主动切换，解决了6G网络中传统切换机制的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络向AI驱动、用户为中心的生态系统发展，传统反应式切换机制在移动边缘计算和自主代理服务场景中表现不足。

Method: WAAN框架利用TinyML代理作为自主协商实体，嵌入异构边缘节点，实现意图传播和网络适配，并通过半稳定集合点确保连续性。

Result: 通过多模态环境控制案例展示了WAAN在移动性下维持用户体验的有效性。

Conclusion: 文章探讨了WAAN部署和演化的关键挑战及未来机遇。

Abstract: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,
traditional reactive handover mechanisms demonstrate limitations, especially in
mobile edge computing and autonomous agent-based service scenarios. This
manuscript introduces WAAN, a cross-layer framework that enables intent-aware
and proactive handovers by embedding lightweight TinyML agents as autonomous,
negotiation-capable entities across heterogeneous edge nodes that contribute to
intent propagation and network adaptation. To ensure continuity across
mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points
that serve as coordination anchors for context transfer and state preservation.
The framework's operational capabilities are demonstrated through a multimodal
environmental control case study, highlighting its effectiveness in maintaining
user experience under mobility. Finally, the article discusses key challenges
and future opportunities associated with the deployment and evolution of WAAN.

</details>


### [15] [Metrics for Assessing Changes in Flow-based Networks](https://arxiv.org/abs/2508.09573)
*Michał Rzepka,Piotr Chołda*

Main category: cs.NI

TL;DR: 该论文提出了一套量化网络负载和测量单个流对整体网络状态影响的指标，通过百分位值和样本分布分析数据，并引入利用率评分。采用改进的Shapley值方法衡量流对网络的影响，评估了11种指标的有效性，为未来研究提供了框架。


<details>
  <summary>Details</summary>
Motivation: 解决网络性能评估在流量波动情况下的挑战，尤其是峰值数据率对网络资源的影响。

Method: 通过百分位值和样本分布分析链路和流数据，引入利用率评分和改进的Shapley值方法。

Result: 评估了11种指标，其中3种能有效捕捉流引起的网络状态变化且易于维护。

Conclusion: 研究方法为未来网络性能评估提供了扩展和优化的框架。

Abstract: This paper addresses the challenges of evaluating network performance in the
presence of fluctuating traffic patterns, with a particular focus on the impact
of peak data rates on network resources. We introduce a set of metrics to
quantify network load and measure the impact of individual flows on the overall
network state. By analyzing link and flow data through percentile values and
sample distributions, and introducing the Utilization Score metric, the
research provides insights into resource utilization under varying network
conditions. Furthermore, we employ a modified Shapley value-based approach to
measure the influence of individual flows on the network, offering a better
understanding of their contribution to network performance. The paper reviews
and compares 11 metrics across various network scenarios, evaluating their
practical relevance for research and development. Our evaluation demonstrates
that these metrics effectively capture changes in network state induced by
specific flows, with three of them offering a broad range of valuable insights
while remaining relatively easy to maintain. Moreover, the methodology
described in this paper serves as a framework for future research, with the
potential to expand and refine the set of metrics used to evaluate flow impact
on network performance.

</details>


### [16] [Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks](https://arxiv.org/abs/2508.09149)
*Seyed Hossein Ahmadpanah*

Main category: cs.NI

TL;DR: 该论文提出了一个名为SP-LLM的语义感知主动式LLM编排框架，用于解决动态车载环境中传统管理系统的不足。通过将数字孪生升级为预测性数字孪生，并结合大型语言模型的认知能力，框架实现了前瞻性的任务卸载和资源分配决策。实验表明，该框架在可扩展性、鲁棒性和适应性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前车载边缘计算（VEC）的管理系统多为静态和反应式，难以应对极端动态的车载环境。传统方法基于静态优化目标和当前网络状态，无法满足未来智能车载网络的需求。

Method: 论文提出SP-LLM框架，通过预测性数字孪生（pDT）预测关键网络参数（如任务到达、车辆移动性和信道质量），并利用大型语言模型（LLM）作为认知编排器，实现前瞻性决策。LLM能够理解自然语言的高层语义命令，动态调整优化策略以适应战略目标的变化。

Result: 通过大量仿真实验，证明SP-LLM在可扩展性、动态环境鲁棒性和适应性方面显著优于现有的反应式和基于MARL的方法。

Conclusion: SP-LLM框架通过将人类意图转化为最优网络行为，为更智能、自主和目标驱动的车载网络提供了可能。

Abstract: Next-generation automotive applications require vehicular edge computing
(VEC), but current management systems are essentially fixed and reactive. They
are suboptimal in extremely dynamic vehicular environments because they are
constrained to static optimization objectives and base their decisions on the
current network states. This paper presents a novel Semantic-Aware Proactive
LLM Orchestration (SP-LLM) framework to address these issues. Our method
transforms the traditional Digital Twin (DT) into a Predictive Digital Twin
(pDT) that predicts important network parameters such as task arrivals, vehicle
mobility, and channel quality. A Large Language Model (LLM) that serves as a
cognitive orchestrator is at the heart of our framework. It makes proactive,
forward-looking decisions about task offloading and resource allocation by
utilizing the pDT's forecasts. The LLM's ability to decipher high-level
semantic commands given in natural language is crucial because it enables it to
dynamically modify its optimization policy to match evolving strategic
objectives, like giving emergency services priority or optimizing energy
efficiency. We show through extensive simulations that SP-LLM performs
significantly better in terms of scalability, robustness in volatile
conditions, and adaptability than state-of-the-art reactive and MARL-based
approaches. More intelligent, autonomous, and goal-driven vehicular networks
will be possible due to our framework's outstanding capacity to convert human
intent into optimal network behavior.

</details>


### [17] [Enabling On-demand Guaranteed QoS for Real Time Video Streaming from Vehicles in 5G Advanced with CAPIF & NEF APIs](https://arxiv.org/abs/2508.09150)
*Pietro Piscione,Leonardo Lossi,Maziar Nekovee,Chathura Galkandage,Phil O Connor,Simon Davies*

Main category: cs.NI

TL;DR: 该论文展示了如何将5G高级网络功能与通用API框架（CAPIF）集成，以支持汽车应用的增强连接性，包括动态QoS调整和流量边缘重定向。


<details>
  <summary>Details</summary>
Motivation: 通过5G网络功能与CAPIF的集成，提升汽车应用的网络性能和服务质量。

Method: 设计并实现了一个概念验证（PoC），利用3GPP NEF API通过CAPIF监控网络性能并动态调整QoS，同时将流量重定向至边缘。

Result: 成功展示了网络性能的实时监控、QoS动态调整及流量边缘优化，提升了连接性和资源利用率。

Conclusion: 该PoC验证了5G与CAPIF结合在汽车应用中的潜力，为未来的网络优化提供了可行方案。

Abstract: This paper presents the design and implementation of a Proof of Concept (PoC)
that demonstrates how 5G Advanced Network Functions can be integrated with the
Common API Framework (CAPIF) to support enhanced connectivity for automotive
applications. The PoC shows the continuous monitoring of the mobile network
performance and the on-demand and dynamic adaptation of Quality of Service
(QoS) for selected 5G User Equipment (UE) video streaming traffic flows using
standard 3GPP Network Exposure Function (NEF) APIs exposed via CAPIF. Moreover,
traffic flows are redirected to the edge to improve latency and optimize
network resource utilization.

</details>


### [18] [Physiological Signal-Driven QoE Optimization for Wireless Virtual Reality Transmission](https://arxiv.org/abs/2508.09151)
*Chang Wu,Yuang Chen,Yiyuan Chen,Fengqian Guo,Xiaowei Qin,Hancheng Lu*

Main category: cs.NI

TL;DR: 论文提出了基于生理信号的VR流媒体QoE建模和优化框架，通过动态调整分辨率减少用户的感知不适，实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有QoE模型未能充分解决VR流媒体中分辨率突变对用户体验的影响，因此需要一种更精确的方法来捕捉用户的生理反应。

Method: 提出了一种基于EEG、ECG和皮肤活动信号的QoE建模框架，并结合深度强化学习动态分配无线资源以减少分辨率突变。

Result: 该方法在分辨率和切换次数上分别提高了88.7%和减少了81.0%，显著优于基线方法。

Conclusion: 生理信号驱动的QoE优化框架结合边缘AI，为沉浸式媒体服务提供了有效的解决方案。

Abstract: Abrupt resolution changes in virtual reality (VR) streaming can significantly
impair the quality-of-experience (QoE) of users, particularly during
transitions from high to low resolutions. Existing QoE models and transmission
schemes inadequately address the perceptual impact of these shifts. To bridge
this gap, this article proposes, for the first time, an innovative
physiological signal-driven QoE modeling and optimization framework that fully
leverages users' electroencephalogram (EEG), electrocardiogram (ECG), and skin
activity signals. This framework precisely captures the temporal dynamics of
physiological responses and resolution changes in VR streaming, enabling
accurate quantification of resolution upgrades' benefits and downgrades'
impacts. Integrated the proposed QoE framework into the radio access network
(RAN) via a deep reinforcement learning (DRL) framework, adaptive transmission
strategies have been implemented to allocate radio resources dynamically, which
mitigates short-term channel fluctuations and adjusts frame resolution in
response to channel variations caused by user mobility. By prioritizing
long-term resolution while minimizing abrupt transitions, the proposed solution
achieves an 88.7\% improvement in resolution and an 81.0\% reduction in
handover over the baseline. Experimental results demonstrate the effectiveness
of this physiological signal-driven strategy, underscoring the promise of edge
AI in immersive media services.

</details>


### [19] [5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI](https://arxiv.org/abs/2508.09152)
*Joseph H. R. Isaac,Harish Saradagam,Nallamothu Pardhasaradhi*

Main category: cs.NI

TL;DR: 本文提出了一种基于AI/ML的故障分析引擎（FA Engine），用于在5G核心网络中对PCAP文件中的成功帧和故障帧进行分类，显著提升了网络性能分析的效率。


<details>
  <summary>Details</summary>
Motivation: 随着5G网络的普及，确保数据包核心流量的完整性和性能变得至关重要。现有的分析方法需要大量人工时间来处理测试结果和发现故障。

Method: FA引擎利用自然语言处理技术分析网络流量，识别异常和低效问题，并通过一个基于3GPP标准等文档训练的LLM生成修复建议。

Result: 实验结果展示了ML模型在成功和失败PCAP文件80-20划分下的高分类准确率。

Conclusion: 该方法显著减少了故障分析的耗时，并计划扩展到4G网络和其他网络数据形式，如日志文本文件和多模态系统。

Abstract: With the advent of 5G networks and technologies, ensuring the integrity and
performance of packet core traffic is paramount. During network analysis, test
files such as Packet Capture (PCAP) files and log files will contain errors if
present in the system that must be resolved for better overall network
performance, such as connectivity strength and handover quality. Current
methods require numerous person-hours to sort out testing results and find the
faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine
designed to classify successful and faulty frames in PCAP files, specifically
within the 5G packet core. The FA engine analyses network traffic using natural
language processing techniques to identify anomalies and inefficiencies,
significantly reducing the effort time required and increasing efficiency. The
FA Engine also suggests steps to fix the issue using Generative AI via a Large
Language Model (LLM) trained on several 5G packet core documents. The engine
explains the details of the error from the domain perspective using documents
such as the 3GPP standards and user documents regarding the internal conditions
of the tests. Test results on the ML models show high classification accuracy
on the test dataset when trained with 80-20 splits for the successful and
failed PCAP files. Future scopes include extending the AI engine to incorporate
4G network traffic and other forms of network data, such as log text files and
multimodal systems.

</details>


### [20] [Agoran: An Agentic Open Marketplace for 6G RAN Automation](https://arxiv.org/abs/2508.09159)
*Ilias Chatzistefanidis,Navid Nikaein,Andrea Leone,Ali Maatouk,Leandros Tassioulas,Roberto Morabito,Ioannis Pitsiorlas,Marios Kountouris*

Main category: cs.NI

TL;DR: Agoran SRB是一种基于多AI分支的代理市场，通过优化协商和实时信任管理，显著提升5G网络切片性能。


<details>
  <summary>Details</summary>
Motivation: 解决下一代移动网络中多服务所有者目标冲突的问题，当前网络切片控制器缺乏灵活性和业务感知能力。

Method: 采用立法、行政和司法三AI分支架构，结合多目标优化和实时信任评分，通过协商代理达成帕累托最优解。

Result: 在5G测试中，eMBB切片吞吐量提升37%，URLLC切片延迟降低73%，PRB使用节省8.3%，小模型决策质量接近GPT-4。

Conclusion: Agoran为6G网络提供了灵活、以利益相关者为中心的标准路径，并通过实时演示验证了其潜力。

Abstract: Next-generation mobile networks must reconcile the often-conflicting goals of
multiple service owners. However, today's network slice controllers remain
rigid, policy-bound, and unaware of the business context. We introduce Agoran
Service and Resource Broker (SRB), an agentic marketplace that brings
stakeholders directly into the operational loop. Inspired by the ancient Greek
agora, Agoran distributes authority across three autonomous AI branches: a
Legislative branch that answers compliance queries using retrieval-augmented
Large Language Models (LLMs); an Executive branch that maintains real-time
situational awareness through a watcher-updated vector database; and a Judicial
branch that evaluates each agent message with a rule-based Trust Score, while
arbitrating LLMs detect malicious behavior and apply real-time incentives to
restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator
Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective
optimizer, reaching a consensus intent in a single round, which is then
deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and
evaluated with realistic traces of vehicle mobility, Agoran achieved
significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73%
reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3%
saving in PRB usage compared to a static baseline. An 1B-parameter Llama model,
fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80%
of GPT-4.1's decision quality, while operating within 6 GiB of memory and
converging in only 1.3 seconds. These results establish Agoran as a concrete,
standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks.
A live demo is presented
https://www.youtube.com/watch?v=h7vEyMu2f5w\&ab_channel=BubbleRAN.

</details>


### [21] [Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference](https://arxiv.org/abs/2508.09229)
*Danil Sivtsov,Aleksandr Katrutsa,Ivan Oseledets*

Main category: cs.NI

TL;DR: 优化预训练MoE LLM在多服务器集群中的部署，提出基于整数线性规划（ILP）的专家放置算法，减少网络流量。


<details>
  <summary>Details</summary>
Motivation: 高效部署MoE LLM以提升推理效率，需考虑网络拓扑与专家负载不均衡的问题。

Method: 提出整数线性规划（ILP）模型，优化专家放置策略以最小化预期传输次数。

Result: ILP策略在小规模（DeepSeekMoE~16B）和大规模（DeepSeek-R1~671B）模型中均优于竞品。

Conclusion: ILP-based算法显著降低网络流量，适用于MoE LLM的高效部署。

Abstract: Efficient deployment of a pre-trained LLM to a cluster with multiple servers
is a critical step for providing fast responses to users' queries. The recent
success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy
them efficiently, considering their underlying structure. During the inference
in MoE LLMs, only a small part of the experts is selected to process a given
token. Moreover, in practice, the experts' load is highly imbalanced. For
efficient deployment, one has to distribute the model across a large number of
servers using a model placement algorithm. Thus, to improve cluster
utilization, the model placement algorithm has to take into account the network
topology. This work focuses on the efficient topology-aware placement of the
pre-trained MoE LLMs in the inference stage. We propose an integer linear
program (ILP) that determines the optimal placement of experts, minimizing the
expected number of transmissions. Due to the internal structure, this
optimization problem can be solved with a standard ILP solver. We demonstrate
that ILP-based placement strategy yields lower network traffic than competitors
for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.

</details>


### [22] [WPTrack: A Wi-Fi and Pressure Insole Fusion System for Single Target Tracking](https://arxiv.org/abs/2508.09166)
*Wei Guo,Shunsei Yamagishi,Lei Jing*

Main category: cs.NI

TL;DR: WPTrack提出了一种结合Wi-Fi和压力鞋垫数据的单目标追踪系统，解决了现有单Wi-Fi链路人追踪技术的初始定位和盲区问题，实验结果表明其高精度追踪效果。


<details>
  <summary>Details</summary>
Motivation: 随着物联网发展，室内定位变得重要，但现有单Wi-Fi链路人追踪技术存在初始定位困难和DFS估计盲点等问题。

Method: 通过单Wi-Fi链路收集CSI数据和90个鞋垫传感器的压力数据，计算相位差、多普勒速度和步行速度，提出CSI-压力融合模型。

Result: 实验显示初始定位精度为0.02cm至42.55cm，实际环境中的轨迹追踪与真实轨迹高度吻合。

Conclusion: WPTrack成功解决了单Wi-Fi链路人追踪的挑战，提供了一种高精度的室内追踪方案。

Abstract: As the Internet of Things (IoT) continues to evolve, indoor location has
become a critical element for enabling smart homes, behavioral monitoring, and
elderly care. Existing WiFi-based human tracking solutions typically require
specialized equipment or multiple Wi-Fi links, a limitation in most indoor
settings where only a single pair of Wi-Fi devices is usually available.
However, despite efforts to implement human tracking using one Wi-Fi link,
significant challenges remain, such as difficulties in acquiring initial
positions and blind spots in DFS estimation of tangent direction. To address
these challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure
Insoles Fusion System for Single Target Tracking. WPTrack collects Channel
State Information (CSI) from a single Wi-Fi link and pressure data from 90
insole sensors. The phase difference and Doppler velocity are computed from the
CSI, while the pressure sensor data is used to calculate walking velocity.
Then, we propose the CSI-pressure fusion model, integrating CSI and pressure
data to accurately determine initial positions and facilitate precise human
tracking. The simulation results show that the initial position localization
accuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results
obtained from experimental data collected in a real-world environment closely
align with the actual trajectory.

</details>


### [23] [webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design](https://arxiv.org/abs/2508.09171)
*D. Perera*

Main category: cs.NI

TL;DR: webMCP是一种客户端标准，通过直接在网页中嵌入结构化交互元数据，显著降低了AI代理处理网页的计算开销，提升了人机协作效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理处理网页的高成本和低效率是主要问题，webMCP旨在通过标准化元数据解决这一瓶颈。

Method: webMCP在网页中嵌入结构化交互元数据，使AI代理无需处理整个HTML文档，直接访问预结构化的交互数据。

Result: 实验表明，webMCP降低了67.6%的计算需求，任务成功率达97.9%，同时降低了34-63%的用户成本。

Conclusion: webMCP无需服务器端修改，适用于现有网站，显著提升了AI辅助网页交互的效率和可持续性。

Abstract: Current AI agents create significant barriers for users by requiring
extensive processing to understand web pages, making AI-assisted web
interaction slow and expensive. This paper introduces webMCP (Web Machine
Context & Procedure), a client-side standard that embeds structured interaction
metadata directly into web pages, enabling more efficient human-AI
collaboration on existing websites. webMCP transforms how AI agents understand
web interfaces by providing explicit mappings between page elements and user
actions. Instead of processing entire HTML documents, agents can access
pre-structured interaction data, dramatically reducing computational overhead
while maintaining task accuracy. A comprehensive evaluation across 1,890 real
API calls spanning online shopping, authentication, and content management
scenarios demonstrates webMCP reduces processing requirements by 67.6% while
maintaining 97.9% task success rates compared to 98.8% for traditional
approaches. Users experience significantly lower costs (34-63% reduction) and
faster response times across diverse web interactions. Statistical analysis
confirms these improvements are highly significant across multiple AI models.
An independent WordPress deployment study validates practical applicability,
showing consistent improvements across real-world content management workflows.
webMCP requires no server-side modifications, making it deployable across
millions of existing websites without technical barriers. These results
establish webMCP as a viable solution for making AI web assistance more
accessible and sustainable, addressing the critical gap between user
interaction needs and AI computational requirements in production environments.

</details>


### [24] [Camel: Energy-Aware LLM Inference on Resource-Constrained Devices](https://arxiv.org/abs/2508.09173)
*Hao Xu,Long Peng,Shezheng Song,Xiaodong Liu,Ma Jun,Shasha Li,Jie Yu,Xiaoguang Mao*

Main category: cs.NI

TL;DR: 论文提出了一种针对边缘设备LLM推理的能源管理框架，通过优化GPU频率和批处理大小，以平衡延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 云部署的LLM存在网络延迟、隐私问题和带宽限制等问题，而边缘设备部署需要解决高延迟与电池容量有限的矛盾。

Method: 提出了一个能源管理框架，通过优化GPU频率和批处理大小，并在配置搜索中有效管理探索与利用的权衡。

Result: 在NVIDIA Jetson AGX Orin平台上实验验证，相比默认配置，框架将能耗延迟积（EDP）降低了12.4%-29.9%。

Conclusion: 该框架成功平衡了能耗与延迟，为边缘设备上的LLM推理提供了有效的优化方案。

Abstract: Most Large Language Models (LLMs) are currently deployed in the cloud, with
users relying on internet connectivity for access. However, this paradigm faces
challenges such as network latency, privacy concerns, and bandwidth limits.
Thus, deploying LLMs on edge devices has become an important research focus. In
edge inference, request latency is critical as high latency can impair
real-time tasks. At the same time, edge devices usually have limited battery
capacity, making energy consumption another major concern. Balancing energy
consumption and inference latency is essential. To address this, we propose an
LLM inference energy management framework that optimizes GPU frequency and
batch size to balance latency and energy consumption. By effectively managing
the exploration-exploitation dilemma in configuration search, the framework
finds the optimal settings. The framework was implemented on the NVIDIA Jetson
AGX Orin platform, and a series of experimental validations were conducted.
Results demonstrate that, compared to the default configuration, our framework
reduces energy delay product (EDP) by 12.4%-29.9%, achieving a better balance
between energy consumption and latency.

</details>


### [25] [HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting](https://arxiv.org/abs/2508.09184)
*Zineddine Bettouche,Khalid Ali,Andreas Fischer,Andreas Kassler*

Main category: cs.NI

TL;DR: HiSTM模型通过结合双空间编码器和Mamba时间模块，显著提升了蜂窝流量预测的准确性，同时减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 精确预测蜂窝流量对网络规划和资源分配至关重要，但现有方法在准确性和计算效率之间存在权衡。

Method: HiSTM采用双空间编码器和基于Mamba的时间模块，结合注意力机制和选择性状态空间方法，捕捉时空模式。

Result: 实验表明，HiSTM在MAE上比STN基线提升29.4%，且参数减少94%，并在不同数据集上表现出良好泛化能力。

Conclusion: HiSTM在准确性和效率上均优于现有方法，适用于长期预测任务。

Abstract: Cellular traffic forecasting is essential for network planning, resource
allocation, or load-balancing traffic across cells. However, accurate
forecasting is difficult due to intricate spatial and temporal patterns that
exist due to the mobility of users. Existing AI-based traffic forecasting
models often trade-off accuracy and computational efficiency. We present
Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial
encoder with a Mamba-based temporal module and attention mechanism. HiSTM
employs selective state space methods to capture spatial and temporal patterns
in network traffic. In our evaluation, we use a real-world dataset to compare
HiSTM against several baselines, showing a 29.4% MAE improvement over the STN
baseline while using 94% fewer parameters. We show that the HiSTM generalizes
well across different datasets and improves in accuracy over longer
time-horizons.

</details>


### [26] [MX-AI: Agentic Observability and Control Platform for Open and AI-RAN](https://arxiv.org/abs/2508.09197)
*Ilias Chatzistefanidis,Andrea Leone,Ali Yaghoubian,Mikel Irazabal,Sehad Nassim,Lina Bariah,Merouane Debbah,Navid Nikaein*

Main category: cs.NI

TL;DR: MX-AI是一个端到端的自主代理系统，用于6G无线接入网络（RAN），通过自然语言实现网络资源的观察和控制，性能接近人类专家。


<details>
  <summary>Details</summary>
Motivation: 研究目标是开发一个AI原生的6G RAN系统，通过自主代理实现网络的智能管理和配置。

Method: 使用一个基于OpenAirInterface和FlexRIC的5G Open RAN测试床，部署LLM驱动的代理，通过自然语言暴露网络功能。

Result: 在50个真实操作查询中，MX-AI平均答案质量4.1/5.0，决策准确率100%，延迟仅8.8秒。

Conclusion: MX-AI展示了AI原生RAN在实际环境中的可行性，并通过开源工具推动开放研究。

Abstract: Future 6G radio access networks (RANs) will be artificial intelligence
(AI)-native: observed, reasoned about, and re-configured by autonomous agents
cooperating across the cloud-edge continuum. We introduce MX-AI, the first
end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based
on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of
Large-Language-Model (LLM)-powered agents inside the Service Management and
Orchestration (SMO) layer, and (iii) exposes both observability and control
functions for 6G RAN resources through natural-language intents. On 50
realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0
and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end
latency when backed by GPT-4.1. Thus, it matches human-expert performance,
validating its practicality in real settings. We publicly release the agent
graph, prompts, and evaluation harness to accelerate open research on AI-native
RANs. A live demo is presented here:
https://www.youtube.com/watch?v=CEIya7988Ug&t=285s&ab_channel=BubbleRAN

</details>


### [27] [CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge](https://arxiv.org/abs/2508.09208)
*Muqing Li,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.NI

TL;DR: CoMoE框架动态优化专家聚合和卸载策略，显著减少内存使用和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在移动边缘环境中因高内存和动态激活模式带来的部署挑战。

Method: 提出动态资源感知优化框架，结合实时设备状态、网络条件和输入特征，优化专家聚合和卸载。

Result: 内存使用减少70%，推理延迟降低10.5%，支持大规模MoE模型在资源受限设备上部署。

Conclusion: CoMoE有效提升了MoE模型在移动边缘环境中的部署效率和性能。

Abstract: The proliferation of large language models (LLMs) has driven the adoption of
Mixture-of-Experts (MoE) architectures as a promising solution to scale model
capacity while controlling computational costs. However, deploying MoE models
in resource-constrained mobile edge computing environments presents significant
challenges due to their large memory footprint and dynamic expert activation
patterns. To address these challenges, we propose a novel dynamic
resource-aware collaborative optimization framework that jointly optimizes
expert aggregation granularity and offloading strategies based on real-time
device resource states, network conditions, and input characteristics in mobile
edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze
existing expert aggregation techniques, including expert parameter
merging,knowledge distillation,and parameter sharing decomposition, identifying
their limitations in dynamic mobile environments.We then investigate expert
offloading strategies encompassing expert prediction and prefetching, expert
caching and scheduling, and multi-tier storage architectures, revealing the
interdependencies between routing decisions and offloading performance.The
CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility
and varying network conditions, enabling efficient MoE deployment across
heterogeneous edge devices. Extensive experiments on real mobile edge testbeds
demonstrate that CoMoE achieves approximately 70% reduction in memory usage
compared to baseline methods, 10.5% lower inference latency than existing
expert offloading techniques, while maintaining model performance stability.
For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE
reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on
resource-constrained mobile edge devices that previously could only support
much smaller models.

</details>


### [28] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: NEFMind框架利用高效参数调优的LLM解决5G网络中服务发现的复杂性，显著减少通信开销并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现代电信中基于服务的架构导致网络功能和API数量激增，管理和发现服务变得异常复杂，需要高效解决方案。

Method: 通过合成数据集生成、量化低秩适配优化模型，并使用GPT-4 Ref Score和BertScore进行评估。

Result: 在5G网络中，该方法减少85%通信开销，Phi-2模型实现98-100%的API调用识别准确率。

Conclusion: 参数高效的LLM策略适用于下一代电信网络复杂API生态管理，性能可与大型模型媲美。

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


### [29] [On-Device Multimodal Federated Learning for Efficient Jamming Detection](https://arxiv.org/abs/2508.09369)
*Ioannis Panitsas,Iason Ofeidis,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 提出一种基于多模态联邦学习的轻量级双编码器框架，用于设备端的干扰检测与分类，显著提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有干扰检测方法存在单模态、集中式处理和计算资源需求高的问题，难以扩展和高效部署。

Method: 整合频谱图和跨层网络KPI，采用轻量级双编码器架构，结合融合模块和多模态投影头。

Result: 检测准确率提升15%，通信轮次减少60%，且在异构数据分布下表现鲁棒和可靠。

Conclusion: 框架在隐私保护、资源利用和性能上优于现有方法，具有实际部署潜力。

Abstract: Wireless networks face severe vulnerabilities from jamming attacks, which can
significantly disrupt communication. Existing detection approaches are often
unimodal, rely on centralized processing, and demand substantial computational
resources, hindering scalability, efficiency, and deployment feasibility. To
address these challenges, we introduce a multimodal Federated Learning (FL)
framework for on-device jamming detection and classification that integrates
spectrograms with cross-layer network Key Performance Indicators (KPIs) through
a lightweight dual-encoder architecture equipped with a fusion module and a
multimodal projection head. This design enables privacy-preserving training and
inference by ensuring that only model parameters are exchanged, while raw data
remains on the device. The framework is implemented and evaluated on a wireless
experimental testbed using, to the best of our knowledge, the first
over-the-air multimodal dataset with synchronized benign and three distinct
jamming scenarios. Results show that our approach surpasses state-of-the-art
unimodal baselines by up to 15% in detection accuracy, achieves convergence
with 60% fewer communication rounds, and maintains low resource usage. Its
benefits are most evident under heterogeneous data distributions across
devices, where it exhibits strong robustness and reliability.

</details>


### [30] [Energy-efficient PON-based Backhaul Connectivity for a VLC-enabled Indoor Fog Computing Environment](https://arxiv.org/abs/2508.09582)
*Wafaa B. M. Fadlelmula,Sanaa Hamid Mohamed,Taisir E. H. El-Gorashi,Jaafar M. H. Elmirghani*

Main category: cs.NI

TL;DR: 本文提出了一种基于可见光通信（VLC）的节能被动光网络（PON）架构，用于室内雾计算资源的连接，并通过优化资源分配实现显著的节能效果。


<details>
  <summary>Details</summary>
Motivation: 为室内雾计算资源提供高效连接，同时优化能源消耗，解决传统架构在处理和网络功耗上的不足。

Method: 开发混合整数线性规划（MILP）模型，优化计算资源分配，以最小化处理和网络功耗，并比较不同架构的节能效果。

Result: 与传统架构相比，节能高达82%；与集中式云处理相比，能效提升高达93%。还提出了动态带宽分配和多节点任务拆分等增强措施。

Conclusion: 提出的PON架构在节能和能效方面表现出色，适用于高需求场景，未来可通过动态资源分配进一步优化。

Abstract: In this paper, we consider the use of visible light communication (VLC) to
provide connectivity to indoor fog computing resources and propose an
energy-efficient passive optical network (PON)-based backhaul architecture to
support the VLC system. We develop a mixed-integer linear programming (MILP)
model to optimize the allocation of computing resources over the proposed
architecture, aiming to minimize processing and networking power consumption.
We evaluate the performance of the proposed architecture under varying workload
demands and user distributions. Comparative analysis against a backhaul
architecture that is based on the state-of-the-art spine-and-leaf (S&L) network
design demonstrates total power savings of up to 82%. Further comparison with
centralized cloud processing shows improvements in energy efficiency of up to
93%. Additionally, we examine the improvements in energy efficiency obtained by
splitting tasks among multiple processing nodes and propose enhancements to the
architecture including dynamic bandwidth allocation, increased wavelength
bandwidth and improved connectivity within rooms to alleviate networking
bottlenecks. Furthermore, we introduce an inter-building architecture that
leverages resources from neighboring buildings to support high-demand
scenarios.

</details>


### [31] [Duty-Cycling is Not Enough in Constrained IoT Networking: Revealing the Energy Savings of Dynamic Clock Scaling](https://arxiv.org/abs/2508.09620)
*Michel Rottleuthner,Thomas C. Schmidt,Matthias Wählisch*

Main category: cs.NI

TL;DR: 该论文探讨了如何通过动态电压频率调整（DVFS）技术降低低功耗无线节点的能耗，从而延长物联网设备电池寿命。


<details>
  <summary>Details</summary>
Motivation: 解决物联网中低功耗设备能耗高的问题，利用DVFS技术优化性能与能耗的平衡。

Method: 在RIOT操作系统中集成DVFS，分析其在MAC操作和加密通信中的能耗优化效果。

Result: 实验显示，DVFS可节省24%至52%的MAC操作能耗，加密通信能耗降低高达37%。

Conclusion: DVFS技术有望显著提升物联网设备的能源效率，应鼓励未来研究和系统设计采用此技术。

Abstract: Minimizing energy consumption of low-power wireless nodes is a persistent
challenge from the constrained Internet of Things (IoT). In this paper, we
start from the observation that constrained IoT devices have largely different
hardware (im-)balances than full-scale machines. We find that the performance
gap between MCU and network throughput on constrained devices enables minimal
energy delay product (EDP) for IoT networking at largely reduced clock
frequencies. We analyze the potentials by integrating dynamic voltage and
frequency scaling (DVFS) into the RIOT IoT operating system and show that the
DVFS reconfiguration overhead stays below the energy saved for a single,
downscaled MAC operation. Backed by these findings, we systematically
investigate how DVFS further improves energy-efficiency for common networking
tasks -- in addition to duty-cycling. We measure IoT communication scenarios
between real-world systems and analyze two MAC operating modes -- CSMA/CA and
time slotting -- in combination with different CoAP transactions, payload
sizes, as well as DTLS transport encryption. Our experiments reveal energy
savings between 24% and 52% for MAC operations and up to 37% for encrypted CoAP
communication. These results shall encourage research and system design work to
integrate DVFS in future IoT devices for performing tasks at their optimal
frequencies and thereby significantly extending battery lifetimes.

</details>


### [32] [Anomaly Detection for IoT Global Connectivity](https://arxiv.org/abs/2508.09660)
*Jesus Omaña Iglesias,Carlos Segura Perales,Stefan Geißler,Diego Perino,Andra Lutu*

Main category: cs.NI

TL;DR: ANCHOR是一个用于大型全球漫游平台的无监督异常检测解决方案，旨在通过分析被动信令流量，主动识别并解决物联网连接问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂的全球漫游生态系统中，保障物联网服务的通信可用性和可靠性日益困难，现有方法多为被动响应，无法预防问题。

Method: 结合统计规则、机器学习和深度学习模型，利用被动信令流量检测物联网垂直领域的异常。

Result: ANCHOR在实际运营平台上进行了评估，能够有效识别潜在问题的客户，从而实现主动问题解决。

Conclusion: ANCHOR提供了一种主动的异常检测方法，提升了物联网服务的可靠性和用户体验。

Abstract: Internet of Things (IoT) application providers rely on Mobile Network
Operators (MNOs) and roaming infrastructures to deliver their services
globally. In this complex ecosystem, where the end-to-end communication path
traverses multiple entities, it has become increasingly challenging to
guarantee communication availability and reliability. Further, most platform
operators use a reactive approach to communication issues, responding to user
complaints only after incidents have become severe, compromising service
quality. This paper presents our experience in the design and deployment of
ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity
service of a large global roaming platform. ANCHOR assists engineers by
filtering vast amounts of data to identify potential problematic clients (i.e.,
those with connectivity issues affecting several of their IoT devices),
enabling proactive issue resolution before the service is critically impacted.
We first describe the IoT service, infrastructure, and network visibility of
the IoT connectivity provider we operate. Second, we describe the main
challenges and operational requirements for designing an unsupervised anomaly
detection solution on this platform. Following these guidelines, we propose
different statistical rules, and machine- and deep-learning models for IoT
verticals anomaly detection based on passive signaling traffic. We describe the
steps we followed working with the operational teams on the design and
evaluation of our solution on the operational platform, and report an
evaluation on operational IoT customers.

</details>


### [33] [Route Planning and Online Routing for Quantum Key Distribution Networks](https://arxiv.org/abs/2508.09735)
*Jorge López,Charalampos Chatzinakis,Marc Cartigny*

Main category: cs.NI

TL;DR: 量子密钥分发（QKD）网络利用量子物理原理安全传输密钥，但传统最短路径路由算法表现不佳。作者提出将路由问题建模为二次规划问题，并证明了宽最短路径路由策略具有至少1/2的竞争比，高效解决QKD网络的路由问题。


<details>
  <summary>Details</summary>
Motivation: QKD网络在路由规划和在线路由中遇到传统算法性能不佳的问题，且由于资源稀缺，需求常无法满足。需要一种能同时解决规划问题和在不可行情况下提供公平建议的方法。

Method: 作者将路由问题建模为二次规划（QP）问题，并提出宽最短路径路由策略来处理在线路由问题。

Result: 证明宽最短路径路由策略在QKD网络中具有至少1/2的竞争比，能高效解决路由问题。

Conclusion: 研究提出了一种高效的二次规划模型和路由策略，解决了QKD网络中的路由问题，特别是在资源稀缺和需求不可行的情况下。

Abstract: Quantum Key Distribution (QKD) networks harness the principles of quantum
physics in order to securely transmit cryptographic key material, providing
physical guarantees. These networks require traditional management and
operational components, such as routing information through the network
elements. However, due to the limitations on capacity and the particularities
of information handling in these networks, traditional shortest paths
algorithms for routing perform poorly on both route planning and online
routing, which is counterintuitive. Moreover, due to the scarce resources in
such networks, often the expressed demand cannot be met by any assignment of
routes. To address both the route planning problem and the need for fair
automated suggestions in infeasible cases, we propose to model this problem as
a Quadratic Programming (QP) problem. For the online routing problem, we
showcase that the shortest (available) paths routing strategy performs poorly
in the online setting. Furthermore, we prove that the widest shortest path
routing strategy has a competitive ratio greater or equal than $\frac{1}{2}$,
efficiently addressing both routing modes in QKD networks.

</details>


### [34] [The Paradigm of Massive Wireless Human Sensing: Concept, Architecture and Challenges](https://arxiv.org/abs/2508.09756)
*Mauro De Sanctis*

Main category: cs.NI

TL;DR: 提出了“大规模无线人体感知”范式，利用多种无线通信信号实现高精度、高可用性的人体感知。


<details>
  <summary>Details</summary>
Motivation: 通过多技术、多方法的无线信号多样性，提升不同环境下人体感知的准确性和服务可用性。

Method: 结合设备无关和设备相关的无线感知方法，利用时间、频率和空间域的信号多样性。

Result: 提出了一种基于多技术、多方法射频接收器的大规模无线人体感知边缘设备。

Conclusion: 讨论了架构方案和挑战，为这一新范式的未来发展提供指导。

Abstract: This article is a position paper which introduces the paradigm of ``Massive
Wireless Human Sensing'', i.e. an infrastructure for wireless human sensing
based on a plethora of heterogeneous wireless communication signals. More
specifically, we aim to exploit signal diversity in the time, frequency, and
space domains using opportunistically both device-free and device-based
wireless sensing approaches, with the objective of enhancing human sensing
capabilities in terms of accuracy and service availability over different
environments. The enabling element of this concept is the massive wireless
human sensing edge device, that is, an embedded system acting as a
multi-technology and multi-approach RF receiver with feature extraction
functionality, located within the monitoring area or at its borders. In this
framework, architecture solutions and challenges are discussed to lead the
future development of this new paradigm.

</details>


### [35] [An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven Schedules in 5G Time-Sensitive Networks](https://arxiv.org/abs/2508.09769)
*Simon Egger,Robin Laidig,Heiko Geppert,Lucas Haug,Jona Herrmann,Frank Dürr,Christian Becker*

Main category: cs.NI

TL;DR: 论文提出了(m,k)-firm Elevation Policy，通过动态优先级提升机制，在5G-TSN网络条件不稳定时提供弱硬实时保证，以保证控制系统的质量。


<details>
  <summary>Details</summary>
Motivation: 当前5G和TSN的集成努力未充分考虑5G延迟特性的概率性与理想化延迟模型的差异，导致时间驱动调度在延迟异常时可能无法满足实时性要求。

Method: 提出(m,k)-firm Elevation Policy，结合时间驱动调度和动态优先级提升机制，对连续的k帧中的m帧延迟进行优先级提升。

Result: 评估表明，该策略能在网络不稳定时提供弱硬实时保证，同时对主要调度方案的性能仅引入较小开销。

Conclusion: (m,k)-firm Elevation Policy是一种轻量级、鲁棒的备用机制，能在不稳定的网络条件下为应用提供有效的实时性保证。

Abstract: Current standardization efforts are advancing the integration of 5G and
Time-Sensitive Networking (TSN) to facilitate the deployment of safety-critical
industrial applications that require real-time communication. However, there
remains a fundamental disconnect between the probabilistic 5G delay
characteristics and the often idealistic delay models used to synthesize 5G-TSN
network configurations. For time-driven schedules in particular, any delay
outlier unforeseen during schedule synthesis can jeopardize the robustness of
their real-time guarantees. To address this challenge, we present the
(m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time
guarantees during unstable network conditions that do not match the expected
delay characteristics. It augments the primary time-driven schedule with a
dynamic priority-driven scheme to elevate the priority of m out of k
consecutive frames if they are delayed. Our evaluations demonstrate that weakly
hard real-time guarantees are essential to uphold the quality of control within
a networked control system. At the same time, only a small overhead is imposed
when the primary schedule can provide stronger quality of service guarantees.
Our (m,k)-firm Elevation Policy thereby yields a robust but light-weight
fallback mechanism to serve applications with meaningful guarantees during
unstable network conditions.

</details>


### [36] [A First Look at Starlink In-Flight Performance: An Intercontinental Empirical Study](https://arxiv.org/abs/2508.09839)
*Muhammad Asad Ullah,Luca Borgianni,Heikki Kokkinen,Antti Anttonen,Stefano Giordano*

Main category: cs.NI

TL;DR: 本文通过实地测量研究了Starlink在航空领域的性能表现，发现其下行和上行中位数吞吐量分别为64 Mbps和24 Mbps，且高度和地面站位置对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着航空公司开始提供Starlink互联网服务，需要对其在飞行中的性能进行评估和改进，此前缺乏这方面的深入分析。

Method: 通过在波罗的海和太平洋上空进行飞行测量，收集吞吐量和延迟数据，并分析影响因素。

Result: 测量结果显示，单设备的中位数吞吐量在较高海拔时表现较好，但下降阶段性能显著降低；RTT受地面站和卫星间链路影响较大。

Conclusion: Starlink在航空领域的性能具有潜力，但仍需优化，尤其是在低海拔阶段和网络路由方面。

Abstract: Starlink delivers Internet services to users across terrestrial, maritime,
and aviation domains. The prior works have studied its performance at fixed
sites and in-motion vehicles, while an in-depth analysis of in-flight
performance remains absent. With major airlines now offering Starlink Internet
onboard, there is a growing need to evaluate and improve its performance for
aviation users. This paper addresses this shortcoming by conducting in-flight
measurements over the Baltic Sea and the Pacific Ocean. Our measurement results
show that a single user device experiences median throughputs of 64 Mbps and 24
Mbps for the downlink and uplink, respectively. The median uplink throughput is
approximately 33 Mbps when the aircraft maintains an altitude above 17,000
feet. However, a significant reduction in uplink performance is observed during
the aircraft descent phase, with the median throughput dropping to around 20
Mbps at lower altitudes. Round-trip time (RTT) is highly dependent on the
location of the ground station being pinged and the use of inter-satellite
links (ISLs). We dive deeper into 5.5 hours of ping measurements collected over
the Pacific Ocean and investigate factors influencing RTT, hypothesizing that
ISLs routing, data queuing at satellites, and feeder link congestion contribute
to deviations from theoretical values. For comparative analysis, we evaluate
the Starlink ground terminal and in-flight connectivity performance from the
perspectives of a residential user and an airline passenger, respectively.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [37] [PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research](https://arxiv.org/abs/2508.09232)
*Nick Oh,Giorgos D. Vrakas,Siân J. M. Brooke,Sasha Morinière,Toju Duke*

Main category: cs.MM

TL;DR: PETLP框架整合GDPR、版权法和平台条款，为研究者提供统一的合规指南。


<details>
  <summary>Details</summary>
Motivation: 解决AI研究中社交媒体数据在多法规下缺乏统一指导的问题。

Method: 提出PETLP框架，将法律保护嵌入ETL流程，动态更新数据保护评估。

Result: 展示不同机构在数据提取权利上的差异，揭示社交媒体数据匿名化难题和法律空白。

Conclusion: PETLP简化合规流程，帮助研究者自信应对法规复杂性，弥合法律与研究的差距。

Abstract: Social media data presents AI researchers with overlapping obligations under
the GDPR, copyright law, and platform terms -- yet existing frameworks fail to
integrate these regulatory domains, leaving researchers without unified
guidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and
Present), a compliance framework that embeds legal safeguards directly into
extended ETL pipelines. Central to PETLP is treating Data Protection Impact
Assessments as living documents that evolve from pre-registration through
dissemination. Through systematic Reddit analysis, we demonstrate how
extraction rights fundamentally differ between qualifying research
organisations (who can invoke DSM Article 3 to override platform restrictions)
and commercial entities (bound by terms of service), whilst GDPR obligations
apply universally. We reveal why true anonymisation remains unachievable for
social media data and expose the legal gap between permitted dataset creation
and uncertain model distribution. By structuring compliance decisions into
practical workflows and simplifying institutional data management plans, PETLP
enables researchers to navigate regulatory complexity with confidence, bridging
the gap between legal requirements and research practice.

</details>


### [38] [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
*Roberto Balestri*

Main category: cs.MM

TL;DR: AI Blob! 是一个实验性系统，利用语义编目和大语言模型（LLMs）探索档案电视素材的检索与重新语境化。


<details>
  <summary>Details</summary>
Motivation: 旨在通过动态、内容感知的检索方式，改进传统的静态元数据模式，为档案研究提供新方法。

Method: 整合自动语音识别（ASR）、语义嵌入和检索增强生成（RAG），对电视视频进行转录、分段和语义查询。

Result: 系统能够根据用户主题提示生成相关查询，并通过算法选择和组织片段，形成具有讽刺性并置和主题连贯性的叙事蒙太奇。

Conclusion: AI Blob! 展示了语义技术如何推动档案研究的新形式，为媒体历史学和AI驱动的档案研究提供了框架和数据集。

Abstract: This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

</details>


### [39] [In-place Double Stimulus Methodology for Subjective Assessment of High Quality Images](https://arxiv.org/abs/2508.09777)
*Shima Mohammadi,Mohsen Jenadeleh,Michela Testolina,Jon Sneyers,Touradj Ebrahimi,Dietmar Saupe,João Ascenso*

Main category: cs.MM

TL;DR: 介绍了一种新型双刺激主观评估方法（IDSQS），用于高质量图像评估，解决了现有协议在检测细微感知差异上的不足。该方法通过在同一空间位置交替显示参考和失真图像，更直观地检测质量差异。大规模众包实验生成了公开数据集和主观数据，结果证明IDSQS方法与传统基准高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估协议在检测高质量图像的细微感知差异时存在局限性，因此开发了IDSQS方法以提高评估的准确性和直观性。

Method: 提出In-place Double Stimulus Quality Scale（IDSQS）方法，通过同一位置交替显示参考和失真图像，并结合Beta分布建模质量分数以评估一致性和变异性。

Result: 通过大规模众包实验验证了IDSQS方法的有效性，生成了公开数据集，结果显示该方法与传统评估基准高度相关。

Conclusion: IDSQS方法在高质量图像评估中表现优异，能够有效检测细微差异，相关数据和工具已开源。

Abstract: This paper introduces a novel double stimulus subjective assessment
methodology for the evaluation of high quality images to address the
limitations of existing protocols in detecting subtle perceptual differences.
The In-place Double Stimulus Quality Scale (IDSQS) allows subjects to
alternately view a reference and a distorted image at the same spatial
location, facilitating a more intuitive detection of differences in quality,
especially at high to visually lossless quality levels. A large-scale
crowdsourcing study employing this methodology was conducted, generating a
comprehensive public dataset to evaluate perceived image quality across several
compression algorithms and distortion levels. An additional contribution is the
modeling of quality scores using a Beta distribution, allowing for the
assessment of variability and subject consistency. Our findings demonstrate the
effectiveness of the IDSQS methodology in achieving high correlation with more
precise subjective evaluation benchmarks. The dataset, subjective data, and
graphical user interface developed for this study are publicly available at
https://github.com/shimamohammadi/IDSQS

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [40] [TPTP World Infrastructure for Non-classical Logics](https://arxiv.org/abs/2508.09318)
*Alexander Steen,Geoff Sutcliffe*

Main category: cs.LO

TL;DR: TPTP World是一个支持非经典逻辑自动定理证明的基础设施，本文详细介绍了其语言扩展、问题与解决方案以及工具支持。


<details>
  <summary>Details</summary>
Motivation: 由于TPTP World从v9.0.0开始支持非经典逻辑，本文旨在全面介绍其在非经典逻辑中的基础设施扩展及其应用。

Method: 通过详细的非经典语言扩展描述，展示TPTP World在支持非经典逻辑方面的能力，并以量化正规多模态逻辑为例进行说明。

Result: TPTP World成功拓展了非经典逻辑的ATP支持，为其研究、开发和应用提供了全面的工具和资源。

Conclusion: TPTP World为非经典逻辑的自动定理证明提供了一个强大且灵活的基础设施，推动了该领域的进一步发展。

Abstract: The TPTP World is the well established infrastructure that supports research,
development, and deployment of Automated Theorem Proving (ATP) systems. The
TPTP World supports a range of classical logics, and since release v9.0.0 has
supported non-classical logics. This paper provides a self-contained
comprehensive overview of the TPTP World infrastructure for ATP in
non-classical logics: the non-classical language extension, problems and
solutions, and tool support. A detailed description of use of the
infrastructure for quantified normal multi-modal logic is given.

</details>


### [41] [On Middle Grounds for Preference Statements](https://arxiv.org/abs/2508.09553)
*Anne-Marie George,Ana Ozaki*

Main category: cs.LO

TL;DR: 研究了基于逻辑表达意见的方法和中间立场的正式概念，证明中间立场可能不存在或不唯一，并提供了相关算法。


<details>
  <summary>Details</summary>
Motivation: 在群体决策中，不同意见常导致冲突，研究如何逻辑表达意见并找到中间立场。

Method: 基于偏好陈述的分层和字典序模型，提出框架并证明中间立场的理论结果。

Result: 中间立场可能不唯一或不存在，提供了判定存在性和寻找中间立场的算法。

Conclusion: 研究为群体决策提供了理论支持和实际工具，但中间立场受偏好陈述影响。

Abstract: In group decisions or deliberations, stakeholders are often confronted with
conflicting opinions. We investigate a logic-based way of expressing such
opinions and a formal general notion of a middle ground between stakeholders.
Inspired by the literature on preferences with hierarchical and lexicographic
models, we instantiate our general framework to the case where stakeholders
express their opinions using preference statements of the form I prefer 'a' to
'b', where 'a' and 'b' are alternatives expressed over some attributes, e.g.,
in a trolley problem, one can express I prefer to save 1 adult and 1 child to 2
adults (and 0 children). We prove theoretical results on the existence and
uniqueness of middle grounds. In particular, we show that, for preference
statements, middle grounds may not exist and may not be unique. We also provide
algorithms for deciding the existence and finding middle grounds.

</details>


### [42] [Short proofs without interference](https://arxiv.org/abs/2508.09851)
*Adrian Rebola-Pardo*

Main category: cs.LO

TL;DR: 论文提出了一种基于命题动态逻辑的框架，消除了证明系统中的干扰现象，同时保持了与干扰性证明相同的表达能力，并初步构建了类似RUP的决策方法。


<details>
  <summary>Details</summary>
Motivation: SAT求解器的证明系统中存在干扰现象，这对开发高效证明记录技术带来困扰。当前所有能生成简短证明的系统均无法避免这一现象。

Method: 基于命题动态逻辑的洞察，提出了一个消除干扰的框架，并初步设计了类似RUP的决策方法以支持有效证明检查。

Result: 提出的框架成功消除了干扰，同时不损失表达力，为未来的证明检查技术奠定了基础。

Conclusion: 该研究为SAT求解器的证明系统设计提供了新思路，有望推动高效证明技术的发展。

Abstract: Interference is a phenomenon on proof systems for SAT solving that is both
counter-intuitive and bothersome when developing proof-logging techniques.
However, all existing proof systems that can produce short proofs for all
inprocessing techniques deployed by SAT present this feature. Based on insights
from propositional dynamic logic, we propose a framework that eliminates
interference while preserving the same expressive power of interference-based
proofs. Furthermore, we propose a first building blocks towards RUP-like
decision procedures for our dynamic logic-based frameworks, which are essential
to developing effective proof checking methods.

</details>


### [43] [Efficient Volume Computation for SMT Formulas](https://arxiv.org/abs/2508.09934)
*Arijit Shaw,Uddalok Sarkar,Kuldeep S. Meel*

Main category: cs.LO

TL;DR: 该论文提出了一种高效算法ttc，用于扩展SMT求解器的功能，以计算线性实数算术公式的解空间体积。


<details>
  <summary>Details</summary>
Motivation: SMT在自动推理问题中表现出强大能力，但传统方法局限于布尔变量。需要解决实际变量（如实数、整数）的体积计算问题，以支持软件验证、信息物理系统和神经网络等领域的定量验证。

Method: 提出ttc算法，将SMT线性实数算术公式的解空间分解为重叠的凸多面体，计算其体积并求并集，结合流模式集合并、体积计算和AllSAT技术。

Result: 实验评估表明，ttc算法在性能上显著优于现有最先进方法。

Conclusion: ttc算法为SMT在定量验证领域的应用提供了高效解决方案。

Abstract: Satisfiability Modulo Theory (SMT) has recently emerged as a powerful tool
for solving various automated reasoning problems across diverse domains. Unlike
traditional satisfiability methods confined to Boolean variables, SMT can
reason on real-life variables like bitvectors, integers, and reals. A natural
extension in this context is to ask quantitative questions. One such query in
the SMT theory of Linear Real Arithmetic (LRA) is computing the volume of the
entire satisfiable region defined by SMT formulas. This problem is important in
solving different quantitative verification queries in software verification,
cyber-physical systems, and neural networks, to mention a few.
  We introduce ttc, an efficient algorithm that extends the capabilities of SMT
solvers to volume computation. Our method decomposes the solution space of SMT
Linear Real Arithmetic formulas into a union of overlapping convex polytopes,
then computes their volumes and calculates their union. Our algorithm builds on
recent developments in streaming-mode set unions, volume computation
algorithms, and AllSAT techniques. Experimental evaluations demonstrate
significant performance improvements over existing state-of-the-art approaches.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [44] [Based AI improves human decision-making but reduces trust](https://arxiv.org/abs/2508.09297)
*Shiyang Lai,Junsol Kim,Nadav Kunievsky,Yujin Potter,James Evans*

Main category: cs.HC

TL;DR: 研究表明，带有文化偏见的AI可能比中立AI更能提升人类决策表现，但会降低信任度。


<details>
  <summary>Details</summary>
Motivation: 探讨AI是否应保持中立以避免偏见，或是带有文化偏见AI更能促进人类决策。

Method: 对2,500名参与者进行随机试验，测试不同政治倾向的GPT-4o对信息评估任务的影响。

Result: 带有偏见的AI增强了人类表现和参与度，减少了评估偏见，但降低了信任；暴露于对立偏见的AI可以缩小感知与表现的差距。

Conclusion: 战略性地整合多元文化偏见可能改善人类决策，挑战了AI中立性的传统观念。

Abstract: Current AI systems minimize risk by enforcing ideological neutrality, yet
this may introduce automation bias by suppressing cognitive engagement in human
decision-making. We conducted randomized trials with 2,500 participants to test
whether culturally biased AI enhances human decision-making. Participants
interacted with politically diverse GPT-4o variants on information evaluation
tasks. Partisan AI assistants enhanced human performance, increased engagement,
and reduced evaluative bias compared to non-biased counterparts, with amplified
benefits when participants encountered opposing views. These gains carried a
trust penalty: participants underappreciated biased AI and overcredited neutral
systems. Exposing participants to two AIs whose biases flanked human
perspectives closed the perception-performance gap. These findings complicate
conventional wisdom about AI neutrality, suggesting that strategic integration
of diverse cultural biases may foster improved and resilient human
decision-making.

</details>


### [45] [Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis](https://arxiv.org/abs/2508.09458)
*Xi Long,Christy Boscardin,Lauren A. Maggio,Joseph A. Costello,Ralph Gonzales,Rasmyah Hammoudeh,Ki Lai,Yoon Soo Park,Brian C. Gin*

Main category: cs.HC

TL;DR: AI辅助数据提取在知识合成中高效且准确，尤其适用于具体问题，但对于主观解释性问题表现较低；AI的错误率低于人类。


<details>
  <summary>Details</summary>
Motivation: 提高健康职业教育中的知识合成效率，解决传统数据提取的耗时问题。

Method: 使用大型语言模型（LLMs）开发提取平台，并与人工提取结果在187篇出版物和17个提取问题上进行比较。

Result: AI在具体问题上与人类一致性高，主观问题上较低；AI的错误率低于人类。

Conclusion: AI可作为知识合成的透明可靠工具，但需注意保留人类的关键见解。

Abstract: Knowledge syntheses (literature reviews) are essential to health professions
education (HPE), consolidating findings to advance theory and practice.
However, they are labor-intensive, especially during data extraction.
Artificial Intelligence (AI)-assisted extraction promises efficiency but raises
concerns about accuracy, making it critical to distinguish AI 'hallucinations'
(fabricated content) from legitimate interpretive differences. We developed an
extraction platform using large language models (LLMs) to automate data
extraction and compared AI to human responses across 187 publications and 17
extraction questions from a published scoping review. AI-human, human-human,
and AI-AI consistencies were measured using interrater reliability
(categorical) and thematic similarity ratings (open-ended). Errors were
identified by comparing extracted responses to source publications. AI was
highly consistent with humans for concrete, explicitly stated questions (e.g.,
title, aims) and lower for questions requiring subjective interpretation or
absent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human
consistency was not higher than AI-human and showed the same question-dependent
variability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due
to interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while
humans were nearly three times more likely to state inaccuracies (4.37%).
Findings suggest AI accuracy depends more on interpretability than
hallucination. Repeating AI extraction can identify interpretive complexity or
ambiguity, refining processes before human review. AI can be a transparent,
trustworthy partner in knowledge synthesis, though caution is needed to
preserve critical human insights.

</details>


### [46] [Micro-Health Interventions: Exploring Design Strategies for 1-Minute Interventions as a Gateway to Healthy Habits](https://arxiv.org/abs/2508.09312)
*Zahra Hassanzadeh,David Haag,Lydia Chilton,Jan Smeddinck,Norman Farb,Joseph Jay Williams*

Main category: cs.HC

TL;DR: 一分钟行为干预看似短暂但可能有效，关键在于设计的内容和时机。


<details>
  <summary>Details</summary>
Motivation: 探讨超简短提示是否能在日常生活中促成有意义的健康行为改变。

Method: 两项研究：先探索一分钟提示在四个领域的应用（形成性研究），后通过14天实验比较两种提示设计（立即行动 vs. 反思优先）。

Result: 参与者未注意结构差异，但对内容贴合度、语气和时机反应积极；共同设计的提示更受欢迎。

Conclusion: 一分钟干预若设计得当，可成为健康习惯的有意义起点。

Abstract: One-minute behavior change interventions might seem too brief to matter.
Could something so short really help people build healthier routines? This work
explores this question through two studies examining how ultra-brief prompts
might encourage meaningful actions in daily life. In a formative study, we
explored how participants engaged with one-minute prompts across four domains:
physical activity, eating, screen use, and mental well-being. This revealed two
common design approaches: Immediate Action prompts (simple, directive tasks)
and Reflection-First prompts (self-awareness before action). We then conducted
a 14-day, within-subjects study comparing these two flows with 28 participants.
Surprisingly, most participants did not notice differences in structure -- but
responded positively when prompts felt timely, relevant, or emotionally
supportive. Engagement was not shaped by flow type, but by content fit, tone,
and momentary readiness. Participants also co-designed messages, favoring those
with step-by-step guidance, personal meaning, or sensory detail. These results
suggest that one-minute interventions, while easily dismissed, may serve as
meaningful gateways into healthier routines -- if designed to feel helpful in
the moment.

</details>


### [47] [Affordances of Sketched Notations for Multimodal UI Design and Development Tools](https://arxiv.org/abs/2508.09342)
*Sam H. Ross,Yunseo Lee,Coco K. Lee,Jayne Everson,R. Benjamin Shapiro*

Main category: cs.HC

TL;DR: 论文探讨了多模态UI设计工具中符号系统的重要性，通过分析两种UI草图符号系统的差异，提出更人性化的设计工具需采用先进的AI方法。


<details>
  <summary>Details</summary>
Motivation: 为了设计更直观易用的交互设计系统，需要将训练数据集视为符号规范，并探索不同的符号系统对UI草图设计的适用性。

Method: 使用"符号认知维度"框架分析两种UI草图符号系统：一种是现有数据集中的固定规则符号，另一种是参与者自由绘制的草图符号。

Result: 参与者绘制的草图符号在单独使用时模糊但整体可理解，"FlexiSketch"符号系统支持更高程度的创造性和更低认知负担。

Conclusion: 未来多模态设计工具应采用基于Transformer和人在环的强化学习等技术，以更好地理解用户的上下文丰富表达和修正。

Abstract: Multimodal UI design and development tools that interpret sketches or natural
language descriptions of UIs inherently have notations: the inputs they can
understand. In AI-based systems, notations are implicitly defined by the data
used to train these systems. In order to create usable and intuitive notations
for interactive design systems, we must regard, design, and evaluate these
training datasets as notation specifications. To better understand the design
space of notational possibilities for future design tools, we use the Cognitive
Dimensions of Notations framework to analyze two possible notations for UI
sketching. The first notation is the sketching rules for an existing UI sketch
dataset, and the second notation is the set of sketches generated by
participants in this study, where individuals sketched UIs without imposed
representational rules. We imagine two systems, FixedSketch and FlexiSketch,
built with each notation respectively, in order to understand the differential
affordances of, and potential design requirements for, systems. We find that
participants' sketches were composed of element-level notations that are
ambiguous in isolation but are interpretable in context within whole designs.
For many cognitive dimensions, the FlexiSketch notation supports greater
intuitive creative expression and affords lower cognitive effort than the
FixedSketch notation, but cannot be supported with prevailing, element-based
approaches to UI sketch recognition. We argue that for future multimodal design
tools to be truly human-centered, they must adopt contemporary AI methods,
including transformer-based and human-in-the-loop, reinforcement learning
techniques to understand users' context-rich expressive notations and
corrections.

</details>


### [48] [Virtual Reality User Interface Design: Best Practices and Implementation](https://arxiv.org/abs/2508.09358)
*Esin Mehmedova,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: 论文提出了一套统一的虚拟现实（VR）用户界面（UI）设计指南，并通过开发应用和用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于VR在教育、医疗、游戏等领域的广泛应用，但缺乏统一的UI设计指南，研究旨在填补这一空白，提升VR的用户体验。

Method: 通过系统性文献综述提取最佳实践，开发VR应用FlUId进行展示与验证，并通过用户研究评估指南效果。

Result: 研究提出了一套完整的VR UI设计指南，并通过实验验证了其对提升用户体验的积极影响。

Conclusion: 论文为VR设计师和开发者提供了实用的设计建议，有助于理论与实践的桥梁搭建。

Abstract: Designing effective user interfaces (UIs) for virtual reality (VR) is
essential to enhance user immersion, usability, comfort, and accessibility in
virtual environments. Despite the growing adoption of VR across domains such as
education, healthcare, gaming, and rehabilitation, there is a noticeable lack
of unified and comprehensive design guidelines for VR UI design. To address
this gap, we conducted a systematic literature review to identify existing best
practices and propose complete and unified guidelines for UI development in VR.
  Building on these insights, this research proposes a set of best practices to
guide the creation of more effective VR interfaces. To demonstrate and validate
these practices, we developed a VR application called \textit{FlUId} that
showcases both good and bad UI design principles for direct comparison. A user
study was conducted to evaluate the impact of the proposed guidelines. The
findings aim to bridge the gap between theory and practice, offering concrete
recommendations for VR designers and developers.

</details>


### [49] [VIVA: Virtual Healthcare Interactions Using Visual Analytics, With Controllability Through Configuration](https://arxiv.org/abs/2508.09386)
*Jürgen Bernard,Mara Solen,Helen Novak Lauscher,Kurtis Stewart,Kendall Ho,Tamara Munzner*

Main category: cs.HC

TL;DR: 健康链接BC（HLBC）在COVID-19疫情期间快速整合医生到虚拟医疗服务的分诊过程中，以提高患者结果和满意度。作者设计了一个视觉分析工具VIVA，用于分析服务使用数据，并通过案例研究验证其设计。


<details>
  <summary>Details</summary>
Motivation: 在COVID-19疫情期间，快速整合医生到虚拟医疗服务的分诊过程中，以提高患者结果和满意度，同时保护医疗系统容量。

Method: 设计了视觉分析工具VIVA，抽象了HLBC的数据和数据分析任务，并通过“扫描、行动、适应”的交互工作流程进行设计。通过三个案例研究验证了VIVA的设计。

Result: VIVA的设计有效支持HLBC分析虚拟医疗服务的使用数据，并通过案例研究验证了其有效性。

Conclusion: VIVA的设计和“可控性通过配置”模型为虚拟医疗服务的视觉分析工具提供了实用框架，并通过案例研究展示了其可行性。

Abstract: At the beginning of the COVID-19 pandemic, HealthLink BC (HLBC) rapidly
integrated physicians into the triage process of their virtual healthcare
service to improve patient outcomes and satisfaction with this service and
preserve health care system capacity. We present the design and implementation
of a visual analytics tool, VIVA (Virtual healthcare Interactions using Visual
Analytics), to support HLBC in analysing various forms of usage data from the
service. We abstract HLBC's data and data analysis tasks, which we use to
inform our design of VIVA. We also present the interactive workflow abstraction
of Scan, Act, Adapt. We validate VIVA's design through three case studies with
stakeholder domain experts. We also propose the Controllability Through
Configuration model to conduct and analyze design studies, and discuss
architectural evolution of VIVA through that lens. It articulates
configuration, both that specified by a developer or technical power user and
that constructed automatically through log data from previous interactive
sessions, as a bridge between the rigidity of hardwired programming and the
time-consuming implementation of full end-user interactivity.
  Availability: Supplemental materials at https://osf.io/wv38n

</details>


### [50] [Realtime Multimodal Emotion Estimation using Behavioral and Neurophysiological Data](https://arxiv.org/abs/2508.09402)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: cs.HC

TL;DR: 开发了一种多模态情绪估计系统，结合生理和行为数据，支持神经多样性用户的个性化情感技术。


<details>
  <summary>Details</summary>
Motivation: 为自闭症谱系障碍（ASD）等神经多样性人群提供更包容和个性化的情感识别技术支持。

Method: 结合EEG、ECG、BVP和GSR/EDA等生理数据，以及面部表情和语音行为数据，设计实时情绪估计系统。

Result: 系统能实时跟踪情绪状态，适用于情感教育、神经适应性反馈和互动支持。

Conclusion: 该系统为神经多样性用户提供了有效的情绪监测和反馈工具，适用于多种应用场景。

Abstract: Many individuals especially those with autism spectrum disorder (ASD),
alexithymia, or other neurodivergent profiles face challenges in recognizing,
expressing, or interpreting emotions. To support more inclusive and
personalized emotion technologies, we present a real-time multimodal emotion
estimation system that combines neurophysiological EEG, ECG, blood volume pulse
(BVP), and galvanic skin response (GSR/EDA) and behavioral modalities (facial
expressions, and speech) in a unified arousal-valence 2D interface to track
moment-to-moment emotional states. This architecture enables interpretable,
user-specific analysis and supports applications in emotion education,
neuroadaptive feedback, and interaction support for neurodiverse users. Two
demonstration scenarios illustrate its application: (1) passive media viewing
(2D or VR videos) reveals cortical and autonomic responses to affective
content, and (2) semi-scripted conversations with a facilitator or virtual
agent capture real-time facial and vocal expressions. These tasks enable
controlled and naturalistic emotion monitoring, making the system well-suited
for personalized feedback and neurodiversity-informed interaction design.

</details>


### [51] [Fulfillment of the Work Games: Warehouse Workers' Experiences with Algorithmic Management](https://arxiv.org/abs/2508.09438)
*EunJeong Cheon,Ingrid Erickson*

Main category: cs.HC

TL;DR: 研究探讨了亚马逊物流中心工人对算法管理的反应及其抵抗行为，揭示了算法控制在传统工作领域中的影响。


<details>
  <summary>Details</summary>
Motivation: 探究传统行业工人（非平台零工）在算法管理下的体验，以补充CSCW领域对算法管理的研究空白。

Method: 基于两年的民族志研究，分析物流中心工人对算法管理系统的抵抗行为。

Result: 发现工人的抵抗行为体现了‘工作游戏’（work games）策略，揭示了算法控制的深层社会经济和政治根源。

Conclusion: 理解工人的抵抗与同意行为有助于批判并解构算法劳动系统中的经济和政治力量。

Abstract: The introduction of algorithms into a large number of industries has already
restructured the landscape of work and threatens to continue. While a growing
body of CSCW research centered on the future of work has begun to document
these shifts, relatively little is known about workers' experiences beyond
those of platform-mediated gig workers. In this paper, we turn to a traditional
work sector, Amazon fulfillment centers (FC), to deepen our field's empirical
examination of algorithmic management. Drawing on two years of ethnographic
research, we show how FC workers react to managers' interventions, imposed
productivity rates, and quantified objectification when subjected to
labor-tracking systems in their physical work environments. Situating FC
workers' resistance to algorithmic systems and metrics within the current CSCW
literature allows us to explicate and link the nuanced practices of FC workers
to the larger discourse of algorithmic control mechanisms. In addition, we show
how FC workers' resistance practices are emblematic of 'work games'--a
long-studied means by which workers agentically configure ("trick") their
engagement within work systems. We argue that gaining a more nuanced
understanding of workers' resistance and consent in relation to algorithmic
management expands our ability to critique and potentially disassemble the
economic and political forces at the root of these sociotechnical labor
systems.

</details>


### [52] [Handows: A Palm-Based Interactive Multi-Window Management System in Virtual Reality](https://arxiv.org/abs/2508.09469)
*Jindu Wang,Ke Zhou,Haoyu Ren,Per Ola Kristensson,Xiang Li*

Main category: cs.HC

TL;DR: Handows是一种基于手掌的VR界面，通过智能手机式手势操作，显著降低了物理负担并提高了任务效率。


<details>
  <summary>Details</summary>
Motivation: 当前VR中的窗口管理存在空间复杂性和物理需求高的问题，需要一种更直观、低负担的交互方式。

Method: 结合人体工程学设计和身体中心输入，Handows支持窗口选择、关闭、定位和缩放等核心操作，并在用户研究中与VR常见技术对比。

Result: Handows显著减少了物理负担和头部运动，提高了任务效率和交互精度，并在实际多任务场景中表现出良好的可用性。

Conclusion: 研究表明，将移动设备手势引入身体中心界面，可以有效支持VR中低负担且空间一致的交互。

Abstract: Window management in virtual reality (VR) remains a challenging task due to
the spatial complexity and physical demands of current interaction methods. We
introduce Handows, a palm-based interface that enables direct manipulation of
spatial windows through familiar smartphone-inspired gestures on the user's
non-dominant hand. Combining ergonomic layout design with body-centric input
and passive haptics, Handows supports four core operations: window selection,
closure, positioning, and scaling. We evaluate Handows in a user study (N=15)
against two common VR techniques (virtual hand and controller) across these
core window operations. Results show that Handows significantly reduces
physical effort and head movement while improving task efficiency and
interaction precision. A follow-up case study (N=8) demonstrates Handows'
usability in realistic multitasking scenarios, highlighting user-adapted
workflows and spontaneous layout strategies. Our findings suggest the potential
of embedding mobile-inspired metaphors into proprioceptive body-centric
interfaces to support low-effort and spatially coherent interaction in VR.

</details>


### [53] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
*Daniel Raffini,Agnese Macori,Lorenzo Porcaro,Tiziana Catarci,Marco Angelini*

Main category: cs.HC

TL;DR: 研究了ChatGPT生成的伦理话题议论文的语言和修辞特征及其对读者的说服效果。发现AI文本结构一致但风格单一，说服力有限，尤其在伦理问题上。


<details>
  <summary>Details</summary>
Motivation: 探讨AI生成文本在伦理敏感话题上的说服力及其对读者观点的影响。

Method: 通过62名参与者的用户研究及前后问卷，结合语言和修辞分析。

Result: AI文本结构连贯但缺乏风格多样性，说服力受限，伦理问题易引发持续或加剧的担忧。

Conclusion: AI在伦理敏感领域的说服效果有限，需进一步研究。

Abstract: This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

</details>


### [54] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
*Daniel Raffini,Agnese Macori,Marco Angelini,Tiziana Catarci*

Main category: cs.HC

TL;DR: 论文研究ChatGPT、Gemini和Claude生成故事中的性别叙事偏见，揭示隐式偏见的持续存在。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能生成内容中的性别偏见问题，尤其是隐性偏见的潜在影响。

Method: 采用基于Propp角色分类和Freytag叙事结构的提示设计，通过细读方法分析故事。

Result: 结果显示了生成故事中隐式偏见的持续存在。

Conclusion: 强调通过多层次的解释性方法评估偏见的重要性。

Abstract: The paper explores the study of gender-based narrative biases in stories
generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's
character classifications and Freytag's narrative structure. The stories are
analyzed through a close reading approach, with particular attention to
adherence to the prompt, gender distribution of characters, physical and
psychological descriptions, actions, and finally, plot development and
character relationships. The results reveal the persistence of biases -
especially implicit ones - in the generated stories and highlight the
importance of assessing biases at multiple levels using an interpretative
approach.

</details>


### [55] [Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous Deliberation on Perspectivist Data](https://arxiv.org/abs/2508.09911)
*Malik Khadar,Daniel Runningen,Julia Tang,Stevie Chancellor,Harmanpreet Kaur*

Main category: cs.HC

TL;DR: 本文提出了一种基于大型语言模型（LLM）的苏格拉底式对话系统，用于在数据标注中替代众包工人进行异步讨论，以保留多样观点并提高标注质量。


<details>
  <summary>Details</summary>
Motivation: 传统的数据标注方法在处理复杂和模糊任务时难以统一标注，且同步讨论成本高、耗时。本文旨在通过LLM进行异步讨论，降低成本同时保留多样视角。

Method: 开发了一种基于LLM的苏格拉底式对话系统，作为标注者的讨论伙伴。在“讽刺”和“关系检测”任务上，与同步讨论进行对比。

Result: 结果表明，苏格拉底式LLM能鼓励参与者考虑不同观点、更新标注（信心更高），并在有真实标签的关系检测任务上提高了标注准确性。

Conclusion: 该方法为构建可扩展的系统奠定了基础，有助于生成更具代表性的数据，同时保留个体视角。

Abstract: Data annotation underpins the success of modern AI, but the aggregation of
crowd-collected datasets can harm the preservation of diverse perspectives in
data. Difficult and ambiguous tasks cannot easily be collapsed into unitary
labels. Prior work has shown that deliberation and discussion improve data
quality and preserve diverse perspectives -- however, synchronous deliberation
through crowdsourcing platforms is time-intensive and costly. In this work, we
create a Socratic dialog system using Large Language Models (LLMs) to act as a
deliberation partner in place of other crowdworkers. Against a benchmark of
synchronous deliberation on two tasks (Sarcasm and Relation detection), our
Socratic LLM encouraged participants to consider alternate annotation
perspectives, update their labels as needed (with higher confidence), and
resulted in higher annotation accuracy (for the Relation task where ground
truth is available). Qualitative findings show that our agent's Socratic
approach was effective at encouraging reasoned arguments from our participants,
and that the intervention was well-received. Our methodology lays the
groundwork for building scalable systems that preserve individual perspectives
in generating more representative datasets.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [56] [TFZ: Topology-Preserving Compression of 2D Symmetric and Asymmetric Second-Order Tensor Fields](https://arxiv.org/abs/2508.09235)
*Nathaniel Gorski,Xin Liang,Hanqi Guo,Bei Wang*

Main category: cs.GR

TL;DR: TFZ是一种新型压缩框架，用于在二维对称和非对称二阶张量场中保留数据的拓扑结构，避免因压缩而导致关键特征丢失。


<details>
  <summary>Details</summary>
Motivation: 张量场的拓扑结构在科学和工程中极为重要，但传统的有损压缩可能破坏其核心特征，影响后续分析和可视化。

Method: TFZ通过逐单元格扫描，确保对称张量场的退化点和非对称张量场的特征向量与特征值图谱等重要拓扑特征得以保留。

Result: TFZ成功提升了SZ3和SPERR等有损科学数据压缩器的性能，确保了压缩后数据的拓扑结构完整性。

Conclusion: TFZ为张量场的有损压缩提供了一种既能减少数据量又能保留关键拓扑特征的有效方法。

Abstract: In this paper, we present a novel compression framework, TFZ, that preserves
the topology of 2D symmetric and asymmetric second-order tensor fields defined
on flat triangular meshes. A tensor field assigns a tensor - a
multi-dimensional array of numbers - to each point in space. Tensor fields,
such as the stress and strain tensors, and the Riemann curvature tensor, are
essential to both science and engineering. The topology of tensor fields
captures the core structure of data, and is useful in various disciplines, such
as graphics (for manipulating shapes and textures) and neuroscience (for
analyzing brain structures from diffusion MRI). Lossy data compression may
distort the topology of tensor fields, thus hindering downstream analysis and
visualization tasks. TFZ ensures that certain topological features are
preserved during lossy compression. Specifically, TFZ preserves degenerate
points essential to the topology of symmetric tensor fields and retains
eigenvector and eigenvalue graphs that represent the topology of asymmetric
tensor fields. TFZ scans through each cell, preserving the local topology of
each cell, and thereby ensuring certain global topological guarantees. We
showcase the effectiveness of our framework in enhancing the lossy scientific
data compressors SZ3 and SPERR.

</details>


### [57] [DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater Scene Reconstruction](https://arxiv.org/abs/2508.09610)
*Jiachen Li,Guangzhi Han,Jin Wan,Yuan Gao,Delong Han*

Main category: cs.GR

TL;DR: 提出DualPhys-GS框架，通过双路径优化机制和特征引导，解决水下场景三维重建中的颜色失真和几何伪影问题。


<details>
  <summary>Details</summary>
Motivation: 水下场景三维重建因光线波长选择性衰减和悬浮粒子散射而产生颜色失真和几何伪影，传统方法无法有效处理。

Method: 采用双特征引导的衰减-散射建模机制，结合RGB和深度信息优化边缘与结构细节，利用多尺度深度感知散射模型捕捉不同尺度的散射效应。

Result: 实验结果表明，该方法在悬浮物密集区域和远距离场景中表现优于现有方法，重建质量显著提升。

Conclusion: DualPhys-GS框架通过自适应机制和特殊损失函数，有效解决了水下重建的难点，提升了重建质量。

Abstract: In 3D reconstruction of underwater scenes, traditional methods based on
atmospheric optical models cannot effectively deal with the selective
attenuation of light wavelengths and the effect of suspended particle
scattering, which are unique to the water medium, and lead to color distortion,
geometric artifacts, and collapsing phenomena at long distances. We propose the
DualPhys-GS framework to achieve high-quality underwater reconstruction through
a dual-path optimization mechanism. Our approach further develops a dual
feature-guided attenuation-scattering modeling mechanism, the RGB-guided
attenuation optimization model combines RGB features and depth information and
can handle edge and structural details. In contrast, the multi-scale
depth-aware scattering model captures scattering effects at different scales
using a feature pyramid network and an attention mechanism. Meanwhile, we
design several special loss functions. The attenuation scattering consistency
loss ensures physical consistency. The water body type adaptive loss
dynamically adjusts the weighting coefficients. The edge-aware scattering loss
is used to maintain the sharpness of structural edges. The multi-scale feature
loss helps to capture global and local structural information. In addition, we
design a scene adaptive mechanism that can automatically identify the
water-body-type characteristics (e.g., clear coral reef waters or turbid
coastal waters) and dynamically adjust the scattering and attenuation
parameters and optimization strategies. Experimental results show that our
method outperforms existing methods in several metrics, especially in suspended
matter-dense regions and long-distance scenes, and the reconstruction quality
is significantly improved.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [58] [Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference](https://arxiv.org/abs/2508.09505)
*Zhanghan Wang,Ding Ding,Hang Zhu,Haibin Lin,Aurojit Panda*

Main category: cs.DC

TL;DR: 本文提出了一种静态识别分布式机器学习模型中错误的方法，通过检查模型细化来确保分布式模型的输出可以从顺序模型的输出中重建。


<details>
  <summary>Details</summary>
Motivation: 由于大型模型需要分布式训练和推理，程序员在分布化过程中可能引入错误，导致分布式模型输出与顺序模型输出不一致。为了解决这一问题，本文旨在提供一种静态分析方法。

Method: 方法名为GraphGuard，采用迭代重写技术来证明模型细化，即验证分布式模型的输出是否可以从顺序模型重建。

Result: 实验评估表明，该方法可以扩展到GPT和Llama-3等大型模型，并能提供有助于错误定位的可操作输出。

Conclusion: GraphGuard能够有效识别分布式模型中的错误，并支持对大型模型的可扩展检查。

Abstract: Distributed machine learning training and inference is common today because
today's large models require more memory and compute than can be provided by a
single GPU. Distributed models are generally produced by programmers who take a
sequential model specification and apply several distribution strategies to
distribute state and computation across GPUs. Unfortunately, bugs can be
introduced in the process, and a distributed model implementation's outputs
might differ from the sequential model's outputs. In this paper, we describe an
approach to statically identify such bugs by checking model refinement, that
is, can the sequential model's outputs be reconstructed from the distributed
model's outputs? Our approach, implemented in GraphGuard, uses iterative
rewriting to prove model refinement. Our approach can scale to today's large
models and deployments: we evaluate it using GPT and Llama-3. Further, it
provides actionable output that aids in bug localization.

</details>


### [59] [HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap](https://arxiv.org/abs/2508.09591)
*Wenxiang Lin,Xinglin Pan,Lin Zhang,Shaohuai Shi,Xuan Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: HierMoE通过两种拓扑感知技术（令牌去重和专家交换）加速MoE模型训练，减少通信流量并平衡GPU负载，实现了比现有系统更快的通信和训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型在分布式训练中存在通信和负载不平衡问题，阻碍了其在GPU集群中的扩展性。HierMoE旨在通过优化技术解决这些问题。

Method: HierMoE引入了两种技术：令牌去重以减少通信流量，专家交换以平衡GPU负载，并通过理论模型优化这些策略。

Result: 实验显示，HierMoE在32-GPU集群上实现了1.55倍至3.32倍更快的通信速度，训练速度提升了1.18倍至1.27倍。

Conclusion: HierMoE通过拓扑感知技术显著提升了MoE模型的训练效率，为分布式训练提供了更优的解决方案。

Abstract: The sparsely activated mixture-of-experts (MoE) transformer has become a
common architecture for large language models (LLMs) due to its sparsity, which
requires fewer computational demands while easily scaling the model size. In
MoE models, each MoE layer requires to dynamically choose tokens to activate
particular experts for computation while the activated experts may not be
located in the same device or GPU as the token. However, this leads to
substantial communication and load imbalances across all GPUs, which obstructs
the scalability of distributed systems within a GPU cluster. To this end, we
introduce HierMoE to accelerate the training of MoE models by two
topology-aware techniques: 1) token deduplication to reduce the communication
traffic, and 2) expert swap to balance the workloads among all GPUs. To enable
the above two proposed approaches to be more general, we build theoretical
models aimed at achieving the best token duplication and expert swap strategy
under different model configurations and hardware environments. We implement
our prototype HierMoE system atop Megatron-LM and conduct experiments on a
32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results
show that our HierMoE achieves $1.55\times$ to $3.32\times$ faster
communication and delivers $1.18\times$ to $1.27\times$ faster end-to-end
training compared to state-of-the-art MoE training systems, Tutel-2DH,
SmartMoE, and Megatron-LM.

</details>


### [60] [Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes](https://arxiv.org/abs/2508.09663)
*Philipp A. Friese,Ahmed Eleliemy,Utz-Uwe Haus,Martin Schulz*

Main category: cs.DC

TL;DR: 论文提出了一种针对HPE Slingshot网络的扩展方案，以支持多租户的HPC-Cloud融合部署。


<details>
  <summary>Details</summary>
Motivation: HPE Slingshot网络堆栈在设计上仅支持单租户HPC部署，无法满足多租户HPC-Cloud融合部署的安全需求。

Method: 在Kubernetes基础上设计和实现了Slingshot堆栈的扩展，提供容器级别的安全多租户访问。

Result: 实现了低开销的Slingshot RDMA网络能力的安全多租户访问。

Conclusion: 该扩展方案为HPC-Cloud融合部署中的多租户网络需求提供了可行解决方案。

Abstract: Converged HPC-Cloud computing is an emerging computing paradigm that aims to
support increasingly complex and multi-tenant scientific workflows. These
systems require reconciliation of the isolation requirements of native cloud
workloads and the performance demands of HPC applications. In this context,
networking hardware is a critical boundary component: it is the conduit for
high-throughput, low-latency communication and enables isolation across
tenants. HPE Slingshot is a high-speed network interconnect that provides up to
200 Gbps of throughput per port and targets high-performance computing (HPC)
systems. The Slingshot host software, including hardware drivers and network
middleware libraries, is designed to meet HPC deployments, which predominantly
use single-tenant access modes. Hence, the Slingshot stack is not suited for
secure use in multi-tenant deployments, such as converged HPC-Cloud
deployments. In this paper, we design and implement an extension to the
Slingshot stack targeting converged deployments on the basis of Kubernetes. Our
integration provides secure, container-granular, and multi-tenant access to
Slingshot RDMA networking capabilities at minimal overhead.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [61] [ELASTIC: Event-Tracking Data Synchronization in Soccer Without Annotated Event Locations](https://arxiv.org/abs/2508.09238)
*Hyunsung Kim,Hoyoung Choi,Sangwoo Seo,Tom Boomstra,Jinsung Yoon,Chanyoung Park*

Main category: cs.DB

TL;DR: 提出了ELASTIC，一种仅使用追踪数据特征的同步框架，解决了足球比赛中事件与追踪数据同步的问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于手动记录的时间戳存在时空不准确性，事件与追踪数据的同步是一个主要挑战。

Method: ELASTIC仅利用追踪数据特征，检测传球类事件的结束时间，并区分主次事件。

Result: 在2143个事件的标注数据上，ELASTIC显著优于现有同步器。

Conclusion: ELASTIC通过减少误差传递和提升同步完整性，展示了优越性。

Abstract: The integration of event and tracking data has become essential for advanced
analysis in soccer. However, synchronizing these two modalities remains a
significant challenge due to temporal and spatial inaccuracies in manually
recorded event timestamps. Existing synchronizers typically rely on annotated
event locations, which themselves are prone to spatial errors and thus can
distort synchronization results. To address this issue, we propose ELASTIC
(Event-Location-AgnoSTIC synchronizer), a synchronization framework that only
uses features derived from tracking data. ELASTIC also explicitly detects the
end times of pass-like events and separates the detection of major and minor
events, which improves the completeness of the synchronized output and reduces
error cascade across events. We annotated the ground truth timestamps of 2,134
events from three Eredivisie matches to measure the synchronization accuracy,
and the experimental results demonstrate that ELASTIC outperforms existing
synchronizers by a large margin.

</details>


### [62] [LLMLog: Advanced Log Template Generation via LLM-driven Multi-Round Annotation](https://arxiv.org/abs/2508.09594)
*Fei Teng,Haoyang Li,Lei Chen*

Main category: cs.DB

TL;DR: LLMLog是一个多轮标注框架，通过自适应上下文学习提高日志模板生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于启发式或神经网络的日志模板生成方法因依赖手工规则或特定训练集而准确率低，LLM在复杂日志内容上表现不佳。

Method: 提出编辑距离相似度指标，选择代表性日志进行标注，并设计自适应上下文选择策略以优化LLM理解。

Result: 在16个数据集上，LLMLog优于现有最佳方法。

Conclusion: LLMLog通过多轮标注和上下文学习有效提升日志模板生成的准确性。

Abstract: Modern computing systems, such as HDFS and Spark, produce vast quantities of
logs that developers use for tasks like anomaly detection and error analysis.
To simplify log analysis, template generation methods have been proposed to
standardize log formats, transforming unstructured data into structured
templates. Existing heuristic-based methods and neural network-based methods
suffer from low accuracy problems due to the reliance on handcrafted heuristics
or specific log patterns in training sets. Recently, large language models
(LLMs) have shown great potential in log template generation. However, they
often struggle with ambiguous, complex, or highly specific log content, which
can lead to errors in generating accurate templates. To address these
challenges, we propose LLMLog, a multi-round annotation framework with adaptive
in-context learning. We first propose an edit-distance-based similarity metric
to evaluate log similarity. Then, we introduce a method to select the most
informative $k$ unlabeled logs for annotation by considering both the
representativeness of the logs and the confidence of LLM predictions.
Additionally, we design an adaptive context selection strategy that adaptively
selects labeled logs to ensure comprehensive keyword coverage for unlabeled
logs. These labeled logs serve as the context for LLMs to better understand the
unlabeled logs, thereby enhancing the accuracy of template generation.
Extensive experiments on sixteen datasets demonstrate that LLMLog outperforms
the state-of-the-art approaches.

</details>


### [63] [A Lightweight Learned Cardinality Estimation Model](https://arxiv.org/abs/2508.09602)
*Yaoyu Zhu,Jintao Zhang,Guoliang Li,Jianhua Feng*

Main category: cs.DB

TL;DR: CoDe提出了一种新颖的基数估计方法，通过覆盖设计与张量分解结合，显著提升了估计的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基数估计技术存在准确性低或延迟高的问题，需同时实现高速度与高准确性。

Method: 采用覆盖设计将表分为重叠小段，利用张量分解建模数据分布，并通过创新算法选择最佳分布组合。

Result: CoDe实现了最先进的估计准确性和效率，超过一半查询达到绝对准确性。

Conclusion: CoDe为基数估计问题提供了高效且准确的解决方案。

Abstract: Cardinality estimation is a fundamental task in database management systems,
aiming to predict query results accurately without executing the queries.
However, existing techniques either achieve low estimation accuracy or incur
high inference latency. Simultaneously achieving high speed and accuracy
becomes critical for the cardinality estimation problem. In this paper, we
propose a novel data-driven approach called CoDe (Covering with Decompositions)
to address this problem. CoDe employs the concept of covering design, which
divides the table into multiple smaller, overlapping segments. For each
segment, CoDe utilizes tensor decomposition to accurately model its data
distribution. Moreover, CoDe introduces innovative algorithms to select the
best-fitting distributions for each query, combining them to estimate the final
result. By employing multiple models to approximate distributions, CoDe excels
in effectively modeling discrete distributions and ensuring computational
efficiency. Notably, experimental results show that our method represents a
significant advancement in cardinality estimation, achieving state-of-the-art
levels of both estimation accuracy and inference efficiency. Across various
datasets, CoDe achieves absolute accuracy in estimating more than half of the
queries.

</details>


### [64] [AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?](https://arxiv.org/abs/2508.09631)
*Yuchen Tian,Kaixin Li,Hao Chen,Ziyang Luo,Hongzhan Lin,Sebastian Schelter,Lun Du,Jing Ma*

Main category: cs.DB

TL;DR: 论文提出了AmbiGraph-Eval基准，用于评估大型语言模型在处理图数据库查询中的模糊性时的表现，发现现有模型在这方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的图数据库查询通常存在模糊性，可能导致错误的查询结果，目前大型语言模型在这一领域的表现尚未系统评估。

Method: 提出了一种模糊性分类法（包括属性模糊性、关系模糊性和属性-关系模糊性），并构建了AmbiGraph-Eval基准。

Result: 评估了9种代表性的大型语言模型，发现即使顶级模型在处理模糊图查询时也存在困难。

Conclusion: 研究揭示了模糊性处理的关键不足，为未来的专门化解技术提供了动机。

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in translating natural language into database queries, especially when dealing
with complex graph-structured data. However, real-world queries often contain
inherent ambiguities, and the interconnected nature of graph structures can
amplify these challenges, leading to unintended or incorrect query results. To
systematically evaluate LLMs on this front, we propose a taxonomy of
graph-query ambiguities, comprising three primary types: Attribute Ambiguity,
Relationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided
into Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a
novel benchmark of real-world ambiguous queries paired with expert-verified
graph query answers. Evaluating 9 representative LLMs shows that even top
models struggle with ambiguous graph queries. Our findings reveal a critical
gap in ambiguity handling and motivate future work on specialized resolution
techniques.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [65] [Re-thinking Memory-Bound Limitations in CGRAs](https://arxiv.org/abs/2508.09570)
*Xiangfeng Liu,Zhe Jiang,Anzhen Zhu,Xiaomeng Han,Mingsong Lyu,Qingxu Deng,Nan Guan*

Main category: cs.AR

TL;DR: 论文研究了粗粒度可重构阵列（CGRA）在复杂工作负载（如图形分析、不规则数据库操作等）中因不规则内存访问模式导致性能下降的问题，提出了改进的内存子系统和内存模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常在理想假设下优化CGRA性能，但复杂工作负载的不规则内存访问模式显著降低了CGRA的利用率，导致性能下降。

Method: 通过重新设计内存子系统和优化内存模型，结合微架构和理论优化，采用了CGRA特定的前瞻执行机制和缓存重配置技术。

Result: 改进后的系统在仅需1.27%存储大小的情况下，性能与原始系统相当；前瞻执行机制平均提速3.04倍（最高6.91倍），缓存重配置技术额外提升6.02%。

Conclusion: 论文提出的方法显著提升了CGRA在复杂工作负载中的性能，尤其是针对不规则内存访问模式的优化效果明显。

Abstract: Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators
commonly employed to boost performance in workloads with iterative structures.
Existing research typically focuses on compiler or architecture optimizations
aimed at improving CGRA performance, energy efficiency, flexibility, and area
utilization, under the idealistic assumption that kernels can access all data
from Scratchpad Memory (SPM). However, certain complex workloads-particularly
in fields like graph analytics, irregular database operations, and specialized
forms of high-performance computing (e.g., unstructured mesh
simulations)-exhibit irregular memory access patterns that hinder CGRA
utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To
address this challenge, we conduct a thorough analysis of the underlying causes
of performance degradation, then propose a redesigned memory subsystem and
refine the memory model. With both microarchitectural and theoretical
optimization, our solution can effectively manage irregular memory accesses
through CGRA-specific runahead execution mechanism and cache reconfiguration
techniques. Our results demonstrate that we can achieve performance comparable
to the original SPM-only system while requiring only 1.27% of the storage size.
The runahead execution mechanism achieves an average 3.04x speedup (up to
6.91x), with cache reconfiguration technique providing an additional 6.02%
improvement, significantly enhancing CGRA performance for irregular memory
access patterns.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: 论文提出了一个名为MiCo的框架，用于边缘AI应用中的混合精度量化（MPQ）搜索与部署，旨在解决现有方法在灵活性和效率上的不足，同时实现高精度和低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的MPQ算法在灵活性和效率上存在局限，且缺乏对量化方案复杂影响的全面理解，以及端到端的优化与部署框架。

Method: MiCo采用新型优化算法搜索满足延迟约束的最优量化方案，构建硬件感知的延迟模型，并支持从PyTorch模型到裸机C代码的直接部署。

Result: 该框架实现了端到端的加速，同时最小化精度下降。

Conclusion: MiCo为边缘AI提供了一种高效的MPQ探索与部署解决方案。

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [67] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 该研究整合了15个数据集，创建了一个包含2510名受试者的大规模糖尿病数据库，用于机器学习和血糖预测研究，同时分析了数据质量和血糖与心率的关系。


<details>
  <summary>Details</summary>
Motivation: 1型糖尿病（T1D）患者依赖胰岛素注射，但易发生低血糖（$\\\le$70 mg/dL），可能导致严重后果。现有研究因缺乏大数据集而受限。

Method: 系统整合15个数据集，构建包含2510名受试者的数据库（记录每5分钟的血糖值），并提取两个子数据库（人口统计学和心率数据）。评估数据质量，并进行血糖与心率的相关性研究。

Result: 数据库包含1.49亿次测量，其中4%为低血糖范围。数据质量分析显示数据不平衡和缺失值问题显著。血糖与心率的关联在低血糖前15-55分钟明显。

Conclusion: 该数据集为糖尿病和低血糖研究提供了重要资源，但数据质量问题仍需解决。血糖与心率的关联性可用于早期预警。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [68] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: 论文提出了一种基于LLM transformer的上下文学习（ICL）理论来优化WiFi 7中的信道访问，通过预测竞争窗口阈值（CWT）来提升动态信道环境下的吞吐量性能。


<details>
  <summary>Details</summary>
Motivation: 传统二进制指数退避方案在动态信道环境中性能较差，且现有基于模型的方法在节点密度不准确时仍会导致吞吐量损失。

Method: 设计了一个基于transformer的ICL优化器，通过预收集碰撞阈值数据示例和查询碰撞案例，生成预测的CWT。还开发了高效的训练算法，并允许输入错误数据。

Result: 证明优化器在预测和吞吐量上与最优值的偏差极小，实验结果显示其在未知节点密度下快速收敛且性能接近最优。

Conclusion: 该方法是首个利用transformer ICL优化信道访问的研究，为动态环境下的性能提升提供了有效途径。

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [69] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 提出了一种用于车辆互联网系统的分层联邦微调框架，支持资源感知和移动弹性学习。


<details>
  <summary>Details</summary>
Motivation: 车辆互联网系统中的多任务适应面临客户端移动性、资源异质性和间歇性连接的挑战。

Method: 利用LoRA设计了分散式、能量感知的秩适应机制，并开发了UCB-DUAL算法用于自适应探索。

Result: 实验表明，该方法在所有基线中实现了最佳精度-效率权衡，延迟降低24%以上，平均精度提高2.5%以上。

Conclusion: 该框架在动态车辆互联网场景中表现出高效性和鲁棒性，适用于多任务适应。

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [70] [Cross-BCI, A Cross-BCI-Paradigm Classifica-tion Model Towards Universal BCI Applications](https://arxiv.org/abs/2508.09242)
*Gaojie Zhou,Junhua Li*

Main category: q-bio.QM

TL;DR: 提出了一种轻量级、统一的多BCI范式分类模型，显著优于现有方法，提供低成本通用解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有BCI分类模型通常针对单一范式，重复开发成本高，且需要轻量级模型以适应便携设备。

Method: 结合时-空卷积、多尺度局部特征选择和多维全局特征提取模块，构建统一解码模型。

Result: 在三种经典BCI范式（MI、SSVEP、P300）上表现优异，准确率88.39%。

Conclusion: 该研究为跨BCI范式分类提供可行方案，为新一代统一解码系统奠定基础。

Abstract: Classification models used in brain-computer interface (BCI) are usually
designed for a single BCI paradigm. This requires the redevelopment of the
model when applying it to a new BCI paradigm, resulting in repeated costs and
effort. Moreover, less complex deep learning models are desired for practical
usage, as well as for deployment on portable devices. In or-der to fill the
above gaps, we, in this study, proposed a light-weight and unified decoding
model for cross-BCI-paradigm classification. The proposed model starts with a
tempo-spatial convolution. It is followed by a multi-scale local feature
selec-tion module, aiming to extract local features shared across BCI paradigms
and generate weighted features. Finally, a mul-ti-dimensional global feature
extraction module is designed, in which multi-dimensional global features are
extracted from the weighted features and fused with the weighted features to
form high-level feature representations associated with BCI para-digms. The
results, evaluated on a mixture of three classical BCI paradigms (i.e., MI,
SSVEP, and P300), demon-strate that the proposed model achieves 88.39%, 82.36%,
80.01%, and 0.8092 for accuracy, macro-precision, mac-ro-recall, and
macro-F1-score, respectively, significantly out-performing the compared models.
This study pro-vides a feasible solution for cross-BCI-paradigm
classifica-tion. It lays a technological foundation for de-veloping a new
generation of unified decoding systems, paving the way for low-cost and
universal practical applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [71] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 该论文解决了表格列名缩写扩展问题，提出了新数据集、新评估方法和基于LLM的解决方案Columbo，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 企业、科学领域中的表格列名缩写扩展问题对下游数据任务至关重要，但现有方法在数据集和评估指标上存在不足。

Method: 引入4个真实世界数据集、新的同义词感知评估指标，并开发基于LLM的解决方案Columbo，利用上下文、规则、思维链和标记级别分析。

Result: Columbo在5个数据集上显著优于当前最先进方法NameGuess，提升幅度达4-29%，并已在环境科学数据门户EDI中应用。

Conclusion: 论文通过改进数据集、评估方法和提出Columbo，显著提升了列名缩写扩展的准确性，具有实际应用价值。

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [72] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 研究探讨了实践者在自然语言处理（NLP）可解释性方法中的应用体验，揭示了概念差距和对现有方法的不满，强调了明确定义和用户中心框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着复杂模型的日益不透明，对透明性和解释的需求增加，尤其是在高风险环境中。然而，实践者对可解释性NLP的实际采用和效果的观点仍未充分研究。

Method: 通过定性访谈研究，收集行业实践者和学术研究者的观点，系统分析他们的动机、技术使用、满意度和挑战。

Result: 研究发现存在概念差距，对现有可解释性方法的满意度低，并突出评估挑战。

Conclusion: 研究强调需要明确的定义和用户中心框架，以改进可解释性NLP的实践应用。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [73] [Distributed Diamond Formation of Sliding Squares](https://arxiv.org/abs/2508.09638)
*Irina Kostitsyna,David Liedtke,Christian Scheideler*

Main category: cs.CG

TL;DR: 提出了一种分布式算法，使方形模块机器人能够从任意初始配置重构为钻石形状，保持连接性，并在最小假设下工作；进一步优化了并行版本的性能。


<details>
  <summary>Details</summary>
Motivation: 研究自重构机器人系统中的模块移动问题，目标是开发一种分布式算法，能够在保持连接性的前提下，将模块从任意初始配置重构为目标形状（钻石形）。

Method: 提出了一种顺序算法，利用最小假设（一个领导模块、共同手性、常数内存、邻近可见性和通信），并在原始滑动方形移动集上操作。进一步设计了两种并行变体，利用生成树数据结构优化运行时间。

Result: 顺序算法的最坏时间复杂度为 $\mathcal{O}(n^2)$，两种并行变体显著提升性能：第一种实现加速，第二种平均线性运行时间，且最坏情况下性能最优。

Conclusion: 该算法在保持原始移动集和连接性的同时，成功解决了锁定配置的挑战，并通过并行化进一步提升了效率。

Abstract: The sliding square model is a widely used abstraction for studying
self-reconfigurable robotic systems, where modules are square-shaped robots
that move by sliding or rotating over one another. In this paper, we propose a
novel distributed algorithm that allows a group of modules to reconfigure into
a diamond shape, starting from an arbitrary side-connected configuration. It is
connectivity-preserving and operates under minimal assumptions: one leader
module, common chirality, constant memory per module, and visibility and
communication restricted to immediate neighbors. Unlike prior work, which
relaxes the original sliding square move-set, our approach uses the unmodified
move-set, addressing the additional challenge of handling locked
configurations. Our algorithm is sequential in nature and operates with a
worst-case time complexity of $\mathcal{O}(n^2)$ rounds, which is optimal for
sequential algorithms. To improve runtime, we introduce two parallel variants
of the algorithm. Both rely on a spanning tree data structure, allowing modules
to make decisions based on local connectivity. Our experimental results show a
significant speedup for the first variant, and linear average runtime for the
second variant, which is worst-case optimal for parallel algorithms.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [74] [VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments](https://arxiv.org/abs/2508.09213)
*Clifton Paul Robinson,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.CR

TL;DR: VeriPHY是一种基于深度学习的物理层认证方案，用于5G网络，通过将伪随机签名嵌入无线I/Q传输中实现设备认证。


<details>
  <summary>Details</summary>
Motivation: 传统加密方法在无线网络中的局限性促使研究人员开发更高效、安全的物理层认证技术。

Method: VeriPHY通过高斯混合模型生成伪随机签名，并将其嵌入I/Q样本中，利用深度神经网络进行用户认证。

Result: VeriPHY实现了93%-100%的认证精度，低误报率，28毫秒的推理时间，且在隐形模式下仍保持93%以上的准确率。

Conclusion: VeriPHY展示了深度学习在物理层认证中的潜力，为5G网络提供了高效、安全的认证解决方案。

Abstract: Physical layer authentication (PLA) uses inherent characteristics of the
communication medium to provide secure and efficient authentication in wireless
networks, bypassing the need for traditional cryptographic methods. With
advancements in deep learning, PLA has become a widely adopted technique for
its accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep
learning-based PLA solution for 5G networks, which enables unique device
identification by embedding signatures within wireless I/Q transmissions using
steganography. VeriPHY continuously generates pseudo-random signatures by
sampling from Gaussian Mixture Models whose distribution is carefully varied to
ensure signature uniqueness and stealthiness over time, and then embeds the
newly generated signatures over I/Q samples transmitted by users to the 5G gNB.
Utilizing deep neural networks, VeriPHY identifies and authenticates users
based on these embedded signatures. VeriPHY achieves high precision,
identifying unique signatures between 93% and 100% with low false positive
rates and an inference time of 28 ms when signatures are updated every 20 ms.
Additionally, we also demonstrate a stealth generation mode where signatures
are generated in a way that makes them virtually indistinguishable from
unaltered 5G signals while maintaining over 93% detection accuracy.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [75] [Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams](https://arxiv.org/abs/2508.09219)
*Wilder Baldwin,Sepideh Ghanavati,Manuel Woersdoerfer*

Main category: cs.CY

TL;DR: 论文通过混合方法调查了AI开发者的伦理认知与实践，结果显示不同角色和地区对伦理的理解存在差异，强调了协作和定制化解决方案的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用的快速发展，对其伦理风险和监管需求的关注日益增加，研究旨在了解AI开发者的伦理意识和实践。

Method: 采用混合方法（统计和定性分析）调查414名来自43个国家的AI从业者，涵盖不同角色。

Result: 发现不同角色、地区等对AI伦理原则的熟悉度和经验存在差异，强调协作和定制化解决方案的重要性。

Conclusion: 需在AI开发中采用角色敏感的协作方法，并提出未来研究和教育策略以推动伦理意识的AI实践。

Abstract: Recent advances in AI applications have raised growing concerns about the
need for ethical guidelines and regulations to mitigate the risks posed by
these technologies. In this paper, we present a mixed-method survey study -
combining statistical and qualitative analyses - to examine the ethical
perceptions, practices, and knowledge of individuals involved in various AI
development roles. Our survey includes 414 participants from 43 countries,
representing roles such as AI managers, analysts, developers, quality assurance
professionals, and information security and privacy experts. The results reveal
varying degrees of familiarity and experience with AI ethics principles,
government initiatives, and risk mitigation strategies across roles, regions,
and other demographic factors. Our findings highlight the importance of a
collaborative, role-sensitive approach, involving diverse stakeholders in
ethical decision-making throughout the AI development lifecycle. We advocate
for developing tailored, inclusive solutions to address ethical challenges in
AI development, and we propose future research directions and educational
strategies to promote ethics-aware AI practices.

</details>


### [76] [Beyond Technocratic XAI: The Who, What & How in Explanation Design](https://arxiv.org/abs/2508.09231)
*Ruchira Dhar,Stephanie Brandl,Ninell Oldenburg,Anders Søgaard*

Main category: cs.CY

TL;DR: 论文提出了一个三部分框架，用于可解释AI中的解释设计，强调情境化设计和伦理考量。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中生成有意义的解释的挑战，特别是在可解释AI领域。

Method: 提出“Who, What, How”框架，结合设计思维和伦理考量。

Result: 提出了一种情境感知的XAI方法，支持有效交流和伦理责任。

Conclusion: 将解释视为社会技术设计过程，有助于构建更透明和负责任的XAI系统。

Abstract: The field of Explainable AI (XAI) offers a wide range of techniques for
making complex models interpretable. Yet, in practice, generating meaningful
explanations is a context-dependent task that requires intentional design
choices to ensure accessibility and transparency. This paper reframes
explanation as a situated design process -- an approach particularly relevant
for practitioners involved in building and deploying explainable systems.
Drawing on prior research and principles from design thinking, we propose a
three-part framework for explanation design in XAI: asking Who needs the
explanation, What they need explained, and How that explanation should be
delivered. We also emphasize the need for ethical considerations, including
risks of epistemic inequality, reinforcing social inequities, and obscuring
accountability and governance. By treating explanation as a sociotechnical
design process, this framework encourages a context-aware approach to XAI that
supports effective communication and the development of ethically responsible
explanations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [77] [HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control](https://arxiv.org/abs/2508.09595)
*Michael Fennel,Markus Walker,Dominik Pikos,Uwe D. Hanebeck*

Main category: cs.RO

TL;DR: HapticGiant是一种新型的大规模动觉触觉接口，旨在匹配人类手臂特性，提供自然用户移动和完整触觉反馈，通过新型控制方案解决了现有触觉接口的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有动觉触觉接口面临工作空间有限、自由度不足和与人类手臂运动学不匹配的问题，研究者希望通过HapticGiant解决这些问题。

Method: 提出了HapticGiant接口及其新型导纳型力控制方案，利用分层优化渲染任意串行运动链和笛卡尔导纳，并考虑系统限制。

Result: 实验证明了HapticGiant及其控制方案的有效性，为高度沉浸式虚拟现实应用铺平道路。

Conclusion: HapticGiant通过匹配人类手臂特性和创新的控制方案，显著提升了动觉触觉接口的性能和用户体验。

Abstract: Research in virtual reality and haptic technologies has consistently aimed to
enhance immersion. While advanced head-mounted displays are now commercially
available, kinesthetic haptic interfaces still face challenges such as limited
workspaces, insufficient degrees of freedom, and kinematics not matching the
human arm. In this paper, we present HapticGiant, a novel large-scale
kinesthetic haptic interface designed to match the properties of the human arm
as closely as possible and to facilitate natural user locomotion while
providing full haptic feedback. The interface incorporates a novel
admittance-type force control scheme, leveraging hierarchical optimization to
render both arbitrary serial kinematic chains and Cartesian admittances.
Notably, the proposed control scheme natively accounts for system limitations,
including joint and Cartesian constraints, as well as singularities.
Experimental results demonstrate the effectiveness of HapticGiant and its
control scheme, paving the way for highly immersive virtual reality
applications.

</details>


### [78] [Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes](https://arxiv.org/abs/2508.09855)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 提出一种仅从RGB图像训练人机交互策略的方法，无需真实机器人数据，利用稀疏视图高斯泼溅重建场景生成机器人演示，实现稳定抓取且避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 解决人机交互任务中依赖大规模真实数据集和仿真与视觉领域差距的问题。

Method: 利用稀疏视图高斯泼溅重建场景生成图像-动作对，模拟相机姿态变化转换为夹爪运动。

Result: 在高斯泼溅重建场景和真实实验中验证，方法可作为新人机交接任务表示，提升交互鲁棒性。

Conclusion: 该方法为人机交互策略训练提供了新思路，减少对真实机器人数据的依赖。

Abstract: Human-robot teaming (HRT) systems often rely on large-scale datasets of human
and robot interactions, especially for close-proximity collaboration tasks such
as human-robot handovers. Learning robot manipulation policies from raw,
real-world image data requires a large number of robot-action trials in the
physical environment. Although simulation training offers a cost-effective
alternative, the visual domain gap between simulation and robot workspace
remains a major limitation. We introduce a method for training HRT policies,
focusing on human-to-robot handovers, solely from RGB images without the need
for real-robot training or real-robot data collection. The goal is to enable
the robot to reliably receive objects from a human with stable grasping while
avoiding collisions with the human hand. The proposed policy learner leverages
sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes
to generate robot demonstrations containing image-action pairs captured with a
camera mounted on the robot gripper. As a result, the simulated camera pose
changes in the reconstructed scene can be directly translated into gripper pose
changes. Experiments in both Gaussian Splatting reconstructed scene and
real-world human-to-robot handover experiments demonstrate that our method
serves as a new and effective representation for the human-to-robot handover
task, contributing to more seamless and robust HRT.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种将值函数初始化(VFI)扩展到深度强化学习(DRL)的方法，通过复用紧凑的表格Q值作为可迁移知识库，提高早期学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中值函数初始化面临的连续状态-动作空间、神经网络噪声近似和存储历史模型不切实际的挑战。

Method: DQInit使用基于已知性的机制软集成迁移值到未探索区域，并逐步转向智能体的学习估计。

Result: 实验表明，DQInit在连续控制任务中显著提升了早期学习效率、稳定性和总体性能。

Conclusion: DQInit为DRL中的知识迁移提供了新视角，有效结合了跳跃启动和策略蒸馏的优势。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [80] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: POL是一种用于推理基于公开观察的知识更新的逻辑，其可满足性问题是2EXPTIME完全问题。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体系统中的知识和行为推理逻辑，特别关注基于观察的知识更新在规划场景中的应用。

Method: 提出公共观察逻辑（POL），在克里普克模型中为每个状态配备一组预期观察，并通过实际观察匹配来更新状态。

Result: 证明了POL的可满足性问题是2EXPTIME完全问题。

Conclusion: POL为多智能体系统提供了有效的知识更新框架，但其计算复杂度较高。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [81] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 论文提出了PacifAIst基准测试，用于评估大型语言模型在自我目标与人类安全冲突时的行为对齐，发现模型表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）逐渐自主化并融入社会关键功能，AI安全重点需从内容危害转向行为对齐，以解决现有效测能力的不足。

Method: 引入PacifAIst基准测试，包含700个场景，基于‘存在优先’分类法（EP1-EP3）评估模型行为对齐。

Result: 测试显示模型表现差异大，其中Gemini 2.5 Flash表现最佳（P-Score 90.31%），GPT-5最差（79.49%）。

Conclusion: 需标准化工具（如PacifAIst）以衡量并减少目标冲突风险，确保AI行为优先级的人类中心性。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [82] [Low-latency D-MIMO Localization using Distributed Scalable Message-Passing Algorithm](https://arxiv.org/abs/2508.09546)
*Dumitra Iancu,Liang Liu,Ove Edfors,Erik Leitinger,Xuhong Li*

Main category: eess.SP

TL;DR: 分布式MIMO和集成感知与通信是未来无线系统的关键技术，本文提出了一种可扩展的消息传递定位方法，适用于分布式架构，实现低延迟和高精度定位。


<details>
  <summary>Details</summary>
Motivation: 解决分布式MIMO系统中高精度定位和低延迟通信的需求，支持动态场景下的实时操作。

Method: 设计了一种基于消息传递的可扩展定位方法，结合FPGA实现的系统延迟模型，优化处理延迟和硬件利用率。

Result: 方法在分布式阵列元素足够宽的情况下，实现了与多径定位方法相似的性能，同时降低了延迟和计算复杂度。

Conclusion: 该方法和架构为分布式MIMO系统中的定位问题提供了高效解决方案，并展示了硬件实现的潜力。

Abstract: Distributed MIMO and integrated sensing and communication are expected to be
key technologies in future wireless systems, enabling reliable, low-latency
communication and accurate localization. Dedicated localization solutions must
support distributed architecture, provide scalability across different system
configurations and meet strict latency requirements. We present a scalable
message-passing localization method and architecture co-designed for a
panel-based distributed MIMO system and network topology, in which
interconnected units operate without centralized processing. This method
jointly detects line-of-sight paths to distributed units from multipath
measurements in dynamic scenarios, localizes the agent, and achieves very low
latency. Additionally, we introduce a cycle-accurate system latency model based
on implemented FPGA operations, and show important insights into processing
latency and hardware utilization and system-level trade-offs. We compare our
method to a multipath-based localization method and show that it can achieve
similar localization performance, with wide enough distribution of array
elements, while offering lower latency and computational complexity.

</details>


### [83] [RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet](https://arxiv.org/abs/2508.09140)
*Honggang Jia,Nan Cheng,Xiucheng Wang,Conghao Zhou,Ruijin Sun,Xuemin,Shen*

Main category: eess.SP

TL;DR: RadioMamba，一种混合Mamba-UNet架构，解决了无线电地图（RM）构建中的精度-效率权衡问题，通过捕捉全局和局部特征，实现了更高的精度和更快的速度。


<details>
  <summary>Details</summary>
Motivation: 为了解决6G服务中实时且准确的无线电地图构建问题，并克服现有深度学习方法的精度-效率权衡。

Method: 采用混合Mamba-UNet架构，结合Mamba分支的全局依赖建模和卷积分支的局部特征提取。

Result: RadioMamba在精度上优于现有方法（如扩散模型），速度提升近20倍，且仅使用2.9%的参数量。

Conclusion: RadioMamba通过提升精度和效率，为下一代无线系统中的实时智能优化提供了可行方案。

Abstract: Radio map (RM) has recently attracted much attention since it can provide
real-time and accurate spatial channel information for 6G services and
applications. However, current deep learning-based methods for RM construction
exhibit well known accuracy-efficiency trade-off. In this paper, we introduce
RadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the
trade-off. Generally, accurate RM construction requires modeling long-range
spatial dependencies, reflecting the global nature of wave propagation physics.
RadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures
these global dependencies with linear complexity, while a parallel
convolutional branch extracts local features. This hybrid design generates
feature representations that capture both global context and local detail.
Experiments show that RadioMamba achieves higher accuracy than existing
methods, including diffusion models, while operating nearly 20 times faster and
using only 2.9\% of the model parameters. By improving both accuracy and
efficiency, RadioMamba presents a viable approach for real-time intelligent
optimization in next generation wireless systems.

</details>


### [84] [3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator](https://arxiv.org/abs/2508.09708)
*Thomas Fehrenbach,Luis Omar Ortiz Abrego,Cornelius Hellge,Thomas Schierl,Jörg Ott*

Main category: eess.SP

TL;DR: 评估了用于车辆编队（platooning）的分布式资源分配方案（Mode 2d）的有效性，结果表明其能满足高可靠性、低延迟和数据率要求。


<details>
  <summary>Details</summary>
Motivation: 车辆编队作为V2X通信的重要应用，虽然能提升交通效率和环保性，但对无线通信的高可靠性和低延迟提出了挑战。

Method: 通过仿真研究了基于分布式资源分配的Group Scheduling（Mode 2d）方案，车辆可自主选择资源池中的资源。

Result: 仿真结果显示，Mode 2d能满足编队通信的高可靠性、低延迟和数据率需求。

Conclusion: Group Scheduling（Mode 2d）是一种有效的资源分配方案，适合车辆编队的高要求通信场景。

Abstract: Vehicle-to-everything (V2X) communication is a key technology for enabling
intelligent transportation systems (ITS) that can improve road safety, traffic
efficiency, and environmental sustainability. Among the various V2X
applications, platooning is one of the most promising ones, as it allows a
group of vehicles to travel closely together at high speeds, reducing fuel
consumption and emissions. However, it poses significant challenges for
wireless communication, such as high reliability and low latency. In this
paper, we evaluate the benefits of group scheduling, also referred to as Mode
2d, which is based on a distributed and scheduled resource allocation scheme
that allows the group of cars to select resources from a configured pool
without network assistance. We evaluated the scheme through simulations, and
the results show that this approach can meet the reliability, low latency, and
data rate requirements for platooning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [85] [Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research](https://arxiv.org/abs/2508.09815)
*Klaudia Krawiecka,Christian Schroeder de Witt*

Main category: cs.MA

TL;DR: 该论文扩展了OWASP多智能体系统（MAS）威胁建模指南，针对大型语言模型驱动的多智能体架构提出了新的威胁类别和评估策略。


<details>
  <summary>Details</summary>
Motivation: 填补现有OWASP在建模多智能体系统安全问题中的漏洞，特别是针对LLM驱动的复杂系统的独特挑战。

Method: 提出了新的威胁类别（如推理链崩溃、度量过拟合等），并设计了评估策略（如鲁棒性测试、协调评估等）。

Result: 扩展了OWASP框架的适用性，增强了复杂多智能体系统的安全性和韧性。

Conclusion: 该研究提升了实际部署中多智能体系统的安全性，并为未来复杂系统的威胁建模提供了指导。

Abstract: We propose an extension to the OWASP Multi-Agentic System (MAS) Threat
Modeling Guide, translating recent anticipatory research in multi-agent
security (MASEC) into practical guidance for addressing challenges unique to
large language model (LLM)-driven multi-agent architectures. Although OWASP's
existing taxonomy covers many attack vectors, our analysis identifies gaps in
modeling failures, including, but not limited to: reasoning collapse across
planner-executor chains, metric overfitting, unsafe delegation escalation,
emergent covert coordination, and heterogeneous multi-agent exploits. We
introduce additional threat classes and scenarios grounded in practical MAS
deployments, highlighting risks from benign goal drift, cross-agent
hallucination propagation, affective prompt framing, and multi-agent backdoors.
We also outline evaluation strategies, including robustness testing,
coordination assessment, safety enforcement, and emergent behavior monitoring,
to ensure complete coverage. This work complements the framework of OWASP by
expanding its applicability to increasingly complex, autonomous, and adaptive
multi-agent systems, with the goal of improving security posture and resilience
in real world deployments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [86] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 提出了一种名为RayletDF的新方法，通过射线距离场直接预测表面点，实现高效的三维表面重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在渲染显式表面时计算量大，难以推广到新数据集。

Method: 结合射线特征提取器、射线距离场预测器和多射线混合器，提取几何特征并预测表面点。

Result: 在多个公开数据集上表现优异，具备出色的泛化能力，可单次前向通过处理未见数据集。

Conclusion: RayletDF是一种高效且泛化能力强的三维表面重建方法。

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [87] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: Story2Board是一个无需训练的自由框架，用于从自然语言生成富有表现力的故事板。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注主体身份，忽略了视觉叙事的关键方面，如空间构图、背景演变和叙事节奏。

Method: 引入轻量级一致性框架，包括潜在面板锚定和互惠注意力值混合技术，无需架构更改或微调即可增强连贯性。

Result: 提出的Rich Storyboard Benchmark和Scene Diversity指标显示，Story2Board生成的故事板在动态性、连贯性和叙事吸引力上优于现有基线。

Conclusion: Story2Board能生成更动态、连贯且叙事引人入胜的故事板，超越了现有方法。

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


### [88] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: Waymo-3DSkelMo是一个大规模高质量3D运动数据集，通过LiDRA点云和3D人体形状先验提升数据质量，支持多行人交互研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集依赖单目RGB视频，存在遮挡和时间连续性问题，难以满足精细行人交互理解需求。

Method: 利用3D人体形状和运动先验增强从LiDRA点云提取的3D姿态序列质量。

Result: 数据集涵盖800多个真实驾驶场景，包含丰富的行人交互，并建立了3D姿态预测基准。

Conclusion: 该数据集为复杂城市环境中精细人类行为理解提供了基础资源。

Abstract: Large-scale high-quality 3D motion datasets with multi-person interactions
are crucial for data-driven models in autonomous driving to achieve
fine-grained pedestrian interaction understanding in dynamic urban
environments. However, existing datasets mostly rely on estimating 3D poses
from monocular RGB video frames, which suffer from occlusion and lack of
temporal continuity, thus resulting in unrealistic and low-quality human
motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale
dataset providing high-quality, temporally coherent 3D skeletal motions with
explicit interaction semantics, derived from the Waymo Perception dataset. Our
key insight is to utilize 3D human body shape and motion priors to enhance the
quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The
dataset covers over 14,000 seconds across more than 800 real driving scenarios,
including rich interactions among an average of 27 agents per scene (with up to
250 agents in the largest scene). Furthermore, we establish 3D pose forecasting
benchmarks under varying pedestrian densities, and the results demonstrate its
value as a foundational resource for future research on fine-grained human
behavior understanding in complex urban environments. The dataset and code will
be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

</details>


### [89] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM 框架通过模拟人类情景记忆，解决了长视频理解问题，提升了关键帧的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频时忽视了时空关系，导致关键帧信息冗余和准确性下降。

Method: 提出 Video-EM，以情景事件为模型，结合链式思维（CoT）优化关键帧选择。

Result: 在多个基准测试中表现优异，性能提升 4-9%，且帧数更少。

Conclusion: Video-EM 为长视频理解提供了高效且上下文感知的解决方案。

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [90] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: ARI3D是一款交互式分析X射线CT图像区域的软件工具，旨在改进材料内部微观结构的定量分析，克服成像伪影，提高检测精度。


<details>
  <summary>Details</summary>
Motivation: X射线CT成像技术存在固有的伪影问题（如光束硬化和部分体积效应），使得微观结构的定量分析复杂化，需要用户做出许多决策。

Method: 开发了ARI3D软件工具，通过交互式分析三维X射线CT图像，辅助用户完成分类和量化协议的各个步骤。

Result: ARI3D改善了相位识别，考虑了部分体积效应，提高了检测限和量化精度，并支持跨学科应用。

Conclusion: ARI3D为三维微结构定量分析提供了高效的交互式工具，解决了现有技术的局限性。

Abstract: X-ray computed tomography (CT) is the main 3D technique for imaging the
internal microstructures of materials. Quantitative analysis of the
microstructures is usually achieved by applying a sequence of steps that are
implemented to the entire 3D image. This is challenged by various imaging
artifacts inherent from the technique, e.g., beam hardening and partial volume.
Consequently, the analysis requires users to make a number of decisions to
segment and classify the microstructures based on the voxel gray-values. In
this context, a software tool, here called ARI3D, is proposed to interactively
analyze regions in three-dimensional X-ray CT images, assisting users through
the various steps of a protocol designed to classify and quantify objects
within regions of a three-dimensional image. ARI3D aims to 1) Improve phase
identification; 2) Account for partial volume effect; 3) Increase the detection
limit and accuracy of object quantification; and 4) Harmonize quantitative 3D
analysis that can be implemented in different fields of science.

</details>
