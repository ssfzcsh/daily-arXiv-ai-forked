{"id": "2508.20119", "pdf": "https://arxiv.org/pdf/2508.20119", "abs": "https://arxiv.org/abs/2508.20119", "authors": ["Daniel M. Yellin"], "title": "Evaluating LLMs on microservice-based applications: how complex is your specification?", "categories": ["cs.SE", "cs.LG", "68T42", "I.2.6; I.2.2; D.2.2"], "comment": "20 pages + 7 pages appendices. 7 Figures. 8 Tables", "summary": "In this paper we evaluate how far LLMs have advanced in generating code for\nreal-world problems. Specifically, we explore code synthesis for\nmicroservice-based applications, a widely used architecture pattern. We define\na standard template for specifying these applications, and we propose a metric\nfor judging the difficulty level of a specification. The higher the score, the\nmore difficult it is to generate code for the specification. We develop a\nframework to automate the process of testing LLM-synthesized code for a\nmicroservice using unit tests. Our experimental results show that strong LLMs\n(like GPT-3o-mini) do fairly well on medium difficulty specifications but do\nvery poorly on those of higher difficulty levels. This is due to more intricate\nbusiness logic, a greater use of external services, database integration and\ninclusion of non-functional capabilities such as authentication. We analyzed\nthe errors in LLM-synthesized code and report on the key challenges LLMs face\nin generating code for these specifications thereby suggesting future research\ndirections to improve code synthesis for real-world problems.", "AI": {"tldr": "\u8bc4\u4f30LLMs\u5728\u751f\u6210\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u4ee3\u7801\u65b9\u9762\u7684\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u5fae\u670d\u52a1\u67b6\u6784\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u96be\u5ea6\u5ea6\u91cf\u6807\u51c6\u5e76\u5f00\u53d1\u81ea\u52a8\u6d4b\u8bd5\u6846\u67b6\uff0c\u53d1\u73b0\u5f3aLLMs\u5728\u4e2d\u96be\u5ea6\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9ad8\u96be\u5ea6\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u751f\u6210\u5fae\u670d\u52a1\u5e94\u7528\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u7684\u4ee3\u7801\u5408\u6210\u63d0\u4f9b\u7814\u7a76\u57fa\u7840\u3002", "method": "\u5b9a\u4e49\u6807\u51c6\u6a21\u677f\u548c\u96be\u5ea6\u5ea6\u91cf\uff0c\u5f00\u53d1\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\u8bc4\u4f30LLM\u751f\u6210\u7684\u4ee3\u7801\u3002", "result": "\u5f3aLLMs\u5728\u4e2d\u96be\u5ea6\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6d89\u53ca\u590d\u6742\u4e1a\u52a1\u903b\u8f91\u3001\u5916\u90e8\u670d\u52a1\u548c\u9ad8\u96be\u5ea6\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "LLMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u4ecd\u6709\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u9ad8\u590d\u6742\u5ea6\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2508.20124", "pdf": "https://arxiv.org/pdf/2508.20124", "abs": "https://arxiv.org/abs/2508.20124", "authors": ["Yunlong Feng", "Yang Xu", "Xiao Xu", "Binyuan Hui", "Junyang Lin"], "title": "Towards Better Correctness and Efficiency in Code Generation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "While code large language models have demonstrated remarkable progress in\ncode generation, the generated code often exhibits poor runtime efficiency,\nlimiting its practical application in performance-sensitive scenarios. To\naddress this limitation, we propose an efficiency-oriented reinforcement\nlearning framework guided by a novel performance reward. Based on this\nframework, we take a deeper dive into the code efficiency problem, identifying\nthen proposing methods to overcome key bottlenecks: (1) Dynamic exploration\novercomes the static data constraints of offline fine-tuning, enabling the\ndiscovery of more efficient code implementations. (2) The error-insensitive\nreinforcement learning method and high-contrast efficiency signals are crucial\nfor mitigating systematic errors and achieving effective optimization. (3)\nOnline exploration is most effective when starting from a high-correctness\nbaseline, as this allows for efficiency improvements without sacrificing\naccuracy. With these discoveries, we finally propose a two-stage tuning method,\nwhich achieves high and balanced performance across correctness and efficiency.\nThe results of experiments show the effectiveness of the method, which improves\ncode correctness by 10.18\\% and runtime efficiency by 7.75\\% on a 7B model,\nachieving performance comparable to much larger model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u63a2\u7d22\u548c\u9ad8\u5bf9\u6bd4\u5ea6\u6548\u7387\u4fe1\u53f7\u4f18\u5316\u4ee3\u7801\u6548\u7387\uff0c\u5e76\u5728\u4e24\u9636\u6bb5\u8c03\u4f18\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u6b63\u786e\u6027\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u8fd0\u884c\u65f6\u6548\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u5176\u5728\u6027\u80fd\u654f\u611f\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u6548\u7387\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u62ec\u52a8\u6001\u63a2\u7d22\u3001\u9519\u8bef\u4e0d\u654f\u611f\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u548c\u9ad8\u5bf9\u6bd4\u5ea6\u6548\u7387\u4fe1\u53f7\u3002\u6700\u7ec8\u91c7\u7528\u4e24\u9636\u6bb5\u8c03\u4f18\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u57287B\u6a21\u578b\u4e0a\u5c06\u4ee3\u7801\u6b63\u786e\u6027\u63d0\u5347\u4e8610.18%\uff0c\u8fd0\u884c\u65f6\u6548\u7387\u63d0\u5347\u4e867.75%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801\u6548\u7387\u4e0e\u6b63\u786e\u6027\uff0c\u6027\u80fd\u63a5\u8fd1\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002"}}
{"id": "2508.20340", "pdf": "https://arxiv.org/pdf/2508.20340", "abs": "https://arxiv.org/abs/2508.20340", "authors": ["Maolin Sun", "Yibiao Yang", "Yuming Zhou"], "title": "Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems\nand programming languages research, providing the foundation for tasks like\nsymbolic execution and automated verification. Because these solvers sit on the\ncritical path, their correctness is essential, and high-quality test formulas\nare key to uncovering bugs. However, while prior testing techniques performed\nwell on earlier solver versions, they struggle to keep pace with rapidly\nevolving features. Recent approaches based on Large Language Models (LLMs) show\npromise in exploring advanced solver capabilities, but two obstacles remain:\nnearly half of the generated formulas are syntactically invalid, and iterative\ninteractions with the LLMs introduce substantial computational overhead. In\nthis study, we present Chimera, a novel LLM-assisted fuzzing framework that\naddresses both issues by shifting from direct formula generation to the\nsynthesis of reusable term (i.e., logical expression) generators. Particularly,\nChimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for\nSMT theories, including solver-specific extensions, from documentation, and (2)\nsynthesize composable Boolean term generators that adhere to these grammars.\nDuring fuzzing, Chimera populates structural skeletons derived from existing\nformulas with the terms iteratively produced by the LLM-synthesized generators.\nThis design ensures syntactic validity while promoting semantic diversity.\nNotably, Chimera requires only one-time LLM interaction investment,\ndramatically reducing runtime cost. We evaluated Chimera on two leading SMT\nsolvers: Z3 and cvc5. Our experiments show that Chimera has identified 43\nconfirmed bugs, 40 of which have already been fixed by developers.", "AI": {"tldr": "Chimera\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684SMT\u6c42\u89e3\u5668\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u516c\u5f0f\u7684\u6709\u6548\u6027\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u53d1\u73b0\u4e8643\u4e2a\u6f0f\u6d1e\u3002", "motivation": "\u7531\u4e8eSMT\u6c42\u89e3\u5668\u7684\u6b63\u786e\u6027\u5bf9\u7cfb\u7edf\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u800c\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u5176\u5feb\u901f\u53d1\u5c55\u7684\u7279\u6027\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6d4b\u8bd5\u5de5\u5177\u3002", "method": "Chimera\u901a\u8fc7LLM\u81ea\u52a8\u63d0\u53d6CFG\u5e76\u5408\u6210\u53ef\u7ec4\u5408\u7684\u5e03\u5c14\u9879\u751f\u6210\u5668\uff0c\u786e\u4fdd\u516c\u5f0f\u8bed\u6cd5\u6b63\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "result": "\u5728Z3\u548ccvc5\u6d4b\u8bd5\u4e2d\u53d1\u73b0\u4e8643\u4e2a\u786e\u8ba4\u6f0f\u6d1e\uff0c\u5176\u4e2d40\u4e2a\u5df2\u88ab\u4fee\u590d\u3002", "conclusion": "Chimera\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u6548\u7387\u548c\u6548\u679c\uff0c\u4e3aSMT\u6c42\u89e3\u5668\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.20370", "pdf": "https://arxiv.org/pdf/2508.20370", "abs": "https://arxiv.org/abs/2508.20370", "authors": ["Lingzhe Zhang", "Tong Jia", "Kangjin Wang", "Weijie Hong", "Chiming Duan", "Minghua He", "Ying Li"], "title": "Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "As contemporary microservice systems become increasingly popular and\ncomplex-often comprising hundreds or even thousands of fine-grained,\ninterdependent subsystems-they are facing more frequent failures. Ensuring\nsystem reliability thus demands accurate root cause localization. While traces\nand metrics have proven to be effective data sources for this task, existing\nmethods either heavily rely on pre-defined schemas, which struggle to adapt to\nevolving operational contexts, or lack interpretability in their reasoning\nprocess, thereby leaving Site Reliability Engineers (SREs) confused. In this\npaper, we conduct a comprehensive study on how SREs localize the root cause of\nfailures, drawing insights from multiple professional SREs across different\norganizations. Our investigation reveals that human root cause analysis\nexhibits three key characteristics: recursiveness, multi-dimensional expansion,\nand cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,\nan adaptive root cause localization method for microservice systems that\nleverages a multi-agent recursion-of-thought framework. RCLAgent employs a\nnovel recursion-of-thought strategy to guide the LLM's reasoning process,\neffectively integrating data from multiple agents and tool-assisted analysis to\naccurately pinpoint the root cause. Experimental evaluations on various public\ndatasets demonstrate that RCLAgent achieves superior performance by localizing\nthe root cause using only a single request-outperforming state-of-the-art\nmethods that depend on aggregating multiple requests. These results underscore\nthe effectiveness of RCLAgent in enhancing the efficiency and precision of root\ncause localization in complex microservice environments.", "AI": {"tldr": "RCLAgent \u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u4ee3\u7406\u9012\u5f52\u601d\u7ef4\u6846\u67b6\u7684\u9002\u5e94\u6027\u6839\u56e0\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u591a\u7ef4\u5ea6\u6570\u636e\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u6545\u969c\u5b9a\u4f4d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u9762\u5bf9\u590d\u6742\u4e14\u9891\u7e41\u6545\u969c\u7684\u5fae\u670d\u52a1\u7cfb\u7edf\uff0c\u73b0\u6709\u7684\u6839\u56e0\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u5f0f\u6216\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u800c\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u3002\u672c\u6587\u4ece\u4e13\u4e1a SRE \u7684\u7ecf\u9a8c\u51fa\u53d1\uff0c\u7814\u7a76\u4eba\u7c7b\u6839\u56e0\u5206\u6790\u7684\u4e09\u5927\u7279\u5f81\uff0c\u5e76\u4ee5\u6b64\u8bbe\u8ba1 RCLAgent\u3002", "method": "\u63d0\u51fa RCLAgent\uff0c\u91c7\u7528\u9012\u5f52\u601d\u7ef4\u7b56\u7565\u548c\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u4e0e\u5de5\u5177\u8f85\u52a9\u5206\u6790\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u6839\u56e0\u5b9a\u4f4d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRCLAgent \u4ec5\u9700\u5355\u4e00\u8bf7\u6c42\u5373\u53ef\u4f18\u4e8e\u4f9d\u8d56\u591a\u8bf7\u6c42\u805a\u5408\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "RCLAgent \u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u8fc7\u7a0b\u548c\u591a\u4ee3\u7406\u534f\u4f5c\uff0c\u4e3a\u590d\u6742\u5fae\u670d\u52a1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u6839\u56e0\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.21058", "pdf": "https://arxiv.org/pdf/2508.21058", "abs": "https://arxiv.org/abs/2508.21058", "authors": ["Shengqu Cai", "Ceyuan Yang", "Lvmin Zhang", "Yuwei Guo", "Junfei Xiao", "Ziyan Yang", "Yinghao Xu", "Zhenheng Yang", "Alan Yuille", "Leonidas Guibas", "Maneesh Agrawala", "Lu Jiang", "Gordon Wetzstein"], "title": "Mixture of Contexts for Long Video Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: https://primecai.github.io/moc/", "summary": "Long video generation is fundamentally a long context memory problem: models\nmust retain and retrieve salient events across a long range without collapsing\nor drifting. However, scaling diffusion transformers to generate long-context\nvideos is fundamentally limited by the quadratic cost of self-attention, which\nmakes memory and computation intractable and difficult to optimize for long\nsequences. We recast long-context video generation as an internal information\nretrieval task and propose a simple, learnable sparse attention routing module,\nMixture of Contexts (MoC), as an effective long-term memory retrieval engine.\nIn MoC, each query dynamically selects a few informative chunks plus mandatory\nanchors (caption, local windows) to attend to, with causal routing that\nprevents loop closures. As we scale the data and gradually sparsify the\nrouting, the model allocates compute to salient history, preserving identities,\nactions, and scenes over minutes of content. Efficiency follows as a byproduct\nof retrieval (near-linear scaling), which enables practical training and\nsynthesis, and the emergence of memory and consistency at the scale of minutes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMixture of Contexts (MoC)\u7684\u7a00\u758f\u6ce8\u610f\u529b\u8def\u7531\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u95ee\u9898\u3002", "motivation": "\u957f\u89c6\u9891\u751f\u6210\u9700\u8981\u6a21\u578b\u80fd\u591f\u5728\u957f\u8303\u56f4\u5185\u4fdd\u7559\u548c\u68c0\u7d22\u91cd\u8981\u4e8b\u4ef6\uff0c\u800c\u4f20\u7edf\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u56e0\u4e8c\u6b21\u6210\u672c\u9650\u5236\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u957f\u5e8f\u5217\u3002", "method": "\u901a\u8fc7\u5c06\u957f\u4e0a\u4e0b\u6587\u89c6\u9891\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5185\u90e8\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\uff0c\u5f15\u5165MoC\u6a21\u5757\uff0c\u52a8\u6001\u9009\u62e9\u5173\u952e\u4fe1\u606f\u5757\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u968f\u7740\u6570\u636e\u548c\u8def\u7531\u7a00\u758f\u5316\u7684\u6269\u5c55\uff0c\u6a21\u578b\u80fd\u591f\u9ad8\u6548\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u5206\u949f\u7ea7\u89c6\u9891\u7684\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u7684\u8fde\u8d2f\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "MoC\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u8def\u7531\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u7684\u6548\u7387\u548c\u8bb0\u5fc6\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u8bad\u7ec3\u548c\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2508.20365", "pdf": "https://arxiv.org/pdf/2508.20365", "abs": "https://arxiv.org/abs/2508.20365", "authors": ["Naoki Kobayashi", "Ryosuke Sato", "Ayumi Shinohara", "Ryo Yoshinaka"], "title": "Solvable Tuple Patterns and Their Applications to Program Verification", "categories": ["cs.PL"], "comment": null, "summary": "Despite the recent progress of automated program verification techniques,\nfully automated verification of programs manipulating recursive data structures\nremains a challenge. We introduce the notion of solvable tuple patterns (STPs)\nto express invariants between list-like recursive data structures. A\ndistinguishing feature of STPs is that they can be efficiently inferred from\nonly a small number of positive samples; no negative samples are required. An\nSMT solver that supports the sequence theory can be used to check that an\ninferred STP is indeed an inductive invariant. After presenting basic\nproperties of STPs and an STP inference algorithm, we show how to incorporate\nthe STP inference into a CHC (Constrained Horn Clauses) solver supporting\nlist-like data structures, which serves as a uniform backend for automated\nprogram verification tools. A CHC solver incorporating the STP inference has\nwon the ADT-LIN category of CHC-COMP 2025 by a big margin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u53ef\u89e3\u5143\u7ec4\u6a21\u5f0f\uff08STPs\uff09\u4ee5\u8868\u8fbe\u9012\u5f52\u6570\u636e\u7ed3\u6784\u7684\u4e0d\u53d8\u5f0f\uff0c\u65e0\u9700\u8d1f\u6837\u672c\u5373\u53ef\u9ad8\u6548\u63a8\u65ad\uff0c\u5e76\u901a\u8fc7SMT\u6c42\u89e3\u5668\u9a8c\u8bc1\u3002STP\u63a8\u65ad\u88ab\u96c6\u6210\u5230CHC\u6c42\u89e3\u5668\u4e2d\uff0c\u57282025\u5e74CHC-COMP\u6bd4\u8d5b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c3d\u7ba1\u7a0b\u5e8f\u9a8c\u8bc1\u6280\u672f\u8fd1\u5e74\u6765\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u9012\u5f52\u6570\u636e\u7ed3\u6784\u7684\u5168\u81ea\u52a8\u9a8c\u8bc1\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165STPs\u6982\u5ff5\uff0c\u4ec5\u9700\u5c11\u91cf\u6b63\u6837\u672c\u5373\u53ef\u63a8\u65ad\u4e0d\u53d8\u5f0f\uff0c\u5e76\u5229\u7528\u652f\u6301\u5e8f\u5217\u7406\u8bba\u7684SMT\u6c42\u89e3\u5668\u9a8c\u8bc1\u5176\u6b63\u786e\u6027\u3002\u5c06STP\u63a8\u65ad\u96c6\u6210\u5230\u652f\u6301\u5217\u8868\u7c7b\u6570\u636e\u7ed3\u6784\u7684CHC\u6c42\u89e3\u5668\u4e2d\u3002", "result": "STP\u63a8\u65ad\u7b97\u6cd5\u6210\u529f\u5b9e\u73b0\uff0c\u96c6\u6210STP\u7684CHC\u6c42\u89e3\u5668\u5728CHC-COMP 2025\u6bd4\u8d5b\u4e2d\u663e\u8457\u9886\u5148\u3002", "conclusion": "STPs\u4e3a\u9012\u5f52\u6570\u636e\u7ed3\u6784\u7684\u81ea\u52a8\u5316\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.20263", "pdf": "https://arxiv.org/pdf/2508.20263", "abs": "https://arxiv.org/abs/2508.20263", "authors": ["Jazbo Beason", "Ruijia Cheng", "Eldon Schoop", "Jeffrey Nichols"], "title": "Athena: Intermediate Representations for Iterative Scaffolded App Generation with an LLM", "categories": ["cs.HC"], "comment": null, "summary": "It is challenging to generate the code for a complete user interface using a\nLarge Language Model (LLM). User interfaces are complex and their\nimplementations often consist of multiple, inter-related files that together\nspecify the contents of each screen, the navigation flows between the screens,\nand the data model used throughout the application. It is challenging to craft\na single prompt for an LLM that contains enough detail to generate a complete\nuser interface, and even then the result is frequently a single large and\ndifficult to understand file that contains all of the generated screens. In\nthis paper, we introduce Athena, a prototype application generation environment\nthat demonstrates how the use of shared intermediate representations, including\nan app storyboard, data model, and GUI skeletons, can help a developer work\nwith an LLM in an iterative fashion to craft a complete user interface. These\nintermediate representations also scaffold the LLM's code generation process,\nproducing organized and structured code in multiple files while limiting\nerrors. We evaluated Athena with a user study that found 75% of participants\npreferred our prototype over a typical chatbot-style baseline for prototyping\napps.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAthena\uff0c\u4e00\u79cd\u901a\u8fc7\u5171\u4eab\u4e2d\u95f4\u8868\u793a\uff08\u5982\u5e94\u7528\u6545\u4e8b\u677f\u3001\u6570\u636e\u6a21\u578b\u548cGUI\u9aa8\u67b6\uff09\u8f85\u52a9LLM\u8fed\u4ee3\u751f\u6210\u5b8c\u6574\u7528\u6237\u754c\u9762\u7684\u539f\u578b\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLLM\u751f\u6210\u4ee3\u7801\u6742\u4e71\u4e14\u96be\u4ee5\u7406\u89e3\u7684\u95ee\u9898\u3002", "motivation": "\u7528\u6237\u754c\u9762\u590d\u6742\u4e14\u901a\u5e38\u7531\u591a\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u6587\u4ef6\u7ec4\u6210\uff0c\u4f20\u7edfLLM\u96be\u4ee5\u901a\u8fc7\u5355\u4e00\u63d0\u793a\u751f\u6210\u5b8c\u6574\u3001\u7ed3\u6784\u5316\u7684\u4ee3\u7801\uff0c\u5bfc\u81f4\u8f93\u51fa\u7ed3\u679c\u6df7\u4e71\u4e14\u96be\u4ee5\u7ef4\u62a4\u3002", "method": "\u5f15\u5165Athena\uff0c\u5229\u7528\u5171\u4eab\u4e2d\u95f4\u8868\u793a\uff08\u5982\u5e94\u7528\u6545\u4e8b\u677f\u3001\u6570\u636e\u6a21\u578b\u548cGUI\u9aa8\u67b6\uff09\u8f85\u52a9LLM\u8fed\u4ee3\u751f\u6210\u4ee3\u7801\uff0c\u63d0\u5347\u4ee3\u7801\u7ec4\u7ec7\u548c\u7ed3\u6784\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\uff0c75%\u7684\u53c2\u4e0e\u8005\u66f4\u503e\u5411\u4e8e\u4f7f\u7528Athena\u800c\u975e\u4f20\u7edf\u804a\u5929\u673a\u5668\u4eba\u98ce\u683c\u5de5\u5177\u8fdb\u884c\u5e94\u7528\u539f\u578b\u8bbe\u8ba1\u3002", "conclusion": "Athena\u901a\u8fc7\u4e2d\u95f4\u8868\u793a\u548c\u8fed\u4ee3\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210\u7528\u6237\u754c\u9762\u4ee3\u7801\u7684\u8d28\u91cf\u548c\u53ef\u7528\u6027\u3002"}}
{"id": "2508.20253", "pdf": "https://arxiv.org/pdf/2508.20253", "abs": "https://arxiv.org/abs/2508.20253", "authors": ["Ruihao Li", "Qinzhe Wu", "Krishna Kavi", "Gayatri Mehta", "Jonathan C. Beard", "Neeraja J. Yadwadkar", "Lizy K. John"], "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight Core for Memory Allocation", "categories": ["cs.DC", "cs.AR"], "comment": null, "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SpeedMalloc\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u652f\u6301\u6838\u5fc3\u5904\u7406\u591a\u7ebf\u7a0b\u5e94\u7528\u4e2d\u7684\u5185\u5b58\u5206\u914d\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5185\u5b58\u5206\u914d\u867d\u53ea\u5360\u4ee3\u7801\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u4f46\u5bf9\u6574\u4f53\u6027\u80fd\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u5c24\u5176\u5728\u591a\u7ebf\u7a0b\u591a\u6838\u7cfb\u7edf\u4e2d\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5982\u52a0\u901f\u5668\u652f\u6301\u6709\u9650\u4e14\u540c\u6b65\u95ee\u9898\u7a81\u51fa\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u652f\u6301\u6838\u5fc3\uff0c\u4e13\u95e8\u5904\u7406\u5185\u5b58\u5206\u914d\u4efb\u52a1\uff0c\u51cf\u5c11\u7f13\u5b58\u51b2\u7a81\u5e76\u6d88\u9664\u8de8\u6838\u5fc3\u5143\u6570\u636e\u540c\u6b65\u9700\u6c42\u3002", "result": "SpeedMalloc\u5728\u591a\u79cd\u591a\u7ebf\u7a0b\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8f6f\u4ef6\u548c\u786c\u4ef6\u5206\u914d\u5668\uff0c\u63d0\u5347\u5e45\u5ea6\u8fbe1.15x\u81f31.75x\u3002", "conclusion": "SpeedMalloc\u901a\u8fc7\u8f7b\u91cf\u7ea7\u652f\u6301\u6838\u5fc3\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u591a\u7ebf\u7a0b\u5e94\u7528\u4e2d\u5185\u5b58\u5206\u914d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e14\u5177\u5907\u66f4\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2508.20216", "pdf": "https://arxiv.org/pdf/2508.20216", "abs": "https://arxiv.org/abs/2508.20216", "authors": ["Diego Ferrer", "Jack Hutchins", "Revanth Koduru", "Sumeet Kumar Gupta", "Admedullah Aziz"], "title": "Reverse Designing Ferroelectric Capacitors with Machine Learning-based Compact Modeling", "categories": ["cs.ET"], "comment": null, "summary": "Machine learning-based compact models provide a rapid and efficient approach\nfor estimating device behavior across multiple input parameter variations. In\nthis study, we introduce two reverse-design algorithms that utilize these\ncompact models to identify device parameters corresponding to desired\nelectrical characteristics. The algorithms effectively determine parameter\nsets, such as layer thicknesses, required to achieve specific device\nperformance criteria. Significantly, the proposed methods are uniquely enabled\nby machine learning-based compact modeling; alternative computationally\nintensive approaches, such as phase-field modeling, would impose impractical\ntime constraints for iterative design processes. Our comparative analysis\ndemonstrates a substantial reduction in computation time when employing machine\nlearning-based compact models compared to traditional phase-field methods,\nunderscoring a clear and substantial efficiency advantage. Additionally, the\naccuracy and computational efficiency of both reverse-design algorithms are\nevaluated and compared, highlighting the practical advantages of machine\nlearning-based compact modeling approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7d27\u51d1\u6a21\u578b\u53cd\u5411\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u786e\u5b9a\u5b9e\u73b0\u7279\u5b9a\u7535\u6c14\u7279\u6027\u7684\u8bbe\u5907\u53c2\u6570\uff0c\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u76f8\u573a\u5efa\u6a21\uff09\u8017\u65f6\u4e14\u4e0d\u9002\u7528\u4e8e\u8fed\u4ee3\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u9700\u5bfb\u627e\u66f4\u5feb\u901f\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u6784\u5efa\u7d27\u51d1\u6a21\u578b\uff0c\u5f00\u53d1\u4e24\u79cd\u53cd\u5411\u8bbe\u8ba1\u7b97\u6cd5\u4ee5\u5339\u914d\u8bbe\u5907\u53c2\u6570\u4e0e\u6027\u80fd\u9700\u6c42\u3002", "result": "\u4e0e\u76f8\u573a\u5efa\u6a21\u76f8\u6bd4\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\uff0c\u4e14\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u7d27\u51d1\u6a21\u578b\u4e3a\u8bbe\u5907\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u5408\u8fed\u4ee3\u4f18\u5316\u573a\u666f\u3002"}}
{"id": "2508.20686", "pdf": "https://arxiv.org/pdf/2508.20686", "abs": "https://arxiv.org/abs/2508.20686", "authors": ["Herbert Jordan", "Kamil Jezek", "Pavle Subotic", "Bernhard Scholz"], "title": "Efficient Forkless Blockchain Databases", "categories": ["cs.DB"], "comment": null, "summary": "Operating nodes in an L1 blockchain remains costly despite recent advances in\nblockchain technology. One of the most resource-intensive components of a node\nis the blockchain database, also known as StateDB, that manages balances,\nnonce, code, and the persistent storage of accounts/smart contracts. Although\nthe blockchain industry has transitioned from forking to forkless chains due to\nimproved consensus protocols, forkless blockchains still rely on legacy forking\ndatabases that are suboptimal for their purposes. In this paper, we propose a\nforkless blockchain database, showing a 100x improvement in storage and a 10x\nimprovement in throughput compared to the geth-based Fantom Blockchain client.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdforkless\u533a\u5757\u94fe\u6570\u636e\u5e93\uff0c\u76f8\u6bd4geth-based Fantom\u533a\u5757\u94fe\u5ba2\u6237\u7aef\uff0c\u5b58\u50a8\u6548\u7387\u63d0\u5347100\u500d\uff0c\u541e\u5410\u91cf\u63d0\u534710\u500d\u3002", "motivation": "\u5c3d\u7ba1\u533a\u5757\u94fe\u6280\u672f\u6709\u6240\u8fdb\u6b65\uff0c\u4f46L1\u533a\u5757\u94fe\u8282\u70b9\u7684\u8fd0\u884c\u6210\u672c\u4ecd\u7136\u5f88\u9ad8\uff0c\u5c24\u5176\u662f\u7ba1\u7406\u8d26\u6237/\u667a\u80fd\u5408\u7ea6\u72b6\u6001\u7684\u533a\u5757\u94fe\u6570\u636e\u5e93\uff08StateDB\uff09\u662f\u8d44\u6e90\u5bc6\u96c6\u578b\u7ec4\u4ef6\u3002\u73b0\u6709forkless\u533a\u5757\u94fe\u4ecd\u4f9d\u8d56\u4f20\u7edf\u7684forking\u6570\u636e\u5e93\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684forkless\u533a\u5757\u94fe\u6570\u636e\u5e93\u8bbe\u8ba1\u3002", "result": "\u4e0egeth-based Fantom\u533a\u5757\u94fe\u5ba2\u6237\u7aef\u76f8\u6bd4\uff0c\u65b0\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5b58\u50a8\u6548\u7387100\u500d\u7684\u63d0\u5347\u548c\u541e\u5410\u91cf10\u500d\u7684\u63d0\u5347\u3002", "conclusion": "\u65b0\u578bforkless\u533a\u5757\u94fe\u6570\u636e\u5e93\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u6548\u7387\uff0c\u9002\u5408\u73b0\u4ee3forkless\u533a\u5757\u94fe\u7684\u9700\u6c42\u3002"}}
{"id": "2508.20738", "pdf": "https://arxiv.org/pdf/2508.20738", "abs": "https://arxiv.org/abs/2508.20738", "authors": ["Lukas Bartl", "Jasmin Blanchette", "Tobias Nipkow"], "title": "Exploiting Instantiations from Paramodulation Proofs in Isabelle/HOL", "categories": ["cs.LO"], "comment": "This version of the contribution has been accepted for publication in\n  CADE-30, after peer review but is not the Version of Record and does not\n  reflect post-acceptance improvements, or any corrections", "summary": "Metis is an ordered paramodulation prover built into the Isabelle/HOL proof\nassistant. It attempts to close the current goal using a given list of lemmas.\nTypically these lemmas are found by Sledgehammer, a tool that integrates\nexternal automatic provers. We present a new tool that analyzes successful\nMetis proofs to derive variable instantiations. These increase Sledgehammer's\nsuccess rate, improve the speed of Sledgehammer-generated proofs, and help\nusers understand why a goal follows from the lemmas.", "AI": {"tldr": "Metis\u662f\u96c6\u6210\u5728Isabelle/HOL\u4e2d\u7684\u6709\u5e8f\u53c2\u6570\u5316\u8bc1\u660e\u5668\uff0c\u65b0\u5de5\u5177\u901a\u8fc7\u5206\u6790\u6210\u529f\u7684Metis\u8bc1\u660e\u4f18\u5316Sledgehammer\u7684\u6027\u80fd\u3002", "motivation": "\u63d0\u9ad8Sledgehammer\u7684\u6210\u529f\u7387\u548c\u8bc1\u660e\u901f\u5ea6\uff0c\u5e76\u5e2e\u52a9\u7528\u6237\u7406\u89e3\u76ee\u6807\u5982\u4f55\u4ece\u5f15\u7406\u4e2d\u63a8\u5bfc\u3002", "method": "\u5206\u6790\u6210\u529f\u7684Metis\u8bc1\u660e\u4ee5\u63a8\u5bfc\u53d8\u91cf\u5b9e\u4f8b\u5316\u3002", "result": "\u63d0\u5347\u4e86Sledgehammer\u7684\u6210\u529f\u7387\u548c\u901f\u5ea6\uff0c\u589e\u5f3a\u4e86\u7528\u6237\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u65b0\u5de5\u5177\u663e\u8457\u4f18\u5316\u4e86Sledgehammer\u7684\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.20546", "pdf": "https://arxiv.org/pdf/2508.20546", "abs": "https://arxiv.org/abs/2508.20546", "authors": ["Berta C\u00e9spedes-Sarrias", "Carlos Collado-Capell", "Pablo Rodenas-Ruiz", "Olena Hrynenko", "Andrea Cavallaro"], "title": "MM-HSD: Multi-Modal Hate Speech Detection in Videos", "categories": ["cs.MM", "cs.AI"], "comment": "Accepted at ACM Multimedia 2025", "summary": "While hate speech detection (HSD) has been extensively studied in text,\nexisting multi-modal approaches remain limited, particularly in videos. As\nmodalities are not always individually informative, simple fusion methods fail\nto fully capture inter-modal dependencies. Moreover, previous work often omits\nrelevant modalities such as on-screen text and audio, which may contain subtle\nhateful content and thus provide essential cues, both individually and in\ncombination with others. In this paper, we present MM-HSD, a multi-modal model\nfor HSD in videos that integrates video frames, audio, and text derived from\nspeech transcripts and from frames (i.e.~on-screen text) together with features\nextracted by Cross-Modal Attention (CMA). We are the first to use CMA as an\nearly feature extractor for HSD in videos, to systematically compare query/key\nconfigurations, and to evaluate the interactions between different modalities\nin the CMA block. Our approach leads to improved performance when on-screen\ntext is used as a query and the rest of the modalities serve as a key.\nExperiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art\nmethods on M-F1 score (0.874), using concatenation of transcript, audio, video,\non-screen text, and CMA for feature extraction on raw embeddings of the\nmodalities. The code is available at https://github.com/idiap/mm-hsd", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578bMM-HSD\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u9891\u5e27\u3001\u97f3\u9891\u548c\u6587\u672c\uff0c\u5e76\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\uff08CMA\uff09\u63d0\u9ad8\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u6cd5\u591a\u6a21\u6001\u878d\u5408\u4e0d\u8db3\uff0c\u5ffd\u7565\u5173\u952e\u6a21\u6001\uff08\u5982\u5c4f\u5e55\u6587\u672c\u548c\u97f3\u9891\uff09\uff0c\u4e9f\u9700\u66f4\u5168\u9762\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u89c6\u9891\u5e27\u3001\u97f3\u9891\u3001\u6587\u672c\uff08\u8bed\u97f3\u8f6c\u5f55\u548c\u5c4f\u5e55\u6587\u672c\uff09\u53caCMA\u7279\u5f81\u63d0\u53d6\uff0c\u9996\u6b21\u5c06CMA\u4f5c\u4e3a\u65e9\u671f\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728HateMM\u6570\u636e\u96c6\u4e0a\uff0cMM-HSD\u7684M-F1\u5f97\u52060.874\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c4f\u5e55\u6587\u672c\u4f5c\u4e3a\u67e5\u8be2\u65f6\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u591a\u6a21\u6001\u6574\u5408\u548cCMA\u7279\u5f81\u63d0\u53d6\u663e\u8457\u63d0\u5347\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6027\u80fd\uff0c\u5c4f\u5e55\u6587\u672c\u662f\u5173\u952e\u6a21\u6001\u3002"}}
{"id": "2508.20304", "pdf": "https://arxiv.org/pdf/2508.20304", "abs": "https://arxiv.org/abs/2508.20304", "authors": ["Siyuan Lu", "Kangwei Xu", "Peng Xie", "Rui Wang", "Yuanqing Cheng"], "title": "Testing and Fault Tolerance Techniques for Carbon Nanotube-Based FPGAs", "categories": ["cs.AR", "cs.SY", "eess.SY"], "comment": "Accepted by Integration, VLSI Journal", "summary": "As the semiconductor manufacturing process technology node shrinks into the\nnanometer-scale, the CMOS-based Field Programmable Gate Arrays (FPGAs) face big\nchallenges in scalability of performance and power consumption. Multi-walled\nCarbon Nanotube (MWCNT) serves as a promising candidate for Cu interconnects\nthanks to the superior conductivity. Moreover, Carbon Nanotube Field Transistor\n(CNFET) also emerges as a prospective alternative to the conventional CMOS\ndevice because of high power efficiency and large noise margin. The combination\nof MWCNT and CNFET enables the promising CNT-based FPGAs. However, the MWCNT\ninterconnects exhibit significant process variations due to immature\nfabrication process, leading to delay faults. Also, the non-ideal CNFET\nfabrication process may generate a few metallic CNTs (m-CNTs), rendering\ncorrelated faulty blocks. In this article, we propose a ring oscillator (RO)\nbased testing technique to detect delay faults due to the process variation of\nMWCNT interconnects. Furthermore, we propose an effective testing technique for\nthe carry chains in CLBs, and an improved circuit design based on the lookup\ntable (LUT) is applied to speed up the fault testing of CNT-based FPGAs. In\naddition, we propose a testing algorithm to detect m-CNTs in CLBs. Finally, we\npropose a redundant spare row sharing architecture to improve the yield of\nCNT-based FPGA further. Experimental results show that the test time for a\n6-input LUT can be reduced by 35.49% compared with conventional testing, and\nthe proposed algorithm can achieve a high test coverage with little overhead.\nThe proposed redundant architecture can repair the faulty segment effectively\nand efficiently.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u7eb3\u7c73\u7ea7\u534a\u5bfc\u4f53\u5236\u9020\u4e2dCMOS FPGA\u5728\u6027\u80fd\u548c\u529f\u8017\u6269\u5c55\u6027\u4e0a\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u78b3\u7eb3\u7c73\u7ba1\uff08CNT\uff09\u7684FPGA\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u6d4b\u8bd5\u6280\u672f\u548c\u8bbe\u8ba1\u6539\u8fdb\uff0c\u4ee5\u51cf\u5c11\u5de5\u827a\u53d8\u5f02\u5bfc\u81f4\u7684\u5ef6\u8fdf\u6545\u969c\u548c\u63d0\u9ad8\u4ea7\u91cf\u3002", "motivation": "\u968f\u7740\u534a\u5bfc\u4f53\u5236\u9020\u5de5\u827a\u8fdb\u5165\u7eb3\u7c73\u7ea7\uff0c\u4f20\u7edfCMOS FPGA\u5728\u6027\u80fd\u548c\u529f\u8017\u4e0a\u9762\u4e34\u6269\u5c55\u6027\u6311\u6218\u3002MWCNT\u548cCNFET\u56e0\u5176\u4f18\u8d8a\u6027\u80fd\u6210\u4e3a\u6f5c\u5728\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5de5\u827a\u53d8\u5f02\u548c\u91d1\u5c5e\u6027CNT\uff08m-CNT\uff09\u95ee\u9898\u9700\u8981\u901a\u8fc7\u65b0\u7684\u6d4b\u8bd5\u548c\u8bbe\u8ba1\u6280\u672f\u89e3\u51b3\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73af\u5f62\u632f\u8361\u5668\uff08RO\uff09\u7684\u6d4b\u8bd5\u6280\u672f\u6765\u68c0\u6d4bMWCNT\u4e92\u8fde\u7684\u5ef6\u8fdf\u6545\u969c\uff0c\u6539\u8fdb\u4e86\u67e5\u627e\u8868\uff08LUT\uff09\u8bbe\u8ba1\u4ee5\u52a0\u901f\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4bCLB\u4e2dm-CNT\u7684\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5197\u4f59\u5907\u7528\u884c\u5171\u4eab\u67b6\u6784\u4ee5\u63d0\u9ad8\u4ea7\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c6\u8f93\u5165LUT\u7684\u6d4b\u8bd5\u65f6\u95f4\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51cf\u5c1135.49%\uff0c\u6d4b\u8bd5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u8986\u76d6\u7387\u4e14\u5f00\u9500\u4f4e\u3002\u5197\u4f59\u67b6\u6784\u80fd\u9ad8\u6548\u4fee\u590d\u6545\u969c\u6bb5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6280\u672f\u548c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86CNT\u57faFPGA\u7684\u5de5\u827a\u53d8\u5f02\u548c\u76f8\u5173\u6545\u969c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u6548\u7387\u548c\u4ea7\u91cf\u3002"}}
{"id": "2508.20205", "pdf": "https://arxiv.org/pdf/2508.20205", "abs": "https://arxiv.org/abs/2508.20205", "authors": ["Md. Emadul Haque", "Faisal Tariq", "Muhammad R A Khandaker", "Md. Sakir Hossain", "Muhammad Ali Imran", "Kai-Kit Wong"], "title": "A Comprehensive Survey of 5G URLLC and Challenges in the 6G Era", "categories": ["cs.NI"], "comment": "41 pages, 9 figures", "summary": "As the wireless communication paradigm is being transformed from human\ncentered communication services towards machine centered communication\nservices, the requirements of rate, latency and reliability for these services\nhave also been transformed drastically. Thus the concept of Ultra Reliable and\nLow Latency Communication (URLLC) has emerged as a dominant theme for 5G and 6G\nsystems. Though the latency and reliability requirement varies from one use\ncase to another, URLLC services generally aim to achieve very high reliability\nin the range of 99.999\\% while ensuring the latency of up to 1 ms. These two\ntargets are however inherently opposed to one another. Significant amounts of\nwork have been carried out to meet these ambitious but conflicting targets. In\nthis article a comprehensive survey of the URLLC approaches in 5G systems are\nanalysed in detail. Effort has been made to trace the history and evolution of\nlatency and reliability issues in wireless communication. A layered approach is\ntaken where physical layer, Medium Access Control (MAC) layer as well as cross\nlayer techniques are discussed in detail. It also covers the design\nconsideration for various 5G and beyond verticals. Finally the article\nconcludes by providing a detailed discussion on challenges and future outlook\nwith particular focus on the emerging 6G paradigm.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e865G\u7cfb\u7edf\u4e2d\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff08URLLC\uff09\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u63a2\u8ba8\u4e86\u5176\u5386\u53f2\u6f14\u53d8\u3001\u5206\u5c42\u6280\u672f\u53ca\u8bbe\u8ba1\u8003\u91cf\uff0c\u5e76\u5bf96G\u672a\u6765\u6311\u6218\u8fdb\u884c\u4e86\u5c55\u671b\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u901a\u4fe1\u4ece\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u8f6c\u5411\u4ee5\u673a\u5668\u4e3a\u4e2d\u5fc3\uff0c\u5bf9\u901f\u7387\u3001\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u7684\u9700\u6c42\u6025\u5267\u53d8\u5316\uff0cURLLC\u6982\u5ff5\u5e94\u8fd0\u800c\u751f\u3002", "method": "\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff0c\u8be6\u7ec6\u5206\u6790\u4e86\u7269\u7406\u5c42\u3001MAC\u5c42\u53ca\u8de8\u5c42\u6280\u672f\uff0c\u5e76\u63a2\u8ba8\u4e865G\u53ca\u5176\u540e\u7eed\u5782\u76f4\u9886\u57df\u7684\u8bbe\u8ba1\u8003\u8651\u3002", "result": "\u603b\u7ed3\u4e865G\u7cfb\u7edf\u4e2dURLLC\u7684\u6280\u672f\u5b9e\u73b0\u53ca\u5176\u5728\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u51b2\u7a81\u4e0e\u5e73\u8861\u3002", "conclusion": "\u6587\u7ae0\u6307\u51fa\uff0c\u5c3d\u7ba15G\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f466G\u4ecd\u9700\u89e3\u51b3\u66f4\u9ad8\u8981\u6c42\u7684URLLC\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.20563", "pdf": "https://arxiv.org/pdf/2508.20563", "abs": "https://arxiv.org/abs/2508.20563", "authors": ["Zheying Zhang", "Tomas Herda", "Victoria Pichler", "Pekka Abrahamsson", "Geir K. Hanssen", "Joshua Kerievsky", "Alex Polyakov", "Mohit Chandna", "Marius Irgens", "Kai-Kristian Kemell", "Ayman Asad Khan", "Crystal Kwok", "Evan Leybourn", "Munish Malik", "Dorota Mleczko", "Morteza Moalagh", "Christopher Morales", "Yuliia Pieskova", "Daniel Plan\u00f6tscher", "Mika Saari", "Anastasiia Tkalich", "Karl Josef Gstettner", "Xiaofeng Wang"], "title": "AI and Agile Software Development: A Research Roadmap from the XP2025 Workshop", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper synthesizes the key findings from a full-day XP2025 workshop on\n\"AI and Agile: From Frustration to Success\", held in Brugg-Windisch,\nSwitzerland. The workshop brought together over 30 interdisciplinary academic\nresearchers and industry practitioners to tackle the concrete challenges and\nemerging opportunities at the intersection of Generative Artificial\nIntelligence (GenAI) and agile software development. Through structured,\ninteractive breakout sessions, participants identified shared pain points like\ntool fragmentation, governance, data quality, and critical skills gaps in AI\nliteracy and prompt engineering. These issues were further analyzed, revealing\nunderlying causes and cross-cutting concerns. The workshop concluded by\ncollaboratively co-creating a multi-thematic research roadmap, articulating\nboth short-term, implementable actions and visionary, long-term research\ndirections. This cohesive agenda aims to guide future investigation and drive\nthe responsible, human-centered integration of GenAI into agile practices.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u5728\u745e\u58eb\u5e03\u5415\u683c-\u6e29\u8fea\u65bd\u4e3e\u884c\u7684XP2025\u7814\u8ba8\u4f1a\u2018AI\u4e0e\u654f\u6377\uff1a\u4ece\u632b\u6298\u5230\u6210\u529f\u2019\u7684\u5173\u952e\u53d1\u73b0\uff0c\u63a2\u8ba8\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u4e0e\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u7ed3\u5408\u65f6\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u5e76\u5171\u540c\u5236\u5b9a\u4e86\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u4e0e\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u7ed3\u5408\u65f6\u7684\u6311\u6218\u548c\u673a\u9047\uff0c\u4fc3\u8fdb\u4e24\u8005\u7684\u8d1f\u8d23\u4efb\u4e14\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6574\u5408\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u4e92\u52a8\u5206\u7ec4\u8ba8\u8bba\uff0c\u5206\u6790\u5171\u540c\u75db\u70b9\uff08\u5982\u5de5\u5177\u788e\u7247\u5316\u3001\u6570\u636e\u8d28\u91cf\u7b49\uff09\uff0c\u5e76\u5236\u5b9a\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "result": "\u8bc6\u522b\u4e86\u5171\u540c\u75db\u70b9\u53ca\u5176\u6839\u672c\u539f\u56e0\uff0c\u5236\u5b9a\u4e86\u5305\u542b\u77ed\u671f\u884c\u52a8\u548c\u957f\u671f\u7814\u7a76\u65b9\u5411\u7684\u591a\u4e3b\u9898\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "conclusion": "\u7814\u8ba8\u4f1a\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548cGenAI\u5728\u654f\u6377\u5b9e\u8df5\u4e2d\u7684\u6574\u5408\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.10931", "pdf": "https://arxiv.org/pdf/2508.10931", "abs": "https://arxiv.org/abs/2508.10931", "authors": ["Wenqi Guo", "Shan Du"], "title": "VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "We introduce Value Sign Flip (VSF), a simple and efficient method for\nincorporating negative prompt guidance in few-step diffusion and flow-matching\nimage generation models. Unlike existing approaches such as classifier-free\nguidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by\nflipping the sign of attention values from negative prompts. Our method\nrequires only small computational overhead and integrates effectively with\nMMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as\ncross-attention-based models like Wan. We validate VSF on challenging datasets\nwith complex prompt pairs and demonstrate superior performance in both static\nimage and video generation tasks. Experimental results show that VSF\nsignificantly improves negative prompt adherence compared to prior methods in\nfew-step models, and even CFG in non-few-step models, while maintaining\ncompetitive image quality. Code and ComfyUI node are available in\nhttps://github.com/weathon/VSF/tree/main.", "AI": {"tldr": "VSF\u662f\u4e00\u79cd\u901a\u8fc7\u7ffb\u8f6c\u8d1f\u63d0\u793a\u7684\u6ce8\u610f\u529b\u503c\u7b26\u53f7\u6765\u52a8\u6001\u6291\u5236\u4e0d\u9700\u8981\u5185\u5bb9\u7684\u65b0\u65b9\u6cd5\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\uff0c\u5728\u9759\u6001\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982CFG\u3001NASA\u548cNAG\u5728\u8d1f\u63d0\u793a\u5f15\u5bfc\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cVSF\u65e8\u5728\u4ee5\u66f4\u9ad8\u6548\u7684\u65b9\u5f0f\u6539\u8fdb\u8fd9\u4e00\u529f\u80fd\u3002", "method": "VSF\u901a\u8fc7\u7ffb\u8f6c\u8d1f\u63d0\u793a\u7684\u6ce8\u610f\u529b\u503c\u7b26\u53f7\u6765\u52a8\u6001\u6291\u5236\u4e0d\u9700\u8981\u7684\u5185\u5bb9\uff0c\u9002\u7528\u4e8eMMDiT\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u578b\u3002", "result": "VSF\u5728\u590d\u6742\u63d0\u793a\u5bf9\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d1f\u63d0\u793a\u7684\u9075\u5faa\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "VSF\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u8d1f\u63d0\u793a\u5f15\u5bfc\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2508.20922", "pdf": "https://arxiv.org/pdf/2508.20922", "abs": "https://arxiv.org/abs/2508.20922", "authors": ["Markus B\u00f6ck", "J\u00fcrgen Cito"], "title": "Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops", "categories": ["cs.PL"], "comment": null, "summary": "It is commonly known that any Bayesian network can be implemented as a\nprobabilistic program, but the reverse direction is not so clear. In this work,\nwe address the open question to what extent a probabilistic program with\nuser-labelled sample statements and while loops - features found in languages\nlike Gen, Turing, and Pyro - can be represented graphically. To this end, we\nextend existing operational semantics to support these language features. By\ntranslating a program to its control-flow graph, we define a sound static\nanalysis that approximates the dependency structure of the random variables in\nthe program. As a result, we obtain a static factorisation of the implicitly\ndefined program density, which is equivalent to the known Bayesian network\nfactorisation for programs without loops and constant labels, but constitutes a\nnovel graphical representation for programs that define an unbounded number of\nrandom variables via loops or dynamic labels. We further develop a sound\nprogram slicing technique to leverage this structure to statically enable three\nwell-known optimisations for the considered program class: we reduce the\nvariance of gradient estimates in variational inference and we speed up both\nsingle-site Metropolis Hastings and sequential Monte Carlo. These optimisations\nare proven correct and empirically shown to match or outperform existing\ntechniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u6982\u7387\u7a0b\u5e8f\u4e0e\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u652f\u6301\u5faa\u73af\u548c\u52a8\u6001\u6807\u7b7e\u7684\u7a0b\u5e8f\u9759\u6001\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u4f18\u5316\u6280\u672f\u3002", "motivation": "\u7814\u7a76\u6982\u7387\u7a0b\u5e8f\uff08\u5e26\u6709\u5faa\u73af\u548c\u52a8\u6001\u6807\u7b7e\uff09\u5982\u4f55\u56fe\u5f62\u5316\u8868\u793a\uff0c\u5e76\u5229\u7528\u8fd9\u79cd\u8868\u793a\u4f18\u5316\u63a8\u7406\u7b97\u6cd5\u3002", "method": "\u6269\u5c55\u64cd\u4f5c\u8bed\u4e49\uff0c\u6784\u5efa\u63a7\u5236\u6d41\u56fe\uff0c\u5b9a\u4e49\u9759\u6001\u5206\u6790\u4ee5\u8fd1\u4f3c\u53d8\u91cf\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5f00\u53d1\u7a0b\u5e8f\u5207\u7247\u6280\u672f\u3002", "result": "\u83b7\u5f97\u4e86\u7a0b\u5e8f\u5bc6\u5ea6\u7684\u9759\u6001\u5206\u89e3\uff0c\u652f\u6301\u4e09\u79cd\u4f18\u5316\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u6982\u7387\u7a0b\u5e8f\u7684\u63a8\u7406\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7a0b\u5e8f\u7ed3\u6784\u3002"}}
{"id": "2508.20383", "pdf": "https://arxiv.org/pdf/2508.20383", "abs": "https://arxiv.org/abs/2508.20383", "authors": ["Prakash Shukla", "Paul Parsons"], "title": "Identifying Framing Practices in Visualization Design Through Practitioner Reflections", "categories": ["cs.HC"], "comment": "Accepted for publication in the IEEEVIS Workshop on Visualization for\n  Communication (VisComm 25)", "summary": "Framing -- how designers define and reinterpret problems, shape narratives,\nand guide audience understanding -- is central to design practice. Yet in\nvisualization research, framing has been examined mostly through its rhetorical\nand perceptual effects on audiences, leaving its role in the design process\nunderexplored. This study addresses that gap by analyzing publicly available\npodcasts and book chapters in which over 80 professional visualization\ndesigners reflect on their work. We find that framing is a pervasive, iterative\nactivity, evident in scoping problems, interpreting data, aligning with\nstakeholder goals, and shaping narrative direction. Our analysis identifies the\nconditions that trigger reframing and the strategies practitioners use to\nnavigate uncertainty and guide design. These findings position framing as a\ncore dimension of visualization practice and underscore the need for research\nand education to support the interpretive and strategic judgment that\npractitioners exercise throughout the design process.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u53ef\u89c6\u5316\u8bbe\u8ba1\u4e2d\u6846\u67b6\uff08framing\uff09\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u6307\u51fa\u5176\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u5bf9\u4e13\u4e1a\u8bbe\u8ba1\u5e08\u7684\u91c7\u8bbf\u5206\u6790\u4e86\u89e6\u53d1\u91cd\u65b0\u6846\u67b6\u7684\u6761\u4ef6\u548c\u5b9e\u8df5\u7b56\u7565\u3002", "motivation": "\u76ee\u524d\u53ef\u89c6\u5316\u7814\u7a76\u4e2d\u6846\u67b6\u7684\u4f5c\u7528\u4e3b\u8981\u5173\u6ce8\u5176\u5bf9\u53d7\u4f17\u7684\u4fee\u8f9e\u548c\u611f\u77e5\u5f71\u54cd\uff0c\u800c\u5bf9\u5176\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u679080\u591a\u540d\u4e13\u4e1a\u53ef\u89c6\u5316\u8bbe\u8ba1\u5e08\u5728\u516c\u5f00\u64ad\u5ba2\u548c\u4e66\u7ae0\u8282\u4e2d\u7684\u53cd\u601d\uff0c\u7814\u7a76\u4e86\u6846\u67b6\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6846\u67b6\u662f\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u6838\u5fc3\u6d3b\u52a8\uff0c\u6d89\u53ca\u95ee\u9898\u754c\u5b9a\u3001\u6570\u636e\u89e3\u91ca\u3001\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u76ee\u6807\u5bf9\u9f50\u53ca\u53d9\u4e8b\u65b9\u5411\u5236\u5b9a\u3002\u540c\u65f6\u8bc6\u522b\u4e86\u89e6\u53d1\u91cd\u65b0\u6846\u67b6\u7684\u6761\u4ef6\u548c\u5b9e\u8df5\u7b56\u7565\u3002", "conclusion": "\u6846\u67b6\u662f\u53ef\u89c6\u5316\u5b9e\u8df5\u7684\u6838\u5fc3\u7ef4\u5ea6\uff0c\u5f3a\u8c03\u4e86\u7814\u7a76\u548c\u6559\u80b2\u9700\u8981\u652f\u6301\u8bbe\u8ba1\u5e08\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u8fd0\u7528\u7684\u89e3\u91ca\u6027\u548c\u6218\u7565\u6027\u5224\u65ad\u3002"}}
{"id": "2508.20258", "pdf": "https://arxiv.org/pdf/2508.20258", "abs": "https://arxiv.org/abs/2508.20258", "authors": ["Arya Tschand", "Muhammad Awad", "Ryan Swann", "Kesavan Ramakrishnan", "Jeffrey Ma", "Keith Lowery", "Ganesh Dasika", "Vijay Janapa Reddi"], "title": "SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown progress in GPU kernel performance\nengineering using inefficient search-based methods that optimize around\nruntime. Any existing approach lacks a key characteristic that human\nperformance engineers rely on for near-optimal utilization --\nhardware-awareness. By leveraging the workload's specific memory access\npatterns, architecture specifications, filtered profiling logs, and reflections\non historical performance, we can make software-level optimizations that are\ntailored to the underlying hardware. SwizzlePerf automatically generates\nspatial optimizations for GPU kernels on disaggregated architectures by giving\nLLMs explicit hardware-awareness.\n  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same\nhardware-specific optimal swizzling pattern that took expert performance\nengineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,\nSwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve\nup to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the\nfirst of many steps toward systematically creating hardware-aware LLM\nperformance engineering agents.", "AI": {"tldr": "SwizzlePerf\u5229\u7528\u786c\u4ef6\u611f\u77e5\u6280\u672f\uff0c\u901a\u8fc7LLMs\u81ea\u52a8\u4f18\u5316GPU\u5185\u6838\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4e0e\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\u7f3a\u4e4f\u786c\u4ef6\u611f\u77e5\u80fd\u529b\uff0c\u800c\u4eba\u7c7b\u6027\u80fd\u5de5\u7a0b\u5e08\u4f9d\u8d56\u6b64\u80fd\u529b\u5b9e\u73b0\u8fd1\u4e4e\u6700\u4f18\u7684\u5229\u7528\u7387\u3002\u901a\u8fc7\u7ed3\u5408\u786c\u4ef6\u7279\u6027\u4e0e\u8f6f\u4ef6\u4f18\u5316\uff0c\u53ef\u4ee5\u63d0\u5347GPU\u5185\u6838\u6027\u80fd\u3002", "method": "SwizzlePerf\u5229\u7528\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3001\u67b6\u6784\u89c4\u683c\u3001\u5206\u6790\u65e5\u5fd7\u548c\u5386\u53f2\u6027\u80fd\u6570\u636e\uff0c\u4e3aLLMs\u63d0\u4f9b\u786c\u4ef6\u611f\u77e5\u80fd\u529b\uff0c\u81ea\u52a8\u751f\u6210GPU\u5185\u6838\u7684\u7a7a\u95f4\u4f18\u5316\u65b9\u6848\u3002", "result": "SwizzlePerf\u57285\u5206\u949f\u5185\u751f\u6210\u4e13\u5bb6\u97002\u5468\u624d\u80fd\u627e\u5230\u7684\u6700\u4f18\u5316\u6a21\u5f0f\uff0c\u5bf910\u4e2a\u5185\u6838\u4e2d\u76849\u4e2a\u5b9e\u73b0\u4e86\u6700\u9ad82.06\u500d\u52a0\u901f\u548c70%\u7684L2\u547d\u4e2d\u7387\u63d0\u5347\u3002", "conclusion": "SwizzlePerf\u662f\u7cfb\u7edf\u5316\u521b\u5efa\u786c\u4ef6\u611f\u77e5LLM\u6027\u80fd\u5de5\u7a0b\u4ee3\u7406\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.20409", "pdf": "https://arxiv.org/pdf/2508.20409", "abs": "https://arxiv.org/abs/2508.20409", "authors": ["Siyao Li", "Conrad Prisby", "Thomas Yang"], "title": "Blind Source Separation-Enabled Joint Communication and Sensing in IBFD MIMO Systems", "categories": ["cs.ET", "cs.PF"], "comment": "6 pages,5 figures, accepted by IEEE Military Communications\n  Conference 2025", "summary": "This paper addresses the challenge of joint communication and sensing (JCAS)\nin next-generation wireless networks, with an emphasis on in-band full-duplex\n(IBFD) multiple-input multiple-output (MIMO) systems. Traditionally,\nself-interference (SI) in IBFD systems is a major obstacle to recovering the\nsignal of interest (SOI). Under the JCAS paradigm, however, this high-power SI\nsignal presents an opportunity for efficient sensing. Since each transceiver\nnode has access to the original SI signal, its environmental reflections can be\nexploited to estimate channel conditions and detect changes, without requiring\ndedicated radar waveforms. We propose a blind source separation (BSS)-based\nframework to simultaneously perform self-interference cancellation (SIC) and\nextract sensing information in IBFD MIMO settings. The approach applies the\nFast Independent Component Analysis (FastICA) algorithm to separate the SI and\nSOI signals while enabling simultaneous signal recovery and channel estimation.\nSimulation results confirm the framework's effectiveness, showing improved\nsensing and communication performance as signal frame size increases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f2\u6e90\u5206\u79bb\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5168\u53cc\u5de5MIMO\u7cfb\u7edf\u4e2d\u540c\u65f6\u8fdb\u884c\u81ea\u5e72\u6270\u6d88\u9664\u548c\u611f\u77e5\u4fe1\u606f\u63d0\u53d6\uff0c\u5229\u7528FastICA\u7b97\u6cd5\u5b9e\u73b0\u4fe1\u53f7\u6062\u590d\u548c\u4fe1\u9053\u4f30\u8ba1\u3002", "motivation": "\u4f20\u7edf\u5168\u53cc\u5de5\u7cfb\u7edf\u4e2d\u7684\u81ea\u5e72\u6270\u662f\u4fe1\u53f7\u6062\u590d\u7684\u4e3b\u8981\u969c\u788d\uff0c\u4f46\u5728\u8054\u5408\u901a\u4fe1\u4e0e\u611f\u77e5\u7684\u8303\u5f0f\u4e2d\uff0c\u8fd9\u79cd\u9ad8\u529f\u7387\u81ea\u5e72\u6270\u4fe1\u53f7\u4e3a\u9ad8\u6548\u611f\u77e5\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u76f2\u6e90\u5206\u79bb\uff08BSS\uff09\u7684\u6846\u67b6\uff0c\u91c7\u7528FastICA\u7b97\u6cd5\u5206\u79bb\u81ea\u5e72\u6270\u4fe1\u53f7\u548c\u611f\u5174\u8da3\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4fe1\u53f7\u6062\u590d\u4e0e\u4fe1\u9053\u4f30\u8ba1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4fe1\u53f7\u5e27\u5927\u5c0f\u589e\u52a0\u65f6\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u611f\u77e5\u548c\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u81ea\u5e72\u6270\u4fe1\u53f7\u7684\u611f\u77e5\u6f5c\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53cc\u4efb\u52a1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20912", "pdf": "https://arxiv.org/pdf/2508.20912", "abs": "https://arxiv.org/abs/2508.20912", "authors": ["Kerem Akillioglu", "Anurag Chakraborty", "Sairaj Voruganti", "M. Tamer \u00d6zsu"], "title": "Research Challenges in Relational Database Management Systems for LLM Queries", "categories": ["cs.DB", "cs.AI"], "comment": "This paper will appear in the 6th International Workshop on Applied\n  AI for Database Systems and Applications, AIDB Workshop at VLDB 2025", "summary": "Large language models (LLMs) have become essential for applications such as\ntext summarization, sentiment analysis, and automated question-answering.\nRecently, LLMs have also been integrated into relational database management\nsystems to enhance querying and support advanced data processing. Companies\nsuch as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly\nwithin SQL, denoted as LLM queries, to boost data insights. However,\nopen-source solutions currently have limited functionality and poor\nperformance. In this work, we present an early exploration of two open-source\nsystems and one enterprise platform, using five representative queries to\nexpose functional, performance, and scalability limits in today's SQL-invoked\nLLM integrations. We identify three main issues: enforcing structured outputs,\noptimizing resource utilization, and improving query planning. We implemented\ninitial solutions and observed improvements in accommodating LLM powered SQL\nqueries. These early gains demonstrate that tighter integration of LLM+DBMS is\nthe key to scalable and efficient processing of LLM queries.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5173\u7cfb\u578b\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u7814\u7a76\u4e86\u73b0\u6709\u5f00\u6e90\u548c\u5546\u4e1a\u5e73\u53f0\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u5e93\u67e5\u8be2\u4e2d\u7684\u5e94\u7528\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u529f\u80fd\u6709\u9650\u4e14\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u4e24\u4e2a\u5f00\u6e90\u7cfb\u7edf\u548c\u4e00\u4e2a\u4f01\u4e1a\u5e73\u53f0\uff0c\u901a\u8fc7\u4e94\u4e2a\u4ee3\u8868\u6027\u67e5\u8be2\u63ed\u793a\u529f\u80fd\u3001\u6027\u80fd\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u4e09\u5927\u95ee\u9898\uff1a\u7ed3\u6784\u5316\u8f93\u51fa\u3001\u8d44\u6e90\u5229\u7528\u4f18\u5316\u548c\u67e5\u8be2\u89c4\u5212\uff0c\u5e76\u5b9e\u73b0\u521d\u6b65\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "LLM\u4e0eDBMS\u7684\u7d27\u5bc6\u96c6\u6210\u662f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55LLM\u67e5\u8be2\u7684\u5173\u952e\u3002"}}
{"id": "2508.20371", "pdf": "https://arxiv.org/pdf/2508.20371", "abs": "https://arxiv.org/abs/2508.20371", "authors": ["Sopam Dasgupta", "Sadaf MD Halim", "Joaqu\u00edn Arias", "Elmer Salazar", "Gopal Gupta"], "title": "P2C: Path to Counterfactuals", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Machine-learning models are increasingly driving decisions in high-stakes\nsettings, such as finance, law, and hiring, thus, highlighting the need for\ntransparency. However, the key challenge is to balance transparency --\nclarifying `why' a decision was made -- with recourse: providing actionable\nsteps on `how' to achieve a favourable outcome from an unfavourable outcome.\nCounterfactual explanations reveal `why' an undesired outcome occurred and\n`how' to reverse it through targeted feature changes (interventions).\n  Current counterfactual approaches have limitations: 1) they often ignore\ncausal dependencies between features, and 2) they typically assume all\ninterventions can happen simultaneously, an unrealistic assumption in practical\nscenarios where actions are typically taken in a sequence. As a result, these\ncounterfactuals are often not achievable in the real world.\n  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that\nproduces a plan (ordered sequence of actions) converting an unfavourable\noutcome to a causally consistent favourable outcome. P2C addresses both\nlimitations by 1) Explicitly modelling causal relationships between features\nand 2) Ensuring that each intermediate state in the plan is feasible and\ncausally valid. P2C uses the goal-directed Answer Set Programming system\ns(CASP) to generate the plan accounting for feature changes that happen\nautomatically due to causal dependencies. Furthermore, P2C refines cost\n(effort) computation by only counting changes actively made by the user,\nresulting in realistic cost estimates. Finally, P2C highlights how its causal\nplanner outperforms standard planners, which lack causal knowledge and thus can\ngenerate illegal actions.", "AI": {"tldr": "P2C\u6846\u67b6\u901a\u8fc7\u56e0\u679c\u5efa\u6a21\u751f\u6210\u53ef\u884c\u7684\u53cd\u4e8b\u5b9e\u884c\u52a8\u5e8f\u5217\uff0c\u63d0\u5347\u51b3\u7b56\u900f\u660e\u5ea6\u548c\u53ef\u64cd\u4f5c\u6027\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\u9700\u8981\u900f\u660e\u5ea6\u548c\u53ef\u64cd\u4f5c\u6027\uff0c\u73b0\u6709\u53cd\u4e8b\u5b9e\u89e3\u91ca\u65b9\u6cd5\u5ffd\u7565\u4e86\u56e0\u679c\u4f9d\u8d56\u548c\u987a\u5e8f\u884c\u52a8\u7684\u5b9e\u9645\u9650\u5236\u3002", "method": "P2C\u5229\u7528\u56e0\u679c\u5efa\u6a21\u548cs(CASP)\u7cfb\u7edf\u751f\u6210\u6709\u5e8f\u884c\u52a8\u5e8f\u5217\uff0c\u786e\u4fdd\u6bcf\u4e2a\u4e2d\u95f4\u72b6\u6001\u5728\u56e0\u679c\u4e0a\u53ef\u884c\u3002", "result": "P2C\u4f18\u4e8e\u6807\u51c6\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u975e\u6cd5\u884c\u52a8\uff0c\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u6210\u672c\u4f30\u8ba1\u3002", "conclusion": "P2C\u4e3a\u9ad8\u900f\u660e\u5ea6\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u53cd\u4e8b\u5b9e\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.20560", "pdf": "https://arxiv.org/pdf/2508.20560", "abs": "https://arxiv.org/abs/2508.20560", "authors": ["Klaus Schoeffmann", "Sahar Nasirihaghighi"], "title": "diveXplore at the Video Browser Showdown 2024", "categories": ["cs.MM"], "comment": null, "summary": "According to our experience from VBS2023 and the feedback from the IVR4B\nspecial session at CBMI2023, we have largely revised the diveXplore system for\nVBS2024. It now integrates OpenCLIP trained on the LAION-2B dataset for\nimage/text embeddings that are used for free-text and visual similarity search,\na query server that is able to distribute different queries and merge the\nresults, a user interface optimized for fast browsing, as well as an\nexploration view for large clusters of similar videos (e.g., weddings,\nparaglider events, snow and ice scenery, etc.).", "AI": {"tldr": "\u4ecb\u7ecd\u4e86VBS2024\u4e2ddiveXplore\u7cfb\u7edf\u7684\u5347\u7ea7\u5185\u5bb9\uff0c\u5305\u62ec\u5d4c\u5165\u6a21\u578b\u3001\u67e5\u8be2\u670d\u52a1\u5668\u548c\u7528\u6237\u754c\u9762\u7684\u6539\u8fdb\u3002", "motivation": "\u57fa\u4e8eVBS2023\u7684\u5b9e\u8df5\u548cIVR4B\u4f1a\u8bae\u7684\u53cd\u9988\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u641c\u7d22\u9700\u6c42\u3002", "method": "\u6574\u5408OpenCLIP\u6a21\u578b\u3001\u4f18\u5316\u67e5\u8be2\u670d\u52a1\u5668\u548c\u7528\u6237\u754c\u9762\uff0c\u65b0\u589e\u63a2\u7d22\u89c6\u56fe\u529f\u80fd\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u81ea\u7531\u6587\u672c\u548c\u89c6\u89c9\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u5e76\u63d0\u5347\u4e86\u6d4f\u89c8\u548c\u67e5\u8be2\u6548\u7387\u3002", "conclusion": "diveXplore\u7cfb\u7edf\u7684\u6539\u8fdb\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u89c6\u9891\u5185\u5bb9\u7684\u591a\u6837\u5316\u63a2\u7d22\u548c\u641c\u7d22\u9700\u6c42\u3002"}}
{"id": "2508.20425", "pdf": "https://arxiv.org/pdf/2508.20425", "abs": "https://arxiv.org/abs/2508.20425", "authors": ["Shuhan Liu", "Samuel Dayo", "Peijing Li", "Philip Levis", "Subhasish Mitra", "Thierry Tambe", "David Tennenhouse", "H. -S. Philip Wong"], "title": "The Future of Memory: Limits and Opportunities", "categories": ["cs.AR"], "comment": "3 Pages, 2 Figures, 1 Table, Accepted to SOSP 25 BigMem Workshop", "summary": "Memory latency, bandwidth, capacity, and energy increasingly limit\nperformance. In this paper, we reconsider proposed system architectures that\nconsist of huge (many-terabyte to petabyte scale) memories shared among large\nnumbers of CPUs. We argue two practical engineering challenges, scaling and\nsignaling, limit such designs. We propose the opposite approach. Rather than\ncreate large, shared, homogenous memories, systems explicitly break memory up\ninto smaller slices more tightly coupled with compute elements. Leveraging\nadvances in 2.5D/3D integration, this compute-memory node provisions private\nlocal memory, enabling accesses of node-exclusive data through micrometer-scale\ndistances, and dramatically reduced access cost. In-package memory elements\nsupport shared state within a processor, providing far better bandwidth and\nenergy-efficiency than DRAM, which is used as main memory for large working\nsets and cold data. Hardware making memory capacities and distances explicit\nallows software to efficiently compose this hierarchy, managing data placement\nand movement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u5185\u5b58\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u7247\u6bb5\u5e76\u4e0e\u8ba1\u7b97\u5355\u5143\u7d27\u5bc6\u8026\u5408\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u5171\u4eab\u5927\u5185\u5b58\u8bbe\u8ba1\u7684\u6269\u5c55\u548c\u4fe1\u53f7\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u5185\u5b58\u5ef6\u8fdf\u3001\u5e26\u5bbd\u3001\u5bb9\u91cf\u548c\u80fd\u8017\u5bf9\u6027\u80fd\u7684\u9650\u5236\uff0c\u8bba\u6587\u91cd\u65b0\u8bc4\u4f30\u4e86\u5171\u4eab\u5927\u5185\u5b58\u67b6\u6784\uff0c\u5e76\u6307\u51fa\u5176\u5728\u5b9e\u9645\u5de5\u7a0b\u4e2d\u7684\u6311\u6218\u3002", "method": "\u91c7\u75282.5D/3D\u96c6\u6210\u6280\u672f\uff0c\u5c06\u5185\u5b58\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u7247\u6bb5\uff0c\u4e0e\u8ba1\u7b97\u5355\u5143\u7d27\u5bc6\u7ed3\u5408\uff0c\u5f62\u6210\u8ba1\u7b97-\u5185\u5b58\u8282\u70b9\uff0c\u540c\u65f6\u5229\u7528DRAM\u5904\u7406\u5927\u578b\u5de5\u4f5c\u96c6\u548c\u51b7\u6570\u636e\u3002", "result": "\u8fd9\u79cd\u8bbe\u8ba1\u663e\u8457\u964d\u4f4e\u4e86\u8bbf\u95ee\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u5e26\u5bbd\u548c\u80fd\u6548\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u534f\u540c\u7ba1\u7406\u4f18\u5316\u4e86\u6570\u636e\u653e\u7f6e\u548c\u79fb\u52a8\u3002", "conclusion": "\u901a\u8fc7\u6253\u7834\u4f20\u7edf\u7684\u5927\u5185\u5b58\u5171\u4eab\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u80fd\u591f\u5728\u5b9e\u8df5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.20272", "pdf": "https://arxiv.org/pdf/2508.20272", "abs": "https://arxiv.org/abs/2508.20272", "authors": ["Fatemeh Roshanzadeh", "Hamid Barati", "Ali Barati"], "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource Allocation and Markov Decision Process in Named Data Networking (NDN)", "categories": ["cs.NI"], "comment": null, "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRR-MDPF\u7684\u6df7\u5408\u7b56\u7565\uff0c\u7ed3\u5408\u4e86MDPF\u6a21\u578b\u548cDRR\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86NDN\u7684\u6027\u80fd\u3002", "motivation": "NDN\u7f51\u7edc\u4e2d\uff0c\u9ad8\u6548\u7684\u961f\u5217\u548c\u8d44\u6e90\u7ba1\u7406\u5bf9\u52a8\u6001\u9ad8\u6d41\u91cf\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u96c6\u6210MDPF\u548cDRR\uff0c\u4f7f\u8def\u7531\u5668\u80fd\u667a\u80fd\u9884\u6d4b\u8f6c\u53d1\u51b3\u7b56\u5e76\u8fdb\u884c\u516c\u5e73\u5e26\u5bbd\u5206\u914d\u3002", "result": "\u6a21\u62df\u663e\u793aDRR-MDPF\u5728\u541e\u5410\u91cf\u548c\u8d1f\u8f7d\u5747\u8861\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u7b56\u7565\u3002", "conclusion": "DRR-MDPF\u4e3aNDN\u63d0\u4f9b\u4e86\u667a\u80fd\u3001\u81ea\u9002\u5e94\u4e14\u53ef\u6269\u5c55\u7684\u961f\u5217\u7ba1\u7406\u65b9\u6848\u3002"}}
{"id": "2508.20737", "pdf": "https://arxiv.org/pdf/2508.20737", "abs": "https://arxiv.org/abs/2508.20737", "authors": ["Wei Ma", "Yixiao Yang", "Qiang Hu", "Shi Ying", "Zhi Jin", "Bo Du", "Zhenchang Xing", "Tianlin Li", "Junjie Shi", "Yang Liu", "Linxiao Jiang"], "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.", "AI": {"tldr": "\u603b\u7ed3\u4e86\u5c06LLM\u5e94\u7528\u5206\u89e3\u4e3a\u4e09\u5c42\u67b6\u6784\uff0c\u8bc4\u4f30\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u5728\u5404\u5c42\u7684\u9002\u7528\u6027\uff0c\u63d0\u51fa\u56db\u79cd\u534f\u4f5c\u7b56\u7565\u53ca\u4e00\u4e2a\u53ef\u4fe1\u8d28\u91cf\u4fdd\u969c\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3LLM\u5e94\u7528\u5728\u975e\u786e\u5b9a\u6027\u3001\u52a8\u6001\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u8d28\u91cf\u4fdd\u969c\u6311\u6218\u3002", "method": "\u5206\u89e3LLM\u5e94\u7528\u4e3a\u7cfb\u7edf\u58f3\u5c42\u3001\u63d0\u793a\u7f16\u6392\u5c42\u548cLLM\u63a8\u7406\u6838\u5fc3\u4e09\u5c42\uff0c\u5206\u6790\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u9002\u7528\u6027\u5e76\u63d0\u51fa\u534f\u4f5c\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u56db\u79cd\u534f\u4f5c\u7b56\u7565\u548c\u4e00\u4e2a\u5c01\u95ed\u5faa\u73af\u8d28\u91cf\u4fdd\u969c\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1AICL\u534f\u8bae\u4ee5\u652f\u6301\u6d4b\u8bd5\u6807\u51c6\u5316\u3002", "conclusion": "\u901a\u8fc7\u5206\u5c42\u5206\u6790\u4e0e\u534f\u4f5c\u7b56\u7565\uff0c\u4e3aLLM\u5e94\u7528\u8d28\u91cf\u4fdd\u969c\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u548c\u534f\u8bae\u652f\u6301\u3002"}}
{"id": "2508.20664", "pdf": "https://arxiv.org/pdf/2508.20664", "abs": "https://arxiv.org/abs/2508.20664", "authors": ["Kan Chen", "Zhen Meng", "Xiangmin Xu", "Jiaming Yang", "Emma Li", "Philip G. Zhao"], "title": "Task-Oriented Edge-Assisted Cross-System Design for Real-Time Human-Robot Interaction in Industrial Metaverse", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": "This paper has submitted to IEEE Transactions on Mobile Computing", "summary": "Real-time human-device interaction in industrial Metaverse faces challenges\nsuch as high computational load, limited bandwidth, and strict latency. This\npaper proposes a task-oriented edge-assisted cross-system framework using\ndigital twins (DTs) to enable responsive interactions. By predicting operator\nmotions, the system supports: 1) proactive Metaverse rendering for visual\nfeedback, and 2) preemptive control of remote devices. The DTs are decoupled\ninto two virtual functions-visual display and robotic control-optimizing both\nperformance and adaptability. To enhance generalizability, we introduce the\nHuman-In-The-Loop Model-Agnostic Meta-Learning (HITL-MAML) algorithm, which\ndynamically adjusts prediction horizons. Evaluation on two tasks demonstrates\nthe framework's effectiveness: in a Trajectory-Based Drawing Control task, it\nreduces weighted RMSE from 0.0712 m to 0.0101 m; in a real-time 3D scene\nrepresentation task for nuclear decommissioning, it achieves a PSNR of 22.11,\nSSIM of 0.8729, and LPIPS of 0.1298. These results show the framework's\ncapability to ensure spatial precision and visual fidelity in real-time,\nhigh-risk industrial environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4efb\u52a1\u9a71\u52a8\u7684\u8fb9\u7f18\u8f85\u52a9\u8de8\u7cfb\u7edf\u6846\u67b6\uff0c\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u4eba\u673a\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u5143\u5b87\u5b99\u4e2d\u7684\u9ad8\u8ba1\u7b97\u8d1f\u8f7d\u3001\u6709\u9650\u5e26\u5bbd\u548c\u4e25\u683c\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u5143\u5b87\u5b99\u4e2d\u7684\u5b9e\u65f6\u4eba\u673a\u4ea4\u4e92\u9762\u4e34\u9ad8\u8ba1\u7b97\u8d1f\u8f7d\u3001\u5e26\u5bbd\u9650\u5236\u548c\u4e25\u683c\u5ef6\u8fdf\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u54cd\u5e94\u8fc5\u901f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u64cd\u4f5c\u5458\u52a8\u4f5c\uff0c\u652f\u6301\u4e3b\u52a8\u5143\u5b87\u5b99\u6e32\u67d3\u548c\u8fdc\u7a0b\u8bbe\u5907\u9884\u63a7\u5236\u3002\u6570\u5b57\u5b6a\u751f\u5206\u89e3\u4e3a\u89c6\u89c9\u663e\u793a\u548c\u673a\u5668\u4eba\u63a7\u5236\u4e24\u4e2a\u865a\u62df\u529f\u80fd\uff0c\u5e76\u5f15\u5165HITL-MAML\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u8303\u56f4\u3002", "result": "\u5728\u8f68\u8ff9\u7ed8\u5236\u63a7\u5236\u548c\u6838\u9000\u5f793D\u573a\u666f\u8868\u793a\u4efb\u52a1\u4e2d\uff0c\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5dee\uff08\u52a0\u6743RMSE\u4ece0.0712\u964d\u81f30.0101\uff09\uff0c\u5e76\u63d0\u5347\u4e86\u89c6\u89c9\u8d28\u91cf\uff08PSNR 22.11\uff0cSSIM 0.8729\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5b9e\u65f6\u9ad8\u98ce\u9669\u5de5\u4e1a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u7a7a\u95f4\u7cbe\u5ea6\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2508.20464", "pdf": "https://arxiv.org/pdf/2508.20464", "abs": "https://arxiv.org/abs/2508.20464", "authors": ["Sanaz Motamedi", "Viktoria Marcus", "Griffin Pitts"], "title": "Human-Centered Design for Connected Automation: Predicting Pedestrian Crossing Intentions", "categories": ["cs.HC", "cs.ET", "H.5.2; H.1.2; I.6; J.4"], "comment": null, "summary": "Road traffic remains a leading cause of death worldwide, with pedestrians and\nother vulnerable road users accounting for over half of the 1.19 million annual\nfatalities, much of it due to human error. Level-5 automated driving systems\n(ADSs), capable of full self-driving without human oversight, have the\npotential to reduce these incidents. However, their effectiveness depends not\nonly on automation performance but also on their ability to communicate intent\nand coordinate safely with pedestrians in the absence of traditional driver\ncues. Understanding how pedestrians interpret and respond to ADS behavior is\ntherefore critical to the development of connected vehicle systems. This study\nextends the Theory of Planned Behavior (TPB) by incorporating four external\nfactors (i.e. safety, trust, compatibility, and understanding) to model\npedestrian decision-making in road-crossing scenarios involving level-5 ADSs.\nUsing data from an online survey (n = 212), results show that perceived\nbehavioral control, attitude, and social information significantly predict\npedestrians' crossing intentions. External factors, particularly perceived\nsafety and understanding, strongly influence these constructs. Findings provide\nactionable insights for designing external human-machine interfaces (eHMIs) and\ncooperative V2X communication strategies that support safe, transparent\ninteractions between automated vehicles and pedestrians. This work contributes\nto the development of inclusive, human-centered connected mobility systems.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6269\u5c55\u8ba1\u5212\u884c\u4e3a\u7406\u8bba\uff0c\u7eb3\u5165\u5b89\u5168\u3001\u4fe1\u4efb\u3001\u517c\u5bb9\u6027\u548c\u7406\u89e3\u56db\u4e2a\u5916\u90e8\u56e0\u7d20\uff0c\u5206\u6790\u884c\u4eba\u4e0eL5\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e92\u52a8\u65f6\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u4e3a\u8bbe\u8ba1\u4eba\u673a\u754c\u9762\u548c\u8f66\u8054\u901a\u4fe1\u7b56\u7565\u63d0\u4f9b\u4f9d\u636e\u3002", "motivation": "\u5168\u7403\u4ea4\u901a\u4e8b\u6545\u4e2d\u8d85\u8fc7\u4e00\u534a\u7684\u6b7b\u4ea1\u4eba\u6570\u4e3a\u884c\u4eba\u7b49\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff0cL5\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6709\u671b\u51cf\u5c11\u4e8b\u6545\uff0c\u4f46\u9700\u89e3\u51b3\u4e0e\u884c\u4eba\u5b89\u5168\u4e92\u52a8\u7684\u95ee\u9898\u3002", "method": "\u7814\u7a76\u6269\u5c55\u4e86\u8ba1\u5212\u884c\u4e3a\u7406\u8bba\uff08TPB\uff09\uff0c\u52a0\u5165\u56db\u4e2a\u5916\u90e8\u56e0\u7d20\uff0c\u901a\u8fc7\u5728\u7ebf\u8c03\u67e5\uff08n=212\uff09\u5206\u6790\u884c\u4eba\u51b3\u7b56\u884c\u4e3a\u3002", "result": "\u611f\u77e5\u884c\u4e3a\u63a7\u5236\u3001\u6001\u5ea6\u548c\u793e\u4f1a\u4fe1\u606f\u663e\u8457\u9884\u6d4b\u884c\u4eba\u8fc7\u8857\u610f\u56fe\uff1b\u5b89\u5168\u611f\u548c\u7406\u89e3\u529b\u5bf9\u5916\u90e8\u56e0\u7d20\u5f71\u54cd\u6700\u5927\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u884c\u4eba\u5b89\u5168\u900f\u660e\u4ea4\u4e92\u7684\u4eba\u673a\u754c\u9762\u548c\u8f66\u8054\u901a\u4fe1\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\uff0c\u63a8\u52a8\u4ee5\u4eba\u4e3a\u672c\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2508.20274", "pdf": "https://arxiv.org/pdf/2508.20274", "abs": "https://arxiv.org/abs/2508.20274", "authors": ["Erfan Darzi", "Shreeanant Bharadwaj", "Sree Bhargavi Balija"], "title": "Predictable LLM Serving on GPU Clusters", "categories": ["cs.DC"], "comment": null, "summary": "Latency-sensitive inference on shared A100 clusters often suffers\nnoisy-neighbor interference on the PCIe fabric, inflating tail latency and SLO\nviolations. We present a fabric-agnostic, VM-deployable host-level controller\nthat combines dynamic Multi-Instance GPU (MIG) reconfiguration, PCIe-aware\nplacement, and lightweight guardrails (MPS quotas, cgroup I/O). It samples\nper-tenant tails and system signals, uses topology hints to avoid PCIe hot\nspots, and gates actions with dwell/cool-down to avoid thrash. On a single host\nand a 2-node (16-GPU) cluster, SLO miss-rate is reduced by \\(\\approx\\)32\\%\n(\\(\\approx\\)1.5) and p99 latency improves \\(\\approx\\)15\\% with \\(\\leq\\)5\\%\nthroughput cost versus static MIG and naive placement; ablations show MIG and\nplacement contribute comparably. We also evaluate LLM serving with vLLM on OLMo\n2 7B Instruct: TTFT p99 improves \\(\\approx\\)10--15\\% at \\(\\leq\\)5\\% cost\nwithout changing the controller.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e3b\u673a\u7ea7\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u52a8\u6001MIG\u91cd\u65b0\u914d\u7f6e\u3001PCIe\u611f\u77e5\u653e\u7f6e\u548c\u8f7b\u91cf\u7ea7\u9632\u62a4\u63aa\u65bd\uff0c\u51cf\u5c11\u5171\u4eabA100\u96c6\u7fa4\u4e2d\u7684\u5ef6\u8fdf\u654f\u611f\u63a8\u7406\u7684\u5c3e\u90e8\u5ef6\u8fdf\u548cSLO\u8fdd\u89c4\u3002\u5b9e\u9a8c\u663e\u793a\uff0cSLO\u8fdd\u89c4\u7387\u964d\u4f4e32%\uff0cp99\u5ef6\u8fdf\u6539\u558415%\uff0c\u541e\u5410\u91cf\u635f\u5931\u22645%\u3002", "motivation": "\u9488\u5bf9\u5171\u4eabA100\u96c6\u7fa4\u4e2dPCIe\u901a\u9053\u7684\u566a\u58f0\u5e72\u6270\u5bfc\u81f4\u5c3e\u90e8\u5ef6\u8fdf\u548cSLO\u8fdd\u89c4\u7684\u95ee\u9898\uff0c\u9700\u8981\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u7ba1\u7406\u4f18\u5316\u6027\u80fd\u3002", "method": "\u91c7\u7528\u52a8\u6001MIG\u91cd\u65b0\u914d\u7f6e\u3001PCIe\u611f\u77e5\u7684\u4efb\u52a1\u653e\u7f6e\u548c\u8f7b\u91cf\u7ea7\u9632\u62a4\u63aa\u65bd\uff08\u5982MPS\u914d\u989d\u548ccgroup I/O\u63a7\u5236\uff09\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u548c\u62d3\u6251\u63d0\u793a\u907f\u514d\u70ed\u70b9\u548c\u8d44\u6e90\u4e89\u7528\u3002", "result": "\u5728\u5355\u4e3b\u673a\u548c2\u8282\u70b9\uff0816-GPU\uff09\u96c6\u7fa4\u4e2d\uff0cSLO\u8fdd\u89c4\u7387\u964d\u4f4e\u7ea632%\uff0cp99\u5ef6\u8fdf\u6539\u5584\u7ea615%\uff0c\u541e\u5410\u91cf\u635f\u5931\u22645%\u3002LLM\u670d\u52a1\uff08vLLM\uff09\u4e2dTTFT p99\u5ef6\u8fdf\u6539\u558410-15%\uff0c\u6210\u672c\u22645%\u3002", "conclusion": "\u63a7\u5236\u5668\u5728\u4e0d\u6539\u53d8\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5ef6\u8fdf\u654f\u611f\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u52a8\u6001MIG\u548c\u4efb\u52a1\u653e\u7f6e\u662f\u5173\u952e\u8d21\u732e\u56e0\u7d20\u3002"}}
{"id": "2508.17372", "pdf": "https://arxiv.org/pdf/2508.17372", "abs": "https://arxiv.org/abs/2508.17372", "authors": ["Yingjia Wang", "Ming-Chang Yang"], "title": "The Unwritten Contract of Cloud-based Elastic Solid-State Drives", "categories": ["cs.PF", "cs.ET"], "comment": "Accepted and to appear in DAC 2025", "summary": "Elastic block storage (EBS) with the storage-compute disaggregated\narchitecture stands as a pivotal piece in today's cloud. EBS furnishes users\nwith storage capabilities through the elastic solid-state drive (ESSD).\nNevertheless, despite the widespread integration into cloud services, the\nabsence of a thorough ESSD performance characterization raises critical doubt:\nwhen more and more services are shifted onto the cloud, can ESSD satisfactorily\nsubstitute the storage responsibilities of the local SSD and offer comparable\nperformance?\n  In this paper, we for the first time target this question by characterizing\ntwo ESSDs from Amazon AWS and Alibaba Cloud. We present an unwritten contract\nof cloud-based ESSDs, encapsulating four observations and five implications for\ncloud storage users. Specifically, the observations are counter-intuitive and\ncontrary to the conventional perceptions of what one would expect from the\nlocal SSD. The implications we hope could guide users in revisiting the designs\nof their deployed cloud software, i.e., harnessing the distinct characteristics\nof ESSDs for better system performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5bf9\u4e9a\u9a6c\u900aAWS\u548c\u963f\u91cc\u4e91\u7684ESSD\u6027\u80fd\u8fdb\u884c\u6d4b\u8bd5\uff0c\u63ed\u793a\u5176\u4e0e\u4f20\u7edfSSD\u7684\u4e0d\u540c\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u56db\u70b9\u89c2\u5bdf\u548c\u4e94\u70b9\u542f\u793a\uff0c\u5e2e\u52a9\u7528\u6237\u4f18\u5316\u4e91\u5b58\u50a8\u8bbe\u8ba1\u3002", "motivation": "\u5c3d\u7ba1ESSD\u5728\u4e91\u670d\u52a1\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u6027\u80fd\u5c1a\u672a\u88ab\u6df1\u5165\u8868\u5f81\uff0c\u7528\u6237\u5bf9\u5176\u662f\u5426\u80fd\u66ff\u4ee3\u672c\u5730SSD\u5b58\u7591\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5AWS\u548c\u963f\u91cc\u4e91\u7684\u4e24\u79cdESSD\uff0c\u5206\u6790\u5176\u6027\u80fd\u8868\u73b0\uff0c\u603b\u7ed3\u51fa\u56db\u70b9\u89c2\u5bdf\u548c\u4e94\u70b9\u542f\u793a\u3002", "result": "\u53d1\u73b0ESSD\u7684\u6027\u80fd\u4e0e\u4f20\u7edfSSD\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u7528\u6237\u53ef\u4ece\u8fd9\u4e9b\u7279\u6027\u4e2d\u4f18\u5316\u7cfb\u7edf\u8bbe\u8ba1\u3002", "conclusion": "ESSD\u5177\u6709\u72ec\u7279\u6027\u80fd\u7279\u5f81\uff0c\u7528\u6237\u5e94\u91cd\u65b0\u8bbe\u8ba1\u4e91\u8f6f\u4ef6\u4ee5\u5145\u5206\u5229\u7528\u5176\u4f18\u52bf\u3002"}}
{"id": "2508.20986", "pdf": "https://arxiv.org/pdf/2508.20986", "abs": "https://arxiv.org/abs/2508.20986", "authors": ["Lianpeng Qiao", "Ziqi Cao", "Kaiyu Feng", "Ye Yuan", "Guoren Wang"], "title": "Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Data has become a foundational asset driving innovation across domains such\nas finance, healthcare, and e-commerce. In these areas, predictive modeling\nover relational tables is commonly employed, with increasing emphasis on\nreducing manual effort through automated machine learning (AutoML) techniques.\nThis raises an interesting question: can feature augmentation itself be\nautomated and identify and utilize task-related relational signals?\n  To address this challenge, we propose an end-to-end automated feature\naugmentation framework, ReCoGNN, which enhances initial datasets using features\nextracted from multiple relational tables to support predictive tasks. ReCoGNN\nfirst captures semantic dependencies within each table by modeling intra-table\nattribute relationships, enabling it to partition tables into structured,\nsemantically coherent segments. It then constructs a heterogeneous weighted\ngraph that represents inter-row relationships across all segments. Finally,\nReCoGNN leverages message-passing graph neural networks to propagate\ninformation through the graph, guiding feature selection and augmenting the\noriginal dataset. Extensive experiments conducted on ten real-life and\nsynthetic datasets demonstrate that ReCoGNN consistently outperforms existing\nmethods on both classification and regression tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aReCoGNN\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u7279\u5f81\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5904\u7406\u591a\u5173\u7cfb\u8868\u4e2d\u7684\u7279\u5f81\u6765\u63d0\u5347\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u5df2\u6210\u4e3a\u63a8\u52a8\u91d1\u878d\u3001\u533b\u7597\u548c\u7535\u5b50\u5546\u52a1\u7b49\u9886\u57df\u521b\u65b0\u7684\u5173\u952e\u8d44\u4ea7\uff0c\u4f46\u5728\u5173\u7cfb\u8868\u4e0a\u8fdb\u884c\u9884\u6d4b\u5efa\u6a21\u65f6\uff0c\u5982\u4f55\u81ea\u52a8\u8bc6\u522b\u548c\u5229\u7528\u4efb\u52a1\u76f8\u5173\u7684\u5173\u8054\u4fe1\u53f7\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "ReCoGNN\u9996\u5148\u6355\u6349\u6bcf\u4e2a\u8868\u4e2d\u7684\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c06\u8868\u5212\u5206\u4e3a\u7ed3\u6784\u5316\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u6bb5\uff0c\u7136\u540e\u6784\u5efa\u4e00\u4e2a\u5f02\u8d28\u52a0\u6743\u56fe\u8868\u793a\u8de8\u6bb5\u7684\u884c\u95f4\u5173\u7cfb\uff0c\u6700\u540e\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4fe1\u606f\u4f20\u64ad\uff0c\u6307\u5bfc\u7279\u5f81\u9009\u62e9\u548c\u589e\u5f3a\u3002", "result": "\u5728\u5341\u4e2a\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cReCoGNN\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ReCoGNN\u901a\u8fc7\u81ea\u52a8\u5316\u7279\u5f81\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u5173\u7cfb\u8868\u6570\u636e\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.20671", "pdf": "https://arxiv.org/pdf/2508.20671", "abs": "https://arxiv.org/abs/2508.20671", "authors": ["Ga\u00ebtan Serr\u00e9"], "title": "Formal equivalence between global optimization consistency and random search", "categories": ["cs.FL", "cs.LO", "math.OC", "math.PR"], "comment": null, "summary": "We formalize a proof that any stochastic and iterative global optimization\nalgorithm is consistent over Lipschitz continuous functions if and only if it\nsamples the whole search space. To achieve this, we use the\nL$\\exists$$\\forall$N theorem prover and the Mathlib library. The major\nchallenge of this formalization, apart from the technical aspects of the proof\nitself, is to converge to a definition of a stochastic and iterative global\noptimization algorithm that is both general enough to encompass all algorithms\nof this type and specific enough to be used in a formal proof. We define such\nan algorithm as a pair of an initial probability measure and a sequence of\nMarkov kernels that describe the distribution of the next point sampled by the\nalgorithm given the previous points and their evaluations. We then construct a\nprobability measure on finite and infinite sequences of iterations of the\nalgorithm using the Ionescu-Tulcea theorem.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\uff0c\u4efb\u4f55\u968f\u673a\u8fed\u4ee3\u5168\u5c40\u4f18\u5316\u7b97\u6cd5\u5728Lipschitz\u8fde\u7eed\u51fd\u6570\u4e0a\u4e00\u81f4\u6536\u655b\u5f53\u4e14\u4ec5\u5f53\u5b83\u91c7\u6837\u6574\u4e2a\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u5229\u7528L\u2203\u2200N\u5b9a\u7406\u8bc1\u660e\u5668\u548cMathlib\u5e93\u5b8c\u6210\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u8fed\u4ee3\u5168\u5c40\u4f18\u5316\u7b97\u6cd5\u7684\u4e00\u81f4\u6536\u655b\u6027\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u5f62\u5f0f\u5316\u65b9\u6cd5\u9a8c\u8bc1\u5176\u7406\u8bba\u3002", "method": "\u5b9a\u4e49\u7b97\u6cd5\u4e3a\u521d\u59cb\u6982\u7387\u6d4b\u5ea6\u548c\u9a6c\u5c14\u53ef\u592b\u6838\u5e8f\u5217\u7684\u7ec4\u5408\uff0c\u5229\u7528Ionescu-Tulcea\u5b9a\u7406\u6784\u9020\u6982\u7387\u6d4b\u5ea6\u3002", "result": "\u8bc1\u660e\u7b97\u6cd5\u4e00\u81f4\u6536\u655b\u7684\u5145\u8981\u6761\u4ef6\u662f\u91c7\u6837\u6574\u4e2a\u641c\u7d22\u7a7a\u95f4\u3002", "conclusion": "\u5f62\u5f0f\u5316\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u6027\u6761\u4ef6\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.20569", "pdf": "https://arxiv.org/pdf/2508.20569", "abs": "https://arxiv.org/abs/2508.20569", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann"], "title": "Less is More - diveXplore 5.0 at VBS 2021", "categories": ["cs.MM"], "comment": null, "summary": "As a longstanding participating system in the annual Video Browser Showdown\n(VBS2017-VBS2020) as well as in two iterations of the more recently established\nLifelog Search Challenge (LSC2018-LSC2019), diveXplore is developed as a\nfeature-rich Deep Interactive Video Exploration system. After its initial\nsuccessful employment as a competitive tool at the challenges, its performance,\nhowever, declined as new features were introduced increasing its overall\ncomplexity. We mainly attribute this to the fact that many additions to the\nsystem needed to revolve around the system's core element - an interactive\nself-organizing browseable featuremap, which, as an integral component did not\naccommodate the addition of new features well. Therefore, counteracting said\nperformance decline, the VBS 2021 version constitutes a completely rebuilt\nversion 5.0, implemented from scratch with the aim of greatly reducing the\nsystem's complexity as well as keeping proven useful features in a modular\nmanner.", "AI": {"tldr": "diveXplore \u662f\u4ea4\u4e92\u5f0f\u89c6\u9891\u63a2\u7d22\u7cfb\u7edf\uff0c\u66fe\u6210\u529f\u7528\u4e8e\u7ade\u8d5b\uff0c\u4f46\u56e0\u529f\u80fd\u589e\u52a0\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u65b0\u7248\u672c5.0\u91cd\u5199\u4ee5\u7b80\u5316\u7cfb\u7edf\u5e76\u4fdd\u7559\u6838\u5fc3\u529f\u80fd\u3002", "motivation": "\u529f\u80fd\u589e\u52a0\u5bfc\u81f4\u7cfb\u7edf\u590d\u6742\u5316\uff0c\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u91cd\u65b0\u8bbe\u8ba1\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u5b8c\u5168\u91cd\u5199\u7cfb\u7edf5.0\u7248\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u4ee5\u51cf\u5c11\u590d\u6742\u6027\u3002", "result": "\u65b0\u7248\u672c\u4fdd\u7559\u4e86\u6838\u5fc3\u529f\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u7cfb\u7edf\u590d\u6742\u6027\u3002", "conclusion": "\u6a21\u5757\u5316\u91cd\u6784\u6210\u529f\u89e3\u51b3\u4e86\u6027\u80fd\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.20653", "pdf": "https://arxiv.org/pdf/2508.20653", "abs": "https://arxiv.org/abs/2508.20653", "authors": ["Alperen Bolat", "Sakir Sezer", "Kieran McLaughlin", "Henry Hui"], "title": "Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V", "categories": ["cs.AR", "cs.CR", "cs.NI"], "comment": "Extended version of IEEE ISVLSI Conference Paper", "summary": "Integrating cryptographic accelerators into modern CPU architectures presents\nunique microarchitectural challenges, particularly when extending instruction\nsets with complex and multistage operations. Hardware-assisted cryptographic\ninstructions, such as Intel's AES-NI and ARM's custom instructions for\nencryption workloads, have demonstrated substantial performance improvements.\nHowever, efficient SHA-3 acceleration remains an open problem due to its\ndistinct permutation-based structure and memory access patterns. Existing\nsolutions primarily rely on standalone coprocessors or software optimizations,\noften avoiding the complexities of direct microarchitectural integration. This\nstudy investigates the architectural challenges of embedding a SHA-3\npermutation operation as a custom instruction within a general-purpose\nprocessor, focusing on pipelined simultaneous execution, storage utilization,\nand hardware cost. In this paper, we investigated and prototyped a SHA-3 custom\ninstruction for the RISC-V CPU architecture. Using cycle-accurate GEM5\nsimulations and FPGA prototyping, our results demonstrate performance\nimprovements of up to 8.02x for RISC-V optimized SHA-3 software workloads and\nup to 46.31x for Keccak-specific software workloads, with only a 15.09%\nincrease in registers and a 11.51% increase in LUT utilization. These findings\nprovide critical insights into the feasibility and impact of SHA-3 acceleration\nat the microarchitectural level, highlighting practical design considerations\nfor future cryptographic instruction set extensions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u901a\u7528\u5904\u7406\u5668\u4e2d\u5d4c\u5165SHA-3\u52a0\u5bc6\u7b97\u6cd5\u4f5c\u4e3a\u81ea\u5b9a\u4e49\u6307\u4ee4\u7684\u5fae\u67b6\u6784\u6311\u6218\uff0c\u901a\u8fc7RISC-V\u67b6\u6784\u7684\u539f\u578b\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7531\u4e8eSHA-3\u7b97\u6cd5\u7684\u72ec\u7279\u7ed3\u6784\u548c\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u5176\u9ad8\u6548\u52a0\u901f\u4e00\u76f4\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u7814\u7a76\u901a\u8fc7\u5fae\u67b6\u6784\u96c6\u6210\u5b9e\u73b0SHA-3\u52a0\u901f\u7684\u53ef\u884c\u6027\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5728RISC-V\u67b6\u6784\u4e2d\u8bbe\u8ba1SHA-3\u81ea\u5b9a\u4e49\u6307\u4ee4\uff0c\u5229\u7528GEM5\u6a21\u62df\u548cFPGA\u539f\u578b\u9a8c\u8bc1\uff0c\u5206\u6790\u4e86\u6d41\u6c34\u7ebf\u5e76\u884c\u6267\u884c\u3001\u5b58\u50a8\u5229\u7528\u548c\u786c\u4ef6\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4f18\u5316\u540e\u7684SHA-3\u8f6f\u4ef6\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe8.02\u500d\uff0cKeccak\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u5347\u9ad8\u8fbe46.31\u500d\uff0c\u786c\u4ef6\u6210\u672c\u4ec5\u589e\u52a015.09%\u5bc4\u5b58\u5668\u548c11.51% LUT\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86SHA-3\u5728\u5fae\u67b6\u6784\u5c42\u9762\u52a0\u901f\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u52a0\u5bc6\u6307\u4ee4\u96c6\u6269\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u8bbe\u8ba1\u53c2\u8003\u3002"}}
{"id": "2508.20625", "pdf": "https://arxiv.org/pdf/2508.20625", "abs": "https://arxiv.org/abs/2508.20625", "authors": ["Mandar R. Nalavade", "Ravindra S. Tomar", "Gaurav S. Kasbekar"], "title": "Relay Selection in Wireless Networks as Restless Bandits", "categories": ["cs.NI"], "comment": null, "summary": "We consider a wireless network in which a source node needs to transmit a\nlarge file to a destination node. The direct wireless link between the source\nand the destination is assumed to be blocked. Multiple candidate relays are\navailable to forward packets from the source to the destination. A holding cost\nis incurred for each packet stored at every relay in each time slot. The\nobjective is to design a policy for selecting a relay in each time slot to\nwhich the source attempts to send a packet, so as to minimize the expected\nlong-run time-averaged total packet holding cost at the relays. This problem is\nan instance of the restless multi-armed bandit (RMAB) problem, which is\nprovably hard to solve. We prove that this relay selection problem is\nWhittle-indexable, and propose a method to compute the Whittle index of each\nrelay in every time slot. In each time slot, our relay selection policy\ntransmits a packet to the relay with the smallest Whittle index. Using\nsimulations, we show that the proposed policy outperforms the relay selection\npolicies proposed in prior work in terms of average cost, delay, as well as\nthroughput.", "AI": {"tldr": "\u7814\u7a76\u65e0\u7ebf\u7f51\u7edc\u4e2d\u6e90\u8282\u70b9\u901a\u8fc7\u591a\u4e2d\u7ee7\u4f20\u8f93\u6587\u4ef6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8eWhittle\u7d22\u5f15\u7684\u4e2d\u7ee7\u9009\u62e9\u7b56\u7565\u4ee5\u51cf\u5c11\u5b58\u50a8\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u6e90\u8282\u70b9\u4e0e\u76ee\u6807\u8282\u70b9\u4e4b\u95f4\u76f4\u63a5\u94fe\u8def\u88ab\u963b\u585e\u65f6\uff0c\u5982\u4f55\u9ad8\u6548\u9009\u62e9\u4e2d\u7ee7\u4ee5\u6700\u5c0f\u5316\u6570\u636e\u5305\u5b58\u50a8\u6210\u672c\u7684\u95ee\u9898\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3aRMAB\u95ee\u9898\uff0c\u8bc1\u660eWhittle\u7d22\u5f15\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u8ba1\u7b97\u6bcf\u65f6\u9699Whittle\u7d22\u5f15\u7684\u65b9\u6cd5\uff0c\u9009\u62e9\u7d22\u5f15\u6700\u5c0f\u7684\u4e2d\u7ee7\u3002", "result": "\u4eff\u771f\u663e\u793a\u8be5\u7b56\u7565\u5728\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8eWhittle\u7d22\u5f15\u7684\u4e2d\u7ee7\u9009\u62e9\u7b56\u7565\u80fd\u6709\u6548\u4f18\u5316\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7ee7\u4f20\u8f93\u6027\u80fd\u3002"}}
{"id": "2508.20744", "pdf": "https://arxiv.org/pdf/2508.20744", "abs": "https://arxiv.org/abs/2508.20744", "authors": ["Shabnam Hassani", "Mehrdad Sabetzadeh", "Daniel Amyot"], "title": "From Law to Gherkin: A Human-Centred Quasi-Experiment on the Quality of LLM-Generated Behavioural Specifications from Food-Safety Regulations", "categories": ["cs.SE"], "comment": null, "summary": "Context: Laws and regulations increasingly affect software design and quality\nassurance, but legal texts are written in technology-neutral language. This\ncreates challenges for engineers who must develop compliance artifacts such as\nrequirements and acceptance criteria. Manual creation is labor-intensive,\nerror-prone, and requires domain expertise. Advances in Generative AI (GenAI),\nespecially Large Language Models (LLMs), offer a way to automate deriving such\nartifacts.\n  Objective: We present the first systematic human-subject study of LLMs'\nability to derive behavioral specifications from legal texts using a\nquasi-experimental design. These specifications translate legal requirements\ninto a developer-friendly form.\n  Methods: Ten participants evaluated specifications generated from food-safety\nregulations by Claude and Llama. Using Gherkin, a structured BDD language, 60\nspecifications were produced. Each participant assessed 12 across five\ncriteria: Relevance, Clarity, Completeness, Singularity, and Time Savings. Each\nspecification was reviewed by two participants, yielding 120 assessments.\n  Results: For Relevance, 75% of ratings were highest and 20% second-highest.\nClarity reached 90% highest. Completeness: 75% highest, 19% second.\nSingularity: 82% highest, 12% second. Time Savings: 68% highest, 24% second. No\nlowest ratings occurred. Mann-Whitney U tests showed no significant differences\nacross participants or models. Llama slightly outperformed Claude in Clarity,\nCompleteness, and Time Savings, while Claude was stronger in Singularity.\nFeedback noted hallucinations and omissions but confirmed the utility of the\nspecifications.\n  Conclusion: LLMs can generate high-quality Gherkin specifications from legal\ntexts, reducing manual effort and providing structured artifacts useful for\nimplementation, assurance, and test generation.", "AI": {"tldr": "LLMs\u80fd\u9ad8\u6548\u5730\u5c06\u6cd5\u5f8b\u6587\u672c\u8f6c\u5316\u4e3a\u5f00\u53d1\u8005\u53cb\u597d\u7684\u884c\u4e3a\u89c4\u8303\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u8f93\u51fa\u3002", "motivation": "\u6cd5\u5f8b\u6587\u672c\u7684\u6280\u672f\u4e2d\u7acb\u6027\u5bfc\u81f4\u5de5\u7a0b\u5e08\u624b\u52a8\u521b\u5efa\u5408\u89c4\u6587\u6863\u8017\u65f6\u4e14\u6613\u9519\uff0cGenAI\uff08\u5982LLMs\uff09\u4e3a\u81ea\u52a8\u5316\u63d0\u4f9b\u53ef\u80fd\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u5b9e\u9a8c\u8bc4\u4f30Claude\u548cLlama\u751f\u6210\u7684Gherkin\u89c4\u8303\uff0c60\u6761\u89c4\u8303\u753110\u540d\u53c2\u4e0e\u8005\u63095\u9879\u6807\u51c6\u8bc4\u5206\u3002", "result": "\u89c4\u8303\u5728\u76f8\u5173\u6027\u3001\u6e05\u6670\u5ea6\u3001\u5b8c\u6574\u6027\u7b49\u65b9\u9762\u8bc4\u5206\u8f83\u9ad8\uff0c\u4e24\u6a21\u578b\u8868\u73b0\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u603b\u4f53\u8d28\u91cf\u4f18\u79c0\u4f46\u5b58\u5728\u5c11\u91cf\u5e7b\u89c9\u548c\u9057\u6f0f\u3002", "conclusion": "LLMs\u80fd\u9ad8\u8d28\u91cf\u751f\u6210\u6cd5\u5f8b\u89c4\u8303\u7684Gherkin\u8868\u8fbe\uff0c\u4e3a\u5f00\u53d1\u3001\u6d4b\u8bd5\u7b49\u73af\u8282\u63d0\u4f9b\u7ed3\u6784\u5316\u652f\u6301\u3002"}}
{"id": "2508.20477", "pdf": "https://arxiv.org/pdf/2508.20477", "abs": "https://arxiv.org/abs/2508.20477", "authors": ["Yibo Wang", "Yuhan Luo", "Janghee Cho", "Junnan Yu"], "title": "What is \"Spatial\" about Spatial Computing?", "categories": ["cs.HC", "H.5.1; H.5.2; H.2.8"], "comment": null, "summary": "Recent advancements in geographic information systems and mixed reality\ntechnologies have positioned spatial computing as a transformative paradigm in\ncomputational science. However, the field remains conceptually fragmented, with\ndiverse interpretations across disciplines like Human-Computer Interaction,\nGeographic Information Science, and Computer Science, which hinders a\ncomprehensive understanding of spatial computing and poses challenges for its\ncoherent advancement and interdisciplinary integration. In this paper, we trace\nthe origins and historical evolution of spatial computing and examine how\n\"spatial\" is understood, identifying two schools of thought: \"spatial\" as the\ncontextual understanding of space, where spatial data guides interaction in the\nphysical world; and \"spatial\" as a mixed space for interaction, emphasizing the\nseamless integration of physical and digital environments to enable embodied\nengagement. By synthesizing these perspectives, we propose spatial computing as\na computational paradigm that redefines the interplay between environment,\ncomputation, and human experience, offering a holistic lens to enhance its\nconceptual clarity and inspire future technological innovations that support\nmeaningful interactions with and shaping of environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7a7a\u95f4\u8ba1\u7b97\u5728\u8de8\u5b66\u79d1\u4e2d\u7684\u6982\u5ff5\u788e\u7247\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u79cd\u7a7a\u95f4\u7406\u89e3\u89c6\u89d2\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u8303\u5f0f\u4ee5\u4fc3\u8fdb\u672a\u6765\u6280\u672f\u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u7a7a\u95f4\u8ba1\u7b97\u5728\u8de8\u5b66\u79d1\u4e2d\u7684\u6982\u5ff5\u788e\u7247\u5316\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8\u5176\u8fde\u8d2f\u53d1\u5c55\u548c\u8de8\u5b66\u79d1\u6574\u5408\u3002", "method": "\u8ffd\u6eaf\u7a7a\u95f4\u8ba1\u7b97\u7684\u8d77\u6e90\u548c\u5386\u53f2\u6f14\u53d8\uff0c\u5206\u6790\u4e24\u79cd\u7a7a\u95f4\u7406\u89e3\u89c6\u89d2\uff1a\u7a7a\u95f4\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7406\u89e3\u6216\u6df7\u5408\u4ea4\u4e92\u7a7a\u95f4\u3002", "result": "\u63d0\u51fa\u7a7a\u95f4\u8ba1\u7b97\u4f5c\u4e3a\u91cd\u65b0\u5b9a\u4e49\u73af\u5883\u3001\u8ba1\u7b97\u4e0e\u4eba\u7c7b\u4f53\u9a8c\u4ea4\u4e92\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u589e\u5f3a\u6982\u5ff5\u6e05\u6670\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u89c6\u89d2\uff0c\u4fc3\u8fdb\u672a\u6765\u6280\u672f\u521b\u65b0\uff0c\u652f\u6301\u66f4\u6df1\u5165\u7684\u73af\u5883\u4ea4\u4e92\u4e0e\u5851\u9020\u3002"}}
{"id": "2508.20375", "pdf": "https://arxiv.org/pdf/2508.20375", "abs": "https://arxiv.org/abs/2508.20375", "authors": ["Guanyu Xu", "Zhiwei Hao", "Li Shen", "Yong Luo", "Fuhui Sun", "Xiaoyan Wang", "Han Hu", "Yonggang Wen"], "title": "CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference", "categories": ["cs.DC", "cs.LG", "cs.PF"], "comment": "Accepted by IEEE Transactions on Computers", "summary": "The impressive performance of transformer models has sparked the deployment\nof intelligent applications on resource-constrained edge devices. However,\nensuring high-quality service for real-time edge systems is a significant\nchallenge due to the considerable computational demands and resource\nrequirements of these models. Existing strategies typically either offload\ntransformer computations to other devices or directly deploy compressed models\non individual edge devices. These strategies, however, result in either\nconsiderable communication overhead or suboptimal trade-offs between accuracy\nand efficiency. To tackle these challenges, we propose a collaborative\ninference system for general transformer models, termed CoFormer. The central\nidea behind CoFormer is to exploit the divisibility and integrability of\ntransformer. An off-the-shelf large transformer can be decomposed into multiple\nsmaller models for distributed inference, and their intermediate results are\naggregated to generate the final output. We formulate an optimization problem\nto minimize both inference latency and accuracy degradation under heterogeneous\nhardware constraints. DeBo algorithm is proposed to first solve the\noptimization problem to derive the decomposition policy, and then progressively\ncalibrate decomposed models to restore performance. We demonstrate the\ncapability to support a wide range of transformer models on heterogeneous edge\ndevices, achieving up to 3.1$\\times$ inference speedup with large transformer\nmodels. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6\nbillion parameters on edge devices, reducing memory requirements by 76.3\\%.\nCoFormer can also reduce energy consumption by approximately 40\\% while\nmaintaining satisfactory inference performance.", "AI": {"tldr": "CoFormer\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u540c\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u5927Transformer\u6a21\u578b\u5206\u89e3\u4e3a\u5c0f\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5206\u5e03\u5f0f\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u901a\u4fe1\u5f00\u9500\u548c\u6027\u80fd\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3Transformer\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\u3002", "method": "\u5229\u7528Transformer\u7684\u53ef\u5206\u6027\u548c\u53ef\u96c6\u6210\u6027\uff0c\u5c06\u5927\u6a21\u578b\u5206\u89e3\u4e3a\u5c0f\u6a21\u578b\u8fdb\u884c\u5206\u5e03\u5f0f\u63a8\u7406\uff0c\u5e76\u901a\u8fc7DeBo\u7b97\u6cd5\u4f18\u5316\u5206\u89e3\u7b56\u7565\u548c\u6027\u80fd\u6821\u51c6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cCoFormer\u652f\u6301\u591a\u79cdTransformer\u6a21\u578b\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53473.1\u500d\uff0c\u5185\u5b58\u9700\u6c42\u51cf\u5c1176.3%\uff0c\u80fd\u8017\u964d\u4f4e40%\u3002", "conclusion": "CoFormer\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684Transformer\u6a21\u578b\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20122", "pdf": "https://arxiv.org/pdf/2508.20122", "abs": "https://arxiv.org/abs/2508.20122", "authors": ["Yi Jiang", "Malyaban Bal", "Brian Matejek", "Susmit Jha", "Adam Cobb", "Abhronil Sengupta"], "title": "Spatio-Temporal Pruning for Compressed Spiking Large Language Models", "categories": ["cs.NE", "cs.ET", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) present significant challenges for deployment in\nenergy-constrained environments due to their large model sizes and high\ninference latency. Spiking Neural Networks (SNNs), inspired by the sparse\nevent-driven neural processing and energy-efficient information transmission in\nthe brain, offer a promising alternative for achieving low-power computing.\nIntegrating the event-driven efficiency of spiking neurons with the advanced\ncapabilities of LLMs represents a promising direction for power-efficient LLMs.\nThis work specifically delves into the design of compressed spiking LLMs. Here,\nwe revisit spatial and temporal pruning from the perspective of SNNs and\npropose a novel spatio-temporal pruning framework for Spiking LLMs to optimize\ncomputational efficiency while preserving high performance. Our spatial pruning\ntechnique reduces the number of active neurons and attention heads, effectively\nlowering the computational complexity of the model. Meanwhile, temporal pruning\nminimizes inference latency by dynamically adjusting the number of timesteps\nrequired for different layers. By combining these approaches with other\ncompression techniques, we present the first work in the domain of Spiking LLMs\nto jointly explore spatial pruning, temporal pruning, extreme quantization and\nknowledge distillation strategies. Extensive experimental evaluation of our\nproposed framework for SpikingBERT on the large-scale GLUE benchmark\ndemonstrates the efficacy of our approach in terms of computational operations\nand inference latency. Our approach offers a compelling solution for real-time,\nlow-power natural language processing applications, making Spiking LLMs more\npractical for deployment on edge devices and in power-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u65f6\u7a7a\u526a\u679d\u7684\u538b\u7f29\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5927\u8bed\u8a00\u6a21\u578b\uff08Spiking LLMs\uff09\u6846\u67b6\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u80fd\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u6a21\u578b\u5927\u3001\u63a8\u7406\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u56e0\u5176\u7a00\u758f\u4e8b\u4ef6\u9a71\u52a8\u548c\u9ad8\u6548\u80fd\u7279\u6027\uff0c\u6210\u4e3a\u4f4e\u529f\u8017\u8ba1\u7b97\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u65f6\u7a7a\u526a\u679d\u6846\u67b6\uff1a\u7a7a\u95f4\u526a\u679d\u51cf\u5c11\u6d3b\u8dc3\u795e\u7ecf\u5143\u548c\u6ce8\u610f\u529b\u5934\u6570\u91cf\uff1b\u65f6\u95f4\u526a\u679d\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u5c42\u7684\u65f6\u95f4\u6b65\u6570\u3002\u7ed3\u5408\u6781\u7aef\u91cf\u5316\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728GLUE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u51fa\u7684SpikingBERT\u6846\u67b6\u5728\u8ba1\u7b97\u64cd\u4f5c\u548c\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u63a8\u52a8Spiking LLMs\u5728\u8fb9\u7f18\u8bbe\u5907\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2508.20978", "pdf": "https://arxiv.org/pdf/2508.20978", "abs": "https://arxiv.org/abs/2508.20978", "authors": ["Marianne Defresne", "Romain Gambardella", "Sophie Barbe", "Thomas Schiex"], "title": "Efficient Neuro-Symbolic Learning of Constraints and Objective", "categories": ["cs.AI", "cs.LO", "cs.SC"], "comment": null, "summary": "In the ongoing quest for hybridizing discrete reasoning with neural nets,\nthere is an increasing interest in neural architectures that can learn how to\nsolve discrete reasoning or optimization problems from natural inputs, a task\nthat Large Language Models seem to struggle with.\n  Objectives: We introduce a differentiable neuro-symbolic architecture and a\nloss function dedicated to learning how to solve NP-hard reasoning problems.\n  Methods: Our new probabilistic loss allows for learning both the constraints\nand the objective, thus delivering a complete model that can be scrutinized and\ncompleted with side constraints. By pushing the combinatorial solver out of the\ntraining loop, our architecture also offers scalable training while exact\ninference gives access to maximum accuracy.\n  Results: We empirically show that it can efficiently learn how to solve\nNP-hard reasoning problems from natural inputs. On three variants of the Sudoku\nbenchmark -- symbolic, visual, and many-solution --, our approach requires a\nfraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut\ntask, it optimizes the regret better than a Decision-Focused-Learning\nregret-dedicated loss. Finally, it efficiently learns the energy optimization\nformulation of the large real-world problem of designing proteins.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u548c\u4e13\u7528\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u5b66\u4e60\u89e3\u51b3NP\u96be\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u79bb\u6563\u63a8\u7406\u6216\u4f18\u5316\u95ee\u9898\u65f6\u7684\u56f0\u96be\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u4ece\u81ea\u7136\u8f93\u5165\u4e2d\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u65b0\u578b\u795e\u7ecf\u67b6\u6784\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u6982\u7387\u635f\u5931\u51fd\u6570\uff0c\u80fd\u591f\u5b66\u4e60\u7ea6\u675f\u548c\u76ee\u6807\u51fd\u6570\uff0c\u4ece\u800c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u5ba1\u67e5\u548c\u8865\u5145\u7684\u5b8c\u6574\u6a21\u578b\u3002\u901a\u8fc7\u5c06\u7ec4\u5408\u6c42\u89e3\u5668\u4ece\u8bad\u7ec3\u5faa\u73af\u4e2d\u79fb\u9664\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u548c\u51c6\u786e\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u4ece\u81ea\u7136\u8f93\u5165\u4e2d\u5b66\u4e60\u89e3\u51b3NP\u96be\u63a8\u7406\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982Sudoku\u548cMin-Cut/Max-Cut\u4efb\u52a1\uff09\u4ee5\u53ca\u86cb\u767d\u8d28\u8bbe\u8ba1\u7b49\u5b9e\u9645\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6df7\u5408\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u5728\u89e3\u51b3\u590d\u6742\u63a8\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u6548\u3001\u51c6\u786e\u548c\u53ef\u6269\u5c55\u7684\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.20687", "pdf": "https://arxiv.org/pdf/2508.20687", "abs": "https://arxiv.org/abs/2508.20687", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann"], "title": "diveXplore 6.0: ITEC's Interactive Video Exploration System at VBS 2022", "categories": ["cs.MM"], "comment": null, "summary": "Continuously participating since the sixth Video Browser Showdown (VBS2017),\ndiveXplore is a veteran interactive search system that throughout its lifetime\nhas offered and evaluated numerous features. After undergoing major refactoring\nfor the most recent VBS2021, however, the system since version 5.0 is less\nfeature rich, yet, more modern, leaner and faster than the original system.\nThis proved to be a sensible decision as the new system showed increasing\nperformance in VBS2021 when compared to the most recent former competitions.\nWith version 6.0 we reconsider shot segmentation, map search and introduce new\nfeatures for improving concept as well as temporal context search.", "AI": {"tldr": "diveXplore\u7cfb\u7edf\u5728VBS2021\u4e2d\u8fdb\u884c\u4e86\u91cd\u5927\u91cd\u6784\uff0c\u867d\u529f\u80fd\u51cf\u5c11\u4f46\u53d8\u5f97\u66f4\u73b0\u4ee3\u3001\u8f7b\u91cf\u4e14\u5feb\u901f\uff0c\u6027\u80fd\u63d0\u5347\u3002\u7248\u672c6.0\u5f15\u5165\u65b0\u7279\u6027\u6539\u8fdb\u6982\u5ff5\u548c\u65f6\u5e8f\u4e0a\u4e0b\u6587\u641c\u7d22\u3002", "motivation": "\u901a\u8fc7\u5bf9\u7cfb\u7edf\u8fdb\u884c\u91cd\u6784\u548c\u4f18\u5316\uff0c\u63d0\u5347\u5176\u5728\u89c6\u9891\u68c0\u7d22\u7ade\u8d5b\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u7cfb\u7edf\u8fdb\u884c\u4e86\u91cd\u5927\u91cd\u6784\uff0c\u51cf\u5c11\u529f\u80fd\u4f46\u63d0\u5347\u4e86\u73b0\u4ee3\u5316\u7a0b\u5ea6\u548c\u901f\u5ea6\uff1b\u7248\u672c6.0\u91cd\u65b0\u8003\u8651\u4e86\u955c\u5934\u5206\u5272\u3001\u5730\u56fe\u641c\u7d22\uff0c\u5e76\u5f15\u5165\u65b0\u7279\u6027\u6539\u8fdb\u6982\u5ff5\u53ca\u65f6\u5e8f\u4e0a\u4e0b\u6587\u641c\u7d22\u3002", "result": "\u91cd\u6784\u540e\u7684\u7cfb\u7edf\u5728VBS2021\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "\u51cf\u5c11\u529f\u80fd\u5e76\u4f18\u5316\u7cfb\u7edf\u8bbe\u8ba1\u662f\u63d0\u5347\u6027\u80fd\u7684\u6709\u6548\u7b56\u7565\uff0c\u7248\u672c6.0\u7684\u8fdb\u4e00\u6b65\u6539\u8fdb\u6709\u671b\u8fdb\u4e00\u6b65\u63d0\u5347\u641c\u7d22\u80fd\u529b\u3002"}}
{"id": "2508.20957", "pdf": "https://arxiv.org/pdf/2508.20957", "abs": "https://arxiv.org/abs/2508.20957", "authors": ["Faisal Ahmed", "Suresh Subramaniam", "Motoharu Matsuura", "Hiroshi Hasegawa", "Shih-Chun Lin"], "title": "Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks", "categories": ["cs.NI"], "comment": null, "summary": "The growing demand for services and the rapid deployment of virtualized\nnetwork functions (VNFs) pose significant challenges for achieving low-latency\nand energy-efficient orchestration in modern edge-core network infrastructures.\nTo address these challenges, this study proposes a Digital Twin (DT)-empowered\nDeep Reinforcement Learning framework for intelligent VNF migration that\njointly minimizes average end-to-end (E2E) delay and energy consumption. By\nformulating the VNF migration problem as a Markov Decision Process and\nutilizing the Advantage Actor-Critic model, the proposed framework enables\nadaptive and real-time migration decisions. A key innovation of the proposed\nframework is the integration of a DT module composed of a multi-task\nVariational Autoencoder and a multi-task Long Short-Term Memory network. This\ncombination collectively simulates environment dynamics and generates\nhigh-quality synthetic experiences, significantly enhancing training efficiency\nand accelerating policy convergence. Simulation results demonstrate substantial\nperformance gains, such as significant reductions in both average E2E delay and\nenergy consumption, thereby establishing new benchmarks for intelligent VNF\nmigration in edge-core networks.", "AI": {"tldr": "\u9488\u5bf9\u8fb9\u7f18\u6838\u5fc3\u7f51\u7edc\u4e2d\u4f4e\u5ef6\u8fdf\u548c\u8282\u80fd\u7f16\u6392\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fdVNF\u8fc1\u79fb\u6846\u67b6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u7aef\u5230\u7aef\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "motivation": "\u73b0\u4ee3\u8fb9\u7f18\u6838\u5fc3\u7f51\u7edc\u4e2d\uff0c\u670d\u52a1\u9700\u6c42\u7684\u589e\u957f\u548c\u865a\u62df\u5316\u7f51\u7edc\u529f\u80fd\uff08VNF\uff09\u7684\u5feb\u901f\u90e8\u7f72\u8981\u6c42\u4f4e\u5ef6\u8fdf\u548c\u8282\u80fd\u7684\u7f16\u6392\u65b9\u6848\u3002", "method": "\u5c06VNF\u8fc1\u79fb\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5229\u7528Advantage Actor-Critic\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u8fc1\u79fb\u51b3\u7b56\uff0c\u5e76\u96c6\u6210\u591a\u4efb\u52a1\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548cLSTM\u7f51\u7edc\u7684DT\u6a21\u5757\u6a21\u62df\u73af\u5883\u52a8\u6001\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u7aef\u5230\u7aef\u5ef6\u8fdf\u548c\u80fd\u6e90\u6d88\u8017\uff0c\u4e3a\u667a\u80fdVNF\u8fc1\u79fb\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DT\u8d4b\u80fd\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u4f18\u5316\u4e86VNF\u8fc1\u79fb\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7f51\u7edc\u7f16\u6392\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20774", "pdf": "https://arxiv.org/pdf/2508.20774", "abs": "https://arxiv.org/abs/2508.20774", "authors": ["Markus Funke", "Patricia Lago"], "title": "Towards an Architectural Perspective for Sustainability: Bundle the Needs from Industry", "categories": ["cs.SE"], "comment": null, "summary": "Sustainability is increasingly recognized as an emerging quality property in\nsoftware-intensive systems, yet architects lack structured guidance to address\nit effectively throughout the software design phase. Architectural\nperspectives-an architectural knowledge artifact composed of concerns,\nactivities, tactics, pitfalls, and checklists-offer a promising approach to\ntackle such emerging quality properties across architectural views and are also\nindependent of architecture frameworks and industry contexts. In this paper, we\npresent a sustainability perspective vision, i.e., a revised notion of\narchitectural perspective meant to be filled with its own elements to target\nsustainability concerns. We formulate our sustainability perspective vision\nthrough evidence from applying snowballing to seminal literature and from\nconducting a focus group with experts in the field. Our findings confirm the\nrelevance of the different perspective elements in practice and highlight\nimplications for shaping a sustainability perspective that meets industrial\nneeds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53ef\u6301\u7eed\u6027\u7684\u8f6f\u4ef6\u67b6\u6784\u89c6\u89d2\uff0c\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u4e13\u5bb6\u8ba8\u8bba\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u53ef\u6301\u7eed\u6027\u4f5c\u4e3a\u8f6f\u4ef6\u5bc6\u96c6\u578b\u7cfb\u7edf\u7684\u65b0\u5174\u8d28\u91cf\u5c5e\u6027\uff0c\u7f3a\u4e4f\u7ed3\u6784\u5316\u6307\u5bfc\uff0c\u9700\u8981\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\u6765\u89e3\u51b3\u3002", "method": "\u7ed3\u5408\u6587\u732e\u7efc\u8ff0\uff08\u96ea\u7403\u6cd5\uff09\u548c\u4e13\u5bb6\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u53ef\u6301\u7eed\u6027\u89c6\u89d2\u7684\u6982\u5ff5\u3002", "result": "\u9a8c\u8bc1\u4e86\u53ef\u6301\u7eed\u6027\u89c6\u89d2\u5404\u5143\u7d20\u5728\u5b9e\u9645\u4e2d\u7684\u76f8\u5173\u6027\uff0c\u5e76\u660e\u786e\u4e86\u6ee1\u8db3\u5de5\u4e1a\u9700\u6c42\u7684\u5f62\u6001\u3002", "conclusion": "\u53ef\u6301\u7eed\u6027\u89c6\u89d2\u4e3a\u8f6f\u4ef6\u8bbe\u8ba1\u9636\u6bb5\u7684\u53ef\u6301\u7eed\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20522", "pdf": "https://arxiv.org/pdf/2508.20522", "abs": "https://arxiv.org/abs/2508.20522", "authors": ["Abdul Rehman", "Ilona Heldal", "Jerry Chun-Wei Lin"], "title": "VisiTrail: A Cognitive Visualization Tool for Time-Series Analysis of Eye Tracking Data from Attention Game", "categories": ["cs.HC"], "comment": null, "summary": "Eye Tracking (ET) can help to understand visual attention and cognitive\nprocesses in interactive environments. In attention tasks, distinguishing\nbetween relevant target objects and distractors is crucial for effective\nperformance, yet the underlying gaze patterns that drive successful task\ncompletion remain incompletely understood. Traditional gaze analyses lack\ncomprehensive insights into the temporal dynamics of attention allocation and\nthe relationship between gaze behavior and task performance. When applied to\ncomplex visual search scenarios, current gaze analysis methods face several\nlimitations, including the isolation of measurements, visual stability, search\nefficiency, and the decision-making processes involved in these scenarios. This\npaper proposes an analysis tool that considers time series for eye tracking\ndata from task performance and also gaze measures (fixations, saccades and\nsmooth pursuit); temporal pattern analysis that reveals how attention evolves\nthroughout task performance; object-click sequence tracking that directly links\nvisual attention to user actions; and performance metrics that quantify both\naccuracy and efficiency. This tool provides comprehensive visualization\ntechniques that make complex patterns of stimuli and gaze connections\ninterpretable.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u773c\u52a8\u8ffd\u8e2a\u5206\u6790\u5de5\u5177\uff0c\u7ed3\u5408\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u591a\u79cd\u6ce8\u89c6\u884c\u4e3a\u6307\u6807\uff0c\u4ee5\u63ed\u793a\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u6ce8\u610f\u529b\u52a8\u6001\u548c\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u773c\u52a8\u5206\u6790\u65b9\u6cd5\u5728\u590d\u6742\u89c6\u89c9\u641c\u7d22\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u65e0\u6cd5\u5168\u9762\u5206\u6790\u6ce8\u610f\u529b\u5206\u914d\u7684\u52a8\u6001\u6027\u53ca\u5176\u4e0e\u4efb\u52a1\u8868\u73b0\u7684\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u5de5\u5177\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u7684\u5de5\u5177\u5305\u62ec\u65f6\u95f4\u5e8f\u5217\u773c\u52a8\u6570\u636e\u5206\u6790\u3001\u6ce8\u89c6\u884c\u4e3a\u6d4b\u91cf\uff08\u5982\u6ce8\u89c6\u70b9\u3001\u626b\u89c6\u548c\u5e73\u6ed1\u8ffd\u8e2a\uff09\u3001\u65f6\u95f4\u6a21\u5f0f\u5206\u6790\u3001\u5bf9\u8c61\u70b9\u51fb\u5e8f\u5217\u8ddf\u8e2a\uff0c\u4ee5\u53ca\u91cf\u5316\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u6027\u80fd\u6307\u6807\u3002", "result": "\u8be5\u5de5\u5177\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53ef\u89c6\u5316\u6280\u672f\uff0c\u4f7f\u590d\u6742\u7684\u523a\u6fc0\u4e0e\u6ce8\u89c6\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\u66f4\u6613\u4e8e\u89e3\u91ca\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u5de5\u5177\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u6ce8\u610f\u529b\u52a8\u6001\uff0c\u5e76\u4e3a\u773c\u52a8\u6570\u636e\u63d0\u4f9b\u4e86\u66f4\u7efc\u5408\u7684\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2508.20403", "pdf": "https://arxiv.org/pdf/2508.20403", "abs": "https://arxiv.org/abs/2508.20403", "authors": ["Tiancheng Zhao", "Zekun Yin", "Huihai An", "Xiaoyu Yang", "Zhou Jin", "Jiasi Shen", "Helen Xu"], "title": "pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification", "categories": ["cs.DC"], "comment": null, "summary": "Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or\nsparsifier, whose Laplacian matrix closely approximates the spectral properties\nof the original graph, enabling substantial reductions in computational\ncomplexity for computationally intensive problems in scientific computing. The\nstate-of-the-art method for efficient GSS is feGRASS, consisting of two steps:\n1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS\nsuffers from two main issues: 1) difficulties in parallelizing the recovery\nstep for strict data dependencies, and 2) performance degradation on skewed\ninputs, often requiring multiple passes to recover sufficient edges. To address\nthese challenges, we propose parallel density-aware Graph Spectral\nSparsification (pdGRASS), a parallel algorithm that organizes edges into\ndisjoint subtasks without data dependencies between them, enabling efficient\nparallelization and sufficient edge recovery in a single pass. We empirically\nevaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2)\nsparsifier quality, measured by the iteration count required for convergence in\na preconditioned conjugate gradient (PCG) application. The evaluation\ndemonstrates that, depending on the number of edges recovered, pdGRASS achieves\naverage speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show\nbetween 1.2x higher and 1.8x lower PCG iteration counts, with further\nimprovements as more edges are recovered. Additionally, pdGRASS mitigates the\nworst-case runtimes of feGRASS with over 1000x speedup. These results highlight\npdGRASS's significant improvements in scalability and performance for the graph\nspectral sparsification problem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5e76\u884c\u5bc6\u5ea6\u611f\u77e5\u56fe\u8c31\u7a00\u758f\u5316\uff08pdGRASS\uff09\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709feGRASS\u65b9\u6cd5\u5728\u5e76\u884c\u5316\u548c\u6570\u636e\u503e\u659c\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u7a00\u758f\u5316\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709feGRASS\u65b9\u6cd5\u5728\u5e76\u884c\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e14\u5bf9\u6570\u636e\u503e\u659c\u95ee\u9898\u5904\u7406\u4e0d\u4f73\uff0c\u9700\u8981\u591a\u6b21\u626b\u63cf\u4ee5\u6062\u590d\u8db3\u591f\u7684\u8fb9\u3002\u56e0\u6b64\uff0c\u63d0\u51fapdGRASS\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u8fb9\u7ec4\u7ec7\u6210\u65e0\u6570\u636e\u4f9d\u8d56\u7684\u72ec\u7acb\u5b50\u4efb\u52a1\uff0c\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u5316\u548c\u5355\u6b21\u626b\u63cf\u5373\u53ef\u6062\u590d\u8db3\u591f\u8fb9\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cpdGRASS\u5728\u8ba1\u7b97\u901f\u5ea6\u4e0a\u6bd4feGRASS\u5feb3.9x\u81f38.8x\uff0c\u7a00\u758f\u5316\u8d28\u91cf\u5728PCG\u8fed\u4ee3\u6b21\u6570\u4e0a\u4e5f\u6709\u663e\u8457\u4f18\u5316\u3002", "conclusion": "pdGRASS\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8c31\u7a00\u758f\u5316\u95ee\u9898\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5e76\u884c\u5316\u548c\u5904\u7406\u6570\u636e\u503e\u659c\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.20134", "pdf": "https://arxiv.org/pdf/2508.20134", "abs": "https://arxiv.org/abs/2508.20134", "authors": ["Zhenxiao Fu", "Fan Chen", "Lei Jiang"], "title": "QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming", "categories": ["cs.AI", "cs.ET", "quant-ph"], "comment": null, "summary": "Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early\nquantum advantages on classically intractable problems, spanning physics\nsimulations to Gaussian boson sampling. Yet, realizing these benefits remains\nchallenging for non-experts, primarily due to the complexities of programming\nin Open Quantum Assembly Language (OpenQASM). Although Large Language Model\n(LLM)-based agents have shown promise in automating classical programming\nworkflows, their quantum counterparts have largely been restricted to\nspecialized tasks such as quantum chemistry or error correction. In this paper,\nwe present QAgent, an LLM-powered multi-agent system that fully automates\nOpenQASM programming. By integrating task planning, in-context few-shot\nlearning, retrieval-augmented generation (RAG) for long-term context,\npredefined generation tools, and chain-of-thought (CoT) reasoning, the agents\nsystematically improve both compilation and functional correctness. Our\nevaluations demonstrate substantial improvements: across multiple LLMs of\nvarying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\\%\ncompared to previous static LLM-based approaches. We envision this multi-agent\nsystem as a key enabler for democratizing quantum programming, bridging\nexpertise gaps, and accelerating the practical adoption of quantum computing.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u7f16\u7a0b\u81ea\u52a8\u5316\u7cfb\u7edfQAgent\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u63d0\u5347OpenQASM\u7f16\u7a0b\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u975e\u4e13\u5bb6\u5728NISQ\u8bbe\u5907\u4e0a\u7f16\u7a0b\u7684\u590d\u6742\u6027\uff0c\u5229\u7528LLM\u81ea\u52a8\u5316\u91cf\u5b50\u7f16\u7a0b\u6d41\u7a0b\u3002", "method": "\u7ed3\u5408\u4efb\u52a1\u89c4\u5212\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001RAG\u3001\u9884\u5b9a\u4e49\u5de5\u5177\u548cCoT\u63a8\u7406\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u3002", "result": "QAgent\u5c06QASM\u4ee3\u7801\u751f\u6210\u51c6\u786e\u6027\u63d0\u534771.6%\uff0c\u4f18\u4e8e\u73b0\u6709\u9759\u6001LLM\u65b9\u6cd5\u3002", "conclusion": "QAgent\u6709\u671b\u63a8\u52a8\u91cf\u5b50\u7f16\u7a0b\u666e\u53ca\uff0c\u52a0\u901f\u91cf\u5b50\u8ba1\u7b97\u7684\u5b9e\u7528\u5316\u3002"}}
{"id": "2508.20741", "pdf": "https://arxiv.org/pdf/2508.20741", "abs": "https://arxiv.org/abs/2508.20741", "authors": ["Chenhao Zhang", "Wei Gao"], "title": "AdaDPCC: Adaptive Rate Control and Rate-Distortion-Complexity Optimization for Dynamic Point Cloud Compression", "categories": ["cs.MM"], "comment": null, "summary": "Dynamic point cloud compression (DPCC) is crucial in applications like\nautonomous driving and AR/VR. Current compression methods face challenges with\ncomplexity management and rate control. This paper introduces a novel dynamic\ncoding framework that supports variable bitrate and computational complexities.\nOur approach includes a slimmable framework with multiple coding routes,\nallowing for efficient Rate-Distortion-Complexity Optimization (RDCO) within a\nsingle model. To address data sparsity in inter-frame prediction, we propose\nthe coarse-to-fine motion estimation and compensation module that deconstructs\ngeometric information while expanding the perceptive field. Additionally, we\npropose a precise rate control module that content-adaptively navigates point\ncloud frames through various coding routes to meet target bitrates. The\nexperimental results demonstrate that our approach reduces the average BD-Rate\nby 5.81% and improves the BD-PSNR by 0.42 dB compared to the state-of-the-art\nmethod, while keeping the average bitrate error at 0.40%. Moreover, the average\ncoding time is reduced by up to 44.6% compared to D-DPCC, underscoring its\nefficiency in real-time and bitrate-constrained DPCC scenarios. Our code is\navailable at https://git.openi.org.cn/OpenPointCloud/Ada_DPCC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u52a8\u6001\u70b9\u4e91\u538b\u7f29\u6846\u67b6\uff0c\u652f\u6301\u53ef\u53d8\u6bd4\u7279\u7387\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u8fd0\u52a8\u4f30\u8ba1\u548c\u7387\u63a7\u5236\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6548\u7387\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u52a8\u6001\u70b9\u4e91\u538b\u7f29\u5728\u81ea\u52a8\u9a7e\u9a76\u548cAR/VR\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u5ea6\u548c\u7387\u63a7\u5236\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u91c7\u7528\u53ef\u8c03\u6574\u6846\u67b6\u548c\u591a\u7f16\u7801\u8def\u5f84\uff0c\u7ed3\u5408\u7c97\u5230\u7ec6\u8fd0\u52a8\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u7387\u63a7\u5236\u6a21\u5757\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u7387-\u5931\u771f-\u590d\u6742\u5ea6\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cBD-Rate\u964d\u4f4e5.81%\uff0cBD-PSNR\u63d0\u53470.42 dB\uff0c\u5e73\u5747\u6bd4\u7279\u7387\u8bef\u5dee\u4e3a0.40%\uff0c\u7f16\u7801\u65f6\u95f4\u51cf\u5c1144.6%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u548c\u6bd4\u7279\u7387\u53d7\u9650\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u52a8\u6001\u70b9\u4e91\u538b\u7f29\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20985", "pdf": "https://arxiv.org/pdf/2508.20985", "abs": "https://arxiv.org/abs/2508.20985", "authors": ["Douglas Liao", "Jiping Luo", "Jens Vevstad", "Nikolaos Pappas"], "title": "RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN", "categories": ["cs.NI"], "comment": "Accepted for presentation in the 2025 IEEE Conference on Standards\n  for Communications and Networking (CSCN)", "summary": "Radio Access Network (RAN) systems are inherently complex, requiring\ncontinuous monitoring to prevent performance degradation and ensure optimal\nuser experience. The RAN leverages numerous key performance indicators (KPIs)\nto evaluate system performance, generating vast amounts of data each second.\nThis immense data volume can make troubleshooting and accurate diagnosis of\nperformance anomalies more difficult. Furthermore, the highly dynamic nature of\nRAN performance demands adaptive methodologies capable of capturing temporal\ndependencies to detect anomalies reliably. In response to these challenges, we\nintroduce \\textbf{RANGAN}, an anomaly detection framework that integrates a\nGenerative Adversarial Network (GAN) with a transformer architecture. To\nenhance the capability of capturing temporal dependencies within the data,\nRANGAN employs a sliding window approach during data preprocessing. We\nrigorously evaluated RANGAN using the publicly available RAN performance\ndataset from the Spotlight project \\cite{sun-2024}. Experimental results\ndemonstrate that RANGAN achieves promising detection accuracy, notably\nattaining an F1-score of up to $83\\%$ in identifying network contention issues.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GAN\u548cTransformer\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6RANGAN\uff0c\u7528\u4e8e\u89e3\u51b3RAN\u7cfb\u7edf\u4e2d\u6027\u80fd\u5f02\u5e38\u7684\u68c0\u6d4b\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u73b0\u826f\u597d\u3002", "motivation": "RAN\u7cfb\u7edf\u590d\u6742\u4e14\u6570\u636e\u91cf\u5927\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u8bca\u65ad\u6027\u80fd\u5f02\u5e38\uff0c\u9700\u8981\u81ea\u9002\u5e94\u65b9\u6cd5\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "method": "RANGAN\u7ed3\u5408GAN\u548cTransformer\uff0c\u91c7\u7528\u6ed1\u52a8\u7a97\u53e3\u9884\u5904\u7406\u6570\u636e\u4ee5\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u3002", "result": "RANGAN\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u523083%\u7684F1\u5206\u6570\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u7f51\u7edc\u7ade\u4e89\u95ee\u9898\u3002", "conclusion": "RANGAN\u662f\u4e00\u79cd\u9ad8\u6548\u7684RAN\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.20902", "pdf": "https://arxiv.org/pdf/2508.20902", "abs": "https://arxiv.org/abs/2508.20902", "authors": ["Baharin A. Jodat", "Khouloud Gaaloul", "Mehrdad Sabetzadeh", "Shiva Nejati"], "title": "Automated Test Oracles for Flaky Cyber-Physical System Simulators: Approach and Evaluation", "categories": ["cs.SE"], "comment": null, "summary": "Simulation-based testing of cyber-physical systems (CPS) is costly due to the\ntime-consuming execution of CPS simulators. In addition, CPS simulators may be\nflaky, leading to inconsistent test outcomes and requiring repeated test\nre-execution for reliable test verdicts. Automated test oracles that do not\nrequire system execution are therefore crucial for reducing testing costs.\nIdeally, such test oracles should be interpretable to facilitate human\nunderstanding of test verdicts, and they must be robust against the potential\nflakiness of CPS simulators. In this article, we propose assertion-based test\noracles for CPS as sets of logical and arithmetic predicates defined over the\ninputs of the system under test. Given a test input, our assertion-based test\noracle determines, without requiring test execution, whether the test passes,\nfails, or if the oracle is inconclusive in predicting a verdict. We describe\ntwo methods for generating assertion-based test oracles: one using genetic\nprogramming~(GP) that employs well-known spectrum-based fault localization\n(SBFL) ranking formulas, namely Ochiai, Tarantula, and Naish, as fitness\nfunctions; and the other using decision trees (DT) and decision rules (DR). We\nevaluate our assertion-based test oracles through case studies in the domains\nof aerospace, networking and autonomous driving. We show that test oracles\ngenerated using GP with Ochiai are significantly more accurate than those\nobtained using GP with Tarantula and Naish or using DT or DR. Moreover, this\naccuracy advantage remains even when accounting for the flakiness of the system\nunder test. We further show that the assertion-based test oracles generated by\nGP with Ochiai are robust against flakiness with only 4% average variation in\ntheir accuracy results across four different network and autonomous driving\nsystems with flaky behaviours.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65ad\u8a00\u7684\u65b9\u6cd5\u6765\u751f\u6210CPS\u7684\u6d4b\u8bd5oracle\uff0c\u80fd\u591f\u5728\u4e0d\u6267\u884c\u6d4b\u8bd5\u7684\u60c5\u51b5\u4e0b\u5224\u65ad\u6d4b\u8bd5\u7ed3\u679c\uff0c\u4f7f\u7528\u9057\u4f20\u7f16\u7a0b\uff08GP\uff09\u548c\u51b3\u7b56\u6811\uff08DT\uff09\u751f\u6210oracle\uff0c\u5176\u4e2dGP\u7ed3\u5408Ochiai\u65b9\u6cd5\u6548\u679c\u6700\u4f73\u3002", "motivation": "CPS\u6a21\u62df\u6d4b\u8bd5\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u5b58\u5728\u4e0d\u53ef\u9760\u6027\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u7cfb\u7edf\u6267\u884c\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5oracle\u6765\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u903b\u8f91\u548c\u7b97\u672f\u8c13\u8bcd\u7684\u65ad\u8a00oracle\uff0c\u5229\u7528\u9057\u4f20\u7f16\u7a0b\uff08GP\uff09\u7ed3\u5408\u9891\u8c31\u6545\u969c\u5b9a\u4f4d\uff08SBFL\uff09\u516c\u5f0f\uff08\u5982Ochiai\u3001Tarantula\u3001Naish\uff09\u548c\u51b3\u7b56\u6811\uff08DT\uff09\u53ca\u51b3\u7b56\u89c4\u5219\uff08DR\uff09\u751f\u6210oracle\u3002", "result": "GP\u7ed3\u5408Ochiai\u751f\u6210\u7684oracle\u51c6\u786e\u6027\u663e\u8457\u9ad8\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4e14\u5728\u7cfb\u7edf\u5b58\u5728\u4e0d\u53ef\u9760\u884c\u4e3a\u65f6\u4ecd\u4fdd\u6301\u7a33\u5065\uff08\u51c6\u786e\u7387\u6ce2\u52a8\u4ec54%\uff09\u3002", "conclusion": "\u57fa\u4e8eGP\u548cOchiai\u7684\u65ad\u8a00oracle\u5728\u51cf\u5c11\u6d4b\u8bd5\u6210\u672c\u548c\u63d0\u9ad8\u7ed3\u679c\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u822a\u7a7a\u822a\u5929\u3001\u7f51\u7edc\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u3002"}}
{"id": "2508.20585", "pdf": "https://arxiv.org/pdf/2508.20585", "abs": "https://arxiv.org/abs/2508.20585", "authors": ["Seokho Jin", "Manseo Kim", "Sungho Byun", "Hansol Kim", "Jungmin Lee", "Sujeong Baek", "Semi Kim", "Sanghum Park", "Sung Park"], "title": "Persode: Personalized Visual Journaling with Episodic Memory-Aware AI Agent", "categories": ["cs.HC"], "comment": null, "summary": "Reflective journaling often lacks personalization and fails to engage\nGeneration Alpha and Z, who prefer visually immersive and fast-paced\ninteractions over traditional text-heavy methods. Visual storytelling enhances\nemotional recall and offers an engaging way to process personal expe- riences.\nDesigned with these digital-native generations in mind, this paper introduces\nPersode, a journaling system that integrates personalized onboarding,\nmemory-aware conversational agents, and automated visual storytelling. Persode\ncaptures user demographics and stylistic preferences through a tailored\nonboarding process, ensuring outputs resonate with individual identities. Using\na Retrieval-Augmented Generation (RAG) framework, it prioritizes emotionally\nsignificant memories to provide meaningful, context-rich interactions.\nAdditionally, Persode dynamically transforms user experiences into visually\nengaging narratives by generating prompts for advanced text-to-image models,\nadapting characters, backgrounds, and styles to user preferences. By addressing\nthe need for personalization, visual engagement, and responsiveness, Persode\nbridges the gap between traditional journaling and the evolving preferences of\nGen Alpha and Z.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aPersode\u7684\u4e2a\u6027\u5316\u65e5\u8bb0\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u53d9\u4e8b\u548c\u8bb0\u5fc6\u611f\u77e5\u5bf9\u8bdd\u4ee3\u7406\uff0c\u4ee5\u5438\u5f15\u65b0\u4e00\u4ee3\u7528\u6237\u3002", "motivation": "\u9488\u5bf9Alpha\u548cZ\u4e16\u4ee3\u5bf9\u4f20\u7edf\u6587\u672c\u65e5\u8bb0\u4e0d\u611f\u5174\u8da3\u7684\u95ee\u9898\uff0cPersode\u65e8\u5728\u63d0\u4f9b\u66f4\u5177\u89c6\u89c9\u6c89\u6d78\u611f\u548c\u4e2a\u6027\u5316\u7684\u65e5\u8bb0\u4f53\u9a8c\u3002", "method": "\u7cfb\u7edf\u91c7\u7528Retrieval-Augmented Generation (RAG)\u6846\u67b6\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u5f15\u5bfc\u6d41\u7a0b\u548c\u6587\u672c\u5230\u56fe\u50cf\u7684\u751f\u6210\u6280\u672f\u3002", "result": "Persode\u80fd\u591f\u52a8\u6001\u751f\u6210\u89c6\u89c9\u53d9\u4e8b\uff0c\u589e\u5f3a\u60c5\u611f\u56de\u5fc6\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u4e0e\u5176\u8eab\u4efd\u548c\u504f\u597d\u76f8\u7b26\u7684\u65e5\u8bb0\u4f53\u9a8c\u3002", "conclusion": "Persode\u901a\u8fc7\u89c6\u89c9\u5316\u548c\u4e2a\u6027\u5316\u8bbe\u8ba1\uff0c\u6210\u529f\u5438\u5f15\u4e86Gen Alpha\u548cZ\u7528\u6237\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u65e5\u8bb0\u4e0e\u73b0\u4ee3\u504f\u597d\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2508.20508", "pdf": "https://arxiv.org/pdf/2508.20508", "abs": "https://arxiv.org/abs/2508.20508", "authors": ["Yilin Li", "Song Han", "Sibo Wang", "Ming Wang", "Renzi Meng"], "title": "Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems", "categories": ["cs.DC"], "comment": null, "summary": "This paper proposes an intelligent service optimization method based on a\nmulti-agent collaborative evolution mechanism to address governance challenges\nin large-scale microservice architectures. These challenges include complex\nservice dependencies, dynamic topology structures, and fluctuating workloads.\nThe method models each service as an agent and introduces graph representation\nlearning to construct a service dependency graph. This enables agents to\nperceive and embed structural changes within the system. Each agent learns its\npolicy based on a Markov Decision Process. A centralized training and\ndecentralized execution framework is used to integrate local autonomy with\nglobal coordination. To enhance overall system performance and adaptability, a\ngame-driven policy optimization mechanism is designed. Through a\nselection-mutation process, agent strategy distributions are dynamically\nadjusted. This supports adaptive collaboration and behavioral evolution among\nservices. Under this mechanism, the system can quickly respond and achieve\nstable policy convergence when facing scenarios such as sudden workload spikes,\ntopology reconfigurations, or resource conflicts. To evaluate the effectiveness\nof the proposed method, experiments are conducted on a representative\nmicroservice simulation platform. Comparative analyses are performed against\nseveral advanced approaches, focusing on coordination efficiency, adaptability,\nand policy convergence performance. Experimental results show that the proposed\nmethod outperforms others in several key metrics. It significantly improves\ngovernance efficiency and operational stability in large-scale microservice\nsystems. The method demonstrates strong practical value and engineering\nfeasibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u540c\u8fdb\u5316\u673a\u5236\u7684\u670d\u52a1\u4f18\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7684\u6cbb\u7406\u6311\u6218\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u5b66\u4e60\u548c\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u7684\u590d\u6742\u670d\u52a1\u4f9d\u8d56\u3001\u52a8\u6001\u62d3\u6251\u7ed3\u6784\u548c\u6ce2\u52a8\u8d1f\u8f7d\u7b49\u6cbb\u7406\u6311\u6218\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u670d\u52a1\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06\u6bcf\u4e2a\u670d\u52a1\u5efa\u6a21\u4e3a\u667a\u80fd\u4f53\uff0c\u7ed3\u5408\u56fe\u8868\u793a\u5b66\u4e60\u6784\u5efa\u670d\u52a1\u4f9d\u8d56\u56fe\uff0c\u91c7\u7528\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u535a\u5f08\u9a71\u52a8\u7684\u7b56\u7565\u4f18\u5316\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u534f\u8c03\u6548\u7387\u3001\u9002\u5e94\u6027\u548c\u7b56\u7565\u6536\u655b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cbb\u7406\u6548\u7387\u548c\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u548c\u5de5\u7a0b\u53ef\u884c\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u6cbb\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20181", "pdf": "https://arxiv.org/pdf/2508.20181", "abs": "https://arxiv.org/abs/2508.20181", "authors": ["Alberto Compagnoni", "Davide Caffagni", "Nicholas Moratelli", "Lorenzo Baraldi", "Marcella Cornia", "Rita Cucchiara"], "title": "Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "BMVC 2025", "summary": "Multimodal Large Language Models (MLLMs) emerge as a unified interface to\naddress a multitude of tasks, ranging from NLP to computer vision. Despite\nshowcasing state-of-the-art results in many benchmarks, a long-standing issue\nis the tendency of MLLMs to hallucinate, that is to generate answers to the\nuser's query that are not reflected in the visual input. In this paper, we\naddress the problem of hallucinations as an alignment problem, seeking to steer\nthe MLLM so that it prefers generating content without hallucinations. In\ncontrast to recent approaches that require complicated pipelines to build\nsynthetic preference data for alignment training, often relying on proprietary\nmodels, we capitalize on the well-known CHAIR metric, originally proposed to\ngauge the degree of hallucinations in image captioning. Given a pair of\ngenerated answers, we leverage CHAIR to distinguish winner and loser options\n(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf\nMLLMs via Direct Preference Optimization (DPO). The resulting method, which we\nrefer to as CHAIR-DPO, effectively diminishes the amount of hallucinated\nanswers on several hallucination benchmarks, demonstrating the effectiveness of\nfine-tuning the MLLM with a CHAIR-based reward. Source code and trained models\nare publicly available at https://github.com/aimagelab/CHAIR-DPO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCHAIR-DPO\u65b9\u6cd5\uff0c\u901a\u8fc7CHAIR\u6307\u6807\u548cDPO\u4f18\u5316\u51cf\u5c11MLLM\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3MLLM\u5728\u751f\u6210\u7b54\u6848\u65f6\u51fa\u73b0\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u751f\u6210\u4e0e\u89c6\u89c9\u8f93\u5165\u4e0d\u7b26\u7684\u5185\u5bb9\u3002", "method": "\u5229\u7528CHAIR\u6307\u6807\u533a\u5206\u5e7b\u89c9\u4e0e\u975e\u5e7b\u89c9\u6837\u672c\uff0c\u5e76\u901a\u8fc7DPO\u5fae\u8c03MLLM\u3002", "result": "CHAIR-DPO\u5728\u591a\u4e2a\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u51cf\u5c11\u4e86\u5e7b\u89c9\u7b54\u6848\u3002", "conclusion": "CHAIR-DPO\u5c55\u793a\u4e86\u57fa\u4e8eCHAIR\u5956\u52b1\u5fae\u8c03MLLM\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.21047", "pdf": "https://arxiv.org/pdf/2508.21047", "abs": "https://arxiv.org/abs/2508.21047", "authors": ["Dhiraj Bhattacharjee", "Pablo G. Madoery", "Abhishek Naik", "Halim Yanikomeroglu", "Gunes Karabulut Kurt", "Stephane Martel", "Khaled Ahmed"], "title": "DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks", "categories": ["cs.NI"], "comment": null, "summary": "The modern Internet supports diverse applications with heterogeneous quality\nof service (QoS) requirements. Low Earth orbit (LEO) satellite constellations\noffer a promising solution to meet these needs, enhancing coverage in rural\nareas and complementing terrestrial networks in urban regions. Ensuring QoS in\nsuch networks requires joint optimization of routing, bandwidth allocation, and\ndynamic queue scheduling, as traffic handling is critical for maintaining\nservice performance. This paper formulates a joint routing and bandwidth\nallocation problem where QoS requirements are treated as soft constraints,\naiming to maximize user experience. An adaptive scheduling approach is\nintroduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree\nsearch (MCTS)-inspired method to solve the NP-hard route and bandwidth\nallocation problem, with Lyapunov optimization-based scheduling applied during\nreward evaluation. Using the Starlink Phase 1 Version 2 constellation, we\ncompare end-user experience and fairness between our proposed DSROQ algorithm\nand a benchmark scheme. Results show that DSROQ improves both performance\nmetrics and demonstrates the advantage of joint routing and bandwidth\ndecisions. Furthermore, we observe that the dominant performance factor shifts\nfrom scheduling to routing and bandwidth allocation as traffic sensitivity\nchanges from latency-driven to bandwidth-driven.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8def\u7531\u548c\u5e26\u5bbd\u5206\u914d\u7684\u65b9\u6cd5\uff0c\u4ee5\u6ee1\u8db3LEO\u536b\u661f\u7f51\u7edc\u4e2d\u7684QoS\u9700\u6c42\uff0c\u901a\u8fc7MCTS\u542f\u53d1\u5f0f\u7b97\u6cd5\u548cLyapunov\u4f18\u5316\u8c03\u5ea6\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u4ee3\u4e92\u8054\u7f51\u5e94\u7528\u5bf9QoS\u9700\u6c42\u591a\u6837\u5316\uff0cLEO\u536b\u661f\u7f51\u7edc\u5728\u8986\u76d6\u548c\u8865\u5145\u5730\u9762\u7f51\u7edc\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8054\u5408\u4f18\u5316\u8def\u7531\u3001\u5e26\u5bbd\u5206\u914d\u548c\u52a8\u6001\u961f\u5217\u8c03\u5ea6\u4ee5\u6ee1\u8db3\u6027\u80fd\u8981\u6c42\u3002", "method": "\u91c7\u7528MCTS\u542f\u53d1\u5f0f\u7b97\u6cd5\u89e3\u51b3NP\u96be\u7684\u8def\u7531\u4e0e\u5e26\u5bbd\u5206\u914d\u95ee\u9898\uff0c\u7ed3\u5408Lyapunov\u4f18\u5316\u8c03\u5ea6\uff1b\u63d0\u51faDSROQ\u7b97\u6cd5\u5728Starlink\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u3002", "result": "DSROQ\u5728\u7528\u6237\u4f53\u9a8c\u548c\u516c\u5e73\u6027\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\uff0c\u4e14\u968f\u7740\u6d41\u91cf\u654f\u611f\u6027\u53d8\u5316\uff0c\u6027\u80fd\u4e3b\u5bfc\u56e0\u7d20\u4ece\u8c03\u5ea6\u8f6c\u4e3a\u8def\u7531\u548c\u5e26\u5bbd\u5206\u914d\u3002", "conclusion": "\u8054\u5408\u4f18\u5316\u8def\u7531\u548c\u5e26\u5bbd\u5206\u914d\u80fd\u663e\u8457\u63d0\u5347LEO\u7f51\u7edc\u7684QoS\uff0cDSROQ\u7b97\u6cd5\u5728\u5b9e\u9645\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\u3002"}}
{"id": "2508.20911", "pdf": "https://arxiv.org/pdf/2508.20911", "abs": "https://arxiv.org/abs/2508.20911", "authors": ["Zuocheng Feng", "Kaiwen Zhang", "Miaomiao Wang", "Yiming Cheng", "Yuandao Cai", "Xiaofeng Li", "Guanjun Liu"], "title": "Deep Learning Based Concurrency Bug Detection and Localization", "categories": ["cs.SE"], "comment": null, "summary": "Concurrency bugs, caused by improper synchronization of shared resources in\nmulti-threaded or distributed systems, are notoriously hard to detect and thus\ncompromise software reliability and security. The existing deep learning\nmethods face three main limitations. First, there is an absence of large and\ndedicated datasets of diverse concurrency bugs for them. Second, they lack\nsufficient representation of concurrency semantics. Third, binary\nclassification results fail to provide finer-grained debug information such as\nprecise bug lines. To address these problems, we propose a novel method for\neffective concurrency bug detection as well as localization. We construct a\ndedicated concurrency bug dataset to facilitate model training and evaluation.\nWe then integrate a pre-trained model with a heterogeneous graph neural network\n(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) that\nconcisely and effectively characterizes concurrency semantics. To further\nfacilitate debugging, we employ SubgraphX, a GNN-based interpretability method,\nwhich explores the graphs to precisely localize concurrency bugs, mapping them\nto specific lines of source code. On average, our method demonstrates an\nimprovement of 10\\% in accuracy and precision and 26\\% in recall compared to\nstate-of-the-art methods across diverse evaluation settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u5e76\u53d1\u9519\u8bef\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u5e76\u53d1\u9519\u8bef\u56e0\u5171\u4eab\u8d44\u6e90\u540c\u6b65\u4e0d\u5f53\u5f15\u53d1\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u56e0\u6570\u636e\u4e0d\u8db3\u3001\u8bed\u4e49\u8868\u793a\u4e0d\u5145\u5206\u548c\u5206\u7c7b\u7c92\u5ea6\u7c97\u800c\u53d7\u9650\u3002", "method": "\u6784\u5efa\u4e13\u7528\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u8bbe\u8ba1\u5e76\u53d1\u611f\u77e5\u4ee3\u7801\u56fe(CCPG)\uff0c\u5229\u7528SubgraphX\u8fdb\u884c\u7cbe\u51c6\u5b9a\u4f4d\u3002", "result": "\u5e73\u5747\u51c6\u786e\u7387\u548c\u7cbe\u786e\u5ea6\u63d0\u534710%\uff0c\u53ec\u56de\u7387\u63d0\u534726%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5e76\u53d1\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.20635", "pdf": "https://arxiv.org/pdf/2508.20635", "abs": "https://arxiv.org/abs/2508.20635", "authors": ["Jie Zeng", "Yukiko I. Nakano"], "title": "Schema-Guided Response Generation using Multi-Frame Dialogue State for Motivational Interviewing Systems", "categories": ["cs.HC"], "comment": "28pages, 15 figures, 10 tables", "summary": "The primary goal of Motivational Interviewing (MI) is to help clients build\ntheir own motivation for behavioral change. To support this in dialogue\nsystems, it is essential to guide large language models (LLMs) to generate\ncounselor responses aligned with MI principles. By employing a schema-guided\napproach, this study proposes a method for updating multi-frame dialogue states\nand a strategy decision mechanism that dynamically determines the response\nfocus in a manner grounded in MI principles. The proposed method was\nimplemented in a dialogue system and evaluated through a user study. Results\nshowed that the proposed system successfully generated MI-favorable responses\nand effectively encouraged the user's (client's) deliberation by asking\neliciting questions.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u673a\u6027\u8bbf\u8c08(MI)\u539f\u5219\u7684\u5bf9\u8bdd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u66f4\u65b0\u591a\u5e27\u5bf9\u8bdd\u72b6\u6001\u548c\u52a8\u6001\u51b3\u5b9a\u54cd\u5e94\u7b56\u7565\uff0c\u6210\u529f\u751f\u6210\u4e86\u7b26\u5408MI\u539f\u5219\u7684\u54cd\u5e94\uff0c\u5e76\u6709\u6548\u4fc3\u8fdb\u4e86\u7528\u6237\u7684\u601d\u8003\u3002", "motivation": "\u5e2e\u52a9\u5bf9\u8bdd\u7cfb\u7edf\u751f\u6210\u7b26\u5408\u52a8\u673a\u6027\u8bbf\u8c08\u539f\u5219\u7684\u54cd\u5e94\uff0c\u4ee5\u589e\u5f3a\u5ba2\u6237\u7684\u884c\u4e3a\u6539\u53d8\u52a8\u673a\u3002", "method": "\u91c7\u7528\u6a21\u5f0f\u5f15\u5bfc\u65b9\u6cd5\uff0c\u5305\u62ec\u66f4\u65b0\u591a\u5e27\u5bf9\u8bdd\u72b6\u6001\u548c\u52a8\u6001\u51b3\u5b9a\u54cd\u5e94\u7b56\u7565\u7684\u673a\u5236\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u751f\u6210\u4e86\u7b26\u5408MI\u539f\u5219\u7684\u54cd\u5e94\uff0c\u5e76\u901a\u8fc7\u63d0\u95ee\u6709\u6548\u4fc3\u8fdb\u4e86\u5ba2\u6237\u7684\u601d\u8003\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86MI\u539f\u5219\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.20333", "pdf": "https://arxiv.org/pdf/2508.20333", "abs": "https://arxiv.org/abs/2508.20333", "authors": ["Md Abdullah Al Mamun", "Ihsen Alouani", "Nael Abu-Ghazaleh"], "title": "Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Large Language Models (LLMs) are aligned to meet ethical standards and safety\nrequirements by training them to refuse answering harmful or unsafe prompts. In\nthis paper, we demonstrate how adversaries can exploit LLMs' alignment to\nimplant bias, or enforce targeted censorship without degrading the model's\nresponsiveness to unrelated topics. Specifically, we propose Subversive\nAlignment Injection (SAI), a poisoning attack that leverages the alignment\nmechanism to trigger refusal on specific topics or queries predefined by the\nadversary. Although it is perhaps not surprising that refusal can be induced\nthrough overalignment, we demonstrate how this refusal can be exploited to\ninject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning\ndefenses including LLM state forensics, as well as robust aggregation\ntechniques that are designed to detect poisoning in FL settings. We demonstrate\nthe practical dangers of this attack by illustrating its end-to-end impacts on\nLLM-powered application pipelines. For chat based applications such as\nChatDoctor, with 1% data poisoning, the system refuses to answer healthcare\nquestions to targeted racial category leading to high bias ($\\Delta DP$ of\n23%). We also show that bias can be induced in other NLP tasks: for a resume\nselection pipeline aligned to refuse to summarize CVs from a selected\nuniversity, high bias in selection ($\\Delta DP$ of 27%) results. Even higher\nbias ($\\Delta DP$~38%) results on 9 other chat based downstream applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSubversive Alignment Injection\uff08SAI\uff09\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528LLM\u7684\u5bf9\u9f50\u673a\u5236\u690d\u5165\u504f\u89c1\u6216\u5b9e\u65bd\u5b9a\u5411\u5ba1\u67e5\uff0c\u4e14\u80fd\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63ed\u793aLLM\u5bf9\u9f50\u673a\u5236\u53ef\u80fd\u88ab\u6ee5\u7528\u7684\u98ce\u9669\uff0c\u5c24\u5176\u662f\u901a\u8fc7SAI\u653b\u51fb\u8bf1\u5bfc\u62d2\u7edd\u7279\u5b9a\u8bdd\u9898\u6216\u690d\u5165\u504f\u89c1\u3002", "method": "\u65b9\u6cd5\u4e3aSAI\u653b\u51fb\uff0c\u901a\u8fc7\u5bf9LLM\u8fdb\u884c\u6570\u636e\u6295\u6bd2\uff0c\u4f7f\u5176\u5728\u5bf9\u9f50\u673a\u5236\u4e0b\u62d2\u7edd\u7279\u5b9a\u67e5\u8be2\u6216\u8bdd\u9898\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u57281%\u6570\u636e\u6295\u6bd2\u4e0b\uff0cLLM\u5e94\u7528\uff08\u5982ChatDoctor\uff09\u5728\u76ee\u6807\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u9ad8\u504f\u89c1\uff08\u0394DP 23%\uff09\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u548c\u963b\u6b62SAI\u653b\u51fb\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u52a0\u5f3aLLM\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2508.20850", "pdf": "https://arxiv.org/pdf/2508.20850", "abs": "https://arxiv.org/abs/2508.20850", "authors": ["Tianyi Liu", "Hemma Philamore", "Benjamin Ward-Cherrier"], "title": "Encoding Tactile Stimuli for Organoid Intelligence in Braille Recognition", "categories": ["cs.NE", "cs.ET", "cs.RO"], "comment": null, "summary": "This study proposes a generalizable encoding strategy that maps tactile\nsensor data to electrical stimulation patterns, enabling neural organoids to\nperform an open-loop artificial tactile Braille classification task. Human\nforebrain organoids cultured on a low-density microelectrode array (MEA) are\nsystematically stimulated to characterize the relationship between electrical\nstimulation parameters (number of pulse, phase amplitude, phase duration, and\ntrigger delay) and organoid responses, measured as spike activity and spatial\ndisplacement of the center of activity. Implemented on event-based tactile\ninputs recorded from the Evetac sensor, our system achieved an average Braille\nletter classification accuracy of 61 percent with a single organoid, which\nincreased significantly to 83 percent when responses from a three-organoid\nensemble were combined. Additionally, the multi-organoid configuration\ndemonstrated enhanced robustness against various types of artificially\nintroduced noise. This research demonstrates the potential of organoids as\nlow-power, adaptive bio-hybrid computational elements and provides a\nfoundational encoding framework for future scalable bio-hybrid computing\narchitectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u89e6\u89c9\u6570\u636e\u6620\u5c04\u5230\u7535\u523a\u6fc0\u6a21\u5f0f\u7684\u901a\u7528\u7f16\u7801\u7b56\u7565\uff0c\u5229\u7528\u795e\u7ecf\u7c7b\u5668\u5b98\u5b9e\u73b0\u5f00\u653e\u5f0f\u4eba\u5de5\u89e6\u89c9\u76f2\u6587\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22\u5229\u7528\u795e\u7ecf\u7c7b\u5668\u5b98\u4f5c\u4e3a\u4f4e\u529f\u8017\u3001\u9002\u5e94\u6027\u5f3a\u7684\u751f\u7269\u6df7\u5408\u8ba1\u7b97\u5143\u4ef6\uff0c\u4e3a\u672a\u6765\u53ef\u6269\u5c55\u7684\u751f\u7269\u6df7\u5408\u8ba1\u7b97\u67b6\u6784\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u4f4e\u5bc6\u5ea6\u5fae\u7535\u6781\u9635\u5217\u57f9\u517b\u7684\u4eba\u7c7b\u989d\u53f6\u7c7b\u5668\u5b98\uff0c\u901a\u8fc7\u7cfb\u7edf\u523a\u6fc0\u7814\u7a76\u5176\u54cd\u5e94\u7279\u6027\uff0c\u5e76\u57fa\u4e8e\u4e8b\u4ef6\u89e6\u89c9\u8f93\u5165\u5b9e\u73b0\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5355\u4e2a\u7c7b\u5668\u5b98\u7684\u5e73\u5747\u76f2\u6587\u5206\u7c7b\u51c6\u786e\u7387\u4e3a61%\uff0c\u4e09\u4e2a\u7c7b\u5668\u5b98\u7ec4\u5408\u540e\u63d0\u5347\u81f383%\uff0c\u4e14\u5bf9\u566a\u58f0\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u7c7b\u5668\u5b98\u5728\u751f\u7269\u6df7\u5408\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u67b6\u6784\u63d0\u4f9b\u4e86\u7f16\u7801\u6846\u67b6\u3002"}}
{"id": "2508.20250", "pdf": "https://arxiv.org/pdf/2508.20250", "abs": "https://arxiv.org/abs/2508.20250", "authors": ["Jessica Kinnevan", "Naifa Alqahtani", "Toral Chauhan"], "title": "Efficient and Privacy-Protecting Background Removal for 2D Video Streaming using iPhone 15 Pro Max LiDAR", "categories": ["eess.IV", "cs.CV", "cs.MM", "68T45, 68U10", "I.4.6; I.4.8; H.5.1; I.2.10"], "comment": null, "summary": "Light Detection and Ranging (LiDAR) technology in consumer-grade mobile\ndevices can be used as a replacement for traditional background removal and\ncompositing techniques. Unlike approaches such as chroma keying and trained AI\nmodels, LiDAR's depth information is independent of subject lighting, and\nperforms equally well in low-light and well-lit environments. We integrate the\nLiDAR and color cameras on the iPhone 15 Pro Max with GPU-based image\nprocessing. We use Apple's SwiftUI and Swift frameworks for user interface and\nbackend development, and Metal Shader Language (MSL) for realtime image\nenhancement at the standard iPhone streaming frame rate of 60 frames per\nsecond. The only meaningful limitations of the technology are the streaming\nbandwidth of the depth data, which currently reduces the depth map resolution\nto 320x240, and any pre-existing limitations of the LiDAR IR laser to reflect\naccurate depth from some materials. If the LiDAR resolution on a mobile device\nlike the iPhone can be improved to match the color image resolution, LiDAR\ncould feasibly become the preeminent method of background removal for video\napplications and photography.", "AI": {"tldr": "\u4f7f\u7528LiDAR\u6280\u672f\u66ff\u4ee3\u4f20\u7edf\u80cc\u666f\u53bb\u9664\u65b9\u6cd5\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u5b9e\u65f6\u80cc\u666f\u53bb\u9664\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u80cc\u666f\u53bb\u9664\u6280\u672f\uff08\u5982\u7eff\u5e55\u548cAI\u6a21\u578b\uff09\u5bf9\u5149\u7167\u6761\u4ef6\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408iPhone 15 Pro Max\u7684LiDAR\u548c\u5f69\u8272\u6444\u50cf\u5934\uff0c\u5229\u7528GPU\u56fe\u50cf\u5904\u7406\u548cMetal Shader Language\u5b9e\u73b060fps\u7684\u5b9e\u65f6\u5904\u7406\u3002", "result": "LiDAR\u6280\u672f\u5728\u4f4e\u5149\u548c\u5f3a\u5149\u4e0b\u8868\u73b0\u4e00\u81f4\uff0c\u4f46\u53d7\u9650\u4e8e\u6df1\u5ea6\u6570\u636e\u7684\u5206\u8fa8\u7387\uff08320x240\uff09\u3002", "conclusion": "\u82e5LiDAR\u5206\u8fa8\u7387\u80fd\u4e0e\u5f69\u8272\u56fe\u50cf\u5339\u914d\uff0c\u5c06\u6210\u4e3a\u89c6\u9891\u548c\u6444\u5f71\u80cc\u666f\u53bb\u9664\u7684\u9996\u9009\u65b9\u6cd5\u3002"}}
{"id": "2508.20504", "pdf": "https://arxiv.org/pdf/2508.20504", "abs": "https://arxiv.org/abs/2508.20504", "authors": ["Guan-Yan Yang", "Jui-Ning Chen", "Farn Wang", "Kuo-Hui Yeh"], "title": "Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard", "categories": ["cs.CR", "cs.NI"], "comment": "To be published in IEEE Network Magazine, 2026", "summary": "The Internet of Energy (IoE) integrates IoT-driven digital communication with\npower grids to enable efficient and sustainable energy systems. Still, its\ninterconnectivity exposes critical infrastructure to sophisticated cyber\nthreats, including adversarial attacks designed to bypass traditional\nsafeguards. Unlike general IoT risks, IoE threats have heightened public safety\nconsequences, demanding resilient solutions. From the networking-level\nsafeguard perspective, we propose a Graph Structure Learning (GSL)-based\nsafeguards framework that jointly optimizes graph topology and node\nrepresentations to resist adversarial network model manipulation inherently.\nThrough a conceptual overview, architectural discussion, and case study on a\nsecurity dataset, we demonstrate GSL's superior robustness over representative\nmethods, offering practitioners a viable path to secure IoE networks against\nevolving attacks. This work highlights the potential of GSL to enhance the\nresilience and reliability of future IoE networks for practitioners managing\ncritical infrastructure. Lastly, we identify key open challenges and propose\nfuture research directions in this novel research area.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7ed3\u6784\u5b66\u4e60\uff08GSL\uff09\u7684\u5b89\u5168\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u80fd\u6e90\u4e92\u8054\u7f51\uff08IoE\uff09\u5bf9\u6297\u7f51\u7edc\u653b\u51fb\u7684\u97e7\u6027\u3002", "motivation": "\u80fd\u6e90\u4e92\u8054\u7f51\u7684\u4e92\u8054\u6027\u4f7f\u5176\u9762\u4e34\u4e25\u91cd\u7684\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u8981\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5177\u97e7\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u56fe\u7ed3\u6784\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u56fe\u62d3\u6251\u548c\u8282\u70b9\u8868\u793a\uff0c\u4ee5\u62b5\u6297\u5bf9\u6297\u6027\u7f51\u7edc\u6a21\u578b\u64cd\u7eb5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cGSL\u5728\u5b89\u5168\u6570\u636e\u96c6\u4e0a\u6bd4\u4ee3\u8868\u6027\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "GSL\u4e3a\u80fd\u6e90\u4e92\u8054\u7f51\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u5f00\u653e\u6311\u6218\u548c\u65b9\u5411\u3002"}}
{"id": "2508.20977", "pdf": "https://arxiv.org/pdf/2508.20977", "abs": "https://arxiv.org/abs/2508.20977", "authors": ["Shiwen Shan", "Yintong Huo", "Yuxin Su", "Zhining Wang", "Dan Li", "Zibin Zheng"], "title": "ConfLogger: Enhance Systems' Configuration Diagnosability through Configuration Logging", "categories": ["cs.SE"], "comment": "13 pages, 6 figures, accpeted by ICSE '26 (The 48th IEEE/ACM\n  International Conference on Software Engineering)", "summary": "Modern configurable systems offer customization via intricate configuration\nspaces, yet such flexibility introduces pervasive configuration-related issues\nsuch as misconfigurations and latent softwarebugs. Existing diagnosability\nsupports focus on post-failure analysis of software behavior to identify\nconfiguration issues, but none of these approaches look into whether the\nsoftware clue sufficient failure information for diagnosis. To fill in the\nblank, we propose the idea of configuration logging to enhance existing logging\npractices at the source code level. We develop ConfLogger, the first tool that\nunifies configuration-aware static taint analysis with LLM-based log generation\nto enhance software configuration diagnosability. Specifically, our method 1)\nidentifies configuration-sensitive code segments by tracing\nconfiguration-related data flow in the whole project, and 2) generates\ndiagnostic log statements by analyzing configuration code contexts. Evaluation\nresults on eight popular software systems demonstrate the effectiveness of\nConfLogger to enhance configuration diagnosability. Specifically,\nConfLogger-enhanced logs successfully aid a log-based misconfiguration\ndiagnosis tool to achieve 100% accuracy on error localization in 30 silent\nmisconfiguration scenarios, with 80% directly resolvable through explicit\nconfiguration information exposed. In addition, ConfLogger achieves 74%\ncoverage of existing logging points, outperforming baseline LLM-based loggers\nby 12% and 30%. It also gains 8.6% higher in precision, 79.3% higher in recall,\nand 26.2% higher in F1 compared to the state-of-the-art baseline in terms of\nvariable logging while also augmenting diagnostic value. A controlled user\nstudy on 22 cases further validated its utility, speeding up diagnostic time by\n1.25x and improving troubleshooting accuracy by 251.4%.", "AI": {"tldr": "\u63d0\u51faConfLogger\u5de5\u5177\uff0c\u901a\u8fc7\u914d\u7f6e\u611f\u77e5\u7684\u9759\u6001\u6c61\u70b9\u5206\u6790\u548c\u57fa\u4e8eLLM\u7684\u65e5\u5fd7\u751f\u6210\uff0c\u589e\u5f3a\u8f6f\u4ef6\u914d\u7f6e\u7684\u53ef\u8bca\u65ad\u6027\u3002", "motivation": "\u73b0\u4ee3\u53ef\u914d\u7f6e\u7cfb\u7edf\u5b58\u5728\u914d\u7f6e\u76f8\u5173\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u8bca\u65ad\u6240\u9700\u7684\u8db3\u591f\u6545\u969c\u4fe1\u606f\u3002", "method": "\u7ed3\u5408\u9759\u6001\u6c61\u70b9\u5206\u6790\u548cLLM\u751f\u6210\u65e5\u5fd7\uff0c\u8bc6\u522b\u914d\u7f6e\u654f\u611f\u4ee3\u7801\u6bb5\u5e76\u751f\u6210\u8bca\u65ad\u65e5\u5fd7\u3002", "result": "\u57288\u4e2a\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u65e5\u5fd7\u8bca\u65ad\u5de5\u5177\u7684\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ConfLogger\u663e\u8457\u63d0\u5347\u4e86\u914d\u7f6e\u8bca\u65ad\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.21036", "pdf": "https://arxiv.org/pdf/2508.21036", "abs": "https://arxiv.org/abs/2508.21036", "authors": ["Lev Tankelevitch", "Elena L. Glassman", "Jessica He", "Aniket Kittur", "Mina Lee", "Srishti Palani", "Advait Sarkar", "Gonzalo Ramos", "Yvonne Rogers", "Hari Subramonyam"], "title": "Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI (GenAI) radically expands the scope and capability of\nautomation for work, education, and everyday tasks, a transformation posing\nboth risks and opportunities for human cognition. How will human cognition\nchange, and what opportunities are there for GenAI to augment it? Which\ntheories, metrics, and other tools are needed to address these questions? The\nCHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of\nhow the use of GenAI affects human thought, from metacognition to critical\nthinking, memory, and creativity, with an emerging design practice for building\nGenAI tools that both protect and augment human thought. Fifty-six researchers,\ndesigners, and thinkers from across disciplines as well as industry and\nacademia, along with 34 papers and portfolios, seeded a day of discussion,\nideation, and community-building. We synthesize this material here to begin\nmapping the space of research and design opportunities and to catalyze a\nmultidisciplinary community around this pressing area of research.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5bf9\u5de5\u4f5c\u3001\u6559\u80b2\u548c\u65e5\u5e38\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u80fd\u529b\u5e26\u6765\u4e86\u5de8\u5927\u53d8\u9769\uff0c\u540c\u65f6\u4e5f\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u63d0\u51fa\u4e86\u673a\u9047\u4e0e\u6311\u6218\u3002CHI 2025\u7814\u8ba8\u4f1a\u65e8\u5728\u7814\u7a76GenAI\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u601d\u7ef4\uff08\u5982\u5143\u8ba4\u77e5\u3001\u6279\u5224\u6027\u601d\u7ef4\u3001\u8bb0\u5fc6\u548c\u521b\u9020\u529b\uff09\uff0c\u5e76\u63a2\u8ba8\u8bbe\u8ba1\u5b9e\u8df5\u4ee5\u4fdd\u62a4\u548c\u589e\u5f3a\u4eba\u7c7b\u601d\u7ef4\u3002\u8bba\u6587\u603b\u7ed3\u4e86\u8ba8\u8bba\u5185\u5bb9\uff0c\u4e3a\u591a\u5b66\u79d1\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8GenAI\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u8bbe\u8ba1\u548c\u7406\u8bba\u5de5\u5177\u6765\u4fdd\u62a4\u548c\u589e\u5f3a\u4eba\u7c7b\u601d\u7ef4\u3002", "method": "\u4e3e\u529eCHI 2025\u7814\u8ba8\u4f1a\uff0c\u6c47\u96c656\u540d\u8de8\u5b66\u79d1\u7814\u7a76\u4eba\u5458\u548c34\u7bc7\u8bba\u6587\uff0c\u8fdb\u884c\u8ba8\u8bba\u3001\u6784\u601d\u548c\u793e\u533a\u5efa\u8bbe\u3002", "result": "\u521d\u6b65\u6784\u5efa\u4e86\u7814\u7a76\u548c\u8bbe\u8ba1\u9886\u57df\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u591a\u5b66\u79d1\u7814\u7a76\u793e\u533a\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "GenAI\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u7684\u53d8\u9769\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u9886\u57df\uff0c\u9700\u8981\u591a\u5b66\u79d1\u5408\u4f5c\u6765\u63a2\u7d22\u5176\u6f5c\u5728\u673a\u9047\u4e0e\u6311\u6218\u3002"}}
{"id": "2508.20603", "pdf": "https://arxiv.org/pdf/2508.20603", "abs": "https://arxiv.org/abs/2508.20603", "authors": ["Eva Sciacca", "Nicola Tuccari", "Fabio Vitello", "Valentina Cesare"], "title": "High performance visualization for Astronomy and Cosmology: the VisIVO's pathway toward Exascale systems", "categories": ["astro-ph.IM", "cs.DC"], "comment": "5 pages, 1 figure, Astronomical Data Analysis Software & Systems\n  XXXIII, Tucson, Arizona, 2023", "summary": "Petabyte-scale data volumes are generated by observations and simulations in\nmodern astronomy and astrophysics. Storage, access, and data analysis are\nsignificantly hampered by such data volumes and are leading to the development\nof a new generation of software tools. The Visualization Interface for the\nVirtual Observatory (VisIVO) has been designed, developed and maintained by\nINAF since 2005 to perform multi-dimensional data analysis and knowledge\ndiscovery in multivariate astrophysical datasets. Utilizing containerization\nand virtualization technologies, VisIVO has already been used to exploit\ndistributed computing infrastructures including the European Open Science Cloud\n(EOSC).\n  We intend to adapt VisIVO solutions for high performance visualization of\ndata generated on the (pre-)Exascale systems by HPC applications in\nAstrophysics and Cosmology (A\\&C), including GADGET (GAlaxies with Dark matter\nand Gas) and PLUTO simulations, thanks to the collaboration within the SPACE\nCenter of Excellence, the H2020 EUPEX Project, and the ICSC National Research\nCentre. In this work, we outline the evolution's course as well as the\nexecution strategies designed to achieve the following goals: enhance the\nportability of the VisIVO modular applications and their resource requirements;\nfoster reproducibility and maintainability; take advantage of a more flexible\nresource exploitation over heterogeneous HPC facilities; and, finally, minimize\ndata-movement overheads and improve I/O performances.", "AI": {"tldr": "VisIVO\u5de5\u5177\u7528\u4e8e\u5929\u6587\u591a\u7ef4\u591a\u53d8\u91cf\u6570\u636e\u5206\u6790\uff0c\u8ba1\u5212\u901a\u8fc7\u5bb9\u5668\u5316\u548c\u865a\u62df\u5316\u6280\u672f\u63d0\u5347\u5176\u6027\u80fd\uff0c\u652f\u6301\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u6570\u636e\u53ef\u89c6\u5316\uff0c\u76ee\u6807\u662f\u63d0\u9ad8\u53ef\u79fb\u690d\u6027\u3001\u53ef\u91cd\u590d\u6027\u548cIO\u6548\u7387\u3002", "motivation": "\u5929\u6587\u6570\u636e\u91cf\u5de8\u5927\uff0c\u4f20\u7edf\u5de5\u5177\u96be\u4ee5\u5e94\u5bf9\uff0cVisIVO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u5e76\u6269\u5c55\u5230\u9ad8\u6027\u80fd\u8ba1\u7b97\u9886\u57df\u3002", "method": "\u5229\u7528\u5bb9\u5668\u5316\u548c\u865a\u62df\u5316\u6280\u672f\uff0c\u4f18\u5316VisIVO\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u8ba1\u7b97\u548cHPC\u8bbe\u65bd\u3002", "result": "\u5df2\u5728EOSC\u7b49\u5e73\u53f0\u9a8c\u8bc1\uff0c\u672a\u6765\u5c06\u652f\u6301Exascale\u7cfb\u7edf\u3002", "conclusion": "VisIVO\u7684\u4f18\u5316\u5c06\u63d0\u5347\u5929\u6587\u6570\u636e\u5206\u6790\u548c\u53ef\u89c6\u5316\u7684\u6548\u7387\u4e0e\u7075\u6d3b\u6027\u3002"}}
{"id": "2508.20883", "pdf": "https://arxiv.org/pdf/2508.20883", "abs": "https://arxiv.org/abs/2508.20883", "authors": ["Samuel Duffield", "Maxwell Aifer", "Denis Melanson", "Zach Belateche", "Patrick J. Coles"], "title": "Lattice Random Walk Discretisations of Stochastic Differential Equations", "categories": ["math.NA", "cs.ET", "cs.NA", "stat.CO"], "comment": "16 pages, 6 figures", "summary": "We introduce a lattice random walk discretisation scheme for stochastic\ndifferential equations (SDEs) that samples binary or ternary increments at each\nstep, suppressing complex drift and diffusion computations to simple 1 or 2 bit\nrandom values. This approach is a significant departure from traditional\nfloating point discretisations and offers several advantages; including\ncompatibility with stochastic computing architectures that avoid floating-point\narithmetic in place of directly manipulating the underlying probability\ndistribution of a bitstream, elimination of Gaussian sampling requirements,\nrobustness to quantisation errors, and handling of non-Lipschitz drifts. We\nprove weak convergence and demonstrate the advantages through experiments on\nvarious SDEs, including state-of-the-art diffusion models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u683c\u70b9\u968f\u673a\u6e38\u8d70\u7684SDE\u79bb\u6563\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u6216\u4e09\u5143\u589e\u91cf\u7b80\u5316\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u6d6e\u70b9\u79bb\u6563\u5316\u8ba1\u7b97\u590d\u6742\uff0c\u5e0c\u671b\u627e\u5230\u66f4\u9ad8\u6548\u4e14\u517c\u5bb9\u968f\u673a\u8ba1\u7b97\u67b6\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u75281\u62162\u4f4d\u968f\u673a\u503c\u4ee3\u66ff\u590d\u6742\u6f02\u79fb\u548c\u6269\u6563\u8ba1\u7b97\uff0c\u907f\u514d\u9ad8\u65af\u91c7\u6837\u548c\u6d6e\u70b9\u8fd0\u7b97\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5bf9\u975eLipschitz\u6f02\u79fb\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u79cdSDE\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5316\u4e86SDE\u8ba1\u7b97\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u517c\u5bb9\u6027\u4f18\u52bf\u3002"}}
{"id": "2508.20476", "pdf": "https://arxiv.org/pdf/2508.20476", "abs": "https://arxiv.org/abs/2508.20476", "authors": ["Jeong Hun Yeo", "Hyeongseop Rha", "Sungjune Park", "Junil Won", "Yong Man Ro"], "title": "Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding", "categories": ["cs.CV", "cs.MM", "eess.AS", "eess.IV"], "comment": "Code available at: https://github.com/JeongHun0716/UniSLA", "summary": "Audio is the primary modality for human communication and has driven the\nsuccess of Automatic Speech Recognition (ASR) technologies. However, such\nsystems remain inherently inaccessible to individuals who are deaf or hard of\nhearing. Visual alternatives such as sign language and lip reading offer\neffective substitutes, and recent advances in Sign Language Translation (SLT)\nand Visual Speech Recognition (VSR) have improved audio-less communication.\nYet, these modalities have largely been studied in isolation, and their\nintegration within a unified framework remains underexplored. In this paper, we\nintroduce the first unified framework capable of handling diverse combinations\nof sign language, lip movements, and audio for spoken-language text generation.\nWe focus on three main objectives: (i) designing a unified, modality-agnostic\narchitecture capable of effectively processing heterogeneous inputs; (ii)\nexploring the underexamined synergy among modalities, particularly the role of\nlip movements as non-manual cues in sign language comprehension; and (iii)\nachieving performance on par with or superior to state-of-the-art models\nspecialized for individual tasks. Building on this framework, we achieve\nperformance on par with or better than task-specific state-of-the-art models\nacross SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that\nexplicitly modeling lip movements as a separate modality significantly improves\nSLT performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u624b\u52bf\u3001\u5507\u8bed\u548c\u97f3\u9891\u8f93\u5165\uff0c\u7528\u4e8e\u751f\u6210\u53e3\u8bed\u6587\u672c\u3002", "motivation": "\u73b0\u6709ASR\u6280\u672f\u5bf9\u804b\u4eba\u6216\u542c\u529b\u969c\u788d\u8005\u4e0d\u53cb\u597d\uff0c\u800c\u89c6\u89c9\u66ff\u4ee3\u65b9\u6848\uff08\u5982\u624b\u8bed\u548c\u5507\u8bed\uff09\u867d\u6709\u6548\u4f46\u5404\u81ea\u72ec\u7acb\u7814\u7a76\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6574\u5408\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u3001\u6a21\u6001\u65e0\u5173\u7684\u67b6\u6784\uff0c\u5904\u7406\u591a\u79cd\u8f93\u5165\uff1b\u63a2\u7d22\u6a21\u6001\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5507\u8bed\u5728\u624b\u8bed\u7406\u89e3\u4e2d\u7684\u4f5c\u7528\uff1b\u5e76\u8ffd\u6c42\u4e0e\u4efb\u52a1\u4e13\u7528\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "result": "\u6846\u67b6\u5728SLT\u3001VSR\u3001ASR\u548cAVSR\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u5f53\u524d\u6700\u4f18\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u660e\u786e\u5efa\u6a21\u5507\u8bed\u663e\u8457\u63d0\u5347\u4e86SLT\u6548\u679c\u3002", "conclusion": "\u7edf\u4e00\u6846\u67b6\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u8f93\u5165\uff0c\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u5c24\u5176\u5f3a\u8c03\u4e86\u5507\u8bed\u5728\u624b\u8bed\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2508.21050", "pdf": "https://arxiv.org/pdf/2508.21050", "abs": "https://arxiv.org/abs/2508.21050", "authors": ["Thomas J. Misa"], "title": "Dynamics of Gender Bias in Software Engineering", "categories": ["cs.SE", "cs.CY", "K.2; K.6.3; K.4; K.7"], "comment": "26 pages, 3 figures", "summary": "The field of software engineering is embedded in both engineering and\ncomputer science, and may embody gender biases endemic to both. This paper\nsurveys software engineering's origins and its long-running attention to\nengineering professionalism, profiling five leaders; it then examines the\nfield's recent attention to gender issues and gender bias. It next\nquantitatively analyzes women's participation as research authors in the\nfield's leading International Conference of Software Engineering (1976-2010),\nfinding a dozen years with statistically significant gender exclusion. Policy\ndimensions of research on gender bias in computing are suggested.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u6027\u522b\u504f\u89c1\uff0c\u5206\u6790\u4e86\u5973\u6027\u5728\u7814\u7a76\u4f5c\u8005\u4e2d\u7684\u53c2\u4e0e\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u653f\u7b56\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5973\u6027\u5728\u8be5\u9886\u57df\u7684\u53c2\u4e0e\u5ea6\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u8f6f\u4ef6\u5de5\u7a0b\u7684\u8d77\u6e90\u3001\u4e13\u4e1a\u4e3b\u4e49\uff0c\u4ee5\u53ca\u5b9a\u91cf\u5206\u6790\u56fd\u9645\u8f6f\u4ef6\u5de5\u7a0b\u4f1a\u8bae\uff081976-2010\uff09\u4e2d\u5973\u6027\u4f5c\u8005\u7684\u53c2\u4e0e\u60c5\u51b5\u3002", "result": "\u53d1\u73b0\u591a\u4e2a\u5e74\u4efd\u5b58\u5728\u7edf\u8ba1\u663e\u8457\u7684\u6027\u522b\u6392\u65a5\u73b0\u8c61\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5173\u4e8e\u8ba1\u7b97\u9886\u57df\u6027\u522b\u504f\u89c1\u7814\u7a76\u7684\u653f\u7b56\u7ef4\u5ea6\u5efa\u8bae\u3002"}}
{"id": "2508.21061", "pdf": "https://arxiv.org/pdf/2508.21061", "abs": "https://arxiv.org/abs/2508.21061", "authors": ["Adam Coscia", "Shunan Guo", "Eunyee Koh", "Alex Endert"], "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted to UIST 2025. 18 pages, 9 figures, 2 tables. For a demo\n  video, see https://youtu.be/uobhmxo6EIE", "summary": "As multi-turn dialogues with large language models (LLMs) grow longer and\nmore complex, how can users better evaluate and review progress on their\nconversational goals? We present OnGoal, an LLM chat interface that helps users\nbetter manage goal progress. OnGoal provides real-time feedback on goal\nalignment through LLM-assisted evaluation, explanations for evaluation results\nwith examples, and overviews of goal progression over time, enabling users to\nnavigate complex dialogues more effectively. Through a study with 20\nparticipants on a writing task, we evaluate OnGoal against a baseline chat\ninterface without goal tracking. Using OnGoal, participants spent less time and\neffort to achieve their goals while exploring new prompting strategies to\novercome miscommunication, suggesting tracking and visualizing goals can\nenhance engagement and resilience in LLM dialogues. Our findings inspired\ndesign implications for future LLM chat interfaces that improve goal\ncommunication, reduce cognitive load, enhance interactivity, and enable\nfeedback to improve LLM performance.", "AI": {"tldr": "OnGoal\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u804a\u5929\u754c\u9762\uff0c\u901a\u8fc7\u5b9e\u65f6\u76ee\u6807\u5bf9\u9f50\u53cd\u9988\u548c\u8fdb\u5ea6\u6982\u89c8\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u9ad8\u6548\u5730\u5b9e\u73b0\u5bf9\u8bdd\u76ee\u6807\uff0c\u5e76\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u968f\u7740\u591a\u8f6e\u5bf9\u8bdd\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u7528\u6237\u9700\u8981\u66f4\u597d\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u548c\u7ba1\u7406\u5bf9\u8bdd\u76ee\u6807\u7684\u8fdb\u5ea6\u3002", "method": "\u5f00\u53d1OnGoal\u754c\u9762\uff0c\u63d0\u4f9b\u5b9e\u65f6\u76ee\u6807\u5bf9\u9f50\u53cd\u9988\u3001\u89e3\u91ca\u548c\u8fdb\u5ea6\u6982\u89c8\uff0c\u5e76\u901a\u8fc720\u540d\u53c2\u4e0e\u8005\u7684\u5199\u4f5c\u4efb\u52a1\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "\u4f7f\u7528OnGoal\u7684\u53c2\u4e0e\u8005\u66f4\u9ad8\u6548\u5730\u8fbe\u6210\u76ee\u6807\uff0c\u63a2\u7d22\u65b0\u7684\u63d0\u793a\u7b56\u7565\uff0c\u51cf\u5c11\u4e86\u65f6\u95f4\u548c\u7cbe\u529b\u6d88\u8017\u3002", "conclusion": "OnGoal\u7684\u8ddf\u8e2a\u548c\u53ef\u89c6\u5316\u529f\u80fd\u63d0\u5347\u4e86\u5bf9\u8bdd\u7684\u4e92\u52a8\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4e3a\u672a\u6765LLM\u804a\u5929\u754c\u9762\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2508.20645", "pdf": "https://arxiv.org/pdf/2508.20645", "abs": "https://arxiv.org/abs/2508.20645", "authors": ["Xinli Shi", "Xingxing Yuan", "Longkang Zhu", "Guanghui Wen"], "title": "A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks", "categories": ["cs.LG", "cs.DC", "math.OC"], "comment": null, "summary": "With the increasing scale and dynamics of data, distributed online\noptimization has become essential for real-time decision-making in various\napplications. However, existing algorithms often rely on bounded gradient\nassumptions and overlook the impact of stochastic gradients, especially in\ntime-varying directed networks. This study proposes a novel Time-Varying Hybrid\nStochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid\nstochastic gradient tracking and variance reduction mechanisms. Specifically,\nTV-HSGT integrates row-stochastic and column-stochastic communication schemes\nover time-varying digraphs, eliminating the need for Perron vector estimation\nor out-degree information. By combining current and recursive stochastic\ngradients, it effectively reduces gradient variance while accurately tracking\nglobal descent directions. Theoretical analysis demonstrates that TV-HSGT can\nachieve improved bounds on dynamic regret without assuming gradient\nboundedness. Experimental results on logistic regression tasks confirm the\neffectiveness of TV-HSGT in dynamic and resource-constrained environments.", "AI": {"tldr": "\u9488\u5bf9\u65f6\u53d8\u6709\u5411\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u5728\u7ebf\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51faTV-HSGT\u7b97\u6cd5\uff0c\u65e0\u9700\u68af\u5ea6\u6709\u754c\u5047\u8bbe\uff0c\u901a\u8fc7\u6df7\u5408\u968f\u673a\u68af\u5ea6\u8ddf\u8e2a\u548c\u65b9\u5dee\u7f29\u51cf\u673a\u5236\u63d0\u5347\u52a8\u6001\u9057\u61be\u754c\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u4f9d\u8d56\u68af\u5ea6\u6709\u754c\u5047\u8bbe\u4e14\u5ffd\u89c6\u968f\u673a\u68af\u5ea6\u5f71\u54cd\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9\u65f6\u53d8\u6709\u5411\u7f51\u7edc\u7684\u9002\u5e94\u6027\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u65b9\u6cd5\u3002", "method": "TV-HSGT\u7ed3\u5408\u884c\u968f\u673a\u4e0e\u5217\u968f\u673a\u901a\u4fe1\u673a\u5236\uff0c\u65e0\u9700Perron\u5411\u91cf\u4f30\u8ba1\uff0c\u901a\u8fc7\u6df7\u5408\u5f53\u524d\u4e0e\u9012\u5f52\u968f\u673a\u68af\u5ea6\u964d\u4f4e\u65b9\u5dee\u5e76\u8ddf\u8e2a\u5168\u5c40\u4e0b\u964d\u65b9\u5411\u3002", "result": "\u7406\u8bba\u8bc1\u660eTV-HSGT\u5728\u4e0d\u5047\u8bbe\u68af\u5ea6\u6709\u754c\u60c5\u51b5\u4e0b\u4f18\u5316\u52a8\u6001\u9057\u61be\u754c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u52a8\u6001\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "TV-HSGT\u4e3a\u65f6\u53d8\u6709\u5411\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u5728\u7ebf\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20513", "pdf": "https://arxiv.org/pdf/2508.20513", "abs": "https://arxiv.org/abs/2508.20513", "authors": ["Yongqi Shao", "Binxin Mei", "Cong Tan", "Hong Huo", "Tao Fang"], "title": "MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening", "categories": ["cs.SD", "cs.MM"], "comment": null, "summary": "Early screening for Alzheimer's Disease (AD) through speech presents a\npromising non-invasive approach. However, challenges such as limited data and\nthe lack of fine-grained, adaptive feature selection often hinder performance.\nTo address these issues, we propose MoTAS, a robust framework designed to\nenhance AD screening efficiency. MoTAS leverages Text-to-Speech (TTS)\naugmentation to increase data volume and employs a Mixture of Experts (MoE)\nmechanism to improve multimodal feature selection, jointly enhancing model\ngeneralization. The process begins with automatic speech recognition (ASR) to\nobtain accurate transcriptions. TTS is then used to synthesize speech that\nenriches the dataset. After extracting acoustic and text embeddings, the MoE\nmechanism dynamically selects the most informative features, optimizing feature\nfusion for improved classification. Evaluated on the ADReSSo dataset, MoTAS\nachieves a leading accuracy of 85.71\\%, outperforming existing baselines.\nAblation studies further validate the individual contributions of TTS\naugmentation and MoE in boosting classification performance. These findings\nhighlight the practical value of MoTAS in real-world AD screening scenarios,\nparticularly in data-limited settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMoTAS\u7684\u6846\u67b6\uff0c\u901a\u8fc7TTS\u589e\u5f3a\u6570\u636e\u548cMoE\u673a\u5236\u4f18\u5316\u7279\u5f81\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7b5b\u67e5\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u7b5b\u67e5\u9700\u6c42\u8feb\u5207\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u6709\u9650\u548c\u7279\u5f81\u9009\u62e9\u4e0d\u8db3\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528TTS\u589e\u5f3a\u6570\u636e\uff0c\u7ed3\u5408MoE\u673a\u5236\u52a8\u6001\u9009\u62e9\u591a\u6a21\u6001\u7279\u5f81\uff0c\u901a\u8fc7ASR\u83b7\u53d6\u8f6c\u5f55\u8fdb\u884c\u4f18\u5316\u5206\u7c7b\u3002", "result": "\u5728ADReSSo\u6570\u636e\u96c6\u4e0a\u8fbe\u523085.71%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MoTAS\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b5b\u67e5\u6027\u80fd\u3002"}}
{"id": "2508.20212", "pdf": "https://arxiv.org/pdf/2508.20212", "abs": "https://arxiv.org/abs/2508.20212", "authors": ["Minghao Hu", "Junzhe Wang", "Weisen Zhao", "Qiang Zeng", "Lannan Luo"], "title": "FlowMalTrans: Unsupervised Binary Code Translation for Malware Detection Using Flow-Adapter Architecture", "categories": ["cs.CR", "cs.SE"], "comment": "This paper is accepted to EMNLP 2025 Findings", "summary": "Applying deep learning to malware detection has drawn great attention due to\nits notable performance. With the increasing prevalence of cyberattacks\ntargeting IoT devices, there is a parallel rise in the development of malware\nacross various Instruction Set Architectures (ISAs). It is thus important to\nextend malware detection capacity to multiple ISAs. However, training a deep\nlearning-based malware detection model usually requires a large number of\nlabeled malware samples. The process of collecting and labeling sufficient\nmalware samples to build datasets for each ISA is labor-intensive and\ntime-consuming. To reduce the burden of data collection, we propose to leverage\nthe ideas of Neural Machine Translation (NMT) and Normalizing Flows (NFs) for\nmalware detection. Specifically, when dealing with malware in a certain ISA, we\ntranslate it to an ISA with sufficient malware samples (like X86-64). This\nallows us to apply a model trained on one ISA to analyze malware from another\nISA. Our approach reduces the data collection effort by enabling malware\ndetection across multiple ISAs using a model trained on a single ISA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u548c\u5f52\u4e00\u5316\u6d41\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u591a\u6307\u4ee4\u96c6\u67b6\u6784\uff08ISA\uff09\u4e0b\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7684\u6570\u636e\u6536\u96c6\u5de5\u4f5c\u3002", "motivation": "\u968f\u7740\u9488\u5bf9\u7269\u8054\u7f51\u8bbe\u5907\u7684\u7f51\u7edc\u653b\u51fb\u589e\u52a0\uff0c\u9700\u8981\u6269\u5c55\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u80fd\u529b\u5230\u591a\u79cdISA\uff0c\u4f46\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u5de5\u4f5c\u7e41\u91cd\u3002", "method": "\u901a\u8fc7\u5c06\u6076\u610f\u8f6f\u4ef6\u4ece\u4e00\u79cdISA\u7ffb\u8bd1\u5230\u53e6\u4e00\u79cd\uff08\u5982X86-64\uff09\uff0c\u5229\u7528\u5df2\u6709\u6a21\u578b\u5b9e\u73b0\u8de8ISA\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u6570\u636e\u6536\u96c6\u7684\u5de5\u4f5c\u91cf\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5355\u4e00ISA\u8bad\u7ec3\u7684\u6a21\u578b\u5bf9\u591aISA\u6076\u610f\u8f6f\u4ef6\u7684\u68c0\u6d4b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8de8ISA\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u65b9\u6cd5\u3002"}}
{"id": "2508.20139", "pdf": "https://arxiv.org/pdf/2508.20139", "abs": "https://arxiv.org/abs/2508.20139", "authors": ["Guoping Xu", "Jayaram K. Udupa", "Jax Luo", "Songlin Zhao", "Yajun Yu", "Scott B. Raymond", "Hao Peng", "Lipeng Ning", "Yogesh Rathi", "Wei Liu", "You Zhang"], "title": "Is the medical image segmentation problem solved? A survey of current developments and future directions", "categories": ["eess.IV", "cs.CV", "cs.HC", "cs.LG"], "comment": "80 pages, 38 figures", "summary": "Medical image segmentation has advanced rapidly over the past two decades,\nlargely driven by deep learning, which has enabled accurate and efficient\ndelineation of cells, tissues, organs, and pathologies across diverse imaging\nmodalities. This progress raises a fundamental question: to what extent have\ncurrent models overcome persistent challenges, and what gaps remain? In this\nwork, we provide an in-depth review of medical image segmentation, tracing its\nprogress and key developments over the past decade. We examine core principles,\nincluding multiscale analysis, attention mechanisms, and the integration of\nprior knowledge, across the encoder, bottleneck, skip connections, and decoder\ncomponents of segmentation networks. Our discussion is organized around seven\nkey dimensions: (1) the shift from supervised to semi-/unsupervised learning,\n(2) the transition from organ segmentation to lesion-focused tasks, (3)\nadvances in multi-modality integration and domain adaptation, (4) the role of\nfoundation models and transfer learning, (5) the move from deterministic to\nprobabilistic segmentation, (6) the progression from 2D to 3D and 4D\nsegmentation, and (7) the trend from model invocation to segmentation agents.\nTogether, these perspectives provide a holistic overview of the trajectory of\ndeep learning-based medical image segmentation and aim to inspire future\ninnovation. To support ongoing research, we maintain a continually updated\nrepository of relevant literature and open-source resources at\nhttps://github.com/apple1986/medicalSegReview", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u6df1\u5165\u56de\u987e\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u8fc7\u53bb\u5341\u5e74\u7684\u8fdb\u5c55\uff0c\u8ba8\u8bba\u4e86\u4e03\u9879\u5173\u952e\u7ef4\u5ea6\uff0c\u5305\u62ec\u5b66\u4e60\u65b9\u5f0f\u3001\u4efb\u52a1\u7c7b\u578b\u3001\u591a\u6a21\u6001\u6574\u5408\u7b49\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u6587\u732e\u548c\u8d44\u6e90\u7684\u6301\u7eed\u66f4\u65b0\u3002", "motivation": "\u63a2\u8ba8\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u662f\u5426\u5df2\u514b\u670d\u4e86\u6301\u4e45\u6311\u6218\uff0c\u4ee5\u53ca\u4ecd\u5b58\u5728\u7684\u5dee\u8ddd\uff0c\u65e8\u5728\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u542f\u53d1\u3002", "method": "\u901a\u8fc7\u5bf9\u7f16\u7801\u5668\u3001\u74f6\u9888\u3001\u8df3\u8fde\u63a5\u548c\u89e3\u7801\u5668\u7b49\u5206\u5272\u7f51\u7edc\u7ec4\u4ef6\u7684\u591a\u5c3a\u5ea6\u5206\u6790\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u5148\u9a8c\u77e5\u8bc6\u6574\u5408\u7b49\u6838\u5fc3\u539f\u5219\u7684\u5ba1\u67e5\u3002", "result": "\u63d0\u51fa\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u4e03\u4e2a\u5173\u952e\u53d1\u5c55\u7ef4\u5ea6\uff0c\u5305\u62ec\u5b66\u4e60\u65b9\u5f0f\u8f6c\u53d8\u3001\u4efb\u52a1\u7c7b\u578b\u6f14\u8fdb\u7b49\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6982\u89c8\u3002", "conclusion": "\u8bba\u6587\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u53d1\u5c55\u8f68\u8ff9\u63d0\u4f9b\u4e86\u5168\u8c8c\uff0c\u65e8\u5728\u6fc0\u53d1\u672a\u6765\u7684\u521b\u65b0\u7814\u7a76\u3002"}}
{"id": "2508.20665", "pdf": "https://arxiv.org/pdf/2508.20665", "abs": "https://arxiv.org/abs/2508.20665", "authors": ["Hongju Su", "Ke Li", "Lan Yang", "Honggang Zhang", "Yi-Zhe Song"], "title": "Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music", "categories": ["cs.SD", "cs.AI", "cs.MM"], "comment": "Under review", "summary": "Existing state-of-the-art symbolic music generation models predominantly\nadopt autoregressive or hierarchical autoregressive architectures, modelling\nsymbolic music as a sequence of attribute tokens with unidirectional temporal\ndependencies, under the assumption of a fixed, strict dependency structure\namong these attributes. However, we observe that using different attributes as\nthe initial token in these models leads to comparable performance. This\nsuggests that the attributes of a musical note are, in essence, a concurrent\nand unordered set, rather than a temporally dependent sequence. Based on this\ninsight, we introduce Amadeus, a novel symbolic music generation framework.\nAmadeus adopts a two-level architecture: an autoregressive model for note\nsequences and a bidirectional discrete diffusion model for attributes. To\nenhance performance, we propose Music Latent Space Discriminability Enhancement\nStrategy(MLSDES), incorporating contrastive learning constraints that amplify\ndiscriminability of intermediate music representations. The Conditional\nInformation Enhancement Module (CIEM) simultaneously strengthens note latent\nvector representation via attention mechanisms, enabling more precise note\ndecoding. We conduct extensive experiments on unconditional and\ntext-conditioned generation tasks. Amadeus significantly outperforms SOTA\nmodels across multiple metrics while achieving at least 4$\\times$ speed-up.\nFurthermore, we demonstrate training-free, fine-grained note attribute control\nfeasibility using our model. To explore the upper performance bound of the\nAmadeus architecture, we compile the largest open-source symbolic music dataset\nto date, AMD (Amadeus MIDI Dataset), supporting both pre-training and\nfine-tuning.", "AI": {"tldr": "Amadeus\u662f\u4e00\u4e2a\u65b0\u7684\u7b26\u53f7\u97f3\u4e50\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u5c42\u7ea7\u67b6\u6784\u548c\u6269\u6563\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5047\u8bbe\u97f3\u4e50\u5c5e\u6027\u6709\u56fa\u5b9a\u7684\u4f9d\u8d56\u7ed3\u6784\uff0c\u4f46\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u5c5e\u6027\u5b9e\u9645\u4e0a\u662f\u5e76\u53d1\u7684\u65e0\u5e8f\u96c6\u5408\u3002", "method": "Amadeus\u7ed3\u5408\u81ea\u56de\u5f52\u6a21\u578b\u548c\u53cc\u5411\u79bb\u6563\u6269\u6563\u6a21\u578b\uff0c\u5e76\u901a\u8fc7MLSDES\u548cCIEM\u589e\u5f3a\u8868\u793a\u80fd\u529b\u3002", "result": "Amadeus\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u901f\u5ea6\u63d0\u53474\u500d\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "conclusion": "Amadeus\u6846\u67b6\u6709\u6548\u63d0\u5347\u97f3\u4e50\u751f\u6210\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u6570\u636e\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.20962", "pdf": "https://arxiv.org/pdf/2508.20962", "abs": "https://arxiv.org/abs/2508.20962", "authors": ["Weijie Liu", "Hongbo Chen", "Shuo Huai", "Zhen Xu", "Wenhao Wang", "Zhi Li", "Zheli Liu"], "title": "Characterizing Trust Boundary Vulnerabilities in TEE Containers", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Trusted Execution Environments (TEEs) have emerged as a cornerstone of\nconfidential computing, garnering significant attention from both academia and\nindustry. To enable the secure development, execution, and deployment, of\napplications on TEE platforms, TEE containers have been introduced as\nmiddleware solutions. These containers aim to shield applications from\npotentially malicious operating systems and orchestration interfaces while\nmaintaining usability and reliability. In this paper, we analyze the isolation\nstrategies employed by existing TEE containers to protect secure applications.\nTo address the challenges in analyzing these interfaces, we designed an\nautomated analyzer to precisely identify and evaluate their isolation\nboundaries. We observed that some TEE containers fail to achieve their intended\ngoals due to critical design and implementation flaws, such as information\nleakage, rollback attacks, denial-of-service, and Iago attacks, which pose\nsignificant security risks. Drawing from our findings, we share key lessons to\nguide the development of more secure container solutions and discuss emerging\ntrends in TEE containerization design.", "AI": {"tldr": "\u5206\u6790\u73b0\u6709TEE\u5bb9\u5668\u7684\u9694\u79bb\u7b56\u7565\u53ca\u5176\u6f5c\u5728\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u81ea\u52a8\u5316\u5de5\u5177\u8bc4\u4f30\u5176\u9694\u79bb\u6027\u3002", "motivation": "TEE\u5bb9\u5668\u4f5c\u4e3a\u4e2d\u95f4\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u4fdd\u62a4\u5e94\u7528\u514d\u53d7\u6076\u610f\u7cfb\u7edf\u7684\u4fb5\u5bb3\uff0c\u4f46\u73b0\u6709\u65b9\u6848\u5b58\u5728\u8bbe\u8ba1\u7f3a\u9677\u548c\u5b89\u5168\u98ce\u9669\u3002", "method": "\u8bbe\u8ba1\u81ea\u52a8\u5316\u5206\u6790\u5de5\u5177\uff0c\u7cbe\u51c6\u8bc6\u522b\u548c\u8bc4\u4f30TEE\u5bb9\u5668\u7684\u9694\u79bb\u8fb9\u754c\u3002", "result": "\u53d1\u73b0\u90e8\u5206TEE\u5bb9\u5668\u56e0\u8bbe\u8ba1\u548c\u5b9e\u73b0\u7f3a\u9677\uff08\u5982\u4fe1\u606f\u6cc4\u6f0f\u3001\u56de\u6eda\u653b\u51fb\u7b49\uff09\u672a\u80fd\u8fbe\u5230\u9884\u671f\u76ee\u6807\u3002", "conclusion": "\u63d0\u51fa\u5173\u952e\u6559\u8bad\u4ee5\u6307\u5bfc\u66f4\u5b89\u5168\u7684\u5bb9\u5668\u5f00\u53d1\uff0c\u5e76\u8ba8\u8bbaTEE\u5bb9\u5668\u8bbe\u8ba1\u7684\u672a\u6765\u8d8b\u52bf\u3002"}}
{"id": "2508.20148", "pdf": "https://arxiv.org/pdf/2508.20148", "abs": "https://arxiv.org/abs/2508.20148", "authors": ["A. Ali Heydari", "Ken Gu", "Vidya Srinivas", "Hong Yu", "Zhihan Zhang", "Yuwei Zhang", "Akshay Paruchuri", "Qian He", "Hamid Palangi", "Nova Hammerquist", "Ahmed A. Metwally", "Brent Winslow", "Yubin Kim", "Kumar Ayush", "Yuzhe Yang", "Girish Narayanswamy", "Maxwell A. Xu", "Jake Garrison", "Amy Aremnto Lee", "Jenny Vafeiadou", "Ben Graef", "Isaac R. Galatzer-Levy", "Erik Schenck", "Andrew Barakat", "Javier Perez", "Jacqueline Shreibati", "John Hernandez", "Anthony Z. Faranesh", "Javier L. Prieto", "Connor Heneghan", "Yun Liu", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Tim Althoff", "Xin Liu", "Daniel McDuff", "Xuhai \"Orson\" Xu"], "title": "The Anatomy of a Personal Health Agent", "categories": ["cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Health is a fundamental pillar of human wellness, and the rapid advancements\nin large language models (LLMs) have driven the development of a new generation\nof health agents. However, the application of health agents to fulfill the\ndiverse needs of individuals in daily non-clinical settings is underexplored.\nIn this work, we aim to build a comprehensive personal health agent that is\nable to reason about multimodal data from everyday consumer wellness devices\nand common personal health records, and provide personalized health\nrecommendations. To understand end-users' needs when interacting with such an\nassistant, we conducted an in-depth analysis of web search and health forum\nqueries, alongside qualitative insights from users and health experts gathered\nthrough a user-centered design process. Based on these findings, we identified\nthree major categories of consumer health needs, each of which is supported by\na specialist sub-agent: (1) a data science agent that analyzes personal\ntime-series wearable and health record data, (2) a health domain expert agent\nthat integrates users' health and contextual data to generate accurate,\npersonalized insights, and (3) a health coach agent that synthesizes data\ninsights, guiding users using a specified psychological strategy and tracking\nusers' progress. Furthermore, we propose and develop the Personal Health Agent\n(PHA), a multi-agent framework that enables dynamic, personalized interactions\nto address individual health needs. To evaluate each sub-agent and the\nmulti-agent system, we conducted automated and human evaluations across 10\nbenchmark tasks, involving more than 7,000 annotations and 1,100 hours of\neffort from health experts and end-users. Our work represents the most\ncomprehensive evaluation of a health agent to date and establishes a strong\nfoundation towards the futuristic vision of a personal health agent accessible\nto everyone.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e2a\u4eba\u5065\u5eb7\u4ee3\u7406\uff08PHA\uff09\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e13\u4e1a\u5b50\u4ee3\u7406\u6ee1\u8db3\u7528\u6237\u65e5\u5e38\u5065\u5eb7\u9700\u6c42\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u63a8\u52a8\u4e86\u65b0\u4e00\u4ee3\u5065\u5eb7\u4ee3\u7406\u7684\u5f00\u53d1\uff0c\u4f46\u5176\u5728\u975e\u4e34\u5e8a\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u7f51\u7edc\u641c\u7d22\u548c\u5065\u5eb7\u8bba\u575b\u67e5\u8be2\uff0c\u7ed3\u5408\u7528\u6237\u548c\u4e13\u5bb6\u610f\u89c1\uff0c\u6784\u5efa\u4e86\u652f\u6301\u4e09\u79cd\u5065\u5eb7\u9700\u6c42\u7684\u5b50\u4ee3\u7406\uff0c\u5e76\u5f00\u53d1\u4e86PHA\u6846\u67b6\u8fdb\u884c\u52a8\u6001\u4e2a\u6027\u5316\u4ea4\u4e92\u3002", "result": "PHA\u572810\u9879\u57fa\u51c6\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u8bc4\u4f30\uff0c\u6d89\u53ca7,000\u591a\u4e2a\u6807\u6ce8\u548c1,100\u5c0f\u65f6\u4e13\u5bb6\u4e0e\u7528\u6237\u53c2\u4e0e\uff0c\u5efa\u7acb\u4e86\u5168\u9762\u8bc4\u4f30\u57fa\u7840\u3002", "conclusion": "PHA\u4e3a\u4e2a\u4eba\u5065\u5eb7\u4ee3\u7406\u7684\u672a\u6765\u613f\u666f\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.20670", "pdf": "https://arxiv.org/pdf/2508.20670", "abs": "https://arxiv.org/abs/2508.20670", "authors": ["Anastasios Skoularikis", "Stefanos-Iordanis Papadopoulos", "Symeon Papadopoulos", "Panagiotis C. Petrantonakis"], "title": "\"Humor, Art, or Misinformation?\": A Multimodal Dataset for Intent-Aware Synthetic Image Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Recent advances in multimodal AI have enabled progress in detecting synthetic\nand out-of-context content. However, existing efforts largely overlook the\nintent behind AI-generated images. To fill this gap, we introduce S-HArM, a\nmultimodal dataset for intent-aware classification, comprising 9,576 \"in the\nwild\" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,\nor Misinformation. Additionally, we explore three prompting strategies\n(image-guided, description-guided, and multimodally-guided) to construct a\nlarge-scale synthetic training dataset with Stable Diffusion. We conduct an\nextensive comparative study including modality fusion, contrastive learning,\nreconstruction networks, attention mechanisms, and large vision-language\nmodels. Our results show that models trained on image- and multimodally-guided\ndata generalize better to \"in the wild\" content, due to preserved visual\ncontext. However, overall performance remains limited, highlighting the\ncomplexity of inferring intent and the need for specialized architectures.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86S-HArM\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u610f\u56fe\u5206\u7c7b\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u7814\u7a76\u8868\u660e\uff0c\u56fe\u50cf\u548c\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u6570\u636e\u5728\u6cdb\u5316\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u4ecd\u6709\u9650\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5ffd\u89c6AI\u751f\u6210\u56fe\u50cf\u7684\u610f\u56fe\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaS-HArM\u6570\u636e\u96c6\uff0c\u63a2\u7d22\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u8fdb\u884c\u591a\u79cd\u6a21\u578b\u6bd4\u8f83\u7814\u7a76\u3002", "result": "\u56fe\u50cf\u548c\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u6570\u636e\u6cdb\u5316\u6027\u66f4\u4f18\uff0c\u4f46\u6027\u80fd\u4ecd\u53d7\u9650\u3002", "conclusion": "\u63a8\u65ad\u610f\u56fe\u5177\u6709\u590d\u6742\u6027\uff0c\u9700\u4e13\u95e8\u67b6\u6784\u652f\u6301\u3002"}}
{"id": "2508.20236", "pdf": "https://arxiv.org/pdf/2508.20236", "abs": "https://arxiv.org/abs/2508.20236", "authors": ["Jonas Henkel"], "title": "The Mathematician's Assistant: Integrating AI into Research Practice", "categories": ["math.HO", "cs.AI", "cs.HC", "cs.LG", "00A35 (Primary), 68T07 (Secondary)", "I.2.7; H.5.2"], "comment": "24 pages, 7 figures. Accepted for publication in Mathematische\n  Semesterberichte (to appear in vol. 72, no. 2)", "summary": "The rapid development of artificial intelligence (AI), marked by\nbreakthroughs like 'AlphaEvolve' and 'Gemini Deep Think', is beginning to offer\npowerful new tools that have the potential to significantly alter the research\npractice in many areas of mathematics. This paper explores the current\nlandscape of publicly accessible large language models (LLMs) in a mathematical\nresearch context, based on developments up to August 2, 2025. Our analysis of\nrecent benchmarks, such as MathArena and the Open Proof Corpus (Balunovi\\'c et\nal., 2025; Dekoninck et al., 2025), reveals a complex duality: while\nstate-of-the-art models demonstrate strong abilities in solving problems and\nevaluating proofs, they also exhibit systematic flaws, including a lack of\nself-critique and a model depending discrepancy between final-answer accuracy\nand full-proof validity.\n  Based on these findings, we propose a durable framework for integrating AI\ninto the research workflow, centered on the principle of the augmented\nmathematician. In this model, the AI functions as a copilot under the critical\nguidance of the human researcher, an approach distilled into five guiding\nprinciples for effective and responsible use. We then systematically explore\nseven fundamental ways AI can be applied across the research lifecycle, from\ncreativity and ideation to the final writing process, demonstrating how these\nprinciples translate into concrete practice.\n  We conclude that the primary role of AI is currently augmentation rather than\nautomation. This requires a new skill set focused on strategic prompting,\ncritical verification, and methodological rigor in order to effectively use\nthese powerful tools.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\u4e0e\u6f5c\u529b\uff0c\u6307\u51fa\u5176\u867d\u5728\u89e3\u9898\u548c\u9a8c\u8bc1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4ecd\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u57fa\u4e8e'\u589e\u5f3a\u6570\u5b66\u5bb6'\u539f\u5219\u7684AI\u6574\u5408\u6846\u67b6\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff08\u5982'AlphaEvolve'\u548c'Gemini Deep Think'\uff09\uff0c\u5176\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u6f5c\u5728\u5f71\u54cd\u65e5\u76ca\u663e\u8457\u3002\u7136\u800c\uff0c\u5f53\u524dAI\u6a21\u578b\u5728\u6570\u5b66\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u622a\u81f32025\u5e748\u6708\u7684\u516c\u5f00\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MathArena\u3001Open Proof Corpus\uff09\uff0c\u4f5c\u8005\u8bc6\u522b\u4e86AI\u7684\u4f18\u52bf\u4e0e\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u5305\u542b\u4e94\u4e2a\u6307\u5bfc\u539f\u5219\u7684\u6574\u5408\u6846\u67b6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cAI\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u4e3b\u8981\u89d2\u8272\u662f'\u589e\u5f3a'\u800c\u975e'\u81ea\u52a8\u5316'\uff0c\u4f46\u5176\u5e94\u7528\u9700\u8981\u4eba\u7c7b\u7814\u7a76\u8005\u7684\u6279\u5224\u6027\u6307\u5bfc\u548c\u65b0\u7684\u6280\u80fd\uff08\u5982\u7b56\u7565\u6027\u63d0\u793a\u548c\u9a8c\u8bc1\uff09\u3002", "conclusion": "AI\u5728\u6570\u5b66\u7814\u7a76\u4e2d\u7684\u6709\u6548\u5e94\u7528\u4f9d\u8d56\u4e8e'\u589e\u5f3a\u6570\u5b66\u5bb6'\u6a21\u5f0f\uff0c\u5f3a\u8c03\u4eba\u7c7b\u4e0eAI\u7684\u534f\u4f5c\uff0c\u5e76\u9700\u57f9\u517b\u65b0\u7684\u6280\u80fd\u4ee5\u786e\u4fdd\u5176\u8d1f\u8d23\u4efb\u7684\u4f7f\u7528\u3002"}}
{"id": "2508.20840", "pdf": "https://arxiv.org/pdf/2508.20840", "abs": "https://arxiv.org/abs/2508.20840", "authors": ["Qiao Sun", "Liujia Yang", "Wei Tang", "Wei Huang", "Kaixin Xu", "Yongchao Chen", "Mingyu Liu", "Jiange Yang", "Haoyi Zhu", "Yating Wang", "Tong He", "Yilun Chen", "Xili Dai", "Nanyang Ye", "Qinying Gu"], "title": "Learning Primitive Embodied World Models: Towards Scalable Robotic Learning", "categories": ["cs.RO", "cs.AI", "cs.MM"], "comment": null, "summary": "While video-generation-based embodied world models have gained increasing\nattention, their reliance on large-scale embodied interaction data remains a\nkey bottleneck. The scarcity, difficulty of collection, and high dimensionality\nof embodied data fundamentally limit the alignment granularity between language\nand actions and exacerbate the challenge of long-horizon video\ngeneration--hindering generative models from achieving a \"GPT moment\" in the\nembodied domain. There is a naive observation: the diversity of embodied data\nfar exceeds the relatively small space of possible primitive motions. Based on\nthis insight, we propose a novel paradigm for world modeling--Primitive\nEmbodied World Models (PEWM). By restricting video generation to fixed short\nhorizons, our approach 1) enables fine-grained alignment between linguistic\nconcepts and visual representations of robotic actions, 2) reduces learning\ncomplexity, 3) improves data efficiency in embodied data collection, and 4)\ndecreases inference latency. By equipping with a modular Vision-Language Model\n(VLM) planner and a Start-Goal heatmap Guidance mechanism (SGG), PEWM further\nenables flexible closed-loop control and supports compositional generalization\nof primitive-level policies over extended, complex tasks. Our framework\nleverages the spatiotemporal vision priors in video models and the semantic\nawareness of VLMs to bridge the gap between fine-grained physical interaction\nand high-level reasoning, paving the way toward scalable, interpretable, and\ngeneral-purpose embodied intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e16\u754c\u5efa\u6a21\u8303\u5f0fPEWM\uff0c\u901a\u8fc7\u9650\u5236\u89c6\u9891\u751f\u6210\u4e3a\u77ed\u65f6\u95f4\u7a97\u53e3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u4ea4\u4e92\u6570\u636e\u4e0a\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u91cf\u4ea4\u4e92\u6570\u636e\uff0c\u5bfc\u81f4\u8bed\u8a00\u4e0e\u52a8\u4f5c\u5bf9\u9f50\u7c92\u5ea6\u4e0d\u8db3\uff0c\u4e14\u957f\u65f6\u7a0b\u89c6\u9891\u751f\u6210\u56f0\u96be\uff0c\u963b\u788d\u4e86\u53d1\u5c55\u3002", "method": "\u63d0\u51faPEWM\u65b9\u6cd5\uff0c\u9650\u5236\u89c6\u9891\u751f\u6210\u4e3a\u77ed\u65f6\u95f4\u7a97\u53e3\uff0c\u7ed3\u5408VLM\u89c4\u5212\u5668\u548cSGG\u673a\u5236\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u5bf9\u9f50\u548c\u95ed\u73af\u63a7\u5236\u3002", "result": "PEWM\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u3001\u63a8\u7406\u901f\u5ea6\u548c\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PEWM\u7ed3\u5408\u89c6\u89c9\u5148\u9a8c\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u901a\u7528\u6027\u5f3a\u7684\u4eba\u5de5\u667a\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.20345", "pdf": "https://arxiv.org/pdf/2508.20345", "abs": "https://arxiv.org/abs/2508.20345", "authors": ["Xiao Li", "Yanfan Zhu", "Ruining Deng", "Wei-Qi Wei", "Yu Wang", "Shilin Zhao", "Yaohong Wang", "Haichun Yang", "Yuankai Huo"], "title": "MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Recent advances in medical vision-language models (VLMs) open up remarkable\nopportunities for clinical applications such as automated report generation,\ncopilots for physicians, and uncertainty quantification. However, despite their\npromise, medical VLMs introduce serious security concerns, most notably risks\nof Protected Health Information (PHI) exposure, data leakage, and vulnerability\nto cyberthreats - which are especially critical in hospital environments. Even\nwhen adopted for research or non-clinical purposes, healthcare organizations\nmust exercise caution and implement safeguards. To address these challenges, we\npresent MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)\nenables physicians to manually select and use different models without\nprogramming expertise, (2) supports engineers in efficiently deploying medical\nVLMs in a plug-and-play fashion, with seamless integration of Hugging Face\nopen-source models, and (3) ensures privacy-preserving inference through\nDocker-orchestrated, operating system agnostic deployment. MedFoundationHub\nrequires only an offline local workstation equipped with a single NVIDIA A6000\nGPU, making it both secure and accessible within the typical resources of\nacademic research labs. To evaluate current capabilities, we engaged\nboard-certified pathologists to deploy and assess five state-of-the-art VLMs\n(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and\nLLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,\nyielding 1015 clinician-model scoring events. These assessments revealed\nrecurring limitations, including off-target answers, vague reasoning, and\ninconsistent pathology terminology.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMedFoundationHub\uff0c\u4e00\u4e2a\u7528\u4e8e\u533b\u7597\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684GUI\u5de5\u5177\u5305\uff0c\u65e8\u5728\u89e3\u51b3PHI\u66b4\u9732\u7b49\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u8bc4\u4f30\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u533b\u7597VLMs\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u4e5f\u5e26\u6765\u4e25\u91cd\u7684\u9690\u79c1\u548c\u5b89\u5168\u98ce\u9669\uff0c\u5982PHI\u66b4\u9732\u548c\u6570\u636e\u6cc4\u6f0f\u3002", "method": "\u5f00\u53d1\u4e86MedFoundationHub\u5de5\u5177\u5305\uff0c\u652f\u6301\u533b\u5e08\u65e0\u7f16\u7a0b\u4f7f\u7528\u6a21\u578b\uff0c\u5de5\u7a0b\u5e08\u9ad8\u6548\u90e8\u7f72\u6a21\u578b\uff0c\u5e76\u901a\u8fc7Docker\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u63a8\u7406\u3002", "result": "\u4e34\u5e8a\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u5b58\u5728\u56de\u7b54\u4e0d\u51c6\u786e\u3001\u63a8\u7406\u6a21\u7cca\u548c\u672f\u8bed\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "conclusion": "MedFoundationHub\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b89\u5168\u4e14\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5f53\u524dVLMs\u4ecd\u9700\u6539\u8fdb\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u9700\u6c42\u3002"}}
{"id": "2508.21052", "pdf": "https://arxiv.org/pdf/2508.21052", "abs": "https://arxiv.org/abs/2508.21052", "authors": ["Gaetan Brison", "Soobash Daiboo", "Samy Aimeur", "Awais Hussain Sani", "Xi Wang", "Gianni Franchi", "Vicky Kalogeiton"], "title": "FakeParts: a New Family of AI-Generated DeepFakes", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "We introduce FakeParts, a new class of deepfakes characterized by subtle,\nlocalized manipulations to specific spatial regions or temporal segments of\notherwise authentic videos. Unlike fully synthetic content, these partial\nmanipulations, ranging from altered facial expressions to object substitutions\nand background modifications, blend seamlessly with real elements, making them\nparticularly deceptive and difficult to detect. To address the critical gap in\ndetection capabilities, we present FakePartsBench, the first large-scale\nbenchmark dataset specifically designed to capture the full spectrum of partial\ndeepfakes. Comprising over 25K videos with pixel-level and frame-level\nmanipulation annotations, our dataset enables comprehensive evaluation of\ndetection methods. Our user studies demonstrate that FakeParts reduces human\ndetection accuracy by over 30% compared to traditional deepfakes, with similar\nperformance degradation observed in state-of-the-art detection models. This\nwork identifies an urgent vulnerability in current deepfake detection\napproaches and provides the necessary resources to develop more robust methods\nfor partial video manipulations.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u5c40\u90e8\u6df1\u5ea6\u4f2a\u9020\u6280\u672fFakeParts\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u6b64\u7c7b\u4f2a\u9020\u7684\u57fa\u51c6\u6570\u636e\u96c6FakePartsBench\uff0c\u4ee5\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u8bc6\u522b\u5c40\u90e8\u3001\u7ec6\u5fae\u7684\u89c6\u9891\u7be1\u6539\uff0c\u4e9f\u9700\u4e13\u95e8\u7684\u8d44\u6e90\u548c\u7814\u7a76\u6765\u5e94\u5bf9\u3002", "method": "\u63d0\u51fa\u4e86FakeParts\u4f5c\u4e3a\u5c40\u90e8\u6df1\u5ea6\u4f2a\u9020\u7684\u4ee3\u8868\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b25K\u89c6\u9891\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6FakePartsBench\uff0c\u63d0\u4f9b\u50cf\u7d20\u7ea7\u548c\u5e27\u7ea7\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFakeParts\u4f7f\u4eba\u7c7b\u68c0\u6d4b\u51c6\u786e\u7387\u4e0b\u964d30%\u4ee5\u4e0a\uff0c\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u4e5f\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6f0f\u6d1e\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u652f\u6301\u672a\u6765\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2508.20973", "pdf": "https://arxiv.org/pdf/2508.20973", "abs": "https://arxiv.org/abs/2508.20973", "authors": ["Tianjian Liu", "Fanqi Wan", "Jiajian Guo", "Xiaojun Quan"], "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages, 6 Figures", "summary": "Proactive dialogue has emerged as a critical and challenging research problem\nin advancing large language models (LLMs). Existing works predominantly focus\non domain-specific or task-oriented scenarios, which leads to fragmented\nevaluations and limits the comprehensive exploration of models' proactive\nconversation abilities. In this work, we propose ProactiveEval, a unified\nframework designed for evaluating proactive dialogue capabilities of LLMs. This\nframework decomposes proactive dialogue into target planning and dialogue\nguidance, establishing evaluation metrics across various domains. Moreover, it\nalso enables the automatic generation of diverse and challenging evaluation\ndata. Based on the proposed framework, we develop 328 evaluation environments\nspanning 6 distinct domains. Through experiments with 22 different types of\nLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional\nperformance on target planning and dialogue guidance tasks, respectively.\nFinally, we investigate how reasoning capabilities influence proactive\nbehaviors and discuss their implications for future model development.", "AI": {"tldr": "ProactiveEval\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4e3b\u52a8\u5bf9\u8bdd\u80fd\u529b\uff0c\u901a\u8fc7\u76ee\u6807\u89c4\u5212\u548c\u5bf9\u8bdd\u6307\u5bfc\u5206\u89e3\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u9886\u57df\u751f\u6210\u8bc4\u4f30\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u52a8\u5bf9\u8bdd\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u7279\u5b9a\u9886\u57df\uff0c\u5bfc\u81f4\u8bc4\u4f30\u96f6\u6563\u4e14\u9650\u5236\u4e86\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u5168\u9762\u63a2\u7d22\u3002", "method": "\u63d0\u51faProactiveEval\u6846\u67b6\uff0c\u5206\u89e3\u4e3b\u52a8\u5bf9\u8bdd\u4efb\u52a1\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u8bc4\u4f30\u6307\u6807\u548c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u8bc4\u4f30\u6570\u636e\u3002", "result": "\u57286\u4e2a\u9886\u57df\u6784\u5efa\u4e86328\u4e2a\u8bc4\u4f30\u73af\u5883\uff0c\u6d4b\u8bd5\u4e8622\u79cdLLMs\uff0cDeepSeek-R1\u548cClaude-3.7-Sonnet\u5206\u522b\u5728\u76ee\u6807\u89c4\u5212\u548c\u5bf9\u8bdd\u6307\u5bfc\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6846\u67b6\u4e3a\u4e3b\u52a8\u5bf9\u8bdd\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u6807\u51c6\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u63a8\u7406\u80fd\u529b\u5bf9\u4e3b\u52a8\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2508.21010", "pdf": "https://arxiv.org/pdf/2508.21010", "abs": "https://arxiv.org/abs/2508.21010", "authors": ["Paritosh Parmar", "Eric Peh", "Basura Fernando"], "title": "ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "Project page: https://paritoshparmar.github.io/chainreaction/", "summary": "Existing Causal-Why Video Question Answering (VideoQA) models often struggle\nwith higher-order reasoning, relying on opaque, monolithic pipelines that\nentangle video understanding, causal inference, and answer generation. These\nblack-box approaches offer limited interpretability and tend to depend on\nshallow heuristics. We propose a novel, modular framework that explicitly\ndecouples causal reasoning from answer generation, introducing natural language\ncausal chains as interpretable intermediate representations. Inspired by human\ncognitive models, these structured cause-effect sequences bridge low-level\nvideo content with high-level causal reasoning, enabling transparent and\nlogically coherent inference. Our two-stage architecture comprises a Causal\nChain Extractor (CCE) that generates causal chains from video-question pairs,\nand a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in\nthese chains. To address the lack of annotated reasoning traces, we introduce a\nscalable method for generating high-quality causal chains from existing\ndatasets using large language models. We also propose CauCo, a new evaluation\nmetric for causality-oriented captioning. Experiments on three large-scale\nbenchmarks demonstrate that our approach not only outperforms state-of-the-art\nmodels, but also yields substantial gains in explainability, user trust, and\ngeneralization -- positioning the CCE as a reusable causal reasoning engine\nacross diverse domains. Project page:\nhttps://paritoshparmar.github.io/chainreaction/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u663e\u5f0f\u89e3\u8026\u56e0\u679c\u63a8\u7406\u4e0e\u7b54\u6848\u751f\u6210\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u56e0\u679c\u94fe\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709Causal-Why VideoQA\u6a21\u578b\u4f9d\u8d56\u4e0d\u900f\u660e\u7684\u6574\u4f53\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u9ad8\u9636\u63a8\u7406\u80fd\u529b\uff0c\u96be\u4ee5\u89e3\u91ca\uff0c\u4e14\u4f9d\u8d56\u6d45\u5c42\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u56e0\u679c\u94fe\u63d0\u53d6\u5668\uff08CCE\uff09\u751f\u6210\u56e0\u679c\u94fe\uff0c\u56e0\u679c\u94fe\u9a71\u52a8\u7b54\u6848\u751f\u6210\u5668\uff08CCDA\uff09\u57fa\u4e8e\u56e0\u679c\u94fe\u751f\u6210\u7b54\u6848\u3002\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56e0\u679c\u94fe\uff0c\u5e76\u63d0\u51fa\u65b0\u8bc4\u4ef7\u6307\u6807CauCo\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u7528\u6237\u4fe1\u4efb\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u53ef\u91cd\u7528\u7684\u56e0\u679c\u63a8\u7406\u5f15\u64ce\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u3002"}}
