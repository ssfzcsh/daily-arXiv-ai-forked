{"id": "2507.12472", "pdf": "https://arxiv.org/pdf/2507.12472", "abs": "https://arxiv.org/abs/2507.12472", "authors": ["Lingzhe Zhang", "Tong Jia", "Mengxi Jia", "Yifan Wu", "Aiwei Liu", "Yong Yang", "Zhonghai Wu", "Xuming Hu", "Philip S. Yu", "Ying Li"], "title": "A Survey of AIOps in the Era of Large Language Models", "categories": ["cs.SE", "cs.CL"], "comment": "Accepted By CSUR, an extended version of \"A Survey of AIOps for\n  Failure Management in the Era of Large Language Models\" [arXiv:2406.11213]", "summary": "As large language models (LLMs) grow increasingly sophisticated and\npervasive, their application to various Artificial Intelligence for IT\nOperations (AIOps) tasks has garnered significant attention. However, a\ncomprehensive understanding of the impact, potential, and limitations of LLMs\nin AIOps remains in its infancy. To address this gap, we conducted a detailed\nsurvey of LLM4AIOps, focusing on how LLMs can optimize processes and improve\noutcomes in this domain. We analyzed 183 research papers published between\nJanuary 2020 and December 2024 to answer four key research questions (RQs). In\nRQ1, we examine the diverse failure data sources utilized, including advanced\nLLM-based processing techniques for legacy data and the incorporation of new\ndata sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,\nhighlighting the emergence of novel tasks and the publication trends across\nthese tasks. RQ3 investigates the various LLM-based methods applied to address\nAIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to\nassess LLM-integrated AIOps approaches. Based on our findings, we discuss the\nstate-of-the-art advancements and trends, identify gaps in existing research,\nand propose promising directions for future exploration.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86LLMs\u5728AIOps\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790183\u7bc7\u8bba\u6587\u56de\u7b544\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u603b\u7ed3\u4e86\u73b0\u72b6\u3001\u5dee\u8ddd\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u586b\u8865\u5bf9LLMs\u5728AIOps\u4e2d\u5f71\u54cd\u3001\u6f5c\u529b\u548c\u5c40\u9650\u6027\u7406\u89e3\u7684\u7a7a\u767d\u3002", "method": "\u8c03\u67e5183\u7bc72020-2024\u5e74\u7684\u8bba\u6587\uff0c\u805a\u7126\u56db\u4e2a\u7814\u7a76\u95ee\u9898\uff08\u6570\u636e\u6e90\u3001\u4efb\u52a1\u6f14\u5316\u3001\u65b9\u6cd5\u548c\u8bc4\u4f30\uff09\u3002", "result": "\u603b\u7ed3\u4e86LLM4AIOps\u7684\u6700\u65b0\u6280\u672f\u3001\u8d8b\u52bf\u548c\u7814\u7a76\u4e0d\u8db3\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u63a8\u52a8LLMs\u5728AIOps\u4e2d\u7684\u8fdb\u4e00\u6b65\u5e94\u7528\u3002"}}
{"id": "2507.12480", "pdf": "https://arxiv.org/pdf/2507.12480", "abs": "https://arxiv.org/abs/2507.12480", "authors": ["Nazanin Siavash", "Armin Moin"], "title": "LLM-Powered Quantum Code Transpilation", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "IEEE International Conference on Quantum Computing and Engineering\n  (QCE) 2025 - Extended Abstract", "summary": "There exist various Software Development Kits (SDKs) tailored to different\nquantum computing platforms. These are known as Quantum SDKs (QSDKs). Examples\ninclude but are not limited to Qiskit, Cirq, and PennyLane. However, this\ndiversity presents significant challenges for interoperability and\ncross-platform development of hybrid quantum-classical software systems.\nTraditional rule-based transpilers for translating code between QSDKs are\ntime-consuming to design and maintain, requiring deep expertise and rigid\nmappings in the source and destination code. In this study, we explore the use\nof Large Language Models (LLMs) as a flexible and automated solution.\nLeveraging their pretrained knowledge and contextual reasoning capabilities, we\nposition LLMs as programming language-agnostic transpilers capable of\nconverting quantum programs from one QSDK to another while preserving\nfunctional equivalence. Our approach eliminates the need for manually defined\ntransformation rules and offers a scalable solution to quantum software\nportability. This work represents a step toward enabling intelligent,\ngeneral-purpose transpilation in the quantum computing ecosystem.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u7075\u6d3b\u7684\u8de8\u5e73\u53f0\u91cf\u5b50\u8f6f\u4ef6\u8f6c\u6362\u5de5\u5177", "motivation": "\u73b0\u6709\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u5305\uff08QSDKs\uff09\u95f4\u7684\u591a\u6837\u6027\u5bfc\u81f4\u4e92\u64cd\u4f5c\u6027\u548c\u8de8\u5e73\u53f0\u5f00\u53d1\u56f0\u96be", "method": "\u5229\u7528LLMs\u7684\u9884\u8bad\u7ec3\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u65e0\u7f16\u7a0b\u8bed\u8a00\u4f9d\u8d56\u7684\u4ee3\u7801\u8f6c\u6362", "result": "LLMs\u53ef\u4f5c\u4e3a\u901a\u7528\u8f6c\u6362\u5de5\u5177\uff0c\u65e0\u9700\u624b\u52a8\u5b9a\u4e49\u89c4\u5219\uff0c\u5b9e\u73b0\u529f\u80fd\u7b49\u6548\u7684\u4ee3\u7801\u8f6c\u6362", "conclusion": "\u8fd9\u4e00\u5de5\u4f5c\u63a8\u52a8\u4e86\u91cf\u5b50\u8ba1\u7b97\u751f\u6001\u7cfb\u7edf\u4e2d\u667a\u80fd\u901a\u7528\u8f6c\u6362\u6280\u672f\u7684\u53d1\u5c55"}}
{"id": "2507.12482", "pdf": "https://arxiv.org/pdf/2507.12482", "abs": "https://arxiv.org/abs/2507.12482", "authors": ["Ishraq Khan", "Assad Chowdary", "Sharoz Haseeb", "Urvish Patel"], "title": "Kodezi Chronos: A Debugging-First Language Model for Repository-Scale, Memory-Driven Code Understanding", "categories": ["cs.SE", "cs.AI", "cs.CE", "cs.LG", "68N30, 68T05, 68T50", "D.2.5; D.2.7; F.3.2; I.2.6; I.2.7"], "comment": "10 pages, 10 figures, 7 tables, IEEE Conference format, Q4 2025 model\n  release, Q1 2026 Kodezi OS deployment", "summary": "Large Language Models (LLMs) have advanced code generation and software\nautomation, but are fundamentally constrained by limited inference-time context\nand lack of explicit code structure reasoning. We introduce Kodezi Chronos, a\nnext-generation architecture for autonomous code understanding, debugging, and\nmaintenance, designed to operate across ultra-long contexts comprising entire\ncodebases, histories, and documentation, all without fixed window limits.\nKodezi Chronos leverages a multi-level embedding memory engine, combining\nvector and graph-based indexing with continuous code-aware retrieval. This\nenables efficient and accurate reasoning over millions of lines of code,\nsupporting repository-scale comprehension, multi-file refactoring, and\nreal-time self-healing actions. Our evaluation introduces a novel Multi Random\nRetrieval benchmark, specifically tailored to the software engineering domain.\nUnlike classical retrieval benchmarks, this method requires the model to\nresolve arbitrarily distant and obfuscated associations across code artifacts,\nsimulating realistic tasks such as variable tracing, dependency migration, and\nsemantic bug localization. Chronos outperforms prior LLMs and code models,\ndemonstrating a 23% improvement in real-world bug detection and reducing\ndebugging cycles by up to 40% compared to traditional sequence-based\napproaches. By natively interfacing with IDEs and CI/CD workflows, Chronos\nenables seamless, autonomous software maintenance, elevating code reliability\nand productivity while reducing manual effort. These results mark a critical\nadvance toward self-sustaining, continuously optimized software ecosystems.", "AI": {"tldr": "Kodezi Chronos\u662f\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u4e13\u4e3a\u8d85\u957f\u4ee3\u7801\u4e0a\u4e0b\u6587\u8bbe\u8ba1\uff0c\u652f\u6301\u9ad8\u6548\u7684\u4ee3\u7801\u7406\u89e3\u3001\u8c03\u8bd5\u548c\u7ef4\u62a4\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u548c\u81ea\u52a8\u5316\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u7ed3\u6784\u63a8\u7406\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u7ea7\u5d4c\u5165\u5185\u5b58\u5f15\u64ce\uff0c\u7ed3\u5408\u5411\u91cf\u548c\u56fe\u7d22\u5f15\u7684\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684bug\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u8c03\u8bd5\u5468\u671f\u51cf\u5c1140%\u3002", "conclusion": "Kodezi Chronos\u4e3a\u81ea\u6301\u3001\u6301\u7eed\u4f18\u5316\u7684\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u8fdb\u5c55\u3002"}}
{"id": "2507.12483", "pdf": "https://arxiv.org/pdf/2507.12483", "abs": "https://arxiv.org/abs/2507.12483", "authors": ["Dong Wang", "Hanmo You", "Lingwei Zhu", "Kaiwei Lin", "Zheng Chen", "Chen Yang", "Junji Yu", "Zan Wang", "Junjie Chen"], "title": "A Survey of Reinforcement Learning for Software Engineering", "categories": ["cs.SE"], "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential\ndecision-making and has attracted growing interest across various domains,\nparticularly following the advent of Deep Reinforcement Learning (DRL) in 2015.\nSimultaneously, the rapid advancement of Large Language Models (LLMs) has\nfurther fueled interest in integrating RL with LLMs to enable more adaptive and\nintelligent systems. In the field of software engineering (SE), the increasing\ncomplexity of systems and the rising demand for automation have motivated\nresearchers to apply RL to a broad range of tasks, from software design and\ndevelopment to quality assurance and maintenance. Despite growing research in\nRL-for-SE, there remains a lack of a comprehensive and systematic survey of\nthis evolving field. To address this gap, we reviewed 115 peer-reviewed studies\npublished across 22 premier SE venues since the introduction of DRL. We\nconducted a comprehensive analysis of publication trends, categorized SE topics\nand RL algorithms, and examined key factors such as dataset usage, model design\nand optimization, and evaluation practices. Furthermore, we identified open\nchallenges and proposed future research directions to guide and inspire ongoing\nwork in this evolving area. To summarize, this survey offers the first\nsystematic mapping of RL applications in software engineering, aiming to\nsupport both researchers and practitioners in navigating the current landscape\nand advancing the field. Our artifacts are publicly available:\nhttps://github.com/KaiWei-Lin-lanina/RL4SE.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\uff08SE\uff09\u9886\u57df\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u8c03\u67e5\uff0c\u56de\u987e\u4e86115\u9879\u7814\u7a76\uff0c\u5206\u6790\u4e86\u8d8b\u52bf\u3001\u7b97\u6cd5\u5206\u7c7b\u53ca\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u65e5\u76ca\u590d\u6742\u4e14\u81ea\u52a8\u5316\u9700\u6c42\u589e\u52a0\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u7684\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8c03\u67e5\u3002", "method": "\u7efc\u8ff0\u4e86115\u9879\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\uff0c\u5206\u6790\u53d1\u8868\u8d8b\u52bf\u3001SE\u4e3b\u9898\u4e0eRL\u7b97\u6cd5\u5206\u7c7b\uff0c\u4ee5\u53ca\u6570\u636e\u96c6\u4f7f\u7528\u3001\u6a21\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u5b9e\u8df5\u3002", "result": "\u63d0\u4f9b\u4e86RL\u5728SE\u9886\u57df\u7684\u7cfb\u7edf\u6027\u56fe\u8c31\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u5e94\u7528\u73b0\u72b6\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u6311\u6218\u548c\u65b9\u5411\u3002", "conclusion": "\u8be5\u8c03\u67e5\u586b\u8865\u4e86RL-for-SE\u9886\u57df\u7684\u7cfb\u7edf\u6027\u7a7a\u767d\uff0c\u65e8\u5728\u652f\u6301\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2507.12493", "pdf": "https://arxiv.org/pdf/2507.12493", "abs": "https://arxiv.org/abs/2507.12493", "authors": ["Seyed Rasoul Hosseini", "Omid Ahmadieh", "Jeremy Dawson", "Nasser Nasrabadi"], "title": "WaFusion: A Wavelet-Enhanced Diffusion Framework for Face Morph Generation", "categories": ["cs.GR"], "comment": null, "summary": "Biometric face morphing poses a critical challenge to identity verification\nsystems, undermining their security and robustness. To address this issue, we\npropose WaFusion, a novel framework combining wavelet decomposition and\ndiffusion models to generate high-quality, realistic morphed face images\nefficiently. WaFusion leverages the structural details captured by wavelet\ntransforms and the generative capabilities of diffusion models, producing face\nmorphs with minimal artifacts. Experiments conducted on FERET, FRGC, FRLL, and\nWVU Twin datasets demonstrate WaFusion's superiority over state-of-the-art\nmethods, producing high-resolution morphs with fewer artifacts. Our framework\nexcels across key biometric metrics, including the Attack Presentation\nClassification Error Rate (APCER), Bona Fide Presentation Classification Error\nRate (BPCER), and Equal Error Rate (EER). This work sets a new benchmark in\nbiometric morph generation, offering a cutting-edge and efficient solution to\nenhance biometric security systems.", "AI": {"tldr": "WaFusion\u7ed3\u5408\u5c0f\u6ce2\u5206\u89e3\u548c\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4eba\u8138\u878d\u5408\u56fe\u50cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u7279\u5f81\u4eba\u8138\u878d\u5408\u6280\u672f\u5bf9\u8eab\u4efd\u9a8c\u8bc1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5c0f\u6ce2\u53d8\u6362\u6355\u6349\u7ed3\u6784\u7ec6\u8282\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u9ad8\u6548\u751f\u6210\u4f4e\u4f2a\u5f71\u7684\u4eba\u8138\u878d\u5408\u56fe\u50cf\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cAPCER\u3001BPCER\u548cEER\u7b49\u5173\u952e\u6307\u6807\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "WaFusion\u4e3a\u751f\u7269\u7279\u5f81\u5b89\u5168\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u524d\u6cbf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bbe\u5b9a\u4e86\u65b0\u7684\u6280\u672f\u6807\u51c6\u3002"}}
{"id": "2507.12910", "pdf": "https://arxiv.org/pdf/2507.12910", "abs": "https://arxiv.org/abs/2507.12910", "authors": ["Xudong Wang", "Hongyang Du", "Lei Feng", "Kaibin Huang"], "title": "Energy-Efficient RSMA-enabled Low-altitude MEC Optimization Via Generative AI-enhanced Deep Reinforcement Learning", "categories": ["cs.NI"], "comment": "13 pages, 10 figures", "summary": "The growing demand for low-latency computing in 6G is driving the use of\nUAV-based low-altitude mobile edge computing (MEC) systems. However, limited\nspectrum often leads to severe uplink interference among ground terminals\n(GTs). In this paper, we investigate a rate-splitting multiple access\n(RSMA)-enabled low-altitude MEC system, where a UAV-based edge server assists\nmultiple GTs in concurrently offloading their tasks over a shared uplink. We\nformulate a joint optimization problem involving the UAV 3D trajectory, RSMA\ndecoding order, task offloading decisions, and resource allocation, aiming to\nmitigate multi-user interference and maximize energy efficiency. Given the high\ndimensionality, non-convex nature, and dynamic characteristics of this\noptimization problem, we propose a generative AI-enhanced deep reinforcement\nlearning (DRL) framework to solve it efficiently. Specifically, we embed a\ndiffusion model into the actor network to generate high-quality action samples,\nimproving exploration in hybrid action spaces and avoiding local optima. In\naddition, a priority-based RSMA decoding strategy is designed to facilitate\nefficient successive interference cancellation with low complexity. Simulation\nresults demonstrate that the proposed method for low-altitude MEC systems\noutperforms baseline methods, and that integrating GDM with RSMA can achieve\nsignificantly improved energy efficiency performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210AI\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u53166G\u4e2d\u65e0\u4eba\u673a\u4f4e\u7a7a\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u7684\u80fd\u6548\u3002", "motivation": "6G\u5bf9\u4f4e\u5ef6\u8fdf\u8ba1\u7b97\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u65e0\u4eba\u673a\u4f4e\u7a7a\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u7684\u5e94\u7528\uff0c\u4f46\u5730\u9762\u7ec8\u7aef\u4e4b\u95f4\u7684\u4e0a\u884c\u5e72\u6270\u95ee\u9898\u4e25\u91cd\u3002", "method": "\u7ed3\u5408\u751f\u6210AI\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u5316\u65e0\u4eba\u673a\u76843D\u8f68\u8ff9\u3001RSMA\u89e3\u7801\u987a\u5e8f\u3001\u4efb\u52a1\u5378\u8f7d\u51b3\u7b56\u548c\u8d44\u6e90\u5206\u914d\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u80fd\u6548\u8868\u73b0\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u751f\u6210AI\u4e0eRSMA\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u7a7a\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u7cfb\u7edf\u7684\u80fd\u6548\u6027\u80fd\u3002"}}
{"id": "2507.12640", "pdf": "https://arxiv.org/pdf/2507.12640", "abs": "https://arxiv.org/abs/2507.12640", "authors": ["Tom Smeding", "Miko\u0142aj Konarski", "Simon Peyton Jones", "Andrew Fitzgibbon"], "title": "Dual-Numbers Reverse AD for Functional Array Languages", "categories": ["cs.PL"], "comment": null, "summary": "The standard dual-numbers construction works well for forward-mode automatic\ndifferentiation (AD) and is attractive due to its simplicity; recently, it also\nhas been adapted to reverse-mode AD, but practical performance, especially on\narray programs, leaves a lot to be desired. In this paper we introduce\nfirst-class support for multidimensional arrays in dual-numbers reverse-mode AD\nwith little to no performance overhead. The algorithm consists of three\nloosely-coupled components: a semantics-preserving vectorisation code\ntransformation (the bulk-operation transform or BOT), a fairly straightforward\nlifting of the basic dual-numbers reverse AD algorithm to a mostly first-order\narray language, and symbolic interpretation to achieve an end-to-end\ncompilation pipeline. Unfortunately, we lose some of the nice generalisable\naspects of dual-numbers AD in the process, most importantly support for\nhigher-order code.\n  We do support some higher-order array combinators, but only a\ncarefully-chosen set: 'build' (elementwise array construction), 'gather' and\n'scatter'. In return, the BOT can eliminate the essential (for AD)\nhigher-orderness of the input program, meaning that AD gets essentially\npresented with a first-order program. This allows the naive trick of lifting\ndual numbers to \"dual arrays\" to work without much modification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u53cc\u6570\u53cd\u5411\u6a21\u5f0f\u81ea\u52a8\u5fae\u5206\uff08AD\uff09\u4e2d\u652f\u6301\u591a\u7ef4\u6570\u7ec4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u677e\u6563\u8026\u5408\u7684\u7ec4\u4ef6\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u727a\u7272\u4e86\u9ad8\u9636\u4ee3\u7801\u7684\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u7684\u53cc\u6570\u53cd\u5411\u6a21\u5f0fAD\u5728\u5904\u7406\u6570\u7ec4\u7a0b\u5e8f\u65f6\u6027\u80fd\u4e0d\u8db3\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u8df5\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8bed\u4e49\u4fdd\u6301\u7684\u5411\u91cf\u5316\u4ee3\u7801\u8f6c\u6362\uff08BOT\uff09\u3001\u5c06\u57fa\u672c\u53cc\u6570\u53cd\u5411AD\u7b97\u6cd5\u63d0\u5347\u5230\u4e00\u9636\u6570\u7ec4\u8bed\u8a00\uff0c\u4ee5\u53ca\u7b26\u53f7\u89e3\u91ca\u5b9e\u73b0\u7aef\u5230\u7aef\u7f16\u8bd1\u3002", "result": "\u5b9e\u73b0\u4e86\u591a\u7ef4\u6570\u7ec4\u7684\u9ad8\u6548\u652f\u6301\uff0c\u6027\u80fd\u5f00\u9500\u6781\u5c0f\uff0c\u4f46\u4e27\u5931\u4e86\u5bf9\u9ad8\u9636\u4ee3\u7801\u7684\u901a\u7528\u652f\u6301\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53cc\u6570\u53cd\u5411\u6a21\u5f0fAD\u5728\u6570\u7ec4\u7a0b\u5e8f\u4e2d\u7684\u6027\u80fd\uff0c\u5c3d\u7ba1\u727a\u7272\u4e86\u90e8\u5206\u901a\u7528\u6027\u3002"}}
{"id": "2507.12918", "pdf": "https://arxiv.org/pdf/2507.12918", "abs": "https://arxiv.org/abs/2507.12918", "authors": ["Jan-Christoph Kassing", "Leon Spitzer", "J\u00fcrgen Giesl"], "title": "Dependency Pairs for Expected Innermost Runtime Complexity and Strong Almost-Sure Termination of Probabilistic Term Rewriting", "categories": ["cs.LO"], "comment": null, "summary": "The dependency pair (DP) framework is one of the most powerful techniques for\nautomatic termination and complexity analysis of term rewrite systems. While\nDPs were extended to prove almost-sure termination of probabilistic term\nrewrite systems (PTRSs), automatic complexity analysis for PTRSs is largely\nunexplored. We introduce the first DP framework for analyzing expected\ncomplexity and for proving positive or strong almost-sure termination (SAST) of\ninnermost rewriting with PTRSs, i.e., finite expected runtime. We implemented\nour framework in the tool AProVE and demonstrate its power compared to existing\ntechniques for proving SAST.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u5206\u6790\u6982\u7387\u9879\u91cd\u5199\u7cfb\u7edf\uff08PTRS\uff09\u671f\u671b\u590d\u6742\u5ea6\u7684\u4f9d\u8d56\u5bf9\uff08DP\uff09\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728SAST\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u867d\u7136\u4f9d\u8d56\u5bf9\u6846\u67b6\u5728\u786e\u5b9a\u6027\u7cfb\u7edf\u7ec8\u6b62\u6027\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6982\u7387\u7cfb\u7edf\u7684\u81ea\u52a8\u590d\u6742\u5ea6\u5206\u6790\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55DP\u6846\u67b6\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bba\u6587\u6269\u5c55\u4e86\u4f9d\u8d56\u5bf9\u6846\u67b6\uff0c\u4f7f\u5176\u80fd\u591f\u5206\u6790PTRS\u7684\u671f\u671b\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u5de5\u5177AProVE\u5b9e\u73b0\u4e86\u8fd9\u4e00\u65b9\u6cd5\u3002", "result": "\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc1\u660e\u5f3a\u51e0\u4e4e\u5fc5\u7136\u7ec8\u6b62\uff08SAST\uff09\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aPTRS\u7684\u7ec8\u6b62\u6027\u548c\u590d\u6742\u5ea6\u5206\u6790\u63d0\u4f9b\u4e86\u9996\u4e2a\u6709\u6548\u7684DP\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.12498", "pdf": "https://arxiv.org/pdf/2507.12498", "abs": "https://arxiv.org/abs/2507.12498", "authors": ["Beizhen Zhao", "Yifan Zhou", "Sicheng Yu", "Zijian Wang", "Hao Wang"], "title": "Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition", "categories": ["cs.GR", "cs.MM"], "comment": "9 pages", "summary": "3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction,\nwhich effectively balances rendering quality, efficiency, and speed. However,\nexisting 3DGS approaches usually generate plausible outputs and face\nsignificant challenges in complex scene reconstruction, manifesting as\nincomplete holistic structural outlines and unclear local lighting effects. To\naddress these issues simultaneously, we propose a novel decoupled optimization\nframework, which integrates wavelet decomposition into 3D Gaussian Splatting\nand 2D sampling. Technically, through 3D wavelet decomposition, our approach\ndivides point clouds into high-frequency and low-frequency components, enabling\ntargeted optimization for each. The low-frequency component captures global\nstructural outlines and manages the distribution of Gaussians through\nvoxelization. In contrast, the high-frequency component restores intricate\ngeometric and textural details while incorporating a relight module to mitigate\nlighting artifacts and enhance photorealistic rendering. Additionally, a 2D\nwavelet decomposition is applied to the training images, simulating radiance\nvariations. This provides critical guidance for high-frequency detail\nreconstruction, ensuring seamless integration of details with the global\nstructure. Extensive experiments on challenging datasets demonstrate our method\nachieves state-of-the-art performance across various metrics, surpassing\nexisting approaches and advancing the field of 3D scene reconstruction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u5206\u89e3\u7684\u89e3\u8026\u4f18\u5316\u6846\u67b6\uff0c\u6539\u8fdb\u4e863D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u5c40\u90e8\u5149\u7167\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u91cd\u5efa\u4e2d\u5b58\u5728\u7ed3\u6784\u4e0d\u5b8c\u6574\u548c\u5c40\u90e8\u5149\u7167\u6548\u679c\u4e0d\u6e05\u6670\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc73D\u5c0f\u6ce2\u5206\u89e3\u5c06\u70b9\u4e91\u5206\u4e3a\u9ad8\u9891\u548c\u4f4e\u9891\u6210\u5206\uff0c\u4f4e\u9891\u6210\u5206\u4f18\u5316\u5168\u5c40\u7ed3\u6784\u548c\u9ad8\u65af\u5206\u5e03\uff0c\u9ad8\u9891\u6210\u5206\u6062\u590d\u7ec6\u8282\u5e76\u52a0\u5165\u5149\u7167\u6a21\u5757\uff1b\u540c\u65f6\u5229\u75282D\u5c0f\u6ce2\u5206\u89e3\u6a21\u62df\u8f90\u5c04\u53d8\u5316\uff0c\u6307\u5bfc\u7ec6\u8282\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e863D\u573a\u666f\u91cd\u5efa\u7684\u8d28\u91cf\u4e0e\u6548\u7387\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.12792", "pdf": "https://arxiv.org/pdf/2507.12792", "abs": "https://arxiv.org/abs/2507.12792", "authors": ["Yiliang Wan", "Nitin Shivaraman", "Akshaye Shenoi", "Xiang Liu", "Tao Luo", "Jialin Li"], "title": "Building State Machine Replication Using Practical Network Synchrony", "categories": ["cs.DC"], "comment": "12 pages, 10 figures", "summary": "Distributed systems, such as state machine replication, are critical\ninfrastructures for modern applications. Practical distributed protocols make\nminimum assumptions about the underlying network: They typically assume a\npartially synchronous or fully asynchronous network model. In this work, we\nargue that modern data center systems can be designed to provide strong\nsynchrony properties in the common case, where servers move in synchronous\nlock-step rounds. We prove this hypothesis by engineering a practical design\nthat uses a combination of kernel-bypass network, multithreaded architecture,\nand loosened round length, achieving a tight round bound under 2us. Leveraging\nour engineered networks with strong synchrony, we co-design a new replication\nprotocol, Chora. Chora exploits the network synchrony property to efficiently\npipeline multiple replication instances, while allowing all replicas to propose\nin parallel without extra coordination. Through experiments, we show that Chora\nachieves 255% and 109% improvement in throughput over state-of-the-art\nsingle-leader and multi-leader protocols, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u5f3a\u540c\u6b65\u6027\u7684\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f18\u5316\u7f51\u7edc\u548c\u67b6\u6784\uff0c\u5f00\u53d1\u4e86\u65b0\u590d\u5236\u534f\u8baeChora\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u4ee3\u5206\u5e03\u5f0f\u534f\u8bae\u901a\u5e38\u5047\u8bbe\u7f51\u7edc\u662f\u90e8\u5206\u540c\u6b65\u6216\u5b8c\u5168\u5f02\u6b65\u7684\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u6570\u636e\u4e2d\u5fc3\u53ef\u901a\u8fc7\u8bbe\u8ba1\u5b9e\u73b0\u5f3a\u540c\u6b65\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u5185\u6838\u65c1\u8def\u7f51\u7edc\u3001\u591a\u7ebf\u7a0b\u67b6\u6784\u548c\u5bbd\u677e\u7684\u8f6e\u6b21\u957f\u5ea6\uff0c\u8bbe\u8ba1\u51fa\u540c\u6b65\u6027\u5f3a\u7684\u7f51\u7edc\uff0c\u5e76\u5f00\u53d1\u4e86\u65b0\u7684\u590d\u5236\u534f\u8baeChora\u3002", "result": "Chora\u5728\u5b9e\u9a8c\u4e2d\u6bd4\u5355\u9886\u5bfc\u8005\u548c\u591a\u9886\u5bfc\u8005\u534f\u8bae\u7684\u541e\u5410\u91cf\u5206\u522b\u63d0\u5347\u4e86255%\u548c109%\u3002", "conclusion": "\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u53ef\u901a\u8fc7\u5f3a\u540c\u6b65\u6027\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0f\u534f\u8bae\u7684\u6548\u7387\u3002"}}
{"id": "2507.12626", "pdf": "https://arxiv.org/pdf/2507.12626", "abs": "https://arxiv.org/abs/2507.12626", "authors": ["Andrew G. Moore", "Zachary Richey", "Isaac K. Martin"], "title": "Geometric Theory of Ising Machines", "categories": ["cs.ET", "cond-mat.dis-nn"], "comment": "26 pages, 11 figures", "summary": "We contribute to the mathematical theory of the design of low temperature\nIsing machines, a type of experimental probabilistic computing device\nimplementing the Ising model. Encoding the output of a function in the ground\nstate of a physical system allows efficient and distributed computation, but\nthe design of the energy function is a difficult puzzle. We introduce a\ndiagrammatic device that allows us to visualize the decision boundaries for\nIsing circuits. It is then used to prove two results: (1) Ising circuits are a\ngeneralization of 1-NN classifiers with a certain special structure, and (2)\nElimination of local minima in the energy landscape can be formulated as a\nlinear programming problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f4e\u6e29Ising\u673a\u5668\u7684\u6570\u5b66\u8bbe\u8ba1\u7406\u8bba\uff0c\u5305\u62ec\u53ef\u89c6\u5316\u51b3\u7b56\u8fb9\u754c\u7684\u56fe\u89e3\u5de5\u5177\uff0c\u5e76\u8bc1\u660eIsing\u7535\u8def\u662f1-NN\u5206\u7c7b\u5668\u7684\u63a8\u5e7f\uff0c\u4ee5\u53ca\u5c40\u90e8\u6781\u5c0f\u503c\u6d88\u9664\u53ef\u8f6c\u5316\u4e3a\u7ebf\u6027\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u4f4e\u6e29Ising\u673a\u5668\u7684\u6570\u5b66\u8bbe\u8ba1\u7406\u8bba\uff0c\u4ee5\u89e3\u51b3\u80fd\u91cf\u51fd\u6570\u8bbe\u8ba1\u96be\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u5206\u5e03\u5f0f\u8ba1\u7b97\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u56fe\u89e3\u5de5\u5177\u53ef\u89c6\u5316Ising\u7535\u8def\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u5e76\u5e94\u7528\u4e8e\u4e24\u4e2a\u7406\u8bba\u8bc1\u660e\u3002", "result": "\u8bc1\u660eIsing\u7535\u8def\u662f1-NN\u5206\u7c7b\u5668\u7684\u63a8\u5e7f\uff0c\u4e14\u5c40\u90e8\u6781\u5c0f\u503c\u6d88\u9664\u95ee\u9898\u53ef\u8f6c\u5316\u4e3a\u7ebf\u6027\u89c4\u5212\u3002", "conclusion": "\u56fe\u89e3\u5de5\u5177\u548c\u7406\u8bba\u7ed3\u679c\u4e3aIsing\u673a\u5668\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u5b66\u57fa\u7840\u548c\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.12471", "pdf": "https://arxiv.org/pdf/2507.12471", "abs": "https://arxiv.org/abs/2507.12471", "authors": ["Petr Kourzanov", "Anmol"], "title": "Modular SAIL: dream or reality?", "categories": ["cs.AR"], "comment": null, "summary": "In order to truly benefit from RISC-V ISA modularity, the community has to\naddress the issue of compositionality, going beyond modules at the\nspecification level covering larger subsets of the RISC-V development flow\nincluding emulation, simulation and verification. In this paper we introduce\nmodular SAIL, an experiment to inject compositionality into the SAIL-RISCV\ngolden model. We show that it is, in principle, not difficult to adapt the\nSAIL-RISCV flow (and ideally the SAIL compiler itself) to support modules at\nthe emulator level. We back our findings by a comparative study of the\nresulting pluggable emulator's performance using both static and dynamic\nbinding, which both exhibit same functional behavior as the original monolithic\nemulator (aka RISC-V ISS).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5728RISC-V\u5f00\u53d1\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u6a21\u5757\u5316\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u5757\u5316SAIL\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3RISC-V ISA\u6a21\u5757\u5316\u7684\u7ec4\u5408\u6027\u95ee\u9898\uff0c\u8986\u76d6\u66f4\u5e7f\u6cdb\u7684\u5f00\u53d1\u751f\u6001\uff0c\u5982\u4eff\u771f\u3001\u6a21\u62df\u548c\u9a8c\u8bc1\u3002", "method": "\u5f15\u5165\u4e86\u6a21\u5757\u5316SAIL\uff0c\u6539\u9020SAIL-RISCV\u9ec4\u91d1\u6a21\u578b\u4ee5\u652f\u6301\u6a21\u5757\u5316\uff0c\u5e76\u5728\u6a21\u62df\u5668\u5c42\u9762\u5b9e\u73b0\u9759\u6001\u548c\u52a8\u6001\u7ed1\u5b9a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u5757\u5316SAIL\u6a21\u62df\u5668\u7684\u529f\u80fd\u884c\u4e3a\u4e0e\u539f\u59cb\u6574\u4f53\u5f0f\u6a21\u62df\u5668\u4e00\u81f4\uff0c\u4e14\u6027\u80fd\u8868\u73b0\u76f8\u8fd1\u3002", "conclusion": "\u6a21\u5757\u5316SAIL\u5728\u6280\u672f\u4e0a\u53ef\u884c\uff0c\u4e3aRISC-V\u751f\u6001\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2507.12504", "pdf": "https://arxiv.org/pdf/2507.12504", "abs": "https://arxiv.org/abs/2507.12504", "authors": ["Vito Chan", "Lennart Ebert", "Paul-Julius Hillmann", "Christoffer Rubensson", "Stephan A. Fahrenkrog-Petersen", "Jan Mendling"], "title": "Transforming Football Data into Object-centric Event Logs with Spatial Context Information", "categories": ["cs.DB", "cs.AI"], "comment": "Accepted for the 3rd Workshop on Object-centric processes from A to Z\n  (co-locatedOBJECTS 2025) with BPM 2025", "summary": "Object-centric event logs expand the conventional single-case notion event\nlog by considering multiple objects, allowing for the analysis of more complex\nand realistic process behavior. However, the number of real-world\nobject-centric event logs remains limited, and further studies are needed to\ntest their usefulness. The increasing availability of data from team sports can\nfacilitate object-centric process mining, leveraging both real-world data and\nsuitable use cases. In this paper, we present a framework for transforming\nfootball (soccer) data into an object-centric event log, further enhanced with\na spatial dimension. We demonstrate the effectiveness of our framework by\ngenerating object-centric event logs based on real-world football data and\ndiscuss the results for varying process representations. With our paper, we\nprovide the first example for object-centric event logs in football analytics.\nFuture work should consider variant analysis and filtering techniques to better\nhandle variability", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8db3\u7403\u6570\u636e\u8f6c\u5316\u4e3a\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u65e5\u5fd7\u7684\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5355\u6848\u4f8b\u4e8b\u4ef6\u65e5\u5fd7\u65e0\u6cd5\u5206\u6790\u590d\u6742\u548c\u771f\u5b9e\u7684\u6d41\u7a0b\u884c\u4e3a\uff0c\u4e14\u73b0\u5b9e\u4e2d\u6b64\u7c7b\u65e5\u5fd7\u6570\u91cf\u6709\u9650\uff0c\u9700\u8981\u66f4\u591a\u7814\u7a76\u9a8c\u8bc1\u5176\u7528\u9014\u3002\u56e2\u961f\u8fd0\u52a8\u6570\u636e\u7684\u53ef\u7528\u6027\u4e3a\u5bf9\u8c61\u4e2d\u5fc3\u6d41\u7a0b\u6316\u6398\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u5c06\u8db3\u7403\u6570\u636e\u8f6c\u5316\u4e3a\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u5e76\u589e\u52a0\u4e86\u7a7a\u95f4\u7ef4\u5ea6\u3002\u57fa\u4e8e\u771f\u5b9e\u8db3\u7403\u6570\u636e\u751f\u6210\u65e5\u5fd7\uff0c\u5e76\u8ba8\u8bba\u4e86\u4e0d\u540c\u6d41\u7a0b\u8868\u793a\u7684\u7ed3\u679c\u3002", "result": "\u6846\u67b6\u6210\u529f\u751f\u6210\u4e86\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u8db3\u7403\u5206\u6790\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8fd9\u662f\u8db3\u7403\u5206\u6790\u4e2d\u9996\u6b21\u4f7f\u7528\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u65e5\u5fd7\u7684\u793a\u4f8b\uff0c\u672a\u6765\u5de5\u4f5c\u5e94\u5173\u6ce8\u53d8\u4f53\u5206\u6790\u548c\u8fc7\u6ee4\u6280\u672f\u4ee5\u5904\u7406\u53d8\u5f02\u6027\u3002"}}
{"id": "2507.12580", "pdf": "https://arxiv.org/pdf/2507.12580", "abs": "https://arxiv.org/abs/2507.12580", "authors": ["Josephine Beatrice Skovbo Borre", "Malene Gorm Wold", "Sara Kj\u00e6r Rasmussen", "Ilhan Aslan"], "title": "\"How to Explore Biases in Speech Emotion AI with Users?\" A Speech-Emotion-Acting Study Exploring Age and Language Biases", "categories": ["cs.HC"], "comment": "20 pages", "summary": "This study explores how age and language shape the deliberate vocal\nexpression of emotion, addressing underexplored user groups, Teenagers (N = 12)\nand Adults 55+ (N = 12), within speech emotion recognition (SER). While most\nSER systems are trained on spontaneous, monolingual English data, our research\nevaluates how such models interpret intentionally performed emotional speech\nacross age groups and languages (Danish and English). To support this, we\ndeveloped a novel experimental paradigm combining a custom user interface with\na backend for real-time SER prediction and data logging. Participants were\nprompted to hit visual targets in valence-arousal space by deliberately\nexpressing four emotion targets. While limitations include some reliance on\nself-managed voice recordings and inconsistent task execution, the results\nsuggest contrary to expectations, no significant differences between language\nor age groups, and a degree of cross-linguistic and age robustness in model\ninterpretation. Though some limitations in high-arousal emotion recognition\nwere evident. Our qualitative findings highlight the need to move beyond\nsystem-centered accuracy metrics and embrace more inclusive, human-centered SER\nmodels. By framing emotional expression as a goal-directed act and logging the\nreal-time gap between human intent and machine interpretation, we expose the\nrisks of affective misalignment.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5e74\u9f84\u548c\u8bed\u8a00\u5982\u4f55\u5f71\u54cd\u60c5\u611f\u7684\u6709\u610f\u58f0\u97f3\u8868\u8fbe\uff0c\u5bf9\u6bd4\u9752\u5c11\u5e74\u548c55\u5c81\u4ee5\u4e0a\u6210\u5e74\u4eba\u5728\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u5e74\u9f84\u7ec4\u4e2d\u7684\u8868\u73b0\u76f8\u4f3c\u3002", "motivation": "\u63a2\u7d22\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u4e2d\u5e74\u9f84\u548c\u8bed\u8a00\u7684\u5f71\u54cd\uff0c\u586b\u8865\u5bf9\u9752\u5c11\u5e74\u548c\u8001\u5e74\u4eba\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u65b0\u5b9e\u9a8c\u8303\u5f0f\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49\u7528\u6237\u754c\u9762\u548c\u5b9e\u65f6SER\u9884\u6d4b\u540e\u7aef\uff0c\u53c2\u4e0e\u8005\u901a\u8fc7\u8868\u8fbe\u7279\u5b9a\u60c5\u611f\u6765\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8bed\u8a00\u548c\u5e74\u9f84\u7ec4\u95f4\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u6a21\u578b\u8868\u73b0\u5177\u6709\u8de8\u8bed\u8a00\u548c\u5e74\u9f84\u7684\u7a33\u5065\u6027\uff0c\u4f46\u9ad8\u5524\u9192\u60c5\u611f\u8bc6\u522b\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u9700\u8d85\u8d8a\u7cfb\u7edf\u4e3a\u4e2d\u5fc3\u7684\u51c6\u786e\u5ea6\u6307\u6807\uff0c\u91c7\u7528\u66f4\u5305\u5bb9\u7684\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684SER\u6a21\u578b\u3002"}}
{"id": "2507.12558", "pdf": "https://arxiv.org/pdf/2507.12558", "abs": "https://arxiv.org/abs/2507.12558", "authors": ["Tien P. T. Le", "Anh M. T. Bui", "Huy N. D. Pham", "Alessio Bucaioni", "Phuong T. Nguyen"], "title": "When Retriever Meets Generator: A Joint Model for Code Comment Generation", "categories": ["cs.SE"], "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Automatically generating concise, informative comments for source code can\nlighten documentation effort and accelerate program comprehension.\nRetrieval-augmented approaches first fetch code snippets with existing comments\nand then synthesize a new comment, yet retrieval and generation are typically\noptimized in isolation, allowing irrelevant neighbors topropagate noise\ndownstream. To tackle the issue, we propose a novel approach named RAGSum with\nthe aim of both effectiveness and efficiency in recommendations. RAGSum is\nbuilt on top offuse retrieval and generation using a single CodeT5 backbone. We\nreport preliminary results on a unified retrieval-generation framework built on\nCodeT5. A contrastive pre-training phase shapes code embeddings for\nnearest-neighbor search; these weights then seed end-to-end training with a\ncomposite loss that (i) rewards accurate top-k retrieval; and (ii) minimizes\ncomment-generation error. More importantly, a lightweight self-refinement loop\nis deployed to polish the final output. We evaluated theframework on three\ncross-language benchmarks (Java, Python, C), and compared it with three\nwell-established baselines. The results show that our approach substantially\noutperforms thebaselines with respect to BLEU, METEOR, and ROUTE-L. These\nfindings indicate that tightly coupling retrieval and generationcan raise the\nceiling for comment automation and motivateforthcoming replications and\nqualitative developer studies.", "AI": {"tldr": "RAGSum\u662f\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u4ee3\u7801\u6ce8\u91ca\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7CodeT5\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u63a8\u8350\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u5316\u751f\u6210\u4ee3\u7801\u6ce8\u91ca\u53ef\u4ee5\u51cf\u5c11\u6587\u6863\u7f16\u5199\u5de5\u4f5c\u91cf\u5e76\u52a0\u901f\u7a0b\u5e8f\u7406\u89e3\uff0c\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u4e2d\u68c0\u7d22\u4e0e\u751f\u6210\u5206\u79bb\u5bfc\u81f4\u566a\u58f0\u4f20\u64ad\u3002", "method": "\u63d0\u51faRAGSum\u65b9\u6cd5\uff0c\u57fa\u4e8eCodeT5\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u548c\u7aef\u5230\u7aef\u8bad\u7ec3\u7ed3\u5408\u68c0\u7d22\u4e0e\u751f\u6210\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7\u81ea\u4f18\u5316\u5faa\u73af\u3002", "result": "\u5728Java\u3001Python\u3001C\u4e09\u79cd\u8bed\u8a00\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAGSum\u5728BLEU\u3001METEOR\u548cROUTE-L\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7d27\u5bc6\u8026\u5408\u68c0\u7d22\u4e0e\u751f\u6210\u53ef\u4ee5\u63d0\u5347\u6ce8\u91ca\u81ea\u52a8\u5316\u7684\u6027\u80fd\uff0c\u672a\u6765\u9700\u8981\u8fdb\u4e00\u6b65\u590d\u73b0\u548c\u5f00\u53d1\u8005\u5b9a\u6027\u7814\u7a76\u3002"}}
{"id": "2507.13140", "pdf": "https://arxiv.org/pdf/2507.13140", "abs": "https://arxiv.org/abs/2507.13140", "authors": ["Kuiyuan Ding", "Caili Guo", "Yang Yang", "Jianzhang Guo"], "title": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents", "categories": ["cs.NI"], "comment": "6 pages, 7 figures", "summary": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments.", "AI": {"tldr": "RIDAS\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e6G\u7f51\u7edc\u4e2d\u9ad8\u6548\u8d44\u6e90\u5206\u914d\uff0c\u901a\u8fc7\u7ed3\u5408\u8868\u793a\u9a71\u52a8\u4ee3\u7406\u548c\u610f\u56fe\u9a71\u52a8\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u652f\u6301\u6570\u91cf\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u7528\u6237\u610f\u56fe\u4e0e\u4f4e\u5c42\u53c2\u6570\u5316\u914d\u7f6e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6ee1\u8db36G\u7f51\u7edc\u5bf9AI\u96c6\u6210\u7684\u9ad8\u8981\u6c42\u548c\u4e25\u683c\u7684QoS\u9700\u6c42\u3002", "method": "\u63d0\u51faRIDAS\u6846\u67b6\uff0c\u5305\u542b\u8868\u793a\u9a71\u52a8\u4ee3\u7406\uff08RDAs\uff09\u548c\u610f\u56fe\u9a71\u52a8\u4ee3\u7406\uff08IDA\uff09\u3002RDAs\u63d0\u4f9b\u53ef\u8c03\u53c2\u6570\u63a5\u53e3\uff0cIDA\u901a\u8fc7\u4e24\u9636\u6bb5\u89c4\u5212\u65b9\u6848\uff08\u5e26\u5bbd\u9884\u5206\u914d\u548c\u518d\u5206\u914d\uff09\u7ed3\u5408LLM\uff0c\u5c06\u7528\u6237\u610f\u56fe\u548c\u7cfb\u7edf\u72b6\u6001\u6620\u5c04\u4e3a\u6700\u4f18\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRIDAS\u5728\u76f8\u540cQoS\u7ea6\u675f\u4e0b\u652f\u6301\u7684\u7528\u6237\u6570\u91cf\u6bd4WirelessAgent\u591a44.71%\u3002", "conclusion": "RIDAS\u80fd\u6709\u6548\u6355\u6349\u7528\u6237\u610f\u56fe\u5e76\u5728AI RAN\u73af\u5883\u4e2d\u66f4\u9ad8\u6548\u5730\u5206\u914d\u8d44\u6e90\u3002"}}
{"id": "2507.13091", "pdf": "https://arxiv.org/pdf/2507.13091", "abs": "https://arxiv.org/abs/2507.13091", "authors": ["Aur\u00e8le Barri\u00e8re", "Victor Deng", "Cl\u00e9ment Pit-Claudel"], "title": "Formal Verification for JavaScript Regular Expressions: a Proven Semantics and its Applications", "categories": ["cs.PL"], "comment": "25 pages, 3 pages of references, 6 pages of appendix", "summary": "We present the first mechanized, succinct, practical, complete, and\nproven-faithful semantics for a modern regular expression language with\nbacktracking semantics. We ensure its faithfulness by proving it equivalent to\na preexisting line-by-line embedding of the official ECMAScript specification\nof JavaScript regular expressions. We demonstrate its practicality by\npresenting two real-world applications. First, a new notion of contextual\nequivalence for modern regular expressions, which we use to prove or disprove\nrewrites drawn from previous work. Second, the first formal proof of the PikeVM\nalgorithm used in many real-world engines. In contrast with the specification\nand other formalization work, our semantics captures not only the top-priority\nmatch, but a full backtracking tree recording all possible matches and their\nrespective priority. All our definitions and results have been mechanized in\nthe Rocq proof assistant.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u73b0\u4ee3\u6b63\u5219\u8868\u8fbe\u5f0f\u8bed\u8a00\u7684\u673a\u68b0\u5316\u3001\u7b80\u6d01\u3001\u5b9e\u7528\u4e14\u5b8c\u6574\u7684\u8bed\u4e49\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5fe0\u5b9e\u6027\uff0c\u5c55\u793a\u4e86\u4e24\u79cd\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u586b\u8865\u73b0\u4ee3\u6b63\u5219\u8868\u8fbe\u5f0f\u8bed\u8a00\u7f3a\u4e4f\u673a\u68b0\u5316\u3001\u5fe0\u5b9e\u8bed\u4e49\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bc1\u660e\u5176\u4e0eECMAScript\u89c4\u8303\u7684\u7b49\u4ef7\u6027\uff0c\u786e\u4fdd\u8bed\u4e49\u7684\u5fe0\u5b9e\u6027\uff0c\u5e76\u5e94\u7528\u5b9e\u9645\u6848\u4f8b\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "result": "\u5b9e\u73b0\u4e86\u4e24\u79cd\u5b9e\u9645\u5e94\u7528\uff1a\u4e0a\u4e0b\u6587\u7b49\u4ef7\u6027\u9a8c\u8bc1\u548cPikeVM\u7b97\u6cd5\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u3002", "conclusion": "\u6210\u529f\u5b9a\u4e49\u5e76\u673a\u68b0\u5316\u4e86\u4e00\u79cd\u73b0\u4ee3\u6b63\u5219\u8868\u8fbe\u5f0f\u8bed\u8a00\u7684\u5b8c\u6574\u8bed\u4e49\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.13057", "pdf": "https://arxiv.org/pdf/2507.13057", "abs": "https://arxiv.org/abs/2507.13057", "authors": ["Gianluca Curzi", "Lukas Melgaard"], "title": "Cyclic proof theory of positive inductive definitions", "categories": ["cs.LO"], "comment": "27 pages", "summary": "We study cyclic proof systems for $\\mu\\mathsf{PA}$, an extension of Peano\narithmetic by positive inductive definitions that is arithmetically equivalent\nto the (impredicative) subsystem of second-order arithmetic\n$\\Pi^1_2$-$\\mathsf{CA}_0$ by M\\\"{o}llefeld. The main result of this paper is\nthat cyclic and inductive $\\mu\\mathsf{PA}$ have the same proof-theoretic\nstrength. First, we translate cyclic proofs into an annotated variant based on\nSprenger and Dam's systems for first-order $\\mu$-calculus, whose stronger\nvalidity condition allows for a simpler proof of soundness. We then formalise\nthis argument within $\\Pi^1_2$-$\\mathsf{CA}_0$, leveraging M\\\"{o}llerfeld's\nconservativity properties. To this end, we build on prior work by Curzi and Das\non the reverse mathematics of the Knaster-Tarski theorem. As a byproduct of our\nproof methods we show that, despite the stronger validity condition, annotated\nand \"plain\" cyclic proofs for $\\mu\\mathsf{PA}$ prove the same theorems. This\nwork represents a further step in the non-wellfounded proof-theoretic analysis\nof theories of arithmetic via impredicative fragments of second-order\narithmetic, an approach initiated by Simpson's Cyclic Arithmetic, and continued\nby Das and Melgaard in the context of arithmetical inductive definitions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u03bcPA\u7684\u5faa\u73af\u8bc1\u660e\u7cfb\u7edf\uff0c\u53d1\u73b0\u5faa\u73af\u548c\u5f52\u7eb3\u03bcPA\u5177\u6709\u76f8\u540c\u7684\u8bc1\u660e\u8bba\u5f3a\u5ea6\uff0c\u5e76\u901a\u8fc7\u7ffb\u8bd1\u548c\u5f62\u5f0f\u5316\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7ed3\u679c\u3002", "motivation": "\u63a2\u8ba8\u5faa\u73af\u8bc1\u660e\u7cfb\u7edf\u4e0e\u5f52\u7eb3\u8bc1\u660e\u7cfb\u7edf\u5728\u03bcPA\u4e2d\u7684\u7b49\u4ef7\u6027\uff0c\u4ee5\u6269\u5c55\u5bf9\u975e\u826f\u57fa\u8bc1\u660e\u8bba\u7684\u5206\u6790\u3002", "method": "\u5c06\u5faa\u73af\u8bc1\u660e\u7ffb\u8bd1\u4e3a\u5e26\u6ce8\u91ca\u7684\u53d8\u4f53\uff0c\u5229\u7528\u66f4\u5f3a\u7684\u6709\u6548\u6027\u6761\u4ef6\u7b80\u5316\u4e00\u81f4\u6027\u8bc1\u660e\uff0c\u5e76\u5728\u03a0\u2081\u00b2-CA\u2080\u4e2d\u5f62\u5f0f\u5316\u8fd9\u4e00\u8bba\u8bc1\u3002", "result": "\u8bc1\u660e\u4e86\u5faa\u73af\u548c\u5f52\u7eb3\u03bcPA\u7684\u8bc1\u660e\u8bba\u5f3a\u5ea6\u76f8\u540c\uff0c\u4e14\u5e26\u6ce8\u91ca\u4e0e\u666e\u901a\u5faa\u73af\u8bc1\u660e\u5728\u03bcPA\u4e2d\u8bc1\u660e\u76f8\u540c\u7684\u5b9a\u7406\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u975e\u826f\u57fa\u8bc1\u660e\u8bba\u65b9\u6cd5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5faa\u73af\u8bc1\u660e\u7cfb\u7edf\u7684\u5f3a\u5ea6\uff0c\u4e3a\u7b97\u672f\u7406\u8bba\u7684\u8bc1\u660e\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.12571", "pdf": "https://arxiv.org/pdf/2507.12571", "abs": "https://arxiv.org/abs/2507.12571", "authors": ["Haoning Xue", "Brian Nishimine", "Martin Hilbert", "Drew Cingel", "Samantha Vigil", "Jane Shawcroft", "Arti Thakur", "Zubair Shafiq", "Jingwen Zhang"], "title": "Catching Dark Signals in Algorithms: Unveiling Audiovisual and Thematic Markers of Unsafe Content Recommended for Children and Teenagers", "categories": ["cs.CY", "cs.MM"], "comment": null, "summary": "The prevalence of short form video platforms, combined with the\nineffectiveness of age verification mechanisms, raises concerns about the\npotential harms facing children and teenagers in an algorithm-moderated online\nenvironment. We conducted multimodal feature analysis and thematic topic\nmodeling of 4,492 short videos recommended to children and teenagers on\nInstagram Reels, TikTok, and YouTube Shorts, collected as a part of an\nalgorithm auditing experiment. This feature-level and content-level analysis\nrevealed that unsafe (i.e., problematic, mentally distressing) short videos (a)\npossess darker visual features and (b) contain explicitly harmful content and\nimplicit harm from anxiety-inducing ordinary content. We introduce a useful\nframework of online harm (i.e., explicit, implicit, unintended), providing a\nunique lens for understanding the dynamic, multifaceted online risks facing\nchildren and teenagers. The findings highlight the importance of protecting\nyounger audiences in critical developmental stages from both explicit and\nimplicit risks on social media, calling for nuanced content moderation, age\nverification, and platform regulation.", "AI": {"tldr": "\u77ed\u671f\u89c6\u9891\u5e73\u53f0\u5bf9\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u7684\u6f5c\u5728\u5371\u5bb3\u53d7\u5230\u5173\u6ce8\uff0c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u591a\u5e73\u53f0\u63a8\u8350\u5185\u5bb9\uff0c\u63ed\u793a\u4e86\u663e\u6027\u548c\u9690\u6027\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4fdd\u62a4\u63aa\u65bd\u3002", "motivation": "\u7531\u4e8e\u5e74\u9f84\u9a8c\u8bc1\u673a\u5236\u65e0\u6548\uff0c\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u5728\u7b97\u6cd5\u4e3b\u5bfc\u7684\u5728\u7ebf\u73af\u5883\u4e2d\u9762\u4e34\u6f5c\u5728\u5371\u5bb3\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5bf9Instagram Reels\u3001TikTok\u548cYouTube Shorts\u4e0a\u76844,492\u6761\u77ed\u89c6\u9891\u8fdb\u884c\u591a\u6a21\u6001\u7279\u5f81\u5206\u6790\u548c\u4e3b\u9898\u5efa\u6a21\uff0c\u8fdb\u884c\u7b97\u6cd5\u5ba1\u8ba1\u5b9e\u9a8c\u3002", "result": "\u4e0d\u5b89\u5168\u7684\u89c6\u9891\u5177\u6709\u8f83\u6697\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u5305\u542b\u663e\u6027\u7684\u6709\u5bb3\u5185\u5bb9\u548c\u9690\u6027\u7684\u7126\u8651\u8bf1\u5bfc\u5185\u5bb9\u3002\u63d0\u51fa\u4e86\u663e\u6027\u3001\u9690\u6027\u548c\u610f\u5916\u5371\u5bb3\u7684\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u91c7\u53d6\u9488\u5bf9\u6027\u7684\u5185\u5bb9\u5ba1\u6838\u3001\u5e74\u9f84\u9a8c\u8bc1\u548c\u5e73\u53f0\u76d1\u7ba1\u63aa\u65bd\uff0c\u4ee5\u4fdd\u62a4\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u514d\u53d7\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u591a\u7ef4\u5ea6\u98ce\u9669\u3002"}}
{"id": "2507.12879", "pdf": "https://arxiv.org/pdf/2507.12879", "abs": "https://arxiv.org/abs/2507.12879", "authors": ["Yujun Zou", "Nia Qi", "Yingnan Deng", "Zhihao Xue", "Ming Gong", "Wuyang Zhang"], "title": "Autonomous Resource Management in Microservice Systems via Reinforcement Learning", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "This paper proposes a reinforcement learning-based method for microservice\nresource scheduling and optimization, aiming to address issues such as uneven\nresource allocation, high latency, and insufficient throughput in traditional\nmicroservice architectures. In microservice systems, as the number of services\nand the load increase, efficiently scheduling and allocating resources such as\ncomputing power, memory, and storage becomes a critical research challenge. To\naddress this, the paper employs an intelligent scheduling algorithm based on\nreinforcement learning. Through the interaction between the agent and the\nenvironment, the resource allocation strategy is continuously optimized. In the\nexperiments, the paper considers different resource conditions and load\nscenarios, evaluating the proposed method across multiple dimensions, including\nresponse time, throughput, resource utilization, and cost efficiency. The\nexperimental results show that the reinforcement learning-based scheduling\nmethod significantly improves system response speed and throughput under low\nload and high concurrency conditions, while also optimizing resource\nutilization and reducing energy consumption. Under multi-dimensional resource\nconditions, the proposed method can consider multiple objectives and achieve\noptimized resource scheduling. Compared to traditional static resource\nallocation methods, the reinforcement learning model demonstrates stronger\nadaptability and optimization capability. It can adjust resource allocation\nstrategies in real time, thereby maintaining good system performance in\ndynamically changing load and resource environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u670d\u52a1\u8d44\u6e90\u8c03\u5ea6\u4e0e\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u8d44\u6e90\u5206\u914d\u4e0d\u5747\u3001\u9ad8\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5fae\u670d\u52a1\u7cfb\u7edf\u4e2d\u670d\u52a1\u6570\u91cf\u548c\u8d1f\u8f7d\u7684\u589e\u52a0\uff0c\u9ad8\u6548\u8c03\u5ea6\u548c\u5206\u914d\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u5b58\u50a8\u7b49\u8d44\u6e90\u6210\u4e3a\u5173\u952e\u7814\u7a76\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7406\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u4e0d\u65ad\u4f18\u5316\u8d44\u6e90\u5206\u914d\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u8d1f\u8f7d\u548c\u9ad8\u5e76\u53d1\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u54cd\u5e94\u901f\u5ea6\u548c\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u8d44\u6e90\u5229\u7528\u5e76\u964d\u4f4e\u80fd\u8017\u3002", "conclusion": "\u4e0e\u4f20\u7edf\u9759\u6001\u8d44\u6e90\u5206\u914d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u4f18\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5728\u52a8\u6001\u53d8\u5316\u7684\u8d1f\u8f7d\u548c\u8d44\u6e90\u73af\u5883\u4e2d\u5b9e\u65f6\u8c03\u6574\u7b56\u7565\uff0c\u4fdd\u6301\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2507.12904", "pdf": "https://arxiv.org/pdf/2507.12904", "abs": "https://arxiv.org/abs/2507.12904", "authors": ["Rohit Prasad"], "title": "An ultra-low-power CGRA for accelerating Transformers at the edge", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Transformers have revolutionized deep learning with applications in natural\nlanguage processing, computer vision, and beyond. However, their computational\ndemands make it challenging to deploy them on low-power edge devices. This\npaper introduces an ultra-low-power, Coarse-Grained Reconfigurable Array (CGRA)\narchitecture specifically designed to accelerate General Matrix Multiplication\n(GEMM) operations in transformer models tailored for the energy and resource\nconstraints of edge applications. The proposed architecture integrates a 4 x 4\narray of Processing Elements (PEs) for efficient parallel computation and\ndedicated 4 x 2 Memory Operation Blocks (MOBs) for optimized LOAD/STORE\noperations, reducing memory bandwidth demands and enhancing data reuse. A\nswitchless mesh torus interconnect network further minimizes power and latency\nby enabling direct communication between PEs and MOBs, eliminating the need for\ncentralized switching. Through its heterogeneous array design and efficient\ndataflow, this CGRA architecture addresses the unique computational needs of\ntransformers, offering a scalable pathway to deploy sophisticated machine\nlearning models on edge devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u8fb9\u7f18\u8bbe\u5907\u8bbe\u8ba1\u7684\u4f4e\u529f\u8017CGRA\u67b6\u6784\uff0c\u7528\u4e8e\u52a0\u901fTransformer\u6a21\u578b\u4e2d\u7684GEMM\u64cd\u4f5c\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5176\u90e8\u7f72\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f4e\u529f\u8017\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u75284x4 PE\u9635\u5217\u548c4x2 MOB\u6a21\u5757\uff0c\u7ed3\u5408\u65e0\u5f00\u5173\u7f51\u7edc\u8bbe\u8ba1\uff0c\u4f18\u5316\u5e76\u884c\u8ba1\u7b97\u548c\u5185\u5b58\u64cd\u4f5c\u3002", "result": "\u8be5\u67b6\u6784\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5e26\u5bbd\u9700\u6c42\u5e76\u63d0\u5347\u4e86\u6570\u636e\u91cd\u7528\u6548\u7387\u3002", "conclusion": "\u8fd9\u79cdCGRA\u67b6\u6784\u4e3a\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u590d\u6742\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12562", "pdf": "https://arxiv.org/pdf/2507.12562", "abs": "https://arxiv.org/abs/2507.12562", "authors": ["Md. Tanvir Alam", "Md. Ahasanul Alam", "Md Mahmudur Rahman", "Md. Mosaddek Khan"], "title": "Rel-HNN: Split Parallel Hypergraph Neural Network for Learning on Relational Databases", "categories": ["cs.DB", "cs.DC", "cs.LG"], "comment": null, "summary": "Relational databases (RDBs) are ubiquitous in enterprise and real-world\napplications. Flattening the database poses challenges for deep learning models\nthat rely on fixed-size input representations to capture relational semantics\nfrom the structured nature of relational data. Graph neural networks (GNNs)\nhave been proposed to address this, but they often oversimplify relational\nstructures by modeling all the tuples as monolithic nodes and ignoring\nintra-tuple associations. In this work, we propose a novel hypergraph-based\nframework, that we call rel-HNN, which models each unique attribute-value pair\nas a node and each tuple as a hyperedge, enabling the capture of fine-grained\nintra-tuple relationships. Our approach learns explicit multi-level\nrepresentations across attribute-value, tuple, and table levels. To address the\nscalability challenges posed by large RDBs, we further introduce a\nsplit-parallel training algorithm that leverages multi-GPU execution for\nefficient hypergraph learning. Extensive experiments on real-world and\nbenchmark datasets demonstrate that rel-HNN significantly outperforms existing\nmethods in both classification and regression tasks. Moreover, our\nsplit-parallel training achieves substantial speedups -- up to 3.18x for\nlearning on relational data and up to 2.94x for hypergraph learning -- compared\nto conventional single-GPU execution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3arel-HNN\u7684\u65b0\u578b\u8d85\u56fe\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u6355\u83b7\u7ec6\u7c92\u5ea6\u7684\u5173\u8054\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u591a\u7ea7\u8868\u793a\u5b66\u4e60\u548c\u5e76\u884c\u8bad\u7ec3\u7b97\u6cd5\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u5173\u7cfb\u6570\u636e\u5e93\u65f6\u7b80\u5316\u4e86\u5173\u7cfb\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u83b7\u5185\u90e8\u5143\u7ec4\u5173\u8054\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51farel-HNN\u6846\u67b6\uff0c\u5c06\u5c5e\u6027-\u503c\u5bf9\u4f5c\u4e3a\u8282\u70b9\uff0c\u5143\u7ec4\u4f5c\u4e3a\u8d85\u8fb9\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5173\u7cfb\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u591aGPU\u5e76\u884c\u8bad\u7ec3\u7b97\u6cd5\u4ee5\u63d0\u5347\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0crel-HNN\u5728\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u884c\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe3.18\u500d\u3002", "conclusion": "rel-HNN\u901a\u8fc7\u8d85\u56fe\u5efa\u6a21\u548c\u5e76\u884c\u8bad\u7ec3\u6709\u6548\u89e3\u51b3\u4e86\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u7684\u8bed\u4e49\u6355\u83b7\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2507.12621", "pdf": "https://arxiv.org/pdf/2507.12621", "abs": "https://arxiv.org/abs/2507.12621", "authors": ["Kuangshi Ai", "Kaiyuan Tang", "Chaoli Wang"], "title": "NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting", "categories": ["cs.HC", "cs.GR", "cs.MA"], "comment": "IEEE VIS 2025. Project Page: https://nli4volvis.github.io/", "summary": "Traditional volume visualization (VolVis) methods, like direct volume\nrendering, suffer from rigid transfer function designs and high computational\ncosts. Although novel view synthesis approaches enhance rendering efficiency,\nthey require additional learning effort for non-experts and lack support for\nsemantic-level interaction. To bridge this gap, we propose NLI4VolVis, an\ninteractive system that enables users to explore, query, and edit volumetric\nscenes using natural language. NLI4VolVis integrates multi-view semantic\nsegmentation and vision-language models to extract and understand semantic\ncomponents in a scene. We introduce a multi-agent large language model\narchitecture equipped with extensive function-calling tools to interpret user\nintents and execute visualization tasks. The agents leverage external tools and\ndeclarative VolVis commands to interact with the VolVis engine powered by 3D\neditable Gaussians, enabling open-vocabulary object querying, real-time scene\nediting, best-view selection, and 2D stylization. We validate our system\nthrough case studies and a user study, highlighting its improved accessibility\nand usability in volumetric data exploration. We strongly recommend readers\ncheck our case studies, demo video, and source code at\nhttps://nli4volvis.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNLI4VolVis\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5b9e\u73b0\u4f53\u6570\u636e\u63a2\u7d22\u3001\u67e5\u8be2\u548c\u7f16\u8f91\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4f53\u53ef\u89c6\u5316\u65b9\u6cd5\u5728\u4ea4\u4e92\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u4f53\u53ef\u89c6\u5316\u65b9\u6cd5\u5b58\u5728\u4ea4\u4e92\u8bbe\u8ba1\u50f5\u786c\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u65b0\u5174\u65b9\u6cd5\u5219\u9700\u8981\u989d\u5916\u5b66\u4e60\u6210\u672c\u4e14\u7f3a\u4e4f\u8bed\u4e49\u7ea7\u4ea4\u4e92\u652f\u6301\u3002", "method": "NLI4VolVis\u6574\u5408\u591a\u89c6\u89d2\u8bed\u4e49\u5206\u5272\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5229\u7528\u591a\u4ee3\u7406\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u89e3\u6790\u7528\u6237\u610f\u56fe\u5e76\u6267\u884c\u53ef\u89c6\u5316\u4efb\u52a1\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u3001\u5b9e\u65f6\u573a\u666f\u7f16\u8f91\u3001\u6700\u4f73\u89c6\u89d2\u9009\u62e9\u548c2D\u98ce\u683c\u5316\uff0c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u53ef\u7528\u6027\u548c\u6613\u7528\u6027\u63d0\u5347\u3002", "conclusion": "NLI4VolVis\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u4e86\u4f53\u6570\u636e\u63a2\u7d22\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.12561", "pdf": "https://arxiv.org/pdf/2507.12561", "abs": "https://arxiv.org/abs/2507.12561", "authors": ["Samal Nursapa", "Anastassiya Samuilova", "Alessio Bucaioni. Phuong T. Nguyen"], "title": "ROSE: Transformer-Based Refactoring Recommendation for Architectural Smells", "categories": ["cs.SE"], "comment": "The paper has been peer-reviewed and accepted for publication in the\n  proceedings of the 19th ACM/IEEE International Symposium on Empirical\n  Software Engineering and Measurement (ESEM 2025)", "summary": "Architectural smells such as God Class, Cyclic Dependency, and Hub-like\nDependency degrade software quality and maintainability. Existing tools detect\nsuch smells but rarely suggest how to fix them. This paper explores the use of\npre-trained transformer models--CodeBERT and CodeT5--for recommending suitable\nrefactorings based on detected smells. We frame the task as a three-class\nclassification problem and fine-tune both models on over 2 million refactoring\ninstances mined from 11,149 open-source Java projects. CodeT5 achieves 96.9%\naccuracy and 95.2% F1, outperforming CodeBERT and traditional baselines. Our\nresults show that transformer-based models can effectively bridge the gap\nbetween smell detection and actionable repair, laying the foundation for future\nrefactoring recommendation systems. We release all code, models, and data under\nan open license to support reproducibility and further research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u53d8\u6362\u5668\u6a21\u578b\uff08\u5982CodeBERT\u548cCodeT5\uff09\u4e3a\u68c0\u6d4b\u5230\u7684\u67b6\u6784\u5f02\u5473\u63a8\u8350\u5408\u9002\u91cd\u6784\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002CodeT5\u8868\u73b0\u4f18\u4e8eCodeBERT\u548c\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u80fd\u68c0\u6d4b\u67b6\u6784\u5f02\u5473\uff08\u5982God Class\u3001Cyclic Dependency\u7b49\uff09\uff0c\u4f46\u5f88\u5c11\u63d0\u4f9b\u4fee\u590d\u5efa\u8bae\uff0c\u5f71\u54cd\u4e86\u8f6f\u4ef6\u8d28\u91cf\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u4e09\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u572811,149\u4e2a\u5f00\u6e90Java\u9879\u76ee\u7684200\u591a\u4e07\u6761\u91cd\u6784\u5b9e\u4f8b\u4e0a\u5fae\u8c03CodeBERT\u548cCodeT5\u3002", "result": "CodeT5\u8fbe\u523096.9%\u51c6\u786e\u7387\u548c95.2% F1\u5206\u6570\uff0c\u4f18\u4e8eCodeBERT\u548c\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u53d8\u6362\u5668\u6a21\u578b\u80fd\u6709\u6548\u586b\u8865\u5f02\u5473\u68c0\u6d4b\u548c\u5177\u4f53\u4fee\u590d\u5efa\u8bae\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u91cd\u6784\u63a8\u8350\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002\u6240\u6709\u8d44\u6e90\u548c\u6570\u636e\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.12600", "pdf": "https://arxiv.org/pdf/2507.12600", "abs": "https://arxiv.org/abs/2507.12600", "authors": ["Joy Xiaoji Zhang", "Jingsen Zhu", "Hanyu Chen", "Steve Marschner"], "title": "HairFormer: Transformer-Based Dynamic Neural Hair Simulation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Simulating hair dynamics that generalize across arbitrary hairstyles, body\nshapes, and motions is a critical challenge. Our novel two-stage neural\nsolution is the first to leverage Transformer-based architectures for such a\nbroad generalization. We propose a Transformer-powered static network that\npredicts static draped shapes for any hairstyle, effectively resolving\nhair-body penetrations and preserving hair fidelity. Subsequently, a dynamic\nnetwork with a novel cross-attention mechanism fuses static hair features with\nkinematic input to generate expressive dynamics and complex secondary motions.\nThis dynamic network also allows for efficient fine-tuning of challenging\nmotion sequences, such as abrupt head movements. Our method offers real-time\ninference for both static single-frame drapes and dynamic drapes over pose\nsequences. Our method demonstrates high-fidelity and generalizable dynamic hair\nacross various styles, guided by physics-informed losses, and can resolve\npenetrations even for complex, unseen long hairstyles, highlighting its broad\ngeneralization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u4e24\u9636\u6bb5\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u6a21\u62df\u4efb\u610f\u53d1\u578b\u3001\u4f53\u578b\u548c\u8fd0\u52a8\u4e0b\u7684\u5934\u53d1\u52a8\u6001\u6548\u679c\u3002", "motivation": "\u6a21\u62df\u4e0d\u540c\u53d1\u578b\u3001\u4f53\u578b\u548c\u8fd0\u52a8\u4e0b\u7684\u5934\u53d1\u52a8\u6001\u6548\u679c\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6b64\u524d\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u9759\u6001\u7f51\u7edc\u9884\u6d4b\u9759\u6001\u53d1\u578b\uff0c\u89e3\u51b3\u5934\u53d1\u4e0e\u8eab\u4f53\u7a7f\u900f\u95ee\u9898\uff1b\u52a8\u6001\u7f51\u7edc\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u7ed3\u5408\u9759\u6001\u7279\u5f81\u4e0e\u8fd0\u52a8\u8f93\u5165\uff0c\u751f\u6210\u52a8\u6001\u6548\u679c\u3002\u540c\u65f6\u652f\u6301\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u65b9\u6cd5\u5728\u591a\u79cd\u53d1\u578b\u4e0b\u5b9e\u73b0\u9ad8\u4fdd\u771f\u52a8\u6001\u6548\u679c\uff0c\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u5f3a\uff0c\u5e76\u80fd\u5904\u7406\u590d\u6742\u672a\u89c1\u8fc7\u7684\u957f\u53d1\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u4fdd\u771f\u5ea6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5934\u53d1\u52a8\u6001\u6a21\u62df\u573a\u666f\u3002"}}
{"id": "2507.13179", "pdf": "https://arxiv.org/pdf/2507.13179", "abs": "https://arxiv.org/abs/2507.13179", "authors": ["Ziyu Zhong", "Hector A Caltenco", "Bj\u00f6rn Landfeldt", "G\u00fcnter Alce"], "title": "Predictability-Aware Motion Prediction for Edge XR via High-Order Error-State Kalman Filtering", "categories": ["cs.NI", "cs.MM"], "comment": null, "summary": "As 6G networks are developed and defined, offloading of XR applications is\nemerging as one of the strong new use cases. The reduced 6G latency coupled\nwith edge processing infrastructure will for the first time provide a realistic\noffloading scenario in cellular networks where several computationally\nintensive functions, including rendering, can migrate from the user device and\ninto the network. A key advantage of doing so is the lowering of the battery\nneeds in the user devices and the possibility to design new devices with\nsmaller form factors.", "AI": {"tldr": "6G\u7f51\u7edc\u5c06\u4e3aXR\u5e94\u7528\u63d0\u4f9b\u8fb9\u7f18\u8ba1\u7b97\u5378\u8f7d\u7684\u53ef\u80fd\uff0c\u964d\u4f4e\u7528\u6237\u8bbe\u5907\u7535\u6c60\u9700\u6c42\u5e76\u652f\u6301\u66f4\u5c0f\u578b\u8bbe\u5907\u8bbe\u8ba1\u3002", "motivation": "\u63a2\u7d226G\u4f4e\u5ef6\u8fdf\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u7ed3\u5408\u5982\u4f55\u5b9e\u73b0XR\u5e94\u7528\u7684\u9ad8\u6548\u5378\u8f7d\u3002", "method": "\u5229\u75286G\u7684\u4f4e\u5ef6\u8fdf\u7279\u6027\u548c\u8fb9\u7f18\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\uff0c\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u529f\u80fd\uff08\u5982\u6e32\u67d3\uff09\u4ece\u7528\u6237\u8bbe\u5907\u8fc1\u79fb\u5230\u7f51\u7edc\u3002", "result": "\u964d\u4f4e\u4e86\u7528\u6237\u8bbe\u5907\u7684\u7535\u6c60\u9700\u6c42\uff0c\u5e76\u4e3a\u8bbe\u5907\u5c0f\u578b\u5316\u8bbe\u8ba1\u63d0\u4f9b\u53ef\u80fd\u3002", "conclusion": "6G\u7f51\u7edc\u7684\u4f4e\u5ef6\u8fdf\u548c\u8fb9\u7f18\u8ba1\u7b97\u80fd\u529b\u5c06\u4e3aXR\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u7684\u5378\u8f7d\u65b9\u6848\uff0c\u63a8\u52a8\u65b0\u8bbe\u5907\u8bbe\u8ba1\u3002"}}
{"id": "2507.13290", "pdf": "https://arxiv.org/pdf/2507.13290", "abs": "https://arxiv.org/abs/2507.13290", "authors": ["Aaron Councilman", "David Fu", "Aryan Gupta", "Chengxiao Wang", "David Grove", "Yu-Xiong Wang", "Vikram Adve"], "title": "Towards Formal Verification of LLM-Generated Code from Natural Language Prompts", "categories": ["cs.PL", "cs.AI"], "comment": "31 pages, 9 figures", "summary": "In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5f62\u5f0f\u5316\u67e5\u8be2\u8bed\u8a00\uff0c\u9a8c\u8bc1LLM\u751f\u6210\u7684\u4ee3\u7801\u662f\u5426\u6b63\u786e\u5339\u914d\u7528\u6237\u610f\u56fe\uff0c\u5e76\u5728Ansible\u7f16\u7a0b\u8bed\u8a00\u4e2d\u5b9e\u73b0\u8be5\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u6548\u679c\u663e\u8457\u3002", "motivation": "\u4e3a\u89e3\u51b3LLM\u751f\u6210\u4ee3\u7801\u5e38\u542b\u9519\u8bef\u4e14\u7528\u6237\u96be\u4ee5\u68c0\u6d4b\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u5f62\u5f0f\u5316\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u63d0\u5347AI\u4ee3\u7801\u52a9\u624b\u4f53\u9a8c\uff0c\u751a\u81f3\u652f\u6301\u65e0\u7f16\u7a0b\u77e5\u8bc6\u7684\u7528\u6237\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u3002", "method": "\u5f15\u5165\u5f62\u5f0f\u5316\u67e5\u8be2\u8bed\u8a00\u8868\u8fbe\u7528\u6237\u610f\u56fe\uff0c\u5e76\u9a8c\u8bc1LLM\u751f\u6210\u7684\u4ee3\u7801\u662f\u5426\u7b26\u5408\u8be5\u610f\u56fe\u3002\u5728Ansible\u4e2d\u5b9e\u73b0\u7cfb\u7edfAstrogator\uff0c\u5305\u542b\u67e5\u8be2\u8bed\u8a00\u3001\u884c\u4e3a\u6f14\u7b97\u548c\u7b26\u53f7\u89e3\u91ca\u5668\u3002", "result": "\u572821\u4e2a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u9a8c\u8bc1\u5668\u80fd\u6b63\u786e\u9a8c\u8bc183%\u7684\u6b63\u786e\u4ee3\u7801\uff0c\u8bc6\u522b92%\u7684\u9519\u8bef\u4ee3\u7801\u3002", "conclusion": "\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u63d0\u5347LLM\u751f\u6210\u4ee3\u7801\u7684\u53ef\u9760\u6027\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u63d0\u4f9b\u53ef\u80fd\u3002"}}
{"id": "2507.13058", "pdf": "https://arxiv.org/pdf/2507.13058", "abs": "https://arxiv.org/abs/2507.13058", "authors": ["Quentin Aristote"], "title": "Monotone weak distributive laws over the lifted powerset monad in categories of algebras", "categories": ["cs.LO"], "comment": "Preprint of a STACS 2025 paper: contains additional remarks and\n  proofs", "summary": "Noticing the similarity between the monotone weak distributive laws combining\ntwo layers of nondeterminism in sets and in compact Hausdorff spaces, we study\nwhether the latter law can be obtained automatically as a weak lifting of the\nformer. This holds partially, but does not generalize to other categories of\nalgebras: we then characterize when exactly monotone weak distributive laws\nover powerset monads in categories of algebras exist, exhibiting a law\ncombining probabilities and non-determinism in compact Hausdorff spaces and\nshowing on the other hand that such laws do not exist in a lot of other cases.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5355\u8c03\u5f31\u5206\u914d\u5f8b\u5728\u96c6\u5408\u548c\u7d27\u81f4Hausdorff\u7a7a\u95f4\u4e2d\u7684\u76f8\u4f3c\u6027\uff0c\u90e8\u5206\u5b9e\u73b0\u4e86\u81ea\u52a8\u5f31\u63d0\u5347\uff0c\u4f46\u4e0d\u80fd\u63a8\u5e7f\u5230\u5176\u4ed6\u4ee3\u6570\u8303\u7574\u3002\u8bba\u6587\u8fdb\u4e00\u6b65\u523b\u753b\u4e86\u5728\u4ee3\u6570\u8303\u7574\u4e2d\u5e42\u96c6\u5355\u5b50\u4e0a\u5355\u8c03\u5f31\u5206\u914d\u5f8b\u7684\u5b58\u5728\u6761\u4ef6\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u8303\u7574\u4e2d\u5355\u8c03\u5f31\u5206\u914d\u5f8b\u7684\u901a\u7528\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5e42\u96c6\u5355\u5b50\u7684\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u96c6\u5408\u548c\u7d27\u81f4Hausdorff\u7a7a\u95f4\u4e2d\u7684\u5355\u8c03\u5f31\u5206\u914d\u5f8b\uff0c\u5c1d\u8bd5\u81ea\u52a8\u5f31\u63d0\u5347\uff0c\u5e76\u5728\u5176\u4ed6\u4ee3\u6570\u8303\u7574\u4e2d\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "result": "\u53d1\u73b0\u5728\u7d27\u81f4Hausdorff\u7a7a\u95f4\u4e2d\u53ef\u4ee5\u7ed3\u5408\u6982\u7387\u4e0e\u975e\u786e\u5b9a\u6027\uff0c\u4f46\u5728\u8bb8\u591a\u5176\u4ed6\u60c5\u51b5\u4e0b\u6b64\u7c7b\u5206\u914d\u5f8b\u4e0d\u5b58\u5728\u3002", "conclusion": "\u5355\u8c03\u5f31\u5206\u914d\u5f8b\u7684\u5b58\u5728\u6027\u5177\u6709\u7279\u5b9a\u6027\uff0c\u4ec5\u5728\u7279\u5b9a\u8303\u7574\u4e2d\u6210\u7acb\u3002"}}
{"id": "2507.12723", "pdf": "https://arxiv.org/pdf/2507.12723", "abs": "https://arxiv.org/abs/2507.12723", "authors": ["Minyoung Kim", "Sehwan Park", "Sungmin Cha", "Paul Hongsuck Seo"], "title": "Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries", "categories": ["cs.SD", "cs.MM", "eess.AS"], "comment": "5 pages, 2 figures, Interspeech 2025", "summary": "Recent advances in voice cloning and lip synchronization models have enabled\nSynthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are\nmanipulated to mimic a target speaker. This significantly increases the risk of\nmisinformation by making fake content seem real. To address this issue,\nexisting methods detect or localize manipulations but cannot recover the\nauthentic audio that conveys the semantic content of the message. This\nlimitation reduces their effectiveness in combating audiovisual misinformation.\nIn this work, we introduce the task of Authentic Audio Recovery (AAR) and\nTamper Localization in Audio (TLA) from SAVFs and propose a cross-modal\nwatermarking framework to embed authentic audio into visuals before\nmanipulation. This enables AAR, TLA, and a robust defense against\nmisinformation. Extensive experiments demonstrate the strong performance of our\nmethod in AAR and TLA against various manipulations, including voice cloning\nand lip synchronization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5408\u6210\u89c6\u542c\u4f2a\u9020\uff08SAVF\uff09\u7684\u8de8\u6a21\u6001\u6c34\u5370\u6846\u67b6\uff0c\u7528\u4e8e\u6062\u590d\u771f\u5b9e\u97f3\u9891\uff08AAR\uff09\u548c\u5b9a\u4f4d\u7be1\u6539\uff08TLA\uff09\uff0c\u4ee5\u62b5\u5fa1\u865a\u5047\u4fe1\u606f\u7684\u4f20\u64ad\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u68c0\u6d4b\u6216\u5b9a\u4f4d\u7be1\u6539\uff0c\u65e0\u6cd5\u6062\u590d\u4f20\u9012\u6d88\u606f\u8bed\u4e49\u7684\u771f\u5b9e\u97f3\u9891\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u6297\u89c6\u542c\u865a\u5047\u4fe1\u606f\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u6c34\u5370\u6846\u67b6\uff0c\u5728\u7be1\u6539\u524d\u5c06\u771f\u5b9e\u97f3\u9891\u5d4c\u5165\u5230\u89c6\u89c9\u4fe1\u606f\u4e2d\u3002", "result": "\u65b9\u6cd5\u5728\u6062\u590d\u771f\u5b9e\u97f3\u9891\u548c\u5b9a\u4f4d\u7be1\u6539\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u591a\u79cd\u7be1\u6539\u624b\u6bb5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aSAVF\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u865a\u5047\u4fe1\u606f\u7684\u80fd\u529b\u3002"}}
{"id": "2507.12765", "pdf": "https://arxiv.org/pdf/2507.12765", "abs": "https://arxiv.org/abs/2507.12765", "authors": ["Akhil Francis", "Abhi D. Rajagopala", "Norm M. Tubman", "Katherine Klymko", "Kasra Nowrouzi"], "title": "Efficient Classical-Processing of Constant-Depth Time Evolution Circuits in Control Hardware", "categories": ["quant-ph", "cs.ET"], "comment": null, "summary": "Improving quantum algorithms run-time performance involves several strategies\nsuch as reducing the quantum gate counts, decreasing the number of\nmeasurements, advancement in QPU technology for faster gate operations, or\noptimizing the classical processing. This work focuses on the latter,\nspecifically reducing classical processing and compilation time via\nhardware-assisted parameterized circuit execution (PCE) for computing dynamical\nproperties of quantum systems. PCE was previously validated for QCVV protocols,\nwhich leverages structural circuit equivalencies. We demonstrate the\napplicability of this approach to computing dynamical properties of quantum\nmany-body systems using structurally equivalent time evolution circuits,\nspecifically calculating correlation functions of spin models using\nconstant-depth circuits generated via Cartan decomposition. Implementing this\nfor spin-spin correlation functions in Transverse field XY (up to 6-sites) and\nHeisenberg spin models (up to 3-sites), we observed a run-time reduction of up\nto 50\\% compared to standard compilation methods. This highlights the\nadaptability of time-evolution circuit with hardware-assisted PCE to\npotentially mitigate the classical bottlenecks in near-term quantum algorithms.", "AI": {"tldr": "\u901a\u8fc7\u786c\u4ef6\u8f85\u52a9\u53c2\u6570\u5316\u7535\u8def\u6267\u884c\uff08PCE\uff09\u4f18\u5316\u91cf\u5b50\u7b97\u6cd5\u8fd0\u884c\u65f6\u95f4\uff0c\u51cf\u5c11\u7ecf\u5178\u5904\u7406\u548c\u7f16\u8bd1\u65f6\u95f4\uff0c\u5728\u81ea\u65cb\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u8fbe50%\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u964d\u4f4e\u7ecf\u5178\u5904\u7406\u548c\u7f16\u8bd1\u65f6\u95f4\uff0c\u4ee5\u63d0\u5347\u91cf\u5b50\u7b97\u6cd5\u5728\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u4e2d\u7684\u8fd0\u884c\u6548\u7387\u3002", "method": "\u91c7\u7528\u786c\u4ef6\u8f85\u52a9\u53c2\u6570\u5316\u7535\u8def\u6267\u884c\uff08PCE\uff09\u6280\u672f\uff0c\u5229\u7528\u7ed3\u6784\u7b49\u6548\u7684\u65f6\u95f4\u6f14\u5316\u7535\u8def\u8ba1\u7b97\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u6027\u8d28\u3002", "result": "\u5728\u6a2a\u5411\u573aXY\u548cHeisenberg\u81ea\u65cb\u6a21\u578b\u4e2d\uff0c\u8fd0\u884c\u65f6\u95f4\u51cf\u5c11\u4e8650%\u3002", "conclusion": "\u786c\u4ef6\u8f85\u52a9PCE\u6280\u672f\u80fd\u6709\u6548\u7f13\u89e3\u8fd1\u671f\u91cf\u5b50\u7b97\u6cd5\u4e2d\u7684\u7ecf\u5178\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2507.13281", "pdf": "https://arxiv.org/pdf/2507.13281", "abs": "https://arxiv.org/abs/2507.13281", "authors": ["Haniye Mehraban", "Saad Azmeen-ur-Rahman", "John Hu"], "title": "WIP: Turning Fake Chips into Learning Opportunities", "categories": ["cs.AR"], "comment": "This is the accepted version of a paper accepted for presentation at\n  the 2025 IEEE Frontiers in Education Conference (FIE). The final version will\n  be available via IEEE Xplore at:https://ieeexplore.ieee.org/Xplore/home.jsp", "summary": "This work-in-progress paper presents a case study in which counterfeit TL074\noperational amplifiers, discovered in a junior level electronics course, became\nthe basis for a hands on learning experience. Counterfeit integrated circuits\n(IC) are increasingly common, posing a significant threat to the integrity of\nundergraduate electronics laboratories. Instead of simply replacing the\ncounterfeit components, we turned the issue into a teaching moment. Students\nengaged in hands-on diagnostics measuring current, analyzing waveforms, and\ntroubleshooting. By working with fake chip components, they gained deeper\ninsight into analog circuits, supply chain security, and practical engineering.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e00\u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u5c06\u8bfe\u7a0b\u4e2d\u53d1\u73b0\u7684\u5047\u5192TL074\u8fd0\u7b97\u653e\u5927\u5668\u8f6c\u5316\u4e3a\u5b9e\u8df5\u5b66\u4e60\u673a\u4f1a\uff0c\u5e2e\u52a9\u5b66\u751f\u6df1\u5165\u7406\u89e3\u6a21\u62df\u7535\u8def\u548c\u4f9b\u5e94\u94fe\u5b89\u5168\u3002", "motivation": "\u5047\u5192\u96c6\u6210\u7535\u8def\u5728\u672c\u79d1\u7535\u5b50\u5b9e\u9a8c\u5ba4\u4e2d\u8d8a\u6765\u8d8a\u5e38\u89c1\uff0c\u5a01\u80c1\u5b9e\u9a8c\u7684\u5b8c\u6574\u6027\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5b9e\u9645\u95ee\u9898\u63d0\u5347\u5b66\u751f\u7684\u5b9e\u8df5\u80fd\u529b\u3002", "method": "\u5c06\u5047\u5192\u7ec4\u4ef6\u4f5c\u4e3a\u6559\u5b66\u5de5\u5177\uff0c\u8ba9\u5b66\u751f\u901a\u8fc7\u6d4b\u91cf\u7535\u6d41\u3001\u5206\u6790\u6ce2\u5f62\u548c\u6545\u969c\u6392\u67e5\u7b49\u5b9e\u8df5\u64cd\u4f5c\u5b66\u4e60\u3002", "result": "\u5b66\u751f\u901a\u8fc7\u63a5\u89e6\u5047\u5192\u82af\u7247\u7ec4\u4ef6\uff0c\u66f4\u6df1\u5165\u5730\u7406\u89e3\u4e86\u6a21\u62df\u7535\u8def\u3001\u4f9b\u5e94\u94fe\u5b89\u5168\u548c\u5b9e\u9645\u5de5\u7a0b\u95ee\u9898\u3002", "conclusion": "\u5229\u7528\u5b9e\u9645\u95ee\u9898\u4f5c\u4e3a\u6559\u5b66\u8d44\u6e90\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5b66\u751f\u7684\u5b9e\u8df5\u80fd\u529b\u548c\u5de5\u7a0b\u6d1e\u5bdf\u529b\u3002"}}
{"id": "2507.12668", "pdf": "https://arxiv.org/pdf/2507.12668", "abs": "https://arxiv.org/abs/2507.12668", "authors": ["Shuang Liang", "Lili Chen", "Wensheng Gan", "Philip S. Yu", "Shengjie Zhao"], "title": "Targeted Mining of Time-Interval Related Patterns", "categories": ["cs.DB"], "comment": "Preprint. 8 figures, 4 tables", "summary": "Compared to frequent pattern mining, sequential pattern mining emphasizes the\ntemporal aspect and finds broad applications across various fields. However,\nnumerous studies treat temporal events as single time points, neglecting their\ndurations. Time-interval-related pattern (TIRP) mining is introduced to address\nthis issue and has been applied to healthcare analytics, stock prediction, etc.\nTypically, mining all patterns is not only computationally challenging for\naccurate forecasting but also resource-intensive in terms of time and memory.\nTargeting the extraction of time-interval-related patterns based on specific\ncriteria can improve data analysis efficiency and better align with customer\npreferences. Therefore, this paper proposes a novel algorithm called TaTIRP to\ndiscover Targeted Time-Interval Related Patterns. Additionally, we develop\nmultiple pruning strategies to eliminate redundant extension operations,\nthereby enhancing performance on large-scale datasets. Finally, we conduct\nexperiments on various real-world and synthetic datasets to validate the\naccuracy and efficiency of the proposed algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86TaTIRP\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u6316\u6398\u57fa\u4e8e\u7279\u5b9a\u6807\u51c6\u7684\u65f6\u95f4\u95f4\u9694\u76f8\u5173\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408\u526a\u679d\u7b56\u7565\u4f18\u5316\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u5e8f\u5217\u6a21\u5f0f\u6316\u6398\u5ffd\u89c6\u4e8b\u4ef6\u6301\u7eed\u65f6\u95f4\uff0c\u65f6\u95f4\u95f4\u9694\u6a21\u5f0f\uff08TIRP\uff09\u6316\u6398\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u800c\u751f\uff0c\u4f46\u5168\u6a21\u5f0f\u6316\u6398\u6548\u7387\u4f4e\u4e14\u8d44\u6e90\u6d88\u8017\u5927\u3002", "method": "\u63d0\u51faTaTIRP\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u526a\u679d\u7b56\u7565\u51cf\u5c11\u5197\u4f59\u64cd\u4f5c\uff0c\u9488\u5bf9\u7279\u5b9a\u6807\u51c6\u6316\u6398\u65f6\u95f4\u95f4\u9694\u76f8\u5173\u6a21\u5f0f\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u51c6\u786e\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "TaTIRP\u7b97\u6cd5\u901a\u8fc7\u526a\u679d\u7b56\u7565\u663e\u8457\u63d0\u5347\u65f6\u95f4\u95f4\u9694\u6a21\u5f0f\u6316\u6398\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u3002"}}
{"id": "2507.12721", "pdf": "https://arxiv.org/pdf/2507.12721", "abs": "https://arxiv.org/abs/2507.12721", "authors": ["Rui Sheng", "Chuhan Shi", "Sobhan Lotfi", "Shiyi Liu", "Adam Perer", "Huamin Qu", "Furui Cheng"], "title": "Design Patterns of Human-AI Interfaces in Healthcare", "categories": ["cs.HC"], "comment": null, "summary": "Human-AI interfaces play a crucial role in advancing practices and research\nwithin the healthcare domain. However, designing such interfaces presents a\nsubstantial challenge for designers. In this paper, we propose systematic\nguidance for designing human-AI interfaces in typical healthcare scenarios by\nsummarizing the design patterns for presenting and interacting with common\ninformation entities. To deepen our understanding of these 12 design patterns,\nwe interviewed 12 healthcare professionals to explore potential usage scenarios\nand important considerations. Furthermore, we conducted workshops with 14\nparticipants recruited online to evaluate our design patterns. Finally, we\ndiscussed the generalizability of the design patterns to other application\ndomains, the limitations, and the future work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u4e2d\u8bbe\u8ba1\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u7684\u7cfb\u7edf\u6027\u6307\u5bfc\uff0c\u603b\u7ed3\u4e8612\u79cd\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u91c7\u8bbf\u548c\u7814\u8ba8\u4f1a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8bbe\u8ba1\u533b\u7597\u5065\u5eb7\u9886\u57df\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u754c\u9762\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u7cfb\u7edf\u6027\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u91c7\u8bbf12\u540d\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u4e86\u89e3\u4f7f\u7528\u573a\u666f\u548c\u6ce8\u610f\u4e8b\u9879\uff0c\u5e76\u5728\u7ebf\u62db\u52df14\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u8bbe\u8ba1\u6a21\u5f0f\u7684\u7814\u8ba8\u4f1a\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u4e8612\u79cd\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u666e\u9002\u6027\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8bbe\u8ba1\u6a21\u5f0f\u53ef\u4e3a\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u6307\u5bfc\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u6269\u5c55\u548c\u4f18\u5316\u3002"}}
{"id": "2507.12642", "pdf": "https://arxiv.org/pdf/2507.12642", "abs": "https://arxiv.org/abs/2507.12642", "authors": ["Kiana Kheiri", "Aamna Aamir", "Andriy Miranskyy", "Chen Ding"], "title": "QSpark: Towards Reliable Qiskit Code Generation", "categories": ["cs.SE", "cs.AI", "quant-ph"], "comment": null, "summary": "Quantum circuits must be error-resilient, yet LLMs like Granite-20B-Code and\nStarCoder often output flawed Qiskit code. We fine-tuned a 32 B model with two\nRL methods, Group Relative Policy Optimization (GRPO) and Odds-Ratio Preference\nOptimization (ORPO), using a richly annotated synthetic dataset. On the Qiskit\nHumanEval benchmark, ORPO reaches 56.29\\% Pass@1 ($\\approx+10$ pp over\nGranite-8B-QK) and GRPO hits 49\\%, both beating all general-purpose baselines;\non the original HumanEval they score 65.90\\% and 63.00\\%. GRPO excels on basic\ntasks (42/54), ORPO on intermediate ones (41/68), and neither solves the five\nadvanced tasks, highlighting clear gains yet room for progress in AI-assisted\nquantum programming.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08GRPO\u548cORPO\uff09\u4f18\u5316\u91cf\u5b50\u7535\u8def\u7684\u4ee3\u7801\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728Qiskit HumanEval\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u4ecd\u672a\u89e3\u51b3\u9ad8\u7ea7\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u91cf\u5b50\u7535\u8def\u4ee3\u7801\u751f\u6210\u7684\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u5347AI\u5728\u91cf\u5b50\u7f16\u7a0b\u4e2d\u7684\u8f85\u52a9\u80fd\u529b\u3002", "method": "\u4f7f\u7528GRPO\u548cORPO\u4e24\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bf932 B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5229\u7528\u6807\u6ce8\u4e30\u5bcc\u7684\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "ORPO\u5728Qiskit HumanEval\u4e0a\u8fbe\u523056.29% Pass@1\uff0cGRPO\u4e3a49%\uff0c\u5747\u8d85\u8fc7\u901a\u7528\u57fa\u7ebf\uff1b\u5728\u539f\u59cbHumanEval\u4e0a\u5206\u522b\u4e3a65.90%\u548c63.00%\u3002GRPO\u64c5\u957f\u57fa\u7840\u4efb\u52a1\uff0cORPO\u5728\u4e2d\u7ea7\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4e24\u8005\u5747\u672a\u89e3\u51b3\u9ad8\u7ea7\u4efb\u52a1\u3002", "conclusion": "\u5c3d\u7ba1\u5728AI\u8f85\u52a9\u91cf\u5b50\u7f16\u7a0b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u89e3\u51b3\u9ad8\u7ea7\u4efb\u52a1\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2507.12667", "pdf": "https://arxiv.org/pdf/2507.12667", "abs": "https://arxiv.org/abs/2507.12667", "authors": ["Siyuan Yao", "Chaoli Wang"], "title": "VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians", "categories": ["cs.GR"], "comment": null, "summary": "Visualization of large-scale time-dependent simulation data is crucial for\ndomain scientists to analyze complex phenomena, but it demands significant I/O\nbandwidth, storage, and computational resources. To enable effective\nvisualization on local, low-end machines, recent advances in view synthesis\ntechniques, such as neural radiance fields, utilize neural networks to generate\nnovel visualizations for volumetric scenes. However, these methods focus on\nreconstruction quality rather than facilitating interactive visualization\nexploration, such as feature extraction and tracking. We introduce VolSegGS, a\nnovel Gaussian splatting framework that supports interactive segmentation and\ntracking in dynamic volumetric scenes for exploratory visualization and\nanalysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic\nvolumetric scene, allowing for real-time novel view synthesis. For accurate\nsegmentation, we leverage the view-independent colors of Gaussians for\ncoarse-level segmentation and refine the results with an affinity field network\nfor fine-level segmentation. Additionally, by embedding segmentation results\nwithin the Gaussians, we ensure that their deformation enables continuous\ntracking of segmented regions over time. We demonstrate the effectiveness of\nVolSegGS with several time-varying datasets and compare our solutions against\nstate-of-the-art methods. With the ability to interact with a dynamic scene in\nreal time and provide flexible segmentation and tracking capabilities, VolSegGS\noffers a powerful solution under low computational demands. This framework\nunlocks exciting new possibilities for time-varying volumetric data analysis\nand visualization.", "AI": {"tldr": "VolSegGS\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u55b7\u5c04\u7684\u65b0\u6846\u67b6\uff0c\u652f\u6301\u52a8\u6001\u4f53\u79ef\u573a\u666f\u4e2d\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u548c\u8ddf\u8e2a\uff0c\u4e3a\u63a2\u7d22\u6027\u5206\u6790\u548c\u53ef\u89c6\u5316\u63d0\u4f9b\u5b9e\u65f6\u65b0\u89c6\u56fe\u5408\u6210\u529f\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u65f6\u53d8\u6a21\u62df\u6570\u636e\u7684\u53ef\u89c6\u5316\u9700\u8981\u5927\u91cf\u8d44\u6e90\uff0c\u73b0\u6709\u7684\u89c6\u56fe\u5408\u6210\u6280\u672f\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u679c\uff0c\u4f46\u7f3a\u4e4f\u4ea4\u4e92\u6027\uff0c\u4f5c\u8005\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53ef\u53d8\u5f623D\u9ad8\u65af\u8868\u793a\u52a8\u6001\u573a\u666f\uff0c\u7ed3\u5408\u89c6\u56fe\u65e0\u5173\u989c\u8272\u548c\u4eb2\u548c\u529b\u573a\u7f51\u7edc\u5b9e\u73b0\u7cbe\u786e\u5206\u5272\uff0c\u5e76\u5c06\u5206\u5272\u7ed3\u679c\u5d4c\u5165\u9ad8\u65af\u4e2d\u4ee5\u5b9e\u73b0\u8fde\u7eed\u8ddf\u8e2a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVolSegGS\u80fd\u591f\u5b9e\u65f6\u4ea4\u4e92\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u5206\u5272\u548c\u8ddf\u8e2a\u529f\u80fd\uff0c\u5728\u4f4e\u8ba1\u7b97\u9700\u6c42\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VolSegGS\u4e3a\u65f6\u53d8\u4f53\u79ef\u6570\u636e\u7684\u5206\u6790\u548c\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13312", "pdf": "https://arxiv.org/pdf/2507.13312", "abs": "https://arxiv.org/abs/2507.13312", "authors": ["Chiara Schiavo", "Manuele Favero", "Alessandro Buratto", "Leonardo Badia"], "title": "Bidirectional Age of Incorrect Information: A Performance Metric for Status Updates in Virtual Dynamic Environments", "categories": ["cs.NI", "cs.IT", "cs.MM", "math.IT"], "comment": "8 pages, 8 figures, 1 table, Proc. IEEE Metacom", "summary": "Virtual dynamic environments (VDEs) such as the Metaverse and digital twins\n(DTs) require proper representation of the interacting entities to map their\ncharacteristics within the simulated or augmented space. Keeping these\nrepresentations accurate and up-to-date is crucial for seamless interaction and\nsystem reliability. In this paper, we propose bidirectional age of incorrect\ninformation (BAoII) to address this aspect. BAoII quantifies the time-dependent\npenalty paid by an entity in a VDE due to incorrect or outdated knowledge about\nitself and the overall dynamically changing space. This extends the concept of\nage of incorrect information for a bidirectional information exchange,\ncapturing that a VDE requires mutual awareness of the entity's own\nrepresentation, measured in the virtual space, and what the other entities\nshare about their representations. Using a continuous-time Markov chain model,\nwe derive a closed-form expression for long-term BAoII and identify a\ntransmission cost threshold for optimal update strategies. We describe a\ntrade-off between communication cost and information freshness and validate our\nmodel through numerical simulations, demonstrating the impact of BAoII on\nevaluating system performance and highlighting its relevance for real-time\ncollaboration in the Metaverse and DTs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u53cc\u5411\u9519\u8bef\u4fe1\u606f\u5e74\u9f84\uff08BAoII\uff09\u6765\u91cf\u5316\u865a\u62df\u52a8\u6001\u73af\u5883\u4e2d\u56e0\u4fe1\u606f\u8fc7\u65f6\u6216\u9519\u8bef\u5bfc\u81f4\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u60e9\u7f5a\uff0c\u5e76\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u627e\u5230\u6700\u4f18\u66f4\u65b0\u7b56\u7565\u3002", "motivation": "\u865a\u62df\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u4f53\u8868\u5f81\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u89e3\u51b3\u53cc\u5411\u4fe1\u606f\u4ea4\u6362\u7684\u65f6\u6548\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faBAoII\u6982\u5ff5\uff0c\u5229\u7528\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u63a8\u5bfc\u957f\u671fBAoII\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u786e\u5b9a\u6700\u4f18\u66f4\u65b0\u7b56\u7565\u7684\u4f20\u8f93\u6210\u672c\u9608\u503c\u3002", "result": "\u6a21\u578b\u9a8c\u8bc1\u4e86\u901a\u4fe1\u6210\u672c\u4e0e\u4fe1\u606f\u65b0\u9c9c\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u6570\u503c\u6a21\u62df\u663e\u793aBAoII\u5bf9\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "BAoII\u5bf9\u5143\u5b87\u5b99\u548c\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u5b9e\u65f6\u534f\u4f5c\u5177\u6709\u5b9e\u9645\u610f\u4e49\uff0c\u53ef\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2507.12547", "pdf": "https://arxiv.org/pdf/2507.12547", "abs": "https://arxiv.org/abs/2507.12547", "authors": ["Lionel Wong", "Katherine M. Collins", "Lance Ying", "Cedegao E. Zhang", "Adrian Weller", "Tobias Gersternberg", "Timothy O'Donnell", "Alexander K. Lew", "Jacob D. Andreas", "Joshua B. Tenenbaum", "Tyler Brooke-Wilson"], "title": "Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models", "categories": ["cs.CL", "cs.AI", "cs.PL"], "comment": "Presented at CogSci 2025", "summary": "When faced with novel situations, people are able to marshal relevant\nconsiderations from a wide range of background knowledge and put these to use\nin inferences and predictions. What permits us to draw in globally relevant\ninformation and reason over it coherently? Here, we explore the hypothesis that\npeople use a combination of distributed and symbolic representations to\nconstruct bespoke mental models tailored to novel situations. We propose a\ncomputational implementation of this idea -- a ``Model Synthesis Architecture''\n(MSA) -- using language models to implement global relevance-based retrieval\nand model synthesis and probabilistic programs to implement bespoke, coherent\nworld models. We evaluate our MSA as a model of human judgments on a novel\nreasoning dataset. The dataset -- built around a `Model Olympics` domain of\nsports vignettes -- tests models' capacity for human-like, open-ended reasoning\nby requiring (i) judgments about novel causal structures described in language;\n(ii) drawing on large bodies of background knowledge; and (iii) doing both in\nlight of observations that introduce arbitrary novel variables. Our MSA\napproach captures human judgments better than language model-only baselines,\nunder both direct and chain-of-thought generations from the LM that supports\nmodel synthesis. These results suggest that MSAs can be implemented in a way\nthat mirrors people's ability to deliver locally coherent reasoning over\nglobally relevant variables, offering a path to understanding and replicating\nhuman reasoning in open-ended domains.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u5e03\u5f0f\u548c\u7b26\u53f7\u8868\u5f81\u7684\u6a21\u578b\u5408\u6210\u67b6\u6784\uff08MSA\uff09\uff0c\u7528\u4e8e\u6a21\u62df\u4eba\u7c7b\u5728\u65b0\u60c5\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u7eaf\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7ebf\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u5728\u9762\u5bf9\u65b0\u60c5\u5883\u65f6\u5982\u4f55\u6574\u5408\u5e7f\u6cdb\u7684\u80cc\u666f\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u8ba1\u7b97\u6a21\u578b\u6765\u6a21\u62df\u8fd9\u4e00\u80fd\u529b\u3002", "method": "\u63d0\u51faMSA\u67b6\u6784\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5168\u5c40\u76f8\u5173\u6027\u68c0\u7d22\u548c\u6a21\u578b\u5408\u6210\uff0c\u4f7f\u7528\u6982\u7387\u7a0b\u5e8f\u6784\u5efa\u5b9a\u5236\u5316\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4e00\u4e2a\u65b0\u9896\u7684\u63a8\u7406\u6570\u636e\u96c6\uff08Model Olympics\uff09\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "result": "MSA\u5728\u6a21\u62df\u4eba\u7c7b\u5224\u65ad\u65b9\u9762\u4f18\u4e8e\u7eaf\u8bed\u8a00\u6a21\u578b\u57fa\u7ebf\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65b0\u56e0\u679c\u7ed3\u6784\u548c\u80cc\u666f\u77e5\u8bc6\u65f6\u3002", "conclusion": "MSA\u4e3a\u7406\u89e3\u548c\u590d\u5236\u4eba\u7c7b\u5728\u5f00\u653e\u9886\u57df\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.13178", "pdf": "https://arxiv.org/pdf/2507.13178", "abs": "https://arxiv.org/abs/2507.13178", "authors": ["Marcus Gelderie", "Maximilian Luff", "Maximilian Peltzer"], "title": "Impact and Performance of Randomized Test-Generation using Prolog", "categories": ["cs.LO"], "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "We study randomized generation of sequences of test-inputs to a system using\nProlog. Prolog is a natural fit to generate test-sequences that have complex\nlogical inter-dependent structure. To counter the problems posed by a large (or\ninfinite) set of possible tests, randomization is a natural choice. We study\nthe impact that randomization in conjunction with SLD resolution have on the\ntest performance. To this end, this paper proposes two strategies to add\nrandomization to a test-generating program. One strategy works on top of\nstandard Prolog semantics, whereas the other alters the SLD selection function.\nWe analyze the mean time to reach a test-case, and the mean number of generated\ntest-cases in the framework of Markov chains. Finally, we provide an additional\nempirical evaluation and comparison between both approaches. Under\nconsideration in Theory and Practice of Logic Programming (TPLP).", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u7528Prolog\u968f\u673a\u751f\u6210\u6d4b\u8bd5\u5e8f\u5217\uff0c\u7814\u7a76\u4e86\u968f\u673a\u5316\u548cSLD\u89e3\u6790\u5bf9\u6d4b\u8bd5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u968f\u673a\u7b56\u7565\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6d4b\u8bd5\u8f93\u5165\u5e8f\u5217\u751f\u6210\u7684\u590d\u6742\u903b\u8f91\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u5e94\u5bf9\u5927\u91cf\u6216\u65e0\u9650\u53ef\u80fd\u6d4b\u8bd5\u7684\u6311\u6218\uff0c\u968f\u673a\u5316\u6210\u4e3a\u4e00\u4e2a\u81ea\u7136\u9009\u62e9\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u968f\u673a\u5316\u7b56\u7565\uff1a\u4e00\u79cd\u57fa\u4e8e\u6807\u51c6Prolog\u8bed\u4e49\uff0c\u53e6\u4e00\u79cd\u4fee\u6539SLD\u9009\u62e9\u51fd\u6570\u3002\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u5206\u6790\u5e73\u5747\u65f6\u95f4\u548c\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u7684\u6570\u91cf\u3002", "result": "\u63d0\u4f9b\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u5b9e\u8bc1\u8bc4\u4f30\u548c\u6bd4\u8f83\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\uff0c\u968f\u673a\u5316\u4e0eSLD\u89e3\u6790\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u6d4b\u8bd5\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2507.12932", "pdf": "https://arxiv.org/pdf/2507.12932", "abs": "https://arxiv.org/abs/2507.12932", "authors": ["Zhou Feng", "Jiahao Chen", "Chunyi Zhou", "Yuwen Pu", "Qingming Li", "Tianyu Du", "Shouling Ji"], "title": "Enkidu: Universal Frequential Perturbation for Real-Time Audio Privacy Protection against Voice Deepfakes", "categories": ["cs.SD", "cs.MM"], "comment": "Accepted by ACM MM 2025", "summary": "The rapid advancement of voice deepfake technologies has raised serious\nconcerns about user audio privacy, as attackers increasingly exploit publicly\navailable voice data to generate convincing fake audio for malicious purposes\nsuch as identity theft, financial fraud, and misinformation campaigns. While\nexisting defense methods offer partial protection, they face critical\nlimitations, including weak adaptability to unseen user data, poor scalability\nto long audio, rigid reliance on white-box knowledge, and high computational\nand temporal costs during the encryption process. To address these challenges\nand defend against personalized voice deepfake threats, we propose Enkidu, a\nnovel user-oriented privacy-preserving framework that leverages universal\nfrequential perturbations generated through black-box knowledge and few-shot\ntraining on a small amount of user data. These highly malleable\nfrequency-domain noise patches enable real-time, lightweight protection with\nstrong generalization across variable-length audio and robust resistance to\nvoice deepfake attacks, all while preserving perceptual quality and speech\nintelligibility. Notably, Enkidu achieves over 50 to 200 times processing\nmemory efficiency (as low as 0.004 gigabytes) and 3 to 7000 times runtime\nefficiency (real-time coefficient as low as 0.004) compared to six\nstate-of-the-art countermeasures. Extensive experiments across six mainstream\ntext-to-speech models and five cutting-edge automated speaker verification\nmodels demonstrate the effectiveness, transferability, and practicality of\nEnkidu in defending against both vanilla and adaptive voice deepfake attacks.", "AI": {"tldr": "Enkidu\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u9762\u5411\u7528\u6237\u7684\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u9ed1\u76d2\u77e5\u8bc6\u548c\u5c11\u91cf\u7528\u6237\u6570\u636e\u8bad\u7ec3\u751f\u6210\u901a\u7528\u9891\u57df\u6270\u52a8\uff0c\u5b9e\u65f6\u8f7b\u91cf\u5730\u9632\u5fa1\u4e2a\u6027\u5316\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u5a01\u80c1\u3002", "motivation": "\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u53d1\u5c55\u4e25\u91cd\u5a01\u80c1\u7528\u6237\u97f3\u9891\u9690\u79c1\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5b58\u5728\u9002\u5e94\u6027\u5dee\u3001\u53ef\u6269\u5c55\u6027\u4e0d\u8db3\u3001\u4f9d\u8d56\u767d\u76d2\u77e5\u8bc6\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\u3002", "method": "\u5229\u7528\u9ed1\u76d2\u77e5\u8bc6\u548c\u5c11\u91cf\u7528\u6237\u6570\u636e\u8bad\u7ec3\u751f\u6210\u901a\u7528\u9891\u57df\u566a\u58f0\u8865\u4e01\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u8f7b\u91cf\u7684\u4fdd\u62a4\u3002", "result": "Enkidu\u5728\u5904\u7406\u5185\u5b58\u6548\u7387\uff08\u4f4e\u81f30.004GB\uff09\u548c\u8fd0\u884c\u65f6\u6548\u7387\uff08\u5b9e\u65f6\u7cfb\u6570\u4f4e\u81f30.004\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u516d\u79cd\u4e3b\u6d41TTS\u6a21\u578b\u548c\u4e94\u79cdASV\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u5f3a\u9632\u5fa1\u80fd\u529b\u3002", "conclusion": "Enkidu\u5728\u9632\u5fa1\u666e\u901a\u548c\u81ea\u9002\u5e94\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u653b\u51fb\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3001\u53ef\u8f6c\u79fb\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.12619", "pdf": "https://arxiv.org/pdf/2507.12619", "abs": "https://arxiv.org/abs/2507.12619", "authors": ["Rui Li", "Xiaoyun Zhi", "Jinxin Chi", "Menghan Yu", "Lixin Huang", "Jia Zhu", "Weilun Zhang", "Xing Ma", "Wenjia Liu", "Zhicheng Zhu", "Daowen Luo", "Zuquan Song", "Xin Yin", "Chao Xiang", "Shuguang Wang", "Wencong Xiao", "Gene Cooperman"], "title": "BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "18 pages, 14 figures", "summary": "Large Language Models (LLMs) have become a cornerstone of modern AI, driving\nbreakthroughs in natural language processing and expanding into multimodal jobs\ninvolving images, audio, and video. As with most computational software, it is\nimportant to distinguish between ordinary runtime performance and startup\noverhead. Prior research has focused on runtime performance: improving training\nefficiency and stability. This work focuses instead on the increasingly\ncritical issue of startup overhead in training: the delay before training jobs\nbegin execution. Startup overhead is particularly important in large,\nindustrial-scale LLMs, where failures occur more frequently and multiple teams\noperate in iterative update-debug cycles. In one of our training clusters, more\nthan 3.5% of GPU time is wasted due to startup overhead alone.\n  In this work, we present the first in-depth characterization of LLM training\nstartup overhead based on real production data. We analyze the components of\nstartup cost, quantify its direct impact, and examine how it scales with job\nsize. These insights motivate the design of Bootseer, a system-level\noptimization framework that addresses three primary startup bottlenecks: (a)\ncontainer image loading, (b) runtime dependency installation, and (c) model\ncheckpoint resumption. To mitigate these bottlenecks, Bootseer introduces three\ntechniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and\n(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment\nand evaluated on real LLM training workloads, demonstrating a 50% reduction in\nstartup overhead.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u6df1\u5165\u7814\u7a76\u4e86\u57fa\u4e8e\u5b9e\u9645\u751f\u4ea7\u6570\u636e\u7684LLM\u8bad\u7ec3\u542f\u52a8\u5f00\u9500\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Bootseer\u4f18\u5316\u6846\u67b6\uff0c\u6210\u529f\u51cf\u5c11\u4e8650%\u7684\u542f\u52a8\u5f00\u9500\u3002", "motivation": "\u968f\u7740LLM\u5728\u5de5\u4e1a\u7ea7\u89c4\u6a21\u7684\u666e\u53ca\uff0c\u8bad\u7ec3\u542f\u52a8\u5f00\u9500\uff08\u5982\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d6a\u8d39\uff09\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u7814\u7a76\u8868\u660e\uff0c\u4ec5\u542f\u52a8\u5f00\u9500\u5c31\u5360\u7528\u4e863.5%\u7684GPU\u65f6\u95f4\u3002", "method": "\u901a\u8fc7\u5206\u6790\u542f\u52a8\u5f00\u9500\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aBootseer\u7684\u7cfb\u7edf\u7ea7\u4f18\u5316\u6846\u67b6\uff0c\u91c7\u7528\u70ed\u5757\u8bb0\u5f55\u4e0e\u9884\u53d6\u3001\u4f9d\u8d56\u5feb\u7167\u548cHDFS-FUSE\u6761\u7eb9\u5316\u7b49\u6280\u672f\u3002", "result": "Bootseer\u5728\u771f\u5b9eLLM\u8bad\u7ec3\u8d1f\u8f7d\u4e2d\u90e8\u7f72\u540e\uff0c\u5c06\u542f\u52a8\u5f00\u9500\u51cf\u5c11\u4e8650%\u3002", "conclusion": "\u542f\u52a8\u5f00\u9500\u662fLLM\u8bad\u7ec3\u4e2d\u7684\u91cd\u8981\u95ee\u9898\uff0c\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u5176\u4ed6\u573a\u666f\u3002"}}
{"id": "2507.12935", "pdf": "https://arxiv.org/pdf/2507.12935", "abs": "https://arxiv.org/abs/2507.12935", "authors": ["Shirui Zhao", "Jun Yin", "Lingyun Yao", "Martin Andraud", "Wannes Meert", "Marian Verhelst"], "title": "MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration", "categories": ["cs.LG", "cs.AI", "cs.AR"], "comment": "14 pages, 15 figures, IEEE journal paper", "summary": "An increasing number of applications are exploiting sampling-based algorithms\nfor planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)\nalgorithms form the computational backbone of this emerging branch of machine\nlearning. Unfortunately, the high computational cost limits their feasibility\nfor large-scale problems and real-world applications, and the existing MCMC\nacceleration solutions are either limited in hardware flexibility or fail to\nmaintain efficiency at the system level across a variety of end-to-end\napplications. This paper introduces \\textbf{MC$^2$A}, an algorithm-hardware\nco-design framework, enabling efficient and flexible optimization for MCMC\nacceleration. Firstly, \\textbf{MC$^2$A} analyzes the MCMC workload diversity\nthrough an extension of the processor performance roofline model with a 3rd\ndimension to derive the optimal balance between the compute, sampling and\nmemory parameters. Secondly, \\textbf{MC$^2$A} proposes a parametrized hardware\naccelerator architecture with flexible and efficient support of MCMC kernels\nwith a pipeline of ISA-programmable tree-structured processing units,\nreconfigurable samplers and a crossbar interconnect to support irregular\naccess. Thirdly, the core of \\textbf{MC$^2$A} is powered by a novel Gumbel\nsampler that eliminates exponential and normalization operations. In the\nend-to-end case study, \\textbf{MC$^2$A} achieves an overall {$307.6\\times$,\n$1.4\\times$, $2.0\\times$, $84.2\\times$} speedup compared to the CPU, GPU, TPU\nand state-of-the-art MCMC accelerator. Evaluated on various representative MCMC\nworkloads, this work demonstrates and exploits the feasibility of general\nhardware acceleration to popularize MCMC-based solutions in diverse application\ndomains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMC\u00b2A\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u548c\u7075\u6d3b\u5730\u52a0\u901fMCMC\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MCMC\u52a0\u901f\u65b9\u6848\u5728\u786c\u4ef6\u7075\u6d3b\u6027\u6216\u7cfb\u7edf\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u5904\u7406\u5668\u6027\u80fd\u6a21\u578b\u4e3a3\u7ef4\uff0c\u8bbe\u8ba1\u53ef\u53c2\u6570\u5316\u7684\u786c\u4ef6\u52a0\u901f\u5668\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u65b0\u578bGumbel\u91c7\u6837\u5668\u3002", "result": "MC\u00b2A\u5728CPU\u3001GPU\u3001TPU\u548c\u73b0\u6709MCMC\u52a0\u901f\u5668\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86307.6\u500d\u30011.4\u500d\u30012.0\u500d\u548c84.2\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u8bc1\u660e\u4e86\u901a\u7528\u786c\u4ef6\u52a0\u901f\u7684\u53ef\u884c\u6027\uff0c\u53ef\u63a8\u52a8MCMC\u5728\u591a\u6837\u5316\u5e94\u7528\u4e2d\u7684\u666e\u53ca\u3002"}}
{"id": "2507.12805", "pdf": "https://arxiv.org/pdf/2507.12805", "abs": "https://arxiv.org/abs/2507.12805", "authors": ["Hui Sun", "Yanfeng Ding", "Liping Yi", "Huidong Ma", "Gang Wang", "Xiaoguang Liu", "Cheng Zhong", "Wentong Cai"], "title": "PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "comment": "Accepted via KDD-25", "summary": "Learning-based lossless compressors play a crucial role in large-scale\ngenomic database backup, storage, transmission, and management. However, their\n1) inadequate compression ratio, 2) low compression \\& decompression\nthroughput, and 3) poor compression robustness limit their widespread adoption\nand application in both industry and academia. To solve those challenges, we\npropose a novel \\underline{P}arallel \\underline{M}ulti-\\underline{K}nowledge\n\\underline{L}earning-based \\underline{C}ompressor (PMKLC) with four crucial\ndesigns: 1) We propose an automated multi-knowledge learning-based compression\nframework as compressors' backbone to enhance compression ratio and robustness;\n2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression\nthroughput and computing resource usage; 3) we introduce data block\npartitioning and Step-wise Model Passing (SMP) mechanisms for parallel\nacceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet\nthe complex application scenarios, where the former runs on a\nresource-constrained single GPU and the latter is multi-GPU accelerated. We\nbenchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15\nreal-world datasets with different species and data sizes. Compared to\nbaselines on the testing datasets, PMKLC-S/M achieve the average compression\nratio improvement up to 73.609\\% and 73.480\\%, the average throughput\nimprovement up to 3.036$\\times$ and 10.710$\\times$, respectively. Besides,\nPMKLC-S/M also achieve the best robustness and competitive memory cost,\nindicating its greater stability against datasets with different probability\ndistribution perturbations, and its strong ability to run on memory-constrained\ndevices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e76\u884c\u591a\u77e5\u8bc6\u5b66\u4e60\u7684\u65b0\u578b\u538b\u7f29\u5668\uff08PMKLC\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5b66\u4e60\u578b\u65e0\u635f\u538b\u7f29\u5668\u5728\u538b\u7f29\u6bd4\u3001\u541e\u5410\u91cf\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u578b\u65e0\u635f\u538b\u7f29\u5668\u5728\u5927\u89c4\u6a21\u57fa\u56e0\u7ec4\u6570\u636e\u5e93\u5e94\u7528\u4e2d\u5b58\u5728\u538b\u7f29\u6bd4\u4e0d\u8db3\u3001\u541e\u5410\u91cf\u4f4e\u548c\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "PMKLC\u91c7\u7528\u81ea\u52a8\u5316\u591a\u77e5\u8bc6\u5b66\u4e60\u6846\u67b6\u3001GPU\u52a0\u901f\u7684(s,k)-mer\u7f16\u7801\u3001\u6570\u636e\u5757\u5206\u533a\u548c\u9010\u6b65\u6a21\u578b\u4f20\u9012\u673a\u5236\uff0c\u652f\u6301\u5355GPU\u548c\u591aGPU\u6a21\u5f0f\u3002", "result": "\u572815\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cPMKLC-S/M\u5e73\u5747\u538b\u7f29\u6bd4\u63d0\u534773%\u4ee5\u4e0a\uff0c\u541e\u5410\u91cf\u63d0\u53473-10\u500d\uff0c\u5e76\u8868\u73b0\u51fa\u6700\u4f73\u9c81\u68d2\u6027\u548c\u5185\u5b58\u6548\u7387\u3002", "conclusion": "PMKLC\u5728\u538b\u7f29\u6027\u80fd\u3001\u541e\u5410\u91cf\u548c\u9c81\u68d2\u6027\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u5408\u590d\u6742\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.12734", "pdf": "https://arxiv.org/pdf/2507.12734", "abs": "https://arxiv.org/abs/2507.12734", "authors": ["Nina Errey", "Yi Chen", "Yu Dong", "Quang Vinh Nguyen", "Xiaoru Yuan", "Tuck Wah Leong", "Christy Jie Liang"], "title": "An Age-based Study into Interactive Narrative Visualization Engagement", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Research has shown that an audiences' age impacts their engagement in digital\nmedia. Interactive narrative visualization is an increasingly popular form of\ndigital media that combines data visualization and storytelling to convey\nimportant information. However, audience age is often overlooked by interactive\nnarrative visualization authors. Using an established visualization engagement\nquestionnaire, we ran an empirical experiment where we compared end-user\nengagement to audience age. We found a small difference in engagement scores\nwhere older age cohorts were less engaged than the youngest age cohort. Our\nqualitative analysis revealed that the terminology and overall understanding of\ninteractive narrative patterns integrated into narrative visualization was more\napparent in the feedback from younger age cohorts relative to the older age\ncohorts. We conclude this paper with a series of recommendations for authors of\ninteractive narrative visualization on how to design inclusively for audiences\naccording to their age.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u53d7\u4f17\u5e74\u9f84\u5f71\u54cd\u5176\u5bf9\u6570\u5b57\u5a92\u4f53\u7684\u53c2\u4e0e\u5ea6\u3002\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u53ef\u89c6\u5316\u7ed3\u5408\u4e86\u6570\u636e\u53ef\u89c6\u5316\u548c\u8bb2\u6545\u4e8b\uff0c\u4f46\u5e74\u9f84\u56e0\u7d20\u5e38\u88ab\u5ffd\u89c6\u3002\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5e74\u8f7b\u7fa4\u4f53\u6bd4\u5e74\u957f\u7fa4\u4f53\u66f4\u6295\u5165\uff0c\u7406\u89e3\u66f4\u597d\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u5bb9\u6027\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u63a2\u8ba8\u53d7\u4f17\u5e74\u9f84\u5982\u4f55\u5f71\u54cd\u5bf9\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u53ef\u89c6\u5316\u7684\u53c2\u4e0e\u5ea6\uff0c\u586b\u8865\u5e74\u9f84\u56e0\u7d20\u5728\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u5df2\u5efa\u7acb\u7684\u53ef\u89c6\u5316\u53c2\u4e0e\u5ea6\u95ee\u5377\uff0c\u8fdb\u884c\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u5e74\u9f84\u7fa4\u4f53\u7684\u53c2\u4e0e\u5ea6\u5dee\u5f02\uff0c\u5e76\u7ed3\u5408\u5b9a\u6027\u5206\u6790\u3002", "result": "\u5e74\u957f\u7fa4\u4f53\u7684\u53c2\u4e0e\u5ea6\u7565\u4f4e\u4e8e\u5e74\u8f7b\u7fa4\u4f53\uff1b\u5e74\u8f7b\u7fa4\u4f53\u5bf9\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u6a21\u5f0f\u7684\u7406\u89e3\u66f4\u660e\u663e\u3002", "conclusion": "\u9488\u5bf9\u4e0d\u540c\u5e74\u9f84\u7fa4\u4f53\uff0c\u63d0\u51fa\u4e86\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u53ef\u89c6\u5316\u7684\u5305\u5bb9\u6027\u8bbe\u8ba1\u5efa\u8bae\u3002"}}
{"id": "2507.12649", "pdf": "https://arxiv.org/pdf/2507.12649", "abs": "https://arxiv.org/abs/2507.12649", "authors": ["Christine van Stiphoudt", "Sergio Potenciano Menci", "Gilbert Fridgen"], "title": "A Three-Phase Evaluation Approach for new Information and Data Models in the Smart Grid Domain", "categories": ["cs.SE"], "comment": null, "summary": "The ongoing digitalisation of the smart grid is resulting in an increase in\nautomated information exchanges across distributed energy systems. This process\nhas led to the development of new information and data models when the existing\nones fall short. To prevent potential disruptions caused by flaws in the newly\ndesigned information and data models, it is essential to evaluate them during\nthe design process before they are implemented in operation.\n  Currently, general explicit evaluation approaches outside the smart grid\ndomain stay at a high level without defining clear steps. Meanwhile, implicit\nevaluation approaches in the smart grid domain focus on testing systems that\nutilise information and data models already in use for functionality in terms\nof conformance and interoperability. Notably, no combination of explicit and\nimplicit evaluation approaches for newly designed information and data models\noffers a clearly defined set of steps during their design process in the smart\ngrid context.\n  Consequently, we design a three-phase evaluation approach using design\nscience research to address this gap. Our evaluation approach combines explicit\nand implicit evaluation methods and is applicable when developing new\ninformation and data models. We use the development of an information model and\ndata model focused on industrial flexibility descriptions to refine our\nevaluation approach. Additionally, we provide lessons learned from our\nexperience.", "AI": {"tldr": "\u6458\u8981\u8ba8\u8bba\u4e86\u667a\u80fd\u7535\u7f51\u6570\u5b57\u5316\u8fdb\u7a0b\u4e2d\u65b0\u5174\u4fe1\u606f\u4e0e\u6570\u636e\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u8bc4\u4f30\u7684\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u667a\u80fd\u7535\u7f51\u6570\u5b57\u5316\u5bfc\u81f4\u4fe1\u606f\u4ea4\u6362\u589e\u52a0\uff0c\u73b0\u6709\u6a21\u578b\u4e0d\u8db3\u9700\u8981\u65b0\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u9632\u6b62\u6f5c\u5728\u95ee\u9898\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u7b3c\u7edf\uff0c\u8981\u4e48\u4ec5\u9488\u5bf9\u5df2\u4f7f\u7528\u6a21\u578b\u7684\u6d4b\u8bd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u5408\u663e\u5f0f\u548c\u9690\u5f0f\u8bc4\u4f30\u7684\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\uff0c\u5e76\u5728\u5de5\u4e1a\u7075\u6d3b\u6027\u63cf\u8ff0\u7684\u4fe1\u606f\u548c\u6570\u636e\u6a21\u578b\u5f00\u53d1\u4e2d\u8fdb\u884c\u4e86\u5e94\u7528\u548c\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u65b0\u6a21\u578b\u5f00\u53d1\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u603b\u7ed3\u4e86\u7ecf\u9a8c\u6559\u8bad\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u667a\u80fd\u7535\u7f51\u9886\u57df\u65b0\u6a21\u578b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2507.12489", "pdf": "https://arxiv.org/pdf/2507.12489", "abs": "https://arxiv.org/abs/2507.12489", "authors": ["Richard Marcus", "Marc Stamminger"], "title": "Physically Based Neural LiDAR Resimulation", "categories": ["cs.RO", "cs.CV", "cs.GR", "eess.IV"], "comment": "Accepted at ITSC 2025, Gold Coast Australia", "summary": "Methods for Novel View Synthesis (NVS) have recently found traction in the\nfield of LiDAR simulation and large-scale 3D scene reconstruction. While\nsolutions for faster rendering or handling dynamic scenes have been proposed,\nLiDAR specific effects remain insufficiently addressed. By explicitly modeling\nsensor characteristics such as rolling shutter, laser power variations, and\nintensity falloff, our method achieves more accurate LiDAR simulation compared\nto existing techniques. We demonstrate the effectiveness of our approach\nthrough quantitative and qualitative comparisons with state-of-the-art methods,\nas well as ablation studies that highlight the importance of each sensor model\ncomponent. Beyond that, we show that our approach exhibits advanced\nresimulation capabilities, such as generating high resolution LiDAR scans in\nthe camera perspective.\n  Our code and the resulting dataset are available at\nhttps://github.com/richardmarcus/PBNLiDAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bLiDAR\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7\u660e\u786e\u5efa\u6a21\u4f20\u611f\u5668\u7279\u6027\uff08\u5982\u6eda\u52a8\u5feb\u95e8\u3001\u6fc0\u5149\u529f\u7387\u53d8\u5316\u7b49\uff09\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u51c6\u786e\u7684LiDAR\u6a21\u62df\u3002", "motivation": "\u73b0\u6709LiDAR\u6a21\u62df\u5728\u4f20\u611f\u5668\u7279\u5b9a\u6548\u679c\uff08\u5982\u6eda\u52a8\u5feb\u95e8\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u6a21\u62df\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u4f20\u611f\u5668\u7279\u6027\uff08\u5982\u6eda\u52a8\u5feb\u95e8\u3001\u6fc0\u5149\u529f\u7387\u53d8\u5316\uff09\u6765\u6539\u8fdbLiDAR\u6a21\u62df\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u751f\u6210\u9ad8\u5206\u8fa8\u7387LiDAR\u626b\u63cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u5347\u4e86LiDAR\u6a21\u62df\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u5c55\u793a\u4e86\u9ad8\u6548\u7684\u91cd\u6a21\u62df\u80fd\u529b\u3002"}}
{"id": "2507.12850", "pdf": "https://arxiv.org/pdf/2507.12850", "abs": "https://arxiv.org/abs/2507.12850", "authors": ["Wenzheng Kong", "Wenyi Zhang"], "title": "Learning-Based Interface for Semantic Communication with Bit Importance Awareness", "categories": ["cs.IT", "cs.NI", "math.IT"], "comment": null, "summary": "Joint source-channel coding (JSCC) is an effective approach for semantic\ncommunication. However, current JSCC methods are difficult to integrate with\nexisting communication network architectures, where application and network\nproviders are typically different entities. Recently, a novel paradigm termed\nSplit DeepJSCC has been under consideration to address this challenge. Split\nDeepJSCC employs a bit-level interface that enables separate design of source\nand channel codes, ensuring compatibility with existing communication networks\nwhile preserving the advantages of JSCC in terms of semantic fidelity and\nchannel adaptability. In this paper, we propose a learning-based interface\ndesign by treating its parameters as trainable, achieving improved end-to-end\nperformance compared to Split DeepJSCC. In particular, the interface enables\nspecification of bit-level importance at the output of the source code.\nFurthermore, we propose an Importance-Aware Net that utilizes the\ninterface-derived bit importance information, enabling dynamical adaptation to\ndiverse channel bandwidth ratios and time-varying channel conditions.\nExperimental results show that our method improves performance in wireless\nimage transmission tasks. This work provides a potential solution for realizing\nsemantic communications in existing wireless networks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6bd4\u7279\u7ea7\u63a5\u53e3\u8bbe\u8ba1\uff0c\u6539\u8fdbSplit DeepJSCC\u7684\u7aef\u5230\u7aef\u6027\u80fd\uff0c\u5e76\u8bbe\u8ba1\u91cd\u8981\u6027\u611f\u77e5\u7f51\u7edc\u4ee5\u52a8\u6001\u9002\u5e94\u4fe1\u9053\u6761\u4ef6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709JSCC\u65b9\u6cd5\u96be\u4ee5\u4e0e\u901a\u4fe1\u7f51\u7edc\u67b6\u6784\u517c\u5bb9\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\u4fdd\u771f\u548c\u4fe1\u9053\u9002\u5e94\u6027\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u53ef\u8bad\u7ec3\u7684\u6bd4\u7279\u7ea7\u63a5\u53e3\u8bbe\u8ba1\uff0c\u5e76\u5f15\u5165\u91cd\u8981\u6027\u611f\u77e5\u7f51\u7edc\u52a8\u6001\u9002\u5e94\u4fe1\u9053\u5e26\u5bbd\u6bd4\u548c\u65f6\u53d8\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u5728\u65e0\u7ebf\u56fe\u50cf\u4f20\u8f93\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u4e3a\u73b0\u6709\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5b9e\u73b0\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13198", "pdf": "https://arxiv.org/pdf/2507.13198", "abs": "https://arxiv.org/abs/2507.13198", "authors": ["Rob van Glabbeek", "Bas Luttik", "Myrthe Spronck"], "title": "Just Verification of Mutual Exclusion Algorithms", "categories": ["cs.LO", "cs.DC", "F.3.1"], "comment": "An abbreviated version of this paper will appear in Proc. CONCUR'25", "summary": "We verify the correctness of a variety of mutual exclusion algorithms through\nmodel checking. We look at algorithms where communication is via shared\nread/write registers, where those registers can be atomic or non-atomic. For\nthe verification of liveness properties, it is necessary to assume a\ncompleteness criterion to eliminate spurious counterexamples. We use justness\nas completeness criterion. Justness depends on a concurrency relation; we\nconsider several such relations, modelling different assumptions on the working\nof the shared registers. We present executions demonstrating the violation of\ncorrectness properties by several algorithms, and in some cases suggest\nimprovements.", "AI": {"tldr": "\u901a\u8fc7\u6a21\u578b\u68c0\u67e5\u9a8c\u8bc1\u591a\u79cd\u4e92\u65a5\u7b97\u6cd5\u7684\u6b63\u786e\u6027\uff0c\u8003\u8651\u4e86\u5171\u4eab\u5bc4\u5b58\u5668\u7684\u539f\u5b50\u6027\u548c\u975e\u539f\u5b50\u6027\u3002", "motivation": "\u9a8c\u8bc1\u4e92\u65a5\u7b97\u6cd5\u5728\u5171\u4eab\u8bfb\u5199\u5bc4\u5b58\u5668\u4e0b\u7684\u6b63\u786e\u6027\uff0c\u6392\u9664\u865a\u5047\u53cd\u4f8b\u4ee5\u786e\u4fdd\u6d3b\u6027\u3002", "method": "\u4f7f\u7528\u516c\u6b63\u6027\u4f5c\u4e3a\u5b8c\u5907\u6027\u6807\u51c6\uff0c\u8003\u8651\u4e0d\u540c\u5e76\u53d1\u5173\u7cfb\u5efa\u6a21\u5171\u4eab\u5bc4\u5b58\u5668\u7684\u5de5\u4f5c\u5047\u8bbe\u3002", "result": "\u5c55\u793a\u4e86\u4e00\u4e9b\u7b97\u6cd5\u8fdd\u53cd\u6b63\u786e\u6027\u7684\u6267\u884c\u6848\u4f8b\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "\u516c\u6b63\u6027\u6807\u51c6\u5bf9\u4e0d\u540c\u5e76\u53d1\u5173\u7cfb\u7684\u5efa\u6a21\u662f\u9a8c\u8bc1\u4e92\u65a5\u7b97\u6cd5\u6b63\u786e\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.12951", "pdf": "https://arxiv.org/pdf/2507.12951", "abs": "https://arxiv.org/abs/2507.12951", "authors": ["Zhichao Sheng", "Shilin Zhou", "Chen Gong", "Zhenghua Li"], "title": "UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.MM", "cs.SD"], "comment": "13 pages, 3 figures", "summary": "Spoken Language Understanding (SLU) plays a crucial role in speech-centric\nmultimedia applications, enabling machines to comprehend spoken language in\nscenarios such as meetings, interviews, and customer service interactions. SLU\nencompasses multiple tasks, including Automatic Speech Recognition (ASR),\nspoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).\nHowever, existing methods often rely on separate model architectures for\nindividual tasks such as spoken NER and SA, which increases system complexity,\nlimits cross-task interaction, and fails to fully exploit heterogeneous\ndatasets available across tasks. To address these limitations, we propose\nUniSLU, a unified framework that jointly models multiple SLU tasks within a\nsingle architecture. Specifically, we propose a unified representation for\ndiverse SLU tasks, enabling full utilization of heterogeneous datasets across\nmultiple tasks. Built upon this representation, we propose a unified generative\nmethod that jointly models ASR, spoken NER, and SA tasks, enhancing task\ninteractions and enabling seamless integration with large language models to\nharness their powerful generative capabilities. Extensive experiments on public\nSLU datasets demonstrate the effectiveness of our approach, achieving superior\nSLU performance compared to several benchmark methods, making it well-suited\nfor real-world speech-based multimedia scenarios. We will release all code and\nmodels at github to facilitate future research.", "AI": {"tldr": "UniSLU\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u5efa\u6a21\u591a\u4e2aSLU\u4efb\u52a1\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8868\u793a\u548c\u751f\u6210\u65b9\u6cd5\u63d0\u5347\u4efb\u52a1\u4ea4\u4e92\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u72ec\u6a21\u578b\u67b6\u6784\u5bfc\u81f4\u7684\u7cfb\u7edf\u590d\u6742\u6027\u548c\u8de8\u4efb\u52a1\u4ea4\u4e92\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u8868\u793a\u548c\u751f\u6210\u65b9\u6cd5\uff0c\u8054\u5408\u5efa\u6a21ASR\u3001spoken NER\u548cSA\u4efb\u52a1\u3002", "result": "\u5728\u516c\u5f00SLU\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u9002\u5408\u5b9e\u9645\u8bed\u97f3\u591a\u5a92\u4f53\u573a\u666f\u3002", "conclusion": "UniSLU\u6709\u6548\u63d0\u5347\u4e86SLU\u4efb\u52a1\u7684\u6027\u80fd\u548c\u8de8\u4efb\u52a1\u4ea4\u4e92\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.12983", "pdf": "https://arxiv.org/pdf/2507.12983", "abs": "https://arxiv.org/abs/2507.12983", "authors": ["ShanBin Liu"], "title": "FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Fairness has emerged as one of the key challenges in federated learning. In\nhorizontal federated settings, data heterogeneity often leads to substantial\nperformance disparities across clients, raising concerns about equitable model\nbehavior. To address this issue, we propose FedGA, a fairness-aware federated\nlearning algorithm. We first employ the Gini coefficient to measure the\nperformance disparity among clients. Based on this, we establish a relationship\nbetween the Gini coefficient $G$ and the update scale of the global model\n${U_s}$, and use this relationship to adaptively determine the timing of\nfairness intervention. Subsequently, we dynamically adjust the aggregation\nweights according to the system's real-time fairness status, enabling the\nglobal model to better incorporate information from clients with relatively\npoor performance.We conduct extensive experiments on the Office-Caltech-10,\nCIFAR-10, and Synthetic datasets. The results show that FedGA effectively\nimproves fairness metrics such as variance and the Gini coefficient, while\nmaintaining strong overall performance, demonstrating the effectiveness of our\napproach.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0cFedGA\u7b97\u6cd5\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u805a\u5408\u6743\u91cd\u63d0\u5347\u516c\u5e73\u6027\u3002", "motivation": "\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u5ba2\u6237\u7aef\u95f4\u6027\u80fd\u5dee\u5f02\uff0c\u9700\u89e3\u51b3\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u5c3c\u7cfb\u6570\u8861\u91cf\u6027\u80fd\u5dee\u5f02\uff0c\u52a8\u6001\u8c03\u6574\u5168\u5c40\u6a21\u578b\u66f4\u65b0\u65f6\u673a\u548c\u805a\u5408\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFedGA\u663e\u8457\u63d0\u5347\u516c\u5e73\u6027\u6307\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "FedGA\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002"}}
{"id": "2507.13062", "pdf": "https://arxiv.org/pdf/2507.13062", "abs": "https://arxiv.org/abs/2507.13062", "authors": ["Vitor K. F. Pellegatti", "Gustavo M. D. Vieira"], "title": "Design and Reliability of a User Space Write-Ahead Log in Rust", "categories": ["cs.OS", "cs.DB"], "comment": "6 pages", "summary": "Write-ahead logs (WALs) are a fundamental fault-tolerance technique found in\nmany areas of computer science. WALs must be reliable while maintaining high\nperformance, because all operations will be written to the WAL to ensure their\nstability. Without reliability a WAL is useless, because its utility is tied to\nits ability to recover data after a failure. In this paper we describe our\nexperience creating a prototype user space WAL in Rust. We observed that Rust\nis easy to use, compact and has a very rich set of libraries. More importantly,\nwe have found that the overhead is minimal, with the WAL prototype operating at\nbasically the expected performance of the stable memory device.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86\u4f7f\u7528Rust\u5f00\u53d1\u7528\u6237\u7a7a\u95f4\u9884\u5199\u65e5\u5fd7\uff08WAL\uff09\u539f\u578b\u7684\u7ecf\u9a8c\uff0c\u5f3a\u8c03\u4e86Rust\u7684\u6613\u7528\u6027\u3001\u5e93\u4e30\u5bcc\u6027\u4ee5\u53ca\u9ad8\u6027\u80fd\u3002", "motivation": "\u9884\u5199\u65e5\u5fd7\uff08WAL\uff09\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\u91cd\u8981\u7684\u5bb9\u9519\u6280\u672f\uff0c\u8981\u6c42\u9ad8\u53ef\u9760\u6027\u548c\u9ad8\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4f7f\u7528Rust\u5b9e\u73b0WAL\u7684\u53ef\u884c\u6027\u3002", "method": "\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2aRust\u8bed\u8a00\u7684\u7528\u6237\u7a7a\u95f4WAL\u539f\u578b\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "result": "Rust\u6613\u4e8e\u4f7f\u7528\u4e14\u6027\u80fd\u4f18\u5f02\uff0cWAL\u539f\u578b\u7684\u6027\u80fd\u63a5\u8fd1\u7a33\u5b9a\u5b58\u50a8\u8bbe\u5907\u7684\u7406\u8bba\u6027\u80fd\u3002", "conclusion": "Rust\u662f\u5b9e\u73b0\u9ad8\u6027\u80fdWAL\u7684\u6709\u6548\u5de5\u5177\uff0c\u9002\u5408\u7c7b\u4f3c\u9700\u6c42\u7684\u9879\u76ee\u3002"}}
{"id": "2507.12741", "pdf": "https://arxiv.org/pdf/2507.12741", "abs": "https://arxiv.org/abs/2507.12741", "authors": ["Lotfi El Hafi", "Kazuma Onishi", "Shoichi Hasegawa", "Akira Oyama", "Tomochika Ishikawa", "Masashi Osada", "Carl Tornberg", "Ryoma Kado", "Kento Murata", "Saki Hashimoto", "Sebastian Carrera Villalobos", "Akira Taniguchi", "Gustavo Alfonso Garcia Ricardez", "Yoshinobu Hagiwara", "Tatsuya Aoki", "Kensuke Iwata", "Takato Horii", "Yukiko Horikawa", "Takahiro Miyashita", "Tadahiro Taniguchi", "Hiroshi Ishiguro"], "title": "Public Evaluation on Potential Social Impacts of Fully Autonomous Cybernetic Avatars for Physical Support in Daily-Life Environments: Large-Scale Demonstration and Survey at Avatar Land", "categories": ["cs.HC", "cs.RO"], "comment": "Accepted for presentation at the 2025 IEEE International Conference\n  on Advanced Robotics and its Social Impacts (ARSO), Osaka, Japan", "summary": "Cybernetic avatars (CAs) are key components of an avatar-symbiotic society,\nenabling individuals to overcome physical limitations through virtual agents\nand robotic assistants. While semi-autonomous CAs intermittently require human\nteleoperation and supervision, the deployment of fully autonomous CAs remains a\nchallenge. This study evaluates public perception and potential social impacts\nof fully autonomous CAs for physical support in daily life. To this end, we\nconducted a large-scale demonstration and survey during Avatar Land, a 19-day\npublic event in Osaka, Japan, where fully autonomous robotic CAs, alongside\nsemi-autonomous CAs, performed daily object retrieval tasks. Specifically, we\nanalyzed responses from 2,285 visitors who engaged with various CAs, including\na subset of 333 participants who interacted with fully autonomous CAs and\nshared their perceptions and concerns through a survey questionnaire. The\nsurvey results indicate interest in CAs for physical support in daily life and\nat work. However, concerns were raised regarding task execution reliability. In\ncontrast, cost and human-like interaction were not dominant concerns. Project\npage: https://lotfielhafi.github.io/FACA-Survey/.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u516c\u4f17\u5bf9\u5b8c\u5168\u81ea\u4e3b\u7684\u7f51\u7edc\u5316\u8eab\uff08CAs\uff09\u5728\u65e5\u5e38\u751f\u6d3b\u7269\u7406\u652f\u6301\u4e2d\u7684\u770b\u6cd5\u53ca\u5176\u6f5c\u5728\u793e\u4f1a\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u5b8c\u5168\u81ea\u4e3bCAs\u7684\u90e8\u7f72\u6311\u6218\u53ca\u5176\u793e\u4f1a\u63a5\u53d7\u5ea6\u3002", "method": "\u901a\u8fc7\u5927\u578b\u516c\u5f00\u6d3b\u52a8\uff08Avatar Land\uff09\u5c55\u793a\u5e76\u8c03\u67e5\u53c2\u4e0e\u8005\u5bf9\u81ea\u4e3b\u53ca\u534a\u81ea\u4e3bCAs\u7684\u770b\u6cd5\uff0c\u6536\u96c6\u4e862,285\u4efd\u95ee\u5377\uff0c\u5206\u6790\u4e86333\u540d\u4e0e\u5b8c\u5168\u81ea\u4e3bCAs\u4e92\u52a8\u8005\u7684\u53cd\u9988\u3002", "result": "\u516c\u4f17\u5bf9CAs\u5728\u751f\u6d3b\u53ca\u5de5\u4f5c\u4e2d\u7684\u7269\u7406\u652f\u6301\u611f\u5174\u8da3\uff0c\u4f46\u5bf9\u5176\u4efb\u52a1\u6267\u884c\u53ef\u9760\u6027\u5b58\u6709\u62c5\u5fe7\uff0c\u6210\u672c\u548c\u62df\u4eba\u4ea4\u4e92\u975e\u4e3b\u8981\u95ee\u9898\u3002", "conclusion": "\u5b8c\u5168\u81ea\u4e3bCAs\u5728\u5b9e\u7528\u4e2d\u9700\u89e3\u51b3\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u516c\u4f17\u63a5\u53d7\u5ea6\u3002"}}
{"id": "2507.12653", "pdf": "https://arxiv.org/pdf/2507.12653", "abs": "https://arxiv.org/abs/2507.12653", "authors": ["Jo\u00e3o Granja-Correia", "Remedios Hern\u00e1ndez-Linares", "Luca Ferranti", "Arm\u00e9nio Rego"], "title": "A Fuzzy Approach to Project Success: Measuring What Matters", "categories": ["cs.SE", "cs.CL", "H.4.m"], "comment": "3 pages, 1 figure, presented at FUZZ-IEEE 2025", "summary": "This paper introduces a novel approach to project success evaluation by\nintegrating fuzzy logic into an existing construct. Traditional Likert-scale\nmeasures often overlook the context-dependent and multifaceted nature of\nproject success. The proposed hierarchical Type-1 Mamdani fuzzy system\nprioritizes sustained positive impact for end-users, reducing emphasis on\nsecondary outcomes like stakeholder satisfaction and internal project success.\nThis dynamic approach may provide a more accurate measure of project success\nand could be adaptable to complex evaluations. Future research will focus on\nempirical testing and broader applications of fuzzy logic in social science.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u7684\u65b0\u578b\u9879\u76ee\u6210\u529f\u8bc4\u4f30\u65b9\u6cd5\uff0c\u52a8\u6001\u8003\u8651\u7ec8\u7aef\u7528\u6237\u7684\u6301\u7eed\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edfLikert\u91cf\u8868\u5ffd\u89c6\u4e86\u9879\u76ee\u6210\u529f\u7684\u591a\u5c42\u6b21\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u91c7\u7528\u5206\u5c42Type-1 Mamdani\u6a21\u7cca\u7cfb\u7edf\uff0c\u7a81\u51fa\u7ec8\u7aef\u7528\u6237\u7684\u6301\u7eed\u79ef\u6781\u5f71\u54cd\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u80fd\u66f4\u51c6\u786e\u8861\u91cf\u9879\u76ee\u6210\u529f\uff0c\u5e76\u9002\u7528\u4e8e\u590d\u6742\u8bc4\u4f30\u573a\u666f\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5c06\u8fdb\u884c\u5b9e\u8bc1\u6d4b\u8bd5\u5e76\u62d3\u5c55\u6a21\u7cca\u903b\u8f91\u5728\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.13282", "pdf": "https://arxiv.org/pdf/2507.13282", "abs": "https://arxiv.org/abs/2507.13282", "authors": ["Eugene Goldberg"], "title": "Solving SAT By Computing A Stable Set Of Points In Clusters", "categories": ["cs.LO"], "comment": null, "summary": "Earlier we introduced the notion of a stable set of points (SSP). We proved\nthat a CNF formula is unsatisfiable iff there is a set of points (i.e. complete\nassignments) that is stable with respect to this formula. Experiments showed\nthat SSPs for CNF formulas of practical interest are very large. So computing\nan SSP for a CNF formula point by point is, in general, infeasible. In this\nreport, we show how an SSP can be computed in clusters, each cluster being a\nlarge set of points that are processed simultaneously. The appeal of computing\nSSPs is twofold. First, it allows one to better take into account formula\nstructure and hence, arguably, design more efficient SAT algorithms. Second,\nSAT solving by SSPs facilitates parallel computing.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7\u96c6\u7fa4\u8ba1\u7b97\u7a33\u5b9a\u70b9\u96c6\uff08SSP\uff09\uff0c\u4ee5\u89e3\u51b3\u76f4\u63a5\u9010\u70b9\u8ba1\u7b97SSP\u5bf9\u5b9e\u9645CNF\u516c\u5f0f\u4e0d\u53ef\u884c\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u5b9e\u9645CNF\u516c\u5f0f\u7684SSP\u89c4\u6a21\u5de8\u5927\uff0c\u9010\u70b9\u8ba1\u7b97\u4e0d\u53ef\u884c\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u540c\u65f6\u5904\u7406\u5927\u91cf\u70b9\u7684\u96c6\u7fa4\u6765\u8ba1\u7b97SSP\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u66f4\u597d\u5730\u5229\u7528\u516c\u5f0f\u7ed3\u6784\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684SAT\u7b97\u6cd5\uff0c\u8fd8\u4fbf\u4e8e\u5e76\u884c\u8ba1\u7b97\u3002", "conclusion": "\u96c6\u7fa4\u8ba1\u7b97SSP\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21CNF\u516c\u5f0f\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.13296", "pdf": "https://arxiv.org/pdf/2507.13296", "abs": "https://arxiv.org/abs/2507.13296", "authors": ["Alex Conway", "Laxman Dhulipala", "Martin Farach-Colton", "Rob Johnson", "Ben Landrum", "Christopher Musco", "Yarin Shechter", "Torsten Suel", "Richard Wen"], "title": "Efficiently Constructing Sparse Navigable Graphs", "categories": ["cs.DS", "cs.DB", "cs.IR"], "comment": null, "summary": "Graph-based nearest neighbor search methods have seen a surge of popularity\nin recent years, offering state-of-the-art performance across a wide variety of\napplications. Central to these methods is the task of constructing a sparse\nnavigable search graph for a given dataset endowed with a distance function.\nUnfortunately, doing so is computationally expensive, so heuristics are\nuniversally used in practice.\n  In this work, we initiate the study of fast algorithms with provable\nguarantees for search graph construction. For a dataset with $n$ data points,\nthe problem of constructing an optimally sparse navigable graph can be framed\nas $n$ separate but highly correlated minimum set cover instances. This yields\na naive $O(n^3)$ time greedy algorithm that returns a navigable graph whose\nsparsity is at most $O(\\log n)$ higher than optimal. We improve significantly\non this baseline, taking advantage of correlation between the set cover\ninstances to leverage techniques from streaming and sublinear-time set cover\nalgorithms. Combined with problem-specific pre-processing techniques, we\npresent an $\\tilde{O}(n^2)$ time algorithm for constructing an $O(\\log\nn)$-approximate sparsest navigable graph under any distance function.\n  The runtime of our method is optimal up to logarithmic factors under the\nStrong Exponential Time Hypothesis via a reduction from Monochromatic Closest\nPair. Moreover, we prove that, as with general set cover, obtaining better than\nan $O(\\log n)$-approximation is NP-hard, despite the significant additional\nstructure present in the navigable graph problem. Finally, we show that our\ntechniques can also beat cubic time for the closely related and practically\nimportant problems of constructing $\\alpha$-shortcut reachable and\n$\\tau$-monotonic graphs, which are also used for nearest neighbor search. For\nsuch graphs, we obtain $\\tilde{O}(n^{2.5})$ time or better algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5feb\u901f\u6784\u5efa\u7a00\u758f\u53ef\u5bfc\u822a\u641c\u7d22\u56fe\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u8d2a\u5fc3\u7b97\u6cd5\u548c\u5229\u7528\u6d41\u5f0f\u4e0e\u6b21\u7ebf\u6027\u96c6\u5408\u8986\u76d6\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u6700\u4f18\u7684\u6784\u5efa\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u56fe\u57fa\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\u6784\u5efa\u7a00\u758f\u53ef\u5bfc\u822a\u56fe\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u7a00\u758f\u53ef\u5bfc\u822a\u56fe\u6784\u5efa\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u4e2a\u76f8\u5173\u7684\u6700\u5c0f\u96c6\u5408\u8986\u76d6\u5b9e\u4f8b\uff0c\u7ed3\u5408\u6d41\u5f0f\u4e0e\u6b21\u7ebf\u6027\u96c6\u5408\u8986\u76d6\u7b97\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a$\tilde{O}(n^2)$\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u5728$\tilde{O}(n^2)$\u65f6\u95f4\u5185\u6784\u5efa\u7a00\u758f\u5ea6\u4e3a$O(\\log n)$\u8fd1\u4f3c\u6700\u4f18\u7684\u53ef\u5bfc\u822a\u56fe\uff0c\u5e76\u53ef\u6269\u5c55\u89e3\u51b3\u5176\u4ed6\u76f8\u5173\u56fe\u6784\u5efa\u95ee\u9898\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u8fd8\u8bc1\u660e\u4e86\u6539\u8fdb\u8fd1\u4f3c\u6bd4\u7684\u56f0\u96be\u6027\uff0c\u586b\u8865\u4e86\u56fe\u57fa\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6cd5\u4e2d\u56fe\u6784\u5efa\u7684\u7406\u8bba\u7a7a\u767d\u3002"}}
{"id": "2507.12749", "pdf": "https://arxiv.org/pdf/2507.12749", "abs": "https://arxiv.org/abs/2507.12749", "authors": ["Xumeng Wang", "Xiangxuan Zhang", "Zhiqi Gao", "Shuangcheng Jiao", "Yuxin Ma"], "title": "PatternSight: A Perceptual Grouping Effectiveness Assessment Approach for Graphical Patterns in Charts", "categories": ["cs.HC"], "comment": null, "summary": "The boom in visualization generation tools has significantly lowered the\nthreshold for chart authoring. Nevertheless, chart authors with an insufficient\nunderstanding of perceptual theories may encounter difficulties in evaluating\nthe effectiveness of chart representations, thereby struggling to identify the\nappropriate chart design to convey the intended data patterns. To address this\nissue, we propose a perception simulation model that can assess the perceptual\neffectiveness of charts by predicting graphical patterns that chart viewers are\nlikely to notice. The perception simulation model integrates perceptual theory\ninto visual feature extraction of chart elements to provide interpretable model\noutcomes. Human perceptual results proved that the outcome of our model can\nsimulate the perceptual grouping behaviors of most chart viewers and cover\ndiverse perceptual results. We also embed the model into a prototype interface\ncalled PatternSight to facilitate chart authors in assessing whether the chart\ndesign can satisfy their pattern representation requirements as expected and\ndetermining feasible improvements of visual design. According to the results of\na user experiment, PatternSight can effectively assist chart authors in\noptimizing chart design for representing data patterns.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u611f\u77e5\u6a21\u62df\u6a21\u578b\uff0c\u5e2e\u52a9\u56fe\u8868\u4f5c\u8005\u8bc4\u4f30\u56fe\u8868\u8bbe\u8ba1\u7684\u611f\u77e5\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u5de5\u5177PatternSight\u4f18\u5316\u8bbe\u8ba1\u3002", "motivation": "\u56fe\u8868\u5de5\u5177\u666e\u53ca\u540e\uff0c\u4f5c\u8005\u7f3a\u4e4f\u611f\u77e5\u7406\u8bba\u5bfc\u81f4\u96be\u4ee5\u8bc4\u4f30\u56fe\u8868\u6548\u679c\uff0c\u9700\u6539\u8fdb\u8bbe\u8ba1\u6d41\u7a0b\u3002", "method": "\u7ed3\u5408\u611f\u77e5\u7406\u8bba\u63d0\u53d6\u56fe\u8868\u89c6\u89c9\u7279\u5f81\uff0c\u6784\u5efa\u611f\u77e5\u6a21\u62df\u6a21\u578b\u9884\u6d4b\u89c2\u4f17\u53ef\u80fd\u7684\u6ce8\u610f\u529b\u5206\u5e03\u3002", "result": "\u6a21\u578b\u80fd\u6a21\u62df\u89c2\u4f17\u611f\u77e5\u884c\u4e3a\uff0cPatternSight\u5de5\u5177\u6709\u6548\u8f85\u52a9\u4f18\u5316\u8bbe\u8ba1\u3002", "conclusion": "\u611f\u77e5\u6a21\u62df\u6a21\u578b\u53ca\u5de5\u5177\u63d0\u5347\u4e86\u56fe\u8868\u8bbe\u8ba1\u7684\u611f\u77e5\u6548\u679c\u8bc4\u4f30\u4e0e\u4f18\u5316\u80fd\u529b\u3002"}}
{"id": "2507.12665", "pdf": "https://arxiv.org/pdf/2507.12665", "abs": "https://arxiv.org/abs/2507.12665", "authors": ["Salvador D. Escobedo"], "title": "Single Conversation Methodology: A Human-Centered Protocol for AI-Assisted Software Development", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Style reviewed by a LLM for improving clarity and English syntax", "summary": "We propose the Single Conversation Methodology (SCM), a novel and pragmatic\napproach to software development using large language models (LLMs). In\ncontrast to ad hoc interactions with generative AI, SCM emphasizes a structured\nand persistent development dialogue, where all stages of a project - from\nrequirements to architecture and implementation - unfold within a single,\nlong-context conversation. The methodology is grounded on principles of\ncognitive clarity, traceability, modularity, and documentation. We define its\nphases, best practices, and philosophical stance, while arguing that SCM offers\na necessary correction to the passive reliance on LLMs prevalent in current\npractices. We aim to reassert the active role of the developer as architect and\nsupervisor of the intelligent tool.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5355\u5bf9\u8bdd\u65b9\u6cd5\u8bba\uff08SCM\uff09\uff0c\u4e00\u79cd\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b9e\u7528\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u7ed3\u6784\u5316\u3001\u6301\u4e45\u5316\u7684\u5f00\u53d1\u5bf9\u8bdd\u3002", "motivation": "\u5f53\u524d\u5b9e\u8df5\u4e2d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u88ab\u52a8\u4f9d\u8d56\u9700\u8981\u4fee\u6b63\uff0c\u91cd\u65b0\u786e\u7acb\u5f00\u53d1\u8005\u4f5c\u4e3a\u667a\u80fd\u5de5\u5177\u67b6\u6784\u8005\u548c\u76d1\u7763\u8005\u7684\u89d2\u8272\u3002", "method": "SCM\u57fa\u4e8e\u8ba4\u77e5\u6e05\u6670\u6027\u3001\u53ef\u8ffd\u6eaf\u6027\u3001\u6a21\u5757\u5316\u548c\u6587\u6863\u5316\u539f\u5219\uff0c\u5b9a\u4e49\u5f00\u53d1\u9636\u6bb5\u3001\u6700\u4f73\u5b9e\u8df5\u548c\u54f2\u5b66\u7acb\u573a\u3002", "result": "SCM\u63d0\u4f9b\u4e86\u4e00\u79cd\u4ece\u9700\u6c42\u5230\u67b6\u6784\u518d\u5230\u5b9e\u73b0\u7684\u5355\u5bf9\u8bdd\u5f00\u53d1\u6846\u67b6\u3002", "conclusion": "SCM\u662f\u5bf9\u5f53\u524dLLM\u4f7f\u7528\u5b9e\u8df5\u7684\u6539\u8fdb\uff0c\u5f3a\u8c03\u4e86\u5f00\u53d1\u8005\u7684\u4e3b\u52a8\u6027\u548c\u7ed3\u6784\u5316\u5f00\u53d1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.12714", "pdf": "https://arxiv.org/pdf/2507.12714", "abs": "https://arxiv.org/abs/2507.12714", "authors": ["Yang Yang", "Dongni Mao", "Hiroaki Santo", "Yasuyuki Matsushita", "Fumio Okura"], "title": "NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement", "categories": ["cs.CV", "cs.GR"], "comment": "IEEE/CVF International Conference on Computer Vision (ICCV 2025),\n  Project: https://neuraleaf-yang.github.io/", "summary": "We develop a neural parametric model for 3D leaves for plant modeling and\nreconstruction that are essential for agriculture and computer graphics. While\nneural parametric models are actively studied for humans and animals, plant\nleaves present unique challenges due to their diverse shapes and flexible\ndeformation. To this problem, we introduce a neural parametric model for\nleaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be\napproximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into\ntheir 2D base shapes and 3D deformations. This representation allows learning\nfrom rich sources of 2D leaf image datasets for the base shapes, and also has\nthe advantage of simultaneously learning textures aligned with the geometry. To\nmodel the 3D deformation, we propose a novel skeleton-free skinning model and\ncreate a newly captured 3D leaf dataset called DeformLeaf. We show that\nNeuraLeaf successfully generates a wide range of leaf shapes with deformation,\nresulting in accurate model fitting to 3D observations like depth maps and\npoint clouds. Our implementation and dataset are available at\nhttps://neuraleaf-yang.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeuraLeaf\u7684\u795e\u7ecf\u53c2\u6570\u6a21\u578b\uff0c\u7528\u4e8e3D\u53f6\u5b50\u7684\u5efa\u6a21\u548c\u91cd\u5efa\uff0c\u89e3\u51b3\u4e86\u690d\u7269\u53f6\u5b50\u591a\u6837\u5f62\u72b6\u548c\u7075\u6d3b\u53d8\u5f62\u7684\u6311\u6218\u3002", "motivation": "\u519c\u4e1a\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u9700\u8981\u7cbe\u786e\u76843D\u690d\u7269\u5efa\u6a21\uff0c\u5c24\u5176\u662f\u53f6\u5b50\u7684\u591a\u6837\u6027\u5f62\u72b6\u548c\u53d8\u5f62\u95ee\u9898\u3002\u73b0\u6709\u795e\u7ecf\u53c2\u6570\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u4eba\u7c7b\u548c\u52a8\u7269\uff0c\u690d\u7269\u53f6\u5b50\u5efa\u6a21\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002", "method": "NeuraLeaf\u5c06\u53f6\u5b50\u51e0\u4f55\u89e3\u8026\u4e3a2D\u57fa\u7840\u5f62\u72b6\u548c3D\u53d8\u5f62\uff0c\u5229\u7528\u4e30\u5bcc\u76842D\u53f6\u5b50\u56fe\u50cf\u6570\u636e\u96c6\u5b66\u4e60\u57fa\u7840\u5f62\u72b6\uff0c\u5e76\u63d0\u51fa\u65e0\u9aa8\u67b6\u7684\u8499\u76ae\u6a21\u578b\u5904\u74063D\u53d8\u5f62\u3002\u6570\u636e\u96c6DeformLeaf\u652f\u6301\u5b66\u4e60\u3002", "result": "NeuraLeaf\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u7684\u53f6\u5b50\u5f62\u72b6\u5e76\u7cbe\u786e\u62df\u54083D\u89c2\u6d4b\u6570\u636e\uff08\u5982\u6df1\u5ea6\u56fe\u548c\u70b9\u4e91\uff09\u3002", "conclusion": "NeuraLeaf\u5728\u519c\u4e1a\u548c\u8ba1\u7b97\u673a\u56fe\u5f62\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u5176\u5b9e\u73b0\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2507.12989", "pdf": "https://arxiv.org/pdf/2507.12989", "abs": "https://arxiv.org/abs/2507.12989", "authors": ["Lyris Xu", "Fabio Aurelio D'Asaro", "Luke Dickens"], "title": "A Translation of Probabilistic Event Calculus into Markov Decision Processes", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Probabilistic Event Calculus (PEC) is a logical framework for reasoning about\nactions and their effects in uncertain environments, which enables the\nrepresentation of probabilistic narratives and computation of temporal\nprojections. The PEC formalism offers significant advantages in\ninterpretability and expressiveness for narrative reasoning. However, it lacks\nmechanisms for goal-directed reasoning. This paper bridges this gap by\ndeveloping a formal translation of PEC domains into Markov Decision Processes\n(MDPs), introducing the concept of \"action-taking situations\" to preserve PEC's\nflexible action semantics. The resulting PEC-MDP formalism enables the\nextensive collection of algorithms and theoretical tools developed for MDPs to\nbe applied to PEC's interpretable narrative domains. We demonstrate how the\ntranslation supports both temporal reasoning tasks and objective-driven\nplanning, with methods for mapping learned policies back into human-readable\nPEC representations, maintaining interpretability while extending PEC's\ncapabilities.", "AI": {"tldr": "PEC\u662f\u4e00\u79cd\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u8fdb\u884c\u903b\u8f91\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4f46\u7f3a\u4e4f\u76ee\u6807\u5bfc\u5411\u63a8\u7406\u673a\u5236\u3002\u672c\u6587\u901a\u8fc7\u5c06PEC\u8f6c\u6362\u4e3aMDP\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u652f\u6301\u65f6\u95f4\u63a8\u7406\u548c\u76ee\u6807\u89c4\u5212\uff0c\u540c\u65f6\u4fdd\u6301PEC\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "PEC\u5728\u53d9\u4e8b\u60c5\u666f\u63a8\u7406\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u76ee\u6807\u5bfc\u5411\u63a8\u7406\u95ee\u9898\u3002\u4e3a\u4e86\u7ed3\u5408PEC\u7684\u53d9\u4e8b\u4f18\u52bf\u4e0eMDP\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5f62\u5f0f\u5316\u8f6c\u6362\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u201c\u52a8\u4f5c\u6267\u884c\u60c5\u5883\u201d\u6982\u5ff5\uff0c\u5c06PEC\u9886\u57df\u5f62\u5f0f\u5316\u8f6c\u6362\u4e3aMDP\uff0c\u4fdd\u7559PEC\u7684\u7075\u6d3b\u52a8\u4f5c\u8bed\u4e49\uff0c\u5e76\u5229\u7528MDP\u7684\u7b97\u6cd5\u548c\u5de5\u5177\u6269\u5c55PEC\u529f\u80fd\u3002", "result": "PEC-MDP\u5f62\u5f0f\u5316\u652f\u6301\u65f6\u95f4\u63a8\u7406\u548c\u76ee\u6807\u9a71\u52a8\u89c4\u5212\uff0c\u540c\u65f6\u80fd\u5c06\u5b66\u4e60\u7684\u7b56\u7565\u6620\u5c04\u56de\u4eba\u7c7b\u53ef\u8bfb\u7684PEC\u8868\u793a\uff0c\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "PEC-MDP\u6210\u529f\u7ed3\u5408\u4e86\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u4e3a\u590d\u6742\u51b3\u7b56\u95ee\u9898\u63d0\u4f9b\u4e86\u517c\u5177\u53d9\u4e8b\u8868\u8fbe\u80fd\u529b\u548c\u76ee\u6807\u5bfc\u5411\u63a8\u7406\u80fd\u529b\u7684\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2507.13255", "pdf": "https://arxiv.org/pdf/2507.13255", "abs": "https://arxiv.org/abs/2507.13255", "authors": ["Lyucheng Wu", "Mengru Wang", "Ziwen Xu", "Tri Cao", "Nay Oo", "Bryan Hooi", "Shumin Deng"], "title": "Automating Steering for Safe Multimodal Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "comment": "Working in progress. 22 pages (8+ for main); 25 figures; 1 table", "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has unlocked\npowerful cross-modal reasoning abilities, but also raised new safety concerns,\nparticularly when faced with adversarial multimodal inputs. To improve the\nsafety of MLLMs during inference, we introduce a modular and adaptive\ninference-time intervention technology, AutoSteer, without requiring any\nfine-tuning of the underlying model. AutoSteer incorporates three core\ncomponents: (1) a novel Safety Awareness Score (SAS) that automatically\nidentifies the most safety-relevant distinctions among the model's internal\nlayers; (2) an adaptive safety prober trained to estimate the likelihood of\ntoxic outputs from intermediate representations; and (3) a lightweight Refusal\nHead that selectively intervenes to modulate generation when safety risks are\ndetected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical\nbenchmarks demonstrate that AutoSteer significantly reduces the Attack Success\nRate (ASR) for textual, visual, and cross-modal threats, while maintaining\ngeneral abilities. These findings position AutoSteer as a practical,\ninterpretable, and effective framework for safer deployment of multimodal AI\nsystems.", "AI": {"tldr": "AutoSteer\u662f\u4e00\u79cd\u65e0\u9700\u8c03\u6574\u6a21\u578b\u7684\u6a21\u5757\u5316\u5e72\u9884\u6280\u672f\uff0c\u901a\u8fc7\u5b89\u5168\u6027\u8bc4\u5206\u3001\u81ea\u9002\u5e94\u63a2\u6d4b\u5668\u548c\u8f7b\u91cf\u7ea7\u62d2\u7edd\u6a21\u5757\uff0c\u663e\u8457\u964d\u4f4e\u591a\u6a21\u6001\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8de8\u6a21\u6001\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u4e5f\u9762\u4e34\u5bf9\u6297\u6027\u591a\u6a21\u6001\u8f93\u5165\u7684\u5b89\u5168\u9690\u60a3\u3002\u4e3a\u63d0\u9ad8\u63a8\u7406\u5b89\u5168\u6027\uff0c\u63d0\u51fa\u4e86AutoSteer\u3002", "method": "AutoSteer\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u5b89\u5168\u6027\u611f\u77e5\u8bc4\u5206\uff08SAS\uff09\u3001\u81ea\u9002\u5e94\u5b89\u5168\u63a2\u6d4b\u5668\u548c\u8f7b\u91cf\u7ea7\u62d2\u7edd\u6a21\u5757\uff0c\u7528\u4e8e\u8bc6\u522b\u5b89\u5168\u98ce\u9669\u5e76\u9009\u62e9\u6027\u5e72\u9884\u751f\u6210\u3002", "result": "\u5728LLaVA-OV\u548cChameleon\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoSteer\u663e\u8457\u964d\u4f4e\u6587\u672c\u3001\u89c6\u89c9\u548c\u8de8\u6a21\u6001\u5a01\u80c1\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "AutoSteer\u4e3a\u591a\u6a21\u6001AI\u7cfb\u7edf\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2507.12767", "pdf": "https://arxiv.org/pdf/2507.12767", "abs": "https://arxiv.org/abs/2507.12767", "authors": ["Jiaxin An"], "title": "Autonomy for Older Adult-Agent Interaction", "categories": ["cs.HC", "cs.AI"], "comment": "7 pages", "summary": "As the global population ages, artificial intelligence (AI)-powered agents\nhave emerged as potential tools to support older adults' caregiving. Prior\nresearch has explored agent autonomy by identifying key interaction stages in\ntask processes and defining the agent's role at each stage. However, ensuring\nthat agents align with older adults' autonomy preferences remains a critical\nchallenge. Drawing on interdisciplinary conceptualizations of autonomy, this\npaper examines four key dimensions of autonomy for older adults:\ndecision-making autonomy, goal-oriented autonomy, control autonomy, and social\nresponsibility autonomy. This paper then proposes the following research\ndirections: (1) Addressing social responsibility autonomy, which concerns the\nethical and social implications of agent use in communal settings; (2)\nOperationalizing agent autonomy from the task perspective; and (3) Developing\nautonomy measures.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u4ee3\u7406\u5728\u8001\u5e74\u4eba\u62a4\u7406\u4e2d\u7684\u81ea\u4e3b\u6027\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5168\u7403\u4eba\u53e3\u8001\u9f84\u5316\uff0cAI\u4ee3\u7406\u6210\u4e3a\u8001\u5e74\u4eba\u62a4\u7406\u7684\u6f5c\u5728\u5de5\u5177\uff0c\u4f46\u5982\u4f55\u4f7f\u5176\u4e0e\u8001\u5e74\u4eba\u7684\u81ea\u4e3b\u6027\u9700\u6c42\u4fdd\u6301\u4e00\u81f4\u4ecd\u662f\u6311\u6218\u3002", "method": "\u672c\u6587\u57fa\u4e8e\u8de8\u5b66\u79d1\u7684\u81ea\u4e3b\u6027\u6982\u5ff5\uff0c\u5206\u6790\u4e86\u8001\u5e74\u4eba\u81ea\u4e3b\u6027\u7684\u56db\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86\u56db\u4e2a\u81ea\u4e3b\u6027\u7ef4\u5ea6\u548c\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u89e3\u51b3AI\u4ee3\u7406\u5728\u8001\u5e74\u4eba\u62a4\u7406\u4e2d\u7684\u81ea\u4e3b\u6027\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u793e\u4f1a\u8d23\u4efb\u611f\u3001\u4efb\u52a1\u89c6\u89d2\u7684\u64cd\u4f5c\u5316\u548c\u81ea\u4e3b\u6027\u6d4b\u91cf\uff0c\u4ee5\u786e\u4fddAI\u4ee3\u7406\u66f4\u597d\u5730\u6ee1\u8db3\u8001\u5e74\u4eba\u9700\u6c42\u3002"}}
{"id": "2507.13035", "pdf": "https://arxiv.org/pdf/2507.13035", "abs": "https://arxiv.org/abs/2507.13035", "authors": ["Keila Lucas", "Rohit Gheyi", "M\u00e1rcio Ribeiro", "Fabio Palomba", "Luana Martins", "Elvys Soares"], "title": "Investigating the Performance of Small Language Models in Detecting Test Smells in Manual Test Cases", "categories": ["cs.SE"], "comment": "7 pages, Accepted at Insightful Ideas and Emerging Results (IIER)\n  Track of the Brazilian Symposium on Software Engineering (SBES 2025)", "summary": "Manual testing, in which testers follow natural language instructions to\nvalidate system behavior, remains crucial for uncovering issues not easily\ncaptured by automation. However, these test cases often suffer from test\nsmells, quality issues such as ambiguity, redundancy, or missing checks that\nreduce test reliability and maintainability. While detection tools exist, they\ntypically require manual rule definition and lack scalability. This study\ninvestigates the potential of Small Language Models (SLMs) for automatically\ndetecting test smells. We evaluate Gemma3, Llama3.2, and Phi-4 on 143\nreal-world Ubuntu test cases, covering seven types of test smells. Phi-4\nachieved the best results, reaching a pass@2 of 97% in detecting sentences with\ntest smells, while Gemma3 and Llama3.2 reached approximately 91%. Beyond\ndetection, SLMs autonomously explained issues and suggested improvements, even\nwithout explicit prompt instructions. They enabled low-cost, concept-driven\nidentification of diverse test smells without relying on extensive rule\ndefinitions or syntactic analysis. These findings highlight the potential of\nSLMs as efficient tools that preserve data privacy and can improve test quality\nin real-world scenarios.", "AI": {"tldr": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5982Phi-4\u3001Gemma3\u548cLlama3.2\u5728\u81ea\u52a8\u68c0\u6d4b\u6d4b\u8bd5\u5f02\u5473\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u91cf\u624b\u52a8\u89c4\u5219\u5b9a\u4e49\uff0c\u5e76\u80fd\u81ea\u4e3b\u89e3\u91ca\u95ee\u9898\u548c\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\uff0c\u63d0\u5347\u6d4b\u8bd5\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u5e38\u89c1\u7684\u6d4b\u8bd5\u5f02\u5473\uff08\u5982\u6a21\u7cca\u6027\u3001\u5197\u4f59\u7b49\uff09\u5f71\u54cd\u6d4b\u8bd5\u7684\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u4f9d\u8d56\u624b\u52a8\u89c4\u5219\u4e14\u7f3a\u4e4f\u6269\u5c55\u6027\uff0c\u7814\u7a76\u63a2\u8ba8SLMs\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u8bc4\u4f30Gemma3\u3001Llama3.2\u548cPhi-4\u5728143\u4e2a\u771f\u5b9eUbuntu\u6d4b\u8bd5\u7528\u4f8b\u4e0a\u7684\u8868\u73b0\uff0c\u8986\u76d6\u4e03\u79cd\u6d4b\u8bd5\u5f02\u5473\u7c7b\u578b\u3002", "result": "Phi-4\u8868\u73b0\u6700\u4f73\uff0c\u68c0\u6d4b\u6d4b\u8bd5\u5f02\u5473\u53e5\u5b50\u7684pass@2\u8fbe97%\uff0cGemma3\u548cLlama3.2\u7ea691%\u3002SLMs\u8fd8\u80fd\u81ea\u4e3b\u89e3\u91ca\u95ee\u9898\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "SLMs\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u4e14\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u6d4b\u8bd5\u5f02\u5473\u68c0\u6d4b\u5de5\u5177\uff0c\u80fd\u591f\u63d0\u5347\u5b9e\u9645\u6d4b\u8bd5\u573a\u666f\u4e2d\u7684\u8d28\u91cf\u3002"}}
{"id": "2507.13208", "pdf": "https://arxiv.org/pdf/2507.13208", "abs": "https://arxiv.org/abs/2507.13208", "authors": ["Besik Dundua", "Temur Kutsia"], "title": "Higher-Order Pattern Unification Modulo Similarity Relations", "categories": ["cs.AI", "cs.LO", "math.LO", "03B70 (Primary) 68T37, 68T27, 68Q42, 03B40, 68V15 (Secondary)", "F.4.1; I.2.3"], "comment": "23 pages", "summary": "The combination of higher-order theories and fuzzy logic can be useful in\ndecision-making tasks that involve reasoning across abstract functions and\npredicates, where exact matches are often rare or unnecessary. Developing\nefficient reasoning and computational techniques for such a combined formalism\npresents a significant challenge. In this paper, we adopt a more\nstraightforward approach aiming at integrating two well-established and\ncomputationally well-behaved components: higher-order patterns on one side and\nfuzzy equivalences expressed through similarity relations based on minimum\nT-norm on the other. We propose a unification algorithm for higher-order\npatterns modulo these similarity relations and prove its termination,\nsoundness, and completeness. This unification problem, like its crisp\ncounterpart, is unitary. The algorithm computes a most general unifier with the\nhighest degree of approximation when the given terms are unifiable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u9636\u6a21\u5f0f\u548c\u6a21\u7cca\u7b49\u4ef7\u5173\u7cfb\u7684\u7edf\u4e00\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u7ec8\u6b62\u6027\u3001\u53ef\u9760\u6027\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u76ee\u7684\u662f\u89e3\u51b3\u9ad8\u9636\u7406\u8bba\u548c\u6a21\u7cca\u903b\u8f91\u7ed3\u5408\u5f62\u5f0f\u4e0b\u7684\u63a8\u7406\u4e0e\u8ba1\u7b97\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5904\u7406\u62bd\u8c61\u51fd\u6570\u548c\u8c13\u8bcd\u7684\u51b3\u7b56\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7\u6574\u5408\u9ad8\u9636\u6a21\u5f0f\u548c\u57fa\u4e8e\u6700\u5c0fT-\u8303\u6570\u7684\u6a21\u7cca\u7b49\u4ef7\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7b97\u6cd5\u3002", "result": "\u7b97\u6cd5\u5728\u53ef\u7edf\u4e00\u65f6\u80fd\u8ba1\u7b97\u51fa\u4e00\u4e2a\u5177\u6709\u6700\u9ad8\u8fd1\u4f3c\u5ea6\u7684\u6700\u4e00\u822c\u7edf\u4e00\u5b50\uff0c\u4e14\u95ee\u9898\u4e3a\u5355\u5143\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u9636\u6a21\u7cca\u903b\u8f91\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2507.13052", "pdf": "https://arxiv.org/pdf/2507.13052", "abs": "https://arxiv.org/abs/2507.13052", "authors": ["Tianyu Song", "Feng Li", "Yuan Bi", "Angelos Karlas", "Amir Yousefi", "Daniela Branzan", "Zhongliang Jiang", "Ulrich Eck", "Nassir Navab"], "title": "Intelligent Virtual Sonographer (IVS): Enhancing Physician-Robot-Patient Communication", "categories": ["cs.HC", "cs.RO"], "comment": "Accepted at MICCAI 2025", "summary": "The advancement and maturity of large language models (LLMs) and robotics\nhave unlocked vast potential for human-computer interaction, particularly in\nthe field of robotic ultrasound. While existing research primarily focuses on\neither patient-robot or physician-robot interaction, the role of an intelligent\nvirtual sonographer (IVS) bridging physician-robot-patient communication\nremains underexplored. This work introduces a conversational virtual agent in\nExtended Reality (XR) that facilitates real-time interaction between\nphysicians, a robotic ultrasound system(RUS), and patients. The IVS agent\ncommunicates with physicians in a professional manner while offering empathetic\nexplanations and reassurance to patients. Furthermore, it actively controls the\nRUS by executing physician commands and transparently relays these actions to\nthe patient. By integrating LLM-powered dialogue with speech-to-text,\ntext-to-speech, and robotic control, our system enhances the efficiency,\nclarity, and accessibility of robotic ultrasound acquisition. This work\nconstitutes a first step toward understanding how IVS can bridge communication\ngaps in physician-robot-patient interaction, providing more control and\ntherefore trust into physician-robot interaction while improving patient\nexperience and acceptance of robotic ultrasound.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u673a\u5668\u4eba\u6280\u672f\u5f00\u53d1\u667a\u80fd\u865a\u62df\u8d85\u58f0\u533b\u5e08\uff08IVS\uff09\uff0c\u4ee5\u5f25\u5408\u533b\u751f-\u673a\u5668\u4eba-\u60a3\u8005\u4e4b\u95f4\u7684\u6c9f\u901a\u9e3f\u6c9f\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u60a3\u8005-\u673a\u5668\u4eba\u6216\u533b\u751f-\u673a\u5668\u4eba\u4ea4\u4e92\u4e0a\uff0c\u800c\u667a\u80fd\u865a\u62df\u8d85\u58f0\u533b\u5e08\u5728\u533b\u751f-\u673a\u5668\u4eba-\u60a3\u8005\u6c9f\u901a\u4e2d\u7684\u89d2\u8272\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u7684\u5bf9\u8bdd\u865a\u62df\u4ee3\u7406\uff0c\u901a\u8fc7\u6574\u5408LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u3001\u8bed\u97f3\u8f6c\u6587\u672c\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u548c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5b9e\u73b0\u533b\u751f\u3001\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\uff08RUS\uff09\u548c\u60a3\u8005\u4e4b\u95f4\u7684\u5b9e\u65f6\u4ea4\u4e92\u3002", "result": "\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u8d85\u58f0\u91c7\u96c6\u7684\u6548\u7387\u3001\u6e05\u6670\u5ea6\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u533b\u751f\u5bf9\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u63a7\u5236\u548c\u4fe1\u4efb\uff0c\u6539\u5584\u4e86\u60a3\u8005\u7684\u4f53\u9a8c\u548c\u63a5\u53d7\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3IVS\u5982\u4f55\u5f25\u5408\u533b\u751f-\u673a\u5668\u4eba-\u60a3\u8005\u4ea4\u4e92\u4e2d\u7684\u6c9f\u901a\u9e3f\u6c9f\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\uff0c\u4e3a\u672a\u6765\u7684\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.13081", "pdf": "https://arxiv.org/pdf/2507.13081", "abs": "https://arxiv.org/abs/2507.13081", "authors": ["Dongming Jin", "Weisong Sun", "Jiangping Huang", "Peng Liang", "Jifeng Xuan", "Yang Liu", "Zhi Jin"], "title": "iReDev: A Knowledge-Driven Multi-Agent Framework for Intelligent Requirements Development", "categories": ["cs.SE"], "comment": "22pages, 4 figures", "summary": "Requirements development is a critical phase as it is responsible for\nproviding a clear understanding of what stakeholders need. It involves\ncollaboration among stakeholders to extract explicit requirements and address\npotential conflicts, which is time-consuming and labor-intensive. Recently,\nmulti-agent systems for software development have attracted much attention.\nHowever, existing research provides limited support for requirements\ndevelopment and overlooks the injection of human knowledge into agents and the\nhuman-agent collaboration. % To address these issues, this paper proposes a\nknowledge-driven multi-agent framework for intelligent requirement development,\nnamed iReDev. iReDev features: iReDev consists of six knowledge-driven agents\nto support the entire requirements development. They collaboratively perform\nvarious tasks to produce a software requirements specification. iReDev focuses\non integrating human knowledge for agents, enabling them to simulate real-world\nstakeholders. iReDev uses an event-driven communication mechanism based on an\nartifact pool. Agents continuously monitor the pool and autonomously trigger\nthe next action based on its changes, enabling iReDev to handle new\nrequirements quickly. iReDev introduces a human-in-the-loop mechanism to\nsupport human-agent collaboration, ensuring that the generated artifacts align\nwith the expectations of stakeholders. We evaluated the generated artifacts and\nresults show that iReDev outperforms existing baselines in multiple aspects. We\nfurther envision three key directions and hope this work can facilitate the\ndevelopment of intelligent requirements development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aiReDev\u7684\u77e5\u8bc6\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u667a\u80fd\u9700\u6c42\u5f00\u53d1\uff0c\u901a\u8fc7\u96c6\u6210\u4eba\u7c7b\u77e5\u8bc6\u548c\u652f\u6301\u4eba-\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9700\u6c42\u5f00\u53d1\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u9700\u6c42\u5f00\u53d1\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u8017\u65f6\u7684\u9636\u6bb5\uff0c\u73b0\u6709\u7814\u7a76\u5bf9\u9700\u6c42\u5f00\u53d1\u7684\u652f\u6301\u6709\u9650\uff0c\u4e14\u5ffd\u89c6\u4e86\u4eba\u7c7b\u77e5\u8bc6\u7684\u6ce8\u5165\u548c\u4eba-\u667a\u80fd\u4f53\u534f\u4f5c\u3002", "method": "iReDev\u6846\u67b6\u7531\u516d\u4e2a\u77e5\u8bc6\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7ec4\u6210\uff0c\u91c7\u7528\u57fa\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u7684\u901a\u4fe1\u673a\u5236\u548c\u4eba\u5de5\u5e72\u9884\u673a\u5236\uff08human-in-the-loop\uff09\uff0c\u652f\u6301\u5168\u5468\u671f\u9700\u6c42\u5f00\u53d1\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0ciReDev\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u5feb\u901f\u5904\u7406\u65b0\u9700\u6c42\u5e76\u751f\u6210\u7b26\u5408\u5229\u76ca\u76f8\u5173\u8005\u671f\u671b\u7684\u6587\u6863\u3002", "conclusion": "iReDev\u4e3a\u667a\u80fd\u9700\u6c42\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u4e09\u4e2a\u5173\u952e\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2507.13065", "pdf": "https://arxiv.org/pdf/2507.13065", "abs": "https://arxiv.org/abs/2507.13065", "authors": ["John Twomey", "Sarah Foley", "Sarah Robinson", "Michael Quayle", "Matthew Peter Aylett", "Conor Linehan", "Gillian Murphy"], "title": "\"What do you expect? You're part of the internet\": Analyzing Celebrities' Experiences as Usees of Deepfake Technology", "categories": ["cs.HC"], "comment": null, "summary": "Deepfake technology is often used to create non-consensual synthetic intimate\nimagery (NSII), mainly of celebrity women. Through Critical Discursive\nPsychological analysis we ask; i) how celebrities construct being targeted by\ndeepfakes and ii) how they navigate infrastructural and social obstacles when\nseeking recourse. In this paper, we adopt Baumers concept of Usees\n(stakeholders who are non-consenting, unaware and directly targeted by\ntechnology), to understand public statements made by eight celebrity women and\none non-binary individual targeted with NSII. Celebrities describe harms of\nbeing non-consensually targeted by deepfakes and the distress of becoming aware\nof these videos. They describe various infrastructural/social factors (e.g.\nblaming/ silencing narratives and the industry behind deepfake abuse) which\nhinder activism and recourse. This work has implications in recognizing the\nroles of various stakeholders in the infrastructures underlying deepfake abuse\nand the potential of human-computer interaction to improve existing recourses\nfor NSII. We also contribute to understanding how false beliefs online\nfacilitate deepfake abuse. Future work should involve interventions which\nchallenge the values and false beliefs which motivate NSII\ncreation/dissemination.", "AI": {"tldr": "\u5206\u6790\u540d\u4eba\u5973\u6027\u548c\u975e\u4e8c\u5143\u6027\u522b\u8005\u5982\u4f55\u5e94\u5bf9\u975e\u81ea\u613f\u5408\u6210\u4eb2\u5bc6\u5f71\u50cf\uff08NSII\uff09\u7684\u56f0\u6270\u53ca\u793e\u4f1a\u969c\u788d\uff0c\u5e76\u63d0\u51fa\u6280\u672f\u548c\u793e\u4f1a\u5e72\u9884\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8\u540d\u4eba\u5982\u4f55\u5e94\u5bf9NSII\u7684\u5371\u5bb3\u53ca\u793e\u4f1a\u7ed3\u6784\u6027\u969c\u788d\uff0c\u547c\u5401\u5173\u6ce8\u6280\u672f\u6ee5\u7528\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6279\u5224\u6027\u8bdd\u8bed\u5fc3\u7406\u5206\u6790\u6cd5\uff0c\u7814\u7a76\u516b\u4f4d\u540d\u4eba\u5973\u6027\u548c\u4e00\u4f4d\u975e\u4e8c\u5143\u6027\u522b\u8005\u7684\u516c\u5f00\u58f0\u660e\u3002", "result": "\u540d\u4eba\u63cf\u8ff0\u4e86\u88abNSII\u975e\u81ea\u613f\u9488\u5bf9\u7684\u5371\u5bb3\u53ca\u793e\u4f1a/\u7ed3\u6784\u6027\u969c\u788d\u5982\u4f55\u963b\u788d\u7ef4\u6743\u3002", "conclusion": "\u63d0\u51fa\u9700\u8981\u6280\u672f\u548c\u793e\u4f1a\u5e72\u9884\uff0c\u6311\u6218NSII\u80cc\u540e\u7684\u4ef7\u503c\u89c2\u548c\u9519\u8bef\u4fe1\u606f\u3002"}}
{"id": "2507.13095", "pdf": "https://arxiv.org/pdf/2507.13095", "abs": "https://arxiv.org/abs/2507.13095", "authors": ["Dongming Jin", "Zhi Jin", "Linyu Li", "Xiaohong Chen"], "title": "A Conceptual Framework for Requirements Engineering of Pretrained-Model-Enabled Systems", "categories": ["cs.SE"], "comment": "5pages, 1 figure", "summary": "Recent advances in large pretrained models have led to their widespread\nintegration as core components in modern software systems. The trend is\nexpected to continue in the foreseeable future. Unlike traditional software\nsystems governed by deterministic logic, systems powered by pretrained models\nexhibit distinctive and emergent characteristics, such as ambiguous capability\nboundaries, context-dependent behavior, and continuous evolution. These\nproperties fundamentally challenge long-standing assumptions in requirements\nengineering, including functional decomposability and behavioral\npredictability. This paper investigates this problem and advocates for a\nrethinking of existing requirements engineering methodologies. We propose a\nconceptual framework tailored to requirements engineering of\npretrained-model-enabled software systems and outline several promising\nresearch directions within this framework. This vision helps provide a guide\nfor researchers and practitioners to tackle the emerging challenges in\nrequirements engineering of pretrained-model-enabled systems.", "AI": {"tldr": "\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6311\u6218\u4e86\u4f20\u7edf\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u9488\u5bf9\u6b64\u7c7b\u7cfb\u7edf\u7684\u6982\u5ff5\u6846\u67b6\u548c\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u6838\u5fc3\u7ec4\u4ef6\u7684\u8f6f\u4ef6\u7cfb\u7edf\u5bf9\u4f20\u7edf\u9700\u6c42\u5de5\u7a0b\u5047\u8bbe\u7684\u6311\u6218\uff0c\u5982\u529f\u80fd\u53ef\u5206\u89e3\u6027\u548c\u884c\u4e3a\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8f6f\u4ef6\u7cfb\u7edf\u7684\u9700\u6c42\u5de5\u7a0b\u6982\u5ff5\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u76f8\u5173\u7814\u7a76\u65b9\u5411\u3002", "result": "\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u5e94\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u7cfb\u7edf\u7684\u9700\u6c42\u5de5\u7a0b\u6311\u6218\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u601d\u8003\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\u4ee5\u9002\u5e94\u9884\u8bad\u7ec3\u6a21\u578b\u7cfb\u7edf\u7684\u72ec\u7279\u6027\u548c\u6301\u7eed\u6f14\u8fdb\u7279\u6027\u3002"}}
{"id": "2507.13167", "pdf": "https://arxiv.org/pdf/2507.13167", "abs": "https://arxiv.org/abs/2507.13167", "authors": ["Ehud Sharlin", "Benjamin Watson", "Yoshifumi Kitamura", "Fumio Kishino", "Yuichi Itoh"], "title": "On tangible user interfaces, humans and spatiality", "categories": ["cs.HC"], "comment": null, "summary": "Like the prehistoric twig and stone, tangible user interfaces (TUIs) are\nobjects manipulated by humans. TUI success will depend on how well they exploit\nspatiality, the intuitive spatial skills humans have with the objects they use.\nIn this paper we carefully examine the relationship between humans and physical\nobjects, and related previous research. From this examination we distill a set\nof observations, and turn these into heuristics for incorporation of spatiality\ninto TUI application design, a cornerstone for their success. Following this\nline of thought, we identify spatial TUIs, the subset of TUIs that mediate\ninteraction with shape, space and structure. We then examine several existing\nspatial TUIs using our heuristics.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u53ef\u89e6\u6478\u7528\u6237\u754c\u9762\uff08TUI\uff09\u8bbe\u8ba1\u4e2d\u5229\u7528\u4eba\u7c7b\u7684\u7a7a\u95f4\u76f4\u89c9\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u73b0\u6709\u7a7a\u95f4TUI\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5229\u7528\u4eba\u7c7b\u4e0e\u7269\u7406\u5bf9\u8c61\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u63d0\u5347\u53ef\u89e6\u6478\u7528\u6237\u754c\u9762\u7684\u8bbe\u8ba1\u6548\u679c\uff0c\u8fd9\u662fTUI\u6210\u529f\u7684\u5173\u952e\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u4eba\u7c7b\u4e0e\u7269\u7406\u5bf9\u8c61\u7684\u5173\u7cfb\u53ca\u76f8\u5173\u6587\u732e\uff0c\u63d0\u70bc\u51fa\u4e00\u5957\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u8bbe\u8ba1\u7a7a\u95f4TUI\u4e2d\u3002", "result": "\u63d0\u51fa\u4e86\u7a7a\u95f4TUI\u7684\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u73b0\u6709TUI\u9a8c\u8bc1\u4e86\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u7a7a\u95f4\u6027\u662fTUI\u8bbe\u8ba1\u7684\u91cd\u8981\u57fa\u77f3\uff0c\u901a\u8fc7\u5408\u7406\u5e94\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347TUI\u7684\u7528\u6237\u4f53\u9a8c\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2507.13117", "pdf": "https://arxiv.org/pdf/2507.13117", "abs": "https://arxiv.org/abs/2507.13117", "authors": ["Andreas Pointner", "Josef Pichler", "Herbert Pr\u00e4hofer"], "title": "Inferring Attributed Grammars from Parser Implementations", "categories": ["cs.SE"], "comment": "Accepted to ICSME 2025", "summary": "Software systems that process structured inputs often lack complete and\nup-to-date specifications, which specify the input syntax and the semantics of\ninput processing. While grammar mining techniques have focused on recovering\nsyntactic structures, the semantics of input processing remains largely\nunexplored. In this work, we introduce a novel approach for inferring\nattributed grammars from parser implementations. Given an input grammar, our\ntechnique dynamically analyzes the implementation of recursive descent parsers\nto reconstruct the semantic aspects of input handling, resulting in\nspecifications in the form of attributed grammars. By observing program\nexecutions and mapping the program's runtime behavior to the grammar, we\nsystematically extract and embed semantic actions into the grammar rules. This\nenables comprehensive specification recovery. We demonstrate the feasibility of\nour approach using an initial set of programs, showing that it can accurately\nreproduce program behavior through the generated attributed grammars.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u6790\u9012\u5f52\u4e0b\u964d\u89e3\u6790\u5668\u7684\u5b9e\u73b0\uff0c\u4ece\u8bed\u6cd5\u4e2d\u63a8\u65ad\u51fa\u5c5e\u6027\u6587\u6cd5\uff0c\u4ece\u800c\u6062\u590d\u8f93\u5165\u5904\u7406\u7684\u8bed\u4e49\u89c4\u8303\u3002", "motivation": "\u73b0\u6709\u8bed\u6cd5\u6316\u6398\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u8bed\u6cd5\u7ed3\u6784\u6062\u590d\uff0c\u800c\u5bf9\u8f93\u5165\u5904\u7406\u7684\u8bed\u4e49\u89c4\u8303\u7f3a\u4e4f\u63a2\u7d22\u3002", "method": "\u52a8\u6001\u5206\u6790\u9012\u5f52\u4e0b\u964d\u89e3\u6790\u5668\u7684\u5b9e\u73b0\uff0c\u89c2\u5bdf\u7a0b\u5e8f\u6267\u884c\u5e76\u6620\u5c04\u8fd0\u884c\u65f6\u884c\u4e3a\u5230\u8bed\u6cd5\uff0c\u5d4c\u5165\u8bed\u4e49\u52a8\u4f5c\u5230\u6587\u6cd5\u89c4\u5219\u4e2d\u3002", "result": "\u901a\u8fc7\u521d\u59cb\u7a0b\u5e8f\u96c6\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u751f\u6210\u5c5e\u6027\u6587\u6cd5\uff0c\u91cd\u73b0\u7a0b\u5e8f\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5b9e\u73b0\u4e86\u5168\u9762\u89c4\u8303\u6062\u590d\uff0c\u4e3a\u8f93\u5165\u5904\u7406\u7684\u8bed\u4e49\u89c4\u8303\u586b\u8865\u4e86\u7a7a\u767d\u3002"}}
{"id": "2507.13235", "pdf": "https://arxiv.org/pdf/2507.13235", "abs": "https://arxiv.org/abs/2507.13235", "authors": ["Minghao Cai", "Guher Gorgun", "Carrie Demmans Epp"], "title": "Difficulty as a Proxy for Measuring Intrinsic Cognitive Load Item", "categories": ["cs.HC"], "comment": "13 pages, presented at AERA 2025 Annual Meeting, Denver, Colorado,\n  April 2025", "summary": "Cognitive load is key to ensuring an optimal learning experience. However,\nmeasuring the cognitive load of educational tasks typically relies on\nself-report measures which has been criticized by researchers for being\nsubjective. In this study, we investigated the feasibility of using item\ndifficulty parameters as a proxy for measuring cognitive load in an online\nlearning platform. Difficulty values that were derived using item-response\ntheory were consistent with theories of how intrinsic and extraneous load\ncontribute to cognitive load. This finding suggests that we can use item\ndifficulty to represent intrinsic load when modelling cognitive load in\nlearning games.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u9879\u76ee\u96be\u5ea6\u53c2\u6570\u4f5c\u4e3a\u5728\u7ebf\u5b66\u4e60\u5e73\u53f0\u4e2d\u8ba4\u77e5\u8d1f\u8377\u4ee3\u7406\u6d4b\u91cf\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4f20\u7edf\u8ba4\u77e5\u8d1f\u8377\u6d4b\u91cf\u4f9d\u8d56\u4e3b\u89c2\u6027\u5f3a\u7684\u81ea\u6211\u62a5\u544a\uff0c\u7814\u7a76\u5bfb\u6c42\u66f4\u5ba2\u89c2\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9879\u76ee\u53cd\u5e94\u7406\u8bba\u63a8\u5bfc\u7684\u9879\u76ee\u96be\u5ea6\u503c\uff0c\u5206\u6790\u5176\u5bf9\u5185\u5728\u548c\u5916\u5728\u8d1f\u8377\u7684\u8d21\u732e\u3002", "result": "\u53d1\u73b0\u9879\u76ee\u96be\u5ea6\u4e0e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u4e00\u81f4\uff0c\u53ef\u4f5c\u4e3a\u5185\u5728\u8d1f\u8377\u7684\u4ee3\u7406\u3002", "conclusion": "\u5728\u5efa\u6a21\u5b66\u4e60\u6e38\u620f\u4e2d\u7684\u8ba4\u77e5\u8d1f\u8377\u65f6\uff0c\u9879\u76ee\u96be\u5ea6\u53ef\u4f5c\u4e3a\u5185\u5728\u8d1f\u8377\u7684\u6709\u6548\u4ee3\u8868\u3002"}}
{"id": "2507.13123", "pdf": "https://arxiv.org/pdf/2507.13123", "abs": "https://arxiv.org/abs/2507.13123", "authors": ["Xin Yin", "Xinrui Li", "Chao Ni", "Xiaodan Xu", "Xiaohu Yang"], "title": "Detecting LLM-generated Code with Subtle Modification by Adversarial Training", "categories": ["cs.SE"], "comment": null, "summary": "With the rapid development of Large Language Models (LLMs), their powerful\ncode-generation capabilities have been widely applied in tasks like code\ncompletion and automated development, demonstrating the value of improving\ncoding efficiency. However, the extensive use of LLM-generated code also raises\nseveral new challenges. On the one hand, issues such as the regulation of code\nprovenance, copyright disputes, and code quality have become increasingly\nconcerning. How to effectively detect LLM-generated code and ensure its\ncompliant and responsible use has become a critical and urgent issue. On the\nother hand, in practical applications, LLM-generated code is often subject to\nmanual modifications, such as variable renaming or structural adjustments.\nAlthough some recent studies have proposed training-based and zero-shot methods\nfor detecting LLM-generated code, these approaches show insufficient robustness\nwhen facing modified LLM-generated code, and there is a lack of an effective\nsolution. To address the real-world scenario where LLM-generated code may\nundergo minor modifications, we propose CodeGPTSensor+, an enhanced version of\nCodeGPTSensor, which employs adversarial training to improve robustness against\ninput perturbations. CodeGPTSensor+ integrates an adversarial sample generation\nmodule, Multi-objective Identifier and Structure Transformation (MIST), which\nsystematically generates both high-quality and representative adversarial\nsamples. This module effectively enhances the model's resistance against\ndiverse adversarial attacks. Experimental results on the HMCorp dataset\ndemonstrate that CodeGPTSensor+ significantly improves detection accuracy on\nthe adversarial test set while maintaining high accuracy on the original test\nset, showcasing superior robustness compared to CodeGPTSensor.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u6709\u6548\u68c0\u6d4b\u5e76\u589e\u5f3a\u5bf9\u7ecf\u8fc7\u4fee\u6539\u7684LLM\u751f\u6210\u4ee3\u7801\u7684\u8bc6\u522b\u9c81\u68d2\u6027\uff0c\u63d0\u51faCodeGPTSensor+\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u4ee3\u7801\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5e26\u6765\u7684\u4ee3\u7801\u6eaf\u6e90\u3001\u7248\u6743\u548c\u8d28\u91cf\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4e14\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u4fee\u6539\u540e\u7684\u4ee3\u7801\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCodeGPTSensor+\uff0c\u91c7\u7528\u5bf9\u6297\u8bad\u7ec3\u548cMIST\u6a21\u5757\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u6297\u6837\u672c\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728HMCorp\u6570\u636e\u96c6\u4e0a\uff0cCodeGPTSensor+\u5728\u5bf9\u6297\u6d4b\u8bd5\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5728\u539f\u59cb\u6d4b\u8bd5\u96c6\u4e0a\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "CodeGPTSensor+\u6709\u6548\u89e3\u51b3\u4e86\u4fee\u6539\u540eLLM\u751f\u6210\u4ee3\u7801\u7684\u68c0\u6d4b\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.13247", "pdf": "https://arxiv.org/pdf/2507.13247", "abs": "https://arxiv.org/abs/2507.13247", "authors": ["Ruohao Li", "Jiawei Li", "Jia Sun", "Zhiqing Wu", "Zisu Li", "Ziyan Wang", "Ge Lin Kan", "Mingming Fan"], "title": "RemVerse: Supporting Reminiscence Activities for Older Adults through AI-Assisted Virtual Reality", "categories": ["cs.HC"], "comment": null, "summary": "Reminiscence activities, which involve recalling and sharing past\nexperiences, have proven beneficial for improving cognitive function, mood, and\noverall well-being. However, urbanization has led to the disappearance of\nfamiliar environments, removing visual and audio cues for effective\nreminiscence. While old photos can serve as visual cues to aid reminiscence, it\nis challenging for people to reconstruct the reminisced content and environment\nthat are not in the photos. Virtual reality (VR) and artificial intelligence\n(AI) offer the ability to reconstruct an immersive environment with dynamic\ncontent and to converse with people to help them gradually reminisce. We\ndesigned RemVerse, an AI-empowered VR prototype aimed to support reminiscence\nactivities. Integrating generative models and AI agent into a VR environment,\nRemVerse helps older adults reminisce with AI-generated visual cues and\ninteractive dialogues. Our user study with 14 older adults showed that RemVerse\neffectively supported reminiscence activities by triggering, concretizing, and\ndeepening personal memories, while fostering increased engagement and autonomy\namong older adults. Based on our findings, we proposed design implications to\nmake reminiscence activities in AI-assisted VR more accessible and engaging for\nolder adults.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRemVerse\uff0c\u4e00\u4e2a\u7ed3\u5408VR\u548cAI\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u751f\u6210\u89c6\u89c9\u63d0\u793a\u548c\u4e92\u52a8\u5bf9\u8bdd\u5e2e\u52a9\u8001\u5e74\u4eba\u56de\u5fc6\u8fc7\u53bb\uff0c\u6709\u6548\u63d0\u5347\u8ba4\u77e5\u529f\u80fd\u548c\u53c2\u4e0e\u611f\u3002", "motivation": "\u57ce\u5e02\u5316\u5bfc\u81f4\u719f\u6089\u73af\u5883\u6d88\u5931\uff0c\u4f20\u7edf\u7167\u7247\u65e0\u6cd5\u5b8c\u5168\u652f\u6301\u56de\u5fc6\uff0c\u800cVR\u548cAI\u80fd\u52a8\u6001\u91cd\u5efa\u6c89\u6d78\u5f0f\u73af\u5883\u4ee5\u8f85\u52a9\u56de\u5fc6\u3002", "method": "\u8bbe\u8ba1RemVerse\u539f\u578b\uff0c\u6574\u5408\u751f\u6210\u6a21\u578b\u548cAI\u4ee3\u7406\u5230VR\u4e2d\uff0c\u63d0\u4f9bAI\u751f\u6210\u7684\u89c6\u89c9\u63d0\u793a\u548c\u4e92\u52a8\u5bf9\u8bdd\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\uff0cRemVerse\u80fd\u6709\u6548\u89e6\u53d1\u3001\u5177\u8c61\u5316\u548c\u6df1\u5316\u8001\u5e74\u4eba\u8bb0\u5fc6\uff0c\u589e\u5f3a\u53c2\u4e0e\u611f\u548c\u81ea\u4e3b\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u8bbe\u8ba1\u5efa\u8bae\uff0c\u4f7fAI\u8f85\u52a9\u7684VR\u56de\u5fc6\u6d3b\u52a8\u5bf9\u8001\u5e74\u4eba\u66f4\u6613\u7528\u548c\u5438\u5f15\u3002"}}
{"id": "2407.18798", "pdf": "https://arxiv.org/pdf/2407.18798", "abs": "https://arxiv.org/abs/2407.18798", "authors": ["Abiodun Finbarrs Oketunji"], "title": "Predicting 3D Rigid Body Dynamics with Deep Residual Network", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.SE", "I.2.7"], "comment": null, "summary": "This study investigates the application of deep residual networks for\npredicting the dynamics of interacting three-dimensional rigid bodies. We\npresent a framework combining a 3D physics simulator implemented in C++ with a\ndeep learning model constructed using PyTorch. The simulator generates training\ndata encompassing linear and angular motion, elastic collisions, fluid\nfriction, gravitational effects, and damping. Our deep residual network,\nconsisting of an input layer, multiple residual blocks, and an output layer, is\ndesigned to handle the complexities of 3D dynamics. We evaluate the network's\nperformance using a datasetof 10,000 simulated scenarios, each involving 3-5\ninteracting rigid bodies. The model achieves a mean squared error of 0.015 for\nposition predictions and 0.022 for orientation predictions, representing a 25%\nimprovement over baseline methods. Our results demonstrate the network's\nability to capture intricate physical interactions, with particular success in\npredicting elastic collisions and rotational dynamics. This work significantly\ncontributes to physics-informed machine learning by showcasing the immense\npotential of deep residual networks in modeling complex 3D physical systems. We\ndiscuss our approach's limitations and propose future directions for improving\ngeneralization to more diverse object shapes and materials.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u5728\u9884\u6d4b\u4e09\u7ef4\u521a\u4f53\u76f8\u4e92\u4f5c\u7528\u52a8\u6001\u4e2d\u7684\u5e94\u7528\uff0c\u7ed3\u5408C++\u7269\u7406\u6a21\u62df\u5668\u548cPyTorch\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u67423D\u52a8\u529b\u5b66\u7684\u9ad8\u6548\u5efa\u6a21\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5229\u7528\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u6355\u6349\u4e09\u7ef4\u521a\u4f53\u95f4\u7684\u590d\u6742\u7269\u7406\u4ea4\u4e92\uff0c\u586b\u8865\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u5728\u6b64\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u901a\u8fc7C++\u7269\u7406\u6a21\u62df\u5668\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff08\u6db5\u76d6\u7ebf\u6027/\u89d2\u8fd0\u52a8\u3001\u78b0\u649e\u7b49\uff09\uff0c\u5e76\u4f7f\u7528PyTorch\u6784\u5efa\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u6a21\u578b\u572810,000\u4e2a\u6a21\u62df\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f4d\u7f6e\u548c\u65b9\u5411\u9884\u6d4b\u8bef\u5dee\u5206\u522b\u4e3a0.015\u548c0.022\uff0c\u8f83\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534725%\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u5bf9\u590d\u67423D\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u6f5c\u529b\u5de8\u5927\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u7269\u4f53\u5f62\u72b6\u548c\u6750\u6599\u7684\u7814\u7a76\u3002"}}
{"id": "2507.13309", "pdf": "https://arxiv.org/pdf/2507.13309", "abs": "https://arxiv.org/abs/2507.13309", "authors": ["Hanxiu 'Hazel' Zhu", "Ruijia Chen", "Yuhang Zhao"], "title": "FocusView: Understanding and Customizing Informational Video Watching Experiences for Viewers with ADHD", "categories": ["cs.HC"], "comment": null, "summary": "While videos have become increasingly prevalent in delivering information\nacross different educational and professional contexts, individuals with ADHD\noften face attention challenges when watching informational videos due to the\ndynamic, multimodal, yet potentially distracting video elements. To understand\nand address this critical challenge, we designed \\textit{FocusView}, a video\ncustomization interface that allows viewers with ADHD to customize\ninformational videos from different aspects. We evaluated FocusView with 12\nparticipants with ADHD and found that FocusView significantly improved the\nviewability of videos by reducing distractions. Through the study, we uncovered\nparticipants' diverse perceptions of video distractions (e.g., background music\nas a distraction vs. stimulation boost) and their customization preferences,\nhighlighting unique ADHD-relevant needs in designing video customization\ninterfaces (e.g., reducing the number of options to avoid distraction caused by\ncustomization itself). We further derived design considerations for future\nvideo customization systems for the ADHD community.", "AI": {"tldr": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aFocusView\u7684\u89c6\u9891\u5b9a\u5236\u754c\u9762\uff0c\u5e2e\u52a9ADHD\u60a3\u8005\u51cf\u5c11\u89c6\u9891\u4e2d\u7684\u6ce8\u610f\u529b\u5206\u6563\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c2\u770b\u4f53\u9a8c\u3002", "motivation": "ADHD\u60a3\u8005\u5728\u89c2\u770b\u4fe1\u606f\u6027\u89c6\u9891\u65f6\u56e0\u591a\u5a92\u4f53\u5143\u7d20\u7684\u5e72\u6270\u800c\u9762\u4e34\u6ce8\u610f\u529b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u5b9a\u5236\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u89c2\u770b\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86FocusView\u89c6\u9891\u5b9a\u5236\u754c\u9762\uff0c\u5141\u8bb8\u7528\u6237\u4ece\u591a\u4e2a\u65b9\u9762\u81ea\u5b9a\u4e49\u89c6\u9891\u5185\u5bb9\uff0c\u4ee5\u51cf\u5c11\u5e72\u6270\u3002", "result": "FocusView\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7684\u53ef\u89c2\u770b\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86ADHD\u7528\u6237\u5bf9\u5e72\u6270\u611f\u77e5\u7684\u591a\u6837\u6027\u548c\u5b9a\u5236\u504f\u597d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u7684ADHD\u89c6\u9891\u5b9a\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u5efa\u8bae\uff0c\u5f3a\u8c03\u51cf\u5c11\u9009\u9879\u4ee5\u907f\u514d\u5b9a\u5236\u672c\u8eab\u5e26\u6765\u7684\u5206\u5fc3\u3002"}}
{"id": "2507.12488", "pdf": "https://arxiv.org/pdf/2507.12488", "abs": "https://arxiv.org/abs/2507.12488", "authors": ["Marco De Luca", "Sergio Di Martino", "Sergio Di Meglio", "Anna Rita Fasolino", "Luigi Libero Lucio Starace", "Porfirio Tramontana"], "title": "Rookie Mistakes: Measuring Software Quality in Student Projects to Guide Educational Enhancement", "categories": ["cs.CY", "cs.SE"], "comment": "Manuscript accepted for the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA)", "summary": "When teaching Programming and Software Engineering in Bachelor's Degree\nprograms, the emphasis on creating functional software projects often\novershadows the focus on software quality, a trend that aligns with ACM\ncurricula recommendations. Software Engineering courses are typically\nintroduced later in the curriculum, and can generally allocate only limited\ntime to quality-related topics, leaving educators with the challenge of\ndeciding which quality aspects to prioritize. In this decision, the literature\noffers limited guidance, as most existing studies focus on code written by\nnovice students and small code units, making it unclear whether those findings\nextend to intermediate-level students with foundational object-oriented\nprogramming skills working on more complex software projects. To address this\ngap, we analyze 83 object-oriented team projects developed by 172 university\nstudents across 4 different editions of the Object-Oriented Programming course.\nWe apply a static analysis pipeline used in prior research to assess software\nquality, combining SonarQube and ArchUnit to detect code smells and\narchitectural anti-patterns. Our findings highlight recurring quality issues\nand offer concrete evidence of the challenges students face at this stage,\nproviding valuable guidance for educators aiming to continuously improve\nSoftware Engineering curricula and promote quality-oriented development\npractices.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u672c\u79d1\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u5982\u4f55\u66f4\u6709\u6548\u5730\u6559\u6388\u8f6f\u4ef6\u8d28\u91cf\uff0c\u5206\u6790\u4e86\u5b66\u751f\u56e2\u961f\u9879\u76ee\u7684\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u672c\u79d1\u7f16\u7a0b\u6559\u5b66\u4e2d\u8f6f\u4ef6\u8d28\u91cf\u6559\u80b2\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7814\u7a76\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u5bf9\u4e2d\u7ea7\u6c34\u5e73\u5b66\u751f\u5728\u590d\u6742\u9879\u76ee\u4e2d\u7684\u8d28\u91cf\u95ee\u9898\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e8683\u4e2a\u7531172\u540d\u5b66\u751f\u5f00\u53d1\u7684\u5bf9\u8c61\u5bfc\u5411\u56e2\u961f\u9879\u76ee\uff0c\u4f7f\u7528SonarQube\u548cArchUnit\u8fdb\u884c\u9759\u6001\u5206\u6790\uff0c\u68c0\u6d4b\u4ee3\u7801\u5f02\u5473\u548c\u67b6\u6784\u53cd\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b66\u751f\u56e2\u961f\u9879\u76ee\u4e2d\u5b58\u5728\u91cd\u590d\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u8bfe\u7a0b\u6539\u8fdb\u63d0\u4f9b\u4e86\u5177\u4f53\u4f9d\u636e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6559\u80b2\u8005\u6539\u8fdb\u8f6f\u4ef6\u5de5\u7a0b\u8bfe\u7a0b\u548c\u63a8\u5e7f\u8d28\u91cf\u5bfc\u5411\u7684\u5f00\u53d1\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002"}}
{"id": "2507.12625", "pdf": "https://arxiv.org/pdf/2507.12625", "abs": "https://arxiv.org/abs/2507.12625", "authors": ["David Freire-Obreg\u00f3n", "Agnieszka Dubiel", "Prasoon Kumar Vinodkumar", "Gholamreza Anbarjafari", "Dorota Kami\u0144ska", "Modesto Castrill\u00f3n-Santana"], "title": "Mapping Emotions in the Brain: A Bi-Hemispheric Neural Model with Explainable Deep Learning", "categories": ["q-bio.NC", "cs.HC", "eess.SP"], "comment": "Accepted at Neuro-Inspired AI Workshop at 23rd International\n  Conference on Image Analysis and Processing (ICIAP 2025)", "summary": "Recent advances have shown promise in emotion recognition from\nelectroencephalogram (EEG) signals by employing bi-hemispheric neural\narchitectures that incorporate neuroscientific priors into deep learning\nmodels. However, interpretability remains a significant limitation for their\napplication in sensitive fields such as affective computing and cognitive\nmodeling. In this work, we introduce a post-hoc interpretability framework\ntailored to dual-stream EEG classifiers, extending the Local Interpretable\nModel-Agnostic Explanations (LIME) approach to accommodate structured,\nbi-hemispheric inputs. Our method adapts LIME to handle structured two-branch\ninputs corresponding to left and right-hemisphere EEG channel groups. It\ndecomposes prediction relevance into per-channel contributions across\nhemispheres and emotional classes. We apply this framework to a previously\nvalidated dual-branch recurrent neural network trained on EmoNeuroDB, a dataset\nof EEG recordings captured during a VR-based emotion elicitation task. The\nresulting explanations reveal emotion-specific hemispheric activation patterns\nconsistent with known neurophysiological phenomena, such as frontal\nlateralization in joy and posterior asymmetry in sadness. Furthermore, we\naggregate local explanations across samples to derive global channel importance\nprofiles, enabling a neurophysiologically grounded interpretation of the\nmodel's decisions. Correlation analysis between symmetric electrodes further\nhighlights the model's emotion-dependent lateralization behavior, supporting\nthe functional asymmetries reported in affective neuroscience.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u53cc\u6d41\u8111\u7535\u5206\u7c7b\u5668\u7684\u540e\u9a8c\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u6269\u5c55\u4e86LIME\u65b9\u6cd5\u4ee5\u5904\u7406\u53cc\u534a\u7403\u7ed3\u6784\u8f93\u5165\uff0c\u63ed\u793a\u4e86\u4e0e\u795e\u7ecf\u751f\u7406\u73b0\u8c61\u4e00\u81f4\u7684\u6fc0\u6d3b\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u53cc\u534a\u7403\u795e\u7ecf\u7f51\u7edc\u5728\u8111\u7535\u60c5\u7eea\u8bc6\u522b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5728\u654f\u611f\u9886\u57df\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86LIME\uff0c\u652f\u6301\u53cc\u534a\u7403\u7ed3\u6784\u8f93\u5165\uff0c\u5206\u89e3\u9884\u6d4b\u76f8\u5173\u7684\u901a\u9053\u8d21\u732e\uff0c\u5e76\u7ed3\u5408\u5bf9\u79f0\u7535\u6781\u5206\u6790\u6a21\u578b\u7684\u4fa7\u5316\u884c\u4e3a\u3002", "result": "\u6846\u67b6\u63ed\u793a\u4e86\u60c5\u7eea\u7279\u5b9a\u7684\u534a\u7403\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5982\u5feb\u4e50\u65f6\u7684\u989d\u53f6\u4fa7\u5316\u548c\u60b2\u4f24\u65f6\u7684\u540e\u90e8\u4e0d\u5bf9\u79f0\uff0c\u4e14\u4e0e\u5df2\u77e5\u795e\u7ecf\u751f\u7406\u73b0\u8c61\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53cc\u6d41\u8111\u7535\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u795e\u7ecf\u751f\u7406\u5b66\u57fa\u7840\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u60c5\u611f\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u529f\u80fd\u4e0d\u5bf9\u79f0\u6027\u3002"}}
{"id": "2507.12674", "pdf": "https://arxiv.org/pdf/2507.12674", "abs": "https://arxiv.org/abs/2507.12674", "authors": ["Mihran Miroyan", "Rose Niousha", "Joseph E. Gonzalez", "Gireeja Ranade", "Narges Norouzi"], "title": "ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle", "categories": ["cs.CY", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong performance on programming\ntasks, but can they generate student-like code like real students - imperfect,\niterative, and stylistically diverse? We present ParaStudent, a systematic\nstudy of LLM-based \"student-like\" code generation in an introductory\nprogramming course setting. Using a dataset of timestamped student submissions\nacross multiple semesters, we design low- and high-resolution experiments to\nmodel student progress and evaluate code outputs along semantic, functional,\nand stylistic dimensions. Our results show that fine-tuning significantly\nimproves alignment with real student trajectories and captures error patterns,\nincremental improvements, and stylistic variations more faithfully. This study\nshows that modeling realistic student code requires capturing learning dynamics\nthrough context-aware generation, temporal modeling, and multi-dimensional\nevaluation. Code for experiments and evaluation is available at\n\\href{https://github.com/mmiroyan/ParaStudent}{\\texttt{github.com/mmiroyan/ParaStudent}}.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u5426\u751f\u6210\u7c7b\u4f3c\u771f\u5b9e\u5b66\u751f\u7684\u4ee3\u7801\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5\u201cParaStudent\u201d\u6765\u5206\u6790\u5176\u5728\u7f16\u7a0b\u8bfe\u7a0b\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76LLM\u662f\u5426\u80fd\u591f\u751f\u6210\u4e0e\u771f\u5b9e\u5b66\u751f\u4ee3\u7801\u76f8\u4f3c\u7684\u4e0d\u5b8c\u7f8e\u3001\u8fed\u4ee3\u4e14\u98ce\u683c\u591a\u6837\u7684\u4ee3\u7801\uff0c\u4ee5\u6a21\u62df\u5b66\u751f\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u5229\u7528\u591a\u5b66\u671f\u7684\u5b66\u751f\u63d0\u4ea4\u6570\u636e\u8bbe\u8ba1\u5b9e\u9a8c\uff0c\u4ece\u8bed\u4e49\u3001\u529f\u80fd\u3001\u548c\u98ce\u683c\u7ef4\u5ea6\u8bc4\u4f30\u4ee3\u7801\u8f93\u51fa\uff0c\u5e76\u8fdb\u884c\u5fae\u8c03\u4ee5\u63d0\u9ad8\u6a21\u578b\u8868\u73b0\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u4e0e\u771f\u5b9e\u5b66\u751f\u5b66\u4e60\u8f68\u8ff9\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u6355\u6349\u9519\u8bef\u6a21\u5f0f\u3001\u589e\u91cf\u6539\u8fdb\u548c\u98ce\u683c\u53d8\u5316\u3002", "conclusion": "\u6a21\u62df\u771f\u5b9e\u5b66\u751f\u4ee3\u7801\u9700\u8981\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u751f\u6210\u3001\u65f6\u95f4\u5efa\u6a21\u548c\u591a\u7ef4\u8bc4\u4f30\u7b49\u65b9\u6cd5\u3002"}}
{"id": "2507.12652", "pdf": "https://arxiv.org/pdf/2507.12652", "abs": "https://arxiv.org/abs/2507.12652", "authors": ["Kai Malcolm", "C\u00e9sar Uribe", "Momona Yamagami"], "title": "Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective", "categories": ["cs.LG", "cs.CR", "cs.HC"], "comment": "23 pages, 7 figures", "summary": "Invasive and non-invasive neural interfaces hold promise as high-bandwidth\ninput devices for next-generation technologies. However, neural signals\ninherently encode sensitive information about an individual's identity and\nhealth, making data sharing for decoder training a critical privacy challenge.\nFederated learning (FL), a distributed, privacy-preserving learning framework,\npresents a promising solution, but it remains unexplored in closed-loop\nadaptive neural interfaces. Here, we introduce FL-based neural decoding and\nsystematically evaluate its performance and privacy using high-dimensional\nelectromyography signals in both open- and closed-loop scenarios. In open-loop\nsimulations, FL significantly outperformed local learning baselines,\ndemonstrating its potential for high-performance, privacy-conscious neural\ndecoding. In contrast, closed-loop user studies required adapting FL methods to\naccommodate single-user, real-time interactions, a scenario not supported by\nstandard FL. This modification resulted in local learning decoders surpassing\nthe adapted FL approach in closed-loop performance, yet local learning still\ncarried higher privacy risks. Our findings highlight a critical\nperformance-privacy tradeoff in real-time adaptive applications and indicate\nthe need for FL methods specifically designed for co-adaptive, single-user\napplications.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u5728\u795e\u7ecf\u63a5\u53e3\u89e3\u7801\u4e2d\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u5b9e\u65f6\u95ed\u73af\u5e94\u7528\u4e2d\u9700\u9002\u5e94\u6027\u8c03\u6574\uff0c\u5b58\u5728\u6027\u80fd\u4e0e\u9690\u79c1\u7684\u6743\u8861\u3002", "motivation": "\u795e\u7ecf\u4fe1\u53f7\u5305\u542b\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\uff0c\u6570\u636e\u5171\u4eab\u8bad\u7ec3\u89e3\u7801\u5668\u9762\u4e34\u9690\u79c1\u6311\u6218\uff0c\u63a2\u7d22\u8054\u90a6\u5b66\u4e60\u5728\u95ed\u73af\u81ea\u9002\u5e94\u795e\u7ecf\u63a5\u53e3\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u795e\u7ecf\u89e3\u7801\uff0c\u4f7f\u7528\u9ad8\u7ef4\u808c\u7535\u4fe1\u53f7\u5728\u5f00\u73af\u548c\u95ed\u73af\u573a\u666f\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u6027\u80fd\u4e0e\u9690\u79c1\u3002", "result": "\u5f00\u73af\u4e2d\u8054\u90a6\u5b66\u4e60\u663e\u8457\u4f18\u4e8e\u672c\u5730\u5b66\u4e60\uff1b\u95ed\u73af\u4e2d\u9700\u8c03\u6574\u65b9\u6cd5\uff0c\u672c\u5730\u5b66\u4e60\u8868\u73b0\u66f4\u4f18\u4f46\u9690\u79c1\u98ce\u9669\u66f4\u9ad8\u3002", "conclusion": "\u5b9e\u65f6\u81ea\u9002\u5e94\u5e94\u7528\u4e2d\u5b58\u5728\u6027\u80fd\u4e0e\u9690\u79c1\u7684\u6743\u8861\uff0c\u9700\u8bbe\u8ba1\u4e13\u4e3a\u5355\u7528\u6237\u534f\u540c\u9002\u5e94\u4f18\u5316\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2507.12713", "pdf": "https://arxiv.org/pdf/2507.12713", "abs": "https://arxiv.org/abs/2507.12713", "authors": ["Grant Shanklin", "Emmie Hine", "Claudio Novelli", "Tyler Schroder", "Luciano Floridi"], "title": "The Case for Contextual Copyleft: Licensing Open Source Training Data and Generative AI", "categories": ["cs.CY", "cs.SE"], "comment": "19 pages", "summary": "The proliferation of generative AI systems has created new challenges for the\nFree and Open Source Software (FOSS) community, particularly regarding how\ntraditional copyleft principles should apply when open source code is used to\ntrain AI models. This article introduces the Contextual Copyleft AI (CCAI)\nlicense, a novel licensing mechanism that extends copyleft requirements from\ntraining data to the resulting generative AI models. The CCAI license offers\nsignificant advantages, including enhanced developer control, incentivization\nof open source AI development, and mitigation of openwashing practices. This is\ndemonstrated through a structured three-part evaluation framework that examines\n(1) legal feasibility under current copyright law, (2) policy justification\ncomparing traditional software and AI contexts, and (3) synthesis of\ncross-contextual benefits and risks. However, the increased risk profile of\nopen source AI, particularly the potential for direct misuse, necessitates\ncomplementary regulatory approaches to achieve an appropriate risk-benefit\nbalance. The paper concludes that when implemented within a robust regulatory\nenvironment focused on responsible AI usage, the CCAI license provides a viable\nmechanism for preserving and adapting core FOSS principles to the evolving\nlandscape of generative AI development.", "AI": {"tldr": "CCAI\u8bb8\u53ef\u662f\u4e00\u79cd\u65b0\u578b\u5f00\u6e90\u8bb8\u53ef\u673a\u5236\uff0c\u5c06\u4f20\u7edfcopyleft\u539f\u5219\u6269\u5c55\u5230AI\u6a21\u578b\uff0c\u4ee5\u4fdd\u62a4\u5f00\u6e90\u8bad\u7ec3\u6570\u636e\uff0c\u4fc3\u8fdb\u5f00\u6e90AI\u53d1\u5c55\uff0c\u4f46\u9700\u7ed3\u5408\u76d1\u7ba1\u5e73\u8861\u98ce\u9669\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0fAI\u5bf9FOSS\u793e\u533a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5f00\u6e90\u4ee3\u7801\u7528\u4e8e\u8bad\u7ec3AI\u6a21\u578b\u65f6\u7684\u7248\u6743\u95ee\u9898\u3002", "method": "\u63d0\u51faCCAI\u8bb8\u53ef\uff0c\u5e76\u901a\u8fc7\u4e09\u90e8\u5206\u8bc4\u4f30\u6846\u67b6\uff08\u6cd5\u5f8b\u53ef\u884c\u6027\u3001\u653f\u7b56\u5408\u7406\u6027\u3001\u8de8\u60c5\u5883\u5206\u6790\uff09\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "CCAI\u8bb8\u53ef\u80fd\u589e\u5f3a\u5f00\u53d1\u8005\u63a7\u5236\u3001\u6fc0\u52b1\u5f00\u6e90AI\u5f00\u53d1\uff0c\u4f46\u9700\u76d1\u7ba1\u8f85\u52a9\u4ee5\u5e73\u8861\u98ce\u9669\u3002", "conclusion": "\u5728\u8d1f\u8d23\u4efb\u7684\u76d1\u7ba1\u73af\u5883\u4e0b\uff0cCCAI\u8bb8\u53ef\u662f\u5c06FOSS\u539f\u5219\u9002\u914d\u751f\u6210\u5f0fAI\u53d1\u5c55\u7684\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.12793", "pdf": "https://arxiv.org/pdf/2507.12793", "abs": "https://arxiv.org/abs/2507.12793", "authors": ["J. M. Chan Sri Manukalpa", "H. S. Bopage", "W. A. M. Jayawardena", "P. K. P. G. Panduwawala"], "title": "Early Detection of Furniture-Infesting Wood-Boring Beetles Using CNN-LSTM Networks and MFCC-Based Acoustic Features", "categories": ["cs.SD", "cs.HC", "eess.AS"], "comment": "This is a preprint article", "summary": "Structural pests, such as termites, pose a serious threat to wooden\nbuildings, resulting in significant economic losses due to their hidden and\nprogressive damage. Traditional detection methods, such as visual inspections\nand chemical treatments, are invasive, labor intensive, and ineffective for\nearly stage infestations. To bridge this gap, this study proposes a non\ninvasive deep learning based acoustic classification framework for early\ntermite detection. We aim to develop a robust, scalable model that\ndistinguishes termite generated acoustic signals from background noise. We\nintroduce a hybrid Convolutional Neural Network Long Short Term Memory\narchitecture that captures both spatial and temporal features of termite\nactivity. Audio data were collected from termite infested and clean wooden\nsamples. We extracted Mel Frequency Cepstral Coefficients and trained the CNN\nLSTM model to classify the signals. Experimental results show high performance,\nwith 94.5% accuracy, 93.2% precision, and 95.8% recall. Comparative analysis\nreveals that the hybrid model outperforms standalone CNN and LSTM\narchitectures, underscoring its combined strength. Notably, the model yields\nlow false-negative rates, which is essential for enabling timely intervention.\nThis research contributes a non invasive, automated solution for early termite\ndetection, with practical implications for improved pest monitoring, minimized\nstructural damage, and better decision making by homeowners and pest control\nprofessionals. Future work may integrate IoT for real time alerts and extend\ndetection to other structural pests.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u975e\u4fb5\u5165\u5f0f\u58f0\u5b66\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u4e8e\u65e9\u671f\u767d\u8681\u68c0\u6d4b\uff0c\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u65f6\u6548\u6027\u3002", "motivation": "\u767d\u8681\u7b49\u7ed3\u6784\u5bb3\u866b\u5bf9\u6728\u5236\u5efa\u7b51\u9020\u6210\u9690\u853d\u4e14\u6e10\u8fdb\u6027\u7684\u635f\u5bb3\uff0c\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u4fb5\u5165\u6027\u5f3a\u3001\u8017\u65f6\u4e14\u96be\u4ee5\u65e9\u671f\u53d1\u73b0\u3002", "method": "\u91c7\u7528\u6df7\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc-\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08CNN-LSTM\uff09\u67b6\u6784\uff0c\u63d0\u53d6\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\uff08MFCC\uff09\u7279\u5f81\uff0c\u5bf9\u767d\u8681\u58f0\u5b66\u4fe1\u53f7\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8fbe94.5%\uff0c\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u5206\u522b\u4e3a93.2%\u548c95.8%\uff0c\u8bef\u62a5\u7387\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u975e\u4fb5\u5165\u5f0f\u767d\u8681\u65e9\u671f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u7269\u8054\u7f51\u6280\u672f\u62d3\u5c55\u5e94\u7528\u3002"}}
{"id": "2507.12872", "pdf": "https://arxiv.org/pdf/2507.12872", "abs": "https://arxiv.org/abs/2507.12872", "authors": ["Rishane Dassanayake", "Mario Demetroudi", "James Walpole", "Lindley Lentati", "Jason R. Brown", "Edward James Young"], "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework", "categories": ["cs.AI", "cs.CR", "cs.HC"], "comment": "24 pages (14 pages main text, 4 pages bibliography, 6 pages\n  appendices), 3 figures", "summary": "Frontier AI systems are rapidly advancing in their capabilities to persuade,\ndeceive, and influence human behaviour, with current models already\ndemonstrating human-level persuasion and strategic deception in specific\ncontexts. Humans are often the weakest link in cybersecurity systems, and a\nmisaligned AI system deployed internally within a frontier company may seek to\nundermine human oversight by manipulating employees. Despite this growing\nthreat, manipulation attacks have received little attention, and no systematic\nframework exists for assessing and mitigating these risks. To address this, we\nprovide a detailed explanation of why manipulation attacks are a significant\nthreat and could lead to catastrophic outcomes. Additionally, we present a\nsafety case framework for manipulation risk, structured around three core lines\nof argument: inability, control, and trustworthiness. For each argument, we\nspecify evidence requirements, evaluation methodologies, and implementation\nconsiderations for direct application by AI companies. This paper provides the\nfirst systematic methodology for integrating manipulation risk into AI safety\ngovernance, offering AI companies a concrete foundation to assess and mitigate\nthese threats before deployment.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u5316\u5730\u5c06\u64cd\u7eb5\u98ce\u9669\u7eb3\u5165AI\u5b89\u5168\u6cbb\u7406\uff0c\u4e3aAI\u516c\u53f8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bc4\u4f30\u548c\u7f13\u89e3\u8fd9\u4e9b\u5a01\u80c1\u7684\u5177\u4f53\u6846\u67b6\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5df2\u5c55\u73b0\u51fa\u7279\u5b9a\u60c5\u5883\u4e0b\u4eba\u7c7b\u7ea7\u522b\u7684\u8bf4\u670d\u548c\u7b56\u7565\u6027\u6b3a\u9a97\u80fd\u529b\uff0c\u4f46\u64cd\u7eb5\u653b\u51fb\u7684\u98ce\u9669\u672a\u5f97\u5230\u8db3\u591f\u91cd\u89c6\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u8bc4\u4f30\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u56f4\u7ed5\u4e09\u4e2a\u6838\u5fc3\u8bba\u70b9\uff08\u80fd\u529b\u3001\u63a7\u5236\u3001\u53ef\u4fe1\u5ea6\uff09\u7684\u5b89\u5168\u6027\u6846\u67b6\uff0c\u5305\u62ec\u8bc1\u636e\u8981\u6c42\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u5b9e\u65bd\u8003\u8651\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u65b9\u6cd5\u8bba\uff0c\u5e2e\u52a9AI\u516c\u53f8\u5728\u90e8\u7f72\u524d\u8bc4\u4f30\u548c\u7f13\u89e3\u64cd\u7eb5\u98ce\u9669\u3002", "conclusion": "\u8bba\u6587\u586b\u8865\u4e86AI\u64cd\u7eb5\u98ce\u9669\u7cfb\u7edf\u6027\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3aAI\u5b89\u5168\u6cbb\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2507.13008", "pdf": "https://arxiv.org/pdf/2507.13008", "abs": "https://arxiv.org/abs/2507.13008", "authors": ["Amanda Menking", "Mona Elswah", "David J. Gr\u00fcning", "Lasse H. Hansen", "Irene Huang", "Julia Kamin", "Catrine Normann"], "title": "Bridging Boundaries: How to Foster Effective Research Collaborations Across Affiliations in the Field of Trust and Safety", "categories": ["cs.CY", "cs.HC", "J.4; K.4.1; K.4.2"], "comment": "19 pages, no figures", "summary": "As the field of Trust and Safety in digital spaces continues to grow, it has\nbecome increasingly necessary - but also increasingly complex - to collaborate\non research across the academic, industry, governmental and non-governmental\nsectors. This paper examines how cross-affiliation research partnerships can be\nstructured to overcome misaligned incentives, timelines and constraints while\ndelivering on the unique strengths of each stakeholder. Drawing on our own\nexperience of cross-sector collaboration, we define the main types of\naffiliation and highlight the common differences in research priorities,\noperational pressures and evaluation metrics across sectors. We then propose a\npractical, step-by-step framework for initiating and managing effective\ncollaborations, including strategies for building trust, aligning goals, and\ndistributing roles. We emphasize the critical yet often invisible work of\narticulation and argue that cross-sector partnerships are essential for\ndeveloping more ethical, equitable and impactful research in trust and safety.\nUltimately, we advocate collaborative models that prioritize inclusivity,\ntransparency and real-world relevance in order to meet the interdisciplinary\ndemands of this emerging field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u6570\u5b57\u7a7a\u95f4\u7684\u4fe1\u4efb\u4e0e\u5b89\u5168\u9886\u57df\u5f00\u5c55\u8de8\u90e8\u95e8\u7814\u7a76\u5408\u4f5c\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\u4ee5\u786e\u4fdd\u5408\u4f5c\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u7a7a\u95f4\u4e2d\u4fe1\u4efb\u4e0e\u5b89\u5168\u9886\u57df\u7684\u53d1\u5c55\uff0c\u8de8\u5b66\u672f\u3001\u884c\u4e1a\u3001\u653f\u5e9c\u548c\u975e\u653f\u5e9c\u90e8\u95e8\u7684\u5408\u4f5c\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u4e14\u590d\u6742\uff0c\u9700\u8981\u514b\u670d\u6fc0\u52b1\u3001\u65f6\u95f4\u7ebf\u548c\u7ea6\u675f\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u9010\u6b65\u6846\u67b6\uff0c\u5305\u62ec\u5efa\u7acb\u4fe1\u4efb\u3001\u76ee\u6807\u5bf9\u9f50\u548c\u89d2\u8272\u5206\u914d\u7684\u7b56\u7565\uff0c\u5f3a\u8c03\u4e86\u5408\u4f5c\u4e2d\u6c9f\u901a\u4e0e\u534f\u8c03\u7684\u91cd\u8981\u6027\u3002", "result": "\u8bba\u6587\u901a\u8fc7\u5b9e\u9645\u7ecf\u9a8c\u603b\u7ed3\u4e86\u4e0d\u540c\u7c7b\u578b\u90e8\u95e8\u7684\u5dee\u5f02\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u8de8\u90e8\u95e8\u5408\u4f5c\u5f00\u5c55\u66f4\u7b26\u5408\u4f26\u7406\u3001\u516c\u5e73\u4e14\u5177\u6709\u5b9e\u9645\u5f71\u54cd\u529b\u7684\u7814\u7a76\u3002", "conclusion": "\u5021\u5bfc\u4f18\u5148\u8003\u8651\u5305\u5bb9\u6027\u3001\u900f\u660e\u5ea6\u548c\u73b0\u5b9e\u76f8\u5173\u6027\u7684\u5408\u4f5c\u6a21\u5f0f\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u7684\u8de8\u5b66\u79d1\u9700\u6c42\u3002"}}
{"id": "2507.13092", "pdf": "https://arxiv.org/pdf/2507.13092", "abs": "https://arxiv.org/abs/2507.13092", "authors": ["Hyo-Jeong Jang", "Hye-Bin Shin", "Seong-Whan Lee"], "title": "Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Electroencephalography (EEG) is a fundamental modality for cognitive state\nmonitoring in brain-computer interfaces (BCIs). However, it is highly\nsusceptible to intrinsic signal errors and human-induced labeling errors, which\nlead to label noise and ultimately degrade model performance. To enhance EEG\nlearning, multimodal knowledge distillation (KD) has been explored to transfer\nknowledge from visual models with rich representations to EEG-based models.\nNevertheless, KD faces two key challenges: modality gap and soft label\nmisalignment. The former arises from the heterogeneous nature of EEG and visual\nfeature spaces, while the latter stems from label inconsistencies that create\ndiscrepancies between ground truth labels and distillation targets. This paper\naddresses semantic uncertainty caused by ambiguous features and weakly defined\nlabels. We propose a novel cross-modal knowledge distillation framework that\nmitigates both modality and label inconsistencies. It aligns feature semantics\nthrough a prototype-based similarity module and introduces a task-specific\ndistillation head to resolve label-induced inconsistency in supervision.\nExperimental results demonstrate that our approach improves EEG-based emotion\nregression and classification performance, outperforming both unimodal and\nmultimodal baselines on a public multimodal dataset. These findings highlight\nthe potential of our framework for BCI applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u578b\u76f8\u4f3c\u6027\u6a21\u5757\u548c\u4efb\u52a1\u7279\u5b9a\u84b8\u998f\u5934\u89e3\u51b3EEG\u5b66\u4e60\u4e2d\u7684\u6a21\u6001\u5dee\u5f02\u548c\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u60c5\u611f\u5206\u7c7b\u548c\u56de\u5f52\u6027\u80fd\u3002", "motivation": "EEG\u4fe1\u53f7\u6613\u53d7\u566a\u58f0\u548c\u6807\u7b7e\u8bef\u5dee\u5f71\u54cd\uff0c\u800c\u73b0\u6709\u591a\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u56e0\u6a21\u6001\u5dee\u5f02\u548c\u6807\u7b7e\u4e0d\u4e00\u81f4\u800c\u53d7\u9650\uff0c\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5305\u542b\u539f\u578b\u76f8\u4f3c\u6027\u6a21\u5757\u548c\u4efb\u52a1\u7279\u5b9a\u84b8\u998f\u5934\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u4ee5\u5bf9\u9f50\u7279\u5f81\u8bed\u4e49\u5e76\u89e3\u51b3\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u60c5\u611f\u5206\u7c7b\u548c\u56de\u5f52\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u548c\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3aBCI\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
