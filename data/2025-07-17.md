<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.HC](#cs.HC) [Total: 19]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Decision Models for Selecting Architecture Patterns and Strategies in Quantum Software Systems](https://arxiv.org/abs/2507.11671)
*Mst Shamima Aktar,Peng Liang,Muhammad Waseem,Amjed Tahir,Mojtaba Shahin,Muhammad Azeem Akbar,Arif Ali Khan,Aakash Ahmad,Musengamana Jean de Dieu,Ruiyin Li*

Main category: cs.SE

TL;DR: 研究提出了量子软件系统的决策模型，帮助开发者选择合适的设计模式和策略，以应对架构设计中的挑战。


<details>
  <summary>Details</summary>
Motivation: 量子软件开发者缺乏选择和实施合适模式和策略的指南，亟需解决这一问题。

Method: 通过数据挖掘和文献综述收集模式和策略，构建决策模型，并访谈16位从业者进行评估。

Result: 决策模型在熟悉度、易理解性、完整性和实用性方面表现良好。

Conclusion: 决策模型能有效辅助量子软件系统的架构设计，数据公开供社区使用。

Abstract: Quantum software represents disruptive technologies in terms of
quantum-specific software systems, services, and applications - leverage the
principles of quantum mechanics via programmable quantum bits (Qubits) that
manipulate quantum gates (QuGates) - to achieve quantum supremacy in computing.
Quantum software architecture enables quantum software developers to abstract
away implementation-specific details (i.e., mapping of Qubits and QuGates to
high-level architectural components and connectors). Architectural patterns and
strategies can provide reusable knowledge and best practices to engineer
quantum software systems effectively and efficiently. However, quantum software
practitioners face significant challenges in selecting and implementing
appropriate patterns and strategies due to the complexity of quantum software
systems and the lack of guidelines. To address these challenges, this study
proposes decision models for selecting patterns and strategies in six critical
design areas in quantum software systems: Communication, Decomposition, Data
Processing, Fault Tolerance, Integration and Optimization, and Algorithm
Implementation. These decision models are constructed based on data collected
from both a mining study (i.e., GitHub and Stack Exchange) and a Systematic
Literature Review, which were used to identify relevant patterns and strategies
with their involved Quality Attributes (QAs). We then conducted semi-structured
interviews with 16 quantum software practitioners to evaluate the familiarity,
understandability, completeness, and usefulness of the proposed decision
models. The results show that the proposed decision models can aid
practitioners in selecting suitable patterns and strategies to address the
challenges related to the architecture design of quantum software systems. The
dataset is available at [6], allowing the community to reproduce and build upon
our findings.

</details>


### [2] [MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization](https://arxiv.org/abs/2507.11687)
*Atharva Naik,Lawanya Baghel,Dhakshin Govindarajan,Darsh Agrawal,Daniel Fried,Carolyn Rose*

Main category: cs.SE

TL;DR: MetaLint是一种新的框架，通过指令调优和合成数据来提升代码质量分析能力，能够适应新的代码模式。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成方面表现良好，但在代码质量分析上因静态训练数据限制而表现不佳，需要一种能适应动态最佳实践的解决方案。

Method: MetaLint采用指令调优的框架，利用合成linter生成的数据，支持从易到难的泛化，无需重新训练即可适应新代码模式。

Result: MetaLint在未见的PEP习惯用法上表现出色，F分数达70.37%，定位能力达26.73%，与更大模型竞争。

Conclusion: MetaLint展示了其在未来代码质量分析中的潜力，尤其是在适应性和泛化能力方面。

Abstract: Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

</details>


### [3] [REST in Pieces: RESTful Design Rule Violations in Student-Built Web Apps](https://arxiv.org/abs/2507.11689)
*Sergio Di Meglio,Valeria Pontillo,Luigi Libero Lucio Starace*

Main category: cs.SE

TL;DR: 研究发现，学生在Web开发课程中开发的REST API常违反基本设计规则，建议加强API设计教学并使用自动化工具改进代码质量。


<details>
  <summary>Details</summary>
Motivation: 计算机科学本科课程中软件质量常被忽视，导致毕业生难以满足行业需求，本研究旨在评估学生代码质量并提出改进建议。

Method: 通过自动化静态分析工具，对40个本科三年级学生开发的全栈Web应用中的REST API设计规则遵守情况进行评估。

Result: 发现学生代码中普遍存在违反REST API设计规则的行为，如端点路径缺失连字符（98%）、复数形式错误（88%）和HTTP方法误用（83%）。

Conclusion: 研究强调需要加强API设计教学，并支持引入自动化工具来提高学生项目的代码质量。

Abstract: In Computer Science Bachelor's programs, software quality is often
underemphasized due to limited time and a focus on foundational skills, leaving
many students unprepared for industry expectations. To better understand the
typical quality of student code and inform both education and hiring practices,
we analyze 40 full-stack web applications developed in a third-year Web
Technologies course. Using an automated static analysis pipeline, we assess
adherence to REST API design rules. Results reveal frequent violations of
foundational conventions, such as missing hyphens in endpoint paths (98%),
incorrect pluralization (88%), and misuse of HTTP methods (83%). These findings
highlight the need for more focused instruction on API design and support the
adoption of automated tools to improve code quality in student projects.

</details>


### [4] [Extremal Testing for Network Software using LLMs](https://arxiv.org/abs/2507.11898)
*Rathin Singha,Harry Qian,Srinath Saikrishnan,Tracy Zhao,Ryan Beckett,Siva Kesava Reddy Kakarla,George Varghese*

Main category: cs.SE

TL;DR: 论文提出了一种基于LLM的自动化极值测试方法，用于网络软件测试，通过生成违反输入约束的测试用例发现新漏洞。


<details>
  <summary>Details</summary>
Motivation: 物理学家常手动测试极端情况以验证理论，但网络软件的极值测试仍需自动化。

Method: 分两步：1) 用LLM生成输入约束（如DNS名称长度限制）；2) 用LLM生成违反约束的测试用例。

Result: 在HTTP、BGP和DNS实现中发现新漏洞，且该方法可扩展至集中式网络软件（如最短路径算法）。

Conclusion: LLM生成的极值测试超越了传统边界值分析，建议通过智能代理进一步自动化。

Abstract: Physicists often manually consider extreme cases when testing a theory. In
this paper, we show how to automate extremal testing of network software using
LLMs in two steps: first, ask the LLM to generate input constraints (e.g., DNS
name length limits); then ask the LLM to generate tests that violate the
constraints. We demonstrate how easy this process is by generating extremal
tests for HTTP, BGP and DNS implementations, each of which uncovered new bugs.
We show how this methodology extends to centralized network software such as
shortest path algorithms, and how LLMs can generate filtering code to reject
extremal input. We propose using agentic AI to further automate extremal
testing. LLM-generated extremal testing goes beyond an old technique in
software testing called Boundary Value Analysis.

</details>


### [5] [A Task Taxonomy for Conformance Checking](https://arxiv.org/abs/2507.11976)
*Jana-Rebecca Rehse,Michael Grohs,Finn Klessascheck,Lisa-Marie Klein,Tatiana von Landesberger,Luise Pufahl*

Main category: cs.SE

TL;DR: 论文提出了一个任务分类法，用于系统化一致性检查中的可视化目的，支持研究人员评估和改进可视化工具的有效性。


<details>
  <summary>Details</summary>
Motivation: 一致性检查的可视化工具缺乏系统性分类，导致其分析目的不明确，难以评估其有用性。

Method: 提出一个任务分类法，通过目标、手段、约束类型、数据特征、数据目标和数据基数等维度对一致性检查任务进行分类。

Result: 通过结合过程挖掘和可视分析的概念，为研究人员提供了一种更清晰的框架，以理解和评估一致性检查的可视化工具。

Conclusion: 任务分类法有助于明确一致性检查可视化的目的，促进跨学科合作，提升工具的实际应用效果。

Abstract: Conformance checking is a sub-discipline of process mining, which compares
observed process traces with a process model to analyze whether the process
execution conforms with or deviates from the process design. Organizations can
leverage this analysis, for example to check whether their processes comply
with internal or external regulations or to identify potential improvements.
Gaining these insights requires suitable visualizations, which make complex
results accessible and actionable. So far, however, the development of
conformance checking visualizations has largely been left to tool vendors. As a
result, current tools offer a wide variety of visual representations for
conformance checking, but the analytical purposes they serve often remain
unclear. However, without a systematic understanding of these purposes, it is
difficult to evaluate the visualizations' usefulness. Such an evaluation hence
requires a deeper understanding of conformance checking as an analysis domain.
To this end, we propose a task taxonomy, which categorizes the tasks that can
occur when conducting conformance checking analyses. This taxonomy supports
researchers in determining the purpose of visualizations, specifying relevant
conformance checking tasks in terms of their goal, means, constraint type, data
characteristics, data target, and data cardinality. Combining concepts from
process mining and visual analytics, we address researchers from both
disciplines to enable and support closer collaborations.

</details>


### [6] [LLAMA: Multi-Feedback Smart Contract Fuzzing Framework with LLM-Guided Seed Generation](https://arxiv.org/abs/2507.12084)
*Keke Gai,Haochen Liang,Jing Yu,Liehuang Zhu,Dusit Niyato*

Main category: cs.SE

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的多反馈智能合约模糊测试框架（LLAMA），通过结合LLMs、进化变异策略和混合测试技术，显著提高了测试覆盖率和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 智能合约模糊测试中，变异调度对测试效果至关重要，但现有研究多关注种子调度和生成，变异调度的研究较少。LLAMA旨在填补这一空白。

Method: LLAMA采用分层提示策略生成语义有效的初始种子，结合多反馈优化机制动态调整种子生成、选择和变异调度，并通过进化模糊测试引擎结合符号执行提升测试深度。

Result: 实验表明，LLAMA在覆盖率和漏洞检测方面优于现有方法，实现了91%的指令覆盖率和90%的分支覆盖率，检测出148个已知漏洞中的132个。

Conclusion: LLAMA展示了其在智能合约安全测试中的高效性、适应性和实用性，为智能合约的安全性提供了有力支持。

Abstract: Smart contracts play a pivotal role in blockchain ecosystems, and fuzzing
remains an important approach to securing smart contracts. Even though mutation
scheduling is a key factor influencing fuzzing effectiveness, existing fuzzers
have primarily explored seed scheduling and generation, while mutation
scheduling has been rarely addressed by prior work. In this work, we propose a
Large Language Models (LLMs)-based Multi-feedback Smart Contract Fuzzing
framework (LLAMA) that integrates LLMs, evolutionary mutation strategies, and
hybrid testing techniques. Key components of the proposed LLAMA include: (i) a
hierarchical prompting strategy that guides LLMs to generate semantically valid
initial seeds, coupled with a lightweight pre-fuzzing phase to select
high-potential inputs; (ii) a multi-feedback optimization mechanism that
simultaneously improves seed generation, seed selection, and mutation
scheduling by leveraging runtime coverage and dependency feedback; and (iii) an
evolutionary fuzzing engine that dynamically adjusts mutation operator
probabilities based on effectiveness, while incorporating symbolic execution to
escape stagnation and uncover deeper vulnerabilities. Our experiments
demonstrate that LLAMA outperforms state-of-the-art fuzzers in both coverage
and vulnerability detection. Specifically, it achieves 91% instruction coverage
and 90% branch coverage, while detecting 132 out of 148 known vulnerabilities
across diverse categories. These results highlight LLAMA's effectiveness,
adaptability, and practicality in real-world smart contract security testing
scenarios.

</details>


### [7] [From Static to Intelligent: Evolving SaaS Pricing with LLMs](https://arxiv.org/abs/2507.12104)
*Francisco Javier Cavero,Juan C. Alonso,Antonio Ruiz-Cortés*

Main category: cs.SE

TL;DR: 论文提出了一种基于LLM的智能定价（iPricing）方法，用于自动化SaaS定价管理，解决了传统手动方法的低效和错误问题。


<details>
  <summary>Details</summary>
Motivation: SaaS市场的快速增长导致定价管理复杂化，目前缺乏自动化工具，亟需智能解决方案。

Method: 采用LLM驱动的AI4Pricing2Yaml工具，结合网页爬取和智能提取技术，将静态HTML定价转换为动态可读的iPricing模型。

Result: 在30个商业SaaS和150多个智能定价的验证中，系统有效提取了关键要素，但仍需解决幻觉、复杂结构和动态内容问题。

Conclusion: 智能定价自动化潜力巨大，未来研究将提升提取能力和适应性。

Abstract: The SaaS paradigm has revolutionized software distribution by offering
flexible pricing options to meet diverse customer needs. However, the rapid
expansion of the SaaS market has introduced significant complexity for DevOps
teams, who must manually manage and evolve pricing structures, an approach that
is both time-consuming and prone to errors. The absence of automated tools for
pricing analysis restricts the ability to efficiently evaluate, optimize, and
scale these models. This paper proposes leveraging intelligent pricing
(iPricing), dynamic, machine-readable pricing models, as a solution to these
challenges. Intelligent pricing enables competitive analysis, streamlines
operational decision-making, and supports continuous pricing evolution in
response to market dynamics, leading to improved efficiency and accuracy. We
present an LLM-driven approach that automates the transformation of static HTML
pricing into iPricing, significantly improving efficiency and consistency while
minimizing human error. Our implementation, AI4Pricing2Yaml, features a basic
Information Extractor that uses web scraping and LLMs technologies to extract
essential pricing components, plans, features, usage limits, and add-ons, from
SaaS websites. Validation against a dataset of 30 distinct commercial SaaS,
encompassing over 150 intelligent pricings, demonstrates the system's
effectiveness in extracting the desired elements across all steps. However,
challenges remain in addressing hallucinations, complex structures, and dynamic
content. This work highlights the potential of automating intelligent pricing
transformation to streamline SaaS pricing management, offering implications for
improved consistency and scalability in an increasingly intricate pricing
landscape. Future research will focus on refining extraction capabilities and
enhancing the system's adaptability to a wider range of SaaS websites.

</details>


### [8] [An Online A/B Testing Decision Support System for Web Usability Assessment Based on a Linguistic Decision-making Methodology: Case of Study a Virtual Learning Environment](https://arxiv.org/abs/2507.12118)
*Noe Zermeño,Cristina Zuheros,Lucas Daniel Del Rosso Calache,Francisco Herrera,Rosana Montes*

Main category: cs.SE

TL;DR: 该论文提出了一种基于用户中心方法的网页可用性评估方法论，结合设计思维和语言决策，并在A/B测试中进行了实际应用验证。


<details>
  <summary>Details</summary>
Motivation: 提高用户对界面（如移动应用和网站）的满意度是研究的主要动机，尤其是如何有效评估网页可用性。

Method: 提出"语言决策"方法论，结合角色扮演和多种可用性测试（如系统可用性量表），并通过A/B测试支持系统实现。

Result: 通过实际案例（墨西哥瓜达拉哈拉大学的Moodle平台）验证了该方法的有效性。

Conclusion: 该方法为网页可用性评估提供了新思路，并展示了在实际应用中的潜力。

Abstract: In recent years, attention has increasingly focused on enhancing user
satisfaction with user interfaces, spanning both mobile applications and
websites. One fundamental aspect of human-machine interaction is the concept of
web usability. In order to assess web usability, the A/B testing technique
enables the comparison of data between two designs. Expanding the scope of
tests to include the designs being evaluated, in conjunction with the
involvement of both real and fictional users, presents a challenge for which
few online tools offer support. We propose a methodology for web usability
evaluation based on user-centered approaches such as design thinking and
linguistic decision-making, named Linguistic Decision-Making for Web Usability
Evaluation. This engages people in role-playing scenarios and conducts a number
of usability tests, including the widely recognized System Usability Scale. We
incorporate the methodology into a decision support system based on A/B
testing. We use real users in a case study to assess three Moodle platforms at
the University of Guadalajara, Mexico.

</details>


### [9] [MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks](https://arxiv.org/abs/2507.12284)
*Artem Chervyakov,Alexander Kharitonov,Pavel Zadorozhny,Adamenko Pavel,Rodion Levichev,Dmitrii Vorobev,Dmitrii Salikhov,Aidar Valeev,Alena Pestova,Maria Dziuba,Ilseyar Alimova,Artem Zavgorodnev,Aleksandr Medvedev,Stanislav Moiseev,Elena Bruches,Daniil Grebenkin,Roman Derunets,Vikulov Vladimir,Anton Emelyanov,Dmitrii Babaev,Vladimir V. Ivanov,Valentin Malykh,Alena Fenogenova*

Main category: cs.SE

TL;DR: MERA Code是新基准，专注于评估俄语最新代码生成LLM的代码质量，填补现有专注于自然语言任务的空白。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注自然语言任务，忽略了代码质量和实际生产性能，需要更全面的评估标准。

Method: 提出MERA Code基准，包含11个任务、8种编程语言，提供开源源码、评分系统和平台。

Result: 评估了开源和前沿API模型，揭示其在非英语编程任务中的局限性。

Conclusion: MERA Code发布以指导研究、预测模型发展趋势并标准化评估流程。

Abstract: Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

</details>


### [10] [GitChameleon: Evaluating AI Code Generation Against Python Library Version Incompatibilities](https://arxiv.org/abs/2507.12367)
*Diganta Misra,Nizar Islah,Victor May,Brice Rauby,Zihan Wang,Justine Gehring,Antonio Orvieto,Muawiz Chaudhary,Eilif B. Muller,Irina Rish,Samira Ebrahimi Kahou,Massimo Caccia*

Main category: cs.SE

TL;DR: GitChameleon是一个新的数据集，用于评估AI在特定库版本下的代码生成能力，揭示现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 软件库快速更新导致代码生成面临版本兼容性问题，需要执行性评估。

Method: 构建328个Python代码补全问题，每个问题绑定特定库版本并提供单元测试。

Result: 当前先进模型的基线成功率仅为48-51%，表明任务复杂性高。

Conclusion: GitChameleon为动态库环境下的代码生成研究提供了基准，帮助开发更可靠的AI方法。

Abstract: The rapid evolution of software libraries poses a considerable hurdle for
code generation, necessitating continuous adaptation to frequent version
updates while preserving backward compatibility. While existing code evolution
benchmarks provide valuable insights, they typically lack execution-based
evaluation for generating code compliant with specific library versions. To
address this, we introduce GitChameleon, a novel, meticulously curated dataset
comprising 328 Python code completion problems, each conditioned on specific
library versions and accompanied by executable unit tests. GitChameleon
rigorously evaluates the capacity of contemporary large language models (LLMs),
LLM-powered agents, code assistants, and RAG systems to perform
version-conditioned code generation that demonstrates functional accuracy
through execution. Our extensive evaluations indicate that state-of-the-art
systems encounter significant challenges with this task; enterprise models
achieving baseline success rates in the 48-51\% range, underscoring the
intricacy of the problem. By offering an execution-based benchmark emphasizing
the dynamic nature of code libraries, GitChameleon enables a clearer
understanding of this challenge and helps guide the development of more
adaptable and dependable AI code generation methods. We make the dataset and
evaluation code publicly available at
https://github.com/mrcabbage972/GitChameleonBenchmark.

</details>


### [11] [SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?](https://arxiv.org/abs/2507.12415)
*Xinyi He,Qian Liu,Mingzhe Du,Lin Yan,Zhijie Fan,Yiming Huang,Zejian Yuan,Zejun Ma*

Main category: cs.SE

TL;DR: 该论文介绍了第一个专门用于评估大语言模型（LLMs）在真实仓库环境下代码性能优化的基准测试SWE-Perf，包含140个实例，揭示了现有LLMs与专家级性能优化之间的巨大差距。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在代码生成和修复方面表现优异，但其在代码性能优化方面的能力尚未充分探索，亟需系统化评估方法。

Method: 提出SWE-Perf基准测试，基于140个GitHub性能优化拉取请求的实例，包含代码库、目标函数、性能测试、专家补丁和可执行环境。

Result: 评估显示现有LLMs（如Agentless和OpenHands）与专家级优化性能存在显著差距。

Conclusion: SWE-Perf填补了LLMs在代码性能优化评估的空白，为该领域的研究提供了重要方向。

Abstract: Code performance optimization is paramount in real-world software engineering
and critical for production-level systems. While Large Language Models (LLMs)
have demonstrated impressive capabilities in code generation and bug fixing,
their proficiency in enhancing code performance at the repository level remains
largely unexplored. To address this gap, we introduce SWE-Perf, the first
benchmark specifically designed to systematically evaluate LLMs on code
performance optimization tasks within authentic repository contexts. SWE-Perf
comprises 140 carefully curated instances, each derived from
performance-improving pull requests from popular GitHub repositories. Each
benchmark instance includes the relevant codebase, target functions,
performance-related tests, expert-authored patches, and executable
environments. Through a comprehensive evaluation of representative methods that
span file-level and repo-level approaches (e.g., Agentless and OpenHands), we
reveal a substantial capability gap between existing LLMs and expert-level
optimization performance, highlighting critical research opportunities in this
emerging field.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [12] [Quantum circuits are just a phase](https://arxiv.org/abs/2507.11676)
*Chris Heunen,Louis Lemonnier,Christopher McNally,Alex Rice*

Main category: cs.PL

TL;DR: 该论文介绍了一种新颖且简单的量子编程语言，通过提升编程抽象级别来解决现有量子编程的可扩展性和清晰度问题。


<details>
  <summary>Details</summary>
Motivation: 当前量子编程语言过于底层，缺乏高级抽象和表达能力，阻碍了可扩展性和高层次推理。

Method: 提出了一种基于相位操作和量子"if let"构造的量子编程语言，专注于特征分解、共轭和受控幺正等量子算法设计的基础构建块。

Result: 通过展示该语言的表达能力和通用性，证明了其在Grover搜索算法、哈密顿模拟等重要量子算法中的自然和简洁表达能力，并实现了原型的编译器。

Conclusion: 该语言为量子编程提供了抽象化和结构化的原则性进步，具有实用性和理论基础。

Abstract: Quantum programs today are written at a low level of abstraction - quantum
circuits akin to assembly languages - and even advanced quantum programming
languages essentially function as circuit description languages. This state of
affairs impedes scalability, clarity, and support for higher-level reasoning.
More abstract and expressive quantum programming constructs are needed.
  To this end, we introduce a novel yet simple quantum programming language for
generating unitaries from "just a phase"; we combine a (global) phase operation
that captures phase shifts with a quantum analogue of the "if let" construct
that captures subspace selection via pattern matching. This minimal language
lifts the focus from quantum gates to eigendecomposition, conjugation, and
controlled unitaries; common building blocks in quantum algorithm design.
  We demonstrate several aspects of the expressive power of our language in
several ways. Firstly, we establish that our representation is universal by
deriving a universal quantum gate set. Secondly, we show that important quantum
algorithms can be expressed naturally and concisely, including Grover's search
algorithm, Hamiltonian simulation, Quantum Fourier Transform, Quantum Signal
Processing, and the Quantum Eigenvalue Transformation. Furthermore, we give
clean denotational semantics grounded in categorical quantum mechanics.
Finally, we implement a prototype compiler that efficiently translates terms of
our language to quantum circuits, and prove that it is sound with respect to
these semantics. Collectively, these contributions show that this construct
offers a principled and practical step toward more abstract and structured
quantum programming.

</details>


### [13] [Picat Through the Lens of Advent of Code](https://arxiv.org/abs/2507.11731)
*Neng-Fa Zhou,Cristian Grozea,Håkan Kjellerstrand,Oisín Mac Fhearaí*

Main category: cs.PL

TL;DR: Picat是一种多范式逻辑编程语言，本文用其解决了2024年Advent of Code的多个问题，展示了其在约束求解和动态编程等任务中的高效与简洁。


<details>
  <summary>Details</summary>
Motivation: Picat语言结合了逻辑、函数式、约束和命令式编程的特点，适合解决AoC中的某些特定问题类型，如逆向工程和路径查找。

Method: 利用Picat内置的约束求解、模式匹配、回溯和动态编程功能，实现了对AoC问题的简洁高效解法。

Result: Picat的SAT约束求解和动态编程功能显著简化了问题实现，且效率优于命令式语言。

Conclusion: Picat在多范式编程语言中表现出色，特别适合需要高级逻辑和约束求解的问题。

Abstract: Picat is a logic-based, multi-paradigm programming language that integrates
features from logic, functional, constraint, and imperative programming
paradigms. This paper presents solutions to several problems from the 2024
Advent of Code (AoC). While AoC problems are not designed for any specific
programming language, certain problem types, such as reverse engineering and
path-finding, are particularly well-suited to Picat due to its built-in
constraint solving, pattern matching, backtracking, and dynamic programming
with tabling. This paper demonstrates that Picat's features, especially its
SAT-based constraint solving and tabling, enable concise, declarative, and
highly efficient implementations of problems that would require significantly
more effort in imperative languages.

</details>


### [14] [Towards Relational Contextual Equality Saturation](https://arxiv.org/abs/2507.11897)
*Tyler Hou,Shadaj Laddad,Joseph M. Hellerstein*

Main category: cs.PL

TL;DR: 本文介绍了将上下文相等性饱和扩展到egglog中的关系相等性饱和的正在进行的工作。


<details>
  <summary>Details</summary>
Motivation: 现有工作已将上下文推理引入egg，本文旨在将其扩展到关系相等性饱和，探索其在关系模型中的应用与挑战。

Method: 总结现有上下文相等性饱和方法，提出将其与关系模型结合的技术路线。

Result: 尚未明确具体结果，但已识别出结合上下文推理与关系模型的挑战。

Conclusion: 本文是正在进行的工作，旨在推动上下文相等性饱和在关系模型中的应用。

Abstract: Equality saturation is a powerful technique for program optimization.
Contextual equality saturation extends this to support rewrite rules that are
conditioned on where a term appears in an expression. Existing work has brought
contextual reasoning to egg; in this paper, we share our ongoing work to extend
this to relational equality saturation in egglog. We summarize the existing
approaches to contextual equality saturation, outline its main applications,
and identify key challenges in combining this approach with relational models.

</details>


### [15] [Universal Synthesis of Differentiably Tunable Numerical Abstract Transformers](https://arxiv.org/abs/2507.11827)
*Shaurya Gomber,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.PL

TL;DR: 提出了一种通用的抽象转换器合成算法，解决现有数值抽象解释中手工定制转换器的局限性，支持多域和任务适应性。


<details>
  <summary>Details</summary>
Motivation: 现有数值抽象分析器依赖手工定制的转换器，缺乏通用性，限制了扩展性和精确性。

Method: 开发了通用转换器合成算法，支持多域操作，并结合梯度引导搜索（AGG）优化转换器选择。

Result: USTAD框架在Zones、Octagons和Polyhedra等域中成功生成精确且可调的转换器，性能优于基线。

Conclusion: 通用转换器合成算法和AGG策略显著提升了数值抽象分析的灵活性和精确性。

Abstract: Numerical abstract interpretation is a widely used framework for the static
analysis of numerical programs. However, existing numerical abstract
interpreters rely on hand-crafted, instruction-specific transformers tailored
to each domain, with no general algorithm for handling common operations across
domains. This limits extensibility, prevents precise compositional reasoning
over instruction sequences, and forces all downstream tasks to use the same
fixed transformer regardless of their precision, efficiency, or task-specific
requirements. To address these limitations, we propose a universal transformer
synthesis algorithm that constructs a parametric family of sound abstract
transformers for any given polyhedral numerical domain and a concrete operator
from the class of Quadratic-Bounded Guarded Operators (QGO), which includes
both individual instructions and structured sequences. Each instantiation in
this family is sound by construction, enabling downstream analyses to adapt the
transformer to their particular needs. The space of transformers is
differentiable but complex. To efficiently explore this space of transformers,
we introduce the Adaptive Gradient Guidance (AGG) procedure, a gradient-guided
search strategy that steers the search process based on downstream analysis
objectives and runtime constraints. We implement these ideas in the USTAD
framework and evaluate their effectiveness across three numerical abstract
domains: Zones, Octagons, and Polyhedra. Our results demonstrate that the
universal synthesis algorithm successfully constructs sound families of
transformers across domains, and that USTAD achieves significant, tunable
precision gains over baselines by leveraging compositional reasoning and
efficient gradient-guided traversal of the transformer space.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [Towards a Non-Binary View of IPv6 Adoption](https://arxiv.org/abs/2507.11678)
*Sulyab Thottungal Valapu,John Heidemann*

Main category: cs.NI

TL;DR: 文章分析了IPv6的当前部署状态，提出从客户端、服务器和云提供商的角度来评估IPv6的使用情况，并揭示了IPv6部署中的不均匀性和改进空间。


<details>
  <summary>Details</summary>
Motivation: 随着IPv6部署的增加，需要从更细致的角度（非二元视角）评估IPv6的使用情况，了解其在客户端、服务器和云提供商中的表现。

Method: 通过观察用户流量、分析网站服务以及研究云提供商的支持情况，评估IPv6的实际使用情况。

Result: 发现IPv6流量存在较大波动，仅12.5%的顶级网站完全支持IPv6；云提供商的支持程度与租户采用率相关。

Conclusion: IPv6部署在增长，但仍有改进空间，尤其是服务商的支持和云提供商的易用性方面。

Abstract: Twelve years have passed since World IPv6 Launch Day, but what is the current
state of IPv6 deployment? Prior work has examined IPv6 status as a binary: can
you use IPv6, or not? As deployment increases we must consider a more nuanced,
non-binary perspective on IPv6: how much and often can a user or a service use
IPv6? We consider this question as a client, server, and cloud provider.
Considering the client's perspective, we observe user traffic. We see that the
fraction of IPv6 traffic a user sends varies greatly, both across users and
day-by-day, with a standard deviation of over 15%. We show this variation
occurs for two main reasons. First, IPv6 traffic is primarily human-generated,
thus showing diurnal patterns. Second, some services are IPv6-forward and
others IPv6-laggards, so as users do different things their fraction of IPv6
varies. We look at server-side IPv6 adoption in two ways. First, we expand
analysis of web services to examine how many are only partially IPv6 enabled
due to their reliance on IPv4-only resources. Our findings reveal that only
12.5% of top 100k websites qualify as fully IPv6-ready. Finally, we examine
cloud support for IPv6. Although all clouds and CDNs support IPv6, we find that
tenant deployment rates vary significantly across providers. We find that ease
of enabling IPv6 in the cloud is correlated with tenant IPv6 adoption rates,
and recommend best practices for cloud providers to improve IPv6 adoption. Our
results suggest IPv6 deployment is growing, but many services lag, presenting a
potential for improvement.

</details>


### [17] [On QoE-Aware Traffic Management for Real-time, Interactive Video with Time-variant Spatial Complexity](https://arxiv.org/abs/2507.11798)
*Szilveszter Nádas,Lars Ernström,David Lindero,Jonathan Lynam*

Main category: cs.NI

TL;DR: 分析了实时交互视频的空间复杂性与QoE的关系，并提出基于效用的资源分配方法，优于静态和均等QoE分配。


<details>
  <summary>Details</summary>
Motivation: 研究实时交互视频中空间复杂性与QoE的关系，优化资源分配以提高性能。

Method: 分析不同内容类型的时间变异性，引入效用模型进行资源分配。

Result: 动态QoE感知资源分配显著优于静态分配，效用模型提高了平均QoE并控制最差情况。

Conclusion: 效用为基础的资源分配在实时视频中表现优越，兼顾性能与公平性。

Abstract: We analyzed spatial complexity, defined as the relationship between the
required bitrate and a corresponding picture Quality of Experience (QoE)
metric, for realistic, long, real-time, interactive video clips. Apart from
variation across different content types, e.g., game genres, we discovered
time-variability within a clip from second to second, and explored the
ramifications for traffic management. We introduced utility as an elegant way
to manage resource sharing preferences. Our analysis of resource sharing
methods shows that frequent QoE-aware reallocation has significant performance
advantages compared to static rate allocation, even in case the latter is based
on rich information about long-term average spatial complexity. We have also
shown that utility-based resource allocation has clear advantages over methods
targeting equal QoE allocation, it increases the average QoE, while it still
controls the worst case QoE.

</details>


### [18] [Native-AI Empowered Scalable Architectures and Solutions for Future Non-Terrestrial Networks: An Overview](https://arxiv.org/abs/2507.11935)
*Jikang Deng,Fizza Hassan,Hui Zhou,Saad Al-Ahmadi,Mohamed-Slim Alouini,Daniel B. Da Costa*

Main category: cs.NI

TL;DR: 本文探讨了将开放式无线接入网络（ORAN）与非地面网络（NTN）结合的框架，以解决后者在高空和移动性带来的运维挑战，并提出了详细架构和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络中，NTN和ORAN的结合潜力巨大，但缺乏智能化和可扩展性管理的能力，导致在开发和运维（DevOps）生命周期中面临挑战。

Method: 提出了一个基于ORAN的NTN框架，包括灵活的fronthaul拆分、增强的RAN智能控制器、可扩展的部署架构和多域服务管理。

Result: 提出了详细的技术解决方案，展示了ORAN如何增强NTN的智能化和可扩展性。

Conclusion: ORAN与NTN结合具有广阔前景，未来可进一步探索与其他技术的融合及潜在应用场景。

Abstract: As the path toward 6G networks is being charted, the emerging applications
have motivated evolutions of network architectures to realize the efficient,
reliable, and flexible wireless networks. Among the potential architectures,
the non-terrestrial network (NTN) and open radio access network (ORAN) have
received increasing interest from both academia and industry. Although the
deployment of NTNs ensures coverage, enhances spectral efficiency, and improves
the resilience of wireless networks. The high altitude and mobility of NTN
present new challenges in the development and operations (DevOps) lifecycle,
hindering intelligent and scalable network management due to the lack of native
artificial intelligence (AI) capability. With the advantages of ORAN in
disaggregation, openness, virtualization, and intelligence, several works
propose integrating ORAN principles into the NTN, focusing mainly on ORAN
deployment options based on transparent and regenerative systems. However, a
holistic view of how to effectively combine ORAN and NTN throughout the DevOps
lifecycle is still missing, especially regarding how intelligent ORAN addresses
the scalability challenges in NTN. Motivated by this, in this paper, we first
provide the background knowledge about ORAN and NTN, outline the
state-of-the-art research on ORAN for NTNs, and present the DevOps challenges
that motivate the adoption of ORAN solutions. We then propose the ORAN-based
NTN framework, discussing its features and architectures in detail. These
include the discussion about flexible fronthaul split, RAN intelligent
controllers (RICs) enhancement for distributed learning, scalable deployment
architecture, and multi-domain service management. Finally, the future research
directions, including combinations of the ORAN-based NTN framework and other
enabling technologies and schemes, as well as the candidate use cases, are
highlighted.

</details>


### [19] [FastReChain: Highly Responsive and Low-Overhead Centralized Route Scheduling in Clos Datacenter Networks](https://arxiv.org/abs/2507.12265)
*Zihan Zhu,Dongchao Wu,Zhanbang Zhang,Jian Yang*

Main category: cs.NI

TL;DR: 提出了一种集中式调度算法，适用于光交换机数据中心网络，支持双向Clos拓扑，实现理论最大吞吐量和最小重排次数。


<details>
  <summary>Details</summary>
Motivation: 解决数据中心网络中光交换机因无缓冲和长切换时间导致的动态调度问题。

Method: 采用替换链概念和位集优化技术设计算法。

Result: 算法时间效率高，重排次数少，支持动态调度和多种实际需求。

Conclusion: 该算法是目前唯一支持双向Clos网络的高效动态调度方案。

Abstract: Ever since Clos topologies were used in datacenter networks (DCNs), a
practical centralized scheduling algorithm that supports dynamic scheduling has
been absent. The introduction of optical switches in DCNs as a future-proof
solution exacerbates this problem due to several properties of optical
switches, such as the fact that they are generally bufferless and therefore
rely on centralized scheduling, and that they have long switching times and
therefore require the number of rearrangements to be minimized.
  In this paper, we propose a centralized scheduling algorithm that achieves
theoretical maximum throughput even in one-rate bidirectional Clos networks,
while producing schemes with near-minimal numbers of rearrangements. It is the
only algorithm that directly supports bidirectional Clos networks and has a
time efficiency high enough to support dynamic scheduling to date. For static
minimal rewiring, its running time ranges from a fraction to a few hundredths
of other algorithms, and the number of rearrangements has also been steadily
improved, allowing for more frequent adjustments and less impact on ongoing
communications. In addition, the algorithm is very flexible and can support
various functional requirements in real-world environments. We achieve this
result through the replacement chain concept and bitset optimization.

</details>


### [20] [LLM-Based Config Synthesis requires Disambiguation](https://arxiv.org/abs/2507.12443)
*Rajdeep Mondal,Nikolaj Bjorner,Todd Millstein,Alan Tang,George Varghese*

Main category: cs.NI

TL;DR: 该论文探讨了利用LLMs进行程序合成时用户意图不明确的问题，提出了一个原型系统Clarify，通过新模块Disambiguator帮助解决意图模糊性。


<details>
  <summary>Details</summary>
Motivation: LLMs在合成网络配置（如路由映射和ACLs）时，经常因为用户意图模糊而导致动作优先级无法推断，尤其是在复杂ACLs中存在大量重叠的情况。

Method: 提出原型系统Clarify，结合LLMs和新模块Disambiguator，通过用户交互明确意图，并验证合成后的路由策略。

Result: 在一个小规模合成工作负载中，Clarify成功在消歧后增量合成了路由策略。

Conclusion: Clarify的方法不仅适用于网络配置合成，还可推广到其他领域，当LLMs能正确合成更新意图但整合时可能产生模糊性时。

Abstract: Beyond hallucinations, another problem in program synthesis using LLMs is
ambiguity in user intent. We illustrate the ambiguity problem in a networking
context for LLM-based incremental configuration synthesis of route-maps and
ACLs. These structures frequently overlap in header space, making the relative
priority of actions impossible for the LLM to infer without user interaction.
Measurements in a large cloud identify complex ACLs with 100's of overlaps,
showing ambiguity is a real problem. We propose a prototype system, Clarify,
which uses an LLM augmented with a new module called a Disambiguator that helps
elicit user intent. On a small synthetic workload, Clarify incrementally
synthesizes routing policies after disambiguation and then verifies them. Our
treatment of ambiguities is useful more generally when the intent of updates
can be correctly synthesized by LLMs, but their integration is ambiguous and
can lead to different global behaviors.

</details>


### [21] [CRAFT: Latency and Cost-Aware Genetic-Based Framework for Node Placement in Edge-Fog Environments](https://arxiv.org/abs/2507.12445)
*Soheil Mahdizadeh,Amir Mahdi Rasouli,Mohammad Pourashory,Sadra Galavani,Mohsen Ansari*

Main category: cs.NI

TL;DR: 提出了一种基于遗传算法的边缘和雾节点优化部署策略，以降低物联网系统的延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 解决云计算在物联网中无法满足实时需求的问题，利用边缘和雾计算优化网络性能。

Method: 使用遗传算法优化边缘和雾节点的部署策略。

Result: 模拟结果显示，延迟降低2.77%，成本减少31.15%。

Conclusion: 遗传算法在优化节点部署上有效，显著降低延迟和成本。

Abstract: Reducing latency in the Internet of Things (IoT) is a critical concern. While
cloud computing facilitates communication, it falls short of meeting real-time
requirements reliably. Edge and fog computing have emerged as viable solutions
by positioning computing nodes closer to end users, offering lower latency and
increased processing power. An edge-fog framework comprises various components,
including edge and fog nodes, whose strategic placement is crucial as it
directly impacts latency and system cost. This paper presents an effective and
tunable node placement strategy based on a genetic algorithm to address the
optimization problem of deploying edge and fog nodes. The main objective is to
minimize latency and cost through optimal node placement. Simulation results
demonstrate that the proposed framework achieves up to 2.77% latency and 31.15%
cost reduction.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [22] [Comment on Decidability of Quasi-Dense Modal Logics by Lyon and Ostropolski-Nalewaja](https://arxiv.org/abs/2507.11644)
*Olivier Gasquet*

Main category: cs.LO

TL;DR: 论文《Lyon24》试图解决准稠密模态逻辑的可判定性问题，并给出了EXPSPACE上限，但证明中存在无法修复的重大错误。


<details>
  <summary>Details</summary>
Motivation: 研究准稠密模态逻辑的可判定性问题及其计算复杂性。

Method: 作者尝试通过复杂的证明方法给出结论和上限。

Result: 证明中存在重大错误，导致结论无法成立。

Conclusion: 准稠密模态逻辑的可判定性问题仍需进一步研究。

Abstract: In \cite{Lyon24} the question of the decidability of quasi-dense modal logics
is answered, and an upper bound in EXPSPACE is given. Unfortunately, authors'
intricate proof contains a major flaw that cannot be fixed, leaving the
question wide open. Once identified, this error roughly amounts to assuming
that the union of two consistent sets is consistent, which is of course wrong.

</details>


### [23] [Counting Answer Sets of Disjunctive Answer Set Programs](https://arxiv.org/abs/2507.11655)
*Mohimenul Kabir,Supratik Chakraborty,Kuldeep S Meel*

Main category: cs.LO

TL;DR: SharpASP-SR 是一种基于投影命题模型计数的减约方法，用于计算析取逻辑程序的答案集数量，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 计算答案集数量在概率推理、网络可靠性分析等领域有重要应用，但析取逻辑程序的计数器设计仍具挑战性。

Method: SharpASP-SR 通过减约方法将问题转化为投影命题模型计数，并引入新表征以确保高效且多项式规模的中间表示。

Result: 实验表明 SharpASP-SR 在答案集数量大的实例上显著优于现有计数器，结合枚举技术的混合方法表现最佳。

Conclusion: SharpASP-SR 是析取逻辑程序计数的高效解决方案，结合混合方法可达到最先进水平。

Abstract: Answer Set Programming (ASP) provides a powerful declarative paradigm for
knowledge representation and reasoning. Recently, counting answer sets has
emerged as an important computational problem with applications in
probabilistic reasoning, network reliability analysis, and other domains. This
has motivated significant research into designing efficient ASP counters. While
substantial progress has been made for normal logic programs, the development
of practical counters for disjunctive logic programs remains challenging.
  We present SharpASP-SR, a novel framework for counting answer sets of
disjunctive logic programs based on subtractive reduction to projected
propositional model counting. Our approach introduces an alternative
characterization of answer sets that enables efficient reduction while ensuring
that intermediate representations remain of polynomial size. This allows
SharpASP-SR to leverage recent advances in projected model counting technology.
Through extensive experimental evaluation on diverse benchmarks, we demonstrate
that SharpASP-SR significantly outperforms existing counters on instances with
large answer set counts. Building on these results, we develop a hybrid
counting approach that combines enumeration techniques with SharpASP-SR to
achieve state-of-the-art performance across the full spectrum of disjunctive
programs.

</details>


### [24] [Anthem 2.0: Automated Reasoning for Answer Set Programming](https://arxiv.org/abs/2507.11704)
*Jorge Fandinno,Christoph Glinzer,Zachary Hansen,Jan Heuer,Yuliya Lierler,Vladimir Lifschitz,Torsten Schaub,Tobias Stolzmann*

Main category: cs.LO

TL;DR: Anthem 2.0是一个用于验证基于mini-gringo语言编写的逻辑程序的工具，支持将逻辑程序转换为逻辑公式并通过一阶定理证明器进行验证。


<details>
  <summary>Details</summary>
Motivation: 为逻辑程序提供一种高效的验证工具，确保其符合一阶规范并支持程序间的等价性证明。

Method: 将逻辑程序转换为逻辑公式，并利用一阶定理证明器进行分析和验证。

Result: Anthem 2.0能够验证程序的紧致性、是否符合规范以及程序间的等价性。

Conclusion: Anthem 2.0是一个强大的验证工具，适用于逻辑程序的多种分析和验证需求。

Abstract: Anthem 2.0 is a tool to aid in the verification of logic programs written in
an expressive fragment of Clingo's input language named mini-gringo, which
includes arithmetic operations and simple choice rules but not aggregates. It
can translate logic programs into formula representations in the logic of
here-and-there, and analyze properties of logic programs such as tightness.
Most importantly, Anthem 2.0 can support program verification by invoking
first-order theorem provers to confirm that a program adheres to a first-order
specification, or to establish strong and external equivalence of programs.
This paper serves as an overview of the system's capabilities. We demonstrate
how to use Anthem 2.0 effectively and interpret its results.

</details>


### [25] [Approximation Fixpoint Theory as a Unifying Framework for Fuzzy Logic Programming Semantics (Extended Version)](https://arxiv.org/abs/2507.11961)
*Pascal Kettmann,Jesse Heyninck,Hannes Strass*

Main category: cs.LO

TL;DR: 论文摘要讨论了将经典的稳定模型和良基语义重构为近似不动点理论（AFT）框架内的模糊逻辑编程，扩展了AFT的应用范围并提出了新的语义变体。


<details>
  <summary>Details</summary>
Motivation: 模糊逻辑编程是处理不确定性推理的重要方法，但缺乏统一的框架将其与经典的逻辑编程语义结合。本文旨在通过AFT框架统一模糊逻辑编程的语义。

Method: 论文将经典的稳定模型和良基语义重构为AFT框架，并将其推广到多值逻辑，同时应用AFT的现有成果。

Result: 研究结果包括澄清现有语义关系、将分层概念从经典逻辑推广到模糊逻辑，并提出更精确的语义变体。

Conclusion: AFT框架成功应用于模糊逻辑编程，不仅扩展了其适用范围，还为语义分析和改进提供了新工具。

Abstract: Fuzzy logic programming is an established approach for reasoning under
uncertainty. Several semantics from classical, two-valued logic programming
have been generalized to the case of fuzzy logic programs. In this paper, we
show that two of the most prominent classical semantics, namely the stable
model and the well-founded semantics, can be reconstructed within the general
framework of approximation fixpoint theory (AFT). This not only widens the
scope of AFT from two- to many-valued logics, but allows a wide range of
existing AFT results to be applied to fuzzy logic programming. As first
examples of such applications, we clarify the formal relationship between
existing semantics, generalize the notion of stratification from classical to
fuzzy logic programs, and devise "more precise" variants of the semantics.

</details>


### [26] [SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques](https://arxiv.org/abs/2507.12286)
*Anouk Oudshoorn,Magdalena Ortiz,Mantas Simkus*

Main category: cs.LO

TL;DR: 本文提出了一种基于核心通用模型的SHACL验证语义，用于处理本体论的存在。通过构建Horn-ALCHIQ逻辑的数据可处理模型，采用重写技术将SHACL验证简化为标准验证，并分析了其计算复杂度。


<details>
  <summary>Details</summary>
Motivation: SHACL和OWL在语义和计算上的差距使其结合具有挑战性。研究旨在解决SHACL在本体存在下的验证问题，填补这一语义鸿沟。

Method: 构建Horn-ALCHIQ逻辑的核心通用模型，利用有限表示法开发重写技术，将本体下的SHACL验证转化为标准验证。

Result: 研究显示，简单的本体论会使SHACL验证问题成为EXPTIME完全问题，数据复杂度下为PTIME完全问题。

Conclusion: 提出的方法有效解决了SHACL与本体结合时的验证问题，为实际应用提供了可行的技术路径。

Abstract: SHACL and OWL are two prominent W3C standards for managing RDF data. These
languages share many features, but they have one fundamental difference: OWL,
designed for inferring facts from incomplete data, makes the open-world
assumption, whereas SHACL is a constraint language that treats the data as
complete and must be validated under the closed-world assumption. The
combination of both formalisms is very appealing and has been called for, but
their semantic gap is a major challenge, semantically and computationally. In
this paper, we advocate a semantics for SHACL validation in the presence of
ontologies based on core universal models. We provide a technique for
constructing these models for ontologies in the rich data-tractable description
logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to
develop a rewriting technique that reduces SHACL validation in the presence of
ontologies to standard validation. Finally, we study the complexity of SHACL
validation in the presence of ontologies, and show that even very simple
ontologies make the problem EXPTIME-complete, and PTIME-complete in data
complexity.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [27] [Generative Intelligence Systems in the Flow of Group Emotions](https://arxiv.org/abs/2507.11831)
*Fernando Koch,Jessica Nahulan,Jeremy Fox,Martin Keen*

Main category: cs.HC

TL;DR: 论文提出了一种情绪传染模型，使人工代理能够检测情绪信号、推断群体情绪模式并生成针对性情感响应，从而实现实时的群体情绪调节。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索数字交互环境中，人工代理如何通过模拟情感行为来影响群体情绪动态，从而增强协作、教育和社会环境中的情感计算能力。

Method: 方法包括设计一个情绪传染模型，通过捕捉人类情感交换、推断群体情绪模式，并生成自适应的情感响应来实时调节群体情绪。

Result: 实验结果表明，该系统能够有效感知和引导群体情绪动态，验证了模型在协作和社会环境中的实用性。

Conclusion: 结论指出，该模型将情感计算从个体层面扩展到群体层面，为协作和教育等领域的情绪调节提供了新工具。

Abstract: Emotional cues frequently arise and shape group dynamics in interactive
settings where multiple humans and artificial agents communicate through shared
digital channels. While artificial agents lack intrinsic emotional states, they
can simulate affective behavior using synthetic modalities such as text or
speech. This work introduces a model for orchestrating emotion contagion,
enabling agents to detect emotional signals, infer group mood patterns, and
generate targeted emotional responses. The system captures human emotional
exchanges and uses this insight to produce adaptive, generative responses that
influence group affect in real time. The model supports applications in
collaborative, educational, and social environments by shifting affective
computing from individual-level reactions to coordinated, group-level emotion
modulation. We present the system architecture and provide experimental results
that illustrate its effectiveness in sensing and steering group mood dynamics.

</details>


### [28] [Neuroaesthetics and the Science of Visual Experience](https://arxiv.org/abs/2507.11599)
*Harish Vijayakumar*

Main category: cs.HC

TL;DR: 论文探讨了神经美学如何通过神经机制解释审美体验，揭示美在大脑中的构建方式及其对设计的启示。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解大脑如何感知和响应视觉美学，以及这对设计领域的潜在影响。

Method: 通过分析感知、情感和认知的交互作用，结合神经科学与心理学的理论框架。

Result: 发现表明，有效的设计不仅关乎表面吸引力，还能通过深刻的视觉体验连接人们。

Conclusion: 神经美学为设计提供了科学基础，强调了视觉体验与人类情感和认知的深度联系。

Abstract: Neuroaesthetics is an interdisciplinary field that brings together
neuroscience, psychology, and the arts to explore how the human brain perceives
and responds to visual beauty. This paper examines the neural mechanisms behind
aesthetic experiences, aiming to explain why certain designs or artworks feel
emotionally or cognitively "right." By analyzing the interaction between
perception, emotion, and cognition, neuroaesthetics reveals how beauty is
constructed in the brain and how this understanding can inform fields such as
graphic and interface design. This paper offers a clear and accessible overview
of core neuroaesthetic principles, making the subject approachable to a wide
audience. The findings suggest that impactful design is more than surface-level
appeal: well-crafted visual experiences can engage, support, and connect people
in meaningful ways.

</details>


### [29] [Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps](https://arxiv.org/abs/2507.11903)
*Daocheng Lin,Yifan Wang,Yutong Yang,Xingyu Lan*

Main category: cs.HC

TL;DR: 本文探讨了数据可视化作为说服工具的潜力，强调了修辞构造的重要性，并以章鱼图为例分析了其跨世纪的修辞策略和社会影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究对可视化作为说服工具的修辞机制关注不足，本文旨在填补这一空白，通过分析章鱼图的修辞策略，揭示其说服力的来源。

Method: 采用修辞图式理论，收集并分析了90个从19世纪至今的章鱼图，研究了其视觉隐喻和修辞策略的构建方式。

Result: 发现章鱼图作为一种设计惯例至今仍活跃，且其修辞策略在不同社会文化背景下呈现多样性。

Conclusion: 章鱼图分析揭示了说服性可视化的修辞机制，并引发了对相关伦理问题的讨论。

Abstract: When designed deliberately, data visualizations can become powerful
persuasive tools, influencing viewers' opinions, values, and actions. While
researchers have begun studying this issue (e.g., to evaluate the effects of
persuasive visualization), we argue that a fundamental mechanism of persuasion
resides in rhetorical construction, a perspective inadequately addressed in
current visualization research. To fill this gap, we present a focused analysis
of octopus maps, a visual genre that has maintained persuasive power across
centuries and achieved significant social impact. Employing rhetorical schema
theory, we collected and analyzed 90 octopus maps spanning from the 19th
century to contemporary times. We closely examined how octopus maps implement
their persuasive intents and constructed a design space that reveals how visual
metaphors are strategically constructed and what common rhetorical strategies
are applied to components such as maps, octopus imagery, and text. Through the
above analysis, we also uncover a set of interesting findings. For instance,
contrary to the common perception that octopus maps are primarily a historical
phenomenon, our research shows that they remain a lively design convention in
today's digital age. Additionally, while most octopus maps stem from Western
discourse that views the octopus as an evil symbol, some designs offer
alternative interpretations, highlighting the dynamic nature of rhetoric across
different sociocultural settings. Lastly, drawing from the lessons provided by
octopus maps, we discuss the associated ethical concerns of persuasive
visualization.

</details>


### [30] [DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday Storytelling](https://arxiv.org/abs/2507.11628)
*Jiangnan Xu,Haeseul Cha,Gosu Choi,Gyu-cheol Lee,Yeo-Jin Yoon,Zucheul Lee,Konstantinos Papangelis,Dae Hyun Kim,Juho Kim*

Main category: cs.HC

TL;DR: DiaryPlay是一个AI辅助的交互式小故事创作系统，通过自然语言输入生成核心故事元素，并利用LLM技术实现分支叙事设计，简化了日常讲故事的创作流程。


<details>
  <summary>Details</summary>
Motivation: 交互式小故事（交互式小插图）是一种流行的沉浸式叙事方式，但由于制作复杂，尚未被日常讲故事者广泛使用。DiaryPlay旨在通过AI辅助降低创作门槛，使其更易于使用。

Method: DiaryPlay接受自然语言故事输入，提取核心元素（环境、角色、事件），并利用LLM技术将单线故事自动转化为分支叙事结构，简化多分支设计。

Result: 技术评估（16人）显示DiaryPlay生成的角色活动在可信度上与人工创作相当；用户研究（16人）表明系统能有效支持创作、保持作者意图并提供吸引人的观看体验。

Conclusion: DiaryPlay通过AI辅助简化了交互式小故事的创作，平衡了创作复杂性与叙事灵活性，适合日常讲故事场景使用。

Abstract: An interactive vignette is a popular and immersive visual storytelling
approach that invites viewers to role-play a character and influences the
narrative in an interactive environment. However, it has not been widely used
by everyday storytellers yet due to authoring complexity, which conflicts with
the immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted
authoring system for interactive vignette creation in everyday storytelling. It
takes a natural language story as input and extracts the three core elements of
an interactive vignette (environment, characters, and events), enabling authors
to focus on refining these elements instead of constructing them from scratch.
Then, it automatically transforms the single-branch story input into a
branch-and-bottleneck structure using an LLM-powered narrative planner, which
enables flexible viewer interactions while freeing the author from
multi-branching. A technical evaluation (N=16) shows that DiaryPlay-generated
character activities are on par with human-authored ones regarding
believability. A user study (N=16) shows that DiaryPlay effectively supports
authors in creating interactive vignette elements, maintains authorial intent
while reacting to viewer interactions, and provides engaging viewing
experiences.

</details>


### [31] [CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations](https://arxiv.org/abs/2507.11677)
*Mashrur Rashik,Jean-Daniel Fekete,Narges Mahyar*

Main category: cs.HC

TL;DR: CLAImate是一款基于AI的原型工具，通过个性化对话和本地化可视化改进气候变化的传播效果。


<details>
  <summary>Details</summary>
Motivation: 传统气候报告过于抽象和技术化，难以与公众产生共鸣，因此需要更个性化的传播方式。

Method: 开发了CLAImate，通过AI生成个性化叙事和本地化可视化，并进行了内部验证、专家评估和试点测试。

Result: CLAImate在SNLI准确率和FACTSCORE上表现良好，专家认可其清晰性，多数用户反馈提升了理解和本地相关性。

Conclusion: CLAImate展示了在气候变化传播中的潜力，但还需解决个性化、准确性和可扩展性等设计挑战。

Abstract: Communicating climate change remains challenging, as climate reports, though
rich in data and visualizations, often feel too abstract or technical for the
public. Although personalization can enhance communication, most tools still
lack the narrative and visualization tailoring needed to connect with
individual experiences. We present CLAImate, an AI-enabled prototype that
personalizes conversation narratives and localizes visualizations based on
users' climate knowledge and geographic location. We evaluated CLAImate through
internal verification of factual correctness, a formative study with experts,
and a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70%
FACTSCORE. Visualization experts appreciated its clarity and personalization,
and seven out of ten UK participants reported better understanding and local
relevance of climate risks with CLAImate. We also discuss design challenges in
personalization, accuracy, and scalability, and outline future directions for
integrating visualizations in personalized conversational interfaces.

</details>


### [32] [GIST: Group Interaction Sensing Toolkit for Mixed Reality](https://arxiv.org/abs/2507.11797)
*Diana Romero,Yasra Chandio,Fatima Anwar,Salma Elmalaki*

Main category: cs.HC

TL;DR: 提出了一种新型的多模态交互感知工具包GIST，用于捕捉MR环境中的团队协作行为，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究团队在混合现实环境中的协作行为，现有方法局限性较大。

Method: 开发了GIST工具包，利用MR头显传感器捕捉多模态交互数据，并进行动态行为分析。

Result: 实验表明，GIST能有效识别行为模式和交互网络的变化。

Conclusion: GIST为研究MR环境中的协作行为提供了一种新的可行方法。

Abstract: Understanding how teams coordinate, share work, and negotiate roles in
immersive environments is critical for designing effective mixed-reality (MR)
applications that support real-time collaboration. However, existing methods
either rely on external cameras and offline annotation or focus narrowly on
single modalities, limiting their validity and applicability. To address this,
we present a novel group interaction sensing toolkit (GIST), a deployable
system that passively captures multi-modal interaction data, such as speech,
gaze, and spatial proximity from commodity MR headset's sensors and
automatically derives both overall static interaction networks and dynamic
moment-by-moment behavior patterns. We evaluate GIST with a human subject study
with 48 participants across 12 four-person groups performing an open-ended
image-sorting task in MR. Our analysis shows strong alignment between the
identified behavior modes and shifts in interaction network structure,
confirming that momentary changes in speech, gaze, and proximity data are
observable through the sensor data.

</details>


### [33] ["Mapping What I Feel": Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships](https://arxiv.org/abs/2507.11841)
*Xingyu Lan,Yutong Yang,Yifan Wang*

Main category: cs.HC

TL;DR: 本文探讨了情感地理可视化设计这一新兴领域，通过PPP模型分析，提出了设计分类法和四个高层设计范式，为未来研究提供了具体指导。


<details>
  <summary>Details</summary>
Motivation: 情感可视化设计涉及多学科，但缺乏细粒度分析。本文聚焦情感地理可视化设计，旨在填补这一研究空白并提供领域特定见解。

Method: 利用地理理论中的PPP模型分析情感地理可视化设计案例，归纳出设计分类法和四个高层设计范式。

Result: 提出了情感地理可视化设计的分类法和四个设计范式（如计算型、拟人型），扩展了现有框架并提供了具体设计示例和分析。

Conclusion: 研究为情感地理可视化设计提供了新的理论支持和实践指导，推动了这一创新领域的发展。

Abstract: Affective visualization design is an emerging research direction focused on
communicating and influencing emotion through visualization. However, as
revealed by previous research, this area is highly interdisciplinary and
involves theories and practices from diverse fields and disciplines, thus
awaiting analysis from more fine-grained angles. To address this need, this
work focuses on a pioneering and relatively mature sub-area, affective
geovisualization design, to further the research in this direction and provide
more domain-specific insights. Through an analysis of a curated corpus of
affective geovisualization designs using the Person-Process-Place (PPP) model
from geographic theory, we derived a design taxonomy that characterizes a
variety of methods for eliciting and enhancing emotions through geographic
visualization. We also identified four underlying high-level design paradigms
of affective geovisualization design (e.g., computational, anthropomorphic)
that guide distinct approaches to linking geographic information with human
experience. By extending existing affective visualization design frameworks
with geographic specificity, we provide additional design examples,
domain-specific analyses, and insights to guide future research and practices
in this underexplored yet highly innovative domain.

</details>


### [34] [Interactive Hybrid Rice Breeding with Parametric Dual Projection](https://arxiv.org/abs/2507.11848)
*Changjian Chen,Pengcheng Wang,Fei Lyu,Zhuo Tang,Li Yang,Long Wang,Yong Cai,Feng Yu,Kenli Li*

Main category: cs.HC

TL;DR: 提出了一种交互式杂交水稻育种的视觉分析方法，通过参数化双投影方法提高育种效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 基因组预测模型的准确性有限，育种仍需结合经验，过程耗时；需简化杂交水稻育种过程。

Method: 开发了参数化双投影方法，支持交互式双分析，并设计了基因和杂交可视化工具。

Result: 方法通过案例研究的定量评估和育种者的积极反馈证明了其有效性。

Conclusion: 该方法显著提高了杂交水稻育种的效率和准确性，为育种者提供了实用工具。

Abstract: Hybrid rice breeding crossbreeds different rice lines and cultivates the
resulting hybrids in fields to select those with desirable agronomic traits,
such as higher yields. Recently, genomic selection has emerged as an efficient
way for hybrid rice breeding. It predicts the traits of hybrids based on their
genes, which helps exclude many undesired hybrids, largely reducing the
workload of field cultivation. However, due to the limited accuracy of genomic
prediction models, breeders still need to combine their experience with the
models to identify regulatory genes that control traits and select hybrids,
which remains a time-consuming process. To ease this process, in this paper, we
proposed a visual analysis method to facilitate interactive hybrid rice
breeding. Regulatory gene identification and hybrid selection naturally
ensemble a dual-analysis task. Therefore, we developed a parametric dual
projection method with theoretical guarantees to facilitate interactive dual
analysis. Based on this dual projection method, we further developed a gene
visualization and a hybrid visualization to verify the identified regulatory
genes and hybrids. The effectiveness of our method is demonstrated through the
quantitative evaluation of the parametric dual projection method, identified
regulatory genes and desired hybrids in the case study, and positive feedback
from breeders.

</details>


### [35] [AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding](https://arxiv.org/abs/2507.11911)
*Xiaoqing Chen,Siyang Li,Dongrui Wu*

Main category: cs.HC

TL;DR: 论文提出了一个无需校准的AFPM框架，通过空间对齐和帧-片编码解决了跨数据集EEG解码的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决脑机接口中EEG信号因通道布局不一致、信号分布非稳态和生理先验整合不足导致的跨数据集学习困难。

Method: AFPM框架包含空间对齐（选择相关通道并统一布局）和帧-片编码（将信号转换为统一时空片）。

Result: AFPM在运动想象和事件相关电位任务上分别实现了4.40%和3.58%的性能提升。

Conclusion: AFPM首次实现了无需校准的跨数据集EEG解码，提升了脑机接口的实用性和推广性。

Abstract: Electroencephalogram (EEG) decoding models for brain-computer interfaces
(BCIs) struggle with cross-dataset learning and generalization due to channel
layout inconsistencies, non-stationary signal distributions, and limited
neurophysiological prior integration. To address these issues, we propose a
plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has
two main components: 1) Spatial Alignment, which selects task-relevant channels
based on brain-region priors, aligns EEG distributions across domains, and
remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,
which models multi-dataset signals into unified spatiotemporal patches for EEG
decoding. Compared to 17 state-of-the-art approaches that need dataset-specific
tuning, the proposed calibration-free AFPM achieves performance gains of up to
4.40% on motor imagery and 3.58% on event-related potential tasks. To our
knowledge, this is the first calibration-free cross-dataset EEG decoding
framework, substantially enhancing the practicalness of BCIs in real-world
applications.

</details>


### [36] [d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement](https://arxiv.org/abs/2507.11960)
*Hyein Hong,Sangbong Yoo,SeokHwan Choi,Jisue Kim,Seongbum Seo,Haneol Cho,Chansoo Kim,Yun Jang*

Main category: cs.HC

TL;DR: 论文提出了一种名为d-DQIVAR的视觉分析系统，旨在通过结合数据和流程驱动的方法来提升数据质量，从而优化机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注数据预处理而非真正的数据质量改进（DQI），且数据驱动的方法在优化机器学习模型性能时效果有限。

Method: 系统采用了数据驱动（如插补、异常检测等）和流程驱动（如评估数据质量维度和模型性能）的方法，并通过视觉分析技术进行整合。

Result: 通过案例研究和用户评估，展示了系统如何有效利用专家和领域知识在实际工作流中提升数据质量。

Conclusion: d-DQIVAR系统是提升数据质量和机器学习模型性能的一种有效工具，特别适用于需要结合多领域知识的场景。

Abstract: Approaches to enhancing data quality (DQ) are classified into two main
categories: data- and process-driven. However, prior research has predominantly
utilized batch data preprocessing within the data-driven framework, which often
proves insufficient for optimizing machine learning (ML) model performance and
frequently leads to distortions in data characteristics. Existing studies have
primarily focused on data preprocessing rather than genuine data quality
improvement (DQI). In this paper, we introduce d-DQIVAR, a novel visual
analytics system designed to facilitate DQI strategies aimed at improving ML
model performance. Our system integrates visual analytics techniques that
leverage both data-driven and process-driven approaches. Data-driven techniques
tackle DQ issues such as imputation, outlier detection, deletion, format
standardization, removal of duplicate records, and feature selection.
Process-driven strategies encompass evaluating DQ and DQI procedures by
considering DQ dimensions and ML model performance and applying the
Kolmogorov-Smirnov test. We illustrate how our system empowers users to harness
expert and domain knowledge effectively within a practical workflow through
case studies, evaluations, and user studies.

</details>


### [37] [Dataset-Adaptive Dimensionality Reduction](https://arxiv.org/abs/2507.11984)
*Hyeon Jeon,Jeongin Park,Soohyun Lee,Dae Hyun Kim,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 提出了一种基于结构复杂性指标的降维（DR）优化方法，减少试错计算开销，提升效率。


<details>
  <summary>Details</summary>
Motivation: 降维技术的选择和超参数优化通常需要大量试错，计算开销大，亟需一种高效的方法。

Method: 利用结构复杂性指标量化数据集固有复杂性，预测降维技术的最大可能精度，减少冗余优化。

Result: 验证了指标能准确近似数据集的真实复杂性，显著提升了降维优化效率且不损失精度。

Conclusion: 提出的数据集自适应方法通过结构复杂性指标，有效指导降维优化，减少计算开销。

Abstract: Selecting the appropriate dimensionality reduction (DR) technique and
determining its optimal hyperparameter settings that maximize the accuracy of
the output projections typically involves extensive trial and error, often
resulting in unnecessary computational overhead. To address this challenge, we
propose a dataset-adaptive approach to DR optimization guided by structural
complexity metrics. These metrics quantify the intrinsic complexity of a
dataset, predicting whether higher-dimensional spaces are necessary to
represent it accurately. Since complex datasets are often inaccurately
represented in two-dimensional projections, leveraging these metrics enables us
to predict the maximum achievable accuracy of DR techniques for a given
dataset, eliminating redundant trials in optimizing DR. We introduce the design
and theoretical foundations of these structural complexity metrics. We
quantitatively verify that our metrics effectively approximate the ground truth
complexity of datasets and confirm their suitability for guiding
dataset-adaptive DR workflow. Finally, we empirically show that our
dataset-adaptive workflow significantly enhances the efficiency of DR
optimization without compromising accuracy.

</details>


### [38] [Envisage: Towards Expressive Visual Graph Querying](https://arxiv.org/abs/2507.11999)
*Xiaolin Wen,Qishuang Fu,Shuangyue Han,Yichen Guo,Joseph K. Liu,Yong Wang*

Main category: cs.HC

TL;DR: Envisage是一个交互式视觉图查询系统，通过支持直观的图结构构建和灵活的参数化规则规范，提高了复杂查询场景中的表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉图查询工具仅支持构建简单和特定的查询图，限制了用户交互式表达查询意图的能力，尤其是对于未明确指定的查询意图。

Method: Envisage包括四个阶段：查询表达、查询验证、渐进式查询执行和结果分析。

Result: 通过案例研究和14名图分析师的深入用户访谈，证明了Envisage在构建、验证和执行复杂图查询方面的有效性和可用性。

Conclusion: Envisage解决了现有视觉图查询工具的局限性，提高了复杂查询场景中的表达能力和用户体验。

Abstract: Graph querying is the process of retrieving information from graph data using
specialized languages (e.g., Cypher), often requiring programming expertise.
Visual Graph Querying (VGQ) streamlines this process by enabling users to
construct and execute queries via an interactive interface without resorting to
complex coding. However, current VGQ tools only allow users to construct simple
and specific query graphs, limiting users' ability to interactively express
their query intent, especially for underspecified query intent. To address
these limitations, we propose Envisage, an interactive visual graph querying
system to enhance the expressiveness of VGQ in complex query scenarios by
supporting intuitive graph structure construction and flexible parameterized
rule specification. Specifically, Envisage comprises four stages: Query
Expression allows users to interactively construct graph queries through
intuitive operations; Query Verification enables the validation of constructed
queries via rule verification and query instantiation; Progressive Query
Execution can progressively execute queries to ensure meaningful querying
results; and Result Analysis facilitates result exploration and interpretation.
To evaluate Envisage, we conducted two case studies and in-depth user
interviews with 14 graph analysts. The results demonstrate its effectiveness
and usability in constructing, verifying, and executing complex graph queries.

</details>


### [39] [Tao-Technology for Teen Mobile Use: Harmonizing Adaptation, Autonomy, and Reflection](https://arxiv.org/abs/2507.12204)
*Pengyu Zhu,Janghee Cho*

Main category: cs.HC

TL;DR: 基于道家哲学（无为、阴阳、自然）提出了一种自组织的适应性技术监管框架——Tao-Technology，旨在动态调整移动技术使用，培养青少年与数字技术的平衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有技术监管机制过于僵化，忽视了青少年的自主性和自然使用模式，需引入更灵活、动态的监管方法。

Method: 结合反思性信息学与信息生态学，提出Tao-Technology框架，通过动态共适应调节取代外部限制。

Result: 技术监管更具灵活性和结构性，支持青少年与数字技术建立平衡、有意义的关系。

Conclusion: Tao-Technology框架为青少年技术使用提供了一种动态、自适应的监管模式，契合道家哲学的自然和谐理念。

Abstract: Adolescents' mobile technology use is often regulated through rigid control
mechanisms that fail to account for their autonomy and natural usage patterns.
Drawing on Taoist philosophy, particularly Wu Wei, Yin-Yang, and Zi Ran, this
position paper proposes Tao-Technology, a self-organizing, adaptive regulatory
framework. Integrating insights from Reflective Informatics and Information
Ecologies, we explore how mobile technology can dynamically adjust to context
while fostering self-reflection and meaning-making. This approach shifts from
external restrictions to dynamic co-adaptative regulation, ensuring technology
governance remains flexible yet structured, supporting adolescents in
cultivating a balanced and intentional relationship with digital technology.

</details>


### [40] [Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness](https://arxiv.org/abs/2507.12212)
*Garyoung Kim,Huisung Kwon,Seoju Yun,Yu-Won Youn*

Main category: cs.HC

TL;DR: 该研究探讨了四种生成式AI模型如何理解和表达“丑陋”，揭示了这些模型在文本和图像中嵌入的文化偏见。研究发现，AI模型倾向于将丑陋与老年白人男性形象联系起来，反映了深层的社会偏见和矛盾性偏见。


<details>
  <summary>Details</summary>
Motivation: 研究旨在批判性地分析生成式AI如何理解和表达“丑陋”，以及这些表达中嵌入的文化偏见，以推动更公正的AI发展。

Method: 通过迭代提示大型语言模型提取13个与“丑陋”相关的形容词，并在四种AI模型和三种提示下生成624张图像。对图像中的人口和社会经济属性进行独立编码和主题分析。

Result: AI模型将“丑陋”与老年白人男性形象联系起来，显示出矛盾性偏见。尽管试图避免对边缘群体的刻板描绘，但负面属性仍被过度投射到主流群体。物理标记（如不对称和衰老）仍是核心视觉主题。

Conclusion: 研究表明，尽管努力实现更平等的表征，生成式AI仍延续固有和矛盾性偏见，突显了开发生态训练范式和包容性AI方法的重要性。

Abstract: Generative AI does not only replicate human creativity but also reproduces
deep-seated cultural biases, making it crucial to critically examine how
concepts like ugliness are understood and expressed by these tools. This study
investigates how four different generative AI models understand and express
ugliness through text and image and explores the biases embedded within these
representations. We extracted 13 adjectives associated with ugliness through
iterative prompting of a large language model and generated 624 images across
four AI models and three prompts. Demographic and socioeconomic attributes
within the images were independently coded and thematically analyzed. Our
findings show that AI models disproportionately associate ugliness with old
white male figures, reflecting entrenched social biases as well as paradoxical
biases, where efforts to avoid stereotypical depictions of marginalized groups
inadvertently result in the disproportionate projection of negative attributes
onto majority groups. Qualitative analysis further reveals that, despite
supposed attempts to frame ugliness within social contexts, conventional
physical markers such as asymmetry and aging persist as central visual motifs.
These findings demonstrate that despite attempts to create more equal
representations, generative AI continues to perpetuate inherited and
paradoxical biases, underscoring the critical work being done to create ethical
AI training paradigms and advance methodologies for more inclusive AI
development.

</details>


### [41] [Humans are more gullible than LLMs in believing common psychological myths](https://arxiv.org/abs/2507.12296)
*Bevan Koopman,Guido Zuccon*

Main category: cs.HC

TL;DR: 论文探讨了大型语言模型（LLMs）是否像人类一样相信心理误区，并提出减轻这种倾向的方法，发现LLMs的误区信念率显著低于人类。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解LLMs是否模仿人类的心理误区信念，并探索减少这种信念的方法，以促进LLM系统的发展。

Method: 使用50个常见的心理误区，在不同提示策略下评估LLMs的误区信念，包括检索增强生成和引导性提示。

Result: LLMs的误区信念率显著低于人类，但用户提示可能影响结果；检索增强生成有效减少误区信念并揭示LLMs潜在的纠偏能力。

Conclusion: 研究对机器心理学领域有贡献，强调了认知科学方法在LLM系统评估和开发中的重要性。

Abstract: Despite widespread debunking, many psychological myths remain deeply
entrenched. This paper investigates whether Large Language Models (LLMs) mimic
human behaviour of myth belief and explores methods to mitigate such
tendencies. Using 50 popular psychological myths, we evaluate myth belief
across multiple LLMs under different prompting strategies, including
retrieval-augmented generation and swaying prompts. Results show that LLMs
exhibit significantly lower myth belief rates than humans, though user
prompting can influence responses. RAG proves effective in reducing myth belief
and reveals latent debiasing potential within LLMs. Our findings contribute to
the emerging field of Machine Psychology and highlight how cognitive science
methods can inform the evaluation and development of LLM-based systems.

</details>


### [42] [TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials](https://arxiv.org/abs/2507.12298)
*Rui Sheng,Xingbo Wang,Jiachen Wang,Xiaofu Jin,Zhonghua Sheng,Zhenxing Xu,Suraj Rajendran,Huamin Qu,Fei Wang*

Main category: cs.HC

TL;DR: 论文提出了TrialCompass，一个支持交互式探索临床试验资格标准的可视化分析系统，结合知识驱动和结果驱动方法，帮助临床医生系统优化标准。


<details>
  <summary>Details</summary>
Motivation: 当前设计临床试验资格标准的方法在支持交互式探索和整合电子健康记录数据方面存在局限性，影响了医疗干预的效果。

Method: 提出了TrialCompass系统，结合知识驱动和结果驱动方法，支持临床医生通过可视化工具迭代探索和优化资格标准，并整合电子健康记录的详细数据。

Result: 使用真实数据集证明了TrialCompass在设计和优化脓毒症休克和脓毒症相关急性肾损伤的资格标准方面的有效性。

Conclusion: TrialCompass为临床试验的资格标准设计提供了新思路，展示了可视化分析在临床研究中的潜力。

Abstract: Eligibility criteria play a critical role in clinical trials by determining
the target patient population, which significantly influences the outcomes of
medical interventions. However, current approaches for designing eligibility
criteria have limitations to support interactive exploration of the large space
of eligibility criteria. They also ignore incorporating detailed
characteristics from the original electronic health record (EHR) data for
criteria refinement. To address these limitations, we proposed TrialCompass, a
visual analytics system integrating a novel workflow, which can empower
clinicians to iteratively explore the vast space of eligibility criteria
through knowledge-driven and outcome-driven approaches. TrialCompass supports
history-tracking to help clinicians trace the evolution of their adjustments
and decisions when exploring various forms of data (i.e., eligibility criteria,
outcome metrics, and detailed characteristics of original EHR data) through
these two approaches. This feature can help clinicians comprehend the impact of
eligibility criteria on outcome metrics and patient characteristics, which
facilitates systematic refinement of eligibility criteria. Using a real-world
dataset, we demonstrated the effectiveness of TrialCompass in providing
insights into designing eligibility criteria for septic shock and
sepsis-associated acute kidney injury. We also discussed the research prospects
of applying visual analytics to clinical trials.

</details>


### [43] [An Analysis of Text Functions in Information Visualization](https://arxiv.org/abs/2507.12334)
*Chase Stokes,Anjana Arunkumar,Marti A. Hearst,Lace Padilla*

Main category: cs.HC

TL;DR: 论文提出了一个理解信息可视化中文本功能的框架，填补了先前分类的空白。通过分析120个实际可视化案例和804个文本元素，识别了10种文本功能，并揭示了四种文本设计策略。研究发现强调了文本在可视化中的多样化作用和灵活性。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的研究探讨了文本元素（如标题和注释）对理解、偏好和预测的影响，但关于文本设计和实际使用的许多问题仍未解决。本文旨在填补这一空白，提供一个更全面的文本功能分类框架。

Method: 通过分析120个真实世界的可视化和804个文本元素，识别了10种文本功能，并进行因子分析，揭示了四种主要的文本设计策略。

Result: 研究发现了10种不同的文本功能，以及四种文本设计策略：归因与变量、注释为核心的设计、视觉装饰和叙述框架。此外，还探讨了标题修辞和文本多功能性等特征。

Conclusion: 该框架为现有文本在可视化中多样化角色的分析增添了重要的细节，展示了文本如何通过不同元素的组合来传达、综合和构建视觉信息。

Abstract: Text is an integral but understudied component of visualization design.
Although recent studies have examined how text elements (e.g., titles and
annotations) influence comprehension, preferences, and predictions, many
questions remain about textual design and use in practice. This paper
introduces a framework for understanding text functions in information
visualizations, building on and filling gaps in prior classifications and
taxonomies. Through an analysis of 120 real-world visualizations and 804 text
elements, we identified ten distinct text functions, ranging from identifying
data mappings to presenting valenced subtext. We further identify patterns in
text usage and conduct a factor analysis, revealing four overarching
text-informed design strategies: Attribution and Variables, Annotation-Centric
Design, Visual Embellishments, and Narrative Framing. In addition to these
factors, we explore features of title rhetoric and text multifunctionality,
while also uncovering previously unexamined text functions, such as text
replacing visual elements. Our findings highlight the flexibility of text,
demonstrating how different text elements in a given design can combine to
communicate, synthesize, and frame visual information. This framework adds
important nuance and detail to existing frameworks that analyze the diverse
roles of text in visualization.

</details>


### [44] [MExplore: an entity-based visual analytics approach for medical expertise acquisition](https://arxiv.org/abs/2507.12337)
*Xiao Pang,Yan Huang,Chang Liu,JiYuan Liu,MingYou Liu*

Main category: cs.HC

TL;DR: 论文介绍了一个名为MExplore的交互式视觉分析系统，用于从非结构化医学文本中提取专业知识，并通过多级可视化框架支持医学知识的学习。


<details>
  <summary>Details</summary>
Motivation: 现有的医学教育工具多基于结构化数据，缺乏处理非结构化文本的方法，而这些文本提供了更灵活和详细的信息。

Method: 系统采用微调的BERT模型提取医学实体，并结合多级可视化框架进行交互式探索。

Result: 通过案例研究、用户研究和专家访谈，证明MExplore显著提升了医学专业知识的学习效果。

Conclusion: MExplore为从非结构化医学文本中获取知识提供了一种有效的交互式方法。

Abstract: Acquiring medical expertise is a critical component of medical education and
professional development. While existing studies focus primarily on
constructing medical knowledge bases or developing learning tools based on the
structured, private healthcare data, they often lack methods for extracting
expertise from unstructured medical texts. These texts constitute a significant
portion of medical literature and offer greater flexibility and detail compared
to structured data formats. Furthermore, many studies fail to provide explicit
analytical and learning pathways in this context.
  This paper introduces MExplore, an interactive visual analytics system
designed to support the acquisition of medical expertise. To address the
challenges of the inconsistencies and confidentiality concerns inherent in
unstructured medical texts, we propose a workflow that employs a fine-tuned
BERT-based model to extract medical entities (MEs) from them. We then present a
novel multilevel visual analysis framework that integrates multiple coordinated
visualizations, enabling a progressive and interactive exploration of medical
knowledge.
  To assess the effectiveness of MExplore, we conducted three case studies, a
user study, and interviews with domain experts. The results indicate that the
system significantly enhances the medical expertise acquisition process,
providing an effective interactive approach for acquiring and retaining
knowledge from medical texts.

</details>


### [45] [Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight](https://arxiv.org/abs/2507.12377)
*Ke Er Amy Zhang,Jodie Jenkinson,Laura Garrison*

Main category: cs.HC

TL;DR: 摘要通过解构方法分析视觉数据新闻中的隐含信念，探讨语言意义的不稳定性，揭示社会力量对新闻业的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示视觉数据新闻中隐含的对立信念（如客观性/主观性），并探讨这些信念如何受到社会和历史因素的影响。

Method: 采用文学批评中的解构方法和谱系学分析，对17名全球视觉数据记者的访谈进行研究，揭示语言和观念的隐含意义。

Result: 研究发现视觉数据新闻中的信念并非孤立，而是外部社会力量和范式转变的产物，这些信念影响了数据叙事的“成功”标准。

Conclusion: 结论认为，结合解构和谱系学理论可以重新定义数据叙事的成功，并丰富可视化研究的多样性，推动对当今数据化问题的社会技术探讨。

Abstract: We conduct a deconstructive reading of a qualitative interview study with 17
visual data journalists from newsrooms across the globe. We borrow a
deconstruction approach from literary critique to explore the instability of
meaning in language and reveal implicit beliefs in words and ideas. Through our
analysis we surface two sets of opposing implicit beliefs in visual data
journalism: objectivity/subjectivity and humanism/mechanism. We contextualize
these beliefs through a genealogical analysis, which brings deconstruction
theory into practice by providing a historic backdrop for these opposing
perspectives. Our analysis shows that these beliefs held within visual data
journalism are not self-enclosed but rather a product of external societal
forces and paradigm shifts over time. Through this work, we demonstrate how
thinking with critical theories such as deconstruction and genealogy can
reframe "success" in visual data storytelling and diversify visualization
research outcomes. These efforts push the ways in which we as researchers
produce domain knowledge to examine the sociotechnical issues of today's values
towards datafication and data visualization.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [46] [Real-Time Cloth Simulation Using WebGPU: Evaluating Limits of High-Resolution](https://arxiv.org/abs/2507.11794)
*Nak-Jun Sung,Jun Ma,TaeHeon Kim,Yoo-joo Choi,Min-Hyung Choi,Min Hong*

Main category: cs.GR

TL;DR: 论文研究了WebGPU在实时布料模拟中的性能优势，相较于WebGL，WebGPU通过并行计算和计算着色器显著提升了高分辨率模拟的效率。


<details>
  <summary>Details</summary>
Motivation: 传统WebGL方法在复杂物理模拟中表现不足，而WebGPU提供了更现代化的图形和计算能力，适合实现高效布料模拟。

Method: 研究采用Mass-Spring方法，结合碰撞检测和响应处理，在WebGPU框架下实现布料模拟系统，并进行性能对比测试。

Result: WebGPU在高分辨率模拟中表现优异，能够保持60fps的帧率，并处理4K至100k节点模型的实时碰撞。

Conclusion: WebGPU在实时性和渲染质量之间取得了良好平衡，为布料模拟提供了高效解决方案。

Abstract: This study explores the capabilities of WebGPU, an emerging web graphics
paradigm, for real-time cloth simulation. Traditional WebGL-based methods have
been in handling complex physical simulations due to their emphasis on graphics
rendering rather than general-purpose GPU (GPGPU) operations. WebGPU, designed
to provide modern 3D graphics and computational capabilities, offers
significant improvements through parallel processing and support for
computational shaders. In this work, we implemented a cloth simulation system
using the Mass-Spring Method within the WebGPU framework, integrating collision
detection and response handling with the 3D surface model. First, comparative
performance evaluations demonstrate that WebGPU substantially outperforms
WebGL, particularly in high-resolution simulations, maintaining 60 frames per
second (fps) even with up to 640K nodes. The second experiment aimed to
determine the real-time limitations of WebGPU and confirmed that WebGPU can
handle real-time collisions between 4K and 100k cloth node models and a 100K
triangle surface model in real-time. These experiments also highlight the
importance of balancing real-time performance with realistic rendering when
handling collisions between cloth models and complex 3D objects. Our source
code is available at https://github.com/nakjun/Cloth-Simulation-WebGPU

</details>


### [47] [Measuring and predicting visual fidelity](https://arxiv.org/abs/2507.11857)
*Benjamin Watson,Alinda Friedman,Aaron McGaffey*

Main category: cs.GR

TL;DR: 研究了测量和预测视觉保真度的技术，使用多边形模型作为视觉刺激，并通过两种简化算法改变其保真度。实验测量了命名时间、评分和偏好三种方法，发现这些方法对简化类型和水平的敏感性不同。自动预测技术在评分上表现较好，但在偏好和命名时间上效果不佳。


<details>
  <summary>Details</summary>
Motivation: 探索不同实验技术和自动方法在测量和预测视觉保真度方面的有效性。

Method: 使用多边形模型和两种简化算法，通过命名时间、评分和偏好三种实验方法测量保真度变化，并用基于图像和模型的自动技术预测这些测量结果。

Result: 实验测量对简化类型和水平敏感，但不同对象类型反应不同。自动技术在预测评分上成功，偏好上效果一般，命名时间上失败。

Conclusion: 总结并提出了改进实验和自动测量视觉保真度方法的建议。

Abstract: This paper is a study of techniques for measuring and predicting visual
fidelity. As visual stimuli we use polygonal models, and vary their fidelity
with two different model simplification algorithms. We also group the stimuli
into two object types: animals and man made artifacts. We examine three
different experimental techniques for measuring these fidelity changes: naming
times, ratings, and preferences. All the measures were sensitive to the type of
simplification and level of simplification. However, the measures differed from
one another in their response to object type. We also examine several automatic
techniques for predicting these experimental measures, including techniques
based on images and on the models themselves. Automatic measures of fidelity
were successful at predicting experimental ratings, less successful at
predicting preferences, and largely failures at predicting naming times. We
conclude with suggestions for use and improvement of the experimental and
automatic measures of visual fidelity.

</details>


### [48] [MOSPA: Human Motion Generation Driven by Spatial Audio](https://arxiv.org/abs/2507.11949)
*Shuyang Xu,Zhiyang Dou,Mingyi Shi,Liang Pan,Leo Ho,Jingbo Wang,Yuan Liu,Cheng Lin,Yuexin Ma,Wenping Wang,Taku Komura*

Main category: cs.GR

TL;DR: 该论文提出了一种空间音频驱动的人类动作生成方法，并引入首个全面的空间音频数据集，通过扩散模型生成多样且真实的动作。


<details>
  <summary>Details</summary>
Motivation: 目前虚拟人类对多样化听觉刺激的动态和真实响应仍是一个未充分研究的挑战，现有模型通常忽略空间音频对人类动作的影响。

Method: 开发了MOSPA框架，一种基于扩散的生成模型，利用有效的融合机制捕捉音频与动作的关系。

Result: 方法在该任务上实现了最先进的性能，并能生成多样化的真实动作。

Conclusion: 论文填补了空间音频驱动动作生成的空白，并公开了模型和数据集。

Abstract: Enabling virtual humans to dynamically and realistically respond to diverse
auditory stimuli remains a key challenge in character animation, demanding the
integration of perceptual modeling and motion synthesis. Despite its
significance, this task remains largely unexplored. Most previous works have
primarily focused on mapping modalities like speech, audio, and music to
generate human motion. As of yet, these models typically overlook the impact of
spatial features encoded in spatial audio signals on human motion. To bridge
this gap and enable high-quality modeling of human movements in response to
spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human
Motion (SAM) dataset, which contains diverse and high-quality spatial audio and
motion data. For benchmarking, we develop a simple yet effective
diffusion-based generative framework for human MOtion generation driven by
SPatial Audio, termed MOSPA, which faithfully captures the relationship between
body motion and spatial audio through an effective fusion mechanism. Once
trained, MOSPA could generate diverse realistic human motions conditioned on
varying spatial audio inputs. We perform a thorough investigation of the
proposed dataset and conduct extensive experiments for benchmarking, where our
method achieves state-of-the-art performance on this task. Our model and
dataset will be open-sourced upon acceptance. Please refer to our supplementary
video for more details.

</details>


### [49] [HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing](https://arxiv.org/abs/2507.11971)
*Tielong Wang,Yuxuan Xiong,Jinfan Liu,Zhifan Zhang,Ye Chen,Yue Shi,Bingbing Ni*

Main category: cs.GR

TL;DR: 提出了一种新型的3D分层代理节点表示方法，解决现有3D表示（如网格、体素、点云和NeRF）在通用性、编辑性和数据复杂度上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有3D表示方法（如网格、NeRF等）在通用性、编辑性和渲染质量之间存在显著局限性，亟需一种更高效、灵活的表征方式。

Method: 采用分层代理节点结构，通过树状组织的稀疏节点和轻量级MLP编码局部形状与纹理信息，实现高效的神经插值和轻量解码。

Result: 实验表明，该方法在3D重建和编辑任务中表现出高效的表达能力、高保真渲染质量和卓越的编辑性能。

Conclusion: 3D分层代理节点表示方法为3D数据的通用处理提供了一种高效且灵活的新途径。

Abstract: Current 3D representations like meshes, voxels, point clouds, and NeRF-based
neural implicit fields exhibit significant limitations: they are often
task-specific, lacking universal applicability across reconstruction,
generation, editing, and driving. While meshes offer high precision, their
dense vertex data complicates editing; NeRFs deliver excellent rendering but
suffer from structural ambiguity, hindering animation and manipulation; all
representations inherently struggle with the trade-off between data complexity
and fidelity. To overcome these issues, we introduce a novel 3D Hierarchical
Proxy Node representation. Its core innovation lies in representing an object's
shape and texture via a sparse set of hierarchically organized
(tree-structured) proxy nodes distributed on its surface and interior. Each
node stores local shape and texture information (implicitly encoded by a small
MLP) within its neighborhood. Querying any 3D coordinate's properties involves
efficient neural interpolation and lightweight decoding from relevant nearby
and parent nodes. This framework yields a highly compact representation where
nodes align with local semantics, enabling direct drag-and-edit manipulation,
and offers scalable quality-complexity control. Extensive experiments across 3D
reconstruction and editing demonstrate our method's expressive efficiency,
high-fidelity rendering quality, and superior editability.

</details>


### [50] [SmokeSVD: Smoke Reconstruction from A Single View via Progressive Novel View Synthesis and Refinement with Diffusion Models](https://arxiv.org/abs/2507.12156)
*Chen Li,Shanshan Dong,Sheng Qiu,Jianmin Han,Zan Gao,Kemeng Huang,Taku Komura*

Main category: cs.GR

TL;DR: 提出了一种新框架SmokeSVD，利用扩散模型和物理引导优化从单视频高效重建动态烟雾。


<details>
  <summary>Details</summary>
Motivation: 解决动态流体稀疏视图重建的挑战性问题，现有方法优化过程耗时且效果有限。

Method: 结合扩散模型的生成能力和物理引导优化，分步生成侧视图、优化密度场，并利用可微分平流重建烟雾。

Result: 实验表明该方法效果优于现有技术，实现了高质量的动态烟雾重建。

Conclusion: SmokeSVD通过创新方法显著提升了动态流体重建的质量和效率。

Abstract: Reconstructing dynamic fluids from sparse views is a long-standing and
challenging problem, due to the severe lack of 3D information from insufficient
view coverage. While several pioneering approaches have attempted to address
this issue using differentiable rendering or novel view synthesis, they are
often limited by time-consuming optimization and refinement processes under
ill-posed conditions. To tackle above challenges, we propose SmokeSVD, an
efficient and effective framework to progressively generate and reconstruct
dynamic smoke from a single video by integrating both the powerful generative
capabilities from diffusion models and physically guided consistency
optimization towards realistic appearance and dynamic evolution. Specifically,
we first propose a physically guided side-view synthesizer based on diffusion
models, which explicitly incorporates divergence and gradient guidance of
velocity fields to generate visually realistic and spatio-temporally consistent
side-view images frame by frame, significantly alleviating the ill-posedness of
single-view reconstruction without imposing additional constraints.
Subsequently, we determine a rough estimation of density field from the pair of
front-view input and side-view synthetic image, and further refine 2D blurry
novel-view images and 3D coarse-grained density field through an iterative
process that progressively renders and enhances the images from increasing
novel viewing angles, generating high-quality multi-view image sequences.
Finally, we reconstruct and estimate the fine-grained density field, velocity
field, and smoke source via differentiable advection by leveraging the
Navier-Stokes equations. Extensive quantitative and qualitative experiments
show that our approach achieves high-quality reconstruction and outperforms
previous state-of-the-art techniques.

</details>


### [51] [Shape Adaptation for 3D Hairstyle Retargeting](https://arxiv.org/abs/2507.12168)
*Lu Yu,Zhong Ren,Youyi Zheng,Xiang Chen,Kun Zhou*

Main category: cs.GR

TL;DR: 提出了一种自动形状适应方法，用于在游戏和VR应用中为目标角色重新调整3D发型，通过多尺度策略和优化问题实现高效处理，并提供用户自定义工具。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在新角色上实现复杂的发型重定向，艺术家面临高几何复杂性和空间交互的挑战。

Method: 将发型适应过程建模为约束优化问题，采用多尺度策略（从粗到细）计算发丝位置，并结合物理变形工具支持用户自定义发际线。

Result: 通过定量和定性实验验证了方法的有效性，适用于多种发型和角色。

Conclusion: 该方法高效且灵活，能够满足游戏和VR应用中发型重定向的需求。

Abstract: It is demanding to author an existing hairstyle for novel characters in games
and VR applications. However, it is a non-trivial task for artists due to the
complicated hair geometries and spatial interactions to preserve. In this
paper, we present an automatic shape adaptation method to retarget 3D
hairstyles. We formulate the adaptation process as a constrained optimization
problem, where all the shape properties and spatial relationships are converted
into individual objectives and constraints. To make such an optimization on
high-resolution hairstyles tractable, we adopt a multi-scale strategy to
compute the target positions of the hair strands in a coarse-to-fine manner.
The global solving for the inter-strands coupling is restricted to the coarse
level, and the solving for fine details is made local and parallel. In
addition, we present a novel hairline edit tool to allow for user customization
during retargeting. We achieve it by solving physics-based deformations of an
embedded membrane to redistribute the hair roots with minimal distortion. We
demonstrate the efficacy of our method through quantitative and qualitative
experiments on various hairstyles and characters.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [52] [Emerging Paradigms in the Energy Sector: Forecasting and System Control Optimisation](https://arxiv.org/abs/2507.12373)
*Dariush Pourkeramati,Gareth Wadge,Rachel Hassall,Charlotte Mitchell,Anish Khadka,Shiwang Jaiswal,Andrew Duncan,Rossella Arcucci*

Main category: cs.ET

TL;DR: 本文探讨了能源领域的前沿预测与管理方法，重点包括能源需求与天气数据结合的预测、建筑能源优化、热网优化及系统框架下的能源管理系统优化，利用机器学习和模型预测控制技术提升能源效率。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源整合、电力系统分散化以及对效率和可持续性的关注增加，能源部门正经历快速变革，需要先进的预测和优化策略以维持电网稳定性和可持续性。

Method: 采用机器学习技术和模型预测控制（MPC），研究涵盖了能源需求预测、建筑能源优化、热网优化及能源管理系统优化。

Result: 通过实际案例，研究表明AI驱动的自动化和集成控制解决方案能显著提高能源效率、降低成本并增强电网韧性。

Conclusion: AI和集成控制技术在推动能源系统向高效、韧性及可持续方向转型中具有巨大潜力。

Abstract: The energy sector is experiencing rapid transformation due to increasing
renewable energy integration, decentralisation of power systems, and a
heightened focus on efficiency and sustainability. With energy demand becoming
increasingly dynamic and generation sources more variable, advanced forecasting
and optimisation strategies are crucial for maintaining grid stability,
cost-effectiveness, and environmental sustainability. This paper explores
emerging paradigms in energy forecasting and management, emphasizing four
critical domains: Energy Demand Forecasting integrated with Weather Data,
Building Energy Optimisation, Heat Network Optimisation, and Energy Management
System (EMS) Optimisation within a System of Systems (SoS) framework.
Leveraging machine learning techniques and Model Predictive Control (MPC), the
study demonstrates substantial enhancements in energy efficiency across scales
-- from individual buildings to complex interconnected energy networks.
Weather-informed demand forecasting significantly improves grid resilience and
resource allocation strategies. Smart building optimisation integrates
predictive analytics to substantially reduce energy consumption without
compromising occupant comfort. Optimising CHP-based heat networks achieves cost
and carbon savings while adhering to operational and asset constraints. At the
systems level, sophisticated EMS optimisation ensures coordinated control of
distributed resources, storage solutions, and demand-side flexibility. Through
real-world case studies we highlight the potential of AI-driven automation and
integrated control solutions in facilitating a resilient, efficient, and
sustainable energy future.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [53] [The AI Shadow War: SaaS vs. Edge Computing Architectures](https://arxiv.org/abs/2507.11545)
*Rhea Pritham Marpu,Kevin J McNamara,Preeti Gupta*

Main category: cs.DC

TL;DR: 本文探讨了集中式云AI与分散式边缘AI在计算能力、能效和数据隐私方面的竞争，指出边缘AI凭借高效能和隐私优势正在挑战云AI。


<details>
  <summary>Details</summary>
Motivation: 研究集中式云AI与分散式边缘AI的竞争，分析边缘AI在性能、能效和隐私方面的突破及其潜在影响。

Method: 通过分析计算能力、能效和数据隐私等关键指标，对比边缘AI和云AI的技术差异和市场趋势。

Result: 边缘AI在能效上比云AI高10,000倍，隐私保护更优，市场预计从2025年的90亿美元增长到2030年的496亿美元。

Conclusion: 边缘AI的技术优势和市场需求将推动混合边缘-云生态系统的兴起。

Abstract: The very DNA of AI architecture presents conflicting paths: centralized
cloud-based models (Software-as-a-Service) versus decentralized edge AI (local
processing on consumer devices). This paper analyzes the competitive
battleground across computational capability, energy efficiency, and data
privacy. Recent breakthroughs show edge AI challenging cloud systems on
performance, leveraging innovations like test-time training and
mixture-of-experts architectures. Crucially, edge AI boasts a 10,000x
efficiency advantage: modern ARM processors consume merely 100 microwatts
forinference versus 1 watt for equivalent cloud processing. Beyond efficiency,
edge AI secures data sovereignty by keeping processing local, dismantling
single points of failure in centralized architectures. This democratizes access
throughaffordable hardware, enables offline functionality, and reduces
environmental impact by eliminating data transmission costs. The edge AI market
projects explosive growth from $9 billion in 2025 to $49.6 billion by 2030
(38.5% CAGR), fueled by privacy demands and real-time analytics. Critical
applications including personalized education, healthcare monitoring,
autonomous transport, and smart infrastructure rely on edge AI's ultra-low
latency (5-10ms versus 100-500ms for cloud). The convergence of architectural
innovation with fundamental physics confirms edge AI's distributed approach
aligns with efficient information processing, signaling the inevitable
emergence of hybrid edge-cloud ecosystems.

</details>


### [54] [A Model Aware AIGC Task Offloading Algorithm in IIoT Edge Computing](https://arxiv.org/abs/2507.11560)
*Xin Wang,Xiao Huan Li,Xun Wang*

Main category: cs.DC

TL;DR: 论文提出了一种针对工业物联网边缘计算环境的AIGC任务卸载框架，通过多智能体深度确定性策略梯度算法优化延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 工业物联网与AIGC的结合在智能制造中带来了新的机遇，但也面临计算密集和低延迟的挑战，传统云计算模型难以满足实时需求。

Method: 设计了基于MADDPG-MATO算法的AIGC任务卸载框架，首次考虑了模型切换带来的延迟和能耗，设备协作卸载任务至合适的边缘服务器。

Result: 实验表明，MADDPG-MATO相比基准算法平均降低延迟6.98%、能耗7.12%，任务完成率提升3.72%。

Conclusion: 提出的算法在动态高负载的工业物联网环境中表现出高效性和鲁棒性。

Abstract: The integration of the Industrial Internet of Things (IIoT) with Artificial
Intelligence-Generated Content (AIGC) offers new opportunities for smart
manufacturing, but it also introduces challenges related to
computation-intensive tasks and low-latency demands. Traditional generative
models based on cloud computing are difficult to meet the real-time
requirements of AIGC tasks in IIoT environments, and edge computing can
effectively reduce latency through task offloading. However, the dynamic nature
of AIGC tasks, model switching delays, and resource constraints impose higher
demands on edge computing environments. To address these challenges, this paper
proposes an AIGC task offloading framework tailored for IIoT edge computing
environments, considering the latency and energy consumption caused by AIGC
model switching for the first time. IIoT devices acted as multi-agent
collaboratively offload their dynamic AIGC tasks to the most appropriate edge
servers deployed with different generative models. A model aware AIGC task
offloading algorithm based on Multi-Agent Deep Deterministic Policy Gradient
(MADDPG-MATO) is devised to minimize the latency and energy. Experimental
results show that MADDPG-MATO outperforms baseline algorithms, achieving an
average reduction of 6.98% in latency, 7.12% in energy consumption, and a 3.72%
increase in task completion rate across four sets of experiments with model
numbers ranging from 3 to 6, it is demonstrated that the proposed algorithm is
robust and efficient in dynamic, high-load IIoT environments.

</details>


### [55] [Environmentally-Conscious Cloud Orchestration Considering Geo-Distributed Data Centers](https://arxiv.org/abs/2507.11563)
*Giulio Attenni,Novella Bartolini*

Main category: cs.DC

TL;DR: 论文提出一种理论方法，旨在通过优化云环境中的任务部署和迁移来减少其环境影响，同时融入可持续性要求。


<details>
  <summary>Details</summary>
Motivation: 随着可持续云服务需求的增长，选择可持续性数据中心的客户需要更准确的服务生态足迹报告，驱动了相关工作。

Method: 通过分析可持续性报告定义数据中心的环境影响概况，建立优化模型，平衡多种环境因素与用户偏好。

Result: 模拟案例研究表明，该方法优于仅优化单一可持续性因素的基线策略，展现了其潜力。

Conclusion: 论文通过理论模型和模拟验证，展示了在云环境中实现可持续资源调度的可行性与优势。

Abstract: This paper presents a theoretical discussion for environmentally-conscious
job deployment and migration in cloud environments, aiming to minimize the
environmental impact of resource provisioning while incorporating
sustainability requirements. As the demand for sustainable cloud services
grows, it is crucial for cloud customers to select data center operators based
on sustainability metrics and to accurately report the ecological footprint of
their services. To this end, we analyze sustainability reports and define
comprehensive environmental impact profiles for data centers, incorporating key
sustainability indicators. We formalize the problem as an optimization model,
balancing multiple environmental factors while respecting user preferences. A
simulative case study demonstrates the {potential} of our approach compared to
baseline strategies that optimize for single sustainability factors.

</details>


### [56] [PGT-I: Scaling Spatiotemporal GNNs with Memory-Efficient Distributed Training](https://arxiv.org/abs/2507.11683)
*Seth Ockerman,Amal Gueroudji,Tanwi Mallick,Yixuan He,Line Pouchard,Robert Ross,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 提出了PGT-I扩展框架，通过分布式数据并行训练和创新的索引批处理技术，显著降低了ST-GNN的内存开销并提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决ST-GNN在大规模数据集上的内存限制问题，当前分布式训练框架缺乏对时空模型的支持。

Method: 结合分布式数据并行训练，提出索引批处理和分布式索引批处理技术，动态构建时空快照以减少内存占用。

Result: 在PeMS数据集上首次实现了全图训练，峰值内存降低89%，128 GPU下速度提升13.1倍。

Conclusion: PGT-I为大规模时空图神经网络训练提供了高效且可扩展的解决方案。

Abstract: Spatiotemporal graph neural networks (ST-GNNs) are powerful tools for
modeling spatial and temporal data dependencies. However, their applications
have been limited primarily to small-scale datasets because of memory
constraints. While distributed training offers a solution, current frameworks
lack support for spatiotemporal models and overlook the properties of
spatiotemporal data. Informed by a scaling study on a large-scale workload, we
present PyTorch Geometric Temporal Index (PGT-I), an extension to PyTorch
Geometric Temporal that integrates distributed data parallel training and two
novel strategies: index-batching and distributed-index-batching. Our index
techniques exploit spatiotemporal structure to construct snapshots dynamically
at runtime, significantly reducing memory overhead, while
distributed-index-batching extends this approach by enabling scalable
processing across multiple GPUs. Our techniques enable the first-ever training
of an ST-GNN on the entire PeMS dataset without graph partitioning, reducing
peak memory usage by up to 89\% and achieving up to a 13.1x speedup over
standard DDP with 128 GPUs.

</details>


### [57] [Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI](https://arxiv.org/abs/2507.11830)
*Samyam Rajbhandari,Mert Hidayetoglu,Aurick Qiao,Ye Wang,Juncheng Yang,Jeff Rasley,Michael Wyatt,Yuxiong He*

Main category: cs.DC

TL;DR: Arctic Inference是一个开源vLLM插件，通过动态并行策略Shift Parallelism，优化推理，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的AI推理系统在延迟、吞吐量和成本之间存在权衡，无法满足实际需求。

Method: 采用Shift Parallelism动态并行策略，结合推测解码、SwiftKV计算减少和优化的嵌入推理。

Result: 实现了请求完成速度提升3.4倍，生成速度提升1.75倍，GPU嵌入推理速度达1.6M tokens/秒。

Conclusion: Arctic Inference为企业和社区提供了高效、成本优化的AI推理解决方案。

Abstract: Inference is now the dominant AI workload, yet existing systems force
trade-offs between latency, throughput, and cost. Arctic Inference, an
open-source vLLM plugin from Snowflake AI Research, introduces Shift
Parallelism, a dynamic parallelism strategy that adapts to real-world traffic
while integrating speculative decoding, SwiftKV compute reduction, and
optimized embedding inference. It achieves up to 3.4 times faster request
completion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for
embeddings, outperforming both latency- and throughput-optimized deployments.
Already powering Snowflake Cortex AI, Arctic Inference delivers
state-of-the-art, cost-effective inference for enterprise AI and is now
available to the community.

</details>


### [58] [Performance Assessment of Load Balancing Methods in Cloud Computing: Analysis of Round Robin, Equally Spread, and Throttled Strategies Using Cloud Analyst](https://arxiv.org/abs/2507.11899)
*Saeid Aghasoleymani Najafabadi*

Main category: cs.DC

TL;DR: 论文通过Cloud Analyst工具评估了不同负载均衡算法在集中式和分布式云环境中的表现，发现分布式资源布局能显著降低响应时间，智能负载均衡策略对优化性能和成本至关重要。


<details>
  <summary>Details</summary>
Motivation: 云环境中工作负载的动态性和不可预测性促使负载均衡策略从静态方法转向更自适应和智能的方法，以保持服务质量和运营效率。

Method: 使用Cloud Analyst模拟工具，评估了多种负载均衡算法（如Round Robin、Equally Spread和Throttled）在不同资源设置（集中式和分布式）下的性能。

Result: 结果显示，在单一数据中心，Round Robin算法处理时间略优；而在分布式数据中心中，Equally Spread和Throttled算法在响应时间和运营成本上表现更佳。

Conclusion: 研究表明，智能动态负载均衡和资源管理策略是优化云性能和成本的关键，需持续评估和整合新兴技术以支持可扩展的云运营。

Abstract: Load balancing plays a pivotal role in cloud computing, ensuring that
resources are optimally allocated to maintain high service quality and
operational efficiency. As workloads in cloud environments become increasingly
dynamic and unpredictable, load balancing strategies are evolving from
traditional static methods to more adaptive and intelligent approaches. In this
study, the Cloud Analyst simulation tool was used to evaluate the performance
of different load balancing algorithms under various scenarios, including both
centralized and distributed resource setups. The results highlight that while
the Round Robin algorithm yields slightly better processing times within a
single data center, Equally Spread and Throttled techniques perform
competitively, especially when network latency is considered. More importantly,
when resources are distributed across multiple data centers, response times are
significantly reduced, emphasizing the value of proximity and efficient load
distribution. In these distributed environments, Equally Spread and Throttled
algorithms not only maintain quick response times but also contribute to lower
operational costs. These findings demonstrate the necessity of strategic
resource placement and proactive infrastructure planning to balance performance
and cost. Adopting intelligent, dynamic load balancing and resource management
practices can help organizations meet evolving cloud demands, optimize costs,
and maintain a competitive advantage. Continuous evaluation and integration of
emerging technologies are crucial for sustaining effective and scalable cloud
operations.

</details>


### [59] [Making Serverless Computing Extensible: A Case Study of Serverless Data Analytics](https://arxiv.org/abs/2507.11929)
*Minchen Yu,Yinghao Ren,Jiamu Zhao,Jiaqi Li*

Main category: cs.DC

TL;DR: 提出了一种可扩展的无服务器计算设计原则，通过Proteus平台实现，支持开发者根据需求定制控制行为，提高性能。


<details>
  <summary>Details</summary>
Motivation: 解决通用无服务器平台性能不足与定制化系统复杂性之间的冲突。

Method: 设计可扩展原则，通过决策工作流抽象实现控制行为的定制化。

Result: Proteus原型优化了分析查询执行，支持跨应用细粒度资源共享。

Conclusion: 可扩展设计原则能平衡性能与通用性，适用于无服务器计算场景。

Abstract: Serverless computing has attracted a broad range of applications due to its
ease of use and resource elasticity. However, developing serverless
applications often poses a dilemma -- relying on general-purpose serverless
platforms can fall short of delivering satisfactory performance for complex
workloads, whereas building application-specific serverless systems undermines
the simplicity and generality. In this paper, we propose an extensible design
principle for serverless computing. We argue that a platform should enable
developers to extend system behaviors for domain-specialized optimizations
while retaining a shared, easy-to-use serverless environment. We take data
analytics as a representative serverless use case and realize this design
principle in Proteus. Proteus introduces a novel abstraction of decision
workflows, allowing developers to customize control-plane behaviors for
improved application performance. Preliminary results show that Proteus's
prototype effectively optimizes analytical query execution and supports
fine-grained resource sharing across diverse applications.

</details>


### [60] [NineToothed: A Triton-Based High-Level Domain-Specific Language for Machine Learning](https://arxiv.org/abs/2507.11978)
*Jiacheng Huang,Zimin Li,Yinghui Li,Haojie Wang*

Main category: cs.DC

TL;DR: 本文介绍了一种名为NineToothed的领域特定语言，旨在通过提供串行语义简化深度学习工作负载的开发过程。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习领域特定语言（如Triton）需要开发者具备并行编程专业知识，且暴露过多底层细节，增加了开发和维护的难度。

Method: NineToothed提供了一种支持串行编程的语言，自动将串行代码转换为并行代码，并包含一个代码生成器以生成高性能并行代码。

Result: 评估结果表明，NineToothed能显著简化计算内核开发，同时保持与Triton相当的性能。

Conclusion: NineToothed通过简化开发流程并减少性能损失，为深度学习工作负载提供了一种更易用的编程模型。

Abstract: The emergence of deep learning domain-specific languages (DSLs) has
substantially reduced the obstacles in developing high-performance,
cross-platform compute kernels. However, current DSLs, such as Triton, still
demand that developers possess expertise in parallel programming and expose
them to many low-level details. This requirement complicates the development
process and adds to the difficulty of maintaining compute kernels.
Consequently, developing a new programming model that supports serial
programming for deep learning workloads is crucial.
  This paper introduces NineToothed, a domain-specific language that offers
serial semantics for machine learning programming. Through the automatic
transformation of serial code into parallel code, NineToothed significantly
streamlines the development process while causing minimal performance
degradation. NineToothed encompasses (1) a language with tensor-oriented
metaprogramming (TOM) that adopts the arrange-and-apply paradigm, enabling the
expression of tiled computations without the need to manage low-level details
and (2) a code generator for generating high-performance parallel code. Our
evaluation results indicate that NineToothed can greatly simplify compute
kernel development while maintaining performance comparable to that of Triton.

</details>


### [61] [ARRC: Explainable, Workflow-Integrated Recommender for Sustainable Resource Optimization Across the Edge-Cloud Continuum](https://arxiv.org/abs/2507.12032)
*Brian-Frederik Jahnke,René Brinkhege,Jan Peter Meyer,Daniel Tebernum,Falk Howar*

Main category: cs.DC

TL;DR: ARRC是一种基于推荐系统的资源优化方法，通过透明、可解释的跨层建议，显著降低操作员工作量并提升资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘-云连续体中资源优化的可持续性、可解释性和可维护性挑战，避免现有方案的局限性。

Method: 引入ARRC推荐系统，封装优化逻辑于可审计的代理中，并通过共享接口协调，直接集成到操作员工作流中。

Result: 实验显示，ARRC减少50%以上操作员工作量，提升计算利用率7.7倍，错误率低于5%。

Conclusion: ARRC通过透明推荐架构实现可持续效率和可维护性，推动边缘-云平台的最佳实践。

Abstract: Achieving sustainable, explainable, and maintainable automation for resource
optimization is a core challenge across the edge-cloud continuum. Persistent
overprovisioning and operational complexity often stem from heterogeneous
platforms and layered abstractions, while systems lacking explainability and
maintainability become fragile, impede safe recovery, and accumulate technical
debt. Existing solutions are frequently reactive, limited to single abstraction
layers, or require intrusive platform changes, leaving efficiency and
maintainability gains unrealized.
  This paper addresses safe, transparent, and low-effort resource optimization
in dynamic, multi-tenant edge-cloud systems, without disrupting operator
workflows or increasing technical debt. We introduce ARRC, a recommender system
rooted in software engineering design principles, which delivers explainable,
cross-layer resource recommendations directly into operator workflows (such as
tickets and GitOps pull requests). ARRC encapsulates optimization logic in
specialized, auditable agents coordinated via a shared interface, supporting
maintainability and extensibility through transparency and the ability to
inspect both recommendations and their rationale.
  Empirical evaluation in a multi-region industrial deployment shows that ARRC
reduces operator workload by over 50%, improves compute utilization by up to
7.7x, and maintains error rates below 5%, with most benefits achieved through
incremental, operator-approved changes. This demonstrates that explainable,
recommendation-based architectures can achieve sustainable efficiency and
maintainability improvements at production scale.
  ARRC provides an empirically evaluated framework for integrating explainable,
workflow-driven automation into resource management, intended to advance best
practices for robust, maintainable, and transparent edge-cloud continuum
platforms.

</details>


### [62] [Distributed Algorithms for Potential Problems](https://arxiv.org/abs/2507.12038)
*Alkida Balliu,Thomas Boudier,Francesco d'Amore,Dennis Olivetti,Gustav Schmid,Jukka Suomela*

Main category: cs.DC

TL;DR: 提出了一个快速分布式算法，用于解决局部势问题（如局部最优割），在有限度图中可在对数多项式轮次内完成。


<details>
  <summary>Details</summary>
Motivation: 局部势问题的分布式轮复杂度长期存在上下界差距较大的问题，尤其是局部最优割问题，需要填补这一理论空白。

Method: 提出了一种分布式算法，适用于有限度图，能在确定性或随机性LOCAL模型中实现对数多项式轮次复杂度。

Result: 证明所有局部势问题（包括局部最优割）在有限度图中均可在对数多项式轮次内解决，填补了理论复杂度差距。

Conclusion: 确定了局部最优割问题的确定轮复杂度为对数多项式级别，解决了长期存在的理论问题。

Abstract: In this work we present a fast distributed algorithm for local potential
problems: these are graph problems where the task is to find a locally optimal
solution where no node can unilaterally improve the utility in its local
neighborhood by changing its own label. A simple example of such a problem is
the task of finding a locally optimal cut, i.e., a cut where for each node at
least half of its incident edges are cut edges. The distributed round
complexity of locally optimal cut has been wide open; the problem is known to
require $\Omega(\log n)$ rounds in the deterministic LOCAL model and
$\Omega(\log \log n)$ rounds in the randomized LOCAL model, but the only known
upper bound is the trivial brute-force solution of $O(n)$ rounds. Locally
optimal cut in bounded-degree graphs is perhaps the simplest example of a
locally checkable labeling problem for which there is still such a large gap
between current upper and lower bounds. We show that in bounded-degree graphs,
all local potential problems, including locally optimal cut, can be solved in
$\log^{O(1)} n$ rounds, both in the deterministic and randomized LOCAL models.
In particular, the deterministic round complexity of the locally optimal cut
problem is now settled to $\log^{\Theta(1)} n$.

</details>


### [63] [Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso](https://arxiv.org/abs/2507.12106)
*Antonio Salis,Gabriele Troina,Gianluca Boanelli,Marco Ottaviano,Paola Fortini,Soraya Versace*

Main category: cs.DC

TL;DR: 通过物联网和数据分析技术，智能绿色城市项目优化了城市绿地管理，提升了居民生活质量。


<details>
  <summary>Details</summary>
Motivation: 城市绿地对居民健康和福祉至关重要，智能管理可以提高效率和服务质量。

Method: 集成物联网系统、数据驱动平台和决策支持系统，实时监测树木和绿地健康状况。

Result: 通过机器学习预测模型优化灌溉，提供定制警报，提升管理效率。

Conclusion: 数字化和创新技术可支持可持续城市治理，增强环境韧性。

Abstract: The efficient design and management of public green spaces is a key factor in
promoting the health and well-being of urban population, as emphasized by the
WHO, UNEP, and EEA. These areas serve as the "green lungs" of the urban
ecosystem, playing a vital role in enhancing quality of life thanks to the
provision of ecosystem services. In this context, the Smart Green City use case
in Campobasso municipality, funded by the Italian Ministry of Enterprises
(MIMIT), emerges as an innovative model for the sustainable management of green
urban areas through the adoption of an advanced system of emerging technologies
integrated and interoperable. The project integrates IoT systems and
data-driven governance platforms, enabling real-time monitoring of the health
status of trees and green areas via a Decision Support System (DSS). It also
facilitates the collection and analysis of data from diverse sources, including
weather conditions, air quality, soil moisture, pollution levels. The resulting
cloud-based platform supports a holistic real time decision making for green
urban managers, technical experts and operational staff. It enables intelligent
control and management of urban green spaces using Tree Talker sensors,
integrated with soil moisture and water potential monitoring systems. Thanks to
predictive models based on machine learning algorithms and real time data
provided by IoT sensors, irrigation of public parks can be optimized by
providing suggestions on when and how much water to apply. Customized alerts
layers are also activated warning users when monitored parameters, such as soil
temperature, humidity, or water potential, exceed predefined thresholds. This
Use Case demonstrates how digitalization, IoT sensors fusion and technological
innovation can support sustainable urban governance, fostering environmental
resilience and improving citizens quality of life.

</details>


### [64] [Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage](https://arxiv.org/abs/2507.12205)
*Junqing Lin,Jingwei Sun,Mingge Lu,Guangzhong Sun*

Main category: cs.DC

TL;DR: EC-SpMV是一种针对稀疏大语言模型（LLM）推理优化的GPU矩阵向量乘法方法，通过分层块提取算法和新型压缩稀疏格式（EC-CSR）显著提升性能并减少存储开销。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵向量乘法（SpMV）已成为本地部署稀疏大语言模型（LLM）的性能瓶颈，现有解决方案无法充分利用稀疏LLM的结构特性。

Method: 提出了分层块提取算法和多粒度块结构捕捉，以及基于Delta索引的压缩稀疏格式（EC-CSR）。

Result: 在LLaMA和OPT模型的稀疏权重矩阵上，EC-SpMV比现有SpMV库快6.44倍，存储开销比CSR减少55.4%。

Conclusion: EC-SpMV有效解决了稀疏LLM推理中的SpMV性能问题，具有显著的性能提升和存储优化效果。

Abstract: Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance
bottleneck in the local deployment of sparse Large Language Models (LLMs),
where inference predominantly operates on workloads during the decoder phase
with a batch size of one. Existing SpMV kernels and sparse matrix formats,
originally designed for scientific computing, fail to exploit the unique
structure patterns inherent in sparse LLMs, resulting in suboptimal performance
and excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized
SpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a
hierarchical block extraction algorithm that captures multiple granularities of
block structures within sparse LLMs, and (2) a novel compressed sparse format
(EC-CSR) that employs delta indexing to reduce storage overhead and enhance
memory access efficiency. Evaluated on real sparse weight matrices from LLaMA
and OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV
libraries and reduces storage overhead by up to 55.4% compared to CSR.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [65] [SIEVE: Effective Filtered Vector Search with Collection of Indexes](https://arxiv.org/abs/2507.11907)
*Zhaoheng Li,Silu Huang,Wei Ding,Yongjoo Park,Jianjun Chen*

Main category: cs.DB

TL;DR: 论文提出了一种新的过滤向量搜索方法，通过构建多个索引来支持不同谓词形式，显著提升了搜索效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的过滤相似性搜索在处理硬约束时效率低下，无法满足多谓词形式的广泛需求。

Method: 提出通过构建多个索引来支持不同谓词形式，利用三维分析模型优化索引构建和查询选择。

Result: 方法在多个数据集上表现优异，最高提速8.06倍，构建时间仅为1%，内存占用仅为标准HNSW图的2.15倍。

Conclusion: 该方法显著提升了过滤向量搜索的效率，适用于不同选择性和形式的谓词需求。

Abstract: Many real-world tasks such as recommending videos with the kids tag can be
reduced to finding most similar vectors associated with hard predicates. This
task, filtered vector search, is challenging as prior state-of-the-art
graph-based (unfiltered) similarity search techniques quickly degenerate when
hard constraints are considered. That is, effective graph-based filtered
similarity search relies on sufficient connectivity for reaching the most
similar items within just a few hops. To consider predicates, recent works
propose modifying graph traversal to visit only the items that may satisfy
predicates. However, they fail to offer the just-a-few-hops property for a wide
range of predicates: they must restrict predicates significantly or lose
efficiency if only a small fraction of items satisfy predicates.
  We propose an opposite approach: instead of constraining traversal, we build
many indexes each serving different predicate forms. For effective
construction, we devise a three-dimensional analytical model capturing
relationships among index size, search time, and recall, with which we follow a
workload-aware approach to pack as many useful indexes as possible into a
collection. At query time, the analytical model is employed yet again to
discern the one that offers the fastest search at a given recall. We show
superior performance and support on datasets with varying selectivities and
forms: our approach achieves up to 8.06x speedup while having as low as 1%
build time versus other indexes, with less than 2.15x memory of a standard HNSW
graph and modest knowledge of past workloads.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [66] [Double Duty: FPGA Architecture to Enable Concurrent LUT and Adder Chain Usage](https://arxiv.org/abs/2507.11709)
*Junius Pun,Xilai Dai,Grace Zgheib,Mahesh A. Iyer,Andrew Boutros,Vaughn Betz,Mohamed S. Abdelfattah*

Main category: cs.AR

TL;DR: 本文提出了一种名为Double Duty的新型FPGA逻辑块架构，通过允许逻辑元素中加法器和查找表（LUT）的并发使用，显著提高了算术密度，并实现了21.6%的面积缩减。


<details>
  <summary>Details</summary>
Motivation: 受深度神经网络（DNNs）中稀疏性和混合精度的启发，作者发现现代FPGA逻辑块架构限制了加法器和LUT的独立使用，阻碍了面积优化。

Method: 作者提出Double Duty架构，通过利用现有输入绕过LUT直接连接到加法器链，实现了加法器和LUT的并发使用。

Result: 实验结果显示，在Stratix-10类似架构上，Double Duty架构在Kratos基准测试中实现了21.6%的面积缩减，Koios和VTR基准测试中分别缩减了9.3%和8.2%，且不影响关键路径延迟。

Conclusion: Double Duty架构通过增加加法器链使用的灵活性，证明了在现代FPGA架构中实现更高密度是可行的，平均面积-延迟乘积提高了9.7%。

Abstract: Flexibility and customization are key strengths of Field-Programmable Gate
Arrays (FPGAs) when compared to other computing devices. For instance, FPGAs
can efficiently implement arbitrary-precision arithmetic operations, and can
perform aggressive synthesis optimizations to eliminate ineffectual operations.
Motivated by sparsity and mixed-precision in deep neural networks (DNNs), we
investigate how to optimize the current logic block architecture to increase
its arithmetic density. We find that modern FPGA logic block architectures
prevent the independent use of adder chains, and instead only allow adder chain
inputs to be fed by look-up table (LUT) outputs. This only allows one of the
two primitives -- either adders or LUTs -- to be used independently in one
logic element and prevents their concurrent use, hampering area optimizations.
In this work, we propose the Double Duty logic block architecture to enable the
concurrent use of the adders and LUTs within a logic element. Without adding
expensive logic cluster inputs, we use 4 of the existing inputs to bypass the
LUTs and connect directly to the adder chain inputs. We accurately model our
changes at both the circuit and CAD levels using open-source FPGA development
tools. Our experimental evaluation on a Stratix-10-like architecture
demonstrates area reductions of 21.6% on adder-intensive circuits from the
Kratos benchmarks, and 9.3% and 8.2% on the more general Koios and VTR
benchmarks respectively. These area improvements come without an impact to
critical path delay, demonstrating that higher density is feasible on modern
FPGA architectures by adding more flexibility in how the adder chain is used.
Averaged across all circuits from our three evaluated benchmark set, our Double
Duty FPGA architecture improves area-delay product by 9.7%.

</details>


### [67] [MOFCO: Mobility- and Migration-Aware Task Offloading in Three-Layer Fog Computing Environments](https://arxiv.org/abs/2507.12028)
*Soheil Mahdizadeh,Elyas Oustad,Mohsen Ansari*

Main category: cs.AR

TL;DR: MOFCO算法通过结合进化博弈论和启发式方法，解决了三层雾计算环境中任务卸载的挑战，显著降低了系统成本。


<details>
  <summary>Details</summary>
Motivation: 解决用户设备移动性导致的服务迁移开销和性能下降问题。

Method: 将任务卸载和资源分配建模为MINLP问题，并采用启发式辅助的进化博弈论方法求解。

Result: 实验显示MOFCO平均降低系统成本19%，某些场景下最高可达43%。

Conclusion: MOFCO在雾计算环境中有效优化了任务卸载和资源分配，显著提升性能。

Abstract: Task offloading in three-layer fog computing environments presents a critical
challenge due to user equipment (UE) mobility, which frequently triggers costly
service migrations and degrades overall system performance. This paper
addresses this problem by proposing MOFCO, a novel Mobility- and
Migration-aware Task Offloading algorithm for Fog Computing environments. The
proposed method formulates task offloading and resource allocation as a
Mixed-Integer Nonlinear Programming (MINLP) problem and employs a
heuristic-aided evolutionary game theory approach to solve it efficiently. To
evaluate MOFCO, we simulate mobile users using SUMO, providing realistic
mobility patterns. Experimental results show that MOFCO reduces system cost,
defined as a combination of latency and energy consumption, by an average of
19% and up to 43% in certain scenarios compared to state-of-the-art methods.

</details>


### [68] [High-Performance Pipelined NTT Accelerators with Homogeneous Digit-Serial Modulo Arithmetic](https://arxiv.org/abs/2507.12418)
*George Alexakis,Dimitrios Schoinianakis,Giorgos Dimitrakopoulos*

Main category: cs.AR

TL;DR: 本文提出了一种基于数字串行模算术的NTT加速器设计，通过优化硬件架构提高时钟频率，减少复杂度，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: NTT在隐私保护技术中至关重要，其效率直接影响FHE的性能。传统硬件加速因大模数运算限制时钟频率和硬件面积，需要改进。

Method: 结合数字串行模算术和冗余数据表示，设计无需中间（反）序列化的模块化流水线NTT加速器。

Result: 实验显示，该架构在高时钟频率下保持并行性，性能超越现有方案并降低硬件复杂度。

Conclusion: 数字串行模算术与冗余表示的结合有效优化了NTT硬件加速，为FHE应用提供了高效解决方案。

Abstract: The Number Theoretic Transform (NTT) is a fundamental operation in
privacy-preserving technologies, particularly within fully homomorphic
encryption (FHE). The efficiency of NTT computation directly impacts the
overall performance of FHE, making hardware acceleration a critical technology
that will enable realistic FHE applications. Custom accelerators, in FPGAs or
ASICs, offer significant performance advantages due to their ability to exploit
massive parallelism and specialized optimizations. However, the operation of
NTT over large moduli requires large word-length modulo arithmetic that limits
achievable clock frequencies in hardware and increases hardware area costs. To
overcome such deficits, digit-serial arithmetic has been explored for modular
multiplication and addition independently. The goal of this work is to leverage
digit-serial modulo arithmetic combined with appropriate redundant data
representation to design modular pipelined NTT accelerators that operate
uniformly on arbitrary small digits, without the need for intermediate
(de)serialization. The proposed architecture enables high clock frequencies
through regular pipelining while maintaining parallelism. Experimental results
demonstrate that the proposed approach outperforms state-of-the-art
implementations and reduces hardware complexity under equal performance and
input-output bandwidth constraints.

</details>


### [69] [Characterizing State Space Model (SSM) and SSM-Transformer Hybrid Language Model Performance with Long Context Length](https://arxiv.org/abs/2507.12442)
*Saptarshi Mitra,Rachid Karami,Haocheng Xu,Sitao Huang,Hyoukjun Kwon*

Main category: cs.AR

TL;DR: 论文比较了Transformer和SSM模型在长上下文推理任务中的性能，发现SSM在长序列处理上表现更优。


<details>
  <summary>Details</summary>
Motivation: 针对传统Transformer在长上下文任务中的高复杂度问题，研究SSM和混合模型的性能优势，为系统优化提供指导。

Method: 通过全面比较Transformer、SSM和混合模型在消费级GPU上的表现，包括延迟和序列长度等方面。

Result: SSM在长序列（220K tokens）上表现优异，比Transformer快4倍，且硬件优化的SSM内核是主要性能瓶颈。

Conclusion: SSM在长上下文任务中更具优势，未来可通过硬件优化进一步提升性能，研究框架将开源以促进进一步探索。

Abstract: The demand for machine intelligence capable of processing continuous,
long-context inputs on local devices is growing rapidly. However, the quadratic
complexity and memory requirements of traditional Transformer architectures
make them inefficient and often unusable for these tasks. This has spurred a
paradigm shift towards new architectures like State Space Models (SSMs) and
hybrids, which promise near-linear scaling. While most current research focuses
on the accuracy and theoretical throughput of these models, a systematic
performance characterization on practical consumer hardware is critically
needed to guide system-level optimization and unlock new applications.
  To address this gap, we present a comprehensive, comparative benchmarking of
carefully selected Transformer, SSM, and hybrid models specifically for
long-context inference on consumer and embedded GPUs. Our analysis reveals that
SSMs are not only viable but superior for this domain, capable of processing
sequences up to 220K tokens on a 24GB consumer GPU-approximately 4x longer than
comparable Transformers. While Transformers may be up to 1.8x faster at short
sequences, SSMs demonstrate a dramatic performance inversion, becoming up to 4x
faster at very long contexts (~57K tokens). Our operator-level analysis reveals
that custom, hardware-aware SSM kernels dominate the inference runtime,
accounting for over 55% of latency on edge platforms, identifying them as a
primary target for future hardware acceleration. We also provide detailed,
device-specific characterization results to guide system co-design for the
edge. To foster further research, we will open-source our characterization
framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [70] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: PolyChartQA是一个多语言图表问答基准，涵盖10种语言的22,606个图表和26,151对问答对，旨在推动全球包容性视觉语言模型的发展。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准主要以英语为中心，限制了其全球可访问性和适用性。

Method: 采用解耦管道分离图表数据和渲染代码，通过翻译数据和重用代码灵活生成多语言图表，并利用先进的大语言模型翻译和严格的质量控制。

Result: 实验显示，英语与其他语言，尤其是非拉丁文的低资源语言，在性能上存在显著差距。

Conclusion: PolyChartQA为系统性评估多语言图表理解奠定了基础，推动了全球包容性视觉语言模型的进步。

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


### [71] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)
*Prashanth Vijayaraghavan,Apoorva Nitsure,Charles Mackin,Luyao Shi,Stefano Ambrogio,Arvind Haran,Viresh Paruthi,Ali Elzein,Dan Coops,David Beymer,Tyler Baldwin,Ehsan Degan*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在硬件描述语言（如VHDL）中的表现不佳问题，并提出了一种名为“Chain-of-Descriptions (CoDes)”的新方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在通用代码任务中表现优异，但在VHDL代码生成和总结任务中表现不足，需要针对性改进。

Method: 提出了CoDes方法，通过生成中间描述步骤并结合原始输入提示，提升LLMs的表现。

Result: 实验表明，CoDes方法在多个指标上显著优于标准提示策略。

Conclusion: CoDes为未来提升LLMs在VHDL领域的研究提供了有效框架。

Abstract: Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

</details>


### [72] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)
*Amos You*

Main category: cs.CL

TL;DR: BlockBPE是一种并行GPU实现的字节对编码（BPE），针对高吞吐量批量推理优化，比现有方法快2-2.5倍。


<details>
  <summary>Details</summary>
Motivation: 现有CPU绑定的BPE实现在大规模语言模型推理中效率低下，需要GPU优化。

Method: 去除Regex预分词，采用并行化的线程块内合并，复杂度降至近似线性。

Result: 在高批量推理中，BlockBPE的吞吐量是tiktoken的2倍，HuggingFace Tokenizers的2.5倍。

Conclusion: BlockBPE在牺牲少量生成质量的前提下，显著提升了GPU上的分词效率。

Abstract: Tokenization is a critical preprocessing step in large language model
pipelines, yet widely-used implementations remain CPU-bound and suboptimal for
batch inference workflows on GPU. We present BlockBPE, a parallel GPU
implementation of byte-pair encoding (BPE) that achieves near linear-time
complexity under realistic assumptions and is optimized for high-throughput,
batch inference. Unlike existing Rust-based tokenizers such as HuggingFace
Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex
pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the
Regex pre-tokenization which leads to small loss in generation quality, but
enables highly parallelized token merges within thread blocks, reducing overall
complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads,
BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over
HuggingFace Tokenizers.

</details>


### [73] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)
*Liu He,Yuanchao Li,Rui Feng,XinRan Han,Yin-Long Liu,Yuwei Yang,Zude Zhu,Jiahong Yuan*

Main category: cs.CL

TL;DR: 研究发现性别偏见在阿尔茨海默病（AD）语音感知中显著存在，男性语音更易被识别为AD，且这种偏见在中文语音中更为明显。


<details>
  <summary>Details</summary>
Motivation: 探讨性别偏见如何影响AD语音的感知，尤其是在不同语言背景下。

Method: 通过16名中文听众评估中文和希腊语语音，并进行声学分析（如闪烁值和语音部分）。

Result: 男性语音更常被识别为AD，闪烁值与AD感知显著相关，而语音部分与AD识别呈负相关。语言对AD感知无显著影响。

Conclusion: 性别偏见在AD语音感知中起关键作用，开发AD检测模型时需考虑这一偏见，并进一步验证不同语言背景下的模型表现。

Abstract: Gender bias has been widely observed in speech perception tasks, influenced
by the fundamental voicing differences between genders. This study reveals a
gender bias in the perception of Alzheimer's Disease (AD) speech. In a
perception experiment involving 16 Chinese listeners evaluating both Chinese
and Greek speech, we identified that male speech was more frequently identified
as AD, with this bias being particularly pronounced in Chinese speech. Acoustic
analysis showed that shimmer values in male speech were significantly
associated with AD perception, while speech portion exhibited a significant
negative correlation with AD identification. Although language did not have a
significant impact on AD perception, our findings underscore the critical role
of gender bias in AD speech perception. This work highlights the necessity of
addressing gender bias when developing AD detection models and calls for
further research to validate model performance across different linguistic
contexts.

</details>


### [74] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CL

TL;DR: 该论文提出了一种多代理辩论框架，通过结合多种LLM架构（如Llama3-8B、Gemma2-9B和Mistral-7B）来增强对用户请求中歧义的检测和解决能力。实验表明，辩论框架显著提升了模型性能，尤其是Mistral-7B主导的辩论达到了76.7%的成功率。


<details>
  <summary>Details</summary>
Motivation: LLM在处理用户请求时存在歧义问题，需要一种更有效的方法来提升其检测和解决能力。

Method: 提出多代理辩论框架，采用三种LLM架构（Llama3-8B、Gemma2-9B和Mistral-7B）和一个包含多样歧义的数据集进行实验评估。

Result: 辩论框架显著提升了Llama3-8B和Mistral-7B的性能，Mistral-7B主导的辩论成功率达到76.7%，尤其在复杂歧义和高效共识方面表现突出。

Conclusion: 多代理辩论框架是一种有效的LLM能力增强方法，为开发更具鲁棒性和自适应性的语言理解系统提供了重要启示。

Abstract: Large Language Models (LLMs) have demonstrated significant capabilities in
understanding and generating human language, contributing to more natural
interactions with complex systems. However, they face challenges such as
ambiguity in user requests processed by LLMs. To address these challenges, this
paper introduces and evaluates a multi-agent debate framework designed to
enhance detection and resolution capabilities beyond single models. The
framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and
Mistral-7B variants) and a dataset with diverse ambiguities. The debate
framework markedly enhanced the performance of Llama3-8B and Mistral-7B
variants over their individual baselines, with Mistral-7B-led debates achieving
a notable 76.7% success rate and proving particularly effective for complex
ambiguities and efficient consensus. While acknowledging varying model
responses to collaborative strategies, these findings underscore the debate
framework's value as a targeted method for augmenting LLM capabilities. This
work offers important insights for developing more robust and adaptive language
understanding systems by showing how structured debates can lead to improved
clarity in interactive systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [75] [Expanding ML-Documentation Standards For Better Security](https://arxiv.org/abs/2507.12003)
*Cara Ellen Appel*

Main category: cs.CR

TL;DR: 论文分析了ML安全和文档记录的现状，指出实践中安全意识和标准化不足，提出改进文档标准以包括安全部分的建议。


<details>
  <summary>Details</summary>
Motivation: 目前ML实践中对安全性的意识较低，文档记录缺乏标准化，导致质量不高。需要通过改进文档标准来弥补ML安全的不足。

Method: 基于对现有文献的广泛回顾，提出在现有ML文档标准（如Model Cards和Datasheets）中扩展安全相关内容。

Result: 现有标准在实践中的采用率低，IT安全常被忽略，改进的文档标准有助于提升ML安全性。

Conclusion: 建议在ML文档中增加专门的安全部分，以解决当前的安全和文档质量问题。

Abstract: This article presents the current state of ML-security and of the
documentation of ML-based systems, models and datasets in research and practice
based on an extensive review of the existing literature. It shows a generally
low awareness of security aspects among ML-practitioners and organizations and
an often unstandardized approach to documentation, leading to overall low
quality of ML-documentation. Existing standards are not regularly adopted in
practice and IT-security aspects are often not included in documentation. Due
to these factors, there is a clear need for improved security documentation in
ML, as one step towards addressing the existing gaps in ML-security. To achieve
this, we propose expanding existing documentation standards for
ML-documentation to include a security section with specific security relevant
information. Implementing this, a novel expanded method of documenting security
requirements in ML-documentation is presented, based on the existing Model
Cards and Datasheets for Datasets standards, but with the recommendation to
adopt these findings in all ML-documentation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [76] [CoCre-Sam (Kokkuri-san): Modeling Ouija Board as Collective Langevin Dynamics Sampling from Fused Language Models](https://arxiv.org/abs/2507.11906)
*Tadahiro Taniguchi,Masatoshi Nagano,Haruumi Omoto,Yoshiki Hayashi*

Main category: cs.MA

TL;DR: 该论文提出了一种名为CoCre-Sam的计算框架，用于解释多人通过物理互动（如使用Ouija板）产生的自发语言现象。


<details>
  <summary>Details</summary>
Motivation: 研究集体活动中（如Ouija板使用）如何通过分散、隐性的语言知识融合产生有意义的语言输出。

Method: 采用Langevin动力学建模，将每个参与者视为具有内部语言模型的智能体，通过集体互动进行随机采样。

Result: 理论证明集体指针运动对应Langevin MCMC采样，模拟实验验证了模型的有效性。

Conclusion: CoCre-Sam为个体隐性知识与集体行动的关联提供了新机制，基于概率采样原理解释复杂互动。

Abstract: Collective human activities like using an Ouija board (or Kokkuri-san) often
produce emergent, coherent linguistic outputs unintended by any single
participant. While psychological explanations such as the ideomotor effect
exist, a computational understanding of how decentralized, implicit linguistic
knowledge fuses through shared physical interaction remains elusive. We
introduce CoCre-Sam (Collective-Creature Sampling), a framework modeling this
phenomenon as collective Langevin dynamics sampling from implicitly fused
language models. Each participant is represented as an agent associated with an
energy landscape derived from an internal language model reflecting linguistic
priors, and agents exert stochastic forces based on local energy gradients. We
theoretically prove that the collective motion of the shared pointer
(planchette) corresponds to Langevin MCMC sampling from the sum of individual
energy landscapes, representing fused collective knowledge. Simulations
validate that CoCre-Sam dynamics effectively fuse different models and generate
meaningful character sequences, while ablation studies confirm the essential
roles of collective interaction and stochasticity. Altogether, CoCre-Sam
provides a novel computational mechanism linking individual implicit knowledge,
embodied collective action, and emergent linguistic phenomena, grounding these
complex interactions in the principles of probabilistic sampling.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [77] [Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification](https://arxiv.org/abs/2507.12042)
*Kazuki Shimada,Archontis Politis,Iran R. Roman,Parthasaarathy Sudarsanam,David Diaz-Guerra,Ruchi Pandey,Kengo Uchida,Yuichiro Koyama,Naoya Takahashi,Takashi Shibuya,Shusuke Takahashi,Tuomas Virtanen,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: DCASE2025 Task 3 挑战赛聚焦于立体声数据的声音事件定位与检测（SELD），引入新的数据集和任务，包括屏幕内外事件分类，并调整了评估指标。


<details>
  <summary>Details</summary>
Motivation: 将SELD任务从多通道音频扩展到立体声数据，以适应更常见的有限视场场景，并解决立体声数据中固有的角度模糊问题。

Method: 使用立体声音频和视频帧作为输入，设计基线系统，支持屏幕内外事件分类，并修改评估指标。

Result: 基线系统在立体声数据上表现良好。

Conclusion: 立体声SELD是可行的研究方向，DCASE2025 Task 3为未来研究提供了新数据集和任务框架。

Abstract: This paper presents the objective, dataset, baseline, and metrics of Task 3
of the DCASE2025 Challenge on sound event localization and detection (SELD). In
previous editions, the challenge used four-channel audio formats of first-order
Ambisonics (FOA) and microphone array. In contrast, this year's challenge
investigates SELD with stereo audio data (termed stereo SELD). This change
shifts the focus from more specialized 360{\deg} audio and audiovisual scene
analysis to more commonplace audio and media scenarios with limited
field-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,
the task focuses on direction-of-arrival (DOA) estimation in the azimuth plane
(left-right axis) along with distance estimation. The challenge remains divided
into two tracks: audio-only and audiovisual, with the audiovisual track
introducing a new sub-task of onscreen/offscreen event classification
necessitated by the limited FOV. This challenge introduces the DCASE2025 Task3
Stereo SELD Dataset, whose stereo audio and perspective video clips are sampled
and converted from the STARSS23 recordings. The baseline system is designed to
process stereo audio and corresponding video frames as inputs. In addition to
the typical SELD event classification and localization, it integrates
onscreen/offscreen classification for the audiovisual track. The evaluation
metrics have been modified to introduce an onscreen/offscreen accuracy metric,
which assesses the models' ability to identify which sound sources are
onscreen. In the experimental evaluation, the baseline system performs
reasonably well with the stereo audio data.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [78] [Multimodal Coordinated Online Behavior: Trade-offs and Strategies](https://arxiv.org/abs/2507.12108)
*Lorenzo Mannocci,Stefano Cresci,Matteo Magnani,Anna Monreale,Maurizio Tesconi*

Main category: cs.SI

TL;DR: 比较了单模态和多模态方法在检测在线协同行为中的效果，发现多模态方法能更全面地理解协同动态。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决传统单模态方法可能忽略多模态协同复杂性的问题，探索多模态协同行为的检测方法。

Method: 对比了弱整合与强整合多模态模型，评估不同数据模态的贡献及多模态实现方式对检测结果的影响。

Result: 结果表明，并非所有模态都能提供独特见解，但多模态方法能更全面地分析协同动态。

Conclusion: 多模态方法提升了在线协同行为的检测和分析能力，为维护数字平台完整性提供了新视角。

Abstract: Coordinated online behavior, which spans from beneficial collective actions
to harmful manipulation such as disinformation campaigns, has become a key
focus in digital ecosystem analysis. Traditional methods often rely on
monomodal approaches, focusing on single types of interactions like co-retweets
or co-hashtags, or consider multiple modalities independently of each other.
However, these approaches may overlook the complex dynamics inherent in
multimodal coordination. This study compares different ways of operationalizing
the detection of multimodal coordinated behavior. It examines the trade-off
between weakly and strongly integrated multimodal models, highlighting the
balance between capturing broader coordination patterns and identifying tightly
coordinated behavior. By comparing monomodal and multimodal approaches, we
assess the unique contributions of different data modalities and explore how
varying implementations of multimodality impact detection outcomes. Our
findings reveal that not all the modalities provide distinct insights, but that
with a multimodal approach we can get a more comprehensive understanding of
coordination dynamics. This work enhances the ability to detect and analyze
coordinated online behavior, offering new perspectives for safeguarding the
integrity of digital platforms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [79] [InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing](https://arxiv.org/abs/2507.12060)
*Kun-Hsiang Lin,Yu-Wen Tseng,Kang-Yang Huang,Jhih-Ciang Wu,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: InstructFLIP是一种新的指令调整框架，利用视觉语言模型（VLMs）增强跨域人脸反欺诈（FAS）的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决人脸反欺诈（FAS）中攻击类型语义理解不足和跨域训练冗余的问题。

Method: 通过集成VLMs和改进元域学习策略，提出InstructFLIP框架，分离指令为内容和风格组件。

Result: 实验证明InstructFLIP在准确率上优于现有最佳模型，并显著减少跨域训练冗余。

Conclusion: InstructFLIP通过视觉语言模型和元域学习有效提升了FAS的泛化能力。

Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.

</details>


### [80] [From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition](https://arxiv.org/abs/2507.11892)
*Yu Liu,Leyuan Qu,Hanlei Shi,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: 论文提出了GRACE方法，通过动态运动建模、语义文本细化和跨模态对齐，解决了动态面部表情识别中未充分利用文本情感线索和无效过滤无关面部动态的问题，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用文本中的细微情感线索，且缺乏有效过滤无关面部动态的机制。

Method: GRACE结合动态运动建模、语义文本细化（CATE模块）和跨模态对齐，通过运动差异加权和熵正则化最优传输实现精准定位情感特征。

Result: 在三个基准数据集上表现优异，特别是在模糊或不平衡情感类别场景下，UAR和WAR均达到SOTA。

Conclusion: GRACE通过多模态细化和对齐，显著提升了动态面部表情识别的性能，为情感计算提供了新思路。

Abstract: Dynamic Facial Expression Recognition (DFER) aims to identify human emotions
from temporally evolving facial movements and plays a critical role in
affective computing. While recent vision-language approaches have introduced
semantic textual descriptions to guide expression recognition, existing methods
still face two key limitations: they often underutilize the subtle emotional
cues embedded in generated text, and they have yet to incorporate sufficiently
effective mechanisms for filtering out facial dynamics that are irrelevant to
emotional expression. To address these gaps, We propose GRACE, Granular
Representation Alignment for Cross-modal Emotion recognition that integrates
dynamic motion modeling, semantic text refinement, and token-level cross-modal
alignment to facilitate the precise localization of emotionally salient
spatiotemporal features. Our method constructs emotion-aware textual
descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and
highlights expression-relevant facial motion through a motion-difference
weighting mechanism. These refined semantic and visual signals are aligned at
the token level using entropy-regularized optimal transport. Experiments on
three benchmark datasets demonstrate that our method significantly improves
recognition performance, particularly in challenging settings with ambiguous or
imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in
terms of both UAR and WAR.

</details>


### [81] [Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli](https://arxiv.org/abs/2507.12009)
*Florian David,Michael Chan,Elenor Morgenroth,Patrik Vuilleumier,Dimitri Van De Ville*

Main category: cs.CV

TL;DR: 提出了一种端到端的深度神经编码-解码模型，用于通过fMRI数据编码和解码大脑对自然刺激的反应，并通过显著性图分析了视觉解码的关键脑区。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用深度学习模型理解大脑在处理自然电影刺激时的视觉处理机制。

Method: 采用时间卷积层的编码-解码架构，利用连续电影帧的时间相关性，填补了电影刺激与fMRI采集之间的时间分辨率差距。

Result: 模型成功预测了视觉皮层及其周边区域的体素活动，并能够从神经活动中重建相应的视觉输入；贡献最大的脑区为枕叶中部、梭状回和距状沟。

Conclusion: 深度学习模型可作为理解视觉处理的工具，特别是在重建边缘、面部和对比度等基本视觉特征方面。

Abstract: We propose an end-to-end deep neural encoder-decoder model to encode and
decode brain activity in response to naturalistic stimuli using functional
magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input
from consecutive film frames, we employ temporal convolutional layers in our
architecture, which effectively allows to bridge the temporal resolution gap
between natural movie stimuli and fMRI acquisitions. Our model predicts
activity of voxels in and around the visual cortex and performs reconstruction
of corresponding visual inputs from neural activity. Finally, we investigate
brain regions contributing to visual decoding through saliency maps. We find
that the most contributing regions are the middle occipital area, the fusiform
area, and the calcarine, respectively employed in shape perception, complex
recognition (in particular face perception), and basic visual features such as
edges and contrasts. These functions being strongly solicited are in line with
the decoder's capability to reconstruct edges, faces, and contrasts. All in
all, this suggests the possibility to probe our understanding of visual
processing in films using as a proxy the behaviour of deep learning models such
as the one proposed in this paper.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [82] [AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce](https://arxiv.org/abs/2507.11597)
*Richard Timpone,Yongwei Yang*

Main category: cs.CY

TL;DR: 论文探讨了AI在研究中带来的效率和质量提升潜力，但也警告了自动化可能带来的风险，强调人机协作的重要性。


<details>
  <summary>Details</summary>
Motivation: 评估AI在数据分析中的潜力与局限，确保研究的真实性和伦理价值。

Method: 基于Truth, Beauty, Justice (TBJ)框架评估AI工具，提出人机协作的工作流程。

Result: AI可以辅助数据科学家，但自动化工具可能带来风险，需加强方法理解和培训。

Conclusion: 提倡AI作为补充工具，同时强调数据科学家在VUCA决策中的核心角色和方法培训的重要性。

Abstract: AI is transforming research. It is being leveraged to construct surveys,
synthesize data, conduct analysis, and write summaries of the results. While
the promise is to create efficiencies and increase quality, the reality is not
always as clear cut. Leveraging our framework of Truth, Beauty, and Justice
(TBJ) which we use to evaluate AI, machine learning and computational models
for effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024),
we consider the potential and limitation of analytic, generative, and agentic
AI to augment data scientists or take on tasks traditionally done by human
analysts and researchers. While AI can be leveraged to assist analysts in their
tasks, we raise some warnings about push-button automation. Just as earlier
eras of survey analysis created some issues when the increased ease of using
statistical software allowed researchers to conduct analyses they did not fully
understand, the new AI tools may create similar but larger risks. We emphasize
a human-machine collaboration perspective (Daugherty and Wilson 2018)
throughout the data science workflow and particularly call out the vital role
that data scientists play under VUCA decision areas. We conclude by encouraging
the advance of AI tools to complement data scientists but advocate for
continued training and understanding of methods to ensure the substantive value
of research is fully achieved by applying, interpreting, and acting upon
results most effectively and ethically.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [83] [Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries](https://arxiv.org/abs/2507.12384)
*Bo Wen,Guoyun Gao,Zhicheng Xu,Ruibin Mao,Xiaojuan Qi,X. Sharon Hu,Xunzhao Yin,Can Li*

Main category: cs.LG

TL;DR: 该论文提出了一种基于$MoS_2$闪存模拟内容可寻址存储器（CAM）的硬件-软件协同设计方法，用于提升树模型的鲁棒性和效率，同时保持高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展引发了对其可信赖性的关注，尤其是在可解释性和鲁棒性方面。现有的树模型在可解释性和准确性上表现良好，但计算成本高且易受设备变化和对抗攻击的影响。

Method: 采用$MoS_2$闪存模拟CAM，利用其固有的软边界特性，设计了一种软树模型推理方法，以降低设备变化和对抗攻击的影响。

Result: 实验表明，该方法在WDBC数据集上达到96%的准确率，且在MNIST数据集上仅损失0.6%的准确率（传统方法损失45.3%），表现出优异的鲁棒性。

Conclusion: 该工作为提升AI的可信赖性和效率提供了专用硬件解决方案。

Abstract: The rapid advancement of artificial intelligence has raised concerns
regarding its trustworthiness, especially in terms of interpretability and
robustness. Tree-based models like Random Forest and XGBoost excel in
interpretability and accuracy for tabular data, but scaling them remains
computationally expensive due to poor data locality and high data dependence.
Previous efforts to accelerate these models with analog content addressable
memory (CAM) have struggled, due to the fact that the difficult-to-implement
sharp decision boundaries are highly susceptible to device variations, which
leads to poor hardware performance and vulnerability to adversarial attacks.
This work presents a novel hardware-software co-design approach using $MoS_2$
Flash-based analog CAM with inherent soft boundaries, enabling efficient
inference with soft tree-based models. Our soft tree model inference
experiments on $MoS_2$ analog CAM arrays show this method achieves exceptional
robustness against device variation and adversarial attacks while achieving
state-of-the-art accuracy. Specifically, our fabricated analog CAM arrays
achieve $96\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,
while maintaining decision explainability. Our experimentally calibrated model
validated only a $0.6\%$ accuracy drop on the MNIST dataset under $10\%$ device
threshold variation, compared to a $45.3\%$ drop for traditional decision
trees. This work paves the way for specialized hardware that enhances AI's
trustworthiness and efficiency.

</details>


### [84] [ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs](https://arxiv.org/abs/2507.11649)
*Daniel Commey,Benjamin Appiah,Griffith S. Klogo,Garth V. Crosby*

Main category: cs.LG

TL;DR: 提出了一种基于零知识证明的联邦学习隐私保护评估协议，通过生成低于阈值的损失证明来避免敏感信息泄露，适用于CNN和MLP模型。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在评估阶段可能通过共享性能指标泄露敏感信息，需要一种隐私保护且可验证的评估方法。

Method: 引入零知识证明（ZKPs），客户端生成本地损失低于阈值的证明，避免了原始损失值的直接共享，并通过自包含模块实现了联邦学习仿真和ZKP电路设计。

Result: 在MNIST和HAR数据集上验证了方法的有效性，评估了计算开销、通信成本和可验证性。

Conclusion: 提出的协议通过ZKPs实现了隐私保护和可验证的联邦学习评估，适用于不同类型的模型和数据集。

Abstract: Federated Learning (FL) enables collaborative model training on decentralized
data without exposing raw data. However, the evaluation phase in FL may leak
sensitive information through shared performance metrics. In this paper, we
propose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to
enable privacy-preserving and verifiable evaluation for FL. Instead of
revealing raw loss values, clients generate a succinct proof asserting that
their local loss is below a predefined threshold. Our approach is implemented
without reliance on external APIs, using self-contained modules for federated
learning simulation, ZKP circuit design, and experimental evaluation on both
the MNIST and Human Activity Recognition (HAR) datasets. We focus on a
threshold-based proof for a simple Convolutional Neural Network (CNN) model
(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate
the approach in terms of computational overhead, communication cost, and
verifiability.

</details>


### [85] [Kevin: Multi-Turn RL for Generating CUDA Kernels](https://arxiv.org/abs/2507.11948)
*Carlo Baronio,Pietro Marsella,Ben Pan,Simon Guo,Silas Alberti*

Main category: cs.LG

TL;DR: 论文提出了一种多轮强化学习（RL）方法Kevin，用于CUDA内核的生成和优化，显著提升了内核的正确性和性能。


<details>
  <summary>Details</summary>
Motivation: GPU内核编写对AI系统效率至关重要，但其迭代性强且具有可验证的奖励（如正确性和加速），适合应用强化学习。

Method: 开发了一种多轮RL训练方法，解决真实场景中的长轨迹学习和多轮奖励分配等挑战。

Result: Kevin的评估显示，其生成的内核正确性从56%提升至82%，平均加速从0.53x提升至1.10x，超越前沿模型。

Conclusion: 研究表明，串行细化比并行采样更有效，且随着细化轮次增加，Kevin的改进速度更高。

Abstract: Writing GPU kernels is a challenging task and critical for AI systems'
efficiency. It is also highly iterative: domain experts write code and improve
performance through execution feedback. Moreover, it presents verifiable
rewards like correctness and speedup, making it a natural environment to apply
Reinforcement Learning (RL). To explicitly incorporate the iterative nature of
this process into training, we develop a flexible multi-turn RL recipe that
addresses unique challenges encountered in real-world settings, such as
learning from long trajectories and effective reward attribution across turns.
We present Kevin - K(ernel D)evin, the first model trained with multi-turn RL
for CUDA kernel generation and optimization. In our evaluation setup, Kevin
shows significant gains over its base model (QwQ-32B), improving correctness of
generated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to
1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini
(0.78x). Finally, we study its behavior across test-time scaling axes: we found
scaling serial refinement more beneficial than parallel sampling. In
particular, when given more refinement turns, Kevin shows a higher rate of
improvement.

</details>


### [86] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Main category: cs.LG

TL;DR: MNIST-Gen是一个自动化、模块化框架，用于生成特定领域的MNIST风格数据集，解决标准数据集在特定任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 标准数据集（如MNIST）在特定领域任务中不适用，且创建自定义数据集耗时且受法律限制。

Method: 结合CLIP语义理解、强化学习和人类反馈，支持分层语义分类和多模式处理。

Result: 生成Tree-MNIST和Food-MNIST数据集，实现85%自动分类准确率和80%时间节省。

Conclusion: MNIST-Gen为领域特定任务提供高效数据生成工具，具有清晰性、模块化和可扩展性。

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [87] [Syntax Repair as Language Intersection](https://arxiv.org/abs/2507.11873)
*Breandan Considine*

Main category: cs.FL

TL;DR: 本文提出了一种修复任意上下文无关语言语法错误的新技术，将其建模为语言交集问题，证明了在给定编辑距离内生成所有有效修复的可能，并在Python语法修复基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决语法错误的修复问题，特别是在任意上下文无关语言中，提出一种能够高效生成所有可能修复方案的方法。

Method: 通过将语法修复问题建模为语言交集问题，利用Bar-Hillel构造和CFL可达性理论，设计了一种基于Brzozowski导数的枚举算法。

Result: 该方法在有限次数的编辑操作内，实现了对数级并行时间的可判定性，并在Python语法修复基准测试中取得了最先进的结果。

Conclusion: 这项技术为语法修复提供了一种高效且通用的解决方案，能够在实际应用中显著提升修复效率。

Abstract: We introduce a new technique for repairing syntax errors in arbitrary
context-free languages. This technique models syntax repair as a language
intersection problem by defining a finite language that provably generates
every syntactically valid repair within a given edit distance. Leveraging a
theoretical connection between the Bar-Hillel construction from formal language
theory and CFL reachability from program analysis, we show that repairability
in a finite number of typographic edits is polylogarithmic parallel time
decidable and provide an enumeration algorithm based on the Brzozowski
derivative. Finally, we evaluate this algorithm and its implementation,
demonstrating state-of-the-art results on a Python syntax repair benchmark.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [88] [A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS](https://arxiv.org/abs/2507.11916)
*Ehsan Futuhi,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 论文提出了一种利用GPU并行计算的新方法，用于优化深度优先搜索（DFS）算法，同时确保最优性。


<details>
  <summary>Details</summary>
Motivation: GPU技术的快速发展为搜索算法提供了并行处理的能力，但目前很少有算法专门为GPU设计。本文旨在填补这一空白。

Method: 提出了一种成本受限的深度优先搜索（CB-DFS）方法，结合CPU和GPU的并行计算能力，扩展了ID A*和BTS算法。

Result: 在3x3魔方和4x4滑块拼图上的实验表明，GPU计算可以高效地批量处理DFS任务。

Conclusion: 通过结合CPU和GPU的并行能力，论文展示了在DFS中高效批处理GPU操作的可行性，并分析了性能影响因素。

Abstract: The rapid advancement of GPU technology has unlocked powerful parallel
processing capabilities, creating new opportunities to enhance classic search
algorithms. A recent successful application of GPUs is in compressing large
pattern database (PDB) heuristics using neural networks while preserving
heuristic admissibility. However, very few algorithms have been designed to
exploit GPUs during search. Several variants of A* exist that batch GPU
computations. In this paper we introduce a method for batching GPU computations
in depth first search. In particular, we describe a new cost-bounded
depth-first search (CB-DFS) method that leverages the combined parallelism of
modern CPUs and GPUs. This is used to create algorithms like \emph{Batch IDA*},
an extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an
extensions of Budgeted Tree Search. Our approach builds on the general approach
used by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality
guarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding
tile puzzle (STP), showing that GPU operations can be efficiently batched in
DFS. Additionally, we conduct extensive experiments to analyze the effects of
hyperparameters, neural network heuristic size, and hardware resources on
performance.

</details>
