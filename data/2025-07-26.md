<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 27]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations](https://arxiv.org/abs/2507.17930)
*Vahid Garousi,Zafar Jafarov*

Main category: cs.SE

TL;DR: 研究探讨了AI在软件工程中的实际应用，提出了一个流程模型和一个决策框架，以帮助开发者更有效地使用AI工具。


<details>
  <summary>Details</summary>
Motivation: AI工具如GitHub Copilot和ChatGPT正在改变软件工程师的工作方式，但如何在实际任务中信任、优化或拒绝AI生成的内容仍需深入研究。

Method: 通过实践者报告和直接的行业观察，研究者提出一个流程模型和一个2D决策框架，涵盖了提示设计、检查、回退和优化等环节。

Result: 研究表明，工程师在人类监督下能够更有效地使用AI工具，模型提供了结构化的指导以支持实际应用。

Conclusion: 该研究为软件工程中的人机协作提供了实用且轻量级的指导，推动了相关讨论。

Abstract: Artificial Intelligence (AI) has the potential to transform Software
Engineering (SE) by enhancing productivity, efficiency, and decision support.
Tools like GitHub Copilot and ChatGPT have given rise to "vibe coding"-an
exploratory, prompt-driven development style. Yet, how software engineers
engage with these tools in daily tasks, especially in deciding whether to
trust, refine, or reject AI-generated outputs, remains underexplored. This
paper presents two complementary contributions. First, a pragmatic process
model capturing real-world AI-assisted SE activities, including prompt design,
inspection, fallback, and refinement. Second, a 2D decision framework that
could help developers reason about trade-offs between effort saved and output
quality. Grounded in practitioner reports and direct observations in three
industry settings across Turkiye and Azerbaijan, our work illustrates how
engineers navigate AI use with human oversight. These models offer structured,
lightweight guidance to support more deliberate and effective use of AI tools
in SE, contributing to ongoing discussions on practical human-AI collaboration.

</details>


### [2] [Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work](https://arxiv.org/abs/2507.17991)
*Peter Eckmann,Adrian Barnett,Alexandra Bannach-Brown,Elisa Pilar Bascunan Atria,Guillaume Cabanac,Louise Delwen Owen Franzen,Małgorzata Anna Gazda,Kaitlyn Hair,James Howison,Halil Kilicoglu,Cyril Labbe,Sarah McCann,Vladislav Nachev,Martijn Roelandse,Maia Salholz-Hillel,Robert Schulz,Gerben ter Riet,Colby Vorland,Anita Bandrowski,Tracey Weissgerber*

Main category: cs.SE

TL;DR: 论文比较了11种自动化工具在9种严谨性标准上的表现，发现某些工具在特定标准（如开放数据检测）中表现优异，而在其他标准（如纳入排除标准检测）中工具组合表现更佳。提出了工具开发的改进建议。


<details>
  <summary>Details</summary>
Motivation: 解决科学报告中的标准化和透明度不足问题，现有检查清单（如ARRIVE和CONSORT）未被充分遵循，自动化工具可填补此缺口。

Method: 比较11种自动化工具在9种严谨性标准上的表现。

Result: 某些工具在特定标准中表现突出，工具组合在某些标准中优于单一工具。

Conclusion: 提出了工具开发的优先改进领域和建议，代码和数据已公开。

Abstract: The causes of the reproducibility crisis include lack of standardization and
transparency in scientific reporting. Checklists such as ARRIVE and CONSORT
seek to improve transparency, but they are not always followed by authors and
peer review often fails to identify missing items. To address these issues,
there are several automated tools that have been designed to check different
rigor criteria. We have conducted a broad comparison of 11 automated tools
across 9 different rigor criteria from the ScreenIT group. We found some
criteria, including detecting open data, where the combination of tools showed
a clear winner, a tool which performed much better than other tools. In other
cases, including detection of inclusion and exclusion criteria, the combination
of tools exceeded the performance of any one tool. We also identified key areas
where tool developers should focus their effort to make their tool maximally
useful. We conclude with a set of insights and recommendations for stakeholders
in the development of rigor and transparency detection tools. The code and data
for the study is available at https://github.com/PeterEckmann1/tool-comparison.

</details>


### [3] [An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges](https://arxiv.org/abs/2507.18029)
*Xiang Echo Chen,Wenhan Zhu,Guoshuai Albert Shi,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 研究探讨了生成式AI（GenAI）在开源游戏开发中的讨论、采用与整合，通过分析GitHub上的问题讨论，比较了GenAI与传统AI（TradAI）及非AI话题的工具、任务与挑战。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI的快速发展，其在游戏设计与开发中的应用逐渐增多，但缺乏对实际开发环境中GenAI采用情况的实证研究，尤其是在开源社区。

Method: 构建开源游戏仓库数据集，对GitHub问题进行开放卡片分类和主题分析，标注类型与内容，对比GenAI、TradAI和非AI组。

Result: 揭示了GenAI在开源游戏开发中的使用模式、开发者关注点及整合实践与传统AI的差异。

Conclusion: 研究为理解GenAI如何影响开源游戏开发的工作流程和痛点提供了实证依据。

Abstract: The growing capabilities of generative AI (GenAI) have begun to reshape how
games are designed and developed, offering new tools for content creation,
gameplay simulation, and design ideation. While prior research has explored
traditional uses of AI in games, such as controlling agents or generating
procedural content. There is limited empirical understanding of how GenAI is
adopted by developers in real-world contexts, especially within the open-source
community. This study aims to explore how GenAI technologies are discussed,
adopted, and integrated into open-source game development by analyzing issue
discussions on GitHub. We investigate the tools, tasks, and challenges
associated with GenAI by comparing GenAI-related issues to those involving
traditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI
differs from other approaches in terms of usage patterns, developer concerns,
and integration practices. To address this objective, we construct a dataset of
open-source game repositories that discuss AI-related topics. We apply open
card sorting and thematic analysis to a stratified sample of GitHub issues,
labelling each by type and content. These annotations enable comparative
analysis across GenAI, TradAI, and NonAI groups, and provide insight into how
GenAI is shaping the workflows and pain points of open-source game developers.

</details>


### [4] [Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping](https://arxiv.org/abs/2507.18037)
*Sivana Hamer,Jacob Bowen,Md Nazmul Haque,Chris Madden,Laurie Williams*

Main category: cs.SE

TL;DR: 论文通过四种独立策略将MITRE ATT&CK攻击技术与P-SSCRM任务映射，帮助软件组织管理供应链攻击风险。


<details>
  <summary>Details</summary>
Motivation: 旨在帮助软件组织识别和管理软件供应链攻击的风险。

Method: 采用四种独立策略达成一致的映射，将P-SSCRM任务与10个框架的任务对应。

Result: 提供了MITRE ATT&CK与其他主要政府及行业框架之间的映射关系。

Conclusion: 该映射为软件供应链风险管理提供了实用工具。

Abstract: The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)
Attack Technique to Proactive Software Supply Chain Risk Management Framework
(P-SSCRM) Task mapping described in this document helps software organizations
to determine how different tasks mitigate the attack techniques of software
supply chain attacks. The mapping was created through four independent
strategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to
one or more tasks from the 10 frameworks, the mapping we provide is also a
mapping between MITRE ATT&CK and other prominent government and industry
frameworks.

</details>


### [5] [Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey](https://arxiv.org/abs/2507.18039)
*Ahmad D. Suleiman,Yiming Tang,Daqing Hou*

Main category: cs.SE

TL;DR: 探究影响计算机教育工作者在软件工程和计算机课程中采用项目制学习（PjBL）的因素。


<details>
  <summary>Details</summary>
Motivation: 项目制学习能提升学生动机、协作和问题解决能力，但教师采用仍不均衡，需研究其障碍与促进策略。

Method: 混合方法研究，通过80名教师的在线调查收集定量与定性数据，统计分析障碍与促进因素。

Result: PjBL虽受重视，但受规划、设计和缺乏机构支持的限制；同行合作和机构激励可促进采用。

Conclusion: 需建立系统性支持结构，帮助教师实验并扩展PjBL实践。

Abstract: This research full paper investigates the factors influencing computing
educators' adoption of project-based learning (PjBL) in software engineering
and computing curricula. Recognized as a student-centered pedagogical approach,
PjBL has the potential to enhance student motivation, engagement, critical
thinking, collaboration, and problem-solving skills. Despite these benefits,
faculty adoption remains inconsistent due to challenges such as insufficient
institutional support, time constraints, limited training opportunities,
designing or sourcing projects, and aligning them with course objectives. This
research explores these barriers and investigates the strategies and resources
that facilitate a successful adoption. Using a mixed-methods approach, data
from 80 computing faculty were collected through an online survey comprising
closed-ended questions to quantify barriers, enablers, and resource needs,
along with an open-ended question to gather qualitative insights. Quantitative
data were analyzed using statistical methods, while qualitative responses
underwent thematic analysis. Results reveal that while PjBL is widely valued,
its adoption is often selective and impacted by challenges in planning and
managing the learning process, designing suitable projects, and a lack of
institutional support, such as time, funding, and teaching assistants. Faculty
are more likely to adopt or sustain PjBL when they have access to peer
collaboration, professional development, and institutional incentives. In
addition, sourcing projects from research, industry partnerships, and borrowing
from peers emerged as key facilitators for new projects. These findings
underscore the need for systemic support structures to empower faculty to
experiment with and scale PjBL practices.

</details>


### [6] [An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows](https://arxiv.org/abs/2507.18062)
*Edward Abrokwah,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: 本研究调查了GitHub Actions（GHA）工作流在开源项目中的结构、复杂性和合规性，旨在揭示其与官方最佳实践的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管GHA提供了官方文档和社区最佳实践，但缺乏对其在现实项目中使用情况的实证理解，可能存在不必要的复杂性问题。

Method: 通过分析Java、Python和C++项目中大量GHA工作流数据，研究其复杂性、模式、合规性及跨语言差异。

Result: 预期发现对最佳实践的遵守情况，以及改进需求，为CI服务提供更清晰指南的建议。

Conclusion: 研究结果将为优化GHA文档和工作流设计提供重要见解。

Abstract: Continuous Integration (CI) has evolved from a tooling strategy to a
fundamental mindset in modern CI engineering. It enables teams to develop,
test, and deliver software rapidly and collaboratively. Among CI services,
GitHub Actions (GHA) has emerged as a dominant service due to its deep
integration with GitHub and a vast ecosystem of reusable workflow actions.
Although GHA provides official documentation and community-supported best
practices, there appears to be limited empirical understanding of how
open-source real-world CI workflows align with such practices. Many workflows
might be unnecessarily complex and not aligned with the simplicity goals of CI
practices. This study will investigate the structure, complexity,
heterogeneity, and compliance of GHA workflows in open-source software
repositories. Using a large dataset of GHA workflows from Java, Python, and C++
repositories, our goal is to (a) identify workflow complexities, (b) analyze
recurring and heterogeneous structuring patterns, (c) assess compliance with
GHA best practices, and (d) uncover differences in CI pipeline design across
programming languages. Our findings are expected to reveal both areas of strong
adherence to best practices and areas for improvement where needed. These
insights will also have implications for CI services, as they will highlight
the need for clearer guidelines and comprehensive examples in CI documentation.

</details>


### [7] [Identifier Name Similarities: An Exploratory Study](https://arxiv.org/abs/2507.18081)
*Carol Wong,Mai Abe,Silvia De Benedictis,Marissa Halim,Anthony Peruma*

Main category: cs.SE

TL;DR: 研究探讨了标识符名称相似性对代码理解的影响，并提出了分类法。


<details>
  <summary>Details</summary>
Motivation: 标识符名称对程序理解至关重要，但研究显示命名不当会增加认知负担并阻碍协作。

Method: 通过开发分类法，分析软件项目中标识符名称相似性的发生情况。

Result: 初步研究结果为分析名称相似性对代码理解、维护和协作的影响提供了平台。

Conclusion: 分类法为后续研究提供了基础，有望进一步细化和扩展。

Abstract: Identifier names, which comprise a significant portion of the codebase, are
the cornerstone of effective program comprehension. However, research has shown
that poorly chosen names can significantly increase cognitive load and hinder
collaboration. Even names that appear readable in isolation may lead to
misunderstandings in contexts when they closely resemble other names in either
structure or functionality. In this exploratory study, we present our
preliminary findings on the occurrence of identifier name similarity in
software projects through the development of a taxonomy that categorizes
different forms of identifier name similarity. We envision our initial taxonomy
providing researchers with a platform to analyze and evaluate the impact of
identifier name similarity on code comprehension, maintainability, and
collaboration among developers, while also allowing for further refinement and
expansion of the taxonomy.

</details>


### [8] [Understanding the Supply Chain and Risks of Large Language Model Applications](https://arxiv.org/abs/2507.18105)
*Yujie Ma,Lili Quan,Xiaofei Xie,Qiang Hu,Jiongchi Yu,Yao Zhang,Sen Chen*

Main category: cs.SE

TL;DR: 本文介绍了首个用于分析和基准测试LLM供应链安全的综合数据集，揭示了LLM应用中深层嵌套依赖和供应链中的重大漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着LLM系统的广泛应用，理解其复杂供应链中的风险变得至关重要，但目前缺乏系统性研究的基准。

Method: 通过收集和分析3,859个实际LLM应用，提取模型微调路径、数据集复用和库依赖，并收集1,555个风险相关问题进行安全评估。

Result: 研究发现LLM应用中存在深层嵌套依赖，供应链中存在大量漏洞，凸显全面安全分析的必要性。

Conclusion: 提出实用建议，以指导开发更安全、可信的LLM系统。

Abstract: The rise of Large Language Models (LLMs) has led to the widespread deployment
of LLM-based systems across diverse domains. As these systems proliferate,
understanding the risks associated with their complex supply chains is
increasingly important. LLM-based systems are not standalone as they rely on
interconnected supply chains involving pretrained models, third-party
libraries, datasets, and infrastructure. Yet, most risk assessments narrowly
focus on model or data level, overlooking broader supply chain vulnerabilities.
While recent studies have begun to address LLM supply chain risks, there
remains a lack of benchmarks for systematic research.
  To address this gap, we introduce the first comprehensive dataset for
analyzing and benchmarking LLM supply chain security. We collect 3,859
real-world LLM applications and perform interdependency analysis, identifying
109,211 models, 2,474 datasets, and 9,862 libraries. We extract model
fine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's
structure. To evaluate security, we gather 1,555 risk-related issues-50 for
applications, 325 for models, 18 for datasets, and 1,229 for libraries from
public vulnerability databases.
  Using this dataset, we empirically analyze component dependencies and risks.
Our findings reveal deeply nested dependencies in LLM applications and
significant vulnerabilities across the supply chain, underscoring the need for
comprehensive security analysis. We conclude with practical recommendations to
guide researchers and developers toward safer, more trustworthy LLM-enabled
systems.

</details>


### [9] [NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition](https://arxiv.org/abs/2507.18130)
*Le Deng,Zhonghao Jiang,Jialun Cao,Michael Pradel,Zhongxin Liu*

Main category: cs.SE

TL;DR: NoCode-bench是一个用于评估LLMs在无代码开发中处理自然语言任务能力的基准测试，结果显示当前LLMs的成功率较低。


<details>
  <summary>Details</summary>
Motivation: 通过自然语言驱动的无代码开发提高生产力和普及开发，利用LLMs潜力实现这一目标。

Method: 构建NoCode-bench基准测试，包含634个任务和114k代码更改，验证LLMs在文档更新和代码实现中的表现。

Result: 最佳LLMs任务成功率仅为15.79%，暴露了跨文件编辑和代码库理解的挑战。

Conclusion: LLMs尚未完全适用于自然语言驱动的无代码开发，NoCode-bench为未来研究提供基础。

Abstract: Natural language-driven no-code development allows users to specify software
functionality using natural language (NL) instead of editing source code,
promising increased productivity and democratized development. Large language
models (LLMs) show potential in enabling this paradigm. In this context,
software documentation acts as an NL specification for functionality. This work
introduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world
NL-driven feature addition tasks, consisting of 634 tasks across 10 projects
and 114k code changes. Each task pairs documentation updates with corresponding
code implementations, validated by developer-written test cases. A subset of
114 high-quality, human-verified instances, NoCode-bench Verified, ensures
reliable evaluation. Our experiments reveal that, despite high token usage, the
best LLMs achieve a task success rate of only 15.79%, highlighting challenges
in cross-file editing, codebase understanding, and tool calling. These findings
indicate that LLMs are not yet ready for fully NL-driven no-code development.
NoCode-bench lays the foundation for future advances in this area.

</details>


### [10] [SMECS: A Software Metadata Extraction and Curation Software](https://arxiv.org/abs/2507.18159)
*Stephan Ferenz,Aida Jafarbigloo,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: SMECS工具通过从GitHub等仓库提取元数据并提供交互式界面简化了高质量元数据的创建，促进了研究软件的FAIR化。


<details>
  <summary>Details</summary>
Motivation: 研究人员和研究软件工程师创建高质量元数据的成本较高，阻碍了FAIR原则的实施。

Method: 开发SMECS工具，集成现有元数据提取和用户友好的界面用于元数据整理，支持导出为CodeMeta文件。

Result: 可用性实验证实SMECS用户体验良好。

Conclusion: SMECS通过简化和自动化元数据创建过程，有效支持了研究软件的FAIR化。

Abstract: Metadata play a crucial role in adopting the FAIR principles for research
software and enables findability and reusability. However, creating
high-quality metadata can be resource-intensive for researchers and research
software engineers. To address this challenge, we developed the Software
Metadata Extraction and Curation Software (SMECS) which integrates the
extraction of metadata from existing sources together with a user-friendly
interface for metadata curation. SMECS extracts metadata from online
repositories such as GitHub and presents it to researchers through an
interactive interface for further curation and export as a CodeMeta file. The
usability of SMECS was evaluated through usability experiments which confirmed
that SMECS provides a satisfactory user experience. SMECS supports the
FAIRification of research software by simplifying metadata creation.

</details>


### [11] [GenAI for Automotive Software Development: From Requirements to Wheels](https://arxiv.org/abs/2507.18223)
*Nenad Petrovic,Fengjunjie Pan,Vahid Zolfaghari,Krzysztof Lebioda,Andre Schamschurko,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出一种GenAI赋能的汽车软件自动化开发方法，强调自动驾驶和高级驾驶辅助系统（ADAS）功能。


<details>
  <summary>Details</summary>
Motivation: 通过自动化开发流程与AI技术，缩短ADAS相关的合规性和再工程周期，减少开发和测试时间。

Method: 结合大型语言模型（LLM）和检索增强生成（RAG），实现需求建模、测试场景生成及代码生成（Python/C++）。

Result: 生成测试场景代码和ADAS功能实现，提升开发效率。

Conclusion: 此方法可显著优化ADAS相关功能的开发与测试流程。

Abstract: This paper introduces a GenAI-empowered approach to automated development of
automotive software, with emphasis on autonomous and Advanced Driver Assistance
Systems (ADAS) capabilities. The process starts with requirements as input,
while the main generated outputs are test scenario code for simulation
environment, together with implementation of desired ADAS capabilities
targeting hardware platform of the vehicle connected to testbench. Moreover, we
introduce additional steps for requirements consistency checking leveraging
Model-Driven Engineering (MDE). In the proposed workflow, Large Language Models
(LLMs) are used for model-based summarization of requirements (Ecore metamodel,
XMI model instance and OCL constraint creation), test scenario generation,
simulation code (Python) and target platform code generation (C++).
Additionally, Retrieval Augmented Generation (RAG) is adopted to enhance test
scenario generation from autonomous driving regulations-related documents. Our
approach aims shorter compliance and re-engineering cycles, as well as reduced
development and testing time when it comes to ADAS-related capabilities.

</details>


### [12] [An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs](https://arxiv.org/abs/2507.18267)
*Zeqin Liao,Zibin Zheng,Peifan Reng,Henglong Liang,Zixu Gao,Zhixiang Chen,Wei Li,Yuhong Nan*

Main category: cs.SE

TL;DR: 该论文首次系统研究了885个EAIR系统漏洞，分类了18个根因、15种症状和13个受影响模块，揭示了EAIR特有的8种症状和8种根因，并为漏洞预测、检测和修复提供了模块映射。


<details>
  <summary>Details</summary>
Motivation: EAIR系统漏洞的普遍性和特殊性尚未被深入理解，这阻碍了相关技术和实践的开发。

Method: 收集80个EAIR项目的885个漏洞，系统分析其症状、根因和模块分布。

Result: 识别了EAIR特有的8种症状和8种根因，构建了根因与模块的映射关系。

Conclusion: 研究结果为EAIR系统漏洞的预测、检测和修复提供了新方向和工具。

Abstract: Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly
evolving technological domain. Ensuring their program correctness is
fundamental to their successful deployment. However, a general and in-depth
understanding of EAIR system bugs remains lacking, which hinders the
development of practices and techniques to tackle EAIR system bugs.
  To bridge this gap, we conducted the first systematic study of 885 EAIR
system bugs collected from 80 EAIR system projects to investigate their
symptoms, underlying causes, and module distribution. Our analysis takes
considerable effort, which classifies these bugs into 18 underlying causes, 15
distinct symptoms, and identifies 13 affected modules. It reveals several new
interesting findings and implications which help shed light on future research
on tackling or repairing EAIR system bugs. First, among the 15 identified
symptoms, our findings highlight 8 symptoms specific to EAIR systems, which is
characterized by severe functional failures and potential physical hazards.
Second, within the 18 underlying causes, we define 8 EAIR-specific causes, the
majority of which stem from the intricate issues of AI- agent reasoning and
decision making. Finally, to facilitate precise and efficient bug prediction,
detection, and repair, we constructed a mapping between underlying causes and
the modules in which they most frequently occur, which enables researchers to
focus diagnostic efforts on the modules most susceptible to specific bug types.

</details>


### [13] [Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling](https://arxiv.org/abs/2507.18289)
*Yan Li,Wenzhang Yang,Yuekun Wang,Jian Gao,Shaohua Wang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: Scheduzz是一种基于LLM的自动库模糊测试技术，通过理解库的合理使用并优化资源调度，显著提高了覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 传统库模糊测试技术需要专家编写高质量的模糊驱动程序，且现有技术因缺乏对库使用规范的遵守而生成不合理的驱动程序，浪费资源。

Method: 利用LLM理解库的合理使用并提取API组合约束，采用双重调度框架管理API组合和模糊驱动程序。

Result: 在33个真实库中测试，Scheduzz比基线方法显著降低了计算开销，在21个库中有16个优于UTopia，覆盖率分别比CKGFuzzer、Promptfuzz和OSS-Fuzz高出1.62倍、1.50倍和1.89倍，发现了33个未知漏洞。

Conclusion: Scheduzz通过结合LLM和优化调度，有效地解决了现有库模糊测试技术的不足，显著提升了测试效率和结果的可靠性。

Abstract: Fuzzing a library requires experts to understand the library usage well and
craft high-quality fuzz drivers, which is tricky and tedious. Therefore, many
techniques have been proposed to automatically generate fuzz drivers. However,
they fail to generate rational fuzz drivers due to the lack of adherence to
proper library usage conventions, such as ensuring a resource is closed after
being opened. To make things worse, existing library fuzzing techniques
unconditionally execute each driver, resulting in numerous irrational drivers
that waste computational resources while contributing little coverage and
generating false positive bug reports.
  To tackle these challenges, we propose a novel automatic library fuzzing
technique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs
to understand rational usage of libraries and extract API combination
constraints. To optimize computational resource utilization, a dual scheduling
framework is implemented to efficiently manage API combinations and fuzz
drivers. The framework models driver generation and the corresponding fuzzing
campaign as an online optimization problem. Within the scheduling loop,
multiple API combinations are selected to generate fuzz drivers, while
simultaneously, various optimized fuzz drivers are scheduled for execution or
suspension.
  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared
to baseline approaches, Scheduzz significantly reduces computational overhead
and outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and
1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,
Promptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,
Scheduzz discovered 33 previously unknown bugs in these well-tested libraries,
3 of which have been assigned CVEs.

</details>


### [14] [YATE: The Role of Test Repair in LLM-Based Unit Test Generation](https://arxiv.org/abs/2507.18316)
*Michael Konstantinou,Renzo Degiovanni,Jie M. Zhang,Mark Harman,Mike Papadakis*

Main category: cs.SE

TL;DR: 论文提出了一种名为YATE的技术，用于修复语言模型生成的错误测试用例，通过静态分析和重新提示，显著提高了代码覆盖率和突变测试效果。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型可以生成单元测试，但许多测试在语法或语义上是错误的。修复这些错误测试可以直接增加测试价值，并间接为生成更多测试提供良好种子。

Method: YATE结合了基于规则的静态分析和重新提示技术，修复语言模型生成的错误测试用例。

Result: 在6个开源项目上评估显示，YATE平均增加了32.06%的代码行覆盖和21.77%的突变杀死率，优于其他LLM方法。

Conclusion: YATE在修复错误测试方面表现优异，显著提升了测试覆盖率和突变测试效果，同时成本与其他方法相当。

Abstract: Recent advances in automated test generation utilises language models to
produce unit tests. While effective, language models tend to generate many
incorrect tests with respect to both syntax and semantics. Although such
incorrect tests can be easily detected and discarded, they constitute a "missed
opportunity" -- if fixed, they are often valuable as they directly add testing
value (they effectively target the underlying program logic to be tested) and
indirectly form good seeds for generating additional tests. To this end, we
propose a simple technique for repairing some of these incorrect tests through
a combination of rule-based static analysis and re-prompting. We evaluate this
simple approach, named YATE, on a set of 6 open-source projects and show that
it can effectively produce tests that cover on average 32.06% more lines and
kill 21.77% more mutants than a plain LLM-based method. We also compare YATE
with four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and
COVERUP and show that it produces tests that cover substantially more code.
YATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%
more mutants at a comparable cost (number of calls to LLMs).

</details>


### [15] [Gotta catch 'em all! Towards File Localisation from Issues at Large](https://arxiv.org/abs/2507.18319)
*Jesse Maarleveld,Jiapan Guo,Daniel Feitosa*

Main category: cs.SE

TL;DR: 该研究提出了一个用于创建问题文件定位数据集的数据管道，评估了传统信息检索方法的基线性能，并分析了已知偏见对数据集的影响。


<details>
  <summary>Details</summary>
Motivation: 研究目标是处理所有类型的问题（而不仅仅是bug），以节省开发者的时间。现有研究多集中于bug或特定类型的问题，而本研究旨在全面覆盖所有问题。

Method: 研究提供了处理任意分支和合并实践的数据管道，使用传统信息检索方法评估基线性能，并通过统计分析探讨已知偏见的影响。

Result: 结果表明，基于bug特定启发式的方法在一般问题上表现不佳，不同问题类型之间存在显著的性能差异，且项目依赖性较强。

Conclusion: 需要研究通用模型，并开发能够适应项目特定特征的方法。

Abstract: Bug localisation, the study of developing methods to localise the files
requiring changes to resolve bugs, has been researched for a long time to
develop methods capable of saving developers' time. Recently, researchers are
starting to consider issues outside of bugs. Nevertheless, most existing
research into file localisation from issues focusses on bugs or uses other
selection methods to ensure only certain types of issues are considered as part
of the focus of the work. Our goal is to work on all issues at large, without
any specific selection.
  In this work, we provide a data pipeline for the creation of issue file
localisation datasets, capable of dealing with arbitrary branching and merging
practices. We provide a baseline performance evaluation for the file
localisation problem using traditional information retrieval approaches.
Finally, we use statistical analysis to investigate the influence of biases
known in the bug localisation community on our dataset.
  Our results show that methods designed using bug-specific heuristics perform
poorly on general issue types, indicating a need for research into general
purpose models. Furthermore, we find that there are small, but statistically
significant differences in performance between different issue types. Finally,
we find that the presence of identifiers have a small effect on performance for
most issue types. Many results are project-dependent, encouraging the
development of methods which can be tuned to project-specific characteristics.

</details>


### [16] [FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping](https://arxiv.org/abs/2507.18339)
*Nils Bosbach,Meik Schmidt,Lukas Jünger,Matthias Berthold,Rainer Leupers*

Main category: cs.SE

TL;DR: 论文提出了一种新框架，通过FMI标准将SystemC虚拟平台与外部工具连接，实现更全面的软件测试和验证。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性增加，需要更全面的测试和虚拟原型设计，但SystemC缺乏对FMI的原生支持，限制了其在协同仿真中的应用。

Method: 利用FMI标准控制SystemC虚拟平台，并展示了一个案例：通过FMI从外部工具获取温度数据，供SystemC仿真中的温度传感器使用。

Result: 未修改的目标软件可在虚拟平台上运行，并从外部工具接收真实环境输入数据，支持全面的软件测试和验证。

Conclusion: 该方法提前完成软件测试，有助于硬件可用时更早完成认证（如ISO 26262），提高开发效率。

Abstract: As systems become more complex, the demand for thorough testing and virtual
prototyping grows. To simulate whole systems, multiple tools are usually needed
to cover different parts. These parts include the hardware of a system and the
environment with which the system interacts. The Functional Mock-up Interface
(FMI) standard for co-simulation can be used to connect these tools.
  The control part of modern systems is usually a computing unit, such as a
System-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software
from a connected memory and interacts with peripherals. To develop software
without requiring access to physical hardware, full-system simulators, the
so-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized
framework for VP development is SystemC TLM. SystemC provides interfaces and
concepts that enable modular design and model exchange. However, SystemC lacks
native FMI support, which limits the integration into broader co-simulation
environments.
  This paper presents a novel framework to control and interact with
SystemC-based VPs using the FMI. We present a case study showing how a
simulated temperature sensor in a SystemC simulation can obtain temperature
values from an external tool via FMI. This approach allows the unmodified
target software to run on the VP and receive realistic environmental input data
such as temperature, velocity, or acceleration values from other tools. Thus,
extensive software testing and verification is enabled. By having tests ready
and the software pre-tested using a VP once the physical hardware is available,
certifications like ISO 26262 can be done earlier.

</details>


### [17] [Automated Code Review Using Large Language Models with Symbolic Reasoning](https://arxiv.org/abs/2507.18476)
*Busra Icoz,Goksel Biricik*

Main category: cs.SE

TL;DR: 论文提出了一种结合符号推理和大型语言模型的混合方法，用于自动化代码审查，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 手动代码审查主观性强且耗时，自动化是理想选择，但现有大型语言模型缺乏逻辑推理能力。

Method: 采用符号推理技术与大型语言模型（如CodeT5、CodeBERT）结合的方法，并在CodexGlue数据集上测试。

Result: 实验表明，该方法显著提升了自动化代码审查的准确性和效率。

Conclusion: 混合方法在代码审查自动化中具有潜力，未来可进一步优化。

Abstract: Code review is one of the key processes in the software development lifecycle
and is essential to maintain code quality. However, manual code review is
subjective and time consuming. Given its rule-based nature, code review is well
suited for automation. In recent years, significant efforts have been made to
automate this process with the help of artificial intelligence. Recent
developments in Large Language Models (LLMs) have also emerged as a promising
tool in this area, but these models often lack the logical reasoning
capabilities needed to fully understand and evaluate code. To overcome this
limitation, this study proposes a hybrid approach that integrates symbolic
reasoning techniques with LLMs to automate the code review process. We tested
our approach using the CodexGlue dataset, comparing several models, including
CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining
symbolic reasoning and prompting techniques with LLMs. Our results show that
this approach improves the accuracy and efficiency of automated code review.

</details>


### [18] [A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat](https://arxiv.org/abs/2507.18515)
*Zezhou Yang,Ting Peng,Cuiyun Gao,Chaozheng Wang,Hailiang Huang,Yuetang Deng*

Main category: cs.SE

TL;DR: 本文研究了检索增强生成（RAG）在闭源代码库中的应用，特别是在微信工业级代码库中，发现相似性RAG优于标识符RAG，且结合词法和语义检索技术效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注开源代码库，而闭源代码库与开源之间存在潜在分布差异，需探索RAG在工业环境中的有效性。

Method: 对26个开源LLM测试两种RAG方法（标识符和相似性），并采用不同检索技术（词法和语义）。

Result: 相似性RAG表现更优，BM25和GTE-Qwen分别是词法和语义检索的最佳选择，两者结合效果最佳。

Conclusion: RAG在闭源代码库中有效，结合词法和语义检索可提升性能，并通过开发者调查验证其实用性。

Abstract: Code completion, a crucial task in software engineering that enhances
developer productivity, has seen substantial improvements with the rapid
advancement of large language models (LLMs). In recent years,
retrieval-augmented generation (RAG) has emerged as a promising method to
enhance the code completion capabilities of LLMs, which leverages relevant
context from codebases without requiring model retraining. While existing
studies have demonstrated the effectiveness of RAG on public repositories and
benchmarks, the potential distribution shift between open-source and
closed-source codebases presents unique challenges that remain unexplored. To
mitigate the gap, we conduct an empirical study to investigate the performance
of widely-used RAG methods for code completion in the industrial-scale codebase
of WeChat, one of the largest proprietary software systems. Specifically, we
extensively explore two main types of RAG methods, namely identifier-based RAG
and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B
parameters. For a more comprehensive analysis, we employ different retrieval
techniques for similarity-based RAG, including lexical and semantic retrieval.
Based on 1,669 internal repositories, we achieve several key findings: (1) both
RAG methods demonstrate effectiveness in closed-source repositories, with
similarity-based RAG showing superior performance, (2) the effectiveness of
similarity-based RAG improves with more advanced retrieval techniques, where
BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior
performance, and (3) the combination of lexical and semantic retrieval
techniques yields optimal results, demonstrating complementary strengths.
Furthermore, we conduct a developer survey to validate the practical utility of
RAG methods in real-world development environments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: 本文提出了一种统一的范畴化方法来扩展Howe's method，用于证明高阶语言行为一致性的程序同余性。


<details>
  <summary>Details</summary>
Motivation: 随着具有定量特性（如概率性）语言的兴起，传统共归纳方法需要扩展以支持更精细的行为一致性，并确保其程序同余性。现有方法需针对特定语言和行为概念进行复杂调整，缺乏统一框架。

Method: 开发了一种基于范畴论的统一方法，通过抽象高阶规范（AHOS）建模语言操作语义，并通过纤维化建模行为一致性。

Result: 证明了在自然条件下，行为一致性的最大双一致性形成程序同余性。

Conclusion: 该方法可广泛应用于高阶语言的行为一致性分析，并通过实例验证了其在概率高阶语言中的有效性。

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Frame-Based Zero-Shot Semantic Channel Equalization for AI-Native Communications](https://arxiv.org/abs/2507.17835)
*Simone Fiorellino,Claudio Battiloro,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.NI

TL;DR: 为解决AI原生无线网络中异构编码器潜在空间不匹配导致的语义噪声问题，提出了一种无需重新训练的Parseval Frame Equalizer（PFE）方法，并通过动态优化策略协调资源以提高性能。


<details>
  <summary>Details</summary>
Motivation: 异构编码器的潜在空间不匹配可能导致语义噪声，影响通信性能，需要一种无需重新训练的解决方案。

Method: 提出PFE方法，通过零射帧对齐异构编码器的潜在空间，并结合动态优化策略协调通信、计算和学习资源。

Result: 仿真结果表明，PFE能有效维持语义一致性，并在多样化网络条件下满足延迟和准确性要求。

Conclusion: PFE为异构编码器的语义通信提供了一种高效的解决方案，适用于动态网络环境。

Abstract: In future AI-native wireless networks, the presence of mismatches between the
latent spaces of independently designed and trained deep neural network (DNN)
encoders may impede mutual understanding due to the emergence of semantic
channel noise. This undermines the receiver's ability to interpret transmitted
representations, thereby reducing overall system performance. To address this
issue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based
semantic channel equalizer that aligns latent spaces of heterogeneous encoders
without requiring system retraining. PFE enables dynamic signal compression and
expansion, mitigating semantic noise while preserving performance on downstream
tasks. Building on this capability, we introduce a dynamic optimization
strategy that coordinates communication, computation, and learning resources to
balance energy consumption, end-to-end (E2E) latency, and task performance in
multi-agent semantic communication scenarios. Extensive simulations confirm the
effectiveness of our approach in maintaining semantic consistency and meeting
long-term constraints on latency and accuracy under diverse and time-varying
network conditions.

</details>


### [21] [ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks](https://arxiv.org/abs/2507.17861)
*Daniel Ricardo Cunha Oliveira,Rodrigo Moreira,Flávio de Oliveira Silva*

Main category: cs.NI

TL;DR: 提出了自动无线电覆盖异常检测与评估方法（ARCADE），用于识别和诊断蜂窝接入网络中的异常，为6G网络中人工智能的更广泛应用提供支持。


<details>
  <summary>Details</summary>
Motivation: 当前5G网络的网络数据分析功能（NWDAF）尚不足以支持网络全自动化，需要更全面的方法来探索未充分研究的网络部分。

Method: 提出ARCADE方法，采用混合网络分析架构，结合人工智能技术检测和诊断网络异常。

Result: ARCADE方法成功展示了在6G网络中如何更广泛地应用人工智能，提升网络性能。

Conclusion: ARCADE为6G网络中的自动化需求提供了可行的解决方案，推动了AI在网络分析中的进一步应用。

Abstract: Artificial Intelligence (AI) plays a key role in developing 6G networks.
While current specifications already include Network Data Analytics Function
(NWDAF) as a network element responsible for providing information about the
core, a more comprehensive approach will be needed to enable automation of
network segments that are not yet fully explored in the context of 5G. In this
paper, we present Automated Radio Coverage Anomalies Detection and Evaluation
(ARCADE), a methodology for identifying and diagnosing anomalies in the
cellular access network. Furthermore, we demonstrate how a hybrid architecture
of network analytics functions in the evolution toward 6G can enhance the
application of AI in a broader network context, using ARCADE as a practical
example of this approach.

</details>


### [22] [Talk with the Things: Integrating LLMs into IoT Networks](https://arxiv.org/abs/2507.17865)
*Alakesh Kalita*

Main category: cs.NI

TL;DR: 论文提出了一种将大型语言模型（LLMs）集成到物联网（IoT）架构中的边缘计算框架，用于实现自然语言控制、上下文感知决策和增强自动化。


<details>
  <summary>Details</summary>
Motivation: LLMs与IoT网络的融合为构建智能、响应迅速且用户友好的系统提供了新机遇。

Method: 采用模块化和轻量级的检索增强生成（RAG）型LLMs，部署在IoT网关的边缘计算设备上，实现本地化处理用户指令和传感器数据。

Result: 通过基于LLaMA 3和Gemma 2B模型的智能家居原型验证了框架的有效性，权衡了模型准确性和推理时间。

Conclusion: 探讨了LLM-based IoT系统的潜在应用及关键挑战。

Abstract: The convergence of Large Language Models (LLMs) and Internet of Things (IoT)
networks open new opportunities for building intelligent, responsive, and
user-friendly systems. This work presents an edge-centric framework that
integrates LLMs into IoT architectures to enable natural language-based
control, context-aware decision-making, and enhanced automation. The proposed
modular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are
deployed on edge computing devices connected to IoT gateways, enabling local
processing of user commands and sensor data for reduced latency, improved
privacy, and enhanced inference quality. We validate the framework through a
smart home prototype using LLaMA 3 and Gemma 2B models for controlling smart
devices. Experimental results highlight the trade-offs between model accuracy
and inference time with respect to models size. At last, we also discuss the
potential applications that can use LLM-based IoT systems, and a few key
challenges associated with such systems.

</details>


### [23] [Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN](https://arxiv.org/abs/2507.17905)
*Mahbubur Rahman*

Main category: cs.NI

TL;DR: 该论文提出了一种改进的LPWAN技术SNOW，通过分布式正交频分复用（D-OFDM）和伪随机噪声（PN）序列，实现了大规模异步传感器的并行数据传输，显著提升了可扩展性。


<details>
  <summary>Details</summary>
Motivation: LPWAN技术在大规模传感器网络中面临数据传输效率低和延迟高的问题，尤其是在物联网（IoT）和信息物理系统（CPS）应用中。

Method: 通过利用D-OFDM子载波和Gold码生成的PN序列，实现了基站（BS）在同一子载波上并行解码多个异步传感器的数据，同时传感器也能在同一子载波上接收不同数据。

Result: 实验结果表明，SNOW技术可以将可扩展性提升约9倍，同时保持数据收集的及时性和传感器的能效。

Conclusion: 改进后的SNOW技术能够支持数万个传感器的网络，为电池寿命更长的时间敏感型IoT和CPS应用提供了可能。

Abstract: LPWANs have become ubiquitous due to their ability to connect sensors over
large geographic areas in a single hop. It is, however, very challenging to
achieve massive scalability in LPWANs, where numerous sensors can transmit data
efficiently and with low latency, which emerging IoT and CPS applications may
require. In this paper, we address the above challenges by significantly
advancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal
frequency division multiplexing, D-OFDM, subcarriers to enable parallel
reception of data to a BS from multiple asynchronous sensors, each using a
different subcarrier. In this paper, we achieve massive scalability in SNOW by
enabling the BS to decode concurrent data from numerous asynchronous sensors on
the same subcarrier while parallelly decoding from other subcarriers as well.
Additionally, we enable numerous asynchronous sensors to receive distinct data
from the BS on the same subcarrier while other sensors also receive data
parallelly on other subcarriers. To do this, we develop a set of Gold
code-based pseudorandom noise or PN sequences that are mutually non-interfering
within and across the subcarriers. Each sensor uses its PN sequence from the
set for encoding or decoding data on its subcarriers, enabling massive
concurrency. Our evaluation results demonstrate that we can achieve
approximately 9x more scalability in SNOW while being timely in data collection
at the BS and energy efficient at the sensors. This may enable emerging IoT and
CPS applications requiring tens of thousands of sensors with longer battery
life and making data-driven, time-sensitive decisions.

</details>


### [24] [Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of Information Optimization in Vehicular Networks](https://arxiv.org/abs/2507.18328)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 研究5G NR V2I Mode 2下的车辆网络公平访问和信息年龄（AoI）问题，提出一种联合优化框架，通过调整SPS选择窗口和LLM-based MOEA/D算法，平衡公平性和数据新鲜度。


<details>
  <summary>Details</summary>
Motivation: 车辆在不同车道速度差异导致RSU驻留时间和通信时长不同，引发网络资源访问不公平，影响驾驶安全，同时需保证数据新鲜度。

Method: 提出联合优化框架，定义公平性指标，使用SHS建模AoI，自适应调整SPS选择窗口，采用LLM-based MOEA/D算法优化。

Result: 仿真结果表明该方案能有效平衡公平访问和最小化AoI。

Conclusion: 该框架解决了Mode 2下公平性和AoI的联合优化问题，为车辆网络提供了及时且相关的数据交付方案。

Abstract: In this paper, we consider the fair access problem and the Age of Information
(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in
vehicular networks. Specifically, vehicles follow Mode 2 to communicate with
Roadside Units (RSUs) to obtain accurate data for driving
assistance.Nevertheless, vehicles often have different velocity when they are
moving in adjacent lanes, leading to difference in RSU dwelltime and
communication duration. This results in unfair access to network resources,
potentially influencing driving safety. To ensure the freshness of received
data, the AoI should be analyzed. Mode 2 introduces a novel preemption
mechanism, necessitating simultaneous optimization of fair access and AoI to
guarantee timely and relevant data delivery. We propose a joint optimization
framework for vehicular network, defining a fairness index and employing
Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By
adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)
in Mode 2, we address the optimization of fairness and AoI. We apply a large
language model (LLM)-Based Multi-objective Evolutionary Algorithm Based on
Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate
the effectiveness of our scheme in balancing fair access and minimizing AoI.

</details>


### [25] [Improving Wi-Fi 8 Latency with Coordinated Spatial Reuse](https://arxiv.org/abs/2507.18480)
*David Nunez,Francesc Wilhelmi,Lorenzo Galati-Giordano,Giovanni Geraci,Boris Bellalta*

Main category: cs.NI

TL;DR: 论文研究了协调空间重用（Co-SR）在Wi-Fi 8网络中的性能表现，提出了一种与标准一致的实现方法，结果显示Co-SR能显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: 应对新兴应用（如云游戏、XR、视频流）对高吞吐、低延迟、高可靠性的需求，优化频谱资源利用率。

Method: 提出与Wi-Fi 8标准化一致的Co-SR实现，并通过Wi-Fi模拟器在四AP的WLAN场景中评估性能。

Result: 与DCF相比，Co-SR的延迟降低了31%至95%。

Conclusion: Co-SR在Wi-Fi 8网络中能有效提升频谱效率，显著改善网络性能。

Abstract: IEEE 802.11 networks continuously adapt to meet the stringent requirements of
emerging applications like cloud gaming, eXtended Reality (XR), and video
streaming services, which require high throughput, low latency, and high
reliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can
potentially contribute to optimizing spectrum resource utilization. This
mechanism is expected to enable simultaneous transmissions, thereby boosting
spectral efficiency in dense environments and increasing the overall network
performance. In this paper, we shed light on the performance of Co-SR for Wi-Fi
8 networks. For that, we propose an implementation of Co-SR aligned with
ongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi
simulator, which allows us to study the performance of the proposed Co-SR
mechanisms in relevant scenarios. The results obtained in a Wireless Local Area
Network (WLAN) consisting of four APs show delay reduction with Co-SR ranging
from 31% to 95% when compared to Distributed Coordination Function (DCF).

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [26] [Program Logics via Distributive Monoidal Categories](https://arxiv.org/abs/2507.18238)
*Filippo Bonchi,Elena Di Lavore,Mario Román,Sam Staton*

Main category: cs.LO

TL;DR: 从范畴论的角度推导出多种程序逻辑，包括正确性、不正确性和关系Hoare逻辑，并基于命令式多元范畴的内部语言设计了Dijkstra守护命令语言的组合子。


<details>
  <summary>Details</summary>
Motivation: 通过统一的范畴论框架，将多种程序逻辑的推导标准化，以提高逻辑的一致性和可扩展性。

Method: 基于命令式范畴的公理（均匀可追踪的分布式复制-丢弃范畴），引入内部语言并设计程序逻辑的组合子。

Result: 成功推导出多种程序逻辑的规则，并实现了对Dijkstra守护命令语言的适配。

Conclusion: 证明了范畴论框架在程序逻辑推导中的普适性和实用性。

Abstract: We derive multiple program logics, including correctness, incorrectness, and
relational Hoare logic, from the axioms of imperative categories: uniformly
traced distributive copy-discard categories. We introduce an internal language
for imperative multicategories, on top of which we derive combinators for an
adaptation of Dijkstra's guarded command language. Rules of program logics are
derived from this internal language.

</details>


### [27] [Resourceful Traces for Commuting Processes](https://arxiv.org/abs/2507.18246)
*Matthew Earnshaw,Chad Nester,Mario Román*

Main category: cs.LO

TL;DR: 该论文提出了一种新的效果范畴（effectful categories）的表示方法，通过考虑Mazurkiewicz迹的动作作为输入到输出的转换，而不仅仅是原子名称。


<details>
  <summary>Details</summary>
Motivation: 旨在为效果范畴（广义Freyd范畴）提供一种新的表示方法，这类结构在副作用计算的语义中非常重要。

Method: 将Mazurkiewicz迹的动作视为输入到输出的转换，并基于此提出了一种图形化演算方法。

Result: 利用这种表示方法构造了自由效果范畴的交换张量积，支持系统的组合与资源共享。

Conclusion: 该方法为效果范畴提供了一种新颖且实用的表示工具，扩展了其应用场景。

Abstract: We show that, when the actions of a Mazurkiewicz trace are considered not
merely as atomic (i.e., mere names) but transformations from a specified type
of inputs to a specified type of outputs, we obtain a novel notion of
presentation for effectful categories (also known as generalised Freyd
categories), a well-known algebraic structure in the semantics of
side-effecting computation. Like the usual representation of traces as graphs,
our notion of presentation gives rise to a graphical calculus for effectful
categories. We use our presentations to give a construction of the commuting
tensor product of free effectful categories, capturing the combination of
systems in which the actions of each must commute with one another, while still
permitting exchange of resources

</details>


### [28] [Distributing Retractions, Weak Distributive Laws and Applications to Monads of Hyperspaces, Continuous Valuations and Measures](https://arxiv.org/abs/2507.18418)
*Jean Goubault-Larrecq*

Main category: cs.LO

TL;DR: 这篇论文探讨了在两个单子S和T之间存在弱分配律的情况下，如何构建一个联合单子U，并提出了通过分配回缩的方法来验证U的正确性。


<details>
  <summary>Details</summary>
Motivation: 研究目的是为了明确在给定条件下构建联合单子U的方法，并通过分配回缩的理论验证其正确性，尤其是在已知U的情况下。

Method: 提出了分配回缩的概念，展示了其与弱分配律之间的一一对应关系，并通过三个应用案例验证了方法的有效性。

Result: 证明了分配回缩与弱分配律在2-范畴设置下的一一对应性，并成功应用于不同单子的联合构建中。

Conclusion: 研究表明，分配回缩是验证联合单子U的有效工具，并且在具体应用中展示了其广泛适用性和实用性。

Abstract: Given two monads $S$, $T$ on a category where idempotents split, and a weak
distributive law between them, one can build a combined monad $U$. Making
explicit what this monad $U$ is requires some effort. When we already have an
idea what $U$ should be, we show how to recognize that $U$ is indeed the
combined monad obtained from $S$ and $T$: it suffices to exhibit what we call a
distributing retraction of $ST$ onto $U$. We show that distributing retractions
and weak distributive laws are in one-to-one correspondence, in a 2-categorical
setting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin
hyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad
of previsions or of forks, depending on the case. As a byproduct, this allows
us to describe the algebras of monads of superlinear, resp. sublinear
previsions. In the category of compact Hausdorff spaces, the Plotkin hyperspace
monad is sometimes known as the Vietoris monad, the monad of probability
valuations coincides with the Radon monad, and we infer that the associated
combined monad is the monad of normalized forks.

</details>


### [29] [Well-Founded Coalgebras Meet König's Lemma](https://arxiv.org/abs/2507.18539)
*Henning Urbat,Thorsten Wißmann*

Main category: cs.LO

TL;DR: 论文提出了一种关于König引理的共代数版本，将其从有限分支树推广到有限内函子的共代数，并展示了其在拓扑图、名义和凸转换系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 扩展König引理的适用范围，从传统的有限分支树到更一般的共代数框架，以增强其在数学和计算机科学中的实用性。

Method: 通过将König引理推广到有限内函子的共代数，并在局部有限可表示范畴（如偏序集、名义集或凸集）中进行分析。

Result: 证明了在温和条件下，所有良基共代数都是其有限生成状态的良基子共代数的定向并，且良基共代数范畴是局部可表示的。

Conclusion: 论文不仅扩展了König引理的应用范围，还提供了关于初始代数和最终递归共代数的新构造和简洁证明。

Abstract: K\"onig's lemma is a fundamental result about trees with countless
applications in mathematics and computer science. In contrapositive form, it
states that if a tree is finitely branching and well-founded (i.e. has no
infinite paths), then it is finite. We present a coalgebraic version of
K\"onig's lemma featuring two dimensions of generalization: from finitely
branching trees to coalgebras for a finitary endofunctor H, and from the base
category of sets to a locally finitely presentable category C, such as the
category of posets, nominal sets, or convex sets. Our coalgebraic K\"onig's
lemma states that, under mild assumptions on C and H, every well-founded
coalgebra for H is the directed join of its well-founded subcoalgebras with
finitely generated state space -- in particular, the category of well-founded
coalgebras is locally presentable. As applications, we derive versions of
K\"onig's lemma for graphs in a topos as well as for nominal and convex
transition systems. Additionally, we show that the key construction underlying
the proof gives rise to two simple constructions of the initial algebra
(equivalently, the final recursive coalgebra) for the functor H: The initial
algebra is both the colimit of all well-founded and of all recursive coalgebras
with finitely presentable state space. Remarkably, this result holds even in
settings where well-founded coalgebras form a proper subclass of recursive
ones. The first construction of the initial algebra is entirely new, while for
the second one our approach yields a short and transparent new correctness
proof.

</details>


### [30] [Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications](https://arxiv.org/abs/2507.18567)
*Ruben Gamboa,Panagiotis Manolios*

Main category: cs.LO

TL;DR: ACL2工作坊是ACL2定理证明系统用户展示相关研究和技术的主要论坛，ACL2是一个强大的自动推理系统。


<details>
  <summary>Details</summary>
Motivation: 促进ACL2定理证明系统的研究和应用交流。

Method: 通过ACL2工作坊的形式，汇集用户和开发者分享研究成果。

Result: ACL2因其在定理证明领域的贡献，2005年获得ACM软件系统奖。

Conclusion: ACL2工作坊和系统在推动定理证明技术发展中具有重要意义。

Abstract: The ACL2 Workshop series is the major technical forum for users of the ACL2
theorem proving system to present research related to the ACL2 theorem prover
and its applications. ACL2 is an industrial-strength automated reasoning
system, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM
Software System Award was awarded to Boyer, Kaufmann, and Moore for their work
on ACL2 and the other theorem provers in the Boyer-Moore family.

</details>


### [31] [Approximate SMT Counting Beyond Discrete Domains](https://arxiv.org/abs/2507.18612)
*Arijit Shaw,Kuldeep S. Meel*

Main category: cs.LO

TL;DR: 论文提出了pact，一种用于混合公式的SMT模型计数器，通过哈希近似计数方法高效估计解的数量，性能显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如位爆破）仅适用于离散变量，而混合公式的解决方案计数具有挑战性，需要扩展SMT能力。

Method: pact利用基于哈希的近似模型计数方法，通过优化哈希函数和对投影变量的对数次SMT求解调用，实现高效计数。

Result: pact在14,202个实例中完成了603个，而基线仅完成了13个，表现出显著的性能提升。

Conclusion: pact为混合SMT公式的模型计数提供了高效且理论可靠的解决方案。

Abstract: Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,
solving complex formulas across discrete and continuous domains. Recent
progress in propositional model counting motivates extending SMT capabilities
toward model counting, especially for hybrid SMT formulas. Existing approaches,
like bit-blasting, are limited to discrete variables, highlighting the
challenge of counting solutions projected onto the discrete domain in hybrid
formulas.
  We introduce pact, an SMT model counter for hybrid formulas that uses
hashing-based approximate model counting to estimate solutions with theoretical
guarantees. pact makes a logarithmic number of SMT solver calls relative to the
projection variables, leveraging optimized hash functions. pact achieves
significant performance improvements over baselines on a large suite of
benchmarks. In particular, out of 14,202 instances, pact successfully finished
on 603 instances, while Baseline could only finish on 13 instances.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [32] [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)
*Liang Zhang,Xiaoming Zhai,Jionghao Lin,Jionghao Lin,Jennifer Kleiman,Diego Zapata-Rivera,Carol Forsyth,Yang Jiang,Xiangen Hu,Arthur C. Graesser*

Main category: cs.HC

TL;DR: 本文研究了LLM代理在教育中的不同沟通策略对数学问题解决的影响，发现双代理协作优于单代理，其中点对点协作效果最佳。


<details>
  <summary>Details</summary>
Motivation: 评估沟通策略对LLM代理问题解决的影响，以优化AI教育中的协作效率。

Method: 在双代理聊天环境中测试四种沟通模式（师生互动、点对点协作、互惠同伴教学和批判性辩论），使用GPT-4o模型和MATH数据集。

Result: 双代理优于单代理，点对点协作准确率最高；陈述、确认和提示等对话行为是关键。

Conclusion: 有效沟通策略对解决AI教育中的复杂问题至关重要。

Abstract: Large Language Model (LLM) agents are increasingly utilized in AI-aided
education to support tutoring and learning. Effective communication strategies
among LLM agents improve collaborative problem-solving efficiency and
facilitate cost-effective adoption in education. However, little research has
systematically evaluated the impact of different communication strategies on
agents' problem-solving. Our study examines four communication modes,
\textit{teacher-student interaction}, \textit{peer-to-peer collaboration},
\textit{reciprocal peer teaching}, and \textit{critical debate}, in a
dual-agent, chat-based mathematical problem-solving environment using the
OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that
dual-agent setups outperform single agents, with \textit{peer-to-peer
collaboration} achieving the highest accuracy. Dialogue acts like statements,
acknowledgment, and hints play a key role in collaborative problem-solving.
While multi-agent frameworks enhance computational tasks, effective
communication strategies are essential for tackling complex problems in AI
education.

</details>


### [33] [A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians](https://arxiv.org/abs/2507.17754)
*Justin Morse,Kurt Gilbert,Kyle Shin,Rick Cooke,Peyton Rose,Jack Sullivan,Angelo Sisante*

Main category: cs.HC

TL;DR: 本文介绍了一种集成在医疗系统内的环境记录应用，利用Whisper和GPT-4o自动生成SOAP笔记和患者指导，显著减轻了医生负担。


<details>
  <summary>Details</summary>
Motivation: 医生职业倦怠问题促使了环境医疗记录工具的推广，旨在减少临床工作负担。

Method: 采用Whisper进行转录，结合模块化上下文学习管道（GPT-4o）生成SOAP笔记，并利用微调BART模型优化内容简洁性。

Result: 生成的笔记质量超过专家手写笔记，540名医生使用过该工具，94%的受访医生表示减轻了认知负担。

Conclusion: AI系统在减轻行政负担和支持高效护理方面具有巨大潜力。

Abstract: Clinician burnout has motivated the growing adoption of ambient medical
scribes in the clinic. In this work, we introduce a custom-built ambient scribe
application integrated into the EHR system at Included Health, a personalized
all-in-one healthcare company offering telehealth services. The application
uses Whisper for transcription and a modular in-context learning pipeline with
GPT-4o to automatically generate SOAP notes and patient instructions. Testing
on mock visit data shows that the notes generated by the application exceed the
quality of expert-written notes as determined by an LLM-as-a-judge. The
application has been widely adopted by the clinical practice, with over 540
clinicians at Included Health using the application at least once. 94% (n = 63)
of surveyed clinicians report reduced cognitive load during visits and 97% (n =
66) report less documentation burden when using the application. Additionally,
we show that post-processing notes with a fine-tuned BART model improves
conciseness. These findings highlight the potential for AI systems to ease
administrative burdens and support clinicians in delivering efficient,
high-quality care.

</details>


### [34] [Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image](https://arxiv.org/abs/2507.17755)
*Jianfeng Lan,Yingjia Huang*

Main category: cs.HC

TL;DR: 研究探讨了抖音和微信对中国男性青少年身体形象的影响，发现抖音使用与外貌评价和身体满意度显著相关，而微信无显著关联。


<details>
  <summary>Details</summary>
Motivation: 探讨不同社交媒体平台（抖音和微信）对中国男性青少年身体形象的具体影响。

Method: 对395名10至24岁男性青少年进行问卷调查，使用MBSRQ-AS评估身体形象维度。

Result: 抖音使用与外貌评价和身体满意度显著相关，微信无显著影响。抖音的视频算法可能强化理想化身体标准的认知影响。

Conclusion: 研究强调了平台特性对社交媒体影响的重要性，并为中国男性青少年的身体形象问题提供了解决思路。

Abstract: In the digital era, social media platforms play a pivotal role in shaping
adolescents' body image perceptions. This study examines how Douyin and WeChat,
two contrasting Chinese social media platforms, influence body image among
Chinese male adolescents. Employing a platformization perspective, we surveyed
395 male adolescents aged 10 to 24 using the Multidimensional Body-Self
Relations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation
and body satisfaction. Our findings reveal that Douyin usage is significantly
correlated with appearance evaluation and body area satisfaction, while WeChat
usage shows no significant correlation with any body image dimensions. These
results suggest that Douyin's algorithm-driven, video-centric environment
intensifies exposure to idealized body standards, impacting users at a
cognitive level. This study underscores the importance of considering
platform-specific characteristics in understanding social media's impact on
body image. It contributes to the broader discourse on how technological design
and content modalities mediate psychological outcomes, offering insights for
addressing body image concerns among male adolescents in China.

</details>


### [35] [Effects of variation in system responsiveness on user performance in virtual environments](https://arxiv.org/abs/2507.18085)
*Benjamin Watson,Neff Walker,William Ribarsky,Victoria Spaulding*

Main category: cs.HC

TL;DR: 论文研究了虚拟环境中的系统响应性（SR），探讨了其对抓取和放置任务性能的影响，发现标准偏差（SDSR）只有在超过82毫秒时才会影响性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解系统响应性（SR）及其波动对虚拟环境中任务性能的影响，为设计优化提供依据。

Method: 采用了三项研究，分别使用11、12和10名参与者，通过组内设计测量MSR和SDSR对抓取和放置任务的影响。

Result: 结果表明，SDSR仅在超过82毫秒时影响性能，放置任务对SR更敏感。

Conclusion: 虚拟环境设计者无需严格控制SDSR，可根据视觉反馈需求调整SR控制，从而优化人机交互体验。

Abstract: System responsiveness (SR) is defined as the elapsed time until a system
responds to user control. SR fluctuates over time, so it must be described
statistically with mean (MSR) and standard deviation (SDSR). In this paper, we
examine SR in virtual environments (VEs), outlining its components and methods
of experimental measurement and manipulation. Three studies of MSR and SDSR
effects on performance of grasp and placement tasks are then presented. The
studies used within-subjects designs with 11, 12, and 10 participants,
respectively. Results showed that SDSR affected performance only if it was
above 82 ms. Placement required more frequent visual feedback and was more
sensitive to SR. We infer that VE designers need not tightly control SDSR and
may wish to vary SR control based on required visual feedback frequency. These
results may be used to improve the human-computer interface in a wide range of
interactive graphical applications, including scientific visualization,
training, mental health, and entertainment.

</details>


### [36] [Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy](https://arxiv.org/abs/2507.17756)
*Josh Hunter,John McDermid,Simon Burton*

Main category: cs.HC

TL;DR: 本研究探讨铁路专业人士对安全概念的看法，旨在为未来行业技术发展提供参考。研究发现对自动化的谨慎态度、偏好辅助技术，以及安全的多维度理解。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解铁路行业对安全的认知，以指导未来技术发展，填补研究与实际应用的差距。

Method: 通过对驾驶员、路线规划者和管理人员的访谈，分析当前安全实践、自动化潜力及铁路系统的复杂性。

Result: 发现对自动化的谨慎态度、偏爱辅助技术，及安全需综合人、系统和技术的多维度模型。

Conclusion: 研究强调需铁路专用因果关系模型，以提升安全评估效果，并推动更有效的安全指标开发。

Abstract: This study investigates how railway professionals perceive safety as a
concept within rail, with the intention to help inform future technological
developments within the industry. Through a series of interviews with drivers,
route planners,and administrative personnel, the research explores the
currentstate of safety practices, the potential for automation and the
understanding of the railway as a system of systems. Key findings highlight a
cautious attitude towards automation, a preference for assistive technologies,
and a complex understanding of safety that integrates human, systematic and
technological factors. The study also addresses the limitations of transferring
automotive automation technologies to railways and the need for a
railway-specific causation model to better evaluate and enhance safety in an
evolving technological landscape. This study aims to bridge thegap between
contemporary research and practical applications, contributing to the
development of more effective safety metrics.

</details>


### [37] [BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches](https://arxiv.org/abs/2507.17757)
*Sam Gordon James,Miranda Elaine Glynis Armstrong,Aisling Ann O'Kane,Harry Emerson,Zahraa S. Abdallah*

Main category: cs.HC

TL;DR: 论文介绍了BrisT1D数据集，包含1型糖尿病管理设备和智能手表数据，以及访谈和焦点小组记录，支持定量和定性研究。


<details>
  <summary>Details</summary>
Motivation: 探索1型糖尿病管理技术的实际应用及额外数据流的潜力，支持未来慢性病管理技术的发展。

Method: 通过纵向研究收集24名英国年轻成年人的设备数据和访谈记录，提供处理后和原始数据。

Result: 数据集支持血糖预测、低血糖预测、闭环算法开发，以及用户体验和智能手表在糖尿病管理中的角色研究。

Conclusion: BrisT1D数据集为1型糖尿病管理的多方面研究提供了实用资源。

Abstract: Background: Type 1 diabetes (T1D) has seen a rapid evolution in management
technology and forms a useful case study for the future management of other
chronic conditions. Further development of this management technology requires
an exploration of its real-world use and the potential of additional data
streams. To facilitate this, we contribute the BrisT1D Dataset to the growing
number of public T1D management datasets. The dataset was developed from a
longitudinal study of 24 young adults in the UK who used a smartwatch alongside
their usual T1D management. Findings: The BrisT1D dataset features both device
data from the T1D management systems and smartwatches used by participants, as
well as transcripts of monthly interviews and focus groups conducted during the
study. The device data is provided in a processed state, for usability and more
rapid analysis, and in a raw state, for in-depth exploration of novel insights
captured in the study. Conclusions: This dataset has a range of potential
applications. The quantitative elements can support blood glucose prediction,
hypoglycaemia prediction, and closed-loop algorithm development. The
qualitative elements enable the exploration of user experiences and opinions,
as well as broader mixed-methods research into the role of smartwatches in T1D
management.

</details>


### [38] [DHMS: A Digital Hostel Management System Integrating Campus ChatBot, Predictive Intelligence, and Real-Time Automation](https://arxiv.org/abs/2507.17759)
*Riddhi Heda,Sidhant Singh,Umair Yasir,Tanmay Jaiswal,Anil Mokhade*

Main category: cs.HC

TL;DR: 传统学术机构的宿舍管理存在效率低下、沟通不畅的问题。DHMS（数字宿舍管理系统）通过现代技术优化管理功能，测试中表现良好，但需进一步测试。


<details>
  <summary>Details</summary>
Motivation: 解决传统宿舍管理系统的效率低下和沟通不畅问题，满足数字化时代学生的需求。

Method: 开发基于现代网络技术、人工智能和云计算的DHMS系统，自动化宿舍分配、申诉处理和沟通等功能。

Result: 模拟测试中，DHMS在宿舍分配上获得92%的学生满意度，聊天机器人响应时间低于1秒。

Conclusion: DHMS展现了良好的潜力，但仍需进一步测试以验证其可扩展性和兼容性。

Abstract: Traditional hostel management practices in academic institutions often suffer
from inefficiencies, delays, and fragmented communication. These systems fail
to meet the expectations of digitally native students and place a significant
operational burden on hostel staff. This paper introduces DHMS (Digital Hostel
Management System), a modular and integrated platform designed to digitize and
streamline essential hostel management functions. DHMS leverages modern web
technologies, artificial intelligence, and cloud infrastructure to automate
room allotment, grievance redressal, gate pass logistics, and communication via
a natural language chatbot. In simulation tests, DHMS achieved a 92% student
satisfaction rate in room allocation and maintained an average chatbot response
time below one second. Additional features include predictive analytics for
proactive maintenance planning and sentiment analysis for feedback processing.
While promising, the system requires further testing for integration across
multiple hostel blocks, user acceptance, scalability under load, and ERP
compatibility before campus-wide deployment. This work discusses the system
architecture, implementation approach, and factors critical to improving user
experience, administrative efficiency, and decision-making processes.

</details>


### [39] [Co-constructing Explanations for AI Systems using Provenance](https://arxiv.org/abs/2507.17761)
*Jan-Christoph Kalo,Fina Polat,Shubha Guha,Paul Groth*

Main category: cs.HC

TL;DR: 提出了一种交互式代理，与用户共同构建基于数据来源的解释，并展示了原型和评估框架。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统复杂，数据来源的解释过于详细且缺乏用户上下文，需要更实用的解释方法。

Method: 开发了一种交互式代理，结合用户需求和数据来源，提供了原型和大语言模型评估框架。

Result: 展示了初步原型和可扩展的评估方法，表明代理能有效帮助用户理解AI系统。

Conclusion: 交互式代理能帮助用户更直观地理解AI系统的数据来源，未来可扩展优化。

Abstract: Modern AI systems are complex workflows containing multiple components and
data sources. Data provenance provides the ability to interrogate and
potentially explain the outputs of these systems. However, provenance is often
too detailed and not contextualized for the user trying to understand the AI
system. In this work, we present our vision for an interactive agent that works
together with the user to co-construct an explanation that is simultaneously
useful to the user as well as grounded in data provenance. To illustrate this
vision, we present: 1) an initial prototype of such an agent; and 2) a scalable
evaluation framework based on user simulations and a large language model as a
judge approach.

</details>


### [40] [Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems](https://arxiv.org/abs/2507.17774)
*Zhangqi Liu*

Main category: cs.HC

TL;DR: 该论文探讨了AI从后台工具转变为设计协作伙伴后，如何革新传统设计工作流，重点关注人类与AI的共创模式。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展为生成式协作工具，需要重新思考其在早期设计阶段的角色，超越传统的自动化和效率提升。

Method: 研究利用大型语言模型（如GPT-4）和多模态扩散模型（如Stable Diffusion）作为创意代理，支持设计师进行提案、批判和修订的迭代循环。

Result: AI不仅是工具，还能作为创意伙伴参与设计过程，推动人类与AI在视觉概念化和决策中的深度协作。

Conclusion: 人类-AI共创模式为设计流程带来新范式，强调AI在创意过程中的主动性和协同能力。

Abstract: As artificial intelligence (AI) continues to evolve from a back-end
computational tool into an interactive, generative collaborator, its
integration into early-stage design processes demands a rethinking of
traditional workflows in human-centered design. This paper explores the
emergent paradigm of human-AI co-creation, where AI is not merely used for
automation or efficiency gains, but actively participates in ideation, visual
conceptualization, and decision-making. Specifically, we investigate the use of
large language models (LLMs) like GPT-4 and multimodal diffusion models such as
Stable Diffusion as creative agents that engage designers in iterative cycles
of proposal, critique, and revision.

</details>


### [41] [Same Data, Different Audiences: Using Personas to Scope a Supercomputing Job Queue Visualization](https://arxiv.org/abs/2507.17898)
*Connor Scully-Allison,Kevin Menear,Kristin Potter,Andrew McNutt,Katherine E. Isaacs,Dmitry Duplyakin*

Main category: cs.HC

TL;DR: 研究通过设计可视化工具Guidepost，支持多用户群体使用超级计算机队列数据，实现了共享任务的交互式支持和特定任务的编程分析。


<details>
  <summary>Details</summary>
Motivation: 探索可视化工具如何支持多用户群体，避免设计局限，提升整体效用。

Method: 采用人物角色分类任务，结合笔记本嵌入式可视化设计，开发Guidepost工具。

Result: 工具成功支持三类用户群体，共享任务通过交互完成，特定任务则通过编程分析实现。

Conclusion: 通过分类任务和交互与编程结合的设计，可视化工具可有效服务多用户群体。

Abstract: Domain-specific visualizations sometimes focus on narrow, albeit important,
tasks for one group of users. This focus limits the utility of a visualization
to other groups working with the same data. While tasks elicited from other
groups can present a design pitfall if not disambiguated, they also present a
design opportunity -- development of visualizations that support multiple
groups. This development choice presents a trade off of broadening the scope
but limiting support for the more narrow tasks of any one group, which in some
cases can enhance the overall utility of the visualization. We investigate this
scenario through a design study where we develop \textit{Guidepost}, a
notebook-embedded visualization of supercomputer queue data that helps
scientists assess supercomputer queue wait times, machine learning researchers
understand prediction accuracy, and system maintainers analyze usage trends. We
adapt the use of personas for visualization design from existing literature in
the HCI and software engineering domains and apply them in categorizing tasks
based on their uniqueness across the stakeholder personas. Under this model,
tasks shared between all groups should be supported by interactive
visualizations and tasks unique to each group can be deferred to scripting with
notebook-embedded visualization design. We evaluate our visualization with nine
expert analysts organized into two groups: a "research analyst" group that uses
supercomputer queue data in their research (representing the Machine Learning
researchers and Jobs Data Analyst personas) and a "supercomputer user" group
that uses this data conditionally (representing the HPC User persona). We find
that our visualization serves our three stakeholder groups by enabling users to
successfully execute shared tasks with point-and-click interaction while
facilitating case-specific programmatic analysis workflows.

</details>


### [42] [Automated Brake Onset Detection in Naturalistic Driving Data](https://arxiv.org/abs/2507.17943)
*Shu-Yuan Liu,Johan Engström,Gustav Markkula*

Main category: cs.HC

TL;DR: 本文提出了一种基于分段线性加速度模型的高效算法，用于自动估计刹车起始时间，适用于无车辆控制信号的大规模数据分析，并通过手动标注进行验证。


<details>
  <summary>Details</summary>
Motivation: 在缺乏车辆控制信号的大规模数据（如自动驾驶系统日志数据）中，现有的刹车起始时间测量方法（如手动标注或依赖车辆控制信号）无法使用，因此需要一种通用且高效的新方法。

Method: 基于分段线性加速度模型开发了一个简单高效的算法，适用于任何包含车辆纵向时间序列数据的驾驶数据，并提出了一种手动标注方法作为验证基准。

Result: 使用R2作为置信度量，算法在自然避撞数据（包括自动驾驶系统和人类驾驶员）中表现出较高的准确性，并验证了其与手动标注的一致性。

Conclusion: 该算法高效、通用性强，适用于所有道路用户和场景类型，且高度可配置，尽管存在一定局限性。

Abstract: Response timing measures play a crucial role in the assessment of automated
driving systems (ADS) in collision avoidance scenarios, including but not
limited to establishing human benchmarks and comparing ADS to human driver
response performance. For example, measuring the response time (of a human
driver or ADS) to a conflict requires the determination of a stimulus onset and
a response onset. In existing studies, response onset relies on manual
annotation or vehicle control signals such as accelerator and brake pedal
movements. These methods are not applicable when analyzing large scale data
where vehicle control signals are not available. This holds in particular for
the rapidly expanding sets of ADS log data where the behavior of surrounding
road users is observed via onboard sensors. To advance evaluation techniques
for ADS and enable measuring response timing when vehicle control signals are
not available, we developed a simple and efficient algorithm, based on a
piecewise linear acceleration model, to automatically estimate brake onset that
can be applied to any type of driving data that includes vehicle longitudinal
time series data. We also proposed a manual annotation method to identify brake
onset and used it as ground truth for validation. R2 was used as a confidence
metric to measure the accuracy of the algorithm, and its classification
performance was analyzed using naturalistic collision avoidance data of both
ADS and humans, where our method was validated against human manual annotation.
Although our algorithm is subject to certain limitations, it is efficient,
generalizable, applicable to any road user and scenario types, and is highly
configurable.

</details>


### [43] [Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale](https://arxiv.org/abs/2507.17985)
*Alex Liu,Lief Esbenshade,Shawon Sarkar,Victor Tian,Zachary Zhang,Kevin He,Min Sun*

Main category: cs.HC

TL;DR: 论文提出了一种结合人类与AI的方法，用于大规模分析14万条教师与AI的互动消息，发现大语言模型（如Claude 3.5 Haiku）能有效支持主题识别和编码任务，并揭示了教师利用AI增强教学的具体模式。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补教师如何实际使用AI工具的空白，并提供一种大规模定性分析的方法，以揭示AI在教育中的实际应用和潜在影响。

Method: 采用四阶段编码流程，包括归纳主题发现、代码本开发、结构化标注和模型基准测试，分析了14万条教师与AI的互动消息。

Result: 发现LLMs（尤其是Claude 3.5 Haiku）在主题识别和编码任务中表现优异，并揭示了教师主要利用AI增强教学实践（79.7%）、内容创作（76.1%）和个性化教学（43.3%）等主要模式。

Conclusion: 研究提供了AI增强定性研究的可扩展模型，并为生成式AI在教育中的角色演变提供了基础见解，对教师培训和专业发展具有直接意义。

Abstract: The integration of large language models (LLMs) into educational tools has
the potential to substantially impact how teachers plan instruction, support
diverse learners, and engage in professional reflection. Yet little is known
about how educators actually use these tools in practice and how their
interactions with AI can be meaningfully studied at scale. This paper presents
a human-AI collaborative methodology for large-scale qualitative analysis of
over 140,000 educator-AI messages drawn from a generative AI platform used by
K-12 teachers. Through a four-phase coding pipeline, we combined inductive
theme discovery, codebook development, structured annotation, and model
benchmarking to examine patterns of educator engagement and evaluate the
performance of LLMs in qualitative coding tasks. We developed a hierarchical
codebook aligned with established teacher evaluation frameworks, capturing
educators' instructional goals, contextual needs, and pedagogical strategies.
Our findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably
support theme identification, extend human recognition in complex scenarios,
and outperform open-weight models in both accuracy and structural reliability.
The analysis also reveals substantive patterns in how educators inquire AI to
enhance instructional practices (79.7 percent of total conversations), create
or adapt content (76.1 percent), support assessment and feedback loop (46.9
percent), attend to student needs for tailored instruction (43.3 percent), and
assist other professional responsibilities (34.2 percent), highlighting
emerging AI-related competencies that have direct implications for teacher
preparation and professional development. This study offers a scalable,
transparent model for AI-augmented qualitative research and provides
foundational insights into the evolving role of generative AI in educational
practice.

</details>


### [44] [Evaluating judgment of spatial correlation in visual displays of scalar field distributions](https://arxiv.org/abs/2507.17997)
*Yayan Zhao,Matthew Berger*

Main category: cs.HC

TL;DR: 研究了2D标量场分布中空间相关性的识别，比较了动画显示和并排放置视图的效果，以及颜色尺度选择的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨人类是否能通过不同视觉显示方式识别标量场中的空间相关性，以及显示方式对判断的影响。

Method: 实验设计比较动画显示和并排放置视图，同时控制空间相关性和尺度可分辨性。

Result: 结果表明分布特性和视觉显示方式对空间相关性判断有显著影响。

Conclusion: 视觉显示方式的选择和分布特性共同影响人类对空间相关性的识别能力。

Abstract: In this work we study the identification of spatial correlation in
distributions of 2D scalar fields, presented across different forms of visual
displays. We study simple visual displays that directly show color-mapped
scalar fields, namely those drawn from a distribution, and whether humans can
identify strongly correlated spatial regions in these displays. In this
setting, the recognition of correlation requires making judgments on a set of
fields, rather than just one field. Thus, in our experimental design we compare
two basic visualization designs: animation-based displays against juxtaposed
views of scalar fields, along different choices of color scales. Moreover, we
investigate the impacts of the distribution itself, controlling for the level
of spatial correlation and discriminability in spatial scales. Our study's
results illustrate the impacts of these distribution characteristics, while
also highlighting how different visual displays impact the types of judgments
made in assessing spatial correlation. Supplemental material is available at
https://osf.io/zn4qy

</details>


### [45] ["I Would Not Be This Version of Myself Today": Elaborating on the Effects of Eudaimonic Gaming Experiences](https://arxiv.org/abs/2507.18084)
*Nisha Devasia,Georgia Kenderova,Michele Newman,Julie Kientz,Jin Ha Lee*

Main category: cs.HC

TL;DR: 该论文研究了数字游戏中幸福感体验（如个人意义和成长）的感知结果，并通过调查和混合方法分析了这些体验如何影响玩家的生活。


<details>
  <summary>Details</summary>
Motivation: 探索游戏中的幸福感体验（eudaimonic gaming）的效应，填补现有研究中关于这些体验如何影响个人积极成果的空白。

Method: 采用混合方法（n = 166的问卷调查），分析有意义游戏体验的分类及其形成的重要子成分。

Result: 研究发现，有意义的游戏体验可以带来反思、学习、社交、健康和职业方面的积极影响。

Conclusion: 扩展了幸福感游戏体验的理论模型，并为研究者和实践者提供了促进玩家积极成果的启示。

Abstract: While much of the research in digital games has emphasized hedonic
experiences, such as flow, enjoyment, and positive affect, recent years have
seen increased interest in eudaimonic gaming experiences, typically
mixed-affect and associated with personal meaningfulness and growth. The
formation of such experiences in games is theorized to have four constituent
elements: motivation, game use, experience, and effects. However, while the
first three elements have been relatively well explored in the literature, the
effects - and how they may influence positive individual outcomes - have been
underexplored thus far. To this end, in this work, we investigate the perceived
outcomes of eudaimonic gaming and how different components of the experience
influence these effects. We conducted a survey (n = 166) in which respondents
recounted meaningful gaming experiences and how they affected their present
lives. We used a mixed-methods approach to classify effects and identify
significant subcomponents of their formation. We contribute an empirical
understanding of how meaningful gaming experiences can lead to positive
reflective, learning, social, health, and career effects, extending current
theoretical models of eudaimonic gaming experiences and offering implications
for how researchers and practitioners might use these findings to promote
positive outcomes for players.

</details>


### [46] [Understood: Real-Time Communication Support for Adults with ADHD Using Mixed Reality](https://arxiv.org/abs/2507.18151)
*Shizhen Zhang,Shengxin Li,Quan Li*

Main category: cs.HC

TL;DR: 研究团队开发了一款名为Understood的混合现实系统，通过实时对话摘要、上下文感知的词汇建议和话题转移检测功能，帮助成人ADHD患者改善沟通障碍。


<details>
  <summary>Details</summary>
Motivation: 成人ADHD患者在沟通中常因执行功能障碍和情绪失调而遇到挑战，现有干预措施主要针对儿童，缺乏适用于成人的日常沟通辅助工具。

Method: 通过半结构化访谈和设计工坊确定沟通障碍，开发基于Microsoft HoloLens 2的混合现实系统，包含实时摘要、词汇建议和话题检测功能。

Result: 用户研究和专家访谈表明，Understood具有高可用性，能有效支持成人ADHD患者的沟通。

Conclusion: Understood为成人ADHD患者提供了一种有效的沟通辅助工具，可作为心理治疗的补充。

Abstract: Adults with Attention Deficit Hyperactivity Disorder (ADHD) often experience
communication challenges, primarily due to executive dysfunction and emotional
dysregulation, even after years of social integration. While existing
interventions predominantly target children through structured or intrusive
methods, adults lack tools that translate clinical strategies into daily
communication support. To address this gap, we present Understood, a Mixed
Reality (MR) system implemented on Microsoft HoloLens 2, designed to assist
adults with ADHD in real-world communication. Through formative semi-structured
interviews and a design workshop, we identified critical communication barriers
and derived design goals for the system. Understood combines three key
features: (1) real-time conversation summarization to reduce cognitive load,
(2) context-aware subsequent word suggestions during moments of disfluency, and
(3) topic shifting detection and reminding to mitigate off-topic transitions. A
within-subjects user study and expert interviews demonstrate that Understood
effectively supports communication with high usability, offering a complement
to therapist-mediated interventions.

</details>


### [47] [ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent](https://arxiv.org/abs/2507.18165)
*Yuheng Zhao,Xueli Shu,Liwen Fan,Lin Gao,Yu Zhang,Siming Chen*

Main category: cs.HC

TL;DR: 该论文提出了一个名为ProactiveVA的框架，通过LLM驱动的UI代理监控用户交互，主动提供上下文感知的辅助。


<details>
  <summary>Details</summary>
Motivation: 当前的VA系统仅在用户明确请求时提供帮助，缺乏主动建议的能力，而复杂数据分析中用户常需即时辅助。

Method: 通过用户交互日志分析帮助寻求行为，设计了三个阶段（感知、推理、行动）的UI代理流程。

Result: 在两类VA系统中实现了该框架，并通过算法评估和用户研究验证了其有效性。

Conclusion: ProactiveVA框架能主动感知用户需求并提供辅助，但还需进一步探索设计权衡。

Abstract: Visual analytics (VA) is typically applied to complex data, thus requiring
complex tools. While visual analytics empowers analysts in data analysis,
analysts may get lost in the complexity occasionally. This highlights the need
for intelligent assistance mechanisms. However, even the latest LLM-assisted VA
systems only provide help when explicitly requested by the user, making them
insufficiently intelligent to offer suggestions when analysts need them the
most. We propose a ProactiveVA framework in which LLM-powered UI agent monitors
user interactions and delivers context-aware assistance proactively. To design
effective proactive assistance, we first conducted a formative study analyzing
help-seeking behaviors in user interaction logs, identifying when users need
proactive help, what assistance they require, and how the agent should
intervene. Based on this analysis, we distilled key design requirements in
terms of intent recognition, solution generation, interpretability and
controllability. Guided by these requirements, we develop a three-stage UI
agent pipeline including perception, reasoning, and acting. The agent
autonomously perceives users' needs from VA interaction logs, providing
tailored suggestions and intuitive guidance through interactive exploration of
the system. We implemented the framework in two representative types of VA
systems, demonstrating its generalizability, and evaluated the effectiveness
through an algorithm evaluation, case and expert study and a user study. We
also discuss current design trade-offs of proactive VA and areas for further
exploration.

</details>


### [48] [Recommender systems, representativeness, and online music: A psychosocial analysis of Italian listeners](https://arxiv.org/abs/2507.18169)
*Lorenzo Porcaro,Chiara Monaldi*

Main category: cs.HC

TL;DR: 论文探讨了音乐推荐系统在文化和心理层面对用户的影响，通过意大利听众的访谈揭示了用户对推荐系统的理解不足及性别差异问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统广泛影响音乐收听，但用户对其代表性问题（如性别差异）的理解不足，缺乏文化和心理视角的研究。

Method: 研究者对意大利音乐听众进行访谈，并通过情感文本分析法分析其叙述，识别共享文化解读。

Result: 研究发现，用户对推荐系统的理解有限，性别差异问题未得到充分认识，呼吁加强算法意识和数字素养。

Conclusion: 需跨学科研究解决代表性问题，提升用户对推荐系统的信任度。

Abstract: Recommender systems shape music listening worldwide due to their widespread
adoption in online platforms. Growing concerns about representational harms
that these systems may cause are nowadays part of the scientific and public
debate, wherein music listener perspectives are oftentimes reported and
discussed from a cognitive-behaviorism perspective, but rarely contextualised
under a psychosocial and cultural lens. We proceed in this direction, by
interviewing a group of Italian music listeners and analysing their narratives
through Emotional Textual Analysis. Thanks to this, we identify shared cultural
repertoires that reveal people's complex relationship with listening practices:
even when familiar with online platforms, listeners may still lack a critical
understanding of recommender systems. Moreover, representational issues,
particularly gender disparities, seem not yet fully grasped in the context of
online music listening. This study underscores the need for interdisciplinary
research to address representational harms, and the role of algorithmic
awareness and digital literacy in developing trustworthy recommender systems.

</details>


### [49] [Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning](https://arxiv.org/abs/2507.18252)
*Dongyang Guo,Yasmeen Abdrabou,Enkeleda Thaqi,Enkelejda Kasneci*

Main category: cs.HC

TL;DR: 提出了一种多模态人机协作框架，用于从眼动追踪数据中提取认知模式，结合专家判断和LLM推理，提升了分析性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 眼动数据虽能揭示认知状态，但因其结构化和非语言特性难以分析。LLMs擅长文本推理，但对时间和数值数据处理较弱。

Method: 1）多阶段管道结合水平和垂直分割与LLM推理；2）专家模型共评分模块；3）基于LSTM和LLM的混合异常检测模块。

Result: 实验显示在多个LLMs和提示策略下，一致性、可解释性和性能提升，难度预测任务准确率最高达50%。

Conclusion: 该框架为认知建模提供了可扩展、可解释的方案，适用于自适应学习、人机交互和教育分析等领域。

Abstract: Eye-tracking data reveals valuable insights into users' cognitive states but
is difficult to analyze due to its structured, non-linguistic nature. While
large language models (LLMs) excel at reasoning over text, they struggle with
temporal and numerical data. This paper presents a multimodal human-AI
collaborative framework designed to enhance cognitive pattern extraction from
eye-tracking signals. The framework includes: (1) a multi-stage pipeline using
horizontal and vertical segmentation alongside LLM reasoning to uncover latent
gaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert
judgment with LLM output to generate trust scores for behavioral
interpretations; and (3) a hybrid anomaly detection module combining LSTM-based
temporal modeling with LLM-driven semantic analysis. Our results across several
LLMs and prompt strategies show improvements in consistency, interpretability,
and performance, with up to 50% accuracy in difficulty prediction tasks. This
approach offers a scalable, interpretable solution for cognitive modeling and
has broad potential in adaptive learning, human-computer interaction, and
educational analytics.

</details>


### [50] [Talking to...uh...um...Machines: The Impact of Disfluent Speech Agents on Partner Models and Perspective Taking](https://arxiv.org/abs/2507.18315)
*Rhys Jacka,Paola R. Peña,Sophie Leonard,Éva Székely,Benjamin R. Cowan*

Main category: cs.HC

TL;DR: 研究了对话中语言不流利对人类与机器对话的影响，结果显示不流利的语音代理被用户认为更胜任。


<details>
  <summary>Details</summary>
Motivation: 探索语言不流利在人际交流中的作用是否会延伸到人机对话中

Method: 使用Namer-Matcher任务，分为流利和不流利语音组，通过PMQ问卷评估

Result: 不流利代理被认为更胜任，但对话灵活性和人性化无差异；不流利代理可能增加自我中心交流

Conclusion: 语言不流利可能影响人机对话中的伙伴模型和语言生成，但效果尚未完全明确

Abstract: Speech disfluencies play a role in perspective-taking and audience design in
human-human communication (HHC), but little is known about their impact in
human-machine dialogue (HMD). In an online Namer-Matcher task, sixty-one
participants interacted with a speech agent using either fluent or disfluent
speech. Participants completed a partner-modelling questionnaire (PMQ) both
before and after the task. Post-interaction evaluations indicated that
participants perceived the disfluent agent as more competent, despite no
significant differences in pre-task ratings. However, no notable differences
were observed in assessments of conversational flexibility or human-likeness.
Our findings also reveal evidence of egocentric and allocentric language
production when participants interact with speech agents. Interaction with
disfluent speech agents appears to increase egocentric communication in
comparison to fluent agents. Although the wide credibility intervals mean this
effect is not clear-cut. We discuss potential interpretations of this finding,
focusing on how disfluencies may impact partner models and language production
in HMD.

</details>


### [51] [PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses](https://arxiv.org/abs/2507.18393)
*Mahiro Ozaki,Li Chen,Shotaro Naganuma,Valdemar Švábenský,Fumiya Okubo,Atsushi Shimada*

Main category: cs.HC

TL;DR: PALM是一种学习分析仪表板，通过整合课程层信息解决学习分析的可扩展性问题，提升学生对学习行为和学术进展的认知。


<details>
  <summary>Details</summary>
Motivation: 传统学习分析研究多关注单课程或学习者，缺乏对课程间关系及长期学习轨迹的框架，PALM旨在填补这一空白。

Method: 开发PALM整合多层教育数据为课程地图，并通过系统评估其对学生学习行为意识和系统性能的影响。

Result: PALM显著提升学生对学习规划的认知和行为反思能力，视觉吸引力和易用性评价优于现有系统。

Conclusion: PALM作为信息资源和工具，推动了自调节学习和参与度，超越了传统学习分析的局限，具有可扩展性潜力。

Abstract: This study proposes and evaluates the PAnoramic Learning Map (PALM), a
learning analytics (LA) dashboard designed to address the scalability
challenges of LA by integrating curriculum-level information. Traditional LA
research has predominantly focused on individual courses or learners and often
lacks a framework that considers the relationships between courses and the
long-term trajectory of learning. To bridge this gap, PALM was developed to
integrate multilayered educational data into a curriculum map, enabling
learners to intuitively understand their learning records and academic
progression. We conducted a system evaluation to assess PALM's effectiveness in
two key areas: (1) its impact on students' awareness of their learning
behaviors, and (2) its comparative performance against existing systems. The
results indicate that PALM enhances learners' awareness of study planning and
reflection, particularly by improving perceived behavioral control through the
visual presentation of individual learning histories and statistical trends,
which clarify the links between learning actions and outcomes. Although PALM
requires ongoing refinement as a system, it received significantly higher
evaluations than existing systems in terms of visual appeal and usability. By
serving as an information resource with previously inaccessible insights, PALM
enhances self-regulated learning and engagement, representing a significant
step beyond conventional LA toward a comprehensive and scalable approach.

</details>


### [52] [Multisensory Integration and Sensory Substitution Across Vision, Audition, and Haptics: Answering the What, Which, and When in Study Protocols](https://arxiv.org/abs/2507.18401)
*Andrew Jeyathasan,Swati Banerjee*

Main category: cs.HC

TL;DR: 多感官整合（MSI）研究需关注跨模态对应、一致性、认知负荷和时间等因素，尤其需探索三模态及以上整合，以设计有效研究协议。


<details>
  <summary>Details</summary>
Motivation: 理解多感官整合（MSI）对日常生活和沉浸技术中感知统一的重要性。

Method: 探讨跨模态对应、一致性、认知负荷和刺激时间等关键因素。

Result: 为设计有效的多感官整合研究协议提供指导。

Conclusion: 多感官整合研究需更全面考虑多模态交互，尤其在三模态及以上场景中。

Abstract: We experience the world through multiple senses that work together to create
a cohesive perception, whether in daily life or immersive technologies.
Understanding this multisensory integration (MSI) requires examining the
interactions between sensory modalities, each with unique temporal dynamics and
characteristics. While most research focuses on unimodal or bimodal cues, the
integration of three or more modalities remains underexplored. MSI studies must
account for factors like cross-modal correspondence, congruence, cognitive
load, and stimulus timing, which become increasingly complex as modalities
multiply. This article examines these key factors and how they can be applied
to 8 design effective MSI study protocols.

</details>


### [53] [Towards Understanding Decision Problems As a Goal of Visualization Design](https://arxiv.org/abs/2507.18428)
*Lena Cibulski,Stefan Bruckner*

Main category: cs.HC

TL;DR: 提出了一个描述决策问题的表征方案，通过数据、用户和任务上下文的关键属性来支持决策任务。


<details>
  <summary>Details</summary>
Motivation: 可视化研究中决策过程常被忽略背景条件，需要更精确的描述来支持决策任务。

Method: 提出通过数据、用户和任务上下文的关键属性来描述决策问题的方案。

Result: 该方案能帮助精确指定决策支持声明，并指导视觉编码和交互设计。

Conclusion: 该表征方案为决策中心可视化研究提供未来方向。

Abstract: Decision-making is a central yet under-defined goal in visualization
research. While existing task models address decision processes, they often
neglect the conditions framing a decision. To better support decision-making
tasks, we propose a characterization scheme that describes decision problems
through key properties of the data, users, and task context. This scheme helps
visualization researchers specify decision-support claims more precisely and
informs the design of appropriate visual encodings and interactions. We
demonstrate the utility of our approach by applying it to characterize decision
tasks targeted by existing design studies, highlighting opportunities for
future research in decision-centric visualization.

</details>


### [54] [High-Dimensional Data Classification in Concentric Coordinates](https://arxiv.org/abs/2507.18450)
*Alice Williams,Boris Kovalerchuk*

Main category: cs.HC

TL;DR: 提出了一种支持低维到高维数据的无损同心坐标系框架，解决了高维数据可视化中的遮挡问题，并支持机器学习算法的可视化和人机交互。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据可视化中无损、无遮挡且计算可行的挑战，同时支持机器学习和人机交互需求。

Method: 引入同心坐标系（Concentric Coordinates），作为平行坐标系和圆形坐标系的扩展，属于广义线坐标可视化的一种形式。

Result: 该框架能够实现高维数据的无损可视化，避免了遮挡问题，并适用于机器学习算法和人机交互。

Conclusion: 同心坐标系为高维数据可视化提供了有效的解决方案，兼具可解释性和计算效率。

Abstract: The visualization of multi-dimensional data with interpretable methods
remains limited by capabilities for both high-dimensional lossless
visualizations that do not suffer from occlusion and that are computationally
capable by parameterized visualization. This paper proposes a low to high
dimensional data supporting framework using lossless Concentric Coordinates
that are a more compact generalization of Parallel Coordinates along with
former Circular Coordinates. These are forms of the General Line Coordinate
visualizations that can directly support machine learning algorithm
visualization and facilitate human interaction.

</details>


### [55] [ForcePinch: Force-Responsive Spatial Interaction for Tracking Speed Control in XR](https://arxiv.org/abs/2507.18510)
*Chenyang Zhang,Tiffany S Ma,John Andrews,Eric J Gonzalez,Mar Gonzalez-Franco,Yalong Yang*

Main category: cs.HC

TL;DR: ForcePinch是一种新型的空间交互方法，通过捏合力调节跟踪速度，实现了快速与精确操作的平滑过渡。


<details>
  <summary>Details</summary>
Motivation: 现有的空间交互技术将跟踪速度与手部动作直接耦合，限制了交互的灵活性。ForcePinch受自然摩擦力控制的启发，旨在提供更直观的交互方式。

Method: 开发了集成了压力传感器的硬件原型，通过捏合力调节跟踪速度，并与Go-Go和PRISM技术进行了对比实验。

Result: 用户研究表明，ForcePinch在不同交互场景中表现独特，具有更高的灵活性和适应性。

Conclusion: ForcePinch展示了力响应交互的多样性和潜力，为未来空间交互设计提供了新思路。

Abstract: Spatial interaction in 3D environments requires balancing efficiency and
precision, which requires dynamic tracking speed adjustments. However, existing
techniques often couple tracking speed adjustments directly with hand
movements, reducing interaction flexibility. Inspired by the natural friction
control inherent in the physical world, we introduce ForcePinch, a novel
force-responsive spatial interaction method that enables users to intuitively
modulate pointer tracking speed and smoothly transition between rapid and
precise movements by varying their pinching force. To implement this concept,
we developed a hardware prototype integrating a pressure sensor with a
customizable mapping function that translates pinching force into tracking
speed adjustments. We conducted a user study with 20 participants performing
well-established 1D, 2D, and 3D object manipulation tasks, comparing ForcePinch
against the distance-responsive technique Go-Go and speed-responsive technique
PRISM. Results highlight distinctive characteristics of the force-responsive
approach across different interaction contexts. Drawing on these findings, we
highlight the contextual meaning and versatility of force-responsive
interactions through four illustrative examples, aiming to inform and inspire
future spatial interaction design.

</details>


### [56] [PosterMate: Audience-driven Collaborative Persona Agents for Poster Design](https://arxiv.org/abs/2507.18572)
*Donghoon Shin,Daniel Lee,Gary Hsieh,Gromit Yeuk-Yin Chan*

Main category: cs.HC

TL;DR: PosterMate是一个海报设计助手，通过生成多样化的人物代理来模拟受众反馈，帮助设计师整合意见并高效完成设计改进。


<details>
  <summary>Details</summary>
Motivation: 传统的海报设计难以获取多样受众的同步反馈，而生成式AI技术为这一过程提供了新的可能性。

Method: PosterMate构建人物代理，模拟受众反馈并通过讨论协调不同观点，最终达成一致意见并整合到设计中。

Result: 用户研究表明PosterMate能捕捉多样化观点，而在线评估显示反馈符合人物身份且讨论能有效综合不同视角。

Conclusion: PosterMate展示了生成式AI在设计反馈中的潜力，可作为高效原型工具。

Abstract: Poster designing can benefit from synchronous feedback from target audiences.
However, gathering audiences with diverse perspectives and reconciling them on
design edits can be challenging. Recent generative AI models present
opportunities to simulate human-like interactions, but it is unclear how they
may be used for feedback processes in design. We introduce PosterMate, a poster
design assistant that facilitates collaboration by creating audience-driven
persona agents constructed from marketing documents. PosterMate gathers
feedback from each persona agent regarding poster components, and stimulates
discussion with the help of a moderator to reach a conclusion. These
agreed-upon edits can then be directly integrated into the poster design.
Through our user study (N=12), we identified the potential of PosterMate to
capture overlooked viewpoints, while serving as an effective prototyping tool.
Additionally, our controlled online evaluation (N=100) revealed that the
feedback from an individual persona agent is appropriate given its persona
identity, and the discussion effectively synthesizes the different persona
agents' perspectives.

</details>


### [57] [MeloKids: Multisensory VR System to Enhance Speech and Motor Coordination in Children with Hearing Loss](https://arxiv.org/abs/2507.18619)
*Yichen Yu,Qiaoran Wang*

Main category: cs.HC

TL;DR: 本研究探讨基于虚拟现实（VR）的多感官反馈技术如何通过整合听觉、视觉和触觉刺激改善听力障碍儿童的语言和运动康复效果。


<details>
  <summary>Details</summary>
Motivation: 听力障碍儿童在语言和运动发展上面临持续挑战，需要创新的康复方法。

Method: 使用功能性近红外光谱技术（fNIRS）评估儿童在不同互动模式下的音高匹配任务中的皮层激活模式。

Result: 研究为设计个性化的互动康复系统提供了证据，这些系统能增强听力障碍儿童的认知参与和运动控制。

Conclusion: 多感官反馈技术和VR的结合有望改善听力障碍儿童的康复效果和认知能力。

Abstract: Children with hearing impairments face ongoing challenges in language and
motor development. This study explores how multi-sensory feedback technology
based on virtual reality (VR), integrating auditory, visual, and tactile
stimuli, can enhance rehabilitation outcomes. Using functional near-infrared
spectroscopy (fNIRS) technology, we assessed cortical activation patterns in
children during pitch-matching tasks across different interaction modes. Our
findings aim to provide evidence for designing personalized, interactive
rehabilitation systems that enhance cognitive engagement and motor control in
children with hearing impairments.

</details>


### [58] [Evaluation of a Provenance Management Tool for Immersive Virtual Fieldwork](https://arxiv.org/abs/2507.18622)
*Armin Bernstetter,Tom Kwasnitschka,Isabella Peters*

Main category: cs.HC

TL;DR: 论文研究了一种数字实验室笔记本（DLB）工具，用于记录和注释地学研究中虚拟实地工作的交互，确保研究可重复性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决科学研究可重复性问题，尤其是在地学领域，通过记录工作流程和可视化状态来提高透明度。

Method: 通过用户研究评估DLB工具，比较沉浸式和非沉浸式环境中使用的感知易用性和有用性。

Result: 参与者认为DLB有用且易用，沉浸式环境下感知易用性更高，但使用模式无显著差异。

Conclusion: DLB工具在支持研究可重复性方面有效，尤其在沉浸式环境中表现出更高的易用性。

Abstract: Ensuring reproducibility of research is an integral part of good scientific
practice. One way to support this is through provenance: information about
research workflows from data gathering to researchers' sensemaking processes
leading to published results. This is highly important in disciplines such as
geosciences, where researchers use software for interactive and immersive
visualizations of geospatial data, doing virtual measurements in simulated
fieldwork on 3D models. We evaluated a provenance management tool, which allows
recording of interactions with a virtual fieldwork tool and annotating
different states of the visualization. The user study investigated how
researchers used this Digital Lab Book (DLB) and whether perceived ease of use
and perceived usefulness differed between groups in immersive or non-immersive
settings. Participants perceived the DLB as both useful and easy to use. While
there were indications of differences in perceived ease of use (higher for
immersive setting), usage patterns showed no significant group differences.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [59] [Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA](https://arxiv.org/abs/2507.17963)
*Rameen Abdal,Or Patashnik,Ekaterina Deyneka,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov,Daniel Cohen-Or,Kfir Aberman*

Main category: cs.GR

TL;DR: 本文提出了一种零样本框架，用于动态概念个性化，无需微调即可生成高质量视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要针对每个实例进行微调，限制了扩展性，本文旨在解决这一问题。

Method: 采用结构化2x2视频网格与轻量级Grid-LoRA适配器，结合Grid Fill模块完成部分布局。

Result: 实验表明，该方法能在单次前向传播中生成高质量、一致的视频，适用于多种场景。

Conclusion: 提出的零样本框架在动态概念个性化方面表现出色，具有广泛适用性。

Abstract: Recent advances in text-to-video generation have enabled high-quality
synthesis from text and image prompts. While the personalization of dynamic
concepts, which capture subject-specific appearance and motion from a single
video, is now feasible, most existing methods require per-instance fine-tuning,
limiting scalability. We introduce a fully zero-shot framework for dynamic
concept personalization in text-to-video models. Our method leverages
structured 2x2 video grids that spatially organize input and output pairs,
enabling the training of lightweight Grid-LoRA adapters for editing and
composition within these grids. At inference, a dedicated Grid Fill module
completes partially observed layouts, producing temporally coherent and
identity preserving outputs. Once trained, the entire system operates in a
single forward pass, generalizing to previously unseen dynamic concepts without
any test-time optimization. Extensive experiments demonstrate high-quality and
consistent results across a wide range of subjects beyond trained concepts and
editing scenarios.

</details>


### [60] [Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation](https://arxiv.org/abs/2507.18352)
*Zhen Han,Mattias Teye,Derek Yadgaroff,Judith Bütepage*

Main category: cs.GR

TL;DR: 利用混合知识蒸馏和伪标签训练小型化模型，减少内存占用和音频上下文需求，实现设备端实时3D面部动画。


<details>
  <summary>Details</summary>
Motivation: 解决现有语音驱动面部动画模型过大、无法设备端实时运行的问题，适用于游戏开发场景。

Method: 采用混合知识蒸馏和伪标签技术，用高性能教师模型训练小型学生模型，仅使用卷积和全连接层。

Result: 模型内存占用降至3.4 MB，所需音频上下文时间降至81 ms，同时保持高质量动画效果。

Conclusion: 实现了设备端推理的小型、高效模型，为数字角色的逼真动画奠定基础。

Abstract: The training of high-quality, robust machine learning models for
speech-driven 3D facial animation requires a large, diverse dataset of
high-quality audio-animation pairs. To overcome the lack of such a dataset,
recent work has introduced large pre-trained speech encoders that are robust to
variations in the input audio and, therefore, enable the facial animation model
to generalize across speakers, audio quality, and languages. However, the
resulting facial animation models are prohibitively large and lend themselves
only to offline inference on a dedicated machine. In this work, we explore
on-device, real-time facial animation models in the context of game
development. We overcome the lack of large datasets by using hybrid knowledge
distillation with pseudo-labeling. Given a large audio dataset, we employ a
high-performing teacher model to train very small student models. In contrast
to the pre-trained speech encoders, our student models only consist of
convolutional and fully-connected layers, removing the need for attention
context or recurrent updates. In our experiments, we demonstrate that we can
reduce the memory footprint to up to 3.4 MB and required future audio context
to up to 81 ms while maintaining high-quality animations. This paves the way
for on-device inference, an important step towards realistic, model-driven
digital characters.

</details>


### [61] [DanceGraph: A Complementary Architecture for Synchronous Dancing Online](https://arxiv.org/abs/2507.18052)
*David Sinclair,Ademyemi Ademola,Babis Koniaris,Kenny Mitchell*

Main category: cs.GR

TL;DR: DanceGraph是一种低延迟的在线舞蹈同步架构，通过高效实时传输和动作预测优化，结合参数化风格化方法提升舞蹈节奏同步效果。


<details>
  <summary>Details</summary>
Motivation: 解决网络化舞蹈同步中的高延迟问题，提升在线舞蹈体验。

Method: 开发实时带宽高效架构以减少延迟，并采用参数化风格化方法动态调整舞蹈动作。

Result: 实现了舞蹈动作与音乐节奏的低延迟同步，增强了交互性和视觉效果。

Conclusion: DanceGraph通过技术优化和交互设计，有效提升了在线舞蹈同步的实时性和体验效果。

Abstract: DanceGraph is an architecture for synchronized online dancing overcoming the
latency of networked body pose sharing. We break down this challenge by
developing a real-time bandwidth-efficient architecture to minimize lag and
reduce the timeframe of required motion prediction for synchronization with the
music's rhythm. In addition, we show an interactive method for the
parameterized stylization of dance motions for rhythmic dance using online
dance correctives.

</details>


### [62] [GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar](https://arxiv.org/abs/2507.18155)
*SeungJun Moon,Hah Min Lew,Seungeun Lee,Ji-Su Kang,Gyeong-Moon Park*

Main category: cs.GR

TL;DR: 本文提出GeoAvatar框架，通过自适应几何高斯分布解决3D头部化身的身份保留与动作生成问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理面部不同区域几何偏差时表现不佳，导致重建和动画质量不足。

Method: GeoAvatar采用自适应预分配阶段（APS）分割高斯分布，提出嘴部结构和分区变形策略，并引入正则化损失实现高斯与3DMM面部的精准绑定。

Result: 实验表明GeoAvatar在重建和新动画场景中优于现有方法。

Conclusion: GeoAvatar通过自适应几何增强和嘴部变形策略，解决了3D头部化身的重建与动画难题。

Abstract: Despite recent progress in 3D head avatar generation, balancing identity
preservation, i.e., reconstruction, with novel poses and expressions, i.e.,
animation, remains a challenge. Existing methods struggle to adapt Gaussians to
varying geometrical deviations across facial regions, resulting in suboptimal
quality. To address this, we propose GeoAvatar, a framework for adaptive
geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation
Stage (APS), an unsupervised method that segments Gaussians into rigid and
flexible sets for adaptive offset regularization. Then, based on mouth anatomy
and dynamics, we introduce a novel mouth structure and the part-wise
deformation strategy to enhance the animation fidelity of the mouth. Finally,
we propose a regularization loss for precise rigging between Gaussians and 3DMM
faces. Moreover, we release DynamicFace, a video dataset with highly expressive
facial motions. Extensive experiments show the superiority of GeoAvatar
compared to state-of-the-art methods in reconstruction and novel animation
scenarios.

</details>


### [63] [PS-GS: Gaussian Splatting for Multi-View Photometric Stereo](https://arxiv.org/abs/2507.18231)
*Yixiao Chen,Bin Liang,Hanzhi Guo,Yongqing Cheng,Jiayi Zhao,Dongdong Weng*

Main category: cs.GR

TL;DR: 论文提出了一种名为PS-GS的方法，通过结合逆向渲染和多视角光度立体视觉（MVPS），高效地联合估计物体的几何、材质和照明，提升了3D重建的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统逆向渲染方法依赖于固定环境照明，其3D重建准确性有限。而现有的多视角光度立体视觉（MVPS）方法在逆向渲染中效率不高。为此，研究团队旨在填补这一空白。

Method: PS-GS方法首先构建一个标准的2D高斯飞溅模型作为初始几何，随后通过包含光照计算多层感知机的完整渲染方程进行延迟逆向渲染，并通过未标定光度立体视觉估计的法线图对渲染法线图进行正则化。此外，提出了2D高斯光线追踪来优化入射光照。

Result: 实验表明，PS-GS在合成和真实数据集上均优于现有方法，在重建准确性和计算效率方面表现优越。

Conclusion: PS-GS方法成功解决了逆向渲染与MVPS结合的挑战，能够用于新颖视角合成、重光照以及材质和形状编辑。

Abstract: Integrating inverse rendering with multi-view photometric stereo (MVPS)
yields more accurate 3D reconstructions than the inverse rendering approaches
that rely on fixed environment illumination. However, efficient inverse
rendering with MVPS remains challenging. To fill this gap, we introduce the
Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently
and jointly estimates the geometry, materials, and lighting of the object that
is illuminated by diverse directional lights (multi-light). Our method first
reconstructs a standard 2D Gaussian splatting model as the initial geometry.
Based on the initialization model, it then proceeds with the deferred inverse
rendering by the full rendering equation containing a lighting-computing
multi-layer perceptron. During the whole optimization, we regularize the
rendered normal maps by the uncalibrated photometric stereo estimated normals.
We also propose the 2D Gaussian ray-tracing for single directional light to
refine the incident lighting. The regularizations and the use of multi-view and
multi-light images mitigate the ill-posed problem of inverse rendering. After
optimization, the reconstructed object can be used for novel-view synthesis,
relighting, and material and shape editing. Experiments on both synthetic and
real datasets demonstrate that our method outperforms prior works in terms of
reconstruction accuracy and computational efficiency.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [64] [Low-power switching of memristors exhibiting fractional-order dynamics](https://arxiv.org/abs/2507.18487)
*Nathan Astin,Yuriy V. Pershin*

Main category: cs.ET

TL;DR: 研究了利用电流脉冲切换具有分数阶行为的忆阻器的最佳策略，发现脉冲宽度选择取决于分数阶导数的阶数和运动方程中的幂指数。


<details>
  <summary>Details</summary>
Motivation: 探索能量高效的忆阻器切换策略，为下一代仿生神经形态计算架构奠定基础。

Method: 采用分数阶微分方程（Caputo型导数）建模忆阻器状态变量，分析不同电流脉冲策略下的焦耳损耗。

Result: 分数阶导数阶数超过幂指数一半时，宽脉冲最佳；否则，零电流加窄高幅脉冲更优。多脉冲控制进一步验证了这一规律。

Conclusion: 研究为设计更节能的神经形态计算系统提供了理论依据，强调了分数阶行为在优化中的关键作用。

Abstract: In this conference contribution, we present some initial results on switching
memristive devices exhibiting fractional-order behavior using current pulses.
In our model, it is assumed that the evolution of a state variable follows a
fractional-order differential equation involving a Caputo-type derivative. A
study of Joule losses demonstrates that the best switching strategy minimizing
these losses depends on the fractional derivative's order and the power
exponent in the equation of motion. It is found that when the order of the
fractional derivative exceeds half of the power exponent, the best approach is
to employ a wide pulse. Conversely, when this condition is not met, Joule
losses are minimized by applying a zero current followed by a narrow current
pulse of the highest allowable amplitude. These findings are explored further
in the context of multi-pulse control. Our research lays the foundation for the
advancement of the next generation of energy-efficient neuromorphic computing
architectures that more closely mimic their biological counterparts.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [65] [Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release](https://arxiv.org/abs/2507.17766)
*Felix Quinque,Alan Aboudib,Szymon Fonau,Rodrigo Lopez Portillo Alcocer,Brian McCrindle,Steffen Cruz*

Main category: cs.DC

TL;DR: Bittensor的SN9展示了去中心化预训练LLM的可行性，但存在模型本地化存储和奖励分配不均的问题。IOTA通过数据并行、连续奖励和激活压缩等技术解决了这些问题。


<details>
  <summary>Details</summary>
Motivation: 解决SN9中模型本地化存储和奖励分配不均的核心问题，实现去中心化LLM预训练的规模化与公平性。

Method: 引入IOTA架构，包括数据与管道并行的SWARM架构、连续奖励机制、激活压缩、Butterfly All-Reduce和CLASP公平归因方案。

Result: 实现模型规模的任意扩展、通信带宽减少128倍、线性可扩展性及公平贡献评估。

Conclusion: IOTA通过协同训练与公平激励，显著提升了去中心化LLM预训练的效率和公平性。

Abstract: In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed
network of incentivized, permissionless actors could each pretrain large
language models (LLMs) ranging from 700 million to 14 billion parameters, while
surpassing established baselines. While that work validated blockchain-based
decentralized pretraining as viable, it contained core issues: (i) every miner
had to fit an entire model locally, and (ii) "winner-takes-all" rewards
encouraged model hoarding.
  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an
architecture that addresses these limitations by transforming SN9's previously
isolated competitors into a single cooperating unit that can scale arbitrarily
while still rewarding each contributor fairly.
  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -
An orchestrator distributes model layers across heterogeneous miners and
streams activations between them, enabling model sizes to scale with the number
of participants rather than being constrained by the VRAM of a single machine;
(2) Granular, continuous incentives - Validators measure each miner's
contribution and allocate token emissions proportionally; (3) Activation
compression - We used model-bottlenecks to cut communication bandwidths of
activations by up to 128x, vastly improving training speed; (4) Butterfly
All-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,
offering linear scalability, redundancy and built-in collusion detection; (5)
CLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair
attribution scheme assigns credit to miners proportional to their marginal
utility and detects exploits, even when contributions are interdependent across
the pipeline.

</details>


### [66] [PolyServe: Efficient Multi-SLO Serving at Scale](https://arxiv.org/abs/2507.17769)
*Kan Zhu,Haiyang Shi,Le Xu,Jiaxin Shan,Arvind Krishnamurthy,Baris Kasikci,Liguang Xie*

Main category: cs.DC

TL;DR: PolyServe 是一个针对多 SLO 需求的 LLM 请求调度系统，通过分组、负载梯度调度和资源共享提升吞吐量，比现有策略提升 1.23 倍。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 应用对延迟要求多样，简单的 LS/BE 分类无法满足需求，导致调度效果不佳。

Method: PolyServe 基于请求的每 token 延迟要求分组，并动态调度到最优服务器，同时支持资源共享和尾部延迟控制。

Result: PolyServe 的吞吐量比现有策略提升 1.23 倍，达到最优吞吐量的 92.5%。

Conclusion: PolyServe 在多 SLO 场景下高效调度，显著提升用户体验和系统性能。

Abstract: Advances in Large Language Models (LLMs) have led to a surge of LLM-powered
applications. These applications have diverse token-generation latency
requirements. As a result, simply classifying workloads as latency-sensitive
(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive
category and results in suboptimal user experiences and scheduling
opportunities. However, efficiently serving requests with multiple SLO
requirements poses significant challenges. First, all requests within a batch
generate new tokens simultaneously, which can misalign them with their distinct
SLO requirements. Moreover, while existing systems focus on auto-scaling for
handling various overall request rates, the diversity of SLOs necessitates
fine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE
scenarios, where BE requests can be aborted at any time to ensure the SLO
attainment of LS requests, those with different latency-sensitive SLOs cannot
tolerate prolonged delays, and tail latency must be controlled.
  To tackle these challenges, we propose PolyServe, a novel multi-SLO
scheduling policy at scale that maintains high SLO attainment while maximizing
throughput. PolyServe first groups requests into multiple bins based on their
per-token latency requirement, then schedules each bin to a subset of the
server fleet. PolyServe routes requests to the highest-load but still
SLO-attainable server to create a load gradient that facilitates auto-scaling.
To increase utilization, PolyServe permits looser-SLO requests to share
tighter-SLO instances when their own servers are saturated. PolyServe uses
profiling data to guide scheduling decisions and manage tail latency through
request-wait-time-aware scheduling, dynamic chunking, and continuous chunked
prefill prediction. PolyServe achieves 1.23x goodput gain compared to existing
policies, achieving up to 92.5% of optimal goodput.

</details>


### [67] [Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale](https://arxiv.org/abs/2507.17770)
*Pei-Kun Yang*

Main category: cs.DC

TL;DR: 本研究比较了五种基于软件的QUBO求解器（Neal、PyTorch CPU/GPU、JAX、SciPy），在随机生成的QUBO矩阵上评估其性能，发现PyTorch在规模与效率间表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了评估不同QUBO求解器在大规模优化问题中的性能差异，帮助用户选择合适的工具。

Method: 生成1000x1000到45000x45000的随机QUBO矩阵，测试六种收敛阈值下的求解器（Neal、PyTorch CPU/GPU、JAX、SciPy）。

Result: Neal能量最低但规模受限；PyTorch扩展性最强且速度快；JAX性能接近PyTorch但规模小；SciPy表现最差。

Conclusion: PyTorch是大规模QUBO问题的最佳平衡选择，尤其在有计算资源支持时。

Abstract: Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework
for modeling combinatorial optimization problems. This study benchmarks five
software-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and
SciPy, on randomly generated QUBO matrices ranging from 1000x1000 to
45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate
their performance in terms of solution quality (energy) and computational time.
Among the solvers tested, Neal achieved the lowest energy values but was
limited to problems with up to 6000 variables due to high memory consumption.
PyTorch produced slightly higher energy results than Neal but demonstrated
superior scalability, solving instances with up to 45000 variables. Its support
for GPU acceleration and CPU multi-threading also resulted in significantly
shorter runtimes. JAX yielded energy values slightly above those of PyTorch and
was limited to 25000 variables, with runtimes comparable to PyTorch on GPU.
SciPy was the most constrained solver, handling only up to 6000 variables and
consistently producing the highest energy values with the longest computation
times. These findings highlight trade-offs between solution quality,
scalability, and runtime efficiency, and suggest that PyTorch is the most
balanced choice for large-scale QUBO problems when computational resources
permit.

</details>


### [68] [Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN Inference Acceleration](https://arxiv.org/abs/2507.17771)
*Dmitri Lyalikov*

Main category: cs.DC

TL;DR: 论文探讨了在资源受限的嵌入式平台上部署现代CNN的硬件集成挑战，提出了基于RISC-V Vector 1.0扩展的灵活编译/执行模型，显著提升预处理速度和降低CPU回退。


<details>
  <summary>Details</summary>
Motivation: 解决异构架构中硬件集成和编译/执行模型的效率问题，以优化CNN在嵌入式平台上的性能与功耗。

Method: 利用RISC-V Vector 1.0扩展设计灵活的编程模型，结合适合的缓存层次结构，减少预处理瓶颈和CPU回退。

Result: 实验显示预处理速度提升9倍，YOLOv3回退层执行速度提升3倍，功耗低于传统并行执行平台。

Conclusion: RISC-V Vector 1.0扩展为异构嵌入式SoC提供了高效的计算与内存管理方案，优于传统平台。

Abstract: The emergence of heterogeneity and domain-specific architectures targeting
deep learning inference show great potential for enabling the deployment of
modern CNNs on resource-constrained embedded platforms. A significant
development is the diversification of custom hardware solely targeting the most
expensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural
processing units), among others, can overcome the approaching limits of
traditional silicon scaling and provide a solution to the power/performance
tradeoff within embedded SoCs. Efficient DSA utilization requires proper system
integration and a compilation/execution model for balanced execution in these
heterogeneous architectures. There is a critical need for proper system
integration and an efficient compilation/execution model for balanced execution
in these heterogeneous architectures. This work highlights the hardware
integration challenges for efficiently placing these units within the memory
hierarchy and correct proximity to other execution blocks. We experimentally
verify performance bottlenecks in CNN execution and pre/post-processing at
runtime, where previous attention has generally been given to accelerator
speedup alone. This work takes advantage of the ratification of the RISC-V
Vector 1.0 extension and demonstrates its potential as a flexible target within
a well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and
CPU fallback processes. Our results show up to a 9x speedup of image
pre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.
We demonstrate RVV-1.0 in exposing a flexible programming model that can enable
a balanced computation and memory footprint on accelerator-rich embedded SoCs
supporting modern deep-learning dataflows while consuming less power than
traditional parallel execution platforms.

</details>


### [69] [Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments](https://arxiv.org/abs/2507.17772)
*Ahmad Alhonainy,Praveen Rao*

Main category: cs.DC

TL;DR: 论文通过引入缓存策略（FIFO、LRU、基于优先级）减少联邦学习中不必要的模型更新传输，降低通信成本，同时保持模型准确性。实验证明该方法在CIFAR-10和医疗数据集上有效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）的通信成本是主要瓶颈，尤其是在资源受限的环境中。通过减少不必要的模型更新传输，可以降低带宽使用。

Method: 引入三种缓存策略（FIFO、LRU、基于优先级），选择性转发重要的模型更新。

Result: 实验表明，该方法显著降低通信成本，同时仅造成最小精度损失。

Conclusion: 智能缓存提升了联邦学习的可扩展性和内存效率，适用于边缘物联网网络，特别适合智能城市和医疗等延迟敏感的应用。

Abstract: Federated Learning (FL) allows multiple distributed devices to jointly train
a shared model without centralizing data, but communication cost remains a
major bottleneck, especially in resource-constrained environments. This paper
introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce
unnecessary model update transmissions. By selectively forwarding significant
updates, our approach lowers bandwidth usage while maintaining model accuracy.
Experiments on CIFAR-10 and medical datasets show reduced communication with
minimal accuracy loss. Results confirm that intelligent caching improves
scalability, memory efficiency, and supports reliable FL in edge IoT networks,
making it practical for deployment in smart cities, healthcare, and other
latency-sensitive applications.

</details>


### [70] [MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation](https://arxiv.org/abs/2507.17773)
*Zhongzhen Wen,Yinghui Zhang,Zhong Li,Zhongxin Liu,Linna Xie,Tian Zhang*

Main category: cs.DC

TL;DR: MultiKernelBench 是一个多平台基准测试，旨在评估大型语言模型（LLMs）在自动生成深度学习（DL）内核方面的表现，解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在硬件支持、内核分类和任务覆盖方面存在不足，阻碍了对LLMs生成DL内核能力的全面评估。

Method: 引入了 MultiKernelBench，覆盖 285 个任务的 14 种内核类别，支持三种硬件平台，并设计了模块化后端抽象层以实现扩展性。还提出了类别感知的单次提示方法，以提高生成质量。

Result: 对7个最先进的LLMs进行了系统评估，发现任务难度差异显著，对训练较少平台的泛化能力差，而针对性提示策略有效。

Conclusion: MultiKernelBench 提供了一个全面的基准测试框架，有助于深入研究LLMs在DL内核生成中的表现，并推动了该领域的发展。

Abstract: The automatic generation of deep learning (DL) kernels using large language
models (LLMs) has emerged as a promising approach to reduce the manual effort
and hardware-specific expertise required for writing high-performance operator
implementations. However, existing benchmarks for evaluating LLMs in this
domain suffer from limited hardware support, coarse-grained kernel
categorization, and imbalanced task coverage. To address these limitations, we
introduce MultiKernelBench, the first comprehensive, multi-platform benchmark
for LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14
well-defined kernel categories and supports three major hardware platforms:
Nvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we
design a modular backend abstraction layer that decouples platform-specific
logic from the core benchmarking infrastructure, allowing easy integration of
new hardware platforms. We further propose a simple yet effective
category-aware one-shot prompting method that improves generation quality by
providing in-category exemplars. Through systematic evaluations of seven
state-of-the-art LLMs, we reveal significant variation in task difficulty, poor
generalization to platforms with less training exposure, and the effectiveness
of targeted prompting strategies. MultiKernelBench is publicly available at
https://github.com/wzzll123/MultiKernelBench.

</details>


### [71] [CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks](https://arxiv.org/abs/2507.17793)
*Joel Brogan,Matthew Yohe,David Cornett*

Main category: cs.DC

TL;DR: CHAMP是一个模块化边缘计算平台，支持动态更换AI "能力卡匣"，用于人脸识别、目标跟踪等任务。其设计包括FPGA加速器和VDiSK操作系统，实验显示其性能接近线性扩展，适用于生物识别、监控等场景。


<details>
  <summary>Details</summary>
Motivation: 为需要灵活、高性能边缘AI系统的现场操作人员提供一个可定制、即插即用的解决方案。

Method: 设计CHAMP平台，采用低功耗FPGA加速器和高速总线，搭配VDiSK操作系统，支持动态配置AI模块和加密数据存储。

Result: 实验显示，平台在1至5个神经计算加速器下性能接近线性扩展，同时揭示了USB3总线的饱和限制。

Conclusion: CHAMP在生物识别、监控等领域具有应用潜力，未来可通过优化总线协议和扩展功能进一步提升性能。

Abstract: What if you could piece together your own custom biometrics and AI analysis
system, a bit like LEGO blocks? We aim to bring that technology to field
operators in the field who require flexible, high-performance edge AI system
that can be adapted on a moment's notice. This paper introduces CHAMP
(Configurable Hot-swappable Architecture for Machine Perception), a modular
edge computing platform that allows operators to dynamically swap in
specialized AI "capability cartridges" for tasks like face recognition, object
tracking, and document analysis. CHAMP leverages low-power FPGA-based
accelerators on a high-throughput bus, orchestrated by a custom operating
system (VDiSK) to enable plug-and-play AI pipelines and cryptographically
secured biometric datasets. In this paper we describe the CHAMP design,
including its modular scaling with multiple accelerators and the VDiSK
operating system for runtime reconfiguration, along with its cryptographic
capabilities to keep data stored on modules safe and private. Experiments
demonstrate near-linear throughput scaling from 1 to 5 neural compute
accelerators, highlighting both the performance gains and saturation limits of
the USB3-based bus. Finally, we discuss applications of CHAMP in field
biometrics, surveillance, and disaster response, and outline future
improvements in bus protocols, cartridge capabilities, and system software.

</details>


### [72] [Optimizing Edge Gaming Slices through an Enhanced User Plane Function and Analytics in Beyond-5G Networks](https://arxiv.org/abs/2507.17843)
*Bruno Marques da Silva,Larissa Ferreira Rodrigues Moreira,Flávio de Oliveira Silva,Rodrigo Moreira*

Main category: cs.DC

TL;DR: 论文提出了一种集成NWDAF和UPF的闭环架构，用于估计用户延迟并增强5G控制平面，结果显示AI模型能支持游戏分类。


<details>
  <summary>Details</summary>
Motivation: 解决移动用户服务管理和服务级别协议合规性方面的挑战，特别是延迟测量问题。

Method: 提出一种集成NWDAF和UPF的闭环架构，嵌入AI模型以实现延迟感知。

Result: 该方法能有效估计用户延迟，支持游戏分类，为移动边缘游戏研究开辟新途径。

Conclusion: 闭环架构和AI模型的结合为5G控制平面的延迟优化提供了可行方案。

Abstract: The latest generation of games and pervasive communication technologies poses
challenges in service management and Service-Level Agreement compliance for
mobile users. State-of-the-art edge-gaming techniques enhance throughput,
reduce latency, and leverage cloud computing. However, further development of
core functions such as the User Plane Function (UPF) is needed for
non-intrusive user latency measurement. This paper proposes a closed-loop
architecture integrating the Network Data Analytics Function (NWDAF) and UPF to
estimate user latency and enhance the 5G control plane by making it
latency-aware. The results show that embedding an artificial intelligence model
within NWDAF enables game classification and opens new avenues for mobile edge
gaming research.

</details>


### [73] [PowerTrip: Exploiting Federated Heterogeneous Datacenter Power for Distributed ML Training](https://arxiv.org/abs/2507.17904)
*Talha Mehboob,Luanzheng Guo,Nathan Tallent,Michael Zink,David Irwin*

Main category: cs.DC

TL;DR: 论文提出PowerTrip系统，动态选择分布式站点以优化计算资源与通信开销，提升大规模AI模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型计算需求超过单数据中心能力，现有方法忽略异构电力资源问题，需解决分布式训练中的电力与通信权衡挑战。

Method: PowerTrip基于电力-成本启发式动态选择站点，采用动态贪婪算法，优化训练效率的边际增益。

Result: 实验表明，与现有基线策略相比，PowerTrip可将训练时间减少50%。

Conclusion: PowerTrip在电力受限的分布式环境中显著提升训练效率，为大规模AI模型训练提供新思路。

Abstract: The exponential growth of large-scale AI models has led to computational and
power demands that can exceed the capacity of a single data center. This is due
to the limited power supplied by regional grids that leads to limited regional
computational power. Consequently, distributing training workloads across
geographically distributed sites has become essential. However, this approach
introduces a significant challenge in the form of communication overhead,
creating a fundamental trade-off between the performance gains from accessing
greater aggregate power and the performance losses from increased network
latency. Although prior work has focused on reducing communication volume or
using heuristics for distribution, these methods assume constant homogeneous
power supplies and ignore the challenge of heterogeneous power availability
between sites.
  To address the challenge of training large models in power-constrained,
geo-distributed environments, we introduce PowerTrip, a system that dynamically
selects a subset of sites during runtime to optimize the power-communication
trade-off. Specifically, PowerTrip selects sites based on a power-to-cost
heuristic, prioritizing those with high power availability and low network
latency. PowerTrip employs a dynamic greedy approach and uses the marginal gain
in training efficiency, i.e., accuracy improvement per unit of time, to
optimize for the number of sites where the performance penalty from network
overhead negates the benefit of adding more computational power. Our
evaluation, which uses real-world Google power traces to model realistic power
capacity constraints, demonstrates that PowerTrip can reduce time-to-accuracy
by up to 50% compared to existing baseline policies.

</details>


### [74] [C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters](https://arxiv.org/abs/2507.18005)
*Shengye Song,Minxian Xu,Zuowei Zhang,Chengxi Gao,Fansong Zeng,Yu Ding,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 该论文研究了大规模微服务集群的资源竞争与干扰问题，提出了基于CPI的干扰预测方法，并设计了C-Koordinator平台，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 微服务在提升资源利用率的同时引入了资源竞争与干扰，亟需设计干扰感知策略以优化性能。

Method: 通过分析大规模微服务集群特征，采用CPI作为干扰度量指标，并构建多维指标的干扰预测模型。设计C-Koordinator平台实现干扰缓解。

Result: 干扰预测模型准确率达90.3%，在各百分位响应时间（P50、P90、P99）下，应用延迟降低16.7%~36.1%。

Conclusion: C-Koordinator平台有效提升了共址环境下微服务的性能稳定性，为大规模部署提供了实用解决方案。

Abstract: Microservices transform traditional monolithic applications into lightweight,
loosely coupled application components and have been widely adopted in many
enterprises. Cloud platform infrastructure providers enhance the resource
utilization efficiency of microservices systems by co-locating different
microservices. However, this approach also introduces resource competition and
interference among microservices. Designing interference-aware strategies for
large-scale, co-located microservice clusters is crucial for enhancing resource
utilization and mitigating competition-induced interference. These challenges
are further exacerbated by unreliable metrics, application diversity, and node
heterogeneity.
  In this paper, we first analyze the characteristics of large-scale and
co-located microservices clusters at Alibaba and further discuss why cycle per
instruction (CPI) is adopted as a metric for interference measurement in
large-scale production clusters, as well as how to achieve accurate prediction
of CPI through multi-dimensional metrics. Based on CPI interference prediction
and analysis, we also present the design of the C-Koordinator platform, an
open-source solution utilized in Alibaba cluster, which incorporates
co-location and interference mitigation strategies. The interference prediction
models consistently achieve over 90.3% accuracy, enabling precise prediction
and rapid mitigation of interference in operational environments. As a result,
application latency is reduced and stabilized across all percentiles (P50, P90,
P99) response time (RT), achieving improvements ranging from 16.7% to 36.1%
under various system loads compared with state-of-the-art system. These results
demonstrate the system's ability to maintain smooth application performance in
co-located environments.

</details>


### [75] [Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling](https://arxiv.org/abs/2507.18006)
*Jingfeng Wu,Yiyuan He,Minxian Xu,Xitong Gao,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: CoCoServe是一个弹性系统，通过模块级的复制和迁移实现动态细粒度扩展，优化大型语言模型（LLM）的资源管理，降低成本46%，并显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统难以平衡服务需求和资源限制，静态部署导致资源利用不足，动态扩展成本高昂。

Method: 提出了CoCoServe系统，采用模块级操作（如复制和迁移）进行动态扩展，开发了自动伸缩机制以优化资源分配和性能。

Result: CoCoServe在保持可用性的情况下降低成本46%，延迟减少14%-75%，吞吐量提升1.16x-4x。

Conclusion: CoCoServe通过模块级动态扩展显著提升了LLM服务的效率和成本效益，优于现有系统。

Abstract: The rise of large language models (LLMs) has created new opportunities across
various fields but has also introduced significant challenges in resource
management. Current LLM serving systems face a fundamental tension: balancing
serving demands with limited resources while adapting to unpredictable traffic
patterns. Static deployments lead to suboptimal resource utilization and
performance degradation under dynamic workloads. Furthermore, the high cost of
adjusting instances hinders dynamic scaling, limiting the true potential of
efficient LLM serving.
  To address this, we propose CoCoServe, an elastic system that facilitates
dynamic and fine-grained scaling. Its key innovation lies in the module-level
operations for the replication and migration of LLM modules, such as decoder
layers and projections. Through a comprehensive analysis of the trade-offs
associated with these operations, we develop an auto-scaling mechanism that
dynamically regulates module-level resource allocation and performance
optimization, enabling a more cost-effective deployment of LLMs. Our evaluation
demonstrates that the scaling operations employed by CoCoServe exhibit
excellent scalability and can reduce costs by 46% while maintaining
availability. Compared to state-of-the-art LLM serving systems (e.g., Hugging
Face Transformers and vLLM), our approach reduces latency by 14%-75% and
achieves 1.16x-4x throughput on average across different model sizes and
workloads.

</details>


### [76] [Cloud Native System for LLM Inference Serving](https://arxiv.org/abs/2507.18007)
*Minxian Xu,Junhan Liao,Jingfeng Wu,Yiyuan He,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 本文探讨了如何利用云原生技术优化大型语言模型（LLM）的推理服务，提升资源效率并解决传统方法中的延迟和扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 传统LLM推理服务存在资源效率低、运营成本高和扩展性差的问题，亟需通过云原生技术改进。

Method: 采用容器化、微服务和动态调度等云原生技术，结合Kubernetes自动扩展，优化资源分配和性能。

Result: 实际评估表明，云原生架构能动态适应工作负载变化，显著降低延迟并提升吞吐量。

Conclusion: 云原生框架有望重塑LLM推理服务的未来，为研究者和行业领导者提供重要启示。

Abstract: Large Language Models (LLMs) are revolutionizing numerous industries, but
their substantial computational demands create challenges for efficient
deployment, particularly in cloud environments. Traditional approaches to
inference serving often struggle with resource inefficiencies, leading to high
operational costs, latency issues, and limited scalability. This article
explores how Cloud Native technologies, such as containerization,
microservices, and dynamic scheduling, can fundamentally improve LLM inference
serving. By leveraging these technologies, we demonstrate how a Cloud Native
system enables more efficient resource allocation, reduces latency, and
enhances throughput in high-demand scenarios. Through real-world evaluations
using Kubernetes-based autoscaling, we show that Cloud Native architectures can
dynamically adapt to workload fluctuations, mitigating performance bottlenecks
while optimizing LLM inference serving performance. This discussion provides a
broader perspective on how Cloud Native frameworks could reshape the future of
scalable LLM inference serving, offering key insights for researchers,
practitioners, and industry leaders in cloud computing and artificial
intelligence.

</details>


### [77] [FCPO: Federated Continual Policy Optimization for Real-Time High-Throughput Edge Video Analytics](https://arxiv.org/abs/2507.18047)
*Lucas Liebe,Thanh-Tung Nguyen,Dongman Lee*

Main category: cs.DC

TL;DR: FCPO结合持续强化学习(CRL)和联邦强化学习(FRL)，在边缘视频分析(EVA)中动态调整批量大小、分辨率和多线程，显著提升了吞吐量并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的调度系统在快速变化的边缘环境中表现不佳，局部强化学习(RL)存在扩展性和适应性不足的问题，亟需一种更高效的解决方案。

Method: 通过结合CRL和FRL，FCPO动态调整推理阶段的参数，并利用多样性感知的经验缓冲区和代理特定的聚合方案。

Result: 在实际EVA测试中，FCPO的吞吐量提升了5倍，延迟降低60%，收敛速度加快20%，且内存消耗减少10倍。

Conclusion: FCPO为边缘视频分析中的实时推理调度提供了一种高效、可扩展且适应性强的解决方案。

Abstract: The growing complexity of Edge Video Analytics (EVA) facilitates new kind of
intelligent applications, but creates challenges in real-time inference serving
systems. State-of-the-art (SOTA) scheduling systems optimize global workload
distributions for heterogeneous devices but often suffer from extended
scheduling cycles, leading to sub-optimal processing in rapidly changing Edge
environments. Local Reinforcement Learning (RL) enables quick adjustments
between cycles but faces scalability, knowledge integration, and adaptability
issues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated
RL (FRL) to address these challenges. This integration dynamically adjusts
inference batch sizes, input resolutions, and multi-threading during pre- and
post-processing. CRL allows agents to learn from changing Markov Decision
Processes, capturing dynamic environmental variations, while FRL improves
generalization and convergence speed by integrating experiences across
inference models. FCPO combines these via an agent-specific aggregation scheme
and a diversity-aware experience buffer. Experiments on a real-world EVA
testbed showed over 5 times improvement in effective throughput, 60% reduced
latency, and 20% faster convergence with up to 10 times less memory consumption
compared to SOTA RL-based approaches.

</details>


### [78] [A large-scale distributed parallel discrete event simulation engines based on Warped2 for Wargaming simulation](https://arxiv.org/abs/2507.18050)
*Xiaoning Jia,Ruilin Kong,Guangya Si,Bilong Shen,Zhe Ji*

Main category: cs.DC

TL;DR: 优化后的PDES框架通过异步监听线程、负载重新平衡策略、实体交互求解器和空间哈希算法解决了传统引擎的资源分配和交互问题，显著提升了计算效率和同步性能。


<details>
  <summary>Details</summary>
Motivation: 传统PDES引擎在大规模仿真中存在资源分配不高效和复杂实体交互未优化的问题，需要一种更高效的解决方案。

Method: 提出了四种协同优化：异步监听线程、METIS负载平衡策略、实体交互求解器和空间哈希算法。

Result: 实验显示框架比基线快16倍，同步开销减少58.18%，负载平衡贡献了57%的改进。

Conclusion: 优化后的框架为大规模仿真中的PDES实现提供了高效解决方案。

Abstract: Rising demand for complex simulations highlights conventional
engines'scalability limits, spurring Parallel Discrete Event Simulation (PDES)
adoption.Warped2, a PDES engine leveraging Time Warp synchronization with
Pending Event Set optimization, delivers strong performance, it struggles with
inherent wargaming limitations: inefficient LP resource allocation during
synchronization and unaddressed complex entity interaction patterns. To address
these challenges, we present an optimized framework featuring four synergistic
improvements: (1) Asynchronous listener threads are introduced to address event
monitoring latency in large-scale scenarios, instead of synchronous polling
mechanisms, (2) METIS-based load rebalancing strategy is incorporated to
address the issue of dynamic event allocation during real-world simulation, (3)
Entity interaction solver with constraint satisfaction mechanisms is designed
to mitigate state conflicts, and (4) Spatial hashing algorithm to overcome
O(n^2) complexity bottlenecks in large-scale nearest-neighbor searches.
Experimental validation through a GridWorld demo demonstrates significant
enhancements in temporal fidelity and computational efficiency. Benchmark
results show our framework achieves 16x acceleration over baseline
implementations and maintains 8x speedup over 1-thread configuration across MPI
and Pthreads implementations.The combined load balancing and LP migration
strategy reduces synchronization overhead by 58.18%, with load balancing
accounting for 57% of the total improvement as the dominant optimization
factor. These improvements provide an enhanced solution for PDES implementation
in large-scale simulation scenarios.

</details>


### [79] [Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning](https://arxiv.org/abs/2507.18459)
*Amir Najjar,Riad Mokadem,Jean-Marc Pierson*

Main category: cs.DC

TL;DR: 提出了一种基于强化学习的云系统数据复制策略，动态适应工作负载变化，优化服务质量与资源利用。


<details>
  <summary>Details</summary>
Motivation: 传统阈值激活机制依赖人工调整，难以适应动态工作负载和架构变化。

Method: 利用强化学习模型（定义状态、动作、奖励）自动学习系统特性，优化服务质量和资源效率。

Result: 该方法在动态环境中自动适应，平衡服务提供商利润与环保影响。

Conclusion: 强化学习的动态适应性为云数据复制提供了更高效的解决方案。

Abstract: The rapid growth of global data volumes has created a demand for scalable
distributed systems that can maintain a high quality of service. Data
replication is a widely used technique that provides fault tolerance, improved
performance and higher availability. Traditional implementations often rely on
threshold-based activation mechanisms, which can vary depending on workload
changes and system architecture. System administrators typically bear the
responsibility of adjusting these thresholds. To address this challenge,
reinforcement learning can be used to dynamically adapt to workload changes and
different architectures. In this paper, we propose a novel data replication
strategy for cloud systems that employs reinforcement learning to automatically
learn system characteristics and adapt to workload changes. The strategy's aim
is to provide satisfactory Quality of Service while optimizing a trade-off
between provider profit and environmental impact. We present the architecture
behind our solution and describe the reinforcement learning model by defining
the states, actions and rewards.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [80] [An advanced AI driven database system](https://arxiv.org/abs/2507.17778)
*M. Tedeschi,S. Rizwan,C. Shringi,V. Devram Chandgir,S. Belich*

Main category: cs.DB

TL;DR: 提出了一种基于AI的新数据库系统，旨在通过自然语言处理和自动化任务减少技术门槛和人为错误。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据库系统对非技术用户不友好的问题，尤其是SQL等查询语言的复杂性。

Method: 结合大型语言模型和机器学习算法，实现自然语言接口、自动化查询生成和数据结构推断。

Result: 开发了支持自然语言交互和自动化任务（如数据建模和性能优化）的AI数据库系统。

Conclusion: 该系统有望降低技术门槛，提高数据库的易用性和效率，减少人为错误。

Abstract: Contemporary database systems, while effective, suffer severe issues related
to complexity and usability, especially among individuals who lack technical
expertise but are unfamiliar with query languages like Structured Query
Language (SQL). This paper presents a new database system supported by
Artificial Intelligence (AI), which is intended to improve the management of
data using natural language processing (NLP) - based intuitive interfaces, and
automatic creation of structured queries and semi-structured data formats like
yet another markup language (YAML), java script object notation (JSON), and
application program interface (API) documentation. The system is intended to
strengthen the potential of databases through the integration of Large Language
Models (LLMs) and advanced machine learning algorithms. The integration is
purposed to allow the automation of fundamental tasks such as data modeling,
schema creation, query comprehension, and performance optimization. We present
in this paper a system that aims to alleviate the main problems with current
database technologies. It is meant to reduce the need for technical skills,
manual tuning for better performance, and the potential for human error. The AI
database employs generative schema inference and format selection to build its
schema models and execution formats.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [81] [Designing High-Performance and Thermally Feasible Multi-Chiplet Architectures enabled by Non-bendable Glass Interposer](https://arxiv.org/abs/2507.18040)
*Harsh Sharma,Janardhan Rao Doppa,Umit Y. Ogras,Partha Pratim Pande*

Main category: cs.AR

TL;DR: 玻璃中介层多芯片架构电气性能优越，但系统尺寸增大时变形问题显著。作者提出热、变形和性能感知的设计框架，通过架构和封装协同优化，实现性能提升和功耗降低。


<details>
  <summary>Details</summary>
Motivation: 解决玻璃中介层多芯片系统因尺寸增大导致的变形问题，同时优化性能和功耗。

Method: 提出一种热、变形和性能感知的设计框架，通过架构和封装协同优化，分解表面和嵌入式芯片以平衡设计目标。

Result: 优化后的架构在深度神经网络任务中性能提升64.7%，功耗降低40%，且制造成本更低。

Conclusion: 该框架成功解决了玻璃中介层系统的关键挑战，实现了性能与可靠性的优化。

Abstract: Multi-chiplet architectures enabled by glass interposer offer superior
electrical performance, enable higher bus widths due to reduced crosstalk, and
have lower capacitance in the redistribution layer than current silicon
interposer-based systems. These advantages result in lower energy per bit,
higher communication frequencies, and extended interconnect range. However,
deformation of the package (warpage) in glass interposer-based systems becomes
a critical challenge as system size increases, leading to severe mechanical
stress and reliability concerns. Beyond a certain size, conventional packaging
techniques fail to manage warpage effectively, necessitating new approaches to
mitigate warpage induced bending with scalable performance for glass interposer
based multi-chiplet systems. To address these inter-twined challenges, we
propose a thermal-, warpage-, and performance-aware design framework that
employs architecture and packaging co-optimization. The proposed framework
disintegrates the surface and embedded chiplets to balance conflicting design
objectives, ensuring optimal trade-offs between performance, power, and
structural reliability. Our experiments demonstrate that optimized
multi-chiplet architectures from our design framework achieve up to 64.7%
performance improvement and 40% power reduction compared to traditional 2.5D
systems to execute deep neural network workloads with lower fabrication costs.

</details>


### [82] [Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving](https://arxiv.org/abs/2507.18454)
*Juntao Zhao,Jiuru Li,Chuan Wu*

Main category: cs.AR

TL;DR: Sandwich是一种基于CPU的LLM服务引擎，通过区分预填充和解码阶段的执行计划并分别优化，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于CPU的解决方案忽视了LLM推理中预填充和解码阶段的工作负载差异，导致性能不优。Sandwich旨在解决这一问题。

Method: Sandwich采用硬件中心的设计，为预填充和解码阶段制定不同的执行计划，并分别优化，同时优化GEMM内核的性能。

Result: 在多种CPU平台上，Sandwich平均提升了2.01倍的吞吐量，降低了延迟，并在单序列和连续批处理服务中表现出色。

Conclusion: Sandwich通过动态优化不同阶段的执行计划，显著提高了CPU在LLM服务中的效率和性能。

Abstract: Utilizing CPUs to serve large language models (LLMs) is a resource-friendly
alternative to GPU serving. Existing CPU-based solutions ignore workload
differences between the prefill and the decode phases of LLM inference,
applying a static per-NUMA (Non-Uniform Memory Access) node model partition and
utilizing vendor libraries for operator-level execution, which is suboptimal.
We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses
different execution plans for the prefill and decode phases and optimizes them
separately.
  We evaluate Sandwich across diverse baselines and datasets on five CPU
platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.
Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory
time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up
to 3.40x lower requirements in single sequence serving, and significant
improvement in Goodput in continuous-batching serving. The GEMM kernels
generated by Sandwich outperform representative vendor kernels and other
dynamic shape solutions, achieving performance comparable to static compilers
with three orders of magnitude less kernel tuning costs.

</details>


### [83] [PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation](https://arxiv.org/abs/2507.18581)
*Ravan Nazaraliyev,Saber Ganjisaffar,Nurlan Nazaraliyev,Nael Abu-Ghazaleh*

Main category: cs.AR

TL;DR: PRACtical优化了DDR5中的PRAC和ABO机制，通过集中计数器和银行级粒度提升性能，同时保持安全性。


<details>
  <summary>Details</summary>
Motivation: 随着DRAM密度增加，Rowhammer攻击变得更加严重，现有PRAC机制存在性能开销和通道停滞问题。

Method: PRACtical引入集中计数器减少延迟，并采用银行级粒度暂停受攻击银行，而非整个通道。

Result: 平均性能提升8%（最高20%），能耗降低19%，性能下降控制在6%以内。

Conclusion: PRACtical在保持安全性的同时显著优化性能，适用于高密度DRAM环境。

Abstract: As DRAM density increases, Rowhammer becomes more severe due to heightened
charge leakage, reducing the number of activations needed to induce bit flips.
The DDR5 standard addresses this threat with in-DRAM per-row activation
counters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.
However, PRAC adds performance overhead by incrementing counters during the
precharge phase, and recovery refreshes stalls the entire memory channel, even
if only one bank is under attack.
  We propose PRACtical, a performance-optimized approach to PRAC+ABO that
maintains the same security guarantees. First, we reduce counter update latency
by introducing a centralized increment circuit, enabling overlap between
counter updates and subsequent row activations in other subarrays. Second, we
enhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead
of stalling the entire channel, only affected banks are paused. This is
achieved through a DRAM-resident register that identifies attacked banks.
  PRACtical improves performance by 8% on average (up to 20%) over the
state-of-the-art, reduces energy by 19%, and limits performance degradation
from aggressive performance attacks to less than 6%, all while preserving
Rowhammer protection.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [84] [ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation](https://arxiv.org/abs/2507.18262)
*Chenyu Su,Weiwei Shang,Chen Qian,Fei Zhang,Shuang Cong*

Main category: cs.RO

TL;DR: ReSem3D是一个统一的机器人操作框架，通过结合VFM和MLLM，实现细粒度视觉定位和动态3D空间约束，解决了现有方法在语义建模、实时闭环规划和鲁棒性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D空间约束建模中存在语义粒度粗、缺乏实时闭环规划和鲁棒性不足的问题，因此提出了ReSem3D框架来解决这些问题。

Method: ReSem3D利用MLLM的分层递归推理与VFM交互，从自然语言指令和RGB-D观测中分阶段自动构建3D空间约束（部件级提取和区域级细化），并将其编码为实时优化目标。

Result: 在语义丰富的家庭和稀疏的化学实验室环境中进行的仿真和实物实验表明，ReSem3D在零样本条件下完成多样操作任务，表现出强适应性和泛化能力。

Conclusion: ReSem3D框架通过动态构建精细的3D空间约束，显著提升机器人操作的语义理解能力和实时响应性能。

Abstract: Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [85] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: VeriMinder是一个交互式系统，用于检测和减轻数据分析中的认知偏差，通过上下文语义映射、Hard-to-Vary原则和优化的LLM生成提示，显著提高了分析质量。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言数据库接口的普及，帮助缺乏统计背景的用户提出无偏差分析问题成为迫切需求，VeriMinder旨在填补这一研究空白。

Method: 采用上下文语义映射框架、Hard-to-Vary原则指导的分析框架以及优化的LLM生成提示方法，通过多候选、批评反馈和自反思结构化的过程生成高质量提示。

Result: 用户体验评估中82.5%的参与者认为其提升了分析质量；比较评估中VeriMinder在具体性、全面性和准确性上至少高出20%。

Conclusion: VeriMinder作为开源系统，有效帮助用户避免“错误问题”漏洞，适用于数据分析社区。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [86] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 研究探讨了维基百科多语言表格数据的跨语言不一致性，开发了相关分析方法，并提出了对AI系统和知识验证的启示。


<details>
  <summary>Details</summary>
Motivation: 维基百科不同语言版本的内容独立更新，导致事实不一致，影响其中立性和可靠性，尤其是AI系统依赖维基百科作为训练数据时。

Method: 开发了一套方法，用于收集、对齐和分析维基百科多语言文章中的表格数据，定义了不一致的类别，并应用定量和定性指标评估数据集。

Result: 研究发现多语言表格数据存在不一致性，对AI系统和知识交互设计提出了挑战。

Conclusion: 研究结果对事实验证、多语言知识交互及依赖维基百科的可靠AI系统设计具有重要意义。

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [87] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: LLMs在道德推理任务中表现不佳，特定任务的微调仍优于提示。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型在专业道德推理任务中的表现，以支持伦理对齐的AI系统开发。

Method: 在Twitter和Reddit数据集上比较LLMs与微调transformer，使用ROC、PR和DET曲线分析。

Result: LLMs表现明显不足，存在高假阴性率和对道德内容的系统低估。

Conclusion: 道德推理应用中，微调方法优于提示法。

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [88] [WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection](https://arxiv.org/abs/2507.18173)
*Haodong Zhu,Wenhao Dong,Linlin Yang,Hong Li,Yuguang Yang,Yangyang Ren,Qingcheng Zhu,Zichao Feng,Changbai Li,Shaohui Lin,Runqi Wang,Xiaoyan Luo,Baochang Zhang*

Main category: cs.CV

TL;DR: WaveMamba是一种跨模态融合方法，通过离散小波变换分解RGB和红外图像的独特频率特征，结合改进的检测头和逆离散小波变换，显著提升目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 利用RGB和红外图像的互补特性提升目标检测性能。

Method: 提出WaveMamba跨模态融合方法，结合离散小波变换和改进的检测头，通过WaveMamba Fusion Block实现低频和高频特征的深度融合。

Result: 在四个基准测试中平均mAP提升4.5%，超越现有最优方法。

Conclusion: WaveMamba方法显著提升了跨模态目标检测的性能。

Abstract: Leveraging the complementary characteristics of visible (RGB) and infrared
(IR) imagery offers significant potential for improving object detection. In
this paper, we propose WaveMamba, a cross-modality fusion method that
efficiently integrates the unique and complementary frequency features of RGB
and IR decomposed by Discrete Wavelet Transform (DWT). An improved detection
head incorporating the Inverse Discrete Wavelet Transform (IDWT) is also
proposed to reduce information loss and produce the final detection results.
The core of our approach is the introduction of WaveMamba Fusion Block (WMFB),
which facilitates comprehensive fusion across low-/high-frequency sub-bands.
Within WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba
framework, first performs initial low-frequency feature fusion with channel
swapping, followed by deep fusion with an advanced gated attention mechanism
for enhanced integration. High-frequency features are enhanced using a strategy
that applies an ``absolute maximum" fusion approach. These advancements lead to
significant performance gains, with our method surpassing state-of-the-art
approaches and achieving average mAP improvements of 4.5% on four benchmarks.

</details>


### [89] [3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation](https://arxiv.org/abs/2507.18625)
*Shuqing Li,Anson Y. Lam,Yun Peng,Wenxuan Wang,Michael R. Lyu*

Main category: cs.CV

TL;DR: Scenethesis 是一种新的 3D 软件合成方法，通过域特定语言 ScenethesisLang 实现用户需求与生成 3D 软件之间的形式化追溯，显著提升了需求的准确性和约束满足度。


<details>
  <summary>Details</summary>
Motivation: 为解决现有 3D 软件生成方法难以处理复杂空间和语义约束且无法细粒度修改的问题。

Method: 基于 ScenethesisLang（一种约束感知的中间表示语言），将 3D 软件合成分解为多个阶段，支持细粒度修改和约束表达。

Result: 准确捕捉 80% 以上用户需求，满足 90% 以上硬约束，视觉评估得分提升 42.8%。

Conclusion: Scenethesis 在 3D 软件生成中表现出色，通过形式化语言和分阶段合成实现了高效的需求满足和约束处理。

Abstract: Graphical user interface (UI) software has undergone a fundamental
transformation from traditional two-dimensional (2D) desktop/web/mobile
interfaces to spatial three-dimensional (3D) environments. While existing work
has made remarkable success in automated 2D software generation, such as
HTML/CSS and mobile app interface code synthesis, the generation of 3D software
still remains under-explored. Current methods for 3D software generation
usually generate the 3D environments as a whole and cannot modify or control
specific elements in the software. Furthermore, these methods struggle to
handle the complex spatial and semantic constraints inherent in the real world.
To address the challenges, we present Scenethesis, a novel
requirement-sensitive 3D software synthesis approach that maintains formal
traceability between user specifications and generated 3D software. Scenethesis
is built upon ScenethesisLang, a domain-specific language that serves as a
granular constraint-aware intermediate representation (IR) to bridge natural
language requirements and executable 3D software. It serves both as a
comprehensive scene description language enabling fine-grained modification of
3D software elements and as a formal constraint-expressive specification
language capable of expressing complex spatial constraints. By decomposing 3D
software synthesis into stages operating on ScenethesisLang, Scenethesis
enables independent verification, targeted modification, and systematic
constraint satisfaction. Our evaluation demonstrates that Scenethesis
accurately captures over 80% of user requirements and satisfies more than 90%
of hard constraints while handling over 100 constraints simultaneously.
Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual
evaluation scores compared to the state-of-the-art method.

</details>


### [90] [SIDA: Synthetic Image Driven Zero-shot Domain Adaptation](https://arxiv.org/abs/2507.18632)
*Ye-Chan Kim,SeungJu Cha,Si-Woo Kim,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 提出了一种基于合成图像的零样本领域自适应方法SIDA，通过图像翻译生成目标领域风格特征，并引入域混合和补丁风格转移模块，显著提升了适应效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动方法难以捕捉真实世界的复杂变化且耗时较长，因此探索利用图像数据提供更细粒度的风格线索。

Method: 通过图像翻译生成合成图像作为目标领域代理，引入域混合和补丁风格转移模块建模真实世界变化。

Result: 在多种零样本适应场景中表现最优，尤其在挑战性领域，同时大幅减少适应时间。

Conclusion: SIDA通过合成图像和高效模块设计，实现了高效的零样本领域自适应，解决了现有方法的局限性。

Abstract: Zero-shot domain adaptation is a method for adapting a model to a target
domain without utilizing target domain image data. To enable adaptation without
target images, existing studies utilize CLIP's embedding space and text
description to simulate target-like style features. Despite the previous
achievements in zero-shot domain adaptation, we observe that these text-driven
methods struggle to capture complex real-world variations and significantly
increase adaptation time due to their alignment process. Instead of relying on
text descriptions, we explore solutions leveraging image data, which provides
diverse and more fine-grained style cues. In this work, we propose SIDA, a
novel and efficient zero-shot domain adaptation method leveraging synthetic
images. To generate synthetic images, we first create detailed, source-like
images and apply image translation to reflect the style of the target domain.
We then utilize the style features of these synthetic images as a proxy for the
target domain. Based on these features, we introduce Domain Mix and Patch Style
Transfer modules, which enable effective modeling of real-world variations. In
particular, Domain Mix blends multiple styles to expand the intra-domain
representations, and Patch Style Transfer assigns different styles to
individual patches. We demonstrate the effectiveness of our method by showing
state-of-the-art performance in diverse zero-shot adaptation scenarios,
particularly in challenging domains. Moreover, our approach achieves high
efficiency by significantly reducing the overall adaptation time.

</details>


### [91] [Real-Time Object Detection and Classification using YOLO for Edge FPGAs](https://arxiv.org/abs/2507.18174)
*Rashed Al Amin,Roman Obermaisser*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv5的资源高效实时对象检测与分类系统，适用于FPGA部署，分类准确率达99%，功耗3.5W，处理速度9FPS。


<details>
  <summary>Details</summary>
Motivation: 尽管现有基于YOLO的检测系统在FPGA上表现优异，但在边缘计算平台的资源效率方面仍存在挑战。

Method: 采用YOLOv5优化架构，在COCO和GTSRD数据集上训练，并部署于Xilinx Kria KV260 FPGA板。

Result: 分类精度达99%，功耗3.5W，处理速度9FPS。

Conclusion: 该方法能有效支持边缘计算中的实时高效对象检测与分类。

Abstract: Object detection and classification are crucial tasks across various
application domains, particularly in the development of safe and reliable
Advanced Driver Assistance Systems (ADAS). Existing deep learning-based methods
such as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and
You Only Look Once (YOLO) have demonstrated high performance in terms of
accuracy and computational speed when deployed on Field-Programmable Gate
Arrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based
object detection and classification systems continue to face challenges in
achieving resource efficiency suitable for edge FPGA platforms. To address this
limitation, this paper presents a resource-efficient real-time object detection
and classification system based on YOLOv5 optimized for FPGA deployment. The
proposed system is trained on the COCO and GTSRD datasets and implemented on
the Xilinx Kria KV260 FPGA board. Experimental results demonstrate a
classification accuracy of 99%, with a power consumption of 3.5W and a
processing speed of 9 frames per second (FPS). These findings highlight the
effectiveness of the proposed approach in enabling real-time,
resource-efficient object detection and classification for edge computing
applications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [92] [Weaving the Future: Generative AI and the Reimagining of Fashion Design](https://arxiv.org/abs/2507.17758)
*Pierre-Marie Chauvin,Angèle Merlin,Xavier Fresquet,Hugo Caselles-Dupré,Benjamin Simmenauer,Mathieu de Fayet*

Main category: cs.CY

TL;DR: 论文探讨生成式AI在时尚设计中的应用，分析其对创意流程的影响及伦理、美学和劳动问题。


<details>
  <summary>Details</summary>
Motivation: 研究AI如何改变时尚设计流程，并探讨其带来的伦理、美学和劳动挑战。

Method: 基于2025年1月研讨会“编织未来”的见解，分析AI从构思到原型设计的创意流程。

Result: 揭示了人类与机器共创的动态关系，以及算法设计在美学创新和环境文化方面的潜力与挑战。

Conclusion: 生成式AI为时尚设计带来创新机遇，但也需解决伦理、环境和文化问题。

Abstract: This paper explores the integration of generative AI into the fashion design
process. Drawing on insights from the January 2025 seminar ``Tisser le futur,''
it investigates how AI reshapes creative workflows, from ideation to
prototyping, while interrogating the ethical, aesthetic, and labor
implications. The paper highlights co-creative dynamics between humans and
machines, the potential for aesthetic innovation, and the environmental and
cultural challenges of algorithmic design.

</details>


### [93] [How Instructional Sequence and Personalized Support Impact Diagnostic Strategy Learning](https://arxiv.org/abs/2507.17760)
*Fatma Betül Güreş,Tanya Nazaretsky,Bahar Radmehr,Martina Rau,Tanja Käser*

Main category: cs.CY

TL;DR: 研究探讨了在情景模拟学习中个性化支持的指令顺序对诊断推理能力的影响，发现先解决问题再指导（PS-I）比先指导后解决问题（I-PS）更有利于知识迁移。


<details>
  <summary>Details</summary>
Motivation: 解决学生在诊断推理中常见的认知偏差问题，探索情景模拟学习中个性化支持的最佳指令顺序。

Method: 采用组间设计，在模拟药店技术员学徒真实客户互动的在线环境PharmaSim中比较I-PS和PS-I的效果。

Result: 两种指令类型均有益，但PS-I在迁移任务中表现显著更优。

Conclusion: 在情景模拟学习中，先解决问题再提供指导更有利于提高学生的知识迁移能力。

Abstract: Supporting students in developing effective diagnostic reasoning is a key
challenge in various educational domains. Novices often struggle with cognitive
biases such as premature closure and over-reliance on heuristics.
Scenario-based learning (SBL) can address these challenges by offering
realistic case experiences and iterative practice, but the optimal sequencing
of instruction and problem-solving activities remains unclear. This study
examines how personalized support can be incorporated into different
instructional sequences and whether providing explicit diagnostic strategy
instruction before (I-PS) or after problem-solving (PS-I) improves learning and
its transfer. We employ a between-groups design in an online SBL environment
called PharmaSim, which simulates real-world client interactions for pharmacy
technician apprentices. Results indicate that while both instruction types are
beneficial, PS-I leads to significantly higher performance in transfer tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 本文提出了一种基于Agentic AI的框架，通过模块化的任务特定代理自动化临床数据从输入到推理的整个流程，减少医疗环境中的AI部署成本和复杂性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的机器学习解决方案因数据预处理碎片化、模型兼容性问题和严格的隐私限制而成本高昂且劳动密集型。

Method: 框架通过多个代理处理结构化和非结构化数据，实现自动特征选择、模型选择和预处理推荐，包括数据匿名化、特征提取、模型匹配和推理输出生成。

Result: 在老年医学、姑息治疗和结肠镜成像等公开数据集上验证了框架的有效性，能够显著减少专家干预需求。

Conclusion: 该框架为临床环境中AI的规模化、低成本部署提供了一条可行路径。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [95] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: 论文研究了使用均值聚合函数的图神经网络（GNNs）的表达能力，发现其在非均匀设置下与比率模态逻辑等效，而在均匀设置下其表达能力低于使用求和或最大聚合的GNNs。


<details>
  <summary>Details</summary>
Motivation: 探讨均值聚合GNNs的表达能力，并与不同聚合方式的GNNs进行比较，以揭示其在不同设置下的理论表现。

Method: 通过非均匀和均匀设置下的分析，比较均值聚合GNNs与比率模态逻辑、模态逻辑及分级模态逻辑的对应关系。

Result: 在非均匀设置下，均值GNNs的表达能力高于最大聚合GNNs但低于求和聚合GNNs；在均匀设置下，均值GNNs的表达能力相对MSO更低。

Conclusion: 均值GNNs的表达能力受设置和假设影响，放松假设可提升其表达力，但总体仍弱于求和或最大聚合GNNs。

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [96] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: 研究探讨图表是否能帮助AI系统分析数据，实验显示视觉语言模型在数据可视化时表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索图表是否对AI系统分析数据有帮助，类似于人类使用图表的方式。

Method: 使用GPT 4.1和Claude 3.5两种视觉语言模型，对比三种分析任务中图表对性能的影响。

Result: 图表辅助的模型表现更精确，尤其在复杂数据集上，且图表内容对提升性能起关键作用。

Conclusion: 初步证明AI系统也能受益于数据可视化。

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [97] [Building an Accelerated OpenFOAM Proof-of-Concept Application using Modern C++](https://arxiv.org/abs/2507.18268)
*Giulio Malenza,Giovanni Stabile,Filippo Spiga,Robert Birke,Marco Aldinucci*

Main category: cs.MS

TL;DR: 该论文探讨了利用现代C++并行结构和高性能计算工具链（如NVIDIA HPC SDK）加速OpenFOAM应用，通过GPU卸载提升性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高高性能计算（HPC）中OpenFOAM应用的性能，研究利用了现代GPU和CPU加速器以及混合精度等特性，旨在通过软件优化提升计算效率。

Method: 采用现代ISO C++并行结构构建概念验证应用，结合NVIDIA HPC SDK等工具链，实现多核执行和GPU卸载的统一代码库。

Result: 研究成功通过GPU卸载提升了OpenFOAM的laplacianFoam应用的性能。

Conclusion: 研究表明，利用C++并行结构和GPU加速是提升OpenFOAM性能的有效途径。

Abstract: The modern trend in High-Performance Computing (HPC) involves the use of
accelerators such as Graphics Processing Units (GPUs) alongside Central
Processing Units (CPUs) to speed up numerical operations in various
applications. Leading manufacturers such as NVIDIA, Intel, and AMD are
constantly advancing these architectures, augmenting them with features such as
mixed precision, enhanced memory hierarchies, and specialised accelerator
silicon blocks (e.g., Tensor Cores on GPU or AMX/SME engines on CPU) to enhance
compute performance. At the same time, significant efforts in software
development are aimed at optimizing the use of these innovations, seeking to
improve usability and accessibility. This work contributes to the
state-of-the-art of OpenFOAM development by presenting a working
Proof-Of-Concept application built using modern ISO C++ parallel constructs.
This approach, combined with an appropriate compiler runtime stack, like the
one provided by the NVIDIA HPC SDK, makes it possible to accelerate
well-defined kernels, allowing multi-core execution and GPU offloading using a
single codebase. The study demonstrates that it is possible to increase the
performance of the OpenFOAM laplacianFoam application by offloading the
computations on NVIDIA GPUs using the C++ parallel construct.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [98] [Formal Verification of the Safegcd Implementation](https://arxiv.org/abs/2507.17956)
*Russell O'Connor,Andrew Poelstra*

Main category: cs.CR

TL;DR: 摘要介绍了一种新的扩展欧几里得算法用于比特币的模块逆运算，并通过Coq验证了其正确性。


<details>
  <summary>Details</summary>
Motivation: 模块逆运算在比特币等数字签名应用中至关重要，但新算法可能引入错误风险，因此需要验证其正确性。

Method: 使用Coq证明助手和Verifiable C的分离逻辑实现，验证了libsecp256k1库中模块逆运算的正确性。

Result: 成功完成了模块逆运算实现的计算机验证证明。

Conclusion: 验证结果表明新算法的实现是正确的，降低了潜在的错误风险。

Abstract: The modular inverse is an essential piece of computation required for
elliptic curve operations used for digital signatures in Bitcoin and other
applications. A novel approach to the extended Euclidean algorithm has been
developed by Bernstein and Yang within the last few years and incorporated into
the libsecp256k1 cryptographic library used by Bitcoin. However, novel
algorithms introduce new risks of errors. To address this we have completed a
computer verified proof of the correctness of (one of) libsecp256k1's modular
inverse implementations with the Coq proof assistant using the Verifiable C's
implementation of separation logic.

</details>


### [99] [MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection](https://arxiv.org/abs/2507.17978)
*Paulo Mendes,Eva Maia,Isabel Praça*

Main category: cs.CR

TL;DR: 该论文提出了一个名为MeAJOR的新型多源钓鱼邮件数据集，通过整合多种来源的数据和广泛的特征工程，解决了现有数据集的局限性。实验表明，该数据集在钓鱼邮件检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 钓鱼邮件通过利用人类弱点对网络安全构成威胁，而机器学习模型的性能依赖于高质量和多样化的训练数据，现有的数据集存在不足。

Method: 整合了135,894个样本，涵盖多种钓鱼策略和合法邮件，设计了广泛的特征工程，并通过四种分类模型评估数据集的有效性。

Result: 使用XGB模型时，F1分数达到98.34%，显示出数据集的优越性能。

Conclusion: MeAJOR数据集通过整合多类特征，解决了类别不平衡、泛化性和可重复性等挑战，为钓鱼检测研究提供了高质量资源。

Abstract: Phishing emails continue to pose a significant threat to cybersecurity by
exploiting human vulnerabilities through deceptive content and malicious
payloads. While Machine Learning (ML) models are effective at detecting
phishing threats, their performance largely relies on the quality and diversity
of the training data. This paper presents MeAJOR (Merged email Assets from
Joint Open-source Repositories) Corpus, a novel, multi-source phishing email
dataset designed to overcome critical limitations in existing resources. It
integrates 135894 samples representing a broad number of phishing tactics and
legitimate emails, with a wide spectrum of engineered features. We evaluated
the dataset's utility for phishing detection research through systematic
experiments with four classification models (RF, XGB, MLP, and CNN) across
multiple feature configurations. Results highlight the dataset's effectiveness,
achieving 98.34% F1 with XGB. By integrating broad features from multiple
categories, our dataset provides a reusable and consistent resource, while
addressing common challenges like class imbalance, generalisability and
reproducibility.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [100] [Quantum Machine Learning Playground](https://arxiv.org/abs/2507.17931)
*Pascal Debus,Sebastian Issel,Kilian Tscharke*

Main category: quant-ph

TL;DR: 介绍了一个创新的交互式可视化工具，旨在简化量子机器学习（QML）算法的理解。


<details>
  <summary>Details</summary>
Motivation: 受经典机器学习可视化工具成功的启发，填补QML领域可视化资源的空白。

Method: 结合量子计算和经典机器学习的可视化隐喻，开发算法可视化概念，并设计为交互式网页应用。

Result: 提供了一个量子机器学习交互应用，降低量子计算的入门门槛。

Conclusion: 该工具有助于促进QML领域的进一步创新和学习。

Abstract: This article introduces an innovative interactive visualization tool designed
to demystify quantum machine learning (QML) algorithms. Our work is inspired by
the success of classical machine learning visualization tools, such as
TensorFlow Playground, and aims to bridge the gap in visualization resources
specifically for the field of QML. The article includes a comprehensive
overview of relevant visualization metaphors from both quantum computing and
classical machine learning, the development of an algorithm visualization
concept, and the design of a concrete implementation as an interactive web
application. By combining common visualization metaphors for the so-called data
re-uploading universal quantum classifier as a representative QML model, this
article aims to lower the entry barrier to quantum computing and encourage
further innovation in the field. The accompanying interactive application is a
proposal for the first version of a quantum machine learning playground for
learning and exploring QML models.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [101] [Topology-Preserving Coupling of Compressible Fluids and Thin Deformables](https://arxiv.org/abs/2507.18460)
*Jonathan Panuelos,Eitan Grinspun,David Levin*

Main category: physics.comp-ph

TL;DR: 提出了一种新的离散化方法，用于耦合可压缩流体和薄变形结构，确保流体域路径连通性，防止泄漏。


<details>
  <summary>Details</summary>
Motivation: 解决流体与薄变形结构耦合时泄漏问题，并精确处理流体-固体界面条件。

Method: 采用约束Voronoi空间分割与Godunov风格有限容积时间积分结合的方法，流体域离散化为与界面精确匹配的单元。

Result: 在多个挑战性场景中验证了方法的有效性，如气球、香槟瓶塞和超音速小行星，展示了流固双向能量传递。

Conclusion: 该方法成功实现了无泄漏的流固耦合模拟，且能精确处理薄结构界面。

Abstract: We present a novel discretization of coupled compressible fluid and thin
deformable structures that provides sufficient and necessary leakproofness by
preserving the path connectedness of the fluid domain. Our method employs a
constrained Voronoi-based spatial partitioning combined with Godunov-style
finite-volume time integration. The fluid domain is discretized into cells that
conform exactly to the fluid-solid interface, allowing boundary conditions to
be sharply resolved exactly at the interface. This enables direct force
exchange between the fluid and solid while ensuring that no fluid leaks through
the solid, even when arbitrarily thin. We validate our approach on a series of
challenging scenarios -- including a balloon propelled by internal compressed
air, a champagne cork ejecting after overcoming friction, and a supersonic
asteroid -- demonstrating bidirectional energy transfer between fluid and
solid.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [102] [Fagin's Theorem for Semiring Turing Machines](https://arxiv.org/abs/2507.18375)
*Guillermo Badia,Manfred Droste,Thomas Eiter,Rafael Kiesel,Carles Noguera,Erik Paul*

Main category: cs.CC

TL;DR: 该论文改进了半环图灵机模型，提出了一个Fagin风格的定理，用于定量复杂性类 NPnewinf{R}，并建立了其与加权存在二阶逻辑的联系。


<details>
  <summary>Details</summary>
Motivation: 研究半环上的定量复杂性，并连接计算复杂性与逻辑表达能力。

Method: 提出新的半环图灵机模型，使用加权存在二阶逻辑描述定量复杂性类 NPnewinf{R}。

Result: 证明了 NPnewinf{R} 可以用加权逻辑形式精确刻画，并比较了其与 Eiter & Kiesel 提出的 NPoldinf{R} 的关系。

Conclusion: 新模型不仅扩展了定量复杂性的逻辑表征能力，还能涵盖已有复杂性结果。

Abstract: In recent years, quantitative complexity over semirings has been intensively
investigated. An important problem in this context is to connect computational
complexity with logical expressiveness. In this paper we improve on the model
of \emph{Semiring Turing Machines} (distinct from so called weighted Turing
machines) introduced by Eiter \& Kiesel (Semiring Reasoning Frameworks in AI
and Their Computational Complexity, \emph{J. Artif. Intell. Res.}, 2023). Our
central result is a Fagin-style theorem for a new quantitative complexity class
using a suitable weighted logical formalism. We show that the quantitative
complexity class that we call \NPnewinf{$\mathcal{R}$}, where $\mathcal{R}$ is
a commutative semiring, can be captured using a version of weighted existential
second-order logic that allows for predicates interpreted as semiring-annotated
relations. This result provides a precise logical characterization of the power
series that form the class \NPnewinf{$\mathcal{R}$}. We also give the exact
relation between Eiter \& Kiesel's version of NP, called
\NPoldinf{$\mathcal{R}$}, and the class \NPnewinf{$\mathcal{R}$}. Incidentally,
we are able to recapture all the complexity results by Eiter \& Kiesel (2023)
in our new model, connecting a quantitative version of NP to various counting
complexity classes.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [103] [On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources](https://arxiv.org/abs/2507.18514)
*Jiping Luo,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 本文研究了有限状态马尔可夫链的语义感知远程估计，提出了一种基于MAP估计器的传输策略，结合AoCE和AoI优化估计性能，并开发了高效算法Insec-SPI。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合AoCE和AoI指标，优化远程估计的传输策略以提高性能。

Method: 采用MAP估计器，利用AoCE和AoI量化误差和信息陈旧性，通过CMDP建模并设计混合策略。

Result: 证明了最优混合策略的存在性，并开发了高效算法Insec-SPI，显著提升了估计质量。

Conclusion: 结合AoCE和AoI的传输策略优于单独使用任一指标，有效提高了远程估计性能。

Abstract: This paper investigates the semantics-aware remote estimation of a
finite-state Markov chain. We employ the maximum a posteriori (MAP) estimator
and aim to devise a transmission policy to optimize estimation performance
subject to a transmission frequency constraint. We leverage two metrics, namely
the Age of Consecutive Error (AoCE) and the Age of Information (AoI), to
quantify, respectively, the significance of estimation error at the transmitter
and the predictability of outdated information at the receiver. The optimal
transmission problem is formulated as a constrained Markov decision process
(CMDP) with unbounded costs. We show the existence of an optimal simple mixture
policy, which randomly selects between two deterministic switching policies
with a fixed probability. Notably, each switching policy triggers a
transmission only when the AoCE exceeds a threshold value that depends on both
the AoI and the instantaneous estimation error. We further derive sufficient
conditions under which the switching policy reduces to a simple threshold
policy; that is, it admits identical thresholds for all estimation errors.
Leveraging these results, we develop an efficient structure-aware algorithm,
Insec-SPI, that computes the optimal policy with reduced computation overhead.
Our results demonstrate that incorporating both AoI and AoCE yields
significantly improved estimation quality compared to using either metric
alone.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [104] [Neuromorphic Computing: A Theoretical Framework for Time, Space, and Energy Scaling](https://arxiv.org/abs/2507.17886)
*James B Aimone*

Main category: cs.NE

TL;DR: 论文探讨了神经形态计算（NMC）作为传统冯·诺依曼架构的低功耗替代品的潜力，重点分析了其能量效率和适用算法类别的独特性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在明确NMC的计算价值，揭示其与传统架构在时间、空间和能量扩展上的差异，并探讨其适用的算法类别。

Method: 通过比较NMC与传统架构（如CPU和GPU）的时间和空间扩展特性，特别是能量扩展的差异（传统系统能量与绝对算法工作量相关，而NMC能量与算法状态导数相关）。

Result: 研究发现，NMC在时间和空间扩展上与理论上的无限处理器传统系统等效，但能量扩展显著不同。NMC特别适用于稀疏算法和迭代优化等特定类别的算法。

Conclusion: NMC因其独特的能量扩展特性，在稀疏算法和优化问题中具有显著优势，是一种有潜力的通用可编程计算架构。

Abstract: Neuromorphic computing (NMC) is increasingly viewed as a low-power
alternative to conventional von Neumann architectures such as central
processing units (CPUs) and graphics processing units (GPUs), however the
computational value proposition has been difficult to define precisely.
  Here, we explain how NMC should be seen as general-purpose and programmable
even though it differs considerably from a conventional stored-program
architecture. We show that the time and space scaling of NMC is equivalent to
that of a theoretically infinite processor conventional system, however the
energy scaling is significantly different. Specifically, the energy of
conventional systems scales with absolute algorithm work, whereas the energy of
neuromorphic systems scales with the derivative of algorithm state. The unique
characteristics of NMC architectures make it well suited for different classes
of algorithms than conventional multi-core systems like GPUs that have been
optimized for dense numerical applications such as linear algebra. In contrast,
the unique characteristics of NMC make it ideally suited for scalable and
sparse algorithms whose activity is proportional to an objective function, such
as iterative optimization and large-scale sampling (e.g., Monte Carlo).

</details>


### [105] [Explicit Sign-Magnitude Encoders Enable Power-Efficient Multipliers](https://arxiv.org/abs/2507.18179)
*Felix Arnold,Maxence Bouvier,Ryan Amaudruz,Renzo Andri,Lukas Cavigelli*

Main category: cs.NE

TL;DR: 提出了一种通过分解固定点乘法器单元以最大化功率效率的方法，利用符号幅度编码降低功耗，实现了显著的功率节省。


<details>
  <summary>Details</summary>
Motivation: 针对AI工作负载中常见的数据分布，优化固定点乘法器的功率效率。

Method: 将乘法器分解为编码块和乘法模块，分别优化合成，利用符号幅度编码降低开关活动。

Result: 在4位乘法器设计中，开关活动降低12.9%（正态分布输入）至33%（限幅输入），合成优化方法可进一步改善5-10%。

Conclusion: 该方法在保持逻辑等效的前提下，显著提升了功率效率，适用于生产就绪系统。

Abstract: This work presents a method to maximize power-efficiency of fixed point
multiplier units by decomposing them into sub-components. First, an encoder
block converts the operands from a two's complement to a sign magnitude
representation, followed by a multiplier module which performs the compute
operation and outputs the resulting value in the original format. This allows
to leverage the power-efficiency of the Sign Magnitude encoding for the
multiplication. To ensure the computing format is not altered, those two
components are synthesized and optimized separately. Our method leads to
significant power savings for input values centered around zero, as commonly
encountered in AI workloads. Under a realistic input stream with values
normally distributed with a standard deviation of 3.0, post-synthesis
simulations of the 4-bit multiplier design show up to 12.9% lower switching
activity compared to synthesis without decomposition. Those gains are achieved
while ensuring compliance into any production-ready system as the overall
circuit stays logic-equivalent. With the compliance lifted and a slightly
smaller input range of -7 to +7, switching activity reductions can reach up to
33%. Additionally, we demonstrate that synthesis optimization methods based on
switching-activity-driven design space exploration can yield a further 5-10%
improvement in power-efficiency compared to a power agnostic approach.

</details>
