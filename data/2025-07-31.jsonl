{"id": "2507.22063", "pdf": "https://arxiv.org/pdf/2507.22063", "abs": "https://arxiv.org/abs/2507.22063", "authors": ["Wenjie Jacky Mo", "Qin Liu", "Xiaofei Wen", "Dongwon Jung", "Hadi Askari", "Wenxuan Zhou", "Zhe Zhao", "Muhao Chen"], "title": "RedCoder: Automated Multi-Turn Red Teaming for Code LLMs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) for code generation (i.e., Code LLMs) have\ndemonstrated impressive capabilities in AI-assisted software development and\ntesting. However, recent studies have shown that these models are prone to\ngenerating vulnerable or even malicious code under adversarial settings.\nExisting red-teaming approaches rely on extensive human effort, limiting their\nscalability and practicality, and generally overlook the interactive nature of\nreal-world AI-assisted programming, which often unfolds over multiple turns. To\nbridge these gaps, we present RedCoder, a red-teaming agent that engages victim\nmodels in multi-turn conversation to elicit vulnerable code. The pipeline to\nconstruct RedCoder begins with a multi-agent gaming process that simulates\nadversarial interactions, yielding a set of prototype conversations and an\narsenal of reusable attack strategies. We then fine-tune an LLM on these\nprototype conversations to serve as the backbone of RedCoder. Once deployed,\nRedCoder autonomously engages Code LLMs in multi-turn conversations,\ndynamically retrieving relevant strategies from the arsenal to steer the\ndialogue toward vulnerability-inducing outputs. Experiments across multiple\nCode LLMs show that our approach outperforms prior single-turn and multi-turn\nred-team methods in inducing vulnerabilities in code generation, offering a\nscalable and effective tool for evaluating the security boundaries of modern\ncode-generation systems."}
{"id": "2507.22064", "pdf": "https://arxiv.org/pdf/2507.22064", "abs": "https://arxiv.org/abs/2507.22064", "authors": ["Michael Cohoon", "Debbie Furman"], "title": "Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper details the machine learning (ML) journey of a group of people\nfocused on software testing. It tells the story of how this group progressed\nthrough a ML workflow (similar to the CRISP-DM process). This workflow consists\nof the following steps and can be used by anyone applying ML techniques to a\nproject: gather the data; clean the data; perform feature engineering on the\ndata; splitting the data into two sets, one for training and one for testing;\nchoosing a machine learning model; training the model; testing the model and\nevaluating the model performance. By following this workflow, anyone can\neffectively apply ML to any project that they are doing."}
{"id": "2507.22065", "pdf": "https://arxiv.org/pdf/2507.22065", "abs": "https://arxiv.org/abs/2507.22065", "authors": ["Xiaotao Feng", "Xiaogang Zhu", "Kun Hu", "Jincheng Wang", "Yingjie Cao", "Guang Gong", "Jianfeng Pan"], "title": "Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "comment": null, "summary": "Fuzzing is highly effective in detecting bugs due to the key contribution of\nrandomness. However, randomness significantly reduces the efficiency of\nfuzzing, causing it to cost days or weeks to expose bugs. Even though directed\nfuzzing reduces randomness by guiding fuzzing towards target buggy locations,\nthe dilemma of randomness still challenges directed fuzzers. Two critical\ncomponents, which are seeds and mutators, contain randomness and are closely\ntied to the conditions required for triggering bugs. Therefore, to address the\nchallenge of randomness, we propose to use large language models (LLMs) to\nremove the randomness in seeds and reduce the randomness in mutators. With\ntheir strong reasoning and code generation capabilities, LLMs can be used to\ngenerate reachable seeds that target pre-determined locations and to construct\nbug-specific mutators tailored for specific bugs. We propose RandLuzz, which\nintegrates LLMs and directed fuzzing, to improve the quality of seeds and\nmutators, resulting in efficient bug exposure. RandLuzz analyzes function call\nchain or functionality to guide LLMs in generating reachable seeds. To\nconstruct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,\nobtaining information such as bug causes and mutation suggestions, which\nfurther help generate code that performs bug-specific mutations. We evaluate\nRandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,\nBeacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers\nachieve an average speedup ranging from 2.1$\\times$ to 4.8$\\times$ compared to\nusing widely-used initial seeds. Additionally, when evaluated on individual\nbugs, RandLuzz achieves up to a 2.7$\\times$ speedup compared to the\nsecond-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60\nseconds."}
{"id": "2507.22066", "pdf": "https://arxiv.org/pdf/2507.22066", "abs": "https://arxiv.org/abs/2507.22066", "authors": ["Dylan Manuel", "Paul Rad"], "title": "CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "The generation of large, high-quality datasets for code understanding and\ngeneration remains a significant challenge, particularly when aligning\ndecompiled binaries with their original source code. To address this, we\npresent CodableLLM, a Python framework designed to automate the creation and\ncuration of datasets by mapping decompiled functions to their corresponding\nsource functions. This process enhances the alignment between decompiled and\nsource code representations, facilitating the development of large language\nmodels (LLMs) capable of understanding and generating code across multiple\nabstraction levels. CodableLLM supports multiple programming languages and\nintegrates with existing decompilers and parsers to streamline dataset\ngeneration. This paper presents the design and implementation of CodableLLM,\nevaluates its performance in dataset creation, and compares it to existing\ntools in the field. The results demonstrate that CodableLLM offers a robust and\nefficient solution for generating datasets tailored for code-focused LLMS."}
{"id": "2507.22131", "pdf": "https://arxiv.org/pdf/2507.22131", "abs": "https://arxiv.org/abs/2507.22131", "authors": ["Theviyanthan Krishnamohan", "Paul Harvey"], "title": "OpenRASE: Service Function Chain Emulation", "categories": ["cs.NI", "cs.DC", "cs.NE"], "comment": "Accepted to IEEE SoftCom 2025", "summary": "Service Function Chains (SFCs) are one of the key enablers in providing\nprogrammable computer networks, paving the way for network autonomy. However,\nthis also introduces new challenges, such as resource allocation and\noptimisation related to their operation, requiring new algorithms to address\nthese challenges. Various tools have been used in the literature to evaluate\nthese algorithms. However, these tools suffer from inaccuracy, low fidelity,\nunscalability, inflexibility, or additional code requirements. This paper\nintroduces an emulator based on Mininet and Docker for SFCs called OpenRASE.\nThe goal of OpenRASE is to enable the exploration of resource allocation\nalgorithms for SFCs in a dynamic setting, allowing real CPU usage and latency\nto be measured. We describe the design and implementation of OpenRASE and\ndiscuss its characteristics. We also experimentally evaluate two different\nalgorithms to address the SFC resource allocation challenge, including an\nonline Genetic Algorithm, using OpenRASE to show its effectiveness and\npracticality for dynamic network conditions."}
{"id": "2507.22069", "pdf": "https://arxiv.org/pdf/2507.22069", "abs": "https://arxiv.org/abs/2507.22069", "authors": ["Tobias Sesterhenn", "Ian Berlot-Attwell", "Janis Zenkner", "Christian Bartelt"], "title": "A Compute-Matched Re-Evaluation of TroVE on MATH", "categories": ["cs.PL", "cs.AI"], "comment": null, "summary": "Reusing established theorems and formulas is central to mathematical problem\nsolving, serving as essential building blocks for tackling increasingly complex\nchallenges. Recent work, TroVE, argues that code-generating Large Language\nModels (LLMs) can benefit similarly on the MATH benchmark by inducing and\nreusing higher-level toolboxes. By allocating computational budget across an\nensemble of three modes -- directly generating code, creating tools, and\nreusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only\nperforms direct generation. However, recent analysis (Berlot-Attwell et al.,\n2024) casts doubt on these gains, noting that the tools created are often\ntrivial or rarely reused, suggesting that improvements may stem from\nself-consistency or self-correction. In this work, we re-evaluate TroVE on\nMATH, analyze the impact of each of its modes, and show that its benefit does\nnot come from these mechanisms, but simply from a higher computational budget\nspent for TroVE compared to PRIMITIVE. To this end, we also perform a small\ncorrection in the original implementation of TroVE's selection mechanism,\nboosting TroVE's performance on MATH by 3\\% in accuracy. After matching for\ncompute, the benefit of TroVE reduces to a marginal improvement of 1\\%,\nsuggesting that this toolbox approach does not provide a significant benefit on\nMATH."}
{"id": "2507.22536", "pdf": "https://arxiv.org/pdf/2507.22536", "abs": "https://arxiv.org/abs/2507.22536", "authors": ["Marco Peressotti"], "title": "Infinite Traces by Finality: a Sheaf-Theoretic Approach", "categories": ["cs.LO"], "comment": null, "summary": "Kleisli categories have long been recognised as a setting for modelling the\nlinear behaviour of various types of systems. However, the final coalgebra in\nsuch settings does not, in general, correspond to a fixed notion of linear\nsemantics. While there are well-understood conditions under which final\ncoalgebras capture finite trace semantics, a general account of infinite trace\nsemantics via finality has remained elusive. In this work, we present a\nsheaf-theoretic framework for infinite trace semantics in Kleisli categories\nthat systematically constructs final coalgebras capturing infinite traces. Our\napproach combines Kleisli categories, sheaves over ordinals, and guarded\n(co)recursion, enabling infinite behaviours to emerge from coherent families of\nfinite approximations via amalgamation. We introduce the notion of guarded\nbehavioural functor and show that, under mild conditions, their final\ncoalgebras directly characterise infinite traces."}
{"id": "2507.22134", "pdf": "https://arxiv.org/pdf/2507.22134", "abs": "https://arxiv.org/abs/2507.22134", "authors": ["Yoonsu Kim", "Brandon Chin", "Kihoon Son", "Seoyoung Kim", "Juho Kim"], "title": "IntentFlow: Interactive Support for Communicating Intent with LLMs in Writing Tasks", "categories": ["cs.HC"], "comment": null, "summary": "While large language models (LLMs) are widely used for writing, users often\nstruggle to express their nuanced and evolving intents through prompt-based\ninterfaces. Intents -- low-level strategies or preferences for achieving a\nwriting goal -- are often vague, fluid, or even subconscious, making it\ndifficult for users to articulate and adjust them. To address this, we present\nIntentFlow, which supports the communication of dynamically evolving intents\nthroughout LLM-assisted writing. IntentFlow extracts goals and intents from\nuser prompts and presents them as editable interface components, which users\ncan revise, remove, or refine via direct manipulation or follow-up prompts.\nVisual links connect each component to the output segments it influences,\nhelping users understand model behavior. In a within-subjects study (N=12),\nparticipants using IntentFlow, compared to a chat-based baseline, expressed\ntheir intents more easily and in detail, engaged in more meaningful actions to\ncommunicate intents, such as adjusting and deleting, and produced outputs that\nbetter aligned with their evolving intents. We found that editable intent\nrepresentations help users refine and consolidate a final set of intents, which\ncan be reused across similar tasks to support consistent and transferable\nLLM-assisted writing."}
{"id": "2507.22245", "pdf": "https://arxiv.org/pdf/2507.22245", "abs": "https://arxiv.org/abs/2507.22245", "authors": ["Igor Sfiligoi", "Emily A. Belli", "Jeff Candy"], "title": "Minimizing CGYRO HPC Communication Costs in Ensembles with XGYRO by Sharing the Collisional Constant Tensor Structure", "categories": ["cs.DC"], "comment": "3 pages, 3 figures, Accepted at ICPP25", "summary": "First-principles fusion plasma simulations are both compute and memory\nintensive, and CGYRO is no exception. The use of many HPC nodes to fit the\nproblem in the available memory thus results in significant communication\noverhead, which is hard to avoid for any single simulation. That said, most\nfusion studies are composed of ensembles of simulations, so we developed a new\ntool, named XGYRO, that executes a whole ensemble of CGYRO simulations as a\nsingle HPC job. By treating the ensemble as a unit, XGYRO can alter the global\nbuffer distribution logic and apply optimizations that are not feasible on any\nsingle simulation, but only on the ensemble as a whole. The main saving comes\nfrom the sharing of the collisional constant tensor structure, since its values\nare typically identical between parameter-sweep simulations. This data\nstructure dominates the memory consumption of CGYRO simulations, so\ndistributing it among the whole ensemble results in drastic memory savings for\neach simulation, which in turn results in overall lower communication overhead."}
{"id": "2507.22143", "pdf": "https://arxiv.org/pdf/2507.22143", "abs": "https://arxiv.org/abs/2507.22143", "authors": ["Muhammad Adnan", "Diego Calvanese", "Julien Corman", "Anton Dignös", "Werner Nutt", "Ognjen Savković"], "title": "Compact Answers to Temporal Path Queries", "categories": ["cs.DB"], "comment": "Extended version of a paper accepted at the ISWC 2025 conference", "summary": "We study path-based graph queries that, in addition to navigation through\nedges, also perform navigation through time. This allows asking questions about\nthe dynamics of networks, like traffic movement, cause-effect relationships, or\nthe spread of a disease. In this setting, a graph consists of triples annotated\nwith validity intervals, and a query produces pairs of nodes where each pair is\nassociated with a binary relation over time. For instance, such a pair could be\ntwo airports, and the temporal relation could map potential departure times to\npossible arrival times. An open question is how to represent such a relation in\na compact form and maintain this property during query evaluation. We\ninvestigate four compact representations of answers to a such queries, which\nare based on alternative ways to encode sets of intervals. We discuss their\nrespective advantages and drawbacks, in terms of conciseness, uniqueness, and\ncomputational cost. Notably, the most refined encoding guarantees that query\nanswers over dense time can be finitely represented."}
{"id": "2507.22221", "pdf": "https://arxiv.org/pdf/2507.22221", "abs": "https://arxiv.org/abs/2507.22221", "authors": ["Nasrin Akbari", "Mehdi Modarressi", "Alireza Khadem"], "title": "A Customized Memory-aware Architecture for Biological Sequence Alignment", "categories": ["cs.AR", "cs.ET", "n/a", "C.3"], "comment": "20 pages, 11 figures", "summary": "Sequence alignment is a fundamental process in computational biology which\nidentifies regions of similarity in biological sequences. With the exponential\ngrowth in the volume of data in bioinformatics databases, the time, processing\npower, and memory bandwidth for comparing a query sequence with the available\ndatabases grows proportionally. The sequence alignment algorithms often involve\nsimple arithmetic operations and feature high degrees of inherent fine-grained\nand coarse-grained parallelism. These features can be potentially exploited by\na massive parallel processor, such as a GPU, to increase throughput. In this\npaper, we show that the excessive memory bandwidth demand of the sequence\nalignment algorithms prevents exploiting the maximum achievable throughput on\nconventional parallel machines. We then propose a memory-aware architecture to\nreduce the bandwidth demand of the sequence alignment algorithms, effectively\npushing the memory wall to extract higher throughput. The design is integrated\nat the logic layer of an emerging 3D DRAM as a processing-in-memory\narchitecture to further increase the available bandwidth. The experimental\nresults show that the proposed architecture results in up to 2.4x speedup over\na GPU-based design. Moreover, by moving the computation closer to the memory,\npower consumption is reduced by 37%, on average."}
{"id": "2507.22301", "pdf": "https://arxiv.org/pdf/2507.22301", "abs": "https://arxiv.org/abs/2507.22301", "authors": ["Hongjian Zhou", "Pingchuan Ma", "Jiaqi Gu"], "title": "Toward Intelligent Electronic-Photonic Design Automation for Large-Scale Photonic Integrated Circuits: from Device Inverse Design to Physical Layout Generation", "categories": ["cs.ET"], "comment": "10 pages. SPIE Optical Design Automation (ODA) 2025", "summary": "Photonic Integrated Circuits (PICs) offer tremendous advantages in bandwidth,\nparallelism, and energy efficiency, making them essential for emerging\napplications in artificial intelligence (AI), high-performance computing (HPC),\nsensing, and communications. However, the design of modern PICs, which now\nintegrate hundreds to thousands of components, remains largely manual,\nresulting in inefficiency, poor scalability, and susceptibility to errors. To\naddress these challenges, we propose PoLaRIS, a comprehensive Intelligent\nElectronic-Photonic Design Automation (EPDA) framework that spans both\ndevice-level synthesis and system-level physical layout. PoLaRIS combines a\nrobust, fabrication-aware inverse design engine with a routing-informed\nplacement and curvy-aware detailed router, enabling the automated generation of\ndesign rule violation (DRV)-free and performance-optimized layouts. By unifying\nphysics-driven optimization with machine learning and domain-specific\nalgorithms, PoLaRIS significantly accelerates PIC development, lowers design\nbarriers, and lays the groundwork for scalable photonic system design\nautomation."}
{"id": "2507.22451", "pdf": "https://arxiv.org/pdf/2507.22451", "abs": "https://arxiv.org/abs/2507.22451", "authors": ["Alexander Batashev"], "title": "Dissecting RISC-V Performance: Practical PMU Profiling and Hardware-Agnostic Roofline Analysis on Emerging Platforms", "categories": ["cs.PF"], "comment": null, "summary": "As RISC-V architectures proliferate across embedded and high-performance\ndomains, developers face persistent challenges in performance optimization due\nto fragmented tooling, immature hardware features, and platform-specific\ndefects. This paper delivers a pragmatic methodology for extracting actionable\nperformance insights on RISC-V systems, even under constrained or unreliable\nhardware conditions. We present a workaround to circumvent hardware bugs in one\nof the popular RISC-V implementations, enabling robust event sampling. For\nmemory-compute bottleneck analysis, we introduce compiler-driven Roofline\ntooling that operates without hardware PMU dependencies, leveraging LLVM-based\ninstrumentation to derive operational intensity and throughput metrics directly\nfrom application IR. Our open source toolchain automates these workarounds,\nunifying PMU data correction and compiler-guided Roofline construction into a\nsingle workflow."}
{"id": "2507.22731", "pdf": "https://arxiv.org/pdf/2507.22731", "abs": "https://arxiv.org/abs/2507.22731", "authors": ["Quanwei Yang", "Luying Huang", "Kaisiyuan Wang", "Jiazhi Guan", "Shengyi He", "Fengguo Li", "Hang Zhou", "Lingyun Yu", "Yingying Li", "Haocheng Feng", "Hongtao Xie"], "title": "GestureHYDRA: Semantic Co-speech Gesture Synthesis via Hybrid Modality Diffusion Transformer and Cascaded-Synchronized Retrieval-Augmented Generation", "categories": ["cs.MM"], "comment": "10 pages, 5 figures, Accepted by ICCV 2025", "summary": "While increasing attention has been paid to co-speech gesture synthesis, most\nprevious works neglect to investigate hand gestures with explicit and essential\nsemantics. In this paper, we study co-speech gesture generation with an\nemphasis on specific hand gesture activation, which can deliver more\ninstructional information than common body movements. To achieve this, we first\nbuild a high-quality dataset of 3D human body movements including a set of\nsemantically explicit hand gestures that are commonly used by live streamers.\nThen we present a hybrid-modality gesture generation system GestureHYDRA built\nupon a hybrid-modality diffusion transformer architecture with novelly designed\nmotion-style injective transformer layers, which enables advanced gesture\nmodeling ability and versatile gesture operations. To guarantee these specific\nhand gestures can be activated, we introduce a cascaded retrieval-augmented\ngeneration strategy built upon a semantic gesture repository annotated for each\nsubject and an adaptive audio-gesture synchronization mechanism, which\nsubstantially improves semantic gesture activation and production efficiency.\nQuantitative and qualitative experiments demonstrate that our proposed approach\nachieves superior performance over all the counterparts. The project page can\nbe found at https://mumuwei.github.io/GestureHYDRA/."}
{"id": "2507.22070", "pdf": "https://arxiv.org/pdf/2507.22070", "abs": "https://arxiv.org/abs/2507.22070", "authors": ["Y. Du"], "title": "Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach", "categories": ["cs.SE", "cs.CE", "cs.PL"], "comment": "7 pages", "summary": "Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present\nsignificant challenges for performance testing, particularly when targeting\nintermediate business interfaces with complex nested data structures.\nTraditional test data generation approaches are inadequate for handling the\nintricate hierarchical and graph-like structures inherent in enterprise\nprotobuf schemas. This paper presents a novel test data generation framework\nthat leverages Python's metaclass system for dynamic type enhancement and\nstatistical analysis of production logs for realistic value domain extraction.\nOur approach combines automatic schema introspection, statistical value\ndistribution analysis, and recursive descent algorithms for handling deeply\nnested structures. Experimental evaluation on three real-world enterprise\nsystems demonstrates up to 95\\% reduction in test data preparation time and\n80\\% improvement in test coverage compared to existing approaches. The\nframework successfully handles protobuf structures with up to 15 levels of\nnesting and generates comprehensive test suites containing over 100,000 test\ncases within seconds."}
{"id": "2507.22317", "pdf": "https://arxiv.org/pdf/2507.22317", "abs": "https://arxiv.org/abs/2507.22317", "authors": ["Ze Zhang", "Qian Dong", "Wenhan Wang"], "title": "AdapSCA-PSO: An Adaptive Localization Algorithm with AI-Based Hybrid SCA-PSO for IoT WSNs", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The accurate localization of sensor nodes is a fundamental requirement for\nthe practical application of the Internet of Things (IoT). To enable robust\nlocalization across diverse environments, this paper proposes a hybrid\nmeta-heuristic localization algorithm. Specifically, the algorithm integrates\nthe Sine Cosine Algorithm (SCA), which is effective in global search, with\nParticle Swarm Optimization (PSO), which excels at local search. An adaptive\nswitching module is introduced to dynamically select between the two\nalgorithms. Furthermore, the initialization, fitness evaluation, and parameter\nsettings of the algorithm have been specifically redesigned and optimized to\naddress the characteristics of the node localization problem. Simulation\nresults across varying numbers of sensor nodes demonstrate that, compared to\nstandalone PSO and the unoptimized SCAPSO algorithm, the proposed method\nsignificantly reduces the number of required iterations and achieves an average\nlocalization error reduction of 84.97%."}
{"id": "2507.22065", "pdf": "https://arxiv.org/pdf/2507.22065", "abs": "https://arxiv.org/abs/2507.22065", "authors": ["Xiaotao Feng", "Xiaogang Zhu", "Kun Hu", "Jincheng Wang", "Yingjie Cao", "Guang Gong", "Jianfeng Pan"], "title": "Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.PL"], "comment": null, "summary": "Fuzzing is highly effective in detecting bugs due to the key contribution of\nrandomness. However, randomness significantly reduces the efficiency of\nfuzzing, causing it to cost days or weeks to expose bugs. Even though directed\nfuzzing reduces randomness by guiding fuzzing towards target buggy locations,\nthe dilemma of randomness still challenges directed fuzzers. Two critical\ncomponents, which are seeds and mutators, contain randomness and are closely\ntied to the conditions required for triggering bugs. Therefore, to address the\nchallenge of randomness, we propose to use large language models (LLMs) to\nremove the randomness in seeds and reduce the randomness in mutators. With\ntheir strong reasoning and code generation capabilities, LLMs can be used to\ngenerate reachable seeds that target pre-determined locations and to construct\nbug-specific mutators tailored for specific bugs. We propose RandLuzz, which\nintegrates LLMs and directed fuzzing, to improve the quality of seeds and\nmutators, resulting in efficient bug exposure. RandLuzz analyzes function call\nchain or functionality to guide LLMs in generating reachable seeds. To\nconstruct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,\nobtaining information such as bug causes and mutation suggestions, which\nfurther help generate code that performs bug-specific mutations. We evaluate\nRandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,\nBeacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers\nachieve an average speedup ranging from 2.1$\\times$ to 4.8$\\times$ compared to\nusing widely-used initial seeds. Additionally, when evaluated on individual\nbugs, RandLuzz achieves up to a 2.7$\\times$ speedup compared to the\nsecond-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60\nseconds."}
{"id": "2507.22705", "pdf": "https://arxiv.org/pdf/2507.22705", "abs": "https://arxiv.org/abs/2507.22705", "authors": ["Kristina Sojakova", "Mihai Codescu", "Joshua Gancher"], "title": "Concrete Security Bounds for Simulation-Based Proofs of Multi-Party Computation Protocols", "categories": ["cs.LO"], "comment": null, "summary": "The concrete security paradigm aims to give precise bounds on the probability\nthat an adversary can subvert a cryptographic mechanism. This is in contrast to\nasymptotic security, where the probability of subversion may be eventually\nsmall, but large enough in practice to be insecure. Fully satisfactory concrete\nsecurity bounds for Multi-Party Computation (MPC) protocols are difficult to\nattain, as they require reasoning about the running time of cryptographic\nadversaries and reductions. In this paper we close this gap by introducing a\nnew foundational approach that allows us to automatically compute concrete\nsecurity bounds for MPC protocols. We take inspiration from the meta-theory of\nIPDL, a prior approach for formally verified distributed cryptography, to\nsupport reasoning about the runtime of protocols and adversarial advantage. For\npractical proof developments, we implement our approach in Maude, an extensible\nlogic for equational rewriting. We carry out four case studies of concrete\nsecurity for simulation-based proofs. Most notably, we deliver the first formal\nverification of the GMW MPC protocol over N parties. To our knowledge, this is\nthe first time that formally verified concrete security bounds are computed for\na proof of an MPC protocol in the style of Universal Composability. Our tool\nprovides a layer of abstraction that allows the user to write proofs at a high\nlevel, which drastically simplifies the proof size. For comparison, a case\nstudy that in prior works required 2019 LoC only takes 567 LoC, thus reducing\nproof size by 72%"}
{"id": "2507.22153", "pdf": "https://arxiv.org/pdf/2507.22153", "abs": "https://arxiv.org/abs/2507.22153", "authors": ["Ethan Wilson", "Vincent Bindschaedler", "Sophie Jörg", "Sean Sheikholeslam", "Kevin Butler", "Eakta Jain"], "title": "Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality", "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Photorealistic 3D avatar generation has rapidly improved in recent years, and\nrealistic avatars that match a user's true appearance are more feasible in\nMixed Reality (MR) than ever before. Yet, there are known risks to sharing\none's likeness online, and photorealistic MR avatars could exacerbate these\nrisks. If user likenesses were to be shared broadly, there are risks for cyber\nabuse or targeted fraud based on user appearances. We propose an alternate\navatar rendering scheme for broader social MR -- synthesizing realistic avatars\nthat preserve a user's demographic identity while being distinct enough from\nthe individual user to protect facial biometric information. We introduce a\nmethodology for privatizing appearance by isolating identity within the feature\nspace of identity-encoding generative models. We develop two algorithms that\nthen obfuscate identity: \\epsmethod{} provides differential privacy guarantees\nand \\thetamethod{} provides fine-grained control for the level of identity\noffset. These methods are shown to successfully generate de-identified virtual\navatars across multiple generative architectures in 2D and 3D. With these\ntechniques, it is possible to protect user privacy while largely preserving\nattributes related to sense of self. Employing these techniques in public\nsettings could enable the use of photorealistic avatars broadly in MR,\nmaintaining high realism and immersion without privacy risk."}
{"id": "2507.22294", "pdf": "https://arxiv.org/pdf/2507.22294", "abs": "https://arxiv.org/abs/2507.22294", "authors": ["Gregor von Laszewski", "Wesley Brewer", "Sean R. Wilkinson", "Andrew Shao", "J. P. Fleischer", "Harshad Pitkar", "Christine R. Kirkpatrick", "Geoffrey C. Fox"], "title": "Towards Experiment Execution in Support of Community Benchmark Workflows for HPC", "categories": ["cs.DC"], "comment": null, "summary": "A key hurdle is demonstrating compute resource capability with limited\nbenchmarks. We propose workflow templates as a solution, offering adaptable\ndesigns for specific scientific applications. Our paper identifies common usage\npatterns for these templates, drawn from decades of HPC experience, including\nrecent work with the MLCommons Science working group.\n  We found that focusing on simple experiment management tools within the\nbroader computational workflow improves adaptability, especially in education.\nThis concept, which we term benchmark carpentry, is validated by two\nindependent tools: Cloudmesh's Experiment Executor and Hewlett Packard\nEnterprise's SmartSim. Both frameworks, with significant functional overlap,\nhave been tested across various scientific applications, including conduction\ncloudmask, earthquake prediction, simulation-AI/ML interactions, and the\ndevelopment of computational fluid dynamics surrogates."}
{"id": "2507.22305", "pdf": "https://arxiv.org/pdf/2507.22305", "abs": "https://arxiv.org/abs/2507.22305", "authors": ["Carolina Cortés Lasalle", "Lisa Ehrlinger", "Lorena Etcheverry", "Felix Naumann"], "title": "Is SHACL Suitable for Data Quality Assessment?", "categories": ["cs.DB"], "comment": "44 pages", "summary": "Knowledge graphs have been widely adopted in both enterprises, such as the\nGoogle Knowledge Graph, and open platforms like Wikidata to represent domain\nknowledge and support analysis with artificial intelligence. They model\nreal-world information as nodes and edges. To embrace flexibility, knowledge\ngraphs often lack enforced schemas (i.e., ontologies), leading to potential\ndata quality issues, such as semantically overlapping nodes. Therefore,\nensuring their quality is essential, as issues in the data can affect\napplications relying on them. To assess the quality of knowledge graphs,\nexisting works either propose high-level frameworks comprising various data\nquality dimensions without concrete implementations, define tools that measure\ndata quality with ad-hoc SPARQL (SPARQL Protocol and RDF Query Language)\nqueries, or promote the usage of constraint languages, such as the Shapes\nConstraint Language (SHACL), to assess and improve the quality of the graph.\nAlthough the latter approaches claim to address data quality assessment, none\nof them comprehensively tries to cover all data quality dimensions. In this\npaper, we explore this gap by investigating the extent to which SHACL can be\nused to assess data quality in knowledge graphs. Specifically, we defined SHACL\nshapes for 69 data quality metrics proposed by Zaveri et al. [1] and\nimplemented a prototype that automatically instantiates these shapes and\ncomputes the corresponding data quality measures from their validation results.\nAll resources are provided for repeatability at\nhttps://github.com/caroocortes/SHACL-DQA-prototype/tree/main"}
{"id": "2507.22511", "pdf": "https://arxiv.org/pdf/2507.22511", "abs": "https://arxiv.org/abs/2507.22511", "authors": ["Kranthi Kumar Talluri", "Christopher Stang", "Galia Weidl"], "title": "Green Wave as an Integral Part for the Optimization of Traffic Efficiency and Safety: A Survey", "categories": ["cs.ET"], "comment": null, "summary": "Green Wave provides practical and advanced solutions to improve traffic\nefficiency and safety through network coordination. Nevertheless, the complete\npotential of Green Wave systems has yet to be explored. Utilizing emerging\ntechnologies and advanced algorithms, such as AI or V2X, would aid in achieving\nmore robust traffic management strategies, especially when integrated with\nGreen Wave. This work comprehensively surveys existing traffic control\nstrategies that enable Green Waves and analyzes their impact on future traffic\nmanagement systems and urban infrastructure. Understanding previous research on\ntraffic management and its effect on traffic efficiency and safety helps\nexplore the integration of Green Wave solutions with smart city initiatives for\neffective traffic signal coordination. This paper also discusses the advantages\nof using Green Wave strategies for emission reduction and considers road safety\nissues for vulnerable road users, such as pedestrians and cyclists. Finally,\nthe existing challenges and research gaps in building robust and successful\nGreen Wave systems are discussed to articulate explicitly the future\nrequirement of sustainable urban transport."}
{"id": "2507.22702", "pdf": "https://arxiv.org/pdf/2507.22702", "abs": "https://arxiv.org/abs/2507.22702", "authors": ["Hendrik Reiter", "Ahmad Rzgar Hamid", "Florian Schlösser", "Mikkel Baun Kjærgaard", "Wilhelm Hasselbring"], "title": "Ecoscape: Fault Tolerance Benchmark for Adaptive Remediation Strategies in Real-Time Edge ML", "categories": ["cs.PF"], "comment": null, "summary": "Edge computing offers significant advantages for realtime data processing\ntasks, such as object recognition, by reducing network latency and bandwidth\nusage. However, edge environments are susceptible to various types of fault. A\nremediator is an automated software component designed to adjust the\nconfiguration parameters of a software service dynamically. Its primary\nfunction is to maintain the services operational state within predefined\nService Level Objectives by applying corrective actions in response to\ndeviations from these objectives. Remediators can be implemented based on the\nKubernetes container orchestration tool by implementing remediation strategies\nsuch as rescheduling or adjusting application parameters. However, currently,\nthere is no method to compare these remediation strategies fairly. This paper\nintroduces Ecoscape, a comprehensive benchmark designed to evaluate the\nperformance of remediation strategies in fault-prone environments. Using Chaos\nEngineering techniques, Ecoscape simulates realistic fault scenarios and\nprovides a quantifiable score to assess the efficacy of different remediation\napproaches. In addition, it is configurable to support domain-specific Service\nLevel Objectives. We demonstrate the capabilities of Ecoscape in edge machine\nlearning inference, offering a clear framework to optimize fault tolerance in\nthese systems without needing a physical edge testbed."}
{"id": "2507.22099", "pdf": "https://arxiv.org/pdf/2507.22099", "abs": "https://arxiv.org/abs/2507.22099", "authors": ["Shuqing Li", "Qiang Chen", "Xiaoxue Ren", "Michael R. Lyu"], "title": "Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "comment": null, "summary": "Physics Engines (PEs) are fundamental software frameworks that simulate\nphysical interactions in applications ranging from entertainment to\nsafety-critical systems. Despite their importance, PEs suffer from physics\nfailures, deviations from expected physical behaviors that can compromise\nsoftware reliability, degrade user experience, and potentially cause critical\nfailures in autonomous vehicles or medical robotics. Current testing approaches\nfor PE-based software are inadequate, typically requiring white-box access and\nfocusing on crash detection rather than semantically complex physics failures.\nThis paper presents the first large-scale empirical study characterizing\nphysics failures in PE-based software. We investigate three research questions\naddressing the manifestations of physics failures, the effectiveness of\ndetection techniques, and developer perceptions of current detection practices.\nOur contributions include: (1) a taxonomy of physics failure manifestations;\n(2) a comprehensive evaluation of detection methods including deep learning,\nprompt-based techniques, and large multimodal models; and (3) actionable\ninsights from developer experiences for improving detection approaches. To\nsupport future research, we release PhysiXFails, code, and other materials at\nhttps://sites.google.com/view/physics-failure-detection."}
{"id": "2507.22071", "pdf": "https://arxiv.org/pdf/2507.22071", "abs": "https://arxiv.org/abs/2507.22071", "authors": ["Niels Glodny"], "title": "Analyzing and Evaluating the Behavior of Git Diff and Merge", "categories": ["cs.SE"], "comment": "Bachelor's thesis", "summary": "Despite being widely used, the algorithms that enable collaboration with Git\nare not well understood. The diff and merge algorithms are particularly\ninteresting, as they could be applied in other contexts. In this thesis, I\ndocument the main functionalities of Git: how diffs are computed, how they are\nused to run merges, and how merges enable more complex operations. In the\nprocess, I show multiple unexpected behaviors in Git, including the following:\nThe histogram diff algorithm has pathological cases where a single-line change\ncan cause the entire rest of the file to be marked as changed. The default\nmerge strategy (ort) can result in merges requiring exponential time in the\nnumber of commits in the history. Merges and rebases are not commutative, and\neven when merges do not result in a conflict, the result is not specified but\ndepends on the diff algorithm used. And finally, sometimes when two sides of a\nmerge add different lines at the same position, the result is not a conflict,\nbut a merge containing both changes after each other, in arbitrary order."}
{"id": "2507.22591", "pdf": "https://arxiv.org/pdf/2507.22591", "abs": "https://arxiv.org/abs/2507.22591", "authors": ["Pablo Picazo-Martinez", "Carlos Barroso-Fernández", "Alejandro Calvillo-Fernandez", "Milan Groshev", "Carlos J. Bernardos", "Antonio de la Oliva", "Alain Mourad"], "title": "802.11bf Multiband Passive Sensing: Reusing Wi-Fi Signaling for Sensing", "categories": ["cs.NI", "eess.SP", "14J60 (Primary) 14F05, 14J26 (Secondary)"], "comment": "16 pages, 16 figures, 4 tables", "summary": "This paper presents a novel multiband passive sensing system that leverages\nIEEE 802.11bf Wi-Fi signals for environmental sensing, focusing on both sub-7\nGHz and millimeter-wave (mmWave) bands. By combining Channel State Information\n(CSI) from multiple bands, the system enhances accuracy and reliability in\ndetecting human presence, movement, and activities in indoor environments.\nUtilizing a novel model, called MILAGRO, the system demonstrates robust\nperformance across different scenarios, including monitoring human presence in\nworkspaces and tracking movement in corridors. Experimental results show high\naccuracy (95-100%), with improved performance by integrating multiband data.\nThe system also addresses key security concerns associated with passive\nsensing, proposing measures to mitigate potential risks. This work advances the\nuse of Wi-Fi for passive sensing by reducing reliance on active sensing\ninfrastructure and extending the capabilities of low-cost, non-intrusive\nenvironmental monitoring."}
{"id": "2507.22070", "pdf": "https://arxiv.org/pdf/2507.22070", "abs": "https://arxiv.org/abs/2507.22070", "authors": ["Y. Du"], "title": "Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach", "categories": ["cs.SE", "cs.CE", "cs.PL"], "comment": "7 pages", "summary": "Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present\nsignificant challenges for performance testing, particularly when targeting\nintermediate business interfaces with complex nested data structures.\nTraditional test data generation approaches are inadequate for handling the\nintricate hierarchical and graph-like structures inherent in enterprise\nprotobuf schemas. This paper presents a novel test data generation framework\nthat leverages Python's metaclass system for dynamic type enhancement and\nstatistical analysis of production logs for realistic value domain extraction.\nOur approach combines automatic schema introspection, statistical value\ndistribution analysis, and recursive descent algorithms for handling deeply\nnested structures. Experimental evaluation on three real-world enterprise\nsystems demonstrates up to 95\\% reduction in test data preparation time and\n80\\% improvement in test coverage compared to existing approaches. The\nframework successfully handles protobuf structures with up to 15 levels of\nnesting and generates comprehensive test suites containing over 100,000 test\ncases within seconds."}
{"id": "2507.22687", "pdf": "https://arxiv.org/pdf/2507.22687", "abs": "https://arxiv.org/abs/2507.22687", "authors": ["Josh Millar", "Ryan Gibb", "Roy Ang", "Anil Madhavapeddy", "Hamed Haddadi"], "title": "Bifröst: Spatial Networking with Bigraphs", "categories": ["cs.NI", "cs.AI", "cs.LO", "cs.MA"], "comment": "Submitted to HotNets 2025", "summary": "Modern networked environments increasingly rely on spatial reasoning, but\nlack a coherent representation for coordinating physical space. Consequently,\ntasks such as enforcing spatial access policies remain fragile and manual. We\nfirst propose a unifying representation based on bigraphs, capturing spatial,\nsocial, and communication relationships within a single formalism, with\nuser-facing tools to generate bigraphs from physical environments. Second, we\npresent a hierarchical agent architecture for distributed spatial reasoning,\nwith runtimes for agentic processes to interact the spatial representation, and\na context-aware execution model that scopes reasoning to the smallest viable\nsubspace. Together, these enable private, reliable, and low-latency spatial\nnetworking that can safely interact with agentic workflows."}
{"id": "2507.22163", "pdf": "https://arxiv.org/pdf/2507.22163", "abs": "https://arxiv.org/abs/2507.22163", "authors": ["DaEun Choi", "Kihoon Son", "Jaesang Yu", "Hyunjoon Jung", "Juho Kim"], "title": "IdeaBlocks: Expressing and Reusing Exploratory Intents for Design Exploration with Generative AI", "categories": ["cs.HC"], "comment": null, "summary": "Generative AI opens new possibilities for design exploration by rapidly\ngenerating images aligned with user goals. However, our formative study (N=7)\nrevealed three key limitations hindering designers' broad and efficient\nexploration when interacting with these models. These include difficulty\nexpressing open-ended exploratory intent, lack of continuity in exploration,\nand limited support for reusing or iterating on previous ideas. We propose\nIdeaBlocks, where users can express their exploratory intents to generative AI\nwith structured input and modularize them into Exploration Blocks. These blocks\ncan be chained for continuous, non-linear exploration and reused across\ncontexts, enabling broad exploration without losing creative momentum. Our user\nstudy with 12 designers showed that participants using IdeaBlocks explored\n112.8% more images with 12.5% greater visual diversity than the baseline. They\nalso developed ideas in more iterative and continuous patterns, such as\nbranching, chaining, and revisiting ideas. We discuss design implications for\nfuture tools to better balance divergent and convergent support during\ndifferent phases of exploration, and to capture and leverage exploratory\nintents more effectively."}
{"id": "2507.22339", "pdf": "https://arxiv.org/pdf/2507.22339", "abs": "https://arxiv.org/abs/2507.22339", "authors": ["Zhuocheng Liu", "Zhishu Shen", "Qiushi Zheng", "Tiehua Zhang", "Zheng Lei", "Jiong Jin"], "title": "A Semi-Supervised Federated Learning Framework with Hierarchical Clustering Aggregation for Heterogeneous Satellite Networks", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Low Earth Orbit (LEO) satellites are emerging as key components of 6G\nnetworks, with many already deployed to support large-scale Earth observation\nand sensing related tasks. Federated Learning (FL) presents a promising\nparadigm for enabling distributed intelligence in these resource-constrained\nand dynamic environments. However, achieving reliable convergence, while\nminimizing both processing time and energy consumption, remains a substantial\nchallenge, particularly in heterogeneous and partially unlabeled satellite\nnetworks. To address this challenge, we propose a novel semi-supervised\nfederated learning framework tailored for LEO satellite networks with\nhierarchical clustering aggregation. To further reduce communication overhead,\nwe integrate sparsification and adaptive weight quantization techniques. In\naddition, we divide the FL clustering into two stages: satellite cluster\naggregation stage and Ground Stations (GSs) aggregation stage. The supervised\nlearning at GSs guides selected Parameter Server (PS) satellites, which in turn\nsupport fully unlabeled satellites during the federated training process.\nExtensive experiments conducted on a satellite network testbed demonstrate that\nour proposal can significantly reduce processing time (up to 3x) and energy\nconsumption (up to 4x) compared to other comparative methods while maintaining\nmodel accuracy."}
{"id": "2507.22384", "pdf": "https://arxiv.org/pdf/2507.22384", "abs": "https://arxiv.org/abs/2507.22384", "authors": ["Umar Siddiqui", "Habiba Youssef", "Adel Sabour", "Mohamed Ali"], "title": "Scalability, Availability, Reproducibility and Extensibility in Islamic Database Systems", "categories": ["cs.DB", "cs.SE"], "comment": null, "summary": "With the widespread of software systems and applications that serve the\nIslamic knowledge domain, several concerns arise. Authenticity and accuracy of\nthe databases that back up these systems are questionable. With the excitement\nthat some software developers and amateur researchers may have, false\nstatements and incorrect claims may be made around numerical signs or miracles\nin the Quran. Reproducibility of these claims may not be addressed by the\npeople making such claims. Moreover, with the increase in the number of users,\nscalability and availability of these systems become a concern. In addition to\nall these concerns, extensibility is also another major issue. Properly\ndesigned systems can be extensible, reusable and built on top of one another,\ninstead of each system being built from scratch every time a new framework is\ndeveloped. In this paper, we introduce the QuranResearch.Org system and its\nvision for scalability, availability, reproducibility and extensibility to\nserve Islamic database systems."}
{"id": "2507.22117", "pdf": "https://arxiv.org/pdf/2507.22117", "abs": "https://arxiv.org/abs/2507.22117", "authors": ["Salwa Shaglel", "Markus Kirsch", "Marten Winkler", "Christian Münch", "Stefan Walter", "Fritz Schinkel", "Martin Kliesch"], "title": "A comprehensive benchmark of an Ising machine on the Max-Cut problem", "categories": ["quant-ph", "cs.ET"], "comment": "24 + 14 pages, 12 figures. Comments welcome!", "summary": "QUBO formulations of combinatorial optimization problems allow for solving\nthem using various quantum heuristics. While large-scale quantum computations\nare currently still out of reach, we can already numerically test such QUBO\nformulations on a perhaps surprisingly large scale. In this work, we benchmark\nFujitsu's Digital Annealer (DA) on the Max-Cut problem, which captures the main\ncomplexity of the QUBO problem. We make a comprehensive benchmark against\nleading other heuristic algorithms on graphs with up to 53,000 variables by\nfocusing on the wall-clock time. Moreover, we compare the DA performance\nagainst published performance results of the D-Wave hybrid quantum-classical\nannealer and the recently proposed QIS3 heuristic. Based on performance\nstatistics for over 2,000 graphs from the MQLib, we find that the DA yields\ncompetitive results. We hope that this benchmark demonstrates the extent to\nwhich large QUBO instances can be heuristically solved today, yielding\nconsistent results across different solvers."}
{"id": "2507.22367", "pdf": "https://arxiv.org/pdf/2507.22367", "abs": "https://arxiv.org/abs/2507.22367", "authors": ["Jia Li", "Yichao He", "Jiacheng Xu", "Tianhao Luo", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors", "categories": ["cs.CL", "cs.MM"], "comment": "8 pages, 3 figures, ACM MM 2025", "summary": "Accurate and reliable personality assessment plays a vital role in many\nfields, such as emotional intelligence, mental health diagnostics, and\npersonalized education. Unlike fleeting emotions, personality traits are\nstable, often subconsciously leaked through language, facial expressions, and\nbody behaviors, with asynchronous patterns across modalities. It was hard to\nmodel personality semantics with traditional superficial features and seemed\nimpossible to achieve effective cross-modal understanding. To address these\nchallenges, we propose a novel personality assessment framework called\n\\textit{\\textbf{Traits Run Deep}}. It employs\n\\textit{\\textbf{psychology-informed prompts}} to elicit high-level\npersonality-relevant semantic representations. Besides, it devises a\n\\textit{\\textbf{Text-Centric Trait Fusion Network}} that anchors rich text\nsemantics to align and integrate asynchronous signals from other modalities. To\nbe specific, such fusion module includes a Chunk-Wise Projector to decrease\ndimensionality, a Cross-Modal Connector and a Text Feature Enhancer for\neffective modality fusion and an ensemble regression head to improve\ngeneralization in data-scarce situations. To our knowledge, we are the first to\napply personality-specific prompts to guide large language models (LLMs) in\nextracting personality-aware semantics for improved representation quality.\nFurthermore, extracting and fusing audio-visual apparent behavior features\nfurther improves the accuracy. Experimental results on the AVI validation set\nhave demonstrated the effectiveness of the proposed components, i.e.,\napproximately a 45\\% reduction in mean squared error (MSE). Final evaluations\non the test set of the AVI Challenge 2025 confirm our method's superiority,\nranking first in the Personality Assessment track. The source code will be made\navailable at https://github.com/MSA-LMC/TraitsRunDeep."}
{"id": "2507.22080", "pdf": "https://arxiv.org/pdf/2507.22080", "abs": "https://arxiv.org/abs/2507.22080", "authors": ["Qiushi Sun", "Jinyang Gong", "Lei Li", "Qipeng Guo", "Fei Yuan"], "title": "CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Acquiring high-quality instruction-code pairs is essential for training Large\nLanguage Models (LLMs) for code generation. Manually curated data is expensive\nand inherently limited in scale, motivating the development of code-centric\nsynthesis methods. Yet, current approaches either focus on augmenting existing\ncode or rely on predefined heuristics, both lacking rigorous data validation,\nwhich results in synthetic data that is ungrounded, repetitive, or overly\nsimplistic. Inspired by collaborative programming practices, we propose\nCodeEvo, a framework that synthesizes code data through iterative interactions\nbetween two LLM agents: a Coder, which generates candidate code and test cases\nbased on given instructions, and a Reviewer, which guides the synthesis process\nby producing new instructions and feedback. We further introduce a hybrid\nfeedback mechanism that combines compiler determinism with the generative\nflexibility of agents, enabling automatic quality control throughout synthesis.\nExtensive experiments demonstrate that models fine-tuned on CodeEvo data\nsignificantly outperform established baselines across code generation\nbenchmarks with various difficulties. In-depth analyses further provide\ninsights from multiple perspectives into effective code-centric data synthesis."}
{"id": "2507.22687", "pdf": "https://arxiv.org/pdf/2507.22687", "abs": "https://arxiv.org/abs/2507.22687", "authors": ["Josh Millar", "Ryan Gibb", "Roy Ang", "Anil Madhavapeddy", "Hamed Haddadi"], "title": "Bifröst: Spatial Networking with Bigraphs", "categories": ["cs.NI", "cs.AI", "cs.LO", "cs.MA"], "comment": "Submitted to HotNets 2025", "summary": "Modern networked environments increasingly rely on spatial reasoning, but\nlack a coherent representation for coordinating physical space. Consequently,\ntasks such as enforcing spatial access policies remain fragile and manual. We\nfirst propose a unifying representation based on bigraphs, capturing spatial,\nsocial, and communication relationships within a single formalism, with\nuser-facing tools to generate bigraphs from physical environments. Second, we\npresent a hierarchical agent architecture for distributed spatial reasoning,\nwith runtimes for agentic processes to interact the spatial representation, and\na context-aware execution model that scopes reasoning to the smallest viable\nsubspace. Together, these enable private, reliable, and low-latency spatial\nnetworking that can safely interact with agentic workflows."}
{"id": "2507.22086", "pdf": "https://arxiv.org/pdf/2507.22086", "abs": "https://arxiv.org/abs/2507.22086", "authors": ["Honghua Dong", "Jiacheng Yang", "Xun Deng", "Yuhe Jiang", "Gennady Pekhimenko", "Fan Long", "Xujie Si"], "title": "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Type inference for dynamic languages like Python is a persistent challenge in\nsoftware engineering. While large language models (LLMs) have shown promise in\ncode understanding, their type inference capabilities remain underexplored. We\nintroduce TypyBench, a benchmark designed to evaluate LLMs' type inference\nacross entire Python repositories. TypyBench features two novel metrics:\nTypeSim, which captures nuanced semantic relationships between predicted and\nground truth types, and TypeCheck, which assesses type consistency across\ncodebases. Our evaluation of various LLMs on a curated dataset of 50\nhigh-quality Python repositories reveals that, although LLMs achieve decent\nTypeSim scores, they struggle with complex nested types and exhibit significant\ntype consistency errors. These findings suggest that future research should\nshift focus from improving type similarity to addressing repository-level\nconsistency. TypyBench provides a foundation for this new direction, offering\ninsights into model performance across different type complexities and usage\ncontexts. Our code and data are available at\nhttps://github.com/typybench/typybench."}
{"id": "2507.22760", "pdf": "https://arxiv.org/pdf/2507.22760", "abs": "https://arxiv.org/abs/2507.22760", "authors": ["Samuel Teuber", "Debasmita Lohar", "Bernhard Beckert"], "title": "Of Good Demons and Bad Angels: Guaranteeing Safe Control under Finite Precision", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.LO", "cs.SY"], "comment": "15 pages, 3 figures, 1 table; Accepted at FMCAD 2025", "summary": "As neural networks (NNs) become increasingly prevalent in safety-critical\nneural network-controlled cyber-physical systems (NNCSs), formally guaranteeing\ntheir safety becomes crucial. For these systems, safety must be ensured\nthroughout their entire operation, necessitating infinite-time horizon\nverification. To verify the infinite-time horizon safety of NNCSs, recent\napproaches leverage Differential Dynamic Logic (dL). However, these dL-based\nguarantees rely on idealized, real-valued NN semantics and fail to account for\nroundoff errors introduced by finite-precision implementations. This paper\nbridges the gap between theoretical guarantees and real-world implementations\nby incorporating robustness under finite-precision perturbations -- in sensing,\nactuation, and computation -- into the safety verification. We model the\nproblem as a hybrid game between a good Demon, responsible for control actions,\nand a bad Angel, introducing perturbations. This formulation enables formal\nproofs of robustness w.r.t. a given (bounded) perturbation. Leveraging this\nbound, we employ state-of-the-art mixed-precision fixed-point tuners to\nsynthesize sound and efficient implementations, thus providing a complete\nend-to-end solution. We evaluate our approach on case studies from the\nautomotive and aeronautics domains, producing efficient NN implementations with\nrigorous infinite-time horizon safety guarantees."}
{"id": "2507.22193", "pdf": "https://arxiv.org/pdf/2507.22193", "abs": "https://arxiv.org/abs/2507.22193", "authors": ["Zeyu Yan", "SuHwan Hong", "Josiah Hester", "Tingyu Cheng", "Huaishu Peng"], "title": "DissolvPCB: Fully Recyclable 3D-Printed Electronics with Liquid Metal Conductors and PVA Substrates", "categories": ["cs.HC"], "comment": null, "summary": "We introduce DissolvPCB, an electronic prototyping technique for fabricating\nfully recyclable printed circuit board assemblies (PCBAs) using affordable FDM\n3D printing, with polyvinyl alcohol (PVA) as a water-soluble substrate and\neutectic gallium-indium (EGaIn) as the conductive material. When obsolete, the\nPCBA can be easily recycled by immersing it in water: the PVA dissolves, the\nEGaIn re-forms into a liquid metal bead, and the electronic components are\nrecovered. These materials can then be reused to fabricate a new PCBA.\n  We present the DissolvPCB workflow, characterize its design parameters,\nevaluate the performance of circuits produced with it, and quantify its\nenvironmental impact through a lifecycle assessment (LCA) comparing it to\nconventional CNC-milled FR-4 boards. We further develop a software plugin that\nautomatically converts PCB design files into 3D-printable circuit substrate\nmodels. To demonstrate the capabilities of DissolvPCB, we fabricate and recycle\nthree functional prototypes: a Bluetooth speaker featuring a double-sided PCB,\na finger fidget toy with a 3D circuit topology, and a shape-changing gripper\nenabled by Joule-heat-driven 4D printing. The paper concludes with a discussion\nof current technical limitations and opportunities for future directions."}
{"id": "2507.22372", "pdf": "https://arxiv.org/pdf/2507.22372", "abs": "https://arxiv.org/abs/2507.22372", "authors": ["Grace Nansamba", "Evelyn Namugwanya", "David Boehme", "Dewi Yokelson", "Riley Shipley", "Derek Schafer", "Michael McKinsey", "Olga Pearce", "Anthony Skjellum"], "title": "Leveraging Caliper and Benchpark to Analyze MPI Communication Patterns: Insights from AMG2023, Kripke, and Laghos", "categories": ["cs.DC"], "comment": "10 pages, 6 figures", "summary": "We introduce ``communication regions'' into the widely used Caliper HPC\nprofiling tool. A communication region is an annotation enabling capture of\nmetrics about the data being communicated (including statistics of these\nmetrics), and metrics about the MPI processes involved in the communications,\nsomething not previously possible in Caliper. We explore the utility of\ncommunication regions with three representative modeling and simulation\napplications, AMG2023, Kripke, and Laghos, all part of the comprehensive\nBenchpark suite that includes Caliper annotations. Enhanced Caliper reveals\ndetailed communication behaviors. Using Caliper and Thicket in tandem, we\ncreate new visualizations of MPI communication patterns, including halo\nexchanges. Our findings reveal communication bottlenecks and detailed\nbehaviors, indicating significant utility of the special-regions addition to\nCaliper. The comparative scaling behavior of both CPU and GPU oriented systems\nare shown; we are able to look at different regions within a given application,\nand see how scalability and message-traffic metrics differ."}
{"id": "2507.22419", "pdf": "https://arxiv.org/pdf/2507.22419", "abs": "https://arxiv.org/abs/2507.22419", "authors": ["Tung-Wei Lin", "Gabe Fierro", "Han Li", "Tianzhen Hong", "Pierluigi Nuzzo", "Alberto Sangiovanni-Vinentelli"], "title": "Systematic Evaluation of Knowledge Graph Repair with Large Language Models", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "We present a systematic approach for evaluating the quality of knowledge\ngraph repairs with respect to constraint violations defined in shapes\nconstraint language (SHACL). Current evaluation methods rely on \\emph{ad hoc}\ndatasets, which limits the rigorous analysis of repair systems in more general\nsettings. Our method addresses this gap by systematically generating violations\nusing a novel mechanism, termed violation-inducing operations (VIOs). We use\nthe proposed evaluation framework to assess a range of repair systems which we\nbuild using large language models. We analyze the performance of these systems\nacross different prompting strategies. Results indicate that concise prompts\ncontaining both the relevant violated SHACL constraints and key contextual\ninformation from the knowledge graph yield the best performance."}
{"id": "2507.22141", "pdf": "https://arxiv.org/pdf/2507.22141", "abs": "https://arxiv.org/abs/2507.22141", "authors": ["Atiquzzaman Mondal", "Waheeb Tashan", "Ayat Al-Olaimat", "Hüseyin Arslan"], "title": "Efficient handover based on Near-field and Far-field RIS for seamless connectivity", "categories": ["eess.SP", "cs.ET"], "comment": "11 pages, 10 figures, IEEE Transactions on Mobile Computing", "summary": "Reconfigurable Intelligent Surfaces (RIS) is becoming a transformative\ntechnology for the upcoming 6G communication networks, providing a way for\nsmartly maneuvering the electromagnetic waves to enhance coverage and\nconnectivity. This paper presents an efficient handover (HO) management scheme\nleveraging RIS in the Fresnel region i.e., in both the near-field (NF) and\nfar-field (FF) regions to reduce signaling overhead and optimize mobility\nmanagement. For this, we analyzed the signal strength variations in the\nconsidered RIS-aided networks, considering the radiative NF and FF regions, and\nderive the probability density function (PDF) of the RIS-UE distance in the NF\nregion to quantify RIS reflection gains along the user equipment (UE)\ntrajectory. We propose a new HO algorithm incorporating several HO categories\nlike hard handover (HHO), soft handover (SHO), RIS-aided cell breathing\n(RIS-CB), and RIS-aided ping-pong avoidance (RIS-PP) strategies. The proposed\nalgorithm uses bit error rate (BER) as a key parameter to predict the\nminimization of unnecessary HOs by using RIS-aided pathways to retain\nconnectivity with the serving base station (BS), which minimizes the\nrequirement for frequent target BS searching and ultimately optimizes the HO.\nBy restricting measurement reports and HO requests, the suggested method\nimproves spectrum efficiency (SE) and energy efficiency (EE), especially in\ncrowded cellular networks. Numerical results highlight significant reductions\nin HO rates and signaling load, ensuring seamless connectivity and improved\nquality of service (QoS) in 6G systems."}
{"id": "2507.22481", "pdf": "https://arxiv.org/pdf/2507.22481", "abs": "https://arxiv.org/abs/2507.22481", "authors": ["Tianyi Liu", "Kejun Wu", "Chen Cai", "Yi Wang", "Kim-Hui Yap", "Lap-Pui Chau"], "title": "Towards Blind Bitstream-corrupted Video Recovery via a Visual Foundation Model-driven Framework", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.MM"], "comment": "10 pages, 5 figures, accepted by ACMMM 2025", "summary": "Video signals are vulnerable in multimedia communication and storage systems,\nas even slight bitstream-domain corruption can lead to significant pixel-domain\ndegradation. To recover faithful spatio-temporal content from corrupted inputs,\nbitstream-corrupted video recovery has recently emerged as a challenging and\nunderstudied task. However, existing methods require time-consuming and\nlabor-intensive annotation of corrupted regions for each corrupted video frame,\nresulting in a large workload in practice. In addition, high-quality recovery\nremains difficult as part of the local residual information in corrupted frames\nmay mislead feature completion and successive content recovery. In this paper,\nwe propose the first blind bitstream-corrupted video recovery framework that\nintegrates visual foundation models with a recovery model, which is adapted to\ndifferent types of corruption and bitstream-level prompts. Within the\nframework, the proposed Detect Any Corruption (DAC) model leverages the rich\npriors of the visual foundation model while incorporating bitstream and\ncorruption knowledge to enhance corruption localization and blind recovery.\nAdditionally, we introduce a novel Corruption-aware Feature Completion (CFC)\nmodule, which adaptively processes residual contributions based on high-level\ncorruption understanding. With VFM-guided hierarchical feature augmentation and\nhigh-level coordination in a mixture-of-residual-experts (MoRE) structure, our\nmethod suppresses artifacts and enhances informative residuals. Comprehensive\nevaluations show that the proposed method achieves outstanding performance in\nbitstream-corrupted video recovery without requiring a manually labeled mask\nsequence. The demonstrated effectiveness will help to realize improved user\nexperience, wider application scenarios, and more reliable multimedia\ncommunication and storage systems."}
{"id": "2507.22085", "pdf": "https://arxiv.org/pdf/2507.22085", "abs": "https://arxiv.org/abs/2507.22085", "authors": ["Vaani Goenka", "Aalok D. Thakkar"], "title": "BOOP: Write Right Code", "categories": ["cs.SE"], "comment": null, "summary": "Novice programmers frequently adopt a syntax-specific and test-case-driven\napproach, writing code first and adjusting until programs compile and test\ncases pass, rather than developing correct solutions through systematic\nreasoning. AI coding tools exacerbate this challenge by providing syntactically\ncorrect but conceptually flawed solutions. In this paper, we introduce BOOP\n(Blueprint, Operations, OCaml, Proof), a structured framework requiring four\nmandatory phases: formal specification, language-agnostic algorithm\ndevelopment, implementation, and correctness proof. This shifts focus from\n``making code work'' to understanding why code is correct.\n  BOOP was implemented at our institution using a VS Code extension and\npreprocessor that enforces constraints and identifies counterproductive\npatterns. Initial evaluation shows improved algorithmic reasoning and reduced\ntrial-and-error debugging. Students reported better edge case understanding and\nproblem decomposition, though some initially found the format verbose.\nInstructors observed stronger foundational skills compared to traditional\napproaches."}
{"id": "2507.22711", "pdf": "https://arxiv.org/pdf/2507.22711", "abs": "https://arxiv.org/abs/2507.22711", "authors": ["Hong-Jun Yoon", "Mariam Kiran", "Danial Ebling", "Joe Breen"], "title": "OFCnetLLM: Large Language Model for Network Monitoring and Alertness", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The rapid evolution of network infrastructure is bringing new challenges and\nopportunities for efficient network management, optimization, and security.\nWith very large monitoring databases becoming expensive to explore, the use of\nAI and Generative AI can help reduce costs of managing these datasets. This\npaper explores the use of Large Language Models (LLMs) to revolutionize network\nmonitoring management by addressing the limitations of query finding and\npattern analysis. We leverage LLMs to enhance anomaly detection, automate\nroot-cause analysis, and automate incident analysis to build a well-monitored\nnetwork management team using AI. Through a real-world example of developing\nour own OFCNetLLM, based on the open-source LLM model, we demonstrate practical\napplications of OFCnetLLM in the OFC conference network. Our model is developed\nas a multi-agent approach and is still evolving, and we present early results\nhere."}
{"id": "2507.22876", "pdf": "https://arxiv.org/pdf/2507.22876", "abs": "https://arxiv.org/abs/2507.22876", "authors": ["Yiwen Sun", "Furong Ye", "Zhihan Chen", "Ke Wei", "Shaowei Cai"], "title": "Automatically discovering heuristics in a complex SAT solver with large language models", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "Satisfiability problem (SAT) is a cornerstone of computational complexity\nwith broad industrial applications, and it remains challenging to optimize\nmodern SAT solvers in real-world settings due to their intricate architectures.\nWhile automatic configuration frameworks have been developed, they rely on\nmanually constrained search spaces and yield limited performance gains. This\nwork introduces a novel paradigm which effectively optimizes complex SAT\nsolvers via Large Language Models (LLMs), and a tool called AutoModSAT is\ndeveloped. Three fundamental challenges are addressed in order to achieve\nsuperior performance: (1) LLM-friendly solver: Systematic guidelines are\nproposed for developing a modularized solver to meet LLMs' compatibility,\nemphasizing code simplification, information share and bug reduction; (2)\nAutomatic prompt optimization: An unsupervised automatic prompt optimization\nmethod is introduced to advance the diversity of LLMs' output; (3) Efficient\nsearch strategy: We design a presearch strategy and an EA evolutionary\nalgorithm for the final efficient and effective discovery of heuristics.\nExtensive experiments across a wide range of datasets demonstrate that\nAutoModSAT achieves 50% performance improvement over the baseline solver and\nachieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,\nAutoModSAT attains a 20% speedup on average compared to parameter-tuned\nalternatives of the SOTA solvers, showcasing the enhanced capability in\nhandling complex problem instances. This work bridges the gap between AI-driven\nheuristics discovery and mission-critical system optimization, and provides\nboth methodological advancements and empirically validated results for\nnext-generation complex solver development."}
{"id": "2507.22241", "pdf": "https://arxiv.org/pdf/2507.22241", "abs": "https://arxiv.org/abs/2507.22241", "authors": ["Victoria Chang", "Caro Williams-Pierce", "Huaishu Peng", "Ge Gao"], "title": "Verisimilitude as Boon and Bane: How People Initiate Opportunistic Interactions at Professional Events in Social VR", "categories": ["cs.HC"], "comment": null, "summary": "Opportunistic interactions-the unstructured exchanges that emerge as\nindividuals become aware of each other's presence-are essential for\nrelationship building and information sharing in everyday life. Yet, fostering\neffective opportunistic interactions has proven challenging, especially at\nprofessional events that have increasingly transitioned from in person to\nonline formats. In the current paper, we offer an in-depth qualitative account\nof how people initiate opportunistic interactions in social VR. Our\nparticipants consisted of 16 individuals with ongoing experience attending\nVR-mediated events in their professional communities. We conducted extensive\nobservations with each participant during one or more events they attended. We\nalso interviewed them after every observed event, obtaining self-reflections on\ntheir attempts to navigate opportunistic interactions with others. Our analysis\nrevealed that participants sought to understand the extent to which social VR\npreserved the real-world meanings of various nonverbal cues, which we refer to\nas verisimilitude. We detailed the unique connections between a person's\nperceived verisimilitude and their social behaviors at each of the three steps\ntoward initiating opportunistic interactions: availability recognition,\nattention capture, and ice-breaking. Across these steps, the VR platform\ntypically replaces complex social mechanisms with feasible technical ones in\norder to function, thereby altering the preconditions necessary for a nonverbal\ncue's social meanings to remain intact. We identified a rich set of strategies\nthat participants developed to assess verisimilitude and act upon it, while\nalso confirming a lack of systematic knowledge guiding their practices. Based\non these findings, we provide actionable insights for social VR platform design\nthat can best support the initiation of opportunistic interactions for\nprofessional purposes."}
{"id": "2507.22801", "pdf": "https://arxiv.org/pdf/2507.22801", "abs": "https://arxiv.org/abs/2507.22801", "authors": ["Shubhradeep Roy", "Suvarthi Sarkar", "Vivek Verma", "Aryabartta Sahu"], "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic Space Partitioning with Erasure Code", "categories": ["cs.DC"], "comment": null, "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."}
{"id": "2507.22701", "pdf": "https://arxiv.org/pdf/2507.22701", "abs": "https://arxiv.org/abs/2507.22701", "authors": ["Haoran Zhang", "Decheng Zuo", "Yu Yan", "Zhiyu Liang", "Hongzhi Wang"], "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases", "categories": ["cs.DB", "H.2.4; H.2.7"], "comment": "16 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference", "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM, an autonomic cache manager powered by our novel AURA algorithm. AURA makes\nstability a first-class design principle by resolving the\nexploitation-exploration dilemma: it achieves this by synthesizing two\northogonal factors, which we introduce as: the H-factor, representing a\ndatabase's proven, historically stable efficiency (exploitation), and the\nV-factor, representing its empirically estimated marginal gain for future\nimprovements (exploration). This dual-factor model, governed by an adaptive\nweight, enables SAM to achieve sustained high performance through strategic\nstability and robustness in volatile conditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and cache pollution attacks. Furthermore, its decision\nlatency is highly scalable, remaining nearly constant as the system grows to\n120 databases. Crucially, SAM achieves superior decision stability --\nmaintaining consistent optimization directions despite noise, avoiding\nperformance oscillations while ensuring predictable Quality of Service. These\nresults prove that a principled, stability-aware design is essential for\nsustained high performance in real-world, large-scale systems."}
{"id": "2507.22221", "pdf": "https://arxiv.org/pdf/2507.22221", "abs": "https://arxiv.org/abs/2507.22221", "authors": ["Nasrin Akbari", "Mehdi Modarressi", "Alireza Khadem"], "title": "A Customized Memory-aware Architecture for Biological Sequence Alignment", "categories": ["cs.AR", "cs.ET", "n/a", "C.3"], "comment": "20 pages, 11 figures", "summary": "Sequence alignment is a fundamental process in computational biology which\nidentifies regions of similarity in biological sequences. With the exponential\ngrowth in the volume of data in bioinformatics databases, the time, processing\npower, and memory bandwidth for comparing a query sequence with the available\ndatabases grows proportionally. The sequence alignment algorithms often involve\nsimple arithmetic operations and feature high degrees of inherent fine-grained\nand coarse-grained parallelism. These features can be potentially exploited by\na massive parallel processor, such as a GPU, to increase throughput. In this\npaper, we show that the excessive memory bandwidth demand of the sequence\nalignment algorithms prevents exploiting the maximum achievable throughput on\nconventional parallel machines. We then propose a memory-aware architecture to\nreduce the bandwidth demand of the sequence alignment algorithms, effectively\npushing the memory wall to extract higher throughput. The design is integrated\nat the logic layer of an emerging 3D DRAM as a processing-in-memory\narchitecture to further increase the available bandwidth. The experimental\nresults show that the proposed architecture results in up to 2.4x speedup over\na GPU-based design. Moreover, by moving the computation closer to the memory,\npower consumption is reduced by 37%, on average."}
{"id": "2507.22676", "pdf": "https://arxiv.org/pdf/2507.22676", "abs": "https://arxiv.org/abs/2507.22676", "authors": ["Jia Li", "Yang Wang", "Wenhao Qian", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "title": "Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment", "categories": ["cs.CL", "cs.MM"], "comment": "8 pages, 4 figures, ACM MM 2025.\n  github:https://github.com/MSA-LMC/365Aspects", "summary": "Interview performance assessment is essential for determining candidates'\nsuitability for professional positions. To ensure holistic and fair\nevaluations, we propose a novel and comprehensive framework that explores\n``365'' aspects of interview performance by integrating \\textit{three}\nmodalities (video, audio, and text), \\textit{six} responses per candidate, and\n\\textit{five} key evaluation dimensions. The framework employs\nmodality-specific feature extractors to encode heterogeneous data streams and\nsubsequently fused via a Shared Compression Multilayer Perceptron. This module\ncompresses multimodal embeddings into a unified latent space, facilitating\nefficient feature interaction. To enhance prediction robustness, we incorporate\na two-level ensemble learning strategy: (1) independent regression heads\npredict scores for each response, and (2) predictions are aggregated across\nresponses using a mean-pooling mechanism to produce final scores for the five\ntarget dimensions. By listening to the unspoken, our approach captures both\nexplicit and implicit cues from multimodal data, enabling comprehensive and\nunbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our\nframework secured first place in the AVI Challenge 2025, demonstrating its\neffectiveness and robustness in advancing automated and multimodal interview\nperformance assessment. The full implementation is available at\nhttps://github.com/MSA-LMC/365Aspects."}
{"id": "2507.22086", "pdf": "https://arxiv.org/pdf/2507.22086", "abs": "https://arxiv.org/abs/2507.22086", "authors": ["Honghua Dong", "Jiacheng Yang", "Xun Deng", "Yuhe Jiang", "Gennady Pekhimenko", "Fan Long", "Xujie Si"], "title": "TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": null, "summary": "Type inference for dynamic languages like Python is a persistent challenge in\nsoftware engineering. While large language models (LLMs) have shown promise in\ncode understanding, their type inference capabilities remain underexplored. We\nintroduce TypyBench, a benchmark designed to evaluate LLMs' type inference\nacross entire Python repositories. TypyBench features two novel metrics:\nTypeSim, which captures nuanced semantic relationships between predicted and\nground truth types, and TypeCheck, which assesses type consistency across\ncodebases. Our evaluation of various LLMs on a curated dataset of 50\nhigh-quality Python repositories reveals that, although LLMs achieve decent\nTypeSim scores, they struggle with complex nested types and exhibit significant\ntype consistency errors. These findings suggest that future research should\nshift focus from improving type similarity to addressing repository-level\nconsistency. TypyBench provides a foundation for this new direction, offering\ninsights into model performance across different type complexities and usage\ncontexts. Our code and data are available at\nhttps://github.com/typybench/typybench."}
{"id": "2507.22851", "pdf": "https://arxiv.org/pdf/2507.22851", "abs": "https://arxiv.org/abs/2507.22851", "authors": ["Yidong Ren", "Maolin Gan", "Chenning Li", "Shakhrul Iman Siam", "Mi Zhang", "Shigang Chen", "Zhichao Cao"], "title": "Morph: ChirpTransformer-based Encoder-decoder Co-design for Reliable LoRa Communication", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "In this paper, we propose Morph, a LoRa encoder-decoder co-design to enhance\ncommunication reliability while improving its computation efficiency in\nextremely-low signal-to-noise ratio (SNR) situations. The standard LoRa encoder\ncontrols 6 Spreading Factors (SFs) to tradeoff SNR tolerance with data rate.\nSF-12 is the maximum SF providing the lowest SNR tolerance on commercial\noff-the-shelf (COTS) LoRa nodes. In Morph, we develop an SF-configuration based\nencoder to mimic the larger SFs beyond SF-12 while it is compatible with COTS\nLoRa nodes. Specifically, we manipulate four SF configurations of a Morph\nsymbol to encode 2-bit data. Accordingly, we recognize the used SF\nconfiguration of the symbol for data decoding. We leverage a Deep Neural\nNetwork (DNN) decoder to fully capture multi-dimensional features among diverse\nSF configurations to maximize the SNR gain. Moreover, we customize the input\nsize, neural network structure, and training method of the DNN decoder to\nimprove its efficiency, reliability, and generalizability. We implement Morph\nwith COTS LoRa nodes and a USRP N210, then evaluate its performance on indoor\nand campus-scale testbeds. Results show that we can reliably decode data at\n-28.8~dB SNR, which is 6.4~dB lower than the standard LoRa with SF-12 chirps.\nIn addition, the computation efficiency of our DNN decoder is about 3x higher\nthan state-of-the-art."}
{"id": "2507.22252", "pdf": "https://arxiv.org/pdf/2507.22252", "abs": "https://arxiv.org/abs/2507.22252", "authors": ["Kexin Liang", "Jan Luca Kästleb", "Bani Anvarib", "Simeon C. Calverta", "J. W. C. van Lint"], "title": "Multidimensional Assessment of Takeover Performance in Conditionally Automated Driving", "categories": ["cs.HC"], "comment": null, "summary": "When automated driving systems encounter complex situations beyond their\noperational capabilities, they issue takeover requests, prompting drivers to\nresume vehicle control and return to the driving loop as a critical safety\nbackup. However, this control transition places significant demands on drivers,\nrequiring them to promptly respond to takeover requests while executing\nhigh-quality interventions. To ensure safe and comfortable control transitions,\nit is essential to develop a deep understanding of the key factors influencing\nvarious takeover performance aspects. This study evaluates drivers' takeover\nperformance across three dimensions: response efficiency, user experience, and\ndriving safety - using a driving simulator experiment. EXtreme Gradient\nBoosting (XGBoost) models are used to investigate the contributions of two\ncritical factors, i.e., Situational Awareness (SA) and Spare Capacity (SC), in\npredicting various takeover performance metrics by comparing the predictive\nresults to the baseline models that rely solely on basic Driver Characteristics\n(DC). The results reveal that (i) higher SA enables drivers to respond to\ntakeover requests more quickly, particularly for reflexive responses; and (ii)\nSC shows a greater overall impact on takeover quality than SA, where higher SC\ngenerally leads to enhanced subjective rating scores and objective execution\ntrajectories. These findings highlight the distinct yet complementary roles of\nSA and SC in shaping performance components, offering valuable insights for\noptimizing human-vehicle interactions and enhancing automated driving system\ndesign."}
{"id": "2507.19802", "pdf": "https://arxiv.org/pdf/2507.19802", "abs": "https://arxiv.org/abs/2507.19802", "authors": ["Ziyu Zhang", "Yuanhao Wei", "Joshua Engels", "Julian Shun"], "title": "CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search", "categories": ["cs.DB", "cs.DC", "cs.DS", "cs.IR"], "comment": null, "summary": "Approximate nearest neighbor search (ANNS) has become a quintessential\nalgorithmic problem for various other foundational data tasks for AI workloads.\nGraph-based ANNS indexes have superb empirical trade-offs in indexing cost,\nquery efficiency, and query approximation quality. Most existing graph-based\nindexes are designed for the static scenario, where there are no updates to the\ndata after the index is constructed. However, full dynamism (insertions,\ndeletions, and searches) is crucial to providing up-to-date responses in\napplications using vector databases. It is desirable that the index efficiently\nsupports updates and search queries concurrently. Existing dynamic graph-based\nindexes suffer from at least one of the following problems: (1) the query\nquality degrades as updates happen; and (2) the graph structure updates used to\nmaintain the index quality upon updates are global and thus expensive. To solve\nthese problems, we propose the CleANN system which consists of three main\ncomponents: (1) workload-aware linking of diverse search tree descendants to\ncombat distribution shift; (2)query-adaptive on-the-fly neighborhood\nconsolidation to efficiently handle deleted nodes; and (3) semi-lazy memory\ncleaning to clean up stale information in the data structure and reduce the\nwork spent by the first two components. We evaluate CleANN on 7 diverse\ndatasets on fully dynamic workloads and find that CleANN has query quality at\nleast as good as if the index had been built statically using the corresponding\ndata. In the in-memory setting using 56 hyper-threads, with all types of\nqueries running concurrently, at the same recall level, CleANN achieves 7-1200x\nthroughput improvement on million-scale real-world datasets. To the best of our\nknowledge, CleANN is the first concurrent ANNS index to achieve such efficiency\nwhile maintaining quality under full dynamism."}
{"id": "2507.22186", "pdf": "https://arxiv.org/pdf/2507.22186", "abs": "https://arxiv.org/abs/2507.22186", "authors": ["Ambarish Singh", "Romila Pradhan"], "title": "SourceSplice: Source Selection for Machine Learning Tasks", "categories": ["cs.LG", "cs.AI", "cs.DB", "I.2.6"], "comment": null, "summary": "Data quality plays a pivotal role in the predictive performance of machine\nlearning (ML) tasks - a challenge amplified by the deluge of data sources\navailable in modern organizations.Prior work in data discovery largely focus on\nmetadata matching, semantic similarity or identifying tables that should be\njoined to answer a particular query, but do not consider source quality for\nhigh performance of the downstream ML task.This paper addresses the problem of\ndetermining the best subset of data sources that must be combined to construct\nthe underlying training dataset for a given ML task.We propose SourceGrasp and\nSourceSplice, frameworks designed to efficiently select a suitable subset of\nsources that maximizes the utility of the downstream ML model.Both the\nalgorithms rely on the core idea that sources (or their combinations)\ncontribute differently to the task utility, and must be judiciously\nchosen.While SourceGrasp utilizes a metaheuristic based on a greediness\ncriterion and randomization, the SourceSplice framework presents a source\nselection mechanism inspired from gene splicing - a core concept used in\nprotein synthesis.We empirically evaluate our algorithms on three real-world\ndatasets and synthetic datasets and show that, with significantly fewer subset\nexplorations, SourceSplice effectively identifies subsets of data sources\nleading to high task utility.We also conduct studies reporting the sensitivity\nof SourceSplice to the decision choices under several settings."}
{"id": "2507.22544", "pdf": "https://arxiv.org/pdf/2507.22544", "abs": "https://arxiv.org/abs/2507.22544", "authors": ["George Tsormpatzoglou", "Filip Sabo", "Aida Todri-Sanial"], "title": "Thermodynamics-Inspired Computing with Oscillatory Neural Networks for Inverse Matrix Computation", "categories": ["cs.LG", "cs.ET"], "comment": "9 pages, 8 figures", "summary": "We describe a thermodynamic-inspired computing paradigm based on oscillatory\nneural networks (ONNs). While ONNs have been widely studied as Ising machines\nfor tackling complex combinatorial optimization problems, this work\ninvestigates their feasibility in solving linear algebra problems, specifically\nthe inverse matrix. Grounded in thermodynamic principles, we analytically\ndemonstrate that the linear approximation of the coupled Kuramoto oscillator\nmodel leads to the inverse matrix solution. Numerical simulations validate the\ntheoretical framework, and we examine the parameter regimes that computation\nhas the highest accuracy."}
{"id": "2507.22223", "pdf": "https://arxiv.org/pdf/2507.22223", "abs": "https://arxiv.org/abs/2507.22223", "authors": ["Kiana Kiashemshaki", "Mohammad Jalili Torkamani", "Negin Mahmoudi"], "title": "Secure coding for web applications: Frameworks, challenges, and the role of LLMs", "categories": ["cs.SE", "D.2; D.2.4; D.4.6; I.2.7"], "comment": "11 pages, 5 figures, 3 tables, 6 listings", "summary": "Secure coding is a critical yet often overlooked practice in software\ndevelopment. Despite extensive awareness efforts, real-world adoption remains\ninconsistent due to organizational, educational, and technical barriers. This\npaper provides a comprehensive review of secure coding practices across major\nframeworks and domains, including web development, DevSecOps, and cloud\nsecurity. It introduces a structured framework comparison and categorizes\nthreats aligned with the OWASP Top 10. Additionally, we explore the rising role\nof Large Language Models (LLMs) in evaluating and recommending secure code,\npresenting a reproducible case study across four major vulnerability types.\nThis paper offers practical insights for researchers, developers, and educators\non integrating secure coding into real-world development processes."}
{"id": "2507.22090", "pdf": "https://arxiv.org/pdf/2507.22090", "abs": "https://arxiv.org/abs/2507.22090", "authors": ["Sergii Kavun"], "title": "Hybrid activation functions for deep neural networks: S3 and S4 -- a novel approach to gradient flow optimization", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.NI", "68T07, 68T05, 65D10, 68Q32", "I.2.6; I.5.1; G.1.2; I.5.2"], "comment": "15 pages, 2 figures, 5 tables", "summary": "Activation functions are critical components in deep neural networks,\ndirectly influencing gradient flow, training stability, and model performance.\nTraditional functions like ReLU suffer from dead neuron problems, while sigmoid\nand tanh exhibit vanishing gradient issues. We introduce two novel hybrid\nactivation functions: S3 (Sigmoid-Softsign) and its improved version S4\n(smoothed S3). S3 combines sigmoid for negative inputs with softsign for\npositive inputs, while S4 employs a smooth transition mechanism controlled by a\nsteepness parameter k. We conducted comprehensive experiments across binary\nclassification, multi-class classification, and regression tasks using three\ndifferent neural network architectures. S4 demonstrated superior performance\ncompared to nine baseline activation functions, achieving 97.4% accuracy on\nMNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression.\nThe function exhibited faster convergence (-19 for ReLU) and maintained stable\ngradient flow across network depths. Comparative analysis revealed S4's\ngradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep\nnetworks. The S4 activation function addresses key limitations of existing\nfunctions through its hybrid design and smooth transition mechanism. The\ntunable parameter k allows adaptation to different tasks and network depths,\nmaking S4 a versatile choice for deep learning applications. These findings\nsuggest that hybrid activation functions represent a promising direction for\nimproving neural network training dynamics."}
{"id": "2507.22262", "pdf": "https://arxiv.org/pdf/2507.22262", "abs": "https://arxiv.org/abs/2507.22262", "authors": ["Kexin Liang", "Simeon C. Calvert", "J. W. C. van Lint"], "title": "Towards Safe and Comfortable Vehicle Control Transitions: A Systematic Review of Takeover Time, Time Budget, and Takeover Performance", "categories": ["cs.HC"], "comment": null, "summary": "Conditionally automated driving systems require human drivers to disengage\nfrom non-driving-related activities and resume vehicle control within limited\ntime budgets when encountering scenarios beyond system capabilities. Ensuring\nsafe and comfortable transitions is critical for reducing driving risks and\nimproving user experience. However, takeovers involve complex human-vehicle\ninteractions, resulting in substantial variability in drivers' responses,\nespecially in takeover time, defined as the duration needed to regain control.\nThis variability presents challenges in setting sufficient time budgets that\nare neither too short (risking safety and comfort) nor too long (reducing\ndriver alertness and transition efficiency).\n  Although previous research has examined the role of time budgets in\ninfluencing takeover time and performance, few studies have systematically\naddressed how to determine sufficient time budgets that adapt to diverse\nscenarios and driver needs. This review supports such efforts by examining the\nentire takeover sequence, including takeover time, time budget, and takeover\nperformance. Specifically, we (i) synthesize causal factors influencing\ntakeover time and propose a taxonomy of its determinants using the\ntask-capability interface model; (ii) review existing work on fixed and\nadaptive time budgets, introducing the concept of the takeover buffer to\ndescribe the gap between takeover time and allocated time budget; (iii) present\na second taxonomy to support standardized and context-sensitive measurement of\ntakeover performance; (iv) propose a conceptual model describing the\nrelationships among takeover time, time budget, and performance; and (v)\noutline a research agenda with six directions."}
{"id": "2507.22131", "pdf": "https://arxiv.org/pdf/2507.22131", "abs": "https://arxiv.org/abs/2507.22131", "authors": ["Theviyanthan Krishnamohan", "Paul Harvey"], "title": "OpenRASE: Service Function Chain Emulation", "categories": ["cs.NI", "cs.DC", "cs.NE"], "comment": "Accepted to IEEE SoftCom 2025", "summary": "Service Function Chains (SFCs) are one of the key enablers in providing\nprogrammable computer networks, paving the way for network autonomy. However,\nthis also introduces new challenges, such as resource allocation and\noptimisation related to their operation, requiring new algorithms to address\nthese challenges. Various tools have been used in the literature to evaluate\nthese algorithms. However, these tools suffer from inaccuracy, low fidelity,\nunscalability, inflexibility, or additional code requirements. This paper\nintroduces an emulator based on Mininet and Docker for SFCs called OpenRASE.\nThe goal of OpenRASE is to enable the exploration of resource allocation\nalgorithms for SFCs in a dynamic setting, allowing real CPU usage and latency\nto be measured. We describe the design and implementation of OpenRASE and\ndiscuss its characteristics. We also experimentally evaluate two different\nalgorithms to address the SFC resource allocation challenge, including an\nonline Genetic Algorithm, using OpenRASE to show its effectiveness and\npracticality for dynamic network conditions."}
{"id": "2507.22550", "pdf": "https://arxiv.org/pdf/2507.22550", "abs": "https://arxiv.org/abs/2507.22550", "authors": ["Filippo Brozzi", "Gloria Turati", "Maurizio Ferrari Dacrema", "Filippo Caruso", "Paolo Cremonesi"], "title": "Hamiltonian Expressibility for Ansatz Selection in Variational Quantum Algorithms", "categories": ["quant-ph", "cs.ET"], "comment": null, "summary": "In the context of Variational Quantum Algorithms (VQAs), selecting an\nappropriate ansatz is crucial for efficient problem-solving. Hamiltonian\nexpressibility has been introduced as a metric to quantify a circuit's ability\nto uniformly explore the energy landscape associated with a Hamiltonian ground\nstate search problem. However, its influence on solution quality remains\nlargely unexplored. In this work, we estimate the Hamiltonian expressibility of\na well-defined set of circuits applied to various Hamiltonians using a Monte\nCarlo-based approach. We analyze how ansatz depth influences expressibility and\nidentify the most and least expressive circuits across different problem types.\nWe then train each ansatz using the Variational Quantum Eigensolver (VQE) and\nanalyze the correlation between solution quality and expressibility.Our results\nindicate that, under ideal or low-noise conditions and particularly for\nsmall-scale problems, ans\\\"atze with high Hamiltonian expressibility yield\nbetter performance for problems with non-diagonal Hamiltonians and\nsuperposition-state solutions. Conversely, circuits with low expressibility are\nmore effective for problems whose solutions are basis states, including those\ndefined by diagonal Hamiltonians. Under noisy conditions, low-expressibility\ncircuits remain preferable for basis-state problems, while intermediate\nexpressibility yields better results for some problems involving\nsuperposition-state solutions."}
{"id": "2507.22324", "pdf": "https://arxiv.org/pdf/2507.22324", "abs": "https://arxiv.org/abs/2507.22324", "authors": ["Cameron S. Movassaghi", "Amanda Momenzadeh", "Jesse G. Meyer"], "title": "From Articles to Code: On-Demand Generation of Core Algorithms from Scientific Publications", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Maintaining software packages imposes significant costs due to dependency\nmanagement, bug fixes, and versioning. We show that rich method descriptions in\nscientific publications can serve as standalone specifications for modern large\nlanguage models (LLMs), enabling on-demand code generation that could supplant\nhuman-maintained libraries. We benchmark state-of-the-art models\n(GPT-o4-mini-high, Gemini Pro 2.5, Claude Sonnet 4) by tasking them with\nimplementing a diverse set of core algorithms drawn from original publications.\nOur results demonstrate that current LLMs can reliably reproduce package\nfunctionality with performance indistinguishable from conventional libraries.\nThese findings foreshadow a paradigm shift toward flexible, on-demand code\ngeneration and away from static, human-maintained packages, which will result\nin reduced maintenance overhead by leveraging published articles as sufficient\ncontext for the automated implementation of analytical workflows."}
{"id": "2507.22165", "pdf": "https://arxiv.org/pdf/2507.22165", "abs": "https://arxiv.org/abs/2507.22165", "authors": ["Gursimran Singh", "H. B. Acharya", "Minseok Kwon"], "title": "Programmable Data Planes for Network Security", "categories": ["cs.CR", "cs.NI"], "comment": "17th International Conference on Networks & Communications (NeTCoM\n  2025)", "summary": "The emergence of programmable data planes, and particularly switches\nsupporting the P4 language, has transformed network security by enabling\ncustomized, line-rate packet processing. These switches, originally intended\nfor flexible forwarding, now play a broader role: detecting and mitigating\nattacks such as DDoS and spoofing, enforcing next-generation firewall policies,\nand even supporting in-network cryptography and machine learning. These\ncapabilities are made possible by techniques such as recirculate-and-truncate\nand lookup-table precomputation, which work around architectural constraints\nlike limited memory and restricted instruction sets. In this paper, we\nsystematize recent advances in security applications built on programmable\nswitches, with an emphasis on the capabilities, challenges, and architectural\nworkarounds. We highlight the non-obvious design techniques that make complex\nin-network security functions feasible despite the constraints of the hardware\nplatform, and also comment on remaining issues and emerging research\ndirections."}
{"id": "2507.22267", "pdf": "https://arxiv.org/pdf/2507.22267", "abs": "https://arxiv.org/abs/2507.22267", "authors": ["Owen Hoffman", "Kangze Peng", "Zehua You", "Sajid Kamal", "Sukrit Venkatagiri"], "title": "Promoting Online Safety by Simulating Unsafe Conversations with LLMs", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI, including large language models (LLMs) have the potential --\nand already are being used -- to increase the speed, scale, and types of unsafe\nconversations online. LLMs lower the barrier for entry for bad actors to create\nunsafe conversations in particular because of their ability to generate\npersuasive and human-like text. In our current work, we explore ways to promote\nonline safety by teaching people about unsafe conversations that can occur\nonline with and without LLMs. We build on prior work that shows that LLMs can\nsuccessfully simulate scam conversations. We also leverage research in the\nlearning sciences that shows that providing feedback on one's hypothetical\nactions can promote learning. In particular, we focus on simulating scam\nconversations using LLMs. Our work incorporates two LLMs that converse with\neach other to simulate realistic, unsafe conversations that people may\nencounter online between a scammer LLM and a target LLM but users of our system\nare asked provide feedback to the target LLM."}
{"id": "2507.22330", "pdf": "https://arxiv.org/pdf/2507.22330", "abs": "https://arxiv.org/abs/2507.22330", "authors": ["Chen Zhang", "Husheng Li", "Xiang Liu", "Linshan Jiang", "Danxin Wang"], "title": "Hypernetworks for Model-Heterogeneous Personalized Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Recent advances in personalized federated learning have focused on addressing\nclient model heterogeneity. However, most existing methods still require\nexternal data, rely on model decoupling, or adopt partial learning strategies,\nwhich can limit their practicality and scalability. In this paper, we revisit\nhypernetwork-based methods and leverage their strong generalization\ncapabilities to design a simple yet effective framework for heterogeneous\npersonalized federated learning. Specifically, we propose MH-pFedHN, which\nleverages a server-side hypernetwork that takes client-specific embedding\nvectors as input and outputs personalized parameters tailored to each client's\nheterogeneous model. To promote knowledge sharing and reduce computation, we\nintroduce a multi-head structure within the hypernetwork, allowing clients with\nsimilar model sizes to share heads. Furthermore, we further propose\nMH-pFedHNGD, which integrates an optional lightweight global model to improve\ngeneralization. Our framework does not rely on external datasets and does not\nrequire disclosure of client model architectures, thereby offering enhanced\nprivacy and flexibility. Extensive experiments on multiple benchmarks and model\nsettings demonstrate that our approach achieves competitive accuracy, strong\ngeneralization, and serves as a robust baseline for future research in\nmodel-heterogeneous personalized federated learning."}
{"id": "2507.22810", "pdf": "https://arxiv.org/pdf/2507.22810", "abs": "https://arxiv.org/abs/2507.22810", "authors": ["Daniel Udekwe", "Dimitrios Bolkas", "Eren Erman Ozguven", "Ren Moses", "Qianwen", "Guo"], "title": "VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education", "categories": ["cs.HC", "cs.ET", "cs.SE"], "comment": null, "summary": "Surveying is a core component of civil engineering education, requiring\nstudents to engage in hands-on spatial measurement, instrumentation handling,\nand field-based decision-making. However, traditional instruction often poses\nlogistical and cognitive challenges that can hinder accessibility and student\nengagement. While virtual laboratories have gained traction in engineering\neducation, few are purposefully designed to support flexible, adaptive learning\nin surveying. To address this gap, we developed Virtual Reality for Immersive\nand Interactive Surveying Education (VRISE), an immersive virtual reality\nlaboratory that replicates ground-based and aerial surveying tasks through\ncustomizable, accessible, and user-friendly modules. VRISE features interactive\nexperiences such as differential leveling with a digital level equipment and\nwaypoint-based drone navigation, enhanced by input smoothing, adaptive\ninterfaces, and real-time feedback to accommodate diverse learning styles.\nEvaluation across multiple user sessions demonstrated consistent gains in\nmeasurement accuracy, task efficiency, and interaction quality, with a clear\nprogression in skill development across the ground-based and aerial surveying\nmodalities. By reducing cognitive load and physical demands, even in tasks\nrequiring fine motor control and spatial reasoning, VRISE demonstrates the\npotential of immersive, repeatable digital environments to enhance surveying\neducation, broaden participation, and strengthen core competencies in a safe\nand engaging setting."}
{"id": "2507.22414", "pdf": "https://arxiv.org/pdf/2507.22414", "abs": "https://arxiv.org/abs/2507.22414", "authors": ["Sungmin Kang", "Haifeng Ruan", "Abhik Roychoudhury"], "title": "AutoCodeSherpa: Symbolic Explanations in AI Coding Agents", "categories": ["cs.SE", "D.2; I.2"], "comment": null, "summary": "Large Language Model (LLM) agents autonomously use external tools on top of\none or more LLMs to accomplish specific tasks. Lately LLM agents for software\nengineering tasks have become popular. These agents can benefit from the use of\nprogram analysis tools working on program representations. This is demonstrated\nby existing agentic AI solutions such as AutoCodeRover or SpecRover which\nperform automated program repair. Specifically the goal of these works is to\nuse program analysis to improve the patch quality. These agents are currently\nbeing used to automatically fix static analysis issues from the widely used\nSonarQube static analyzer.\n  Nevertheless, for the agents to be deployed in a production environment,\nagents need to suggest software artifacts, such as patches, with evidence and\nwith high confidence. In this work, we provide a workflow where an agent\nprovides explanations of the bug in the form of symbolic formulae. The\nexplanations are in the form of input conditions, infection conditions and\noutput conditions, implemented as property based tests (PBT) and\nprogram-internal symbolic expressions. These can help in human developer\ncognition of the agent outputs as well as in achieving completely automated\nagentic workflows for software. The human developer can benefit from the input\ncondition, represented as a PBT, to generate various concrete inputs showing a\ngiven issue. Furthermore, since the PBTs are executable, our explanations are\nexecutable as well. We can thus also use the explanations in a completely\nautomated issue resolution environment for accepting or rejecting the patches\nthat are suggested by patching agents such as AutoCodeRover. Finally, as\nagentic AI approaches continue to develop, the program analysis driven\nexplanations can be provided to other LLM-based repair techniques such as\nAgentless to improve their output."}
{"id": "2507.22300", "pdf": "https://arxiv.org/pdf/2507.22300", "abs": "https://arxiv.org/abs/2507.22300", "authors": ["Phuc Truong Loc Nguyen", "Thanh Hung Do"], "title": "ConGaIT: A Clinician-Centered Dashboard for Contestable AI in Parkinson's Disease Care", "categories": ["cs.HC"], "comment": null, "summary": "AI-assisted gait analysis holds promise for improving Parkinson's Disease\n(PD) care, but current clinical dashboards lack transparency and offer no\nmeaningful way for clinicians to interrogate or contest AI decisions. We\npresent Con-GaIT (Contestable Gait Interpretation & Tracking), a\nclinician-centered system that advances Contestable AI through a tightly\nintegrated interface designed for interpretability, oversight, and procedural\nrecourse. Grounded in HCI principles, ConGaIT enables structured disagreement\nvia a novel Contest & Justify interaction pattern, supported by visual\nexplanations, role-based feedback, and traceable justification logs. Evaluated\nusing the Contestability Assessment Score (CAS), the framework achieves a score\nof 0.970, demonstrating that contestability can be operationalized through\nhuman-centered design in compliance with emerging regulatory standards. A\ndemonstration of the framework is available at\nhttps://github.com/hungdothanh/Con-GaIT."}
{"id": "2507.22442", "pdf": "https://arxiv.org/pdf/2507.22442", "abs": "https://arxiv.org/abs/2507.22442", "authors": ["Yukai Zhao", "Shaohua Wang", "Jue Wang", "Xing Hu", "Xin Xia"], "title": "Ensemble Fuzzing with Dynamic Resource Scheduling and Multidimensional Seed Evaluation", "categories": ["cs.SE"], "comment": "first submit", "summary": "Fuzzing is widely used for detecting bugs and vulnerabilities, with various\ntechniques proposed to enhance its effectiveness. To combine the advantages of\nmultiple technologies, researchers proposed ensemble fuzzing, which integrates\nmultiple base fuzzers. Despite promising results, state-of-the-art ensemble\nfuzzing techniques face limitations in resource scheduling and performance\nevaluation, leading to unnecessary resource waste. In this paper, we propose\nLegion, a novel ensemble fuzzing framework that dynamically schedules resources\nduring the ensemble fuzzing campaign. We designed a novel resource scheduling\nalgorithm based on the upper confidence bound algorithm to reduce the resource\nconsumption of ineffective base fuzzers. Additionally, we introduce a\nmultidimensional seed evaluation strategy, which considers multiple metrics to\nachieve more comprehensive fine-grained performance evaluation. We implemented\nLegion as a prototype tool and evaluated its effectiveness on Google's\nfuzzer-test-suite as well as real-world open-source projects. Results show that\nLegion outperforms existing state-of-the-art base fuzzers and ensemble fuzzing\ntechniques, detecting 20 vulnerabilities in real-world open-source\nprojects-five previously unknown and three classified as CVEs."}
{"id": "2507.22329", "pdf": "https://arxiv.org/pdf/2507.22329", "abs": "https://arxiv.org/abs/2507.22329", "authors": ["Erin Gatz", "Yasmine Kotturi", "Andrea Afua Kwamya", "Sarah Fox"], "title": "A Node on the Constellation: The Role of Feminist Makerspaces in Building and Sustaining Alternative Cultures of Technology Production", "categories": ["cs.HC"], "comment": null, "summary": "Feminist makerspaces offer community led alternatives to dominant tech\ncultures by centering care, mutual aid, and collective knowledge production.\nWhile prior CSCW research has explored their inclusive practices, less is known\nabout how these spaces sustain themselves over time. Drawing on interviews with\n18 founders and members across 8 U.S. feminist makerspaces as well as\nautoethnographic reflection, we examine the organizational and relational\npractices that support long-term endurance. We find that sustainability is not\nachieved through growth or institutionalization, but through care-driven\nstewardship, solidarity with local justice movements, and shared governance.\nThese social practices position feminist makerspaces as prefigurative\ncounterspaces - sites that enact, rather than defer, feminist values in\neveryday practice. This paper offers empirical insight into how feminist\nmakerspaces persist amid structural precarity, and highlights the forms of\nlabor and coalition-building that underpin alternative sociotechnical\ninfrastructures."}
{"id": "2507.22538", "pdf": "https://arxiv.org/pdf/2507.22538", "abs": "https://arxiv.org/abs/2507.22538", "authors": ["Matilde Gargiani", "Robin Sieber", "Philip Pawlowsky", "John Lygeros"], "title": "Inside madupite: Technical Design and Performance", "categories": ["cs.SE"], "comment": null, "summary": "In this work, we introduce and benchmark madupite, a newly proposed\nhigh-performance solver designed for large-scale discounted infinite-horizon\nMarkov decision processes with finite state and action spaces. After a brief\noverview of the class of mathematical optimization methods on which madupite\nrelies, we provide details on implementation choices, technical design and\ndeployment. We then demonstrate its scalability and efficiency by showcasing\nits performance on the solution of Markov decision processes arising from\ndifferent application areas, including epidemiology and classical control.\nMadupite sets a new standard as, to the best of our knowledge, it is the only\nsolver capable of efficiently computing exact solutions for large-scale Markov\ndecision processes, even when these exceed the memory capacity of modern\nlaptops and operate in near-undiscounted settings. This is possible as madupite\ncan work in a fully distributed manner and therefore leverage the memory\nstorage and computation capabilities of modern high-performance computing\nclusters. This key feature enables the solver to efficiently handle problems of\nmedium to large size in an exact manner instead of necessarily resorting to\nfunction approximations. Moreover, madupite is unique in allowing users to\ncustomize the solution algorithm to better exploit the specific structure of\ntheir problem, significantly accelerating convergence especially in\nlarge-discount factor settings. Overall, madupite represents a significant\nadvancement, offering unmatched scalability and flexibility in solving\nlarge-scale Markov decision processes."}
{"id": "2507.22352", "pdf": "https://arxiv.org/pdf/2507.22352", "abs": "https://arxiv.org/abs/2507.22352", "authors": ["Mykola Maslych", "Mohammadreza Katebi", "Christopher Lee", "Yahya Hmaiti", "Amirpouya Ghasemaghaei", "Christian Pumarada", "Janneese Palmer", "Esteban Segarra Martinez", "Marco Emporio", "Warren Snipes", "Ryan P. McMahan", "Joseph J. LaViola Jr"], "title": "Mitigating Response Delays in Free-Form Conversations with LLM-powered Intelligent Virtual Agents", "categories": ["cs.HC", "H.1.2; H.5.2; I.2.7; I.3.7"], "comment": "15 pages, 8 figures. Published at the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25), July 8-10, 2025, Waterloo, Canada.\n  Open-source code available at https://github.com/ISUE/iva-cui", "summary": "We investigated the challenges of mitigating response delays in free-form\nconversations with virtual agents powered by Large Language Models (LLMs)\nwithin Virtual Reality (VR). For this, we used conversational fillers, such as\ngestures and verbal cues, to bridge delays between user input and system\nresponses and evaluate their effectiveness across various latency levels and\ninteraction scenarios. We found that latency above 4 seconds degrades quality\nof experience, while natural conversational fillers improve perceived response\ntime, especially in high-delay conditions. Our findings provide insights for\npractitioners and researchers to optimize user engagement whenever\nconversational systems' responses are delayed by network limitations or slow\nhardware. We also contribute an open-source pipeline that streamlines deploying\nconversational agents in virtual environments."}
{"id": "2507.22580", "pdf": "https://arxiv.org/pdf/2507.22580", "abs": "https://arxiv.org/abs/2507.22580", "authors": ["Marcos Fuster-Pena", "David de-Fitero-Dominguez", "Antonio Garcia-Cabot", "Eva Garcia-Lopez"], "title": "RePaCA: Leveraging Reasoning Large Language Models for Static Automated Patch Correctness Assessment", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Automated Program Repair (APR) seeks to automatically correct software bugs\nwithout requiring human intervention. However, existing tools tend to generate\npatches that satisfy test cases without fixing the underlying bug, those are\nknown as overfitting patches. To address this issue, Automated Patch\nCorrectness Assessment (APCA) attempts to identify overfitting patches\ngenerated by APR tools. It can be solved as a static approach, meaning that no\nadditional information is needed beyond the original and fixed code snippets.\nCurrent static techniques often struggle with reliability, flexibility and\ntransparency. To address these issues, we introduce RePaCA, a novel static APCA\ntechnique that leverages Large Language Models (LLMs) specialized in thinking\ntasks. Our model is prompted with both buggy and fixed code snippets and guided\nto generate a Chain of Thought that analyses code differences, reasons about\nhow the patch addresses the root cause, and ultimately provides a binary\nclassification: correct or overfitting. To enhance these reasoning capabilities\nfor the APCA task specifically, the LLM is finetuned using Reinforcement\nLearning with the Group Relative Policy Optimization algorithm. When evaluated\non a standard Defects4J-derived test, our approach achieves state-of-the-art\nperformance, with 83.1% accuracy and an 84.8% F1-score. Furthermore, our model\ndemonstrates superior generalization capabilities when trained on different\ndatasets, outperforming the leading technique. This reasoning capability also\nprovides enhanced explainability for the patch assessment. These findings\nunderscore the considerable promise of finetuned, reasoning LLMs to advance\nstatic APCA by enhancing accuracy, generalization, and explainability."}
{"id": "2507.22382", "pdf": "https://arxiv.org/pdf/2507.22382", "abs": "https://arxiv.org/abs/2507.22382", "authors": ["Adel Sabour", "Ahmed Gadallah", "Hesham Hefny"], "title": "A Fuzzy Set-based Approach for Matching Hand-Drawing Shapes of Touch-based Gestures for Graphical Passwords", "categories": ["cs.HC"], "comment": null, "summary": "This paper presents a two-dimension fuzzy set based approach for matching\ntouch-based gestures using fuzzy cued click point technique. The pro posed\napproach aims mainly to improve the acceptance of the most closed inac curate\nhand drawn gestures generated by the user compared with a predefined referenced\ngesture value that is stored in the user profile. Commonly, gestures are used\nin order to facilitate the interactive capabilities between humans and\ncomputerized systems. Unfortunately, most of current gesturing techniques don't\ndeal at the same level of inaccuracy of gesturing, resulted from the nature of\nhu man fingers and hands movements. This paper aims, in a more flexible manner,\nto tackle the inaccuracy problem existed with gesture-based interactions\nbetween humans and a computerized system."}
{"id": "2507.22610", "pdf": "https://arxiv.org/pdf/2507.22610", "abs": "https://arxiv.org/abs/2507.22610", "authors": ["Ali Asgari", "Milan de Koning", "Pouria Derakhshanfar", "Annibale Panichella"], "title": "Metamorphic Testing of Deep Code Models: A Systematic Literature Review", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language models and deep learning models designed for code intelligence\nhave revolutionized the software engineering field due to their ability to\nperform various code-related tasks. These models can process source code and\nsoftware artifacts with high accuracy in tasks such as code completion, defect\ndetection, and code summarization; therefore, they can potentially become an\nintegral part of modern software engineering practices. Despite these\ncapabilities, robustness remains a critical quality attribute for deep-code\nmodels as they may produce different results under varied and adversarial\nconditions (e.g., variable renaming). Metamorphic testing has become a widely\nused approach to evaluate models' robustness by applying semantic-preserving\ntransformations to input programs and analyzing the stability of model outputs.\nWhile prior research has explored testing deep learning models, this systematic\nliterature review focuses specifically on metamorphic testing for deep code\nmodels. By studying 45 primary papers, we analyze the transformations,\ntechniques, and evaluation methods used to assess robustness. Our review\nsummarizes the current landscape, identifying frequently evaluated models,\nprogramming tasks, datasets, target languages, and evaluation metrics, and\nhighlights key challenges and future directions for advancing the field."}
{"id": "2507.22455", "pdf": "https://arxiv.org/pdf/2507.22455", "abs": "https://arxiv.org/abs/2507.22455", "authors": ["A. E. Fuentes-Cortázar", "A. Rivera-Hernández", "J. R. Rojano-Cáceres"], "title": "Analysis of User Experience Evaluation Methods for Deaf users: A Case Study on a mobile App", "categories": ["cs.HC", "H.5.2"], "comment": "15 pages, 2 figures, presented in Ibero-American Conference on\n  Human-Computer Interaction 2025", "summary": "User Experience (UX) evaluation methods that are commonly used with hearing\nusers may not be functional or effective for Deaf users. This is because these\nmethods are primarily designed for users with hearing abilities, which can\ncreate limitations in the interaction, perception, and understanding of the\nmethods for Deaf individuals. Furthermore, traditional UX evaluation approaches\noften fail to address the unique accessibility needs of Deaf users, resulting\nin an incomplete or biased assessment of their user experience. This research\nfocused on analyzing a set of UX evaluation methods recommended for use with\nDeaf users, with the aim of validating the accessibility of each method through\nfindings and limitations. The results indicate that, although these evaluation\nmethods presented here are commonly recommended in the literature for use with\nDeaf users, they present various limitations that must be addressed in order to\nbetter adapt to the communication skills specific to the Deaf community. This\nresearch concludes that evaluation methods must be adapted to ensure accessible\nsoftware evaluation for Deaf individuals, enabling the collection of data that\naccurately reflects their experiences and needs."}
{"id": "2507.22659", "pdf": "https://arxiv.org/pdf/2507.22659", "abs": "https://arxiv.org/abs/2507.22659", "authors": ["Sabrina Kaniewski", "Fabian Schmidt", "Markus Enzweiler", "Michael Menth", "Tobias Heer"], "title": "A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": "36 pages + 17 pages references, 6 tables, 10 figures", "summary": "The increasing adoption of Large Language Models (LLMs) in software\nengineering has sparked interest in their use for software vulnerability\ndetection. However, the rapid development of this field has resulted in a\nfragmented research landscape, with diverse studies that are difficult to\ncompare due to differences in, e.g., system designs and dataset usage. This\nfragmentation makes it difficult to obtain a clear overview of the\nstate-of-the-art or compare and categorize studies meaningfully. In this work,\nwe present a comprehensive systematic literature review (SLR) of LLM-based\nsoftware vulnerability detection. We analyze 227 studies published between\nJanuary 2020 and June 2025, categorizing them by task formulation, input\nrepresentation, system architecture, and adaptation techniques. Further, we\nanalyze the datasets used, including their characteristics, vulnerability\ncoverage, and diversity. We present a fine-grained taxonomy of vulnerability\ndetection approaches, identify key limitations, and outline actionable future\nresearch opportunities. By providing a structured overview of the field, this\nreview improves transparency and serves as a practical guide for researchers\nand practitioners aiming to conduct more comparable and reproducible research.\nWe publicly release all artifacts and maintain a living repository of LLM-based\nsoftware vulnerability detection studies."}
{"id": "2507.22614", "pdf": "https://arxiv.org/pdf/2507.22614", "abs": "https://arxiv.org/abs/2507.22614", "authors": ["Francis Geng", "Anshul Shah", "Haolin Li", "Nawab Mulla", "Steven Swanson", "Gerald Soosai Raj", "Daniel Zingaro", "Leo Porter"], "title": "Exploring Student-AI Interactions in Vibe Coding", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Background and Context. Chat-based and inline-coding-based GenAI has already\nhad substantial impact on the CS Education community. The recent introduction\nof ``vibe coding'' may further transform how students program, as it introduces\na new way for students to create software projects with minimal oversight.\n  Objectives. The purpose of this study is to understand how students in\nintroductory programming and advanced software engineering classes interact\nwith a vibe coding platform (Replit) when creating software and how the\ninteractions differ by programming background.\n  Methods. Interview participants were asked to think-aloud while building a\nweb application using Replit. Thematic analysis was then used to analyze the\nvideo recordings with an emphasis on the interactions between the student and\nReplit.\n  Findings. For both groups, the majority of student interactions with Replit\nwere to test or debug the prototype and only rarely did students visit code.\nPrompts by advanced software engineering students were much more likely to\ninclude relevant app feature and codebase contexts than those by introductory\nprogramming students."}
{"id": "2507.22664", "pdf": "https://arxiv.org/pdf/2507.22664", "abs": "https://arxiv.org/abs/2507.22664", "authors": ["Mashal Afzal Memon", "Gianluca Filippone", "Gian Luca Scoccia", "Marco Autili", "Paola Inverardi"], "title": "RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The presence of autonomous systems is growing at a fast pace and it is\nimpacting many aspects of our lives. Designed to learn and act independently,\nthese systems operate and perform decision-making without human intervention.\nHowever, they lack the ability to incorporate users' ethical preferences, which\nare unique for each individual in society and are required to personalize the\ndecision-making processes. This reduces user trust and prevents autonomous\nsystems from behaving according to the moral beliefs of their end-users. When\nmultiple systems interact with differing ethical preferences, they must\nnegotiate to reach an agreement that satisfies the ethical beliefs of all the\nparties involved and adjust their behavior consequently. To address this\nchallenge, this paper proposes RobEthiChor, an approach that enables autonomous\nsystems to incorporate user ethical preferences and contextual factors into\ntheir decision-making through ethics-based negotiation. RobEthiChor features a\ndomain-agnostic reference architecture for designing autonomous systems capable\nof ethic-based negotiating. The paper also presents RobEthiChor-Ros, an\nimplementation of RobEthiChor within the Robot Operating System (ROS), which\ncan be deployed on robots to provide them with ethics-based negotiation\ncapabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real\nrobots and ran scenarios where a pair of robots negotiate upon resource\ncontention. Experimental results demonstrate the feasibility and effectiveness\nof the system in realizing ethics-based negotiation. RobEthiChor allowed robots\nto reach an agreement in more than 73\\% of the scenarios with an acceptable\nnegotiation time (0.67s on average). Experiments also demonstrate that the\nnegotiation approach implemented in RobEthiChor is scalable."}
{"id": "2507.22671", "pdf": "https://arxiv.org/pdf/2507.22671", "abs": "https://arxiv.org/abs/2507.22671", "authors": ["Sami Saeed Alghamdi", "Christopher Bull", "Ahmed Kharrufa"], "title": "Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SE", "H.5.2; H.5.4"], "comment": "10 pages, 9 figures", "summary": "Many people learn programming independently from online resources and often\nreport struggles in achieving their personal learning goals. Learners\nfrequently describe their experiences as isolating and frustrating, challenged\nby abundant uncertainties, information overload, and distraction, compounded by\nlimited guidance. At the same time, social media serves as a personal space\nwhere many engage in diverse self-regulation practices, including help-seeking,\nusing external memory aids (e.g., self-notes), self-reflection, emotion\nregulation, and self-motivation. For instance, learners often mark achievements\nand set milestones through their posts. In response, we developed a system\nconsisting of a web platform and browser extensions to support self-regulation\nonline. The design aims to add learner-defined structure to otherwise\nunstructured experiences and bring meaning to curation and reflection\nactivities by translating them into learning stories with AI-generated\nfeedback. We position storytelling as an integrative approach to design that\nconnects resource curation, reflective and sensemaking practice, and narrative\npractices learners already use across social platforms. We recruited 15\ninformal programming learners who are regular social media users to engage with\nthe system in a self-paced manner; participation concluded upon submitting a\nlearning story and survey. We used three quantitative scales and a qualitative\nsurvey to examine users' characteristics and perceptions of the system's\nsupport for their self-regulation. User feedback suggests the system's\nviability as a self-regulation aid. Learners particularly valued in-situ\nreflection, automated story feedback, and video annotation, while other\nfeatures received mixed views. We highlight perceived benefits, friction\npoints, and design opportunities for future AI-augmented self-regulation tools."}
{"id": "2507.22800", "pdf": "https://arxiv.org/pdf/2507.22800", "abs": "https://arxiv.org/abs/2507.22800", "authors": ["Rui Ren"], "title": "The Multi-Agent Fault Localization System Based on Monte Carlo Tree Search Approach", "categories": ["cs.SE"], "comment": null, "summary": "In real-world scenarios, due to the highly decoupled and flexible nature of\nmicroservices, it poses greater challenges to system reliability. The more\nfrequent occurrence of incidents has created a demand for Root Cause\nAnalysis(RCA) methods that enable rapid identification and recovery of\nincidents. Large language model (LLM) provides a new path for quickly locating\nand recovering from incidents by leveraging their powerful generalization\nability combined with expert experience. Current LLM for RCA frameworks are\nbased on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM\nand the propagation nature of anomalies often lead to incorrect localization\nresults. Moreover, the massive amount of anomalous information generated in\nlarge, complex systems presents a huge challenge for the context window length\nof LLMs. To address these challenges, we propose KnowledgeMind, an innovative\nLLM multi-agent system based on Monte Carlo Tree Search and a knowledge base\nreward mechanism for standardized service-by-service reasoning. Compared to\nState-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration\napproach significantly reduces the burden on the maximum context window length,\nrequiring only one-tenth of its size. Additionally, by incorporating a\nrule-based real-time reward mechanism, our method effectively mitigates\nhallucinations during the inference process. Compared to the SOTA LLM for RCA\nframework, our method achieves a 49.29% to 128.35% improvement in root cause\nlocalization accuracy."}
{"id": "2507.22810", "pdf": "https://arxiv.org/pdf/2507.22810", "abs": "https://arxiv.org/abs/2507.22810", "authors": ["Daniel Udekwe", "Dimitrios Bolkas", "Eren Erman Ozguven", "Ren Moses", "Qianwen", "Guo"], "title": "VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education", "categories": ["cs.HC", "cs.ET", "cs.SE"], "comment": null, "summary": "Surveying is a core component of civil engineering education, requiring\nstudents to engage in hands-on spatial measurement, instrumentation handling,\nand field-based decision-making. However, traditional instruction often poses\nlogistical and cognitive challenges that can hinder accessibility and student\nengagement. While virtual laboratories have gained traction in engineering\neducation, few are purposefully designed to support flexible, adaptive learning\nin surveying. To address this gap, we developed Virtual Reality for Immersive\nand Interactive Surveying Education (VRISE), an immersive virtual reality\nlaboratory that replicates ground-based and aerial surveying tasks through\ncustomizable, accessible, and user-friendly modules. VRISE features interactive\nexperiences such as differential leveling with a digital level equipment and\nwaypoint-based drone navigation, enhanced by input smoothing, adaptive\ninterfaces, and real-time feedback to accommodate diverse learning styles.\nEvaluation across multiple user sessions demonstrated consistent gains in\nmeasurement accuracy, task efficiency, and interaction quality, with a clear\nprogression in skill development across the ground-based and aerial surveying\nmodalities. By reducing cognitive load and physical demands, even in tasks\nrequiring fine motor control and spatial reasoning, VRISE demonstrates the\npotential of immersive, repeatable digital environments to enhance surveying\neducation, broaden participation, and strengthen core competencies in a safe\nand engaging setting."}
{"id": "2507.22853", "pdf": "https://arxiv.org/pdf/2507.22853", "abs": "https://arxiv.org/abs/2507.22853", "authors": ["Haichuan Hu", "Xiaochen Xie", "Quanjun Zhang"], "title": "Repair-R1: Better Test Before Repair", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step."}
{"id": "2507.22839", "pdf": "https://arxiv.org/pdf/2507.22839", "abs": "https://arxiv.org/abs/2507.22839", "authors": ["Javier Jimenez-Honrado", "Javier Gomez Garcia", "Felipe Costa-Tebar", "Felix A. Marco", "Jose A. Gallud", "Gabriel Sebastian Rivera"], "title": "Progressive Web Application for Storytelling Therapy Support", "categories": ["cs.HC"], "comment": "Interaccion 2024", "summary": "In spite of all advances promoted by information technologies, there are\nstill activities where this technology is not applied for reasons such as being\ncarried out in non-profit organizations or because they have not adapted to\nthis modernization. Until recently, the way to work with mobile devices was\neither by connecting through a web page with the device's browser, or by\ndownloading an application from the corresponding platform. But lately,\ntechnologies are being developed that aim to break with this, as in the case of\nProgressive Web Applications (PWA). One of the advantages offered by PWA is to\naccess the web page and install it as an application on the device. The purpose\nof this article is to design a progressive Web application for the support of\nStorytelling Therapy, one of the novel therapies applied in the field of mental\nhealth. In addition to providing a software application to enhance Storytelling\nTherapy workshops, it is also intended to analyze and verify the advantages of\nPWA in a real case."}
{"id": "2507.22871", "pdf": "https://arxiv.org/pdf/2507.22871", "abs": "https://arxiv.org/abs/2507.22871", "authors": ["Domhnall Carlin", "Austen Rainer"], "title": "Tracking research software outputs in the UK", "categories": ["cs.SE", "cs.DL", "D.2.13"], "comment": null, "summary": "Research software is crucial in the research process and the growth of Open\nScience underscores the importance of accessing research artifacts, like data\nand code, raising traceability challenges among outputs. While it is a clear\nprinciple that research code, along with other essential outputs, should be\nrecognised as artifacts of the research process, the how of this principle\nremains variable. This study examines where UK academic institutions store and\nregister software as a unique research output, searching the UKRI's Gateway to\nResearch (GtR) metadata for publicly funded research software in the UK. The\nquantity of software reported as research outcomes remains low in proportion to\nother categories. Artifact sharing appears low, with one-quarter of the\nreported software having no links and 45% having either a missing or erroneous\nURL. Of the valid URLs, we find the single largest category is Public\nCommercial Code Repository, with GitHub being the host of 18% of all publicly\nfunded research software listed. These observations are contrasted with past\nfindings from 2023 and finally, we discuss the lack of artifact sharing in UK\nresearch, with resulting implications for the maintenance and evolution of\nresearch software. Without dissemination, research software risks demotion to a\ntransient artifact, useful only to meet short term research demands but\nultimately lost to the broader enterprise of science."}
{"id": "2507.22094", "pdf": "https://arxiv.org/pdf/2507.22094", "abs": "https://arxiv.org/abs/2507.22094", "authors": ["Nicholas Mehlman", "Jean-Christophe Gagnon-Audet", "Michael Shvartsman", "Kelvin Niu", "Alexander H. Miller", "Shagun Sodhani"], "title": "Scaling and Distilling Transformer Models for sEMG", "categories": ["eess.AS", "cs.AI", "cs.HC", "cs.LG"], "comment": "Accepted at TMLR 2025 (https://openreview.net/forum?id=hFPWThwUiZ),\n  11 pages", "summary": "Surface electromyography (sEMG) signals offer a promising avenue for\ndeveloping innovative human-computer interfaces by providing insights into\nmuscular activity. However, the limited volume of training data and\ncomputational constraints during deployment have restricted the investigation\nof scaling up the model size for solving sEMG tasks. In this paper, we\ndemonstrate that vanilla transformer models can be effectively scaled up on\nsEMG data and yield improved cross-user performance up to 110M parameters,\nsurpassing the model size regime investigated in other sEMG research (usually\n<10M parameters). We show that >100M-parameter models can be effectively\ndistilled into models 50x smaller with minimal loss of performance (<1.5%\nabsolute). This results in efficient and expressive models suitable for complex\nreal-time sEMG tasks in real-world environments."}
{"id": "2507.22099", "pdf": "https://arxiv.org/pdf/2507.22099", "abs": "https://arxiv.org/abs/2507.22099", "authors": ["Shuqing Li", "Qiang Chen", "Xiaoxue Ren", "Michael R. Lyu"], "title": "Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "comment": null, "summary": "Physics Engines (PEs) are fundamental software frameworks that simulate\nphysical interactions in applications ranging from entertainment to\nsafety-critical systems. Despite their importance, PEs suffer from physics\nfailures, deviations from expected physical behaviors that can compromise\nsoftware reliability, degrade user experience, and potentially cause critical\nfailures in autonomous vehicles or medical robotics. Current testing approaches\nfor PE-based software are inadequate, typically requiring white-box access and\nfocusing on crash detection rather than semantically complex physics failures.\nThis paper presents the first large-scale empirical study characterizing\nphysics failures in PE-based software. We investigate three research questions\naddressing the manifestations of physics failures, the effectiveness of\ndetection techniques, and developer perceptions of current detection practices.\nOur contributions include: (1) a taxonomy of physics failure manifestations;\n(2) a comprehensive evaluation of detection methods including deep learning,\nprompt-based techniques, and large multimodal models; and (3) actionable\ninsights from developer experiences for improving detection approaches. To\nsupport future research, we release PhysiXFails, code, and other materials at\nhttps://sites.google.com/view/physics-failure-detection."}
{"id": "2507.22205", "pdf": "https://arxiv.org/pdf/2507.22205", "abs": "https://arxiv.org/abs/2507.22205", "authors": ["Black Sun", "Die", "Hu"], "title": "CTG-Insight: A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Remote fetal monitoring technologies are becoming increasingly common. Yet,\nmost current systems offer limited interpretability, leaving expectant parents\nwith raw cardiotocography (CTG) data that is difficult to understand. In this\nwork, we present CTG-Insight, a multi-agent LLM system that provides structured\ninterpretations of fetal heart rate (FHR) and uterine contraction (UC) signals.\nDrawing from established medical guidelines, CTG-Insight decomposes each CTG\ntrace into five medically defined features: baseline, variability,\naccelerations, decelerations, and sinusoidal pattern, each analyzed by a\ndedicated agent. A final aggregation agent synthesizes the outputs to deliver a\nholistic classification of fetal health, accompanied by a natural language\nexplanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare\nit against deep learning models and the single-agent LLM baseline. Results show\nthat CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score\n(97.8%) while producing transparent and interpretable outputs. This work\ncontributes an interpretable and extensible CTG analysis framework."}
{"id": "2507.22371", "pdf": "https://arxiv.org/pdf/2507.22371", "abs": "https://arxiv.org/abs/2507.22371", "authors": ["Lei Yu", "Shiqi Cheng", "Zhirong Huang", "Jingyuan Zhang", "Chenjie Shen", "Junyi Lu", "Li Yang", "Fengjun Zhang", "Jiajia Ma"], "title": "SAEL: Leveraging Large Language Models with Adaptive Mixture-of-Experts for Smart Contract Vulnerability Detection", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": "Accepted to ICSME 2025", "summary": "With the increasing security issues in blockchain, smart contract\nvulnerability detection has become a research focus. Existing vulnerability\ndetection methods have their limitations: 1) Static analysis methods struggle\nwith complex scenarios. 2) Methods based on specialized pre-trained models\nperform well on specific datasets but have limited generalization capabilities.\nIn contrast, general-purpose Large Language Models (LLMs) demonstrate\nimpressive ability in adapting to new vulnerability patterns. However, they\noften underperform on specific vulnerability types compared to methods based on\nspecialized pre-trained models. We also observe that explanations generated by\ngeneral-purpose LLMs can provide fine-grained code understanding information,\ncontributing to improved detection performance.\n  Inspired by these observations, we propose SAEL, an LLM-based framework for\nsmart contract vulnerability detection. We first design targeted prompts to\nguide LLMs in identifying vulnerabilities and generating explanations, which\nserve as prediction features. Next, we apply prompt-tuning on CodeT5 and T5 to\nprocess contract code and explanations, enhancing task-specific performance. To\ncombine the strengths of each approach, we introduce an Adaptive\nMixture-of-Experts architecture. This dynamically adjusts feature weights via a\nGating Network, which selects relevant features using TopK filtering and\nSoftmax normalization, and incorporates a Multi-Head Self-Attention mechanism\nto enhance cross-feature relationships. This design enables effective\nintegration of LLM predictions, explanation features, and code features through\ngradient optimization. The loss function jointly considers both independent\nfeature performance and overall weighted predictions. Experiments show that\nSAEL outperforms existing methods across various vulnerabilities."}
{"id": "2507.22358", "pdf": "https://arxiv.org/pdf/2507.22358", "abs": "https://arxiv.org/abs/2507.22358", "authors": ["Hussein Mozannar", "Gagan Bansal", "Cheng Tan", "Adam Fourney", "Victor Dibia", "Jingya Chen", "Jack Gerrits", "Tyler Payne", "Matheus Kunzler Maldaner", "Madeleine Grunde-McLaughlin", "Eric Zhu", "Griffin Bassman", "Jacob Alber", "Peter Chang", "Ricky Loynd", "Friederike Niedtner", "Ece Kamar", "Maya Murad", "Rafah Hosn", "Saleema Amershi"], "title": "Magentic-UI: Towards Human-in-the-loop Agentic Systems", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "AI agents powered by large language models are increasingly capable of\nautonomously completing complex, multi-step tasks using external tools. Yet,\nthey still fall short of human-level performance in most domains including\ncomputer use, software development, and research. Their growing autonomy and\nability to interact with the outside world, also introduces safety and security\nrisks including potentially misaligned actions and adversarial manipulation. We\nargue that human-in-the-loop agentic systems offer a promising path forward,\ncombining human oversight and control with AI efficiency to unlock productivity\nfrom imperfect systems. We introduce Magentic-UI, an open-source web interface\nfor developing and studying human-agent interaction. Built on a flexible\nmulti-agent architecture, Magentic-UI supports web browsing, code execution,\nand file manipulation, and can be extended with diverse tools via Model Context\nProtocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for\nenabling effective, low-cost human involvement: co-planning, co-tasking,\nmulti-tasking, action guards, and long-term memory. We evaluate Magentic-UI\nacross four dimensions: autonomous task completion on agentic benchmarks,\nsimulated user testing of its interaction capabilities, qualitative studies\nwith real users, and targeted safety assessments. Our findings highlight\nMagentic-UI's potential to advance safe and efficient human-agent\ncollaboration."}
{"id": "2507.22384", "pdf": "https://arxiv.org/pdf/2507.22384", "abs": "https://arxiv.org/abs/2507.22384", "authors": ["Umar Siddiqui", "Habiba Youssef", "Adel Sabour", "Mohamed Ali"], "title": "Scalability, Availability, Reproducibility and Extensibility in Islamic Database Systems", "categories": ["cs.DB", "cs.SE"], "comment": null, "summary": "With the widespread of software systems and applications that serve the\nIslamic knowledge domain, several concerns arise. Authenticity and accuracy of\nthe databases that back up these systems are questionable. With the excitement\nthat some software developers and amateur researchers may have, false\nstatements and incorrect claims may be made around numerical signs or miracles\nin the Quran. Reproducibility of these claims may not be addressed by the\npeople making such claims. Moreover, with the increase in the number of users,\nscalability and availability of these systems become a concern. In addition to\nall these concerns, extensibility is also another major issue. Properly\ndesigned systems can be extensible, reusable and built on top of one another,\ninstead of each system being built from scratch every time a new framework is\ndeveloped. In this paper, we introduce the QuranResearch.Org system and its\nvision for scalability, availability, reproducibility and extensibility to\nserve Islamic database systems."}
{"id": "2507.22365", "pdf": "https://arxiv.org/pdf/2507.22365", "abs": "https://arxiv.org/abs/2507.22365", "authors": ["ZhaoBin Li", "Mark Steyvers"], "title": "Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making", "categories": ["cs.AI", "cs.HC"], "comment": "26 pages, 5 figures, submitted to Decision Analysis", "summary": "In settings where human decision-making relies on AI input, both the\npredictive accuracy of the AI system and the reliability of its confidence\nestimates influence decision quality. We highlight the role of AI metacognitive\nsensitivity -- its ability to assign confidence scores that accurately\ndistinguish correct from incorrect predictions -- and introduce a theoretical\nframework for assessing the joint impact of AI's predictive accuracy and\nmetacognitive sensitivity in hybrid decision-making settings. Our analysis\nidentifies conditions under which an AI with lower predictive accuracy but\nhigher metacognitive sensitivity can enhance the overall accuracy of human\ndecision making. Finally, a behavioral experiment confirms that greater AI\nmetacognitive sensitivity improves human decision performance. Together, these\nfindings underscore the importance of evaluating AI assistance not only by\naccuracy but also by metacognitive sensitivity, and of optimizing both to\nachieve superior decision outcomes."}
{"id": "2507.22671", "pdf": "https://arxiv.org/pdf/2507.22671", "abs": "https://arxiv.org/abs/2507.22671", "authors": ["Sami Saeed Alghamdi", "Christopher Bull", "Ahmed Kharrufa"], "title": "Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SE", "H.5.2; H.5.4"], "comment": "10 pages, 9 figures", "summary": "Many people learn programming independently from online resources and often\nreport struggles in achieving their personal learning goals. Learners\nfrequently describe their experiences as isolating and frustrating, challenged\nby abundant uncertainties, information overload, and distraction, compounded by\nlimited guidance. At the same time, social media serves as a personal space\nwhere many engage in diverse self-regulation practices, including help-seeking,\nusing external memory aids (e.g., self-notes), self-reflection, emotion\nregulation, and self-motivation. For instance, learners often mark achievements\nand set milestones through their posts. In response, we developed a system\nconsisting of a web platform and browser extensions to support self-regulation\nonline. The design aims to add learner-defined structure to otherwise\nunstructured experiences and bring meaning to curation and reflection\nactivities by translating them into learning stories with AI-generated\nfeedback. We position storytelling as an integrative approach to design that\nconnects resource curation, reflective and sensemaking practice, and narrative\npractices learners already use across social platforms. We recruited 15\ninformal programming learners who are regular social media users to engage with\nthe system in a self-paced manner; participation concluded upon submitting a\nlearning story and survey. We used three quantitative scales and a qualitative\nsurvey to examine users' characteristics and perceptions of the system's\nsupport for their self-regulation. User feedback suggests the system's\nviability as a self-regulation aid. Learners particularly valued in-situ\nreflection, automated story feedback, and video annotation, while other\nfeatures received mixed views. We highlight perceived benefits, friction\npoints, and design opportunities for future AI-augmented self-regulation tools."}
{"id": "2507.22665", "pdf": "https://arxiv.org/pdf/2507.22665", "abs": "https://arxiv.org/abs/2507.22665", "authors": ["Max Sondag", "Christofer Meinecke", "Dennis Collaris", "Tatiana von Landesberger", "Stef van den Elzen"], "title": "Cluster-Based Random Forest Visualization and Interpretation", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "Random forests are a machine learning method used to automatically classify\ndatasets and consist of a multitude of decision trees. While these random\nforests often have higher performance and generalize better than a single\ndecision tree, they are also harder to interpret. This paper presents a\nvisualization method and system to increase interpretability of random forests.\nWe cluster similar trees which enables users to interpret how the model\nperforms in general without needing to analyze each individual decision tree in\ndetail, or interpret an oversimplified summary of the full forest. To\nmeaningfully cluster the decision trees, we introduce a new distance metric\nthat takes into account both the decision rules as well as the predictions of a\npair of decision trees. We also propose two new visualization methods that\nvisualize both clustered and individual decision trees: (1) The Feature Plot,\nwhich visualizes the topological position of features in the decision trees,\nand (2) the Rule Plot, which visualizes the decision rules of the decision\ntrees. We demonstrate the efficacy of our approach through a case study on the\n\"Glass\" dataset, which is a relatively complex standard machine learning\ndataset, as well as a small user study."}
{"id": "2507.22810", "pdf": "https://arxiv.org/pdf/2507.22810", "abs": "https://arxiv.org/abs/2507.22810", "authors": ["Daniel Udekwe", "Dimitrios Bolkas", "Eren Erman Ozguven", "Ren Moses", "Qianwen", "Guo"], "title": "VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education", "categories": ["cs.HC", "cs.ET", "cs.SE"], "comment": null, "summary": "Surveying is a core component of civil engineering education, requiring\nstudents to engage in hands-on spatial measurement, instrumentation handling,\nand field-based decision-making. However, traditional instruction often poses\nlogistical and cognitive challenges that can hinder accessibility and student\nengagement. While virtual laboratories have gained traction in engineering\neducation, few are purposefully designed to support flexible, adaptive learning\nin surveying. To address this gap, we developed Virtual Reality for Immersive\nand Interactive Surveying Education (VRISE), an immersive virtual reality\nlaboratory that replicates ground-based and aerial surveying tasks through\ncustomizable, accessible, and user-friendly modules. VRISE features interactive\nexperiences such as differential leveling with a digital level equipment and\nwaypoint-based drone navigation, enhanced by input smoothing, adaptive\ninterfaces, and real-time feedback to accommodate diverse learning styles.\nEvaluation across multiple user sessions demonstrated consistent gains in\nmeasurement accuracy, task efficiency, and interaction quality, with a clear\nprogression in skill development across the ground-based and aerial surveying\nmodalities. By reducing cognitive load and physical demands, even in tasks\nrequiring fine motor control and spatial reasoning, VRISE demonstrates the\npotential of immersive, repeatable digital environments to enhance surveying\neducation, broaden participation, and strengthen core competencies in a safe\nand engaging setting."}
