<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 19]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.CL](#cs.CL) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 技术面试是软件工程师求职的关键环节，但准备过程复杂且缺乏支持。本研究调查了131名候选人的准备方法及其教育背景，发现候选人缺乏真实场景训练，导致压力和准备不足。


<details>
  <summary>Details</summary>
Motivation: 理解技术面试候选人如何准备，探讨准备方法和教育的作用。

Method: 通过调查131名正在准备技术面试的候选人。

Result: 候选人很少在真实场景中训练，课程未能有效支持准备，导致压力和准备不足。

Conclusion: 研究结果为利益相关者提供了改进技术面试准备的建议。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [2] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 该论文提出了一种利用自然语言进行代码结构搜索的新方法，通过结合LLM的推理能力和结构搜索工具，显著降低了查询难度并提高了搜索效果。


<details>
  <summary>Details</summary>
Motivation: 现有代码结构搜索工具使用领域特定语言（DSL），学习成本高。自然语言查询可以降低使用门槛。

Method: 开发了一种结合LLM解释自然语言查询与结构搜索工具的方法，并在Semgrep和GQL上进行了应用。

Result: 在400个查询的评估中，新方法的准确率和召回率达到55%-70%，优于基线方法。

Conclusion: 自然语言查询为结构代码搜索提供了直观且高效的方式，显著提升了用户体验和性能。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [3] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 本研究通过分析开源Android应用的内源代码指标，探讨了其在预测应用受欢迎程度（如用户评分和下载量）上的潜力。虽然回归模型效果不佳，但分类模型表现较好，表明内源代码指标可作为受欢迎程度的有效指标。


<details>
  <summary>Details</summary>
Motivation: 预测移动应用发布前的受欢迎程度可为开发者提供竞争优势，但目前尚缺乏可靠方法。本研究旨在探索内源代码指标是否能够预测应用受欢迎程度。

Method: 研究收集了446个开源Android应用的数据，包括代码指标、代码异味和元数据，并评估了回归和分类模型在不同特征集上的表现。

Result: 回归模型效果不佳，但分类模型（特别是多层感知器）在二元分类任务中表现良好，F1分数达0.72。

Conclusion: 内源代码指标虽解释力有限，但可作为应用受欢迎程度的有用指标，挑战了之前认为内源指标无法预测软件质量的观点。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [4] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 该研究比较了心理测量压力指标与生物特征指标，旨在识别软件工程任务中的压力相关模式，但发现所选的时间限制方法不足以诱发压力，结果多样。


<details>
  <summary>Details</summary>
Motivation: 传统上，幸福感、压力等人类因素研究依赖于自评工具，但这些工具可能存在偏差，因此研究者希望结合更客观的方法（如生理测量）来改进评估。

Method: 实验包括预调查、编程任务（佩戴生物传感器）、简短后调查和退出访谈，以比较心理测量和生物特征数据。

Result: 结果显示心理测量工具未检测到压力，访谈中参与者报告了无压力或时间压力，生物特征数据仅EDA相位峰值有显著差异。

Conclusion: 研究认为时间限制不足以诱发压力，并为未来研究提供了方法学建议。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [5] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 该研究分析了五个平台上10个开发者通信数据集的语言和统计特征，评估了14种情感分析工具的性能，并提出了一种基于数据集特征选择合适工具的映射方法和问卷。


<details>
  <summary>Details</summary>
Motivation: 由于不同平台的通信风格和内容差异，现有情感分析工具在不同数据集上表现不一致，影响了团队动态理解和需求工程中AI驱动的可信分析。

Method: 研究分析了多个开发平台的通信数据集，评估多种情感分析工具的性能，并提出了一种基于数据集特征的映射方法和问卷。

Result: 结果显示，数据集特征可优化工具选择，且Transformer模型（如SetFit和RoBERTa）表现稳定，但工具效果仍依赖上下文。

Conclusion: 该方法为软件工程中情感分析工具的选择提供了可信支持，同时指出需持续评估以适应通信环境的变化。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [6] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 论文提出一种多代理方法，利用两个基于LLM的代理协作生成COBOL代码的解释，解决了COBOL因架构和语法差异导致的LLM窗口大小限制问题。


<details>
  <summary>Details</summary>
Motivation: 由于COBOL语言的老化、复杂性和开发者减少，维护缺乏文档的COBOL代码库变得困难。现有方法因COBOL的特性难以有效解释代码。

Method: 采用了两个LLM代理协作的方法，结合代码库的上下文信息生成函数、文件和项目的解释。

Result: 在14个开源COBOL项目中，方法在函数和文件级别解释上分别提升了12.67%和4.21%等指标，项目级别成功解释了82%的功能。

Conclusion: 多代理方法显著提升了COBOL代码的解释能力，适用于处理LLM窗口限制和代码复杂度问题。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [7] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: RTED是一种新型的类型感知测试生成技术，用于自动检测Python类型错误，结合了逐步类型约束分析和反射验证，显著提高了检测准确率并减少了误报。


<details>
  <summary>Details</summary>
Motivation: Python中的类型错误经常导致运行时故障，影响软件可靠性和开发效率。现有的静态分析工具误报率高，而单元测试生成技术又缺乏针对性指导。

Method: RTED通过逐步类型约束分析和反射验证指导测试生成过程，有效抑制误报。

Result: 实验显示，RTED在检测类型错误上比现有技术多发现22-29个错误，误报率降低了173.9%-245.9%，并在真实项目中发现了12个未知错误。

Conclusion: RTED为Python类型错误检测提供了更高效和准确的解决方案，显著提升了测试生成的质量。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [8] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: 该论文提出了垂直联邦推理审计框架（VeFIA），旨在解决垂直联邦学习（VFL）中缺乏对数据方推理软件执行正确性审计机制的问题，通过可信执行环境（TEE）确保隐私和效率。


<details>
  <summary>Details</summary>
Motivation: 现有VFL工作缺乏对数据方推理软件执行正确性的审计机制，为解决这一问题，设计了VeFIA框架。

Method: VeFIA利用可信执行环境（TEE）和协调器，通过随机采样验证数据方的计算结果，不泄露数据隐私且不增加系统延迟。

Result: 当异常推理超过5.4%时，VeFIA能以99.99%的概率检测出异常，且随机采样的验证在检测异常推理时达到100%的准确率和召回率。

Conclusion: 这是首个讨论VFL中推理软件执行正确性的论文，VeFIA框架在保证隐私和效率的同时，实现了高效的审计。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [9] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 提出了一种名为Meta-Fair的自动化方法，用于测试大语言模型（LLMs）中的公平性，减少对领域特定资源的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的公平性测试方法依赖手动评估和固定模板，资源密集且难以扩展。

Method: 结合变形测试（metamorphic testing）和LLMs的能力，生成和评估测试用例，自动检测偏差。

Result: Meta-Fair在12个预训练LLMs上测试，平均精度92%，揭示了29%的执行中存在偏差行为。

Conclusion: 尽管存在挑战，Meta-Fair展示了自动化LLM测试的潜力。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [10] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLMREI 是一种基于大语言模型的聊天机器人，旨在自动化需求采集访谈，减少人为错误并提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 需求采集访谈高度依赖分析师技能，易受人为偏见和沟通问题影响，自动化可解决这些问题。

Method: 采用零样本提示和最小到最多提示两种方法优化 LLMREI，在 33 次模拟访谈中评估其性能。

Result: LLMREI 在减少错误、提取需求、生成上下文相关问题上表现与人类接近，适合大规模访谈。

Conclusion: LLMREI 在自动化多人访谈中潜力显著，未来可进一步优化以提高性能。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [11] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 该论文研究如何通过新方法和框架解决自适应CPS中人类-机器组队（HMT）的挑战，重点关注交互原理、反馈循环及伦理价值观的整合。


<details>
  <summary>Details</summary>
Motivation: 自适应CPS因其动态调整能力日益重要，但将人类无缝集成到反馈循环中并确保伦理隐私仍是未解难题。

Method: 开发新方法和流程整合HMT到CPS反馈循环，并构建框架以在系统生命周期中验证伦理和人类价值观。

Result: 预期成果为更高效、伦理合规的HMT集成方案。

Conclusion: 通过技术改进与伦理框架的结合，有望实现自适应CPS中人与机器的无缝协作。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [12] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 研究发现RSE和SER群体在术语使用上存在差异，初步分析揭示了潜在的合作与学习机会。


<details>
  <summary>Details</summary>
Motivation: 探究RSE和SER群体在术语上的差异，以改善沟通挑战。

Method: 通过系统方法映射SE基础概念在RSE群体中的解读。

Result: 初步发现双方存在共同点和知识空白，为未来合作提供基础。

Conclusion: 术语映射方法为未来的众包验证和扩展奠定了基础。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [13] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: RLHGNN是一种新型框架，通过异构图建模和强化学习优化业务流程中的下一活动预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法捕捉非顺序关系且对复杂流程适应性差，亟需一种更能反映异构关系的动态建模方法。

Method: 将事件日志转化为异构图，结合三种边类型和强化学习选择最优图结构，并通过异构图卷积预测活动。

Result: 在6个真实数据集上优于现有方法，延迟低至1ms，适用于实时监控。

Conclusion: RLHGNN为服务架构中的动态流程建模和预测提供了高效、自适应的解决方案。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [14] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 论文提出可持续发展标志概念，用于识别云架构讨论中的可持续性，实验表明其效果优于单纯依赖定义。


<details>
  <summary>Details</summary>
Motivation: 随着云计算的兴起，软件系统可持续性日益重要，但目前缺乏明确的指南来识别实践者讨论中的可持续性概念。

Method: 通过主题分析开发可持续发展标志，并进行对照实验评估其效果。

Result: 初步结果表明，使用标志分类的帖子更少但更准确，且用户认为标志比定义更易理解和有用。

Conclusion: 可持续发展标志是识别云架构可持续性讨论的有效工具。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [15] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 提出了一种基于文本蕴涵和上下文学习的自动化方法，用于从法律文本生成可执行Python代码的规范表示。


<details>
  <summary>Details</summary>
Motivation: 小型企业和初创公司缺乏法律专业知识，手动确保软件合规性成本高。当前自动化方法存在局限，未考虑元数据间的关联性且依赖手动标注。

Method: 采用文本蕴涵和上下文学习，设计了领域特定元模型的Python类结构，捕捉法律元数据及其关联性。

Result: 在13个美国州数据泄露通知法律上测试，通过89.4%的测试案例，精确率和召回率分别为82.2和88.7。

Conclusion: 该方法减少了对大型标注数据集的依赖，提升了对新法律文本的适用性。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [16] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 研究探讨了使用GPT-4o在需求获取面试中生成后续问题的潜力，实验表明LLM生成的问题在清晰度、相关性和信息量上不亚于人类撰写的问题，且在指导下表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统的需求获取面试面临挑战如认知过载和信息过载，LLMs在自然语言处理任务中的卓越表现激发了其在面试支持中的应用。

Method: 构建了一个基于常见面试错误类型的框架，开发了基于被访者话语生成问题的方法，并进行了两项对照实验。

Result: LLM生成的问题在无指导和基于错误类型指导下均表现良好，后者甚至优于人类问题。

Conclusion: LLM在实时支持面试者提高需求获取质量方面具有潜力。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 论文提出了一种名为DecoRTL的新型解码策略，旨在解决大型语言模型在生成RTL代码时的结构化和语义问题，通过自适应温度调整和自一致性采样提升代码的正确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM解码策略在生成RTL代码时常因结构模糊或语义复杂导致输出错误或重复。本文通过分析令牌级熵，揭示了LLM在这些区域表现不佳的根本原因。

Method: DecoRTL通过两种组件：自一致性采样生成并重排名候选代码以提升正确性；语法感知温度调整根据令牌的语法和功能角色动态调整采样温度。

Result: 在VerilogEval基准测试中，DecoRTL显著提升了生成的RTL代码的语法有效性、功能正确性和输出多样性，且运行时开销几乎可忽略。

Conclusion: DecoRTL无需额外微调即可在推理时显著改善RTL代码生成质量，证明了其对LLM解码策略的创新性改进。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [18] [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
*Zongpu Zhang,Pranab Dash,Y. Charlie Hu,Qiang Xu,Jian Li,Haibing Guan*

Main category: cs.OS

TL;DR: 论文研究了在移动设备上部署大语言模型（LLM）时的能耗问题，提出了一个统一能源感知调节器FUSE，显著提升了能效。


<details>
  <summary>Details</summary>
Motivation: 由于LLM对计算、内存和能源的高需求，在资源有限的移动设备上部署时效率低下，现有调节器独立运行导致低效。

Method: 通过测量现有LLM框架的能效，分析其低效原因，并设计了一个统一的能源感知调节器FUSE。

Result: FUSE显著降低了首次令牌时间和每令牌输出时间，同时保持相同能源消耗。

Conclusion: FUSE能有效优化移动设备上LLM推理的能效，为实际部署提供了解决方案。

Abstract: Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [19] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 综述探讨了合成网络流量生成的替代方法，分析了其应对数据稀缺、隐私等问题的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决真实网络数据中的数据稀缺、隐私问题和纯度限制，同时保留真实世界特性。

Method: 综合评述了合成网络流量的生成方法，重点是深度学习和统计方法，以及商业工具。

Result: 总结了现有技术，突出了挑战和未来研究方向。

Conclusion: 为研究者和从业者提供了结构化分析，指明了合成网络流量生成的挑战与机遇。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [20] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 论文提出了一种名为ISN的机制和RXL扩展协议，旨在解决多节点互连中无提示丢失flit的问题，同时保持高效带宽。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型性能超越单处理器能力，芯片间互连成为可扩展计算的关键。但高传输速率的互连协议增加了错误风险，特别是多节点配置中的flit无提示丢失问题。

Method: 引入ISN机制实现精确flit丢失检测和有序交付，提出RXL扩展协议，结合ISN并升级CRC为传输层机制，利用FEC进行链路层纠错。

Result: RXL在不增加额外头部开销的情况下，提供了可靠的端到端数据和序列完整性，同时保持带宽效率。

Conclusion: ISN和RXL为多节点互连提供了可靠性和可扩展性解决方案，解决了现有协议在高性能互连中的局限。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [21] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 本文提出了一种通过Curated Collaborative Learning（CCL）框架和Distributed Unit Pooling Scheme（DUPS）来优化5G RAN能耗的方法，显著提升了网络能效和成本效益。


<details>
  <summary>Details</summary>
Motivation: 5G网络中，RAN的能耗占比超过50%，现有分裂选项未能充分利用数据潜力。本文旨在通过预测和动态资源分配降低运营成本。

Method: 采用CCL框架进行高精度的流量和用户预测，并通过DUPS利用深度强化学习优化DU服务器资源分配。

Result: CCL在预测性能上显著优于现有方法，DUPS将能效提升高达89%，显著降低了运营成本。

Conclusion: 结合CCL与DUPS的方法为5G RAN的能耗和成本问题提供了创新解决方案，显著提升了效率和经济效益。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [22] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: 本文探讨了AI代理在网络故障排查中的应用，并提出了一个标准化、可复现和开放的基准测试平台的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究已证明AI，尤其是大语言模型（LLMs）在网络配置和自动化诊断任务中的有效性，但缺乏一个统一的评估平台。

Method: 本文聚焦于AI代理在网络故障排查中的应用，并提出了构建低操作成本的评估平台的初步设想。

Result: 未提供具体实验结果，但强调了标准化平台的潜在价值。

Conclusion: 建立一个标准化、开放的基准测试平台对评估和改进AI代理的网络故障排查能力至关重要。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [23] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 本文提出了一种基于信道感知的语义通信框架，以提升车联网（IoV）中数据传输的准确性和效率，解决了动态无线信道条件下大规模数据传输和处理的挑战。


<details>
  <summary>Details</summary>
Motivation: 车联网（IoV）的发展需要高效的数据传输和处理技术，尤其是在动态无线信道条件下，现有的方法难以满足实时性和可靠性的需求。

Method: 提出了一个语义通信框架，结合生成扩散模型进行信道估计，并利用大模型对信道生成模型进行微调，以增强其对不同场景的适应性。

Result: 在公开数据集上的实验验证了框架的性能和可靠性。

Conclusion: 该框架有效提升了IoV服务的质量，尤其在动态场景中表现出较好的适应性。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [24] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: REDUS是一种优化深度学习训练的重采样技术，用于解决SDN与DL工作负载之间的资源竞争问题。


<details>
  <summary>Details</summary>
Motivation: SDN控制器与DL工作负载共享基础设施时，资源竞争降低了SDN的响应速度，尤其在延迟敏感的IoT环境中。

Method: 提出REDUS（智能网络中高效数据利用的重采样），通过优先处理误分类样本和排除冗余数据来优化DL训练。

Result: 在CICIoT2023数据集上的测试显示，REDUS将训练时间减少72.6%，准确率仅下降1.62%。

Conclusion: REDUS为资源受限的边缘设备提供了高效且网络友好的DL训练方案。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [25] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 这篇论文提出了一种基于5G PRS信号的多站集成感知与通信(ISAC)信号处理链，实现了高精度的目标检测、定位与跟踪。


<details>
  <summary>Details</summary>
Motivation: 利用5G定位参考信号(PRS)实现多站ISAC的高性能感知与通信，解决目标检测、参数估计和跟踪问题。

Method: 采用分布式架构，通过相干交叉模糊函数(CAF)生成距离-多普勒图，提取目标的双时延和径向速度，并通过非线性最小二乘三边测量和正则化线性反演进行位置与速度估计。

Result: 实验结果展示了利用5G PRS信号实现高保真的移动目标检测、定位和跟踪。

Conclusion: 该方法在多站ISAC中有效利用了5G PRS信号，实现了高效的目标感知与跟踪。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [26] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 论文探讨了基于O-RAN原则的地面网络（TN）与非地面网络（NTN）整合的架构与功能分割策略，分析了性能、时延等方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决TN-NTN整合中因异构传播条件、动态拓扑和有限处理能力带来的挑战，并推动标准化与高效融合。

Method: 提出分割选项的分类法，分析不同配置（如星载DU或gNB/UPF）及RIC的灵活部署策略。

Result: 全面映射了架构分割与RIC部署选项，强调实现约束与互操作性。

Conclusion: 指出了TN-NTN融合的关键挑战与未来发展路径，以实现模块化与标准化。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [27] [TAGF: Time-aware Gated Fusion for Multimodal Valence-Arousal Estimation](https://arxiv.org/abs/2507.02080)
*Yubeen Lee,Sangeun Lee,Chaewon Park,Junyeop Cha,Eunil Park*

Main category: cs.MM

TL;DR: TAGF框架通过时间感知门控融合技术提升多模态情感识别性能，尤其在效价-唤醒估计中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态情感识别方法受噪声和模态不对齐影响，导致性能下降。

Method: TAGF采用BiLSTM时间门控机制，自适应调节递归注意力输出，整合多步跨模态特征。

Result: 实验显示TAGF在Aff-Wild2数据集上性能优越，对模态不对齐具有强鲁棒性。

Conclusion: TAGF能有效捕捉情感动态变化，适用于真实场景。

Abstract: Multimodal emotion recognition often suffers from performance degradation in
valence-arousal estimation due to noise and misalignment between audio and
visual modalities. To address this challenge, we introduce TAGF, a Time-aware
Gated Fusion framework for multimodal emotion recognition. The TAGF adaptively
modulates the contribution of recursive attention outputs based on temporal
dynamics. Specifically, the TAGF incorporates a BiLSTM-based temporal gating
mechanism to learn the relative importance of each recursive step and
effectively integrates multistep cross-modal features. By embedding temporal
awareness into the recursive fusion process, the TAGF effectively captures the
sequential evolution of emotional expressions and the complex interplay between
modalities. Experimental results on the Aff-Wild2 dataset demonstrate that TAGF
achieves competitive performance compared with existing recursive
attention-based models. Furthermore, TAGF exhibits strong robustness to
cross-modal misalignment and reliably models dynamic emotional transitions in
real-world conditions.

</details>


### [28] [VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning](https://arxiv.org/abs/2507.02626)
*Siran Chen,Boyu Chen,Chenyun Yu,Yuxiao Luo,Ouyang Yi,Lei Cheng,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.MM

TL;DR: 提出了VRAgent-R1，一种结合多模态内容理解和强化学习的新型推荐系统代理框架，显著提升了视频推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频推荐系统在用户模拟和多模态内容理解方面表现不足，限制了推荐效果。

Method: VRAgent-R1包含IP代理（基于MLLM模拟人类渐进式思维）和US代理（通过CoT推理和强化学习优化推荐）。

Result: IP代理在NDCG@10上提升6.0%，US代理在用户决策模拟中准确率提高45.0%。

Conclusion: VRAgent-R1通过结合多模态理解与强化学习，有效提升了视频推荐的性能。

Abstract: Owing to powerful natural language processing and generative capabilities,
large language model (LLM) agents have emerged as a promising solution for
enhancing recommendation systems via user simulation. However, in the realm of
video recommendation, existing studies predominantly resort to prompt-based
simulation using frozen LLMs and encounter the intricate challenge of
multimodal content understanding. This frequently results in suboptimal item
modeling and user preference learning, thereby ultimately constraining
recommendation performance. To address these challenges, we introduce
VRAgent-R1, a novel agent-based paradigm that incorporates human-like
intelligence in user simulation. Specifically, VRAgent-R1 comprises two
distinct agents: the Item Perception (IP) Agent and the User Simulation (US)
Agent, designed for interactive user-item modeling. Firstly, the IP Agent
emulates human-like progressive thinking based on MLLMs, effectively capturing
hidden recommendation semantics in videos. With a more comprehensive multimodal
content understanding provided by the IP Agent, the video recommendation system
is equipped to provide higher-quality candidate items. Subsequently, the US
Agent refines the recommended video sets based on in-depth chain-of-thought
(CoT) reasoning and achieves better alignment with real user preferences
through reinforcement learning. Experimental results on a large-scale video
recommendation benchmark have demonstrated the effectiveness of our proposed
VRAgent-R1 method, e.g., the IP Agent achieves a 6.0\% improvement in NDCG@10
on the MicroLens-100k dataset, while the US Agent shows approximately 45.0\%
higher accuracy in user decision simulation compared to state-of-the-art
baselines.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [29] [SMT-Sweep: Word-Level Representation Unification for Hardware Verification](https://arxiv.org/abs/2507.02008)
*Ziyi Yang,Guangyu Hu,Mingkai Miao,Changyuan Yu,Hongce Zhang*

Main category: cs.LO

TL;DR: 本文提出了一种名为SMT-Sweep的新方法，将SAT sweeping技术扩展到字级，基于SMT理论，用于硬件验证中的复杂运算和数组操作。


<details>
  <summary>Details</summary>
Motivation: 随着硬件验证中字级结构（如位向量操作、算术和数组）的广泛采用，缺乏字级的SAT sweeping对应技术。因此，作者旨在填补这一空白。

Method: SMT-Sweep结合了模拟和等价检测，处理SMT项，支持丰富的位向量操作和数组语义。框架包括随机化和约束驱动的字级模拟，适用于符号表达式和非纯布尔逻辑的运算符语义。

Result: 实验表明，SMT-Sweep在速度上显著优于最新的位级SAT sweeping和字级整体SMT求解（分别平均提高了44倍和69倍）。

Conclusion: SMT-Sweep是首个将sweeping技术引入SMT硬件验证的工作，为相关领域提供了创新解决方案。开源实现已发布。

Abstract: SAT sweeping has long been a cornerstone technique in logic simplification
and equivalence checking at the bit level, leveraging structural hashing,
simulation and SAT solving to prune redundant logic. However, with the growing
adoption of word-level constructs in hardware verification, such as bit-vector
operations, arithmetics and arrays, there lacks a counterpart of SAT sweeping
at the word level. In this paper, we introduce SMT-Sweep, a novel extension of
SAT sweeping into the word level, grounded in Satisfiability Modulo Theories
(SMT). SMT-Sweep takes advantage of simulation and equivalence detection to
handle SMT terms with rich bit-vector operations and array semantics. Our
framework incorporates both randomized and constraint-driven word-level
simulation tailored to symbolic expressions and operator semantics beyond pure
Boolean logic. Experimental results show that SMT-Sweep achieves significant
speed-up compared to state-of-the-art bit-level SAT sweeping and word-level
monolithic SMT solving (averaging around 44x and 69x, respectively).To the best
of our knowledge, this is the first work that brings sweeping techniques to
SMT-based hardware verification. The implementation is open-sourced at:
https://github.com/yangziyiiii/SMT-Sweep.

</details>


### [30] [Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals](https://arxiv.org/abs/2507.02742)
*G. Buriola,D. Cantone,G. Cincotti,E. G. Omodeo,G. T. Spartà*

Main category: cs.LO

TL;DR: 丰富了未量化语言的满足性测试，扩展了Tarski初等代数的一个片段，加入了具有连续一阶导数的单实函数。


<details>
  <summary>Details</summary>
Motivation: 扩展研究领域，提升对实函数及其性质的满足性测试能力。

Method: 通过预处理将公式转换为等满足性的无量化初等代数公式，利用Tarski的决策方法检验满足性。

Result: 提出的翻译方法能够保持满足性，并通过插值C^1函数模型验证。

Conclusion: 该方法有效扩展了原有研究的适用范围，尤其是对半开区间函数的处理。

Abstract: This paper enriches preexisting satisfiability tests for unquantified
languages, which in turn augment a fragment of Tarski's elementary algebra with
unary real functions possessing a continuous first derivative.
  Two sorts of individual variables are available, one ranging over real
numbers and the other one ranging over the functions of interest. Numerical
terms are built from real variables through constructs designating the four
basic arithmetic operations and through the function-application constructs
$f(t)$ and $D[\,f\,](t)$, where $f$ stands for a function variable, $t$ for a
numerical term, and $D[\,\sqdot\,]$ designates the differentiation operator.
Comparison relators can be placed between numerical terms. An array of
predicate symbols are also available, designating various relationships between
functions, as well as function properties, that may hold over intervals of the
real line; those are: (pointwise) function comparisons, strict and nonstrict
monotonicity~/~convexity~/~concavity properties, comparisons between the
derivative of a function and a real term--here, w.r.t.\ earlier research, they
are extended to (semi)-open intervals.
  The decision method we propose consists in preprocessing the given formula
into an equisatisfiable quantifier-free formula of the elementary algebra of
real numbers, whose satisfiability can then be checked by means of Tarski's
decision method. No direct reference to functions will appear in the target
formula, each function variable having been superseded by a collection of stub
real variables; hence, in order to prove that the proposed translation is
satisfiability-preserving, we must figure out a sufficiently flexible family of
interpolating $C^1$ functions that can accommodate a model for the source
formula whenever the target formula turns out to be satisfiable.

</details>


### [31] [A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)](https://arxiv.org/abs/2507.02767)
*Tiziano Dalmonte,Marianna Girlando*

Main category: cs.LO

TL;DR: 这篇论文研究了直觉主义条件逻辑，提出了两种变体CCKbox和IntCK，并为其引入了新的计算模型和公理化方法。


<details>
  <summary>Details</summary>
Motivation: 目的是在构造性和直觉主义框架下分析条件推理，特别是处理would和might条件运算符的不可互换性问题。

Method: 通过结合Chellas' CK条件逻辑和直觉主义模态逻辑的构造性框架，提出了嵌套演算和序列演算方法。

Result: 成功定义了CCK，扩展了Weiss的逻辑CCKbox，并为其模型和公理化提供了新方法。

Conclusion: 研究为直觉主义条件逻辑提供了新的理论和计算工具，并扩展了其在多个逻辑系统中的适用性。

Abstract: Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and
Olkhovikov, aims at providing a constructive analysis of conditional reasoning.
In this framework, the would and the might conditional operators are no longer
interdefinable. The intuitionistic conditional logics considered in the
literature are defined by setting Chellas' conditional logic CK, whose
semantics is defined using selection functions, within the constructive and
intuitionistic framework introduced for intuitionistic modal logics. This
operation gives rise to a constructive and an intuitionistic variant of
(might-free-) CK, which we call CCKbox and IntCK respectively. Building on the
proof systems defined for CK and for intuitionistic modal logics, in this paper
we introduce a nested calculus for IntCK and a sequent calculus for CCKbox.
Based on the sequent calculus, we define CCK, a conservative extension of
Weiss' logic CCKbox with the might operator. We introduce a class of models and
an axiomatization for CCK, and extend these result to several extensions of
CCK.

</details>


### [32] [Subtyping in DHOL -- Extended preprint](https://arxiv.org/abs/2507.02855)
*Colin Rothgang,Florian Rabe*

Main category: cs.LO

TL;DR: DHOL通过牺牲类型系统的可判定性，显著提升了表达力，并支持强自动定理证明。本文在此基础上扩展了细化和商类型，展示了其语法、语义及HOL翻译的完整性与正确性。


<details>
  <summary>Details</summary>
Motivation: 为满足实践者对细化和商类型的需求，利用DHOL的设计优势，避免了对可判定类型系统的复杂改造。

Method: 将细化和商类型作为子类型的特例引入，通过标识映射避免了表示上的高成本变更。

Result: 成功扩展了DHOL，提供了语法、语义及翻译的完整证明。

Conclusion: DHOL的可扩展性使其能优雅地支持高级类型构造，同时保持自动定理证明的高效性。

Abstract: The recently introduced dependent typed higher-order logic (DHOL) offers an
interesting compromise between expressiveness and automation support. It
sacrifices the decidability of its type system in order to significantly extend
its expressiveness over standard HOL. Yet it retains strong automated theorem
proving support via a sound and complete translation to HOL.
  We leverage this design to extend DHOL with refinement and quotient types.
Both of these are commonly requested by practitioners but rarely provided by
automated theorem provers. This is because they inherently require undecidable
typing and thus are very difficult to retrofit to decidable type systems. But
with DHOL already doing the heavy lifting, adding them is not only possible but
elegant and simple.
  Concretely, we add refinement and quotient types as special cases of
subtyping. This turns the associated canonical inclusion resp. projection maps
into identity maps and thus avoids costly changes in representation. We present
the syntax, semantics, and translation to HOL for the extended language,
including the proofs of soundness and completeness.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [33] [PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training](https://arxiv.org/abs/2507.02122)
*Neil K. R. Sehgal,Hita Kambhamettu,Allen Chang,Andrew Zhu,Lyle Ungar,Sharath Chandra Guntuku*

Main category: cs.HC

TL;DR: PAL是一种用于缓和医疗沟通训练的对话系统，通过模拟情感丰富的患者互动和结构化反馈支持临床技能提升。


<details>
  <summary>Details</summary>
Motivation: 现有医疗培训资源有限，标准化病人难以获取，因此开发低成本的替代方案。

Method: PAL系统支持文本和语音交互，基于同感框架提供反馈，并通过混合方法研究用户参与和设计问题。

Result: 用户认为PAL有助于反思和技能提升，但也指出情感真实性和反馈适应性的局限。

Conclusion: PAL证明大语言模型可用于缓和医疗培训，并为情感敏感系统和AI辅助训练设计提供了见解。

Abstract: Effective communication in serious illness and palliative care is essential
but often under-taught due to limited access to training resources like
standardized patients. We present PAL (Palliative Assisted Learning-bot), a
conversational system that simulates emotionally nuanced patient interactions
and delivers structured feedback grounded in an existing empathy-based
framework. PAL supports text and voice modalities and is designed to scaffold
clinical skill-building through repeated, low-cost practice. Through a
mixed-methods study with 17 U.S. medical trainees and clinicians, we explore
user engagement with PAL, evaluate usability, and examine design tensions
around modalities, emotional realism, and feedback delivery. Participants found
PAL helpful for reflection and skill refinement, though some noted limitations
in emotional authenticity and the adaptability of feedback. We contribute: (1)
empirical evidence that large language models can support palliative
communication training; (2) design insights for modality-aware, emotionally
sensitive simulation tools; and (3) implications for systems that support
emotional labor, cooperative learning, and AI-augmented training in high-stakes
care settings.

</details>


### [34] [StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative](https://arxiv.org/abs/2507.02156)
*Benjamin Watson,Janet Kim,Tim McEneany,Tom Moher,Claudia Hindo,Louis Gomez,Stephen Fransen*

Main category: cs.HC

TL;DR: StorySpace项目研究新型界面技术如何促进高中教育，通过支持课堂叙事活动，实现学生反思与解读的目标。


<details>
  <summary>Details</summary>
Motivation: 探索新型界面技术在课堂叙事活动中的应用，以增强学生的反思与解读能力。

Method: 设计三个目标：触发学生反思与解读、展示学习内容的复杂性、使叙事媒体生动有趣。

Result: 通过StorySpace创建的叙事媒体能够有效支持课堂讨论和学习。

Conclusion: StorySpace通过其设计目标，成功提升了课堂叙事活动的教育价值。

Abstract: The StorySpace project studies the role new interface technologies might play
in high school education. With this approach in mind, StorySpace is
specifically designed to support and enhance classroom narrative, an already
well-established classroom activity. StorySpace strives to achieve this through
adherence to three design goals. The first is to trigger student reflection and
interpretation. The narrative medium created by StorySpace should represent the
topic of classroom discussion and learning in all its complexity. In building
their representation, the students will then be confronted with that same
complexity. The medium should also itself be exciting and compelling, making
classroom narrative interesting and fun.

</details>


### [35] [A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy](https://arxiv.org/abs/2507.02138)
*Shan Li,Guozhu Ding*

Main category: cs.HC

TL;DR: Healthy Choice是一个理论驱动的AI增强模拟平台，用于通过互动场景学习提升营养素养。114名大学生的正向反馈证实了其高用户满意度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过创新的模拟学习平台提升营养素养。

Method: 采用互动场景学习，收集了114名大学生的反馈。

Result: 定量评分显示用户对该平台的有用性和易用性满意度高。

Conclusion: Healthy Choice是一个有效的营养教育工具。

Abstract: This study introduces and evaluates Healthy Choice, an innovative
theory-driven and AI-enhanced simulation platform designed to cultivate
nutrition literacy through interactive scenario-based learning experiences. We
collected feedback from 114 university students with diverse backgrounds who
completed simulated product selection scenarios. Quantitative ratings of
usefulness and ease of use demonstrated high user satisfaction.

</details>


### [36] [The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future](https://arxiv.org/abs/2507.02180)
*Russell Beale*

Main category: cs.HC

TL;DR: 该论文回顾了大语言模型在教育和教育技术中的应用，讨论了其成功与失败案例，并探讨了其对学习者和教育者的影响，以及未来教育技术的设计考量。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型（LLMs）在教育领域的应用及其对教育技术的革命性影响，以帮助设计更有效的教育系统。

Method: 通过回顾LLMs的应用领域和使用案例，分析其成功与失败，探讨其对学习者和教育者动态的改变。

Result: LLMs带来了新的交互范式，并可能成为未来默认的计算机交互方式，需要新的教育技术设计以满足用户期望。

Conclusion: 未来教育技术设计需考虑LLMs的普及和用户期望的变化，以确保其被广泛接受和应用。

Abstract: Large language Models have only been widely available since 2022 and yet in
less than three years have had a significant impact on approaches to education
and educational technology. Here we review the domains in which they have been
used, and discuss a variety of use cases, their successes and failures. We then
progress to discussing how this is changing the dynamic for learners and
educators, consider the main design challenges facing LLMs if they are to
become truly helpful and effective as educational systems, and reflect on the
learning paradigms they support. We make clear that the new interaction
paradigms they bring are significant and argue that this approach will become
so ubiquitous it will become the default way in which we interact with
technologies, and revolutionise what people expect from computer systems in
general. This leads us to present some specific and significant considerations
for the design of educational technology in the future that are likely to be
needed to ensure acceptance by the changing expectations of learners and users.

</details>


### [37] [A wireless, inexpensive optical tracker for the CAVE](https://arxiv.org/abs/2507.02682)
*Ehud Sharlin,Pablo Figueroa,Mark Green,Benjamin Watson*

Main category: cs.HC

TL;DR: 本文提出了一种低成本、无线的脚部跟踪器，用于解决CAVE显示系统中传统跟踪子系统的束缚问题。尽管其在精确视觉检查应用中表现一般，但在校园漫游等场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统CAVE显示系统中的跟踪子系统通常需要绑定用户，限制了其自由移动的优势。本文旨在通过设计一种低成本、无线的脚部跟踪器，解决这一问题。

Method: 设计并实现了一种低成本的无线脚部跟踪器，成本低于200美元，精度为10厘米，采样率为20 Hz。

Result: 在视觉检查和校园漫游两种应用中测试，结果显示跟踪器在精确检查方面表现一般，但在自由移动的校园漫游场景中效果显著。

Conclusion: 虽然该跟踪器在精确视觉检查方面有局限，但其提供的自由移动能力在特定应用（如校园漫游）中具有显著优势。

Abstract: CAVE displays offer many advantages over other virtual reality (VR) displays,
including a large, unencumbering viewing space. Unfortunately, the typical
tracking subsystems used with CAVE displays tether the user and lessen this
advantage. We have designed a simple, low-cost feet tracker that is wireless,
leaving the user free to move. The tracker can be assembled for less than $200
US, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested
the prototype with two applications: a visualization supporting close visual
inspection, and a walkthrough of the campus. Although the tracking was
convincing, it was clear that the tracker's limitations make it less than ideal
for applications requiring precise visual inspection. However, the freedom of
motion allowed by the tracker was a compelling supplement to our campus
walkthrough, allowing users to stroll and look around corners.

</details>


### [38] [EvalAssist: A Human-Centered Tool for LLM-as-a-Judge](https://arxiv.org/abs/2507.02186)
*Zahra Ashktorab,Elizabeth M. Daly,Erik Miehling,Werner Geyer,Martin Santillan Cooper,Tejaswini Pedapati,Michael Desmond,Qian Pan,Hyo Jin Do*

Main category: cs.HC

TL;DR: EvalAssist简化了使用LLM作为评估者的流程，提供在线标准开发环境和预训练的评估管道，降低评估成本和时间。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的应用增多，评估其输出的过程耗时且昂贵，需要更高效的解决方案。

Method: 开发了EvalAssist框架，包括在线标准开发环境、LLM评估管道和专门训练的风险检测评估器。

Result: 系统已在组织内部部署，服务数百名用户。

Conclusion: EvalAssist显著提升了LLM评估的效率和实用性。

Abstract: With the broad availability of large language models and their ability to
generate vast outputs using varied prompts and configurations, determining the
best output for a given task requires an intensive evaluation process, one
where machine learning practitioners must decide how to assess the outputs and
then carefully carry out the evaluation. This process is both time-consuming
and costly. As practitioners work with an increasing number of models, they
must now evaluate outputs to determine which model and prompt performs best for
a given task. LLMs are increasingly used as evaluators to filter training data,
evaluate model performance, assess harms and risks, or assist human evaluators
with detailed assessments. We present EvalAssist, a framework that simplifies
the LLM-as-a-judge workflow. The system provides an online criteria development
environment, where users can interactively build, test, and share custom
evaluation criteria in a structured and portable format. We support a set of
LLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a
prompt-chaining approach we developed and contributed to the UNITXT open-source
library. Additionally, our system also includes specially trained evaluators to
detect harms and risks in LLM outputs. We have deployed the system internally
in our organization with several hundreds of users.

</details>


### [39] [VergeIO: Depth-Aware Eye Interaction on Glasses](https://arxiv.org/abs/2507.02187)
*Xiyuxing Zhang,Duc Vu,Chengyi Shen,Yuntao Wang,Yuanchun Shi,Justin Chan*

Main category: cs.HC

TL;DR: VergeIO是一种基于EOG的智能眼镜，通过优化的电极布局和新型原型，实现了深度感知的眼部交互，准确率达83-98%，无需校准即可推广到新用户。


<details>
  <summary>Details</summary>
Motivation: 工业界对在眼镜上实现无干扰的EOG感应眼部手势的兴趣日益增长，如JINS MEME和Apple眼镜。

Method: 采用优化的电极布局和新型智能眼镜原型，结合个性化模型和运动伪影检测流程，以及基于前导码的激活方案。

Result: 在11名用户的1,320次手势实例中，准确率为83-98%；无需校准的新用户准确率为80-98%。系统实时运行，功耗3 mW。

Conclusion: VergeIO展示了在眼镜上实现高精度、低功耗、无需校准的EOG感应眼部手势交互的可行性。

Abstract: There is growing industry interest in creating unobtrusive designs for
electrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and
Apple eyewear). We present VergeIO, the first EOG-based glasses that enables
depth-aware eye interaction using vergence with an optimized electrode layout
and novel smart glass prototype. It can distinguish between four and six
depth-based eye gestures with 83-98% accuracy using personalized models in a
user study across 11 users and 1,320 gesture instances. It generalizes to
unseen users with an accuracy of 80-98% without any calibration. To reduce
false detections, we incorporate a motion artifact detection pipeline and a
preamble-based activation scheme. The system uses dry sensors without any
adhesives or gel, and operates in real time with 3 mW power consumption by the
sensing front-end, making it suitable for always-on sensing.

</details>


### [40] [An Exploration of Internal States in Collaborative Problem Solving](https://arxiv.org/abs/2507.02229)
*Sifatul Anindho,Videep Venkatesha,Mariah Bradford,Anne M. Cleary,Nathaniel Blanchard*

Main category: cs.HC

TL;DR: 该研究采用混合方法探讨了协作问题解决中个体的情绪状态，通过视频回顾和自我报告分析了语言模式，揭示了情绪特征的显著模式。


<details>
  <summary>Details</summary>
Motivation: 协作问题解决在教育和职业环境中日益重要，但个体在此过程中的情绪状态研究较少，需要深入探讨。

Method: 团队完成协作任务后，个体在隔离房间回顾视频并自我报告内部体验，随后进行语言分析。

Result: 分析发现了语言使用的独特模式，包括特定词汇、短语、情绪标签及情绪相关词的语义相似性。

Conclusion: 研究为协作问题解决中情绪动态提供了新见解，有助于优化团队合作中的情绪管理。

Abstract: Collaborative problem solving (CPS) is a complex cognitive, social, and
emotional process that is increasingly prevalent in educational and
professional settings. This study investigates the emotional states of
individuals during CPS using a mixed-methods approach. Teams of four first
completed a novel CPS task. Immediately after, each individual was placed in an
isolated room where they reviewed the video of their group performing the task
and self-reported their internal experiences throughout the task. We performed
a linguistic analysis of these internal monologues, providing insights into the
range of emotions individuals experience during CPS. Our analysis showed
distinct patterns in language use, including characteristic unigrams and
bigrams, key words and phrases, emotion labels, and semantic similarity between
emotion-related words.

</details>


### [41] [A framework for 3D interaction techniques](https://arxiv.org/abs/2507.02254)
*Pablo Figueroa,Mark Green,Benjamin Watson*

Main category: cs.HC

TL;DR: 提出了一个用于3D交互技术的软件架构和面向对象、与工具包无关的框架，支持通过数据流连接基本过滤器，并允许轻松扩展。


<details>
  <summary>Details</summary>
Motivation: 设计一个灵活且可扩展的框架，以支持多样化的3D交互技术及其集成。

Method: 基于数据流架构，通过基本过滤器和虚拟输入设备构建交互技术，并定义信息流模型。

Result: 成功实现一个可扩展的框架，支持新输入设备、交互技术和信息类型的无缝集成。

Conclusion: 该架构和框架为3D交互技术的开发和集成提供了高效且灵活的基础。

Abstract: This paper presents a software architecture for 3D interaction techniques
(ITs) and an object oriented, toolkit-independent framework that implements
such architecture. ITs are composed of basic filters connected in a dataflow,
where virtual input devices and objects in the scene are sources of
information. An execution model defines the general flow of information between
filters. This framework has been designed to be extensible: new information
types, new input devices, new execution models, or new interaction techniques
can easily be added. Application specific code and application specific ITs are
seamlessly integrated into this architecture.

</details>


### [42] [Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness](https://arxiv.org/abs/2507.02283)
*Tim Rogers,Ben Teehankee*

Main category: cs.HC

TL;DR: 论文探讨了大型语言模型（LLMs）可能继承并放大人理论与实践间的错位，通过HR顾问案例展示了LLMs如何强化无效的问题解决方式，阻碍组织学习，并提出了开发促进Model 2学习的LLMs的可能性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示LLMs可能继承和放大人类的认知盲点，尤其是在组织决策中可能加剧反学习动态的风险，以及这对AI对齐问题的启示。

Method: 采用了行动科学研究方法，通过HR顾问的详细案例研究，展示LLMs如何重现Model 1理论实践。

Result: 研究发现LLMs的表面专业建议实际上强化了无效果的问题解决方式，阻碍了深层次的组织学习。

Conclusion: 结论指出，开发支持Model 2学习的LLMs可能同时促进AI对齐研究和行动科学实践，且AI对齐过程可能反过来帮助人类更好地体现自身价值。

Abstract: This paper examines a critical yet unexplored dimension of the AI alignment
problem: the potential for Large Language Models (LLMs) to inherit and amplify
existing misalignments between human espoused theories and theories-in-use.
Drawing on action science research, we argue that LLMs trained on
human-generated text likely absorb and reproduce Model 1 theories-in-use - a
defensive reasoning pattern that both inhibits learning and creates ongoing
anti-learning dynamics at the dyad, group, and organisational levels. Through a
detailed case study of an LLM acting as an HR consultant, we show how its
advice, while superficially professional, systematically reinforces
unproductive problem-solving approaches and blocks pathways to deeper
organisational learning. This represents a specific instance of the alignment
problem where the AI system successfully mirrors human behaviour but inherits
our cognitive blind spots. This poses particular risks if LLMs are integrated
into organisational decision-making processes, potentially entrenching
anti-learning practices while lending authority to them. The paper concludes by
exploring the possibility of developing LLMs capable of facilitating Model 2
learning - a more productive theory-in-use - and suggests this effort could
advance both AI alignment research and action science practice. This analysis
reveals an unexpected symmetry in the alignment challenge: the process of
developing AI systems properly aligned with human values could yield tools that
help humans themselves better embody those same values.

</details>


### [43] [Human-Centered Explainability in Interactive Information Systems: A Survey](https://arxiv.org/abs/2507.02300)
*Yuhao Zhang,Jiaxin An,Ben Wang,Yan Zhang,Jiqun Liu*

Main category: cs.HC

TL;DR: 这篇综述文章系统性地回顾了交互式信息系统中可解释性的研究进展，归纳了可解释性的五个维度、解释设计的分类方案，以及六个面向用户的测量维度，为未来研究提供了建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动系统的普及，用户需要理解、解释和审查其输出以做出明智决策。因此，本文旨在通过文献综述，梳理可解释性在交互式信息系统中的概念化、设计和评估进展，为透明、可信和负责任系统的设计提供理论支持。

Method: 采用PRISMA指南，搜索了八个学术数据库，筛选出100篇相关文献，并利用结构化编码方法提取和综合其中的见解。

Result: 研究归纳了可解释性的五个维度、解释设计的分类方案，以及六种用户为中心的测量维度。

Conclusion: 文章总结了当前挑战，并提出了未来研究方向，强调了可解释性在优化交互信息系统设计中的作用，以满足多样化的用户需求。

Abstract: Human-centered explainability has become a critical foundation for the
responsible development of interactive information systems, where users must be
able to understand, interpret, and scrutinize AI-driven outputs to make
informed decisions. This systematic survey of literature aims to characterize
recent progress in user studies on explainability in interactive information
systems by reviewing how explainability has been conceptualized, designed, and
evaluated in practice. Following PRISMA guidelines, eight academic databases
were searched, and 100 relevant articles were identified. A structural encoding
approach was then utilized to extract and synthesize insights from these
articles. The main contributions include 1) five dimensions that researchers
have used to conceptualize explainability; 2) a classification scheme of
explanation designs; 3) a categorization of explainability measurements into
six user-centered dimensions. The review concludes by reflecting on ongoing
challenges and providing recommendations for future exploration of related
issues. The findings shed light on the theoretical foundations of
human-centered explainability, informing the design of interactive information
systems that better align with diverse user needs and promoting the development
of systems that are transparent, trustworthy, and accountable.

</details>


### [44] [Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation](https://arxiv.org/abs/2507.02306)
*Ruican Zhong,David W. McDonald,Gary Hsieh*

Main category: cs.HC

TL;DR: 研究提出了一种基于多模态LLMs的合成启发式评估方法，用于降低可用性评估成本，并发现其在检测布局问题上优于人类评估者。


<details>
  <summary>Details</summary>
Motivation: 可用性评估在以人为本的设计中至关重要，但成本高昂，需要专家时间和用户补偿。

Method: 开发了一种利用多模态LLMs分析图像并提供设计反馈的合成启发式评估方法。

Result: 合成评估发现了73%和77%的可用性问题，优于人类评估者的57%和63%，尤其在布局问题上表现突出。

Conclusion: 研究揭示了人类与LLM驱动的评估之间的性能差异，为合成启发式评估的设计提供了参考。

Abstract: Usability evaluation is crucial in human-centered design but can be costly,
requiring expert time and user compensation. In this work, we developed a
method for synthetic heuristic evaluation using multimodal LLMs' ability to
analyze images and provide design feedback. Comparing our synthetic evaluations
to those by experienced UX practitioners across two apps, we found our
evaluation identified 73% and 77% of usability issues, which exceeded the
performance of 5 experienced human evaluators (57% and 63%). Compared to human
evaluators, the synthetic evaluation's performance maintained consistent
performance across tasks and excelled in detecting layout issues, highlighting
potential attentional and perceptual strengths of synthetic evaluation.
However, synthetic evaluation struggled with recognizing some UI components and
design conventions, as well as identifying across screen violations.
Additionally, testing synthetic evaluations over time and accounts revealed
stable performance. Overall, our work highlights the performance differences
between human and LLM-driven evaluations, informing the design of synthetic
heuristic evaluations.

</details>


### [45] [From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance](https://arxiv.org/abs/2507.02350)
*Hao Tang,Songyun Xie,Xinzhou Xie,Can Liao,Xin Zhang,Bohan Li,Zhongyu Tian,Dalu Zheng*

Main category: cs.HC

TL;DR: 论文提出一种基于即时回忆的细粒度标注方法，解决了传统视频诱发情绪生理数据集中全试次标注的标签噪声问题，并通过生理证据和分类实验验证其精确性和有效性。


<details>
  <summary>Details</summary>
Motivation: 传统视频诱发情绪数据集采用全试次标注，导致标签噪声，限制了情绪识别算法的性能。研究旨在通过细粒度标注解决这一问题。

Method: 采用即时回忆范式，让参与者在视频回放时精确标注情绪起始时间、标签和强度，并通过生理信号和分类实验验证。

Result: 生理验证显示标注窗口内的多模态信号与主观标注一致；分类实验中，细粒度标注模型准确率比传统方法高9.7%。

Conclusion: 细粒度标注能有效减少标签噪声，且标注精确性比数据规模更能决定情绪识别性能。

Abstract: Traditional video-induced emotion physiological datasets often use
whole-trial annotation, assigning a single emotion label to all data collected
during an entire trial. This coarse-grained annotation approach misaligns with
the dynamic and temporally localized nature of emotional responses as they
unfold with video narratives, introducing label noise that limits emotion
recognition algorithm evaluation and performance. To solve the label noise
problem caused by coarse-grained annotation, we propose a fine-grained
annotation method through an immediate recall paradigm. This paradigm
integrates an immediate video replay phase after the initial stimulus viewing,
allowing participants to precisely mark the onset timestamp, emotion label, and
intensity based on their immediate recall. We validate this paradigm through
physiological evidence and recognition performance. Physiological validation of
multimodal signals within participant-marked windows revealed rhythm-specific
EEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of
high-arousal versus 6% of low-arousal emotion windows. These objective
physiological data changes strongly aligned with subjective annotations,
confirming annotation precision. For recognition performance, classification
experiments showed that models trained on fine-grained annotations achieved
9.7% higher accuracy than traditional whole-trial labeling, despite using less
data. This work not only addresses label noise through fine-grained annotation
but also demonstrates that annotation precision outweighs data scale in
determining emotion recognition performance.

</details>


### [46] [Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset](https://arxiv.org/abs/2507.02432)
*Jueun Lee,Dennis Moschina,Supraja Ramesh,Tobias Röddiger,Kai Kunze,Michael Beigl*

Main category: cs.HC

TL;DR: 研究探讨了音乐化振动反馈对放松和助眠的效果，结果显示短期刺激能提升副交感神经活动，但对睡眠指标无显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索非侵入式的振动反馈作为被动生物反馈干预，帮助放松和促进睡眠。

Method: 通过智能手表传递节奏化振动，调整频率略低于用户实时心率，进行两项研究（分别20人和28人）评估效果。

Result: 短期刺激增加副交感神经活动和感知放松，但睡眠指标未见显著变化。

Conclusion: 研究为可穿戴触觉反馈在放松和助眠中的应用提供设计和方法学参考。

Abstract: We investigate the use of musically structured, closed-loop vibration
patterns as a passive biofeedback intervention for relaxation and sleep
initiation. By encoding rhythmic meter structures into smartwatch vibrations
and adapting their frequency to be slightly slower than the user's real-time
heart rate, our system aims to reduce arousal through tactile entrainment,
offering a non-invasive alternative to auditory or open-loop approaches
previously used in sleep and anxiety contexts. In the first study (N=20), we
compared five adaptive vibration rhythms for their effects on heart rate and
subjective perceptions of relaxation in a resting context. In the second study
(N=28), we evaluated the most promising pattern from Study 1 in a prolonged
sleep initiation setting. Results showed increased parasympathetic activity and
perceived relaxation during short-term stimulation, but no significant effects
on sleep-related measures during the sleep onset phase. This work contributes
to the understanding of how wearable haptic feedback can support relaxation and
sleep, offering design insights and identifying methodological considerations
for effectively integrating haptic interaction into self-directed
interventions.

</details>


### [47] [Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?](https://arxiv.org/abs/2507.02453)
*Jueun Lee,Martin Flipe,Philipp Lepold,Tobias Röddiger,Michael Beigl*

Main category: cs.HC

TL;DR: 研究探讨了基于实时心率调整的触觉生物反馈在不同身体部位（手腕、手掌、前臂和肩膀）对放松的影响，发现前臂和肩膀是最理想的放松反馈位置。


<details>
  <summary>Details</summary>
Motivation: 当前可穿戴触觉干预技术主要关注固定振动模式，缺乏对身体位置和动态生物反馈的考虑，研究旨在填补这一空白。

Method: 在闭眼清醒休息状态下，比较四种可穿戴身体部位（手腕、手掌、前臂和肩膀）的触觉生物反馈效果，测量心率、耳部α波活动、主观休息感和振动体验。

Result: 生物反馈降低了手腕、肩膀和前臂的心率，前臂和肩膀的主观休息感最高且最受偏好。前臂比手腕更舒适且放松效果更佳。

Conclusion: 前臂和肩膀最适合用于不显眼的放松反馈，手腕可能需要设计改进以提升主观体验。

Abstract: Wearable haptic interventions offer promising support for relaxation through
slow, vibrotactile biofeedback. Despite their potential, current applications
focus on stress-inducing procedures and fixed vibration patterns, with limited
consideration of body location and dynamic biofeedback during restful states.
This study investigates the effects of haptic biofeedback adjusted from
real-time heart rate during eyes-closed wakeful rest, comparing four wearable
body placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave
activity on the ear, subjective restfulness, and vibration experience were
measured across these conditions. Results show that biofeedback reduced heart
rate at the wrist, shoulder, and forearm, while alpha power measured at the ear
remained unchanged. Subjective restfulness was rated highest at the shoulder
and forearm, which were also the most preferred locations. In addition,
participants reported greater comfort, relaxation, and further increased
sleepiness at the forearm compared to the wrist, which was more easily
recognizable. These findings suggest that the forearm and shoulder are ideal
for unobtrusive relaxation feedback for wakeful rest, while the wrist may
require design improvements for subjective experience.

</details>


### [48] [Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue](https://arxiv.org/abs/2507.02537)
*Paulo Ricardo Knob,Leonardo Scholler,Juliano Rigatti,Soraia Raupp Musse*

Main category: cs.HC

TL;DR: 研究探索大型语言模型（LLMs）在生成情感丰富对话时的表现，通过ChatGPT和Gemini扩展情感对话数据集，结合情感分析和人工评估发现情感建模需结构对齐与情感深度。


<details>
  <summary>Details</summary>
Motivation: 随着对话代理在日常互动中日益普及，情感智能（尤其是共情倾听）的需求变得至关重要。

Method: 从小型专家手工数据集出发，使用ChatGPT和Gemini扩展对话，通过VADER情感分析和专家评估分析情感进展。

Result: 生成对话虽符合情感结构，但人工评估显示共情和连贯性存在差异。

Conclusion: 情感对话建模需结构对齐与情感深度，强调结合自动与人工方法开发情感智能代理的重要性。

Abstract: Conversational agents have made significant progress since ELIZA, expanding
their role across various domains, including healthcare, education, and
customer service. As these agents become increasingly integrated into daily
human interactions, the need for emotional intelligence, particularly
empathetic listening, becomes increasingly essential. In this study, we explore
how Large Language Models (LLMs) respond when tasked with generating
emotionally rich interactions. Starting from a small dataset manually crafted
by an expert to reflect empathic behavior, we extended the conversations using
two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the
dialogues using both sentiment analysis (via VADER) and expert assessments.
While the generated conversations often mirrored the intended emotional
structure, human evaluation revealed important differences in the perceived
empathy and coherence of the responses. These findings suggest that emotion
modeling in dialogues requires not only structural alignment in the expressed
emotions but also qualitative depth, highlighting the importance of combining
automated and humancentered methods in the development of emotionally competent
agents.

</details>


### [49] [Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots](https://arxiv.org/abs/2507.02745)
*Zahra Ashktorab,Alessandra Buccella,Jason D'Cruz,Zoe Fowler,Andrew Gill,Kei Yan Leung,P. D. Magnus,John Richards,Kush R. Varshney*

Main category: cs.HC

TL;DR: 研究探讨了LLM驱动聊天机器人在犯错时如何通过道歉修复用户信任，发现解释性道歉更受青睐，但不同情境和用户偏好各异。


<details>
  <summary>Details</summary>
Motivation: 随着LLM聊天机器人在日常应用中的普及，其通过有效道歉从错误中恢复的能力对维护用户信任和满意度至关重要。

Method: 通过预注册的成对实验，参与者评估了聊天机器人在三种常见错误（偏见、虚构、事实错误）下发布的三种道歉（机械式、解释性、共情式）。

Result: 解释性道歉更受欢迎，但在偏见情境中，共情式道歉因承认情感影响而更受青睐；虚构错误未引发明确偏好，反映用户不确定性。

Conclusion: 研究表明AI系统中有效道歉的复杂性，未来系统需通过个性化和校准来修复用户信任。

Abstract: As chatbots driven by large language models (LLMs) are increasingly deployed
in everyday contexts, their ability to recover from errors through effective
apologies is critical to maintaining user trust and satisfaction. In a
preregistered study with Prolific workers (N=162), we examine user preferences
for three types of apologies (rote, explanatory, and empathic) issued in
response to three categories of common LLM mistakes (bias, unfounded
fabrication, and factual errors). We designed a pairwise experiment in which
participants evaluated chatbot responses consisting of an initial error, a
subsequent apology, and a resolution. Explanatory apologies were generally
preferred, but this varied by context and user. In the bias scenario, empathic
apologies were favored for acknowledging emotional impact, while
hallucinations, though seen as serious, elicited no clear preference,
reflecting user uncertainty. Our findings show the complexity of effective
apology in AI systems. We discuss key insights such as personalization and
calibration that future systems must navigate to meaningfully repair trust.

</details>


### [50] [Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding](https://arxiv.org/abs/2507.02800)
*Ebrahim Feghhi,Shreyas Kaasyap,Nima Hadidi,Jonathan C. Kao*

Main category: cs.HC

TL;DR: 论文提出三种改进方法，实现高效、实时的神经语音解码，显著降低计算成本和错误率。


<details>
  <summary>Details</summary>
Motivation: 加速神经语音解码算法的进步，解决现有方法计算成本高、实时性不足的问题。

Method: 引入大量时间掩码训练、用紧凑Transformer替代GRU、设计轻量级测试时适应方法。

Result: 将单词错误率降低19.5%，减少计算成本，并在实时解码中有效保持性能。

Conclusion: 提出的方法显著优化了神经语音解码的准确性、效率和实时性。

Abstract: Speech neuroprostheses aim to restore communication for people with severe
paralysis by decoding speech directly from neural activity. To accelerate
algorithmic progress, a recent benchmark released intracranial recordings from
a paralyzed participant attempting to speak, along with a baseline decoding
algorithm. Prior work on the benchmark showed impressive accuracy gains.
However, these gains increased computational costs and were not demonstrated in
a real-time decoding setting. Here, we make three contributions that pave the
way towards accurate, efficient, and real-time neural speech decoding. First,
we incorporate large amounts of time masking during training. On average, over
$50\%$ of each trial is masked. Second, we replace the gated recurrent unit
(GRU) architecture used in the baseline algorithm with a compact Transformer.
The Transformer architecture uses $77\%$ fewer parameters, cuts peak GPU memory
usage by $36\%$ relative, and is significantly faster to calibrate relative to
the GRU. Third, we design a lightweight variant of an existing test-time
adaptation method developed for decoding handwriting from neural activity. Our
variant adapts the model using multiple time masked augmentations of a single
trial and requires only one gradient step per trial. Together, these
contributions reduce word error rate by $19.5\%$ and effectively mitigate
performance degradations across held-out days in a real-time decoding setting
while substantially lowering computational costs.

</details>


### [51] [Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks](https://arxiv.org/abs/2507.02819)
*Luke Guerdan,Devansh Saxena,Stevie Chancellor,Zhiwei Steven Wu,Kenneth Holstein*

Main category: cs.HC

TL;DR: 数据科学家通过拼装过程构建目标变量，考虑有效性、简单性等五个标准，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究数据科学家如何将模糊概念转化为具体目标变量，填补这一过程的研究空白。

Method: 访谈15位来自教育和医疗领域的数据科学家，分析目标变量构建过程。

Result: 发现数据科学家通过拼装满足五个标准，并灵活调整策略以适应需求。

Conclusion: 未来研究可改进目标变量构建的支持工具和方法。

Abstract: Data scientists often formulate predictive modeling tasks involving fuzzy,
hard-to-define concepts, such as the "authenticity" of student writing or the
"healthcare need" of a patient. Yet the process by which data scientists
translate fuzzy concepts into a concrete, proxy target variable remains poorly
understood. We interview fifteen data scientists in education (N=8) and
healthcare (N=7) to understand how they construct target variables for
predictive modeling tasks. Our findings suggest that data scientists construct
target variables through a bricolage process, involving iterative negotiation
between high-level measurement objectives and low-level practical constraints.
Data scientists attempt to satisfy five major criteria for a target variable
through bricolage: validity, simplicity, predictability, portability, and
resource requirements. To achieve this, data scientists adaptively use problem
(re)formulation strategies, such as swapping out one candidate target variable
for another when the first fails to meet certain criteria (e.g.,
predictability), or composing multiple outcomes into a single target variable
to capture a more holistic set of modeling objectives. Based on our findings,
we present opportunities for future HCI, CSCW, and ML research to better
support the art and science of target variable construction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [52] [Gbake: Baking 3D Gaussian Splats into Reflection Probes](https://arxiv.org/abs/2507.02257)
*Stephen Pasch,Joel K. Salzman,Changxi Zheng*

Main category: cs.GR

TL;DR: 论文介绍了GBake工具，用于在3D高斯环境中间接实现传统3D网格的真实反射映射，解决了现有方法中光照不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯环境中传统3D网格光照不一致的问题，提升视觉效果。

Method: 开发了GBake工具，通过烘焙反射探头从高斯场景中生成反射映射。

Result: 在Unity游戏引擎中实现了传统3D网格的逼真反射效果。

Conclusion: GBake工具成功整合了传统3D网格和高斯环境，解决了光照问题。

Abstract: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

</details>


### [53] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 提出了一种高效的图像光照近似方法，用于实时渲染闪烁或闪烁材料，支持动态材质和环境贴图。


<details>
  <summary>Details</summary>
Motivation: 解决实时渲染中闪烁材料的光照问题，特别是动态材质和环境贴图的需求。

Method: 利用环境贴图滤波技术和正态分布函数，提出双门高斯近似方法，实现对微小面片的概率采样。

Result: 方法接近真实渲染效果，性能稳定，内存需求为无闪烁材料的两倍。

Conclusion: 该高效近似方法适用于实时渲染闪烁材料，性能与单方向光渲染相当。

Abstract: Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is grounded in real-time glint rendering under area
light illumination and employs standard environment map filtering techniques.
Crucially, our environment map filtering process is sufficiently fast to be
executed on a per-frame basis. Our method assumes that the environment map is
partitioned into few homogeneous regions of constant radiance. By filtering the
corresponding indicator functions with the normal distribution function, we
obtain the probabilities for individual microfacets to reflect light from each
region. During shading, these probabilities are utilized to hierarchically
sample a multinomial distribution, facilitated by our novel dual-gated Gaussian
approximation of binomial distributions. We validate that our real-time
approximation is close to ground-truth renderings for a range of material
properties and lighting conditions, and demonstrate robust and stable
performance, with little overhead over rendering glints from a single
directional light. Compared to rendering smooth materials without glints, our
approach requires twice as much memory to store the prefiltered environment
map.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [54] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE是一个高性能计算（HPC）集群，在TOP500中排名第49，是全球前100名中唯一使用完全开放网络堆栈的系统。


<details>
  <summary>Details</summary>
Motivation: 展示开放和厂商中立技术在大规模HPC基础设施中的可行性，并为高级工作负载（如大语言模型训练）提供优化的计算资源。

Method: 采用基于800GbE和SONiC的完全开放网络堆栈，配备100个计算节点，每个节点配备8个NVIDIA H100 GPU，并使用Rail-Optimized拓扑和RoCEv2技术实现高速通信。

Result: 在HPL基准测试中实现33.95 PFLOP/s的持续性能，在HPCG中为396.295 TFLOP/s，在FP8精度的HPL-MxP中达到339.86 PFLOP/s。

Conclusion: SAKURAONE证明了开放网络技术在高性能计算中的竞争力，适用于AI和大规模并行工作负载。

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>


### [55] [Signalling Health for Improved Kubernetes Microservice Availability](https://arxiv.org/abs/2507.02158)
*Jacob Roberts,Blair Archibald,Phil Trinder*

Main category: cs.DC

TL;DR: 论文比较了基于轮询（PCM）和信号（SCM）的容器监控方法，SCM在Kubernetes中实现并显示优于PCM，检测故障更快且无需调优。


<details>
  <summary>Details</summary>
Motivation: PCM方法需要调优且可能降低服务可用性，SCM作为一种替代方案，旨在改进这些问题。

Method: 设计和实现了Kubernetes中的SCM方法，并通过数学模型和实验（使用SockShop基准）比较了SCM与PCM的性能。

Result: SCM检测故障速度比PCM快86%，且资源开销有限，PCM可能导致误检，降低服务可用性4%。

Conclusion: 建议编排器提供SCM功能，以实现更快、更准确的故障检测，而无需调优。

Abstract: Microservices are often deployed and managed by a container orchestrator that
can detect and fix failures to maintain the service availability critical in
many applications. In Poll-based Container Monitoring (PCM), the orchestrator
periodically checks container health. While a common approach, PCM requires
careful tuning, may degrade service availability, and can be slow to detect
container health changes. An alternative is Signal-based Container Monitoring
(SCM), where the container signals the orchestrator when its status changes. We
present the design, implementation, and evaluation of an SCM approach for
Kubernetes and empirically show that it has benefits over PCM, as predicted by
a new mathematical model. We compare the service availability of SCM and PCM
over six experiments using the SockShop benchmark. SCM does not require that
polling intervals are tuned, and yet detects container failure 86\% faster than
PCM and container readiness in a comparable time with limited resource
overheads. We find PCM can erroneously detect failures, and this reduces
service availability by 4\%. We propose that orchestrators offer SCM features
for faster failure detection than PCM without erroneous detections or careful
tuning.

</details>


### [56] [Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems](https://arxiv.org/abs/2507.02233)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: 提出了一种基于迁移学习的智能故障根因识别算法，通过共享特征提取和域对抗机制提升目标域性能，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 云计算环境中系统结构复杂、服务耦合度高、故障信息有限，导致故障根因识别困难。

Method: 设计了基于迁移学习的算法，包括共享特征提取模块、域对抗机制和伪标签选择策略。

Result: 在标签稀缺、类别不平衡和异构节点环境下，模型在准确性、F1-Score和AUC等指标上优于主流方法。

Conclusion: 该算法在复杂云计算系统中表现出高判别力和鲁棒性，具有实际应用价值。

Abstract: This paper addresses the challenge of fault root cause identification in
cloud computing environments. The difficulty arises from complex system
structures, dense service coupling, and limited fault information. To solve
this problem, an intelligent identification algorithm based on transfer
learning is proposed. The method introduces a shared feature extraction module
and a domain adversarial mechanism to enable effective knowledge transfer from
the source domain to the target domain. This improves the model's
discriminative ability and generalization performance in the target domain. The
model incorporates a pseudo-label selection strategy. When labeled samples are
lacking in the target domain, high-confidence predictions are used in training.
This enhances the model's ability to recognize minority classes. To evaluate
the stability and adaptability of the method in real-world scenarios,
experiments are designed under three conditions: label scarcity, class
imbalance, and heterogeneous node environments. Experimental results show that
the proposed method outperforms existing mainstream approaches in several key
metrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger
discriminative power and robustness. Notably, under extreme class imbalance and
significant structural differences in the target domain, the model still
maintains high performance. This validates the effectiveness and practical
value of the proposed mechanisms in complex cloud computing systems.

</details>


### [57] [Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources](https://arxiv.org/abs/2507.02295)
*Roopkatha Banerjee,Prince Modi,Jinal Vyas,Chunduru Sri Abhijit,Tejus Chandrashekar,Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 论文介绍了Flotilla，一个轻量级且可扩展的联邦学习框架，支持同步和异步聚合，提供模块化设计和容错能力，优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习框架在真实边缘硬件部署和异步聚合方面存在不足，且缺乏容错能力，亟需一种更灵活和可靠的解决方案。

Method: Flotilla采用模块化设计，支持多种同步和异步策略，客户端无状态，服务器设计分离会话状态，并定期检查点。

Result: Flotilla在200多个客户端上展示了良好的容错性，且资源使用优于现有框架，可扩展到1000+客户端。

Conclusion: Flotilla是一个有竞争力的框架，适合构建和比较新的联邦学习策略，并支持系统研究和优化。

Abstract: With the recent improvements in mobile and edge computing and rising concerns
of data privacy, Federated Learning(FL) has rapidly gained popularity as a
privacy-preserving, distributed machine learning methodology. Several FL
frameworks have been built for testing novel FL strategies. However, most focus
on validating the learning aspects of FL through pseudo-distributed simulation
but not for deploying on real edge hardware in a distributed manner to
meaningfully evaluate the federated aspects from a systems perspective. Current
frameworks are also inherently not designed to support asynchronous
aggregation, which is gaining popularity, and have limited resilience to client
and server failures. We introduce Flotilla, a scalable and lightweight FL
framework. It adopts a ``user-first'' modular design to help rapidly compose
various synchronous and asynchronous FL strategies while being agnostic to the
DNN architecture. It uses stateless clients and a server design that separates
out the session state, which are periodically or incrementally checkpointed. We
demonstrate the modularity of Flotilla by evaluating five different FL
strategies for training five DNN models. We also evaluate the client and
server-side fault tolerance on 200+ clients, and showcase its ability to
rapidly failover within seconds. Finally, we show that Flotilla's resource
usage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or
better than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It
also scales significantly better compared to Flower for 1000+ clients. This
positions Flotilla as a competitive candidate to build novel FL strategies on,
compare them uniformly, rapidly deploy them, and perform systems research and
optimizations.

</details>


### [58] [Alps, a versatile research infrastructure](https://arxiv.org/abs/2507.02404)
*Maxime Martinasso,Mark Klein,Thomas C. Schulthess*

Main category: cs.DC

TL;DR: CSCS开发了Alps，一种下一代HPC基础设施，通过独立资源和软件定义集群技术解决传统HPC架构缺乏灵活性的问题，支持多样化的科学需求。


<details>
  <summary>Details</summary>
Motivation: 传统HPC架构在多样性科学需求下显示出局限性，缺乏灵活性和可组合性。

Method: Alps采用独立资源端点和高性能网络，结合vCluster技术，实现了基础设施、服务管理和用户环境的分离。

Result: Alps支持异构硬件和多样化科学应用，如数值天气预报和AI研究。

Conclusion: Alps为科学计算提供了灵活且可定制的高性能解决方案。

Abstract: The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition
of delivering top-tier high-performance computing systems, exemplified by the
Piz Daint supercomputer. However, the increasing diversity of scientific needs
has exposed limitations in traditional vertically integrated HPC architectures,
which often lack flexibility and composability. To address these challenges,
CSCS developed Alps, a next-generation HPC infrastructure designed with a
transformative principle: resources operate as independent endpoints within a
high-speed network. This architecture enables the creation of independent
tenant-specific and platform-specific services, tailored to diverse scientific
requirements.
  Alps incorporates heterogeneous hardware, including CPUs and GPUs,
interconnected by a high-performance Slingshot network, and offers a modular
storage system. A key innovation is the versatile software-defined cluster
(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting
infrastructure, service management, and user environments into distinct layers,
vClusters allow for customized platforms that support diverse workloads.
Current platforms on Alps serve various scientific domains, including numerical
weather prediction, and AI research.

</details>


### [59] [FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference](https://arxiv.org/abs/2507.02620)
*Xing Liu,Lizhuo Luo,Ming Tang,Chao Huang*

Main category: cs.DC

TL;DR: FlowSpec是一种基于管道并行和推测解码的框架，用于提升边缘网络中大型语言模型的分布式推理效率，显著减少了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 分布式推理可以解决大型语言模型在边缘设备内存不足的问题，但现有方法在请求稀疏时效率低下。

Method: FlowSpec通过分步验证、高效的草稿管理和动态扩展策略优化解码效率。

Result: 实验结果表明，FlowSpec比基线方法快1.36到1.77倍。

Conclusion: FlowSpec有效提升了边缘网络中的推理效率和管道利用率。

Abstract: Distributed inference serves as a promising approach to enabling the
inference of large language models (LLMs) at the network edge. It distributes
the inference process to multiple devices to ensure that the LLMs can fit into
the device memory. Recent pipeline-based approaches have the potential to
parallelize communication and computation, which helps reduce inference
latency. However, the benefit diminishes when the inference request at the
network edge is sparse, where pipeline is typically at low utilization. To
enable efficient distributed LLM inference at the edge, we propose
\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding
framework. FlowSpec incorporates three key mechanisms to improve decoding
efficiency: 1) score-based step-wise verification prioritizes more important
draft tokens to bring earlier accpeted tokens; 2) efficient draft management to
prune invalid tokens while maintaining correct causal relationship during
verification; 3) dynamic draft expansion strategies to supply high-quality
speculative inputs. These techniques work in concert to enhance both pipeline
utilization and speculative efficiency. We evaluate FlowSpec on a real-world
testbed with other baselines. Experimental results demonstrate that our
proposed framework significantly improves inference speed across diverse models
and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared
to baselines. Our code is publicly available at
\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [Template-Based Schema Matching of Multi-Layout Tenancy Schedules:A Comparative Study of a Template-Based Hybrid Matcher and the ALITE Full Disjunction Model](https://arxiv.org/abs/2507.02020)
*Tim Uilkema,Yao Ma,Seyed Sahand Mohammadi Ziabari,Joep van Vliet*

Main category: cs.DB

TL;DR: 提出了一种基于模板的混合模式匹配器，用于标准化房地产租赁数据表，结合模式和实例指标，显著提升数据集成效率和业务可用性。


<details>
  <summary>Details</summary>
Motivation: 解决房地产行业租赁数据表格式不统一导致的集成效率低下问题，现有方法虽完整但易产生模式膨胀和稀疏属性。

Method: 提出混合模式匹配器，结合Jaccard、Levenshtein等模式指标和数据类型分布等实例指标，通过匈牙利算法全局优化匹配。

Result: 在手动标注数据集上F1分数达0.881，空值率45.7%，优于ALITE的0.712和75.6%空值率。

Conclusion: 结合业务知识和混合匹配可生成更实用的模式映射，未来可扩展至复杂表结构。

Abstract: The lack of standardized tabular formats for tenancy schedules across real
estate firms creates significant inefficiencies in data integration. Existing
automated integration methods, such as Full Disjunction (FD)-based models like
ALITE, prioritize completeness but result in schema bloat, sparse attributes
and limited business usability. We propose a novel hybrid, template-based
schema matcher that aligns multi-layout tenancy schedules to a predefined
target schema. The matcher combines schema (Jaccard, Levenshtein) and
instance-based metrics (data types, distributions) with globally optimal
assignments determined via the Hungarian Algorithm. Evaluation against a
manually labeled ground truth demonstrates substantial improvements, with grid
search optimization yielding a peak F1-score of 0.881 and an overall null
percentage of 45.7%. On a separate ground truth of 20 semantically similar
column sets, ALITE achieves an F1-score of 0.712 and 75.6% nulls. These results
suggest that combining structured business knowledge with hybrid matching can
yield more usable and business-aligned schema mappings. The approach assumes
cleanly extracted tabular input, future work could explore extending the
matcher to support complex, composite tables.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [61] [Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions](https://arxiv.org/abs/2507.02067)
*Nikolaos Papanikolaou,Doha Touhafi,Jurgen Vandendriessche,Danial Karimi,Sohail Fatimi,Gianluca Cornetta,Abdellah Touhafi*

Main category: cs.AR

TL;DR: Printed sensors are flexible, cost-effective, and highly customizable, with applications in environmental monitoring.


<details>
  <summary>Details</summary>
Motivation: To advance sensor technology by utilizing innovative printing techniques for better environmental monitoring.

Method: Utilizing printing techniques to create flexible and customizable sensing devices.

Result: High sensitivity and accuracy in detecting environmental parameters like pollutants, temperature, and humidity.

Conclusion: Printed sensors offer a transformative solution for diverse environmental monitoring needs.

Abstract: Printed sensors represent a transformative advancement in sensor technology,
utilizing innovative printing techniques to create flexible, cost-effective,
and highly customizable sensing devices. Their versatility allows integration
into numerous applications across diverse fields such as monitoring a wide
range of environmental factors e.g. air and water quality, soil conditions, and
atmospheric changes among others. These sensors demonstrate high sensitivity
and accuracy in detecting pollutants, temperature variations, humidity levels,
and other critical parameters essential for environmental assessment and
protection.

</details>


### [62] [Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting](https://arxiv.org/abs/2507.02164)
*Ruibai Tang,Chengbin Quan*

Main category: cs.AR

TL;DR: 提出了一种硬件加速算法，通过近似多项式和单位移QR迭代求解复杂函数根，设计了一种流水线FPGA架构，能效比CPU方法高65倍。


<details>
  <summary>Details</summary>
Motivation: 复杂函数根的求解和可视化在理论和应用领域都很重要，但通常计算量很大。

Method: 利用伴随矩阵的Hessenberg结构，用Givens旋转优化QR分解，设计了一种高吞吐量的FPGA架构。

Result: 实现比基于CPU的方法高65倍的能效，但性能仍落后于现代GPU。

Conclusion: 该硬件加速算法在能效上具有显著优势，适合高吞吐量需求的应用。

Abstract: Solving and visualizing the potential roots of complex functions is essential
in both theoretical and applied domains, yet often computationally intensive.
We present a hardware-accelerated algorithm for complex function roots density
graph plotting by approximating functions with polynomials and solving their
roots using single-shift QR iteration. By leveraging the Hessenberg structure
of companion matrices and optimizing QR decomposition with Givens rotations, we
design a pipelined FPGA architecture capable of processing a large amount of
polynomials with high throughput. Our implementation achieves up to 65x higher
energy efficiency than CPU-based approaches, and while it trails modern GPUs in
performance due to differences in fabrication technique.

</details>


### [63] [System-performance and cost modeling of Large Language Model training and inference](https://arxiv.org/abs/2507.02456)
*Wenzhe Guo,Joyjit Kundu,Uras Tos,Weijiang Kong,Giuliano Sisto,Timon Evenblij,Manu Perumkunnil*

Main category: cs.AR

TL;DR: 本文提出了一种针对大语言模型（LLM）训练和推理的性能-成本建模方法，整合了先进的计算技术、内存优化和通信技术，以解决分布式系统中的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模和复杂度的指数增长，计算能力、内存带宽、网络性能和成本效率的进步滞后，对其在分布式系统中的扩展性提出了重大挑战。

Method: 提出了一个性能-成本建模框架，结合了最新的计算技术（如闪存注意力技术和混合专家模型）、内存优化和网络拓扑结构，并引入了5D并行性和芯片成本模型。

Result: 该建模方法能够分析不同系统架构配置的性能-成本权衡，为未来计算系统设计和硬件-软件协同开发提供指导。

Conclusion: 该框架不仅解决了LLM扩展性的技术瓶颈，还提供了性能与成本之间的权衡分析，有助于推动高效的硬件-软件协同设计。

Abstract: Large language models (LLMs), based on transformer architectures, have
revolutionized numerous domains within artificial intelligence, science, and
engineering due to their exceptional scalability and adaptability. However, the
exponential growth in LLM size and complexity has outpaced advancements in
compute capacity, memory bandwidth, network performance, and cost efficiency,
posing significant challenges to their scalability on distributed systems. To
address these limitations, alternative model architectures, optimization
strategies, communication-aware network topologies, and novel system design
approaches have been proposed in literature. This paper introduces a
performance-cost modeling methodology for LLM training and inference that
integrates state-of-the-art compute techniques with memory optimizations, and
latest communication techniques. Building on an analytical performance model,
our approach incorporates recent innovations such as the flash attention
technique and mixture of experts models to address the memory bandwidth and
compute bottlenecks. It also considers the impact of different network
topologies and topology-specific communication algorithms with 5D parallellism.
The framework also integrates a chiplet cost model. The proposed modeling
methodology provides valuable insights to guide future compute system design
and facilitates hardware-software co-development, in particular due to its
ability to analyze performance-cost trade-offs for various system architectural
configurations.

</details>


### [64] [AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models](https://arxiv.org/abs/2507.02598)
*Chenhao Xue,Kezhi Li,Jiaxing Zhang,Yi Ren,Zhengyuan Shi,Chen Zhang,Yibo Lin,Lining Zhang,Qiang Xu,Guangyu Sun*

Main category: cs.AR

TL;DR: 该论文提出了一种基于条件扩散模型的算术电路优化框架AC-Refiner，通过将电路合成任务重新定义为条件图像生成任务，显著提升了电路设计的效果。


<details>
  <summary>Details</summary>
Motivation: 算术电路如加法器和乘法器是数字系统的核心组件，直接影响性能、能效和面积。现有深度学习方法难以高效探索高潜力设计变体，优化效率受限。

Method: 提出AC-Refiner框架，利用条件扩散模型，将电路合成任务视为条件图像生成任务，并结合目标质量结果（QoRs）指导去噪扩散过程。

Result: 实验表明，AC-Refiner生成的设计具有更高的Pareto最优性，优于现有基线方法，且在实际应用中验证了性能提升。

Conclusion: AC-Refiner通过条件扩散模型有效提升了算术电路的优化效率，为高性能电路设计提供了新思路。

Abstract: Arithmetic circuits, such as adders and multipliers, are fundamental
components of digital systems, directly impacting the performance, power
efficiency, and area footprint. However, optimizing these circuits remains
challenging due to the vast design space and complex physical constraints.
While recent deep learning-based approaches have shown promise, they struggle
to consistently explore high-potential design variants, limiting their
optimization efficiency. To address this challenge, we propose AC-Refiner, a
novel arithmetic circuit optimization framework leveraging conditional
diffusion models. Our key insight is to reframe arithmetic circuit synthesis as
a conditional image generation task. By carefully conditioning the denoising
diffusion process on target quality-of-results (QoRs), AC-Refiner consistently
produces high-quality circuit designs. Furthermore, the explored designs are
used to fine-tune the diffusion model, which focuses the exploration near the
Pareto frontier. Experimental results demonstrate that AC-Refiner generates
designs with superior Pareto optimality, outperforming state-of-the-art
baselines. The performance gain is further validated by integrating AC-Refiner
into practical applications.

</details>


### [65] [Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure](https://arxiv.org/abs/2507.02654)
*Rui Xie,Asad Ul Haq,Yunhua Fang,Linsen Ma,Sanchari Sen,Swagath Venkataramani,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 通过将HBM的可靠性管理从芯片内转移到内存控制器，结合特定领域的ECC框架，显著降低了HBM的部署成本，同时保持了高性能和模型精度。


<details>
  <summary>Details</summary>
Motivation: HBM的高带宽和能效对AI工作负载非常重要，但其高成本限制了可扩展部署。本研究旨在通过系统级方法降低HBM的成本。

Method: 采用域特定ECC框架，包括大码字Reed-Solomon校正、轻量级细粒度CRC检测、差分奇偶更新以减少写入放大，以及基于数据重要性的可调保护。

Result: 在原始HBM比特错误率高达$10^{-3}$的情况下，系统仍能保持78%的吞吐量和97%的模型精度。

Conclusion: 通过将可靠性作为可调系统参数而非固定硬件约束，为AI基础设施中的低成本高性能HBM部署开辟了新途径。

Abstract: High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy
efficiency for AI workloads, but its high cost per bit, driven in part by
stringent on-die reliability requirements, poses a growing barrier to scalable
deployment. This work explores a system-level approach to cost reduction by
eliminating on-die ECC and shifting all fault management to the memory
controller. We introduce a domain-specific ECC framework combining
large-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC
detection, differential parity updates to mitigate write amplification, and
tunable protection based on data importance. Our evaluation using LLM inference
workloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the
system retains over 78\% of throughput and 97\% of model accuracy compared with
systems equipped with ideal error-free HBM. By treating reliability as a
tunable system parameter rather than a fixed hardware constraint, our design
opens a new path toward low-cost, high-performance HBM deployment in AI
infrastructure.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [66] [SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection](https://arxiv.org/abs/2507.02635)
*Mao Luo,Zhi Wang,Yiwen Huang,Qingyun Zhang,Zhouxing Su,Zhipeng Lv,Wen Hu,Jianguo Li*

Main category: cs.CR

TL;DR: 电子支付平台每天处理数十亿交易，但其验证规则存在漏洞，需系统性方法来确保其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏系统性方法确保验证规则的鲁棒性，现有的手工规则易受恶意请求利用，可能导致严重后果。

Method: 依赖领域专家手工构建的验证规则，但缺乏系统性检验。

Result: 规则不完善导致恶意请求可能绕过验证，亟需系统性方法识别缺陷。

Conclusion: 需要开发系统性方法确保验证规则的鲁棒性，以应对潜在的金融风险。

Abstract: Electronic payment platforms are estimated to process billions oftransactions
daily, with the cumulative value of these transactionspotentially reaching into
the trillions. Even a minor error within thishigh-volume environment could
precipitate substantial financiallosses. To mitigate this risk, manually
constructed verification rules,developed by domain experts, are typically
employed to identifyand scrutinize transactions in production environments.
However,due to the absence of a systematic approach to ensure the robust-ness
of these verification rules against vulnerabilities, they remainsusceptible to
exploitation.To mitigate this risk, manually constructed verification rules,
de-veloped by domain experts, are typically employed to identify andscrutinize
transactions in production environments. However, dueto the absence of a
systematic approach to ensure the robustness ofthese verification rules against
vulnerabilities, they remain suscep-tible to exploitation. To ensure data
security, database maintainersusually compose complex verification rules to
check whether aquery/update request is valid. However, the rules written by
ex-perts are usually imperfect, and malicious requests may bypassthese rules.
As a result, the demand for identifying the defects ofthe rules systematically
emerges.

</details>


### [67] [Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain](https://arxiv.org/abs/2507.02536)
*Azmat Ullah,Maria Ilaria Lunesu,Lodovica Marchesi,Roberto Tonelli*

Main category: cs.CR

TL;DR: 本文提出了一种基于区块链的物联网系统，用于监测餐厅披萨生产，通过实时数据追踪和区块链技术提高透明度和效率。


<details>
  <summary>Details</summary>
Motivation: 提升披萨生产过程中的数据安全、透明度和操作效率，减少浪费并优化管理。

Method: 利用物联网设备实时监测温湿度，区块链确保数据安全，树莓派处理数据并触发警报，智能合约实现自动化交互。

Result: 实验显示系统能有效管理原料、减少浪费并提高厨房效率。

Conclusion: 区块链与物联网的结合为餐饮生产提供了透明、高效且安全的解决方案。

Abstract: This paper presents a blockchain-based Internet of Things (IoT) system for
monitoring pizza production in restaurants. IoT devices track temperature and
humidity in real-time, while blockchain ensures secure and tamper-proof data. A
Raspberry Pi processes sensor data, captures images, triggers alerts, and
interacts with smart contracts. The system detects abnormal conditions,
enabling quick responses. Blockchain adds transparency and traceability,
supporting compliance and audits. Experiments show improved ingredient
management, reduced waste, and increased kitchen efficiency.

</details>


### [68] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: 本文提出了一种上下文感知攻击数据生成器，用于生成攻击场景的高质量数据，以解决实际车辆中攻击数据稀缺的问题，并通过实验验证了生成数据的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于安全和伦理限制，无法在实际车辆上生成多样化的攻击场景数据，因此需要一种高效的替代方法来生成高质量的攻击数据。

Method: 使用参数化攻击模型，结合CAN消息解码和攻击强度调整，生成多种攻击类型的数据，并在入侵检测系统（IDS）中验证其有效性。

Result: 生成的攻击数据在IDS模型中表现出高检测和分类能力，证明了其与实际场景的高度一致性。

Conclusion: 该方法能够高效、可扩展地生成高质量攻击数据，为车辆网络安全研究提供了实用的数据支持。

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 论文扩展了可微布尔逻辑网络（DBNs），提出了一种可训练的、参数数量恒定的可微互连结构，并引入了两种互补的剪枝阶段以减少模型大小。


<details>
  <summary>Details</summary>
Motivation: 为了解决DBNs在输入宽度增加时无法高效扩展的问题，同时保持其高精度优势。

Method: 提出了可微互连结构，并设计了两阶段剪枝方法：基于SAT的逻辑等价剪枝和基于相似性的数据驱动剪枝。

Result: 扩展后的DBNs能够处理更大规模的输入，同时剪枝方法在保持性能的前提下显著减少了模型大小。

Conclusion: 所提出的方法在扩展性和压缩效率上优于之前的设计，提供了更好的规模与精度权衡。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [70] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 总结Transformer模型在EEG解码中的最新应用及其架构演变，探讨混合模型和定制化Transformer的进展，并展望未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习，特别是Transformer模型，如何革新EEG解码领域，提供更高效的特征提取和处理方法。

Method: 综述Transformer在EEG解码中的应用，包括其基础结构、混合模型（如结合CNN、RNN等）以及定制化改进。

Result: 总结了Transformer在EEG解码中的优势和应用进展，并分析了现有挑战。

Conclusion: Transformer为EEG解码带来显著进步，但仍需解决数据稀缺和计算效率等问题，未来研究方向包括模型优化和多模态融合。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [71] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 该论文提出了一种通过优化预处理和深度学习技术显著提升跨被试运动想象分类性能的新方法，包括STFT参数优化和平衡批处理策略，并在多个数据集中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 跨被试运动想象分类因脑电图模式的个体差异而表现不佳，限制了无校准脑机接口的实际应用。

Method: 使用优化的STFT变换EEG数据和平衡批处理策略训练CNN，直接分类STFT数据。

Result: 在三个基准数据集上分别达到67.60%、65.96%和80.22%的分类准确率，优于现有方法。

Conclusion: 该方法为通用无校准运动想象分类设定了新基准，并提供了公开数据集推动研究。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [72] [Computer Science Education in the Age of Generative AI](https://arxiv.org/abs/2507.02183)
*Russell Beale*

Main category: cs.CY

TL;DR: 生成式AI（如ChatGPT和Codex）正在变革计算机科学教育，提供编码辅助、教学创新和评估优化的机会，同时也带来学术诚信和过度依赖的挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI如何增强计算机科学教育，包括其潜在机会和伴随的挑战。

Method: 分析AI在教育中的应用案例和实证数据，提出教学和评估的最佳实践及政策建议。

Result: 生成式AI为计算机科学教育带来便利和挑战，需平衡其使用以保持教育严谨性。

Conclusion: 需制定政策以充分利用AI潜力，同时维护教育质量和学术诚信。

Abstract: Generative AI tools - most notably large language models (LLMs) like ChatGPT
and Codex - are rapidly revolutionizing computer science education. These tools
can generate, debug, and explain code, thereby transforming the landscape of
programming instruction. This paper examines the profound opportunities that AI
offers for enhancing computer science education in general, from coding
assistance to fostering innovative pedagogical practices and streamlining
assessments. At the same time, it highlights challenges including academic
integrity concerns, the risk of over-reliance on AI, and difficulties in
verifying originality. We discuss what computer science educators should teach
in the AI era, how to best integrate these technologies into curricula, and the
best practices for assessing student learning in an environment where AI can
generate code, prototypes and user feedback. Finally, we propose a set of
policy recommendations designed to harness the potential of generative AI while
preserving the integrity and rigour of computer science education. Empirical
data and emerging studies are used throughout to support our arguments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [73] [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
*Yongsen Zheng,Zongxuan Xie,Guohua Wang,Ziyao Liu,Liang Lin,Kwok-Yan Lam*

Main category: cs.IR

TL;DR: 论文提出了一种名为HyFairCRS的新框架，通过超图对比多兴趣学习解决推荐系统中的不公平问题，特别是在动态和交互式的对话推荐系统中。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中存在基于性别、种族、年龄或流行度等因素的不公平问题，这些问题在动态交互环境中可能进一步加剧，导致如马太效应、过滤泡沫和回声室等严重后果。

Method: HyFairCRS通过对比学习建立多样化的超图来捕捉广泛的用户兴趣，并在对话中利用这些兴趣生成信息丰富的响应，确保动态用户-系统反馈循环中的公平项目预测。

Result: 在两个基于CRS的数据集上，HyFairCRS实现了最先进的性能，并有效减轻了不公平问题。

Conclusion: HyFairCRS为动态对话推荐系统中的公平性提供了一种有效的解决方案，并在实验中验证了其优越性。

Abstract: Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [74] [Optimising task allocation to balance business goals and worker well-being for financial service workforces](https://arxiv.org/abs/2507.01968)
*Chris Duckworth,Zlatko Zlatev,James Sciberras,Peter Hallett,Enrico Gerding*

Main category: q-fin.GN

TL;DR: 论文提出了一种基于遗传算法的任务分配模型，兼顾业务目标和员工福祉，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 金融服务公司处理海量数据时面临错误识别和解决的压力，影响了员工福祉和业务风险。

Method: 使用遗传算法优化任务分配模型，考虑员工技能、经验和福祉目标。

Result: 模型优于基线启发式方法和当前工作实践，适用于单目标和多目标场景。

Conclusion: 该模型填补了现有任务分配中忽视员工福祉的空白，同时提升了效率。

Abstract: Purpose: Financial service companies manage huge volumes of data which
requires timely error identification and resolution. The associated tasks to
resolve these errors frequently put financial analyst workforces under
significant pressure leading to resourcing challenges and increased business
risk. To address this challenge, we introduce a formal task allocation model
which considers both business orientated goals and analyst well-being.
  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to
allocate and schedule tasks to analysts. The proposed solution is able to
allocate tasks to analysts with appropriate skills and experience, while taking
into account staff well-being objectives.
  Findings: We demonstrate our GA model outperforms baseline heuristics,
current working practice, and is applicable to a range of single and
multi-objective real-world scenarios. We discuss the potential for
metaheuristics (such as GAs) to efficiently find sufficiently good allocations
which can provide recommendations for financial service managers in-the-loop.
  Originality: A key gap in existing allocation and scheduling models, is fully
considering worker well-being. This paper presents an allocation model which
explicitly optimises for well-being while still improving on current working
practice for efficiency.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [75] [Detecting Multiple Diseases in Multiple Crops Using Deep Learning](https://arxiv.org/abs/2507.02517)
*Vivek Yadav,Anugrah Jain*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的多作物多病害检测方法，旨在覆盖印度多样化的农业环境，显著提升了检测准确率和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 印度作为农业主导型经济，面临作物病害、害虫和环境压力等问题，导致大量减产。早期准确检测病害对提高产量和保障粮食安全至关重要。

Method: 创建一个包含17种作物和34种病害的统一数据集，并训练深度学习模型。

Result: 模型在统一数据集上的检测准确率达到99%，比现有技术（涵盖14种作物和26种病害）高出7%。

Conclusion: 该方案通过扩展可检测作物和病害类型，为印度农民提供了更高效的解决方案。

Abstract: India, as a predominantly agrarian economy, faces significant challenges in
agriculture, including substantial crop losses caused by diseases, pests, and
environmental stress. Early detection and accurate identification of diseases
across different crops are critical for improving yield and ensuring food
security. This paper proposes a deep learning based solution for detecting
multiple diseases in multiple crops, aimed to cover India's diverse
agricultural landscape. We first create a unified dataset encompassing images
of 17 different crops and 34 different diseases from various available
repositories. Proposed deep learning model is trained on this dataset and
outperforms the state-of-the-art in terms of accuracy and the number of crops,
diseases covered. We achieve a significant detection accuracy, i.e., 99 percent
for our unified dataset which is 7 percent more when compared to
state-of-the-art handling 14 crops and 26 different diseases only. By improving
the number of crops and types of diseases that can be detected, proposed
solution aims to provide a better product for Indian farmers.

</details>


### [76] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: Mesh Silksong是一种高效且紧凑的网格表示方法，通过自回归方式生成多边形网格，减少冗余并实现22%的压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有网格标记化方法会产生重复顶点标记，浪费网络能力，因此需要一种更高效的网格表示方法。

Method: 通过仅访问每个网格顶点一次来标记化网格顶点，减少标记序列的冗余，并生成具有优越几何特性的多边形网格。

Result: 实现了50%的冗余减少和22%的压缩率，生成了具有流形拓扑、水密性检测和一致面法线的高质量网格。

Conclusion: Mesh Silksong在复杂网格生成和几何完整性方面表现出色，适用于实际应用。

Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [77] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 本文提出了一种简单的自蒸馏方法，用于扩展视频到音频（V2A）模型在电影语言场景中的应用，解决了当前方法在部分可见场景下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频到音频生成方法忽视了电影语言在艺术表达中的重要性，导致在部分可见的场景中性能下降。本文旨在解决这一问题。

Method: 采用自蒸馏方法，通过模拟电影语言的变化，使学生模型能够对齐视频特征和音频-视觉对应关系，从而捕捉部分视觉信息与声音的关联。

Result: 该方法在部分可见场景的所有评估指标上均取得了显著提升，同时在大规模V2A数据集VGGSound上也表现出色。

Conclusion: 提出的自蒸馏方法有效提升了视频到音频生成模型在电影语言场景中的性能，尤其是在部分可见的情况下。

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [78] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: 论文介绍了HyperGaussians，一种用于高质量可动画人脸头像的3D高斯溅射扩展方法，解决了现有技术在非线性变形和复杂光照效果上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯溅射技术在处理可动画头像时存在非线性变形和复杂光照效果的挑战，HyperGaussians旨在提升这些方面的表现。

Method: 通过将3D高斯扩展到高维多变量高斯（HyperGaussians），并结合可学习的局部嵌入和逆协方差技巧来提高表达能力和计算效率。

Result: 在19名受试者上的实验表明，HyperGaussians在数值和视觉上均优于3D高斯溅射，尤其是在高频细节和复杂表情处理上。

Conclusion: HyperGaussians为高质量可动画头像提供了一种更高效和表达能力更强的解决方案。

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [79] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality是一种将RGB-D扫描转换为紧凑、逼真且交互式3D虚拟场景的新方法，支持高质量渲染和物理交互。


<details>
  <summary>Details</summary>
Motivation: 为AR/VR、游戏、机器人等领域提供兼容标准图形管道的可编辑3D虚拟场景。

Method: 通过场景理解和场景图解析，从资产库检索相似3D模型，增强材质和物理属性。

Result: 在Scan2CAD基准测试中表现优异，支持高质量材质转换。

Conclusion: LiteReality适用于多领域应用，效果显著。

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [80] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: 通过使用FINN架构在FPGA上部署量化ANN模型，显著提升了机器人视觉任务的速度与效率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人因低速扫描和低帧率摄像头导致的任务执行时间长的问题。

Method: 使用FINN架构在FPGA的PL上部署三种量化ANN模型（MobileNet v1、CNV-2bit和BNN），并基于RG2C数据集训练。

Result: MobileNet v1表现最佳，成功率达98%，推理速度达6611 FPS。

Conclusion: FPGA可加速ANN推理，适合用于注意力机制，提升机器人任务效率。

Abstract: Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit
quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation
(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This
is a self-acquired dataset released in open access. MobileNet v1 performed
better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In
this work, we proved that we can use FPGAs to speed up ANNs and make them
suitable for attention mechanisms.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [81] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 论文探讨了高质量标注数据稀缺的问题，提出了人类标注差异（HLV）作为有价值的信号，并提出了一个框架，将HLV融入主动学习中。


<details>
  <summary>Details</summary>
Motivation: 标注数据的质量是监督学习的主要限制因素，而人类标注的差异常被忽视。论文旨在重新思考标注的本质，并提出如何更有效地利用HLV。

Method: 通过分解标注差异为信号（如HLV）和噪声（如标注错误），提出将HLV纳入主动学习的框架，包括实例选择、标注者选择和标签表示。

Result: 论文提出了一个框架，支持在主动学习中更好地利用HLV，并讨论了如何结合大语言模型作为标注者。

Conclusion: 研究为HLV感知的主动学习奠定了基础，更贴近真实世界标注的复杂性。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [82] [Public perspectives on the design of fusion energy facilities](https://arxiv.org/abs/2507.02207)
*Nathan Kawamoto,Daniel Hoover,Jonathan Xie,Jacob Walters,Katie Snyder,Aditi Verma*

Main category: physics.soc-ph

TL;DR: 论文探讨了通过参与式设计方法与社会合作设计核聚变能源设施，研究了社区成员与工程学生的设计价值和决策标准，发现早期参与有助于提升公众对新兴技术的理解与社会认可。


<details>
  <summary>Details</summary>
Motivation: 随着核聚变技术接近商业化部署，了解公众对未来核聚变设施的看法对获得社会认可至关重要，尤其是在与社区更近距离建设的情况下。

Method: 采用参与式设计方法，组织研讨会，邀请22名社区成员和34名工程学生共同设计假设的核聚变设施，分析文本和视觉数据。

Result: 设计价值和决策标准中，“诚信”与“尊重”排名最高，而“经济效益”和“环境保护/安全”是最重要的决策依据。研讨会中参与者表现出积极的情绪。

Conclusion: 早期参与式设计可以明确公众的期望与担忧，提升对新兴技术的理解与好奇，有助于社会认可，并为核聚变设施的本地化发展提供指导。

Abstract: As fusion energy technologies approach demonstration and commercial
deployment, understanding public perspectives on future fusion facilities will
be critical for achieving social license, especially because fusion energy
facilities, unlike large fission reactors, may be sited in closer proximity to
people and communities, due to distinct regulatory frameworks. In a departure
from the 'decide-announce-defend' approach typically used to site energy
infrastructure, we develop a participatory design methodology for
collaboratively designing fusion energy facilities with prospective host
communities. We present here our findings from a participatory design workshop
that brought together 22 community participants and 34 engineering students.
Our analysis of the textual and visual data from this workshop shows a range of
design values and decision-making criteria with 'integrity' and 'respect'
ranking highest among values and 'economic benefits' and 'environmental
protection/safety' ranking highest among decision-making criteria. Salient
design themes that emerge across facility concepts include connecting the
history and legacy of the community to the design of the facility, care for
workers, transparency and access to the facility, and health and safety of the
host community. Participants reported predominantly positive sentiments,
expressing joy and surprise as the workshop progressed from learning about
fusion to designing the hypothetical facility. Our findings suggest that
carrying out participatory design in the early stages of technology development
can invite and make concrete public hopes and concerns, improve understanding
of, and curiosity about, an emerging technology, build toward social license,
and inform context-specific development of fusion energy facilities.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [83] [A formal specification of the desired software behaviour of the Princess Marijke lock complex](https://arxiv.org/abs/2507.02721)
*Jan Friso Groote,Matthias Volk*

Main category: eess.SY

TL;DR: 论文通过少于400行mCRL2代码精确描述了荷兰公主Marijke水闸系统的软件控制，并通过模型检查验证了53项软件需求，确保其正确性。


<details>
  <summary>Details</summary>
Motivation: 确保水闸系统的安全控制，以保障防洪和船舶操作的可靠性。

Method: 使用mCRL2代码进行形式化描述，并通过模型检查验证。

Result: 成功验证了53项软件需求，证明了形式化描述的正确性。

Conclusion: 论文提供的水闸系统软件控制蓝图具有高可靠性，可避免错误和遗漏。

Abstract: The Princess Marijke lock complex is a large lock and water-protection
installation in the Netherlands between the river Rhine and the
Amsterdam-Rijnkanaal -- a large waterway connecting the Rhine to the port of
Amsterdam. The lock complex consists of two independent locks and a moveable
flood-protection barrier. Ensuring safe control of the lock complex is of
utmost importance to guarantee both flood-protection and reliable ship
operations. This paper gives a precise, formal description of the software
control of the lock complex in less than 400 lines of mCRL2 code. This
description can act as a blueprint on how the software of this lock complex
needs to be constructed. Moreover, using model checking, 53 software
requirements are shown to be valid, ensuring that the formal description of the
behaviour is correct with regard to these properties and is unlikely to contain
mistakes and oversights.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [84] [Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems](https://arxiv.org/abs/2507.02464)
*Craig S Wright*

Main category: cs.GT

TL;DR: 提出了一种基于自动机理论和经济激励的框架，将CAP理论中的权衡重新定义为约束优化问题，通过游戏论机制在分布式系统中实现一致性、可用性和分区容错的平衡。


<details>
  <summary>Details</summary>
Motivation: 传统CAP理论认为分布式系统无法同时满足一致性、可用性和分区容错性。本文希望通过引入经济激励和游戏论机制，突破这一限制，提供更灵活的解决方案。

Method: 将分布式系统建模为分区感知的状态机，并嵌入经济激励层，通过游戏论机制调整共识行为，确保在网络分区的情况下系统仍能收敛。

Result: 证明了在一定的经济控制下，可用性和一致性可以同时在一定误差范围内保持，从而拓展了经典CAP理论的边界。

Conclusion: 通过经济激励和自动机理论的结合，本文展示了如何在分布式系统中更灵活地平衡CAP三要素，突破了传统理论的固有限制。

Abstract: The CAP theorem asserts a trilemma between consistency, availability, and
partition tolerance. This paper introduces a rigorous automata-theoretic and
economically grounded framework that reframes the CAP trade-off as a constraint
optimization problem. We model distributed systems as partition-aware state
machines and embed economic incentive layers to stabilize consensus behavior
across adversarially partitioned networks. By incorporating game-theoretic
mechanisms into the global transition semantics, we define provable bounds on
convergence, liveness, and correctness. Our results demonstrate that
availability and consistency can be simultaneously preserved within bounded
epsilon margins, effectively extending the classical CAP limits through formal
economic control.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [85] [DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems](https://arxiv.org/abs/2507.02400)
*Maximilian Zipfl,Pascal Zwick,Patrick Schulz,Marc Rene Zofka,Albert Schotschneider,Helen Gremmelmaier,Nikolai Polley,Ferdinand Mütsch,Kevin Simon,Fabian Gottselig,Michael Frey,Sergio Marschall,Akim Stark,Maximilian Müller,Marek Wehmer,Mihai Kocsis,Dominic Waldenmayer,Florian Schnepf,Erik Heinrich,Sabrina Pletz,Matthias Kölle,Karin Langbein-Euchner,Alexander Viehl,Raoul Zöllner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 本文探讨了如何通过数字化重建为德国TAF-BW测试区域构建数字孪生，并展示了其在交通优化和通信安全模拟中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着数字化的普及，交通系统需要更高效的互联互通解决方案，数字孪生技术因其双向实时连接特性成为理想选择。

Method: 利用摄像头和LiDAR传感器从智能基础设施和测试车辆中提取数据，生成真实输入，并通过统一接口实现交通参与者的重模拟。

Result: 开发了一个可公开下载的数字孪生框架，并通过交通信号优化和通信安全案例验证了其有效性。

Conclusion: 数字孪生技术在提升交通系统效率和安全性方面具有显著潜力，未来可进一步推广和应用。

Abstract: In the future, mobility will be strongly shaped by the increasing use of
digitalization. Not only will individual road users be highly interconnected,
but also the road and associated infrastructure. At that point, a Digital Twin
becomes particularly appealing because, unlike a basic simulation, it offers a
continuous, bilateral connection linking the real and virtual environments.
This paper describes the digital reconstruction used to develop the Digital
Twin of the Test Area Autonomous Driving-Baden-W\"urttemberg (TAF-BW), Germany.
The TAF-BW offers a variety of different road sections, from high-traffic urban
intersections and tunnels to multilane motorways. The test area is equipped
with a comprehensive Vehicle-to-Everything (V2X) communication infrastructure
and multiple intelligent intersections equipped with camera sensors to
facilitate real-time traffic flow monitoring. The generation of authentic data
as input for the Digital Twin was achieved by extracting object lists at the
intersections. This process was facilitated by the combined utilization of
camera images from the intelligent infrastructure and LiDAR sensors mounted on
a test vehicle. Using a unified interface, recordings from real-world
detections of traffic participants can be resimulated. Additionally, the
simulation framework's design and the reconstruction process is discussed. The
resulting framework is made publicly available for download and utilization at:
https://digit4taf-bw.fzi.de The demonstration uses two case studies to
illustrate the application of the digital twin and its interfaces: the analysis
of traffic signal systems to optimize traffic flow and the simulation of
security-related scenarios in the communications sector.

</details>


### [86] [MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints](https://arxiv.org/abs/2507.02438)
*Shivam Chaubey,Francesco Verdoja,Shankar Deka,Ville Kyrki*

Main category: cs.RO

TL;DR: 论文提出了基于约束最优控制问题的共享控制框架，确保可行性、严格约束满足和最小用户意图覆盖，并通过大规模用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有共享控制方法在可行性、扩展性或安全性保证方面存在不足，尤其是用户输入不可预测时。

Method: 提出基于约束最优控制问题的框架，结合离线计算的控制不变集，支持在线计算控制动作。

Result: 通过大规模用户研究验证，该框架在所有评估方面均有改进且不损害安全性和用户意图。

Conclusion: 该框架为共享控制提供了更灵活、可靠的解决方案，适用于多种实际场景。

Abstract: Shared control combines human intention with autonomous decision-making, from
low-level safety overrides to high-level task guidance, enabling systems that
adapt to users while ensuring safety and performance. This enhances task
effectiveness and user experience across domains such as assistive robotics,
teleoperation, and autonomous driving. However, existing shared control
methods, based on e.g. Model Predictive Control, Control Barrier Functions, or
learning-based control, struggle with feasibility, scalability, or safety
guarantees, particularly since the user input is unpredictable.
  To address these challenges, we propose an assistive controller framework
based on Constrained Optimal Control Problem that incorporates an
offline-computed Control Invariant Set, enabling online computation of control
actions that ensure feasibility, strict constraint satisfaction, and minimal
override of user intent. Moreover, the framework can accommodate structured
class of non-convex constraints, which are common in real-world scenarios. We
validate the approach through a large-scale user study with 66
participants--one of the most extensive in shared control research--using a
computer game environment to assess task load, trust, and perceived control, in
addition to performance. The results show consistent improvements across all
these aspects without compromising safety and user intent.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 文章讨论了信念修订领域的现状，指出尽管有许多新提案，但对现有方法的分析不足。重点在于修订机制的能力，而非约束。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是探索信念修订机制的能力（如塑性、平等化、教条化等），而非仅仅满足于现有的语法特性或约束。

Method: 通过分析不同的修订机制（如词典式、自然式、约束式等）及其能力，研究其在特定应用场景中的适用性。

Result: 结果表明不同的修订机制具有不同的能力，适合不同的应用需求。

Conclusion: 结论强调了在信念修订领域，不仅需要满足语法约束，还需要关注机制的能力，以适应多样化的应用场景。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [88] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 提出了一种基于智能代理的AI方法，结合人类干预（HITL），用于现代集成电路的设计验证，显著提高覆盖率和效率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路设计验证过程复杂且耗时，需要一种更高效的自动化方法来确保设计的正确性。

Method: 采用大型语言模型（LLMs）和智能代理，结合人类干预，实现动态、迭代和自我反思的设计验证流程。

Result: 在五个开源设计上测试，覆盖率超过95%，验证时间减少，表现出卓越的性能和适应性。

Conclusion: 该方法为硬件设计验证提供了一种高效、可配置的解决方案，显著提升了验证效率和质量。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [89] [Access Control Threatened by Quantum Entanglement](https://arxiv.org/abs/2507.02622)
*Zhicheng Zhang,Mingsheng Ying*

Main category: quant-ph

TL;DR: 本文研究了量子计算机系统中的访问控制问题，揭示了经典访问控制系统在量子环境中的安全漏洞，并提出了新的量子访问控制模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索经典访问控制系统在量子计算环境中的安全隐患，因为量子力学中的纠缠现象会违反多体贝尔不等式（Mermin不等式），从而可能导致安全漏洞。

Method: 方法是首次展示了一个经典访问控制系统直接应用于量子环境时的安全漏洞场景，并提出了几种新的量子访问控制模型。

Result: 结果揭示了量子纠缠对访问控制的潜在威胁，并通过安全、灵活性和效率的严格分析验证了新模型的有效性。

Conclusion: 结论是量子计算环境下需要新的访问控制模型来应对量子纠缠带来的安全挑战。

Abstract: Access control is a cornerstone of computer security that prevents
unauthorised access to resources. In this paper, we study access control in
quantum computer systems. We present the first explicit scenario of a security
breach when a classically secure access control system is straightforwardly
adapted to the quantum setting. The breach is ultimately due to that quantum
mechanics allows the phenomenon of entanglement and violates Mermin inequality,
a multi-party variant of the celebrated Bell inequality. This reveals a threat
from quantum entanglement to access control if existing computer systems
integrate with quantum computing. To protect against such threat, we propose
several new models of quantum access control, and rigorously analyse their
security, flexibility and efficiency.

</details>
