{"id": "2509.03541", "pdf": "https://arxiv.org/pdf/2509.03541", "abs": "https://arxiv.org/abs/2509.03541", "authors": ["Chong Wang", "Haoning Wu", "Peng Liang", "Maya Daneva", "Marten van Sinderen"], "title": "Towards the Datasets Used in Requirements Engineering of Mobile Apps: Preliminary Findings from a Systematic Mapping Study", "categories": ["cs.SE"], "comment": null, "summary": "[Background] Research on requirements engineering (RE) for mobile apps\nemploys datasets formed by app users, developers or vendors. However, little is\nknown about the sources of these datasets in terms of platforms and the RE\nactivities that were researched with the help of the respective datasets.\n[Aims] The goal of this paper is to investigate the state-of-the-art of the\ndatasets of mobile apps used in existing RE research. [Method] We carried out a\nsystematic mapping study by following the guidelines of Kitchenham et al.\n[Results] Based on 43 selected papers, we found that Google Play and Apple App\nStore provide the datasets for more than 90% of published research in RE for\nmobile apps. We also found that the most investigated RE activities - based on\ndatasets, are requirements elicitation and requirements analysis. [Conclusions]\nOur most important conclusions are: (1) there is a growth in the use of\ndatasets for RE research of mobile apps since 2012, (2) the RE knowledge for\nmobile apps might be skewed due to the overuse of Google Play and Apple App\nStore, (3) there are attempts to supplement reviews of apps from repositories\nwith other data sources, (4) there is a need to expand the alternative sources\nand experiments with complimentary use of multiple sources, if the community\nwants more generalizable results. Plus, it is expected to expand the research\non other RE activities, beyond elicitation and analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\u53d1\u73b0\uff0c\u79fb\u52a8\u5e94\u7528\u9700\u6c42\u5de5\u7a0b\u7814\u7a76\u7684\u6570\u636e\u96c6\u4e3b\u8981\u6765\u81eaGoogle Play\u548cApple App Store\uff0c\u4e14\u96c6\u4e2d\u5728\u9700\u6c42\u83b7\u53d6\u548c\u5206\u6790\u6d3b\u52a8\u4e0a\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u96c6\u6765\u6e90\u591a\u6837\u5316\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u79fb\u52a8\u5e94\u7528\u9700\u6c42\u5de5\u7a0b\u4e2d\u6570\u636e\u96c6\u7684\u4f7f\u7528\u73b0\u72b6\uff0c\u63ed\u793a\u73b0\u6709\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528Kitchenham\u7b49\u7684\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\u6307\u5357\uff0c\u5206\u6790\u4e8643\u7bc7\u76f8\u5173\u8bba\u6587\u3002", "result": "\u53d1\u73b090%\u4ee5\u4e0a\u7684\u7814\u7a76\u4f7f\u7528Google Play\u548cApple App Store\u6570\u636e\u96c6\uff0c\u4e14\u9700\u6c42\u83b7\u53d6\u548c\u5206\u6790\u6d3b\u52a8\u4e3a\u4e3b\u3002", "conclusion": "\u9700\u6269\u5c55\u6570\u636e\u96c6\u6765\u6e90\u548c\u7814\u7a76\u6d3b\u52a8\u8303\u56f4\u4ee5\u63d0\u9ad8\u7ed3\u679c\u7684\u666e\u9002\u6027\u3002"}}
{"id": "2509.03554", "pdf": "https://arxiv.org/pdf/2509.03554", "abs": "https://arxiv.org/abs/2509.03554", "authors": ["Cheng-Yang Tsai", "Tzu-Wei Huang", "Jen-Wei Shih", "I-Hsiang Wang", "Yu-Cheng Lin", "Rung-Bin Lin"], "title": "A Multi-stage Error Diagnosis for APB Transaction", "categories": ["cs.SE"], "comment": null, "summary": "Functional verification and debugging are critical bottlenecks in modern\nSystem-on-Chip (SoC) design, with manual detection of Advanced Peripheral Bus\n(APB) transaction errors in large Value Change Dump (VCD) files being\ninefficient and error-prone. Addressing the 2025 ICCAD Contest Problem D, this\nstudy proposes an automated error diagnosis framework using a hierarchical\nRandom Forest-based architecture. The multi-stage error diagnosis employs four\npre-trained binary classifiers to sequentially detect Out-of-Range Access,\nAddress Corruption, and Data Corruption errors, prioritizing high-certainty\naddress-related faults before tackling complex data errors to enhance\nefficiency. Experimental results show an overall accuracy of 91.36%, with\nnear-perfect precision and recall for address errors and robust performance for\ndata errors. Although the final results of the ICCAD 2025 CAD Contest are yet\nto be announced as of the submission date, our team achieved first place in the\nbeta stage, highlighting the method's competitive strength. This research\nvalidates the potential of hierarchical machine learning as a powerful\nautomated tool for hardware debugging in Electronic Design Automation (EDA).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u968f\u673a\u68ee\u6797\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u68c0\u6d4bSoC\u8bbe\u8ba1\u4e2d\u7684APB\u4e8b\u52a1\u9519\u8bef\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a91.36%\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u4ee3SoC\u8bbe\u8ba1\u4e2d\uff0c\u529f\u80fd\u9a8c\u8bc1\u548c\u8c03\u8bd5\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u624b\u52a8\u68c0\u6d4bVCD\u6587\u4ef6\u4e2d\u7684APB\u4e8b\u52a1\u9519\u8bef\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5206\u5c42\u968f\u673a\u68ee\u6797\u67b6\u6784\uff0c\u901a\u8fc7\u56db\u4e2a\u9884\u8bad\u7ec3\u7684\u4e8c\u5206\u7c7b\u5668\u4f9d\u6b21\u68c0\u6d4b\u8303\u56f4\u5916\u8bbf\u95ee\u3001\u5730\u5740\u635f\u574f\u548c\u6570\u636e\u635f\u574f\u9519\u8bef\uff0c\u4f18\u5148\u5904\u7406\u9ad8\u786e\u4fe1\u5ea6\u5730\u5740\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6574\u4f53\u51c6\u786e\u7387\u4e3a91.36%\uff0c\u5730\u5740\u9519\u8bef\u68c0\u6d4b\u8fd1\u4e4e\u5b8c\u7f8e\uff0c\u6570\u636e\u9519\u8bef\u8868\u73b0\u7a33\u5065\uff0c\u56e2\u961f\u5728ICCAD 2025\u6d4b\u8bd5\u9636\u6bb5\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "\u5206\u5c42\u673a\u5668\u5b66\u4e60\u5728EDA\u786c\u4ef6\u8c03\u8bd5\u4e2d\u5177\u6709\u5f3a\u5927\u6f5c\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u81ea\u52a8\u5316\u5de5\u5177\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.03668", "pdf": "https://arxiv.org/pdf/2509.03668", "abs": "https://arxiv.org/abs/2509.03668", "authors": ["Matt Rau", "Chris Brown", "John Edwards"], "title": "Parse Tree Tracking Through Time for Programming Process Analysis at Scale", "categories": ["cs.SE"], "comment": null, "summary": "Background and Context: Programming process data can be utilized to\nunderstand the processes students use to write computer programming\nassignments. Keystroke- and line-level event logs have been used in the past in\nvarious ways, primarily in high-level descriptive statistics (e.g., timings,\ncharacter deletion rate, etc). Analysis of behavior in context (e.g., how much\ntime students spend working on loops) has been cumbersome because of our\ninability to automatically track high-level code representations, such as\nabstract syntax trees, through time and unparseable states.\n  Objective: Our study has two goals. The first is to design the first\nalgorithm that tracks parse tree nodes through time. Second, we utilize this\nalgorithm to perform a partial replication study of prior work that used manual\ntracking of code representations, as well as other novel analyses of student\nprogramming behavior that can now be done at scale.\n  Method: We use two algorithms presented in this paper to track parse tree\nnodes through time and construct tree representations for unparseable code\nstates. We apply these algorithms to a public keystroke data from student\ncoursework in a 2021 CS1 course and conduct analysis on the resulting parse\ntrees.\n  Findings: We discover newly observable statistics at scale, including that\ncode is deleted at similar rates inside and outside of conditionals and loops,\na third of commented out code is eventually restored, and that frequency with\nwhich students jump around in their code may not be indicative of struggle.\n  Implications: The ability to track parse trees through time opens the door to\nunderstanding new dimensions of student programming, such as best practices of\nstructural development of code over time, quantitative measurement of what\nsyntactic constructs students struggle most with, refactoring behavior, and\nattention shifting within the code.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8ddf\u8e2a\u62bd\u8c61\u8bed\u6cd5\u6811\u8282\u70b9\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u5b66\u751f\u5728\u7f16\u7a0b\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u4e86\u4e00\u4e9b\u65b0\u7684\u7edf\u8ba1\u53d1\u73b0\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8ddf\u8e2a\u62bd\u8c61\u8bed\u6cd5\u6811\u8282\u70b9\u6765\u66f4\u6df1\u5165\u5730\u7406\u89e3\u5b66\u751f\u7684\u7f16\u7a0b\u884c\u4e3a\uff0c\u586b\u8865\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u5c42\u6b21\u4ee3\u7801\u8868\u793a\u81ea\u52a8\u5316\u8ddf\u8e2a\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ddf\u8e2a\u8bed\u6cd5\u6811\u8282\u70b9\u5e76\u4e3a\u4e0d\u53ef\u89e3\u6790\u7684\u4ee3\u7801\u72b6\u6001\u6784\u5efa\u6811\u8868\u793a\uff0c\u5e94\u7528\u4e8e\u516c\u5f00\u7684\u5b66\u751f\u7f16\u7a0b\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e9b\u65b0\u7684\u7edf\u8ba1\u73b0\u8c61\uff0c\u4f8b\u5982\u4ee3\u7801\u5220\u9664\u7387\u5728\u6761\u4ef6\u8bed\u53e5\u5185\u5916\u76f8\u4f3c\uff0c\u4ee5\u53ca\u5b66\u751f\u4ee3\u7801\u8df3\u8f6c\u884c\u4e3a\u4e0d\u4e00\u5b9a\u53cd\u6620\u56f0\u96be\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5316\u5b66\u751f\u7f16\u7a0b\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7ef4\u5ea6\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u4ee3\u7801\u7ed3\u6784\u53d1\u5c55\u548c\u5b66\u751f\u7f16\u7a0b\u56f0\u96be\u3002"}}
{"id": "2509.03848", "pdf": "https://arxiv.org/pdf/2509.03848", "abs": "https://arxiv.org/abs/2509.03848", "authors": ["Rodrigo Oliveira Zacarias", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems", "categories": ["cs.SE", "cs.HC"], "comment": "36 pages Submitted to the ACM Transactions on Software Engineering\n  and Methodology. 2025", "summary": "Software ecosystems (SECO) have become a dominant paradigm in the software\nindustry, enabling third-party developers to co-create value through\ncomplementary components and services. While Developer Experience (DX) is\nincreasingly recognized as critical for sustainable SECO, transparency remains\nan underexplored factor shaping how developers perceive and interact with\necosystems. Existing studies acknowledge transparency as essential for trust,\nfairness, and engagement, yet its relationship with DX has not been\nsystematically conceptualized. Hence, this work aims to advance the\nunderstanding of transparency in SECO from a developer-centered perspective. To\nthis end, we propose SECO-TransDX (Transparency in Software Ecosystems from a\nDeveloper Experience Perspective), a conceptual model that introduces the\nnotion of DX-driven transparency. The model identifies 63 interrelated\nconcepts, including conditioning factors, ecosystem procedures, artifacts, and\nrelational dynamics that influence how transparency is perceived and\nconstructed during developer interactions. SECO-TransDX was built upon prior\nresearch and refined through a Delphi study with experts from academia and\nindustry. It offers a structured lens to examine how transparency mediates DX\nacross technical, social, and organizational layers. For researchers, it lays\nthe groundwork for future studies and tool development; for practitioners, it\nsupports the design of trustworthy, developer-centered platforms that improve\ntransparency and foster long-term engagement in SECO.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SECO-TransDX\u6a21\u578b\uff0c\u4ece\u5f00\u53d1\u8005\u4f53\u9a8c\uff08DX\uff09\u89d2\u5ea6\u63a2\u8ba8\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\uff08SECO\uff09\u4e2d\u7684\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u65e8\u5728\u7cfb\u7edf\u6027\u7406\u89e3\u5176\u5bf9\u5f00\u53d1\u8005\u611f\u77e5\u548c\u4e92\u52a8\u7684\u5f71\u54cd\u3002", "motivation": "\u900f\u660e\u5ea6\u662f\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u5173\u952e\u56e0\u7d20\uff0c\u73b0\u6709\u7814\u7a76\u867d\u627f\u8ba4\u5176\u5bf9\u4fe1\u4efb\u3001\u516c\u5e73\u548c\u53c2\u4e0e\u7684\u91cd\u8981\u6027\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6982\u5ff5\u5316\u4e0e\u5f00\u53d1\u8005\u4f53\u9a8c\u7684\u5173\u7cfb\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86SECO-TransDX\u6982\u5ff5\u6a21\u578b\uff0c\u57fa\u4e8e\u5148\u524d\u7814\u7a76\u5e76\u901a\u8fc7\u5fb7\u5c14\u83f2\u6cd5\uff08Delphi study\uff09\u4e0e\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u4e13\u5bb6\u5171\u540c\u5b8c\u5584\u3002\u6a21\u578b\u5305\u542b63\u4e2a\u76f8\u5173\u6982\u5ff5\u3002", "result": "SECO-TransDX\u6a21\u578b\u63ed\u793a\u4e86\u900f\u660e\u5ea6\u5982\u4f55\u901a\u8fc7\u6280\u672f\u3001\u793e\u4f1a\u548c\u7ec4\u7ec7\u5c42\u9762\u4e2d\u4ecb\u5f00\u53d1\u8005\u4f53\u9a8c\uff0c\u4e3a\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89c6\u89d2\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u540c\u65f6\u5e2e\u52a9\u5b9e\u8df5\u8005\u8bbe\u8ba1\u66f4\u5177\u900f\u660e\u5ea6\u7684\u5f00\u53d1\u8005\u4e2d\u5fc3\u5e73\u53f0\uff0c\u4fc3\u8fdb\u957f\u671f\u53c2\u4e0e\u3002"}}
{"id": "2509.04253", "pdf": "https://arxiv.org/pdf/2509.04253", "abs": "https://arxiv.org/abs/2509.04253", "authors": ["Siyuan He", "Songlin Jia", "Yuyan Bao", "Tiark Rompf"], "title": "When Lifetimes Liberate: A Type System for Arenas with Higher-Order Reachability Tracking", "categories": ["cs.PL"], "comment": null, "summary": "Static resource management in higher-order functional languages remains\nelusive due to tensions between control, expressiveness, and flexibility.\nRegion-based systems [Grossman et al. 2002; Tofte et al. 2001] offer control\nover lifetimes and expressive in-region sharing, but restrict resources to\nlexical scopes. Rust, an instance of ownership types [Clarke et al. 2013],\noffers non-lexical lifetimes and robust safety guarantees, yet its global\ninvariants make common sharing patterns hard to express. Reachability types\n[Wei et al. 2024] enable reasoning about sharing and separation, but lack\npractical tools for controlling resource lifetimes.\n  In this work, we try to unify their strengths. Our solution enables grouping\nresources as arenas for arbitrary sharing and static guarantees of lexically\nscoped lifetimes. Crucially, arenas and lexical lifetimes are not the only\nchoice: users may also manage resources individually, with non-lexical\nlifetimes. Regardless of mode, resources share the same type, preserving the\nhigher-order parametric nature of the language.\n  Obtaining static safety guarantee in a higher-order language with flexible\nsharing is nontrivial. To this end, we propose two new extensions atop\nreachability types [Wei et al. 2024]. First, A<: features a novel\ntwo-dimensional store model to enable coarse-grained reachability tracking for\narbitrarily shared resources within arenas. Building on this, {A}<: establishes\nlexical lifetime control with static guarantees. As the first reachability\nformalism presented for lifetime control, {A}<: avoids the complication of\nflow-sensitive reasoning and retains expressive power and simplicity. Both\ncalculi are formalized and proven type safe in Rocq.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u9759\u6001\u8d44\u6e90\u7ba1\u7406\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u533a\u57df\u7cfb\u7edf\u548c\u6240\u6709\u6743\u7c7b\u578b\u7684\u4f18\u70b9\uff0c\u652f\u6301\u7075\u6d3b\u7684\u8d44\u6e90\u5171\u4eab\u548c\u9759\u6001\u751f\u547d\u5468\u671f\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u9ad8\u9636\u51fd\u6570\u8bed\u8a00\u4e2d\u9759\u6001\u8d44\u6e90\u7ba1\u7406\u7684\u96be\u9898\uff0c\u7ed3\u5408\u533a\u57df\u7cfb\u7edf\u548c\u6240\u6709\u6743\u7c7b\u578b\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u8d44\u6e90\u7ba1\u7406\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u6269\u5c55\uff1aA<:\u548c{A}<:\uff0c\u5206\u522b\u652f\u6301\u7c97\u7c92\u5ea6\u53ef\u8fbe\u6027\u8ddf\u8e2a\u548c\u8bcd\u6cd5\u751f\u547d\u5468\u671f\u63a7\u5236\uff0c\u5e76\u5728Rocq\u4e2d\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "result": "\u5b9e\u73b0\u4e86\u9759\u6001\u5b89\u5168\u4fdd\u969c\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u548c\u7b80\u5355\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7edf\u4e00\u4e86\u533a\u57df\u7cfb\u7edf\u548c\u6240\u6709\u6743\u7c7b\u578b\u7684\u4f18\u52bf\uff0c\u4e3a\u9ad8\u9636\u8bed\u8a00\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03855", "pdf": "https://arxiv.org/pdf/2509.03855", "abs": "https://arxiv.org/abs/2509.03855", "authors": ["Zhouyi Zhou", "Zhili Liu", "Shancong Zhang", "Jiemin Li", "Dengke Du", "Mengke Sun", "Zhiqiang Wang", "Hongyan Liu", "Guogai Xu"], "title": "Towards Deterministic Sub-0.5 us Response on Linux through Interrupt Isolation", "categories": ["cs.OS", "68M20", "D.4.7"], "comment": "9 pages, 11 figures", "summary": "Real-time responsiveness in Linux is often constrained by interrupt\ncontention and timer handling overhead, making it challenging to achieve\nsub-microsecond latency. This work introduces an interrupt isolation approach\nthat centralizes and minimizes timer interrupt interference across CPU cores.\nBy enabling a dedicated API to selectively invoke timer handling routines and\nsuppress non-critical inter-processor interrupts, our design significantly\nreduces jitter and response latency. Experiments conducted on an ARM-based\nmulticore platform demonstrate that the proposed mechanism consistently\nachieves sub-0.5 us response times, outperforming conventional Linux PREEMPT-RT\nconfigurations. These results highlight the potential of interrupt isolation as\na lightweight and effective strategy for deterministic real-time workloads in\ngeneral-purpose operating systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2d\u65ad\u9694\u79bb\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u4e2d\u548c\u6700\u5c0f\u5316\u8ba1\u65f6\u5668\u4e2d\u65ad\u5e72\u6270\uff0c\u663e\u8457\u964d\u4f4e\u6296\u52a8\u548c\u54cd\u5e94\u5ef6\u8fdf\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u5728ARM\u591a\u6838\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4f4e\u4e8e0.5\u5fae\u79d2\u7684\u54cd\u5e94\u65f6\u95f4\u3002", "motivation": "Linux\u7cfb\u7edf\u7684\u5b9e\u65f6\u54cd\u5e94\u80fd\u529b\u5e38\u53d7\u4e2d\u65ad\u51b2\u7a81\u548c\u8ba1\u65f6\u5668\u5904\u7406\u5f00\u9500\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5b9e\u73b0\u4e9a\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e2d\u65ad\u9694\u79bb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e13\u7528API\u9009\u62e9\u6027\u8c03\u7528\u8ba1\u65f6\u5668\u5904\u7406\u7a0b\u5e8f\u5e76\u6291\u5236\u975e\u5173\u952e\u7684\u8de8\u5904\u7406\u5668\u4e2d\u65ad\u3002", "result": "\u5728ARM\u591a\u6838\u5e73\u53f0\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u673a\u5236\u80fd\u7a33\u5b9a\u5b9e\u73b0\u4f4e\u4e8e0.5\u5fae\u79d2\u7684\u54cd\u5e94\u65f6\u95f4\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684Linux PREEMPT-RT\u914d\u7f6e\u3002", "conclusion": "\u4e2d\u65ad\u9694\u79bb\u662f\u4e00\u79cd\u8f7b\u91cf\u4e14\u6709\u6548\u7684\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u901a\u7528\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u786e\u5b9a\u6027\u5b9e\u65f6\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6f5c\u5728\u65b9\u6848\u3002"}}
{"id": "2509.03680", "pdf": "https://arxiv.org/pdf/2509.03680", "abs": "https://arxiv.org/abs/2509.03680", "authors": ["Ruofan Liang", "Kai He", "Zan Gojcic", "Igor Gilitschenski", "Sanja Fidler", "Nandita Vijaykumar", "Zian Wang"], "title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/LuxDiT/", "summary": "Estimating scene lighting from a single image or video remains a longstanding\nchallenge in computer vision and graphics. Learning-based approaches are\nconstrained by the scarcity of ground-truth HDR environment maps, which are\nexpensive to capture and limited in diversity. While recent generative models\noffer strong priors for image synthesis, lighting estimation remains difficult\ndue to its reliance on indirect visual cues, the need to infer global\n(non-local) context, and the recovery of high-dynamic-range outputs. We propose\nLuxDiT, a novel data-driven approach that fine-tunes a video diffusion\ntransformer to generate HDR environment maps conditioned on visual input.\nTrained on a large synthetic dataset with diverse lighting conditions, our\nmodel learns to infer illumination from indirect visual cues and generalizes\neffectively to real-world scenes. To improve semantic alignment between the\ninput and the predicted environment map, we introduce a low-rank adaptation\nfinetuning strategy using a collected dataset of HDR panoramas. Our method\nproduces accurate lighting predictions with realistic angular high-frequency\ndetails, outperforming existing state-of-the-art techniques in both\nquantitative and qualitative evaluations.", "AI": {"tldr": "LuxDiT\u901a\u8fc7\u5fae\u8c03\u89c6\u9891\u6269\u6563Transformer\u751f\u6210HDR\u73af\u5883\u5149\u8d34\u56fe\uff0c\u89e3\u51b3\u4e86\u4ece\u5355\u56fe\u50cf\u6216\u89c6\u9891\u4f30\u8ba1\u5149\u7167\u7684\u96be\u9898\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u4ece\u5355\u56fe\u50cf\u6216\u89c6\u9891\u4f30\u8ba1\u5149\u7167\u7684\u6311\u6218\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u591a\u6837\u5316\u548c\u6602\u8d35\u7684\u771f\u5b9eHDR\u6570\u636e\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u89c6\u9891\u6269\u6563Transformer\u8fdb\u884c\u5fae\u8c03\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u96c6\u548c\u4f4e\u79e9\u9002\u5e94\u7b56\u7565\uff0c\u9884\u6d4bHDR\u73af\u5883\u5149\u8d34\u56fe\u3002", "result": "LuxDiT\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u751f\u6210\u7684\u5149\u7167\u9884\u6d4b\u5177\u6709\u771f\u5b9e\u7684\u9ad8\u9891\u7ec6\u8282\u3002", "conclusion": "LuxDiT\u5c55\u793a\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5149\u7167\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u5b66\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03565", "pdf": "https://arxiv.org/pdf/2509.03565", "abs": "https://arxiv.org/abs/2509.03565", "authors": ["Qi Chen", "Jingxuan Wei", "Zhuoya Yao", "Haiguang Wang", "Gaowei Wu", "Bihui Yu", "Siyuan Li", "Cheng Tan"], "title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference", "categories": ["cs.CL", "cs.MM"], "comment": "Accepted to ACM MM 2025", "summary": "Understanding how scientific ideas evolve requires more than summarizing\nindividual papers-it demands structured, cross-document reasoning over\nthematically related research. In this work, we formalize multi-document\nscientific inference, a new task that extracts and aligns motivation,\nmethodology, and experimental results across related papers to reconstruct\nresearch development chains. This task introduces key challenges, including\ntemporally aligning loosely structured methods and standardizing heterogeneous\nexperimental tables. We present ResearchPulse, an agent-based framework that\nintegrates instruction planning, scientific content extraction, and structured\nvisualization. It consists of three coordinated agents: a Plan Agent for task\ndecomposition, a Mmap-Agent that constructs motivation-method mind maps, and a\nLchart-Agent that synthesizes experimental line charts. To support this task,\nwe introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper\nclusters. Experiments show that our system, despite using 7B-scale agents,\nconsistently outperforms strong baselines like GPT-4o in semantic alignment,\nstructural consistency, and visual fidelity. The dataset are available in\nhttps://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.", "AI": {"tldr": "\u63d0\u51faResearchPulse\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u7cfb\u7edf\u5b9e\u73b0\u8de8\u6587\u6863\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\uff0c\u63d0\u53d6\u5e76\u5bf9\u9f50\u76f8\u5173\u8bba\u6587\u4e2d\u7684\u52a8\u673a\u3001\u65b9\u6cd5\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f18\u4e8eGPT-4o\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u601d\u60f3\u7684\u6f14\u53d8\u9700\u8981\u901a\u8fc7\u8de8\u6587\u6863\u63a8\u7406\u6765\u7406\u89e3\u76f8\u5173\u7814\u7a76\u7684\u4e3b\u9898\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86ResearchPulse\u6846\u67b6\uff0c\u5305\u542b\u4efb\u52a1\u5206\u89e3\u7684Plan Agent\u3001\u6784\u5efa\u52a8\u673a-\u65b9\u6cd5\u601d\u7ef4\u5bfc\u56fe\u7684Mmap-Agent\u548c\u5408\u6210\u5b9e\u9a8c\u7ebf\u56fe\u7684Lchart-Agent\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u8bed\u4e49\u5bf9\u9f50\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u53ef\u89c6\u5316\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8eGPT-4o\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ResearchPulse\u4e3a\u79d1\u5b66\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u7814\u7a76\u53d1\u5c55\u94fe\u7684\u91cd\u5efa\u3002"}}
{"id": "2509.03667", "pdf": "https://arxiv.org/pdf/2509.03667", "abs": "https://arxiv.org/abs/2509.03667", "authors": ["Vivek Vasan", "Alexander Nico-Katz", "Boulat A. Bash", "Daniel C. Kilper", "Marco Ruffini"], "title": "Entanglement Purification With Finite Latency Classical Communication in Quantum Networks", "categories": ["cs.NI"], "comment": null, "summary": "Quantum networks rely on high fidelity entangled pairs distributed to nodes,\nbut maintaining their fidelity is challenged by environmental decoherence\nduring storage. Entanglement purification is used to restore fidelity, but the\nidle periods imposed by the associated classical communication delays\ncounteract this goal by exposing the states to further decoherence. In this\nwork, we analyze the practical viability of entanglement purification protocols\n(BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet\nprotocol (IP) communications networks. We present a comprehensive performance\nevaluation of these protocols in various network conditions for a range of\nquantum memory technologies. We employ a microscopic Lindblad treatment of the\nunderlying quantum dynamics, and use current-generation metropolitan IP network\nlatency statistics and parameters drawn from quantum memory testbeds. In doing\nso we identify the regions in which entanglement purification succeeds and\nfails, delineated by break-even iso-fidelity contours in the phase space. We\nthen determine the total number of entangled pairs required to complete a\nmulti-round purification protocol, and the steady-state throughput of entangled\npairs with purified fidelities that exceed application-specific thresholds.\nThis provides latency budgets, memory quality targets, and resource-overhead\nestimates for deploying purification on current and near-future networks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728IP\u901a\u4fe1\u7f51\u7edc\u4e2d\u975e\u5373\u65f6\u7ecf\u5178\u534f\u8c03\u4e0b\uff0c\u7ea0\u7f20\u7eaf\u5316\u534f\u8bae\uff08BBPSSW\u3001DEJMPS\uff09\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u548c\u91cf\u5b50\u5b58\u50a8\u6280\u672f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u91cf\u5b50\u7f51\u7edc\u4e2d\u7ea0\u7f20\u5bf9\u5728\u5b58\u50a8\u671f\u95f4\u7531\u4e8e\u73af\u5883\u9000\u76f8\u5e72\u5bfc\u81f4\u4fdd\u771f\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u7ecf\u5178\u901a\u4fe1\u5ef6\u8fdf\u5e26\u6765\u7684\u989d\u5916\u9000\u76f8\u5e72\u6311\u6218\u3002", "method": "\u91c7\u7528\u5fae\u89c2Lindblad\u65b9\u6cd5\u5206\u6790\u91cf\u5b50\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u5f53\u524d\u57ce\u57dfIP\u7f51\u7edc\u5ef6\u8fdf\u7edf\u8ba1\u548c\u91cf\u5b50\u5b58\u50a8\u6d4b\u8bd5\u5e73\u53f0\u53c2\u6570\uff0c\u8bc4\u4f30\u534f\u8bae\u6027\u80fd\uff0c\u5e76\u786e\u5b9a\u6210\u529f\u4e0e\u5931\u8d25\u7684\u533a\u57df\u3002", "result": "\u786e\u5b9a\u4e86\u7ea0\u7f20\u7eaf\u5316\u6210\u529f\u7684\u4e34\u754c\u4fdd\u771f\u5ea6\u7b49\u503c\u7ebf\uff0c\u8ba1\u7b97\u4e86\u5b8c\u6210\u591a\u8f6e\u7eaf\u5316\u6240\u9700\u7684\u7ea0\u7f20\u5bf9\u6570\u91cf\uff0c\u4ee5\u53ca\u6ee1\u8db3\u5e94\u7528\u7279\u5b9a\u9608\u503c\u7684\u9ad8\u4fdd\u771f\u7ea0\u7f20\u5bf9\u7684\u7a33\u6001\u541e\u5410\u91cf\u3002", "conclusion": "\u4e3a\u5728\u5f53\u524d\u548c\u672a\u6765\u7f51\u7edc\u4e2d\u90e8\u7f72\u7ea0\u7f20\u7eaf\u5316\u63d0\u4f9b\u4e86\u5ef6\u8fdf\u9884\u7b97\u3001\u5b58\u50a8\u8d28\u91cf\u76ee\u6807\u548c\u8d44\u6e90\u5f00\u9500\u4f30\u8ba1\u3002"}}
{"id": "2509.03560", "pdf": "https://arxiv.org/pdf/2509.03560", "abs": "https://arxiv.org/abs/2509.03560", "authors": ["Atanu Kundu", "Pratyay Sarkar", "Rajarshi Ray"], "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine Hybrid Systems", "categories": ["cs.LO"], "comment": null, "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCEGAR\u7684\u7ec4\u5408\u6df7\u5408\u7cfb\u7edf\u6709\u754c\u53ef\u8fbe\u6027\u5206\u6790\u7b97\u6cd5\uff0c\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u4ea7\u54c1\u81ea\u52a8\u673a\uff0c\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7ec4\u5408\u6df7\u5408\u7cfb\u7edf\u53ef\u8fbe\u6027\u5206\u6790\u4e2d\u7684\u8bed\u4e49\u4fdd\u7559\u548c\u72b6\u6001\u7a7a\u95f4\u7206\u70b8\u95ee\u9898\u3002", "method": "\u7ed3\u5408CEGAR\u539f\u5219\uff0c\u5728\u79bb\u6563\u62bd\u8c61\u4e2d\u641c\u7d22\u53cd\u4f8b\u5e76\u901a\u8fc7\u7b26\u53f7\u53ef\u8fbe\u6027\u5206\u6790\u9a8c\u8bc1\uff1b\u4f18\u5316\u5305\u62ec\u7f13\u5b58\u7ed3\u679c\u548c\u6df7\u5408\u4e0d\u540c\u7ec4\u5408\u8bed\u4e49\u3002", "result": "\u5728\u5de5\u5177SAT-Reach\u4e2d\u5b9e\u73b0\u4e86\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u7ec4\u5408\u6df7\u5408\u7cfb\u7edf\u7684\u53ef\u8fbe\u6027\u5206\u6790\u3002"}}
{"id": "2509.03678", "pdf": "https://arxiv.org/pdf/2509.03678", "abs": "https://arxiv.org/abs/2509.03678", "authors": ["Xianghan Wang", "Chingshuan Hsiao", "Shimei Qiu"], "title": "Promisedland: An XR Narrative Attraction Integrating Diorama-to-Virtual Workflow and Elemental Storytelling", "categories": ["cs.HC", "cs.MM"], "comment": "Accepted to the Proceedings of the 2025 11th International Conference\n  on Virtual Reality (ICVR 2025). ISBN: 979-8-3503-9272-2. \\c{opyright} 2025\n  IEEE. This is the author-accepted manuscript. The final version will be\n  available via IEEE Xplore", "summary": "Promisedland is a mixed-reality (MR) narrative attraction that combines\ncultural storytelling, ecological education, and an innovative hybrid\nproduction workflow. Set in a future Earth suffering from elemental imbalance,\nusers embark on an interactive journey guided by symbolic characters to restore\nharmony through the collection of five classical elements: metal, wood, water,\nfire, and earth. To prototype this experience, we introduce a low-cost,\nhigh-fidelity Diorama-to-Virtual pipeline - handcrafting physical scale models,\n3D scanning, and integrating them into Unreal Engine. This process enables\nrapid spatial prototyping while preserving the material expressiveness and\nnarrative consistency of the physical environment. To further enhance\nimmersion, the experience incorporates a Stewart Platform to provide motion\nfeedback synchronized with the virtual ride dynamics, reinforcing spatial\npresence and embodied engagement. The final prototype runs on Meta Quest,\nsupporting dynamic interactions and real-time visual feedback. Promisedland\noffers a replicable design blueprint for future XR narrative installations\nacross museums, cultural exhibitions, and themed entertainment. It proposes a\nnew framework for XR Narrative Attractions - where physical and digital\nelements converge to deepen immersion, agency, and emotional engagement.", "AI": {"tldr": "Promisedland\u662f\u4e00\u4e2a\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u53d9\u4e8b\u4f53\u9a8c\u9879\u76ee\uff0c\u7ed3\u5408\u4e86\u6587\u5316\u53d9\u4e8b\u3001\u751f\u6001\u6559\u80b2\u53ca\u521b\u65b0\u7684\u6df7\u5408\u751f\u4ea7\u6d41\u7a0b\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u65c5\u7a0b\u6062\u590d\u5143\u7d20\u5e73\u8861\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6df7\u5408\u73b0\u5b9e\u6280\u672f\u63d0\u4f9b\u4e00\u4e2a\u6c89\u6d78\u5f0f\u7684\u53d9\u4e8b\u4f53\u9a8c\uff0c\u7ed3\u5408\u6587\u5316\u548c\u751f\u6001\u6559\u80b2\uff0c\u540c\u65f6\u5c55\u793a\u4f4e\u6210\u672c\u3001\u9ad8\u4fdd\u771f\u7684\u539f\u578b\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528Diorama-to-Virtual\u6d41\u7a0b\uff0c\u5305\u62ec\u624b\u5de5\u5236\u4f5c\u7269\u7406\u6a21\u578b\u30013D\u626b\u63cf\u540e\u96c6\u6210\u5230Unreal Engine\uff0c\u7ed3\u5408Stewart\u5e73\u53f0\u63d0\u4f9b\u8fd0\u52a8\u53cd\u9988\u3002", "result": "\u6700\u7ec8\u539f\u578b\u5728Meta Quest\u4e0a\u8fd0\u884c\uff0c\u652f\u6301\u52a8\u6001\u4ea4\u4e92\u548c\u5b9e\u65f6\u89c6\u89c9\u53cd\u9988\uff0c\u63d0\u5347\u4e86\u6c89\u6d78\u611f\u548c\u60c5\u611f\u53c2\u4e0e\u5ea6\u3002", "conclusion": "Promisedland\u4e3a\u672a\u6765XR\u53d9\u4e8b\u88c5\u7f6e\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u8bbe\u8ba1\u84dd\u56fe\uff0c\u63d0\u51fa\u4e86\u7269\u7406\u4e0e\u6570\u5b57\u5143\u7d20\u878d\u5408\u7684\u65b0\u6846\u67b6\u3002"}}
{"id": "2509.04085", "pdf": "https://arxiv.org/pdf/2509.04085", "abs": "https://arxiv.org/abs/2509.04085", "authors": ["Stanly Wilson", "Kwabena Adu-Duodu", "Yinhao Li", "Ringo Sham", "Yingli Wang", "Ellis Solaiman", "Charith Perera", "Rajiv Ranjan", "Omer Rana"], "title": "Trustworthy Second-hand Marketplace for Built Environment", "categories": ["cs.DC", "cs.ET"], "comment": null, "summary": "The construction industry faces significant challenges regarding material\nwaste and sustainable practices, necessitating innovative solutions that\nintegrate automation, traceability, and decentralised decision-making to enable\nefficient material reuse. This paper presents a blockchain-enabled digital\nmarketplace for sustainable construction material reuse, ensuring transparency\nand traceability using InterPlanetary File System (IPFS). The proposed\nframework enhances trust and accountability in material exchange, addressing\nkey challenges in industrial automation and circular supply chains. A framework\nhas been developed to demonstrate the operational processes of the marketplace,\nillustrating its practical application and effectiveness. Our contributions\nshow how the marketplace can facilitate the efficient and trustworthy exchange\nof reusable materials, representing a substantial step towards more sustainable\nconstruction practices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u6570\u5b57\u5e02\u573a\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u900f\u660e\u548c\u53ef\u8ffd\u6eaf\u7684\u673a\u5236\u4fc3\u8fdb\u53ef\u6301\u7eed\u5efa\u7b51\u6750\u6599\u7684\u518d\u5229\u7528\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u5faa\u73af\u4f9b\u5e94\u94fe\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u5728\u6750\u6599\u6d6a\u8d39\u548c\u53ef\u6301\u7eed\u5b9e\u8df5\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6574\u5408\u81ea\u52a8\u5316\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u5206\u6563\u51b3\u7b56\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6750\u6599\u518d\u5229\u7528\u3002", "method": "\u91c7\u7528\u533a\u5757\u94fe\u6280\u672f\u548cIPFS\uff08\u661f\u9645\u6587\u4ef6\u7cfb\u7edf\uff09\u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u5b57\u5e02\u573a\u6846\u67b6\uff0c\u786e\u4fdd\u6750\u6599\u4ea4\u6362\u7684\u900f\u660e\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "result": "\u5f00\u53d1\u7684\u6846\u67b6\u5c55\u793a\u4e86\u5e02\u573a\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u4e14\u53ef\u4fe1\u5730\u4fc3\u8fdb\u53ef\u518d\u5229\u7528\u6750\u6599\u7684\u4ea4\u6362\u3002", "conclusion": "\u8be5\u6570\u5b57\u5e02\u573a\u6846\u67b6\u4e3a\u66f4\u53ef\u6301\u7eed\u7684\u5efa\u7b51\u5b9e\u8df5\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\uff0c\u6539\u5584\u4e86\u6750\u6599\u518d\u5229\u7528\u7684\u6548\u7387\u4e0e\u4fe1\u4efb\u95ee\u9898\u3002"}}
{"id": "2509.03653", "pdf": "https://arxiv.org/pdf/2509.03653", "abs": "https://arxiv.org/abs/2509.03653", "authors": ["Siddharth Samsi", "Dan Campbell", "Emanuel Scoullos", "Oded Green"], "title": "Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software", "categories": ["cs.DC"], "comment": null, "summary": "The HPEC Graph Challenge is a collection of benchmarks representing complex\nworkloads that test the hardware and software components of HPC systems, which\ntraditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph\nIsomorphism, focused on several compute-bound and memory-bound kernels. The\nmost recent of the challenges, the Anonymized Network Sensing Graph Challenge,\nrepresents a shift in direction, as it represents a longer end-to-end workload\nthat requires many more software components, including, but not limited to,\ndata I/O, data structures for representing graph data, and a wide range of\nfunctions for data preparation and network analysis. A notable feature of this\nnew graph challenge is the use of GraphBLAS to represent the computational\naspects of the problem statement. In this paper, we show an alternative\ninterpretation of the GraphBLAS formulations using the language of data\nscience. With this formulation, we show that the new graph challenge can be\nimplemented using off-the-shelf ETL tools available in open-source, enterprise\nsoftware such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software,\nRAPIDS cuDF and cupy, we enable significant software acceleration without\nrequiring any specific HPC code and show speedups, over the same code running\nwith Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an\nNVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.", "AI": {"tldr": "HPEC Graph Challenge\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u5229\u7528GraphBLAS\u548c\u6570\u636e\u79d1\u5b66\u8bed\u8a00\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\uff08\u5982NVIDIA RAPIDS\uff09\u5b9e\u73b0\u9ad8\u6548\u52a0\u901f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfHPC\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982LINPACK\uff09\u65e0\u6cd5\u8986\u76d6\u590d\u6742\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u65b0\u57fa\u51c6\u6d4b\u8bd5Anonymized Network Sensing Graph Challenge\u9700\u8981\u66f4\u591a\u8f6f\u4ef6\u7ec4\u4ef6\u548c\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u3002", "method": "\u4f7f\u7528GraphBLAS\u548c\u6570\u636e\u79d1\u5b66\u8bed\u8a00\u91cd\u65b0\u89e3\u91ca\u95ee\u9898\uff0c\u5229\u7528NVIDIA RAPIDS\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u73b0\u6210ETL\u5de5\u5177\uff08\u5982cuDF\u548ccupy\uff09\u5b9e\u73b0\u52a0\u901f\u3002", "result": "\u5728NVIDIA A100\u3001H100\u548cH200 GPU\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86147x-509x\u3001243x-1269x\u548c332x-2185x\u7684\u52a0\u901f\u3002", "conclusion": "\u5f00\u6e90\u4f01\u4e1a\u5de5\u5177\u80fd\u5728\u65e0\u9700\u4e13\u7528HPC\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u65b0\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04423", "pdf": "https://arxiv.org/pdf/2509.04423", "abs": "https://arxiv.org/abs/2509.04423", "authors": ["Fatima Zulfiqar Ali", "Atrooba Ilyas"], "title": "Design and Development of a Web Platform for Blood Donation Management", "categories": ["cs.SE", "cs.DB"], "comment": "10 pages, 6 figures, conference", "summary": "Blood donation is a critical component of healthcare, yet locating suitable\ndonors in emergencies often presents significant challenges. This paper\npresents the design and development of a Blood Donation Web Platform, a\nweb-based system that connects patients, donors, and administrators within a\ncentralized digital space. The platform allows interested donors to register\ntheir personal information, including blood group, contact details, and\navailability. Patients can search for donors based on blood group and location,\nand the system provides a list of nearby donors who are ready to donate. The\nplatform design was guided by use case, database, class, and sequence diagrams\nto ensure a well-structured and efficient system architecture. Modern web\ntechnologies, including PHP (Laravel framework), HTML, CSS, Bootstrap, and\nMySQL, supported by XAMPP and Visual Studio Code, were employed to implement a\ndynamic, interactive, and user-friendly platform. By streamlining donor\nrefgistration, blood requests, and communication, the proposed system reduces\ndelays and complexities in emergencies, improving timely accessibility of blood\nand enhancing overall efficiency in blood donation services.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u732e\u8840\u5e73\u53f0\uff0c\u901a\u8fc7\u96c6\u4e2d\u5316\u7ba1\u7406\u8fde\u63a5\u60a3\u8005\u3001\u732e\u8840\u8005\u548c\u7ba1\u7406\u5458\uff0c\u4f18\u5316\u4e86\u732e\u8840\u670d\u52a1\u7684\u6548\u7387\u548c\u53ca\u65f6\u6027\u3002", "motivation": "\u89e3\u51b3\u7d27\u6025\u60c5\u51b5\u4e0b\u5bfb\u627e\u5408\u9002\u732e\u8840\u8005\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u732e\u8840\u670d\u52a1\u7684\u6548\u7387\u548c\u53ef\u53ca\u6027\u3002", "method": "\u4f7f\u7528PHP (Laravel\u6846\u67b6)\u3001HTML\u3001CSS\u3001Bootstrap\u548cMySQL\u7b49\u6280\u672f\uff0c\u901a\u8fc7\u7528\u4f8b\u3001\u6570\u636e\u5e93\u3001\u7c7b\u548c\u5e8f\u5217\u56fe\u8bbe\u8ba1\u5f00\u53d1\u5e73\u53f0\u3002", "result": "\u5e73\u53f0\u5b9e\u73b0\u4e86\u732e\u8840\u8005\u7684\u6ce8\u518c\u3001\u60a3\u8005\u7684\u641c\u7d22\u529f\u80fd\uff0c\u51cf\u5c11\u4e86\u7d27\u6025\u60c5\u51b5\u4e0b\u7684\u5ef6\u8bef\u548c\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u5e73\u53f0\u901a\u8fc7\u6570\u5b57\u5316\u7ba1\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u732e\u8840\u670d\u52a1\u7684\u6548\u7387\u548c\u53ca\u65f6\u6027\u3002"}}
{"id": "2509.03846", "pdf": "https://arxiv.org/pdf/2509.03846", "abs": "https://arxiv.org/abs/2509.03846", "authors": ["Md Rownak Hossain Chowdhury", "Mostafizur Rahman"], "title": "Hardware-Aware Data and Instruction Mapping for AI Tasks: Balancing Parallelism, I/O and Memory Tradeoffs", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "We introduce a mapping framework for deep learning inference that takes\nadvantage of predictable neural network behavior to plan both computation and\ncommunication ahead of time. The framework generates a unified stream of\ninstructions and data, enabling the hardware to execute operations and route\ninformation on its own, without frequent involvement from the host and with\nminimal off-chip memory use. This naturally reduces reliance on I/O, off-chip\nmemory, and host control. By leveraging fine-grained message passing on a\nprogrammable, message-based compute architecture, the framework keeps data\nmovement local and coordinates computation across the array using techniques\nsuch as stationary-weight reuse, in-array multicasting, and staged reductions.\nApplied to VGG-19, the framework sustains high utilization (88 to 92 percent),\nwith over 97 percent of messages generated internally and nearly 89 percent of\ntime consumed on-chip transfers. Computation throughput scales beyond 1 TFLOP/s\non larger arrays, while traffic reductions from reuse and local aggregation\nreach up to 100 MB per layer. Overall, the results highlight the effectiveness\nof streaming-based computation and show how our mapper enables this execution\nstyle by tightly coordinating data and instruction flow across the hardware.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u6620\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u884c\u4e3a\u63d0\u524d\u89c4\u5212\u8ba1\u7b97\u548c\u901a\u4fe1\uff0c\u51cf\u5c11\u4e3b\u673a\u5e72\u9884\u548c\u7247\u5916\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u4e2d\u5bf9\u4e3b\u673a\u63a7\u5236\u3001I/O\u548c\u7247\u5916\u5185\u5b58\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u9884\u6d4b\u884c\u4e3a\u63d0\u524d\u89c4\u5212\u7684\u6846\u67b6\u3002", "method": "\u6846\u67b6\u751f\u6210\u7edf\u4e00\u6307\u4ee4\u548c\u6570\u636e\u6d41\uff0c\u5229\u7528\u6d88\u606f\u4f20\u9012\u67b6\u6784\u5b9e\u73b0\u6570\u636e\u5c40\u90e8\u6027\u548c\u8ba1\u7b97\u534f\u8c03\uff0c\u5982\u6743\u91cd\u590d\u7528\u548c\u591a\u64ad\u6280\u672f\u3002", "result": "\u5728VGG-19\u4e0a\uff0c\u6846\u67b6\u4fdd\u630188-92%\u7684\u9ad8\u5229\u7528\u7387\uff0c97%\u7684\u6d88\u606f\u5185\u90e8\u751f\u6210\uff0c\u8ba1\u7b97\u541e\u5410\u91cf\u8d851TFLOP/s\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6d41\u5f0f\u8ba1\u7b97\u548c\u7d27\u5bc6\u534f\u8c03\u6570\u636e\u4e0e\u6307\u4ee4\u6d41\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u6267\u884c\u6548\u7387\u3002"}}
{"id": "2509.03875", "pdf": "https://arxiv.org/pdf/2509.03875", "abs": "https://arxiv.org/abs/2509.03875", "authors": ["Ziyou Jiang", "Mingyang Li", "Guowei Yang", "Lin Shi", "Qing Wang"], "title": "VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from Rich-Text Issue Report", "categories": ["cs.SE"], "comment": "25 pages, 7 figures, submitting to TOSEM journal", "summary": "Software vulnerabilities exist in open-source software (OSS), and the\ndevelopers who discover these vulnerabilities may submit issue reports (IRs) to\ndescribe their details. Security practitioners need to spend a lot of time\nmanually identifying vulnerability-related IRs from the community, and the time\ngap may be exploited by attackers to harm the system. Previously, researchers\nhave proposed automatic approaches to facilitate identifying these\nvulnerability-related IRs, but these works focus on textual descriptions but\nlack the comprehensive analysis of IR's rich-text information. In this paper,\nwe propose VulRTex, a reasoning-guided approach to identify\nvulnerability-related IRs with their rich-text information. In particular,\nVulRTex first utilizes the reasoning ability of the Large Language Model (LLM)\nto prepare the Vulnerability Reasoning Database with historical IRs. Then, it\nretrieves the relevant cases from the prepared reasoning database to generate\nreasoning guidance, which guides LLM to identify vulnerabilities by reasoning\nanalysis on target IRs' rich-text information. To evaluate the performance of\nVulRTex, we conduct experiments on 973,572 IRs, and the results show that\nVulRTex achieves the highest performance in identifying the\nvulnerability-related IRs and predicting CWE-IDs when the dataset is\nimbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and\n+10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches.\nFurthermore, VulRTex has been applied to identify 30 emerging vulnerabilities\nacross 10 representative OSS projects in 2024's GitHub IRs, and 11 of them are\nsuccessfully assigned CVE-IDs, which illustrates VulRTex's practicality.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u63d0\u51faVulRTex\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\u7684\u63a8\u7406\u6307\u5bfc\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u4e2d\u7684\u6f0f\u6d1e\u76f8\u5173\u62a5\u544a\uff08IR\uff09\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u65f6\u95f4\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u5b89\u5168\u4ece\u4e1a\u8005\u9700\u624b\u52a8\u7b5b\u9009\u6f0f\u6d1e\u76f8\u5173IR\uff0c\u6548\u7387\u4f4e\u4e14\u53ef\u80fd\u88ab\u653b\u51fb\u8005\u5229\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6587\u672c\u63cf\u8ff0\uff0c\u5ffd\u89c6IR\u7684\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\u3002", "method": "VulRTex\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u6784\u5efa\u6f0f\u6d1e\u63a8\u7406\u6570\u636e\u5e93\uff0c\u5e76\u901a\u8fc7\u5386\u53f2\u6848\u4f8b\u751f\u6210\u63a8\u7406\u6307\u5bfc\uff0c\u7ed3\u5408\u76ee\u6807IR\u7684\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u63a8\u7406\u5206\u6790\u3002", "result": "\u5728973,572\u4e2aIR\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cVulRTex\u5728\u8bc6\u522b\u6f0f\u6d1e\u76f8\u5173IR\u548c\u9884\u6d4bCWE-ID\u4e0a\u6027\u80fd\u6700\u4f73\uff0cF1\u63d0\u534711.0%\uff0cAUPRC\u63d0\u534720.2%\uff0c\u65f6\u95f4\u6210\u672c\u964d\u4f4e50%\u3002", "conclusion": "VulRTex\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6210\u529f\u8bc6\u522b30\u4e2a\u65b0\u5174\u6f0f\u6d1e\uff0c\u5176\u4e2d11\u4e2a\u88ab\u5206\u914dCVE-ID\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.03753", "pdf": "https://arxiv.org/pdf/2509.03753", "abs": "https://arxiv.org/abs/2509.03753", "authors": ["Michael Greer"], "title": "Memory Optimization for Convex Hull Support Point Queries", "categories": ["cs.GR", "cs.CG", "cs.RO", "68U05", "I.3.5"], "comment": "6 pages, 15 figures", "summary": "This paper evaluates several improvements to the memory layout of convex\nhulls to improve computation times for support point queries. The support point\nquery is a fundamental part of common collision algorithms, and the work\npresented achieves a significant speedup depending on the number of vertices of\nthe convex hull.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u51e0\u79cd\u51f8\u5305\u5185\u5b58\u5e03\u5c40\u6539\u8fdb\u65b9\u6848\uff0c\u4ee5\u63d0\u5347\u652f\u6301\u70b9\u67e5\u8be2\u7684\u8ba1\u7b97\u901f\u5ea6\u3002", "motivation": "\u652f\u6301\u70b9\u67e5\u8be2\u662f\u5e38\u89c1\u78b0\u649e\u7b97\u6cd5\u7684\u6838\u5fc3\u90e8\u5206\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u5e03\u5c40\u6765\u63d0\u9ad8\u5176\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u51e0\u79cd\u51f8\u5305\u5185\u5b58\u5e03\u5c40\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u4ee5\u4f18\u5316\u652f\u6301\u70b9\u67e5\u8be2\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6539\u8fdb\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\uff0c\u5177\u4f53\u6548\u679c\u53d6\u51b3\u4e8e\u51f8\u5305\u7684\u9876\u70b9\u6570\u91cf\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u51f8\u5305\u7684\u5185\u5b58\u5e03\u5c40\uff0c\u53ef\u4ee5\u6709\u6548\u52a0\u901f\u652f\u6301\u70b9\u67e5\u8be2\uff0c\u4ece\u800c\u63d0\u5347\u78b0\u649e\u7b97\u6cd5\u7684\u6574\u4f53\u6548\u7387\u3002"}}
{"id": "2509.03762", "pdf": "https://arxiv.org/pdf/2509.03762", "abs": "https://arxiv.org/abs/2509.03762", "authors": ["Sathwik Chadaga", "Eytan Modiano"], "title": "Drift Plus Optimistic Penalty -- A Learning Framework for Stochastic Network Optimization", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "We consider the problem of joint routing and scheduling in queueing networks,\nwhere the edge transmission costs are unknown. At each time-slot, the network\ncontroller receives noisy observations of transmission costs only for those\nedges it selects for transmission. The network controller's objective is to\nmake routing and scheduling decisions so that the total expected cost is\nminimized. This problem exhibits an exploration-exploitation trade-off,\nhowever, previous bandit-style solutions cannot be directly applied to this\nproblem due to the queueing dynamics. In order to ensure network stability, the\nnetwork controller needs to optimize throughput and cost simultaneously. We\nshow that the best achievable cost is lower bounded by the solution to a static\noptimization problem, and develop a network control policy using techniques\nfrom Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show\nthat the policy achieves a sub-linear regret of order $O(\\sqrt{T}\\log T)$, as\ncompared to the best policy that has complete knowledge of arrivals and costs.\nFinally, we evaluate the proposed policy using simulations and show that its\nregret is indeed sub-linear.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u961f\u5217\u7f51\u7edc\u4e2d\u8def\u7531\u548c\u8c03\u5ea6\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5176\u4e2d\u8fb9\u7f18\u4f20\u8f93\u6210\u672c\u672a\u77e5\u3002\u63a7\u5236\u5668\u9700\u901a\u8fc7\u566a\u58f0\u89c2\u6d4b\u52a8\u6001\u8c03\u6574\u51b3\u7b56\uff0c\u4ee5\u6700\u5c0f\u5316\u603b\u671f\u671b\u6210\u672c\uff0c\u5e76\u786e\u4fdd\u7f51\u7edc\u7a33\u5b9a\u6027\u3002", "motivation": "\u5728\u961f\u5217\u7f51\u7edc\u4e2d\uff0c\u672a\u77e5\u7684\u8fb9\u7f18\u4f20\u8f93\u6210\u672c\u548c\u7f51\u7edc\u7a33\u5b9a\u6027\u9700\u6c42\u589e\u52a0\u4e86\u8def\u7531\u548c\u8c03\u5ea6\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u540c\u65f6\u4f18\u5316\u541e\u5410\u91cf\u548c\u6210\u672c\u3002", "method": "\u7ed3\u5408Lyapunov\u6f02\u79fb\u52a0\u60e9\u7f5a\u4f18\u5316\u548c\u591a\u81c2\u8001\u864e\u673a\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u3002", "result": "\u7b56\u7565\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8e\u5b8c\u5168\u4e86\u89e3\u5230\u8fbe\u548c\u6210\u672c\u7684\u6700\u4f73\u7b56\u7565\u7684$O(\\sqrt{T}\\log T)$\u6b21\u7ebf\u6027\u9057\u61be\u3002", "conclusion": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5176\u9057\u61be\u786e\u5b9e\u4e3a\u6b21\u7ebf\u6027\u3002"}}
{"id": "2509.04129", "pdf": "https://arxiv.org/pdf/2509.04129", "abs": "https://arxiv.org/abs/2509.04129", "authors": ["Mickael Randour"], "title": "Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on Controllers in Reactive Synthesis", "categories": ["cs.LO", "cs.AI", "cs.FL", "math.PR"], "comment": "Invited paper at RP 2025", "summary": "In the game-theoretic approach to controller synthesis, we model the\ninteraction between a system to be controlled and its environment as a game\nbetween these entities, and we seek an appropriate (e.g., winning or optimal)\nstrategy for the system. This strategy then serves as a formal blueprint for a\nreal-world controller. A common belief is that simple (e.g., using limited\nmemory) strategies are better: corresponding controllers are easier to conceive\nand understand, and cheaper to produce and maintain.\n  This invited contribution focuses on the complexity of strategies in a\nvariety of synthesis contexts. We discuss recent results concerning memory and\nrandomness, and take a brief look at what lies beyond our traditional notions\nof complexity for strategies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u63a7\u5236\u5668\u5408\u6210\u4e2d\u7b56\u7565\u7684\u590d\u6742\u6027\uff0c\u8ba8\u8bba\u4e86\u4e0d\u540c\u5408\u6210\u73af\u5883\u4e0b\u8bb0\u5fc6\u548c\u968f\u673a\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u63a7\u5236\u5668\u5408\u6210\u4e2d\u7b80\u5355\u7b56\u7565\u7684\u4f18\u52bf\uff0c\u4ee5\u53ca\u7b56\u7565\u590d\u6742\u6027\u5bf9\u5b9e\u9645\u63a7\u5236\u5668\u8bbe\u8ba1\u548c\u7ef4\u62a4\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u5206\u6790\u7cfb\u7edf\u4e0e\u73af\u5883\u4ea4\u4e92\u65f6\u7684\u7b56\u7565\u590d\u6742\u6027\u95ee\u9898\u3002", "result": "\u603b\u7ed3\u4e86\u5173\u4e8e\u8bb0\u5fc6\u548c\u968f\u673a\u6027\u5728\u7b56\u7565\u4e2d\u4f5c\u7528\u7684\u6700\u65b0\u7814\u7a76\u6210\u679c\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5bf9\u7b56\u7565\u590d\u6742\u6027\u4f20\u7edf\u6982\u5ff5\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u9700\u6c42\u3002"}}
{"id": "2509.03693", "pdf": "https://arxiv.org/pdf/2509.03693", "abs": "https://arxiv.org/abs/2509.03693", "authors": ["Yeaeun Gong", "Yifan Liu", "Lanyu Shang", "Na Wei", "Dong Wang"], "title": "Designing Effective AI Explanations for Misinformation Detection: A Comparative Study of Content, Social, and Combined Explanations", "categories": ["cs.HC", "cs.MM"], "comment": "To appear at CSCW 2025", "summary": "In this paper, we study the problem of AI explanation of misinformation,\nwhere the goal is to identify explanation designs that help improve users'\nmisinformation detection abilities and their overall user experiences. Our work\nis motivated by the limitations of current Explainable AI (XAI) approaches,\nwhich predominantly focus on content explanations that elucidate the linguistic\nfeatures and sentence structures of the misinformation. To address this\nlimitation, we explore various explanations beyond content explanation, such as\n\"social explanation\" that considers the broader social context surrounding\nmisinformation, as well as a \"combined explanation\" where both the content and\nsocial explanations are presented in scenarios that are either aligned or\nmisaligned with each other. To evaluate the comparative effectiveness of these\nAI explanations, we conduct two online crowdsourcing experiments in the\nCOVID-19 (Study 1 on Prolific) and Politics domains (Study 2 on MTurk). Our\nresults show that AI explanations are generally effective in aiding users to\ndetect misinformation, with effectiveness significantly influenced by the\nalignment between content and social explanations. We also find that the order\nin which explanation types are presented - specifically, whether a content or\nsocial explanation comes first - can influence detection accuracy, with\ndifferences found between the COVID-19 and Political domains. This work\ncontributes towards more effective design of AI explanations, fostering a\ndeeper understanding of how different explanation types and their combinations\ninfluence misinformation detection.", "AI": {"tldr": "\u7814\u7a76AI\u89e3\u91ca\u5bf9\u7528\u6237\u8bc6\u522b\u865a\u5047\u4fe1\u606f\u7684\u5e2e\u52a9\uff0c\u63a2\u8ba8\u4e86\u5185\u5bb9\u89e3\u91ca\u548c\u793e\u4f1a\u89e3\u91ca\u7684\u6548\u679c\u53ca\u5176\u7ec4\u5408\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u8868\u660e\u89e3\u91ca\u7c7b\u578b\u548c\u987a\u5e8f\u5bf9\u68c0\u6d4b\u51c6\u786e\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91caAI\uff08XAI\uff09\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5185\u5bb9\u89e3\u91ca\uff0c\u5ffd\u7565\u4e86\u793e\u4f1a\u80cc\u666f\uff0c\u56e0\u6b64\u7814\u7a76\u5176\u4ed6\u89e3\u91ca\u5f62\u5f0f\uff08\u5982\u793e\u4f1a\u89e3\u91ca\u53ca\u5176\u7ec4\u5408\uff09\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u4f17\u5305\u5b9e\u9a8c\uff08COVID-19\u548c\u653f\u6cbb\u9886\u57df\uff09\u6bd4\u8f83\u5185\u5bb9\u89e3\u91ca\u3001\u793e\u4f1a\u89e3\u91ca\u53ca\u5176\u7ec4\u5408\u7684\u6548\u679c\u3002", "result": "AI\u89e3\u91ca\u80fd\u6709\u6548\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u865a\u5047\u4fe1\u606f\uff0c\u6548\u679c\u53d7\u5185\u5bb9\u4e0e\u793e\u4f1a\u89e3\u91ca\u7684\u4e00\u81f4\u6027\u53ca\u5448\u73b0\u987a\u5e8f\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684AI\u89e3\u91ca\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86\u793e\u4f1a\u80cc\u666f\u4e0e\u5185\u5bb9\u7ed3\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.04250", "pdf": "https://arxiv.org/pdf/2509.04250", "abs": "https://arxiv.org/abs/2509.04250", "authors": ["Shota Arai", "David Selby", "Andrew Vargo", "Sebastian Vollmer"], "title": "How many patients could we save with LLM priors?", "categories": ["stat.ME", "cs.AI", "cs.ET", "cs.IR", "stat.AP"], "comment": "9 pages, 4 figures", "summary": "Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u5148\u9a8c\u5206\u5e03\u7684\u5206\u5c42\u8d1d\u53f6\u65af\u5efa\u6a21\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u7684\u6570\u636e\u7edf\u8ba1\u6548\u7387\u3002", "motivation": "\u51cf\u5c11\u4e34\u5e8a\u8bd5\u9a8c\u6240\u9700\u60a3\u8005\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7edf\u8ba1\u6548\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u6539\u8fdb\u5b89\u5168\u6027\u8bc4\u4f30\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3LLM\u76f4\u63a5\u751f\u6210\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u8d85\u53c2\u6570\u5148\u9a8c\u5206\u5e03\uff0c\u53d6\u4ee3\u4f20\u7edf\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "LLM\u751f\u6210\u7684\u5148\u9a8c\u5206\u5e03\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u5143\u5206\u6790\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4e34\u5e8a\u8bd5\u9a8c\u6548\u7387\uff0c\u51cf\u5c11\u60a3\u8005\u6570\u91cf\uff0c\u5e76\u53ef\u80fd\u6539\u53d8\u836f\u7269\u5b89\u5168\u76d1\u6d4b\u548c\u76d1\u7ba1\u51b3\u7b56\u7684\u65b9\u5f0f\u3002"}}
{"id": "2509.03755", "pdf": "https://arxiv.org/pdf/2509.03755", "abs": "https://arxiv.org/abs/2509.03755", "authors": ["John Augustine", "Soumyottam Chatterjee", "Valerie King", "Manish Kumar", "Shachar Meir", "David Peleg"], "title": "Distributed Download from an External Data Source in Asynchronous Faulty Settings", "categories": ["cs.DC"], "comment": null, "summary": "The distributedData Retrieval (DR) model consists of $k$ peers connected by a\ncomplete peer-to-peer communication network, and a trusted external data source\nthat stores an array $\\textbf{X}$ of $n$ bits ($n \\gg k$). Up to $\\beta k$ of\nthe peers might fail in any execution (for $\\beta \\in [0, 1)$). Peers can\nobtain the information either by inexpensive messages passed among themselves\nor through expensive queries to the source array $\\textbf{X}$. In the DR model,\nwe focus on designing protocols that minimize the number of queries performed\nby any nonfaulty peer (a measure referred to as query complexity) while\nmaximizing the resilience parameter $\\beta$.\n  The Download problem requires each nonfaulty peer to correctly learn the\nentire array $\\textbf{X}$. Earlier work on this problem focused on synchronous\ncommunication networks and established several deterministic and randomized\nupper and lower bounds. Our work is the first to extend the study of\ndistributed data retrieval to asynchronous communication networks. We address\nthe Download problem under both the Byzantine and crash failure models. We\npresent query-optimal deterministic solutions in an asynchronous model that can\ntolerate any fixed fraction $\\beta<1$ of crash faults. In the Byzantine failure\nmodel, it is known that deterministic protocols incur a query complexity of\n$\\Omega(n)$ per peer, even under synchrony. We extend this lower bound to\nrandomized protocols in the asynchronous model for $\\beta \\geq 1/2$, and\nfurther show that for $\\beta < 1/2$, a randomized protocol exists with\nnear-optimal query complexity. To the best of our knowledge, this is the first\nwork to address the Download problem in asynchronous communication networks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5f02\u6b65\u901a\u4fe1\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u6570\u636e\u68c0\u7d22\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u5728\u5d29\u6e83\u548c\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\u4e0b\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u4f18\u5316\u3002", "motivation": "\u6269\u5c55\u5206\u5e03\u5f0f\u6570\u636e\u68c0\u7d22\u7684\u7814\u7a76\u81f3\u5f02\u6b65\u7f51\u7edc\uff0c\u89e3\u51b3\u73b0\u6709\u540c\u6b65\u7f51\u7edc\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u786e\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\u4ee5\u5bb9\u5fcd\u5d29\u6e83\u6545\u969c\uff0c\u5e76\u5728\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\u4e2d\u5206\u6790\u968f\u673a\u534f\u8bae\u7684\u67e5\u8be2\u590d\u6742\u5ea6\u3002", "result": "\u5728\u5d29\u6e83\u6545\u969c\u6a21\u578b\u4e2d\uff0c\u63d0\u51fa\u6700\u4f18\u67e5\u8be2\u590d\u6742\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff1b\u5728\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\u4e2d\uff0c\u5c55\u793a\u4e86\u968f\u673a\u534f\u8bae\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5728\u5f02\u6b65\u7f51\u7edc\u4e2d\u89e3\u51b3\u4e86\u6570\u636e\u68c0\u7d22\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u5206\u5e03\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.04153", "pdf": "https://arxiv.org/pdf/2509.04153", "abs": "https://arxiv.org/abs/2509.04153", "authors": ["Safa Mohammed Sali", "Mahmoud Meribout", "Ashiyana Abdul Majeed"], "title": "Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations", "categories": ["cs.AR"], "comment": null, "summary": "This paper presents a comprehensive review of recent advances in deploying\nconvolutional neural networks (CNNs) for object detection, classification, and\ntracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand\nfor real-time computer vision applications in domains such as autonomous\nvehicles, robotics, and surveillance, FPGAs have emerged as a powerful\nalternative to GPUs and ASICs due to their reconfigurability, low power\nconsumption, and deterministic latency. We critically examine state-of-the-art\nFPGA implementations of CNN-based vision tasks, covering algorithmic\ninnovations, hardware acceleration techniques, and the integration of\noptimization strategies like pruning, quantization, and sparsity-aware methods\nto maximize performance within hardware constraints. This survey also explores\nthe landscape of modern FPGA platforms, including classical LUT-DSP based\narchitectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration\nPlatforms (ACAPs), comparing their capabilities in handling deep learning\nworkloads. Furthermore, we review available software development tools such as\nVitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the\ndesign and deployment of AI models on FPGAs. The paper uniquely discusses\nhybrid architecture that combine GPUs and FPGAs for collaborative acceleration\nof AI inference, addressing challenges related to energy efficiency and\nthroughput. Additionally, we highlight hardware-software co-design practices,\ndataflow optimizations, and pipelined processing techniques essential for\nreal-time inference on resource-constrained devices. Through this survey,\nresearchers and engineers are equipped with insights to develop\nnext-generation, power-efficient, and high-performance vision systems optimized\nfor FPGA deployment in edge and embedded applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86FPGA\u4e0a\u90e8\u7f72CNN\u7528\u4e8e\u5b9e\u65f6\u89c6\u89c9\u4efb\u52a1\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u7b97\u6cd5\u3001\u786c\u4ef6\u52a0\u901f\u53ca\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u63a2\u8ba8\u4e86FPGA\u5e73\u53f0\u548c\u5f00\u53d1\u5de5\u5177\uff0c\u4e3a\u9ad8\u6548\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u7b49\u5b9e\u65f6\u89c6\u89c9\u5e94\u7528\u9700\u6c42\u7684\u589e\u957f\uff0cFPGA\u56e0\u5176\u4f4e\u529f\u8017\u548c\u786e\u5b9a\u6027\u5ef6\u8fdf\u6210\u4e3aGPU\u548cASIC\u7684\u6709\u529b\u66ff\u4ee3\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u7684\u6280\u672f\u603b\u7ed3\u3002", "method": "\u901a\u8fc7\u5206\u6790FPGA\u4e0a\u7684CNN\u5b9e\u73b0\u6280\u672f\uff08\u5982\u526a\u679d\u3001\u91cf\u5316\u7b49\uff09\u3001\u786c\u4ef6\u5e73\u53f0\uff08\u5982SoC FPGA\u3001ACAPs\uff09\u53ca\u5f00\u53d1\u5de5\u5177\uff08\u5982Vitis AI\uff09\uff0c\u5e76\u63a2\u8ba8\u6df7\u5408\u67b6\u6784\uff08GPU+FPGA\uff09\u548c\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u3002", "result": "\u603b\u7ed3\u4e86FPGA\u90e8\u7f72CNN\u7684\u5173\u952e\u6280\u672f\u4e0e\u4f18\u5316\u7b56\u7565\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u5b9e\u65f6\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u8fb9\u7f18\u5e94\u7528\u7684\u5f00\u53d1\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86FPGA\u90e8\u7f72\u9ad8\u6548\u89c6\u89c9\u7cfb\u7edf\u7684\u5168\u9762\u6307\u5357\uff0c\u63a8\u52a8\u4e0b\u4e00\u4ee3\u4f4e\u529f\u8017\u9ad8\u6027\u80fd\u8fb9\u7f18\u8ba1\u7b97\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.03876", "pdf": "https://arxiv.org/pdf/2509.03876", "abs": "https://arxiv.org/abs/2509.03876", "authors": ["Xingchu Chen", "Chengwei Liu", "Jialun Cao", "Yang Xiao", "Xinyue Cai", "Yeting Li", "Jingyi Shi", "Tianqi Sun", "Haiming Chen ang Wei Huo"], "title": "Vulnerability-Affected Versions Identification: How Far Are We?", "categories": ["cs.SE"], "comment": null, "summary": "Identifying which software versions are affected by a vulnerability is\ncritical for patching, risk mitigation.Despite a growing body of tools, their\nreal-world effectiveness remains unclear due to narrow evaluation scopes often\nlimited to early SZZ variants, outdated techniques, and small or\ncoarse-graineddatasets. In this paper, we present the first comprehensive\nempirical study of vulnerability affected versions identification. We curate a\nhigh quality benchmark of 1,128 real-world C/C++ vulnerabilities and\nsystematically evaluate 12 representative tools from both tracing and matching\nparadigms across four dimensions: effectiveness at both vulnerability and\nversion levels, root causes of false positives and negatives, sensitivity to\npatch characteristics, and ensemble potential. Our findings reveal fundamental\nlimitations: no tool exceeds 45.0% accuracy, with key challenges stemming from\nheuristic dependence, limited semantic reasoning, and rigid matching logic.\nPatch structures such as add-only and cross-file changes further hinder\nperformance. Although ensemble strategies can improve results by up to 10.1%,\noverall accuracy remains below 60.0%, highlighting the need for fundamentally\nnew approaches. Moreover, our study offers actionable insights to guide tool\ndevelopment, combination strategies, and future research in this critical area.\nFinally, we release the replicated code and benchmark on our website to\nencourage future contributions.outdated techniques, and small or coarse grained\ndatasets.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u7814\u7a76\u4e86\u6f0f\u6d1e\u5f71\u54cd\u7248\u672c\u7684\u8bc6\u522b\u95ee\u9898\uff0c\u8bc4\u4f30\u4e8612\u79cd\u5de5\u5177\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5176\u51c6\u786e\u7387\u666e\u904d\u4f4e\u4e8e45.0%\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u8bc6\u522b\u53d7\u6f0f\u6d1e\u5f71\u54cd\u7684\u8f6f\u4ef6\u7248\u672c\u5bf9\u4e8e\u8865\u4e01\u548c\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u5de5\u5177\u7684\u8bc4\u4f30\u8303\u56f4\u6709\u9650\uff0c\u4e9f\u9700\u66f4\u5168\u9762\u7684\u7814\u7a76\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6574\u74061128\u4e2aC/C++\u6f0f\u6d1e\u7684\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u4ece\u56db\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u8bc4\u4f3012\u79cd\u4ee3\u8868\u6027\u5de5\u5177\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5de5\u5177\u7684\u6700\u9ad8\u51c6\u786e\u7387\u4ec5\u4e3a45.0%\uff0c\u4e3b\u8981\u6311\u6218\u6765\u81ea\u542f\u53d1\u5f0f\u4f9d\u8d56\u3001\u8bed\u4e49\u63a8\u7406\u4e0d\u8db3\u548c\u5339\u914d\u903b\u8f91\u50f5\u5316\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u65b0\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u5de5\u5177\u5f00\u53d1\u548c\u7ec4\u5408\u7b56\u7565\u7684\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2509.03775", "pdf": "https://arxiv.org/pdf/2509.03775", "abs": "https://arxiv.org/abs/2509.03775", "authors": ["Sankeerth Durvasula", "Sharanshangar Muhunthan", "Zain Moustafa", "Richard Chen", "Ruofan Liang", "Yushi Guan", "Nilesh Ahuja", "Nilesh Jain", "Selvakumar Panneer", "Nandita Vijaykumar"], "title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world\nscenes with high quality and real-time rendering. Typically, a higher quality\nrepresentation can be achieved by using a large number of 3D Gaussians.\nHowever, using large 3D Gaussian counts significantly increases the GPU device\nmemory for storing model parameters. A large model thus requires powerful GPUs\nwith high memory capacities for training and has slower training/rendering\nlatencies due to the inefficiencies of memory access and data movement. In this\nwork, we introduce ContraGS, a method to enable training directly on compressed\n3DGS representations without reducing the Gaussian Counts, and thus with a\nlittle loss in model quality. ContraGS leverages codebooks to compactly store a\nset of Gaussian parameter vectors throughout the training process, thereby\nsignificantly reducing memory consumption. While codebooks have been\ndemonstrated to be highly effective at compressing fully trained 3DGS models,\ndirectly training using codebook representations is an unsolved challenge.\nContraGS solves the problem of learning non-differentiable parameters in\ncodebook-compressed representations by posing parameter estimation as a\nBayesian inference problem. To this end, ContraGS provides a framework that\neffectively uses MCMC sampling to sample over a posterior distribution of these\ncompressed representations. With ContraGS, we demonstrate that ContraGS\nsignificantly reduces the peak memory during training (on average 3.49X) and\naccelerated training and rendering (1.36X and 1.88X on average, respectively),\nwhile retraining close to state-of-art quality.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u662f\u4e00\u79cd\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u6e32\u67d3\u6280\u672f\uff0c\u4f46\u9ad8\u6a21\u578b\u8d28\u91cf\u9700\u8981\u5927\u91cf3D\u9ad8\u65af\u5206\u5e03\uff0c\u5bfc\u81f4GPU\u5185\u5b58\u6d88\u8017\u5927\u3001\u8bad\u7ec3\u901f\u5ea6\u6162\u3002ContraGS\u901a\u8fc7\u4ee3\u7801\u7c3f\u538b\u7f29\u5b58\u50a8\u9ad8\u65af\u53c2\u6570\u5411\u91cf\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u8bad\u7ec3\u538b\u7f29\u8868\u793a\u7684\u96be\u9898\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u5e76\u52a0\u901f\u8bad\u7ec3\u548c\u6e32\u67d3\u3002", "motivation": "\u9ad8\u5185\u5b58\u6d88\u8017\u548c\u4f4e\u6548\u7684\u6570\u636e\u79fb\u52a8\u9650\u5236\u4e863DGS\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u6709\u9650\u7684\u8bbe\u5907\u4e0a\u3002", "method": "ContraGS\u5229\u7528\u4ee3\u7801\u7c3f\u7d27\u51d1\u5b58\u50a8\u9ad8\u65af\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u975e\u53ef\u5fae\u5206\u53c2\u6570\u7684\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u7f57\u91c7\u6837\u6765\u8bad\u7ec3\u538b\u7f29\u8868\u793a\u3002", "result": "ContraGS\u5e73\u5747\u51cf\u5c113.49\u500d\u5cf0\u503c\u5185\u5b58\uff0c\u8bad\u7ec3\u52a0\u901f1.36\u500d\uff0c\u6e32\u67d3\u52a0\u901f1.88\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1SOTA\u7684\u8d28\u91cf\u3002", "conclusion": "ContraGS\u4e3a3DGS\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5185\u5b58\u4f18\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u538b\u7f29\u8868\u793a\u76f4\u63a5\u8bad\u7ec3\u7684\u96be\u9898\u3002"}}
{"id": "2509.03692", "pdf": "https://arxiv.org/pdf/2509.03692", "abs": "https://arxiv.org/abs/2509.03692", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann"], "title": "lifeXplore at the Lifelog Search Challenge 2021", "categories": ["cs.IR", "cs.MM"], "comment": null, "summary": "Since its first iteration in 2018, the Lifelog Search Challenge (LSC)\ncontinues to rise in popularity as an interactive lifelog data retrieval\ncompetition, co-located at the ACM International Conference on Multimedia\nRetrieval (ICMR). The goal of this annual live event is to search a large\ncorpus of lifelogging data for specifically announced memories using a\npurposefully developed tool within a limited amount of time. As long-standing\nparticipants, we present our improved lifeXplore - a retrieval system combining\nchronologic day summary browsing with interactive combinable concept filtering.\nCompared to previous versions, the tool is improved by incorporating temporal\nqueries, advanced day summary features as well as usability improvements.", "AI": {"tldr": "Lifelog Search Challenge (LSC)\u662f\u4e00\u9879\u4ea4\u4e92\u5f0f\u751f\u547d\u65e5\u5fd7\u6570\u636e\u68c0\u7d22\u7ade\u8d5b\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u6539\u8fdb\u7684lifeXplore\u7cfb\u7edf\uff0c\u7ed3\u5408\u65f6\u95f4\u7ebf\u6458\u8981\u6d4f\u89c8\u548c\u4ea4\u4e92\u5f0f\u6982\u5ff5\u8fc7\u6ee4\u529f\u80fd\u3002", "motivation": "\u63d0\u5347\u751f\u547d\u65e5\u5fd7\u6570\u636e\u7684\u68c0\u7d22\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\uff0c\u6ee1\u8db3LSC\u7ade\u8d5b\u4e2d\u5bf9\u5feb\u901f\u68c0\u7d22\u7279\u5b9a\u8bb0\u5fc6\u7684\u9700\u6c42\u3002", "method": "\u6539\u8fdb\u7684lifeXplore\u7cfb\u7edf\uff0c\u7ed3\u5408\u65f6\u95f4\u7ebf\u6458\u8981\u6d4f\u89c8\u3001\u4ea4\u4e92\u5f0f\u6982\u5ff5\u8fc7\u6ee4\uff0c\u5e76\u65b0\u589e\u65f6\u95f4\u67e5\u8be2\u3001\u9ad8\u7ea7\u6458\u8981\u529f\u80fd\u548c\u53ef\u7528\u6027\u4f18\u5316\u3002", "result": "\u7cfb\u7edf\u529f\u80fd\u66f4\u5f3a\u5927\uff0c\u68c0\u7d22\u6548\u7387\u66f4\u9ad8\uff0c\u7528\u6237\u4f53\u9a8c\u66f4\u4f18\u3002", "conclusion": "\u6539\u8fdb\u7684lifeXplore\u7cfb\u7edf\u80fd\u66f4\u597d\u5730\u6ee1\u8db3LSC\u7ade\u8d5b\u9700\u6c42\uff0c\u5c55\u73b0\u4e86\u751f\u547d\u65e5\u5fd7\u68c0\u7d22\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2509.03818", "pdf": "https://arxiv.org/pdf/2509.03818", "abs": "https://arxiv.org/abs/2509.03818", "authors": ["Sherwan Jalal Abdullah", "Sravan Reddy Chintareddy", "Victor S. Frost", "Shawn Keshmiri", "Morteza Hashemi"], "title": "A Versatile and Programmable UAV Platform for Radio Access Network and End-to-End Cellular Measurements", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "In this work, we develop a measurement platform to capture mobile network\nperformance metrics including coverage and quality of service in regions where\nconventional coverage testing approaches are frequently time-intensive,\nlabor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing\nmethods are used to collect cellular network performance metrics. However,\nthese approaches are inadequate in rural areas due to low-density population,\nand difficult terrain. The platform described here is a UAV-based and is\ndesigned to investigate the mobile network performance through aerial\noperations and gather Radio Access Network (RAN) signal alongside end-to-end\nnetwork performance metrics. Our platform gathers metrics through the\nintegration of an onboard computation unit and commercial off-the-shelf\ncellular modem. The gathered data are subsequently analyzed and displayed using\ngeospatial mapping utilities and statistical techniques to deliver key\nobservations on cellular network performance. Experimental results showed that\nthe received signal power improves at higher altitudes due to enhanced\nline-of-sight (LoS) conditions as expected. However, the signal quality\ndegrades as a result of increased interference from neighboring cells. The\nanalysis reveals that for most of the geographic area covered in the initial\nexperiments the system maintained acceptable signal quality, with adequate\nthroughput performance for both uplink and downlink communications, while\nmaintaining satisfactory round-trip time characteristics. Notably, the\nexperiment showed that a strong radio signal metric for a given cell does not\nnecessarily translate to consistent spatial coverage across the tested region.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u79fb\u52a8\u7f51\u7edc\u6027\u80fd\u6d4b\u91cf\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u8986\u76d6\u7684\u5730\u533a\uff0c\u901a\u8fc7\u7a7a\u4e2d\u64cd\u4f5c\u548c\u6570\u636e\u5206\u6790\u63d0\u4f9b\u7f51\u7edc\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u7fa4\u4f17\u5305\u65b9\u6cd5\u5728\u4eba\u53e3\u7a00\u5c11\u548c\u5730\u5f62\u590d\u6742\u7684\u519c\u6751\u5730\u533a\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u5b89\u5168\u7684\u7f51\u7edc\u6d4b\u91cf\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5e73\u53f0\u6574\u5408\u4e86\u673a\u8f7d\u8ba1\u7b97\u5355\u5143\u548c\u5546\u7528\u8702\u7a9d\u8c03\u5236\u89e3\u8c03\u5668\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u6536\u96c6RAN\u4fe1\u53f7\u548c\u7aef\u5230\u7aef\u7f51\u7edc\u6027\u80fd\u6570\u636e\uff0c\u5e76\u5229\u7528\u5730\u7406\u7a7a\u95f4\u6620\u5c04\u548c\u7edf\u8ba1\u5206\u6790\u5c55\u793a\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8\u6d77\u62d4\u4e0b\u4fe1\u53f7\u529f\u7387\u56e0\u89c6\u7ebf\u6761\u4ef6\u6539\u5584\u800c\u589e\u5f3a\uff0c\u4f46\u4fe1\u53f7\u8d28\u91cf\u56e0\u90bb\u8fd1\u5c0f\u533a\u5e72\u6270\u800c\u4e0b\u964d\uff1b\u5c3d\u7ba1\u5982\u6b64\uff0c\u7cfb\u7edf\u5728\u5927\u591a\u6570\u533a\u57df\u4fdd\u6301\u4e86\u53ef\u63a5\u53d7\u7684\u4fe1\u53f7\u8d28\u91cf\u548c\u541e\u5410\u91cf\u3002", "conclusion": "\u65e0\u4eba\u673a\u5e73\u53f0\u4e3a\u519c\u6751\u548c\u590d\u6742\u5730\u5f62\u533a\u57df\u7684\u7f51\u7edc\u6027\u80fd\u6d4b\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4f46\u4fe1\u53f7\u5f3a\u5ea6\u4e0e\u8986\u76d6\u8303\u56f4\u5e76\u975e\u5b8c\u5168\u76f8\u5173\u3002"}}
{"id": "2509.04347", "pdf": "https://arxiv.org/pdf/2509.04347", "abs": "https://arxiv.org/abs/2509.04347", "authors": ["Johanna Brunar", "Michael Pinsker", "Moritz Sch\u00f6bi"], "title": "Janus-faces of temporal constraint languages: a dichotomy of expressivity", "categories": ["cs.LO"], "comment": "21 pages", "summary": "The Bodirsky-K\\'ara classification of temporal constraint languages stands as\none of the earliest and most seminal complexity classifications within\ninfinite-domain Constraint Satisfaction Problems (CSPs), yet it remains one of\nthe most mysterious in terms of algorithms and algebraic invariants for the\ntractable cases. We show that those temporal languages which do not\npp-construct EVERYTHING (and thus by the classification are solvable in\npolynomial time) have, in fact, very limited expressive power as measured by\nthe graphs and hypergraphs they can pp-interpret. This limitation yields many\npreviously unknown algebraic consequences, while also providing new, uniform\nproofs for known invariance properties. In particular, we show that such\ntemporal constraint languages admit $4$-ary pseudo-Siggers polymorphisms -- a\nresult that sustains the possibility that the existence of such polymorphisms\nextends to the much broader context of the Bodirsky-Pinsker conjecture.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u65f6\u95f4\u7ea6\u675f\u8bed\u8a00\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u53ef\u89e3\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u8868\u8fbe\u80fd\u529b\u6781\u5176\u6709\u9650\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u4ee3\u6570\u7ed3\u679c\u548c\u4e00\u81f4\u6027\u8bc1\u660e\u3002", "motivation": "\u7814\u7a76\u65f6\u95f4\u7ea6\u675f\u8bed\u8a00\u7684\u590d\u6742\u6027\u5206\u7c7b\uff0c\u7279\u522b\u5173\u6ce8\u5176\u5728\u53ef\u89e3\u60c5\u51b5\u4e0b\u7684\u8868\u8fbe\u80fd\u529b\u548c\u4ee3\u6570\u7279\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u65f6\u95f4\u7ea6\u675f\u8bed\u8a00\u7684pp-\u89e3\u91ca\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u8868\u8fbe\u80fd\u529b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5229\u7528\u4f2aSiggers\u591a\u6001\u6027\u7b49\u5de5\u5177\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u8bed\u8a00\u4e0d\u4ec5\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u8fd8\u652f\u63014\u5143\u4f2aSiggers\u591a\u6001\u6027\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684Bodirsky-Pinsker\u731c\u60f3\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u65e0\u9650\u57df\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u7684\u590d\u6742\u6027\u5206\u7c7b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u548c\u6f5c\u5728\u7684\u7edf\u4e00\u8bc1\u660e\u65b9\u6cd5\u3002"}}
{"id": "2509.03741", "pdf": "https://arxiv.org/pdf/2509.03741", "abs": "https://arxiv.org/abs/2509.03741", "authors": ["Eduardo Davalos", "Yike Zhang", "Shruti Jain", "Namrata Srivastava", "Trieu Truong", "Nafees-ul Haque", "Tristan Van", "Jorge Salas", "Sara McFadden", "Sun-Joo Cho", "Gautam Biswas", "Amanda Goodwin"], "title": "Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support", "categories": ["cs.HC", "cs.AI"], "comment": "22 pages, 9 figures, 3 tables, submitted to IUI2026", "summary": "Eye-tracking offers rich insights into student cognition and engagement, but\nremains underutilized in classroom-facing educational technology due to\nchallenges in data interpretation and accessibility. In this paper, we present\nthe iterative design and evaluation of a gaze-based learning analytics\ndashboard for English Language Arts (ELA), developed through five studies\ninvolving teachers and students. Guided by user-centered design and data\nstorytelling principles, we explored how gaze data can support reflection,\nformative assessment, and instructional decision-making. Our findings\ndemonstrate that gaze analytics can be approachable and pedagogically valuable\nwhen supported by familiar visualizations, layered explanations, and narrative\nscaffolds. We further show how a conversational agent, powered by a large\nlanguage model (LLM), can lower cognitive barriers to interpreting gaze data by\nenabling natural language interactions with multimodal learning analytics. We\nconclude with design implications for future EdTech systems that aim to\nintegrate novel data modalities in classroom contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u773c\u52a8\u6570\u636e\u7684\u5b66\u4e60\u5206\u6790\u4eea\uff0c\u901a\u8fc7\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u548c\u6570\u636e\u53d9\u4e8b\u539f\u5219\uff0c\u5e2e\u52a9\u5b66\u751f\u548c\u6559\u5e08\u66f4\u597d\u5730\u7406\u89e3\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u773c\u52a8\u6570\u636e\u5728\u6559\u5b66\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u5b58\u5728\u6570\u636e\u89e3\u91ca\u548c\u53ef\u8bbf\u95ee\u6027\u6311\u6218\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e94\u9879\u7814\u7a76\uff0c\u7ed3\u5408\u6559\u5e08\u548c\u5b66\u751f\u53cd\u9988\uff0c\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u652f\u6301\u82f1\u8bed\u8bed\u8a00\u827a\u672f\u7684\u4eea\u8868\u76d8\uff0c\u5229\u7528\u53ef\u89c6\u5316\u3001\u5206\u5c42\u89e3\u91ca\u548cLLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u4ee3\u7406\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7ed3\u5408\u719f\u6089\u7684\u53ef\u89c6\u5316\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7684\u773c\u52a8\u5206\u6790\u53ef\u4ee5\u63d0\u9ad8\u6570\u636e\u7684\u53ef\u7406\u89e3\u548c\u6559\u5b66\u4ef7\u503c\u3002", "conclusion": "\u672a\u6765\u6559\u80b2\u6280\u672f\u7cfb\u7edf\u5e94\u6574\u5408\u65b0\u9896\u6570\u636e\u6a21\u6001\uff0c\u5e76\u6ce8\u91cd\u7528\u6237\u53cb\u597d\u7684\u4ea4\u4e92\u8bbe\u8ba1\u3002"}}
{"id": "2509.04004", "pdf": "https://arxiv.org/pdf/2509.04004", "abs": "https://arxiv.org/abs/2509.04004", "authors": ["Avisek Sharma", "Satakshi Ghosh", "Buddhadeb Sau"], "title": "Gathering of asynchronous robots on circle with limited visibility using finite communication", "categories": ["cs.DC"], "comment": null, "summary": "This work addresses the gathering problem for a set of autonomous, anonymous,\nand homogeneous robots with limited visibility operating in a continuous\ncircle. The robots are initially placed at distinct positions, forming a\nrotationally asymmetric configuration. The robots agree on the clockwise\ndirection. In the $\\theta$-visibility model, a robot can only see those robots\non the circle that are at an angular distance $<\\theta$ from it. Di Luna\n\\textit{et. al.} [DISC'20] have shown that, in $\\pi/2$ visibility, gathering is\nimpossible. In addition, they provided an algorithm for robots with $\\pi$\nvisibility, operating under a semi-synchronous scheduler. In the $\\pi$\nvisibility model, only one point, the point at the angular distance $\\pi$ is\nremoved from the visibility. Ghosh \\textit{et. al.} [SSS'23] provided a\ngathering algorithm for $\\pi$ visibility model with robot having finite memory\n($\\mathcal{FSTA}$), operating under a special asynchronous scheduler.\n  If the robots can see all points on the circle, then the gathering can be\ndone by electing a leader in the weakest robot model under a fully asynchronous\nscheduler. However, previous works have shown that even the removal of one\npoint from the visibility makes gathering difficult. In both works, the robots\nhad rigid movement. In this work, we propose an algorithm that solves the\ngathering problem under the $\\pi$-visibility model for robots that have finite\ncommunication ability ($\\mathcal{FCOM}$). In this work the robot movement is\nnon-rigid and the robots work under a fully asynchronous scheduler.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6709\u9650\u53ef\u89c1\u6027\uff08\u03c0-\u53ef\u89c1\u6027\uff09\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u673a\u5668\u4eba\u5177\u6709\u6709\u9650\u901a\u4fe1\u80fd\u529b\uff08FCOM\uff09\uff0c\u8fd0\u52a8\u662f\u975e\u521a\u6027\u7684\uff0c\u4e14\u5728\u5168\u5f02\u6b65\u8c03\u5ea6\u5668\u4e0b\u5de5\u4f5c\u3002", "motivation": "\u9488\u5bf9\u6709\u9650\u53ef\u89c1\u6027\u673a\u5668\u4eba\u96be\u4ee5\u5728\u5706\u4e0a\u805a\u96c6\u7684\u95ee\u9898\uff0c\u8fc7\u53bb\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u521a\u6027\u8fd0\u52a8\u548c\u7279\u5b9a\u8c03\u5ea6\u6761\u4ef6\u4e0b\u3002\u672c\u6587\u65e8\u5728\u653e\u5bbd\u8fd9\u4e9b\u9650\u5236\uff0c\u63a2\u7d22\u975e\u521a\u6027\u8fd0\u52a8\u548c\u5168\u5f02\u6b65\u6761\u4ef6\u4e0b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5177\u6709\u6709\u9650\u901a\u4fe1\u80fd\u529b\uff08FCOM\uff09\u7684\u673a\u5668\u4eba\uff0c\u5728\u03c0-\u53ef\u89c1\u6027\u6a21\u578b\u4e0b\u8fd0\u884c\uff0c\u91c7\u7528\u975e\u521a\u6027\u8fd0\u52a8\u548c\u5168\u5f02\u6b65\u8c03\u5ea6\u5668\u3002", "result": "\u6210\u529f\u89e3\u51b3\u4e86\u03c0-\u53ef\u89c1\u6027\u6a21\u578b\u4e0b\u7684\u805a\u96c6\u95ee\u9898\uff0c\u673a\u5668\u4eba\u80fd\u591f\u5728\u975e\u521a\u6027\u8fd0\u52a8\u548c\u5168\u5f02\u6b65\u6761\u4ef6\u4e0b\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "\u672c\u7814\u7a76\u6269\u5c55\u4e86\u6709\u9650\u53ef\u89c1\u6027\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\u7684\u89e3\u51b3\u8303\u56f4\uff0c\u4e3a\u975e\u521a\u6027\u548c\u5168\u5f02\u6b65\u6761\u4ef6\u4e0b\u7684\u7c7b\u4f3c\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2509.04162", "pdf": "https://arxiv.org/pdf/2509.04162", "abs": "https://arxiv.org/abs/2509.04162", "authors": ["Safa Mohammed Sali", "Mahmoud Meribout", "Ashiyana Abdul Majeed"], "title": "Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations", "categories": ["cs.AR"], "comment": null, "summary": "Transformers and vision-language models (VLMs) have emerged as dominant\narchitectures in computer vision and multimodal AI, offering state-of-the-art\nperformance in tasks such as image classification, object detection, visual\nquestion answering, and caption generation. However, their high computational\ncomplexity, large memory footprints, and irregular data access patterns present\nsignificant challenges for deployment in latency- and power-constrained\nenvironments. Field-programmable gate arrays (FPGAs) provide an attractive\nhardware platform for such workloads due to their reconfigurability,\nfine-grained parallelism, and potential for energy-efficient acceleration. This\npaper presents a comprehensive review of design trade-offs, optimization\nstrategies, and implementation challenges for FPGA-based inference of\ntransformers and VLMs. We examine critical factors such as device-class\nselection, memory subsystem constraints, dataflow orchestration, quantization\nstrategies, sparsity exploitation, and toolchain choices, alongside\nmodality-specific issues unique to VLMs, including heterogeneous compute\nbalancing and cross-attention memory management. Additionally, we discuss\nemerging trends in hardware-algorithm co-design, highlighting innovations in\nattention mechanisms, compression, and modular overlays to improve efficiency\nand adaptability. Practical issues such as runtime flexibility, verification\noverhead, and the absence of standardized FPGA multimodal benchmarks are also\nconsidered. Finally, we outline future directions toward scalable, portable,\nand reconfigurable FPGA solutions that adapt to evolving model architectures\nwhile sustaining high utilization and predictable performance. This synthesis\noffers both a technical foundation and a forward-looking perspective to help\nbridge the gap between advanced multimodal AI models and efficient FPGA\ndeployment.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86FPGA\u5728Transformer\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u8bbe\u8ba1\u6743\u8861\u3001\u4f18\u5316\u7b56\u7565\u53ca\u5b9e\u73b0\u6311\u6218\uff0c\u63a2\u8ba8\u4e86\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u7684\u65b0\u8d8b\u52bf\uff0c\u65e8\u5728\u63d0\u9ad8\u90e8\u7f72\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "motivation": "Transformer\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u9700\u6c42\uff0c\u4f7f\u5176\u5728\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u529f\u8017\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002FPGA\u56e0\u5176\u53ef\u91cd\u6784\u6027\u548c\u9ad8\u6548\u5e76\u884c\u6027\u6210\u4e3a\u7406\u60f3\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bbe\u5907\u7c7b\u522b\u9009\u62e9\u3001\u5185\u5b58\u7ea6\u675f\u3001\u6570\u636e\u6d41\u7f16\u6392\u3001\u91cf\u5316\u7b56\u7565\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u7ed3\u5408\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\uff0c\u63a2\u7d22FPGA\u7684\u9ad8\u6548\u63a8\u7406\u5b9e\u73b0\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6280\u672f\u57fa\u7840\uff0c\u5e76\u5c55\u671b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u79fb\u690d\u7684FPGA\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u652f\u6301\u4e0d\u65ad\u6f14\u8fdb\u7684\u6a21\u578b\u67b6\u6784\u3002", "conclusion": "FPGA\u5728Transformer\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u672a\u6765\u9700\u89e3\u51b3\u6807\u51c6\u57fa\u51c6\u7f3a\u5931\u548c\u8fd0\u884c\u65f6\u7075\u6d3b\u6027\u7b49\u95ee\u9898\u3002"}}
{"id": "2509.03896", "pdf": "https://arxiv.org/pdf/2509.03896", "abs": "https://arxiv.org/abs/2509.03896", "authors": ["Zushuai Zhang", "Elliott Wen", "Ewan Tempero"], "title": "Analyzing Variations in Dependency Distributions Due to Code Smell Interactions", "categories": ["cs.SE"], "comment": null, "summary": "The existence of dependencies between modules, such as classes, can mean that\nchanging a module triggers ripple effects that make maintenance complex and\ncostly, so the advice is to minimize dependencies between modules. It is\ntherefore important to understand the circumstances that can lead to increased\ndependencies. Recent studies suggest that code smells, which are\ncharacteristics of code that indicate potential design issues, may interact in\nways that increase dependencies between modules. In this study, we aim to\nconfirm previous observations and investigate whether and how the distribution\nof static dependencies changes in the presence of code smell interactions. We\nconducted a dependency analysis on 116 open-source Java systems to quantify the\ninteractions, comparing interactions among code smells and interactions between\ncode smells and non-code smells. Our results suggest that while interactions\nbetween code smell pairs are associated with increases in certain dependencies\nand decreases in others, overall, they are associated with an increase in total\ndependencies. For example, the median number of dependencies between Feature\nEnvy methods and Data Classes is seven times as many as when the methods are\nnon-Feature Envy methods, increasing from 1 to 7. This implies that developers\nshould prioritize addressing code smells that interact with each other, rather\nthan code smells that exist only in isolation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4ee3\u7801\u5f02\u5473\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u4f1a\u589e\u52a0\u6a21\u5757\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5f00\u53d1\u8005\u5e94\u4f18\u5148\u89e3\u51b3\u76f8\u4e92\u4f5c\u7528\u7684\u4ee3\u7801\u5f02\u5473\u800c\u975e\u5b64\u7acb\u5b58\u5728\u7684\u5f02\u5473\u3002", "motivation": "\u6a21\u5757\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u4f1a\u589e\u52a0\u7ef4\u62a4\u7684\u590d\u6742\u6027\u548c\u6210\u672c\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u4e86\u89e3\u5bfc\u81f4\u4f9d\u8d56\u589e\u52a0\u7684\u4ee3\u7801\u5f02\u5473\u4e92\u52a8\u60c5\u51b5\u3002", "method": "\u5bf9116\u4e2a\u5f00\u6e90Java\u7cfb\u7edf\u8fdb\u884c\u4f9d\u8d56\u5206\u6790\uff0c\u91cf\u5316\u4ee3\u7801\u5f02\u5473\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u4e0e\u9759\u6001\u4f9d\u8d56\u5206\u5e03\u7684\u5173\u7cfb\u3002", "result": "\u4ee3\u7801\u5f02\u5473\u5bf9\u7684\u4ea4\u4e92\u4f1a\u5bfc\u81f4\u67d0\u4e9b\u4f9d\u8d56\u589e\u52a0\uff08\u5982Feature Envy\u4e0eData Class\u7684\u4f9d\u8d56\u4e2d\u4f4d\u6570\u4ece1\u589e\u81f37\uff09\uff0c\u603b\u4f53\u4e0a\u589e\u52a0\u603b\u4f9d\u8d56\u3002", "conclusion": "\u5f00\u53d1\u8005\u5e94\u4f18\u5148\u5904\u7406\u76f8\u4e92\u4f5c\u7528\u7684\u4ee3\u7801\u5f02\u5473\u4ee5\u51cf\u5c11\u6a21\u5757\u95f4\u7684\u4f9d\u8d56\u3002"}}
{"id": "2509.04047", "pdf": "https://arxiv.org/pdf/2509.04047", "abs": "https://arxiv.org/abs/2509.04047", "authors": ["Ashish Tiwari", "Satyam Bhardwaj", "Yash Bachwana", "Parag Sarvoday Sahu", "T. M. Feroz Ali", "Bhargava Chintalapati", "Shanmuganathan Raman"], "title": "TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "To appear in Pacific Graphics 2025 (CGF Journal Track), Project page:\n  https://yashbachwana.github.io/TensoIS/", "summary": "Estimating scattering parameters of heterogeneous media from images is a\nseverely under-constrained and challenging problem. Most of the existing\napproaches model BSSRDF either through an analysis-by-synthesis approach,\napproximating complex path integrals, or using differentiable volume rendering\ntechniques to account for heterogeneity. However, only a few studies have\napplied learning-based methods to estimate subsurface scattering parameters,\nbut they assume homogeneous media. Interestingly, no specific distribution is\nknown to us that can explicitly model the heterogeneous scattering parameters\nin the real world. Notably, procedural noise models such as Perlin and Fractal\nPerlin noise have been effective in representing intricate heterogeneities of\nnatural, organic, and inorganic surfaces. Leveraging this, we first create\nHeteroSynth, a synthetic dataset comprising photorealistic images of\nheterogeneous media whose scattering parameters are modeled using Fractal\nPerlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a\nlearning-based feed-forward framework to estimate these Perlin-distributed\nheterogeneous scattering parameters from sparse multi-view image observations.\nInstead of directly predicting the 3D scattering parameter volume, TensoIS uses\nlearnable low-rank tensor components to represent the scattering volume. We\nevaluate TensoIS on unseen heterogeneous variations over shapes from the\nHeteroSynth test set, smoke and cloud geometries obtained from open-source\nrealistic volumetric simulations, and some real-world samples to establish its\neffectiveness for inverse scattering. Overall, this study is an attempt to\nexplore Perlin noise distribution, given the lack of any such well-defined\ndistribution in literature, to potentially model real-world heterogeneous\nscattering in a feed-forward manner.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5TensoIS\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u4f30\u8ba1\u5f02\u8d28\u6563\u5c04\u53c2\u6570\uff0c\u5e76\u521b\u5efa\u4e86\u5408\u6210\u6570\u636e\u96c6HeteroSynth\u3002", "motivation": "\u89e3\u51b3\u5f02\u8d28\u4ecb\u8d28\u6563\u5c04\u53c2\u6570\u4f30\u8ba1\u8fd9\u4e00\u4e25\u91cd\u53d7\u9650\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u5229\u7528Perlin\u566a\u58f0\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5f02\u8d28\u6027\u3002", "method": "\u901a\u8fc7Fractal Perlin\u566a\u58f0\u521b\u5efa\u5408\u6210\u6570\u636e\u96c6HeteroSynth\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684TensoIS\u6846\u67b6\uff0c\u7528\u4f4e\u79e9\u5f20\u91cf\u5206\u91cf\u8868\u793a\u6563\u5c04\u4f53\u79ef\u3002", "result": "TensoIS\u5728HeteroSynth\u6d4b\u8bd5\u96c6\u3001\u70df\u96fe\u548c\u4e91\u51e0\u4f55\u4f53\u4ee5\u53ca\u771f\u5b9e\u6837\u672c\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a2\u7d22\u4e86Perlin\u566a\u58f0\u5206\u5e03\uff0c\u4e3a\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5f02\u8d28\u6563\u5c04\u53c2\u6570\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03901", "pdf": "https://arxiv.org/pdf/2509.03901", "abs": "https://arxiv.org/abs/2509.03901", "authors": ["Katarzyna Kosek-Szott", "Szymon Szott", "Wojciech Ciezobka", "Maksymilian Wojnar", "Krzysztof Rusek", "Jonathan Segev"], "title": "Indoor Positioning with Wi-Fi Location: A Survey of IEEE 802.11mc/az/bk Fine Timing Measurement Research", "categories": ["cs.NI"], "comment": "30 pages, survey paper", "summary": "Indoor positioning is an enabling technology for home, office, and industrial\nnetwork users because it provides numerous information and communication\ntechnology (ICT) and Internet of things (IoT) functionalities such as indoor\nnavigation, smart meter localization, asset tracking, support for emergency\nservices, and detection of hazardous situations. The IEEE 802.11mc fine timing\nmeasurement (FTM) protocol (commercially known as Wi-Fi Location) has great\npotential to enable indoor positioning in future generation devices, primarily\nbecause of the high availability of Wi-Fi networks, FTM's high accuracy and\ndevice support. Furthermore, new FTM enhancements are available in the released\n(802.11az) and recently completed (802.11bk) amendments. Despite the multitude\nof literature reviews on indoor positioning, a survey dedicated to FTM and its\nrecent enhancements has so far been lacking. We fill this gap by classifying\nand reviewing over 180 research papers related to the practical accuracy\nachieved with FTM, methods for improving its accuracy (also with machine\nlearning), combining FTM with other indoor positioning systems, FTM-based\napplications, and security issues. Based on the conducted survey, we summarize\nthe most important research achievements and formulate open areas for further\nresearch.", "AI": {"tldr": "\u672c\u6587\u586b\u8865\u4e86\u5173\u4e8eIEEE 802.11mc FTM\u534f\u8bae\u53ca\u5176\u6700\u65b0\u589e\u5f3a\u6280\u672f\u5728\u5ba4\u5185\u5b9a\u4f4d\u4e2d\u5e94\u7528\u7684\u6587\u732e\u7efc\u8ff0\u7a7a\u767d\uff0c\u5206\u7c7b\u5e76\u56de\u987e\u4e86180\u591a\u7bc7\u76f8\u5173\u7814\u7a76\u8bba\u6587\u3002", "motivation": "\u5ba4\u5185\u5b9a\u4f4d\u6280\u672f\u5728\u5bb6\u5ead\u3001\u529e\u516c\u5ba4\u548c\u5de5\u4e1a\u7f51\u7edc\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u9700\u6c42\uff0cIEEE 802.11mc FTM\u534f\u8bae\u56e0\u5176\u9ad8\u7cbe\u5ea6\u548c\u8bbe\u5907\u652f\u6301\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u76f8\u5173\u7efc\u8ff0\u4ecd\u7f3a\u4e4f\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u548c\u56de\u987e180\u591a\u7bc7\u7814\u7a76\u8bba\u6587\uff0c\u5206\u6790FTM\u5728\u5b9e\u9645\u7cbe\u5ea6\u3001\u673a\u5668\u5b66\u4e60\u6539\u8fdb\u65b9\u6cd5\u3001\u4e0e\u5176\u4ed6\u7cfb\u7edf\u7684\u7ed3\u5408\u3001\u5e94\u7528\u53ca\u5b89\u5168\u95ee\u9898\u7b49\u65b9\u9762\u7684\u7814\u7a76\u6210\u679c\u3002", "result": "\u603b\u7ed3\u4e86\u6700\u91cd\u8981\u7684\u7814\u7a76\u6210\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u89e3\u51b3\u7684\u95ee\u9898\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3aFTM\u5728\u5ba4\u5185\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5168\u9762\u53c2\u8003\uff0c\u5e76\u6307\u51fa\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u5f00\u653e\u9886\u57df\u3002"}}
{"id": "2509.04041", "pdf": "https://arxiv.org/pdf/2509.04041", "abs": "https://arxiv.org/abs/2509.04041", "authors": ["Daniel Raggi", "Gem Stapleton", "Mateja Jamnik", "Aaron Stockdill", "Grecia Garcia Garcia", "Peter C-H. Cheng"], "title": "Oruga: An Avatar of Representational Systems Theory", "categories": ["cs.AI", "cs.LO", "68T30, 68T27, 03B35", "I.2.4; I.2.3; F.4.1; F.4.3"], "comment": null, "summary": "Humans use representations flexibly. We draw diagrams, change representations\nand exploit creative analogies across different domains. We want to harness\nthis kind of power and endow machines with it to make them more compatible with\nhuman use. Previously we developed Representational Systems Theory (RST) to\nstudy the structure and transformations of representations. In this paper we\npresent Oruga (caterpillar in Spanish; a symbol of transformation), an\nimplementation of various aspects of RST. Oruga consists of a core of data\nstructures corresponding to concepts in RST, a language for communicating with\nthe core, and an engine for producing transformations using a method we call\nstructure transfer. In this paper we present an overview of the core and\nlanguage of Oruga, with a brief example of the kind of transformation that\nstructure transfer can execute.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOruga\u7684\u7cfb\u7edf\uff0c\u57fa\u4e8eRepresentational Systems Theory (RST)\uff0c\u65e8\u5728\u901a\u8fc7\u6570\u636e\u7ed3\u6784\u548c\u8bed\u8a00\u5de5\u5177\u5b9e\u73b0\u7075\u6d3b\u8868\u793a\u8f6c\u6362\u3002", "motivation": "\u76ee\u6807\u662f\u8ba9\u673a\u5668\u5177\u5907\u4eba\u7c7b\u7075\u6d3b\u4f7f\u7528\u8868\u793a\uff08\u5982\u56fe\u8868\u3001\u7c7b\u6bd4\uff09\u7684\u80fd\u529b\uff0c\u4ee5\u589e\u5f3a\u4e0e\u4eba\u7c7b\u7684\u517c\u5bb9\u6027\u3002", "method": "\u5f00\u53d1Oruga\u7cfb\u7edf\uff0c\u5305\u542bRST\u6838\u5fc3\u6570\u636e\u7ed3\u6784\u3001\u901a\u4fe1\u8bed\u8a00\u53ca\u57fa\u4e8e\u7ed3\u6784\u8f6c\u6362\u7684\u5f15\u64ce\u3002", "result": "\u5c55\u793a\u4e86Oruga\u7684\u6838\u5fc3\u4e0e\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u8f6c\u6362\u793a\u4f8b\u9a8c\u8bc1\u5176\u80fd\u529b\u3002", "conclusion": "Oruga\u7cfb\u7edf\u4e3a\u673a\u5668\u5b9e\u73b0\u7075\u6d3b\u8868\u793a\u8f6c\u6362\u63d0\u4f9b\u4e86\u521d\u6b65\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.03792", "pdf": "https://arxiv.org/pdf/2509.03792", "abs": "https://arxiv.org/abs/2509.03792", "authors": ["Ryo Yonetani", "Kotaro Hara"], "title": "Map as a By-product: Collective Landmark Mapping from IMU Data and User-provided Texts in Situated Tasks", "categories": ["cs.HC"], "comment": "(c) 2025 Copyright held by the owner/author(s). Publication rights\n  licensed to ACM. This is the author's version of the work. It is posted here\n  for your personal use. Not for redistribution. The definitive Version of\n  Record was published in Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.\n  9, 3, Article 146 (September 2025), https://doi.org/10.1145/3749455", "summary": "This paper presents Collective Landmark Mapper, a novel map-as-a-by-product\nsystem for generating semantic landmark maps of indoor environments. Consider\nusers engaged in situated tasks that require them to navigate these\nenvironments and regularly take notes on their smartphones. Collective Landmark\nMapper exploits the smartphone's IMU data and the user's free text input during\nthese tasks to identify a set of landmarks encountered by the user. The\nidentified landmarks are then aggregated across multiple users to generate a\nunified map representing the positions and semantic information of all\nlandmarks. In developing the proposed system, we focused specifically on retail\napplications and conducted a formative interview with stakeholders to confirm\ntheir practical needs that motivate the map-as-a-byproduct approach. Our user\nstudy demonstrates the feasibility of the proposed system and its superior\nmapping performance in two different setups: creating a product availability\nmap from restocking checklist tasks at a retail store and constructing a room\nusage map from office inspection tasks, further demonstrating the potential\napplicability to non-retail applications.", "AI": {"tldr": "Collective Landmark Mapper\u5229\u7528\u667a\u80fd\u624b\u673aIMU\u6570\u636e\u548c\u7528\u6237\u6587\u672c\u8f93\u5165\u751f\u6210\u8bed\u4e49\u5730\u6807\u5730\u56fe\uff0c\u9002\u7528\u4e8e\u96f6\u552e\u548c\u975e\u96f6\u552e\u573a\u666f\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u5728\u5ba4\u5185\u5bfc\u822a\u548c\u4efb\u52a1\u4e2d\u8bb0\u5f55\u5730\u6807\u7684\u9700\u6c42\uff0c\u751f\u6210\u7edf\u4e00\u7684\u8bed\u4e49\u5730\u56fe\uff0c\u6ee1\u8db3\u96f6\u552e\u548c\u975e\u96f6\u552e\u5e94\u7528\u7684\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u5229\u7528\u667a\u80fd\u624b\u673aIMU\u6570\u636e\u548c\u7528\u6237\u81ea\u7531\u6587\u672c\u8f93\u5165\u8bc6\u522b\u5730\u6807\uff0c\u5e76\u901a\u8fc7\u591a\u7528\u6237\u6570\u636e\u805a\u5408\u751f\u6210\u7edf\u4e00\u5730\u56fe\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\u7cfb\u7edf\u53ef\u884c\u4e14\u5728\u96f6\u552e\u548c\u975e\u96f6\u552e\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u96f6\u552e\u73af\u5883\u4e2d\u80fd\u6709\u6548\u751f\u6210\u8bed\u4e49\u5730\u6807\u5730\u56fe\u3002"}}
{"id": "2509.04038", "pdf": "https://arxiv.org/pdf/2509.04038", "abs": "https://arxiv.org/abs/2509.04038", "authors": ["Benjamin Heymann"], "title": "Counterfactual simulations for large scale systems with burnout variables", "categories": ["cs.DC", "math.OC", "stat.ME"], "comment": null, "summary": "We consider large-scale systems influenced by burnout variables - state\nvariables that start active, shape dynamics, and irreversibly deactivate once\ncertain conditions are met. Simulating what-if scenarios in such systems is\ncomputationally demanding, as alternative trajectories often require sequential\nprocessing, which does not scale very well. This challenge arises in settings\nlike online advertising, because of campaigns budgets, complicating\ncounterfactual analysis despite rich data availability. We introduce a new type\nof algorithms based on what we refer to as uncertainty relaxation, that enables\nefficient parallel computation, significantly improving scalability for\ncounterfactual estimation in systems with burnout variables.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u677e\u5f1b\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5177\u6709\u201cburnout\u53d8\u91cf\u201d\u7684\u7cfb\u7edf\u5728\u53cd\u4e8b\u5b9e\u4f30\u8ba1\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\uff0cburnout\u53d8\u91cf\u7684\u5b58\u5728\u4f7f\u5f97\u53cd\u4e8b\u5b9e\u5206\u6790\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u987a\u5e8f\u5904\u7406\u7684\u573a\u666f\u4e0b\uff0c\u5982\u5728\u7ebf\u5e7f\u544a\u4e2d\u7684\u9884\u7b97\u7ba1\u7406\u3002", "method": "\u5f15\u5165\u4e86\u4e0d\u786e\u5b9a\u6027\u677e\u5f1b\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u6765\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5177\u6709burnout\u53d8\u91cf\u7684\u7cfb\u7edf\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u677e\u5f1b\u7b97\u6cd5\u4e3a\u89e3\u51b3\u53cd\u4e8b\u5b9e\u5206\u6790\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04173", "pdf": "https://arxiv.org/pdf/2509.04173", "abs": "https://arxiv.org/abs/2509.04173", "authors": ["Safa Sali", "Anis Meribout", "Ashiyana Majeed", "Mahmoud Meribout", "Juan Pablo", "Varun Tiwari", "Asma Baobaid"], "title": "Real-time Object Detection and Associated Hardware Accelerators Targeting Autonomous Vehicles: A Review", "categories": ["cs.AR"], "comment": null, "summary": "The efficiency of object detectors depends on factors like detection\naccuracy, processing time, and computational resources. Processing time is\ncrucial for real-time applications, particularly for autonomous vehicles (AVs),\nwhere instantaneous responses are vital for safety. This review paper provides\na concise yet comprehensive survey of real-time object detection (OD)\nalgorithms for autonomous cars delving into their hardware accelerators (HAs).\nNon-neural network-based algorithms, which use statistical image processing,\nhave been entirely substituted by AI algorithms, such as different models of\nconvolutional neural networks (CNNs). Their intrinsically parallel features led\nthem to be deployable into edge-based HAs of various types, where GPUs and, to\na lesser extent, ASIC (application-specific integrated circuit) remain the most\nwidely used. Throughputs of hundreds of frames/s (fps) could be reached;\nhowever, handling object detection for all the cameras available in a typical\nAV requires further hardware and algorithmic improvements. The intensive\ncompetition between AV providers has limited the disclosure of algorithms,\nfirmware, and even hardware platform details. This remains a hurdle for\nresearchers, as commercial systems provide valuable insights while academics\nundergo lengthy training and testing on restricted datasets and road scenarios.\nConsequently, many AV research papers may not be reflected in end products,\nbeing developed under limited conditions. This paper surveys state-of-the-art\nOD algorithms and aims to bridge the gap with technologies in commercial AVs.\nTo our knowledge, this aspect has not been addressed in earlier surveys. Hence,\nthe paper serves as a tangible reference for researchers designing future\ngenerations of vehicles, expected to be fully autonomous for comfort and\nsafety.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u56de\u987e\u4e86\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u53ca\u5176\u786c\u4ef6\u52a0\u901f\u5668\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u7b97\u6cd5\u4e0e\u5546\u4e1a\u6280\u672f\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u7814\u7a76\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u7684\u6548\u7387\u53ca\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u586b\u8865\u5b66\u672f\u754c\u4e0e\u5546\u4e1a\u6280\u672f\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u7efc\u8ff0\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5c24\u5176\u662f\u57fa\u4e8eCNN\u7684\u6a21\u578b\uff0c\u4ee5\u53ca\u5b83\u4eec\u5728GPU\u548cASIC\u7b49\u786c\u4ef6\u52a0\u901f\u5668\u4e0a\u7684\u90e8\u7f72\u3002", "result": "\u5c3d\u7ba1\u5df2\u6709\u7b97\u6cd5\u80fd\u8fbe\u5230\u6bcf\u79d2\u6570\u767e\u5e27\u7684\u5904\u7406\u901f\u5ea6\uff0c\u4f46\u5728\u591a\u6444\u50cf\u5934\u573a\u666f\u4e0b\u4ecd\u9700\u786c\u4ef6\u548c\u7b97\u6cd5\u6539\u8fdb\u3002", "conclusion": "\u8bba\u6587\u4e3a\u672a\u6765\u5168\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5e76\u6307\u51fa\u5546\u4e1a\u7cfb\u7edf\u7684\u4fdd\u5bc6\u6027\u662f\u5f53\u524d\u7814\u7a76\u7684\u6311\u6218\u3002"}}
{"id": "2509.03900", "pdf": "https://arxiv.org/pdf/2509.03900", "abs": "https://arxiv.org/abs/2509.03900", "authors": ["Yuvraj Agrawal"], "title": "The Auth Shim: A Lightweight Architectural Pattern for Integrating Enterprise SSO with Standalone Open-Source Applications", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Open-source software OSS is widely adopted in enterprise settings, but\nstandalone tools often lack native support for protocols like SAML or OIDC,\ncreating a critical security integration gap. This paper introduces and\nformalizes the Auth Shim, a lightweight architectural pattern designed to solve\nthis problem. The Auth Shim is a minimal, external proxy service that acts as a\ncompatibility layer, translating requests from an enterprise Identity Provider\nIdP into the native session management mechanism of a target application. A key\nprerequisite for this pattern is that the target application must expose a\nprogrammatic, secure administrative API. We present a case study of the\npattern's implementation at Adobe to integrate a popular OSS BI tool with Okta\nSAML, which enabled automated Role-Based Access Control RBAC via IAM group\nmapping and eliminated manual user provisioning. By defining its components,\ninteractions, and production deployment considerations, this paper provides a\nreusable, secure, and cost-effective blueprint for integrating any standalone\nOSS tool into an enterprise SSO ecosystem, thereby enabling organizations to\nembrace open-source innovation without compromising on security governance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Auth Shim\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u67b6\u6784\u6a21\u5f0f\uff0c\u7528\u4e8e\u89e3\u51b3\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7f3a\u4e4f\u539f\u751fSAML\u6216OIDC\u534f\u8bae\u652f\u6301\u7684\u5b89\u5168\u96c6\u6210\u95ee\u9898\u3002", "motivation": "\u4f01\u4e1a\u5e7f\u6cdb\u91c7\u7528\u5f00\u6e90\u8f6f\u4ef6\uff0c\u4f46\u8fd9\u4e9b\u5de5\u5177\u901a\u5e38\u7f3a\u4e4f\u5bf9SAML\u6216OIDC\u7b49\u534f\u8bae\u7684\u539f\u751f\u652f\u6301\uff0c\u5bfc\u81f4\u5b89\u5168\u96c6\u6210\u7f3a\u53e3\u3002", "method": "Auth Shim\u662f\u4e00\u79cd\u5916\u90e8\u4ee3\u7406\u670d\u52a1\uff0c\u4f5c\u4e3a\u517c\u5bb9\u5c42\uff0c\u5c06\u4f01\u4e1a\u8eab\u4efd\u63d0\u4f9b\u5546\uff08IdP\uff09\u7684\u8bf7\u6c42\u8f6c\u6362\u4e3a\u76ee\u6807\u5e94\u7528\u7684\u539f\u751f\u4f1a\u8bdd\u7ba1\u7406\u673a\u5236\u3002\u76ee\u6807\u5e94\u7528\u9700\u63d0\u4f9b\u5b89\u5168\u7684\u7f16\u7a0b\u7ba1\u7406API\u3002", "result": "\u901a\u8fc7\u5728Adobe\u5b9e\u65bd\u8be5\u6a21\u5f0f\uff0c\u6210\u529f\u5c06\u4e00\u6b3e\u6d41\u884c\u7684\u5f00\u6e90BI\u5de5\u5177\u4e0eOkta SAML\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8eIAM\u7ec4\u7684\u81ea\u52a8\u5316RBAC\u5e76\u6d88\u9664\u4e86\u624b\u52a8\u7528\u6237\u914d\u7f6e\u3002", "conclusion": "Auth Shim\u4e3a\u5c06\u4efb\u4f55\u72ec\u7acbOSS\u5de5\u5177\u96c6\u6210\u5230\u4f01\u4e1aSSO\u751f\u6001\u7cfb\u7edf\u4e2d\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u4f7f\u7528\u3001\u5b89\u5168\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u84dd\u56fe\uff0c\u4f7f\u4f01\u4e1a\u80fd\u591f\u5728\u4fdd\u969c\u5b89\u5168\u6cbb\u7406\u7684\u540c\u65f6\u62e5\u62b1\u5f00\u6e90\u521b\u65b0\u3002"}}
{"id": "2509.04058", "pdf": "https://arxiv.org/pdf/2509.04058", "abs": "https://arxiv.org/abs/2509.04058", "authors": ["Lei Zhong", "Yi Yang", "Changjian Li"], "title": "SMooGPT: Stylized Motion Generation using Large Language Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u65b0\u65b9\u6cd5SMooGPT\uff0c\u901a\u8fc7\u8eab\u4f53\u90e8\u5206\u6587\u672c\u7a7a\u95f4\u751f\u6210\u98ce\u683c\u5316\u52a8\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u3001\u63a7\u5236\u548c\u6cdb\u5316\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u98ce\u683c\u5316\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u63a7\u5236\u6709\u9650\u3001\u6cdb\u5316\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u516c\u5f00\u6570\u636e\u96c6\u504f\u597d\u7684\u52a8\u4f5c\uff08\u5982\u884c\u8d70\uff09\u4e4b\u5916\u8868\u73b0\u4e0d\u4f73\u3002\u57fa\u4e8e\u4eba\u7c7b\u52a8\u4f5c\u53ef\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001LLM\u5bf9\u52a8\u4f5c\u7684\u7406\u89e3\u80fd\u529b\u53ca\u52a8\u4f5c\u7684\u7ec4\u5408\u6027\u7279\u70b9\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u63a8\u7406-\u7ec4\u5408-\u751f\u6210\u7684\u6846\u67b6\uff0c\u5229\u7528\u8eab\u4f53\u90e8\u5206\u6587\u672c\u7a7a\u95f4\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u901a\u8fc7\u5fae\u8c03\u7684LLM\uff08SMooGPT\uff09\u4f5c\u4e3a\u63a8\u7406\u5668\u3001\u7ec4\u5408\u5668\u548c\u751f\u6210\u5668\uff0c\u751f\u6210\u98ce\u683c\u5316\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u548c\u7528\u6237\u611f\u77e5\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7eaf\u6587\u672c\u9a71\u52a8\u7684\u98ce\u683c\u5316\u52a8\u4f5c\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u3001\u7ec6\u7c92\u5ea6\u63a7\u5236\u80fd\u529b\u548c\u5bf9\u65b0\u98ce\u683c\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7LLM\u7ed3\u5408\u8eab\u4f53\u90e8\u5206\u6587\u672c\u7a7a\u95f4\u7684\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u98ce\u683c\u5316\u52a8\u4f5c\u751f\u6210\u7684\u53ef\u63a7\u6027\u3001\u6cdb\u5316\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2509.03883", "pdf": "https://arxiv.org/pdf/2509.03883", "abs": "https://arxiv.org/abs/2509.03883", "authors": ["Haiwei Xue", "Xiangyang Luo", "Zhanghao Hu", "Xin Zhang", "Xunzhi Xiang", "Yuqin Dai", "Jianzhuang Liu", "Zhensong Zhang", "Minglei Li", "Jian Yang", "Fei Ma", "Zhiyong Wu", "Changpeng Yang", "Zonghong Dai", "Fei Richard Yu"], "title": "Human Motion Video Generation: A Survey", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by TPAMI. Github Repo:\n  https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation IEEE Access:\n  https://ieeexplore.ieee.org/document/11106267", "summary": "Human motion video generation has garnered significant research interest due\nto its broad applications, enabling innovations such as photorealistic singing\nheads or dynamic avatars that seamlessly dance to music. However, existing\nsurveys in this field focus on individual methods, lacking a comprehensive\noverview of the entire generative process. This paper addresses this gap by\nproviding an in-depth survey of human motion video generation, encompassing\nover ten sub-tasks, and detailing the five key phases of the generation\nprocess: input, motion planning, motion video generation, refinement, and\noutput. Notably, this is the first survey that discusses the potential of large\nlanguage models in enhancing human motion video generation. Our survey reviews\nthe latest developments and technological trends in human motion video\ngeneration across three primary modalities: vision, text, and audio. By\ncovering over two hundred papers, we offer a thorough overview of the field and\nhighlight milestone works that have driven significant technological\nbreakthroughs. Our goal for this survey is to unveil the prospects of human\nmotion video generation and serve as a valuable resource for advancing the\ncomprehensive applications of digital humans. A complete list of the models\nexamined in this survey is available in Our Repository\nhttps://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4eba\u7c7b\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u9996\u6b21\u5168\u9762\u68b3\u7406\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u4e94\u4e2a\u5173\u952e\u9636\u6bb5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7efc\u8ff0\u4ec5\u5173\u6ce8\u4e2a\u522b\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\u7684\u5168\u9762\u6982\u8ff0\uff0c\u672c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u4e09\u79cd\u4e3b\u8981\u6a21\u6001\uff08\u89c6\u89c9\u3001\u6587\u672c\u3001\u97f3\u9891\uff09\uff0c\u8be6\u7ec6\u5206\u6790\u4e86\u5341\u4e2a\u5b50\u4efb\u52a1\u548c\u4e94\u4e2a\u751f\u6210\u9636\u6bb5\uff1a\u8f93\u5165\u3001\u8fd0\u52a8\u89c4\u5212\u3001\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u3001\u4f18\u5316\u548c\u8f93\u51fa\u3002", "result": "\u7efc\u8ff0\u8986\u76d6200\u591a\u7bc7\u8bba\u6587\uff0c\u68b3\u7406\u4e86\u6280\u672f\u53d1\u5c55\u8d8b\u52bf\u5e76\u7a81\u51fa\u4e86\u91cc\u7a0b\u7891\u5de5\u4f5c\u3002", "conclusion": "\u672c\u6587\u4e3a\u6570\u5b57\u4eba\u7c7b\u7efc\u5408\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u5e76\u5c55\u671b\u4e86\u4eba\u7c7b\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u7684\u672a\u6765\u524d\u666f\u3002"}}
{"id": "2509.03935", "pdf": "https://arxiv.org/pdf/2509.03935", "abs": "https://arxiv.org/abs/2509.03935", "authors": ["Sungho Cho", "Sung Il Choi", "Seung Hyun Oh", "Ian P. Roberts", "Sang Hyun Lee"], "title": "Autonomous Task Offloading of Vehicular Edge Computing with Parallel Computation Queues", "categories": ["cs.NI"], "comment": null, "summary": "This work considers a parallel task execution strategy in vehicular edge\ncomputing (VEC) networks, where edge servers are deployed along the roadside to\nprocess offloaded computational tasks of vehicular users. To minimize the\noverall waiting delay among vehicular users, a novel task offloading solution\nis implemented based on the network cooperation balancing resource\nunder-utilization and load congestion. Dual evaluation through theoretical and\nnumerical ways shows that the developed solution achieves a globally optimal\ndelay reduction performance compared to existing methods, which is also\napproved by the feasibility test over a real-map virtual environment. The\nin-depth analysis reveals that predicting the instantaneous processing power of\nedge servers facilitates the identification of overloaded servers, which is\ncritical for determining network delay. By considering discrete variables of\nthe queue, the proposed technique's precise estimation can effectively address\nthese combinatorial challenges to achieve optimal performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f66\u8054\u7f51\u8fb9\u7f18\u8ba1\u7b97\uff08VEC\uff09\u4e2d\u7684\u5e76\u884c\u4efb\u52a1\u5378\u8f7d\u7b56\u7565\uff0c\u65e8\u5728\u901a\u8fc7\u8d44\u6e90\u534f\u4f5c\u5e73\u8861\u51cf\u5c11\u7528\u6237\u7b49\u5f85\u5ef6\u8fdf\uff0c\u7406\u8bba\u4e0e\u6570\u503c\u9a8c\u8bc1\u8868\u660e\u5176\u5168\u5c40\u6700\u4f18\u6027\u3002", "motivation": "\u73b0\u6709\u8f66\u8054\u7f51\u8fb9\u7f18\u8ba1\u7b97\u4e2d\uff0c\u4efb\u52a1\u5378\u8f7d\u5e38\u56e0\u8d44\u6e90\u5229\u7528\u4e0d\u5747\u6216\u8d1f\u8f7d\u62e5\u585e\u5bfc\u81f4\u5ef6\u8fdf\uff0c\u9700\u4e00\u79cd\u9ad8\u6548\u7b56\u7565\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u7f51\u7edc\u534f\u4f5c\u5e73\u8861\u8d44\u6e90\u5229\u7528\u4e0e\u8d1f\u8f7d\uff0c\u9884\u6d4b\u670d\u52a1\u5668\u77ac\u65f6\u5904\u7406\u80fd\u529b\u5e76\u5206\u6790\u961f\u5217\u79bb\u6563\u53d8\u91cf\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u4f30\u8ba1\u548c\u4efb\u52a1\u5206\u914d\u3002", "result": "\u7406\u8bba\u4e0e\u6570\u503c\u9a8c\u8bc1\u8bc1\u660e\uff0c\u8be5\u65b9\u6848\u5728\u5ef6\u8fdf\u51cf\u5c11\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u771f\u5b9e\u5730\u56fe\u865a\u62df\u73af\u5883\u6d4b\u8bd5\u4e5f\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002", "conclusion": "\u901a\u8fc7\u9884\u6d4b\u670d\u52a1\u5668\u5904\u7406\u80fd\u529b\u548c\u7cbe\u786e\u961f\u5217\u5206\u6790\uff0c\u8be5\u7b56\u7565\u80fd\u6709\u6548\u8bc6\u522b\u8fc7\u8f7d\u670d\u52a1\u5668\u5e76\u4f18\u5316\u5ef6\u8fdf\uff0c\u4e3a\u8f66\u8054\u7f51\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04192", "pdf": "https://arxiv.org/pdf/2509.04192", "abs": "https://arxiv.org/abs/2509.04192", "authors": ["Vera Koponen"], "title": "Domain size asymptotics for Markov logic networks", "categories": ["cs.AI", "cs.LO", "math.LO", "68T27, 68T30, 68T37, 03C13", "I.2; F.4; G.3"], "comment": null, "summary": "A Markov logic network (MLN) determines a probability distribution on the set\nof structures, or ``possible worlds'', with an arbitrary finite domain. We\nstudy the properties of such distributions as the domain size tends to\ninfinity. Three types of concrete examples of MLNs will be considered, and the\nproperties of random structures with domain sizes tending to infinity will be\nstudied: (1) Arbitrary quantifier-free MLNs over a language with only one\nrelation symbol which has arity 1. In this case we give a pretty complete\ncharacterization of the possible limit behaviours of random structures. (2) An\nMLN that favours graphs with fewer triangles (or more generally, fewer\nk-cliques). As a corollary of the analysis a ``$\\delta$-approximate 0-1 law''\nfor first-order logic is obtained. (3) An MLN that favours graphs with fewer\nvertices with degree higher than a fixed (but arbitrary) number. The analysis\nshows that depending on which ``soft constraints'' an MLN uses the limit\nbehaviour of random structures can be quite different, and the weights of the\nsoft constraints may, or may not, have influence on the limit behaviour. It\nwill also be demonstrated, using (1), that quantifier-free MLNs and lifted\nBayesian networks (in a broad sense) are asymptotically incomparable, roughly\nmeaning that there is a sequence of distributions on possible worlds with\nincreasing domain sizes that can be defined by one of the formalisms but not\neven approximated by the other. In a rather general context it is also shown\nthat on large domains the distribution determined by an MLN concentrates almost\nall its probability mass on a totally different part of the space of possible\nworlds than the uniform distribution does.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9a6c\u5c14\u79d1\u592b\u903b\u8f91\u7f51\u7edc\uff08MLN\uff09\u5728\u57df\u5927\u5c0f\u8d8b\u4e8e\u65e0\u7a77\u65f6\u5206\u5e03\u6027\u8d28\u7684\u6781\u9650\u884c\u4e3a\uff0c\u901a\u8fc7\u4e09\u7c7b\u5177\u4f53\u793a\u4f8b\u5c55\u793a\u4e86MLN\u7684\u968f\u673a\u7ed3\u6784\u884c\u4e3a\u7684\u591a\u6837\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86MLN\u4e0e\u63d0\u5347\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u6e10\u8fd1\u4e0d\u53ef\u6bd4\u6027\u3002", "motivation": "\u63a2\u7d22MLN\u5728\u4e0d\u540c\u57df\u5927\u5c0f\u4e0b\u7684\u5206\u5e03\u884c\u4e3a\uff0c\u7279\u522b\u662f\u6781\u9650\u60c5\u51b5\uff0c\u4ee5\u63ed\u793a\u8f6f\u7ea6\u675f\u5bf9\u968f\u673a\u7ed3\u6784\u5f71\u54cd\u7684\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u4e09\u7c7bMLN\u5177\u4f53\u793a\u4f8b\uff08\u4e00\u5143\u5173\u7cfb\u7b26\u53f7\u7684\u91cf\u5316\u81ea\u7531MLN\u3001\u51cf\u5c11\u4e09\u89d2\u5f62\u6216k-clique\u7684MLN\u3001\u9650\u5236\u9ad8\u9876\u70b9\u5ea6\u7684MLN\uff09\uff0c\u5206\u6790\u57df\u5927\u5c0f\u8d8b\u4e8e\u65e0\u7a77\u65f6\u7684\u5206\u5e03\u6027\u8d28\u3002", "result": "\u5c55\u793a\u4e86MLN\u7684\u6781\u9650\u884c\u4e3a\u56e0\u8f6f\u7ea6\u675f\u800c\u5f02\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u6743\u91cd\u5f71\u54cd\u663e\u8457\uff1b\u540c\u65f6\u8bc1\u660e\u4e86MLN\u4e0e\u63d0\u5347\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u6e10\u8fd1\u4e0d\u53ef\u6bd4\u6027\u3002", "conclusion": "MLN\u7684\u6781\u9650\u884c\u4e3a\u590d\u6742\u591a\u6837\uff0c\u8f6f\u7ea6\u675f\u7684\u9009\u62e9\u548c\u6743\u91cd\u53ef\u80fd\u663e\u8457\u5f71\u54cd\u5206\u5e03\uff0c\u4e14\u5728\u5927\u578b\u57df\u4e2dMLN\u5206\u5e03\u4e0e\u5747\u5300\u5206\u5e03\u5dee\u5f02\u663e\u8457\u3002"}}
{"id": "2509.03812", "pdf": "https://arxiv.org/pdf/2509.03812", "abs": "https://arxiv.org/abs/2509.03812", "authors": ["Shadeeb Hossain", "Natalie Sommer", "Neda Adib"], "title": "Exploring the Integration of Extended Reality and Artificial Intelligence (AI) for Remote STEM Education and Assessment", "categories": ["cs.HC"], "comment": "9 pages, 5 figures, 1 table", "summary": "This paper presents a dynamic gamification architecture for an Extended\nReality Artificial Intelligence virtual training environment designed to\nenhance STEM education through immersive adaptive, and kinesthetic learning.\nThe proposed system can be introduced in four phases: Introduction Phase,\nComponent Development Phase, Fault Introduction and Correction Phase and\nGenerative AI XR scenarios Phase. Security and privacy are discussed via a\ndefense-in-depth approach spanning client, middleware, and backend layers,\nincorporating AES 256 encryption, multi-factor authentication, role-based\naccess control and GDPR or FERPA compliance. Risks such as sensor exploitation,\nperceptual manipulation, and virtual physical harm are identified, with\nmitigation strategies embedded at the design stage. Potential barriers to large\nscale adoption-including technical complexity, cost of deployment, and need for\ncybersecurity expertise are discussed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6e38\u620f\u5316\u67b6\u6784\uff0c\u7528\u4e8e\u6269\u5c55\u73b0\u5b9e\u4eba\u5de5\u667a\u80fd\u865a\u62df\u57f9\u8bad\u73af\u5883\uff0c\u65e8\u5728\u901a\u8fc7\u6c89\u6d78\u5f0f\u3001\u81ea\u9002\u5e94\u548c\u52a8\u89c9\u5b66\u4e60\u589e\u5f3aSTEM\u6559\u80b2\u3002\u7cfb\u7edf\u5206\u4e3a\u56db\u4e2a\u9636\u6bb5\u5f15\u5165\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b89\u5168\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u3002", "motivation": "\u901a\u8fc7\u6c89\u6d78\u5f0f\u548c\u81ea\u9002\u5e94\u7684\u5b66\u4e60\u65b9\u5f0f\u63d0\u5347STEM\u6559\u80b2\u6548\u679c\uff0c\u540c\u65f6\u89e3\u51b3\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u56db\u9636\u6bb5\u52a8\u6001\u6e38\u620f\u5316\u67b6\u6784\uff0c\u91c7\u7528\u5206\u5c42\u5b89\u5168\u63aa\u65bd\uff08AES 256\u52a0\u5bc6\u3001\u591a\u56e0\u7d20\u8ba4\u8bc1\u7b49\uff09\u3002", "result": "\u8bc6\u522b\u4e86\u6f5c\u5728\u98ce\u9669\u5e76\u63d0\u51fa\u4e86\u7f13\u89e3\u7b56\u7565\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u5927\u89c4\u6a21\u91c7\u7528\u7684\u969c\u788d\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u63d0\u5347STEM\u6559\u80b2\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u514b\u670d\u6280\u672f\u548c\u6210\u672c\u7b49\u6311\u6218\u3002"}}
{"id": "2509.04084", "pdf": "https://arxiv.org/pdf/2509.04084", "abs": "https://arxiv.org/abs/2509.04084", "authors": ["Chenxuan Yao", "Yuchong Hu", "Feifan Liu", "Zhengyu Liu", "Dan Feng"], "title": "LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems", "categories": ["cs.DC"], "comment": null, "summary": "Distributed training of large deep-learning models often leads to failures,\nso checkpointing is commonly employed for recovery. State-of-the-art studies\nfocus on frequent checkpointing for fast recovery from failures. However, it\ngenerates numerous checkpoints, incurring substantial costs and thus degrading\ntraining performance. Recently, differential checkpointing has been proposed to\nreduce costs, but it is limited to recommendation systems, so its application\nto general distributed training systems remains unexplored.\n  This paper proposes LowDiff, an efficient frequent checkpointing framework\nthat \\textit{reuses} compressed gradients, serving as differential checkpoints\nto reduce cost. Furthermore, LowDiff incorporates a batched gradient write\noptimization to persist these differentials to storage efficiently. It also\ndynamically tunes both the checkpoint frequency and the batching size to\nmaximize performance. We further enhance LowDiff with a layer-wise gradient\nreusing and snapshotting approach and a CPU-based asynchronous persistence\nstrategy, enabling frequent checkpointing without gradient compression.\nExperiments on various workloads show that LowDiff can achieve checkpointing\nfrequency up to per iteration with less than 3.1\\% runtime overhead.", "AI": {"tldr": "LowDiff\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9891\u7e41\u68c0\u67e5\u70b9\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u538b\u7f29\u68af\u5ea6\u4f5c\u4e3a\u5dee\u5206\u68c0\u67e5\u70b9\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u7ed3\u5408\u6279\u91cf\u68af\u5ea6\u5199\u5165\u4f18\u5316\uff0c\u52a8\u6001\u8c03\u6574\u68c0\u67e5\u70b9\u9891\u7387\u548c\u6279\u91cf\u5927\u5c0f\u4ee5\u6700\u5927\u5316\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u9891\u7e41\u68c0\u67e5\u70b9\u751f\u6210\u5927\u91cf\u68c0\u67e5\u70b9\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u6602\u4e14\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u5dee\u5206\u68c0\u67e5\u70b9\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u63a8\u8350\u7cfb\u7edf\uff0c\u672a\u80fd\u5e7f\u6cdb\u7528\u4e8e\u901a\u7528\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86LowDiff\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u538b\u7f29\u68af\u5ea6\u4f5c\u4e3a\u5dee\u5206\u68c0\u67e5\u70b9\uff0c\u5e76\u7ed3\u5408\u6279\u91cf\u68af\u5ea6\u5199\u5165\u4f18\u5316\u548c\u52a8\u6001\u8c03\u6574\u7b56\u7565\u3002\u8fd8\u5f15\u5165\u4e86\u5206\u5c42\u68af\u5ea6\u91cd\u7528\u548cCPU\u5f02\u6b65\u6301\u4e45\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLowDiff\u80fd\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u5b9e\u73b0\u68c0\u67e5\u70b9\uff0c\u8fd0\u884c\u65f6\u5f00\u9500\u4f4e\u4e8e3.1%\u3002", "conclusion": "LowDiff\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9891\u7e41\u68c0\u67e5\u70b9\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.04070", "pdf": "https://arxiv.org/pdf/2509.04070", "abs": "https://arxiv.org/abs/2509.04070", "authors": ["Paresh Baidya", "Rourab Paul", "Vikas Srivastava", "Sumit Kumar Debnath"], "title": "Error Detection Schemes for Barrett Reduction of CT-BU on FPGA in Post Quantum Cryptography", "categories": ["cs.CR", "cs.AR"], "comment": null, "summary": "A fault can occur naturally or intentionally. However, intentionally\ninjecting faults into hardware accelerators of Post-Quantum Cryptographic (PQC)\nalgorithms may leak sensitive information. This intentional fault injection in\nside-channel attacks compromises the reliability of PQC implementations. The\nrecently NIST-standardized key encapsulation mechanism (KEM), Kyber may also\nleak information at the hardware implementation level. This work proposes three\nefficient and lightweight recomputation-based fault detection methods for\nBarrett Reduction in the Cooley-Tukey Butterfly Unit (CT-BU) of Kyber on a\nField Programmable Gate Array (FPGA). The CT-BU and Barrett Reduction are\nfundamental components in structured lattice-based PQC algorithms, including\nKyber, NTRU, Falcon, CRYSTALS-Dilithium, etc. This paper introduces a new\nalgorithm, Recomputation with Swapped Operand (RESWO), for fault detection.\nWhile Recomputation with Negated Operand (RENO) and Recomputation with Shifted\nOperand (RESO) are existing methods used in other PQC hardware algorithms. To\nthe best of our knowledge, RENO and RESO have never been used in Barrett\nReduction before. The proposed RESWO method consumes a similar number of slices\ncompared to RENO and RESO. However, RESWO shows lesser delay compared to both\nRENO and RESO. The fault detection efficiency of RESWO, RENO, and RESO is\nnearly 100%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u57fa\u4e8e\u91cd\u8ba1\u7b97\u7684\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff08RENO\u3001RESO\u548cRESWO\uff09\uff0c\u7528\u4e8eKyber\u7b97\u6cd5\u7684Barrett Reduction\u6a21\u5757\uff0c\u5176\u4e2dRESWO\u662f\u65b0\u65b9\u6cd5\uff0c\u6548\u7387\u63a5\u8fd1100%\u3002", "motivation": "\u91cf\u5b50\u540e\u52a0\u5bc6\uff08PQC\uff09\u7b97\u6cd5\u5728\u786c\u4ef6\u52a0\u901f\u5668\u4e2d\u53ef\u80fd\u56e0\u6545\u610f\u6ce8\u5165\u7684\u6545\u969c\u800c\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u5f71\u54cd\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86RESWO\u7b97\u6cd5\uff08\u91cd\u8ba1\u7b97\u4ea4\u6362\u64cd\u4f5c\u6570\uff09\uff0c\u5e76\u7ed3\u5408\u73b0\u6709RENO\u548cRESO\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8eKyber\u7684Barrett Reduction\u6a21\u5757\u3002", "result": "RESWO\u5ef6\u8fdf\u4f4e\u4e8eRENO\u548cRESO\uff0c\u4e09\u8005\u6545\u969c\u68c0\u6d4b\u6548\u7387\u5747\u63a5\u8fd1100%\u3002", "conclusion": "RESWO\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8ePQC\u7b97\u6cd5\u7684\u786c\u4ef6\u5b9e\u73b0\u3002"}}
{"id": "2509.04078", "pdf": "https://arxiv.org/pdf/2509.04078", "abs": "https://arxiv.org/abs/2509.04078", "authors": ["Jingjing Liu", "Zeming Liu", "Zihao Cheng", "Mengliang He", "Xiaoming Shi", "Yuhang Guo", "Xiangrong Zhu", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": "30 pages, 12 figures, EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86RepoDebug\uff0c\u4e00\u4e2a\u591a\u4efb\u52a1\u3001\u591a\u8bed\u8a00\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8c03\u8bd5\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u4efb\u52a1\u3001\u8bed\u8a00\u548c\u9519\u8bef\u7c7b\u578b\u591a\u6837\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u8bc4\u4f30\u4e8610\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ed3\u5e93\u7ea7\u8c03\u8bd5\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u51fd\u6570\u7ea7\u4ee3\u7801\u4fee\u590d\uff0c\u5ffd\u7565\u4e86\u66f4\u590d\u6742\u548c\u73b0\u5b9e\u7684\u4ed3\u5e93\u7ea7\u573a\u666f\uff0c\u5bfc\u81f4\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ed3\u5e93\u7ea7\u8c03\u8bd5\u4e2d\u7684\u6311\u6218\u7406\u89e3\u4e0d\u5b8c\u6574\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRepoDebug\u7684\u6570\u636e\u96c6\uff0c\u652f\u63018\u79cd\u7f16\u7a0b\u8bed\u8a00\u30013\u79cd\u8c03\u8bd5\u4efb\u52a1\u548c22\u79cd\u9519\u8bef\u5b50\u7c7b\u578b\u3002", "result": "\u8bc4\u4f30\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578bClaude 3.5 Sonnect\uff0c\u5728\u4ed3\u5e93\u7ea7\u8c03\u8bd5\u4e2d\u4ecd\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "RepoDebug\u586b\u8865\u4e86\u4ed3\u5e93\u7ea7\u8c03\u8bd5\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u9886\u57df\u7684\u8868\u73b0\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2509.04145", "pdf": "https://arxiv.org/pdf/2509.04145", "abs": "https://arxiv.org/abs/2509.04145", "authors": ["Dongliang Cao", "Guoxing Sun", "Marc Habermann", "Florian Bernard"], "title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4e2a\u6027\u5316\u6e32\u67d3\u4e0e\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u9ad8\u771f\u5b9e\u611f\u548c\u59ff\u6001\u76f8\u5173\u53d8\u5f62\u751f\u6210\u52a8\u6001\u4eba\u50cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u9ad8\u771f\u5b9e\u611f\u548c\u8de8\u8eab\u4efd\u7684\u52a8\u6001\u4eba\u50cf\u751f\u6210\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u4f18\u5316\u4e2a\u6027\u5316UNet\u6355\u6349\u59ff\u6001\u7ec6\u8282\uff0c\u518d\u8bad\u7ec3\u8d85\u6269\u6563\u6a21\u578b\u751f\u6210\u7f51\u7edc\u6743\u91cd\u3002", "result": "\u5728\u5927\u89c4\u6a21\u591a\u89c6\u89d2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u52a8\u6001\u4eba\u50cf\u751f\u6210\u3002"}}
{"id": "2509.04086", "pdf": "https://arxiv.org/pdf/2509.04086", "abs": "https://arxiv.org/abs/2509.04086", "authors": ["Yaru Chen", "Faegheh Sardari", "Peiliang Zhang", "Ruohao Guo", "Yang Xiang", "Zhenbo Li", "Wenwu Wang"], "title": "TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Audio-Visual Video Parsing (AVVP) task aims to identify event categories and\ntheir occurrence times in a given video with weakly supervised labels. Existing\nmethods typically fall into two categories: (i) designing enhanced\narchitectures based on attention mechanism for better temporal modeling, and\n(ii) generating richer pseudo-labels to compensate for the absence of\nframe-level annotations. However, the first type methods treat noisy\nsegment-level pseudo labels as reliable supervision and the second type methods\nlet indiscriminate attention spread them across all frames, the initial errors\nare repeatedly amplified during training. To address this issue, we propose a\nmethod that combines the Bi-Directional Text Fusion (BiT) module and\nCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate the\nstrengths and complementarity of the two previous research directions. We first\nperform semantic injection and dynamic calibration on audio and visual modality\nfeatures through the BiT module, to locate and purify cleaner and richer\nsemantic cues. Then, we leverage the CATS module for semantic propagation and\nconnection to enable precise semantic information dissemination across time.\nExperimental results demonstrate that our proposed method achieves\nstate-of-the-art (SOTA) performance in multiple key indicators on two benchmark\ndatasets, LLP and UnAV-100.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u5411\u6587\u672c\u878d\u5408\uff08BiT\uff09\u6a21\u5757\u548c\u7c7b\u522b\u611f\u77e5\u65f6\u5e8f\u56fe\uff08CATS\uff09\u6a21\u5757\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\uff08AVVP\uff09\u4efb\u52a1\u4e2d\u566a\u58f0\u6807\u7b7e\u548c\u6ce8\u610f\u529b\u5206\u6563\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AVVP\u65b9\u6cd5\u5728\u566a\u58f0\u6807\u7b7e\u5904\u7406\u548c\u6ce8\u610f\u529b\u673a\u5236\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8bef\u5dee\u88ab\u653e\u5927\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u7814\u7a76\u65b9\u5411\u7684\u4f18\u52bf\uff0c\u63d0\u5347\u89e3\u6790\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7BiT\u6a21\u5757\u5bf9\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u7279\u5f81\u8fdb\u884c\u8bed\u4e49\u6ce8\u5165\u548c\u52a8\u6001\u6821\u51c6\uff0c\u5b9a\u4f4d\u5e76\u51c0\u5316\u8bed\u4e49\u7ebf\u7d22\uff1b\u5229\u7528CATS\u6a21\u5757\u8fdb\u884c\u8bed\u4e49\u4f20\u64ad\u548c\u8fde\u63a5\uff0c\u5b9e\u73b0\u8de8\u65f6\u95f4\u7684\u7cbe\u786e\u8bed\u4e49\u4fe1\u606f\u5206\u53d1\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728LLP\u548cUnAV-100\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408BiT\u548cCATS\u6a21\u5757\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u63d0\u5347\u4e86AVVP\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
