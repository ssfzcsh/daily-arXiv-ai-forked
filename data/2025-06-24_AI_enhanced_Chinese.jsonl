{"id": "2506.17306", "pdf": "https://arxiv.org/pdf/2506.17306", "abs": "https://arxiv.org/abs/2506.17306", "authors": ["Jake Zappin", "Trevor Stalnaker", "Oscar Chaparro", "Denys Poshyvanyk"], "title": "Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners", "categories": ["cs.SE"], "comment": null, "summary": "Quantum software engineering is an emerging discipline with distinct\nchallenges, particularly in testing and debugging. As quantum computing\ntransitions from theory to implementation, developers face issues not present\nin classical software development, such as probabilistic execution, limited\nobservability, shallow abstractions, and low awareness of quantum-specific\ntools. To better understand current practices, we surveyed 26 quantum software\ndevelopers from academia and industry and conducted follow-up interviews\nfocused on testing, debugging, and recurring challenges. All participants\nreported engaging in testing, with unit testing (88%), regression testing\n(54%), and acceptance testing (54%) being the most common. However, only 31%\nreported using quantum-specific testing tools, relying instead on manual\nmethods. Debugging practices were similarly grounded in classical strategies,\nsuch as print statements, circuit visualizations, and simulators, which\nrespondents noted do not scale well. The most frequently cited sources of bugs\nwere classical in nature-library updates (81%), developer mistakes (68%), and\ncompatibility issues (62%)-often worsened by limited abstraction in existing\nSDKs. These findings highlight the urgent need for better-aligned testing and\ndebugging tools, integrated more seamlessly into the workflows of quantum\ndevelopers. We present these results in detail and offer actionable\nrecommendations grounded in the real-world needs of practitioners.", "AI": {"tldr": "\u91cf\u5b50\u8f6f\u4ef6\u5de5\u7a0b\u9762\u4e34\u6d4b\u8bd5\u548c\u8c03\u8bd5\u7684\u72ec\u7279\u6311\u6218\uff0c\u5f00\u53d1\u8005\u591a\u4f9d\u8d56\u7ecf\u5178\u65b9\u6cd5\u800c\u975e\u91cf\u5b50\u4e13\u7528\u5de5\u5177\uff0c\u4e9f\u9700\u6539\u8fdb\u5de5\u5177\u548c\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u4ece\u7406\u8bba\u8f6c\u5411\u5b9e\u73b0\uff0c\u5f00\u53d1\u8005\u9762\u4e34\u6982\u7387\u6267\u884c\u3001\u6709\u9650\u53ef\u89c2\u6d4b\u6027\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e86\u89e3\u5f53\u524d\u5b9e\u8df5\u4ee5\u6539\u8fdb\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u8c03\u67e526\u540d\u91cf\u5b50\u8f6f\u4ef6\u5f00\u53d1\u8005\u548c\u540e\u7eed\u8bbf\u8c08\uff0c\u805a\u7126\u6d4b\u8bd5\u3001\u8c03\u8bd5\u548c\u5e38\u89c1\u6311\u6218\u3002", "result": "\u591a\u6570\u5f00\u53d1\u8005\u4f7f\u7528\u7ecf\u5178\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u91cf\u5b50\u4e13\u7528\u5de5\u5177\u4f7f\u7528\u7387\u4f4e\uff0831%\uff09\uff0c\u5e38\u89c1bug\u6e90\u4e8e\u7ecf\u5178\u95ee\u9898\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u8d34\u5408\u91cf\u5b50\u5f00\u53d1\u8005\u9700\u6c42\u7684\u6d4b\u8bd5\u548c\u8c03\u8bd5\u5de5\u5177\uff0c\u5e76\u63d0\u4f9b\u5b9e\u8df5\u5bfc\u5411\u7684\u5efa\u8bae\u3002"}}
{"id": "2506.17313", "pdf": "https://arxiv.org/pdf/2506.17313", "abs": "https://arxiv.org/abs/2506.17313", "authors": ["Jonathan Reif", "Daniel Dittler", "Milapji Singh Gill", "Tam\u00e1s Farkas", "Valentin Stegmaier", "Felix Gehlhoff", "Tobias Kleinert", "Michael Weyrich"], "title": "An Expert Survey on Models and Digital Twins", "categories": ["cs.SE"], "comment": "This article is accepted at CIRP ICME and for publication in Procedia\n  CIRP", "summary": "Digital Twins (DTs) are becoming increasingly vital for future industrial\napplications, enhancing monitoring, control, and optimization of physical\nassets. This enhancement is made possible by integrating various Digital Models\n(DMs) within DTs, which must interoperate to represent different system aspects\nand fulfill diverse application purposes. However, industry perspectives on the\nchallenges and research needs for integrating these models are rarely obtained.\nThus, this study conducts an expert survey across multiple application domains\nto identify and analyze the challenges in utilizing diverse DMs within DTs. The\nresults reveal missing standardized interfaces, high manual adaptation effort,\nand limited support for model reuse across lifecycle phases, highlighting\nfuture research needs in automated model composition and semantics-based\ninteroperability.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4e13\u5bb6\u8c03\u67e5\u63ed\u793a\u4e86\u6570\u5b57\u5b6a\u751f\u4e2d\u6570\u5b57\u6a21\u578b\u96c6\u6210\u7684\u6311\u6218,\u5305\u62ec\u7f3a\u4e4f\u6807\u51c6\u5316\u63a5\u53e3\u3001\u9ad8\u4eba\u5de5\u9002\u914d\u6210\u672c\u548c\u6a21\u578b\u91cd\u7528\u652f\u6301\u4e0d\u8db3,\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u96c6\u6210\u591a\u79cd\u6570\u5b57\u6a21\u578b,\u4f46\u884c\u4e1a\u5bf9\u5176\u6311\u6218\u548c\u7814\u7a76\u9700\u6c42\u7f3a\u4e4f\u7cfb\u7edf\u4e86\u89e3\u3002", "method": "\u901a\u8fc7\u8de8\u9886\u57df\u7684\u4e13\u5bb6\u8c03\u67e5,\u5206\u6790\u6570\u5b57\u6a21\u578b\u5728\u6570\u5b57\u5b6a\u751f\u4e2d\u7684\u5e94\u7528\u6311\u6218\u3002", "result": "\u53d1\u73b0\u6807\u51c6\u5316\u63a5\u53e3\u7f3a\u5931\u3001\u624b\u52a8\u9002\u914d\u6210\u672c\u9ad8\u3001\u6a21\u578b\u91cd\u7528\u652f\u6301\u6709\u9650\u7b49\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u9700\u7814\u7a76\u81ea\u52a8\u5316\u6a21\u578b\u7ec4\u5408\u548c\u57fa\u4e8e\u8bed\u4e49\u7684\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2506.17330", "pdf": "https://arxiv.org/pdf/2506.17330", "abs": "https://arxiv.org/abs/2506.17330", "authors": ["Simon Thorne"], "title": "Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE", "categories": ["cs.SE"], "comment": "18 Pages, 10 Tables, 1 Colour Figure", "summary": "Large Language Models (LLMs) have demonstrated some significant capabilities\nacross various domains; however, their effectiveness in spreadsheet related\ntasks remains underexplored. This study introduces a foundation for a\ncomprehensive benchmark framework to evaluate the performance of leading LLMs\nin executing spreadsheet functions, formula generation and data manipulation\ntasks. The benchmark encompasses tasks ranging from basic formula creation to\ncomplex, real world spreadsheet scenarios. Our findings reveal that while LLMs\nexhibit proficiency in straightforward tasks, they often falter in complex,\nmulti step operations, frequently producing plausible yet incorrect outputs.\nThese results underscore the limitations of current LLMs in handling\nspreadsheet tasks that require precise logical reasoning and highlight the need\nfor integrating symbolic reasoning capabilities into LLM architectures. To\nsupport this, we introduce FLARE (Formula Logic, Auditing, Reasoning and\nEvaluation) a new benchmark for evaluating LLM performance on real-world\nspreadsheet logic, auditing, and reasoning tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7535\u5b50\u8868\u683c\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u57fa\u51c6\u6846\u67b6FLARE\uff0c\u53d1\u73b0LLMs\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u7535\u5b50\u8868\u683c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6db5\u76d6\u4ece\u57fa\u7840\u516c\u5f0f\u751f\u6210\u5230\u590d\u6742\u573a\u666f\u7684\u57fa\u51c6\u8bc4\u4f30\u6846\u67b6FLARE\u3002", "result": "LLMs\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u8f93\u51fa\u3002", "conclusion": "\u5f53\u524d\u7684LLMs\u5728\u9700\u8981\u7cbe\u786e\u903b\u8f91\u63a8\u7406\u7684\u7535\u5b50\u8868\u683c\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u6574\u5408\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.17335", "pdf": "https://arxiv.org/pdf/2506.17335", "abs": "https://arxiv.org/abs/2506.17335", "authors": ["Shuo Yan", "Ruochen Li", "Ziming Luo", "Zimu Wang", "Daoyang Li", "Liqiang Jing", "Kaiyu He", "Peilin Wu", "George Michalopoulos", "Yue Zhang", "Ziyang Zhang", "Mian Zhang", "Zhiyu Chen", "Xinya Du"], "title": "LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language model (LLM) agents have demonstrated remarkable potential in\nadvancing scientific discovery. However, their capability in the fundamental\nyet crucial task of reproducing code from research papers, especially in the\nNLP domain, remains underexplored. This task includes unique complex reasoning\nchallenges in the intellectual synthesis of abstract concepts and the\ncomprehension of code repositories with interdependent files. Motivated by this\ngap, we present LMR-BENCH, a benchmark designed to systematically evaluate the\ncapability of LLM agents on code reproduction from Language Modeling Research.\nIt consists of 28 code reproduction tasks derived from 23 research papers\npublished in top-tier NLP venues over the past five years, spanning nine\nfundamental categories. Models are provided with a research paper, a code\nrepository containing one or more masked functions, and instructions for\nimplementing these functions. We conduct extensive experiments in standard\nprompting and LLM agent settings with state-of-the-art LLMs, evaluating the\naccuracy of unit tests and performing LLM-based evaluation of code correctness.\nExperimental results reveal that even the most advanced models still exhibit\npersistent limitations in scientific reasoning and code synthesis, highlighting\ncritical gaps in LLM agents' ability to autonomously reproduce scientific\nresearch", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLMR-BENCH\u57fa\u51c6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u4eceNLP\u7814\u7a76\u8bba\u6587\u4e2d\u590d\u73b0\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u5408\u6210\u65b9\u9762\u4ecd\u6709\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u76ee\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u4ece\u7814\u7a76\u8bba\u6587\u4e2d\u590d\u73b0\u4ee3\u7801\u8fd9\u4e00\u57fa\u7840\u4f46\u5173\u952e\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728NLP\u9886\u57df\u3002", "method": "\u63d0\u51faLMR-BENCH\u57fa\u51c6\uff0c\u5305\u542b28\u9879\u4ee3\u7801\u590d\u73b0\u4efb\u52a1\uff0c\u6db5\u76d623\u7bc7\u8fc7\u53bb\u4e94\u5e74\u9876\u7ea7NLP\u4f1a\u8bae\u8bba\u6587\u7684\u4e5d\u7c7b\u57fa\u7840\u4efb\u52a1\u3002\u6a21\u578b\u9700\u6839\u636e\u8bba\u6587\u3001\u542b\u63a9\u7801\u51fd\u6570\u7684\u4ee3\u7801\u4ed3\u5e93\u548c\u5b9e\u73b0\u6307\u4ee4\u5b8c\u6210\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u5728\u6807\u51c6\u63d0\u793a\u548c\u6700\u65b0LLM\u4ee3\u7406\u8bbe\u7f6e\u4e0b\uff0c\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u5408\u6210\u65b9\u9762\u4ecd\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002", "conclusion": "LLM\u4ee3\u7406\u5728\u81ea\u4e3b\u590d\u73b0\u79d1\u5b66\u7814\u7a76\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u6709\u91cd\u5927\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u5408\u6210\u7684\u80fd\u529b\u3002"}}
{"id": "2506.17471", "pdf": "https://arxiv.org/pdf/2506.17471", "abs": "https://arxiv.org/abs/2506.17471", "authors": ["Kaushik Kulkarni", "Andreas Kl\u00f6ckner"], "title": "Code Generation for Near-Roofline Finite Element Actions on GPUs from Symbolic Variational Forms", "categories": ["cs.DC", "cs.MS", "cs.NA", "cs.PF", "math.NA", "65Y05"], "comment": null, "summary": "We present a novel parallelization strategy for evaluating Finite Element\nMethod (FEM) variational forms on GPUs, focusing on those that are expressible\nthrough the Unified Form Language (UFL) on simplex meshes. We base our approach\non code transformations, wherein we construct a space of scheduling candidates\nand rank them via a heuristic cost model to effectively handle the large\ndiversity of computational workloads that can be expressed in this way. We\npresent a design of a search space to which the cost model is applied, along\nwith an associated pruning strategy to limit the number of configurations that\nneed to be empirically evaluated. The goal of our design is to strike a balance\nbetween the device's latency-hiding capabilities and the amount of state space,\na key factor in attaining near-roofline performance.\n  To make our work widely available, we have prototyped our parallelization\nstrategy within the \\textsc{Firedrake} framework, a UFL-based FEM solver. We\nevaluate the performance of our parallelization scheme on two generations of\nNvidia GPUs, specifically the Titan V (Volta architecture) and Tesla K40c\n(Kepler architecture), across a range of operators commonly used in\napplications, including fluid dynamics, wave propagation, and structural\nmechanics, in 2D and 3D geometries. Our results demonstrate that our proposed\nalgorithm achieves more than $50\\%$ roofline performance in $65\\%$ of the test\ncases on both devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eGPU\u4e0a\u8bc4\u4f30\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u53d8\u5206\u5f62\u5f0f\u7684\u5e76\u884c\u5316\u7b56\u7565\uff0c\u901a\u8fc7UFL\u8bed\u8a00\u5728\u5355\u7eaf\u5f62\u7f51\u683c\u4e0a\u5b9e\u73b0\u3002", "motivation": "\u76ee\u6807\u662f\u901a\u8fc7\u4ee3\u7801\u53d8\u6362\u548c\u542f\u53d1\u5f0f\u6210\u672c\u6a21\u578b\u5904\u7406\u591a\u6837\u5316\u7684\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u4ee5\u5e73\u8861\u5ef6\u8fdf\u9690\u85cf\u548c\u72b6\u6001\u7a7a\u95f4\uff0c\u4ece\u800c\u8fbe\u5230\u63a5\u8fd1\u5c4b\u9876\u7ebf\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8c03\u5ea6\u5019\u9009\u7a7a\u95f4\u53ca\u5176\u526a\u679d\u7b56\u7565\uff0c\u7ed3\u5408\u6210\u672c\u6a21\u578b\u4f18\u5316\u914d\u7f6e\u9009\u62e9\uff0c\u5e76\u5728Firedrake\u6846\u67b6\u4e2d\u5b9e\u73b0\u539f\u578b\u3002", "result": "\u5728Titan V\u548cTesla K40c GPU\u4e0a\u6d4b\u8bd5\uff0c65%\u7684\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc750%\u7684\u5c4b\u9876\u7ebf\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2506.17506", "pdf": "https://arxiv.org/pdf/2506.17506", "abs": "https://arxiv.org/abs/2506.17506", "authors": ["Lesheng Jin", "Zhenyuan Ruan", "Haohui Mai", "Jingbo Shang"], "title": "VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM", "categories": ["cs.CL", "cs.OS"], "comment": null, "summary": "Modern GPUs evolve rapidly, yet production compilers still rely on\nhand-crafted register allocation heuristics that require substantial re-tuning\nfor each hardware generation. We introduce VeriLocc, a framework that combines\nlarge language models (LLMs) with formal compiler techniques to enable\ngeneralizable and verifiable register allocation across GPU architectures.\nVeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)\ninto target-specific register assignments, aided by static analysis for\ncross-architecture normalization and generalization and a verifier-guided\nregeneration loop to ensure correctness. Evaluated on matrix multiplication\n(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot\naccuracy and near-100% pass@100. Case study shows that VeriLocc discovers more\nperformant assignments than expert-tuned libraries, outperforming rocBLAS by\nover 10% in runtime.", "AI": {"tldr": "VeriLocc\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5f62\u5f0f\u7f16\u8bd1\u5668\u6280\u672f\uff0c\u5b9e\u73b0\u8de8GPU\u67b6\u6784\u7684\u53ef\u9a8c\u8bc1\u5bc4\u5b58\u5668\u5206\u914d\uff0c\u6027\u80fd\u4f18\u4e8e\u4eba\u5de5\u8c03\u4f18\u5e93\u3002", "motivation": "\u73b0\u4ee3GPU\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u751f\u4ea7\u7f16\u8bd1\u5668\u4ecd\u4f9d\u8d56\u4eba\u5de5\u8c03\u4f18\u7684\u5bc4\u5b58\u5668\u5206\u914d\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u9700\u9488\u5bf9\u6bcf\u4ee3\u786c\u4ef6\u91cd\u65b0\u8c03\u4f18\u3002", "method": "VeriLocc\u901a\u8fc7\u5fae\u8c03LLM\u5c06\u4e2d\u95f4\u8868\u793a\uff08MIRs\uff09\u8f6c\u6362\u4e3a\u76ee\u6807\u5bc4\u5b58\u5668\u5206\u914d\uff0c\u8f85\u4ee5\u9759\u6001\u5206\u6790\u548c\u9a8c\u8bc1\u5f15\u5bfc\u7684\u518d\u751f\u5faa\u73af\u3002", "result": "\u5728GEMM\u548cMHA\u4efb\u52a1\u4e2d\uff0cVeriLocc\u5355\u6b21\u51c6\u786e\u738785-99%\uff0cpass@100\u63a5\u8fd1100%\uff0c\u6027\u80fd\u6bd4rocBLAS\u9ad810%\u4ee5\u4e0a\u3002", "conclusion": "VeriLocc\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u53ef\u9a8c\u8bc1\u7684\u5bc4\u5b58\u5668\u5206\u914d\u65b9\u6cd5\uff0c\u6027\u80fd\u8d85\u8d8a\u4e13\u5bb6\u8c03\u4f18\u5e93\u3002"}}
{"id": "2506.17494", "pdf": "https://arxiv.org/pdf/2506.17494", "abs": "https://arxiv.org/abs/2506.17494", "authors": ["Ji-Youn Jung", "Devansh Saxena", "Minjung Park", "Jini Kim", "Jodi Forlizzi", "Kenneth Holstein", "John Zimmerman"], "title": "Making the Right Thing: Bridging HCI and Responsible AI in Early-Stage AI Concept Selection", "categories": ["cs.HC"], "comment": "Accepted for publication in Designing Interactive Systems Conference\n  (DIS '25), July 5--9, 2025, Funchal, Portugal. ACM, New York, NY, USA, 21\n  pages", "summary": "AI projects often fail due to financial, technical, ethical, or user\nacceptance challenges -- failures frequently rooted in early-stage decisions.\nWhile HCI and Responsible AI (RAI) research emphasize this, practical\napproaches for identifying promising concepts early remain limited. Drawing on\nResearch through Design, this paper investigates how early-stage AI concept\nsorting in commercial settings can reflect RAI principles. Through three design\nexperiments -- including a probe study with industry practitioners -- we\nexplored methods for evaluating risks and benefits using multidisciplinary\ncollaboration. Participants demonstrated strong receptivity to addressing RAI\nconcerns early in the process and effectively identified low-risk, high-benefit\nAI concepts. Our findings highlight the potential of a design-led approach to\nembed ethical and service design thinking at the front end of AI innovation. By\nexamining how practitioners reason about AI concepts, our study invites HCI and\nRAI communities to see early-stage innovation as a critical space for engaging\nethical and commercial considerations together.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728AI\u521b\u65b0\u7684\u65e9\u671f\u9636\u6bb5\u901a\u8fc7\u8bbe\u8ba1\u5b9e\u9a8c\u548c\u591a\u5b66\u79d1\u5408\u4f5c\uff0c\u6709\u6548\u5730\u8bc6\u522b\u5e76\u6574\u5408\u8d1f\u8d23\u4efbAI\uff08RAI\uff09\u539f\u5219\uff0c\u4ee5\u51cf\u5c11\u5931\u8d25\u98ce\u9669\u5e76\u63d0\u5347\u9879\u76ee\u6210\u529f\u7387\u3002", "motivation": "AI\u9879\u76ee\u5e38\u56e0\u65e9\u671f\u51b3\u7b56\u5931\u8bef\u800c\u5931\u8d25\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u5728\u65e9\u671f\u9636\u6bb5\u8bc6\u522b\u6709\u6f5c\u529b\u7684\u6982\u5ff5\u5e76\u878d\u5165RAI\u539f\u5219\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u8bbe\u8ba1\u5b9e\u9a8c\uff08\u5305\u62ec\u4e0e\u884c\u4e1a\u4ece\u4e1a\u8005\u7684\u63a2\u9488\u7814\u7a76\uff09\uff0c\u91c7\u7528\u591a\u5b66\u79d1\u534f\u4f5c\u7684\u65b9\u6cd5\u8bc4\u4f30AI\u6982\u5ff5\u7684\u98ce\u9669\u4e0e\u6536\u76ca\u3002", "result": "\u53c2\u4e0e\u8005\u8868\u73b0\u51fa\u5bf9\u65e9\u671f\u5904\u7406RAI\u95ee\u9898\u7684\u9ad8\u5ea6\u63a5\u53d7\u5ea6\uff0c\u5e76\u80fd\u6709\u6548\u8bc6\u522b\u4f4e\u98ce\u9669\u3001\u9ad8\u6536\u76ca\u7684AI\u6982\u5ff5\u3002", "conclusion": "\u8bbe\u8ba1\u4e3b\u5bfc\u7684\u65b9\u6cd5\u5728AI\u521b\u65b0\u524d\u7aef\u5d4c\u5165\u4f26\u7406\u548c\u670d\u52a1\u8bbe\u8ba1\u601d\u7ef4\u5177\u6709\u6f5c\u529b\uff0c\u547c\u5401HCI\u548cRAI\u793e\u533a\u5171\u540c\u5173\u6ce8\u65e9\u671f\u521b\u65b0\u7684\u4f26\u7406\u4e0e\u5546\u4e1a\u8003\u91cf\u3002"}}
{"id": "2506.17623", "pdf": "https://arxiv.org/pdf/2506.17623", "abs": "https://arxiv.org/abs/2506.17623", "authors": ["Yuesheng Huang", "Peng Zhang", "Riliang Liu", "Jiaqi Liang"], "title": "Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?", "categories": ["cs.MM", "cs.CV"], "comment": "4 figures,7 tables", "summary": "A significant ``modality gap\" exists between the abundance of text-only data\nand the increasing power of multimodal models. This work systematically\ninvestigates whether images generated on-the-fly by Text-to-Image (T2I) models\ncan serve as a valuable complementary modality for text-centric tasks. Through\na comprehensive evaluation framework on text classification, we analyze the\nimpact of critical variables, including T2I model quality, prompt engineering\nstrategies, and multimodal fusion architectures. Our findings demonstrate that\nthis``synthetic perception\" can yield significant performance gains, even when\naugmenting strong large language model baselines. However, we find the\neffectiveness of this approach is highly conditional, depending critically on\nthe semantic alignment between text and the generated image, the inherent\n``visual groundability\" of the task, and the generative fidelity of the T2I\nmodel. Our work establishes the first rigorous benchmark for this paradigm,\nproviding a clear analysis of its potential and current limitations, and\ndemonstrating its viability as a pathway to enrich language understanding in\ntraditionally unimodal scenarios.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u53ef\u4ee5\u4f5c\u4e3a\u6587\u672c\u4e2d\u5fc3\u4efb\u52a1\u7684\u6709\u6548\u8865\u5145\u6a21\u6001\uff0c\u4f46\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6587\u672c\u4e0e\u751f\u6210\u56fe\u50cf\u7684\u8bed\u4e49\u5bf9\u9f50\u3001\u4efb\u52a1\u7684\u2018\u89c6\u89c9\u53ef\u63a5\u5730\u6027\u2019\u4ee5\u53caT2I\u6a21\u578b\u7684\u751f\u6210\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5f25\u8865\u6587\u672c\u6570\u636e\u4e0e\u591a\u6a21\u6001\u6a21\u578b\u4e4b\u95f4\u7684\u2018\u6a21\u6001\u5dee\u8ddd\u2019\uff0c\u63a2\u7d22\u52a8\u6001\u751f\u6210\u7684\u56fe\u50cf\u662f\u5426\u80fd\u4e3a\u7eaf\u6587\u672c\u4efb\u52a1\u63d0\u4f9b\u989d\u5916\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790T2I\u6a21\u578b\u8d28\u91cf\u3001\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u548c\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\u7684\u5f71\u54cd\u3002", "result": "\u2018\u5408\u6210\u611f\u77e5\u2019\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4f46\u6548\u679c\u53d7\u9650\u4e8e\u8bed\u4e49\u5bf9\u9f50\u3001\u4efb\u52a1\u89c6\u89c9\u53ef\u63a5\u5730\u6027\u548cT2I\u6a21\u578b\u8d28\u91cf\u3002", "conclusion": "\u8be5\u8303\u5f0f\u4e3a\u4e30\u5bcc\u5355\u6a21\u6001\u573a\u666f\u4e0b\u7684\u8bed\u8a00\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u8bed\u4e49\u5bf9\u9f50\u548c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.17276", "pdf": "https://arxiv.org/pdf/2506.17276", "abs": "https://arxiv.org/abs/2506.17276", "authors": ["Alexandre Le Nepvou"], "title": "Modal Logic for Stratified Becoming: Actualization Beyond Possible Worlds", "categories": ["cs.LO", "cs.AI"], "comment": "This paper develops the formal logical foundations of the stratified\n  actualization framework presented in a companion paper currently under review\n  at Erkenntnis (manuscript ID: ERKE-D-25-00410)", "summary": "This article develops a novel framework for modal logic based on the idea of\nstratified actualization, rather than the classical model of global possible\nworlds. Traditional Kripke semantics treat modal operators as quantification\nover fully determinate alternatives, neglecting the local, dynamic, and often\nasymmetric nature of actualization processes. We propose a system Stratified\nActualization Logic (SAL) in which modalities are indexed by levels of\nontological stability, interpreted as admissibility regimes. Each modality\noperates over a structured layer of possibility, grounded in the internal\ncoherence of transitions between layers. We formally define the syntax and\nsemantics of SAL, introduce its axioms, and prove soundness and completeness.\nApplications are discussed in connection with temporal becoming, quantum\ndecoherence domains, and modal metaphysics. The result is a logic that captures\nthe ontological structure of actualization without recourse to abstract\npossible worlds, offering a stratified alternative to standard modal realism.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u5b9e\u73b0\u7684\u65b0\u6a21\u6001\u903b\u8f91\u6846\u67b6\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u5168\u5c40\u53ef\u80fd\u4e16\u754c\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u514b\u91cc\u666e\u514b\u8bed\u4e49\u5b66\u65e0\u6cd5\u6355\u6349\u5c40\u90e8\u3001\u52a8\u6001\u548c\u975e\u5bf9\u79f0\u7684\u5b9e\u9645\u5316\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u5206\u5c42\u5b9e\u73b0\u903b\u8f91\uff08SAL\uff09\uff0c\u901a\u8fc7\u7d22\u5f15\u6a21\u6001\u8fd0\u7b97\u7b26\u5230\u672c\u4f53\u7a33\u5b9a\u6027\u5c42\u7ea7\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u7684\u53ef\u80fd\u6027\u5c42\u3002", "result": "\u8bc1\u660e\u4e86SAL\u7684\u8bed\u6cd5\u3001\u8bed\u4e49\u3001\u516c\u7406\u53ca\u5b8c\u5907\u6027\uff0c\u5e76\u5e94\u7528\u4e8e\u65f6\u95f4\u6f14\u53d8\u3001\u91cf\u5b50\u9000\u76f8\u5e72\u7b49\u9886\u57df\u3002", "conclusion": "SAL\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u62bd\u8c61\u53ef\u80fd\u4e16\u754c\u7684\u5206\u5c42\u6a21\u6001\u903b\u8f91\uff0c\u4e30\u5bcc\u4e86\u6a21\u6001\u5b9e\u5728\u8bba\u3002"}}
{"id": "2506.17259", "pdf": "https://arxiv.org/pdf/2506.17259", "abs": "https://arxiv.org/abs/2506.17259", "authors": ["Sebastian Barros"], "title": "The Case for a Horizontal Federated AI operating System for Telcos", "categories": ["cs.NI"], "comment": "22 pages", "summary": "As artificial intelligence capabilities rapidly advance, Telco operators face\na growing need to unify fragmented AI efforts across customer experience,\nnetwork operations, and service orchestration. This paper proposes the design\nand deployment of a horizontal federated AI operating system tailored for the\ntelecommunications domain. Unlike vertical vendor-driven platforms, this system\nacts as a common execution and coordination layer, enabling Telcos to deploy AI\nagents at scale while preserving data locality, regulatory compliance, and\narchitectural heterogeneity. We argue that such an operating system must expose\ntightly scoped abstractions for telemetry ingestion, agent execution, and model\nlifecycle management. It should support federated training across sovereign\noperators, offer integration hooks into existing OSS and BSS systems, and\ncomply with TM Forum and O-RAN standards. Importantly, the platform must be\ngoverned through a neutral foundation model to ensure portability,\ncompatibility, and multi-vendor extensibility. This architecture offers a path\nto break the current silos, unlock ecosystem-level intelligence, and provide a\nfoundation for agent-based automation across the Telco stack. The case for this\nhorizontal layer is not only technical but structural, redefining how\nintelligence is deployed and composed in a distributed network environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u7535\u4fe1\u9886\u57df\u7684\u6a2a\u5411\u8054\u90a6AI\u64cd\u4f5c\u7cfb\u7edf\uff0c\u65e8\u5728\u7edf\u4e00\u5206\u6563\u7684AI\u5e94\u7528\uff0c\u652f\u6301\u6570\u636e\u672c\u5730\u5316\u548c\u591a\u5382\u5546\u6269\u5c55\u3002", "motivation": "\u7535\u4fe1\u8fd0\u8425\u5546\u9700\u8981\u6574\u5408\u5206\u6563\u7684AI\u80fd\u529b\u4ee5\u63d0\u5347\u5ba2\u6237\u4f53\u9a8c\u3001\u7f51\u7edc\u8fd0\u8425\u548c\u670d\u52a1\u7f16\u6392\u3002", "method": "\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e00\u4e2a\u6a2a\u5411\u8054\u90a6AI\u64cd\u4f5c\u7cfb\u7edf\uff0c\u63d0\u4f9b\u901a\u7528\u6267\u884c\u548c\u534f\u8c03\u5c42\uff0c\u652f\u6301\u8054\u90a6\u8bad\u7ec3\u548c\u73b0\u6709\u7cfb\u7edf\u7684\u96c6\u6210\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u6253\u7834\u73b0\u6709\u5b64\u5c9b\uff0c\u5b9e\u73b0\u8de8\u8fd0\u8425\u5546\u7684\u667a\u80fd\u534f\u540c\u548c\u81ea\u52a8\u5316\u3002", "conclusion": "\u8be5\u64cd\u4f5c\u7cfb\u7edf\u4e0d\u4ec5\u6280\u672f\u4e0a\u53ef\u884c\uff0c\u8fd8\u80fd\u91cd\u6784\u5206\u5e03\u5f0f\u7f51\u7edc\u4e2d\u667a\u80fd\u90e8\u7f72\u4e0e\u7ec4\u5408\u7684\u65b9\u5f0f\u3002"}}
{"id": "2506.17301", "pdf": "https://arxiv.org/pdf/2506.17301", "abs": "https://arxiv.org/abs/2506.17301", "authors": ["Guian Fang", "Yuchao Gu", "Mike Zheng Shou"], "title": "FramePrompt: In-context Controllable Animation with Zero Structural Changes", "categories": ["cs.GR"], "comment": "Project page: https://frameprompt.github.io/", "summary": "Generating controllable character animation from a reference image and motion\nguidance remains a challenging task due to the inherent difficulty of injecting\nappearance and motion cues into video diffusion models. Prior works often rely\non complex architectures, explicit guider modules, or multi-stage processing\npipelines, which increase structural overhead and hinder deployment. Inspired\nby the strong visual context modeling capacity of pre-trained video diffusion\ntransformers, we propose FramePrompt, a minimalist yet powerful framework that\ntreats reference images, skeleton-guided motion, and target video clips as a\nunified visual sequence. By reformulating animation as a conditional future\nprediction task, we bypass the need for guider networks and structural\nmodifications. Experiments demonstrate that our method significantly\noutperforms representative baselines across various evaluation metrics while\nalso simplifying training. Our findings highlight the effectiveness of\nsequence-level visual conditioning and demonstrate the potential of pre-trained\nmodels for controllable animation without architectural changes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFramePrompt\u7684\u7b80\u6d01\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u53c2\u8003\u56fe\u50cf\u3001\u9aa8\u9abc\u5f15\u5bfc\u8fd0\u52a8\u548c\u76ee\u6807\u89c6\u9891\u7247\u6bb5\u89c6\u4e3a\u7edf\u4e00\u7684\u89c6\u89c9\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86\u53ef\u63a7\u89d2\u8272\u52a8\u753b\u751f\u6210\u7684\u6311\u6218\uff0c\u65e0\u9700\u590d\u6742\u67b6\u6784\u6216\u591a\u9636\u6bb5\u5904\u7406\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u63a7\u89d2\u8272\u52a8\u753b\u751f\u6210\u4e2d\u56e0\u590d\u6742\u67b6\u6784\u6216\u989d\u5916\u6a21\u5757\u800c\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86FramePrompt\u6846\u67b6\uff0c\u5c06\u52a8\u753b\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6761\u4ef6\u5f0f\u672a\u6765\u9884\u6d4b\u4efb\u52a1\uff0c\u7edf\u4e00\u5904\u7406\u89c6\u89c9\u5e8f\u5217\uff0c\u907f\u514d\u4f7f\u7528\u989d\u5916\u7684\u5f15\u5bfc\u7f51\u7edc\u6216\u7ed3\u6784\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u7b80\u5316\u4e86\u8bad\u7ec3\u6d41\u7a0b\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u5e8f\u5217\u7ea7\u89c6\u89c9\u6761\u4ef6\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0d\u6539\u53d8\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u63a7\u52a8\u753b\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.17226", "pdf": "https://arxiv.org/pdf/2506.17226", "abs": "https://arxiv.org/abs/2506.17226", "authors": ["Ashish Manchanda", "Prem Prakash Jayaraman", "Abhik Banerjee", "Kaneez Fizza", "Arkady Zaslavsky"], "title": "DCMF: A Dynamic Context Monitoring and Caching Framework for Context Management Platforms", "categories": ["cs.DB"], "comment": null, "summary": "The rise of context-aware IoT applications has increased the demand for\ntimely and accurate context information. Context is derived by aggregating and\ninferring from dynamic IoT data, making it highly volatile and posing\nchallenges in maintaining freshness and real-time accessibility. Caching is a\npotential solution, but traditional policies struggle with the transient nature\nof context in IoT (e.g., ensuring real-time access for frequent queries or\nhandling fast-changing data). To address this, we propose the Dynamic Context\nMonitoring Framework (DCMF) to enhance context caching in Context Management\nPlatforms (CMPs) by dynamically evaluating and managing context. DCMF comprises\ntwo core components: the Context Evaluation Engine (CEE) and the Context\nManagement Module (CMM). The CEE calculates the Probability of Access (PoA)\nusing parameters such as Quality of Service (QoS), Quality of Context (QoC),\nCost of Context (CoC), timeliness, and Service Level Agreements (SLAs),\nassigning weights to assess access likelihood. Based on this, the CMM applies a\nhybrid Dempster-Shafer approach to manage Context Freshness (CF), updating\nbelief levels and confidence scores to determine whether to cache, evict, or\nrefresh context items. We implemented DCMF in a Context-as-a-Service (CoaaS)\nplatform and evaluated it using real-world smart city data, particularly\ntraffic and roadwork scenarios. Results show DCMF achieves a 12.5% higher cache\nhit rate and reduces cache expiry by up to 60% compared to the m-CAC technique,\nensuring timely delivery of relevant context and reduced latency. These results\ndemonstrate DCMF's scalability and suitability for dynamic context-aware IoT\nenvironments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u52a8\u6001\u4e0a\u4e0b\u6587\u76d1\u63a7\u6846\u67b6\uff08DCMF\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u7269\u8054\u7f51\u4e2d\u4e0a\u4e0b\u6587\u7f13\u5b58\u7684\u6548\u679c\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u548c\u7ba1\u7406\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f13\u5b58\u547d\u4e2d\u7387\u5e76\u51cf\u5c11\u4e86\u7f13\u5b58\u8fc7\u671f\u3002", "motivation": "\u968f\u7740\u4e0a\u4e0b\u6587\u611f\u77e5\u7269\u8054\u7f51\u5e94\u7528\u7684\u5174\u8d77\uff0c\u5bf9\u53ca\u65f6\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u4e0a\u4e0b\u6587\u7684\u9ad8\u52a8\u6001\u6027\u548c\u6613\u53d8\u6027\u5e26\u6765\u4e86\u7ef4\u62a4\u5b9e\u65f6\u6027\u7684\u6311\u6218\u3002\u4f20\u7edf\u7f13\u5b58\u7b56\u7565\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "DCMF\u6846\u67b6\u5305\u62ec\u4e0a\u4e0b\u6587\u8bc4\u4f30\u5f15\u64ce\uff08CEE\uff09\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u6a21\u5757\uff08CMM\uff09\u3002CEE\u901a\u8fc7\u8ba1\u7b97\u8bbf\u95ee\u6982\u7387\uff08PoA\uff09\uff0c\u7ed3\u5408\u591a\u4e2a\u53c2\u6570\u8bc4\u4f30\u4e0a\u4e0b\u6587\uff1bCMM\u91c7\u7528Dempster-Shafer\u65b9\u6cd5\u7ba1\u7406\u4e0a\u4e0b\u6587\u65b0\u9c9c\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDCMF\u5728\u667a\u80fd\u57ce\u5e02\u6570\u636e\uff08\u5982\u4ea4\u901a\u548c\u9053\u8def\u65bd\u5de5\u573a\u666f\uff09\u4e2d\uff0c\u7f13\u5b58\u547d\u4e2d\u7387\u63d0\u9ad812.5%\uff0c\u7f13\u5b58\u8fc7\u671f\u51cf\u5c1160%\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "conclusion": "DCMF\u5728\u52a8\u6001\u4e0a\u4e0b\u6587\u611f\u77e5\u7269\u8054\u7f51\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u80fd\u6709\u6548\u63d0\u5347\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u5b9e\u65f6\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.18003", "pdf": "https://arxiv.org/pdf/2506.18003", "abs": "https://arxiv.org/abs/2506.18003", "authors": ["Carol Jingyi Li", "Ruilin Wu", "Philip H. W. Leong"], "title": "AMD Versal Implementations of FAM and SSCA Estimators", "categories": ["cs.AR"], "comment": null, "summary": "Cyclostationary analysis is widely used in signal processing, particularly in\nthe analysis of human-made signals, and spectral correlation density (SCD) is\noften used to characterise cyclostationarity. Unfortunately, for real-time\napplications, even utilising the fast Fourier transform (FFT), the high\ncomputational complexity associated with estimating the SCD limits its\napplicability. In this work, we present optimised, high-speed\nfield-programmable gate array (FPGA) implementations of two SCD estimation\ntechniques. Specifically, we present an implementation of the FFT accumulation\nmethod (FAM) running entirely on the AMD Versal AI engine (AIE) array. We also\nintroduce an efficient implementation of the strip spectral correlation\nanalyser (SSCA) that can be used for window sizes up to $2^{20}$. For both\ntechniques, a generalised methodology is presented to parallelise the\ncomputation while respecting memory size and data bandwidth constraints.\nCompared to an NVIDIA GeForce RTX 3090 graphics processing unit (GPU) which\nuses a similar 7nm technology to our FPGA, for the same accuracy, our FAM/SSCA\nimplementations achieve speedups of 4.43x/1.90x and a 30.5x/24.5x improvement\nin energy efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u4f18\u5316\u540e\u7684SCD\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7FPGA\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u4f20\u7edfSCD\u4f30\u8ba1\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "method": "\u91c7\u7528FPGA\u5b9e\u73b0FFT\u7d2f\u79ef\u6cd5\uff08FAM\uff09\u548c\u6761\u5e26\u8c31\u76f8\u5173\u5206\u6790\u6cd5\uff08SSCA\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u5e76\u884c\u5316\u8ba1\u7b97\u7684\u901a\u7528\u65b9\u6cd5\u3002", "result": "\u76f8\u8f83\u4e8eNVIDIA RTX 3090 GPU\uff0cFAM\u548cSSCA\u5206\u522b\u5b9e\u73b0\u4e864.43\u500d/1.90\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c30.5\u500d/24.5\u500d\u7684\u80fd\u6548\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86FPGA\u5728\u9ad8\u6548SCD\u4f30\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u65f6\u4fe1\u53f7\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.18560", "pdf": "https://arxiv.org/pdf/2506.18560", "abs": "https://arxiv.org/abs/2506.18560", "authors": ["Jiexin Zhang", "Shu Xu", "Chunguo Li", "Yongming Huang", "Luxi Yang"], "title": "Efficient Beam Selection for ISAC in Cell-Free Massive MIMO via Digital Twin-Assisted Deep Reinforcement Learning", "categories": ["cs.ET"], "comment": "Submitted to IEEE Transactions on Wireless Communications", "summary": "Beamforming enhances signal strength and quality by focusing energy in\nspecific directions. This capability is particularly crucial in cell-free\nintegrated sensing and communication (ISAC) systems, where multiple distributed\naccess points (APs) collaborate to provide both communication and sensing\nservices. In this work, we first derive the distribution of joint target\ndetection probabilities across multiple receiving APs under false alarm rate\nconstraints, and then formulate the beam selection procedure as a Markov\ndecision process (MDP). We establish a deep reinforcement learning (DRL)\nframework, in which reward shaping and sinusoidal embedding are introduced to\nfacilitate agent learning. To eliminate the high costs and associated risks of\nreal-time agent-environment interactions, we further propose a novel digital\ntwin (DT)-assisted offline DRL approach. Different from traditional online DRL,\na conditional generative adversarial network (cGAN)-based DT module, operating\nas a replica of the real world, is meticulously designed to generate virtual\nstate-action transition pairs and enrich data diversity, enabling offline\nadjustment of the agent's policy. Additionally, we address the\nout-of-distribution issue by incorporating an extra penalty term into the loss\nfunction design. The convergency of agent-DT interaction and the upper bound of\nthe Q-error function are theoretically derived. Numerical results demonstrate\nthe remarkable performance of our proposed approach, which significantly\nreduces online interaction overhead while maintaining effective beam selection\nacross diverse conditions including strict false alarm control, low\nsignal-to-noise ratios, and high target velocities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6ce2\u675f\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u8702\u7a9d\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u548c\u5956\u52b1\u8bbe\u8ba1\u51cf\u5c11\u5b9e\u65f6\u4ea4\u4e92\u6210\u672c\uff0c\u5e76\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u65e0\u8702\u7a9d\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u7cfb\u7edf\u4e2d\uff0c\u6ce2\u675f\u6210\u5f62\u5bf9\u63d0\u5347\u4fe1\u53f7\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u9ad8\u6210\u672c\u548c\u98ce\u9669\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u6ce2\u675f\u9009\u62e9\uff0c\u5f15\u5165\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u6570\u5b57\u5b6a\u751f\u6a21\u5757\u751f\u6210\u865a\u62df\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u89e3\u51b3\u6570\u636e\u591a\u6837\u6027\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5728\u7ebf\u4ea4\u4e92\u5f00\u9500\uff0c\u540c\u65f6\u5728\u4e25\u683c\u8bef\u62a5\u7387\u63a7\u5236\u3001\u4f4e\u4fe1\u566a\u6bd4\u548c\u9ad8\u76ee\u6807\u901f\u5ea6\u7b49\u6761\u4ef6\u4e0b\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u6570\u5b57\u5b6a\u751f\u548c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u4e3a\u65e0\u8702\u7a9dISAC\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u98ce\u9669\u7684\u6ce2\u675f\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2506.17338", "pdf": "https://arxiv.org/pdf/2506.17338", "abs": "https://arxiv.org/abs/2506.17338", "authors": ["Duong Bach"], "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning", "categories": ["cs.DC", "cs.AI", "cs.MA"], "comment": "13 pages", "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCo-Forgetting Protocol\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u540c\u6b65\u7ba1\u7406\u5171\u4eab\u77e5\u8bc6\u7684\u5185\u5b58\u4fee\u526a\u3002\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u8bed\u4e49\u6295\u7968\u3001\u591a\u5c3a\u5ea6\u65f6\u95f4\u8870\u51cf\u51fd\u6570\u548cPBFT\u5171\u8bc6\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u7ba1\u7406\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u534f\u8bae\u5728\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3001\u6295\u7968\u51c6\u786e\u6027\u548c\u5171\u8bc6\u6210\u529f\u7387\u7b49\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u786e\u4fdd\u5206\u5e03\u5f0f\u5185\u5b58\u4fdd\u6301\u540c\u6b65\u3001\u76f8\u5173\u6027\u5e76\u907f\u514d\u79ef\u7d2f\u8fc7\u65f6\u6216\u65e0\u7528\u7684\u6570\u636e\u662f\u4e00\u9879\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u501f\u9274\u751f\u7269\u9057\u5fd8\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540c\u6b65\u5185\u5b58\u4fee\u526a\u7684\u534f\u8bae\u3002", "method": "Co-Forgetting Protocol\u6574\u5408\u4e86\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u4e0a\u4e0b\u6587\u611f\u77e5\u8bed\u4e49\u6295\u7968\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7DistilBERT\u6a21\u578b\u8bc4\u4f30\u5185\u5b58\u9879\u7684\u76f8\u5173\u6027\uff1b2) \u591a\u5c3a\u5ea6\u65f6\u95f4\u8870\u51cf\u51fd\u6570\uff0c\u6839\u636e\u5185\u5b58\u9879\u7684\u5e74\u9f84\u548c\u8bbf\u95ee\u9891\u7387\u52a8\u6001\u8c03\u6574\u91cd\u8981\u6027\uff1b3) PBFT\u5171\u8bc6\u673a\u5236\uff0c\u786e\u4fdd\u51b3\u7b56\u5728\u6076\u610f\u6216\u6545\u969c\u6761\u4ef6\u4e0b\u4ecd\u53ef\u9760\u3002", "result": "\u5b9e\u9a8c\u6a21\u62df\u663e\u793a\uff0c\u534f\u8bae\u5728500\u4e2a\u5468\u671f\u5185\u51cf\u5c1152%\u7684\u5185\u5b58\u5360\u7528\uff0c\u9057\u5fd8\u51b3\u7b56\u7684\u6295\u7968\u51c6\u786e\u7387\u8fbe88%\uff0cPBFT\u5171\u8bc6\u6210\u529f\u7387\u4e3a92%\uff0c\u5185\u5b58\u8bbf\u95ee\u7f13\u5b58\u547d\u4e2d\u7387\u4e3a82%\u3002", "conclusion": "Co-Forgetting Protocol\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5065\u58ee\u7684\u5185\u5b58\u7ba1\u7406\uff0c\u4e3a\u89e3\u51b3\u5171\u4eab\u77e5\u8bc6\u7684\u540c\u6b65\u548c\u4fee\u526a\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17369", "pdf": "https://arxiv.org/pdf/2506.17369", "abs": "https://arxiv.org/abs/2506.17369", "authors": ["Zhiyuan Pan", "Xing Hu", "Xin Xia", "Xiaohu Yang"], "title": "Re-Evaluating Code LLM Benchmarks Under Semantic Mutation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "In the era of large language models (LLMs), code benchmarks have become an\nimportant research area in software engineering and are widely used by\npractitioners. These benchmarks evaluate the performance of LLMs on specific\ncode-related tasks, such as code understanding and generation. A critical step\nin constructing code benchmarks is the design of prompts. However, as existing\ncode benchmarks typically rely on a single prompt template per task, they are\nprone to the issue of prompt sensitivity, where minor prompt variations could\nresult in substantial performance variations, leading to unreliable evaluations\nof model capabilities.\n  While previous studies have explored prompt sensitivity, their experimental\ndesigns and findings are limited to traditional natural language processing\n(NLP) tasks. In this paper, we present an empirical study to investigate prompt\nsensitivity in code benchmarks. We first propose a general framework that\nmodifies prompt templates in a manner that preserves both their semantics and\ntheir structure as much as possible. Based on the framework, we conduct\nextensive experiments across eight code benchmark tasks on 10 representative\nopen-source LLMs, with each task featuring 100 semantically similar prompt\ntemplates. We then analyze the evaluation results using various statistical\nmetrics, focusing on both absolute and relative model performance. Our findings\nsuggest that even slight prompt variations can lead to significant shifts in\nperformance. Additionally, we observe that such variations can introduce\ninconsistencies in the performance rankings across different models. These\ninsights highlight the need for considering prompt sensitivity when designing\nfuture code benchmarks, to ensure more reliable and accurate evaluation of LLM\ncapabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u793a\u6a21\u677f\u7684\u654f\u611f\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u5fae\u5c0f\u7684\u63d0\u793a\u53d8\u5316\u4e5f\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u663e\u8457\u6ce2\u52a8\uff0c\u5f71\u54cd\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\uff0c\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u63d0\u793a\u6a21\u677f\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u7ed3\u6784\u4fdd\u6301\u7684\u63d0\u793a\u6a21\u677f\u4fee\u6539\u65b9\u6cd5\uff0c\u57288\u4e2a\u4ee3\u7801\u57fa\u51c6\u4efb\u52a1\u4e0a\u5bf910\u4e2a\u5f00\u6e90LLM\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u6bcf\u4efb\u52a1\u4f7f\u7528100\u4e2a\u76f8\u4f3c\u63d0\u793a\u6a21\u677f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u7ec6\u5fae\u7684\u63d0\u793a\u53d8\u5316\u4e5f\u4f1a\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u5e76\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u6392\u540d\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u8bbe\u8ba1\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u65f6\u9700\u8003\u8651\u63d0\u793a\u654f\u611f\u6027\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.18381", "pdf": "https://arxiv.org/pdf/2506.18381", "abs": "https://arxiv.org/abs/2506.18381", "authors": ["Yiwei Liu", "Yi-Chia Cheng", "Cheng-Shang Chang"], "title": "Consistent Channel Hopping Algorithms for the Multichannel Rendezvous Problem with Heterogeneous Available Channel Sets", "categories": ["cs.NI", "cs.PF"], "comment": "19 pages, 10 figures", "summary": "We propose a theoretical framework for consistent channel hopping algorithms\nto address the multichannel rendezvous problem (MRP) in wireless networks with\nheterogeneous available channel sets. A channel selection function is called\nconsistent if the selected channel remains unchanged when the available channel\nset shrinks, provided the selected channel is still available. We show that all\nconsistent channel selection functions are equivalent to the function that\nalways selects the smallest-index channel under appropriate channel relabeling.\nThis leads to a natural representation of a consistent channel hopping\nalgorithm as a sequence of permutations. For the two-user MRP, we characterize\nrendezvous time slots using a fictitious user and derive tight bounds on the\nmaximum time-to-rendezvous (MTTR) and expected time-to-rendezvous (ETTR).\nNotably, the ETTR is shown to be the inverse of the Jaccard index when\npermutations are randomly selected. We also prove that consistent channel\nhopping algorithms maximize the rendezvous probability. To reduce\nimplementation complexity, we propose the modulo algorithm, which uses modular\narithmetic with one-cycle permutations and achieves performance comparable to\nlocality-sensitive hashing (LSH)-based algorithms. The framework is extended to\nmultiple users, with novel strategies such as stick-together, spread-out, and a\nhybrid method that accelerates rendezvous in both synchronous and asynchronous\nsettings. Simulation results confirm the effectiveness and scalability of the\nproposed algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u81f4\u7684\u9891\u9053\u9009\u62e9\u7b97\u6cd5\u89e3\u51b3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u591a\u9891\u9053\u4f1a\u5408\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u53ef\u7528\u9891\u9053\u96c6\u4e2d\u7684\u591a\u9891\u9053\u4f1a\u5408\u95ee\u9898\uff0c\u786e\u4fdd\u9891\u9053\u9009\u62e9\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u8bbe\u8ba1\u4e00\u81f4\u7684\u9891\u9053\u9009\u62e9\u51fd\u6570\uff0c\u5c06\u5176\u8868\u793a\u4e3a\u6392\u5217\u5e8f\u5217\uff0c\u5e76\u63d0\u51fa\u6a21\u7b97\u6cd5\u964d\u4f4e\u590d\u6742\u5ea6\u3002", "result": "\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u4f1a\u5408\u65f6\u95f4\u754c\u9650\u53ca\u6700\u4f18\u6027\uff0c\u6a21\u62df\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6269\u5c55\u6027\u3002", "conclusion": "\u6846\u67b6\u548c\u7b97\u6cd5\u5728\u591a\u7528\u6237\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u65e0\u7ebf\u7f51\u7edc\u4f1a\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u548c\u5b9e\u8df5\u65b9\u6848\u3002"}}
{"id": "2506.17538", "pdf": "https://arxiv.org/pdf/2506.17538", "abs": "https://arxiv.org/abs/2506.17538", "authors": ["Yile Gu", "Rohan Kadekodi", "Hoang Nguyen", "Keisuke Kamahori", "Yiyu Liu", "Baris Kasikci"], "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User Devices", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.OS"], "comment": "The code is available at https://github.com/efeslab/ConsumerBench", "summary": "The recent shift in Generative AI (GenAI) applications from cloud-only\nenvironments to end-user devices introduces new challenges in resource\nmanagement, system efficiency, and user experience. This paper presents\nConsumerBench, a comprehensive benchmarking framework designed to evaluate the\nsystem efficiency and response time of GenAI models running on end-user\ndevices. Unlike existing benchmarks that assume exclusive model access on\ndedicated GPUs, ConsumerBench simulates realistic multi-application scenarios\nexecuting concurrently on constrained hardware. Furthermore, ConsumerBench\nsupports customizable workflows that simulate complex tasks requiring\ncoordination among multiple applications. ConsumerBench captures both\napplication-level metrics, including latency and Service Level Objective (SLO)\nattainment, and system-level metrics like CPU/GPU utilization and memory\nbandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies\nin resource sharing, unfair scheduling under greedy allocation, and performance\npitfalls of static model server configurations. The paper also provides\npractical insights for model developers and system designers, highlighting the\nbenefits of custom kernels tailored to consumer-grade GPU architectures and the\nvalue of implementing SLO-aware scheduling strategies.", "AI": {"tldr": "ConsumerBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7aef\u7528\u6237\u8bbe\u5907\u4e0aGenAI\u6a21\u578b\u7cfb\u7edf\u6548\u7387\u548c\u54cd\u5e94\u65f6\u95f4\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u6a21\u62df\u591a\u5e94\u7528\u573a\u666f\u5e76\u63ed\u793a\u8d44\u6e90\u5206\u914d\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3GenAI\u4ece\u4e91\u7aef\u8f6c\u5411\u7aef\u7528\u6237\u8bbe\u5907\u65f6\u5728\u8d44\u6e90\u7ba1\u7406\u3001\u7cfb\u7edf\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u7684\u65b0\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u591a\u5e94\u7528\u5e76\u53d1\u8fd0\u884c\u7684\u573a\u666f\uff0c\u5b9a\u5236\u590d\u6742\u4efb\u52a1\u5de5\u4f5c\u6d41\uff0c\u5e76\u6355\u83b7\u5e94\u7528\u548c\u7cfb\u7edf\u7ea7\u6307\u6807\u3002", "result": "\u53d1\u73b0\u8d44\u6e90\u5171\u4eab\u6548\u7387\u4f4e\u3001\u5206\u914d\u4e0d\u516c\u5e73\u53ca\u9759\u6001\u6a21\u578b\u670d\u52a1\u5668\u914d\u7f6e\u7684\u6027\u80fd\u95ee\u9898\u3002", "conclusion": "\u4e3a\u6a21\u578b\u5f00\u53d1\u8005\u548c\u7cfb\u7edf\u8bbe\u8ba1\u8005\u63d0\u4f9b\u5b9e\u7528\u89c1\u89e3\uff0c\u5982\u5b9a\u5236\u5185\u6838\u548cSLO\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\u7684\u4ef7\u503c\u3002"}}
{"id": "2506.17606", "pdf": "https://arxiv.org/pdf/2506.17606", "abs": "https://arxiv.org/abs/2506.17606", "authors": ["Ryo Takahashi", "Takashi Sato", "Wakako Yukita", "Tomoyuki Yokota", "Takao Someya", "Yoshihiro Kawahara"], "title": "Full-body WPT: wireless powering with meandered e-textiles", "categories": ["cs.HC"], "comment": null, "summary": "We present Full-body WPT, wireless power networking around the human body\nusing a meandered textile coil. Unlike traditional inductive systems that emit\nstrong fields into the deep tissue inside the body, the meander coil enables\nlocalized generation of strong magnetic field constrained to the skin surface,\neven when scaled to the size of the human body. Such localized inductive system\nenhances both safety and efficiency of wireless power around the body.\nFurthermore, the use of low-loss conductive yarn achieve energy-efficient and\nlightweight design. We analyze the performance of our design through\nsimulations and experimental prototypes, demonstrating high power transfer\nefficiency and adaptability to user movement and posture. Our system provides a\nsafe and efficient distributed power network using meandered textile coils\nintegrated into wearable materials, highlighting the potential of body-centric\nwireless power networking as a foundational layer for ubiquitous health\nmonitoring, augmented reality, and human-machine interaction systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u86c7\u5f62\u7eba\u7ec7\u7ebf\u5708\u7684\u5168\u8eab\u65e0\u7ebf\u7535\u529b\u4f20\u8f93\u6280\u672f\uff0c\u5c06\u5f3a\u78c1\u573a\u9650\u5236\u5728\u76ae\u80a4\u8868\u9762\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u611f\u5e94\u7cfb\u7edf\u4f1a\u5411\u4f53\u5185\u6df1\u90e8\u7ec4\u7ec7\u53d1\u5c04\u5f3a\u78c1\u573a\uff0c\u5b58\u5728\u5b89\u5168\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u86c7\u5f62\u7eba\u7ec7\u7ebf\u5708\u548c\u4f4e\u635f\u8017\u5bfc\u7535\u7eb1\u7ebf\u8bbe\u8ba1\u5c40\u90e8\u611f\u5e94\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u9ad8\u529f\u7387\u4f20\u8f93\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u53ef\u7a7f\u6234\u5065\u5eb7\u76d1\u6d4b\u3001\u589e\u5f3a\u73b0\u5b9e\u548c\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b89\u5168\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u7535\u529b\u7f51\u7edc\u57fa\u7840\u3002"}}
{"id": "2506.18055", "pdf": "https://arxiv.org/pdf/2506.18055", "abs": "https://arxiv.org/abs/2506.18055", "authors": ["Jason Clarke", "Yoshihiko Gotoh", "Stefan Goetze"], "title": "Face-Voice Association for Audiovisual Active Speaker Detection in Egocentric Recordings", "categories": ["cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to EUSIPCO 2025. 5 pages, 1 figure. To appear in the\n  Proceedings of the 33rd European Signal Processing Conference (EUSIPCO),\n  September 8-12, 2025, Palermo, Italy", "summary": "Audiovisual active speaker detection (ASD) is conventionally performed by\nmodelling the temporal synchronisation of acoustic and visual speech cues. In\negocentric recordings, however, the efficacy of synchronisation-based methods\nis compromised by occlusions, motion blur, and adverse acoustic conditions. In\nthis work, a novel framework is proposed that exclusively leverages cross-modal\nface-voice associations to determine speaker activity. An existing face-voice\nassociation model is integrated with a transformer-based encoder that\naggregates facial identity information by dynamically weighting each frame\nbased on its visual quality. This system is then coupled with a front-end\nutterance segmentation method, producing a complete ASD system. This work\ndemonstrates that the proposed system, Self-Lifting for audiovisual active\nspeaker detection(SL-ASD), achieves performance comparable to, and in certain\ncases exceeding, that of parameter-intensive synchronisation-based approaches\nwith significantly fewer learnable parameters, thereby validating the\nfeasibility of substituting strict audiovisual synchronisation modelling with\nflexible biometric associations in challenging egocentric scenarios.", "AI": {"tldr": "SL-ASD \u662f\u4e00\u79cd\u57fa\u4e8e\u8de8\u6a21\u6001\u4eba\u8138-\u58f0\u97f3\u5173\u8054\u7684\u4e3b\u52a8\u8bf4\u8bdd\u8005\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\uff08egocentric\uff09\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u4f20\u7edf\u540c\u6b65\u65b9\u6cd5\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u97f3\u9891\u89c6\u89c9\u540c\u6b65\u65b9\u6cd5\u56e0\u906e\u6321\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u6076\u52a3\u7684\u58f0\u5b66\u6761\u4ef6\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u6574\u5408\u4eba\u8138-\u58f0\u97f3\u5173\u8054\u6a21\u578b\u4e0e\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\uff0c\u52a8\u6001\u52a0\u6743\u6bcf\u5e27\u89c6\u89c9\u8d28\u91cf\uff0c\u5e76\u914d\u5408\u524d\u7aef\u8bed\u97f3\u5206\u6bb5\u65b9\u6cd5\u3002", "result": "SL-ASD \u5728\u6027\u80fd\u4e0a\u5ab2\u7f8e\u6216\u8d85\u8fc7\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u53c2\u6570\u66f4\u5c11\u3002", "conclusion": "\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0c\u53ef\u7528\u4eba\u8138-\u58f0\u97f3\u5173\u8054\u66ff\u4ee3\u4e25\u683c\u7684\u97f3\u89c6\u9891\u540c\u6b65\u5efa\u6a21\u3002"}}
{"id": "2506.17331", "pdf": "https://arxiv.org/pdf/2506.17331", "abs": "https://arxiv.org/abs/2506.17331", "authors": ["Craig Steven Wright"], "title": "Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems", "categories": ["cs.LO", "cs.CL", "math.LO", "68T27, 03B70", "I.2.4; I.2.3"], "comment": "126 pages, 0 figures, includes formal frameworks and architecture\n  blueprint; no prior version; suitable for submission under AI and Logic\n  categories", "summary": "This paper develops a comprehensive framework for artificial intelligence\nsystems that operate under strict epistemic constraints, moving beyond\nstochastic language prediction to support structured reasoning, propositional\ncommitment, and contradiction detection. It formalises belief representation,\nmetacognitive processes, and normative verification, integrating symbolic\ninference, knowledge graphs, and blockchain-based justification to ensure\ntruth-preserving, auditably rational epistemic agents.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u7cfb\u7edf\u6846\u67b6\uff0c\u652f\u6301\u7ed3\u6784\u5316\u63a8\u7406\u548c\u77db\u76fe\u68c0\u6d4b\uff0c\u786e\u4fdd\u5176\u5408\u7406\u6027\u3002", "motivation": "\u89e3\u51b3AI\u7cfb\u7edf\u5728\u4e25\u683c\u8ba4\u77e5\u7ea6\u675f\u4e0b\u7684\u8868\u73b0\u95ee\u9898\uff0c\u8d85\u8d8a\u968f\u673a\u8bed\u8a00\u9884\u6d4b\u3002", "method": "\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u3001\u77e5\u8bc6\u56fe\u8c31\u548c\u533a\u5757\u94fe\u6280\u672f\uff0c\u5f62\u5f0f\u5316\u4fe1\u5ff5\u8868\u793a\u548c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u73b0\u4e86\u80fd\u4fdd\u6301\u771f\u7406\u4e14\u53ef\u5ba1\u8ba1\u7684\u8ba4\u77e5\u4ee3\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7ea7\u7684\u8ba4\u77e5\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u9700\u4e25\u683c\u9a8c\u8bc1\u7684\u573a\u666f\u3002"}}
{"id": "2506.17343", "pdf": "https://arxiv.org/pdf/2506.17343", "abs": "https://arxiv.org/abs/2506.17343", "authors": ["Pavel Malinovskiy"], "title": "Solving the Problem of Poor Internet Connectivity in Dhaka: Innovative Solutions Using Advanced WebRTC and Adaptive Streaming Technologies", "categories": ["cs.NI", "C.2.1; C.2.2; C.2.4; H.4.3"], "comment": "12 pages, 4 figures, published in IRJMETS (Vol. 7, Issue 3, 2025)", "summary": "Dhaka, Bangladesh, one of the world's most densely populated cities, faces\nsevere challenges in maintaining reliable, high-speed internet connectivity.\nThis paper presents an innovative framework that addresses poor mobile data\nconnections through the integration of advanced WebRTC technology with adaptive\nstreaming and server-side recording solutions. Focusing on the unique network\nconditions in Dhaka in 2025, our approach combines dynamic transcoding,\nreal-time error correction, and optimized interface selection to enhance\nconnectivity. We analyze empirical data on connection speeds, mobile tower\ndensity, district-level population statistics, and social media usage.\nExtensive mathematical formulations, including novel models for bitrate\nestimation, round-trip time optimization, and reliability analysis, are\nprovided alongside detailed diagrams and multiple examples of code in both\nPython and C++. Experimental results demonstrate significant improvements in\nthroughput, latency reduction, and overall service quality, offering a scalable\nblueprint for next-generation communication systems in hyper-dense urban\nenvironments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u96c6\u6210WebRTC\u6280\u672f\u548c\u81ea\u9002\u5e94\u6d41\u5a92\u4f53\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u8fbe\u5361\u5e02\u79fb\u52a8\u6570\u636e\u8fde\u63a5\u6027\u80fd\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u8fbe\u5361\u4f5c\u4e3a\u4eba\u53e3\u5bc6\u96c6\u57ce\u5e02\uff0c\u7f51\u7edc\u8fde\u63a5\u95ee\u9898\u4e25\u91cd\uff0c\u9700\u8981\u4e00\u79cd\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u5347\u79fb\u52a8\u6570\u636e\u7684\u53ef\u9760\u6027\u548c\u901f\u5ea6\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u8f6c\u7801\u3001\u5b9e\u65f6\u7ea0\u9519\u548c\u4f18\u5316\u63a5\u53e3\u9009\u62e9\u7684\u6280\u672f\uff0c\u5e76\u5206\u6790\u4e86\u7f51\u7edc\u901f\u5ea6\u3001\u57fa\u7ad9\u5bc6\u5ea6\u548c\u793e\u4ea4\u5a92\u4f53\u4f7f\u7528\u7b49\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3001\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u5e76\u63d0\u5347\u4e86\u6574\u4f53\u670d\u52a1\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u5bc6\u5ea6\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u4e0b\u4e00\u4ee3\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17450", "pdf": "https://arxiv.org/pdf/2506.17450", "abs": "https://arxiv.org/abs/2506.17450", "authors": ["Jiacheng Chen", "Ramin Mehran", "Xuhui Jia", "Saining Xie", "Sanghyun Woo"], "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://blenderfusion.github.io", "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.", "AI": {"tldr": "BlenderFusion\u662f\u4e00\u4e2a\u751f\u6210\u5f0f\u89c6\u89c9\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u7ec4\u5408\u7269\u4f53\u3001\u76f8\u673a\u548c\u80cc\u666f\u5408\u6210\u65b0\u573a\u666f\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u590d\u6742\u573a\u666f\u7f16\u8f91\u4efb\u52a1\u4e2d\u7269\u4f53\u3001\u76f8\u673a\u548c\u80cc\u666f\u7684\u534f\u540c\u63a7\u5236\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42-\u7f16\u8f91-\u5408\u6210\u6d41\u7a0b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u5173\u952e\u8bad\u7ec3\u7b56\u7565\uff08\u6e90\u63a9\u7801\u548c\u6a21\u62df\u7269\u4f53\u6296\u52a8\uff09\u3002", "result": "\u5728\u590d\u6742\u5408\u6210\u573a\u666f\u7f16\u8f91\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "BlenderFusion\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u7a0b\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u89c6\u89c9\u573a\u666f\u7f16\u8f91\u3002"}}
{"id": "2506.17451", "pdf": "https://arxiv.org/pdf/2506.17451", "abs": "https://arxiv.org/abs/2506.17451", "authors": ["Aida Sheshbolouki", "M. Tamer Ozsu"], "title": "Transient Concepts in Streaming Graphs", "categories": ["cs.DB"], "comment": null, "summary": "Concept Drift (CD) occurs when a change in a hidden context can induce\nchanges in a target concept. CD is a natural phenomenon in non-stationary\nsettings such as data streams. Understanding, detection, and adaptation to CD\nin streaming data is (i) vital for effective and efficient analytics as\nreliable output depends on adaptation to fresh input, (ii) challenging as it\nrequires efficient operations as well as effective performance evaluations, and\n(iii) impactful as it applies to a variety of use cases and is a crucial\ninitial step for data management systems. Current works are mostly focused on\npassive CD detection as part of supervised adaptation, on independently\ngenerated data instances or graph snapshots, on target concepts as a function\nof data labels, on static data management, and on specific temporal order of\ndata record. These methods do not always work. We revisit CD for the streaming\ngraphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for\nstreaming graph CD detection and prediction. Both frameworks discern the change\nof generative source. SGDD detects the CDs due to the changes of generative\nparameters with significant delays such that it is difficult to evaluate the\nperformance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds\nahead of their occurrence, without accessing the payloads of data records.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6d41\u56fe\u8bbe\u7f6e\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\uff08CD\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6846\u67b6SGDD\u548cSGDP\u7528\u4e8e\u68c0\u6d4b\u548c\u9884\u6d4bCD\uff0c\u5206\u522b\u9488\u5bf9\u5ef6\u8fdf\u8f83\u5927\u7684\u53c2\u6570\u53d8\u5316\u548c\u63d0\u524d\u9884\u6d4bCD\u3002", "motivation": "CD\u5728\u6d41\u6570\u636e\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u6d41\u56fe\u573a\u666f\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u8feb\u5207\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u6709\u6548\u68c0\u6d4b\u548c\u9884\u6d4bCD\u3002", "method": "\u63d0\u51fa\u4e86SGDD\u548cSGDP\u4e24\u79cd\u6846\u67b6\uff0c\u5206\u522b\u7528\u4e8e\u68c0\u6d4b\u751f\u6210\u53c2\u6570\u53d8\u5316\u7684CD\u548c\u5728\u6570\u636e\u8bb0\u5f55\u672a\u5230\u8fbe\u65f6\u63d0\u524d\u9884\u6d4bCD\u3002", "result": "SGDD\u68c0\u6d4b\u5ef6\u8fdf\u8f83\u5927\u7684CD\uff0cSGDP\u80fd\u57287374\u81f30.19\u6beb\u79d2\u524d\u9884\u6d4bCD\uff0c\u4e14\u65e0\u9700\u8bbf\u95ee\u6570\u636e\u8bb0\u5f55\u8d1f\u8f7d\u3002", "conclusion": "\u65b0\u6846\u67b6\u5728\u6d41\u56feCD\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u6570\u636e\u7ba1\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18530", "pdf": "https://arxiv.org/pdf/2506.18530", "abs": "https://arxiv.org/abs/2506.18530", "authors": ["Muhammad Ihsan Al Hafiz", "Naresh Ravichandran", "Anders Lansner", "Pawel Herman", "Artur Podobas"], "title": "Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Edge AI applications increasingly require models that can learn and adapt\non-device with minimal energy budget. Traditional deep learning models, while\npowerful, are often overparameterized, energy-hungry, and dependent on cloud\nconnectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian\nConfidence Propagation Neural Network (BCPNN), propose a neuromorphic\nalternative by mimicking cortical architecture and biologically-constrained\nlearning. They offer sparse architectures with local learning rules and\nunsupervised/semi-supervised learning, making them well-suited for low-power\nedge intelligence. However, existing BCPNN implementations rely on GPUs or\ndatacenter FPGAs, limiting their applicability to embedded systems. This work\npresents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+\nSoC using High-Level Synthesis. We implement both online learning and\ninference-only kernels with support for variable and mixed precision. Evaluated\non MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to\n17.5x latency and 94% energy savings over ARM baselines, without sacrificing\naccuracy. This work enables practical neuromorphic computing on edge devices,\nbridging the gap between brain-like learning and real-world deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u5d4c\u5165\u5f0f\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u5b9e\u73b0\u4f4e\u529f\u8017\u7684\u8fb9\u7f18\u4eba\u5de5\u667a\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8111\u542f\u53d1\u795e\u7ecf\u7f51\u7edc\uff08BLNN\uff09\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u8017\u9ad8\u4e14\u4f9d\u8d56\u4e91\u7aef\uff0c\u800c\u8111\u542f\u53d1\u795e\u7ecf\u7f51\u7edc\uff08BLNN\uff09\u5982BCPNN\u5177\u6709\u7a00\u758f\u67b6\u6784\u548c\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u3002\u4f46\u73b0\u6709\u5b9e\u73b0\u4f9d\u8d56GPU\u6216\u6570\u636e\u4e2d\u5fc3FPGA\uff0c\u9650\u5236\u4e86\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eZynq UltraScale+ SoC\u7684\u5d4c\u5165\u5f0fFPGA\u52a0\u901f\u5668\uff0c\u652f\u6301\u5728\u7ebf\u5b66\u4e60\u548c\u63a8\u65ad\u5185\u6838\uff0c\u5e76\u91c7\u7528\u9ad8\u5c42\u6b21\u7684\u7efc\u5408\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "\u5728MNIST\u3001\u80ba\u708e\u548c\u4e73\u817a\u764c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0eARM\u57fa\u51c6\u76f8\u6bd4\uff0c\u8be5\u52a0\u901f\u5668\u5b9e\u73b0\u4e8617.5\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c94%\u7684\u80fd\u8017\u8282\u7701\uff0c\u4e14\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002", "conclusion": "\u8fd9\u4e00\u7814\u7a76\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u7528\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u7f29\u5c0f\u4e86\u8111\u542f\u53d1\u5b66\u4e60\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.17268", "pdf": "https://arxiv.org/pdf/2506.17268", "abs": "https://arxiv.org/abs/2506.17268", "authors": ["Jun Wook Heo", "Raja Jurdak", "Sara Khalifa"], "title": "Optimal Operating Strategy for PV-BESS Households: Balancing Self-Consumption and Self-Sufficiency", "categories": ["eess.SY", "cs.ET", "cs.SY"], "comment": null, "summary": "High penetration of Photovoltaic (PV) generation and Battery Energy Storage\nSystem (BESS) in individual households increases the demand for solutions to\ndetermine the optimal PV generation power and the capacity of BESS.\nSelf-consumption and self-sufficiency are essential for optimising the\noperation of PV-BESS systems in households, aiming to minimise power import\nfrom and export to the main grid. However, self-consumption and\nself-sufficiency are not independent; they share a linear relationship. This\npaper demonstrates this relationship and proposes an optimal operating strategy\nthat considers power generation and consumption profiles to maximise\nself-consumption and self-sufficiency in households equipped with a PV-BESS. We\nclassify self-consumption and self-sufficiency patterns into four categories\nbased on the ratio of self-sufficiency to self-consumption for each household\nand determine the optimal PV generation and BESS capacities using both a\nmathematical calculation and this ratio. These optimal operation values for\neach category are then simulated using Model Predictive Control (MPC) and\nReinforcement Learning (RL)-based battery charging and discharging scheduling\nmodels. The results show that the ratio between self-consumption and\nself-sufficiency is a useful metric for determining the optimal capacity of\nPV-BESS systems to maximise the local utilisation of PV-generated power.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5bb6\u5ead\u5149\u4f0f-\u50a8\u80fd\u7cfb\u7edf\uff08PV-BESS\uff09\u4e2d\u81ea\u6d88\u8d39\u548c\u81ea\u7ed9\u81ea\u8db3\u7684\u7ebf\u6027\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u548c\u5206\u7c7b\u65b9\u6cd5\u786e\u5b9a\u6700\u4f73PV\u53d1\u7535\u548cBESS\u5bb9\u91cf\uff0c\u7ed3\u5408MPC\u548cRL\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u81ea\u6d88\u8d39\u4e0e\u81ea\u7ed9\u81ea\u8db3\u7684\u6bd4\u7387\u662f\u4f18\u5316\u7cfb\u7edf\u5bb9\u91cf\u7684\u6709\u6548\u6307\u6807\u3002", "motivation": "\u968f\u7740\u5bb6\u5ead\u5149\u4f0f\u548c\u50a8\u80fd\u7cfb\u7edf\u7684\u666e\u53ca\uff0c\u4e9f\u9700\u4e00\u79cd\u65b9\u6cd5\u6765\u786e\u5b9a\u6700\u4f18\u7684\u53d1\u7535\u548c\u50a8\u80fd\u5bb9\u91cf\uff0c\u4ee5\u6700\u5927\u5316\u81ea\u6d88\u8d39\u548c\u81ea\u7ed9\u81ea\u8db3\uff0c\u51cf\u5c11\u5bf9\u4e3b\u7535\u7f51\u7684\u4f9d\u8d56\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u5206\u6790\u81ea\u6d88\u8d39\u4e0e\u81ea\u7ed9\u81ea\u8db3\u7684\u7ebf\u6027\u5173\u7cfb\uff0c\u5c06\u5bb6\u5ead\u5206\u7c7b\u4e3a\u56db\u79cd\u6a21\u5f0f\uff0c\u5e76\u7ed3\u5408MPC\u548cRL\u6a21\u578b\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u81ea\u6d88\u8d39\u4e0e\u81ea\u7ed9\u81ea\u8db3\u7684\u6bd4\u7387\u662f\u4f18\u5316PV-BESS\u7cfb\u7edf\u5bb9\u91cf\u7684\u5173\u952e\u6307\u6807\uff0c\u53ef\u6709\u6548\u63d0\u5347\u672c\u5730\u5149\u4f0f\u7535\u529b\u7684\u5229\u7528\u7387\u3002", "conclusion": "\u8bba\u6587\u4e3a\u5bb6\u5ead\u5149\u4f0f-\u50a8\u80fd\u7cfb\u7edf\u7684\u4f18\u5316\u8fd0\u884c\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u9645\u6307\u5bfc\uff0c\u8bc1\u660e\u4e86\u81ea\u6d88\u8d39\u4e0e\u81ea\u7ed9\u81ea\u8db3\u6bd4\u7387\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.17357", "pdf": "https://arxiv.org/pdf/2506.17357", "abs": "https://arxiv.org/abs/2506.17357", "authors": ["Zhenyu Lei", "Jin-Kao Hao", "Qinghua Wu"], "title": "Speeding up Local Optimization in Vehicle Routing with Tensor-based GPU Acceleration", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Local search plays a central role in many effective heuristic algorithms for\nthe vehicle routing problem (VRP) and its variants. However, neighborhood\nexploration is known to be computationally expensive and time consuming,\nespecially for large instances or problems with complex constraints. In this\nstudy, we explore a promising direction to address this challenge by\nintroducing an original tensor-based GPU acceleration method designed to speed\nup the commonly used local search operators in vehicle routing. By using an\nattribute-based representation, the method offers broad extensibility, making\nit applicable to different VRP variants. Its low-coupling architecture, with\nintensive computations completely offloaded to the GPU, ensures seamless\nintegration in various local search-based algorithms and frameworks, leading to\nsignificant improvements in computational efficiency and potentially improved\nsolution quality. Through comparative experiments on benchmark instances of\nthree routing problems, we demonstrate the substantial computational advantages\nof the proposed approach over traditional CPU-based implementations. We also\nprovide a detailed analysis of the strengths and limitations of the method,\nproviding valuable insights into its performance characteristics and\nidentifying potential bottlenecks in practical applications. These findings\ncontribute to a better understanding and suggest directions for future\nimprovements.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f20\u91cf\u7684GPU\u52a0\u901f\u65b9\u6cd5\uff0c\u4f18\u5316\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u4e2d\u7684\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5c40\u90e8\u641c\u7d22\u5728\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u4e2d\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\uff0c\u5c24\u5176\u662f\u5927\u89c4\u6a21\u6216\u590d\u6742\u7ea6\u675f\u95ee\u9898\uff0c\u9700\u9ad8\u6548\u52a0\u901f\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5c5e\u6027\u7684\u5f20\u91cf\u8868\u793a\u548cGPU\u52a0\u901f\uff0c\u8bbe\u8ba1\u4f4e\u8026\u5408\u67b6\u6784\uff0c\u5c06\u8ba1\u7b97\u4efb\u52a1\u5b8c\u5168\u5378\u8f7d\u5230GPU\u3002", "result": "\u5728\u4e09\u4e2a\u8def\u7531\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edfCPU\u5b9e\u73b0\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u53ef\u80fd\u6539\u8fdb\u89e3\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c40\u90e8\u641c\u7d22\u63d0\u4f9b\u9ad8\u6548\u52a0\u901f\u65b9\u6848\uff0c\u4f46\u5176\u6027\u80fd\u7279\u70b9\u548c\u6f5c\u5728\u74f6\u9888\u4ecd\u9700\u8fdb\u4e00\u6b65\u5206\u6790\u4f18\u5316\u3002"}}
{"id": "2506.17539", "pdf": "https://arxiv.org/pdf/2506.17539", "abs": "https://arxiv.org/abs/2506.17539", "authors": ["Sidong Feng", "Changhao Du", "Huaxiao Liu", "Qingnan Wang", "Zhengwei Lv", "Mengfei Wang", "Chunyang Chen"], "title": "Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing", "categories": ["cs.SE"], "comment": "Accepted to International Conference on Software Engineering (ICSE\n  2026)", "summary": "The growing dependence on mobile phones and their apps has made multi-user\ninteractive features, like chat calls, live streaming, and video conferencing,\nindispensable for bridging the gaps in social connectivity caused by physical\nand situational barriers. However, automating these interactive features for\ntesting is fraught with challenges, owing to their inherent need for timely,\ndynamic, and collaborative user interactions, which current automated testing\nmethods inadequately address. Inspired by the concept of agents designed to\nautonomously and collaboratively tackle problems, we propose MAdroid, a novel\nmulti-agent approach powered by the Large Language Models (LLMs) to automate\nthe multi-user interactive task for app feature testing. Specifically, MAdroid\nemploys two functional types of multi-agents: user agents (Operator) and\nsupervisor agents (Coordinator and Observer). Each agent takes a specific role:\nthe Coordinator directs the interactive task; the Operator mimics user\ninteractions on the device; and the Observer monitors and reviews the task\nautomation process. Our evaluation, which included 41 multi-user interactive\ntasks, demonstrates the effectiveness of our approach, achieving 82.9% of the\ntasks with 96.8% action similarity, outperforming the ablation studies and\nstate-of-the-art baselines. Additionally, a preliminary investigation\nunderscores MAdroid's practicality by helping identify 11 multi-user\ninteractive bugs during regression app testing, confirming its potential value\nin real-world software development contexts.", "AI": {"tldr": "MAdroid\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u4ee3\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u591a\u7528\u6237\u4ea4\u4e92\u5e94\u7528\u6d4b\u8bd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u4e2d\u7684\u591a\u7528\u6237\u4ea4\u4e92\u529f\u80fd\uff08\u5982\u89c6\u9891\u901a\u8bdd\u3001\u76f4\u64ad\uff09\u5728\u6d4b\u8bd5\u81ea\u52a8\u5316\u4e2d\u5b58\u5728\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u52a8\u6001\u534f\u4f5c\u9700\u6c42\u3002", "method": "MAdroid\u91c7\u7528\u4e09\u79cd\u4ee3\u7406\uff08\u534f\u8c03\u8005\u3001\u64cd\u4f5c\u8005\u3001\u89c2\u5bdf\u8005\uff09\u534f\u4f5c\u5b8c\u6210\u4efb\u52a1\uff0c\u64cd\u4f5c\u8005\u6a21\u62df\u7528\u6237\u884c\u4e3a\uff0c\u534f\u8c03\u8005\u6307\u5bfc\u4efb\u52a1\uff0c\u89c2\u5bdf\u8005\u76d1\u63a7\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAdroid\u572841\u9879\u4efb\u52a1\u4e2d\u5b8c\u6210\u7387\u8fbe82.9%\uff0c\u52a8\u4f5c\u76f8\u4f3c\u5ea6\u4e3a96.8%\uff0c\u5e76\u6210\u529f\u8bc6\u522b11\u4e2a\u56de\u5f52\u6d4b\u8bd5\u4e2d\u7684\u4ea4\u4e92\u6027Bug\u3002", "conclusion": "MAdroid\u9a8c\u8bc1\u4e86\u591a\u4ee3\u7406\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u6d4b\u8bd5\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.17877", "pdf": "https://arxiv.org/pdf/2506.17877", "abs": "https://arxiv.org/abs/2506.17877", "authors": ["Chuanyu Xue", "Tianyu Zhang", "Andrew Loveless", "Song Han"], "title": "Supporting Deterministic Traffic on Standard NICs", "categories": ["cs.NI", "cs.OS"], "comment": "20 pages", "summary": "Networked mission-critical applications (e.g., avionic control and industrial\nautomation systems) require deterministic packet transmissions to support a\nrange of sensing and control tasks with stringent timing constraints. While\nspecialized network infrastructure (e.g., time-sensitive networking (TSN)\nswitches) provides deterministic data transport across the network, achieving\nstrict end-to-end timing guarantees requires equally capable end devices to\nsupport deterministic traffic. These end devices, however, often employ\ngeneral-purpose computing platforms like standard PCs, which lack native\nsupport for deterministic traffic and suffer from unpredictable delays\nintroduced by their software stack and system architecture. Although\nspecialized NICs with hardware scheduling offload can mitigate this problem,\nthe limited compatibility hinders their widespread adoption, particularly for\ncost-sensitive applications or in legacy devices.\n  To fill this gap, this paper proposes a novel software-based driver model,\nnamely KeepON, to enable the support of deterministic packet transmissions on\nend devices equipped with standard NICs. The key idea of KeepON is to have the\nNIC keep on transmitting fixed-size data chunks as placeholders, thereby\nmaintaining a predictable temporal transmission pattern. The real-time packets\ngenerated by the mission-critical application(s) will then be precisely\ninserted into this stream by replacing placeholders at the designated position\nto ensure their accurate transmission time. We implement and evaluate KeepON by\nmodifying the network driver on a Raspberry Pi using its standard NIC. Our\nexperiments demonstrate that KeepON can achieve x162 times scheduling accuracy\ncomparable to its default driver, and x2.6 times compared to hardware-based\nsolution, thus enabling precise timing control on standard commodity hardware.", "AI": {"tldr": "KeepON\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u4ef6\u7684\u9a71\u52a8\u6a21\u578b\uff0c\u652f\u6301\u5728\u6807\u51c6\u7f51\u5361\u4e0a\u5b9e\u73b0\u786e\u5b9a\u6027\u5305\u4f20\u8f93\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u8c03\u5ea6\u51c6\u786e\u6027\u663e\u8457\u4f18\u4e8e\u9ed8\u8ba4\u9a71\u52a8\u548c\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5173\u952e\u4efb\u52a1\u7684\u7f51\u7edc\u5e94\u7528\u9700\u8981\u786e\u5b9a\u6027\u5305\u4f20\u8f93\uff0c\u4f46\u6807\u51c6\u7ec8\u7aef\u8bbe\u5907\u7f3a\u4e4f\u539f\u751f\u652f\u6301\uff0c\u4e14\u4e13\u7528\u786c\u4ef6\u517c\u5bb9\u6027\u6709\u9650\u3002", "method": "\u901a\u8fc7\u8ba9\u7f51\u5361\u6301\u7eed\u4f20\u8f93\u56fa\u5b9a\u5927\u5c0f\u7684\u6570\u636e\u5757\u4f5c\u4e3a\u5360\u4f4d\u7b26\uff0c\u5e76\u5728\u6307\u5b9a\u4f4d\u7f6e\u66ff\u6362\u4e3a\u5b9e\u65f6\u5305\uff0c\u786e\u4fdd\u4f20\u8f93\u65f6\u95f4\u7684\u7cbe\u786e\u6027\u3002", "result": "KeepON\u5728\u6807\u51c6\u786c\u4ef6\u4e0a\u7684\u8c03\u5ea6\u51c6\u786e\u6027\u8fbe\u5230\u9ed8\u8ba4\u9a71\u52a8\u7684162\u500d\uff0c\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u76842.6\u500d\u3002", "conclusion": "KeepON\u80fd\u591f\u5728\u4f4e\u6210\u672c\u6807\u51c6\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u786e\u5b9a\u6027\u5305\u4f20\u8f93\uff0c\u89e3\u51b3\u4e86\u517c\u5bb9\u6027\u548c\u6210\u672c\u95ee\u9898\u3002"}}
{"id": "2506.17890", "pdf": "https://arxiv.org/pdf/2506.17890", "abs": "https://arxiv.org/abs/2506.17890", "authors": ["Tobias Weinberg", "Claire O'Connor", "Ricardo E. Gonzalez Penuela", "Stephanie Valencia", "Thijs Roumen"], "title": "One Does Not Simply 'Mm-hmm': Exploring Backchanneling in the AAC Micro-Culture", "categories": ["cs.HC"], "comment": "See our project and video at:\n  https://tobiwg.com/research/one_does_not_simply_hm-hmm/", "summary": "Backchanneling (e.g., \"uh-huh\", \"hmm\", a simple nod) encompasses a big part\nof everyday communication; it is how we negotiate the turn to speak, it signals\nour engagement, and shapes the flow of our conversations. For people with\nspeech and motor impairments, backchanneling is limited to a reduced set of\nmodalities, and their Augmentative and Alternative Communication (AAC)\ntechnology requires visual attention, making it harder to observe non-verbal\ncues of conversation partners. We explore how users of AAC technology approach\nbackchanneling and create their own unique channels and communication culture.\nWe conducted a workshop with 4 AAC users to understand the unique\ncharacteristics of backchanneling in AAC. We explored how backchanneling\nchanges when pairs of AAC users communicate vs when an AAC user communicates\nwith a non-AAC user. We contextualize these findings through four in-depth\ninterviews with speech-language pathologists (SLPs). We conclude with a\ndiscussion about backchanneling as a micro-cultural practice, rethinking\nembodiment and mediation in AAC technology, and providing design\nrecommendations for timely multi-modal backchanneling while respecting\ndifferent communication cultures.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8f85\u52a9\u4e0e\u66ff\u4ee3\u6c9f\u901a\uff08AAC\uff09\u6280\u672f\u7528\u6237\u7684\u53cd\u9988\u884c\u4e3a\uff0c\u53d1\u73b0\u4ed6\u4eec\u5f62\u6210\u72ec\u7279\u7684\u6c9f\u901a\u6587\u5316\uff0c\u5e76\u901a\u8fc7\u4e0e\u6280\u672f\u6539\u8fdb\u7684\u8bbe\u8ba1\u5efa\u8bae\u3002", "motivation": "\u65e5\u5e38\u6c9f\u901a\u4e2d\u53cd\u9988\u884c\u4e3a\uff08\u5982\u70b9\u5934\u3001\u9644\u548c\uff09\u5bf9\u5bf9\u8bdd\u81f3\u5173\u91cd\u8981\uff0c\u4f46AAC\u7528\u6237\u53d7\u9650\u4e8e\u6280\u672f\u800c\u8868\u73b0\u4e0d\u8db3\uff0c\u7814\u7a76\u63a2\u8ba8\u5176\u53cd\u9988\u884c\u4e3a\u7684\u7279\u70b9\u3002", "method": "\u901a\u8fc74\u540dAAC\u7528\u6237\u7684\u7814\u8ba8\u4f1a\u548c4\u540d\u8a00\u8bed\u75c5\u7406\u5b66\u5bb6\u7684\u8bbf\u8c08\uff0c\u5206\u6790AAC\u7528\u6237\u4e0e\u975eAAC\u7528\u6237\u5bf9\u8bdd\u4e2d\u53cd\u9988\u884c\u4e3a\u7684\u5dee\u5f02\u3002", "result": "AAC\u7528\u6237\u5f62\u6210\u4e86\u72ec\u7279\u7684\u53cd\u9988\u6c9f\u901a\u65b9\u5f0f\uff0c\u53cd\u9988\u884c\u4e3a\u5728AAC\u7528\u6237\u4e4b\u95f4\u4e0eAAC\u7528\u6237\u4e0e\u975eAAC\u7528\u6237\u5bf9\u8bdd\u4e2d\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u53cd\u9988\u884c\u4e3a\u662f\u4e00\u79cd\u5fae\u89c2\u6587\u5316\u5b9e\u8df5\uff0c\u9700\u5728AAC\u6280\u672f\u4e2d\u91cd\u65b0\u601d\u8003\u5177\u8eab\u6027\u548c\u4e2d\u4ecb\u6027\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001\u53cd\u9988\u529f\u80fd\u65f6\u9700\u8981\u5c0a\u91cd\u4e0d\u540c\u6c9f\u901a\u6587\u5316\u3002"}}
{"id": "2506.17342", "pdf": "https://arxiv.org/pdf/2506.17342", "abs": "https://arxiv.org/abs/2506.17342", "authors": ["Zijian Long", "Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "title": "Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.MM", "cs.NI"], "comment": "Accepted by IEEE Transactions on Computational Social Systems", "summary": "The social metaverse is a growing digital ecosystem that blends virtual and\nphysical worlds. It allows users to interact socially, work, shop, and enjoy\nentertainment. However, privacy remains a major challenge, as immersive\ninteractions require continuous collection of biometric and behavioral data. At\nthe same time, ensuring high-quality, low-latency streaming is difficult due to\nthe demands of real-time interaction, immersive rendering, and bandwidth\noptimization. To address these issues, we propose ASMS (Adaptive Social\nMetaverse Streaming), a novel streaming system based on Federated Multi-Agent\nProximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which\nintegrates federated learning (FL) and deep reinforcement learning (DRL) to\ndynamically adjust streaming bit rates while preserving user privacy.\nExperimental results show that ASMS improves user experience by at least 14%\ncompared to existing streaming methods across various network conditions.\nTherefore, ASMS enhances the social metaverse experience by providing seamless\nand immersive streaming, even in dynamic and resource-constrained networks,\nwhile ensuring that sensitive user data remains on local devices.", "AI": {"tldr": "ASMS\u662f\u4e00\u79cd\u57fa\u4e8eF-MAPPO\u7684\u65b0\u578b\u6d41\u5a92\u4f53\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u793e\u4ea4\u5143\u5b87\u5b99\u4e2d\u7684\u9690\u79c1\u548c\u6d41\u5a92\u4f53\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u81f3\u5c1114%\u3002", "motivation": "\u793e\u4ea4\u5143\u5b87\u5b99\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u9690\u79c1\u548c\u6d41\u5a92\u4f53\u8d28\u91cf\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u7528\u6237\u6570\u636e\u53c8\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6d41\u5a92\u4f53\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faASMS\u7cfb\u7edf\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\uff0c\u52a8\u6001\u8c03\u6574\u6d41\u5a92\u4f53\u6bd4\u7279\u7387\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cASMS\u5728\u5404\u79cd\u7f51\u7edc\u6761\u4ef6\u4e0b\u6bd4\u5176\u4ed6\u6d41\u5a92\u4f53\u65b9\u6cd5\u7528\u6237\u4f53\u9a8c\u63d0\u5347\u81f3\u5c1114%\u3002", "conclusion": "ASMS\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u52a8\u6001\u7f51\u7edc\u4e2d\u63d0\u4f9b\u65e0\u7f1d\u548c\u6c89\u6d78\u5f0f\u7684\u6d41\u5a92\u4f53\u4f53\u9a8c\uff0c\u540c\u65f6\u786e\u4fdd\u7528\u6237\u6570\u636e\u9690\u79c1\u3002"}}
{"id": "2506.17602", "pdf": "https://arxiv.org/pdf/2506.17602", "abs": "https://arxiv.org/abs/2506.17602", "authors": ["Alessandro Abate", "Omid Akbarzadeh", "Henk A. P. Blom", "Sofie Haesaert", "Sina Hassani", "Abolfazl Lavaei", "Frederik Baymler Mathiesen", "Rahul Misra", "Amy Nejati", "Mathis Niehage", "Fie \u00d8rum", "Anne Remke", "Behrad Samari", "Ruohan Wang", "Rafal Wisniewski", "Ben Wooding", "Mahdieh Zaker"], "title": "ARCH-COMP25 Category Report: Stochastic Models", "categories": ["cs.LO", "cs.SY", "eess.SY"], "comment": null, "summary": "This report is concerned with a friendly competition for formal verification\nand policy synthesis of stochastic models. The main goal of the report is to\nintroduce new benchmarks and their properties within this category and\nrecommend next steps toward next year's edition of the competition. In\nparticular, this report introduces three recently developed software tools, a\nnew water distribution network benchmark, and a collection of simplified\nbenchmarks intended to facilitate further comparisons among tools that were\npreviously not directly comparable. This friendly competition took place as\npart of the workshop Applied Verification for Continuous and Hybrid Systems\n(ARCH) in Summer 2025.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u573a\u5173\u4e8e\u968f\u673a\u6a21\u578b\u5f62\u5f0f\u9a8c\u8bc1\u548c\u7b56\u7565\u5408\u6210\u7684\u53cb\u597d\u7ade\u8d5b\uff0c\u5305\u62ec\u65b0\u57fa\u51c6\u3001\u5de5\u5177\u548c\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u63a8\u52a8\u968f\u673a\u6a21\u578b\u5f62\u5f0f\u9a8c\u8bc1\u548c\u7b56\u7565\u5408\u6210\u9886\u57df\u7684\u7814\u7a76\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u57fa\u51c6\u548c\u5de5\u5177\u4fc3\u8fdb\u6bd4\u8f83\u548c\u8fdb\u6b65\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e09\u79cd\u65b0\u5f00\u53d1\u7684\u8f6f\u4ef6\u5de5\u5177\u3001\u4e00\u4e2a\u65b0\u7684\u4f9b\u6c34\u7f51\u7edc\u57fa\u51c6\u548c\u4e00\u7ec4\u7b80\u5316\u57fa\u51c6\u3002", "result": "\u7ade\u8d5b\u57282025\u5e74\u590f\u5b63\u7684ARCH\u7814\u8ba8\u4f1a\u4e0a\u6210\u529f\u4e3e\u529e\uff0c\u4e3a\u65b0\u5de5\u5177\u548c\u57fa\u51c6\u7684\u6bd4\u8f83\u63d0\u4f9b\u4e86\u5e73\u53f0\u3002", "conclusion": "\u62a5\u544a\u603b\u7ed3\u4e86\u7ade\u8d5b\u6210\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u660e\u5e74\u7ade\u8d5b\u7684\u6539\u8fdb\u5efa\u8bae\u3002"}}
{"id": "2506.17570", "pdf": "https://arxiv.org/pdf/2506.17570", "abs": "https://arxiv.org/abs/2506.17570", "authors": ["Sun Wei", "Fang Minghong", "Li Mengyuan"], "title": "VReaves: Eavesdropping on Virtual Reality App Identity and Activity via Electromagnetic Side Channels", "categories": ["cs.NI", "cs.CR", "cs.HC"], "comment": null, "summary": "Virtual reality (VR) has recently proliferated significantly, consisting of\nheadsets or head-mounted displays (HMDs) and hand controllers for an embodied\nand immersive experience. The VR device is usually embedded with different\nkinds of IoT sensors, such as cameras, microphones, communication sensors, etc.\nHowever, VR security has not been scrutinized from a physical hardware point of\nview, especially electromagnetic emanations (EM) that are automatically and\nunintentionally emitted from the VR headset. This paper presents VReaves, a\nsystem that can eavesdrop on the electromagnetic emanation side channel of a VR\nheadset for VR app identification and activity recognition. To do so, we first\ncharacterize the electromagnetic emanations from the embedded IoT sensors\n(e.g., cameras and microphones) in the VR headset through a signal processing\npipeline and further propose machine learning models to identify the VR app and\nrecognize the VR app activities. Our experimental evaluation with commercial\noff-the-shelf VR devices demonstrates the efficiency of VR app identification\nand activity recognition via electromagnetic emanation side channel.", "AI": {"tldr": "VR\u8bbe\u5907\u65e0\u610f\u4e2d\u53d1\u51fa\u7684\u7535\u78c1\u8f90\u5c04\u53ef\u88ab\u5229\u7528\u8fdb\u884c\u5e94\u7528\u8bc6\u522b\u548c\u6d3b\u52a8\u8ffd\u8e2a\uff0cVReaves\u7cfb\u7edf\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u548c\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u4e86\u8fd9\u4e00\u70b9\u3002", "motivation": "VR\u8bbe\u5907\u7684\u5b89\u5168\u6027\uff0c\u5c24\u5176\u662f\u786c\u4ef6\u5c42\u9762\u7684\u7535\u78c1\u8f90\u5c04\u6cc4\u6f0f\u95ee\u9898\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u6d41\u7a0b\u5206\u6790VR\u8bbe\u5907\u4e2d\u5d4c\u5165\u5f0f\u4f20\u611f\u5668\u7684\u7535\u78c1\u8f90\u5c04\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bc6\u522b\u5e94\u7528\u548c\u6d3b\u52a8\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cVReaves\u80fd\u6709\u6548\u901a\u8fc7\u7535\u78c1\u8f90\u5c04\u4fa7\u4fe1\u9053\u8bc6\u522bVR\u5e94\u7528\u548c\u6d3b\u52a8\u3002", "conclusion": "VR\u8bbe\u5907\u7684\u7535\u78c1\u8f90\u5c04\u4fa7\u4fe1\u9053\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u9700\u5f15\u8d77\u91cd\u89c6\u3002"}}
{"id": "2506.17636", "pdf": "https://arxiv.org/pdf/2506.17636", "abs": "https://arxiv.org/abs/2506.17636", "authors": ["Shihan Chen", "Zhaojin Li", "Zeyu Chen", "Qingsong Yan", "Gaoyang Shen", "Ran Duan"], "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene", "categories": ["cs.GR", "cs.CV", "eess.IV"], "comment": "IROS 2025", "summary": "Recent developments in 3D Gaussian Splatting have made significant advances\nin surface reconstruction. However, scaling these methods to large-scale scenes\nremains challenging due to high computational demands and the complex dynamic\nappearances typical of outdoor environments. These challenges hinder the\napplication in aerial surveying and autonomous driving. This paper proposes a\nnovel solution to reconstruct large-scale surfaces with fine details,\nsupervised by full-sized images. Firstly, we introduce a coarse-to-fine\nstrategy to reconstruct a coarse model efficiently, followed by adaptive scene\npartitioning and sub-scene refining from image segments. Additionally, we\nintegrate a decoupling appearance model to capture global appearance variations\nand a transient mask model to mitigate interference from moving objects.\nFinally, we expand the multi-view constraint and introduce a single-view\nregularization for texture-less areas. Our experiments were conducted on the\npublicly available dataset GauU-Scene V2, which was captured using unmanned\naerial vehicles. To the best of our knowledge, our method outperforms existing\nNeRF-based and Gaussian-based methods, achieving high-fidelity visual results\nand accurate surface from full-size image optimization. Open-source code will\nbe available on GitHub.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c97\u5230\u7cbe\u7b56\u7565\u548c\u81ea\u9002\u5e94\u573a\u666f\u5206\u5272\uff0c\u7ed3\u5408\u89e3\u8026\u5916\u89c2\u6a21\u578b\u548c\u77ac\u6001\u63a9\u7801\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u4e0b\u7684\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u95ee\u9898\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u9762\u4e34\u9ad8\u8ba1\u7b97\u9700\u6c42\u548c\u590d\u6742\u52a8\u6001\u5916\u89c2\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u573a\u666f\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7c97\u5230\u7cbe\u7b56\u7565\uff0c\u5148\u9ad8\u6548\u6784\u5efa\u7c97\u7cd9\u6a21\u578b\uff0c\u518d\u81ea\u9002\u5e94\u5206\u5272\u573a\u666f\u5e76\u4ece\u56fe\u50cf\u7247\u6bb5\u7ec6\u5316\uff1b\u6574\u5408\u89e3\u8026\u5916\u89c2\u6a21\u578b\u548c\u77ac\u6001\u63a9\u7801\u6a21\u578b\u4ee5\u5904\u7406\u5168\u5c40\u5916\u89c2\u53d8\u5316\u548c\u79fb\u52a8\u5bf9\u8c61\u5e72\u6270\uff1b\u6269\u5c55\u591a\u89c6\u89d2\u7ea6\u675f\u5e76\u5f15\u5165\u5355\u89c6\u89d2\u6b63\u5219\u5316\u3002", "result": "\u5728GauU-Scene V2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u6548\u679c\u548c\u8868\u9762\u91cd\u5efa\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8eNeRF\u548c\u9ad8\u65af\u6cfc\u6e85\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u7684\u96be\u9898\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u4ee3\u7801\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.17702", "pdf": "https://arxiv.org/pdf/2506.17702", "abs": "https://arxiv.org/abs/2506.17702", "authors": ["Stefan Mengel"], "title": "Lower Bounds for Conjunctive Query Evaluation", "categories": ["cs.DB", "cs.CC"], "comment": "paper for the tutorial at PODS 2025", "summary": "In this tutorial, we will survey known results on the complexity of\nconjunctive query evaluation in different settings, ranging from Boolean\nqueries over counting to more complex models like enumeration and direct\naccess. A particular focus will be on showing how different relatively recent\nhypotheses from complexity theory connect to query answering and allow showing\nthat known algorithms in several cases can likely not be improved.", "AI": {"tldr": "\u672c\u6b21\u6559\u7a0b\u7efc\u8ff0\u4e86\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u8054\u67e5\u8be2\u8bc4\u4f30\u7684\u590d\u6742\u6027\u7ed3\u679c\uff0c\u7740\u91cd\u5c55\u793a\u4e86\u590d\u6742\u6027\u7406\u8bba\u4e2d\u7684\u65b0\u5047\u8bbe\u5982\u4f55\u4e0e\u67e5\u8be2\u56de\u7b54\u76f8\u5173\u8054\uff0c\u5e76\u8bc1\u660e\u67d0\u4e9b\u7b97\u6cd5\u53ef\u80fd\u65e0\u6cd5\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "motivation": "\u65e8\u5728\u603b\u7ed3\u8054\u67e5\u8be2\u8bc4\u4f30\u5728\u4e0d\u540c\u6a21\u578b\u4e0b\u7684\u590d\u6742\u6027\uff0c\u5e76\u63a2\u8ba8\u590d\u6742\u6027\u7406\u8bba\u4e2d\u7684\u65b0\u5047\u8bbe\u5982\u4f55\u5f71\u54cd\u67e5\u8be2\u56de\u7b54\u7684\u7b97\u6cd5\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u5df2\u77e5\u7ed3\u679c\uff0c\u5206\u6790\u4ece\u5e03\u5c14\u67e5\u8be2\u5230\u8ba1\u6570\u3001\u679a\u4e3e\u548c\u76f4\u63a5\u8bbf\u95ee\u7b49\u590d\u6742\u6a21\u578b\u7684\u590d\u6742\u6027\uff0c\u7ed3\u5408\u590d\u6742\u6027\u7406\u8bba\u7684\u65b0\u5047\u8bbe\u8fdb\u884c\u63a2\u8ba8\u3002", "result": "\u5c55\u793a\u4e86\u65b0\u5047\u8bbe\u5982\u4f55\u8bc1\u660e\u67d0\u4e9b\u67e5\u8be2\u56de\u7b54\u7b97\u6cd5\u53ef\u80fd\u5df2\u8fbe\u5230\u6700\u4f18\uff0c\u96be\u4ee5\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "conclusion": "\u590d\u6742\u6027\u7406\u8bba\u7684\u65b0\u5047\u8bbe\u4e3a\u8054\u67e5\u8be2\u8bc4\u4f30\u7684\u7b97\u6cd5\u4e0a\u9650\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u67e5\u8be2\u4f18\u5316\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.17588", "pdf": "https://arxiv.org/pdf/2506.17588", "abs": "https://arxiv.org/abs/2506.17588", "authors": ["Bhaskar Gaur", "Himanshu Thapliyal"], "title": "Residue Number System (RNS) based Distributed Quantum Multiplication", "categories": ["quant-ph", "cs.AR", "cs.DC", "math.QA"], "comment": "4 pages, 4 figures, 4 tables", "summary": "Multiplication of quantum states is a frequently used function or subroutine\nin quantum algorithms and applications, making quantum multipliers an essential\ncomponent of quantum arithmetic. However, quantum multiplier circuits suffer\nfrom high Toffoli depth and T gate usage, which ultimately affects their\nscalability and applicability on quantum computers. To address these issues, we\npropose utilizing the Residue Number System (RNS) based distributed quantum\nmultiplication, which executes multiple quantum modulo multiplication circuits\nacross quantum computers or jobs with lower Toffoli depth and T gate usage.\nTowards this end, we propose a design of Quantum Diminished-1 Modulo $(2^n+1)$\nMultiplier, an essential component of RNS based distributed quantum\nmultiplication. We provide estimates of quantum resource usage and compare them\nwith those of an existing non-distributed quantum multiplier for 6 to 16 qubit\nsized output. Our comparative analysis estimates up to 46.018% lower Toffoli\ndepth, and reduction in T gates of 34.483% to 86.25%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u6b8b\u6570\u7cfb\u7edf\uff08RNS\uff09\u7684\u5206\u5e03\u5f0f\u91cf\u5b50\u4e58\u6cd5\uff0c\u4ee5\u964d\u4f4e\u91cf\u5b50\u4e58\u6cd5\u5668\u7684Toffoli\u6df1\u5ea6\u548cT\u95e8\u4f7f\u7528\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86Quantum Diminished-1 Modulo $(2^n+1)$ Multiplier\u4f5c\u4e3a\u5176\u5173\u952e\u7ec4\u4ef6\u3002", "motivation": "\u91cf\u5b50\u4e58\u6cd5\u5728\u91cf\u5b50\u7b97\u6cd5\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u91cf\u5b50\u4e58\u6cd5\u5668\u5b58\u5728\u9ad8Toffoli\u6df1\u5ea6\u548cT\u95e8\u4f7f\u7528\u91cf\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528RNS\u8fdb\u884c\u5206\u5e03\u5f0f\u91cf\u5b50\u4e58\u6cd5\uff0c\u901a\u8fc7\u591a\u4e2a\u91cf\u5b50\u6a21\u4e58\u6cd5\u7535\u8def\u7684\u5e76\u884c\u6267\u884c\uff0c\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002\u8bbe\u8ba1\u4e86Quantum Diminished-1 Modulo $(2^n+1)$ Multiplier\u4f5c\u4e3a\u6838\u5fc3\u7ec4\u4ef6\u3002", "result": "\u4e0e\u73b0\u6709\u975e\u5206\u5e03\u5f0f\u91cf\u5b50\u4e58\u6cd5\u5668\u76f8\u6bd4\uff0cToffoli\u6df1\u5ea6\u964d\u4f4e\u4e8646.018%\uff0cT\u95e8\u4f7f\u7528\u91cf\u51cf\u5c11\u4e8634.483%\u81f386.25%\u3002", "conclusion": "\u57fa\u4e8eRNS\u7684\u5206\u5e03\u5f0f\u91cf\u5b50\u4e58\u6cd5\u80fd\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\uff0c\u63d0\u5347\u91cf\u5b50\u4e58\u6cd5\u5668\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.17442", "pdf": "https://arxiv.org/pdf/2506.17442", "abs": "https://arxiv.org/abs/2506.17442", "authors": ["Hao Guan", "David Bates", "Li Zhou"], "title": "Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation", "categories": ["cs.AI", "cs.ET", "cs.LG"], "comment": "15 pages, 5 figures", "summary": "Artificial intelligence (AI) is increasingly integrated into modern\nhealthcare, offering powerful support for clinical decision-making. However, in\nreal-world settings, AI systems may experience performance degradation over\ntime, due to factors such as shifting data distributions, changes in patient\ncharacteristics, evolving clinical protocols, and variations in data quality.\nThese factors can compromise model reliability, posing safety concerns and\nincreasing the likelihood of inaccurate predictions or adverse outcomes. This\nreview presents a forward-looking perspective on monitoring and maintaining the\n\"health\" of AI systems in healthcare. We highlight the urgent need for\ncontinuous performance monitoring, early degradation detection, and effective\nself-correction mechanisms. The paper begins by reviewing common causes of\nperformance degradation at both data and model levels. We then summarize key\ntechniques for detecting data and model drift, followed by an in-depth look at\nroot cause analysis. Correction strategies are further reviewed, ranging from\nmodel retraining to test-time adaptation. Our survey spans both traditional\nmachine learning models and state-of-the-art large language models (LLMs),\noffering insights into their strengths and limitations. Finally, we discuss\nongoing technical challenges and propose future research directions. This work\naims to guide the development of reliable, robust medical AI systems capable of\nsustaining safe, long-term deployment in dynamic clinical settings.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u533b\u7597\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6027\u80fd\u9000\u5316\u95ee\u9898\u7684\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u76d1\u63a7\u548c\u7ef4\u62a4AI\u7cfb\u7edf\u5065\u5eb7\u7684\u65b9\u6cd5\u3002", "motivation": "\u533b\u7597AI\u7cfb\u7edf\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u5176\u6027\u80fd\u53ef\u80fd\u56e0\u6570\u636e\u5206\u5e03\u53d8\u5316\u3001\u60a3\u8005\u7279\u5f81\u6539\u53d8\u7b49\u56e0\u7d20\u800c\u4e0b\u964d\uff0c\u5f71\u54cd\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u6587\u7ae0\u56de\u987e\u4e86\u6027\u80fd\u9000\u5316\u7684\u5e38\u89c1\u539f\u56e0\uff0c\u603b\u7ed3\u4e86\u68c0\u6d4b\u6570\u636e\u6f02\u79fb\u548c\u6a21\u578b\u6f02\u79fb\u7684\u6280\u672f\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u6839\u672c\u539f\u56e0\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u4ece\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u5230\u6d4b\u8bd5\u65f6\u9002\u5e94\u7684\u6821\u6b63\u7b56\u7565\u3002", "result": "\u7efc\u8ff0\u6db5\u76d6\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u5e94\u5bf9\u6027\u80fd\u9000\u5316\u7684\u6280\u672f\u65b9\u6cd5\u3002", "conclusion": "\u6587\u7ae0\u6307\u51fa\u4e86\u5f53\u524d\u7684\u6280\u672f\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u6307\u5bfc\u5f00\u53d1\u53ef\u9760\u3001\u7a33\u5065\u7684\u533b\u7597AI\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u957f\u671f\u9700\u6c42\u3002"}}
{"id": "2506.17627", "pdf": "https://arxiv.org/pdf/2506.17627", "abs": "https://arxiv.org/abs/2506.17627", "authors": ["Hongzhou Rao", "Yanjie Zhao", "Wenjie Zhu", "Ling Xiao", "Meizhen Wang", "Haoyu Wang"], "title": "CodeMorph: Mitigating Data Leakage in Large Language Model Assessment", "categories": ["cs.SE"], "comment": "Accepted by ICSE 2025 (Industry Challenge Track)", "summary": "Concerns about benchmark leakage in large language models for code (Code\nLLMs) have raised issues of data contamination and inflated evaluation metrics.\nThe diversity and inaccessibility of many training datasets make it difficult\nto prevent data leakage entirely, even with time lag strategies. Consequently,\ngenerating new datasets through code perturbation has become essential.\nHowever, existing methods often fail to produce complex and diverse variations,\nstruggle with complex cross-file dependencies, and lack support for multiple\nprogramming languages, which limits their effectiveness in enhancing LLM\nevaluations for coding tasks. To fill this gap, we propose CodeMorph, an\napproach designed to support multiple programming languages while preserving\ncross-file dependencies to mitigate data leakage. CodeMorph consists of two\nmain components that work together to enhance the perturbation process. The\nfirst component employs 26 semantic-preserving transformation methods to\niteratively perturb code, generating diverse variations while ensuring that the\nmodified code remains compilable. The second component introduces a genetic\nalgorithm-based selection algorithm, PESO, to identify the more effective\nperturbation method for each iteration by targeting lower similarity scores\nbetween the perturbed and original code, thereby enhancing overall perturbation\neffectiveness. Experimental results demonstrate that after applying CodeMorph,\nthe accuracy of the LLM on code completion tasks across five programming\nlanguages decreased by an average of 24.67%, with Python showing the most\nsignificant reduction at 45%. The similarity score of code optimized by PESO\nis, on average, 7.01% lower than that of randomly perturbed code, peaking at a\nreduction of 42.86%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CodeMorph\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee3\u7801\u6270\u52a8\u548c\u9057\u4f20\u7b97\u6cd5\u964d\u4f4e\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u57fa\u51c6\u6d4b\u8bd5\u6cc4\u6f0f\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u590d\u6742\u591a\u6837\u7684\u4ee3\u7801\u53d8\u5f02\uff0c\u4e14\u4e0d\u652f\u6301\u591a\u8bed\u8a00\u548c\u8de8\u6587\u4ef6\u4f9d\u8d56\u3002", "method": "CodeMorph\u7ed3\u540826\u79cd\u8bed\u4e49\u4fdd\u6301\u7684\u4ee3\u7801\u6270\u52a8\u65b9\u6cd5\u548c\u9057\u4f20\u7b97\u6cd5PESO\uff0c\u8fed\u4ee3\u751f\u6210\u591a\u6837\u5316\u4e14\u53ef\u7f16\u8bd1\u7684\u4ee3\u7801\u53d8\u5f02\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u5e94\u7528CodeMorph\u540e\uff0c\u6a21\u578b\u5728\u4e94\u79cd\u8bed\u8a00\u4e0a\u7684\u4ee3\u7801\u5b8c\u6210\u51c6\u786e\u7387\u5e73\u5747\u4e0b\u964d24.67%\uff0cPESO\u4f18\u5316\u7684\u4ee3\u7801\u76f8\u4f3c\u5ea6\u5e73\u5747\u964d\u4f4e7.01%\u3002", "conclusion": "CodeMorph\u6709\u6548\u7f13\u89e3\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u53ef\u9760\u6027\u3002"}}
{"id": "2506.17936", "pdf": "https://arxiv.org/pdf/2506.17936", "abs": "https://arxiv.org/abs/2506.17936", "authors": ["Romy M\u00fcller"], "title": "When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Concept-based explainable artificial intelligence (C-XAI) can help reveal the\ninner representations of AI models. Understanding these representations is\nparticularly important in complex tasks like safety evaluation. Such tasks rely\non high-level semantic information (e.g., about actions) to make decisions\nabout abstract categories (e.g., whether a situation is dangerous). In this\ncontext, it may desirable for C-XAI concepts to show some variability,\nsuggesting that the AI is capable of generalising beyond the concrete details\nof a situation. However, it is unclear whether people recognise and appreciate\nsuch generalisations and can distinguish them from other, less desirable forms\nof imprecision. This was investigated in an experimental railway safety\nscenario. Participants evaluated the performance of a simulated AI that\nevaluated whether traffic scenes involving people were dangerous. To explain\nthese decisions, the AI provided concepts in the form of similar image\nsnippets. These concepts differed in their match with the classified image,\neither regarding a highly relevant feature (i.e., relation to tracks) or a less\nrelevant feature (i.e., actions). Contrary to the hypotheses, concepts that\ngeneralised over less relevant features led to ratings that were lower than for\nprecisely matching concepts and comparable to concepts that systematically\nmisrepresented these features. Conversely, participants were highly sensitive\nto imprecisions in relevant features. These findings cast doubts on whether\npeople spontaneously recognise generalisations. Accordingly, they might not be\nable to infer from C-XAI concepts whether AI models have gained a deeper\nunderstanding of complex situations.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86C-XAI\u6982\u5ff5\u7684\u53ef\u53d8\u6027\u662f\u5426\u88ab\u7528\u6237\u8bc6\u522b\u548c\u6b23\u8d4f\uff0c\u53d1\u73b0\u5728\u94c1\u8def\u5b89\u5168\u573a\u666f\u4e2d\uff0c\u7528\u6237\u5bf9\u4e0d\u76f8\u5173\u7279\u5f81\u6cdb\u5316\u7684\u6982\u5ff5\u8bc4\u4ef7\u8f83\u4f4e\u3002", "motivation": "\u7814\u7a76\u7528\u6237\u662f\u5426\u80fd\u591f\u8bc6\u522b\u548c\u6b23\u8d4fC-XAI\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u5b89\u5168\u8bc4\u4f30\uff09\u4e2d\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u94c1\u8def\u5b89\u5168\u573a\u666f\uff0c\u53c2\u4e0e\u8005\u8bc4\u4f30\u6a21\u62dfAI\u7684\u6027\u80fd\uff0cAI\u63d0\u4f9b\u7684\u6982\u5ff5\u4ee5\u76f8\u4f3c\u56fe\u50cf\u7247\u6bb5\u5f62\u5f0f\u5448\u73b0\uff0c\u5206\u4e3a\u9ad8\u76f8\u5173\u7279\u5f81\u548c\u4f4e\u76f8\u5173\u7279\u5f81\u7684\u5339\u914d\u3002", "result": "\u7528\u6237\u5bf9\u4e0d\u76f8\u5173\u7279\u5f81\u6cdb\u5316\u7684\u6982\u5ff5\u8bc4\u4ef7\u8f83\u4f4e\uff0c\u4f46\u5bf9\u76f8\u5173\u7279\u5f81\u7684\u4e0d\u7cbe\u786e\u975e\u5e38\u654f\u611f\u3002", "conclusion": "\u7528\u6237\u53ef\u80fd\u65e0\u6cd5\u81ea\u53d1\u8bc6\u522b\u6cdb\u5316\uff0c\u8d28\u7591C-XAI\u662f\u5426\u80fd\u5c55\u793aAI\u5bf9\u590d\u6742\u60c5\u5883\u7684\u6df1\u5c42\u7406\u89e3\u3002"}}
{"id": "2506.17351", "pdf": "https://arxiv.org/pdf/2506.17351", "abs": "https://arxiv.org/abs/2506.17351", "authors": ["Mostafa Shahin", "Beena Ahmed", "Julien Epps"], "title": "Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "comment": null, "summary": "Cognitive impairment (CI) is of growing public health concern, and early\ndetection is vital for effective intervention. Speech has gained attention as a\nnon-invasive and easily collectible biomarker for assessing cognitive decline.\nTraditional CI detection methods typically rely on supervised models trained on\nacoustic and linguistic features extracted from speech, which often require\nmanual annotation and may not generalise well across datasets and languages. In\nthis work, we propose the first zero-shot speech-based CI detection method\nusing the Qwen2- Audio AudioLLM, a model capable of processing both audio and\ntext inputs. By designing prompt-based instructions, we guide the model in\nclassifying speech samples as indicative of normal cognition or cognitive\nimpairment. We evaluate our approach on two datasets: one in English and\nanother multilingual, spanning different cognitive assessment tasks. Our\nresults show that the zero-shot AudioLLM approach achieves performance\ncomparable to supervised methods and exhibits promising generalizability and\nconsistency across languages, tasks, and datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u6837\u672c\u5b66\u4e60\u7684\u8bed\u97f3\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528Qwen2-Audio AudioLLM\u6a21\u578b\u5904\u7406\u8bed\u97f3\u548c\u6587\u672c\u8f93\u5165\uff0c\u901a\u8fc7\u8bbe\u8ba1\u63d0\u793a\u6307\u4ee4\u5b9e\u73b0\u5206\u7c7b\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u4e14\u5728\u591a\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u8ba4\u77e5\u969c\u788d\uff08CI\uff09\u65e9\u671f\u68c0\u6d4b\u5bf9\u516c\u5171\u536b\u751f\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6709\u76d1\u7763\u6a21\u578b\u4e14\u6cdb\u5316\u6027\u5dee\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528Qwen2-Audio AudioLLM\u6a21\u578b\uff0c\u8bbe\u8ba1\u63d0\u793a\u6307\u4ee4\u5b9e\u73b0\u96f6\u6837\u672c\u5206\u7c7b\uff0c\u65e0\u9700\u624b\u52a8\u63d0\u53d6\u7279\u5f81\u6216\u8bad\u7ec3\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u82f1\u8bed\u548c\u591a\u8bed\u8a00\uff09\u4e0a\u8bc4\u4f30\uff0c\u96f6\u6837\u672c\u65b9\u6cd5\u6027\u80fd\u63a5\u8fd1\u6709\u76d1\u7763\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u8272\u7684\u6cdb\u5316\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u96f6\u6837\u672cAudioLLM\u65b9\u6cd5\u4e3a\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u591a\u8bed\u8a00\u548c\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.18439", "pdf": "https://arxiv.org/pdf/2506.18439", "abs": "https://arxiv.org/abs/2506.18439", "authors": ["Deren Lin", "Tianrong Lin"], "title": "Computational Complexity of Model-Checking Quantum Pushdown Systems", "categories": ["cs.LO", "cs.CC", "68Q15, 03B25"], "comment": "[v1] first edition, mistakes are inevitable; comments are welcome.\n  arXiv admin note: substantial text overlap with arXiv:2209.10517", "summary": "In this paper, we study the problem of model-checking quantum pushdown\nsystems from a computational complexity point of view. We arrive at the\nfollowing equally important, interesting new results:\n  We first extend the notions of the {\\it probabilistic pushdown systems} and\n{\\it Markov chains} to their quantum analogues and investigate the question of\nwhether it is necessary to define a quantum analogue of {\\it probabilistic\ncomputational tree logic} to describe the probabilistic and branching-time\nproperties of the {\\it quantum Markov chain}. We study its model-checking\nquestion and show that model-checking of {\\it stateless quantum pushdown\nsystems (qBPA)} against {\\it probabilistic computational tree logic (PCTL)} is\ngenerally undecidable, i.e., there exists no algorithm for model-checking {\\it\nstateless quantum pushdown systems} against {\\it probabilistic computational\ntree logic}.\n  We then study in which case there exists an algorithm for model-checking {\\it\nstateless quantum pushdown systems} and show that the problem of model-checking\n{\\it stateless quantum pushdown systems} against {\\it bounded probabilistic\ncomputational tree logic} (bPCTL) is decidable, and further show that this\nproblem is in $NP$-complete. Our reduction is from the {\\it bounded Post\nCorrespondence Problem} for the first time, a well-known $NP$-complete problem.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4ece\u8ba1\u7b97\u590d\u6742\u6027\u89d2\u5ea6\u5bf9\u91cf\u5b50\u4e0b\u63a8\u7cfb\u7edf\u8fdb\u884c\u6a21\u578b\u68c0\u9a8c\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u91cf\u5b50\u4e0b\u63a8\u7cfb\u7edf\u548c\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u65b0\u6982\u5ff5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u4e0e\u6982\u7387\u8ba1\u7b97\u6811\u903b\u8f91\u7684\u5173\u7cfb\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u65e0\u72b6\u6001\u91cf\u5b50\u4e0b\u63a8\u7cfb\u7edf\u5bf9PCTL\u7684\u6a21\u578b\u68c0\u9a8c\u901a\u5e38\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\uff0c\u4f46\u5bf9\u6709\u754c\u7684PCTL\u662f\u53ef\u5224\u5b9a\u7684\uff0c\u4e14\u8be5\u95ee\u9898\u662fNP\u5b8c\u5168\u7684\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u7684\u53d1\u5c55\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u63cf\u8ff0\u548c\u9a8c\u8bc1\u91cf\u5b50\u7cfb\u7edf\u7684\u884c\u4e3a\uff0c\u56e0\u6b64\u7814\u7a76\u91cf\u5b50\u4e0b\u63a8\u7cfb\u7edf\u7684\u6a21\u578b\u68c0\u9a8c\u95ee\u9898\u5177\u6709\u91cd\u8981\u7406\u8bba\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u6982\u7387\u4e0b\u63a8\u7cfb\u7edf\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u5230\u91cf\u5b50\u7248\u672c\uff0c\u5e76\u63a2\u8ba8\u662f\u5426\u9700\u8981\u5b9a\u4e49\u91cf\u5b50\u6982\u7387\u8ba1\u7b97\u6811\u903b\u8f91\u3002\u7814\u7a76\u4e86\u65e0\u72b6\u6001\u91cf\u5b50\u4e0b\u63a8\u7cfb\u7edf\u5bf9PCTL\u548cbPCTL\u7684\u6a21\u578b\u68c0\u9a8c\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u65e0\u72b6\u6001\u91cf\u5b50\u4e0b\u63a8\u7cfb\u7edf\u5bf9PCTL\u7684\u6a21\u578b\u68c0\u9a8c\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\uff0c\u4f46\u5bf9bPCTL\u662f\u53ef\u5224\u5b9a\u7684\u4e14\u4e3aNP\u5b8c\u5168\u95ee\u9898\u3002", "conclusion": "\u91cf\u5b50\u4e0b\u63a8\u7cfb\u7edf\u7684\u6a21\u578b\u68c0\u9a8c\u95ee\u9898\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u662f\u53ef\u5224\u5b9a\u7684\uff0c\u4f46\u4e00\u822c\u60c5\u51b5\u4e0b\u4e0d\u53ef\u5224\u5b9a\uff0c\u8fd9\u4e3a\u91cf\u5b50\u8ba1\u7b97\u7684\u7406\u8bba\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u590d\u6742\u6027\u7ed3\u679c\u3002"}}
{"id": "2506.17658", "pdf": "https://arxiv.org/pdf/2506.17658", "abs": "https://arxiv.org/abs/2506.17658", "authors": ["Qiong Liu", "Jianke Lin", "Tianzhu Zhang", "Leonardo Linguaglossa"], "title": "Non-Intrusive MLOps-Driven Performance Intelligence in Software Data Planes", "categories": ["cs.NI"], "comment": null, "summary": "The last decade has witnessed the proliferation of network function\nvirtualization (NFV) in the telco industry, thanks to its unparalleled\nflexibility, scalability, and cost-effectiveness. However, as the NFV\ninfrastructure is shared by virtual network functions (VNFs), sporadic resource\ncontentions are inevitable. Such contention makes it extremely challenging to\nguarantee the performance of the provisioned network services, especially in\nhigh-speed regimes (e.g., Gigabit Ethernet). Existing solutions typically rely\non direct traffic analysis (e.g., packet- or flow-level measurements) to detect\nperformance degradation and identify bottlenecks, which is not always\napplicable due to significant integration overhead and system-level\nconstraints.\n  This paper complements existing solutions with a lightweight, non-intrusive\nframework for online performance inference and adaptation. Instead of direct\ndata-plane collection, we reuse hardware features in the underlying NFV\ninfrastructure, introducing negligible interference in the data plane. This\nframework can be integrated into existing NFV systems with minimal engineering\neffort and operates without the need for predefined traffic models or\nVNF-specific customization. Through comprehensive evaluation across diverse NFV\nscenarios, our Drift-Resilient and Self-Tuning (DRST) framework delivers\naccurate performance inference, runtime bottleneck diagnose, and automated\nadaptation under runtime drift, via a lightweight MLOps pipeline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u975e\u4fb5\u5165\u5f0f\u7684\u5728\u7ebf\u6027\u80fd\u63a8\u65ad\u548c\u9002\u5e94\u6846\u67b6DRST\uff0c\u7528\u4e8e\u89e3\u51b3NFV\u57fa\u7840\u8bbe\u65bd\u4e2d\u8d44\u6e90\u7ade\u4e89\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "NFV\u56e0\u5176\u7075\u6d3b\u6027\u548c\u6210\u672c\u6548\u76ca\u800c\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8d44\u6e90\u5171\u4eab\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\u5728\u9ad8\u5e26\u5bbd\u73af\u5883\u4e0b\u5c24\u4e3a\u7a81\u51fa\uff0c\u73b0\u6709\u65b9\u6848\u56e0\u96c6\u6210\u590d\u6742\u548c\u7cfb\u7edf\u9650\u5236\u96be\u4ee5\u9002\u7528\u3002", "method": "\u901a\u8fc7\u590d\u7528\u5e95\u5c42NFV\u57fa\u7840\u8bbe\u65bd\u7684\u786c\u4ef6\u7279\u6027\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u6570\u636e\u5e73\u9762\u91c7\u96c6\uff0c\u63d0\u51faDRST\u6846\u67b6\uff0c\u652f\u6301\u65e0\u9700\u9884\u5b9a\u4e49\u6d41\u91cf\u6a21\u578b\u6216VNF\u5b9a\u5236\u5316\u7684\u8f7b\u91cf\u7ea7MLOps\u6d41\u7a0b\u3002", "result": "DRST\u6846\u67b6\u5728\u591a\u6837\u5316NFV\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u51c6\u786e\u7684\u6027\u80fd\u63a8\u65ad\u3001\u8fd0\u884c\u65f6\u74f6\u9888\u8bca\u65ad\u548c\u81ea\u52a8\u5316\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8865\u5145\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u5f00\u9500\u3001\u9ad8\u6548\u7684\u6027\u80fd\u7ba1\u7406\u65b9\u6cd5\u3002"}}
{"id": "2506.17770", "pdf": "https://arxiv.org/pdf/2506.17770", "abs": "https://arxiv.org/abs/2506.17770", "authors": ["Tomas Akenine-M\u00f6ller", "Pontus Ebelin", "Matt Pharr", "Bartlomiej Wronski"], "title": "Collaborative Texture Filtering", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to ACM/EG Symposium on High Performance Graphics (HPG), 2025", "summary": "Recent advances in texture compression provide major improvements in\ncompression ratios, but cannot use the GPU's texture units for decompression\nand filtering. This has led to the development of stochastic texture filtering\n(STF) techniques to avoid the high cost of multiple texel evaluations with such\nformats. Unfortunately, those methods can give undesirable visual appearance\nchanges under magnification and may contain visible noise and flicker despite\nthe use of spatiotemporal denoisers. Recent work substantially improves the\nquality of magnification filtering with STF by sharing decoded texel values\nbetween nearby pixels (Wronski 2025). Using GPU wave communication intrinsics,\nthis sharing can be performed inside actively executing shaders without memory\ntraffic overhead. We take this idea further and present novel algorithms that\nuse wave communication between lanes to avoid repeated texel decompression\nprior to filtering. By distributing unique work across lanes, we can achieve\nzero-error filtering using <=1 texel evaluations per pixel given a sufficiently\nlarge magnification factor. For the remaining cases, we propose novel filtering\nfallback methods that also achieve higher quality than prior approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7eb9\u7406\u538b\u7f29\u7b97\u6cd5\uff0c\u5229\u7528GPU\u6ce2\u901a\u4fe1\u6280\u672f\u907f\u514d\u4e86\u91cd\u590d\u7684\u7eb9\u7406\u89e3\u538b\u7f29\uff0c\u4ece\u800c\u5728\u653e\u5927\u8fc7\u6ee4\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u753b\u8d28\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u7eb9\u7406\u538b\u7f29\u6280\u672f\u5728\u653e\u5927\u8fc7\u6ee4\u65f6\u4f1a\u5bfc\u81f4\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u3001\u566a\u58f0\u548c\u95ea\u70c1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u538b\u7f29\u548c\u8fc7\u6ee4\u65b9\u6cd5\u3002", "method": "\u5229\u7528GPU\u6ce2\u901a\u4fe1\u6280\u672f\u5728\u7740\u8272\u5668\u4e2d\u5171\u4eab\u5df2\u89e3\u7801\u7684\u7eb9\u7406\u503c\uff0c\u907f\u514d\u91cd\u590d\u89e3\u538b\u7f29\uff0c\u5e76\u4e3a\u7279\u6b8a\u60c5\u51b5\u8bbe\u8ba1\u4e86\u9ad8\u8d28\u91cf\u7684\u5907\u7528\u8fc7\u6ee4\u65b9\u6cd5\u3002", "result": "\u5728\u8db3\u591f\u5927\u7684\u653e\u5927\u56e0\u5b50\u4e0b\uff0c\u5b9e\u73b0\u4e86\u96f6\u8bef\u5dee\u8fc7\u6ee4\uff0c\u4e14\u6bcf\u6b21\u50cf\u7d20\u4ec5\u9700\u22641\u6b21\u7eb9\u7406\u8bc4\u4f30\uff1b\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u7684\u5907\u7528\u65b9\u6cd5\u4e5f\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7eb9\u7406\u538b\u7f29\u5728\u653e\u5927\u8fc7\u6ee4\u65f6\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u6548\u7387\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7f3a\u9677\u3002"}}
{"id": "2506.18013", "pdf": "https://arxiv.org/pdf/2506.18013", "abs": "https://arxiv.org/abs/2506.18013", "authors": ["Muhammad Farhan", "Henning Koehler", "Qing Wang"], "title": "Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road Networks", "categories": ["cs.DB", "cs.DS"], "comment": null, "summary": "Computing the shortest-path distance between any two given vertices in road\nnetworks is an important problem. A tremendous amount of research has been\nconducted to address this problem, most of which are limited to static road\nnetworks. Since road networks undergo various real-time traffic conditions,\nthere is a pressing need to address this problem for dynamic road networks.\nExisting state-of-the-art methods incrementally maintain an indexing structure\nto reflect dynamic changes on road networks. However, these methods suffer from\neither slow query response time or poor maintenance performance, particularly\nwhen road networks are large. In this work, we propose an efficient solution\n\\emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road\nnetworks from a novel perspective, which incorporates two hierarchies with\ndifferent but complementary data structures to support efficient query and\nupdate processing. Specifically, our proposed solution is comprised of three\nmain components: \\emph{query hierarchy}, \\emph{update hierarchy}, and\n\\emph{hierarchical labelling}, where \\emph{query hierarchy} enables efficient\nquery answering by exploring only a small subset of vertices in the labels of\ntwo query vertices and \\emph{update hierarchy} supports efficient maintenance\nof distance labelling under edge weight increase or decrease. We further\ndevelop dynamic algorithms to reflect dynamic changes by efficiently\nmaintaining the update hierarchy and hierarchical labelling. We also propose a\nparallel variant of our dynamic algorithms by exploiting labelling structure.\nWe evaluate our methods on 10 large road networks and it shows that our methods\nsignificantly outperform the state-of-the-art methods, i.e., achieving\nconsiderably faster construction and update time, while being consistently 2-4\ntimes faster in terms of query processing and consuming only 10\\%-20\\%\nlabelling space.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aDual-Hierarchy Labelling (DHL)\u7684\u9ad8\u6548\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u9053\u8def\u7f51\u7edc\u4e2d\u5feb\u901f\u8ba1\u7b97\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\uff0c\u901a\u8fc7\u53cc\u91cd\u5c42\u6b21\u7ed3\u6784\u4f18\u5316\u67e5\u8be2\u548c\u7ef4\u62a4\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u9053\u8def\u7f51\u7edc\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u67e5\u8be2\u54cd\u5e94\u6162\u6216\u7ef4\u62a4\u6548\u7387\u4f4e\uff0c\u5c24\u5176\u662f\u5927\u578b\u7f51\u7edc\u3002", "method": "\u7ed3\u5408\u67e5\u8be2\u5c42\u6b21\u548c\u66f4\u65b0\u5c42\u6b21\u7684\u53cc\u91cd\u7ed3\u6784\uff0c\u652f\u6301\u9ad8\u6548\u67e5\u8be2\u548c\u7ef4\u62a4\uff1b\u63d0\u51fa\u5e76\u884c\u7b97\u6cd5\u4f18\u5316\u52a8\u6001\u66f4\u65b0\u3002", "result": "\u572810\u4e2a\u5927\u578b\u9053\u8def\u7f51\u7edc\u4e2d\u6d4b\u8bd5\uff0cDHL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u67e5\u8be2\u901f\u5ea6\u63d0\u9ad82-4\u500d\uff0c\u6784\u5efa\u548c\u7ef4\u62a4\u65f6\u95f4\u66f4\u5feb\uff0c\u6807\u7b7e\u7a7a\u95f4\u4ec5\u970010-20%\u3002", "conclusion": "DHL\u4e3a\u52a8\u6001\u9053\u8def\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.17935", "pdf": "https://arxiv.org/pdf/2506.17935", "abs": "https://arxiv.org/abs/2506.17935", "authors": ["Zhengwu Huang", "Ding Deng", "Pengyue Sun", "Guangfu Sun", "Xiaomei Tang"], "title": "Cost-Effective Optimization and Implementation of the CRT-Paillier Decryption Algorithm for Enhanced Performance", "categories": ["cs.CR", "cs.AR"], "comment": "19 pages,7 figures", "summary": "To address the privacy protection problem in cloud computing, privacy\nenhancement techniques such as the Paillier additive homomorphism algorithm are\nreceiving widespread attention. Paillier algorithm allows addition and scalar\nmultiplication operations in dencrypted state, which can effectively protect\nprivacy. However, its computational efficiency is limited by complex modulo\noperations due to the ciphertext expansion followed by encryption. To\naccelerate its decryption operation, the Chinese Remainder Theorem (CRT) is\noften used to optimize these modulo operations, which lengthens the decryption\ncomputation chain in turn. To address this issue, we propose an eCRT-Paillier\ndecryption algorithm that shortens the decryption computation chain by\ncombining precomputed parameters and eliminating extra judgment operations\nintroduced by Montgomery modular multiplications. These two improvements reduce\n50% modular multiplications and 60% judgment operations in the postprocessing\nof the CRT-Paillier decryption algorithm. Based on these improvements, we\npropose a highly parallel full-pipeline architecture to eliminate stalls caused\nby multiplier reuse in traditional modular exponentiation operations. This\narchitecture also adopts some optimizations such as simplifying modular\nexponentiation units by dividing the exponent into segments and parallelizing\ndata flow by multi-core instantiation. Finally, a high-throughput and efficient\nPaillier accelerator named MESA was implemented on the Xilinx Virtex-7 FPGA for\nevaluation, which can complete a decryption using 2048-bit key within 0.577ms\nunder 100 MHz clock frequency. Compared to prior works, MESA demonstrates a\nthroughput improvement of 1.16 to 313.21 under identical conditions, also with\nenhancements in area efficiency for LUT, DSP, and FF of 3.32 to 117.55, 1.49 to\n1.64, and 2.94 to 9.94, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684eCRT-Paillier\u89e3\u5bc6\u7b97\u6cd5\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u53c2\u6570\u548c\u4f18\u5316Montgomery\u6a21\u4e58\u64cd\u4f5c\uff0c\u51cf\u5c11\u8ba1\u7b97\u94fe\u957f\u5ea6\u548c\u5224\u65ad\u64cd\u4f5c\uff0c\u63d0\u5347\u89e3\u5bc6\u6548\u7387\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5e76\u884c\u67b6\u6784MESA\u3002", "motivation": "\u5f53\u524dPaillier\u7b97\u6cd5\u5728\u89e3\u5bc6\u65f6\u8ba1\u7b97\u6548\u7387\u53d7\u9650\u4e8e\u590d\u6742\u7684\u6a21\u8fd0\u7b97\u548c\u5bc6\u7801\u6269\u5c55\uff0c\u4f7f\u7528CRT\u4f18\u5316\u867d\u80fd\u52a0\u901f\u6a21\u8fd0\u7b97\uff0c\u4f46\u4f1a\u5ef6\u957f\u89e3\u5bc6\u8ba1\u7b97\u94fe\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u6cd5\u3002", "method": "\u7ed3\u5408\u9884\u8ba1\u7b97\u53c2\u6570\u548c\u6d88\u9664\u5197\u4f59\u5224\u65ad\u64cd\u4f5c\uff0c\u7f29\u77ed\u89e3\u5bc6\u94fe\uff1b\u8bbe\u8ba1\u5e76\u884c\u5316\u5168\u6d41\u6c34\u7ebf\u67b6\u6784\uff0c\u907f\u514d\u4e58\u6cd5\u5668\u590d\u7528\u5e26\u6765\u7684\u505c\u6ede\u3002", "result": "MESA\u57282048\u4f4d\u5bc6\u94a5\u4e0b\u89e3\u5bc6\u4ec5\u97000.577ms\uff08100 MHz\uff09\uff0c\u541e\u5410\u91cf\u63d0\u53471.16\u81f3313.21\u500d\uff0c\u8d44\u6e90\u6548\u7387\u4e5f\u663e\u8457\u63d0\u5347\u3002", "conclusion": "eCRT-Paillier\u548cMESA\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86Paillier\u7b97\u6cd5\u7684\u89e3\u5bc6\u6548\u7387\u548c\u5e76\u884c\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u9ad8\u6027\u80fd\u9690\u79c1\u4fdd\u62a4\u573a\u666f\u3002"}}
{"id": "2506.17508", "pdf": "https://arxiv.org/pdf/2506.17508", "abs": "https://arxiv.org/abs/2506.17508", "authors": ["Sajratul Y. Rubaiat", "Syed N. Sakib", "Hasan M. Jamil"], "title": "Mapping the Evolution of Research Contributions using KnoVo", "categories": ["cs.DL", "cs.AI", "cs.DB", "cs.ET", "cs.IR"], "comment": null, "summary": "This paper presents KnoVo (Knowledge Evolution), an intelligent framework\ndesigned for quantifying and analyzing the evolution of research novelty in the\nscientific literature. Moving beyond traditional citation analysis, which\nprimarily measures impact, KnoVo determines a paper's novelty relative to both\nprior and subsequent work within its multilayered citation network. Given a\ntarget paper's abstract, KnoVo utilizes Large Language Models (LLMs) to\ndynamically extract dimensions of comparison (e.g., methodology, application,\ndataset). The target paper is then compared to related publications along these\nsame extracted dimensions. This comparative analysis, inspired by tournament\nselection, yields quantitative novelty scores reflecting the relative\nimprovement, equivalence, or inferiority of the target paper in specific\naspects. By aggregating these scores and visualizing their progression, for\ninstance, through dynamic evolution graphs and comparative radar charts, KnoVo\nfacilitates researchers not only to assess originality and identify similar\nwork, but also to track knowledge evolution along specific research dimensions,\nuncover research gaps, and explore cross-disciplinary connections. We\ndemonstrate these capabilities through a detailed analysis of 20 diverse papers\nfrom multiple scientific fields and report on the performance of various\nopen-source LLMs within the KnoVo framework.", "AI": {"tldr": "KnoVo\u662f\u4e00\u4e2a\u667a\u80fd\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u6790\u79d1\u7814\u6587\u732e\u4e2d\u7684\u7814\u7a76\u65b0\u9896\u6027\uff0c\u901a\u8fc7\u591a\u5c42\u5f15\u6587\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u6bd4\u8f83\u7ef4\u5ea6\uff0c\u5e76\u751f\u6210\u5b9a\u91cf\u65b0\u9896\u6027\u5206\u6570\u3002", "motivation": "\u4f20\u7edf\u5f15\u6587\u5206\u6790\u4e3b\u8981\u8861\u91cf\u7814\u7a76\u5f71\u54cd\uff0cKnoVo\u65e8\u5728\u901a\u8fc7\u65b0\u9896\u6027\u5206\u6790\u5e2e\u52a9\u8bc4\u4f30\u539f\u521b\u6027\u3001\u53d1\u73b0\u7814\u7a76\u7a7a\u767d\u548c\u8de8\u5b66\u79d1\u8054\u7cfb\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u52a8\u6001\u63d0\u53d6\u6bd4\u8f83\u7ef4\u5ea6\uff08\u5982\u65b9\u6cd5\u3001\u5e94\u7528\u3001\u6570\u636e\u96c6\uff09\uff0c\u5e76\u901a\u8fc7\u9526\u6807\u8d5b\u5f0f\u5206\u6790\u751f\u6210\u65b0\u9896\u6027\u5206\u6570\u3002", "result": "\u901a\u8fc720\u7bc7\u591a\u9886\u57df\u8bba\u6587\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u5c55\u793a\u4e86KnoVo\u7684\u80fd\u529b\uff0c\u5e76\u8bc4\u4f30\u4e86\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "KnoVo\u4e3a\u79d1\u7814\u65b0\u9896\u6027\u5206\u6790\u63d0\u4f9b\u4e86\u521b\u65b0\u5de5\u5177\uff0c\u652f\u6301\u77e5\u8bc6\u6f14\u53d8\u7684\u53ef\u89c6\u5316\u548c\u7814\u7a76\u7a7a\u767d\u7684\u53d1\u73b0\u3002"}}
{"id": "2506.17638", "pdf": "https://arxiv.org/pdf/2506.17638", "abs": "https://arxiv.org/abs/2506.17638", "authors": ["Yanzhou Mu", "Rong Wang", "Juan Zhai", "Chunrong Fang", "Xiang Chen", "Zhiyuan Peng", "Peiran Yang", "Ruixiang Qian", "Shaoyu Yang", "Zhenyu Chen"], "title": "Deep Learning Framework Testing via Model Mutation: How Far Are We?", "categories": ["cs.SE"], "comment": "27 pages, 9 figures", "summary": "Deep Learning (DL) frameworks are a fundamental component of DL development.\nTherefore, the detection of DL framework defects is important and challenging.\nAs one of the most widely adopted DL testing techniques, model mutation has\nrecently gained significant attention. In this study, we revisit the defect\ndetection ability of existing mutation-based testing methods and investigate\nthe factors that influence their effectiveness. To begin with, we reviewed\nexisting methods and observed that many of them mutate DL models (e.g.,\nchanging their parameters) without any customization, ignoring the unique\nchallenges in framework testing. Another issue with these methods is their\nlimited effectiveness, characterized by a high rate of false positives caused\nby illegal mutations arising from the use of generic, non-customized mutation\noperators. Moreover, we tracked the defects identified by these methods and\ndiscovered that most of them were ignored by developers. Motivated by these\nobservations, we investigate the effectiveness of existing mutation-based\ntesting methods in detecting important defects that have been authenticated by\nframework developers. We begin by collecting defect reports from three popular\nframeworks and classifying them based on framework developers' ratings to build\na comprehensive dataset. We then perform an in-depth analysis to uncover\nvaluable insights. Based on our findings, we propose optimization strategies to\naddress the shortcomings of existing approaches. Following these optimizations,\nwe identified seven new defects, four of which were confirmed by developers as\nhigh-priority issues, with three resolved. In summary, we identified 39 unique\ndefects across just 23 models, of which 31 were confirmed by developers, and\neight have been fixed.", "AI": {"tldr": "\u91cd\u65b0\u8bc4\u4f30\u4e86\u73b0\u6709\u57fa\u4e8e\u53d8\u5f02\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7f3a\u9677\u68c0\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u6210\u529f\u8bc6\u522b\u4e86\u591a\u4e2a\u9ad8\u4f18\u5148\u7ea7\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7f3a\u9677\u65f6\u5b58\u5728\u9ad8\u5047\u9633\u6027\u7387\u548c\u5ffd\u89c6\u72ec\u7279\u6311\u6218\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5f00\u53d1\u8005\u5ffd\u7565\u5927\u90e8\u5206\u53d1\u73b0\u7684\u7f3a\u9677\u3002", "method": "\u6536\u96c6\u4e86\u4e09\u4e2a\u6d41\u884c\u6846\u67b6\u7684\u7f3a\u9677\u62a5\u544a\u5e76\u5206\u7c7b\uff0c\u6df1\u5165\u5206\u6790\u540e\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u7b56\u7565\u3002", "result": "\u4f18\u5316\u540e\uff0c\u8bc6\u522b\u4e8639\u4e2a\u72ec\u7279\u7f3a\u9677\uff08\u5176\u4e2d31\u4e2a\u88ab\u5f00\u53d1\u8005\u786e\u8ba4\uff0c8\u4e2a\u5df2\u4fee\u590d\uff09\uff0c\u5305\u62ec7\u4e2a\u65b0\u7f3a\u9677\uff084\u4e2a\u88ab\u6807\u8bb0\u4e3a\u9ad8\u4f18\u5148\u7ea7\uff0c3\u4e2a\u5df2\u89e3\u51b3\uff09\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u4f18\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\uff0c\u4e14\u5f00\u53d1\u8005\u786e\u8ba4\u548c\u4fee\u590d\u7387\u8f83\u9ad8\u3002"}}
{"id": "2506.18119", "pdf": "https://arxiv.org/pdf/2506.18119", "abs": "https://arxiv.org/abs/2506.18119", "authors": ["Jaime Banks", "Zhixin Li"], "title": "Conceptualization, Operationalization, and Measurement of Machine Companionship: A Scoping Review", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The notion of machine companions has long been embedded in\nsocial-technological imaginaries. Recent advances in AI have moved those media\nmusings into believable sociality manifested in interfaces, robotic bodies, and\ndevices. Those machines are often referred to colloquially as \"companions\" yet\nthere is little careful engagement of machine companionship (MC) as a formal\nconcept or measured variable. This PRISMA-guided scoping review systematically\nsamples, surveys, and synthesizes current scholarly works on MC (N = 71;\n2017-2025), to that end. Works varied widely in considerations of MC according\nto guiding theories, dimensions of a-priori specified properties (subjectively\npositive, sustained over time, co-active, autotelic), and in measured concepts\n(with more than 50 distinct measured variables). WE ultimately offer a\nliterature-guided definition of MC as an autotelic, coordinated connection\nbetween human and machine that unfolds over time and is subjectively positive.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u673a\u5668\u4f34\u4fa3\uff08MC\uff09\u4f5c\u4e3a\u4e00\u4e2a\u6b63\u5f0f\u6982\u5ff5\u7684\u5b9a\u4e49\u548c\u6d4b\u91cf\u53d8\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u6587\u732e\u6307\u5bfc\u7684\u5b9a\u4e49\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u4f34\u4fa3\u5728\u793e\u4ea4\u6280\u672f\u60f3\u8c61\u4e2d\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u4f5c\u4e3a\u6b63\u5f0f\u6982\u5ff5\u6216\u6d4b\u91cf\u53d8\u91cf\u7684\u6df1\u5165\u7814\u7a76\u3002", "method": "\u91c7\u7528PRISMA\u6307\u5bfc\u7684\u8303\u56f4\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7b5b\u9009\u3001\u8c03\u67e5\u548c\u7efc\u5408\u4e862017\u81f32025\u5e74\u768471\u7bc7\u5b66\u672f\u6587\u732e\u3002", "result": "\u7814\u7a76\u53d1\u73b0MC\u7684\u5b9a\u4e49\u548c\u6d4b\u91cf\u53d8\u91cf\u5b58\u5728\u5e7f\u6cdb\u5dee\u5f02\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86MC\u7684\u5b9a\u4e49\uff1a\u4e00\u79cd\u81ea\u4e3b\u7684\u3001\u534f\u8c03\u7684\u4eba\u673a\u5173\u7cfb\uff0c\u968f\u65f6\u95f4\u5c55\u5f00\u4e14\u4e3b\u89c2\u79ef\u6781\u3002", "conclusion": "\u8bba\u6587\u4e3a\u673a\u5668\u4f34\u4fa3\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9a\u4e49\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.17499", "pdf": "https://arxiv.org/pdf/2506.17499", "abs": "https://arxiv.org/abs/2506.17499", "authors": ["Xuanyu Zhuang", "Geoffroy Peeters", "Ga\u00ebl Richard"], "title": "Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training", "categories": ["cs.LG", "cs.MM", "cs.SD"], "comment": null, "summary": "In few-shot classification tasks (so-called episodes), a small set of labeled\nsupport samples is provided during inference to aid the classification of\nunlabeled query samples. Metric-based models typically operate by computing\nsimilarities between query and support embeddings within a learned metric\nspace, followed by nearest-neighbor classification. However, these labeled\nsupport samples are often underutilized--they are only used for similarity\ncomparison, despite their potential to fine-tune and adapt the metric space\nitself to the classes in the current episode. To address this, we propose a\nseries of simple yet effective episode-specific, during-inference fine-tuning\nmethods for metric-based models, including Rotational Division Fine-Tuning\n(RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and\nAugmented Division Fine-Tuning (ADFT). These methods construct pseudo\nsupport-query pairs from the given support set to enable fine-tuning even for\nnon-parametric models. Nevertheless, the severely limited amount of data in\neach task poses a substantial risk of overfitting when applying such\nfine-tuning strategies. To mitigate this, we further propose to train the\nmetric-based model within an optimization-based meta-learning framework. With\nthe combined efforts of episode-specific fine-tuning and optimization-based\nmeta-training, metric-based models are equipped with the ability to rapidly\nadapt to the limited support samples during inference while avoiding\noverfitting. We validate our approach on three audio datasets from diverse\ndomains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken\nkeywords), and Medley-solos-DB (musical instrument). Experimental results\ndemonstrate that our approach consistently improves performance for all\nevaluated metric-based models (especially for attention-based models) and\ngeneralizes well across different audio domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9488\u5bf9\u5c0f\u6837\u672c\u5206\u7c7b\u4efb\u52a1\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u652f\u6301\u6837\u672c\u4f18\u5316\u5ea6\u91cf\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u5143\u5b66\u4e60\u6846\u67b6\u907f\u514d\u8fc7\u62df\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5ea6\u91cf\u6a21\u578b\u5728\u63a8\u7406\u65f6\u672a\u5145\u5206\u5229\u7528\u652f\u6301\u6837\u672c\uff0c\u4ec5\u7528\u4e8e\u76f8\u4f3c\u6027\u6bd4\u8f83\uff0c\u5ffd\u7565\u4e86\u4f18\u5316\u5ea6\u91cf\u7a7a\u95f4\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86RDFT\u53ca\u5176\u53d8\u4f53IDFT\u548cADFT\uff0c\u901a\u8fc7\u6784\u5efa\u4f2a\u652f\u6301-\u67e5\u8be2\u5bf9\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408\u4f18\u5316\u5143\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u97f3\u9891\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u652f\u6301\u6837\u672c\uff0c\u901a\u8fc7\u5fae\u8c03\u548c\u5143\u5b66\u4e60\u63d0\u9ad8\u4e86\u5c0f\u6837\u672c\u5206\u7c7b\u6027\u80fd\u5e76\u907f\u514d\u8fc7\u62df\u5408\u3002"}}
{"id": "2506.18541", "pdf": "https://arxiv.org/pdf/2506.18541", "abs": "https://arxiv.org/abs/2506.18541", "authors": ["\u00c9l\u00e9anore Meyer", "J\u00fcrgen Giesl"], "title": "Deciding Termination of Simple Randomized Loops", "categories": ["cs.LO"], "comment": null, "summary": "We show that universal positive almost sure termination (UPAST) is decidable\nfor a class of simple randomized programs, i.e., it is decidable whether the\nexpected runtime of such a program is finite for all inputs. Our class contains\nall programs that consist of a single loop, with a linear loop guard and a loop\nbody composed of two linear commuting and diagonalizable updates. In each\niteration of the loop, the update to be carried out is picked at random,\naccording to a fixed probability. We show the decidability of UPAST for this\nclass of programs, where the program's variables and inputs may range over\nvarious sub-semirings of the real numbers. In this way, we extend a line of\nresearch initiated by Tiwari in 2004 into the realm of randomized programs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4e00\u7c7b\u7b80\u5355\u968f\u673a\u5316\u7a0b\u5e8f\u7684\u901a\u7528\u6b63\u51e0\u4e4e\u5fc5\u7136\u7ec8\u6b62\u6027\uff08UPAST\uff09\u662f\u53ef\u5224\u5b9a\u7684\uff0c\u5373\u53ef\u4ee5\u5224\u5b9a\u8fd9\u4e9b\u7a0b\u5e8f\u5728\u6240\u6709\u8f93\u5165\u4e0b\u7684\u671f\u671b\u8fd0\u884c\u65f6\u662f\u5426\u6709\u9650\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u5316\u7a0b\u5e8f\u7684UPAST\u95ee\u9898\u662f\u4e3a\u4e86\u6269\u5c55\u5bf9\u7a0b\u5e8f\u7ec8\u6b62\u6027\u7684\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728\u8f93\u5165\u8303\u56f4\u5e7f\u6cdb\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u7814\u7a76\u5305\u542b\u5355\u5faa\u73af\u3001\u7ebf\u6027\u5faa\u73af\u6761\u4ef6\u548c\u7531\u4e24\u4e2a\u7ebf\u6027\u53ef\u4ea4\u6362\u4e14\u53ef\u5bf9\u89d2\u5316\u66f4\u65b0\u7ec4\u6210\u7684\u5faa\u73af\u4f53\u7684\u7a0b\u5e8f\uff0c\u5176\u4e2d\u6bcf\u6b21\u8fed\u4ee3\u968f\u673a\u9009\u62e9\u66f4\u65b0\u64cd\u4f5c\u3002", "result": "\u8bc1\u660e\u4e86\u8fd9\u7c7b\u7a0b\u5e8f\u7684UPAST\u662f\u53ef\u5224\u5b9a\u7684\uff0c\u4e14\u9002\u7528\u4e8e\u53d8\u91cf\u7684\u591a\u79cd\u5b50\u534a\u73af\u8303\u56f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86Tiwari\u57282004\u5e74\u63d0\u51fa\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5c06\u7ed3\u679c\u63a8\u5e7f\u5230\u968f\u673a\u5316\u7a0b\u5e8f\u4e2d\u3002"}}
{"id": "2506.17678", "pdf": "https://arxiv.org/pdf/2506.17678", "abs": "https://arxiv.org/abs/2506.17678", "authors": ["Mehmet Kaan Erol", "Eyup Emre Ulku"], "title": "Location Information Sharing Using Software Defined Radio in Multi-UAV Systems", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": "10 pages, 6 figure", "summary": "SDR (Software Defined Radio) provides flexible, reproducible, and\nlonger-lasting radio tools for military and civilian wireless communications\ninfrastructure. SDR is a radio communication system whose components are\nimplemented as software. This study aims to establish multi-channel wireless\ncommunication with FANET between two SDRs to share location information and\nexamine it in a realistic test environment. We used multi-channel token\ncirculation as a channel access protocol and GNU Radio platform for SDR\nsoftware development. The structures of the communication layer, including the\nprotocols, communication systems, and network structures suggested in the\nstudies in the literature, are generally tested in the simulation environment.\nThe simulation environment provides researchers with fast and easy development\nand testing, but disadvantages exist. These cause a product to be isolated from\nhardware, software, and cost effects encountered while developing and\nenvironmental factors affecting the communication channel while testing.\nAnother contribution of the study is to present the developed block diagrams\nand codes as clear and reproducible. The developed software and block diagrams\nare available at github.com/knrl/uav-in-802.11-gnuradio.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5728SDR\uff08\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535\uff09\u4e0a\u5efa\u7acb\u591a\u901a\u9053\u65e0\u7ebf\u901a\u4fe1\u7f51\u7edc\uff08FANET\uff09\uff0c\u5171\u4eab\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5e76\u5728\u5b9e\u9645\u6d4b\u8bd5\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u901a\u4fe1\u5c42\u7ed3\u6784\u901a\u5e38\u5728\u4eff\u771f\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u672a\u80fd\u5145\u5206\u53cd\u6620\u5b9e\u9645\u786c\u4ef6\u3001\u8f6f\u4ef6\u53ca\u73af\u5883\u56e0\u7d20\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u591a\u901a\u9053\u4ee4\u724c\u5faa\u73af\u4f5c\u4e3a\u4fe1\u9053\u63a5\u5165\u534f\u8bae\uff0c\u5e76\u4f7f\u7528GNU Radio\u5e73\u53f0\u8fdb\u884cSDR\u8f6f\u4ef6\u5f00\u53d1\u3002", "result": "\u5f00\u53d1\u4e86\u6e05\u6670\u7684\u3001\u53ef\u590d\u73b0\u7684\u5757\u56fe\u548c\u4ee3\u7801\uff0c\u5e76\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u901a\u4fe1\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u5b9e\u9645\u6d4b\u8bd5\u8865\u5145\u4e86\u4eff\u771f\u73af\u5883\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u5b9e\u73b0\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.18017", "pdf": "https://arxiv.org/pdf/2506.18017", "abs": "https://arxiv.org/abs/2506.18017", "authors": ["Yang Li", "Victor Cheung", "Xinhai Liu", "Yuguang Chen", "Zhongjin Luo", "Biwen Lei", "Haohan Weng", "Zibo Zhao", "Jingwei Huang", "Zhuo Chen", "Chunchao Guo"], "title": "Auto-Regressive Surface Cutting", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Tech. report. https://victorcheung12.github.io/seamgpt", "summary": "Surface cutting is a fundamental task in computer graphics, with applications\nin UV parameterization, texture mapping, and mesh decomposition. However,\nexisting methods often produce technically valid but overly fragmented atlases\nthat lack semantic coherence. We introduce SeamGPT, an auto-regressive model\nthat generates cutting seams by mimicking professional workflows. Our key\ntechnical innovation lies in formulating surface cutting as a next token\nprediction task: sample point clouds on mesh vertices and edges, encode them as\nshape conditions, and employ a GPT-style transformer to sequentially predict\nseam segments with quantized 3D coordinates. Our approach achieves exceptional\nperformance on UV unwrapping benchmarks containing both manifold and\nnon-manifold meshes, including artist-created, and 3D-scanned models. In\naddition, it enhances existing 3D segmentation tools by providing clean\nboundaries for part decomposition.", "AI": {"tldr": "SeamGPT\u662f\u4e00\u79cd\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u751f\u6210\u5207\u5272\u7ebf\uff0c\u5c06\u8868\u9762\u5207\u5272\u4efb\u52a1\u8f6c\u5316\u4e3atoken\u9884\u6d4b\u95ee\u9898\uff0c\u5728UV\u5c55\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u8868\u9762\u5207\u5272\u65b9\u6cd5\u5e38\u4ea7\u751f\u6280\u672f\u6709\u6548\u4f46\u8bed\u4e49\u4e0d\u8fde\u8d2f\u7684\u5206\u5272\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u8868\u9762\u5207\u5272\u4f5c\u4e3atoken\u9884\u6d4b\u4efb\u52a1\uff0c\u5229\u7528GPT\u98ce\u683c\u8f6c\u6362\u5668\u9884\u6d4b\u91cf\u53163D\u5750\u6807\u7684\u5207\u5272\u7ebf\u6bb5\u3002", "result": "SeamGPT\u5728\u591a\u79cdUV\u5c55\u5f00\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u6d41\u5f62\u548c\u975e\u6d41\u5f62\u7f51\u683c\uff09\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5e76\u80fd\u6539\u55843D\u5206\u5272\u5de5\u5177\u7684\u8fb9\u754c\u6548\u679c\u3002", "conclusion": "SeamGPT\u4e3a\u8868\u9762\u5207\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.18062", "pdf": "https://arxiv.org/pdf/2506.18062", "abs": "https://arxiv.org/abs/2506.18062", "authors": ["Samirasadat Jamalidinan", "Kazem Cheshmi"], "title": "Floating-Point Data Transformation for Lossless Compression", "categories": ["cs.DB", "cs.DC"], "comment": null, "summary": "Floating-point data is widely used across various domains. Depending on the\nrequired precision, each floating-point value can occupy several bytes.\nLossless storage of this information is crucial due to its critical accuracy,\nas seen in applications such as medical imaging and language model weights. In\nthese cases, data size is often significant, making lossless compression\nessential. Previous approaches either treat this data as raw byte streams for\ncompression or fail to leverage all patterns within the dataset. However,\nbecause multiple bytes represent a single value and due to inherent patterns in\nfloating-point representations, some of these bytes are correlated. To leverage\nthis property, we propose a novel data transformation method called Typed Data\nTransformation (\\DTT{}) that groups related bytes together to improve\ncompression. We implemented and tested our approach on various datasets across\nboth CPU and GPU. \\DTT{} achieves a geometric mean compression ratio\nimprovement of 1.16$\\times$ over state-of-the-art compression tools such as\nzstd, while also improving both compression and decompression throughput by\n1.18--3.79$\\times$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u8f6c\u6362\u65b9\u6cd5DTT\uff0c\u901a\u8fc7\u5c06\u76f8\u5173\u5b57\u8282\u5206\u7ec4\u6765\u63d0\u9ad8\u6d6e\u70b9\u6570\u636e\u7684\u538b\u7f29\u6548\u7387\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "motivation": "\u6d6e\u70b9\u6570\u636e\u5b58\u50a8\u9700\u6c42\u5927\u4e14\u7cbe\u5ea6\u5173\u952e\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u5176\u5185\u90e8\u6a21\u5f0f\uff0c\u4e9f\u9700\u9ad8\u6548\u538b\u7f29\u65b9\u6cd5\u3002", "method": "DTT\u65b9\u6cd5\u5c06\u76f8\u5173\u5b57\u8282\u5206\u7ec4\uff0c\u63d0\u5347\u538b\u7f29\u6548\u7387\uff0c\u5e76\u5728CPU\u548cGPU\u4e0a\u5b9e\u73b0\u6d4b\u8bd5\u3002", "result": "DTT\u6bd4zstd\u7b49\u5de5\u5177\u538b\u7f29\u6bd4\u63d0\u53471.16\u500d\uff0c\u538b\u7f29\u548c\u89e3\u538b\u901f\u5ea6\u63d0\u53471.18-3.79\u500d\u3002", "conclusion": "DTT\u901a\u8fc7\u5229\u7528\u6d6e\u70b9\u6570\u636e\u5185\u90e8\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6548\u7387\u548c\u901f\u5ea6\u3002"}}
{"id": "2506.18495", "pdf": "https://arxiv.org/pdf/2506.18495", "abs": "https://arxiv.org/abs/2506.18495", "authors": ["Aniss Bessalah", "Hatem Mohamed Abdelmoumen", "Karima Benatchba", "Hadjer Benmeziane"], "title": "AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm\nfor accelerating Deep Neural Networks (DNNs), offering significant energy and\nlatency benefits over conventional digital hardware. However, state-of-the-art\nneural networks are not inherently designed for AIMC, as they fail to account\nfor its unique non-idealities. Neural Architecture Search (NAS) is thus needed\nto systematically discover neural architectures optimized explicitly for AIMC\nconstraints. However, comparing NAS methodologies and extracting insights about\nrobust architectures for AIMC requires a dedicated NAS benchmark that\nexplicitly accounts for AIMC-specific hardware non-idealities. To address this,\nwe introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for\nAIMC. Our study reveals three key insights: (1) standard quantization\ntechniques fail to capture AIMC-specific noises, (2) robust architectures tend\nto feature wider and branched blocks, (3) skip connections improve resilience\nto temporal drift noise. These insights highlight the limitations of current\nNAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the\nimplementations used in this paper can be found at\nhttps://github.com/IBM/analog-nas/tree/main/analognasbench.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\uff08AIMC\uff09\u7684NAS\u57fa\u51c6\u6d4b\u8bd5AnalogNAS-Bench\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u91cf\u5316\u6280\u672f\u548c\u67b6\u6784\u8bbe\u8ba1\u5728AIMC\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u672a\u9488\u5bf9AIMC\u7684\u72ec\u7279\u975e\u7406\u60f3\u7279\u6027\u8bbe\u8ba1\uff0c\u9700\u901a\u8fc7NAS\u4f18\u5316\u67b6\u6784\u4ee5\u9002\u914dAIMC\u7ea6\u675f\u3002", "method": "\u5f00\u53d1\u4e86\u4e13\u7528NAS\u57fa\u51c6\u6d4b\u8bd5AnalogNAS-Bench\uff0c\u5206\u6790AIMC\u7279\u5b9a\u566a\u58f0\u4e0b\u7684\u67b6\u6784\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u6807\u51c6\u91cf\u5316\u6280\u672f\u65e0\u6cd5\u6355\u6349AIMC\u566a\u58f0\uff0c\u7a33\u5065\u67b6\u6784\u9700\u66f4\u5bbd\u3001\u5206\u652f\u5316\u6a21\u5757\u548c\u8df3\u8dc3\u8fde\u63a5\u3002", "conclusion": "AnalogNAS-Bench\u4e3a\u672a\u6765AIMC\u4f18\u5316\u7684NAS\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u65b9\u5411\u3002"}}
{"id": "2506.17580", "pdf": "https://arxiv.org/pdf/2506.17580", "abs": "https://arxiv.org/abs/2506.17580", "authors": ["Sajratul Y. Rubaiat", "Hasan M. Jamil"], "title": "Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models", "categories": ["cs.IR", "cs.AI", "cs.DL", "cs.ET"], "comment": null, "summary": "The exponential growth of scientific literature challenges researchers\nextracting and synthesizing knowledge. Traditional search engines return many\nsources without direct, detailed answers, while general-purpose LLMs may offer\nconcise responses that lack depth or omit current information. LLMs with search\ncapabilities are also limited by context window, yielding short, incomplete\nanswers. This paper introduces WISE (Workflow for Intelligent Scientific\nKnowledge Extraction), a system addressing these limits by using a structured\nworkflow to extract, refine, and rank query-specific knowledge. WISE uses an\nLLM-powered, tree-based architecture to refine data, focusing on query-aligned,\ncontext-aware, and non-redundant information. Dynamic scoring and ranking\nprioritize unique contributions from each source, and adaptive stopping\ncriteria minimize processing overhead. WISE delivers detailed, organized\nanswers by systematically exploring and synthesizing knowledge from diverse\nsources. Experiments on HBB gene-associated diseases demonstrate WISE reduces\nprocessed text by over 80% while achieving significantly higher recall over\nbaselines like search engines and other LLM-based approaches. ROUGE and BLEU\nmetrics reveal WISE's output is more unique than other systems, and a novel\nlevel-based metric shows it provides more in-depth information. We also explore\nhow the WISE workflow can be adapted for diverse domains like drug discovery,\nmaterial science, and social science, enabling efficient knowledge extraction\nand synthesis from unstructured scientific papers and web sources.", "AI": {"tldr": "WISE\u7cfb\u7edf\u901a\u8fc7\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u89e3\u51b3\u79d1\u5b66\u6587\u732e\u77e5\u8bc6\u63d0\u53d6\u96be\u9898\uff0c\u663e\u8457\u51cf\u5c11\u6587\u672c\u5904\u7406\u91cf\u5e76\u63d0\u9ad8\u53ec\u56de\u7387\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63d0\u4f9b\u8be6\u7ec6\u3001\u6df1\u5ea6\u548c\u6700\u65b0\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLLM\u7684\u6811\u5f62\u67b6\u6784\uff0c\u52a8\u6001\u8bc4\u5206\u548c\u6392\u540d\uff0c\u81ea\u9002\u5e94\u505c\u6b62\u6807\u51c6\uff0c\u63d0\u53d6\u548c\u7ec4\u7ec7\u67e5\u8be2\u76f8\u5173\u77e5\u8bc6\u3002", "result": "\u5728HBB\u57fa\u56e0\u76f8\u5173\u75be\u75c5\u5b9e\u9a8c\u4e2d\uff0cWISE\u51cf\u5c1180%\u6587\u672c\u5904\u7406\u91cf\uff0c\u53ec\u56de\u7387\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "WISE\u53ef\u6269\u5c55\u5230\u836f\u7269\u53d1\u73b0\u3001\u6750\u6599\u79d1\u5b66\u7b49\u9886\u57df\uff0c\u9ad8\u6548\u63d0\u53d6\u548c\u5408\u6210\u975e\u7ed3\u6784\u5316\u79d1\u5b66\u6587\u732e\u77e5\u8bc6\u3002"}}
{"id": "2506.17551", "pdf": "https://arxiv.org/pdf/2506.17551", "abs": "https://arxiv.org/abs/2506.17551", "authors": ["Haowei Yang", "Yu Tian", "Zhongheng Yang", "Zhao Wang", "Chengrui Zhou", "Dannier Li"], "title": "Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "With the rapid adoption of large language models (LLMs) in recommendation\nsystems, the computational and communication bottlenecks caused by their\nmassive parameter sizes and large data volumes have become increasingly\nprominent. This paper systematically investigates two classes of optimization\nmethods-model parallelism and data parallelism-for distributed training of LLMs\nin recommendation scenarios. For model parallelism, we implement both tensor\nparallelism and pipeline parallelism, and introduce an adaptive load-balancing\nmechanism to reduce cross-device communication overhead. For data parallelism,\nwe compare synchronous and asynchronous modes, combining gradient compression\nand sparsification techniques with an efficient aggregation communication\nframework to significantly improve bandwidth utilization. Experiments conducted\non a real-world recommendation dataset in a simulated service environment\ndemonstrate that our proposed hybrid parallelism scheme increases training\nthroughput by over 30% and improves resource utilization by approximately 20%\ncompared to traditional single-mode parallelism, while maintaining strong\nscalability and robustness. Finally, we discuss trade-offs among different\nparallel strategies in online deployment and outline future directions\ninvolving heterogeneous hardware integration and automated scheduling\ntechnologies.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u6a21\u578b\u5e76\u884c\u548c\u6570\u636e\u5e76\u884c\u4f18\u5316\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u63d0\u51fa\u6df7\u5408\u5e76\u884c\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u7531\u4e8eLLMs\u53c2\u6570\u91cf\u5de8\u5927\u548c\u6570\u636e\u91cf\u5e9e\u5927\uff0c\u5176\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u4f7f\u7528\u65f6\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u74f6\u9888\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u4f18\u5316\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u7814\u7a76\u4e86\u6a21\u578b\u5e76\u884c\uff08\u5305\u62ec\u5f20\u91cf\u5e76\u884c\u548c\u7ba1\u9053\u5e76\u884c\uff09\u548c\u6570\u636e\u5e76\u884c\uff08\u540c\u6b65\u548c\u5f02\u6b65\u6a21\u5f0f\uff09\uff0c\u5e76\u7ed3\u5408\u68af\u5ea6\u538b\u7f29\u548c\u7a00\u758f\u5316\u6280\u672f\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u548c\u9ad8\u6548\u805a\u5408\u901a\u4fe1\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6df7\u5408\u5e76\u884c\u65b9\u6848\u6bd4\u4f20\u7edf\u5355\u6a21\u5f0f\u5e76\u884c\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u534730%\u4ee5\u4e0a\uff0c\u8d44\u6e90\u5229\u7528\u7387\u63d0\u5347\u7ea620%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8bba\u6587\u5206\u6790\u4e86\u4e0d\u540c\u5e76\u884c\u7b56\u7565\u7684\u6743\u8861\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u5728\u5f02\u6784\u786c\u4ef6\u96c6\u6210\u548c\u81ea\u52a8\u5316\u8c03\u5ea6\u6280\u672f\u65b9\u9762\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.17642", "pdf": "https://arxiv.org/pdf/2506.17642", "abs": "https://arxiv.org/abs/2506.17642", "authors": ["Shaoyu Yang", "Chunrong Fang", "Haifeng Lin", "Xiang Chen", "Zhenyu Chen"], "title": "May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs", "categories": ["cs.SE"], "comment": null, "summary": "Artificial Intelligence (AI) Infrastructures, represented by Deep Learning\n(DL) frameworks, have served as fundamental DL systems over the last decade.\nHowever, the bugs in DL frameworks could lead to catastrophic consequences in\nsome critical scenarios (e.g., healthcare and autonomous driving). A simple yet\neffective way to find bugs in DL frameworks is fuzz testing (Fuzzing).\nUnfortunately, existing fuzzing techniques have not comprehensively considered\nmultiple types of feedback. Additionally, they analyze feedback in a\ncoarse-grained manner, such as mutating the test cases only according to\nwhether the coverage increases. Recently, researchers introduced Large Language\nModels (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only\nfocus on using LLMs to generate test cases while overlooking their potential to\nanalyze feedback information, failing to create more valid and diverse test\ncases. To fill this gap, we propose FUEL to break the seal of Feedback-driven\nfuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,\nnamely analysis LLM and generation LLM. Analysis LLM agent infers analysis\nsummaries from feedback information, while the generation LLM agent creates\ntests guided by these analysis summaries. So far, FUEL has detected 104 bugs\nfor PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,\nand 5 assigned with CVE IDs. Our work indicates that considering multiple types\nof feedback is beneficial to fuzzing performance, and leveraging LLMs to\nanalyze feedback information is a promising direction. Our artifact is\navailable at https://github.com/NJU-iSE/FUEL", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFUEL\u7684\u53cd\u9988\u9a71\u52a8\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u7f3a\u9677\u68c0\u6d4b\uff0c\u5229\u7528\u4e24\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\u5206\u6790\u53cd\u9988\u4fe1\u606f\u5e76\u751f\u6210\u591a\u6837\u5316\u6d4b\u8bd5\u7528\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u6548\u679c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u7f3a\u9677\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff08\u5982\u533b\u7597\u548c\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff09\uff0c\u800c\u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u6280\u672f\u672a\u80fd\u5168\u9762\u5229\u7528\u591a\u7c7b\u53cd\u9988\u4fe1\u606f\uff0c\u4e14\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u4ec5\u7528\u4e8e\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5ffd\u89c6\u4e86\u5176\u5206\u6790\u53cd\u9988\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faFUEL\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2aLLM\u4ee3\u7406\uff1a\u5206\u6790LLM\u4ece\u53cd\u9988\u4fe1\u606f\u4e2d\u63d0\u53d6\u5206\u6790\u6458\u8981\uff0c\u751f\u6210LLM\u6839\u636e\u6458\u8981\u6307\u5bfc\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u3002", "result": "FUEL\u5728PyTorch\u548cTensorFlow\u4e2d\u53d1\u73b0104\u4e2a\u7f3a\u9677\uff0c\u5176\u4e2d93\u4e2a\u4e3a\u65b0\u7f3a\u9677\uff0c47\u4e2a\u5df2\u4fee\u590d\uff0c5\u4e2a\u83b7\u5f97CVE\u7f16\u53f7\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7efc\u5408\u8003\u8651\u591a\u7c7b\u53cd\u9988\u4fe1\u606f\u80fd\u63d0\u5347\u6a21\u7cca\u6d4b\u8bd5\u6027\u80fd\uff0c\u5e76\u8bc1\u660eLLM\u5206\u6790\u53cd\u9988\u662f\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2506.18143", "pdf": "https://arxiv.org/pdf/2506.18143", "abs": "https://arxiv.org/abs/2506.18143", "authors": ["Lancelot Blanchard", "Cameron Holt", "Joseph A. Paradiso"], "title": "AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "H.5.5"], "comment": "4 pages, 3 figures", "summary": "Vocals harmonizers are powerful tools to help solo vocalists enrich their\nmelodies with harmonically supportive voices. These tools exist in various\nforms, from commercially available pedals and software to custom-built systems,\neach employing different methods to generate harmonies. Traditional harmonizers\noften require users to manually specify a key or tonal center, while others\nallow pitch selection via an external keyboard-both approaches demanding some\ndegree of musical expertise. The AI Harmonizer introduces a novel approach by\nautonomously generating musically coherent four-part harmonies without\nrequiring prior harmonic input from the user. By integrating state-of-the-art\ngenerative AI techniques for pitch detection and voice modeling with\ncustom-trained symbolic music models, our system arranges any vocal melody into\nrich choral textures. In this paper, we present our methods, explore potential\napplications in performance and composition, and discuss future directions for\nreal-time implementations. While our system currently operates offline, we\nbelieve it represents a significant step toward AI-assisted vocal performance\nand expressive musical augmentation. We release our implementation on GitHub.", "AI": {"tldr": "AI Harmonizer \u662f\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u751f\u6210\u56db\u90e8\u548c\u58f0\u7684\u5de5\u5177\uff0c\u65e0\u9700\u7528\u6237\u8f93\u5165\u548c\u58f0\u4fe1\u606f\uff0c\u5229\u7528\u5148\u8fdb\u7684AI\u6280\u672f\u5b9e\u73b0\u3002", "motivation": "\u65e8\u5728\u4e3a\u72ec\u5531\u6b4c\u624b\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u97f3\u4e50\u4e13\u4e1a\u77e5\u8bc6\u5373\u53ef\u751f\u6210\u4e30\u5bcc\u548c\u58f0\u7684\u5de5\u5177\uff0c\u7b80\u5316\u4f20\u7edf\u548c\u58f0\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u751f\u6210\u5f0fAI\u6280\u672f\u548c\u7b26\u53f7\u97f3\u4e50\u6a21\u578b\uff0c\u901a\u8fc7\u97f3\u9ad8\u68c0\u6d4b\u548c\u58f0\u97f3\u5efa\u6a21\u81ea\u52a8\u751f\u6210\u548c\u58f0\u3002", "result": "\u7cfb\u7edf\u53ef\u79bb\u7ebf\u751f\u6210\u4e30\u5bcc\u7684\u5408\u5531\u7eb9\u7406\uff0c\u672a\u6765\u53ef\u80fd\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002", "conclusion": "AI Harmonizer \u4e3aAI\u8f85\u52a9\u97f3\u4e50\u8868\u6f14\u548c\u521b\u4f5c\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.17707", "pdf": "https://arxiv.org/pdf/2506.17707", "abs": "https://arxiv.org/abs/2506.17707", "authors": ["Jihyun Kim", "Junho Park", "Kyeongbo Kong", "Suk-Ju Kang"], "title": "Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "We present Programmable-Room, a framework which interactively generates and\nedits a 3D room mesh, given natural language instructions. For precise control\nof a room's each attribute, we decompose the challenging task into simpler\nsteps such as creating plausible 3D coordinates for room meshes, generating\npanorama images for the texture, constructing 3D meshes by integrating the\ncoordinates and panorama texture images, and arranging furniture. To support\nthe various decomposed tasks with a unified framework, we incorporate visual\nprogramming (VP). VP is a method that utilizes a large language model (LLM) to\nwrite a Python-like program which is an ordered list of necessary modules for\nthe various tasks given in natural language. We develop most of the modules.\nEspecially, for the texture generating module, we utilize a pretrained\nlarge-scale diffusion model to generate panorama images conditioned on text and\nvisual prompts (i.e., layout, depth, and semantic map) simultaneously.\nSpecifically, we enhance the panorama image generation quality by optimizing\nthe training objective with a 1D representation of a panorama scene obtained\nfrom bidirectional LSTM. We demonstrate Programmable-Room's flexibility in\ngenerating and editing 3D room meshes, and prove our framework's superiority to\nan existing model quantitatively and qualitatively. Project page is available\nin https://jihyun0510.github.io/Programmable_Room_Page/.", "AI": {"tldr": "Programmable-Room\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4ea4\u4e92\u5f0f\u751f\u6210\u548c\u7f16\u8f913D\u623f\u95f4\u7f51\u683c\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u7f16\u7a0b\u548c\u6269\u6563\u6a21\u578b\u4ee5\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b9e\u73b0\u5bf93D\u623f\u95f4\u7f51\u683c\u7684\u7cbe\u786e\u63a7\u5236\u548c\u7f16\u8f91\uff0c\u4ece\u800c\u7b80\u5316\u7528\u6237\u64cd\u4f5c\u3002", "method": "\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u6b65\u9aa4\uff08\u5982\u751f\u62103D\u5750\u6807\u3001\u5168\u666f\u56fe\u50cf\u3001\u6784\u9020\u7f51\u683c\u7b49\uff09\uff0c\u5e76\u5229\u7528\u89c6\u89c9\u7f16\u7a0b\uff08\u57fa\u4e8eLLM\u751f\u6210Python\u7c7b\u7a0b\u5e8f\uff09\u548c\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u6846\u67b6\u5728\u751f\u6210\u548c\u7f16\u8f913D\u623f\u95f4\u7f51\u683c\u65b9\u9762\u8868\u73b0\u51fa\u7075\u6d3b\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "Programmable-Room\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u5148\u8fdb\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u76843D\u623f\u95f4\u751f\u6210\u4e0e\u7f16\u8f91\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17460", "pdf": "https://arxiv.org/pdf/2506.17460", "abs": "https://arxiv.org/abs/2506.17460", "authors": ["Val\u00e9rie Berth\u00e9", "Toghrul Karimov", "Mihir Vahanwala"], "title": "Automata on $S$-adic words", "categories": ["cs.FL", "cs.LO"], "comment": null, "summary": "A fundamental question in logic and verification is the following: for which\nunary predicates $P_1, \\ldots, P_k$ is the monadic second-order theory of\n$\\langle \\mathbb{N}; <, P_1, \\ldots, P_k \\rangle$ decidable? Equivalently, for\nwhich infinite words $\\alpha$ can we decide whether a given B\\\"uchi automaton\n$A$ accepts $\\alpha$? Carton and Thomas showed decidability in case $\\alpha$ is\na fixed point of a letter-to-word substitution $\\sigma$, i.e., $\\sigma(\\alpha)\n= \\alpha$. However, abundantly more words, e.g., Sturmian words, are\ncharacterised by a broader notion of self-similarity that uses a set $S$ of\nsubstitutions. A word $\\alpha$ is said to be directed by a sequence $s =\n(\\sigma_n)_{n \\in \\mathbb{N}}$ over $S$ if there is a sequence of words\n$(\\alpha_n)_{n \\in \\mathbb{N}}$ such that $\\alpha_0 = \\alpha$ and $\\alpha_n =\n\\sigma_n(\\alpha_{n+1})$ for all $n$; such $\\alpha$ is called $S$-adic. We study\nthe automaton acceptance problem for such words and prove, among others, the\nfollowing. Given finite $S$ and an automaton $A$, we can compute an automaton\n$B$ that accepts $s \\in S^\\omega$ if and only if $s$ directs a word $\\alpha$\naccepted by $A$. Thus we can algorithmically answer questions of the form\n\"Which $S$-adic words are accepted by a given automaton $A$?\"", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u5e7f\u4e49\u81ea\u76f8\u4f3c\u6027\uff08$S$-adic\uff09\u4e0b\u7684\u65e0\u9650\u5b57\u7684\u81ea\u52a8\u673a\u63a5\u53d7\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u53ef\u4ee5\u901a\u8fc7\u8ba1\u7b97\u81ea\u52a8\u673a$B$\u6765\u56de\u7b54\u54ea\u4e9b$S$-adic\u5b57\u88ab\u81ea\u52a8\u673a$A$\u63a5\u53d7\u3002", "motivation": "\u63a2\u7d22\u5728\u903b\u8f91\u548c\u9a8c\u8bc1\u4e2d\uff0c\u54ea\u4e9b\u4e00\u5143\u8c13\u8bcd\u7684\u4e8c\u9636\u7406\u8bba\u662f\u53ef\u5224\u5b9a\u7684\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5e7f\u4e49\u81ea\u76f8\u4f3c\u6027\u7684\u65e0\u9650\u5b57\u3002", "method": "\u7814\u7a76\u7531\u4e00\u7ec4\u66ff\u6362$S$\u751f\u6210\u7684$S$-adic\u5b57\uff0c\u5e76\u6784\u5efa\u81ea\u52a8\u673a$B$\u6765\u5224\u65ad$A$\u662f\u5426\u63a5\u53d7\u8fd9\u4e9b\u5b57\u3002", "result": "\u8bc1\u660e\u4e86\u53ef\u4ee5\u901a\u8fc7\u8ba1\u7b97\u81ea\u52a8\u673a$B$\u6765\u56de\u7b54\u54ea\u4e9b$S$-adic\u5b57\u88ab\u81ea\u52a8\u673a$A$\u63a5\u53d7\u3002", "conclusion": "\u5728\u6709\u9650\u66ff\u6362\u96c6$S$\u4e0b\uff0c\u53ef\u4ee5\u7b97\u6cd5\u5316\u89e3\u51b3$S$-adic\u5b57\u7684\u81ea\u52a8\u673a\u63a5\u53d7\u95ee\u9898\u3002"}}
{"id": "2506.17821", "pdf": "https://arxiv.org/pdf/2506.17821", "abs": "https://arxiv.org/abs/2506.17821", "authors": ["Samuel Oluwafemi Adebayo"], "title": "The Blind Spot of BGP Anomaly Detection: Why LSTM Autoencoders Fail on Real-World Outages", "categories": ["cs.NI"], "comment": null, "summary": "Deep learning has significant potential to make the Internet's Border Gateway\nProtocol (BGP) secure by detecting anomalous routing activity. However, all but\na few of these approaches rely on the implicit assumption that anomalies\nmanifest as noisy, high-complexity outliers from some normal baseline. This\nwork challenges this assumption by investigating if a best-in-class detection\nmodel built on this assumption can effectively deal with real-world security\nevents' diverse signatures. We employ an LSTM-based autoencoder, a classical\nexample of a reconstruction-based anomaly detector, as our test vehicle. We\nthen contrast this model with a representative sampling of historical BGP\nanomalies, including the Slammer worm and the Moscow blackout, and with a\nsimulated 'BGP storm' designed as a positive control. Our experience unveils a\nblind spot of our model: the model easily identifies the synthetic anomaly of\nhigh complexity but invariably fails to identify real-world events that\nmanifest in the form of a \"signal loss\" (e.g., Slammer, Moscow Blackout) or\n\"low-deviation\" (e.g., WannaCry) signature. We demonstrate that the model\nmistakenly recognizes the abrupt cut-off of BGP updates during catastrophic\nfailures as a signal of extreme stability, leading to reconstruction errors of\nvirtually zero and total failure to detect. We conclude that the\ncharacterization of BGP anomalies as high-reconstruction-error events alone is\na weak and dangerous oversimplification. Our research provides the data-driven\ncase for why hybrid, multi-modal detection systems capable of identifying both\nhigh-complexity and signal-loss signatures are required to enable end-to-end\nBGP security.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u5728\u68c0\u6d4bBGP\u5f02\u5e38\u65f6\u5b58\u5728\u76f2\u70b9\uff0c\u4ec5\u4f9d\u8d56\u9ad8\u590d\u6742\u5ea6\u5f02\u5e38\u5047\u8bbe\u4e0d\u5145\u5206\u3002", "motivation": "\u63a2\u8ba8\u73b0\u6709BGP\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u6709\u6548\u5e94\u5bf9\u591a\u6837\u5316\u771f\u5b9e\u4e8b\u4ef6\uff0c\u5982\u4fe1\u53f7\u4e22\u5931\u6216\u4f4e\u504f\u5dee\u7b7e\u540d\u3002", "method": "\u4f7f\u7528LSTM\u81ea\u7f16\u7801\u5668\u5bf9\u6bd4\u771f\u5b9e\u5386\u53f2\u4e8b\u4ef6\uff08\u5982Slammer\u8815\u866b\uff09\u548c\u6a21\u62dfBGP\u98ce\u66b4\u3002", "result": "\u6a21\u578b\u8bc6\u522b\u9ad8\u590d\u6742\u5ea6\u5f02\u5e38\u4f46\u5ffd\u7565\u4fe1\u53f7\u4e22\u5931\u6216\u4f4e\u504f\u5dee\u4e8b\u4ef6\uff0c\u8bef\u5c06\u707e\u96be\u6027\u6545\u969c\u89c6\u4e3a\u7a33\u5b9a\u4fe1\u53f7\u3002", "conclusion": "\u9700\u6df7\u5408\u591a\u6a21\u6001\u68c0\u6d4b\u7cfb\u7edf\u4ee5\u5168\u9762\u8986\u76d6BGP\u5f02\u5e38\u7c7b\u578b\uff0c\u786e\u4fdd\u5b89\u5168\u6027\u3002"}}
{"id": "2506.18251", "pdf": "https://arxiv.org/pdf/2506.18251", "abs": "https://arxiv.org/abs/2506.18251", "authors": ["Chao Li", "Jiawei Fan", "Anbang Yao"], "title": "Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "This work is accepted to ICML 2025. The project page:\n  https://github.com/deep-optimization/Morse", "summary": "In this paper, we present Morse, a simple dual-sampling framework for\naccelerating diffusion models losslessly. The key insight of Morse is to\nreformulate the iterative generation (from noise to data) process via taking\nadvantage of fast jump sampling and adaptive residual feedback strategies.\nSpecifically, Morse involves two models called Dash and Dot that interact with\neach other. The Dash model is just the pre-trained diffusion model of any type,\nbut operates in a jump sampling regime, creating sufficient space for sampling\nefficiency improvement. The Dot model is significantly faster than the Dash\nmodel, which is learnt to generate residual feedback conditioned on the\nobservations at the current jump sampling point on the trajectory of the Dash\nmodel, lifting the noise estimate to easily match the next-step estimate of the\nDash model without jump sampling. By chaining the outputs of the Dash and Dot\nmodels run in a time-interleaved fashion, Morse exhibits the merit of flexibly\nattaining desired image generation performance while improving overall runtime\nefficiency. With our proposed weight sharing strategy between the Dash and Dot\nmodels, Morse is efficient for training and inference. Our method shows a\nlossless speedup of 1.78X to 3.31X on average over a wide range of sampling\nstep budgets relative to 9 baseline diffusion models on 6 image generation\ntasks. Furthermore, we show that our method can be also generalized to improve\nthe Latent Consistency Model (LCM-SDXL, which is already accelerated with\nconsistency distillation technique) tailored for few-step text-to-image\nsynthesis. The code and models are available at\nhttps://github.com/deep-optimization/Morse.", "AI": {"tldr": "Morse\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u53cc\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u5feb\u901f\u8df3\u8dc3\u91c7\u6837\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u53cd\u9988\u7b56\u7565\uff0c\u65e0\u635f\u52a0\u901f\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u8fed\u4ee3\u8017\u65f6\uff0cMorse\u65e8\u5728\u901a\u8fc7\u53cc\u91c7\u6837\u7b56\u7565\u63d0\u9ad8\u6548\u7387\u800c\u4e0d\u635f\u5931\u6027\u80fd\u3002", "method": "Morse\u5305\u542bDash\u548cDot\u4e24\u4e2a\u6a21\u578b\uff1aDash\u662f\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u8df3\u8dc3\u91c7\u6837\uff1bDot\u5feb\u901f\u751f\u6210\u6b8b\u5dee\u53cd\u9988\uff0c\u63d0\u5347\u566a\u58f0\u4f30\u8ba1\u3002\u901a\u8fc7\u65f6\u95f4\u4ea4\u9519\u8f93\u51fa\u548c\u6743\u91cd\u5171\u4eab\u7b56\u7565\u4f18\u5316\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMorse\u57286\u9879\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u5e73\u5747\u65e0\u635f\u52a0\u901f1.78X\u81f33.31X\uff0c\u5e76\u53ef\u63a8\u5e7f\u5230\u57fa\u4e8e\u4e00\u81f4\u6027\u84b8\u998f\u7684\u6a21\u578b\uff08\u5982LCM-SDXL\uff09\u3002", "conclusion": "Morse\u901a\u8fc7\u53cc\u91c7\u6837\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u65e0\u635f\u3002"}}
{"id": "2506.18252", "pdf": "https://arxiv.org/pdf/2506.18252", "abs": "https://arxiv.org/abs/2506.18252", "authors": ["Jinjin Zhao"], "title": "Learning Lineage Constraints for Data Science Operations", "categories": ["cs.DB"], "comment": null, "summary": "Data science workflows often integrate functionalities from a diverse set of\nlibraries and frameworks. Tasks such as debugging require data lineage that\ncrosses library boundaries. The problem is that the way that \"lineage\" is\nrepresented is often intimately tied to particular data models and data\nmanipulation paradigms. Inspired by the use of intermediate representations\n(IRs) in cross-library performance optimizations, this vision paper proposes a\nsimilar architecture for lineage - how do we specify logical lineage across\nlibraries in a common parameterized way? In practice, cross-library workflows\nwill contain both known operations and unknown operations, so a key design of\nXProv to link both materialized lineage graphs of data transformations and the\naforementioned abstracted logical patterns. We further discuss early ideas on\nhow to infer logical patterns when only the materialized graphs are available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u5e93\u6570\u636e\u8840\u7f18\u7684\u7edf\u4e00\u8868\u793a\u65b9\u6cd5XProv\uff0c\u7ed3\u5408\u5177\u4f53\u8f6c\u6362\u56fe\u548c\u62bd\u8c61\u903b\u8f91\u6a21\u5f0f\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u5de5\u4f5c\u6d41\u4e2d\u8de8\u5e93\u8c03\u8bd5\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u5e38\u6d89\u53ca\u591a\u79cd\u5e93\uff0c\u800c\u8c03\u8bd5\u9700\u8981\u8de8\u5e93\u8840\u7f18\uff0c\u5f53\u524d\u8840\u7f18\u8868\u793a\u4e0e\u7279\u5b9a\u6570\u636e\u6a21\u578b\u7d27\u5bc6\u8026\u5408\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002", "method": "\u501f\u9274\u8de8\u5e93\u6027\u80fd\u4f18\u5316\u7684\u4e2d\u95f4\u8868\u793a\u601d\u60f3\uff0c\u8bbe\u8ba1XProv\u67b6\u6784\uff0c\u7edf\u4e00\u53c2\u6570\u5316\u8868\u793a\u903b\u8f91\u8840\u7f18\uff0c\u5e76\u94fe\u63a5\u5177\u4f53\u8f6c\u6362\u56fe\u548c\u62bd\u8c61\u903b\u8f91\u6a21\u5f0f\u3002", "result": "\u63d0\u51faXProv\u6846\u67b6\uff0c\u652f\u6301\u5df2\u77e5\u548c\u672a\u77e5\u64cd\u4f5c\u7684\u94fe\u8def\uff0c\u63a2\u8ba8\u5982\u4f55\u4ece\u5177\u4f53\u56fe\u63a8\u6d4b\u903b\u8f91\u6a21\u5f0f\u3002", "conclusion": "XProv\u4e3a\u8de8\u5e93\u6570\u636e\u8840\u7f18\u63d0\u4f9b\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63d0\u5347\u8c03\u8bd5\u6548\u7387\u3002"}}
{"id": "2506.18780", "pdf": "https://arxiv.org/pdf/2506.18780", "abs": "https://arxiv.org/abs/2506.18780", "authors": ["Shuangbao Paul Wang"], "title": "Design high-confidence computers using trusted instructional set architecture and emulators", "categories": ["cs.CR", "cs.AR"], "comment": null, "summary": "High-confidence computing relies on trusted instructional set architecture,\nsealed kernels, and secure operating systems. Cloud computing depends on\ntrusted systems for virtualization tasks. Branch predictions and pipelines are\nessential in improving performance of a CPU/GPU. But Spectre and Meltdown make\nmodern processors vulnerable to be exploited. Disabling the prediction and\npipeline is definitely not a good solution. On the other hand, current software\npatches can only address non-essential issues around Meltdown. This paper\nintroduces a holistic approach in trusted computer architecture design and\nemulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u53ef\u4fe1\u8ba1\u7b97\u673a\u67b6\u6784\u8bbe\u8ba1\u4e0e\u4eff\u771f\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u4ee3\u5904\u7406\u5668\u56e0Spectre\u548cMeltdown\u6f0f\u6d1e\u5bfc\u81f4\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u5904\u7406\u5668\u56e0\u5206\u652f\u9884\u6d4b\u548c\u6d41\u6c34\u7ebf\u6280\u672f\u5b58\u5728Spectre\u548cMeltdown\u6f0f\u6d1e\uff0c\u73b0\u6709\u8f6f\u4ef6\u8865\u4e01\u65e0\u6cd5\u6839\u672c\u89e3\u51b3\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u53ef\u4fe1\u67b6\u6784\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u5168\u9762\u7684\u8ba1\u7b97\u673a\u67b6\u6784\u8bbe\u8ba1\u4e0e\u4eff\u771f\u65b9\u6cd5\uff0c\u7ed3\u5408\u53ef\u4fe1\u6307\u4ee4\u96c6\u67b6\u6784\u548c\u5bc6\u5c01\u5185\u6838\u7b49\u6280\u672f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u5904\u7406\u5668\u5b89\u5168\u6f0f\u6d1e\u7684\u65b0\u65b9\u6848\uff0c\u907f\u514d\u4e86\u7981\u7528\u5206\u652f\u9884\u6d4b\u548c\u6d41\u6c34\u7ebf\u7684\u6027\u80fd\u635f\u5931\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u6027\u80fd\u4e14\u5b89\u5168\u7684\u8ba1\u7b97\u673a\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2506.17672", "pdf": "https://arxiv.org/pdf/2506.17672", "abs": "https://arxiv.org/abs/2506.17672", "authors": ["Weiming Mai", "Jie Gao", "Oded Cats"], "title": "Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks", "categories": ["cs.LG", "cs.ET"], "comment": null, "summary": "In ride-hailing systems, drivers decide whether to accept or reject ride\nrequests based on factors such as order characteristics, traffic conditions,\nand personal preferences. Accurately predicting these decisions is essential\nfor improving the efficiency and reliability of these systems. Traditional\nmodels, such as the Random Utility Maximization (RUM) approach, typically\npredict drivers' decisions by assuming linear correlations among attributes.\nHowever, these models often fall short because they fail to account for\nnon-linear interactions between attributes and do not cater to the unique,\npersonalized preferences of individual drivers. In this paper, we develop a\nmethod for learning personalized utility functions using hypernetwork and\nensemble learning. Hypernetworks dynamically generate weights for a linear\nutility function based on trip request data and driver profiles, capturing the\nnon-linear relationships. An ensemble of hypernetworks trained on different\ndata segments further improve model adaptability and generalization by\nintroducing controlled randomness, thereby reducing over-fitting. We validate\nthe performance of our ensemble hypernetworks model in terms of prediction\naccuracy and uncertainty estimation in a real-world dataset. The results\ndemonstrate that our approach not only accurately predicts each driver's\nutility but also effectively balances the needs for explainability and\nuncertainty quantification. Additionally, our model serves as a powerful tool\nfor revealing the personalized preferences of different drivers, clearly\nillustrating which attributes largely impact their rider acceptance decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u548c\u96c6\u6210\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u6548\u7528\u51fd\u6570\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u7f51\u7ea6\u8f66\u53f8\u673a\u7684\u63a5\u5355\u51b3\u7b56\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\u548c\u4e2a\u6027\u5316\u504f\u597d\u7684\u95ee\u9898\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u53f8\u673a\u7684\u63a5\u5355\u51b3\u7b56\u5bf9\u4e8e\u63d0\u9ad8\u7f51\u7ea6\u8f66\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7ebf\u6027\u6a21\u578b\u5728\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\u548c\u4e2a\u6027\u5316\u504f\u597d\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u8d85\u7f51\u7edc\u52a8\u6001\u751f\u6210\u7ebf\u6027\u6548\u7528\u51fd\u6570\u7684\u6743\u91cd\uff0c\u7ed3\u5408\u96c6\u6210\u5b66\u4e60\u4ece\u4e0d\u540c\u6570\u636e\u7247\u6bb5\u8bad\u7ec3\u591a\u4e2a\u8d85\u7f51\u7edc\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u51fa\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\uff0c\u540c\u65f6\u5e73\u8861\u4e86\u89e3\u91ca\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9700\u6c42\uff0c\u5e76\u80fd\u63ed\u793a\u53f8\u673a\u7684\u4e2a\u6027\u5316\u504f\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u8d85\u7f51\u7edc\u6a21\u578b\u5728\u9884\u6d4b\u53f8\u673a\u63a5\u5355\u51b3\u7b56\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u975e\u7ebf\u6027\u5173\u7cfb\u548c\u4e2a\u6027\u5316\u504f\u597d\uff0c\u4e3a\u7f51\u7ea6\u8f66\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2506.17721", "pdf": "https://arxiv.org/pdf/2506.17721", "abs": "https://arxiv.org/abs/2506.17721", "authors": ["Prabhat Kumar Chand", "Apurba Das", "Anisur Rahaman Molla"], "title": "Distributed Butterfly Analysis using Mobile Agents", "categories": ["cs.DC", "cs.MA"], "comment": null, "summary": "Butterflies, or 4-cycles in bipartite graphs, are crucial for identifying\ncohesive structures and dense subgraphs. While agent-based data mining is\ngaining prominence, its application to bipartite networks remains relatively\nunexplored. We propose distributed, agent-based algorithms for \\emph{Butterfly\nCounting} in a bipartite graph $G((A,B),E)$. Agents first determine their\nrespective partitions and collaboratively construct a spanning tree, electing a\nleader within $O(n \\log \\lambda)$ rounds using only $O(\\log \\lambda)$ bits per\nagent. A novel meeting mechanism between adjacent agents improves efficiency\nand eliminates the need for prior knowledge of the graph, requiring only the\nhighest agent ID $\\lambda$ among the $n$ agents. Notably, our techniques\nnaturally extend to general graphs, where leader election and spanning tree\nconstruction maintain the same round and memory complexities. Building on these\nfoundations, agents count butterflies per node in $O(\\Delta)$ rounds and\ncompute the total butterfly count of $G$ in $O(\\Delta+\\min\\{|A|,|B|\\})$ rounds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u3001\u57fa\u4e8e\u4ee3\u7406\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u4e8c\u5206\u56fe\u4e2d\u9ad8\u6548\u8ba1\u7b97\u8774\u8776\u7ed3\u6784\uff084-\u5faa\u73af\uff09\uff0c\u65e0\u9700\u9884\u5148\u4e86\u89e3\u56fe\u7684\u7ed3\u6784\u3002", "motivation": "\u4e8c\u5206\u56fe\u4e2d\u7684\u8774\u8776\u7ed3\u6784\u5bf9\u8bc6\u522b\u5bc6\u96c6\u5b50\u56fe\u5f88\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u57fa\u4e8e\u4ee3\u7406\u7684\u6570\u636e\u6316\u6398\u65b9\u6cd5\u5728\u4e8c\u5206\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u8f83\u5c11\u3002", "method": "\u4ee3\u7406\u901a\u8fc7\u534f\u4f5c\u6784\u5efa\u751f\u6210\u6811\u5e76\u9009\u4e3e\u9886\u5bfc\u8005\uff0c\u5229\u7528\u65b0\u9896\u7684\u76f8\u90bb\u4ee3\u7406\u4f1a\u9762\u673a\u5236\uff0c\u9ad8\u6548\u5730\u5b8c\u6210\u8774\u8776\u8ba1\u6570\u3002", "result": "\u7b97\u6cd5\u5728\u4fdd\u6301\u4f4e\u8f6e\u6570\u548c\u5185\u5b58\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8774\u8776\u8ba1\u6570\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u4e8c\u5206\u56fe\uff0c\u8fd8\u53ef\u6269\u5c55\u5230\u4e00\u822c\u56fe\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u6548\u7387\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2506.17647", "pdf": "https://arxiv.org/pdf/2506.17647", "abs": "https://arxiv.org/abs/2506.17647", "authors": ["Yixian Qi", "Jiajun Jiang", "Fengjie Li", "Bowen Chen", "Hongyu Zhang", "Junjie Chen"], "title": "Improving Compiler Bug Isolation by Leveraging Large Language Models", "categories": ["cs.SE"], "comment": "12 pages, 7 figures", "summary": "Compilers play a foundational role in building reliable software systems, and\nbugs within them can lead to catastrophic consequences. The compilation process\ntypically involves hundreds of files, making traditional automated bug\nisolation techniques inapplicable due to scalability or effectiveness issues.\nCurrent mainstream compiler bug localization techniques have limitations in\ntest program mutation and resource consumption. Inspired by the recent advances\nof pre-trained Large Language Models (LLMs), we propose an innovative approach\nnamed AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)\nemploys specialized prompts to guide LLM in reordering suspicious file\nrankings. This approach leverages four types of information: the failing test\nprogram, source file function summaries, lists of suspicious files identified\nthrough analyzing test coverage, as well as compilation configurations with\nrelated output messages, resulting in a refined ranking of suspicious files.\nOur evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and\nFuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers\ndemonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,\n300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,\nrespectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the\nablation study underscores the significance of each component in our approach.", "AI": {"tldr": "AutoCBI\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6539\u8fdb\u7f16\u8bd1\u5668\u9519\u8bef\u5b9a\u4f4d\uff0c\u901a\u8fc7\u603b\u7ed3\u6587\u4ef6\u529f\u80fd\u548c\u91cd\u65b0\u6392\u5e8f\u53ef\u7591\u6587\u4ef6\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7f16\u8bd1\u5668\u9519\u8bef\u5b9a\u4f4d\u6280\u672f\u5728\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "AutoCBI\u7ed3\u5408LLM\u603b\u7ed3\u7f16\u8bd1\u5668\u6587\u4ef6\u529f\u80fd\uff0c\u5e76\u4f7f\u7528\u4e13\u95e8\u63d0\u793a\u91cd\u65b0\u6392\u5e8f\u53ef\u7591\u6587\u4ef6\u6392\u540d\uff0c\u5229\u7528\u56db\u79cd\u4fe1\u606f\u7c7b\u578b\u4f18\u5316\u7ed3\u679c\u3002", "result": "\u5728GCC\u548cLLVM\u7684120\u4e2a\u771f\u5b9e\u9519\u8bef\u6d4b\u8bd5\u4e2d\uff0cAutoCBI\u5728Top-1\u6392\u540d\u4e2d\u7684\u8868\u73b0\u4f18\u4e8eDiWi\u3001RecBi\u548cFuseFL\uff0c\u63d0\u5347\u4e8666.67%\u5230340%\u3002", "conclusion": "AutoCBI\u4e3a\u7f16\u8bd1\u5668\u9519\u8bef\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.18196", "pdf": "https://arxiv.org/pdf/2506.18196", "abs": "https://arxiv.org/abs/2506.18196", "authors": ["Fangzheng Liu", "Lancelot Blanchard", "Don D. Haddad", "Joseph A. Paradiso"], "title": "Two Sonification Methods for the MindCube", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS", "H.5.5"], "comment": "5 pages, 5 figures", "summary": "In this work, we explore the musical interface potential of the MindCube, an\ninteractive device designed to study emotions. Embedding diverse sensors and\ninput devices, this interface resembles a fidget cube toy commonly used to help\nusers relieve their stress and anxiety. As such, it is a particularly\nwell-suited controller for musical systems that aim to help with emotion\nregulation. In this regard, we present two different mappings for the MindCube,\nwith and without AI. With our generative AI mapping, we propose a way to infuse\nmeaning within a latent space and techniques to navigate through it with an\nexternal controller. We discuss our results and propose directions for future\nwork.", "AI": {"tldr": "\u63a2\u7d22MindCube\u4f5c\u4e3a\u97f3\u4e50\u754c\u9762\u7684\u6f5c\u529b\uff0c\u5229\u7528\u5176\u4f20\u611f\u5668\u548c\u8f93\u5165\u8bbe\u5907\u8bbe\u8ba1\u4e24\u79cd\u6620\u5c04\uff08\u542bAI\u548c\u4e0d\u542bAI\uff09\uff0c\u7528\u4e8e\u60c5\u611f\u8c03\u8282\u7684\u97f3\u4e50\u7cfb\u7edf\u3002", "motivation": "MindCube\u662f\u4e00\u6b3e\u7528\u4e8e\u7814\u7a76\u60c5\u611f\u7684\u4ea4\u4e92\u8bbe\u5907\uff0c\u9002\u5408\u4f5c\u4e3a\u97f3\u4e50\u7cfb\u7edf\u7684\u63a7\u5236\u5668\uff0c\u5e2e\u52a9\u8c03\u8282\u60c5\u7eea\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cdMindCube\u6620\u5c04\u65b9\u6848\uff0c\u4e00\u79cd\u5305\u542b\u751f\u6210\u5f0fAI\uff0c\u63d0\u51fa\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6ce8\u5165\u610f\u4e49\u5e76\u901a\u8fc7\u5916\u90e8\u63a7\u5236\u5668\u5bfc\u822a\u7684\u6280\u672f\u3002", "result": "\u8ba8\u8bba\u4e86\u7814\u7a76\u6210\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u65b9\u5411\u3002", "conclusion": "MindCube\u5728\u97f3\u4e50\u754c\u9762\u548c\u60c5\u611f\u8c03\u8282\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0cAI\u6280\u672f\u7684\u5e94\u7528\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.17912", "pdf": "https://arxiv.org/pdf/2506.17912", "abs": "https://arxiv.org/abs/2506.17912", "authors": ["Chuhao Jin", "Haosen Li", "Bingzi Zhang", "Che Liu", "Xiting Wang", "Ruihua Song", "Wenbing Huang", "Ying Qin", "Fuzheng Zhang", "Di Zhang"], "title": "PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis", "categories": ["cs.CV", "cs.MM"], "comment": "14 pages, 7 figures", "summary": "Recent advances in large language models (LLMs) have enabled breakthroughs in\nmany multimodal generation tasks, but a significant performance gap still\nexists in text-to-motion generation, where LLM-based methods lag far behind\nnon-LLM methods. We identify the granularity of motion tokenization as a\ncritical bottleneck: fine-grained tokenization induces local dependency issues,\nwhere LLMs overemphasize short-term coherence at the expense of global semantic\nalignment, while coarse-grained tokenization sacrifices motion details. To\nresolve this issue, we propose PlanMoGPT, an LLM-based framework integrating\nprogressive planning and flow-enhanced fine-grained motion tokenization. First,\nour progressive planning mechanism leverages LLMs' autoregressive capabilities\nto hierarchically generate motion tokens by starting from sparse global plans\nand iteratively refining them into full sequences. Second, our flow-enhanced\ntokenizer doubles the downsampling resolution and expands the codebook size by\neight times, minimizing detail loss during discretization, while a\nflow-enhanced decoder recovers motion nuances. Extensive experiments on\ntext-to-motion benchmarks demonstrate that it achieves state-of-the-art\nperformance, improving FID scores by 63.8% (from 0.380 to 0.141) on\nlong-sequence generation while enhancing motion diversity by 49.9% compared to\nexisting methods. The proposed framework successfully resolves the\ndiversity-quality trade-off that plagues current non-LLM approaches,\nestablishing new standards for text-to-motion generation.", "AI": {"tldr": "PlanMoGPT\u901a\u8fc7\u6e10\u8fdb\u5f0f\u89c4\u5212\u548c\u6d41\u589e\u5f3a\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u6807\u8bb0\u5316\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u7c92\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u8fd0\u52a8\u6807\u8bb0\u5316\u7684\u7c92\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51faPlanMoGPT\u6846\u67b6\uff0c\u7ed3\u5408\u6e10\u8fdb\u5f0f\u89c4\u5212\u548c\u6d41\u589e\u5f3a\u7ec6\u7c92\u5ea6\u6807\u8bb0\u5316\uff0c\u901a\u8fc7\u5206\u5c42\u751f\u6210\u8fd0\u52a8\u6807\u8bb0\u548c\u4f18\u5316\u89e3\u7801\u5668\u7ec6\u8282\u6062\u590d\u3002", "result": "\u5728\u6587\u672c\u5230\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFID\u5206\u6570\u63d0\u534763.8%\uff0c\u8fd0\u52a8\u591a\u6837\u6027\u589e\u52a049.9%\uff0c\u8fbe\u5230\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "PlanMoGPT\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6837\u6027-\u8d28\u91cf\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2506.17776", "pdf": "https://arxiv.org/pdf/2506.17776", "abs": "https://arxiv.org/abs/2506.17776", "authors": ["Dyuman Aditya", "Colton Payne", "Mario Leiva", "Paulo Shakarian"], "title": "Machine Learning Model Integration with Open World Temporal Logic for Process Automation", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": null, "summary": "Recent advancements in Machine Learning (ML) have yielded powerful models\ncapable of extracting structured information from diverse and complex data\nsources. However, a significant challenge lies in translating these perceptual\nor extractive outputs into actionable, reasoned decisions within complex\noperational workflows. To address these challenges, this paper introduces a\nnovel approach that integrates the outputs from various machine learning models\ndirectly with the PyReason framework, an open-world temporal logic programming\nreasoning engine. PyReason's foundation in generalized annotated logic allows\nfor the seamless incorporation of real-valued outputs (e.g., probabilities,\nconfidence scores) from diverse ML models, treating them as truth intervals\nwithin its logical framework. Crucially, PyReason provides mechanisms,\nimplemented in Python, to continuously poll ML model outputs, convert them into\nlogical facts, and dynamically recompute the minimal model, ensuring real-tine\nadaptive decision-making. Furthermore, its native support for temporal\nreasoning, knowledge graph integration, and fully explainable interface traces\nenables sophisticated analysis over time-sensitive process data and existing\norganizational knowledge. By combining the strengths of perception and\nextraction from ML models with the logical deduction and transparency of\nPyReason, we aim to create a powerful system for automating complex processes.\nThis integration finds utility across numerous domains, including\nmanufacturing, healthcare, and business operations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8f93\u51fa\u4e0ePyReason\u6846\u67b6\u7ed3\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u73b0\u5b9e\u65f6\u81ea\u9002\u5e94\u51b3\u7b56\u3002", "motivation": "\u89e3\u51b3\u5982\u4f55\u5c06\u673a\u5668\u5b66\u4e60\u7684\u611f\u77e5\u4e0e\u63d0\u53d6\u80fd\u529b\u8f6c\u5316\u4e3a\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u7684\u53ef\u64cd\u4f5c\u51b3\u7b56\u3002", "method": "\u901a\u8fc7PyReason\u6846\u67b6\uff0c\u5c06ML\u6a21\u578b\u7684\u5b9e\u503c\u8f93\u51fa\uff08\u5982\u6982\u7387\uff09\u8f6c\u6362\u4e3a\u903b\u8f91\u4e8b\u5b9e\uff0c\u5e76\u52a8\u6001\u8ba1\u7b97\u6700\u5c0f\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u4e86\u7ed3\u5408ML\u611f\u77e5\u80fd\u529b\u548cPyReason\u903b\u8f91\u63a8\u7406\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u652f\u6301\u5b9e\u65f6\u51b3\u7b56\u548c\u65f6\u95f4\u654f\u611f\u5206\u6790\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5236\u9020\u3001\u533b\u7597\u7b49\u591a\u4e2a\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.18407", "pdf": "https://arxiv.org/pdf/2506.18407", "abs": "https://arxiv.org/abs/2506.18407", "authors": ["Yiyao Wang", "Bo Pan", "Ke Wang", "Han Liu", "Jinyuan Mao", "Yuxin Liu", "Minfeng Zhu", "Bo Zhang", "Weifeng Chen", "Xiuqi Huang", "Wei Chen"], "title": "What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Direct volume rendering (DVR) is a fundamental technique for visualizing\nvolumetric data, with transfer functions (TFs) playing a crucial role in\nextracting meaningful structures. However, designing effective TFs remains\nunintuitive due to the semantic gap between user intent and TF parameter space.\nResearchers have developed numerous TF optimization methods to bridge this gap.\nHowever, existing methods still face two challenges: large exploration space\nand weak generalizability. To address these issues, we propose What You Think\nis What You Get (WYTWYG) framework, which leveraging Multi-model Large Language\nModels (MLLMs) to guide the TF optimization based on user intent. Specifically,\nwe first introduce a novel TF optimization approach comprising two core\ncomponents: (1) an evolution-based explorer for effective exploration of the TF\nspace, and (2) a volume rendering quality evaluator based on MLLMs to provide\ngeneralizable visual guidance. We further propose a TF interactive design\nsystem based on this approach. We demonstrate the general applicability of our\nframework through three case studies, and validate the effectiveness of each\ncomponent through extensive experiments. Our code is available at:\nhttps://github.com/wyysteelhead/TFevolve.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684WYTWYG\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u76f4\u63a5\u4f53\u6e32\u67d3\uff08DVR\uff09\u4e2d\u7684\u4f20\u9012\u51fd\u6570\uff08TFs\uff09\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u7d22\u7a7a\u95f4\u5927\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u9012\u51fd\u6570\uff08TFs\uff09\u5728\u4f53\u6570\u636e\u6e32\u67d3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bbe\u8ba1\u9ad8\u6548TFs\u4ecd\u56e0\u7528\u6237\u610f\u56fe\u4e0eTF\u53c2\u6570\u7a7a\u95f4\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u8ddd\u800c\u56f0\u96be\u3002\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u9762\u4e34\u63a2\u7d22\u7a7a\u95f4\u5927\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86WYTWYG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u90e8\u5206\uff1a(1)\u57fa\u4e8e\u8fdb\u5316\u7684\u63a2\u7d22\u5668\u7528\u4e8e\u9ad8\u6548\u63a2\u7d22TF\u7a7a\u95f4\uff0c(2)\u57fa\u4e8eMLLMs\u7684\u6e32\u67d3\u8d28\u91cf\u8bc4\u4f30\u5668\u63d0\u4f9b\u901a\u7528\u89c6\u89c9\u6307\u5bfc\u3002\u5e76\u6784\u5efa\u4e86\u4ea4\u4e92\u8bbe\u8ba1\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027\u3002", "conclusion": "WYTWYG\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8fdb\u5316\u548cMLLMs\uff0c\u663e\u8457\u63d0\u5347\u4e86TF\u4f18\u5316\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u4f53\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.18255", "pdf": "https://arxiv.org/pdf/2506.18255", "abs": "https://arxiv.org/abs/2506.18255", "authors": ["Jinjin Zhao", "Sanjay Krishnan"], "title": "Fast Capture of Cell-Level Provenance in Numpy", "categories": ["cs.DB"], "comment": null, "summary": "Effective provenance tracking enhances reproducibility, governance, and data\nquality in array workflows. However, significant challenges arise in capturing\nthis provenance, including: (1) rapidly evolving APIs, (2) diverse operation\ntypes, and (3) large-scale datasets. To address these challenges, this paper\npresents a prototype annotation system designed for arrays, which captures\ncell-level provenance specifically within the numpy library. With this\nprototype, we explore straightforward memory optimizations that substantially\nreduce annotation latency. We envision this provenance capture approach for\narrays as part of a broader governance system for tracking for structured data\nworkflows and diverse data science applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6570\u7ec4\u7684\u6ce8\u91ca\u7cfb\u7edf\u539f\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u6eaf\u6e90\u8ddf\u8e2a\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728numpy\u5e93\u4e2d\u6355\u83b7\u7ec6\u80de\u7ea7\u6eaf\u6e90\u3002", "motivation": "\u63d0\u9ad8\u6570\u7ec4\u5de5\u4f5c\u6d41\u7684\u53ef\u91cd\u590d\u6027\u3001\u6cbb\u7406\u548c\u6570\u636e\u8d28\u91cf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u539f\u578b\u6ce8\u91ca\u7cfb\u7edf\uff0c\u63a2\u7d22\u4e86\u5185\u5b58\u4f18\u5316\u4ee5\u51cf\u5c11\u6ce8\u91ca\u5ef6\u8fdf\u3002", "result": "\u7cfb\u7edf\u663e\u8457\u964d\u4f4e\u4e86\u6ce8\u91ca\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4f5c\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u5de5\u4f5c\u6d41\u548c\u591a\u6837\u5316\u6570\u636e\u79d1\u5b66\u5e94\u7528\u7684\u6cbb\u7406\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\u3002"}}
{"id": "2506.17865", "pdf": "https://arxiv.org/pdf/2506.17865", "abs": "https://arxiv.org/abs/2506.17865", "authors": ["Dinesh Reddy Ankireddy", "Sudipta Paria", "Aritra Dasgupta", "Sandip Ray", "Swarup Bhunia"], "title": "LASA: Enhancing SoC Security Verification with LLM-Aided Property Generation", "categories": ["cs.CR", "cs.ET"], "comment": "9 pages, 5 figures, 5 tables", "summary": "Ensuring the security of modern System-on-Chip (SoC) designs poses\nsignificant challenges due to increasing complexity and distributed assets\nacross the intellectual property (IP) blocks. Formal property verification\n(FPV) provides the capability to model and validate design behaviors through\nsecurity properties with model checkers; however, current practices require\nsignificant manual efforts to create such properties, making them\ntime-consuming, costly, and error-prone. The emergence of Large Language Models\n(LLMs) has showcased remarkable proficiency across diverse domains, including\nHDL code generation and verification tasks. Current LLM-based techniques often\nproduce vacuous assertions and lack efficient prompt generation, comprehensive\nverification, and bug detection. This paper presents LASA, a novel framework\nthat leverages LLMs and retrieval-augmented generation (RAG) to produce\nnon-vacuous security properties and SystemVerilog Assertions (SVA) from design\nspecifications and related documentation for bus-based SoC designs. LASA\nintegrates commercial EDA tool for FPV to generate coverage metrics and\niteratively refines prompts through a feedback loop to enhance coverage. The\neffectiveness of LASA is validated through various open-source SoC designs,\ndemonstrating high coverage values with an average of ~88\\%, denoting\ncomprehensive verification through efficient generation of security properties\nand SVAs. LASA also demonstrates bug detection capabilities, identifying five\nunique bugs in the buggy OpenTitan SoC from Hack@DAC'24 competition.", "AI": {"tldr": "LASA\u6846\u67b6\u5229\u7528LLM\u548cRAG\u6280\u672f\uff0c\u81ea\u52a8\u751f\u6210\u975e\u7a7a\u6d1e\u7684\u5b89\u5168\u5c5e\u6027\u548cSystemVerilog\u65ad\u8a00\uff0c\u63d0\u9ad8SoC\u8bbe\u8ba1\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u4ee3SoC\u8bbe\u8ba1\u590d\u6742\u5ea6\u9ad8\uff0c\u4f20\u7edf\u7684FPV\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5de5\u4f5c\u751f\u6210\u5b89\u5168\u5c5e\u6027\uff0c\u8017\u65f6\u4e14\u6613\u9519\u3002LLM\u548cRAG\u6280\u672f\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "LASA\u7ed3\u5408LLM\u548cRAG\u6280\u672f\uff0c\u4ece\u8bbe\u8ba1\u89c4\u8303\u548c\u6587\u6863\u4e2d\u751f\u6210\u975e\u7a7a\u6d1e\u7684\u5b89\u5168\u5c5e\u6027\u548cSVA\uff0c\u96c6\u6210EDA\u5de5\u5177\u8fdb\u884cFPV\uff0c\u5e76\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u4f18\u5316\u63d0\u793a\u4ee5\u63d0\u5347\u8986\u76d6\u7387\u3002", "result": "\u5728\u591a\u4e2a\u5f00\u6e90SoC\u8bbe\u8ba1\u4e2d\uff0cLASA\u5e73\u5747\u8986\u76d6\u7387\u7ea688%\uff0c\u5e76\u5728OpenTitan SoC\u4e2d\u53d1\u73b05\u4e2a\u72ec\u7279\u6f0f\u6d1e\u3002", "conclusion": "LASA\u8bc1\u660e\u4e86\u5229\u7528LLM\u548cRAG\u6280\u672f\u53ef\u9ad8\u6548\u751f\u6210\u5b89\u5168\u5c5e\u6027\u548cSVA\uff0c\u663e\u8457\u63d0\u5347SoC\u9a8c\u8bc1\u7684\u5168\u9762\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2506.17739", "pdf": "https://arxiv.org/pdf/2506.17739", "abs": "https://arxiv.org/abs/2506.17739", "authors": ["Paul Kilian", "Philipp Wiesner", "Odej Kao"], "title": "Choosing the Right Battery Model for Data Center Simulations", "categories": ["cs.DC"], "comment": "Presented at the 1st International Workshop on Low Carbon Computing\n  (LOCO) in Glasgow, Scotland, UK, on 3 December 2024", "summary": "As demand for computing resources continues to rise, the increasing cost of\nelectricity and anticipated regulations on carbon emissions are prompting\nchanges in data center power systems. Many providers are now operating compute\nnodes in microgrids, close to renewable power generators and energy storage, to\nmaintain full control over the cost and origin of consumed electricity.\nRecently, new co-simulation testbeds have emerged that integrate\ndomain-specific simulators to support research, development, and testing of\nsuch systems in a controlled environment. Yet, choosing an appropriate battery\nmodel for data center simulations remains challenging, as it requires balancing\nsimulation speed, realism, and ease of configuration.\n  In this paper, we implement four different battery models for data center\nscenarios within the co-simulation framework Vessim and analyze their behavior.\nThe results show that linear models, which consider inefficiencies and power\nlimits, closely match the behavior of complex physics-based models in\nshort-term experiments while offering faster execution, and not requiring\nknowledge on electrochemical reactions and circuit-level dynamics. In contrast,\nsimple, lossless models fail to accurately represent complex behavior and\nprovide no further runtime advantage.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u56db\u79cd\u7535\u6c60\u6a21\u578b\u5728\u6570\u636e\u4e2d\u5fc3\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7ebf\u6027\u6a21\u578b\u5728\u77ed\u671f\u5185\u65e2\u80fd\u4fdd\u6301\u9ad8\u4eff\u771f\u901f\u5ea6\uff0c\u53c8\u80fd\u63a5\u8fd1\u590d\u6742\u7269\u7406\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u589e\u957f\u548c\u78b3\u6392\u653e\u6cd5\u89c4\u7684\u4e25\u683c\u5316\uff0c\u6570\u636e\u4e2d\u5fc3\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u7535\u529b\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u800c\u9009\u62e9\u5408\u9002\u7684\u7535\u6c60\u6a21\u578b\u662f\u5176\u4e2d\u7684\u5173\u952e\u6311\u6218\u4e4b\u4e00\u3002", "method": "\u5728Vessim\u534f\u540c\u4eff\u771f\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u56db\u79cd\u4e0d\u540c\u7684\u7535\u6c60\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u7ebf\u6027\u6a21\u578b\u5728\u77ed\u671f\u5185\u65e2\u80fd\u4fdd\u6301\u9ad8\u4eff\u771f\u901f\u5ea6\uff0c\u53c8\u80fd\u63a5\u8fd1\u590d\u6742\u7269\u7406\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u800c\u7b80\u5355\u7684\u65e0\u635f\u6a21\u578b\u5219\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u590d\u6742\u884c\u4e3a\u4e14\u65e0\u8fd0\u884c\u65f6\u4f18\u52bf\u3002", "conclusion": "\u5728\u6570\u636e\u4e2d\u5fc3\u7684\u4eff\u771f\u7814\u7a76\u4e2d\uff0c\u7ebf\u6027\u6a21\u578b\u662f\u5e73\u8861\u901f\u5ea6\u548c\u51c6\u786e\u6027\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2506.17772", "pdf": "https://arxiv.org/pdf/2506.17772", "abs": "https://arxiv.org/abs/2506.17772", "authors": ["Haoran Xue", "Gias Uddin", "Song Wang"], "title": "PAGENT: Learning to Patch Software Engineering Agents", "categories": ["cs.SE"], "comment": null, "summary": "LLM Agents produce patches automatically to resolve an issue. However, they\ncan generate inaccurate patches. Little is known about the root causes behind\nthose failed patches or how those could be fixed. This paper reports an\nempirical study of the failed patches generated by seven top LLM code agents.\nWe collected 114 issues from the SWE-bench Lite dataset that remained\nunresolved across the agents. The seven agents produced a total of 769 failed\npatches for those issues, which we checked with a combination of GPT-4o and\nmanual analysis. We present a taxonomy of the failure reasons across the\npatches. The taxonomy contains six categories, with several sub-categories\nunder each category. For example, a frequently observed category is the\ninability of an LLM to correctly infer/produce the appropriate variable type in\nthe produced patch. As a first step towards addressing such type-related\nerrors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis\ntechniques like CFG creation and exploration to infer the type of information\nof a patch. PAGENT does this by applying repository-level static code analysis\ntechniques. Then, PAGENT refines the inferred type by further utilizing an\nLLM-based inference technique. We tested PAGENT on all 127 type-related failed\npatches from the top three agents in our study. PAGENT could fix 29 of the 127\nfailed patches.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7531\u4e03\u79cd\u9876\u7ea7LLM\u4ee3\u7801\u4ee3\u7406\u751f\u6210\u7684\u5931\u8d25\u8865\u4e01\uff0c\u63d0\u51fa\u4e86\u516d\u7c7b\u5931\u8d25\u539f\u56e0\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7a0b\u5e8f\u5206\u6790\u7684\u8865\u4e01\u4ee3\u7406PAGENT\uff0c\u7528\u4e8e\u89e3\u51b3\u7c7b\u578b\u76f8\u5173\u9519\u8bef\u3002", "motivation": "\u7406\u89e3LLM\u4ee3\u7406\u751f\u6210\u5931\u8d25\u8865\u4e01\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u6536\u96c6114\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5206\u6790769\u4e2a\u5931\u8d25\u8865\u4e01\uff0c\u8bbe\u8ba1PAGENT\u8fdb\u884c\u7c7b\u578b\u63a8\u65ad\u548c\u8865\u4e01\u4fee\u590d\u3002", "result": "PAGENT\u6210\u529f\u4fee\u590d\u4e86127\u4e2a\u7c7b\u578b\u76f8\u5173\u5931\u8d25\u8865\u4e01\u4e2d\u768429\u4e2a\u3002", "conclusion": "PAGENT\u662f\u4e00\u79cd\u6709\u6548\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u6539\u8fdbLLM\u4ee3\u7406\u751f\u6210\u7684\u8865\u4e01\u8d28\u91cf\u3002"}}
{"id": "2506.18269", "pdf": "https://arxiv.org/pdf/2506.18269", "abs": "https://arxiv.org/abs/2506.18269", "authors": ["Min Yin", "Haoyu Liu", "Boyi Lian", "Chunlei Chai"], "title": "Co-persona: Leveraging LLMs and Expert Collaboration to Understand User Personas through Social Media Data Analysis", "categories": ["cs.HC"], "comment": "17pages,5figures,8tables", "summary": "This study introduces \\textsc{Co-Persona}, a framework bridging large-scale\nsocial media analysis and user understanding via integration of Large Language\nModels (LLMs) and expert validation. Through a case study of B.Co, a Chinese\nmanufacturer, we applied \\textsc{Co-Persona} to bedside lamp development by\nanalyzing 38 million posts from Xiao Hongshu. Our multi-stage NLP processing\nrevealed five user personas based on nighttime behaviors: Health Aficionados,\nNight Owls, Interior Decorators, Child-care Workers, and Workaholics. These\npersonas exhibit distinct pre-sleep activities and product preferences. The\nmethod enhances manufacturers' ability to interpret social data while\npreserving user-centric insights, offering actionable strategies for targeted\nmarketing and product design. This work advances both theoretical persona\ndevelopment and practical consumer-driven innovation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCo-Persona\u7684\u6846\u67b6\uff0c\u7ed3\u5408LLMs\u548c\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u901a\u8fc7\u5206\u6790\u5c0f\u7ea2\u4e66\u4e0a\u76843800\u4e07\u6761\u5e16\u5b50\uff0c\u4e3aB.Co\u516c\u53f8\u5f00\u53d1\u5e8a\u5934\u706f\u4ea7\u54c1\u63d0\u53d6\u4e86\u4e94\u79cd\u7528\u6237\u753b\u50cf\u3002", "motivation": "\u901a\u8fc7\u5927\u89c4\u6a21\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e0e\u7528\u6237\u7406\u89e3\u7684\u7ed3\u5408\uff0c\u5e2e\u52a9\u5236\u9020\u5546\u66f4\u597d\u5730\u89e3\u8bfb\u7528\u6237\u884c\u4e3a\u5e76\u6307\u5bfc\u4ea7\u54c1\u8bbe\u8ba1\u548c\u8425\u9500\u7b56\u7565\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5NLP\u5904\u7406\u6280\u672f\uff0c\u7ed3\u5408LLMs\u548c\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u5206\u6790\u7528\u6237\u591c\u95f4\u884c\u4e3a\u5e76\u63d0\u53d6\u7528\u6237\u753b\u50cf\u3002", "result": "\u63ed\u793a\u4e86\u4e94\u79cd\u7528\u6237\u753b\u50cf\uff08\u5982\u5065\u5eb7\u7231\u597d\u8005\u3001\u591c\u732b\u5b50\u7b49\uff09\uff0c\u6bcf\u79cd\u753b\u50cf\u6709\u72ec\u7279\u7684\u7761\u524d\u6d3b\u52a8\u548c\u4ea7\u54c1\u504f\u597d\u3002", "conclusion": "Co-Persona\u6846\u67b6\u4e3a\u5236\u9020\u5546\u63d0\u4f9b\u4e86\u57fa\u4e8e\u6570\u636e\u7684\u7528\u6237\u6d1e\u5bdf\uff0c\u63a8\u52a8\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u7684\u7528\u6237\u753b\u50cf\u7814\u7a76\u3002"}}
{"id": "2506.18021", "pdf": "https://arxiv.org/pdf/2506.18021", "abs": "https://arxiv.org/abs/2506.18021", "authors": ["Chi Xie", "Shuang Liang", "Jie Li", "Feng Zhu", "Rui Zhao", "Yichen Wei", "Shengjie Zhao"], "title": "On the Robustness of Human-Object Interaction Detection against Distribution Shift", "categories": ["cs.CV", "cs.MM"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Human-Object Interaction (HOI) detection has seen substantial advances in\nrecent years. However, existing works focus on the standard setting with ideal\nimages and natural distribution, far from practical scenarios with inevitable\ndistribution shifts. This hampers the practical applicability of HOI detection.\nIn this work, we investigate this issue by benchmarking, analyzing, and\nenhancing the robustness of HOI detection models under various distribution\nshifts. We start by proposing a novel automated approach to create the first\nrobustness evaluation benchmark for HOI detection. Subsequently, we evaluate\nmore than 40 existing HOI detection models on this benchmark, showing their\ninsufficiency, analyzing the features of different frameworks, and discussing\nhow the robustness in HOI is different from other tasks. With the insights from\nsuch analyses, we propose to improve the robustness of HOI detection methods\nthrough: (1) a cross-domain data augmentation integrated with mixup, and (2) a\nfeature fusion strategy with frozen vision foundation models. Both are simple,\nplug-and-play, and applicable to various methods. Our experimental results\ndemonstrate that the proposed approach significantly increases the robustness\nof various methods, with benefits on standard benchmarks, too. The dataset and\ncode will be released.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u548c\u63d0\u5347\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u68c0\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7a33\u5065\u6027\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u65b0\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u57fa\u51c6\u548c\u4e24\u79cd\u7b80\u5355\u6709\u6548\u7684\u589e\u5f3a\u7b56\u7565\u3002", "motivation": "\u73b0\u6709HOI\u68c0\u6d4b\u7814\u7a76\u5ffd\u89c6\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5176\u7a33\u5065\u6027\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u5316\u65b9\u6cd5\u521b\u5efa\u9996\u4e2aHOI\u7a33\u5065\u6027\u8bc4\u4f30\u57fa\u51c6\uff1b\u5206\u679040\u591a\u4e2a\u73b0\u6709\u6a21\u578b\u7684\u4e0d\u8db3\uff1b\u901a\u8fc7\u8de8\u57df\u6570\u636e\u589e\u5f3a\u548c\u7279\u5f81\u878d\u5408\u7b56\u7565\u63d0\u5347\u7a33\u5065\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u6a21\u578b\u7684\u7a33\u5065\u6027\uff0c\u5e76\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86HOI\u68c0\u6d4b\u7a33\u5065\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u589e\u5f3a\u7b56\u7565\u3002"}}
{"id": "2506.17792", "pdf": "https://arxiv.org/pdf/2506.17792", "abs": "https://arxiv.org/abs/2506.17792", "authors": ["Alexandros Evangelidis", "Gricel V\u00e1zquez", "Simos Gerasimou"], "title": "Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition", "categories": ["cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Software-intensive systems, such as software product lines and robotics,\nutilise Markov decision processes (MDPs) to capture uncertainty and analyse\nsequential decision-making problems. Despite the usefulness of conventional\npolicy synthesis methods, they fail to scale to large state spaces. Our\napproach addresses this issue and accelerates policy synthesis in large MDPs by\ndynamically refining the MDP and iteratively selecting the most fragile MDP\nregions for refinement. This iterative procedure offers a balance between\naccuracy and efficiency, as refinement occurs only when necessary. Through a\ncomprehensive empirical evaluation comprising diverse case studies and MDPs up\nto 1M states, we demonstrate significant performance improvements yielded by\nour approach compared to the leading probabilistic model checker PRISM (up to\n2x), thus offering a very competitive solution for real-world policy synthesis\ntasks in larger MDPs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4f18\u5316\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u7b56\u7565\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u9009\u62e9\u6700\u8106\u5f31\u533a\u57df\u8fdb\u884c\u7ec6\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21MDP\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7b56\u7565\u5408\u6210\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5927\u89c4\u6a21MDP\u72b6\u6001\u7a7a\u95f4\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u51c6\u786e\u6027\u548c\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u52a8\u6001\u7ec6\u5316MDP\uff0c\u8fed\u4ee3\u9009\u62e9\u6700\u8106\u5f31\u533a\u57df\u8fdb\u884c\u4f18\u5316\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8fdb\u884c\u7ec6\u5316\u3002", "result": "\u5728\u5305\u542b\u591a\u79cd\u6848\u4f8b\u548c\u9ad8\u8fbe1M\u72b6\u6001\u7684MDP\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\uff0c\u6bd4PRISM\u5feb\u591a\u8fbe2\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21MDP\u4e2d\u7684\u5b9e\u9645\u7b56\u7565\u5408\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5177\u6709\u7ade\u4e89\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17911", "pdf": "https://arxiv.org/pdf/2506.17911", "abs": "https://arxiv.org/abs/2506.17911", "authors": ["Shefali Goel", "Vinod Kumar Verma", "Abhishek Verma"], "title": "LiSec-RTF: Reinforcing RPL Resilience Against Routing Table Falsification Attack in 6LoWPAN", "categories": ["cs.NI", "cs.CR"], "comment": "in IEEE Transactions on Mobile Computing, 2025", "summary": "Routing Protocol for Low-Power and Lossy Networks (RPL) is an\nenergy-efficient routing solution for IPv6 over Low-Power Wireless Personal\nArea Networks (6LoWPAN), recommended for resource-constrained devices. While\nRPL offers significant benefits, its security vulnerabilities pose challenges,\nparticularly due to unauthenticated control messages used to establish and\nmaintain routing information. These messages are susceptible to manipulation,\nenabling malicious nodes to inject false routing data. A notable security\nconcern is the Routing Table Falsification (RTF) attack, where attackers forge\nDestination Advertisement Object (DAO) messages to promote fake routes via a\nparent nodes routing table. Experimental results indicate that RTF attacks\nsignificantly reduce packet delivery ratio, increase end-to-end delay, and\nleverage power consumption. Currently, no effective countermeasures exist in\nthe literature, reinforcing the need for a security solution to prevent network\ndisruption and protect user applications. This paper introduces a Lightweight\nSecurity Solution against Routing Table Falsification Attack (LiSec-RTF),\nleveraging Physical Unclonable Functions (PUFs) to generate unique\nauthentication codes, termed Licenses. LiSec-RTF mitigates RTF attack impact\nwhile considering the resource limitations of 6LoWPAN devices in both static\nand mobile scenarios. Our testbed experiments indicate that LiSec-RTF\nsignificantly improves network performance compared to standard RPL under RTF\nattacks, thereby ensuring reliable and efficient operation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5b89\u5168\u65b9\u6848LiSec-RTF\uff0c\u5229\u7528\u7269\u7406\u4e0d\u53ef\u514b\u9686\u51fd\u6570\uff08PUFs\uff09\u751f\u6210\u72ec\u7279\u7684\u8ba4\u8bc1\u7801\uff0c\u4ee5\u5e94\u5bf9RPL\u534f\u8bae\u4e2d\u7684\u8def\u7531\u8868\u4f2a\u9020\uff08RTF\uff09\u653b\u51fb\uff0c\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "RPL\u534f\u8bae\u57286LoWPAN\u7f51\u7edc\u4e2d\u56e0\u63a7\u5236\u6d88\u606f\u672a\u8ba4\u8bc1\u800c\u5bb9\u6613\u53d7\u5230\u8def\u7531\u8868\u4f2a\u9020\u653b\u51fb\uff0c\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8ePUFs\u7684LiSec-RTF\u65b9\u6848\uff0c\u751f\u6210\u72ec\u7279\u8ba4\u8bc1\u7801\uff08Licenses\uff09\u6765\u9a8c\u8bc1\u8def\u7531\u6d88\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLiSec-RTF\u80fd\u663e\u8457\u63d0\u9ad8\u7f51\u7edc\u6027\u80fd\uff0c\u51cf\u5c11RTF\u653b\u51fb\u7684\u5f71\u54cd\u3002", "conclusion": "LiSec-RTF\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u9759\u6001\u548c\u79fb\u52a8\u573a\u666f\u7684\u8f7b\u91cf\u7ea7\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86RPL\u534f\u8bae\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2506.18601", "pdf": "https://arxiv.org/pdf/2506.18601", "abs": "https://arxiv.org/abs/2506.18601", "authors": ["Denys Rozumnyi", "Jonathon Luiten", "Numair Khan", "Johannes Sch\u00f6nberger", "Peter Kontschieder"], "title": "BulletGen: Improving 4D Reconstruction with Bullet-Time Generation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Transforming casually captured, monocular videos into fully immersive dynamic\nexperiences is a highly ill-posed task, and comes with significant challenges,\ne.g., reconstructing unseen regions, and dealing with the ambiguity in\nmonocular depth estimation. In this work we introduce BulletGen, an approach\nthat takes advantage of generative models to correct errors and complete\nmissing information in a Gaussian-based dynamic scene representation. This is\ndone by aligning the output of a diffusion-based video generation model with\nthe 4D reconstruction at a single frozen \"bullet-time\" step. The generated\nframes are then used to supervise the optimization of the 4D Gaussian model.\nOur method seamlessly blends generative content with both static and dynamic\nscene components, achieving state-of-the-art results on both novel-view\nsynthesis, and 2D/3D tracking tasks.", "AI": {"tldr": "BulletGen\u5229\u7528\u751f\u6210\u6a21\u578b\u6821\u6b63\u9ad8\u65af\u52a8\u6001\u573a\u666f\u8868\u793a\u4e2d\u7684\u9519\u8bef\u5e76\u8865\u5168\u7f3a\u5931\u4fe1\u606f\uff0c\u901a\u8fc7\u6269\u6563\u89c6\u9891\u751f\u6210\u6a21\u578b\u5bf9\u9f504D\u91cd\u5efa\uff0c\u5b9e\u73b0\u65b0\u89c6\u89d2\u5408\u6210\u548c2D/3D\u8ddf\u8e2a\u7684\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5355\u76ee\u89c6\u9891\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u672a\u89c2\u5bdf\u533a\u57df\u548c\u6df1\u5ea6\u4f30\u8ba1\u6b67\u4e49\u6027\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6269\u6563\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0e4D\u9ad8\u65af\u52a8\u6001\u573a\u666f\u8868\u793a\uff0c\u751f\u6210\u5e27\u7528\u4e8e\u4f18\u53164D\u9ad8\u65af\u6a21\u578b\u3002", "result": "\u5728\u751f\u6210\u5185\u5bb9\u4e0e\u9759\u6001/\u52a8\u6001\u573a\u666f\u878d\u5408\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8ddf\u8e2a\u4efb\u52a1\u8fbe\u5230SOTA\u3002", "conclusion": "BulletGen\u901a\u8fc7\u751f\u6210\u5185\u5bb9\u589e\u5f3a4D\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u52a8\u6001\u573a\u666f\u7684\u6c89\u6d78\u5f0f\u4f53\u9a8c\u8d28\u91cf\u3002"}}
{"id": "2506.18257", "pdf": "https://arxiv.org/pdf/2506.18257", "abs": "https://arxiv.org/abs/2506.18257", "authors": ["Jinjin Zhao", "Sanjay Krishnan"], "title": "TableVault: Managing Dynamic Data Collections for LLM-Augmented Workflows", "categories": ["cs.DB"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating\nand executing complex data tasks. However, their integration into more complex\ndata workflows introduces significant management challenges. In response, we\npresent TableVault - a data management system designed to handle dynamic data\ncollections in LLM-augmented environments. TableVault meets the demands of\nthese workflows by supporting concurrent execution, ensuring reproducibility,\nmaintaining robust data versioning, and enabling composable workflow design. By\nmerging established database methodologies with emerging LLM-driven\nrequirements, TableVault offers a transparent platform that efficiently manages\nboth structured data and associated data artifacts.", "AI": {"tldr": "TableVault\u662f\u4e00\u6b3e\u4e3aLLM\u589e\u5f3a\u73af\u5883\u8bbe\u8ba1\u7684\u6570\u636e\u7ba1\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u5e76\u53d1\u6267\u884c\u3001\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u3001\u6570\u636e\u7248\u672c\u63a7\u5236\u548c\u53ef\u7ec4\u5408\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u6570\u636e\u5de5\u4f5c\u6d41\u4e2d\u7684\u7ba1\u7406\u6311\u6218\u3002", "method": "\u7ed3\u5408\u4f20\u7edf\u6570\u636e\u5e93\u65b9\u6cd5\u548cLLM\u9a71\u52a8\u9700\u6c42\uff0c\u63d0\u4f9b\u900f\u660e\u5e73\u53f0\u7ba1\u7406\u7ed3\u6784\u5316\u6570\u636e\u548c\u76f8\u5173\u6570\u636e\u5de5\u4ef6\u3002", "result": "TableVault\u80fd\u9ad8\u6548\u5904\u7406\u52a8\u6001\u6570\u636e\u96c6\u5408\uff0c\u6ee1\u8db3\u590d\u6742\u5de5\u4f5c\u6d41\u9700\u6c42\u3002", "conclusion": "TableVault\u4e3aLLM\u589e\u5f3a\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18424", "pdf": "https://arxiv.org/pdf/2506.18424", "abs": "https://arxiv.org/abs/2506.18424", "authors": ["Chengjie Liu", "Weiyu Chen", "Huiyao Xu", "Yuan Du", "Jun Yang", "Li Du"], "title": "A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction", "categories": ["cs.AI", "cs.ET"], "comment": "Accepted by ISEDA 2025", "summary": "In the design process of the analog circuit pre-layout phase, device sizing\nis an important step in determining whether an analog circuit can meet the\nrequired performance metrics. Many existing techniques extract the circuit\nsizing task as a mathematical optimization problem to solve and continuously\nimprove the optimization efficiency from a mathematical perspective. But they\nignore the automatic introduction of prior knowledge, fail to achieve effective\npruning of the search space, which thereby leads to a considerable compression\nmargin remaining in the search space. To alleviate this problem, we propose a\nlarge language model (LLM)-based multi-agent framework for analog circuits'\nsizing relationships extraction from academic papers. The search space in the\nsizing process can be effectively pruned based on the sizing relationship\nextracted by this framework. Eventually, we conducted tests on 3 types of\ncircuits, and the optimization efficiency was improved by $2.32 \\sim 26.6\n\\times$. This work demonstrates that the LLM can effectively prune the search\nspace for analog circuit sizing, providing a new solution for the combination\nof LLMs and conventional analog circuit design automation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591aagent\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5b66\u672f\u8bba\u6587\u4e2d\u63d0\u53d6\u6a21\u62df\u7535\u8def\u7684\u5c3a\u5bf8\u5173\u7cfb\uff0c\u4ece\u800c\u6709\u6548\u4fee\u526a\u641c\u7d22\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u5728\u6a21\u62df\u7535\u8def\u9884\u5e03\u5c40\u9636\u6bb5\uff0c\u4f20\u7edf\u65b9\u6cd5\u5ffd\u7565\u5148\u9a8c\u77e5\u8bc6\u7684\u81ea\u52a8\u5f15\u5165\uff0c\u5bfc\u81f4\u641c\u7d22\u7a7a\u95f4\u538b\u7f29\u4e0d\u8db3\uff0c\u5f71\u54cd\u4f18\u5316\u6548\u7387\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591aagent\u6846\u67b6\uff0c\u4ece\u5b66\u672f\u8bba\u6587\u4e2d\u63d0\u53d6\u7535\u8def\u7684\u5c3a\u5bf8\u5173\u7cfb\uff0c\u7528\u4e8e\u4fee\u526a\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u57283\u79cd\u7535\u8def\u4e0a\u6d4b\u8bd5\uff0c\u4f18\u5316\u6548\u7387\u63d0\u9ad82.32\u81f326.6\u500d\u3002", "conclusion": "LLM\u80fd\u6709\u6548\u4fee\u526a\u6a21\u62df\u7535\u8def\u5c3a\u5bf8\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4e3aLLM\u4e0e\u4f20\u7edf\u81ea\u52a8\u5316\u8bbe\u8ba1\u65b9\u6cd5\u7684\u7ed3\u5408\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2506.17757", "pdf": "https://arxiv.org/pdf/2506.17757", "abs": "https://arxiv.org/abs/2506.17757", "authors": ["Antonio Cruciani"], "title": "Maintaining a Bounded Degree Expander in Dynamic Peer-to-Peer Networks", "categories": ["cs.DC"], "comment": null, "summary": "We study the problem of maintaining robust and sparse overlay networks in\nfully distributed settings where nodes continuously join and leave the system.\nThis scenario closely models real-world unstructured peer-to-peer networks,\nwhere maintaining a well-connected yet low-degree communication graph is\ncrucial. We generalize a recent protocol by Becchetti et al. [SODA 2020] that\nrelies on a simple randomized connection strategy to build an expander topology\nwith high probability to a dynamic networks with churn setting. In this work,\nthe network dynamism is governed by an oblivious adversary that controls which\nnodes join and leave the system in each round. The adversary has full knowledge\nof the system and unbounded computational power, but cannot see the random\nchoices made by the protocol. Our analysis builds on the framework of Augustine\net al. [FOCS 2015], and shows that our distributed algorithm maintains a\nconstant-degree expander graph with high probability, despite a continuous\nadversarial churn with a rate of up to $\\mathcal{O}(n/polylog(n))$ per round,\nwhere $n$ is the stable network size. The protocol and proof techniques are not\nnew, but together they resolve a specific open problem raised in prior work.\nThe result is a simple, fully distributed, and churn-resilient protocol with\nprovable guarantees that align with observed empirical behavior.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u8282\u70b9\u9891\u7e41\u52a0\u5165\u548c\u9000\u51fa\u7684\u5b8c\u5168\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7ef4\u62a4\u5065\u58ee\u4e14\u7a00\u758f\u7684\u8986\u76d6\u7f51\u7edc\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u8fde\u63a5\u7b56\u7565\u7684\u52a8\u6001\u6269\u5c55\u7f51\u7edc\u534f\u8bae\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u975e\u7ed3\u6784\u5316P2P\u7f51\u7edc\u4e2d\u7ef4\u62a4\u4f4e\u5ea6\u901a\u4fe1\u56fe\u7684\u95ee\u9898\u3002", "method": "\u6269\u5c55\u4e86Becchetti\u7b49\u4eba\u7684\u968f\u673a\u8fde\u63a5\u7b56\u7565\uff0c\u7ed3\u5408\u52a8\u6001\u7f51\u7edc\u548c\u8282\u70b9\u6d41\u5931\u7684\u5bf9\u6297\u6027\u6a21\u578b\u3002", "result": "\u534f\u8bae\u5728\u5bf9\u6297\u6027\u6d41\u5931\u4e0b\u4ecd\u80fd\u9ad8\u6982\u7387\u7ef4\u6301\u5e38\u6570\u5ea6\u6269\u5c55\u56fe\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u5168\u5206\u5e03\u5f0f\u4e14\u6297\u6d41\u5931\u7684\u534f\u8bae\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2506.17798", "pdf": "https://arxiv.org/pdf/2506.17798", "abs": "https://arxiv.org/abs/2506.17798", "authors": ["Wang Lingxiang", "Quanzhi Fu", "Wenjia Song", "Gelei Deng", "Yi Liu", "Dan Williams", "Ying Zhang"], "title": "SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "The integration of open-source third-party library dependencies in Java\ndevelopment introduces significant security risks when these libraries contain\nknown vulnerabilities. Existing Software Composition Analysis (SCA) tools\nstruggle to effectively detect vulnerable API usage from these libraries due to\nlimitations in understanding API usage semantics and computational challenges\nin analyzing complex codebases, leading to inaccurate vulnerability alerts that\nburden development teams and delay critical security fixes.\n  To address these challenges, we proposed SAVANT by leveraging two insights:\nproof-of-vulnerability test cases demonstrate how vulnerabilities can be\ntriggered in specific contexts, and Large Language Models (LLMs) can understand\ncode semantics. SAVANT combines semantic preprocessing with LLM-powered context\nanalysis for accurate vulnerability detection. SAVANT first segments source\ncode into meaningful blocks while preserving semantic relationships, then\nleverages LLM-based reflection to analyze API usage context and determine\nactual vulnerability impacts. Our evaluation on 55 real-world applications\nshows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and\n78.5% F1-score, outperforming state-of-the-art SCA tools.", "AI": {"tldr": "SAVANT\u662f\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u9884\u5904\u7406\u548cLLM\u7684\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4f9d\u8d56\u5e93\u4e2d\u7684API\u4f7f\u7528\u4e0a\u4e0b\u6587\uff0c\u6709\u6548\u63d0\u9ad8\u4e86Java\u5f00\u53d1\u4e2d\u5f00\u6e90\u7b2c\u4e09\u65b9\u5e93\u7684\u5b89\u5168\u98ce\u9669\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709SCA\u5de5\u5177\u56e0\u96be\u4ee5\u7406\u89e3API\u4f7f\u7528\u8bed\u4e49\u548c\u5206\u6790\u590d\u6742\u4ee3\u7801\u5e93\u7684\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u6f0f\u6d1e\u68c0\u6d4b\u4e0d\u51c6\u786e\uff0c\u589e\u52a0\u4e86\u5f00\u53d1\u56e2\u961f\u7684\u8d1f\u62c5\u5e76\u5ef6\u8bef\u5b89\u5168\u4fee\u590d\u3002", "method": "SAVANT\u7ed3\u5408\u6f0f\u6d1e\u6d4b\u8bd5\u7528\u4f8b\u548cLLM\u7684\u4ee3\u7801\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u8bed\u4e49\u9884\u5904\u7406\u548cLLM\u9a71\u52a8\u7684\u4e0a\u4e0b\u6587\u5206\u6790\uff0c\u51c6\u786e\u68c0\u6d4b\u6f0f\u6d1e\u3002", "result": "\u572855\u4e2a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cSAVANT\u7684\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u3001\u51c6\u786e\u7387\u548cF1\u5206\u6570\u5747\u4f18\u4e8e\u73b0\u6709SCA\u5de5\u5177\u3002", "conclusion": "SAVANT\u901a\u8fc7\u521b\u65b0\u7684\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u5206\u6790\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u7684\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2506.18308", "pdf": "https://arxiv.org/pdf/2506.18308", "abs": "https://arxiv.org/abs/2506.18308", "authors": ["Feiqi Gu", "Zhixiong Wang", "Zhenyu Wang", "Dengbo He"], "title": "Supporting Car-Following Behavior through V2V-Based Beyond-Visual-Range Information Display", "categories": ["cs.HC"], "comment": null, "summary": "Rear-end collisions constituted a large portion of crashes on the road,\ndespite efforts to mitigate rear-end collisions, such as forward collision\nwarnings. The chance of rear-end collisions is closely related to drivers'\ncar-following (CF) behaviors in the traffic flow. Given that drivers may rely\non more than the information of the direct lead vehicle (DLV) when making CF\ndecisions, expanding drivers' perceptual range by providing beyond-visual-range\n(BVR) information based on vehicle-to-vehicle (V2V) communication may enhance\nCF safety. Thus, four different human-machine interfaces (HMIs) providing\nvarious types of BVR information in CF events were designed, including\nBrake-HMI showing only brake action of indirect lead vehicles (ILV), Dis-HMI\nand THW-HMI showing the relative distance and time headway between the ILV and\nDLV, respectively, and Video-HMI showing the live-stream video of ILV from the\nperspective of DLV. A driving simulator experiment with 40 participants was\nconducted to evaluate the impact of BVR-based HMI on driving safety in CF\nevents. We found that, in general, BVR information could improve CF safety\nwithout overloading drivers and compromising their visual attention allocation\nstrategies, particularly among novice drivers, by enabling quicker brake\nresponses and increasing time headway and time-to-collision in brake events.\nThe Brake-HMI yielded the safest performance in chain brake events, whereas\nVideo-HMI increased attentional demands without observable benefits. This\nresearch provides insights into enabling drivers' BVR perception based on V2V\ncommunication to enhance driving safety in CF scenarios.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u901a\u8fc7\u8f66\u8054\u7f51\uff08V2V\uff09\u901a\u4fe1\u63d0\u4f9b\u8d85\u89c6\u8ddd\uff08BVR\uff09\u4fe1\u606f\u5bf9\u8ddf\u8f66\u884c\u4e3a\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u8bbe\u8ba1\u56db\u79cd\u4eba\u673a\u754c\u9762\uff08HMI\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53d1\u73b0BVR\u4fe1\u606f\u80fd\u63d0\u5347\u5b89\u5168\uff0c\u4f46\u4e0d\u540cHMI\u6548\u679c\u4e0d\u540c\u3002", "motivation": "\u8ffd\u5c3e\u4e8b\u6545\u9891\u53d1\uff0c\u4f20\u7edf\u7684\u524d\u65b9\u78b0\u649e\u9884\u8b66\u6548\u679c\u6709\u9650\u3002\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u6269\u5c55\u9a7e\u9a76\u5458\u7684\u611f\u77e5\u8303\u56f4\uff0c\u5229\u7528V2V\u901a\u4fe1\u63d0\u4f9b\u7684BVR\u4fe1\u606f\uff0c\u63d0\u5347\u8ddf\u8f66\u884c\u4e3a\u7684\u5b89\u5168\u3002", "method": "\u8bbe\u8ba1\u4e86\u56db\u79cdHMI\uff08Brake-HMI\u3001Dis-HMI\u3001THW-HMI\u3001Video-HMI\uff09\uff0c\u63d0\u4f9b\u4e0d\u540c\u7c7b\u578b\u7684BVR\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u9a7e\u9a76\u6a21\u62df\u5668\u5b9e\u9a8c\uff0840\u540d\u53c2\u4e0e\u8005\uff09\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "BVR\u4fe1\u606f\u6574\u4f53\u80fd\u63d0\u5347\u5b89\u5168\uff0c\u5c24\u5176\u662f\u65b0\u624b\u9a7e\u9a76\u5458\uff0cBrake-HMI\u5728\u94fe\u5f0f\u5236\u52a8\u4e8b\u4ef6\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cVideo-HMI\u589e\u52a0\u4e86\u6ce8\u610f\u529b\u8d1f\u62c5\u4f46\u65e0\u663e\u8457\u76ca\u5904\u3002", "conclusion": "\u5229\u7528V2V\u901a\u4fe1\u63d0\u4f9bBVR\u4fe1\u606f\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8ddf\u8f66\u884c\u4e3a\u7684\u5b89\u5168\u6027\uff0c\u4f46\u9700\u9009\u62e9\u5408\u9002\u7684\u4fe1\u606f\u5448\u73b0\u65b9\u5f0f\u4ee5\u907f\u514d\u989d\u5916\u8d1f\u62c5\u3002"}}
{"id": "2506.18034", "pdf": "https://arxiv.org/pdf/2506.18034", "abs": "https://arxiv.org/abs/2506.18034", "authors": ["Fenghe Tang", "Wenxin Ma", "Zhiyang He", "Xiaodong Tao", "Zihang Jiang", "S. Kevin Zhou"], "title": "Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by MICCAI 2025. Code: https://github.com/FengheTan9/LLM4Seg", "summary": "With the advancement of Large Language Model (LLM) for natural language\nprocessing, this paper presents an intriguing finding: a frozen pre-trained LLM\nlayer can process visual tokens for medical image segmentation tasks.\nSpecifically, we propose a simple hybrid structure that integrates a\npre-trained, frozen LLM layer within the CNN encoder-decoder segmentation\nframework (LLM4Seg). Surprisingly, this design improves segmentation\nperformance with a minimal increase in trainable parameters across various\nmodalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our\nin-depth analysis reveals the potential of transferring LLM's semantic\nawareness to enhance segmentation tasks, offering both improved global\nunderstanding and better local modeling capabilities. The improvement proves\nrobust across different LLMs, validated using LLaMA and DeepSeek.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c42\u53ef\u7528\u4e8e\u5904\u7406\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLLM4Seg\u7684\u6df7\u5408\u7ed3\u6784\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u4e14\u8bad\u7ec3\u53c2\u6570\u589e\u52a0\u6781\u5c11\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3LLM\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5229\u7528\u5176\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "method": "\u63d0\u51faLLM4Seg\u7ed3\u6784\uff0c\u5c06\u9884\u8bad\u7ec3\u4e14\u51bb\u7ed3\u7684LLM\u5c42\u96c6\u6210\u5230CNN\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u5272\u6846\u67b6\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u6a21\u6001\u4e0a\u63d0\u9ad8\u4e86\u5206\u5272\u6027\u80fd\uff0c\u4e14\u5bf9\u4e0d\u540cLLM\uff08\u5982LLaMA\u548cDeepSeek\uff09\u5747\u6709\u6548\u3002", "conclusion": "LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u53ef\u6709\u6548\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.18559", "pdf": "https://arxiv.org/pdf/2506.18559", "abs": "https://arxiv.org/abs/2506.18559", "authors": ["Hong Qing Yu"], "title": "T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent", "categories": ["cs.AI", "cs.LO", "I.2.7; F.4.1"], "comment": null, "summary": "Large language models excel at generating fluent text but frequently struggle\nwith structured reasoning involving temporal constraints, causal relationships,\nand probabilistic reasoning. To address these limitations, we propose Temporal\nCausal Probabilistic Description Logic (T-CPDL), an integrated framework that\nextends traditional Description Logic with temporal interval operators,\nexplicit causal relationships, and probabilistic annotations. We present two\ndistinct variants of T-CPDL: one capturing qualitative temporal relationships\nthrough Allen's interval algebra, and another variant enriched with explicit\ntimestamped causal assertions. Both variants share a unified logical structure,\nenabling complex reasoning tasks ranging from simple temporal ordering to\nnuanced probabilistic causation. Empirical evaluations on temporal reasoning\nand causal inference benchmarks confirm that T-CPDL substantially improves\ninference accuracy, interpretability, and confidence calibration of language\nmodel outputs. By delivering transparent reasoning paths and fine-grained\ntemporal and causal semantics, T-CPDL significantly enhances the capability of\nlanguage models to support robust, explainable, and trustworthy\ndecision-making. This work also lays the groundwork for developing advanced\nLogic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially\nboosting the reasoning capabilities and efficiency of knowledge graph-enhanced\nRAG systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aT-CPDL\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u3001\u56e0\u679c\u548c\u6982\u7387\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u7ea6\u675f\u3001\u56e0\u679c\u5173\u7cfb\u548c\u6982\u7387\u63a8\u7406\u7b49\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u6269\u5c55\u63cf\u8ff0\u903b\u8f91\uff0c\u5f15\u5165\u65f6\u95f4\u533a\u95f4\u64cd\u4f5c\u7b26\u3001\u663e\u5f0f\u56e0\u679c\u5173\u7cfb\u548c\u6982\u7387\u6807\u6ce8\uff0c\u63d0\u51fa\u4e24\u79cdT-CPDL\u53d8\u4f53\u3002", "result": "\u5728\u65f6\u95f4\u63a8\u7406\u548c\u56e0\u679c\u63a8\u65ad\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cT-CPDL\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "conclusion": "T-CPDL\u589e\u5f3a\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u900f\u660e\u5ea6\u4e0e\u53ef\u4fe1\u5ea6\uff0c\u5e76\u4e3aLogic-RAG\u6846\u67b6\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.18159", "pdf": "https://arxiv.org/pdf/2506.18159", "abs": "https://arxiv.org/abs/2506.18159", "authors": ["Siddique Abubakr Muntaka", "Jacques Bou Abdo", "Kemi Akanbi", "Sunkanmi Oluwadare", "Faiza Hussein", "Oliver Konyo", "Michael Asante"], "title": "Mapping The Invisible Internet: Framework and Dataset", "categories": ["cs.NI"], "comment": "PREPRINT SUBMITTED TO DATA IN BRIEF (ELSEVIER)", "summary": "This article presents a novel dataset focusing on the network layer of the\nInvisible Internet Project (I2P), where prior research has predominantly\nexamined application layers like the dark web. Data was collected through the\nSWARM- I2P framework, deploying I2P routers as mapping agents, utilizing\ndynamic port mapping (30000-50000 range). The dataset documents over 50,000\nnodes, including 2,077 FastSet nodes and 2,331 high-capacity nodes\ncharacterized by bandwidth, latency (mean 121.21ms +- 48.50), and uptime\nmetrics. It contains 1,997 traffic records (1,003,032 packets/bytes) and\n4,222,793 records (2,147,585,625 packets/bytes), with geographic distributions\nfor 3,444 peers showing capacity metrics (mean 8.57 +- 1.20). Collection\nmethods included router console queries (127.0.0.1:port/tunnels), netDb\nanalysis, and passive monitoring, with anonymized identifiers. Data is\nstructured in CSV/TXT formats (Zenodo) with collection scripts (GitHub).\nPotential applications include tunnel peer selection analysis, anonymity\nnetwork resilience studies, and adversarial modelling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u805a\u7126\u4e8eI2P\u7f51\u7edc\u5c42\u7684\u65b0\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u5148\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5e94\u7528\u5c42\u7684\u7a7a\u767d\u3002", "motivation": "\u6b64\u524d\u7814\u7a76\u591a\u5173\u6ce8\u6697\u7f51\u7b49\u5e94\u7528\u5c42\uff0c\u800c\u672c\u7814\u7a76\u901a\u8fc7\u6536\u96c6I2P\u7f51\u7edc\u5c42\u6570\u636e\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u7f51\u7edc\u5206\u6790\u57fa\u7840\u3002", "method": "\u4f7f\u7528SWARM-I2P\u6846\u67b6\u90e8\u7f72I2P\u8def\u7531\u5668\u4f5c\u4e3a\u6620\u5c04\u4ee3\u7406\uff0c\u7ed3\u5408\u52a8\u6001\u7aef\u53e3\u6620\u5c04\uff0c\u901a\u8fc7\u8def\u7531\u5668\u67e5\u8be2\u3001netDb\u5206\u6790\u548c\u88ab\u52a8\u76d1\u63a7\u6536\u96c6\u6570\u636e\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b50,000\u591a\u4e2a\u8282\u70b9\uff0c\u8be6\u5c3d\u7684\u5e26\u5bbd\u3001\u5ef6\u8fdf\u548c\u8fd0\u884c\u65f6\u95f4\u6307\u6807\uff0c\u4ee5\u53ca\u5927\u91cf\u6d41\u91cf\u8bb0\u5f55\u548c\u5730\u7406\u4f4d\u7f6e\u5206\u5e03\u6570\u636e\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u96a7\u9053\u5bf9\u7b49\u9009\u62e9\u3001\u533f\u540d\u7f51\u7edc\u5f39\u6027\u7814\u7a76\u548c\u5bf9\u6297\u5efa\u6a21\u7b49\u5e94\u7528\u3002"}}
{"id": "2506.18680", "pdf": "https://arxiv.org/pdf/2506.18680", "abs": "https://arxiv.org/abs/2506.18680", "authors": ["Anindita Ghosh", "Bing Zhou", "Rishabh Dabral", "Jian Wang", "Vladislav Golyanik", "Christian Theobalt", "Philipp Slusallek", "Chuan Guo"], "title": "DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": "11 pages, 7 figures, 2 tables, accepted in ACM Siggraph 2025\n  conference track", "summary": "We present DuetGen, a novel framework for generating interactive two-person\ndances from music. The key challenge of this task lies in the inherent\ncomplexities of two-person dance interactions, where the partners need to\nsynchronize both with each other and with the music. Inspired by the recent\nadvances in motion synthesis, we propose a two-stage solution: encoding\ntwo-person motions into discrete tokens and then generating these tokens from\nmusic. To effectively capture intricate interactions, we represent both\ndancers' motions as a unified whole to learn the necessary motion tokens, and\nadopt a coarse-to-fine learning strategy in both the stages. Our first stage\nutilizes a VQ-VAE that hierarchically separates high-level semantic features at\na coarse temporal resolution from low-level details at a finer resolution,\nproducing two discrete token sequences at different abstraction levels.\nSubsequently, in the second stage, two generative masked transformers learn to\nmap music signals to these dance tokens: the first producing high-level\nsemantic tokens, and the second, conditioned on music and these semantic\ntokens, producing the low-level tokens. We train both transformers to learn to\npredict randomly masked tokens within the sequence, enabling them to\niteratively generate motion tokens by filling an empty token sequence during\ninference. Through the hierarchical masked modeling and dedicated interaction\nrepresentation, DuetGen achieves the generation of synchronized and interactive\ntwo-person dances across various genres. Extensive experiments and user studies\non a benchmark duet dance dataset demonstrate state-of-the-art performance of\nDuetGen in motion realism, music-dance alignment, and partner coordination.", "AI": {"tldr": "DuetGen\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u97f3\u4e50\u751f\u6210\u4e92\u52a8\u53cc\u4eba\u821e\u8e48\u3002\u5b83\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u7f16\u7801\u52a8\u4f5c\u4e3a\u79bb\u6563\u4ee4\u724c\u5e76\u751f\u6210\u4ee4\u724c\uff09\u548c\u5206\u5c42\u5efa\u6a21\u6280\u672f\uff08\u6355\u6349\u4ea4\u4e92\u7ec6\u8282\uff09\uff0c\u5b9e\u73b0\u4e86\u821e\u8e48\u4e0e\u97f3\u4e50\u540c\u6b65\u53ca\u821e\u4f34\u534f\u8c03\uff0c\u6548\u679c\u9886\u5148\u3002", "motivation": "\u89e3\u51b3\u53cc\u4eba\u821e\u8e48\u751f\u6210\u7684\u590d\u6742\u6311\u6218\uff0c\u5305\u62ec\u821e\u4f34\u95f4\u7684\u4e92\u52a8\u540c\u6b65\u4ee5\u53ca\u4e0e\u97f3\u4e50\u7684\u534f\u8c03\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u7528VQ-VAE\u5c06\u53cc\u4eba\u52a8\u4f5c\u7f16\u7801\u4e3a\u5206\u5c42\u79bb\u6563\u4ee4\u724c\uff1b2\uff09\u901a\u8fc7\u63a9\u7801\u53d8\u6362\u5668\u4ece\u97f3\u4e50\u751f\u6210\u5206\u5c42\u4ee4\u724c\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u52a8\u4f5c\u771f\u5b9e\u6027\u3001\u97f3\u4e50\u821e\u8e48\u5bf9\u9f50\u53ca\u821e\u4f34\u534f\u8c03\u5747\u8fbe\u5230\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "DuetGen\u901a\u8fc7\u5206\u5c42\u5efa\u6a21\u548c\u4ea4\u4e92\u8868\u793a\uff0c\u6210\u529f\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u4e92\u52a8\u53cc\u4eba\u821e\u8e48\u3002"}}
{"id": "2506.18772", "pdf": "https://arxiv.org/pdf/2506.18772", "abs": "https://arxiv.org/abs/2506.18772", "authors": ["Hassan S. Al Khatib", "Subash Neupane", "Sudip Mittal", "Shahram Rahimi", "Nina Marhamati", "Sean Bozorgzad"], "title": "Patient Journey Ontology: Representing Medical Encounters for Enhanced Patient-Centric Applications", "categories": ["cs.DB"], "comment": null, "summary": "The healthcare industry is moving towards a patient-centric paradigm that\nrequires advanced methods for managing and representing patient data. This\npaper presents a Patient Journey Ontology (PJO), a framework that aims to\ncapture the entirety of a patient's healthcare encounters. Utilizing\nontologies, the PJO integrates different patient data sources like medical\nhistories, diagnoses, treatment pathways, and outcomes; it enables semantic\ninteroperability and enhances clinical reasoning. By capturing temporal,\nsequential, and causal relationships between medical encounters, the PJO\nsupports predictive analytics, enabling earlier interventions and optimized\ntreatment plans. The ontology's structure, including its main classes,\nsubclasses, properties, and relationships, as detailed in the paper,\ndemonstrates its ability to provide a holistic view of patient care.\nQuantitative and qualitative evaluations by Subject Matter Experts (SMEs)\ndemonstrate strong capabilities in patient history retrieval, symptom tracking,\nand provider interaction representation, while identifying opportunities for\nenhanced diagnosis-symptom linking. These evaluations reveal the PJO's\nreliability and practical applicability, demonstrating its potential to enhance\npatient outcomes and healthcare efficiency. This work contributes to the\nongoing efforts of knowledge representation in healthcare, offering a reliable\ntool for personalized medicine, patient journey analysis and advancing the\ncapabilities of Generative AI in healthcare applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u60a3\u8005\u65c5\u7a0b\u672c\u4f53\uff08PJO\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u548c\u7ba1\u7406\u60a3\u8005\u591a\u6e90\u6570\u636e\uff0c\u652f\u6301\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u548c\u4e34\u5e8a\u63a8\u7406\uff0c\u589e\u5f3a\u9884\u6d4b\u5206\u6790\u548c\u4e2a\u6027\u5316\u533b\u7597\u3002", "motivation": "\u533b\u7597\u884c\u4e1a\u5411\u60a3\u8005\u4e2d\u5fc3\u5316\u8f6c\u578b\uff0c\u9700\u8981\u5148\u8fdb\u65b9\u6cd5\u7ba1\u7406\u60a3\u8005\u6570\u636e\uff0c\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u548c\u533b\u7597\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u672c\u4f53\u8bba\u6784\u5efaPJO\uff0c\u6574\u5408\u75c5\u53f2\u3001\u8bca\u65ad\u3001\u6cbb\u7597\u7b49\u6570\u636e\uff0c\u6355\u6349\u65f6\u95f4\u3001\u5e8f\u5217\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u652f\u6301\u9884\u6d4b\u5206\u6790\u3002", "result": "\u4e13\u5bb6\u8bc4\u4f30\u663e\u793aPJO\u5728\u60a3\u8005\u5386\u53f2\u68c0\u7d22\u3001\u75c7\u72b6\u8ddf\u8e2a\u548c\u63d0\u4f9b\u8005\u4ea4\u4e92\u8868\u793a\u65b9\u9762\u6027\u80fd\u4f18\u8d8a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "PJO\u4e3a\u77e5\u8bc6\u8868\u793a\u548c\u751f\u6210AI\u5728\u533b\u7597\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u63d0\u5347\u60a3\u8005\u7ed3\u679c\u548c\u533b\u7597\u6548\u7387\u3002"}}
{"id": "2506.17793", "pdf": "https://arxiv.org/pdf/2506.17793", "abs": "https://arxiv.org/abs/2506.17793", "authors": ["Anton Melnychuk", "Bryan SebaRaj"], "title": "Implementation and Evaluation of Fast Raft for Hierarchical Consensus", "categories": ["cs.DC", "68M14", "D.4.5; C.2.4; C.4"], "comment": "8 pages, 1 figure. Undergraduate research project (Yale University).\n  Implementation available at: https://github.com/anton-mel/FastRaft", "summary": "We present the first open-source implementation and evaluation of Fast Raft,\na hierarchical consensus protocol designed for dynamic, distributed\nenvironments. Fast Raft reduces the number of message rounds needed to commit\nlog entries compared to standard Raft by introducing a fast-track mechanism and\nreducing leader dependence. Our implementation uses gRPC and Kubernetes-based\ndeployment across AWS availability zones. Experimental results demonstrate a\nthroughput improvement and reduced commit latency under low packet loss\nconditions, while maintaining Raft's safety and liveness guarantees.", "AI": {"tldr": "Fast Raft \u662f\u4e00\u79cd\u5206\u5c42\u5171\u8bc6\u534f\u8bae\uff0c\u51cf\u5c11\u63d0\u4ea4\u65e5\u5fd7\u6761\u76ee\u6240\u9700\u7684\u6d88\u606f\u8f6e\u6b21\uff0c\u901a\u8fc7\u5feb\u901f\u901a\u9053\u673a\u5236\u548c\u51cf\u5c11\u5bf9 leader \u7684\u4f9d\u8d56\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u4f20\u7edf Raft \u534f\u8bae\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u5347\u5171\u8bc6\u6548\u7387\u3002", "method": "\u5f15\u5165\u5feb\u901f\u901a\u9053\u673a\u5236\uff0c\u51cf\u5c11 leader \u4f9d\u8d56\uff0c\u4f7f\u7528 gRPC \u548c Kubernetes \u5728 AWS \u591a\u53ef\u7528\u533a\u90e8\u7f72\u3002", "result": "\u5728\u4f4e\u4e22\u5305\u6761\u4ef6\u4e0b\uff0c\u541e\u5410\u91cf\u63d0\u5347\u4e14\u63d0\u4ea4\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301 Raft \u7684\u5b89\u5168\u6027\u548c\u6d3b\u6027\u3002", "conclusion": "Fast Raft \u5728\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u5171\u8bc6\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2506.17812", "pdf": "https://arxiv.org/pdf/2506.17812", "abs": "https://arxiv.org/abs/2506.17812", "authors": ["Noble Saji Mathews", "Meiyappan Nagappan"], "title": "Is Your Automated Software Engineer Trustworthy?", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used in software\nengineering tasks, with an increased focus on bug report resolution over the\npast year. However, most proposed systems fail to properly handle uncertain or\nincorrect inputs and outputs. Existing LLM-based tools and coding agents\nrespond to every issue and generate a patch for every case, even when the input\nis vague or their own output is incorrect. There are no mechanisms in place to\nabstain when confidence is low. This leads to unreliable behaviour, such as\nhallucinated code changes or responses based on vague issue reports. We\nintroduce BouncerBench, a benchmark that evaluates whether LLM-based software\nagents can refuse to act when inputs are ill-defined or refuse to respond when\ntheir own outputs are likely to be incorrect. Unlike prior benchmarks that\nimplicitly incentivize models to generate responses even when uncertain,\nBouncerBench aims to improve precision by targeting two overlooked failure\npoints: (1) vague or underspecified issue descriptions in tickets and (2)\nlogically or functionally incorrect code patches created by the system. It\nmeasures whether proposed systems can distinguish actionable issues from vague\ntickets and valid patches from untrustworthy ones. We also implement a basic\ninput and output bouncer, evaluating how well current LLMs can abstain when\nneeded. Our results show that most models fail to abstain from underspecified\ninputs or incorrect outputs. Hence, we conclude that there is significant room\nfor improvement before LLMs can be trusted to make correct decisions and\nrecommendations in real-world software engineering workflows. BouncerBench\nprovides a first step toward evaluating and building more cautious, trustworthy\ncode agents. The replication package, dataset, and leaderboard can be found at\nbouncerbench.com", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aBouncerBench\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u8f6f\u4ef6\u4ee3\u7406\u662f\u5426\u80fd\u62d2\u7edd\u5904\u7406\u6a21\u7cca\u8f93\u5165\u6216\u9519\u8bef\u8f93\u51fa\uff0c\u4ee5\u63d0\u9ad8\u7cbe\u786e\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5de5\u5177\u5728\u5904\u7406\u6a21\u7cca\u6216\u9519\u8bef\u8f93\u5165\u65f6\u7f3a\u4e4f\u62d2\u7edd\u673a\u5236\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u884c\u4e3a\uff0c\u5982\u751f\u6210\u9519\u8bef\u4ee3\u7801\u3002", "method": "\u5f15\u5165BouncerBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cd\u70b9\u5173\u6ce8\u6a21\u7cca\u95ee\u9898\u548c\u9519\u8bef\u8865\u4e01\uff0c\u5e76\u5b9e\u73b0\u8f93\u5165\u8f93\u51fa\u7b5b\u9009\u5668\u3002", "result": "\u7ed3\u679c\u663e\u793a\u5927\u591a\u6570\u6a21\u578b\u65e0\u6cd5\u5728\u9700\u8981\u65f6\u62d2\u7edd\u5904\u7406\uff0c\u8868\u660eLLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "BouncerBench\u4e3a\u8bc4\u4f30\u548c\u6784\u5efa\u66f4\u53ef\u9760\u7684\u4ee3\u7801\u4ee3\u7406\u63d0\u4f9b\u4e86\u521d\u6b65\u5de5\u5177\u3002"}}
{"id": "2506.18317", "pdf": "https://arxiv.org/pdf/2506.18317", "abs": "https://arxiv.org/abs/2506.18317", "authors": ["Emerson Sie", "Enguang Fan", "Federico Cifuentes-Urtubey", "Deepak Vasisht"], "title": "Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi Ranging", "categories": ["cs.HC", "cs.NI", "cs.RO"], "comment": null, "summary": "Indoor localization opens the path to potentially transformative\napplications. Although many indoor localization methods have been proposed over\nthe years, they remain too impractical for widespread deployment in the real\nworld. In this paper, we introduce PeepLoc, a deployable and scalable\nWi-Fi-based solution for indoor localization that relies only on pre-existing\ndevices and infrastructure. Specifically, PeepLoc works on any mobile device\nwith an unmodified Wi-Fi transceiver and in any indoor environment with a\nsufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the\ncore of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain\nnon-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel\nbootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and\ncrowdsourcing to opportunistically initialize pre-existing APs as anchor points\nwithin an environment. We implement PeepLoc using commodity hardware and\nevaluate it extensively across 4 campus buildings. We show PeepLoc leads to a\nmean and median positional error of 3.41 m and 3.06 m respectively, which is\nsuperior to existing deployed indoor localization systems and is competitive\nwith commodity GPS in outdoor environments.", "AI": {"tldr": "PeepLoc\u662f\u4e00\u79cd\u57fa\u4e8eWi-Fi\u7684\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u5229\u7528\u73b0\u6709\u8bbe\u5907\u548c\u57fa\u7840\u8bbe\u65bd\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709\u5ba4\u5185\u5b9a\u4f4d\u65b9\u6cd5\u5b9e\u7528\u6027\u4e0d\u8db3\uff0cPeepLoc\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u53ef\u90e8\u7f72\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u975e\u534f\u4f5c\u98de\u884c\u65f6\u95f4\u6d4b\u91cf\u548c\u57fa\u4e8e\u884c\u4eba\u822a\u4f4d\u63a8\u7b97\u7684\u4f17\u5305\u673a\u5236\uff0c\u5229\u7528\u73b0\u6709Wi-Fi\u8bbe\u5907\u5b9e\u73b0\u5b9a\u4f4d\u3002", "result": "\u57284\u5ea7\u6821\u56ed\u5efa\u7b51\u4e2d\u6d4b\u8bd5\uff0c\u5e73\u5747\u548c\u4e2d\u4f4d\u5b9a\u4f4d\u8bef\u5dee\u5206\u522b\u4e3a3.41\u7c73\u548c3.06\u7c73\u3002", "conclusion": "PeepLoc\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u4e14\u53ef\u4e0e\u5ba4\u5916GPS\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2506.18866", "pdf": "https://arxiv.org/pdf/2506.18866", "abs": "https://arxiv.org/abs/2506.18866", "authors": ["Qijun Gan", "Ruizi Yang", "Jianke Zhu", "Shaofei Xue", "Steven Hoi"], "title": "OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Project page: https://omni-avatar.github.io/", "summary": "Significant progress has been made in audio-driven human animation, while\nmost existing methods focus mainly on facial movements, limiting their ability\nto create full-body animations with natural synchronization and fluidity. They\nalso struggle with precise prompt control for fine-grained generation. To\ntackle these challenges, we introduce OmniAvatar, an innovative audio-driven\nfull-body video generation model that enhances human animation with improved\nlip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise\nmulti-hierarchical audio embedding strategy to better capture audio features in\nthe latent space, enhancing lip-syncing across diverse scenes. To preserve the\ncapability for prompt-driven control of foundation models while effectively\nincorporating audio features, we employ a LoRA-based training approach.\nExtensive experiments show that OmniAvatar surpasses existing models in both\nfacial and semi-body video generation, offering precise text-based control for\ncreating videos in various domains, such as podcasts, human interactions,\ndynamic scenes, and singing. Our project page is\nhttps://omni-avatar.github.io/.", "AI": {"tldr": "OmniAvatar\u662f\u4e00\u79cd\u521b\u65b0\u7684\u97f3\u9891\u9a71\u52a8\u5168\u8eab\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c42\u7ea7\u97f3\u9891\u5d4c\u5165\u7b56\u7565\u548cLoRA\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5507\u540c\u6b65\u548c\u81ea\u7136\u52a8\u4f5c\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u4eba\u4f53\u52a8\u753b\u65b9\u6cd5\u591a\u5173\u6ce8\u9762\u90e8\u52a8\u4f5c\uff0c\u96be\u4ee5\u751f\u6210\u81ea\u7136\u540c\u6b65\u7684\u5168\u8eab\u52a8\u753b\uff0c\u4e14\u7f3a\u4e4f\u7cbe\u786e\u63d0\u793a\u63a7\u5236\u3002", "method": "\u91c7\u7528\u50cf\u7d20\u7ea7\u591a\u5c42\u7ea7\u97f3\u9891\u5d4c\u5165\u7b56\u7565\u548cLoRA\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u97f3\u9891\u7279\u5f81\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\u63a7\u5236\u80fd\u529b\u3002", "result": "\u5728\u9762\u90e8\u548c\u534a\u8eab\u4f53\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u591a\u9886\u57df\u7cbe\u786e\u6587\u672c\u63a7\u5236\u3002", "conclusion": "OmniAvatar\u5728\u97f3\u9891\u9a71\u52a8\u5168\u8eab\u52a8\u753b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u51c6\u786e\u6027\u548c\u81ea\u7136\u6027\uff0c\u6269\u5c55\u4e86\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.18867", "pdf": "https://arxiv.org/pdf/2506.18867", "abs": "https://arxiv.org/abs/2506.18867", "authors": ["Yuqi Meng", "Yihao Shi", "Kemeng Huang", "Ning Guo", "Taku Komura", "Yin Yang", "Minchen Li"], "title": "A B-Spline Finite Element Method for Cloth Simulation", "categories": ["cs.GR"], "comment": "19 pages, 18 figures", "summary": "We present a B-spline finite element method (FEM) for cloth simulation.\nBuilding on quadratic B-spline basis functions, our method provides a globally\n$C^1$-continuous displacement field, enabling consistent and accurate\ndiscretization of both membrane and bending energies. This smooth\nrepresentation effectively mitigates locking artifacts and mesh dependency\nissues commonly observed with linear FEM. To further improve efficiency, we\ndevelop a reduced integration scheme that separately optimizes quadrature rules\nfor membrane and bending energies, further reducing computational overhead\nwhile maintaining accuracy. We validate our approach through extensive\nexperiments, demonstrating improved accuracy, visual quality, and efficiency\ncompared to linear FEM and recent higher-order methods. Our method enables\nrealistic simulation of complex wrinkling dynamics across varying material\nparameters, offering a promising new spatial discretization for cloth\nsimulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eB\u6837\u6761\u7684\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u7528\u4e8e\u5e03\u6599\u6a21\u62df\uff0c\u89e3\u51b3\u4e86\u7ebf\u6027FEM\u4e2d\u5e38\u89c1\u7684\u9501\u5b9a\u4f2a\u5f71\u548c\u7f51\u683c\u4f9d\u8d56\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u7ebf\u6027FEM\u5728\u5e03\u6599\u6a21\u62df\u4e2d\u5b58\u5728\u9501\u5b9a\u4f2a\u5f71\u548c\u7f51\u683c\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5e73\u6ed1\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u6a21\u62df\u6548\u679c\u3002", "method": "\u4f7f\u7528\u4e8c\u6b21B\u6837\u6761\u57fa\u51fd\u6570\u6784\u5efa\u5168\u5c40C1\u8fde\u7eed\u4f4d\u79fb\u573a\uff0c\u5206\u522b\u4f18\u5316\u819c\u548c\u5f2f\u66f2\u80fd\u91cf\u7684\u79ef\u5206\u89c4\u5219\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u89c6\u89c9\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u7ebf\u6027FEM\u548c\u8fd1\u671f\u9ad8\u9636\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e03\u6599\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4\u79bb\u6563\u5316\u65b9\u6848\uff0c\u80fd\u591f\u771f\u5b9e\u6a21\u62df\u590d\u6742\u8936\u76b1\u52a8\u529b\u5b66\u3002"}}
{"id": "2506.18842", "pdf": "https://arxiv.org/pdf/2506.18842", "abs": "https://arxiv.org/abs/2506.18842", "authors": ["Patrick Beukema", "Henry Herzog", "Yawen Zhang", "Hunter Pitelka", "Favyen Bastani"], "title": "LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth", "categories": ["cs.DB", "cs.CV", "cs.LG"], "comment": "8 pages, 7 figures, 1 table, ICML 2025 ML4RS", "summary": "We introduce a new dataset and algorithm for fast and efficient coastal\ndistance calculations from Anywhere on Earth (AoE). Existing global coastal\ndatasets are only available at coarse resolution (e.g. 1-4 km) which limits\ntheir utility. Publicly available satellite imagery combined with computer\nvision enable much higher precision. We provide a global coastline dataset at\n10 meter resolution, a 100+ fold improvement in precision over existing data.\nTo handle the computational challenge of querying at such an increased scale,\nwe introduce a new library: Layered Iterative Geospatial Hierarchical\nTerrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both\nexceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM\nto achieve millisecond online inference, making it well suited for real-time\napplications in resource-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\u548c\u7b97\u6cd5\uff0c\u7528\u4e8e\u5168\u7403\u8303\u56f4\u5185\u7684\u9ad8\u7cbe\u5ea6\u6d77\u5cb8\u8ddd\u79bb\u8ba1\u7b97\uff0c\u5206\u8fa8\u7387\u63d0\u5347100\u500d\u4ee5\u4e0a\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u8ba1\u7b97\u5e93Lighthouse\u3002", "motivation": "\u73b0\u6709\u5168\u7403\u6d77\u5cb8\u6570\u636e\u96c6\u5206\u8fa8\u7387\u8f83\u4f4e\uff081-4\u516c\u91cc\uff09\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u7ed3\u5408\u516c\u5f00\u536b\u661f\u56fe\u50cf\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u63d0\u4f9b10\u7c73\u5206\u8fa8\u7387\u7684\u5168\u7403\u6d77\u5cb8\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1Lighthouse\u5e93\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "Lighthouse\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u97001 CPU\u548c2GB RAM\u5373\u53ef\u5b9e\u73b0\u6beb\u79d2\u7ea7\u5728\u7ebf\u63a8\u65ad\u3002", "conclusion": "\u65b0\u6570\u636e\u96c6\u548c\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6d77\u5cb8\u8ddd\u79bb\u8ba1\u7b97\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2506.17991", "pdf": "https://arxiv.org/pdf/2506.17991", "abs": "https://arxiv.org/abs/2506.17991", "authors": ["Thien Tran", "Jonathan Kua", "Minh Tran", "Honghao Lyu", "Thuong Hoang", "Jiong Jin"], "title": "CFTel: A Practical Architecture for Robust and Scalable Telerobotics with Cloud-Fog Automation", "categories": ["cs.DC", "cs.RO"], "comment": "6 pages, 1 figure, accepted paper on the 23rd IEEE International\n  Conference on Industrial Informatics (INDIN), July 12-15, 2025, Kunming,\n  China", "summary": "Telerobotics is a key foundation in autonomous Industrial Cyber-Physical\nSystems (ICPS), enabling remote operations across various domains. However,\nconventional cloud-based telerobotics suffers from latency, reliability,\nscalability, and resilience issues, hindering real-time performance in critical\napplications. Cloud-Fog Telerobotics (CFTel) builds on the Cloud-Fog Automation\n(CFA) paradigm to address these limitations by leveraging a distributed\nCloud-Edge-Robotics computing architecture, enabling deterministic\nconnectivity, deterministic connected intelligence, and deterministic networked\ncomputing. This paper synthesizes recent advancements in CFTel, aiming to\nhighlight its role in facilitating scalable, low-latency, autonomous, and\nAI-driven telerobotics. We analyze architectural frameworks and technologies\nthat enable them, including 5G Ultra-Reliable Low-Latency Communication, Edge\nIntelligence, Embodied AI, and Digital Twins. The study demonstrates that CFTel\nhas the potential to enhance real-time control, scalability, and autonomy while\nsupporting service-oriented solutions. We also discuss practical challenges,\nincluding latency constraints, cybersecurity risks, interoperability issues,\nand standardization efforts. This work serves as a foundational reference for\nresearchers, stakeholders, and industry practitioners in future telerobotics\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Cloud-Fog Telerobotics\uff08CFTel\uff09\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u4e91\u57fa\u8fdc\u7a0b\u673a\u5668\u4eba\u6280\u672f\u7684\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u8ba1\u7b97\u67b6\u6784\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\u4e0e\u81ea\u4e3b\u6027\u3002", "motivation": "\u4f20\u7edf\u4e91\u57fa\u8fdc\u7a0b\u673a\u5668\u4eba\u6280\u672f\u5728\u5ef6\u8fdf\u3001\u53ef\u9760\u6027\u548c\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5173\u952e\u5e94\u7528\u4e2d\u3002CFTel\u67b6\u6784\u65e8\u5728\u901a\u8fc7\u5206\u5e03\u5f0f\u8ba1\u7b97\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CFTel\u57fa\u4e8eCloud-Fog Automation\uff08CFA\uff09\u8303\u5f0f\uff0c\u5229\u7528\u5206\u5e03\u5f0fCloud-Edge-Robotics\u8ba1\u7b97\u67b6\u6784\uff0c\u7ed3\u54085G\u3001\u8fb9\u7f18\u667a\u80fd\u3001\u6570\u5b57\u5b6a\u751f\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u786e\u5b9a\u6027\u8fde\u63a5\u548c\u8ba1\u7b97\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cCFTel\u80fd\u591f\u63d0\u5347\u5b9e\u65f6\u63a7\u5236\u3001\u6269\u5c55\u6027\u548c\u81ea\u4e3b\u6027\uff0c\u5e76\u652f\u6301\u9762\u5411\u670d\u52a1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "CFTel\u4e3a\u672a\u6765\u8fdc\u7a0b\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u53c2\u8003\uff0c\u540c\u65f6\u4e5f\u9700\u89e3\u51b3\u5ef6\u8fdf\u3001\u7f51\u7edc\u5b89\u5168\u548c\u6807\u51c6\u5316\u7b49\u5b9e\u8df5\u6311\u6218\u3002"}}
{"id": "2506.17833", "pdf": "https://arxiv.org/pdf/2506.17833", "abs": "https://arxiv.org/abs/2506.17833", "authors": ["Giorgio Amasanti", "Jasmin Jahic"], "title": "The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study", "categories": ["cs.SE"], "comment": "Accepted for presentation at the International Workshop on\n  AI-Assisted Software Architecting (AISA 2025), colocated with the 19th\n  European Conference on Software Architecture (ECSA 2025), to be held 15-19\n  September 2025 in Limassol, Cyprus", "summary": "AI-powered software tools are widely used to assist software engineers.\nHowever, there is still a need to understand the productivity benefits of such\ntools for software engineers. In addition to short-term benefits, there is a\nquestion of how adopting AI-generated solutions affects the quality of software\nover time (e.g., maintainability and extendability).\n  To provide some insight on these questions, we conducted a survey among\nsoftware practitioners who use AI tools. Based on the data collected from our\nsurvey, we conclude that AI tools significantly increase the productivity of\nsoftware engineers. However, the productivity benefits of using AI tools reduce\nas projects become more complex. The results also show that there are no\nsignificant negative influences of adopting AI-generated solutions on software\nquality, as long as those solutions are limited to smaller code snippets.\nHowever, when solving larger and more complex problems, AI tools generate\nsolutions of a lower quality, indicating the need for architects to perform\nproblem decomposition and solution integration.", "AI": {"tldr": "AI\u5de5\u5177\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u751f\u4ea7\u529b\uff0c\u4f46\u5bf9\u9879\u76ee\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u6548\u679c\u51cf\u5f31\uff1b\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u65e0\u663e\u8457\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u9700\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u7814\u7a76AI\u5de5\u5177\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u751f\u4ea7\u529b\u548c\u8f6f\u4ef6\u8d28\u91cf\u7684\u957f\u671f\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9\u4f7f\u7528AI\u5de5\u5177\u7684\u8f6f\u4ef6\u4ece\u4e1a\u8005\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\u3002", "result": "AI\u5de5\u5177\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u4f46\u5bf9\u590d\u6742\u9879\u76ee\u6548\u679c\u964d\u4f4e\uff1bAI\u751f\u6210\u7684\u4ee3\u7801\u7247\u6bb5\u5bf9\u8d28\u91cf\u65e0\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u590d\u6742\u95ee\u9898\u9700\u4eba\u5de5\u5206\u89e3\u548c\u96c6\u6210\u3002", "conclusion": "AI\u5de5\u5177\u5728\u63d0\u5347\u751f\u4ea7\u529b\u548c\u4ee3\u7801\u8d28\u91cf\u65b9\u9762\u6709\u6548\uff0c\u4f46\u590d\u6742\u573a\u666f\u4ecd\u9700\u4eba\u5de5\u5e72\u9884\u3002"}}
{"id": "2506.18455", "pdf": "https://arxiv.org/pdf/2506.18455", "abs": "https://arxiv.org/abs/2506.18455", "authors": ["Nan Cao", "Xiaoyu Qi", "Chuer Chen", "Xiaoke Yan"], "title": "CODS : A Theoretical Model for Computational Design Based on Design Space", "categories": ["cs.HC"], "comment": null, "summary": "We introduce CODS (Computational Optimization in Design Space), a theoretical\nmodel that frames computational design as a constrained optimization problem\nover a structured, multi-dimensional design space. Unlike existing methods that\nrely on handcrafted heuristics or domain-specific rules, CODS provides a\ngeneralizable and interpretable framework that supports diverse design tasks.\nGiven a user requirement and a well-defined design space, CODS automatically\nderives soft and hard constraints using large language models through a\nstructured prompt engineering pipeline. These constraints guide the\noptimization process to generate design solutions that are coherent,\nexpressive, and aligned with user intent. We validate our approach across two\ndomains-visualization design and knitwear generation-demonstrating superior\nperformance in design quality, intent alignment, and user preference compared\nto existing LLM-based methods. CODS offers a unified foundation for scalable,\ncontrollable, and AI-powered design automation.", "AI": {"tldr": "CODS\u662f\u4e00\u4e2a\u7406\u8bba\u6a21\u578b\uff0c\u5c06\u8ba1\u7b97\u8bbe\u8ba1\u89c6\u4e3a\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7LLM\u81ea\u52a8\u751f\u6210\u7ea6\u675f\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bbe\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u542f\u53d1\u5f0f\u6216\u9886\u57df\u7279\u5b9a\u89c4\u5219\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002CODS\u65e8\u5728\u63d0\u4f9b\u7edf\u4e00\u7684AI\u8bbe\u8ba1\u81ea\u52a8\u5316\u6846\u67b6\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u81ea\u52a8\u751f\u6210\u8f6f\u786c\u7ea6\u675f\uff0c\u6307\u5bfc\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\u7684\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u53ef\u89c6\u5316\u8bbe\u8ba1\u548c\u9488\u7ec7\u54c1\u751f\u6210\u9886\u57df\uff0cCODS\u5728\u8bbe\u8ba1\u8d28\u91cf\u3001\u610f\u56fe\u5bf9\u9f50\u548c\u7528\u6237\u504f\u597d\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709LLM\u65b9\u6cd5\u3002", "conclusion": "CODS\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u7684AI\u8bbe\u8ba1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u7840\u3002"}}
{"id": "2506.18881", "pdf": "https://arxiv.org/pdf/2506.18881", "abs": "https://arxiv.org/abs/2506.18881", "authors": ["Xinyu Zhang", "Dong Gong", "Zicheng Duan", "Anton van den Hengel", "Lingqiao Liu"], "title": "Let Your Video Listen to Your Music!", "categories": ["cs.CV", "cs.MM"], "comment": "project page: https://zhangxinyu-xyz.github.io/MVAA/", "summary": "Aligning the rhythm of visual motion in a video with a given music track is a\npractical need in multimedia production, yet remains an underexplored task in\nautonomous video editing. Effective alignment between motion and musical beats\nenhances viewer engagement and visual appeal, particularly in music videos,\npromotional content, and cinematic editing. Existing methods typically depend\non labor-intensive manual cutting, speed adjustments, or heuristic-based\nediting techniques to achieve synchronization. While some generative models\nhandle joint video and music generation, they often entangle the two\nmodalities, limiting flexibility in aligning video to music beats while\npreserving the full visual content. In this paper, we propose a novel and\nefficient framework, termed MVAA (Music-Video Auto-Alignment), that\nautomatically edits video to align with the rhythm of a given music track while\npreserving the original visual content. To enhance flexibility, we modularize\nthe task into a two-step process in our MVAA: aligning motion keyframes with\naudio beats, followed by rhythm-aware video inpainting. Specifically, we first\ninsert keyframes at timestamps aligned with musical beats, then use a\nframe-conditioned diffusion model to generate coherent intermediate frames,\npreserving the original video's semantic content. Since comprehensive test-time\ntraining can be time-consuming, we adopt a two-stage strategy: pretraining the\ninpainting module on a small video set to learn general motion priors, followed\nby rapid inference-time fine-tuning for video-specific adaptation. This hybrid\napproach enables adaptation within 10 minutes with one epoch on a single NVIDIA\n4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show\nthat our approach can achieve high-quality beat alignment and visual\nsmoothness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMVAA\u7684\u6846\u67b6\uff0c\u80fd\u81ea\u52a8\u5c06\u89c6\u9891\u5185\u5bb9\u4e0e\u97f3\u4e50\u8282\u62cd\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u539f\u89c6\u9891\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u901a\u8fc7\u5173\u952e\u5e27\u5bf9\u9f50\u548c\u8282\u594f\u611f\u77e5\u7684\u89c6\u9891\u4fee\u590d\u5b9e\u73b0\u9ad8\u6548\u7f16\u8f91\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u4e0e\u97f3\u4e50\u8282\u594f\u5bf9\u9f50\u7684\u5b9e\u9645\u9700\u6c42\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6216\u542f\u53d1\u5f0f\u6280\u672f\uff0c\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u5e0c\u671b\u81ea\u52a8\u5316\u5e76\u4fdd\u7559\u539f\u89c6\u9891\u5185\u5bb9\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u5173\u952e\u5e27\u4e0e\u97f3\u4e50\u8282\u62cd\u5bf9\u9f50\uff1b2) \u57fa\u4e8e\u5e27\u6761\u4ef6\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u4e2d\u95f4\u5e27\uff0c\u4fdd\u6301\u8bed\u4e49\u5185\u5bb9\u3002\u9884\u8bad\u7ec3\u540e\u5feb\u901f\u5fae\u8c03\u4ee5\u9002\u5e94\u7279\u5b9a\u89c6\u9891\u3002", "result": "\u5728\u5355\u5757NVIDIA 4090 GPU\u4e0a10\u5206\u949f\u5185\u5b8c\u6210\u9002\u914d\uff0c\u5b9e\u9a8c\u663e\u793a\u80fd\u9ad8\u8d28\u91cf\u5bf9\u9f50\u8282\u62cd\u5e76\u4fdd\u6301\u89c6\u89c9\u6d41\u7545\u3002", "conclusion": "MVAA\u6846\u67b6\u9ad8\u6548\u7075\u6d3b\uff0c\u4e3a\u591a\u5a92\u4f53\u5236\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.18584", "pdf": "https://arxiv.org/pdf/2506.18584", "abs": "https://arxiv.org/abs/2506.18584", "authors": ["Francesco Malandrino", "Olga Chukhno", "Alessandro Catania", "Antonella Molinaro", "Carla Fabiana Chiasserini"], "title": "XR Offloading Across Multiple Time Scales: The Roles of Power, Temperature, and Energy", "categories": ["cs.NI"], "comment": null, "summary": "Extended reality (XR) devices, commonly known as wearables, must handle\nsignificant computational loads under tight latency constraints. To meet these\ndemands, they rely on a combination of on-device processing and edge\noffloading. This letter focuses on offloading strategies for wearables by\nconsidering their impact across three time scales: instantaneous power\nconsumption, short-term temperature fluctuations, and long-term battery\nduration. We introduce a comprehensive system model that captures these\ntemporal dynamics, and propose a stochastic and stationary offloading strategy,\ncalled TAO (for temperature-aware offloading), designed to minimize the\noffloading cost while adhering to power, thermal, and energy constraints. Our\nperformance evaluation, leveraging COMSOL models of real-world wearables,\nconfirms that TAO reduces offloading cost by over 35% compared to\nstate-of-the-art approaches, without violating the wearable operational limits.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTAO\u7684\u6e29\u5ea6\u611f\u77e5\u5378\u8f7d\u7b56\u7565\uff0c\u7528\u4e8e\u4f18\u5316XR\u8bbe\u5907\u7684\u8ba1\u7b97\u4efb\u52a1\u5206\u914d\uff0c\u8003\u8651\u4e86\u77ac\u65f6\u529f\u8017\u3001\u77ed\u671f\u6e29\u5ea6\u6ce2\u52a8\u548c\u957f\u671f\u7535\u6c60\u5bff\u547d\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5378\u8f7d\u6210\u672c\u3002", "motivation": "XR\u8bbe\u5907\uff08\u5982\u53ef\u7a7f\u6234\u8bbe\u5907\uff09\u9700\u8981\u5728\u4e25\u683c\u7684\u5ef6\u8fdf\u9650\u5236\u4e0b\u5904\u7406\u5927\u91cf\u8ba1\u7b97\u4efb\u52a1\uff0c\u800c\u73b0\u6709\u5378\u8f7d\u7b56\u7565\u672a\u80fd\u5168\u9762\u8003\u8651\u4e0d\u540c\u65f6\u95f4\u5c3a\u5ea6\u7684\u5f71\u54cd\u3002", "method": "\u4f5c\u8005\u5f15\u5165\u4e86\u4e00\u4e2a\u7efc\u5408\u7cfb\u7edf\u6a21\u578b\uff0c\u6355\u6349\u4e86\u529f\u8017\u3001\u6e29\u5ea6\u548c\u80fd\u91cf\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u4e14\u9759\u6001\u7684\u5378\u8f7d\u7b56\u7565TAO\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cTAO\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u51cf\u5c11\u8d85\u8fc735%\u7684\u5378\u8f7d\u6210\u672c\uff0c\u540c\u65f6\u4e0d\u8fdd\u53cd\u8bbe\u5907\u8fd0\u884c\u9650\u5236\u3002", "conclusion": "TAO\u901a\u8fc7\u5168\u9762\u8003\u8651\u65f6\u95f4\u5c3a\u5ea6\u7684\u5f71\u54cd\uff0c\u4e3aXR\u8bbe\u5907\u7684\u4efb\u52a1\u5378\u8f7d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18671", "pdf": "https://arxiv.org/pdf/2506.18671", "abs": "https://arxiv.org/abs/2506.18671", "authors": ["Yuqin Dai", "Wanlu Zhu", "Ronghui Li", "Xiu Li", "Zhenyu Zhang", "Jun Li", "Jian Yang"], "title": "TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "comment": null, "summary": "Music-driven dance generation has garnered significant attention due to its\nwide range of industrial applications, particularly in the creation of group\nchoreography. During the group dance generation process, however, most existing\nmethods still face three primary issues: multi-dancer collisions, single-dancer\nfoot sliding and abrupt swapping in the generation of long group dance. In this\npaper, we propose TCDiff++, a music-driven end-to-end framework designed to\ngenerate harmonious group dance. Specifically, to mitigate multi-dancer\ncollisions, we utilize a dancer positioning embedding to better maintain the\nrelative positioning among dancers. Additionally, we incorporate a\ndistance-consistency loss to ensure that inter-dancer distances remain within\nplausible ranges. To address the issue of single-dancer foot sliding, we\nintroduce a swap mode embedding to indicate dancer swapping patterns and design\na Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For\nlong group dance generation, we present a long group diffusion sampling\nstrategy that reduces abrupt position shifts by injecting positional\ninformation into the noisy input. Furthermore, we integrate a Sequence Decoder\nlayer to enhance the model's ability to selectively process long sequences.\nExtensive experiments demonstrate that our TCDiff++ achieves state-of-the-art\nperformance, particularly in long-duration scenarios, ensuring high-quality and\ncoherent group dance generation.", "AI": {"tldr": "TCDiff++\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u97f3\u4e50\u9a71\u52a8\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u7fa4\u4f53\u821e\u8e48\u751f\u6210\u4e2d\u7684\u78b0\u649e\u3001\u811a\u6ed1\u52a8\u548c\u4f4d\u7f6e\u7a81\u53d8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7fa4\u4f53\u821e\u8e48\u751f\u6210\u4e2d\u5b58\u5728\u591a\u821e\u8005\u78b0\u649e\u3001\u5355\u821e\u8005\u811a\u6ed1\u52a8\u4ee5\u53ca\u957f\u5e8f\u5217\u751f\u6210\u65f6\u7684\u4f4d\u7f6e\u7a81\u7136\u4ea4\u6362\u95ee\u9898\u3002", "method": "\u63d0\u51faTCDiff++\u6846\u67b6\uff0c\u5305\u62ec\u821e\u8005\u5b9a\u4f4d\u5d4c\u5165\u3001\u8ddd\u79bb\u4e00\u81f4\u6027\u635f\u5931\u3001\u4ea4\u6362\u6a21\u5f0f\u5d4c\u5165\u3001\u811a\u6b65\u9002\u914d\u5668\u548c\u957f\u5e8f\u5217\u6269\u6563\u91c7\u6837\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTCDiff++\u5728\u957f\u65f6\u95f4\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u8fde\u8d2f\u7684\u7fa4\u4f53\u821e\u8e48\u3002", "conclusion": "TCDiff++\u901a\u8fc7\u591a\u6a21\u5757\u534f\u540c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7fa4\u4f53\u821e\u8e48\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u884c\u4e1a\u9886\u5148\u6548\u679c\u3002"}}
{"id": "2506.18024", "pdf": "https://arxiv.org/pdf/2506.18024", "abs": "https://arxiv.org/abs/2506.18024", "authors": ["Thien Tran", "Quang Nguyen", "Jonathan Kua", "Minh Tran", "Toan Luu", "Thuong Hoang", "Jiong Jin"], "title": "Leveraging Cloud-Fog Automation for Autonomous Collision Detection and Classification in Intelligent Unmanned Surface Vehicles", "categories": ["cs.DC", "cs.RO"], "comment": "6 pages, 5 figures, accepted paper on the 23rd IEEE International\n  Conference on Industrial Informatics (INDIN), July 12-15, 2025, Kunming,\n  China", "summary": "Industrial Cyber-Physical Systems (ICPS) technologies are foundational in\ndriving maritime autonomy, particularly for Unmanned Surface Vehicles (USVs).\nHowever, onboard computational constraints and communication latency\nsignificantly restrict real-time data processing, analysis, and predictive\nmodeling, hence limiting the scalability and responsiveness of maritime ICPS.\nTo overcome these challenges, we propose a distributed Cloud-Edge-IoT\narchitecture tailored for maritime ICPS by leveraging design principles from\nthe recently proposed Cloud-Fog Automation paradigm. Our proposed architecture\ncomprises three hierarchical layers: a Cloud Layer for centralized and\ndecentralized data aggregation, advanced analytics, and future model\nrefinement; an Edge Layer that executes localized AI-driven processing and\ndecision-making; and an IoT Layer responsible for low-latency sensor data\nacquisition. Our experimental results demonstrated improvements in\ncomputational efficiency, responsiveness, and scalability. When compared with\nour conventional approaches, we achieved a classification accuracy of 86\\%,\nwith an improved latency performance. By adopting Cloud-Fog Automation, we\naddress the low-latency processing constraints and scalability challenges in\nmaritime ICPS applications. Our work offers a practical, modular, and scalable\nframework to advance robust autonomy and AI-driven decision-making and autonomy\nfor intelligent USVs in future maritime ICPS.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6d77\u4e0a\u5de5\u4e1a\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\uff08ICPS\uff09\u7684\u5206\u5e03\u5f0f\u4e91-\u8fb9\u7f18-\u7269\u8054\u7f51\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5c42\u5904\u7406\u89e3\u51b3\u4e86\u8ba1\u7b97\u548c\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u6570\u636e\u5904\u7406\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u6d77\u4e0a\u65e0\u4eba\u8239\uff08USVs\uff09\u7684\u5de5\u4e1a\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\uff08ICPS\uff09\u56e0\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u548c\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u6570\u636e\u5904\u7406\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u57fa\u4e8eCloud-Fog Automation\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u5c42\u5206\u5e03\u5f0f\u67b6\u6784\uff08\u4e91\u5c42\u3001\u8fb9\u7f18\u5c42\u548c\u7269\u8054\u7f51\u5c42\uff09\uff0c\u5206\u522b\u8d1f\u8d23\u6570\u636e\u805a\u5408\u3001\u672c\u5730\u5316AI\u5904\u7406\u4ee5\u53ca\u4f4e\u5ef6\u8fdf\u6570\u636e\u91c7\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728\u8ba1\u7b97\u6548\u7387\u3001\u54cd\u5e94\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523086%\uff0c\u5e76\u6539\u5584\u4e86\u5ef6\u8fdf\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6d77\u4e0aICPS\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u3001\u6a21\u5757\u5316\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u667a\u80fdUSV\u7684\u81ea\u4e3b\u6027\u548cAI\u51b3\u7b56\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.17937", "pdf": "https://arxiv.org/pdf/2506.17937", "abs": "https://arxiv.org/abs/2506.17937", "authors": ["Tommi Mikkonen", "Antero Taivalsaari"], "title": "Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Software development is currently under a paradigm shift in which artificial\nintelligence and generative software reuse are taking the center stage in\nsoftware creation. Consequently, earlier software reuse practices and methods\nare rapidly being replaced by AI-assisted approaches in which developers place\ntheir trust on code that has been generated by artificial intelligence. This is\nleading to a new form of software reuse that is conceptually not all that\ndifferent from cargo cult development. In this paper we discuss the\nimplications of AI-assisted generative software reuse in the context of\nemerging \"AI native\" software engineering, bring forth relevant questions, and\ndefine a tentative research agenda and call to action for tackling some of the\ncentral issues associated with this approach.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u8f85\u52a9\u751f\u6210\u8f6f\u4ef6\u590d\u7528\u5728\u2018AI\u539f\u751f\u2019\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u76f8\u5173\u95ee\u9898\uff0c\u5e76\u5b9a\u4e49\u4e86\u7814\u7a76\u8bae\u7a0b\u548c\u884c\u52a8\u547c\u5401\uff0c\u4ee5\u89e3\u51b3\u76f8\u5173\u7684\u6838\u5fc3\u95ee\u9898\u3002", "motivation": "\u968f\u7740AI\u548c\u751f\u6210\u5f0f\u8f6f\u4ef6\u590d\u7528\u6210\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u7684\u6838\u5fc3\uff0c\u4f20\u7edf\u590d\u7528\u65b9\u6cd5\u88abAI\u8f85\u52a9\u65b9\u6cd5\u53d6\u4ee3\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u8ba8\u8fd9\u79cd\u8f6c\u53d8\u7684\u6f5c\u5728\u95ee\u9898\u53ca\u5176\u5728\u2018AI\u539f\u751f\u2019\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790AI\u8f85\u52a9\u751f\u6210\u8f6f\u4ef6\u590d\u7528\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u76f8\u5173\u95ee\u9898\uff0c\u5e76\u5b9a\u4e49\u7814\u7a76\u8bae\u7a0b\u548c\u884c\u52a8\u547c\u5401\u3002", "result": "\u6307\u51faAI\u8f85\u52a9\u8f6f\u4ef6\u590d\u7528\u53ef\u80fd\u5bfc\u81f4\u7c7b\u4f3c\u2018\u8d27\u7269\u5d07\u62dc\u5f00\u53d1\u2019\u7684\u95ee\u9898\uff0c\u5e76\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "conclusion": "AI\u8f85\u52a9\u751f\u6210\u8f6f\u4ef6\u590d\u7528\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u9700\u8981\u660e\u786e\u7814\u7a76\u65b9\u5411\u548c\u884c\u52a8\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002"}}
{"id": "2506.18605", "pdf": "https://arxiv.org/pdf/2506.18605", "abs": "https://arxiv.org/abs/2506.18605", "authors": ["Ronald Cumbal", "Didem Gurdur Broo", "Ginevra Castellano"], "title": "Crowdsourcing eHMI Designs: A Participatory Approach to Autonomous Vehicle-Pedestrian Communication", "categories": ["cs.HC"], "comment": "Paper has been accepted by the 2025 34th IEEE International\n  Conference on Robot and Human Interactive Communication (ROMAN). IEEE\n  copyright process completed", "summary": "As autonomous vehicles become more integrated into shared human environments,\neffective communication with road users is essential for ensuring safety. While\nprevious research has focused on developing external Human-Machine Interfaces\n(eHMIs) to facilitate these interactions, we argue that involving users in the\nearly creative stages can help address key challenges in the development of\nthis technology. To explore this, our study adopts a participatory,\ncrowd-sourced approach to gather user-generated ideas for eHMI designs.\nParticipants were first introduced to fundamental eHMI concepts, equipping them\nto sketch their own design ideas in response to scenarios with varying levels\nof perceived risk. An initial pre-study with 29 participants showed that while\nthey actively engaged in the process, there was a need to refine task\nobjectives and encourage deeper reflection. To address these challenges, a\nfollow-up study with 50 participants was conducted. The results revealed a\nstrong preference for autonomous vehicles to communicate their awareness and\nintentions using lights (LEDs and projections), symbols, and text.\nParticipants' sketches prioritized multi-modal communication, directionality,\nand adaptability to enhance clarity, consistently integrating familiar vehicle\nelements to improve intuitiveness.", "AI": {"tldr": "\u901a\u8fc7\u53c2\u4e0e\u5f0f\u548c\u4f17\u5305\u65b9\u6cd5\uff0c\u7814\u7a76\u6536\u96c6\u7528\u6237\u751f\u6210\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5916\u90e8\u4eba\u673a\u754c\u9762\uff08eHMI\uff09\u8bbe\u8ba1\uff0c\u53d1\u73b0\u7528\u6237\u504f\u597d\u706f\u5149\u3001\u7b26\u53f7\u548c\u6587\u5b57\u7b49\u591a\u6a21\u6001\u901a\u4fe1\u65b9\u5f0f\u3002", "motivation": "\u4e3a\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5171\u4eab\u73af\u5883\u4e2d\u7684\u5b89\u5168\uff0c\u9700\u8981\u6709\u6548\u7684\u901a\u4fe1\u65b9\u5f0f\u3002\u7528\u6237\u65e9\u671f\u53c2\u4e0e\u8bbe\u8ba1\u53ef\u89e3\u51b3\u6280\u672f\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u53c2\u4e0e\u5f0f\u3001\u4f17\u5305\u65b9\u6cd5\u8fdb\u884c\u4e24\u9636\u6bb5\u7814\u7a76\uff0c\u9996\u5148\u5f15\u5165eHMI\u6982\u5ff5\uff0c\u6536\u96c6\u7528\u6237\u8bbe\u8ba1\u8349\u56fe\uff1b\u540e\u7eed\u6539\u8fdb\u4efb\u52a1\u76ee\u6807\u5e76\u9f13\u52b1\u6df1\u5165\u53cd\u601d\u3002", "result": "\u7528\u6237\u504f\u597d\u591a\u6a21\u6001\u901a\u4fe1\uff08\u5982\u706f\u5149\u3001\u7b26\u53f7\u3001\u6587\u672c\uff09\uff0c\u5f3a\u8c03\u65b9\u5411\u6027\u548c\u9002\u5e94\u6027\uff0c\u540c\u65f6\u7ed3\u5408\u719f\u6089\u5143\u7d20\u63d0\u5347\u76f4\u89c2\u6027\u3002", "conclusion": "\u7528\u6237\u65e9\u671f\u53c2\u4e0e\u8bbe\u8ba1\u53ef\u4f18\u5316eHMI\u7684\u529f\u80fd\u4e0e\u76f4\u89c2\u6027\uff0c\u591a\u6a21\u6001\u901a\u4fe1\u65b9\u5f0f\u88ab\u5e7f\u6cdb\u8ba4\u53ef\u4e3a\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u7684\u6709\u6548\u624b\u6bb5\u3002"}}
{"id": "2506.18898", "pdf": "https://arxiv.org/pdf/2506.18898", "abs": "https://arxiv.org/abs/2506.18898", "authors": ["Jiaming Han", "Hao Chen", "Yang Zhao", "Hanyu Wang", "Qi Zhao", "Ziyan Yang", "Hao He", "Xiangyu Yue", "Lu Jiang"], "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "Project page: https://tar.csuhan.com", "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6Tar\uff0c\u901a\u8fc7\u5171\u4eab\u7684\u79bb\u6563\u8bed\u4e49\u8868\u793a\u7edf\u4e00\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\uff0c\u6838\u5fc3\u662f\u6587\u672c\u5bf9\u9f50\u5206\u8bcd\u5668\uff08TA-Tok\uff09\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u8f93\u5165\u8f93\u51fa\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u95f4\u7684\u7edf\u4e00\u8868\u793a\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u4f7f\u7528TA-Tok\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u79bb\u6563\u4ee4\u724c\uff0c\u7ed3\u5408\u6bd4\u4f8b\u81ea\u9002\u5e94\u7f16\u7801\u89e3\u7801\u53ca\u751f\u6210\u53bb\u4ee4\u724c\u5668\uff0c\u5229\u7528\u4e24\u79cd\u4e92\u8865\u7684\u53bb\u4ee4\u724c\u5668\u4f18\u5316\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTar\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "Tar\u6846\u67b6\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u9ad8\u6548\u6027\u548c\u6027\u80fd\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.18660", "pdf": "https://arxiv.org/pdf/2506.18660", "abs": "https://arxiv.org/abs/2506.18660", "authors": ["Xinyi Lin", "Peizheng Li", "Adnan Aijaz"], "title": "RL-Driven Semantic Compression Model Selection and Resource Allocation in Semantic Communication Systems", "categories": ["cs.NI"], "comment": "Accepted by PIMRC 2025", "summary": "Semantic communication (SemCom) is an emerging paradigm that leverages\nsemantic-level understanding to improve communication efficiency, particularly\nin resource-constrained scenarios. However, existing SemCom systems often\noverlook diverse computational and communication capabilities and requirements\namong different users. Motivated by the need to adaptively balance semantic\naccuracy, latency, and energy consumption, this paper presents a reinforcement\nlearning (RL)-driven framework for semantic compression model (SCM) selection\nand resource allocation in multi-user SemCom systems. To address the challenges\nof balancing image reconstruction quality and communication performance, a\nsystem-level optimization metric called Rate-Distortion Efficiency (RDE) has\nbeen defined. The framework considers multiple SCMs with varying complexity and\nresource requirements. A proximal policy optimization (PPO)-based RL approach\nis developed to dynamically select SCMs and allocate bandwidth and power under\nnon-convex constraints. Simulations demonstrate that the proposed method\noutperforms several baseline strategies. This paper also discusses the\ngeneralization ability, computational complexity, scalability, and practical\nimplications of the framework for real-world SemCom systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u7528\u6237\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u538b\u7f29\u6a21\u578b\u9009\u62e9\u548c\u8d44\u6e90\u5206\u914d\uff0c\u901a\u8fc7\u5b9a\u4e49RDE\u6307\u6807\u4f18\u5316\u56fe\u50cf\u91cd\u6784\u8d28\u91cf\u4e0e\u901a\u4fe1\u6027\u80fd\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u5ffd\u89c6\u7528\u6237\u95f4\u8ba1\u7b97\u548c\u901a\u4fe1\u80fd\u529b\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u9700\u5728\u8bed\u4e49\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\u4e4b\u95f4\u81ea\u9002\u5e94\u5e73\u8861\u3002", "method": "\u91c7\u7528PPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u52a8\u6001\u9009\u62e9SCM\u5e76\u5206\u914d\u5e26\u5bbd\u548c\u529f\u7387\uff0c\u5b9a\u4e49RDE\u6307\u6807\u4f5c\u4e3a\u7cfb\u7edf\u7ea7\u4f18\u5316\u6807\u51c6\u3002", "result": "\u4eff\u771f\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u7b56\u7565\uff0c\u5e76\u8ba8\u8bba\u4e86\u6846\u67b6\u7684\u6cdb\u5316\u80fd\u529b\u3001\u590d\u6742\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u9645\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a8\u5e7f\u548c\u5e94\u7528\u3002"}}
{"id": "2506.17613", "pdf": "https://arxiv.org/pdf/2506.17613", "abs": "https://arxiv.org/abs/2506.17613", "authors": ["Ling Li", "Daniel Gibney", "Sharma V. Thankachan", "Solon P. Pissis", "Grigorios Loukides"], "title": "Contextual Pattern Mining and Counting", "categories": ["cs.DS", "cs.DB"], "comment": "27 pages, 15 figures", "summary": "Given a string $P$ of length $m$, a longer string $T$ of length $n>m$, and\ntwo integers $l\\geq 0$ and $r\\geq 0$, the context of $P$ in $T$ is the set of\nall string pairs $(L,R)$, with $|L|=l$ and $|R|=r$, such that the string $LPR$\noccurs in $T$. We introduce two problems related to the notion of context: (1)\nthe Contextual Pattern Mining (CPM) problem, which given $T$, $(m,l,r)$, and an\ninteger $\\tau>0$, asks for outputting the context of each substring $P$ of\nlength $m$ of $T$, provided that the size of the context of $P$ is at least\n$\\tau$; and (2) the Contextual Pattern Counting (CPC) problem, which asks for\npreprocessing $T$ so that the size of the context of a given query string $P$\nof length $m$ can be found efficiently.\n  For CPM, we propose a linear-work algorithm that either uses only internal\nmemory, or a bounded amount of internal memory and external memory, which\nallows much larger datasets to be handled. For CPC, we propose an\n$\\widetilde{\\mathcal{O}}(n)$-space index that can be constructed in\n$\\widetilde{\\mathcal{O}}n)$ time and answers queries in\n$\\mathcal{O}(m)+\\widetilde{\\mathcal{O}}(1)$ time. We further improve the\npractical performance of the CPC index by optimizations that exploit the LZ77\nfactorization of $T$ and an upper bound on the query length. Using\nbillion-letter datasets from different domains, we show that the external\nmemory version of our CPM algorithm can deal with very large datasets using a\nsmall amount of internal memory while its runtime is comparable to that of the\ninternal memory version. Interestingly, we also show that our optimized index\nfor CPC outperforms an approach based on the state of the art for the reporting\nversion of CPC [Navarro, SPIRE 2020] in terms of query time, index size,\nconstruction time, and construction space, often by more than an order of\nmagnitude.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u4e0e\u4e0a\u4e0b\u6587\u6a21\u5f0f\u76f8\u5173\u7684\u95ee\u9898\uff08CPM\u548cCPC\uff09\uff0c\u5e76\u5206\u522b\u7ed9\u51fa\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u548c\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u9ad8\u6548\u5730\u6316\u6398\u548c\u8ba1\u6570\u5b57\u7b26\u4e32\u4e0a\u4e0b\u6587\uff0c\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "CPM\u95ee\u9898\u4f7f\u7528\u7ebf\u6027\u5de5\u4f5c\u7b97\u6cd5\uff0c\u652f\u6301\u5185\u5b58\u548c\u5916\u90e8\u5b58\u50a8\uff1bCPC\u95ee\u9898\u63d0\u51fa\u57fa\u4e8eLZ77\u5206\u89e3\u7684\u7d22\u5f15\u4f18\u5316\u3002", "result": "CPM\u7b97\u6cd5\u80fd\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1bCPC\u7d22\u5f15\u5728\u67e5\u8be2\u65f6\u95f4\u3001\u6784\u9020\u65f6\u95f4\u548c\u7a7a\u95f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u548c\u7d22\u5f15\u4f18\u5316\u5728\u6027\u80fd\u548c\u5b9e\u7528\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u9886\u57df\u7684\u5927\u89c4\u6a21\u6570\u636e\u5904\u7406\u3002"}}
{"id": "2506.18259", "pdf": "https://arxiv.org/pdf/2506.18259", "abs": "https://arxiv.org/abs/2506.18259", "authors": ["Jer Shyuan Ng", "Aditya Pribadi Kalapaaking", "Xiaoyu Xia", "Dusit Niyato", "Ibrahim Khalil", "Iqbal Gondal"], "title": "Edge Association Strategies for Synthetic Data Empowered Hierarchical Federated Learning with Non-IID Data", "categories": ["cs.DC"], "comment": null, "summary": "In recent years, Federated Learning (FL) has emerged as a widely adopted\nprivacy-preserving distributed training approach, attracting significant\ninterest from both academia and industry. Research efforts have been dedicated\nto improving different aspects of FL, such as algorithm improvement, resource\nallocation, and client selection, to enable its deployment in distributed edge\nnetworks for practical applications. One of the reasons for the poor FL model\nperformance is due to the worker dropout during training as the FL server may\nbe located far away from the FL workers. To address this issue, an Hierarchical\nFederated Learning (HFL) framework has been introduced, incorporating an\nadditional layer of edge servers to relay communication between the FL server\nand workers. While the HFL framework improves the communication between the FL\nserver and workers, large number of communication rounds may still be required\nfor model convergence, particularly when FL workers have non-independent and\nidentically distributed (non-IID) data. Moreover, the FL workers are assumed to\nfully cooperate in the FL training process, which may not always be true in\npractical situations. To overcome these challenges, we propose a\nsynthetic-data-empowered HFL framework that mitigates the statistical issues\narising from non-IID local datasets while also incentivizing FL worker\nparticipation. In our proposed framework, the edge servers reward the FL\nworkers in their clusters for facilitating the FL training process. To improve\nthe performance of the FL model given the non-IID local datasets of the FL\nworkers, the edge servers generate and distribute synthetic datasets to FL\nworkers within their clusters. FL workers determine which edge server to\nassociate with, considering the computational resources required to train on\nboth their local datasets and the synthetic datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5408\u6210\u6570\u636e\u7684\u5c42\u6b21\u8054\u90a6\u5b66\u4e60\uff08HFL\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u548c\u53c2\u4e0e\u8005\u6fc0\u52b1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5c42\u6b21\u8054\u90a6\u5b66\u4e60\uff08HFL\uff09\u6846\u67b6\u4ecd\u5b58\u5728\u901a\u4fe1\u8f6e\u6b21\u591a\u548c\u53c2\u4e0e\u8005\u5408\u4f5c\u4e0d\u5145\u5206\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u573a\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u8fb9\u7f18\u670d\u52a1\u5668\u751f\u6210\u548c\u5206\u53d1\u5408\u6210\u6570\u636e\uff0c\u6fc0\u52b1\u5de5\u4eba\u53c2\u4e0e\u5e76\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u51cf\u5c11\u4e86\u901a\u4fe1\u8f6e\u6b21\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u4e86\u53c2\u4e0e\u8005\u7684\u6fc0\u52b1\u95ee\u9898\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u8d4b\u80fd\u7684HFL\u6846\u67b6\u6709\u6548\u5e94\u5bf9\u4e86\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u548c\u53c2\u4e0e\u8005\u6fc0\u52b1\u7684\u53cc\u91cd\u6311\u6218\u3002"}}
{"id": "2506.17948", "pdf": "https://arxiv.org/pdf/2506.17948", "abs": "https://arxiv.org/abs/2506.17948", "authors": ["Mahzabin Tamanna", "Yash Chandrani", "Matthew Burrows", "Brandon Wroblewski", "Laurie Williams", "Dominik Wermke"], "title": "Build It Clean: Large-Scale Detection of Code Smells in Build Scripts", "categories": ["cs.SE"], "comment": "12 pages, 5 tables, 2 figures", "summary": "Build scripts are files that automate the process of compiling source code,\nmanaging dependencies, running tests, and packaging software into deployable\nartifacts. These scripts are ubiquitous in modern software development\npipelines for streamlining testing and delivery. While developing build\nscripts, practitioners may inadvertently introduce code smells. Code smells are\nrecurring patterns of poor coding practices that may lead to build failures or\nincrease risk and technical debt. The goal of this study is to aid\npractitioners in avoiding code smells in build scripts through an empirical\nstudy of build scripts and issues on GitHub. We employed a mixed-methods\napproach, combining qualitative and quantitative analysis. We conducted a\nqualitative analysis of 2000 build-script-related GitHub issues. Next, we\ndeveloped a static analysis tool, Sniffer, to identify code smells in 5882\nbuild scripts of Maven, Gradle, CMake, and Make files, collected from 4877\nopen-source GitHub repositories. We identified 13 code smell categories, with a\ntotal of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,\n337 in CMake, and 6160 in Makefiles.\n  Our analysis revealed that Insecure URLs were the most prevalent code smell\nin Maven build scripts, while Hardcoded Paths/URLs were commonly observed in\nboth Gradle and CMake scripts. Wildcard Usage emerged as the most frequent\nsmell in Makefiles. The co-occurrence analysis revealed strong associations\nbetween specific smell pairs of Hardcoded Paths/URLs with Duplicates, and\nInconsistent Dependency Management with Empty or Incomplete Tags, indicating\npotential underlying issues in the build script structure and maintenance\npractices. Based on our findings, we recommend strategies to mitigate the\nexistence of code smells in build scripts to improve the efficiency,\nreliability, and maintainability of software projects.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790GitHub\u4e0a\u7684\u6784\u5efa\u811a\u672c\u548c\u95ee\u9898\uff0c\u8bc6\u522b\u4e8613\u7c7b\u4ee3\u7801\u5f02\u5473\uff0c\u5e76\u5f00\u53d1\u4e86\u9759\u6001\u5206\u6790\u5de5\u5177Sniffer\uff0c\u63d0\u51fa\u4e86\u6539\u5584\u6784\u5efa\u811a\u672c\u7684\u5efa\u8bae\u3002", "motivation": "\u5e2e\u52a9\u5f00\u53d1\u8005\u907f\u514d\u6784\u5efa\u811a\u672c\u4e2d\u7684\u4ee3\u7801\u5f02\u5473\uff0c\u63d0\u9ad8\u8f6f\u4ef6\u9879\u76ee\u7684\u6548\u7387\u3001\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5b9a\u6027\u5206\u67902000\u4e2aGitHub\u95ee\u9898\uff0c\u5b9a\u91cf\u5206\u67905882\u4e2a\u6784\u5efa\u811a\u672c\uff0c\u5f00\u53d1\u5de5\u5177Sniffer\u8bc6\u522b\u5f02\u5473\u3002", "result": "\u53d1\u73b013\u7c7b\u5f02\u5473\u517110895\u6b21\u51fa\u73b0\uff0c\u4e0d\u540c\u6784\u5efa\u5de5\u5177\u4e2d\u5f02\u5473\u5206\u5e03\u4e0d\u540c\uff0c\u67d0\u4e9b\u5f02\u5473\u5bf9\u5f3a\u5173\u8054\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\uff0c\u63d0\u51fa\u4e86\u7b56\u7565\u4ee5\u51cf\u5c11\u6784\u5efa\u811a\u672c\u4e2d\u7684\u5f02\u5473\uff0c\u6539\u5584\u5f00\u53d1\u5b9e\u8df5\u3002"}}
{"id": "2506.18648", "pdf": "https://arxiv.org/pdf/2506.18648", "abs": "https://arxiv.org/abs/2506.18648", "authors": ["Leonie Kallabis", "Timo Bertram", "Florian Rupp"], "title": "Deceptive Game Design? Investigating the Impact of Visual Card Style on Player Perception", "categories": ["cs.HC"], "comment": "8 pages, 7 figures, 1 table. Accepted at the 2025 IEEE Conference on\n  Games (IEEE CoG)", "summary": "The visual style of game elements considerably contributes to the overall\nexperience. Aesthetics influence player appeal, while the abilities of game\npieces define their in-game functionality. In this paper, we investigate how\nthe visual style of collectible cards influences the players' perception of the\ncard's actual strength in the game. Using the popular trading card game Magic:\nThe Gathering, we conduct a single-blind survey study that examines how players\nperceive the strength of AI-generated cards that are shown in two contrasting\nvisual styles: cute and harmless, or heroic and mighty. Our analysis reveals\nthat some participants are influenced by a card's visual appearance when\njudging its in-game strength. Overall, differences in style perception are\nnormally distributed around a neutral center, but individual participants vary\nin both directions: some generally perceive the cute style to be stronger,\nwhereas others believe that the heroic style is better.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6e38\u620f\u5361\u724c\u89c6\u89c9\u98ce\u683c\uff08\u53ef\u7231\u4e0e\u82f1\u96c4\u98ce\uff09\u5bf9\u73a9\u5bb6\u611f\u77e5\u5361\u724c\u5f3a\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u73a9\u5bb6\u5bf9\u6b64\u7684\u611f\u77e5\u5b58\u5728\u4e2a\u4f53\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u89c6\u89c9\u98ce\u683c\u5982\u4f55\u5f71\u54cd\u73a9\u5bb6\u5bf9\u5361\u724c\u5b9e\u9645\u5f3a\u5ea6\u7684\u5224\u65ad\u3002", "method": "\u901a\u8fc7\u5355\u76f2\u8c03\u67e5\uff0c\u4f7f\u7528\u300a\u9b54\u6cd5\u98ce\u4e91\u4f1a\u300b\u7684AI\u751f\u6210\u5361\u724c\uff0c\u5bf9\u6bd4\u53ef\u7231\u4e0e\u82f1\u96c4\u98ce\u683c\u3002", "result": "\u73a9\u5bb6\u5bf9\u98ce\u683c\u5f71\u54cd\u7684\u611f\u77e5\u5448\u6b63\u6001\u5206\u5e03\uff0c\u4f46\u4e2a\u4f53\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u89c6\u89c9\u98ce\u683c\u5bf9\u5361\u724c\u5f3a\u5ea6\u611f\u77e5\u6709\u4e00\u5b9a\u5f71\u54cd\uff0c\u4f46\u6548\u679c\u56e0\u4eba\u800c\u5f02\u3002"}}
{"id": "2506.17977", "pdf": "https://arxiv.org/pdf/2506.17977", "abs": "https://arxiv.org/abs/2506.17977", "authors": ["Tingting Zhu", "Tingyang Chen", "Yinghui Wu", "Arijit Khan", "Xiangyu Ke"], "title": "SliceGX: Layer-wise GNN Explanation with Model-slicing", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "Ensuring the trustworthiness of graph neural networks (GNNs) as black-box\nmodels requires effective explanation methods. Existing GNN explanations\ntypically apply input perturbations to identify subgraphs that are responsible\nfor the occurrence of the final output of GNNs. However, such approaches lack\nfiner-grained, layer-wise analysis of how intermediate representations\ncontribute to the final result, capabilities that are crucial for model\ndiagnosis and architecture optimization. This paper introduces SliceGX, a novel\nGNN explanation approach that generates explanations at specific GNN layers in\na progressive manner. Given a GNN M, a set of selected intermediate layers, and\na target layer, SliceGX automatically segments M into layer blocks (\"model\nslice\") and discovers high-quality explanatory subgraphs in each layer block\nthat clarifies the occurrence of output of M at the targeted layer. Although\nfinding such layer-wise explanations is computationally challenging, we develop\nefficient algorithms and optimization techniques that incrementally generate\nand maintain these subgraphs with provable approximation guarantees.\nAdditionally, SliceGX offers a SPARQL-like query interface, providing\ndeclarative access and search capacities for the generated explanations.\nThrough experiments on large real-world graphs and representative GNN\narchitectures, we verify the effectiveness and efficiency of SliceGX, and\nillustrate its practical utility in supporting model debugging.", "AI": {"tldr": "SliceGX\u662f\u4e00\u79cd\u65b0\u9896\u7684GNN\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u6e10\u8fdb\u5f0f\u5206\u6790\u751f\u6210\u89e3\u91ca\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5c42\u95f4\u5206\u6790\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709GNN\u89e3\u91ca\u65b9\u6cd5\u901a\u5e38\u901a\u8fc7\u8f93\u5165\u6270\u52a8\u8bc6\u522b\u5b50\u56fe\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u8868\u793a\u5982\u4f55\u5f71\u54cd\u6700\u7ec8\u7ed3\u679c\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u8fd9\u5bf9\u6a21\u578b\u8bca\u65ad\u548c\u67b6\u6784\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "SliceGX\u5c06GNN\u81ea\u52a8\u5206\u5272\u4e3a\u5c42\u5757\uff08\u6a21\u578b\u5207\u7247\uff09\uff0c\u5e76\u5728\u6bcf\u4e2a\u5c42\u5757\u4e2d\u53d1\u73b0\u9ad8\u8d28\u91cf\u89e3\u91ca\u5b50\u56fe\uff0c\u901a\u8fc7\u9ad8\u6548\u7b97\u6cd5\u548c\u4f18\u5316\u6280\u672f\u9010\u6b65\u751f\u6210\u548c\u7ef4\u6301\u5b50\u56fe\uff0c\u5e76\u63d0\u4f9bSPARQL-like\u67e5\u8be2\u63a5\u53e3\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SliceGX\u5728\u5927\u578b\u771f\u5b9e\u56fe\u6570\u636e\u548c\u5178\u578bGNN\u67b6\u6784\u4e0a\u7684\u6709\u6548\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6a21\u578b\u8c03\u8bd5\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "SliceGX\u901a\u8fc7\u5206\u5c42\u6e10\u8fdb\u5f0f\u5206\u6790\u548c\u9ad8\u6548\u7b97\u6cd5\uff0c\u4e3aGNN\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u652f\u6301\u6a21\u578b\u8bca\u65ad\u548c\u4f18\u5316\u3002"}}
{"id": "2506.18401", "pdf": "https://arxiv.org/pdf/2506.18401", "abs": "https://arxiv.org/abs/2506.18401", "authors": ["Hagit Attiya", "Armando Casta\u00f1eda", "Constantin Enea"], "title": "The Power of Strong Linearizability: the Difficulty of Consistent Refereeing", "categories": ["cs.DC"], "comment": null, "summary": "This paper studies the relation between agreement and strongly linearizable\nimplementations of various objects. This leads to new results about\nimplementations of concurrent objects from various primitives including window\nregisters and interfering primitives. We consider implementations that provide\nboth strong linearizability and decisive linearizability.\n  We identify that lock-free, respectively, wait-free, strongly linearizable\nimplementations of several concurrent objects entail a form of agreement that\nis weaker than consensus but impossible to strongly-linearizable implement with\ncombinations of non-universal primitives. In both cases, lock-free and\nwait-free, this form of agreement requires a distinguished process to referee a\ncompetition that involves all other processes. Our results show that consistent\nrefereeing of such competitions (i.e. the outcome of the competition does not\nchange in extensions of the current execution) requires high coordination\npower.\n  More specifically, two contest objects are defined and used to capture the\npower of strong linearizability in lock-free and wait-free implementations,\nrespectively. Both objects are strictly weaker than consensus, in the sense\nthat they have a wait-free linearizable (in fact, decisively linearizable)\nimplementation from reads and writes. The contest objects capture strong\nlinearizability since (1) they have strongly linearizable implementations from\nseveral ``high-level'' objects like stacks, queues, snapshots, counters, and\ntherefore, impossibility results for them carry over to these objects, and (2)\nthey admit powerful impossibility results for strong linearizability that\ninvolve window registers and interfering primitives, which are non-universal.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f3a\u7ebf\u6027\u5316\u5b9e\u73b0\u4e0e\u4e00\u81f4\u6027\u534f\u8bae\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u4e86\u975e\u901a\u7528\u539f\u8bed\u65e0\u6cd5\u5b9e\u73b0\u5f3a\u7ebf\u6027\u5316\u7684\u65b0\u7ed3\u679c\u3002", "motivation": "\u63a2\u8ba8\u5f3a\u7ebf\u6027\u5316\u548c\u51b3\u5b9a\u6027\u7ebf\u6027\u5316\u5728\u5e76\u53d1\u5bf9\u8c61\u5b9e\u73b0\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u4e0e\u4e00\u81f4\u6027\u534f\u8bae\u7684\u5173\u7cfb\u3002", "method": "\u5b9a\u4e49\u4e86\u4e24\u79cd\u7ade\u8d5b\u5bf9\u8c61\u6765\u6355\u6349\u5f3a\u7ebf\u6027\u5316\u7684\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5b9e\u73b0\u9650\u5236\u3002", "result": "\u53d1\u73b0\u5f3a\u7ebf\u6027\u5316\u9700\u8981\u9ad8\u534f\u8c03\u80fd\u529b\uff0c\u4e14\u7ade\u8d5b\u5bf9\u8c61\u6bd4\u5171\u8bc6\u66f4\u5f31\uff0c\u4f46\u4ecd\u5177\u6709\u5f3a\u5927\u9650\u5236\u6027\u3002", "conclusion": "\u5f3a\u7ebf\u6027\u5316\u5728\u975e\u901a\u7528\u539f\u8bed\u4e2d\u96be\u4ee5\u5b9e\u73b0\uff0c\u7ade\u8d5b\u5bf9\u8c61\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.18050", "pdf": "https://arxiv.org/pdf/2506.18050", "abs": "https://arxiv.org/abs/2506.18050", "authors": ["Lyuye Zhang", "Jian Zhang", "Kaixuan Li", "Chong Wang", "Chengwei Liu", "Jiahui Wu", "Sen Chen", "Yaowen Zheng", "Yang Liu"], "title": "VFArch\u0113: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software", "categories": ["cs.SE"], "comment": "15 pages", "summary": "Software Composition Analysis (SCA) has become pivotal in addressing\nvulnerabilities inherent in software project dependencies. In particular,\nreachability analysis is increasingly used in Open-Source Software (OSS)\nprojects to identify reachable vulnerabilities (e.g., CVEs) through call\ngraphs, enabling a focus on exploitable risks. Performing reachability analysis\ntypically requires the vulnerable function (VF) to track the call chains from\ndownstream applications. However, such crucial information is usually\nunavailable in modern vulnerability databases like NVD. While directly\nextracting VF from modified functions in vulnerability patches is intuitive,\npatches are not always available. Moreover, our preliminary study shows that\nover 26% of VF do not exist in the modified functions. Meanwhile, simply\nignoring patches to search vulnerable functions suffers from overwhelming\nnoises and lexical gaps between descriptions and source code. Given that almost\nhalf of the vulnerabilities are equipped with patches, a holistic solution that\nhandles both scenarios with and without patches is required. To meet real-world\nneeds and automatically localize VF, we present VFArch\\=e, a dual-mode approach\ndesigned for disclosed vulnerabilities, applicable in scenarios with or without\navailable patch links. The experimental results of VFArch\\=e on our constructed\nbenchmark dataset demonstrate significant efficacy regarding three metrics,\nachieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for\nPatch-present and Patch-absent modes, respectively. Moreover, VFArch\\=e has\nproven its applicability in real-world scenarios by successfully locating VF\nfor 43 out of 50 latest vulnerabilities with reasonable efforts and\nsignificantly reducing 78-89% false positives of SCA tools.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVFArch\\=e\u7684\u53cc\u6a21\u5f0f\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6709\u65e0\u8865\u4e01\u7684\u60c5\u51b5\u4e0b\u81ea\u52a8\u5b9a\u4f4d\u8f6f\u4ef6\u4f9d\u8d56\u4e2d\u7684\u6f0f\u6d1e\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u6f0f\u6d1e\u6570\u636e\u5e93\uff08\u5982NVD\uff09\u901a\u5e38\u7f3a\u5c11\u6f0f\u6d1e\u51fd\u6570\u7684\u5173\u952e\u4fe1\u606f\uff0c\u800c\u8865\u4e01\u53c8\u53ef\u80fd\u4e0d\u53ef\u7528\u6216\u4e0d\u5b8c\u5168\u8986\u76d6\u6f0f\u6d1e\u51fd\u6570\u3002\u9700\u8981\u4e00\u79cd\u7efc\u5408\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u8fd9\u4e24\u79cd\u60c5\u51b5\u3002", "method": "VFArch\\=e\u91c7\u7528\u53cc\u6a21\u5f0f\u8bbe\u8ba1\uff0c\u9488\u5bf9\u516c\u5f00\u6f0f\u6d1e\u5728\u6709\u8865\u4e01\u548c\u65e0\u8865\u4e01\u7684\u4e24\u79cd\u573a\u666f\u4e0b\u8fdb\u884c\u6f0f\u6d1e\u51fd\u6570\u5b9a\u4f4d\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cVFArch\\=e\u5728Patch-present\u548cPatch-absent\u6a21\u5f0f\u4e0b\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u9ad8\u51fa1.3\u500d\u548c1.9\u500d\u7684Mean Reciprocal Rank\uff0c\u5e76\u80fd\u663e\u8457\u51cf\u5c11SCA\u5de5\u5177\u768478-89%\u8bef\u62a5\u3002", "conclusion": "VFArch\\=e\u80fd\u591f\u9ad8\u6548\u5b9a\u4f4d\u6f0f\u6d1e\u51fd\u6570\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\uff0c\u4e3a\u89e3\u51b3\u8f6f\u4ef6\u4f9d\u8d56\u6f0f\u6d1e\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2506.18706", "pdf": "https://arxiv.org/pdf/2506.18706", "abs": "https://arxiv.org/abs/2506.18706", "authors": ["Roi Alfassi", "Angelora Cooper", "Zoe Mitchell", "Mary Calabro", "Orit Shaer", "Osnat Mokryn"], "title": "Fanfiction in the Age of AI: Community Perspectives on Creativity, Authenticity and Adoption", "categories": ["cs.HC"], "comment": "Accepted for publication in the International Journal of\n  Human-Computer Interaction, June 2025", "summary": "The integration of Generative AI (GenAI) into creative communities, like\nfanfiction, is reshaping how stories are created, shared, and valued. This\nstudy investigates the perceptions of 157 active fanfiction members, both\nreaders and writers, regarding AI-generated content in fanfiction. Our research\nexplores the impact of GenAI on community dynamics, examining how AI affects\nthe participatory and collaborative nature of these spaces. The findings reveal\nresponses ranging from cautious acceptance of AI's potential for creative\nenhancement to concerns about authenticity, ethical issues, and the erosion of\nhuman-centered values. Participants emphasized the importance of transparency\nand expressed worries about losing social connections. Our study highlights the\nneed for thoughtful AI integration in creative platforms using design\ninterventions that enable ethical practices, promote transparency, increase\nengagement and connection, and preserve the community's core values.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86GenAI\u5728\u7c89\u4e1d\u5c0f\u8bf4\u521b\u4f5c\u4e2d\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86157\u540d\u793e\u533a\u6210\u5458\u5bf9AI\u751f\u6210\u5185\u5bb9\u7684\u6001\u5ea6\uff0c\u63ed\u793a\u4e86\u4ece\u8c28\u614e\u63a5\u53d7\u5230\u62c5\u5fe7\u7684\u591a\u65b9\u9762\u53cd\u5e94\u3002", "motivation": "\u63a2\u8ba8GenAI\u5982\u4f55\u5f71\u54cd\u521b\u610f\u793e\u533a\u7684\u52a8\u6001\uff0c\u5c24\u5176\u662f\u7c89\u4e1d\u5c0f\u8bf4\u4e2d\u7684\u6545\u4e8b\u521b\u4f5c\u4e0e\u5171\u4eab\u65b9\u5f0f\u3002", "method": "\u8c03\u67e5\u4e86157\u540d\u6d3b\u8dc3\u7c89\u4e1d\u5c0f\u8bf4\u793e\u533a\u6210\u5458\u7684\u770b\u6cd5\uff0c\u5305\u62ec\u8bfb\u8005\u548c\u4f5c\u8005\u3002", "result": "\u53d1\u73b0\u6210\u5458\u5bf9AI\u7684\u6001\u5ea6\u591a\u6837\uff0c\u4ece\u63a5\u53d7\u521b\u610f\u589e\u5f3a\u5230\u62c5\u5fe7\u771f\u5b9e\u6027\u3001\u4f26\u7406\u548c\u793e\u4ea4\u8fde\u63a5\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u8bbe\u8ba1\u5e72\u9884\u63aa\u65bd\uff0c\u786e\u4fddAI\u5728\u521b\u610f\u5e73\u53f0\u4e2d\u7684\u4f26\u7406\u6574\u5408\uff0c\u4fdd\u6301\u793e\u533a\u6838\u5fc3\u4ef7\u503c\u3002"}}
{"id": "2506.17371", "pdf": "https://arxiv.org/pdf/2506.17371", "abs": "https://arxiv.org/abs/2506.17371", "authors": ["Thilina Pathirana", "Ruxandra F. Olimid"], "title": "Secret Sharing in 5G-MEC: Applicability for joint Security and Dependability", "categories": ["cs.CR", "cs.NI"], "comment": "10 pages, 5 figures, Accepted to the proceedings of 22nd\n  International Conference on Privacy, Security, and Trust (PST2025)", "summary": "Multi-access Edge Computing (MEC), an enhancement of 5G, processes data\ncloser to its generation point, reducing latency and network load. However, the\ndistributed and edge-based nature of 5G-MEC presents privacy and security\nchallenges, including data exposure risks. Ensuring efficient manipulation and\nsecurity of sensitive data at the edge is crucial. To address these challenges,\nwe investigate the usage of threshold secret sharing in 5G-MEC storage, an\napproach that enhances both security and dependability. A (k,n) threshold\nsecret sharing scheme splits and stores sensitive data among n nodes, requiring\nat least k nodes for reconstruction. The solution ensures confidentiality by\nprotecting data against fewer than k colluding nodes and enhances availability\nby tolerating up to n-k failing nodes. This approach mitigates threats such as\nunauthorized access and node failures, whether accidental or intentional. We\nfurther discuss a method for selecting the convenient MEHs to store the shares,\nconsidering the MEHs' trustworthiness level as a main criterion. Although we\ndefine our proposal in the context of secret-shared data storage, it can be\nseen as an independent, standalone selection process for 5G-MEC trustworthy\nnode selection in other scenarios too.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u57285G-MEC\u5b58\u50a8\u4e2d\u4f7f\u7528\u9608\u503c\u79d8\u5bc6\u5171\u4eab\u6280\u672f\u4ee5\u89e3\u51b3\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\u3002", "motivation": "5G-MEC\u5177\u6709\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u6548\u7387\u7684\u4f18\u52bf\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u95ee\u9898\uff0c\u5c24\u5176\u662f\u8fb9\u7f18\u8282\u70b9\u7684\u5206\u5e03\u7279\u6027\u589e\u52a0\u4e86\u98ce\u9669\u3002", "method": "\u91c7\u7528(k,n)\u9608\u503c\u79d8\u5bc6\u5171\u4eab\u65b9\u6848\uff0c\u5c06\u654f\u611f\u6570\u636e\u5206\u7247\u5b58\u50a8\u4e8e\u591a\u4e2a\u8fb9\u7f18\u8282\u70b9\uff0c\u9700\u81f3\u5c11k\u4e2a\u8282\u70b9\u534f\u4f5c\u624d\u80fd\u91cd\u5efa\u6570\u636e\u3002\u540c\u65f6\u63d0\u51fa\u57fa\u4e8e\u8282\u70b9\u53ef\u4fe1\u5ea6\u9009\u62e9\u5b58\u50a8\u8282\u70b9\u7684\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u9632\u6b62\u6700\u591ak-1\u4e2a\u8282\u70b9\u5408\u8c0b\u7a83\u53d6\u6570\u636e\uff0c\u8fd8\u80fd\u5bb9\u5fcd\u6700\u591an-k\u4e2a\u8282\u70b9\u5931\u8d25\uff0c\u63d0\u5347\u4e86\u6570\u636e\u7684\u5b89\u5168\u6027\u548c\u53ef\u7528\u6027\u3002", "conclusion": "\u9608\u503c\u79d8\u5bc6\u5171\u4eab\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e5G-MEC\u73af\u5883\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u9700\u8981\u53ef\u4fe1\u8282\u70b9\u9009\u62e9\u7684\u573a\u666f\u3002"}}
{"id": "2506.18499", "pdf": "https://arxiv.org/pdf/2506.18499", "abs": "https://arxiv.org/abs/2506.18499", "authors": ["Alessandra Agostini", "Andrea Maurino", "Blerina Spahiu"], "title": "PuckTrick: A Library for Making Synthetic Data More Realistic", "categories": ["cs.LG", "cs.AI", "cs.DB", "H.4.1; I.2.1"], "comment": "17 pages, 3 figures", "summary": "The increasing reliance on machine learning (ML) models for decision-making\nrequires high-quality training data. However, access to real-world datasets is\noften restricted due to privacy concerns, proprietary restrictions, and\nincomplete data availability. As a result, synthetic data generation (SDG) has\nemerged as a viable alternative, enabling the creation of artificial datasets\nthat preserve the statistical properties of real data while ensuring privacy\ncompliance. Despite its advantages, synthetic data is often overly clean and\nlacks real-world imperfections, such as missing values, noise, outliers, and\nmisclassified labels, which can significantly impact model generalization and\nrobustness. To address this limitation, we introduce Pucktrick, a Python\nlibrary designed to systematically contaminate synthetic datasets by\nintroducing controlled errors. The library supports multiple error types,\nincluding missing data, noisy values, outliers, label misclassification,\nduplication, and class imbalance, offering a structured approach to evaluating\nML model resilience under real-world data imperfections. Pucktrick provides two\ncontamination modes: one for injecting errors into clean datasets and another\nfor further corrupting already contaminated datasets. Through extensive\nexperiments on real-world financial datasets, we evaluate the impact of\nsystematic data contamination on model performance. Our findings demonstrate\nthat ML models trained on contaminated synthetic data outperform those trained\non purely synthetic, error-free data, particularly for tree-based and linear\nmodels such as SVMs and Extra Trees.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Pucktrick\u5e93\uff0c\u7528\u4e8e\u901a\u8fc7\u5f15\u5165\u53d7\u63a7\u9519\u8bef\u6765\u6c61\u67d3\u5408\u6210\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7528\u6c61\u67d3\u540e\u7684\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u7eaf\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u548c\u4e13\u6709\u9650\u5236\uff0c\u771f\u5b9e\u6570\u636e\u96be\u4ee5\u83b7\u53d6\uff0c\u800c\u5408\u6210\u6570\u636e\u53c8\u8fc7\u4e8e\u5e72\u51c0\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u7684\u590d\u6742\u6027\u3002Pucktrick\u901a\u8fc7\u5f15\u5165\u53d7\u63a7\u9519\u8bef\uff0c\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faPucktrick\u5e93\uff0c\u652f\u6301\u591a\u79cd\u9519\u8bef\u7c7b\u578b\uff08\u5982\u7f3a\u5931\u503c\u3001\u566a\u58f0\u3001\u5f02\u5e38\u503c\u7b49\uff09\uff0c\u63d0\u4f9b\u4e24\u79cd\u6c61\u67d3\u6a21\u5f0f\uff1a\u5bf9\u5e72\u51c0\u6570\u636e\u96c6\u6216\u5df2\u6c61\u67d3\u6570\u636e\u96c6\u8fdb\u4e00\u6b65\u6ce8\u5165\u9519\u8bef\u3002", "result": "\u5728\u771f\u5b9e\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7528\u53d7\u6c61\u67d3\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u5c24\u5176\u662f\u6811\u6a21\u578b\u548c\u7ebf\u6027\u6a21\u578b\uff09\u8868\u73b0\u4f18\u4e8e\u7eaf\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "Pucktrick\u901a\u8fc7\u589e\u5f3a\u5408\u6210\u6570\u636e\u7684\u590d\u6742\u6027\uff0c\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.17236", "pdf": "https://arxiv.org/pdf/2506.17236", "abs": "https://arxiv.org/abs/2506.17236", "authors": ["Serdar Metin"], "title": "Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems", "categories": ["cs.CR", "cs.CE", "cs.DC"], "comment": "PhD thesis, 98 pages, 4 figures", "summary": "The present dissertation addresses the problem of fairly distributing shared\nresources in non-commercial blockchain networks. Blockchains are distributed\nsystems that order and timestamp records of a given network of users, in a\npublic, cryptographically secure, and consensual way. The records, which may in\nkind be events, transaction orders, sets of rules for structured transactions\netc. are placed within well-defined datastructures called blocks, and they are\nlinked to each other by the virtue of cryptographic pointers, in a total\nordering which represents their temporal relations of succession. The ability\nto operate on the blockchain, and/or to contribute a record to the content of a\nblock are shared resources of the blockchain systems. In commercial networks,\nthese resources are exchanged in return for fiat money, and consequently,\nfairness is not a relevant problem in terms of computer engineering. In\nnon-commercial networks, however, monetary solutions are not available, by\ndefinition. The present non-commercial blockchain networks employ trivial\ndistribution mechanisms called faucets, which offer fixed amounts of free\ntokens (called cryptocurrencies) specific to the given network. This mechanism,\nalthough simple and efficient, is prone to denial of service (DoS) attacks and\ncannot address the fairness problem. In the present dissertation, the faucet\nmechanism is adapted for fair distribution, in line with Max-min Fairness\nscheme. In total, we contributed 6 distinct Max-min Fair algorithms as\nefficient blockchain faucets. The algorithms we contribute are resistant to DoS\nattacks, low-cost in terms of blockchain computation economics, and they also\nallow for different user weighting policies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u975e\u5546\u4e1a\u533a\u5757\u94fe\u7f51\u7edc\u4e2d\u516c\u5e73\u5206\u914d\u5171\u4eab\u8d44\u6e90\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u673a\u5236\uff08\u5982\u201cfaucet\u201d\uff09\u6613\u53d7\u62d2\u7edd\u670d\u52a1\u653b\u51fb\u548c\u7f3a\u4e4f\u516c\u5e73\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u975e\u5546\u4e1a\u533a\u5757\u94fe\u7f51\u7edc\u4e2d\uff0c\u7531\u4e8e\u65e0\u6cd5\u4f7f\u7528\u8d27\u5e01\u89e3\u51b3\u65b9\u6848\uff0c\u73b0\u6709\u8d44\u6e90\u5206\u914d\u673a\u5236\uff08\u5982\u201cfaucet\u201d\uff09\u5b58\u5728\u516c\u5e73\u6027\u548c\u5b89\u5168\u6027\u95ee\u9898\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u5c06\u201cfaucet\u201d\u673a\u5236\u4e0eMax-min Fairness\u65b9\u6848\u7ed3\u5408\uff0c\u63d0\u51fa\u4e866\u79cd\u9ad8\u6548\u4e14\u6297\u62d2\u7edd\u670d\u52a1\u653b\u51fb\u7684\u7b97\u6cd5\u3002", "result": "\u8fd9\u4e9b\u7b97\u6cd5\u4e0d\u4ec5\u6297\u653b\u51fb\uff0c\u8fd8\u5177\u5907\u4f4e\u6210\u672c\u7279\u6027\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u7684\u7528\u6237\u6743\u91cd\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cMax-min Fair\u7b97\u6cd5\u4e3a\u533a\u5757\u94fe\u8d44\u6e90\u5206\u914d\u63d0\u4f9b\u4e86\u66f4\u516c\u5e73\u548c\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18191", "pdf": "https://arxiv.org/pdf/2506.18191", "abs": "https://arxiv.org/abs/2506.18191", "authors": ["Masudul Hasan Masud Bhuiyan", "Gianluca De Stefano", "Giancarlo Pellegrino", "Cristian-Alexandru Staicu"], "title": "Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Static analysis plays a key role in finding bugs, including security issues.\nA critical step in static analysis is building accurate call graphs that model\nfunction calls in a program. However, due to hard-to-analyze language features,\nexisting call graph construction algorithms for JavaScript are neither sound\nnor complete. Prior work shows that even advanced solutions produce false edges\nand miss valid ones. In this work, we assist these tools by identifying missed\ncall edges. Our main idea is to frame the problem as link prediction on full\nprogram graphs, using a rich representation with multiple edge types. Our\napproach, GRAPHIA, leverages recent advances in graph neural networks to model\nnon-local relationships between code elements. Concretely, we propose\nrepresenting JavaScript programs using a combination of syntactic- and\nsemantic-based edges. GRAPHIA can learn from imperfect labels, including static\ncall edges from existing tools and dynamic edges from tests, either from the\nsame or different projects. Because call graphs are sparse, standard machine\nlearning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by\nranking function definitions for each unresolved call site. We conduct a\nlarge-scale evaluation on 50 popular JavaScript libraries with 163K call edges\n(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M\nstructural and 386K semantic edges. It ranks the correct target as the top\ncandidate in over 42% of unresolved cases and within the top 5 in 72% of cases,\nreducing the manual effort needed for analysis. Our results show that\nlearning-based methods can improve the recall of JavaScript call graph\nconstruction. To our knowledge, this is the first work to apply GNN-based link\nprediction to full multi-file program graphs for interprocedural analysis.", "AI": {"tldr": "GRAPHIA\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u6539\u8fdbJavaScript\u8c03\u7528\u56fe\u6784\u5efa\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u7a0b\u5e8f\u56fe\u8868\u793a\u548c\u591a\u7c7b\u578b\u8fb9\u94fe\u63a5\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u672a\u89e3\u6790\u8c03\u7528\u8fb9\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709JavaScript\u8c03\u7528\u56fe\u6784\u5efa\u5de5\u5177\u65e2\u4e0d\u5065\u5168\u4e5f\u4e0d\u5b8c\u6574\uff0c\u5e38\u9057\u6f0f\u6709\u6548\u8fb9\u6216\u4ea7\u751f\u9519\u8bef\u8fb9\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u63d0\u5347\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faGRAPHIA\uff0c\u7ed3\u5408\u8bed\u6cd5\u548c\u8bed\u4e49\u8fb9\u8868\u793a\u7a0b\u5e8f\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u4ee3\u7801\u5143\u7d20\u95f4\u7684\u975e\u5c40\u90e8\u5173\u7cfb\uff0c\u652f\u6301\u4ece\u4e0d\u5b8c\u7f8e\u6807\u7b7e\u4e2d\u5b66\u4e60\u3002", "result": "\u572850\u4e2a\u6d41\u884cJavaScript\u5e93\u4e0a\u8bc4\u4f30\uff0cGRAPHIA\u572842%\u672a\u89e3\u6790\u8c03\u7528\u4e2d\u5c06\u6b63\u786e\u76ee\u6807\u6392\u540d\u7b2c\u4e00\uff0c72%\u7684\u60c5\u51b5\u4e0b\u4f4d\u5217\u524d\u4e94\u3002", "conclusion": "\u5b66\u4e60\u578b\u65b9\u6cd5\u80fd\u63d0\u5347\u8c03\u7528\u56fe\u6784\u5efa\u7684\u53ec\u56de\u7387\uff0cGRAPHIA\u662f\u9996\u4e2a\u5c06GNN\u94fe\u63a5\u9884\u6d4b\u5e94\u7528\u4e8e\u591a\u6587\u4ef6\u7a0b\u5e8f\u56fe\u8fdb\u884c\u8fc7\u7a0b\u95f4\u5206\u6790\u7684\u5de5\u4f5c\u3002"}}
{"id": "2506.18711", "pdf": "https://arxiv.org/pdf/2506.18711", "abs": "https://arxiv.org/abs/2506.18711", "authors": ["Marianne Bossema", "Somaya Ben Allouch", "Aske Plaat", "Rob Saunders"], "title": "LLM-enhanced Interactions in Human-Robot Collaborative Drawing with Older Adults", "categories": ["cs.HC"], "comment": null, "summary": "The goal of this study is to identify factors that support and enhance older\nadults' creative experiences in human-robot co-creativity. Because the research\ninto the use of robots for creativity support with older adults remains\nunderexplored, we carried out an exploratory case study. We took a\nparticipatory approach and collaborated with professional art educators to\ndesign a course Drawing with Robots for adults aged 65 and over. The course\nfeatured human-human and human-robot drawing activities with various types of\nrobots. We observed collaborative drawing interactions, interviewed\nparticipants on their experiences, and analyzed collected data. Findings show\nthat participants preferred acting as curators, evaluating creative suggestions\nfrom the robot in a teacher or coach role. When we enhanced a robot with a\nmultimodal Large Language Model (LLM), participants appreciated its spoken\ndialogue capabilities. They reported however, that the robot's feedback\nsometimes lacked an understanding of the context, and sensitivity to their\nartistic goals and preferences. Our findings highlight the potential of\nLLM-enhanced robots to support creativity and offer future directions for\nadvancing human-robot co-creativity with older adults.", "AI": {"tldr": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u652f\u6301\u8001\u5e74\u4eba\u53c2\u4e0e\u4eba\u673a\u5171\u521b\u521b\u610f\u4f53\u9a8c\u7684\u56e0\u7d20\uff0c\u901a\u8fc7\u8bbe\u8ba1\u201c\u4e0e\u673a\u5668\u4eba\u4e00\u8d77\u7ed8\u753b\u201d\u8bfe\u7a0b\u5e76\u89c2\u5bdf\u4e92\u52a8\uff0c\u53d1\u73b0\u8001\u5e74\u4eba\u66f4\u503e\u5411\u4e8e\u62c5\u4efb\u7b56\u5c55\u4eba\u89d2\u8272\uff0c\u8bc4\u4f30\u673a\u5668\u4eba\u7684\u521b\u610f\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5982\u4f55\u5229\u7528\u673a\u5668\u4eba\u589e\u5f3a\u8001\u5e74\u4eba\u7684\u521b\u9020\u529b\u4f53\u9a8c\uff0c\u586b\u8865\u8fd9\u4e00\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u53c2\u4e0e\u5f0f\u65b9\u6cd5\uff0c\u4e0e\u4e13\u4e1a\u827a\u672f\u6559\u80b2\u8005\u5408\u4f5c\u8bbe\u8ba1\u8bfe\u7a0b\u201c\u4e0e\u673a\u5668\u4eba\u4e00\u8d77\u7ed8\u753b\u201d\uff0c\u89c2\u5bdf\u4e92\u52a8\u5e76\u8fdb\u884c\u8bbf\u8c08\u4e0e\u5206\u6790\u3002", "result": "\u53d1\u73b0\u8001\u5e74\u4eba\u66f4\u559c\u6b22\u4f5c\u4e3a\u7b56\u5c55\u4eba\u89d2\u8272\u8bc4\u4f30\u673a\u5668\u4eba\u5efa\u8bae\uff1b\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u53d7\u597d\u8bc4\uff0c\u4f46\u53cd\u9988\u7f3a\u4e4f\u5bf9\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u548c\u827a\u672f\u76ee\u6807\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLM\u589e\u5f3a\u7684\u673a\u5668\u4eba\u5177\u6709\u652f\u6301\u521b\u9020\u529b\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u4eba\u673a\u5171\u521b\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2506.17507", "pdf": "https://arxiv.org/pdf/2506.17507", "abs": "https://arxiv.org/abs/2506.17507", "authors": ["Michael T. Goodrich", "Vinesh Sridhar"], "title": "Optimal Parallel Algorithms for Convex Hulls in 2D and 3D under Noisy Primitive Operations", "categories": ["cs.CG", "cs.DC"], "comment": "17 pages, 3 figures. Accepted at the 37th Canadian Conference on\n  Computational Geometry, 2025", "summary": "In the noisy primitives model, each primitive comparison performed by an\nalgorithm, e.g., testing whether one value is greater than another, returns the\nincorrect answer with random, independent probability p < 1/2 and otherwise\nreturns a correct answer. This model was first applied in the context of\nsorting and searching, and recent work by Eppstein, Goodrich, and Sridhar\nextends this model to sequential algorithms involving geometric primitives such\nas orientation and sidedness tests. However, their approaches appear to be\ninherently sequential; hence, in this paper, we study parallel computational\ngeometry algorithms for 2D and 3D convex hulls in the noisy primitives model.\nWe give the first optimal parallel algorithms in the noisy primitives model for\n2D and 3D convex hulls in the CREW PRAM model. The main technical contribution\nof our work concerns our ability to detect and fix errors during intermediate\nsteps of our algorithm using a generalization of the failure sweeping\ntechnique.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u566a\u58f0\u539f\u59cb\u6a21\u578b\u4e0b\u7684\u5e76\u884c\u8ba1\u7b97\u51e0\u4f55\u7b97\u6cd5\uff0c\u7279\u522b\u662f2D\u548c3D\u51f8\u5305\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u5728CREW PRAM\u6a21\u578b\u4e2d\u7684\u6700\u4f18\u5e76\u884c\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e7f\u4e49\u7684\u5931\u8d25\u626b\u63cf\u6280\u672f\u6765\u68c0\u6d4b\u548c\u4fee\u590d\u4e2d\u95f4\u6b65\u9aa4\u7684\u9519\u8bef\u3002", "motivation": "\u566a\u58f0\u539f\u59cb\u6a21\u578b\u4e2d\uff0c\u51e0\u4f55\u7b97\u6cd5\u7684\u5e76\u884c\u5316\u7814\u7a76\u8f83\u5c11\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u987a\u5e8f\u7b97\u6cd5\uff0c\u56e0\u6b64\u63a2\u7d22\u5e76\u884c\u7b97\u6cd5\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u91c7\u7528\u5e7f\u4e49\u7684\u5931\u8d25\u626b\u63cf\u6280\u672f\u6765\u68c0\u6d4b\u548c\u4fee\u590d\u7b97\u6cd5\u4e2d\u7684\u4e2d\u95f4\u9519\u8bef\uff0c\u5b9e\u73b0\u5728CREW PRAM\u6a21\u578b\u4e0b\u7684\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u9996\u6b21\u63d0\u51fa\u4e86\u5728\u566a\u58f0\u539f\u59cb\u6a21\u578b\u4e2d\u9488\u5bf92D\u548c3D\u51f8\u5305\u95ee\u9898\u7684\u6700\u4f18\u5e76\u884c\u7b97\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u6280\u672f\u6539\u8fdb\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5e76\u884c\u7b97\u6cd5\u5728\u566a\u58f0\u539f\u59cb\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18219", "pdf": "https://arxiv.org/pdf/2506.18219", "abs": "https://arxiv.org/abs/2506.18219", "authors": ["Ulrike M. Graetsch", "Rashina Hoda", "Hourieh Khalazjadeh", "Mojtaba Shahin", "John Grundy"], "title": "Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study", "categories": ["cs.SE"], "comment": "25 pages", "summary": "Context: There is an increase in the investment and development of\ndata-intensive (DI) solutions, systems that manage large amounts of data.\nWithout careful management, this growing investment will also grow associated\ntechnical debt (TD). Delivery of DI solutions requires a multidisciplinary\nskill set, but there is limited knowledge about how multidisciplinary teams\ndevelop DI systems and manage TD.\n  Objective: This research contributes empirical, practice based insights about\nmultidisciplinary DI team TD management practices.\n  Method: This research was conducted as an exploratory observation case study.\nWe used socio-technical grounded theory (STGT) for data analysis to develop\nconcepts and categories that articulate TD and TDs debt management practices.\n  Results: We identify TD that the DI team deals with, in particular technical\ndata components debt and pipeline debt. We explain how the team manages the TD,\nassesses TD, what TD treatments they consider and how they implement TD\ntreatments to fit sprint capacity constraints.\n  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss\ntheir implications and highlight the need for new implementation patterns and\ntool support for multidisciplinary DI teams.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u5b66\u79d1\u6570\u636e\u5bc6\u96c6\u578b\u56e2\u961f\u7684\u6280\u503a\u7ba1\u7406\u5b9e\u8df5\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u4e86\u56e2\u961f\u5982\u4f55\u8bc4\u4f30\u548c\u5904\u7406\u6280\u503a\uff0c\u5e76\u63d0\u51fa\u5bf9\u65b0\u6a21\u5f0f\u548c\u5de5\u5177\u652f\u6301\u7684\u9700\u6c42\u3002", "motivation": "\u6570\u636e\u5bc6\u96c6\u578b\u7cfb\u7edf\u7684\u6295\u8d44\u589e\u52a0\u5bfc\u81f4\u6280\u503a\u98ce\u9669\u4e0a\u5347\uff0c\u4f46\u591a\u5b66\u79d1\u56e2\u961f\u5982\u4f55\u7ba1\u7406\u6280\u503a\u7684\u5b9e\u8bc1\u7814\u7a76\u6709\u9650\u3002", "method": "\u91c7\u7528\u63a2\u7d22\u6027\u89c2\u5bdf\u6848\u4f8b\u7814\u7a76\u548c\u793e\u4f1a\u6280\u672f\u624e\u6839\u7406\u8bba\u5206\u6790\u6570\u636e\u3002", "result": "\u8bc6\u522b\u4e86\u6570\u636e\u7ec4\u4ef6\u503a\u52a1\u548c\u7ba1\u9053\u503a\u52a1\uff0c\u63cf\u8ff0\u4e86\u56e2\u961f\u7684\u6280\u503a\u8bc4\u4f30\u548c\u5904\u7406\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e0e\u73b0\u6709\u6280\u503a\u5206\u7c7b\u4e00\u81f4\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u65b0\u7684\u5b9e\u65bd\u6a21\u5f0f\u548c\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2506.18727", "pdf": "https://arxiv.org/pdf/2506.18727", "abs": "https://arxiv.org/abs/2506.18727", "authors": ["Xingyu Xiao", "Jiejuan Tong", "Jun Sun", "Zhe Sui", "Jingang Liang", "Hongru Zhao", "Jun Zhao", "Haitao Wang"], "title": "AutoGraph: A Knowledge-Graph Framework for Modeling Interface Interaction and Automating Procedure Execution in Digital Nuclear Control Rooms", "categories": ["cs.HC", "cs.SE"], "comment": null, "summary": "Digitalization in nuclear power plant (NPP) control rooms is reshaping how\noperators interact with procedures and interface elements. However, existing\ncomputer-based procedures (CBPs) often lack semantic integration with\nhuman-system interfaces (HSIs), limiting their capacity to support intelligent\nautomation and increasing the risk of human error, particularly under dynamic\nor complex operating conditions. In this study, we present AutoGraph, a\nknowledge-graph-based framework designed to formalize and automate procedure\nexecution in digitalized NPP environments.AutoGraph integrates (1) a proposed\nHTRPM tracking module to capture operator interactions and interface element\nlocations; (2) an Interface Element Knowledge Graph (IE-KG) encoding spatial,\nsemantic, and structural properties of HSIs; (3) automatic mapping from textual\nprocedures to executable interface paths; and (4) an execution engine that maps\ntextual procedures to executable interface paths. This enables the\nidentification of cognitively demanding multi-action steps and supports fully\nautomated execution with minimal operator input. We validate the framework\nthrough representative control room scenarios, demonstrating significant\nreductions in task completion time and the potential to support real-time human\nreliability assessment. Further integration into dynamic HRA frameworks (e.g.,\nCOGMIF) and real-time decision support systems (e.g., DRIF) illustrates\nAutoGraph extensibility in enhancing procedural safety and cognitive\nperformance in complex socio-technical systems.", "AI": {"tldr": "AutoGraph\u662f\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u65e8\u5728\u6570\u5b57\u5316\u6838\u7535\u5382\u63a7\u5236\u5ba4\u4e2d\u81ea\u52a8\u5316\u6267\u884c\u7a0b\u5e8f\uff0c\u51cf\u5c11\u4eba\u4e3a\u9519\u8bef\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u7b97\u673a\u5316\u7a0b\u5e8f\uff08CBPs\uff09\u7f3a\u4e4f\u4e0e\u4eba\u7c7b\u7cfb\u7edf\u754c\u9762\uff08HSI\uff09\u7684\u8bed\u4e49\u96c6\u6210\uff0c\u9650\u5236\u4e86\u667a\u80fd\u81ea\u52a8\u5316\u7684\u80fd\u529b\u5e76\u589e\u52a0\u4e86\u4eba\u4e3a\u9519\u8bef\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u6216\u590d\u6742\u6761\u4ef6\u4e0b\u3002", "method": "AutoGraph\u901a\u8fc7\u56db\u4e2a\u6a21\u5757\u5b9e\u73b0\uff1aHTRPM\u8ddf\u8e2a\u6a21\u5757\u6355\u83b7\u64cd\u4f5c\u5458\u4ea4\u4e92\u3001IE-KG\u7f16\u7801HSI\u5c5e\u6027\u3001\u81ea\u52a8\u5c06\u6587\u672c\u7a0b\u5e8f\u6620\u5c04\u4e3a\u53ef\u6267\u884c\u8def\u5f84\u3001\u4ee5\u53ca\u6267\u884c\u5f15\u64ce\u3002", "result": "\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4e86\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u5e76\u652f\u6301\u5b9e\u65f6\u4eba\u7c7b\u53ef\u9760\u6027\u8bc4\u4f30\u3002", "conclusion": "AutoGraph\u53ef\u6269\u5c55\u6027\u9ad8\uff0c\u80fd\u589e\u5f3a\u590d\u6742\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u4e2d\u7684\u7a0b\u5e8f\u5b89\u5168\u6027\u548c\u8ba4\u77e5\u6027\u80fd\u3002"}}
{"id": "2506.17510", "pdf": "https://arxiv.org/pdf/2506.17510", "abs": "https://arxiv.org/abs/2506.17510", "authors": ["Rafael Ferreira da Silva", "Milad Abolhasani", "Dionysios A. Antonopoulos", "Laura Biven", "Ryan Coffee", "Ian T. Foster", "Leslie Hamilton", "Shantenu Jha", "Theresa Mayer", "Benjamin Mintz", "Robert G. Moore", "Salahudin Nimer", "Noah Paulson", "Woong Shin", "Frederic Suter", "Mitra Taheri", "Michela Taufer", "Newell R. Washburn"], "title": "A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery", "categories": ["cs.CY", "cs.DC", "physics.soc-ph"], "comment": null, "summary": "Scientific discovery is being revolutionized by AI and autonomous systems,\nyet current autonomous laboratories remain isolated islands unable to\ncollaborate across institutions. We present the Autonomous Interconnected\nScience Lab Ecosystem (AISLE), a grassroots network transforming fragmented\ncapabilities into a unified system that shorten the path from ideation to\ninnovation to impact and accelerates discovery from decades to months. AISLE\naddresses five critical dimensions: (1) cross-institutional equipment\norchestration, (2) intelligent data management with FAIR compliance, (3)\nAI-agent driven orchestration grounded in scientific principles, (4)\ninteroperable agent communication interfaces, and (5) AI/ML-integrated\nscientific education. By connecting autonomous agents across institutional\nboundaries, autonomous science can unlock research spaces inaccessible to\ntraditional approaches while democratizing cutting-edge technologies. This\nparadigm shift toward collaborative autonomous science promises breakthroughs\nin sustainable energy, materials development, and public health.", "AI": {"tldr": "AISLE\u662f\u4e00\u4e2a\u81ea\u4e3b\u4e92\u8054\u7684\u79d1\u5b66\u5b9e\u9a8c\u5ba4\u751f\u6001\u7f51\u7edc\uff0c\u65e8\u5728\u6574\u5408\u5206\u6563\u7684\u81ea\u6cbb\u5b9e\u9a8c\u5ba4\u8d44\u6e90\uff0c\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u8fc7\u7a0b\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u6cbb\u5b9e\u9a8c\u5ba4\u5b64\u7acb\u8fd0\u884c\uff0c\u65e0\u6cd5\u8de8\u673a\u6784\u534f\u4f5c\uff0c\u9650\u5236\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u6548\u7387\u548c\u8303\u56f4\u3002", "method": "AISLE\u901a\u8fc7\u4e94\u4e2a\u5173\u952e\u7ef4\u5ea6\u5b9e\u73b0\u534f\u4f5c\uff1a\u8de8\u673a\u6784\u8bbe\u5907\u534f\u8c03\u3001\u667a\u80fd\u6570\u636e\u7ba1\u7406\u3001\u57fa\u4e8e\u79d1\u5b66\u539f\u7406\u7684AI\u4ee3\u7406\u9a71\u52a8\u3001\u4e92\u64cd\u4f5c\u7684\u4ee3\u7406\u901a\u4fe1\u63a5\u53e3\u4ee5\u53caAI/ML\u96c6\u6210\u79d1\u5b66\u6559\u80b2\u3002", "result": "AISLE\u80fd\u591f\u5c06\u79d1\u5b66\u53d1\u73b0\u7684\u65f6\u95f4\u4ece\u6570\u5341\u5e74\u7f29\u77ed\u5230\u6570\u6708\uff0c\u5e76\u5f00\u62d3\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u89e6\u53ca\u7684\u7814\u7a76\u9886\u57df\u3002", "conclusion": "\u534f\u4f5c\u81ea\u6cbb\u79d1\u5b66\u7684\u8303\u5f0f\u8f6c\u53d8\u5c06\u5728\u53ef\u6301\u7eed\u80fd\u6e90\u3001\u6750\u6599\u5f00\u53d1\u548c\u516c\u5171\u536b\u751f\u7b49\u9886\u57df\u5e26\u6765\u7a81\u7834\u3002"}}
{"id": "2506.18289", "pdf": "https://arxiv.org/pdf/2506.18289", "abs": "https://arxiv.org/abs/2506.18289", "authors": ["Saurabhsingh Rajput", "Mootez Saad", "Tushar Sharma"], "title": "Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations", "categories": ["cs.SE", "cs.AI"], "comment": "In review", "summary": "AI's exponential growth intensifies computational demands and energy\nchallenges. While practitioners employ various optimization techniques, that we\nrefer as \"knobs\" in this paper, to tune model efficiency, these are typically\nafterthoughts and reactive ad-hoc changes applied in isolation without\nunderstanding their combinatorial effects on energy efficiency. This paper\nemphasizes on treating energy efficiency as the first-class citizen and as a\nfundamental design consideration for a compute-intensive pipeline. We show that\nstrategic selection across five AI pipeline phases (data, model, training,\nsystem, inference) creates cascading efficiency. Experimental validation shows\northogonal combinations reduce energy consumption by up to $94.6$% while\npreserving $95.95$% of the original F1 score of non-optimized pipelines. This\ncurated approach provides actionable frameworks for informed sustainable AI\nthat balance efficiency, performance, and environmental responsibility.", "AI": {"tldr": "\u8bba\u6587\u5f3a\u8c03\u5c06\u80fd\u6e90\u6548\u7387\u4f5c\u4e3aAI\u7ba1\u9053\u7684\u6838\u5fc3\u8bbe\u8ba1\u8003\u8651\uff0c\u901a\u8fc7\u4e94\u4e2a\u9636\u6bb5\u7684\u4f18\u5316\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u80fd\u6e90\u8282\u7701\u548c\u6027\u80fd\u4fdd\u7559\u3002", "motivation": "AI\u7684\u8ba1\u7b97\u9700\u6c42\u548c\u80fd\u6e90\u6311\u6218\u65e5\u76ca\u589e\u957f\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u591a\u4e3a\u5b64\u7acb\u53cd\u5e94\uff0c\u7f3a\u4e4f\u5bf9\u80fd\u6e90\u6548\u7387\u7684\u7cfb\u7edf\u6027\u8003\u8651\u3002", "method": "\u5728\u6570\u636e\u3001\u6a21\u578b\u3001\u8bad\u7ec3\u3001\u7cfb\u7edf\u548c\u63a8\u7406\u4e94\u4e2a\u9636\u6bb5\u8fdb\u884c\u6b63\u4ea4\u7ec4\u5408\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u80fd\u6e90\u6d88\u8017\u51cf\u5c11\u9ad8\u8fbe94.6%\uff0c\u540c\u65f6\u4fdd\u755995.95%\u7684\u539f\u59cb\u6027\u80fd\uff08F1\u5206\u6570\uff09\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e73\u8861\u6548\u7387\u3001\u6027\u80fd\u548c\u73af\u4fdd\u7684\u53ef\u64cd\u4f5c\u6027\u6846\u67b6\uff0c\u63a8\u52a8\u53ef\u6301\u7eedAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.18742", "pdf": "https://arxiv.org/pdf/2506.18742", "abs": "https://arxiv.org/abs/2506.18742", "authors": ["R. Lukyanenko", "O. Pastor", "V. C. Storey"], "title": "Conceptual Modelling for Life Sciences Based on Systemist Foundations", "categories": ["cs.HC"], "comment": null, "summary": "All aspects of our society, including the life sciences, need a mechanism for\npeople working within them to represent the concepts they employ to carry out\ntheir research. For the information systems being designed and developed to\nsupport researchers and scientists in conducting their work, conceptual models\nof the relevant domains are usually designed as both blueprints for a system\nbeing developed and as a means of communication between the designer and\ndeveloper. Most conceptual modelling concepts are generic in the sense that\nthey are applied with the same understanding across many applications. Problems\nin the life sciences, however, are especially complex and important, because\nthey deal with humans, their well-being, and their interactions with the\nenvironment as well as other organisms. This work proposes a systemist\nperspective for creating a conceptual model of a life scientist's problem. We\nintroduce the notion of a system and then show how it can be applied to the\ndevelopment of an information system for handling genomic-related information.\nWe extend our discussion to show how the proposed systemist perspective can\nsupport the modelling of precision medicine. This research recognizes\nchallenges in life sciences research of how to model problems to better\nrepresent the connections between physical and digital worlds. We propose a new\nnotation that explicitly incorporates systemist thinking, as well as the\ncomponents of systems based on recent ontological foundations. The new notation\ncaptures important semantics in the domain of life sciences. It may be used to\nfacilitate understanding, communication and problem-solving more broadly. We\nalso provide a precise, sound, ontologically supported characterization of the\nterm system, as a basic construct for conceptual modelling in life sciences.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u8bba\u7684\u89c6\u89d2\uff0c\u7528\u4e8e\u521b\u5efa\u751f\u547d\u79d1\u5b66\u95ee\u9898\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7b26\u53f7\u6765\u6355\u83b7\u751f\u547d\u79d1\u5b66\u9886\u57df\u7684\u91cd\u8981\u8bed\u4e49\u3002", "motivation": "\u751f\u547d\u79d1\u5b66\u95ee\u9898\u7684\u590d\u6742\u6027\u548c\u91cd\u8981\u6027\u9700\u8981\u4e00\u4e2a\u66f4\u597d\u7684\u6982\u5ff5\u6a21\u578b\u6765\u8868\u793a\u7269\u7406\u548c\u6570\u5b57\u4e16\u754c\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u7cfb\u7edf\u6982\u5ff5\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u7b26\u53f7\uff0c\u7ed3\u5408\u7cfb\u7edf\u8bba\u601d\u7ef4\u548c\u57fa\u4e8e\u672c\u4f53\u8bba\u7684\u7ec4\u4ef6\u3002", "result": "\u65b0\u7b26\u53f7\u80fd\u591f\u66f4\u597d\u5730\u8868\u793a\u751f\u547d\u79d1\u5b66\u9886\u57df\u7684\u8bed\u4e49\uff0c\u652f\u6301\u7cbe\u51c6\u533b\u5b66\u548c\u57fa\u56e0\u7ec4\u4fe1\u606f\u7cfb\u7edf\u7684\u5efa\u6a21\u3002", "conclusion": "\u7cfb\u7edf\u8bba\u89c6\u89d2\u548c\u65b0\u7b26\u53f7\u4e3a\u751f\u547d\u79d1\u5b66\u7684\u6982\u5ff5\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u548c\u89e3\u51b3\u95ee\u9898\u3002"}}
{"id": "2506.18315", "pdf": "https://arxiv.org/pdf/2506.18315", "abs": "https://arxiv.org/abs/2506.18315", "authors": ["Lehan He", "Zeren Chen", "Zhe Zhang", "Jing Shao", "Xiang Gao", "Lu Sheng"], "title": "Use Property-Based Testing to Bridge LLM Code Generation and Validation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) excel at code generation, but ensuring their\noutputs to be functionally correct, especially in complex programming tasks, is\na persistent challenge. While traditional Test-Driven Development (TDD) offers\na path for code refinement, its efficacy with LLMs is often undermined by the\nscarcity of high-quality test cases or the pitfalls of automated test\ngeneration, including biased tests or inaccurate output predictions that can\nmisdirect the correction process. This paper introduces Property-Generated\nSolver, a novel framework that leverages Property-Based Testing (PBT) to\nvalidate high-level program properties or invariants, instead of relying on\nspecific input-output examples. These properties are often simpler to define\nand verify than directly predicting exhaustive test oracles, breaking the\n\"cycle of self-deception\" where tests might share flaws with the code they are\nmeant to validate. Property-Generated Solver employs two collaborative\nLLM-based agents: a Generator dedicated to code generation and iterative\nrefinement, and a Tester that manages the PBT life-cycle and formulate\nsemantically rich feedback from property violations. The resulting\ncomprehensive and actionable feedback then guides the Generator in its\nrefinement efforts. By establishing PBT as the core validation engine within\nthis iterative, closed-loop paradigm, Property-Generated Solver provides a\nrobust mechanism for steering LLMs towards more correct and generalizable code.\nExtensive experimental results on multiple code generation benchmarks\ndemonstrate that Property-Generated Solver achieves substantial pass@1\nimprovements, ranging from 23.1% to 37.3% relative gains over established TDD\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProperty-Generated Solver\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u5c5e\u6027\u7684\u6d4b\u8bd5\uff08PBT\uff09\u9a8c\u8bc1\u4ee3\u7801\u7684\u9ad8\u5c42\u5c5e\u6027\uff0c\u800c\u975e\u5177\u4f53\u8f93\u5165-\u8f93\u51fa\u793a\u4f8b\uff0c\u4ece\u800c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u529f\u80fd\u7684\u6b63\u786e\u6027\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u3002\u4f20\u7edf\u7684\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u56e0\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u7a00\u7f3a\u6216\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u7684\u7f3a\u9677\uff08\u5982\u504f\u5dee\u6d4b\u8bd5\u6216\u4e0d\u51c6\u786e\u7684\u8f93\u51fa\u9884\u6d4b\uff09\u800c\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u6846\u67b6\u91c7\u7528\u4e24\u4e2aLLM\u4ee3\u7406\uff1aGenerator\u8d1f\u8d23\u4ee3\u7801\u751f\u6210\u4e0e\u8fed\u4ee3\u4f18\u5316\uff0cTester\u7ba1\u7406PBT\u751f\u547d\u5468\u671f\u5e76\u901a\u8fc7\u5c5e\u6027\u8fdd\u89c4\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u53cd\u9988\u3002\u8fd9\u79cd\u95ed\u73af\u8fed\u4ee3\u6a21\u5f0f\u5c06PBT\u4f5c\u4e3a\u6838\u5fc3\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProperty-Generated Solver\u76f8\u8f83\u4f20\u7edfTDD\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0cpass@1\u76f8\u5bf9\u589e\u76ca\u8fbe23.1%\u81f337.3%\u3002", "conclusion": "\u901a\u8fc7PBT\u9a8c\u8bc1\u9ad8\u5c42\u5c5e\u6027\u548c\u53cc\u4ee3\u7406\u534f\u4f5c\u673a\u5236\uff0cProperty-Generated Solver\u663e\u8457\u63d0\u9ad8\u4e86LLM\u751f\u6210\u4ee3\u7801\u7684\u6b63\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.18743", "pdf": "https://arxiv.org/pdf/2506.18743", "abs": "https://arxiv.org/abs/2506.18743", "authors": ["J. Recker", "R. Lukyanenko", "M. A. Jabbari", "B. M. Samuel", "A. Castellanos"], "title": "From Representation to Mediation: A New Agenda for Conceptual Modeling Research in A Digital World", "categories": ["cs.HC"], "comment": null, "summary": "The role of information systems (IS) as representations of real-world systems\nis changing in an increasingly digitalized world, suggesting that conceptual\nmodeling is losing its relevance to the IS field. We argue the opposite:\nConceptual modeling research is more relevant to the IS field than ever, but it\nrequires an update with current theory. We develop a new theoretical framework\nof conceptual modeling that delivers a fundamental shift in the assumptions\nthat govern research in this area. This move can make traditional knowledge\nabout conceptual modeling consistent with the emerging requirements of a\ndigital world. Our framework draws attention to the role of conceptual modeling\nscripts as mediators between physical and digital realities. We identify new\nresearch questions about grammars, methods, scripts, agents, and contexts that\nare situated in intertwined physical and digital realities. We discuss several\nimplications for conceptual modeling scholarship that relate to the necessity\nof developing new methods and grammars for conceptual modeling, broadening the\nmethodological array of conceptual modeling scholarship, and considering new\ndependent variables.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u6982\u5ff5\u5efa\u6a21\u5728\u6570\u5b57\u5316\u65f6\u4ee3\u7684\u7814\u7a76\u4ef7\u503c\u4e0d\u4ec5\u672a\u51cf\uff0c\u53cd\u800c\u66f4\u52a0\u91cd\u8981\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u7406\u8bba\u6846\u67b6\u4ee5\u9002\u914d\u6570\u5b57\u4e16\u754c\u7684\u9700\u6c42\u3002", "motivation": "\u63a2\u8ba8\u5728\u6570\u5b57\u5316\u80cc\u666f\u4e0b\uff0c\u6982\u5ff5\u5efa\u6a21\u7814\u7a76\u7684\u6301\u7eed\u91cd\u8981\u6027\u4e0e\u65b0\u7406\u8bba\u6846\u67b6\u7684\u5fc5\u8981\u6027\u3002", "method": "\u5f00\u53d1\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u6982\u5ff5\u5efa\u6a21\u811a\u672c\u4f5c\u4e3a\u7269\u7406\u4e0e\u6570\u5b57\u73b0\u5b9e\u7684\u6865\u6881\uff0c\u5e76\u63d0\u51fa\u76f8\u5173\u7814\u7a76\u95ee\u9898\u3002", "result": "\u6846\u67b6\u63d0\u5347\u4e86\u4f20\u7edf\u6982\u5ff5\u5efa\u6a21\u77e5\u8bc6\u4e0e\u6570\u5b57\u4e16\u754c\u9700\u6c42\u7684\u5951\u5408\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u6982\u5ff5\u5efa\u6a21\u7814\u7a76\u9700\u8981\u65b0\u65b9\u6cd5\u548c\u8bed\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u6570\u5b57\u65f6\u4ee3\u7684\u6311\u6218\u3002"}}
{"id": "2506.17713", "pdf": "https://arxiv.org/pdf/2506.17713", "abs": "https://arxiv.org/abs/2506.17713", "authors": ["Piyush Pradhan", "Pierre Gentine", "Shaina Kelly"], "title": "JAX-LaB: A High-Performance, Differentiable, Lattice Boltzmann Library for Modeling Multiphase Fluid Dynamics in Geosciences and Engineering", "categories": ["physics.comp-ph", "cs.DC", "physics.flu-dyn"], "comment": "31 pages, 13 figures", "summary": "We present JAX-LaB, a differentiable, Python-based Lattice Boltzmann library\nfor simulating multiphase and multiphysics flows in hydrologic, geologic, and\nengineered porous media. Built as an extension of the XLB library, JAX-LaB\nutilizes JAX for computations and offers a performant, hardware-agnostic\nimplementation that integrates seamlessly with machine learning workflows and\nscales efficiently across CPUs, GPUs, and distributed systems. Multiphase\ninteractions are modeled using the Shan-Chen pseudopotential method, which is\ncoupled with an equation of state and an improved forcing scheme to obtain\nliquid-vapor densities that are consistent with Maxwell's construction,\nenabling simulations of systems with very large density ratios while\nmaintaining minimal spurious currents. Wetting is handled using the \"improved\"\nvirtual density scheme, which allows precise control of contact angles and\neliminates non-physical films seen in other Shan-Chen wetting methods. We\nvalidate the library through several analytical benchmarks, such as Laplace's\nlaw, capillary rise, and cocurrent multicomponent flow, and demonstrate some\nexemplary use cases for the library. We also report single- and multi-GPU\nperformance scaling of the library. The library is open-source under the Apache\nlicense and available at https://github.com/piyush-ppradhan/JAX-LaB.", "AI": {"tldr": "JAX-LaB\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u53ef\u5faeLattice Boltzmann\u5e93\uff0c\u7528\u4e8e\u6a21\u62df\u6c34\u6587\u3001\u5730\u8d28\u548c\u5de5\u7a0b\u591a\u5b54\u4ecb\u8d28\u4e2d\u7684\u591a\u76f8\u548c\u591a\u7269\u7406\u6d41\u52a8\u3002\u5b83\u7ee7\u627fXLB\u5e93\uff0c\u5229\u7528JAX\u5b9e\u73b0\u9ad8\u6027\u80fd\u786c\u4ef6\u65e0\u5173\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u5e76\u652f\u6301CPU\u3001GPU\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u4e0e\u673a\u5668\u5b66\u4e60\u96c6\u6210\u7684\u591a\u76f8\u591a\u7269\u7406\u6d41\u52a8\u6a21\u62df\u5de5\u5177\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\u4e2d\u7684\u5bc6\u5ea6\u6bd4\u548c\u63a5\u89e6\u89d2\u63a7\u5236\u95ee\u9898\u3002", "method": "\u4f7f\u7528Shan-Chen\u4f2a\u52bf\u65b9\u6cd5\u5efa\u6a21\u591a\u76f8\u4ea4\u4e92\uff0c\u7ed3\u5408\u72b6\u6001\u65b9\u7a0b\u548c\u6539\u8fdb\u7684\u5f3a\u5236\u65b9\u6848\uff0c\u786e\u4fdd\u5bc6\u5ea6\u6bd4\u5927\u65f6\u6a21\u62df\u7cbe\u5ea6\uff1b\u91c7\u7528\u6539\u8fdb\u7684\u865a\u62df\u5bc6\u5ea6\u65b9\u6848\u5904\u7406\u6da6\u6e7f\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u5b9a\u5f8b\u3001\u6bdb\u7ec6\u4e0a\u5347\u7b49\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5e93\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5355GPU\u548c\u591aGPU\u7684\u6027\u80fd\u6269\u5c55\u3002", "conclusion": "JAX-LaB\u662f\u4e00\u4e2a\u5f00\u6e90\u9ad8\u6027\u80fd\u5e93\uff0c\u9002\u7528\u4e8e\u590d\u6742\u6d41\u52a8\u7cfb\u7edf\u7684\u6a21\u62df\uff0c\u5e76\u652f\u6301\u673a\u5668\u5b66\u4e60\u96c6\u6210\u548c\u591a\u786c\u4ef6\u5e73\u53f0\u3002"}}
{"id": "2506.18329", "pdf": "https://arxiv.org/pdf/2506.18329", "abs": "https://arxiv.org/abs/2506.18329", "authors": ["Elijah Zolduoarrati", "Sherlock A. Licorish", "Nigel Stanger"], "title": "Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow", "categories": ["cs.SE", "D.2.8; C.4; I.2.7; I.2.6; I.5.1"], "comment": "46 pages, 17 tables, 7 figures", "summary": "Previous studies that used data from Stack Overflow to develop predictive\nmodels often employed limited benchmarks of 3-5 models or adopted arbitrary\nselection methods. Despite being insightful, their limited scope suggests the\nneed to benchmark more models to avoid overlooking untested algorithms. Our\nstudy evaluates 21 algorithms across three tasks: predicting the number of\nquestion a user is likely to answer, their code quality violations, and their\ndropout status. We employed normalisation, standardisation, as well as\nlogarithmic and power transformations paired with Bayesian hyperparameter\noptimisation and genetic algorithms. CodeBERT, a pre-trained language model for\nboth natural and programming languages, was fine-tuned to classify user dropout\ngiven their posts (questions and answers) and code snippets. We found Bagging\nensemble models combined with standardisation achieved the highest R2 value\n(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,\nfollowed by Bagging and Epsilon Support Vector Machine models, consistently\ndemonstrated superior performance to other benchmarked algorithms in predicting\nuser code quality across multiple quality dimensions and languages. Extreme\nGradient Boosting paired with log-transformation exhibited the highest F1-score\n(0.825) in predicting user dropout. CodeBERT was able to classify user dropout\nwith a final F1-score of 0.809, validating the performance of Extreme Gradient\nBoosting that was solely based on numerical data. Overall, our benchmarking of\n21 algorithms provides multiple insights. Researchers can leverage findings\nregarding the most suitable models for specific target variables, and\npractitioners can utilise the identified optimal hyperparameters to reduce the\ninitial search space during their own hyperparameter tuning processes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e8621\u79cd\u7b97\u6cd5\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u3001\u53d8\u6362\u7b49\u65b9\u6cd5\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0Bagging\u548cCodeBERT\u7b49\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u91c7\u7528\u7684\u6a21\u578b\u8f83\u5c11\u6216\u9009\u62e9\u65b9\u6cd5\u968f\u610f\uff0c\u53ef\u80fd\u5ffd\u89c6\u672a\u6d4b\u8bd5\u7b97\u6cd5\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u3001\u53d8\u6362\u7b49\u65b9\u6cd5\u7ed3\u5408\u8d1d\u53f6\u65af\u8d85\u53c2\u6570\u4f18\u5316\u548c\u9057\u4f20\u7b97\u6cd5\uff0c\u8bc4\u4f3021\u79cd\u7b97\u6cd5\uff0c\u5e76\u5fae\u8c03CodeBERT\u6a21\u578b\u3002", "result": "Bagging\u6a21\u578b\u5728\u9884\u6d4b\u7528\u6237\u56de\u7b54\u6570\u4e0a\u8868\u73b0\u6700\u4f73\uff08R2=0.821\uff09\uff0cCodeBERT\u5728\u7528\u6237\u6d41\u5931\u5206\u7c7b\u4e2dF1-score\u4e3a0.809\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\u7684\u6307\u5bfc\uff0c\u6269\u5c55\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7684\u5e7f\u5ea6\u3002"}}
{"id": "2506.18749", "pdf": "https://arxiv.org/pdf/2506.18749", "abs": "https://arxiv.org/abs/2506.18749", "authors": ["Abdul Basit", "Maha Nawaz", "Muhammad Shafique"], "title": "BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility", "categories": ["cs.HC", "cs.AI", "cs.RO", "I.2.9; I.2.7"], "comment": "9 pages, 12 figures, Accepted at IJCNN 2025", "summary": "Non-invasive brain-computer interfaces (BCIs) have the potential to enable\nintuitive control of prosthetic limbs for individuals with upper limb\namputations. However, existing EEG-based control systems face challenges\nrelated to signal noise, classification accuracy, and real-time adaptability.\nIn this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic\nsystem that integrates ensemble learning-based EEG classification with a\nhuman-in-the-loop (HITL) correction framework for enhanced responsiveness.\nUnlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims\nto interpret EEG-driven motor intent, enabling movement control without\nreliance on residual muscle activity. To improve classification robustness,\nBRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,\nachieving a classification accuracy of 96% across test subjects. EEG signals\nare preprocessed using a bandpass filter (0.5-45 Hz), Independent Component\nAnalysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature\nextraction to minimize contamination from electromyographic (EMG) and\nelectrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic\nspeech recognition (ASR) to facilitate intuitive mode switching between\ndifferent degrees of freedom (DOF) in the prosthetic arm. The system operates\nin real time, with a response latency of 150 ms, leveraging Lab Streaming Layer\n(LSL) networking for synchronized data acquisition. The system is evaluated on\nan in-house fabricated prosthetic arm and on multiple participants highlighting\nthe generalizability across users. The system is optimized for low-power\nembedded deployment, ensuring practical real-world application beyond\nhigh-performance computing environments. Our results indicate that BRAVE offers\na promising step towards robust, real-time, non-invasive prosthetic control.", "AI": {"tldr": "BRAVE\u662f\u4e00\u79cd\u7ed3\u5408\u8111\u7535\u56fe\uff08EEG\uff09\u548c\u8bed\u97f3\u63a7\u5236\u7684\u5047\u80a2\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210\u96c6\u6210\u5b66\u4e60\u5206\u7c7b\u548c\u4eba\u673a\u4ea4\u4e92\uff08HITL\uff09\u6821\u6b63\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u548c\u4f4e\u5ef6\u8fdf\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eEEG\u7684\u5047\u80a2\u63a7\u5236\u7cfb\u7edf\u5b58\u5728\u4fe1\u53f7\u566a\u58f0\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u9002\u5e94\u6027\u7b49\u95ee\u9898\uff0cBRAVE\u65e8\u5728\u901a\u8fc7\u975e\u4fb5\u5165\u65b9\u5f0f\u5b9e\u73b0\u66f4\u76f4\u89c2\u7684\u63a7\u5236\u3002", "method": "\u91c7\u7528LSTM\u3001CNN\u548c\u968f\u673a\u68ee\u6797\u7684\u96c6\u6210\u5b66\u4e60\u6846\u67b6\u5904\u7406EEG\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u5b9e\u73b0\u6a21\u5f0f\u5207\u6362\u3002", "result": "\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523096%\uff0c\u54cd\u5e94\u5ef6\u8fdf150\u6beb\u79d2\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7528\u6237\u3002", "conclusion": "BRAVE\u4e3a\u5b9e\u65f6\u3001\u975e\u4fb5\u5165\u6027\u5047\u80a2\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.17767", "pdf": "https://arxiv.org/pdf/2506.17767", "abs": "https://arxiv.org/abs/2506.17767", "authors": ["Hsuan-Po Liu", "Hessam Mahdavifar"], "title": "A Locally Differential Private Coding-Assisted Succinct Histogram Protocol", "categories": ["cs.CR", "cs.DC", "cs.LG", "eess.SP"], "comment": null, "summary": "A succinct histogram captures frequent items and their frequencies across\nclients and has become increasingly important for large-scale,\nprivacy-sensitive machine learning applications. To develop a rigorous\nframework to guarantee privacy for the succinct histogram problem, local\ndifferential privacy (LDP) has been utilized and shown promising results. To\npreserve data utility under LDP, which essentially works by intentionally\nadding noise to data, error-correcting codes naturally emerge as a promising\ntool for reliable information collection. This work presents the first\npractical $(\\epsilon,\\delta)$-LDP protocol for constructing succinct histograms\nusing error-correcting codes. To this end, polar codes and their\nsuccessive-cancellation list (SCL) decoding algorithms are leveraged as the\nunderlying coding scheme. More specifically, our protocol introduces\nGaussian-based perturbations to enable efficient soft decoding. Experiments\ndemonstrate that our approach outperforms prior methods, particularly for items\nwith low true frequencies, while maintaining similar frequency estimation\naccuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bef\u5dee\u6821\u6b63\u7801\u7684\u5b9e\u7528\u6027$(\u03f5,\u03b4)$-LDP\u534f\u8bae\uff0c\u7528\u4e8e\u6784\u5efa\u7b80\u6d01\u76f4\u65b9\u56fe\uff0c\u5e76\u901a\u8fc7\u6781\u5316\u7801\u548c\u8f6f\u89e3\u7801\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u3001\u9690\u79c1\u654f\u611f\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u4e2d\uff0c\u7b80\u6d01\u76f4\u65b9\u56fe\uff08capturing frequent items and their frequencies\uff09\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u7136\u800c\uff0c\u5728\u672c\u5730\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u6846\u67b6\u4e0b\uff0c\u5982\u4f55\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u6570\u636e\u6548\u7528\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u5229\u7528\u6781\u5316\u7801\u53ca\u5176\u9010\u6b21\u53d6\u6d88\u5217\u8868\uff08SCL\uff09\u89e3\u7801\u7b97\u6cd5\u4f5c\u4e3a\u5e95\u5c42\u7f16\u7801\u65b9\u6848\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u9ad8\u65af\u6270\u52a8\u7684\u8f6f\u89e3\u7801\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4fe1\u606f\u6536\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u9891\u7387\u9879\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9891\u7387\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u8bef\u5dee\u6821\u6b63\u7801\u4e0eLDP\u7ed3\u5408\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u7b80\u6d01\u76f4\u65b9\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u4f4e\u9891\u9879\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.18359", "pdf": "https://arxiv.org/pdf/2506.18359", "abs": "https://arxiv.org/abs/2506.18359", "authors": ["Juanita Gomez", "Emily Lovell", "Stephanie Lieggi", "Alvaro A. Cardenas", "James Davis"], "title": "Recipe for Discovery: A Framework for Systematic Open Source Project Identification", "categories": ["cs.SE"], "comment": null, "summary": "Open source software development, particularly within institutions such as\nuniversities and research laboratories, is often decentralized and difficult to\ntrack. Despite producing highly impactful tools in science, these efforts often\ngo unrecognized due to a lack of visibility and institutional awareness. This\npaper addresses the challenge of discovering, classifying, and analyzing open\nsource software projects developed across distributed institutional systems. We\npresent a framework for systematically identifying institutional affiliated\nrepositories, using the University of California (UC) system as a case study.\n  Using GitHub's REST API, we build a pipeline to discover relevant\nrepositories and extract meaningful metadata. We then propose and evaluate\nmultiple classification strategies, including both traditional machine learning\nmodels and large language models (LLMs), to distinguish affiliated projects\nfrom unrelated repositories and generate accurate insights into the academic\nopen source landscape. Our results show that the framework is effective at\nscale, discovering over 52,000 repositories and predicting institutional\naffiliation with high accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u8bc6\u522b\u548c\u5206\u7c7b\u5927\u5b66\u548c\u7814\u7a76\u5b9e\u9a8c\u5ba4\u7b49\u673a\u6784\u5f00\u53d1\u7684\u5f00\u653e\u6e90\u4ee3\u7801\u8f6f\u4ef6\u9879\u76ee\u7684\u6846\u67b6\uff0c\u5e76\u4ee5\u52a0\u5dde\u5927\u5b66\u7cfb\u7edf\u4e3a\u4f8b\uff0c\u901a\u8fc7GitHub API\u6784\u5efa\u7ba1\u9053\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u9ad8\u6548\u8bc6\u522b\u4e865.2\u4e07\u4e2a\u76f8\u5173\u4ed3\u5e93\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u6e90\u4ee3\u7801\u5f00\u53d1\u4e2d\u56e0\u5206\u6563\u6027\u548c\u7f3a\u4e4f\u53ef\u89c1\u6027\u5bfc\u81f4\u7684\u9879\u76ee\u96be\u4ee5\u8ffd\u8e2a\u548c\u8ba4\u53ef\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528GitHub REST API\u6784\u5efa\u7ba1\u9053\uff0c\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4ed3\u5e93\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6210\u529f\u53d1\u73b05.2\u4e07\u4e2a\u4ed3\u5e93\uff0c\u5e76\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u4e86\u673a\u6784\u4ece\u5c5e\u5173\u7cfb\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89c4\u6a21\u5316\u8bc6\u522b\u548c\u5206\u6790\u5b66\u672f\u5f00\u653e\u6e90\u4ee3\u7801\u9879\u76ee\u3002"}}
{"id": "2506.18760", "pdf": "https://arxiv.org/pdf/2506.18760", "abs": "https://arxiv.org/abs/2506.18760", "authors": ["Adarsa Sivaprasad", "Ehud Reiter", "David McLernon", "Nava Tintarev", "Siladitya Bhattacharya", "Nir Oren"], "title": "Patient-Centred Explainability in IVF Outcome Prediction", "categories": ["cs.HC"], "comment": null, "summary": "This paper evaluates the user interface of an in vitro fertility (IVF)\noutcome prediction tool, focussing on its understandability for patients or\npotential patients. We analyse four years of anonymous patient feedback,\nfollowed by a user survey and interviews to quantify trust and\nunderstandability. Results highlight a lay user's need for prediction model\n\\emph{explainability} beyond the model feature space. We identify user concerns\nabout data shifts and model exclusions that impact trust. The results call\nattention to the shortcomings of current practices in explainable AI research\nand design and the need for explainability beyond model feature space and\nepistemic assumptions, particularly in high-stakes healthcare contexts where\nusers gather extensive information and develop complex mental models. To\naddress these challenges, we propose a dialogue-based interface and explore\nuser expectations for personalised explanations.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u8bd5\u7ba1\u5a74\u513f\u7ed3\u679c\u9884\u6d4b\u5de5\u5177\u7684\u7528\u6237\u754c\u9762\uff0c\u91cd\u70b9\u5173\u6ce8\u60a3\u8005\u6216\u6f5c\u5728\u60a3\u8005\u5bf9\u5176\u7406\u89e3\u7684\u7a0b\u5ea6\u3002\u901a\u8fc7\u5206\u6790\u533f\u540d\u60a3\u8005\u53cd\u9988\u3001\u7528\u6237\u8c03\u67e5\u548c\u8bbf\u8c08\uff0c\u91cf\u5316\u4e86\u4fe1\u4efb\u548c\u7406\u89e3\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u666e\u901a\u7528\u6237\u9700\u8981\u8d85\u51fa\u6a21\u578b\u7279\u5f81\u7a7a\u95f4\u7684\u89e3\u91ca\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63d0\u9ad8\u60a3\u8005\u5728\u8bd5\u7ba1\u5a74\u513f\u7ed3\u679c\u9884\u6d4b\u5de5\u5177\u4e2d\u7684\u7406\u89e3\u548c\u4fe1\u4efb\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u533b\u7597\u80cc\u666f\u4e0b\uff0c\u7528\u6237\u9700\u8981\u8d85\u51fa\u4f20\u7edf\u6a21\u578b\u89e3\u91ca\u8303\u56f4\u7684\u4e2a\u6027\u5316\u89e3\u91ca\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u56db\u5e74\u7684\u533f\u540d\u60a3\u8005\u53cd\u9988\u3001\u8fdb\u884c\u7528\u6237\u8c03\u67e5\u548c\u8bbf\u8c08\uff0c\u4ee5\u53ca\u63d0\u51fa\u57fa\u4e8e\u5bf9\u8bdd\u7684\u754c\u9762\u4ee5\u89e3\u51b3\u7528\u6237\u9700\u6c42\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u6237\u5bf9\u6570\u636e\u53d8\u5316\u548c\u6a21\u578b\u6392\u9664\u7684\u62c5\u5fe7\u5f71\u54cd\u4e86\u4fe1\u4efb\uff0c\u5f53\u524d\u7684\u89e3\u91ca\u6027AI\u7814\u7a76\u548c\u8bbe\u8ba1\u672a\u80fd\u6ee1\u8db3\u7528\u6237\u5bf9\u9884\u6d4b\u6a21\u578b\u89e3\u91ca\u6027\u7684\u9700\u6c42\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\u547c\u5401\u5728\u9ad8\u98ce\u9669\u533b\u7597\u80cc\u666f\u4e0b\uff0c\u9700\u8981\u8d85\u8d8a\u6a21\u578b\u7279\u5f81\u7a7a\u95f4\u548c\u8ba4\u77e5\u5047\u8bbe\u7684\u89e3\u91ca\u6027\u8bbe\u8ba1\uff0c\u5e76\u63a2\u7d22\u4e2a\u6027\u5316\u89e3\u91ca\u7684\u7528\u6237\u671f\u671b\u3002"}}
{"id": "2506.17870", "pdf": "https://arxiv.org/pdf/2506.17870", "abs": "https://arxiv.org/abs/2506.17870", "authors": ["Jianhang Xie", "Chuntao Ding", "Xiaqing Li", "Shenyuan Ren", "Yidong Li", "Zhichao Lu"], "title": "NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "IEEE Transactions on Mobile Computing, accepted manuscript, DOI:\n  10.1109/TMC.2025.3582583; Code: https://github.com/jianhayes/NESTQUANT", "summary": "Deploying quantized deep neural network (DNN) models with resource adaptation\ncapabilities on ubiquitous Internet of Things (IoT) devices to provide\nhigh-quality AI services can leverage the benefits of compression and meet\nmulti-scenario resource requirements. However, existing dynamic/mixed precision\nquantization requires retraining or special hardware, whereas post-training\nquantization (PTQ) has two limitations for resource adaptation: (i) The\nstate-of-the-art PTQ methods only provide one fixed bitwidth model, which makes\nit challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying\nmultiple PTQ models with diverse bitwidths consumes large storage resources and\nswitching overheads. To this end, this paper introduces a resource-friendly\npost-training integer-nesting quantization, i.e., NestQuant, for on-device\nquantized model switching on IoT devices. The proposed NestQuant incorporates\nthe integer weight decomposition, which bit-wise splits quantized weights into\nhigher-bit and lower-bit weights of integer data types. It also contains a\ndecomposed weights nesting mechanism to optimize the higher-bit weights by\nadaptive rounding and nest them into the original quantized weights. In\ndeployment, we can send and store only one NestQuant model and switch between\nthe full-bit/part-bit model by paging in/out lower-bit weights to adapt to\nresource changes and reduce consumption. Experimental results on the\nImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve\nhigh performance in top-1 accuracy, and reduce in terms of data transmission,\nstorage consumption, and switching overheads. In particular, the ResNet-101\nwith INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and\npart-bit models, respectively, and reduce switching overheads by approximately\n78.1% compared with diverse bitwidths PTQ models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNestQuant\u7684\u8d44\u6e90\u53cb\u597d\u578b\u540e\u8bad\u7ec3\u6574\u6570\u5d4c\u5957\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u5b9e\u73b0\u91cf\u5316\u6a21\u578b\u5207\u6362\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u7269\u8054\u7f51\u8bbe\u5907\u8d44\u6e90\u53d8\u5316\u4ee5\u53ca\u90e8\u7f72\u591a\u6a21\u578b\u5e26\u6765\u7684\u5b58\u50a8\u548c\u5207\u6362\u5f00\u9500\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6574\u6570\u6743\u91cd\u5206\u89e3\u548c\u5d4c\u5957\u673a\u5236\uff0c\u5c06\u91cf\u5316\u6743\u91cd\u6309\u4f4d\u62c6\u5206\u4e3a\u9ad8\u4f4d\u548c\u4f4e\u4f4d\u6574\u6570\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u820d\u5165\u4f18\u5316\u9ad8\u4f4d\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNestQuant\u5728ImageNet-1K\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u663e\u8457\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u3001\u5b58\u50a8\u548c\u5207\u6362\u5f00\u9500\u3002\u5982ResNet-101\u5728INT8\u5d4c\u5957INT6\u65f6\uff0c\u5207\u6362\u5f00\u9500\u964d\u4f4e78.1%\u3002", "conclusion": "NestQuant\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8d44\u6e90\u81ea\u9002\u5e94\u91cf\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7269\u8054\u7f51\u8bbe\u5907\u90e8\u7f72\uff0c\u5177\u6709\u6027\u80fd\u9ad8\u3001\u5f00\u9500\u4f4e\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.18394", "pdf": "https://arxiv.org/pdf/2506.18394", "abs": "https://arxiv.org/abs/2506.18394", "authors": ["Xiao Cheng", "Zhihao Guo", "Huan Huo", "Yulei Sui"], "title": "Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval", "categories": ["cs.SE"], "comment": null, "summary": "Memory-related errors in C programming continue to pose significant\nchallenges in software development, primarily due to the complexities of manual\nmemory management inherent in the language. These errors frequently serve as\nvectors for severe vulnerabilities, while their repair requires extensive\nknowledge of program logic and C's memory model. Automated Program Repair (APR)\nhas emerged as a critical research area to address these challenges.\nTraditional APR approaches rely on expert-designed strategies and predefined\ntemplates, which are labor-intensive and constrained by the effectiveness of\nmanual specifications. Deep learning techniques offer a promising alternative\nby automatically extracting repair patterns, but they require substantial\ntraining datasets and often lack interpretability.\n  This paper introduces LTFix, a novel approach that harnesses the potential of\nLarge Language Models (LLMs) for automated memory error repair, especially for\ncomplex repository-level errors that span multiple functions and files. We\naddress two fundamental challenges in LLM-based memory error repair: a limited\nunderstanding of interprocedural memory management patterns and context window\nlimitations for repository-wide analysis. Our approach utilizes a finite\ntypestate automaton to guide the tracking of error-propagation paths and\ncontext trace, capturing both spatial (memory states) and temporal (execution\nhistory) dimensions of error behavior. This typestate-guided context retrieval\nstrategy provides the LLM with concise yet semantically rich information\nrelevant to erroneous memory management, effectively addressing the token\nlimitation of LLMs.", "AI": {"tldr": "LTFix\u662f\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u4fee\u590dC\u8bed\u8a00\u5185\u5b58\u9519\u8bef\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6709\u9650\u7c7b\u578b\u72b6\u6001\u81ea\u52a8\u673a\u8ffd\u8e2a\u9519\u8bef\u4f20\u64ad\u8def\u5f84\u548c\u4e0a\u4e0b\u6587\u8f68\u8ff9\uff0c\u89e3\u51b3LLM\u5728\u8de8\u51fd\u6570\u548c\u6587\u4ef6\u7ea7\u9519\u8bef\u4fee\u590d\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7531\u4e8eC\u8bed\u8a00\u624b\u52a8\u5185\u5b58\u7ba1\u7406\u7684\u590d\u6742\u6027\uff0c\u5185\u5b58\u76f8\u5173\u9519\u8bef\u5e38\u5e38\u5bfc\u81f4\u4e25\u91cd\u6f0f\u6d1e\uff0c\u800c\u4f20\u7edf\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u8bbe\u8ba1\u7b56\u7565\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u80fd\u81ea\u52a8\u63d0\u53d6\u4fee\u590d\u6a21\u5f0f\u4f46\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "LTFix\u91c7\u7528\u6709\u9650\u7c7b\u578b\u72b6\u6001\u81ea\u52a8\u673a\uff0c\u6307\u5bfc\u8ffd\u8e2a\u9519\u8bef\u4f20\u64ad\u8def\u5f84\u548c\u4e0a\u4e0b\u6587\u8f68\u8ff9\uff0c\u7ed3\u5408\u5185\u5b58\u72b6\u6001\u548c\u6267\u884c\u5386\u53f2\u7684\u65f6\u7a7a\u7ef4\u5ea6\uff0c\u4e3aLLM\u63d0\u4f9b\u7b80\u6d01\u800c\u8bed\u4e49\u4e30\u5bcc\u7684\u4fe1\u606f\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u7406\u89e3\u8de8\u8fc7\u7a0b\u5185\u5b58\u7ba1\u7406\u6a21\u5f0f\u548c\u5b58\u50a8\u5e93\u8303\u56f4\u5206\u6790\u4e2d\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u95ee\u9898\u3002", "conclusion": "LTFix\u4e3aLLM\u5728\u590d\u6742\u5b58\u50a8\u5e93\u7ea7\u5185\u5b58\u9519\u8bef\u4fee\u590d\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e73\u8861\u4e86\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2506.18770", "pdf": "https://arxiv.org/pdf/2506.18770", "abs": "https://arxiv.org/abs/2506.18770", "authors": ["Aditya Bhattacharya", "Simone Stumpf", "Katrien Verbert"], "title": "Importance of User Control in Data-Centric Steering for Healthcare Experts", "categories": ["cs.HC"], "comment": "It is a pre-print version. For the full paper, please view the actual\n  published version", "summary": "As Artificial Intelligence (AI) becomes increasingly integrated into\nhigh-stakes domains like healthcare, effective collaboration between healthcare\nexperts and AI systems is critical. Data-centric steering, which involves\nfine-tuning prediction models by improving training data quality, plays a key\nrole in this process. However, little research has explored how varying levels\nof user control affect healthcare experts during data-centric steering. We\naddress this gap by examining manual and automated steering approaches through\na between-subjects, mixed-methods user study with 74 healthcare experts. Our\nfindings show that manual steering, which grants direct control over training\ndata, significantly improves model performance while maintaining trust and\nsystem understandability. Based on these findings, we propose design\nimplications for a hybrid steering system that combines manual and automated\napproaches to increase user involvement during human-AI collaboration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u533b\u7597\u9886\u57df\u4e2d\u4eba\u673a\u534f\u4f5c\u4e2d\u7528\u6237\u63a7\u5236\u6c34\u5e73\u5bf9\u6570\u636e\u4e2d\u5fc3\u8f6c\u5411\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u624b\u52a8\u8f6c\u5411\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u4fdd\u6301\u4fe1\u4efb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u624b\u52a8\u548c\u81ea\u52a8\u65b9\u6cd5\u7684\u6df7\u5408\u8f6c\u5411\u7cfb\u7edf\u8bbe\u8ba1\u3002", "motivation": "\u968f\u7740AI\u5728\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u7814\u7a76\u7528\u6237\u63a7\u5236\u6c34\u5e73\u5982\u4f55\u5f71\u54cd\u533b\u7597\u4e13\u5bb6\u5728\u6570\u636e\u4e2d\u5fc3\u8f6c\u5411\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u6548\u679c\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u5305\u542b74\u540d\u533b\u7597\u4e13\u5bb6\u7684\u6df7\u5408\u65b9\u6cd5\u7528\u6237\u7814\u7a76\uff0c\u6bd4\u8f83\u624b\u52a8\u548c\u81ea\u52a8\u8f6c\u5411\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u624b\u52a8\u8f6c\u5411\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4fdd\u6301\u7528\u6237\u5bf9\u7cfb\u7edf\u7684\u4fe1\u4efb\u548c\u7406\u89e3\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u624b\u52a8\u548c\u81ea\u52a8\u8f6c\u5411\u7684\u6df7\u5408\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4ee5\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2506.17900", "pdf": "https://arxiv.org/pdf/2506.17900", "abs": "https://arxiv.org/abs/2506.17900", "authors": ["Cheng Ji", "Huaiying Luo"], "title": "Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms", "categories": ["cs.AI", "cs.DC"], "comment": "Accepted by 2025 8th International Conference on Advanced Electronic\n  Materials, Computers and Software Engineering (AEMCSE 2025)", "summary": "With the increasing complexity and rapid expansion of the scale of AI systems\nin cloud platforms, the log data generated during system operation is massive,\nunstructured, and semantically ambiguous, which brings great challenges to\nfault location and system self-repair. In order to solve this problem, this\npaper proposes an intelligent log processing and automatic debugging framework\nbased on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This\nmethod is extended on the basis of the existing pre-trained Transformer model,\nand integrates a multi-stage semantic inference mechanism to realize the\ncontext understanding of system logs and the automatic reconstruction of fault\nchains. Firstly, the system log is dynamically structured, and the unsupervised\nclustering and embedding mechanism is used to extract the event template and\nsemantic schema. Subsequently, the fine-tuned LLM combined with the multi-round\nattention mechanism to perform contextual reasoning on the log sequence to\ngenerate potential fault assumptions and root cause paths. Furthermore, this\npaper introduces a reinforcement learning-based policy-guided recovery planner,\nwhich is driven by the remediation strategy generated by LLM to support dynamic\ndecision-making and adaptive debugging in the cloud environment. Compared with\nthe existing rule engine or traditional log analysis system, the proposed model\nhas stronger semantic understanding ability, continuous learning ability and\nheterogeneous environment adaptability. Experiments on the cloud platform log\ndataset show that LLM-ID improves the fault location accuracy by 16.2%, which\nis significantly better than the current mainstream methods", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u65e5\u5fd7\u5904\u7406\u4e0e\u81ea\u52a8\u8c03\u8bd5\u6846\u67b6LLM-ID\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u8bed\u4e49\u63a8\u7406\u673a\u5236\u63d0\u5347\u6545\u969c\u5b9a\u4f4d\u51c6\u786e\u738716.2%\u3002", "motivation": "\u89e3\u51b3\u4e91\u5e73\u53f0AI\u7cfb\u7edf\u65e5\u5fd7\u6570\u636e\u91cf\u5927\u3001\u975e\u7ed3\u6784\u5316\u548c\u8bed\u4e49\u6a21\u7cca\u5bfc\u81f4\u7684\u6545\u969c\u5b9a\u4f4d\u4e0e\u81ea\u4fee\u590d\u96be\u9898\u3002", "method": "\u6269\u5c55\u9884\u8bad\u7ec3Transformer\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u9636\u6bb5\u8bed\u4e49\u63a8\u7406\u3001\u65e0\u76d1\u7763\u805a\u7c7b\u4e0e\u5d4c\u5165\u673a\u5236\u3001\u591a\u8f6e\u6ce8\u610f\u529b\u673a\u5236\u53ca\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5f15\u5bfc\u7684\u6062\u590d\u89c4\u5212\u5668\u3002", "result": "\u5728\u4e91\u5e73\u53f0\u65e5\u5fd7\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cLLM-ID\u6545\u969c\u5b9a\u4f4d\u51c6\u786e\u7387\u63d0\u534716.2%\uff0c\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "LLM-ID\u5177\u5907\u66f4\u5f3a\u7684\u8bed\u4e49\u7406\u89e3\u3001\u6301\u7eed\u5b66\u4e60\u548c\u5f02\u6784\u73af\u5883\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u4e91\u73af\u5883\u8c03\u8bd5\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18398", "pdf": "https://arxiv.org/pdf/2506.18398", "abs": "https://arxiv.org/abs/2506.18398", "authors": ["Hao Wu", "Haijun Wang", "Shangwang Li", "Yin Wu", "Ming Fan", "Wuxia Jin", "Yitao Zhao", "Ting Liu"], "title": "Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis", "categories": ["cs.SE"], "comment": null, "summary": "Rug pull scams have emerged as a persistent threat to cryptocurrency, causing\nsignificant financial losses. A typical scenario involves scammers deploying\nhoneypot contracts to attract investments, restricting token sales, and\ndraining the funds, which leaves investors with worthless tokens. Current\nmethods either rely on predefined patterns to detect code risks or utilize\nstatistical transaction data to train detection models. However, real-world Rug\nPull schemes often involve a complex interplay between malicious code and\nsuspicious transaction behaviors. These methods, which solely focus on one\naspect, fall short in detecting such schemes effectively.\n  In this paper, we propose RPhunter, a novel technique that integrates code\nand transaction for Rug Pull detection. First, RPhunter establishes declarative\nrules and performs flow analysis to extract code risk information, further\nconstructing a semantic risk code graph (SRCG). Meanwhile, to leverage\ntransaction information, RPhunter formulates dynamic token transaction\nactivities as a token flow behavior graph (TFBG) in which nodes and edges are\ncharacterized from network structure and market manipulation perspectives.\nFinally, RPhunter employs graph neural networks to extract complementary\nfeatures from SRCG and TFBG, integrating them through an attention fusion model\nto enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull\nincidents from code and transaction aspects and constructed a ground-truth\ndataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,\na recall of 93.8% and an F1 score of 94.5%, which highlights superior\nperformance compared to existing state-of-the-art methods. Furthermore, when\napplied to the real-world scenarios, RPhunter has identified 4801 Rug Pull\ntokens, achieving a precision of 91%.", "AI": {"tldr": "RPhunter\u901a\u8fc7\u7ed3\u5408\u4ee3\u7801\u548c\u4ea4\u6613\u6570\u636e\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u52a0\u5bc6\u8d27\u5e01\u4e2d\u7684Rug Pull\u9a97\u5c40\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684Rug Pull\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4ee3\u7801\u6216\u4ea4\u6613\u884c\u4e3a\u5355\u4e00\u65b9\u9762\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7684\u9a97\u5c40\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7efc\u5408\u65b9\u6cd5\u3002", "method": "RPhunter\u7ed3\u5408\u4ee3\u7801\u98ce\u9669\u56fe\uff08SRCG\uff09\u548c\u4ea4\u6613\u884c\u4e3a\u56fe\uff08TFBG\uff09\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u878d\u5408\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u5728\u4e00\u7ec4645\u4e2a\u771f\u5b9e\u6848\u4f8b\u4e2d\uff0cRPhunter\u7684\u7cbe\u786e\u7387\u8fbe\u523095.3%\uff0c\u53ec\u56de\u738793.8%\uff0cF1\u5206\u657094.5%\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8bc6\u522b\u51fa4801\u4e2aRug Pull\u4ee3\u5e01\u3002", "conclusion": "RPhunter\u901a\u8fc7\u6574\u5408\u4ee3\u7801\u548c\u4ea4\u6613\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86Rug Pull\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2506.18786", "pdf": "https://arxiv.org/pdf/2506.18786", "abs": "https://arxiv.org/abs/2506.18786", "authors": ["Yitong Zhu", "Guanxuan Jiang", "Zhuowen Liang", "Yuyang Wang"], "title": "Flow-Aware Diffusion for Real-Time VR Restoration: Enhancing Spatiotemporal Coherence and Efficiency", "categories": ["cs.HC"], "comment": null, "summary": "Cybersickness remains a critical barrier to the widespread adoption of\nVirtual Reality (VR), particularly in scenarios involving intense or artificial\nmotion cues. Among the key contributors is excessive optical flow-perceived\nvisual motion that, when unmatched by vestibular input, leads to sensory\nconflict and discomfort. While previous efforts have explored geometric or\nhardware based mitigation strategies, such methods often rely on predefined\nscene structures, manual tuning, or intrusive equipment. In this work, we\npropose U-MAD, a lightweight, real-time, AI-based solution that suppresses\nperceptually disruptive optical flow directly at the image level. Unlike prior\nhandcrafted approaches, this method learns to attenuate high-intensity motion\npatterns from rendered frames without requiring mesh-level editing or scene\nspecific adaptation. Designed as a plug and play module, U-MAD integrates\nseamlessly into existing VR pipelines and generalizes well to procedurally\ngenerated environments. The experiments show that U-MAD consistently reduces\naverage optical flow and enhances temporal stability across diverse scenes. A\nuser study further confirms that reducing visual motion leads to improved\nperceptual comfort and alleviated cybersickness symptoms. These findings\ndemonstrate that perceptually guided modulation of optical flow provides an\neffective and scalable approach to creating more user-friendly immersive\nexperiences. The code will be released at https://github.com/XXXXX (upon\npublication).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86U-MAD\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5b9e\u65f6\u3001\u57fa\u4e8eAI\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6291\u5236\u56fe\u50cf\u5c42\u9762\u7684\u5149\u5b66\u6d41\u52a8\u6765\u51cf\u8f7b\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u6655\u52a8\u75c7\u3002", "motivation": "\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u8fc7\u5ea6\u5149\u5b66\u6d41\u52a8\u4e0e\u4f53\u611f\u8f93\u5165\u4e0d\u5339\u914d\u5bfc\u81f4\u611f\u5b98\u51b2\u7a81\u548c\u4e0d\u9002\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u573a\u666f\u7ed3\u6784\u6216\u786c\u4ef6\uff0c\u96be\u4ee5\u7075\u6d3b\u9002\u7528\u3002", "method": "\u63d0\u51faU-MAD\uff0c\u901a\u8fc7AI\u5b66\u4e60\u76f4\u63a5\u5728\u6e32\u67d3\u5e27\u4e2d\u51cf\u5c11\u9ad8\u5f3a\u5ea6\u8fd0\u52a8\u6a21\u5f0f\uff0c\u65e0\u9700\u7f51\u683c\u7f16\u8f91\u6216\u573a\u666f\u9002\u914d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eU-MAD\u80fd\u51cf\u5c11\u5149\u5b66\u6d41\u52a8\u5e76\u63d0\u5347\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\u5176\u6709\u6548\u7f13\u89e3\u4e86\u6655\u52a8\u75c7\u75c7\u72b6\u3002", "conclusion": "\u5149\u5b66\u6d41\u52a8\u7684\u611f\u77e5\u8c03\u5236\u662f\u521b\u9020\u66f4\u53cb\u597d\u6c89\u6d78\u4f53\u9a8c\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.18403", "pdf": "https://arxiv.org/pdf/2506.18403", "abs": "https://arxiv.org/abs/2506.18403", "authors": ["Muntasir Adnan", "Carlos C. N. Kuhn"], "title": "The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The effectiveness of AI debugging follows a predictable exponential decay\npattern; most models lose 60-80% of their debugging capability within just 2-3\nattempts, despite iterative debugging being a critical capability for practical\ncode generation systems. We introduce the Debugging Decay Index (DDI), a\nmathematical framework that quantifies when debugging becomes ineffective and\npredicts intervention points. Our strategic fresh start approach shifts from\nexploitation to exploration at strategic points in the debugging process,\ndemonstrating that well-timed interventions can rescue the effectiveness of\ndebugging. DDI reveals a fundamental limitation in current AI debugging and\nprovides the first quantitative framework for optimising iterative code\ngeneration strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aDebugging Decay Index (DDI)\u7684\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316AI\u8c03\u8bd5\u6548\u679c\u968f\u5c1d\u8bd5\u6b21\u6570\u8870\u51cf\u7684\u60c5\u51b5\uff0c\u5e76\u901a\u8fc7\u9002\u65f6\u5e72\u9884\u6062\u590d\u8c03\u8bd5\u6548\u7387\u3002", "motivation": "AI\u8c03\u8bd5\u80fd\u529b\u5728\u77ed\u671f\u5185\u663e\u8457\u8870\u51cf\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7684\u6548\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u91cf\u5316\u8fd9\u79cd\u8870\u51cf\u5e76\u4f18\u5316\u8c03\u8bd5\u7b56\u7565\u3002", "method": "\u5f15\u5165\u4e86Debugging Decay Index (DDI)\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u6218\u7565\u6027\u2018\u91cd\u65b0\u5f00\u59cb\u2019\u65b9\u6cd5\uff0c\u5728\u8c03\u8bd5\u8fc7\u7a0b\u4e2d\u9002\u65f6\u4ece\u5229\u7528\u8f6c\u5411\u63a2\u7d22\u3002", "result": "DDI\u63ed\u793a\u4e86\u5f53\u524dAI\u8c03\u8bd5\u7684\u6839\u672c\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u9002\u65f6\u5e72\u9884\u663e\u8457\u63d0\u5347\u4e86\u8c03\u8bd5\u6548\u7387\u3002", "conclusion": "DDI\u4e3a\u4f18\u5316\u8fed\u4ee3\u4ee3\u7801\u751f\u6210\u7b56\u7565\u63d0\u4f9b\u4e86\u9996\u4e2a\u91cf\u5316\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u7684AI\u8c03\u8bd5\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.17314", "pdf": "https://arxiv.org/pdf/2506.17314", "abs": "https://arxiv.org/abs/2506.17314", "authors": ["Adnan Qidwai", "Srija Mukhopadhyay", "Prerana Khatiwada", "Dan Roth", "Vivek Gupta"], "title": "PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights", "categories": ["cs.CL", "cs.HC"], "comment": "9 Pages, 9 Figures. Accepted at ACL 2025 System Demonstration Track", "summary": "Accurate and complete product descriptions are crucial for e-commerce, yet\nseller-provided information often falls short. Customer reviews offer valuable\ndetails but are laborious to sift through manually. We present PRAISE: Product\nReview Attribute Insight Structuring Engine, a novel system that uses Large\nLanguage Models (LLMs) to automatically extract, compare, and structure\ninsights from customer reviews and seller descriptions. PRAISE provides users\nwith an intuitive interface to identify missing, contradictory, or partially\nmatching details between these two sources, presenting the discrepancies in a\nclear, structured format alongside supporting evidence from reviews. This\nallows sellers to easily enhance their product listings for clarity and\npersuasiveness, and buyers to better assess product reliability. Our\ndemonstration showcases PRAISE's workflow, its effectiveness in generating\nactionable structured insights from unstructured reviews, and its potential to\nsignificantly improve the quality and trustworthiness of e-commerce product\ncatalogs.", "AI": {"tldr": "PRAISE\u7cfb\u7edf\u5229\u7528LLMs\u81ea\u52a8\u4ece\u5ba2\u6237\u8bc4\u4ef7\u548c\u5356\u5bb6\u63cf\u8ff0\u4e2d\u63d0\u53d6\u3001\u5bf9\u6bd4\u548c\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u5e2e\u52a9\u63d0\u5347\u7535\u5546\u4ea7\u54c1\u63cf\u8ff0\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u6539\u5584\u7535\u5546\u5e73\u53f0\u4e0a\u4ea7\u54c1\u63cf\u8ff0\u7684\u4e0d\u8db3\uff0c\u5229\u7528\u5ba2\u6237\u8bc4\u4ef7\u4e2d\u7684\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u63d0\u5347\u4e70\u5356\u53cc\u65b9\u7684\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u63d0\u53d6\u5e76\u7ed3\u6784\u5316\u5ba2\u6237\u8bc4\u4ef7\u548c\u5356\u5bb6\u63cf\u8ff0\u4e2d\u7684\u4fe1\u606f\uff0c\u63d0\u4f9b\u754c\u9762\u5c55\u793a\u5dee\u5f02\u3002", "result": "PRAISE\u80fd\u591f\u751f\u6210\u7ed3\u6784\u5316\u7684\u3001\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4ea7\u54c1\u76ee\u5f55\u7684\u8d28\u91cf\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "PRAISE\u901a\u8fc7\u81ea\u52a8\u5316\u5904\u7406\u5ba2\u6237\u8bc4\u4ef7\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ea7\u54c1\u63cf\u8ff0\u4e0d\u5b8c\u5584\u7684\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.18193", "pdf": "https://arxiv.org/pdf/2506.18193", "abs": "https://arxiv.org/abs/2506.18193", "authors": ["Zih-Hao Huang", "You-Teng Lin", "Hung-Hsuan Chen"], "title": "DeInfoReg: A Decoupled Learning Framework for Better Training Throughput", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "This paper introduces Decoupled Supervised Learning with Information\nRegularization (DeInfoReg), a novel approach that transforms a long gradient\nflow into multiple shorter ones, thereby mitigating the vanishing gradient\nproblem. Integrating a pipeline strategy, DeInfoReg enables model\nparallelization across multiple GPUs, significantly improving training\nthroughput. We compare our proposed method with standard backpropagation and\nother gradient flow decomposition techniques. Extensive experiments on diverse\ntasks and datasets demonstrate that DeInfoReg achieves superior performance and\nbetter noise resistance than traditional BP models and efficiently utilizes\nparallel computing resources. The code for reproducibility is available at:\nhttps://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.", "AI": {"tldr": "DeInfoReg\u901a\u8fc7\u5206\u89e3\u68af\u5ea6\u6d41\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u5e76\u884c\u8ba1\u7b97\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u3002", "motivation": "\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5e76\u5229\u7528\u5e76\u884c\u8ba1\u7b97\u63d0\u9ad8\u8bad\u7ec3\u541e\u5410\u91cf\u3002", "method": "\u91c7\u7528\u5206\u89e3\u68af\u5ea6\u6d41\u548c\u4fe1\u606f\u6b63\u5219\u5316\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u591aGPU\u5e76\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u6297\u566a\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "DeInfoReg\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5e76\u884c\u8ba1\u7b97\u73af\u5883\u3002"}}
{"id": "2506.18790", "pdf": "https://arxiv.org/pdf/2506.18790", "abs": "https://arxiv.org/abs/2506.18790", "authors": ["Mohamad Omar Nachawati"], "title": "ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering", "categories": ["cs.SE"], "comment": null, "summary": "This paper introduces ModeliHub, a Web-based, federated analytics platform\ndesigned specifically for model-based systems engineering with Modelica.\nModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke\nfederation architecture that provides systems engineers with a Modelica-based,\nunified system model of repositories containing heterogeneous engineering\nartifacts. From this unified system model, ModeliHub's Virtual Twin engine\nprovides a real-time, interactive simulation environment for deploying Modelica\nsimulation models that represent digital twins of the virtual prototype of the\nsystem under development at a particular iteration of the iterative systems\nengineering life cycle. The implementation of ModeliHub is centered around its\nextensible, Modelica compiler frontend developed in Isomorphic TypeScript that\ncan run seamlessly across browser, desktop and server environments. This\narchitecture aims to strike a balance between rigor and agility, enabling\nseamless integration and analysis across various engineering domains.", "AI": {"tldr": "ModeliHub\u662f\u4e00\u4e2a\u57fa\u4e8eWeb\u7684\u8054\u90a6\u5206\u6790\u5e73\u53f0\uff0c\u4e13\u4e3aModelica\u6a21\u578b\u7cfb\u7edf\u5de5\u7a0b\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u7cfb\u7edf\u6a21\u578b\u548c\u5b9e\u65f6\u4eff\u771f\u73af\u5883\u3002", "motivation": "\u4e3a\u7cfb\u7edf\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e00\u4e2a\u57fa\u4e8eModelica\u7684\u7edf\u4e00\u5e73\u53f0\uff0c\u652f\u6301\u5f02\u6784\u5de5\u7a0b\u5de5\u4ef6\u7684\u96c6\u6210\u548c\u5206\u6790\u3002", "method": "\u91c7\u7528\u4e2d\u5fc3\u8f90\u5c04\u5f0f\u8054\u90a6\u67b6\u6784\uff0c\u901a\u8fc7Modelica\u7f16\u8bd1\u5668\u524d\u7aef\u5b9e\u73b0\u8de8\u6d4f\u89c8\u5668\u3001\u684c\u9762\u548c\u670d\u52a1\u5668\u7684\u65e0\u7f1d\u8fd0\u884c\u3002", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4ea4\u4e92\u7684\u6570\u5b57\u5b6a\u751f\u4eff\u771f\u73af\u5883\uff0c\u652f\u6301\u5de5\u7a0b\u9886\u57df\u7684\u65e0\u7f1d\u96c6\u6210\u4e0e\u5206\u6790\u3002", "conclusion": "ModeliHub\u5728\u7cfb\u7edf\u5de5\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u4e25\u8c28\u4e0e\u654f\u6377\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u7684\u5de5\u7a0b\u5206\u6790\u3002"}}
{"id": "2506.17356", "pdf": "https://arxiv.org/pdf/2506.17356", "abs": "https://arxiv.org/abs/2506.17356", "authors": ["Jionghao Lin", "Jiarui Rao", "Yiyang Zhao", "Yuting Wang", "Ashish Gurung", "Amanda Barany", "Jaclyn Ocumpaugh", "Ryan S. Baker", "Kenneth R. Koedinger"], "title": "Automatic Large Language Models Creation of Interactive Learning Lessons", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Full Research Paper, 15 pages, In Proceedings of 20th European\n  Conference on Technology Enhanced Learning (ECTEL2025)", "summary": "We explore the automatic generation of interactive, scenario-based lessons\ndesigned to train novice human tutors who teach middle school mathematics\nonline. Employing prompt engineering through a Retrieval-Augmented Generation\napproach with GPT-4o, we developed a system capable of creating structured\ntutor training lessons. Our study generated lessons in English for three key\ntopics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior,\nand Turning on Cameras, using a task decomposition prompting strategy that\nbreaks lesson generation into sub-tasks. The generated lessons were evaluated\nby two human evaluators, who provided both quantitative and qualitative\nevaluations using a comprehensive rubric informed by lesson design research.\nResults demonstrate that the task decomposition strategy led to higher-rated\nlessons compared to single-step generation. Human evaluators identified several\nstrengths in the LLM-generated lessons, including well-structured content and\ntime-saving potential, while also noting limitations such as generic feedback\nand a lack of clarity in some instructional sections. These findings underscore\nthe potential of hybrid human-AI approaches for generating effective lessons in\ntutor training.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u81ea\u52a8\u751f\u6210\u4e92\u52a8\u5f0f\u573a\u666f\u8bfe\u7a0b\uff0c\u7528\u4e8e\u57f9\u8bad\u6559\u6388\u521d\u4e2d\u6570\u5b66\u7684\u7ebf\u4e0a\u65b0\u624b\u5bfc\u5e08\u3002\u901a\u8fc7Retrieval-Augmented Generation\u548cGPT-4o\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u5f00\u53d1\u4e86\u80fd\u751f\u6210\u7ed3\u6784\u5316\u8bfe\u7a0b\u7684\u8bad\u7ec3\u7cfb\u7edf\uff0c\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u4efb\u52a1\u5206\u89e3\u7b56\u7565\u4f18\u4e8e\u5355\u6b65\u751f\u6210\u3002", "motivation": "\u65e8\u5728\u5229\u7528AI\u6280\u672f\u63d0\u5347\u65b0\u624b\u5bfc\u5e08\u7684\u57f9\u8bad\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u8fdc\u7a0b\u6559\u80b2\u4e2d\u5173\u952e\u6559\u5b66\u6280\u80fd\u7684\u57f9\u517b\u3002", "method": "\u91c7\u7528Retrieval-Augmented Generation\u4e0eGPT-4o\u7ed3\u5408\u7684\u4efb\u52a1\u5206\u89e3\u63d0\u793a\u7b56\u7565\uff0c\u751f\u6210\u8bfe\u7a0b\u5e76\u4ea4\u7531\u4eba\u7c7b\u8bc4\u4f30\u3002", "result": "\u4efb\u52a1\u5206\u89e3\u751f\u6210\u7684\u8bfe\u7a0b\u8bc4\u5206\u66f4\u9ad8\uff0c\u5177\u5907\u5185\u5bb9\u7ed3\u6784\u6e05\u6670\u548c\u8282\u7701\u65f6\u95f4\u7684\u4f18\u70b9\uff0c\u4f46\u53cd\u9988\u8f83\u6cdb\u3001\u90e8\u5206\u6307\u4ee4\u6a21\u7cca\u3002", "conclusion": "\u6df7\u5408\u4eba\u7c7b\u4e0eAI\u7684\u65b9\u6cd5\u5728\u5bfc\u5e08\u57f9\u8bad\u8bfe\u7a0b\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u7ec6\u8282\u3002"}}
{"id": "2506.18384", "pdf": "https://arxiv.org/pdf/2506.18384", "abs": "https://arxiv.org/abs/2506.18384", "authors": ["Quinten De Man", "Laxman Dhulipala", "Kishen N Gowda"], "title": "Fully-Dynamic Parallel Algorithms for Single-Linkage Clustering", "categories": ["cs.DS", "cs.DC"], "comment": "To appear at SPAA 2025", "summary": "Single-linkage clustering is a popular form of hierarchical agglomerative\nclustering (HAC) where the distance between two clusters is defined as the\nminimum distance between any pair of points across the two clusters. In\nsingle-linkage HAC, the output is typically the single-linkage dendrogram\n(SLD), which is the binary tree representing the hierarchy of clusters formed\nby iteratively contracting the two closest clusters. In the dynamic setting,\nprior work has only studied maintaining a minimum spanning forest over the data\nsince single-linkage HAC reduces to computing the SLD on the minimum spanning\nforest of the data.\n  In this paper, we study the problem of maintaining the SLD in the\nfully-dynamic setting. We assume the input is a dynamic forest $F$\n(representing the minimum spanning forest of the data) which receives a\nsequence of edge insertions and edge deletions. To our knowledge, no prior work\nhas provided algorithms to update an SLD asymptotically faster than recomputing\nit from scratch. All of our update algorithms are asymptotically faster than\nthe best known static SLD computation algorithm, which takes $O(n \\log h)$ time\nwhere $h$ is the height of the dendrogram ($h \\leq n-1$). Furthermore, our\nalgorithms are much faster in many cases, such as when $h$ is low. Our first\nset of results are an insertion algorithm in $O(h)$ time and a deletion\nalgorithm in $O(h \\log (1+n/h))$ time. Next, we describe parallel and\nbatch-parallel versions of these algorithms which are work-efficient or nearly\nwork-efficient and have poly-logarithmic depth. Finally, we show how to perform\ninsertions near-optimally in $O(c \\log(1+n/c))$ time, where $c$ is the number\nof structural changes in the dendrogram caused by the update, and give a\nwork-efficient parallel version of this algorithm that has polylogarithmic\ndepth.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5b8c\u5168\u52a8\u6001\u73af\u5883\u4e0b\u7ef4\u62a4\u5355\u94fe\u63a5\u5c42\u6b21\u805a\u7c7b\u6811\uff08SLD\uff09\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6bd4\u73b0\u6709\u9759\u6001\u7b97\u6cd5\u66f4\u9ad8\u6548\u7684\u52a8\u6001\u66f4\u65b0\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u94fe\u63a5\u5c42\u6b21\u805a\u7c7b\u6811\uff08SLD\uff09\u7ef4\u62a4\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e0b\u6548\u7387\u4e0d\u8db3\uff0c\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u6570\u636e\u7684\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u52a8\u6001\u66f4\u65b0\u7b97\u6cd5\uff0c\u5305\u62ec\u63d2\u5165\u3001\u5220\u9664\u3001\u5e76\u884c\u53ca\u6279\u91cf\u5e76\u884c\u7248\u672c\uff0c\u91cd\u70b9\u4f18\u5316\u4e86\u65f6\u95f4\u590d\u6742\u5ea6\u548c\u5e76\u884c\u6548\u7387\u3002", "result": "\u52a8\u6001\u63d2\u5165\u7b97\u6cd5\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e3aO(h)\uff0c\u5220\u9664\u7b97\u6cd5\u4e3aO(h log(1+n/h))\uff0c\u5e76\u652f\u6301\u9ad8\u6548\u5e76\u884c\u5904\u7406\u3002", "conclusion": "\u8fd9\u4e9b\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86SLD\u7684\u7ef4\u62a4\u6548\u7387\uff0c\u5c24\u5176\u5728\u6811\u4f4e\u9ad8\u5ea6\u6216\u7ed3\u6784\u53d8\u5316\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2506.18796", "pdf": "https://arxiv.org/pdf/2506.18796", "abs": "https://arxiv.org/abs/2506.18796", "authors": ["Kishanthan Thangarajah", "Boyuan Chen", "Shi Chang", "Ahmed E. Hassan"], "title": "Context-Aware CodeLLM Eviction for AI-assisted Coding", "categories": ["cs.SE"], "comment": "12 pages, 6 figures", "summary": "AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are\nincreasingly integrated into modern software development workflows. To address\nconcerns around privacy, latency, and model customization, many enterprises opt\nto self-host these models. However, the diversity and growing number of\nCodeLLMs, coupled with limited accelerator memory, introduce practical\nchallenges in model management and serving efficiency. This paper presents\nCACE, a novel context-aware model eviction strategy designed specifically to\noptimize self-hosted CodeLLM serving under resource constraints. Unlike\ntraditional eviction strategies based solely on recency (e.g., Least Recently\nUsed), CACE leverages multiple context-aware factors, including model load\ntime, task-specific latency sensitivity, expected output length, and recent\nusage and future demand tracked through a sliding window. We evaluate CACE\nusing realistic workloads that include both latency-sensitive code completion\nand throughput-intensive code reasoning tasks. Our experiments show that CACE\nreduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while\nsignificantly lowering the number of model evictions compared to\nstate-of-the-art systems. Ablation studies further demonstrate the importance\nof multi-factor eviction in balancing responsiveness and resource efficiency.\nThis work contributes practical strategies for deploying scalable, low-latency\nAI coding assistants in real-world software engineering environments.", "AI": {"tldr": "CACE\u662f\u4e00\u79cd\u9488\u5bf9\u81ea\u6258\u7ba1CodeLLM\u670d\u52a1\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6a21\u578b\u9a71\u9010\u7b56\u7565\uff0c\u65e8\u5728\u4f18\u5316\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u670d\u52a1\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u5728\u81ea\u6258\u7ba1AI\u8f85\u52a9\u7f16\u7801\u5de5\u5177\u65f6\u7684\u9690\u79c1\u3001\u5ef6\u8fdf\u548c\u6a21\u578b\u5b9a\u5236\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u6a21\u578b\u7ba1\u7406\u548c\u670d\u52a1\u6548\u7387\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faCACE\u7b56\u7565\uff0c\u5229\u7528\u6a21\u578b\u52a0\u8f7d\u65f6\u95f4\u3001\u4efb\u52a1\u5ef6\u8fdf\u654f\u611f\u6027\u3001\u8f93\u51fa\u957f\u5ea6\u3001\u8fd1\u671f\u4f7f\u7528\u548c\u672a\u6765\u9700\u6c42\u7b49\u591a\u56e0\u7d20\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6a21\u578b\u9a71\u9010\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCACE\u80fd\u51cf\u5c11\u9996\u6b21\u4ee4\u724c\u65f6\u95f4\uff08TTFT\uff09\u548c\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6a21\u578b\u9a71\u9010\u6b21\u6570\u3002", "conclusion": "CACE\u4e3a\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u4e2d\u90e8\u7f72\u53ef\u6269\u5c55\u3001\u4f4e\u5ef6\u8fdf\u7684AI\u7f16\u7801\u52a9\u624b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7b56\u7565\u3002"}}
{"id": "2506.17364", "pdf": "https://arxiv.org/pdf/2506.17364", "abs": "https://arxiv.org/abs/2506.17364", "authors": ["Alvaro Becerra", "Roberto Daza", "Ruth Cobos", "Aythami Morales", "Mutlu Cukurova", "Julian Fierrez"], "title": "AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning", "categories": ["cs.CY", "cs.AI", "cs.CV", "cs.HC"], "comment": "Accepted in EC-TEL25: 20th European Conference on Technology Enhanced\n  Learning, Newcastle and Durham, UK, 15-19 September 2025", "summary": "This work investigates the use of multimodal biometrics to detect\ndistractions caused by smartphone use during tasks that require sustained\nattention, with a focus on computer-based online learning. Although the methods\nare applicable to various domains, such as autonomous driving, we concentrate\non the challenges learners face in maintaining engagement amid internal (e.g.,\nmotivation), system-related (e.g., course design) and contextual (e.g.,\nsmartphone use) factors. Traditional learning platforms often lack detailed\nbehavioral data, but Multimodal Learning Analytics (MMLA) and biosensors\nprovide new insights into learner attention. We propose an AI-based approach\nthat leverages physiological signals and head pose data to detect phone use.\nOur results show that single biometric signals, such as brain waves or heart\nrate, offer limited accuracy, while head pose alone achieves 87%. A multimodal\nmodel combining all signals reaches 91% accuracy, highlighting the benefits of\nintegration. We conclude by discussing the implications and limitations of\ndeploying these models for real-time support in online learning environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u591a\u6a21\u6001\u751f\u7269\u7279\u5f81\u68c0\u6d4b\u667a\u80fd\u624b\u673a\u4f7f\u7528\u5bfc\u81f4\u7684\u6ce8\u610f\u529b\u5206\u6563\uff0c\u91cd\u70b9\u5173\u6ce8\u5728\u7ebf\u5b66\u4e60\u573a\u666f\u3002\u901a\u8fc7\u751f\u7406\u4fe1\u53f7\u548c\u5934\u90e8\u59ff\u52bf\u6570\u636e\uff0c\u7ed3\u5408\u591a\u79cd\u4fe1\u53f7\u7684\u591a\u6a21\u6001\u6a21\u578b\u51c6\u786e\u7387\u8fbe91%\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5b66\u4e60\u8005\u5728\u5728\u7ebf\u5b66\u4e60\u4e2d\u56e0\u5185\u90e8\u3001\u7cfb\u7edf\u548c\u4e0a\u4e0b\u6587\u56e0\u7d20\uff08\u5982\u667a\u80fd\u624b\u673a\u4f7f\u7528\uff09\u5bfc\u81f4\u7684\u6ce8\u610f\u529b\u5206\u6563\u95ee\u9898\u3002\u4f20\u7edf\u5b66\u4e60\u5e73\u53f0\u7f3a\u4e4f\u8be6\u7ec6\u7684\u884c\u4e3a\u6570\u636e\uff0c\u800c\u591a\u6a21\u6001\u5b66\u4e60\u5206\u6790\u548c\u751f\u7269\u4f20\u611f\u5668\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eAI\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u7406\u4fe1\u53f7\uff08\u5982\u8111\u7535\u6ce2\u3001\u5fc3\u7387\uff09\u548c\u5934\u90e8\u59ff\u52bf\u6570\u636e\u68c0\u6d4b\u624b\u673a\u4f7f\u7528\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u5355\u4e00\u4fe1\u53f7\u548c\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u5355\u4e00\u751f\u7406\u4fe1\u53f7\uff08\u5982\u8111\u7535\u6ce2\u6216\u5fc3\u7387\uff09\u51c6\u786e\u6027\u6709\u9650\uff0c\u800c\u4ec5\u5934\u90e8\u59ff\u52bf\u6570\u636e\u8fbe\u523087%\u51c6\u786e\u7387\u3002\u591a\u6a21\u6001\u6a21\u578b\u7ed3\u5408\u6240\u6709\u4fe1\u53f7\u540e\u51c6\u786e\u7387\u63d0\u5347\u81f391%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u6574\u5408\u7684\u4f18\u52bf\uff0c\u5e76\u8ba8\u8bba\u4e86\u5728\u5728\u7ebf\u5b66\u4e60\u73af\u5883\u4e2d\u5b9e\u65f6\u90e8\u7f72\u6b64\u7c7b\u6a21\u578b\u7684\u6f5c\u5728\u5f71\u54cd\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2506.18824", "pdf": "https://arxiv.org/pdf/2506.18824", "abs": "https://arxiv.org/abs/2506.18824", "authors": ["Islem Bouzenia", "Michael Pradel"], "title": "Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86\u4e09\u79cd\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u6210\u529f\u4e0e\u5931\u8d25\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u8bbe\u8ba1\u7684\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5185\u90e8\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u9ad8\u5176\u900f\u660e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u7edf\u4e00\u4e86\u4e09\u79cd\u4ee3\u7406\u7684\u4ea4\u4e92\u65e5\u5fd7\u683c\u5f0f\uff0c\u5b9a\u91cf\u5206\u6790\u7ed3\u6784\u7279\u6027\u3001\u884c\u4e3a\u6a21\u5f0f\u548ctoken\u4f7f\u7528\uff0c\u5b9a\u6027\u8bc4\u4f30\u63a8\u7406\u4e00\u81f4\u6027\u548c\u53cd\u9988\u6574\u5408\u3002", "result": "\u8bc6\u522b\u4e86\u6210\u529f\u548c\u5931\u8d25\u6267\u884c\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u63d0\u4f9b\u4e86\u6539\u8fdb\u4ee3\u7406\u8bbe\u8ba1\u7684\u5177\u4f53\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u6709\u52a9\u4e8e\u63d0\u5347\u4ee3\u7406\u7684\u900f\u660e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.17467", "pdf": "https://arxiv.org/pdf/2506.17467", "abs": "https://arxiv.org/abs/2506.17467", "authors": ["Weixin Liang"], "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Stanford CS PhD Dissertation", "summary": "Large language models (LLMs) have shown significant potential to change how\nwe write, communicate, and create, leading to rapid adoption across society.\nThis dissertation examines how individuals and institutions are adapting to and\nengaging with this emerging technology through three research directions.\nFirst, I demonstrate how the institutional adoption of AI detectors introduces\nsystematic biases, particularly disadvantaging writers of non-dominant language\nvarieties, highlighting critical equity concerns in AI governance. Second, I\npresent novel population-level algorithmic approaches that measure the\nincreasing adoption of LLMs across writing domains, revealing consistent\npatterns of AI-assisted content in academic peer reviews, scientific\npublications, consumer complaints, corporate communications, job postings, and\ninternational organization press releases. Finally, I investigate LLMs'\ncapability to provide feedback on research manuscripts through a large-scale\nempirical analysis, offering insights into their potential to support\nresearchers who face barriers in accessing timely manuscript feedback,\nparticularly early-career researchers and those from under-resourced settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u793e\u4f1a\u5f71\u54cd\uff0c\u5305\u62ecAI\u68c0\u6d4b\u5668\u7684\u504f\u89c1\u3001LLMs\u5728\u5404\u9886\u57df\u7684\u666e\u53ca\u53ca\u5176\u5728\u79d1\u7814\u53cd\u9988\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76LLMs\u5982\u4f55\u6539\u53d8\u5199\u4f5c\u3001\u6c9f\u901a\u548c\u521b\u4f5c\u65b9\u5f0f\uff0c\u5e76\u63a2\u7d22\u4e2a\u4eba\u548c\u673a\u6784\u5982\u4f55\u9002\u5e94\u8fd9\u4e00\u65b0\u5174\u6280\u672f\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u7814\u7a76\u65b9\u5411\uff1a\u5206\u6790AI\u68c0\u6d4b\u5668\u7684\u504f\u89c1\u3001\u91cf\u5316LLMs\u5728\u5404\u9886\u57df\u7684\u91c7\u7528\u60c5\u51b5\u3001\u8bc4\u4f30LLMs\u5728\u79d1\u7814\u53cd\u9988\u4e2d\u7684\u6f5c\u529b\u3002", "result": "\u53d1\u73b0AI\u68c0\u6d4b\u5668\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\u3001LLMs\u5728\u591a\u4e2a\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u3001LLMs\u80fd\u4e3a\u79d1\u7814\u63d0\u4f9b\u6709\u6548\u53cd\u9988\u3002", "conclusion": "LLMs\u7684\u666e\u53ca\u5f15\u53d1\u516c\u5e73\u95ee\u9898\uff0c\u4f46\u540c\u65f6\u4e5f\u4e3a\u79d1\u7814\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u9700\u8c28\u614e\u7ba1\u7406\u5176\u5e94\u7528\u3002"}}
{"id": "2506.17315", "pdf": "https://arxiv.org/pdf/2506.17315", "abs": "https://arxiv.org/abs/2506.17315", "authors": ["Chuan Yan", "Liuhuo Wan", "Bowei Guan", "Fengqi Yu", "Guangdong Bai", "Jin Song Dong"], "title": "Tracking GPTs Third Party Service: Automation, Analysis, and Insights", "categories": ["cs.CR", "cs.SE"], "comment": "The 1st International Workshop on LLM App Store Analysis (LLMapp\n  2025)", "summary": "ChatGPT has quickly advanced from simple natural language processing to\ntackling more sophisticated and specialized tasks. Drawing inspiration from the\nsuccess of mobile app ecosystems, OpenAI allows developers to create\napplications that interact with third-party services, known as GPTs. GPTs can\nchoose to leverage third-party services to integrate with specialized APIs for\ndomain-specific applications. However, the way these disclose privacy setting\ninformation limits accessibility and analysis, making it challenging to\nsystematically evaluate the data privacy implications of third-party integrate\nto GPTs. In order to support academic research on the integration of\nthird-party services in GPTs, we introduce GPTs-ThirdSpy, an automated\nframework designed to extract privacy settings of GPTs. GPTs-ThirdSpy provides\nacademic researchers with real-time, reliable metadata on third-party services\nused by GPTs, enabling in-depth analysis of their integration, compliance, and\npotential security risks. By systematically collecting and structuring this\ndata, GPTs-ThirdSpy facilitates large-scale research on the transparency and\nregulatory challenges associated with the GPT app ecosystem.", "AI": {"tldr": "GPTs-ThirdSpy\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u53d6GPTs\u7684\u9690\u79c1\u8bbe\u7f6e\uff0c\u652f\u6301\u5b66\u672f\u7814\u7a76\u5bf9\u7b2c\u4e09\u65b9\u670d\u52a1\u96c6\u6210\u5728GPTs\u4e2d\u7684\u6570\u636e\u9690\u79c1\u5f71\u54cd\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "GPTs\u4e0e\u7b2c\u4e09\u65b9\u670d\u52a1\u7684\u96c6\u6210\u65b9\u5f0f\u9650\u5236\u4e86\u9690\u79c1\u8bbe\u7f6e\u4fe1\u606f\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u5206\u6790\uff0c\u96be\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u5176\u6570\u636e\u9690\u79c1\u5f71\u54cd\u3002", "method": "\u5f15\u5165GPTs-ThirdSpy\u6846\u67b6\uff0c\u81ea\u52a8\u63d0\u53d6GPTs\u7684\u9690\u79c1\u8bbe\u7f6e\uff0c\u63d0\u4f9b\u5b9e\u65f6\u3001\u53ef\u9760\u7684\u7b2c\u4e09\u65b9\u670d\u52a1\u5143\u6570\u636e\u3002", "result": "GPTs-ThirdSpy\u652f\u6301\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u5206\u6790GPTs\u751f\u6001\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u76d1\u7ba1\u6311\u6218\u3002", "conclusion": "GPTs-ThirdSpy\u4e3a\u5b66\u672f\u7814\u7a76\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u4fc3\u8fdb\u5bf9GPTs\u4e2d\u7b2c\u4e09\u65b9\u670d\u52a1\u96c6\u6210\u7684\u6df1\u5165\u5206\u6790\u4e0e\u76d1\u7ba1\u6539\u8fdb\u3002"}}
{"id": "2506.17323", "pdf": "https://arxiv.org/pdf/2506.17323", "abs": "https://arxiv.org/abs/2506.17323", "authors": ["Tamas Bisztray", "Bilel Cherif", "Richard A. Dubniczky", "Nils Gruschka", "Bertalan Borsos", "Mohamed Amine Ferrag", "Attila Kovacs", "Vasileios Mavroeidis", "Norbert Tihanyi"], "title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "Detecting AI-generated code, deepfakes, and other synthetic content is an\nemerging research challenge. As code generated by Large Language Models (LLMs)\nbecomes more common, identifying the specific model behind each sample is\nincreasingly important. This paper presents the first systematic study of LLM\nauthorship attribution for C programs. We released CodeT5-Authorship, a novel\nmodel that uses only the encoder layers from the original CodeT5\nencoder-decoder architecture, discarding the decoder to focus on\nclassification. Our model's encoder output (first token) is passed through a\ntwo-layer classification head with GELU activation and dropout, producing a\nprobability distribution over possible authors. To evaluate our approach, we\nintroduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs\ngenerated by eight state-of-the-art LLMs across diverse tasks. We compare our\nmodel to seven traditional ML classifiers and eight fine-tuned transformer\nmodels, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,\nLongformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model\nachieves 97.56% accuracy in distinguishing C programs generated by closely\nrelated models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class\nattribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,\nGPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the\nCodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant\nGoogle Colab scripts on GitHub: https://github.com/LLMauthorbench/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5730\u7814\u7a76\u4e86LLM\u751f\u6210\u7684C\u7a0b\u5e8f\u4f5c\u8005\u5f52\u5c5e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCodeT5-Authorship\u7684\u65b0\u6a21\u578b\uff0c\u5e76\u901a\u8fc7LLM-AuthorBench\u57fa\u51c6\u6d4b\u8bd5\u5c55\u793a\u4e86\u5176\u5353\u8d8a\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u7684\u4ee3\u7801\u8d8a\u6765\u8d8a\u666e\u904d\uff0c\u8bc6\u522b\u4ee3\u7801\u80cc\u540e\u5177\u4f53\u6a21\u578b\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u5e76\u63d0\u4f9b\u5f00\u6e90\u5de5\u5177\u3002", "method": "\u4f5c\u8005\u57fa\u4e8eCodeT5\u7684\u7f16\u7801\u5668\u5c42\u6784\u5efa\u4e86CodeT5-Authorship\u6a21\u578b\uff0c\u820d\u5f03\u89e3\u7801\u5668\u4ee5\u4e13\u6ce8\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u5206\u7c7b\u5934\u8f93\u51fa\u4f5c\u8005\u7684\u6982\u7387\u5206\u5e03\u3002", "result": "\u5728\u4e8c\u8fdb\u5236\u5206\u7c7b\u4e2d\uff0c\u6a21\u578b\u5bf9GPT-4.1\u548cGPT-4o\u7684\u533a\u5206\u51c6\u786e\u7387\u8fbe97.56%\uff0c\u591a\u5206\u7c7b\u4e2d\u4e94\u79cd\u4e3b\u6d41LLM\u7684\u51c6\u786e\u7387\u4e3a95.40%\u3002", "conclusion": "CodeT5-Authorship\u6a21\u578b\u5728LLM\u4f5c\u8005\u5f52\u5c5e\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5f00\u6e90\u4e86\u6a21\u578b\u4e0e\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002"}}
{"id": "2506.17834", "pdf": "https://arxiv.org/pdf/2506.17834", "abs": "https://arxiv.org/abs/2506.17834", "authors": ["Carter Blair", "Kate Larson", "Edith Law"], "title": "Reflective Verbal Reward Design for Pluralistic Alignment", "categories": ["cs.AI", "cs.HC", "I.2.6; H.5.2; I.2.7"], "comment": "9 pages, 3 figures, accepted to the IJCAI 2025 Human-Centred AI\n  track. Project repository at: https://osf.io/8yxf2/", "summary": "AI agents are commonly aligned with \"human values\" through reinforcement\nlearning from human feedback (RLHF), where a single reward model is learned\nfrom aggregated human feedback and used to align an agent's behavior. However,\nhuman values are not homogeneous--different people hold distinct and sometimes\nconflicting values. Aggregating feedback into a single reward model risks\ndisproportionately suppressing minority preferences. To address this, we\npresent a novel reward modeling approach for learning individualized reward\nmodels. Our approach uses a language model to guide users through reflective\ndialogues where they critique agent behavior and construct their preferences.\nThis personalized dialogue history, containing the user's reflections and\ncritiqued examples, is then used as context for another language model that\nserves as an individualized reward function (what we call a \"verbal reward\nmodel\") for evaluating new trajectories. In studies with 30 participants, our\nmethod achieved a 9-12% improvement in accuracy over non-reflective verbal\nreward models while being more sample efficient than traditional supervised\nlearning methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2a\u6027\u5316\u7684\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7528\u6237\u53cd\u601d\u5bf9\u8bdd\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfRLHF\u4e2d\u5355\u4e00\u5956\u52b1\u6a21\u578b\u5ffd\u89c6\u5c11\u6570\u504f\u597d\u7684\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u4ef7\u503c\u89c2\u5177\u6709\u591a\u6837\u6027\uff0c\u4f20\u7edfRLHF\u65b9\u6cd5\u901a\u8fc7\u5355\u4e00\u5956\u52b1\u6a21\u578b\u805a\u5408\u53cd\u9988\u53ef\u80fd\u538b\u5236\u5c11\u6570\u504f\u597d\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e2a\u6027\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7528\u6237\u8fdb\u884c\u53cd\u601d\u5bf9\u8bdd\uff0c\u8bb0\u5f55\u4e2a\u6027\u5316\u5bf9\u8bdd\u5386\u53f2\uff0c\u5e76\u4ee5\u6b64\u4f5c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u5165\uff0c\u6784\u5efa\u4e2a\u6027\u5316\u7684\u201c\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u201d\u3002", "result": "\u572830\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u975e\u53cd\u601d\u6027\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad89-12%\uff0c\u4e14\u6837\u672c\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u4e2a\u6027\u5316\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u591a\u6837\u5316\u7684\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2506.18199", "pdf": "https://arxiv.org/pdf/2506.18199", "abs": "https://arxiv.org/abs/2506.18199", "authors": ["Bushra Asseri", "Estabrag Abdelaziz", "Areej Al-Wabil"], "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5bf9\u963f\u62c9\u4f2f\u4eba\u548c\u7a46\u65af\u6797\u7684\u6587\u5316\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u603b\u7ed3\u4e86\u4e94\u79cd\u4e3b\u8981\u7b56\u7565\u53ca\u5176\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u5bf9\u963f\u62c9\u4f2f\u4eba\u548c\u7a46\u65af\u6797\u7684\u6587\u5316\u504f\u89c1\u5f15\u53d1\u4e86\u4f26\u7406\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u8fd9\u4e00\u7fa4\u4f53\u7684\u504f\u89c1\u7f13\u89e3\u7814\u7a76\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7684\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u4e862021-2024\u5e74\u95f48\u9879\u5173\u4e8e\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e94\u79cd\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u53ef\u6709\u6548\u51cf\u5c11\u504f\u89c1\uff0c\u5176\u4e2d\u7ed3\u6784\u5316\u591a\u6b65\u6d41\u7a0b\u6548\u679c\u6700\u4f73\uff08\u504f\u89c1\u51cf\u5c11\u8fbe87.7%\uff09\uff0c\u4f46\u6587\u5316\u63d0\u793a\u66f4\u6613\u4e8e\u5b9e\u65bd\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u867d\u6709\u6548\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u5f00\u53d1\u6587\u5316\u9002\u5e94\u6027\u6280\u672f\u548c\u7279\u5b9a\u8bc4\u4ef7\u8d44\u6e90\uff0c\u5e76\u7ed3\u5408\u5176\u4ed6\u53bb\u504f\u65b9\u6cd5\u4ee5\u7ef4\u6301\u6a21\u578b\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.18245", "pdf": "https://arxiv.org/pdf/2506.18245", "abs": "https://arxiv.org/abs/2506.18245", "authors": ["Lei Yu", "Zhirong Huang", "Hang Yuan", "Shiqi Cheng", "Li Yang", "Fengjun Zhang", "Chenjie Shen", "Jiajia Ma", "Jingyuan Zhang", "Junyi Lu", "Chun Zuo"], "title": "Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": "Accepted to ISSTA 2025", "summary": "Smart contract vulnerability detection remains a major challenge in\nblockchain security. Existing vulnerability detection methods face two main\nissues: (1) Existing datasets lack comprehensive coverage and high-quality\nexplanations for preference learning. (2) Large language models (LLMs) often\nstruggle with accurately interpreting specific concepts in smart contract\nsecurity. Empirical analysis shows that even after continual pre-training (CPT)\nand supervised fine-tuning (SFT), LLMs may misinterpret the execution order of\nstate changes, resulting in incorrect explanations despite making correct\ndetection decisions. To address these challenges, we propose Smart-LLaMA-DPO\nbased on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major\nvulnerability types and machine-unauditable vulnerabilities, including precise\nlabels, explanations, and locations for SFT, as well as high-quality and\nlow-quality output pairs for Direct Preference Optimization (DPO). Second, we\nperform CPT using large-scale smart contract to enhance the LLM's understanding\nof specific security practices in smart contracts. Futhermore, we conduct SFT\nwith our comprehensive dataset. Finally, we apply DPO, leveraging human\nfeedback and a specially designed loss function that increases the probability\nof preferred explanations while reducing the likelihood of non-preferred\noutputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:\nreentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,\nas well as machine-unauditable vulnerabilities. Our method significantly\noutperforms state-of-the-art baselines, with average improvements of 10.43% in\nF1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human\nevaluation confirm that our method generates more correct, thorough, and clear\nexplanations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Smart-LLaMA-DPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u5b9a\u5411\u504f\u597d\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u6570\u636e\u96c6\u8986\u76d6\u4e0d\u5168\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7279\u5b9a\u5b89\u5168\u6982\u5ff5\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8eLLaMA-3.1-8B\uff0c\u6784\u5efa\u5168\u9762\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5b9a\u5411\u504f\u597d\u4f18\u5316\u3002", "result": "\u5728F1\u5206\u6570\u548c\u51c6\u786e\u7387\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u534710.43%\u548c7.87%\uff0c\u4e14\u751f\u6210\u7684\u89e3\u91ca\u66f4\u6b63\u786e\u3001\u5168\u9762\u548c\u6e05\u6670\u3002", "conclusion": "Smart-LLaMA-DPO\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u89e3\u91ca\u8d28\u91cf\u3002"}}
{"id": "2506.18201", "pdf": "https://arxiv.org/pdf/2506.18201", "abs": "https://arxiv.org/abs/2506.18201", "authors": ["Bushra Asseri", "Estabraq Abdelaziz", "Maha Al Mogren", "Tayef Alhefdhi", "Areej Al-Wabil"], "title": "Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications", "categories": ["cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Emotion recognition capabilities in multimodal AI systems are crucial for\ndeveloping culturally responsive educational technologies, yet remain\nunderexplored for Arabic language contexts where culturally appropriate\nlearning tools are critically needed. This study evaluates the emotion\nrecognition performance of two advanced multimodal large language models,\nGPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook\nillustrations. We assessed both models across three prompting strategies\n(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic\nstorybooks, comparing model predictions with human annotations based on\nPlutchik's emotional framework. GPT-4o consistently outperformed Gemini across\nall conditions, achieving the highest macro F1-score of 59% with\nchain-of-thought prompting compared to Gemini's best performance of 43%. Error\nanalysis revealed systematic misclassification patterns, with valence\ninversions accounting for 60.7% of errors, while both models struggled with\nculturally nuanced emotions and ambiguous narrative contexts. These findings\nhighlight fundamental limitations in current models' cultural understanding and\nemphasize the need for culturally sensitive training approaches to develop\neffective emotion-aware educational technologies for Arabic-speaking learners.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86GPT-4o\u548cGemini 1.5 Pro\u4e24\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u963f\u62c9\u4f2f\u513f\u7ae5\u6545\u4e8b\u4e66\u63d2\u56fe\u4e2d\u60c5\u611f\u7684\u80fd\u529b\uff0c\u53d1\u73b0GPT-4o\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eGemini\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u6a21\u578b\u5728\u6587\u5316\u7406\u89e3\u4e0a\u7684\u5c40\u9650\u6027\u548c\u5bf9\u6a21\u7cca\u53d9\u4e8b\u7684\u5904\u7406\u56f0\u96be\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u8a00\u80cc\u666f\u4e0b\u7f3a\u4e4f\u6587\u5316\u654f\u611f\u7684\u3001\u80fd\u591f\u8bc6\u522b\u60c5\u611f\u7684\u6559\u80b2\u6280\u672f\u5de5\u5177\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u591a\u6a21\u6001AI\u7cfb\u7edf\u5728\u60c5\u611f\u8bc6\u522b\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u752875\u5f20\u963f\u62c9\u4f2f\u6545\u4e8b\u4e66\u63d2\u56fe\uff0c\u901a\u8fc7\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\uff09\u8bc4\u4f30GPT-4o\u548cGemini 1.5 Pro\u7684\u60c5\u611f\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u4e0e\u57fa\u4e8ePlutchik\u60c5\u611f\u6846\u67b6\u7684\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "GPT-4o\u5728\u6240\u6709\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8eGemini\uff0c\u5176\u6700\u4f73\u5b8fF1\u5f97\u5206\u4e3a59%\uff0c\u800cGemini\u4e3a43%\u3002\u6a21\u578b\u5728\u6587\u5316\u7ec6\u8282\u60c5\u611f\u548c\u6a21\u7cca\u53d9\u4e8b\u4e2d\u7684\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u6587\u5316\u7406\u89e3\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6587\u5316\u654f\u611f\u7684\u57f9\u8bad\u65b9\u6cd5\u6765\u5f00\u53d1\u9002\u7528\u4e8e\u963f\u62c9\u4f2f\u5b66\u4e60\u8005\u7684\u60c5\u611f\u8bc6\u522b\u6559\u80b2\u6280\u672f\u3002"}}
{"id": "2506.18458", "pdf": "https://arxiv.org/pdf/2506.18458", "abs": "https://arxiv.org/abs/2506.18458", "authors": ["Noah H. Oldfield", "Christoph Laaber", "Shaukat Ali"], "title": "Bloch Vector Assertions for Debugging Quantum Programs", "categories": ["quant-ph", "cs.SE"], "comment": "Journal Submission, 40 pages", "summary": "Quantum programs must be reliable to ensure trustworthy results, yet\ndebugging them is notoriously challenging due to quantum-specific faults like\ngate misimplementations and hardware noise, as well as their inherently\nprobabilistic nature. Assertion-based debugging provides a promising solution\nby enabling localized correctness checks during execution. However, current\napproaches face challenges including manual assertion generation, reliance on\nmid-circuit-measurements, and poor scalability. In this paper, we present Bloq,\na scalable, automated fault localization approach introducing\nBloch-vector-based assertions utilizing expectation value measurements of Pauli\noperators, enabling low-overhead fault localization without mid-circuit\nmeasurements. In addition, we introduce AutoBloq, a component of Bloq for\nautomatically generating assertion schemes from quantum algorithms. An\nexperimental evaluation over 684432 programs using two algorithms (Quantum\nFourier Transform (QFT) and Grover) shows that Bloq consistently outperforms\nthe state-of-the-art approach Proq, notably as circuit depth and noise\nincrease. For Grover, Bloq achieves a mean F1 score across all experimental\ninstances of 0.74 versus 0.38 for Proq under ideal conditions, and maintains\nperformance under noise (0.43 versus 0.06). Bloq also reduces Proq's runtime by\na factor of 5 and circuit depth overhead by a factor of 23. These results\nunderline Bloq's potential to make assertion-based debugging scalable and\neffective for near-term quantum devices.", "AI": {"tldr": "Bloq\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBloch\u5411\u91cf\u7684\u65ad\u8a00\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5b9a\u4f4d\u91cf\u5b50\u7a0b\u5e8f\u4e2d\u7684\u6545\u969c\uff0c\u907f\u514d\u4e86\u624b\u52a8\u65ad\u8a00\u548c\u4e2d\u7535\u8def\u6d4b\u91cf\u7684\u9650\u5236\uff0c\u4e14\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u7a0b\u5e8f\u8c03\u8bd5\u56f0\u96be\uff0c\u73b0\u6709\u65ad\u8a00\u65b9\u6cd5\u5b58\u5728\u624b\u52a8\u751f\u6210\u3001\u4e2d\u7535\u8def\u6d4b\u91cf\u4f9d\u8d56\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165Bloch-vector-based\u65ad\u8a00\u548cAutoBloq\u81ea\u52a8\u751f\u6210\u65ad\u8a00\u65b9\u6848\uff0c\u5229\u7528Pauli\u7b97\u5b50\u7684\u671f\u671b\u503c\u6d4b\u91cf\u8fdb\u884c\u4f4e\u5f00\u9500\u6545\u969c\u5b9a\u4f4d\u3002", "result": "\u5728684432\u4e2a\u7a0b\u5e8f\u5b9e\u9a8c\u4e2d\uff0cBloq\u6027\u80fd\u4f18\u4e8eProq\uff0cF1\u5f97\u5206\u663e\u8457\u63d0\u5347\uff0c\u4e14\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u548c\u7535\u8def\u6df1\u5ea6\u5f00\u9500\u3002", "conclusion": "Bloq\u4e3a\u8fd1\u91cf\u5b50\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u65ad\u8a00\u8c03\u8bd5\u65b9\u6cd5\u3002"}}
{"id": "2506.18365", "pdf": "https://arxiv.org/pdf/2506.18365", "abs": "https://arxiv.org/abs/2506.18365", "authors": ["Imene Tarakli", "Samuele Vinanzi", "Richard Moore", "Alessandro Di Nuovo"], "title": "Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Despite growing interest in Learning-by-Teaching (LbT), few studies have\nexplored how this paradigm can be implemented with autonomous, peer-like social\nrobots in real classrooms. Most prior work has relied on scripted or\nWizard-of-Oz behaviors, limiting our understanding of how real-time,\ninteractive learning can be supported by artificial agents. This study\naddresses this gap by introducing Interactive Reinforcement Learning (RL) as a\ncognitive model for teachable social robots. We conducted two between-subject\nexperiments with 58 primary school children, who either taught a robot or\npracticed independently on a tablet while learning French vocabulary\n(memorization) and grammatical rules (inference). The robot, powered by\nInteractive RL, learned from the child's evaluative feedback. Children in the\nLbT condition achieved significantly higher retention gains compared to those\nin the self-practice condition, especially on the grammar task. Learners with\nlower prior knowledge benefited most from teaching the robot. Behavioural\nmetrics revealed that children adapted their teaching strategies over time and\nengaged more deeply during inference tasks. This work makes two contributions:\n(1) it introduces Interactive RL as a pedagogically effective and scalable\nmodel for peer-robot learning, and (2) it demonstrates, for the first time, the\nfeasibility of deploying multiple autonomous robots simultaneously in real\nclassrooms. These findings extend theoretical understanding of LbT by showing\nthat social robots can function not only as passive tutees but as adaptive\npartners that enhance meta-cognitive engagement and long-term learning\noutcomes.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5f3a\u5316\u5b66\u4e60\uff08Interactive RL\uff09\u5b9e\u73b0\u793e\u4ea4\u673a\u5668\u4eba\u5728\u771f\u5b9e\u8bfe\u5802\u4e2d\u7684\u6559\u5b66\u4e92\u52a8\uff0c\u53d1\u73b0\u513f\u7ae5\u5728\u6559\u6388\u673a\u5668\u4eba\u5b66\u4e60\u65f6\u6bd4\u81ea\u4e3b\u5b66\u4e60\u8868\u73b0\u66f4\u597d\uff0c\u5c24\u5176\u662f\u8bed\u6cd5\u4efb\u52a1\u548c\u4f4e\u77e5\u8bc6\u6c34\u5e73\u5b66\u751f\u53d7\u76ca\u66f4\u591a\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u5b66\u4e60-\u6559\u5b66\uff08LbT\uff09\u6a21\u5f0f\u7684\u7814\u7a76\u591a\u4f9d\u8d56\u4e8e\u811a\u672c\u5316\u6216\u6a21\u62df\u884c\u4e3a\uff0c\u7f3a\u4e4f\u5bf9\u81ea\u4e3b\u793e\u4ea4\u673a\u5668\u4eba\u5728\u771f\u5b9e\u8bfe\u5802\u4e2d\u5b9e\u65f6\u4e92\u52a8\u5b66\u4e60\u7684\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff0c\u8ba958\u540d\u5c0f\u5b66\u751f\u6559\u6388\u4ea4\u4e92\u5f0fRL\u9a71\u52a8\u7684\u673a\u5668\u4eba\u6216\u81ea\u4e3b\u5b66\u4e60\u6cd5\u8bed\u8bcd\u6c47\u548c\u8bed\u6cd5\uff0c\u5bf9\u6bd4\u4e24\u79cd\u5b66\u4e60\u65b9\u5f0f\u7684\u6548\u679c\u3002", "result": "\u6559\u6388\u673a\u5668\u4eba\u7684\u5b66\u751f\u5728\u8bed\u6cd5\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u66f4\u4f18\uff0c\u4f4e\u77e5\u8bc6\u6c34\u5e73\u5b66\u751f\u53d7\u76ca\u6700\u5927\uff1b\u884c\u4e3a\u6570\u636e\u663e\u793a\u5b66\u751f\u9010\u6e10\u8c03\u6574\u6559\u5b66\u7b56\u7565\u5e76\u66f4\u6df1\u5165\u53c2\u4e0e\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "\u4ea4\u4e92\u5f0fRL\u662f\u6559\u5b66\u673a\u5668\u4eba\u7684\u6709\u6548\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u591a\u81ea\u4e3b\u673a\u5668\u4eba\u540c\u65f6\u5728\u771f\u5b9e\u8bfe\u5802\u4e2d\u5b9e\u73b0\u7684\u53ef\u884c\u6027\uff0c\u62d3\u5c55\u4e86LbT\u7406\u8bba\u4e2d\u673a\u5668\u4eba\u4f5c\u4e3a\u9002\u5e94\u6027\u4f19\u4f34\u7684\u89d2\u8272\u3002"}}
{"id": "2506.18470", "pdf": "https://arxiv.org/pdf/2506.18470", "abs": "https://arxiv.org/abs/2506.18470", "authors": ["Daniele Canavese", "Leonardo Regano", "Bjorn De Sutter", "Cataldo Basile"], "title": "Automatic Selection of Protections to Mitigate Risks Against Software Applications", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "This paper introduces a novel approach for the automated selection of\nsoftware protections to mitigate MATE risks against critical assets within\nsoftware applications. We formalize the key elements involved in protection\ndecision-making - including code artifacts, assets, security requirements,\nattacks, and software protections - and frame the protection process through a\ngame-theoretic model. In this model, a defender strategically applies\nprotections to various code artifacts of a target application, anticipating\nrepeated attack attempts by adversaries against the confidentiality and\nintegrity of the application's assets. The selection of the optimal defense\nmaximizes resistance to attacks while ensuring the application remains usable\nby constraining the overhead introduced by protections. The game is solved\nthrough a heuristic based on a mini-max depth-first exploration strategy,\naugmented with dynamic programming optimizations for improved efficiency.\nCentral to our formulation is the introduction of the Software Protection\nIndex, an original contribution that extends existing notions of potency and\nresilience by evaluating protection effectiveness against attack paths using\nsoftware metrics and expert assessments. We validate our approach through a\nproof-of-concept implementation and expert evaluations, demonstrating that\nautomated software protection is a practical and effective solution for risk\nmitigation in software.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u65b0\u578b\u81ea\u52a8\u5316\u8f6f\u4ef6\u4fdd\u62a4\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u9632\u5fa1\u7b56\u7565\u62b5\u6297\u653b\u51fb\u5e76\u4fdd\u6301\u5e94\u7528\u53ef\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4ef6\u4e2d\u5173\u952e\u8d44\u4ea7\u7684MATE\u98ce\u9669\uff0c\u4f18\u5316\u4fdd\u62a4\u51b3\u7b56\u4ee5\u62b5\u6297\u653b\u51fb\u5e76\u6700\u5c0f\u5316\u6027\u80fd\u5f00\u9500\u3002", "method": "\u4f7f\u7528\u535a\u5f08\u8bba\u6a21\u578b\u548c\u542f\u53d1\u5f0f\u6700\u5c0f\u6700\u5927\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u52a8\u6001\u7f16\u7a0b\u4f18\u5316\uff0c\u5b9a\u4e49\u8f6f\u4ef6\u4fdd\u62a4\u6307\u6570\u8bc4\u4f30\u4fdd\u62a4\u6548\u679c\u3002", "result": "\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u548c\u4e13\u5bb6\u8bc4\u4f30\u8bc1\u660e\u8be5\u65b9\u6cd5\u5b9e\u7528\u4e14\u6709\u6548\u3002", "conclusion": "\u81ea\u52a8\u5316\u8f6f\u4ef6\u4fdd\u62a4\u662f\u98ce\u9669\u7f13\u89e3\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18466", "pdf": "https://arxiv.org/pdf/2506.18466", "abs": "https://arxiv.org/abs/2506.18466", "authors": ["Matti Kr\u00fcger", "Daniel Tanneberg", "Chao Wang", "Stephan Hasler", "Michael Gienger"], "title": "Mirror Eyes: Explainable Human-Robot Interaction at a Glance", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN)", "summary": "The gaze of a person tends to reflect their interest. This work explores what\nhappens when this statement is taken literally and applied to robots. Here we\npresent a robot system that employs a moving robot head with a screen-based eye\nmodel that can direct the robot's gaze to points in physical space and present\na reflection-like mirror image of the attended region on top of each eye. We\nconducted a user study with 33 participants, who were asked to instruct the\nrobot to perform pick-and-place tasks, monitor the robot's task execution, and\ninterrupt it in case of erroneous actions. Despite a deliberate lack of\ninstructions about the role of the eyes and a very brief system exposure,\nparticipants felt more aware about the robot's information processing, detected\nerroneous actions earlier, and rated the user experience higher when eye-based\nmirroring was enabled compared to non-reflective eyes. These results suggest a\nbeneficial and intuitive utilization of the introduced method in cooperative\nhuman-robot interaction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u5982\u4f55\u4f7f\u7528\u7c7b\u4f3c\u4eba\u773c\u7684\u6ce8\u89c6\u53cd\u5c04\u529f\u80fd\uff0c\u4ee5\u63d0\u9ad8\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5408\u4f5c\u6548\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u589e\u5f3a\u7528\u6237\u5bf9\u673a\u5668\u4eba\u4fe1\u606f\u5904\u7406\u7684\u611f\u77e5\uff0c\u5e76\u63d0\u5347\u6574\u4f53\u4f53\u9a8c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u673a\u5668\u4eba\u6ce8\u89c6\u884c\u4e3a\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e00\u6837\u53cd\u6620\u5176\u5174\u8da3\uff0c\u4ece\u800c\u63d0\u5347\u4eba\u673a\u5408\u4f5c\u7684\u76f4\u89c2\u6027\u548c\u6548\u7387\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u673a\u5668\u4eba\u5934\u90e8\u7cfb\u7edf\uff0c\u5176\u5c4f\u5e55\u773c\u7403\u80fd\u6ce8\u89c6\u7269\u7406\u7a7a\u95f4\u7684\u70b9\uff0c\u5e76\u5728\u773c\u7403\u4e0a\u53cd\u5c04\u88ab\u6ce8\u89c6\u533a\u57df\u7684\u56fe\u50cf\u3002\u901a\u8fc733\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e86\u8fd9\u79cd\u8bbe\u8ba1\u5728\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u6ca1\u6709\u660e\u786e\u89e3\u91ca\u773c\u7403\u529f\u80fd\uff0c\u5e26\u6709\u53cd\u5c04\u6ce8\u89c6\u529f\u80fd\u7684\u673a\u5668\u4eba\u80fd\u8ba9\u7528\u6237\u66f4\u5feb\u53d1\u73b0\u9519\u8bef\u884c\u4e3a\uff0c\u5e76\u663e\u8457\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u673a\u5668\u4eba\u773c\u7403\u53cd\u5c04\u529f\u80fd\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u5177\u6709\u76f4\u89c2\u4e14\u6709\u76ca\u7684\u4f5c\u7528\uff0c\u53ef\u7528\u4e8e\u63d0\u5347\u5408\u4f5c\u6548\u7387\u3002"}}
{"id": "2506.18795", "pdf": "https://arxiv.org/pdf/2506.18795", "abs": "https://arxiv.org/abs/2506.18795", "authors": ["Jiachi Chen", "Yiming Shen", "Jiashuo Zhang", "Zihao Li", "John Grundy", "Zhenzhe Shao", "Yanlin Wang", "Jiashui Wang", "Ting Chen", "Zibin Zheng"], "title": "FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction", "categories": ["cs.CR", "cs.SE", "D.2.4; I.2.7"], "comment": "Accepted for the 48th International Conference on Software\n  Engineering (ICSE 2026)", "summary": "High-quality smart contract vulnerability datasets are critical for\nevaluating security tools and advancing smart contract security research. Two\nmajor limitations of current manual dataset construction are (1)\nlabor-intensive and error-prone annotation processes limiting the scale,\nquality, and evolution of the dataset, and (2) absence of standardized\nclassification rules results in inconsistent vulnerability categories and\nlabeling results across different datasets. To address these limitations, we\npresent FORGE, the first automated approach for constructing smart contract\nvulnerability datasets. FORGE leverages an LLM-driven pipeline to extract\nhigh-quality vulnerabilities from real-world audit reports and classify them\naccording to the CWE, the most widely recognized classification in software\nsecurity. FORGE employs a divide-and-conquer strategy to extract structured and\nself-contained vulnerability information from these reports. Additionally, it\nuses a tree-of-thoughts technique to classify the vulnerability information\ninto the hierarchical CWE classification. To evaluate FORGE's effectiveness, we\nrun FORGE on 6,454 real-world audit reports and generate a dataset comprising\n81,390 solidity files and 27,497 vulnerability findings across 296 CWE\ncategories. Manual assessment of the dataset demonstrates high extraction\nprecision and classification consistency with human experts (precision of 95.6%\nand inter-rater agreement k-$\\alpha$ of 0.87). We further validate the\npracticality of our dataset by benchmarking 13 existing security tools on our\ndataset. The results reveal the significant limitations in current detection\ncapabilities. Furthermore, by analyzing the severity-frequency distribution\npatterns through a unified CWE perspective in our dataset, we highlight\ninconsistency between current smart contract research focus and priorities\nidentified from real-world vulnerabilities...", "AI": {"tldr": "FORGE\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6784\u5efa\u9ad8\u8d28\u91cf\u667a\u80fd\u5408\u7ea6\u6f0f\u6d1e\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u624b\u52a8\u6807\u6ce8\u65b9\u6cd5\u7684\u4e24\u5927\u9650\u5236\uff1a\u52b3\u52a8\u5bc6\u96c6\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u5206\u7c7b\u3002", "motivation": "\u5f53\u524d\u624b\u52a8\u6784\u5efa\u6570\u636e\u96c6\u52b3\u52a8\u5bc6\u96c6\u4e14\u5206\u7c7b\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "FORGE\u5229\u7528LLM\u9a71\u52a8\u6d41\u7a0b\u4ece\u5ba1\u8ba1\u62a5\u544a\u4e2d\u63d0\u53d6\u6f0f\u6d1e\uff0c\u5e76\u91c7\u7528\u5206\u6cbb\u7b56\u7565\u548c\u6811\u72b6\u601d\u7ef4\u6280\u672f\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u751f\u6210\u7684\u5305\u542b27,497\u4e2a\u6f0f\u6d1e\u7684\u6570\u636e\u96c6\u5728\u7cbe\u5ea6\uff0895.6%\uff09\u548c\u5206\u7c7b\u4e00\u81f4\u6027\uff08k-\u03b1=0.87\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FORGE\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u5b89\u5168\u5de5\u5177\u7684\u5c40\u9650\u6027\u53ca\u7814\u7a76\u91cd\u70b9\u4e0e\u5b9e\u9645\u6f0f\u6d1e\u7684\u4e0d\u4e00\u81f4\u3002"}}
