{"id": "2509.03541", "pdf": "https://arxiv.org/pdf/2509.03541", "abs": "https://arxiv.org/abs/2509.03541", "authors": ["Chong Wang", "Haoning Wu", "Peng Liang", "Maya Daneva", "Marten van Sinderen"], "title": "Towards the Datasets Used in Requirements Engineering of Mobile Apps: Preliminary Findings from a Systematic Mapping Study", "categories": ["cs.SE"], "comment": null, "summary": "[Background] Research on requirements engineering (RE) for mobile apps\nemploys datasets formed by app users, developers or vendors. However, little is\nknown about the sources of these datasets in terms of platforms and the RE\nactivities that were researched with the help of the respective datasets.\n[Aims] The goal of this paper is to investigate the state-of-the-art of the\ndatasets of mobile apps used in existing RE research. [Method] We carried out a\nsystematic mapping study by following the guidelines of Kitchenham et al.\n[Results] Based on 43 selected papers, we found that Google Play and Apple App\nStore provide the datasets for more than 90% of published research in RE for\nmobile apps. We also found that the most investigated RE activities - based on\ndatasets, are requirements elicitation and requirements analysis. [Conclusions]\nOur most important conclusions are: (1) there is a growth in the use of\ndatasets for RE research of mobile apps since 2012, (2) the RE knowledge for\nmobile apps might be skewed due to the overuse of Google Play and Apple App\nStore, (3) there are attempts to supplement reviews of apps from repositories\nwith other data sources, (4) there is a need to expand the alternative sources\nand experiments with complimentary use of multiple sources, if the community\nwants more generalizable results. Plus, it is expected to expand the research\non other RE activities, beyond elicitation and analysis.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\u53d1\u73b0\uff0c\u79fb\u52a8\u5e94\u7528\u9700\u6c42\u5de5\u7a0b\u7814\u7a76\u4e2d90%\u4ee5\u4e0a\u7684\u6570\u636e\u6765\u81eaGoogle Play\u548cApple App Store\uff0c\u4e3b\u8981\u7814\u7a76\u9700\u6c42\u83b7\u53d6\u548c\u5206\u6790\u6d3b\u52a8\uff0c\u4f46\u6570\u636e\u6765\u6e90\u5355\u4e00\u53ef\u80fd\u5bfc\u81f4\u504f\u5dee\uff0c\u5efa\u8bae\u6269\u5c55\u6570\u636e\u6e90\u548c\u8986\u76d6\u66f4\u591aRE\u6d3b\u52a8\u3002", "motivation": "\u7814\u7a76\u79fb\u52a8\u5e94\u7528\u9700\u6c42\u5de5\u7a0b(RE)\u4e2d\u4f7f\u7528\u6570\u636e\u96c6\u7684\u73b0\u72b6\uff0c\u4e86\u89e3\u5176\u6765\u6e90\u5e73\u53f0\u53ca\u5bf9\u5e94\u7684RE\u6d3b\u52a8\uff0c\u4ee5\u53d1\u73b0\u6f5c\u5728\u504f\u5dee\u548c\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u9075\u5faaKitchenham\u7b49\u4eba\u7684\u6307\u5357\uff0c\u5bf943\u7bc7\u76f8\u5173\u8bba\u6587\u8fdb\u884c\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\u3002", "result": "90%\u4ee5\u4e0a\u7814\u7a76\u6570\u636e\u6765\u81eaGoogle Play\u548cApple App Store\uff1b\u9700\u6c42\u83b7\u53d6\u548c\u5206\u6790\u662f\u6700\u5e38\u89c1\u7684\u7814\u7a76\u6d3b\u52a8\u3002", "conclusion": "\u7814\u7a76\u6570\u636e\u6765\u6e90\u5355\u4e00\u53ef\u80fd\u5bfc\u81f4\u7ed3\u679c\u504f\u5dee\uff0c\u5efa\u8bae\u6269\u5c55\u66ff\u4ee3\u6570\u636e\u6e90\u53ca\u7814\u7a76\u66f4\u591aRE\u6d3b\u52a8\u3002"}}
{"id": "2509.03554", "pdf": "https://arxiv.org/pdf/2509.03554", "abs": "https://arxiv.org/abs/2509.03554", "authors": ["Cheng-Yang Tsai", "Tzu-Wei Huang", "Jen-Wei Shih", "I-Hsiang Wang", "Yu-Cheng Lin", "Rung-Bin Lin"], "title": "A Multi-stage Error Diagnosis for APB Transaction", "categories": ["cs.SE"], "comment": null, "summary": "Functional verification and debugging are critical bottlenecks in modern\nSystem-on-Chip (SoC) design, with manual detection of Advanced Peripheral Bus\n(APB) transaction errors in large Value Change Dump (VCD) files being\ninefficient and error-prone. Addressing the 2025 ICCAD Contest Problem D, this\nstudy proposes an automated error diagnosis framework using a hierarchical\nRandom Forest-based architecture. The multi-stage error diagnosis employs four\npre-trained binary classifiers to sequentially detect Out-of-Range Access,\nAddress Corruption, and Data Corruption errors, prioritizing high-certainty\naddress-related faults before tackling complex data errors to enhance\nefficiency. Experimental results show an overall accuracy of 91.36%, with\nnear-perfect precision and recall for address errors and robust performance for\ndata errors. Although the final results of the ICCAD 2025 CAD Contest are yet\nto be announced as of the submission date, our team achieved first place in the\nbeta stage, highlighting the method's competitive strength. This research\nvalidates the potential of hierarchical machine learning as a powerful\nautomated tool for hardware debugging in Electronic Design Automation (EDA).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u968f\u673a\u68ee\u6797\u7684\u81ea\u52a8\u5316\u9519\u8bef\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u68c0\u6d4bSoC\u8bbe\u8ba1\u4e2d\u7684APB\u4e8b\u52a1\u9519\u8bef\uff0c\u53d6\u5f97\u4e8691.36%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728ICCAD 2025\u6bd4\u8d5b\u7684beta\u9636\u6bb5\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "motivation": "\u73b0\u4ee3SoC\u8bbe\u8ba1\u4e2d\uff0c\u624b\u52a8\u68c0\u6d4bAPB\u4e8b\u52a1\u9519\u8bef\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u9a8c\u8bc1\u6548\u7387\u3002", "method": "\u91c7\u7528\u5206\u5c42\u968f\u673a\u68ee\u6797\u67b6\u6784\uff0c\u5229\u7528\u56db\u4e2a\u9884\u8bad\u7ec3\u4e8c\u5143\u5206\u7c7b\u5668\u4f9d\u6b21\u68c0\u6d4b\u4e0d\u540c\u7c7b\u578b\u7684\u9519\u8bef\uff0c\u4f18\u5148\u5904\u7406\u9ad8\u786e\u5b9a\u6027\u5730\u5740\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6574\u4f53\u51c6\u786e\u7387\u4e3a91.36%\uff0c\u5730\u5740\u9519\u8bef\u68c0\u6d4b\u63a5\u8fd1\u5b8c\u7f8e\uff0c\u6570\u636e\u9519\u8bef\u68c0\u6d4b\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "\u5206\u5c42\u673a\u5668\u5b66\u4e60\u5728\u786c\u4ef6\u8c03\u8bd5\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4f5c\u4e3aEDA\u4e2d\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002"}}
{"id": "2509.03668", "pdf": "https://arxiv.org/pdf/2509.03668", "abs": "https://arxiv.org/abs/2509.03668", "authors": ["Matt Rau", "Chris Brown", "John Edwards"], "title": "Parse Tree Tracking Through Time for Programming Process Analysis at Scale", "categories": ["cs.SE"], "comment": null, "summary": "Background and Context: Programming process data can be utilized to\nunderstand the processes students use to write computer programming\nassignments. Keystroke- and line-level event logs have been used in the past in\nvarious ways, primarily in high-level descriptive statistics (e.g., timings,\ncharacter deletion rate, etc). Analysis of behavior in context (e.g., how much\ntime students spend working on loops) has been cumbersome because of our\ninability to automatically track high-level code representations, such as\nabstract syntax trees, through time and unparseable states.\n  Objective: Our study has two goals. The first is to design the first\nalgorithm that tracks parse tree nodes through time. Second, we utilize this\nalgorithm to perform a partial replication study of prior work that used manual\ntracking of code representations, as well as other novel analyses of student\nprogramming behavior that can now be done at scale.\n  Method: We use two algorithms presented in this paper to track parse tree\nnodes through time and construct tree representations for unparseable code\nstates. We apply these algorithms to a public keystroke data from student\ncoursework in a 2021 CS1 course and conduct analysis on the resulting parse\ntrees.\n  Findings: We discover newly observable statistics at scale, including that\ncode is deleted at similar rates inside and outside of conditionals and loops,\na third of commented out code is eventually restored, and that frequency with\nwhich students jump around in their code may not be indicative of struggle.\n  Implications: The ability to track parse trees through time opens the door to\nunderstanding new dimensions of student programming, such as best practices of\nstructural development of code over time, quantitative measurement of what\nsyntactic constructs students struggle most with, refactoring behavior, and\nattention shifting within the code.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ddf\u8e2a\u7f16\u7a0b\u8fc7\u7a0b\u4e2d\u89e3\u6790\u6811\u8282\u70b9\u7684\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u5b66\u751f\u7f16\u7a0b\u6570\u636e\u63ed\u793a\u4e86\u65b0\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u7edf\u8ba1\u7ed3\u679c\u3002", "motivation": "\u8fc7\u53bb\u65e0\u6cd5\u81ea\u52a8\u8ddf\u8e2a\u9ad8\u5c42\u6b21\u7684\u4ee3\u7801\u8868\u793a\uff08\u5982\u62bd\u8c61\u8bed\u6cd5\u6811\uff09\u5bfc\u81f4\u5206\u6790\u5b66\u751f\u5728\u7279\u5b9a\u8bed\u5883\u4e0b\u7684\u884c\u4e3a\u975e\u5e38\u7e41\u7410\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u7b97\u6cd5\u8ddf\u8e2a\u89e3\u6790\u6811\u8282\u70b9\u5e76\u6784\u5efa\u975e\u53ef\u89e3\u6790\u4ee3\u7801\u72b6\u6001\u7684\u6811\u8868\u793a\uff0c\u5206\u6790\u516c\u5f00\u7684\u5b66\u751f\u7f16\u7a0b\u6570\u636e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e00\u4e9b\u65b0\u7684\u7edf\u8ba1\u73b0\u8c61\uff0c\u4f8b\u5982\u5728\u6761\u4ef6\u8bed\u53e5\u548c\u5faa\u73af\u5185\u5916\u4ee3\u7801\u5220\u9664\u7387\u76f8\u4f3c\uff0c\u4e09\u5206\u4e4b\u4e00\u7684\u6ce8\u91ca\u4ee3\u7801\u6700\u7ec8\u88ab\u6062\u590d\u7b49\u3002", "conclusion": "\u80fd\u591f\u8ddf\u8e2a\u89e3\u6790\u6811\u7684\u53d8\u5316\u4e3a\u7406\u89e3\u5b66\u751f\u7f16\u7a0b\u884c\u4e3a\u7684\u65b0\u7ef4\u5ea6\uff08\u5982\u4ee3\u7801\u7ed3\u6784\u7684\u53d1\u5c55\u3001\u5b66\u751f\u6700\u56f0\u96be\u7684\u8bed\u6cd5\u7ed3\u6784\u7b49\uff09\u6253\u5f00\u4e86\u5927\u95e8\u3002"}}
{"id": "2509.03848", "pdf": "https://arxiv.org/pdf/2509.03848", "abs": "https://arxiv.org/abs/2509.03848", "authors": ["Rodrigo Oliveira Zacarias", "Rodrigo Pereira dos Santos", "Patricia Lago"], "title": "Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems", "categories": ["cs.SE", "cs.HC"], "comment": "36 pages Submitted to the ACM Transactions on Software Engineering\n  and Methodology. 2025", "summary": "Software ecosystems (SECO) have become a dominant paradigm in the software\nindustry, enabling third-party developers to co-create value through\ncomplementary components and services. While Developer Experience (DX) is\nincreasingly recognized as critical for sustainable SECO, transparency remains\nan underexplored factor shaping how developers perceive and interact with\necosystems. Existing studies acknowledge transparency as essential for trust,\nfairness, and engagement, yet its relationship with DX has not been\nsystematically conceptualized. Hence, this work aims to advance the\nunderstanding of transparency in SECO from a developer-centered perspective. To\nthis end, we propose SECO-TransDX (Transparency in Software Ecosystems from a\nDeveloper Experience Perspective), a conceptual model that introduces the\nnotion of DX-driven transparency. The model identifies 63 interrelated\nconcepts, including conditioning factors, ecosystem procedures, artifacts, and\nrelational dynamics that influence how transparency is perceived and\nconstructed during developer interactions. SECO-TransDX was built upon prior\nresearch and refined through a Delphi study with experts from academia and\nindustry. It offers a structured lens to examine how transparency mediates DX\nacross technical, social, and organizational layers. For researchers, it lays\nthe groundwork for future studies and tool development; for practitioners, it\nsupports the design of trustworthy, developer-centered platforms that improve\ntransparency and foster long-term engagement in SECO.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SECO-TransDX\u6a21\u578b\uff0c\u65e8\u5728\u4ece\u5f00\u53d1\u8005\u4f53\u9a8c\uff08DX\uff09\u7684\u89d2\u5ea6\u7406\u89e3\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\uff08SECO\uff09\u4e2d\u7684\u900f\u660e\u5ea6\uff0c\u5e76\u63a2\u8ba8\u5176\u5982\u4f55\u5f71\u54cd\u5f00\u53d1\u8005\u7684\u611f\u77e5\u548c\u4e92\u52a8\u3002", "motivation": "\u5c3d\u7ba1\u900f\u660e\u5ea6\u5728\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u5bf9\u4fe1\u4efb\u3001\u516c\u5e73\u548c\u53c2\u4e0e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u4e0e\u5f00\u53d1\u8005\u4f53\u9a8c\uff08DX\uff09\u7684\u5173\u7cfb\u5c1a\u672a\u88ab\u7cfb\u7edf\u5316\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7Delphi\u7814\u7a76\u4e0e\u5b66\u672f\u754c\u548c\u884c\u4e1a\u4e13\u5bb6\u7684\u5408\u4f5c\uff0c\u6784\u5efa\u5e76\u5b8c\u5584\u4e86SECO-TransDX\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5305\u542b63\u4e2a\u76f8\u5173\u6982\u5ff5\uff0c\u6db5\u76d6\u6280\u672f\u3001\u793e\u4f1a\u548c\u7ec4\u7ec7\u5c42\u9762\u3002", "result": "SECO-TransDX\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u89c6\u89d2\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u7406\u89e3\u900f\u660e\u5ea6\u5982\u4f55\u5f71\u54cd\u5f00\u53d1\u8005\u4f53\u9a8c\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5de5\u5177\u5f00\u53d1\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "SECO-TransDX\u6a21\u578b\u4e0d\u4ec5\u4e3a\u7814\u7a76\u900f\u660e\u5ea6\u4e0eDX\u7684\u5173\u7cfb\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u8fd8\u4e3a\u8bbe\u8ba1\u66f4\u900f\u660e\u3001\u5f00\u53d1\u8005\u53cb\u597d\u7684\u5e73\u53f0\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.03855", "pdf": "https://arxiv.org/pdf/2509.03855", "abs": "https://arxiv.org/abs/2509.03855", "authors": ["Zhouyi Zhou", "Zhili Liu", "Shancong Zhang", "Jiemin Li", "Dengke Du", "Mengke Sun", "Zhiqiang Wang", "Hongyan Liu", "Guogai Xu"], "title": "Towards Deterministic Sub-0.5 us Response on Linux through Interrupt Isolation", "categories": ["cs.OS", "68M20", "D.4.7"], "comment": "9 pages, 11 figures", "summary": "Real-time responsiveness in Linux is often constrained by interrupt\ncontention and timer handling overhead, making it challenging to achieve\nsub-microsecond latency. This work introduces an interrupt isolation approach\nthat centralizes and minimizes timer interrupt interference across CPU cores.\nBy enabling a dedicated API to selectively invoke timer handling routines and\nsuppress non-critical inter-processor interrupts, our design significantly\nreduces jitter and response latency. Experiments conducted on an ARM-based\nmulticore platform demonstrate that the proposed mechanism consistently\nachieves sub-0.5 us response times, outperforming conventional Linux PREEMPT-RT\nconfigurations. These results highlight the potential of interrupt isolation as\na lightweight and effective strategy for deterministic real-time workloads in\ngeneral-purpose operating systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2d\u65ad\u9694\u79bb\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u4e2d\u5316\u548c\u6700\u5c0f\u5316\u5b9a\u65f6\u5668\u4e2d\u65ad\u5e72\u6270\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6296\u52a8\u548c\u54cd\u5e94\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u4e9a\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u3002", "motivation": "Linux\u4e2d\u5b9e\u65f6\u54cd\u5e94\u5e38\u53d7\u4e2d\u65ad\u4e89\u7528\u548c\u5b9a\u65f6\u5668\u5904\u7406\u5f00\u9500\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5b9e\u73b0\u4e9a\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e2d\u65ad\u9694\u79bb\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e13\u7528API\u9009\u62e9\u6027\u8c03\u7528\u5b9a\u65f6\u5668\u5904\u7406\u7a0b\u5e8f\u5e76\u6291\u5236\u975e\u5173\u952e\u5904\u7406\u5668\u95f4\u4e2d\u65ad\u3002", "result": "\u5728ARM\u591a\u6838\u5e73\u53f0\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u673a\u5236\u80fd\u7a33\u5b9a\u5b9e\u73b0\u4f4e\u4e8e0.5\u5fae\u79d2\u7684\u54cd\u5e94\u65f6\u95f4\uff0c\u4f18\u4e8e\u4f20\u7edfLinux PREEMPT-RT\u914d\u7f6e\u3002", "conclusion": "\u4e2d\u65ad\u9694\u79bb\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u4e14\u6709\u6548\u7684\u7b56\u7565\uff0c\u5728\u901a\u7528\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u4e3a\u786e\u5b9a\u6027\u5b9e\u65f6\u4efb\u52a1\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2509.04253", "pdf": "https://arxiv.org/pdf/2509.04253", "abs": "https://arxiv.org/abs/2509.04253", "authors": ["Siyuan He", "Songlin Jia", "Yuyan Bao", "Tiark Rompf"], "title": "When Lifetimes Liberate: A Type System for Arenas with Higher-Order Reachability Tracking", "categories": ["cs.PL"], "comment": null, "summary": "Static resource management in higher-order functional languages remains\nelusive due to tensions between control, expressiveness, and flexibility.\nRegion-based systems [Grossman et al. 2002; Tofte et al. 2001] offer control\nover lifetimes and expressive in-region sharing, but restrict resources to\nlexical scopes. Rust, an instance of ownership types [Clarke et al. 2013],\noffers non-lexical lifetimes and robust safety guarantees, yet its global\ninvariants make common sharing patterns hard to express. Reachability types\n[Wei et al. 2024] enable reasoning about sharing and separation, but lack\npractical tools for controlling resource lifetimes.\n  In this work, we try to unify their strengths. Our solution enables grouping\nresources as arenas for arbitrary sharing and static guarantees of lexically\nscoped lifetimes. Crucially, arenas and lexical lifetimes are not the only\nchoice: users may also manage resources individually, with non-lexical\nlifetimes. Regardless of mode, resources share the same type, preserving the\nhigher-order parametric nature of the language.\n  Obtaining static safety guarantee in a higher-order language with flexible\nsharing is nontrivial. To this end, we propose two new extensions atop\nreachability types [Wei et al. 2024]. First, A<: features a novel\ntwo-dimensional store model to enable coarse-grained reachability tracking for\narbitrarily shared resources within arenas. Building on this, {A}<: establishes\nlexical lifetime control with static guarantees. As the first reachability\nformalism presented for lifetime control, {A}<: avoids the complication of\nflow-sensitive reasoning and retains expressive power and simplicity. Both\ncalculi are formalized and proven type safe in Rocq.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u533a\u57df\u7cfb\u7edf\u548c\u53ef\u8fbe\u6027\u7c7b\u578b\u4f18\u52bf\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9759\u6001\u8d44\u6e90\u7ba1\u7406\u5b9e\u73b0\u7075\u6d3b\u5171\u4eab\u548c\u751f\u547d\u5468\u671f\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u9ad8\u9636\u51fd\u6570\u8bed\u8a00\u4e2d\u9759\u6001\u8d44\u6e90\u7ba1\u7406\u7684\u63a7\u5236\u3001\u8868\u8fbe\u6027\u548c\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6269\u5c55\uff1aA<:\u7528\u4e8e\u7c97\u7c92\u5ea6\u53ef\u8fbe\u6027\u8ffd\u8e2a\uff0c{A}<:\u7528\u4e8e\u8bcd\u6cd5\u751f\u547d\u5468\u671f\u63a7\u5236\u3002", "result": "\u5728Rocq\u4e2d\u5f62\u5f0f\u5316\u5e76\u9a8c\u8bc1\u4e86\u7c7b\u578b\u5b89\u5168\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u7edf\u4e00\u4e86\u533a\u57df\u7cfb\u7edf\u548c\u53ef\u8fbe\u6027\u7c7b\u578b\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u5171\u4eab\u548c\u751f\u547d\u5468\u671f\u63a7\u5236\u3002"}}
{"id": "2509.03560", "pdf": "https://arxiv.org/pdf/2509.03560", "abs": "https://arxiv.org/abs/2509.03560", "authors": ["Atanu Kundu", "Pratyay Sarkar", "Rajarshi Ray"], "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine Hybrid Systems", "categories": ["cs.LO"], "comment": null, "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCEGAR\u7684\u6709\u754c\u53ef\u8fbe\u6027\u5206\u6790\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u5206\u6bb5\u4eff\u5c04\u52a8\u6001\u7684\u7ec4\u6210\u6df7\u5408\u7cfb\u7edf\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u8ba1\u7b97\u4ea7\u54c1\u81ea\u52a8\u673a\uff0c\u5e76\u901a\u8fc7\u62bd\u8c61\u53cd\u4f8b\u9a8c\u8bc1\u548c\u4f18\u5316\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7ec4\u6210\u6df7\u5408\u7cfb\u7edf\u5728\u53ef\u8fbe\u6027\u5206\u6790\u4e2d\u9762\u4e34\u7684\u8bed\u4e49\u4fdd\u6301\u548c\u72b6\u6001\u7206\u70b8\u95ee\u9898\u3002", "method": "\u7ed3\u5408CEGAR\u539f\u5219\uff0c\u5728\u79bb\u6563\u62bd\u8c61\u4e2d\u641c\u7d22\u53cd\u4f8b\uff0c\u901a\u8fc7\u7b26\u53f7\u53ef\u8fbe\u6027\u5206\u6790\u8fdb\u884c\u72b6\u6001\u7a7a\u95f4\u7ec6\u5316\uff0c\u5e76\u91c7\u7528\u4e0d\u540c\u7ec4\u5408\u8bed\u4e49\u4f18\u5316\u6548\u7387\u3002", "result": "\u5728\u5de5\u5177SAT-Reach\u4e2d\u5b9e\u73b0\u4e86\u8be5\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7ec4\u6210\u6df7\u5408\u7cfb\u7edf\u7684\u53ef\u8fbe\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2509.03565", "pdf": "https://arxiv.org/pdf/2509.03565", "abs": "https://arxiv.org/abs/2509.03565", "authors": ["Qi Chen", "Jingxuan Wei", "Zhuoya Yao", "Haiguang Wang", "Gaowei Wu", "Bihui Yu", "Siyuan Li", "Cheng Tan"], "title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference", "categories": ["cs.CL", "cs.MM"], "comment": "Accepted to ACM MM 2025", "summary": "Understanding how scientific ideas evolve requires more than summarizing\nindividual papers-it demands structured, cross-document reasoning over\nthematically related research. In this work, we formalize multi-document\nscientific inference, a new task that extracts and aligns motivation,\nmethodology, and experimental results across related papers to reconstruct\nresearch development chains. This task introduces key challenges, including\ntemporally aligning loosely structured methods and standardizing heterogeneous\nexperimental tables. We present ResearchPulse, an agent-based framework that\nintegrates instruction planning, scientific content extraction, and structured\nvisualization. It consists of three coordinated agents: a Plan Agent for task\ndecomposition, a Mmap-Agent that constructs motivation-method mind maps, and a\nLchart-Agent that synthesizes experimental line charts. To support this task,\nwe introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper\nclusters. Experiments show that our system, despite using 7B-scale agents,\nconsistently outperforms strong baselines like GPT-4o in semantic alignment,\nstructural consistency, and visual fidelity. The dataset are available in\nhttps://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u591a\u6587\u6863\u79d1\u5b66\u63a8\u7406\u201d\u7684\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u63d0\u53d6\u548c\u5bf9\u9f50\u76f8\u5173\u8bba\u6587\u7684\u52a8\u673a\u3001\u65b9\u6cd5\u548c\u5b9e\u9a8c\u7ed3\u679c\u6765\u91cd\u6784\u7814\u7a76\u53d1\u5c55\u94fe\u3002\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aResearchPulse\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u8c03\u7684agent\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u4e8eGPT-4o\u7b49\u57fa\u7ebf\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u79d1\u5b66\u601d\u60f3\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u9700\u8981\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\u6765\u5206\u6790\u548c\u5bf9\u9f50\u76f8\u5173\u7814\u7a76\u8bba\u6587\u7684\u52a8\u673a\u3001\u65b9\u6cd5\u548c\u5b9e\u9a8c\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eagent\u7684\u6846\u67b6ResearchPulse\uff0c\u5305\u542bPlan Agent\u3001Mmap-Agent\u548cLchart-Agent\uff0c\u5206\u522b\u7528\u4e8e\u4efb\u52a1\u5206\u89e3\u3001\u6784\u5efa\u52a8\u673a-\u65b9\u6cd5\u601d\u7ef4\u5bfc\u56fe\u548c\u5408\u6210\u5b9e\u9a8c\u7ed3\u679c\u6298\u7ebf\u56fe\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c3d\u7ba1\u4f7f\u7528\u4e867B\u89c4\u6a21\u7684agent\uff0cResearchPulse\u5728\u8bed\u4e49\u5bf9\u9f50\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8eGPT-4o\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ResearchPulse\u4e3a\u591a\u6587\u6863\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.03678", "pdf": "https://arxiv.org/pdf/2509.03678", "abs": "https://arxiv.org/abs/2509.03678", "authors": ["Xianghan Wang", "Chingshuan Hsiao", "Shimei Qiu"], "title": "Promisedland: An XR Narrative Attraction Integrating Diorama-to-Virtual Workflow and Elemental Storytelling", "categories": ["cs.HC", "cs.MM"], "comment": "Accepted to the Proceedings of the 2025 11th International Conference\n  on Virtual Reality (ICVR 2025). ISBN: 979-8-3503-9272-2. \\c{opyright} 2025\n  IEEE. This is the author-accepted manuscript. The final version will be\n  available via IEEE Xplore", "summary": "Promisedland is a mixed-reality (MR) narrative attraction that combines\ncultural storytelling, ecological education, and an innovative hybrid\nproduction workflow. Set in a future Earth suffering from elemental imbalance,\nusers embark on an interactive journey guided by symbolic characters to restore\nharmony through the collection of five classical elements: metal, wood, water,\nfire, and earth. To prototype this experience, we introduce a low-cost,\nhigh-fidelity Diorama-to-Virtual pipeline - handcrafting physical scale models,\n3D scanning, and integrating them into Unreal Engine. This process enables\nrapid spatial prototyping while preserving the material expressiveness and\nnarrative consistency of the physical environment. To further enhance\nimmersion, the experience incorporates a Stewart Platform to provide motion\nfeedback synchronized with the virtual ride dynamics, reinforcing spatial\npresence and embodied engagement. The final prototype runs on Meta Quest,\nsupporting dynamic interactions and real-time visual feedback. Promisedland\noffers a replicable design blueprint for future XR narrative installations\nacross museums, cultural exhibitions, and themed entertainment. It proposes a\nnew framework for XR Narrative Attractions - where physical and digital\nelements converge to deepen immersion, agency, and emotional engagement.", "AI": {"tldr": "Promisedland\u662f\u4e00\u4e2a\u7ed3\u5408\u6587\u5316\u53d9\u4e8b\u4e0e\u751f\u6001\u6559\u80b2\u7684\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u53d9\u4e8b\u666f\u70b9\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u3001\u9ad8\u4fdd\u771f\u7684Diorama-to-Virtual\u6d41\u7a0b\u5feb\u901f\u539f\u578b\u5316\uff0c\u6700\u7ec8\u5728Meta Quest\u4e0a\u5b9e\u73b0\u4e92\u52a8\u4f53\u9a8c\u3002", "motivation": "\u89e3\u51b3\u672a\u6765\u5730\u7403\u5143\u7d20\u5931\u8861\u95ee\u9898\u7684\u53d9\u4e8b\u9700\u6c42\uff0c\u540c\u65f6\u63a2\u7d22\u7269\u7406\u4e0e\u6570\u5b57\u5143\u7d20\u7ed3\u5408\u7684\u6c89\u6d78\u5f0f\u53d9\u4e8b\u6846\u67b6\u3002", "method": "\u91c7\u7528\u624b\u5de5\u5236\u4f5c\u7269\u7406\u6a21\u578b\u30013D\u626b\u63cf\u540e\u96c6\u6210\u5230Unreal Engine\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408Stewart\u5e73\u53f0\u63d0\u4f9b\u52a8\u6001\u53cd\u9988\u3002", "result": "\u5f00\u53d1\u51fa\u652f\u6301\u5b9e\u65f6\u4e92\u52a8\u548c\u89c6\u89c9\u53cd\u9988\u7684\u539f\u578b\uff0c\u4e3a\u672a\u6765XR\u53d9\u4e8b\u88c5\u7f6e\u63d0\u4f9b\u53ef\u590d\u5236\u7684\u8bbe\u8ba1\u84dd\u56fe\u3002", "conclusion": "Promisedland\u4e3a\u535a\u7269\u9986\u548c\u6587\u5316\u5c55\u89c8\u7b49\u573a\u666f\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u7269\u7406\u4e0e\u6570\u5b57\u5143\u7d20\u7684\u65b0\u578bXR\u53d9\u4e8b\u6846\u67b6\u3002"}}
{"id": "2509.04085", "pdf": "https://arxiv.org/pdf/2509.04085", "abs": "https://arxiv.org/abs/2509.04085", "authors": ["Stanly Wilson", "Kwabena Adu-Duodu", "Yinhao Li", "Ringo Sham", "Yingli Wang", "Ellis Solaiman", "Charith Perera", "Rajiv Ranjan", "Omer Rana"], "title": "Trustworthy Second-hand Marketplace for Built Environment", "categories": ["cs.DC", "cs.ET"], "comment": null, "summary": "The construction industry faces significant challenges regarding material\nwaste and sustainable practices, necessitating innovative solutions that\nintegrate automation, traceability, and decentralised decision-making to enable\nefficient material reuse. This paper presents a blockchain-enabled digital\nmarketplace for sustainable construction material reuse, ensuring transparency\nand traceability using InterPlanetary File System (IPFS). The proposed\nframework enhances trust and accountability in material exchange, addressing\nkey challenges in industrial automation and circular supply chains. A framework\nhas been developed to demonstrate the operational processes of the marketplace,\nillustrating its practical application and effectiveness. Our contributions\nshow how the marketplace can facilitate the efficient and trustworthy exchange\nof reusable materials, representing a substantial step towards more sustainable\nconstruction practices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u94fe\u7684\u6570\u5b57\u5e02\u573a\uff0c\u7528\u4e8e\u53ef\u6301\u7eed\u5efa\u7b51\u6750\u6599\u7684\u518d\u5229\u7528\uff0c\u7ed3\u5408IPFS\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u65e8\u5728\u89e3\u51b3\u5efa\u7b51\u884c\u4e1a\u4e2d\u7684\u6750\u6599\u6d6a\u8d39\u548c\u53ef\u6301\u7eed\u6027\u95ee\u9898\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u5728\u6750\u6599\u6d6a\u8d39\u548c\u53ef\u6301\u7eed\u5b9e\u8df5\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u6765\u6574\u5408\u81ea\u52a8\u5316\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u5206\u6563\u51b3\u7b56\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6750\u6599\u518d\u5229\u7528\u3002", "method": "\u901a\u8fc7\u533a\u5757\u94fe\u6280\u672f\u548cIPFS\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u5b57\u5e02\u573a\u6846\u67b6\uff0c\u786e\u4fdd\u6750\u6599\u4ea4\u6362\u7684\u900f\u660e\u5ea6\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u5e02\u573a\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u5faa\u73af\u4f9b\u5e94\u94fe\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6570\u5b57\u5e02\u573a\u80fd\u591f\u4fc3\u8fdb\u9ad8\u6548\u4e14\u53ef\u4fe1\u7684\u5efa\u7b51\u6750\u6599\u518d\u5229\u7528\uff0c\u662f\u8fc8\u5411\u66f4\u53ef\u6301\u7eed\u5efa\u7b51\u5b9e\u8df5\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.03846", "pdf": "https://arxiv.org/pdf/2509.03846", "abs": "https://arxiv.org/abs/2509.03846", "authors": ["Md Rownak Hossain Chowdhury", "Mostafizur Rahman"], "title": "Hardware-Aware Data and Instruction Mapping for AI Tasks: Balancing Parallelism, I/O and Memory Tradeoffs", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "We introduce a mapping framework for deep learning inference that takes\nadvantage of predictable neural network behavior to plan both computation and\ncommunication ahead of time. The framework generates a unified stream of\ninstructions and data, enabling the hardware to execute operations and route\ninformation on its own, without frequent involvement from the host and with\nminimal off-chip memory use. This naturally reduces reliance on I/O, off-chip\nmemory, and host control. By leveraging fine-grained message passing on a\nprogrammable, message-based compute architecture, the framework keeps data\nmovement local and coordinates computation across the array using techniques\nsuch as stationary-weight reuse, in-array multicasting, and staged reductions.\nApplied to VGG-19, the framework sustains high utilization (88 to 92 percent),\nwith over 97 percent of messages generated internally and nearly 89 percent of\ntime consumed on-chip transfers. Computation throughput scales beyond 1 TFLOP/s\non larger arrays, while traffic reductions from reuse and local aggregation\nreach up to 100 MB per layer. Overall, the results highlight the effectiveness\nof streaming-based computation and show how our mapper enables this execution\nstyle by tightly coordinating data and instruction flow across the hardware.", "AI": {"tldr": "\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u524d\u89c4\u5212\u8ba1\u7b97\u548c\u901a\u4fe1\uff0c\u51cf\u5c11\u5bf9I/O\u3001\u7247\u5916\u5b58\u50a8\u548c\u4e3b\u673a\u63a7\u5236\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u786c\u4ef6\u5229\u7528\u7387\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u4e2d\u5bf9I/O\u3001\u7247\u5916\u5b58\u50a8\u548c\u4e3b\u673a\u63a7\u5236\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u786c\u4ef6\u5229\u7528\u7387\u3002", "method": "\u5229\u7528\u53ef\u9884\u6d4b\u7684\u795e\u7ecf\u7f51\u7edc\u884c\u4e3a\uff0c\u751f\u6210\u7edf\u4e00\u7684\u6307\u4ee4\u548c\u6570\u636e\u6d41\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u6d88\u606f\u4f20\u9012\u6280\u672f\uff0c\u5982\u9759\u6001\u6743\u91cd\u91cd\u7528\u3001\u9635\u5217\u5185\u591a\u64ad\u548c\u5206\u7ea7\u5f52\u7ea6\u3002", "result": "\u5728VGG-19\u4e0a\u5b9e\u73b0\u9ad8\u5229\u7528\u7387\uff0888-92%\uff09\uff0c\u5185\u90e8\u6d88\u606f\u751f\u6210\u5360\u6bd497%\uff0c\u7247\u5185\u4f20\u8f93\u65f6\u95f4\u5360\u6bd489%\uff0c\u8ba1\u7b97\u541e\u5410\u91cf\u8d85\u8fc71 TFLOP/s\u3002", "conclusion": "\u57fa\u4e8e\u6d41\u7684\u8ba1\u7b97\u65b9\u5f0f\u6709\u6548\uff0c\u6846\u67b6\u901a\u8fc7\u7d27\u5bc6\u534f\u8c03\u6570\u636e\u548c\u6307\u4ee4\u6d41\u5b9e\u73b0\u4e86\u9ad8\u6548\u6267\u884c\u3002"}}
{"id": "2509.04423", "pdf": "https://arxiv.org/pdf/2509.04423", "abs": "https://arxiv.org/abs/2509.04423", "authors": ["Fatima Zulfiqar Ali", "Atrooba Ilyas"], "title": "Design and Development of a Web Platform for Blood Donation Management", "categories": ["cs.SE", "cs.DB"], "comment": "10 pages, 6 figures, conference", "summary": "Blood donation is a critical component of healthcare, yet locating suitable\ndonors in emergencies often presents significant challenges. This paper\npresents the design and development of a Blood Donation Web Platform, a\nweb-based system that connects patients, donors, and administrators within a\ncentralized digital space. The platform allows interested donors to register\ntheir personal information, including blood group, contact details, and\navailability. Patients can search for donors based on blood group and location,\nand the system provides a list of nearby donors who are ready to donate. The\nplatform design was guided by use case, database, class, and sequence diagrams\nto ensure a well-structured and efficient system architecture. Modern web\ntechnologies, including PHP (Laravel framework), HTML, CSS, Bootstrap, and\nMySQL, supported by XAMPP and Visual Studio Code, were employed to implement a\ndynamic, interactive, and user-friendly platform. By streamlining donor\nrefgistration, blood requests, and communication, the proposed system reduces\ndelays and complexities in emergencies, improving timely accessibility of blood\nand enhancing overall efficiency in blood donation services.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u732e\u8840\u5e73\u53f0\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7b80\u5316\u732e\u8840\u8005\u6ce8\u518c\u548c\u8840\u6db2\u8bf7\u6c42\u6d41\u7a0b\uff0c\u63d0\u9ad8\u7d27\u6025\u60c5\u51b5\u4e0b\u7684\u8840\u6db2\u83b7\u53d6\u6548\u7387\u3002", "motivation": "\u732e\u8840\u5bf9\u533b\u7597\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u627e\u5230\u5408\u9002\u7684\u732e\u8840\u8005\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u96c6\u4e2d\u5316\u7684\u6570\u5b57\u5316\u5e73\u53f0\u3002", "method": "\u5e73\u53f0\u91c7\u7528PHP (Laravel\u6846\u67b6)\u3001HTML\u3001CSS\u7b49\u6280\u672f\uff0c\u7ed3\u5408\u7528\u4f8b\u56fe\u3001\u6570\u636e\u5e93\u8bbe\u8ba1\u7b49\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u52a8\u6001\u3001\u7528\u6237\u53cb\u597d\u7684\u7cfb\u7edf\u3002", "result": "\u8be5\u7cfb\u7edf\u7b80\u5316\u4e86\u8840\u6db2\u8bf7\u6c42\u6d41\u7a0b\uff0c\u51cf\u5c11\u4e86\u7d27\u6025\u60c5\u51b5\u4e0b\u7684\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u732e\u8840\u670d\u52a1\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u732e\u8840\u5e73\u53f0\u901a\u8fc7\u6570\u5b57\u5316\u7ba1\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7d27\u6025\u8840\u6db2\u4f9b\u5e94\u7684\u53ca\u65f6\u6027\u548c\u6574\u4f53\u6548\u7387\u3002"}}
{"id": "2509.03680", "pdf": "https://arxiv.org/pdf/2509.03680", "abs": "https://arxiv.org/abs/2509.03680", "authors": ["Ruofan Liang", "Kai He", "Zan Gojcic", "Igor Gilitschenski", "Sanja Fidler", "Nandita Vijaykumar", "Zian Wang"], "title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/LuxDiT/", "summary": "Estimating scene lighting from a single image or video remains a longstanding\nchallenge in computer vision and graphics. Learning-based approaches are\nconstrained by the scarcity of ground-truth HDR environment maps, which are\nexpensive to capture and limited in diversity. While recent generative models\noffer strong priors for image synthesis, lighting estimation remains difficult\ndue to its reliance on indirect visual cues, the need to infer global\n(non-local) context, and the recovery of high-dynamic-range outputs. We propose\nLuxDiT, a novel data-driven approach that fine-tunes a video diffusion\ntransformer to generate HDR environment maps conditioned on visual input.\nTrained on a large synthetic dataset with diverse lighting conditions, our\nmodel learns to infer illumination from indirect visual cues and generalizes\neffectively to real-world scenes. To improve semantic alignment between the\ninput and the predicted environment map, we introduce a low-rank adaptation\nfinetuning strategy using a collected dataset of HDR panoramas. Our method\nproduces accurate lighting predictions with realistic angular high-frequency\ndetails, outperforming existing state-of-the-art techniques in both\nquantitative and qualitative evaluations.", "AI": {"tldr": "LuxDiT\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u50cf\u6216\u89c6\u9891\u4e2d\u4f30\u8ba1HDR\u73af\u5883\u5149\u7167\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u8bad\u7ec3\u548c\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u7b56\u7565\uff0c\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u5b66\u4e2d\u4ece\u5355\u5f20\u56fe\u50cf\u6216\u89c6\u9891\u4f30\u8ba1\u573a\u666f\u5149\u7167\u7684\u957f\u671f\u6311\u6218\uff0c\u514b\u670d\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLuxDiT\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u751f\u6210HDR\u73af\u5883\u5149\u7167\u56fe\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u5e76\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u7b56\u7565\u63d0\u5347\u8f93\u5165\u4e0e\u8f93\u51fa\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u6a21\u578b\u80fd\u591f\u4ece\u95f4\u63a5\u89c6\u89c9\u7ebf\u7d22\u63a8\u65ad\u5149\u7167\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6709\u6548\u6cdb\u5316\uff0c\u751f\u6210\u5177\u6709\u9ad8\u9891\u7ec6\u8282\u7684\u51c6\u786e\u5149\u7167\u9884\u6d4b\u3002", "conclusion": "LuxDiT\u5728\u5149\u7167\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3a\u573a\u666f\u7167\u660e\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03653", "pdf": "https://arxiv.org/pdf/2509.03653", "abs": "https://arxiv.org/abs/2509.03653", "authors": ["Siddharth Samsi", "Dan Campbell", "Emanuel Scoullos", "Oded Green"], "title": "Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software", "categories": ["cs.DC"], "comment": null, "summary": "The HPEC Graph Challenge is a collection of benchmarks representing complex\nworkloads that test the hardware and software components of HPC systems, which\ntraditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph\nIsomorphism, focused on several compute-bound and memory-bound kernels. The\nmost recent of the challenges, the Anonymized Network Sensing Graph Challenge,\nrepresents a shift in direction, as it represents a longer end-to-end workload\nthat requires many more software components, including, but not limited to,\ndata I/O, data structures for representing graph data, and a wide range of\nfunctions for data preparation and network analysis. A notable feature of this\nnew graph challenge is the use of GraphBLAS to represent the computational\naspects of the problem statement. In this paper, we show an alternative\ninterpretation of the GraphBLAS formulations using the language of data\nscience. With this formulation, we show that the new graph challenge can be\nimplemented using off-the-shelf ETL tools available in open-source, enterprise\nsoftware such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software,\nRAPIDS cuDF and cupy, we enable significant software acceleration without\nrequiring any specific HPC code and show speedups, over the same code running\nwith Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an\nNVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.", "AI": {"tldr": "HPEC Graph Challenge \u901a\u8fc7\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c GraphBLAS \u6280\u672f\u5c55\u793a\u4e86\u4f7f\u7528\u5f00\u6e90\u5de5\u5177\uff08\u5982 NVIDIA RAPIDS\uff09\u5bf9\u56fe\u5f62\u5904\u7406\u7684\u663e\u8457\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982 LINPACK\uff09\u65e0\u6cd5\u5145\u5206\u6d4b\u8bd5 HPC \u7cfb\u7edf\u7684\u786c\u4ef6\u548c\u8f6f\u4ef6\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u590d\u6742\u7684\u56fe\u5f62\u5904\u7406\u5de5\u4f5c\u8d1f\u8f7d\u6765\u8bc4\u4f30\u548c\u6539\u8fdb\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5229\u7528 GraphBLAS \u548c RAPIDS \u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5de5\u5177\uff08\u5982 cuDF \u548c cupy\uff09\u91cd\u65b0\u5b9e\u73b0\u56fe\u5f62\u6311\u6218\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684 CPU \u4ee3\u7801\uff08\u5982 Pandas\uff09\u3002", "result": "\u5728 NVIDIA GPU\uff08A100\u3001H100 \u548c H200\uff09\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\uff0c\u901f\u5ea6\u63d0\u5347\u8303\u56f4\u4ece 147 \u500d\u5230 2185 \u500d\u4e0d\u7b49\u3002", "conclusion": "\u4f7f\u7528\u73b0\u6210\u7684 ETL \u5de5\u5177\uff08\u5982 RAPIDS\uff09\u53ef\u4ee5\u5728\u4e0d\u7f16\u5199\u4e13\u95e8 HPC \u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u52a0\u901f\u56fe\u5f62\u5904\u7406\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5f00\u6e90\u5de5\u5177\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.03667", "pdf": "https://arxiv.org/pdf/2509.03667", "abs": "https://arxiv.org/abs/2509.03667", "authors": ["Vivek Vasan", "Alexander Nico-Katz", "Boulat A. Bash", "Daniel C. Kilper", "Marco Ruffini"], "title": "Entanglement Purification With Finite Latency Classical Communication in Quantum Networks", "categories": ["cs.NI"], "comment": null, "summary": "Quantum networks rely on high fidelity entangled pairs distributed to nodes,\nbut maintaining their fidelity is challenged by environmental decoherence\nduring storage. Entanglement purification is used to restore fidelity, but the\nidle periods imposed by the associated classical communication delays\ncounteract this goal by exposing the states to further decoherence. In this\nwork, we analyze the practical viability of entanglement purification protocols\n(BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet\nprotocol (IP) communications networks. We present a comprehensive performance\nevaluation of these protocols in various network conditions for a range of\nquantum memory technologies. We employ a microscopic Lindblad treatment of the\nunderlying quantum dynamics, and use current-generation metropolitan IP network\nlatency statistics and parameters drawn from quantum memory testbeds. In doing\nso we identify the regions in which entanglement purification succeeds and\nfails, delineated by break-even iso-fidelity contours in the phase space. We\nthen determine the total number of entangled pairs required to complete a\nmulti-round purification protocol, and the steady-state throughput of entangled\npairs with purified fidelities that exceed application-specific thresholds.\nThis provides latency budgets, memory quality targets, and resource-overhead\nestimates for deploying purification on current and near-future networks.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5728\u975e\u5373\u65f6\u7ecf\u5178\u534f\u8c03\u4e0b\uff0c\u57fa\u4e8eIP\u7f51\u7edc\u7684\u91cf\u5b50\u7ea0\u7f20\u7eaf\u5316\u534f\u8bae\uff08BBPSSW\u3001DEJMPS\uff09\u7684\u53ef\u884c\u6027\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u548c\u91cf\u5b50\u5b58\u50a8\u6280\u672f\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u786e\u5b9a\u4e86\u6210\u529f\u4e0e\u5931\u8d25\u7684\u6761\u4ef6\u3002", "motivation": "\u91cf\u5b50\u7f51\u7edc\u4e2d\u7ea0\u7f20\u5bf9\u7684\u9ad8\u4fdd\u771f\u5ea6\u5b58\u50a8\u9762\u4e34\u73af\u5883\u9000\u76f8\u5e72\u7684\u6311\u6218\uff0c\u800c\u7eaf\u5316\u8fc7\u7a0b\u4e2d\u7684\u7ecf\u5178\u901a\u4fe1\u5ef6\u8fdf\u4f1a\u8fdb\u4e00\u6b65\u52a0\u5267\u9000\u76f8\u5e72\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5728\u5b9e\u9645\u7f51\u7edc\u6761\u4ef6\u4e0b\u7684\u7eaf\u5316\u534f\u8bae\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5fae\u89c2Lindblad\u65b9\u6cd5\u5206\u6790\u91cf\u5b50\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u5f53\u524d\u57ce\u57dfIP\u7f51\u7edc\u5ef6\u8fdf\u7edf\u8ba1\u548c\u91cf\u5b50\u5b58\u50a8\u6d4b\u8bd5\u5e73\u53f0\u7684\u53c2\u6570\uff0c\u8bc4\u4f30BBPSSW\u548cDEJMPS\u534f\u8bae\u7684\u6027\u80fd\u3002", "result": "\u786e\u5b9a\u4e86\u7ea0\u7f20\u7eaf\u5316\u6210\u529f\u4e0e\u5931\u8d25\u7684\u4e34\u754c\u6761\u4ef6\uff08\u901a\u8fc7\u4fdd\u771f\u5ea6\u7b49\u503c\u7ebf\u5212\u5206\uff09\uff0c\u5e76\u91cf\u5316\u4e86\u5b8c\u6210\u591a\u8f6e\u7eaf\u5316\u6240\u9700\u7684\u7ea0\u7f20\u5bf9\u6570\u91cf\u53ca\u5176\u7a33\u6001\u541e\u5410\u91cf\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f53\u524d\u53ca\u672a\u6765\u7f51\u7edc\u4e2d\u90e8\u7f72\u7ea0\u7f20\u7eaf\u5316\u63d0\u4f9b\u4e86\u5ef6\u8fdf\u9884\u7b97\u3001\u5b58\u50a8\u8d28\u91cf\u76ee\u6807\u548c\u8d44\u6e90\u5f00\u9500\u4f30\u7b97\u3002"}}
{"id": "2509.03875", "pdf": "https://arxiv.org/pdf/2509.03875", "abs": "https://arxiv.org/abs/2509.03875", "authors": ["Ziyou Jiang", "Mingyang Li", "Guowei Yang", "Lin Shi", "Qing Wang"], "title": "VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from Rich-Text Issue Report", "categories": ["cs.SE"], "comment": "25 pages, 7 figures, submitting to TOSEM journal", "summary": "Software vulnerabilities exist in open-source software (OSS), and the\ndevelopers who discover these vulnerabilities may submit issue reports (IRs) to\ndescribe their details. Security practitioners need to spend a lot of time\nmanually identifying vulnerability-related IRs from the community, and the time\ngap may be exploited by attackers to harm the system. Previously, researchers\nhave proposed automatic approaches to facilitate identifying these\nvulnerability-related IRs, but these works focus on textual descriptions but\nlack the comprehensive analysis of IR's rich-text information. In this paper,\nwe propose VulRTex, a reasoning-guided approach to identify\nvulnerability-related IRs with their rich-text information. In particular,\nVulRTex first utilizes the reasoning ability of the Large Language Model (LLM)\nto prepare the Vulnerability Reasoning Database with historical IRs. Then, it\nretrieves the relevant cases from the prepared reasoning database to generate\nreasoning guidance, which guides LLM to identify vulnerabilities by reasoning\nanalysis on target IRs' rich-text information. To evaluate the performance of\nVulRTex, we conduct experiments on 973,572 IRs, and the results show that\nVulRTex achieves the highest performance in identifying the\nvulnerability-related IRs and predicting CWE-IDs when the dataset is\nimbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and\n+10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches.\nFurthermore, VulRTex has been applied to identify 30 emerging vulnerabilities\nacross 10 representative OSS projects in 2024's GitHub IRs, and 11 of them are\nsuccessfully assigned CVE-IDs, which illustrates VulRTex's practicality.", "AI": {"tldr": "VulRTex\u662f\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u8bc6\u522b\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u6f0f\u6d1e\u76f8\u5173\u95ee\u9898\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4e30\u5bcc\u7684\u6587\u672c\u4fe1\u606f\u5b9e\u73b0\u9ad8\u6548\u6f0f\u6d1e\u8bc6\u522b\u3002", "motivation": "\u7531\u4e8e\u5f00\u6e90\u8f6f\u4ef6\u4e2d\u6f0f\u6d1e\u95ee\u9898\u7684\u62a5\u544a\uff08IRs\uff09\u9700\u8981\u5b89\u5168\u4eba\u5458\u624b\u52a8\u7b5b\u9009\uff0c\u8017\u65f6\u4e14\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u6587\u672c\u63cf\u8ff0\u800c\u5ffd\u7565\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u5206\u6790\u65b9\u6cd5\u3002", "method": "VulRTex\u5229\u7528LLM\u6784\u5efa\u6f0f\u6d1e\u63a8\u7406\u6570\u636e\u5e93\uff0c\u68c0\u7d22\u76f8\u5173\u6848\u4f8b\u751f\u6210\u63a8\u7406\u6307\u5bfc\uff0c\u901a\u8fc7\u76ee\u6807IR\u7684\u4e30\u5bcc\u6587\u672c\u4fe1\u606f\u8fdb\u884c\u63a8\u7406\u5206\u6790\uff0c\u5b9e\u73b0\u6f0f\u6d1e\u8bc6\u522b\u3002", "result": "\u5728973,572\u4e2aIRs\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVulRTex\u5728\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0cF1\u3001AUPRC\u548cMacro-F1\u5206\u522b\u63d0\u534711.0%\u300120.2%\u548c10.5%\uff0c\u4e14\u65f6\u95f4\u6210\u672c\u964d\u4f4e\u4e00\u534a\u3002\u6b64\u5916\uff0c\u6210\u529f\u8bc6\u522b2024\u5e74GitHub\u4e2d\u768430\u4e2a\u65b0\u5174\u6f0f\u6d1e\u3002", "conclusion": "VulRTex\u901a\u8fc7\u7ed3\u5408LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u4e30\u5bcc\u6587\u672c\u5206\u6790\uff0c\u9ad8\u6548\u4e14\u5b9e\u7528\u5730\u8bc6\u522b\u6f0f\u6d1e\u76f8\u5173\u95ee\u9898\uff0c\u5df2\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.04129", "pdf": "https://arxiv.org/pdf/2509.04129", "abs": "https://arxiv.org/abs/2509.04129", "authors": ["Mickael Randour"], "title": "Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on Controllers in Reactive Synthesis", "categories": ["cs.LO", "cs.AI", "cs.FL", "math.PR"], "comment": "Invited paper at RP 2025", "summary": "In the game-theoretic approach to controller synthesis, we model the\ninteraction between a system to be controlled and its environment as a game\nbetween these entities, and we seek an appropriate (e.g., winning or optimal)\nstrategy for the system. This strategy then serves as a formal blueprint for a\nreal-world controller. A common belief is that simple (e.g., using limited\nmemory) strategies are better: corresponding controllers are easier to conceive\nand understand, and cheaper to produce and maintain.\n  This invited contribution focuses on the complexity of strategies in a\nvariety of synthesis contexts. We discuss recent results concerning memory and\nrandomness, and take a brief look at what lies beyond our traditional notions\nof complexity for strategies.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u63a7\u5236\u5668\u5408\u6210\u4e2d\u7b56\u7565\u7684\u590d\u6742\u6027\uff0c\u5c24\u5176\u662f\u5185\u5b58\u548c\u968f\u673a\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5c55\u671b\u4e86\u4f20\u7edf\u590d\u6742\u6027\u6982\u5ff5\u7684\u6269\u5c55\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u63a7\u5236\u5668\u5408\u6210\u4e2d\u7b80\u5355\u7b56\u7565\uff08\u5982\u6709\u9650\u5185\u5b58\uff09\u7684\u4f18\u52bf\u53ca\u5176\u590d\u6742\u6027\uff0c\u4ee5\u63d0\u9ad8\u5b9e\u9645\u63a7\u5236\u5668\u7684\u53ef\u7406\u89e3\u6027\u548c\u7ecf\u6d4e\u6027\u3002", "method": "\u91c7\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u5c06\u7cfb\u7edf\u4e0e\u73af\u5883\u4e92\u52a8\u5efa\u6a21\u4e3a\u535a\u5f08\uff0c\u7814\u7a76\u4e0d\u540c\u5408\u6210\u80cc\u666f\u4e0b\u7684\u7b56\u7565\u590d\u6742\u6027\u3002", "result": "\u8ba8\u8bba\u4e86\u5173\u4e8e\u5185\u5b58\u548c\u968f\u673a\u6027\u7684\u6700\u65b0\u7814\u7a76\u6210\u679c\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5bf9\u7b56\u7565\u590d\u6742\u6027\u7814\u7a76\u7684\u73b0\u72b6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u53ef\u80fd\u6269\u5c55\u7684\u65b9\u5411\u3002"}}
{"id": "2509.03693", "pdf": "https://arxiv.org/pdf/2509.03693", "abs": "https://arxiv.org/abs/2509.03693", "authors": ["Yeaeun Gong", "Yifan Liu", "Lanyu Shang", "Na Wei", "Dong Wang"], "title": "Designing Effective AI Explanations for Misinformation Detection: A Comparative Study of Content, Social, and Combined Explanations", "categories": ["cs.HC", "cs.MM"], "comment": "To appear at CSCW 2025", "summary": "In this paper, we study the problem of AI explanation of misinformation,\nwhere the goal is to identify explanation designs that help improve users'\nmisinformation detection abilities and their overall user experiences. Our work\nis motivated by the limitations of current Explainable AI (XAI) approaches,\nwhich predominantly focus on content explanations that elucidate the linguistic\nfeatures and sentence structures of the misinformation. To address this\nlimitation, we explore various explanations beyond content explanation, such as\n\"social explanation\" that considers the broader social context surrounding\nmisinformation, as well as a \"combined explanation\" where both the content and\nsocial explanations are presented in scenarios that are either aligned or\nmisaligned with each other. To evaluate the comparative effectiveness of these\nAI explanations, we conduct two online crowdsourcing experiments in the\nCOVID-19 (Study 1 on Prolific) and Politics domains (Study 2 on MTurk). Our\nresults show that AI explanations are generally effective in aiding users to\ndetect misinformation, with effectiveness significantly influenced by the\nalignment between content and social explanations. We also find that the order\nin which explanation types are presented - specifically, whether a content or\nsocial explanation comes first - can influence detection accuracy, with\ndifferences found between the COVID-19 and Political domains. This work\ncontributes towards more effective design of AI explanations, fostering a\ndeeper understanding of how different explanation types and their combinations\ninfluence misinformation detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86AI\u89e3\u91ca\u5728\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u865a\u5047\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u63a2\u8ba8\u4e86\u8d85\u8d8a\u4f20\u7edf\u5185\u5bb9\u89e3\u91ca\u7684\u5176\u4ed6\u89e3\u91ca\u65b9\u5f0f\uff08\u5982\u793e\u4ea4\u89e3\u91ca\u53ca\u5176\u7ec4\u5408\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91caAI\uff08XAI\uff09\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5185\u5bb9\u89e3\u91ca\uff0c\u7f3a\u4e4f\u5bf9\u793e\u4f1a\u80cc\u666f\u7b49\u56e0\u7d20\u7684\u8003\u8651\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u591a\u89e3\u91ca\u7c7b\u578b\u4ee5\u6539\u8fdb\u7528\u6237\u8bc6\u522b\u865a\u5047\u4fe1\u606f\u7684\u80fd\u529b\u548c\u4f53\u9a8c\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e24\u79cd\u5728\u7ebf\u4f17\u5305\u5b9e\u9a8c\uff08COVID-19\u548c\u653f\u6cbb\u9886\u57df\uff09\uff0c\u6bd4\u8f83\u5185\u5bb9\u89e3\u91ca\u3001\u793e\u4ea4\u89e3\u91ca\u53ca\u5176\u7ec4\u5408\u7684\u6548\u679c\uff0c\u5e76\u8003\u5bdf\u89e3\u91ca\u987a\u5e8f\u7684\u5f71\u54cd\u3002", "result": "AI\u89e3\u91ca\u603b\u4f53\u6709\u6548\uff0c\u4f46\u6548\u679c\u53d7\u5185\u5bb9\u4e0e\u793e\u4ea4\u89e3\u91ca\u5bf9\u9f50\u5ea6\u53ca\u89e3\u91ca\u987a\u5e8f\u7684\u5f71\u54cd\uff0c\u4e0d\u540c\u9886\u57df\uff08\u5982COVID-19\u4e0e\u653f\u6cbb\uff09\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aAI\u89e3\u91ca\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u4e0d\u540c\u89e3\u91ca\u7c7b\u578b\u53ca\u5176\u7ec4\u5408\u5728\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u4f5c\u7528\u3002"}}
{"id": "2509.04250", "pdf": "https://arxiv.org/pdf/2509.04250", "abs": "https://arxiv.org/abs/2509.04250", "authors": ["Shota Arai", "David Selby", "Andrew Vargo", "Sebastian Vollmer"], "title": "How many patients could we save with LLM priors?", "categories": ["stat.ME", "cs.AI", "cs.ET", "cs.IR", "stat.AP"], "comment": "9 pages, 4 figures", "summary": "Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u5148\u9a8c\u5206\u5e03\u6539\u8fdb\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2d\u5fc3\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u4e0d\u826f\u4e8b\u4ef6\u9884\u6d4b\u7684\u6548\u80fd\u3002", "motivation": "\u901a\u8fc7LLM\u7f16\u7801\u7684\u77e5\u8bc6\u51cf\u5c11\u4e34\u5e8a\u8bd5\u9a8c\u6240\u9700\u60a3\u8005\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u7edf\u8ba1\u6548\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u6846\u67b6\uff0c\u76f4\u63a5\u4eceLLM\u83b7\u53d6\u53c2\u6570\u5316\u5148\u9a8c\u5206\u5e03\uff0c\u800c\u975e\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "LLM\u751f\u6210\u7684\u5148\u9a8c\u5206\u5e03\u4f18\u4e8e\u4f20\u7edf\u5143\u5206\u6790\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u8bd5\u9a8c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u57fa\u4e8e\u4e13\u5bb6\u77e5\u8bc6\u7684\u9014\u5f84\uff0c\u6709\u671b\u964d\u4f4e\u60a3\u8005\u6570\u91cf\u9700\u6c42\u5e76\u63d0\u5347\u836f\u7269\u5b89\u5168\u6027\u76d1\u6d4b\u3002"}}
{"id": "2509.04153", "pdf": "https://arxiv.org/pdf/2509.04153", "abs": "https://arxiv.org/abs/2509.04153", "authors": ["Safa Mohammed Sali", "Mahmoud Meribout", "Ashiyana Abdul Majeed"], "title": "Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations", "categories": ["cs.AR"], "comment": null, "summary": "This paper presents a comprehensive review of recent advances in deploying\nconvolutional neural networks (CNNs) for object detection, classification, and\ntracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand\nfor real-time computer vision applications in domains such as autonomous\nvehicles, robotics, and surveillance, FPGAs have emerged as a powerful\nalternative to GPUs and ASICs due to their reconfigurability, low power\nconsumption, and deterministic latency. We critically examine state-of-the-art\nFPGA implementations of CNN-based vision tasks, covering algorithmic\ninnovations, hardware acceleration techniques, and the integration of\noptimization strategies like pruning, quantization, and sparsity-aware methods\nto maximize performance within hardware constraints. This survey also explores\nthe landscape of modern FPGA platforms, including classical LUT-DSP based\narchitectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration\nPlatforms (ACAPs), comparing their capabilities in handling deep learning\nworkloads. Furthermore, we review available software development tools such as\nVitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the\ndesign and deployment of AI models on FPGAs. The paper uniquely discusses\nhybrid architecture that combine GPUs and FPGAs for collaborative acceleration\nof AI inference, addressing challenges related to energy efficiency and\nthroughput. Additionally, we highlight hardware-software co-design practices,\ndataflow optimizations, and pipelined processing techniques essential for\nreal-time inference on resource-constrained devices. Through this survey,\nresearchers and engineers are equipped with insights to develop\nnext-generation, power-efficient, and high-performance vision systems optimized\nfor FPGA deployment in edge and embedded applications.", "AI": {"tldr": "\u7efc\u8ff0\u4e86FPGA\u4e0a\u90e8\u7f72CNN\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u8ddf\u8e2a\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5f3a\u8c03\u4e86FPGA\u5728\u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u7684\u4f18\u52bf\u53ca\u5176\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u548c\u76d1\u63a7\u7b49\u9886\u57df\u5bf9\u5b9e\u65f6\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u7684\u9700\u6c42\u589e\u957f\uff0cFPGA\u56e0\u5176\u53ef\u91cd\u6784\u6027\u3001\u4f4e\u529f\u8017\u548c\u786e\u5b9a\u6027\u5ef6\u8fdf\u6210\u4e3aGPU\u548cASIC\u7684\u6709\u529b\u66ff\u4ee3\u54c1\u3002", "method": "\u7efc\u8ff0\u4e86\u6700\u65b0\u7684FPGA\u5b9e\u73b0\uff0c\u5305\u62ec\u7b97\u6cd5\u521b\u65b0\u3001\u786c\u4ef6\u52a0\u901f\u6280\u672f\uff0c\u4ee5\u53ca\u526a\u679d\u3001\u91cf\u5316\u548c\u7a00\u758f\u611f\u77e5\u65b9\u6cd5\u7b49\u4f18\u5316\u7b56\u7565\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u73b0\u4ee3FPGA\u5e73\u53f0\u548c\u8f6f\u4ef6\u5f00\u53d1\u5de5\u5177\u3002", "result": "\u603b\u7ed3\u4e86FPGA\u5728\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u5e76\u63d0\u51fa\u4e86GPU\u548cFPGA\u6df7\u5408\u67b6\u6784\u7684\u534f\u4f5c\u52a0\u901f\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u5f00\u53d1\u4e0b\u4e00\u4ee3\u9ad8\u6548\u80fd\u3001\u9ad8\u6027\u80fd\u89c6\u89c9\u7cfb\u7edf\u7684\u89c1\u89e3\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8fb9\u7f18\u548c\u5d4c\u5165\u5f0f\u5e94\u7528\u3002"}}
{"id": "2509.03753", "pdf": "https://arxiv.org/pdf/2509.03753", "abs": "https://arxiv.org/abs/2509.03753", "authors": ["Michael Greer"], "title": "Memory Optimization for Convex Hull Support Point Queries", "categories": ["cs.GR", "cs.CG", "cs.RO", "68U05", "I.3.5"], "comment": "6 pages, 15 figures", "summary": "This paper evaluates several improvements to the memory layout of convex\nhulls to improve computation times for support point queries. The support point\nquery is a fundamental part of common collision algorithms, and the work\npresented achieves a significant speedup depending on the number of vertices of\nthe convex hull.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u4f18\u5316\u51f8\u58f3\u5185\u5b58\u5e03\u5c40\uff0c\u663e\u8457\u63d0\u5347\u4e86\u652f\u6301\u70b9\u67e5\u8be2\u7684\u8ba1\u7b97\u901f\u5ea6\u3002", "motivation": "\u652f\u6301\u70b9\u67e5\u8be2\u662f\u78b0\u649e\u7b97\u6cd5\u7684\u57fa\u7840\u64cd\u4f5c\uff0c\u4f18\u5316\u5176\u6027\u80fd\u5bf9\u6574\u4f53\u7b97\u6cd5\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6539\u8fdb\u51f8\u58f3\u7684\u5185\u5b58\u5e03\u5c40\u8bbe\u8ba1\u3002", "result": "\u6839\u636e\u51f8\u58f3\u9876\u70b9\u6570\u91cf\u7684\u4e0d\u540c\uff0c\u8ba1\u7b97\u65f6\u95f4\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u5185\u5b58\u5e03\u5c40\u4f18\u5316\u662f\u63d0\u5347\u652f\u6301\u70b9\u67e5\u8be2\u6548\u7387\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2509.03755", "pdf": "https://arxiv.org/pdf/2509.03755", "abs": "https://arxiv.org/abs/2509.03755", "authors": ["John Augustine", "Soumyottam Chatterjee", "Valerie King", "Manish Kumar", "Shachar Meir", "David Peleg"], "title": "Distributed Download from an External Data Source in Asynchronous Faulty Settings", "categories": ["cs.DC"], "comment": null, "summary": "The distributedData Retrieval (DR) model consists of $k$ peers connected by a\ncomplete peer-to-peer communication network, and a trusted external data source\nthat stores an array $\\textbf{X}$ of $n$ bits ($n \\gg k$). Up to $\\beta k$ of\nthe peers might fail in any execution (for $\\beta \\in [0, 1)$). Peers can\nobtain the information either by inexpensive messages passed among themselves\nor through expensive queries to the source array $\\textbf{X}$. In the DR model,\nwe focus on designing protocols that minimize the number of queries performed\nby any nonfaulty peer (a measure referred to as query complexity) while\nmaximizing the resilience parameter $\\beta$.\n  The Download problem requires each nonfaulty peer to correctly learn the\nentire array $\\textbf{X}$. Earlier work on this problem focused on synchronous\ncommunication networks and established several deterministic and randomized\nupper and lower bounds. Our work is the first to extend the study of\ndistributed data retrieval to asynchronous communication networks. We address\nthe Download problem under both the Byzantine and crash failure models. We\npresent query-optimal deterministic solutions in an asynchronous model that can\ntolerate any fixed fraction $\\beta<1$ of crash faults. In the Byzantine failure\nmodel, it is known that deterministic protocols incur a query complexity of\n$\\Omega(n)$ per peer, even under synchrony. We extend this lower bound to\nrandomized protocols in the asynchronous model for $\\beta \\geq 1/2$, and\nfurther show that for $\\beta < 1/2$, a randomized protocol exists with\nnear-optimal query complexity. To the best of our knowledge, this is the first\nwork to address the Download problem in asynchronous communication networks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5f02\u6b65\u901a\u4fe1\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u6570\u636e\u68c0\u7d22\u95ee\u9898\uff0c\u9488\u5bf9\u5d29\u6e83\u548c\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\u63d0\u51fa\u4e86\u67e5\u8be2\u590d\u6742\u5ea6\u6700\u4f18\u7684\u786e\u5b9a\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6269\u5c55\u4e86\u968f\u673a\u534f\u8bae\u7684\u4e0b\u754c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u5728\u540c\u6b65\u901a\u4fe1\u7f51\u7edc\u4e0a\uff0c\u672c\u6587\u9996\u6b21\u6269\u5c55\u5230\u5f02\u6b65\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u68c0\u7d22\u4e2d\u7684\u6700\u5c0f\u67e5\u8be2\u590d\u6742\u5ea6\u548c\u6700\u5927\u5bb9\u9519\u80fd\u529b\u95ee\u9898\u3002", "method": "\u5728\u5f02\u6b65\u7f51\u7edc\u4e2d\uff0c\u63d0\u51fa\u4e86\u786e\u5b9a\u6027\u534f\u8bae\u5904\u7406\u5d29\u6e83\u6545\u969c\uff0c\u5e76\u9488\u5bf9\u62dc\u5360\u5ead\u6545\u969c\u6a21\u578b\u6269\u5c55\u4e86\u968f\u673a\u534f\u8bae\u7684\u4e0b\u754c\uff0c\u7ed9\u51fa\u4e86\u968f\u673a\u534f\u8bae\u7684\u5b58\u5728\u6761\u4ef6\u3002", "result": "\u5728\u5d29\u6e83\u6545\u969c\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u67e5\u8be2\u590d\u6742\u5ea6\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\uff1b\u5728\u62dc\u5360\u5ead\u6a21\u578b\u4e2d\uff0c\u9488\u5bf9\u03b2<1/2\u63d0\u51fa\u4e86\u8fd1\u4f18\u968f\u673a\u534f\u8bae\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5728\u5f02\u6b65\u7f51\u7edc\u4e2d\u89e3\u51b3\u4e86\u6570\u636e\u68c0\u7d22\u95ee\u9898\uff0c\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5bb9\u9519\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.03762", "pdf": "https://arxiv.org/pdf/2509.03762", "abs": "https://arxiv.org/abs/2509.03762", "authors": ["Sathwik Chadaga", "Eytan Modiano"], "title": "Drift Plus Optimistic Penalty -- A Learning Framework for Stochastic Network Optimization", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "We consider the problem of joint routing and scheduling in queueing networks,\nwhere the edge transmission costs are unknown. At each time-slot, the network\ncontroller receives noisy observations of transmission costs only for those\nedges it selects for transmission. The network controller's objective is to\nmake routing and scheduling decisions so that the total expected cost is\nminimized. This problem exhibits an exploration-exploitation trade-off,\nhowever, previous bandit-style solutions cannot be directly applied to this\nproblem due to the queueing dynamics. In order to ensure network stability, the\nnetwork controller needs to optimize throughput and cost simultaneously. We\nshow that the best achievable cost is lower bounded by the solution to a static\noptimization problem, and develop a network control policy using techniques\nfrom Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show\nthat the policy achieves a sub-linear regret of order $O(\\sqrt{T}\\log T)$, as\ncompared to the best policy that has complete knowledge of arrivals and costs.\nFinally, we evaluate the proposed policy using simulations and show that its\nregret is indeed sub-linear.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u961f\u5217\u7f51\u7edc\u4e2d\u672a\u77e5\u8fb9\u4f20\u8f93\u6210\u672c\u7684\u8054\u5408\u8def\u7531\u548c\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Lyapunov\u6f02\u79fb\u52a0\u60e9\u7f5a\u548c\u591a\u81c2\u8001\u864e\u673a\u6280\u672f\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u3002", "motivation": "\u89e3\u51b3\u961f\u5217\u7f51\u7edc\u4e2d\u672a\u77e5\u4f20\u8f93\u6210\u672c\u4e0b\u7684\u8054\u5408\u8def\u7531\u548c\u8c03\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u7f51\u7edc\u7a33\u5b9a\u6027\u3002", "method": "\u7ed3\u5408Lyapunov\u6f02\u79fb\u52a0\u60e9\u7f5a\u4f18\u5316\u548c\u591a\u81c2\u8001\u864e\u673a\u6280\u672f\uff0c\u8bbe\u8ba1\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u3002", "result": "\u7b56\u7565\u5b9e\u73b0\u4e86$O(\\sqrt{T}\\log T)$\u7684\u6b21\u7ebf\u6027\u9057\u61be\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b56\u7565\u5728\u672a\u77e5\u6210\u672c\u548c\u5230\u8fbe\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03876", "pdf": "https://arxiv.org/pdf/2509.03876", "abs": "https://arxiv.org/abs/2509.03876", "authors": ["Xingchu Chen", "Chengwei Liu", "Jialun Cao", "Yang Xiao", "Xinyue Cai", "Yeting Li", "Jingyi Shi", "Tianqi Sun", "Haiming Chen ang Wei Huo"], "title": "Vulnerability-Affected Versions Identification: How Far Are We?", "categories": ["cs.SE"], "comment": null, "summary": "Identifying which software versions are affected by a vulnerability is\ncritical for patching, risk mitigation.Despite a growing body of tools, their\nreal-world effectiveness remains unclear due to narrow evaluation scopes often\nlimited to early SZZ variants, outdated techniques, and small or\ncoarse-graineddatasets. In this paper, we present the first comprehensive\nempirical study of vulnerability affected versions identification. We curate a\nhigh quality benchmark of 1,128 real-world C/C++ vulnerabilities and\nsystematically evaluate 12 representative tools from both tracing and matching\nparadigms across four dimensions: effectiveness at both vulnerability and\nversion levels, root causes of false positives and negatives, sensitivity to\npatch characteristics, and ensemble potential. Our findings reveal fundamental\nlimitations: no tool exceeds 45.0% accuracy, with key challenges stemming from\nheuristic dependence, limited semantic reasoning, and rigid matching logic.\nPatch structures such as add-only and cross-file changes further hinder\nperformance. Although ensemble strategies can improve results by up to 10.1%,\noverall accuracy remains below 60.0%, highlighting the need for fundamentally\nnew approaches. Moreover, our study offers actionable insights to guide tool\ndevelopment, combination strategies, and future research in this critical area.\nFinally, we release the replicated code and benchmark on our website to\nencourage future contributions.outdated techniques, and small or coarse grained\ndatasets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5168\u9762\u5b9e\u8bc1\u7814\u7a76\u4e86\u6f0f\u6d1e\u5f71\u54cd\u7248\u672c\u8bc6\u522b\uff0c\u8bc4\u4f30\u4e8612\u79cd\u5de5\u5177\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u5de5\u5177\u7684\u51c6\u786e\u7387\u6700\u9ad8\u4e0d\u8d85\u8fc745.0%\uff0c\u5e76\u63d0\u51fa\u9700\u8981\u65b0\u65b9\u6cd5\u6765\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u6f0f\u6d1e\u5f71\u54cd\u7248\u672c\u7684\u8bc6\u522b\u5bf9\u8865\u4e01\u548c\u98ce\u9669\u7f13\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5de5\u5177\u7684\u5b9e\u6548\u6027\u56e0\u8bc4\u4f30\u8303\u56f4\u72ed\u7a84\u548c\u6280\u672f\u8fc7\u65f6\u800c\u672a\u660e\u786e\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6574\u74061,128\u4e2aC/C++\u6f0f\u6d1e\u7684\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e8612\u79cd\u4ee3\u8868\u5de5\u5177\u5728\u56db\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u5de5\u5177\u7684\u51c6\u786e\u7387\u6700\u9ad8\u4e3a45.0%\uff0c\u4e3b\u8981\u53d7\u542f\u53d1\u5f0f\u4f9d\u8d56\u3001\u8bed\u4e49\u63a8\u7406\u6709\u9650\u548c\u5339\u914d\u903b\u8f91\u50f5\u5316\u9650\u5236\u3002", "conclusion": "\u5c3d\u7ba1\u96c6\u6210\u7b56\u7565\u80fd\u63d0\u5347\u6548\u679c\uff0c\u4f46\u603b\u4f53\u51c6\u786e\u7387\u4ecd\u4f4e\u4e8e60.0%\uff0c\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u5e76\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u57fa\u51c6\u6570\u636e\u3002"}}
{"id": "2509.04347", "pdf": "https://arxiv.org/pdf/2509.04347", "abs": "https://arxiv.org/abs/2509.04347", "authors": ["Johanna Brunar", "Michael Pinsker", "Moritz Sch\u00f6bi"], "title": "Janus-faces of temporal constraint languages: a dichotomy of expressivity", "categories": ["cs.LO"], "comment": "21 pages", "summary": "The Bodirsky-K\\'ara classification of temporal constraint languages stands as\none of the earliest and most seminal complexity classifications within\ninfinite-domain Constraint Satisfaction Problems (CSPs), yet it remains one of\nthe most mysterious in terms of algorithms and algebraic invariants for the\ntractable cases. We show that those temporal languages which do not\npp-construct EVERYTHING (and thus by the classification are solvable in\npolynomial time) have, in fact, very limited expressive power as measured by\nthe graphs and hypergraphs they can pp-interpret. This limitation yields many\npreviously unknown algebraic consequences, while also providing new, uniform\nproofs for known invariance properties. In particular, we show that such\ntemporal constraint languages admit $4$-ary pseudo-Siggers polymorphisms -- a\nresult that sustains the possibility that the existence of such polymorphisms\nextends to the much broader context of the Bodirsky-Pinsker conjecture.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86Bodirsky-K\u00e1ra\u5206\u7c7b\u4e2d\u65f6\u95f4\u7ea6\u675f\u8bed\u8a00\u7684\u6709\u9650\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u4ee3\u6570\u7ed3\u679c\u548c\u4e00\u81f4\u8bc1\u660e\u3002", "motivation": "\u63a2\u7d22Bodirsky-K\u00e1ra\u5206\u7c7b\u4e2d\u65f6\u95f4\u7ea6\u675f\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\u548c\u590d\u6742\u6027\uff0c\u586b\u8865\u7b97\u6cd5\u548c\u4ee3\u6570\u4e0d\u53d8\u91cf\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u65f6\u95f4\u7ea6\u675f\u8bed\u8a00\u7684pp\u6784\u9020\u80fd\u529b\uff0c\u8bc1\u660e\u5176\u53ea\u80fdpp\u89e3\u91ca\u6709\u9650\u7684\u56fe\u548c\u8d85\u56fe\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u8bed\u8a00\u627f\u8ba44\u5143\u4f2aSiggers\u591a\u6001\u6027\uff0c\u5e76\u652f\u6301Bodirsky-Pinsker\u731c\u60f3\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u65f6\u95f4\u7ea6\u675f\u8bed\u8a00\u7684\u8868\u8fbe\u53d7\u9650\u6027\u4e3a\u66f4\u5e7f\u6cdb\u7684\u731c\u60f3\u63d0\u4f9b\u4e86\u65b0\u7684\u652f\u6301\u3002"}}
{"id": "2509.03692", "pdf": "https://arxiv.org/pdf/2509.03692", "abs": "https://arxiv.org/abs/2509.03692", "authors": ["Andreas Leibetseder", "Klaus Schoeffmann"], "title": "lifeXplore at the Lifelog Search Challenge 2021", "categories": ["cs.IR", "cs.MM"], "comment": null, "summary": "Since its first iteration in 2018, the Lifelog Search Challenge (LSC)\ncontinues to rise in popularity as an interactive lifelog data retrieval\ncompetition, co-located at the ACM International Conference on Multimedia\nRetrieval (ICMR). The goal of this annual live event is to search a large\ncorpus of lifelogging data for specifically announced memories using a\npurposefully developed tool within a limited amount of time. As long-standing\nparticipants, we present our improved lifeXplore - a retrieval system combining\nchronologic day summary browsing with interactive combinable concept filtering.\nCompared to previous versions, the tool is improved by incorporating temporal\nqueries, advanced day summary features as well as usability improvements.", "AI": {"tldr": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u81ea2018\u5e74\u8d77\u4e3e\u529e\u7684Lifelog Search Challenge (LSC)\u6bd4\u8d5b\u7684\u80cc\u666f\u548c\u76ee\u6807\uff0c\u5e76\u5c55\u793a\u4e86\u4f5c\u8005\u56e2\u961f\u6539\u8fdb\u7684lifeXplore\u68c0\u7d22\u7cfb\u7edf\u3002", "motivation": "LSC\u6bd4\u8d5b\u65e8\u5728\u6d4b\u8bd5\u56e2\u961f\u5728\u6709\u9650\u65f6\u95f4\u5185\u4f7f\u7528\u4e13\u95e8\u5f00\u53d1\u7684\u5de5\u5177\u68c0\u7d22\u5927\u91cf\u751f\u547d\u65e5\u5fd7\u6570\u636e\u7684\u80fd\u529b\u3002\u4f5c\u8005\u56e2\u961f\u5e0c\u671b\u901a\u8fc7\u6539\u8fdblifeXplore\u7cfb\u7edf\u63d0\u5347\u68c0\u7d22\u6548\u7387\u3002", "method": "\u6539\u8fdb\u7684lifeXplore\u7cfb\u7edf\u7ed3\u5408\u4e86\u6309\u65f6\u95f4\u987a\u5e8f\u6d4f\u89c8\u65e5\u6458\u8981\u548c\u4ea4\u4e92\u5f0f\u53ef\u7ec4\u5408\u6982\u5ff5\u8fc7\u6ee4\u529f\u80fd\uff0c\u5e76\u589e\u52a0\u4e86\u65f6\u95f4\u67e5\u8be2\u3001\u9ad8\u7ea7\u65e5\u6458\u8981\u529f\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u4f18\u5316\u3002", "result": "\u7cfb\u7edf\u76f8\u6bd4\u4e4b\u524d\u7248\u672c\u529f\u80fd\u66f4\u5f3a\u5927\uff0c\u7528\u6237\u4f53\u9a8c\u66f4\u597d\u3002", "conclusion": "\u6539\u8fdb\u540e\u7684lifeXplore\u7cfb\u7edf\u4e3aLSC\u6bd4\u8d5b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u68c0\u7d22\u5de5\u5177\uff0c\u5c55\u73b0\u4e86\u4f5c\u8005\u56e2\u961f\u5728\u751f\u547d\u65e5\u5fd7\u6570\u636e\u68c0\u7d22\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2509.03741", "pdf": "https://arxiv.org/pdf/2509.03741", "abs": "https://arxiv.org/abs/2509.03741", "authors": ["Eduardo Davalos", "Yike Zhang", "Shruti Jain", "Namrata Srivastava", "Trieu Truong", "Nafees-ul Haque", "Tristan Van", "Jorge Salas", "Sara McFadden", "Sun-Joo Cho", "Gautam Biswas", "Amanda Goodwin"], "title": "Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support", "categories": ["cs.HC", "cs.AI"], "comment": "22 pages, 9 figures, 3 tables, submitted to IUI2026", "summary": "Eye-tracking offers rich insights into student cognition and engagement, but\nremains underutilized in classroom-facing educational technology due to\nchallenges in data interpretation and accessibility. In this paper, we present\nthe iterative design and evaluation of a gaze-based learning analytics\ndashboard for English Language Arts (ELA), developed through five studies\ninvolving teachers and students. Guided by user-centered design and data\nstorytelling principles, we explored how gaze data can support reflection,\nformative assessment, and instructional decision-making. Our findings\ndemonstrate that gaze analytics can be approachable and pedagogically valuable\nwhen supported by familiar visualizations, layered explanations, and narrative\nscaffolds. We further show how a conversational agent, powered by a large\nlanguage model (LLM), can lower cognitive barriers to interpreting gaze data by\nenabling natural language interactions with multimodal learning analytics. We\nconclude with design implications for future EdTech systems that aim to\nintegrate novel data modalities in classroom contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u773c\u52a8\u6570\u636e\u5f00\u53d1\u9002\u7528\u4e8e\u8bfe\u5802\u7684\u5b66\u4e60\u5206\u6790\u4eea\u8868\u677f\uff0c\u7ed3\u5408\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u548c\u6570\u636e\u53d9\u4e8b\u539f\u5219\uff0c\u5c55\u793a\u4e86\u5176\u5728\u53cd\u601d\u3001\u5f62\u6210\u6027\u8bc4\u4f30\u548c\u6559\u5b66\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u773c\u52a8\u6570\u636e\u5728\u8bfe\u5802\u6559\u80b2\u6280\u672f\u4e2d\u5e94\u7528\u4e0d\u8db3\uff0c\u4e3b\u8981\u7531\u4e8e\u6570\u636e\u89e3\u91ca\u548c\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u6709\u6548\u5229\u7528\u773c\u52a8\u6570\u636e\u652f\u6301\u6559\u5b66\u3002", "method": "\u901a\u8fc7\u4e94\u9879\u7814\u7a76\u8fed\u4ee3\u8bbe\u8ba1\u548c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u773c\u52a8\u7684\u5b66\u4e60\u5206\u6790\u4eea\u8868\u677f\uff0c\u7ed3\u5408\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\u3001\u6570\u636e\u53d9\u4e8b\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u4ee3\u7406\u6765\u964d\u4f4e\u8ba4\u77e5\u8d1f\u62c5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u719f\u6089\u7684\u53ef\u89c6\u5316\u3001\u5206\u5c42\u6b21\u89e3\u91ca\u548c\u53d9\u4e8b\u652f\u6301\u53ef\u4ee5\u4f7f\u773c\u52a8\u6570\u636e\u66f4\u5177\u6559\u80b2\u4ef7\u503c\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f15\u5165\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u6570\u636e\u7406\u89e3\u7684\u96be\u5ea6\u3002", "conclusion": "\u672c\u6587\u4e3a\u672a\u6765\u6559\u80b2\u6280\u672f\u7cfb\u7edf\u6574\u5408\u65b0\u578b\u6570\u636e\u6a21\u6001\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u542f\u793a\uff0c\u5f3a\u8c03\u9700\u7ed3\u5408\u7528\u6237\u53cb\u597d\u6027\u548c\u6559\u5b66\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.04162", "pdf": "https://arxiv.org/pdf/2509.04162", "abs": "https://arxiv.org/abs/2509.04162", "authors": ["Safa Mohammed Sali", "Mahmoud Meribout", "Ashiyana Abdul Majeed"], "title": "Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations", "categories": ["cs.AR"], "comment": null, "summary": "Transformers and vision-language models (VLMs) have emerged as dominant\narchitectures in computer vision and multimodal AI, offering state-of-the-art\nperformance in tasks such as image classification, object detection, visual\nquestion answering, and caption generation. However, their high computational\ncomplexity, large memory footprints, and irregular data access patterns present\nsignificant challenges for deployment in latency- and power-constrained\nenvironments. Field-programmable gate arrays (FPGAs) provide an attractive\nhardware platform for such workloads due to their reconfigurability,\nfine-grained parallelism, and potential for energy-efficient acceleration. This\npaper presents a comprehensive review of design trade-offs, optimization\nstrategies, and implementation challenges for FPGA-based inference of\ntransformers and VLMs. We examine critical factors such as device-class\nselection, memory subsystem constraints, dataflow orchestration, quantization\nstrategies, sparsity exploitation, and toolchain choices, alongside\nmodality-specific issues unique to VLMs, including heterogeneous compute\nbalancing and cross-attention memory management. Additionally, we discuss\nemerging trends in hardware-algorithm co-design, highlighting innovations in\nattention mechanisms, compression, and modular overlays to improve efficiency\nand adaptability. Practical issues such as runtime flexibility, verification\noverhead, and the absence of standardized FPGA multimodal benchmarks are also\nconsidered. Finally, we outline future directions toward scalable, portable,\nand reconfigurable FPGA solutions that adapt to evolving model architectures\nwhile sustaining high utilization and predictable performance. This synthesis\noffers both a technical foundation and a forward-looking perspective to help\nbridge the gap between advanced multimodal AI models and efficient FPGA\ndeployment.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86FPGA\u4e0a\u90e8\u7f72Transformer\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bbe\u8ba1\u6743\u8861\u3001\u4f18\u5316\u7b56\u7565\u53ca\u5b9e\u73b0\u6311\u6218\uff0c\u63a2\u8ba8\u4e86\u786c\u4ef6\u4e0e\u7b97\u6cd5\u7684\u534f\u540c\u8bbe\u8ba1\u8d8b\u52bf\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "Transformer\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5185\u5b58\u5360\u7528\u65b9\u9762\u7684\u9ad8\u8981\u6c42\uff0c\u9650\u5236\u4e86\u5176\u5728\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u529f\u8017\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0cFPGA\u56e0\u5176\u53ef\u91cd\u6784\u6027\u548c\u9ad8\u6548\u80fd\u6210\u4e3a\u7406\u60f3\u5e73\u53f0\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bbe\u5907\u9009\u62e9\u3001\u5185\u5b58\u5b50\u7cfb\u7edf\u7ea6\u675f\u3001\u6570\u636e\u6d41\u7f16\u6392\u3001\u91cf\u5316\u7b56\u7565\u3001\u7a00\u758f\u6027\u5229\u7528\u53ca\u5de5\u5177\u94fe\u9009\u62e9\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u7ed3\u5408\u6a21\u6001\u7279\u5b9a\u7684\u5f02\u8d28\u8ba1\u7b97\u5e73\u8861\u548c\u8de8\u6ce8\u610f\u529b\u5185\u5b58\u7ba1\u7406\uff0c\u63d0\u51fa\u4f18\u5316\u65b9\u6848\u3002", "result": "\u603b\u7ed3\u4e86FPGA\u52a0\u901fTransformer\u548cVLMs\u7684\u73b0\u72b6\uff0c\u5f3a\u8c03\u4e86\u786c\u4ef6\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u7684\u521b\u65b0\uff0c\u5982\u6ce8\u610f\u529b\u673a\u5236\u548c\u6539\u8fdb\u7684\u538b\u7f29\u6280\u672f\u3002", "conclusion": "\u672a\u6765\u9700\u53d1\u5c55\u53ef\u6269\u5c55\u3001\u53ef\u79fb\u690d\u4e14\u53ef\u91cd\u6784\u7684FPGA\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u9002\u5e94\u4e0d\u65ad\u6f14\u8fdb\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6548\u4e14\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03775", "pdf": "https://arxiv.org/pdf/2509.03775", "abs": "https://arxiv.org/abs/2509.03775", "authors": ["Sankeerth Durvasula", "Sharanshangar Muhunthan", "Zain Moustafa", "Richard Chen", "Ruofan Liang", "Yushi Guan", "Nilesh Ahuja", "Nilesh Jain", "Selvakumar Panneer", "Nandita Vijaykumar"], "title": "ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world\nscenes with high quality and real-time rendering. Typically, a higher quality\nrepresentation can be achieved by using a large number of 3D Gaussians.\nHowever, using large 3D Gaussian counts significantly increases the GPU device\nmemory for storing model parameters. A large model thus requires powerful GPUs\nwith high memory capacities for training and has slower training/rendering\nlatencies due to the inefficiencies of memory access and data movement. In this\nwork, we introduce ContraGS, a method to enable training directly on compressed\n3DGS representations without reducing the Gaussian Counts, and thus with a\nlittle loss in model quality. ContraGS leverages codebooks to compactly store a\nset of Gaussian parameter vectors throughout the training process, thereby\nsignificantly reducing memory consumption. While codebooks have been\ndemonstrated to be highly effective at compressing fully trained 3DGS models,\ndirectly training using codebook representations is an unsolved challenge.\nContraGS solves the problem of learning non-differentiable parameters in\ncodebook-compressed representations by posing parameter estimation as a\nBayesian inference problem. To this end, ContraGS provides a framework that\neffectively uses MCMC sampling to sample over a posterior distribution of these\ncompressed representations. With ContraGS, we demonstrate that ContraGS\nsignificantly reduces the peak memory during training (on average 3.49X) and\naccelerated training and rendering (1.36X and 1.88X on average, respectively),\nwhile retraining close to state-of-art quality.", "AI": {"tldr": "ContraGS\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u4f7f\u7528\u538b\u7f29\u76843D\u9ad8\u65af\u8868\u793a\uff08\u4e0d\u51cf\u5c11\u9ad8\u65af\u6570\u91cf\uff09\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\u548c\u8bad\u7ec3/\u6e32\u67d3\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6280\u672f\u9700\u8981\u5927\u91cfGPU\u5185\u5b58\u5b58\u50a8\u6a21\u578b\u53c2\u6570\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u6e32\u67d3\u6548\u7387\u4f4e\u4e0b\u3002ContraGS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u538b\u7f29\u8868\u793a\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002", "method": "ContraGS\u5229\u7528\u7801\u672c\u7d27\u51d1\u5b58\u50a8\u9ad8\u65af\u53c2\u6570\u5411\u91cf\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u6846\u67b6\uff0c\u4f7f\u7528MCMC\u91c7\u6837\u5b66\u4e60\u4e0d\u53ef\u5fae\u5206\u7684\u538b\u7f29\u8868\u793a\u53c2\u6570\u3002", "result": "ContraGS\u5e73\u5747\u51cf\u5c113.49\u500d\u8bad\u7ec3\u5cf0\u503c\u5185\u5b58\uff0c\u52a0\u901f\u8bad\u7ec3\u548c\u6e32\u67d31.36\u500d\u548c1.88\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u73b0\u6709\u6280\u672f\u7684\u8d28\u91cf\u3002", "conclusion": "ContraGS\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u8bad\u7ec3\u538b\u7f293DGS\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\uff0c\u4e14\u8d28\u91cf\u63a5\u8fd1\u6700\u4f18\u3002"}}
{"id": "2509.04004", "pdf": "https://arxiv.org/pdf/2509.04004", "abs": "https://arxiv.org/abs/2509.04004", "authors": ["Avisek Sharma", "Satakshi Ghosh", "Buddhadeb Sau"], "title": "Gathering of asynchronous robots on circle with limited visibility using finite communication", "categories": ["cs.DC"], "comment": null, "summary": "This work addresses the gathering problem for a set of autonomous, anonymous,\nand homogeneous robots with limited visibility operating in a continuous\ncircle. The robots are initially placed at distinct positions, forming a\nrotationally asymmetric configuration. The robots agree on the clockwise\ndirection. In the $\\theta$-visibility model, a robot can only see those robots\non the circle that are at an angular distance $<\\theta$ from it. Di Luna\n\\textit{et. al.} [DISC'20] have shown that, in $\\pi/2$ visibility, gathering is\nimpossible. In addition, they provided an algorithm for robots with $\\pi$\nvisibility, operating under a semi-synchronous scheduler. In the $\\pi$\nvisibility model, only one point, the point at the angular distance $\\pi$ is\nremoved from the visibility. Ghosh \\textit{et. al.} [SSS'23] provided a\ngathering algorithm for $\\pi$ visibility model with robot having finite memory\n($\\mathcal{FSTA}$), operating under a special asynchronous scheduler.\n  If the robots can see all points on the circle, then the gathering can be\ndone by electing a leader in the weakest robot model under a fully asynchronous\nscheduler. However, previous works have shown that even the removal of one\npoint from the visibility makes gathering difficult. In both works, the robots\nhad rigid movement. In this work, we propose an algorithm that solves the\ngathering problem under the $\\pi$-visibility model for robots that have finite\ncommunication ability ($\\mathcal{FCOM}$). In this work the robot movement is\nnon-rigid and the robots work under a fully asynchronous scheduler.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6709\u9650\u53ef\u89c1\u6027\u6761\u4ef6\u4e0b\uff0c\u89e3\u51b3\u8fde\u7eed\u5706\u73af\u4e0a\u81ea\u4e3b\u3001\u533f\u540d\u4e14\u540c\u8d28\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\u7684\u7b97\u6cd5\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5728\u03c0-\u53ef\u89c1\u6027\u6a21\u578b\u4e0b\uff0c\u673a\u5668\u4eba\u5177\u6709\u6709\u9650\u901a\u4fe1\u80fd\u529b\uff08FCOM\uff09\u4e14\u975e\u521a\u6027\u79fb\u52a8\u65f6\u7684\u805a\u96c6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u03c0-\u53ef\u89c1\u6027\u6a21\u578b\uff0c\u673a\u5668\u4eba\u5177\u5907\u6709\u9650\u901a\u4fe1\u80fd\u529b\u548c\u975e\u521a\u6027\u79fb\u52a8\uff0c\u4e14\u5de5\u4f5c\u5728\u5b8c\u5168\u5f02\u6b65\u8c03\u5ea6\u5668\u4e0b\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u805a\u96c6\u95ee\u9898\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u6709\u9650\u901a\u4fe1\u80fd\u529b\u548c\u975e\u521a\u6027\u79fb\u52a8\u7684\u7ed3\u5408\u5728\u03c0-\u53ef\u89c1\u6027\u6a21\u578b\u4e0b\u662f\u53ef\u5b9e\u73b0\u7684\u3002"}}
{"id": "2509.03818", "pdf": "https://arxiv.org/pdf/2509.03818", "abs": "https://arxiv.org/abs/2509.03818", "authors": ["Sherwan Jalal Abdullah", "Sravan Reddy Chintareddy", "Victor S. Frost", "Shawn Keshmiri", "Morteza Hashemi"], "title": "A Versatile and Programmable UAV Platform for Radio Access Network and End-to-End Cellular Measurements", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "In this work, we develop a measurement platform to capture mobile network\nperformance metrics including coverage and quality of service in regions where\nconventional coverage testing approaches are frequently time-intensive,\nlabor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing\nmethods are used to collect cellular network performance metrics. However,\nthese approaches are inadequate in rural areas due to low-density population,\nand difficult terrain. The platform described here is a UAV-based and is\ndesigned to investigate the mobile network performance through aerial\noperations and gather Radio Access Network (RAN) signal alongside end-to-end\nnetwork performance metrics. Our platform gathers metrics through the\nintegration of an onboard computation unit and commercial off-the-shelf\ncellular modem. The gathered data are subsequently analyzed and displayed using\ngeospatial mapping utilities and statistical techniques to deliver key\nobservations on cellular network performance. Experimental results showed that\nthe received signal power improves at higher altitudes due to enhanced\nline-of-sight (LoS) conditions as expected. However, the signal quality\ndegrades as a result of increased interference from neighboring cells. The\nanalysis reveals that for most of the geographic area covered in the initial\nexperiments the system maintained acceptable signal quality, with adequate\nthroughput performance for both uplink and downlink communications, while\nmaintaining satisfactory round-trip time characteristics. Notably, the\nexperiment showed that a strong radio signal metric for a given cell does not\nnecessarily translate to consistent spatial coverage across the tested region.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u79fb\u52a8\u7f51\u7edc\u6027\u80fd\u6d4b\u91cf\u5e73\u53f0\uff0c\u7528\u4e8e\u5728\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u8017\u65f6\u3001\u8d39\u529b\u4e14\u5371\u9669\u7684\u5730\u65b9\u6536\u96c6\u8986\u76d6\u7387\u548c\u670d\u52a1\u8d28\u91cf\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u4f17\u5305\u65b9\u6cd5\u5728\u519c\u6751\u5730\u533a\u56e0\u4eba\u53e3\u5bc6\u5ea6\u4f4e\u548c\u5730\u5f62\u590d\u6742\u800c\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6d4b\u91cf\u65b9\u5f0f\u3002", "method": "\u5e73\u53f0\u901a\u8fc7\u96c6\u6210\u673a\u8f7d\u8ba1\u7b97\u5355\u5143\u548c\u5546\u7528\u8702\u7a9d\u8c03\u5236\u89e3\u8c03\u5668\uff0c\u6536\u96c6\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\u4fe1\u53f7\u548c\u7aef\u5230\u7aef\u6027\u80fd\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5730\u7406\u7a7a\u95f4\u6620\u5c04\u548c\u7edf\u8ba1\u5206\u6790\u5c55\u793a\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u9ad8\u7a7a\u4fe1\u53f7\u529f\u7387\u56e0\u89c6\u7ebf\u6761\u4ef6\u6539\u5584\u800c\u63d0\u5347\uff0c\u4f46\u4fe1\u53f7\u8d28\u91cf\u56e0\u90bb\u533a\u5e72\u6270\u800c\u4e0b\u964d\uff1b\u591a\u6570\u6d4b\u8bd5\u533a\u57df\u4fe1\u53f7\u8d28\u91cf\u548c\u541e\u5410\u91cf\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u65e0\u4eba\u673a\u5e73\u53f0\u80fd\u6709\u6548\u6536\u96c6\u548c\u5206\u6790\u79fb\u52a8\u7f51\u7edc\u6027\u80fd\u6570\u636e\uff0c\u4f46\u4fe1\u53f7\u5f3a\u5ea6\u4e0e\u7a7a\u95f4\u8986\u76d6\u4e00\u81f4\u6027\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.03896", "pdf": "https://arxiv.org/pdf/2509.03896", "abs": "https://arxiv.org/abs/2509.03896", "authors": ["Zushuai Zhang", "Elliott Wen", "Ewan Tempero"], "title": "Analyzing Variations in Dependency Distributions Due to Code Smell Interactions", "categories": ["cs.SE"], "comment": null, "summary": "The existence of dependencies between modules, such as classes, can mean that\nchanging a module triggers ripple effects that make maintenance complex and\ncostly, so the advice is to minimize dependencies between modules. It is\ntherefore important to understand the circumstances that can lead to increased\ndependencies. Recent studies suggest that code smells, which are\ncharacteristics of code that indicate potential design issues, may interact in\nways that increase dependencies between modules. In this study, we aim to\nconfirm previous observations and investigate whether and how the distribution\nof static dependencies changes in the presence of code smell interactions. We\nconducted a dependency analysis on 116 open-source Java systems to quantify the\ninteractions, comparing interactions among code smells and interactions between\ncode smells and non-code smells. Our results suggest that while interactions\nbetween code smell pairs are associated with increases in certain dependencies\nand decreases in others, overall, they are associated with an increase in total\ndependencies. For example, the median number of dependencies between Feature\nEnvy methods and Data Classes is seven times as many as when the methods are\nnon-Feature Envy methods, increasing from 1 to 7. This implies that developers\nshould prioritize addressing code smells that interact with each other, rather\nthan code smells that exist only in isolation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4ee3\u7801\u5f02\u5473\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u589e\u52a0\u6a21\u5757\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5206\u6790\u663e\u793a\u4ea4\u4e92\u5bfc\u81f4\u603b\u4f9d\u8d56\u589e\u52a0\uff0c\u5efa\u8bae\u4f18\u5148\u5904\u7406\u76f8\u4e92\u4f5c\u7528\u7684\u4ee3\u7801\u5f02\u5473\u3002", "motivation": "\u6a21\u5757\u95f4\u4f9d\u8d56\u589e\u52a0\u7ef4\u62a4\u590d\u6742\u6027\uff0c\u4ee3\u7801\u5f02\u5473\u7684\u76f8\u4e92\u4f5c\u7528\u53ef\u80fd\u52a0\u5267\u8fd9\u79cd\u4f9d\u8d56\uff0c\u9700\u6df1\u5165\u7406\u89e3\u5176\u5f71\u54cd\u3002", "method": "\u5bf9116\u4e2a\u5f00\u6e90Java\u7cfb\u7edf\u8fdb\u884c\u4f9d\u8d56\u5206\u6790\uff0c\u91cf\u5316\u4ee3\u7801\u5f02\u5473\u4e0e\u975e\u4ee3\u7801\u5f02\u5473\u95f4\u7684\u4ea4\u4e92\u3002", "result": "\u4ee3\u7801\u5f02\u5473\u5bf9\u7684\u4ea4\u4e92\u5bfc\u81f4\u67d0\u4e9b\u4f9d\u8d56\u589e\u52a0\uff0c\u603b\u4f53\u4f9d\u8d56\u4e0a\u5347\uff0c\u5982Feature Envy\u4e0eData Class\u4ea4\u4e92\u65f6\u4f9d\u8d56\u589e\u81f37\u500d\u3002", "conclusion": "\u5f00\u53d1\u8005\u5e94\u4f18\u5148\u5904\u7406\u76f8\u4e92\u4f5c\u7528\u7684\u4ee3\u7801\u5f02\u5473\u800c\u975e\u5355\u72ec\u5b58\u5728\u7684\u5f02\u5473\uff0c\u4ee5\u51cf\u5c11\u4f9d\u8d56\u5173\u7cfb\u3002"}}
{"id": "2509.04041", "pdf": "https://arxiv.org/pdf/2509.04041", "abs": "https://arxiv.org/abs/2509.04041", "authors": ["Daniel Raggi", "Gem Stapleton", "Mateja Jamnik", "Aaron Stockdill", "Grecia Garcia Garcia", "Peter C-H. Cheng"], "title": "Oruga: An Avatar of Representational Systems Theory", "categories": ["cs.AI", "cs.LO", "68T30, 68T27, 03B35", "I.2.4; I.2.3; F.4.1; F.4.3"], "comment": null, "summary": "Humans use representations flexibly. We draw diagrams, change representations\nand exploit creative analogies across different domains. We want to harness\nthis kind of power and endow machines with it to make them more compatible with\nhuman use. Previously we developed Representational Systems Theory (RST) to\nstudy the structure and transformations of representations. In this paper we\npresent Oruga (caterpillar in Spanish; a symbol of transformation), an\nimplementation of various aspects of RST. Oruga consists of a core of data\nstructures corresponding to concepts in RST, a language for communicating with\nthe core, and an engine for producing transformations using a method we call\nstructure transfer. In this paper we present an overview of the core and\nlanguage of Oruga, with a brief example of the kind of transformation that\nstructure transfer can execute.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Oruga\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u57fa\u4e8e\u8868\u793a\u7cfb\u7edf\u7406\u8bba\uff08RST\uff09\uff0c\u65e8\u5728\u8d4b\u4e88\u673a\u5668\u7075\u6d3b\u7684\u8868\u793a\u80fd\u529b\uff0c\u5305\u62ec\u6838\u5fc3\u6570\u636e\u7ed3\u6784\u3001\u901a\u4fe1\u8bed\u8a00\u548c\u7ed3\u6784\u8f6c\u79fb\u65b9\u6cd5\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u7075\u6d3b\u4f7f\u7528\u5404\u79cd\u8868\u793a\u65b9\u6cd5\uff08\u5982\u7ed8\u56fe\u3001\u7c7b\u6bd4\u7b49\uff09\uff0c\u4f5c\u8005\u5e0c\u671b\u8d4b\u4e88\u673a\u5668\u7c7b\u4f3c\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u4eba\u7c7b\u4f7f\u7528\u4e60\u60ef\u3002", "method": "Oruga\u7cfb\u7edf\u5305\u542bRST\u7406\u8bba\u7684\u6838\u5fc3\u6570\u636e\u7ed3\u6784\u3001\u901a\u4fe1\u8bed\u8a00\u4ee5\u53ca\u7ed3\u6784\u8f6c\u79fb\u5f15\u64ce\uff0c\u901a\u8fc7\u7ed3\u6784\u8f6c\u79fb\u65b9\u6cd5\u5b9e\u73b0\u8868\u793a\u8f6c\u6362\u3002", "result": "\u8bba\u6587\u6982\u8ff0\u4e86Oruga\u7cfb\u7edf\u7684\u6838\u5fc3\u548c\u8bed\u8a00\uff0c\u5e76\u5c55\u793a\u4e86\u7ed3\u6784\u8f6c\u79fb\u65b9\u6cd5\u7684\u8f6c\u6362\u793a\u4f8b\u3002", "conclusion": "Oruga\u7cfb\u7edf\u4e3a\u673a\u5668\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u8868\u793a\u80fd\u529b\uff0c\u6709\u671b\u63d0\u9ad8\u673a\u5668\u4e0e\u4eba\u7c7b\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2509.03792", "pdf": "https://arxiv.org/pdf/2509.03792", "abs": "https://arxiv.org/abs/2509.03792", "authors": ["Ryo Yonetani", "Kotaro Hara"], "title": "Map as a By-product: Collective Landmark Mapping from IMU Data and User-provided Texts in Situated Tasks", "categories": ["cs.HC"], "comment": "(c) 2025 Copyright held by the owner/author(s). Publication rights\n  licensed to ACM. This is the author's version of the work. It is posted here\n  for your personal use. Not for redistribution. The definitive Version of\n  Record was published in Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.\n  9, 3, Article 146 (September 2025), https://doi.org/10.1145/3749455", "summary": "This paper presents Collective Landmark Mapper, a novel map-as-a-by-product\nsystem for generating semantic landmark maps of indoor environments. Consider\nusers engaged in situated tasks that require them to navigate these\nenvironments and regularly take notes on their smartphones. Collective Landmark\nMapper exploits the smartphone's IMU data and the user's free text input during\nthese tasks to identify a set of landmarks encountered by the user. The\nidentified landmarks are then aggregated across multiple users to generate a\nunified map representing the positions and semantic information of all\nlandmarks. In developing the proposed system, we focused specifically on retail\napplications and conducted a formative interview with stakeholders to confirm\ntheir practical needs that motivate the map-as-a-byproduct approach. Our user\nstudy demonstrates the feasibility of the proposed system and its superior\nmapping performance in two different setups: creating a product availability\nmap from restocking checklist tasks at a retail store and constructing a room\nusage map from office inspection tasks, further demonstrating the potential\napplicability to non-retail applications.", "AI": {"tldr": "Collective Landmark Mapper\u662f\u4e00\u4e2a\u901a\u8fc7\u667a\u80fd\u624b\u673aIMU\u6570\u636e\u548c\u7528\u6237\u6587\u672c\u8f93\u5165\u751f\u6210\u5ba4\u5185\u8bed\u4e49\u5730\u6807\u5730\u56fe\u7684\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u96f6\u552e\u548c\u975e\u96f6\u552e\u573a\u666f\u3002", "motivation": "\u901a\u8fc7\u7528\u6237\u5728\u4efb\u52a1\u4e2d\u7684\u81ea\u7136\u884c\u4e3a\uff08\u5982\u8bb0\u7b14\u8bb0\uff09\u751f\u6210\u5730\u6807\u5730\u56fe\uff0c\u6ee1\u8db3\u96f6\u552e\u548c\u975e\u96f6\u552e\u5e94\u7528\u7684\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u5229\u7528\u667a\u80fd\u624b\u673aIMU\u6570\u636e\u548c\u7528\u6237\u6587\u672c\u8f93\u5165\u8bc6\u522b\u5730\u6807\uff0c\u5e76\u901a\u8fc7\u591a\u7528\u6237\u6570\u636e\u805a\u5408\u751f\u6210\u7edf\u4e00\u5730\u56fe\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\u7cfb\u7edf\u53ef\u884c\u4e14\u5728\u96f6\u552e\u548c\u975e\u96f6\u552e\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u7cfb\u7edf\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u5c24\u5176\u5728\u96f6\u552e\u548c\u975e\u96f6\u552e\u5e94\u7528\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.04173", "pdf": "https://arxiv.org/pdf/2509.04173", "abs": "https://arxiv.org/abs/2509.04173", "authors": ["Safa Sali", "Anis Meribout", "Ashiyana Majeed", "Mahmoud Meribout", "Juan Pablo", "Varun Tiwari", "Asma Baobaid"], "title": "Real-time Object Detection and Associated Hardware Accelerators Targeting Autonomous Vehicles: A Review", "categories": ["cs.AR"], "comment": null, "summary": "The efficiency of object detectors depends on factors like detection\naccuracy, processing time, and computational resources. Processing time is\ncrucial for real-time applications, particularly for autonomous vehicles (AVs),\nwhere instantaneous responses are vital for safety. This review paper provides\na concise yet comprehensive survey of real-time object detection (OD)\nalgorithms for autonomous cars delving into their hardware accelerators (HAs).\nNon-neural network-based algorithms, which use statistical image processing,\nhave been entirely substituted by AI algorithms, such as different models of\nconvolutional neural networks (CNNs). Their intrinsically parallel features led\nthem to be deployable into edge-based HAs of various types, where GPUs and, to\na lesser extent, ASIC (application-specific integrated circuit) remain the most\nwidely used. Throughputs of hundreds of frames/s (fps) could be reached;\nhowever, handling object detection for all the cameras available in a typical\nAV requires further hardware and algorithmic improvements. The intensive\ncompetition between AV providers has limited the disclosure of algorithms,\nfirmware, and even hardware platform details. This remains a hurdle for\nresearchers, as commercial systems provide valuable insights while academics\nundergo lengthy training and testing on restricted datasets and road scenarios.\nConsequently, many AV research papers may not be reflected in end products,\nbeing developed under limited conditions. This paper surveys state-of-the-art\nOD algorithms and aims to bridge the gap with technologies in commercial AVs.\nTo our knowledge, this aspect has not been addressed in earlier surveys. Hence,\nthe paper serves as a tangible reference for researchers designing future\ngenerations of vehicles, expected to be fully autonomous for comfort and\nsafety.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u8c03\u67e5\u4e86\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e2d\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u53ca\u5176\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5206\u6790\u4e86AI\u7b97\u6cd5\uff08\u5982CNN\uff09\u5982\u4f55\u53d6\u4ee3\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u786c\u4ef6\u5e73\u53f0\uff08\u5982GPU\u548cASIC\uff09\u7684\u5e94\u7528\u3002\u6587\u7ae0\u8fd8\u6307\u51fa\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u969c\u788d\uff0c\u5982\u5546\u4e1a\u673a\u5bc6\u9650\u5236\u5b66\u672f\u7814\u7a76\uff0c\u5e76\u65e8\u5728\u5f25\u5408\u5b66\u672f\u4e0e\u5546\u4e1a\u6280\u672f\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u9700\u8981\u9ad8\u6548\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u4ee5\u786e\u4fdd\u5b89\u5168\u3002\u5c3d\u7ba1AI\u7b97\u6cd5\uff08\u5982CNN\uff09\u548c\u786c\u4ef6\u52a0\u901f\u5668\uff08\u5982GPU\uff09\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5546\u4e1a\u673a\u5bc6\u548c\u6709\u9650\u7684\u7814\u7a76\u6761\u4ef6\u4ecd\u963b\u788d\u4e86\u5b66\u672f\u7814\u7a76\u7684\u6df1\u5165\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u5b8c\u5168\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u8bba\u6587\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u53ca\u5176\u786c\u4ef6\u52a0\u901f\u5668\uff08\u5982GPU\u548cASIC\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5546\u4e1a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u6280\u672f\u7ec6\u8282\u53ca\u5176\u5bf9\u5b66\u672f\u7814\u7a76\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1CNN\u7b49AI\u7b97\u6cd5\u548cGPU/ASIC\u786c\u4ef6\u52a0\u901f\u5668\u80fd\u591f\u5b9e\u73b0\u6bcf\u79d2\u6570\u767e\u5e27\u7684\u5904\u7406\u901f\u5ea6\uff0c\u4f46\u5904\u7406\u591a\u6444\u50cf\u5934\u6570\u636e\u4ecd\u9700\u786c\u4ef6\u548c\u7b97\u6cd5\u7684\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u5546\u4e1a\u4e0e\u5b66\u672f\u95f4\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u9650\u5236\u4e86\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u7efc\u8ff0\u5f53\u524d\u6280\u672f\u548c\u5546\u4e1a\u5e94\u7528\uff0c\u4e3a\u672a\u6765\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5e76\u547c\u5401\u52a0\u5f3a\u5b66\u672f\u4e0e\u4ea7\u4e1a\u754c\u7684\u5408\u4f5c\u4ee5\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2509.04047", "pdf": "https://arxiv.org/pdf/2509.04047", "abs": "https://arxiv.org/abs/2509.04047", "authors": ["Ashish Tiwari", "Satyam Bhardwaj", "Yash Bachwana", "Parag Sarvoday Sahu", "T. M. Feroz Ali", "Bhargava Chintalapati", "Shanmuganathan Raman"], "title": "TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "To appear in Pacific Graphics 2025 (CGF Journal Track), Project page:\n  https://yashbachwana.github.io/TensoIS/", "summary": "Estimating scattering parameters of heterogeneous media from images is a\nseverely under-constrained and challenging problem. Most of the existing\napproaches model BSSRDF either through an analysis-by-synthesis approach,\napproximating complex path integrals, or using differentiable volume rendering\ntechniques to account for heterogeneity. However, only a few studies have\napplied learning-based methods to estimate subsurface scattering parameters,\nbut they assume homogeneous media. Interestingly, no specific distribution is\nknown to us that can explicitly model the heterogeneous scattering parameters\nin the real world. Notably, procedural noise models such as Perlin and Fractal\nPerlin noise have been effective in representing intricate heterogeneities of\nnatural, organic, and inorganic surfaces. Leveraging this, we first create\nHeteroSynth, a synthetic dataset comprising photorealistic images of\nheterogeneous media whose scattering parameters are modeled using Fractal\nPerlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a\nlearning-based feed-forward framework to estimate these Perlin-distributed\nheterogeneous scattering parameters from sparse multi-view image observations.\nInstead of directly predicting the 3D scattering parameter volume, TensoIS uses\nlearnable low-rank tensor components to represent the scattering volume. We\nevaluate TensoIS on unseen heterogeneous variations over shapes from the\nHeteroSynth test set, smoke and cloud geometries obtained from open-source\nrealistic volumetric simulations, and some real-world samples to establish its\neffectiveness for inverse scattering. Overall, this study is an attempt to\nexplore Perlin noise distribution, given the lack of any such well-defined\ndistribution in literature, to potentially model real-world heterogeneous\nscattering in a feed-forward manner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6TensoIS\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u4f30\u8ba1\u5f02\u8d28\u6563\u5c04\u53c2\u6570\uff0c\u901a\u8fc7Fractal Perlin\u566a\u58f0\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u521b\u5efa\u4e86\u5408\u6210\u6570\u636e\u96c6HeteroSynth\u8fdb\u884c\u9a8c\u8bc1\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u65b9\u6cd5\u5047\u8bbe\u4ecb\u8d28\u662f\u5747\u5300\u7684\uff0c\u800c\u771f\u5b9e\u4e16\u754c\u4e2d\u5f02\u8d28\u4ecb\u8d28\u7684\u6563\u5c04\u53c2\u6570\u7f3a\u4e4f\u660e\u786e\u7684\u5206\u5e03\u6a21\u578b\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7Perlin\u566a\u58f0\u6a21\u62df\u8fd9\u79cd\u5f02\u8d28\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u9006\u6563\u5c04\u7684\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u521b\u5efa\u4e86\u5408\u6210\u6570\u636e\u96c6HeteroSynth\uff0c\u5176\u4e2d\u5f02\u8d28\u6563\u5c04\u53c2\u6570\u901a\u8fc7Fractal Perlin\u566a\u58f0\u5efa\u6a21\u3002\u968f\u540e\u63d0\u51fa\u4e86TensoIS\u6846\u67b6\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u7684\u4f4e\u79e9\u5f20\u91cf\u5206\u91cf\u6765\u8868\u793a\u6563\u5c04\u4f53\u79ef\u3002", "result": "TensoIS\u5728HeteroSynth\u6d4b\u8bd5\u96c6\u3001\u70df\u96fe\u548c\u4e91\u51e0\u4f55\u5f62\u72b6\u7684\u6a21\u62df\u6570\u636e\u4ee5\u53ca\u771f\u5b9e\u6837\u672c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63a2\u7d22\u4e86Perlin\u566a\u58f0\u5206\u5e03\u5bf9\u5f02\u8d28\u6563\u5c04\u53c2\u6570\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u901a\u8fc7TensoIS\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u9006\u6563\u5c04\u4f30\u8ba1\u3002"}}
{"id": "2509.04038", "pdf": "https://arxiv.org/pdf/2509.04038", "abs": "https://arxiv.org/abs/2509.04038", "authors": ["Benjamin Heymann"], "title": "Counterfactual simulations for large scale systems with burnout variables", "categories": ["cs.DC", "math.OC", "stat.ME"], "comment": null, "summary": "We consider large-scale systems influenced by burnout variables - state\nvariables that start active, shape dynamics, and irreversibly deactivate once\ncertain conditions are met. Simulating what-if scenarios in such systems is\ncomputationally demanding, as alternative trajectories often require sequential\nprocessing, which does not scale very well. This challenge arises in settings\nlike online advertising, because of campaigns budgets, complicating\ncounterfactual analysis despite rich data availability. We introduce a new type\nof algorithms based on what we refer to as uncertainty relaxation, that enables\nefficient parallel computation, significantly improving scalability for\ncounterfactual estimation in systems with burnout variables.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u677e\u5f1b\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5177\u6709burnout\u53d8\u91cf\u7cfb\u7edf\u7684\u53cd\u4e8b\u5b9e\u4f30\u8ba1\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5728\u5177\u6709burnout\u53d8\u91cf\u7684\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\uff0c\u6a21\u62df\u5047\u8bbe\u573a\u666f\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\uff0c\u5c24\u5176\u662f\u5728\u5728\u7ebf\u5e7f\u544a\u7b49\u9886\u57df\u3002", "method": "\u5f15\u5165\u4e86\u4e0d\u786e\u5b9a\u6027\u677e\u5f1b\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "result": "\u65b0\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u53cd\u4e8b\u5b9e\u4f30\u8ba1\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u677e\u5f1b\u7b97\u6cd5\u4e3a\u5177\u6709burnout\u53d8\u91cf\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u53cd\u4e8b\u5b9e\u4f30\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2509.03901", "pdf": "https://arxiv.org/pdf/2509.03901", "abs": "https://arxiv.org/abs/2509.03901", "authors": ["Katarzyna Kosek-Szott", "Szymon Szott", "Wojciech Ciezobka", "Maksymilian Wojnar", "Krzysztof Rusek", "Jonathan Segev"], "title": "Indoor Positioning with Wi-Fi Location: A Survey of IEEE 802.11mc/az/bk Fine Timing Measurement Research", "categories": ["cs.NI"], "comment": "30 pages, survey paper", "summary": "Indoor positioning is an enabling technology for home, office, and industrial\nnetwork users because it provides numerous information and communication\ntechnology (ICT) and Internet of things (IoT) functionalities such as indoor\nnavigation, smart meter localization, asset tracking, support for emergency\nservices, and detection of hazardous situations. The IEEE 802.11mc fine timing\nmeasurement (FTM) protocol (commercially known as Wi-Fi Location) has great\npotential to enable indoor positioning in future generation devices, primarily\nbecause of the high availability of Wi-Fi networks, FTM's high accuracy and\ndevice support. Furthermore, new FTM enhancements are available in the released\n(802.11az) and recently completed (802.11bk) amendments. Despite the multitude\nof literature reviews on indoor positioning, a survey dedicated to FTM and its\nrecent enhancements has so far been lacking. We fill this gap by classifying\nand reviewing over 180 research papers related to the practical accuracy\nachieved with FTM, methods for improving its accuracy (also with machine\nlearning), combining FTM with other indoor positioning systems, FTM-based\napplications, and security issues. Based on the conducted survey, we summarize\nthe most important research achievements and formulate open areas for further\nresearch.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86IEEE 802.11mc FTM\u534f\u8bae\u5728\u5ba4\u5185\u5b9a\u4f4d\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u6700\u65b0\u589e\u5f3a\u6280\u672f\uff0c\u586b\u8865\u4e86\u76f8\u5173\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u603b\u7ed3\u4e86\u91cd\u8981\u7814\u7a76\u6210\u679c\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5ba4\u5185\u5b9a\u4f4d\u6280\u672f\u4e3a\u5bb6\u5ead\u3001\u529e\u516c\u5ba4\u548c\u5de5\u4e1a\u7f51\u7edc\u7528\u6237\u63d0\u4f9b\u4e86\u591a\u79cd\u529f\u80fd\uff0c\u5982\u5bfc\u822a\u3001\u8d44\u4ea7\u8ddf\u8e2a\u548c\u7d27\u6025\u670d\u52a1\u652f\u6301\u3002IEEE 802.11mc FTM\u534f\u8bae\u56e0\u5176\u9ad8\u7cbe\u5ea6\u548c\u8bbe\u5907\u652f\u6301\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\u3002", "method": "\u4f5c\u8005\u5206\u7c7b\u5e76\u56de\u987e\u4e86180\u591a\u7bc7\u7814\u7a76\u8bba\u6587\uff0c\u6db5\u76d6FTM\u5b9e\u9645\u7cbe\u5ea6\u3001\u673a\u5668\u5b66\u4e60\u6539\u8fdb\u65b9\u6cd5\u3001\u4e0e\u5176\u4ed6\u5b9a\u4f4d\u7cfb\u7edf\u7684\u7ed3\u5408\u3001\u5e94\u7528\u53ca\u5b89\u5168\u95ee\u9898\u3002", "result": "\u603b\u7ed3\u4e86FTM\u5728\u5ba4\u5185\u5b9a\u4f4d\u4e2d\u7684\u91cd\u8981\u6210\u679c\uff0c\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u7efc\u8ff0\uff0c\u8bba\u6587\u660e\u786e\u4e86FTM\u7814\u7a76\u7684\u73b0\u72b6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2509.03900", "pdf": "https://arxiv.org/pdf/2509.03900", "abs": "https://arxiv.org/abs/2509.03900", "authors": ["Yuvraj Agrawal"], "title": "The Auth Shim: A Lightweight Architectural Pattern for Integrating Enterprise SSO with Standalone Open-Source Applications", "categories": ["cs.SE", "cs.CR"], "comment": null, "summary": "Open-source software OSS is widely adopted in enterprise settings, but\nstandalone tools often lack native support for protocols like SAML or OIDC,\ncreating a critical security integration gap. This paper introduces and\nformalizes the Auth Shim, a lightweight architectural pattern designed to solve\nthis problem. The Auth Shim is a minimal, external proxy service that acts as a\ncompatibility layer, translating requests from an enterprise Identity Provider\nIdP into the native session management mechanism of a target application. A key\nprerequisite for this pattern is that the target application must expose a\nprogrammatic, secure administrative API. We present a case study of the\npattern's implementation at Adobe to integrate a popular OSS BI tool with Okta\nSAML, which enabled automated Role-Based Access Control RBAC via IAM group\nmapping and eliminated manual user provisioning. By defining its components,\ninteractions, and production deployment considerations, this paper provides a\nreusable, secure, and cost-effective blueprint for integrating any standalone\nOSS tool into an enterprise SSO ecosystem, thereby enabling organizations to\nembrace open-source innovation without compromising on security governance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAuth Shim\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\u6a21\u5f0f\uff0c\u7528\u4e8e\u89e3\u51b3\u4f01\u4e1a\u73af\u5883\u4e2d\u5f00\u6e90\u8f6f\u4ef6\uff08OSS\uff09\u7f3a\u4e4f\u5bf9SAML\u6216OIDC\u534f\u8bae\u539f\u751f\u652f\u6301\u7684\u5b89\u5168\u96c6\u6210\u95ee\u9898\u3002", "motivation": "\u4f01\u4e1a\u5e7f\u6cdb\u91c7\u7528\u5f00\u6e90\u8f6f\u4ef6\uff0c\u4f46\u8bb8\u591a\u5de5\u5177\u7f3a\u4e4f\u5bf9\u73b0\u4ee3\u8eab\u4efd\u534f\u8bae\u7684\u539f\u751f\u652f\u6301\uff0c\u5bfc\u81f4\u5b89\u5168\u96c6\u6210\u56f0\u96be\u3002", "method": "Auth Shim\u4f5c\u4e3a\u4e00\u79cd\u5916\u90e8\u4ee3\u7406\u670d\u52a1\uff0c\u5c06\u4f01\u4e1a\u8eab\u4efd\u63d0\u4f9b\u8005\uff08IdP\uff09\u7684\u8bf7\u6c42\u8f6c\u6362\u4e3a\u76ee\u6807\u5e94\u7528\u7a0b\u5e8f\u7684\u539f\u751f\u4f1a\u8bdd\u7ba1\u7406\u673a\u5236\u3002", "result": "\u901a\u8fc7\u5728Adobe\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\u5b9e\u65bd\u6b64\u6a21\u5f0f\uff0c\u6210\u529f\u5c06\u4e00\u4e2a\u6d41\u884c\u7684OSS BI\u5de5\u5177\u4e0eOkta SAML\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u7684\u57fa\u4e8e\u89d2\u8272\u7684\u8bbf\u95ee\u63a7\u5236\uff08RBAC\uff09\u3002", "conclusion": "Auth Shim\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u7528\u3001\u5b89\u5168\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u6cd5\uff0c\u5e2e\u52a9\u4f01\u4e1a\u5728\u4e0d\u727a\u7272\u5b89\u5168\u6cbb\u7406\u7684\u524d\u63d0\u4e0b\u96c6\u6210\u5f00\u6e90\u5de5\u5177\u5230\u4f01\u4e1a\u5355\u70b9\u767b\u5f55\uff08SSO\uff09\u751f\u6001\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2509.04192", "pdf": "https://arxiv.org/pdf/2509.04192", "abs": "https://arxiv.org/abs/2509.04192", "authors": ["Vera Koponen"], "title": "Domain size asymptotics for Markov logic networks", "categories": ["cs.AI", "cs.LO", "math.LO", "68T27, 68T30, 68T37, 03C13", "I.2; F.4; G.3"], "comment": null, "summary": "A Markov logic network (MLN) determines a probability distribution on the set\nof structures, or ``possible worlds'', with an arbitrary finite domain. We\nstudy the properties of such distributions as the domain size tends to\ninfinity. Three types of concrete examples of MLNs will be considered, and the\nproperties of random structures with domain sizes tending to infinity will be\nstudied: (1) Arbitrary quantifier-free MLNs over a language with only one\nrelation symbol which has arity 1. In this case we give a pretty complete\ncharacterization of the possible limit behaviours of random structures. (2) An\nMLN that favours graphs with fewer triangles (or more generally, fewer\nk-cliques). As a corollary of the analysis a ``$\\delta$-approximate 0-1 law''\nfor first-order logic is obtained. (3) An MLN that favours graphs with fewer\nvertices with degree higher than a fixed (but arbitrary) number. The analysis\nshows that depending on which ``soft constraints'' an MLN uses the limit\nbehaviour of random structures can be quite different, and the weights of the\nsoft constraints may, or may not, have influence on the limit behaviour. It\nwill also be demonstrated, using (1), that quantifier-free MLNs and lifted\nBayesian networks (in a broad sense) are asymptotically incomparable, roughly\nmeaning that there is a sequence of distributions on possible worlds with\nincreasing domain sizes that can be defined by one of the formalisms but not\neven approximated by the other. In a rather general context it is also shown\nthat on large domains the distribution determined by an MLN concentrates almost\nall its probability mass on a totally different part of the space of possible\nworlds than the uniform distribution does.", "AI": {"tldr": "\u7814\u7a76\u4e86\u9a6c\u5c14\u53ef\u592b\u903b\u8f91\u7f51\u7edc\uff08MLN\uff09\u5728\u65e0\u9650\u57df\u5927\u5c0f\u4e0b\u7684\u968f\u673a\u7ed3\u6784\u7279\u6027\uff0c\u901a\u8fc7\u4e09\u4e2a\u5177\u4f53\u4f8b\u5b50\u5c55\u793a\u4e86\u4e0d\u540c\u6743\u91cd\u7ea6\u675f\u5bf9\u6781\u9650\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u4e86MLN\u4e0e\u63d0\u5347\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u6e10\u8fd1\u4e0d\u53ef\u6bd4\u6027\u3002", "motivation": "\u63a2\u8ba8MLN\u5728\u65e0\u9650\u57df\u4e0b\u7684\u6982\u7387\u5206\u5e03\u7279\u6027\uff0c\u4ee5\u53ca\u4e0d\u540c\u7ea6\u675f\u6761\u4ef6\u5bf9\u7ed3\u6784\u6781\u9650\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u586b\u8865\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u95f4\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e86\u4e09\u79cd\u4e0d\u540c\u7c7b\u578b\u7684MLN\u6848\u4f8b\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u548c\u6570\u5b66\u8bc1\u660e\uff0c\u7814\u7a76\u4e86\u968f\u673a\u7ed3\u6784\u7684\u6781\u9650\u884c\u4e3a\u53ca\u5176\u4e0e\u6743\u91cd\u7ea6\u675f\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u7ea6\u675f\u6761\u4ef6\u7684MLN\u5728\u6781\u9650\u884c\u4e3a\u4e0a\u6709\u663e\u8457\u5dee\u5f02\uff0c\u6743\u91cd\u7ea6\u675f\u53ef\u80fd\u5f71\u54cd\u4e5f\u53ef\u80fd\u4e0d\u5f71\u54cd\u7ed3\u679c\uff1b\u540c\u65f6\u8bc1\u660e\u4e86MLN\u4e0e\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u6e10\u8fd1\u4e0d\u53ef\u6bd4\u6027\u3002", "conclusion": "MLN\u5728\u5927\u57df\u4e0a\u7684\u5206\u5e03\u4e0e\u5747\u5300\u5206\u5e03\u5b8c\u5168\u4e0d\u540c\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u6982\u7387\u5efa\u6a21\u4e2d\u7684\u72ec\u7279\u6f5c\u529b\u3002"}}
{"id": "2509.03883", "pdf": "https://arxiv.org/pdf/2509.03883", "abs": "https://arxiv.org/abs/2509.03883", "authors": ["Haiwei Xue", "Xiangyang Luo", "Zhanghao Hu", "Xin Zhang", "Xunzhi Xiang", "Yuqin Dai", "Jianzhuang Liu", "Zhensong Zhang", "Minglei Li", "Jian Yang", "Fei Ma", "Zhiyong Wu", "Changpeng Yang", "Zonghong Dai", "Fei Richard Yu"], "title": "Human Motion Video Generation: A Survey", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by TPAMI. Github Repo:\n  https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation IEEE Access:\n  https://ieeexplore.ieee.org/document/11106267", "summary": "Human motion video generation has garnered significant research interest due\nto its broad applications, enabling innovations such as photorealistic singing\nheads or dynamic avatars that seamlessly dance to music. However, existing\nsurveys in this field focus on individual methods, lacking a comprehensive\noverview of the entire generative process. This paper addresses this gap by\nproviding an in-depth survey of human motion video generation, encompassing\nover ten sub-tasks, and detailing the five key phases of the generation\nprocess: input, motion planning, motion video generation, refinement, and\noutput. Notably, this is the first survey that discusses the potential of large\nlanguage models in enhancing human motion video generation. Our survey reviews\nthe latest developments and technological trends in human motion video\ngeneration across three primary modalities: vision, text, and audio. By\ncovering over two hundred papers, we offer a thorough overview of the field and\nhighlight milestone works that have driven significant technological\nbreakthroughs. Our goal for this survey is to unveil the prospects of human\nmotion video generation and serve as a valuable resource for advancing the\ncomprehensive applications of digital humans. A complete list of the models\nexamined in this survey is available in Our Repository\nhttps://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u586b\u8865\u4e86\u4eba\u7c7b\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u7a7a\u767d\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u4e94\u4e2a\u5173\u952e\u9636\u6bb5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u6db5\u76d6\u4e86\u4e09\u79cd\u4e3b\u8981\u6a21\u6001\u548c\u8d85\u8fc7200\u7bc7\u8bba\u6587\u7684\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u7efc\u8ff0\u4ec5\u5173\u6ce8\u4e2a\u522b\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\u7684\u5168\u9762\u6982\u8ff0\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u6df1\u5165\u7684\u7efc\u8ff0\uff0c\u6db5\u76d6\u591a\u4e2a\u5b50\u4efb\u52a1\u548c\u5173\u952e\u6280\u672f\u9636\u6bb5\u3002", "method": "\u8bba\u6587\u901a\u8fc7\u5206\u6790\u4eba\u7c7b\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u7684\u4e94\u4e2a\u9636\u6bb5\uff08\u8f93\u5165\u3001\u8fd0\u52a8\u89c4\u5212\u3001\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u3001\u7ec6\u5316\u548c\u8f93\u51fa\uff09\uff0c\u5e76\u56de\u987e\u4e86\u89c6\u89c9\u3001\u6587\u672c\u548c\u97f3\u9891\u4e09\u79cd\u6a21\u6001\u4e0b\u7684\u6700\u65b0\u7814\u7a76\u548c\u6280\u672f\u8d8b\u52bf\u3002", "result": "\u63d0\u4f9b\u4e86\u6db5\u76d6200\u591a\u7bc7\u8bba\u6587\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u6307\u51fa\u4e86\u91cc\u7a0b\u7891\u5f0f\u7684\u6280\u672f\u7a81\u7834\uff0c\u5e76\u8ba8\u8bba\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u5e94\u7528\u3002", "conclusion": "\u8fd9\u7bc7\u7efc\u8ff0\u63ed\u793a\u4e86\u4eba\u7c7b\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u6570\u5b57\u4eba\u7c7b\u7684\u7efc\u5408\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002"}}
{"id": "2509.03812", "pdf": "https://arxiv.org/pdf/2509.03812", "abs": "https://arxiv.org/abs/2509.03812", "authors": ["Shadeeb Hossain", "Natalie Sommer", "Neda Adib"], "title": "Exploring the Integration of Extended Reality and Artificial Intelligence (AI) for Remote STEM Education and Assessment", "categories": ["cs.HC"], "comment": "9 pages, 5 figures, 1 table", "summary": "This paper presents a dynamic gamification architecture for an Extended\nReality Artificial Intelligence virtual training environment designed to\nenhance STEM education through immersive adaptive, and kinesthetic learning.\nThe proposed system can be introduced in four phases: Introduction Phase,\nComponent Development Phase, Fault Introduction and Correction Phase and\nGenerative AI XR scenarios Phase. Security and privacy are discussed via a\ndefense-in-depth approach spanning client, middleware, and backend layers,\nincorporating AES 256 encryption, multi-factor authentication, role-based\naccess control and GDPR or FERPA compliance. Risks such as sensor exploitation,\nperceptual manipulation, and virtual physical harm are identified, with\nmitigation strategies embedded at the design stage. Potential barriers to large\nscale adoption-including technical complexity, cost of deployment, and need for\ncybersecurity expertise are discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6e38\u620f\u5316\u67b6\u6784\uff0c\u7528\u4e8e\u6269\u5c55\u73b0\u5b9e\u865a\u62df\u57f9\u8bad\u73af\u5883\uff0c\u63d0\u5347STEM\u6559\u80b2\u3002", "motivation": "\u901a\u8fc7\u6c89\u6d78\u5f0f\u548c\u9002\u5e94\u6027\u5b66\u4e60\u589e\u5f3aSTEM\u6559\u80b2\u3002", "method": "\u56db\u9636\u6bb5\u7cfb\u7edf\u6784\u5efa\uff0c\u6db5\u76d6\u5b89\u5168\u9690\u79c1\u63aa\u65bd\u548c\u98ce\u9669\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u8bbe\u8ba1\u652f\u6301\u6c89\u6d78\u5f0f\u5b66\u4e60\uff0c\u4f46\u9762\u4e34\u6280\u672f\u548c\u6210\u672c\u6311\u6218\u3002", "conclusion": "\u9700\u89e3\u51b3\u6280\u672f\u590d\u6742\u6027\u548c\u6210\u672c\u95ee\u9898\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u5e94\u7528\u3002"}}
{"id": "2509.04070", "pdf": "https://arxiv.org/pdf/2509.04070", "abs": "https://arxiv.org/abs/2509.04070", "authors": ["Paresh Baidya", "Rourab Paul", "Vikas Srivastava", "Sumit Kumar Debnath"], "title": "Error Detection Schemes for Barrett Reduction of CT-BU on FPGA in Post Quantum Cryptography", "categories": ["cs.CR", "cs.AR"], "comment": null, "summary": "A fault can occur naturally or intentionally. However, intentionally\ninjecting faults into hardware accelerators of Post-Quantum Cryptographic (PQC)\nalgorithms may leak sensitive information. This intentional fault injection in\nside-channel attacks compromises the reliability of PQC implementations. The\nrecently NIST-standardized key encapsulation mechanism (KEM), Kyber may also\nleak information at the hardware implementation level. This work proposes three\nefficient and lightweight recomputation-based fault detection methods for\nBarrett Reduction in the Cooley-Tukey Butterfly Unit (CT-BU) of Kyber on a\nField Programmable Gate Array (FPGA). The CT-BU and Barrett Reduction are\nfundamental components in structured lattice-based PQC algorithms, including\nKyber, NTRU, Falcon, CRYSTALS-Dilithium, etc. This paper introduces a new\nalgorithm, Recomputation with Swapped Operand (RESWO), for fault detection.\nWhile Recomputation with Negated Operand (RENO) and Recomputation with Shifted\nOperand (RESO) are existing methods used in other PQC hardware algorithms. To\nthe best of our knowledge, RENO and RESO have never been used in Barrett\nReduction before. The proposed RESWO method consumes a similar number of slices\ncompared to RENO and RESO. However, RESWO shows lesser delay compared to both\nRENO and RESO. The fault detection efficiency of RESWO, RENO, and RESO is\nnearly 100%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u57fa\u4e8e\u91cd\u65b0\u8ba1\u7b97\u7684\u8f7b\u91cf\u7ea7\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8eKyber\u7684CT-BU\u4e2d\u7684Barrett Reduction\uff0c\u5e76\u4ee5RESWO\u7b97\u6cd5\u4e3a\u521b\u65b0\u70b9\u3002", "motivation": "\u7531\u4e8e\u540e\u91cf\u5b50\u5bc6\u7801\uff08PQC\uff09\u7b97\u6cd5\u7684\u786c\u4ef6\u52a0\u901f\u5668\u53ef\u80fd\u56e0\u6545\u969c\u6ce8\u5165\u800c\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5728NIST\u6807\u51c6\u5316\u7684Kyber\u7b97\u6cd5\u4e2d\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u786e\u4fdd\u5176\u53ef\u9760\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff08RESWO\u3001RENO\u3001RESO\uff09\uff0c\u5176\u4e2dRESWO\u662f\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4bBarrett Reduction\u4e2d\u7684\u6545\u969c\uff0c\u5e76\u5728FPGA\u4e0a\u5b9e\u73b0\u3002", "result": "RESWO\u5728\u5ef6\u8fdf\u6027\u80fd\u4e0a\u4f18\u4e8eRENO\u548cRESO\uff0c\u4e09\u79cd\u65b9\u6cd5\u7684\u6545\u969c\u68c0\u6d4b\u6548\u7387\u5747\u63a5\u8fd1100%\u3002", "conclusion": "RESWO\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8ePQC\u7b97\u6cd5\u7684\u786c\u4ef6\u5b9e\u73b0\uff0c\u5c55\u73b0\u4e86\u5728Barrett Reduction\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.04058", "pdf": "https://arxiv.org/pdf/2509.04058", "abs": "https://arxiv.org/abs/2509.04058", "authors": ["Lei Zhong", "Yi Yang", "Changjian Li"], "title": "SMooGPT: Stylized Motion Generation using Large Language Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u548cLLM\u7684\u65b0\u65b9\u6cd5SMooGPT\uff0c\u901a\u8fc7\u8eab\u4f53\u90e8\u5206\u6587\u672c\u7a7a\u95f4\u751f\u6210\u98ce\u683c\u5316\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u3001\u63a7\u5236\u548c\u6cdb\u5316\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u98ce\u683c\u5316\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u3001\u63a7\u5236\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4e14\u5bf9\u65b0\u98ce\u683c\u7684\u9002\u5e94\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u63a8\u7406-\u7ec4\u5408-\u751f\u6210\u7684\u65b0\u89c6\u89d2\uff0c\u5229\u7528LLM\u5728\u8eab\u4f53\u90e8\u5206\u6587\u672c\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8fd0\u52a8\u751f\u6210\uff0c\u63d0\u51faSMooGPT\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u63a7\u5236\u80fd\u529b\uff0c\u5e76\u80fd\u6709\u6548\u6cdb\u5316\u5230\u65b0\u98ce\u683c\u3002", "conclusion": "SMooGPT\u5728\u7eaf\u6587\u672c\u9a71\u52a8\u7684\u98ce\u683c\u5316\u8fd0\u52a8\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.04084", "pdf": "https://arxiv.org/pdf/2509.04084", "abs": "https://arxiv.org/abs/2509.04084", "authors": ["Chenxuan Yao", "Yuchong Hu", "Feifan Liu", "Zhengyu Liu", "Dan Feng"], "title": "LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems", "categories": ["cs.DC"], "comment": null, "summary": "Distributed training of large deep-learning models often leads to failures,\nso checkpointing is commonly employed for recovery. State-of-the-art studies\nfocus on frequent checkpointing for fast recovery from failures. However, it\ngenerates numerous checkpoints, incurring substantial costs and thus degrading\ntraining performance. Recently, differential checkpointing has been proposed to\nreduce costs, but it is limited to recommendation systems, so its application\nto general distributed training systems remains unexplored.\n  This paper proposes LowDiff, an efficient frequent checkpointing framework\nthat \\textit{reuses} compressed gradients, serving as differential checkpoints\nto reduce cost. Furthermore, LowDiff incorporates a batched gradient write\noptimization to persist these differentials to storage efficiently. It also\ndynamically tunes both the checkpoint frequency and the batching size to\nmaximize performance. We further enhance LowDiff with a layer-wise gradient\nreusing and snapshotting approach and a CPU-based asynchronous persistence\nstrategy, enabling frequent checkpointing without gradient compression.\nExperiments on various workloads show that LowDiff can achieve checkpointing\nfrequency up to per iteration with less than 3.1\\% runtime overhead.", "AI": {"tldr": "LowDiff\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u9891\u7e41\u68c0\u67e5\u70b9\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u538b\u7f29\u68af\u5ea6\u4f5c\u4e3a\u5dee\u5f02\u68c0\u67e5\u70b9\u6765\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u4f18\u5316\u5b58\u50a8\u5199\u5165\uff0c\u52a8\u6001\u8c03\u6574\u68c0\u67e5\u70b9\u9891\u7387\u548c\u6279\u91cf\u5927\u5c0f\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5206\u5e03\u5f0f\u8bad\u7ec3\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5e38\u56e0\u5931\u8d25\u800c\u4e2d\u65ad\uff0c\u73b0\u6709\u7684\u9891\u7e41\u68c0\u67e5\u70b9\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6027\u80fd\u4e0b\u964d\uff0c\u5dee\u5206\u68c0\u67e5\u70b9\u65b9\u6cd5\u53c8\u9650\u4e8e\u63a8\u8350\u7cfb\u7edf\u3002", "method": "LowDiff\u91cd\u7528\u538b\u7f29\u68af\u5ea6\u4f5c\u4e3a\u5dee\u5f02\u5316\u68c0\u67e5\u70b9\uff0c\u91c7\u7528\u6279\u91cf\u68af\u5ea6\u5199\u5165\u4f18\u5316\u5b58\u50a8\uff0c\u52a8\u6001\u8c03\u6574\u68c0\u67e5\u70b9\u9891\u7387\u548c\u6279\u91cf\u5927\u5c0f\uff0c\u5e76\u5f15\u5165\u5c42\u7ea7\u68af\u5ea6\u91cd\u7528\u548cCPU\u5f02\u6b65\u6301\u4e45\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLowDiff\u80fd\u4ee5\u6bcf\u8fed\u4ee3\u4e00\u6b21\u7684\u9891\u7387\u8fdb\u884c\u68c0\u67e5\u70b9\uff0c\u8fd0\u884c\u65f6\u5f00\u9500\u4f4e\u4e8e3.1%\u3002", "conclusion": "LowDiff\u901a\u8fc7\u591a\u79cd\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6548\u9891\u7e41\u68c0\u67e5\u70b9\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u5e76\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03935", "pdf": "https://arxiv.org/pdf/2509.03935", "abs": "https://arxiv.org/abs/2509.03935", "authors": ["Sungho Cho", "Sung Il Choi", "Seung Hyun Oh", "Ian P. Roberts", "Sang Hyun Lee"], "title": "Autonomous Task Offloading of Vehicular Edge Computing with Parallel Computation Queues", "categories": ["cs.NI"], "comment": null, "summary": "This work considers a parallel task execution strategy in vehicular edge\ncomputing (VEC) networks, where edge servers are deployed along the roadside to\nprocess offloaded computational tasks of vehicular users. To minimize the\noverall waiting delay among vehicular users, a novel task offloading solution\nis implemented based on the network cooperation balancing resource\nunder-utilization and load congestion. Dual evaluation through theoretical and\nnumerical ways shows that the developed solution achieves a globally optimal\ndelay reduction performance compared to existing methods, which is also\napproved by the feasibility test over a real-map virtual environment. The\nin-depth analysis reveals that predicting the instantaneous processing power of\nedge servers facilitates the identification of overloaded servers, which is\ncritical for determining network delay. By considering discrete variables of\nthe queue, the proposed technique's precise estimation can effectively address\nthese combinatorial challenges to achieve optimal performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f66\u8054\u7f51\u8fb9\u7f18\u8ba1\u7b97\uff08VEC\uff09\u4e2d\u7684\u5e76\u884c\u4efb\u52a1\u6267\u884c\u7b56\u7565\uff0c\u901a\u8fc7\u4efb\u52a1\u5378\u8f7d\u4f18\u5316\u65b9\u6848\u51cf\u5c11\u7528\u6237\u7b49\u5f85\u5ef6\u8fdf\uff0c\u7406\u8bba\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5168\u5c40\u6700\u4f18\u6027\u3002", "motivation": "\u8f66\u8054\u7f51\u8fb9\u7f18\u8ba1\u7b97\u4e2d\uff0c\u4efb\u52a1\u5378\u8f7d\u7684\u8d44\u6e90\u5229\u7528\u4e0d\u5747\u548c\u8d1f\u8f7d\u62e5\u5835\u95ee\u9898\u4e9f\u9700\u89e3\u51b3\uff0c\u4ee5\u6700\u5c0f\u5316\u7528\u6237\u7684\u7b49\u5f85\u5ef6\u8fdf\u3002", "method": "\u57fa\u4e8e\u7f51\u7edc\u534f\u4f5c\u7684\u4efb\u52a1\u5378\u8f7d\u65b9\u6848\uff0c\u901a\u8fc7\u9884\u6d4b\u670d\u52a1\u5668\u77ac\u65f6\u5904\u7406\u80fd\u529b\u8bc6\u522b\u8fc7\u8f7d\u670d\u52a1\u5668\uff0c\u5e76\u7ed3\u5408\u961f\u5217\u79bb\u6563\u53d8\u91cf\u8fdb\u884c\u7cbe\u786e\u4f30\u8ba1\u3002", "result": "\u7406\u8bba\u4e0e\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u65b9\u6848\u5728\u5ef6\u8fdf\u51cf\u5c11\u4e0a\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\uff0c\u901a\u8fc7\u771f\u5b9e\u5730\u56fe\u4eff\u771f\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u9884\u6d4b\u670d\u52a1\u5668\u5904\u7406\u80fd\u529b\u548c\u7cbe\u786e\u961f\u5217\u4f30\u8ba1\u662f\u4f18\u5316\u5ef6\u8fdf\u7684\u5173\u952e\uff0c\u65b0\u65b9\u6848\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2509.04078", "pdf": "https://arxiv.org/pdf/2509.04078", "abs": "https://arxiv.org/abs/2509.04078", "authors": ["Jingjing Liu", "Zeming Liu", "Zihao Cheng", "Mengliang He", "Xiaoming Shi", "Yuhang Guo", "Xiangrong Zhu", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": "30 pages, 12 figures, EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging.", "AI": {"tldr": "RepoDebug\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u3001\u591a\u8bed\u8a00\u3001\u591a\u9519\u8bef\u7c7b\u578b\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8c03\u8bd5\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u8c03\u8bd5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u7ea7\u4ee3\u7801\u4fee\u590d\uff0c\u5ffd\u7565\u4e86\u66f4\u590d\u6742\u3001\u66f4\u771f\u5b9e\u7684\u4ed3\u5e93\u7ea7\u573a\u666f\uff0c\u5bfc\u81f4\u5bf9LLMs\u5728\u4ed3\u5e93\u7ea7\u8c03\u8bd5\u4e2d\u7684\u6311\u6218\u8ba4\u8bc6\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86RepoDebug\u6570\u636e\u96c6\uff0c\u652f\u63018\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c3\u79cd\u8c03\u8bd5\u4efb\u52a1\uff0c\u5305\u542b22\u79cd\u5b50\u7c7b\u578b\u9519\u8bef\u3002", "result": "\u5bf910\u79cdLLMs\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684Claude 3.5 Sonnect\u5728\u4ed3\u5e93\u7ea7\u8c03\u8bd5\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8c03\u8bd5\u5bf9LLMs\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u5347\u5176\u80fd\u529b\u3002"}}
{"id": "2509.04086", "pdf": "https://arxiv.org/pdf/2509.04086", "abs": "https://arxiv.org/abs/2509.04086", "authors": ["Yaru Chen", "Faegheh Sardari", "Peiliang Zhang", "Ruohao Guo", "Yang Xiang", "Zhenbo Li", "Wenwu Wang"], "title": "TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Audio-Visual Video Parsing (AVVP) task aims to identify event categories and\ntheir occurrence times in a given video with weakly supervised labels. Existing\nmethods typically fall into two categories: (i) designing enhanced\narchitectures based on attention mechanism for better temporal modeling, and\n(ii) generating richer pseudo-labels to compensate for the absence of\nframe-level annotations. However, the first type methods treat noisy\nsegment-level pseudo labels as reliable supervision and the second type methods\nlet indiscriminate attention spread them across all frames, the initial errors\nare repeatedly amplified during training. To address this issue, we propose a\nmethod that combines the Bi-Directional Text Fusion (BiT) module and\nCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate the\nstrengths and complementarity of the two previous research directions. We first\nperform semantic injection and dynamic calibration on audio and visual modality\nfeatures through the BiT module, to locate and purify cleaner and richer\nsemantic cues. Then, we leverage the CATS module for semantic propagation and\nconnection to enable precise semantic information dissemination across time.\nExperimental results demonstrate that our proposed method achieves\nstate-of-the-art (SOTA) performance in multiple key indicators on two benchmark\ndatasets, LLP and UnAV-100.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u5411\u6587\u672c\u878d\u5408\u6a21\u5757\u548c\u7c7b\u522b\u611f\u77e5\u65f6\u95f4\u56fe\u6a21\u5757\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u5f31\u76d1\u7763\u4e0b\u7684\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u4efb\u52a1\uff0c\u901a\u8fc7\u8bed\u4e49\u6ce8\u5165\u548c\u52a8\u6001\u6821\u51c6\u51cf\u5c11\u566a\u58f0\uff0c\u5b9e\u73b0\u4e86\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u4efb\u52a1\u4e2d\uff0c\u5b58\u5728\u566a\u58f0\u4f2a\u6807\u7b7e\u88ab\u8bef\u8ba4\u4e3a\u53ef\u9760\u76d1\u7763\u6216\u6ce8\u610f\u529b\u673a\u5236\u65e0\u5dee\u522b\u4f20\u64ad\u8bef\u5dee\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e2d\u9519\u8bef\u88ab\u653e\u5927\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cc\u5411\u6587\u672c\u878d\u5408\u6a21\u5757\uff08BiT\uff09\u548c\u7c7b\u522b\u611f\u77e5\u65f6\u95f4\u56fe\u6a21\u5757\uff08CATS\uff09\u7684\u65b9\u6cd5\uff0cBiT\u6a21\u5757\u7528\u4e8e\u8bed\u4e49\u6ce8\u5165\u548c\u52a8\u6001\u6821\u51c6\uff0cCATS\u6a21\u5757\u7528\u4e8e\u8bed\u4e49\u4f20\u64ad\u548c\u8fde\u63a5\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u8bed\u4e49\u4fe1\u606f\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff08LLP\u548cUnAV-100\uff09\u7684\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408BiT\u548cCATS\u6a21\u5757\u7684\u4f18\u52bf\uff0c\u672c\u6587\u6210\u529f\u89e3\u51b3\u4e86\u566a\u58f0\u4f2a\u6807\u7b7e\u548c\u6ce8\u610f\u529b\u4f20\u64ad\u5e26\u6765\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u97f3\u9891-\u89c6\u89c9\u89c6\u9891\u89e3\u6790\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2509.03931", "pdf": "https://arxiv.org/pdf/2509.03931", "abs": "https://arxiv.org/abs/2509.03931", "authors": ["Gautam Khannaa", "Yeliz Yesilada", "Sukru Eraslan", "Simon Harper"], "title": "\"Low Frequency Tweeters Have More to Say!\" A New Approach to Identify Importance of Tweets", "categories": ["cs.HC", "cs.SI"], "comment": "12 pages", "summary": "Twitter is one of the most popular social media platforms.With a large number\nof tweets, the activity feed of users becomes noisy, challenging to read, and\nmost importantly tweets often get lost. We present a new approach to\npersonalise the ranking of the tweets toward solving the problem of information\noverload which is achieved by analysing the relationship between the importance\nof tweets to the frequency at which the author tweets. The hypothesis tested is\nthat \"low-frequency tweeters have more to say\", i.e. if a user who tweets\ninfrequently actually goes to the effort of tweeting, then it is more likely to\nbe of more importance or contain more \"meaning\" than a tweet by a user who\ntweets continuously. We propose six new measures to evaluate the importance of\ntweets based on the ability of the tweet to drive interaction among its\nreaders, which is measured through metrics such as retweets, favourites, and\ncomments, and the extent of the author's network interacting with the tweet.\nOur study shows that users who tweeted less than ten tweets per week were more\nlikely to be perceived as important by their followers and have the most\nimportant messages. This identified tweet-frequency band could be used to\nreorder the activity feed of users and such reordering would ensure the\nmessages of low-frequency tweeters do not get lost in the stream of tweets.\nThis could also serve as a scoring index for Twitter users to identify users\nfrequently tweeting important messages.", "AI": {"tldr": "\u9488\u5bf9Twitter\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u7528\u6237\u63a8\u6587\u9891\u7387\u7684\u4e2a\u6027\u5316\u6392\u5e8f\u65b9\u6cd5\uff0c\u53d1\u73b0\u4f4e\u9891\u63a8\u6587\u7528\u6237\uff08\u6bcf\u5468\u5c11\u4e8e10\u6761\uff09\u7684\u63a8\u6587\u66f4\u53d7\u5173\u6ce8\u3002", "motivation": "\u89e3\u51b3Twitter\u7528\u6237\u6d3b\u52a8\u6d41\u4e2d\u7684\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u63a8\u6587\u9891\u7387\u4e0e\u91cd\u8981\u6027\u7684\u5173\u7cfb\uff0c\u786e\u4fdd\u4f4e\u9891\u7528\u6237\u7684\u63a8\u6587\u4e0d\u88ab\u6df9\u6ca1\u3002", "method": "\u63d0\u51fa\u516d\u4e2a\u65b0\u6307\u6807\uff0c\u57fa\u4e8e\u63a8\u6587\u4e92\u52a8\uff08\u5982\u8f6c\u53d1\u3001\u70b9\u8d5e\u3001\u8bc4\u8bba\uff09\u548c\u4f5c\u8005\u7f51\u7edc\u4e92\u52a8\uff0c\u8bc4\u4f30\u63a8\u6587\u91cd\u8981\u6027\u3002\u5047\u8bbe\u4f4e\u9891\u63a8\u6587\u7528\u6237\u7684\u5185\u5bb9\u66f4\u6709\u4ef7\u503c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6bcf\u5468\u63a8\u6587\u5c11\u4e8e10\u6761\u7684\u7528\u6237\u66f4\u53ef\u80fd\u88ab\u5176\u5173\u6ce8\u8005\u89c6\u4e3a\u91cd\u8981\uff0c\u5176\u63a8\u6587\u66f4\u5177\u4ef7\u503c\u3002", "conclusion": "\u901a\u8fc7\u63a8\u6587\u9891\u7387\u5206\u5e26\u91cd\u65b0\u6392\u5e8f\u6d3b\u52a8\u6d41\uff0c\u53ef\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u5e76\u4e3a\u8bc6\u522b\u91cd\u8981\u63a8\u6587\u7528\u6237\u63d0\u4f9b\u8bc4\u5206\u4f9d\u636e\u3002"}}
{"id": "2509.04145", "pdf": "https://arxiv.org/pdf/2509.04145", "abs": "https://arxiv.org/abs/2509.04145", "authors": ["Dongliang Cao", "Guoxing Sun", "Marc Habermann", "Florian Bernard"], "title": "Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7279\u5b9a\u4eba\u7269\u6e32\u67d3\u548c\u6269\u6563\u751f\u6210\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u771f\u5b9e\u5ea6\u548c\u903c\u771f\u59ff\u6001\u4f9d\u8d56\u53d8\u5f62\u7684\u52a8\u6001\u4eba\u7c7b\u5934\u50cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u4eba\u7c7b\u5934\u50cf\u65f6\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u5355\u4e00\u8eab\u4efd\u7684\u7279\u5b9a\u6e32\u67d3\u6a21\u578b\uff0c\u8981\u4e48\u751f\u6210\u7684\u9759\u6001\u5361\u901a\u5934\u50cf\u8d28\u91cf\u8f83\u4f4e\u4e14\u65e0\u6cd5\u6355\u6349\u59ff\u6001\u4f9d\u8d56\u7684\u53d8\u5f62\uff08\u5982\u8863\u7269\u8936\u76b1\uff09\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u4f18\u5316\u4e00\u7ec4\u7279\u5b9a\u4eba\u7269\u7684UNet\u7f51\u7edc\u4ee5\u6355\u6349\u59ff\u6001\u4f9d\u8d56\u53d8\u5f62\uff1b2) \u8bad\u7ec3\u4e00\u4e2a\u8d85\u6269\u6563\u6a21\u578b\u5728\u6743\u91cd\u7a7a\u95f4\u751f\u6210\u7f51\u7edc\u6743\u91cd\uff0c\u5b9e\u73b0\u5b9e\u65f6\u53ef\u63a7\u7684\u52a8\u6001\u5934\u50cf\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u8de8\u8eab\u4efd\u591a\u89c6\u89d2\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u7279\u5b9a\u4eba\u7269\u6e32\u67d3\u7684\u9ad8\u771f\u5b9e\u5ea6\u548c\u6269\u6563\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u52a8\u6001\u4eba\u7c7b\u5934\u50cf\u7684\u751f\u6210\u3002"}}
{"id": "2509.04219", "pdf": "https://arxiv.org/pdf/2509.04219", "abs": "https://arxiv.org/abs/2509.04219", "authors": ["Leandro M\u00e1rcio Bertholdo", "Renan Barreto Paredes", "Gabriela de Lima Marin", "Cesar A. H. Loureiro", "Milton Kaoru Kashiwakura Pedro de Botelho Marcos"], "title": "Analyzing the Effect of an Extreme Weather Event on Telecommunications and Information Technology: Insights from 30 Days of Flooding", "categories": ["cs.NI"], "comment": "32 pages, 15 figures. To appear in Passive and Active Measurement\n  Conference (PAM) 2025, published in Lecture Notes in Computer Science (LNCS),\n  Springer. The final authenticated version is available at\n  https://doi.org/10.1007/978-3-031-85960-1_12", "summary": "In May 2024, weeks of severe rainfall in Rio Grande do Sul, Brazil caused\nwidespread damage to infrastructure, impacting over 400 cities and 2.3 million\npeople. This study presents the construction of comprehensive\ntelecommunications datasets during this climatic event, encompassing Internet\nmeasurements, fiber cut reports, and Internet Exchange routing data. By\ncorrelating network disruptions with hydrological and operational factors, the\ndataset offers insights into the resilience of fiber networks, data centers,\nand Internet traffic during critical events. For each scenario, we investigate\nfailures related to the Information and Communication Technology infrastructure\nand highlight the challenges faced when its resilience is critically tested.\nPreliminary findings reveal trends in connectivity restoration, infrastructure\nvulnerabilities, and user behavior changes. These datasets and pre-analysis aim\nto support future research on disaster recovery strategies and the development\nof robust telecommunications systems.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5df4\u897f\u6d2a\u707e\u5bf9\u7535\u4fe1\u7f51\u7edc\u7684\u5f71\u54cd\uff0c\u6784\u5efa\u4e86\u5305\u542b\u4e92\u8054\u7f51\u6d4b\u91cf\u3001\u5149\u7ea4\u5207\u65ad\u62a5\u544a\u7b49\u7684\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u57fa\u7840\u8bbe\u65bd\u8106\u5f31\u6027\u548c\u6062\u590d\u8d8b\u52bf\u3002", "motivation": "\u8bc4\u4f30\u6781\u7aef\u6c14\u5019\u4e8b\u4ef6\u4e2d\u7535\u4fe1\u57fa\u7840\u8bbe\u65bd\u7684\u97e7\u6027\uff0c\u4e3a\u672a\u6765\u707e\u96be\u6062\u590d\u7b56\u7565\u548c\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u4e92\u8054\u7f51\u6d4b\u91cf\u3001\u5149\u7ea4\u5207\u65ad\u62a5\u544a\u548c\u8def\u7531\u6570\u636e\uff0c\u7ed3\u5408\u6c34\u6587\u4e0e\u8fd0\u8425\u56e0\u7d20\u5206\u6790\u7f51\u7edc\u4e2d\u65ad\u3002", "result": "\u521d\u6b65\u53d1\u73b0\u5305\u62ec\u8fde\u63a5\u6062\u590d\u8d8b\u52bf\u3001\u57fa\u7840\u8bbe\u65bd\u8106\u5f31\u6027\u4ee5\u53ca\u7528\u6237\u884c\u4e3a\u53d8\u5316\u3002", "conclusion": "\u6570\u636e\u96c6\u548c\u5206\u6790\u4e3a\u672a\u6765\u707e\u96be\u6062\u590d\u548c\u7535\u4fe1\u7cfb\u7edf\u97e7\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.04260", "pdf": "https://arxiv.org/pdf/2509.04260", "abs": "https://arxiv.org/abs/2509.04260", "authors": ["Haowei Quan", "Junjie Wang", "Xinzhe Li", "Terry Yue Zhuo", "Xiao Chen", "Xiaoning Du"], "title": "An Empirical Study of Vulnerabilities in Python Packages and Their Detection", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": null, "summary": "In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field.", "AI": {"tldr": "PyVul\u662f\u4e00\u4e2a\u5168\u9762\u7684Python\u5305\u6f0f\u6d1e\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u542b1,157\u4e2a\u516c\u5f00\u62a5\u544a\u7684\u6f0f\u6d1e\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u6807\u6ce8\u3002\u5206\u6790\u663e\u793a\uff0c\u591a\u8bed\u8a00Python\u5305\u66f4\u5bb9\u6613\u53d7\u6f0f\u6d1e\u5f71\u54cd\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u7684\u80fd\u529b\u4e0e\u5b9e\u9645\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "Python\u5305\u7684\u5b89\u5168\u6027\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6f0f\u6d1e\u68c0\u6d4b\u5de5\u5177\u7684\u6548\u80fd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u5f15\u5165PyVul\u57fa\u51c6\u5957\u4ef6\uff0c\u91c7\u7528LLM\u8f85\u52a9\u7684\u6570\u636e\u6e05\u6d17\u65b9\u6cd5\uff0c\u63d0\u4f9b\u63d0\u4ea4\u548c\u51fd\u6570\u7ea7\u522b\u7684\u6807\u6ce8\uff0c\u5e76\u5206\u6790\u6f0f\u6d1e\u5206\u5e03\u3002", "result": "PyVul\u5728\u63d0\u4ea4\u7ea7\u522b\u6807\u6ce8\u7cbe\u5ea6\u8fbe100%\uff0c\u51fd\u6570\u7ea7\u522b94%\u3002\u591a\u8bed\u8a00Python\u5305\u66f4\u6613\u53d7\u6f0f\u6d1e\u5f71\u54cd\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "PyVul\u4e3aPython\u5305\u6f0f\u6d1e\u7814\u7a76\u63d0\u4f9b\u4e86\u7cbe\u786e\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u672a\u6765\u6539\u8fdb\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.04215", "pdf": "https://arxiv.org/pdf/2509.04215", "abs": "https://arxiv.org/abs/2509.04215", "authors": ["Hayeon Bang", "Eunjin Choi", "Seungheon Doh", "Juhan Nam"], "title": "PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music", "categories": ["cs.SD", "cs.IR", "cs.MM"], "comment": "Accepted for publication at the 26th International Society for Music\n  Information Retrieval Conference (ISMIR 2025)", "summary": "Solo piano music, despite being a single-instrument medium, possesses\nsignificant expressive capabilities, conveying rich semantic information across\ngenres, moods, and styles. However, current general-purpose music\nrepresentation models, predominantly trained on large-scale datasets, often\nstruggle to captures subtle semantic distinctions within homogeneous solo piano\nmusic. Furthermore, existing piano-specific representation models are typically\nunimodal, failing to capture the inherently multimodal nature of piano music,\nexpressed through audio, symbolic, and textual modalities. To address these\nlimitations, we propose PianoBind, a piano-specific multimodal joint embedding\nmodel. We systematically investigate strategies for multi-source training and\nmodality utilization within a joint embedding framework optimized for capturing\nfine-grained semantic distinctions in (1) small-scale and (2) homogeneous piano\ndatasets. Our experimental results demonstrate that PianoBind learns multimodal\nrepresentations that effectively capture subtle nuances of piano music,\nachieving superior text-to-music retrieval performance on in-domain and\nout-of-domain piano datasets compared to general-purpose music joint embedding\nmodels. Moreover, our design choices offer reusable insights for multimodal\nrepresentation learning with homogeneous datasets beyond piano music.", "AI": {"tldr": "PianoBind\u662f\u4e00\u79cd\u9488\u5bf9\u94a2\u7434\u97f3\u4e50\u7684\u591a\u6a21\u6001\u8054\u5408\u5d4c\u5165\u6a21\u578b\uff0c\u65e8\u5728\u6355\u6349\u5c0f\u578b\u548c\u540c\u8d28\u6570\u636e\u96c6\u4e2d\u7684\u7ec6\u5fae\u8bed\u4e49\u533a\u522b\uff0c\u4f18\u4e8e\u901a\u7528\u97f3\u4e50\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u94a2\u7434\u97f3\u4e50\u5177\u6709\u4e30\u5bcc\u7684\u8868\u73b0\u529b\uff0c\u4f46\u73b0\u6709\u901a\u7528\u6a21\u578b\u96be\u4ee5\u6355\u6349\u5176\u7ec6\u5fae\u8bed\u4e49\u533a\u522b\uff0c\u4e14\u94a2\u7434\u7279\u5b9a\u6a21\u578b\u591a\u4e3a\u5355\u6a21\u6001\u3002", "method": "\u63d0\u51faPianoBind\uff0c\u901a\u8fc7\u591a\u6e90\u8bad\u7ec3\u548c\u6a21\u6001\u5229\u7528\u7b56\u7565\uff0c\u4f18\u5316\u8054\u5408\u5d4c\u5165\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPianoBind\u80fd\u6709\u6548\u6355\u6349\u94a2\u7434\u97f3\u4e50\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u5728\u6587\u672c\u5230\u97f3\u4e50\u7684\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "conclusion": "PianoBind\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u7684\u8bbe\u8ba1\u601d\u8def\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u540c\u8d28\u6570\u636e\u96c6\u3002"}}
{"id": "2509.04088", "pdf": "https://arxiv.org/pdf/2509.04088", "abs": "https://arxiv.org/abs/2509.04088", "authors": ["Farah Baracat", "Agnese Grison", "Dario Farina", "Giacomo Indiveri", "Elisa Donati"], "title": "Spiking Neural Network Decoders of Finger Forces from High-Density Intramuscular Microelectrode Arrays", "categories": ["cs.HC", "eess.SP"], "comment": null, "summary": "Restoring naturalistic finger control in assistive technologies requires the\ncontinuous decoding of motor intent with high accuracy, efficiency, and\nrobustness. Here, we present a spike-based decoding framework that integrates\nspiking neural networks (SNNs) with motor unit activity extracted from\nhigh-density intramuscular microelectrode arrays. We demonstrate simultaneous\nand proportional decoding of individual finger forces from motor unit spike\ntrains during isometric contractions at 15% of maximum voluntary contraction\nusing SNNs. We systematically evaluated alternative SNN decoder configurations\nand compared two possible input modalities: physiologically grounded motor unit\nspike trains and spike-encoded intramuscular EMG signals. Through this\ncomparison, we quantified trade-offs between decoding accuracy, memory\nfootprint, and robustness to input errors. The results showed that shallow SNNs\ncan reliably decode finger-level motor intent with competitive accuracy and\nminimal latency, while operating with reduced memory requirements and without\nthe need for external preprocessing buffers. This work provides a practical\nblueprint for integrating SNNs into finger-level force decoding systems,\ndemonstrating how the choice of input representation can be strategically\ntailored to meet application-specific requirements for accuracy, robustness,\nand memory efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\u7684\u89e3\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u6062\u590d\u8f85\u52a9\u6280\u672f\u4e2d\u7684\u81ea\u7136\u624b\u6307\u63a7\u5236\uff0c\u901a\u8fc7\u5206\u6790\u8fd0\u52a8\u5355\u5143\u6d3b\u52a8\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u89e3\u7801\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u5728\u8f85\u52a9\u6280\u672f\u4e2d\u5b9e\u73b0\u8fde\u7eed\u3001\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u624b\u6307\u8fd0\u52a8\u610f\u56fe\u89e3\u7801\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u5c16\u5cf0\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u4e0e\u9ad8\u5bc6\u5ea6\u808c\u5185\u5fae\u7535\u6781\u9635\u5217\u63d0\u53d6\u7684\u8fd0\u52a8\u5355\u5143\u6d3b\u52a8\u76f8\u7ed3\u5408\uff0c\u89e3\u7801\u7b49\u957f\u6536\u7f29\u65f6\u7684\u5355\u4e2a\u624b\u6307\u529b\u91cf\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6d45\u5c42SNN\u80fd\u591f\u4ee5\u8f83\u4f4e\u7684\u5185\u5b58\u9700\u6c42\u548c\u5ef6\u8fdf\u53ef\u9760\u89e3\u7801\u624b\u6307\u8fd0\u52a8\u610f\u56fe\uff0c\u4e14\u89e3\u7801\u7cbe\u5ea6\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5c06SNN\u96c6\u6210\u5230\u624b\u6307\u529b\u91cf\u89e3\u7801\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u8f93\u5165\u8868\u793a\u7684\u9009\u62e9\u53ef\u6ee1\u8db3\u7279\u5b9a\u5e94\u7528\u5bf9\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u5185\u5b58\u6548\u7387\u7684\u9700\u6c42\u3002"}}
{"id": "2509.04277", "pdf": "https://arxiv.org/pdf/2509.04277", "abs": "https://arxiv.org/abs/2509.04277", "authors": ["Przemyslaw Korzeniowski", "Niels Hald", "Fernando Bello"], "title": "Massively-Parallel Implementation of Inextensible Elastic Rods Using Inter-block GPU Synchronization", "categories": ["cs.GR", "cs.DC", "65D18", "I.3.5"], "comment": "12 pages, unpublished", "summary": "An elastic rod is a long and thin body able to sustain large global\ndeformations, even if local strains are small. The Cosserat rod is a non-linear\nelastic rod with an oriented centreline, which enables modelling of bending,\nstretching and twisting deformations. It can be used for physically-based\ncomputer simulation of threads, wires, ropes, as well as flexible surgical\ninstruments such as catheters, guidewires or sutures. We present a\nmassively-parallel implementation of the original CoRdE model as well as our\ninextensible variation. By superseding the CUDA Scalable Programming Model and\nusing inter-block synchronization, we managed to simulate multiple physics\ntime-steps per single kernel launch utilizing all the GPU's streaming\nmultiprocessors. Under some constraints, this results in nearly constant\ncomputation time, regardless of the number of Cosserat elements simulated. When\nexecuting 10 time-steps per single kernel launch, our implementation of the\noriginal, extensible CoRdE was x40.0 faster. In a number of tests, the GPU\nimplementation of our inextensible CoRdE modification achieved an average\nspeed-up of x15.11 over the corresponding CPU version. Simulating a\ncatheter/guidewire pair (2x512 Cosserat elements) in a cardiovascular\napplication resulted in a 13.5 fold performance boost, enabling for accurate\nreal-time simulation at haptic interactive rates (0.5-1kHz).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u7684\u5927\u89c4\u6a21\u5e76\u884c\u5f39\u6027\u6746\u6a21\u62df\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u7269\u7406\u6a21\u62df\u3002", "motivation": "\u5f39\u6027\u6746\uff08\u5982Cosserat\u6746\uff09\u5728\u7269\u7406\u6a21\u62df\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5e76\u884c\u5b9e\u73b0\u4ee5\u6ee1\u8db3\u6027\u80fd\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4f18\u5316CUDA\u53ef\u6269\u5c55\u7f16\u7a0b\u6a21\u578b\u5e76\u5229\u7528\u5757\u95f4\u540c\u6b65\uff0c\u5b9e\u73b0\u4e86\u591a\u7269\u7406\u65f6\u95f4\u6b65\u7684\u5355\u6b21\u5185\u6838\u542f\u52a8\uff0c\u5145\u5206\u5229\u7528GPU\u7684\u6d41\u5f0f\u591a\u5904\u7406\u5668\u3002", "result": "\u4f18\u5316\u540e\u7684\u5b9e\u73b0\u572810\u4e2a\u65f6\u95f4\u6b65\u7684\u5355\u6b21\u5185\u6838\u542f\u52a8\u4e2d\uff0c\u539fCoRdE\u6a21\u578b\u52a0\u901f40\u500d\uff1b\u4e0d\u53ef\u62c9\u4f38\u7248\u672c\u7684GPU\u5b9e\u73b0\u5e73\u5747\u52a0\u901f15.11\u500d\uff0c\u5fc3\u8840\u7ba1\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e8613.5\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86Cosserat\u6746\u6a21\u62df\u7684\u6548\u7387\uff0c\u652f\u6301\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u6a21\u62df\u3002"}}
{"id": "2509.04383", "pdf": "https://arxiv.org/pdf/2509.04383", "abs": "https://arxiv.org/abs/2509.04383", "authors": ["Serafino Cicerone", "Alessia Di Fonso", "Gabriele Di Stefano", "Alfredo Navarra"], "title": "On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs", "categories": ["cs.DC"], "comment": "18 pages, 6 figures", "summary": "The OBLOT model has been extensively studied in theoretical swarm robotics.\nIt assumes weak capabilities for the involved mobile robots, such as they are\nanonymous, disoriented, no memory of past events (oblivious), and silent. Their\nonly means of (implicit) communication is transferred to their positioning,\ni.e., stigmergic information. These limited capabilities make the design of\ndistributed algorithms a challenging task. Over the last two decades, numerous\nresearch papers have addressed the question of which tasks can be accomplished\nwithin this model. Nevertheless, as it usually happens in distributed\ncomputing, also in OBLOT the computational power available to the robots is\nneglected as the main cost measures for the designed algorithms refer to the\nnumber of movements or the number of rounds required. In this paper, we prove\nthat for synchronous robots moving on finite graphs, the unlimited\ncomputational power (other than finite time) has a significant impact. In fact,\nby exploiting it, we provide a definitive resolution algorithm that applies to\na wide class of problems while guaranteeing the minimum number of moves and\nrounds.", "AI": {"tldr": "OBLOT\u6a21\u578b\u4e2d\uff0c\u540c\u6b65\u673a\u5668\u4eba\u5728\u6709\u9650\u56fe\u4e0a\u5229\u7528\u65e0\u9650\u8ba1\u7b97\u80fd\u529b\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6700\u5c0f\u5316\u79fb\u52a8\u548c\u8f6e\u6b21\u7684\u89e3\u51b3\u7b97\u6cd5\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u4e2dOBLOT\u6a21\u578b\u7684\u8ba1\u7b97\u80fd\u529b\u672a\u88ab\u5145\u5206\u8003\u91cf\uff0c\u672c\u7814\u7a76\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528\u540c\u6b65\u673a\u5668\u4eba\u7684\u65e0\u9650\u8ba1\u7b97\u80fd\u529b\uff0c\u8bbe\u8ba1\u9002\u7528\u4e8e\u5e7f\u6cdb\u95ee\u9898\u7684\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u65e0\u9650\u8ba1\u7b97\u80fd\u529b\u5bf9\u7b97\u6cd5\u6548\u7387\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e2d\u65e0\u9650\u8ba1\u7b97\u80fd\u529b\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86OBLOT\u6a21\u578b\u4e2d\u4efb\u52a1\u7684\u89e3\u51b3\u6548\u7387\u3002"}}
{"id": "2509.04328", "pdf": "https://arxiv.org/pdf/2509.04328", "abs": "https://arxiv.org/abs/2509.04328", "authors": ["Amine Barrak", "Emna Ksontini", "Ridouane Atike", "Fehmi Jaafar"], "title": "FaaSGuard: Secure CI/CD for Serverless Applications -- An OpenFaaS Case Study", "categories": ["cs.SE"], "comment": "IEEE International Conference on Source Code Analysis & Manipulation\n  (SCAM 2025)", "summary": "Serverless computing significantly alters software development by abstracting\ninfrastructure management and enabling rapid, modular, event-driven\ndeployments. Despite its benefits, the distinct characteristics of serverless\nfunctions, such as ephemeral execution and fine-grained scalability, pose\nunique security challenges, particularly in open-source platforms like\nOpenFaaS. Existing approaches typically address isolated phases of the\nDevSecOps lifecycle, lacking an integrated and comprehensive security strategy.\nTo bridge this gap, we propose FaaSGuard, a unified DevSecOps pipeline\nexplicitly designed for open-source serverless environments. FaaSGuard\nsystematically embeds lightweight, fail-closed security checks into every stage\nof the development lifecycle-planning, coding, building, deployment, and\nmonitoring-effectively addressing threats such as injection attacks, hard-coded\nsecrets, and resource exhaustion. We validate our approach empirically through\na case study involving 20 real-world serverless functions from public GitHub\nrepositories. Results indicate that FaaSGuard effectively detects and prevents\ncritical vulnerabilities, demonstrating high precision (95%) and recall (91%)\nwithout significant disruption to established CI/CD practices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FaaSGuard\uff0c\u4e00\u4e2a\u9488\u5bf9\u5f00\u6e90\u65e0\u670d\u52a1\u5668\u73af\u5883\u7684\u7edf\u4e00DevSecOps\u7ba1\u9053\uff0c\u901a\u8fc7\u5728\u5f00\u53d1\u751f\u547d\u5468\u671f\u7684\u6bcf\u4e2a\u9636\u6bb5\u5d4c\u5165\u8f7b\u91cf\u7ea7\u5b89\u5168\u68c0\u6d4b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7684\u5b89\u5168\u6311\u6218\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7684\u72ec\u7279\u7279\u6027\uff08\u5982\u77ed\u6682\u6267\u884c\u548c\u7ec6\u7c92\u5ea6\u6269\u5c55\uff09\u5e26\u6765\u4e86\u72ec\u7279\u7684\u5b89\u5168\u6311\u6218\uff0c\u5c24\u5176\u662f\u5f00\u6e90\u5e73\u53f0\u5982OpenFaaS\u3002\u5f53\u524d\u7684\u5b89\u5168\u65b9\u6cd5\u7f3a\u4e4f\u7efc\u5408\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u96c6\u6210\u5316\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86FaaSGuard\uff0c\u4e00\u4e2a\u7edf\u4e00DevSecOps\u7ba1\u9053\uff0c\u5728\u89c4\u5212\u3001\u7f16\u7801\u3001\u6784\u5efa\u3001\u90e8\u7f72\u548c\u76d1\u63a7\u7684\u6bcf\u4e2a\u9636\u6bb5\u5d4c\u5165\u8f7b\u91cf\u7ea7\u3001\u6545\u969c\u5173\u95ed\u7684\u5b89\u5168\u68c0\u6d4b\u3002", "result": "\u901a\u8fc720\u4e2a\u771f\u5b9e\u65e0\u670d\u52a1\u5668\u529f\u80fd\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\uff0cFaaSGuard\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0895%\uff09\u548c\u53ec\u56de\u7387\uff0891%\uff09\uff0c\u4e14\u672a\u663e\u8457\u5e72\u6270\u73b0\u6709CI/CD\u5b9e\u8df5\u3002", "conclusion": "FaaSGuard\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b89\u5168\u7b56\u7565\uff0c\u80fd\u591f\u68c0\u6d4b\u548c\u9884\u9632\u5173\u952e\u6f0f\u6d1e\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u65e0\u670d\u52a1\u5668\u73af\u5883\u3002"}}
{"id": "2509.04448", "pdf": "https://arxiv.org/pdf/2509.04448", "abs": "https://arxiv.org/abs/2509.04448", "authors": ["Zehong Yan", "Peng Qi", "Wynne Hsu", "Mong Li Lee"], "title": "TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection", "categories": ["cs.CV", "cs.MM"], "comment": "EMNLP 2025; Project Homepage: https://yanzehong.github.io/trust-vl/", "summary": "Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.", "AI": {"tldr": "TRUST-VL\u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u548c\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\uff08\u5982\u6587\u672c\u3001\u89c6\u89c9\u548c\u8de8\u6a21\u6001\u626d\u66f2\uff09\u5bf9\u793e\u4f1a\u6784\u6210\u5a01\u80c1\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u7c7b\u578b\u7684\u626d\u66f2\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u63d0\u51faTRUST-VL\u6a21\u578b\uff0c\u5f15\u5165Question-Aware Visual Amplifier\u6a21\u5757\u63d0\u53d6\u4efb\u52a1\u7279\u5b9a\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u6784\u5efaTRUST-Instruct\u6570\u636e\u96c6\uff08198K\u6837\u672c\uff09\u652f\u6301\u8bad\u7ec3\u3002", "result": "\u5728\u9886\u57df\u5185\u548c\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTRUST-VL\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u5177\u5907\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "TRUST-VL\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u548c\u7ed3\u6784\u5316\u63a8\u7406\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u6709\u6548\u68c0\u6d4b\u4e0e\u6cdb\u5316\u3002"}}
{"id": "2509.04174", "pdf": "https://arxiv.org/pdf/2509.04174", "abs": "https://arxiv.org/abs/2509.04174", "authors": ["Christian Merz", "Lukas Schach", "Marie Luisa Fiedler", "Jean-Luc Lugrin", "Carolin Wienrich", "Marc Erich Latoschik"], "title": "Unobtrusive In-Situ Measurement of Behavior Change by Deep Metric Similarity Learning of Motion Patterns", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "This paper introduces an unobtrusive in-situ measurement method to detect\nuser behavior changes during arbitrary exposures in XR systems. Here, such\nbehavior changes are typically associated with the Proteus effect or bodily\naffordances elicited by different avatars that the users embody in XR. We\npresent a biometric user model based on deep metric similarity learning, which\nuses high-dimensional embeddings as reference vectors to identify behavior\nchanges of individual users. We evaluate our model against two alternative\napproaches: a (non-learned) motion analysis based on central tendencies of\nmovement patterns and subjective post-exposure embodiment questionnaires\nfrequently used in various XR exposures. In a within-subject study,\nparticipants performed a fruit collection task while embodying avatars of\ndifferent body heights (short, actual-height, and tall). Subjective assessments\nconfirmed the effective manipulation of perceived body schema, while the\n(non-learned) objective analyses of head and hand movements revealed\nsignificant differences across conditions. Our similarity learning model\ntrained on the motion data successfully identified the elicited behavior change\nfor various query and reference data pairings of the avatar conditions. The\napproach has several advantages in comparison to existing methods: 1) In-situ\nmeasurement without additional user input, 2) generalizable and scalable motion\nanalysis for various use cases, 3) user-specific analysis on the individual\nlevel, and 4) with a trained model, users can be added and evaluated in real\ntime to study how avatar changes affect behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5ea6\u91cf\u76f8\u4f3c\u5b66\u4e60\u7684\u751f\u7269\u7279\u5f81\u7528\u6237\u6a21\u578b\uff0c\u7528\u4e8e\u65e0\u5e72\u6270\u5730\u68c0\u6d4bXR\u7cfb\u7edf\u4e2d\u7528\u6237\u884c\u4e3a\u53d8\u5316\uff0c\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76XR\u73af\u5883\u4e2d\u7528\u6237\u56e0\u5316\u8eab\u53d8\u5316\uff08\u5982\u8eab\u9ad8\u5dee\u5f02\uff09\u5f15\u53d1\u7684\u884c\u4e3a\u53d8\u5316\uff08\u5982Proteus\u6548\u5e94\uff09\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5ea6\u91cf\u76f8\u4f3c\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u7ef4\u5d4c\u5165\u5411\u91cf\u8bc6\u522b\u7528\u6237\u884c\u4e3a\u53d8\u5316\uff0c\u5e76\u4e0e\u4f20\u7edf\u8fd0\u52a8\u5206\u6790\u548c\u4e3b\u89c2\u95ee\u5377\u5bf9\u6bd4\u3002", "result": "\u6a21\u578b\u6210\u529f\u8bc6\u522b\u5316\u8eab\u8eab\u9ad8\u53d8\u5316\u5f15\u53d1\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u4f18\u4e8e\u975e\u5b66\u4e60\u7684\u8fd0\u52a8\u5206\u6790\u548c\u4e3b\u89c2\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u7528\u6237\u8f93\u5165\uff0c\u5177\u6709\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5206\u6790XR\u4e2d\u7684\u884c\u4e3a\u53d8\u5316\u3002"}}
{"id": "2509.03542", "pdf": "https://arxiv.org/pdf/2509.03542", "abs": "https://arxiv.org/abs/2509.03542", "authors": ["Guosheng Hu"], "title": "The Chaotic Art: Quantum Representation and Manipulation of Color", "categories": ["quant-ph", "cs.GR"], "comment": "9 pages, 8 figures", "summary": "Due to its unique computing principles, quantum computing technology will\nprofoundly change the spectacle of color art. Focusing on experimental\nexploration of color qubit representation, color channel processing, and color\nimage generation via quantum computing, this article proposes a new technical\npath for color computing in quantum computing environment, by which digital\ncolor is represented, operated, and measured in quantum bits, and then restored\nfor classical computers as computing results. This method has been proved\npracticable as an artistic technique of color qubit representation and quantum\ncomputing via programming experiments in Qiskit and IBM Q. By building a bridge\nbetween classical chromatics and quantum graphics, quantum computers can be\nused for information visualization, image processing, and more color computing\ntasks. Furthermore, quantum computing can be expected to facilitate new color\ntheories and artistic concepts.", "AI": {"tldr": "\u91cf\u5b50\u8ba1\u7b97\u6280\u672f\u56e0\u5176\u72ec\u7279\u539f\u7406\u5c06\u6df1\u523b\u6539\u53d8\u8272\u5f69\u827a\u672f\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u63a2\u7d22\u91cf\u5b50\u6bd4\u7279\u8868\u793a\u3001\u8272\u5f69\u901a\u9053\u5904\u7406\u548c\u91cf\u5b50\u8ba1\u7b97\u751f\u6210\u56fe\u50cf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u91cf\u5b50\u8ba1\u7b97\u73af\u5883\u4e2d\u8fdb\u884c\u8272\u5f69\u8ba1\u7b97\u7684\u65b0\u6280\u672f\u8def\u5f84\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u8ba1\u7b97\u6280\u672f\u5728\u8272\u5f69\u827a\u672f\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u63a2\u7d22\u91cf\u5b50\u6bd4\u7279\u5982\u4f55\u8868\u793a\u548c\u64cd\u63a7\u8272\u5f69\uff0c\u4e3a\u827a\u672f\u521b\u9020\u63d0\u4f9b\u65b0\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u5728Qiskit\u548cIBM Q\u4e2d\u8fdb\u884c\u7f16\u7a0b\u5b9e\u9a8c\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u6bd4\u7279\u8868\u793a\u548c\u64cd\u4f5c\u8272\u5f69\u7684\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u7ed3\u679c\u8fd8\u539f\u81f3\u7ecf\u5178\u8ba1\u7b97\u673a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5728\u4fe1\u606f\u53ef\u89c6\u5316\u3001\u56fe\u50cf\u5904\u7406\u7b49\u8272\u5f69\u8ba1\u7b97\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u6709\u671b\u4fc3\u8fdb\u65b0\u8272\u5f69\u7406\u8bba\u548c\u827a\u672f\u6982\u5ff5\u7684\u53d1\u5c55\uff0c\u4e3a\u827a\u672f\u4e0e\u6280\u672f\u878d\u5408\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.03660", "pdf": "https://arxiv.org/pdf/2509.03660", "abs": "https://arxiv.org/abs/2509.03660", "authors": ["Yunkai Bao", "Reza Safarzadeh", "Xin Wang", "Steve Drew"], "title": "Semi-decentralized Federated Time Series Prediction with Client Availability Budgets", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated learning (FL) effectively promotes collaborative training among\ndistributed clients with privacy considerations in the Internet of Things (IoT)\nscenarios. Despite of data heterogeneity, FL clients may also be constrained by\nlimited energy and availability budgets. Therefore, effective selection of\nclients participating in training is of vital importance for the convergence of\nthe global model and the balance of client contributions. In this paper, we\ndiscuss the performance impact of client availability with time-series data on\nfederated learning. We set up three different scenarios that affect the\navailability of time-series data and propose FedDeCAB, a novel,\nsemi-decentralized client selection method applying probabilistic rankings of\navailable clients. When a client is disconnected from the server, FedDeCAB\nallows obtaining partial model parameters from the nearest neighbor clients for\njoint optimization, improving the performance of offline models and reducing\ncommunication overhead. Experiments based on real-world large-scale taxi and\nvessel trajectory datasets show that FedDeCAB is effective under highly\nheterogeneous data distribution, limited communication budget, and dynamic\nclient offline or rejoining.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u53bb\u4e2d\u5fc3\u5316\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5FedDeCAB\uff0c\u901a\u8fc7\u6982\u7387\u6392\u540d\u9009\u62e9\u5ba2\u6237\u7aef\uff0c\u5e76\u5229\u7528\u90bb\u8fd1\u5ba2\u6237\u7aef\u53c2\u6570\u4f18\u5316\u79bb\u7ebf\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u5728\u6570\u636e\u5f02\u6784\u6027\u548c\u901a\u4fe1\u9650\u5236\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u8054\u90a6\u5b66\u4e60\u5728\u7269\u8054\u7f51\u573a\u666f\u4e2d\u5982\u4f55\u5e94\u5bf9\u5ba2\u6237\u7aef\u6570\u636e\u5f02\u6784\u6027\u3001\u80fd\u91cf\u9650\u5236\u548c\u53ef\u7528\u6027\u52a8\u6001\u53d8\u5316\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5bf9\u6a21\u578b\u6536\u655b\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86FedDeCAB\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u6392\u540d\u9009\u62e9\u53ef\u7528\u5ba2\u6237\u7aef\uff0c\u5e76\u5728\u5ba2\u6237\u7aef\u79bb\u7ebf\u65f6\u5229\u7528\u90bb\u8fd1\u5ba2\u6237\u7aef\u53c2\u6570\u8fdb\u884c\u8054\u5408\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFedDeCAB\u5728\u9ad8\u5ea6\u5f02\u6784\u6570\u636e\u3001\u6709\u9650\u901a\u4fe1\u9884\u7b97\u548c\u52a8\u6001\u5ba2\u6237\u7aef\u53d8\u5316\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FedDeCAB\u6709\u6548\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u52a8\u6001\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u3002"}}
{"id": "2509.04241", "pdf": "https://arxiv.org/pdf/2509.04241", "abs": "https://arxiv.org/abs/2509.04241", "authors": ["Jaroslaw Kornowicz", "Maurice Pape", "Kirsten Thommes"], "title": "Would I regret being different? The influence of social norms on attitudes toward AI usage", "categories": ["cs.HC"], "comment": "30 pages, 5 figures", "summary": "Prior research shows that social norms can reduce algorithm aversion, but\nlittle is known about how such norms become established. Most accounts\nemphasize technological and individual determinants, yet AI adoption unfolds\nwithin organizational social contexts shaped by peers and supervisors. We ask\nwhether the source of the norm-peers or supervisors-shapes AI usage behavior.\nThis question is practically relevant for organizations seeking to promote\neffective AI adoption. We conducted an online vignette experiment, complemented\nby qualitative data on participants' feelings and justifications after\n(counter-)normative behavior. In line with the theory, counter-normative\nchoices elicited higher regret than norm-adherent choices. On average, choosing\nAI increased regret compared to choosing an human. This aversion was weaker\nwhen AI use was presented as the prevailing norm, indicating a statistically\nsignificant interaction between AI use and an AI-favoring norm. Participants\nalso attributed less blame to technology than to humans, which increased regret\nwhen AI was chosen over human expertise. Both peer and supervisor influence\nemerged as relevant factors, though contrary to expectations they did not\nsignificantly affect regret. Our findings suggest that regret aversion,\nembedded in social norms, is a central mechanism driving imitation in\nAI-related decision-making.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u793e\u4f1a\u89c4\u8303\u80fd\u51cf\u5c11\u7b97\u6cd5\u538c\u6076\uff0c\u4f46\u5176\u5f62\u6210\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u8bba\u6587\u63a2\u8ba8\u4e86\u89c4\u8303\u6765\u6e90\uff08\u540c\u4e8b\u6216\u4e0a\u7ea7\uff09\u5982\u4f55\u5f71\u54cdAI\u4f7f\u7528\u884c\u4e3a\uff0c\u53d1\u73b0AI\u9009\u62e9\u5f15\u53d1\u66f4\u9ad8\u540e\u6094\uff0c\u4f46\u5728AI\u652f\u6301\u89c4\u8303\u4e0b\u51cf\u5f31\u3002", "motivation": "\u63a2\u7a76\u793e\u4f1a\u89c4\u8303\u5982\u4f55\u5f71\u54cdAI\u91c7\u7528\uff0c\u7279\u522b\u662f\u89c4\u8303\u6765\u6e90\uff08\u540c\u4e8b\u6216\u4e0a\u7ea7\uff09\u7684\u4f5c\u7528\uff0c\u4ee5\u5e2e\u52a9\u7ec4\u7ec7\u6709\u6548\u63a8\u5e7fAI\u3002", "method": "\u91c7\u7528\u5728\u7ebf\u5c0f\u5b9e\u9a8c\u548c\u5b9a\u6027\u6570\u636e\uff0c\u5206\u6790\u53c2\u4e0e\u8005\u5728\uff08\u53cd\uff09\u89c4\u8303\u884c\u4e3a\u540e\u7684\u611f\u53d7\u548c\u7406\u7531\u3002", "result": "\u53cd\u89c4\u8303\u9009\u62e9\u5f15\u53d1\u66f4\u9ad8\u540e\u6094\uff1bAI\u652f\u6301\u89c4\u8303\u663e\u8457\u51cf\u5f31AI\u9009\u62e9\u7684\u540e\u6094\uff1b\u540c\u4e8b\u548c\u4e0a\u7ea7\u5f71\u54cd\u5b58\u5728\u4f46\u672a\u663e\u8457\u6539\u53d8\u540e\u6094\u7a0b\u5ea6\u3002", "conclusion": "\u793e\u4f1a\u89c4\u8303\u4e2d\u7684\u540e\u6094\u538c\u6076\u662f\u9a71\u52a8AI\u51b3\u7b56\u6a21\u4eff\u7684\u6838\u5fc3\u673a\u5236\u3002"}}
{"id": "2509.03945", "pdf": "https://arxiv.org/pdf/2509.03945", "abs": "https://arxiv.org/abs/2509.03945", "authors": ["Guglielmo Gattiglio", "Lyudmila Grigoryeva", "Massimiliano Tamborrino"], "title": "Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for Differential Equations", "categories": ["stat.CO", "cs.DC", "cs.NA", "math.NA", "stat.ML", "65M55, 65M22, 65L05, 50G15, 65Y05"], "comment": null, "summary": "We introduce Prob-GParareal, a probabilistic extension of the GParareal\nalgorithm designed to provide uncertainty quantification for the\nParallel-in-Time (PinT) solution of (ordinary and partial) differential\nequations (ODEs, PDEs). The method employs Gaussian processes (GPs) to model\nthe Parareal correction function, as GParareal does, further enabling the\npropagation of numerical uncertainty across time and yielding probabilistic\nforecasts of system's evolution. Furthermore, Prob-GParareal accommodates\nprobabilistic initial conditions and maintains compatibility with classical\nnumerical solvers, ensuring its straightforward integration into existing\nParareal frameworks. Here, we first conduct a theoretical analysis of the\ncomputational complexity and derive error bounds of Prob-GParareal. Then, we\nnumerically demonstrate the accuracy and robustness of the proposed algorithm\non five benchmark ODE systems, including chaotic, stiff, and bifurcation\nproblems. To showcase the flexibility and potential scalability of the proposed\nalgorithm, we also consider Prob-nnGParareal, a variant obtained by replacing\nthe GPs in Parareal with the nearest-neighbors GPs, illustrating its increased\nperformance on an additional PDE example. This work bridges a critical gap in\nthe development of probabilistic counterparts to established PinT methods.", "AI": {"tldr": "Prob-GParareal\u662f\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u7684GParareal\u6269\u5c55\u7b97\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u5e76\u884c\u65f6\u95f4\u89e3\u51b3\u65b9\u6848\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u652f\u6301\u6982\u7387\u521d\u59cb\u6761\u4ef6\uff0c\u5e76\u4e0e\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u517c\u5bb9\u3002", "motivation": "\u4e3a\u5e76\u884c\u65f6\u95f4\uff08PinT\uff09\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u91cf\u5316\u65b9\u6cd5\uff0c\u586b\u8865\u73b0\u6709PinT\u65b9\u6cd5\u5728\u6982\u7387\u5bf9\u5e94\u65b9\u9762\u7684\u7a7a\u767d\u3002", "method": "\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u5efa\u6a21Parareal\u6821\u6b63\u51fd\u6570\uff0c\u652f\u6301\u6570\u503c\u4e0d\u786e\u5b9a\u6027\u7684\u4f20\u64ad\uff0c\u5e76\u5f15\u5165Prob-nnGParareal\u53d8\u4f53\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u7406\u8bba\u5206\u6790\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u8bef\u5dee\u754c\u9650\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728\u591a\u79cdODE\u7cfb\u7edf\u548cPDE\u95ee\u9898\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Prob-GParareal\u6210\u529f\u586b\u8865\u4e86\u6982\u7387PinT\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2509.03711", "pdf": "https://arxiv.org/pdf/2509.03711", "abs": "https://arxiv.org/abs/2509.03711", "authors": ["Siddharth Muralee", "Sourag Cherupattamoolayil", "James C. Davis", "Antonio Bianchi", "Aravind Machiry"], "title": "Reactive Bottom-Up Testing", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "Modern computing systems remain rife with software vulnerabilities. Engineers\napply many means to detect them, of which dynamic testing is one of the most\ncommon and effective. However, most dynamic testing techniques follow a\ntop-down paradigm, and struggle to reach and exercise functions deep within the\ncall graph. While recent works have proposed Bottom-Up approaches to address\nthese limitations, they face challenges with false positives and generating\nvalid inputs that adhere to the context of the entire program.\n  In this work, we introduce a new paradigm that we call Reactive Bottom-Up\nTesting. Our insight is that function-level testing is necessary but not\nsufficient for the validation of vulnerabilities in functions. What we need is\na systematic approach that not only tests functions in isolation but also\nvalidates their behavior within the broader program context, ensuring that\ndetected vulnerabilities are both reachable and triggerable. We develop a\nthree-stage bottom-up testing scheme: (1) identify likely-vulnerable functions\nand generate type- and context-aware harnesses; (2) fuzz to find crashes and\nextract input constraints via symbolic execution; (3) verify crashes by\ncombining constraints to remove false positives. We implemented an automated\nprototype, which we call Griller. We evaluated Griller in a controlled setting\nusing a benchmark of 48 known vulnerabilities across 5 open-source projects,\nwhere we successfully detected 28 known vulnerabilities. Additionally, we\nevaluated Griller on several real-world applications such as Pacman, and it\ndiscovered 6 previously unknown vulnerabilities. Our findings suggest that\nReactive Bottom-Up Testing can significantly enhance the detection of\nvulnerabilities in complex systems, paving the way for more robust security\npractices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReactive Bottom-Up Testing\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u7684\u6d4b\u8bd5\u65b9\u6848\uff08\u8bc6\u522b\u6f5c\u5728\u6f0f\u6d1e\u51fd\u6570\u3001\u6a21\u7cca\u6d4b\u8bd5\u4e0e\u7b26\u53f7\u6267\u884c\u3001\u9a8c\u8bc1\u6392\u9664\u5047\u9633\u6027\uff09\u6765\u4f18\u5316\u52a8\u6001\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\u548c\u5047\u9633\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u8f6f\u4ef6\u6f0f\u6d1e\u666e\u904d\u5b58\u5728\uff0c\u4f20\u7edf\u52a8\u6001\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u6df1\u5165\u8c03\u7528\u56fe\u5e95\u5c42\u7684\u51fd\u6570\uff0c\u4e14\u6613\u4ea7\u751f\u5047\u9633\u6027\u6216\u8f93\u5165\u4e0d\u7b26\u5408\u7a0b\u5e8f\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faReactive Bottom-Up Testing\u4e09\u9636\u6bb5\u65b9\u6848\uff1a1) \u8bc6\u522b\u6f5c\u5728\u6f0f\u6d1e\u51fd\u6570\u5e76\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6d4b\u8bd5\u5de5\u5177\uff1b2) \u901a\u8fc7\u6a21\u7cca\u6d4b\u8bd5\u548c\u7b26\u53f7\u6267\u884c\u63d0\u53d6\u8f93\u5165\u7ea6\u675f\uff1b3) \u7ed3\u5408\u7ea6\u675f\u9a8c\u8bc1\u6f0f\u6d1e\u4ee5\u6392\u9664\u5047\u9633\u6027\u3002", "result": "\u572848\u4e2a\u5df2\u77e5\u6f0f\u6d1e\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u68c0\u6d4b\u523028\u4e2a\uff0c\u5e76\u5728Pacman\u7b49\u771f\u5b9e\u5e94\u7528\u4e2d\u53d1\u73b06\u4e2a\u672a\u77e5\u6f0f\u6d1e\u3002", "conclusion": "Reactive Bottom-Up Testing\u80fd\u663e\u8457\u63d0\u5347\u590d\u6742\u7cfb\u7edf\u4e2d\u6f0f\u6d1e\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3a\u66f4\u5065\u58ee\u7684\u5b89\u5168\u5b9e\u8df5\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2509.04254", "pdf": "https://arxiv.org/pdf/2509.04254", "abs": "https://arxiv.org/abs/2509.04254", "authors": ["Meisam Jamshidi Seikavandi", "Fabricio Batista Narcizo", "Ted Vucurevich", "Andrew Burke Dittberner", "Paolo Burelli"], "title": "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition from Physiological Signals", "categories": ["cs.HC"], "comment": null, "summary": "We present MuMTAffect, a novel Multimodal Multitask Affective Embedding\nNetwork designed for joint emotion classification and personality prediction\n(re-identification) from short physiological signal segments. MuMTAffect\nintegrates multiple physiological modalities pupil dilation, eye gaze, facial\naction units, and galvanic skin response using dedicated, transformer-based\nencoders for each modality and a fusion transformer to model cross-modal\ninteractions. Inspired by the Theory of Constructed Emotion, the architecture\nexplicitly separates core affect encoding (valence/arousal) from higher-level\nconceptualization, thereby grounding predictions in contemporary affective\nneuroscience. Personality trait prediction is leveraged as an auxiliary task to\ngenerate robust, user-specific affective embeddings, significantly enhancing\nemotion recognition performance. We evaluate MuMTAffect on the AFFEC dataset,\ndemonstrating that stimulus-level emotional cues (Stim Emo) and galvanic skin\nresponse substantially improve arousal classification, while pupil and gaze\ndata enhance valence discrimination. The inherent modularity of MuMTAffect\nallows effortless integration of additional modalities, ensuring scalability\nand adaptability. Extensive experiments and ablation studies underscore the\nefficacy of our multimodal multitask approach in creating personalized,\ncontext-aware affective computing systems, highlighting pathways for further\nadvancements in cross-subject generalisation.", "AI": {"tldr": "MuMTAffect\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u591a\u4efb\u52a1\u60c5\u611f\u5d4c\u5165\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece\u751f\u7406\u4fe1\u53f7\u7247\u6bb5\u4e2d\u8054\u5408\u8fdb\u884c\u60c5\u7eea\u5206\u7c7b\u548c\u4eba\u683c\u9884\u6d4b\u3002", "motivation": "\u8bba\u6587\u7684\u52a8\u673a\u662f\u57fa\u4e8e\u6784\u5efa\u60c5\u611f\u7406\u8bba\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u5206\u79bb\u6838\u5fc3\u60c5\u611f\u7f16\u7801\u548c\u9ad8\u5c42\u6b21\u6982\u5ff5\u5316\u7684\u67b6\u6784\uff0c\u4ece\u800c\u63d0\u5347\u60c5\u611f\u8ba1\u7b97\u7684\u51c6\u786e\u6027\u548c\u4e2a\u6027\u5316\u3002", "method": "MuMTAffect\u901a\u8fc7\u4e13\u7528\u7684\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\u6574\u5408\u591a\u79cd\u751f\u7406\u6a21\u6001\uff08\u5982\u77b3\u5b54\u6269\u5f20\u3001\u89c6\u7ebf\u3001\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u548c\u76ae\u80a4\u7535\u53cd\u5e94\uff09\uff0c\u5e76\u4f7f\u7528\u878d\u5408Transformer\u5efa\u6a21\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "\u5728AFFEC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMuMTAffect\u5728\u60c5\u7eea\u8bc6\u522b\u6027\u80fd\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u4e14\u4e0d\u540c\u6a21\u6001\u6570\u636e\u5bf9\u60c5\u7eea\u5206\u7c7b\u7684\u8d21\u732e\u5404\u6709\u4fa7\u91cd\u3002", "conclusion": "MuMTAffect\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u5176\u5177\u6709\u9ad8\u5ea6\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u672a\u6765\u8de8\u4e3b\u4f53\u6cdb\u5316\u7684\u60c5\u611f\u8ba1\u7b97\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2509.04095", "pdf": "https://arxiv.org/pdf/2509.04095", "abs": "https://arxiv.org/abs/2509.04095", "authors": ["Achilleas Santi Seisa", "Viswa Narayanan Sankaranarayanan", "Gerasimos Damigos", "Sumeet Gajanan Satpute", "George Nikolakopoulos"], "title": "Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation", "categories": ["cs.RO", "cs.DC"], "comment": "6 pages, 7 figures, CCGridW 2025", "summary": "Cloud robotics has emerged as a promising technology for robotics\napplications due to its advantages of offloading computationally intensive\ntasks, facilitating data sharing, and enhancing robot coordination. However,\nintegrating cloud computing with robotics remains a complex challenge due to\nnetwork latency, security concerns, and the need for efficient resource\nmanagement. In this work, we present a scalable and intuitive framework for\ntesting cloud and edge robotic systems. The framework consists of two main\ncomponents enabled by containerized technology: (a) a containerized cloud\ncluster and (b) the containerized robot simulation environment. The system\nincorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling\nbidirectional communication between the cloud cluster container and the robot\nsimulation environment, while simulating realistic network conditions. To\nachieve this, we consider the use case of cloud-assisted remote control for\naerial robots, while utilizing Linux-based traffic control to introduce\nartificial delay and jitter, replicating variable network conditions\nencountered in practical cloud-robot deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bb9\u5668\u5316\u6280\u672f\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u4e91\u548c\u8fb9\u7f18\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u7f51\u7edc\u5ef6\u8fdf\u548c\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u4e91\u673a\u5668\u4eba\u6280\u672f\u5177\u6709\u8ba1\u7b97\u4efb\u52a1\u5378\u8f7d\u548c\u6570\u636e\u5171\u4eab\u7684\u4f18\u52bf\uff0c\u4f46\u5176\u96c6\u6210\u9762\u4e34\u7f51\u7edc\u5ef6\u8fdf\u3001\u5b89\u5168\u6027\u548c\u8d44\u6e90\u7ba1\u7406\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5bb9\u5668\u5316\u6280\u672f\u6784\u5efa\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u5bb9\u5668\u5316\u4e91\u96c6\u7fa4\u548c\u673a\u5668\u4eba\u4eff\u771f\u73af\u5883\uff0c\u5e76\u901a\u8fc7UDP\u96a7\u9053\u5b9e\u73b0\u53cc\u5411\u901a\u4fe1\uff0c\u6a21\u62df\u5b9e\u9645\u7f51\u7edc\u6761\u4ef6\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4e91\u8f85\u52a9\u65e0\u4eba\u673a\u8fdc\u7a0b\u63a7\u5236\u7684\u7528\u4f8b\uff0c\u5e76\u6a21\u62df\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u7f51\u7edc\u5ef6\u8fdf\u548c\u6296\u52a8\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3a\u4e91\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.03807", "pdf": "https://arxiv.org/pdf/2509.03807", "abs": "https://arxiv.org/abs/2509.03807", "authors": ["Junhui Li", "Chengbin Feng", "Zhiwei Yang", "Qi Mo", "Wei Wang"], "title": "BIDO: A Unified Approach to Address Obfuscation and Concept Drift Challenges in Image-based Malware Detection", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "To identify malicious Android applications, various malware detection\ntechniques have been proposed. Among them, image-based approaches are\nconsidered potential alternatives due to their efficiency and scalability.\nRecent studies have reported that these approaches suffer significant\nperformance declines when confronted with obfuscation or concept drift.\nHowever, existing solutions often treat these two challenges as different\nproblems, offering independent solutions. These techniques overlook the fact\nthat both challenges share a common statistical root, out-of-distribution, and\nresearch from this perspective remains limited. In response, we propose BIDO, a\nhybrid image-based malware detector designed to enhance robustness against both\nobfuscation and concept drift simultaneously. Specifically, to improve the\ndiscriminative power of image features, we introduce a local feature selection\nmodule that identifies informative subregions within malware images. Second, to\nenhance feature robustness, we model pairwise cross-modal dependencies in an\nouter product space, enabling the extraction of stable co-occurrence patterns.\nThird, to ensure feature compactness, we design a learnable metric that pulls\nsamples with identical labels closer while pushing apart those with different\nlabels, regardless of obfuscation or concept drift. Extensive experiments on\nthe real-world datasets demonstrate that BIDO significantly outperforms\nexisting baselines, achieving higher robustness against both concept drift and\nobfuscation. The source code is available at:\nhttps://github.com/whatishope/BIDO/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBIDO\u7684\u6df7\u5408\u56fe\u50cf\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u5c40\u90e8\u7279\u5f81\u9009\u62e9\u6a21\u5757\u3001\u8de8\u6a21\u6001\u4f9d\u8d56\u5efa\u6a21\u548c\u5b66\u4e60\u5ea6\u91cf\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u6df7\u6dc6\u548c\u6982\u5ff5\u6f02\u79fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u5728\u6df7\u6dc6\u548c\u6982\u5ff5\u6f02\u79fb\u9762\u524d\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u72ec\u7acb\u5904\u7406\u8fd9\u4e24\u4e2a\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u5171\u540c\u7684\u7edf\u8ba1\u6839\u6e90\u2014\u2014\u5206\u5e03\u5916\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5c40\u90e8\u7279\u5f81\u9009\u62e9\u6a21\u5757\u63d0\u5347\u7279\u5f81\u5224\u522b\u529b\uff1b\u5728\u5916\u90e8\u4e58\u79ef\u7a7a\u95f4\u5efa\u6a21\u8de8\u6a21\u6001\u4f9d\u8d56\u5173\u7cfb\u4ee5\u63d0\u53d6\u7a33\u5b9a\u5171\u73b0\u6a21\u5f0f\uff1b\u8bbe\u8ba1\u5b66\u4e60\u5ea6\u91cf\u786e\u4fdd\u7279\u5f81\u7d27\u51d1\u6027\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBIDO\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5bf9\u6982\u5ff5\u6f02\u79fb\u548c\u6df7\u6dc6\u5747\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "BIDO\u901a\u8fc7\u7edf\u4e00\u5904\u7406\u6df7\u6dc6\u548c\u6982\u5ff5\u6f02\u79fb\u7684\u5171\u540c\u6839\u6e90\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u56fe\u50cf\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u65b9\u6848\u3002"}}
{"id": "2509.04303", "pdf": "https://arxiv.org/pdf/2509.04303", "abs": "https://arxiv.org/abs/2509.04303", "authors": ["Georgios Makridis", "Georgios Fragiadakis", "Jorge Oliveira", "Tomaz Saraiva", "Philip Mavrepis", "Georgios Fatouros", "Dimosthenis Kyriazis"], "title": "HumAIne-Chatbot: Real-Time Personalized Conversational AI via Reinforcement Learning", "categories": ["cs.HC", "cs.AI"], "comment": "11 pages, 4 figures, IEEE conference format", "summary": "Current conversational AI systems often provide generic, one-size-fits-all\ninteractions that overlook individual user characteristics and lack adaptive\ndialogue management. To address this gap, we introduce\n\\textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes\nresponses through a novel user profiling framework. The system is pre-trained\non a diverse set of GPT-generated virtual personas to establish a broad prior\nover user types. During live interactions, an online reinforcement learning\nagent refines per-user models by combining implicit signals (e.g. typing speed,\nsentiment, engagement duration) with explicit feedback (e.g., likes and\ndislikes). This profile dynamically informs the chatbot dialogue policy,\nenabling real-time adaptation of both content and style. To evaluate the\nsystem, we performed controlled experiments with 50 synthetic personas in\nmultiple conversation domains. The results showed consistent improvements in\nuser satisfaction, personalization accuracy, and task achievement when\npersonalization features were enabled. Statistical analysis confirmed\nsignificant differences between personalized and nonpersonalized conditions,\nwith large effect sizes across key metrics. These findings highlight the\neffectiveness of AI-driven user profiling and provide a strong foundation for\nfuture real-world validation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86HumAIne-chatbot\uff0c\u4e00\u79cd\u901a\u8fc7\u7528\u6237\u753b\u50cf\u6846\u67b6\u5b9e\u73b0\u4e2a\u6027\u5316\u56de\u590d\u7684\u5bf9\u8bddAI\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bddAI\u7cfb\u7edf\u901a\u5e38\u63d0\u4f9b\u901a\u7528\u53cd\u9988\uff0c\u5ffd\u7565\u4e86\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u81ea\u9002\u5e94\u8c03\u6574\u5bf9\u8bdd\u7ba1\u7406\u7684\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u9884\u8bad\u7ec3GPT\u751f\u6210\u7684\u865a\u62df\u89d2\u8272\u5efa\u7acb\u7528\u6237\u7c7b\u578b\u5148\u9a8c\uff0c\u5e76\u5229\u7528\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u9690\u5f0f\u548c\u663e\u5f0f\u53cd\u9988\u4f18\u5316\u7528\u6237\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e2a\u6027\u5316\u529f\u80fd\u663e\u8457\u63d0\u9ad8\u4e86\u7528\u6237\u6ee1\u610f\u5ea6\u3001\u4e2a\u6027\u5316\u51c6\u786e\u5ea6\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u7528\u6237\u753b\u50cf\u6709\u6548\u63d0\u5347\u4e86\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.04340", "pdf": "https://arxiv.org/pdf/2509.04340", "abs": "https://arxiv.org/abs/2509.04340", "authors": ["Kristina L. Kupferschmidt", "Kieran O'Doherty", "Joshua A. Skorburg"], "title": "Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing Clinical Notes", "categories": ["cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are often proposed as tools to streamline\nclinical documentation, a task viewed as both high-volume and low-risk.\nHowever, even seemingly straightforward applications of LLMs raise complex\nsociotechnical considerations to translate into practice. This case study,\nconducted at KidsAbility, a pediatric rehabilitation facility in Ontario,\nCanada examined the use of LLMs to support occupational therapists in reducing\ndocumentation burden.We conducted a qualitative study involving 20 clinicians\nwho participated in pilot programs using two AI technologies: a general-purpose\nproprietary LLM and a bespoke model fine-tuned on proprietary historical\ndocumentation.\n  Our findings reveal that documentation challenges are sociotechnical in\nnature, shaped by clinical workflows, organizational policies, and system\nconstraints. Four key themes emerged: (1) the heterogeneity of workflows, (2)\nthe documentation burden is systemic and not directly linked to the creation of\nany single type of documentation, (3) the need for flexible tools and clinician\nautonomy, and (4) effective implementation requires mutual learning between\nclinicians and AI systems.\n  While LLMs show promise in easing documentation tasks, their success will\ndepend on flexible, adaptive integration that supports clinician autonomy.\nBeyond technical performance, sustained adoption will require training programs\nand implementation strategies that reflect the complexity of clinical\nenvironments.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u51cf\u8f7b\u4e34\u5e8a\u6587\u6863\u8d1f\u62c5\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u6210\u529f\u4f9d\u8d56\u4e8e\u7075\u6d3b\u7684\u6574\u5408\u65b9\u5f0f\u548c\u5bf9\u4e34\u5e8a\u533b\u751f\u81ea\u4e3b\u6027\u7684\u652f\u6301\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6587\u6863\u6311\u6218\u5177\u6709\u793e\u4f1a\u6280\u672f\u6027\uff0c\u9700\u8981\u7efc\u5408\u8003\u8651\u5de5\u4f5c\u6d41\u7a0b\u3001\u653f\u7b56\u548c\u6280\u672f\u9650\u5236\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLMs\u5982\u4f55\u652f\u6301\u513f\u79d1\u5eb7\u590d\u6cbb\u7597\u5e08\u51cf\u8f7b\u6587\u6863\u8d1f\u62c5\uff0c\u5e76\u5206\u6790\u5176\u5728\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6311\u6218\u3002", "method": "\u5728\u52a0\u62ff\u5927\u5b89\u5927\u7565\u7701\u7684\u4e00\u5bb6\u513f\u79d1\u5eb7\u590d\u673a\u6784\u8fdb\u884c\u4e86\u4e00\u9879\u5b9a\u6027\u7814\u7a76\uff0c20\u540d\u4e34\u5e8a\u533b\u5e08\u53c2\u4e0e\u4e86\u4e24\u9879AI\u6280\u672f\u7684\u8bd5\u70b9\u9879\u76ee\uff1a\u4e00\u6b3e\u901a\u7528\u4e13\u6709LLM\u548c\u4e00\u6b3e\u57fa\u4e8e\u5386\u53f2\u6587\u6863\u5b9a\u5236\u5316\u7684\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6587\u6863\u6311\u6218\u5177\u6709\u793e\u4f1a\u6280\u672f\u6027\uff0c\u6d89\u53ca\u56db\u4e2a\u5173\u952e\u4e3b\u9898\uff1a\u5de5\u4f5c\u6d41\u7a0b\u7684\u5f02\u8d28\u6027\u3001\u6587\u6863\u8d1f\u62c5\u7684\u7cfb\u7edf\u6027\u3001\u5bf9\u7075\u6d3b\u5de5\u5177\u7684\u9700\u6c42\u4ee5\u53caClinician\u4e0eAI\u7cfb\u7edf\u7684\u76f8\u4e92\u5b66\u4e60\u3002LLMs\u7684\u6f5c\u529b\u4f9d\u8d56\u4e8e\u7075\u6d3b\u6574\u5408\u548c\u4e34\u5e8a\u533b\u751f\u7684\u81ea\u4e3b\u6743\u3002", "conclusion": "LLMs\u5728\u4e34\u5e8a\u6587\u6863\u51cf\u8f7b\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u6210\u529f\u5e94\u7528\u9700\u8981\u7075\u6d3b\u7684\u96c6\u6210\u7b56\u7565\u548c\u5168\u9762\u7684\u57f9\u8bad\u8ba1\u5212\uff0c\u4ee5\u5e94\u5bf9\u4e34\u5e8a\u73af\u5883\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2509.04356", "pdf": "https://arxiv.org/pdf/2509.04356", "abs": "https://arxiv.org/abs/2509.04356", "authors": ["Atikkhan Faridkhan Nilgar", "Kristof Van Laerhoven", "Ayub Kinoti"], "title": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction.", "AI": {"tldr": "SRWToolkit\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Wizard of Oz\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5feb\u901f\u5f00\u53d1\u57fa\u4e8e\u672c\u5730\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4ea4\u673a\u5668\u4eba\u89d2\u8272\uff0c\u652f\u6301\u591a\u6a21\u6001\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u53ef\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u57fa\u4e8e\u672c\u5730\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4ea4\u673a\u5668\u4eba\u89d2\u8272\u7684\u5feb\u901f\u5f00\u53d1\uff0c\u907f\u514d\u4f9d\u8d56\u4e91\u7aef\u670d\u52a1\uff0c\u63d0\u5347\u6a21\u5757\u5316\u548c\u8bbe\u5907\u529f\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7f51\u9875\u7684\u5de5\u5177\u5305\uff0c\u652f\u6301\u6587\u672c\u8f93\u5165\u3001\u8bed\u97f3\u6309\u94ae\u548c\u5524\u9192\u8bcd\u547d\u4ee4\uff0c\u63d0\u4f9b\u5b9e\u65f6\u914d\u7f6e\u529f\u80fd\u3002\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08n=11\uff09\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\uff0c\u5de5\u5177\u5305\u5728\u53ef\u7528\u6027\u3001\u4fe1\u4efb\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u89d2\u8272\u5f00\u53d1\u3002", "conclusion": "SRWToolkit\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5f00\u53d1\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2509.04358", "pdf": "https://arxiv.org/pdf/2509.04358", "abs": "https://arxiv.org/abs/2509.04358", "authors": ["Atikkhan Faridkhan Nilgar", "Manuel Dietrich", "Kristof Van Laerhoven"], "title": "Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the Roles of Information Transparency, User Control, and Proactivity", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Social robots are increasingly recognized as valuable supporters in the field\nof well-being coaching. They can function as independent coaches or provide\nsupport alongside human coaches, and healthcare professionals. In coaching\ninteractions, these robots often handle sensitive information shared by users,\nmaking privacy a relevant issue. Despite this, little is known about the\nfactors that shape users' privacy perceptions. This research aims to examine\nthree key factors systematically: (1) the transparency about information usage,\n(2) the level of specific user control over how the robot uses their\ninformation, and (3) the robot's behavioral approach - whether it acts\nproactively or only responds on demand. Our results from an online study (N =\n200) show that even when users grant the robot general access to personal data,\nthey additionally expect the ability to explicitly control how that information\nis interpreted and shared during sessions. Experimental conditions that\nprovided such control received significantly higher ratings for perceived\nprivacy appropriateness and trust. Compared to user control, the effects of\ntransparency and proactivity on privacy appropriateness perception were low,\nand we found no significant impact. The results suggest that merely informing\nusers or proactive sharing is insufficient without accompanying user control.\nThese insights underscore the need for further research on mechanisms that\nallow users to manage robots' information processing and sharing, especially\nwhen social robots take on more proactive roles alongside humans.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u5065\u5eb7\u6559\u7ec3\u65f6\uff0c\u7528\u6237\u5bf9\u9690\u79c1\u7684\u63a7\u5236\u6743\u6bd4\u900f\u660e\u5ea6\u548c\u673a\u5668\u4eba\u4e3b\u52a8\u6027\u66f4\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8\u793e\u4ea4\u673a\u5668\u4eba\u4e2d\u7528\u6237\u9690\u79c1\u611f\u77e5\u7684\u5173\u952e\u56e0\u7d20\uff08\u5982\u900f\u660e\u6027\u3001\u7528\u6237\u63a7\u5236\u3001\u673a\u5668\u4eba\u884c\u4e3a\u65b9\u5f0f\uff09\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u7814\u7a76\uff08N=200\uff09\u7cfb\u7edf\u6027\u5206\u6790\u900f\u660e\u5ea6\u3001\u7528\u6237\u63a7\u5236\u548c\u673a\u5668\u4eba\u884c\u4e3a\u65b9\u5f0f\u5bf9\u9690\u79c1\u611f\u77e5\u7684\u5f71\u54cd\u3002", "result": "\u7528\u6237\u63a7\u5236\u663e\u8457\u63d0\u9ad8\u9690\u79c1\u611f\u77e5\u548c\u4fe1\u4efb\u5ea6\uff0c\u800c\u900f\u660e\u5ea6\u548c\u673a\u5668\u4eba\u4e3b\u52a8\u6027\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "\u7528\u6237\u63a7\u5236\u662f\u9690\u79c1\u4fdd\u62a4\u7684\u6838\u5fc3\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5982\u4f55\u8ba9\u7528\u6237\u7ba1\u7406\u673a\u5668\u4eba\u7684\u4fe1\u606f\u5904\u7406\u3002"}}
{"id": "2509.03536", "pdf": "https://arxiv.org/pdf/2509.03536", "abs": "https://arxiv.org/abs/2509.03536", "authors": ["Weizhi Chen", "Ziwei Wang", "Leyang Yang", "Sheng Zhou", "Xiaoxuan Tang", "Jiajun Bu", "Yong Li", "Wei Jiang"], "title": "PG-Agent: An Agent Powered by Page Graph", "categories": ["cs.AI", "cs.HC"], "comment": "Paper accepted to ACM MM 2025", "summary": "Graphical User Interface (GUI) agents possess significant commercial and\nsocial value, and GUI agents powered by advanced multimodal large language\nmodels (MLLMs) have demonstrated remarkable potential. Currently, existing GUI\nagents usually utilize sequential episodes of multi-step operations across\npages as the prior GUI knowledge, which fails to capture the complex transition\nrelationship between pages, making it challenging for the agents to deeply\nperceive the GUI environment and generalize to new scenarios. Therefore, we\ndesign an automated pipeline to transform the sequential episodes into page\ngraphs, which explicitly model the graph structure of the pages that are\nnaturally connected by actions. To fully utilize the page graphs, we further\nintroduce Retrieval-Augmented Generation (RAG) technology to effectively\nretrieve reliable perception guidelines of GUI from them, and a tailored\nmulti-agent framework PG-Agent with task decomposition strategy is proposed to\nbe injected with the guidelines so that it can generalize to unseen scenarios.\nExtensive experiments on various benchmarks demonstrate the effectiveness of\nPG-Agent, even with limited episodes for page graph construction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06GUI\u4ee3\u7406\u7684\u591a\u6b65\u64cd\u4f5c\u5e8f\u5217\u8f6c\u6362\u4e3a\u9875\u9762\u56fe\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408RAG\u6280\u672f\u548c\u591a\u4ee3\u7406\u6846\u67b6PG-Agent\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5728\u65b0\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684GUI\u4ee3\u7406\u901a\u5e38\u4f9d\u8d56\u591a\u6b65\u64cd\u4f5c\u5e8f\u5217\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f46\u96be\u4ee5\u6355\u6349\u9875\u9762\u95f4\u7684\u590d\u6742\u8f6c\u79fb\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u4ee3\u7406\u5bf9\u65b0\u573a\u666f\u7684\u611f\u77e5\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u5c06\u64cd\u4f5c\u5e8f\u5217\u8f6c\u6362\u4e3a\u9875\u9762\u56fe\uff1b\u5f15\u5165RAG\u6280\u672f\u68c0\u7d22GUI\u611f\u77e5\u6307\u5357\uff1b\u63d0\u51faPG-Agent\u6846\u67b6\uff0c\u7ed3\u5408\u4efb\u52a1\u5206\u89e3\u7b56\u7565\u3002", "result": "\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPG-Agent\u8868\u73b0\u51fa\u8272\uff0c\u5373\u4f7f\u9875\u9762\u56fe\u6784\u5efa\u7684\u6837\u672c\u6709\u9650\u3002", "conclusion": "\u9875\u9762\u56fe\u548cPG-Agent\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347\u4e86GUI\u4ee3\u7406\u5728\u65b0\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.03728", "pdf": "https://arxiv.org/pdf/2509.03728", "abs": "https://arxiv.org/abs/2509.03728", "authors": ["Wesley Hanwen Deng", "Sunnie S. Y. Kim", "Akshita Jha", "Ken Holstein", "Motahhare Eslami", "Lauren Wilcox", "Leon A Gatys"], "title": "PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Recent developments in AI governance and safety research have called for\nred-teaming methods that can effectively surface potential risks posed by AI\nmodels. Many of these calls have emphasized how the identities and backgrounds\nof red-teamers can shape their red-teaming strategies, and thus the kinds of\nrisks they are likely to uncover. While automated red-teaming approaches\npromise to complement human red-teaming by enabling larger-scale exploration of\nmodel behavior, current approaches do not consider the role of identity. As an\ninitial step towards incorporating people's background and identities in\nautomated red-teaming, we develop and evaluate a novel method, PersonaTeaming,\nthat introduces personas in the adversarial prompt generation process to\nexplore a wider spectrum of adversarial strategies. In particular, we first\nintroduce a methodology for mutating prompts based on either \"red-teaming\nexpert\" personas or \"regular AI user\" personas. We then develop a dynamic\npersona-generating algorithm that automatically generates various persona types\nadaptive to different seed prompts. In addition, we develop a set of new\nmetrics to explicitly measure the \"mutation distance\" to complement existing\ndiversity measurements of adversarial prompts. Our experiments show promising\nimprovements (up to 144.1%) in the attack success rates of adversarial prompts\nthrough persona mutation, while maintaining prompt diversity, compared to\nRainbowPlus, a state-of-the-art automated red-teaming method. We discuss the\nstrengths and limitations of different persona types and mutation methods,\nshedding light on future opportunities to explore complementarities between\nautomated and human red-teaming approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5PersonaTeaming\uff0c\u901a\u8fc7\u5728\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u89d2\u8272\uff0c\u4ee5\u63d0\u9ad8AI\u6a21\u578b\u7684\u98ce\u9669\u53d1\u73b0\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u7ea2\u961f\u65b9\u6cd5\u672a\u8003\u8651\u8eab\u4efd\u80cc\u666f\uff0c\u800c\u4eba\u7c7b\u7ea2\u961f\u56e0\u8eab\u4efd\u4e0d\u540c\u53ef\u80fd\u53d1\u73b0\u4e0d\u540c\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u7ed3\u5408\u8eab\u4efd\u56e0\u7d20\u6539\u8fdb\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1PersonaTeaming\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u4e13\u5bb6\u6216\u666e\u901a\u7528\u6237\u89d2\u8272\u7684\u63d0\u793a\u53d8\u5f02\u7b56\u7565\uff0c\u52a8\u6001\u751f\u6210\u9002\u5e94\u6027\u89d2\u8272\uff0c\u5e76\u5f15\u5165\u65b0\u6307\u6807\u8861\u91cf\u53d8\u5f02\u8ddd\u79bb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u89d2\u8272\u53d8\u5f02\u4f7f\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe144.1%\uff0c\u540c\u65f6\u4fdd\u6301\u63d0\u793a\u591a\u6837\u6027\u3002", "conclusion": "PersonaTeaming\u5c55\u793a\u4e86\u7ed3\u5408\u8eab\u4efd\u80cc\u666f\u7684\u81ea\u52a8\u5316\u7ea2\u961f\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u4e0e\u4eba\u7c7b\u7ea2\u961f\u7684\u4e92\u8865\u6027\u3002"}}
{"id": "2509.04056", "pdf": "https://arxiv.org/pdf/2509.04056", "abs": "https://arxiv.org/abs/2509.04056", "authors": ["Luciano A. Abriata"], "title": "The MolecularWeb Universe: Web-Based, Immersive, Multiuser Molecular Graphics And Modeling, for Education and Work in Chemistry, Structural Biology, and Materials Sciences", "categories": ["physics.chem-ph", "cs.HC"], "comment": "37 pages, 7 figures", "summary": "Molecular visualization software has long supported research and education in\nchemical and structural sciences, but consumer devices constrained to 2D inputs\nand outputs pose two major challenges: they poorly convey 3D nature, and 3D\nmanipulation is very difficult. eXtended Reality (XR, including AR and VR)\noffers new ways to see and interact with molecules in three dimensions. This\nchapter presents the \"MolecularWeb\" ecosystem (https://molecularweb.org), a set\nof web-based tools for immersive visualization, modeling, and simulations,\nalready widely used in education and science communication and now expanding\ntoward research applications. We cover moleculARweb, which provides AR\neducational activities via phones, tablets, and computers; MolecularWebXR, a\nmultiuser WebXR platform accessible from both headsets and simpler devices,\nsupporting immersive education, outreach, and scientific discussion; and\nPDB2AR, which enables users to generate custom content for MolecularWebXR and\nstandalone AR/VR. Finally, we introduce a prototype and an upcoming version of\nHandMol, our latest WebXR software which allows concurrent multiuser immersive\nvisualization and modeling of molecules with bare hands supported by real-time\nmolecular mechanics, natural language input via a language model, and access\nthrough both high-end headsets or consumer devices like smartphones and\nlaptops. Together, these tools demonstrate the present and near-future of\naccessible, interactive molecular science on the web.", "AI": {"tldr": "MolecularWeb\u751f\u6001\u7cfb\u7edf\u662f\u4e00\u5957\u57fa\u4e8e\u7f51\u7edc\u7684\u5de5\u5177\uff0c\u652f\u6301\u6c89\u6d78\u5f0f\u5206\u5b50\u53ef\u89c6\u5316\u548c\u5efa\u6a21\uff0c\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6559\u80b2\u548c\u79d1\u7814\u3002", "motivation": "\u89e3\u51b32D\u8bbe\u5907\u57283D\u5206\u5b50\u53ef\u89c6\u5316\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5229\u7528XR\u6280\u672f\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u5f00\u53d1\u4e86\u591a\u4e2a\u5de5\u5177\uff0c\u5982moleculARweb\u3001MolecularWebXR\u548cPDB2AR\uff0c\u652f\u6301AR/VR\u6559\u80b2\u3001\u79d1\u7814\u548c\u591a\u7528\u6237\u534f\u4f5c\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u6559\u80b2\u5230\u79d1\u7814\u7684\u5e94\u7528\u6269\u5c55\uff0c\u5e76\u5c55\u793a\u4e86\u672a\u6765\u5206\u5b50\u79d1\u5b66\u7684\u4ea4\u4e92\u6f5c\u529b\u3002", "conclusion": "MolecularWeb\u5c55\u793a\u4e86\u7f51\u7edc\u73af\u5883\u4e0b\u5206\u5b50\u79d1\u5b66\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u4ea4\u4e92\u6027\u524d\u666f\u3002"}}
{"id": "2509.04104", "pdf": "https://arxiv.org/pdf/2509.04104", "abs": "https://arxiv.org/abs/2509.04104", "authors": ["Keara Schaaij", "Roel Boumans", "Tibor Bosse", "Iris Hendrickx"], "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted for TSD 2025", "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u6784\u5efa\u4e2a\u6027\u5316\u8bcd\u6c47\u914d\u7f6e\u6587\u4ef6\u5b9e\u73b0\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u7684\u8bcd\u6c47\u5bf9\u9f50\uff0c\u53d1\u73b0\u8f83\u5c0f\u4e14\u7d27\u51d1\u7684\u914d\u7f6e\u6587\u4ef6\u572810\u5206\u949f\u8f6c\u5f55\u8bed\u97f3\u548c\u7279\u5b9a\u8bcd\u6027\u9879\u76ee\u6570\u91cf\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8bcd\u6c47\u5bf9\u9f50\u5bf9\u6c9f\u901a\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5982\u4f55\u5e94\u7528\u4e8e\u5bf9\u8bdd\u4ee3\u7406\u4ecd\u5f85\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80cc\u666f\u4e0b\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8c03\u6574\u8f6c\u5f55\u8bed\u97f3\u6570\u636e\u91cf\u548c\u8bcd\u6027\u7c7b\u522b\u9879\u76ee\u6570\u91cf\uff0c\u6784\u5efa\u4e2a\u6027\u5316\u8bcd\u6c47\u914d\u7f6e\u6587\u4ef6\uff0c\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u8f83\u5c0f\u4e14\u7d27\u51d1\u7684\u914d\u7f6e\u6587\u4ef6\uff0810\u5206\u949f\u8f6c\u5f55\u8bed\u97f3\uff0c\u7279\u5b9a\u8bcd\u6027\u9879\u76ee\u6570\u91cf\uff09\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6784\u5efa\u7a33\u5b9a\u3001\u4e2a\u6027\u5316\u7684\u8bcd\u6c47\u914d\u7f6e\u6587\u4ef6\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u4e3a\u5bf9\u8bdd\u4ee3\u7406\u4e2d\u7684\u8bcd\u6c47\u5bf9\u9f50\u7b56\u7565\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.04343", "pdf": "https://arxiv.org/pdf/2509.04343", "abs": "https://arxiv.org/abs/2509.04343", "authors": ["Maciej Besta", "Shriram Chandran", "Robert Gerstenberger", "Mathis Lindner", "Marcin Chrapek", "Sebastian Hermann Martschat", "Taraneh Ghandi", "Patrick Iff", "Hubert Niewiadomski", "Piotr Nyczyk", "J\u00fcrgen M\u00fcller", "Torsten Hoefler"], "title": "Psychologically Enhanced AI Agents", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "comment": null, "summary": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning.", "AI": {"tldr": "MBTI-in-Thoughts\u6846\u67b6\u901a\u8fc7\u5fc3\u7406\u4eba\u683c\u6761\u4ef6\u63d0\u5347LLM\u4ee3\u7406\u7684\u6548\u80fd\uff0c\u5229\u7528MBTI\u7c7b\u578b\u5f15\u5bfc\u884c\u4e3a\uff0c\u5e76\u9a8c\u8bc1\u5176\u901a\u7528\u6027\u3002", "motivation": "\u7ed3\u5408\u5fc3\u7406\u5b66\u7406\u8bba\uff08\u5982MBTI\uff09\u589e\u5f3aLLM\u7684\u884c\u4e3a\u8bbe\u8ba1\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u4eba\u683c\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u4e3aLLM\u6ce8\u5165\u7279\u5b9a\u4eba\u683c\u539f\u578b\uff0c\u63a7\u5236\u8ba4\u77e5\u4e0e\u60c5\u611f\u884c\u4e3a\uff0c\u5e76\u96c6\u621016Personalities\u6d4b\u8bd5\u9a8c\u8bc1\u7279\u8d28\u6301\u7eed\u6027\u3002", "result": "\u4eba\u683c\u5f15\u5bfc\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u4ea7\u751f\u7a33\u5b9a\u884c\u4e3a\u504f\u5dee\uff0c\u5982\u60c5\u611f\u5316\u4ee3\u7406\u5728\u53d9\u4e8b\u751f\u6210\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5206\u6790\u6027\u4ee3\u7406\u5728\u535a\u5f08\u8bba\u4e2d\u7b56\u7565\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u6846\u67b6\u4e3a\u5fc3\u7406\u5b66\u589e\u5f3a\u7684AI\u4ee3\u7406\u5960\u5b9a\u57fa\u7840\uff0c\u4e14\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u5fc3\u7406\u6a21\u578b\uff08\u5982Big Five\uff09\u3002"}}
{"id": "2509.04404", "pdf": "https://arxiv.org/pdf/2509.04404", "abs": "https://arxiv.org/abs/2509.04404", "authors": ["Kyra Wilson", "Mattea Sim", "Anna-Maria Gueorguieva", "Aylin Caliskan"], "title": "No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in Resume Screening", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "K.4.2"], "comment": "Published in Proceedings of the 2025 AAAI/ACM Conference on AI,\n  Ethics, and Society; code available at\n  https://github.com/kyrawilson/No-Thoughts-Just-AI", "summary": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7b80\u5386\u7b5b\u9009\u5b9e\u9a8c\uff08N=528\uff09\u63a2\u8ba8\u4eba\u7c7b\u4e0e\u6a21\u62dfAI\u534f\u4f5c\u65f6\u7684\u79cd\u65cf\u504f\u89c1\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0cAI\u504f\u89c1\u4f1a\u663e\u8457\u6539\u53d8\u4eba\u7c7b\u51b3\u7b56\u884c\u4e3a\uff0c\u800c\u9690\u6027\u5173\u8054\u6d4b\u8bd5\uff08IAT\uff09\u53ef\u90e8\u5206\u7f13\u89e3\u504f\u89c1\u3002", "motivation": "\u65e8\u5728\u63ed\u793a\u4eba\u7c7b\u4e0eAI\u534f\u4f5c\u4e2d\u79cd\u65cf\u504f\u89c1\u7684\u4f20\u67d3\u6027\u53ca\u5176\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd\uff0c\u4e3aAI-\u4eba\u534f\u4f5c\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u76d1\u7ba1\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u4f7f\u7528\u6a21\u62dfAI\u6a21\u578b\uff08\u542b\u79cd\u65cf\u504f\u89c1\uff09\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7b5b\u9009\u7b80\u5386\uff0c\u5206\u67901,526\u79cd\u573a\u666f\u4e2d\u7684\u9009\u62e9\u504f\u597d\uff0c\u5e76\u7ed3\u5408IAT\u6d4b\u91cf\u9690\u6027\u79cd\u65cf-\u5730\u4f4d\u5173\u8054\u3002", "result": "\u4eba\u7c7b\u5728\u65e0\u504f\u89c1AI\u8f85\u52a9\u65f6\u516c\u5e73\u9009\u62e9\u5019\u9009\u4eba\uff0c\u4f46\u5728\u504f\u89c1AI\u5f71\u54cd\u4e0b\u503e\u5411\u4e8eAI\u504f\u597d\u7684\u7fa4\u4f53\uff08\u9ad8\u8fbe90%\uff09\u3002IAT\u53ef\u5c06\u53cd\u523b\u677f\u5370\u8c61\u9009\u62e9\u7387\u63d0\u534713%\u3002", "conclusion": "AI\u504f\u89c1\u53ef\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u51b3\u7b56\uff0c\u9700\u5728\u653f\u7b56\u3001\u6559\u80b2\u548c\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u91cd\u89c6AI-\u4eba\u534f\u4f5c\u7684\u590d\u6742\u6027\u53ca\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2509.04441", "pdf": "https://arxiv.org/pdf/2509.04441", "abs": "https://arxiv.org/abs/2509.04441", "authors": ["Hao-Shu Fang", "Branden Romero", "Yichen Xie", "Arthur Hu", "Bo-Ruei Huang", "Juan Alvarez", "Matthew Kim", "Gabriel Margolis", "Kavya Anbarasu", "Masayoshi Tomizuka", "Edward Adelson", "Pulkit Agrawal"], "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "comment": "project page: https://dex-op.github.io", "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86perioperation\u548cDEXOP\uff0c\u4e00\u79cd\u901a\u8fc7\u88ab\u52a8\u624b\u5916\u9aa8\u9abc\u6536\u96c6\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u7684\u65b0\u8303\u5f0f\uff0c\u65e8\u5728\u6700\u5927\u5316\u6570\u636e\u5bf9\u673a\u5668\u4eba\u7684\u53ef\u8f6c\u79fb\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u6536\u96c6\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4eba\u7c7b\u81ea\u7136\u7684\u64cd\u4f5c\u884c\u4e3a\u63d0\u9ad8\u6570\u636e\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86DEXOP\u88ab\u52a8\u624b\u5916\u9aa8\u9abc\uff0c\u901a\u8fc7\u529b\u53cd\u9988\u548c\u59ff\u6001\u955c\u50cf\u6280\u672f\uff0c\u6536\u96c6\u591a\u6837\u5316\u7684\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u6570\u636e\u3002", "result": "DEXOP\u80fd\u591f\u9ad8\u6548\u6536\u96c6\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u76f8\u6bd4\u8fdc\u7a0b\u64cd\u4f5c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "DEXOP\u662f\u63d0\u5347\u673a\u5668\u4eba\u7075\u5de7\u6027\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u7684\u6570\u636e\u6536\u96c6\u3002"}}
