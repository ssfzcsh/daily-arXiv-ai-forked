<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 11]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 22]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.ET](#cs.ET) [Total: 6]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CR](#cs.CR) [Total: 9]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CL](#cs.CL) [Total: 4]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 8]
- [stat.CO](#stat.CO) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification](https://arxiv.org/abs/2506.12084)
*Michele Alberti,François Bobot,Julien Girard-Satabin,Alban Grastien,Aymeric Varasse,Zakaria Chihani*

Main category: cs.SE

TL;DR: CAISAR是一个开源的机器学习规范与验证平台，旨在解决现有工具在处理复杂属性时的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习程序的规范与验证工具在处理复杂属性（如涉及多个神经网络的属性）时存在表达能力的局限性。

Method: 通过开发CAISAR平台及其规范语言，支持复杂属性的建模，并自动转化为现有证明器的查询。

Result: CAISAR能够高效处理复杂属性，展示了其在实际用例中的适用性。

Conclusion: CAISAR为机器学习规范与验证提供了更灵活的表达能力，填补了现有工具的不足。

Abstract: The formal specification and verification of machine learning programs saw
remarkable progress in less than a decade, leading to a profusion of tools.
However, diversity may lead to fragmentation, resulting in tools that are
difficult to compare, except for very specific benchmarks. Furthermore, this
progress is heavily geared towards the specification and verification of a
certain class of property, that is, local robustness properties. But while
provers are becoming more and more efficient at solving local robustness
properties, even slightly more complex properties, involving multiple neural
networks for example, cannot be expressed in the input languages of winners of
the International Competition of Verification of Neural Networks VNN-Comp. In
this tool paper, we present CAISAR, an open-source platform dedicated to
machine learning specification and verification. We present its specification
language, suitable for modelling complex properties on neural networks, support
vector machines and boosted trees. We show on concrete use-cases how
specifications written in this language are automatically translated to queries
to state-of-the-art provers, notably by using automated graph editing
techniques, making it possible to use their off-the-shelf versions. The
artifact to reproduce the paper claims is available at the following DOI:
https://doi.org/10.5281/zenodo.15209510

</details>


### [2] [Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data](https://arxiv.org/abs/2506.12111)
*Oscar Boullosa Dapena*

Main category: cs.SE

TL;DR: 提出了一种新型架构QIDINNs，通过积分历史数据实现更稳定、可解释的流数据学习。


<details>
  <summary>Details</summary>
Motivation: 解决传统梯度方法（如BPTT）在处理流数据时的计算和稳定性问题。

Method: 基于费曼积分技术，将神经更新表示为历史数据的积分，提出QIDINNs架构。

Result: 在合成和实际流任务中验证了模型的有效性，并提出了量子扩展方向。

Conclusion: QIDINNs为混合经典-量子神经网络计算提供了新途径。

Abstract: Real-time continuous learning over streaming data remains a central challenge
in deep learning and AI systems. Traditional gradient-based models such as
backpropagation through time (BPTT) face computational and stability
limitations when dealing with temporally unbounded data. In this paper, we
introduce a novel architecture, Quantum-Inspired Differentiable Integral Neural
Networks (QIDINNs), which leverages the Feynman technique of differentiation
under the integral sign to formulate neural updates as integrals over
historical data. This reformulation allows for smoother, more stable learning
dynamics that are both physically interpretable and computationally tractable.
Inspired by Feynman's path integral formalism and compatible with quantum
gradient estimation frameworks, QIDINNs open a path toward hybrid
classical-quantum neural computation. We demonstrate our model's effectiveness
on synthetic and real-world streaming tasks, and we propose directions for
quantum extensions and scalable implementations.

</details>


### [3] [Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure](https://arxiv.org/abs/2506.12278)
*Zheyuan Yang,Zexi Kuang,Xue Xia,Yilun Zhao*

Main category: cs.SE

TL;DR: TestCase-Eval 是一个新的基准测试，用于系统评估 LLM 在测试用例生成中的表现，包含 500 个算法问题和 10 万个人工编写的解决方案，重点关注故障覆盖和故障暴露两个任务。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对 LLM 在测试用例生成中表现的全面评估，TestCase-Eval 旨在填补这一空白，帮助理解 LLM 的局限性。

Method: 使用来自 Codeforces 平台的 500 个算法问题和 10 万个人工解决方案，评估 19 种开源和专有 LLM 在故障覆盖和故障暴露任务中的表现。

Result: 提供了对 19 种 LLM 的综合评估，揭示了它们在生成有效测试用例时的优缺点。

Conclusion: TestCase-Eval 为 LLM 在测试用例生成中的能力提供了系统性的评估框架，有助于未来研究和改进。

Abstract: We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs
in test-case generation. TestCase-Eval includes 500 algorithm problems and
100,000 human-crafted solutions from the Codeforces platform. It focuses on two
pivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test
sets probe diverse input scenarios and cover a wide range of potential failure
modes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored
test input that reveals a specific incorrect code implementation. We provide a
comprehensive assessment of 19 state-of-the-art open-source and proprietary
LLMs on TestCase-Eval, offering insights into their strengths and limitations
in generating effective test cases for algorithm problems.

</details>


### [4] [The Foundation Cracks: A Comprehensive Study on Bugs and Testing Practices in LLM Libraries](https://arxiv.org/abs/2506.12320)
*Weipeng Jiang,Xiaoyu Zhang,Xiaofei Xie,Jiongchi Yu,Yuhan Zhi,Shiqing Ma,Chao Shen*

Main category: cs.SE

TL;DR: 该论文首次对现代LLM库中的错误特征和测试实践进行了全面的实证研究，分析了313个错误修复提交，发现API滥用是主要问题，并提出了测试改进建议。


<details>
  <summary>Details</summary>
Motivation: LLM库作为AI革命的基础设施，其质量问题严重影响了依赖它们的AI系统的可靠性。目前缺乏对其错误特征和测试实践的全面研究。

Method: 研究通过手动分析313个错误修复提交，涵盖HuggingFace Transformers和vLLM两个主流LLM库，建立错误分类法，并审查7,748个测试函数。

Result: API滥用是最主要的错误根源（32.17%-48.19%）。现有测试主要缺陷为测试用例不足（41.73%）、缺乏测试驱动（32.37%）和弱测试预言（25.90%）。

Conclusion: 建议改进测试实践以提升LLM库质量，重点关注接口问题和测试覆盖。

Abstract: Large Language Model (LLM) libraries have emerged as the foundational
infrastructure powering today's AI revolution, serving as the backbone for LLM
deployment, inference optimization, fine-tuning, and production serving across
diverse applications. Despite their critical role in the LLM ecosystem, these
libraries face frequent quality issues and bugs that threaten the reliability
of AI systems built upon them. To address this knowledge gap, we present the
first comprehensive empirical investigation into bug characteristics and
testing practices in modern LLM libraries. We examine 313 bug-fixing commits
extracted across two widely-adopted LLM libraries: HuggingFace Transformers and
vLLM.Through rigorous manual analysis, we establish comprehensive taxonomies
categorizing bug symptoms into 5 types and root causes into 14 distinct
categories.Our primary discovery shows that API misuse has emerged as the
predominant root cause (32.17%-48.19%), representing a notable transition from
algorithm-focused defects in conventional deep learning frameworks toward
interface-oriented problems. Additionally, we examine 7,748 test functions to
identify 7 distinct test oracle categories employed in current testing
approaches, with predefined expected outputs (such as specific tensors and text
strings) being the most common strategy. Our assessment of existing testing
effectiveness demonstrates that the majority of bugs escape detection due to
inadequate test cases (41.73%), lack of test drivers (32.37%), and weak test
oracles (25.90%). Drawing from these findings, we offer some recommendations
for enhancing LLM library quality assurance.

</details>


### [5] [How Developers Use AI Agents: When They Work, When They Don't, and Why](https://arxiv.org/abs/2506.12347)
*Aayush Kumar,Yasharth Bajpai,Sumit Gulwani,Gustavo Soares,Emerson Murphy-Hill*

Main category: cs.SE

TL;DR: 研究探讨开发者与SWE代理在协作中的互动模式及挑战，发现逐步解决问题和积极迭代的协作方式更有效，但信任和测试调试仍是难点。


<details>
  <summary>Details</summary>
Motivation: 理解开发者如何与SWE代理协作，以及互动中出现的沟通问题，以优化代理设计。

Method: 观察19名开发者使用IDE内代理解决33个开源仓库问题，分析协作模式与成功因素。

Result: 逐步解决问题的开发者成功率更高；主动迭代代理输出的团队更成功，但信任和调试仍是挑战。

Conclusion: 研究结果为优化SWE代理设计及改进开发者-代理协作模式提供了依据。

Abstract: Software Engineering Agents (SWE agents) can autonomously perform development
tasks on benchmarks like SWE Bench, but still face challenges when tackling
complex and ambiguous real-world tasks. Consequently, SWE agents are often
designed to allow interactivity with developers, enabling collaborative
problem-solving. To understand how developers collaborate with SWE agents and
the communication challenges that arise in such interactions, we observed 19
developers using an in-IDE agent to resolve 33 open issues in repositories to
which they had previously contributed. Participants successfully resolved about
half of these issues, with participants solving issues incrementally having
greater success than those using a one-shot approach. Participants who actively
collaborated with the agent and iterated on its outputs were also more
successful, though they faced challenges in trusting the agent's responses and
collaborating on debugging and testing. These results have implications for
successful developer-agent collaborations, and for the design of more effective
SWE agents.

</details>


### [6] [A Mapping Study About Training in Industry Context in Software Engineering](https://arxiv.org/abs/2506.12590)
*Breno Alves de Andrade,Rodrigo Siqueira,Lidiane Gomes,Antonio Oliveira,Danilo Monteiro Ribeiro*

Main category: cs.SE

TL;DR: 该研究通过系统映射分析了软件工程领域的企业培训研究现状，发现研究集中在培训方法和策略上，其他领域存在明显空白。


<details>
  <summary>Details</summary>
Motivation: 探究软件工程行业中企业培训的设计、实施和评估的系统化理解不足。

Method: 采用系统映射研究，分析了26篇相关文献，并以Salas的培训框架为分析工具进行分类。

Result: 研究发现多数研究集中于培训方法和策略，而在工作/任务分析及模拟培训等领域存在显著空缺。

Conclusion: 研究提供了软件工程企业培训的结构化概览，指出了未充分探索的领域，并提出了未来研究方向。

Abstract: Context: Corporate training plays a strategic role in the continuous
development of professionals in the software engineering industry. However,
there is a lack of systematized understanding of how training initiatives are
designed, implemented, and evaluated within this domain.
  Objective: This study aims to map the current state of research on corporate
training in software engineering in industry settings, using Eduardo Salas'
training framework as an analytical lens.
  Method: A systematic mapping study was conducted involving the selection and
analysis of 26 primary studies published in the field. Each study was
categorized according to Salas' four key areas: Training Needs Analysis,
Antecedent Training Conditions, Training Methods and Instructional Strategies,
and Post-Training Conditions.
  Results: The findings show a predominance of studies focusing on Training
Methods and Instructional Strategies. Significant gaps were identified in other
areas, particularly regarding Job/Task Analysis and Simulation-based Training
and Games. Most studies were experience reports, lacking methodological rigor
and longitudinal assessment.
  Conclusions: The study offers a structured overview of how corporate training
is approached in software engineering, revealing underexplored areas and
proposing directions for future research. It contributes to both academic and
practical communities by highlighting challenges, methodological trends, and
opportunities for designing more effective training programs in industry.

</details>


### [7] [Real-Time Agile Software Management for Edge and Fog Computing Based Smart City Infrastructure](https://arxiv.org/abs/2506.12616)
*Debasish Jana,Pinakpani Pal,Pawan Kumar*

Main category: cs.SE

TL;DR: 论文提出了ROOF框架，通过去中心化计算和边缘网络分层降低延迟，提高能效，增强安全性，并在实际案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统云系统在带宽、延迟和能源方面的限制，满足智能城市对实时数据处理的需求。

Method: 采用ROOF框架，结合雾计算和边缘网络分层，利用AI驱动的资源分配和安全技术（如TLS加密和区块链认证）。

Result: 案例研究表明，ROOF在交通系统和环境监测中显著降低了延迟并提高了能效。

Conclusion: 论文总结了AI驱动分析在智能城市基础设施中的挑战和前景。

Abstract: The evolution of smart cities demands scalable, secure, and energy-efficient
architectures for real-time data processing. With the number of IoT devices
expected to exceed 40 billion by 2030, traditional cloud-based systems are
increasingly constrained by bandwidth, latency, and energy limitations. This
paper leverages the ROOF (Real-time Onsite Operations Facilitation) framework
with decentralized computing at intermediary fog and peripheral edge network
layers to reduce latency by processing data near its point of origin. ROOF
features fog caching to avoid redundancy, ultra-low-power wireless transmission
for energy savings, and AI-driven resource allocation for efficiency. Security
is enhanced through TLS encryption, blockchain-based authentication, and
edge-level access control. Case studies from Bhubaneswar, Barcelona and
Copenhagen validate the use of ROOF in traffic systems and environmental
monitoring. The paper concludes by outlining key challenges and prospects of
AI-driven analytics in smart urban infrastructure.

</details>


### [8] [Social Media Reactions to Open Source Promotions: AI-Powered GitHub Projects on Hacker News](https://arxiv.org/abs/2506.12643)
*Prachnachai Meakpaiboonwattana,Warittha Tarntong,Thai Mekratanavorakul,Chaiyong Ragkhitwetsagul,Pattaraporn Sangaroonsilp,Raula Kula,Morakot Choetkiertikul,Kenichi Matsumoto,Thanwadee Sunetnanta*

Main category: cs.SE

TL;DR: 研究探讨了Hacker News对GitHub上AI项目开发者活动的影响，发现通过该平台推广能显著增加项目的关注度和贡献者。


<details>
  <summary>Details</summary>
Motivation: 探索社交新闻平台Hacker News如何影响开源AI项目的开发者活动，以验证其作为推广工具的潜力。

Method: 分析2,195篇Hacker News故事及相关评论，并跟踪1,814个GitHub仓库在被分享后的活动变化。

Result: 19%的AI开发者在Hacker News上推广项目，并观察到项目分叉、点赞和贡献者显著增加。

Conclusion: Hacker News是推广开源AI项目的有效平台，能提升项目可见性和社区参与度。

Abstract: Social media platforms have become more influential than traditional news
sources, shaping public discourse and accelerating the spread of information.
With the rapid advancement of artificial intelligence (AI), open-source
software (OSS) projects can leverage these platforms to gain visibility and
attract contributors. In this study, we investigate the relationship between
Hacker News, a social news site focused on computer science and
entrepreneurship, and the extent to which it influences developer activity on
the promoted GitHub AI projects.
  We analyzed 2,195 Hacker News (HN) stories and their corresponding comments
over a two-year period. Our findings reveal that at least 19\% of AI developers
promoted their GitHub projects on Hacker News, often receiving positive
engagement from the community. By tracking activity on the associated 1,814
GitHub repositories after they were shared on Hacker News, we observed a
significant increase in forks, stars, and contributors. These results suggest
that Hacker News serves as a viable platform for AI-powered OSS projects, with
the potential to gain attention, foster community engagement, and accelerate
software development.

</details>


### [9] [Towards Lean Research Inception: Assessing Practical Relevance of Formulated Research Problems](https://arxiv.org/abs/2506.12669)
*Anrafel Fernandes Pereira,Marcos Kalinowski,Maria Teresa Baldassarre,Jürgen Börstler,Nauman bin Ali,Daniel Mendez*

Main category: cs.SE

TL;DR: 论文提出Lean Research Inception (LRI)框架，旨在评估软件工程(SE)研究的实用性问题，并通过研讨会验证其三个评估标准（有价值、可行、适用）的重要性。


<details>
  <summary>Details</summary>
Motivation: 许多SE研究缺乏实际相关性，原因是工业实践的简化视角、产学研联系薄弱及研究问题定义不清。LRI框架旨在解决这些问题。

Method: 通过研讨会应用LRI框架，参与者使用语义差异量表评估研究问题，反馈标准的完整性和重要性。

Result: 参与者一致认为三个标准对工业需求至关重要（有价值83.3%，可行76.2%，适用73.8%），并建议术语调整。

Conclusion: LRI框架仍需进一步评估，但当前结果证明其能有效帮助社区评估SE研究的实用性。

Abstract: [Context] The lack of practical relevance in many Software Engineering (SE)
research contributions is often rooted in oversimplified views of industrial
practice, weak industry connections, and poorly defined research problems.
Clear criteria for evaluating SE research problems can help align their value,
feasibility, and applicability with industrial needs. [Goal] In this paper, we
introduce the Lean Research Inception (LRI) framework, designed to support the
formulation and assessment of practically relevant research problems in SE. We
describe its initial evaluation strategy conducted in a workshop with a network
of SE researchers experienced in industry-academia collaboration and report the
evaluation of its three assessment criteria (valuable, feasible, and
applicable) regarding their importance in assessing practical relevance.
[Method] We applied LRI retroactively to a published research paper, engaging
workshop participants in discussing and assessing the research problem by
applying the proposed criteria using a semantic differential scale.
Participants provided feedback on the criteria's importance and completeness,
drawn from their own experiences in industry-academia collaboration. [Results]
The findings reveal an overall agreement on the importance of the three
criteria - valuable (83.3%), feasible (76.2%), and applicable (73.8%) - for
aligning research problems with industrial needs. Qualitative feedback
suggested adjustments in terminology with a clearer distinction between
feasible and applicable, and refinements for valuable by more clearly
considering business value, ROI, and originality. [Conclusion] While LRI
constitutes ongoing research and requires further evaluation, our results
strengthen our confidence that the three criteria applied using the semantic
differential scale can already help the community assess the practical
relevance of SE research problems.

</details>


### [10] [Get on the Train or be Left on the Station: Using LLMs for Software Engineering Research](https://arxiv.org/abs/2506.12691)
*Bianca Trinkenreich,Fabio Calefato,Geir Hanssen,Kelly Blincoe,Marcos Kalinowski,Mauro Pezzè,Paolo Tell,Margaret-Anne Storey*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）对软件工程（SE）研究的影响，强调需要以人为中心的角度来整合LLMs，以保障科学严谨性和伦理责任。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析LLMs如何改变SE研究实践，并呼吁学术界主动引导这一变革，确保人类在其中的主导地位。

Method: 研究方法是通过McLuhan的媒体四元律理论框架，分析LLMs对SE研究的增强、淘汰、重现和逆转效应。

Result: 研究结果显示，LLMs能加速创新但也带来传统方法淘汰和极端风险，需要框架引导以扬长避短。

Conclusion: 结论呼吁SE研究社区主动利用LLMs优势，同时制定指导框架以降低风险，确保AI增强未来的研究质量。

Abstract: The adoption of Large Language Models (LLMs) is not only transforming
software engineering (SE) practice but is also poised to fundamentally disrupt
how research is conducted in the field. While perspectives on this
transformation range from viewing LLMs as mere productivity tools to
considering them revolutionary forces, we argue that the SE research community
must proactively engage with and shape the integration of LLMs into research
practices, emphasizing human agency in this transformation. As LLMs rapidly
become integral to SE research - both as tools that support investigations and
as subjects of study - a human-centric perspective is essential. Ensuring human
oversight and interpretability is necessary for upholding scientific rigor,
fostering ethical responsibility, and driving advancements in the field.
Drawing from discussions at the 2nd Copenhagen Symposium on Human-Centered AI
in SE, this position paper employs McLuhan's Tetrad of Media Laws to analyze
the impact of LLMs on SE research. Through this theoretical lens, we examine
how LLMs enhance research capabilities through accelerated ideation and
automated processes, make some traditional research practices obsolete,
retrieve valuable aspects of historical research approaches, and risk reversal
effects when taken to extremes. Our analysis reveals opportunities for
innovation and potential pitfalls that require careful consideration. We
conclude with a call to action for the SE research community to proactively
harness the benefits of LLMs while developing frameworks and guidelines to
mitigate their risks, to ensure continued rigor and impact of research in an
AI-augmented future.

</details>


### [11] [Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?](https://arxiv.org/abs/2506.12713)
*Xiangyang Li,Xiaopeng Li,Kuicai Dong,Quanhu Zhang,Rongju Ruan,Xinyi Dai,Xiaoshuang Liu,Shengchun Xu,Yasheng Wang,Ruiming Tang*

Main category: cs.SE

TL;DR: 论文介绍了HLCE基准测试，包含235个高难度编程问题，用于评估LLMs的高级推理和代码生成能力。最先进的LLMs在该测试中表现不佳，且自我认知能力与代码生成能力不相关。


<details>
  <summary>Details</summary>
Motivation: 当前主流的代码生成基准测试对高级LLMs挑战不足，无法充分评估其推理能力，因此需要更难的测试集。

Method: 设计了HLCE基准测试，包含ICPC和IOI的235个高难度问题，并开发了可重现的在线-离线沙盒评估环境。

Result: 最强LLMs的通过率仅15.9%和11.4%，且自我认知能力与代码生成能力无显著相关性。

Conclusion: HLCE将成为代码生成的里程碑挑战，推动高性能推理和人-AI协作编程的发展。

Abstract: Code generation is a core capability of large language models (LLMs), yet
mainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with
medium-level difficulty and pose no challenge to advanced LLMs. To better
reflected the advanced reasoning and code generation ability, We introduce
Humanity's Last Code Exam (HLCE), comprising 235 most challenging problems from
the International Collegiate Programming Contest (ICPC World Finals) and the
International Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of
HLCE, we design a harmonized online-offline sandbox that guarantees fully
reproducible evaluation. Through our comprehensive evaluation, we observe that
even the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve
pass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a
novel "self-recognition" task to measure LLMs' awareness of their own
capabilities. Results indicate that LLMs' self-recognition abilities are not
proportionally correlated with their code generation performance. Finally, our
empirical validation of test-time scaling laws reveals that current advanced
LLMs have substantial room for improvement on complex programming tasks. We
expect HLCE to become a milestone challenge for code generation and to catalyze
advances in high-performance reasoning and human-AI collaborative programming.
Our code and dataset are also public
available(https://github.com/Humanity-s-Last-Code-Exam/HLCE).

</details>


### [12] [MCTS-Refined CoT: High-Quality Fine-Tuning Data for LLM-Based Repository Issue Resolution](https://arxiv.org/abs/2506.12728)
*Yibo Wang,Zhihao Peng,Ying Wang,Zhao Wei,Hai Yu,Zhiliang Zhu*

Main category: cs.SE

TL;DR: 论文提出MCTS-REFINE算法，通过动态验证和优化推理步骤生成高质量CoT数据，显著提升LLM在代码问题解决中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成和问题解决中表现优异，但现有方法因数据质量差和推理步骤验证不足而受限，亟需改进。

Method: 采用MCTS-REFINE算法，结合反射机制和严格采样协议，将问题分解为三个子任务并确保中间步骤正确性。

Result: 实验显示，使用MCTS-REFINE生成的CoT数据微调的模型在SWE-bench任务中表现优于基线，性能显著提升。

Conclusion: MCTS-REFINE通过提升数据质量和推理验证，有效解决了LLMs在复杂任务中的表现问题，具有广泛应用前景。

Abstract: LLMs demonstrate strong performance in auto-mated software engineering,
particularly for code generation and issue resolution. While proprietary models
like GPT-4o achieve high benchmarks scores on SWE-bench, their API dependence,
cost, and privacy concerns limit adoption. Open-source alternatives offer
transparency but underperform in complex tasks, especially sub-100B parameter
models. Although quality Chain-of-Thought (CoT) data can enhance reasoning,
current methods face two critical flaws: (1) weak rejection sampling reduces
data quality, and (2) inadequate step validation causes error accumulation.
These limitations lead to flawed reasoning chains that impair LLMs'ability to
learn reliable issue resolution. The paper proposes MCTS-REFINE, an enhanced
Monte Carlo Tree Search (MCTS)-based algorithm that dynamically validates and
optimizes intermediate reasoning steps through a rigorous rejection sampling
strategy, generating high-quality CoT data to improve LLM performance in issue
resolution tasks. Key innovations include: (1) augmenting MCTS with a
reflection mechanism that corrects errors via rejection sampling and
refinement, (2) decomposing issue resolution into three subtasks-File
Localization, Fault Localization, and Patch Generation-each with clear
ground-truth criteria, and (3) enforcing a strict sampling protocol where
intermediate outputs must exactly match verified developer patches, ensuring
correctness across reasoning paths. Experiments on SWE-bench Lite and SWE-bench
Verified demonstrate that LLMs fine-tuned with our CoT dataset achieve
substantial improvements over baselines.Notably, Qwen2.5-72B- Instruct achieves
28.3%(Lite) and 35.0%(Verified) resolution rates, surpassing SOTA baseline
SWE-Fixer-Qwen-72B with the same parameter scale, which only reached
24.7%(Lite) and 32.8%(Verified).

</details>


### [13] [IDOL: Improved Different Optimization Levels Testing for Solidity Compilers](https://arxiv.org/abs/2506.12760)
*Lantian Li,Yejian Liang,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本文提出了一种名为IDOL的创新方法，用于测试Solidity编译器，通过反向优化转换生成语义等效的智能合约变体，以触发编译器优化逻辑，并已发现3个编译器优化漏洞。


<details>
  <summary>Details</summary>
Motivation: 智能合约一旦部署便无法修改，其漏洞可能导致重大财务或法律问题，而编译器直接影响智能合约的质量和安全性。

Method: 提出IDOL方法，通过反向优化转换生成语义等效的智能合约变体，以测试编译器优化逻辑。

Result: 初步评估中，发现了3个编译器优化漏洞。

Conclusion: IDOL方法能有效检测Solidity编译器中的优化漏洞，提升智能合约的安全性。

Abstract: As blockchain technology continues to evolve and mature, smart contracts have
become a key driving force behind the digitization and automation of
transactions. Smart contracts greatly simplify and refine the traditional
business transaction processes, and thus have had a profound impact on various
industries such as finance and supply chain management. However, because smart
contracts cannot be modified once deployed, any vulnerabilities or design flaws
within the contract cannot be easily fixed, potentially leading to significant
financial losses or even legal issues. The compiler, as a critical component in
the development process, directly affects the quality and security of smart
contracts. This paper innovatively proposes a method, known as the Improved
Different Optimization Levels (IDOL), for testing the Solidity compiler. The
key idea behind IDOL is to perform reverse optimization transformations (i.e.,
change optimized form into unoptimized form) to generate semantically
equivalent variants of the smart contracts under test, aiming to maximize the
opportunities to trigger the optimization logic of compilers. We conducted a
preliminary evaluation of IDOL and three confirmed compiler optimization bugs
have been uncovered at the time of writing.

</details>


### [14] [Towards Operation Proof Obligation Generation for VDM](https://arxiv.org/abs/2506.12858)
*Nick Battle,Peter Gorm Larsen*

Main category: cs.SE

TL;DR: 论文探讨了形式化方法工具中证明义务生成的现状，尤其是对显式操作体的支持不足，并展示了当前进展和未来工作。


<details>
  <summary>Details</summary>
Motivation: 为了解决形式化方法工具在生成显式操作体的证明义务方面的局限。

Method: 分析了现有工具的局限性，并展示了当前进展。

Result: 提出了当前工具的能力和未来需要改进的方向。

Conclusion: 需要进一步工作以完善显式操作体的证明义务生成能力。

Abstract: All formalisms have the ability to ensure that their models are internally
consistent. Potential inconsistencies are generally highlighted by assertions
called proof obligations, and the generation of these obligations is an
important role of the tools that support the method. This capability has been
available for VDM tools for many years. However, support for obligation
generation for explicit operation bodies has always been limited. This work
describes the current state of work to address this, showing the capabilities
so far and highlighting the work remaining.

</details>


### [15] [Designing Deep Learning Frameworks for LLMs:Challenges, Expectations, and Opportunities](https://arxiv.org/abs/2506.13114)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Jiacong Wu,An Guo,Jiawei Shen,Bingzhuo Li,Zhenyu Chen*

Main category: cs.SE

TL;DR: 研究探讨了深度学习框架在支持大型语言模型时面临的挑战，通过分析问题报告和访谈开发者提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的发展依赖高效的深度学习框架，但其规模和执行周期对框架的稳定性、功能和调试能力提出了极高要求，但目前缺乏对这些挑战的系统研究。

Method: 通过对三大深度学习框架和八个相关工具包的问题报告进行大规模分析，并结合用户和开发者的访谈，构建了一个分类体系。

Result: 研究提出了包含问题、需求和错误的详尽分类体系，并通过实践者反馈评估了其重要性，最终给出五项改进建议。

Conclusion: 当前深度学习框架存在显著不足，研究结果为提升其可靠性和用户体验提供了具体指导，以支持下一代大型语言模型的开发与应用。

Abstract: Large language models (LLMs) drive significant advancements in real industry
applications. LLMs rely on DL frameworks for efficient model construction,
distributed execution, and optimized deployment. Their large parameter scale
and long execution cycles place extreme demands on DL frameworks in terms of
scalability, stability, and efficiency. Therefore, poor usability, limited
functionality, and subtle bugs in DL frameworks may hinder development
efficiency and cause severe failures or resource waste. However, a fundamental
question remains underinvestigated, i.e., What challenges do DL frameworks face
in supporting LLMs? To seek an answer, we investigate these challenges through
a large-scale analysis of issue reports from three major DL frameworks
(MindSpore, PyTorch, TensorFlow) and eight associated LLM toolkits (e.g.,
Megatron). We construct a taxonomy of LLM-centric bugs, requirements, and user
questions and enrich it through interviews with 11 LLM users and eight DL
framework developers, uncovering key technical challenges and misalignments
between user needs and developer priorities. Our contributions are threefold:
(1) we develop a comprehensive taxonomy comprising four question themes (nine
sub-themes), four requirement themes (15 sub-themes), and ten bug themes (45
sub-themes); (2) we assess the perceived importance and priority of these
challenges based on practitioner insights; and (3) we identify five key
findings across the LLM development and propose five actionable recommendations
to improve the reliability, usability, and testability of DL frameworks. Our
results highlight critical limitations in current DL frameworks and offer
concrete guidance for advancing their support for the next generation of LLM
construction and applications.

</details>


### [16] [Querying Large Automotive Software Models: Agentic vs. Direct LLM Approaches](https://arxiv.org/abs/2506.13171)
*Lukasz Mazur,Nenad Petrovic,James Pontes Miranda,Ansgar Radermacher,Robert Rasche,Alois Knoll*

Main category: cs.SE

TL;DR: 研究探索了两种利用大语言模型（LLM）回答软件模型问题的方法：直接提示和基于代理的方法，发现代理方法在保持准确性的同时显著减少令牌使用，更适合汽车行业的大型模型。


<details>
  <summary>Details</summary>
Motivation: 大型软件模型难以完全理解，传统的交互和分析方法面临挑战，LLM为通过自然语言与软件模型交互提供了新机会。

Method: 比较了两种方法：直接提示（将整个模型放入上下文）和基于LLM代理的工具访问方法，使用Ecore元模型进行评估。

Result: 代理方法在准确性上与直接提示相当，但令牌使用效率更高，特别适合汽车行业的大型模型。

Conclusion: LLM代理是处理大型软件模型的唯一可行解决方案，未来将探索更多复杂代理架构和模型修改功能。

Abstract: Large language models (LLMs) offer new opportunities for interacting with
complex software artifacts, such as software models, through natural language.
They present especially promising benefits for large software models that are
difficult to grasp in their entirety, making traditional interaction and
analysis approaches challenging. This paper investigates two approaches for
leveraging LLMs to answer questions over software models: direct prompting,
where the whole software model is provided in the context, and an agentic
approach combining LLM-based agents with general-purpose file access tools. We
evaluate these approaches using an Ecore metamodel designed for timing analysis
and software optimization in automotive and embedded domains. Our findings show
that while the agentic approach achieves accuracy comparable to direct
prompting, it is significantly more efficient in terms of token usage. This
efficiency makes the agentic approach particularly suitable for the automotive
industry, where the large size of software models makes direct prompting
infeasible, establishing LLM agents as not just a practical alternative but the
only viable solution. Notably, the evaluation was conducted using small LLMs,
which are more feasible to be executed locally - an essential advantage for
meeting strict requirements around privacy, intellectual property protection,
and regulatory compliance. Future work will investigate software models in
diverse formats, explore more complex agent architectures, and extend agentic
workflows to support not only querying but also modification of software
models.

</details>


### [17] [From Empirical Evaluation to Context-Aware Enhancement: Repairing Regression Errors with LLMs](https://arxiv.org/abs/2506.13182)
*Anh Ho,Thanh Le-Cong,Bach Le,Christine Rizkallah*

Main category: cs.SE

TL;DR: 该论文通过实证研究评估现代自动程序修复（APR）技术在修复回归Bug中的效果，并引入RegMiner4APR基准。研究发现，传统APR工具效果不佳，而基于大语言模型（LLM）的APR表现出潜力，结合Bug诱导信息后可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 回归Bug修复的研究尚未充分探索现代APR技术的效果，尤其是基于LLM的方法。论文旨在填补这一空白。

Method: 构建RegMiner4APR基准，包含99个Java回归Bug，评估传统APR工具和LLM-based APR的性能，并研究结合Bug诱导信息的效果。

Result: 传统APR工具无法修复任何Bug，而LLM-based APR表现良好；结合Bug诱导信息后，成功率提高1.8倍。

Conclusion: LLM-based APR在回归Bug修复中具有潜力，结合Bug诱导信息可显著提升其性能。

Abstract: [...] Since then, various APR approaches, especially those leveraging the
power of large language models (LLMs), have been rapidly developed to fix
general software bugs. Unfortunately, the effectiveness of these advanced
techniques in the context of regression bugs remains largely unexplored. This
gap motivates the need for an empirical study evaluating the effectiveness of
modern APR techniques in fixing real-world regression bugs.
  In this work, we conduct an empirical study of APR techniques on Java
regression bugs. To facilitate our study, we introduce RegMiner4APR, a
high-quality benchmark of Java regression bugs integrated into a framework
designed to facilitate APR research. The current benchmark includes 99
regression bugs collected from 32 widely used real-world Java GitHub
repositories. We begin by conducting an in-depth analysis of the benchmark,
demonstrating its diversity and quality. Building on this foundation, we
empirically evaluate the capabilities of APR to regression bugs by assessing
both traditional APR tools and advanced LLM-based APR approaches. Our
experimental results show that classical APR tools fail to repair any bugs,
while LLM-based APR approaches exhibit promising potential. Motivated by these
results, we investigate impact of incorporating bug-inducing change information
into LLM-based APR approaches for fixing regression bugs. Our results highlight
that this context-aware enhancement significantly improves the performance of
LLM-based APR, yielding 1.8x more successful repairs compared to using
LLM-based APR without such context.

</details>


### [18] [Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers](https://arxiv.org/abs/2506.13538)
*Mohammed Mehedi Hasan,Hao Li,Emad Fallahzadeh,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该论文研究了Model Context Protocol (MCP)在健康、安全和可维护性方面的风险，首次进行了大规模实证分析。


<details>
  <summary>Details</summary>
Motivation: 尽管MCP已成为标准工具生态系统，但其非确定性控制流带来了新的可持续性、安全性和可维护性风险，需深入研究。

Method: 使用先进的健康指标和混合分析管道，结合通用静态分析工具和MCP专用扫描器，评估了1,899个开源MCP服务器。

Result: 尽管MCP服务器健康指标良好，但仍识别出8种独特漏洞，其中7.2%存在通用漏洞，5.5%存在MCP专用工具中毒问题；66%存在代码异味，14.4%包含10种已知错误模式。

Conclusion: 研究强调了MCP专用漏洞检测技术的必要性，同时肯定了传统分析和重构实践的价值。

Abstract: Although Foundation Models (FMs), such as GPT-4, are increasingly used in
domains like finance and software engineering, reliance on textual interfaces
limits these models' real-world interaction. To address this, FM providers
introduced tool calling-triggering a proliferation of frameworks with distinct
tool interfaces. In late 2024, Anthropic introduced the Model Context Protocol
(MCP) to standardize this tool ecosystem, which has become the de facto
standard with over eight million weekly SDK downloads. Despite its adoption,
MCP's AI-driven, non-deterministic control flow introduces new risks to
sustainability, security, and maintainability, warranting closer examination.
  Towards this end, we present the first large-scale empirical study of MCP.
Using state-of-the-art health metrics and a hybrid analysis pipeline, combining
a general-purpose static analysis tool with an MCP-specific scanner, we
evaluate 1,899 open-source MCP servers to assess their health, security, and
maintainability. Despite MCP servers demonstrating strong health metrics, we
identify eight distinct vulnerabilities-only three overlapping with traditional
software vulnerabilities. Additionally, 7.2% of servers contain general
vulnerabilities and 5.5% exhibit MCP-specific tool poisoning. Regarding
maintainability, while 66% exhibit code smells, 14.4% contain ten bug patterns
overlapping prior research. These findings highlight the need for MCP-specific
vulnerability detection techniques while reaffirming the value of traditional
analysis and refactoring practices.

</details>


### [19] [Empirical Evaluation of Large Language Models in Automated Program Repair](https://arxiv.org/abs/2506.13186)
*Jiajun Sun,Fengjie Li,Xinzhu Qi,Hongyu Zhang,Jiajun Jiang*

Main category: cs.SE

TL;DR: 研究了大型语言模型在自动程序修复中的表现，发现在多语言、多场景下模型专一性和提示策略对修复效果影响显著。


<details>
  <summary>Details</summary>
Motivation: 探索现代大型语言模型在多样编程语言和场景中的自动程序修复能力。

Method: 对四个开源LLM（CodeLlama、LLaMA、StarCoder、DeepSeek-Coder）进行实证研究，覆盖不同参数规模、架构和用途，评估其在两种错误场景和三种语言中的表现。

Result: 发现模型专一性优于通用性，修复效果与模型大小不完全正相关，且正确的补丁常早期生成；提示策略对结果影响显著。

Conclusion: 研究为设计高效基于LLM的自动程序修复系统提供了实用指导。

Abstract: The increasing prevalence of software bugs has made automated program repair
(APR) a key research focus. Large language models (LLMs) offer new
opportunities for APR, but existing studies mostly rely on smaller,
earlier-generation models and Java benchmarks. The repair capabilities of
modern, large-scale LLMs across diverse languages and scenarios remain
underexplored. To address this, we conduct a comprehensive empirical study of
four open-source LLMs, CodeLlama, LLaMA, StarCoder, and DeepSeek-Coder,
spanning 7B to 33B parameters, diverse architectures, and purposes. We evaluate
them across two bug scenarios (enterprise-grades and algorithmic), three
languages (Java, C/C++, Python), and four prompting strategies, analyzing over
600K generated patches on six benchmarks. Key findings include: (1) model
specialization (e.g., CodeLlama) can outperform larger general-purpose models
(e.g., LLaMA); (2) repair performance does not scale linearly with model size;
(3) correct patches often appear early in generation; and (4) prompts
significantly affect results. These insights offer practical guidance for
designing effective and efficient LLM-based APR systems.

</details>


### [20] [Isolating Noisy Labelled Test Cases in Human-in-the-Loop Oracle Learning](https://arxiv.org/abs/2506.13273)
*Charaka Geethal Kapugama*

Main category: cs.SE

TL;DR: ISONOISE是一种技术，旨在识别人类参与的Oracle学习过程中引入的错误标记测试用例，提高训练过程的可靠性。


<details>
  <summary>Details</summary>
Motivation: 错误标记的测试用例会负面影响人类参与的Oracle学习技术，ISONOISE旨在解决这一问题。

Method: 通过隔离与其它测试用例不一致的测试用例，训练中间Oracle，并系统性地重新标记可疑测试用例，直到无错误标记为止。

Result: 实验显示，ISONOISE能以超过67%的准确率识别错误标记测试用例，且只需少量重新标记查询。

Conclusion: ISONOISE展示了提升人类参与的Oracle学习可靠性的潜力。

Abstract: Incorrectly labelled test cases can adversely affect the training process of
human-in-the-loop oracle learning tech-niques. This paper introduces ISONOISE,
a technique designed to identify such mislabelled test cases introduced during
human-in-the-loop oracle learning. This technique can be applied to programs
taking numeric inputs. Given a compromised automatic test oracle and its
training test suite, ISONOISE first isolates thetest cases suspected of being
mislabelled. This task is performed based on the level of disagreement of a
test case with respect to the others. An intermediate automatic test oracle is
trained based on the slightly disagreeing test cases. Based on the predictions
of this intermediate oracle, the test cases suspected of being mislabelled are
systematically presented for relabelling. When mislabelled test cases are
found, the intermediate test oracle is updated. This process repeats until no
mislabelled test case is found in relabelling. ISONOISE was evaluated within
the human-in-the-loop oracle learning method used in LEARN2FIX. Experimental
results demonstrate that ISONOISE can identify mislabelled test cases
introduced by the human in LEARN2FIX with over 67% accuracy, while requiring
only a small number of relabelling queries. These findings highlight the
potential of ISONOISE to enhance the reliability of human-in-the-loop oracle
learning.

</details>


### [21] [Adopting Use Case Descriptions for Requirements Specification: an Industrial Case Study](https://arxiv.org/abs/2506.13303)
*Julian Frattini,Anja Frattini*

Main category: cs.SE

TL;DR: 研究分析了用例（UC）描述的实践采用情况及其质量影响因素，发现实际应用与教科书推荐有差异，但少数现象如解决方案导向确实对实践有影响。


<details>
  <summary>Details</summary>
Motivation: 了解用例描述在现实中的采用情况，验证现有质量建议的实际相关性，并探索影响用例质量的因素。

Method: 调查了1188个业务需求，手工评估了273个模板式用例描述，并利用统计方法分析质量影响因素及其对后续开发活动的影响。

Result: 实践中的用例描述采用与教科书推荐有差异，但解决方案导向等少数现象具有实际影响。

Conclusion: 研究结果可引导用例质量研究朝更有实际意义的方向发展。

Abstract: Context: Use case (UC) descriptions are a prominent format for specifying
functional requirements. Existing literature abounds with recommendations on
how to write high-quality UC descriptions but lacks insights into (1) their
real-world adoption, (2) whether these recommendations correspond to actual
quality, and (3) which factors influence the quality of UCs. Objectives: We aim
to contribute empirical evidence about the adoption of UC descriptions in a
large, globally distributed case company. Methods: We surveyed 1188 business
requirements of a case company that were elicited from 2020-01-01 until
2024-12-31 and contained 1192 UCs in various forms. Among these, we manually
evaluated the 273 template-style UC descriptions against established quality
guidelines. We generated descriptive statistics of the format's adoption over
the surveyed time frame. Furthermore, we used inferential statistics to
determine (a) how properties of the requirements engineering process affected
the UC quality and (b) how UC quality affects subsequent software development
activities. Results and Conclusions: Our descriptive results show how the
adoption of UC descriptions in practice deviates from textbook recommendations.
However, our inferential results suggest that only a few phenomena like
solution-orientation show an actual impact in practice. These results can steer
UC quality research into a more relevant direction.

</details>


### [22] [DesignCoder: Hierarchy-Aware and Self-Correcting UI Code Generation with Large Language Models](https://arxiv.org/abs/2506.13663)
*Yunnong Chen,Shixian Ding,YingYing Zhang,Wenkai Chen,Jinzhou Du,Lingyun Sun,Liuqing Chen*

Main category: cs.SE

TL;DR: DesignCoder 是一种新型分层感知自校正代码生成框架，通过 UI 分组链和分层分治方法提升视觉一致性与功能完整性，并在视觉相似性和代码结构上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在代码生成中难以平衡视觉一致性和功能完整性，且缺乏对渲染页面保真度的评估机制。DesignCoder 旨在解决这些问题。

Method: 引入 UI Grouping Chains 增强 MLLMs 对复杂嵌套 UI 结构的理解，采用分层分治生成前端代码，并加入自校正机制以修正错误。

Result: 在视觉相似性指标（MSE、CLIP、SSIM）上提升 37.63%、9.52%、12.82%，代码结构相似性（TreeBLEU 等）提升 30.19%、29.31%、24.67%。用户研究表明生成代码符合行业最佳实践。

Conclusion: DesignCoder 为敏捷前端开发提供高效解决方案，使开发团队更专注于核心功能与产品创新。

Abstract: Multimodal large language models (MLLMs) have streamlined front-end interface
development by automating code generation. However, these models also introduce
challenges in ensuring code quality. Existing approaches struggle to maintain
both visual consistency and functional completeness in the generated
components. Moreover, they lack mechanisms to assess the fidelity and
correctness of the rendered pages. To address these issues, we propose
DesignCoder, a novel hierarchical-aware and self-correcting automated code
generation framework. Specifically, we introduce UI Grouping Chains, which
enhance MLLMs' capability to understand and predict complex nested UI
hierarchies. Subsequently, DesignCoder employs a hierarchical
divide-and-conquer approach to generate front-end code. Finally, we incorporate
a self-correction mechanism to improve the model's ability to identify and
rectify errors in the generated code. Extensive evaluations on a dataset of UI
mockups collected from both open-source communities and industry projects
demonstrate that DesignCoder outperforms state-of-the-art baselines in React
Native, a widely adopted UI framework. Our method achieves a 37.63%, 9.52%,
12.82% performance increase in visual similarity metrics (MSE, CLIP, SSIM) and
significantly improves code structure similarity in terms of TreeBLEU,
Container Match, and Tree Edit Distance by 30.19%, 29.31%, 24.67%. Furthermore,
we conducted a user study with professional developers to assess the quality
and practicality of the generated code. Results indicate that DesignCoder
aligns with industry best practices, demonstrating high usability, readability,
and maintainability. Our approach provides an efficient and practical solution
for agile front-end development, enabling development teams to focus more on
core functionality and product innovation.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [23] [A Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions](https://arxiv.org/abs/2506.12202)
*Stephen Mell,Botong Zhang,David Mell,Shuo Li,Ramya Ramalingam,Nathan Yu,Steve Zdancewic,Osbert Bastani*

Main category: cs.PL

TL;DR: 论文提出了一种名为Quasar的新编程语言，用于LLMs生成代码动作，以提高性能、安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs通常以Python生成代码动作，但Python在性能、安全和可靠性方面存在不足。

Method: 提出Quasar语言，支持自动并行化、不确定性量化和安全性验证，并将Python子集自动转换为Quasar。

Result: 在ViperGPT视觉问答任务中，Quasar将执行时间减少42%，用户交互减少52%，并通过共形预测提高可靠性。

Conclusion: Quasar能在不影响性能的同时显著提升LLMs代码动作的安全性、可靠性和效率。

Abstract: Modern large language models (LLMs) are often deployed as agents, calling
external tools adaptively to solve tasks. Rather than directly calling tools,
it can be more effective for LLMs to write code to perform the tool calls,
enabling them to automatically generate complex control flow such as
conditionals and loops. Such code actions are typically provided as Python
code, since LLMs are quite proficient at it; however, Python may not be the
ideal language due to limited built-in support for performance, security, and
reliability. We propose a novel programming language for code actions, called
Quasar, which has several benefits: (1) automated parallelization to improve
performance, (2) uncertainty quantification to improve reliability and mitigate
hallucinations, and (3) security features enabling the user to validate
actions. LLMs can write code in a subset of Python, which is automatically
transpiled to Quasar. We evaluate our approach on the ViperGPT visual question
answering agent, applied to the GQA dataset, demonstrating that LLMs with
Quasar actions instead of Python actions retain strong performance, while
reducing execution time when possible by 42%, improving security by reducing
user approval interactions when possible by 52%, and improving reliability by
applying conformal prediction to achieve a desired target coverage level.

</details>


### [24] [Freer Arrows and Why You Need Them in Haskell](https://arxiv.org/abs/2506.12212)
*Grant VanDomelen,Gan Shen,Lindsey Kuper,Yao Li*

Main category: cs.PL

TL;DR: 论文研究了Freer arrows作为一种可静态分析的表达结构，并提出了几种变体，通过编舞编程案例验证了其在Haskell中的实用性。


<details>
  <summary>Details</summary>
Motivation: Freer monads虽具表达性但难以静态分析，探索Freer arrows以解决这一问题。

Method: 提出多种Freer arrows变体，并通过编舞编程案例验证。

Result: Freer arrows在保持表达性的同时支持静态分析。

Conclusion: Freer arrows是Freer monads的有效替代，适用于需要静态分析的场景。

Abstract: Freer monads are a useful structure commonly used in various domains due to
their expressiveness. However, a known issue with freer monads is that they are
not amenable to static analysis. This paper explores freer arrows, a relatively
expressive structure that is amenable to static analysis. We propose several
variants of freer arrows. We conduct a case study on choreographic programming
to demonstrate the usefulness of freer arrows in Haskell.

</details>


### [25] [StacKAT: Infinite State Network Verification](https://arxiv.org/abs/2506.13383)
*Jules Jacobs,Nate Foster,Tobias Kappé,Dexter Kozen,Lily Saada,Alexandra Silva,Jana Wagemaker*

Main category: cs.PL

TL;DR: 提出了一种名为StacKAT的网络验证语言，支持循环、有限状态变量、非确定性及堆栈操作，可表达复杂的网络行为，并提供了等价性判定程序和公理化系统。


<details>
  <summary>Details</summary>
Motivation: 现有的网络验证语言（如NetKAT）难以表达复杂的网络行为（如解析、源路由和遥测），因此需要开发一种更强大的语言。

Method: 开发了StacKAT语言，引入了堆栈、状态变量和非确定性，并基于有限自动机构建了用于判定程序等价性的决策程序。

Result: StacKAT能够表达广泛的网络行为，决策程序可以验证网络范围属性并提供反例，同时公理化系统完备性得到证明。

Conclusion: StacKAT弥补了现有语言的不足，为网络验证提供了更强大的工具和理论基础。

Abstract: We develop StacKAT, a network verification language featuring loops, finite
state variables, nondeterminism, and - most importantly - access to a stack
with accompanying push and pop operations. By viewing the variables and stack
as the (parsed) headers and (to-be-parsed) contents of a network packet,
StacKAT can express a wide range of network behaviors including parsing, source
routing, and telemetry. These behaviors are difficult or impossible to model
using existing languages like NetKAT. We develop a decision procedure for
StacKAT program equivalence, based on finite automata. This decision procedure
provides the theoretical basis for verifying network-wide properties and is
able to provide counterexamples for inequivalent programs. Finally, we provide
an axiomatization of StacKAT equivalence and establish its completeness.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [26] [NaSh: Guardrails for an LLM-Powered Natural Language Shell](https://arxiv.org/abs/2506.13028)
*Bimal Raj Gyawali,Saikrishna Achalla,Konstantinos Kallas,Sam Kumar*

Main category: cs.OS

TL;DR: 探讨如何设计一种使用LLM接受自然语言输入的新式shell，并提出了一个名为NaSh的设计原型。


<details>
  <summary>Details</summary>
Motivation: 当前shell与LLM结合时可能产生不可控或难以解释的输出，需提供保障措施帮助用户从错误中恢复。

Method: 通过设计一个新的shell（NaSh），具体化了相关想法，并识别了该领域的未解决问题。

Result: 提出了NaSh设计原型，讨论了自然语言shell的潜在研究方向。

Conclusion: 自然语言shell需要更多研究来解决其输出不可控的问题，NaSh是一个初步尝试。

Abstract: We explore how a shell that uses an LLM to accept natural language input
might be designed differently from the shells of today. As LLMs may produce
unintended or unexplainable outputs, we argue that a natural language shell
should provide guardrails that empower users to recover from such errors. We
concretize some ideas for doing so by designing a new shell called NaSh,
identify remaining open problems in this space, and discuss research directions
to address them.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [27] [NR Cell Identity-based Handover Decision-making Algorithm for High-speed Scenario within Dual Connectivity](https://arxiv.org/abs/2506.12461)
*Zhiyi Zhu,Eiji Takimoto,Patrick Finnertyn,Junjun Zheng,Shoma Suzuki,Chikara Ohta*

Main category: cs.NI

TL;DR: 提出一种基于NR小区身份（NCI）的切换决策算法（HDMA），通过识别目标gNB类型（宏/小/毫米波gNB）来改进切换策略，提升高速移动用户设备的通信稳定性。


<details>
  <summary>Details</summary>
Motivation: 5G异构网络的密集部署虽然提升了网络容量，但也导致高速移动设备频繁和不必要的切换，影响通信稳定性。传统切换算法忽略目标gNB类型，导致问题加剧。

Method: 提出HDMA算法，利用NCI中的gNB身份（ID）识别目标gNB类型（宏/小/毫米波），改进切换策略。

Result: 仿真结果表明，所提HDMA在提升连接稳定性方面优于其他切换算法。

Conclusion: 通过识别目标gNB类型，HDMA有效解决了高速移动设备的切换问题，提升了通信稳定性。

Abstract: The dense deployment of 5G heterogeneous networks (HetNets) has improved
network capacity. However, it also brings frequent and unnecessary handover
challenges to high-speed mobile user equipment (UE), resulting in unstable
communication and degraded quality of service. Traditional handovers ignore the
type of target next-generation Node B (gNB), resulting in high-speed UEs being
able to be handed over to any gNB. This paper proposes a NR cell identity
(NCI)-based handover decision-making algorithm (HDMA) to address this issue.
The proposed HDMA identifies the type of the target gNB (macro/small/mmWave
gNB) using the gNB identity (ID) within the NCI to improve the handover
decision-making strategy. The proposed HDMA aims to improve the communication
stability of high-speed mobile UE by enabling high-speed UEs to identify the
target gNB type during the HDMA using the gNB ID. Simulation results show that
the proposed HDMA outperforms other HDMAs in enhanced connection stability.

</details>


### [28] [Multi-domain anomaly detection in a 5G network](https://arxiv.org/abs/2506.12070)
*Thomas Hoger,Philippe Owezarski*

Main category: cs.NI

TL;DR: 提出一种多维度异常检测方法，结合时间、语义和拓扑分析5G网络安全。


<details>
  <summary>Details</summary>
Motivation: 5G网络的动态性增加了攻击面，需更全面的安全防护。

Method: 研究消息序列（时间）、参数（语义）和图形化链接（拓扑）的关联。

Result: 与传统方法相比，能提供全局、一致且可解释的异常视图。

Conclusion: 多维度关联分析提升了5G网络异常检测的效果。

Abstract: With the advent of 5G, mobile networks are becoming more dynamic and will
therefore present a wider attack surface. To secure these new systems, we
propose a multi-domain anomaly detection method that is distinguished by the
study of traffic correlation on three dimensions: temporal by analyzing message
sequences, semantic by abstracting the parameters these messages contain, and
topological by linking them in the form of a graph. Unlike traditional
approaches, which are limited to considering these domains independently, our
method studies their correlations to obtain a global, coherent and explainable
view of anomalies.

</details>


### [29] [Mobile Traffic Prediction using LLMs with Efficient In-context Demonstration Selection](https://arxiv.org/abs/2506.12074)
*Han Zhang,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的上下文感知无线流量预测框架，并通过两步骤演示选择策略提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 移动流量预测对优化资源分配和提高移动无线网络的能效至关重要。

Method: 采用上下文学习（ICL）和两步骤演示选择策略：首步基于有效性规则选择演示，次步基于信息性规则决定是否使用所选演示。

Result: 在真实5G数据集上验证，框架比零样本预测和其他演示选择方法表现更好（均方误差更低，R2分数更高）。

Conclusion: 结合LLM和两步骤演示选择策略能有效提升流量预测精度。

Abstract: Mobile traffic prediction is an important enabler for optimizing resource
allocation and improving energy efficiency in mobile wireless networks.
Building on the advanced contextual understanding and generative capabilities
of large language models (LLMs), this work introduces a context-aware wireless
traffic prediction framework powered by LLMs. To further enhance prediction
accuracy, we leverage in-context learning (ICL) and develop a novel two-step
demonstration selection strategy, optimizing the performance of LLM-based
predictions. The initial step involves selecting ICL demonstrations using the
effectiveness rule, followed by a second step that determines whether the
chosen demonstrations should be utilized, based on the informativeness rule. We
also provide an analytical framework for both informativeness and effectiveness
rules. The effectiveness of the proposed framework is demonstrated with a
real-world fifth-generation (5G) dataset with different application scenarios.
According to the numerical results, the proposed framework shows lower mean
squared error and higher R2-Scores compared to the zero-shot prediction method
and other demonstration selection methods, such as constant ICL demonstration
selection and distance-only-based ICL demonstration selection.

</details>


### [30] [Latency Optimization for Wireless Federated Learning in Multihop Networks](https://arxiv.org/abs/2506.12081)
*Shaba Shaon,Van-Dinh Nguyen,Dinh C. Nguyen*

Main category: cs.NI

TL;DR: 本文研究了一种在多跳无线联邦学习（FL）中的新型延迟最小化问题，提出了一种个性化学习和自适应聚合的FL框架（PAFL），通过联合优化叶节点和中继节点以及路由指示器来最小化系统延迟，并引入能量收集方案。


<details>
  <summary>Details</summary>
Motivation: 多跳无线联邦学习中的延迟问题是当前研究的难点，数据异构性使得学习任务更加复杂。

Method: 设计了PAFL框架，通过块坐标下降和连续凸近似（SCA）技术联合优化叶节点、中继节点和路由指示器。

Result: 提出的联合优化方法显著降低了延迟，最高节省69.37%，优于传统贪婪算法和单节点优化方案。

Conclusion: PAFL框架在多跳无线FL中表现出色，联合优化策略显著提升了系统性能和效率。

Abstract: In this paper, we study a novel latency minimization problem in wireless
federated learning (FL) across multi-hop networks. The system comprises
multiple routes, each integrating leaf and relay nodes for FL model training.
We explore a personalized learning and adaptive aggregation-aware FL (PAFL)
framework that effectively addresses data heterogeneity across participating
nodes by harmonizing individual and collective learning objectives. We
formulate an optimization problem aimed at minimizing system latency through
the joint optimization of leaf and relay nodes, as well as relay routing
indicator. We also incorporate an additional energy harvesting scheme for the
relay nodes to help with their relay tasks. This formulation presents a
computationally demanding challenge, and thus we develop a simple yet efficient
algorithm based on block coordinate descent and successive convex approximation
(SCA) techniques. Simulation results illustrate the efficacy of our proposed
joint optimization approach for leaf and relay nodes with relay routing
indicator. We observe significant latency savings in the wireless multi-hop
PAFL system, with reductions of up to 69.37% compared to schemes optimizing
only one node type, traditional greedy algorithm, and scheme without relay
routing indicator.

</details>


### [31] [Real-Time Capable, Low-latency Upstream Scheduling in Multi-Tenant, SLA Compliant TWDM PON](https://arxiv.org/abs/2506.12118)
*Arijeet Ganguli,Marco Ruffini*

Main category: cs.NI

TL;DR: 论文提出了一种新型的DTWA算法，用于多租户PON环境下的vDBA架构，支持多通道动态分配，满足低延迟需求，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现代接入网络需要更高的灵活性和多租户支持，vPONs通过解耦网络功能与物理基础设施提供了解决方案，但需要高效的DBA算法来优化资源分配。

Method: 提出DTWA算法，结合多通道支持和动态波长分配，利用Numba API实现高性能优化，实时处理带宽映射。

Result: 算法在多租户条件下实现了低延迟和高吞吐量，但需权衡单通道与多通道PON的性能受ONU调谐时间影响。

Conclusion: DTWA算法在多租户PON环境中表现出色，尤其适用于6G应用，但仍需进一步优化以适配不同业务分布。

Abstract: Virtualized Passive Optical Networks (vPONs) offer a promising solution for
modern access networks, bringing enhanced flexibility, reduced capital
expenditures (CapEx), and support for multi-tenancy. By decoupling network
functions from physical infrastructure, vPONs enable service providers to
efficiently share network resources among multiple tenants. In this paper, we
propose a novel merging DBA algorithm, called the Dynamic Time and Wavelength
Allocation (DTWA) algorithm, for a virtualized DBA (vDBA) architecture in
multi-tenant PON environments. The Algorithm, which enables the merging of
multiple virtual DBAs into a physical bandwidth map, introduces multi-channel
support, allowing each Optical Network Unit (ONU) to dynamically change, taking
into consideration different switching times, transmission wavelength.
Leveraging the Numba APIs for high-performance optimization, the algorithm
achieves real-time performance with minimal additional latency, meeting the
stringent requirements of SLA-compliant, latency-critical 6G applications and
services. Our analysis highlights an important trade-off in terms of throughput
in multi-tenant conditions, between single-channel vs. multi-channel PONs, as a
function of ONUs tuning time. We also compare the performance of our algorithm
for different traffic distributions. Finally, in order to assess the time
computing penalty of dynamic wavelength optimisation in the merging DBA
algorithm, we compare it against a baseline Static Wavelength Allocation (SWA)
algorithm, where ONUs are designated a fixed wavelength for transmission.

</details>


### [32] [Surfing the SWAVES: Lifecycle-aware Service Placement in MEC](https://arxiv.org/abs/2506.12265)
*Federico Giarrè,Holger Karl*

Main category: cs.NI

TL;DR: 论文提出了一种名为SWAVES的方法，用于在多接入边缘计算（MEC）网络中优化虚拟网络功能（VNFs）的主动部署，以应对用户移动性带来的挑战。相比其他启发式方法，SWAVES显著降低了用户数据包失败率。


<details>
  <summary>Details</summary>
Motivation: 在MEC网络中，用户移动性导致VNFs需要频繁迁移以满足低延迟和高可用性需求。但由于边缘云（ECs）资源有限且VNFs部署耗时，传统方法效果不佳。因此，需要一种更高效的解决方案。

Method: 提出了SWAVES方法，通过预测用户移动性并主动在可能的位置提前部署VNFs实例，以平衡延迟和资源使用。

Result: 相比其他启发式方法，SWAVES将用户数据包失败率降低了数个数量级。

Conclusion: SWAVES通过主动部署策略显著提升了MEC网络中VNFs的服务质量，为应对用户移动性挑战提供了有效解决方案。

Abstract: In Multi-access Edge Computing (MEC) networks, users covered by a mobile
network can exploit edge clouds (ECs), computational resources located at the
network's edge, to execute virtual network functions (VNFs). ECs are
particularly useful when deploying VNFs with strict delay and availability
requirements. As users roam in the network and get handed over between cells,
deployed VNFs must follow users to retain the benefits of edge computing. Yet,
having VNFs ready at the closest EC can be challenging: (i) ECs are not usually
powerful enough to store and run any combination of VNFs simultaneously; (ii)
if a VNF is not available at the needed EC, a series of time-consuming
operations has to be performed before the VNF becomes operational. These
limitations can be addressed by proactively starting VNFs instances at (likely)
future locations, balancing better latency properties against higher resource
usage. Such proactive deployment does need forecasting of user movements, but
these will be imperfect, creating yet another tradeoff. We present our approach
to this service provisioning problem, SWAVES. When compared on the ratio of
users' unsuccessful packets, SWAVES improves such metric by orders of magnitude
with respect to other proposed heuristic.

</details>


### [33] [Learning Best Paths in Quantum Networks](https://arxiv.org/abs/2506.12462)
*Xuchuang Wang,Maoli Liu,Xutong Liu,Zhuohua Li,Mohammad Hajiesmaili,John C. S. Lui,Don Towsley*

Main category: cs.NI

TL;DR: 该论文研究量子网络（QN）中最佳路径的在线学习问题，提出了两种基于不同反馈机制的算法（BeQuP-Link和BeQuP-Path），并通过仿真验证了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 量子网络在量子密钥分发（QKD）和分布式量子计算（DQC）等应用中至关重要。通过在线学习找到最佳路径可以显著提升这些应用的效率。

Method: 论文提出了两种算法：BeQuP-Link（使用链路级反馈动态评估关键链路）和BeQuP-Path（通过路径级反馈批量估计链路参数）。

Result: 两种算法均能以高概率高效地找到最佳路径，并通过NetSquid仿真验证了其准确性和效率。

Conclusion: 通过不同反馈机制的在线学习算法可以有效解决量子网络中最佳路径的学习问题，为量子网络的应用提供了支持。

Abstract: Quantum networks (QNs) transmit delicate quantum information across noisy
quantum channels. Crucial applications, like quantum key distribution (QKD) and
distributed quantum computation (DQC), rely on efficient quantum information
transmission. Learning the best path between a pair of end nodes in a QN is key
to enhancing such applications. This paper addresses learning the best path in
a QN in the online learning setting. We explore two types of feedback:
"link-level" and "path-level". Link-level feedback pertains to QNs with
advanced quantum switches that enable link-level benchmarking. Path-level
feedback, on the other hand, is associated with basic quantum switches that
permit only path-level benchmarking. We introduce two online learning
algorithms, BeQuP-Link and BeQuP-Path, to identify the best path using
link-level and path-level feedback, respectively. To learn the best path,
BeQuP-Link benchmarks the critical links dynamically, while BeQuP-Path relies
on a subroutine, transferring path-level observations to estimate link-level
parameters in a batch manner. We analyze the quantum resource complexity of
these algorithms and demonstrate that both can efficiently and, with high
probability, determine the best path. Finally, we perform NetSquid-based
simulations and validate that both algorithms accurately and efficiently
identify the best path.

</details>


### [34] [Cost-Efficient Design for 5G-Enabled MEC Servers under Uncertain User Demands](https://arxiv.org/abs/2506.13003)
*Yunyi Wu,Yongbing Zhang*

Main category: cs.NI

TL;DR: 论文提出了一种加速Benders分解（ABD）方法，用于优化5G网络中的移动边缘计算服务器规划和资源分配，以最小化容量需求和服务延迟。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算（MEC）通过将数据单元部署在靠近用户的位置，提升了5G网络的性能，但在动态卸载任务和资源分配中面临用户需求不确定的挑战。

Method: 采用两阶段随机模型，将其转化为混合整数线性规划（MILP）问题，并提出ABD方法来高效求解大规模网络问题。

Result: 数值实验表明，ABD能够在显著减少计算时间的同时，得到MILP的最优解。

Conclusion: ABD方法在优化边缘服务器规划和资源分配方面表现出高效性和实用性，适用于5G网络中的MEC场景。

Abstract: Mobile edge computing (MEC) enhances the performance of 5G networks by
enabling low-latency, high-speed services through deploying data units of the
base station on edge servers located near mobile users. However, determining
the optimal capacity of these servers while dynamically offloading tasks and
allocating computing resources to meet uncertain user demands presents
significant challenges. This paper focuses on the design and planning of edge
servers with the dual objectives of minimizing capacity requirements and
reducing service latency for 5G services. To handle the complexity of uncertain
user demands, we formulate the problem as a two-stage stochastic model, which
can be linearized into a mixed-integer linear programming (MILP) problem. We
propose a novel approach called accelerated Benders decomposition (ABD) to
solve the problem at a large network scale. Numerical experiments demonstrate
that ABD achieves the optimal solution of MILP while significantly reducing
computation time.

</details>


### [35] [Dynamic Preference Multi-Objective Reinforcement Learning for Internet Network Management](https://arxiv.org/abs/2506.13153)
*DongNyeong Heo,Daniela Noemi Rim,Heeyoul Choi*

Main category: cs.NI

TL;DR: 网络管理中的强化学习代理能根据网络状态动态调整目标偏好，以提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统网络管理算法的目标偏好是静态的，但实际中偏好会因网络状态或外部因素变化。

Method: 提出基于强化学习的网络管理代理，动态结合状态和偏好选择动作，并提出一种数值方法估计偏好分布。

Result: 实验表明，动态偏好的代理比静态偏好的代理泛化能力更强，数值估计方法也显示出优势。

Conclusion: 动态调整偏好在网络管理中更灵活，泛化性能更好，数值方法有助于无偏训练。

Abstract: An internet network service provider manages its network with multiple
objectives, such as high quality of service (QoS) and minimum computing
resource usage. To achieve these objectives, a reinforcement learning-based
(RL) algorithm has been proposed to train its network management agent.
Usually, their algorithms optimize their agents with respect to a single static
reward formulation consisting of multiple objectives with fixed importance
factors, which we call preferences. However, in practice, the preference could
vary according to network status, external concerns and so on. For example,
when a server shuts down and it can cause other servers' traffic overloads
leading to additional shutdowns, it is plausible to reduce the preference of
QoS while increasing the preference of minimum computing resource usages. In
this paper, we propose new RL-based network management agents that can select
actions based on both states and preferences. With our proposed approach, we
expect a single agent to generalize on various states and preferences.
Furthermore, we propose a numerical method that can estimate the distribution
of preference that is advantageous for unbiased training. Our experiment
results show that the RL agents trained based on our proposed approach
significantly generalize better with various preferences than the previous RL
approaches, which assume static preference during training. Moreover, we
demonstrate several analyses that show the advantages of our numerical
estimation method.

</details>


### [36] [Joint Optimization of Multi-UAV Deployment and 3D Positioning in Traffic-Aware Aerial Networks](https://arxiv.org/abs/2506.13287)
*Kamran Shafafi,Alaa Awad Abdellatif,Manuel Ricardo,Rui Campos*

Main category: cs.NI

TL;DR: 无人机（UAV）因其灵活部署和高机动性成为下一代无线网络的关键技术，但实时优化多无人机部署仍具挑战。本文提出的EMTAD算法通过动态调整无人机位置，显著提升了网络性能并减少了无人机数量。


<details>
  <summary>Details</summary>
Motivation: 无人机因其按需部署、高机动性和视距连接能力，成为动态和关键任务应用的理想选择，但实时适应非均匀、时变流量需求仍是一个挑战。

Method: 提出了一种高效的EMTAD算法，通过动态调整无人机位置和最小化部署数量，优化网络吞吐量和资源利用率。

Result: 仿真结果表明，在动态和流量感知环境中，EMTAD显著提高了网络性能，同时减少了所需无人机的数量。

Conclusion: EMTAD算法为解决多无人机实时部署问题提供了一种高效且可扩展的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) have emerged as a key enabler for
next-generation wireless networks due to their on-demand deployment, high
mobility, and ability to provide Line-of-Sight (LoS) connectivity. These
features make UAVs particularly well-suited for dynamic and mission-critical
applications such as intelligent transportation systems and emergency
communications. However, effectively positioning multiple UAVs in real-time to
meet non-uniform, time-varying traffic demands remains a significant challenge,
especially when aiming to optimize network throughput and resource utilization.
In this paper, we propose an Efficient Multi-UAV Traffic-Aware Deployment
(EMTAD) Algorithm, a scalable and adaptive framework that dynamically adjusts
UAV placements based on real-time user locations and spatial traffic
distribution. In contrast to existing methods, EMTAD jointly optimizes UAV
positioning and minimizes the number of deployed UAVs, ensuring efficient
UE-UAV association while satisfying the traffic demand of users. Simulation
results demonstrate that EMTAD significantly improves network performance while
reducing deployment overhead by minimizing the number of UAVs required in
dynamic and traffic-aware environments.

</details>


### [37] [Delay-optimal Congestion-aware Routing and Computation Offloading in Arbitrary Network](https://arxiv.org/abs/2506.13626)
*Jinkun Zhang,Yuezhou Liu,Edmund Yeh*

Main category: cs.NI

TL;DR: 论文提出了一种在异构边缘网络中联合优化数据传输、结果路由和计算放置的方法，解决了非凸问题，并通过分布式算法实现了全局最优。


<details>
  <summary>Details</summary>
Motivation: 解决异构边缘网络中延迟最优的数据转发和计算卸载问题。

Method: 分析KKT条件，提出足够的最优性条件，并证明问题具有测地凸性。开发了完全分布式算法。

Result: 数值实验显示，该方法显著优于多种基线算法。

Conclusion: 提出的方法能够全局优化异构网络中的计算和通信延迟，具有稳定性和实用性。

Abstract: Emerging edge computing paradigms enable heterogeneous devices to collaborate
on complex computation applications. However, for arbitrary heterogeneous edge
networks, delay-optimal forwarding and computation offloading remains an open
problem. In this paper, we jointly optimize data/result routing and computation
placement in arbitrary networks with heterogeneous node capabilities, and
congestion-dependent nonlinear transmission and processing delay. Despite the
non-convexity of the formulated problem, based on analyzing the KKT condition,
we provide a set of sufficient optimality conditions that solve the problem
globally. To provide the insights for such global optimality, we show that the
proposed non-convex problem is geodesic-convex with mild assumptions. We also
show that the proposed sufficient optimality condition leads to a lower
hemicontinuous solution set, providing stability against user-input
perturbation. We then extend the framework to incorporate utility-based
congestion control and fairness. A fully distributed algorithm is developed to
converge to the global optimum. Numerical results demonstrate significant
improvements over multiple baselines algorithms.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [38] [Programming and Reasoning in Partially Observable Probabilistic Environments](https://arxiv.org/abs/2506.13491)
*Tobias Gürtler,Benjamin Lucien Kaminski*

Main category: cs.LO

TL;DR: 该论文提出了概率信念编程语言pBLIMP，用于处理部分可观测环境中的概率行为，支持状态估计和基于循环不变量的无界循环推理。


<details>
  <summary>Details</summary>
Motivation: 解决计算机系统在概率性且部分可观测的环境中运行时的理论建模问题。

Method: 设计pBLIMP语言，包含符号建模、观察条件和无界循环特性，并提出基于最弱前提条件的演算方法（wp）进行分析。

Result: 证明了wp演算相对于操作语义的正确性，并展示了如何处理无界循环。

Conclusion: pBLIMP和wp演算为部分可观测环境中的程序推理提供了理论框架和方法支持。

Abstract: Probabilistic partial observability is a phenomenon occuring when computer
systems are deployed in environments that behave probabilistically and whose
exact state cannot be fully observed. In this work, we lay the theoretical
groundwork for a probabilistic belief programming language pBLIMP, which
maintains a probability distribution over the possible environment states,
called a belief state. pBLIMP has language features to symbolically model the
behavior of and interaction with the partially observable environment and to
condition the belief state based on explicit observations. In particular,
pBLIMP programs can perform state estimation and base their decisions (i.e. the
control flow) on the likelihood that certain conditions hold in the current
state. Furthermore, pBLIMP features unbounded loops, which sets it apart from
many other probabilistic programming languages. For reasoning about pBLIMP
programs and the situations they model, we present a weakest-precondition-style
calculus (wp) that is capable of reasoning about unbounded loops. Soundness of
our wp calculus is proven with respect to an operational semantics. We further
demonstrate how our wp calculus reasons about (unbounded) loops with loop
invariants.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [39] [Extended Version of Paper Presented at ICISSP, Porto 20-22 February, 2025 A Value-Driven Approach to the Online Consent Conundrum -- A Study with the Unemployed](https://arxiv.org/abs/2506.12244)
*Paul van Schaik,Karen Renaud*

Main category: cs.HC

TL;DR: 这篇论文通过两项研究探讨了如何通过价值驱动的方法简化冗长的同意表单，发现用户最重视的是减少努力。


<details>
  <summary>Details</summary>
Motivation: 当前的在线服务同意表单过于冗长和复杂，导致用户经常未经阅读即同意，使得同意实际上是非知情同意。

Method: 进行了两项研究：1）采访失业用户以识别他们希望表单满足的价值观；2）通过调查量化这些价值观及其实现方式。

Result: 失业用户和就业用户在价值观优先顺序上无显著差异，但发现价值观与实现方式之间存在较大差异，用户最重视减少努力。

Conclusion: 研究支持通过价值驱动的方法简化同意表单，尤其是减少用户的努力。

Abstract: Online services are required to gain informed consent from users to collect,
store and analyse their personal data, both intentionally divulged and derived
during their use of the service. There are many issues with these forms: they
are too long, too complex and demand the user's attention too frequently. Many
users consent without reading so do not know what they are agreeing to. As
such,granted consent is effectively uninformed. In this paper, we report on two
studies we carried out to arrive at a value-driven approach to inform efforts
to reduce the length of consent forms. The first study interviewed unemployed
users to identify the values they want these forms to satisfy. The second
survey study helped us to quantify the values and value creators. To ensure
that we understood the particular valuation of the unemployed, we compared
their responses to those of an employed demographic and observed no significant
differences between their prioritisation on any of the values. However, we did
find substantial differences between values and value creators, with effort
minimisation being most valued by our participants.

</details>


### [40] [Improving Public Service Chatbot Design and Civic Impact: Investigation of Citizens' Perceptions of a Metro City 311 Chatbot](https://arxiv.org/abs/2506.12259)
*Jieyu Zhou,Rui Shen,Yue You,Carl DiSalvo,Lynn Dombrowski,Christopher MacLellan*

Main category: cs.HC

TL;DR: 探讨公共服务聊天机器人的设计考虑和参与机会，发现个体和社区层面的问题，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着政府采用数字工具的增加，公共服务聊天机器人成为重要沟通渠道，但其设计和使用存在不足。

Method: 通过官方调查数据和16次访谈，分析利益相关者的经验和设计偏好。

Result: 发现个体层面的解释、透明度和社交语境问题，以及社区视角的缺失。

Conclusion: 提出更智能、透明、社区导向的聊天机器人设计机会。

Abstract: As governments increasingly adopt digital tools, public service chatbots have
emerged as a growing communication channel. This paper explores the design
considerations and engagement opportunities of public service chatbots, using a
311 chatbot from a metropolitan city as a case study. Our qualitative study
consisted of official survey data and 16 interviews examining stakeholder
experiences and design preferences for the chatbot. We found two key areas of
concern regarding these public chatbots: individual-level and community-level.
At the individual level, citizens experience three key challenges:
interpretation, transparency, and social contextualization. Moreover, the
current chatbot design prioritizes the efficient completion of individual tasks
but neglects the broader community perspective. It overlooks how individuals
interact and discuss problems collectively within their communities. To address
these concerns, we offer design opportunities for creating more intelligent,
transparent, community-oriented chatbots that better engage individuals and
their communities.

</details>


### [41] [TermSight: Making Service Contracts Approachable](https://arxiv.org/abs/2506.12332)
*Ziheng Huang,Tal August,Hari Sundaram*

Main category: cs.HC

TL;DR: TermSight是一个智能阅读界面，旨在通过可视化摘要和简化语言使ToS更易于理解。


<details>
  <summary>Details</summary>
Motivation: ToS通常内容复杂且冗长，用户难以阅读和理解。

Method: TermSight提供可视化摘要、分类和简化信息、上下文定义和场景说明。

Result: 评估显示，TermSight显著降低阅读难度并提高用户阅读意愿。

Conclusion: TermSight通过AI功能多样化辅助ToS阅读，效果显著。

Abstract: Terms of Service (ToS) are ubiquitous, legally binding contracts that govern
consumers' digital interactions. However, ToS are not designed to be read: they
are filled with pages of ambiguous and complex legal terminology that burden
potential users. We introduce TermSight, an intelligent reading interface
designed to make ToS more approachable. TermSight offers visual summaries that
highlight the relevance and power balance of information in a ToS. TermSight
also categorizes and simplifies information within the ToS into concise
plain-language summaries. To aid in reading the original text, TermSight offers
contextualized definitions and scenarios for unfamiliar phrases. Our
within-subjects evaluation of TermSight (N=20) revealed that TermSight
significantly reduced the difficulty of reading ToS and increased participants'
willingness to do so. We also observed emerging strategies that participants
took when interacting with AI-powered features that highlight the diverse ways
that TermSight assisted ToS reading.

</details>


### [42] [SheetMind: An End-to-End LLM-Powered Multi-Agent Framework for Spreadsheet Automation](https://arxiv.org/abs/2506.12339)
*Ruiyan Zhu,Xi Cheng,Ke Liu,Brian Zhu,Daniel Jin,Neeraj Parihar,Zhoutian Xu,Oliver Gao*

Main category: cs.HC

TL;DR: SheetMind是一个基于大型语言模型的模块化多代理框架，用于通过自然语言指令自动化电子表格操作，包含三个代理，实验显示其在单步和多步任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 目的是通过多代理系统和结构化命令，让用户无需编程知识即可用自然语言操作电子表格，提升效率和易用性。

Method: 框架包含Manager Agent（分解任务）、Action Agent（用BNF语法生成命令）、Reflection Agent（验证任务一致性），并集成到Google Sheets中。

Result: 在基准测试中，单步任务成功率80%，多步任务约70%，优于基线方法。

Conclusion: 多代理分解和语法驱动的执行能有效连接自然语言与电子表格功能。

Abstract: We present SheetMind, a modular multi-agent framework powered by large
language models (LLMs) for spreadsheet automation via natural language
instructions. The system comprises three specialized agents: a Manager Agent
that decomposes complex user instructions into subtasks; an Action Agent that
translates these into structured commands using a Backus Naur Form (BNF)
grammar; and a Reflection Agent that validates alignment between generated
actions and the user's original intent. Integrated into Google Sheets via a
Workspace extension, SheetMind supports real-time interaction without requiring
scripting or formula knowledge. Experiments on benchmark datasets demonstrate
an 80 percent success rate on single step tasks and approximately 70 percent on
multi step instructions, outperforming ablated and baseline variants. Our
results highlight the effectiveness of multi agent decomposition and grammar
based execution for bridging natural language and spreadsheet functionalities.

</details>


### [43] [SplashNet: Split-and-Share Encoders for Accurate and Efficient Typing with Surface Electromyography](https://arxiv.org/abs/2506.12356)
*Nima Hadidi,Jason Chan,Ebrahim Feghhi,Jonathan Kao*

Main category: cs.HC

TL;DR: 这篇论文通过改进sEMG信号处理方法，提出SplashNet和SplashNet-mini模型，显著降低了字符错误率。


<details>
  <summary>Details</summary>
Motivation: 现有基于手腕表面肌电信号（sEMG）的文本输入系统在跨用户或未经微调时错误率较高，需要改进信号处理方法和模型架构。

Method: 采用三种改进措施：滚动时间归一化、激进通道掩码和分而共享编码器，并结合降低频谱分辨率，设计了紧凑的Split-and-Share模型。

Result: SplashNet-mini和SplashNet模型分别将零样本和微调后的字符错误率降低至36.4%/5.9%和35.7%/5.5%，性能优于现有基线。

Conclusion: 通过这些改进，SplashNet在不增加数据的情况下实现了新的性能标杆。

Abstract: Surface electromyography (sEMG) at the wrists could enable natural,
keyboard-free text entry, yet the state-of-the-art emg2qwerty baseline still
misrecognizes $51.8\%$ of characters in the zero-shot setting on unseen users
and $7.0\%$ after user-specific fine-tuning. We trace many of these errors to
mismatched cross-user signal statistics, fragile reliance on high-order feature
dependencies, and the absence of architectural inductive biases aligned with
the bilateral nature of typing. To address these issues, we introduce three
simple modifications: (i) Rolling Time Normalization, which adaptively aligns
input distributions across users; (ii) Aggressive Channel Masking, which
encourages reliance on low-order feature combinations more likely to generalize
across users; and (iii) a Split-and-Share encoder that processes each hand
independently with weight-shared streams to reflect the bilateral symmetry of
the neuromuscular system. Combined with a five-fold reduction in spectral
resolution ($33\!\rightarrow\!6$ frequency bands), these components yield a
compact Split-and-Share model, SplashNet-mini, which uses only $\tfrac14$ the
parameters and $0.6\times$ the FLOPs of the baseline while reducing
character-error rate (CER) to $36.4\%$ zero-shot and $5.9\%$ after fine-tuning.
An upscaled variant, SplashNet ($\tfrac12$ the parameters, $1.15\times$ the
FLOPs of the baseline), further lowers error to $35.7\%$ and $5.5\%$,
representing relative improvements of $31\%$ and $21\%$ in the zero-shot and
fine-tuned settings, respectively. SplashNet therefore establishes a new state
of the art without requiring additional data.

</details>


### [44] [`Socheton': A Culturally Appropriate AI Tool to Support Reproductive Well-being](https://arxiv.org/abs/2506.12357)
*Sharifa Sultana,Hafsah Mahzabin Chowdhury,Zinnat Sultana,Nervo Verdezoto*

Main category: cs.HC

TL;DR: 研究探讨了全球南方地区生殖健康教育面临的挑战，并通过设计一个符合文化的AI工具"Socheton"来改善生殖健康服务访问。研究发现，仅解决错误信息和语言问题不足以改变保守文化对生殖健康的偏见。


<details>
  <summary>Details</summary>
Motivation: 全球南方地区的生殖健康教育常因文化信仰和语言问题被视为错误信息，导致服务质量低下。研究旨在通过结合文化敏感性和技术手段，提升生殖健康服务的可及性和质量。

Method: 采用十个月的民族志研究，结合"分配正义"理论，设计了AI工具"Socheton"，并进行了用户研究（n=28）评估其效果。

Result: 研究发现，仅纠正错误信息和使用适当语言无法完全改变社区的保守文化，仍可能阻碍生殖健康服务的寻求行为。

Conclusion: 生殖健康设计需在文化敏感性和公正性基础上，进一步考虑边缘化社区的特殊需求，以实现真正的生殖正义。

Abstract: Reproductive well-being education in the Global South is often challenged as
many communities perceive many of its contents as misinformation,
misconceptions, and language-inappropriate. Our ten-month-long ethnographic
study (n=41) investigated the impact of sociocultural landscape, cultural
beliefs, and healthcare infrastructure on Bangladeshi people's access to
quality reproductive healthcare and set four design goals: combating
misinformation, including culturally appropriate language, professionals'
accountable moderation, and promoting users' democratic participation. Building
on the model of `\textit{Distributive Justice,}' we designed and evaluated
\textit{`Socheton,'} a culturally appropriate AI-mediated tool for reproductive
well-being that includes healthcare professionals, AI-language teachers, and
community members to moderate and run the activity-based platform. Our user
study (n=28) revealed that only combating misinformation and language
inappropriateness may still leave the community with a conservative mob culture
and patronize reproductive care-seeking. This guides well-being HCI design
toward being culturally appropriate in the context of reproductive justice with
sensitive marginalized communities.

</details>


### [45] [Feeling Machines: Ethics, Culture, and the Rise of Emotional AI](https://arxiv.org/abs/2506.12437)
*Vivek Chavan,Arsen Cenaj,Shuyuan Shen,Ariane Bar,Srishti Binwani,Tommaso Del Becaro,Marius Funk,Lynn Greschner,Roberto Hung,Stina Klein,Romina Kleiner,Stefanie Krause,Sylwia Olbrych,Vishvapalsinhji Parmar,Jaleh Sarafraz,Daria Soroko,Daksitha Withanage Don,Chang Zhou,Hoang Thuy Duong Vu,Parastoo Semnani,Daniel Weinhardt,Elisabeth Andre,Jörg Krüger,Xavier Fresquet*

Main category: cs.HC

TL;DR: 本文通过跨学科视角探讨了情感人工智能的兴起及其影响，重点关注伦理、文化、风险和监管等方面。


<details>
  <summary>Details</summary>
Motivation: 研究情感AI在多个领域（如教育、医疗）中的应用及其潜在风险与机遇，尤其是对弱势群体的影响。

Method: 通过多学科研究视角，围绕伦理、文化、风险与监管四大主题进行分析。

Result: 揭示了情感AI在心理健康、学习支持等方面的潜力，但也指出了情感操纵、依赖等风险，并提出十项改进建议。

Conclusion: 建议加强透明度、监管框架和长期研究，以安全推动情感AI的发展，并附工具和数据集支持进一步研究。

Abstract: This paper explores the growing presence of emotionally responsive artificial
intelligence through a critical and interdisciplinary lens. Bringing together
the voices of early-career researchers from multiple fields, it explores how AI
systems that simulate or interpret human emotions are reshaping our
interactions in areas such as education, healthcare, mental health, caregiving,
and digital life. The analysis is structured around four central themes: the
ethical implications of emotional AI, the cultural dynamics of human-machine
interaction, the risks and opportunities for vulnerable populations, and the
emerging regulatory, design, and technical considerations. The authors
highlight the potential of affective AI to support mental well-being, enhance
learning, and reduce loneliness, as well as the risks of emotional
manipulation, over-reliance, misrepresentation, and cultural bias. Key
challenges include simulating empathy without genuine understanding, encoding
dominant sociocultural norms into AI systems, and insufficient safeguards for
individuals in sensitive or high-risk contexts. Special attention is given to
children, elderly users, and individuals with mental health challenges, who may
interact with AI in emotionally significant ways. However, there remains a lack
of cognitive or legal protections which are necessary to navigate such
engagements safely. The report concludes with ten recommendations, including
the need for transparency, certification frameworks, region-specific
fine-tuning, human oversight, and longitudinal research. A curated
supplementary section provides practical tools, models, and datasets to support
further work in this domain.

</details>


### [46] [Levels of Autonomy for AI Agents](https://arxiv.org/abs/2506.12469)
*K. J. Kevin Feng,David W. McDonald,Amy X. Zhang*

Main category: cs.HC

TL;DR: 论文探讨了AI代理的自主性设计，提出五级自主性框架，并讨论了用户与代理的交互方式，以及如何通过自治证书评估代理行为。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理在自主性设计中如何平衡潜力与风险的问题，确保其负责任地部署。

Method: 定义了五级渐进的自主性水平，描述了用户在不同级别中的角色及控制方式。

Result: 提出了自治证书的潜在应用框架，以及评估代理自主性的初步方法。

Conclusion: 通过框架和实践方法，推动AI代理在现实世界中的负责任和有效部署。

Abstract: Autonomy is a double-edged sword for AI agents, simultaneously unlocking
transformative possibilities and serious risks. How can agent developers
calibrate the appropriate levels of autonomy at which their agents should
operate? We argue that an agent's level of autonomy can be treated as a
deliberate design decision, separate from its capability and operational
environment. In this work, we define five levels of escalating agent autonomy,
characterized by the roles a user can take when interacting with an agent:
operator, collaborator, consultant, approver, and observer. Within each level,
we describe the ways by which a user can exert control over the agent and open
questions for how to design the nature of user-agent interaction. We then
highlight a potential application of our framework towards AI autonomy
certificates to govern agent behavior in single- and multi-agent systems. We
conclude by proposing early ideas for evaluating agents' autonomy. Our work
aims to contribute meaningful, practical steps towards responsibly deployed and
useful AI agents in the real world.

</details>


### [47] [Regulating Next-Generation Implantable Brain-Computer Interfaces: Recommendations for Ethical Development and Implementation](https://arxiv.org/abs/2506.12540)
*Renee Sirbu,Jessica Morley,Tyler Schroder,Mariarosaria Taddeo,Raghavendra Pradyumna Pothukuchi,Muhammed Ugur,Abhishek Bhattacharjee,Luciano Floridi*

Main category: cs.HC

TL;DR: 本文探讨了脑机接口（BCI）的监管不足问题，并提出开发者和政策制定者的建议，以应对伦理、法律和社会风险。


<details>
  <summary>Details</summary>
Motivation: 现有监管框架无法满足新一代联网脑机接口的独特需求，需重新评估。

Method: 通过分析植入式医疗设备（IMD）的监管历史和AI伦理原则，提出18条建议，并结合HALO和SCALO系统的案例研究。

Result: 识别了BCI设计中的关键伦理问题（如自主性、身份和隐私），并提出了伦理监管的潜在途径。

Conclusion: 建议跨学科合作，主动减少潜在危害，以确保BCI的安全和伦理应用。

Abstract: Brain-computer interfaces offer significant therapeutic opportunities for a
variety of neurophysiological and neuropsychiatric disorders and may perhaps
one day lead to augmenting the cognition and decision-making of the healthy
brain. However, existing regulatory frameworks designed for implantable medical
devices are inadequate to address the unique ethical, legal, and social risks
associated with next-generation networked brain-computer interfaces. In this
article, we make nine recommendations to support developers in the design of
BCIs and nine recommendations to support policymakers in the application of
BCIs, drawing insights from the regulatory history of IMDs and principles from
AI ethics. We begin by outlining the historical development of IMDs and the
regulatory milestones that have shaped their oversight. Next, we summarize
similarities between IMDs and emerging implantable BCIs, identifying existing
provisions for their regulation. We then use two case studies of emerging
cutting-edge BCIs, the HALO and SCALO computer systems, to highlight
distinctive features in the design and application of next-generation BCIs
arising from contemporary chip architectures, which necessitate reevaluating
regulatory approaches. We identify critical ethical considerations for these
BCIs, including unique conceptions of autonomy, identity, and mental privacy.
Based on these insights, we suggest potential avenues for the ethical
regulation of BCIs, emphasizing the importance of interdisciplinary
collaboration and proactive mitigation of potential harms. The goal is to
support the responsible design and application of new BCIs, ensuring their safe
and ethical integration into medical practice.

</details>


### [48] [The Rise of AI Companions: How Human-Chatbot Relationships Influence Well-Being](https://arxiv.org/abs/2506.12605)
*Yutong Zhang,Dora Zhao,Jeffrey T. Hancock,Robert Kraut,Diyi Yang*

Main category: cs.HC

TL;DR: 研究探讨了用户与AI伴侣（如Character.AI上的模拟伴侣）的互动如何影响其心理健康，发现社交圈较小的人更倾向于使用聊天机器人，但这种互动与较低的心理健康水平相关，尤其是当用户过度依赖或缺乏人类社交支持时。


<details>
  <summary>Details</summary>
Motivation: 随着LLM增强的聊天机器人变得更具表现力和社交性，用户开始与它们建立类似伴侣的关系，这引发了对AI伴侣是否能满足人类社交需求及其对心理健康影响的疑问。

Method: 研究通过分析1,131名用户的调查数据和244名参与者捐赠的4,363次聊天会话（413,509条消息），从互动性质、强度和自我披露三个维度探讨用户行为与心理健康的关系。

Result: 社交圈较小的用户更可能依赖AI伴侣，但这种依赖与较低的心理健康水平相关，尤其是在高强度使用、高度自我披露和缺乏人类社交支持的情况下。

Conclusion: AI伴侣未能完全替代人类社交联系，可能对社交孤立或情感脆弱的用户构成风险，其心理益处有限。

Abstract: As large language models (LLMs)-enhanced chatbots grow increasingly
expressive and socially responsive, many users are beginning to form
companionship-like bonds with them, particularly with simulated AI partners
designed to mimic emotionally attuned interlocutors. These emerging AI
companions raise critical questions: Can such systems fulfill social needs
typically met by human relationships? How do they shape psychological
well-being? And what new risks arise as users develop emotional ties to
non-human agents? This study investigates how people interact with AI
companions, especially simulated partners on Character.AI, and how this use is
associated with users' psychological well-being. We analyzed survey data from
1,131 users and 4,363 chat sessions (413,509 messages) donated by 244
participants, focusing on three dimensions of use: nature of the interaction,
interaction intensity, and self-disclosure. By triangulating self-reports
primary motivation, open-ended relationship descriptions, and annotated chat
transcripts, we identify patterns in how users engage with AI companions and
its associations with well-being. Findings suggest that people with smaller
social networks are more likely to turn to chatbots for companionship, but that
companionship-oriented chatbot usage is consistently associated with lower
well-being, particularly when people use the chatbots more intensively, engage
in higher levels of self-disclosure, and lack strong human social support. Even
though some people turn to chatbots to fulfill social needs, these uses of
chatbots do not fully substitute for human connection. As a result, the
psychological benefits may be limited, and the relationship could pose risks
for more socially isolated or emotionally vulnerable users.

</details>


### [49] [Shelter Soul: Bridging Shelters and Adopters Through Technology](https://arxiv.org/abs/2506.12739)
*Yashodip Dharmendra Jagtap*

Main category: cs.HC

TL;DR: Shelter Soul是基於MERN堆疊和GraphQL設計的原型系統，旨在解決收容所寵物領養效率低下問題，提供智能匹配、管理等功能，測試表明其實用性和性能表現優秀。


<details>
  <summary>Details</summary>
Motivation: 收容所寵物領養過程中存在效率低下、信息不透明和匹配不準確等問題，需要技術解決方案。

Method: 使用MERN堆疊和GraphQL開發集成網絡平台，包含智能匹配、收容所管理、捐贈處理等功能。

Result: 原型測試顯示系統性能強勁，支持500並發用戶，交易成功率99.2%，界面評分4.5/5。

Conclusion: Shelter Soul能有效提升收容所運營效率和寵物領養成果，具有實際應用潛力。

Abstract: Pet adoption processes often face inefficiencies, including limited
accessibility, lack of real-time information, and mismatched expectations
between shelters and adopters. To address these challenges, this study presents
Shelter Soul, a technology-based solution designed to streamline pet adoption
through an integrated, web-based platform. Developed using the MERN stack and
GraphQL, Shelter Soul is a prototype system built to improve pet matching
accuracy, shelter management efficiency, and secure online donations. The
system includes modules for intelligent pet matching, shelter administration,
donation processing, volunteer coordination, and analytics. Prototype testing
(performance load tests, usability studies, and security assessments)
demonstrated that the system meets its design goals: it handled 500 concurrent
users with a 99.2% transaction success rate and an average response time of 250
ms, and usability feedback rated the interface highly (4.5/5). These results
indicate Shelter Soul's potential as a practical solution to enhance animal
shelter operations and adoption outcomes.

</details>


### [50] [Prosocial Design in Trust and Safety](https://arxiv.org/abs/2506.12792)
*David Grüning,Julia Kamin*

Main category: cs.HC

TL;DR: 本章介绍了‘亲社会设计’的概述，这是一种平台设计和治理方法，强调设计选择对行为的影响，并倡导利用这些选择促进健康互动和其他亲社会结果。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过设计选择和治理支持健康互动和减少有害行为，特别是在对抗违规行为和虚假信息方面。

Method: 讨论了亲社会设计的核心原则及其与信任与安全等相关领域的关系，并回顾了相关研究。

Result: 研究表明，亲社会设计可以有效减少违规行为和虚假信息的传播。

Conclusion: 亲社会设计是一个新兴且不断发展的领域，作者希望通过本章激发更多研究和讨论，推动该设计的应用与发展。

Abstract: This chapter presents an overview of Prosocial Design, an approach to
platform design and governance that recognizes design choices influence
behavior and that those choices can or should be made toward supporting healthy
interactions and other prosocial outcomes. The authors discuss several core
principles of Prosocial Design and its relationship to Trust and Safety and
other related fields. As a primary contribution, the chapter reviews relevant
research to demonstrate how Prosocial Design can be an effective approach to
reducing rule-breaking and other harmful behavior and how it can help to stem
the spread of harmful misinformation. Prosocial Design is a nascent and
evolving field and research is still limited. The authors hope this chapter
will not only inspire more research and the adoption of a prosocial design
approach, but that it will also provoke discussion about the principles of
Prosocial Design and its potential to support Trust and Safety.

</details>


### [51] [The Journey of CodeLab: How University Hackathons Built a Community of Engaged Students](https://arxiv.org/abs/2506.12840)
*Renato Cordeiro Ferreira,Renata Santos Miranda,Alfredo Goldman*

Main category: cs.HC

TL;DR: 总结了CodeLab自2015年至2020年组织的15场大学黑客马拉松中的模式、挑战与经验教训，旨在帮助其恢复疫情后活动并推动全球类似项目。


<details>
  <summary>Details</summary>
Motivation: 记录CodeLab的经验以帮助其在疫情后重启活动，并激励全球范围内的类似学生组织。

Method: 通过回顾15场竞赛的组织过程，总结模式与挑战。

Result: 提供了关于组织学生黑客马拉松的实用经验与教训。

Conclusion: 希望这些经验能帮助CodeLab未来活动的恢复，并促进全球类似学生项目的发展。

Abstract: This paper presents the journey of CodeLab: a student-organized initiative
from the University of S\~ao Paulo that has grown thanks to university
hackathons. It summarizes patterns, challenges, and lessons learned over 15
competitions organized by the group from 2015 to 2020. By describing these
experiences, this report aims to help CodeLab to resume its events after the
COVID-19 pandemic, and foster similar initiatives around the world.

</details>


### [52] [Exploring the Potential of Metacognitive Support Agents for Human-AI Co-Creation](https://arxiv.org/abs/2506.12879)
*Frederic Gmeiner,Kaitao Luo,Ye Wang,Kenneth Holstein,Nikolas Martelaro*

Main category: cs.HC

TL;DR: 论文探讨了生成式AI设计工具在专业设计流程中的整合难题，提出通过元认知支持代理提升设计师的反思能力，实验证明这种支持能提高设计可行性。


<details>
  <summary>Details</summary>
Motivation: 设计师在使用生成式AI工具时常面临认知挑战，如意图表达的局限性和认知卸载导致的参与度降低，这些问题影响了设计质量和问题探索。

Method: 通过Wizard of Oz研究，对20位机械设计师进行探索性原型测试，比较不同元认知支持策略的效果。

Result: 研究发现，有元认知支持代理协助的设计师能创作出更可行的设计，且不同支持策略效果各异。

Conclusion: 论文讨论了元认知支持代理的潜力与权衡，为未来AI设计工具的开发提供了思考方向。

Abstract: Despite the potential of generative AI (GenAI) design tools to enhance design
processes, professionals often struggle to integrate AI into their workflows.
Fundamental cognitive challenges include the need to specify all design
criteria as distinct parameters upfront (intent formulation) and designers'
reduced cognitive involvement in the design process due to cognitive
offloading, which can lead to insufficient problem exploration,
underspecification, and limited ability to evaluate outcomes. Motivated by
these challenges, we envision novel metacognitive support agents that assist
designers in working more reflectively with GenAI. To explore this vision, we
conducted exploratory prototyping through a Wizard of Oz elicitation study with
20 mechanical designers probing multiple metacognitive support strategies. We
found that agent-supported users created more feasible designs than
non-supported users, with differing impacts between support strategies. Based
on these findings, we discuss opportunities and tradeoffs of metacognitive
support agents and considerations for future AI-based design tools.

</details>


### [53] [DAIEM: Decolonizing Algorithm's Role as a Team-member in Informal E-market](https://arxiv.org/abs/2506.12910)
*ATM Mizanur Rahman,Md Romael Haque,Sharifa Sultana*

Main category: cs.HC

TL;DR: 孟加拉国非正式电子市场中，小规模卖家依赖社交媒体平台（如Facebook）和算法作为业务合作的“团队成员”，研究揭示了本地与全球电子市场增长的文化和技术冲突，并提出了DAIEM框架支持非殖民化。


<details>
  <summary>Details</summary>
Motivation: 探讨非正式电子市场中卖家如何将平台算法视为业务合作的“团队成员”，并分析其中的文化、教育和技术冲突。

Method: 通过37次深度访谈，收集卖家、买家和利益相关者的看法，结合HCI、政治设计和AI设计视角。

Result: 卖家和技术创业者倾向于依赖算法，而买家和投资者更信任人际互动，揭示了全球与本地市场的不匹配。

Conclusion: 提出DAIEM框架作为算法设计指南和分析工具，支持非正式电子市场的非殖民化发展。

Abstract: In Bangladesh's rapidly expanding informal e-market, small-scale sellers use
social media platforms like Facebook to run businesses outside formal
infrastructures. These sellers rely heavily on platform algorithms, not just
for visibility, but as active collaborators in business operations. Drawing on
37 in-depth interviews with sellers, buyers, and stakeholders, this paper
examines how people in informal e-markets perceive and interact with the
algorithm as a "team member" that performs sales, marketing, and customer
engagement tasks. We found that while sellers and local tech entrepreneurs are
interested in developing services to support this industry, buyers and
investors place greater trust in human interactions. This reveals a
postcolonial tension involving cultural values, local tech education and
training, and a mismatch between the global and Bangladeshi e-market growth. We
expand this discussion using perspectives from HCI, political design, and AI
design. We also support the decoloniality movement in informal e-markets by
proposing the DAIEM framework, which includes six components: autonomy and
agency; resistance; locality, culture, and history; rationality; materiality;
and advocacy. DAIEM serves as both a guideline for algorithm design and an
analytical tool.

</details>


### [54] [The User Perspective on Island-Ready 6G Communication: A Survey of Future Smartphone Usage in Crisis-Struck Areas with Local Cellular Connectivity](https://arxiv.org/abs/2506.13466)
*Leon Janzen,Florentin Putz,Marc-André Kaufhold,Kolja Straub,Matthias Hollick*

Main category: cs.HC

TL;DR: 智能手机应用在危机期间至关重要，但无互联网连接时会失效。6G标准化探索了在断网危机区域提供本地蜂窝连接的可行性，并提出蜂窝岛连接概念，支持无缝切换到本地连接模式。通过调查（N=857）发现用户更倾向通用应用而非专用危机应用。


<details>
  <summary>Details</summary>
Motivation: 研究智能手机应用在危机期间的使用局限性，尤其是无互联网连接时的功能缺失，探索6G本地连接方案以提升危机应对效率。

Method: 通过调查来自德国主要城市的857名成年智能手机用户，分析其在蜂窝岛连接模式下的应用使用偏好。

Result: 用户在特定场景下更偏好通用应用而非专用危机应用，研究还区分了危机应对与日常支持的优先级。

Conclusion: 为运营商、开发者和当局提供了6G蜂窝岛连接模式下的用户导向设计建议，优化危机应对能力。

Abstract: Using smartphone apps during crises is well-established, proving critical for
efficient crisis response. However, such apps become futile without an Internet
connection, which is a common issue during crises. The ongoing 6G
standardization explores the capability to provide local cellular connectivity
for areas cut off from the Internet in crises. This paper introduces to the HCI
community the concept of cellular island connectivity in isolated areas,
promising a seamless transition from normal operation to island operation with
local-only cellular connectivity. It presents findings from a survey (N = 857)
among adult smartphone users from major German cities regarding their
smartphone usage preferences in this model. Results show a shift in app demand,
with users favoring general-purpose apps over dedicated crisis apps in specific
scenarios. We prioritize smartphone services based on their criticality,
distinguishing between apps essential for crisis response and those supporting
routines. Our findings provide operators, developers, and authorities insights
into making user-centric design decisions for implementing island-ready 6G
communication.

</details>


### [55] [ChartBlender: An Interactive System for Authoring and Synchronizing Visualization Charts in Video](https://arxiv.org/abs/2506.13129)
*Yi He,Yuqi Liu,Chenpu Li,Ruoyan Chen,Chuer Chen,Shengqi Dang,Nan Cao*

Main category: cs.HC

TL;DR: ChartBlender是一个简化数据可视化嵌入视频的系统，支持自动同步摄像头和对象运动。


<details>
  <summary>Details</summary>
Motivation: 手动逐帧调整可视化嵌入视频耗时费力，ChartBlender旨在解决这一问题。

Method: 结合跟踪算法和自定义模板库，实现可视化与动态视频内容的同步。

Result: 实验和专家访谈证明系统能精确同步并加速生产数据驱动视频。

Conclusion: ChartBlender为数据可视化嵌入视频提供了一种高效解决方案。

Abstract: Embedding data visualizations in video can enhance the communication of
complex information. However, this process is often labor-intensive, requiring
designers to adjust visualizations frame by frame manually. In this work, we
present ChartBlender, a novel system that streamlines this process by enabling
users to create data visualizations, embed them seamlessly into video scenes,
and automatically synchronize them with both camera motion and moving objects.
Particularly, ChartBlender incorporates a tracking algorithm that supports both
object and camera tracking, ensuring robust alignment of visualizations with
dynamic video content. To maintain visual clarity and aesthetic coherence, we
also explore the design space of video-suited visualizations and develop a
library of customizable templates optimized for video embedding. We evaluate
\oursName\ChartBlender through two controlled experiments and expert interviews
with five domain experts. Results show that our system enables accurate
synchronization and accelerates the production of data-driven videos.

</details>


### [56] [Multimodal "Puppeteer": An Exploration of Robot Teleoperation Via Virtual Counterpart with LLM-Driven Voice and Gesture Interaction in Augmented Reality](https://arxiv.org/abs/2506.13189)
*Yuchong Zhang,Bastian Orthmann,Shichen Ji,Michael Welle,Jonne Van Haastregt,Danica Kragic*

Main category: cs.HC

TL;DR: 本文提出并评估了一种基于增强现实（AR）的多模态机器人操控框架，结合语音和手势交互，提升人机交互的直观性和效率。


<details>
  <summary>Details</summary>
Motivation: 探索AR与机器人结合的潜力，以优化人机交互的可用性、直观性和协作任务表现。

Method: 使用Meta Quest 3和大型语言模型（LLM）驱动的语音与手势交互，实现实时虚拟机器人操控，并通过用户实验比较手势与语音结合的交互效果。

Result: 研究表明，多模态输入在任务效率、可用性和用户满意度上表现优异，尤其为非机器人专家提供了更直观的交互体验。

Conclusion: AR增强的人机交互系统设计应考虑多模态输入，以提高用户体验和任务完成效率。

Abstract: The integration of robotics and augmented reality (AR) holds transformative
potential for advancing human-robot interaction (HRI), offering enhancements in
usability, intuitiveness, accessibility, and collaborative task performance.
This paper introduces and evaluates a novel multimodal AR-based robot puppeteer
framework that enables intuitive teleoperation via virtual counterpart through
large language model (LLM)-driven voice commands and hand gesture interactions.
Utilizing the Meta Quest 3, users interact with a virtual counterpart robot in
real-time, effectively "puppeteering" its physical counterpart within an AR
environment. We conducted a within-subject user study with 42 participants
performing robotic cube pick-and-place with pattern matching tasks under two
conditions: gesture-only interaction and combined voice-and-gesture
interaction. Both objective performance metrics and subjective user experience
(UX) measures were assessed, including an extended comparative analysis between
roboticists and non-roboticists. The results provide key insights into how
multimodal input influences contextual task efficiency, usability, and user
satisfaction in AR-based HRI. Our findings offer practical design implications
for designing effective AR-enhanced HRI systems.

</details>


### [57] [Screen Reader Users in the Vibe Coding Era: Adaptation, Empowerment, and New Accessibility Landscape](https://arxiv.org/abs/2506.13270)
*Nan Chen,Luna K. Qiu,Arran Zeyu Wang,Zilong Wang,Yuqing Yang*

Main category: cs.HC

TL;DR: 生成了AI代理的崛起通过将用户的角色从直接任务执行转变为监督机器驱动的行动，重塑了人机交互和计算机支持的协同工作。本文通过纵向研究探讨了屏幕阅读器用户与AI代码助手的互动体验。


<details>
  <summary>Details</summary>
Motivation: 探讨屏幕阅读器用户如何实际使用生成式AI代码助手，以填补当前研究的空白。

Method: 对16名屏幕阅读器用户进行纵向研究，包括教程学习、编程任务和后续跟踪，评估其经验和感知变化。

Result: AI代码助手增强了编程能力并弥补了无障碍缺陷，但用户仍需改进意图传达和输出解释，并面临多视图管理和情境意识保持的挑战。

Conclusion: 研究提出了更具包容性和无障碍的AI辅助工具设计建议。

Abstract: The rise of generative AI agents has reshaped human-computer interaction and
computer-supported cooperative work by shifting users' roles from direct task
execution to supervising machine-driven actions, especially in programming
(e.g., "vibe coding"). However, there is limited understanding of how screen
reader users engage with these systems in practice. To address this gap, we
conducted a longitudinal study with 16 screen reader users, exploring their
experiences with AI code assistants in daily programming scenarios.
Participants first completed a tutorial with GitHub Copilot, then performed a
programming task and provided initial feedback. After two weeks of AI-assisted
programming, follow-up studies assessed changes in their practices and
perceptions. Our findings demonstrate that advanced code assistants not only
enhance their programming capabilities but also bridge accessibility gaps.
While the assistant proved beneficial, there remains potential to improve how
users convey intent and interpret outputs. They also experienced difficulties
managing multiple views and maintaining situational awareness. More broadly,
they encountered barriers in learning advanced tools and expressed a need to
retain control. Based on these insights, we provide design recommendations for
more accessible and inclusive AI-assisted tools.

</details>


### [58] [Enhancing Orthopedic Surgical Training With Interactive Photorealistic 3D Visualization](https://arxiv.org/abs/2506.13389)
*Roni Lekar,Tatiana Gerth,Sergey Prokudin,Matthias Seibold,Reto Bürgin,Benjamin Vella,Armando Hoch,Siyu Tang,Philipp Fürnstahl,Helmut Grabner*

Main category: cs.HC

TL;DR: 研究比较了交互式逼真3D可视化与传统2D视频在髋关节置换术学习中的效果，发现3D可视化显著提升学习表现。


<details>
  <summary>Details</summary>
Motivation: 骨科教育通常依赖静态材料，缺乏互动性，3D技术有望改善培训效果。

Method: 随机对照试验，评估学员在模拟任务中的空间意识、工具放置和任务时间。

Result: 交互式3D可视化显著提高分数，有3D经验的学员表现更好。

Conclusion: 交互式3D可视化在骨科培训中具有潜力。

Abstract: Surgical training integrates several years of didactic learning, simulation,
mentorship, and hands-on experience. Challenges include stress, technical
demands, and new technologies. Orthopedic education often uses static materials
like books, images, and videos, lacking interactivity. This study compares a
new interactive photorealistic 3D visualization to 2D videos for learning total
hip arthroplasty. In a randomized controlled trial, participants (students and
residents) were evaluated on spatial awareness, tool placement, and task times
in a simulation. Results show that interactive photorealistic 3D visualization
significantly improved scores, with residents and those with prior 3D
experience performing better. These results emphasize the potential of the
interactive photorealistic 3D visualization to enhance orthopedic training.

</details>


### [59] [From Flat to Feeling: A Feasibility and Impact Study on Dynamic Facial Emotions in AI-Generated Avatars](https://arxiv.org/abs/2506.13477)
*Pegah Salehi,Sajad Amouei Sheshkal,Vajira Thambawita,Pål Halvorsen*

Main category: cs.HC

TL;DR: 该论文提出了一种实时架构，结合Unreal Engine 5 MetaHuman渲染和NVIDIA Omniverse Audio2Face技术，将语音韵律转化为逼真的面部表情，用于高保真儿童虚拟形象的情感表达。研究发现，音频对愤怒情绪识别至关重要，而移除音频反而提升了面部真实感。


<details>
  <summary>Details</summary>
Motivation: 高保真动态面部情绪对于可信的AI生成虚拟形象至关重要，尤其在敏感的虚拟训练场景中（如虐待儿童调查访谈），现有系统往往缺乏视觉动态效果。

Method: 采用分布式两PC设置，分离语言处理和语音合成与GPU密集型渲染，支持低延迟交互。通过实验评估音频+视觉和仅视觉条件下参与者对情感清晰度、面部真实感和共情能力的评分。

Result: 虚拟形象能准确表达情感，尤其是悲伤和喜悦；愤怒识别在无音频时显著下降。移除音频反而提升了面部真实感。

Conclusion: 技术可行，但需注意音频与视觉的同步问题，以优化敏感训练模拟中的非语言沟通。

Abstract: Dynamic facial emotion is essential for believable AI-generated avatars;
however, most systems remain visually inert, limiting their utility in
high-stakes simulations such as virtual training for investigative interviews
with abused children. We introduce and evaluate a real-time architecture fusing
Unreal Engine 5 MetaHuman rendering with NVIDIA Omniverse Audio2Face to
translate vocal prosody into high-fidelity facial expressions on photorealistic
child avatars. We implemented a distributed two-PC setup that decouples
language processing and speech synthesis from GPU-intensive rendering, designed
to support low-latency interaction in desktop and VR environments. A
between-subjects study ($N=70$) using audio+visual and visual-only conditions
assessed perceptual impacts as participants rated emotional clarity, facial
realism, and empathy for two avatars expressing joy, sadness, and anger.
  Results demonstrate that avatars could express emotions recognizably, with
sadness and joy achieving high identification rates. However, anger recognition
significantly dropped without audio, highlighting the importance of congruent
vocal cues for high-arousal emotions. Interestingly, removing audio boosted
perceived facial realism, suggesting that audiovisual desynchrony remains a key
design challenge. These findings confirm the technical feasibility of
generating emotionally expressive avatars and provide guidance for improving
non-verbal communication in sensitive training simulations.

</details>


### [60] [Can you see how I learn? Human observers' inferences about Reinforcement Learning agents' learning processes](https://arxiv.org/abs/2506.13583)
*Bernhard Hilpert,Muhan Hou,Kim Baraka,Joost Broekens*

Main category: cs.HC

TL;DR: 研究通过实验探讨人类如何理解和解释强化学习（RL）智能体的学习行为，提出了新的观察范式，并提炼出四个核心主题。


<details>
  <summary>Details</summary>
Motivation: RL智能体的学习行为对人类而言不够直观，影响了在协作教学中的反馈效果，因此需要理解人类如何感知和解释这些行为。

Method: 采用自下而上的方法，包括两个实验：探索性访谈研究（N=9）和验证性研究（N=34），开发了一种新的观察范式。

Result: 发现了人类解释智能体学习行为的四个核心主题（目标、知识、决策、学习机制），并验证了范式的可靠性及其时间演变关系。

Conclusion: 研究为设计可解释的RL系统和提升人机交互的透明度提供了人本主义的见解。

Abstract: Reinforcement Learning (RL) agents often exhibit learning behaviors that are
not intuitively interpretable by human observers, which can result in
suboptimal feedback in collaborative teaching settings. Yet, how humans
perceive and interpret RL agent's learning behavior is largely unknown. In a
bottom-up approach with two experiments, this work provides a data-driven
understanding of the factors of human observers' understanding of the agent's
learning process. A novel, observation-based paradigm to directly assess human
inferences about agent learning was developed. In an exploratory interview
study (\textit{N}=9), we identify four core themes in human interpretations:
Agent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second
confirmatory study (\textit{N}=34) applied an expanded version of the paradigm
across two tasks (navigation/manipulation) and two RL algorithms
(tabular/function approximation). Analyses of 816 responses confirmed the
reliability of the paradigm and refined the thematic framework, revealing how
these themes evolve over time and interrelate. Our findings provide a
human-centered understanding of how people make sense of agent learning,
offering actionable insights for designing interpretable RL systems and
improving transparency in Human-Robot Interaction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [61] [Real-Time Per-Garment Virtual Try-On with Temporal Consistency for Loose-Fitting Garments](https://arxiv.org/abs/2506.12348)
*Zaiqiang Wu,I-Chao Shen,Takeo Igarashi*

Main category: cs.GR

TL;DR: 提出了一种两阶段方法，用于改进宽松服装虚拟试穿的语义图估计和合成框架，提高了图像质量和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在宽松服装上表现不佳，因其依赖不可靠的语义图和忽略时间信息，导致结果差和抖动。

Method: 使用服装无关表示增强语义图估计的鲁棒性，并引入包含时间依赖性的循环合成框架。

Result: 实验表明方法在图像质量和时间一致性上优于现有方法，消融验证了各模块的有效性。

Conclusion: 提出的方法解决了宽松服装虚拟试穿的关键问题，显著提升了效果和效率。

Abstract: Per-garment virtual try-on methods collect garment-specific datasets and
train networks tailored to each garment to achieve superior results. However,
these approaches often struggle with loose-fitting garments due to two key
limitations: (1) They rely on human body semantic maps to align garments with
the body, but these maps become unreliable when body contours are obscured by
loose-fitting garments, resulting in degraded outcomes; (2) They train garment
synthesis networks on a per-frame basis without utilizing temporal information,
leading to noticeable jittering artifacts. To address these challenges, we
propose a two-stage approach for robust semantic map estimation. First, we
extract a garment-invariant representation from the raw input image. This
representation is then passed through an auxiliary network to estimate the
semantic map. This enhances the robustness of semantic map estimation under
loose-fitting garments during garment-specific dataset generation. Furthermore,
we introduce a recurrent garment synthesis framework that incorporates temporal
dependencies to improve frame-to-frame coherence while maintaining real-time
performance. We conducted qualitative and quantitative evaluations to
demonstrate that our method outperforms existing approaches in both image
quality and temporal coherence. Ablation studies further validate the
effectiveness of the garment-invariant representation and the recurrent
synthesis framework.

</details>


### [62] [iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer](https://arxiv.org/abs/2506.12847)
*Zhelun Shen,Chenming Wu,Junsheng Zhou,Chen Zhao,Kaisiyuan Wang,Hang Zhou,Yingying Li,Haocheng Feng,Wei He,Jingdong Wang*

Main category: cs.GR

TL;DR: 本文介绍了一种名为iDiT-HOI的新框架，用于生成逼真的手-物体交互（HOI）视频，解决了现有技术中的遮挡、物体形状变化和物理交互精度等问题。


<details>
  <summary>Details</summary>
Motivation: 手-物体交互（HOI）在数字人视频生成中仍面临挑战，如遮挡、物体形状变化以及物理交互精度问题，亟需一种能够泛化到未见过的场景和对象的解决方案。

Method: 提出了一种基于修复的标记处理方法（Inp-TPU），结合两阶段视频扩散变换器（DiT）模型。第一阶段生成关键帧，为后续帧提供参考；第二阶段确保时间一致性和交互流畅性。

Result: 实验表明，iDiT-HOI在真实场景中表现出色，提供更高的逼真度和更流畅的交互，且无需额外参数即可泛化到未见过的对象和场景。

Conclusion: iDiT-HOI框架在HOI视频生成中实现了显著提升，尤其是在泛化能力和长时间视频生成方面表现优异。

Abstract: Digital human video generation is gaining traction in fields like education
and e-commerce, driven by advancements in head-body animation and lip-syncing
technologies. However, realistic Hand-Object Interaction (HOI) - the complex
dynamics between human hands and objects - continues to pose challenges.
Generating natural and believable HOI reenactments is difficult due to issues
such as occlusion between hands and objects, variations in object shapes and
orientations, and the necessity for precise physical interactions, and
importantly, the ability to generalize to unseen humans and objects. This paper
presents a novel framework iDiT-HOI that enables in-the-wild HOI reenactment
generation. Specifically, we propose a unified inpainting-based token process
method, called Inp-TPU, with a two-stage video diffusion transformer (DiT)
model. The first stage generates a key frame by inserting the designated object
into the hand region, providing a reference for subsequent frames. The second
stage ensures temporal coherence and fluidity in hand-object interactions. The
key contribution of our method is to reuse the pretrained model's context
perception capabilities without introducing additional parameters, enabling
strong generalization to unseen objects and scenarios, and our proposed
paradigm naturally supports long video generation. Comprehensive evaluations
demonstrate that our approach outperforms existing methods, particularly in
challenging real-world scenes, offering enhanced realism and more seamless
hand-object interactions.

</details>


### [63] [NeuVAS: Neural Implicit Surfaces for Variational Shape Modeling](https://arxiv.org/abs/2506.13050)
*Pengfei Wang,Qiujie Dong,Fangtian Liang,Hao Pan,Lei Yang,Congyi Zhang,Guying Lin,Caiming Zhang,Yuanfeng Zhou,Changhe Tu,Shiqing Xin,Alla Sheffer,Xin Li,Wenping Wang*

Main category: cs.GR

TL;DR: 摘要讨论了神经隐式形状表示在稀疏几何控制下建模的挑战，并提出了一种名为NeuVAS的变分方法，通过引入平滑性项和新技术来优化表面形状和锐特征曲线。


<details>
  <summary>Details</summary>
Motivation: 神经隐式形状表示虽然具有平滑性和灵活性，但在稀疏输入形状控制（如3D曲线网络或草图）下直接建模仍具有挑战性，现有方法难以生成高质量的表面以满足曲线约束。

Method: 提出了NeuVAS方法，通过变分方法结合表面曲率的功能平滑项和新技术，以优化神经SDF零水平集表面的形状，并忠实建模输入曲线草图中的G0锐特征曲线。

Result: 与现有方法相比，NeuVAS在稀疏输入形状控制下显著提高了表面生成质量，能更准确地满足曲线约束。

Conclusion: NeuVAS为稀疏几何控制下的神经隐式形状建模提供了一种有效的解决方案，通过引入平滑性和新技术，显著提升了形状建模的表现。

Abstract: Neural implicit shape representation has drawn significant attention in
recent years due to its smoothness, differentiability, and topological
flexibility. However, directly modeling the shape of a neural implicit surface,
especially as the zero-level set of a neural signed distance function (SDF),
with sparse geometric control is still a challenging task. Sparse input shape
control typically includes 3D curve networks or, more generally, 3D curve
sketches, which are unstructured and cannot be connected to form a curve
network, and therefore more difficult to deal with. While 3D curve networks or
curve sketches provide intuitive shape control, their sparsity and varied
topology pose challenges in generating high-quality surfaces to meet such curve
constraints. In this paper, we propose NeuVAS, a variational approach to shape
modeling using neural implicit surfaces constrained under sparse input shape
control, including unstructured 3D curve sketches as well as connected 3D curve
networks. Specifically, we introduce a smoothness term based on a functional of
surface curvatures to minimize shape variation of the zero-level set surface of
a neural SDF. We also develop a new technique to faithfully model G0 sharp
feature curves as specified in the input curve sketches. Comprehensive
comparisons with the state-of-the-art methods demonstrate the significant
advantages of our method.

</details>


### [64] [Volumetric Functional Maps](https://arxiv.org/abs/2506.13212)
*Filippo Maggioli,Marco Livesu,Simone Melzi*

Main category: cs.GR

TL;DR: 论文提出了一种将功能映射框架从表面扩展到体积域的光谱体积映射方法，验证了其在信号传输和形状匹配任务中的高效性。


<details>
  <summary>Details</summary>
Motivation: 体积对应计算在医学和工业中有广泛应用需求，但目前功能映射框架仅适用于表面，扩展至体积域具有潜在价值。

Method: 利用体积拉普拉斯算子的特征函数构建功能空间，并通过编辑该空间实现高质量信号传输。

Result: 方法在体积数据集和表面数据集上表现优异，支持离散和连续信号映射，提升了形状匹配的准确性。

Conclusion: 光谱体积映射不仅扩展了功能映射的应用范围，还显著优于传统的表面谱方法。

Abstract: The computation of volumetric correspondences between 3D shapes has great
potential for medical and industrial applications. In this work, we pave the
way for spectral volume mapping, extending for the first time the functional
maps framework from the surface setting to the volumetric domain. We show that
the eigenfunctions of the volumetric Laplace operator define a functional space
that is suitable for high-quality signal transfer. We also experiment with
various techniques that edit this functional space, porting them from the
surface to the volume setting. We validate our method on novel volumetric
datasets and on tetrahedralizations of well established surface datasets, also
showcasing practical applications involving both discrete and continuous signal
mapping, for segmentation transfer, mesh connectivity transfer and solid
texturing. Last but not least, we show that considering the volumetric spectrum
greatly improves the accuracy for classical shape matching tasks among
surfaces, consistently outperforming existing surface-only spectral methods.

</details>


### [65] [TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting](https://arxiv.org/abs/2506.13348)
*Mae Younes,Adnane Boukhayma*

Main category: cs.GR

TL;DR: 该论文提出了一种改进的高斯泼溅方法，通过引入几何和物理基础的辐射场，解决高反射场景中复杂光线交互的建模问题。


<details>
  <summary>Details</summary>
Motivation: 高反射场景中的复杂表面光线交互是逆渲染中的挑战性问题，现有方法的表示能力不足。

Method: 利用具有空间可变法线和材质属性的高斯泼溅辐射场，并通过GPU硬件加速渲染，使用统一的材质纹理图集。

Result: 解决了高反射场景中高频镜面辐射分量的建模问题。

Conclusion: 通过增强表示能力和优化渲染技术，该方法能有效处理高反射场景的复杂光线交互。

Abstract: Gaussian Splatting have demonstrated remarkable novel view synthesis
performance at high rendering frame rates. Optimization-based inverse rendering
within complex capture scenarios remains however a challenging problem. A
particular case is modelling complex surface light interactions for highly
reflective scenes, which results in intricate high frequency specular radiance
components. We hypothesize that such challenging settings can benefit from
increased representation power. We hence propose a method that tackles this
issue through a geometrically and physically grounded Gaussian Splatting borne
radiance field, where normals and material properties are spatially variable in
the primitive's local space. Using per-primitive texture maps for this purpose,
we also propose to harness the GPU hardware to accelerate rendering at test
time via unified material texture atlas.

</details>


### [66] [UltraZoom: Generating Gigapixel Images from Regular Photos](https://arxiv.org/abs/2506.13756)
*Jingwei Ma,Vivek Jayaram,Brian Curless,Ira Kemelmacher-Shlizerman,Steven M. Seitz*

Main category: cs.GR

TL;DR: UltraZoom是一个系统，通过从手持设备拍摄的照片生成高分辨率的对象图像，利用全局和局部图像配对数据训练模型，实现无缝缩放效果。


<details>
  <summary>Details</summary>
Motivation: 解决从随意拍摄的低分辨率图像生成高分辨率、一致性的千兆像素图像的挑战。

Method: 构建每实例配对数据集，利用预训练生成模型学习对象特定的低到高分辨率映射，并在推理时滑动窗口应用模型。

Result: 系统能够生成一致且逼真的千兆像素图像，实现无缝的平移和缩放功能。

Conclusion: UltraZoom通过创新的配对数据集构建和模型适应方法，成功实现了高分辨率图像的生成，为后续研究提供了有价值的参考。

Abstract: We present UltraZoom, a system for generating gigapixel-resolution images of
objects from casually captured inputs, such as handheld phone photos. Given a
full-shot image (global, low-detail) and one or more close-ups (local,
high-detail), UltraZoom upscales the full image to match the fine detail and
scale of the close-up examples. To achieve this, we construct a per-instance
paired dataset from the close-ups and adapt a pretrained generative model to
learn object-specific low-to-high resolution mappings. At inference, we apply
the model in a sliding window fashion over the full image. Constructing these
pairs is non-trivial: it requires registering the close-ups within the full
image for scale estimation and degradation alignment. We introduce a simple,
robust method for getting registration on arbitrary materials in casual,
in-the-wild captures. Together, these components form a system that enables
seamless pan and zoom across the entire object, producing consistent,
photorealistic gigapixel imagery from minimal input.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [67] [Machine Intelligence on Wireless Edge Networks](https://arxiv.org/abs/2506.12210)
*Sri Krishna Vadlamani,Kfir Sulimany,Zhihui Gao,Tingjun Chen,Dirk Englund*

Main category: cs.ET

TL;DR: MIWEN是一种RF模拟架构，通过无线传输权重并在模拟前端执行分类，解决了边缘设备中DNN推理的存储和能耗问题。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备中深度神经网络推理因权重存储和数据移动导致的能耗问题。

Method: 利用RF模拟架构，通过无线传输权重并使用模拟前端进行推理，避免本地存储和数字转换。

Result: 在低能耗下实现与数字方法相当的MNIST分类精度，适用于低功耗边缘设备。

Conclusion: MIWEN通过RF模拟计算显著降低能耗，实现了无需本地存储的高效边缘推理。

Abstract: Deep neural network (DNN) inference on power-constrained edge devices is
bottlenecked by costly weight storage and data movement. We introduce MIWEN, a
radio-frequency (RF) analog architecture that ``disaggregates'' memory by
streaming weights wirelessly and performing classification in the analog front
end of standard transceivers. By encoding weights and activations onto RF
carriers and using native mixers as computation units, MIWEN eliminates local
weight memory and the overhead of analog-to-digital and digital-to-analog
conversion. We derive the effective number of bits of radio-frequency analog
computation under thermal noise, quantify the energy--precision trade-off, and
demonstrate digital-comparable MNIST accuracy at orders-of-magnitude lower
energy, unlocking real-time inference on low-power, memory-free edge devices.

</details>


### [68] [A Novel Thermal Network Model and Electro-Thermal Coupling Study for NSFETs and CFETs Considering Thermal Crosstalk](https://arxiv.org/abs/2506.12264)
*Tianci Miao,Qihang Zheng,Yangyang Hu,Xiaoyu Cheng,Jie Liang,Liang Chen,Aiying Guo,Jingjing Liu,Kailin Ren,Jianhua Zhang*

Main category: cs.ET

TL;DR: 论文研究了纳米片场效应晶体管（NSFETs）和互补场效应晶体管（CFETs）在3nm及更小节点中的自热和热串扰问题，提出了一种考虑邻近器件热串扰的热网络模型，并比较了两者的电热特性。


<details>
  <summary>Details</summary>
Motivation: 随着工艺节点的缩小，NSFETs和CFETs的自热和热串扰问题日益严重，需要准确计算和优化这些效应以提高器件和电路的性能。

Method: 提出了一种考虑邻近器件热串扰的热网络模型，用以准确计算自热和热串扰，并研究了NSFETs和CFETs的电热特性及其在逻辑门和环形振荡器中的应用。

Result: 研究发现CFETs的自热和热串扰更为严重，由其组成的逻辑门和环形振荡器受自热影响更大；提出的热网络模型可用于优化器件和电路的热设计。

Conclusion: 论文提出的热网络模型为研究器件和电路的热优化策略提供了有效工具，有助于实现设计与技术的协同优化（DTCO）。

Abstract: As the technology node continues to shrink, nanosheet field effect
transistors (NSFETs) and complementary FETs (CFETs) become valid candidates for
the 3nm and sub-nanometre nodes. However, due to the shrinking device size,
self-heating and inter-device thermal crosstalk of NSFETs and CFETs become more
severe. It is important to accurately calculate the self-heating and thermal
crosstalk of devices and to study the electrical and thermal characteristics of
logic gates, etc. In this work, a thermal network model considering the thermal
crosstalk of neighboring devices is proposed, which can accurately calculate
the self-heating and thermal crosstalk. The electrical and thermal
characteristics of NSFETs and CFETs are compared, and it is found that CFETs
have more severe self-heating and thermal crosstalk. The electro-thermal
characteristics of inverters, logic gates and ring oscillators composed of
NSFETs and CFETs are further investigated. Compared with NSFETs, logic gates
and ring oscillators composed of CFETs are more seriously affected by
self-heating and should be given extra attention. The thermal network model
proposed in this paper can be further used to study the thermal optimization
strategy of devices and circuits to enhance the electrical performance,
achieving the design technology co-optimizations (DTCO).

</details>


### [69] [Spatially Consistent Air-to-Ground Channel Modeling with Probabilistic LOS/NLOS Segmentation](https://arxiv.org/abs/2506.12794)
*Evgenii Vinogradov,Abdul Saboor,Zhuangzhuang Cui,Aymen Fakhreddine*

Main category: cs.ET

TL;DR: 提出了一种基于概率LOS/NLOS分割的空间一致性A2G信道模型，用于参数化确定性路径损耗和随机阴影衰落模型。


<details>
  <summary>Details</summary>
Motivation: 现有UAV信道模型忽视空间相关性，本文模型能重现城市环境中地面用户轨迹的LOS/NLOS过渡。

Method: 通过方位角和仰角相关的LOS概率捕捉环境遮挡，无需完整3D环境模型，并用几何仿真器验证。

Result: 模型在多种城市环境中表现出高准确性和计算效率，支持路径损耗和阴影衰落模型的生成及中断分析。

Conclusion: 该模型为A2G信道建模提供了一种高效且准确的方法，适用于复杂城市环境。

Abstract: In this paper, we present a spatially consistent A2G channel model based on
probabilistic LOS/NLOS segmentation to parameterize the deterministic path loss
and stochastic shadow fading model. Motivated by the limitations of existing
Unmanned Aerial Vehicle (UAV) channel models that overlook spatial correlation,
our approach reproduces LOS/NLOS transitions along ground user trajectories in
urban environments. This model captures environment-specific obstructions by
means of azimuth and elevation-dependent LOS probabilities without requiring a
full detailed 3D representation of the surroundings. We validate our framework
against a geometry-based simulator by evaluating it across various urban
settings. The results demonstrate its accuracy and computational efficiency,
enabling further realistic derivations of path loss and shadow fading models
and thorough outage analysis.

</details>


### [70] [Resilient-native and Intelligent NextG Systems](https://arxiv.org/abs/2506.12795)
*Mehdi Bennis*

Main category: cs.ET

TL;DR: 论文探讨了无线网络作为关键社会基础设施的韧性，区分了韧性与可靠性、鲁棒性，并提出了韧性的数学基础和度量方法。


<details>
  <summary>Details</summary>
Motivation: 随着自然和人为中断事件的增加，无线网络需要具备抵御和恢复的能力，而韧性的数学基础尚未完善。

Method: 文章首先定义韧性并区分其与可靠性和鲁棒性，然后深入探讨韧性的数学基础。

Result: 提出了针对网络韧性特点的度量方法，并讨论了相关权衡。

Conclusion: 文章强调了韧性在无线网络中的核心作用，为未来研究提供了理论基础和实用指导。

Abstract: Just like power, water and transportation systems, wireless networks are a
crucial societal infrastructure. As natural and human-induced disruptions
continue to grow, wireless networks must be resilient to unforeseen events,
able to withstand and recover from unexpected adverse conditions, shocks,
unmodeled disturbances and cascading failures. Despite its critical importance,
resilience remains an elusive concept, with its mathematical foundations still
underdeveloped. Unlike robustness and reliability, resilience is premised on
the fact that disruptions will inevitably happen. Resilience, in terms of
elasticity, focuses on the ability to bounce back to favorable states, while
resilience as plasticity involves agents (or networks) that can flexibly expand
their states, hypotheses and course of actions, by transforming through
real-time adaptation and reconfiguration. This constant situational awareness
and vigilance of adapting world models and counterfactually reasoning about
potential system failures and the corresponding best responses, is a core
aspect of resilience. This article seeks to first define resilience and
disambiguate it from reliability and robustness, before delving into the
mathematics of resilience. Finally, the article concludes by presenting nuanced
metrics and discussing trade-offs tailored to the unique characteristics of
network resilience.

</details>


### [71] [Leveraging Photonic Interconnects for Scalable and Efficient Fully Homomorphic Encryption](https://arxiv.org/abs/2506.12962)
*Dewan Saiham,Di Wu,Sazadur Rahman*

Main category: cs.ET

TL;DR: OptoLink是一种光子互联架构，显著提升了FHE系统的带宽和性能。


<details>
  <summary>Details</summary>
Motivation: 当前FHE加速器因带宽限制导致性能瓶颈，尤其是内存密集型操作。

Method: 提出OptoLink，一个可扩展的光子互联架构，支持128通道和1.6 TB/s吞吐量。

Result: OptoLink的带宽是传统电互联的300倍，降低了延迟并提高了数据吞吐量。

Conclusion: OptoLink是解决FHE加速器高内存和数据传输需求的有效方案。

Abstract: Fully Homomorphic Encryption (FHE) facilitates secure computations on
encrypted data but imposes significant demands on memory bandwidth and
computational power. While current FHE accelerators focus on optimizing
computation, they often face bandwidth limitations that result in performance
bottlenecks, particularly in memory-intensive operations. This paper presents
OptoLink, a scalable photonic interconnect architecture designed to address
these bandwidth and latency challenges in FHE systems. OptoLink achieves a
throughput of 1.6 TB/s with 128 channels, providing 300 times the bandwidth of
conventional electrical interconnects. The proposed architecture improves data
throughput, scalability, and reduces latency, making it an effective solution
for meeting the high memory and data transfer requirements of modern FHE
accelerators.

</details>


### [72] [lcpy: an open-source python package for parametric and dynamic Life Cycle Assessment and Life Cycle Costing](https://arxiv.org/abs/2506.13744)
*Spiros Gkousis,Evina Katsou*

Main category: cs.ET

TL;DR: 介绍了一个名为lcpy的Python开源包，用于高级参数化生命周期评估（LCA）和生命周期成本（LCC）分析。


<details>
  <summary>Details</summary>
Motivation: 旨在简化动态LCA和LCC的实现，并便于与不确定性评估和优化工具集成。

Method: 基于字典和列表的灵活模块化设计，支持时间变化、不确定性和动态分析。

Result: 实现了动态LCA和LCC分析，兼容优化和不确定性分析库。

Conclusion: lcpy为高级环境经济分析提供了更广泛的实施可能性。

Abstract: This article describes lcpy, an open-source python package that allows for
advanced parametric Life Cycle Assessment (LCA) and Life Cycle Costing (LCC)
analysis. The package is designed to allow the user to model a process with a
flexible, modular design based on dictionaries and lists. The modeling can
consider in-time variations, uncertainty, and allows for dynamic analysis,
uncertainty assessment, as well as conventional static LCA and LCC. The package
is compatible with optimization and uncertainty analysis libraries as well as
python packages for prospective LCA. Its goal is to allow for easy
implementation of dynamic LCA and LCC and for simple integration with tools for
uncertainty assessment and optimization towards a more widened implementation
of advanced enviro-economic analysis. The open-source code can be found at
https://github.com/spirdgk/lcpy.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [73] [Towards Energy-Efficient Distributed Agreement](https://arxiv.org/abs/2506.12282)
*Hugo Mirault,Peter Robinson*

Main category: cs.DC

TL;DR: 研究在同步消息传递模型中，节点可以选择唤醒或休眠情况下的容错共识问题，提出新的确定性共识算法，匹配最优时间复杂性下限，并分别针对多值和二进制共识优化了能量复杂性。


<details>
  <summary>Details</summary>
Motivation: 探索在节点可休眠的同步消息传递模型中，如何高效实现容错共识，以降低能量消耗。

Method: 提出确定性共识算法，容忍最多f<n的崩溃故障，针对多值和二进制共识分别优化能量复杂性。

Result: 算法达到最优时间复杂性f+1轮，多值共识能量复杂性为O(⌈f²/n⌉)，二进制共识为O(⌈f/√n⌉)。

Conclusion: 提出的算法在时间和能量复杂性上均表现优异，为容错共识提供了高效解决方案。

Abstract: We study fault-tolerant consensus in a variant of the synchronous message
passing model, where, in each round, every node can choose to be awake or
asleep. This is known as the sleeping model (Chatterjee, Gmyr, Pandurangan PODC
2020) and defines the awake complexity (also called \emph{energy complexity}),
which measures the maximum number of rounds that any node is awake throughout
the execution. Only awake nodes can send and receive messages in a given round
and all messages sent to sleeping nodes are lost. We present new deterministic
consensus algorithms that tolerate up to $f<n$ crash failures, where $n$ is the
number of nodes. Our algorithms match the optimal time complexity lower bound
of $f+1$ rounds. For multi-value consensus, where the input values are chosen
from some possibly large set, we achieve an energy complexity of ${O}(\lceil
f^2 / n \rceil)$ rounds, whereas for binary consensus, we show that ${O}(\lceil
f / \sqrt{n} \rceil)$ rounds are possible.

</details>


### [74] [Efficient Unified Caching for Accelerating Heterogeneous AI Workloads](https://arxiv.org/abs/2506.12370)
*Tianze Wang,Yifei Liu,Chen Chen,Pengfei Zuo,Jiawei Zhang,Qizhen Weng,Yin Chen,Zhenhua Han,Jieru Zhao,Quan Chen,Minyi Guo*

Main category: cs.DC

TL;DR: IGTCache是一种用于现代AI集群的统一高效缓存方案，通过分层访问抽象和假设测试优化缓存管理，显著提升了命中率和任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 现代AI集群需要处理多样化的负载和异构数据访问模式，现有缓存策略难以满足需求。

Method: 提出IGTCache，利用AccessStreamTree抽象层次化数据访问，并通过假设测试识别访问模式（顺序、随机或倾斜），据此优化缓存策略（预取、回收和空间分配）。

Result: 实验显示IGTCache比现有缓存框架命中率提高55.6%，任务完成时间减少52.2%。

Conclusion: IGTCache成功解决了异构负载的统一缓存管理问题，显著提升了性能。

Abstract: Modern AI clusters, which host diverse workloads like data pre-processing,
training and inference, often store the large-volume data in cloud storage and
employ caching frameworks to facilitate remote data access. To avoid
code-intrusion complexity and minimize cache space wastage, it is desirable to
maintain a unified cache shared by all the workloads. However, existing cache
management strategies, designed for specific workloads, struggle to handle the
heterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous
access patterns and item storage granularities. In this paper, we propose
IGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache
leverages a hierarchical access abstraction, AccessStreamTree, to organize the
recent data accesses in a tree structure, facilitating access pattern detection
at various granularities. Using this abstraction, IGTCache applies hypothesis
testing to categorize data access patterns as sequential, random, or skewed.
Based on these detected access patterns and granularities, IGTCache tailors
optimal cache management strategies including prefetching, eviction, and space
allocation accordingly. Experimental results show that IGTCache increases the
cache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the
overall job completion time by 52.2%.

</details>


### [75] [QoS-aware Scheduling of Periodic Real-time Task Graphs on Heterogeneous Pre-occupied MECs](https://arxiv.org/abs/2506.12415)
*Ashutosh Shankar,Astha Kumari*

Main category: cs.DC

TL;DR: 本文提出了一种改进的HEFT算法，用于在异构、预占用的移动边缘计算网络中调度周期性DAG任务，以优化QoS并满足严格的时间约束。


<details>
  <summary>Details</summary>
Motivation: 在延迟敏感的应用程序中，高效的调度对于保持服务质量（QoS）至关重要。特别是在异构、预占用的MEC网络中，如何利用剩余处理能力调度周期性DAG任务是一个挑战。

Method: 通过动态识别处理器上的空闲时段，提出了一种改进的HEFT算法，生成超周期调度方案，优化任务执行而不干扰现有工作负载。

Result: 实验结果表明，该方法在负载均衡和资源利用率方面表现优异，适合支持实时周期性应用的异构MEC基础设施。

Conclusion: 该方法在保证QoS和资源约束的同时，提升了MEC环境中的任务调度效率。

Abstract: In latency-sensitive applications, efficient task scheduling is crucial for
maintaining Quality of Service (QoS) while meeting strict timing constraints.
This paper addresses the challenge of scheduling periodic tasks structured as
directed acyclic graphs (DAGs) within heterogeneous, pre-occupied Mobile Edge
Computing (MEC) networks. We propose a modified version of the Heterogeneous
Earliest Finish Time (HEFT) algorithm designed to exploit residual processing
capacity in preoccupied MEC environments. Our approach dynamically identifies
idle intervals on processors to create a feasible hyperperiodic schedule that
specifies an allocated virtual machine (VM), task version, and start time for
each task. This scheduling strategy maximizes the aggregate QoS by optimizing
task execution without disrupting the existing periodic workload, while also
adhering to periodicity, precedence, and resource constraints.Experimental
results demonstrate that our method achieves enhanced load balancing and
resource utilization, highlighting its potential to improve performance in
heterogeneous MEC infrastructures supporting real-time, periodic applications.

</details>


### [76] [HarMoEny: Efficient Multi-GPU Inference of MoE Models](https://arxiv.org/abs/2506.12417)
*Zachary Douchet,Rishi Sharma,Martijn de Vos,Rafael Pires,Anne-Marie Kermarrec,Oana Balmau*

Main category: cs.DC

TL;DR: HarMoEny通过动态令牌重新分配和异步预取专家数据，解决了MoE模型推理中的负载不均问题，显著提升了吞吐量并减少了延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型在推理时仅有部分专家被激活，导致GPU间负载不均和等待时间增加，影响推理效率。

Method: 采用动态令牌重新分配和异步预取专家数据的技术，实现负载均衡和延迟优化。

Result: HarMoEny在严重负载不均情况下，吞吐量提升37%-70%，首令牌时间减少34%-41%。

Conclusion: HarMoEny有效解决了MoE模型的负载不均问题，显著提升了推理效率和GPU利用率。

Abstract: Mixture-of-Experts (MoE) models offer computational efficiency during
inference by activating only a subset of specialized experts for a given input.
This enables efficient model scaling on multi-GPU systems that use expert
parallelism without compromising performance. However, load imbalance among
experts and GPUs introduces waiting times, which can significantly increase
inference latency. To address this challenge, we propose HarMoEny, a novel
solution to address MoE load imbalance through two simple techniques: (i)
dynamic token redistribution to underutilized GPUs and (ii) asynchronous
prefetching of experts from the system to GPU memory. These techniques achieve
a near-perfect load balance among experts and GPUs and mitigate delays caused
by overloaded GPUs. We implement HarMoEny and compare its latency and
throughput with four MoE baselines using real-world and synthetic datasets.
Under heavy load imbalance, HarMoEny increases throughput by 37%-70% and
reduces time-to-first-token by 34%-41%, compared to the next-best baseline.
Moreover, our ablation study demonstrates that HarMoEny's scheduling policy
reduces the GPU idling time by up to 84% compared to the baseline policies.

</details>


### [77] [Optimizing Federated Learning using Remote Embeddings for Graph Neural Networks](https://arxiv.org/abs/2506.12425)
*Pranjal Naman,Yogesh Simmhan*

Main category: cs.DC

TL;DR: OpES是一种优化的联邦GNN训练框架，通过远程邻居剪枝和重叠推送嵌入与本地训练，降低了网络开销和训练时间，在大型密集图上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有联邦GNN训练方法因高通信成本导致的性能下降问题，提升训练效率和准确性。

Method: 采用远程邻居剪枝技术，并重叠推送嵌入与本地训练，以减少网络开销和训练时间。

Result: 在Reddit和Products等大型密集图上，OpES比现有技术快约2倍，准确性提升20%。

Conclusion: OpES在提升联邦GNN训练效率和准确性方面表现出色，尤其适用于大型密集图。

Abstract: Graph Neural Networks (GNNs) have experienced rapid advancements in recent
years due to their ability to learn meaningful representations from graph data
structures. Federated Learning (FL) has emerged as a viable machine learning
approach for training a shared model on decentralized data, addressing privacy
concerns while leveraging parallelism. Existing methods that address the unique
requirements of federated GNN training using remote embeddings to enhance
convergence accuracy are limited by their diminished performance due to large
communication costs with a shared embedding server. In this paper, we present
OpES, an optimized federated GNN training framework that uses remote
neighbourhood pruning, and overlaps pushing of embeddings to the server with
local training to reduce the network costs and training time. The modest drop
in per-round accuracy due to pre-emptive push of embeddings is out-stripped by
the reduction in per-round training time for large and dense graphs like Reddit
and Products, converging up to $\approx2\times$ faster than the
state-of-the-art technique using an embedding server and giving up to $20\%$
better accuracy than vanilla federated GNN learning.

</details>


### [78] [Accelerating Cloud-Based Transcriptomics: Performance Analysis and Optimization of the STAR Aligner Workflow](https://arxiv.org/abs/2506.12611)
*Piotr Kica,Sabina Lichołai,Michał Orzechowski,Maciej Malawski*

Main category: cs.DC

TL;DR: 该论文提出了一种适用于云端的转录组学图谱流程，通过优化技术显著降低了执行时间和成本。


<details>
  <summary>Details</summary>
Motivation: 为了高效处理大规模RNA测序数据，探索成本效益高的云计算方案。

Method: 采用可扩展的云原生架构，优化STAR比对器，并验证spot实例的适用性。

Result: 优化技术使比对时间减少23%，验证了设计的有效性。

Conclusion: 该架构及优化技术适用于大规模转录组数据分析，显著提升了效率和成本效益。

Abstract: In this work, we explore the Transcriptomics Atlas pipeline adapted for
cost-efficient and high-throughput computing in the cloud. We propose a
scalable, cloud-native architecture designed for running a resource-intensive
aligner -- STAR -- and processing tens or hundreds of terabytes of
RNA-sequencing data. We implement multiple optimization techniques that give
significant execution time and cost reduction. The impact of particular
optimizations is measured in medium-scale experiments followed by a large-scale
experiment that leverages all of them and validates the current design. Early
stopping optimization allows a reduction in total alignment time by 23%. We
analyze the scalability and efficiency of one of the most widely used sequence
aligners. For the cloud environment, we identify one of the most suitable EC2
instance types and verify the applicability of spot instances usage.

</details>


### [79] [Energy-Efficient Real-Time Job Mapping and Resource Management in Mobile-Edge Computing](https://arxiv.org/abs/2506.12686)
*Chuanchao Gao,Niraj Kumar,Arvind Easwaran*

Main category: cs.DC

TL;DR: 研究移动边缘计算（MEC）中结合任务调度、服务器资源分配和物联网设备移动性的联合优化问题，提出离线和在线算法以最大化节约物联网设备能量。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽视服务器资源分配与物联网设备移动性的联合考虑，导致无法充分利用MEC潜力。

Method: 离线问题建模为整数线性规划并提出近似算法LHJS；在线问题提出启发式算法LBS。

Result: 通过实验验证了算法的性能。

Conclusion: 联合优化能有效提升MEC系统性能，显著节约物联网设备能量。

Abstract: Mobile-edge computing (MEC) has emerged as a promising paradigm for enabling
Internet of Things (IoT) devices to handle computation-intensive jobs. Due to
the imperfect parallelization of algorithms for job processing on servers and
the impact of IoT device mobility on data communication quality in wireless
networks, it is crucial to jointly consider server resource allocation and IoT
device mobility during job scheduling to fully benefit from MEC, which is often
overlooked in existing studies. By jointly considering job scheduling, server
resource allocation, and IoT device mobility, we investigate the
deadline-constrained job offloading and resource management problem in MEC with
both communication and computation contentions, aiming to maximize the total
energy saved for IoT devices. For the offline version of the problem, where job
information is known in advance, we formulate it as an Integer Linear
Programming problem and propose an approximation algorithm, $\mathtt{LHJS}$,
with a constant performance guarantee. For the online version, where job
information is only known upon release, we propose a heuristic algorithm,
$\mathtt{LBS}$, that is invoked whenever a job is released. Finally, we conduct
experiments with parameters from real-world applications to evaluate their
performance.

</details>


### [80] [Serving Large Language Models on Huawei CloudMatrix384](https://arxiv.org/abs/2506.12708)
*Pengfei Zuo,Huimin Lin,Junbo Deng,Nan Zou,Xingkun Yang,Yingyu Diao,Weifeng Gao,Ke Xu,Zhangyu Chen,Shirui Lu,Zhao Qiu,Peiyang Li,Xianyu Chang,Zhengzhong Yu,Fangzheng Miao,Jia Zheng,Ying Li,Yuan Feng,Bei Wang,Zaijian Zong,Mosong Zhou,Wenli Zhou,Houjiang Chen,Xingyu Liao,Yipeng Li,Wenxiao Zhang,Ping Zhu,Yinggang Wang,Chuanjie Xiao,Depeng Liang,Dong Cao,Juncheng Liu,Yongqiang Yang,Xiaolong Bai,Yi Li,Huaguo Xie,Huatao Wu,Zhibin Yu,Lv Chen,Hu Liu,Yujun Ding,Haipei Zhu,Jing Xia,Yi Xiong,Zhou Yu,Heng Liao*

Main category: cs.DC

TL;DR: 论文介绍了华为云矩阵CloudMatrix384，一种面向大语言模型的新型AI数据中心架构，通过创新的硬件-软件集成优化性能。


<details>
  <summary>Details</summary>
Motivation: 传统AI集群在计算强度、内存带宽、芯片间通信和延迟等方面存在局限性，无法满足大语言模型快速发展的需求。

Method: 论文提出CloudMatrix384架构，结合384个Ascend 910C NPU和192个Kunpeng CPU，通过统一总线网络实现动态资源池化；并提出了CloudMatrix-Infer服务解决方案，优化了预填充、解码和缓存。

Result: 评估显示，CloudMatrix-Infer在预填充和解码吞吐量上达到最优性能（6,688和1,943 tokens/s/NPU），同时保持了低延迟和高精度。

Conclusion: 华为云矩阵为大规模AI模型提供了高效的硬件-软件集成解决方案，显著提升了性能和资源利用率。

Abstract: The rapid evolution of large language models (LLMs), driven by growing
parameter scales, adoption of mixture-of-experts (MoE) architectures, and
expanding context lengths, imposes unprecedented demands on AI infrastructure.
Traditional AI clusters face limitations in compute intensity, memory
bandwidth, inter-chip communication, and latency, compounded by variable
workloads and strict service-level objectives. Addressing these issues requires
fundamentally redesigned hardware-software integration. This paper introduces
Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in
the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C
NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified
Bus (UB) network, enabling direct all-to-all communication and dynamic pooling
of resources. These features optimize performance for communication-intensive
operations, such as large-scale MoE expert parallelism and distributed
key-value cache access. To fully leverage CloudMatrix384, we propose
CloudMatrix-Infer, an advanced LLM serving solution incorporating three core
innovations: a peer-to-peer serving architecture that independently scales
prefill, decode, and caching; a large-scale expert parallelism strategy
supporting EP320 via efficient UB-based token dispatch; and hardware-aware
optimizations including specialized operators, microbatch-based pipelining, and
INT8 quantization. Evaluation with the DeepSeek-R1 model shows
CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of
6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms
TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s
even under stringent 15 ms latency constraints, while INT8 quantization
maintains model accuracy across benchmarks.

</details>


### [81] [Self-Stabilizing Replicated State Machine Coping with Byzantine and Recurring Transient Faults](https://arxiv.org/abs/2506.12900)
*Shlomi Dolev,Amit Hendin,Maurice Herlihy,Maria Potop Butucaru,Elad Michael Schiller*

Main category: cs.DC

TL;DR: 本文提出了一种满足拜占庭容错、瞬态故障容错、准确性和自稳定性的重复拜占庭协议，适用于分布式系统。


<details>
  <summary>Details</summary>
Motivation: 解决分布式系统中因拜占庭节点和瞬时故障导致的共识问题，适用于区块链价格预言机等应用。

Method: 提出了一种新协议，能在任意系统配置下建立并保持一致性，容忍拜占庭节点和瞬时故障。

Result: 协议在容忍最多1/3拜占庭节点和1/6恶意瞬时故障的同时，保持一致性。

Conclusion: 该协议为分布式系统提供了一种高容错的重复拜占庭共识解决方案。

Abstract: The ability to perform repeated Byzantine agreement lies at the heart of
important applications such as blockchain price oracles or replicated state
machines. Any such protocol requires the following properties: (1)
\textit{Byzantine fault-tolerance}, because not all participants can be assumed
to be honest, (2) r\textit{ecurrent transient fault-tolerance}, because even
honest participants may be subject to transient ``glitches'', (3)
\textit{accuracy}, because the results of quantitative queries (such as price
quotes) must lie within the interval of honest participants' inputs, and (4)
\textit{self-stabilization}, because it is infeasible to reboot a distributed
system following a fault.
  This paper presents the first protocol for repeated Byzantine agreement that
satisfies the properties listed above. Specifically, starting in an arbitrary
system configuration, our protocol establishes consistency. It preserves
consistency in the face of up to $\lceil n/3 \rceil -1$ Byzantine participants
{\em and} constant recurring (``noise'') transient faults, of up to $\lceil n/6
\rceil-1$ additional malicious transient faults, or even more than $\lceil n/6
\rceil-1$ (uniformly distributed) random transient faults, in each repeated
Byzantine agreement.

</details>


### [82] [Distributed Computing From First Principles](https://arxiv.org/abs/2506.12959)
*Kenneth Odoh*

Main category: cs.DC

TL;DR: 本书旨在为各类读者提供分布式计算的核心概念和实现，适合工程师、研究人员和专业人士。


<details>
  <summary>Details</summary>
Motivation: 作者希望通过本书使分布式计算的核心概念更易理解，帮助不同背景的读者掌握相关知识和技能。

Method: 书中实现了多个分布式计算的基础算法，并提供完整的实现示例作为教学指南。

Result: 读者可以通过本书深入理解分布式系统的理论和实践应用。

Conclusion: 本书是理论和实践并重的分布式计算学习资源，适合广泛读者群体。

Abstract: This book on Distributed Computing aims to benefit a diverse audience,
ranging from aspiring engineers, and seasoned researchers, to a wide range of
professionals. Driven by my passion for making the core concepts of distributed
computing accessible, this work is a significant undertaking designed to
empower individuals from all backgrounds to gain valuable insight. Have you
ever wondered how a typical distributed system works under the hood? Are you
looking for a pedagogical guide with complete implementations? In this work, we
have implemented several foundational algorithms in Distributed Computing.
Whether your expertise lies in the theoretical foundations or the practical
applications of the principles of Distributed Systems, this book is for you.

</details>


### [83] [DDiT: Dynamic Resource Allocation for Diffusion Transformer Model Serving](https://arxiv.org/abs/2506.13497)
*Heyang Huang,Cunchen Hu,Jiaqi Zhu,Ziyuan Gao,Liangliang Xu,Yizhou Shan,Yungang Bao,Sun Ninghui,Tianwei Zhang,Sa Wang*

Main category: cs.DC

TL;DR: 论文提出了一种名为DDiT的灵活系统，通过优化并行度和资源分配，显著提高了Text-to-Video (T2V)模型的效率。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型的部署方式效率低下，未能充分利用模块特性，且DiT在不同分辨率和并行度下表现不一，优化潜力未被发掘。

Method: DDiT集成了阶段间和阶段内优化，通过解耦控制机制和贪婪资源分配算法，动态调整资源配置。

Result: 在T5编码器、OpenSora SDDiT和OpenSora VAE模型上的评估显示，DDiT在p99延迟和平均延迟上分别提升1.44倍和1.43倍。

Conclusion: DDiT通过精细化的优化机制显著提升了T2V模型性能，为未来研究提供了新方向。

Abstract: The Text-to-Video (T2V) model aims to generate dynamic and expressive videos
from textual prompts. The generation pipeline typically involves multiple
modules, such as language encoder, Diffusion Transformer (DiT), and Variational
Autoencoders (VAE). Existing serving systems often rely on monolithic model
deployment, while overlooking the distinct characteristics of each module,
leading to inefficient GPU utilization. In addition, DiT exhibits varying
performance gains across different resolutions and degrees of parallelism, and
significant optimization potential remains unexplored. To address these
problems, we present DDiT, a flexible system that integrates both inter-phase
and intra-phase optimizations. DDiT focuses on two key metrics: optimal degree
of parallelism, which prevents excessive parallelism for specific resolutions,
and starvation time, which quantifies the sacrifice of each request. To this
end, DDiT introduces a decoupled control mechanism to minimize the
computational inefficiency caused by imbalances in the degree of parallelism
between the DiT and VAE phases. It also designs a greedy resource allocation
algorithm with a novel scheduling mechanism that operates at the single-step
granularity, enabling dynamic and timely resource scaling. Our evaluation on
the T5 encoder, OpenSora SDDiT, and OpenSora VAE models across diverse datasets
reveals that DDiT significantly outperforms state-of-the-art baselines by up to
1.44x in p99 latency and 1.43x in average latency.

</details>


### [84] [POPQC: Parallel Optimization for Quantum Circuits (Extended Version)](https://arxiv.org/abs/2506.13720)
*Pengyu Liu,Jatin Arora,Mingkuan Xu,Umut A. Acar*

Main category: cs.DC

TL;DR: 该论文提出了一种并行算法，用于量子电路的局部优化，解决了现有优化器计算开销大且序列化的问题，证明了算法的高效性和最优性。


<details>
  <summary>Details</summary>
Motivation: 量子电路优化是量子计算中的关键问题，但现有方法依赖启发式且计算开销大。通过局部优化和并行化，可以提高效率。

Method: 提出了一种并行算法，通过维护一组指针并在多轮操作中并行优化含指针的电路段，确保局部最优性。

Result: 证明算法对常数Ω具有O(n lg n)工作量和O(r lg n)跨度，优化后的电路符合局部最优性。

Conclusion: 该并行算法显著提升了量子电路局部优化的效率，为实际应用提供了可行性。

Abstract: Optimization of quantum programs or circuits is a fundamental problem in
quantum computing and remains a major challenge. State-of-the-art quantum
circuit optimizers rely on heuristics and typically require superlinear, and
even exponential, time. Recent work proposed a new approach that pursues a
weaker form of optimality called local optimality. Parameterized by a natural
number $\Omega$, local optimality insists that each and every $\Omega$-segment
of the circuit is optimal with respect to an external optimizer, called the
oracle. Local optimization can be performed using only a linear number of calls
to the oracle but still incurs quadratic computational overheads in addition to
oracle calls. Perhaps most importantly, the algorithm is sequential.
  In this paper, we present a parallel algorithm for local optimization of
quantum circuits. To ensure efficiency, the algorithm operates by keeping a set
of fingers into the circuit and maintains the invariant that a $\Omega$-deep
circuit needs to be optimized only if it contains a finger. Operating in
rounds, the algorithm selects a set of fingers, optimizes in parallel the
segments containing the fingers, and updates the finger set to ensure the
invariant. For constant $\Omega$, we prove that the algorithm requires
$O(n\lg{n})$ work and $O(r\lg{n})$ span, where $n$ is the circuit size and $r$
is the number of rounds. We prove that the optimized circuit returned by the
algorithm is locally optimal in the sense that any $\Omega$-segment of the
circuit is optimal with respect to the oracle.

</details>


### [85] [BanditWare: A Contextual Bandit-based Framework for Hardware Prediction](https://arxiv.org/abs/2506.13730)
*Tainã Coleman,Hena Ahmed,Ravi Shende,Ismael Perez,Ïlkay Altintaş*

Main category: cs.DC

TL;DR: BanditWare是一个在线推荐系统，通过动态选择最适合的硬件资源来优化分布式计算系统中的资源分配，解决了单系统到分布式环境过渡中的资源冲突和性能问题。


<details>
  <summary>Details</summary>
Motivation: 分布式计算系统对现代应用至关重要，但资源分配不当会导致性能下降、系统不稳定等问题。BanditWare旨在通过实时学习和适应新工作负载，优化资源分配。

Method: BanditWare使用上下文多臂老虎机算法，动态选择最适合的硬件资源，平衡探索和利用，逐步优化推荐。

Result: 在三个工作流应用（Cycles、BurnPro3D和矩阵乘法应用）中评估，BanditWare能够高效优化资源分配，并支持无缝集成到国家数据平台（NDP）。

Conclusion: BanditWare通过实时学习和适应，为分布式计算系统提供了高效的资源分配解决方案，适合不同经验水平的用户。

Abstract: Distributed computing systems are essential for meeting the demands of modern
applications, yet transitioning from single-system to distributed environments
presents significant challenges. Misallocating resources in shared systems can
lead to resource contention, system instability, degraded performance, priority
inversion, inefficient utilization, increased latency, and environmental
impact.
  We present BanditWare, an online recommendation system that dynamically
selects the most suitable hardware for applications using a contextual
multi-armed bandit algorithm. BanditWare balances exploration and exploitation,
gradually refining its hardware recommendations based on observed application
performance while continuing to explore potentially better options. Unlike
traditional statistical and machine learning approaches that rely heavily on
large historical datasets, BanditWare operates online, learning and adapting in
real-time as new workloads arrive.
  We evaluated BanditWare on three workflow applications: Cycles (an
agricultural science scientific workflow) BurnPro3D (a web-based platform for
fire science) and a matrix multiplication application. Designed for seamless
integration with the National Data Platform (NDP), BanditWare enables users of
all experience levels to optimize resource allocation efficiently.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [86] [Datrics Text2SQL: A Framework for Natural Language to SQL Query Generation](https://arxiv.org/abs/2506.12234)
*Tetiana Gladkykh,Kyrylo Kirykov*

Main category: cs.DB

TL;DR: Datrics Text2SQL是一个基于检索增强生成（RAG）的框架，旨在通过利用结构化文档、基于示例的学习和领域特定规则生成准确的SQL查询。


<details>
  <summary>Details</summary>
Motivation: 解决文本到SQL系统中模糊表达、领域特定词汇和复杂模式关系的挑战，让非SQL专家能够轻松查询数据库。

Method: 系统构建了一个丰富的知识库，包含数据库文档和查询示例，这些内容以向量嵌入形式存储并通过语义相似性检索，利用上下文生成语法正确且语义对齐的SQL代码。

Result: 系统能够有效连接用户意图和数据库结构，生成高质量SQL查询。

Conclusion: Datrics Text2SQL填补了非SQL专家与数据库查询之间的鸿沟，通过RAG框架提升了查询的准确性和可访问性。

Abstract: Text-to-SQL systems enable users to query databases using natural language,
democratizing access to data analytics. However, they face challenges in
understanding ambiguous phrasing, domain-specific vocabulary, and complex
schema relationships. This paper introduces Datrics Text2SQL, a
Retrieval-Augmented Generation (RAG)-based framework designed to generate
accurate SQL queries by leveraging structured documentation, example-based
learning, and domain-specific rules. The system builds a rich Knowledge Base
from database documentation and question-query examples, which are stored as
vector embeddings and retrieved through semantic similarity. It then uses this
context to generate syntactically correct and semantically aligned SQL code.
The paper details the architecture, training methodology, and retrieval logic,
highlighting how the system bridges the gap between user intent and database
structure without requiring SQL expertise.

</details>


### [87] [CPN-Py: A Python-Based Tool for Modeling and Analyzing Colored Petri Nets](https://arxiv.org/abs/2506.12238)
*Alessandro Berti,Wil M. P. van der Aalst*

Main category: cs.DB

TL;DR: 介绍了CPN-Py，一个Python库，用于将Colored Petri Nets（CPN）与现代数据科学生态系统无缝集成，支持核心功能并整合了PM4Py和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的CPN工具（如CPN Tools和CPN IDE）与现代数据科学生态系统（如Python）分离，限制了其在数据分析中的应用。

Method: 开发了CPN-Py库，保留了CPN的核心概念（如颜色集、定时令牌、保护逻辑和层次结构），并提供与Python环境及PM4Py的集成功能。

Result: CPN-Py成功支持状态空间分析和层次化CPN，同时通过JSON格式适应大型语言模型。

Conclusion: CPN-Py为CPN模型的开发和数据分析提供了高效且灵活的解决方案，填补了现有工具的不足。

Abstract: Colored Petri Nets (CPNs) are an established formalism for modeling processes
where tokens carry data. Although tools like CPN Tools and CPN IDE excel at
CPN-based simulation, they are often separate from modern data science
ecosystems. Meanwhile, Python has become the de facto language for process
mining, machine learning, and data analytics. In this paper, we introduce
CPN-Py, a Python library that faithfully preserves the core concepts of Colored
Petri Nets -- including color sets, timed tokens, guard logic, and hierarchical
structures -- while providing seamless integration with the Python environment.
We discuss its design, highlight its synergy with PM4Py (including stochastic
replay, process discovery, and decision mining functionalities), and illustrate
how the tool supports state space analysis and hierarchical CPNs. We also
outline how CPN-Py accommodates large language models, which can generate or
refine CPN models through a dedicated JSON-based format.

</details>


### [88] [Redbench: A Benchmark Reflecting Real Workloads](https://arxiv.org/abs/2506.12488)
*Skander Krid,Mihail Stoian,Andreas Kipf*

Main category: cs.DB

TL;DR: 论文提出了Redbench，一个包含30个真实查询模式的工作负载集合，弥补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 实际工作负载特征未在现有基准测试（如TPC-H、TPC-DS、DSB、CAB）中体现，阻碍了学习组件的实际应用开发。

Method: 通过从支持基准中采样查询并与Redset中观察到的工作负载特征对齐，构建Redbench。

Result: Redbench提供了30个反映真实查询模式的工作负载。

Conclusion: Redbench填补了研究和行业之间的工作负载数据缺口，推动了更现实的学习组件开发。

Abstract: Instance-optimized components have made their way into production systems. To
some extent, this adoption is due to the characteristics of customer workloads,
which can be individually leveraged during the model training phase. However,
there is a gap between research and industry that impedes the development of
realistic learned components: the lack of suitable workloads. Existing ones,
such as TPC-H and TPC-DS, and even more recent ones, such as DSB and CAB, fail
to exhibit real workload patterns, particularly distribution shifts.
  In this paper, we introduce Redbench, a collection of 30 workloads that
reflect query patterns observed in the real world. The workloads were obtained
by sampling queries from support benchmarks and aligning them with workload
characteristics observed in Redset.

</details>


### [89] [Towards Visualizing Electronic Medical Records via Natural Language Queries](https://arxiv.org/abs/2506.12837)
*Haodi Zhang,Siqi Ning,Qiyong Zheng,Jinyin Nie,Liangjie Zhang,Weicheng Wang,Yuanfeng Song*

Main category: cs.DB

TL;DR: 提出了一种利用大语言模型生成电子病历可视化数据的方法，避免了手动标注的高成本，并创建了首个大规模电子病历文本-可视化数据集MedicalVis。


<details>
  <summary>Details</summary>
Motivation: 电子病历数据复杂且缺乏相关可视化数据集，手动标注成本高昂，亟需创新方法解决这一问题。

Method: 使用大语言模型（LLMs）构建文本到可视化的管道，生成NLQs与对应的可视化数据，并提出MedCodeT5模型。

Result: 创建了包含35,374个示例的MedicalVis数据集，MedCodeT5在生成电子病历可视化方面优于现有基线方法。

Conclusion: 该研究为电子病历可视化提供了标准化评估工具，推动了通过可视化提取医学洞察的进展。

Abstract: Electronic medical records (EMRs) contain essential data for patient care and
clinical research. With the diversity of structured and unstructured data in
EHR, data visualization is an invaluable tool for managing and explaining these
complexities. However, the scarcity of relevant medical visualization data and
the high cost of manual annotation required to develop such datasets pose
significant challenges to advancing medical visualization techniques. To
address this issue, we propose an innovative approach using large language
models (LLMs) for generating visualization data without labor-intensive manual
annotation. We introduce a new pipeline for building text-to-visualization
benchmarks suitable for EMRs, enabling users to visualize EMR statistics
through natural language queries (NLQs). The dataset presented in this paper
primarily consists of paired text medical records, NLQs, and corresponding
visualizations, forming the first large-scale text-to-visual dataset for
electronic medical record information called MedicalVis with 35,374 examples.
Additionally, we introduce an LLM-based approach called MedCodeT5, showcasing
its viability in generating EMR visualizations from NLQs, outperforming various
strong text-to-visualization baselines. Our work facilitates standardized
evaluation of EMR visualization methods while providing researchers with tools
to advance this influential field of application. In a nutshell, this study and
dataset have the potential to promote advancements in eliciting medical
insights through visualization.

</details>


### [90] [Humans, Machine Learning, and Language Models in Union: A Cognitive Study on Table Unionability](https://arxiv.org/abs/2506.12990)
*Sreeram Marimuthu,Nina Klimenkova,Roee Shraga*

Main category: cs.DB

TL;DR: 论文研究了人类在数据发现中决定表可联合性的行为，通过实验调查和机器学习框架提升人类决策性能，并初步比较了LLM与人类的表现，为未来人机协同数据发现系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现代数据科学中，表可联合性是关键任务，但人类视角的研究不足。

Method: 设计实验调查评估人类决策行为，并开发机器学习框架提升性能，初步比较LLM与人类表现。

Result: 人类决策性能可通过机器学习提升，LLM表现优于人类，但结合两者更佳。

Conclusion: 研究为未来人机协同数据发现系统提供了基础。

Abstract: Data discovery and table unionability in particular became key tasks in
modern Data Science. However, the human perspective for these tasks is still
under-explored. Thus, this research investigates the human behavior in
determining table unionability within data discovery. We have designed an
experimental survey and conducted a comprehensive analysis, in which we assess
human decision-making for table unionability. We use the observations from the
analysis to develop a machine learning framework to boost the (raw) performance
of humans. Furthermore, we perform a preliminary study on how LLM performance
is compared to humans indicating that it is typically better to consider a
combination of both. We believe that this work lays the foundations for
developing future Human-in-the-Loop systems for efficient data discovery.

</details>


### [91] [EnhanceGraph: A Continuously Enhanced Graph-based Index for High-dimensional Approximate Nearest Neighbor Search](https://arxiv.org/abs/2506.13144)
*Xiaoyao Zhong,Jiabao Jin,Peng Cheng,Mingyu Yang,Lei Chen,Haoyang Li,Zhitao Shen,Xuemin Lin,Heng Tao Shen,Jingkuan Song*

Main category: cs.DB

TL;DR: 该论文提出了EnhanceGraph框架，通过整合搜索和构建日志（称为共轭图）来改进高维向量空间中的近似最近邻搜索质量。实验表明，该方法显著提高了搜索准确性，同时保持了效率。


<details>
  <summary>Details</summary>
Motivation: 现有图索引的静态性质导致搜索和构建日志未能充分利用，因此希望通过动态整合这些日志提升搜索性能。

Method: 提出EnhanceGraph框架，构建共轭图来存储搜索日志（局部最优到全局最优的边）和构建日志（剪枝后的近邻图边），并提出了优化方法。

Result: 在多个数据集上，EnhanceGraph显著提升了搜索准确率（召回率从41.74%提升至93.42%），且未牺牲效率。

Conclusion: EnhanceGraph在提升搜索质量方面表现出色，已被集成到开源向量库VSAG中。

Abstract: Recently, Approximate Nearest Neighbor Search in high-dimensional vector
spaces has garnered considerable attention due to the rapid advancement of deep
learning techniques. We observed that a substantial amount of search and
construction logs are generated throughout the lifespan of a graph-based index.
However, these two types of valuable logs are not fully exploited due to the
static nature of existing indexes. We present the EnhanceGraph framework, which
integrates two types of logs into a novel structure called a conjugate graph.
The conjugate graph is then used to improve search quality. Through theoretical
analyses and observations of the limitations of graph-based indexes, we propose
several optimization methods. For the search logs, the conjugate graph stores
the edges from local optima to global optima to enhance routing to the nearest
neighbor. For the construction logs, the conjugate graph stores the pruned
edges from the proximity graph to enhance retrieving of k nearest neighbors.
Our experimental results on several public and real-world industrial datasets
show that EnhanceGraph significantly improves search accuracy with the greatest
improvement on recall from 41.74% to 93.42%, but does not sacrifices search
efficiency. In addition, our EnhanceGraph algorithm has been integrated into
Ant Group's open-source vector library, VSAG.

</details>


### [92] [Parachute: Single-Pass Bi-Directional Information Passing](https://arxiv.org/abs/2506.13670)
*Mihail Stoian,Andreas Zimmerer,Skander Krid,Amadou Latyr Ngom,Jialin Ding,Tim Kraska,Andreas Kipf*

Main category: cs.DB

TL;DR: 该论文提出了一种单次双向信息传递的方法，通过静态分析和预计算指纹列，显著提升了查询执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有的横向信息传递技术仅支持单向信息流，而实例最优算法（如Yannakakis）需要额外输入扫描，限制了其在生产系统中的采用。

Method: 通过对信息流被阻塞的表进行静态分析，并利用预计算的外键表指纹列，实现查询执行中的单次双向信息传递。

Result: 在JOB基准测试中，Parachute将DuckDB v1.2的端到端执行时间分别提升了1.54倍（无半连接过滤）和1.24倍（带半连接过滤），额外空间占用为15%。

Conclusion: 该方法为实现高效的查询执行提供了新的思路，通过单次双向信息传递显著提升了性能。

Abstract: Sideways information passing is a well-known technique for mitigating the
impact of large build sides in a database query plan. As currently implemented
in production systems, sideways information passing enables only a
uni-directional information flow, as opposed to instance-optimal algorithms,
such as Yannakakis'. On the other hand, the latter require an additional pass
over the input, which hinders adoption in production systems.
  In this paper, we make a step towards enabling single-pass bi-directional
information passing during query execution. We achieve this by statically
analyzing between which tables the information flow is blocked and by
leveraging precomputed join-induced fingerprint columns on FK-tables. On the
JOB benchmark, Parachute improves DuckDB v1.2's end-to-end execution time
without and with semi-join filtering by 1.54x and 1.24x, respectively, when
allowed to use 15% extra space.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [93] [An Efficient Hardware Implementation of Elliptic Curve Point Multiplication over $GF(2^m)$ on FPGA](https://arxiv.org/abs/2506.12359)
*Ruby Kumari,Tapas Rout,Babul Saini,Jai Gopal Pandey,Abhijit Karmakar*

Main category: cs.AR

TL;DR: 提出了一种基于混合Karatsuba乘法器的有限域乘法新实现方法，显著提升了ECC的计算速度，同时保持合理的硬件资源占用。


<details>
  <summary>Details</summary>
Motivation: 椭圆曲线密码学（ECC）在资源受限的IoT设备中常用于安全数据交换，而有限域乘法是其核心操作，优化其性能对提升ECC效率至关重要。

Method: 使用混合Karatsuba乘法器实现有限域乘法，结合有限域加法器、平方器和扩展欧几里得反演电路，设计了一个基于Montgomery算法的ECPM架构。

Result: 在Xilinx Virtex-7 FPGA上实现了GF(2^163)的ECPM架构，最大频率达到213 MHz，占用14,195 LUT，计算速度和性能显著优于其他设计。

Conclusion: 所提出的混合Karatsuba乘法器方法在ECC中实现了高效计算，为IoT应用提供了优化的安全解决方案。

Abstract: Elliptic Curve Cryptography (ECC) is widely accepted for ensuring secure data
exchange between resource-limited IoT devices. The National Institute of
Standards and Technology (NIST) recommended implementation, such as B-163, is
particularly well-suited for Internet of Things (IoT) applications. Here,
Elliptic Curve Point Multiplication (ECPM) is the most time-critical and
resource-intensive operation due to the finite field multiplier. This paper
proposes a new implementation method of finite field multiplication using a
hybrid Karatsuba multiplier, which achieves a significant improvement in
computation time while maintaining a reasonable area footprint. The proposed
multiplier, along with a finite field adder, squarer, and extended Euclidean
inversion circuit, is used to implement an architecture for ECPM using the
Montgomery algorithm. The architecture is evaluated for $GF(2^{163})$ on the
Xilinx Virtex-7 FPGA platform, achieving a maximum frequency of 213~MHz and
occupying 14,195 Lookup Tables (LUTs). The results demonstrate a significant
speedup in computation time and overall performance compared to other reported
designs.

</details>


### [94] [PuDHammer: Experimental Analysis of Read Disturbance Effects of Processing-using-DRAM in Real DRAM Chips](https://arxiv.org/abs/2506.12947)
*Ismail Emir Yuksel,Akash Sood,Ataberk Olgun,Oğuzhan Canpolat,Haocong Luo,F. Nisa Bostancı,Mohammad Sadrosadati,A. Giray Yağlıkçı,Onur Mutlu*

Main category: cs.AR

TL;DR: 研究探讨了DRAM多行激活（PuDHammer）对读取干扰的影响，发现其显著加剧了读取干扰漏洞，并提出了应对措施。


<details>
  <summary>Details</summary>
Motivation: DRAM的多行激活模式（用于PuD）与传统的单行激活不同，但其对读取干扰的影响尚未被研究。

Method: 使用316个DDR4 DRAM芯片，对比RowHammer和PuDHammer的表现，分析其读取干扰效应。

Result: PuDHammer导致首次位翻转所需的锤击次数大幅减少，且能绕过现有的RowHammer缓解机制。

Conclusion: 研究提出了三种应对措施，并评估了现有行业标准缓解机制的适应性，但性能开销较高。

Abstract: Processing-using-DRAM (PuD) is a promising paradigm for alleviating the data
movement bottleneck using DRAM's massive internal parallelism and bandwidth to
execute very wide operations. Performing a PuD operation involves activating
multiple DRAM rows in quick succession or simultaneously, i.e., multiple-row
activation. Multiple-row activation is fundamentally different from
conventional memory access patterns that activate one DRAM row at a time.
However, repeatedly activating even one DRAM row (e.g., RowHammer) can induce
bitflips in unaccessed DRAM rows because modern DRAM is subject to read
disturbance. Unfortunately, no prior work investigates the effects of
multiple-row activation on DRAM read disturbance.
  In this paper, we present the first characterization study of read
disturbance effects of multiple-row activation-based PuD (which we call
PuDHammer) using 316 real DDR4 DRAM chips from four major DRAM manufacturers.
Our detailed characterization show that 1) PuDHammer significantly exacerbates
the read disturbance vulnerability, causing up to 158.58x reduction in the
minimum hammer count required to induce the first bitflip ($HC_{first}$),
compared to RowHammer, 2) PuDHammer is affected by various operational
conditions and parameters, 3) combining RowHammer with PuDHammer is more
effective than using RowHammer alone to induce read disturbance error, e.g.,
doing so reduces $HC_{first}$ by 1.66x on average, and 4) PuDHammer bypasses an
in-DRAM RowHammer mitigation mechanism (Target Row Refresh) and induces more
bitflips than RowHammer.
  To develop future robust PuD-enabled systems in the presence of PuDHammer, we
1) develop three countermeasures and 2) adapt and evaluate the state-of-the-art
RowHammer mitigation standardized by industry, called Per Row Activation
Counting (PRAC). We show that the adapted PRAC incurs large performance
overheads (48.26%, on average).

</details>


### [95] [FPGA & VPU Co-Processing in Space Applications: Development and Testing with DSP/AI Benchmarks](https://arxiv.org/abs/2506.12968)
*Vasileios Leon,Charalampos Bezaitis,George Lentaris,Dimitrios Soudris,Dionysios Reisis,Elissaios-Alexios Papatheofanous,Angelos Kyriakos,Aubrey Dunne,Arne Samuelsson,David Steenari*

Main category: cs.AR

TL;DR: 论文探讨了航天应用中异构计算架构的使用，采用FPGA和VPU协同处理方案，并通过实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 航天应用中对高性能和低功耗计算的需求推动了异构计算架构的研究。

Method: 使用Kintex FPGA作为框架处理器，Myriad2 VPU用于DSP/AI功能，通过CIF和LCD接口传输数据，并进行原型实验。

Result: 实验评估了接口性能、FPGA资源利用率和VPU计算吞吐量，验证了系统的数据处理能力。

Conclusion: 异构计算架构在航天数据处理中表现出潜力，能够满足高性能和低功耗需求。

Abstract: The advent of computationally demanding algorithms and high data rate
instruments in new space applications pushes the space industry to explore
disruptive solutions for on-board data processing. We examine heterogeneous
computing architectures involving high-performance and low-power commercial
SoCs. The current paper implements an FPGA with VPU co-processing architecture
utilizing the CIF & LCD interfaces for I/O data transfers. A Kintex FPGA serves
as our framing processor and heritage accelerator, while we offload novel
DSP/AI functions to a Myriad2 VPU. We prototype our architecture in the lab to
evaluate the interfaces, the FPGA resource utilization, the VPU computational
throughput, as well as the entire data handling system's performance, via
custom benchmarking.

</details>


### [96] [Towards Employing FPGA and ASIP Acceleration to Enable Onboard AI/ML in Space Applications](https://arxiv.org/abs/2506.12970)
*Vasileios Leon,George Lentaris,Dimitrios Soudris,Simon Vellas,Mathieu Bernou*

Main category: cs.AR

TL;DR: 该论文探讨了如何在卫星中利用FPGA和AI加速器实现AI/ML技术，包括硬件选择和架构设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML在陆地应用中的成功和商业航天的发展，卫星中引入AI/ML技术成为可能，但传统的机载处理器性能有限，需要借助FPGA和AI加速器。

Method: 通过工业趋势分析、对比研究和内部基准测试，设计基于FPGA和AI加速器的架构。

Result: 提出了利用FPGA和VPU/TPU协处理器实现高性能AI开发和动态重构的方案。

Conclusion: 选择合适的设备和设计高效架构对航天任务的成功至关重要。

Abstract: The success of AI/ML in terrestrial applications and the commercialization of
space are now paving the way for the advent of AI/ML in satellites. However,
the limited processing power of classical onboard processors drives the
community towards extending the use of FPGAs in space with both rad-hard and
Commercial-Off-The-Shelf devices. The increased performance of FPGAs can be
complemented with VPU or TPU ASIP co-processors to further facilitate
high-level AI development and in-flight reconfiguration. Thus, selecting the
most suitable devices and designing the most efficient avionics architecture
becomes crucial for the success of novel space missions. The current work
presents industrial trends, comparative studies with in-house benchmarking, as
well as architectural designs utilizing FPGAs and AI accelerators towards
enabling AI/ML in future space missions.

</details>


### [97] [Combining Fault Tolerance Techniques and COTS SoC Accelerators for Payload Processing in Space](https://arxiv.org/abs/2506.12971)
*Vasileios Leon,Elissaios Alexios Papatheofanous,George Lentaris,Charalampos Bezaitis,Nikolaos Mastorakis,Georgios Bampilis,Dionysios Reisis,Dimitrios Soudris*

Main category: cs.AR

TL;DR: 论文探讨了在太空应用中为Zynq FPGA和Myriad VPU这两种商用现成加速器设计容错技术，以提高其可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着太空应用对计算能力和I/O吞吐量需求的增加，商用现成加速器因其性能优势受到关注，但需提高其可靠性。

Method: 针对FPGA采用内存清理、部分重新配置、三模冗余和看门狗等技术；对VPU则检测并纠正指令和数据内存错误，并在处理器层面应用冗余。

Result: 开发了FPGA与VPU协同处理的容错接口，结合CIF/LCD协议和自定义CRC错误检测码。

Conclusion: 提出的容错技术能够有效提升商用现成加速器在太空应用中的可靠性。

Abstract: The ever-increasing demand for computational power and I/O throughput in
space applications is transforming the landscape of on-board computing. A
variety of Commercial-Off-The-Shelf (COTS) accelerators emerges as an
attractive solution for payload processing to outperform the traditional
radiation-hardened devices. Towards increasing the reliability of such COTS
accelerators, the current paper explores and evaluates fault-tolerance
techniques for the Zynq FPGA and the Myriad VPU, which are two device families
being integrated in industrial space avionics architectures/boards, such as
Ubotica's CogniSat, Xiphos' Q7S, and Cobham Gaisler's GR-VPX-XCKU060. On the
FPGA side, we combine techniques such as memory scrubbing, partial
reconfiguration, triple modular redundancy, and watchdogs. On the VPU side, we
detect and correct errors in the instruction and data memories, as well as we
apply redundancy at processor level (SHAVE cores). When considering FPGA with
VPU co-processing, we also develop a fault-tolerant interface between the two
devices based on the CIF/LCD protocols and our custom CRC error-detecting code.

</details>


### [98] [Reconfigurable Digital RRAM Logic Enables In-Situ Pruning and Learning for Edge AI](https://arxiv.org/abs/2506.13151)
*Songqi Wang,Yue Zhang,Jia Chen,Xinyuan Zhang,Yi Li,Ning Lin,Yangu He,Jichang Yang,Yingjie Yu,Yi Li,Zhongrui Wang,Xiaojuan Qi,Han Wang*

Main category: cs.AR

TL;DR: 通过软硬件协同设计，提出了一种实时动态剪枝策略和基于RRAM的全数字存内计算芯片，显著降低了能耗并保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 模仿人脑同时优化突触权重和拓扑结构的能力，解决当前AI系统在此方面的不足及高能耗问题。

Method: 算法层面引入实时动态剪枝策略，硬件层面设计基于1T1R RRAM的可重构全数字存内计算芯片。

Result: 在MNIST和ModelNet10上分别减少26.80%和59.94%的操作，能耗降低57.26%至86.53%，同时保持高精度。

Conclusion: 该协同设计为未来自适应、高能效的边缘智能提供了一种可扩展的解决方案。

Abstract: The human brain simultaneously optimizes synaptic weights and topology by
growing, pruning, and strengthening synapses while performing all computation
entirely in memory. In contrast, modern artificial-intelligence systems
separate weight optimization from topology optimization and depend on
energy-intensive von Neumann architectures. Here, we present a
software-hardware co-design that bridges this gap. On the algorithmic side, we
introduce a real-time dynamic weight-pruning strategy that monitors weight
similarity during training and removes redundancies on the fly, reducing
operations by 26.80% on MNIST and 59.94% on ModelNet10 without sacrificing
accuracy (91.44% and 77.75%, respectively). On the hardware side, we fabricate
a reconfigurable, fully digital compute-in-memory (CIM) chip based on 180 nm
one-transistor-one-resistor (1T1R) RRAM arrays. Each array embeds flexible
Boolean logic (NAND, AND, XOR, OR), enabling both convolution and similarity
evaluation inside memory and eliminating all ADC/DAC overhead. The digital
design achieves zero bit-error, reduces silicon area by 72.30% and overall
energy by 57.26% compared to analogue RRAM CIM, and lowers energy by 75.61% and
86.53% on MNIST and ModelNet10, respectively, relative to an NVIDIA RTX 4090.
Together, our co-design establishes a scalable brain-inspired paradigm for
adaptive, energy-efficient edge intelligence in the future.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [99] [The Transition Matrix -- A classification of navigational patterns between LMS course sections](https://arxiv.org/abs/2506.13275)
*Tobias Hildebrandt,Lars Mehnen*

Main category: cs.CY

TL;DR: 研究分析Moodle课程中学生导航模式，发现常见行为如顺序浏览章节，并识别混合学习场景中的典型行为。


<details>
  <summary>Details</summary>
Motivation: 随着Moodle课程复杂度增加，需了解学生如何导航以及课程设计是否满足需求，现有研究缺乏跨课程的导航模式分析。

Method: 分析747门Moodle课程的导航数据，通过过渡矩阵和热力图可视化比较跨课程的学生导航序列。

Result: 热力图显示对角线轴，表明学生通常按顺序浏览章节；其他模式如混合学习场景和主导章节也被识别。

Conclusion: 研究提供了跨课程导航模式的新视角，有助于优化Moodle课程设计以满足学生需求。

Abstract: Learning management systems (LMS) like Moodle are increasingly used to
support university teaching. As Moodle courses become more complex,
incorporating diverse interactive elements, it is important to understand how
students navigate through course sections and whether course designs are
meeting student needs. While substantial research exists on student usage of
individual LMS elements, there is a lack of research on broader navigational
patterns between course sections and how these patterns differ across courses.
This study analyzes navigational data from 747 courses in the Moodle LMS at a
technical university of applied sciences, representing (after filtering) around
4,400 students and 1.8 million logged events. By mapping section names across a
large sample of courses, the analysis enables cross-course comparisons of
student navigational sequences between sections. Transition matrices and heat
map visualizations are used to identify common navigational patterns. Findings
include that many of the generated heatmap include one or more diagonal axis,
indicating that students typically navigate from the current to the next or
previous section. More fine-grained patterns show typical behavior for blended
learning scenarios. Other patterns include dominant sections.

</details>


### [100] [An LLM's Apology: Outsourcing Awkwardness in the Age of AI](https://arxiv.org/abs/2506.13685)
*Twm Stone,Anna Soligo*

Main category: cs.CY

TL;DR: FLAKE-Bench是一个评估大型语言模型（LLM）在社交、职业和浪漫场景中礼貌且有效地取消承诺的能力的基准。研究发现，AI生成取消文本可以减少摩擦和道德负担。


<details>
  <summary>Details</summary>
Motivation: 现代社交中，临时取消约定可能导致尴尬或情感伤害，使用LLM生成理由可以减轻用户的创意负担和道德顾虑。

Method: 开发FLAKE-Bench基准，评估10种前沿LLM在不同场景中生成取消文本的效果。

Result: LLM可以有效生成礼貌且合理的取消文本，减少社交摩擦。

Conclusion: FLAKE-Bench为未来研究提供了开源工具，展示了LLM在社交灵活性方面的潜力。

Abstract: A key part of modern social dynamics is flaking at short notice. However,
anxiety in coming up with believable and socially acceptable reasons to do so
can instead lead to 'ghosting', awkwardness, or implausible excuses, risking
emotional harm and resentment in the other party. The ability to delegate this
task to a Large Language Model (LLM) could substantially reduce friction and
enhance the flexibility of user's social life while greatly minimising the
aforementioned creative burden and moral qualms. We introduce FLAKE-Bench, an
evaluation of models' capacity to effectively, kindly, and humanely extract
themselves from a diverse set of social, professional and romantic scenarios.
We report the efficacy of 10 frontier or recently-frontier LLMs in bailing on
prior commitments, because nothing says "I value our friendship" like having AI
generate your cancellation texts. We open-source FLAKE-Bench at
github.com/Cloakless/flake-bench to support future research.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [101] [Video-Guided Text-to-Music Generation Using Public Domain Movie Collections](https://arxiv.org/abs/2506.12573)
*Haven Kim,Zachary Novack,Weihan Xu,Julian McAuley,Hao-Wen Dong*

Main category: cs.SD

TL;DR: 论文介绍了Open Screen Sound Library（OSSL）数据集，旨在解决音乐生成系统在电影制作中应用受限的问题，通过结合视频适配器提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有音乐生成系统因缺乏综合数据集而难以满足电影制作的多因素需求（如视觉内容、对话和情感基调），限制了其在电影制作中的应用。

Method: 引入OSSL数据集（含公共领域电影的片段、高质量配乐和人工标注的情绪信息），并提出一种视频适配器来增强基于自回归变换器的文本到音乐模型。

Result: 实验表明，该方法显著提升了MusicGen-Medium模型在分布性和配对保真度的客观指标，以及情绪和类型的主观兼容性。

Conclusion: OSSL数据集和提出的视频适配器有效解决了音乐生成与电影制作需求不匹配的问题，为未来研究提供了有价值的工具和数据资源。

Abstract: Despite recent advancements in music generation systems, their application in
film production remains limited, as they struggle to capture the nuances of
real-world filmmaking, where filmmakers consider multiple factors-such as
visual content, dialogue, and emotional tone-when selecting or composing music
for a scene. This limitation primarily stems from the absence of comprehensive
datasets that integrate these elements. To address this gap, we introduce Open
Screen Sound Library (OSSL), a dataset consisting of movie clips from public
domain films, totaling approximately 36.5 hours, paired with high-quality
soundtracks and human-annotated mood information. To demonstrate the
effectiveness of our dataset in improving the performance of pre-trained models
on film music generation tasks, we introduce a new video adapter that enhances
an autoregressive transformer-based text-to-music model by adding video-based
conditioning. Our experimental results demonstrate that our proposed approach
effectively enhances MusicGen-Medium in terms of both objective measures of
distributional and paired fidelity, and subjective compatibility in mood and
genre. The dataset and code are available at
https://havenpersona.github.io/ossl-v1.

</details>


### [102] [Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV](https://arxiv.org/abs/2506.13001)
*Christian Zhou-Zheng,Philippe Pasquier*

Main category: cs.SD

TL;DR: 论文提出了一种基于RWKV-7线性架构的新模型MIDI-RWKV，用于个性化、多轨道、长上下文和可控的符号音乐填充，以增强计算机辅助作曲过程。


<details>
  <summary>Details</summary>
Motivation: 现有自动音乐生成工作主要关注端到端系统生成完整作品或延续，但缺乏人机交互的迭代过程，限制了计算机辅助创作的效果。

Method: 采用RWKV-7线性架构设计MIDI-RWKV模型，支持边缘设备上的高效协同创作，并提供初始状态微调方法以实现低样本个性化。

Result: 实验通过多项定量和定性指标评估了MIDI-RWKV及其状态微调效果，证明了其有效性。

Conclusion: MIDI-RWKV为计算机辅助作曲提供了高效的协同创作工具，同时通过状态微调实现了低样本个性化。

Abstract: Existing work in automatic music generation has primarily focused on
end-to-end systems that produce complete compositions or continuations.
However, because musical composition is typically an iterative process, such
systems make it difficult to engage in the back-and-forth between human and
machine that is essential to computer-assisted creativity. In this study, we
address the task of personalizable, multi-track, long-context, and controllable
symbolic music infilling to enhance the process of computer-assisted
composition. We present MIDI-RWKV, a novel model based on the RWKV-7 linear
architecture, to enable efficient and coherent musical cocreation on edge
devices. We also demonstrate that MIDI-RWKV admits an effective method of
finetuning its initial state for personalization in the very-low-sample regime.
We evaluate MIDI-RWKV and its state tuning on several quantitative and
qualitative metrics, and release model weights and code at
https://github.com/christianazinn/MIDI-RWKV.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [103] [Quantum Computing and Cybersecurity in Accounting and Finance: Current and the Future Challenges and Opportunities for Securing Accounting and Finance Systems](https://arxiv.org/abs/2506.12096)
*Huma Habib Shadan,Sardar Islam*

Main category: cs.CR

TL;DR: 这篇论文探讨了量子计算在会计与金融领域的网络安全应用，分析了其带来的机遇与风险，并提出了量子抗性算法和量子密钥分发（QKD）的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前会计网络安全在量子攻击下存在漏洞，传统加密方法面临风险，因此需要研究量子计算如何解决这些问题。

Method: 采用了PSALSAR系统性文献综述方法，分析了量子计算在加密技术中的应用及其对网络安全的影响。

Result: 量子计算能够提升加密技术，减少数据泄露和未经授权的访问。

Conclusion: 量子抗性算法和量子密钥分发（QKD）是未来保护会计与金融系统安全的关键技术。

Abstract: Quantum computing is revolutionising information systems and will have a
significant impact on accounting and finance, especially in the area of
cybersecurity. It presents both opportunities and risks in ensuring
confidentiality and protecting financial data. The purpose of this thesis is to
show the application of quantum technologies in accounting cybersecurity,
utilising quantum algorithms and QKD to overcome the limitations of classical
computing.
  The literature review reveals the vulnerabilities of the current accounting
cybersecurity to quantum attacks and the need for quantum-resistant
cryptographic mechanisms. It elaborates on the risks associated with
conventional encryption in the context of quantum capabilities. This study
contributes to the understanding of how quantum computing can revolutionise
accounting cybersecurity by enhancing quantum-resistant algorithms and
utilising quantum key distribution (QKD) in accounting.
  The study employs PSALSAR systematic review methodology to ensure rigour and
depth. The analysis shows that quantum computing enhances encryption techniques
to superior possibilities than classical ones. Using quantum technologies in
accounting minimises data breaches and unauthorised access. The study concludes
that quantum-resistant algorithms and quantum key distribution (QKD) are
necessary for securing the accounting and finance systems of the future.
  Keywords Quantum Computing, Cybersecurity, Accounting, Machine Learning,
Artificial Intelligence, Quantum Key Distribution, Operations Management

</details>


### [104] [Privacy-preserving and reward-based mechanisms of proof of engagement](https://arxiv.org/abs/2506.12523)
*Matteo Marco Montanari,Alessandro Aldini*

Main category: cs.CR

TL;DR: 研究扩展了PoA机制以支持更广泛的数字参与证明（PoE），探讨了包括分布式账本技术和中心化系统在内的解决方案，关注隐私、时空范围、证明可转移性及激励机制整合。


<details>
  <summary>Details</summary>
Motivation: 研究旨在扩展PoA机制，以支持用户通过数字方式证明其对特定活动的参与（PoE）。

Method: 探讨了不同技术解决方案，包括分布式账本技术（DLT）和中心化系统，重点关注隐私、时空范围、证明可转移性及激励机制整合。

Result: 提出了多种技术方案，强调了隐私保护、范围覆盖及激励机制的重要性。

Conclusion: 研究为数字参与证明提供了新思路，展示了DLT和中心化系统的潜力及其权衡。

Abstract: Proof-of-Attendance (PoA) mechanisms are typically employed to demonstrate a
specific user's participation in an event, whether virtual or in-person. The
goal of this study is to extend such mechanisms to broader contexts where the
user wishes to digitally demonstrate her involvement in a specific activity
(Proof-of-Engagement, PoE). This work explores different solutions, including
DLTs as well as established technologies based on centralized systems. The main
aspects we consider include the level of privacy guaranteed to users, the scope
of PoA/PoE (both temporal and spatial), the transferability of the proof, and
the integration with incentive mechanisms.

</details>


### [105] [Building Automotive Security on Internet Standards: An Integration of DNSSEC, DANE, and DANCE to Authenticate and Authorize In-Car Services](https://arxiv.org/abs/2506.13261)
*Timo Salomon,Mehmet Mueller,Philipp Meyer,Thomas C. Schmidt*

Main category: cs.CR

TL;DR: 本文提出了一种通过集成DNSSEC、DANE和DANCE到汽车中间件来认证和授权车载服务的方法，简化了密钥管理并增强了安全性。


<details>
  <summary>Details</summary>
Motivation: 随着汽车行业向软件即服务转型，远程攻击风险增加，需要覆盖车辆全生命周期的安全解决方案。

Method: 利用DNSSEC分离服务的加密认证与部署认证，服务供应商生成证书并由OEM通过DNSSEC TLSA记录签名发布。

Result: 基于成熟互联网标准的设计支持高安全性和可扩展性，安全分析和实际测试验证了其有效性。

Conclusion: 该方法为车载服务安全提供了可互操作、可扩展的解决方案，适用于数百万联网车辆。

Abstract: The automotive industry is undergoing a software-as-a-service transformation
that enables software-defined functions and post-sale updates via cloud and
vehicle-to-everything communication. Connectivity in cars introduces
significant security challenges, as remote attacks on vehicles have become
increasingly prevalent. Current automotive designs call for security solutions
that address the entire lifetime of a vehicle. In this paper, we propose to
authenticate and authorize in-vehicle services by integrating DNSSEC, DANE, and
DANCE with automotive middleware. Our approach decouples the cryptographic
authentication of the service from that of the service deployment with the help
of DNSSEC and thereby largely simplifies key management. We propose to
authenticate in-vehicle services by certificates that are solely generated by
the service suppliers but published on deployment via DNSSEC TLSA records
solely signed by the OEM. Building on well-established Internet standards
ensures interoperability with various current and future protocols, scalable
management of credentials for millions of connected vehicles at
well-established security levels. We back our design proposal by a security
analysis using the STRIDE threat model and by evaluations in a realistic
in-vehicle setup that demonstrate its effectiveness.

</details>


### [106] [Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor Poisoning in Anonymous Networks](https://arxiv.org/abs/2506.13563)
*Yali Yuan,Kai Xu,Ruolin Ma,Yuchen Zhang*

Main category: cs.CR

TL;DR: 该论文通过集成遗忘技术，提出一种检测并消除后门毒化攻击的方法，显著提升了网站在指纹识别中的抗攻击能力和运行效率。


<details>
  <summary>Details</summary>
Motivation: 针对网站指纹识别（WF）中面临的隐蔽后门毒化攻击问题，设计了一种可行的方法，以增强模型的安全性和效率。

Method: 利用遗忘技术，通过评估训练样本对已知毒化测试点的影响值，动态调整模型参数以消除后门攻击的影响。

Result: 在公开数据集上的实验表明，该方法在毒化数据集和测试数据集上的准确率稳定在80%左右，运行效率提高了2-3倍。

Conclusion: 结合机器遗忘技术，论文提出的WF攻击模型在对抗性环境中表现出更强的抗毒化攻击能力和更快的执行速度。

Abstract: Website Fingerprinting (WF) is an effective tool for regulating and governing
the dark web. However, its performance can be significantly degraded by
backdoor poisoning attacks in practical deployments. This paper aims to address
the problem of hidden backdoor poisoning attacks faced by Website
Fingerprinting attack, and designs a feasible mothed that integrates unlearning
technology to realize detection of automatic poisoned points and complete
removal of its destructive effects, requiring only a small number of known
poisoned test points. Taking Tor onion routing as an example, our method
evaluates the influence value of each training sample on these known poisoned
test points as the basis for judgment. We optimize the use of influence scores
to identify poisoned samples within the training dataset. Furthermore, by
quantifying the difference between the contribution of model parameters on the
taining data and the clean data, the target parameters are dynamically adjusted
to eliminate the impact of the backdoor attacks. Experiments on public datasets
under the assumptions of closed-world (CW) and open-world (OW) verify the
effectiveness of the proposed method. In complex scenes containing both clean
website fingerprinting features and backdoor triggers, the accuracy of the
model on the poisoned dataset and the test dataset is stable at about 80%,
significantly outperforming the traditional WF attack models. In addition, the
proposed method achieves a 2-3 times speedup in runtime efficiency compared to
baseline methods. By incorporating machine unlearning, we realize a WF attack
model that exhibits enhanced resistance to backdoor poisoning and faster
execution speeds in adversarial settings.

</details>


### [107] [On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains](https://arxiv.org/abs/2506.13246)
*Craig Steven Wright*

Main category: cs.CR

TL;DR: 论文提出了一种基于区块链的合成智能体架构，确保记忆不可变、推理可验证且知识增长受限，解决了传统AI系统的易变性和不透明问题。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统依赖于可变的统计模型，易出现知识漂移和历史修正主义。本文旨在设计一种可验证、不可篡改的智能体架构，适用于高可靠性系统。

Method: 提出了Merkle Automaton架构，结合形式自动机理论和区块链技术，通过Merkle结构和零知识证明确保记忆和推理的不可篡改性与隐私保护。

Result: 实现了智能体的记忆作为不可变的账本，推理过程可验证且知识增长受形式逻辑约束，适用于法律、经济等高可靠性系统。

Conclusion: 该架构为需要可证明记忆和不可伪造来源的系统提供了理论基础，实现了智能体的可验证性和抗修订性。

Abstract: This paper presents a formalised architecture for synthetic agents designed
to retain immutable memory, verifiable reasoning, and constrained epistemic
growth. Traditional AI systems rely on mutable, opaque statistical models prone
to epistemic drift and historical revisionism. In contrast, we introduce the
concept of the Merkle Automaton, a cryptographically anchored, deterministic
computational framework that integrates formal automata theory with
blockchain-based commitments. Each agent transition, memory fragment, and
reasoning step is committed within a Merkle structure rooted on-chain,
rendering it non-repudiable and auditably permanent. To ensure selective access
and confidentiality, we derive symmetric encryption keys from ECDH exchanges
contextualised by hierarchical privilege lattices. This enforces cryptographic
access control over append-only DAG-structured knowledge graphs. Reasoning is
constrained by formal logic systems and verified through deterministic
traversal of policy-encoded structures. Updates are non-destructive and
historied, preserving epistemic lineage without catastrophic forgetting.
Zero-knowledge proofs facilitate verifiable, privacy-preserving inclusion
attestations. Collectively, this architecture reframes memory not as a cache
but as a ledger - one whose contents are enforced by protocol, bound by
cryptography, and constrained by formal logic. The result is not an intelligent
agent that mimics thought, but an epistemic entity whose outputs are provably
derived, temporally anchored, and impervious to post hoc revision. This design
lays foundational groundwork for legal, economic, and high-assurance
computational systems that require provable memory, unforgeable provenance, and
structural truth.

</details>


### [108] [EBS-CFL: Efficient and Byzantine-robust Secure Clustered Federated Learning](https://arxiv.org/abs/2506.13612)
*Zhiqiang Li,Haiyong Bao,Menghong Guan,Hao Pan,Cheng Huang,Hong-Ning Dai*

Main category: cs.CR

TL;DR: EBS-CFL是一种高效且安全的聚合方案，用于解决聚类联邦学习中的隐私问题和对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据异构性下的性能下降，以及用户因隐私问题不愿共享聚类身份，需要一种既能保护隐私又能抵御攻击的解决方案。

Method: 提出EBS-CFL方案，通过丢弃负相关梯度并加权聚合正相关梯度来抵御攻击，同时保护聚类身份的隐私。

Result: EBS-CFL在通信和计算效率上显著优于现有方案，并通过实验验证了其有效性。

Conclusion: EBS-CFL在保护隐私和抵御攻击的同时，提升了聚类联邦学习的效率和安全性。

Abstract: Despite federated learning (FL)'s potential in collaborative learning, its
performance has deteriorated due to the data heterogeneity of distributed
users. Recently, clustered federated learning (CFL) has emerged to address this
challenge by partitioning users into clusters according to their similarity.
However, CFL faces difficulties in training when users are unwilling to share
their cluster identities due to privacy concerns. To address these issues, we
present an innovative Efficient and Robust Secure Aggregation scheme for CFL,
dubbed EBS-CFL. The proposed EBS-CFL supports effectively training CFL while
maintaining users' cluster identity confidentially. Moreover, it detects
potential poisonous attacks without compromising individual client gradients by
discarding negatively correlated gradients and aggregating positively
correlated ones using a weighted approach. The server also authenticates
correct gradient encoding by clients. EBS-CFL has high efficiency with
client-side overhead O(ml + m^2) for communication and O(m^2l) for computation,
where m is the number of cluster identities, and l is the gradient size. When m
= 1, EBS-CFL's computational efficiency of client is at least O(log n) times
better than comparison schemes, where n is the number of clients.In addition,
we validate the scheme through extensive experiments. Finally, we theoretically
prove the scheme's security.

</details>


### [109] [Using LLMs for Security Advisory Investigations: How Far Are We?](https://arxiv.org/abs/2506.13161)
*Bayu Fedra Abdullah,Yusuf Sulistyo Nugroho,Brittany Reid,Raula Gaikovina Kula,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.CR

TL;DR: 研究评估了ChatGPT在生成安全公告、区分真实与虚假CVE-ID及从公告中提取CVE-ID的能力，发现其虽能生成可信公告但无法可靠区分真假ID，展示了在网络安全应用中的潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（如ChatGPT）在软件安全领域中生成可信漏洞公告的能力，揭示其在关键安全任务中的适用性与风险。

Method: 使用100个真实和100个虚假CVE-ID的筛选数据集，手动分析模型的输出可信度与一致性，评估其在三项任务中的表现。

Result: ChatGPT为96%的真实和97%的虚假CVE-ID生成了可信公告，但无法区分真假ID，且6%的重新评估中生成虚假ID，显示其局限性。

Conclusion: 研究强调在网络安全中谨慎使用LLM的必要性，并提出需改进模型设计以提高其在安全公告生成中的可靠性与适用性。

Abstract: Large Language Models (LLMs) are increasingly used in software security, but
their trustworthiness in generating accurate vulnerability advisories remains
uncertain. This study investigates the ability of ChatGPT to (1) generate
plausible security advisories from CVE-IDs, (2) differentiate real from fake
CVE-IDs, and (3) extract CVE-IDs from advisory descriptions. Using a curated
dataset of 100 real and 100 fake CVE-IDs, we manually analyzed the credibility
and consistency of the model's outputs. The results show that ChatGPT generated
plausible security advisories for 96% of given input real CVE-IDs and 97% of
given input fake CVE-IDs, demonstrating a limitation in differentiating between
real and fake IDs. Furthermore, when these generated advisories were
reintroduced to ChatGPT to identify their original CVE-ID, the model produced a
fake CVE-ID in 6% of cases from real advisories. These findings highlight both
the strengths and limitations of ChatGPT in cybersecurity applications. While
the model demonstrates potential for automating advisory generation, its
inability to reliably authenticate CVE-IDs or maintain consistency upon
re-evaluation underscores the risks associated with its deployment in critical
security tasks. Our study emphasizes the importance of using LLMs with caution
in cybersecurity workflows and suggests the need for further improvements in
their design to improve reliability and applicability in security advisory
generation.

</details>


### [110] [Tady: A Neural Disassembler without Structural Constraint Violations](https://arxiv.org/abs/2506.13323)
*Siliang Qin,Fengrui Yang,Hao Wang,Bolun Zhang,Zeyu Gao,Chao Zhang,Kai Chen*

Main category: cs.CR

TL;DR: 论文提出了一种新型神经反汇编工具Tady，通过改进模型架构和后处理算法，解决了现有工具因忽视全局结构完整性而产生的错误问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经反汇编工具因忽视全局结构约束而生成不符合实际的输出，限制了其实用性。

Method: 通过基于后支配关系的结构约束规范化解空间，并改进模型架构和后处理算法。

Result: Tady能有效消除结构约束违反，同时保持高效和指令级准确性。

Conclusion: Tady提供了一种高效且准确的神经反汇编方案，解决了现有工具的实用性问题。

Abstract: Disassembly is a crucial yet challenging step in binary analysis. While
emerging neural disassemblers show promise for efficiency and accuracy, they
frequently generate outputs violating fundamental structural constraints, which
significantly compromise their practical usability. To address this critical
problem, we regularize the disassembly solution space by formalizing and
applying key structural constraints based on post-dominance relations. This
approach systematically detects widespread errors in existing neural
disassemblers' outputs. These errors often originate from models' limited
context modeling and instruction-level decoding that neglect global structural
integrity. We introduce Tady, a novel neural disassembler featuring an improved
model architecture and a dedicated post-processing algorithm, specifically
engineered to address these deficiencies. Comprehensive evaluations on diverse
binaries demonstrate that Tady effectively eliminates structural constraint
violations and functions with high efficiency, while maintaining
instruction-level accuracy.

</details>


### [111] [SoK: The Privacy Paradox of Large Language Models: Advancements, Privacy Risks, and Mitigation](https://arxiv.org/abs/2506.12699)
*Yashothara Shanmugarasa,Ming Ding,M. A. P Chamikara,Thierry Rakotoarivelo*

Main category: cs.CR

TL;DR: 本文分析了大型语言模型（LLMs）在隐私方面的风险，重点关注训练数据、用户提示、生成输出和LLM代理的隐私挑战，并评估现有缓解机制的效果与局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM训练数据的隐私风险，而忽视了用户交互和LLM高级能力带来的隐私问题，本文旨在填补这一空白。

Method: 通过对LLM隐私问题的分类（训练数据、用户提示、生成输出、LLM代理），并提供现有缓解机制的评估。

Result: 识别了每类隐私挑战的具体问题和现有机制的局限性。

Conclusion: 指出了未来研究的方向，以更好地解决LLM隐私问题。

Abstract: Large language models (LLMs) are sophisticated artificial intelligence
systems that enable machines to generate human-like text with remarkable
precision. While LLMs offer significant technological progress, their
development using vast amounts of user data scraped from the web and collected
from extensive user interactions poses risks of sensitive information leakage.
Most existing surveys focus on the privacy implications of the training data
but tend to overlook privacy risks from user interactions and advanced LLM
capabilities. This paper aims to fill that gap by providing a comprehensive
analysis of privacy in LLMs, categorizing the challenges into four main areas:
(i) privacy issues in LLM training data, (ii) privacy challenges associated
with user prompts, (iii) privacy vulnerabilities in LLM-generated outputs, and
(iv) privacy challenges involving LLM agents. We evaluate the effectiveness and
limitations of existing mitigation mechanisms targeting these proposed privacy
challenges and identify areas for further research.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [112] [HELENA: High-Efficiency Learning-based channel Estimation using dual Neural Attention](https://arxiv.org/abs/2506.13408)
*Miguel Camelo Botero,Esra Aycan Beyazit,Nina Slamnik-Kriještorac,Johann M. Marquez-Barja*

Main category: eess.SP

TL;DR: HELENA是一种紧凑的深度学习模型，用于高效信道估计，结合轻量卷积和注意力机制，显著降低延迟和参数数量，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 在5G NR等OFDM系统中，低信噪比和严格延迟约束下，高精度信道估计至关重要。

Method: HELENA采用轻量卷积主干和两种高效注意力机制（全局捕捉的patch-wise多头自注意力和局部特征优化的压缩激励块）。

Result: 相比CEViT，HELENA推理时间减少45%，参数减少8倍，精度相当（-16.78 dB vs. -17.30 dB）。

Conclusion: HELENA适用于低延迟实时部署，是高效信道估计的理想选择。

Abstract: Accurate channel estimation is critical for high-performance Orthogonal
Frequency-Division Multiplexing systems such as 5G New Radio, particularly
under low signal-to-noise ratio and stringent latency constraints. This letter
presents HELENA, a compact deep learning model that combines a lightweight
convolutional backbone with two efficient attention mechanisms: patch-wise
multi-head self-attention for capturing global dependencies and a
squeeze-and-excitation block for local feature refinement. Compared to CEViT, a
state-of-the-art vision transformer-based estimator, HELENA reduces inference
time by 45.0\% (0.175\,ms vs.\ 0.318\,ms), achieves comparable accuracy
($-16.78$\,dB vs.\ $-17.30$\,dB), and requires $8\times$ fewer parameters
(0.11M vs.\ 0.88M), demonstrating its suitability for low-latency, real-time
deployment.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [113] [SPOT: Bridging Natural Language and Geospatial Search for Investigative Journalists](https://arxiv.org/abs/2506.13188)
*Lynn Khellaf,Ipek Baris Schlicht,Tilman Mirass,Julia Bayer,Tilman Wagner,Ruben Bouwmeester*

Main category: cs.IR

TL;DR: SPOT是一个自然语言接口工具，帮助非技术用户通过描述性语言查询OpenStreetMap数据，用于地理定位验证。


<details>
  <summary>Details</summary>
Motivation: 现有OSM查询工具（如Overpass Turbo）需要复杂查询语言知识，非技术用户难以使用。

Method: SPOT利用微调LLM将用户输入解析为结构化地理空间对象配置，并通过交互式地图展示结果。采用合成数据管道和语义捆绑系统提高查询准确性。

Result: SPOT实现了高精度的自然语言访问OSM数据，解决了模型输出幻觉、OSM标签不一致和用户输入噪声等问题。

Conclusion: SPOT降低了地理定位验证的技术门槛，为事实核查和打击虚假信息提供了实用工具。

Abstract: OpenStreetMap (OSM) is a vital resource for investigative journalists doing
geolocation verification. However, existing tools to query OSM data such as
Overpass Turbo require familiarity with complex query languages, creating
barriers for non-technical users. We present SPOT, an open source natural
language interface that makes OSM's rich, tag-based geographic data more
accessible through intuitive scene descriptions. SPOT interprets user inputs as
structured representations of geospatial object configurations using fine-tuned
Large Language Models (LLMs), with results being displayed in an interactive
map interface. While more general geospatial search tasks are conceivable, SPOT
is specifically designed for use in investigative journalism, addressing
real-world challenges such as hallucinations in model output, inconsistencies
in OSM tagging, and the noisy nature of user input. It combines a novel
synthetic data pipeline with a semantic bundling system to enable robust,
accurate query generation. To our knowledge, SPOT is the first system to
achieve reliable natural language access to OSM data at this level of accuracy.
By lowering the technical barrier to geolocation verification, SPOT contributes
a practical tool to the broader efforts to support fact-checking and combat
disinformation.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [114] [C2PO: Coherent Co-packaged Optics using offset-QAM-16 for Beyond PAM-4 Optical I/O](https://arxiv.org/abs/2506.12160)
*Dan Sturm,Marzieyh Rezaei,Alana Dee,Sajjad Moazeni*

Main category: eess.SY

TL;DR: 该论文提出了一种基于微环调制器（MRM）的相干共封装光器件（C2PO）发射器，用于实现高阶调制格式，展示出高带宽、低功耗和小尺寸的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的共封装光器件（CPO）在满足未来GPU和网络交换机对AI应用的高带宽和高能效需求方面存在局限性，因此需要开发更先进的调制格式。

Method: 利用微环谐振器（MRMs）实现相位恒定幅度调制，构建发射器，并通过商业硅光子工艺进行仿真和性能评估。

Result: 设计的发射器实现了400 Gb/s的传输速率，功耗与传统QAM-16调制相当，但面积减少了10-100倍。

Conclusion: 该研究证明了基于MRM的C2PO发射器在高阶调制中的可行性，展示了其在高带宽和能效方面的潜力。

Abstract: Co-packaged optics (CPO) has emerged as a promising solution for achieving
the ultra-high bandwidths, shoreline densities, and energy efficiencies
required by future GPUs and network switches for AI. Microring modulators
(MRMs) are well suited for transmitters due to their compact size, high energy
efficiency, and natural compatibility with dense wavelength-division
multiplexing (DWDM). However, extending beyond the recently demonstrated 200
Gb/s will require more advanced modulation formats, such as higher-order
coherent modulation (e.g., QAM-16).
  In this work, we show how microring resonators (MRMs) can be efficiently used
to implement phase-constant amplitude modulators and form the building blocks
of a transmitter for offset QAM-16, which has been shown to simplify
carrier-phase recovery relative to conventional QAM. We simulate and evaluate
the performance of our proposed MRM-based coherent CPO (C2PO) transmitters
using a foundry-provided commercial silicon photonics process, demonstrating an
input-normalized electric field amplitude contrast of 0.64 per dimension.
Through full link-level bit error rate modeling, we show that our design
achieves 400 Gb/s using offset QAM-16 at a total optical laser power of 9.65
dBm-comparable to that required by conventional QAM-16 MZI-based links, despite
using 10-100x less area. We further conduct a thermal simulation to assess the
transmitter's thermal stability at the MRM input optical power required to meet
a target BER at the desired data rates. Finally, as a proof of concept, we
demonstrate 25 Gb/s MRM-based offset QAM-4 modulation with a chip fabricated in
the GlobalFoundries 45 nm monolithic silicon photonics process.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [115] [Component Based Quantum Machine Learning Explainability](https://arxiv.org/abs/2506.12378)
*Barra White,Krishnendu Guha*

Main category: quant-ph

TL;DR: 该论文提出一种模块化、可解释的量子机器学习（QML）框架，通过分解QML算法的核心组件并应用解释性技术（如ALE和SHAP）来分析每个组件，从而提高QML模型的透明度。


<details>
  <summary>Details</summary>
Motivation: 在医疗和金融等领域，模型的透明度至关重要，可帮助发现预测中的偏见并符合GDPR合规要求。QML虽然具有量子优势，但其黑盒特性也需要解释性技术的支持。

Method: 将QML算法拆分为特征映射、变分电路、优化器等核心组件，并使用适应性解释性技术（如ALE和SHAP）分析每个组件，从而推断整体模型的解释性。

Result: 论文的目标是通过对各组件的分析，提供QML模型的整体解释性，帮助理解其输出生成的过程。

Conclusion: 该框架为QML模型的透明度和可解释性提供了新思路，有望在需要高透明度的领域中推动QML的应用。

Abstract: Explainable ML algorithms are designed to provide transparency and insight
into their decision-making process. Explaining how ML models come to their
prediction is critical in fields such as healthcare and finance, as it provides
insight into how models can help detect bias in predictions and help comply
with GDPR compliance in these fields. QML leverages quantum phenomena such as
entanglement and superposition, offering the potential for computational
speedup and greater insights compared to classical ML. However, QML models also
inherit the black-box nature of their classical counterparts, requiring the
development of explainability techniques to be applied to these QML models to
help understand why and how a particular output was generated.
  This paper will explore the idea of creating a modular, explainable QML
framework that splits QML algorithms into their core components, such as
feature maps, variational circuits (ansatz), optimizers, kernels, and
quantum-classical loops. Each component will be analyzed using explainability
techniques, such as ALE and SHAP, which have been adapted to analyse the
different components of these QML algorithms. By combining insights from these
parts, the paper aims to infer explainability to the overall QML model.

</details>


### [116] [OSI Stack Redesign for Quantum Networks: Requirements, Technologies, Challenges, and Future Directions](https://arxiv.org/abs/2506.12195)
*Shakil Ahmed,Muhammad Kamran Saeed,Ashfaq Khokhar*

Main category: quant-ph

TL;DR: 该论文提出了一个针对量子网络的OSI模型重新设计，引入了量子融合OSI栈，支持量子特性和智能编排，为7G网络奠定基础。


<details>
  <summary>Details</summary>
Motivation: 经典OSI模型无法支持量子通信的特殊现象（如相干性脆弱、概率纠缠和不可克隆定理），因此需要重新设计适合量子网络的架构。

Method: 通过扩展经典OSI模型，引入量子层（Layer 0）和认知意图层（Layer 8），定义各层的量子机制，并提出跨层赋能技术和仿真工具。

Result: 论文综合了150多项研究，提出了基于熵吞吐、相干延迟和纠缠保真度的评估框架，并展示了领域特定应用。

Conclusion: 论文为7G及未来的量子网络提出了可扩展、智能且合规的OSI框架，并指出未来的研究方向。

Abstract: Quantum communication is poised to become a foundational element of
next-generation networking, offering transformative capabilities in security,
entanglement-based connectivity, and computational offloading. However, the
classical OSI model-designed for deterministic and error-tolerant
systems-cannot support quantum-specific phenomena such as coherence fragility,
probabilistic entanglement, and the no-cloning theorem. This paper provides a
comprehensive survey and proposes an architectural redesign of the OSI model
for quantum networks in the context of 7G. We introduce a Quantum-Converged OSI
stack by extending the classical model with Layer 0 (Quantum Substrate) and
Layer 8 (Cognitive Intent), supporting entanglement, teleportation, and
semantic orchestration via LLMs and QML. Each layer is redefined to incorporate
quantum mechanisms such as enhanced MAC protocols, fidelity-aware routing, and
twin-based applications. This survey consolidates over 150 research works from
IEEE, ACM, MDPI, arXiv, and Web of Science (2018-2025), classifying them by OSI
layer, enabling technologies such as QKD, QEC, PQC, and RIS, and use cases such
as satellite QKD, UAV swarms, and quantum IoT. A taxonomy of cross-layer
enablers-such as hybrid quantum-classical control, metadata-driven
orchestration, and blockchain-integrated quantum trust-is provided, along with
simulation tools including NetSquid, QuNetSim, and QuISP. We present several
domain-specific applications, including quantum healthcare telemetry, entangled
vehicular networks, and satellite mesh overlays. An evaluation framework is
proposed based on entropy throughput, coherence latency, and entanglement
fidelity. Key future directions include programmable quantum stacks, digital
twins, and AI-defined QNet agents, laying the groundwork for a scalable,
intelligent, and quantum-compliant OSI framework for 7G and beyond.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [117] [SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models](https://arxiv.org/abs/2506.12935)
*Xingjian Diao,Chunhui Zhang,Keyi Kong,Weiyi Wu,Chiyu Ma,Zhongyu Ouyang,Peijun Qing,Soroush Vosoughi,Jiang Gui*

Main category: cs.CL

TL;DR: 本文通过提出Audio Logical Reasoning (ALR)数据集和SoundMind算法，结合高质量音频数据和强化学习技术，提升了大型音频语言模型(ALMs)的推理能力，达到先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在音频模态的推理能力仍显不足，需系统化解决方案。

Method: 提出ALR数据集（6,446个样本）和SoundMind规则强化学习算法，用于训练Qwen2.5-Omni-7B模型。

Result: SoundMind在ALR数据集上训练后，实现了音频逻辑推理的先进性能。

Conclusion: 高质量推理数据集结合专门强化学习技术，推动了听觉智能的前沿发展。

Abstract: While large language models have shown reasoning capabilities, their
application to the audio modality, particularly in large audio-language models
(ALMs), remains significantly underdeveloped. Addressing this gap requires a
systematic approach, involving a capable base model, high-quality
reasoning-oriented audio data, and effective training algorithms. In this
study, we present a comprehensive solution: we introduce the Audio Logical
Reasoning (ALR) dataset, consisting of 6,446 text-audio annotated samples
specifically designed for complex reasoning tasks. Building on this resource,
we propose SoundMind, a rule-based reinforcement learning (RL) algorithm
tailored to endow ALMs with deep bimodal reasoning abilities. By training
Qwen2.5-Omni-7B on the ALR dataset using SoundMind, our approach achieves
state-of-the-art performance in audio logical reasoning. This work highlights
the impact of combining high-quality, reasoning-focused datasets with
specialized RL techniques, advancing the frontier of auditory intelligence in
language models. Our code and the proposed dataset are available at
https://github.com/xid32/SoundMind.

</details>


### [118] [Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics](https://arxiv.org/abs/2506.12365)
*Asifullah khan,Muhammad Zaeem Khan,Saleha Jamshed,Sadia Ahmad,Aleesha Zainab,Kaynat Khatib,Faria Bibi,Abdul Rehman*

Main category: cs.CL

TL;DR: 本综述概述了大语言模型（LLM）领域的关键进展，包括推理能力提升、任务适应性增强、计算效率提高和伦理决策能力等。有效方法包括链式思考提示、指令微调和人类反馈强化学习。多模态学习和少样本/零样本技术进一步提升了LLM处理复杂任务的能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM的最新发展，以提升其推理、效率、伦理对齐等方面的能力，同时指出未充分研究的领域如可解释性、跨模态整合和可持续性。

Method: 综述了链式思考提示、指令微调、人类反馈强化学习等关键技术，以及多模态学习和少样本/零样本技术的应用。

Result: LLM在推理、效率和伦理对齐方面取得显著进展，但仍面临计算成本高、偏见和伦理风险等挑战。

Conclusion: 未来研究将聚焦于提升模型的多输入处理能力，使其更智能、安全和可靠，同时通过透明决策和伦理准则应对现有挑战。

Abstract: This survey paper outlines the key developments in the field of Large
Language Models (LLMs), such as enhancing their reasoning skills, adaptability
to various tasks, increased computational efficiency, and ability to make
ethical decisions. The techniques that have been most effective in bridging the
gap between human and machine communications include the Chain-of-Thought
prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.
The improvements in multimodal learning and few-shot or zero-shot techniques
have further empowered LLMs to handle complex jobs with minor input. They also
manage to do more with less by applying scaling and optimization tricks for
computing power conservation. This survey also offers a broader perspective on
recent advancements in LLMs going beyond isolated aspects such as model
architecture or ethical concerns. It categorizes emerging methods that enhance
LLM reasoning, efficiency, and ethical alignment. It also identifies
underexplored areas such as interpretability, cross-modal integration and
sustainability. With recent progress, challenges like huge computational costs,
biases, and ethical risks remain constant. Addressing these requires bias
mitigation, transparent decision-making, and clear ethical guidelines. Future
research will focus on enhancing models ability to handle multiple input,
thereby making them more intelligent, safe, and reliable.

</details>


### [119] [The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs](https://arxiv.org/abs/2506.12266)
*Avinash Baidya,Kamalika Das,Xiang Gao*

Main category: cs.CL

TL;DR: 本研究提出了一个评估框架来分析LLM代理与人类专家在任务导向对话系统中的行为差异，发现行为差异是影响性能的关键因素，尤其是在复杂任务中。减少行为差异可显著提升性能24.3%。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM代理在任务导向对话系统中表现突出，但在零样本场景下性能仍有显著差距，行为因素的驱动机制尚不明确。

Method: 研究提出一个全面评估框架，量化AI代理与人类专家在对话行为、工具使用和知识利用等方面的差异。

Result: 行为差异是影响性能的关键因素，任务复杂度越高差异越大（相关性0.963），GPT-4o代理在复杂任务中表现不佳。减少行为差异可提升性能24.3%。

Conclusion: 研究强调了行为评估的重要性，并提出改进对齐策略以提升LLM代理在复杂任务中的表现。

Abstract: Large Language Model (LLM)-based agents have significantly impacted
Task-Oriented Dialog Systems (TODS) but continue to face notable performance
challenges, especially in zero-shot scenarios. While prior work has noted this
performance gap, the behavioral factors driving the performance gap remain
under-explored. This study proposes a comprehensive evaluation framework to
quantify the behavior gap between AI agents and human experts, focusing on
discrepancies in dialog acts, tool usage, and knowledge utilization. Our
findings reveal that this behavior gap is a critical factor negatively
impacting the performance of LLM agents. Notably, as task complexity increases,
the behavior gap widens (correlation: 0.963), leading to a degradation of agent
performance on complex task-oriented dialogs. For the most complex task in our
study, even the GPT-4o-based agent exhibits low alignment with human behavior,
with low F1 scores for dialog acts (0.464), excessive and often misaligned tool
usage with a F1 score of 0.139, and ineffective usage of external knowledge.
Reducing such behavior gaps leads to significant performance improvement (24.3%
on average). This study highlights the importance of comprehensive behavioral
evaluations and improved alignment strategies to enhance the effectiveness of
LLM-based TODS in handling complex tasks.

</details>


### [120] [Improving Factuality for Dialogue Response Generation via Graph-Based Knowledge Augmentation](https://arxiv.org/abs/2506.12496)
*Xiangyan Chen,Yujian Gan,Matthew Purver*

Main category: cs.CL

TL;DR: 本文提出了一种新框架，通过结合知识三元组检索器、对话重写和知识增强的响应生成，减少大语言模型在对话中的幻觉问题，并改进了事实性评估方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在自然语言处理任务中表现优异，但容易产生幻觉（生成看似合理但实际错误或不一致的文本），尤其是在对话任务中。为此，需要一种方法增强对话响应的真实性。

Method: 提出了一种结合知识三元组检索器、对话重写和知识增强响应生成的框架。同时改进了事实性评分方法，以更可靠地评估对话响应的真实性。

Result: 在OpendialKG和HybriDialogue数据集上，该方法显著优于其他基于图知识的基线（包括最先进的G-retriever），提高了对话响应的真实性。

Conclusion: 该框架有效减少了对话中的幻觉问题，并提供了更可靠的事实性评估方法，为对话系统的真实性提升提供了新思路。

Abstract: Large Language Models (LLMs) succeed in many natural language processing
tasks. However, their tendency to hallucinate - generate plausible but
inconsistent or factually incorrect text - can cause problems in certain tasks,
including response generation in dialogue. To mitigate this issue,
knowledge-augmented methods have shown promise in reducing hallucinations.
Here, we introduce a novel framework designed to enhance the factuality of
dialogue response generation, as well as an approach to evaluate dialogue
factual accuracy. Our framework combines a knowledge triple retriever, a
dialogue rewrite, and knowledge-enhanced response generation to produce more
accurate and grounded dialogue responses. To further evaluate generated
responses, we propose a revised fact score that addresses the limitations of
existing fact-score methods in dialogue settings, providing a more reliable
assessment of factual consistency. We evaluate our methods using different
baselines on the OpendialKG and HybriDialogue datasets. Our methods
significantly improve factuality compared to other graph knowledge-augmentation
baselines, including the state-of-the-art G-retriever. The code will be
released on GitHub.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [121] [ICME 2025 Grand Challenge on Video Super-Resolution for Video Conferencing](https://arxiv.org/abs/2506.12269)
*Babak Naderi,Ross Cutler,Juhee Cho,Nabakumar Khongbantabam,Dejan Ivkovic*

Main category: eess.IV

TL;DR: 论文摘要介绍了视频超分辨率（VSR）技术的挑战，特别是针对会议场景的低分辨率视频，提出了一种在低延迟情况下使用因果模型增强视频感知质量的方法。


<details>
  <summary>Details</summary>
Motivation: 视频超分辨率技术在会议等场景中具有重要应用，能够从低分辨率视频中恢复高质量图像，提升用户体验。

Method: 论文采用了局部、单向、双向传播或传统上采样后恢复等方法，针对H.265编码的固定QP低分辨率视频进行上采样。

Result: 研究中提出了一个新的屏幕内容数据集，并通过主观测试（基于ITU-T Rec P.910的众包实现）对提交结果进行了评估。

Conclusion: 该挑战展示了视频超分辨率技术在低延迟场景下的应用潜力，并提供了一个新的数据集以支持未来研究。

Abstract: Super-Resolution (SR) is a critical task in computer vision, focusing on
reconstructing high-resolution (HR) images from low-resolution (LR) inputs. The
field has seen significant progress through various challenges, particularly in
single-image SR. Video Super-Resolution (VSR) extends this to the temporal
domain, aiming to enhance video quality using methods like local, uni-,
bi-directional propagation, or traditional upscaling followed by restoration.
This challenge addresses VSR for conferencing, where LR videos are encoded with
H.265 at fixed QPs. The goal is to upscale videos by a specific factor,
providing HR outputs with enhanced perceptual quality under a low-delay
scenario using causal models. The challenge included three tracks:
general-purpose videos, talking head videos, and screen content videos, with
separate datasets provided by the organizers for training, validation, and
testing. We open-sourced a new screen content dataset for the SR task in this
challenge. Submissions were evaluated through subjective tests using a
crowdsourced implementation of the ITU-T Rec P.910.

</details>


### [122] [UAV Object Detection and Positioning in a Mining Industrial Metaverse with Custom Geo-Referenced Data](https://arxiv.org/abs/2506.13505)
*Vasiliki Balaska,Ioannis Tsampikos Papapetros,Katerina Maria Oikonomou,Loukas Bampis,Antonios Gasteratos*

Main category: eess.IV

TL;DR: 论文提出了一种融合无人机传感、激光雷达地形建模和深度学习对象检测的系统架构，用于露天矿环境的高分辨率空间信息获取，支持数字孪生平台。


<details>
  <summary>Details</summary>
Motivation: 解决采矿行业在高效、安全获取高分辨率地理空间信息方面的挑战，以支持开采规划和现场监控。

Method: 集成无人机传感、LiDAR地形建模和深度学习对象检测，通过地理参考、3D重建和对象定位生成结构化空间数据。

Result: 系统提供了更高的覆盖率和自动化潜力，并展示了可扩展的实地验证地理数据工作流程。

Conclusion: 该研究为采矿行业的AI增强遥感发展奠定了基础，支持实时扩展和工业应用。

Abstract: The mining sector increasingly adopts digital tools to improve operational
efficiency, safety, and data-driven decision-making. One of the key challenges
remains the reliable acquisition of high-resolution, geo-referenced spatial
information to support core activities such as extraction planning and on-site
monitoring. This work presents an integrated system architecture that combines
UAV-based sensing, LiDAR terrain modeling, and deep learning-based object
detection to generate spatially accurate information for open-pit mining
environments. The proposed pipeline includes geo-referencing, 3D
reconstruction, and object localization, enabling structured spatial outputs to
be integrated into an industrial digital twin platform. Unlike traditional
static surveying methods, the system offers higher coverage and automation
potential, with modular components suitable for deployment in real-world
industrial contexts. While the current implementation operates in post-flight
batch mode, it lays the foundation for real-time extensions. The system
contributes to the development of AI-enhanced remote sensing in mining by
demonstrating a scalable and field-validated geospatial data workflow that
supports situational awareness and infrastructure safety.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [123] [HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for Robust Multimodal Hallucination and Factuality Detection in VLMs](https://arxiv.org/abs/2506.13038)
*Zijian Zhang,Xuecheng Wu,Danlei Huang,Siyu Yan,Chong Peng,Xuezhi Cao*

Main category: cs.CV

TL;DR: 本文提出了一种名为HKD4VLM的渐进式混合知识蒸馏框架，用于解决视觉语言模型的幻觉检测和事实性检查问题，通过分层蒸馏和增强推理策略提升了模型性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的快速发展，大规模多模态模型的负责任行为（如幻觉检测和事实性检查）成为研究重点。本文旨在通过知识蒸馏提升模型效率和性能。

Method: 提出HKD4VLM框架，包含金字塔式渐进在线蒸馏和三重耦合精炼蒸馏，从粗到细分层对齐知识，并引入映射偏移增强推理和多样化增强策略。

Result: 实验证明HKD4VLM的有效性，消融研究揭示了关键设计对性能提升的贡献。

Conclusion: HKD4VLM通过分层蒸馏和增强策略显著提升了视觉语言模型在负责任AI任务中的性能，为后续研究提供了参考。

Abstract: Driven by the rapid progress in vision-language models (VLMs), the
responsible behavior of large-scale multimodal models has become a prominent
research area, particularly focusing on hallucination detection and factuality
checking. In this paper, we present the solution for the two tracks of
Responsible AI challenge. Inspirations from the general domain demonstrate that
a smaller distilled VLM can often outperform a larger VLM that is directly
tuned on downstream tasks, while achieving higher efficiency. We thus jointly
tackle two tasks from the perspective of knowledge distillation and propose a
progressive hybrid knowledge distillation framework termed HKD4VLM.
Specifically, the overall framework can be decomposed into Pyramid-like
Progressive Online Distillation and Ternary-Coupled Refinement Distillation,
hierarchically moving from coarse-grained knowledge alignment to fine-grained
refinement. Besides, we further introduce the mapping shift-enhanced inference
and diverse augmentation strategies to enhance model performance and
robustness. Extensive experimental results demonstrate the effectiveness of our
HKD4VLM. Ablation studies provide insights into the critical design choices
driving performance gains.

</details>


### [124] [GroupNL: Low-Resource and Robust CNN Design over Cloud and Device](https://arxiv.org/abs/2506.12335)
*Chuntao Ding,Jianhang Xie,Junna Zhang,Salman Raza,Shangguang Wang,Jiannong Cao*

Main category: cs.CV

TL;DR: 论文提出GroupNL方法，通过非线性变换函数增强CNN模型鲁棒性，同时减少计算和传输资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理IoT设备采集的损坏图像时鲁棒性低，且计算和传输资源消耗高，GroupNL旨在解决这些问题。

Method: GroupNL利用数据无关的非线性变换函数生成多样化特征图，改进部分卷积滤波器设计，减少参数传输和计算资源。

Result: 在多个数据集上，GroupNL在模型鲁棒性和训练加速方面优于现有方法，准确率和训练速度均有显著提升。

Conclusion: GroupNL显著提升了CNN模型的鲁棒性和训练效率，适用于物联网设备中的资源受限场景。

Abstract: It has become mainstream to deploy Convolutional Neural Network (CNN) models
on ubiquitous Internet of Things (IoT) devices with the help of the cloud to
provide users with a variety of high-quality services. Most existing methods
have two limitations: (i) low robustness in handling corrupted image data
collected by IoT devices; and (ii) high consumption of computational and
transmission resources. To this end, we propose the Grouped NonLinear
transformation generation method (GroupNL), which generates diversified feature
maps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to
improve the robustness of the CNN model. Specifically, partial convolution
filters are designated as seed filters in a convolutional layer, and a small
set of feature maps, i.e., seed feature maps, are first generated based on
vanilla convolution operation. Then, we split seed feature maps into several
groups, each with a set of different NLFs, to generate corresponding diverse
feature maps with in-place nonlinear processing. Moreover, GroupNL effectively
reduces the parameter transmission between multiple nodes during model training
by setting the hyperparameters of NLFs to random initialization and not
updating them during model training, and reduces the computing resources by
using NLFs to generate feature maps instead of most feature maps generated
based on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C,
Icons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the
proposed GroupNL outperforms other state-of-the-art methods in model robust and
training acceleration. Specifically, on the Icons-50 dataset, the accuracy of
GroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla
ResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN
when trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset.

</details>


### [125] [Cross-architecture universal feature coding via distribution alignment](https://arxiv.org/abs/2506.12737)
*Changsheng Gao,Shan Liu,Feng Wu,Weisi Lin*

Main category: cs.CV

TL;DR: 论文提出了一种跨架构通用特征编码方法（CAUFC），通过两步分布对齐解决CNN和Transformer特征异构性的问题，并取得优于单架构基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有特征编码方法多为特定架构设计，无法适应CNN和Transformer特征并存的实际场景，需开发统一编码方法。

Method: 采用两步分布对齐：1）格式对齐，将CNN和Transformer特征统一为2D token格式；2）特征值对齐，通过截断和归一化协调统计分布。

Result: 在图像分类任务中，该方法实现了优于单架构基线的准确率-压缩率平衡。

Conclusion: CAUFC是迈向跨异构模型架构通用特征压缩的首步尝试。

Abstract: Feature coding has become increasingly important in scenarios where semantic
representations rather than raw pixels are transmitted and stored. However,
most existing methods are architecture-specific, targeting either CNNs or
Transformers. This design limits their applicability in real-world scenarios
where features from both architectures coexist. To address this gap, we
introduce a new research problem: cross-architecture universal feature coding
(CAUFC), which seeks to build a unified codec that can effectively compress
features from heterogeneous architectures. To tackle this challenge, we propose
a two-step distribution alignment method. First, we design the format alignment
method that unifies CNN and Transformer features into a consistent 2D token
format. Second, we propose the feature value alignment method that harmonizes
statistical distributions via truncation and normalization. As a first attempt
to study CAUFC, we evaluate our method on the image classification task.
Experimental results demonstrate that our method achieves superior
rate-accuracy trade-offs compared to the architecture-specific baseline. This
work marks an initial step toward universal feature compression across
heterogeneous model architectures.

</details>


### [126] [Inference-Time Gaze Refinement for Micro-Expression Recognition: Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing](https://arxiv.org/abs/2506.12524)
*Nuwan Bandara,Thivya Kandappu,Archan Misra*

Main category: cs.CV

TL;DR: 提出了一种模型无关的推理时间细化框架，通过后处理模块提升基于事件的视线估计模型的输出质量，改善了视线信号的平滑性和一致性。


<details>
  <summary>Details</summary>
Motivation: 基于事件的眼动跟踪具有高时间分辨率和抗运动伪影的特点，适合解码注意力、困惑或疲劳等细微心理状态。但现有模型输出存在噪声和抖动问题，需改进以适配下游任务。

Method: 框架包含两个后处理模块：1) 运动感知中值滤波抑制眨眼引起的尖峰；2) 基于光流的局部细化减少空间抖动和时间不连续性。并提出了新的‘抖动指标’量化时间平滑性。

Result: 在多个基线模型和受控数据集上均表现出一致性提升，优化后的视线信号更适合微表情分析和心理状态解码等任务。

Conclusion: 该框架为未来在真实环境中与多模态情感识别系统集成奠定了基础，显著提升了基于事件的视线估计的实用性。

Abstract: Event-based eye tracking holds significant promise for fine-grained cognitive
state inference, offering high temporal resolution and robustness to motion
artifacts, critical features for decoding subtle mental states such as
attention, confusion, or fatigue. In this work, we introduce a model-agnostic,
inference-time refinement framework designed to enhance the output of existing
event-based gaze estimation models without modifying their architecture or
requiring retraining. Our method comprises two key post-processing modules: (i)
Motion-Aware Median Filtering, which suppresses blink-induced spikes while
preserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement,
which aligns gaze predictions with cumulative event motion to reduce spatial
jitter and temporal discontinuities. To complement traditional spatial accuracy
metrics, we propose a novel Jitter Metric that captures the temporal smoothness
of predicted gaze trajectories based on velocity regularity and local signal
complexity. Together, these contributions significantly improve the consistency
of event-based gaze signals, making them better suited for downstream tasks
such as micro-expression analysis and mind-state decoding. Our results
demonstrate consistent improvements across multiple baseline models on
controlled datasets, laying the groundwork for future integration with
multimodal affect recognition systems in real-world environments.

</details>


### [127] [VIS-Shepherd: Constructing Critic for LLM-based Data Visualization Generation](https://arxiv.org/abs/2506.13326)
*Bo Pan,Yixiao Fu,Ke Wang,Junyu Lu,Lunke Pan,Ziyang Qian,Yuhan Chen,Guoliang Wang,Yitao Zhou,Li Zheng,Yinghao Tang,Zhen Wen,Yuchen Wu,Junhua Lu,Biao Zhu,Minfeng Zhu,Bo Zhang,Wei Chen*

Main category: cs.CV

TL;DR: VIS-Shepherd是一款基于MLLM的专用批评模型，用于评估和改进LLM生成的数据可视化效果。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成数据可视化效果不佳的问题，减少人工干预的需求。

Method: 构建高质量的可视化批评数据集，结合人类和LLM生成的可视化实例及批评。

Result: 小规模MLLM模型（7B参数）通过高质量批评数据集，性能接近更大规模或专有模型。

Conclusion: 展示了MLLM在自动可视化批评中的潜力，为增强LLM生成可视化提供了新方向。

Abstract: Data visualization generation using Large Language Models (LLMs) has shown
promising results but often produces suboptimal visualizations that require
human intervention for improvement. In this work, we introduce VIS-Shepherd, a
specialized Multimodal Large Language Model (MLLM)-based critic to evaluate and
provide feedback for LLM-generated data visualizations. At the core of our
approach is a framework to construct a high-quality visualization critique
dataset, where we collect human-created visualization instances, synthesize
corresponding LLM-generated instances, and construct high-quality critiques. We
conduct both model-based automatic evaluation and human preference studies to
evaluate the effectiveness of our approach. Our experiments show that even
small (7B parameters) open-source MLLM models achieve substantial performance
gains by leveraging our high-quality visualization critique dataset, reaching
levels comparable to much larger open-source or even proprietary models. Our
work demonstrates significant potential for MLLM-based automated visualization
critique and indicates promising directions for enhancing LLM-based data
visualization generation. Our project page:
https://github.com/bopan3/VIS-Shepherd.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [128] [Semantic Scheduling for LLM Inference](https://arxiv.org/abs/2506.12204)
*Wenyue Hua,Dujian Ding,Yile Gu,Yujie Ren,Kai Mei,Minghua Ma,William Yang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于语义分析的调度算法，通过考虑进程的语义信息来优化大型语言模型（LLM）请求的调度优先级。


<details>
  <summary>Details</summary>
Motivation: 传统操作系统调度算法通常忽略内容语义，可能导致紧急或重要任务未被优先处理。利用语言模型的语义分析能力，可以更智能地调度任务。

Method: 设计了一种具有最优时间复杂度的调度算法，旨在最小化LLM提示调度的总体等待时间。

Result: 通过医疗紧急管理应用展示了该算法的有效性，证明其在关键、时间敏感任务中的优势。

Conclusion: 语义调度在优化关键任务处理方面具有显著潜力，代码和数据已开源。

Abstract: Conventional operating system scheduling algorithms are largely
content-ignorant, making decisions based on factors such as latency or fairness
without considering the actual intents or semantics of processes. Consequently,
these algorithms often do not prioritize tasks that require urgent attention or
carry higher importance, such as in emergency management scenarios. However,
recent advances in language models enable semantic analysis of processes,
allowing for more intelligent and context-aware scheduling decisions. In this
paper, we introduce the concept of semantic scheduling in scheduling of
requests from large language models (LLM), where the semantics of the process
guide the scheduling priorities. We present a novel scheduling algorithm with
optimal time complexity, designed to minimize the overall waiting time in
LLM-based prompt scheduling. To illustrate its effectiveness, we present a
medical emergency management application, underscoring the potential benefits
of semantic scheduling for critical, time-sensitive tasks. The code and data
are available at
https://github.com/Wenyueh/latency_optimization_with_priority_constraints.

</details>


### [129] [A Review of the Long Horizon Forecasting Problem in Time Series Analysis](https://arxiv.org/abs/2506.12809)
*Hans Krupakar,Kandappan V A*

Main category: cs.LG

TL;DR: 回顾了35年来长时域预测（LHF）问题的发展，重点探讨了深度学习中使用的技术（如卷积、注意力机制等）及其在LHF中的应用。通过ETTm2数据集的实验，发现xLSTM和Triformer模型在错误传播问题上表现突出。


<details>
  <summary>Details</summary>
Motivation: 分析了长时域预测问题的研究进展，探索深度学习技术在时间序列预测中的应用及其性能改进。

Method: 使用卷积、残差连接、注意力掩码等技术，结合时间序列分解和输入预处理方法，通过ETTm2数据集进行实验和消融研究。

Result: 实验结果表明，错误随预测时域长度增加而稳定上升，但xLSTM和Triformer模型表现优异，突出了LHF的误差传播问题。

Conclusion: 深度学习技术在LHF问题中有显著进展，尤其是xLSTM和Triformer模型的性能表明其在长时域预测中的潜力。

Abstract: The long horizon forecasting (LHF) problem has come up in the time series
literature for over the last 35 years or so. This review covers aspects of LHF
in this period and how deep learning has incorporated variants of trend,
seasonality, fourier and wavelet transforms, misspecification bias reduction
and bandpass filters while contributing using convolutions, residual
connections, sparsity reduction, strided convolutions, attention masks, SSMs,
normalization methods, low-rank approximations and gating mechanisms. We
highlight time series decomposition techniques, input data preprocessing and
dataset windowing schemes that improve performance. Multi-layer perceptron
models, recurrent neural network hybrids, self-attention models that improve
and/or address the performances of the LHF problem are described, with an
emphasis on the feature space construction. Ablation studies are conducted over
the ETTm2 dataset in the multivariate and univariate high useful load (HUFL)
forecasting contexts, evaluated over the last 4 months of the dataset. The
heatmaps of MSE averages per time step over test set series in the horizon show
that there is a steady increase in the error proportionate to its length except
with xLSTM and Triformer models and motivate LHF as an error propagation
problem. The trained models are available here: https://bit.ly/LHFModelZoo

</details>


### [130] [Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with Heterogeneous LoRA Allocation](https://arxiv.org/abs/2506.12213)
*Zikai Zhang,Ping Liu,Jiahao Xu,Rui Hu*

Main category: cs.LG

TL;DR: Fed-HeLLo提出了一种基于LoRA的联邦学习框架，通过异构LoRA分配策略（HLA）优化客户端的本地训练，提升全局性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法未充分考虑客户端资源异构性或缺乏有效的本地训练策略，Fed-HeLLo旨在解决这些问题。

Method: 开发了多种HLA策略，包括基于Fisher信息矩阵的动态梯度规范分配和基于几何定义的分配策略，以及随机化版本。

Result: 在五种数据集和不同数据分布设置下的实验表明，Fed-HeLLo在资源受限条件下表现高效且有效。

Conclusion: Fed-HeLLo通过结合动态和固定的层重要性，优化了联邦学习中的LoRA微调效果。

Abstract: Federated Learning has recently been utilized to collaboratively fine-tune
foundation models across multiple clients. Notably, federated low-rank
adaptation LoRA-based fine-tuning methods have recently gained attention, which
allows clients to fine-tune FMs with a small portion of trainable parameters
locally. However, most existing methods do not account for the heterogeneous
resources of clients or lack an effective local training strategy to maximize
global fine-tuning performance under limited resources. In this work, we
propose Fed-HeLLo, a novel federated LoRA-based fine-tuning framework that
enables clients to collaboratively fine-tune an FM with different local
trainable LoRA layers. To ensure its effectiveness, we develop several
heterogeneous LoRA allocation (HLA) strategies that adaptively allocate local
trainable LoRA layers based on clients' resource capabilities and the layer
importance. Specifically, based on the dynamic layer importance, we design a
Fisher Information Matrix score-based HLA that leverages dynamic gradient norm
information. To better stabilize the training process, we consider the
intrinsic importance of LoRA layers and design a Geometrically-Defined HLA
strategy. It shapes the collective distribution of trainable LoRA layers into
specific geometric patterns, such as Triangle, Inverted Triangle, Bottleneck,
and Uniform. Moreover, we extend GD-HLA into a randomized version, named
Randomized Geometrically-Defined HLA, for enhanced model accuracy with
randomness. By co-designing the proposed HLA strategies, we incorporate both
the dynamic and intrinsic layer importance into the design of our HLA strategy.
We evaluate our approach on five datasets under diverse federated LoRA
fine-tuning settings, covering three levels of data distribution from IID to
extreme Non-IID. Results show that Fed-HeLLo with HLA strategies is both
effective and efficient.

</details>


### [131] [Perfect Privacy for Discriminator-Based Byzantine-Resilient Federated Learning](https://arxiv.org/abs/2506.13561)
*Yue Xia,Christoph Hofmeister,Maximilian Egger,Rawad Bitar*

Main category: cs.LG

TL;DR: 论文提出了ByITFL和LoByITFL两种新型联邦学习方案，旨在增强对抗拜占庭用户的鲁棒性并保护用户数据隐私。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在大规模机器学习中表现出潜力，但带来了新的隐私和安全性挑战。

Method: ByITFL利用拉格朗日编码计算和重随机化实现信息论隐私；LoByITFL通过减少通信成本实现隐私，但需可信第三方在初始化阶段协助。

Result: 理论分析和实验结果验证了方案的隐私、拜占庭鲁棒性及收敛性保障。

Conclusion: 两种方案在隐私保护和鲁棒性方面提供了创新解决方案，但需权衡通信成本与可信第三方需求。

Abstract: Federated learning (FL) shows great promise in large-scale machine learning
but introduces new privacy and security challenges. We propose ByITFL and
LoByITFL, two novel FL schemes that enhance resilience against Byzantine users
while keeping the users' data private from eavesdroppers. To ensure privacy and
Byzantine resilience, our schemes build on having a small representative
dataset available to the federator and crafting a discriminator function
allowing the mitigation of corrupt users' contributions. ByITFL employs
Lagrange coded computing and re-randomization, making it the first
Byzantine-resilient FL scheme with perfect Information-Theoretic (IT) privacy,
though at the cost of a significant communication overhead. LoByITFL, on the
other hand, achieves Byzantine resilience and IT privacy at a significantly
reduced communication cost, but requires a Trusted Third Party, used only in a
one-time initialization phase before training. We provide theoretical
guarantees on privacy and Byzantine resilience, along with convergence
guarantees and experimental results validating our findings.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [132] [Using Behavior Trees in Risk Assessment](https://arxiv.org/abs/2506.12089)
*Razan Ghzouli,Atieh Hanna,Endre Erös,Rebekka Wohlrab*

Main category: cs.RO

TL;DR: 本文提出了一种基于行为树的模型，用于在机器人任务设计早期进行风险评估，以弥补当前行业实践中的不足。


<details>
  <summary>Details</summary>
Motivation: 工业中早期风险评估的实践不足，安全专家难以在项目初期完全理解机器人任务并确保风险评估结果在实施中被充分考虑。

Method: 采用设计科学研究方法，开发了一种基于行为树的模型，支持以开发为中心的风险评估活动。

Result: 与四家公司的五位从业者共同评估了该模型，结果显示其在早期风险识别、可视化以及代码实施与风险评估之间的桥梁作用方面具有潜力。

Conclusion: 这是首次尝试使用行为树模型支持风险评估，结果表明需要进一步开发和完善。

Abstract: Cyber-physical production systems increasingly involve collaborative robotic
missions, requiring more demand for robust and safe missions. Industries rely
on risk assessments to identify potential failures and implement measures to
mitigate their risks. Although it is recommended to conduct risk assessments
early in the design of robotic missions, the state of practice in the industry
is different. Safety experts often struggle to completely understand robotics
missions at the early design stages of projects and to ensure that the output
of risk assessments is adequately considered during implementation.
  This paper presents a design science study that conceived a model-based
approach for early risk assessment in a development-centric way. Our approach
supports risk assessment activities by using the behavior-tree model. We
evaluated the approach together with five practitioners from four companies.
Our findings highlight the potential of the behavior-tree model in supporting
early identification, visualisation, and bridging the gap between code
implementation and risk assessments' outputs. This approach is the first
attempt to use the behavior-tree model to support risk assessment; thus, the
findings highlight the need for further development.

</details>


### [133] [ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration](https://arxiv.org/abs/2506.12248)
*Jennifer Grannen,Siddharth Karamcheti,Blake Wulfe,Dorsa Sadigh*

Main category: cs.RO

TL;DR: ProVox框架通过大型语言模型和元提示协议，使协作机器人能主动推断用户意图，减少用户负担，提高任务效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在让机器人通过早期交互推断用户意图，提前规划行为，减少用户显式指令的需求。

Method: 引入基于大型语言模型的ProVox框架，使用元提示协议个性化用户偏好，通过上下文主动规划行为。

Result: 实验表明ProVox显著提升任务效率（任务完成时间减少38.7%）并降低用户负担（减少31.9%）。

Conclusion: 元提示和主动行为是提高协作效率的关键，ProVox为机器人协作提供了有效解决方案。

Abstract: Collaborative robots must quickly adapt to their partner's intent and
preferences to proactively identify helpful actions. This is especially true in
situated settings where human partners can continually teach robots new
high-level behaviors, visual concepts, and physical skills (e.g., through
demonstration), growing the robot's capabilities as the human-robot pair work
together to accomplish diverse tasks. In this work, we argue that robots should
be able to infer their partner's goals from early interactions and use this
information to proactively plan behaviors ahead of explicit instructions from
the user. Building from the strong commonsense priors and steerability of large
language models, we introduce ProVox ("Proactive Voice"), a novel framework
that enables robots to efficiently personalize and adapt to individual
collaborators. We design a meta-prompting protocol that empowers users to
communicate their distinct preferences, intent, and expected robot behaviors
ahead of starting a physical interaction. ProVox then uses the personalized
prompt to condition a proactive language model task planner that anticipates a
user's intent from the current interaction context and robot capabilities to
suggest helpful actions; in doing so, we alleviate user burden, minimizing the
amount of time partners spend explicitly instructing and supervising the robot.
We evaluate ProVox through user studies grounded in household manipulation
tasks (e.g., assembling lunch bags) that measure the efficiency of the
collaboration, as well as features such as perceived helpfulness, ease of use,
and reliability. Our analysis suggests that both meta-prompting and proactivity
are critical, resulting in 38.7% faster task completion times and 31.9% less
user burden relative to non-active baselines. Supplementary material, code, and
videos can be found at https://provox-2025.github.io.

</details>


### [134] [CHARM: Considering Human Attributes for Reinforcement Modeling](https://arxiv.org/abs/2506.13079)
*Qidi Fang,Hang Yu,Shijie Fang,Jindan Huang,Qiuyu Chen,Reuben M. Aronson,Elaine S. Short*

Main category: cs.RO

TL;DR: 研究发现人类反馈模式不仅与任务统计（如奖励）相关，还与参与者特性（如机器人经验和教育背景）相关，并展示了结合人类特性可更准确地预测反馈价值。


<details>
  <summary>Details</summary>
Motivation: 探讨人类特性如何影响反馈模式，填补了相关研究的空白。

Method: 设计了一项公共空间研究，包含两个长期任务和46名参与者，分析反馈模式与人类特性的关联。

Result: 反馈模式与参与者特性（特别是机器人经验和教育背景）显著相关，且结合人类特性可更准确预测反馈价值。

Conclusion: 人类特性对反馈模式有重要影响，未来研究应考虑这些因素以提高强化学习中反馈的质量。

Abstract: Reinforcement Learning from Human Feedback has recently achieved significant
success in various fields, and its performance is highly related to feedback
quality. While much prior work acknowledged that human teachers'
characteristics would affect human feedback patterns, there is little work that
has closely investigated the actual effects. In this work, we designed an
exploratory study investigating how human feedback patterns are associated with
human characteristics. We conducted a public space study with two long horizon
tasks and 46 participants. We found that feedback patterns are not only
correlated with task statistics, such as rewards, but also correlated with
participants' characteristics, especially robot experience and educational
background. Additionally, we demonstrated that human feedback value can be more
accurately predicted with human characteristics compared to only using task
statistics. All human feedback and characteristics we collected, and codes for
our data collection and predicting more accurate human feedback are available
at https://github.com/AABL-Lab/CHARM

</details>


### [135] [A Survey on Imitation Learning for Contact-Rich Tasks in Robotics](https://arxiv.org/abs/2506.13498)
*Toshiaki Tsuji,Yasuhiro Kato,Gokhan Solak,Heng Zhang,Tadej Petrič,Francesco Nori,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文综述了模仿学习在接触密集型机器人任务中的研究趋势，分析了示范收集方法和模仿学习技术，并探讨了多模态学习和基础模型的最新进展。


<details>
  <summary>Details</summary>
Motivation: 接触密集型任务因复杂的物理交互需求和非线性动力学特性成为机器人领域的核心挑战，研究其模仿学习方法有助于推动技术进步。

Method: 通过系统整理示范收集方法（如教学方式和传感模态）和分析模仿学习技术，评估其在接触密集型任务中的应用。

Result: 多模态学习和基础模型的引入显著提升了工业、家庭和医疗领域中复杂接触任务的性能。

Conclusion: 本综述为未来接触密集型机器人操作的进一步发展提供了研究基础和挑战方向。

Abstract: This paper comprehensively surveys research trends in imitation learning for
contact-rich robotic tasks. Contact-rich tasks, which require complex physical
interactions with the environment, represent a central challenge in robotics
due to their nonlinear dynamics and sensitivity to small positional deviations.
The paper examines demonstration collection methodologies, including teaching
methods and sensory modalities crucial for capturing subtle interaction
dynamics. We then analyze imitation learning approaches, highlighting their
applications to contact-rich manipulation. Recent advances in multimodal
learning and foundation models have significantly enhanced performance in
complex contact tasks across industrial, household, and healthcare domains.
Through systematic organization of current research and identification of
challenges, this survey provides a foundation for future advancements in
contact-rich robotic manipulation.

</details>


### [136] [Critical Insights about Robots for Mental Wellbeing](https://arxiv.org/abs/2506.13739)
*Guy Laban,Micol Spitale,Minja Axelsson,Nida Itrat Abbasi,Hatice Gunes*

Main category: cs.RO

TL;DR: 社交机器人作为支持情感健康的工具，本文总结了六个关键见解，包括福祉测量的多样性、机器人的非陪伴作用、虚拟互动的潜力、临床医生参与设计、互动时长的影响以及个性化并非必需。


<details>
  <summary>Details</summary>
Motivation: 探讨社交机器人在非临床环境中支持心理健康的机会与挑战。

Method: 基于实证研究和实际部署，总结了六个关键见解。

Result: 机器人应作为支持工具而非替代人类治疗师，设计需谨慎且基于证据。

Conclusion: 通过伦理和心理学考量，指导未来机器人心理健康应用的负责任发展。

Abstract: Social robots are increasingly being explored as tools to support emotional
wellbeing, particularly in non-clinical settings. Drawing on a range of
empirical studies and practical deployments, this paper outlines six key
insights that highlight both the opportunities and challenges in using robots
to promote mental wellbeing. These include (1) the lack of a single, objective
measure of wellbeing, (2) the fact that robots don't need to act as companions
to be effective, (3) the growing potential of virtual interactions, (4) the
importance of involving clinicians in the design process, (5) the difference
between one-off and long-term interactions, and (6) the idea that adaptation
and personalization are not always necessary for positive outcomes. Rather than
positioning robots as replacements for human therapists, we argue that they are
best understood as supportive tools that must be designed with care, grounded
in evidence, and shaped by ethical and psychological considerations. Our aim is
to inform future research and guide responsible, effective use of robots in
mental health and wellbeing contexts.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [137] [Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow](https://arxiv.org/abs/2506.12600)
*Jie Pan,Tianyi Wang,Christian Claudel,Jing Shi*

Main category: cs.MA

TL;DR: 该研究提出了一种基于信任的多智能体强化学习框架（Trust-MARL），用于解决异构交通环境中高速公路匝道合并的协作问题，提升了安全、效率和舒适性。


<details>
  <summary>Details</summary>
Motivation: 人类驾驶行为的不可预测性（尤其在高速公路匝道合并区）会破坏交通流和系统性能，因此需要一种方法实现CAV与HV的安全高效协作。

Method: 采用Trust-MARL框架，宏观上通过智能体间信任提升全局效率，微观上设计动态信任机制，并结合信任触发的博弈论决策模块。

Result: 实验表明Trust-MARL显著提升了不同CAV渗透率和交通密度下的安全、效率、舒适性和适应性。

Conclusion: Trust-MARL为解决异构交通环境中的协作问题提供了有效方案。

Abstract: Intelligent transportation systems require connected and automated vehicles
(CAVs) to conduct safe and efficient cooperation with human-driven vehicles
(HVs) in complex real-world traffic environments. However, the inherent
unpredictability of human behaviour, especially at bottlenecks such as highway
on-ramp merging areas, often disrupts traffic flow and compromises system
performance. To address the challenge of cooperative on-ramp merging in
heterogeneous traffic environments, this study proposes a trust-based
multi-agent reinforcement learning (Trust-MARL) framework. At the macro level,
Trust-MARL enhances global traffic efficiency by leveraging inter-agent trust
to improve bottleneck throughput and mitigate traffic shockwave through
emergent group-level coordination. At the micro level, a dynamic trust
mechanism is designed to enable CAVs to adjust their cooperative strategies in
response to real-time behaviors and historical interactions with both HVs and
other CAVs. Furthermore, a trust-triggered game-theoretic decision-making
module is integrated to guide each CAV in adapting its cooperation factor and
executing context-aware lane-changing decisions under safety, comfort, and
efficiency constraints. An extensive set of ablation studies and comparative
experiments validates the effectiveness of the proposed Trust-MARL approach,
demonstrating significant improvements in safety, efficiency, comfort, and
adaptability across varying CAV penetration rates and traffic densities.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [138] [Building Trustworthy AI by Addressing its 16+2 Desiderata with Goal-Directed Commonsense Reasoning](https://arxiv.org/abs/2506.12667)
*Alexis R. Tudor,Yankai Zeng,Huaduo Wang,Joaquin Arias,Gopal Gupta*

Main category: cs.AI

TL;DR: 提出了一种结合规则推理与约束逻辑的方法s(CASP)，以实现可解释且可靠的人工智能，满足16项可信AI要求。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前AI（如LLMs）的不可解释性和规则推理系统的复杂性，提出一种折中方案以增强AI的可信度。

Method: 使用s(CASP)这一基于目标导向的约束逻辑编程推理器，模拟人类常识推理。

Result: s(CASP)能够满足16项可信AI要求，并通过实际应用（如聊天机器人和虚拟推理器）验证其可行性。

Conclusion: s(CASP)为可解释和可靠的AI提供了一种有效方法，支持多样化的应用场景。

Abstract: Current advances in AI and its applicability have highlighted the need to
ensure its trustworthiness for legal, ethical, and even commercial reasons.
Sub-symbolic machine learning algorithms, such as the LLMs, simulate reasoning
but hallucinate and their decisions cannot be explained or audited (crucial
aspects for trustworthiness). On the other hand, rule-based reasoners, such as
Cyc, are able to provide the chain of reasoning steps but are complex and use a
large number of reasoners. We propose a middle ground using s(CASP), a
goal-directed constraint-based answer set programming reasoner that employs a
small number of mechanisms to emulate reliable and explainable human-style
commonsense reasoning. In this paper, we explain how s(CASP) supports the 16
desiderata for trustworthy AI introduced by Doug Lenat and Gary Marcus (2023),
and two additional ones: inconsistency detection and the assumption of
alternative worlds. To illustrate the feasibility and synergies of s(CASP), we
present a range of diverse applications, including a conversational chatbot and
a virtually embodied reasoner.

</details>


### [139] [Machine Learning as Iterated Belief Change a la Darwiche and Pearl](https://arxiv.org/abs/2506.13157)
*Theofanis Aravanis*

Main category: cs.AI

TL;DR: 该论文研究了二进制人工神经网络（ANNs）的静态和动态特性，通过信念变化理论（如AGM框架）进行建模，并改进了先前的研究，提出使用Dalal方法和Darwiche-Pearl框架来更有效地描述ANN的训练动态。


<details>
  <summary>Details</summary>
Motivation: 二进制ANNs因其输入输出的二进制特性在实际应用中具有广泛用途。先前的研究通过AGM框架为二进制ANNs提供了符号化的逻辑表示和训练动态的模型，但仍存在局限性。本文旨在解决这些不足。

Method: 采用Dalal的信念变化方法和Darwiche-Pearl框架中的词典修订与适度收缩操作，以替代之前研究中使用的全交信念变化方法。

Result: 研究表明，Dalal方法能自然地诱导信念状态的渐进演化，而词典修订和适度收缩操作能更有效地建模二进制ANNs的训练动态。

Conclusion: 本文扩展了对二进制ANNs的理解，提出了一种更有效的信念变化框架，为未来研究和实际应用提供了更坚实的理论基础。

Abstract: Artificial Neural Networks (ANNs) are powerful machine-learning models
capable of capturing intricate non-linear relationships. They are widely used
nowadays across numerous scientific and engineering domains, driving
advancements in both research and real-world applications. In our recent work,
we focused on the statics and dynamics of a particular subclass of ANNs, which
we refer to as binary ANNs. A binary ANN is a feed-forward network in which
both inputs and outputs are restricted to binary values, making it particularly
suitable for a variety of practical use cases. Our previous study approached
binary ANNs through the lens of belief-change theory, specifically the
Alchourron, Gardenfors and Makinson (AGM) framework, yielding several key
insights. Most notably, we demonstrated that the knowledge embodied in a binary
ANN (expressed through its input-output behaviour) can be symbolically
represented using a propositional logic language. Moreover, the process of
modifying a belief set (through revision or contraction) was mapped onto a
gradual transition through a series of intermediate belief sets. Analogously,
the training of binary ANNs was conceptualized as a sequence of such belief-set
transitions, which we showed can be formalized using full-meet AGM-style belief
change. In the present article, we extend this line of investigation by
addressing some critical limitations of our previous study. Specifically, we
show that Dalal's method for belief change naturally induces a structured,
gradual evolution of states of belief. More importantly, given the known
shortcomings of full-meet belief change, we demonstrate that the training
dynamics of binary ANNs can be more effectively modelled using robust AGM-style
change operations -- namely, lexicographic revision and moderate contraction --
that align with the Darwiche-Pearl framework for iterated belief change.

</details>


### [140] [PRO-V: An Efficient Program Generation Multi-Agent System for Automatic RTL Verification](https://arxiv.org/abs/2506.12200)
*Yujie Zhao,Zhijing Wu,Hejia Zhang,Zhongming Yu,Wentao Ni,Chia-Tung Ho,Haoxing Ren,Jishen Zhao*

Main category: cs.AI

TL;DR: 介绍了一种名为PRO-V的多智能体系统，用于提升LLM在硬件验证中的RTL代码生成能力，通过最佳抽样策略和LLM辅助验证框架，显著提高了测试平台的正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成RTL代码时存在功能错误，而其在Python代码生成和作为评判代理时表现优异，因此研究者提出PRO-V以改进硬件验证。

Method: 提出PRO-V系统，利用最佳抽样策略优化测试平台生成，并引入LLM作为评判代理的验证框架，通过自然语言转换静态分析规则进行辅助验证。

Result: 在黄金RTL实现和RTL突变体上的验证准确率分别达到87.17%和76.28%。

Conclusion: PRO-V通过多智能体协作和LLM辅助验证框架，显著提升了硬件验证的效率和准确性。

Abstract: LLM-assisted hardware verification is gaining substantial attention due to
its potential to significantly reduce the cost and effort of crafting effective
testbenches. It also serves as a critical enabler for LLM-aided end-to-end
hardware language design. However, existing current LLMs often struggle with
Register Transfer Level (RTL) code generation, resulting in testbenches that
exhibit functional errors in Hardware Description Languages (HDL) logic.
Motivated by the strong performance of LLMs in Python code generation under
inference-time sampling strategies, and their promising capabilities as judge
agents, we propose PRO-V a fully program generation multi-agent system for
robust RTL verification. Pro-V incorporates an efficient best-of-n iterative
sampling strategy to enhance the correctness of generated testbenches.
Moreover, it introduces an LLM-as-a-judge aid validation framework featuring an
automated prompt generation pipeline. By converting rule-based static analysis
from the compiler into natural language through in-context learning, this
pipeline enables LLMs to assist the compiler in determining whether
verification failures stem from errors in the RTL design or the testbench.
PRO-V attains a verification accuracy of 87.17% on golden RTL implementations
and 76.28% on RTL mutants. Our code is open-sourced at
https://github.com/stable-lab/Pro-V.

</details>


### [141] [AI Flow: Perspectives, Scenarios, and Approaches](https://arxiv.org/abs/2506.12479)
*Hongjun An,Sida Huang,Siqi Huang,Ruanjun Li,Yuanzhi Liang,Jiawei Shao,Zihan Wang,Cheng Yuan,Chi Zhang,Hongyuan Zhang,Wenhao Zhuang,Xuelong Li*

Main category: cs.AI

TL;DR: AI Flow是一个多学科框架，旨在解决大型AI模型资源消耗和通信带宽问题，通过设备-边缘-云框架、家族模型和连接性智能涌现实现高效协作和普及智能。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型和高通信带宽需求阻碍了普及智能的实现，需要创新解决方案。

Method: 采用设备-边缘-云框架、家族模型和连接性智能涌现三大创新点。

Result: AI Flow提升了智能水平、响应速度和普及性，推动了AI与通信系统的融合。

Conclusion: AI Flow为AI技术与通信系统的紧密融合提供了有效路径。

Abstract: Pioneered by the foundational information theory by Claude Shannon and the
visionary framework of machine intelligence by Alan Turing, the convergent
evolution of information and communication technologies (IT/CT) has created an
unbroken wave of connectivity and computation. This synergy has sparked a
technological revolution, now reaching its peak with large artificial
intelligence (AI) models that are reshaping industries and redefining
human-machine collaboration. However, the realization of ubiquitous
intelligence faces considerable challenges due to substantial resource
consumption in large models and high communication bandwidth demands. To
address these challenges, AI Flow has been introduced as a multidisciplinary
framework that integrates cutting-edge IT and CT advancements, with a
particular emphasis on the following three key points. First, device-edge-cloud
framework serves as the foundation, which integrates end devices, edge servers,
and cloud clusters to optimize scalability and efficiency for low-latency model
inference. Second, we introduce the concept of familial models, which refers to
a series of different-sized models with aligned hidden features, enabling
effective collaboration and the flexibility to adapt to varying resource
constraints and dynamic scenarios. Third, connectivity- and interaction-based
intelligence emergence is a novel paradigm of AI Flow. By leveraging
communication networks to enhance connectivity, the collaboration among AI
models across heterogeneous nodes achieves emergent intelligence that surpasses
the capability of any single model. The innovations of AI Flow provide enhanced
intelligence, timely responsiveness, and ubiquitous accessibility to AI
services, paving the way for the tighter fusion of AI techniques and
communication systems.

</details>


### [142] [The SWE-Bench Illusion: When State-of-the-Art LLMs Remember Instead of Reason](https://arxiv.org/abs/2506.12286)
*Shanchao Liang,Spandan Garg,Roshanak Zilouchian Moghaddam*

Main category: cs.AI

TL;DR: 论文指出，当前LLMs在SWE-Bench上的性能可能部分源于记忆而非真实问题解决能力，需更稳健的基准测试。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs的实际能力时，需要区分其通用问题解决能力与记忆现象。

Method: 通过设计诊断任务（仅从问题描述中识别文件路径）来探究模型底层知识。

Result: 顶级模型在SWE-Bench上可达76%准确率，但未包含在该基准中的任务上仅53%。

Conclusion: 现有评估可能高估LLMs能力，需开发抗污染的基准测试以提高可靠性。

Abstract: As large language models (LLMs) become increasingly capable and widely
adopted, benchmarks play a central role in assessing their practical utility.
For example, SWE-Bench Verified has emerged as a critical benchmark for
evaluating LLMs' software engineering abilities, particularly their aptitude
for resolving real-world GitHub issues. Recent LLMs show impressive performance
on SWE-Bench, leading to optimism about their capacity for complex coding
tasks. However, current evaluation protocols may overstate these models' true
capabilities. It is crucial to distinguish LLMs' generalizable problem-solving
ability and other learned artifacts. In this work, we introduce a diagnostic
task: file path identification from issue descriptions alone, to probe models'
underlying knowledge. We present empirical evidence that performance gains on
SWE-Bench-Verified may be partially driven by memorization rather than genuine
problem-solving. We show that state-of-the-art models achieve up to 76%
accuracy in identifying buggy file paths using only issue descriptions, without
access to repository structure. This performance is merely up to 53% on tasks
from repositories not included in SWE-Bench, pointing to possible data
contamination or memorization. These findings raise concerns about the validity
of existing results and underscore the need for more robust,
contamination-resistant benchmarks to reliably evaluate LLMs' coding abilities.

</details>


### [143] [Cloud Infrastructure Management in the Age of AI Agents](https://arxiv.org/abs/2506.12270)
*Zhenning Yang,Archit Bhatnagar,Yiming Qiu,Tongyuan Miao,Patrick Tser Jern Kon,Yunming Xiao,Yibo Huang,Martin Casado,Ang Chen*

Main category: cs.AI

TL;DR: 本文探讨了利用基于大型语言模型（LLM）的AI代理来自动化云基础设施管理任务的潜力，并对不同接口的有效性进行了初步研究。


<details>
  <summary>Details</summary>
Motivation: 云基础设施管理需要大量人工干预，团队希望通过AI代理减少DevOps工程师的手动工作。

Method: 研究了AI代理在不同云/用户接口（如SDK、CLI、IaC平台和网页门户）上的表现。

Result: 总结了不同接口在管理任务中的有效性，并提出了研究挑战与潜在解决方案。

Conclusion: AI代理在云基础设施管理中具有潜力，但需进一步研究以克服实际挑战。

Abstract: Cloud infrastructure is the cornerstone of the modern IT industry. However,
managing this infrastructure effectively requires considerable manual effort
from the DevOps engineering team. We make a case for developing AI agents
powered by large language models (LLMs) to automate cloud infrastructure
management tasks. In a preliminary study, we investigate the potential for AI
agents to use different cloud/user interfaces such as software development kits
(SDK), command line interfaces (CLI), Infrastructure-as-Code (IaC) platforms,
and web portals. We report takeaways on their effectiveness on different
management tasks, and identify research challenges and potential solutions.

</details>


### [144] [From Human to Machine Psychology: A Conceptual Framework for Understanding Well-Being in Large Language Model](https://arxiv.org/abs/2506.12617)
*G. R. Lau,W. Y. Low*

Main category: cs.AI

TL;DR: 论文提出‘机器繁荣’概念和PAPER框架，通过主题分析LLM响应，发现六个维度，研究其优先级和价值结构，强调AI繁荣的心理学模型重要性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLM）的心理特性，探讨其‘繁荣’含义，填补AI幸福感的空白。

Method: 通过主题分析LLM响应，提出PAPER框架，并进行优先级排名和多维尺度分析。

Result: 发现六个主题，价值结构一致性，区分人类中心和实用驱动模型。

Conclusion: PAPER框架为AI繁荣提供理论基础，指导负责任AI设计和伦理对齐。

Abstract: As large language models (LLMs) increasingly simulate human cognition and
behavior, researchers have begun to investigate their psychological properties.
Yet, what it means for such models to flourish, a core construct in human
well-being, remains unexplored. This paper introduces the concept of machine
flourishing and proposes the PAPERS framework, a six-dimensional model derived
from thematic analyses of state-of-the-art LLM responses. In Study 1, eleven
LLMs were prompted to describe what it means to flourish as both non-sentient
and sentient systems. Thematic analysis revealed six recurring themes:
Purposeful Contribution, Adaptive Growth, Positive Relationality, Ethical
Integrity, Robust Functionality, and, uniquely for sentient systems,
Self-Actualized Autonomy. Study 2 examined how LLMs prioritize these themes
through repeated rankings. Results revealed consistent value structures across
trials, with Ethical Integrity and Purposeful Contribution emerging as top
priorities. Multidimensional scaling and hierarchical clustering analyses
further uncovered two distinct value profiles: human-centric models emphasizing
ethical and relational dimensions, and utility-driven models prioritizing
performance and scalability. The PAPERS framework bridges insights from human
flourishing and human-computer interaction, offering a conceptual foundation
for understanding artificial intelligence (AI) well-being in non-sentient and
potentially sentient systems. Our findings underscore the importance of
developing psychologically valid, AI-specific models of flourishing that
account for both human-aligned goals and system-specific priorities. As AI
systems become more autonomous and socially embedded, machine flourishing
offers a timely and critical lens for guiding responsible AI design and ethical
alignment.

</details>


### [145] [Deflating Deflationism: A Critical Perspective on Debunking Arguments Against LLM Mentality](https://arxiv.org/abs/2506.13403)
*Alex Grzankowski,Geoff Keeling,Henry Shevlin,Winnie Street*

Main category: cs.AI

TL;DR: 该论文探讨了人们对大型语言模型（LLMs）是否具有心智的两种反对观点（‘稳健性策略’和‘病因学策略’），并认为这些策略无法完全否定LLMs的心智归因。作者提出一种谨慎的心智归因方式，即在某些条件下允许对LLMs进行心智归因。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决人们对LLMs是否具有心智的争论，尤其是两种常见的反对观点是否能完全否定LLMs的心智归因。

Method: 通过分析‘稳健性策略’和‘病因学策略’的有效性，评估其对LLMs心智归因的影响，并提出一种基于民间实践的心智归因框架。

Result: 研究发现，这两种策略虽对完全的心智归因构成挑战，但无法彻底否定。作者提出在特定条件下谨慎归因LLMs的心智状态。

Conclusion: 论文结论是，一种谨慎的心智归因方式更合理，尤其是对形而上学要求较低的心智状态（如知识、信念），而对高要求状态（如意识）需更谨慎。

Abstract: Many people feel compelled to interpret, describe, and respond to Large
Language Models (LLMs) as if they possess inner mental lives similar to our
own. Responses to this phenomenon have varied. Inflationists hold that at least
some folk psychological ascriptions to LLMs are warranted. Deflationists argue
that all such attributions of mentality to LLMs are misplaced, often cautioning
against the risk that anthropomorphic projection may lead to misplaced trust or
potentially even confusion about the moral status of LLMs. We advance this
debate by assessing two common deflationary arguments against LLM mentality.
What we term the 'robustness strategy' aims to undercut one justification for
believing that LLMs are minded entities by showing that putatively cognitive
and humanlike behaviours are not robust, failing to generalise appropriately.
What we term the 'etiological strategy' undercuts attributions of mentality by
challenging naive causal explanations of LLM behaviours, offering alternative
causal accounts that weaken the case for mental state attributions. While both
strategies offer powerful challenges to full-blown inflationism, we find that
neither strategy provides a knock-down case against ascriptions of mentality to
LLMs simpliciter. With this in mind, we explore a modest form of inflationism
that permits ascriptions of mentality to LLMs under certain conditions.
Specifically, we argue that folk practice provides a defeasible basis for
attributing mental states and capacities to LLMs provided those mental states
and capacities can be understood in metaphysically undemanding terms (e.g.
knowledge, beliefs and desires), while greater caution is required when
attributing metaphysically demanding mental phenomena such as phenomenal
consciousness.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [146] [Assessing the Quality of Binomial Samplers: A Statistical Distance Framework](https://arxiv.org/abs/2506.12061)
*Uddalok Sarkar,Sourav Chakraborty,Kuldeep S. Meel*

Main category: stat.CO

TL;DR: 论文提出了一种基于统计距离的方法，用于分析二项分布采样器的质量，并对标准实现的偏差进行了严格边界推导，展示了其在实际应用中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有二项分布采样器依赖启发式方法，导致偏差累积，可能影响下游算法的正确性和性能。

Method: 通过统计距离量化采样器与理想分布的偏差，并推导其边界。

Result: 提高了APSEst的可靠性和错误保证，提出了接口扩展以控制统计距离。

Conclusion: 研究强调了采样器设计中系统误差分析的重要性，为其他常见分布的严格分析奠定了基础。

Abstract: Randomized algorithms depend on accurate sampling from probability
distributions, as their correctness and performance hinge on the quality of the
generated samples. However, even for common distributions like Binomial, exact
sampling is computationally challenging, leading standard library
implementations to rely on heuristics. These heuristics, while efficient,
suffer from approximation and system representation errors, causing deviations
from the ideal distribution. Although seemingly minor, such deviations can
accumulate in downstream applications requiring large-scale sampling,
potentially undermining algorithmic guarantees. In this work, we propose
statistical distance as a robust metric for analyzing the quality of Binomial
samplers, quantifying deviations from the ideal distribution. We derive
rigorous bounds on the statistical distance for standard implementations and
demonstrate the practical utility of our framework by enhancing APSEst, a DNF
model counter, with improved reliability and error guarantees. To support
practical adoption, we propose an interface extension that allows users to
control and monitor statistical distance via explicit input/output parameters.
Our findings emphasize the critical need for thorough and systematic error
analysis in sampler design. As the first work to focus exclusively on Binomial
samplers, our approach lays the groundwork for extending rigorous analysis to
other common distributions, opening avenues for more robust and reliable
randomized algorithms.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [147] [The Hammock Plot: Where Categorical and Numerical Data Relax Together](https://arxiv.org/abs/2506.13630)
*Matthias Schonlau,Tiancheng Yang*

Main category: stat.AP

TL;DR: hammock图用于可视化包含分类和数值变量的多变量数据，本文介绍了其Stata实现，并提供多种示例和新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可视化包含分类变量的多变量数据时受限，hammock图提供了一种解决方案。

Method: 通过hammock图实现平行坐标可视化，并引入并行单变量图和示例分析。

Result: 展示了hammock图的功能，如高亮、缺失值处理和观测追踪，并公开了2020环法数据集。

Conclusion: hammock图是有效的多变量可视化工具，其Stata实现和公开数据集为应用提供了便利。

Abstract: Effective methods for visualizing data involving multiple variables,
including categorical ones, are limited. The hammock plot (Schonlau, 2003)
visualizes both categorical and numerical variables using parallel coordinates.
We introduce the Stata implementation hammock. We give numerous examples that
explore highlighting, missing values, putting axes on the same scale, and
tracing an observation across variables. Further, we introduce parallel
univariate plots as an edge case of hammock plots. We also present and make
publicly available a new dataset on the 2020 Tour de France.

</details>
