<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 18]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 27]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Software Engineers Engage with AI: A Pragmatic Process Model and Decision Framework Grounded in Industry Observations](https://arxiv.org/abs/2507.17930)
*Vahid Garousi,Zafar Jafarov*

Main category: cs.SE

TL;DR: 论文探讨AI如何辅助软件工程，提出过程模型和决策框架以优化开发者的使用体验。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具在软件开发中的实际应用，尤其是开发者如何信任和优化AI输出。

Method: 通过实践观察和行业案例，构建过程模型和2D决策框架。

Result: 提出结构化方法以提升AI工具的使用效率和效果。

Conclusion: 研究成果为AI与人类协作提供了实用指导，促进更有效的软件开发。

Abstract: Artificial Intelligence (AI) has the potential to transform Software
Engineering (SE) by enhancing productivity, efficiency, and decision support.
Tools like GitHub Copilot and ChatGPT have given rise to "vibe coding"-an
exploratory, prompt-driven development style. Yet, how software engineers
engage with these tools in daily tasks, especially in deciding whether to
trust, refine, or reject AI-generated outputs, remains underexplored. This
paper presents two complementary contributions. First, a pragmatic process
model capturing real-world AI-assisted SE activities, including prompt design,
inspection, fallback, and refinement. Second, a 2D decision framework that
could help developers reason about trade-offs between effort saved and output
quality. Grounded in practitioner reports and direct observations in three
industry settings across Turkiye and Azerbaijan, our work illustrates how
engineers navigate AI use with human oversight. These models offer structured,
lightweight guidance to support more deliberate and effective use of AI tools
in SE, contributing to ongoing discussions on practical human-AI collaboration.

</details>


### [2] [Use as Directed? A Comparison of Software Tools Intended to Check Rigor and Transparency of Published Work](https://arxiv.org/abs/2507.17991)
*Peter Eckmann,Adrian Barnett,Alexandra Bannach-Brown,Elisa Pilar Bascunan Atria,Guillaume Cabanac,Louise Delwen Owen Franzen,Małgorzata Anna Gazda,Kaitlyn Hair,James Howison,Halil Kilicoglu,Cyril Labbe,Sarah McCann,Vladislav Nachev,Martijn Roelandse,Maia Salholz-Hillel,Robert Schulz,Gerben ter Riet,Colby Vorland,Anita Bandrowski,Tracey Weissgerber*

Main category: cs.SE

TL;DR: 论文分析了科学报告中的可重复性危机原因，比较了11种自动化工具在9种严谨标准上的表现，并提出了工具开发的建议。


<details>
  <summary>Details</summary>
Motivation: 解决科学报告缺乏标准化和透明度导致的重复性危机，通过自动化工具提升严谨性和透明度。

Method: 对11种自动化工具在9种严谨标准上进行广泛比较，并分析其表现。

Result: 某些标准下单一工具表现突出，而其他标准下工具组合表现更优；提出了工具开发的关键改进方向。

Conclusion: 总结了自动化工具在提升科学报告透明度方面的作用，并为开发者提供了具体建议。

Abstract: The causes of the reproducibility crisis include lack of standardization and
transparency in scientific reporting. Checklists such as ARRIVE and CONSORT
seek to improve transparency, but they are not always followed by authors and
peer review often fails to identify missing items. To address these issues,
there are several automated tools that have been designed to check different
rigor criteria. We have conducted a broad comparison of 11 automated tools
across 9 different rigor criteria from the ScreenIT group. We found some
criteria, including detecting open data, where the combination of tools showed
a clear winner, a tool which performed much better than other tools. In other
cases, including detection of inclusion and exclusion criteria, the combination
of tools exceeded the performance of any one tool. We also identified key areas
where tool developers should focus their effort to make their tool maximally
useful. We conclude with a set of insights and recommendations for stakeholders
in the development of rigor and transparency detection tools. The code and data
for the study is available at https://github.com/PeterEckmann1/tool-comparison.

</details>


### [3] [An Empirical Study of GenAI Adoption in Open-Source Game Development: Tools, Tasks, and Developer Challenges](https://arxiv.org/abs/2507.18029)
*Xiang Echo Chen,Wenhan Zhu,Guoshuai Albert Shi,Michael W. Godfrey*

Main category: cs.SE

TL;DR: 研究探讨了生成式AI在开源游戏开发中的应用，比较了其与传统AI和非AI话题的差异，通过分析GitHub问题讨论揭示了使用模式和开发者的关注点。


<details>
  <summary>Details</summary>
Motivation: 现有研究对生成式AI在开源社区中的实际应用缺乏实证理解，特别是在游戏开发领域。

Method: 构建开源游戏仓库数据集，通过卡片分类和主题分析对GitHub问题进行标注，比较生成式AI、传统AI和非AI话题。

Result: 揭示了生成式AI在游戏开发中的独特使用模式、开发者关注点和整合实践。

Conclusion: 生成式AI为游戏开发提供了新工具和方法，但同时也带来了独特的挑战和关注点。

Abstract: The growing capabilities of generative AI (GenAI) have begun to reshape how
games are designed and developed, offering new tools for content creation,
gameplay simulation, and design ideation. While prior research has explored
traditional uses of AI in games, such as controlling agents or generating
procedural content. There is limited empirical understanding of how GenAI is
adopted by developers in real-world contexts, especially within the open-source
community. This study aims to explore how GenAI technologies are discussed,
adopted, and integrated into open-source game development by analyzing issue
discussions on GitHub. We investigate the tools, tasks, and challenges
associated with GenAI by comparing GenAI-related issues to those involving
traditional AI (TradAI) and NonAI topics. Our goal is to uncover how GenAI
differs from other approaches in terms of usage patterns, developer concerns,
and integration practices. To address this objective, we construct a dataset of
open-source game repositories that discuss AI-related topics. We apply open
card sorting and thematic analysis to a stratified sample of GitHub issues,
labelling each by type and content. These annotations enable comparative
analysis across GenAI, TradAI, and NonAI groups, and provide insight into how
GenAI is shaping the workflows and pain points of open-source game developers.

</details>


### [4] [Your ATs to Ts: MITRE ATT&CK Attack Technique to P-SSCRM Task Mapping](https://arxiv.org/abs/2507.18037)
*Sivana Hamer,Jacob Bowen,Md Nazmul Haque,Chris Madden,Laurie Williams*

Main category: cs.SE

TL;DR: 文档提出了MITRE ATT&CK攻击技术与P-SSCRM任务的映射关系，帮助组织识别如何通过任务缓解软件供应链攻击。


<details>
  <summary>Details</summary>
Motivation: 为软件组织提供一种方法，通过任务映射来应对软件供应链攻击。

Method: 通过四种独立策略确定P-SSCRM任务与MITRE ATT&CK攻击技术的映射关系。

Result: P-SSCRM任务被映射到10个框架中的多个任务，同时实现了MITRE ATT&CK与其他政府及行业框架的映射。

Conclusion: 该映射关系为软件供应链攻击的风险管理提供了实用工具。

Abstract: The MITRE Adversarial Tactics, Techniques and Common Knowledge (MITRE ATT&CK)
Attack Technique to Proactive Software Supply Chain Risk Management Framework
(P-SSCRM) Task mapping described in this document helps software organizations
to determine how different tasks mitigate the attack techniques of software
supply chain attacks. The mapping was created through four independent
strategies to find agreed-upon mappings. Because each P-SSCRM task is mapped to
one or more tasks from the 10 frameworks, the mapping we provide is also a
mapping between MITRE ATT&CK and other prominent government and industry
frameworks.

</details>


### [5] [Factors Impacting Faculty Adoption of Project-Based Learning in Computing Education: a Survey](https://arxiv.org/abs/2507.18039)
*Ahmad D. Suleiman,Yiming Tang,Daqing Hou*

Main category: cs.SE

TL;DR: 研究探讨了影响计算机教育工作者在教学中采用项目式学习（PjBL）的因素，揭示了其价值与采用的障碍，并提出支持策略。


<details>
  <summary>Details</summary>
Motivation: 项目式学习（PjBL）能提升学生动机、参与度和技能，但教师采用率不高，研究旨在解决这一矛盾。

Method: 采用混合方法，通过调查80名计算机教师的定量和定性数据，分析障碍和促进因素。

Result: PjBL广受重视，但采用受限；教师需要支持（如培训、资源）才能更广泛实施。

Conclusion: 需系统性支持（如协作、资源）以促进PjBL的推广和实施。

Abstract: This research full paper investigates the factors influencing computing
educators' adoption of project-based learning (PjBL) in software engineering
and computing curricula. Recognized as a student-centered pedagogical approach,
PjBL has the potential to enhance student motivation, engagement, critical
thinking, collaboration, and problem-solving skills. Despite these benefits,
faculty adoption remains inconsistent due to challenges such as insufficient
institutional support, time constraints, limited training opportunities,
designing or sourcing projects, and aligning them with course objectives. This
research explores these barriers and investigates the strategies and resources
that facilitate a successful adoption. Using a mixed-methods approach, data
from 80 computing faculty were collected through an online survey comprising
closed-ended questions to quantify barriers, enablers, and resource needs,
along with an open-ended question to gather qualitative insights. Quantitative
data were analyzed using statistical methods, while qualitative responses
underwent thematic analysis. Results reveal that while PjBL is widely valued,
its adoption is often selective and impacted by challenges in planning and
managing the learning process, designing suitable projects, and a lack of
institutional support, such as time, funding, and teaching assistants. Faculty
are more likely to adopt or sustain PjBL when they have access to peer
collaboration, professional development, and institutional incentives. In
addition, sourcing projects from research, industry partnerships, and borrowing
from peers emerged as key facilitators for new projects. These findings
underscore the need for systemic support structures to empower faculty to
experiment with and scale PjBL practices.

</details>


### [6] [An Empirical Study of Complexity, Heterogeneity, and Compliance of GitHub Actions Workflows](https://arxiv.org/abs/2507.18062)
*Edward Abrokwah,Taher A. Ghaleb*

Main category: cs.SE

TL;DR: GitHub Actions（GHA）是现代CI服务中的主流工具，但其复杂性与实际使用是否符合最佳实践仍需探究。本研究通过分析开源项目中GHA工作流的结构、复杂性和合规性，旨在发现问题和改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管GHA提供了官方文档和社区最佳实践，但对开源项目中GHA工作流如何符合这些实践的实证研究有限。许多工作流可能过于复杂，未达到CI的简单性目标。

Method: 研究使用来自Java、Python和C++仓库的大型GHA工作流数据集，分析其复杂性、重复模式和异构性，并评估其与GHA最佳实践的符合程度。

Result: 预计发现工作流设计中的强项和改进空间，以及不同编程语言间的设计差异。

Conclusion: 研究结果将为CI服务提供改进文档和指南的参考，强调清晰示例和指导的必要性。

Abstract: Continuous Integration (CI) has evolved from a tooling strategy to a
fundamental mindset in modern CI engineering. It enables teams to develop,
test, and deliver software rapidly and collaboratively. Among CI services,
GitHub Actions (GHA) has emerged as a dominant service due to its deep
integration with GitHub and a vast ecosystem of reusable workflow actions.
Although GHA provides official documentation and community-supported best
practices, there appears to be limited empirical understanding of how
open-source real-world CI workflows align with such practices. Many workflows
might be unnecessarily complex and not aligned with the simplicity goals of CI
practices. This study will investigate the structure, complexity,
heterogeneity, and compliance of GHA workflows in open-source software
repositories. Using a large dataset of GHA workflows from Java, Python, and C++
repositories, our goal is to (a) identify workflow complexities, (b) analyze
recurring and heterogeneous structuring patterns, (c) assess compliance with
GHA best practices, and (d) uncover differences in CI pipeline design across
programming languages. Our findings are expected to reveal both areas of strong
adherence to best practices and areas for improvement where needed. These
insights will also have implications for CI services, as they will highlight
the need for clearer guidelines and comprehensive examples in CI documentation.

</details>


### [7] [Identifier Name Similarities: An Exploratory Study](https://arxiv.org/abs/2507.18081)
*Carol Wong,Mai Abe,Silvia De Benedictis,Marissa Halim,Anthony Peruma*

Main category: cs.SE

TL;DR: 论文初步研究了标识符名称相似性对代码理解的影响，并提出了一种分类法。


<details>
  <summary>Details</summary>
Motivation: 标识符名称对代码理解至关重要，但名称相似性可能导致误解，增加认知负担。

Method: 通过开发一种分类法，研究标识符名称相似性的不同形式。

Result: 初步分类法为分析名称相似性对代码可理解性和协作的影响提供了基础。

Conclusion: 该分类法有助于未来研究和改进标识符名称的设计。

Abstract: Identifier names, which comprise a significant portion of the codebase, are
the cornerstone of effective program comprehension. However, research has shown
that poorly chosen names can significantly increase cognitive load and hinder
collaboration. Even names that appear readable in isolation may lead to
misunderstandings in contexts when they closely resemble other names in either
structure or functionality. In this exploratory study, we present our
preliminary findings on the occurrence of identifier name similarity in
software projects through the development of a taxonomy that categorizes
different forms of identifier name similarity. We envision our initial taxonomy
providing researchers with a platform to analyze and evaluate the impact of
identifier name similarity on code comprehension, maintainability, and
collaboration among developers, while also allowing for further refinement and
expansion of the taxonomy.

</details>


### [8] [Understanding the Supply Chain and Risks of Large Language Model Applications](https://arxiv.org/abs/2507.18105)
*Yujie Ma,Lili Quan,Xiaofei Xie,Qiang Hu,Jiongchi Yu,Yao Zhang,Sen Chen*

Main category: cs.SE

TL;DR: 该论文提出了第一个全面分析大语言模型（LLM）供应链安全的数据集，揭示了LLM应用中深层依赖性和供应链中的重大漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的系统广泛应用，其复杂供应链中的风险逐渐显现，但目前的风险评估缺乏系统性基准。

Method: 收集3,859个真实LLM应用，分析其依赖关系（模型、数据集、库），并从公开漏洞数据库中提取1,555个风险相关问题进行安全评估。

Result: 研究发现LLM应用存在深层依赖性和供应链中的重大漏洞，呼吁进行全面安全分析。

Conclusion: 论文提出了实用建议，以帮助开发更安全、可信的LLM系统。

Abstract: The rise of Large Language Models (LLMs) has led to the widespread deployment
of LLM-based systems across diverse domains. As these systems proliferate,
understanding the risks associated with their complex supply chains is
increasingly important. LLM-based systems are not standalone as they rely on
interconnected supply chains involving pretrained models, third-party
libraries, datasets, and infrastructure. Yet, most risk assessments narrowly
focus on model or data level, overlooking broader supply chain vulnerabilities.
While recent studies have begun to address LLM supply chain risks, there
remains a lack of benchmarks for systematic research.
  To address this gap, we introduce the first comprehensive dataset for
analyzing and benchmarking LLM supply chain security. We collect 3,859
real-world LLM applications and perform interdependency analysis, identifying
109,211 models, 2,474 datasets, and 9,862 libraries. We extract model
fine-tuning paths, dataset reuse, and library reliance, mapping the ecosystem's
structure. To evaluate security, we gather 1,555 risk-related issues-50 for
applications, 325 for models, 18 for datasets, and 1,229 for libraries from
public vulnerability databases.
  Using this dataset, we empirically analyze component dependencies and risks.
Our findings reveal deeply nested dependencies in LLM applications and
significant vulnerabilities across the supply chain, underscoring the need for
comprehensive security analysis. We conclude with practical recommendations to
guide researchers and developers toward safer, more trustworthy LLM-enabled
systems.

</details>


### [9] [NoCode-bench: A Benchmark for Evaluating Natural Language-Driven Feature Addition](https://arxiv.org/abs/2507.18130)
*Le Deng,Zhonghao Jiang,Jialun Cao,Michael Pradel,Zhongxin Liu*

Main category: cs.SE

TL;DR: NoCode-bench是一个用于评估大型语言模型在自然语言驱动无代码开发中性能的基准测试，包含634个任务。实验表明，尽管模型消耗大量标记，任务成功率仅为15.79%，表明当前模型尚无法完全支持自然语言驱动的无代码开发。


<details>
  <summary>Details</summary>
Motivation: 自然语言驱动的无代码开发有望提高生产力和普及软件开发，但需要评估大型语言模型在这一范式中的表现。

Method: 创建NoCode-bench基准测试，包含634个任务，涵盖10个项目，并验证代码实现的正确性。

Result: 最佳模型的任务成功率为15.79%，凸显了跨文件编辑和代码库理解等挑战。

Conclusion: 大型语言模型目前无法胜任完全的自然语言驱动无代码开发，NoCode-bench为未来研究奠定基础。

Abstract: Natural language-driven no-code development allows users to specify software
functionality using natural language (NL) instead of editing source code,
promising increased productivity and democratized development. Large language
models (LLMs) show potential in enabling this paradigm. In this context,
software documentation acts as an NL specification for functionality. This work
introduces NoCode-bench, a benchmark designed to evaluate LLMs on real-world
NL-driven feature addition tasks, consisting of 634 tasks across 10 projects
and 114k code changes. Each task pairs documentation updates with corresponding
code implementations, validated by developer-written test cases. A subset of
114 high-quality, human-verified instances, NoCode-bench Verified, ensures
reliable evaluation. Our experiments reveal that, despite high token usage, the
best LLMs achieve a task success rate of only 15.79%, highlighting challenges
in cross-file editing, codebase understanding, and tool calling. These findings
indicate that LLMs are not yet ready for fully NL-driven no-code development.
NoCode-bench lays the foundation for future advances in this area.

</details>


### [10] [SMECS: A Software Metadata Extraction and Curation Software](https://arxiv.org/abs/2507.18159)
*Stephan Ferenz,Aida Jafarbigloo,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: SMECS是为简化研究软件的元数据创建而开发的工具，支持FAIR原则的实现。


<details>
  <summary>Details</summary>
Motivation: 高质量元数据创建对研究者和工程师资源消耗大，SMECS旨在解决这一问题。

Method: 通过从GitHub等在线仓库提取元数据，并提供交互界面进行进一步处理和导出为CodeMeta文件。

Result: 用户实验证实SMECS提供了满意的用户体验。

Conclusion: SMECS通过简化元数据创建，支持研究软件的FAIR化。

Abstract: Metadata play a crucial role in adopting the FAIR principles for research
software and enables findability and reusability. However, creating
high-quality metadata can be resource-intensive for researchers and research
software engineers. To address this challenge, we developed the Software
Metadata Extraction and Curation Software (SMECS) which integrates the
extraction of metadata from existing sources together with a user-friendly
interface for metadata curation. SMECS extracts metadata from online
repositories such as GitHub and presents it to researchers through an
interactive interface for further curation and export as a CodeMeta file. The
usability of SMECS was evaluated through usability experiments which confirmed
that SMECS provides a satisfactory user experience. SMECS supports the
FAIRification of research software by simplifying metadata creation.

</details>


### [11] [GenAI for Automotive Software Development: From Requirements to Wheels](https://arxiv.org/abs/2507.18223)
*Nenad Petrovic,Fengjunjie Pan,Vahid Zolfaghari,Krzysztof Lebioda,Andre Schamschurko,Alois Knoll*

Main category: cs.SE

TL;DR: 论文提出了一种基于GenAI的汽车软件开发自动化方法，重点针对自动驾驶和高级驾驶辅助系统（ADAS），通过使用LLM和RAG技术优化需求一致性检查、测试场景生成和代码实现。


<details>
  <summary>Details</summary>
Motivation: 旨在缩短ADAS相关功能的合规性和再工程周期，减少开发和测试时间。

Method: 采用基于LLM的需求摘要、测试场景生成、仿真代码（Python）及目标平台代码（C++）生成，并结合RAG技术从法规文档中增强测试场景生成。

Result: 实现了更高效的需求一致性检查和自动化开发流程。

Conclusion: 该方法显著提升了ADAS功能的开发效率，减少了时间和资源消耗。

Abstract: This paper introduces a GenAI-empowered approach to automated development of
automotive software, with emphasis on autonomous and Advanced Driver Assistance
Systems (ADAS) capabilities. The process starts with requirements as input,
while the main generated outputs are test scenario code for simulation
environment, together with implementation of desired ADAS capabilities
targeting hardware platform of the vehicle connected to testbench. Moreover, we
introduce additional steps for requirements consistency checking leveraging
Model-Driven Engineering (MDE). In the proposed workflow, Large Language Models
(LLMs) are used for model-based summarization of requirements (Ecore metamodel,
XMI model instance and OCL constraint creation), test scenario generation,
simulation code (Python) and target platform code generation (C++).
Additionally, Retrieval Augmented Generation (RAG) is adopted to enhance test
scenario generation from autonomous driving regulations-related documents. Our
approach aims shorter compliance and re-engineering cycles, as well as reduced
development and testing time when it comes to ADAS-related capabilities.

</details>


### [12] [An Empirical Study on Embodied Artificial Intelligence Robot (EAIR) Software Bugs](https://arxiv.org/abs/2507.18267)
*Zeqin Liao,Zibin Zheng,Peifan Reng,Henglong Liang,Zixu Gao,Zhixiang Chen,Wei Li,Yuhong Nan*

Main category: cs.SE

TL;DR: 该论文对885个EAIR系统bug进行了系统性研究，识别了18种根本原因、15种症状和13个受影响模块，帮助未来研究和修复EAIR系统bug。


<details>
  <summary>Details</summary>
Motivation: 缺乏对EAIR系统bug的全面理解，阻碍了相关技术和实践的开发。

Method: 对80个EAIR系统项目中的885个bug进行系统分析，分类症状、原因和模块分布。

Result: 发现8种EAIR特有症状和8种特有原因，并构建了原因与模块的映射关系。

Conclusion: 研究为EAIR系统bug的预测、检测和修复提供了新见解和方向。

Abstract: Embodied Artificial Intelligence Robots (EAIR) is an emerging and rapidly
evolving technological domain. Ensuring their program correctness is
fundamental to their successful deployment. However, a general and in-depth
understanding of EAIR system bugs remains lacking, which hinders the
development of practices and techniques to tackle EAIR system bugs.
  To bridge this gap, we conducted the first systematic study of 885 EAIR
system bugs collected from 80 EAIR system projects to investigate their
symptoms, underlying causes, and module distribution. Our analysis takes
considerable effort, which classifies these bugs into 18 underlying causes, 15
distinct symptoms, and identifies 13 affected modules. It reveals several new
interesting findings and implications which help shed light on future research
on tackling or repairing EAIR system bugs. First, among the 15 identified
symptoms, our findings highlight 8 symptoms specific to EAIR systems, which is
characterized by severe functional failures and potential physical hazards.
Second, within the 18 underlying causes, we define 8 EAIR-specific causes, the
majority of which stem from the intricate issues of AI- agent reasoning and
decision making. Finally, to facilitate precise and efficient bug prediction,
detection, and repair, we constructed a mapping between underlying causes and
the modules in which they most frequently occur, which enables researchers to
focus diagnostic efforts on the modules most susceptible to specific bug types.

</details>


### [13] [Scheduzz: Constraint-based Fuzz Driver Generation with Dual Scheduling](https://arxiv.org/abs/2507.18289)
*Yan Li,Wenzhang Yang,Yuekun Wang,Jian Gao,Shaohua Wang,Yinxing Xue,Lijun Zhang*

Main category: cs.SE

TL;DR: Scheduzz是一种基于LLM的自动库模糊测试技术，通过理解库的合理使用和优化API组合，显著提高了覆盖率和效率，并发现了33个未知bug。


<details>
  <summary>Details</summary>
Motivation: 现有库模糊测试技术因缺乏对库使用规范的遵守而生成不合理的模糊驱动程序，浪费计算资源并产生误报。需要一种能理解库合理使用并优化资源分配的技术。

Method: 利用LLM理解库的合理使用并提取API组合约束，采用双调度框架管理API组合和模糊驱动程序，将其建模为在线优化问题。

Result: 在33个真实库中测试，Scheduzz比基准方法显著减少计算开销，覆盖率达CKGFuzzer、Promptfuzz和OSS-Fuzz的1.62x、1.50x和1.89x，发现33个未知bug。

Conclusion: Scheduzz通过LLM和双调度框架有效提升库模糊测试的效率和覆盖率，并能发现未知漏洞。

Abstract: Fuzzing a library requires experts to understand the library usage well and
craft high-quality fuzz drivers, which is tricky and tedious. Therefore, many
techniques have been proposed to automatically generate fuzz drivers. However,
they fail to generate rational fuzz drivers due to the lack of adherence to
proper library usage conventions, such as ensuring a resource is closed after
being opened. To make things worse, existing library fuzzing techniques
unconditionally execute each driver, resulting in numerous irrational drivers
that waste computational resources while contributing little coverage and
generating false positive bug reports.
  To tackle these challenges, we propose a novel automatic library fuzzing
technique, Scheduzz, an LLM-based library fuzzing technique. It leverages LLMs
to understand rational usage of libraries and extract API combination
constraints. To optimize computational resource utilization, a dual scheduling
framework is implemented to efficiently manage API combinations and fuzz
drivers. The framework models driver generation and the corresponding fuzzing
campaign as an online optimization problem. Within the scheduling loop,
multiple API combinations are selected to generate fuzz drivers, while
simultaneously, various optimized fuzz drivers are scheduled for execution or
suspension.
  We implemented Scheduzz and evaluated it in 33 real-world libraries. Compared
to baseline approaches, Scheduzz significantly reduces computational overhead
and outperforms UTopia on 16 out of 21 libraries. It achieves 1.62x, 1.50x, and
1.89x higher overall coverage than the state-of-the-art techniques CKGFuzzer,
Promptfuzz, and the handcrafted project OSS-Fuzz, respectively. In addition,
Scheduzz discovered 33 previously unknown bugs in these well-tested libraries,
3 of which have been assigned CVEs.

</details>


### [14] [YATE: The Role of Test Repair in LLM-Based Unit Test Generation](https://arxiv.org/abs/2507.18316)
*Michael Konstantinou,Renzo Degiovanni,Jie M. Zhang,Mark Harman,Mike Papadakis*

Main category: cs.SE

TL;DR: 论文提出了一种名为YATE的简单技术，通过静态分析和重新提示修复语言模型生成的错误测试，显著提高了测试覆盖率和突变杀伤率。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型生成的测试中存在大量语法和语义错误，修复这些错误可以提升测试价值并为生成更多测试提供优质种子。

Method: 结合基于规则的静态分析和重新提示技术，修复语言模型生成的错误测试。

Result: YATE在6个开源项目上评价，平均覆盖了32.06%更多代码行，杀伤21.77%更多突变体，且效果优于其他四种LLM方法。

Conclusion: YATE是一种简单高效的技术，能以较低成本显著提升测试效果。

Abstract: Recent advances in automated test generation utilises language models to
produce unit tests. While effective, language models tend to generate many
incorrect tests with respect to both syntax and semantics. Although such
incorrect tests can be easily detected and discarded, they constitute a "missed
opportunity" -- if fixed, they are often valuable as they directly add testing
value (they effectively target the underlying program logic to be tested) and
indirectly form good seeds for generating additional tests. To this end, we
propose a simple technique for repairing some of these incorrect tests through
a combination of rule-based static analysis and re-prompting. We evaluate this
simple approach, named YATE, on a set of 6 open-source projects and show that
it can effectively produce tests that cover on average 32.06% more lines and
kill 21.77% more mutants than a plain LLM-based method. We also compare YATE
with four other LLM-based methods, namely HITS, SYMPROMPT, TESTSPARK and
COVERUP and show that it produces tests that cover substantially more code.
YATE achieves 22% higher line coverage, 20% higher branch coverage and kill 20%
more mutants at a comparable cost (number of calls to LLMs).

</details>


### [15] [Gotta catch 'em all! Towards File Localisation from Issues at Large](https://arxiv.org/abs/2507.18319)
*Jesse Maarleveld,Jiapan Guo,Daniel Feitosa*

Main category: cs.SE

TL;DR: 论文提出了一种适用于所有类型问题的文件定位数据管道，发现针对特定问题的启发式方法在通用问题上表现不佳，需研究通用模型。


<details>
  <summary>Details</summary>
Motivation: 研究目标是处理所有类型的问题，而非仅限于缺陷，以减少开发者的时间消耗。

Method: 1. 开发了一个数据管道，支持任意分支和合并实践；2. 使用传统信息检索方法评估文件定位性能；3. 通过统计分析探讨已知偏差对数据集的影响。

Result: 1. 特定启发式方法在通用问题上表现不佳；2. 不同问题类型间的性能差异显著但微小；3. 标识符对多数问题类型性能影响较小；4. 结果因项目而异，需开发可调方法。

Conclusion: 需研究通用模型，同时考虑项目特异性以提高文件定位性能。

Abstract: Bug localisation, the study of developing methods to localise the files
requiring changes to resolve bugs, has been researched for a long time to
develop methods capable of saving developers' time. Recently, researchers are
starting to consider issues outside of bugs. Nevertheless, most existing
research into file localisation from issues focusses on bugs or uses other
selection methods to ensure only certain types of issues are considered as part
of the focus of the work. Our goal is to work on all issues at large, without
any specific selection.
  In this work, we provide a data pipeline for the creation of issue file
localisation datasets, capable of dealing with arbitrary branching and merging
practices. We provide a baseline performance evaluation for the file
localisation problem using traditional information retrieval approaches.
Finally, we use statistical analysis to investigate the influence of biases
known in the bug localisation community on our dataset.
  Our results show that methods designed using bug-specific heuristics perform
poorly on general issue types, indicating a need for research into general
purpose models. Furthermore, we find that there are small, but statistically
significant differences in performance between different issue types. Finally,
we find that the presence of identifiers have a small effect on performance for
most issue types. Many results are project-dependent, encouraging the
development of methods which can be tuned to project-specific characteristics.

</details>


### [16] [FMI Meets SystemC: A Framework for Cross-Tool Virtual Prototyping](https://arxiv.org/abs/2507.18339)
*Nils Bosbach,Meik Schmidt,Lukas Jünger,Matthias Berthold,Rainer Leupers*

Main category: cs.SE

TL;DR: 提出了一种新框架，通过FMI标准将SystemC虚拟平台与外部工具集成，实现更广泛的协同仿真，从而支持完整的软件测试和验证。


<details>
  <summary>Details</summary>
Motivation: 随着系统复杂性增加，需要更全面的测试和虚拟原型，但SystemC缺乏原生FMI支持，限制了其在协同仿真中的应用。

Method: 开发了一个框架，通过FMI控制SystemC虚拟平台，并展示了一个案例：通过FMI从外部工具获取温度数据输入到SystemC仿真中的温度传感器。

Result: 无需修改目标软件即可运行并接收环境数据（如温度、速度、加速度），支持更早完成软件测试和认证。

Conclusion: 该框架为SystemC虚拟平台提供了FMI集成能力，有助于加速软件开发和认证流程。

Abstract: As systems become more complex, the demand for thorough testing and virtual
prototyping grows. To simulate whole systems, multiple tools are usually needed
to cover different parts. These parts include the hardware of a system and the
environment with which the system interacts. The Functional Mock-up Interface
(FMI) standard for co-simulation can be used to connect these tools.
  The control part of modern systems is usually a computing unit, such as a
System-on-a-Chip (SoC) or Microcontroller Unit (MCU), which executes software
from a connected memory and interacts with peripherals. To develop software
without requiring access to physical hardware, full-system simulators, the
so-called Virtual Platforms (VPs), are commonly used. The IEEE-standardized
framework for VP development is SystemC TLM. SystemC provides interfaces and
concepts that enable modular design and model exchange. However, SystemC lacks
native FMI support, which limits the integration into broader co-simulation
environments.
  This paper presents a novel framework to control and interact with
SystemC-based VPs using the FMI. We present a case study showing how a
simulated temperature sensor in a SystemC simulation can obtain temperature
values from an external tool via FMI. This approach allows the unmodified
target software to run on the VP and receive realistic environmental input data
such as temperature, velocity, or acceleration values from other tools. Thus,
extensive software testing and verification is enabled. By having tests ready
and the software pre-tested using a VP once the physical hardware is available,
certifications like ISO 26262 can be done earlier.

</details>


### [17] [Automated Code Review Using Large Language Models with Symbolic Reasoning](https://arxiv.org/abs/2507.18476)
*Busra Icoz,Goksel Biricik*

Main category: cs.SE

TL;DR: 提出了一种结合符号推理和大型语言模型的混合方法，用于自动化代码审查，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 手动代码审查主观且耗时，人工智能自动化潜力大，但现有大型语言模型缺乏逻辑推理能力。

Method: 结合符号推理技术与大型语言模型，测试了多种模型如CodeT5、CodeBERT和GraphCodeBERT。

Result: 在CodexGlue数据集上测试表明，该方法提高了自动化代码审查的准确性和效率。

Conclusion: 混合方法能有效提升自动化代码审查的性能。

Abstract: Code review is one of the key processes in the software development lifecycle
and is essential to maintain code quality. However, manual code review is
subjective and time consuming. Given its rule-based nature, code review is well
suited for automation. In recent years, significant efforts have been made to
automate this process with the help of artificial intelligence. Recent
developments in Large Language Models (LLMs) have also emerged as a promising
tool in this area, but these models often lack the logical reasoning
capabilities needed to fully understand and evaluate code. To overcome this
limitation, this study proposes a hybrid approach that integrates symbolic
reasoning techniques with LLMs to automate the code review process. We tested
our approach using the CodexGlue dataset, comparing several models, including
CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining
symbolic reasoning and prompting techniques with LLMs. Our results show that
this approach improves the accuracy and efficiency of automated code review.

</details>


### [18] [A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat](https://arxiv.org/abs/2507.18515)
*Zezhou Yang,Ting Peng,Cuiyun Gao,Chaozheng Wang,Hailiang Huang,Yuetang Deng*

Main category: cs.SE

TL;DR: 研究探讨了检索增强生成（RAG）在闭源代码库中的代码补全性能，发现相似性RAG优于标识符RAG，且结合词法和语义检索技术效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨RAG在闭源代码库中的表现，填补现有研究在开源与闭源代码库分布差异上的空白。

Method: 在微信的工业级代码库中测试26个开源LLM的两种RAG方法（标识符和相似性），并比较不同检索技术。

Result: 相似性RAG表现更好，BM25和GTE-Qwen检索技术最优；词法和语义检索结合效果最佳。

Conclusion: RAG在闭源代码库中有效，开发者调查验证了其实际应用价值。

Abstract: Code completion, a crucial task in software engineering that enhances
developer productivity, has seen substantial improvements with the rapid
advancement of large language models (LLMs). In recent years,
retrieval-augmented generation (RAG) has emerged as a promising method to
enhance the code completion capabilities of LLMs, which leverages relevant
context from codebases without requiring model retraining. While existing
studies have demonstrated the effectiveness of RAG on public repositories and
benchmarks, the potential distribution shift between open-source and
closed-source codebases presents unique challenges that remain unexplored. To
mitigate the gap, we conduct an empirical study to investigate the performance
of widely-used RAG methods for code completion in the industrial-scale codebase
of WeChat, one of the largest proprietary software systems. Specifically, we
extensively explore two main types of RAG methods, namely identifier-based RAG
and similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B
parameters. For a more comprehensive analysis, we employ different retrieval
techniques for similarity-based RAG, including lexical and semantic retrieval.
Based on 1,669 internal repositories, we achieve several key findings: (1) both
RAG methods demonstrate effectiveness in closed-source repositories, with
similarity-based RAG showing superior performance, (2) the effectiveness of
similarity-based RAG improves with more advanced retrieval techniques, where
BM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior
performance, and (3) the combination of lexical and semantic retrieval
techniques yields optimal results, demonstrating complementary strengths.
Furthermore, we conduct a developer survey to validate the practical utility of
RAG methods in real-world development environments.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [19] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: 提出了一个统一的范畴化方法(Howe's方法)，通过抽象高阶规范(AHOS)和纤维化模型，证明了行为一致性在自然条件下形成程序同余。


<details>
  <summary>Details</summary>
Motivation: 为了解决在具有定量特征的高阶语言中，如何保证共归纳推理的可靠性问题，即行为一致性需要形成程序同余。

Method: 使用抽象高阶规范(AHOS)建模高阶语言，并通过纤维化模型表示行为一致性，提出统一的Howe's方法。

Result: 在自然条件下，证明了最大行为(双)一致性在操作模型中形成同余。

Conclusion: 该方法推广了Howe's方法，并能适用于不同的语言和行为一致性，例如概率高阶语言的互模拟和行为伪度量。

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Frame-Based Zero-Shot Semantic Channel Equalization for AI-Native Communications](https://arxiv.org/abs/2507.17835)
*Simone Fiorellino,Claudio Battiloro,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.NI

TL;DR: 提出一种零样本的语义通道均衡器PFE，用于对齐异构编码器的潜在空间，解决语义噪声问题，并优化通信、计算和学习资源的动态分配。


<details>
  <summary>Details</summary>
Motivation: 未来AI原生无线网络中，独立训练的深度神经网络编码器潜在空间不匹配会导致语义噪声，影响系统性能。

Method: 提出Parseval Frame Equalizer (PFE)，无需重新训练即可对齐潜在空间，并通过动态优化策略协调资源。

Result: 仿真验证PFE能有效保持语义一致性，并在多变网络条件下满足长期延迟和准确性约束。

Conclusion: PFE为解决语义通信中的潜在空间不匹配问题提供了有效解决方案。

Abstract: In future AI-native wireless networks, the presence of mismatches between the
latent spaces of independently designed and trained deep neural network (DNN)
encoders may impede mutual understanding due to the emergence of semantic
channel noise. This undermines the receiver's ability to interpret transmitted
representations, thereby reducing overall system performance. To address this
issue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based
semantic channel equalizer that aligns latent spaces of heterogeneous encoders
without requiring system retraining. PFE enables dynamic signal compression and
expansion, mitigating semantic noise while preserving performance on downstream
tasks. Building on this capability, we introduce a dynamic optimization
strategy that coordinates communication, computation, and learning resources to
balance energy consumption, end-to-end (E2E) latency, and task performance in
multi-agent semantic communication scenarios. Extensive simulations confirm the
effectiveness of our approach in maintaining semantic consistency and meeting
long-term constraints on latency and accuracy under diverse and time-varying
network conditions.

</details>


### [21] [ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks](https://arxiv.org/abs/2507.17861)
*Daniel Ricardo Cunha Oliveira,Rodrigo Moreira,Flávio de Oliveira Silva*

Main category: cs.NI

TL;DR: 本文介绍了ARCADE方法，用于检测和评估蜂窝接入网络中的异常，并探讨6G网络中AI的应用。


<details>
  <summary>Details</summary>
Motivation: 当前5G网络中的网络数据分析功能（NWDAF）尚不足以实现全面自动化，需更全面的方法支持6G发展。

Method: 提出ARCADE方法，通过混合架构的网络分析功能检测和诊断异常。

Result: ARCADE作为6G中AI应用的实际案例，展示了更广泛的网络自动化能力。

Conclusion: 6G网络需更全面的AI方法，ARCADE是迈向这一目标的重要步骤。

Abstract: Artificial Intelligence (AI) plays a key role in developing 6G networks.
While current specifications already include Network Data Analytics Function
(NWDAF) as a network element responsible for providing information about the
core, a more comprehensive approach will be needed to enable automation of
network segments that are not yet fully explored in the context of 5G. In this
paper, we present Automated Radio Coverage Anomalies Detection and Evaluation
(ARCADE), a methodology for identifying and diagnosing anomalies in the
cellular access network. Furthermore, we demonstrate how a hybrid architecture
of network analytics functions in the evolution toward 6G can enhance the
application of AI in a broader network context, using ARCADE as a practical
example of this approach.

</details>


### [22] [Talk with the Things: Integrating LLMs into IoT Networks](https://arxiv.org/abs/2507.17865)
*Alakesh Kalita*

Main category: cs.NI

TL;DR: 论文提出了一种基于边缘计算的框架，将大语言模型（LLMs）与物联网（IoT）结合，实现自然语言控制和上下文感知决策，并通过实验验证了智能家居原型的效果。


<details>
  <summary>Details</summary>
Motivation: 通过融合LLMs和IoT网络，构建更智能、响应更快且用户友好的系统。

Method: 采用轻量级的检索增强生成（RAG）LLMs，部署在边缘设备上，实现本地化处理用户命令和传感器数据。

Result: 实验展示了模型大小与推理时间及准确性之间的权衡，并成功验证了智能家居原型的可行性。

Conclusion: LLM-based IoT系统具有广阔的应用前景，但也面临一些关键挑战。

Abstract: The convergence of Large Language Models (LLMs) and Internet of Things (IoT)
networks open new opportunities for building intelligent, responsive, and
user-friendly systems. This work presents an edge-centric framework that
integrates LLMs into IoT architectures to enable natural language-based
control, context-aware decision-making, and enhanced automation. The proposed
modular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are
deployed on edge computing devices connected to IoT gateways, enabling local
processing of user commands and sensor data for reduced latency, improved
privacy, and enhanced inference quality. We validate the framework through a
smart home prototype using LLaMA 3 and Gemma 2B models for controlling smart
devices. Experimental results highlight the trade-offs between model accuracy
and inference time with respect to models size. At last, we also discuss the
potential applications that can use LLM-based IoT systems, and a few key
challenges associated with such systems.

</details>


### [23] [Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN](https://arxiv.org/abs/2507.17905)
*Mahbubur Rahman*

Main category: cs.NI

TL;DR: 本文提出了一种基于SNOW技术的LPWAN解决方案，通过使用Gold码伪随机噪声序列实现大规模并发的数据传输，显著提升了系统的可扩展性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决LPWAN在大规模传感器网络中实现高效、低延迟数据传输的挑战。

Method: 利用分布式正交频分复用（D-OFDM）和Gold码伪随机噪声序列实现多传感器在同一子载波上的并行数据传输。

Result: 实验结果表明，系统可扩展性提升了约9倍，同时在数据收集的时效性和传感器能耗方面表现优异。

Conclusion: 该方法为需要大量传感器支持且对时效性和能耗敏感的物联网和CPS应用提供了可行的解决方案。

Abstract: LPWANs have become ubiquitous due to their ability to connect sensors over
large geographic areas in a single hop. It is, however, very challenging to
achieve massive scalability in LPWANs, where numerous sensors can transmit data
efficiently and with low latency, which emerging IoT and CPS applications may
require. In this paper, we address the above challenges by significantly
advancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal
frequency division multiplexing, D-OFDM, subcarriers to enable parallel
reception of data to a BS from multiple asynchronous sensors, each using a
different subcarrier. In this paper, we achieve massive scalability in SNOW by
enabling the BS to decode concurrent data from numerous asynchronous sensors on
the same subcarrier while parallelly decoding from other subcarriers as well.
Additionally, we enable numerous asynchronous sensors to receive distinct data
from the BS on the same subcarrier while other sensors also receive data
parallelly on other subcarriers. To do this, we develop a set of Gold
code-based pseudorandom noise or PN sequences that are mutually non-interfering
within and across the subcarriers. Each sensor uses its PN sequence from the
set for encoding or decoding data on its subcarriers, enabling massive
concurrency. Our evaluation results demonstrate that we can achieve
approximately 9x more scalability in SNOW while being timely in data collection
at the BS and energy efficient at the sensors. This may enable emerging IoT and
CPS applications requiring tens of thousands of sensors with longer battery
life and making data-driven, time-sensitive decisions.

</details>


### [24] [Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of Information Optimization in Vehicular Networks](https://arxiv.org/abs/2507.18328)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 研究在5G NR V2I Mode 2车辆网络中公平访问和信息时效性（AoI）问题，提出联合优化框架，通过调整SPS选择窗口和LLM-Based MOEA/D算法平衡公平性和AoI。


<details>
  <summary>Details</summary>
Motivation: 车辆速度差异导致RSU停留时间和通信时长不同，引发网络资源访问不公平，影响驾驶安全；同时需保证数据的时效性。

Method: 定义公平性指标，用SHS建模AoI，调整Mode 2的SPS选择窗口，采用LLM-Based MOEA/D算法联合优化公平性和AoI。

Result: 仿真结果表明方案能有效平衡公平访问和最小化AoI。

Conclusion: 联合优化框架解决了车辆网络中的公平性和时效性问题，提升了驾驶安全性。

Abstract: In this paper, we consider the fair access problem and the Age of Information
(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in
vehicular networks. Specifically, vehicles follow Mode 2 to communicate with
Roadside Units (RSUs) to obtain accurate data for driving
assistance.Nevertheless, vehicles often have different velocity when they are
moving in adjacent lanes, leading to difference in RSU dwelltime and
communication duration. This results in unfair access to network resources,
potentially influencing driving safety. To ensure the freshness of received
data, the AoI should be analyzed. Mode 2 introduces a novel preemption
mechanism, necessitating simultaneous optimization of fair access and AoI to
guarantee timely and relevant data delivery. We propose a joint optimization
framework for vehicular network, defining a fairness index and employing
Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By
adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)
in Mode 2, we address the optimization of fairness and AoI. We apply a large
language model (LLM)-Based Multi-objective Evolutionary Algorithm Based on
Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate
the effectiveness of our scheme in balancing fair access and minimizing AoI.

</details>


### [25] [Improving Wi-Fi 8 Latency with Coordinated Spatial Reuse](https://arxiv.org/abs/2507.18480)
*David Nunez,Francesc Wilhelmi,Lorenzo Galati-Giordano,Giovanni Geraci,Boris Bellalta*

Main category: cs.NI

TL;DR: 论文探讨了协调空间重用（Co-SR）在Wi-Fi 8网络中优化频谱资源利用的性能，通过实现与Wi-Fi 8标准对齐的Co-SR机制，在模拟器中验证其有效性。结果显示，与分布式协调功能（DCF）相比，延迟减少31%至95%。


<details>
  <summary>Details</summary>
Motivation: 为满足云游戏、扩展现实（XR）和视频流等新兴应用对高吞吐量、低延迟和高可靠性的需求，需要优化IEEE 802.11网络的频谱资源利用。

Method: 提出与Wi-Fi 8标准化对齐的Co-SR机制，并在Wi-Fi模拟器中通过四个接入点（AP）的无线局域网（WLAN）场景进行性能评估。

Result: 与DCF相比，Co-SR机制在测试中将延迟降低31%至95%。

Conclusion: Co-SR机制在密集环境中能够显著提升频谱效率，从而提高整体网络性能，支持Wi-Fi 8网络的未来应用需求。

Abstract: IEEE 802.11 networks continuously adapt to meet the stringent requirements of
emerging applications like cloud gaming, eXtended Reality (XR), and video
streaming services, which require high throughput, low latency, and high
reliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can
potentially contribute to optimizing spectrum resource utilization. This
mechanism is expected to enable simultaneous transmissions, thereby boosting
spectral efficiency in dense environments and increasing the overall network
performance. In this paper, we shed light on the performance of Co-SR for Wi-Fi
8 networks. For that, we propose an implementation of Co-SR aligned with
ongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi
simulator, which allows us to study the performance of the proposed Co-SR
mechanisms in relevant scenarios. The results obtained in a Wireless Local Area
Network (WLAN) consisting of four APs show delay reduction with Co-SR ranging
from 31% to 95% when compared to Distributed Coordination Function (DCF).

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [26] [Program Logics via Distributive Monoidal Categories](https://arxiv.org/abs/2507.18238)
*Filippo Bonchi,Elena Di Lavore,Mario Román,Sam Staton*

Main category: cs.LO

TL;DR: 论文通过统一追踪分布式副本丢弃范畴的框架，推导出多种程序逻辑；并引入内部语言以适配Dijkstra的防护命令语言。


<details>
  <summary>Details</summary>
Motivation: 动机是通过范畴论的基础建立统一的程序逻辑框架，以涵盖正确性、不正确性和关系Hoare逻辑。

Method: 方法包括引入一种用于命令式多范畴的内部语言，并从中派生程序逻辑的规则。

Result: 结果是成功推导出多种程序逻辑，并开发了适配Dijkstra防护命令语言的组合器。

Conclusion: 结论是通过范畴论可以统一地构建和派生程序逻辑，为程序验证提供理论基础。

Abstract: We derive multiple program logics, including correctness, incorrectness, and
relational Hoare logic, from the axioms of imperative categories: uniformly
traced distributive copy-discard categories. We introduce an internal language
for imperative multicategories, on top of which we derive combinators for an
adaptation of Dijkstra's guarded command language. Rules of program logics are
derived from this internal language.

</details>


### [27] [Resourceful Traces for Commuting Processes](https://arxiv.org/abs/2507.18246)
*Matthew Earnshaw,Chad Nester,Mario Román*

Main category: cs.LO

TL;DR: 该论文提出了一种新的有效类别表示方法，基于Mazurkiewicz迹的动作从输入到输出的转换，而非仅作为原子名称。


<details>
  <summary>Details</summary>
Motivation: 探索有效类别（广义Freyd类别）的新表示方法，以更好地模拟具有副作用的计算。

Method: 将Mazurkiewicz迹的动作视为输入到输出的转换，提出一种新的图形表示法。

Result: 开发了一种有效类别的图形计算工具，并用于构建自由有效类别的交换张量积。

Conclusion: 该方法为有效类别提供了新的表示工具，支持资源交换和动作间的交换性。

Abstract: We show that, when the actions of a Mazurkiewicz trace are considered not
merely as atomic (i.e., mere names) but transformations from a specified type
of inputs to a specified type of outputs, we obtain a novel notion of
presentation for effectful categories (also known as generalised Freyd
categories), a well-known algebraic structure in the semantics of
side-effecting computation. Like the usual representation of traces as graphs,
our notion of presentation gives rise to a graphical calculus for effectful
categories. We use our presentations to give a construction of the commuting
tensor product of free effectful categories, capturing the combination of
systems in which the actions of each must commute with one another, while still
permitting exchange of resources

</details>


### [28] [Distributing Retractions, Weak Distributive Laws and Applications to Monads of Hyperspaces, Continuous Valuations and Measures](https://arxiv.org/abs/2507.18418)
*Jean Goubault-Larrecq*

Main category: cs.LO

TL;DR: 论文研究了在范畴中两个单子 $S$ 和 $T$ 通过弱分配律结合成单子 $U$ 的构建方法，并提出了验证 $U$ 是结合单子的条件——分配收缩。


<details>
  <summary>Details</summary>
Motivation: 目的是明确由弱分配律结合的两个单子 $S$ 和 $T$ 生成的单子 $U$ 的具体形式，并提供一种验证 $U$ 正确性的方法。

Method: 通过分配收缩的概念，在 2-范畴设定中证明了它与弱分配律的一一对应关系，并通过三个应用实例验证了理论。

Result: 证明了分配收缩与弱分配律的一一对应性，并成功构建了多个具体的单子实例，如预现值单子和分叉单子。

Conclusion: 提出的方法能够有效验证结合单子的正确性，并扩展了对特定单子（如 Smyth、Hoare、Plotkin 超空间单子）的理解与应用。

Abstract: Given two monads $S$, $T$ on a category where idempotents split, and a weak
distributive law between them, one can build a combined monad $U$. Making
explicit what this monad $U$ is requires some effort. When we already have an
idea what $U$ should be, we show how to recognize that $U$ is indeed the
combined monad obtained from $S$ and $T$: it suffices to exhibit what we call a
distributing retraction of $ST$ onto $U$. We show that distributing retractions
and weak distributive laws are in one-to-one correspondence, in a 2-categorical
setting. We give three applications, where $S$ is the Smyth, Hoare or Plotkin
hyperspace monad, $T$ is a monad of continuous valuations, and $U$ is a monad
of previsions or of forks, depending on the case. As a byproduct, this allows
us to describe the algebras of monads of superlinear, resp. sublinear
previsions. In the category of compact Hausdorff spaces, the Plotkin hyperspace
monad is sometimes known as the Vietoris monad, the monad of probability
valuations coincides with the Radon monad, and we infer that the associated
combined monad is the monad of normalized forks.

</details>


### [29] [Well-Founded Coalgebras Meet König's Lemma](https://arxiv.org/abs/2507.18539)
*Henning Urbat,Thorsten Wißmann*

Main category: cs.LO

TL;DR: 该论文提出了一个基于余代数的König引理版本，从有限分支树推广到有限余函子的余代数，并展示了其在多种范畴中的应用。


<details>
  <summary>Details</summary>
Motivation: 通过推广König引理，研究其在更广泛的范畴中的适用性，为计算机科学和数学中的余代数理论提供新的工具和视角。

Method: 使用余代数和局部有限可呈现范畴的理论框架，从有限分支树推广到有限余函子的余代数，并在不同范畴中验证其适用性。

Result: 证明了在温和条件下，任何良基余代数都是其有限生成状态空间的良基子余代数的有向并，且良基余代数的范畴是局部可呈现的。

Conclusion: 论文不仅扩展了König引理的应用范围，还为初始代数的构造提供了新的简洁方法，同时提供了透明的新证明。

Abstract: K\"onig's lemma is a fundamental result about trees with countless
applications in mathematics and computer science. In contrapositive form, it
states that if a tree is finitely branching and well-founded (i.e. has no
infinite paths), then it is finite. We present a coalgebraic version of
K\"onig's lemma featuring two dimensions of generalization: from finitely
branching trees to coalgebras for a finitary endofunctor H, and from the base
category of sets to a locally finitely presentable category C, such as the
category of posets, nominal sets, or convex sets. Our coalgebraic K\"onig's
lemma states that, under mild assumptions on C and H, every well-founded
coalgebra for H is the directed join of its well-founded subcoalgebras with
finitely generated state space -- in particular, the category of well-founded
coalgebras is locally presentable. As applications, we derive versions of
K\"onig's lemma for graphs in a topos as well as for nominal and convex
transition systems. Additionally, we show that the key construction underlying
the proof gives rise to two simple constructions of the initial algebra
(equivalently, the final recursive coalgebra) for the functor H: The initial
algebra is both the colimit of all well-founded and of all recursive coalgebras
with finitely presentable state space. Remarkably, this result holds even in
settings where well-founded coalgebras form a proper subclass of recursive
ones. The first construction of the initial algebra is entirely new, while for
the second one our approach yields a short and transparent new correctness
proof.

</details>


### [30] [Proceedings 19th International Workshop on the ACL2 Theorem Prover and Its Applications](https://arxiv.org/abs/2507.18567)
*Ruben Gamboa,Panagiotis Manolios*

Main category: cs.LO

TL;DR: ACL2研讨会是ACL2定理证明系统用户展示相关研究的主要平台，ACL2是工业级的自动推理系统，属于Boyer-Moore家族定理证明器的最新成员。


<details>
  <summary>Details</summary>
Motivation: 促进ACL2定理证明系统及其应用的研究交流。

Method: 通过研讨会形式，汇集研究者和用户，展示研究成果。

Result: ACL2作为工业级自动推理系统，其重要性得到认可，2005年ACM软件系统奖授予其开发者。

Conclusion: ACL2及其家族在定理证明领域具有重要影响，研讨会为其用户提供了研究和技术交流的关键平台。

Abstract: The ACL2 Workshop series is the major technical forum for users of the ACL2
theorem proving system to present research related to the ACL2 theorem prover
and its applications. ACL2 is an industrial-strength automated reasoning
system, the latest in the Boyer-Moore family of theorem provers. The 2005 ACM
Software System Award was awarded to Boyer, Kaufmann, and Moore for their work
on ACL2 and the other theorem provers in the Boyer-Moore family.

</details>


### [31] [Approximate SMT Counting Beyond Discrete Domains](https://arxiv.org/abs/2507.18612)
*Arijit Shaw,Kuldeep S. Meel*

Main category: cs.LO

TL;DR: 本文介绍了pact，一种用于混合公式的SMT模型计数器，采用哈希近似计数方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如bit-blasting）仅适用于离散变量，难以处理混合公式中的投影变量计数问题，因此需要扩展SMT求解器的能力。

Method: 利用基于哈希的近似模型计数方法，通过对数级数量的SMT求解器调用和优化的哈希函数，估计混合公式的解。

Result: 在大量基准测试中，pact表现优于基线方法，成功完成603个实例，而基线仅完成13个。

Conclusion: pact为混合SMT公式的模型计数提供了高效且理论保障的解决方案。

Abstract: Satisfiability Modulo Theory (SMT) solvers have advanced automated reasoning,
solving complex formulas across discrete and continuous domains. Recent
progress in propositional model counting motivates extending SMT capabilities
toward model counting, especially for hybrid SMT formulas. Existing approaches,
like bit-blasting, are limited to discrete variables, highlighting the
challenge of counting solutions projected onto the discrete domain in hybrid
formulas.
  We introduce pact, an SMT model counter for hybrid formulas that uses
hashing-based approximate model counting to estimate solutions with theoretical
guarantees. pact makes a logarithmic number of SMT solver calls relative to the
projection variables, leveraging optimized hash functions. pact achieves
significant performance improvements over baselines on a large suite of
benchmarks. In particular, out of 14,202 instances, pact successfully finished
on 603 instances, while Baseline could only finish on 13 instances.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [32] [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)
*Liang Zhang,Xiaoming Zhai,Jionghao Lin,Jionghao Lin,Jennifer Kleiman,Diego Zapata-Rivera,Carol Forsyth,Yang Jiang,Xiangen Hu,Arthur C. Graesser*

Main category: cs.HC

TL;DR: 研究探讨了四种沟通模式对LLM代理在教育中协作问题解决的影响，发现双代理设置优于单代理，其中点对点协作模式表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在AI辅助教育中的应用增多，有效沟通策略对提高问题解决效率和降低成本至关重要，但相关研究较少。

Method: 研究在数学问题解决环境中测试了四种沟通模式（师生互动、点对点协作、互惠同伴教学和批判性辩论），采用GPT-4o模型和MATH数据集进行评估。

Result: 结果显示双代理设置优于单代理，点对点协作模式准确率最高，沟通行为（如陈述、确认和提示）在协作中起关键作用。

Conclusion: 尽管多代理框架提升计算任务效率，但有效沟通策略是解决AI教育中复杂问题的关键。

Abstract: Large Language Model (LLM) agents are increasingly utilized in AI-aided
education to support tutoring and learning. Effective communication strategies
among LLM agents improve collaborative problem-solving efficiency and
facilitate cost-effective adoption in education. However, little research has
systematically evaluated the impact of different communication strategies on
agents' problem-solving. Our study examines four communication modes,
\textit{teacher-student interaction}, \textit{peer-to-peer collaboration},
\textit{reciprocal peer teaching}, and \textit{critical debate}, in a
dual-agent, chat-based mathematical problem-solving environment using the
OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that
dual-agent setups outperform single agents, with \textit{peer-to-peer
collaboration} achieving the highest accuracy. Dialogue acts like statements,
acknowledgment, and hints play a key role in collaborative problem-solving.
While multi-agent frameworks enhance computational tasks, effective
communication strategies are essential for tackling complex problems in AI
education.

</details>


### [33] [A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians](https://arxiv.org/abs/2507.17754)
*Justin Morse,Kurt Gilbert,Kyle Shin,Rick Cooke,Peyton Rose,Jack Sullivan,Angelo Sisante*

Main category: cs.HC

TL;DR: 研究介绍了整合到电子健康记录系统中的环境医疗记录应用，通过Whisper和GPT-4等技术自动生成SOAP笔记，显著减轻临床医生的负担。


<details>
  <summary>Details</summary>
Motivation: 临床医生的工作负担和倦怠问题促使了环境医疗记录应用的开发和推广。

Method: 应用整合Whisper用于转录，结合GPT-4的上下文学习管道自动生成SOAP笔记和患者指导，并使用微调的BART模型提升简洁性。

Result: 生成笔记的质量超过专家手写笔记，540多名临床医生使用该应用，94%的受访医生报告认知负荷降低，97%报告文档负担减轻。

Conclusion: AI系统有潜力减轻行政管理负担，支持临床医生提供高效、高质量的医疗服务。

Abstract: Clinician burnout has motivated the growing adoption of ambient medical
scribes in the clinic. In this work, we introduce a custom-built ambient scribe
application integrated into the EHR system at Included Health, a personalized
all-in-one healthcare company offering telehealth services. The application
uses Whisper for transcription and a modular in-context learning pipeline with
GPT-4o to automatically generate SOAP notes and patient instructions. Testing
on mock visit data shows that the notes generated by the application exceed the
quality of expert-written notes as determined by an LLM-as-a-judge. The
application has been widely adopted by the clinical practice, with over 540
clinicians at Included Health using the application at least once. 94% (n = 63)
of surveyed clinicians report reduced cognitive load during visits and 97% (n =
66) report less documentation burden when using the application. Additionally,
we show that post-processing notes with a fine-tuned BART model improves
conciseness. These findings highlight the potential for AI systems to ease
administrative burdens and support clinicians in delivering efficient,
high-quality care.

</details>


### [34] [Effects of variation in system responsiveness on user performance in virtual environments](https://arxiv.org/abs/2507.18085)
*Benjamin Watson,Neff Walker,William Ribarsky,Victoria Spaulding*

Main category: cs.HC

TL;DR: 论文研究了虚拟环境中系统响应性（SR）的统计特征（MSR和SDSR）及其对操作任务的影响，发现SDSR超过82ms时显著影响任务表现。


<details>
  <summary>Details</summary>
Motivation: 探讨虚拟环境中系统响应性的波动及其对用户操作任务的影响，为优化人机交互设计提供依据。

Method: 通过三个实验研究MSR和SDSR对抓取和放置任务的影响，采用被试内设计。

Result: SDSR仅在超过82ms时影响任务表现，放置任务对SR更敏感且需要更多视觉反馈。

Conclusion: 虚拟环境设计无需严格控制SDSR，但应根据任务需求调整SR控制，适用于多种交互图形应用。

Abstract: System responsiveness (SR) is defined as the elapsed time until a system
responds to user control. SR fluctuates over time, so it must be described
statistically with mean (MSR) and standard deviation (SDSR). In this paper, we
examine SR in virtual environments (VEs), outlining its components and methods
of experimental measurement and manipulation. Three studies of MSR and SDSR
effects on performance of grasp and placement tasks are then presented. The
studies used within-subjects designs with 11, 12, and 10 participants,
respectively. Results showed that SDSR affected performance only if it was
above 82 ms. Placement required more frequent visual feedback and was more
sensitive to SR. We infer that VE designers need not tightly control SDSR and
may wish to vary SR control based on required visual feedback frequency. These
results may be used to improve the human-computer interface in a wide range of
interactive graphical applications, including scientific visualization,
training, mental health, and entertainment.

</details>


### [35] [Between Filters and Feeds: Investigating Douyin and WeChat's Influence on Chinese Adolescent Body Image](https://arxiv.org/abs/2507.17755)
*Jianfeng Lan,Yingjia Huang*

Main category: cs.HC

TL;DR: 研究探讨了抖音和微信对中国男性青少年体像感知的影响，发现抖音使用与体像满意度显著相关，而微信则无显著影响。


<details>
  <summary>Details</summary>
Motivation: 数字化时代下，社交媒体平台对青少年体像感知的影响日益重要，尤其是不同平台特性的差异值得探究。

Method: 采用平台化视角，对395名10至24岁中国男性青少年进行问卷调查，使用MBSRQ-AS工具评估体像感知。

Result: 抖音使用显著相关于体像满意度和外观评价，而微信使用与体像维度无显著关联。

Conclusion: 平台特性和内容形式（如抖音的视频中心化算法）对体像感知有重要影响，需针对性采取措施。

Abstract: In the digital era, social media platforms play a pivotal role in shaping
adolescents' body image perceptions. This study examines how Douyin and WeChat,
two contrasting Chinese social media platforms, influence body image among
Chinese male adolescents. Employing a platformization perspective, we surveyed
395 male adolescents aged 10 to 24 using the Multidimensional Body-Self
Relations Questionnaire-Appearance Scales (MBSRQ-AS) to assess self-evaluation
and body satisfaction. Our findings reveal that Douyin usage is significantly
correlated with appearance evaluation and body area satisfaction, while WeChat
usage shows no significant correlation with any body image dimensions. These
results suggest that Douyin's algorithm-driven, video-centric environment
intensifies exposure to idealized body standards, impacting users at a
cognitive level. This study underscores the importance of considering
platform-specific characteristics in understanding social media's impact on
body image. It contributes to the broader discourse on how technological design
and content modalities mediate psychological outcomes, offering insights for
addressing body image concerns among male adolescents in China.

</details>


### [36] [Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy](https://arxiv.org/abs/2507.17756)
*Josh Hunter,John McDermid,Simon Burton*

Main category: cs.HC

TL;DR: 研究探讨铁路专业人员对安全概念的认知，为未来技术发展提供参考。


<details>
  <summary>Details</summary>
Motivation: 了解铁路安全实践现状，探讨自动化潜力及系统复杂性，以指导技术发展。

Method: 通过采访驾驶员、路线规划人员和行政人员，分析安全实践和自动化态度。

Result: 发现对自动化持谨慎态度，偏好辅助技术，安全认知涵盖人、系统和技术的综合因素。

Conclusion: 需铁路专用的因果关系模型评估安全，弥补研究与实际应用间的差距，推动更有效的安全指标。

Abstract: This study investigates how railway professionals perceive safety as a
concept within rail, with the intention to help inform future technological
developments within the industry. Through a series of interviews with drivers,
route planners,and administrative personnel, the research explores the
currentstate of safety practices, the potential for automation and the
understanding of the railway as a system of systems. Key findings highlight a
cautious attitude towards automation, a preference for assistive technologies,
and a complex understanding of safety that integrates human, systematic and
technological factors. The study also addresses the limitations of transferring
automotive automation technologies to railways and the need for a
railway-specific causation model to better evaluate and enhance safety in an
evolving technological landscape. This study aims to bridge thegap between
contemporary research and practical applications, contributing to the
development of more effective safety metrics.

</details>


### [37] [BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches](https://arxiv.org/abs/2507.17757)
*Sam Gordon James,Miranda Elaine Glynis Armstrong,Aisling Ann O'Kane,Harry Emerson,Zahraa S. Abdallah*

Main category: cs.HC

TL;DR: 论文介绍了BrisT1D数据集，从24名英国年轻成年人的长期研究中收集了T1D管理设备数据和智能手表数据，以及访谈记录。数据集支持血糖预测和用户体验研究。


<details>
  <summary>Details</summary>
Motivation: 探索1型糖尿病（T1D）管理技术在现实世界中的应用潜力，并通过公开数据集促进相关研究的发展。

Method: 从24名使用智能手表的T1D患者中收集设备数据和访谈记录，数据集提供处理和原始两种格式。

Result: 数据集包含定量和定性数据，可用于血糖预测、低血糖预测、闭环算法开发以及用户体验研究。

Conclusion: BrisT1D数据集为T1D管理的多个研究方向提供了支持，包括技术开发和用户行为分析。

Abstract: Background: Type 1 diabetes (T1D) has seen a rapid evolution in management
technology and forms a useful case study for the future management of other
chronic conditions. Further development of this management technology requires
an exploration of its real-world use and the potential of additional data
streams. To facilitate this, we contribute the BrisT1D Dataset to the growing
number of public T1D management datasets. The dataset was developed from a
longitudinal study of 24 young adults in the UK who used a smartwatch alongside
their usual T1D management. Findings: The BrisT1D dataset features both device
data from the T1D management systems and smartwatches used by participants, as
well as transcripts of monthly interviews and focus groups conducted during the
study. The device data is provided in a processed state, for usability and more
rapid analysis, and in a raw state, for in-depth exploration of novel insights
captured in the study. Conclusions: This dataset has a range of potential
applications. The quantitative elements can support blood glucose prediction,
hypoglycaemia prediction, and closed-loop algorithm development. The
qualitative elements enable the exploration of user experiences and opinions,
as well as broader mixed-methods research into the role of smartwatches in T1D
management.

</details>


### [38] [DHMS: A Digital Hostel Management System Integrating Campus ChatBot, Predictive Intelligence, and Real-Time Automation](https://arxiv.org/abs/2507.17759)
*Riddhi Heda,Sidhant Singh,Umair Yasir,Tanmay Jaiswal,Anil Mokhade*

Main category: cs.HC

TL;DR: 论文介绍了DHMS（数字宿舍管理系统），一个通过现代技术优化宿舍管理的平台，测试结果显示高满意度和效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统宿舍管理系统效率低下、沟通不畅，无法满足数字化学生的需求，增加了工作人员的负担。

Method: 采用现代网页技术、人工智能和云基础设施，开发了模块化平台DHMS，实现房间分配、投诉处理等功能自动化，并利用聊天机器人和预测分析。

Result: 模拟测试中，DHMS在房间分配上达到92%满意度，聊天机器人响应时间低于1秒，具备预测性维护和情感分析功能。

Conclusion: 尽管效果良好，仍需进一步测试集成性、用户接受度和扩展性，论文探讨了系统架构及提升用户体验的关键因素。

Abstract: Traditional hostel management practices in academic institutions often suffer
from inefficiencies, delays, and fragmented communication. These systems fail
to meet the expectations of digitally native students and place a significant
operational burden on hostel staff. This paper introduces DHMS (Digital Hostel
Management System), a modular and integrated platform designed to digitize and
streamline essential hostel management functions. DHMS leverages modern web
technologies, artificial intelligence, and cloud infrastructure to automate
room allotment, grievance redressal, gate pass logistics, and communication via
a natural language chatbot. In simulation tests, DHMS achieved a 92% student
satisfaction rate in room allocation and maintained an average chatbot response
time below one second. Additional features include predictive analytics for
proactive maintenance planning and sentiment analysis for feedback processing.
While promising, the system requires further testing for integration across
multiple hostel blocks, user acceptance, scalability under load, and ERP
compatibility before campus-wide deployment. This work discusses the system
architecture, implementation approach, and factors critical to improving user
experience, administrative efficiency, and decision-making processes.

</details>


### [39] [Co-constructing Explanations for AI Systems using Provenance](https://arxiv.org/abs/2507.17761)
*Jan-Christoph Kalo,Fina Polat,Shubha Guha,Paul Groth*

Main category: cs.HC

TL;DR: 提出了一种交互式代理，帮助用户与AI系统共同构建基于数据溯源的可解释性解释。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统复杂，数据溯源虽能解释输出但过于详细且缺乏上下文，用户难以理解。

Method: 开发了交互式代理原型，结合用户模拟和大语言模型作为评估框架。

Result: 该方法提供了一种实用且基于数据溯源的解释方式。

Conclusion: 交互式代理为用户与AI系统的可解释性提供了新思路。

Abstract: Modern AI systems are complex workflows containing multiple components and
data sources. Data provenance provides the ability to interrogate and
potentially explain the outputs of these systems. However, provenance is often
too detailed and not contextualized for the user trying to understand the AI
system. In this work, we present our vision for an interactive agent that works
together with the user to co-construct an explanation that is simultaneously
useful to the user as well as grounded in data provenance. To illustrate this
vision, we present: 1) an initial prototype of such an agent; and 2) a scalable
evaluation framework based on user simulations and a large language model as a
judge approach.

</details>


### [40] [Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems](https://arxiv.org/abs/2507.17774)
*Zhangqi Liu*

Main category: cs.HC

TL;DR: 探讨AI从后台计算工具转变为设计协作伙伴的新范式，研究大型语言模型和多模态扩散模型如何参与设计的提案、批评和修改循环。


<details>
  <summary>Details</summary>
Motivation: 重新思考传统以人为中心的设计流程，以适应AI在早期设计阶段作为创意协作伙伴的集成。

Method: 使用大型语言模型（如GPT-4）和多模态扩散模型（如Stable Diffusion）作为创意代理，参与设计的迭代循环。

Result: 提出了一种新的设计流程，其中AI不仅是自动化工具，还能积极参与创意生成和决策。

Conclusion: 人机协作的设计模式有望为早期设计过程带来更高效的创意和决策支持。

Abstract: As artificial intelligence (AI) continues to evolve from a back-end
computational tool into an interactive, generative collaborator, its
integration into early-stage design processes demands a rethinking of
traditional workflows in human-centered design. This paper explores the
emergent paradigm of human-AI co-creation, where AI is not merely used for
automation or efficiency gains, but actively participates in ideation, visual
conceptualization, and decision-making. Specifically, we investigate the use of
large language models (LLMs) like GPT-4 and multimodal diffusion models such as
Stable Diffusion as creative agents that engage designers in iterative cycles
of proposal, critique, and revision.

</details>


### [41] [Same Data, Different Audiences: Using Personas to Scope a Supercomputing Job Queue Visualization](https://arxiv.org/abs/2507.17898)
*Connor Scully-Allison,Kevin Menear,Kristin Potter,Andrew McNutt,Katherine E. Isaacs,Dmitry Duplyakin*

Main category: cs.HC

TL;DR: 论文探讨如何通过多用户视角设计可视化工具Guidepost，兼顾不同用户群体的需求，提升工具的通用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决领域特定可视化工具对单一用户群体的局限性，试图通过支持多群体任务来增强工具的实用性。

Method: 通过设计研究开发Guidepost工具，结合不同用户角色（科学家、机器学习研究者、系统管理员）的任务需求，采用交互式可视化与脚本化设计相结合的方法。

Result: 评估结果表明，工具成功支持了共享任务的交互式操作，同时为特定任务提供了编程分析的工作流。

Conclusion: 结论指出，通过多用户视角设计可视化工具可以有效平衡通用性和特定任务支持，提升整体实用性。

Abstract: Domain-specific visualizations sometimes focus on narrow, albeit important,
tasks for one group of users. This focus limits the utility of a visualization
to other groups working with the same data. While tasks elicited from other
groups can present a design pitfall if not disambiguated, they also present a
design opportunity -- development of visualizations that support multiple
groups. This development choice presents a trade off of broadening the scope
but limiting support for the more narrow tasks of any one group, which in some
cases can enhance the overall utility of the visualization. We investigate this
scenario through a design study where we develop \textit{Guidepost}, a
notebook-embedded visualization of supercomputer queue data that helps
scientists assess supercomputer queue wait times, machine learning researchers
understand prediction accuracy, and system maintainers analyze usage trends. We
adapt the use of personas for visualization design from existing literature in
the HCI and software engineering domains and apply them in categorizing tasks
based on their uniqueness across the stakeholder personas. Under this model,
tasks shared between all groups should be supported by interactive
visualizations and tasks unique to each group can be deferred to scripting with
notebook-embedded visualization design. We evaluate our visualization with nine
expert analysts organized into two groups: a "research analyst" group that uses
supercomputer queue data in their research (representing the Machine Learning
researchers and Jobs Data Analyst personas) and a "supercomputer user" group
that uses this data conditionally (representing the HPC User persona). We find
that our visualization serves our three stakeholder groups by enabling users to
successfully execute shared tasks with point-and-click interaction while
facilitating case-specific programmatic analysis workflows.

</details>


### [42] [Automated Brake Onset Detection in Naturalistic Driving Data](https://arxiv.org/abs/2507.17943)
*Shu-Yuan Liu,Johan Engström,Gustav Markkula*

Main category: cs.HC

TL;DR: 该论文提出了一种基于分段线性加速度模型的算法，用于自动估算刹车起始时间，适用于没有车辆控制信号的大规模数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动标注或车辆控制信号，无法适用于大规模ADS日志数据，因此需要开发一种新的自动估算方法。

Method: 提出了一种基于分段线性加速度模型的算法，并通过手动标注方法作为验证基准，使用R2作为置信度指标评估算法准确性。

Result: 算法在自然碰撞避免数据上验证了其高效性和通用性，适用于各类道路使用者和场景。

Conclusion: 尽管算法有一定局限性，但其高效、通用、可配置性强，适用于大规模数据分析。

Abstract: Response timing measures play a crucial role in the assessment of automated
driving systems (ADS) in collision avoidance scenarios, including but not
limited to establishing human benchmarks and comparing ADS to human driver
response performance. For example, measuring the response time (of a human
driver or ADS) to a conflict requires the determination of a stimulus onset and
a response onset. In existing studies, response onset relies on manual
annotation or vehicle control signals such as accelerator and brake pedal
movements. These methods are not applicable when analyzing large scale data
where vehicle control signals are not available. This holds in particular for
the rapidly expanding sets of ADS log data where the behavior of surrounding
road users is observed via onboard sensors. To advance evaluation techniques
for ADS and enable measuring response timing when vehicle control signals are
not available, we developed a simple and efficient algorithm, based on a
piecewise linear acceleration model, to automatically estimate brake onset that
can be applied to any type of driving data that includes vehicle longitudinal
time series data. We also proposed a manual annotation method to identify brake
onset and used it as ground truth for validation. R2 was used as a confidence
metric to measure the accuracy of the algorithm, and its classification
performance was analyzed using naturalistic collision avoidance data of both
ADS and humans, where our method was validated against human manual annotation.
Although our algorithm is subject to certain limitations, it is efficient,
generalizable, applicable to any road user and scenario types, and is highly
configurable.

</details>


### [43] [Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale](https://arxiv.org/abs/2507.17985)
*Alex Liu,Lief Esbenshade,Shawon Sarkar,Victor Tian,Zachary Zhang,Kevin He,Min Sun*

Main category: cs.HC

TL;DR: 论文研究了教育工作者如何实际使用大型语言模型（LLMs）工具，并提出了一种人机协作方法，分析了14万条教育工作者与AI的互动消息，揭示了LLMs在支持教学实践中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索教育工作者如何实际使用LLMs工具，并研究其与AI的互动模式，以期对教师培训和专业发展提供指导。

Method: 采用四阶段编码流程（包括主题发现、代码书开发、结构化注释和模型评估），结合定性分析，利用生成式AI平台的数据。

Result: 研究表明LLMs（特别是Claude 3.5 Haiku）能可靠支持主题识别，并在复杂场景中扩展人类认知。教育工作者主要利用AI提升教学实践（79.7%）、创建内容（76.1%）等。

Conclusion: 研究提供了一种可扩展的AI增强定性研究方法，并揭示了生成式AI在教育实践中的潜在作用，对教师培训和专业发展有直接意义。

Abstract: The integration of large language models (LLMs) into educational tools has
the potential to substantially impact how teachers plan instruction, support
diverse learners, and engage in professional reflection. Yet little is known
about how educators actually use these tools in practice and how their
interactions with AI can be meaningfully studied at scale. This paper presents
a human-AI collaborative methodology for large-scale qualitative analysis of
over 140,000 educator-AI messages drawn from a generative AI platform used by
K-12 teachers. Through a four-phase coding pipeline, we combined inductive
theme discovery, codebook development, structured annotation, and model
benchmarking to examine patterns of educator engagement and evaluate the
performance of LLMs in qualitative coding tasks. We developed a hierarchical
codebook aligned with established teacher evaluation frameworks, capturing
educators' instructional goals, contextual needs, and pedagogical strategies.
Our findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably
support theme identification, extend human recognition in complex scenarios,
and outperform open-weight models in both accuracy and structural reliability.
The analysis also reveals substantive patterns in how educators inquire AI to
enhance instructional practices (79.7 percent of total conversations), create
or adapt content (76.1 percent), support assessment and feedback loop (46.9
percent), attend to student needs for tailored instruction (43.3 percent), and
assist other professional responsibilities (34.2 percent), highlighting
emerging AI-related competencies that have direct implications for teacher
preparation and professional development. This study offers a scalable,
transparent model for AI-augmented qualitative research and provides
foundational insights into the evolving role of generative AI in educational
practice.

</details>


### [44] [Evaluating judgment of spatial correlation in visual displays of scalar field distributions](https://arxiv.org/abs/2507.17997)
*Yayan Zhao,Matthew Berger*

Main category: cs.HC

TL;DR: 研究2D标量场分布中空间相关性的识别，比较动画和并列视图的效果，并分析不同颜色尺度的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨人类是否能在视觉显示中识别空间相关性，并比较不同可视化设计的有效性。

Method: 实验设计比较动画和并列视图，研究颜色尺度和分布特性对判断的影响。

Result: 结果显示分布特性和视觉显示方式对空间相关性判断有显著影响。

Conclusion: 可视化设计和分布特性共同影响空间相关性的识别，为未来研究提供参考。

Abstract: In this work we study the identification of spatial correlation in
distributions of 2D scalar fields, presented across different forms of visual
displays. We study simple visual displays that directly show color-mapped
scalar fields, namely those drawn from a distribution, and whether humans can
identify strongly correlated spatial regions in these displays. In this
setting, the recognition of correlation requires making judgments on a set of
fields, rather than just one field. Thus, in our experimental design we compare
two basic visualization designs: animation-based displays against juxtaposed
views of scalar fields, along different choices of color scales. Moreover, we
investigate the impacts of the distribution itself, controlling for the level
of spatial correlation and discriminability in spatial scales. Our study's
results illustrate the impacts of these distribution characteristics, while
also highlighting how different visual displays impact the types of judgments
made in assessing spatial correlation. Supplemental material is available at
https://osf.io/zn4qy

</details>


### [45] ["I Would Not Be This Version of Myself Today": Elaborating on the Effects of Eudaimonic Gaming Experiences](https://arxiv.org/abs/2507.18084)
*Nisha Devasia,Georgia Kenderova,Michele Newman,Julie Kientz,Jin Ha Lee*

Main category: cs.HC

TL;DR: 论文探讨了数字游戏中eudaimonic体验的影响及其对玩家生活的积极效果，通过调查和混合方法分析了其形成机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是填补现有文献中关于eudaimonic游戏体验效果的空白，探索其对个人成长的积极影响。

Method: 采用了混合方法，包括对166名受访者的调查，分析其有意义游戏体验及其后续影响。

Result: 研究发现有意义游戏体验能带来反思、学习、社交、健康和职业方面的积极效果。

Conclusion: 研究扩展了eudaimonic游戏体验的理论模型，为研究者和从业者利用这些发现促进玩家积极结果提供了启示。

Abstract: While much of the research in digital games has emphasized hedonic
experiences, such as flow, enjoyment, and positive affect, recent years have
seen increased interest in eudaimonic gaming experiences, typically
mixed-affect and associated with personal meaningfulness and growth. The
formation of such experiences in games is theorized to have four constituent
elements: motivation, game use, experience, and effects. However, while the
first three elements have been relatively well explored in the literature, the
effects - and how they may influence positive individual outcomes - have been
underexplored thus far. To this end, in this work, we investigate the perceived
outcomes of eudaimonic gaming and how different components of the experience
influence these effects. We conducted a survey (n = 166) in which respondents
recounted meaningful gaming experiences and how they affected their present
lives. We used a mixed-methods approach to classify effects and identify
significant subcomponents of their formation. We contribute an empirical
understanding of how meaningful gaming experiences can lead to positive
reflective, learning, social, health, and career effects, extending current
theoretical models of eudaimonic gaming experiences and offering implications
for how researchers and practitioners might use these findings to promote
positive outcomes for players.

</details>


### [46] [Understood: Real-Time Communication Support for Adults with ADHD Using Mixed Reality](https://arxiv.org/abs/2507.18151)
*Shizhen Zhang,Shengxin Li,Quan Li*

Main category: cs.HC

TL;DR: Understood是一款基于微软HoloLens2的混合现实系统，旨在通过实时对话摘要、上下文感知的词建议和主题转移提醒，帮助成人ADHD患者改善日常沟通。


<details>
  <summary>Details</summary>
Motivation: 成人ADHD患者因执行功能障碍和情绪失调常面临沟通障碍，但现有干预措施多针对儿童，成人缺乏实用工具。

Method: 通过半结构化访谈和设计研讨会识别沟通障碍，开发了具有实时摘要、词建议和主题提醒功能的MR系统。

Result: 用户研究和专家访谈显示，该系统能有效支持沟通且具有高可用性。

Conclusion: Understood为成人ADHD患者提供了一种非侵入性的日常沟通辅助工具，可作为治疗的补充。

Abstract: Adults with Attention Deficit Hyperactivity Disorder (ADHD) often experience
communication challenges, primarily due to executive dysfunction and emotional
dysregulation, even after years of social integration. While existing
interventions predominantly target children through structured or intrusive
methods, adults lack tools that translate clinical strategies into daily
communication support. To address this gap, we present Understood, a Mixed
Reality (MR) system implemented on Microsoft HoloLens 2, designed to assist
adults with ADHD in real-world communication. Through formative semi-structured
interviews and a design workshop, we identified critical communication barriers
and derived design goals for the system. Understood combines three key
features: (1) real-time conversation summarization to reduce cognitive load,
(2) context-aware subsequent word suggestions during moments of disfluency, and
(3) topic shifting detection and reminding to mitigate off-topic transitions. A
within-subjects user study and expert interviews demonstrate that Understood
effectively supports communication with high usability, offering a complement
to therapist-mediated interventions.

</details>


### [47] [ProactiveVA: Proactive Visual Analytics with LLM-Based UI Agent](https://arxiv.org/abs/2507.18165)
*Yuheng Zhao,Xueli Shu,Liwen Fan,Lin Gao,Yu Zhang,Siming Chen*

Main category: cs.HC

TL;DR: 提出了一种基于LLM的ProactiveVA框架，通过监测用户交互主动提供帮助，解决了现有VA系统仅在用户请求时提供帮助的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VA系统通常只在用户明确请求时提供帮助，无法在用户最需要时主动提供智能辅助。这促使设计一种能更主动识别用户需求并提供帮助的框架。

Method: 通过分析用户交互日志中的求助行为，设计了三阶段的UI代理流程（感知、推理和行动），并将其应用于两种典型的VA系统。

Result: 通过算法评估、案例和专家研究以及用户研究验证了框架的有效性和通用性。

Conclusion: ProactiveVA框架能够主动识别用户需求并提供帮助，但仍需在设计权衡和进一步探索方面进行优化。

Abstract: Visual analytics (VA) is typically applied to complex data, thus requiring
complex tools. While visual analytics empowers analysts in data analysis,
analysts may get lost in the complexity occasionally. This highlights the need
for intelligent assistance mechanisms. However, even the latest LLM-assisted VA
systems only provide help when explicitly requested by the user, making them
insufficiently intelligent to offer suggestions when analysts need them the
most. We propose a ProactiveVA framework in which LLM-powered UI agent monitors
user interactions and delivers context-aware assistance proactively. To design
effective proactive assistance, we first conducted a formative study analyzing
help-seeking behaviors in user interaction logs, identifying when users need
proactive help, what assistance they require, and how the agent should
intervene. Based on this analysis, we distilled key design requirements in
terms of intent recognition, solution generation, interpretability and
controllability. Guided by these requirements, we develop a three-stage UI
agent pipeline including perception, reasoning, and acting. The agent
autonomously perceives users' needs from VA interaction logs, providing
tailored suggestions and intuitive guidance through interactive exploration of
the system. We implemented the framework in two representative types of VA
systems, demonstrating its generalizability, and evaluated the effectiveness
through an algorithm evaluation, case and expert study and a user study. We
also discuss current design trade-offs of proactive VA and areas for further
exploration.

</details>


### [48] [Recommender systems, representativeness, and online music: A psychosocial analysis of Italian listeners](https://arxiv.org/abs/2507.18169)
*Lorenzo Porcaro,Chiara Monaldi*

Main category: cs.HC

TL;DR: 研究了音乐推荐系统对意大利听众的影响，发现听众缺乏对算法的批判性理解，尤其是性别差异问题未受关注。


<details>
  <summary>Details</summary>
Motivation: 探讨推荐系统在音乐领域的潜在危害，尤其是从心理社会和文化角度分析听众的体验。

Method: 采访意大利听众，并通过情感文本分析（Emotional Textual Analysis）解析叙事。

Result: 发现听众对推荐系统缺乏批判性理解，性别差异问题未被充分认识。

Conclusion: 强调跨学科研究和算法意识的重要性，以构建更可信的推荐系统。

Abstract: Recommender systems shape music listening worldwide due to their widespread
adoption in online platforms. Growing concerns about representational harms
that these systems may cause are nowadays part of the scientific and public
debate, wherein music listener perspectives are oftentimes reported and
discussed from a cognitive-behaviorism perspective, but rarely contextualised
under a psychosocial and cultural lens. We proceed in this direction, by
interviewing a group of Italian music listeners and analysing their narratives
through Emotional Textual Analysis. Thanks to this, we identify shared cultural
repertoires that reveal people's complex relationship with listening practices:
even when familiar with online platforms, listeners may still lack a critical
understanding of recommender systems. Moreover, representational issues,
particularly gender disparities, seem not yet fully grasped in the context of
online music listening. This study underscores the need for interdisciplinary
research to address representational harms, and the role of algorithmic
awareness and digital literacy in developing trustworthy recommender systems.

</details>


### [49] [Multimodal Behavioral Patterns Analysis with Eye-Tracking and LLM-Based Reasoning](https://arxiv.org/abs/2507.18252)
*Dongyang Guo,Yasmeen Abdrabou,Enkeleda Thaqi,Enkelejda Kasneci*

Main category: cs.HC

TL;DR: 提出了一种多模态人机协作框架，用于从眼动追踪数据中提取认知模式，结合专家评分和LLM推理，提高了分析的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 眼动数据具有非语言和时间序列特性，传统LLMs难以处理，需要一种更有效的分析方法。

Method: 框架包括多阶段流水线、专家模型共评分模块和混合异常检测模块，结合LLM和专家知识。

Result: 在多个LLMs和提示策略下，框架提升了分析的准确性和可解释性，预测任务准确率高达50%。

Conclusion: 该方法为认知建模提供了可扩展、可解释的解决方案，适用于自适应学习、人机交互和教育分析。

Abstract: Eye-tracking data reveals valuable insights into users' cognitive states but
is difficult to analyze due to its structured, non-linguistic nature. While
large language models (LLMs) excel at reasoning over text, they struggle with
temporal and numerical data. This paper presents a multimodal human-AI
collaborative framework designed to enhance cognitive pattern extraction from
eye-tracking signals. The framework includes: (1) a multi-stage pipeline using
horizontal and vertical segmentation alongside LLM reasoning to uncover latent
gaze patterns; (2) an Expert-Model Co-Scoring Module that integrates expert
judgment with LLM output to generate trust scores for behavioral
interpretations; and (3) a hybrid anomaly detection module combining LSTM-based
temporal modeling with LLM-driven semantic analysis. Our results across several
LLMs and prompt strategies show improvements in consistency, interpretability,
and performance, with up to 50% accuracy in difficulty prediction tasks. This
approach offers a scalable, interpretable solution for cognitive modeling and
has broad potential in adaptive learning, human-computer interaction, and
educational analytics.

</details>


### [50] [Talking to...uh...um...Machines: The Impact of Disfluent Speech Agents on Partner Models and Perspective Taking](https://arxiv.org/abs/2507.18315)
*Rhys Jacka,Paola R. Peña,Sophie Leonard,Éva Székely,Benjamin R. Cowan*

Main category: cs.HC

TL;DR: 研究探讨了在人与机器对话中，语言不流利对感知和能力评价的影响，发现不流利的语音代理被认为更有能力，但未显著影响对话灵活性或拟人化评分。


<details>
  <summary>Details</summary>
Motivation: 探索语言不流利在人与机器对话中对感知和语言产生的影响，填补了该领域的研究空白。

Method: 61名参与者在在线Namer-Matcher任务中与语音代理互动，分流畅和不流利两组，前后填写伙伴建模问卷（PMQ）。

Result: 不流利的代理被认为更有能力，但对话灵活性和拟人化评分无显著差异；互动中表现出更多自我中心语言。

Conclusion: 语言不流利可能影响机器对话中的伙伴模型和语言产生，但其效果尚不完全明确，需进一步研究。

Abstract: Speech disfluencies play a role in perspective-taking and audience design in
human-human communication (HHC), but little is known about their impact in
human-machine dialogue (HMD). In an online Namer-Matcher task, sixty-one
participants interacted with a speech agent using either fluent or disfluent
speech. Participants completed a partner-modelling questionnaire (PMQ) both
before and after the task. Post-interaction evaluations indicated that
participants perceived the disfluent agent as more competent, despite no
significant differences in pre-task ratings. However, no notable differences
were observed in assessments of conversational flexibility or human-likeness.
Our findings also reveal evidence of egocentric and allocentric language
production when participants interact with speech agents. Interaction with
disfluent speech agents appears to increase egocentric communication in
comparison to fluent agents. Although the wide credibility intervals mean this
effect is not clear-cut. We discuss potential interpretations of this finding,
focusing on how disfluencies may impact partner models and language production
in HMD.

</details>


### [51] [PALM: PAnoramic Learning Map Integrating Learning Analytics and Curriculum Map for Scalable Insights Across Courses](https://arxiv.org/abs/2507.18393)
*Mahiro Ozaki,Li Chen,Shotaro Naganuma,Valdemar Švábenský,Fumiya Okubo,Atsushi Shimada*

Main category: cs.HC

TL;DR: PALM是一个学习分析仪表盘，通过整合课程级信息解决学习分析的可扩展性问题，提升学生对学习行为和学术进展的认知。


<details>
  <summary>Details</summary>
Motivation: 传统学习分析主要关注单一课程或学习者，缺乏对课程间关系和长期学习轨迹的考虑。PALM旨在填补这一空白，通过多层面教育数据的整合提升学习效果。

Method: PALM是一个仪表盘系统，集成多层级教育数据形成课程地图，帮助学生直观理解学习记录和学术进展。通过系统评估检验PALM在提升学习行为意识和系统性能方面的效果。

Result: PALM显著提高了学生对学习规划和反思的认知，尤其在行为控制感知方面表现突出，视觉吸引力和易用性评价高于现有系统。

Conclusion: PALM通过提供此前无法获取的洞察，增强了自主学习能力，代表了学习分析迈向全面和可扩展的重要一步。

Abstract: This study proposes and evaluates the PAnoramic Learning Map (PALM), a
learning analytics (LA) dashboard designed to address the scalability
challenges of LA by integrating curriculum-level information. Traditional LA
research has predominantly focused on individual courses or learners and often
lacks a framework that considers the relationships between courses and the
long-term trajectory of learning. To bridge this gap, PALM was developed to
integrate multilayered educational data into a curriculum map, enabling
learners to intuitively understand their learning records and academic
progression. We conducted a system evaluation to assess PALM's effectiveness in
two key areas: (1) its impact on students' awareness of their learning
behaviors, and (2) its comparative performance against existing systems. The
results indicate that PALM enhances learners' awareness of study planning and
reflection, particularly by improving perceived behavioral control through the
visual presentation of individual learning histories and statistical trends,
which clarify the links between learning actions and outcomes. Although PALM
requires ongoing refinement as a system, it received significantly higher
evaluations than existing systems in terms of visual appeal and usability. By
serving as an information resource with previously inaccessible insights, PALM
enhances self-regulated learning and engagement, representing a significant
step beyond conventional LA toward a comprehensive and scalable approach.

</details>


### [52] [Multisensory Integration and Sensory Substitution Across Vision, Audition, and Haptics: Answering the What, Which, and When in Study Protocols](https://arxiv.org/abs/2507.18401)
*Andrew Jeyathasan,Swati Banerjee*

Main category: cs.HC

TL;DR: 本文探讨多感官整合（MSI）的关键因素，并研究如何设计有效的MSI研究协议。


<details>
  <summary>Details</summary>
Motivation: 通过多感官整合研究，理解不同感官之间的相互作用及其对感知的影响，尤其在多模态（三种或以上）情境下的研究不足。

Method: 分析多感官整合中的关键因素，如跨模态对应、一致性、认知负荷和刺激时序，并提出研究设计建议。

Result: 指出多感官整合研究中复杂性的增加，并提供了设计有效研究协议的指导。

Conclusion: 多感官整合研究需综合考虑多种因素，未来的研究应扩展至更多模态的情境。

Abstract: We experience the world through multiple senses that work together to create
a cohesive perception, whether in daily life or immersive technologies.
Understanding this multisensory integration (MSI) requires examining the
interactions between sensory modalities, each with unique temporal dynamics and
characteristics. While most research focuses on unimodal or bimodal cues, the
integration of three or more modalities remains underexplored. MSI studies must
account for factors like cross-modal correspondence, congruence, cognitive
load, and stimulus timing, which become increasingly complex as modalities
multiply. This article examines these key factors and how they can be applied
to 8 design effective MSI study protocols.

</details>


### [53] [Towards Understanding Decision Problems As a Goal of Visualization Design](https://arxiv.org/abs/2507.18428)
*Lena Cibulski,Stefan Bruckner*

Main category: cs.HC

TL;DR: 本文提出了一种用于可视化研究中决策任务的表征方案，通过数据、用户和任务上下文的关键属性来描述决策问题。


<details>
  <summary>Details</summary>
Motivation: 现有任务模型在处理决策过程时往往忽略了决策的背景条件，不足以支持决策任务。

Method: 提出一种表征方案，通过数据、用户和任务上下文的关键属性来描述决策问题。

Result: 该方案帮助研究者更精确地定义决策支持目标，并为合适的视觉编码和交互设计提供依据。

Conclusion: 本文的方法为决策中心的可视化研究提供了新方向，展示了未来研究的潜力。

Abstract: Decision-making is a central yet under-defined goal in visualization
research. While existing task models address decision processes, they often
neglect the conditions framing a decision. To better support decision-making
tasks, we propose a characterization scheme that describes decision problems
through key properties of the data, users, and task context. This scheme helps
visualization researchers specify decision-support claims more precisely and
informs the design of appropriate visual encodings and interactions. We
demonstrate the utility of our approach by applying it to characterize decision
tasks targeted by existing design studies, highlighting opportunities for
future research in decision-centric visualization.

</details>


### [54] [High-Dimensional Data Classification in Concentric Coordinates](https://arxiv.org/abs/2507.18450)
*Alice Williams,Boris Kovalerchuk*

Main category: cs.HC

TL;DR: 提出了一种支持低维到高维数据的无损同心坐标框架，解决了高维数据可视化中的遮挡和计算复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 解决高维数据可视化中存在的遮挡和计算复杂度问题，提供更紧凑且无损的解决方案。

Method: 使用同心坐标（Concentric Coordinates）作为平行坐标和圆形坐标的推广，支持机器学习和人机交互。

Result: 提出的框架能够无损可视化高维数据，并支持机器学习算法的可视化。

Conclusion: 同心坐标框架在高维数据可视化中表现优异，为机器学习和交互提供了新的可能性。

Abstract: The visualization of multi-dimensional data with interpretable methods
remains limited by capabilities for both high-dimensional lossless
visualizations that do not suffer from occlusion and that are computationally
capable by parameterized visualization. This paper proposes a low to high
dimensional data supporting framework using lossless Concentric Coordinates
that are a more compact generalization of Parallel Coordinates along with
former Circular Coordinates. These are forms of the General Line Coordinate
visualizations that can directly support machine learning algorithm
visualization and facilitate human interaction.

</details>


### [55] [ForcePinch: Force-Responsive Spatial Interaction for Tracking Speed Control in XR](https://arxiv.org/abs/2507.18510)
*Chenyang Zhang,Tiffany S Ma,John Andrews,Eric J Gonzalez,Mar Gonzalez-Franco,Yalong Yang*

Main category: cs.HC

TL;DR: 论文提出了一种名为ForcePinch的新型力响应空间交互方法，通过捏压力的变化来动态调整指针跟踪速度，提升3D环境中的交互灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有技术在调整跟踪速度时通常与手势直接耦合，限制了交互的灵活性，而自然界中的摩擦力控制为此提供了灵感。

Method: 开发了硬件原型，集成压力传感器和自定义映射函数，将捏压力转换为跟踪速度调整。通过用户研究比较了ForcePinch与Go-Go和PRISM技术。

Result: 研究表明，力响应方法在不同交互场景中展现出独特优势。

Conclusion: 论文通过四个示例展示了力响应交互的多样性和适用性，为未来空间交互设计提供了启示。

Abstract: Spatial interaction in 3D environments requires balancing efficiency and
precision, which requires dynamic tracking speed adjustments. However, existing
techniques often couple tracking speed adjustments directly with hand
movements, reducing interaction flexibility. Inspired by the natural friction
control inherent in the physical world, we introduce ForcePinch, a novel
force-responsive spatial interaction method that enables users to intuitively
modulate pointer tracking speed and smoothly transition between rapid and
precise movements by varying their pinching force. To implement this concept,
we developed a hardware prototype integrating a pressure sensor with a
customizable mapping function that translates pinching force into tracking
speed adjustments. We conducted a user study with 20 participants performing
well-established 1D, 2D, and 3D object manipulation tasks, comparing ForcePinch
against the distance-responsive technique Go-Go and speed-responsive technique
PRISM. Results highlight distinctive characteristics of the force-responsive
approach across different interaction contexts. Drawing on these findings, we
highlight the contextual meaning and versatility of force-responsive
interactions through four illustrative examples, aiming to inform and inspire
future spatial interaction design.

</details>


### [56] [PosterMate: Audience-driven Collaborative Persona Agents for Poster Design](https://arxiv.org/abs/2507.18572)
*Donghoon Shin,Daniel Lee,Gary Hsieh,Gromit Yeuk-Yin Chan*

Main category: cs.HC

TL;DR: PosterMate是一种海报设计助手，通过模拟多样化的用户角色代理来提供反馈，促进合作设计。


<details>
  <summary>Details</summary>
Motivation: 海报设计需要多样化的用户反馈，但实际收集和整合这些反馈具有挑战性。生成式AI可以模拟人类互动，但其在设计反馈中的应用尚不明确。

Method: PosterMate通过营销文档构建用户角色代理，收集每个代理的反馈，并通过调解员促成讨论，达成一致意见后直接应用到设计中。

Result: 用户研究（N=12）显示PosterMate能捕捉被忽视的观点，并作为有效的原型工具；在线评估（N=100）证实反馈符合角色身份且能有效综合不同观点。

Conclusion: PosterMate通过AI模拟多样化用户反馈，为海报设计提供了一种高效且多元的合作工具。

Abstract: Poster designing can benefit from synchronous feedback from target audiences.
However, gathering audiences with diverse perspectives and reconciling them on
design edits can be challenging. Recent generative AI models present
opportunities to simulate human-like interactions, but it is unclear how they
may be used for feedback processes in design. We introduce PosterMate, a poster
design assistant that facilitates collaboration by creating audience-driven
persona agents constructed from marketing documents. PosterMate gathers
feedback from each persona agent regarding poster components, and stimulates
discussion with the help of a moderator to reach a conclusion. These
agreed-upon edits can then be directly integrated into the poster design.
Through our user study (N=12), we identified the potential of PosterMate to
capture overlooked viewpoints, while serving as an effective prototyping tool.
Additionally, our controlled online evaluation (N=100) revealed that the
feedback from an individual persona agent is appropriate given its persona
identity, and the discussion effectively synthesizes the different persona
agents' perspectives.

</details>


### [57] [MeloKids: Multisensory VR System to Enhance Speech and Motor Coordination in Children with Hearing Loss](https://arxiv.org/abs/2507.18619)
*Yichen Yu,Qiaoran Wang*

Main category: cs.HC

TL;DR: 本研究探讨了基于VR的多感官反馈技术如何提升听力障碍儿童的语言和运动康复效果，结合fNIRS技术评估了不同互动模式下的大脑皮层激活模式，旨在为个性化康复系统设计提供依据。


<details>
  <summary>Details</summary>
Motivation: 听力障碍儿童在语言和运动发展方面面临持续挑战，需探索更有效的康复方法。

Method: 结合VR的多感官反馈技术和fNIRS，评估儿童在不同互动模式下的皮层激活。

Result: 研究结果支持多感官反馈技术对康复效果的积极影响。

Conclusion: 多感官反馈技术有望为听力障碍儿童提供更有效的个性化康复方案。

Abstract: Children with hearing impairments face ongoing challenges in language and
motor development. This study explores how multi-sensory feedback technology
based on virtual reality (VR), integrating auditory, visual, and tactile
stimuli, can enhance rehabilitation outcomes. Using functional near-infrared
spectroscopy (fNIRS) technology, we assessed cortical activation patterns in
children during pitch-matching tasks across different interaction modes. Our
findings aim to provide evidence for designing personalized, interactive
rehabilitation systems that enhance cognitive engagement and motor control in
children with hearing impairments.

</details>


### [58] [Evaluation of a Provenance Management Tool for Immersive Virtual Fieldwork](https://arxiv.org/abs/2507.18622)
*Armin Bernstetter,Tom Kwasnitschka,Isabella Peters*

Main category: cs.HC

TL;DR: 研究提出了一个用于记录科研工作流程的“数字实验书”（DLB）工具，支持沉浸式和非沉浸式设置，并评估其可用性和实用性。


<details>
  <summary>Details</summary>
Motivation: 确保科研的可重复性是良好科学实践的重要组成部分，特别是在涉及交互式可视化工具的学科中，记录工作流程（溯源）尤为重要。

Method: 通过用户研究评估了一个溯源管理工具（DLB），记录用户与虚拟田野工具的交互，并比较沉浸式与非沉浸式设置下的使用体验。

Result: 用户普遍认为DLB既实用又易用，沉浸式设置在易用性方面评分更高，但两组的使用模式无显著差异。

Conclusion: DLB是一种有效的溯源工具，沉浸式设置可能提升易用性，但仍需进一步研究其适用性。

Abstract: Ensuring reproducibility of research is an integral part of good scientific
practice. One way to support this is through provenance: information about
research workflows from data gathering to researchers' sensemaking processes
leading to published results. This is highly important in disciplines such as
geosciences, where researchers use software for interactive and immersive
visualizations of geospatial data, doing virtual measurements in simulated
fieldwork on 3D models. We evaluated a provenance management tool, which allows
recording of interactions with a virtual fieldwork tool and annotating
different states of the visualization. The user study investigated how
researchers used this Digital Lab Book (DLB) and whether perceived ease of use
and perceived usefulness differed between groups in immersive or non-immersive
settings. Participants perceived the DLB as both useful and easy to use. While
there were indications of differences in perceived ease of use (higher for
immersive setting), usage patterns showed no significant group differences.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [59] [Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA](https://arxiv.org/abs/2507.17963)
*Rameen Abdal,Or Patashnik,Ekaterina Deyneka,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov,Daniel Cohen-Or,Kfir Aberman*

Main category: cs.GR

TL;DR: 论文提出了一种零样本框架，用于文本到视频模型中动态概念的个性化，无需逐实例微调，提高了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要逐实例微调，限制了动态概念个性化的可扩展性。

Method: 使用结构化2x2视频网格和Grid-LoRA适配器，结合Grid Fill模块完成部分观察的布局。

Result: 实验表明，该方法在未经优化的动态概念和编辑场景中，能生成高质量且一致的视频。

Conclusion: 提出的框架在零样本下实现了动态概念的个性化，具有广泛适用性和高效性。

Abstract: Recent advances in text-to-video generation have enabled high-quality
synthesis from text and image prompts. While the personalization of dynamic
concepts, which capture subject-specific appearance and motion from a single
video, is now feasible, most existing methods require per-instance fine-tuning,
limiting scalability. We introduce a fully zero-shot framework for dynamic
concept personalization in text-to-video models. Our method leverages
structured 2x2 video grids that spatially organize input and output pairs,
enabling the training of lightweight Grid-LoRA adapters for editing and
composition within these grids. At inference, a dedicated Grid Fill module
completes partially observed layouts, producing temporally coherent and
identity preserving outputs. Once trained, the entire system operates in a
single forward pass, generalizing to previously unseen dynamic concepts without
any test-time optimization. Extensive experiments demonstrate high-quality and
consistent results across a wide range of subjects beyond trained concepts and
editing scenarios.

</details>


### [60] [DanceGraph: A Complementary Architecture for Synchronous Dancing Online](https://arxiv.org/abs/2507.18052)
*David Sinclair,Ademyemi Ademola,Babis Koniaris,Kenny Mitchell*

Main category: cs.GR

TL;DR: DanceGraph是一个用于解决在线舞蹈同步中网络延迟问题的架构，通过实时高效带宽设计减少延迟，并结合参数化风格化方法提升舞蹈与音乐节奏的同步。


<details>
  <summary>Details</summary>
Motivation: 解决在线舞蹈中因网络延迟导致的同步问题，提升舞蹈与音乐节奏的一致性。

Method: 采用实时带宽高效的架构减少延迟，并结合参数化风格化的舞蹈修正方法。

Result: 成功减少了延迟，并实现了舞蹈与音乐节奏的同步。

Conclusion: DanceGraph为在线舞蹈同步提供了一种高效的解决方案，具有潜在的应用价值。

Abstract: DanceGraph is an architecture for synchronized online dancing overcoming the
latency of networked body pose sharing. We break down this challenge by
developing a real-time bandwidth-efficient architecture to minimize lag and
reduce the timeframe of required motion prediction for synchronization with the
music's rhythm. In addition, we show an interactive method for the
parameterized stylization of dance motions for rhythmic dance using online
dance correctives.

</details>


### [61] [Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation](https://arxiv.org/abs/2507.18352)
*Zhen Han,Mattias Teye,Derek Yadgaroff,Judith Bütepage*

Main category: cs.GR

TL;DR: 论文提出了一种轻量化的实时面部动画模型，适用于游戏开发，通过知识蒸馏和伪标注技术解决了数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前高精度面部动画模型对大规模高质量数据依赖性强，且模型体积庞大，难以在设备端实时运行。需要开发轻量化、实时性强的模型。

Method: 采用混合知识蒸馏和伪标注技术，利用大规模音频数据集训练小型学生模型，仅包含卷积和全连接层，避免注意力机制和循环更新。

Result: 模型内存占用降至3.4 MB，所需音频上下文时间缩短至81 ms，同时保持高质量动画效果。

Conclusion: 该方法为设备端实时推理提供了可行方案，推动模型驱动的数字角色发展。

Abstract: The training of high-quality, robust machine learning models for
speech-driven 3D facial animation requires a large, diverse dataset of
high-quality audio-animation pairs. To overcome the lack of such a dataset,
recent work has introduced large pre-trained speech encoders that are robust to
variations in the input audio and, therefore, enable the facial animation model
to generalize across speakers, audio quality, and languages. However, the
resulting facial animation models are prohibitively large and lend themselves
only to offline inference on a dedicated machine. In this work, we explore
on-device, real-time facial animation models in the context of game
development. We overcome the lack of large datasets by using hybrid knowledge
distillation with pseudo-labeling. Given a large audio dataset, we employ a
high-performing teacher model to train very small student models. In contrast
to the pre-trained speech encoders, our student models only consist of
convolutional and fully-connected layers, removing the need for attention
context or recurrent updates. In our experiments, we demonstrate that we can
reduce the memory footprint to up to 3.4 MB and required future audio context
to up to 81 ms while maintaining high-quality animations. This paves the way
for on-device inference, an important step towards realistic, model-driven
digital characters.

</details>


### [62] [GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar](https://arxiv.org/abs/2507.18155)
*SeungJun Moon,Hah Min Lew,Seungeun Lee,Ji-Su Kang,Gyeong-Moon Park*

Main category: cs.GR

TL;DR: GeoAvatar提出了一种自适应几何高斯泼溅框架,通过APS分割高斯点集并引入嘴部结构和正则化损失,提升了3D头部重建与动画质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D头部化身生成方法在身份保存与新颖姿势/表情动画之间存在平衡挑战,几何适应性不足。

Method: 采用Adaptive Pre-allocation Stage(APS)分割高斯点集,设计嘴部结构与分块变形策略,并引入正则化损失。

Result: 实验表明,GeoAvatar在重建和动画场景中优于现有方法。

Conclusion: GeoAvatar通过自适应几何处理和嘴部动态优化,显著提升了3D头部化身的生成质量。

Abstract: Despite recent progress in 3D head avatar generation, balancing identity
preservation, i.e., reconstruction, with novel poses and expressions, i.e.,
animation, remains a challenge. Existing methods struggle to adapt Gaussians to
varying geometrical deviations across facial regions, resulting in suboptimal
quality. To address this, we propose GeoAvatar, a framework for adaptive
geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation
Stage (APS), an unsupervised method that segments Gaussians into rigid and
flexible sets for adaptive offset regularization. Then, based on mouth anatomy
and dynamics, we introduce a novel mouth structure and the part-wise
deformation strategy to enhance the animation fidelity of the mouth. Finally,
we propose a regularization loss for precise rigging between Gaussians and 3DMM
faces. Moreover, we release DynamicFace, a video dataset with highly expressive
facial motions. Extensive experiments show the superiority of GeoAvatar
compared to state-of-the-art methods in reconstruction and novel animation
scenarios.

</details>


### [63] [PS-GS: Gaussian Splatting for Multi-View Photometric Stereo](https://arxiv.org/abs/2507.18231)
*Yixiao Chen,Bin Liang,Hanzhi Guo,Yongqing Cheng,Jiayi Zhao,Dongdong Weng*

Main category: cs.GR

TL;DR: 该论文提出了一种结合高斯溅射和多视角光度立体（PS-GS）的高效逆渲染方法，显著提升了3D重建的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的逆渲染方法依赖于固定的环境光照，导致3D重建精度不足。为解决这一问题，作者提出了一种结合多视角和多光源的方法，以更准确地估计物体的几何、材质和光照。

Method: 论文首先通过标准2D高斯溅射模型初始化几何形状，随后基于全渲染方程进行逆渲染，并通过未标定的光度立体法估计的法线对渲染的法线图进行正则化。此外，还提出了2D高斯光线追踪方法以优化入射光照。

Result: 实验表明，该方法在合成和真实数据集上均优于现有方法，实现了更高的重建精度和计算效率。

Conclusion: PS-GS方法通过结合多视角和多光源图像，有效解决了逆渲染的病态问题，并为新视角合成、重新照明以及材质和形状编辑提供了实用工具。

Abstract: Integrating inverse rendering with multi-view photometric stereo (MVPS)
yields more accurate 3D reconstructions than the inverse rendering approaches
that rely on fixed environment illumination. However, efficient inverse
rendering with MVPS remains challenging. To fill this gap, we introduce the
Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently
and jointly estimates the geometry, materials, and lighting of the object that
is illuminated by diverse directional lights (multi-light). Our method first
reconstructs a standard 2D Gaussian splatting model as the initial geometry.
Based on the initialization model, it then proceeds with the deferred inverse
rendering by the full rendering equation containing a lighting-computing
multi-layer perceptron. During the whole optimization, we regularize the
rendered normal maps by the uncalibrated photometric stereo estimated normals.
We also propose the 2D Gaussian ray-tracing for single directional light to
refine the incident lighting. The regularizations and the use of multi-view and
multi-light images mitigate the ill-posed problem of inverse rendering. After
optimization, the reconstructed object can be used for novel-view synthesis,
relighting, and material and shape editing. Experiments on both synthetic and
real datasets demonstrate that our method outperforms prior works in terms of
reconstruction accuracy and computational efficiency.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [64] [Low-power switching of memristors exhibiting fractional-order dynamics](https://arxiv.org/abs/2507.18487)
*Nathan Astin,Yuriy V. Pershin*

Main category: cs.ET

TL;DR: 研究了基于分数阶行为的忆阻器件切换策略，揭示最佳切换方式取决于分数阶导数的阶数与运动方程中的幂指数关系。


<details>
  <summary>Details</summary>
Motivation: 为下一代节能神经形态计算架构提供理论基础，更接近生物模拟。

Method: 采用Caputo型分数阶微分方程建模，分析焦耳损耗与切换策略的关系。

Result: 发现分数阶导数阶数超过幂指数一半时，宽脉冲最佳；否则零电流后窄脉冲更优。

Conclusion: 为多脉冲控制下的忆阻器件应用提供了指导，推动高效神经形态计算发展。

Abstract: In this conference contribution, we present some initial results on switching
memristive devices exhibiting fractional-order behavior using current pulses.
In our model, it is assumed that the evolution of a state variable follows a
fractional-order differential equation involving a Caputo-type derivative. A
study of Joule losses demonstrates that the best switching strategy minimizing
these losses depends on the fractional derivative's order and the power
exponent in the equation of motion. It is found that when the order of the
fractional derivative exceeds half of the power exponent, the best approach is
to employ a wide pulse. Conversely, when this condition is not met, Joule
losses are minimized by applying a zero current followed by a narrow current
pulse of the highest allowable amplitude. These findings are explored further
in the context of multi-pulse control. Our research lays the foundation for the
advancement of the next generation of energy-efficient neuromorphic computing
architectures that more closely mimic their biological counterparts.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [65] [Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release](https://arxiv.org/abs/2507.17766)
*Felix Quinque,Alan Aboudib,Szymon Fonau,Rodrigo Lopez Portillo Alcocer,Brian McCrindle,Steffen Cruz*

Main category: cs.DC

TL;DR: Bittensor的SN9展示了分布式网络可以预训练大型语言模型，但存在模型本地化和奖励机制问题。IOTA架构通过协同合作、公平奖励和高效通信解决了这些问题。


<details>
  <summary>Details</summary>
Motivation: 解决SN9中每个矿工需本地存储整个模型和奖励机制导致模型囤积的问题，通过协同训练和公平激励实现高效、可扩展的分布式训练。

Method: 采用数据与管道并行的SWARM架构、连续粒度激励、激活压缩、Butterfly All-Reduce参数聚合和CLASP贡献评估方案。

Result: 实现了模型规模随参与者数量扩展、带宽减少128倍、线性扩展性和冗余，以及公平的贡献分配与漏洞检测。

Conclusion: IOTA提供了一种高效、公平且可扩展的分布式训练架构，解决了SN9的局限性。

Abstract: In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed
network of incentivized, permissionless actors could each pretrain large
language models (LLMs) ranging from 700 million to 14 billion parameters, while
surpassing established baselines. While that work validated blockchain-based
decentralized pretraining as viable, it contained core issues: (i) every miner
had to fit an entire model locally, and (ii) "winner-takes-all" rewards
encouraged model hoarding.
  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an
architecture that addresses these limitations by transforming SN9's previously
isolated competitors into a single cooperating unit that can scale arbitrarily
while still rewarding each contributor fairly.
  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -
An orchestrator distributes model layers across heterogeneous miners and
streams activations between them, enabling model sizes to scale with the number
of participants rather than being constrained by the VRAM of a single machine;
(2) Granular, continuous incentives - Validators measure each miner's
contribution and allocate token emissions proportionally; (3) Activation
compression - We used model-bottlenecks to cut communication bandwidths of
activations by up to 128x, vastly improving training speed; (4) Butterfly
All-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,
offering linear scalability, redundancy and built-in collusion detection; (5)
CLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair
attribution scheme assigns credit to miners proportional to their marginal
utility and detects exploits, even when contributions are interdependent across
the pipeline.

</details>


### [66] [PolyServe: Efficient Multi-SLO Serving at Scale](https://arxiv.org/abs/2507.17769)
*Kan Zhu,Haiyang Shi,Le Xu,Jiaxin Shan,Arvind Krishnamurthy,Baris Kasikci,Liguang Xie*

Main category: cs.DC

TL;DR: 论文提出了PolyServe，一种针对多服务等级协议（SLO）的大规模调度策略，旨在优化LLM服务的延迟要求和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有系统将工作负载简单分类为延迟敏感（LS）或尽力而为（BE），无法满足多SLO需求的复杂场景，导致用户体验和调度机会的优化不足。

Method: PolyServe通过基于每个令牌的延迟要求将请求分组，将每个组调度到服务器子集，并利用负载梯度和资源共享策略优化调度。

Result: 与现有策略相比，PolyServe实现了1.23倍的吞吐量增益，达到最优吞吐量的92.5%。

Conclusion: PolyServe通过高效调度和多SLO管理，显著提升了LLM应用的性能和服务质量。

Abstract: Advances in Large Language Models (LLMs) have led to a surge of LLM-powered
applications. These applications have diverse token-generation latency
requirements. As a result, simply classifying workloads as latency-sensitive
(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive
category and results in suboptimal user experiences and scheduling
opportunities. However, efficiently serving requests with multiple SLO
requirements poses significant challenges. First, all requests within a batch
generate new tokens simultaneously, which can misalign them with their distinct
SLO requirements. Moreover, while existing systems focus on auto-scaling for
handling various overall request rates, the diversity of SLOs necessitates
fine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE
scenarios, where BE requests can be aborted at any time to ensure the SLO
attainment of LS requests, those with different latency-sensitive SLOs cannot
tolerate prolonged delays, and tail latency must be controlled.
  To tackle these challenges, we propose PolyServe, a novel multi-SLO
scheduling policy at scale that maintains high SLO attainment while maximizing
throughput. PolyServe first groups requests into multiple bins based on their
per-token latency requirement, then schedules each bin to a subset of the
server fleet. PolyServe routes requests to the highest-load but still
SLO-attainable server to create a load gradient that facilitates auto-scaling.
To increase utilization, PolyServe permits looser-SLO requests to share
tighter-SLO instances when their own servers are saturated. PolyServe uses
profiling data to guide scheduling decisions and manage tail latency through
request-wait-time-aware scheduling, dynamic chunking, and continuous chunked
prefill prediction. PolyServe achieves 1.23x goodput gain compared to existing
policies, achieving up to 92.5% of optimal goodput.

</details>


### [67] [Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale](https://arxiv.org/abs/2507.17770)
*Pei-Kun Yang*

Main category: cs.DC

TL;DR: 该研究对五种基于软件的QUBO求解器进行了基准测试，比较了它们在随机生成的QUBO矩阵上的性能，重点关注解的质量和计算时间。结果显示，不同求解器在解质量、可扩展性和运行时间之间各有优缺点，PyTorch在大型问题上表现最为均衡。


<details>
  <summary>Details</summary>
Motivation: 比较不同软件QUBO求解器在大规模问题上的性能表现，为研究者提供选择依据。

Method: 使用随机生成的QUBO矩阵（1000x1000到45000x45000）和六种收敛阈值（10^-1到10^-6），测试Neal、PyTorch（CPU/GPU）、JAX和SciPy的解质量和计算时间。

Result: Neal解质量最优但仅适用于小规模问题；PyTorch解质量略差但可扩展性强，适合大规模问题；JAX表现接近PyTorch但规模受限；SciPy表现最差。

Conclusion: PyTorch是大规模QUBO问题的均衡选择，具有较好的可扩展性和计算效率。

Abstract: Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework
for modeling combinatorial optimization problems. This study benchmarks five
software-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and
SciPy, on randomly generated QUBO matrices ranging from 1000x1000 to
45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate
their performance in terms of solution quality (energy) and computational time.
Among the solvers tested, Neal achieved the lowest energy values but was
limited to problems with up to 6000 variables due to high memory consumption.
PyTorch produced slightly higher energy results than Neal but demonstrated
superior scalability, solving instances with up to 45000 variables. Its support
for GPU acceleration and CPU multi-threading also resulted in significantly
shorter runtimes. JAX yielded energy values slightly above those of PyTorch and
was limited to 25000 variables, with runtimes comparable to PyTorch on GPU.
SciPy was the most constrained solver, handling only up to 6000 variables and
consistently producing the highest energy values with the longest computation
times. These findings highlight trade-offs between solution quality,
scalability, and runtime efficiency, and suggest that PyTorch is the most
balanced choice for large-scale QUBO problems when computational resources
permit.

</details>


### [68] [Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN Inference Acceleration](https://arxiv.org/abs/2507.17771)
*Dmitri Lyalikov*

Main category: cs.DC

TL;DR: 该论文指出深度学习中异构架构的潜力，并提出高效的编译/执行模型以解决资源受限平台上的CNN部署问题，重点关注硬件集成和预处理瓶颈。


<details>
  <summary>Details</summary>
Motivation: 研究异构架构和定制硬件在深度学习中的应用，以克服传统硅缩放限制，解决嵌入式SoC中的性能/功耗平衡问题。

Method: 利用RISC-V Vector 1.0扩展，提出一种灵活的编程模型和缓存层次方案，优化CNN执行的预处理和CPU回退过程。

Result: 实验显示预处理速度提升9倍，YOLOv3回退层执行速度提升3倍，同时功耗低于传统并行平台。

Conclusion: RVV-1.0为异构嵌入式SoC提供了一种高效的编程模型，显著提升了计算与内存效率，优化了深度学习数据流。

Abstract: The emergence of heterogeneity and domain-specific architectures targeting
deep learning inference show great potential for enabling the deployment of
modern CNNs on resource-constrained embedded platforms. A significant
development is the diversification of custom hardware solely targeting the most
expensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural
processing units), among others, can overcome the approaching limits of
traditional silicon scaling and provide a solution to the power/performance
tradeoff within embedded SoCs. Efficient DSA utilization requires proper system
integration and a compilation/execution model for balanced execution in these
heterogeneous architectures. There is a critical need for proper system
integration and an efficient compilation/execution model for balanced execution
in these heterogeneous architectures. This work highlights the hardware
integration challenges for efficiently placing these units within the memory
hierarchy and correct proximity to other execution blocks. We experimentally
verify performance bottlenecks in CNN execution and pre/post-processing at
runtime, where previous attention has generally been given to accelerator
speedup alone. This work takes advantage of the ratification of the RISC-V
Vector 1.0 extension and demonstrates its potential as a flexible target within
a well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and
CPU fallback processes. Our results show up to a 9x speedup of image
pre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.
We demonstrate RVV-1.0 in exposing a flexible programming model that can enable
a balanced computation and memory footprint on accelerator-rich embedded SoCs
supporting modern deep-learning dataflows while consuming less power than
traditional parallel execution platforms.

</details>


### [69] [Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments](https://arxiv.org/abs/2507.17772)
*Ahmad Alhonainy,Praveen Rao*

Main category: cs.DC

TL;DR: 论文提出了一种基于缓存的策略（FIFO、LRU和优先级）来减少联邦学习中不必要的模型更新传输，从而降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在资源受限环境下通信成本高的问题，通过智能缓存优化模型更新传输。

Method: 采用FIFO、LRU和基于优先级的缓存策略，选择性转发重要更新。

Result: 在CIFAR-10和医疗数据集上的实验表明，通信量减少且模型准确性几乎不受影响。

Conclusion: 智能缓存提高了联邦学习的可扩展性和内存效率，适用于边缘物联网网络中的智能城市和医疗等延迟敏感应用。

Abstract: Federated Learning (FL) allows multiple distributed devices to jointly train
a shared model without centralizing data, but communication cost remains a
major bottleneck, especially in resource-constrained environments. This paper
introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce
unnecessary model update transmissions. By selectively forwarding significant
updates, our approach lowers bandwidth usage while maintaining model accuracy.
Experiments on CIFAR-10 and medical datasets show reduced communication with
minimal accuracy loss. Results confirm that intelligent caching improves
scalability, memory efficiency, and supports reliable FL in edge IoT networks,
making it practical for deployment in smart cities, healthcare, and other
latency-sensitive applications.

</details>


### [70] [MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation](https://arxiv.org/abs/2507.17773)
*Zhongzhen Wen,Yinghui Zhang,Zhong Li,Zhongxin Liu,Linna Xie,Tian Zhang*

Main category: cs.DC

TL;DR: MultiKernelBench是一个全面的基准测试，用于评估LLM在生成深度学习核的能力，支持多平台并提供模块化设计。


<details>
  <summary>Details</summary>
Motivation: 解决现有基准测试在硬件支持、任务覆盖和分类方面的不足。

Method: 设计了多平台模块化基准测试，并提出类别感知的单次提示方法。

Result: 揭示了任务难度差异大，对新硬件平台泛化能力差，提示策略有效。

Conclusion: MultiKernelBench为评估LLM生成DL核提供了可靠工具，未来可扩展性强。

Abstract: The automatic generation of deep learning (DL) kernels using large language
models (LLMs) has emerged as a promising approach to reduce the manual effort
and hardware-specific expertise required for writing high-performance operator
implementations. However, existing benchmarks for evaluating LLMs in this
domain suffer from limited hardware support, coarse-grained kernel
categorization, and imbalanced task coverage. To address these limitations, we
introduce MultiKernelBench, the first comprehensive, multi-platform benchmark
for LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14
well-defined kernel categories and supports three major hardware platforms:
Nvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we
design a modular backend abstraction layer that decouples platform-specific
logic from the core benchmarking infrastructure, allowing easy integration of
new hardware platforms. We further propose a simple yet effective
category-aware one-shot prompting method that improves generation quality by
providing in-category exemplars. Through systematic evaluations of seven
state-of-the-art LLMs, we reveal significant variation in task difficulty, poor
generalization to platforms with less training exposure, and the effectiveness
of targeted prompting strategies. MultiKernelBench is publicly available at
https://github.com/wzzll123/MultiKernelBench.

</details>


### [71] [CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks](https://arxiv.org/abs/2507.17793)
*Joel Brogan,Matthew Yohe,David Cornett*

Main category: cs.DC

TL;DR: CHAMP是一个模块化边缘计算平台，支持动态更换AI‘能力卡’，用于人脸识别、物体跟踪等任务，采用低功耗FPGA加速器和专用操作系统（VDiSK），实现即插即用的AI流程和安全数据存储。


<details>
  <summary>Details</summary>
Motivation: 为现场操作员提供灵活、高性能的边缘AI系统，能够快速适应不同任务需求。

Method: 设计了CHAMP平台，包括模块化架构、FPGA加速器、VDiSK操作系统，支持动态配置和加密数据存储。

Result: 实验展示了从1到5个神经计算加速器的近线性吞吐扩展，并揭示了性能增益和总线限制。

Conclusion: CHAMP在生物识别、监控和灾害响应中有广泛应用前景，未来可改进总线协议和系统软件。

Abstract: What if you could piece together your own custom biometrics and AI analysis
system, a bit like LEGO blocks? We aim to bring that technology to field
operators in the field who require flexible, high-performance edge AI system
that can be adapted on a moment's notice. This paper introduces CHAMP
(Configurable Hot-swappable Architecture for Machine Perception), a modular
edge computing platform that allows operators to dynamically swap in
specialized AI "capability cartridges" for tasks like face recognition, object
tracking, and document analysis. CHAMP leverages low-power FPGA-based
accelerators on a high-throughput bus, orchestrated by a custom operating
system (VDiSK) to enable plug-and-play AI pipelines and cryptographically
secured biometric datasets. In this paper we describe the CHAMP design,
including its modular scaling with multiple accelerators and the VDiSK
operating system for runtime reconfiguration, along with its cryptographic
capabilities to keep data stored on modules safe and private. Experiments
demonstrate near-linear throughput scaling from 1 to 5 neural compute
accelerators, highlighting both the performance gains and saturation limits of
the USB3-based bus. Finally, we discuss applications of CHAMP in field
biometrics, surveillance, and disaster response, and outline future
improvements in bus protocols, cartridge capabilities, and system software.

</details>


### [72] [Optimizing Edge Gaming Slices through an Enhanced User Plane Function and Analytics in Beyond-5G Networks](https://arxiv.org/abs/2507.17843)
*Bruno Marques da Silva,Larissa Ferreira Rodrigues Moreira,Flávio de Oliveira Silva,Rodrigo Moreira*

Main category: cs.DC

TL;DR: 论文提出了一种闭环架构，集成了NWDAF和UPF以估算用户延迟并增强5G控制平面的延迟感知能力，AI模型嵌入NWDAF可实现游戏分类。


<details>
  <summary>Details</summary>
Motivation: 当前边缘游戏技术在吞吐量和延迟方面表现优异，但仍需改进核心功能（如UPF）以实现非侵入式用户延迟测量，从而更好地管理服务并遵守SLA。

Method: 通过整合NWDAF和UPF，提出了一种闭环架构，利用AI模型估算用户延迟并增强5G控制平面的延迟感知能力。

Result: 结果显示，在NWDAF中嵌入AI模型能有效分类游戏，并为移动边缘游戏研究开辟了新方向。

Conclusion: 该闭环架构为5G网络中的延迟管理和游戏分类提供了创新解决方案，具有实际应用潜力。

Abstract: The latest generation of games and pervasive communication technologies poses
challenges in service management and Service-Level Agreement compliance for
mobile users. State-of-the-art edge-gaming techniques enhance throughput,
reduce latency, and leverage cloud computing. However, further development of
core functions such as the User Plane Function (UPF) is needed for
non-intrusive user latency measurement. This paper proposes a closed-loop
architecture integrating the Network Data Analytics Function (NWDAF) and UPF to
estimate user latency and enhance the 5G control plane by making it
latency-aware. The results show that embedding an artificial intelligence model
within NWDAF enables game classification and opens new avenues for mobile edge
gaming research.

</details>


### [73] [PowerTrip: Exploiting Federated Heterogeneous Datacenter Power for Distributed ML Training](https://arxiv.org/abs/2507.17904)
*Talha Mehboob,Luanzheng Guo,Nathan Tallent,Michael Zink,David Irwin*

Main category: cs.DC

TL;DR: 论文提出PowerTrip系统，动态选择站点优化电力-通信权衡，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的计算和电力需求超过单一数据中心能力，需分布式训练，但网络延迟和电力异构性带来挑战。

Method: PowerTrip基于电力-成本启发式动态选择站点，采用贪婪算法优化训练效率。

Result: 实验表明，PowerTrip相比基线策略可将训练时间减少50%。

Conclusion: PowerTrip有效解决了电力受限、地理分布式环境中的模型训练问题。

Abstract: The exponential growth of large-scale AI models has led to computational and
power demands that can exceed the capacity of a single data center. This is due
to the limited power supplied by regional grids that leads to limited regional
computational power. Consequently, distributing training workloads across
geographically distributed sites has become essential. However, this approach
introduces a significant challenge in the form of communication overhead,
creating a fundamental trade-off between the performance gains from accessing
greater aggregate power and the performance losses from increased network
latency. Although prior work has focused on reducing communication volume or
using heuristics for distribution, these methods assume constant homogeneous
power supplies and ignore the challenge of heterogeneous power availability
between sites.
  To address the challenge of training large models in power-constrained,
geo-distributed environments, we introduce PowerTrip, a system that dynamically
selects a subset of sites during runtime to optimize the power-communication
trade-off. Specifically, PowerTrip selects sites based on a power-to-cost
heuristic, prioritizing those with high power availability and low network
latency. PowerTrip employs a dynamic greedy approach and uses the marginal gain
in training efficiency, i.e., accuracy improvement per unit of time, to
optimize for the number of sites where the performance penalty from network
overhead negates the benefit of adding more computational power. Our
evaluation, which uses real-world Google power traces to model realistic power
capacity constraints, demonstrates that PowerTrip can reduce time-to-accuracy
by up to 50% compared to existing baseline policies.

</details>


### [74] [C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters](https://arxiv.org/abs/2507.18005)
*Shengye Song,Minxian Xu,Zuowei Zhang,Chengxi Gao,Fansong Zeng,Yu Ding,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 微服务架构在云平台中广泛应用，但资源竞争和干扰问题突出。本文提出基于CPI指标的干扰预测方法，并设计C-Koordinator平台，显著提升性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模微服务集群中的资源竞争和干扰问题，提升资源利用率和应用性能。

Method: 采用CPI作为干扰度量指标，结合多维指标实现精准预测，并开发C-Koordinator平台集成了干扰缓解策略。

Result: 干扰预测模型准确率超过90.3%，应用延迟显著降低，性能提升16.7%至36.1%。

Conclusion: C-Koordinator平台有效缓解了微服务集群中的干扰问题，提升了系统稳定性和性能。

Abstract: Microservices transform traditional monolithic applications into lightweight,
loosely coupled application components and have been widely adopted in many
enterprises. Cloud platform infrastructure providers enhance the resource
utilization efficiency of microservices systems by co-locating different
microservices. However, this approach also introduces resource competition and
interference among microservices. Designing interference-aware strategies for
large-scale, co-located microservice clusters is crucial for enhancing resource
utilization and mitigating competition-induced interference. These challenges
are further exacerbated by unreliable metrics, application diversity, and node
heterogeneity.
  In this paper, we first analyze the characteristics of large-scale and
co-located microservices clusters at Alibaba and further discuss why cycle per
instruction (CPI) is adopted as a metric for interference measurement in
large-scale production clusters, as well as how to achieve accurate prediction
of CPI through multi-dimensional metrics. Based on CPI interference prediction
and analysis, we also present the design of the C-Koordinator platform, an
open-source solution utilized in Alibaba cluster, which incorporates
co-location and interference mitigation strategies. The interference prediction
models consistently achieve over 90.3% accuracy, enabling precise prediction
and rapid mitigation of interference in operational environments. As a result,
application latency is reduced and stabilized across all percentiles (P50, P90,
P99) response time (RT), achieving improvements ranging from 16.7% to 36.1%
under various system loads compared with state-of-the-art system. These results
demonstrate the system's ability to maintain smooth application performance in
co-located environments.

</details>


### [75] [Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling](https://arxiv.org/abs/2507.18006)
*Jingfeng Wu,Yiyuan He,Minxian Xu,Xitong Gao,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 论文提出了CoCoServe系统，通过模块级动态扩展优化LLM服务资源管理，显著降低成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统在资源管理和动态负载适应方面存在挑战，静态部署导致资源利用率和性能低下。

Method: 设计CoCoServe系统，支持模块级复制和迁移，开发自动扩展机制动态调节资源分配。

Result: CoCoServe降低成本46%，延迟减少14%-75%，吞吐量提升1.16x-4x。

Conclusion: 模块级动态扩展是优化LLM服务的高效方法，CoCoServe展示了显著性能和成本优势。

Abstract: The rise of large language models (LLMs) has created new opportunities across
various fields but has also introduced significant challenges in resource
management. Current LLM serving systems face a fundamental tension: balancing
serving demands with limited resources while adapting to unpredictable traffic
patterns. Static deployments lead to suboptimal resource utilization and
performance degradation under dynamic workloads. Furthermore, the high cost of
adjusting instances hinders dynamic scaling, limiting the true potential of
efficient LLM serving.
  To address this, we propose CoCoServe, an elastic system that facilitates
dynamic and fine-grained scaling. Its key innovation lies in the module-level
operations for the replication and migration of LLM modules, such as decoder
layers and projections. Through a comprehensive analysis of the trade-offs
associated with these operations, we develop an auto-scaling mechanism that
dynamically regulates module-level resource allocation and performance
optimization, enabling a more cost-effective deployment of LLMs. Our evaluation
demonstrates that the scaling operations employed by CoCoServe exhibit
excellent scalability and can reduce costs by 46% while maintaining
availability. Compared to state-of-the-art LLM serving systems (e.g., Hugging
Face Transformers and vLLM), our approach reduces latency by 14%-75% and
achieves 1.16x-4x throughput on average across different model sizes and
workloads.

</details>


### [76] [Cloud Native System for LLM Inference Serving](https://arxiv.org/abs/2507.18007)
*Minxian Xu,Junhan Liao,Jingfeng Wu,Yiyuan He,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: Cloud Native 技术通过容器化、微服务和动态调度，显著提升大语言模型（LLM）的推理服务效率，优化资源分配并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统推理服务方法在云环境中存在资源浪费、高成本和可扩展性差的问题，亟需更高效的解决方案。

Method: 利用 Cloud Native 技术（如 Kubernetes 自动扩展）动态调整资源，应对工作负载波动。

Result: 实验表明 Cloud Native 架构能有效减少延迟、提升吞吐量，并优化推理服务性能。

Conclusion: Cloud Native 框架为可扩展的 LLM 推理服务提供了新的方向，对云计算和 AI 行业具有重要启示。

Abstract: Large Language Models (LLMs) are revolutionizing numerous industries, but
their substantial computational demands create challenges for efficient
deployment, particularly in cloud environments. Traditional approaches to
inference serving often struggle with resource inefficiencies, leading to high
operational costs, latency issues, and limited scalability. This article
explores how Cloud Native technologies, such as containerization,
microservices, and dynamic scheduling, can fundamentally improve LLM inference
serving. By leveraging these technologies, we demonstrate how a Cloud Native
system enables more efficient resource allocation, reduces latency, and
enhances throughput in high-demand scenarios. Through real-world evaluations
using Kubernetes-based autoscaling, we show that Cloud Native architectures can
dynamically adapt to workload fluctuations, mitigating performance bottlenecks
while optimizing LLM inference serving performance. This discussion provides a
broader perspective on how Cloud Native frameworks could reshape the future of
scalable LLM inference serving, offering key insights for researchers,
practitioners, and industry leaders in cloud computing and artificial
intelligence.

</details>


### [77] [FCPO: Federated Continual Policy Optimization for Real-Time High-Throughput Edge Video Analytics](https://arxiv.org/abs/2507.18047)
*Lucas Liebe,Thanh-Tung Nguyen,Dongman Lee*

Main category: cs.DC

TL;DR: 针对边缘视频分析的实时推理服务系统，提出结合持续强化学习（CRL）与联邦强化学习（FRL）的FCPO方法，显著提升吞吐量、延迟和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 边缘视频分析的复杂性增加，导致现有调度系统在动态环境中表现不佳，需要快速适应和高扩展性解决方案。

Method: FCPO结合CRL和FRL，动态调整推理批处理大小、输入分辨率和多线程处理，并通过多样感知经验缓冲区优化学习。

Result: 实验显示，FCPO在吞吐量、延迟和收敛速度上显著优于现有方法，且内存消耗更低。

Conclusion: FCPO为边缘视频分析提供了一种高效的实时推理优化方案。

Abstract: The growing complexity of Edge Video Analytics (EVA) facilitates new kind of
intelligent applications, but creates challenges in real-time inference serving
systems. State-of-the-art (SOTA) scheduling systems optimize global workload
distributions for heterogeneous devices but often suffer from extended
scheduling cycles, leading to sub-optimal processing in rapidly changing Edge
environments. Local Reinforcement Learning (RL) enables quick adjustments
between cycles but faces scalability, knowledge integration, and adaptability
issues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated
RL (FRL) to address these challenges. This integration dynamically adjusts
inference batch sizes, input resolutions, and multi-threading during pre- and
post-processing. CRL allows agents to learn from changing Markov Decision
Processes, capturing dynamic environmental variations, while FRL improves
generalization and convergence speed by integrating experiences across
inference models. FCPO combines these via an agent-specific aggregation scheme
and a diversity-aware experience buffer. Experiments on a real-world EVA
testbed showed over 5 times improvement in effective throughput, 60% reduced
latency, and 20% faster convergence with up to 10 times less memory consumption
compared to SOTA RL-based approaches.

</details>


### [78] [A large-scale distributed parallel discrete event simulation engines based on Warped2 for Wargaming simulation](https://arxiv.org/abs/2507.18050)
*Xiaoning Jia,Ruilin Kong,Guangya Si,Bilong Shen,Zhe Ji*

Main category: cs.DC

TL;DR: 论文提出优化框架，解决PDES引擎的效率问题，通过四项改进实现16倍加速和58.18%的同步开销降低。


<details>
  <summary>Details</summary>
Motivation: 现有PDES引擎在资源分配和复杂实体交互方面存在不足，需要优化以提高大规模模拟的效率。

Method: 采用异步监听线程、METIS负载均衡、实体交互求解器和空间哈希算法四项改进。

Result: 实验验证框架性能提升显著，实现了16倍加速并降低同步开销58.18%。

Conclusion: 优化框架为大规模模拟提供了高效的PDES解决方案。

Abstract: Rising demand for complex simulations highlights conventional
engines'scalability limits, spurring Parallel Discrete Event Simulation (PDES)
adoption.Warped2, a PDES engine leveraging Time Warp synchronization with
Pending Event Set optimization, delivers strong performance, it struggles with
inherent wargaming limitations: inefficient LP resource allocation during
synchronization and unaddressed complex entity interaction patterns. To address
these challenges, we present an optimized framework featuring four synergistic
improvements: (1) Asynchronous listener threads are introduced to address event
monitoring latency in large-scale scenarios, instead of synchronous polling
mechanisms, (2) METIS-based load rebalancing strategy is incorporated to
address the issue of dynamic event allocation during real-world simulation, (3)
Entity interaction solver with constraint satisfaction mechanisms is designed
to mitigate state conflicts, and (4) Spatial hashing algorithm to overcome
O(n^2) complexity bottlenecks in large-scale nearest-neighbor searches.
Experimental validation through a GridWorld demo demonstrates significant
enhancements in temporal fidelity and computational efficiency. Benchmark
results show our framework achieves 16x acceleration over baseline
implementations and maintains 8x speedup over 1-thread configuration across MPI
and Pthreads implementations.The combined load balancing and LP migration
strategy reduces synchronization overhead by 58.18%, with load balancing
accounting for 57% of the total improvement as the dominant optimization
factor. These improvements provide an enhanced solution for PDES implementation
in large-scale simulation scenarios.

</details>


### [79] [Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning](https://arxiv.org/abs/2507.18459)
*Amir Najjar,Riad Mokadem,Jean-Marc Pierson*

Main category: cs.DC

TL;DR: 提出了一种基于强化学习的云系统数据复制策略，动态适应工作负载变化，平衡服务质量和资源效率。


<details>
  <summary>Details</summary>
Motivation: 全球数据量激增需要可扩展的分布式系统，传统阈值方法依赖人工调整，缺乏动态适应性。

Method: 利用强化学习模型，通过定义状态、动作和奖励，自动学习系统特性并适应工作负载变化。

Result: 提出了一种优化服务质量和资源效率的数据复制策略，适用于云系统。

Conclusion: 强化学习策略能动态适应系统变化，为云数据复制提供高效解决方案。

Abstract: The rapid growth of global data volumes has created a demand for scalable
distributed systems that can maintain a high quality of service. Data
replication is a widely used technique that provides fault tolerance, improved
performance and higher availability. Traditional implementations often rely on
threshold-based activation mechanisms, which can vary depending on workload
changes and system architecture. System administrators typically bear the
responsibility of adjusting these thresholds. To address this challenge,
reinforcement learning can be used to dynamically adapt to workload changes and
different architectures. In this paper, we propose a novel data replication
strategy for cloud systems that employs reinforcement learning to automatically
learn system characteristics and adapt to workload changes. The strategy's aim
is to provide satisfactory Quality of Service while optimizing a trade-off
between provider profit and environmental impact. We present the architecture
behind our solution and describe the reinforcement learning model by defining
the states, actions and rewards.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [80] [An advanced AI driven database system](https://arxiv.org/abs/2507.17778)
*M. Tedeschi,S. Rizwan,C. Shringi,V. Devram Chandgir,S. Belich*

Main category: cs.DB

TL;DR: 本文提出了一种由AI支持的数据库系统，旨在通过自然语言处理和机器学习简化数据库管理，减少技术需求。


<details>
  <summary>Details</summary>
Motivation: 当前数据库系统对非技术人员复杂且难以使用，需要简化操作并减少错误。

Method: 集成大型语言模型和机器学习算法，通过自然语言界面和自动生成查询来管理数据。

Result: 系统能够自动完成数据建模、模式创建和性能优化，减少技术依赖。

Conclusion: AI支持的数据库系统有望解决现有技术问题，提升易用性和自动化水平。

Abstract: Contemporary database systems, while effective, suffer severe issues related
to complexity and usability, especially among individuals who lack technical
expertise but are unfamiliar with query languages like Structured Query
Language (SQL). This paper presents a new database system supported by
Artificial Intelligence (AI), which is intended to improve the management of
data using natural language processing (NLP) - based intuitive interfaces, and
automatic creation of structured queries and semi-structured data formats like
yet another markup language (YAML), java script object notation (JSON), and
application program interface (API) documentation. The system is intended to
strengthen the potential of databases through the integration of Large Language
Models (LLMs) and advanced machine learning algorithms. The integration is
purposed to allow the automation of fundamental tasks such as data modeling,
schema creation, query comprehension, and performance optimization. We present
in this paper a system that aims to alleviate the main problems with current
database technologies. It is meant to reduce the need for technical skills,
manual tuning for better performance, and the potential for human error. The AI
database employs generative schema inference and format selection to build its
schema models and execution formats.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [81] [Designing High-Performance and Thermally Feasible Multi-Chiplet Architectures enabled by Non-bendable Glass Interposer](https://arxiv.org/abs/2507.18040)
*Harsh Sharma,Janardhan Rao Doppa,Umit Y. Ogras,Partha Pratim Pande*

Main category: cs.AR

TL;DR: 玻璃中介层多芯片架构具有优异电性能，但面临封装变形问题。为此，作者提出了一种热、形变和性能感知的设计框架，通过架构与封装协同优化，显著提升性能并降低功耗。


<details>
  <summary>Details</summary>
Motivation: 解决玻璃中介层多芯片系统因封装变形导致的机械应力和可靠性问题，同时优化性能与功耗。

Method: 提出一种热、形变和性能感知的设计框架，通过表面与嵌入芯片的协同优化，平衡性能、功耗和结构可靠性。

Result: 实验显示，优化后的多芯片架构性能提升64.7%，功耗降低40%，同时降低制造成本。

Conclusion: 该框架有效解决了玻璃中介层系统的封装变形问题，并显著提升了性能与功耗表现。

Abstract: Multi-chiplet architectures enabled by glass interposer offer superior
electrical performance, enable higher bus widths due to reduced crosstalk, and
have lower capacitance in the redistribution layer than current silicon
interposer-based systems. These advantages result in lower energy per bit,
higher communication frequencies, and extended interconnect range. However,
deformation of the package (warpage) in glass interposer-based systems becomes
a critical challenge as system size increases, leading to severe mechanical
stress and reliability concerns. Beyond a certain size, conventional packaging
techniques fail to manage warpage effectively, necessitating new approaches to
mitigate warpage induced bending with scalable performance for glass interposer
based multi-chiplet systems. To address these inter-twined challenges, we
propose a thermal-, warpage-, and performance-aware design framework that
employs architecture and packaging co-optimization. The proposed framework
disintegrates the surface and embedded chiplets to balance conflicting design
objectives, ensuring optimal trade-offs between performance, power, and
structural reliability. Our experiments demonstrate that optimized
multi-chiplet architectures from our design framework achieve up to 64.7%
performance improvement and 40% power reduction compared to traditional 2.5D
systems to execute deep neural network workloads with lower fabrication costs.

</details>


### [82] [Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving](https://arxiv.org/abs/2507.18454)
*Juntao Zhao,Jiuru Li,Chuan Wu*

Main category: cs.AR

TL;DR: Sandwich是一种针对CPU优化的LLM服务引擎，通过分别优化预填充和解码阶段，显著提高了吞吐量和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CPU的解决方案在处理LLM推理时忽略了预填充和解码阶段的工作负载差异，导致性能不佳。

Method: 提出Sandwich引擎，针对预填充和解码阶段采用不同的执行计划并分别优化，利用生成的GEMM内核提升性能。

Result: 在五种CPU平台上，Sandwich实现了平均2.01倍的吞吐量提升，90%的TTFT和TPOT延迟满足需求，且在单序列和连续批量服务中表现优异。

Conclusion: Sandwich通过在CPU上优化LLM服务，显著提升了性能，且内核调优成本极低。

Abstract: Utilizing CPUs to serve large language models (LLMs) is a resource-friendly
alternative to GPU serving. Existing CPU-based solutions ignore workload
differences between the prefill and the decode phases of LLM inference,
applying a static per-NUMA (Non-Uniform Memory Access) node model partition and
utilizing vendor libraries for operator-level execution, which is suboptimal.
We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses
different execution plans for the prefill and decode phases and optimizes them
separately.
  We evaluate Sandwich across diverse baselines and datasets on five CPU
platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.
Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory
time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up
to 3.40x lower requirements in single sequence serving, and significant
improvement in Goodput in continuous-batching serving. The GEMM kernels
generated by Sandwich outperform representative vendor kernels and other
dynamic shape solutions, achieving performance comparable to static compilers
with three orders of magnitude less kernel tuning costs.

</details>


### [83] [PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation](https://arxiv.org/abs/2507.18581)
*Ravan Nazaraliyev,Saber Ganjisaffar,Nurlan Nazaraliyev,Nael Abu-Ghazaleh*

Main category: cs.AR

TL;DR: PRACtical是一种针对DRAM中Rowhammer问题的性能优化方案，通过集中化计数器电路和银行级粒度缓解策略，提高了DDR5 PRAC+ABO的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 随着DRAM密度增加，Rowhammer问题因电荷泄漏加剧而更严重，现有DDR5标准中的PRAC和ABO虽能缓解，但因性能开销和全局刷新策略效率不足，需优化。

Method: 提出PRACtical，通过集中化计数器电路减少延迟，并引入银行级RFM_ab策略，仅暂停受攻击的银行而非整个通道。

Result: PRACtical平均性能提升8%（最高20%），能耗降低19%，并将性能攻击导致的影响限制在6%以内，同时保持Rowhammer防护。

Conclusion: PRACtical通过高效设计和银行级缓解策略，显著优化了DRAM性能与能效，同时保障安全。

Abstract: As DRAM density increases, Rowhammer becomes more severe due to heightened
charge leakage, reducing the number of activations needed to induce bit flips.
The DDR5 standard addresses this threat with in-DRAM per-row activation
counters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.
However, PRAC adds performance overhead by incrementing counters during the
precharge phase, and recovery refreshes stalls the entire memory channel, even
if only one bank is under attack.
  We propose PRACtical, a performance-optimized approach to PRAC+ABO that
maintains the same security guarantees. First, we reduce counter update latency
by introducing a centralized increment circuit, enabling overlap between
counter updates and subsequent row activations in other subarrays. Second, we
enhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead
of stalling the entire channel, only affected banks are paused. This is
achieved through a DRAM-resident register that identifies attacked banks.
  PRACtical improves performance by 8% on average (up to 20%) over the
state-of-the-art, reduces energy by 19%, and limits performance degradation
from aggressive performance attacks to less than 6%, all while preserving
Rowhammer protection.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [84] [On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources](https://arxiv.org/abs/2507.18514)
*Jiping Luo,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 研究语义感知的有限状态马尔可夫链远程估计，提出结合AoCE和AoI的传输策略优化方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决远程估计中传输频率受限下的性能优化问题，研究AoCE和AoI在语义感知中的作用。

Method: 使用MAP估计器，通过约束MDP建模，提出混合策略和阈值策略。

Result: 证明混合策略的存在性，开发高效算法Insec-SPI，显著提升估计质量。

Conclusion: AoCE和AoI结合优于单一指标，Insec-SPI算法高效且计算开销低。

Abstract: This paper investigates the semantics-aware remote estimation of a
finite-state Markov chain. We employ the maximum a posteriori (MAP) estimator
and aim to devise a transmission policy to optimize estimation performance
subject to a transmission frequency constraint. We leverage two metrics, namely
the Age of Consecutive Error (AoCE) and the Age of Information (AoI), to
quantify, respectively, the significance of estimation error at the transmitter
and the predictability of outdated information at the receiver. The optimal
transmission problem is formulated as a constrained Markov decision process
(CMDP) with unbounded costs. We show the existence of an optimal simple mixture
policy, which randomly selects between two deterministic switching policies
with a fixed probability. Notably, each switching policy triggers a
transmission only when the AoCE exceeds a threshold value that depends on both
the AoI and the instantaneous estimation error. We further derive sufficient
conditions under which the switching policy reduces to a simple threshold
policy; that is, it admits identical thresholds for all estimation errors.
Leveraging these results, we develop an efficient structure-aware algorithm,
Insec-SPI, that computes the optimal policy with reduced computation overhead.
Our results demonstrate that incorporating both AoI and AoCE yields
significantly improved estimation quality compared to using either metric
alone.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [85] [Fagin's Theorem for Semiring Turing Machines](https://arxiv.org/abs/2507.18375)
*Guillermo Badia,Manfred Droste,Thomas Eiter,Rafael Kiesel,Carles Noguera,Erik Paul*

Main category: cs.CC

TL;DR: 论文改进了半环图灵机模型，提出了新的定量复杂度类NPnewinf{$\mathcal{R}$}，并通过加权二阶逻辑精确刻画其表达能力，同时比较了新类与Eiter & Kiesel的NPoldinf{$\mathcal{R}$}的关系。


<details>
  <summary>Details</summary>
Motivation: 将计算复杂度与逻辑表达能力相结合，改进现有模型并探索定量复杂度类的逻辑特征。

Method: 引入半环注释关系的加权二阶逻辑，定义新复杂度类NPnewinf{$\mathcal{R}$}，并分析其与现有类的关系。

Result: 新类NPnewinf{$\mathcal{R}$}可通过加权逻辑精确表征，且能重现Eiter & Kiesel的所有复杂度结果。

Conclusion: 新模型提供了更精确的定量复杂度逻辑描述，扩展了半环计算的理论框架。

Abstract: In recent years, quantitative complexity over semirings has been intensively
investigated. An important problem in this context is to connect computational
complexity with logical expressiveness. In this paper we improve on the model
of \emph{Semiring Turing Machines} (distinct from so called weighted Turing
machines) introduced by Eiter \& Kiesel (Semiring Reasoning Frameworks in AI
and Their Computational Complexity, \emph{J. Artif. Intell. Res.}, 2023). Our
central result is a Fagin-style theorem for a new quantitative complexity class
using a suitable weighted logical formalism. We show that the quantitative
complexity class that we call \NPnewinf{$\mathcal{R}$}, where $\mathcal{R}$ is
a commutative semiring, can be captured using a version of weighted existential
second-order logic that allows for predicates interpreted as semiring-annotated
relations. This result provides a precise logical characterization of the power
series that form the class \NPnewinf{$\mathcal{R}$}. We also give the exact
relation between Eiter \& Kiesel's version of NP, called
\NPoldinf{$\mathcal{R}$}, and the class \NPnewinf{$\mathcal{R}$}. Incidentally,
we are able to recapture all the complexity results by Eiter \& Kiesel (2023)
in our new model, connecting a quantitative version of NP to various counting
complexity classes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 提出了一个Agentic AI框架，自动化医疗数据分析流程，从数据输入到推断，通过模块化代理减少人工干预，提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域机器学习解决方案部署中的高成本和碎片化问题，提供自动化、隐私合规的数据处理方案。

Method: 使用模块化任务代理处理结构化和非结构化数据，自动完成特征选择、模型选择和预处理推荐。

Result: 在老年医学、姑息治疗和结肠镜成像数据集上验证了框架的有效性，实现了自动化和高效的数据分析。

Conclusion: 框架降低了机器学习生命周期中的专家干预需求，为临床环境提供了可扩展、成本效益高的AI部署路径。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [87] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: 本文研究了使用均值聚合函数的图神经网络（GNNs）的表达能力，发现在非均匀设置下，其表达能力与比率模态逻辑相当，低于sum聚合GNNs但高于max聚合GNNs；在均匀设置下，相对MSO的表现力与非交替模态逻辑相同。


<details>
  <summary>Details</summary>
Motivation: 探索不同聚合函数的GNNs的表达能力差异，特别是在非均匀和均匀设置下的表现，以理解其理论极限。

Method: 通过理论分析，比较均值、sum和max聚合GNNs的表达能力，并将其与模态逻辑和MSO进行关联。

Result: 均值聚合GNNs在非均匀设置下能力介于sum和max之间；均匀设置下其表现力较弱，但放松假设时可提升。

Conclusion: 均值聚合GNNs的表达能力受聚合方式和设置影响，理论分析为设计更高效的GNNs提供了指导。

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [88] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: 研究表明，AI系统在处理复杂数据时，通过可视化（如散点图）可以提高分析的准确性和精确性。


<details>
  <summary>Details</summary>
Motivation: 探讨可视化（如图表）是否对AI系统的数据分析有帮助。

Method: 实验使用GPT 4.1和Claude 3.5两种商业视觉语言模型，比较在提供原始数据、空白图表和错误匹配图表时，AI对合成数据集的分析表现。

Result: 实验结果显示，AI系统在数据伴随散点图时表现更优，尤其是数据复杂度高时。

Conclusion: AI系统像人类一样能从可视化中受益。

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [89] [Formal Verification of the Safegcd Implementation](https://arxiv.org/abs/2507.17956)
*Russell O'Connor,Andrew Poelstra*

Main category: cs.CR

TL;DR: 本文验证了比特币中使用的模块化逆算法（由Bernstein和Yang提出）的正确性，通过Coq证明助手和Verifiable C的分离逻辑实现。


<details>
  <summary>Details</summary>
Motivation: 模块化逆算法在比特币的数字签名中至关重要，但新引入的算法可能带来错误风险。

Method: 使用Coq证明助手和Verifiable C的分离逻辑实现，验证libsecp256k1库中模块化逆算法的正确性。

Result: 成功完成了对算法正确性的计算机验证。

Conclusion: 通过形式化验证，确保了比特币中模块化逆算法的可靠性。

Abstract: The modular inverse is an essential piece of computation required for
elliptic curve operations used for digital signatures in Bitcoin and other
applications. A novel approach to the extended Euclidean algorithm has been
developed by Bernstein and Yang within the last few years and incorporated into
the libsecp256k1 cryptographic library used by Bitcoin. However, novel
algorithms introduce new risks of errors. To address this we have completed a
computer verified proof of the correctness of (one of) libsecp256k1's modular
inverse implementations with the Coq proof assistant using the Verifiable C's
implementation of separation logic.

</details>


### [90] [MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection](https://arxiv.org/abs/2507.17978)
*Paulo Mendes,Eva Maia,Isabel Praça*

Main category: cs.CR

TL;DR: 该论文介绍了MeAJOR Corpus数据集，用于提升钓鱼邮件检测的机器学习模型性能，整合了多样化的钓鱼邮件样本和特征，通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 钓鱼邮件通过欺骗性内容和恶意载荷严重威胁网络安全，现有数据集的局限影响了机器学习模型的检测效果。

Method: 整合多个开源资源构建MeAJOR Corpus数据集，包含135,894个多样化样本和特征，并采用四种分类模型进行测试。

Result: 实验结果显示XGB模型在数据集上表现最佳，F1分数达98.34%，数据集解决了类别不平衡、泛化性和可复现性问题。

Conclusion: MeAJOR Corpus为钓鱼邮件检测研究提供了高质量的多样化数据集，提升了模型性能和研究可复现性。

Abstract: Phishing emails continue to pose a significant threat to cybersecurity by
exploiting human vulnerabilities through deceptive content and malicious
payloads. While Machine Learning (ML) models are effective at detecting
phishing threats, their performance largely relies on the quality and diversity
of the training data. This paper presents MeAJOR (Merged email Assets from
Joint Open-source Repositories) Corpus, a novel, multi-source phishing email
dataset designed to overcome critical limitations in existing resources. It
integrates 135894 samples representing a broad number of phishing tactics and
legitimate emails, with a wide spectrum of engineered features. We evaluated
the dataset's utility for phishing detection research through systematic
experiments with four classification models (RF, XGB, MLP, and CNN) across
multiple feature configurations. Results highlight the dataset's effectiveness,
achieving 98.34% F1 with XGB. By integrating broad features from multiple
categories, our dataset provides a reusable and consistent resource, while
addressing common challenges like class imbalance, generalisability and
reproducibility.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [91] [Neuromorphic Computing: A Theoretical Framework for Time, Space, and Energy Scaling](https://arxiv.org/abs/2507.17886)
*James B Aimone*

Main category: cs.NE

TL;DR: NMC是一种低功耗计算架构，虽然其存储程序设计与传统系统不同，但具有通用性和可编程性。


<details>
  <summary>Details</summary>
Motivation: 探讨NMC作为传统CPU和GPU的低功耗替代方案的计算价值。

Method: 分析NMC的时间和空间扩展性及其能量效率，与传统架构进行比较。

Result: NMC的能量消耗与算法状态的导数相关，适合稀疏算法和迭代优化。

Conclusion: NMC在特定算法类型（如稀疏和优化问题）中具有显著优势。

Abstract: Neuromorphic computing (NMC) is increasingly viewed as a low-power
alternative to conventional von Neumann architectures such as central
processing units (CPUs) and graphics processing units (GPUs), however the
computational value proposition has been difficult to define precisely.
  Here, we explain how NMC should be seen as general-purpose and programmable
even though it differs considerably from a conventional stored-program
architecture. We show that the time and space scaling of NMC is equivalent to
that of a theoretically infinite processor conventional system, however the
energy scaling is significantly different. Specifically, the energy of
conventional systems scales with absolute algorithm work, whereas the energy of
neuromorphic systems scales with the derivative of algorithm state. The unique
characteristics of NMC architectures make it well suited for different classes
of algorithms than conventional multi-core systems like GPUs that have been
optimized for dense numerical applications such as linear algebra. In contrast,
the unique characteristics of NMC make it ideally suited for scalable and
sparse algorithms whose activity is proportional to an objective function, such
as iterative optimization and large-scale sampling (e.g., Monte Carlo).

</details>


### [92] [Explicit Sign-Magnitude Encoders Enable Power-Efficient Multipliers](https://arxiv.org/abs/2507.18179)
*Felix Arnold,Maxence Bouvier,Ryan Amaudruz,Renzo Andri,Lukas Cavigelli*

Main category: cs.NE

TL;DR: 提出一种通过分解固定点乘法器单元以实现更高能效的方法，利用符号幅度编码的优势，并在保持逻辑等效的同时实现显著的功耗降低。


<details>
  <summary>Details</summary>
Motivation: 在AI工作负载中，输入值通常围绕零分布，需优化乘法器能效以降低功耗。

Method: 将乘法器分解为编码器和计算模块，分别优化，利用符号幅度编码提升能效。

Result: 在4位乘法器中，切换活动减少达12.9%，特定条件下可提升至33%，结合优化方法可再提升5-10%。

Conclusion: 该方法有效降低了乘法器的功耗，适用于实际生产系统。

Abstract: This work presents a method to maximize power-efficiency of fixed point
multiplier units by decomposing them into sub-components. First, an encoder
block converts the operands from a two's complement to a sign magnitude
representation, followed by a multiplier module which performs the compute
operation and outputs the resulting value in the original format. This allows
to leverage the power-efficiency of the Sign Magnitude encoding for the
multiplication. To ensure the computing format is not altered, those two
components are synthesized and optimized separately. Our method leads to
significant power savings for input values centered around zero, as commonly
encountered in AI workloads. Under a realistic input stream with values
normally distributed with a standard deviation of 3.0, post-synthesis
simulations of the 4-bit multiplier design show up to 12.9% lower switching
activity compared to synthesis without decomposition. Those gains are achieved
while ensuring compliance into any production-ready system as the overall
circuit stays logic-equivalent. With the compliance lifted and a slightly
smaller input range of -7 to +7, switching activity reductions can reach up to
33%. Additionally, we demonstrate that synthesis optimization methods based on
switching-activity-driven design space exploration can yield a further 5-10%
improvement in power-efficiency compared to a power agnostic approach.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [93] [Building an Accelerated OpenFOAM Proof-of-Concept Application using Modern C++](https://arxiv.org/abs/2507.18268)
*Giulio Malenza,Giovanni Stabile,Filippo Spiga,Robert Birke,Marco Aldinucci*

Main category: cs.MS

TL;DR: 本文总结了现代高性能计算中使用加速器（如GPU）优化数值操作的趋势，并提出了一个基于ISO C++并行结构的OpenFOAM概念验证应用，成功通过GPU卸载提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着HPC中加速器（如GPU）的普及，优化其使用以提升计算效率和可访问性成为研究重点。本文旨在通过现代C++并行结构为OpenFOAM开发提供新思路。

Method: 采用现代ISO C++并行结构，结合NVIDIA HPC SDK编译器运行时栈，实现多核执行和GPU卸载的单一代码库。

Result: 研究表明，通过C++并行结构在NVIDIA GPU上卸载计算，显著提升了OpenFOAM的laplacianFoam应用性能。

Conclusion: 本文验证了现代C++并行结构在OpenFOAM中的可行性，为HPC应用的高效实现提供了新方向。

Abstract: The modern trend in High-Performance Computing (HPC) involves the use of
accelerators such as Graphics Processing Units (GPUs) alongside Central
Processing Units (CPUs) to speed up numerical operations in various
applications. Leading manufacturers such as NVIDIA, Intel, and AMD are
constantly advancing these architectures, augmenting them with features such as
mixed precision, enhanced memory hierarchies, and specialised accelerator
silicon blocks (e.g., Tensor Cores on GPU or AMX/SME engines on CPU) to enhance
compute performance. At the same time, significant efforts in software
development are aimed at optimizing the use of these innovations, seeking to
improve usability and accessibility. This work contributes to the
state-of-the-art of OpenFOAM development by presenting a working
Proof-Of-Concept application built using modern ISO C++ parallel constructs.
This approach, combined with an appropriate compiler runtime stack, like the
one provided by the NVIDIA HPC SDK, makes it possible to accelerate
well-defined kernels, allowing multi-core execution and GPU offloading using a
single codebase. The study demonstrates that it is possible to increase the
performance of the OpenFOAM laplacianFoam application by offloading the
computations on NVIDIA GPUs using the C++ parallel construct.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [94] [WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection](https://arxiv.org/abs/2507.18173)
*Haodong Zhu,Wenhao Dong,Linlin Yang,Hong Li,Yuguang Yang,Yangyang Ren,Qingcheng Zhu,Zichao Feng,Changbai Li,Shaohui Lin,Runqi Wang,Xiaoyan Luo,Baochang Zhang*

Main category: cs.CV

TL;DR: WaveMamba是一种跨模态融合方法，通过离散小波变换分解RGB和红外图像的互补频率特征，提出了改进的检测头和逆离散小波变换以减少信息损失，并通过WaveMamba融合块实现低频和高频子带的全面融合。


<details>
  <summary>Details</summary>
Motivation: 利用可见光（RGB）和红外（IR）图像的互补特性，提升目标检测性能。

Method: 采用离散小波变换分解图像特征，提出WaveMamba融合块，结合低频和高频子带融合策略。

Result: 在四个基准测试上平均mAP提升了4.5%，超越了现有最佳方法。

Conclusion: WaveMamba在跨模态融合和目标检测任务中表现出显著优势。

Abstract: Leveraging the complementary characteristics of visible (RGB) and infrared
(IR) imagery offers significant potential for improving object detection. In
this paper, we propose WaveMamba, a cross-modality fusion method that
efficiently integrates the unique and complementary frequency features of RGB
and IR decomposed by Discrete Wavelet Transform (DWT). An improved detection
head incorporating the Inverse Discrete Wavelet Transform (IDWT) is also
proposed to reduce information loss and produce the final detection results.
The core of our approach is the introduction of WaveMamba Fusion Block (WMFB),
which facilitates comprehensive fusion across low-/high-frequency sub-bands.
Within WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba
framework, first performs initial low-frequency feature fusion with channel
swapping, followed by deep fusion with an advanced gated attention mechanism
for enhanced integration. High-frequency features are enhanced using a strategy
that applies an ``absolute maximum" fusion approach. These advancements lead to
significant performance gains, with our method surpassing state-of-the-art
approaches and achieving average mAP improvements of 4.5% on four benchmarks.

</details>


### [95] [3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation](https://arxiv.org/abs/2507.18625)
*Shuqing Li,Anson Y. Lam,Yun Peng,Wenxuan Wang,Michael R. Lyu*

Main category: cs.CV

TL;DR: Scenethesis是一种新的3D软件合成方法，通过领域特定语言ScenethesisLang实现用户需求与生成软件之间的形式化追溯，支持细粒度修改和复杂空间约束的表达。


<details>
  <summary>Details</summary>
Motivation: 现有3D软件生成方法通常将环境作为一个整体生成，难以修改特定元素且难以处理复杂空间和语义约束，因此需要一种更灵活、约束感知的方法。

Method: 提出Scenethesis方法，基于ScenethesisLang（一种领域特定语言）作为中间表示，将3D软件合成分解为多个阶段，支持独立验证、目标修改和系统约束满足。

Result: Scenethesis能准确捕获80%以上的用户需求，满足90%以上的硬约束，同时处理100多个约束条件，并在BLIP-2视觉评估中比现有方法提高42.8%。

Conclusion: Scenethesis通过精细化的中间表示和分阶段合成方法，显著提升了3D软件生成的质量和灵活性，能够更好地满足复杂需求。

Abstract: Graphical user interface (UI) software has undergone a fundamental
transformation from traditional two-dimensional (2D) desktop/web/mobile
interfaces to spatial three-dimensional (3D) environments. While existing work
has made remarkable success in automated 2D software generation, such as
HTML/CSS and mobile app interface code synthesis, the generation of 3D software
still remains under-explored. Current methods for 3D software generation
usually generate the 3D environments as a whole and cannot modify or control
specific elements in the software. Furthermore, these methods struggle to
handle the complex spatial and semantic constraints inherent in the real world.
To address the challenges, we present Scenethesis, a novel
requirement-sensitive 3D software synthesis approach that maintains formal
traceability between user specifications and generated 3D software. Scenethesis
is built upon ScenethesisLang, a domain-specific language that serves as a
granular constraint-aware intermediate representation (IR) to bridge natural
language requirements and executable 3D software. It serves both as a
comprehensive scene description language enabling fine-grained modification of
3D software elements and as a formal constraint-expressive specification
language capable of expressing complex spatial constraints. By decomposing 3D
software synthesis into stages operating on ScenethesisLang, Scenethesis
enables independent verification, targeted modification, and systematic
constraint satisfaction. Our evaluation demonstrates that Scenethesis
accurately captures over 80% of user requirements and satisfies more than 90%
of hard constraints while handling over 100 constraints simultaneously.
Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual
evaluation scores compared to the state-of-the-art method.

</details>


### [96] [SIDA: Synthetic Image Driven Zero-shot Domain Adaptation](https://arxiv.org/abs/2507.18632)
*Ye-Chan Kim,SeungJu Cha,Si-Woo Kim,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: SIDA提出了一种基于合成图像的零样本域自适应方法，通过生成目标域风格的合成图像，并利用其风格特征进行高效适配，显著减少了适应时间。


<details>
  <summary>Details</summary>
Motivation: 现有的基于文本描述的零样本域自适应方法难以捕捉复杂的现实变化，且适应时间较长。因此，探索基于图像数据的解决方案以提供更细粒度的风格线索。

Method: 通过生成源域风格的详细图像并应用图像翻译生成目标域风格的合成图像。引入Domain Mix和Patch Style Transfer模块，分别用于混合多风格和局部风格迁移。

Result: 在多样化的零样本适配场景中表现出SOTA性能，特别是在挑战性领域，同时显著减少了适应时间。

Conclusion: SIDA通过利用合成图像作为目标域代理，有效解决了文本驱动方法的局限性，实现了高效且高性能的零样本域自适应。

Abstract: Zero-shot domain adaptation is a method for adapting a model to a target
domain without utilizing target domain image data. To enable adaptation without
target images, existing studies utilize CLIP's embedding space and text
description to simulate target-like style features. Despite the previous
achievements in zero-shot domain adaptation, we observe that these text-driven
methods struggle to capture complex real-world variations and significantly
increase adaptation time due to their alignment process. Instead of relying on
text descriptions, we explore solutions leveraging image data, which provides
diverse and more fine-grained style cues. In this work, we propose SIDA, a
novel and efficient zero-shot domain adaptation method leveraging synthetic
images. To generate synthetic images, we first create detailed, source-like
images and apply image translation to reflect the style of the target domain.
We then utilize the style features of these synthetic images as a proxy for the
target domain. Based on these features, we introduce Domain Mix and Patch Style
Transfer modules, which enable effective modeling of real-world variations. In
particular, Domain Mix blends multiple styles to expand the intra-domain
representations, and Patch Style Transfer assigns different styles to
individual patches. We demonstrate the effectiveness of our method by showing
state-of-the-art performance in diverse zero-shot adaptation scenarios,
particularly in challenging domains. Moreover, our approach achieves high
efficiency by significantly reducing the overall adaptation time.

</details>


### [97] [Real-Time Object Detection and Classification using YOLO for Edge FPGAs](https://arxiv.org/abs/2507.18174)
*Rashed Al Amin,Roman Obermaisser*

Main category: cs.CV

TL;DR: 论文提出了一种基于YOLOv5的资源高效实时物体检测与分类系统，针对FPGA边缘平台优化，实现了高性能与低功耗。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO方法在边缘FPGA平台上资源效率不足，亟需一种更高效的解决方案。

Method: 采用YOLOv5，并在Xilinx Kria KV260 FPGA板上实现，训练数据来自COCO和GTSRD数据集。

Result: 分类准确率达到99%，功耗为3.5W，处理速度为9 FPS。

Conclusion: 该方法在边缘计算中实现了高效、实时的目标检测与分类。

Abstract: Object detection and classification are crucial tasks across various
application domains, particularly in the development of safe and reliable
Advanced Driver Assistance Systems (ADAS). Existing deep learning-based methods
such as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and
You Only Look Once (YOLO) have demonstrated high performance in terms of
accuracy and computational speed when deployed on Field-Programmable Gate
Arrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based
object detection and classification systems continue to face challenges in
achieving resource efficiency suitable for edge FPGA platforms. To address this
limitation, this paper presents a resource-efficient real-time object detection
and classification system based on YOLOv5 optimized for FPGA deployment. The
proposed system is trained on the COCO and GTSRD datasets and implemented on
the Xilinx Kria KV260 FPGA board. Experimental results demonstrate a
classification accuracy of 99%, with a power consumption of 3.5W and a
processing speed of 9 frames per second (FPS). These findings highlight the
effectiveness of the proposed approach in enabling real-time,
resource-efficient object detection and classification for edge computing
applications.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [98] [Weaving the Future: Generative AI and the Reimagining of Fashion Design](https://arxiv.org/abs/2507.17758)
*Pierre-Marie Chauvin,Angèle Merlin,Xavier Fresquet,Hugo Caselles-Dupré,Benjamin Simmenauer,Mathieu de Fayet*

Main category: cs.CY

TL;DR: 探讨生成式AI在时尚设计中的应用，分析其对创意流程、伦理及文化的影响。


<details>
  <summary>Details</summary>
Motivation: 研究AI如何重塑时尚设计的创意流程，并探讨其带来的伦理和美学挑战。

Method: 基于2025年研讨会“编织未来”的见解，分析AI在从构思到原型设计中的作用。

Result: 揭示了人机协作的动态、美学创新的潜力以及算法设计的环境和文化问题。

Conclusion: AI在时尚设计中具有创新潜力，但也需解决伦理和环境等问题。

Abstract: This paper explores the integration of generative AI into the fashion design
process. Drawing on insights from the January 2025 seminar ``Tisser le futur,''
it investigates how AI reshapes creative workflows, from ideation to
prototyping, while interrogating the ethical, aesthetic, and labor
implications. The paper highlights co-creative dynamics between humans and
machines, the potential for aesthetic innovation, and the environmental and
cultural challenges of algorithmic design.

</details>


### [99] [How Instructional Sequence and Personalized Support Impact Diagnostic Strategy Learning](https://arxiv.org/abs/2507.17760)
*Fatma Betül Güreş,Tanya Nazaretsky,Bahar Radmehr,Martina Rau,Tanja Käser*

Main category: cs.CY

TL;DR: 研究中比较了两种教学序列（先指导后问题解决I-PS与先问题解决后指导PS-I）在药学模拟环境中对学习效果的影响，发现PS-I在迁移任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 支持学生发展有效的诊断推理能力是教育领域的关键挑战，新手常受认知偏见困扰，情境学习（SBL）能提供帮助，但最佳教学序列尚不明确。

Method: 使用在线SBL环境PharmaSim，采用组间设计，比较了I-PS与PS-I两种教学序列对药学技术人员学徒学习效果的影响。

Result: 两种教学方式均有益，但PS-I在迁移任务中表现显著更优。

Conclusion: 先问题解决后指导的教学序列更有利于学习效果的迁移。

Abstract: Supporting students in developing effective diagnostic reasoning is a key
challenge in various educational domains. Novices often struggle with cognitive
biases such as premature closure and over-reliance on heuristics.
Scenario-based learning (SBL) can address these challenges by offering
realistic case experiences and iterative practice, but the optimal sequencing
of instruction and problem-solving activities remains unclear. This study
examines how personalized support can be incorporated into different
instructional sequences and whether providing explicit diagnostic strategy
instruction before (I-PS) or after problem-solving (PS-I) improves learning and
its transfer. We employ a between-groups design in an online SBL environment
called PharmaSim, which simulates real-world client interactions for pharmacy
technician apprentices. Results indicate that while both instruction types are
beneficial, PS-I leads to significantly higher performance in transfer tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [100] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: VeriMinder是一个交互式系统，旨在检测和缓解自然语言数据库接口中的认知偏见，通过创新的语义映射和分析框架提升数据分析质量。


<details>
  <summary>Details</summary>
Motivation: 帮助缺乏统计背景的用户在自然语言数据库接口中避免认知偏见，提出无偏见的分析问题。

Method: 1. 上下文语义映射框架；2. 基于Hard-to-Vary原则的分析框架；3. 优化的LLM驱动系统生成高质量任务特定提示。

Result: 82.5%用户认为系统提升了分析质量；在具体性、全面性和准确性指标上，VeriMinder显著优于其他方法（至少20%）。

Conclusion: VeriMinder通过创新的方法有效解决了数据分析中的"错误问题"漏洞，其开源代码为进一步研究提供了便利。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [101] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 研究探讨了维基百科多语言版本中结构化内容（尤其是表格数据）的不一致问题，并提出了一种收集、对齐和分析的方法，指出了对AI系统可靠性的影响。


<details>
  <summary>Details</summary>
Motivation: 维基百科各语言版本的内容独立编写和更新，导致事实不一致，影响了其作为可靠知识源和AI训练数据的质量。

Method: 开发了一种方法来收集、对齐和分析多语言维基百科文章中的表格数据，定义了不一致的类别，并应用定量和定性指标评估多语言对齐。

Result: 通过样本数据集展示了不一致性的具体表现，为事实验证和多语言知识交互提供了见解。

Conclusion: 研究结果对提高维基百科内容的可靠性、多语言知识交互及依赖维基百科的AI系统设计具有重要意义。

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [102] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 对LLMs和微调模型在道德内容检测上的表现进行了比较分析。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在专业道德推理任务中的表现及其局限性。

Method: 通过ROC、PR和DET曲线分析，比较LLMs和微调模型在Twitter和Reddit数据集上的表现。

Result: LLMs在道德内容检测上表现不佳，存在高假阴性率和系统性漏检，提示工程效果有限。

Conclusion: 在道德推理应用中，任务特定微调优于提示工程。

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [103] [ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation](https://arxiv.org/abs/2507.18262)
*Chenyu Su,Weiwei Shang,Chen Qian,Fei Zhang,Shuang Cong*

Main category: cs.RO

TL;DR: ReSem3D提出了一种基于语义驱动和3D空间约束的机器人操作框架，结合视觉基础模型（VFMs）和多模态大语言模型（MLLMs），解决现有方法语义粒度粗、缺乏实时闭环规划和语义多样性环境中鲁棒性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在机器人操作中语义约束建模不精细、缺乏实时性，且在语义多样性环境中表现不佳。ReSem3D旨在通过结合VFMs和MLLMs，构建细粒度的3D空间约束，提升操作的适应性和泛化能力。

Method: ReSem3D通过MLLMs的分层递归推理与VFMs交互，分两个阶段（部件级提取和区域级细化）自动构建3D空间约束，并将其编码为实时优化目标。

Result: 在家庭和实验室环境中的实验表明，ReSem3D能够零样本完成多种操作任务，表现出强大的适应性和泛化能力。

Conclusion: ReSem3D通过语义驱动和3D空间约束，实现了对动态扰动的实时反应，为机器人操作提供了鲁棒且灵活的解决方案。

Abstract: Semantics-driven 3D spatial constraints align highlevel semantic
representations with low-level action spaces, facilitating the unification of
task understanding and execution in robotic manipulation. The synergistic
reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation
Models (VFMs) enables cross-modal 3D spatial constraint construction.
Nevertheless, existing methods have three key limitations: (1) coarse semantic
granularity in constraint modeling, (2) lack of real-time closed-loop planning,
(3) compromised robustness in semantically diverse environments. To address
these challenges, we propose ReSem3D, a unified manipulation framework for
semantically diverse environments, leveraging the synergy between VFMs and
MLLMs to achieve fine-grained visual grounding and dynamically constructs
hierarchical 3D spatial constraints for real-time manipulation. Specifically,
the framework is driven by hierarchical recursive reasoning in MLLMs, which
interact with VFMs to automatically construct 3D spatial constraints from
natural language instructions and RGB-D observations in two stages: part-level
extraction and region-level refinement. Subsequently, these constraints are
encoded as real-time optimization objectives in joint space, enabling reactive
behavior to dynamic disturbances. Extensive simulation and real-world
experiments are conducted in semantically rich household and sparse chemical
lab environments. The results demonstrate that ReSem3D performs diverse
manipulation tasks under zero-shot conditions, exhibiting strong adaptability
and generalization. Code and videos at https://resem3d.github.io.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [104] [Topology-Preserving Coupling of Compressible Fluids and Thin Deformables](https://arxiv.org/abs/2507.18460)
*Jonathan Panuelos,Eitan Grinspun,David Levin*

Main category: physics.comp-ph

TL;DR: 提出了一种新的离散化方法，用于处理可压缩流体与薄可变形结构的耦合问题，确保流体域路径连接性以防止泄漏。


<details>
  <summary>Details</summary>
Motivation: 解决流体与薄结构耦合时的泄漏问题，同时实现精确的力交换。

Method: 采用约束Voronoi空间分割与Godunov型有限体积时间积分相结合的方法，精确离散流体域与固体界面。

Result: 验证了在多种挑战性场景（如气球、香槟塞、超音速小行星）中的双向能量传递能力。

Conclusion: 该方法有效解决了流体泄漏问题，并实现了精确的流体-固体耦合模拟。

Abstract: We present a novel discretization of coupled compressible fluid and thin
deformable structures that provides sufficient and necessary leakproofness by
preserving the path connectedness of the fluid domain. Our method employs a
constrained Voronoi-based spatial partitioning combined with Godunov-style
finite-volume time integration. The fluid domain is discretized into cells that
conform exactly to the fluid-solid interface, allowing boundary conditions to
be sharply resolved exactly at the interface. This enables direct force
exchange between the fluid and solid while ensuring that no fluid leaks through
the solid, even when arbitrarily thin. We validate our approach on a series of
challenging scenarios -- including a balloon propelled by internal compressed
air, a champagne cork ejecting after overcoming friction, and a supersonic
asteroid -- demonstrating bidirectional energy transfer between fluid and
solid.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [105] [Quantum Machine Learning Playground](https://arxiv.org/abs/2507.17931)
*Pascal Debus,Sebastian Issel,Kilian Tscharke*

Main category: quant-ph

TL;DR: 本文介绍了一种创新的交互式可视化工具，旨在简化量子机器学习(QML)算法的理解。


<details>
  <summary>Details</summary>
Motivation: 受经典机器学习可视化工具（如TensorFlow Playground）成功的启发，旨在填补量子机器学习领域可视化资源的空白。

Method: 结合量子计算和经典机器学习中的可视化隐喻，开发了一种算法可视化概念，并设计了一个交互式网页应用。

Result: 通过以代表性QML模型（数据重上传通用量子分类器）为例，降低了量子计算的门槛，推动了该领域的创新。

Conclusion: 此交互式应用是量子机器学习学习和探索的首个版本工具，具有潜力推动QML的发展。

Abstract: This article introduces an innovative interactive visualization tool designed
to demystify quantum machine learning (QML) algorithms. Our work is inspired by
the success of classical machine learning visualization tools, such as
TensorFlow Playground, and aims to bridge the gap in visualization resources
specifically for the field of QML. The article includes a comprehensive
overview of relevant visualization metaphors from both quantum computing and
classical machine learning, the development of an algorithm visualization
concept, and the design of a concrete implementation as an interactive web
application. By combining common visualization metaphors for the so-called data
re-uploading universal quantum classifier as a representative QML model, this
article aims to lower the entry barrier to quantum computing and encourage
further innovation in the field. The accompanying interactive application is a
proposal for the first version of a quantum machine learning playground for
learning and exploring QML models.

</details>
