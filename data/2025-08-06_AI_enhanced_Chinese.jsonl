{"id": "2508.02721", "pdf": "https://arxiv.org/pdf/2508.02721", "abs": "https://arxiv.org/abs/2508.02721", "authors": ["Libin Qiu", "Yuhang Ye", "Zhirong Gao", "Xide Zou", "Junfu Chen", "Ziming Gui", "Weizhi Huang", "Xiaobo Xue", "Wenkai Qiu", "Kun Zhao"], "title": "Blueprint First, Model Second: A Framework for Deterministic LLM Workflow", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": "8 pages, 6 figures, 3 tables", "summary": "While powerful, the inherent non-determinism of large language model (LLM)\nagents limits their application in structured operational environments where\nprocedural fidelity and predictable execution are strict requirements. This\nlimitation stems from current architectures that conflate probabilistic,\nhigh-level planning with low-level action execution within a single generative\nprocess. To address this, we introduce the Source Code Agent framework, a new\nparadigm built on the \"Blueprint First, Model Second\" philosophy. Our framework\ndecouples the workflow logic from the generative model. An expert-defined\noperational procedure is first codified into a source code-based Execution\nBlueprint, which is then executed by a deterministic engine. The LLM is\nstrategically invoked as a specialized tool to handle bounded, complex\nsub-tasks within the workflow, but never to decide the workflow's path. We\nconduct a comprehensive evaluation on the challenging tau-bench benchmark,\ndesigned for complex user-tool-rule scenarios. Our results demonstrate that the\nSource Code Agent establishes a new state-of-the-art, outperforming the\nstrongest baseline by 10.1 percentage points on the average Pass^1 score while\ndramatically improving execution efficiency. Our work enables the verifiable\nand reliable deployment of autonomous agents in applications governed by strict\nprocedural logic.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Source Code Agent\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5de5\u4f5c\u6d41\u903b\u8f91\u4e0e\u751f\u6210\u6a21\u578b\u5206\u79bb\uff0c\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u4e0d\u53ef\u9884\u6d4b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6267\u884c\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u7684\u975e\u786e\u5b9a\u6027\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u4e25\u683c\u7a0b\u5e8f\u6267\u884c\u7684\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u786e\u4fdd\u6267\u884c\u7684\u53ef\u9884\u6d4b\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u201c\u84dd\u56fe\u4f18\u5148\uff0c\u6a21\u578b\u5176\u6b21\u201d\u7684\u65b9\u6cd5\uff0c\u5c06\u4e13\u5bb6\u5b9a\u4e49\u7684\u64cd\u4f5c\u7a0b\u5e8f\u7f16\u7801\u4e3a\u6267\u884c\u84dd\u56fe\uff0c\u7531\u786e\u5b9a\u6027\u5f15\u64ce\u6267\u884c\uff0cLLM\u4ec5\u7528\u4e8e\u5904\u7406\u7279\u5b9a\u7684\u5b50\u4efb\u52a1\u3002", "result": "\u5728tau-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSource Code Agent\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747Pass^1\u5206\u6570\u6bd4\u6700\u5f3a\u57fa\u7ebf\u9ad8\u51fa10.1\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6267\u884c\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9700\u8981\u4e25\u683c\u7a0b\u5e8f\u903b\u8f91\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u4e14\u53ef\u9760\u7684\u81ea\u4e3b\u4ee3\u7406\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2508.02729", "pdf": "https://arxiv.org/pdf/2508.02729", "abs": "https://arxiv.org/abs/2508.02729", "authors": ["Zhuoran Liu"], "title": "Interpreting Performance Profiles with Deep Learning", "categories": ["cs.SE", "cs.AI", "cs.PF"], "comment": "Master of Science in Computer Science thesis, North Carolina State\n  University, 2022. Advisor: Dr. Xu Liu", "summary": "Profiling tools (also known as profilers) play an important role in\nunderstanding program performance at runtime, such as hotspots, bottlenecks,\nand inefficiencies. While profilers have been proven to be useful, they give\nextra burden to software engineers. Software engineers, as the users, are\nresponsible to interpret the complex performance data and identify actionable\noptimization in program source code. However, it can be challenging for users\nto associate inefficiencies with the program semantics, especially if the users\nare not the authors of the code, which limits the applicability of profilers.\n  In this thesis, we explore a new direction to combine performance profiles\nand program semantics with a deep learning approach. The key idea is to glean\ncode summary for semantic information (at a certain level) and integrate it\ninto a profiler, which can better understand program inefficiencies for\nactionable optimization. To be concrete, we combine profiles generated by Async\nProfiler (the state-of-the-art Java profiler) with code summarization from a\nfine-tuned CodeBERT-based model. We demonstrate the code summaries of any\nselected call path in a graphic user interface. Our system can effectively\nassist analysis on many Java benchmarks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6027\u80fd\u5206\u6790\u548c\u7a0b\u5e8f\u8bed\u4e49\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u63d0\u5347\u6027\u80fd\u5206\u6790\u5de5\u5177\u7684\u53ef\u64cd\u4f5c\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6027\u80fd\u5206\u6790\u5de5\u5177\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u8d1f\u62c5\u8f83\u91cd\uff0c\u5c24\u5176\u662f\u975e\u4ee3\u7801\u4f5c\u8005\u96be\u4ee5\u5173\u8054\u6027\u80fd\u6570\u636e\u4e0e\u7a0b\u5e8f\u8bed\u4e49\uff0c\u9650\u5236\u4e86\u5de5\u5177\u7684\u9002\u7528\u6027\u3002", "method": "\u7ed3\u5408Async Profiler\u751f\u6210\u7684\u6027\u80fd\u6570\u636e\u4e0e\u57fa\u4e8eCodeBERT\u6a21\u578b\u7684\u4ee3\u7801\u6458\u8981\u6280\u672f\uff0c\u901a\u8fc7\u56fe\u5f62\u754c\u9762\u5c55\u793a\u8c03\u7528\u8def\u5f84\u7684\u4ee3\u7801\u6458\u8981\u3002", "result": "\u7cfb\u7edf\u80fd\u6709\u6548\u8f85\u52a9\u5206\u6790\u591a\u79cdJava\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u3002", "conclusion": "\u7ed3\u5408\u6027\u80fd\u5206\u6790\u4e0e\u7a0b\u5e8f\u8bed\u4e49\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u6027\u80fd\u5206\u6790\u5de5\u5177\u7684\u5b9e\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.02732", "pdf": "https://arxiv.org/pdf/2508.02732", "abs": "https://arxiv.org/abs/2508.02732", "authors": ["Sherman Wong", "Jalaj Bhandari", "Leo Zhou Fan Yang", "Xylan Xu", "Yi Zhuang", "Cem Cayiroglu", "Payal Bhuptani", "Sheela Yadawad", "Hung Duong"], "title": "A Note on Code Quality Score: LLMs for Maintainable Large Codebases", "categories": ["cs.SE", "cs.AI"], "comment": "24 pages, ICLR format", "summary": "Maintaining code quality in large-scale software systems presents significant\nchallenges, particularly in settings where a large numbers of engineers work\nconcurrently on a codebase. This paper introduces Code Quality Score (CQS)\nsystem to automatically detect issues with a set of code changes and provide\nactionable insights. At its core, the CQS system is powered by two Llama3\nmodels, fine-tuned (with SFT and offline RL approaches), to a) detect common\ncode quality issues related to coding best practices and b) to provide good\n``critiques'' for LLM-generated code review respectively. To maintain good user\nexperience, we layer the system with hand-crafted rules to filter out incorrect\nresponses/hallucinations. Offline evaluations show that our CQS system is able\nto achieve an impressive precision rate for identifying valid issues. This\nsystem has already been rolled out to developers in an industrial scale setting\nand has consistently achieved 60\\% week over week user helpfulness rate,\ndemonstrating its effectiveness in a real-world environment. In this paper, we\npresent details of the CQS system along with some learnings on curating\ndeveloper feedback to create training data for LLM fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCode Quality Score (CQS)\u7cfb\u7edf\uff0c\u901a\u8fc7\u5fae\u8c03\u7684Llama3\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\u5e76\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\uff0c\u7ed3\u5408\u4eba\u5de5\u89c4\u5219\u8fc7\u6ee4\u9519\u8bef\u54cd\u5e94\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u5927\u89c4\u6a21\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u591a\u4eba\u534f\u4f5c\u5f00\u53d1\u65f6\u7ef4\u62a4\u4ee3\u7801\u8d28\u91cf\u5177\u6709\u6311\u6218\u6027\u3002", "method": "CQS\u7cfb\u7edf\u4f7f\u7528\u4e24\u4e2a\u5fae\u8c03\u7684Llama3\u6a21\u578b\uff08SFT\u548c\u79bb\u7ebfRL\uff09\u68c0\u6d4b\u4ee3\u7801\u6700\u4f73\u5b9e\u8df5\u95ee\u9898\uff0c\u5e76\u4e3aLLM\u751f\u6210\u7684\u4ee3\u7801\u8bc4\u5ba1\u63d0\u4f9b\u5efa\u8bae\uff0c\u8f85\u4ee5\u4eba\u5de5\u89c4\u5219\u8fc7\u6ee4\u9519\u8bef\u54cd\u5e94\u3002", "result": "\u79bb\u7ebf\u8bc4\u4f30\u663e\u793aCQS\u5728\u8bc6\u522b\u6709\u6548\u95ee\u9898\u65b9\u9762\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u7528\u6237\u6ee1\u610f\u5ea6\u6bcf\u5468\u8fbe60%\u3002", "conclusion": "CQS\u7cfb\u7edf\u5728\u5927\u89c4\u6a21\u5de5\u4e1a\u73af\u5883\u4e2d\u6709\u6548\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u5f00\u53d1\u8005\u53cd\u9988\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2508.02733", "pdf": "https://arxiv.org/pdf/2508.02733", "abs": "https://arxiv.org/abs/2508.02733", "authors": ["Rijul Jain", "Shraddha Barke", "Gabriel Ebner", "Md Rakib Hossain Misu", "Shan Lu", "Sarah Fakhoury"], "title": "What's in a Proof? Analyzing Expert Proof-Writing Processes in F* and Verus", "categories": ["cs.SE", "cs.HC"], "comment": null, "summary": "Proof-oriented programming languages (POPLs) empower developers to write code\nalongside formal correctness proofs, providing formal guarantees that the code\nadheres to specified requirements. Despite their powerful capabilities, POPLs\npresent a steep learning curve and have not yet been adopted by the broader\nsoftware community. The lack of understanding about the proof-development\nprocess and how expert proof developers interact with POPLs has hindered the\nadvancement of effective proof engineering and the development of\nproof-synthesis models/tools.\n  In this work, we conduct a user study, involving the collection and analysis\nof fine-grained source code telemetry from eight experts working with two\nlanguages, F* and Verus. Results reveal interesting trends and patterns about\nhow experts reason about proofs and key challenges encountered during the proof\ndevelopment process. We identify three distinct strategies and multiple\ninformal practices that are not captured final code snapshots, yet are\npredictive of task outcomes. We translate these findings into concrete design\nguidance for AI proof assistants: bias toward early specification drafting,\nexplicit sub-goal decomposition, bounded active errors, and disciplined\nverifier interaction. We also present a case study of an F* proof agent\ngrounded in these recommendations, and demonstrate improved performance over\nbaseline LLMs", "AI": {"tldr": "POPLs\uff08\u9762\u5411\u8bc1\u660e\u7684\u7f16\u7a0b\u8bed\u8a00\uff09\u5177\u6709\u5f3a\u5927\u529f\u80fd\u4f46\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0c\u963b\u788d\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff0c\u5206\u6790\u4e13\u5bb6\u5982\u4f55\u4f7f\u7528F*\u548cVerus\uff0c\u63d0\u51fa\u8bbe\u8ba1AI\u8bc1\u660e\u52a9\u624b\u7684\u5efa\u8bae\uff0c\u5e76\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "POPLs\u867d\u80fd\u63d0\u4f9b\u5f62\u5f0f\u5316\u6b63\u786e\u6027\u8bc1\u660e\uff0c\u4f46\u7531\u4e8e\u5b66\u4e60\u96be\u5ea6\u9ad8\u4e14\u7f3a\u4e4f\u5bf9\u5176\u5f00\u53d1\u8fc7\u7a0b\u7684\u6df1\u5165\u4e86\u89e3\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u548c\u5de5\u5177\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u6536\u96c6\u548c\u5206\u6790\u516b\u4f4d\u4e13\u5bb6\u4f7f\u7528F*\u548cVerus\u7684\u4ee3\u7801\u9065\u6d4b\u6570\u636e\uff0c\u63ed\u793a\u4e13\u5bb6\u5728\u8bc1\u660e\u5f00\u53d1\u4e2d\u7684\u7b56\u7565\u548c\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e09\u79cd\u7b56\u7565\u548c\u591a\u79cd\u672a\u8bb0\u5f55\u7684\u975e\u6b63\u5f0f\u5b9e\u8df5\uff0c\u63d0\u51faAI\u8bc1\u660e\u52a9\u624b\u7684\u56db\u9879\u8bbe\u8ba1\u5efa\u8bae\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u9a8c\u8bc1\u6548\u679c\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347POPLs\u53ef\u7528\u6027\u548c\u5de5\u5177\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0cAI\u8bc1\u660e\u52a9\u624b\u5728\u9075\u5faa\u5efa\u8bae\u540e\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2508.02857", "pdf": "https://arxiv.org/pdf/2508.02857", "abs": "https://arxiv.org/abs/2508.02857", "authors": ["Mikhail Mints", "Finn Voichick", "Leonidas Lampropoulos", "Robert Rand"], "title": "Compositional Quantum Control Flow with Efficient Compilation in Qunity", "categories": ["cs.PL", "quant-ph"], "comment": "88 pages, 30 figures", "summary": "Most existing quantum programming languages are based on the quantum circuit\nmodel of computation, as higher-level abstractions are particularly challenging\nto implement - especially ones relating to quantum control flow. The Qunity\nlanguage, proposed by Voichick et al., offered such an abstraction in the form\nof a quantum control construct, with great care taken to ensure that the\nresulting language is still realizable. However, Qunity lacked a working\nimplementation, and the originally proposed compilation procedure was very\ninefficient, with even simple quantum algorithms compiling to unreasonably\nlarge circuits.\n  In this work, we focus on the efficient compilation of high-level quantum\ncontrol flow constructs, using Qunity as our starting point. We introduce a\nwider range of abstractions on top of Qunity's core language that offer\ncompelling trade-offs compared to its existing control construct. We create a\ncomplete implementation of a Qunity compiler, which converts high-level Qunity\ncode into the quantum assembly language OpenQASM 3. We develop optimization\ntechniques for multiple stages of the Qunity compilation procedure, including\nboth low-level circuit optimizations as well as methods that consider the\nhigh-level structure of a Qunity program, greatly reducing the number of qubits\nand gates used by the compiler.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9ad8\u6548\u7f16\u8bd1\u91cf\u5b50\u63a7\u5236\u6d41\u6784\u9020\u7684\u65b9\u6cd5\uff0c\u4ee5Qunity\u8bed\u8a00\u4e3a\u57fa\u7840\uff0c\u63d0\u51fa\u4f18\u5316\u6280\u672f\u4ee5\u51cf\u5c11\u91cf\u5b50\u6bd4\u7279\u548c\u95e8\u7684\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u7f16\u7a0b\u8bed\u8a00\u591a\u57fa\u4e8e\u91cf\u5b50\u7535\u8def\u6a21\u578b\uff0c\u9ad8\u5c42\u6b21\u7684\u62bd\u8c61\u5b9e\u73b0\u56f0\u96be\uff0c\u5c24\u5176\u662f\u91cf\u5b50\u63a7\u5236\u6d41\u3002Qunity\u8bed\u8a00\u63d0\u51fa\u4e86\u91cf\u5b50\u63a7\u5236\u6784\u9020\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u73b0\u4e14\u7f16\u8bd1\u6548\u7387\u4f4e\u3002", "method": "\u4ee5Qunity\u4e3a\u57fa\u7840\uff0c\u5f15\u5165\u66f4\u591a\u62bd\u8c61\u5c42\uff0c\u5f00\u53d1\u5b8c\u6574\u7684Qunity\u7f16\u8bd1\u5668\uff0c\u5c06\u4ee3\u7801\u8f6c\u4e3aOpenQASM 3\uff0c\u5e76\u4f18\u5316\u7f16\u8bd1\u8fc7\u7a0b\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684Qunity\u7f16\u8bd1\u5668\uff0c\u663e\u8457\u51cf\u5c11\u91cf\u5b50\u6bd4\u7279\u548c\u95e8\u7684\u4f7f\u7528\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u7f16\u8bd1\u6280\u672f\uff0cQunity\u8bed\u8a00\u7684\u9ad8\u5c42\u6b21\u91cf\u5b50\u63a7\u5236\u6d41\u6784\u9020\u5f97\u4ee5\u9ad8\u6548\u5b9e\u73b0\u3002"}}
{"id": "2508.03147", "pdf": "https://arxiv.org/pdf/2508.03147", "abs": "https://arxiv.org/abs/2508.03147", "authors": ["Shunyuan Shang", "Emna Zedini", "Abla Kammoun", "Mohamed-Slim Alouini"], "title": "A Novel Hybrid Optical and STAR IRS System for NTN Communications", "categories": ["cs.PF", "cs.IT", "math.IT"], "comment": null, "summary": "This paper proposes a novel non-terrestrial networks (NTNs) system that\nintegrates optical intelligent reflecting surfaces (OIRS) and simultaneous\ntransmitting and reflecting Intelligent reflecting surfaces (STAR-IRS) to\naddress critical challenges in next-generation communication networks. The\nproposed system model features a signal transmitted from the optical ground\nstation (OGS) to the earth station (ES) via an OIRS mounted horizontally on a\nhigh altitude platform (HAP). The ES uses an amplify-and-forward (AF) relay\nwith fixed gain for signal relaying, which is then transmitted through a\nSTAR-IRS vertically installed on a building to facilitate communication with\nboth indoor and outdoor users. The FSO link incorporates (multiple-input\nmultiple-output) MIMO technology, and this paper develops a channel model\nspecifically designed for scenarios where the number of OIRS units exceeds one.\nFor the radio-frequency (RF) link, a novel and highly precise approximation\nmethod is introduced, offering superior accuracy compared to traditional\napproaches based on the central limit theorem (CLT). Closed-form analytical\nexpressions for key performance metrics, including outage probability (OP),\nergodic capacity and average bit error rate (BER) are derived in terms of the\nbivariate Fox-H function for this novel five hops system. Asymptotic\nexpressions at high SNR are also presented, providing insights into system\ndiversity order.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5149\u5b66\u667a\u80fd\u53cd\u5c04\u9762\uff08OIRS\uff09\u548c\u540c\u65f6\u6536\u53d1\u667a\u80fd\u53cd\u5c04\u9762\uff08STAR-IRS\uff09\u7684\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u4e0b\u4e00\u4ee3\u901a\u4fe1\u7f51\u7edc\u7684\u5173\u952e\u95ee\u9898\u3002\u7cfb\u7edf\u91c7\u7528\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u6280\u672f\u548c\u65b0\u578b\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4f18\u5316\u4e86\u6027\u80fd\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u4e0b\u4e00\u4ee3\u901a\u4fe1\u7f51\u7edc\u4e2d\u4fe1\u53f7\u4f20\u8f93\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408OIRS\u548cSTAR-IRS\u63d0\u5347\u7cfb\u7edf\u8986\u76d6\u548c\u6027\u80fd\u3002", "method": "\u7cfb\u7edf\u6a21\u578b\u5305\u62ec\u5149\u5b66\u5730\u9762\u7ad9\uff08OGS\uff09\u901a\u8fc7OIRS\u4f20\u8f93\u4fe1\u53f7\u5230\u5730\u9762\u7ad9\uff08ES\uff09\uff0c\u518d\u901a\u8fc7AF\u4e2d\u7ee7\u548cSTAR-IRS\u8986\u76d6\u5ba4\u5185\u5916\u7528\u6237\u3002\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u591aOIRS\u573a\u666f\u7684\u4fe1\u9053\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u8fd1\u4f3c\u65b9\u6cd5\u4f18\u5316RF\u94fe\u8def\u3002", "result": "\u63a8\u5bfc\u4e86\u95ed\u5408\u5f62\u5f0f\u7684\u5173\u952e\u6027\u80fd\u6307\u6807\uff08\u5982\u4e2d\u65ad\u6982\u7387\u3001\u904d\u5386\u5bb9\u91cf\u548c\u5e73\u5747\u8bef\u7801\u7387\uff09\u7684\u53cc\u53d8\u91cfFox-H\u51fd\u6570\u8868\u8fbe\u5f0f\uff0c\u5e76\u5206\u6790\u4e86\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u7684\u7cfb\u7edf\u591a\u6837\u6027\u9636\u6570\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u6a21\u578b\u548c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u6027\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2508.03445", "pdf": "https://arxiv.org/pdf/2508.03445", "abs": "https://arxiv.org/abs/2508.03445", "authors": ["Patrick Paetzold", "Rebecca Kehlbeck", "Yumeng Xue", "Bin Chen", "Yunhai Wang", "Oliver Deussen"], "title": "Neighborhood-Preserving Voronoi Treemaps", "categories": ["cs.GR"], "comment": null, "summary": "Voronoi treemaps are used to depict nodes and their hierarchical\nrelationships simultaneously. However, in addition to the hierarchical\nstructure, data attributes, such as co-occurring features or similarities,\nfrequently exist. Examples include geographical attributes like shared borders\nbetween countries or contextualized semantic information such as embedding\nvectors derived from large language models. In this work, we introduce a\nVoronoi treemap algorithm that leverages data similarity to generate\nneighborhood-preserving treemaps. First, we extend the treemap layout pipeline\nto consider similarity during data preprocessing. We then use a Kuhn-Munkres\nmatching of similarities to centroidal Voronoi tessellation (CVT) cells to\ncreate initial Voronoi diagrams with equal cell sizes for each level. Greedy\nswapping is used to improve the neighborhoods of cells to match the data's\nsimilarity further. During optimization, cell areas are iteratively adjusted to\ntheir respective sizes while preserving the existing neighborhoods. We\ndemonstrate the practicality of our approach through multiple real-world\nexamples drawn from infographics and linguistics. To quantitatively assess the\nresulting treemaps, we employ treemap metrics and measure neighborhood\npreservation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684Voronoi\u6811\u56fe\u7b97\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u6570\u636e\u76f8\u4f3c\u6027\u6765\u751f\u6210\u4fdd\u7559\u90bb\u57df\u5173\u7cfb\u7684\u6811\u56fe\u3002", "motivation": "\u73b0\u6709\u7684Voronoi\u6811\u56fe\u4ec5\u80fd\u5c55\u793a\u8282\u70b9\u53ca\u5176\u5c42\u6b21\u7ed3\u6784\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6570\u636e\u901a\u5e38\u8fd8\u5305\u542b\u5176\u4ed6\u5c5e\u6027\uff08\u5982\u5730\u7406\u8fb9\u754c\u6216\u8bed\u4e49\u76f8\u4f3c\u6027\uff09\u3002\u8fd9\u4e9b\u5c5e\u6027\u9700\u8981\u88ab\u7eb3\u5165\u7b97\u6cd5\u4e2d\u4ee5\u751f\u6210\u66f4\u4f18\u7684\u7ed3\u679c\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff1a1\uff09\u5728\u6570\u636e\u9884\u5904\u7406\u9636\u6bb5\u5f15\u5165\u76f8\u4f3c\u6027\uff1b2\uff09\u4f7f\u7528Kuhn-Munkres\u5339\u914d\u5c06\u76f8\u4f3c\u6027\u4e0eCVT\u5355\u5143\u5bf9\u9f50\uff1b3\uff09\u901a\u8fc7\u8d2a\u5fc3\u4ea4\u6362\u4f18\u5316\u90bb\u57df\u5173\u7cfb\uff1b4\uff09\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u8c03\u6574\u5355\u5143\u5927\u5c0f\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\uff08\u4fe1\u606f\u56fe\u8868\u548c\u8bed\u8a00\u5b66\u6570\u636e\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5b9a\u91cf\u8bc4\u4f30\u4e86\u90bb\u57df\u4fdd\u7559\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u6570\u636e\u76f8\u4f3c\u6027\u878d\u5165Voronoi\u6811\u56fe\uff0c\u751f\u6210\u4e86\u4fdd\u7559\u90bb\u57df\u5173\u7cfb\u7684\u6811\u56fe\uff0c\u5b9e\u7528\u6027\u5f3a\u3002"}}
{"id": "2508.03521", "pdf": "https://arxiv.org/pdf/2508.03521", "abs": "https://arxiv.org/abs/2508.03521", "authors": ["Naroa Coretti Sanchez", "Kent Larson"], "title": "Understanding Demand for Shared Autonomous Micro-Mobility", "categories": ["cs.ET", "cs.CY"], "comment": null, "summary": "This study examines the behavioral and environmental implications of shared\nautonomous micro-mobility systems, focusing on autonomous bicycles and their\nintegration with transit in the U.S. While prior research has addressed\noperational and lifecycle aspects, a critical gap remains in understanding\nwhich modes these services are likely to substitute, who is most inclined to\nadopt them, and how service attributes influence user decisions. We design a\ncontext-aware stated preference survey grounded in real-world trips and\nestimate discrete choice models, including a hybrid model incorporating latent\nattitudes. Findings indicate that adoption, mode shift, and environmental\nimpacts are highly sensitive to service design. Scenarios with minimal wait and\ncost yield high adoption but increase emissions, while moderate waits are more\nlikely to reduce impacts. Adoption likelihood varies with demographic\ncharacteristics, and outcomes depend on city type, context, and infrastructure\nassumptions. These insights can inform the development of more sustainable and\nequitable mobility systems.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5171\u4eab\u81ea\u52a8\u9a7e\u9a76\u5fae\u51fa\u884c\u7cfb\u7edf\u7684\u884c\u4e3a\u548c\u73af\u5883\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u81ea\u884c\u8f66\u4e0e\u7f8e\u56fd\u516c\u4ea4\u7684\u6574\u5408\u3002\u586b\u8865\u4e86\u5173\u4e8e\u670d\u52a1\u66ff\u4ee3\u6a21\u5f0f\u3001\u7528\u6237\u63a5\u53d7\u5ea6\u53ca\u670d\u52a1\u5c5e\u6027\u5f71\u54cd\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u548c\u79bb\u6563\u9009\u62e9\u6a21\u578b\u5206\u6790\u53d1\u73b0\u670d\u52a1\u8bbe\u8ba1\u5bf9\u73af\u5883\u548c\u7528\u6237\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u586b\u8865\u5bf9\u5171\u4eab\u81ea\u52a8\u9a7e\u9a76\u5fae\u51fa\u884c\u7cfb\u7edf\u66ff\u4ee3\u6a21\u5f0f\u3001\u7528\u6237\u63a5\u53d7\u5ea6\u53ca\u670d\u52a1\u5c5e\u6027\u5f71\u54cd\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u771f\u5b9e\u51fa\u884c\u7684\u60c5\u5883\u611f\u77e5\u504f\u597d\u8c03\u67e5\uff0c\u5e76\u4f30\u8ba1\u79bb\u6563\u9009\u62e9\u6a21\u578b\uff0c\u5305\u62ec\u4e00\u4e2a\u878d\u5165\u6f5c\u5728\u6001\u5ea6\u7684\u6df7\u5408\u6a21\u578b\u3002", "result": "\u670d\u52a1\u8bbe\u8ba1\u663e\u8457\u5f71\u54cd\u91c7\u7528\u7387\u3001\u6a21\u5f0f\u8f6c\u6362\u548c\u73af\u5883\u5f71\u54cd\uff1b\u4f4e\u7b49\u5f85\u65f6\u95f4\u548c\u6210\u672c\u589e\u52a0\u91c7\u7528\u7387\u4f46\u63d0\u9ad8\u6392\u653e\uff1b\u4e2d\u7b49\u7b49\u5f85\u65f6\u95f4\u66f4\u53ef\u80fd\u51cf\u5c11\u5f71\u54cd\uff1b\u91c7\u7528\u7387\u56e0\u4eba\u53e3\u7279\u5f81\u548c\u57ce\u5e02\u7c7b\u578b\u800c\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u53ef\u6301\u7eed\u548c\u516c\u5e73\u7684\u51fa\u884c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2508.02960", "pdf": "https://arxiv.org/pdf/2508.02960", "abs": "https://arxiv.org/abs/2508.02960", "authors": ["Pedro Duarte", "Andr\u00e9 Coelho", "Manuel Ricardo"], "title": "A Reinforcement Learning Framework for Mobility Control of gNBs in Dynamic Radio Access Networks", "categories": ["cs.NI"], "comment": null, "summary": "The increasing complexity of wireless environments, characterized by user\nmobility and dynamic obstructions, poses challenges for the maintenance of\nLine-of-Sight (LoS) connectivity. Mobile base stations (gNBs) stand as a\npromising solution by physically relocating to restore or sustain LoS, thereby\nnecessitating the development of intelligent algorithms for autonomous movement\ncontrol.\n  As part of the CONVERGE research project, which is developing an experimental\nchamber to integrate computer vision (CV) into mobile networks and enhance\nQuality of Service (QoS) in dynamic wireless environments, this paper presents\ntwo key contributions. First, we introduce the CONVERGE Chamber Simulator\n(CC-SIM), a 3D simulation environment for developing, training, and validating\nmobility control algorithms for mobile gNBs. CC-SIM models user and obstacle\nmobility, visual occlusion, and Radio Frequency (RF) propagation behavior. It\nsupports both offline reinforcement learning and real-time testing through\ntight integration with a standalone 5G system via the OpenAirInterface (OAI) RF\nsimulator, enabling validation under realistic network conditions.\n  Second, leveraging CC-SIM, we develop a Deep Q-Network (DQN) agent that\nlearns to reposition the gNB proactively in response to dynamic environmental\nchanges. Experiments across three representative use cases show that the\ntrained agent significantly reduces LoS blockage time - by up to 42% - when\ncompared to static deployments. These results highlight the effectiveness of\nlearning-based mobility control in adaptive next-generation wireless networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79fb\u52a8\u57fa\u7ad9(gNB)\u7684\u667a\u80fd\u79fb\u52a8\u63a7\u5236\u7b97\u6cd5\uff0c\u901a\u8fc73D\u6a21\u62df\u73af\u5883CC-SIM\u548c\u6df1\u5ea6Q\u7f51\u7edc(DQN)\u663e\u8457\u51cf\u5c11\u4e86\u52a8\u6001\u65e0\u7ebf\u73af\u5883\u4e2d\u7684\u89c6\u7ebf\u963b\u585e\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ebf\u73af\u5883\u4e2d\u7531\u4e8e\u7528\u6237\u79fb\u52a8\u548c\u52a8\u6001\u969c\u788d\u7269\u5bfc\u81f4\u7684\u89c6\u7ebf(LoS)\u8fde\u63a5\u95ee\u9898\uff0c\u63d0\u5347\u670d\u52a1\u8d28\u91cf(QoS)\u3002", "method": "\u5f00\u53d1\u4e86CONVERGE Chamber Simulator (CC-SIM)\u6a21\u62df\u73af\u5883\uff0c\u5e76\u5229\u7528\u6df1\u5ea6Q\u7f51\u7edc(DQN)\u8bad\u7ec3\u79fb\u52a8\u63a7\u5236\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u9759\u6001\u90e8\u7f72\u76f8\u6bd4\uff0cDQN\u4ee3\u7406\u80fd\u5c06LoS\u963b\u585e\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe42%\u3002", "conclusion": "\u57fa\u4e8e\u5b66\u4e60\u7684\u79fb\u52a8\u63a7\u5236\u7b97\u6cd5\u5728\u4e0b\u4e00\u4ee3\u81ea\u9002\u5e94\u65e0\u7ebf\u7f51\u7edc\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\u3002"}}
{"id": "2508.03410", "pdf": "https://arxiv.org/pdf/2508.03410", "abs": "https://arxiv.org/abs/2508.03410", "authors": ["Baoquan Zhao", "Xiaofan Ma", "Qianshi Pang", "Ruomei Wang", "Fan Zhou", "Shujin Lin"], "title": "VisAug: Facilitating Speech-Rich Web Video Navigation and Engagement with Auto-Generated Visual Augmentations", "categories": ["cs.MM", "cs.HC"], "comment": null, "summary": "The widespread adoption of digital technology has ushered in a new era of\ndigital transformation across all aspects of our lives. Online learning,\nsocial, and work activities, such as distance education, videoconferencing,\ninterviews, and talks, have led to a dramatic increase in speech-rich video\ncontent. In contrast to other video types, such as surveillance footage, which\ntypically contain abundant visual cues, speech-rich videos convey most of their\nmeaningful information through the audio channel. This poses challenges for\nimproving content consumption using existing visual-based video summarization,\nnavigation, and exploration systems. In this paper, we present VisAug, a novel\ninteractive system designed to enhance speech-rich video navigation and\nengagement by automatically generating informative and expressive visual\naugmentations based on the speech content of videos. Our findings suggest that\nthis system has the potential to significantly enhance the consumption and\nengagement of information in an increasingly video-driven digital landscape.", "AI": {"tldr": "VisAug\u662f\u4e00\u79cd\u65b0\u578b\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c6\u9891\u8bed\u97f3\u5185\u5bb9\u751f\u6210\u89c6\u89c9\u589e\u5f3a\uff0c\u6539\u5584\u4ee5\u8bed\u97f3\u4e3a\u4e3b\u7684\u89c6\u9891\u5bfc\u822a\u548c\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u6280\u672f\u7684\u666e\u53ca\uff0c\u8bed\u97f3\u4e3a\u4e3b\u7684\u89c6\u9891\u5185\u5bb9\u6fc0\u589e\uff0c\u73b0\u6709\u89c6\u89c9\u4e3a\u4e3b\u7684\u89c6\u9891\u7cfb\u7edf\u96be\u4ee5\u6ee1\u8db3\u5176\u9700\u6c42\u3002", "method": "\u5f00\u53d1VisAug\u7cfb\u7edf\uff0c\u81ea\u52a8\u6839\u636e\u8bed\u97f3\u5185\u5bb9\u751f\u6210\u89c6\u89c9\u589e\u5f3a\uff0c\u63d0\u5347\u89c6\u9891\u5bfc\u822a\u548c\u53c2\u4e0e\u5ea6\u3002", "result": "VisAug\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u6d88\u8d39\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "conclusion": "VisAug\u4e3a\u65e5\u76ca\u89c6\u9891\u5316\u7684\u6570\u5b57\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02679", "pdf": "https://arxiv.org/pdf/2508.02679", "abs": "https://arxiv.org/abs/2508.02679", "authors": ["Wayupuk Sommuang", "Kun Kerdthaisong", "Pasin Buakhaw", "Aslan B. Wong", "Nutchanon Yongsatianchot"], "title": "LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data", "categories": ["cs.HC"], "comment": null, "summary": "Students' mental well-being is vital for academic success, with activities\nsuch as studying, socializing, and sleeping playing a role. Current mobile\nsensing data highlight this intricate link using statistical and machine\nlearning analyses. We propose a novel LLM agent-based simulation framework to\nmodel student activities and mental health using the StudentLife Dataset. Each\nLLM agent was initialized with personality questionnaires and guided by\nsmartphone sensing data throughout the simulated semester. These agents predict\nindividual behaviors, provide self-reported mental health data via ecological\nmomentary assessments (EMAs), and complete follow-up personality\nquestionnaires. To ensure accuracy, we investigated various prompting\ntechniques, memory systems, and activity-based mental state management\nstrategies that dynamically update an agent's mental state based on their daily\nactivities. This simulation goes beyond simply replicating existing data. This\nallows us to explore new scenarios that are not present in the original\ndataset, such as peer influence through agent-to-agent interactions and the\nimpact of social media. Furthermore, we can conduct intervention studies by\nmanipulating activity patterns via sensing signals and personality traits using\nquestionnaire responses. This provides valuable insights into the behavioral\nchanges that could enhance student well-being. The framework also facilitates\nhypothetical interviews with LLM agents, offering deeper insights into their\nmental health. This study showcases the power of LLM-driven behavioral modeling\nwith sensing data, opening new avenues for understanding and supporting student\nmental health.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u5b66\u751f\u6d3b\u52a8\u4e0e\u5fc3\u7406\u5065\u5eb7\uff0c\u5229\u7528StudentLife\u6570\u636e\u96c6\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u4ee3\u7406\u5fc3\u7406\u72b6\u6001\u6765\u63a2\u7d22\u65b0\u573a\u666f\u548c\u5e72\u9884\u7814\u7a76\u3002", "motivation": "\u5b66\u751f\u5fc3\u7406\u5065\u5eb7\u5bf9\u5b66\u4e1a\u6210\u529f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7684\u7814\u7a76\u7f3a\u4e4f\u5bf9\u590d\u6742\u884c\u4e3a\u548c\u5fc3\u7406\u72b6\u6001\u52a8\u6001\u4ea4\u4e92\u7684\u6df1\u5165\u6a21\u62df\u3002", "method": "\u4f7f\u7528LLM\u4ee3\u7406\u6a21\u62df\u5b66\u751f\u884c\u4e3a\uff0c\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u548c\u667a\u80fd\u624b\u673a\u611f\u77e5\u6570\u636e\u521d\u59cb\u5316\u4ee3\u7406\uff0c\u5e76\u7ed3\u5408\u63d0\u793a\u6280\u672f\u3001\u8bb0\u5fc6\u7cfb\u7edf\u548c\u6d3b\u52a8\u7ba1\u7406\u7b56\u7565\u52a8\u6001\u66f4\u65b0\u5fc3\u7406\u72b6\u6001\u3002", "result": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u590d\u73b0\u73b0\u6709\u6570\u636e\uff0c\u8fd8\u80fd\u63a2\u7d22\u65b0\u573a\u666f\uff08\u5982\u540c\u4f34\u5f71\u54cd\u548c\u793e\u4ea4\u5a92\u4f53\u4f5c\u7528\uff09\uff0c\u5e76\u652f\u6301\u5e72\u9884\u7814\u7a76\u548c\u865a\u62df\u8bbf\u8c08\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u884c\u4e3a\u5efa\u6a21\u4e3a\u7406\u89e3\u548c\u652f\u6301\u5b66\u751f\u5fc3\u7406\u5065\u5eb7\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.02977", "pdf": "https://arxiv.org/pdf/2508.02977", "abs": "https://arxiv.org/abs/2508.02977", "authors": ["Dongho Yoon", "Gungyu Lee", "Jaewon Chang", "Yunjae Lee", "Dongjae Lee", "Minsoo Rhu"], "title": "Mamba-X: An End-to-End Vision Mamba Accelerator for Edge Computing Devices", "categories": ["cs.AR"], "comment": "Accepted for publication at the 44th International Conference on\n  Computer-Aided Design (ICCAD), 2025", "summary": "Transformers have proven effective in language modeling but are limited by\nhigh computational and memory demands that grow quadratically with input\nsequence length. State space models (SSMs) offer a promising alternative by\nreducing attention complexity from $O(L^2)$ to $O(L)$ while also lowering\noverall memory consumption. Vision Mamba adapts the SSM approach for computer\nvision tasks, achieving lower latency and memory consumption than traditional\ntransformer models. However, deploying Vision Mamba on edge devices is\nchallenging due to its sequential scan operations, which hinder GPU efficiency.\nWe propose Mamba-X, an end-to-end Vision Mamba accelerator that includes a\nsystolic scan array to maximize parallelism and minimize memory traffic, along\nwith a hybrid, hardware-friendly quantization technique to reduce memory usage\nand improve hardware efficiency without sacrificing accuracy.", "AI": {"tldr": "Vision Mamba \u662f\u4e00\u79cd\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u867d\u7136\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u6d88\u8017\uff0c\u4f46\u5176\u987a\u5e8f\u626b\u63cf\u64cd\u4f5c\u5bfc\u81f4 GPU \u6548\u7387\u4f4e\u4e0b\u3002Mamba-X \u662f\u4e00\u79cd\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5e76\u884c\u626b\u63cf\u548c\u91cf\u5316\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002", "motivation": "Transformer \u6a21\u578b\u5728\u8bed\u8a00\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u968f\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u800c\u663e\u8457\u4e0a\u5347\u3002\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u7684 Vision Mamba \u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u4ecd\u5b58\u5728 GPU \u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa Mamba-X \u52a0\u901f\u5668\uff0c\u91c7\u7528 systolic scan \u9635\u5217\u5b9e\u73b0\u5e76\u884c\u626b\u63cf\u4ee5\u51cf\u5c11\u5185\u5b58\u6d41\u91cf\uff0c\u5e76\u4f7f\u7528\u786c\u4ef6\u53cb\u597d\u7684\u6df7\u5408\u91cf\u5316\u6280\u672f\u964d\u4f4e\u5185\u5b58\u5360\u7528\u3002", "result": "Mamba-X \u80fd\u591f\u5728\u4e0d\u635f\u5931\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u5347 Vision Mamba \u7684\u786c\u4ef6\u6548\u7387\u548c\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "Mamba-X \u901a\u8fc7\u5e76\u884c\u626b\u63cf\u548c\u91cf\u5316\u4f18\u5316\uff0c\u89e3\u51b3\u4e86 Vision Mamba \u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03471", "pdf": "https://arxiv.org/pdf/2508.03471", "abs": "https://arxiv.org/abs/2508.03471", "authors": ["Suvam Kumar Das", "Suprio Ray"], "title": "Learned Adaptive Indexing", "categories": ["cs.DB"], "comment": null, "summary": "Indexes can significantly improve search performance in relational databases.\nHowever, if the query workload changes frequently or new data updates occur\ncontinuously, it may not be worthwhile to build a conventional index upfront\nfor query processing. Adaptive indexing is a technique in which an index gets\nbuilt on the fly as a byproduct of query processing. In recent years, research\nin database indexing has taken a new direction where machine learning models\nare employed for the purpose of indexing. These indexes, known as learned\nindexes, can be more efficient compared to traditional indexes such as B+-tree\nin terms of memory footprints and query performance. However, a learned index\nhas to be constructed upfront and requires training the model in advance, which\nbecomes a challenge in dynamic situations when workload changes frequently. To\nthe best of our knowledge, no learned indexes exist yet for adaptive indexing.\nWe propose a novel learned approach for adaptive indexing. It is built on the\nfly as queries are submitted and utilizes learned models for indexing data. To\nenhance query performance, we employ a query workload prediction technique that\nmakes future workload projection based on past workload data. We have evaluated\nour learned adaptive indexing approach against existing adaptive indexes for\nvarious query workloads. Our results show that our approach performs better\nthan others in most cases, offering 1.2x - 5.6x improvement in query\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u7d22\u5f15\u65b9\u6cd5\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u52a8\u6001\u6784\u5efa\u7d22\u5f15\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u81ea\u9002\u5e94\u7d22\u5f15\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7d22\u5f15\u5728\u9891\u7e41\u53d8\u5316\u7684\u67e5\u8be2\u8d1f\u8f7d\u6216\u6570\u636e\u66f4\u65b0\u573a\u666f\u4e0b\u6548\u7387\u4f4e\u4e0b\uff0c\u800c\u73b0\u6709\u7684\u5b66\u4e60\u7d22\u5f15\u9700\u8981\u9884\u5148\u8bad\u7ec3\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b66\u4e60\u6a21\u578b\u7684\u81ea\u9002\u5e94\u7d22\u5f15\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u67e5\u8be2\u5904\u7406\u52a8\u6001\u6784\u5efa\u7d22\u5f15\uff0c\u5e76\u7ed3\u5408\u67e5\u8be2\u8d1f\u8f7d\u9884\u6d4b\u6280\u672f\uff0c\u57fa\u4e8e\u5386\u53f2\u6570\u636e\u9884\u6d4b\u672a\u6765\u8d1f\u8f7d\uff0c\u4f18\u5316\u7d22\u5f15\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6bd4\u5176\u4ed6\u81ea\u9002\u5e94\u7d22\u5f15\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u67e5\u8be2\u6027\u80fd\u63d0\u53471.2\u81f35.6\u500d\u3002", "conclusion": "\u5b66\u4e60\u81ea\u9002\u5e94\u7d22\u5f15\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u6570\u636e\u5e93\u7d22\u5f15\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.02705", "pdf": "https://arxiv.org/pdf/2508.02705", "abs": "https://arxiv.org/abs/2508.02705", "authors": ["Wei Li", "Limei Hu", "Feng Chen", "Ye Yao"], "title": "Low-Communication Resilient Distributed Estimation Algorithm Based on Memory Mechanism", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "In multi-task adversarial networks, the accurate estimation of unknown\nparameters in a distributed algorithm is hindered by attacked nodes or links.\nTo tackle this challenge, this brief proposes a low-communication resilient\ndistributed estimation algorithm. First, a node selection strategy based on\nreputation is introduced that allows nodes to communicate with more reliable\nsubset of neighbors. Subsequently, to discern trustworthy intermediate\nestimates, the Weighted Support Vector Data Description (W-SVDD) model is\nemployed to train the memory data. This trained model contributes to reinforce\nthe resilience of the distributed estimation process against the impact of\nattacked nodes or links. Additionally, an event-triggered mechanism is\nintroduced to minimize ineffective updates to the W-SVDD model, and a suitable\nthreshold is derived based on assumptions. The convergence of the algorithm is\nanalyzed. Finally, simulation results demonstrate that the proposed algorithm\nachieves superior performance with less communication cost compared to other\nalgorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u901a\u4fe1\u5f39\u6027\u5206\u5e03\u5f0f\u4f30\u8ba1\u7b97\u6cd5\uff0c\u901a\u8fc7\u8282\u70b9\u9009\u62e9\u7b56\u7565\u548cW-SVDD\u6a21\u578b\u63d0\u5347\u6297\u653b\u51fb\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e8b\u4ef6\u89e6\u53d1\u673a\u5236\u51cf\u5c11\u65e0\u6548\u66f4\u65b0\u3002", "motivation": "\u591a\u4efb\u52a1\u5bf9\u6297\u7f51\u7edc\u4e2d\uff0c\u53d7\u653b\u51fb\u7684\u8282\u70b9\u6216\u94fe\u8def\u963b\u788d\u4e86\u672a\u77e5\u53c2\u6570\u7684\u51c6\u786e\u4f30\u8ba1\uff0c\u9700\u8981\u4e00\u79cd\u4f4e\u901a\u4fe1\u4e14\u6297\u653b\u51fb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u58f0\u8a89\u7684\u8282\u70b9\u9009\u62e9\u7b56\u7565\u548cW-SVDD\u6a21\u578b\u8bad\u7ec3\u6570\u636e\uff0c\u52a0\u5165\u4e8b\u4ef6\u89e6\u53d1\u673a\u5236\u4f18\u5316\u66f4\u65b0\u3002", "result": "\u4eff\u771f\u8868\u660e\u7b97\u6cd5\u5728\u4f4e\u901a\u4fe1\u6210\u672c\u4e0b\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\u3002", "conclusion": "\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u4f30\u8ba1\u7684\u6297\u653b\u51fb\u80fd\u529b\u548c\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2508.02764", "pdf": "https://arxiv.org/pdf/2508.02764", "abs": "https://arxiv.org/abs/2508.02764", "authors": ["Konstantin Doubrovinski"], "title": "When are two algorithms the same? Towards addressing Hilbert's 24th problem", "categories": ["cs.LO"], "comment": null, "summary": "The informal question of when two theorem proofs are \"essentially the same\"\ngoes back to David Hilbert, who considered adding it (or something largely\nequivalent) to his famous list of open problems, but eventually decided to\nleave it out. Given that the notion of a formal proof is closely related to\nthat of a (computer) program, i.e. a recursive function, it may be useful to\nask the same question with regard to programs instead. Here we propose a\nminimalistic approach to this question within Recursion Theory, building\nheavily on the use of Kolmogorov Complexity.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5224\u65ad\u4e24\u4e2a\u5b9a\u7406\u8bc1\u660e\u6216\u7a0b\u5e8f\u662f\u5426'\u672c\u8d28\u4e0a\u76f8\u540c'\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u9012\u5f52\u7406\u8bba\u548c\u67ef\u5c14\u83ab\u54e5\u6d1b\u592b\u590d\u6742\u5ea6\u7684\u7b80\u7ea6\u65b9\u6cd5\u3002", "motivation": "\u53d7\u5e0c\u5c14\u4f2f\u7279\u542f\u53d1\uff0c\u7814\u7a76\u5b9a\u7406\u8bc1\u660e\u6216\u7a0b\u5e8f\u7684'\u672c\u8d28\u76f8\u540c\u6027'\u3002", "method": "\u57fa\u4e8e\u9012\u5f52\u7406\u8bba\u548c\u67ef\u5c14\u83ab\u54e5\u6d1b\u592b\u590d\u6742\u5ea6\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u7ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u4e3a\u5b9a\u7406\u8bc1\u660e\u548c\u7a0b\u5e8f\u7684\u76f8\u4f3c\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2508.02820", "pdf": "https://arxiv.org/pdf/2508.02820", "abs": "https://arxiv.org/abs/2508.02820", "authors": ["David Svoboda", "Lori Flynn", "William Klieber", "Michael Duggan", "Nicholas Reimer", "Joseph Sible"], "title": "Automated Code Repair for C/C++ Static Analysis Alerts", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "(Note: This work is a preprint.) Static analysis (SA) tools produce many\ndiagnostic alerts indicating that source code in C or C++ may be defective and\npotentially vulnerable to security exploits. Many of these alerts are false\npositives. Identifying the true-positive alerts and repairing the defects in\nthe associated code are huge efforts that automated program repair (APR) tools\ncan help with. Our experience showed us that APR can reduce the number of SA\nalerts significantly and reduce the manual effort of analysts to review code.\nThis engineering experience paper details the application of design,\ndevelopment, and performance testing to an APR tool we built that repairs C/C++\ncode associated with 3 categories of alerts produced by multiple SA tools. Its\nrepairs are simple and local. Furthermore, our findings convinced the\nmaintainers of the CERT Coding Standards to re-assess and update the metrics\nused to assess when violations of guidelines are detectable or repairable. We\ndiscuss engineering design choices made to support goals of trustworthiness and\nacceptability to developers. Our APR tool repaired 8718 out of 9234 alerts\nproduced by one SA tool on one codebase. It can repair 3 flaw categories. For 2\nflaw categories, 2 SA tools, and 2 codebases, our tool repaired or dismissed as\nfalse positives over 80% of alerts, on average. Tests showed repairs did not\nappreciably degrade the performance of the code or cause new alerts to appear\n(with the possible exception of sqlite3.c). This paper describes unique\ncontributions that include a new empirical analysis of SA data, our selection\nmethod for flaw categories to repair, publication of our APR tool, and a\ndataset of SA alerts from open-source SA tools run on open-source codebases. It\ndiscusses positive and negative results and lessons learned.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5e94\u7528\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\uff08APR\uff09\u5de5\u5177\u663e\u8457\u51cf\u5c11\u4e86\u9759\u6001\u5206\u6790\uff08SA\uff09\u5de5\u5177\u4ea7\u751f\u7684\u8b66\u62a5\u6570\u91cf\uff0c\u964d\u4f4e\u4e86\u4eba\u5de5\u5ba1\u67e5\u4ee3\u7801\u7684\u5de5\u4f5c\u91cf\u3002", "motivation": "\u51cf\u5c11\u9759\u6001\u5206\u6790\u5de5\u5177\u4ea7\u751f\u7684\u5047\u9633\u6027\u8b66\u62a5\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u4fee\u590d\u7f3a\u9677\u4ee3\u7801\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u548c\u5f00\u53d1\u4e86\u4e00\u4e2aAPR\u5de5\u5177\uff0c\u7528\u4e8e\u4fee\u590dC/C++\u4ee3\u7801\u4e2d\u4e0e3\u7c7bSA\u8b66\u62a5\u76f8\u5173\u7684\u95ee\u9898\u3002\u4fee\u590d\u7b80\u5355\u4e14\u5c40\u90e8\u5316\u3002", "result": "\u8be5\u5de5\u5177\u6210\u529f\u4fee\u590d\u4e868718/9234\u4e2aSA\u8b66\u62a5\uff0c\u5bf9\u4e24\u7c7b\u7f3a\u9677\u548c\u4e24\u4e2a\u4ee3\u7801\u5e93\u7684\u4fee\u590d\u6216\u5047\u9633\u6027\u6392\u9664\u7387\u8d85\u8fc780%\u3002", "conclusion": "APR\u5de5\u5177\u663e\u8457\u63d0\u9ad8\u4e86SA\u8b66\u62a5\u5904\u7406\u7684\u6548\u7387\uff0c\u4f46\u9700\u8981\u6ce8\u610f\u4fee\u590d\u53ef\u80fd\u5e26\u6765\u7684\u65b0\u95ee\u9898\uff08\u5982sqlite3.c\uff09\u3002"}}
{"id": "2508.03558", "pdf": "https://arxiv.org/pdf/2508.03558", "abs": "https://arxiv.org/abs/2508.03558", "authors": ["M Zafir Sadik Khan", "Nowfel Mashnoor", "Mohammad Akyash", "Kimia Azar", "Hadi Kamali"], "title": "SAGE-HLS: Syntax-Aware AST-Guided LLM for High-Level Synthesis Code Generation", "categories": ["cs.PL"], "comment": "Accepted to the IEEE International Conference on Computer Design\n  (ICCD 2025)", "summary": "In today's rapidly evolving field of electronic design automation (EDA), the\ncomplexity of hardware designs is increasing, necessitating more sophisticated\nautomation solutions. High-level synthesis (HLS), as a pivotal solution,\nautomates hardware designs from high-level abstractions (e.g., C/C++). However,\nit faces significant challenges, particularly in design space exploration and\noptimization. While large language models (LLMs) have shown notable\ncapabilities in code generation, their application to HLS has been limited due\nto the scarcity of (publicly) available HLS code datasets. Hence, research in\nthis domain has primarily focused on techniques such as prompt engineering and\nretrieval-augmented generation (RAG). To overcome this limitation, this paper\nintroduces SAGE-HLS, the first-of-its-kind fine-tuned LLM specifically for HLS\ncode generation. Our method includes three key advancements: (i) We implement\nVerilog-to-C/C++ porting, converting verified and synthesizable Verilog codes\ninto corresponding C, creating a dataset of 16.7K HLS codes; (ii) We implement\na fine-tuning strategy, which is based on instruction prompting to code\ngeneration guided by abstract syntax tree (AST); (iii) We develop a\nsemi-automated evaluation framework using VerilogEval to assess the\nfunctionality of the generated HLS code. Our experiments show that SAGE-HLS,\nfined-tuned on the QwenCoder (2.5) 7B model, achieves a near 100% success rate\nin code synthesizability and a 75% success rate in functional correctness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SAGE-HLS\uff0c\u4e00\u79cd\u4e13\u95e8\u7528\u4e8eHLS\u4ee3\u7801\u751f\u6210\u7684\u5fae\u8c03LLM\uff0c\u901a\u8fc7\u521b\u9020\u6570\u636e\u96c6\u548c\u6539\u8fdb\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u5408\u6210\u548c\u529f\u80fd\u6b63\u786e\u6027\u3002", "motivation": "\u7531\u4e8eHLS\u9886\u57df\u7684\u516c\u5f00\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u73b0\u6709LLM\u5e94\u7528\u53d7\u9650\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u4f18\u5316HLS\u4ee3\u7801\u751f\u6210\u3002", "method": "\u65b9\u6cd5\u5305\u62ecVerilog-to-C/C++\u8f6c\u6362\u521b\u5efa\u6570\u636e\u96c6\u3001\u57fa\u4e8eAST\u7684\u5fae\u8c03\u7b56\u7565\u4ee5\u53ca\u534a\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSAGE-HLS\u5728\u4ee3\u7801\u53ef\u5408\u6210\u6027\u4e0a\u63a5\u8fd1100%\u6210\u529f\u7387\uff0c\u529f\u80fd\u6b63\u786e\u6027\u8fbe75%\u3002", "conclusion": "SAGE-HLS\u4e3aHLS\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2508.03179", "pdf": "https://arxiv.org/pdf/2508.03179", "abs": "https://arxiv.org/abs/2508.03179", "authors": ["Ulugbek Alibekov", "Vanessa Staderini", "Philipp Schneider", "Doris Antensteiner"], "title": "Advancing Precision in Multi-Point Cloud Fusion Environments", "categories": ["cs.CV", "cs.GR"], "comment": "Accpeted for publication in Communications in Computer and\n  Information Science, Springer", "summary": "This research focuses on visual industrial inspection by evaluating point\nclouds and multi-point cloud matching methods. We also introduce a synthetic\ndataset for quantitative evaluation of registration method and various distance\nmetrics for point cloud comparison. Additionally, we present a novel\nCloudCompare plugin for merging multiple point clouds and visualizing surface\ndefects, enhancing the accuracy and efficiency of automated inspection systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bc4\u4f30\u70b9\u4e91\u548c\u591a\u70b9\u4e91\u5339\u914d\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u89c6\u89c9\u5de5\u4e1a\u68c0\u6d4b\uff0c\u5e76\u5f15\u5165\u5408\u6210\u6570\u636e\u96c6\u548c\u6539\u8fdb\u7684CloudCompare\u63d2\u4ef6\u4ee5\u63d0\u9ad8\u81ea\u52a8\u68c0\u6d4b\u7cfb\u7edf\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u70b9\u4e91\u5339\u914d\u548c\u8868\u9762\u7f3a\u9677\u5206\u6790\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5408\u6210\u6570\u636e\u96c6\u7528\u4e8e\u914d\u51c6\u65b9\u6cd5\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5f00\u53d1CloudCompare\u63d2\u4ef6\u4ee5\u5b9e\u73b0\u591a\u70b9\u4e91\u5408\u5e76\u548c\u8868\u9762\u7f3a\u9677\u53ef\u89c6\u5316\u3002", "result": "\u63d0\u9ad8\u4e86\u70b9\u4e91\u5339\u914d\u7684\u7cbe\u5ea6\u548c\u81ea\u52a8\u5316\u68c0\u6d4b\u7cfb\u7edf\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u65b0\u5de5\u5177\u548c\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4e1a\u68c0\u6d4b\u4e2d\u70b9\u4e91\u5206\u6790\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.02737", "pdf": "https://arxiv.org/pdf/2508.02737", "abs": "https://arxiv.org/abs/2508.02737", "authors": ["Tasnia Nobi Afee", "Jack Hutchins", "Md Mazharul Islam", "Thomas Kampfe", "Ahmedullah Aziz"], "title": "Embedding-Enhanced Probabilistic Modeling of Ferroelectric Field Effect Transistors (FeFETs)", "categories": ["cs.LG", "cs.ET"], "comment": "15 pages, 6 figures, manuscript yet not submitted anywhere", "summary": "FeFETs hold strong potential for advancing memory and logic technologies, but\ntheir inherent randomness arising from both operational cycling and fabrication\nvariability poses significant challenges for accurate and reliable modeling.\nCapturing this variability is critical, as it enables designers to predict\nbehavior, optimize performance, and ensure reliability and robustness against\nvariations in manufacturing and operating conditions. Existing deterministic\nand machine learning-based compact models often fail to capture the full extent\nof this variability or lack the mathematical smoothness required for stable\ncircuit-level integration. In this work, we present an enhanced probabilistic\nmodeling framework for FeFETs that addresses these limitations. Building upon a\nMixture Density Network (MDN) foundation, our approach integrates C-infinity\ncontinuous activation functions for smooth, stable learning and a\ndevice-specific embedding layer to capture intrinsic physical variability\nacross devices. Sampling from the learned embedding distribution enables the\ngeneration of synthetic device instances for variability-aware simulation. With\nan R2 of 0.92, the model demonstrates high accuracy in capturing the\nvariability of FeFET current behavior. Altogether, this framework provides a\nscalable, data-driven solution for modeling the full stochastic behavior of\nFeFETs and offers a strong foundation for future compact model development and\ncircuit simulation integration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\uff08MDN\uff09\u7684\u6982\u7387\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u6349FeFET\u7684\u968f\u673a\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "FeFET\u7684\u968f\u673a\u6027\u6765\u81ea\u64cd\u4f5c\u5faa\u73af\u548c\u5236\u9020\u53d8\u5f02\u6027\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u6355\u6349\u5176\u5168\u90e8\u7279\u5f81\u6216\u7f3a\u4e4f\u6570\u5b66\u5e73\u6ed1\u6027\uff0c\u5f71\u54cd\u7535\u8def\u7ea7\u96c6\u6210\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528C-\u65e0\u7a77\u8fde\u7eed\u6fc0\u6d3b\u51fd\u6570\u7684MDN\u6a21\u578b\uff0c\u7ed3\u5408\u5668\u4ef6\u7279\u5b9a\u7684\u5d4c\u5165\u5c42\uff0c\u4ee5\u6355\u6349\u5668\u4ef6\u95f4\u7684\u672c\u5f81\u7269\u7406\u53d8\u5f02\u6027\uff0c\u5e76\u751f\u6210\u5408\u6210\u5668\u4ef6\u5b9e\u4f8b\u3002", "result": "\u6a21\u578b\u5728\u6355\u6349FeFET\u7535\u6d41\u884c\u4e3a\u53d8\u5f02\u6027\u65f6\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff08R2\u4e3a0.92\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aFeFET\u7684\u968f\u673a\u884c\u4e3a\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7684\u7d27\u51d1\u6a21\u578b\u5f00\u53d1\u548c\u7535\u8def\u4eff\u771f\u96c6\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.03095", "pdf": "https://arxiv.org/pdf/2508.03095", "abs": "https://arxiv.org/abs/2508.03095", "authors": ["Aditi Singh", "Abul Ehtesham", "Ramesh Raskar", "Mahesh Lambe", "Pradyumna Chari", "Jared James Grogan", "Abhishek Singh", "Saket Kumar"], "title": "A Survey of AI Agent Registry Solutions", "categories": ["cs.NI", "cs.AI", "cs.MA"], "comment": null, "summary": "As As autonomous AI agents scale across cloud, enterprise, and decentralized\nenvironments, the need for standardized registry systems to support discovery,\nidentity, and capability sharing has become essential. This paper surveys three\nprominent registry approaches each defined by a unique metadata model: MCP's\nmcp.json, A2A's Agent Card, and NANDA's AgentFacts. MCP uses a centralized\nmetaregistry with GitHub authenticated publishing and structured metadata for\nserver discovery. A2A enables decentralized interaction via JSON-based Agent\nCards, discoverable through well-known URIs, curated catalogs, or direct\nconfiguration. NANDA Index introduces AgentFacts, a cryptographically\nverifiable and privacy-preserving metadata model designed for dynamic\ndiscovery, credentialed capabilities, and cross-domain interoperability. These\napproaches are compared across four dimensions: security, scalability,\nauthentication, and maintainability. The paper concludes with suggestions and\nrecommendations to guide future design and adoption of registry systems for the\nInternet of AI Agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8c03\u67e5\u4e86\u4e09\u79cdAI\u4ee3\u7406\u6ce8\u518c\u7cfb\u7edf\u65b9\u6cd5\uff0c\u5206\u522b\u57fa\u4e8e\u4e0d\u540c\u7684\u5143\u6570\u636e\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u5728\u5b89\u5168\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u8ba4\u8bc1\u548c\u7ef4\u62a4\u6027\u65b9\u9762\u7684\u4f18\u52a3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u8bbe\u8ba1\u7684\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u4ee3\u7406\u5728\u4e91\u3001\u4f01\u4e1a\u548c\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u7684\u666e\u53ca\uff0c\u6807\u51c6\u5316\u6ce8\u518c\u7cfb\u7edf\u5bf9\u4e8e\u53d1\u73b0\u3001\u8eab\u4efd\u548c\u80fd\u529b\u5171\u4eab\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u5206\u6790\u4e86\u4e09\u79cd\u6ce8\u518c\u65b9\u6cd5\uff1aMCP\u7684mcp.json\uff08\u96c6\u4e2d\u5f0f\u5143\u6ce8\u518c\uff09\u3001A2A\u7684Agent Card\uff08\u57fa\u4e8eJSON\u7684\u53bb\u4e2d\u5fc3\u5316\u4ea4\u4e92\uff09\u548cNANDA\u7684AgentFacts\uff08\u52a0\u5bc6\u53ef\u9a8c\u8bc1\u7684\u9690\u79c1\u4fdd\u62a4\u6a21\u578b\uff09\u3002", "result": "\u7814\u7a76\u6bd4\u8f83\u4e86\u8fd9\u4e09\u79cd\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u8ba4\u8bc1\u548c\u7ef4\u62a4\u6027\u56db\u4e2a\u7ef4\u5ea6\u7684\u8868\u73b0\uff0c\u603b\u7ed3\u4e86\u5404\u81ea\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u672a\u6765\u8bbe\u8ba1\u548c\u91c7\u7528AI\u4ee3\u7406\u6ce8\u518c\u7cfb\u7edf\u7684\u5efa\u8bae\uff0c\u5f3a\u8c03\u4e86\u8de8\u9886\u57df\u4e92\u64cd\u4f5c\u6027\u548c\u52a8\u6001\u53d1\u73b0\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.03583", "pdf": "https://arxiv.org/pdf/2508.03583", "abs": "https://arxiv.org/abs/2508.03583", "authors": ["Quang-Linh Tran", "Binh Nguyen", "Gareth J. F. Jones", "Cathal Gurrin"], "title": "OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering Dataset", "categories": ["cs.MM", "cs.IR"], "comment": null, "summary": "Lifelogging refers to the process of passively collecting, storing, and\nanalysing personal daily life data using wearable devices. This data can\nsupport applications in memory preservation and enhancement. For example, using\nan ask-and-answer strategy, question-answering (QA) on lifelog data opens an\ninteractive and interesting way to explore memorable events and insights into\ndaily life. However, research resources for QA on lifelog data are limited to\nsmall-sized or synthetic QA datasets. In this paper, we present a novel lifelog\nQA dataset called OpenLifelogQA, building upon an 18-month lifelog dataset. Our\ndataset focuses on an open-ended and practical QA with real-world application\nin daily lifelog usage. We construct 14,187 pairs of Q&A with diverse types and\ndifficulty levels. A baseline experiment is reported for this dataset with\ncompetitive average performance of 89.7% BERT Score, 25.87% ROUGE-L and 3.9665\nLLM Score from LLaVA-NeXT-Interleave 7B model. We release this Q&A dataset to\nthe research community to support new research into lifelog technologies, such\nas enabling personal chat-based assistants for lifelog data to become a\nreality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aOpenLifelogQA\u7684\u65b0\u578b\u751f\u547d\u65e5\u5fd7\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u57fa\u4e8e18\u4e2a\u6708\u7684\u751f\u547d\u65e5\u5fd7\u6570\u636e\uff0c\u5305\u542b14,187\u5bf9\u591a\u6837\u5316\u4e14\u96be\u5ea6\u4e0d\u540c\u7684\u95ee\u7b54\uff0c\u652f\u6301\u751f\u547d\u65e5\u5fd7\u6280\u672f\u7684\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u547d\u65e5\u5fd7\u95ee\u7b54\u7814\u7a76\u8d44\u6e90\u6709\u9650\uff0c\u6570\u636e\u96c6\u591a\u4e3a\u5c0f\u578b\u6216\u5408\u6210\u6570\u636e\uff0c\u9650\u5236\u4e86\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u8bba\u6587\u57fa\u4e8e18\u4e2a\u6708\u7684\u751f\u547d\u65e5\u5fd7\u6570\u636e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5f00\u653e\u5f0f\u7684\u95ee\u7b54\u6570\u636e\u96c6OpenLifelogQA\uff0c\u5305\u542b\u591a\u79cd\u7c7b\u578b\u548c\u96be\u5ea6\u7684\u95ee\u7b54\u5bf9\uff0c\u5e76\u8fdb\u884c\u4e86\u57fa\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u57fa\u7ebf\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBERT Score\u4e3a89.7%\uff0cROUGE-L\u4e3a25.87%\uff0cLLM Score\u4e3a3.9665\uff08\u6765\u81eaLLaVA-NeXT-Interleave 7B\u6a21\u578b\uff09\u3002", "conclusion": "OpenLifelogQA\u6570\u636e\u96c6\u7684\u53d1\u5e03\u5c06\u652f\u6301\u751f\u547d\u65e5\u5fd7\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4f8b\u5982\u5b9e\u73b0\u57fa\u4e8e\u804a\u5929\u7684\u4e2a\u4eba\u751f\u547d\u65e5\u5fd7\u52a9\u624b\u3002"}}
{"id": "2508.02680", "pdf": "https://arxiv.org/pdf/2508.02680", "abs": "https://arxiv.org/abs/2508.02680", "authors": ["Pragya Singh", "Ankush Gupta", "Mohan Kumar", "Pushpendra Singh"], "title": "AnnoSense: A Framework for Physiological Emotion Data Collection in Everyday Settings for AI", "categories": ["cs.HC", "cs.AI"], "comment": "To be published in IMWUT, September 2025", "summary": "Emotional and mental well-being are vital components of quality of life, and\nwith the rise of smart devices like smartphones, wearables, and artificial\nintelligence (AI), new opportunities for monitoring emotions in everyday\nsettings have emerged. However, for AI algorithms to be effective, they require\nhigh-quality data and accurate annotations. As the focus shifts towards\ncollecting emotion data in real-world environments to capture more authentic\nemotional experiences, the process of gathering emotion annotations has become\nincreasingly complex. This work explores the challenges of everyday emotion\ndata collection from the perspectives of key stakeholders. We collected 75\nsurvey responses, performed 32 interviews with the public, and 3 focus group\ndiscussions (FGDs) with 12 mental health professionals. The insights gained\nfrom a total of 119 stakeholders informed the development of our framework,\nAnnoSense, designed to support everyday emotion data collection for AI. This\nframework was then evaluated by 25 emotion AI experts for its clarity,\nusefulness, and adaptability. Lastly, we discuss the potential next steps and\nimplications of AnnoSense for future research in emotion AI, highlighting its\npotential to enhance the collection and analysis of emotion data in real-world\ncontexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6536\u96c6\u60c5\u611f\u6570\u636e\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aAnnoSense\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u60c5\u611fAI\u7684\u6570\u636e\u6536\u96c6\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u60c5\u611f\u76d1\u6d4b\u5728\u65e5\u5e38\u73af\u5883\u4e2d\u7684\u5e94\u7528\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u51c6\u786e\u6807\u6ce8\u4ecd\u662fAI\u7b97\u6cd5\u6709\u6548\u7684\u5173\u952e\u3002", "method": "\u901a\u8fc775\u4efd\u8c03\u67e5\u95ee\u5377\u300132\u6b21\u516c\u4f17\u8bbf\u8c08\u548c3\u6b21\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\uff08\u517112\u4f4d\u5fc3\u7406\u5065\u5eb7\u4e13\u5bb6\uff09\u6536\u96c6\u5229\u76ca\u76f8\u5173\u8005\u7684\u53cd\u9988\uff0c\u5f00\u53d1\u4e86AnnoSense\u6846\u67b6\uff0c\u5e76\u753125\u4f4d\u60c5\u611fAI\u4e13\u5bb6\u8bc4\u4f30\u3002", "result": "AnnoSense\u6846\u67b6\u5728\u6e05\u6670\u6027\u3001\u5b9e\u7528\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u5f97\u5230\u4e86\u4e13\u5bb6\u7684\u8ba4\u53ef\u3002", "conclusion": "AnnoSense\u6846\u67b6\u6709\u671b\u6539\u8fdb\u771f\u5b9e\u73af\u5883\u4e2d\u60c5\u611f\u6570\u636e\u7684\u6536\u96c6\u4e0e\u5206\u6790\uff0c\u4e3a\u672a\u6765\u60c5\u611fAI\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.02992", "pdf": "https://arxiv.org/pdf/2508.02992", "abs": "https://arxiv.org/abs/2508.02992", "authors": ["Peijing Li", "Muhammad Shahir Abdurraman", "Rachel Cleaveland", "Sergey Legtchenko", "Philip Levis", "Ioan Stefanovici", "Thierry Tambe", "David Tennenhouse", "Caroline Trippel"], "title": "Towards Memory Specialization: A Case for Long-Term and Short-Term RAM", "categories": ["cs.AR", "cs.ET"], "comment": "9 pages, 3 figures", "summary": "Both SRAM and DRAM have stopped scaling: there is no technical roadmap to\nreduce their cost (per byte/GB). As a result, memory now dominates system cost.\nThis paper argues for a paradigm shift from today's simple memory hierarchy\ntoward specialized memory architectures that exploit application-specific\naccess patterns. Rather than relying solely on traditional off-chip DRAM and\non-chip SRAM, we envisage memory systems equipped with additional types of\nmemory whose performance trade-offs benefit workloads through non-hierarchical\noptimization. We propose two new memory classes deserving explicit OS support:\nlong-term RAM (LtRAM) optimized for read-intensive data with long lifetimes,\nand short-term RAM (StRAM) designed for transient, frequently-accessed data\nwith short lifetimes. We explore underlying device technologies that could\nimplement these classes, including their evolution and their potential\nintegration into current system designs given emerging workload requirements.\nWe identify critical research challenges to realize what we believe is a\nnecessary evolution toward more efficient and scalable computing systems\ncapable of meeting future demands.", "AI": {"tldr": "SRAM\u548cDRAM\u7684\u6269\u5c55\u5df2\u505c\u6b62\uff0c\u6210\u672c\u65e0\u6cd5\u964d\u4f4e\uff0c\u5185\u5b58\u6210\u4e3a\u7cfb\u7edf\u4e3b\u8981\u6210\u672c\u3002\u672c\u6587\u4e3b\u5f20\u4ece\u7b80\u5355\u7684\u5185\u5b58\u5c42\u6b21\u8f6c\u5411\u5229\u7528\u5e94\u7528\u7279\u5b9a\u8bbf\u95ee\u6a21\u5f0f\u7684\u4e13\u7528\u5185\u5b58\u67b6\u6784\u3002", "motivation": "\u4f20\u7edfSRAM\u548cDRAM\u7684\u6269\u5c55\u53d7\u963b\uff0c\u5185\u5b58\u6210\u672c\u6210\u4e3a\u74f6\u9888\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u5185\u5b58\u67b6\u6784\u4ee5\u5e94\u5bf9\u672a\u6765\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u5185\u5b58\u7c7b\u522b\uff1aLtRAM\uff08\u9488\u5bf9\u957f\u671f\u5b58\u5728\u7684\u8bfb\u5bc6\u96c6\u578b\u6570\u636e\uff09\u548cStRAM\uff08\u9488\u5bf9\u77ed\u671f\u9891\u7e41\u8bbf\u95ee\u7684\u4e34\u65f6\u6570\u636e\uff09\uff0c\u5e76\u63a2\u8ba8\u5176\u6280\u672f\u5b9e\u73b0\u548c\u7cfb\u7edf\u96c6\u6210\u3002", "result": "\u901a\u8fc7\u975e\u5206\u5c42\u4f18\u5316\uff0c\u8fd9\u4e9b\u4e13\u7528\u5185\u5b58\u7c7b\u522b\u53ef\u4ee5\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63a8\u52a8\u8fd9\u79cd\u5185\u5b58\u67b6\u6784\u7684\u6f14\u53d8\u662f\u5b9e\u73b0\u672a\u6765\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u8ba1\u7b97\u7cfb\u7edf\u7684\u5173\u952e\u3002"}}
{"id": "2508.03565", "pdf": "https://arxiv.org/pdf/2508.03565", "abs": "https://arxiv.org/abs/2508.03565", "authors": ["Junfeng Liu", "Haoxuan Xie", "Siqiang Luo"], "title": "[Technical Report] ArceKV: Towards Workload-driven LSM-compactions for Key-Value Store Under Dynamic Workloads", "categories": ["cs.DB", "H.2.0"], "comment": "17 pages, 11 figures", "summary": "Key-value stores underpin a wide range of applications due to their\nsimplicity and efficiency. Log-Structured Merge Trees (LSM-trees) dominate as\ntheir underlying structure, excelling at handling rapidly growing data. Recent\nresearch has focused on optimizing LSM-tree performance under static workloads\nwith fixed read-write ratios. However, real-world workloads are highly dynamic,\nand existing workload-aware approaches often struggle to sustain optimal\nperformance or incur substantial transition overhead when workload patterns\nshift. To address this, we propose ElasticLSM, which removes traditional\nLSM-tree structural constraints to allow more flexible management actions\n(i.e., compactions and write stalls) creating greater opportunities for\ncontinuous performance optimization. We further design Arce, a lightweight\ncompaction decision engine that guides ElasticLSM in selecting the optimal\naction from its expanded action space. Building on these components, we\nimplement ArceKV, a full-fledged key-value store atop RocksDB. Extensive\nevaluations demonstrate that ArceKV outperforms state-of-the-art compaction\nstrategies across diverse workloads, delivering around 3x faster performance in\ndynamic scenarios.", "AI": {"tldr": "ElasticLSM\u548cArceKV\u901a\u8fc7\u4f18\u5316LSM\u6811\u7684\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u952e\u503c\u5b58\u50a8\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u5177\u6709\u9ad8\u5ea6\u52a8\u6001\u6027\uff0c\u800c\u73b0\u6709\u7684\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u65b9\u6cd5\u96be\u4ee5\u7ef4\u6301\u6700\u4f18\u6027\u80fd\u6216\u8f6c\u6362\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51faElasticLSM\u653e\u5bbdLSM\u6811\u7684\u7ed3\u6784\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7Arce\u8f7b\u91cf\u7ea7\u51b3\u7b56\u5f15\u64ce\u6307\u5bfc\u6700\u4f18\u64cd\u4f5c\u3002", "result": "ArceKV\u5728\u52a8\u6001\u573a\u666f\u4e2d\u6027\u80fd\u63d0\u5347\u7ea63\u500d\u3002", "conclusion": "ElasticLSM\u548cArceKV\u4e3a\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u952e\u503c\u5b58\u50a8\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.02708", "pdf": "https://arxiv.org/pdf/2508.02708", "abs": "https://arxiv.org/abs/2508.02708", "authors": ["Mario Scrocca", "Marco Grassi", "Alessio Carenini", "Jean-Paul Calbimonte", "Darko Anicic", "Irene Celino"], "title": "A DataOps Toolbox Enabling Continuous Semantic Integration of Devices for Edge-Cloud AI Applications", "categories": ["cs.DC"], "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution will\n  be published in The Semantic Web - ISWC 2025", "summary": "The implementation of AI-based applications in complex environments often\nrequires the collaboration of several devices spanning from edge to cloud.\nIdentifying the required devices and configuring them to collaborate is a\nchallenge relevant to different scenarios, like industrial shopfloors, road\ninfrastructures, and healthcare therapies. We discuss the design and\nimplementation of a DataOps toolbox leveraging Semantic Web technologies and a\nlow-code mechanism to address heterogeneous data interoperability requirements\nin the development of such applications. The toolbox supports a continuous\nsemantic integration approach to tackle various types of devices, data formats,\nand semantics, as well as different communication interfaces. The paper\npresents the application of the toolbox to three use cases from different\ndomains, the DataOps pipelines implemented, and how they guarantee\ninteroperability of static nodes' information and runtime data exchanges.\nFinally, we discuss the results from the piloting activities in the use cases\nand the lessons learned.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u7f51\u7edc\u6280\u672f\u548c\u4f4e\u4ee3\u7801\u673a\u5236\u7684DataOps\u5de5\u5177\u7bb1\uff0c\u65e8\u5728\u89e3\u51b3AI\u5e94\u7528\u4e2d\u591a\u8bbe\u5907\u534f\u4f5c\u7684\u6570\u636e\u4e92\u64cd\u4f5c\u6027\u6311\u6218\u3002", "motivation": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u65bdAI\u5e94\u7528\u9700\u8981\u591a\u8bbe\u5907\u534f\u4f5c\uff0c\u4f46\u4e0d\u540c\u8bbe\u5907\u3001\u6570\u636e\u683c\u5f0f\u548c\u901a\u4fe1\u63a5\u53e3\u7684\u5f02\u6784\u6027\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u652f\u6301\u6301\u7eed\u8bed\u4e49\u96c6\u6210\u7684DataOps\u5de5\u5177\u7bb1\uff0c\u7ed3\u5408\u8bed\u4e49\u7f51\u7edc\u6280\u672f\u548c\u4f4e\u4ee3\u7801\u673a\u5236\uff0c\u89e3\u51b3\u6570\u636e\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\u3002", "result": "\u5de5\u5177\u7bb1\u5728\u4e09\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u7528\u4f8b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9759\u6001\u8282\u70b9\u4fe1\u606f\u548c\u8fd0\u884c\u65f6\u6570\u636e\u4ea4\u6362\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "\u901a\u8fc7\u8bd5\u70b9\u6d3b\u52a8\u9a8c\u8bc1\u4e86\u5de5\u5177\u7bb1\u7684\u6709\u6548\u6027\uff0c\u5e76\u603b\u7ed3\u4e86\u7ecf\u9a8c\u6559\u8bad\uff0c\u4e3a\u672a\u6765\u7c7b\u4f3c\u5e94\u7528\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2508.02774", "pdf": "https://arxiv.org/pdf/2508.02774", "abs": "https://arxiv.org/abs/2508.02774", "authors": ["Zoran Majkic"], "title": "Intensional FOL over Belnap's Billatice for Strong-AI Robotics", "categories": ["cs.LO"], "comment": "28 pages", "summary": "AGI (Strong AI) aims to create intelligent robots that are quasi\nindistinguishable from the human mind. Like a child, the AGI robot would have\nto learn through input and experiences, constantly progressing and advancing\nits abilities over time. The AGI robot would require an intelligence more close\nto human's intelligence: it would have a self-aware consciousness that has the\nability to solve problems, learn, and plan. Based on this approach an\nIntensional many-sorted First-order Logic (IFOL), as an extension of a standard\nFOL with Tarskian's semantics, is proposed in order to avoid the problems of\nstandard 2-valued FOL with paradoxes (inconsistent formulae) and a necessity\nfor robots to work with incomplete (unknown) knowledge as well. This is a more\nsophisticated version of IFOL with the same syntax but different semantics,\nable to deal with truth-ordering and knowledge-ordering as well, based on the\nwell known Belnap's billatice with four truth-values that extend the set of\nclassical two truth-values.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u7684\u4e00\u9636\u903b\u8f91\uff08IFOL\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5f3a\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u4e2d\u7684\u6096\u8bba\u548c\u4e0d\u5b8c\u6574\u77e5\u8bc6\u95ee\u9898\u3002", "motivation": "\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u63a5\u8fd1\u4eba\u7c7b\u667a\u80fd\u7684AGI\u673a\u5668\u4eba\uff0c\u5176\u9700\u8981\u5177\u5907\u81ea\u6211\u610f\u8bc6\u3001\u5b66\u4e60\u80fd\u529b\u548c\u89c4\u5212\u80fd\u529b\uff0c\u4f46\u6807\u51c6\u4e8c\u503c\u903b\u8f91\u5b58\u5728\u6096\u8bba\u548c\u4e0d\u5b8c\u6574\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684IFOL\u903b\u8f91\uff0c\u57fa\u4e8eBelnap\u7684\u56db\u503c\u683c\u5b50\u7406\u8bba\uff0c\u80fd\u591f\u5904\u7406\u771f\u503c\u6392\u5e8f\u548c\u77e5\u8bc6\u6392\u5e8f\u3002", "result": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u6807\u51c6\u4e8c\u503c\u903b\u8f91\u4e2d\u7684\u6096\u8bba\u548c\u4e0d\u5b8c\u6574\u77e5\u8bc6\u95ee\u9898\u3002", "conclusion": "\u6269\u5c55\u7684IFOL\u903b\u8f91\u4e3aAGI\u673a\u5668\u4eba\u7684\u667a\u80fd\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2508.02827", "pdf": "https://arxiv.org/pdf/2508.02827", "abs": "https://arxiv.org/abs/2508.02827", "authors": ["Ora Nova Fandina", "Eitan Farchi", "Shmulik Froimovich", "Rami Katan", "Alice Podolsky", "Orna Raz", "Avi Ziv"], "title": "Automated Validation of LLM-based Evaluators for Software Engineering Artifacts", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Automation in software engineering increasingly relies on large language\nmodels (LLMs) to generate, review, and assess code artifacts. However,\nestablishing LLMs as reliable evaluators remains an open challenge: human\nevaluations are costly, subjective and non scalable, while existing automated\nmethods fail to discern fine grained variations in artifact quality.\n  We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation),\nan automated framework for benchmarking LLM based evaluators across software\nengineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder\napplies novel generation techniques to automatically synthesize artifacts with\nprogressively reduced quality, and Evaluator Tester quantifies each candidate\nevaluator configuration by measuring how closely its rankings align with\nexpected ordering.\n  A key feature of REFINE is controllability: users can tune the granularity of\ndegradation to progressively refine evaluator configurations, from coarse\nfiltering to stress testing on subtle quality gaps.\n  While the methodology is general, we focus on coding tasks reflecting the\npractical demands in our production setting. REFINE was integrated into IBM's\ninternal development workflows and applied to code generation, translation, and\nsummarization for COBOL, an enterprise critical programming language, using\nindustrial data. It was used to identify LLM as a Judge configurations that\nlifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks.\nThese nuance sensitive evaluators are now actively used by model training teams\nto support model release decisions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faREFINE\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u751f\u6210\u4e0d\u540c\u8d28\u91cf\u7684\u4ee3\u7801\u5e76\u7ed3\u5408\u671f\u671b\u6392\u5e8f\u6d4b\u8bd5\u8bc4\u4f30\u5668\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524dLLM\u4f5c\u4e3a\u4ee3\u7801\u8bc4\u4f30\u5668\u4ecd\u4e0d\u53ef\u9760\uff0c\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\uff0c\u73b0\u6709\u81ea\u52a8\u65b9\u6cd5\u65e0\u6cd5\u8bc6\u522b\u7ec6\u7c92\u5ea6\u8d28\u91cf\u5dee\u5f02\u3002", "method": "REFINE\u6846\u67b6\u5305\u62ecHierarchy Dataset Builder\uff08\u751f\u6210\u8d28\u91cf\u9012\u51cf\u7684\u4ee3\u7801\uff09\u548cEvaluator Tester\uff08\u6d4b\u8bd5\u8bc4\u4f30\u5668\u6392\u5e8f\u4e0e\u9884\u671f\u7684\u4e00\u81f4\u6027\uff09\u3002", "result": "REFINE\u5728IBM\u5185\u90e8\u5de5\u4f5c\u6d41\u4e2d\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u5668\u914d\u7f6e\u7684\u51c6\u786e\u5ea6\uff0c\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8bc4\u5206\u4ece0.7\u63d0\u9ad8\u52300.9\u3002", "conclusion": "REFINE\u901a\u8fc7\u53ef\u63a7\u7684\u7ec6\u7c92\u5ea6\u6d4b\u8bd5\u63d0\u5347LLM\u8bc4\u4f30\u5668\u7684\u53ef\u9760\u6027\uff0c\u5b9e\u9645\u5e94\u7528\u4e8e\u5de5\u4e1a\u573a\u666f\u652f\u6301\u6a21\u578b\u53d1\u5e03\u51b3\u7b56\u3002"}}
{"id": "2508.03640", "pdf": "https://arxiv.org/pdf/2508.03640", "abs": "https://arxiv.org/abs/2508.03640", "authors": ["Pedro Vasconcelos"], "title": "Teaching Introductory Functional Programming Using Haskelite", "categories": ["cs.PL", "D.3.2;D.3.4;K.3.1"], "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "Learning functional programming requires learning a substitution-based\ncomputational model. While substitution should be a familiar concept from\nhigh-school algebra, students often have difficulty applying it to new\nsettings, such as recursive definitions, algebraic data types and higher-order\nfunctions. Step-by-step interpreters have been shown to help beginners by\nclarifying misconceptions and improving understanding.\n  This paper reports on the experience of using a step-by-step tracing\ninterpreter for a subset of Haskell while teaching an introductory functional\nprogramming course at the University of Porto. We describe the use of the\ninterpreter, present some feedback obtained from students, reflect on the\nlessons learned and point directions for further work.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6559\u6388\u51fd\u6570\u5f0f\u7f16\u7a0b\u65f6\uff0c\u4f7f\u7528\u9010\u6b65\u8ffd\u8e2a\u89e3\u91ca\u5668\u6765\u5e2e\u52a9\u5b66\u751f\u7406\u89e3\u66ff\u6362\u6a21\u578b\u7684\u7ecf\u9a8c\u3002", "motivation": "\u5b66\u751f\u5728\u5b66\u4e60\u51fd\u6570\u5f0f\u7f16\u7a0b\u65f6\uff0c\u5e38\u5e38\u96be\u4ee5\u5c06\u4ee3\u6570\u4e2d\u7684\u66ff\u6362\u6982\u5ff5\u5e94\u7528\u4e8e\u9012\u5f52\u5b9a\u4e49\u3001\u4ee3\u6570\u6570\u636e\u7c7b\u578b\u548c\u9ad8\u9636\u51fd\u6570\u7b49\u65b0\u573a\u666f\u3002", "method": "\u5728\u6ce2\u5c14\u56fe\u5927\u5b66\u7684\u5165\u95e8\u8bfe\u7a0b\u4e2d\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a\u9488\u5bf9Haskell\u5b50\u96c6\u7684\u9010\u6b65\u8ffd\u8e2a\u89e3\u91ca\u5668\u3002", "result": "\u5b66\u751f\u4eec\u53cd\u9988\u8be5\u89e3\u91ca\u5668\u6709\u52a9\u4e8e\u6f84\u6e05\u8bef\u89e3\u5e76\u63d0\u5347\u7406\u89e3\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u7ecf\u9a8c\u548c\u6559\u8bad\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u65b9\u5411\u3002"}}
{"id": "2508.03415", "pdf": "https://arxiv.org/pdf/2508.03415", "abs": "https://arxiv.org/abs/2508.03415", "authors": ["Shivangi Nigam", "Adarsh Prasad Behera", "Shekhar Verma", "P. Nagabhushan"], "title": "Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "This paper is currently under review for publication in an IEEE\n  Transactions. If accepted, the copyright will be transferred to IEEE", "summary": "This paper presents Fd-CycleGAN, an image-to-image (I2I) translation\nframework that enhances latent representation learning to approximate real data\ndistributions. Building upon the foundation of CycleGAN, our approach\nintegrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to\ncapture fine-grained local pixel semantics while preserving structural\ncoherence from the source domain. We employ distribution-based loss metrics,\nincluding KL/JS divergence and log-based similarity measures, to explicitly\nquantify the alignment between real and generated image distributions in both\nspatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we\nconduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a\nsynthetically augmented Strike-off dataset. Compared to baseline CycleGAN and\nother state-of-the-art methods, our approach demonstrates superior perceptual\nquality, faster convergence, and improved mode diversity, particularly in\nlow-data regimes. By effectively capturing local and global distribution\ncharacteristics, Fd-CycleGAN achieves more visually coherent and semantically\nconsistent translations. Our results suggest that frequency-guided latent\nlearning significantly improves generalization in image translation tasks, with\npromising applications in document restoration, artistic style transfer, and\nmedical image synthesis. We also provide comparative insights with\ndiffusion-based generative models, highlighting the advantages of our\nlightweight adversarial approach in terms of training efficiency and\nqualitative output.", "AI": {"tldr": "Fd-CycleGAN\u901a\u8fc7\u7ed3\u5408Local Neighborhood Encoding\u548c\u9891\u7387\u611f\u77e5\u76d1\u7763\uff0c\u6539\u8fdb\u4e86CycleGAN\u7684\u6f5c\u5728\u8868\u793a\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u7ffb\u8bd1\u3002", "motivation": "\u73b0\u6709\u7684CycleGAN\u5728\u4f4e\u6570\u636e\u91cf\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\uff0cFd-CycleGAN\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u548c\u5206\u5e03\u5bf9\u9f50\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86Local Neighborhood Encoding\u548c\u9891\u7387\u611f\u77e5\u76d1\u7763\uff0c\u5e76\u7ed3\u5408KL/JS\u6563\u5ea6\u7b49\u5206\u5e03\u635f\u5931\u6765\u91cf\u5316\u751f\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u7684\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5728Horse2Zebra\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebfCycleGAN\u548c\u5176\u4ed6SOTA\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u611f\u77e5\u8d28\u91cf\u3001\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6a21\u6001\u591a\u6837\u6027\u3002", "conclusion": "Fd-CycleGAN\u5728\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6587\u6863\u4fee\u590d\u3001\u827a\u672f\u98ce\u683c\u8fc1\u79fb\u548c\u533b\u5b66\u56fe\u50cf\u5408\u6210\u7b49\u9886\u57df\u3002"}}
{"id": "2508.02816", "pdf": "https://arxiv.org/pdf/2508.02816", "abs": "https://arxiv.org/abs/2508.02816", "authors": ["Dylan Stow", "Russell Barnes", "Eren Kurshan", "Yuan Xie"], "title": "Thermal-Aware 3D Design for Side-Channel Information Leakage", "categories": ["cs.CR", "cs.ET"], "comment": null, "summary": "Side-channel attacks are important security challenges as they reveal\nsensitive information about on-chip activities. Among such attacks, the thermal\nside-channel has been shown to disclose the activities of key functional blocks\nand even encryption keys. This paper proposes a novel approach to proactively\nconceal critical activities in the functional layers while minimizing the power\ndissipation by (i) leveraging inherent characteristics of 3D integration to\nprotect from side-channel attacks and (ii) dynamically generating custom\nactivity patterns to match the activity to be concealed in the functional\nlayers. Experimental analysis shows that 3D technology combined with the\nproposed run-time algorithm effectively reduces the Side channel vulnerability\nFactor (SVF) below 0.05 and the Spatial Thermal Side-channel Factor (STSF)\nbelow 0.59.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u75283D\u96c6\u6210\u6280\u672f\u548c\u52a8\u6001\u6d3b\u52a8\u6a21\u5f0f\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u70ed\u4fa7\u4fe1\u9053\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u70ed\u4fa7\u4fe1\u9053\u653b\u51fb\u53ef\u4ee5\u6cc4\u9732\u82af\u7247\u5173\u952e\u529f\u80fd\u5757\u7684\u6d3b\u52a8\u751a\u81f3\u52a0\u5bc6\u5bc6\u94a5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e3b\u52a8\u4fdd\u62a4\u673a\u5236\u6765\u9690\u85cf\u5173\u952e\u6d3b\u52a8\u5e76\u964d\u4f4e\u529f\u8017\u3002", "method": "\u7ed3\u54083D\u96c6\u6210\u7684\u56fa\u6709\u7279\u6027\u53ca\u52a8\u6001\u751f\u6210\u7684\u81ea\u5b9a\u4e49\u6d3b\u52a8\u6a21\u5f0f\uff0c\u9690\u85cf\u529f\u80fd\u5c42\u4e2d\u7684\u5173\u952e\u6d3b\u52a8\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5c06\u4fa7\u4fe1\u9053\u8106\u5f31\u56e0\u5b50\uff08SVF\uff09\u964d\u81f30.05\u4ee5\u4e0b\uff0c\u7a7a\u95f4\u70ed\u4fa7\u4fe1\u9053\u56e0\u5b50\uff08STSF\uff09\u964d\u81f30.59\u4ee5\u4e0b\u3002", "conclusion": "3D\u6280\u672f\u4e0e\u8fd0\u884c\u65f6\u7b97\u6cd5\u7ed3\u5408\uff0c\u53ef\u9ad8\u6548\u4fdd\u62a4\u82af\u7247\u514d\u53d7\u70ed\u4fa7\u4fe1\u9053\u653b\u51fb\u3002"}}
{"id": "2508.03101", "pdf": "https://arxiv.org/pdf/2508.03101", "abs": "https://arxiv.org/abs/2508.03101", "authors": ["Sichao Wang", "Ramesh Raskar", "Mahesh Lambe", "Pradyumna Chari", "Rekha Singhal", "Shailja Gupta", "Rajesh Ranjan", "Ken Huang"], "title": "Using the NANDA Index Architecture in Practice: An Enterprise Perspective", "categories": ["cs.NI", "cs.AI", "cs.MA"], "comment": null, "summary": "The proliferation of autonomous AI agents represents a paradigmatic shift\nfrom traditional web architectures toward collaborative intelligent systems\nrequiring sophisticated mechanisms for discovery, authentication, capability\nverification, and secure collaboration across heterogeneous protocol\nenvironments. This paper presents a comprehensive framework addressing the\nfundamental infrastructure requirements for secure, trustworthy, and\ninteroperable AI agent ecosystems. We introduce the NANDA (Networked AI Agents\nin a Decentralized Architecture) framework, providing global agent discovery,\ncryptographically verifiable capability attestation through AgentFacts, and\ncross-protocol interoperability across Anthropic's Modal Context Protocol\n(MCP), Google's Agent-to-Agent (A2A), Microsoft's NLWeb, and standard HTTPS\ncommunications. NANDA implements Zero Trust Agentic Access (ZTAA) principles,\nextending traditional Zero Trust Network Access (ZTNA) to address autonomous\nagent security challenges including capability spoofing, impersonation attacks,\nand sensitive data leakage. The framework defines Agent Visibility and Control\n(AVC) mechanisms enabling enterprise governance while maintaining operational\nautonomy and regulatory compliance. Our approach transforms isolated AI agents\ninto an interconnected ecosystem of verifiable, trustworthy intelligent\nservices, establishing foundational infrastructure for large-scale autonomous\nagent deployment across enterprise and consumer environments. This work\naddresses the critical gap between current AI agent capabilities and\ninfrastructure requirements for secure, scalable, multi-agent collaboration,\npositioning the foundation for next-generation autonomous intelligent systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86NANDA\u6846\u67b6\uff0c\u89e3\u51b3\u81ea\u4e3bAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u7684\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\uff0c\u5305\u62ec\u5168\u5c40\u53d1\u73b0\u3001\u80fd\u529b\u9a8c\u8bc1\u548c\u8de8\u534f\u8bae\u4e92\u64cd\u4f5c\u6027\uff0c\u540c\u65f6\u5f15\u5165\u96f6\u4fe1\u4efb\u539f\u5219\u548c\u6cbb\u7406\u673a\u5236\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u4e2d\u7f3a\u4e4f\u5b89\u5168\u3001\u53ef\u9760\u4e14\u4e92\u64cd\u4f5c\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u963b\u788d\u4e86\u5927\u89c4\u6a21\u591a\u4ee3\u7406\u534f\u4f5c\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51faNANDA\u6846\u67b6\uff0c\u6574\u5408\u5168\u7403\u4ee3\u7406\u53d1\u73b0\u3001\u5bc6\u7801\u5b66\u9a8c\u8bc1\u7684\u80fd\u529b\u8bc1\u660e\uff08AgentFacts\uff09\u3001\u8de8\u534f\u8bae\u4e92\u64cd\u4f5c\u6027\uff0c\u5e76\u5e94\u7528\u96f6\u4fe1\u4efb\u4ee3\u7406\u8bbf\u95ee\uff08ZTAA\uff09\u539f\u5219\u3002", "result": "NANDA\u5b9e\u73b0\u4e86\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff0c\u652f\u6301\u4f01\u4e1a\u6cbb\u7406\u548c\u5408\u89c4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4ee3\u7406\u7684\u81ea\u4e3b\u6027\u3002", "conclusion": "NANDA\u586b\u8865\u4e86AI\u4ee3\u7406\u80fd\u529b\u4e0e\u5b89\u5168\u57fa\u7840\u8bbe\u65bd\u9700\u6c42\u4e4b\u95f4\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.03039", "pdf": "https://arxiv.org/pdf/2508.03039", "abs": "https://arxiv.org/abs/2508.03039", "authors": ["Yiran Meng", "Junhong Ye", "Wei Zhou", "Guanghui Yue", "Xudong Mao", "Ruomei Wang", "Baoquan Zhao"], "title": "VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Cross-video question answering presents significant challenges beyond\ntraditional single-video understanding, particularly in establishing meaningful\nconnections across video streams and managing the complexity of multi-source\ninformation retrieval. We introduce VideoForest, a novel framework that\naddresses these challenges through person-anchored hierarchical reasoning. Our\napproach leverages person-level features as natural bridge points between\nvideos, enabling effective cross-video understanding without requiring\nend-to-end training. VideoForest integrates three key innovations: 1) a\nhuman-anchored feature extraction mechanism that employs ReID and tracking\nalgorithms to establish robust spatiotemporal relationships across multiple\nvideo sources; 2) a multi-granularity spanning tree structure that\nhierarchically organizes visual content around person-level trajectories; and\n3) a multi-agent reasoning framework that efficiently traverses this\nhierarchical structure to answer complex cross-video queries. To evaluate our\napproach, we develop CrossVideoQA, a comprehensive benchmark dataset\nspecifically designed for person-centric cross-video analysis. Experimental\nresults demonstrate VideoForest's superior performance in cross-video reasoning\ntasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior\nanalysis, and 51.67% in summarization and reasoning, significantly\noutperforming existing methods. Our work establishes a new paradigm for\ncross-video understanding by unifying multiple video streams through\nperson-level features, enabling sophisticated reasoning across distributed\nvisual information while maintaining computational efficiency.", "AI": {"tldr": "VideoForest\u6846\u67b6\u901a\u8fc7\u4eba\u7269\u951a\u5b9a\u7684\u5206\u5c42\u63a8\u7406\u89e3\u51b3\u8de8\u89c6\u9891\u95ee\u7b54\u7684\u6311\u6218\uff0c\u6574\u5408\u4e86\u4eba\u7269\u7279\u5f81\u63d0\u53d6\u3001\u591a\u7c92\u5ea6\u6811\u7ed3\u6784\u548c\u591a\u4ee3\u7406\u63a8\u7406\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8de8\u89c6\u9891\u95ee\u7b54\u9762\u4e34\u591a\u89c6\u9891\u6d41\u95f4\u5efa\u7acb\u8054\u7cfb\u548c\u591a\u6e90\u4fe1\u606f\u68c0\u7d22\u7684\u590d\u6742\u6027\uff0c\u4f20\u7edf\u5355\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u91c7\u7528\u4eba\u7269\u7279\u5f81\u4f5c\u4e3a\u6865\u6881\uff0c\u7ed3\u5408ReID\u548c\u8ddf\u8e2a\u7b97\u6cd5\u3001\u591a\u7c92\u5ea6\u6811\u7ed3\u6784\u548c\u591a\u4ee3\u7406\u63a8\u7406\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cVideoForest\u5728\u4eba\u7269\u8bc6\u522b\u3001\u884c\u4e3a\u5206\u6790\u53ca\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "VideoForest\u901a\u8fc7\u4eba\u7269\u7ea7\u7279\u5f81\u7edf\u4e00\u591a\u89c6\u9891\u6d41\uff0c\u5b9e\u73b0\u4e86\u8de8\u89c6\u9891\u7406\u89e3\u7684\u65b0\u8303\u5f0f\uff0c\u517c\u5177\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2508.02817", "pdf": "https://arxiv.org/pdf/2508.02817", "abs": "https://arxiv.org/abs/2508.02817", "authors": ["Nilesh Kumar Sahu", "Aditya Sneh", "Snehil Gupta", "Haroon R Lone"], "title": "Real-World Receptivity to Adaptive Mental Health Interventions: Findings from an In-the-Wild Study", "categories": ["cs.HC", "cs.AI", "cs.CY", "eess.SP"], "comment": null, "summary": "The rise of mobile health (mHealth) technologies has enabled real-time\nmonitoring and intervention for mental health conditions using passively sensed\nsmartphone data. Building on these capabilities, Just-in-Time Adaptive\nInterventions (JITAIs) seek to deliver personalized support at opportune\nmoments, adapting to users' evolving contexts and needs. Although prior\nresearch has examined how context affects user responses to generic\nnotifications and general mHealth messages, relatively little work has explored\nits influence on engagement with actual mental health interventions.\nFurthermore, while much of the existing research has focused on detecting when\nusers might benefit from an intervention, less attention has been paid to\nunderstanding receptivity, i.e., users' willingness and ability to engage with\nand act upon the intervention.\n  In this study, we investigate user receptivity through two components:\nacceptance(acknowledging or engaging with a prompt) and feasibility (ability to\nact given situational constraints). We conducted a two-week in-the-wild study\nwith 70 students using a custom Android app, LogMe, which collected passive\nsensor data and active context reports to prompt mental health interventions.\nThe adaptive intervention module was built using Thompson Sampling, a\nreinforcement learning algorithm. We address four research questions relating\nsmartphone features and self-reported contexts to acceptance and feasibility,\nand examine whether an adaptive reinforcement learning approach can optimize\nintervention delivery by maximizing a combined receptivity reward. Our results\nshow that several types of passively sensed data significantly influenced user\nreceptivity to interventions. Our findings contribute insights into the design\nof context-aware, adaptive interventions that are not only timely but also\nactionable in real-world settings.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u6237\u5bf9\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u7684\u63a5\u53d7\u5ea6\uff08\u63a5\u53d7\u548c\u53ef\u884c\u6027\uff09\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u5e72\u9884\u65f6\u673a\u3002", "motivation": "\u79fb\u52a8\u5065\u5eb7\u6280\u672f\u867d\u80fd\u5b9e\u65f6\u76d1\u6d4b\u5fc3\u7406\u5065\u5eb7\uff0c\u4f46\u5bf9\u7528\u6237\u63a5\u53d7\u5e72\u9884\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u5c24\u5176\u662f\u5982\u4f55\u6839\u636e\u60c5\u5883\u4f18\u5316\u5e72\u9884\u3002", "method": "\u901a\u8fc7\u4e24\u5468\u7684\u5b9e\u5730\u7814\u7a76\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49App\u6536\u96c6\u88ab\u52a8\u4f20\u611f\u5668\u6570\u636e\u548c\u4e3b\u52a8\u60c5\u5883\u62a5\u544a\uff0c\u91c7\u7528Thompson Sampling\u7b97\u6cd5\u4f18\u5316\u5e72\u9884\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u88ab\u52a8\u611f\u77e5\u6570\u636e\u663e\u8457\u5f71\u54cd\u7528\u6237\u5bf9\u5e72\u9884\u7684\u63a5\u53d7\u5ea6\uff0c\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u4f18\u5316\u5e72\u9884\u65f6\u673a\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u60c5\u5883\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5e72\u9884\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5f3a\u8c03\u5e72\u9884\u9700\u517c\u987e\u53ca\u65f6\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u3002"}}
{"id": "2508.03674", "pdf": "https://arxiv.org/pdf/2508.03674", "abs": "https://arxiv.org/abs/2508.03674", "authors": ["Abhishek Vijaya Kumar", "Eric Ding", "Arjun Devraj", "Rachee Singh"], "title": "Morphlux: Programmable chip-to-chip photonic fabrics in multi-accelerator servers for ML", "categories": ["cs.NI", "cs.AR", "cs.LG"], "comment": null, "summary": "We optically interconnect accelerator chips (e.g., GPUs, TPUs) within compute\nservers using newly viable programmable chip-to-chip photonic fabrics. In\ncontrast, today, commercial multi-accelerator compute servers that are\nworkhorses of ML, use electrical interconnects to network accelerator chips in\nthe server. However, recent trends have shown an interconnect bandwidth wall\ncaused by accelerator FLOPS scaling at a faster rate than the bandwidth of the\ninterconnect between accelerators in the same server. This has led to\nunder-utilization and idling of GPU resources in cloud datacenters. We develop\nMorphlux, a server-scale programmable photonic fabric, to interconnect\naccelerators within servers. We show that augmenting state-of-the-art photonic\nML-centric datacenters with Morphlux can improve the bandwidth of tenant\ncompute allocations by up to 66% and reduce compute fragmentation by up to 70%.\nWe develop a novel end-to-end hardware prototype of Morphlux to demonstrate\nthese performance benefits, which translate to 1.72x improvement in training\nthroughput of ML models. By rapidly programming the server-scale fabric in our\nhardware testbed, Morphlux can logically replace a failed accelerator chip in\n1.2 seconds.", "AI": {"tldr": "Morphlux\u662f\u4e00\u79cd\u53ef\u7f16\u7a0b\u5149\u5b50\u4e92\u8fde\u67b6\u6784\uff0c\u7528\u4e8e\u670d\u52a1\u5668\u5185\u52a0\u901f\u5668\u82af\u7247\u7684\u5149\u5b66\u4e92\u8fde\uff0c\u89e3\u51b3\u7535\u6c14\u4e92\u8fde\u5e26\u5bbd\u4e0d\u8db3\u5bfc\u81f4\u7684\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u63d0\u5347ML\u6a21\u578b\u8bad\u7ec3\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u670d\u52a1\u5668\u4e2d\u52a0\u901f\u5668\u82af\u7247\u95f4\u7684\u7535\u6c14\u4e92\u8fde\u5e26\u5bbd\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8d44\u6e90\u95f2\u7f6e\u3002\u5229\u7528\u5149\u4e92\u8fde\u6280\u672f\u7a81\u7834\u5e26\u5bbd\u9650\u5236\u662f\u7814\u7a76\u52a8\u673a\u3002", "method": "\u5f00\u53d1Morphlux\uff0c\u4e00\u79cd\u670d\u52a1\u5668\u89c4\u6a21\u7684\u53ef\u7f16\u7a0b\u5149\u5b50\u4e92\u8fde\u67b6\u6784\uff0c\u901a\u8fc7\u5149\u5b66\u4e92\u8fde\u52a0\u901f\u5668\u82af\u7247\uff0c\u5e76\u5c55\u793a\u5176\u786c\u4ef6\u539f\u578b\u3002", "result": "Morphlux\u5c06\u79df\u6237\u8ba1\u7b97\u5206\u914d\u7684\u5e26\u5bbd\u63d0\u534766%\uff0c\u8ba1\u7b97\u788e\u7247\u51cf\u5c1170%\uff0c\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u9ad81.72\u500d\uff0c\u5e76\u80fd\u57281.2\u79d2\u5185\u903b\u8f91\u66ff\u6362\u6545\u969c\u82af\u7247\u3002", "conclusion": "Morphlux\u8bc1\u660e\u4e86\u5149\u4e92\u8fde\u5728\u63d0\u5347\u670d\u52a1\u5668\u5185\u52a0\u901f\u5668\u4e92\u8fde\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.02758", "pdf": "https://arxiv.org/pdf/2508.02758", "abs": "https://arxiv.org/abs/2508.02758", "authors": ["Yihao Ang", "Qiang Wang", "Qiang Huang", "Yifan Bao", "Xinyu Xi", "Anthony K. H. Tung", "Chen Jin", "Zhiyong Huang"], "title": "CTBench: Cryptocurrency Time Series Generation Benchmark", "categories": ["q-fin.ST", "cs.AI", "cs.CE", "cs.DB", "cs.LG"], "comment": "14 pages, 14 figures, and 3 tables", "summary": "Synthetic time series are essential tools for data augmentation, stress\ntesting, and algorithmic prototyping in quantitative finance. However, in\ncryptocurrency markets, characterized by 24/7 trading, extreme volatility, and\nrapid regime shifts, existing Time Series Generation (TSG) methods and\nbenchmarks often fall short, jeopardizing practical utility. Most prior work\n(1) targets non-financial or traditional financial domains, (2) focuses\nnarrowly on classification and forecasting while neglecting crypto-specific\ncomplexities, and (3) lacks critical financial evaluations, particularly for\ntrading applications. To address these gaps, we introduce \\textsf{CTBench}, the\nfirst comprehensive TSG benchmark tailored for the cryptocurrency domain.\n\\textsf{CTBench} curates an open-source dataset from 452 tokens and evaluates\nTSG models across 13 metrics spanning 5 key dimensions: forecasting accuracy,\nrank fidelity, trading performance, risk assessment, and computational\nefficiency. A key innovation is a dual-task evaluation framework: (1) the\n\\emph{Predictive Utility} task measures how well synthetic data preserves\ntemporal and cross-sectional patterns for forecasting, while (2) the\n\\emph{Statistical Arbitrage} task assesses whether reconstructed series support\nmean-reverting signals for trading. We benchmark eight representative models\nfrom five methodological families over four distinct market regimes, uncovering\ntrade-offs between statistical fidelity and real-world profitability. Notably,\n\\textsf{CTBench} offers model ranking analysis and actionable guidance for\nselecting and deploying TSG models in crypto analytics and strategy\ndevelopment.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86CTBench\uff0c\u4e00\u4e2a\u9488\u5bf9\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u7684\u65f6\u95f4\u5e8f\u5217\u751f\u6210\uff08TSG\uff09\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u586b\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8be5\u9886\u57df\u7684\u4e0d\u8db3\u3002", "motivation": "\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u7684\u7279\u6b8a\u6027\uff0824/7\u4ea4\u6613\u3001\u9ad8\u6ce2\u52a8\u6027\u548c\u5feb\u901f\u53d8\u5316\uff09\u5bfc\u81f4\u73b0\u6709TSG\u65b9\u6cd5\u4e0d\u9002\u7528\uff0c\u7f3a\u4e4f\u91d1\u878d\u573a\u666f\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86CTBench\uff0c\u5305\u542b\u5f00\u6e90\u6570\u636e\u96c6\u548c13\u9879\u6307\u6807\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u9884\u6d4b\u3001\u4ea4\u6613\u3001\u98ce\u9669\u7b49\u65b9\u9762\u7684\u8868\u73b0\uff0c\u91c7\u7528\u53cc\u4efb\u52a1\u6846\u67b6\u3002", "result": "\u6d4b\u8bd5\u4e868\u79cd\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u7edf\u8ba1\u4fdd\u771f\u5ea6\u4e0e\u5b9e\u9645\u76c8\u5229\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u4f9b\u6a21\u578b\u9009\u62e9\u548c\u90e8\u7f72\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "conclusion": "CTBench\u4e3a\u52a0\u5bc6\u8d27\u5e01\u9886\u57df\u7684TSG\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u51c6\uff0c\u5177\u6709\u5b9e\u7528\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2508.02866", "pdf": "https://arxiv.org/pdf/2508.02866", "abs": "https://arxiv.org/abs/2508.02866", "authors": ["Renan Souza", "Amal Gueroudji", "Stephen DeWitt", "Daniel Rosendo", "Tirthankar Ghosal", "Robert Ross", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "title": "PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows", "categories": ["cs.DC", "cs.DB", "68T42, 68T30, 68P20, 68Q85, 68M14,", "D.2.12; H.2.4; I.2.11; C.2.4; H.3.4"], "comment": "Paper under peer-reviewed evaluation", "summary": "Foundation models, such as Large Language Models (LLMs), are increasingly\nused as core components of AI agents in complex, large-scale workflows across\nfederated and heterogeneous environments. In agentic workflows, autonomous\nagents plan tasks, interact with humans and peers, and shape scientific\noutcomes. This makes transparency, traceability, reproducibility, and\nreliability essential. However, AI-based agents can hallucinate or reason\nincorrectly, and their decisions may propagate errors through the workflow,\nespecially when one agent's output feeds into another's input. Therefore,\nfine-grained provenance is essential to link agent decisions, their end-to-end\ncontext, and downstream impacts. While provenance techniques have long\nsupported reproducibility and workflow data understanding, they fail to capture\nand relate agent-centric metadata (prompts, responses, and decisions) with the\nrest of the workflow. In this paper, we introduce PROV-AGENT, a provenance\nmodel that extends W3C PROV and leverages the Model Context Protocol (MCP) to\nintegrate agent interactions into end-to-end workflow provenance. Our\ncontributions include: (1) a provenance model tailored for agentic workflows,\n(2) a near real-time, open-source system for capturing agentic provenance, and\n(3) a cross-facility evaluation spanning edge, cloud, and HPC environments,\ndemonstrating support for critical provenance queries and agent reliability\nanalysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPROV-AGENT\u6a21\u578b\uff0c\u6269\u5c55W3C PROV\u5e76\u5229\u7528MCP\uff0c\u4ee5\u6355\u83b7\u4ee3\u7406\u5728\u5de5\u4f5c\u6d41\u4e2d\u7684\u4ea4\u4e92\uff0c\u63d0\u5347\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u7531\u4e8eAI\u4ee3\u7406\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u53ef\u80fd\u4ea7\u751f\u9519\u8bef\u4e14\u9519\u8bef\u4f20\u64ad\u96be\u4ee5\u8ffd\u8e2a\uff0c\u9700\u8981\u7ec6\u7c92\u5ea6\u6eaf\u6e90\u6765\u94fe\u63a5\u4ee3\u7406\u51b3\u7b56\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u63d0\u51faPROV-AGENT\u6a21\u578b\uff0c\u6269\u5c55W3C PROV\u6807\u51c6\u5e76\u7ed3\u5408MCP\uff0c\u6574\u5408\u4ee3\u7406\u4ea4\u4e92\u5230\u7aef\u5230\u7aef\u5de5\u4f5c\u6d41\u6eaf\u6e90\u4e2d\u3002", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5f00\u6e90\u7cfb\u7edf\uff0c\u652f\u6301\u8de8\u73af\u5883\u7684\u6eaf\u6e90\u67e5\u8be2\u548c\u4ee3\u7406\u53ef\u9760\u6027\u5206\u6790\u3002", "conclusion": "PROV-AGENT\u6709\u6548\u63d0\u5347\u4e86\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\uff0c\u652f\u6301\u5173\u952e\u6eaf\u6e90\u5206\u6790\u3002"}}
{"id": "2508.03574", "pdf": "https://arxiv.org/pdf/2508.03574", "abs": "https://arxiv.org/abs/2508.03574", "authors": ["Michael Benedikt", "Chia-Hsuan Lu", "Tony Tan"], "title": "Analysis of logics with arithmetic", "categories": ["cs.LO"], "comment": null, "summary": "We present new results on finite satisfiability of logics with counting and\narithmetic. This includes tight bounds on the complexity for two-variable logic\nwith counting and cardinality comparisons between unary formulas, and also on\nlogics with so-called local Presburger quantifiers. In the process, we provide\nsimpler proofs of some key prior results on finite satisfiability and\nsemi-linearity of the spectrum for these logics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5173\u4e8e\u8ba1\u6570\u548c\u7b97\u672f\u903b\u8f91\u7684\u6709\u9650\u53ef\u6ee1\u8db3\u6027\u7684\u65b0\u7ed3\u679c\uff0c\u5305\u62ec\u4e8c\u53d8\u91cf\u903b\u8f91\u4e0e\u4e00\u5143\u516c\u5f0f\u95f4\u7684\u57fa\u6570\u6bd4\u8f83\u7684\u7d27\u590d\u6742\u5ea6\u754c\u9650\uff0c\u4ee5\u53ca\u5c40\u90e8Presburger\u91cf\u8bcd\u903b\u8f91\u7684\u754c\u9650\u3002", "motivation": "\u7814\u7a76\u6709\u9650\u53ef\u6ee1\u8db3\u6027\u5728\u903b\u8f91\u548c\u7b97\u672f\u4e2d\u7684\u590d\u6742\u6027\uff0c\u7b80\u5316\u5173\u952e\u7ed3\u679c\u7684\u8bc1\u660e\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e8c\u53d8\u91cf\u903b\u8f91\u4e0e\u4e00\u5143\u516c\u5f0f\u7684\u57fa\u6570\u6bd4\u8f83\uff0c\u4ee5\u53ca\u5c40\u90e8Presburger\u91cf\u8bcd\u903b\u8f91\uff0c\u91cd\u65b0\u68b3\u7406\u4e86\u76f8\u5173\u7406\u8bba\u3002", "result": "\u5f97\u51fa\u4e86\u7d27\u7684\u590d\u6742\u5ea6\u754c\u9650\uff0c\u5e76\u7b80\u5316\u4e86\u73b0\u6709\u7ed3\u679c\u7684\u8bc1\u660e\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63a8\u8fdb\u4e86\u5bf9\u6709\u9650\u53ef\u6ee1\u8db3\u6027\u7684\u7406\u89e3\uff0c\u8fd8\u7b80\u5316\u4e86\u76f8\u5173\u903b\u8f91\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.02968", "pdf": "https://arxiv.org/pdf/2508.02968", "abs": "https://arxiv.org/abs/2508.02968", "authors": ["Shavindra Wickramathilaka", "John Grundy", "Kashumi Madampe", "Omar Haggag"], "title": "Developer Perceptions on Utilising Low-Code Approaches to Build Accessible and Adaptive Applications for Seniors", "categories": ["cs.SE"], "comment": "This paper has been submitted to ACM Transactions on Software\n  Engineering and Methodology (TOSEM)", "summary": "The global ageing population presents a growing societal challenge, creating\nan urgent need for inclusive technologies that promote autonomy among older\nadults. Software practitioners can address this by delivering digital services\nthat enhance seniors' independence and reduce reliance on routine support from\nfamily members and healthcare infrastructure. However, traditional development\npractices, constrained by time and resources, often result in applications with\nmajor accessibility and personalisation barriers. Increasing pressure from\nregulatory requirements, such as the European Accessibility Act (EAA), and the\npersonal empathy many developers feel toward supporting their older loved ones\nand their own future selves have created a demand for tools that support the\ndevelopment of accessible and adaptive software. To address this demand, this\npaper presents an interview-based empirical study with 18 software\npractitioners, evaluating AdaptForge: a low-code model-driven engineering (MDE)\ntool that enables the efficient creation of accessible and adaptive\napplications for senior users by mitigating development constraints through\nautomated code generation. Based on these insights, we identify developer\nexpectations for adopting such tools as industry-standard solutions and provide\nempirically grounded recommendations for designing low-code tools that support\naccessible and adaptive software development.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u4f4e\u4ee3\u7801\u5de5\u5177AdaptForge\u5e2e\u52a9\u5f00\u53d1\u8005\u9ad8\u6548\u521b\u5efa\u9002\u5408\u8001\u5e74\u4eba\u7684\u65e0\u969c\u788d\u548c\u81ea\u9002\u5e94\u5e94\u7528\u3002", "motivation": "\u5168\u7403\u8001\u9f84\u5316\u95ee\u9898\u65e5\u76ca\u4e25\u5cfb\uff0c\u4e9f\u9700\u6280\u672f\u624b\u6bb5\u63d0\u5347\u8001\u5e74\u4eba\u7684\u81ea\u4e3b\u6027\u3002\u5f00\u53d1\u8005\u9762\u4e34\u65f6\u95f4\u548c\u8d44\u6e90\u9650\u5236\uff0c\u96be\u4ee5\u6ee1\u8db3\u65e0\u969c\u788d\u548c\u4e2a\u6027\u5316\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8bbf\u8c0818\u4f4d\u8f6f\u4ef6\u4ece\u4e1a\u8005\uff0c\u8bc4\u4f30\u4f4e\u4ee3\u7801\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\u5de5\u5177AdaptForge\u7684\u6548\u679c\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86\u5f00\u53d1\u8005\u5bf9\u8fd9\u7c7b\u5de5\u5177\u7684\u671f\u671b\uff0c\u5e76\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5efa\u8bae\u3002", "conclusion": "\u4f4e\u4ee3\u7801\u5de5\u5177\u6709\u6f5c\u529b\u6210\u4e3a\u884c\u4e1a\u6807\u51c6\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u65e0\u969c\u788d\u548c\u81ea\u9002\u5e94\u8f6f\u4ef6\u5f00\u53d1\u3002"}}
{"id": "2508.03113", "pdf": "https://arxiv.org/pdf/2508.03113", "abs": "https://arxiv.org/abs/2508.03113", "authors": ["John Zinky", "Hema Seshadri", "Mahesh Lambe", "Pradyumna Chari", "Ramesh Raskar"], "title": "NANDA Adaptive Resolver: Architecture for Dynamic Resolution of AI Agent Names", "categories": ["cs.NI", "cs.AI", "cs.MA"], "comment": null, "summary": "AdaptiveResolver is a dynamic microservice architecture designed to address\nthe limitations of static endpoint resolution for AI agent communication in\ndistributed, heterogeneous environments. Unlike traditional DNS or static URLs,\nAdaptiveResolver enables context-aware, real-time selection of communication\nendpoints based on factors such as geographic location, system load, agent\ncapabilities, and security threats. Agents advertise their Agent Name and\ncontext requirements through Agent Fact cards in an Agent Registry/Index. A\nrequesting Agent discovers a Target Agent using the registry. The Requester\nAgent can then resolve the Target Agent Name to obtain a tailored communication\nchannel to the agent based on actual environmental context between the agents.\nThe architecture supports negotiation of trust, quality of service, and\nresource constraints, facilitating flexible, secure, and scalable\nagent-to-agent interactions that go beyond the classic client-server model.\nAdaptiveResolver provides a foundation for robust, future-proof agent\ncommunication that can evolve with increasing ecosystem complexity.", "AI": {"tldr": "AdaptiveResolver\u662f\u4e00\u79cd\u52a8\u6001\u5fae\u670d\u52a1\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u5206\u5e03\u5f0f\u5f02\u6784\u73af\u5883\u4e2dAI\u4ee3\u7406\u901a\u4fe1\u7684\u9759\u6001\u7aef\u70b9\u89e3\u6790\u9650\u5236\u3002", "motivation": "\u4f20\u7edfDNS\u6216\u9759\u6001URL\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u73af\u5883\u4e2d\u7684\u4ee3\u7406\u901a\u4fe1\u9700\u6c42\uff0cAdaptiveResolver\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u5b9e\u65f6\u9009\u62e9\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4ee3\u7406\u901a\u8fc7Agent Fact\u5361\u7247\u5728\u6ce8\u518c\u8868\u4e2d\u5e7f\u544a\u540d\u79f0\u548c\u4e0a\u4e0b\u6587\u9700\u6c42\uff0c\u8bf7\u6c42\u4ee3\u7406\u53ef\u901a\u8fc7\u6ce8\u518c\u8868\u53d1\u73b0\u76ee\u6807\u4ee3\u7406\u5e76\u83b7\u53d6\u5b9a\u5236\u5316\u901a\u4fe1\u901a\u9053\u3002", "result": "\u8be5\u67b6\u6784\u652f\u6301\u4fe1\u4efb\u3001\u670d\u52a1\u8d28\u91cf\u548c\u8d44\u6e90\u7ea6\u675f\u7684\u534f\u5546\uff0c\u5b9e\u73b0\u7075\u6d3b\u3001\u5b89\u5168\u4e14\u53ef\u6269\u5c55\u7684\u4ee3\u7406\u95f4\u4ea4\u4e92\u3002", "conclusion": "AdaptiveResolver\u4e3a\u672a\u6765\u590d\u6742\u7684\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u6f14\u8fdb\u7684\u901a\u4fe1\u57fa\u7840\u3002"}}
{"id": "2508.03397", "pdf": "https://arxiv.org/pdf/2508.03397", "abs": "https://arxiv.org/abs/2508.03397", "authors": ["Xinzhu Li", "Juepeng Zheng", "Yikun Chen", "Xudong Mao", "Guanghui Yue", "Wei Zhou", "Chenlei Lv", "Ruomei Wang", "Fan Zhou", "Baoquan Zhao"], "title": "DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Robust gait recognition requires highly discriminative representations, which\nare closely tied to input modalities. While binary silhouettes and skeletons\nhave dominated recent literature, these 2D representations fall short of\ncapturing sufficient cues that can be exploited to handle viewpoint variations,\nand capture finer and meaningful details of gait. In this paper, we introduce a\nnovel framework, termed DepthGait, that incorporates RGB-derived depth maps and\nsilhouettes for enhanced gait recognition. Specifically, apart from the 2D\nsilhouette representation of the human body, the proposed pipeline explicitly\nestimates depth maps from a given RGB image sequence and uses them as a new\nmodality to capture discriminative features inherent in human locomotion. In\naddition, a novel multi-scale and cross-level fusion scheme has also been\ndeveloped to bridge the modality gap between depth maps and silhouettes.\nExtensive experiments on standard benchmarks demonstrate that the proposed\nDepthGait achieves state-of-the-art performance compared to peer methods and\nattains an impressive mean rank-1 accuracy on the challenging datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDepthGait\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408RGB\u6df1\u5ea6\u56fe\u548c\u526a\u5f71\u6765\u63d0\u5347\u6b65\u6001\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e2d\u76842D\u526a\u5f71\u548c\u9aa8\u9abc\u6a21\u578b\u5728\u89c6\u89d2\u53d8\u5316\u548c\u7ec6\u8282\u6355\u6349\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u8f93\u5165\u6a21\u6001\u6765\u63d0\u9ad8\u5224\u522b\u6027\u3002", "method": "DepthGait\u6846\u67b6\u4eceRGB\u56fe\u50cf\u5e8f\u5217\u4e2d\u4f30\u8ba1\u6df1\u5ea6\u56fe\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u548c\u8de8\u7ea7\u522b\u878d\u5408\u65b9\u6848\u6765\u5f25\u5408\u6df1\u5ea6\u56fe\u4e0e\u526a\u5f71\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u5f02\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDepthGait\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684rank-1\u51c6\u786e\u7387\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u6df1\u5ea6\u56fe\u7684\u5f15\u5165\u548c\u591a\u6a21\u6001\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u6b65\u6001\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.02823", "pdf": "https://arxiv.org/pdf/2508.02823", "abs": "https://arxiv.org/abs/2508.02823", "authors": ["Wenshuo Zhang", "Leixian Shen", "Shuchang Xu", "Jindu Wang", "Jian Zhao", "Huamin Qu", "Linping Yuan"], "title": "NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "comment": "Accepted in UIST 2025", "summary": "Conversational LLMs have been widely adopted by domain users with limited\nprogramming experience to solve domain problems. However, these users often\nface misalignment between their intent and generated code, resulting in\nfrustration and rounds of clarification. This work first investigates the cause\nof this misalignment, which dues to bidirectional ambiguity: both user intents\nand coding tasks are inherently nonlinear, yet must be expressed and\ninterpreted through linear prompts and code sequences. To address this, we\npropose direct intent-task matching, a new human-LLM interaction paradigm that\nexternalizes and enables direct manipulation of the LLM understanding, i.e.,\nthe coding tasks and their relationships inferred by the LLM prior to code\ngeneration. As a proof-of-concept, this paradigm is then implemented in\nNeuroSync, which employs a knowledge distillation pipeline to extract LLM\nunderstanding, user intents, and their mappings, and enhances the alignment by\nallowing users to intuitively inspect and edit them via visualizations. We\nevaluate the algorithmic components of NeuroSync via technical experiments, and\nassess its overall usability and effectiveness via a user study (N=12). The\nresults show that it enhances intent-task alignment, lowers cognitive effort,\nand improves coding efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNeuroSync\uff0c\u901a\u8fc7\u76f4\u63a5\u610f\u56fe-\u4efb\u52a1\u5339\u914d\u89e3\u51b3\u7528\u6237\u610f\u56fe\u4e0eLLM\u751f\u6210\u4ee3\u7801\u4e4b\u95f4\u7684\u53cc\u5411\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u5bf9\u9f50\u6548\u679c\u548c\u7f16\u7801\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u610f\u56fe\u4e0e\u751f\u6210\u4ee3\u7801\u95f4\u7684\u504f\u5dee\u95ee\u9898\uff0c\u56e0\u7528\u6237\u610f\u56fe\u548c\u7f16\u7801\u4efb\u52a1\u5747\u975e\u7ebf\u6027\uff0c\u9700\u901a\u8fc7\u7ebf\u6027\u63d0\u793a\u548c\u4ee3\u7801\u5e8f\u5217\u8868\u8fbe\u3002", "method": "\u63d0\u51fa\u76f4\u63a5\u610f\u56fe-\u4efb\u52a1\u5339\u914d\u8303\u5f0f\uff0c\u5b9e\u73b0LLM\u7406\u89e3\u7684\u5916\u90e8\u5316\u548c\u53ef\u89c6\u5316\u7f16\u8f91\uff0c\u5e76\u901a\u8fc7NeuroSync\u5b9e\u73b0\u77e5\u8bc6\u84b8\u998f\u4e0e\u7528\u6237\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u663e\u793aNeuroSync\u589e\u5f3a\u610f\u56fe-\u4efb\u52a1\u5bf9\u9f50\uff0c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u62c5\uff0c\u63d0\u9ad8\u7f16\u7801\u6548\u7387\u3002", "conclusion": "\u76f4\u63a5\u610f\u56fe-\u4efb\u52a1\u5339\u914d\u548cNeuroSync\u80fd\u6709\u6548\u89e3\u51b3\u7528\u6237\u4e0eLLM\u4ea4\u4e92\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.03418", "pdf": "https://arxiv.org/pdf/2508.03418", "abs": "https://arxiv.org/abs/2508.03418", "authors": ["Ron van der Meyden"], "title": "Optimal Simultaneous Byzantine Agreement, Common Knowledge and Limited Information Exchange", "categories": ["cs.DC", "cs.LO", "68W15, 03B42, 03B70, 68Q60, 68Q85, 68M15"], "comment": null, "summary": "In order to develop solutions that perform actions as early as possible,\nanalysis of distributed algorithms using epistemic logic has generally\nconcentrated on ``full information protocols'', which may be inefficient with\nrespect to space and computation time. The paper reconsiders the epistemic\nanalysis of the problem of Simultaneous Byzantine Agreement with respect to\nweaker, but more practical, exchanges of information. The paper first clarifies\nsome issues concerning both the specification of this problem and the knowledge\nbased program characterizing its solution, concerning the distinction between\nthe notions of ``nonfaulty'' and ``not yet failed'', on which there are\nvariances in the literature. It is then shown that, when implemented relative\nto a given failure model and an information exchange protocol satisfying\ncertain conditions, this knowledge based program yields a protocol that is\noptimal relative to solutions using the same information exchange. Conditions\nare also identified under which this implementation is also an optimum, but an\nexample is provided that shows this does not hold in general.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u57fa\u4e8e\u77e5\u8bc6\u5206\u6790\u7684\u540c\u6b65\u62dc\u5360\u5ead\u534f\u8bae\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u4fe1\u606f\u4ea4\u6362\u534f\u8bae\uff0c\u5e76\u6f84\u6e05\u4e86\u76f8\u5173\u6982\u5ff5\u4e0a\u7684\u6df7\u6dc6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5206\u5e03\u5f0f\u7b97\u6cd5\u4e2d\u5168\u4fe1\u606f\u534f\u8bae\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u66f4\u5b9e\u7528\u7684\u4fe1\u606f\u4ea4\u6362\u65b9\u5f0f\uff0c\u4ee5\u4f18\u5316\u534f\u8bae\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002", "method": "\u901a\u8fc7\u77e5\u8bc6\u57fa\u7840\u7a0b\u5e8f\uff0c\u7ed3\u5408\u7279\u5b9a\u7684\u6545\u969c\u6a21\u578b\u548c\u4fe1\u606f\u4ea4\u6362\u534f\u8bae\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6848\u4f18\u4e8e\u4f20\u7edf\u5168\u4fe1\u606f\u534f\u8bae\uff0c\u4f46\u4e5f\u5b58\u5728\u4e0d\u9002\u7528\u7684\u60c5\u51b5\u3002", "conclusion": "\u672c\u6587\u4e3a\u540c\u6b65\u62dc\u5360\u5ead\u534f\u8bae\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u660e\u786e\u4e86\u5176\u9002\u7528\u8303\u56f4\u3002"}}
{"id": "2508.01263", "pdf": "https://arxiv.org/pdf/2508.01263", "abs": "https://arxiv.org/abs/2508.01263", "authors": ["Long S. T. Nguyen", "Khang H. N. Vo", "Thu H. A. Nguyen", "Tuan C. Bui", "Duc Q. Nguyen", "Thanh-Tung Tran", "Anh D. Nguyen", "Minh L. Nguyen", "Fabien Baldacci", "Thang H. Bui", "Emanuel Di Nardo", "Angelo Ciaramella", "Son H. Le", "Ihsan Ullah", "Lorenzo Di Rocco", "Tho T. Quan"], "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025", "categories": ["cs.CL", "cs.AI", "cs.LO"], "comment": "The XAI Challenge @ TRNS-AI Workshop, IJCNN 2025: Explainable AI for\n  Educational Question Answering. Website:\n  https://sites.google.com/view/trns-ai/challenge/", "summary": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e862025\u5e74XAI\u6311\u6218\u8d5b\uff0c\u63a2\u8ba8\u4e86\u5728\u6559\u80b2\u4e2d\u7ed3\u5408\u8f7b\u91cf\u7ea7LLM\u4e0e\u7b26\u53f7\u7cfb\u7edf\u4ee5\u5b9e\u73b0\u900f\u660e\u4e14\u53ef\u89e3\u91ca\u7684AI\u95ee\u7b54\u7cfb\u7edf\u7684\u52aa\u529b\u3002", "motivation": "\u89e3\u51b3\u6559\u80b2\u9886\u57dfAI\u900f\u660e\u5ea6\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u9700\u6c42\uff0c\u901a\u8fc7\u7ade\u8d5b\u4fc3\u8fdbLLM\u4e0e\u7b26\u53f7\u63a8\u7406\u7684\u7ed3\u5408\u3002", "method": "\u57fa\u4e8e\u903b\u8f91\u6a21\u677f\u6784\u9020\u6570\u636e\u96c6\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7LLM\u6216\u6df7\u5408\u7cfb\u7edf\u5f00\u53d1\u95ee\u7b54\u7cfb\u7edf\uff0c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002", "result": "\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u5c55\u793a\u4e86LLM\u4e0e\u7b26\u53f7\u63a8\u7406\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u7ade\u8d5b\u4e3a\u672a\u6765XAI\u6559\u80b2\u7cfb\u7edf\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u63a8\u52a8\u4e86AI\u900f\u660e\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u53d1\u5c55\u3002"}}
{"id": "2508.02998", "pdf": "https://arxiv.org/pdf/2508.02998", "abs": "https://arxiv.org/abs/2508.02998", "authors": ["Haiyang Li"], "title": "MRG-Bench: Evaluating and Exploring the Requirements of Context for Repository-Level Code Generation", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode generation. However, current evaluation datasets suffer from issues such\nas the lack of runnable test cases, deviation from the distribution of\nreal-world code, and the ability to evaluate only the Python language. These\nlimitations undermine the credibility of the evaluation results.\n  To address these limitations, we introduce \\textbf{MRG-Bench} (Multi-language\nRepository-level Code Generation Benchmark), a novel dataset that provides a\nmore accurate evaluation of LLMs in practical repository-level code generation\ntasks. MRG-Bench has three main features: (1) practical data sourced from\nreal-world code repositories that align to the practical distribution, (2)\nmultiple programming languages support, including Python, Java, and Go, and (3)\nproject-level runnable test cases to assess the quality of the generated code.\n  Based on MRG-Bench, we conducted extensive experiments including large\nlanguage models, long-context models, and RAG-related methods. These evaluation\nresults demonstrate that \\textbf{current repository-level code generation\ntechniques suffer from significant performance deficiencies}. To further\ninvestigate why models fail, we designed novel experiments to annotate the\nunderlying causes of generation errors. The results explicitly show that the\nmajority of methods suffer from \"\\textbf{difficulty in understanding user\nrequirements},\" failing to comprehend their assigned tasks accurately.\nMoreover, the impact of different repository-level contexts on this issue\nexhibits significant disparities across different programming languages,\nsuggesting that, in practice, specialized contextual information needs to be\ndesigned for different languages.", "AI": {"tldr": "MRG-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u4ee3\u7801\u5206\u5e03\u548c\u53ef\u8fd0\u884c\u6d4b\u8bd5\u7b49\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u4ee3\u7801\u751f\u6210\u6280\u672f\u5728\u7406\u89e3\u7528\u6237\u9700\u6c42\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4e0d\u540c\u8bed\u8a00\u9700\u8981\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u6570\u636e\u96c6\u5b58\u5728\u7f3a\u4e4f\u53ef\u8fd0\u884c\u6d4b\u8bd5\u3001\u504f\u79bb\u771f\u5b9e\u4ee3\u7801\u5206\u5e03\u548c\u4ec5\u652f\u6301Python\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51faMRG-Bench\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u4ee3\u7801\u5e93\u6570\u636e\u3001\u652f\u6301\u591a\u8bed\u8a00\uff08Python\u3001Java\u3001Go\uff09\u548c\u9879\u76ee\u7ea7\u53ef\u8fd0\u884c\u6d4b\u8bd5\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u591a\u9879\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u4ee3\u7801\u751f\u6210\u6280\u672f\u5728\u7406\u89e3\u7528\u6237\u9700\u6c42\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u5bf9\u4e0a\u4e0b\u6587\u7684\u9700\u6c42\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u8bed\u8a00\u8bbe\u8ba1\u4e13\u95e8\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ee5\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2508.03120", "pdf": "https://arxiv.org/pdf/2508.03120", "abs": "https://arxiv.org/abs/2508.03120", "authors": ["Jiangyou Zhu", "Hongyu Deng", "He Chen"], "title": "Can Large Language Models Identify Materials from Radar Signals?", "categories": ["eess.SP", "cs.ET", "cs.RO"], "comment": null, "summary": "Accurately identifying the material composition of objects is a critical\ncapability for AI robots powered by large language models (LLMs) to perform\ncontext-aware manipulation. Radar technologies offer a promising sensing\nmodality for material recognition task. When combined with deep learning, radar\ntechnologies have demonstrated strong potential in identifying the material of\nvarious objects. However, existing radar-based solutions are often constrained\nto closed-set object categories and typically require task-specific data\ncollection to train deep learning models, largely limiting their practical\napplicability. This raises an important question: Can we leverage the powerful\nreasoning capabilities of pre-trained LLMs to directly infer material\ncomposition from raw radar signals? Answering this question is non-trivial due\nto the inherent redundancy of radar signals and the fact that pre-trained LLMs\nhave no prior exposure to raw radar data during training. To address this, we\nintroduce LLMaterial, the first study to investigate the feasibility of using\nLLM to identify materials directly from radar signals. First, we introduce a\nphysics-informed signal processing pipeline that distills high-redundancy radar\nraw data into a set of compact intermediate parameters that encapsulate the\nmaterial's intrinsic characteristics. Second, we adopt a retrieval-augmented\ngeneration (RAG) strategy to provide the LLM with domain-specific knowledge,\nenabling it to interpret and reason over the extracted intermediate parameters.\nLeveraging this integration, the LLM is empowered to perform step-by-step\nreasoning on the condensed radar features, achieving open-set material\nrecognition directly from raw radar signals. Preliminary results show that\nLLMaterial can effectively distinguish among a variety of common materials,\nhighlighting its strong potential for real-world material identification\napplications.", "AI": {"tldr": "LLMaterial\u5229\u7528\u9884\u8bad\u7ec3\u7684LLM\u76f4\u63a5\u4ece\u96f7\u8fbe\u4fe1\u53f7\u4e2d\u63a8\u65ad\u6750\u6599\u7ec4\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u96f7\u8fbe\u6280\u672f\u5c40\u9650\u4e8e\u5c01\u95ed\u96c6\u5bf9\u8c61\u548c\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u6536\u96c6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u96f7\u8fbe\u6280\u672f\u901a\u5e38\u5c40\u9650\u4e8e\u5c01\u95ed\u96c6\u5bf9\u8c61\u7c7b\u522b\uff0c\u5e76\u4e14\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u7684\u6570\u636e\u6765\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u662f\u5426\u80fd\u5229\u7528\u9884\u8bad\u7ec3\u7684LLM\u76f4\u63a5\u4ece\u96f7\u8fbe\u4fe1\u53f7\u4e2d\u63a8\u65ad\u6750\u6599\u7ec4\u6210\u3002", "method": "1. \u5f15\u5165\u7269\u7406\u4fe1\u53f7\u5904\u7406\u6d41\u7a0b\uff0c\u5c06\u9ad8\u5197\u4f59\u96f7\u8fbe\u539f\u59cb\u6570\u636e\u538b\u7f29\u4e3a\u5c01\u88c5\u6750\u6599\u5185\u5728\u7279\u5f81\u7684\u4e2d\u95f4\u53c2\u6570\uff1b2. \u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7b56\u7565\uff0c\u4e3aLLM\u63d0\u4f9b\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\uff0c\u4f7f\u5176\u80fd\u591f\u89e3\u91ca\u548c\u63a8\u7406\u4e2d\u95f4\u53c2\u6570\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0cLLMaterial\u80fd\u6709\u6548\u533a\u5206\u591a\u79cd\u5e38\u89c1\u6750\u6599\uff0c\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u6750\u6599\u8bc6\u522b\u5e94\u7528\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "LLMaterial\u8bc1\u660e\u4e86\u5229\u7528\u9884\u8bad\u7ec3\u7684LLM\u8fdb\u884c\u5f00\u653e\u96c6\u6750\u6599\u8bc6\u522b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.03146", "pdf": "https://arxiv.org/pdf/2508.03146", "abs": "https://arxiv.org/abs/2508.03146", "authors": ["Kostas Chounos", "Katerina Kyriakou", "Thanasis Korakis"], "title": "Scalability and Performance Evaluation of IEEE 802.11ah IoT Deployments: A Testbed Approach", "categories": ["cs.NI"], "comment": null, "summary": "This work focuses on the development and assessment of modern wireless\nInternet of Things (IoT) architectures, with relevance to emerging 5G and\nbeyond applications. To analyze the growing demands for data, and their impact,\nwe built an IEEE 802.11ah (WiFi Halow) office testbed for real-world\nexperimentation. This deployment allows us to uncover the practical performance\nand scalability limitations of such networks under various challenging\nscenarios. To the best of our knowledge, this is the first study to consider\ncomplex real-world IEEE 802.11ah implementations, aiming specifically to reveal\nunexpected performance behaviors, such as significant throughput degradation\narising in closely deployed wireless links. Our findings show that intense\nnetwork contention and Adjacent Channel Interference (ACI), drastically impact\nthe performance of the wireless links involved. Beyond evaluating network\nperformance, our experimental analysis also considers the energy consumption of\nthe devices under test, offering a more holistic perspective on the feasibility\nof IEEE 802.11ah in real-world deployments. The effective disclosure of such\nunexpected phenomena, can lead to well planned decisions and energy consumption\noptimization across the IoT to Cloud continuum.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u9762\u54115G\u53ca\u672a\u6765\u5e94\u7528\u7684\u73b0\u4ee3\u65e0\u7ebf\u7269\u8054\u7f51\u67b6\u6784\uff0c\u901a\u8fc7\u771f\u5b9e\u90e8\u7f72\u7684IEEE 802.11ah\u6d4b\u8bd5\u5e8a\u63ed\u793a\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6027\u80fd\u548c\u6269\u5c55\u6027\u9650\u5236\uff0c\u53d1\u73b0\u7f51\u7edc\u7ade\u4e89\u548c\u90bb\u9053\u5e72\u6270\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u8bbe\u5907\u80fd\u8017\u7684\u5168\u89c6\u89d2\u5206\u6790\u3002", "motivation": "\u5206\u6790\u6570\u636e\u9700\u6c42\u589e\u957f\u5bf9\u65e0\u7ebf\u7269\u8054\u7f51\u67b6\u6784\u7684\u5f71\u54cd\uff0c\u586b\u8865IEEE 802.11ah\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u6027\u80fd\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efaIEEE 802.11ah\u529e\u516c\u6d4b\u8bd5\u5e8a\uff0c\u8fdb\u884c\u771f\u5b9e\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u6027\u80fd\u3001\u6269\u5c55\u6027\u548c\u80fd\u8017\u3002", "result": "\u53d1\u73b0\u7f51\u7edc\u7ade\u4e89\u548c\u90bb\u9053\u5e72\u6270\u5bfc\u81f4\u541e\u5410\u91cf\u663e\u8457\u4e0b\u964d\uff0c\u80fd\u8017\u5206\u6790\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5168\u9762\u89c6\u89d2\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u4f18\u5316\u7269\u8054\u7f51\u5230\u4e91\u7684\u90e8\u7f72\u51b3\u7b56\u548c\u80fd\u8017\u7ba1\u7406\u3002"}}
{"id": "2508.03448", "pdf": "https://arxiv.org/pdf/2508.03448", "abs": "https://arxiv.org/abs/2508.03448", "authors": ["Jan Melechovsky", "Ambuj Mehrish", "Dorien Herremans"], "title": "SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "comment": null, "summary": "Music recordings often suffer from audio quality issues such as excessive\nreverberation, distortion, clipping, tonal imbalances, and a narrowed stereo\nimage, especially when created in non-professional settings without specialized\nequipment or expertise. These problems are typically corrected using separate\nspecialized tools and manual adjustments. In this paper, we introduce\nSonicMaster, the first unified generative model for music restoration and\nmastering that addresses a broad spectrum of audio artifacts with text-based\ncontrol. SonicMaster is conditioned on natural language instructions to apply\ntargeted enhancements, or can operate in an automatic mode for general\nrestoration. To train this model, we construct the SonicMaster dataset, a large\ndataset of paired degraded and high-quality tracks by simulating common\ndegradation types with nineteen degradation functions belonging to five\nenhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our\napproach leverages a flow-matching generative training paradigm to learn an\naudio transformation that maps degraded inputs to their cleaned, mastered\nversions guided by text prompts. Objective audio quality metrics demonstrate\nthat SonicMaster significantly improves sound quality across all artifact\ncategories. Furthermore, subjective listening tests confirm that listeners\nprefer SonicMaster's enhanced outputs over the original degraded audio,\nhighlighting the effectiveness of our unified approach.", "AI": {"tldr": "SonicMaster\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u97f3\u4e50\u4fee\u590d\u548c\u6bcd\u5e26\u5904\u7406\uff0c\u901a\u8fc7\u6587\u672c\u63a7\u5236\u89e3\u51b3\u591a\u79cd\u97f3\u9891\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u975e\u4e13\u4e1a\u73af\u5883\u4e0b\u97f3\u4e50\u5f55\u97f3\u4e2d\u5e38\u89c1\u7684\u97f3\u9891\u8d28\u91cf\u95ee\u9898\uff0c\u5982\u6df7\u54cd\u3001\u5931\u771f\u7b49\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u591a\u4e2a\u5de5\u5177\u548c\u624b\u52a8\u8c03\u6574\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u5c06\u964d\u7ea7\u97f3\u9891\u6620\u5c04\u5230\u4fee\u590d\u7248\u672c\uff0c\u8bad\u7ec3\u6570\u636e\u4e3a\u6a21\u62df\u591a\u79cd\u964d\u7ea7\u7684\u9ad8\u8d28\u91cf\u97f3\u9891\u5bf9\u3002", "result": "\u5ba2\u89c2\u6307\u6807\u663e\u793aSonicMaster\u663e\u8457\u63d0\u5347\u97f3\u9891\u8d28\u91cf\uff0c\u4e3b\u89c2\u6d4b\u8bd5\u4e5f\u8bc1\u5b9e\u7528\u6237\u504f\u597d\u5176\u4fee\u590d\u7ed3\u679c\u3002", "conclusion": "SonicMaster\u4f5c\u4e3a\u4e00\u79cd\u7edf\u4e00\u65b9\u6cd5\uff0c\u5728\u97f3\u4e50\u4fee\u590d\u548c\u6bcd\u5e26\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2508.02868", "pdf": "https://arxiv.org/pdf/2508.02868", "abs": "https://arxiv.org/abs/2508.02868", "authors": ["Kaixuan Wang", "Loraine Clarke", "Carl-Cyril J Dreue", "Guancheng Zhou", "Jason T. Jacques"], "title": "Critical Challenges in Content Moderation for People Who Use Drugs (PWUD): Insights into Online Harm Reduction Practices from Moderators", "categories": ["cs.HC", "cs.CY"], "comment": "22 pages", "summary": "Online communities serve as essential support channels for People Who Use\nDrugs (PWUD), providing access to peer support and harm reduction information.\nThe moderation of these communities involves consequential decisions affecting\nmember safety, yet existing sociotechnical systems provide insufficient support\nfor moderators. Through interviews with experienced moderators from PWUD forums\non Reddit, we analyse the unique nature of this work. We argue that this work\nconstitutes a distinct form of public health intervention characterised by\nthree moderation challenges: the need for specialised, expert risk assessment;\ntime-critical crisis response; and the navigation of a structural conflict\nbetween platform policies and community safety goals. We demonstrate how\ncurrent moderation systems are insufficient in supporting PWUD communities. For\nexample, policies minimising platforms' legal exposure to illicit activities\ncan inadvertently push moderators to implement restrictive rules to protect\ncommunity's existence, which can limit such a vulnerable group's ability to\nshare potentially life-saving resources online. We conclude by identifying two\nnecessary shifts in sociotechnical design to support moderators' work: first,\nmoving to automated tools that support human sensemaking in contexts with\ncompeting interests; and second, shifting from systems that require moderators\nto perform low-level rule programming to those that enable high-level,\nexample-based instruction. Further, we highlight how the design of\nsociotechnical systems in online spaces could impact harm reduction efforts\naimed at improving health outcomes for PWUD communities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76Reddit\u4e0a\u836f\u7269\u4f7f\u7528\u8005\u7684\u5728\u7ebf\u793e\u533a\u7ba1\u7406\uff0c\u63a2\u8ba8\u4e86\u7ba1\u7406\u5de5\u4f5c\u7684\u72ec\u7279\u6027\u53ca\u5176\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u793e\u4f1a\u6280\u672f\u8bbe\u8ba1\u7684\u5efa\u8bae\u3002", "motivation": "\u8c03\u67e5\u5728\u7ebf\u793e\u533a\u5bf9\u836f\u7269\u4f7f\u7528\u8005\u7684\u652f\u6301\u4f5c\u7528\uff0c\u4ee5\u53ca\u73b0\u6709\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u5bf9\u7ba1\u7406\u5de5\u4f5c\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u91c7\u8bbfReddit\u4e0a\u836f\u7269\u4f7f\u7528\u8005\u793e\u533a\u7684\u7ecf\u9a8c\u4e30\u5bcc\u7684\u7ba1\u7406\u5458\uff0c\u5206\u6790\u7ba1\u7406\u5de5\u4f5c\u7684\u7279\u70b9\u3002", "result": "\u53d1\u73b0\u7ba1\u7406\u5de5\u4f5c\u5177\u6709\u4e09\u4e2a\u72ec\u7279\u6311\u6218\uff1a\u4e13\u4e1a\u98ce\u9669\u8bc4\u4f30\u3001\u7d27\u6025\u5371\u673a\u5e94\u5bf9\u53ca\u5e73\u53f0\u653f\u7b56\u4e0e\u793e\u533a\u5b89\u5168\u76ee\u6807\u7684\u51b2\u7a81\u3002\u5f53\u524d\u7cfb\u7edf\u4e0d\u8db3\u4ee5\u652f\u6301\u8fd9\u4e9b\u793e\u533a\u3002", "conclusion": "\u63d0\u51fa\u6539\u8fdb\u793e\u4f1a\u6280\u672f\u8bbe\u8ba1\u7684\u4e24\u4e2a\u65b9\u5411\uff1a\u81ea\u52a8\u5316\u5de5\u5177\u652f\u6301\u4eba\u7c7b\u51b3\u7b56\u548c\u9ad8\u7ea7\u522b\u6307\u4ee4\u53d6\u4ee3\u4f4e\u7ea7\u522b\u89c4\u5219\u7f16\u7a0b\uff0c\u5e76\u5f3a\u8c03\u5176\u5bf9\u51cf\u5c11\u5371\u5bb3\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.03513", "pdf": "https://arxiv.org/pdf/2508.03513", "abs": "https://arxiv.org/abs/2508.03513", "authors": ["Zhu Zhu", "Yu Sun", "Dhatri Parakal", "Bo Fang", "Steven Farrell", "Gregory H. Bauer", "Brett Bode", "Ian T. Foster", "Michael E. Papka", "William Gropp", "Zhao Zhang", "Lishan Yang"], "title": "Understanding the Landscape of Ampere GPU Memory Errors", "categories": ["cs.DC"], "comment": null, "summary": "Graphics Processing Units (GPUs) have become a de facto solution for\naccelerating high-performance computing (HPC) applications. Understanding their\nmemory error behavior is an essential step toward achieving efficient and\nreliable HPC systems. In this work, we present a large-scale\ncross-supercomputer study to characterize GPU memory reliability, covering\nthree supercomputers - Delta, Polaris, and Perlmutter - all equipped with\nNVIDIA A100 GPUs. We examine error logs spanning 67.77 million GPU device-hours\nacross 10,693 GPUs. We compare error rates and mean-time-between-errors (MTBE)\nand highlight both shared and distinct error characteristics among these three\nsystems. Based on these observations and analyses, we discuss the implications\nand lessons learned, focusing on the reliable operation of supercomputers, the\nchoice of checkpointing interval, and the comparison of reliability\ncharacteristics with those of previous-generation GPUs. Our characterization\nstudy provides valuable insights into fault-tolerant HPC system design and\noperation, enabling more efficient execution of HPC applications.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u8de8\u8d85\u7ea7\u8ba1\u7b97\u673a\u7814\u7a76\uff0c\u5206\u6790\u4e86NVIDIA A100 GPU\u7684\u5185\u5b58\u53ef\u9760\u6027\uff0c\u63d0\u4f9b\u4e86\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5b9d\u8d35\u89c1\u89e3\u3002", "motivation": "\u7406\u89e3GPU\u5185\u5b58\u9519\u8bef\u884c\u4e3a\u662f\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u5173\u952e\u3002", "method": "\u7814\u7a76\u8986\u76d6\u4e09\u4e2a\u8d85\u7ea7\u8ba1\u7b97\u673a\uff08Delta\u3001Polaris\u3001Perlmutter\uff09\uff0c\u5206\u679067.77\u767e\u4e07GPU\u8bbe\u5907\u5c0f\u65f6\u7684\u9519\u8bef\u65e5\u5fd7\u3002", "result": "\u6bd4\u8f83\u4e86\u9519\u8bef\u7387\u548c\u5e73\u5747\u9519\u8bef\u95f4\u9694\u65f6\u95f4\uff08MTBE\uff09\uff0c\u63ed\u793a\u4e86\u4e09\u4e2a\u7cfb\u7edf\u7684\u5171\u6027\u4e0e\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u53ef\u9760\u6027\u8fd0\u884c\u3001\u68c0\u67e5\u70b9\u95f4\u9694\u9009\u62e9\u53ca\u4e0e\u524d\u4ee3GPU\u7684\u5bf9\u6bd4\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.03012", "pdf": "https://arxiv.org/pdf/2508.03012", "abs": "https://arxiv.org/abs/2508.03012", "authors": ["Zexiong Ma", "Chao Peng", "Qunhong Zeng", "Pengfei Gao", "Yanzhen Zou", "Bing Xie"], "title": "Tool-integrated Reinforcement Learning for Repo Deep Search", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.", "AI": {"tldr": "ToolTrain\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u5de5\u5177\u96c6\u6210\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u62d2\u7edd\u91c7\u6837\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5de5\u5177\u96c6\u6210\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347LLMs\u5728\u95ee\u9898\u5b9a\u4f4d\u4e2d\u4f7f\u7528\u68c0\u7d22\u5de5\u5177\u7684\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u5904\u7406\u95ee\u9898\u5b9a\u4f4d\u65f6\uff0c\u867d\u7136\u7ed3\u5408\u4e86\u4ed3\u5e93\u68c0\u7d22\u5de5\u5177\uff0c\u4f46\u4ecd\u96be\u4ee5\u9ad8\u6548\u5b8c\u6210\u591a\u6b65\u63a8\u7406\u548c\u5bfc\u822a\u4efb\u52a1\u3002", "method": "\u63d0\u51faToolTrain\u6846\u67b6\uff0c\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u62d2\u7edd\u91c7\u6837\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5de5\u5177\u96c6\u6210\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u4f18\u5316LLMs\u5728\u95ee\u9898\u5b9a\u4f4d\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002", "result": "32B\u6a21\u578b\u5728\u51fd\u6570\u7ea7\u5b9a\u4f4d\u4e0a\u8868\u73b0\u4f18\u4e8eClaude-3.7\uff0c\u4e14\u5b9a\u4f4d\u6027\u80fd\u63d0\u5347\u76f4\u63a5\u6539\u5584\u4e86\u7aef\u5230\u7aef\u95ee\u9898\u89e3\u51b3\u6548\u679c\u3002", "conclusion": "ToolTrain\u8bc1\u660e\u4e86\u901a\u8fc7\u8bad\u7ec3\u4f18\u5316\u95ee\u9898\u5b9a\u4f4d\u80fd\u529b\u662f\u63d0\u5347\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2508.03435", "pdf": "https://arxiv.org/pdf/2508.03435", "abs": "https://arxiv.org/abs/2508.03435", "authors": ["Thomas S. Heinze", "Andr\u00e9 Sch\u00e4fer", "Wolfram Amme"], "title": "StoneDetector: Conventional and versatile code clone detection for Java", "categories": ["cs.SE", "cs.PL"], "comment": "supplementary information available at\n  https://stonedetector.fmi.uni-jena.de/", "summary": "Copy & paste is a widespread practice when developing software and, thus,\nduplicated and subsequently modified code occurs frequently in software\nprojects. Since such code clones, i.e., identical or similar fragments of code,\ncan bloat software projects and cause issues like bug or vulnerability\npropagation, their identification is of importance. In this paper, we present\nthe StoneDetector platform and its underlying method for finding code clones in\nJava source and Bytecode. StoneDetector implements a conventional clone\ndetection approach based upon the textual comparison of paths derived from the\ncode's representation by dominator trees. In this way, the tool does not only\nfind exact and syntactically similar near-miss code clones, but also code\nclones that are harder to detect due to their larger variety in the syntax. We\ndemonstrate StoneDetector's versatility as a conventional clone detection\nplatform and analyze its various available configuration parameters, including\nthe usage of different string metrics, hashing algorithms, etc. In our\nexhaustive evaluation with other conventional clone detectors on several\nstate-of-the-art benchmarks, we can show StoneDetector's performance and\nscalability in finding code clones in both, Java source and Bytecode.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86StoneDetector\u5e73\u53f0\u53ca\u5176\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4bJava\u6e90\u4ee3\u7801\u548c\u5b57\u8282\u7801\u4e2d\u7684\u4ee3\u7801\u514b\u9686\uff0c\u901a\u8fc7\u652f\u914d\u6811\u8def\u5f84\u7684\u6587\u672c\u6bd4\u8f83\u5b9e\u73b0\u9ad8\u6548\u8bc6\u522b\u3002", "motivation": "\u4ee3\u7801\u514b\u9686\u53ef\u80fd\u5bfc\u81f4\u9879\u76ee\u81a8\u80c0\u548c\u95ee\u9898\u4f20\u64ad\uff0c\u56e0\u6b64\u8bc6\u522b\u5b83\u4eec\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u652f\u914d\u6811\u8def\u5f84\u7684\u6587\u672c\u6bd4\u8f83\uff0c\u4f7f\u7528\u4e0d\u540c\u5b57\u7b26\u4e32\u5ea6\u91cf\u548c\u54c8\u5e0c\u7b97\u6cd5\u914d\u7f6e\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStoneDetector\u5c55\u73b0\u4e86\u9ad8\u6548\u7684\u6027\u80fd\u548c\u6269\u5c55\u6027\u3002", "conclusion": "StoneDetector\u662f\u4e00\u79cd\u591a\u529f\u80fd\u4e14\u9ad8\u6548\u7684\u4ee3\u7801\u514b\u9686\u68c0\u6d4b\u5de5\u5177\u3002"}}
{"id": "2508.03274", "pdf": "https://arxiv.org/pdf/2508.03274", "abs": "https://arxiv.org/abs/2508.03274", "authors": ["Ramaswamy Palaniappan", "Surej Mouli", "Howard Bowman", "Ian McLoughlin"], "title": "Investigating the Cognitive Response of Brake Lights in Initiating Braking Action Using EEG", "categories": ["eess.SP", "cs.ET", "cs.HC", "cs.IR"], "comment": "arXiv admin note: text overlap with arXiv:2010.10584", "summary": "Half of all road accidents result from either lack of driver attention or\nfrom maintaining insufficient separation between vehicles. Collision from the\nrear, in particular, has been identified as the most common class of accident\nin the UK, and its influencing factors have been widely studied for many years.\nRear-mounted stop lamps, illuminated when braking, are the primary mechanism to\nalert following drivers to the need to reduce speed or brake. This paper\ndevelops a novel brain response approach to measuring subject reaction to\ndifferent brake light designs. A variety of off-the-shelf brake light\nassemblies are tested in a physical simulated driving environment to assess the\ncognitive reaction times of 22 subjects. Eight pairs of LED-based and two pairs\nof incandescent bulb-based brake light assemblies are used and\nelectroencephalogram (EEG) data recorded. Channel Pz is utilised to extract the\nP3 component evoked during the decision making process that occurs in the brain\nwhen a participant decides to lift their foot from the accelerator and depress\nthe brake. EEG analysis shows that both incandescent bulb-based lights are\nstatistically slower to evoke cognitive responses than all tested LED-based\nlights. Between the LED designs, differences are evident, but not statistically\nsignificant, attributed to the significant amount of movement artifact in the\nEEG signal.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u5239\u8f66\u706f\u8bbe\u8ba1\u5bf9\u9a7e\u9a76\u5458\u53cd\u5e94\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u901a\u8fc7EEG\u5206\u6790\u53d1\u73b0LED\u5239\u8f66\u706f\u7684\u8ba4\u77e5\u53cd\u5e94\u65f6\u95f4\u663e\u8457\u5feb\u4e8e\u4f20\u7edf\u706f\u6ce1\u5239\u8f66\u706f\u3002", "motivation": "\u534a\u6570\u4ea4\u901a\u4e8b\u6545\u6e90\u4e8e\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u4e0d\u96c6\u4e2d\u6216\u8f66\u8ddd\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u8ffd\u5c3e\u4e8b\u6545\u5728\u82f1\u56fd\u6700\u5e38\u89c1\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6d4b\u91cf\u9a7e\u9a76\u5458\u5bf9\u5239\u8f66\u706f\u8bbe\u8ba1\u7684\u53cd\u5e94\uff0c\u63d0\u5347\u884c\u8f66\u5b89\u5168\u3002", "method": "\u5728\u6a21\u62df\u9a7e\u9a76\u73af\u5883\u4e2d\u6d4b\u8bd522\u540d\u53d7\u8bd5\u8005\u5bf910\u79cd\u5239\u8f66\u706f\uff088\u79cdLED\u30012\u79cd\u4f20\u7edf\u706f\u6ce1\uff09\u7684\u53cd\u5e94\uff0c\u4f7f\u7528EEG\u8bb0\u5f55P3\u6210\u5206\u5206\u6790\u8ba4\u77e5\u53cd\u5e94\u65f6\u95f4\u3002", "result": "EEG\u6570\u636e\u663e\u793a\uff0c\u4f20\u7edf\u706f\u6ce1\u5239\u8f66\u706f\u7684\u8ba4\u77e5\u53cd\u5e94\u65f6\u95f4\u663e\u8457\u6162\u4e8e\u6240\u6709LED\u5239\u8f66\u706f\uff1bLED\u8bbe\u8ba1\u95f4\u5dee\u5f02\u4e0d\u660e\u663e\uff0c\u53ef\u80fd\u4e0eEEG\u4fe1\u53f7\u4e2d\u7684\u8fd0\u52a8\u4f2a\u5f71\u6709\u5173\u3002", "conclusion": "LED\u5239\u8f66\u706f\u5728\u5f15\u53d1\u9a7e\u9a76\u5458\u5feb\u901f\u8ba4\u77e5\u53cd\u5e94\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u706f\u6ce1\u8bbe\u8ba1\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u8ffd\u5c3e\u4e8b\u6545\u3002"}}
{"id": "2508.03171", "pdf": "https://arxiv.org/pdf/2508.03171", "abs": "https://arxiv.org/abs/2508.03171", "authors": ["Chien-Wei Fu", "Meng-Lin Ku"], "title": "Energy-efficient Federated Learning for UAV Communications", "categories": ["cs.NI", "cs.IT", "math.IT"], "comment": null, "summary": "In this paper, we propose an unmanned aerial vehicle (UAV)-assisted federated\nlearning (FL) framework that jointly optimizes UAV trajectory, user\nparticipation, power allocation, and data volume control to minimize overall\nsystem energy consumption. We begin by deriving the convergence accuracy of the\nFL model under multiple local updates, enabling a theoretical understanding of\nhow user participation and data volume affect FL learning performance. The\nresulting joint optimization problem is non-convex; to address this, we employ\nalternating optimization (AO) and successive convex approximation (SCA)\ntechniques to convexify the non-convex constraints, leading to the design of an\niterative energy consumption optimization (ECO) algorithm. Simulation results\nconfirm that ECO consistently outperform existing baseline schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u7528\u6237\u53c2\u4e0e\u3001\u529f\u7387\u5206\u914d\u548c\u6570\u636e\u91cf\u63a7\u5236\uff0c\u4ee5\u6700\u5c0f\u5316\u7cfb\u7edf\u80fd\u8017\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u65e0\u4eba\u673a\u8f85\u52a9\u4e0b\u7684\u80fd\u8017\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u5b66\u4e60\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u548c\u9010\u6b21\u51f8\u903c\u8fd1\uff08SCA\uff09\u6280\u672f\uff0c\u5c06\u975e\u51f8\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u8fed\u4ee3\u80fd\u91cf\u6d88\u8017\u4f18\u5316\uff08ECO\uff09\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cECO\u7b97\u6cd5\u5728\u80fd\u8017\u4f18\u5316\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u7b97\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u7cfb\u7edf\u80fd\u8017\uff0c\u5e76\u63d0\u9ad8\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2508.02958", "pdf": "https://arxiv.org/pdf/2508.02958", "abs": "https://arxiv.org/abs/2508.02958", "authors": ["Daniel Killough", "Justin Feng", "Zheng Xue \"ZX\" Ching", "Daniel Wang", "Rithvik Dyava", "Yapeng Tian", "Yuhang Zhao"], "title": "VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People", "categories": ["cs.HC"], "comment": "17 pages, 10 figures, 2 tables, LaTeX; To be published in ACM's 2025\n  Symposium on User Interface Software and Technology (UIST 2025)", "summary": "Virtual Reality (VR) is inaccessible to blind people. While research has\ninvestigated many techniques to enhance VR accessibility, they require\nadditional developer effort to integrate. As such, most mainstream VR apps\nremain inaccessible as the industry de-prioritizes accessibility. We present\nVRSight, an end-to-end system that recognizes VR scenes post hoc through a set\nof AI models (e.g., object detection, depth estimation, LLM-based atmosphere\ninterpretation) and generates tone-based, spatial audio feedback, empowering\nblind users to interact in VR without developer intervention. To enable virtual\nelement detection, we further contribute DISCOVR, a VR dataset consisting of 30\nvirtual object classes from 17 social VR apps, substituting real-world datasets\nthat remain not applicable to VR contexts. Nine participants used VRSight to\nexplore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in\nfacilitating social tasks like avatar awareness and available seat\nidentification.", "AI": {"tldr": "VRSight \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u901a\u8fc7 AI \u6a21\u578b\u8bc6\u522b VR \u573a\u666f\u5e76\u751f\u6210\u7a7a\u95f4\u97f3\u9891\u53cd\u9988\uff0c\u5e2e\u52a9\u76f2\u4eba\u65e0\u9700\u5f00\u53d1\u8005\u5e72\u9884\u5373\u53ef\u5728 VR \u4e2d\u4ea4\u4e92\u3002", "motivation": "\u76ee\u524d VR \u5bf9\u76f2\u4eba\u7684\u53ef\u8bbf\u95ee\u6027\u4e0d\u8db3\uff0c\u4e3b\u6d41 VR \u5e94\u7528\u7f3a\u4e4f\u65e0\u969c\u788d\u8bbe\u8ba1\u3002", "method": "\u5229\u7528 AI \u6a21\u578b\uff08\u5982\u7269\u4f53\u68c0\u6d4b\u3001\u6df1\u5ea6\u4f30\u8ba1\u548c\u57fa\u4e8e LLM \u7684\u6c1b\u56f4\u89e3\u91ca\uff09\u8bc6\u522b VR \u573a\u666f\uff0c\u5e76\u751f\u6210\u7a7a\u95f4\u97f3\u9891\u53cd\u9988\u3002", "result": "\u53c2\u4e0e\u8005\u6210\u529f\u4f7f\u7528 VRSight \u5728 VR \u4e2d\u5b8c\u6210\u793e\u4ea4\u4efb\u52a1\uff08\u5982\u8bc6\u522b\u865a\u62df\u89d2\u8272\u548c\u53ef\u7528\u5ea7\u4f4d\uff09\u3002", "conclusion": "VRSight \u6709\u6548\u63d0\u5347\u4e86 VR \u5bf9\u76f2\u4eba\u7fa4\u4f53\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u65e0\u9700\u989d\u5916\u5f00\u53d1\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2508.03567", "pdf": "https://arxiv.org/pdf/2508.03567", "abs": "https://arxiv.org/abs/2508.03567", "authors": ["Oscar Ferraz", "Vitor Silva", "Gabriel Falcao"], "title": "In-Memory Non-Binary LDPC Decoding", "categories": ["cs.DC", "cs.CE"], "comment": "23 pages, 10 figures, and 4 tables", "summary": "Low-density parity-check (LDPC) codes are an important feature of several\ncommunication and storage applications, offering a flexible and effective\nmethod for error correction. These codes are computationally complex and\nrequire the exploitation of parallel processing to meet real-time constraints.\nAs advancements in arithmetic and logic unit technology allowed for higher\nperformance of computing systems, memory technology has not kept the same pace\nof development, creating a data movement bottleneck and affecting parallel\nprocessing systems more dramatically. To alleviate the severity of this\nbottleneck, several solutions have been proposed, namely the processing\nin-memory (PiM) paradigm that involves the design of compute units to where (or\nnear) the data is stored, utilizing thousands of low-complexity processing\nunits to perform out bit-wise and simple arithmetic operations. This paper\npresents a novel efficient solution for near-memory non-binary LDPC decoders in\nthe UPMEM system, for the best of our knowledge the first real hardware\nPiM-based non-binary LDPC decoder that is benchmarked against low-power GPU\nparallel solutions highly optimized for throughput performance. PiM-based\nnon-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, which is\neven competitive when compared against implementations running in edge GPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u8fd1\u5b58\u50a8\u8ba1\u7b97\uff08PiM\uff09\u7684\u975e\u4e8c\u8fdb\u5236LDPC\u89e3\u7801\u5668\uff0c\u5728UPMEM\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e8676 Mbit/s\u7684\u89e3\u7801\u541e\u5410\u91cf\uff0c\u6027\u80fd\u4f18\u4e8e\u4f4e\u529f\u8017GPU\u65b9\u6848\u3002", "motivation": "\u5185\u5b58\u6280\u672f\u53d1\u5c55\u6ede\u540e\u5bfc\u81f4\u6570\u636e\u79fb\u52a8\u74f6\u9888\uff0c\u5f71\u54cd\u5e76\u884c\u5904\u7406\u7cfb\u7edf\u6027\u80fd\uff0c\u9700\u901a\u8fc7\u8fd1\u5b58\u50a8\u8ba1\u7b97\uff08PiM\uff09\u7f13\u89e3\u3002", "method": "\u8bbe\u8ba1\u975e\u4e8c\u8fdb\u5236LDPC\u89e3\u7801\u5668\uff0c\u91c7\u7528PiM\u8303\u5f0f\uff0c\u5c06\u8ba1\u7b97\u5355\u5143\u7f6e\u4e8e\u6570\u636e\u5b58\u50a8\u4f4d\u7f6e\u9644\u8fd1\u3002", "result": "PiM\u65b9\u6848\u5b9e\u73b076 Mbit/s\u89e3\u7801\u541e\u5410\u91cf\uff0c\u6027\u80fd\u4f18\u4e8e\u4f18\u5316\u7684\u4f4e\u529f\u8017GPU\u65b9\u6848\u3002", "conclusion": "PiM\u8303\u5f0f\u4e3a\u975e\u4e8c\u8fdb\u5236LDPC\u89e3\u7801\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u7ade\u4e89\u529b\u5f3a\u3002"}}
{"id": "2508.03432", "pdf": "https://arxiv.org/pdf/2508.03432", "abs": "https://arxiv.org/abs/2508.03432", "authors": ["C\u00e9lia Borlido", "Ganna Kudryavtseva", "Brett McLean"], "title": "Difference-restriction algebras with operators", "categories": ["math.LO", "cs.LO"], "comment": "35 pages", "summary": "We exhibit an adjunction between a category of abstract algebras of partial\nfunctions that we call difference-restriction algebras and a category of\nHausdorff \\'etale spaces. Difference-restriction algebras are those algebras\nisomorphic to a collection of partial functions closed under relative\ncomplement and domain restriction. Our adjunction generalises the adjunction\nbetween the category of generalised Boolean algebras and the category of\nHausdorff spaces. We define the finitary compatible completion of a\ndifference-restriction algebra and show that the monad induced by our\nadjunction yields the finitary compatible completion of any\ndifference-restriction algebra. As a corollary, the adjunction restricts to a\nduality between the finitarily compatibly complete difference-restriction\nalgebras and the locally compact zero-dimensional Hausdorff \\'etale spaces,\ngeneralising the duality between generalised Boolean algebras and locally\ncompact zero-dimensional Hausdorff spaces. We then extend these adjunction,\nduality, and completion results to difference-restriction algebras equipped\nwith arbitrary additional compatibility preserving operators.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u90e8\u5206\u51fd\u6570\u7684\u62bd\u8c61\u4ee3\u6570\u4e0eHausdorff \u00e9tale\u7a7a\u95f4\u4e4b\u95f4\u7684\u4f34\u968f\u5173\u7cfb\uff0c\u5e76\u63a8\u5e7f\u4e86\u5e7f\u4e49\u5e03\u5c14\u4ee3\u6570\u4e0eHausdorff\u7a7a\u95f4\u7684\u4f34\u968f\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u90e8\u5206\u51fd\u6570\u7684\u4ee3\u6570\u7ed3\u6784\u4e0e\u62d3\u6251\u7a7a\u95f4\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u63a8\u5e7f\u73b0\u6709\u7406\u8bba\u3002", "method": "\u5b9a\u4e49\u5dee\u5f02\u9650\u5236\u4ee3\u6570\u53ca\u5176\u6709\u9650\u517c\u5bb9\u5b8c\u5907\u5316\uff0c\u901a\u8fc7\u4f34\u968f\u5173\u7cfb\u6784\u9020\u5b8c\u5907\u5316\u3002", "result": "\u4f34\u968f\u5173\u7cfb\u6269\u5c55\u5230\u5c40\u90e8\u7d27\u96f6\u7ef4Hausdorff \u00e9tale\u7a7a\u95f4\u4e0e\u6709\u9650\u517c\u5bb9\u5b8c\u5907\u5dee\u5f02\u9650\u5236\u4ee3\u6570\u4e4b\u95f4\u7684\u5bf9\u5076\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f34\u968f\u5173\u7cfb\u548c\u5bf9\u5076\u6027\uff0c\u63a8\u5e7f\u4e86\u5e7f\u4e49\u5e03\u5c14\u4ee3\u6570\u4e0e\u7a7a\u95f4\u7684\u7ecf\u5178\u7ed3\u679c\uff0c\u5e76\u6269\u5c55\u5230\u5305\u542b\u9644\u52a0\u7b97\u5b50\u7684\u4ee3\u6570\u3002"}}
{"id": "2508.03215", "pdf": "https://arxiv.org/pdf/2508.03215", "abs": "https://arxiv.org/abs/2508.03215", "authors": ["Dongming Jin", "Zhi Jin", "Linyu Li", "Zheng Fang", "Jia Li", "Xiaohong Chen"], "title": "A System Model Generation Benchmark from Natural Language Requirements", "categories": ["cs.SE"], "comment": "16 pages, 14 figures", "summary": "System models, a critical artifact in software development, provide a formal\nabstraction of both the structural and behavioral aspects of software systems,\nwhich can facilitate the early requirements analysis and architecture design.\nHowever, developing system models remains challenging due to the specific\nsyntax of model description languages and the relative scarcity of public model\nexamples. While large language models (LLMs) have shown promise in generating\ncode with programming languages and could potentially aid in system model\ndevelopment, no benchmarks currently exist for evaluating their ability to\ngenerate system models with specific description languages. We present\nSysMBench, which comprises 151 human-curated scenarios spanning a wide range of\npopular domains and varying difficulty levels. Each scenario mainly comprises a\nnatural language requirements description, a system model expressed in a\nspecific model description language, and a visualized system model diagram. The\nrequirements description is fed as user input to the LLM, the system model with\ndescription language is used to verify if the generated system model conforms\nto the requirements, and the visualized diagram serves to support manual\nvalidation. We introduce SysMEval, a semantic-aware evaluation metric to\nevaluate the quality of generated system models. We evaluate 17 popular LLMs on\nthis task with three traditional metrics and SysMEval, from directly prompting\nto three commonly used enhancement strategies. Our in-depth evaluation shows\nthat LLMs perform poorly on SysMBench, with the highest BLEU of 4% and\nSysMEval-F1 of 62%. We release the SysMBench and its evaluation framework to\nenable future research on LLM-based system model generation.", "AI": {"tldr": "SysMBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7cfb\u7edf\u6a21\u578b\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b151\u4e2a\u573a\u666f\u3002\u7814\u7a76\u8868\u660e\u76ee\u524dLLM\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u7531\u4e8e\u7cfb\u7edf\u6a21\u578b\u5f00\u53d1\u7684\u590d\u6742\u6027\u548c\u63cf\u8ff0\u8bed\u8a00\u7684\u7279\u5b9a\u6027\uff0c\u9700\u8981\u8bc4\u4f30LLM\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u76f8\u5173\u57fa\u51c6\u3002", "method": "\u63d0\u51faSysMBench\u57fa\u51c6\uff0c\u5305\u542b\u573a\u666f\u3001\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u7cfb\u7edf\u6a21\u578b\u548c\u53ef\u89c6\u5316\u56fe\u8868\u3002\u4f7f\u7528SysMEval\u8bc4\u4f30\u6307\u6807\u5bf917\u4e2aLLM\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "LLM\u5728SysMBench\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u6700\u9ad8BLEU\u4ec5\u4e3a4%\uff0cSysMEval-F1\u4e3a62%\u3002", "conclusion": "SysMBench\u4e3a\u672a\u6765LLM\u5728\u7cfb\u7edf\u6a21\u578b\u751f\u6210\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u4f46\u76ee\u524d\u6a21\u578b\u7684\u6027\u80fd\u4ecd\u9700\u63d0\u5347\u3002"}}
{"id": "2508.03603", "pdf": "https://arxiv.org/pdf/2508.03603", "abs": "https://arxiv.org/abs/2508.03603", "authors": ["Iti Shree", "Karine Even-Mendoz", "Tomasz Radzik"], "title": "ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated Test Programs", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Existing LLM-based compiler fuzzers often produce syntactically or\nsemantically invalid test programs, limiting their effectiveness in exercising\ncompiler optimizations and backend components. We introduce ReFuzzer, a\nframework for refining LLM-generated test programs by systematically detecting\nand correcting compilation and runtime violations (e.g. division by zero or\narray out-of-bounds accesses). ReFuzzer employs a feedback loop with a local\nLLM to validate and filter erroneous programs before execution, improving\nfuzzing effectiveness beyond crash detection and enabling the generation of\ndiverse yet valid test programs.\n  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box\nfuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'\nvalidity from 47.0-49.4% to 96.6-97.3%, with an average processing time of\n2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing\nsignificantly increased code coverage in critical optimization and IR\ngeneration components. For example, vectorization coverage had an absolute\nimprovement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing,\nenhancing testing effectiveness.", "AI": {"tldr": "ReFuzzer\u662f\u4e00\u79cd\u901a\u8fc7\u68c0\u6d4b\u548c\u4fee\u6b63\u7f16\u8bd1\u4e0e\u8fd0\u884c\u65f6\u9519\u8bef\u6765\u4f18\u5316LLM\u751f\u6210\u7684\u6d4b\u8bd5\u7a0b\u5e8f\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u7a0b\u5e8f\u7684\u6709\u6548\u6027\u548c\u4ee3\u7801\u8986\u76d6\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u7f16\u8bd1\u5668\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u5e38\u751f\u6210\u8bed\u6cd5\u6216\u8bed\u4e49\u65e0\u6548\u7684\u6d4b\u8bd5\u7a0b\u5e8f\uff0c\u9650\u5236\u4e86\u5176\u5728\u6d4b\u8bd5\u7f16\u8bd1\u5668\u4f18\u5316\u548c\u540e\u7aef\u7ec4\u4ef6\u4e2d\u7684\u6548\u679c\u3002", "method": "ReFuzzer\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u4e0e\u672c\u5730LLM\u7ed3\u5408\uff0c\u9a8c\u8bc1\u5e76\u8fc7\u6ee4\u9519\u8bef\u7a0b\u5e8f\uff0c\u63d0\u9ad8\u6a21\u7cca\u6d4b\u8bd5\u7684\u6709\u6548\u6027\u3002", "result": "ReFuzzer\u5c06\u6d4b\u8bd5\u7a0b\u5e8f\u7684\u6709\u6548\u6027\u4ece47.0-49.4%\u63d0\u5347\u81f396.6-97.3%\uff0c\u5173\u952e\u4f18\u5316\u7ec4\u4ef6\u7684\u4ee3\u7801\u8986\u76d6\u7387\u663e\u8457\u589e\u52a0\u3002", "conclusion": "ReFuzzer\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u7a0b\u5e8f\u7684\u8d28\u91cf\u548c\u6a21\u7cca\u6d4b\u8bd5\u7684\u6548\u679c\uff0c\u4e3a\u7f16\u8bd1\u5668\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.03324", "pdf": "https://arxiv.org/pdf/2508.03324", "abs": "https://arxiv.org/abs/2508.03324", "authors": ["Satyapreet Singh Yadav", "Chandra Sekhar Seelamantula", "Chetan Singh Thakur"], "title": "Live Demonstration: Neuromorphic Radar for Gesture Recognition", "categories": ["cs.CV", "cs.ET", "cs.NE", "cs.SY", "eess.SY"], "comment": "Neuromorphic Radar, Hand Gesture Recognition, Event-Driven,\n  Sigma-Delta Encoding, Sparse Representation. Presented in ICASSP 2025 at\n  Hyderabad, India", "summary": "We present a neuromorphic radar framework for real-time, low-power hand\ngesture recognition (HGR) using an event-driven architecture inspired by\nbiological sensing. Our system comprises a 24 GHz Doppler radar front-end and a\ncustom neuromorphic sampler that converts intermediate-frequency (IF) signals\ninto sparse spike-based representations via asynchronous sigma-delta encoding.\nThese events are directly processed by a lightweight neural network deployed on\na Cortex-M0 microcontroller, enabling low-latency inference without requiring\nspectrogram reconstruction. Unlike conventional radar HGR pipelines that\ncontinuously sample and process data, our architecture activates only when\nmeaningful motion is detected, significantly reducing memory, power, and\ncomputation overhead. Evaluated on a dataset of five gestures collected from\nseven users, our system achieves > 85% real-time accuracy. To the best of our\nknowledge, this is the first work that employs bio-inspired asynchronous\nsigma-delta encoding and an event-driven processing framework for radar-based\nHGR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u7684\u795e\u7ecf\u5f62\u6001\u96f7\u8fbe\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u3001\u4f4e\u529f\u8017\u7684\u624b\u52bf\u8bc6\u522b\uff0c\u91c7\u7528\u5f02\u6b65Sigma-Delta\u7f16\u7801\u548c\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u80fd\u6548\u3002", "motivation": "\u4f20\u7edf\u96f7\u8fbe\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u9700\u8981\u8fde\u7eed\u91c7\u6837\u548c\u5904\u7406\u6570\u636e\uff0c\u529f\u8017\u548c\u8ba1\u7b97\u5f00\u9500\u8f83\u5927\u3002\u672c\u6587\u53d7\u751f\u7269\u611f\u77e5\u542f\u53d1\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e8b\u4ef6\u9a71\u52a8\u7684\u67b6\u6784\uff0c\u4ee5\u964d\u4f4e\u80fd\u8017\u548c\u5ef6\u8fdf\u3002", "method": "\u4f7f\u752824 GHz\u591a\u666e\u52d2\u96f7\u8fbe\u524d\u7aef\u548c\u81ea\u5b9a\u4e49\u795e\u7ecf\u5f62\u6001\u91c7\u6837\u5668\uff0c\u5c06\u4e2d\u9891\u4fe1\u53f7\u901a\u8fc7\u5f02\u6b65Sigma-Delta\u7f16\u7801\u8f6c\u6362\u4e3a\u7a00\u758f\u7684\u57fa\u4e8e\u4e8b\u4ef6\u7684\u7279\u5f81\u3002\u8fd9\u4e9b\u4e8b\u4ef6\u7531Cortex-M0\u5fae\u63a7\u5236\u5668\u4e0a\u7684\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u5904\u7406\u3002", "result": "\u57287\u4f4d\u7528\u6237\u76845\u79cd\u624b\u52bf\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5b9e\u65f6\u51c6\u786e\u7387>85%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u3001\u529f\u8017\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u96f7\u8fbe\u624b\u52bf\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4f4e\u529f\u8017\u3001\u4f4e\u5ef6\u8fdf\u89e3\u51b3\u65b9\u6848\uff0c\u9996\u6b21\u5c06\u5f02\u6b65Sigma-Delta\u7f16\u7801\u548c\u4e8b\u4ef6\u9a71\u52a8\u5904\u7406\u5e94\u7528\u4e8e\u8be5\u9886\u57df\u3002"}}
{"id": "2508.03287", "pdf": "https://arxiv.org/pdf/2508.03287", "abs": "https://arxiv.org/abs/2508.03287", "authors": ["Falk Dettinger", "Matthias Wei\u00df", "Daniel Baumann", "Martin Sommer", "Michael Weyrich"], "title": "Directives for Function Offloading in 5G Networks Based on a Performance Characteristics Analysis", "categories": ["cs.NI", "cs.DC", "C.2.4; C.4"], "comment": "7 pages, 4 figures", "summary": "Cloud-based offloading helps address energy consumption and performance\nchallenges in executing resource-intensive vehicle algorithms. Utilizing 5G,\nwith its low latency and high bandwidth, enables seamless vehicle-to-cloud\nintegration. Currently, only non-standalone 5G is publicly available, and\nreal-world applications remain underexplored compared to theoretical studies.\nThis paper evaluates 5G non-standalone networks for cloud execution of vehicle\nfunctions, focusing on latency, Round Trip Time, and packet delivery. Tests\nused two AI-based algorithms -- emotion recognition and object recognition --\nalong an 8.8 km route in Baden-W\\\"urttemberg, Germany, encompassing urban,\nrural, and forested areas. Two platforms were analyzed: a cloudlet in Frankfurt\nand a cloud in Mannheim, employing various deployment strategies like\nconventional applications and containerized and container-orchestrated setups.\nKey findings highlight an average signal quality of 84 %, with no connectivity\ninterruptions despite minor drops in built-up areas. Packet analysis revealed a\nPacket Error Rate below 0.1 % for both algorithms. Transfer times varied\nsignificantly depending on the geographical location and the backend servers'\nnetwork connections, while processing times were mainly influenced by the\ncomputation hardware in use. Additionally, cloud offloading seems only be a\nsuitable option, when a round trip time of more than 150 ms is possible.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e865G\u975e\u72ec\u7acb\u7f51\u7edc\u5728\u8f66\u8f86\u529f\u80fd\u4e91\u6267\u884c\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u5ef6\u8fdf\u3001\u5f80\u8fd4\u65f6\u95f4\u548c\u6570\u636e\u5305\u4ea4\u4ed8\u7387\uff0c\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\u4fe1\u53f7\u8d28\u91cf\u826f\u597d\uff0c\u6570\u636e\u4f20\u8f93\u7a33\u5b9a\uff0c\u4f46\u4e91\u5378\u8f7d\u7684\u6548\u679c\u53d7\u5730\u7406\u548c\u786c\u4ef6\u9650\u5236\u3002", "motivation": "\u9488\u5bf95G\u975e\u72ec\u7acb\u7f51\u7edc\u5728\u8f66\u8f86\u7b97\u6cd5\u4e91\u5378\u8f7d\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u8f83\u5c11\uff0c\u7814\u7a76\u586b\u8865\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\uff0c\u63a2\u7d22\u5176\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u5728\u5fb7\u56fd\u4e0d\u540c\u5730\u7406\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e24\u79cdAI\u7b97\u6cd5\uff08\u60c5\u7eea\u8bc6\u522b\u548c\u7269\u4f53\u8bc6\u522b\uff09\uff0c\u4f7f\u7528\u6cd5\u5170\u514b\u798f\u7684\u4e91\u7aef\u548c\u66fc\u6d77\u59c6\u7684\u4e91\u7aef\uff0c\u91c7\u7528\u591a\u79cd\u90e8\u7f72\u7b56\u7565\u8fdb\u884c\u5206\u6790\u3002", "result": "\u4fe1\u53f7\u8d28\u91cf\u5e73\u574784%\uff0c\u65e0\u8fde\u63a5\u4e2d\u65ad\uff0c\u6570\u636e\u5305\u9519\u8bef\u7387\u4f4e\u4e8e0.1%\uff0c\u4f20\u8f93\u65f6\u95f4\u56e0\u5730\u800c\u5f02\uff0c\u5904\u7406\u65f6\u95f4\u53d7\u8ba1\u7b97\u786c\u4ef6\u5f71\u54cd\u3002\u4e91\u5378\u8f7d\u9700\u5f80\u8fd4\u65f6\u95f4\u8d85\u8fc7150ms\u624d\u9002\u7528\u3002", "conclusion": "5G\u975e\u72ec\u7acb\u7f51\u7edc\u5728\u8f66\u8f86\u4e91\u5378\u8f7d\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u4f46\u5730\u7406\u548c\u786c\u4ef6\u56e0\u7d20\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u4e91\u5378\u8f7d\u7684\u5b9e\u7528\u6027\u53d6\u51b3\u4e8e\u5177\u4f53\u573a\u666f\u548c\u65f6\u95f4\u8981\u6c42\u3002"}}
{"id": "2508.03014", "pdf": "https://arxiv.org/pdf/2508.03014", "abs": "https://arxiv.org/abs/2508.03014", "authors": ["Jingyan Wang", "Yang Zhao", "Haotian Mao", "Xubo Yang"], "title": "Survey of Large Language Models in Extended Reality: Technical Paradigms and Application Frontiers", "categories": ["cs.HC", "I.2, H.5"], "comment": "29 pages, 5 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation, and their integration with\nExtended Reality (XR) is poised to transform how users interact with immersive\nenvironments. This survey provides a comprehensive review of recent\ndevelopments at the intersection of LLMs and XR, offering a structured\norganization of research along both technical and application dimensions. We\npropose a taxonomy of LLM-enhanced XR systems centered on key technical\nparadigms -- such as interactive agent control, XR development toolkits, and\ngenerative scene synthesis -- and discuss how these paradigms enable novel\ncapabilities in XR. In parallel, we examine how LLM-driven techniques support\npractical XR applications across diverse domains, including immersive\neducation, clinical healthcare, and industrial manufacturing. By connecting\nthese technical paradigms with application frontiers, our survey highlights\ncurrent trends, delineates design considerations, and identifies open\nchallenges in building LLM-augmented XR systems. This work provides insights\nthat can guide researchers and practitioners in advancing the state of the art\nin intelligent XR experiences.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u6269\u5c55\u73b0\u5b9e\uff08XR\uff09\u7ed3\u5408\u7684\u73b0\u72b6\uff0c\u63d0\u4f9b\u4e86\u6280\u672f\u8303\u5f0f\u548c\u5e94\u7528\u9886\u57df\u7684\u5206\u7c7b\uff0c\u5e76\u63a2\u8ba8\u4e86\u53d1\u5c55\u8d8b\u52bf\u548c\u6311\u6218\u3002", "motivation": "\u901a\u8fc7\u878d\u5408LLM\u4e0eXR\u6280\u672f\uff0c\u63d0\u5347\u7528\u6237\u5728\u6c89\u6d78\u5f0f\u73af\u5883\u4e2d\u7684\u4ea4\u4e92\u4f53\u9a8c\uff0c\u63a8\u52a8\u667a\u80fdXR\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6280\u672f\u8303\u5f0f\u7684\u5206\u7c7b\u6cd5\uff0c\u5305\u62ec\u4ea4\u4e92\u4ee3\u7406\u63a7\u5236\u3001XR\u5f00\u53d1\u5de5\u5177\u5305\u548c\u751f\u6210\u5f0f\u573a\u666f\u5408\u6210\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u6280\u672f\u5982\u4f55\u652f\u6301XR\u5e94\u7528\u3002", "result": "\u8bba\u6587\u603b\u7ed3\u4e86LLM\u4e0eXR\u7ed3\u5408\u7684\u6280\u672f\u548c\u5e94\u7528\u8d8b\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u8bbe\u8ba1\u8003\u8651\u548c\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u667a\u80fdXR\u4f53\u9a8c\u7684\u524d\u6cbf\u53d1\u5c55\u3002"}}
{"id": "2508.03611", "pdf": "https://arxiv.org/pdf/2508.03611", "abs": "https://arxiv.org/abs/2508.03611", "authors": ["Wei Da", "Evangelia Kalyvianaki"], "title": "Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling", "categories": ["cs.DC", "cs.AI"], "comment": "12 pages, 8 figures excluding appendix", "summary": "This paper presents Block, a distributed scheduling framework designed to\noptimize load balancing and auto-provisioning across instances in large\nlanguage model serving frameworks by leveraging contextual information from\nincoming requests. Unlike popular model serving systems that rely on monolithic\nand heuristic task schedulers, Block operates as a fully distributed,\nstateless, and predictive scheduling system to achieve low overhead,\nreliability, and scalability. It leverages the deterministic and predictable\ncharacteristics of LLM inferences, such as host configurations, response\nlengths, and hardware performance, to make scheduling decisions based on\naccurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block\nsignificantly outperforms heuristic schedulers, boosting serving capacity by up\nto 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance\ngains remain consistent across diverse models, workloads and configurations.\nCode and data are open-sourced.", "AI": {"tldr": "Block\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u8bf7\u6c42\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u4e2d\u7684\u8d1f\u8f7d\u5747\u8861\u548c\u81ea\u52a8\u8d44\u6e90\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u670d\u52a1\u5bb9\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u5355\u4f53\u7684\u542f\u53d1\u5f0f\u4efb\u52a1\u8c03\u5ea6\u5668\uff0c\u96be\u4ee5\u6ee1\u8db3\u5927\u89c4\u6a21\u670d\u52a1\u7684\u4f4e\u5f00\u9500\u3001\u53ef\u9760\u6027\u548c\u53ef\u6269\u5c55\u6027\u9700\u6c42\u3002Block\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "Block\u91c7\u7528\u5b8c\u5168\u5206\u5e03\u5f0f\u3001\u65e0\u72b6\u6001\u548c\u9884\u6d4b\u6027\u8c03\u5ea6\uff0c\u5229\u7528LLM\u63a8\u7406\u7684\u786e\u5b9a\u6027\u7279\u6027\uff08\u5982\u4e3b\u673a\u914d\u7f6e\u3001\u54cd\u5e94\u957f\u5ea6\u548c\u786c\u4ef6\u6027\u80fd\uff09\u8fdb\u884c\u51b3\u7b56\u3002", "result": "\u572812\u4e2aGPU\u96c6\u7fa4\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cBlock\u6bd4\u542f\u53d1\u5f0f\u8c03\u5ea6\u5668\u6027\u80fd\u63d0\u5347\u663e\u8457\uff0c\u670d\u52a1\u5bb9\u91cf\u63d0\u534716.7%\uff0cP99\u5c3e\u90e8\u5ef6\u8fdf\u964d\u4f4e49.5%\u3002", "conclusion": "Block\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u8c03\u5ea6\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u548c\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4e14\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.03258", "pdf": "https://arxiv.org/pdf/2508.03258", "abs": "https://arxiv.org/abs/2508.03258", "authors": ["Yueyue Liu", "Hongyu Zhang", "Yuantian Miao"], "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SmartLLMs\u8c03\u5ea6\u5668\uff08SLS\uff09\uff0c\u4e00\u79cd\u52a8\u6001\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u90e8\u7f72\u6548\u7387\u3002SLS\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u548c\u52a8\u6001\u66f4\u65b0\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u73b0\u6709LLM\u90e8\u7f72\u5728\u591a\u6837\u4efb\u52a1\u4e2d\u9762\u4e34\u9ad8\u6210\u672c\u3001\u957f\u54cd\u5e94\u65f6\u95f4\u548c\u6027\u80fd\u4e0d\u7a33\u5b9a\u7684\u6311\u6218\uff0c\u9759\u6001\u8c03\u5ea6\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "SLS\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u81ea\u9002\u5e94\u7f13\u5b58\u7ba1\u7406\u5668\u3001\u6027\u80fd-\u6210\u672c\u4f18\u5316\u8c03\u5ea6\u5668\u548c\u52a8\u6001\u66f4\u65b0\u7ba1\u7406\u5668\uff0c\u5206\u522b\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3001\u52a8\u6001\u5206\u914d\u4efb\u52a1\u5e76\u5b9e\u65f6\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSLS\u5728\u65e5\u5fd7\u89e3\u6790\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u5347198.82%\uff0c\u5904\u7406\u65f6\u95f4\u51cf\u5c1163.28%\u3002", "conclusion": "SLS\u901a\u8fc7\u52a8\u6001\u8c03\u5ea6\u548c\u5b9e\u65f6\u53cd\u9988\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u5b9e\u7528\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u3002"}}
{"id": "2508.03638", "pdf": "https://arxiv.org/pdf/2508.03638", "abs": "https://arxiv.org/abs/2508.03638", "authors": ["Marco T. Moraz\u00e1n", "Oliwia Kempinski", "Andr\u00e9s M. Garced"], "title": "Design Support for Multitape Turing Machines", "categories": ["cs.FL", "cs.HC", "cs.PL", "cs.SE"], "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "Many Formal Languages and Automata Theory courses introduce students to\nTuring machine extensions. One of the most widely-used extensions endows Turing\nmachines with multiple tapes. Although multitape Turing machines are an\nabstraction to simplify Turing machine design, students find them no less\nchallenging. To aid students in understanding these machines, the FSM\nprogramming language provides support for their definition and execution. This,\nhowever, has proven insufficient for many students to understand the\noperational semantics of such machines and to understand why such machines\naccept or reject a word. To address this problem, three visualization tools\nhave been developed. The first is a dynamic visualization tool that simulates\nmachine execution. The second is a static visualization tool that automatically\nrenders a graphic for a multitape Turing machine's transition diagram. The\nthird is a static visualization tool that automatically renders computation\ngraphs for multitape Turing machines. This article presents these tools and\nillustrates how they are used to help students design and implement multitape\nTuring machines. In addition, empirical data is presented that suggests these\ntools are well-received and found useful by students.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u4e09\u79cd\u53ef\u89c6\u5316\u5de5\u5177\u5e2e\u52a9\u5b66\u751f\u66f4\u597d\u5730\u7406\u89e3\u548c\u8bbe\u8ba1\u591a\u5e26\u56fe\u7075\u673a\uff0c\u89e3\u51b3\u4e86\u5b66\u751f\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u56f0\u96be\u3002", "motivation": "\u5b66\u751f\u5728\u5b66\u4e60\u591a\u5e26\u56fe\u7075\u673a\u65f6\u9762\u4e34\u7406\u89e3\u64cd\u4f5c\u8bed\u4e49\u548c\u63a5\u53d7/\u62d2\u7edd\u5355\u8bcd\u539f\u56e0\u7684\u56f0\u96be\uff0c\u73b0\u6709\u7684FSM\u7f16\u7a0b\u8bed\u8a00\u652f\u6301\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u53ef\u89c6\u5316\u5de5\u5177\uff1a\u52a8\u6001\u6a21\u62df\u6267\u884c\u5de5\u5177\u3001\u9759\u6001\u6e32\u67d3\u591a\u5e26\u56fe\u7075\u673a\u8f6c\u79fb\u56fe\u5de5\u5177\u548c\u9759\u6001\u6e32\u67d3\u8ba1\u7b97\u56fe\u5de5\u5177\u3002", "result": "\u8fd9\u4e9b\u5de5\u5177\u88ab\u5b66\u751f\u5e7f\u6cdb\u63a5\u53d7\u5e76\u8ba4\u4e3a\u6709\u7528\uff0c\u5b9e\u8bc1\u6570\u636e\u652f\u6301\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u89c6\u5316\u5de5\u5177\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u5bf9\u591a\u5e26\u56fe\u7075\u673a\u7684\u7406\u89e3\u548c\u8bbe\u8ba1\u80fd\u529b\u3002"}}
{"id": "2508.03376", "pdf": "https://arxiv.org/pdf/2508.03376", "abs": "https://arxiv.org/abs/2508.03376", "authors": ["Jun Wu", "Jiaqi Yang", "Jicun Li", "Wei Xie", "Xiang-Yang Li"], "title": "Efficient Variational Quantum Algorithms via Circuit Knitting and Architecture Search", "categories": ["quant-ph", "cs.ET"], "comment": "13 pages, 10 figures", "summary": "Current quantum hardware presents a significant limitation in the number of\navailable qubits compared to the requirements of practical quantum algorithms.\nCircuit knitting has been proposed as a solution to this issue by partitioning\nlarger quantum circuits into smaller parts that can be executed by current\ndevices. However, this approach often leads to a high sampling overhead, which\nincreases exponentially with the number of cut points. In this paper, we\nintroduce CKVQA, a framework that applies circuit knitting to variational\nquantum algorithms (VQAs). By employing a quantum circuit architecture search\nadapted to this scenario, CKVQA aims to minimize the sampling overhead by\nidentifying parameterized quantum circuits that achieve a favorable balance\nbetween algorithmic performance and sampling overhead. Additionally, since\ncircuit knitting generates multiple subcircuits, we have developed a\nsubcircuit-level optimization method to accelerate the training of VQAs and\nreduce overall execution time. We apply this framework to two widely-used VQAs:\nthe Quantum Approximate Optimization Algorithm and the Variational Quantum\nEigensolver. Our numerical results demonstrate that the CKVQA framework\nsignificantly reduces the sampling overheads while maintaining comparable\naccuracy to conventional parameterized quantum circuit designs.", "AI": {"tldr": "CKVQA\u6846\u67b6\u901a\u8fc7\u7535\u8def\u7f16\u7ec7\u548c\u5b50\u7535\u8def\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u91c7\u6837\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4f20\u7edf\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u91cf\u5b50\u786c\u4ef6\u53ef\u7528\u7684\u91cf\u5b50\u6bd4\u7279\u6570\u8fdc\u4f4e\u4e8e\u5b9e\u7528\u91cf\u5b50\u7b97\u6cd5\u7684\u9700\u6c42\uff0c\u7535\u8def\u7f16\u7ec7\u867d\u80fd\u89e3\u51b3\u4f46\u5e26\u6765\u9ad8\u91c7\u6837\u5f00\u9500\u3002", "method": "\u5f15\u5165CKVQA\u6846\u67b6\uff0c\u7ed3\u5408\u91cf\u5b50\u7535\u8def\u67b6\u6784\u641c\u7d22\u548c\u5b50\u7535\u8def\u7ea7\u4f18\u5316\uff0c\u5e94\u7528\u4e8e\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\uff08VQA\uff09\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793aCKVQA\u663e\u8457\u964d\u4f4e\u91c7\u6837\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4f20\u7edf\u8bbe\u8ba1\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "CKVQA\u4e3a\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7535\u8def\u7ec7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03321", "pdf": "https://arxiv.org/pdf/2508.03321", "abs": "https://arxiv.org/abs/2508.03321", "authors": ["J\u00f6rn Bodenhausen", "Simon Mangel", "Thomas Vogt", "Martin Henze"], "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT Scenarios", "categories": ["cs.NI", "cs.CR"], "comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)", "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.", "AI": {"tldr": "BiTHaC\u901a\u8fc7\u53cc\u5411TLS\u63e1\u624b\u7f13\u5b58\u51cf\u5c11\u5de5\u4e1a\u7269\u8054\u7f51\u4e2d\u7684\u5e26\u5bbd\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5de5\u4e1a\u7269\u8054\u7f51\u8bbe\u5907\u8d44\u6e90\u53d7\u9650\uff0c\u4f20\u7edfTLS\u63e1\u624b\u5f00\u9500\u5927\uff0c\u9700\u4f18\u5316\u3002", "method": "\u5229\u7528\u91cd\u590dTLS\u63e1\u624b\u9759\u6001\u90e8\u5206\uff08\u5982\u8bc1\u4e66\uff09\u5b9e\u73b0\u7f13\u5b58\uff0c\u907f\u514d\u5197\u4f59\u4f20\u8f93\u3002", "result": "\u5e26\u5bbd\u6d88\u8017\u51cf\u5c1161.1%\uff0c\u8ba1\u7b97\u5f00\u9500\u964d\u4f4e8.5%\uff0c\u5185\u5b58\u5f00\u9500\u53ef\u63a7\u3002", "conclusion": "BiTHaC\u5728\u4fdd\u8bc1TLS\u5b89\u5168\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2508.03061", "pdf": "https://arxiv.org/pdf/2508.03061", "abs": "https://arxiv.org/abs/2508.03061", "authors": ["Shuchang Xu"], "title": "Facilitating Visual Media Exploration for Blind and Low Vision Users through AI-Powered Interactive Storytelling", "categories": ["cs.HC"], "comment": null, "summary": "Empowering blind and low vision (BLV) users to explore visual media improves\ncontent comprehension, strengthens user agency, and fulfills diverse\ninformation needs. However, most existing tools separate exploration from the\nmain narration, which disrupts the narrative flow, increases cognitive load,\nand limits deep engagement with visual media. To address these challenges, my\nPhD research introduces the paradigm of AI-powered interactive storytelling,\nwhich leverages AI to generate interactive narratives, enabling BLV users to\nexplore visual media within a coherent storytelling experience. I have\noperationalized this paradigm through three techniques: (1) Hierarchical\nNarrative, which supports photo-collection exploration at different levels of\ndetail; (2) Parallel Narrative, which provides seamless access to time-synced\nvideo comments; and (3) Branching Narrative, which enables immersive navigation\nof 360{\\deg} videos. Together, these techniques demonstrate that AI-powered\ninteractive storytelling can effectively balance user agency with narrative\ncoherence across diverse media formats. My future work will advance this\nparadigm by enabling more personalized and expressive storytelling experiences\nfor BLV audiences.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdAI\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u8303\u5f0f\uff0c\u5e2e\u52a9\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u5728\u8fde\u8d2f\u7684\u53d9\u4e8b\u4e2d\u63a2\u7d22\u89c6\u89c9\u5a92\u4f53\uff0c\u901a\u8fc7\u4e09\u79cd\u6280\u672f\u5b9e\u73b0\uff1a\u5c42\u6b21\u53d9\u4e8b\u3001\u5e76\u884c\u53d9\u4e8b\u548c\u5206\u652f\u53d9\u4e8b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5c06\u89c6\u89c9\u5a92\u4f53\u63a2\u7d22\u4e0e\u4e3b\u8981\u53d9\u4e8b\u5206\u79bb\uff0c\u7834\u574f\u4e86\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u589e\u52a0\u4e86\u8ba4\u77e5\u8d1f\u62c5\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u6539\u5584\u8fd9\u4e00\u73b0\u8c61\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528AI\u751f\u6210\u4ea4\u4e92\u5f0f\u53d9\u4e8b\uff0c\u5f00\u53d1\u4e86\u5206\u5c42\u53d9\u4e8b\u3001\u5e76\u884c\u53d9\u4e8b\u548c\u5206\u652f\u53d9\u4e8b\u4e09\u79cd\u6280\u672f\uff0c\u5206\u522b\u9488\u5bf9\u7167\u7247\u96c6\u3001\u89c6\u9891\u8bc4\u8bba\u548c360\u5ea6\u89c6\u9891\u7684\u63a2\u7d22\u3002", "result": "\u7814\u7a76\u8bc1\u660e\uff0cAI\u9a71\u52a8\u7684\u4ea4\u4e92\u5f0f\u53d9\u4e8b\u80fd\u5728\u591a\u79cd\u5a92\u4f53\u683c\u5f0f\u4e2d\u5e73\u8861\u7528\u6237\u81ea\u4e3b\u6027\u4e0e\u53d9\u4e8b\u8fde\u8d2f\u6027\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u8fdb\u4e00\u6b65\u63d0\u5347\u53d9\u4e8b\u7684\u4e2a\u6027\u5316\u548c\u8868\u8fbe\u529b\uff0c\u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u7528\u6237\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4f53\u9a8c\u3002"}}
{"id": "2508.03148", "pdf": "https://arxiv.org/pdf/2508.03148", "abs": "https://arxiv.org/abs/2508.03148", "authors": ["Yicheng Feng", "Xin Tan", "Kin Hang Sew", "Yimin Jiang", "Yibo Zhu", "Hong Xu"], "title": "Frontier: Simulating the Next Generation of LLM Inference Systems", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Large Language Model (LLM) inference is growing increasingly complex with the\nrise of Mixture-of-Experts (MoE) models and disaggregated architectures that\ndecouple components like prefill/decode (PD) or attention/FFN (AF) for\nheterogeneous scaling. Existing simulators, architected for co-located, dense\nmodels, are unable to capture the intricate system dynamics of these emerging\nparadigms. We present Frontier, a high-fidelity simulator designed from the\nground up for this new landscape. Frontier introduces a unified framework to\nmodel both co-located and disaggregated systems, providing native support for\nMoE inference with expert parallelism (EP). It enables the simulation of\ncomplex workflows like cross-cluster expert routing and advanced pipelining\nstrategies for latency hiding. To ensure fidelity and usability, Frontier\nincorporates refined operator models for improved accuracy. Frontier empowers\nthe community to design and optimize the future of LLM inference at scale.", "AI": {"tldr": "Frontier\u662f\u4e00\u4e2a\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\uff0c\u4e13\u4e3a\u590d\u6742\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u8bbe\u8ba1\uff0c\u652f\u6301MoE\u6a21\u578b\u548c\u89e3\u8026\u67b6\u6784\u3002", "motivation": "\u968f\u7740MoE\u6a21\u578b\u548c\u89e3\u8026\u67b6\u6784\u7684\u5174\u8d77\uff0c\u73b0\u6709\u6a21\u62df\u5668\u65e0\u6cd5\u6355\u6349\u65b0\u7684\u7cfb\u7edf\u52a8\u6001\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6a21\u62df\u5668\u6765\u652f\u6301\u8fd9\u4e9b\u65b0\u5174\u8303\u5f0f\u3002", "method": "Frontier\u91c7\u7528\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u5171\u5740\u548c\u89e3\u8026\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u5bf9MoE\u63a8\u7406\u7684\u539f\u751f\u652f\u6301\uff0c\u5305\u62ec\u4e13\u5bb6\u5e76\u884c\uff08EP\uff09\u548c\u590d\u6742\u5de5\u4f5c\u6d41\u6a21\u62df\u3002", "result": "Frontier\u901a\u8fc7\u6539\u8fdb\u7684\u64cd\u4f5c\u6a21\u578b\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5e76\u652f\u6301\u8de8\u96c6\u7fa4\u4e13\u5bb6\u8def\u7531\u548c\u9ad8\u7ea7\u6d41\u6c34\u7ebf\u7b56\u7565\u3002", "conclusion": "Frontier\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u4ee5\u8bbe\u8ba1\u548c\u4f18\u5316\u672a\u6765\u5927\u89c4\u6a21LLM\u63a8\u7406\u3002"}}
{"id": "2508.03298", "pdf": "https://arxiv.org/pdf/2508.03298", "abs": "https://arxiv.org/abs/2508.03298", "authors": ["Kristian Kolthoff", "Felix Kretzer", "Christian Bartelt", "Alexander Maedche", "Simone Paolo Ponzetto"], "title": "GUI-ReRank: Enhancing GUI Retrieval with Multi-Modal LLM-based Reranking", "categories": ["cs.SE"], "comment": null, "summary": "GUI prototyping is a fundamental component in the development of modern\ninteractive systems, which are now ubiquitous across diverse application\ndomains. GUI prototypes play a critical role in requirements elicitation by\nenabling stakeholders to visualize, assess, and refine system concepts\ncollaboratively. Moreover, prototypes serve as effective tools for early\ntesting, iterative evaluation, and validation of design ideas with both end\nusers and development teams. Despite these advantages, the process of\nconstructing GUI prototypes remains resource-intensive and time-consuming,\nfrequently demanding substantial effort and expertise. Recent research has\nsought to alleviate this burden through NL-based GUI retrieval approaches,\nwhich typically rely on embedding-based retrieval or tailored ranking models\nfor specific GUI repositories. However, these methods often suffer from limited\nretrieval performance and struggle to generalize across arbitrary GUI datasets.\nIn this work, we present GUI-ReRank, a novel framework that integrates rapid\nembedding-based constrained retrieval models with highly effective MLLM-based\nreranking techniques. GUI-ReRank further introduces a fully customizable GUI\nrepository annotation and embedding pipeline, enabling users to effortlessly\nmake their own GUI repositories searchable, which allows for rapid discovery of\nrelevant GUIs for inspiration or seamless integration into customized LLM-based\nRAG workflows. We evaluated our approach on an established NL-based GUI\nretrieval benchmark, demonstrating that GUI-ReRank significantly outperforms\nSOTA tailored LTR models in both retrieval accuracy and generalizability.\nAdditionally, we conducted a comprehensive cost and efficiency analysis of\nemploying MLLMs for reranking, providing valuable insights regarding the\ntrade-offs between retrieval effectiveness and computational resources. Video:\nhttps://youtu.be/_7x9UCh82ug", "AI": {"tldr": "GUI-ReRank\u662f\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5feb\u901f\u5d4c\u5165\u68c0\u7d22\u4e0eMLLM\u91cd\u6392\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86GUI\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "GUI\u539f\u578b\u6784\u5efa\u8d44\u6e90\u5bc6\u96c6\u4e14\u8017\u65f6\uff0c\u73b0\u6709NL\u68c0\u7d22\u65b9\u6cd5\u6027\u80fd\u6709\u9650\u4e14\u96be\u4ee5\u901a\u7528\u3002", "method": "\u96c6\u6210\u5feb\u901f\u5d4c\u5165\u68c0\u7d22\u4e0eMLLM\u91cd\u6392\u6280\u672f\uff0c\u63d0\u4f9b\u81ea\u5b9a\u4e49GUI\u4ed3\u5e93\u6807\u6ce8\u548c\u5d4c\u5165\u6d41\u7a0b\u3002", "result": "\u5728GUI\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aSOTA\u6a21\u578b\uff0c\u63d0\u4f9b\u68c0\u7d22\u6548\u679c\u4e0e\u8ba1\u7b97\u8d44\u6e90\u7684\u6743\u8861\u5206\u6790\u3002", "conclusion": "GUI-ReRank\u63d0\u9ad8\u4e86\u68c0\u7d22\u6548\u7387\u548c\u901a\u7528\u6027\uff0c\u9002\u5408\u96c6\u6210\u5230LLM\u5de5\u4f5c\u6d41\u4e2d\u3002"}}
{"id": "2508.03639", "pdf": "https://arxiv.org/pdf/2508.03639", "abs": "https://arxiv.org/abs/2508.03639", "authors": ["Marco T. Moraz\u00e1n", "Shamil Dzhatdoyev", "Josephine Des Rosiers", "Tijana Mini\u0107", "Andr\u00e9s M. Garced", "David Anthony K. Fields"], "title": "A Design Recipe and Recipe-Based Errors for Regular Expressions", "categories": ["cs.FL", "cs.HC", "cs.PL", "cs.SE"], "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "This article presents a novel framework to provide Formal Languages and\nAutomata Theory students design support for the development of regular\nexpressions. This framework includes a design recipe for regular expressions\nand a customized error messaging system. The error messaging system produces\nrecipe-based errors that include the step of the design recipe not successfully\ncompleted. Furthermore, the error messages follow the established practices of\nbeing concise, succinct, jargon-free, and nonprescriptive. In addition, a\nshorthand syntax developed for writing unit tests is described. The in-class\nuse of the design recipe is illustrated, two debugging sessions using the\ndescribed system are discussed, and the implementation of the error messaging\nsystem is briefly sketched.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u652f\u6301\u5f62\u5f0f\u8bed\u8a00\u4e0e\u81ea\u52a8\u673a\u7406\u8bba\u5b66\u751f\u5b66\u4e60\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u8bbe\u8ba1\u3002\u6846\u67b6\u5305\u62ec\u8bbe\u8ba1\u65b9\u6cd5\u548c\u5b9a\u5236\u9519\u8bef\u63d0\u793a\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u5b66\u751f\u5b66\u4e60\u6b63\u5219\u8868\u8fbe\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u652f\u6301\uff0c\u6539\u8fdb\u9519\u8bef\u63d0\u793a\u7684\u6e05\u6670\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bbe\u8ba1\u65b9\u6cd5\u3001\u5b9a\u5236\u7684\u9519\u8bef\u63d0\u793a\u7cfb\u7edf\u548c\u7b80\u5316\u7684\u5355\u5143\u6d4b\u8bd5\u8bed\u6cd5\u3002", "result": "\u6846\u67b6\u5728\u8bfe\u5802\u4e2d\u4f7f\u7528\uff0c\u6f14\u793a\u4e86\u8c03\u8bd5\u4f1a\u8bdd\uff0c\u5e76\u521d\u6b65\u5b9e\u73b0\u4e86\u9519\u8bef\u63d0\u793a\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u652f\u6301\u5b66\u751f\u5b66\u4e60\uff0c\u9519\u8bef\u63d0\u793a\u7cfb\u7edf\u8bbe\u8ba1\u5b9e\u7528\u4e14\u7b26\u5408\u6700\u4f73\u5b9e\u8df5\u3002"}}
{"id": "2508.03584", "pdf": "https://arxiv.org/pdf/2508.03584", "abs": "https://arxiv.org/abs/2508.03584", "authors": ["Fatih Gulec", "Hamdan Awan", "Nigel Wallbridge", "Andrew W. Eckford"], "title": "Decoding and Engineering the Phytobiome Communication for Smart Agriculture", "categories": ["eess.SP", "cs.AI", "cs.ET", "cs.NI", "q-bio.MN"], "comment": "Under revision for IEEE Communications Magazine", "summary": "Smart agriculture applications, integrating technologies like the Internet of\nThings and machine learning/artificial intelligence (ML/AI) into agriculture,\nhold promise to address modern challenges of rising food demand, environmental\npollution, and water scarcity. Alongside the concept of the phytobiome, which\ndefines the area including the plant, its environment, and associated\norganisms, and the recent emergence of molecular communication (MC), there\nexists an important opportunity to advance agricultural science and practice\nusing communication theory. In this article, we motivate to use the\ncommunication engineering perspective for developing a holistic understanding\nof the phytobiome communication and bridge the gap between the phytobiome\ncommunication and smart agriculture. Firstly, an overview of phytobiome\ncommunication via molecular and electrophysiological signals is presented and a\nmulti-scale framework modeling the phytobiome as a communication network is\nconceptualized. Then, how this framework is used to model electrophysiological\nsignals is demonstrated with plant experiments. Furthermore, possible smart\nagriculture applications, such as smart irrigation and targeted delivery of\nagrochemicals, through engineering the phytobiome communication are proposed.\nThese applications merge ML/AI methods with the Internet of Bio-Nano-Things\nenabled by MC and pave the way towards more efficient, sustainable, and\neco-friendly agricultural production. Finally, the implementation challenges,\nopen research issues, and industrial outlook for these applications are\ndiscussed.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u901a\u4fe1\u5de5\u7a0b\u89c6\u89d2\u7406\u89e3\u690d\u7269-\u73af\u5883-\u751f\u7269\u7fa4\u843d\uff08phytobiome\uff09\u7684\u901a\u4fe1\uff0c\u5e76\u5c06\u5176\u4e0e\u667a\u80fd\u519c\u4e1a\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c3a\u5ea6\u6846\u67b6\u6a21\u578b\u53ca\u5176\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u73b0\u4ee3\u519c\u4e1a\u9762\u4e34\u98df\u7269\u9700\u6c42\u589e\u957f\u3001\u73af\u5883\u6c61\u67d3\u548c\u6c34\u8d44\u6e90\u77ed\u7f3a\u7684\u6311\u6218\uff0c\u667a\u80fd\u519c\u4e1a\u6280\u672f\uff08\u5982\u7269\u8054\u7f51\u548cML/AI\uff09\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002\u8bba\u6587\u65e8\u5728\u5229\u7528\u901a\u4fe1\u7406\u8bba\u63a8\u52a8\u519c\u4e1a\u79d1\u5b66\u548c\u5b9e\u8df5\u3002", "method": "\u9996\u5148\u6982\u8ff0\u4e86phytobiome\u901a\u8fc7\u5206\u5b50\u548c\u7535\u751f\u7406\u4fe1\u53f7\u8fdb\u884c\u7684\u901a\u4fe1\uff0c\u5e76\u63d0\u51fa\u4e86\u5c06\u5176\u5efa\u6a21\u4e3a\u901a\u4fe1\u7f51\u7edc\u7684\u591a\u5c3a\u5ea6\u6846\u67b6\u3002\u901a\u8fc7\u690d\u7269\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7535\u751f\u7406\u4fe1\u53f7\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u667a\u80fd\u519c\u4e1a\u5e94\u7528\uff08\u5982\u667a\u80fd\u704c\u6e89\u548c\u7cbe\u51c6\u519c\u5316\u8f93\u9001\uff09\uff0c\u5e76\u5c06ML/AI\u4e0e\u5206\u5b50\u901a\u4fe1\uff08MC\uff09\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4e3a\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u548c\u73af\u4fdd\u7684\u519c\u4e1a\u751f\u4ea7\u94fa\u5e73\u9053\u8def\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86phytobiome\u901a\u4fe1\u6a21\u578b\u5728\u667a\u80fd\u519c\u4e1a\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b9e\u65bd\u6311\u6218\u3001\u5f00\u653e\u7814\u7a76\u95ee\u9898\u548c\u5de5\u4e1a\u524d\u666f\u3002"}}
{"id": "2508.03182", "pdf": "https://arxiv.org/pdf/2508.03182", "abs": "https://arxiv.org/abs/2508.03182", "authors": ["Sangho Suh", "Michael Lai", "Kevin Pu", "Steven P. Dow", "Tovi Grossman"], "title": "StoryEnsemble: Enabling Dynamic Exploration & Iteration in the Design Process with AI and Forward-Backward Propagation", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Design processes involve exploration, iteration, and movement across\ninterconnected stages such as persona creation, problem framing, solution\nideation, and prototyping. However, time and resource constraints often hinder\ndesigners from exploring broadly, collecting feedback, and revisiting earlier\nassumptions-making it difficult to uphold core design principles in practice.\nTo better understand these challenges, we conducted a formative study with 15\nparticipants-comprised of UX practitioners, students, and instructors. Based on\nthe findings, we developed StoryEnsemble, a tool that integrates AI into a\nnode-link interface and leverages forward and backward propagation to support\ndynamic exploration and iteration across the design process. A user study with\n10 participants showed that StoryEnsemble enables rapid, multi-directional\niteration and flexible navigation across design stages. This work advances our\nunderstanding of how AI can foster more iterative design practices by\nintroducing novel interactions that make exploration and iteration more fluid,\naccessible, and engaging.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u56e0\u65f6\u95f4\u548c\u8d44\u6e90\u9650\u5236\u5bfc\u81f4\u63a2\u7d22\u548c\u8fed\u4ee3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aStoryEnsemble\u7684\u5de5\u5177\uff0c\u901a\u8fc7AI\u548c\u8282\u70b9\u94fe\u63a5\u754c\u9762\u652f\u6301\u52a8\u6001\u63a2\u7d22\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\u5176\u80fd\u6709\u6548\u63d0\u5347\u8fed\u4ee3\u6548\u7387\u3002", "motivation": "\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u65f6\u95f4\u548c\u8d44\u6e90\u7684\u9650\u5236\u5e38\u963b\u788d\u8bbe\u8ba1\u5e08\u8fdb\u884c\u5e7f\u6cdb\u63a2\u7d22\u548c\u53cd\u9988\u6536\u96c6\uff0c\u96be\u4ee5\u5728\u5b9e\u8df5\u4e2d\u575a\u6301\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff0c\u4e3a\u6b64\u7814\u7a76\u56e2\u961f\u8bd5\u56fe\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u56e2\u961f\u8fdb\u884c\u4e8615\u4eba\u7684\u5f62\u6210\u6027\u7814\u7a76\uff0c\u5f00\u53d1\u4e86StoryEnsemble\u5de5\u5177\uff0c\u7ed3\u5408AI\u548c\u8282\u70b9\u94fe\u63a5\u754c\u9762\uff0c\u652f\u6301\u52a8\u6001\u63a2\u7d22\u548c\u591a\u65b9\u5411\u8fed\u4ee3\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cStoryEnsemble\u80fd\u5b9e\u73b0\u5feb\u901f\u3001\u591a\u65b9\u5411\u7684\u8fed\u4ee3\uff0c\u5e76\u652f\u6301\u7075\u6d3b\u7684\u8bbe\u8ba1\u9636\u6bb5\u5bfc\u822a\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86AI\u5982\u4f55\u901a\u8fc7\u65b0\u9896\u7684\u4ea4\u4e92\u65b9\u5f0f\u652f\u6301\u66f4\u6d41\u7545\u3001\u6613\u7528\u548c\u5438\u5f15\u4eba\u7684\u8bbe\u8ba1\u8fed\u4ee3\u5b9e\u8df5\u3002"}}
{"id": "2508.03329", "pdf": "https://arxiv.org/pdf/2508.03329", "abs": "https://arxiv.org/abs/2508.03329", "authors": ["Mari Ashiga", "Vardan Voskanyan", "Fateme Dinmohammadi", "Jingzhi Gong", "Paul Brookes", "Matthew Truscott", "Rafail Giavrimis", "Mike Basios", "Leslie Kanthan", "Wei Jie"], "title": "Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to ASE'25 Industry Showcase", "summary": "Recent advancements in Large Language Models (LLMs) for code optimization\nhave enabled industrial platforms to automate software performance engineering\nat unprecedented scale and speed. Yet, organizations in regulated industries\nface strict constraints on which LLMs they can use - many cannot utilize\ncommercial models due to data privacy regulations and compliance requirements,\ncreating a significant challenge for achieving high-quality code optimization\nwhile maintaining cost-effectiveness. We address this by implementing a\nMixture-of-Agents (MoA) approach that directly synthesizes code from multiple\nspecialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm\n(GA)-based ensemble system and individual LLM optimizers using real-world\nindustrial codebases. Our key contributions include: (1) First MoA application\nto industrial code optimization using real-world codebases; (2) Empirical\nevidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost\nsavings and 28.6% to 32.2% faster optimization times for regulated\nenvironments; (3) Deployment guidelines demonstrating GA's advantage with\ncommercial models while both ensembles outperform individual LLMs; and (4)\nReal-world validation across 50 code snippets and seven LLM combinations,\ngenerating over 8,700 variants, addresses gaps in industrial LLM ensemble\nevaluation. This provides actionable guidance for organizations balancing\nregulatory compliance with optimization performance in production environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df7\u5408\uff08MoA\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u53d7\u76d1\u7ba1\u884c\u4e1a\u4e2d\u65e0\u6cd5\u4f7f\u7528\u5546\u4e1a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ee3\u7801\u4f18\u5316\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5f00\u6e90\u6a21\u578b\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u53d7\u76d1\u7ba1\u884c\u4e1a\u56e0\u6570\u636e\u9690\u79c1\u548c\u5408\u89c4\u8981\u6c42\u65e0\u6cd5\u4f7f\u7528\u5546\u4e1aLLMs\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u4f4e\u6210\u672c\u7684\u4ee3\u7801\u4f18\u5316\u3002", "method": "\u91c7\u7528Mixture-of-Agents\uff08MoA\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4e2a\u4e13\u4e1aLLMs\uff0c\u5e76\u4e0e\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u7cfb\u7edf\u548c\u5355\u4e2aLLM\u4f18\u5316\u5668\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "MoA\u5728\u5f00\u6e90\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8282\u770114.3%-22.2%\u6210\u672c\uff0c\u4f18\u5316\u65f6\u95f4\u7f29\u77ed28.6%-32.2%\uff1bGA\u5728\u5546\u4e1a\u6a21\u578b\u4e0a\u66f4\u5177\u4f18\u52bf\uff0c\u4f46\u4e24\u79cd\u96c6\u6210\u65b9\u6cd5\u5747\u4f18\u4e8e\u5355\u4e2aLLM\u3002", "conclusion": "MoA\u65b9\u6cd5\u4e3a\u53d7\u76d1\u7ba1\u884c\u4e1a\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u4ee3\u7801\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u5408\u89c4\u6027\u4e0e\u6027\u80fd\u3002"}}
{"id": "2508.03641", "pdf": "https://arxiv.org/pdf/2508.03641", "abs": "https://arxiv.org/abs/2508.03641", "authors": ["Marco T. Moraz\u00e1n", "David Anthony K. Fields", "Andr\u00e9s M. Garced", "Tijana Mini\u0107"], "title": "Visual Execution and Validation of Finite-State Machines and Pushdown Automata", "categories": ["cs.FL", "cs.HC", "cs.PL", "cs.SE"], "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "In Formal Languages and Automata Theory courses, students find understanding\nnondeterministic finite-state and pushdown automata difficult. In many cases,\nthis means that it is challenging for them to comprehend the operational\nsemantics of such machines and, as a consequence, determine why a word is\naccepted or rejected. This is not entirely surprising, because students are\nmostly trained to design and implement deterministic programs. Comprehension of\npushdown automata is further complicated, because reasoning about the stack is\nnecessary. A common difficulty students face, for example, is understanding\nthat two different computations on the same word may reach the same state with\ndifferent stack values. To aid student understanding, we present two novel\ndynamic visualization tools for FSM -- a domain-specific programming language\nfor the Automata Theory classroom -- to support the design of such machines.\nThese tools visualize all computations that may be performed, respectively, by\na nondeterministic finite-state machine or by a pushdown automata in a stepwise\nmanner. In addition, these tools aid the machine verification process by\nallowing users to visually validate whether the properties a state represents\nhold when a machine transitions into it.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u52a8\u6001\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5e2e\u52a9\u5b66\u751f\u7406\u89e3\u975e\u786e\u5b9a\u6027\u6709\u9650\u72b6\u6001\u673a\u548c\u4e0b\u63a8\u81ea\u52a8\u673a\u7684\u5de5\u4f5c\u539f\u7406\u3002", "motivation": "\u5b66\u751f\u5728\u5b66\u4e60\u975e\u786e\u5b9a\u6027\u6709\u9650\u72b6\u6001\u673a\u548c\u4e0b\u63a8\u81ea\u52a8\u673a\u65f6\u9047\u5230\u56f0\u96be\uff0c\u5c24\u5176\u662f\u7406\u89e3\u5176\u64cd\u4f5c\u8bed\u4e49\u548c\u6808\u7684\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e24\u79cd\u52a8\u6001\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u9010\u6b65\u5c55\u793a\u975e\u786e\u5b9a\u6027\u6709\u9650\u72b6\u6001\u673a\u548c\u4e0b\u63a8\u81ea\u52a8\u673a\u7684\u6240\u6709\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u5e76\u652f\u6301\u89c6\u89c9\u9a8c\u8bc1\u72b6\u6001\u5c5e\u6027\u3002", "result": "\u5de5\u5177\u5e2e\u52a9\u5b66\u751f\u7406\u89e3\u673a\u5668\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u5e76\u652f\u6301\u9a8c\u8bc1\u72b6\u6001\u7684\u5c5e\u6027\u3002", "conclusion": "\u52a8\u6001\u53ef\u89c6\u5316\u5de5\u5177\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u751f\u5728\u5b66\u4e60\u975e\u786e\u5b9a\u6027\u6709\u9650\u72b6\u6001\u673a\u548c\u4e0b\u63a8\u81ea\u52a8\u673a\u65f6\u7684\u7406\u89e3\u96be\u9898\u3002"}}
{"id": "2508.03666", "pdf": "https://arxiv.org/pdf/2508.03666", "abs": "https://arxiv.org/abs/2508.03666", "authors": ["Willem Fourie"], "title": "Beyond risk: A proto-framework for assessing the societal impact of AI systems", "categories": ["cs.CY", "cs.AI", "cs.ET"], "comment": null, "summary": "In the discourse on AI regulation, 'responsible AI' is the dominant paradigm,\nwith the focus on mitigating the risks related to AI systems. While this focus\nis important and necessary, it has limited use for a systematic consideration\nof AI's societal impact. This paper proposes a proto-framework for assessing\nthe societal impact of AI systems by operationalising the concept of freedom.\nThis proto-framework is intended as a step towards a fully operationalised\nframework to be used in policymaking contexts. By drawing on Kantian philosophy\nand related contemporary interpretations, freedom is developed as the\ncounterpart to the concept of responsibility. Two dimensions of freedom are\ndeveloped in further detail: freedom as capability and freedom as opportunity.\nThese two dimensions of freedom are then applied in a proto-framework that\nsystematically considers AI's impact on society using the Sustainable\nDevelopment Goals. This proto-framework aims to complement current risk-based\napproaches and thereby offers a first step towards operationalising the concept\nof freedom in AI regulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u539f\u578b\u6846\u67b6\uff0c\u901a\u8fc7'\u81ea\u7531'\u6982\u5ff5\u8bc4\u4f30AI\u7684\u793e\u4f1a\u5f71\u54cd\uff0c\u4ee5\u8865\u5145\u5f53\u524d\u57fa\u4e8e\u98ce\u9669\u7684AI\u76d1\u7ba1\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d'\u8d1f\u8d23\u4efbAI'\u8303\u5f0f\u4e3b\u8981\u5173\u6ce8\u98ce\u9669\u7f13\u89e3\uff0c\u4f46\u5bf9AI\u793e\u4f1a\u5f71\u54cd\u7684\u7cfb\u7edf\u6027\u8003\u91cf\u4e0d\u8db3\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u81ea\u7531\u6982\u5ff5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u5eb7\u5fb7\u54f2\u5b66\u548c\u5f53\u4ee3\u8be0\u91ca\uff0c\u63d0\u51fa\u81ea\u7531\u7684\u4e24\u4e2a\u7ef4\u5ea6\uff08\u80fd\u529b\u548c\u673a\u4f1a\uff09\uff0c\u5e76\u57fa\u4e8e\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u6784\u5efa\u539f\u578b\u6846\u67b6\u3002", "result": "\u539f\u578b\u6846\u67b6\u4e3a\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bc4\u4f30AI\u793e\u4f1a\u5f71\u54cd\u7684\u5de5\u5177\uff0c\u65e8\u5728\u8865\u5145\u73b0\u6709\u98ce\u9669\u5bfc\u5411\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u9996\u6b21\u5c06\u81ea\u7531\u6982\u5ff5\u64cd\u4f5c\u5316\u4e8eAI\u76d1\u7ba1\u4e2d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.02856", "pdf": "https://arxiv.org/pdf/2508.02856", "abs": "https://arxiv.org/abs/2508.02856", "authors": ["Seyed Bagher Hashemi Natanzi", "Hossein Mohammadi", "Bo Tang", "Vuk Marojevic"], "title": "Secure mmWave Beamforming with Proactive-ISAC Defense Against Beam-Stealing Attacks", "categories": ["eess.SP", "cs.AI", "cs.NI"], "comment": null, "summary": "Millimeter-wave (mmWave) communication systems face increasing susceptibility\nto advanced beam-stealing attacks, posing a significant physical layer security\nthreat. This paper introduces a novel framework employing an advanced Deep\nReinforcement Learning (DRL) agent for proactive and adaptive defense against\nthese sophisticated attacks. A key innovation is leveraging Integrated Sensing\nand Communications (ISAC) capabilities for active, intelligent threat\nassessment. The DRL agent, built on a Proximal Policy Optimization (PPO)\nalgorithm, dynamically controls ISAC probing actions to investigate suspicious\nactivities. We introduce an intensive curriculum learning strategy that\nguarantees the agent experiences successful detection during training to\novercome the complex exploration challenges inherent to such a\nsecurity-critical task. Consequently, the agent learns a robust and adaptive\npolicy that intelligently balances security and communication performance.\nNumerical results demonstrate that our framework achieves a mean attacker\ndetection rate of 92.8% while maintaining an average user SINR of over 13 dB.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u4e3b\u52a8\u9632\u5fa1\u6846\u67b6\uff0c\u7528\u4e8e\u5e94\u5bf9\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6ce2\u675f\u7a83\u53d6\u653b\u51fb\uff0c\u901a\u8fc7\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u80fd\u529b\u5b9e\u73b0\u667a\u80fd\u5a01\u80c1\u8bc4\u4f30\u3002", "motivation": "\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u9762\u4e34\u9ad8\u7ea7\u6ce2\u675f\u7a83\u53d6\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u4e9f\u9700\u4e00\u79cd\u4e3b\u52a8\u3001\u81ea\u9002\u5e94\u7684\u9632\u5fa1\u65b9\u6848\u4ee5\u786e\u4fdd\u7269\u7406\u5c42\u5b89\u5168\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684DRL\u4ee3\u7406\uff0c\u7ed3\u5408ISAC\u80fd\u529b\u52a8\u6001\u63a2\u6d4b\u53ef\u7591\u6d3b\u52a8\uff0c\u5e76\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u786e\u4fdd\u8bad\u7ec3\u6210\u529f\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e8692.8%\u7684\u5e73\u5747\u653b\u51fb\u68c0\u6d4b\u7387\u548c13 dB\u4ee5\u4e0a\u7684\u7528\u6237\u4fe1\u5e72\u566a\u6bd4\uff08SINR\uff09\u3002", "conclusion": "\u8be5DRL\u6846\u67b6\u80fd\u6709\u6548\u5e73\u8861\u5b89\u5168\u6027\u548c\u901a\u4fe1\u6027\u80fd\uff0c\u4e3a\u6beb\u7c73\u6ce2\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2508.03216", "pdf": "https://arxiv.org/pdf/2508.03216", "abs": "https://arxiv.org/abs/2508.03216", "authors": ["Hikari Yanagawa", "Yuichi Hiroi", "Satomi Tokida", "Yuji Hatada", "Takefumi Hiraki"], "title": "Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse", "categories": ["cs.HC", "cs.AI"], "comment": "11 pages + supplement 3 pages. To appear in IEEE ISMAR 2025", "summary": "While commercial metaverse platforms offer diverse user-generated content,\nthey lack effective navigation assistance that can dynamically adapt to users'\ninterests and intentions. Although previous research has investigated on-demand\nagents in controlled environments, implementation in commercial settings with\ndiverse world configurations and platform constraints remains challenging.\n  We present Navigation Pixie, an on-demand navigation agent employing a\nloosely coupled architecture that integrates structured spatial metadata with\nLLM-based natural language processing while minimizing platform dependencies,\nwhich enables experiments on the extensive user base of commercial metaverse\nplatforms. Our cross-platform experiments on commercial metaverse platform\nCluster with 99 PC client and 94 VR-HMD participants demonstrated that\nNavigation Pixie significantly increased dwell time and free exploration\ncompared to fixed-route and no-agent conditions across both platforms.\nSubjective evaluations revealed consistent on-demand preferences in PC\nenvironments versus context-dependent social perception advantages in VR-HMD.\nThis research contributes to advancing VR interaction design through\nconversational spatial navigation agents, establishes cross-platform evaluation\nmethodologies revealing environment-dependent effectiveness, and demonstrates\nempirical experimentation frameworks for commercial metaverse platforms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9002\u5e94\u7528\u6237\u5174\u8da3\u548c\u610f\u56fe\u7684\u5bfc\u822a\u52a9\u624bNavigation Pixie\uff0c\u901a\u8fc7\u5728\u5546\u4e1a\u5143\u5b87\u5b99\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5546\u4e1a\u5143\u5b87\u5b99\u5e73\u53f0\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u7528\u6237\u5174\u8da3\u548c\u610f\u56fe\u7684\u5bfc\u822a\u8f85\u52a9\u5de5\u5177\uff0c\u73b0\u6709\u7814\u7a76\u5728\u5546\u4e1a\u5316\u73af\u5883\u4e2d\u7684\u5e94\u7528\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u677e\u6563\u8026\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u7a7a\u95f4\u5143\u6570\u636e\u548c\u57fa\u4e8eLLM\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u51cf\u5c11\u5e73\u53f0\u4f9d\u8d56\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eNavigation Pixie\u663e\u8457\u589e\u52a0\u4e86\u7528\u6237\u5728\u5e73\u53f0\u7684\u505c\u7559\u65f6\u95f4\u548c\u81ea\u7531\u63a2\u7d22\u884c\u4e3a\uff0c\u4e3b\u89c2\u8bc4\u4ef7\u663e\u793aPC\u548cVR-HMD\u73af\u5883\u4e2d\u7528\u6237\u504f\u597d\u4e0d\u540c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86VR\u4ea4\u4e92\u8bbe\u8ba1\u4e2d\u5bf9\u8bdd\u5f0f\u7a7a\u95f4\u5bfc\u822a\u52a9\u624b\u7684\u53d1\u5c55\uff0c\u5e76\u5efa\u7acb\u4e86\u8de8\u5e73\u53f0\u8bc4\u4f30\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2508.03340", "pdf": "https://arxiv.org/pdf/2508.03340", "abs": "https://arxiv.org/abs/2508.03340", "authors": ["Alex Wolf", "Marco Edoardo Palma", "Pooja Rani", "Harald C. Gall"], "title": "Key-Augmented Neural Triggers for Knowledge Sharing", "categories": ["cs.SE"], "comment": null, "summary": "Repository-level code comprehension and knowledge sharing remain core\nchallenges in software engineering. Large language models (LLMs) have shown\npromise by generating explanations of program structure and logic. However,\nthese approaches still face limitations: First, relevant knowledge is\ndistributed across multiple files within a repository, aka semantic\nfragmentation. Second, retrieval inefficiency and attention saturation degrade\nperformance in RAG pipelines, where long, unaligned contexts overwhelm\nattention. Third, repository specific training data is scarce and often\noutdated. Finally, proprietary LLMs hinder industrial adoption due to privacy\nand deployment constraints. To address these issues, we propose Key-Augmented\nNeural Triggers (KANT), a novel approach that embeds knowledge anchors into\nboth training and inference. Unlike prior methods, KANT enables internal access\nto repository specific knowledge, reducing fragmentation and grounding\ninference in localized context. Moreover, we synthesize specialized data\ndirectly from code. At inference, knowledge anchors replace verbose context,\nreducing token overhead and latency while supporting efficient, on premise\ndeployment. We evaluate KANT via: a qualitative human evaluation of the\nsynthesized dataset's intent coverage and quality across five dimensions;\ncompare against SOTA baselines across five qualitative dimensions and inference\nspeed; and replication across different LLMs to assess generalizability.\nResults show that the synthetic training data aligned with information-seeking\nneeds. KANT achieved over 60% preference from human annotators and a LocalStack\nexpert (preferring 79% of cases). Also, KANT reduced inference latency by up to\n85% across all models. Overall, it is well-suited for scalable, low-latency,\non-premise deployments, providing a strong foundation for code comprehension.", "AI": {"tldr": "KANT\u901a\u8fc7\u5d4c\u5165\u77e5\u8bc6\u951a\u70b9\u89e3\u51b3\u4ee3\u7801\u5e93\u77e5\u8bc6\u788e\u7247\u5316\u548cRAG\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u9002\u5408\u672c\u5730\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7801\u5e93\u77e5\u8bc6\u788e\u7247\u5316\u3001RAG\u6548\u7387\u4f4e\u4e0b\u3001\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u53ca\u9690\u79c1\u95ee\u9898\u3002", "method": "\u63d0\u51faKANT\u65b9\u6cd5\uff0c\u5d4c\u5165\u77e5\u8bc6\u951a\u70b9\u5e76\u5408\u6210\u4e13\u7528\u6570\u636e\uff0c\u4f18\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "result": "\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u6570\u636e\u8d28\u91cf\u9ad8\uff0cKANT\u504f\u597d\u738760%\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e85%\u3002", "conclusion": "KANT\u4e3a\u4ee3\u7801\u7406\u89e3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u672c\u5730\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2508.03678", "pdf": "https://arxiv.org/pdf/2508.03678", "abs": "https://arxiv.org/abs/2508.03678", "authors": ["Yangtian Zi", "Harshitha Menon", "Arjun Guha"], "title": "More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation", "categories": ["cs.CL", "cs.LG", "cs.PL"], "comment": null, "summary": "State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general\nbenchmarks like HumanEval but underperform on specialized suites such as\nParEval. Is this due to LLMs missing domain knowledge or insufficient prompt\ndetail is given? To answer this, we introduce PartialOrderEval, which augments\nany code generation benchmark with a partial order of prompts from minimal to\nmaximally detailed. Applying it to HumanEval and both serial and OpenMP subsets\nof ParEval, we measure how pass@1 scales with prompt specificity. Our\nexperiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of\nprompt sensitivity across different tasks, and a qualitative analysis\nhighlights explicit I/O specifications, edge-case handling, and stepwise\nbreakdowns as the key drivers of prompt detail improvement.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728\u901a\u7528\u4e0e\u4e13\u7528\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u63d0\u51faPartialOrderEval\u65b9\u6cd5\u5206\u6790\u63d0\u793a\u7ec6\u8282\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u4e13\u7528\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u539f\u56e0\u662f\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u8fd8\u662f\u63d0\u793a\u7ec6\u8282\u4e0d\u8db3\u3002", "method": "\u5f15\u5165PartialOrderEval\u5de5\u5177\uff0c\u5728HumanEval\u548cParEval\u57fa\u51c6\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u8be6\u7ec6\u7a0b\u5ea6\u7684\u63d0\u793a\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u4e0d\u540c\u4efb\u52a1\u5bf9\u63d0\u793a\u7ec6\u8282\u7684\u654f\u611f\u6027\u4e0d\u540c\uff0c\u5173\u952e\u6539\u8fdb\u56e0\u7d20\u5305\u62ec\u663e\u5f0fI/O\u89c4\u8303\u3001\u8fb9\u754c\u6848\u4f8b\u5904\u7406\u548c\u5206\u6b65\u5206\u89e3\u3002", "conclusion": "\u63d0\u793a\u7ec6\u8282\u7684\u8be6\u7ec6\u7a0b\u5ea6\u5bf9LLMs\u5728\u4e13\u7528\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u3002"}}
{"id": "2508.03579", "pdf": "https://arxiv.org/pdf/2508.03579", "abs": "https://arxiv.org/abs/2508.03579", "authors": ["Weiyao Zhang", "Jinyang Li", "Qi Song", "Miao Wang", "Chungang Lin", "Haitong Luo", "Xuying Meng", "Yujun Zhang"], "title": "Heterogeneity-Oblivious Robust Federated Learning", "categories": ["cs.LG", "cs.NI"], "comment": "Under review", "summary": "Federated Learning (FL) remains highly vulnerable to poisoning attacks,\nespecially under real-world hyper-heterogeneity, where clients differ\nsignificantly in data distributions, communication capabilities, and model\narchitectures. Such heterogeneity not only undermines the effectiveness of\naggregation strategies but also makes attacks more difficult to detect.\nFurthermore, high-dimensional models expand the attack surface. To address\nthese challenges, we propose Horus, a heterogeneity-oblivious robust FL\nframework centered on low-rank adaptations (LoRAs). Rather than aggregating\nfull model parameters, Horus inserts LoRAs into empirically stable layers and\naggregates only LoRAs to reduce the attack surface.We uncover a key empirical\nobservation that the input projection (LoRA-A) is markedly more stable than the\noutput projection (LoRA-B) under heterogeneity and poisoning. Leveraging this,\nwe design a Heterogeneity-Oblivious Poisoning Score using the features from\nLoRA-A to filter poisoned clients. For the remaining benign clients, we propose\nprojection-aware aggregation mechanism to preserve collaborative signals while\nsuppressing drifts, which reweights client updates by consistency with the\nglobal directions. Extensive experiments across diverse datasets, model\narchitectures, and attacks demonstrate that Horus consistently outperforms\nstate-of-the-art baselines in both robustness and accuracy.", "AI": {"tldr": "Horus \u662f\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94\uff08LoRAs\uff09\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5728\u8d85\u5f02\u6784\u73af\u5883\u4e2d\u7684\u4e2d\u6bd2\u653b\u51fb\u95ee\u9898\uff0c\u901a\u8fc7\u805a\u5408 LoRAs \u51cf\u5c11\u653b\u51fb\u9762\uff0c\u5e76\u5229\u7528\u7a33\u5b9a\u6027\u7279\u5f81\u8fc7\u6ee4\u6076\u610f\u5ba2\u6237\u7aef\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u8d85\u5f02\u6784\u73af\u5883\u4e2d\u6781\u6613\u53d7\u5230\u4e2d\u6bd2\u653b\u51fb\uff0c\u4e14\u9ad8\u7ef4\u5ea6\u6a21\u578b\u6269\u5927\u4e86\u653b\u51fb\u9762\u3002\u4f20\u7edf\u805a\u5408\u7b56\u7565\u6548\u679c\u4e0d\u4f73\uff0c\u653b\u51fb\u96be\u4ee5\u68c0\u6d4b\u3002", "method": "Horus \u5728\u7a33\u5b9a\u5c42\u4e2d\u63d2\u5165 LoRAs\uff0c\u4ec5\u805a\u5408 LoRAs \u4ee5\u51cf\u5c11\u653b\u51fb\u9762\u3002\u5229\u7528\u8f93\u5165\u6295\u5f71\uff08LoRA-A\uff09\u7684\u7a33\u5b9a\u6027\u7279\u5f81\u8bbe\u8ba1\u4e2d\u6bd2\u5206\u6570\u8fc7\u6ee4\u6076\u610f\u5ba2\u6237\u7aef\uff0c\u5e76\u5bf9\u826f\u6027\u5ba2\u6237\u7aef\u91c7\u7528\u6295\u5f71\u611f\u77e5\u805a\u5408\u673a\u5236\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u3001\u6a21\u578b\u67b6\u6784\u548c\u653b\u51fb\u573a\u666f\u4e2d\uff0cHorus \u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "Horus \u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u548c\u7a33\u5b9a\u6027\u7279\u5f81\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u5f02\u6784\u73af\u5883\u4e0b\u8054\u90a6\u5b66\u4e60\u7684\u4e2d\u6bd2\u653b\u51fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.03281", "pdf": "https://arxiv.org/pdf/2508.03281", "abs": "https://arxiv.org/abs/2508.03281", "authors": ["Pavlo Bazilinskyy", "Francesco Walker", "Debargha Dey", "Tram Thi Minh Tran", "Hyungchai Park", "Hyochang Kim", "Hyunmin Kang", "Patrick Ebel"], "title": "Quo-Vadis Multi-Agent Automotive Research? Insights from a Participatory Workshop and Questionnaire", "categories": ["cs.HC"], "comment": null, "summary": "The transition to mixed-traffic environments that involve automated vehicles,\nmanually operated vehicles, and vulnerable road users presents new challenges\nfor human-centered automotive research. Despite this, most studies in the\ndomain focus on single-agent interactions. This paper reports on a\nparticipatory workshop (N = 15) and a questionnaire (N = 19) conducted during\nthe AutomotiveUI '24 conference to explore the state of multi-agent automotive\nresearch. The participants discussed methodological challenges and\nopportunities in real-world settings, simulations, and computational modeling.\nKey findings reveal that while the value of multi-agent approaches is widely\nrecognized, practical and technical barriers hinder their implementation. The\nstudy highlights the need for interdisciplinary methods, better tools, and\nsimulation environments that support scalable, realistic, and ethically\ninformed multi-agent research.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6df7\u5408\u4ea4\u901a\u73af\u5883\u4e2d\u591a\u4e3b\u4f53\u7814\u7a76\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u5f3a\u8c03\u9700\u8de8\u5b66\u79d1\u65b9\u6cd5\u548c\u66f4\u597d\u7684\u5de5\u5177\u652f\u6301\u3002", "motivation": "\u7814\u7a76\u6df7\u5408\u4ea4\u901a\u73af\u5883\uff08\u5305\u542b\u81ea\u52a8\u9a7e\u9a76\u3001\u4eba\u5de5\u9a7e\u9a76\u8f66\u8f86\u548c\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff09\u4e2d\u591a\u4e3b\u4f53\u7814\u7a76\u7684\u73b0\u72b6\u53ca\u5176\u6311\u6218\u3002", "method": "\u901a\u8fc7\u53c2\u4e0e\u5f0f\u7814\u8ba8\u4f1a\uff08N=15\uff09\u548c\u95ee\u5377\u8c03\u67e5\uff08N=19\uff09\u63a2\u7d22\u591a\u4e3b\u4f53\u7814\u7a76\u65b9\u6cd5\u8bba\u3002", "result": "\u591a\u4e3b\u4f53\u65b9\u6cd5\u4ef7\u503c\u88ab\u5e7f\u6cdb\u8ba4\u53ef\uff0c\u4f46\u5b9e\u9645\u64cd\u4f5c\u548c\u6280\u672f\u969c\u788d\u963b\u788d\u5176\u5e94\u7528\u3002", "conclusion": "\u9700\u8981\u8de8\u5b66\u79d1\u65b9\u6cd5\u3001\u66f4\u597d\u7684\u5de5\u5177\u548c\u4eff\u771f\u73af\u5883\u4ee5\u652f\u6301\u53ef\u6269\u5c55\u4e14\u7b26\u5408\u4f26\u7406\u7684\u591a\u4e3b\u4f53\u7814\u7a76\u3002"}}
{"id": "2508.03369", "pdf": "https://arxiv.org/pdf/2508.03369", "abs": "https://arxiv.org/abs/2508.03369", "authors": ["Beatriz Santana", "Lidiv\u00e2nio Monte", "Bianca Santana de Ara\u00fajo Silva", "Glauco Carneiro", "S\u00e1vio Freire", "Jos\u00e9 Amancio Macedo Santos", "Manoel Mendon\u00e7a"], "title": "Psychological safety in software workplaces: A systematic literature review", "categories": ["cs.SE"], "comment": null, "summary": "Context: Psychological safety (PS) is an important factor influencing team\nwell-being and performance, particularly in collaborative and dynamic domains\nsuch as software development. Despite its acknowledged significance, research\non PS within the field of software engineering remains limited. The\nsocio-technical complexities and fast-paced nature of software development\npresent challenges to cultivating PS. To the best of our knowledge, no\nsystematic secondary study has synthesized existing knowledge on PS in the\ncontext of software engineering.\n  Objective: This study aims to systematically review and synthesize the\nexisting body of knowledge on PS in software engineering. Specifically, it\nseeks to identify the potential antecedents and consequences associated with\nthe presence or absence of PS among individuals involved in the software\ndevelopment process.\n  Methods: A systematic literature review was conducted, encompassing studies\nretrieved from four digital libraries. The extracted data were subjected to\nboth quantitative and qualitative analyses.\n  Results: The findings indicate a growing academic interest in PS within\nsoftware engineering, with the majority of studies grounded in Edmondson's\nframework. Factors antecedents of PS were identified at the individual, team,\nand organizational levels, including team autonomy, agile methodologies, and\nleadership behaviors.\n  Conclusion: PS fosters innovation, learning, and team performance within\nsoftware development. However, significant gaps persist in understanding the\ncontextual factors influencing PS, its underlying mechanisms, and effective\nstrategies for its enhancement. Future research should address these gaps by\ninvestigating the practical applications of PS within diverse organizational\nsettings in the software engineering domain.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5fc3\u7406\u5b89\u5168\uff08PS\uff09\u7684\u524d\u56e0\u540e\u679c\uff0c\u53d1\u73b0PS\u5bf9\u56e2\u961f\u521b\u65b0\u548c\u5b66\u4e60\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u4f46\u7814\u7a76\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u5fc3\u7406\u5b89\u5168\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u7684\u5065\u5eb7\u548c\u8868\u73b0\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u6709\u9650\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u603b\u7ed3\u73b0\u6709\u77e5\u8bc6\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u6790\u6765\u81ea\u56db\u4e2a\u6570\u5b57\u56fe\u4e66\u9986\u7684\u7814\u7a76\u6570\u636e\uff0c\u7ed3\u5408\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u3002", "result": "\u7814\u7a76\u663e\u793aPS\u7684\u524d\u56e0\u5305\u62ec\u56e2\u961f\u81ea\u4e3b\u6743\u3001\u654f\u6377\u65b9\u6cd5\u548c\u9886\u5bfc\u884c\u4e3a\uff0cPS\u80fd\u4fc3\u8fdb\u521b\u65b0\u548c\u56e2\u961f\u8868\u73b0\u3002", "conclusion": "PS\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u5f71\u54cd\u56e0\u7d20\u548c\u63d0\u5347\u7b56\u7565\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u7ec4\u7ec7\u73af\u5883\u4e2d\u3002"}}
{"id": "2508.03293", "pdf": "https://arxiv.org/pdf/2508.03293", "abs": "https://arxiv.org/abs/2508.03293", "authors": ["Duc-An Nguyen", "Clara Colombatto", "Steve Fleming", "Ingmar Posner", "Nick Hawes", "Raunak Bhattacharyya"], "title": "Enhancing Joint Human-AI Inference in Robot Missions: A Confidence-Based Approach", "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "Joint human-AI inference holds immense potential to improve outcomes in\nhuman-supervised robot missions. Current day missions are generally in the\nAI-assisted setting, where the human operator makes the final inference based\non the AI recommendation. However, due to failures in human judgement on when\nto accept or reject the AI recommendation, complementarity is rarely achieved.\nWe investigate joint human-AI inference where the inference made with higher\nconfidence is selected. Through a user study with N=100 participants on a\nrepresentative simulated robot teleoperation task, specifically studying the\ninference of robots' control delays we show that: a) Joint inference accuracy\nis higher and its extent is regulated by the confidence calibration of the AI\nagent, and b) Humans change their inferences based on AI recommendations and\nthe extent and direction of this change is also regulated by the confidence\ncalibration of the AI agent. Interestingly, our results show that pairing\npoorly-calibrated AI-DSS with humans hurts performance instead of helping the\nteam, reiterating the need for AI-based decision support systems with good\nmetacognitive sensitivity. To the best of our knowledge, our study presents the\nfirst application of a maximum-confidence-based heuristic for joint human-AI\ninference within a simulated robot teleoperation task.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u6700\u9ad8\u7f6e\u4fe1\u5ea6\u9009\u62e9\u7684\u8054\u5408\u4eba-AI\u63a8\u7406\uff0c\u53d1\u73b0\u5176\u51c6\u786e\u6027\u53d7AI\u4fe1\u5fc3\u6821\u51c6\u7684\u5f71\u54cd\uff0c\u4e14\u4eba\u7c7b\u4f1a\u6839\u636eAI\u63a8\u8350\u8c03\u6574\u63a8\u7406\u3002\u7ed3\u679c\u5f3a\u8c03\u826f\u597d\u6821\u51c6\u7684AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524dAI\u8f85\u52a9\u4efb\u52a1\u4e2d\uff0c\u4eba\u7c7b\u64cd\u4f5c\u5458\u5e38\u56e0\u5224\u65ad\u5931\u8bef\u672a\u80fd\u5b9e\u73b0\u4e92\u8865\u6027\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8054\u5408\u4eba-AI\u63a8\u7406\uff0c\u4ee5\u63d0\u5347\u4efb\u52a1\u6548\u679c\u3002", "method": "\u901a\u8fc7100\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\uff0c\u5728\u6a21\u62df\u673a\u5668\u4eba\u8fdc\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u9009\u62e9\u7684\u8054\u5408\u63a8\u7406\uff0c\u5e76\u5206\u6790AI\u4fe1\u5fc3\u6821\u51c6\u7684\u5f71\u54cd\u3002", "result": "\u8054\u5408\u63a8\u7406\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u4e14\u4eba\u7c7b\u4f1a\u6839\u636eAI\u63a8\u8350\u8c03\u6574\u63a8\u7406\uff1b\u6821\u51c6\u4e0d\u4f73\u7684AI\u7cfb\u7edf\u4f1a\u635f\u5bb3\u56e2\u961f\u8868\u73b0\u3002", "conclusion": "\u5f3a\u8c03AI\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u9700\u5177\u5907\u826f\u597d\u7684\u5143\u8ba4\u77e5\u654f\u611f\u6027\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u4eba-AI\u534f\u4f5c\u3002"}}
{"id": "2508.03393", "pdf": "https://arxiv.org/pdf/2508.03393", "abs": "https://arxiv.org/abs/2508.03393", "authors": ["Muhammad Zohaib", "Muhammad Azeem Akbar", "Sami Hyrynsalmi", "Arif Ali Khan"], "title": "Agentic AI in 6G Software Businesses: A Layered Maturity Model", "categories": ["cs.SE", "cs.AI"], "comment": "6 pages, 3 figures and FIT'25 Conference", "summary": "The emergence of agentic AI systems in 6G software businesses presents both\nstrategic opportunities and significant challenges. While such systems promise\nincreased autonomy, scalability, and intelligent decision-making across\ndistributed environments, their adoption raises concerns regarding technical\nimmaturity, integration complexity, organizational readiness, and\nperformance-cost trade-offs. In this study, we conducted a preliminary thematic\nmapping to identify factors influencing the adoption of agentic software within\nthe context of 6G. Drawing on a multivocal literature review and targeted\nscanning, we identified 29 motivators and 27 demotivators, which were further\ncategorized into five high-level themes in each group. This thematic mapping\noffers a structured overview of the enabling and inhibiting forces shaping\norganizational readiness for agentic transformation. Positioned as a\nfeasibility assessment, the study represents an early phase of a broader\nresearch initiative aimed at developing and validating a layered maturity model\ngrounded in CMMI model with the software architectural three dimensions\npossibly Data, Business Logic, and Presentation. Ultimately, this work seeks to\nprovide a practical framework to help software-driven organizations assess,\nstructure, and advance their agent-first capabilities in alignment with the\ndemands of 6G.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e866G\u8f6f\u4ef6\u4e1a\u52a1\u4e2d\u4ee3\u7406AI\u7cfb\u7edf\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u901a\u8fc7\u4e3b\u9898\u6620\u5c04\u8bc6\u522b\u4e8629\u4e2a\u4fc3\u8fdb\u56e0\u7d20\u548c27\u4e2a\u6291\u5236\u56e0\u7d20\uff0c\u5e76\u5f52\u7c7b\u4e3a\u4e94\u5927\u4e3b\u9898\u3002", "motivation": "\u4ee3\u7406AI\u7cfb\u7edf\u57286G\u73af\u5883\u4e2d\u5177\u6709\u81ea\u4e3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u667a\u80fd\u51b3\u7b56\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u91c7\u7528\u9762\u4e34\u6280\u672f\u3001\u96c6\u6210\u548c\u7ec4\u7ec7\u7b49\u591a\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u591a\u58f0\u90e8\u6587\u732e\u7efc\u8ff0\u548c\u9488\u5bf9\u6027\u626b\u63cf\uff0c\u8fdb\u884c\u521d\u6b65\u4e3b\u9898\u6620\u5c04\uff0c\u8bc6\u522b\u5f71\u54cd\u4ee3\u7406\u8f6f\u4ef6\u91c7\u7528\u7684\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e8629\u4e2a\u4fc3\u8fdb\u56e0\u7d20\u548c27\u4e2a\u6291\u5236\u56e0\u7d20\uff0c\u5e76\u5f52\u7c7b\u4e3a\u4e94\u4e2a\u9ad8\u7ea7\u4e3b\u9898\uff0c\u4e3a\u7ec4\u7ec7\u8bc4\u4f30\u4ee3\u7406\u8f6c\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89c6\u89d2\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u8f6f\u4ef6\u5f00\u53d1\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u4ee5\u8bc4\u4f30\u548c\u63d0\u5347\u5176\u4ee3\u7406\u4f18\u5148\u80fd\u529b\uff0c\u9002\u5e946G\u9700\u6c42\u3002"}}
{"id": "2508.03681", "pdf": "https://arxiv.org/pdf/2508.03681", "abs": "https://arxiv.org/abs/2508.03681", "authors": ["Shreya Meel", "Mohamed Nomeir", "Pasan Dissanayake", "Sanghamitra Dutta", "Sennur Ulukus"], "title": "What If, But Privately: Private Counterfactual Retrieval", "categories": ["cs.IT", "cs.CR", "cs.LG", "cs.NI", "eess.SP", "math.IT"], "comment": "arXiv admin note: text overlap with arXiv:2410.13812,\n  arXiv:2411.10429", "summary": "Transparency and explainability are two important aspects to be considered\nwhen employing black-box machine learning models in high-stake applications.\nProviding counterfactual explanations is one way of catering this requirement.\nHowever, this also poses a threat to the privacy of the institution that is\nproviding the explanation, as well as the user who is requesting it. In this\nwork, we are primarily concerned with the user's privacy who wants to retrieve\na counterfactual instance, without revealing their feature vector to the\ninstitution. Our framework retrieves the exact nearest neighbor counterfactual\nexplanation from a database of accepted points while achieving perfect,\ninformation-theoretic, privacy for the user. First, we introduce the problem of\nprivate counterfactual retrieval (PCR) and propose a baseline PCR scheme that\nkeeps the user's feature vector information-theoretically private from the\ninstitution. Building on this, we propose two other schemes that reduce the\namount of information leaked about the institution database to the user,\ncompared to the baseline scheme. Second, we relax the assumption of mutability\nof all features, and consider the setting of immutable PCR (I-PCR). Here, the\nuser retrieves the nearest counterfactual without altering a private subset of\ntheir features, which constitutes the immutable set, while keeping their\nfeature vector and immutable set private from the institution. For this, we\npropose two schemes that preserve the user's privacy information-theoretically,\nbut ensure varying degrees of database privacy. Third, we extend our PCR and\nI-PCR schemes to incorporate user's preference on transforming their\nattributes, so that a more actionable explanation can be received. Finally, we\npresent numerical results to support our theoretical findings, and compare the\ndatabase leakage of the proposed schemes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u68c0\u7d22\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u9ed1\u7bb1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u9ed1\u7bb1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u8981\u63d0\u4f9b\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5a01\u80c1\u7528\u6237\u548c\u673a\u6784\u7684\u9690\u79c1\u3002\u8bba\u6587\u65e8\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u540c\u65f6\u63d0\u4f9b\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "method": "\u8bba\u6587\u9996\u5148\u63d0\u51fa\u4e86\u79c1\u6709\u53cd\u4e8b\u5b9e\u68c0\u7d22\uff08PCR\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u7ebfPCR\u65b9\u6848\u4ee5\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002\u968f\u540e\u63d0\u51fa\u4e24\u79cd\u6539\u8fdb\u65b9\u6848\u4ee5\u51cf\u5c11\u6570\u636e\u5e93\u4fe1\u606f\u6cc4\u9732\u3002\u63a5\u7740\u8ba8\u8bba\u4e86\u4e0d\u53ef\u53d8PCR\uff08I-PCR\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002\u6700\u540e\u6269\u5c55\u4e86\u65b9\u6848\u4ee5\u652f\u6301\u7528\u6237\u504f\u597d\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u6848\u80fd\u591f\u4fe1\u606f\u8bba\u5b8c\u7f8e\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u540c\u65f6\u51cf\u5c11\u6570\u636e\u5e93\u4fe1\u606f\u6cc4\u9732\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\u68c0\u7d22\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.03355", "pdf": "https://arxiv.org/pdf/2508.03355", "abs": "https://arxiv.org/abs/2508.03355", "authors": ["Zhuoqun Jiang", "ShunYi Yeo", "Wei Xuan Donovan Seow", "Simon Perrault"], "title": "Remini: Leveraging Chatbot-Mediated Mutual Reminiscence for Promoting Positive Affect and Feeling of Connectedness among Loved Ones", "categories": ["cs.HC"], "comment": "Camera-ready submission for PACM HCI, CSCW 2025", "summary": "Mutual reminiscence, defined as revisiting shared positive memories through\nreciprocal self-disclosure, strengthens emotional bonds, enhances well-being,\nand deepens intimacy. However, most technology-mediated reminiscence tools\nemphasize individual reflection or one-way storytelling, which overlooks the\ndynamic, interactive dialogue essential for meaningful mutual reminiscence. To\naddress this limitation, we introduce Remini, a chatbot designed to support\nreciprocal self-disclosure between close partners such as couples, friends, or\nfamily members. Grounded in the Social Functions of Autobiographical Memory\n(SFAM) framework, Remini uses conversational AI to guide emotionally rich\nexchanges through five narrative phases: rapport building, memory narration,\nelaboration, reflection, and summary. In a mixed-method, both between- and\nwithin- subjects study (N = 48, 24 dyads), we compare Remini to a baseline\nchatbot that offers minimal memory-trigger prompts. Our findings show that\nstructured guidance from Remini significantly improves positive affect, feeling\nof connection, and engagement. It also fosters more detailed narrative\nco-construction and greater reciprocal self-disclosure. Participant feedback\nhighlights the practical value, perceived benefits, and design considerations\nof chatbot-mediated reminiscence. We contribute empirically grounded design\nimplications for conversational agents that strengthen human connection through\nmutual reminiscence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRemini\u804a\u5929\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u652f\u6301\u4f34\u4fa3\u95f4\u7684\u4e92\u60e0\u81ea\u6211\u62ab\u9732\uff0c\u589e\u5f3a\u60c5\u611f\u8054\u7cfb\u548c\u5e78\u798f\u611f\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5de5\u5177\u591a\u5173\u6ce8\u4e2a\u4f53\u53cd\u601d\u6216\u5355\u5411\u53d9\u4e8b\uff0c\u5ffd\u7565\u4e86\u4e92\u52a8\u5bf9\u8bdd\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u8bbe\u8ba1\u4e86Remini\u3002", "method": "\u57fa\u4e8eSFAM\u6846\u67b6\uff0cRemini\u901a\u8fc7\u4e94\u4e2a\u53d9\u4e8b\u9636\u6bb5\u5f15\u5bfc\u5bf9\u8bdd\uff0c\u5e76\u572848\u4eba\u7684\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u4e2d\u4e0e\u57fa\u7ebf\u804a\u5929\u673a\u5668\u4eba\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u53d1\u73b0Remini\u663e\u8457\u63d0\u5347\u79ef\u6781\u60c5\u7eea\u3001\u8fde\u63a5\u611f\u548c\u53c2\u4e0e\u5ea6\uff0c\u5e76\u4fc3\u8fdb\u66f4\u8be6\u7ec6\u7684\u53d9\u4e8b\u5171\u5efa\u3002", "conclusion": "\u7814\u7a76\u4e3a\u901a\u8fc7\u4e92\u60e0\u56de\u5fc6\u589e\u5f3a\u4eba\u9645\u8054\u7cfb\u7684\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8bbe\u8ba1\u4f9d\u636e\u3002"}}
{"id": "2508.03430", "pdf": "https://arxiv.org/pdf/2508.03430", "abs": "https://arxiv.org/abs/2508.03430", "authors": ["Iyad Rahwan", "Azim Shariff", "Jean-Fran\u00e7ois Bonnefon"], "title": "The Science Fiction Science Method", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Predicting the social and behavioral impact of future technologies, before\nthey are achieved, would allow us to guide their development and regulation\nbefore these impacts get entrenched. Traditionally, this prediction has relied\non qualitative, narrative methods. Here we describe a method which uses\nexperimental methods to simulate future technologies, and collect quantitative\nmeasures of the attitudes and behaviors of participants assigned to controlled\nvariations of the future. We call this method 'science fiction science'. We\nsuggest that the reason why this method has not been fully embraced yet,\ndespite its potential benefits, is that experimental scientists may be\nreluctant to engage in work facing such serious validity threats as science\nfiction science. To address these threats, we consider possible constraints on\nthe kind of technology that science fiction science may study, as well as the\nunconventional, immersive methods that science fiction science may require. We\nseek to provide perspective on the reasons why this method has been\nmarginalized for so long, what benefits it would bring if it could be built on\nstrong yet unusual methods, and how we can normalize these methods to help the\ndiverse community of science fiction scientists to engage in a virtuous cycle\nof validity improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u2018\u79d1\u5e7b\u79d1\u5b66\u2019\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6a21\u62df\u672a\u6765\u6280\u672f\u5e76\u91cf\u5316\u8bc4\u4f30\u53c2\u4e0e\u8005\u6001\u5ea6\u548c\u884c\u4e3a\uff0c\u4ee5\u9884\u6d4b\u6280\u672f\u7684\u793e\u4f1a\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u5b9a\u6027\u65b9\u6cd5\u96be\u4ee5\u9884\u6d4b\u672a\u6765\u6280\u672f\u7684\u793e\u4f1a\u884c\u4e3a\u5f71\u54cd\uff0c\u9700\u8981\u5b9a\u91cf\u5b9e\u9a8c\u65b9\u6cd5\u8f85\u52a9\u53d1\u5c55", "method": "\u4f7f\u7528\u5b9e\u9a8c\u65b9\u6cd5\u6a21\u62df\u672a\u6765\u6280\u672f\uff0c\u63a7\u5236\u53d8\u91cf\u5e76\u91cf\u5316\u53c2\u4e0e\u8005\u6001\u5ea6\u548c\u884c\u4e3a", "result": "\u79d1\u5e7b\u79d1\u5b66\u65b9\u6cd5\u672a\u88ab\u5e7f\u6cdb\u63a5\u53d7\uff0c\u4f46\u5176\u6f5c\u529b\u53ef\u901a\u8fc7\u6539\u8fdb\u65b9\u6cd5\u5b9e\u73b0", "conclusion": "\u901a\u8fc7\u89c4\u8303\u548c\u6539\u8fdb\u65b9\u6cd5\uff0c\u79d1\u5e7b\u79d1\u5b66\u53ef\u6210\u4e3a\u9884\u6d4b\u672a\u6765\u6280\u672f\u5f71\u54cd\u7684\u6709\u6548\u5de5\u5177"}}
{"id": "2508.03470", "pdf": "https://arxiv.org/pdf/2508.03470", "abs": "https://arxiv.org/abs/2508.03470", "authors": ["Dong wang", "Junji Yu", "Honglin Shu", "Michael Fu", "Chakkrit Tantithamthavorn", "Yasutaka Kamei", "Junjie Chen"], "title": "On the Evaluation of Large Language Models in Multilingual Vulnerability Repair", "categories": ["cs.SE"], "comment": null, "summary": "Various Deep Learning-based approaches with pre-trained language models have\nbeen proposed for automatically repairing software vulnerabilities. However,\nthese approaches are limited to a specific programming language (C/C++). Recent\nadvances in large language models (LLMs) offer language-agnostic capabilities\nand strong semantic understanding, exhibiting potential to overcome\nmultilingual vulnerability limitations. Although some work has begun to explore\nLLMs' repair performance, their effectiveness is unsatisfactory. To address\nthese limitations, we conducted a large-scale empirical study to investigate\nthe performance of automated vulnerability repair approaches and\nstate-of-the-art LLMs across seven programming languages. Results show GPT-4o,\ninstruction-tuned with few-shot prompting, performs competitively against the\nleading approach, VulMaster. Additionally, the LLM-based approach shows\nsuperior performance in repairing unique vulnerabilities and is more likely to\nrepair the most dangerous vulnerabilities. Instruction-tuned GPT-4o\ndemonstrates strong generalization on vulnerabilities in previously unseen\nlanguage, outperforming existing approaches. Analysis shows Go consistently\nachieves the highest effectiveness across all model types, while C/C++ performs\nthe worst. Based on findings, we discuss the promise of LLM on multilingual\nvulnerability repair and the reasons behind LLM's failed cases. This work takes\nthe first look at repair approaches and LLMs across multiple languages,\nhighlighting the promising future of adopting LLMs for multilingual\nvulnerability repair.", "AI": {"tldr": "GPT-4o\u5728\u6307\u4ee4\u8c03\u4f18\u548c\u5c11\u6837\u672c\u63d0\u793a\u4e0b\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u591a\u8bed\u8a00\u6f0f\u6d1e\u4fee\u590d\u4e2d\u4f18\u8d8a\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8bed\u8a00\u6f0f\u6d1e\u4fee\u590d\u4e2d\u7684\u6f5c\u529b\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8eC/C++\u7684\u95ee\u9898\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u81ea\u52a8\u6f0f\u6d1e\u4fee\u590d\u65b9\u6cd5\u548cLLMs\u5728\u4e03\u79cd\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u6307\u4ee4\u8c03\u4f18\u7684GPT-4o\u3002", "result": "GPT-4o\u5728\u4fee\u590d\u72ec\u7279\u6f0f\u6d1e\u548c\u6700\u5371\u9669\u6f0f\u6d1e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5728\u65b0\u8bed\u8a00\u7684\u6f0f\u6d1e\u4fee\u590d\u4e2d\u6cdb\u5316\u80fd\u529b\u5f3a\u3002Go\u8bed\u8a00\u4fee\u590d\u6548\u679c\u6700\u4f73\uff0cC/C++\u6700\u5dee\u3002", "conclusion": "LLMs\u5728\u591a\u8bed\u8a00\u6f0f\u6d1e\u4fee\u590d\u4e2d\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5931\u8d25\u6848\u4f8b\u95ee\u9898\u3002"}}
{"id": "2508.03547", "pdf": "https://arxiv.org/pdf/2508.03547", "abs": "https://arxiv.org/abs/2508.03547", "authors": ["Ada Yi Zhao", "Aditya Gunturu", "Ellen Yi-Luen Do", "Ryo Suzuki"], "title": "Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs and Vision Models", "categories": ["cs.HC"], "comment": "To appear at UIST 2025", "summary": "Large language models (LLMs) have enabled the automatic generation of\nstep-by-step augmented reality (AR) instructions for a wide range of physical\ntasks. However, existing LLM-based AR guidance often lacks rich visual\naugmentations to effectively embed instructions into spatial context for a\nbetter user understanding. We present Guided Reality, a fully automated AR\nsystem that generates embedded and dynamic visual guidance based on\nstep-by-step instructions. Our system integrates LLMs and vision models to: 1)\ngenerate multi-step instructions from user queries, 2) identify appropriate\ntypes of visual guidance, 3) extract spatial information about key interaction\npoints in the real world, and 4) embed visual guidance in physical space to\nsupport task execution. Drawing from a corpus of user manuals, we define five\ncategories of visual guidance and propose an identification strategy based on\nthe current step. We evaluate the system through a user study (N=16),\ncompleting real-world tasks and exploring the system in the wild. Additionally,\nfour instructors shared insights on how Guided Reality could be integrated into\ntheir training workflows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGuided Reality\u7684\u81ea\u52a8\u5316AR\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86LLMs\u548c\u89c6\u89c9\u6a21\u578b\uff0c\u751f\u6210\u52a8\u6001\u89c6\u89c9\u6307\u5bfc\u4ee5\u63d0\u5347AR\u6307\u4ee4\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684LLM\u751f\u6210\u7684AR\u6307\u4ee4\u7f3a\u4e4f\u4e30\u5bcc\u7684\u89c6\u89c9\u589e\u5f3a\uff0c\u9650\u5236\u4e86\u7528\u6237\u5bf9\u7269\u7406\u4efb\u52a1\u7684\u7406\u89e3\u548c\u6267\u884c\u3002", "method": "\u7cfb\u7edf\u6574\u5408LLMs\u548c\u89c6\u89c9\u6a21\u578b\uff0c\u751f\u6210\u591a\u6b65\u6307\u4ee4\u3001\u8bc6\u522b\u89c6\u89c9\u6307\u5bfc\u7c7b\u578b\u3001\u63d0\u53d6\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u5c06\u89c6\u89c9\u6307\u5bfc\u5d4c\u5165\u7269\u7406\u7a7a\u95f4\u3002", "result": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\uff08N=16\uff09\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u57f9\u8bad\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "Guided Reality\u80fd\u6709\u6548\u63d0\u5347AR\u6307\u4ee4\u7684\u89c6\u89c9\u5d4c\u5165\u6548\u679c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03487", "pdf": "https://arxiv.org/pdf/2508.03487", "abs": "https://arxiv.org/abs/2508.03487", "authors": ["Yuanpeng Li", "Qi Long", "Zhiyuan Yao", "Jian Xu", "Lintao Xie", "Xu He", "Lu Geng", "Xin Han", "Yueyan Chen", "Wenbo Duan"], "title": "BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "As enterprise codebases continue to grow in scale and complexity, the volume\nof lint errors far exceeds engineers' manual remediation capacity, leading to\ncontinuous accumulation of technical debt and hindered development efficiency.\nThis paper presents BitsAI-Fix, an automated lint error remediation workflow\nbased on Large Language Models (LLMs), designed to address this critical\nchallenge in industrial-scale environments. BitsAI-Fix employs tree-sitter for\ncontext expansion and generates search-and-replace format patches through\nspecially trained LLMs, followed by lint scan re-verification to output final\nremediation results. Additionally, our approach introduces an innovative\nprogressive reinforcement learning (RL) training strategy that can\nautomatically acquire verifiable training data during the project cold-start\nphase and continuously iterate the model by collecting online samples through\nfeedback after system deployment. Furthermore, we designed a targeted\nrule-based reward mechanism that combines format rewards and correctness\nrewards while penalizing redundant modifications. We also propose a \"code diff\nmatching\" methodology to continuously track online effectiveness. In production\ndeployment at ByteDance, our solution has supported over 5,000 engineers,\nresolved more than 12,000 static analysis issues, achieved approximately 85%\nremediation accuracy, with around 1,000 weekly active adopters. This work\ndemonstrates the practical feasibility of LLM-based code remediation solutions\nin enterprise environments and serves as a reference for automated code fix in\nlarge-scale industrial scenarios.", "AI": {"tldr": "BitsAI-Fix\u5229\u7528LLMs\u81ea\u52a8\u4fee\u590d\u4ee3\u7801\u4e2d\u7684lint\u9519\u8bef\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u6269\u5c55\u3001\u641c\u7d22\u66ff\u6362\u8865\u4e01\u751f\u6210\u53ca\u91cd\u65b0\u9a8c\u8bc1\uff0c\u7ed3\u5408\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u548c\u9488\u5bf9\u6027\u5956\u52b1\u673a\u5236\uff0c\u5728\u5b57\u8282\u8df3\u52a8\u751f\u4ea7\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u4f01\u4e1a\u4ee3\u7801\u5e93\u89c4\u6a21\u6269\u5927\u5bfc\u81f4lint\u9519\u8bef\u8fdc\u8d85\u4eba\u5de5\u4fee\u590d\u80fd\u529b\uff0c\u6280\u672f\u503a\u52a1\u79ef\u7d2f\u963b\u788d\u5f00\u53d1\u6548\u7387\u3002", "method": "\u91c7\u7528tree-sitter\u6269\u5c55\u4e0a\u4e0b\u6587\uff0c\u8bad\u7ec3LLMs\u751f\u6210\u8865\u4e01\uff0c\u7ed3\u5408\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u548c\u89c4\u5219\u5956\u52b1\u673a\u5236\u3002", "result": "\u5728\u5b57\u8282\u8df3\u52a8\u652f\u63015000+\u5de5\u7a0b\u5e08\uff0c\u4fee\u590d12000+\u9759\u6001\u5206\u6790\u95ee\u9898\uff0c\u51c6\u786e\u7387\u7ea685%\u3002", "conclusion": "\u57fa\u4e8eLLMs\u7684\u4ee3\u7801\u4fee\u590d\u65b9\u6848\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u53ef\u884c\uff0c\u4e3a\u5927\u89c4\u6a21\u5de5\u4e1a\u573a\u666f\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2508.03630", "pdf": "https://arxiv.org/pdf/2508.03630", "abs": "https://arxiv.org/abs/2508.03630", "authors": ["Zhuohao Jerry Zhang", "Ruiqi Chen", "Mingyuan Zhong", "Jacob O. Wobbrock"], "title": "SlideAudit: A Dataset and Taxonomy for Automated Evaluation of Presentation Slides", "categories": ["cs.HC"], "comment": "UIST 2025", "summary": "Automated evaluation of specific graphic designs like presentation slides is\nan open problem. We present SlideAudit, a dataset for automated slide\nevaluation. We collaborated with design experts to develop a thorough taxonomy\nof slide design flaws. Our dataset comprises 2400 slides collected and\nsynthesized from multiple sources, including a subset intentionally modified\nwith specific design problems. We then fully annotated them using our taxonomy\nthrough strictly trained crowdsourcing from Prolific. To evaluate whether AI is\ncapable of identifying design flaws, we compared multiple large language models\nunder different prompting strategies, and with an existing design critique\npipeline. We show that AI models struggle to accurately identify slide design\nflaws, with F1 scores ranging from 0.331 to 0.655. Notably, prompting\ntechniques leveraging our taxonomy achieved the highest performance. We further\nconducted a remediation study to assess AI's potential for improving slides.\nAmong 82.0% of slides that showed significant improvement, 87.8% of them were\nimproved more with our taxonomy, further demonstrating its utility.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86SlideAudit\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u6f14\u793a\u5e7b\u706f\u7247\u8bbe\u8ba1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7AI\u6a21\u578b\u6d4b\u8bd5\u4e86\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\u7684\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u5316\u8bc4\u4f30\u7279\u5b9a\u56fe\u5f62\u8bbe\u8ba1\uff08\u5982\u6f14\u793a\u5e7b\u706f\u7247\uff09\u662f\u4e00\u4e2a\u5c1a\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e13\u5bb6\u5408\u4f5c\u5f00\u53d1\u4e86\u5e7b\u706f\u7247\u8bbe\u8ba1\u7f3a\u9677\u7684\u5206\u7c7b\u6cd5\uff0c\u6536\u96c6\u5e76\u6807\u6ce8\u4e862400\u5f20\u5e7b\u706f\u7247\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u7b56\u7565\u3002", "result": "AI\u6a21\u578b\u5728\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cF1\u5206\u6570\u57280.331\u52300.655\u4e4b\u95f4\uff0c\u4f46\u57fa\u4e8e\u5206\u7c7b\u6cd5\u7684\u63d0\u793a\u7b56\u7565\u6548\u679c\u6700\u4f73\u300287.8%\u7684\u6539\u8fdb\u5e7b\u706f\u7247\u5f97\u76ca\u4e8e\u5206\u7c7b\u6cd5\u3002", "conclusion": "SlideAudit\u6570\u636e\u96c6\u548c\u5206\u7c7b\u6cd5\u4e3a\u81ea\u52a8\u5316\u5e7b\u706f\u7247\u8bbe\u8ba1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5c3d\u7ba1AI\u5f53\u524d\u8868\u73b0\u6709\u9650\uff0c\u4f46\u5206\u7c7b\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6539\u8fdb\u6548\u679c\u3002"}}
{"id": "2508.03560", "pdf": "https://arxiv.org/pdf/2508.03560", "abs": "https://arxiv.org/abs/2508.03560", "authors": ["Yi Gui", "Zhen Li", "Zhongyi Zhang", "Guohao Wang", "Tianpeng Lv", "Gaoyang Jiang", "Yi Liu", "Dongping Chen", "Yao Wan", "Hongyu Zhang", "Wenbin Jiang", "Xuanhua Shi", "Hai Jin"], "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought", "categories": ["cs.SE"], "comment": "KDD 2025 v2", "summary": "Converting webpage designs into code (design-to-code) plays a vital role in\nUser Interface (UI) development for front-end developers, bridging the gap\nbetween visual design and functional implementation. While recent Multimodal\nLarge Language Models (MLLMs) have shown significant potential in\ndesign-to-code tasks, they often fail to accurately preserve the layout during\ncode generation. To this end, we draw inspiration from the Chain-of-Thought\n(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that\nenhances layout preservation in webpage design during code generation with\nLayout-as-Thought (LaT). Specifically, we first introduce a simple yet\nefficient algorithm to divide the webpage design into image blocks. Next, we\nprompt MLLMs using a CoTbased approach to generate code for each block.\nFinally, we apply two assembly strategies-absolute positioning and an\nMLLM-based method-followed by dynamic selection to determine the optimal\noutput. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs\n(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly\nintroduced, more challenging benchmark (CC-HARD) that features complex layouts.\nThe experimental results on automatic metrics demonstrate significant\nimprovements. Specifically, TreeBLEU scores increased by 66.67% and MAE\ndecreased by 38% when using DeepSeek-VL2, compared to direct prompting.\nMoreover, the human preference evaluation results indicate that annotators\nfavor the webpages generated by LaTCoder in over 60% of cases, providing strong\nevidence of the effectiveness of our method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLaTCoder\u65b9\u6cd5\uff0c\u5229\u7528Layout-as-Thought\uff08LaT\uff09\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u9875\u8bbe\u8ba1\u8f6c\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u5e03\u5c40\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u5206\u5757\u751f\u6210\u548c\u52a8\u6001\u9009\u62e9\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u9875\u8bbe\u8ba1\u8f6c\u4ee3\u7801\u4efb\u52a1\u4e2d\u5e03\u5c40\u4fdd\u7559\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "1. \u5206\u5757\u7b97\u6cd5\u5c06\u7f51\u9875\u8bbe\u8ba1\u62c6\u5206\u4e3a\u56fe\u50cf\u5757\uff1b2. CoT-based\u65b9\u6cd5\u9010\u5757\u751f\u6210\u4ee3\u7801\uff1b3. \u7edd\u5bf9\u5b9a\u4f4d\u548cMLLM-based\u4e24\u79cd\u7ec4\u88c5\u7b56\u7565\u52a8\u6001\u9009\u62e9\u6700\u4f18\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cDeepSeek-VL2\u7684TreeBLEU\u63d0\u534766.67%\uff0cMAE\u964d\u4f4e38%\uff1b\u4eba\u5de5\u8bc4\u4f30\u4e2d60%\u4ee5\u4e0a\u7684\u6848\u4f8b\u66f4\u504f\u597dLaTCoder\u751f\u6210\u7684\u7f51\u9875\u3002", "conclusion": "LaTCoder\u663e\u8457\u6539\u8fdb\u4e86\u5e03\u5c40\u4fdd\u7559\u6548\u679c\uff0c\u8bc1\u660e\u4e86LaT\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.03651", "pdf": "https://arxiv.org/pdf/2508.03651", "abs": "https://arxiv.org/abs/2508.03651", "authors": ["Ruei-Che Chang", "Rosiana Natalie", "Wenqian Xu", "Jovan Zheng Feng Yap", "Anhong Guo"], "title": "Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired", "categories": ["cs.HC", "cs.AI"], "comment": "ACM ASSETS 2025", "summary": "Recent advancements in large multimodal models have provided blind or\nvisually impaired (BVI) individuals with new capabilities to interpret and\nengage with the real world through interactive systems that utilize live video\nfeeds. However, the potential benefits and challenges of such capabilities to\nsupport diverse real-world assistive tasks remain unclear. In this paper, we\npresent findings from an exploratory study with eight BVI participants.\nParticipants used ChatGPT's Advanced Voice with Video, a state-of-the-art live\nvideo AI released in late 2024, in various real-world scenarios, from locating\nobjects to recognizing visual landmarks, across unfamiliar indoor and outdoor\nenvironments. Our findings indicate that current live video AI effectively\nprovides guidance and answers for static visual scenes but falls short in\ndelivering essential live descriptions required in dynamic situations. Despite\ninaccuracies in spatial and distance information, participants leveraged the\nprovided visual information to supplement their mobility strategies. Although\nthe system was perceived as human-like due to high-quality voice interactions,\nassumptions about users' visual abilities, hallucinations, generic responses,\nand a tendency towards sycophancy led to confusion, distrust, and potential\nrisks for BVI users. Based on the results, we discuss implications for\nassistive video AI agents, including incorporating additional sensing\ncapabilities for real-world use, determining appropriate intervention timing\nbeyond turn-taking interactions, and addressing ecological and safety concerns.", "AI": {"tldr": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u8fdb\u6b65\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u4e86\u901a\u8fc7\u5b9e\u65f6\u89c6\u9891\u7cfb\u7edf\u4e0e\u73b0\u5b9e\u4e16\u754c\u4e92\u52a8\u7684\u80fd\u529b\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u8f85\u52a9\u4efb\u52a1\u4e2d\u7684\u76ca\u5904\u4e0e\u6311\u6218\u5c1a\u4e0d\u660e\u786e\u3002", "motivation": "\u63a2\u7d22\u5b9e\u65f6\u89c6\u9891AI\u5728\u89c6\u969c\u4eba\u58eb\u8f85\u52a9\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u4e0e\u5c40\u9650\u6027\u3002", "method": "\u5bf9\u516b\u4f4d\u89c6\u969c\u53c2\u4e0e\u8005\u8fdb\u884c\u63a2\u7d22\u6027\u7814\u7a76\uff0c\u4f7f\u7528ChatGPT\u7684Advanced Voice with Video\u5728\u964c\u751f\u73af\u5883\u4e2d\u5b8c\u6210\u591a\u79cd\u4efb\u52a1\u3002", "result": "\u5f53\u524d\u7cfb\u7edf\u80fd\u6709\u6548\u5904\u7406\u9759\u6001\u573a\u666f\uff0c\u4f46\u5728\u52a8\u6001\u60c5\u5883\u4e2d\u63d0\u4f9b\u7684\u5b9e\u65f6\u63cf\u8ff0\u4e0d\u8db3\uff0c\u4e14\u5b58\u5728\u4fe1\u606f\u4e0d\u51c6\u786e\u3001\u5e7b\u89c9\u7b49\u95ee\u9898\u3002", "conclusion": "\u9700\u589e\u5f3a\u5b9e\u65f6\u611f\u77e5\u80fd\u529b\u3001\u4f18\u5316\u5e72\u9884\u65f6\u673a\uff0c\u5e76\u89e3\u51b3\u751f\u6001\u4e0e\u5b89\u5168\u95ee\u9898\u4ee5\u6539\u8fdb\u8f85\u52a9\u89c6\u9891AI\u3002"}}
{"id": "2508.03673", "pdf": "https://arxiv.org/pdf/2508.03673", "abs": "https://arxiv.org/abs/2508.03673", "authors": ["Shengnan Yang", "Rongqian Ma"], "title": "Classifying Epistemic Relationships in Human-AI Interaction: An Exploratory Approach", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "As AI systems become integral to knowledge-intensive work, questions arise\nnot only about their functionality but also their epistemic roles in human-AI\ninteraction. While HCI research has proposed various AI role typologies, it\noften overlooks how AI reshapes users' roles as knowledge contributors. This\nstudy examines how users form epistemic relationships with AI-how they assess,\ntrust, and collaborate with it in research and teaching contexts. Based on 31\ninterviews with academics across disciplines, we developed a five-part codebook\nand identified five relationship types: Instrumental Reliance, Contingent\nDelegation, Co-agency Collaboration, Authority Displacement, and Epistemic\nAbstention. These reflect variations in trust, assessment modes, tasks, and\nhuman epistemic status. Our findings show that epistemic roles are dynamic and\ncontext-dependent. We argue for shifting beyond static metaphors of AI toward a\nmore nuanced framework that captures how humans and AI co-construct knowledge,\nenriching HCI's understanding of the relational and normative dimensions of AI\nuse.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u7c7b\u4e0eAI\u5728\u77e5\u8bc6\u8d21\u732e\u4e2d\u7684\u52a8\u6001\u8ba4\u77e5\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u5173\u7cfb\u7c7b\u578b\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u8d85\u8d8a\u9759\u6001AI\u9690\u55bb\uff0c\u91c7\u7528\u66f4\u7ec6\u81f4\u7684\u6846\u67b6\u6765\u7406\u89e3\u4eba\u673a\u77e5\u8bc6\u5171\u5efa\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u5de5\u4f5c\u4e2d\u7684\u666e\u53ca\uff0c\u7814\u7a76AI\u5982\u4f55\u91cd\u5851\u7528\u6237\u7684\u89d2\u8272\u53ca\u5176\u4e0e\u4eba\u7c7b\u7684\u8ba4\u77e5\u5173\u7cfb\u53d8\u5f97\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u5bf931\u4f4d\u8de8\u5b66\u79d1\u5b66\u8005\u7684\u8bbf\u8c08\uff0c\u5f00\u53d1\u4e86\u4e94\u90e8\u5206\u7f16\u7801\u624b\u518c\uff0c\u8bc6\u522b\u51fa\u4e94\u79cd\u8ba4\u77e5\u5173\u7cfb\u7c7b\u578b\u3002", "result": "\u53d1\u73b0\u8ba4\u77e5\u89d2\u8272\u662f\u52a8\u6001\u4e14\u4f9d\u8d56\u60c5\u5883\u7684\uff0c\u63d0\u51fa\u4e94\u79cd\u5173\u7cfb\u7c7b\u578b\uff1a\u5de5\u5177\u4f9d\u8d56\u3001\u6709\u6761\u4ef6\u59d4\u6258\u3001\u5171\u540c\u4ee3\u7406\u534f\u4f5c\u3001\u6743\u5a01\u66ff\u4ee3\u548c\u8ba4\u77e5\u56de\u907f\u3002", "conclusion": "\u547c\u5401HCI\u9886\u57df\u8d85\u8d8a\u9759\u6001AI\u9690\u55bb\uff0c\u91c7\u7528\u66f4\u7ec6\u81f4\u7684\u6846\u67b6\u4ee5\u6355\u6349\u4eba\u673a\u77e5\u8bc6\u5171\u5efa\u7684\u591a\u6837\u6027\u548c\u89c4\u8303\u6027\u7ef4\u5ea6\u3002"}}
{"id": "2508.03642", "pdf": "https://arxiv.org/pdf/2508.03642", "abs": "https://arxiv.org/abs/2508.03642", "authors": ["Oliver Westphal"], "title": "Intent Preserving Generation of Diverse and Idiomatic (Code-)Artifacts", "categories": ["cs.SE"], "comment": "In Proceedings TFPiE 2025, arXiv:2508.02305", "summary": "When automatically generating programming exercise tasks one often also needs\nto automatically generate programs. At the very least when providing sample\nsolutions is part of automated feedback. But programs can also be used as part\nof the exercise task description to communicate a task's requirements.\n  Writing good program generators that produce varied yet idiomatic code while\nbeing easily adaptable for new tasks is challenging. The challenges are\nintensified if task generation requires additional artifacts, like a more\ngeneral behavior specification for testing or additional textual descriptions.\nManually writing generators for multiple different but strongly related\nartifacts gets complicated quickly.\n  We present an approach where instead of writing monolithic generators for\nmultiple connected artifacts one specifies a small set of abstract building\nblocks and for each such building block defines sets of concrete realizations\nfor various kinds of artifacts. Then the intended structure of the resulting\nartifacts is specified as a composition of the small abstract building blocks.\nThis abstract description then serves as the common source from which related\nartifacts can be derived automatically. The approach is generic in the kind of\nartifacts it can produce and is therefore adaptable to a wide range of\ncontexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5b9a\u4e49\u62bd\u8c61\u6784\u5efa\u5757\u6765\u81ea\u52a8\u751f\u6210\u7f16\u7a0b\u7ec3\u4e60\u4efb\u52a1\u548c\u76f8\u5173\u5de5\u4ef6\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u4e3a\u6bcf\u4e2a\u76f8\u5173\u5de5\u4ef6\u5355\u72ec\u7f16\u5199\u5e9e\u5927\u751f\u6210\u5668\u7684\u590d\u6742\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u751f\u6210\u7f16\u7a0b\u7ec3\u4e60\u4efb\u52a1\u65f6\uff0c\u901a\u5e38\u9700\u8981\u751f\u6210\u7a0b\u5e8f\uff08\u5982\u793a\u4f8b\u89e3\u51b3\u65b9\u6848\u6216\u4efb\u52a1\u63cf\u8ff0\uff09\uff0c\u4f46\u7f16\u5199\u751f\u6210\u5668\u53ef\u80fd\u590d\u6742\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u9700\u8981\u591a\u79cd\u76f8\u5173\u5de5\u4ef6\u65f6\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u4e00\u7ec4\u62bd\u8c61\u6784\u5efa\u5757\u53ca\u5176\u5177\u4f53\u5b9e\u73b0\uff0c\u518d\u4ee5\u8fd9\u4e9b\u6784\u5efa\u5757\u7684\u7ec4\u5408\u7ed3\u6784\u6765\u63cf\u8ff0\u76ee\u6807\u5de5\u4ef6\uff0c\u4ece\u800c\u81ea\u52a8\u751f\u6210\u591a\u79cd\u76f8\u5173\u5de5\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u591a\u6837\u5316\u4e14\u7b26\u5408\u4e60\u60ef\u7684\u4ee3\u7801\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0a\u4e0b\u6587\uff0c\u907f\u514d\u4e86\u4e3a\u6bcf\u4e2a\u5de5\u4ef6\u5355\u72ec\u7f16\u5199\u751f\u6210\u5668\u7684\u9ebb\u70e6\u3002", "conclusion": "\u57fa\u4e8e\u62bd\u8c61\u6784\u5efa\u5757\u7684\u65b9\u6cd5\u7b80\u5316\u4e86\u7f16\u7a0b\u7ec3\u4e60\u4efb\u52a1\u53ca\u76f8\u5173\u5de5\u4ef6\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u5177\u6709\u901a\u7528\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2508.02736", "pdf": "https://arxiv.org/pdf/2508.02736", "abs": "https://arxiv.org/abs/2508.02736", "authors": ["Yusheng Zheng", "Yanpeng Hu", "Tong Yu", "Andi Quinn"], "title": "AgentSight: System-Level Observability for AI Agents Using eBPF", "categories": ["cs.OS", "cs.SE"], "comment": null, "summary": "Modern software infrastructure increasingly relies on LLM agents for\ndevelopment and maintenance, such as Claude Code and Gemini-cli. However, these\nAI agents differ fundamentally from traditional deterministic software, posing\na significant challenge to conventional monitoring and debugging. This creates\na critical semantic gap: existing tools observe either an agent's high-level\nintent (via LLM prompts) or its low-level actions (e.g., system calls), but\ncannot correlate these two views. This blindness makes it difficult to\ndistinguish between benign operations, malicious attacks, and costly failures.\nWe introduce AgentSight, an AgentOps observability framework that bridges this\nsemantic gap using a hybrid approach. Our approach, boundary tracing, monitors\nagents from outside their application code at stable system interfaces using\neBPF. AgentSight intercepts TLS-encrypted LLM traffic to extract semantic\nintent, monitors kernel events to observe system-wide effects, and causally\ncorrelates these two streams across process boundaries using a real-time engine\nand secondary LLM analysis. This instrumentation-free technique is\nframework-agnostic, resilient to rapid API changes, and incurs less than 3%\nperformance overhead. Our evaluation shows AgentSight detects prompt injection\nattacks, identifies resource-wasting reasoning loops, and reveals hidden\ncoordination bottlenecks in multi-agent systems. AgentSight is released as an\nopen-source project at https://github.com/agent-sight/agentsight.", "AI": {"tldr": "AgentSight\u662f\u4e00\u4e2aAgentOps\u53ef\u89c2\u6d4b\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u754c\u8ffd\u8e2a\u6280\u672f\u586b\u8865LLM\u4ee3\u7406\u610f\u56fe\u4e0e\u4f4e\u5c42\u884c\u4e3a\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u4f7f\u7528eBPF\u5b9e\u73b0\u4f4e\u5f00\u9500\u76d1\u63a7\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u5173\u8054LLM\u4ee3\u7406\u7684\u9ad8\u5c42\u610f\u56fe\u4e0e\u4f4e\u5c42\u884c\u4e3a\uff0c\u5bfc\u81f4\u96be\u4ee5\u533a\u5206\u826f\u6027\u64cd\u4f5c\u3001\u6076\u610f\u653b\u51fb\u548c\u6545\u969c\u3002", "method": "\u7ed3\u5408\u8fb9\u754c\u8ffd\u8e2a\u6280\u672f\uff0c\u901a\u8fc7eBPF\u76d1\u63a7\u7cfb\u7edf\u63a5\u53e3\uff0c\u63d0\u53d6\u52a0\u5bc6\u6d41\u91cf\u8bed\u4e49\u610f\u56fe\uff0c\u5e76\u4e0e\u5185\u6838\u4e8b\u4ef6\u56e0\u679c\u5173\u8054\u3002", "result": "AgentSight\u80fd\u68c0\u6d4b\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3001\u8d44\u6e90\u6d6a\u8d39\u5faa\u73af\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u74f6\u9888\uff0c\u6027\u80fd\u5f00\u9500\u4f4e\u4e8e3%\u3002", "conclusion": "AgentSight\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u4fb5\u5165\u3001\u6846\u67b6\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86LLM\u4ee3\u7406\u76d1\u63a7\u7684\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2508.02926", "pdf": "https://arxiv.org/pdf/2508.02926", "abs": "https://arxiv.org/abs/2508.02926", "authors": ["Arthur Cho"], "title": "GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics", "categories": ["cs.LG", "cs.AI", "cs.HC", "I.2.6; I.2.7"], "comment": "26 pages, 1 table. Open-source implementation available on PyPI\n  (grandjury package) and GitHub. Dataset available on Hugging Face under\n  CC-BY-4.0 license", "summary": "Generative Machine Learning models have become central to modern systems,\npowering applications in creative writing, summarization, multi-hop reasoning,\nand context-aware dialogue. These models underpin large-scale AI assistants,\nworkflow automation, and autonomous decision-making. In such domains,\nacceptable response is rarely absolute or static, but plural and highly\ncontext-dependent. Yet standard evaluation regimes still rely on static,\nbenchmark-style tests, incentivizing optimization toward leaderboard scores\nrather than alignment with dynamic user needs or evolving realities. GrandJury\nintroduces a formal evaluation protocol combining time-decayed aggregation,\ncomplete traceability, with the support of dynamic, transparent task rubric\nattribution, and multi-rater human judgment. Together, these elements enable\npluralistic, accountable evaluation that captures evolving consensus and\nsurfaces disagreement. We provide an open-source implementation (grandjury PyPI\npackage) and a public collection of Large Language Model (LLM) inference\noutputs to illustrate the need and method. GrandJury provides a new paradigm\nfor AI practitioners when evaluating machine learning outputs without absolute\nground truth.", "AI": {"tldr": "GrandJury\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u7ed3\u5408\u65f6\u95f4\u8870\u51cf\u805a\u5408\u3001\u5b8c\u6574\u8ffd\u8e2a\u548c\u52a8\u6001\u4efb\u52a1\u8bc4\u5206\uff0c\u652f\u6301\u591a\u5143\u5316\u548c\u53ef\u8ffd\u8d23\u7684AI\u6a21\u578b\u8bc4\u4f30\u3002", "motivation": "\u73b0\u884cAI\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u7528\u6237\u9700\u6c42\u6216\u4e0d\u65ad\u53d8\u5316\u7684\u5b9e\u9645\u60c5\u51b5\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "GrandJury\u534f\u8bae\u6574\u5408\u4e86\u65f6\u95f4\u8870\u51cf\u805a\u5408\u3001\u5b8c\u6574\u8ffd\u8e2a\u3001\u52a8\u6001\u4efb\u52a1\u8bc4\u5206\u548c\u591a\u8bc4\u59d4\u4eba\u7c7b\u5224\u65ad\uff0c\u63d0\u4f9b\u900f\u660e\u4e14\u591a\u5143\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5f00\u6e90\u5b9e\u73b0\u548c\u516c\u5f00\u6570\u636e\u96c6\u5c55\u793a\u4e86\u65b0\u8bc4\u4f30\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\uff0c\u4e3aAI\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u65e0\u7edd\u5bf9\u771f\u5b9e\u60c5\u51b5\u4e0b\u7684\u8bc4\u4f30\u65b0\u8303\u5f0f\u3002", "conclusion": "GrandJury\u4e3aAI\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u900f\u660e\u548c\u53ef\u8ffd\u8d23\u7684\u65b9\u6848\u3002"}}
{"id": "2508.03037", "pdf": "https://arxiv.org/pdf/2508.03037", "abs": "https://arxiv.org/abs/2508.03037", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 5 figures, 5 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape.", "AI": {"tldr": "\u5206\u6790\u4e862013\u5e74\u81f32025\u5e74AI\u751f\u6210\u827a\u672f\u7684\u82f1\u6587\u8ba8\u8bba\uff0c\u63ed\u793a\u827a\u672f\u5bb6\u89c2\u70b9\u4e0e\u4e3b\u6d41\u53d9\u4e8b\u7684\u504f\u79bb\uff0c\u6280\u672f\u672f\u8bed\u53ef\u80fd\u963b\u788d\u827a\u672f\u5bb6\u53c2\u4e0e\u3002", "motivation": "\u63a2\u8ba8AI\u5982\u4f55\u5f71\u54cd\u827a\u672f\u521b\u4f5c\u53ca\u827a\u672f\u5bb6\u751f\u8ba1\uff0c\u5f3a\u8c03\u827a\u672f\u5bb6\u58f0\u97f3\u5728\u8ba8\u8bba\u4e2d\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "\u91c7\u7528BERTopic\u65b9\u6cd5\u5206\u6790439\u7bc7500\u5b57\u6587\u672c\uff0c\u8bc6\u522b\u4e94\u4e2a\u7a33\u5b9a\u4e3b\u9898\u7fa4\u3002", "result": "\u53d1\u73b0\u6280\u672f\u672f\u8bed\u53ef\u80fd\u6210\u4e3a\u95e8\u69db\uff0c\u827a\u672f\u5bb6\u5173\u5207\u5e38\u88ab\u5ffd\u7565\u3002", "conclusion": "\u547c\u5401\u66f4\u591a\u900f\u660e\u5ea6\u548c\u827a\u672f\u5bb6\u89c6\u89d2\u7684\u53c2\u4e0e\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.03379", "pdf": "https://arxiv.org/pdf/2508.03379", "abs": "https://arxiv.org/abs/2508.03379", "authors": ["Wenxin Mao", "Zhitao Wang Long Wang", "Sirong Chen", "Cuiyun Gao", "Luyang Cao", "Ziming Liu", "Qiming Zhang", "Jun Zhou", "Zhi Jin"], "title": "Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) excel at generating code from natural language\n(NL) descriptions. However, the plain textual descriptions are inherently\nambiguous and often fail to capture complex requirements like intricate system\nbehaviors, conditional logic, and architectural constraints; implicit data\ndependencies in service-oriented architectures are difficult to infer and\nhandle correctly. To bridge this gap, we propose a novel step-by-step code\ngeneration framework named UML2Dep by leveraging unambiguous formal\nspecifications of complex requirements. First, we introduce an enhanced Unified\nModeling Language (UML) sequence diagram tailored for service-oriented\narchitectures. This diagram extends traditional visual syntax by integrating\ndecision tables and API specifications, explicitly formalizing structural\nrelationships and business logic flows in service interactions to rigorously\neliminate linguistic ambiguity. Second, recognizing the critical role of data\nflow, we introduce a dedicated data dependency inference (DDI) task. DDI\nsystematically constructs an explicit data dependency graph prior to actual\ncode synthesis. To ensure reliability, we formalize DDI as a constrained\nmathematical reasoning task through novel prompting strategies, aligning with\nLLMs' excellent mathematical strengths. Additional static parsing and\ndependency pruning further reduce context complexity and cognitive load\nassociated with intricate specifications, thereby enhancing reasoning accuracy\nand efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUML2Dep\u7684\u5206\u6b65\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u89c4\u8303\u6d88\u9664\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u6b67\u4e49\u6027\uff0c\u63d0\u5347LLM\u751f\u6210\u4ee3\u7801\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5728\u590d\u6742\u9700\u6c42\uff08\u5982\u7cfb\u7edf\u884c\u4e3a\u3001\u6761\u4ef6\u903b\u8f91\u548c\u67b6\u6784\u7ea6\u675f\uff09\u4e2d\u5bb9\u6613\u4ea7\u751f\u6b67\u4e49\uff0c\u7279\u522b\u662f\u5728\u670d\u52a1\u5bfc\u5411\u67b6\u6784\u4e2d\u9690\u542b\u7684\u6570\u636e\u4f9d\u8d56\u6027\u95ee\u9898\u96be\u4ee5\u5904\u7406\u3002", "method": "1. \u63d0\u51fa\u4e86\u589e\u5f3a\u7248\u7684UML\u5e8f\u5217\u56fe\uff0c\u6574\u5408\u51b3\u7b56\u8868\u548cAPI\u89c4\u8303\u4ee5\u6d88\u9664\u6b67\u4e49\u30022. \u5f15\u5165\u6570\u636e\u4f9d\u8d56\u63a8\u65ad\uff08DDI\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u7ea6\u675f\u6570\u5b66\u63a8\u7406\u548c\u63d0\u793a\u7b56\u7565\u6784\u5efa\u663e\u5f0f\u6570\u636e\u4f9d\u8d56\u56fe\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u5f62\u5f0f\u5316\u89c4\u8303\u548c\u9759\u6001\u89e3\u6790\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "UML2Dep\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u7684\u6b67\u4e49\u6027\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u9700\u6c42\u7684\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03488", "pdf": "https://arxiv.org/pdf/2508.03488", "abs": "https://arxiv.org/abs/2508.03488", "authors": ["Khaled Bachir Delassi", "Lakhdar Zeggane", "Hadda Cherroun", "Abdelhamid Haouhat", "Kaoutar Bouzouad"], "title": "VQA support to Arabic Language Learning Educational Tool", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "We address the problem of scarcity of educational Arabic Language Learning\ntools that advocate modern pedagogical models such as active learning which\nensures language proficiency. In fact, we investigate the design and evaluation\nof an AI-powered educational tool designed to enhance Arabic language learning\nfor non-native speakers with beginner-to-intermediate proficiency level. The\ntool leverages advanced AI models to generate interactive visual quizzes,\ndeploying Visual Question Answering as the primary activity. Adopting a\nconstructivist learning approach, the system encourages active learning through\nreal-life visual quizzes, and image-based questions that focus on improving\nvocabulary, grammar, and comprehension. The system integrates Vision-Language\nPretraining models to generate contextually relevant image description from\nwhich Large Language Model generate assignments based on customized Arabic\nlanguage Learning quizzes thanks to prompting.\n  The effectiveness of the tool is evaluated through a manual annotated\nbenchmark consisting of 1266 real-life visual quizzes, with human participants\nproviding feedback. The results show a suitable accuracy rates, validating the\ntool's potential to bridge the gap in Arabic language education and\nhighlighting the tool's promise as a reliable, AI-powered resource for Arabic\nlearners, offering personalized and interactive learning experiences.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\uff1aAI\u9a71\u52a8\u7684\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u5de5\u5177\uff0c\u901a\u8fc7\u89c6\u89c9\u95ee\u7b54\u548c\u4e3b\u52a8\u5b66\u4e60\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u5b66\u4e60\u5de5\u5177\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u73b0\u4ee3\u6559\u80b2\u5b66\u6a21\u578b\uff08\u5982\u4e3b\u52a8\u5b66\u4e60\uff09\u7684\u5de5\u5177\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2aAI\u5de5\u5177\uff0c\u5229\u7528\u89c6\u89c9\u95ee\u7b54\u548c\u6784\u5efa\u4e3b\u4e49\u5b66\u4e60\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ea4\u4e92\u5f0f\u6d4b\u9a8c\u3002", "result": "\u5de5\u5177\u57281266\u4e2a\u89c6\u89c9\u95ee\u7b54\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5408\u9002\u7684\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u5177\u80fd\u6709\u6548\u586b\u8865\u963f\u62c9\u4f2f\u8bed\u6559\u80b2\u7a7a\u767d\uff0c\u4e3a\u5b66\u4e60\u8005\u63d0\u4f9b\u4e2a\u6027\u5316\u548c\u4e92\u52a8\u7684\u5b66\u4e60\u4f53\u9a8c\u3002"}}
{"id": "2508.03501", "pdf": "https://arxiv.org/pdf/2508.03501", "abs": "https://arxiv.org/abs/2508.03501", "authors": ["Alexander Golubev", "Maria Trofimova", "Sergei Polezhaev", "Ibragim Badertdinov", "Maksim Nekrashevich", "Anton Shevtsov", "Simon Karasik", "Sergey Abramov", "Andrei Andriushchenko", "Filipp Fisin", "Sergei Skvortsov", "Boris Yangel"], "title": "Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning", "categories": ["cs.LG", "cs.CL", "cs.SE"], "comment": null, "summary": "Research on applications of Reinforcement Learning (RL) to Large Language\nModels (LLMs) has mostly been focused on single-turn problems, such as\nmathematical reasoning or single-shot code generation. While these problems can\nbe viewed as token-level multi-turn MDPs, this view corresponds to a degenerate\ncase of multi-turn interaction where the environment provides no feedback. This\ncontrasts with many real-world domains, such as software engineering (SWE),\nwhich require rich multi-turn interactions with a stateful environment that\nresponds to each action with a non-trivial observation.\n  To bridge this gap, we demonstrate the successful application of RL to this\ngeneral regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)\nalgorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world\nsoftware engineering tasks. Our approach increases the agent's success rate on\nthe SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to\n39%, without relying on any teacher models. On SWE-rebench, our agent matches\nor outperforms leading open-weight models such as DeepSeek-V3-0324 and\nQwen3-235B-A22B using an identical scaffolding, offering a viable path toward\nbuilding more capable autonomous agents for complex real-world problems based\non open models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u9700\u8981\u591a\u8f6e\u4ea4\u4e92\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\uff08\u5982\u8f6f\u4ef6\u5de5\u7a0b\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6210\u529f\u6848\u4f8b\u3002", "motivation": "\u586b\u8865\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u8f6e\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u7a7a\u767d\uff0c\u7279\u522b\u662f\u8f6f\u4ef6\u5de5\u7a0b\u8fd9\u7c7b\u9700\u8981\u4e0e\u73af\u5883\u591a\u6b21\u4ea4\u4e92\u7684\u9886\u57df\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684DAPO\u7b97\u6cd5\uff0c\u57fa\u4e8eQwen2.5-72B-Instruct\u6a21\u578b\u8bad\u7ec3\u667a\u80fd\u4f53\u3002", "result": "\u5728SWE-bench\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u4ece20%\u63d0\u5347\u81f339%\uff0c\u5e76\u5728SWE-rebench\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "\u8bc1\u660e\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u8f6e\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.03514", "pdf": "https://arxiv.org/pdf/2508.03514", "abs": "https://arxiv.org/abs/2508.03514", "authors": ["Pavlos Panagiotidis", "Victor Zhi Heung Ngo", "Sean Myatt", "Roma Patel", "Rachel Ramchurn", "Alan Chamberlain", "Ayse Kucukyilmaz"], "title": "Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours", "categories": ["cs.RO", "cs.HC"], "comment": "The paper is accepted for presentation to International Conference on\n  Social Robotics + AI (https://icsr2025.eu/)", "summary": "In this paper, we propose theatre-in-the-loop, a framework for developing\nexpressive robot behaviours tailored to artistic performance through a\ndirector-guided puppeteering workflow. Leveraging theatrical methods, we use\nnarrative objectives to direct a puppeteer in generating improvised robotic\ngestures that convey specific emotions. These improvisations are captured and\ncurated to build a dataset of reusable movement templates for standalone\nplayback in future autonomous performances. Initial trials demonstrate the\nfeasibility of this approach, illustrating how the workflow enables precise\nsculpting of robotic gestures into coherent emotional arcs while revealing\nchallenges posed by the robot's mechanical constraints. We argue that this\npractice-led framework provides a model for interdisciplinary teams creating\nsocially expressive robot behaviours, contributing to (1) theatre as an\ninteractive training ground for human-robot interaction and (2) co-creation\nmethodologies between humans and machines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bfc\u6f14\u6307\u5bfc\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u620f\u5267\u5316\u65b9\u6cd5\u751f\u6210\u673a\u5668\u4eba\u60c5\u611f\u5316\u884c\u4e3a\u3002", "motivation": "\u4e3a\u827a\u672f\u8868\u6f14\u5f00\u53d1\u5177\u6709\u8868\u8fbe\u529b\u7684\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u901a\u8fc7\u620f\u5267\u65b9\u6cd5\u6307\u5bfc\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u3002", "method": "\u5229\u7528\u5bfc\u6f14\u6307\u5bfc\u7684\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u5373\u5174\u624b\u52bf\u751f\u6210\u60c5\u611f\u8868\u8fbe\uff0c\u5e76\u5efa\u7acb\u53ef\u91cd\u7528\u7684\u52a8\u4f5c\u6a21\u677f\u5e93\u3002", "result": "\u521d\u6b65\u8bd5\u9a8c\u8bc1\u660e\u4e86\u6846\u67b6\u7684\u53ef\u884c\u6027\uff0c\u4f46\u673a\u5668\u4eba\u673a\u68b0\u9650\u5236\u5e26\u6765\u6311\u6218\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8de8\u5b66\u79d1\u56e2\u961f\u521b\u5efa\u793e\u4ea4\u673a\u5668\u4eba\u884c\u4e3a\u63d0\u4f9b\u4e86\u6a21\u578b\uff0c\u5e76\u4fc3\u8fdb\u4e86\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.03588", "pdf": "https://arxiv.org/pdf/2508.03588", "abs": "https://arxiv.org/abs/2508.03588", "authors": ["Zhaoyi Meng", "Fenglei Xu", "Wenxiang Zhao", "Wansen Wang", "Wenchao Huang", "Jie Cui", "Hong Zhong", "Yan Xiong"], "title": "MalFlows: Context-aware Fusion of Heterogeneous Flow Semantics for Android Malware Detection", "categories": ["cs.CR", "cs.SE"], "comment": "Submitted to TDSC", "summary": "Static analysis, a fundamental technique in Android app examination, enables\nthe extraction of control flows, data flows, and inter-component communications\n(ICCs), all of which are essential for malware detection. However, existing\nmethods struggle to leverage the semantic complementarity across different\ntypes of flows for representing program behaviors, and their context-unaware\nnature further hinders the accuracy of cross-flow semantic integration. We\npropose and implement MalFlows, a novel technique that achieves context-aware\nfusion of heterogeneous flow semantics for Android malware detection. Our goal\nis to leverage complementary strengths of the three types of flow-related\ninformation for precise app profiling. We adopt a heterogeneous information\nnetwork (HIN) to model the rich semantics across these program flows. We\nfurther propose flow2vec, a context-aware HIN embedding technique that\ndistinguishes the semantics of HIN entities as needed based on contextual\nconstraints across different flows and learns accurate app representations\nthrough the joint use of multiple meta-paths. The representations are finally\nfed into a channel-attention-based deep neural network for malware\nclassification. To the best of our knowledge, this is the first study to\ncomprehensively aggregate the strengths of diverse flow-related information for\nassessing maliciousness within apps. We evaluate MalFlows on a large-scale\ndataset comprising over 20 million flow instances extracted from more than\n31,000 real-world apps. Experimental results demonstrate that MalFlows\noutperforms representative baselines in Android malware detection, and\nmeanwhile, validate the effectiveness of flow2vec in accurately learning app\nrepresentations from the HIN constructed over the heterogeneous flows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMalFlows\u6280\u672f\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5f02\u6784\u6d41\u8bed\u4e49\u878d\u5408\uff0c\u63d0\u5347\u5b89\u5353\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u4e0d\u540c\u7c7b\u578b\u6d41\u8bed\u4e49\u4e92\u8865\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u9650\u5236\u4e86\u6d41\u8bed\u4e49\u6574\u5408\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5f02\u6784\u4fe1\u606f\u7f51\u7edc\uff08HIN\uff09\u5efa\u6a21\u6d41\u8bed\u4e49\uff0c\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u7684HIN\u5d4c\u5165\u6280\u672fflow2vec\uff0c\u5e76\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u572831,000\u591a\u4e2a\u771f\u5b9e\u5e94\u7528\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0cMalFlows\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86flow2vec\u7684\u6709\u6548\u6027\u3002", "conclusion": "MalFlows\u662f\u9996\u4e2a\u5168\u9762\u6574\u5408\u591a\u7c7b\u6d41\u4fe1\u606f\u7684\u7814\u7a76\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u6027\u80fd\u3002"}}
