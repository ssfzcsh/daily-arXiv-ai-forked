<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 16]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CASCADE: LLM-Powered JavaScript Deobfuscator at Google](https://arxiv.org/abs/2507.17691)
*Shan Jiang,Pranoy Kovuri,David Tao,Zhixun Tan*

Main category: cs.SE

TL;DR: CASCADE 是一种结合 Gemini 和 JSIR 的新型混合方法，用于高效反混淆 JavaScript 代码，恢复原始语义并减少逆向工程工作量。


<details>
  <summary>Details</summary>
Motivation: 解决 JavaScript 代码混淆带来的分析挑战，如测试、静态分析和恶意软件检测。

Method: 利用 Gemini 识别关键前导函数，并通过 JSIR 进行代码转换，恢复原始字符串和 API 名称。

Result: 显著提高反混淆效率，减少硬编码规则，已在 Google 生产环境中部署。

Conclusion: CASCADE 在可靠性和灵活性上优于现有技术，为代码分析提供实用解决方案。

Abstract: Software obfuscation, particularly prevalent in JavaScript, hinders code
comprehension and analysis, posing significant challenges to software testing,
static analysis, and malware detection. This paper introduces CASCADE, a novel
hybrid approach that integrates the advanced coding capabilities of Gemini with
the deterministic transformation capabilities of a compiler Intermediate
Representation (IR), specifically JavaScript IR (JSIR). By employing Gemini to
identify critical prelude functions, the foundational components underlying the
most prevalent obfuscation techniques, and leveraging JSIR for subsequent code
transformations, CASCADE effectively recovers semantic elements like original
strings and API names, and reveals original program behaviors. This method
overcomes limitations of existing static and dynamic deobfuscation techniques,
eliminating hundreds to thousands of hardcoded rules while achieving
reliability and flexibility. CASCADE is already deployed in Google's production
environment, demonstrating substantial improvements in JavaScript deobfuscation
efficiency and reducing reverse engineering efforts.

</details>


### [2] [Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots](https://arxiv.org/abs/2507.17049)
*Pablo Valle,Chengjie Lu,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: 该论文提出了针对VLA模型的八种不确定性指标和五种质量指标，用于更全面地评估机器人操作任务的质量和模型自信度。通过大规模实证研究，发现这些指标与专家评估有中度到强相关性，挑战了仅依赖二元成功率的评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型的评估仅依赖任务成功率，无法捕捉任务执行质量和模型自信度。为填补这一空白，作者提出新指标以改进评估方法。

Method: 设计了八种不确定性指标和五种质量指标，通过在908次成功任务中应用三种VLA模型，结合专家人工标注分析指标与评估的相关性。

Result: 多个指标与专家评估有中度到强相关性，部分指标还能区分不同质量的任务执行。

Conclusion: 研究发现新指标能更全面地评估VLA模型，为实时监控和适应性增强提供了方向。

Abstract: Visual Language Action (VLA) models are a multi-modal class of Artificial
Intelligence (AI) systems that integrate visual perception, natural language
understanding, and action planning to enable agents to interpret their
environment, comprehend instructions, and perform embodied tasks autonomously.
Recently, significant progress has been made to advance this field. These kinds
of models are typically evaluated through task success rates, which fail to
capture the quality of task execution and the mode's confidence in its
decisions. In this paper, we propose eight uncertainty metrics and five quality
metrics specifically designed for VLA models for robotic manipulation tasks. We
assess their effectiveness through a large-scale empirical study involving 908
successful task executions from three state-of-the-art VLA models across four
representative robotic manipulation tasks. Human domain experts manually
labeled task quality, allowing us to analyze the correlation between our
proposed metrics and expert judgments. The results reveal that several metrics
show moderate to strong correlation with human assessments, highlighting their
utility for evaluating task quality and model confidence. Furthermore, we found
that some of the metrics can discriminate between high-, medium-, and
low-quality executions from unsuccessful tasks, which can be interesting when
test oracles are not available. Our findings challenge the adequacy of current
evaluation practices that rely solely on binary success rates and pave the way
for improved real-time monitoring and adaptive enhancement of VLA-enabled
robotic systems.

</details>


### [3] [Assessing Reliability of Statistical Maximum Coverage Estimators in Fuzzing](https://arxiv.org/abs/2507.17093)
*Danushka Liyanage,Nelum Attanayake,Zijian Luo,Rahul Gopinath*

Main category: cs.SE

TL;DR: 评估基于生物统计学物种丰富度估计器的可达性估计方法在模糊测试中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于模糊测试中可达性估计的重要性及缺乏可靠基准，本研究旨在验证现有估计器的准确性。

Method: 提出合成程序框架生成真实基准，并设计可靠性检查方法测试实际程序。

Result: 研究确定了当前估计器的可靠性，并制定了评估未来改进的协议。

Conclusion: 通过合成与实际程序的双重验证，为可达性估计提供了更可靠的评估方法。

Abstract: Background: Fuzzers are often guided by coverage, making the estimation of
maximum achievable coverage a key concern in fuzzing. However, achieving 100%
coverage is infeasible for most real-world software systems, regardless of
effort. While static reachability analysis can provide an upper bound, it is
often highly inaccurate. Recently, statistical estimation methods based on
species richness estimators from biostatistics have been proposed as a
potential solution. Yet, the lack of reliable benchmarks with labeled ground
truth has limited rigorous evaluation of their accuracy.
  Objective: This work examines the reliability of reachability estimators from
two axes: addressing the lack of labeled ground truth and evaluating their
reliability on real-world programs.
  Methods: (1) To address the challenge of labeled ground truth, we propose an
evaluation framework that synthetically generates large programs with complex
control flows, ensuring well-defined reachability and providing ground truth
for evaluation. (2) To address the criticism from use of synthetic benchmarks,
we adapt a reliability check for reachability estimators on real-world
benchmarks without labeled ground truth -- by varying the size of sampling
units, which, in theory, should not affect the estimate.
  Results: These two studies together will help answer the question of whether
current reachability estimators are reliable, and defines a protocol to
evaluate future improvements in reachability estimation.

</details>


### [4] [Can LLMs Write CI? A Study on Automatic Generation of GitHub Actions Configurations](https://arxiv.org/abs/2507.17165)
*Taher A. Ghaleb,Dulina Rathnayake*

Main category: cs.SE

TL;DR: 论文评估了六种大型语言模型（LLM）在从自然语言描述生成GitHub Actions配置时的表现，发现零-shot提示下最高相似度为69%，但完美匹配率仅3%。代码预训练模型表现略逊于通用模型，揭示了LLM在CI配置生成中的局限性。


<details>
  <summary>Details</summary>
Motivation: 开发者编写YAML配置繁琐且易错，而LLM在此任务中的应用尚未充分探索。本文旨在填补这一空白。

Method: 评估了六种LLM（三种通用基础和三种代码预训练模型），并创建了一个标注数据集，将描述与最佳实践YAML配置配对。

Result: 零-shot提示下最高相似度为69%，完美匹配仅3%。代码预训练模型表现略差，存在步骤缺失、误解描述等问题。

Conclusion: LLM在CI配置生成中仍需改进。研究为提升LLM与配置语言的适配性提供了方向。

Abstract: Continuous Integration (CI) services, such as GitHub Actions, require
developers to write YAML-based configurations, which can be tedious and
error-prone. Despite the increasing use of Large Language Models (LLMs) to
automate software engineering tasks, their ability to generate CI
configurations remains underexplored. This paper presents a preliminary study
evaluating six LLMs for generating GitHub Actions configurations from natural
language descriptions. We assess three general-purpose foundation models
(GPT-4o, Llama, and Gemma) and three code-pretrained models (GPT-4.1, Code
Llama, and CodeGemma). We also introduce the first labeled dataset of its kind,
constructed from GitHub Actions documentation, pairing descriptions with
corresponding best-practice YAML configurations. Zero-shot prompting achieves
up to 69% similarity with the ground truth, with only 3% perfect matches.
Code-pretrained models slightly underperform compared to general-purpose ones
in YAML-based CI tasks, revealing LLM limitations for CI configuration
generation. Analyzing GPT-4o outputs reveals issues like missing or renamed
steps, misinterpreted descriptions, and unnecessary additions that may affect
structural and contextual correctness, indicating a gap between generation
quality and the precision required for executable CI configurations. Our
research offers insights for improving LLM alignment with configuration
languages and guiding future efforts on CI automation and tooling support.

</details>


### [5] [On the Feasibility of Quantum Unit Testing](https://arxiv.org/abs/2507.17235)
*Andriy Miranskyy,José Campos,Anila Mjeda,Lei Zhang,Ignacio García Rodríguez de Guzmán*

Main category: cs.SE

TL;DR: 该论文研究了量子软件测试中的量子中心化单元测试，比较了传统统计方法与专为量子电路设计的测试方法。通过实验验证，量子中心化测试（如状态向量测试和逆向测试）在精度和效率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件复杂性的增加，传统验证方法面临挑战。本研究旨在探索更有效的量子软件测试策略，为未来量子计算的可靠性和扩展性提供支持。

Method: 研究对比了传统统计测试与量子中心化测试（如状态向量测试、交换测试和逆向测试），并通过1796880个变异量子电路进行实证分析。

Result: 量子中心化测试（尤其是状态向量测试和逆向测试）在检测电路误差方面表现更优，显著减少了误报和漏报，同时降低了所需测量次数。

Conclusion: 该研究为量子软件测试提供了更精确高效的策略，支持未来容错量子计算机的发展，并促进了量子软件工程的可靠性实践。

Abstract: The increasing complexity of quantum software presents significant challenges
for software verification and validation, particularly in the context of unit
testing. This work presents a comprehensive study on quantum-centric unit
tests, comparing traditional statistical approaches with tests specifically
designed for quantum circuits. These include tests that run only on a classical
computer, such as the Statevector test, as well as those executable on quantum
hardware, such as the Swap test and the novel Inverse test. Through an
empirical study and detailed analysis on 1,796,880 mutated quantum circuits, we
investigate (a) each test's ability to detect subtle discrepancies between the
expected and actual states of a quantum circuit, and (b) the number of
measurements required to achieve high reliability. The results demonstrate that
quantum-centric tests, particularly the Statevector test and the Inverse test,
provide clear advantages in terms of precision and efficiency, reducing both
false positives and false negatives compared to statistical tests. This work
contributes to the development of more robust and scalable strategies for
testing quantum software, supporting the future adoption of fault-tolerant
quantum computers and promoting more reliable practices in quantum software
engineering.

</details>


### [6] [Understanding Prompt Programming Tasks and Questions](https://arxiv.org/abs/2507.17264)
*Jenny T. Liang,Chenyang Yang,Agnia Sergeyuk,Travis D. Breaux,Brad A. Myers*

Main category: cs.SE

TL;DR: 本文研究了提示编程中的开发者需求，通过分类任务和问题，发现当前工具支持不足，提出了改进机会。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型（如大语言模型）的广泛应用，开发者通过提示编程实现新功能，但当前工具未能充分支持开发者的需求，研究旨在揭示这些需求并为工具改进提供依据。

Method: 通过访谈16名提示程序员、观察8名开发者修改提示内容，以及调查50名开发者，构建了25个任务和51个问题的分类体系，并比较了48个研究及商业工具。

Result: 发现提示编程的支持不足，所有任务仍需手动完成，51个问题中有16个（包括多数重要问题）未得到解答。

Conclusion: 研究揭示了提示编程工具的改进机会，呼吁未来工具更好地支持开发者需求。

Abstract: Prompting foundation models (FMs) like large language models (LLMs) have
enabled new AI-powered software features (e.g., text summarization) that
previously were only possible by fine-tuning FMs. Now, developers are embedding
prompts in software, known as prompt programs. The process of prompt
programming requires the developer to make many changes to their prompt. Yet,
the questions developers ask to update their prompt is unknown, despite the
answers to these questions affecting how developers plan their changes. With
the growing number of research and commercial prompt programming tools, it is
unclear whether prompt programmers' needs are being adequately addressed. We
address these challenges by developing a taxonomy of 25 tasks prompt
programmers do and 51 questions they ask, measuring the importance of each task
and question. We interview 16 prompt programmers, observe 8 developers make
prompt changes, and survey 50 developers. We then compare the taxonomy with 48
research and commercial tools. We find that prompt programming is not
well-supported: all tasks are done manually, and 16 of the 51 questions --
including a majority of the most important ones -- remain unanswered. Based on
this, we outline important opportunities for prompt programming tools.

</details>


### [7] [Lessons from a Big-Bang Integration: Challenges in Edge Computing and Machine Learning](https://arxiv.org/abs/2507.17270)
*Alessandro Aneggi,Andrea Janes*

Main category: cs.SE

TL;DR: 该报告分析了一个分布式实时分析系统项目，因采用一次性集成方法导致失败，仅运行6分钟。建议早期模拟部署和改进沟通。


<details>
  <summary>Details</summary>
Motivation: 研究分布式实时分析系统项目中集成失败的根源，探索如何改进复杂分布式项目的管理方法。

Method: 通过根因分析，识别技术和组织障碍，如沟通不足和缺乏早期测试。

Result: 项目因一次性集成失败，系统仅运行6分钟。建议采用早期模拟和结构化集成周期。

Conclusion: 传统敏捷方法在复杂分布式项目中受限，需结合模拟驱动和结构化集成以降低风险。

Abstract: This experience report analyses a one year project focused on building a
distributed real-time analytics system using edge computing and machine
learning. The project faced critical setbacks due to a big-bang integration
approach, where all components developed by multiple geographically dispersed
partners were merged at the final stage. The integration effort resulted in
only six minutes of system functionality, far below the expected 40 minutes.
Through root cause analysis, the study identifies technical and organisational
barriers, including poor communication, lack of early integration testing, and
resistance to topdown planning. It also considers psychological factors such as
a bias toward fully developed components over mockups. The paper advocates for
early mock based deployment, robust communication infrastructures, and the
adoption of topdown thinking to manage complexity and reduce risk in reactive,
distributed projects. These findings underscore the limitations of traditional
Agile methods in such contexts and propose simulation-driven engineering and
structured integration cycles as key enablers for future success.

</details>


### [8] [Seed&Steer: Guiding Large Language Models with Compilable Prefix and Branch Signals for Unit Test Generation](https://arxiv.org/abs/2507.17271)
*Shuaiyu Zhou,Zhengran Zeng,Xiaoling Zhou,Rui Xie,Shikun Zhang,Wei Ye*

Main category: cs.SE

TL;DR: 本文提出了一种名为Seed&Steer的两步法，结合传统单元测试技术与大语言模型（LLM），通过分离前缀生成和断言生成，显著提高了测试编译成功率与覆盖率。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在单元测试生成中的应用，发现前缀生成和断言生成分别影响编译成功率和测试覆盖率，亟需一种有效方法解决这些挑战。

Method: 采用Seed&Steer方法，先使用传统工具生成高编译成功率的方法调用作为种子，再通过LLM生成多样化执行路径和断言。

Result: 在五个真实Java项目中，Seed&Steer显著提升编译通过率和测试覆盖率，具体表现为编译成功率提升7%，覆盖率提高1.09*至1.26*。

Conclusion: Seed&Steer结合传统技术与LLM，有效解决了编译与覆盖率问题，为单元测试自动化提供了新思路。

Abstract: Unit tests play a vital role in the software development lifecycle. Recent
advances in Large Language Model (LLM)-based approaches have significantly
improved automated test generation, garnering attention from both academia and
industry. We revisit LLM-based unit test generation from a novel perspective by
decoupling prefix generation and assertion generation. To characterize their
respective challenges, we define Initialization Complexity and adopt Cyclomatic
Complexity to measure the difficulty of prefix and assertion generation,
revealing that the former primarily affects compilation success, while the
latter influences test coverage. To address these challenges, we propose
Seed&Steer, a two-step approach that combines traditional unit testing
techniques with the capabilities of large language models. Seed&Steer leverages
conventional unit testing tools (e.g., EvoSuite) to generate method invocations
with high compilation success rates, which serve as seeds to guide LLMs in
constructing effective test contexts. It then introduces branching cues to help
LLMs explore diverse execution paths (e.g., normal, boundary, and exception
cases) and generate assertions with high coverage. We evaluate Seed&Steer on
five real-world Java projects against state-of-the-art baselines. Results show
that Seed&Steer improves the compilation pass rate by approximately 7%,
successfully compiling 792 and 887 previously failing cases on two LLMs. It
also achieves up to ~73% branch and line coverage across focal methods of
varying complexity, with coverage improvements ranging from 1.09* to 1.26*. Our
code, dataset, and experimental scripts will be publicly released to support
future research and reproducibility.

</details>


### [9] [Data Virtualization for Machine Learning](https://arxiv.org/abs/2507.17293)
*Saiful Khan,Joyraj Chakraborty,Philip Beaucamp,Niraj Bhujel,Min Chen*

Main category: cs.SE

TL;DR: 论文提出了一种数据虚拟化服务的架构与实现，以支持多并发机器学习工作流中的数据存储、处理和维护。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习工作流的增多和复杂化，中间数据的存储、处理和维护成为组织中的关键问题，数据虚拟化技术变得至关重要。

Method: 设计并实现了一个数据虚拟化服务，重点介绍了其服务架构和服务操作。

Result: 该基础设施目前支持6个机器学习应用，每个应用包含多个工作流，并能支持未来更多的应用和工作流扩展。

Conclusion: 数据虚拟化服务是解决机器学习工作流中数据管理问题的有效方案，具备可扩展性。

Abstract: Nowadays, machine learning (ML) teams have multiple concurrent ML workflows
for different applications. Each workflow typically involves many experiments,
iterations, and collaborative activities and commonly takes months and
sometimes years from initial data wrangling to model deployment.
Organizationally, there is a large amount of intermediate data to be stored,
processed, and maintained. \emph{Data virtualization} becomes a critical
technology in an infrastructure to serve ML workflows. In this paper, we
present the design and implementation of a data virtualization service,
focusing on its service architecture and service operations. The infrastructure
currently supports six ML applications, each with more than one ML workflow.
The data virtualization service allows the number of applications and workflows
to grow in the coming years.

</details>


### [10] [How Do Code Smells Affect Skill Growth in Scratch Novice Programmers?](https://arxiv.org/abs/2507.17314)
*Ricardo Hidalgo Aragón,Jesús M. González-Barahona,Gregorio Robles*

Main category: cs.SE

TL;DR: 论文研究了新手在Scratch中创建的代码块项目中的设计问题（代码异味）与计算思维（CT）能力之间的关系，通过大规模数据分析揭示了CT维度与代码异味之间的联系，旨在为教育干预和工具开发提供依据。


<details>
  <summary>Details</summary>
Motivation: 研究背景是代码异味在专业代码中被广泛研究，但在新手创建的块状项目中仍不明确，Scratch环境为研究设计问题与计算思维技能的关系提供了独特机会。

Method: 从约200万个公共Scratch项目中随机抽样，使用开源工具提取9个CT分数和40个代码异味指标，通过描述性分析、相关测试、交叉验证和机器学习模型进行量化分析。

Result: 研究结果将首次大规模揭示特定CT能力与设计缺陷之间的关联，为教育课程和反馈系统提供依据，并为研究社区提供开放数据集和分析框架。

Conclusion: 论文通过分析编程习惯对早期技能习得的影响，推动了计算教育理论和软件维护工具的进步。

Abstract: Context. Code smells, which are recurring anomalies in design or style, have
been extensively researched in professional code. However, their significance
in block-based projects created by novices is still largely unknown.
Block-based environments such as Scratch offer a unique, data-rich setting to
examine how emergent design problems intersect with the cultivation of
computational-thinking (CT) skills. Objective. This research explores the
connection between CT proficiency and design-level code smells--issues that may
hinder software maintenance and evolution--in programs created by Scratch
developers. We seek to identify which CT dimensions align most strongly with
which code smells and whether task context moderates those associations.
Method. A random sample of aprox. 2 million public Scratch projects is mined.
Using open-source linters, we extract nine CT scores and 40 code smell
indicators from these projects. After rigorous pre-processing, we apply
descriptive analytics, robust correlation tests, stratified cross-validation,
and exploratory machine-learning models; qualitative spot-checks contextualize
quantitative patterns. Impact. The study will deliver the first large-scale,
fine-grained map linking specific CT competencies to concrete design flaws and
antipatterns. Results are poised to (i) inform evidence-based curricula and
automated feedback systems, (ii) provide effect-size benchmarks for future
educational interventions, and (iii) supply an open, pseudonymized dataset and
reproducible analysis pipeline for the research community. By clarifying how
programming habits influence early skill acquisition, the work advances both
computing-education theory and practical tooling for sustainable software
maintenance and evolution.

</details>


### [11] [Roseau: Fast, Accurate, Source-based API Breaking Change Analysis in Java](https://arxiv.org/abs/2507.17369)
*Corentin Latappy,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes,Lina Ochoa*

Main category: cs.SE

TL;DR: Roseau是一种新型静态分析工具，用于从代码构建技术无关的API模型，以研究API演化和检测破坏性变更（BCs），优于现有工具JApiCmp和Revapi。


<details>
  <summary>Details</summary>
Motivation: 研究API演化和BCs对库维护者和研究人员至关重要，但现有工具依赖二进制JAR文件，限制了其适用性。

Method: 提出Roseau，支持从源代码或字节码构建API模型，并进行大规模纵向分析。

Result: Roseau在准确性和性能上优于基准工具，F1得分为0.99，分析速度显著提升。

Conclusion: Roseau为API演化和BCs分析提供了高效且准确的工具，适用于大规模研究。

Abstract: Understanding API evolution and the introduction of breaking changes (BCs) in
software libraries is essential for library maintainers to manage backward
compatibility and for researchers to conduct empirical studies on software
library evolution. In Java, tools such as JApiCmp and Revapi are commonly used
to detect BCs between library releases, but their reliance on binary JARs
limits their applicability. This restriction hinders large-scale longitudinal
studies of API evolution and fine-grained analyses such as commit-level BC
detection. In this paper, we introduce Roseau, a novel static analysis tool
that constructs technology-agnostic API models from library code equipped with
rich semantic analyses. API models can be analyzed to study API evolution and
compared to identify BCs between any two versions of a library (releases,
commits, branches, etc.). Unlike traditional approaches, Roseau can build API
models from source code or bytecode, and is optimized for large-scale
longitudinal analyses of library histories. We assess the accuracy,
performance, and suitability of Roseau for longitudinal studies of API
evolution, using JApiCmp and Revapi as baselines. We extend and refine an
established benchmark of BCs and show that Roseau achieves higher accuracy (F1
= 0.99) than JApiCmp (F1 = 0.86) and Revapi (F1 = 0.91). We analyze 60 popular
libraries from Maven Central and find that Roseau delivers excellent
performance, detecting BCs between versions in under two seconds, including in
libraries with hundreds of thousands of lines of code. We further illustrate
the limitations of JApiCmp and Revapi for longitudinal studies and the novel
analysis capabilities offered by Roseau by tracking the evolution of Google's
Guava API and the introduction of BCs over 14 years and 6,839 commits, reducing
analysis times from a few days to a few minutes.

</details>


### [12] [Investigating Training Data Detection in AI Coders](https://arxiv.org/abs/2507.17389)
*Tianlin Li,Yunxiang Wei,Zhiming Li,Aishan Liu,Qing Guo,Xianglong Liu,Dongning Sun,Yang Liu*

Main category: cs.SE

TL;DR: 论文研究了代码大语言模型（CodeLLMs）中训练数据检测（TDD）的挑战，提出CodeSnitch基准数据集，并评估了七种TDD方法在代码数据上的表现。


<details>
  <summary>Details</summary>
Motivation: CodeLLMs在软件开发中广泛应用，但其输出可能包含敏感代码片段，引发隐私和知识产权问题，需要研究TDD方法的有效性。

Method: 研究了七种TDD方法，引入CodeSnitch数据集，设计了基于代码克隆分类的突变策略以测试鲁棒性。

Result: 在三个编程语言的9000个代码样本上评估，提供了当前TDD技术在代码数据上的系统性分析。

Conclusion: 研究为未来开发更有效和鲁棒的TDD方法提供了参考。

Abstract: Recent advances in code large language models (CodeLLMs) have made them
indispensable tools in modern software engineering. However, these models
occasionally produce outputs that contain proprietary or sensitive code
snippets, raising concerns about potential non-compliant use of training data,
and posing risks to privacy and intellectual property. To ensure responsible
and compliant deployment of CodeLLMs, training data detection (TDD) has become
a critical task. While recent TDD methods have shown promise in natural
language settings, their effectiveness on code data remains largely
underexplored. This gap is particularly important given code's structured
syntax and distinct similarity criteria compared to natural language. To
address this, we conduct a comprehensive empirical study of seven
state-of-the-art TDD methods on source code data, evaluating their performance
across eight CodeLLMs. To support this evaluation, we introduce CodeSnitch, a
function-level benchmark dataset comprising 9,000 code samples in three
programming languages, each explicitly labeled as either included or excluded
from CodeLLM training. Beyond evaluation on the original CodeSnitch, we design
targeted mutation strategies to test the robustness of TDD methods under three
distinct settings. These mutation strategies are grounded in the
well-established Type-1 to Type-4 code clone detection taxonomy. Our study
provides a systematic assessment of current TDD techniques for code and offers
insights to guide the development of more effective and robust detection
methods in the future.

</details>


### [13] [AssertFlip: Reproducing Bugs via Inversion of LLM-Generated Passing Tests](https://arxiv.org/abs/2507.17542)
*Lara Khatib,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: AssertFlip是一种利用大型语言模型（LLMs）自动生成可复现错误测试（BRTs）的新技术，其通过首先生成通过测试再反转以检测错误，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决开源和工业环境中大多数错误因缺乏可执行测试而难以复现和诊断的问题。

Method: 首先生成通过测试，再反转这些测试以在错误存在时失败。

Result: 在SWT-Bench基准测试中表现最优，失败到成功率为43.6%。

Conclusion: AssertFlip通过LLMs生成BRTs，显著提升了错误复现的效率。

Abstract: Bug reproduction is critical in the software debugging and repair process,
yet the majority of bugs in open-source and industrial settings lack executable
tests to reproduce them at the time they are reported, making diagnosis and
resolution more difficult and time-consuming. To address this challenge, we
introduce AssertFlip, a novel technique for automatically generating Bug
Reproducible Tests (BRTs) using large language models (LLMs). Unlike existing
methods that attempt direct generation of failing tests, AssertFlip first
generates passing tests on the buggy behaviour and then inverts these tests to
fail when the bug is present. We hypothesize that LLMs are better at writing
passing tests than ones that crash or fail on purpose. Our results show that
AssertFlip outperforms all known techniques in the leaderboard of SWT-Bench, a
benchmark curated for BRTs. Specifically, AssertFlip achieves a fail-to-pass
success rate of 43.6% on the SWT-Bench-Verified subset.

</details>


### [14] [CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement Learning](https://arxiv.org/abs/2507.17548)
*Lingxiao Tang,He Ye,Zhongxin Liu,Xiaoxue Ren,Lingfeng Bao*

Main category: cs.SE

TL;DR: 论文提出了一种名为CodeReasoner的框架，通过改进数据集构建和两阶段训练过程，显著提升了大型语言模型在代码推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调方法在代码推理任务中表现有限且泛化能力不足，主要原因是训练数据质量低和微调方法难以教授通用推理技能。

Method: CodeReasoner框架包括构建专注于Python程序核心执行逻辑的数据集，通过指令调优注入执行相关知识，并使用GRPO强化学习进一步优化推理和泛化能力。

Result: 在三个广泛使用的代码推理基准测试中，CodeReasoner（7B模型）性能提升了27.1%至40.2%，且与GPT-4o在关键任务中表现相当；14B模型在所有基准上超越GPT-4o。

Conclusion: CodeReasoner通过两阶段训练和高质量数据集显著提升了代码推理能力，强调了推理链条的重要性。

Abstract: Code reasoning is a fundamental capability for large language models (LLMs)
in the code domain. It involves understanding and predicting a program's
execution behavior, such as determining the output for a given input or whether
a specific statement will be executed. This capability is essential for
downstream tasks like debugging, code generation, and program repair. Prior
approaches mainly rely on supervised fine-tuning to improve performance in code
reasoning tasks. However, they often show limited gains and fail to generalize
across diverse scenarios. We argue this is due to two core issues: the low
quality of training data and the limitations of supervised fine-tuning, which
struggles to teach general reasoning skills. To address these challenges, we
propose CodeReasoner, a framework that spans both dataset construction and a
two-stage training process. First, we introduce a method to construct datasets
that focus on the core execution logic of Python programs. Next, we apply
instruction tuning to inject execution-specific knowledge distilled from a
powerful teacher model. We then enhance reasoning and generalization through
GRPO reinforcement learning on top of the fine-tuned model. Experiments on
three widely-used code reasoning benchmarks show that CodeReasoner improves
performance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the
7B model matches GPT-4o on key tasks like input/output and coverage prediction.
When scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.
Ablation studies confirm the effectiveness of each training stage and highlight
the importance of reasoning chains.

</details>


### [15] [Contextual Code Retrieval for Commit Message Generation: A Preliminary Study](https://arxiv.org/abs/2507.17690)
*Bo Xiong,Linghao Zhang,Chong Wang,Peng Liang*

Main category: cs.SE

TL;DR: 提出了一种基于上下文代码检索的方法C3Gen，用于改进提交信息生成（CMG），通过检索并整合相关代码片段提供更丰富的上下文信息，从而生成更全面且信息丰富的提交信息。


<details>
  <summary>Details</summary>
Motivation: 现有的提交信息生成方法仅依赖代码差异，缺乏完整上下文，难以生成高质量的提交信息。

Method: 提出C3Gen方法，通过检索提交相关的代码片段并将其整合到模型输入中，提供更丰富的上下文信息。

Result: 实验表明，C3Gen能够利用额外信息生成更全面且实用的提交信息，并分析了相似性度量的可靠性问题。

Conclusion: C3Gen通过引入上下文代码提升了提交信息生成的实用性和信息量，为CMG提供了新的实证见解。

Abstract: A commit message describes the main code changes in a commit and plays a
crucial role in software maintenance. Existing commit message generation (CMG)
approaches typically frame it as a direct mapping which inputs a code diff and
produces a brief descriptive sentence as output. However, we argue that relying
solely on the code diff is insufficient, as raw code diff fails to capture the
full context needed for generating high-quality and informative commit
messages. In this paper, we propose a contextual code retrieval-based method
called C3Gen to enhance CMG by retrieving commit-relevant code snippets from
the repository and incorporating them into the model input to provide richer
contextual information at the repository scope. In the experiments, we
evaluated the effectiveness of C3Gen across various models using four objective
and three subjective metrics. Meanwhile, we design and conduct a human
evaluation to investigate how C3Gen-generated commit messages are perceived by
human developers. The results show that by incorporating contextual code into
the input, C3Gen enables models to effectively leverage additional information
to generate more comprehensive and informative commit messages with greater
practical value in real-world development scenarios. Further analysis
underscores concerns about the reliability of similaritybased metrics and
provides empirical insights for CMG.

</details>


### [16] [Educational Insights from Code: Mapping Learning Challenges in Object-Oriented Programming through Code-Based Evidence](https://arxiv.org/abs/2507.17743)
*Andre Menolli,Bruno Strik*

Main category: cs.SE

TL;DR: 该研究探讨了面向对象编程中代码问题与学习难点之间的关系，建立了代码异味和SOLID原则与学习困难的联系，并开发了一个概念模型。


<details>
  <summary>Details</summary>
Motivation: 尽管现有研究发现了面向对象编程中的代码问题，但少有研究探讨这些问题与学习困难的关系，因此本研究旨在填补这一空白。

Method: 通过定性分析和文献综述，识别学习难点类别，并与代码异味和SOLID原则建立联系，开发概念模型并请专家评估其适用性。

Result: 开发了一个将代码问题与学习难点关联的概念模型，专家评估证实了其在教育场景中的相关性和实用性。

Conclusion: 研究为教育者提供了将代码问题与学习困难关联的工具，有助于改进面向对象编程的教学方法。

Abstract: Object-Oriented programming is frequently challenging for undergraduate
Computer Science students, particularly in understanding abstract concepts such
as encapsulation, inheritance, and polymorphism. Although the literature
outlines various methods to identify potential design and coding issues in
object-oriented programming through source code analysis, such as code smells
and SOLID principles, few studies explore how these code-level issues relate to
learning difficulties in Object-Oriented Programming. In this study, we explore
the relationship of the code issue indicators with common challenges
encountered during the learning of object-oriented programming. Using
qualitative analysis, we identified the main categories of learning
difficulties and, through a literature review, established connections between
these difficulties, code smells, and violations of the SOLID principles. As a
result, we developed a conceptual map that links code-related issues to
specific learning challenges in Object-Oriented Programming. The model was then
evaluated by an expert who applied it in the analysis of the student code to
assess its relevance and applicability in educational contexts.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [Hiord: An Approach to the Specification and Verification of Higher-Order (C)LP Programs](https://arxiv.org/abs/2507.17233)
*Marco Ciccalè,Daniel Jurjo-Rivas,Jose F. Morales,Pedro López-García,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 提出了一种静态验证高阶(C)LP程序和高阶断言的新方法，基于抽象解释将谓词属性降为一阶属性。


<details>
  <summary>Details</summary>
Motivation: 目前对高阶程序的运行时验证已有研究，但编译时验证尚未充分探索。

Method: 利用谓词属性描述高阶参数，通过抽象顺序关系和抽象解释实现静态验证。

Result: 实现了原型并在Ciao系统中通过多个示例进行了评估。

Conclusion: 该方法具有通用性，适用于类似的高阶程序验证场景。

Abstract: Higher-order constructs enable more expressive and concise code by allowing
procedures to be parameterized by other procedures. Assertions allow expressing
partial program specifications, which can be verified either at compile time
(statically) or run time (dynamically). In higher-order programs, assertions
can also describe higher-order arguments. While in the context of (C)LP,
run-time verification of higher-order assertions has received some attention,
compile-time verification remains relatively unexplored. We propose a novel
approach for statically verifying higher-order (C)LP programs with higher-order
assertions. Although we use the Ciao assertion language for illustration, our
approach is quite general and we believe is applicable to similar contexts.
Higher-order arguments are described using predicate properties -- a special
kind of property which exploits the (Ciao) assertion language. We refine the
syntax and semantics of these properties and introduce an abstract criterion to
determine conformance to a predicate property at compile time, based on a
semantic order relation comparing the predicate property with the predicate
assertions. We then show how to handle these properties using an abstract
interpretation-based static analyzer for programs with first-order assertions
by reducing predicate properties to first-order properties. Finally, we report
on a prototype implementation and evaluate it through various examples within
the Ciao system.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [18] [LLM Meets the Sky: Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks](https://arxiv.org/abs/2507.17188)
*Lijie Zheng,Ji He,Shih Yu Chang,Yulong Shen,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出了一种在异构无人机网络中最大化保密率的物理层安全方法，结合SDR和LLM引导的强化学习优化无人机轨迹和预编码。


<details>
  <summary>Details</summary>
Motivation: 解决异构无人机网络中能量约束下的保密率最大化问题，弥补现有研究中无人机能力假设单一和能量-安全权衡被忽略的不足。

Method: 采用分层优化框架：内层用SDR和DC编程解决预编码问题；外层用LLM引导的多智能体强化学习优化轨迹。

Result: 仿真显示该方法在保密率和能效上优于现有基线，且在无人机群规模和随机性下具有鲁棒性。

Conclusion: 提出的框架有效结合了通信优化和无人机运动规划，为异构无人机网络的物理层安全提供了实用解决方案。

Abstract: This work tackles the physical layer security (PLS) problem of maximizing the
secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy
constraints. Unlike prior studies that assume uniform UAV capabilities or
overlook energy-security trade-offs, we consider a realistic scenario where
UAVs with diverse payloads and computation resources collaborate to serve
ground terminals in the presence of eavesdroppers. To manage the complex
coupling between UAV motion and communication, we propose a hierarchical
optimization framework. The inner layer uses a semidefinite relaxation
(SDR)-based S2DC algorithm combining penalty functions and difference-of-convex
(d.c.) programming to solve the secrecy precoding problem with fixed UAV
positions. The outer layer introduces a Large Language Model (LLM)-guided
heuristic multi-agent reinforcement learning approach (LLM-HeMARL) for
trajectory optimization. LLM-HeMARL efficiently incorporates expert heuristics
policy generated by the LLM, enabling UAVs to learn energy-aware,
security-driven trajectories without the inference overhead of real-time LLM
calls. The simulation results show that our method outperforms existing
baselines in secrecy rate and energy efficiency, with consistent robustness
across varying UAV swarm sizes and random seeds.

</details>


### [19] [Closed-Form and Boundary Expressions for Task-Success Probability in Status-Driven Systems](https://arxiv.org/abs/2507.17195)
*Jianpeng Qi,Chao Liu,Rui Wang,Junyu Dong,Yanwei Yu*

Main category: cs.NI

TL;DR: 论文提出了一个统一的解析框架，用于计算在计算优先网络系统中任务成功的概率，考虑了随机到达、有限服务器容量和双向链路延迟等因素。


<details>
  <summary>Details</summary>
Motivation: 在计算优先网络系统中，由于任务动态到达、计算资源有限且随机，及时有效地传播服务器状态至关重要。现有模型难以准确捕获任务成功的概率。

Method: 通过将AP转发规则抽象为单一概率，并通过拉普拉斯变换建模所有网络和等待延迟，提出了一个解析框架，得到了任务成功概率的闭式表达式及其上下界。

Result: 实验验证表明，理论预测和上下界能够一致地包围观测到的成功率。上下界与经验任务成功概率的误差分别在0.01和0.016以内。

Conclusion: 该框架仅需两个可互换的输入（转发概率和延迟变换），具有广泛的适用性和灵活性，能够适应不同的转发策略和延迟分布。

Abstract: Timely and efficient dissemination of server status is critical in
compute-first networking systems, where user tasks arrive dynamically and
computing resources are limited and stochastic. In such systems, the access
point plays a key role in forwarding tasks to a server based on its latest
received server status. However, modeling the task-success probability
suffering the factors of stochastic arrivals, limited server capacity, and
bidirectional link delays. Therefore, we introduce a unified analytical
framework that abstracts the AP forwarding rule as a single probability and
models all network and waiting delays via their Laplace transforms. This
approach yields a closed form expression for the end to end task success
probability, together with upper and lower bounds that capture Erlang loss
blocking, information staleness, and random uplink/downlink delays. We validate
our results through simulations across a wide range of parameters, showing that
theoretical predictions and bounds consistently enclose observed success rates.
Our framework requires only two interchangeable inputs (the forwarding
probability and the delay transforms), making it readily adaptable to
alternative forwarding policies and delay distributions. Experiments
demonstrate that our bounds are able to achieve accuracy within 0.01 (upper
bound) and 0.016 (lower bound) of the empirical task success probability.

</details>


### [20] [Custody Transfer and Compressed Status Reporting for Bundle Protocol Version 7](https://arxiv.org/abs/2507.17403)
*Alice Le Bihan,Felix Flentge,Juan A. Fraire*

Main category: cs.NI

TL;DR: 本文提出了一种新的BPv7保管转移方法，解决了间歇连接和长延迟带来的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着太空任务的增加，需要一种高效可靠的网络中心通信方法替代点对点通信。

Method: 介绍了BPv7的新保管转移流程，包括序列号策略、新的保管转移扩展块和压缩报告扩展块。

Result: 这些机制已在ESA BP实现中进行了原型设计，并在地球观测和月球通信模拟场景中进行了测试。

Conclusion: 为DTN可靠传输领域提供了新的解决方案，并展望了未来工作。

Abstract: As space missions increase, there is a growing need to replace point-to-point
communication with an efficient and reliable network-centric communication
approach. Disruption/Delay Tolerant Networking (DTN) with the Bundle Protocol
(BP) has been selected as an interoperable network protocol in the LunaNet
Interoperability Specification. It is also considered for future Earth
Observation and Mars communication scenarios. In a DTN, the "bundle" -- the
fundamental data unit of BP -- requires dedicated mechanisms to ensure
reliability due to the challenges posed by intermittent connectivity and long
delays. The previous version of BP, BPv6, contained a mechanism for reliable
transfer between "custodial nodes" called "custody transfer". However, this
approach has been removed from the core protocol specification for BPv7, which
requires a corresponding BP reliability extension to be defined separately.
This paper introduces a new custody transfer process for BPv7 (expected to be
published by CCSDS as an experimental specification in 2025). The core features
of this new custody transfer method for BPv7 are: (1) A strategy to efficiently
identify sets of bundles by sequence numbering (2) A new Custody Transfer
Extension Block and a corresponding administrative record, Compressed Custody
Signal, to efficiently report on the acceptance or rejection of custody using
sequence numbering (3) A new Compressed Reporting Extension Block requesting
reporting on bundle processing steps using a corresponding administrative
record with sequence numbering for efficiency. The paper will describe those
concepts and their design, specification, and implementation in detail. These
mechanisms have been prototyped in the ESA BP implementation and tested in
Earth Observation and Lunar communication simulation scenarios. The results
will be presented, as will an outlook on future work in the DTN reliable
transfer domain.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [21] [A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task](https://arxiv.org/abs/2507.17232)
*Mashiro Toyooka,Kiyoharu Aizawa,Yoko Yamakata*

Main category: cs.MM

TL;DR: 该论文研究了大型语言模型（LLMs）在烹饪食谱中对食材状态的理解能力，并提出了新的任务和数据集。


<details>
  <summary>Details</summary>
Motivation: 由于烹饪食谱中常省略食材的中间状态，LLMs难以准确跟踪和理解食谱内容。

Method: 作者构建了一个标注清晰的日本食谱数据集，并设计了三个新任务来评估LLMs对食材状态变化的识别能力。

Result: 实验表明，学习食材状态知识显著提升了LLMs对烹饪过程的理解能力，达到了与商业模型相当的性能。

Conclusion: 通过状态探测方法评估LLMs的世界理解能力，可以显著改进其在具体领域（如烹饪）中的表现。

Abstract: Large Language Models (LLMs) are trained on a vast amount of procedural
texts, but they do not directly observe real-world phenomena. In the context of
cooking recipes, this poses a challenge, as intermediate states of ingredients
are often omitted, making it difficult for models to track ingredient states
and understand recipes accurately. In this paper, we apply state probing, a
method for evaluating a language model's understanding of the world, to the
domain of cooking. We propose a new task and dataset for evaluating how well
LLMs can recognize intermediate ingredient states during cooking procedures. We
first construct a new Japanese recipe dataset with clear and accurate
annotations of ingredient state changes, collected from well-structured and
controlled recipe texts. Using this dataset, we design three novel tasks to
evaluate whether LLMs can track ingredient state transitions and identify
ingredients present at intermediate steps. Our experiments with widely used
LLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state
knowledge improves their understanding of cooking processes, achieving
performance comparable to commercial LLMs.

</details>


### [22] [QuMAB: Query-based Multi-annotator Behavior Pattern Learning](https://arxiv.org/abs/2507.17653)
*Liyun Zhang,Zheng Lian,Hong Liu,Takanori Takebe,Yuta Nakashima*

Main category: cs.MM

TL;DR: 论文提出了从传统样本级标注聚合转向标注者行为建模的新范式，通过捕捉标注者行为模式来提升标注效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统方法将标注差异视为噪声，忽视了主观任务中标注者行为的多样性，且稀疏标注覆盖使得统计聚合不可靠。

Method: 提出QuMATL方法，通过轻量查询建模单个标注者行为，并利用标注者间相关性作为隐式正则化防止过拟合，同时提供可视化分析。

Result: 贡献了两个大规模密集标注数据集STREET和AMER，验证了方法的有效性和可解释性。

Conclusion: 标注者行为建模能更好地利用标注差异信息，降低标注成本并提升模型泛化能力。

Abstract: Multi-annotator learning traditionally aggregates diverse annotations to
approximate a single ground truth, treating disagreements as noise. However,
this paradigm faces fundamental challenges: subjective tasks often lack
absolute ground truth, and sparse annotation coverage makes aggregation
statistically unreliable. We introduce a paradigm shift from sample-wise
aggregation to annotator-wise behavior modeling. By treating annotator
disagreements as valuable information rather than noise, modeling
annotator-specific behavior patterns can reconstruct unlabeled data to reduce
annotation cost, enhance aggregation reliability, and explain annotator
decision behavior. To this end, we propose QuMATL (Query-based Multi-Annotator
Behavior Pattern Learning), which uses light-weight queries to model individual
annotators while capturing inter-annotator correlations as implicit
regularization, preventing overfitting to sparse individual data while
maintaining individualization and improving generalization, with a
visualization of annotator focus regions offering an explainable analysis of
behavior understanding. We contribute two large-scale datasets with dense
per-annotator labels: STREET (4,300 labels/annotator) and AMER (average 3,118
labels/annotator), the first multimodal multi-annotator dataset.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Integrating Belief Domains into Probabilistic Logic Programs](https://arxiv.org/abs/2507.17291)
*Damiano Azzolini,Fabrizio Riguzzi,Theresa Swift*

Main category: cs.LO

TL;DR: 本文提出了一种基于区间概率的容量逻辑程序扩展，以解决传统分布语义中难以表达认知不确定性的问题，适用于实际应用。


<details>
  <summary>Details</summary>
Motivation: 传统概率逻辑编程使用点概率，难以表达认知不确定性（如计算机视觉模型中的分类层次）。本文旨在通过引入信念函数来解决这一问题。

Method: 扩展了分布语义，引入了基于区间概率的容量逻辑程序，并分析了其适用性。

Result: 新框架能够有效处理认知不确定性，并通过区间概率提供更灵活的表达能力。

Conclusion: 容量逻辑程序扩展了传统分布语义，为处理认知不确定性提供了一种实用的方法。

Abstract: Probabilistic Logic Programming (PLP) under the Distribution Semantics is a
leading approach to practical reasoning under uncertainty. An advantage of the
Distribution Semantics is its suitability for implementation as a Prolog or
Python library, available through two well-maintained implementations, namely
ProbLog and cplint/PITA. However, current formulations of the Distribution
Semantics use point-probabilities, making it difficult to express epistemic
uncertainty, such as arises from, for example, hierarchical classifications
from computer vision models. Belief functions generalize probability measures
as non-additive capacities, and address epistemic uncertainty via interval
probabilities. This paper introduces interval-based Capacity Logic Programs
based on an extension of the Distribution Semantics to include belief
functions, and describes properties of the new framework that make it amenable
to practical applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [24] [Assessing Medical Training Skills via Eye and Head Movements](https://arxiv.org/abs/2507.16819)
*Kayhan Latifzadeh,Luis A. Leiva,Klen Čopič Pucihar,Matjaž Kljun,Iztok Devetak,Lili Steblovnik*

Main category: cs.HC

TL;DR: 通过眼部和头部运动分析临床技能发展，研究显示眼动和头动数据能有效区分训练和未训练的从业者，为计算模型的技能评估奠定基础。


<details>
  <summary>Details</summary>
Motivation: 探索眼部和头部运动在临床技能发展中的作用，为技能评估提供客观数据支持。

Method: 24名从业者参与模拟婴儿分娩训练，测量瞳孔反应率、注视时间和角速度等指标。

Result: 头部特征F1分数0.85，AUC 0.86；瞳孔特征F1分数0.77，AUC 0.85。

Conclusion: 眼动追踪眼镜可作为传统评估方法的补充，支持临床技能的隐性评估和训练。

Abstract: We examined eye and head movements to gain insights into skill development in
clinical settings. A total of 24 practitioners participated in simulated baby
delivery training sessions. We calculated key metrics, including pupillary
response rate, fixation duration, or angular velocity. Our findings indicate
that eye and head tracking can effectively differentiate between trained and
untrained practitioners, particularly during labor tasks. For example,
head-related features achieved an F1 score of 0.85 and AUC of 0.86, whereas
pupil-related features achieved F1 score of 0.77 and AUC of 0.85. The results
lay the groundwork for computational models that support implicit skill
assessment and training in clinical settings by using commodity eye-tracking
glasses as a complementary device to more traditional evaluation methods such
as subjective scores.

</details>


### [25] [Evaluation of the effects of frame time variation on VR task performance](https://arxiv.org/abs/2507.17139)
*Benjamin Watson,Victoria Spaulding,Neff Walker,William Ribarsky*

Main category: cs.HC

TL;DR: 研究了帧时间变化（包括平均帧时间的偏差和波动周期）对虚拟环境中任务性能的影响。结果显示，在大多数应用可接受的帧时间范围内，较大的振幅偏差和较宽的周期范围不会显著影响任务性能，但在沉浸式VR的最低帧时间下，帧时间变化会对闭环任务性能产生显著影响。


<details>
  <summary>Details</summary>
Motivation: 探索帧时间变化对虚拟环境中任务性能的影响，为虚拟环境和沉浸式应用的设计者提供参考，帮助他们应对因环境复杂性导致的帧时间波动问题。

Method: 选择了典型或未来可能常见的开环和闭环任务，通过实验分析帧时间变化（偏差和波动周期）对任务性能的影响。

Result: 在可接受的帧时间范围内，帧时间变化不会显著影响任务性能；但在沉浸式VR的最低帧时间下，帧时间变化会显著影响闭环任务性能。

Conclusion: 研究结果为虚拟环境设计者提供了重要参考，帮助他们在设计时更好地控制帧时间波动，尤其是在沉浸式VR应用中。

Abstract: We present a first study of the effects of frame time variations, in both
deviation around mean frame times and period of fluctuation, on task
performance in a virtual environment (VE). Chosen are open and closed loop
tasks that are typical for current applications or likely to be prominent in
future ones. The results show that at frame times in the range deemed
acceptable for many applications, fairly large deviations in amplitude over a
fairly wide range of periods do not significantly affect task performance.
However, at a frame time often considered a minimum for immersive VR, frame
time variations do produce significant effects on closed loop task performance.
The results will be of use to designers of VEs and immersive applications, who
often must control frame time variations due to large fluctuations of
complexity (graphical and otherwise) in the VE.

</details>


### [26] [Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances](https://arxiv.org/abs/2507.17024)
*Chase Stokes,Kylie Lin,Cindy Xiong Bearfield*

Main category: cs.HC

TL;DR: 该论文探讨了四种可视化研究方法（自由回答、图表排序、结论排序和显著性评分）在评估图表设计对读者解读的影响时的效果，发现组合方法更有效，并测试了GPT-4o作为人类代理的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了解决通过自由回答分析图表解读的规模化难题，研究寻找替代方法来高效评估图表设计对读者结论的影响。

Method: 测试了四种研究方法（自由回答、图表排序、结论排序和显著性评分），并用GPT-4o作为人类代理进行对比分析。

Result: 排序和评分方法的组合可作为自由回答的有效替代；GPT-4o在显著性评分中表现最佳，但其他方法受限。

Conclusion: 研究强调需根据目的选择或组合方法，并明确GPT-4o等工具的适用性与局限性。

Abstract: A growing body of work on visualization affordances highlights how specific
design choices shape reader takeaways from information visualizations. However,
mapping the relationship between design choices and reader conclusions often
requires labor-intensive crowdsourced studies, generating large corpora of
free-response text for analysis. To address this challenge, we explored
alternative scalable research methodologies to assess chart affordances. We
test four elicitation methods from human-subject studies: free response,
visualization ranking, conclusion ranking, and salience rating, and compare
their effectiveness in eliciting reader interpretations of line charts, dot
plots, and heatmaps. Overall, we find that while no method fully replicates
affordances observed in free-response conclusions, combinations of ranking and
rating methods can serve as an effective proxy at a broad scale. The two
ranking methodologies were influenced by participant bias towards certain chart
types and the comparison of suggested conclusions. Rating conclusion salience
could not capture the specific variations between chart types observed in the
other methods. To supplement this work, we present a case study with GPT-4o,
exploring the use of large language models (LLMs) to elicit human-like chart
interpretations. This aligns with recent academic interest in leveraging LLMs
as proxies for human participants to improve data collection and analysis
efficiency. GPT-4o performed best as a human proxy for the salience rating
methodology but suffered from severe constraints in other areas. Overall, the
discrepancies in affordances we found between various elicitation
methodologies, including GPT-4o, highlight the importance of intentionally
selecting and combining methods and evaluating trade-offs.

</details>


### [27] [HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery](https://arxiv.org/abs/2507.17209)
*Haoran Jiang,Shaohan Shi,Yunjie Yao,Chang Jiang,Quan Li*

Main category: cs.HC

TL;DR: 提出HypoChainer框架，结合人类专家、LLM和知识图谱，优化生物医学领域的假设生成与验证。


<details>
  <summary>Details</summary>
Motivation: 传统研究受限于认知和成本，而现有AI工具有可靠性问题，需更高效、可解释的方法。

Method: 分三阶段：探索与情境化、假设链形成、验证优先级排序，融合LLM、知识图谱和交互式可视化。

Result: 案例研究和专家访谈证明其有效，支持可解释、可扩展的科学发现。

Conclusion: HypoChainer解决了现有方法的局限性，提升了科学发现的效率与可靠性。

Abstract: Modern scientific discovery faces growing challenges in integrating vast and
heterogeneous knowledge critical to breakthroughs in biomedicine and drug
development. Traditional hypothesis-driven research, though effective, is
constrained by human cognitive limits, the complexity of biological systems,
and the high cost of trial-and-error experimentation. Deep learning models,
especially graph neural networks (GNNs), have accelerated prediction
generation, but the sheer volume of outputs makes manual selection for
validation unscalable. Large language models (LLMs) offer promise in filtering
and hypothesis generation, yet suffer from hallucinations and lack grounding in
structured knowledge, limiting their reliability. To address these issues, we
propose HypoChainer, a collaborative visualization framework that integrates
human expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance
hypothesis generation and validation. HypoChainer operates in three stages:
First, exploration and contextualization -- experts use retrieval-augmented
LLMs (RAGs) and dimensionality reduction to navigate large-scale GNN
predictions, assisted by interactive explanations. Second, hypothesis chain
formation -- experts iteratively examine KG relationships around predictions
and semantically linked entities, refining hypotheses with LLM and KG
suggestions. Third, validation prioritization -- refined hypotheses are
filtered based on KG-supported evidence to identify high-priority candidates
for experimentation, with visual analytics further strengthening weak links in
reasoning. We demonstrate HypoChainer's effectiveness through case studies in
two domains and expert interviews, highlighting its potential to support
interpretable, scalable, and knowledge-grounded scientific discovery.

</details>


### [28] [OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena](https://arxiv.org/abs/2507.17218)
*Yang Ouyang,Yuchen Wu,Xiyuan Wang,Laixin Xie,Weicong Cheng,Jianping Gan,Quan Li,Xiaojuan Ma*

Main category: cs.HC

TL;DR: OceanVive是一个沉浸式交互可视化系统，用于将复杂的海洋数据转化为可导航的空间叙事，提升海洋科学传播效果。


<details>
  <summary>Details</summary>
Motivation: 传统静态可视化和文本报告难以有效传达海洋变化的复杂性，亟需一种更直观的传播方式。

Method: 开发OceanVive系统，结合自适应视觉编码、上下文叙事和直观导航，通过桌面平板管理大屏幕内容。

Result: 专家访谈验证了该系统在提升科学传播和公众理解方面的潜力。

Conclusion: OceanVive通过交互式可视化有效解决了海洋科学传播复杂性的问题。

Abstract: Communicating the complexity of oceanic phenomena-such as hypoxia and
acidification-poses a persistent challenge for marine science. Despite advances
in sensing technologies and computational models, conventional formats like
static visualizations and text-based reports often fall short in conveying the
dynamics of ocean changes. To address this gap, we present OceanVive, an
immersive and interactive visualization system that transforms complex ocean
datasets into navigable spatial narratives. OceanVive incorporates an
exploratory panel on a table-sized tablet for managing immersive content on a
large screen and integrates adaptive visual encodings, contextual storytelling,
and intuitive navigation pathways to support effective communication. We
validate the system through expert interviews, demonstrating its potential to
enhance science communication and promote deeper public understanding.

</details>


### [29] [A "watch your replay videos" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection](https://arxiv.org/abs/2507.17226)
*Sarah "Magz" Fernandez,Greg L Nelson*

Main category: cs.HC

TL;DR: 该研究探讨了通过视频反思作业帮助学生理解生成式AI如何改变编程过程，发现学生不仅反思了AI的使用，还提升了编程的元认知技能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在改变计算机教育，但现有干预措施多聚焦于AI工具的使用，而非其对编程过程的影响。本研究旨在填补这一空白。

Method: 在软件工程课程中，学生录制两次编程过程（未使用AI和使用AI），并通过DEAL框架分析视频，回答反思问题。

Result: 学生通过反思认识到规划、调试和求助行为的改进，并学会批判性使用AI，同时意外引发了对非AI编程的反思。

Conclusion: 结构化视频反思能有效培养编程的元认知技能，适用于AI时代的学习和终身学习。

Abstract: Generative AI is disrupting computing education. Most interventions focus on
teaching GenAI use rather than helping students understand how AI changes their
programming process. We designed and deployed a novel comparative video
reflection assignment adapting the Describe, Examine, then Articulate Learning
(DEAL) framework. In an introductory software engineering course, students
recorded themselves programming during their team project two times: first
without, then with using generative AI. Students then analyzed their own videos
using a scaffolded set of reflection questions, including on their programming
process and human, internet, and AI help-seeking. We conducted a qualitative
thematic analysis of the reflections, finding students developed insights about
planning, debugging, and help-seeking behaviors that transcended AI use.
Students reported learning to slow down and understand before writing or
generating code, recognized patterns in their problem-solving approaches, and
articulated specific process improvements. Students also learned and reflected
on AI limits and downsides, and strategies to use AI more critically, including
better prompting but also to benefit their learning instead of just completing
tasks. Unexpectedly, the comparative reflection also scaffolded reflection on
programming not involving AI use, and even led to students spontaneously
setting future goals to adopt video and other regular reflection. This work
demonstrates structured reflection on programming session videos can develop
metacognitive skills essential for programming with and without generative AI
and also lifelong learning in our evolving field.

</details>


### [30] [Designing for Learning with Generative AI is a Wicked Problem: An Illustrative Longitudinal Qualitative Case Series](https://arxiv.org/abs/2507.17230)
*Clara Scalzer,Saurav Pokhrel,Sara Hunt,Greg L Nelson*

Main category: cs.HC

TL;DR: 学生认为学习有意义且与未来职业相关时会更愿意继续学习。计算教育者面临如何将生成式AI（GenAI）融入课程以支持学习、动机、伦理和职业发展的复杂挑战。研究发现，这是一个“棘手”问题，解决一个目标可能损害其他目标。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决如何将GenAI融入教育以平衡学习、伦理、动机和职业发展的挑战。

Method: 通过对一门GenAI融入创意媒体课程的学生进行纵向定性研究，分析两名学生的经历。

Result: 研究发现，提升GenAI技能可能降低伦理意识，而提高伦理意识可能阻碍技能学习，且GenAI熟练度并未增强职业信心。

Conclusion: 支持学生在GenAI时代的发展是一个多维度的“棘手”问题，需要综合设计而非单一优化。

Abstract: Students continue their education when they feel their learning is meaningful
and relevant for their future careers. Computing educators now face the
challenge of preparing students for careers increasingly shaped by generative
AI (GenAI) with the goals of supporting their learning, motivation, ethics, and
career development. Our longitudinal qualitative study of students in a
GenAI-integrated creative media course shows how this is a "wicked" problem:
progress on one goal can then impede progress on other goals. Students
developed concerning patterns despite extensive instruction in critical and
ethical GenAI use including prompt engineering, ethics and bias, and industry
panels on GenAI's career impact. We present an analysis of two students'
experiences to showcase this complexity. Increasing GenAI use skills can lower
ethics; for example, Pat started from purposefully avoiding GenAI use, to
dependency. He described himself as a "notorious cheater" who now uses GenAi to
"get all the right answers" while acknowledging he's learning less. Increasing
ethical awareness can lower the learning of GenAI use skills; for example,
Jay's newfound environmental concerns led to self-imposed usage limits that
impeded skill development, and new serious fears that GenAI would eliminate
creative careers they had been passionate about. Increased GenAI proficiency, a
potential career skill, did not improve their career confidence. These findings
suggest that supporting student development in the GenAI era is a "wicked"
problem requiring multi-dimensional evaluation and design, rather than
optimizing learning, GenAI skills, ethics, or career motivation individually.

</details>


### [31] [Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations](https://arxiv.org/abs/2507.17248)
*Xiaoan Liu,Difan Jia,Xianhao Carton Liu,Mar Gonzalez-Franco,Chen Zhu-Tian*

Main category: cs.HC

TL;DR: 通过引入代理（抽象的虚拟对象代表现实物体）来解耦混合现实中的交互问题，Reality Proxy系统实现了更灵活的选择和操作。


<details>
  <summary>Details</summary>
Motivation: 解决混合现实中由于物体拥挤、距离远或被遮挡导致的选择和操作困难。

Method: 提出Reality Proxy系统，将交互从物理对象转移到其代理上，并通过AI增强代理的语义属性和空间关系。

Result: 系统支持多种新颖交互（如快速浏览、属性筛选、嵌套导航和多对象选择），并在多个场景中验证了其实用性。

Conclusion: 代理抽象为未来混合现实系统提供了一种强大且通用的交互范式。

Abstract: Interacting with real-world objects in Mixed Reality (MR) often proves
difficult when they are crowded, distant, or partially occluded, hindering
straightforward selection and manipulation. We observe that these difficulties
stem from performing interaction directly on physical objects, where input is
tightly coupled to their physical constraints. Our key insight is to decouple
interaction from these constraints by introducing proxies-abstract
representations of real-world objects. We embody this concept in Reality Proxy,
a system that seamlessly shifts interaction targets from physical objects to
their proxies during selection. Beyond facilitating basic selection, Reality
Proxy uses AI to enrich proxies with semantic attributes and hierarchical
spatial relationships of their corresponding physical objects, enabling novel
and previously cumbersome interactions in MR - such as skimming,
attribute-based filtering, navigating nested groups, and complex multi object
selections - all without requiring new gestures or menu systems. We demonstrate
Reality Proxy's versatility across diverse scenarios, including office
information retrieval, large-scale spatial navigation, and multi-drone control.
An expert evaluation suggests the system's utility and usability, suggesting
that proxy-based abstractions offer a powerful and generalizable interaction
paradigm for future MR systems.

</details>


### [32] [High-Density EEG Enables the Fastest Visual Brain-Computer Interfaces](https://arxiv.org/abs/2507.17242)
*Gege Ming,Weihua Pei,Sen Tian,Xiaogang Chen,Xiaorong Gao,Yijun Wang*

Main category: cs.HC

TL;DR: 该研究提出了一种频率-相位-空间融合编码方法，结合256通道高密度脑电图记录，显著提升了脑机接口（BCI）系统的信息传输速率（ITR）。


<details>
  <summary>Details</summary>
Motivation: 当前视觉BCI系统的信息传输速率不足，限制了其实际应用。研究旨在通过挖掘视觉感知中的空间信息，提升系统性能。

Method: 采用频率-相位-空间融合编码方法，结合高密度（256通道）脑电图记录，开发高速BCI系统。

Result: 在不同配置下，理论ITR提升最高达195.56%，在线系统平均实际ITR达到472.7 bpm。

Conclusion: 研究证明了高密度脑电图在解码视觉刺激时空信息中的关键作用和巨大潜力。

Abstract: Brain-computer interface (BCI) technology establishes a direct communication
pathway between the brain and external devices. Current visual BCI systems
suffer from insufficient information transfer rates (ITRs) for practical use.
Spatial information, a critical component of visual perception, remains
underexploited in existing systems because the limited spatial resolution of
recording methods hinders the capture of the rich spatiotemporal dynamics of
brain signals. This study proposed a frequency-phase-space fusion encoding
method, integrated with 256-channel high-density electroencephalogram (EEG)
recordings, to develop high-speed BCI systems. In the classical frequency-phase
encoding 40-target BCI paradigm, the 256-66, 128-32, and 64-21 electrode
configurations brought theoretical ITR increases of 83.66%, 79.99%, and 55.50%
over the traditional 64-9 setup. In the proposed frequency-phase-space encoding
200-target BCI paradigm, these increases climbed to 195.56%, 153.08%, and
103.07%. The online BCI system achieved an average actual ITR of 472.7 bpm.
This study demonstrates the essential role and immense potential of
high-density EEG in decoding the spatiotemporal information of visual stimuli.

</details>


### [33] [EventLines: Time Compression for Discrete Event Timelines](https://arxiv.org/abs/2507.17320)
*Yuet Ling Wong,Niklas Elmqvist*

Main category: cs.HC

TL;DR: 论文提出EventLines技术，通过动态调整时间尺度以更高效地展示离散事件序列中突发性行为，解决了标准时间轴在显示这类数据时的不足。


<details>
  <summary>Details</summary>
Motivation: 离散事件序列（如出版物时间、项目里程碑、患者治疗中的用药记录）常表现出突发性行为，而传统线性时间轴无法有效展示这类数据，造成视觉混乱和空间浪费。

Method: 提出EventLines技术，动态调整时间尺度以匹配事件分布，并通过时间轴的视觉表现传达变化的尺度。采用众包图形感知研究评估不同时间尺度表示对时间感知的影响。

Result: 研究通过图形感知实验验证了EventLines在展示突发性事件序列时的有效性，解决了传统方法的局限性。

Conclusion: EventLines通过非线性的时间尺度调整，优化了突发性事件序列的展示效果，为类似数据的可视化提供了新方法。

Abstract: Discrete event sequences serve as models for numerous real-world datasets,
including publications over time, project milestones, and medication dosing
during patient treatments. These event sequences typically exhibit bursty
behavior, where events cluster together in rapid succession, interspersed with
periods of inactivity. Standard timeline charts with linear time axes fail to
adequately represent such data, resulting in cluttered regions during event
bursts while leaving other areas unutilized. We introduce EventLines, a novel
technique that dynamically adjusts the time scale to match the underlying event
distribution, enabling more efficient use of screen space. To address the
challenges of non-linear time scaling, EventLines employs the time axis's
visual representation itself to communicate the varying scale. We present
findings from a crowdsourced graphical perception study that examines how
different time scale representations influence temporal perception.

</details>


### [34] [Layered Interactions: Exploring Non-Intrusive Digital Craftsmanship Design Through Lacquer Art Interfaces](https://arxiv.org/abs/2507.17430)
*Yan Dong,Hanjie Yu,Yanran Chen,Zipeng Zhang,Qiong Wu*

Main category: cs.HC

TL;DR: 研究提出了一种名为'分层交互'的设计方法，将人机交互技术与传统漆器工艺结合，增强了传统工艺在现代数字背景下的适应性和实用性。


<details>
  <summary>Details</summary>
Motivation: 整合技术与工艺特色是数字工艺领域的关键问题，旨在提升传统工艺的现代实用性。

Method: 通过利用漆器的多层结构和材料特性，嵌入交互电路与可编程硬件，开发了漆器工具包，并进行用户实验和半结构化访谈。

Result: 研究表明，该方法不仅使传统工匠更容易接触技术，还提升了交互界面的物质性和情感品质，促进了工匠与技术人员的合作。

Conclusion: 研究为人机交互领域引入了跨学科视角，拓宽了交互界面的材料和设计可能性。

Abstract: Integrating technology with the distinctive characteristics of craftsmanship
has become a key issue in the field of digital craftsmanship. This paper
introduces Layered Interactions, a design approach that seamlessly merges
Human-Computer Interaction (HCI) technologies with traditional lacquerware
craftsmanship. By leveraging the multi-layer structure and material properties
of lacquerware, we embed interactive circuits and integrate programmable
hardware within the layers, creating tangible interfaces that support diverse
interactions. This method enhances the adaptability and practicality of
traditional crafts in modern digital contexts. Through the development of a
lacquerware toolkit, along with user experiments and semi-structured
interviews, we demonstrate that this approach not only makes technology more
accessible to traditional artisans but also enhances the materiality and
emotional qualities of interactive interfaces. Additionally, it fosters mutual
learning and collaboration between artisans and technologists. Our research
introduces a cross-disciplinary perspective to the HCI community, broadening
the material and design possibilities for interactive interfaces.

</details>


### [35] [SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2507.17524)
*Jiahao Tang,Youjun Li,Xiangting Fan,Yangxuan Zheng,Siyuan Lu,Xueping Li,Peng Fang,Chenxi Li,Zi-Gang Huang*

Main category: cs.HC

TL;DR: 提出一种无监督域适应网络SDC-Net，通过数据增强、动态分布对齐和双域相似性一致性学习，解决跨被试EEG情感识别的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中跨被试差异和标注数据不足的问题。

Method: 采用Same-Subject Same-Trial Mixup数据增强、RKHS中的动态分布对齐及伪标签策略、双域相似性一致性学习。

Result: 在SEED、SEED-IV和Faced数据集上表现优异，优于现有无监督域适应方法。

Conclusion: SDC-Net显著提升情感解码的准确性和泛化能力，为个性化aBCIs应用奠定基础。

Abstract: Electroencephalography(EEG) based emotion recognition holds great promise for
affective brain-computer interfaces (aBCIs), yet practical deployment remains
challenging due to substantial inter-subject variability and the lack of
labeled data in target domains. To overcome these limitations, we present a
novel unsupervised Semantic-Dynamic Consistency domain adaptation network for
fully label-free cross-subject EEG emotion recognition. First, we introduce a
Same-Subject Same-Trial Mixup strategy that generates augmented samples via
intra-trial interpolation, enhancing data diversity while explicitly preserving
individual identity to mitigate label ambiguity. Second, we construct a dynamic
distribution alignment module in reproducing kernel Hilbert space (RKHS),
jointly aligning marginal and conditional distributions through multi-objective
kernel mean embedding, and leveraging a confidence-aware pseudo-labeling
strategy to ensure stable adaptation. Third, we propose a dual-domain
similarity consistency learning mechanism that enforces cross-domain structural
constraints based on latent pairwise similarities, enabling semantic boundary
learning without relying on temporal synchronization or label priors. To
validate the effectiveness and robustness of the proposed SDC-Net, extensive
experiments are conducted on three widely used EEG benchmark datasets: SEED,
SEED-IV, and Faced. Comparative results against existing unsupervised domain
adaptation methods demonstrate that SDC-Net achieves state-of-the-art
performance in emotion recognition under both cross-subject and cross-session
conditions. This advancement significantly improves the accuracy and
generalization capability of emotion decoding, and lays a solid foundation for
real-world applications of personalized affective brain-computer interfaces
(aBCIs). The source code will be released at
https://github.com/XuanSuTrum/SDC-Net.

</details>


### [36] [Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams](https://arxiv.org/abs/2507.17543)
*Xue Wen Tan,Kenneth See,Stanley Kok*

Main category: cs.HC

TL;DR: 该论文提出了ASR框架，利用生成式AI方法帮助用户主动识别和理解即时通讯平台中的诈骗信息，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着诈骗信息的快速增长，用户在安全和财务方面面临日益严峻的挑战，亟需一种能够主动识别和理解诈骗的工具。

Method: ASR框架基于大型语言模型，预测诈骗者回复、生成逼真的诈骗对话，并提供实时解释性支持。同时，论文开发了针对诈骗对话的领域专用模型ScamGPT-J。

Result: 实验表明，ASR框架显著提高了诈骗检测能力，尤其是在工作诈骗等复杂场景中，并揭示了用户对AI生成支持的反感与高风险用户群体的矛盾。

Conclusion: 研究强调了以用户为中心的设计在AI驱动的诈骗预防中的重要性，为可解释、以人为本的AI系统提供了理论和实践基础。

Abstract: The rapid growth of messaging scams creates an escalating challenge for user
security and financial safety. In this paper, we present the Anticipate,
Simulate, Reason (ASR) framework, a generative AI method that enables users to
proactively identify and comprehend scams within instant messaging platforms.
Using large language models, ASR predicts scammer responses, creates realistic
scam conversations, and delivers real-time, interpretable support to end-users.
We develop ScamGPT-J, a domain-specific language model fine-tuned on a new,
high-quality dataset of scam conversations covering multiple scam types.
Thorough experimental evaluation shows that the ASR framework substantially
enhances scam detection, particularly in challenging contexts such as job
scams, and uncovers important demographic patterns in user vulnerability and
perceptions of AI-generated assistance. Our findings reveal a contradiction
where those most at risk are often least receptive to AI support, emphasizing
the importance of user-centered design in AI-driven fraud prevention. This work
advances both the practical and theoretical foundations for interpretable,
human-centered AI systems in combating evolving digital threats.

</details>


### [37] [Explainable AI for Collaborative Assessment of 2D/3D Registration Quality](https://arxiv.org/abs/2507.17597)
*Sue Min Cho,Alexander Do,Russell H. Taylor,Mathias Unberath*

Main category: cs.HC

TL;DR: 摘要提出了一种基于AI的2D/3D配准质量验证框架，结合可解释性特征，旨在提升人类操作员的决策能力，并通过实验比较了AI、人类以及两者协作的效果。


<details>
  <summary>Details</summary>
Motivation: 手术数字化转型中，2D/3D配准算法的偶尔错误可能引发严重后果，现有可视化方法不足以保证质量，需要更可靠的验证手段。

Method: 提出首个针对2D/3D配准质量验证的AI框架，结合可解释性特征（XAI），并通过算法和人为中心评估比较了四种条件（AI-only、human-only、human-AI、human-XAI）。

Result: 可解释性特征虽小幅提升用户信任和纠错意愿，但未超越单独AI的整体性能。

Conclusion: 未来工作可通过优化算法设计和人机协作，进一步提升2D/3D配准质量验证的鲁棒性。

Abstract: As surgery embraces digital transformation--integrating sophisticated
imaging, advanced algorithms, and robotics to support and automate complex
sub-tasks--human judgment of system correctness remains a vital safeguard for
patient safety. This shift introduces new "operator-type" roles tasked with
verifying complex algorithmic outputs, particularly at critical junctures of
the procedure, such as the intermediary check before drilling or implant
placement. A prime example is 2D/3D registration, a key enabler of image-based
surgical navigation that aligns intraoperative 2D images with preoperative 3D
data. Although registration algorithms have advanced significantly, they
occasionally yield inaccurate results. Because even small misalignments can
lead to revision surgery or irreversible surgical errors, there is a critical
need for robust quality assurance. Current visualization-based strategies alone
have been found insufficient to enable humans to reliably detect 2D/3D
registration misalignments. In response, we propose the first artificial
intelligence (AI) framework trained specifically for 2D/3D registration quality
verification, augmented by explainability features that clarify the model's
decision-making. Our explainable AI (XAI) approach aims to enhance informed
decision-making for human operators by providing a second opinion together with
a rationale behind it. Through algorithm-centric and human-centered
evaluations, we systematically compare four conditions: AI-only, human-only,
human-AI, and human-XAI. Our findings reveal that while explainability features
modestly improve user trust and willingness to override AI errors, they do not
exceed the standalone AI in aggregate performance. Nevertheless, future work
extending both the algorithmic design and the human-XAI collaboration elements
holds promise for more robust quality assurance of 2D/3D registration.

</details>


### [38] [Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills](https://arxiv.org/abs/2507.17688)
*Mohammad Nur Hossain Khan,David creswell,Jordan Albert,Patrick O'Connell,Shawn Fallon,Mathew Polowitz,Xuhai "orson" Xu,Bashima islam*

Main category: cs.HC

TL;DR: 该论文探讨了通过呼吸生物信号反馈和正念技能提升系统可用性的方法，开发了一种基于智能手机加速度计的无穿戴设备呼吸追踪算法，并提出了首个定量框架来评估正念技能。


<details>
  <summary>Details</summary>
Motivation: 智能手机正念应用普及，但长期用户参与度低，研究旨在通过呼吸反馈和技能估计提升系统效果。

Method: 开发加速度计呼吸追踪算法和正念技能评估框架，通过261次测试验证，并与标准应用对比用户研究。

Result: 呼吸追踪MAE为1.6次/分钟，技能评估F1分数80-84%，呼吸反馈显著提升系统可用性。

Conclusion: 智能手机传感器可用于增强数字正念训练，提供实用工具。

Abstract: Mindfulness training is widely recognized for its benefits in reducing
depression, anxiety, and loneliness. With the rise of smartphone-based
mindfulness apps, digital meditation has become more accessible, but sustaining
long-term user engagement remains a challenge. This paper explores whether
respiration biosignal feedback and mindfulness skill estimation enhance system
usability and skill development. We develop a smartphone's accelerometer-based
respiration tracking algorithm, eliminating the need for additional wearables.
Unlike existing methods, our approach accurately captures slow breathing
patterns typical of mindfulness meditation. Additionally, we introduce the
first quantitative framework to estimate mindfulness skills-concentration,
sensory clarity, and equanimity-based on accelerometer-derived respiration
data. We develop and test our algorithms on 261 mindfulness sessions in both
controlled and real-world settings. A user study comparing an experimental
group receiving biosignal feedback with a control group using a standard app
shows that respiration feedback enhances system usability. Our respiration
tracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute,
closely aligning with ground truth data, while our mindfulness skill estimation
attains F1 scores of 80-84% in tracking skill progression. By integrating
respiration tracking and mindfulness estimation into a commercial app, we
demonstrate the potential of smartphone sensors to enhance digital mindfulness
training.

</details>


### [39] [DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models](https://arxiv.org/abs/2507.17734)
*Liwenhan Xie,Yanna Lin,Can Liu,Huamin Qu,Xinhuan Shu*

Main category: cs.HC

TL;DR: DataWink是一个通过示例驱动的系统，帮助非专业用户创建美观的数据可视化图表。


<details>
  <summary>Details</summary>
Motivation: 解决非设计或技术用户创建高质量可视化的困难。

Method: 结合大型多模态模型提取SVG示例的数据编码，提供对话式交互和动态控件。

Result: 用户研究表明系统易学且有效。

Conclusion: 示例驱动方法有助于普及可视化创作。

Abstract: Creating aesthetically pleasing data visualizations remains challenging for
users without design expertise or familiarity with visualization tools. To
address this gap, we present DataWink, a system that enables users to create
custom visualizations by adapting high-quality examples. Our approach combines
large multimodal models (LMMs) to extract data encoding from existing SVG-based
visualization examples, featuring an intermediate representation of
visualizations that bridges primitive SVG and visualization programs. Users may
express adaptation goals to a conversational agent and control the visual
appearance through widgets generated on demand. With an interactive interface,
users can modify both data mappings and visual design elements while
maintaining the original visualization's aesthetic quality. To evaluate
DataWink, we conduct a user study (N=12) with replication and free-form
exploration tasks. As a result, DataWink is recognized for its learnability and
effectiveness in personalized authoring tasks. Our results demonstrate the
potential of example-driven approaches for democratizing visualization
creation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [40] [Controllable Video Generation: A Survey](https://arxiv.org/abs/2507.16869)
*Yue Ma,Kunyu Feng,Zhongyuan Hu,Xinyu Wang,Yucheng Wang,Mingzhe Zheng,Xuanhua He,Chenyang Zhu,Hongyu Liu,Yingqing He,Zeyu Wang,Zhifeng Li,Xiu Li,Wei Liu,Dan Xu,Linfeng Zhang,Qifeng Chen*

Main category: cs.GR

TL;DR: 综述了可控视频生成的关键概念、理论进展及控制机制分类。


<details>
  <summary>Details</summary>
Motivation: 解决当前视频生成基础模型主要依赖文本提示、难以满足复杂多模态用户需求的局限性。

Method: 通过整合相机运动、深度图等非文本条件扩展预训练视频生成模型，分析不同控制信号如何引导生成过程。

Result: 系统分类了现有方法（单条件、多条件和通用可控生成），并整理了相关文献库。

Conclusion: 可控视频生成的研究提升了AIGC系统的灵活性和实用性，未来需进一步探索多模态控制。

Abstract: With the rapid development of AI-generated content (AIGC), video generation
has emerged as one of its most dynamic and impactful subfields. In particular,
the advancement of video generation foundation models has led to growing demand
for controllable video generation methods that can more accurately reflect user
intent. Most existing foundation models are designed for text-to-video
generation, where text prompts alone are often insufficient to express complex,
multi-modal, and fine-grained user requirements. This limitation makes it
challenging for users to generate videos with precise control using current
models. To address this issue, recent research has explored the integration of
additional non-textual conditions, such as camera motion, depth maps, and human
pose, to extend pretrained video generation models and enable more controllable
video synthesis. These approaches aim to enhance the flexibility and practical
applicability of AIGC-driven video generation systems. In this survey, we
provide a systematic review of controllable video generation, covering both
theoretical foundations and recent advances in the field. We begin by
introducing the key concepts and commonly used open-source video generation
models. We then focus on control mechanisms in video diffusion models,
analyzing how different types of conditions can be incorporated into the
denoising process to guide generation. Finally, we categorize existing methods
based on the types of control signals they leverage, including single-condition
generation, multi-condition generation, and universal controllable generation.
For a complete list of the literature on controllable video generation
reviewed, please visit our curated repository at
https://github.com/mayuelala/Awesome-Controllable-Video-Generation.

</details>


### [41] [StreamME: Simplify 3D Gaussian Avatar within Live Stream](https://arxiv.org/abs/2507.17029)
*Luchuan Song,Yang Zhou,Zhan Xu,Yi Zhou,Deepali Aneja,Chenliang Xu*

Main category: cs.GR

TL;DR: StreamME是一种快速3D头像重建方法，支持实时视频流的同步记录与重建，无需预缓存数据，并优化了训练效率和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有3D头像重建方法对预缓存数据的依赖和低效问题，同时保护用户隐私并降低通信带宽。

Method: 基于3D高斯泼溅（3DGS）技术，简化几何依赖并引入基于主点的简化策略，优化点云分布以提高效率。

Result: 实现了快速适应面部表情的高效训练，并在保持渲染质量的同时减少计算资源需求。

Conclusion: StreamME为VR系统和在线会议提供了高效、隐私保护的解决方案，并可直接应用于动画和重新照明等下游应用。

Abstract: We propose StreamME, a method focuses on fast 3D avatar reconstruction. The
StreamME synchronously records and reconstructs a head avatar from live video
streams without any pre-cached data, enabling seamless integration of the
reconstructed appearance into downstream applications. This exceptionally fast
training strategy, which we refer to as on-the-fly training, is central to our
approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating
the reliance on MLPs in deformable 3DGS and relying solely on geometry, which
significantly improves the adaptation speed to facial expression. To further
ensure high efficiency in on-the-fly training, we introduced a simplification
strategy based on primary points, which distributes the point clouds more
sparsely across the facial surface, optimizing points number while maintaining
rendering quality. Leveraging the on-the-fly training capabilities, our method
protects the facial privacy and reduces communication bandwidth in VR system or
online conference. Additionally, it can be directly applied to downstream
application such as animation, toonify, and relighting. Please refer to our
project page for more details: https://songluchuan.github.io/StreamME/.

</details>


### [42] [GhostUMAP2: Measuring and Analyzing (r,d)-Stability of UMAP](https://arxiv.org/abs/2507.17174)
*Myeongwon Jung,Takanori Fujiwara,Jaemin Jo*

Main category: cs.GR

TL;DR: 论文提出了一个框架来评估UMAP随机优化过程中数据点投影的稳定性，并开发了高效的计算方法和可视化工具。


<details>
  <summary>Details</summary>
Motivation: UMAP的随机优化过程对结果的影响尚未充分研究，作者发现其投影结果常因随机性而不稳定。

Method: 引入(r,d)-稳定性概念和“幽灵”点来模拟随机性影响，开发了自适应丢弃方案以减少计算成本。

Result: 实验证明框架有效，能够识别不稳定点，并展示了在真实数据集上的应用。

Conclusion: 论文提出了解决UMAP随机性问题的框架，并提供了使用指南和可视化工具。

Abstract: Despite the widespread use of Uniform Manifold Approximation and Projection
(UMAP), the impact of its stochastic optimization process on the results
remains underexplored. We observed that it often produces unstable results
where the projections of data points are determined mostly by chance rather
than reflecting neighboring structures. To address this limitation, we
introduce (r,d)-stability to UMAP: a framework that analyzes the stochastic
positioning of data points in the projection space. To assess how stochastic
elements, specifically initial projection positions and negative sampling,
impact UMAP results, we introduce "ghosts", or duplicates of data points
representing potential positional variations due to stochasticity. We define a
data point's projection as (r,d)-stable if its ghosts perturbed within a circle
of radius r in the initial projection remain confined within a circle of radius
d for their final positions. To efficiently compute the ghost projections, we
develop an adaptive dropping scheme that reduces a runtime up to 60% compared
to an unoptimized baseline while maintaining approximately 90% of unstable
points. We also present a visualization tool that supports the interactive
exploration of the (r,d)-stability of data points. Finally, we demonstrate the
effectiveness of our framework by examining the stability of projections of
real-world datasets and present usage guidelines for the effective use of our
framework.

</details>


### [43] [A Scientist Question: Research on the Impact of Super Structured Quadrilateral Meshes on Convergence and Accuracy of Finite Element Analysis](https://arxiv.org/abs/2507.17184)
*Hui Zhao*

Main category: cs.GR

TL;DR: 提出了一种新的研究方向：研究超结构化四边形网格的整体全局排列结构对有限元计算收敛性和精度的影响，以解决当前依赖经验的非严谨问题。


<details>
  <summary>Details</summary>
Motivation: 当前工业界和学术界在模拟过程中严重依赖经验的非严谨状态，需明确哪种网格全局排列能确保有限元计算收敛。

Method: 通过应用现代二维和三维几何拓扑理论（如模空间、Teichmüller空间等）生成可控整体排列的超结构化四边形网格。

Result: 新研究方向可系统性探索网格整体结构对计算性能的影响。

Conclusion: 研究为有限元计算中的网格生成提供了新的理论基础，有望提升收敛性和精度。

Abstract: In the current practices of both industry and academia, the convergence and
accuracy of finite element calculations are closely related to the methods and
quality of mesh generation. For years, the research on high-quality mesh
generation in the domestic academic field has mainly referred to the local
quality of quadrilaterals and hexahedrons approximating that of squares and
cubes. The main contribution of this paper is to propose a brand-new research
direction and content: it is necessary to explore and study the influence of
the overall global arrangement structure and pattern of super structured
quadrilateral meshes on the convergence and calculation accuracy of finite
element calculations. Through the research in this new field, it can help solve
the non-rigorous state of serious reliance on "experience" in the mesh
generation stage during simulation in the current industry and academia, and
make clear judgments on which global arrangements of mesh generation can ensure
the convergence of finite element calculations. In order to generate and design
super-structured quadrilateral meshes with controllable overall arrangement
structures, a large number of modern two-dimensional and three-dimensional
geometric topology theories are required, such as moduli space, Teichm\"uller
space, harmonic foliations, dynamical systems, surface mappings, meromorphic
quadratic differentials, surface mappings, etc.

</details>


### [44] [Visualization-Driven Illumination for Density Plots](https://arxiv.org/abs/2507.17265)
*Xin Chen,Yunhai Wang,Huaiwei Bao,Kecheng Lu,Jaemin Jo,Chi-Wing Fu,Jean-Daniel Fekete*

Main category: cs.GR

TL;DR: 提出了一种新颖的可视化驱动光照模型，用于增强密度图的细节显示，同时避免颜色失真。


<details>
  <summary>Details</summary>
Motivation: 现有密度图的光照模型可能导致颜色失真并隐藏低密度区域的细节，影响密度值的查找、比较和异常值检测。

Method: 开发了一种可视化驱动的光照模型和新的图像合成技术，以减少图像阴影与颜色编码密度值之间的干扰。

Result: 通过定量研究、实证评估和案例研究验证了方法的有效性，测试了多达200万个数据点的数据集。

Conclusion: 该技术显著提升了密度图的细节显示和异常值检测能力，适用于大规模数据可视化。

Abstract: We present a novel visualization-driven illumination model for density plots,
a new technique to enhance density plots by effectively revealing the detailed
structures in high- and medium-density regions and outliers in low-density
regions, while avoiding artifacts in the density field's colors. When
visualizing large and dense discrete point samples, scatterplots and dot
density maps often suffer from overplotting, and density plots are commonly
employed to provide aggregated views while revealing underlying structures.
Yet, in such density plots, existing illumination models may produce color
distortion and hide details in low-density regions, making it challenging to
look up density values, compare them, and find outliers. The key novelty in
this work includes (i) a visualization-driven illumination model that
inherently supports density-plot-specific analysis tasks and (ii) a new image
composition technique to reduce the interference between the image shading and
the color-encoded density values. To demonstrate the effectiveness of our
technique, we conducted a quantitative study, an empirical evaluation of our
technique in a controlled study, and two case studies, exploring twelve
datasets with up to two million data point samples.

</details>


### [45] [Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting](https://arxiv.org/abs/2507.17336)
*Hyeongmin Lee,Kyungjune Baek*

Main category: cs.GR

TL;DR: 提出了一种针对4D高斯泼溅的端到端RD优化压缩框架，显著提升了存储效率，同时保持高渲染质量。


<details>
  <summary>Details</summary>
Motivation: 4DGS在动态场景渲染中存储需求大，缺乏高效的压缩框架，限制了实际应用和边缘设备处理。

Method: 基于Ex4DGS，采用小波变换压缩运动轨迹，优化存储效率，实现压缩效率和渲染质量的用户可控平衡。

Result: 实验显示方法实现了91倍的压缩比，同时保持高视觉保真度。

Conclusion: 该方法适用于从边缘设备到高性能环境的实时动态场景渲染。

Abstract: Dynamic 4D Gaussian Splatting (4DGS) effectively extends the high-speed
rendering capabilities of 3D Gaussian Splatting (3DGS) to represent volumetric
videos. However, the large number of Gaussians, substantial temporal
redundancies, and especially the absence of an entropy-aware compression
framework result in large storage requirements. Consequently, this poses
significant challenges for practical deployment, efficient edge-device
processing, and data transmission. In this paper, we introduce a novel
end-to-end RD-optimized compression framework tailored for 4DGS, aiming to
enable flexible, high-fidelity rendering across varied computational platforms.
Leveraging Fully Explicit Dynamic Gaussian Splatting (Ex4DGS), one of the
state-of-the-art 4DGS methods, as our baseline, we start from the existing 3DGS
compression methods for compatibility while effectively addressing additional
challenges introduced by the temporal axis. In particular, instead of storing
motion trajectories independently per point, we employ a wavelet transform to
reflect the real-world smoothness prior, significantly enhancing storage
efficiency. This approach yields significantly improved compression ratios and
provides a user-controlled balance between compression efficiency and rendering
quality. Extensive experiments demonstrate the effectiveness of our method,
achieving up to 91x compression compared to the original Ex4DGS model while
maintaining high visual fidelity. These results highlight the applicability of
our framework for real-time dynamic scene rendering in diverse scenarios, from
resource-constrained edge devices to high-performance environments.

</details>


### [46] [Parametric Integration with Neural Integral Operators](https://arxiv.org/abs/2507.17440)
*Christoph Schied,Alexander Keller*

Main category: cs.GR

TL;DR: 通过在学习阶段对光线传输进行参数化积分，实现实时、无需多帧数据的材料无关去噪（MAD），提升渲染图像质量。


<details>
  <summary>Details</summary>
Motivation: 实时渲染受限于采样预算，常导致噪声图像。传统去噪方法在着色后过滤，而材料无关去噪在着色前进行，更高效。

Method: 利用神经网络近似光线传输积分算子，实现参数化积分；结合现有去噪器和时域抗锯齿技术，单帧数据即可操作。

Result: 方法实时高效，易于训练，兼容基于物理的渲染算法，显著提升图像质量。

Conclusion: 材料无关去噪（MAD）提供了一种高效、通用的实时渲染去噪方案。

Abstract: Real-time rendering imposes strict limitations on the sampling budget for
light transport simulation, often resulting in noisy images. However, denoisers
have demonstrated that it is possible to produce noise-free images through
filtering. We enhance image quality by removing noise before material shading,
rather than filtering already shaded noisy images. This approach allows for
material-agnostic denoising (MAD) and leverages machine learning by
approximating the light transport integral operator with a neural network,
effectively performing parametric integration with neural operators. Our method
operates in real-time, requires data from only a single frame, seamlessly
integrates with existing denoisers and temporal anti-aliasing techniques, and
is efficient to train. Additionally, it is straightforward to incorporate with
physically based rendering algorithms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [47] [Mapple: A Domain-Specific Language for Mapping Distributed Heterogeneous Parallel Programs](https://arxiv.org/abs/2507.17087)
*Anjiang Wei,Rohan Yadav,Hang Song,Wonchan Lee,Ke Wang,Alex Aiken*

Main category: cs.DC

TL;DR: Mapple 是一种高级声明式编程接口，用于优化分布式应用程序的映射，减少了代码量并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 优化分布式异构系统的并行程序通常需要大量代码修改，而现有任务编程系统的映射接口过于底层。

Method: Mapple 提供了一种高级声明式接口和 transformation 原语，特别是 decompose 原语，以减少通信量，并在 Legion 运行时上实现。

Result: 在九个应用中，Mapple 将映射代码量减少了 14 倍，性能提升最高达 1.34 倍，decompose 原语比现有启发式方法提升了 1.83 倍。

Conclusion: Mapple 简化了高性能分布式应用程序映射的开发。

Abstract: Optimizing parallel programs for distributed heterogeneous systems remains a
complex task, often requiring significant code modifications. Task-based
programming systems improve modularity by separating performance decisions from
core application logic, but their mapping interfaces are often too low-level.
In this work, we introduce Mapple, a high-level, declarative programming
interface for mapping distributed applications. Mapple provides transformation
primitives to resolve dimensionality mismatches between iteration and processor
spaces, including a key primitive, decompose, that helps minimize communication
volume. We implement Mapple on top of the Legion runtime by translating Mapple
mappers into its low-level C++ interface. Across nine applications, including
six matrix multiplication algorithms and three scientific computing workloads,
Mapple reduces mapper code size by 14X and enables performance improvements of
up to 1.34X over expert-written C++ mappers. In addition, the decompose
primitive achieves up to 1.83X improvement over existing
dimensionality-resolution heuristics. These results demonstrate that Mapple
simplifies the development of high-performance mappers for distributed
applications.

</details>


### [48] [PathWeaver: A High-Throughput Multi-GPU System for Graph-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.17094)
*Sukjin Kim,Seongyeon Park,Si Ung Noh,Junguk Hong,Taehee Kwon,Hunseong Lim,Jinho Lee*

Main category: cs.DC

TL;DR: PathWeaver 是一个新型多GPU框架，用于扩展和加速大规模数据集的近似最近邻搜索（ANNS），通过减少冗余搜索、优化查询起始点和过滤无关数据点，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速ANNS方法在多GPU环境下扩展性不足，仅仅将额外GPU用于扩展内存容量，导致效率低下。

Method: PathWeaver 提出了基于流水线的路径扩展、利用代表数据集优化起始点的幽灵暂存技术，以及方向引导选择以减少不必要计算。

Result: PathWeaver 在95%召回率下实现了3.24倍几何平均加速和最高5.30倍的速度提升。

Conclusion: PathWeaver 是一种高效的多GPU ANNS框架，能够显著提升大规模数据集的处理速度和效率。

Abstract: Graph-based Approximate Nearest Neighbor Search (ANNS) is widely adopted in
numerous applications, such as recommendation systems, natural language
processing, and computer vision. While recent works on GPU-based acceleration
have significantly advanced ANNS performance, the ever-growing scale of
datasets now demands efficient multi-GPU solutions. However, the design of
existing works overlooks multi-GPU scalability, resulting in naive approaches
that treat additional GPUs as a means to extend memory capacity for large
datasets. This inefficiency arises from partitioning the dataset and
independently searching for data points similar to the queries in each GPU. We
therefore propose PathWeaver, a novel multi-GPU framework designed to scale and
accelerate ANNS for large datasets. First, we propose pipelining-based path
extension, a GPU-aware pipelining mechanism that reduces prior work's redundant
search iterations by leveraging GPU-to-GPU communication. Second, we design
ghost staging that leverages a representative dataset to identify optimal query
starting points, reducing the search space for challenging queries. Finally, we
introduce direction-guided selection, a data selection technique that filters
irrelevant points early in the search process, minimizing unnecessary memory
accesses and distance computations. Comprehensive evaluations across diverse
datasets demonstrate that PathWeaver achieves 3.24$\times$ geomean speedup and
up to 5.30$\times$ speedup on 95% recall rate over state-of-the-art
multi-GPU-based ANNS frameworks.

</details>


### [49] [BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving](https://arxiv.org/abs/2507.17120)
*Wanyi Zheng,Minxian Xu,Shengye Song,Kejiang Ye*

Main category: cs.DC

TL;DR: 论文提出BucketServe框架，通过动态分桶和优先级调度优化LLM推理性能，显著提升吞吐量和系统负载能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统在异构工作负载下存在GPU内存利用不足和延迟增加的问题，难以适应动态工作负载波动。

Method: BucketServe通过按序列长度分组请求、动态调整批大小及优先级调度来优化性能。

Result: 实验显示BucketServe吞吐量提升3.58倍，负载能力提高1.975倍，优于UELLM和DistServe。

Conclusion: BucketServe有效解决了LLM服务系统中的资源利用和延迟问题，显著提升了性能。

Abstract: Large language models (LLMs) have become increasingly popular in various
areas, traditional business gradually shifting from rule-based systems to
LLM-based solutions. However, the inference of LLMs is resource-intensive or
latency-sensitive, posing significant challenges for serving systems. Existing
LLM serving systems often use static or continuous batching strategies, which
can lead to inefficient GPU memory utilization and increased latency,
especially under heterogeneous workloads. These methods may also struggle to
adapt to dynamic workload fluctuations, resulting in suboptimal throughput and
potential service level objective (SLO) violations. In this paper, we introduce
BucketServe, a bucket-based dynamic batching framework designed to optimize LLM
inference performance. By grouping requests into size-homogeneous buckets based
on sequence length, BucketServe minimizes padding overhead and optimizes GPU
memory usage through real-time batch size adjustments preventing out-of-memory
(OOM) errors. It introduces adaptive bucket splitting/merging and
priority-aware scheduling to mitigate resource fragmentation and ensure SLO
compliance. Experiment shows that BucketServe significantly outperforms UELLM
in throughput, achieving up to 3.58x improvement. It can also handle 1.93x more
request load under the SLO attainment of 80% compared with DistServe and
demonstrates 1.975x higher system load capacity compared to the UELLM.

</details>


### [50] [Auto-scaling Approaches for Cloud-native Applications: A Survey and Taxonomy](https://arxiv.org/abs/2507.17128)
*Minxian Xu,Linfeng Wen,Junhan Liao,Huaming Wu,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 本文系统回顾了2020年以来云原生应用自动扩展的前沿方法，提出了从基础设施、架构、扩展方法、优化目标和行为建模五个角度的分类体系，并讨论了各种方法的优缺点和应用场景，最后指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着云原生应用交互复杂性和负载变化的增加，传统自动扩展方法面临挑战，需要更先进的算法优化资源利用和性能。

Method: 通过文献综述，提出了从五个角度的分类体系，并对各种方法进行了比较和深入讨论。

Result: 总结各类方法的优缺点和应用场景，识别了当前研究中的空白和未解决问题。

Conclusion: 未来研究应关注大模型应用、微服务依赖管理和元学习技术，以提高模型的适应性和适用性。

Abstract: The interactions within cloud-native applications are complex, with a
constantly changing number of services and loads, posing higher demands on
auto-scaling approach. This mainly involves several challenges such as
microservices dependency analysis, performance profiling, anomaly detection,
workload characterization and task co-location. Therefore, some advanced
algorithms have been investigated into auto-scaling cloud-native applications
to optimize system and application performance. These algorithms can learn from
historical data and appropriately adjust resource allocation based on the
current environment and load conditions to optimize resource utilization and
system performance. In this paper, we systematically review the literature on
state-of-the-art auto-scaling approaches for cloud-native applications from
2020, and further explore the technological evolution. Additionally, we propose
a detailed taxonomy to categorize current research from five perspectives,
including infrastructure, architecture, scaling methods, optimization
objectives, and behavior modeling. Then, we provide a comprehensive comparison
and in-depth discussion of the key features, advantages, limitations, and
application scenarios of each approach, considering their performance in
diverse environments and under various conditions. Finally, we summarize the
current state of research in this field, identify the gaps and unresolved
challenges, and emphasize promising directions for future exploration,
particularly in areas such as the application of large models, microservice
dependency management, and the use of meta-learning techniques to enhance model
applicability and adaptability across different environments.

</details>


### [51] [BrownoutServe: SLO-Aware Inference Serving under Bursty Workloads for MoE-based LLMs](https://arxiv.org/abs/2507.17133)
*Jianmin Hu,Minxian Xu,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 提出了BrownoutServe框架，通过动态调整和联合专家优化MoE架构LLM的推理效率和服务可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决现有MoE架构LLM系统中由于静态模型放置和缺乏动态工作负载适应而导致的低效率和资源利用不足问题。

Method: 引入联合专家（united experts）以减少专家访问次数和推理延迟，并提出动态降载机制（dynamic brownout mechanism）自适应调整处理。

Result: 在多种工作负载下，BrownoutServe实现了比vLLM高2.07倍的吞吐量，并将SLO违规减少了90.28%。

Conclusion: BrownoutServe显著提升了MoE架构LLM的推理效率和可靠性，适用于突发流量场景。

Abstract: In recent years, the Mixture-of-Experts (MoE) architecture has been widely
applied to large language models (LLMs), providing a promising solution that
activates only a subset of the model's parameters during computation, thereby
reducing overall memory requirements and allowing for faster inference compared
to dense models. Despite these advantages, existing systems still face issues
of low efficiency due to static model placement and lack of dynamic workloads
adaptation. This leads to suboptimal resource utilization and increased
latency, especially during bursty requests periods.
  To address these challenges, this paper introduces BrownoutServe, a novel
serving framework designed to optimize inference efficiency and maintain
service reliability for MoE-based LLMs under dynamic computational demands and
traffic conditions. BrownoutServe introduces "united experts" that integrate
knowledge from multiple experts, reducing the times of expert access and
inference latency. Additionally, it proposes a dynamic brownout mechanism to
adaptively adjust the processing of certain tokens, optimizing inference
performance while guaranteeing service level objectives (SLOs) are met. Our
evaluations show the effectiveness of BrownoutServe under various workloads: it
achieves up to 2.07x throughput improvement compared to vLLM and reduces SLO
violations by 90.28%, showcasing its robustness under bursty traffic while
maintaining acceptable inference accuracy.

</details>


### [52] [Efficient Column-Wise N:M Pruning on RISC-V CPU](https://arxiv.org/abs/2507.17301)
*Chi-Wei Chu,Ding-Yong Hong,Jan-Jan Wu*

Main category: cs.DC

TL;DR: 提出了一种基于列级N:M剪枝策略的方法，优化了卷积神经网络的计算效率，并结合XNNPACK和AITemplate技术，显著提升了推理速度和准确率。


<details>
  <summary>Details</summary>
Motivation: 卷积操作是深度学习中性能瓶颈之一，剪枝技术的实施方式对其效率和内存占用有重要影响，因此需要一种高效且低损耗的剪枝方法。

Method: 提出了列级N:M剪枝策略，结合XNNPACK支持RISC-V架构，同时融合im2col和数据打包操作以减少内存开销，并利用AITemplate分析最佳实现。

Result: 方法使ResNet推理速度提升高达4.0倍，ImageNet top-1准确率仅下降2.1%以内。

Conclusion: 列级N:M剪枝策略及其配套优化技术显著提升了卷积神经网络的计算效率和模型性能。

Abstract: In deep learning frameworks, weight pruning is a widely used technique for
improving computational efficiency by reducing the size of large models. This
is especially critical for convolutional operators, which often act as
performance bottlenecks in convolutional neural networks (CNNs). However, the
effectiveness of pruning heavily depends on how it is implemented, as different
methods can significantly impact both computational performance and memory
footprint. In this work, we propose a column-wise N:M pruning strategy applied
at the tile level and modify XNNPACK to enable efficient execution of pruned
models on the RISC-V vector architecture. Additionally, we propose fusing the
operations of im2col and data packing to minimize redundant memory accesses and
memory overhead. To further optimize performance, we incorporate AITemplate's
profiling technique to identify the optimal implementation for each
convolutional operator. Our proposed approach effectively increases ResNet
inference throughput by as much as 4.0x, and preserves ImageNet top-1 accuracy
within 2.1\% of the dense baseline.

</details>


### [53] [Multiprocessor Scheduling with Memory Constraints: Fundamental Properties and Finding Optimal Solutions](https://arxiv.org/abs/2507.17411)
*Pál András Papp,Toni Böhnlein,A. N. Yzelman*

Main category: cs.DC

TL;DR: 本文研究了在多处理器和两级内存层次结构中调度通用计算DAG的问题，分析了其计算复杂性，并证明了分离优化并行化和内存管理会导致解偏离最优解。提出了一种基于整数线性规划（ILP）的调度算法，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究多处理器和两级内存层次结构中DAG调度问题，以解决负载平衡、通信和缓存限制带来的数据移动问题。

Method: 采用整数线性规划（ILP）表示和解决问题，并开发了一种基于ILP的整体调度算法。

Result: 实验结果表明，基于ILP的方法比传统调度算法和内存管理策略的组合效果更好。

Conclusion: 基于ILP的调度算法在处理DAG调度问题时具有显著优势，能够有效优化并行化和内存管理的综合性能。

Abstract: We study the problem of scheduling a general computational DAG on multiple
processors in a 2-level memory hierarchy. This setting is a natural
generalization of several prominent models in the literature, and it
simultaneously captures workload balancing, communication, and data movement
due to cache size limitations. We first analyze the fundamental properties of
this problem from a theoretical perspective, such as its computational
complexity. We also prove that optimizing parallelization and memory management
separately, as done in many applications, can result in a solution that is a
linear factor away from the optimum.
  On the algorithmic side, we discuss a natural technique to represent and
solve the problem as an Integer Linear Program (ILP). We develop a holistic
scheduling algorithm based on this approach, and we experimentally study its
performance and properties on a small benchmark of computational tasks. Our
results confirm that the ILP-based method can indeed find considerably better
solutions than a baseline which combines classical scheduling algorithms and
memory management policies.

</details>


### [54] [Distributed P2P quantile tracking with relative value error](https://arxiv.org/abs/2507.17458)
*Marco Pulimeno,Italo Epicoco,Massimo Cafaro*

Main category: cs.DC

TL;DR: DUDDSketch是一种分布式版本的UDDSketch算法，用于精确跟踪分位数，适用于非结构化的P2P网络。


<details>
  <summary>Details</summary>
Motivation: 设计一个完全去中心化、基于gossip的分布式协议，以在非结构化P2P网络中准确跟踪分位数。

Method: 采用分布式协议设计，并通过数学证明其正确性，同时进行广泛的实验验证。

Result: 实验结果表明，该算法能够收敛到顺序算法提供的结果，验证了其有效性。

Conclusion: DUDDSketch是一个有效的分布式分位数跟踪算法，适用于非结构化P2P网络。

Abstract: In this paper we present \textsc{DUDDSketch}, a distributed version of the
\textsc{UDDSketch} algorithm for accurate tracking of quantiles. The algorithm
is a fully decentralized, gossip-based distributed protocol working in the
context of unstructured P2P networks. We discuss the algorithm's design and
formally prove its correctness. We also show, through extensive experimental
results, that the algorithm converges to the results provided by the sequential
algorithm, which is a fundamental and highly desirable property.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [55] [Triadic First-Order Logic Queries in Temporal Networks](https://arxiv.org/abs/2507.17215)
*Omkar Bhalerao,Yunjie Pan,C. Seshadhri,Nishil Talati*

Main category: cs.DB

TL;DR: 该论文提出了一种新的基于阈值一阶逻辑（FOL）的时序网络模体分析方法，并设计了高效算法FOLTY，用于处理复杂的时序模体查询。


<details>
  <summary>Details</summary>
Motivation: 现有的时序网络模体计数方法主要关注简单的三元查询，无法表达更复杂的模式。作者受逻辑和数据库理论启发，希望通过引入阈值一阶逻辑来丰富模体分析的表达能力。

Method: 提出阈值一阶逻辑（FOL）模体分析框架，支持包含存在量词和阈值全称量词的查询。设计算法FOLTY，利用专门的时序数据结构实现高效查询。

Result: FOLTY在稀疏图中的理论时间复杂度与现有最佳时序三角计数算法一致。实际测试中，该算法能在不到一小时内处理包含近7000万条边的图。

Conclusion: FOLTY算法为模体分析开辟了新的研究方向，展示了阈值一阶逻辑在时序网络中的潜力。

Abstract: Motif counting is a fundamental problem in network analysis, and there is a
rich literature of theoretical and applied algorithms for this problem. Given a
large input network $G$, a motif $H$ is a small "pattern" graph indicative of
special local structure. Motif/pattern mining involves finding all matches of
this pattern in the input $G$. The simplest, yet challenging, case of motif
counting is when $H$ has three vertices, often called a "triadic" query. Recent
work has focused on "temporal graph mining", where the network $G$ has edges
with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of
"thresholded First Order Logic (FOL) Motif Analysis" for massive temporal
networks. A typical triadic motif query asks for the existence of three
vertices that form a desired temporal pattern. An "FOL" motif query is obtained
by having both existential and thresholded universal quantifiers. This allows
for query semantics that can mine richer information from networks. A typical
triadic query would be "find all triples of vertices $u,v,w$ such that they
form a triangle within one hour". A thresholded FOL query can express "find all
pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$
also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic
queries. The theoretical running time of FOLTY matches the best known running
time for temporal triangle counting in sparse graphs. We give an efficient
implementation of FOLTY using specialized temporal data structures. FOLTY has
excellent empirical behavior, and can answer triadic FOL queries on graphs with
nearly 70M edges is less than hour on commodity hardware. Our work has the
potential to start a new research direction in the classic well-studied problem
of motif analysis.

</details>


### [56] [Unfolding Data Quality Dimensions in Practice: A Survey](https://arxiv.org/abs/2507.17507)
*Vasileios Papastergios,Lisa Ehrlinger,Anastasios Gounaris*

Main category: cs.DB

TL;DR: 该论文旨在通过系统地将数据质量工具的低层功能与高层维度连接起来，弥合数据质量理论与实践的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管研究和标准化尝试（如ISO/IEC 25012）广泛存在，但数据质量维度的实际应用仍不清晰。需要将理论与实践结合，为实践者和研究者提供统一的视角。

Method: 通过分析七个开源数据质量工具的功能，系统地映射其功能与数据质量维度之间的关系，揭示它们之间的多对多关系。

Result: 提供了一份全面的映射，展示了单个功能及其变体如何部分贡献于单一维度的评估。

Conclusion: 这项系统性调查为实践者和研究者提供了关于数据质量检查分散情况的统一视图，并为跨多维度质量评估提供了可行的见解。

Abstract: Data quality describes the degree to which data meet specific requirements
and are fit for use by humans and/or downstream tasks (e.g., artificial
intelligence). Data quality can be assessed across multiple high-level concepts
called dimensions, such as accuracy, completeness, consistency, or timeliness.
While extensive research and several attempts for standardization (e.g.,
ISO/IEC 25012) exist for data quality dimensions, their practical application
often remains unclear. In parallel to research endeavors, a large number of
tools have been developed that implement functionalities for the detection and
mitigation of specific data quality issues, such as missing values or outliers.
With this paper, we aim to bridge this gap between data quality theory and
practice by systematically connecting low-level functionalities offered by data
quality tools with high-level dimensions, revealing their many-to-many
relationships. Through an examination of seven open-source data quality tools,
we provide a comprehensive mapping between their functionalities and the data
quality dimensions, demonstrating how individual functionalities and their
variants partially contribute to the assessment of single dimensions. This
systematic survey provides both practitioners and researchers with a unified
view on the fragmented landscape of data quality checks, offering actionable
insights for quality assessment across multiple dimensions.

</details>


### [57] [SHINE: A Scalable HNSW Index in Disaggregated Memory](https://arxiv.org/abs/2507.17647)
*Manuel Widmoser,Daniel Kocher,Nikolaus Augsten*

Main category: cs.DB

TL;DR: 提出了一种用于分布式内存的扩展HNSW索引方法，解决传统分区方法精度低的问题，通过高效缓存机制克服网络带宽限制。


<details>
  <summary>Details</summary>
Motivation: 解决大规模高维向量在分布式系统中的近似最近邻搜索问题，同时保持单机HNSW的精度。

Method: 构建图保留索引，结合高效缓存机制和逻辑缓存组合，提升整体缓存效果。

Result: 在评估中验证了方法的效率和可扩展性，达到与单机HNSW相同的精度。

Conclusion: 该方法在分布式内存中实现了高效、高精度的ANN搜索，适用于现代数据中心架构。

Abstract: Approximate nearest neighbor (ANN) search is a fundamental problem in
computer science for which in-memory graph-based methods, such as Hierarchical
Navigable Small World (HNSW), perform exceptionally well. To scale beyond
billions of high-dimensional vectors, the index must be distributed. The
disaggregated memory architecture physically separates compute and memory into
two distinct hardware units and has become popular in modern data centers. Both
units are connected via RDMA networks that allow compute nodes to directly
access remote memory and perform all the computations, posing unique challenges
for disaggregated indexes.
  In this work, we propose a scalable HNSW index for ANN search in
disaggregated memory. In contrast to existing distributed approaches, which
partition the graph at the cost of accuracy, our method builds a
graph-preserving index that reaches the same accuracy as a single-machine HNSW.
Continuously fetching high-dimensional vector data from remote memory leads to
severe network bandwidth limitations, which we overcome by employing an
efficient caching mechanism. Since answering a single query involves processing
numerous unique graph nodes, caching alone is not sufficient to achieve high
scalability. We logically combine the caches of the compute nodes to increase
the overall cache effectiveness and confirm the efficiency and scalability of
our method in our evaluation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [58] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
*Bowen Zheng,Ming Ma,Zhongqiao Lin,Tianming Yang*

Main category: cs.CL

TL;DR: 提出SPADE解码方法，对齐中间层与输出层表示，结合混合早期退出算法降低推理成本而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 减少大型语言模型的计算成本，同时避免早期退出算法因中间层与输出层表示不对齐导致的性能下降。

Method: 开发SPADE解码方法，传播最小化序列（起始和答案标记），训练线性近似模型计算基于熵的置信度指标，混合早期退出算法。

Result: 显著降低推理成本，保持准确性，为实际应用提供高效解决方案。

Conclusion: SPADE与混合早期退出算法结合，有效提升大型语言模型部署的效率和可扩展性。

Abstract: Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

</details>


### [59] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
*Danny D. Leybzon,Shreyas Tirumala,Nishant Jain,Summer Gillen,Michael Jackson,Cameron McPhee,Jennifer Schmidt*

Main category: cs.CL

TL;DR: 论文研究了利用语音AI技术进行电话调查的可行性，结合LLM、ASR和语音合成技术开发了一个系统，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着语音AI系统的兴起，研究人员希望通过AI电话调查实现量化研究的规模化，同时兼顾人机交互的友好性和方法的严谨性。

Method: 开发了一个基于LLM、ASR和语音合成技术的AI系统，用于定量调查，严格遵守研究规范如随机化问题顺序等。

Result: 通过两项试点调查验证，发现较短的工具和更灵活的AI访谈者能提高完成率、降低中断率并提升满意度。

Conclusion: 语音AI技术可用于定量调查，优化设计和交互体验能进一步提高效果。

Abstract: With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [60] [Principled Multimodal Representation Learning](https://arxiv.org/abs/2507.17343)
*Xiaohao Liu,Xiaobo Xia,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了一种名为PMRL的新框架，通过优化表示矩阵的主导奇异值，实现多模态的无锚点依赖对齐，解决了传统方法的限制。


<details>
  <summary>Details</summary>
Motivation: 传统多模态表示学习方法依赖预定义的锚点模态，限制了跨模态对齐。PMRL旨在消除这种依赖，以更稳定的方式实现多模态对齐。

Method: PMRL通过优化表示矩阵的秩1 Gram矩阵性质，采用基于softmax的损失函数和实例对比正则化，实现多模态对齐。

Result: 在多种任务上的实验验证了PMRL优于基线方法。

Conclusion: PMRL提供了一种无锚点依赖、稳定的多模态表示学习框架，具有广泛的应用潜力。

Abstract: Multimodal representation learning seeks to create a unified representation
space by integrating diverse data modalities to improve multimodal
understanding. Traditional methods often depend on pairwise contrastive
learning, which relies on a predefined anchor modality, restricting alignment
across all modalities. Recent advances have investigated the simultaneous
alignment of multiple modalities, yet several challenges remain, such as
limitations imposed by fixed anchor points and instability arising from
optimizing the product of singular values. To address the challenges, in this
paper, we propose Principled Multimodal Representation Learning (PMRL), a novel
framework that achieves simultaneous alignment of multiple modalities without
anchor dependency in a more stable manner. Specifically, grounded in the
theoretical insight that full alignment corresponds to a rank-1 Gram matrix,
PMRL optimizes the dominant singular value of the representation matrix to
align modalities along a shared leading direction. We propose a softmax-based
loss function that treats singular values as logits to prioritize the largest
singular value. Besides, instance-wise contrastive regularization on the
leading eigenvectors maintains inter-instance separability and prevents
representation collapse. Extensive experiments across diverse tasks demonstrate
PMRL's superiority compared to baseline methods. The source code will be
publicly available.

</details>


### [61] [HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning](https://arxiv.org/abs/2507.17402)
*Li Jun,Wang Jinpeng,Tan Chaolei,Lian Niu,Chen Long,Zhang Min,Wang Yaowei,Xia Shu-Tao,Chen Bin*

Main category: cs.CV

TL;DR: HLFormer提出了一种双曲空间建模框架，用于解决部分相关视频检索中欧几里得空间几何扭曲的问题，通过混合空间编码和动态特征融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在欧几里得空间中存在几何扭曲问题，无法有效建模视频的层次结构，导致时间建模效果不佳。

Method: 提出HLFormer框架，结合洛伦兹注意力块和欧几里得注意力块进行混合空间编码，并引入均值引导的自适应交互模块和偏序保持损失。

Result: 实验表明HLFormer优于现有最优方法。

Conclusion: 双曲空间建模能有效提升部分相关视频检索的性能，混合空间编码和动态融合是关键。

Abstract: Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of
matching untrimmed videos with text queries describing only partial content.
Existing methods suffer from geometric distortion in Euclidean space that
sometimes misrepresents the intrinsic hierarchical structure of videos and
overlooks certain hierarchical semantics, ultimately leading to suboptimal
temporal modeling. To address this issue, we propose the first hyperbolic
modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space
learning to compensate for the suboptimal hierarchical modeling capabilities of
Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block
and Euclidean Attention Block to encode video embeddings in hybrid spaces,
using the Mean-Guided Adaptive Interaction Module to dynamically fuse features.
Additionally, we introduce a Partial Order Preservation Loss to enforce "text <
video" hierarchy through Lorentzian cone constraints. This approach further
enhances cross-modal matching by reinforcing partial relevance between video
content and text queries. Extensive experiments show that HLFormer outperforms
state-of-the-art methods. Code is released at
https://github.com/lijun2005/ICCV25-HLFormer.

</details>


### [62] [Yume: An Interactive World Generation Model](https://arxiv.org/abs/2507.17744)
*Xiaofeng Mao,Shaoheng Lin,Zhen Li,Chuanhao Li,Wenshuo Peng,Tong He,Jiangmiao Pang,Mingmin Chi,Yu Qiao,Kaipeng Zhang*

Main category: cs.CV

TL;DR: Yume 是一个通过图片、文本或视频创建交互式、真实且动态世界的项目。本文介绍了其初步版本，能够从图片生成动态世界，并通过键盘控制探索。技术核心包括相机运动量化、视频生成架构、先进采样器和模型加速。


<details>
  <summary>Details</summary>
Motivation: 旨在打造一个高保真、交互式的视频世界生成方法，通过多模态输入实现动态世界探索与控制。

Method: 采用四部分框架：相机运动量化、基于 MVDT 的视频生成架构、AAM 和 TTS-SDE 采样器优化、模型加速技术。

Result: 在高质量数据集上训练，并在多样场景中取得显著效果。

Conclusion: Yume 展示了高效动态世界生成的潜力，并计划持续更新以实现其原始目标。

Abstract: Yume aims to use images, text, or videos to create an interactive, realistic,
and dynamic world, which allows exploration and control using peripheral
devices or neural signals. In this report, we present a preview version of
\method, which creates a dynamic world from an input image and allows
exploration of the world using keyboard actions. To achieve this high-fidelity
and interactive video world generation, we introduce a well-designed framework,
which consists of four main components, including camera motion quantization,
video generation architecture, advanced sampler, and model acceleration. First,
we quantize camera motions for stable training and user-friendly interaction
using keyboard inputs. Then, we introduce the Masked Video Diffusion
Transformer~(MVDT) with a memory module for infinite video generation in an
autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)
and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)
are introduced to the sampler for better visual quality and more precise
control. Moreover, we investigate model acceleration by synergistic
optimization of adversarial distillation and caching mechanisms. We use the
high-quality world exploration dataset \sekai to train \method, and it achieves
remarkable results in diverse scenes and applications. All data, codebase, and
model weights are available on https://github.com/stdstu12/YUME. Yume will
update monthly to achieve its original goal. Project page:
https://stdstu12.github.io/YUME-Project/.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [63] [Symmetric Private Information Retrieval (SPIR) on Graph-Based Replicated Systems](https://arxiv.org/abs/2507.17736)
*Shreya Meel,Sennur Ulukus*

Main category: cs.IT

TL;DR: 该论文研究了基于图模型复制的对称私有信息检索（SPIR）问题，提出了一种可行方案并验证了其下载速率的下界，同时证明了消息特定随机性的最小大小，以及对路径图和正则图类的确切SPIR容量。


<details>
  <summary>Details</summary>
Motivation: 研究复制的数据库模型中SPIR问题，探索服务器间的随机性分配对SPIR性能的影响。

Method: 提出了一种基于图模型的SPIR方案，分析了消息特定随机性的最小需求，并通过上下界匹配推导了特定图类的SPIR容量。

Result: 确立了SPIR容量的下界，证明了消息特定随机性的最小大小等于消息大小，并推导了路径图和正则图的精确SPIR容量。

Conclusion: 该研究为复制的数据库模型中的SPIR问题提供了理论支撑，并验证了特定图类的SPIR容量精确解。

Abstract: We introduce the problem of symmetric private information retrieval (SPIR) on
replicated databases modeled by a simple graph. In this model, each vertex
corresponds to a server, and a message is replicated on two servers if and only
if there is an edge between them. We consider the setting where the server-side
common randomness necessary to accomplish SPIR is also replicated at the
servers according to the graph, and we call this as message-specific common
randomness. In this setting, we establish a lower bound on the SPIR capacity,
i.e., the maximum download rate, for general graphs, by proposing an achievable
SPIR scheme. Next, we prove that, for any SPIR scheme to be feasible, the
minimum size of message-specific randomness should be equal to the size of a
message. Finally, by providing matching upper bounds, we derive the exact SPIR
capacity for the class of path and regular graphs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [64] [Analysis of Post-Quantum Cryptography in User Equipment in 5G and Beyond](https://arxiv.org/abs/2507.17074)
*Sanzida Hoque,Abdullah Aydeger,Engin Zeydan,Madhusanka Liyanage*

Main category: cs.CR

TL;DR: 本文详细评估了NIST选定的后量子密码算法在5G网络中用户设备间通信的性能，发现ML-KEM和ML-DSA组合效率最佳，适合时延敏感应用。


<details>
  <summary>Details</summary>
Motivation: 量子计算的兴起威胁传统公钥密码系统安全性，促使转向后量子密码（PQC），但其在无线通信环境中的实际性能尚缺乏研究。

Method: 使用完整5G仿真堆栈（Open5GS和UERANSIM）和PQC-enabled TLS 1.3，测试了NIST选定的PQC算法在5G网络中的性能，包括密钥封装机制和数字签名方案。

Result: ML-KEM与ML-DSA组合在时延敏感应用中效率最高，而SPHINCS+和HQC组合因计算和传输开销较大，不适合时间敏感的5G场景。

Conclusion: 后量子密码算法在5G网络中需权衡安全性与性能，ML-KEM和ML-DSA组合是当前最优选择。

Abstract: The advent of quantum computing threatens the security of classical
public-key cryptographic systems, prompting the transition to post-quantum
cryptography (PQC). While PQC has been analyzed in theory, its performance in
practical wireless communication environments remains underexplored. This paper
presents a detailed implementation and performance evaluation of NIST-selected
PQC algorithms in user equipment (UE) to UE communications over 5G networks.
Using a full 5G emulation stack (Open5GS and UERANSIM) and PQC-enabled TLS 1.3
via BoringSSL and liboqs, we examine key encapsulation mechanisms and digital
signature schemes across realistic network conditions. We evaluate performance
based on handshake latency, CPU and memory usage, bandwidth, and retransmission
rates, under varying cryptographic configurations and client loads. Our
findings show that ML-KEM with ML-DSA offers the best efficiency for
latency-sensitive applications, while SPHINCS+ and HQC combinations incur
higher computational and transmission overheads, making them unsuitable for
security-critical but time-sensitive 5G scenarios.

</details>


### [65] [SoK: Securing the Final Frontier for Cybersecurity in Space-Based Infrastructure](https://arxiv.org/abs/2507.17064)
*Nafisa Anjum,Tasnuva Farheen*

Main category: cs.CR

TL;DR: 该研究全面分析了空间网络攻击载体及缓解措施，提出了风险评分框架，并指出了未来研究的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着现代技术的发展，空间资产对关键基础设施和国家安全的重要性日益增加，但其面临严重的网络攻击威胁，需要强化安全防御。

Method: 研究采用综合方法分析了地面、空间、卫星及卫星星座等可能的网络攻击载体，并评估了相关缓解措施的有效性。

Result: 研究提出了风险评分框架，并识别了未来开发与测试先进技术解决方案的潜在挑战。

Conclusion: 研究强调了空间网络安全措施的紧迫性，并为未来的研究和实践提供了指导。

Abstract: With the advent of modern technology, critical infrastructure,
communications, and national security depend increasingly on space-based
assets. These assets, along with associated assets like data relay systems and
ground stations, are, therefore, in serious danger of cyberattacks. Strong
security defenses are essential to ensure data integrity, maintain secure
operations, and protect assets in space and on the ground against various
threats. Previous research has found discrete vulnerabilities in space systems
and suggested specific solutions to address them. Such research has yielded
valuable insights, but lacks a thorough examination of space cyberattack
vectors and a rigorous assessment of the efficacy of mitigation techniques.
This study tackles this issue by taking a comprehensive approach to analyze the
range of possible space cyber-attack vectors, which include ground, space,
satellite, and satellite constellations. In order to address the particular
threats, the study also assesses the efficacy of mitigation measures that are
linked with space infrastructures and proposes a Risk Scoring Framework. Based
on the analysis, this paper identifies potential research challenges for
developing and testing cutting-edge technology solutions, encouraging robust
cybersecurity measures needed in space.

</details>


### [66] [Active Attack Resilience in 5G: A New Take on Authentication and Key Agreement](https://arxiv.org/abs/2507.17491)
*Nazatul H. Sultan,Xinlong Guan,Josef Pieprzyk,Wei Ni,Sharif Abuadbba,Hajime Suzuki*

Main category: cs.CR

TL;DR: 5G-AKA协议存在安全和性能缺陷，本文提出改进方案，包括无状态设计和完美前向保密，通过ProVerif验证其安全性，并在性能上优于5G-AKA。


<details>
  <summary>Details</summary>
Motivation: 5G网络扩展至关键基础设施，需提升认证协议的安全性与效率，弥补5G-AKA在主动攻击防御和完美前向保密上的不足。

Method: 1. 提出无状态版本，避免序列号依赖；2. 引入完美前向保密机制。通过ProVerif分析安全性，并原型实现性能评估。

Result: 改进协议在安全性和性能上均优于5G-AKA，计算开销小，适用于5G及未来网络。

Conclusion: 改进协议解决了5G-AKA的核心问题，兼具高效与强安全性，为5G及未来网络提供实用方案。

Abstract: As 5G networks expand into critical infrastructure, secure and efficient user
authentication is more important than ever. The 5G-AKA protocol, standardized
by 3GPP in TS 33.501, is central to authentication in current 5G deployments.
It provides mutual authentication, user privacy, and key secrecy. However,
despite its adoption, 5G-AKA has known limitations in both security and
performance. While it focuses on protecting privacy against passive attackers,
recent studies show its vulnerabilities to active attacks. It also relies on a
sequence number mechanism to prevent replay attacks, requiring perfect
synchronization between the device and the core network. This stateful design
adds complexity, causes desynchronization, and incurs extra communication
overhead. More critically, 5G-AKA lacks Perfect Forward Secrecy (PFS), exposing
past communications if long-term keys are compromised-an increasing concern
amid sophisticated threats. This paper proposes an enhanced authentication
protocol that builds on 5G-AKA's design while addressing its shortcomings.
First, we introduce a stateless version that removes sequence number reliance,
reducing complexity while staying compatible with existing SIM cards and
infrastructure. We then extend this design to add PFS with minimal
cryptographic overhead. Both protocols are rigorously analyzed using ProVerif,
confirming their compliance with all major security requirements, including
resistance to passive and active attacks, as well as those defined by 3GPP and
academic studies. We also prototype both protocols and evaluate their
performance against 5G-AKA and 5G-AKA' (USENIX'21). Our results show the
proposed protocols offer stronger security with only minor computational
overhead, making them practical, future-ready solutions for 5G and beyond.

</details>


### [67] [Rethinking HSM and TPM Security in the Cloud: Real-World Attacks and Next-Gen Defenses](https://arxiv.org/abs/2507.17655)
*Shams Shaikh,Trima P. Fernandes e Fizardo*

Main category: cs.CR

TL;DR: 论文分析了云环境中HSMs和TPMs的安全挑战，提出了替代方案以适应现代云安全需求。


<details>
  <summary>Details</summary>
Motivation: 随着组织快速迁移至云端，密钥管理的安全性问题日益突出，传统的HSMs和TPMs在云环境中面临新威胁。

Method: 分析HSMs和TPMs的安全漏洞，探讨机密计算、后量子密码和去中心化密钥管理等替代方案。

Result: 研究发现HSMs和TPMs仍有价值，但现代云安全需要更灵活的分层架构。

Conclusion: 研究为云架构师和安全工程师提供了强化加密信任的策略，以应对不断演变的威胁。

Abstract: As organizations rapidly migrate to the cloud, the security of cryptographic
key management has become a growing concern. Hardware Security Modules (HSMs)
and Trusted Platform Modules (TPMs), traditionally seen as the gold standard
for securing encryption keys and digital trust, are increasingly challenged by
cloud-native threats. Real-world breaches have exposed weaknesses in cloud
deployments, including misconfigurations, API abuse, and privilege escalations,
allowing attackers to access sensitive key material and bypass protections.
These incidents reveal that while the hardware remains secure, the surrounding
cloud ecosystem introduces systemic vulnerabilities. This paper analyzes
notable security failures involving HSMs and TPMs, identifies common attack
vectors, and questions longstanding assumptions about their effectiveness in
distributed environments. We explore alternative approaches such as
confidential computing, post-quantum cryptography, and decentralized key
management. Our findings highlight that while HSMs and TPMs still play a role,
modern cloud security requires more adaptive, layered architectures. By
evaluating both current weaknesses and emerging models, this research equips
cloud architects and security engineers with strategies to reinforce
cryptographic trust in the evolving threat landscape.

</details>


### [68] [Explainable Vulnerability Detection in C/C++ Using Edge-Aware Graph Attention Networks](https://arxiv.org/abs/2507.16540)
*Radowanul Haque,Aftab Ali,Sally McClean,Naveed Khan*

Main category: cs.CR

TL;DR: ExplainVulD是一种基于图的框架，用于检测C/C++代码中的安全漏洞，通过双通道嵌入和边缘感知注意力机制解决类别不平衡问题，并提供可解释的输出。


<details>
  <summary>Details</summary>
Motivation: 真实数据集中漏洞函数占比低导致类别不平衡，现有学习方法召回率高但误报率高且缺乏可解释性，阻碍其在安全工作中的集成。

Method: 构建代码属性图，使用双通道嵌入表示节点（捕捉语义和结构信息），并通过边缘感知注意力机制处理边类型嵌入。采用类别加权交叉熵损失解决类别不平衡。

Result: 在ReVeal数据集上，ExplainVulD平均准确率88.25%，F1得分48.23%，相对ReVeal模型分别提升4.6%和16.9%，并显著优于静态分析工具。

Conclusion: ExplainVulD不仅提高了检测性能，还能提供可解释的输出，增强安全排障的透明度和信任度。

Abstract: Detecting security vulnerabilities in source code remains challenging,
particularly due to class imbalance in real-world datasets where vulnerable
functions are under-represented. Existing learning-based methods often optimise
for recall, leading to high false positive rates and reduced usability in
development workflows. Furthermore, many approaches lack explainability,
limiting their integration into security workflows. This paper presents
ExplainVulD, a graph-based framework for vulnerability detection in C/C++ code.
The method constructs Code Property Graphs and represents nodes using
dual-channel embeddings that capture both semantic and structural information.
These are processed by an edge-aware attention mechanism that incorporates
edge-type embeddings to distinguish among program relations. To address class
imbalance, the model is trained using class-weighted cross-entropy loss.
ExplainVulD achieves a mean accuracy of 88.25 percent and an F1 score of 48.23
percent across 30 independent runs on the ReVeal dataset. These results
represent relative improvements of 4.6 percent in accuracy and 16.9 percent in
F1 score compared to the ReVeal model, a prior learning-based method. The
framework also outperforms static analysis tools, with relative gains of 14.0
to 14.1 percent in accuracy and 132.2 to 201.2 percent in F1 score. Beyond
improved detection performance, ExplainVulD produces explainable outputs by
identifying the most influential code regions within each function, supporting
transparency and trust in security triage.

</details>


### [69] [Revisiting Pre-trained Language Models for Vulnerability Detection](https://arxiv.org/abs/2507.16887)
*Youpeng Li,Weiliang Qi,Xuyu Wang,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: 该论文评估了17种预训练语言模型（PLMs）在漏洞检测中的表现，发现专注于代码语法和语义的模型表现更优，但在实际应用中仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估PLMs在真实世界漏洞检测中的有效性，发现现有研究在数据准备、评估设置和实验设计上的不足。

Method: 引入RevisitVD框架，通过新建数据集，比较PLM在微调和提示工程下的表现，并分析其对代码规范化、抽象化和语义保持变换的鲁棒性。

Result: 研究发现，专注于代码语法和语义的PLMs表现更优，但在处理复杂依赖、代码扰动和有限上下文窗口时存在挑战。

Conclusion: 结论强调了在实际场景中全面评估模型性能的重要性，并提出了未来提升PLMs在漏洞检测中有效性的方向。

Abstract: The rapid advancement of pre-trained language models (PLMs) has demonstrated
promising results for various code-related tasks. However, their effectiveness
in detecting real-world vulnerabilities remains a critical challenge. % for the
security community. While existing empirical studies evaluate PLMs for
vulnerability detection (VD), their inadequate consideration in data
preparation, evaluation setups, and experimental settings undermines the
accuracy and comprehensiveness of evaluations. This paper introduces RevisitVD,
an extensive evaluation of 17 PLMs spanning smaller code-specific PLMs and
large-scale PLMs using newly constructed datasets. Specifically, we compare the
performance of PLMs under both fine-tuning and prompt engineering, assess their
effectiveness and generalizability across various training and testing
settings, and analyze their robustness against code normalization, abstraction,
and semantic-preserving transformations.
  Our findings reveal that, for VD tasks, PLMs incorporating pre-training tasks
designed to capture the syntactic and semantic patterns of code outperform both
general-purpose PLMs and those solely pre-trained or fine-tuned on large code
corpora. However, these models face notable challenges in real-world scenarios,
such as difficulties in detecting vulnerabilities with complex dependencies,
handling perturbations introduced by code normalization and abstraction, and
identifying semantic-preserving vulnerable code transformations. Also, the
truncation caused by the limited context windows of PLMs can lead to a
non-negligible amount of labeling errors. This study underscores the importance
of thorough evaluations of model performance in practical scenarios and
outlines future directions to help enhance the effectiveness of PLMs for
realistic VD applications.

</details>


### [70] [Enabling Cyber Security Education through Digital Twins and Generative AI](https://arxiv.org/abs/2507.17518)
*Vita Santa Barletta,Vito Bavaro,Miriana Calvano,Antonio Curci,Antonio Piccinno,Davide Pio Posa*

Main category: cs.CR

TL;DR: 该研究探讨了如何将数字孪生（DTs）与渗透测试工具及大型语言模型（LLMs）结合，以提升网络安全教育和操作准备能力。


<details>
  <summary>Details</summary>
Motivation: 通过模拟真实的网络环境，研究旨在提供一种实用、互动式的框架，帮助学习者探索漏洞和防御策略，弥补理论知识与实际应用之间的差距。

Method: 研究开发了Red Team Knife（RTK），一种基于网络杀伤链模型的渗透测试工具包，并与数字孪生和大型语言模型结合，提供实时反馈和自适应学习支持。

Result: 初步结果表明，该集成框架显著提高了网络安全培训的效果和相关性，增强了学习者在漏洞评估、威胁检测和安全操作方面的实践能力。

Conclusion: 数字孪生和大型语言模型的结合能够革新网络安全教育，满足行业不断变化的需求。

Abstract: Digital Twins (DTs) are gaining prominence in cybersecurity for their ability
to replicate complex IT (Information Technology), OT (Operational Technology),
and IoT (Internet of Things) infrastructures, allowing for real time
monitoring, threat analysis, and system simulation. This study investigates how
integrating DTs with penetration testing tools and Large Language Models (LLMs)
can enhance cybersecurity education and operational readiness. By simulating
realistic cyber environments, this approach offers a practical, interactive
framework for exploring vulnerabilities and defensive strategies. At the core
of this research is the Red Team Knife (RTK), a custom penetration testing
toolkit aligned with the Cyber Kill Chain model. RTK is designed to guide
learners through key phases of cyberattacks, including reconnaissance,
exploitation, and response within a DT powered ecosystem. The incorporation of
Large Language Models (LLMs) further enriches the experience by providing
intelligent, real-time feedback, natural language threat explanations, and
adaptive learning support during training exercises. This combined DT LLM
framework is currently being piloted in academic settings to develop hands on
skills in vulnerability assessment, threat detection, and security operations.
Initial findings suggest that the integration significantly improves the
effectiveness and relevance of cybersecurity training, bridging the gap between
theoretical knowledge and real-world application. Ultimately, the research
demonstrates how DTs and LLMs together can transform cybersecurity education to
meet evolving industry demands.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [71] [A Virtual Quantum Network Prototype for Open Access](https://arxiv.org/abs/2507.17495)
*Raj Kamleshkumar Madhu,Visuttha Manthamkarn,Zheshen Zhang,Jianqing Liu*

Main category: quant-ph

TL;DR: 论文提出了一种基于软件的量子网络虚拟化平台，旨在解决量子网络系统的可扩展性和远程访问问题。平台通过云计算虚拟化核心硬件组件，并利用匈牙利算法确保资源公平分配。


<details>
  <summary>Details</summary>
Motivation: 当前量子网络系统规模有限、高度定制化且缺乏全球扩展路径，主要受限于专业人员短缺、基础设施获取困难及高昂成本。

Method: 开发了一个开源的量子网络虚拟化平台，通过云应用虚拟化实验室规模的量子网络测试台核心硬件组件，包括时间标记器和光学开关，并采用匈牙利算法确保用户间的公平资源分配。

Result: 性能分析表明，该原型在硬件、软件和云平台方面均表现出功能性和高效性。

Conclusion: 该平台为量子网络的可扩展性和远程访问提供了可行的解决方案，有望推动量子网络的广泛应用。

Abstract: The rise of quantum networks has revolutionized domains such as
communication, sensing, and cybersecurity. Despite this progress, current
quantum network systems remain limited in scale, are highly
application-specific (e.g., for quantum key distribution), and lack a clear
road map for global expansion. These limitations are largely driven by a
shortage of skilled professionals, limited accessibility to quantum
infrastructure, and the high complexity and cost associated with building and
operating quantum hardware. To address these challenges, this paper proposes an
open-access software-based quantum network virtualization platform designed to
facilitate scalable and remote interaction with quantum hardware. The system is
built around a cloud application that virtualizes the core hardware components
of a lab-scale quantum network testbed, including the time tagger and optical
switch, enabling users to perform coincidence counts of the photon
entanglements while ensuring fair resource allocation. The fairness is ensured
by employing the Hungarian Algorithm to allocate nearly equal effective
entanglement rates among users. We provide implementation details and
performance analysis from the perspectives of hardware, software, and cloud
platform, which demonstrates the functionality and efficiency of the developed
prototype.

</details>


### [72] [Comparing performance of variational quantum algorithm simulations on HPC systems](https://arxiv.org/abs/2507.17614)
*Marco De Pascale,Tobias Valentin Bauer,Yaknan John Gambo,Mario Hernández Vera,Stefan Huber,Burak Mete,Amit Jamadagni,Amine Bentellis,Marita Oliv,Luigi Iapichino,Jeanette Miriam Lorenz*

Main category: quant-ph

TL;DR: 论文提出了一种通用描述方法，用于在量子计算模拟器间一致地转换问题定义，并通过三个用例验证其有效性，同时指出了变分量子算法的扩展性限制。


<details>
  <summary>Details</summary>
Motivation: 变分量子算法在当前NISQ设备中具有重要意义，但参数空间大导致不同模拟器的结果比较困难且易出错。

Method: 使用通用描述方法（包括哈密顿量和ansatz定义）在多个HPC系统和软件模拟器上运行三个用例（氢分子基态计算、MaxCut、旅行商问题）。

Result: 工具链能成功在不同模拟器间转换问题定义，但变分算法因运行时间长限制了扩展性，部分通过作业数组等技术缓解。

Conclusion: 解析工具在探索HPC性能和变分算法结果比较方面具有潜力。

Abstract: Variational quantum algorithms are of special importance in the research on
quantum computing applications because of their applicability to current Noisy
Intermediate-Scale Quantum (NISQ) devices. The main building blocks of these
algorithms (among them, the definition of the Hamiltonian and of the ansatz,
the optimizer) define a relatively large parameter space, making the comparison
of results and performance between different approaches and software simulators
cumbersome and prone to errors. In this paper, we employ a generic description
of the problem, in terms of both Hamiltonian and ansatz, to port a problem
definition consistently among different simulators. Three use cases of
relevance for current quantum hardware (ground state calculation for the
Hydrogen molecule, MaxCut, Travelling Salesman Problem) have been run on a set
of HPC systems and software simulators to study the dependence of performance
on the runtime environment, the scalability of the simulation codes and the
mutual agreement of the physical results, respectively. The results show that
our toolchain can successfully translate a problem definition between different
simulators. On the other hand, variational algorithms are limited in their
scaling by the long runtimes with respect to their memory footprint, so they
expose limited parallelism to computation. This shortcoming is partially
mitigated by using techniques like job arrays. The potential of the parser tool
for exploring HPC performance and comparisons of results of variational
algorithm simulations is highlighted.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [73] [AI in Design Education at College Level-Educators' Perspectives and Challenges](https://arxiv.org/abs/2507.17481)
*Lizhu Zhang,Cecilia X. Wang*

Main category: cs.CY

TL;DR: 研究探讨了AI在设计教育中的角色、教育者的观点及挑战，发现AI已成为设计教育的重要工具和理论框架。


<details>
  <summary>Details</summary>
Motivation: 探索AI对设计教育的影响及教育者的看法。

Method: 通过半结构化深度访谈美国设计学院的七名教师。

Result: AI被广泛应用于设计教育中，教育者需先掌握基本设计技能。

Conclusion: 需促进教育者与AI的合作，并关注伦理和版权问题。

Abstract: Artificial intelligence has deeply permeated numerous fields, especially the
design area which relies on technology as a tool for innovation. This change
naturally extends to the field of design education, which is closest to design
practice. This has led to further exploration of the impact of AI on
college-level education in the design discipline. This study aims to examine
how current design educators perceive the role of AI in college-level design
education, their perspectives on integrating AI into teaching and research, and
their concerns regarding its potential challenges in design education and
research. Through qualitative, semi-structured, in-depth interviews with seven
faculties in U.S. design colleges, the findings reveal that AI, as a tool and
source of information, has become an integral part of design education. AI-
derived functionalities are increasingly utilized in design software, and
educators are actively incorporating AI as a theoretical framework in their
teaching. Educators can guide students in using AI tools, but only if they
first acquire a strong foundation in basic design principles and skills. This
study also indicates the importance of promoting a cooperative relationship
between design educators and AI. At the same time, educators express
anticipation for advancements in ethical standards, authenticity, and the
resolution of copyright issues related to AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [74] [Efficient Neural Network Verification via Order Leading Exploration of Branch-and-Bound Trees](https://arxiv.org/abs/2507.17453)
*Guanqin Zhang,Kota Fukuda,Zhenya Zhang,H. M. N. Dilum Bandara,Shiping Chen,Jianjun Zhao,Yulei Sui*

Main category: cs.LG

TL;DR: 论文提出Oliva框架，通过优先处理更可能包含反例的子问题，提升神经网络验证效率，实验验证其速度提升效果显著。


<details>
  <summary>Details</summary>
Motivation: 神经网络对对抗性扰动的脆弱性需要严格的验证技术，而现有分支定界方法效率不足。

Method: 提出基于子问题优先级的验证框架Oliva，包括贪婪策略和模拟退火策略。

Result: 在MNIST和CIFAR10数据集上，Oliva速度提升最高达25倍和80倍。

Conclusion: Oliva能高效验证神经网络，显著优于现有方法。

Abstract: The vulnerability of neural networks to adversarial perturbations has
necessitated formal verification techniques that can rigorously certify the
quality of neural networks. As the state-of-the-art, branch and bound (BaB) is
a "divide-and-conquer" strategy that applies off-the-shelf verifiers to
sub-problems for which they perform better. While BaB can identify the
sub-problems that are necessary to be split, it explores the space of these
sub-problems in a naive "first-come-first-serve" manner, thereby suffering from
an issue of inefficiency to reach a verification conclusion. To bridge this
gap, we introduce an order over different sub-problems produced by BaB,
concerning with their different likelihoods of containing counterexamples.
Based on this order, we propose a novel verification framework Oliva that
explores the sub-problem space by prioritizing those sub-problems that are more
likely to find counterexamples, in order to efficiently reach the conclusion of
the verification. Even if no counterexample can be found in any sub-problem, it
only changes the order of visiting different sub-problem and so will not lead
to a performance degradation. Specifically, Oliva has two variants, including
$Oliva^{GR}$, a greedy strategy that always prioritizes the sub-problems that
are more likely to find counterexamples, and $Oliva^{SA}$, a balanced strategy
inspired by simulated annealing that gradually shifts from exploration to
exploitation to locate the globally optimal sub-problems. We experimentally
evaluate the performance of Oliva on 690 verification problems spanning over 5
models with datasets MNIST and CIFAR10. Compared to the state-of-the-art
approaches, we demonstrate the speedup of Oliva for up to 25X in MNIST, and up
to 80X in CIFAR10.

</details>


### [75] [Enhancing Quantum Federated Learning with Fisher Information-Based Optimization](https://arxiv.org/abs/2507.17580)
*Amandeep Singh Bhatia,Sabre Kais*

Main category: cs.LG

TL;DR: 提出了一种基于Fisher信息的量子联邦学习（QFL）算法，解决了传统联邦学习中的通信成本、数据异构性和隐私问题，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）虽受欢迎，但面临通信成本高、数据异构性、隐私威胁等问题。结合量子计算与Fisher信息，旨在提升模型性能并保护数据隐私。

Method: 利用Fisher信息量化量子状态的几何和统计特性，设计QFL算法，识别并保留对模型性能关键的参数，在异构数据分布式环境下进行训练。

Result: 在ADNI和MNIST数据集上的实验表明，QFL算法在性能和鲁棒性上优于传统的量子联邦平均方法。

Conclusion: 量子联邦学习结合Fisher信息是一种有效且可行的方法，能够提升模型性能并应对联邦学习中的挑战。

Abstract: Federated Learning (FL) has become increasingly popular across different
sectors, offering a way for clients to work together to train a global model
without sharing sensitive data. It involves multiple rounds of communication
between the global model and participating clients, which introduces several
challenges like high communication costs, heterogeneous client data, prolonged
processing times, and increased vulnerability to privacy threats. In recent
years, the convergence of federated learning and parameterized quantum circuits
has sparked significant research interest, with promising implications for
fields such as healthcare and finance. By enabling decentralized training of
quantum models, it allows clients or institutions to collaboratively enhance
model performance and outcomes while preserving data privacy. Recognizing that
Fisher information can quantify the amount of information that a quantum state
carries under parameter changes, thereby providing insight into its geometric
and statistical properties. We intend to leverage this property to address the
aforementioned challenges. In this work, we propose a Quantum Federated
Learning (QFL) algorithm that makes use of the Fisher information computed on
local client models, with data distributed across heterogeneous partitions.
This approach identifies the critical parameters that significantly influence
the quantum model's performance, ensuring they are preserved during the
aggregation process. Our research assessed the effectiveness and feasibility of
QFL by comparing its performance against other variants, and exploring the
benefits of incorporating Fisher information in QFL settings. Experimental
results on ADNI and MNIST datasets demonstrate the effectiveness of our
approach in achieving better performance and robustness against the quantum
federated averaging method.

</details>


### [76] [Eco-Friendly AI: Unleashing Data Power for Green Federated Learning](https://arxiv.org/abs/2507.17241)
*Mattia Sabella,Monica Vitali*

Main category: cs.LG

TL;DR: 论文提出了一种数据中心的绿色联邦学习方法，通过减少训练数据量来降低联邦学习的环境影响。


<details>
  <summary>Details</summary>
Motivation: 人工智能和机器学习的广泛应用带来了显著的环境影响，尤其是在能源消耗和碳排放方面。联邦学习虽降低了数据传输成本并增强隐私，但其数据源的异质性和环境问题仍需解决。

Method: 分析联邦数据集的特征，基于质量指标选择最优数据子集，并选择环境影响最小的联邦节点，开发推荐系统优化配置。

Result: 时间序列分类应用中，该方法在减少联邦学习任务的环境影响方面表现出良好效果。

Conclusion: 数据中心的绿色联邦学习方法能有效降低环境影响，为绿色AI提供了新思路。

Abstract: The widespread adoption of Artificial Intelligence (AI) and Machine Learning
(ML) comes with a significant environmental impact, particularly in terms of
energy consumption and carbon emissions. This pressing issue highlights the
need for innovative solutions to mitigate AI's ecological footprint. One of the
key factors influencing the energy consumption of ML model training is the size
of the training dataset. ML models are often trained on vast amounts of data
continuously generated by sensors and devices distributed across multiple
locations. To reduce data transmission costs and enhance privacy, Federated
Learning (FL) enables model training without the need to move or share raw
data. While FL offers these advantages, it also introduces challenges due to
the heterogeneity of data sources (related to volume and quality),
computational node capabilities, and environmental impact.
  This paper contributes to the advancement of Green AI by proposing a
data-centric approach to Green Federated Learning. Specifically, we focus on
reducing FL's environmental impact by minimizing the volume of training data.
Our methodology involves the analysis of the characteristics of federated
datasets, the selecting of an optimal subset of data based on quality metrics,
and the choice of the federated nodes with the lowest environmental impact. We
develop a comprehensive methodology that examines the influence of data-centric
factors, such as data quality and volume, on FL training performance and carbon
emissions. Building on these insights, we introduce an interactive
recommendation system that optimizes FL configurations through data reduction,
minimizing environmental impact during training. Applying this methodology to
time series classification has demonstrated promising results in reducing the
environmental impact of FL tasks.

</details>


### [77] [P3SL: Personalized Privacy-Preserving Split Learning on Heterogeneous Edge Devices](https://arxiv.org/abs/2507.17228)
*Wei Fan,JinYi Yoon,Xiaochang Li,Huajie Shao,Bo Ji*

Main category: cs.LG

TL;DR: P3SL是一个面向异构边缘设备的个性化隐私保护分割学习框架，支持客户自定义隐私需求与本地模型，同时通过双层优化技术确定最佳分割点。


<details>
  <summary>Details</summary>
Motivation: 现有分割学习框架在异构环境中忽视个性化隐私需求和本地模型定制，P3SL旨在解决这一问题。

Method: 设计了支持个性化隐私保护和本地模型定制的分割学习流程，并采用双层优化技术让客户自主选择分割点。

Result: 在7种设备上测试显示，P3SL在平衡能耗、隐私泄漏与模型准确性方面表现优异。

Conclusion: P3SL有效解决了异构边缘设备在隐私保护与模型性能间的权衡问题。

Abstract: Split Learning (SL) is an emerging privacy-preserving machine learning
technique that enables resource constrained edge devices to participate in
model training by partitioning a model into client-side and server-side
sub-models. While SL reduces computational overhead on edge devices, it
encounters significant challenges in heterogeneous environments where devices
vary in computing resources, communication capabilities, environmental
conditions, and privacy requirements. Although recent studies have explored
heterogeneous SL frameworks that optimize split points for devices with varying
resource constraints, they often neglect personalized privacy requirements and
local model customization under varying environmental conditions. To address
these limitations, we propose P3SL, a Personalized Privacy-Preserving Split
Learning framework designed for heterogeneous, resource-constrained edge device
systems. The key contributions of this work are twofold. First, we design a
personalized sequential split learning pipeline that allows each client to
achieve customized privacy protection and maintain personalized local models
tailored to their computational resources, environmental conditions, and
privacy needs. Second, we adopt a bi-level optimization technique that empowers
clients to determine their own optimal personalized split points without
sharing private sensitive information (i.e., computational resources,
environmental conditions, privacy requirements) with the server. This approach
balances energy consumption and privacy leakage risks while maintaining high
model accuracy. We implement and evaluate P3SL on a testbed consisting of 7
devices including 4 Jetson Nano P3450 devices, 2 Raspberry Pis, and 1 laptop,
using diverse model architectures and datasets under varying environmental
conditions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [78] [The Wilhelm Tell Dataset of Affordance Demonstrations](https://arxiv.org/abs/2507.17401)
*Rachel Ringe,Mihai Pomarlan,Nikolaos Tsiogkas,Stefano De Giorgis,Maria Hedblom,Rainer Malaka*

Main category: cs.RO

TL;DR: 该论文提出了一种新的数据集，用于学习常见家庭任务中的affordances，区别于以往基于静态图像或形状的方法。


<details>
  <summary>Details</summary>
Motivation: 为了改进机器人在人类环境中感知行动可能性的能力。

Method: 通过收集视频序列（包含第一和第三人称视角）及任务中的affordances元数据，构建数据集。

Result: 数据集包含约7小时的人类活动记录，展示了任务执行和预备动作。

Conclusion: 该数据集有助于训练感知系统识别affordances，对协作服务机器人尤其重要。

Abstract: Affordances - i.e. possibilities for action that an environment or objects in
it provide - are important for robots operating in human environments to
perceive. Existing approaches train such capabilities on annotated static
images or shapes. This work presents a novel dataset for affordance learning of
common household tasks. Unlike previous approaches, our dataset consists of
video sequences demonstrating the tasks from first- and third-person
perspectives, along with metadata about the affordances that are manifested in
the task, and is aimed towards training perception systems to recognize
affordance manifestations. The demonstrations were collected from several
participants and in total record about seven hours of human activity. The
variety of task performances also allows studying preparatory maneuvers that
people may perform for a task, such as how they arrange their task space, which
is also relevant for collaborative service robots.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: 本文提出了一种自动混合接地的解决方案，通过基于数据结构的启发式算法决定何时使用体解耦接地或标准自底向上接地。


<details>
  <summary>Details</summary>
Motivation: 解决接地瓶颈问题，促进答案集编程在工业中的广泛应用。

Method: 引入了一种基于规则结构和实例数据的启发式分裂算法，用于决定接地策略。

Result: 实验结果表明，在难以接地的情况下表现优异，而在难以求解的情况下接近现有最佳性能。

Conclusion: 自动混合接地方案在提升接地效率和性能方面具有潜力。

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [80] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: 研究了基于知识依赖性的控制查询评估，结合最优GA传感器和BUCQs查询，确保安全性并优化计算效率，提出了适用于特定场景的一阶重写算法。


<details>
  <summary>Details</summary>
Motivation: 探究在知识依赖性框架下如何安全有效地控制查询结果，以保护敏感信息并优化计算性能。

Method: 结合最优GA传感器和布尔联合查询（BUCQs），提出了一种基于交集的安全评估方法，并设计了一阶重写算法。

Result: 证明了该方法在DL-Lite_R知识库中的高效性（AC^0数据复杂度），并通过实验验证了其实际可行性。

Conclusion: 该方法在特定条件下能同时满足安全性和计算效率，为控制查询评估提供了实用解决方案。

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [81] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: 论文探讨了将AI整合到车辆中作为感知平台的重要性，旨在推动从被动维护到主动维护的转变，并促进跨学科对话。


<details>
  <summary>Details</summary>
Motivation: 将AI引入车辆，使其成为感知平台，从而将车辆维护从被动转变为主动。

Method: 提供概念和技术视角，旨在激发跨学科对话，并指导未来研究。

Result: 提出了AI助手同时理解机器和驾驶员需求的观点。

Conclusion: 未来研究方向包括智能车辆系统、预测性维护和AI驱动的用户交互。

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [82] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: LLM驱动的自主代理结合实时优化算法，提出“共生代理”范式，提升6G网络实时决策能力，显著减少错误并降低资源开销。


<details>
  <summary>Details</summary>
Motivation: 通过结合LLM与实时优化算法，从专用智能转向更广泛的AGI驱动网络，以提升网络管理的适应性和效率。

Method: 设计两种代理（无线接入网优化器和SLA多代理协商器），提出端到端AGI网络架构，并在5G测试床上验证。

Result: 共生代理比独立LLM代理减少决策错误5倍，小模型资源开销降低99.9%，多代理协作减少RAN过载约44%。

Conclusion: 共生代理为下一代AGI驱动网络提供灵活高效的解决方案，确保在LLM发展中保持适应性和可信性。

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>
