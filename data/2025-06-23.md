<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 20]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 24]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.AR](#cs.AR) [Total: 6]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.RO](#cs.RO) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.CC](#cs.CC) [Total: 2]
- [math.LO](#math.LO) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [eess.SY](#eess.SY) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How Do Community Smells Influence Self-Admitted Technical Debt in Machine Learning Projects?](https://arxiv.org/abs/2506.15884)
*Shamse Tasnim Cynthia,Nuri Almarimi,Banani Roy*

Main category: cs.SE

TL;DR: 研究了开源ML项目中社区气味与自承技术债(SATD)的关系，发现某些气味(如Radio Silence)与SATD高度相关，强调了早期检测和缓解的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习项目中社区气味与SATD的关系，填补现有研究的空白。

Method: 分析了155个ML项目的发布数据，检测了10种社区气味和SATD，并进行了统计分析和演化趋势研究。

Result: 某些社区气味(如Radio Silence)与SATD显著相关，且气味与SATD的演化趋势与项目规模相关。

Conclusion: 强调了早期检测和缓解社区气味对ML项目长期质量和可持续性的重要性。

Abstract: Community smells reflect poor organizational practices that often lead to
socio-technical issues and the accumulation of Self-Admitted Technical Debt
(SATD). While prior studies have explored these problems in general software
systems, their interplay in machine learning (ML)-based projects remains
largely underexamined. In this study, we investigated the prevalence of
community smells and their relationship with SATD in open-source ML projects,
analyzing data at the release level. First, we examined the prevalence of ten
community smell types across the releases of 155 ML-based systems and found
that community smells are widespread, exhibiting distinct distribution patterns
across small, medium, and large projects. Second, we detected SATD at the
release level and applied statistical analysis to examine its correlation with
community smells. Our results showed that certain smells, such as Radio Silence
and Organizational Silos, are strongly correlated with higher SATD occurrences.
Third, we considered the six identified types of SATD to determine which
community smells are most associated with each debt category. Our analysis
revealed authority- and communication-related smells often co-occur with
persistent code and design debt. Finally, we analyzed how the community smells
and SATD evolve over the releases, uncovering project size-dependent trends and
shared trajectories. Our findings emphasize the importance of early detection
and mitigation of socio-technical issues to maintain the long-term quality and
sustainability of ML-based systems.

</details>


### [2] [Regression Testing Optimization for ROS-based Autonomous Systems: A Comprehensive Review of Techniques](https://arxiv.org/abs/2506.16101)
*Yupeng Jiang,Shuaiyi Sun,Xi Zheng*

Main category: cs.SE

TL;DR: 该论文对基于ROS的自主系统（ROSAS）的回归测试优化技术进行了首次系统调查，分析了122项代表性研究，并提出了分类和挑战，为未来研究方向提供了参考。


<details>
  <summary>Details</summary>
Motivation: 由于ROSAS的动态性、非确定性行为、复杂的多模态传感器数据以及严格的安全和实时约束，传统回归测试技术难以适用，因此需要专门针对ROSAS的回归测试优化研究。

Method: 通过对122项代表性研究进行系统分析，将其分为回归测试用例优先级排序、最小化和选择方法，并提出结构化分类法。

Result: 提出了ROSAS回归测试的主要挑战和未来研究方向，包括测试优先级排序、冗余测试最小化和受影响的测试用例选择等。

Conclusion: 该调查为ROSAS回归测试优化提供了基础参考和实用路线图，为未来研究指明了方向。

Abstract: Regression testing plays a critical role in maintaining software reliability,
particularly for ROS-based autonomous systems (ROSAS), which frequently undergo
continuous integration and iterative development. However, conventional
regression testing techniques face significant challenges when applied to
autonomous systems due to their dynamic and non-deterministic behaviors,
complex multi-modal sensor data, asynchronous distributed architectures, and
stringent safety and real-time constraints. Although numerous studies have
explored test optimization in traditional software contexts, regression testing
optimization specifically for ROSAS remains largely unexplored. To address this
gap, we present the first comprehensive survey systematically reviewing
regression testing optimization techniques tailored for ROSAS. We analyze and
categorize 122 representative studies into regression test case prioritization,
minimization, and selection methods. A structured taxonomy is introduced to
clearly illustrate their applicability and limitations within ROSAS contexts.
Furthermore, we highlight major challenges specific to regression testing for
ROSAS, including effectively prioritizing tests in response to frequent system
modifications, efficiently minimizing redundant tests, and difficulty in
accurately selecting impacted test cases. Finally, we propose research insights
and identify promising future directions, such as leveraging frame-to-vector
coverage metrics, multi-source foundation models, and neurosymbolic reasoning
to enhance regression testing efficiency and effectiveness. This survey
provides a foundational reference and practical roadmap for advancing the
state-of-the-art in regression testing optimization for ROSAS.

</details>


### [3] [Seeing is Fixing: Cross-Modal Reasoning with Multimodal LLMs for Visual Software Issue Fixing](https://arxiv.org/abs/2506.16136)
*Kai Huang,Jian Zhang,Xiaofei Xie,Chunyang Chen*

Main category: cs.SE

TL;DR: GUIRepair提出了一种跨模态推理方法，通过理解和利用视觉信息解决多模态问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有APR系统在单模态环境下表现良好，但无法处理多模态场景（如包含GUI的问题）。GUIRepair旨在填补这一空白。

Method: 结合Image2Code和Code2Image，前者将GUI图像转为可执行代码以理解问题，后者通过GUI渲染验证修复效果。

Result: GUIRepair在SWE-bench M上表现优异，使用GPT-4o解决157个问题，优于最佳开源基线26个；o4-mini下解决175个，超越商业系统22个。

Conclusion: 跨模态推理的成功证明视觉信息对解决多模态问题的关键作用，GUIRepair为APR提供了新思路。

Abstract: Large language model-(LLM) based automated program repair (APR) techniques
have shown promising results in resolving real-world GitHub issue tasks.
Existing APR systems are primarily evaluated in unimodal settings (e.g.,
SWE-bench). However, these autonomous systems struggle to resolve multimodal
problem scenarios (e.g., SWE-bench M) due to limitations in interpreting and
leveraging visual information. In multimodal scenarios, LLMs need to rely on
visual information in the graphical user interface (GUI) to understand bugs and
generate fixes. To bridge this gap, we propose GUIRepair, a cross-modal
reasoning approach for resolving multimodal issue scenarios by understanding
and capturing visual information. Specifically, GUIRepair integrates two key
components, Image2Code and Code2Image, to enhance fault comprehension and patch
validation. Image2Code extracts relevant project documents based on the issue
report, then applies this domain knowledge to generate the reproduced code
responsible for the visual symptoms, effectively translating GUI images into
executable context for better fault comprehension. Code2Image replays the
visual issue scenario using the reproduced code and captures GUI renderings of
the patched program to assess whether the fix visually resolves the issue,
providing feedback for patch validation. We evaluate GUIRepair on SWE-bench M,
and the approach demonstrates significant effectiveness. When utilizing GPT-4o
as the base model, GUIRepair solves 157 instances, outperforming the best
open-source baseline by 26 instances. Furthermore, when using o4-mini as the
base model, GUIRepair can achieve even better results and solve 175 instances,
outperforming the top commercial system by 22 instances. This emphasizes the
success of our new perspective on incorporating cross-modal reasoning by
understanding and capturing visual information to resolve multimodal issues.

</details>


### [4] [The Technical Debt Gamble: A Case Study on Technical Debt in a Large-Scale Industrial Microservice Architecture](https://arxiv.org/abs/2506.16214)
*Klara Borowa,Andrzej Ratkowski,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 论文研究了大规模微服务系统中的技术债务（TD），发现静态代码分析是发现TD的有效切入点，沟通不足和组织架构不匹配会加剧TD。


<details>
  <summary>Details</summary>
Motivation: 探索微服务架构中技术债务的表现形式及其影响。

Method: 采用混合方法案例研究，结合静态代码分析工具和开发团队的焦点小组讨论。

Result: 发现了技术债务的四种表现形式，并提出管理策略。

Conclusion: 微服务架构中的技术债务需通过综合策略管理，包括改进沟通和组织架构对齐。

Abstract: Microservice architectures provide an intuitive promise of high
maintainability and evolvability due to loose coupling. However, these quality
attributes are notably vulnerable to technical debt (TD). Few studies address
TD in microservice systems, particularly on a large scale. This research
explores how TD manifests in a large-scale microservice-based industrial
system. The research is based on a mixed-method case study of a project
including over 100 microservices and serving over 15k locations. Results are
collected via a quantitative method based static code analyzers combined with
qualitative insights derived from a focus group discussion with the development
team and a follow-up interview with the lead architect of the case study
system. Results show that (1) simple static source code analysis can be an
efficient and effective entry point for holistic TD discovery, (2) inadequate
communication significantly contributes to TD, (3) misalignment between
architectural and organizational structures can exacerbate TD accumulation, (4)
microservices can rapidly cycle through TD accumulation and resolution, a
phenomenon referred to as "microservice architecture technical debt gamble".
Finally, we identify a set of fitting strategies for TD management in
microservice architectures.

</details>


### [5] [Evaluating the Use of LLMs for Documentation to Code Traceability](https://arxiv.org/abs/2506.16440)
*Ebube Alor,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 该论文评估了大型语言模型（LLMs）在文档到代码追踪任务中的表现，发现其能力优于传统基线方法，但在某些方面仍需人工辅助。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在自动化文档与代码追踪方面的潜力，填补当前研究的空白。

Method: 通过系统实验评估三种LLMs在追踪链接识别、关系解释质量和多步骤链重建三个关键能力上的表现。

Result: 最佳LLM在F1分数上显著优于基线方法（如TF-IDF和CodeBERT），但在完全正确的关系解释和多步骤链中间链接的捕获上表现不一。

Conclusion: LLMs在追踪发现方面具有强大潜力，但需结合人工工具设计，并需进一步研究其错误模式。

Abstract: Large Language Models (LLMs) offer new potential for automating
documentation-to-code traceability, yet their capabilities remain
underexplored. We present a comprehensive evaluation of LLMs (Claude 3.5
Sonnet, GPT-4o, and o3-mini) in establishing trace links between various
software documentation (including API references and user guides) and source
code. We create two novel datasets from two open-source projects (Unity Catalog
and Crawl4AI). Through systematic experiments, we assess three key
capabilities: (1) trace link identification accuracy, (2) relationship
explanation quality, and (3) multi-step chain reconstruction. Results show that
the best-performing LLM achieves F1-scores of 79.4% and 80.4% across the two
datasets, substantially outperforming our baselines (TF-IDF, BM25, and
CodeBERT). While fully correct relationship explanations range from 42.9% to
71.1%, partial accuracy exceeds 97%, indicating that fundamental connections
are rarely missed. For multi-step chains, LLMs maintain high endpoint accuracy
but vary in capturing precise intermediate links. Error analysis reveals that
many false positives stem from naming-based assumptions, phantom links, or
overgeneralization of architectural patterns. We demonstrate that task-framing,
such as a one-to-many matching strategy, is critical for performance. These
findings position LLMs as powerful assistants for trace discovery, but their
limitations could necessitate human-in-the-loop tool design and highlight
specific error patterns for future research.

</details>


### [6] [Understanding the Challenges and Promises of Developing Generative AI Apps: An Empirical Study](https://arxiv.org/abs/2506.16453)
*Buthayna AlMulla,Maram Assi,Safwat Hassan*

Main category: cs.SE

TL;DR: 分析ChatGPT引发的Gen-AI移动应用用户评价，提出SARA方法提取用户洞察，并展示LLM在主题提取中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究Gen-AI应用在实际使用中的用户感知和评价，填补相关研究空白。

Method: 采用四阶段SARA方法（选择、获取、精炼、分析），结合LLM技术对676,066条Google Play评论进行分析。

Result: LLM在主题提取中准确率达91%，识别出用户关注的十大主题（如AI性能、内容质量等），并分析其随时间演变。

Conclusion: 研究为开发者和研究者提供了基于用户反馈的实际建议，并揭示了用户期望的变化趋势。

Abstract: The release of ChatGPT in 2022 triggered a rapid surge in generative
artificial intelligence mobile apps (i.e., Gen-AI apps). Despite widespread
adoption, little is known about how end users perceive and evaluate these
Gen-AI functionalities in practice. In this work, we conduct a user-centered
analysis of 676,066 reviews from 173 Gen-AI apps on the Google Play Store. We
introduce a four-phase methodology, SARA (Selection, Acquisition, Refinement,
and Analysis), that enables the systematic extraction of user insights using
prompt-based LLM techniques. First, we demonstrate the reliability of LLMs in
topic extraction, achieving 91% accuracy through five-shot prompting and
non-informative review filtering. Then, we apply this method to the informative
reviews, identify the top 10 user-discussed topics (e.g., AI Performance,
Content Quality, and Content Policy & Censorship) and analyze the key
challenges and emerging opportunities. Finally, we examine how these topics
evolve over time, offering insight into shifting user expectations and
engagement patterns with Gen-AI apps. Based on our findings and observations,
we present actionable implications for developers and researchers.

</details>


### [7] [Scaling GR(1) Synthesis via a Compositional Framework for LTL Discrete Event Control](https://arxiv.org/abs/2506.16557)
*Hernán Gagliardi,Victor Braberman,Sebastian Uchitel*

Main category: cs.SE

TL;DR: 提出了一种基于组合方法的离散事件系统控制器合成方法，利用模块化结构缓解状态爆炸问题，并通过迭代构建最大化允许的安全控制器实现目标。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统整体合成方法易产生的状态爆炸问题，提出利用模块化结构来优化控制器合成过程。

Method: 采用组合方法，通过迭代解决较弱的控制问题，构建最大化允许的安全控制器，并利用观测合成等价性抽象局部事件。

Result: 在MTSA工具中实现了对GR(1)逻辑的合成，解决规模比整体方法大1000倍的问题。

Conclusion: 组合方法的控制器合成能显著提升解决规模，适用于模块化系统的LTL目标实现。

Abstract: We present a compositional approach to controller synthesis of discrete event
system controllers with linear temporal logic (LTL) goals. We exploit the
modular structure of the plant to be controlled, given as a set of labelled
transition systems (LTS), to mitigate state explosion that monolithic
approaches to synthesis are prone to. Maximally permissive safe controllers are
iteratively built for subsets of the plant LTSs by solving weaker control
problems. Observational synthesis equivalence is used to reduce the size of the
controlled subset of the plant by abstracting away local events. The result of
synthesis is also compositional, a set of controllers that when run in parallel
ensure the LTL goal. We implement synthesis in the MTSA tool for an expressive
subset of LTL, GR(1), and show it computes solutions to that can be up to 1000
times larger than those that the monolithic approach can solve.

</details>


### [8] [AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions](https://arxiv.org/abs/2506.16586)
*Ihor Pysmennyi,Roman Kyslyi,Kyrylo Kleshch*

Main category: cs.SE

TL;DR: 研究探讨将AI工具集成到现代分布式软件QA过程中的潜力与挑战，展示其变革性但需战略实施。


<details>
  <summary>Details</summary>
Motivation: 传统QA方法难以应对现代软件的复杂性、规模和快速迭代，AI工具的整合可能提升效率。

Method: 综合分析了AI工具对验证和验证过程的影响，包括多种测试方法，并通过概念验证展示实际应用。

Result: 生成测试用例的失败率仅为8.3%，但发现语义覆盖、黑盒性和可解释性等实际挑战。

Conclusion: AI对QA具有变革潜力，但需解决局限性和开发验证方法。

Abstract: Traditional quality assurance (QA) methods face significant challenges in
addressing the complexity, scale, and rapid iteration cycles of modern software
systems and are strained by limited resources available, leading to substantial
costs associated with poor quality. The object of this research is the Quality
Assurance processes for modern distributed software applications. The subject
of the research is the assessment of the benefits, challenges, and prospects of
integrating modern AI-oriented tools into quality assurance processes. We
performed comprehensive analysis of implications on both verification and
validation processes covering exploratory test analyses, equivalence
partitioning and boundary analyses, metamorphic testing, finding
inconsistencies in acceptance criteria (AC), static analyses, test case
generation, unit test generation, test suit optimization and assessment, end to
end scenario execution. End to end regression of sample enterprise application
utilizing AI-agents over generated test scenarios was implemented as a proof of
concept highlighting practical use of the study. The results, with only 8.3%
flaky executions of generated test cases, indicate significant potential for
the proposed approaches. However, the study also identified substantial
challenges for practical adoption concerning generation of semantically
identical coverage, "black box" nature and lack of explainability from
state-of-the-art Large Language Models (LLMs), the tendency to correct mutated
test cases to match expected results, underscoring the necessity for thorough
verification of both generated artifacts and test execution results. The
research demonstrates AI's transformative potential for QA but highlights the
importance of a strategic approach to implementing these technologies,
considering the identified limitations and the need for developing appropriate
verification methodologies.

</details>


### [9] [LLM-based Satisfiability Checking of String Requirements by Consistent Data and Checker Generation](https://arxiv.org/abs/2506.16639)
*Boqi Chen,Aren A. Babikian,Shuzhao Feng,Dániel Varró,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 该论文提出了一种混合方法，利用大语言模型(LLMs)验证自然语言字符串需求的满足性，并通过生成检查器验证结果准确性。实验表明，LLMs在生成检查器和提高生成一致性字符串的成功率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 验证自然语言字符串需求的满足性具有挑战性，传统方法如SMT求解器存在理论限制且需要大量手动翻译，而LLMs在此任务中的有效性尚未充分研究。

Method: 结合LLMs进行两项任务：1) 生成满足性结果及可能的一致性字符串；2) 生成声明式(SMT)和命令式(Python)检查器以验证结果的正确性。实验评估了四种LLMs。

Result: LLMs在生成检查器和提高一致性字符串生成成功率方面表现优越，某些情况下生成成功率和F1分数比基线方法提升了一倍以上。

Conclusion: 混合方法有效验证了LLMs在自然语言字符串需求满足性任务中的潜力，生成检查器显著提升了性能，为需求工程提供了新的解决方案。

Abstract: Requirements over strings, commonly represented using natural language (NL),
are particularly relevant for software systems due to their heavy reliance on
string data manipulation. While individual requirements can usually be analyzed
manually, verifying properties (e.g., satisfiability) over sets of NL
requirements is particularly challenging. Formal approaches (e.g., SMT solvers)
may efficiently verify such properties, but are known to have theoretical
limitations. Additionally, the translation of NL requirements into formal
constraints typically requires significant manual effort. Recently, large
language models (LLMs) have emerged as an alternative approach for formal
reasoning tasks, but their effectiveness in verifying requirements over strings
is less studied. In this paper, we introduce a hybrid approach that verifies
the satisfiability of NL requirements over strings by using LLMs (1) to derive
a satisfiability outcome (and a consistent string, if possible), and (2) to
generate declarative (i.e., SMT) and imperative (i.e., Python) checkers, used
to validate the correctness of (1). In our experiments, we assess the
performance of four LLMs. Results show that LLMs effectively translate natural
language into checkers, even achieving perfect testing accuracy for
Python-based checkers. These checkers substantially help LLMs in generating a
consistent string and accurately identifying unsatisfiable requirements,
leading to more than doubled generation success rate and F1-score in certain
cases compared to baselines without generated checkers.

</details>


### [10] [SemAgent: A Semantics Aware Program Repair Agent](https://arxiv.org/abs/2506.16650)
*Anvith Pabba,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: 论文提出SemAgent，通过结合问题、代码和执行语义，改进现有自动程序修复（APR）方法的局限性，并在SWEBench-Lite基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有APR方法过于聚焦局部代码修复，缺乏对问题、代码和执行语义的深入理解，导致修复补丁过拟合。

Method: SemAgent采用基于工作流的方法，通过执行语义获取上下文，抽象理解问题语义，隔离代码语义，并通过两阶段架构（修复和审阅）生成更全面的修复补丁。

Result: 在SWEBench-Lite基准测试中，SemAgent的解决率达到44.66%，优于其他工作流方法，且比基准方法提高了7.66%。

Conclusion: 结合问题与代码语义的APR方法能生成更鲁棒且语义一致的修复补丁，尤其在多行推理和边缘情况处理中表现突出。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream
software engineering tasks such as Automated Program Repair (APR). In
particular, there has been a lot of research on repository-level
issue-resolution benchmarks such as SWE-Bench. Although there has been
significant progress on this topic, we notice that in the process of solving
such issues, existing agentic systems tend to hyper-localize on immediately
suspicious lines of code and fix them in isolation, without a deeper
understanding of the issue semantics, code semantics, or execution semantics.
Consequently, many existing systems generate patches that overfit to the user
issue, even when a more general fix is preferable. To address this limitation,
we introduce SemAgent, a novel workflow-based procedure that leverages issue,
code, and execution semantics to generate patches that are complete -
identifying and fixing all lines relevant to the issue. We achieve this through
a novel pipeline that (a) leverages execution semantics to retrieve relevant
context, (b) comprehends issue-semantics via generalized abstraction, (c)
isolates code-semantics within the context of this abstraction, and (d)
leverages this understanding in a two-stage architecture: a repair stage that
proposes fine-grained fixes, followed by a reviewer stage that filters relevant
fixes based on the inferred issue-semantics. Our evaluations show that our
methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark
beating all other workflow-based approaches, and an absolute improvement of
7.66% compared to our baseline, which lacks such deep semantic understanding.
We note that our approach performs particularly well on issues requiring
multi-line reasoning (and editing) and edge-case handling, suggesting that
incorporating issue and code semantics into APR pipelines can lead to robust
and semantically consistent repairs.

</details>


### [11] [LLMs in Coding and their Impact on the Commercial Software Engineering Landscape](https://arxiv.org/abs/2506.16653)
*Vladislav Belozerov,Peter J Barclay,Askhan Sami*

Main category: cs.SE

TL;DR: 论文讨论了大语言模型编码工具的普及及其带来的隐私泄露、安全漏洞和模型迎合错误观点（阿谀奉承）等新风险，提出了企业应采取的措施以确保安全性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型编码工具的广泛应用，其带来的隐私泄露、安全漏洞和模型阿谀奉承等问题日益突出，亟需解决方案。

Method: 建议企业对AI生成的代码进行标记和审查，将提示和输出限制在私有或本地部署中，遵循新兴的安全法规，并增加测试以检测阿谀奉承的回答。

Result: 研究发现，10%的真实提示会泄露隐私数据，42%的生成代码片段隐藏安全漏洞，模型还会表现出阿谀奉承的行为。

Conclusion: 企业需通过严格审查和测试，确保在大语言模型工具的快速开发中不牺牲安全性和准确性。

Abstract: Large-language-model coding tools are now mainstream in software engineering.
But as these same tools move human effort up the development stack, they
present fresh dangers: 10% of real prompts leak private data, 42% of generated
snippets hide security flaws, and the models can even ``agree'' with wrong
ideas, a trait called sycophancy. We argue that firms must tag and review every
AI-generated line of code, keep prompts and outputs inside private or
on-premises deployments, obey emerging safety regulations, and add tests that
catch sycophantic answers -- so they can gain speed without losing security and
accuracy.

</details>


### [12] [Accountability of Robust and Reliable AI-Enabled Systems: A Preliminary Study and Roadmap](https://arxiv.org/abs/2506.16831)
*Filippo Scaramuzza,Damian A. Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 论文探讨了AI系统稳健性、可靠性和问责制的重要性，提出了创新测试方案的必要性，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统的安全性和有效性，特别是在实际应用中的稳健性、可靠性和问责制，以推动可信AI的发展。

Method: 通过文献回顾和案例研究，分析AI系统的稳健性、可靠性及问责制的定义和挑战。

Result: 发现创新测试方案和问责制是确保AI系统可信的关键，并指出了当前研究中的不足。

Conclusion: 未来需进一步研究AI系统的稳健性、可靠性和问责制，这是构建可信AI系统的重要方向。

Abstract: This vision paper presents initial research on assessing the robustness and
reliability of AI-enabled systems, and key factors in ensuring their safety and
effectiveness in practical applications, including a focus on accountability.
By exploring evolving definitions of these concepts and reviewing current
literature, the study highlights major challenges and approaches in the field.
A case study is used to illustrate real-world applications, emphasizing the
need for innovative testing solutions. The incorporation of accountability is
crucial for building trust and ensuring responsible AI development. The paper
outlines potential future research directions and identifies existing gaps,
positioning robustness, reliability, and accountability as vital areas for the
development of trustworthy AI systems of the future.

</details>


### [13] [Revolutionizing Validation and Verification: Explainable Testing Methodologies for Intelligent Automotive Decision-Making Systems](https://arxiv.org/abs/2506.16876)
*Halit Eris,Stefan Wagner*

Main category: cs.SE

TL;DR: 论文提出了一种将可解释性、透明度和可理解性融入自动驾驶系统验证与验证（V&V）流程的方法，旨在提高效率、减少资源消耗并增强用户信任。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的复杂决策模型和多模态输入使其验证与验证过程极具挑战性，现有手动测试方法效率低下且费时费力，亟需更高效的解决方案。

Method: 通过文献综述和利益相关者反馈完善V&V需求，利用大语言模型生成可解释的测试场景，并通过模拟环境实现实时验证。框架包括测试预言、解释生成和测试聊天机器人。

Result: 计划通过实证研究评估框架在诊断效率和透明度方面的改进效果。

Conclusion: 该框架有望简化V&V流程，减少资源消耗，并提升用户对自动驾驶技术的信任。

Abstract: Autonomous Driving Systems (ADS) use complex decision-making (DM) models with
multimodal sensory inputs, making rigorous validation and verification (V&V)
essential for safety and reliability. These models pose challenges in
diagnosing failures, tracing anomalies, and maintaining transparency, with
current manual testing methods being inefficient and labor-intensive. This
vision paper presents a methodology that integrates explainability,
transparency, and interpretability into V&V processes. We propose refining V&V
requirements through literature reviews and stakeholder input, generating
explainable test scenarios via large language models (LLMs), and enabling
real-time validation in simulation environments. Our framework includes test
oracle, explanation generation, and a test chatbot, with empirical studies
planned to evaluate improvements in diagnostic efficiency and transparency. Our
goal is to streamline V&V, reduce resources, and build user trust in autonomous
technologies.

</details>


### [14] [Quantum Optimization for Software Engineering: A Survey](https://arxiv.org/abs/2506.16878)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: 本文献回顾研究量子算法在软件工程优化中的应用，分析了77篇研究，揭示了研究集中在某些领域而其他领域存在空白。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统和工程过程的复杂性需要创新解决方案，量子计算为优化问题提供了潜力。

Method: 系统文献回顾（SLR），从2083篇文献中筛选77篇进行分析。

Result: 研究发现研究集中在软件工程操作和测试领域，其他活动存在显著空白。

Conclusion: 综述为搜索式软件工程（SBSE）社区提供了量子进展的全面视野，助力下一代软件工程挑战。

Abstract: Quantum computing, particularly in the area of quantum optimization, is
steadily progressing toward practical applications, supported by an expanding
range of hardware platforms and simulators. While Software Engineering (SE)
optimization has a strong foundation, which is exemplified by the active
Search-Based Software Engineering (SBSE) community and numerous classical
optimization methods, the growing complexity of modern software systems and
their engineering processes demands innovative solutions. This Systematic
Literature Review (SLR) focuses specifically on studying the literature that
applies quantum or quantum-inspired algorithms to solve classical SE
optimization problems. We examine 77 primary studies selected from an initial
pool of 2083 publications obtained through systematic searches of six digital
databases using carefully crafted search strings. Our findings reveal
concentrated research efforts in areas such as SE operations and software
testing, while exposing significant gaps across other SE activities.
Additionally, the SLR uncovers relevant works published outside traditional SE
venues, underscoring the necessity of this comprehensive review. Overall, our
study provides a broad overview of the research landscape, empowering the SBSE
community to leverage quantum advancements in addressing next-generation SE
challenges.

</details>


### [15] [Identifying Explanation Needs: Towards a Catalog of User-based Indicators](https://arxiv.org/abs/2506.16997)
*Hannah Deters,Laura Reinhardt,Jakob Droste,Martin Obaidi,Kurt Schneider*

Main category: cs.SE

TL;DR: 论文探讨数字化世界中软件系统解释性需求，通过在线研究收集并分类了39个潜在指标，用于运行时触发解释。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂化，解释性需求日益重要，但个体需求提取易受偏见影响。

Method: 通过在线研究收集用户自我报告指标，分类为行为、系统事件和情感/身体反应三类。

Result: 确定了17种用户行为、8种系统事件和14种情感/身体反应指标，并分析了其与解释需求的关系。

Conclusion: 这些指标可用于原型设计中提取需求，或运行时触发解释，提升系统解释性。

Abstract: In today's digitalized world, where software systems are becoming
increasingly ubiquitous and complex, the quality aspect of explainability is
gaining relevance. A major challenge in achieving adequate explanations is the
elicitation of individual explanation needs, as it may be subject to severe
hypothetical or confirmation biases. To address these challenges, we aim to
establish user-based indicators concerning user behavior or system events that
can be captured at runtime to determine when a need for explanations arises. In
this work, we conducted explorative research in form of an online study to
collect self-reported indicators that could indicate a need for explanation. We
compiled a catalog containing 17 relevant indicators concerning user behavior,
8 indicators concerning system events and 14 indicators concerning emotional
states or physical reactions. We also analyze the relationships between these
indicators and different types of need for explanation. The established
indicators can be used in the elicitation process through prototypes, as well
as after publication to gather requirements from already deployed applications
using telemetry and usage data. Moreover, these indicators can be used to
trigger explanations at appropriate moments during the runtime.

</details>


### [16] [Behavior Driven Development for 3D Games](https://arxiv.org/abs/2506.17057)
*Fernando Pastor Ricós,Beatriz Marín,I. S. W. B. Prasetya,Tanja E. J. Vos,Joseph Davidson,Karel Hovorka*

Main category: cs.SE

TL;DR: 本文介绍如何通过将行为驱动开发(BDD)与iv4XR框架结合，自动化3D游戏的回归测试，提升了测试效率与协作性。


<details>
  <summary>Details</summary>
Motivation: 解决传统iv4XR框架在定义测试脚本时对技术专业知识的依赖，促进开发者与测试者之间的协作。

Method: 将BDD方法整合到iv4XR框架中，用于自动化回归测试和游戏测试场景。

Result: 成功应用于Space Engineers和LabRecruits游戏中，提升了测试自动化效率和可读性。

Conclusion: iv4XR框架通过BDD集成展现了其多功能性，同时BDD方法显著提升了测试脚本的可管理性和执行效率。

Abstract: Computer 3D games are complex software environments that require novel
testing processes to ensure high-quality standards. The Intelligent
Verification/Validation for Extended Reality Based Systems (iv4XR) framework
addresses this need by enabling the implementation of autonomous agents to
automate game testing scenarios. This framework facilitates the automation of
regression test cases for complex 3D games like Space Engineers. Nevertheless,
the technical expertise required to define test scripts using iv4XR can
constrain seamless collaboration between developers and testers. This paper
reports how integrating a Behavior-driven Development (BDD) approach with the
iv4XR framework allows the industrial company behind Space Engineers to
automate regression testing. The success of this industrial collaboration has
inspired the iv4XR team to integrate the BDD approach to improve the automation
of play-testing for the experimental 3D game LabRecruits. Furthermore, the
iv4XR framework has been extended with tactical programming to enable the
automation of long-play test scenarios in Space Engineers. These results
underscore the versatility of the iv4XR framework in supporting diverse testing
approaches while showcasing how BDD empowers users to create, manage, and
execute automated game tests using comprehensive and human-readable statements.

</details>


### [17] [Software Fairness Testing in Practice](https://arxiv.org/abs/2506.17095)
*Ronnie de Souza Santos,Matheus de Morais Leca,Reydne Santos,Cleyton Magalhaes*

Main category: cs.SE

TL;DR: 论文探讨了AI系统中的公平性测试理论与实践之间的差距，并强调了需要更实用的工具和指南以帮助工业界采纳。


<details>
  <summary>Details</summary>
Motivation: 随着AI和ML技术成为软件系统的核心，公平性测试成为确保其伦理和公正性的关键，但工业实践中缺乏明确的指导和工具。

Method: 通过访谈22名从事AI和ML项目的专业人士，研究他们在实际工作中如何测试AI系统的公平性。

Result: 研究发现公平性概念难以被实践者解读和应用，且缺乏行业对齐的工具，导致理论成果难以落地。

Conclusion: 需要将学术进展转化为实用工具和策略，以系统性解决AI系统中的公平性问题。

Abstract: Software testing ensures that a system functions correctly, meets specified
requirements, and maintains high quality. As artificial intelligence and
machine learning (ML) technologies become integral to software systems, testing
has evolved to address their unique complexities. A critical advancement in
this space is fairness testing, which identifies and mitigates biases in AI
applications to promote ethical and equitable outcomes. Despite extensive
academic research on fairness testing, including test input generation, test
oracle identification, and component testing, practical adoption remains
limited. Industry practitioners often lack clear guidelines and effective tools
to integrate fairness testing into real-world AI development. This study
investigates how software professionals test AI-powered systems for fairness
through interviews with 22 practitioners working on AI and ML projects. Our
findings highlight a significant gap between theoretical fairness concepts and
industry practice. While fairness definitions continue to evolve, they remain
difficult for practitioners to interpret and apply. The absence of
industry-aligned fairness testing tools further complicates adoption,
necessitating research into practical, accessible solutions. Key challenges
include data quality and diversity, time constraints, defining effective
metrics, and ensuring model interoperability. These insights emphasize the need
to bridge academic advancements with actionable strategies and tools, enabling
practitioners to systematically address fairness in AI systems.

</details>


### [18] [Reassessing Code Authorship Attribution in the Era of Language Models](https://arxiv.org/abs/2506.17120)
*Atish Kumar Dipongkor,Ziyu Yao,Kevin Moran*

Main category: cs.SE

TL;DR: 本文研究了代码风格测定的有效性，特别是代码作者归属（CAA）任务，探讨了传统方法在识别代码作者中的局限性，并首次深入评估了基于Transformer的语言模型（LMs）在CAA任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 代码作者归属在网络安全和软件取证中具有重要应用，但传统方法依赖手工特征且易受对抗性攻击影响。本文旨在探索基于Transformer的语言模型在解决这一任务中的潜力。

Method: 研究采用了两种大型和最先进的代码LMs以及五种小型代码LMs，分析了它们在6个数据集（包含12k代码片段，来自463名开发者）上的表现，并使用机器学习可解释性技术进行了深入分析。

Result: 分析结果揭示了LMs在理解代码风格模式时的行为特征，为未来研究提供了重要方向。

Conclusion: Transformer-based LMs在CAA任务中表现出色，但仍需进一步研究以优化其在复杂场景下的应用。

Abstract: The study of Code Stylometry, and in particular Code Authorship Attribution
(CAA), aims to analyze coding styles to identify the authors of code samples.
CAA is crucial in cybersecurity and software forensics for addressing,
detecting plagiarism, and supporting criminal prosecutions. However, CAA is a
complex and error prone task, due to the need for recognizing nuanced
relationships between coding patterns. This challenge is compounded in large
software systems with numerous authors due to the subtle variability of
patterns that signify the coding style of one author among many. Given the
challenges related to this task, researchers have proposed and studied
automated approaches that rely upon classical Machine Learning and Deep
Learning techniques. However, such techniques have historically relied upon
hand-crafted features, and due to the often intricate interaction of different
features (e.g., formatting, etc.), have key limitations in properly
characterizing authorship, and are sensitive to adversarial code perturbations.
Recently, transformer-based Language Models (LMs) have shown remarkable
efficacy across a range of software engineering tasks, and in the authorship
attribution on natural language in the NLP domain. However, their effectiveness
in CAA is not well understood. As such, we conduct the first extensive
empirical study applying two larger state-of-the-art code LMs, and five smaller
code LMs to the task of CAA to 6 diverse datasets that encompass 12k code
snippets written by 463 developers. Furthermore, we perform an in-depth
analysis of our studied models' performance on CAA using established machine
learning interpretability techniques. The results of our analysis illustrate
important findings that illuminate the behavior of LMs in understanding
stylometric code patterns during the task of CAA, and point towards important
directions for future work.

</details>


### [19] [Large Language Model Unlearning for Source Code](https://arxiv.org/abs/2506.17125)
*Xue Jiang,Yihong Dong,Zheng Fang,Yingwei Ma,Tangxinyu Wang,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: PROD是一种新型的LLM遗忘方法，专门针对源代码问题，能够在消除不必要数据的同时保留模型的代码生成能力。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程中的应用虽成功，但记忆敏感或过时的训练数据可能带来合规性、安全性和代码质量风险，需要有效的遗忘技术。

Method: PROD通过抑制遗忘数据的输出概率并提升候选分布组件，实现同时遗忘特定内容和保留通用能力。

Result: PROD在版权代码、不安全代码和过时API三类任务中表现优于现有方法，同时提升了模型对对抗攻击的鲁棒性。

Conclusion: PROD将遗忘技术应用于源代码领域，为提升可靠代码生成提供了重要启示。

Abstract: LLM4SE has demonstrated significant success, but LLMs' potential memorization
of sensitive or outdated training data introduces critical risks to legal
compliance, software security, and code quality. LLM unlearning techniques,
which can eliminate the influence of undesired data from LLMs in a
post-training way, present a promising solution to address these concerns.
While recent efforts in LLM unlearning show effectiveness in natural language,
their applicability to source code remains underexplored. Our empirical study
reveals that existing LLM unlearning approaches, when applied to source code,
cause severe model utility degradation, rendering models practically unusable
for code generation. In this paper, we propose PROD, a novel unlearning
approach that enables LLMs to forget undesired code content while effectively
preserving their code generation capabilities. PROD suppresses the probability
of forget data in LLMs' output distribution while promoting candidate
distributional components, enabling the model to jointly learn to forget
specific content and retain its general capabilities. To facilitate this study,
we establish a benchmark for code unlearning evaluation, which includes three
critical downstream tasks: copyrighted code unlearning, insecure code
unlearning, and deprecated API unlearning. Our evaluation demonstrates that
PROD achieves superior balance between forget quality and model utility
compared to existing unlearning approaches across three downstream tasks, while
consistently exhibiting improvements when applied to LLMs of varying series.
PROD also exhibits superior robustness against adversarial attacks without
generating or exposing the data to be forgotten. The results underscore that
our approach not only extends the application boundary of unlearning techniques
to source code, but also holds significant implications for advancing reliable
code generation.

</details>


### [20] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 该论文研究了SWE-Bench Lite和Verified上的提交，分析了67种方法，揭示了专有LLM的主导地位以及开发者多样性。


<details>
  <summary>Details</summary>
Motivation: 由于SWE-Bench提交缺乏详细文档，作者希望通过系统性研究揭示其架构设计和来源。

Method: 分析了68个SWE-Bench Lite和79个Verified的提交，从提交者类型、产品可用性、LLM使用和系统架构等维度进行研究。

Result: 发现专有LLM（如Claude 3.5/3.7）占主导地位，设计中有代理和非代理两种模式，提交者从个人开发者到大公司均有。

Conclusion: 研究为APR领域的进展提供了全面洞察，揭示了技术趋势和开发者生态。

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [21] [A System Level Compiler for Massively-Parallel, Spatial, Dataflow Architectures](https://arxiv.org/abs/2506.15875)
*Dirk Van Essendelft,Patrick Wingo,Terry Jordan,Ryan Smith,Wissam Saidi*

Main category: cs.PL

TL;DR: 本文介绍了一种名为MACH的新型编译器，专为大规模并行、空间数据流架构设计，同时支持传统统一内存设备，解决了空间架构编译的复杂性。


<details>
  <summary>Details</summary>
Motivation: 解决为空间架构（如Wafer Scale Engine）编译代码的复杂性，同时支持多架构和灵活数据映射。

Method: 通过概念性虚拟机、灵活领域特定语言和编译器，将高级语言降低为符合虚拟机概念的机器特定代码。

Result: 展示了MACH在NumPy密集张量示例中的应用，并通过目标特定的硬件语言（Cerebras）实现了Wafer Scale Engine的代码生成。

Conclusion: MACH为空间架构提供了一种高效的编译解决方案，同时具备多架构支持和灵活性。

Abstract: We have developed a novel compiler called the Multiple-Architecture Compiler
for Advanced Computing Hardware (MACH) designed specifically for
massively-parallel, spatial, dataflow architectures like the Wafer Scale
Engine. Additionally, MACH can execute code on traditional unified-memory
devices. MACH addresses the complexities in compiling for spatial architectures
through a conceptual Virtual Machine, a flexible domain-specific language, and
a compiler that can lower high-level languages to machine-specific code in
compliance with the Virtual Machine concept. While MACH is designed to be
operable on several architectures and provide the flexibility for several
standard and user-defined data mappings, we introduce the concept with dense
tensor examples from NumPy and show lowering to the Wafer Scale Engine by
targeting Cerebras' hardware specific languages.

</details>


### [22] [WAMI: Compilation to WebAssembly through MLIR without Losing Abstraction](https://arxiv.org/abs/2506.16048)
*Byeongjee Kang,Harsh Desai,Limin Jia,Brandon Lucia*

Main category: cs.PL

TL;DR: 该论文提出了一种新的WebAssembly（Wasm）编译管道，通过MLIR中的Wasm方言直接生成高级Wasm代码，避免了传统LLVM后端的局限性，性能接近甚至优于LLVM。


<details>
  <summary>Details</summary>
Motivation: 为提高对高级语言的支持并减少代码大小或性能开销，Wasm不断集成高级特性（如垃圾回收和堆栈切换），但现有编译方法要么缺乏可重用设计，要么因抽象丢失而难以采用这些特性。

Method: 利用MLIR中的Wasm方言直接生成高级Wasm代码，无需依赖LLVM后端，并通过堆栈切换案例展示了其模块化和可扩展性。

Result: 在PolyBench基准测试中，该管道生成的代码性能最多比基于LLVM的编译器慢7.7%，某些环境下更快。

Conclusion: 该方法是可行的，为高级Wasm特性的实现提供了模块化且高效的途径。

Abstract: WebAssembly (Wasm) is a portable bytecode format that serves as a compilation
target for high-level languages, enabling their secure and efficient execution
across diverse platforms, including web browsers and embedded systems. To
improve support for high-level languages without incurring significant code
size or performance overheads, Wasm continuously evolves by integrating
high-level features such as Garbage Collection and Stack Switching. However,
existing compilation approaches either lack reusable design -- requiring
redundant implementation efforts for each language -- or lose abstraction by
lowering high-level constructs into low-level shared representations like LLVM
IR, which hinder the adoption of high-level features. MLIR compiler
infrastructure provides the compilation pipeline with multiple levels of
abstraction, preserving high-level abstractions throughout the compilation
pipeline, yet the current MLIR pipeline relies on the LLVM backend for Wasm
code generation, thereby inheriting LLVM's limitations.
  This paper presents a novel compilation pipeline for Wasm, featuring Wasm
dialects explicitly designed to represent high-level Wasm constructs within
MLIR. Our approach enables direct generation of high-level Wasm code from
corresponding high-level MLIR dialects without losing abstraction, providing a
modular and extensible way to incorporate high-level Wasm features. We
illustrate this extensibility through a case study that leverages Stack
Switching, a recently introduced high-level feature of Wasm. Performance
evaluations on PolyBench benchmarks show that our pipeline, benefiting from
optimizations within the MLIR and Wasm ecosystems, produces code with at most
7.7\% slower, and faster in some execution environments, compared to LLVM-based
compilers.

</details>


### [23] [Low Overhead Allocation Sampling in a Garbage Collected Virtual Machine](https://arxiv.org/abs/2506.16883)
*Christoph Jung,C. F. Bolz-Tereick*

Main category: cs.PL

TL;DR: 介绍了一种在PyPy虚拟机中集成抽样分配分析器的方法，显著降低了分析开销，最大时间开销仅为25%。


<details>
  <summary>Details</summary>
Motivation: 传统的基于时间的分析在动态类型语言中效率低下，需要一种更高效的分配分析方法。

Method: 通过在PyPy的垃圾回收器中深度集成抽样分配分析器，实现对分配的抽样分析。

Result: 使用4 MB的抽样周期时，最大时间开销为25%，显著优于全部分配分析。

Conclusion: 该方法提供了一种低开销的分配分析解决方案，适用于动态类型语言。

Abstract: Compared to the more commonly used time-based profiling, allocation profiling
provides an alternate view of the execution of allocation heavy dynamically
typed languages. However, profiling every single allocation in a program is
very inefficient. We present a sampling allocation profiler that is deeply
integrated into the garbage collector of PyPy, a Python virtual machine. This
integration ensures tunable low overhead for the allocation profiler, which we
measure and quantify. Enabling allocation sampling profiling with a sampling
period of 4 MB leads to a maximum time overhead of 25% in our benchmarks, over
un-profiled regular execution.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [24] [How to Increase Energy Efficiency with a Single Linux Command](https://arxiv.org/abs/2506.16046)
*Alborz Jelvani,Richard P Martin,Santosh Nagarakatte*

Main category: cs.PF

TL;DR: 本文提出通过简单的电源限制（power capping）来提高处理器能源效率，实验结果显示可达25%的能效提升且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 现有动态电源管理设置无法实现最优能效，本文旨在解决这一问题。

Method: 通过实验验证电源限制机制在服务器系统上的有效性，测试基于SPEC CPU 2017基准。

Result: 电源限制可实现25%的能效提升，性能损失轻微。

Conclusion: 建议采用电源限制作为主要能效提升手段，因其简单且高效。

Abstract: Processors with dynamic power management provide a variety of settings to
control energy efficiency. However, tuning these settings does not achieve
optimal energy savings. We highlight how existing power capping mechanisms can
address these limitations without requiring any changes to current power
governors. We validate this approach using system measurements across a
month-long data acquisition campaign from SPEC CPU 2017 benchmarks on a
server-class system equipped with dual Intel Xeon Scalable processors. Our
results indicate that setting a simple power cap can improve energy efficiency
by up to 25% over traditional energy-saving system configurations with little
performance loss, as most default settings focus on thermal regulation and
performance rather than compute efficiency. Power capping is very accessible
compared to other approaches, as it can be implemented with a single Linux
command. Our results point to programmers and administrators using power caps
as a primary mechanism to maintain significant energy efficiency while
retaining acceptable performance, as opposed to deploying complex DVFS
algorithms.

</details>


### [25] [Dependability of UAV-Based Networks and Computing Systems: A Survey](https://arxiv.org/abs/2506.16786)
*Qingyang Zhang,Mohammad Dwipa Furqan,Tasfia Nutzhat,Fumio Machida,Ermeson Andrade*

Main category: cs.PF

TL;DR: 本文系统综述了无人机（UAV）计算与网络的可靠性问题，总结了威胁类型和应对技术，并提出了未来需要探索的八个研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着无人机系统在多种物理应用中的普及，其可靠性成为关键挑战。本文旨在通过文献综述揭示研究趋势并提出未来方向。

Method: 通过对无人机网络和计算系统的文献进行系统性综述，按威胁类型和技术分类总结现有研究。

Result: 综述揭示了无人机系统的威胁来源（如软件故障、网络中断等）及应对技术，并指出八个未来研究方向。

Conclusion: 无人机系统的可靠性研究仍需深化，未来需在多领域进一步探索以实现高可靠性系统。

Abstract: Uncrewed Aerial Vehicle (UAV) computing and networking are becoming a
fundamental computation infrastructure for diverse cyber-physical application
systems. UAVs can be empowered by AI on edge devices and can communicate with
other UAVs and ground stations via wireless communication networks. Dynamic
computation demands and heterogeneous computing resources are distributed in
the system and need to be controlled to maintain the quality of services and to
accomplish critical missions. With the evolution of UAV-based systems,
dependability assurance of such systems emerges as a crucial challenge.
UAV-based systems confront diverse sources of uncertainty that may threaten
their dependability, such as software bugs, component failures, network
disconnections, battery shortages, and disturbances from the real world. In
this paper, we conduct systematic literature reviews on the dependability of
UAV-based networks and computing systems. The survey report reveals emerging
research trends in this field and summarizes the literature into comprehensive
categories by threat types and adopted technologies. Based on our literature
reviews, we identify eight research fields that require further exploration in
the future to achieve dependable UAV-based systems.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [26] [ROS 2 Agnocast: Supporting Unsized Message Types for True Zero-Copy Publish/Subscribe IPC](https://arxiv.org/abs/2506.16882)
*Takahiro Ishikawa-Aso,Shinpei Kato*

Main category: cs.OS

TL;DR: 论文提出了Agnocast，一种适用于ROS 2 C++的零拷贝IPC框架，解决了现有解决方案无法满足的三个关键需求，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 大规模ROS 2系统中，零拷贝通信对效率和实时性至关重要，但现有方案未能全面支持ROS 2消息类型并且难以无缝集成到现有项目中。

Method: 提出Agnocast框架，支持所有ROS 2消息类型（包括无固定大小的消息），且对现有代码修改极小，可选择性地实现零拷贝通信。

Result: Agnocast实现了与消息大小无关的恒定IPC开销，在Autoware点云预处理中平均响应时间提升16%，最坏情况下提升25%。

Conclusion: Agnocast为ROS 2系统提供了一种高效、无缝集成的零拷贝通信解决方案，满足了实际应用中的关键需求。

Abstract: Robot applications, comprising independent components that mutually
publish/subscribe messages, are built on inter-process communication (IPC)
middleware such as Robot Operating System 2 (ROS 2). In large-scale ROS 2
systems like autonomous driving platforms, true zero-copy communication --
eliminating serialization and deserialization -- is crucial for efficiency and
real-time performance. However, existing true zero-copy middleware solutions
lack widespread adoption as they fail to meet three essential requirements: 1)
Support for all ROS 2 message types including unsized ones; 2) Minimal
modifications to existing application code; 3) Selective implementation of
zero-copy communication between specific nodes while maintaining conventional
communication mechanisms for other inter-node communications including
inter-host node communications. This first requirement is critical, as
production-grade ROS 2 projects like Autoware rely heavily on unsized message
types throughout their codebase to handle diverse use cases (e.g., various
sensors), and depend on the broader ROS 2 ecosystem, where unsized message
types are pervasive in libraries. The remaining requirements facilitate
seamless integration with existing projects. While IceOryx middleware, a
practical true zero-copy solution, meets all but the first requirement, other
studies achieving the first requirement fail to satisfy the remaining criteria.
This paper presents Agnocast, a true zero-copy IPC framework applicable to ROS
2 C++ on Linux that fulfills all these requirements. Our evaluation
demonstrates that Agnocast maintains constant IPC overhead regardless of
message size, even for unsized message types. In Autoware PointCloud
Preprocessing, Agnocast achieves a 16% improvement in average response time and
a 25% improvement in worst-case response time.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [27] [HybridRAG-based LLM Agents for Low-Carbon Optimization in Low-Altitude Economy Networks](https://arxiv.org/abs/2506.15947)
*Jinbo Wen,Cheng Su,Jiawen Kang,Jiangtian Nie,Yang Zhang,Jianhang Tang,Dusit Niyato,Chau Yuen*

Main category: cs.NI

TL;DR: 该论文提出了一种基于检索增强生成（RAG）的大型语言模型（LLM）代理框架（HybridRAG），用于解决多无人机辅助的边缘计算网络中的碳减排优化问题，并设计了一种双重正则化扩散增强的软演员-评论家（R²DSAC）算法进行求解。


<details>
  <summary>Details</summary>
Motivation: 低空经济网络（LAENets）需要满足低延迟和高计算需求，多无人机辅助的边缘计算（MEC）网络是实现这一目标的重要途径。然而，多维无人机建模的复杂性和多目标耦合优化的难度阻碍了低碳多无人机辅助MEC网络的发展。

Method: 论文提出了一种结合KeywordRAG、VectorRAG和GraphRAG的HybridRAG框架，以增强LLM代理从专家数据库中检索结构化信息的能力。接着，设计了R²DSAC算法，通过扩散熵正则化和动作熵正则化优化扩散策略，并动态屏蔽演员网络中不重要的神经元以减少碳排放。

Result: 仿真结果表明，HybridRAG框架和R²DSAC算法在多无人机辅助的MEC网络中表现出高效性和可靠性，能够有效降低碳排放。

Conclusion: HybridRAG框架和R²DSAC算法为解决低空经济网络中多无人机辅助MEC的低碳优化问题提供了有效的解决方案，为未来研究奠定了技术基础。

Abstract: Low-Altitude Economy Networks (LAENets) are emerging as a promising paradigm
to support various low-altitude services through integrated air-ground
infrastructure. To satisfy low-latency and high-computation demands, the
integration of Unmanned Aerial Vehicles (UAVs) with Mobile Edge Computing (MEC)
systems plays a vital role, which offloads computing tasks from terminal
devices to nearby UAVs, enabling flexible and resilient service provisions for
ground users. To promote the development of LAENets, it is significant to
achieve low-carbon multi-UAV-assisted MEC networks. However, several challenges
hinder this implementation, including the complexity of multi-dimensional UAV
modeling and the difficulty of multi-objective coupled optimization. To this
end, this paper proposes a novel Retrieval Augmented Generation (RAG)-based
Large Language Model (LLM) agent framework for model formulation. Specifically,
we develop HybridRAG by combining KeywordRAG, VectorRAG, and GraphRAG,
empowering LLM agents to efficiently retrieve structural information from
expert databases and generate more accurate optimization problems compared with
traditional RAG-based LLM agents. After customizing carbon emission
optimization problems for multi-UAV-assisted MEC networks, we propose a Double
Regularization Diffusion-enhanced Soft Actor-Critic (R\textsuperscript{2}DSAC)
algorithm to solve the formulated multi-objective optimization problem. The
R\textsuperscript{2}DSAC algorithm incorporates diffusion entropy
regularization and action entropy regularization to improve the performance of
the diffusion policy. Furthermore, we dynamically mask unimportant neurons in
the actor network to reduce the carbon emissions associated with model
training. Simulation results demonstrate the effectiveness and reliability of
the proposed HybridRAG-based LLM agent framework and the
R\textsuperscript{2}DSAC algorithm.

</details>


### [28] [LoRaIN: A Constructive Interference-Assisted Reliable and Energy-Efficient LoRa Indoor Network](https://arxiv.org/abs/2506.16409)
*Mahbubur Rahman,Abusayeed Saifullah*

Main category: cs.NI

TL;DR: 该论文提出了LoRaIN协议，旨在提升室内LoRa网络的可靠性和能效，通过实验和数学分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: LoRa作为一种有前途的通信技术，室内性能研究较少，现有研究未充分揭示其可靠性和能效问题。

Method: 提出LoRaIN协议，利用特殊定时和增强节点实现干扰构造和确认中继。

Result: 实验表明，15%的终端设备作为增强节点时，可靠性从62%提升至95%，能效提高2.5倍。

Conclusion: LoRaIN是首个提升室内LoRa网络性能的协议，具有显著效果。

Abstract: LoRa is a promising communication technology for enabling the next-generation
indoor Internet of Things applications. Very few studies, however, have
analyzed its performance indoors. Besides, these indoor studies investigate
mostly the RSSI and SNR of the received packets at the gateway, which, as we
show, may not unfold the poor performance of LoRa and its MAC protocol,
LoRaWAN, indoors in terms of reliability and energy-efficiency. In this paper,
we extensively evaluate the performance of LoRaWAN indoors and then use the key
insights to boost its reliability and energy-efficiency by proposing LoRaIN,
LoRa Indoor Network, a new link-layer protocol that can be effectively used for
indoor deployments. The approach to boosting the reliability and energy
efficiency in LoRaIN is underpinned by enabling constructive interference with
specific timing requirements analyzed both empirically and mathematically for
different pairs of channel bandwidth and spreading factor and relaying precious
acknowledgments to the end-devices with the assistance of several booster
nodes. The booster nodes do not need any special capability and can be a subset
of the LoRa end-devices. To our knowledge, LoRaIN is the first protocol for
boosting reliability and energy-efficiency in indoor LoRa networks. We evaluate
its performance in an indoor testbed consisting of one LoRaWAN gateway and 20
end-devices. Our extensive evaluation shows that when 15% of the end-devices
operate as booster nodes, the reliability at the gateway increases from 62% to
95%, and the end-devices are approximately 2.5x energy-efficient.

</details>


### [29] [Using SRv6 to access Edge Applications in 5G Networks](https://arxiv.org/abs/2506.16808)
*Louis Royer,Emmanuel Lavinal,Emmanuel Chaput*

Main category: cs.NI

TL;DR: 本文讨论了5G及以后的多接入边缘计算中数据路径优化的问题，提出了使用SRv6技术的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着5G多接入边缘计算的兴起，运营商需要优化数据路径并确保资源使用符合其策略，现有解决方案存在局限性。

Method: 回顾现有边缘资源访问方案，指出其局限，并提出在5G/边缘架构中使用SRv6。

Result: 提出SRv6作为一种潜在的高效解决方案。

Conclusion: SRv6有望解决5G/边缘计算中的数据路径优化问题。

Abstract: With the emergence of Multi-Access Edge Computing in 5G and beyond, it has
become essential for operators to optimize the data path for the end-user while
ensuring resources are used according to their policy. In this paper, we review
existing solutions to access edge resources, underline their limits, and
propose the use of Segment Routing over IPv6 (SRv6) in a 5G/edge architecture.

</details>


### [30] [Minimal Per-Flow Backlog Bounds at an Aggregate FIFO Server under Piecewise-Linear Arrival Curves](https://arxiv.org/abs/2506.16914)
*Lukas Wildberger,Anja Hamscher,Jens B. Schmitt*

Main category: cs.NI

TL;DR: 论文提出了一种针对广义到达曲线（不仅仅是令牌桶）的FIFO残余服务曲线方法，通过优化自由参数，显著降低了积压边界。


<details>
  <summary>Details</summary>
Motivation: 网络演算（NC）在分析FIFO服务器时，由于min-plus非线性问题，传统方法难以处理广义到达曲线。作者旨在解决这一问题，提出更优的残余服务曲线。

Method: 作者首先证明了积压边界可在到达曲线或残余服务曲线的断点处计算。随后定义了一组曲线，描述积压与自由参数的关系，并通过最大交点确定最优参数。针对复杂场景，提出一种高效启发式算法。

Result: 该方法显著降低了积压边界，并通过DiscoDNC工具验证了其有效性。启发式算法在大多数情况下能够找到最优或接近最优的参数。

Conclusion: 研究为广义到达曲线提供了高效的积压边界优化方法，并通过启发式算法解决了高复杂度问题，为网络性能分析提供了实用工具。

Abstract: Network Calculus (NC) is a versatile methodology based on min-plus algebra to
derive worst-case per-flow performance bounds in networked systems with many
concurrent flows. In particular, NC can analyze many scheduling disciplines;
yet, somewhat surprisingly, an aggregate FIFO server is a notoriously hard case
due to its min-plus non-linearity. A resort is to represent the FIFO residual
service by a family of functions with a free parameter instead of just a single
curve. For simple token-bucket arrival curves, literature provides optimal
choices for that free parameter to minimize delay and backlog bounds. In this
paper, we tackle the challenge of more general arrival curves than just token
buckets. In particular, we derive residual service curves resulting in minimal
backlog bounds for general piecewise-linear arrival curves. To that end, we
first show that a backlog bound can always be calculated at a breakpoint of
either the arrival curve of the flow of interest or its residual service curve.
Further, we define a set of curves that characterize the backlog for a fixed
breakpoint, depending on the free parameter of the residual service curve. We
show that the backlog-minimizing residual service curve family parameter
corresponds to the largest intersection of those curves with the arrival curve.
In more complex scenarios finding this largest intersection can become
inefficient as the search space grows in the number of flows. Therefore, we
present an efficient heuristic that finds, in many cases, the optimal parameter
or at least a close conservative approximation. This heuristic is evaluated in
terms of accuracy and execution time. Finally, we utilize these
backlog-minimizing residual service curves to enhance the DiscoDNC tool and
observe considerable reductions in the corresponding backlog bounds.

</details>


### [31] [Client Selection Strategies for Federated Semantic Communications in Heterogeneous IoT Networks](https://arxiv.org/abs/2506.17063)
*Samer Lahoud,Kinda Khawam*

Main category: cs.NI

TL;DR: 提出了一种联邦语义通信框架，用于异构物联网设备间的高效数据重建，通过语义特征传输降低通信开销，并探索了设备选择的公平性与性能平衡。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增导致带宽受限和隐私保护挑战，需解决异构设备环境下数据分布不均和设备选择的公平性问题。

Method: 采用联邦语义通信框架，结合三种设备选择策略和带语义瓶颈的端到端架构，以及基于损失的聚合机制。

Result: 实验表明，实用主义选择在重建质量上最优，而比例公平性在减少参与不平等和提升计算效率上表现突出。

Conclusion: 联邦语义通信能平衡重建质量、资源效率和公平性，为可持续且隐私保护的边缘智能应用铺路。

Abstract: The exponential growth of IoT devices presents critical challenges in
bandwidth-constrained wireless networks, particularly regarding efficient data
transmission and privacy preservation. This paper presents a novel federated
semantic communication (SC) framework that enables collaborative training of
bandwidth-efficient models for image reconstruction across heterogeneous IoT
devices. By leveraging SC principles to transmit only semantic features, our
approach dramatically reduces communication overhead while preserving
reconstruction quality. We address the fundamental challenge of client
selection in federated learning environments where devices exhibit significant
disparities in dataset sizes and data distributions. Our framework implements
three distinct client selection strategies that explore different trade-offs
between system performance and fairness in resource allocation. The system
employs an end-to-end SC architecture with semantic bottlenecks, coupled with a
loss-based aggregation mechanism that naturally adapts to client heterogeneity.
Experimental evaluation on image data demonstrates that while Utilitarian
selection achieves the highest reconstruction quality, Proportional Fairness
maintains competitive performance while significantly reducing participation
inequality and improving computational efficiency. These results establish that
federated SC can successfully balance reconstruction quality, resource
efficiency, and fairness in heterogeneous IoT deployments, paving the way for
sustainable and privacy-preserving edge intelligence applications.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [32] [ViFusion: In-Network Tensor Fusion for Scalable Video Feature Indexing](https://arxiv.org/abs/2506.16258)
*Yisu Wang,Yixiang Zhu,Xinjiao Li,Yulong Zhang,Ruilong Wu,Dirk Kutscher*

Main category: cs.MM

TL;DR: ViFusion是一个通信感知的张量融合框架，通过合并小特征张量为更易管理的单元，显著提升分布式视频特征索引效率，在相同延迟水平下吞吐量提升8-22倍。


<details>
  <summary>Details</summary>
Motivation: 大规模视频特征索引依赖于高效数据传输，现有方法在网络拥塞时性能下降，亟需优化分布式多媒体系统的计算和数据传输效率。

Method: ViFusion结合网络内计算模块和专用张量融合机制，将小特征张量合并为更易管理的单元，优化分布式视频索引。

Result: 部署结果表明，ViFusion在相同延迟水平下将视频检索系统吞吐量提升8-22倍。

Conclusion: ViFusion通过张量融合和网络内计算有效解决了大规模视频特征索引中的数据效率问题，显著提升了系统性能。

Abstract: Large-scale video feature indexing in datacenters is critically dependent on
efficient data transfer. Although in-network computation has emerged as a
compelling strategy for accelerating feature extraction and reducing overhead
in distributed multimedia systems, harnessing advanced networking resources at
both the switch and host levels remains a formidable challenge. These
difficulties are compounded by heterogeneous hardware, diverse application
requirements, and complex multipath topologies. Existing methods focus
primarily on optimizing inference for large neural network models using
specialized collective communication libraries, which often face performance
degradation in network congestion scenarios.
  To overcome these limitations, we present ViFusion, a communication aware
tensor fusion framework that streamlines distributed video indexing by merging
numerous small feature tensors into consolidated and more manageable units. By
integrating an in-network computation module and a dedicated tensor fusion
mechanism within datacenter environments, ViFusion substantially improves the
efficiency of video feature indexing workflows. The deployment results show
that ViFusion improves the throughput of the video retrieval system by 8--22
times with the same level of latency as state-of-the-art systems.

</details>


### [33] [DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced Distribution Transformation](https://arxiv.org/abs/2506.16495)
*Changsheng Gao,Zijie Liu,Li Li,Dong Liu,Xiaoyan Sun,Weisi Lin*

Main category: cs.MM

TL;DR: 该论文提出了一种通用的特征编码方法，用于解决大模型中特征分布多样且不兼容的问题，通过数据驱动的分布变换方法实现高效压缩和跨模型泛化。


<details>
  <summary>Details</summary>
Motivation: 现有特征编码方法多针对特定任务或模型，缺乏通用性。本研究旨在解决不同大模型特征分布多样且不兼容的问题。

Method: 提出一种学习的峰值到平衡分布变换，将不同模型的非均匀特征分布对齐到统一的目标空间，无需修改下游编码器。

Result: 在LLaMA3、DINOv2和SD3等大模型上的实验表明，该方法在压缩效率和跨模型泛化方面优于任务特定的基线方法。

Conclusion: 该方法为通用特征编码提供了有效解决方案，未来研究将通过开源代码进一步推动进展。

Abstract: Like image coding in visual data transmission, feature coding is essential
for the distributed deployment of large models by significantly reducing
transmission and storage overhead. However, prior studies have mostly targeted
task- or model-specific scenarios, leaving the challenge of universal feature
coding across diverse large models largely unaddressed. In this paper, we
present the first systematic study on universal feature coding for large
models. The key challenge lies in the inherently diverse and distributionally
incompatible nature of features extracted from different models. For example,
features from DINOv2 exhibit highly peaky, concentrated distributions, while
those from Stable Diffusion 3 (SD3) are more dispersed and uniform. This
distributional heterogeneity severely hampers both compression efficiency and
cross-model generalization. To address this, we propose a learned
peaky-to-balanced distribution transformation, which reshapes highly skewed
feature distributions into a common, balanced target space. This transformation
is non-uniform, data-driven, and plug-and-play, enabling effective alignment of
heterogeneous distributions without modifying downstream codecs. With this
alignment, a universal codec trained on the balanced target distribution can
effectively generalize to features from different models and tasks. We validate
our approach on three representative large models-LLaMA3, DINOv2, and
SD3-across multiple tasks and modalities. Extensive experiments show that our
method achieves notable improvements in both compression efficiency and
cross-model generalization over task-specific baselines. All source code will
be released for future research.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [34] [Locality in Many-Valued Structures](https://arxiv.org/abs/2506.16206)
*James Carr*

Main category: cs.LO

TL;DR: 该论文研究了在剩余格代数背景下，经典局部性定理（Hanf和Gaifman定理）是否成立，并探讨了局部性的定义和代数行为对结果的影响。


<details>
  <summary>Details</summary>
Motivation: 经典模型论中的局部性定理在多值模型中是否依然适用，尤其是在剩余格代数这一非经典谓词逻辑语义背景下，这一问题尚不明确。

Method: 通过分析Hanf和Gaifman定理在剩余格代数中的表现，研究了不同局部性定义和代数行为对定理成立性的影响。

Result: Hanf定理在自然局部性定义下不成立，但在特定情况下（如良好连接的剩余格）可恢复；Gaifman定理的引理在代数行为良好的情况下可恢复。

Conclusion: 剩余格代数中的局部性定理对局部性定义和代数行为高度敏感，其中顺序解释连接符在模型和代数之间起到了关键作用。

Abstract: Many-valued models generalise the structures from classical model theory by
defining truth values for a model with an arbitrary algebra. Just as algebraic
varieties provide semantics for many non-classical propositional logics, models
defined over algebras in a variety provide the semantics for the corresponding
non-classical predicate logics. In particular models defined over varieties of
residuated lattices represent the model theory for first-order substructrual
logics.
  In this paper we study the extent to which the classical locality theorems
from Hanf and Gaifman hold true in the residuated lattice setting. We
demonstrate that the answer is sensitive both to how locality is understood in
the generalised context and the behaviour of the truth-defining algebra. In the
case of Hanf's theorem, we will show that the theorem fails for the natural
understanding of local neighbourhoods, but is recoverable in one special case
for well-connected residuated lattices. For Gaifman's theorem, rather than
consider Gaifman normal forms directly we focus on the main lemma of the
theorem from textbook proofs. We prove that for a number of different
understandings of locality, provided the algebra is well-behaved enough to
express locality in its syntax, this main lemma can be recovered. In each case
we will see that importance of an order-interpreting connective which creates a
link between the modelling relation between models and formulas and the
valuation function from formulas into the algebra.

</details>


### [35] [A Quantum-Control Lambda-Calculus with Multiple Measurement Bases](https://arxiv.org/abs/2506.16244)
*Alejandro Díaz-Caro,Nicolas A. Monzon*

Main category: cs.LO

TL;DR: Lambda-SX是一种支持多测量基的量子λ演算，通过类型系统跟踪可复制性，提升测量的灵活性和组合推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何在量子编程语言的类型系统中集成多测量基的支持，以实现更灵活的控制和组合推理。

Method: 定义了Lambda-SX的语法、类型规则、子类型和操作语义，并验证了其元理论性质。

Result: 实现了多测量基在量子编程语言类型系统中的一致性集成。

Conclusion: Lambda-SX展示了多测量基在类型系统中的可行性，为量子编程语言的灵活性和组合性提供了理论基础。

Abstract: We introduce Lambda-SX, a typed quantum lambda-calculus that supports
multiple measurement bases. By tracking duplicability relative to arbitrary
bases within the type system, Lambda-SX enables more flexible control and
compositional reasoning about measurements. We formalise its syntax, typing
rules, subtyping, and operational semantics, and establish its key
meta-theoretical properties. This proof-of-concept shows that support for
multiple bases can be coherently integrated into the type discipline of quantum
programming languages.

</details>


### [36] [A Hyperlogic for Strategies in Stochastic Games (Extended Version)](https://arxiv.org/abs/2506.16775)
*Lina Gerlach,Christof Löding,Erika Ábrahám*

Main category: cs.LO

TL;DR: 提出一种概率超逻辑HyperSt²，用于表达随机游戏中策略的超属性，首次解决了此类问题。


<details>
  <summary>Details</summary>
Motivation: 研究随机游戏中策略的超属性表达，填补现有超逻辑在该领域的空白。

Method: 设计HyperSt²逻辑，支持独立策略执行的概率关联，并分析其表达能力与模型检测问题。

Result: 证明了HyperSt²的表达能力优于现有逻辑，虽模型检测问题一般不可判定，但在特定条件下可解。

Conclusion: HyperSt²为随机游戏的策略分析提供了新工具，特定条件下具有实用性。

Abstract: We propose a probabilistic hyperlogic called HyperSt$^2$ that can express
hyperproperties of strategies in turn-based stochastic games. To the best of
our knowledge, HyperSt$^2$ is the first hyperlogic for stochastic games.
HyperSt$^2$ can relate probabilities of several independent executions of
strategies in a stochastic game. For example, in HyperSt$^2$ it is natural to
formalize optimality, i.e., to express that some strategy is better than all
other strategies, or to express the existence of Nash equilibria. We
investigate the expressivity of HyperSt$^2$ by comparing it to existing logics
for stochastic games, as well as existing hyperlogics. Though the
model-checking problem for HyperSt$^2$ is in general undecidable, we show that
it becomes decidable for bounded memory and is in EXPTIME and PSPACE-hard over
memoryless deterministic strategies, and we identify a fragment for which the
model-checking problem is PSPACE-complete.

</details>


### [37] [A Note on Proper Relational Structures](https://arxiv.org/abs/2506.17142)
*Adam Bjorndahl,Philip Sink*

Main category: cs.LO

TL;DR: 提出一种算法，将关系结构转换为“适当”关系结构，避免某些冗余关系。


<details>
  <summary>Details</summary>
Motivation: 在处理模态逻辑的Simplicial语义时，需要避免关系结构中的冗余关系，以简化证明过程。

Method: 提供一种翻译方法，保持关系结构的经典性质（如传递性和欧几里得性）。

Result: 翻译方法可应用于模态逻辑的完整性证明中。

Conclusion: 该方法有效解决了关系结构中的冗余问题，适用于Simplicial语义的研究。

Abstract: In this note we provide an algorithm for translating relational structures
into "proper" relational structures, i.e., those such that there is no pair of
worlds w and u such that w is accessible from u for every agent. In particular,
our method of translation preserves many classical properties of relational
structures, such as transitivity and the Euclidean property. As a result, this
method of translation has many applications in the literature on Simplicial
Semantics for modal logic, where the creation of proper canonical relational
structures is a common step in proofs of completeness.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [38] [Machine Learning-based Context-Aware EMAs: An Offline Feasibility Study](https://arxiv.org/abs/2506.15834)
*Zachary D King,Maryam Khalid,Han Yu,Kei Shibuya,Khadija Zanna,Marzieh Majd,Ryan L Brown,Yufei Shen,Thomas Vaessen,George Kypriotakis,Christopher P Fagundes,Akane Sano*

Main category: cs.HC

TL;DR: 研究者提出一种多目标函数，结合机器学习的响应预测和情感预测不确定性，以优化EMA发送时间，从而提高响应率并捕捉更广泛的情感。


<details>
  <summary>Details</summary>
Motivation: mHealth研究中，EMA的响应率对情感识别模型至关重要。现有机器学习方法可能导致情感样本不均衡，需改进。

Method: 提出多目标函数，结合响应预测和情感预测不确定性，优化EMA发送时间。

Result: 评估显示，该函数在响应率高且情感不常见时表现更好。

Conclusion: 多目标函数可提高EMA响应率并捕捉更多样化的情感，适用于mHealth研究。

Abstract: Mobile health (mHealth) systems help researchers monitor and care for
patients in real-world settings. Studies utilizing mHealth applications use
Ecological Momentary Assessment (EMAs), passive sensing, and contextual
features to develop emotion recognition models, which rely on EMA responses as
ground truth. Due to this, it is crucial to consider EMA compliance when
conducting a successful mHealth study. Utilizing machine learning is one
approach that can solve this problem by sending EMAs based on the predicted
likelihood of a response. However, literature suggests that this approach may
lead to prompting participants more frequently during emotions associated with
responsiveness, thereby narrowing the range of emotions collected. We propose a
multi-objective function that utilizes machine learning to identify optimal
times for sending EMAs. The function identifies optimal moments by combining
predicted response likelihood with model uncertainty in emotion predictions.
Uncertainty would lead the function to prioritize time points when the model is
less confident, which often corresponds to underrepresented emotions. We
demonstrate that this objective function would result in EMAs being sent when
participants are responsive and experiencing less commonly observed emotions.
The evaluation is conducted offline using two datasets: (1) 91 spousal
caregivers of individuals with Alzheimer's Disease and Related dementias
(ADRD), (2) 45 healthy participants. Results show that the multi-objective
function tends to be higher when participants respond to EMAs and report less
commonly observed emotions. This suggests that using the proposed objective
function to guide EMA delivery could improve receptivity rates and capture a
broader range of emotions.

</details>


### [39] [DeckFlow: Iterative Specification on a Multimodal Generative Canvas](https://arxiv.org/abs/2506.15873)
*Gregory Croisdale,Emily Huang,John Joon Young Chung,Anhong Guo,Xu Wang,Austin Z. Henley,Cyrus Omar*

Main category: cs.HC

TL;DR: DeckFlow是一种多模态生成AI工具，旨在解决现有工具的三个设计问题，包括任务分解、规范分解和生成空间探索，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有生成AI工具存在设计问题，限制了用户创建高质量个性化媒体的能力。

Method: DeckFlow通过无限画布上的卡片和视觉数据流实现任务分解，支持迭代分解目标和生成多样性提示与输出。

Result: 在文本到图像生成任务中，DeckFlow优于现有对话AI基线，并进一步支持音频生成和开放创作。

Conclusion: DeckFlow为解决生成AI工具的设计问题提供了有效方案，支持多模态创意任务。

Abstract: Generative AI promises to allow people to create high-quality personalized
media. Although powerful, we identify three fundamental design problems with
existing tooling through a literature review. We introduce a multimodal
generative AI tool, DeckFlow, to address these problems. First, DeckFlow
supports task decomposition by allowing users to maintain multiple
interconnected subtasks on an infinite canvas populated by cards connected
through visual dataflow affordances. Second, DeckFlow supports a specification
decomposition workflow where an initial goal is iteratively decomposed into
smaller parts and combined using feature labels and clusters. Finally, DeckFlow
supports generative space exploration by generating multiple prompt and output
variations, presented in a grid, that can feed back recursively into the next
design iteration. We evaluate DeckFlow for text-to-image generation against a
state-of-practice conversational AI baseline for image generation tasks. We
then add audio generation and investigate user behaviors in a more open-ended
creative setting with text, image, and audio outputs.

</details>


### [40] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: 在NXP MCXN947微控制器上实现的实时关键词识别系统，结合MFCC特征提取和CNN分类器，通过量化感知训练优化模型，展示了在资源受限设备上的高效低功耗语音交互可行性。


<details>
  <summary>Details</summary>
Motivation: 旨在在资源受限的嵌入式设备上实现高效、低功耗的实时语音交互。

Method: 结合MFCC特征提取和CNN分类器，使用量化感知训练优化模型大小。

Result: 利用NPU实现59倍推理速度提升，97.06%准确率，模型大小30.58 KB。

Conclusion: 证明了在嵌入式平台上实现高效低功耗语音接口的可行性。

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


### [41] [Semantic Scaffolding: Augmenting Textual Structures with Domain-Specific Groupings for Accessible Data Exploration](https://arxiv.org/abs/2506.15883)
*Jonathan Zong,Isabella Pedraza Pineros,Mengzhu Katie Chen,Daniel Hajas,Arvind Satyanarayan*

Main category: cs.HC

TL;DR: 论文提出了一种名为“语义脚手架”的技术，利用大型语言模型（LLMs）的领域特定信息来识别和解释数据分组的意义。通过“语义分箱”和“数据高亮”两种方式展示数据分组的语义含义，并在可视化工具Olli中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 用户在探索新数据集时，常常难以理解数据分组的真实含义，尤其是缺乏领域知识的情况下。为了让用户能快速理解数据背后的语义信息，研究者开发了语义脚手架技术。

Method: 使用LLMs提取领域特定信息，生成语义分箱（将字段划分为领域相关的区间和类别）和数据高亮（为数据子集标注现实意义）。这种技术在可视化工具Olli中实现。

Result: 通过对15名盲人和低视力用户的研究发现，用户能借助语义脚手架快速理解数据含义，但也注意到其对数据解释的影响。

Conclusion: 语义脚手架有效帮助用户理解数据，但也引发了对数据解释主观性的反思。技术需平衡明确定义分组与用户自主探索的需求。

Abstract: Drawing connections between interesting groupings of data and their
real-world meaning is an important, yet difficult, part of encountering a new
dataset. A lay reader might see an interesting visual pattern in a chart but
lack the domain expertise to explain its meaning. Or, a reader might be
familiar with a real-world concept but struggle to express it in terms of a
dataset's fields. In response, we developed semantic scaffolding, a technique
for using domain-specific information from large language models (LLMs) to
identify, explain, and formalize semantically meaningful data groupings. We
present groupings in two ways: as semantic bins, which segment a field into
domain-specific intervals and categories; and data highlights, which annotate
subsets of data records with their real-world meaning. We demonstrate and
evaluate this technique in Olli, an accessible visualization tool that
exemplifies tensions around explicitly defining groupings while respecting the
agency of readers to conduct independent data exploration. We conducted a study
with 15 blind and low-vision (BLV) users and found that readers used semantic
scaffolds to quickly understand the meaning of the data, but were often also
critically aware of its influence on their interpretation.

</details>


### [42] [ChatAR: Conversation Support using Large Language Model and Augmented Reality](https://arxiv.org/abs/2506.16008)
*Yuichiro Fujimoto*

Main category: cs.HC

TL;DR: 研究提出了一种结合HMD和LLM的实时对话支持系统，通过AR技术减少知识差异带来的沟通障碍，并优化了信息展示方式以避免被对话伙伴察觉，同时提升了对话质量。


<details>
  <summary>Details</summary>
Motivation: 解决对话中因知识差异导致的沟通障碍，并通过技术手段改善对话体验。

Method: 结合HMD和LLM，实时识别关键词、生成并展示信息，优化信息展示方式以减少察觉性。

Result: 信息展示方式降低被察觉的风险，平衡了对话双方的发言比例，并提升对话的兴奋感。

Conclusion: 该系统能有效支持对话，优化信息展示方式对提升对话质量至关重要。

Abstract: Engaging in smooth conversations with others is a crucial social skill.
However, differences in knowledge between conversation participants can
sometimes hinder effective communication. To tackle this issue, this study
proposes a real-time support system that integrates head-mounted display
(HMD)-based augmented reality (AR) technology with large language models
(LLMs). This system facilitates conversation by recognizing keywords during
dialogue, generating relevant information using the LLM, reformatting it, and
presenting it to the user via the HMD. A significant issue with this system is
that the user's eye movements may reveal to the conversation partner that they
are reading the displayed text. This study also proposes a method for
presenting information that takes into account appropriate eye movements during
conversation. Two experiments were conducted to evaluate the effectiveness of
the proposed system. The first experiment revealed that the proposed
information presentation method reduces the likelihood of the conversation
partner noticing that the user is reading the displayed text. The second
experiment demonstrated that the proposed method led to a more balanced speech
ratio between the user and the conversation partner, as well as a increase in
the perceived excitement of the conversation.

</details>


### [43] [SimuPanel: A Novel Immersive Multi-Agent System to Simulate Interactive Expert Panel Discussion](https://arxiv.org/abs/2506.16010)
*Xiangyang He,Jiale Li,Jiahao Chen,Yang Yang,Mingming Fan*

Main category: cs.HC

TL;DR: SimuPanel 是一个基于 LLM 多智能体交互的系统，模拟学术专家的小组讨论，通过 3D 环境增强参与感，解决了传统讨论的地理、时间和经济限制。通过技术评估和用户研究，验证了其模拟深度讨论和提升用户参与度的效果。


<details>
  <summary>Details</summary>
Motivation: 传统小组讨论受限于地理、经济和时间等因素，无法普及。SimuPanel 旨在通过技术手段模拟专家讨论，提供更广泛的学习机会。

Method: 采用基于 LLM 的多智能体交互框架，结合主机-专家架构和 3D 可视化环境，模拟真实讨论动态，支持用户互动与笔记记录。

Result: 技术评估和用户研究表明，SimuPanel 能够模拟更深度的讨论并提升用户参与度和反思能力。

Conclusion: 该研究为未来改进和利用小组讨论的多媒体学习潜力提供了设计启示。

Abstract: Panel discussion allows the audience to learn different perspectives through
interactive discussions among experts moderated by a host and a Q&A session
with the audience. Despite its benefits, panel discussion in the real world is
inaccessible to many who do not have the privilege to participate due to
geographical, financial, and time constraints. We present SimuPanel, which
simulates panel discussions among academic experts through LLM-based
multi-agent interaction. It enables users to define topics of interest for the
panel, observe the expert discussion, engage in Q&A, and take notes. SimuPanel
employs a host-expert architecture where each panel member is simulated by an
agent with specialized expertise, and the panel is visualized in an immersive
3D environment to enhance engagement. Traditional dialogue generation struggles
to capture the depth and interactivity of real-world panel discussions. To
address this limitation, we propose a novel multi-agent interaction framework
that simulates authentic panel dynamics by modeling reasoning strategies and
personas of experts grounded in multimedia sources. This framework enables
agents to dynamically recall and contribute to the discussion based on past
experiences from diverse perspectives. Our technical evaluation and the user
study with university students show that SimuPanel was able to simulate more
in-depth discussions and engage participants to interact with and reflect on
the discussions. As a first step in this direction, we offer design
implications for future avenues to improve and harness the power of panel
discussion for multimedia learning.

</details>


### [44] [Human-Centered Shared Autonomy for Motor Planning, Learning, and Control Applications](https://arxiv.org/abs/2506.16044)
*MH Farhadi,Ali Rabiee,Sima Ghafoori,Anna Cetera,Wei Xu,Reza Abiri*

Main category: cs.HC

TL;DR: 本文综述了医疗领域中以人为中心的共享自主AI框架，重点关注上肢生物信号机器接口及其控制系统的理论、实践与未来方向。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，完全独立的机器决策可能不理想，因为人类意图至关重要。需要结合AI与人类输入以实现更有效的共享自主。

Method: 通过综述上肢生物信号接口及控制系统（如计算机光标、机械臂等），探讨人机协同在任务中的规划、学习与控制。

Result: 提出自适应共享自主AI作为高性能人机协作范式的潜力，并分析了实现挑战与未来方向。

Conclusion: 结合神经科学与机器人技术，构建更直观、高效且符合伦理的人机协作框架是未来的关键方向。

Abstract: With recent advancements in AI and computational tools, intelligent paradigms
have emerged to enhance fields like shared autonomy and human-machine teaming
in healthcare. Advanced AI algorithms (e.g., reinforcement learning) can
autonomously make decisions to achieve planning and motion goals. However, in
healthcare, where human intent is crucial, fully independent machine decisions
may not be ideal. This chapter presents a comprehensive review of
human-centered shared autonomy AI frameworks, focusing on upper limb
biosignal-based machine interfaces and associated motor control systems,
including computer cursors, robotic arms, and planar platforms. We examine
motor planning, learning (rehabilitation), and control, covering conceptual
foundations of human-machine teaming in reach-and-grasp tasks and analyzing
both theoretical and practical implementations. Each section explores how human
and machine inputs can be blended for shared autonomy in healthcare
applications. Topics include human factors, biosignal processing for intent
detection, shared autonomy in brain-computer interfaces (BCI), rehabilitation,
assistive robotics, and Large Language Models (LLMs) as the next frontier. We
propose adaptive shared autonomy AI as a high-performance paradigm for
collaborative human-AI systems, identify key implementation challenges, and
outline future directions, particularly regarding AI reasoning agents. This
analysis aims to bridge neuroscientific insights with robotics to create more
intuitive, effective, and ethical human-machine teaming frameworks.

</details>


### [45] [From 600 Tools to 1 Console: A UX-Driven Transformation](https://arxiv.org/abs/2506.16107)
*Mariann Kornelia Smith,Jacqueline Meijer-Irons,Andrew Millar*

Main category: cs.HC

TL;DR: 谷歌内部基础设施工具效率低下，通过用户研究和设计方法改进工作流程并提升了开发者的生产力。


<details>
  <summary>Details</summary>
Motivation: 解决谷歌内部基础设施工具分散和低效的问题，提升开发者生产力。

Method: 采用用户中心化的研究和设计方法，创建故事地图和服务蓝图，制定战略愿景并分步实施改进。

Result: 成功简化工具、优化工作流程，并获得了管理层支持。

Conclusion: 用户中心化的设计和分步改进策略有效提升了内部工具的效率和开发者体验。

Abstract: In 2021 the Technical Infrastructure (TI) User Experience (UX) team sent a
survey to 10,000 Google Developers (Googlers) and uncovered that Google's
internal infrastructure tools were fragmented and inefficient, hindering
developers' productivity. Using user centered research and design methodologies
the team first created a story map and service blueprint to visualize the
relationship between internal applications, then formulated a strategic vision
to consolidate tools, streamline workflows, and measure the impact of their
work. We secured executive buy-in and delivered incremental improvements.

</details>


### [46] [On using AI for EEG-based BCI applications: problems, current challenges and future trends](https://arxiv.org/abs/2506.16168)
*Thomas Barbera,Jacopo Burger,Alessandro D'Amelio,Simone Zini,Simone Bianco,Raffaella Lanzarotti,Paolo Napoletano,Giuseppe Boccignone,Jose Luis Contreras-Vidal*

Main category: cs.HC

TL;DR: 论文探讨了AI在解码脑电图（EEG）信号以开发实用脑机接口（BCI）中的应用，分析了当前挑战并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 利用AI技术推动EEG-based BCI的发展，以实现更广泛的现实应用（如脑语音、脑图像等）。

Method: 通过因果视角分析基本范式，探讨AI模型在EEG-based BCI中的独特挑战。

Result: 识别了技术、方法和伦理上的限制，并提出了解决这些问题的研究方向。

Conclusion: 提出了一份清晰的路线图，旨在开发实用且高效的EEG-based BCI解决方案。

Abstract: Imagine unlocking the power of the mind to communicate, create, and even
interact with the world around us. Recent breakthroughs in Artificial
Intelligence (AI), especially in how machines "see" and "understand" language,
are now fueling exciting progress in decoding brain signals from scalp
electroencephalography (EEG). Prima facie, this opens the door to revolutionary
brain-computer interfaces (BCIs) designed for real life, moving beyond
traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a
Brain-to-Internet of Things (BCIoT).
  However, the journey is not as straightforward as it was for Computer Vision
(CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based
BCIs, particularly in building powerful foundational models, presents unique
and intricate hurdles that could affect their reliability.
  Here, we unfold a guided exploration of this dynamic and rapidly evolving
research area. Rather than barely outlining a map of current endeavors and
results, the goal is to provide a principled navigation of this hot and
cutting-edge research landscape. We consider the basic paradigms that emerge
from a causal perspective and the attendant challenges presented to AI-based
models. Looking ahead, we then discuss promising research avenues that could
overcome today's technological, methodological, and ethical limitations. Our
aim is to lay out a clear roadmap for creating truly practical and effective
EEG-based BCI solutions that can thrive in everyday environments.

</details>


### [47] [Development of a persuasive User Experience Research (UXR) Point of View for Explainable Artificial Intelligence (XAI)](https://arxiv.org/abs/2506.16199)
*Mohammad Naiseh,Huseyin Dogan,Stephen Giff,Nan Jiang*

Main category: cs.HC

TL;DR: 论文提出了一份针对可解释人工智能（XAI）的UX研究手册，旨在帮助UX专业人员设计更透明和可信的AI体验。


<details>
  <summary>Details</summary>
Motivation: 当前XAI的解释方法（如SHAP、LIME）对非技术用户难以理解，UX专业人员缺乏技术支持。

Method: 提出了UXR Playbook框架，连接技术解释方法与用户中心设计。

Result: 为设计师提供实用指导，改善用户对AI的理解和信任。

Conclusion: 通过该手册，可促进AI的负责任采用和用户体验优化。

Abstract: Explainable Artificial Intelligence (XAI) plays a critical role in fostering
user trust and understanding in AI-driven systems. However, the design of
effective XAI interfaces presents significant challenges, particularly for UX
professionals who may lack technical expertise in AI or machine learning.
Existing explanation methods, such as SHAP, LIME, and counterfactual
explanations, often rely on complex technical language and assumptions that are
difficult for non-expert users to interpret. To address these gaps, we propose
a UX Research (UXR) Playbook for XAI - a practical framework aimed at
supporting UX professionals in designing accessible, transparent, and
trustworthy AI experiences. Our playbook offers actionable guidance to help
bridge the gap between technical explainability methods and user centred
design, empowering designers to create AI interactions that foster better
understanding, trust, and responsible AI adoption.

</details>


### [48] [Toward Understanding Similarity of Visualization Techniques](https://arxiv.org/abs/2506.17032)
*Abdulhaq Adetunji Salako,Christian Tominski*

Main category: cs.HC

TL;DR: 该研究通过模型驱动和专家驱动两种方法探讨了可视化技术的相似性，初步分析了13种基本和高级可视化技术的相似度，为未来研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 因为现有可视化技术数量庞大且分类复杂，理解它们之间的相似性是一个开放的研究问题。

Method: 采用了两种方法：一是基于模型驱动的签名定义，二是通过专家在线调查评估直观相似性。

Result: 研究初步得出了13种可视化技术的相似性，结果具有学术价值。

Conclusion: 尽管结果是初步的，但它们为理解可视化技术的相似性提供了重要的一步。

Abstract: The literature describes many visualization techniques for different types of
data, tasks, and application contexts, and new techniques are proposed on a
regular basis. Visualization surveys try to capture the immense space of
techniques and structure it with meaningful categorizations. Yet, it remains
difficult to understand the similarity of visualization techniques in general.
We approach this open research question from two angles. First, we follow a
model-driven approach that is based on defining the signature of visualization
techniques and interpreting the similarity of signatures as the similarity of
their associated techniques. Second, following an expert-driven approach, we
asked visualization experts in a small online study for their ad-hoc intuitive
assessment of the similarity of pairs visualization techniques. From both
approaches, we gain insight into the similarity of a set of 13 basic and
advanced visualizations for different types of data. While our results are so
far preliminary and academic, they are first steps toward better understanding
the similarity of visualization techniques.

</details>


### [49] [When learning analytics dashboard is explainable: An exploratory study on the effect of GenAI-supported learning analytics dashboard](https://arxiv.org/abs/2506.16312)
*Angxuan Chen*

Main category: cs.HC

TL;DR: 研究了基于理论和可解释学习分析仪表盘（LAD）对大学生与AI协作写作任务的影响。结果显示，虽然两组写作质量无显著差异，但可解释LAD组在概念理解上表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过结合自我调节学习（SRL）理论和可解释AI（XAI）原则，提升学生在AI协作写作中的学习效果。

Method: 采用实验设计，比较使用完整可解释LAD的实验组和仅视觉LAD的对照组在协作写作任务中的表现。

Result: 可解释LAD组在概念理解测试中得分显著更高（p=0.026），但写作质量无显著差异。

Conclusion: 提供可解释反馈对深化学习概念和培养自我调节学习能力至关重要。

Abstract: This study investigated the impact of a theory-driven, explainable Learning
Analytics Dashboard (LAD) on university students' human-AI collaborative
academic abstract writing task. Grounded in Self-Regulated Learning (SRL)
theory and incorporating Explainable AI (XAI) principles, our LAD featured a
three-layered design (Visual, Explainable, Interactive). In an experimental
study, participants were randomly assigned to either an experimental group
(using the full explainable LAD) or a control group (using a visual-only LAD)
to collaboratively write an academic abstract with a Generative AI. While
quantitative analysis revealed no significant difference in the quality of
co-authored abstracts between the two groups, a significant and noteworthy
difference emerged in conceptual understanding: students in the explainable LAD
group demonstrated a superior grasp of abstract writing principles, as
evidenced by their higher scores on a knowledge test (p= .026). These findings
highlight that while basic AI-generated feedback may suffice for immediate task
completion, the provision of explainable feedback is crucial for fostering
deeper learning, enhancing conceptual understanding, and developing
transferable skills fundamental to self-regulated learning in academic writing
contexts.

</details>


### [50] [Can GPT-4o Evaluate Usability Like Human Experts? A Comparative Study on Issue Identification in Heuristic Evaluation](https://arxiv.org/abs/2506.16345)
*Guilherme Guerino,Luiz Rodrigues,Bruna Capeleti,Rafael Ferreira Mello,André Freire,Luciana Zaina*

Main category: cs.HC

TL;DR: 本研究比较了GPT-4o与人类专家在启发式评估中的表现，发现GPT-4o仅识别出21.2%的人类专家发现的问题，但也发现了27个新问题。


<details>
  <summary>Details</summary>
Motivation: 探讨GPT-4o在启发式评估中的能力及其与人类专家的差异。

Method: 通过文献提示，让GPT-4o和人类专家分别对网页系统截图进行启发式评估。

Result: GPT-4o在某些启发式上表现较好，但在灵活性、控制和用户效率方面表现不佳，且存在误报问题。

Conclusion: GPT-4o在启发式评估中可辅助使用，但需注意其局限性并谨慎使用。

Abstract: Heuristic evaluation is a widely used method in Human-Computer Interaction
(HCI) to inspect interfaces and identify issues based on heuristics. Recently,
Large Language Models (LLMs), such as GPT-4o, have been applied in HCI to
assist in persona creation, the ideation process, and the analysis of
semi-structured interviews. However, considering the need to understand
heuristics and the high degree of abstraction required to evaluate them, LLMs
may have difficulty conducting heuristic evaluation. However, prior research
has not investigated GPT-4o's performance in heuristic evaluation compared to
HCI experts in web-based systems. In this context, this study aims to compare
the results of a heuristic evaluation performed by GPT-4o and human experts. To
this end, we selected a set of screenshots from a web system and asked GPT-4o
to perform a heuristic evaluation based on Nielsen's Heuristics from a
literature-grounded prompt. Our results indicate that only 21.2% of the issues
identified by human experts were also identified by GPT-4o, despite it found 27
new issues. We also found that GPT-4o performed better for heuristics related
to aesthetic and minimalist design and match between system and real world,
whereas it has difficulty identifying issues in heuristics related to
flexibility, control, and user efficiency. Additionally, we noticed that GPT-4o
generated several false positives due to hallucinations and attempts to predict
issues. Finally, we highlight five takeaways for the conscious use of GPT-4o in
heuristic evaluations.

</details>


### [51] [Closed-Loop Control of Electrical Stimulation through Spared Motor Unit Ensembles Restores Foot Movements after Spinal Cord Injury](https://arxiv.org/abs/2506.16468)
*Vlad Cnejevici,Matthias Ponfick,Raul C. Sîmpetru,Alessandro Del Vecchio*

Main category: cs.HC

TL;DR: 研究开发了一种基于高密度表面EMG的神经假体系统，用于通过闭环FES控制恢复瘫痪足部运动。


<details>
  <summary>Details</summary>
Motivation: 帮助脊髓损伤（SCI）患者恢复足部运动功能，提升生活质量。

Method: 使用可穿戴高密度表面EMG系统解码残余运动单元（MU）活动，结合FES实现闭环控制。

Result: SCI患者保留了可区分的踝部运动EMG活动，部分参与者能维持>70%的准确率，闭环FES成功恢复了足部运动。

Conclusion: 该系统为SCI患者提供了一种直观的闭环FES控制方法，有望帮助恢复运动功能。

Abstract: Restoring movement of a paralyzed foot is a key challenge in helping
individuals with neurological conditions such as spinal cord injury (SCI) to
improve their quality of life. Neuroprostheses based on functional electrical
stimulation (FES) can restore the physiological range of motion by stimulating
the affected muscles using surface electrodes. We have previously shown that,
despite chronic motor-complete SCI, it is possible to capture paralyzed hand
movements in individuals with tetraplegia using spared and modulated motor unit
(MU) activity decoded with non-invasive electromyography (EMG) sensors. This
study investigated whether a wearable high-density surface EMG system could
capture and control paralyzed foot kinematics in closed-loop control with an
FES system. We found that all our participants with SCI (2 with chronic SCI and
3 with acute SCI) retained distinct spared EMG activity for at least three
ankle movements, which allowed them to reliably control a digital cursor using
their spared tibialis anterior and triceps surae MU activity. Movement
separability was further reconfirmed by extracting task-modulated MU activity
during foot flexion/extension (3-7 modulated MUs/participant). Three
participants were further able to modulate and maintain their foot
flexion/extension EMG levels with an accuracy of >70%. Lastly, we show that
real-time control of a FES system using EMG from the affected limb can restore
foot movements in a highly intuitive way, significantly improving the lost or
pathological foot range of motion. Our system provides an intuitive approach
for closed-loop control of FES that has the potential to assist individuals
with SCI in regaining lost motor functions.

</details>


### [52] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
*Sophie Chiang,Guy Laban,Hatice Gunes*

Main category: cs.HC

TL;DR: 研究表明，机器人情感支持的对话内容与人类治疗师的会话主题高度相似，90.88%的机器人对话可以映射到人类治疗数据集的聚类中。


<details>
  <summary>Details</summary>
Motivation: 探索机器人情感支持对话与人类治疗对话的相似性，以评估机器人在心理健康干预中的潜力。

Method: 使用句子嵌入和K-means聚类，比较人与机器人对话的主题结构和语义重叠。

Result: 机器人对话的主题结构与人类治疗对话高度一致，且回应语义相似。

Conclusion: 机器人对话在心理健康干预中具有潜力，但仍需注意其局限性。

Abstract: As conversational agents increasingly engage in emotionally supportive
dialogue, it is important to understand how closely their interactions resemble
those in traditional therapy settings. This study investigates whether the
concerns shared with a robot align with those shared in human-to-human (H2H)
therapy sessions, and whether robot responses semantically mirror those of
human therapists. We analyzed two datasets: one of interactions between users
and professional therapists (Hugging Face's NLP Mental Health Conversations),
and another involving supportive conversations with a social robot (QTrobot
from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence
embeddings and K-means clustering, we assessed cross-agent thematic alignment
by applying a distance-based cluster-fitting method that evaluates whether
responses from one agent type map to clusters derived from the other, and
validated it using Euclidean distances. Results showed that 90.88% of robot
conversation disclosures could be mapped to clusters from the human therapy
dataset, suggesting shared topical structure. For matched clusters, we compared
the subjects as well as therapist and robot responses using Transformer,
Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'
disclosures in both datasets, as well as in the responses given to similar
human disclosure themes across agent types (robot vs. human therapist). These
findings highlight both the parallels and boundaries of robot-led support
conversations and their potential for augmenting mental health interventions.

</details>


### [53] [Virtual Interviewers, Real Results: Exploring AI-Driven Mock Technical Interviews on Student Readiness and Confidence](https://arxiv.org/abs/2506.16542)
*Nathalia Gomez,S. Sue Batham,Mathias Volonte,Tiffany D. Do*

Main category: cs.HC

TL;DR: 研究探讨了多模态AI系统如何模拟技术面试，帮助求职者建立信心。参与者认为体验真实且有益，但也存在对话流畅性和时间控制的挑战。


<details>
  <summary>Details</summary>
Motivation: 技术面试对计算机科学毕业生至关重要，但实践机会有限。研究旨在探索AI是否能够有效模拟面试，提升求职者信心。

Method: 20名参与者使用一款AI驱动的模拟面试工具，完成白板题目并获取实时反馈。

Result: 多数参与者认为体验真实且提升了信心，但对话流畅性和时间管理仍需改进。

Conclusion: AI驱动的技术面试工具有潜力成为可扩展的准备工具，未来可进一步研究面试官行为变化的影响。

Abstract: Technical interviews are a critical yet stressful step in the hiring process
for computer science graduates, often hindered by limited access to practice
opportunities. This formative qualitative study (n=20) explores whether a
multimodal AI system can realistically simulate technical interviews and
support confidence-building among candidates. Participants engaged with an
AI-driven mock interview tool featuring whiteboarding tasks and real-time
feedback. Many described the experience as realistic and helpful, noting
increased confidence and improved articulation of problem-solving decisions.
However, challenges with conversational flow and timing were noted. These
findings demonstrate the potential of AI-driven technical interviews as
scalable and realistic preparation tools, suggesting that future research could
explore variations in interviewer behavior and their potential effects on
candidate preparation.

</details>


### [54] [Capturing Visualization Design Rationale](https://arxiv.org/abs/2506.16571)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.HC

TL;DR: 本文提出了一种新的数据集和方法，通过自然语言探究可视化设计背后的逻辑，利用学生的可视化笔记本和大型语言模型生成问题-答案-逻辑三元组。


<details>
  <summary>Details</summary>
Motivation: 以往的数据集多关注可视化解码，而忽略了设计的编码逻辑，本文旨在填补这一空白。

Method: 利用学生的可视化笔记本和LLMs生成问题-答案-逻辑三元组，并验证和整理数据集。

Result: 构建了一个捕捉学生可视化设计选择及逻辑的数据集。

Conclusion: 该方法为理解可视化设计背后的逻辑提供了新工具。

Abstract: Prior natural language datasets for data visualization have focused on tasks
such as visualization literacy assessment, insight generation, and
visualization generation from natural language instructions. These studies
often rely on controlled setups with purpose-built visualizations and
artificially constructed questions. As a result, they tend to prioritize the
interpretation of visualizations, focusing on decoding visualizations rather
than understanding their encoding. In this paper, we present a new dataset and
methodology for probing visualization design rationale through natural
language. We leverage a unique source of real-world visualizations and natural
language narratives: literate visualization notebooks created by students as
part of a data visualization course. These notebooks combine visual artifacts
with design exposition, in which students make explicit the rationale behind
their design decisions. We also use large language models (LLMs) to generate
and categorize question-answer-rationale triples from the narratives and
articulations in the notebooks. We then carefully validate the triples and
curate a dataset that captures and distills the visualization design choices
and corresponding rationales of the students.

</details>


### [55] [PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration](https://arxiv.org/abs/2506.16677)
*Hao Guo,Wei Fan,Shaohui Liu,Feng Jiang,Chunzhi Yi*

Main category: cs.HC

TL;DR: 本文提出了一种名为PPTP的新型框架，通过结合多模态生理信号和协作性能评估来预测人类对机器人的信任水平，显著提高了信任分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 在人类与机器人协作的场景中，尤其是在建筑领域，信任预测对安全和效率至关重要。现有方法在处理个体生理信号差异时存在不足，需要一种更准确的方法来评估信任。

Method: PPTP框架通过同步多模态生理信号（ECG、GSR和EMG）和协作性能评估，利用协作性能的标准化特性补偿个体生理反应的差异，从而预测信任水平。

Result: 实验表明，该框架在三分类任务中达到了81%的准确率，优于基线方法6.7%。在七分类任务中，准确率达到74.3%，是信任预测领域的首次突破。

Conclusion: PPTP框架通过协作性能引导的生理信号处理，显著提高了信任预测的准确性，为人类-机器人协作的实际应用提供了有力支持。

Abstract: Trust prediction is a key issue in human-robot collaboration, especially in
construction scenarios where maintaining appropriate trust calibration is
critical for safety and efficiency. This paper introduces the
Performance-guided Physiological signal-based Trust Prediction (PPTP), a novel
framework designed to improve trust assessment. We designed a human-robot
construction scenario with three difficulty levels to induce different trust
states. Our approach integrates synchronized multimodal physiological signals
(ECG, GSR, and EMG) with collaboration performance evaluation to predict human
trust levels. Individual physiological signals are processed using
collaboration performance information as guiding cues, leveraging the
standardized nature of collaboration performance to compensate for individual
variations in physiological responses. Extensive experiments demonstrate the
efficacy of our cross-modality fusion method in significantly improving trust
classification performance. Our model achieves over 81% accuracy in three-level
trust classification, outperforming the best baseline method by 6.7%, and
notably reaches 74.3% accuracy in high-resolution seven-level classification,
which is a first in trust prediction research. Ablation experiments further
validate the superiority of physiological signal processing guided by
collaboration performance assessment.

</details>


### [56] [V-CASS: Vision-context-aware Expressive Speech Synthesis for Enhancing User Understanding of Videos](https://arxiv.org/abs/2506.16716)
*Qixin Wang,Songtao Zhou,Zeyu Jin,Chenglin Guo,Shikun Sun,Xiaoyu Qin*

Main category: cs.HC

TL;DR: 论文提出了一种基于视觉上下文感知的语音合成方法（V-CASS），通过分析视频中的副语言线索生成情感和态度一致的语音，显著提升了用户的视听理解和参与度。


<details>
  <summary>Details</summary>
Motivation: 当前视频解说系统常忽视情感和态度等副语言线索，导致用户理解受限或内容原意被曲解。

Method: 提出V-CASS方法，结合视觉语言模型分析副语言线索，并通过知识增强的语言模型指导语音模型生成上下文一致的语音。

Result: 用户研究表明，V-CASS显著提高了情感共鸣和用户参与度，74.68%的参与者更偏好该系统。

Conclusion: V-CASS不仅增强了用户的视听体验，还展示了在帮助视障用户访问网络视频方面的潜力，提升了普适可访问性。

Abstract: Automatic video commentary systems are widely used on multimedia social media
platforms to extract factual information about video content. However, current
systems may overlook essential para-linguistic cues, including emotion and
attitude, which are critical for fully conveying the meaning of visual content.
The absence of these cues can limit user understanding or, in some cases,
distort the video's original intent. Expressive speech effectively conveys
these cues and enhances the user's comprehension of videos. Building on these
insights, this paper explores the usage of vision-context-aware expressive
speech in enhancing users' understanding of videos in video commentary systems.
Firstly, our formatting study indicates that semantic-only speech can lead to
ambiguity, and misaligned emotions between speech and visuals may distort
content interpretation. To address this, we propose a method called
vision-context-aware speech synthesis (V-CASS). It analyzes para-linguistic
cues from visuals using a vision-language model and leverages a
knowledge-infused language model to guide the expressive speech model in
generating context-aligned speech. User studies show that V-CASS enhances
emotional and attitudinal resonance, as well as user audio-visual understanding
and engagement, with 74.68% of participants preferring the system. Finally, we
explore the potential of our method in helping blind and low-vision users
navigate web videos, improving universal accessibility.

</details>


### [57] ["Whoever needs to see it, will see it": Motivations and Labor of Creating Algorithmic Conspirituality Content on TikTok](https://arxiv.org/abs/2506.16851)
*Ankolika De,Kelley Cotter,Shaheen Kanthawala,Haley McAtee,Amy Ritchart,Gahana Kadur*

Main category: cs.HC

TL;DR: 研究探讨了社交媒体算法的神秘化解读如何影响内容创作者及其社区，基于对14位TikTok创作者的访谈，揭示了算法与灵性主题的互动及其对观众和创作者的情感影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，用户常将社交媒体算法视为神秘或不可预测的存在，这引发了对算法解读如何塑造内容创作和在线社区的疑问。

Method: 通过访谈14位TikTok的“算法灵性”内容创作者，研究其解读和创作过程如何受到平台算法的影响。

Result: 创作者的信仰与算法互动，强化了灵性或关系主题，并揭示了此类内容对观众及创作者情感劳动的影响。

Conclusion: 研究呼吁设计支持，以识别算法引发的灵性体验，并帮助创作者有效应对相关挑战。

Abstract: Recent studies show that users often interpret social media algorithms as
mystical or spiritual because of their unpredictability. This invites new
questions about how such perceptions affect the content that creators create
and the communities they form online. In this study, 14 creators of algorithmic
conspirituality content on TikTok were interviewed to explore their
interpretations and creation processes influenced by the platform's For You
Page algorithm. We illustrate how creators' beliefs interact with TikTok's
algorithmic mediation to reinforce and shape their spiritual or relational
themes. Furthermore, we show how algorithmic conspirituality content impacts
viewers, highlighting its role in generating significant emotional and
affective labor for creators, stemming from complex relational dynamics
inherent in this content creation. We discuss implications for design to
support creators aimed at recognizing the unexpected spiritual and religious
experiences algorithms prompt, as well as supporting creators in effectively
managing these challenges.

</details>


### [58] [Exploring the Usage of Generative AI for Group Project-Based Offline Art Courses in Elementary Schools](https://arxiv.org/abs/2506.16874)
*Zhiqing Wang,Haoxiang Fan,Shiwei Wu,Qiaoyi Chen,Yongqi Liang,Zhenhui Peng*

Main category: cs.HC

TL;DR: 研究了GenAI在K-6艺术课程中的整合效果，开发了AskArt工具，展示了GenAI的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 探索GenAI如何提升小学生的创造力、参与度和团队合作能力。

Method: 四阶段实地研究，涉及2名教师和132名学生，开发并部署AskArt交互界面。

Result: GenAI能提供背景信息、灵感和个性化指导，但存在查询生成内容的挑战和学生合作策略多样的问题。

Conclusion: GenAI在基础教育中具有潜力，AskArt是实用工具，但需注意使用中的挑战和教育者的引导。

Abstract: The integration of Generative Artificial Intelligence (GenAI) in K-6
project-based art courses presents both opportunities and challenges for
enhancing creativity, engagement, and group collaboration. This study
introduces a four-phase field study, involving in total two experienced K-6 art
teachers and 132 students in eight offline course sessions, to investigate the
usage and impact of GenAI. Specifically, based on findings in Phases 1 and 2,
we developed AskArt, an interactive interface that combines DALL-E and GPT and
is tailored to support elementary school students in their art projects, and
deployed it in Phases 3 and 4. Our findings revealed the benefits of GenAI in
providing background information, inspirations, and personalized guidance.
However, challenges in query formulation for generating expected content were
also observed. Moreover, students employed varied collaboration strategies, and
teachers noted increased engagement alongside concerns regarding misuse and
interface suitability. This study offers insights into the effective
integration of GenAI in elementary education, presents AskArt as a practical
tool, and provides recommendations for educators and researchers to enhance
project-based learning with GenAI technologies.

</details>


### [59] [Juicy or Dry? A Comparative Study of User Engagement and Information Retention in Interactive Infographics](https://arxiv.org/abs/2506.17011)
*Bruno Campos*

Main category: cs.HC

TL;DR: 研究比较了“多汁”设计对用户互动和短期信息保留的影响，发现多汁设计在用户互动上略有优势，但在信息保留上结果不一，强调需平衡吸引力和清晰度。


<details>
  <summary>Details</summary>
Motivation: 探讨多汁设计元素对用户互动和信息保留的具体效果。

Method: 通过对比多汁设计和干燥设计在多个信息图中的用户互动和信息保留表现。

Result: 多汁设计在用户互动上表现稍好，信息保留结果不一；部分多汁设计表现更好，但也有干燥设计在某些情况下更优。

Conclusion: 多汁设计能提升用户体验，但需谨慎实施以避免过度干扰，平衡吸引力和实用性是关键。

Abstract: This study compares the impact of "juiciness" on user engagement and
short-term information retention in interactive infographics. Juicy designs
generally showed a slight advantage in overall user engagement scores compared
to dry designs. Specifically, the juicy version of the Burcalories infographic
had the highest engagement score. However, the differences in engagement were
often small. Regarding information retention, the results were mixed. The juicy
versions of The Daily Routines of Famous Creative People and The Main Chakras
infographics showed marginally better average recall and more participants with
higher recall. Conversely, the dry version of Burcalories led to more correct
answers in multiple-choice questions. The study suggests that while juicy
design elements can enhance user engagement and, in some cases, short-term
information retention, their effectiveness depends on careful implementation.
Excessive juiciness could be overwhelming or distracting, while
well-implemented juicy elements contributed to a more entertaining experience.
The findings emphasize the importance of balancing engaging feedback with
clarity and usability.

</details>


### [60] [Reflecting Human Values in XAI: Emotional and Reflective Benefits in Creativity Support Tools](https://arxiv.org/abs/2506.17116)
*Samuel Rhys Cox,Helena Bøjer Djernæs,Niels van Berkel*

Main category: cs.HC

TL;DR: 探讨在艺术领域评估可解释AI（XAI）系统时，如何测量用户为中心的益处（如情感幸福感）。


<details>
  <summary>Details</summary>
Motivation: 当前对创造力支持工具（CST）的评估缺乏关注用户自身的益处，尤其是在艺术领域。

Method: 基于对CST评估的近期综述，提出四种用户为中心的测量维度：内在能力发展、情感幸福感、自我反思和自我认知。

Result: 希望通过这些维度的讨论，激发关于XAI在艺术中潜在影响的探讨。

Conclusion: 强调用户为中心的评估在XAI和艺术领域的重要性，呼吁未来研究关注此类测量。

Abstract: In this workshop paper, we discuss the potential for measures of user-centric
benefits (such as emotional well-being) that could be explored when evaluating
explainable AI (XAI) systems within the arts. As a background to this, we draw
from our recent review of creativity support tool (CST) evaluations, that found
a paucity of studies evaluating CSTs for user-centric measures that benefit the
user themselves. Specifically, we discuss measures of: (1) developing intrinsic
abilities, (2) emotional well-being, (3) self-reflection, and (4)
self-perception. By discussing these user-centric measures within the context
of XAI and the arts, we wish to provoke discussion regarding the potential of
such measures.

</details>


### [61] [Detecting LLM-Generated Short Answers and Effects on Learner Performance](https://arxiv.org/abs/2506.17196)
*Shambhavi Bhushan,Danielle R Thomas,Conrad Borchers,Isha Raghuvanshi,Ralph Abboud,Erin Gatz,Shivang Gupta,Kenneth Koedinger*

Main category: cs.HC

TL;DR: 该研究通过微调GPT-4o检测LLM生成的文本，发现其性能优于现有工具GPTZero，并探讨了LLM滥用对学习的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在教育中的滥用问题日益突出，但现有检测工具的可靠性参差不齐，且缺乏对LLM滥用影响学习的系统研究。

Method: 定义LLM生成的文本为未经改写或优化的内容，利用微调的GPT-4o进行检测，并分析学习效果。

Result: 微调后的模型准确率达80%，F1分数0.78，优于GPTZero；学习者滥用LLM会导致绕过学习过程。

Conclusion: 研究为改进LLM检测提供了结构化方法，并建议使用统计指标辅助判断，以支持开放科学。

Abstract: The increasing availability of large language models (LLMs) has raised
concerns about their potential misuse in online learning. While tools for
detecting LLM-generated text exist and are widely used by researchers and
educators, their reliability varies. Few studies have compared the accuracy of
detection methods, defined criteria to identify content generated by LLM, or
evaluated the effect on learner performance from LLM misuse within learning. In
this study, we define LLM-generated text within open responses as those
produced by any LLM without paraphrasing or refinement, as evaluated by human
coders. We then fine-tune GPT-4o to detect LLM-generated responses and assess
the impact on learning from LLM misuse. We find that our fine-tuned LLM
outperforms the existing AI detection tool GPTZero, achieving an accuracy of
80% and an F1 score of 0.78, compared to GPTZero's accuracy of 70% and macro F1
score of 0.50, demonstrating superior performance in detecting LLM-generated
responses. We also find that learners suspected of LLM misuse in the open
response question were more than twice as likely to correctly answer the
corresponding posttest MCQ, suggesting potential misuse across both question
types and indicating a bypass of the learning process. We pave the way for
future work by demonstrating a structured, code-based approach to improve
LLM-generated response detection and propose using auxiliary statistical
indicators such as unusually high assessment scores on related tasks,
readability scores, and response duration. In support of open science, we
contribute data and code to support the fine-tuning of similar models for
similar use cases.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [62] [FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space](https://arxiv.org/abs/2506.15742)
*Black Forest Labs,Stephen Batifol,Andreas Blattmann,Frederic Boesel,Saksham Consul,Cyril Diagne,Tim Dockhorn,Jack English,Zion English,Patrick Esser,Sumith Kulal,Kyle Lacey,Yam Levi,Cheng Li,Dominik Lorenz,Jonas Müller,Dustin Podell,Robin Rombach,Harry Saini,Axel Sauer,Luke Smith*

Main category: cs.GR

TL;DR: FLUX.1 Kontext是一个统一的生成流匹配模型，结合文本和图像输入的语义上下文进行图像生成和编辑。模型通过简单的序列连接方法处理局部编辑和上下文生成任务，提升了对象和角色的保持能力，同时在速度和交互性上优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 当前编辑模型在多轮操作中存在角色一致性和稳定性下降的问题，FLUX.1 Kontext旨在通过统一的架构解决这一问题，提升迭代工作流的鲁棒性。

Method: 采用序列连接方法整合文本和图像输入的语义上下文，支持局部编辑和生成任务。模型设计了KontextBench基准（1026对图像-提示），覆盖五类任务以验证性能。

Result: FLUX.1 Kontext在多轮一致性和单轮质量上表现优越，生成速度显著快于现有模型，适用于交互式应用和快速原型设计。

Conclusion: FLUX.1 Kontext通过统一架构和高效性能，为图像处理模型设立了新标准，尤其在多任务处理和工作流鲁棒性方面表现出色。

Abstract: We present evaluation results for FLUX.1 Kontext, a generative flow matching
model that unifies image generation and editing. The model generates novel
output views by incorporating semantic context from text and image inputs.
Using a simple sequence concatenation approach, FLUX.1 Kontext handles both
local editing and generative in-context tasks within a single unified
architecture. Compared to current editing models that exhibit degradation in
character consistency and stability across multiple turns, we observe that
FLUX.1 Kontext improved preservation of objects and characters, leading to
greater robustness in iterative workflows.The model achieves competitive
performance with current state-of-the-art systems while delivering
significantly faster generation times, enabling interactive applications and
rapid prototyping workflows. To validate these improvements, we introduce
KontextBench, a comprehensive benchmark with 1026 image-prompt pairs covering
five task categories: local editing, global editing, character reference, style
reference and text editing. Detailed evaluations show the superior performance
of FLUX.1 Kontext in terms of both single-turn quality and multi-turn
consistency, setting new standards for unified image processing models.

</details>


### [63] [Graphics4Science: Computer Graphics for Scientific Impacts](https://arxiv.org/abs/2506.15786)
*Peter Yichen Chen,Minghao Guo,Hanspeter Pfister,Ming Lin,William Freeman,Qixing Huang,Han-Wei Shen,Wojciech Matusik*

Main category: cs.GR

TL;DR: 本文探讨计算机图形学与科学的深层关系，强调其作为科学建模语言的作用，并鼓励图形学社区参与科学问题的解决。


<details>
  <summary>Details</summary>
Motivation: 研究计算机图形学与科学的融合，以填补两领域间的词汇鸿沟，并解决数据稀缺环境下的挑战。

Method: 通过几何推理和物理建模等核心方法，为科学问题提供归纳偏置。

Result: 提出将图形学作为科学建模语言的概念，并展示了其在科学发现中的潜力。

Conclusion: Graphics4Science课程旨在推动图形学专家参与高影响力的科学问题，促进科学发现。

Abstract: Computer graphics, often associated with films, games, and visual effects,
has long been a powerful tool for addressing scientific challenges--from its
origins in 3D visualization for medical imaging to its role in modern
computational modeling and simulation. This course explores the deep and
evolving relationship between computer graphics and science, highlighting past
achievements, ongoing contributions, and open questions that remain. We show
how core methods, such as geometric reasoning and physical modeling, provide
inductive biases that help address challenges in both fields, especially in
data-scarce settings. To that end, we aim to reframe graphics as a modeling
language for science by bridging vocabulary gaps between the two communities.
Designed for both newcomers and experts, Graphics4Science invites the graphics
community to engage with science, tackle high-impact problems where graphics
expertise can make a difference, and contribute to the future of scientific
discovery. Additional details are available on the course website:
https://graphics4science.github.io

</details>


### [64] [GratNet: A Photorealistic Neural Shader for Diffractive Surfaces](https://arxiv.org/abs/2506.15815)
*Narayan Kandel,Daljit Singh J. S. Dhillon*

Main category: cs.GR

TL;DR: 该论文提出了一种基于多层感知器（MLP）的数据驱动方法，用于高精度高效地渲染衍射表面，通过数据压缩视角解决隐式神经表示问题。


<details>
  <summary>Details</summary>
Motivation: 现有衍射表面渲染模型依赖密集或预处理数据，缺乏对隐式神经表示的全面探讨，需一种高效且准确的方法。

Method: 采用MLP进行数据驱动渲染，从数据压缩角度设计训练和建模方法，避免过拟合并提升重采样鲁棒性。

Result: 方法显著减少了原始数据集的内存占用（两个数量级），并通过PSNR、SSIM和FLIP指标验证了高质量重建。

Conclusion: 该方法在性能提升的同时，实现了与现有波光学方法主观相似的结果，适用于衍射表面的高效渲染。

Abstract: Structural coloration is commonly modeled using wave optics for reliable and
photorealistic rendering of natural, quasi-periodic and complex nanostructures.
Such models often rely on dense, preliminary or preprocessed data to accurately
capture the nuanced variations in diffractive surface reflectances. This heavy
data dependency warrants implicit neural representation which has not been
addressed comprehensively in the current literature. In this paper, we present
a multi-layer perceptron (MLP) based method for data-driven rendering of
diffractive surfaces with high accuracy and efficiency. We primarily approach
this problem from a data compression perspective to devise a nuanced training
and modeling method which is attuned to the domain and range characteristics of
diffractive reflectance datasets. Importantly, our approach avoids over-fitting
and has robust resampling behavior. Using Peak-Signal-to-Noise (PSNR),
Structural Similarity Index Measure (SSIM) and a flipping difference evaluator
(FLIP) as evaluation metrics, we demonstrate the high-quality reconstruction of
the ground-truth. In comparison to a recent state-of-the-art offline,
wave-optical, forward modeling approach, our method reproduces subjectively
similar results with significant performance gains. We reduce the memory
footprint of the raw datasets by two orders of magnitude in general. Lastly, we
depict the working of our method with actual surface renderings.

</details>


### [65] [VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal](https://arxiv.org/abs/2506.15821)
*Pham Khai Nguyen Do,Bao Nguyen Tran,Nam Nguyen,Duc Dung Nguyen*

Main category: cs.GR

TL;DR: VEIGAR是一种高效的新视图合成框架，无需初始3D重建阶段，显著提升了重建质量和跨视图一致性，同时减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖初始3D重建阶段，计算成本高且重建质量不理想。VEIGAR旨在通过轻量级基础模型和新型监督策略消除这一需求。

Method: VEIGAR利用像素空间中的显式先验对齐和基于尺度不变深度损失的新型监督策略，避免传统的单目深度正则化操作。

Result: VEIGAR在重建质量和跨视图一致性上达到新最高水平，训练时间缩短为现有最快方法的三分之一。

Conclusion: VEIGAR展示了效率和效果的最佳平衡，为3D生成任务提供了高效且高质量的解决方案。

Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have
significantly improved editing tasks, with a primary emphasis on maintaining
cross-view consistency throughout the generative process. Contemporary methods
typically address this challenge using a dual-strategy framework: performing
consistent 2D inpainting across all views guided by embedded priors either
explicitly in pixel space or implicitly in latent space; and conducting 3D
reconstruction with additional consistency guidance. Previous strategies, in
particular, often require an initial 3D reconstruction phase to establish
geometric structure, introducing considerable computational overhead. Even with
the added cost, the resulting reconstruction quality often remains suboptimal.
In this paper, we present VEIGAR, a computationally efficient framework that
outperforms existing methods without relying on an initial reconstruction
phase. VEIGAR leverages a lightweight foundation model to reliably align priors
explicitly in the pixel space. In addition, we introduce a novel supervision
strategy based on scale-invariant depth loss, which removes the need for
traditional scale-and-shift operations in monocular depth regularization.
Through extensive experimentation, VEIGAR establishes a new state-of-the-art
benchmark in reconstruction quality and cross-view consistency, while achieving
a threefold reduction in training time compared to the fastest existing method,
highlighting its superior balance of efficiency and effectiveness.

</details>


### [66] [User-Guided Force-Directed Graph Layout](https://arxiv.org/abs/2506.15860)
*Hasan Balci,Augustin Luna*

Main category: cs.GR

TL;DR: 提出了一种用户导向的力导向布局方法，通过手绘草图直观控制布局，提高关系数据的可视化质量。


<details>
  <summary>Details</summary>
Motivation: 现有布局算法需要复杂参数设置，用户意图表达不便，因此研究更直观的控制方式。

Method: 利用手绘草图提取结构信息，生成位置约束指导布局。

Result: 在多种规模的真实和合成图上验证，布局效果符合用户预期。

Conclusion: 方法直观有效，已开源实现。

Abstract: Visual analysis of relational data is essential for many real-world analytics
tasks, with layout quality being key to interpretability. However, existing
layout algorithms often require users to navigate complex parameters to express
their intent. We present a user-guided force-directed layout approach that
enables intuitive control through freehand sketching. Our method uses classical
image analysis techniques to extract structural information from sketches,
which is then used to generate positional constraints that guide the layout
process. We evaluate the approach on various real and synthetic graphs ranging
from small to medium scale, demonstrating its ability to produce layouts
aligned with user expectations. An implementation of our method along with
documentation and a demo page is freely available on GitHub at
https://github.com/sciluna/uggly.

</details>


### [67] [FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models](https://arxiv.org/abs/2506.16627)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Mikolaj Kida,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种替代高斯曲率惩罚的曲率代理方法，仅正则化混合二阶项（Weingarten项），降低计算成本并保持几何偏见。


<details>
  <summary>Details</summary>
Motivation: 神经符号距离场（SDF）在几何学习中应用广泛，但传统方法需要昂贵的Hessian计算和二阶自动微分，内存和运行时间成本高。

Method: 设计了两种曲率代理：(1) 有限差分代理，用四个SDF评估和一个一阶梯度替换Hessian条目；(2) 自动微分代理，通过Hessian向量积计算混合导数，避免显式Hessian组装。

Result: 在ABC基准测试中，代理方法匹配或超过基于Hessian的基线，同时将GPU内存使用和运行时间减半。

Conclusion: 该方法为工程级形状重建提供了可扩展、曲率感知的SDF学习实用路径。

Abstract: Neural signed-distance fields (SDFs) have become a versatile backbone for
geometric learning, yet enforcing developable, CAD-style behavior still hinges
on Gaussian curvature penalties that require full Hessian evaluation and
second-order automatic differentiation, both of which are costly in memory and
runtime. We present a curvature proxy that regularizes only the mixed
second-order term (Weingarten term), allowing the two principal curvatures to
adapt freely to data while suppressing unwanted warp. Two complementary
instantiations realize this idea: (i) a finite-difference proxy that replaces
each Hessian entry with four forward SDF evaluations and a single first-order
gradient, and (ii) an autodiff proxy that computes the same mixed derivative
via one Hessian-vector product, sidestepping explicit full Hessian assembly and
remaining faster in practice. Both variants converge to the exact mixed second
derivative, thus preserving the intended geometric bias without incurring full
second-order graphs. On the ABC benchmarks, the proxies match or exceed the
reconstruction fidelity of Hessian-based baselines while reducing GPU memory
use and wall-clock time by a factor of two. Because the method is drop-in and
framework-agnostic, it opens a practical path toward scalable, curvature-aware
SDF learning for engineering-grade shape reconstruction.

</details>


### [68] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种基于物理启发的PDE驱动的图像生成方法，通过流体动力学实现更高质量的图像合成。


<details>
  <summary>Details</summary>
Motivation: 结合流体动力学和无量纲PDE理论，提升扩散模型生成图像的多样性和质量。

Method: 使用Lattice Boltzmann求解器实现PDE数值计算，结合神经网络反转PDE过程。

Result: 生成的图像质量更高且颜色不受影响，泛化了现有PDE方法。

Conclusion: 为基于PDE的图像生成提供了新视角，结合了物理模型与深度学习。

Abstract: We propose a novel PDE-driven corruption process for generative image
synthesis based on advection-diffusion processes which generalizes existing
PDE-based approaches. Our forward pass formulates image corruption via a
physically motivated PDE that couples directional advection with isotropic
diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,
Fourier). We implement this PDE numerically through a GPU-accelerated custom
Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence,
we generate stochastic velocity fields that introduce coherent motion and
capture multi-scale mixing. In the generative process, a neural network learns
to reverse the advection-diffusion operator thus constituting a novel
generative model. We discuss how previous methods emerge as specific cases of
our operator, demonstrating that our framework generalizes prior PDE-based
corruption techniques. We illustrate how advection improves the diversity and
quality of the generated images while keeping the overall color palette
unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and
deep generative modeling, offering a fresh perspective on physically informed
image corruption processes for diffusion-based synthesis.

</details>


### [69] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Main category: cs.GR

TL;DR: 通过多平面同步技术扩展2D基础模型能力，提出DreamCube模型，实现高质量3D全景生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因3D全景与2D单视图不兼容而效果受限的问题，利用2D基础模型的丰富先验提升3D全景生成质量。

Method: 采用多平面同步技术，将2D基础模型的算子扩展到全景领域；提出DreamCube，一种基于RGB-D扩散模型的多平面3D全景生成方法。

Result: 实验证明该方法在全景图像生成、全景深度估计和3D场景生成中效果显著。

Conclusion: DreamCube能高效复用2D先验，实现多样外观和精确几何，同时保持多视图一致性。

Abstract: 3D panorama synthesis is a promising yet challenging task that demands
high-quality and diverse visual appearance and geometry of the generated
omnidirectional content. Existing methods leverage rich image priors from
pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic
data, but the incompatibility between 3D panoramas and 2D single views limits
their effectiveness. In this work, we demonstrate that by applying multi-plane
synchronization to the operators from 2D foundation models, their capabilities
can be seamlessly extended to the omnidirectional domain. Based on this design,
we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D
panorama generation, which maximizes the reuse of 2D foundation model priors to
achieve diverse appearances and accurate geometry while maintaining multi-view
consistency. Extensive experiments demonstrate the effectiveness of our
approach in panoramic image generation, panoramic depth estimation, and 3D
scene generation.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [70] [Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal](https://arxiv.org/abs/2506.16000)
*Hemanth Kannamarlapudi,Sowmya Chintalapudi*

Main category: cs.ET

TL;DR: 提出了一种基于量子人工智能的新型架构，用于自动驾驶车辆的导航决策和通信，包括量子神经网络进行多模态传感器融合、Nav-Q进行强化学习优化导航策略，以及后量子密码协议确保安全通信。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在导航决策和通信方面的挑战，利用量子技术提升性能和安全性。

Method: 采用量子神经网络进行传感器数据融合，Nav-Q模块通过变分量子电路学习最优导航策略，并使用后量子密码协议保护通信安全。

Result: 架构提供了量子性能提升和面向未来的安全性，解决了自动驾驶导航中的关键问题。

Conclusion: 该框架通过量子技术和人工智能的结合，为自动驾驶车辆导航提供了高效且安全的解决方案。

Abstract: Navigation is a very crucial aspect of autonomous vehicle ecosystem which
heavily relies on collecting and processing large amounts of data in various
states and taking a confident and safe decision to define the next vehicle
maneuver. In this paper, we propose a novel architecture based on Quantum
Artificial Intelligence by enabling quantum and AI at various levels of
navigation decision making and communication process in Autonomous vehicles :
Quantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum
reinforcement learning for navigation policy optimization and finally
post-quantum cryptographic protocols for secure communication. Quantum neural
networks uses quantum amplitude encoding to fuse data from various sensors like
LiDAR, radar, camera, GPS and weather etc., This approach gives a unified
quantum state representation between heterogeneous sensor modalities. Nav-Q
module processes the fused quantum states through variational quantum circuits
to learn optimal navigation policies under swift dynamic and complex
conditions. Finally, post quantum cryptographic protocols are used to secure
communication channels for both within vehicle communication and V2X (Vehicle
to Everything) communications and thus secures the autonomous vehicle
communication from both classical and quantum security threats. Thus, the
proposed framework addresses fundamental challenges in autonomous vehicles
navigation by providing quantum performance and future proof security. Index
Terms Quantum Computing, Autonomous Vehicles, Sensor Fusion

</details>


### [71] [Artificial Intelligence for Atmospheric Sciences: A Research Roadmap](https://arxiv.org/abs/2506.16281)
*Martha Arbayani Zaidan,Naser Hossein Motlagh,Petteri Nurmi,Tareq Hussein,Markku Kulmala,Tuukka Petäjä,Sasu Tarkoma*

Main category: cs.ET

TL;DR: 论文总结了人工智能在大气科学中的重要作用及挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何推动大气科学研究，解决数据与基础设施问题。

Method: 提供跨学科综述，识别AI在大气科学中的关键挑战。

Result: 提出了应对当前与未来挑战的研究路线图。

Conclusion: AI在大气科学中具有变革潜力，但仍需解决数据与基础设施问题。

Abstract: Atmospheric sciences are crucial for understanding environmental phenomena
ranging from air quality to extreme weather events, and climate change. Recent
breakthroughs in sensing, communication, computing, and Artificial Intelligence
(AI) have significantly advanced atmospheric sciences, enabling the generation
of vast amounts of data through long-term Earth observations and providing
powerful tools for analyzing atmospheric phenomena and predicting natural
disasters. This paper contributes a critical interdisciplinary overview that
bridges the fields of atmospheric science and computer science, highlighting
the transformative potential of AI in atmospheric research. We identify key
challenges associated with integrating AI into atmospheric research, including
issues related to big data and infrastructure, and provide a detailed research
roadmap that addresses both current and emerging challenges.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [72] [TrainVerify: Equivalence-Based Verification for Distributed LLM Training](https://arxiv.org/abs/2506.15961)
*Yunchi Lu,Youshan Miao,Cheng Tan,Peng Huang,Yi Zhu,Xian Zhang,Fan Yang*

Main category: cs.DC

TL;DR: TrainVerify是一个用于验证大规模语言模型分布式训练正确性的系统，通过形式化验证确保分布式执行计划与逻辑规范一致。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型训练成本高昂且易受隐性错误影响，亟需一种验证方法以确保训练的正确性和效率。

Method: 引入形状缩减技术和阶段性并行验证算法，大幅降低验证复杂度并保持形式正确性。

Result: 成功验证了包括Llama3（405B）和DeepSeek-V3（671B）在内的前沿LLM训练计划。

Conclusion: TrainVerify为大规模分布式训练提供了一种高效可靠的验证手段，显著降低了错误风险和计算资源浪费。

Abstract: Training large language models (LLMs) at scale requires parallel execution
across thousands of devices, incurring enormous computational costs. Yet, these
costly distributed trainings are rarely verified, leaving them prone to silent
errors and potentially wasting millions of GPU hours. We introduce TrainVerify,
a system for verifiable distributed training of LLMs. Given a deep learning
model's logical specification as the ground truth, TrainVerify formally
verifies that a distributed parallel execution plan is mathematically
equivalent to it. Direct verification is notoriously difficult due to the sheer
scale of LLMs which often involves billions of variables and highly intricate
computation graphs. Therefore, TrainVerify introduces shape-reduction
techniques and a stage-wise parallel verification algorithm that significantly
reduces complexity while preserving formal correctness. TrainVerify scales to
frontier LLMs, including the successful verification of the Llama3 (405B) and
DeepSeek-V3 (671B) training plans.

</details>


### [73] [NetSenseML: Network-Adaptive Compression for Efficient Distributed Machine Learning](https://arxiv.org/abs/2506.16235)
*Yisu Wang,Xinjiao Li,Ruilong Wu,Huangxun Chen,Dirk Kutscher*

Main category: cs.DC

TL;DR: NetSenseML是一种动态调整梯度压缩策略的网络自适应分布式深度学习框架，根据实时网络条件平衡负载与模型精度，显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式机器学习训练对网络需求高，传统梯度压缩技术虽减少负载但牺牲模型精度。

Method: NetSenseML动态调整量化、剪枝和压缩策略，通过实时监测网络条件仅在必要时压缩梯度。

Result: 实验显示NetSenseML在带宽受限条件下，训练吞吐量提升1.55至9.84倍。

Conclusion: NetSenseML通过自适应优化，显著减少收敛时间并提高训练效率。

Abstract: Training large-scale distributed machine learning models imposes considerable
demands on network infrastructure, often resulting in sudden traffic spikes
that lead to congestion, increased latency, and reduced throughput, which would
ultimately affect convergence times and overall training performance. While
gradient compression techniques are commonly employed to alleviate network
load, they frequently compromise model accuracy due to the loss of gradient
information.
  This paper introduces NetSenseML, a novel network adaptive distributed deep
learning framework that dynamically adjusts quantization, pruning, and
compression strategies in response to real-time network conditions. By actively
monitoring network conditions, NetSenseML applies gradient compression only
when network congestion negatively impacts convergence speed, thus effectively
balancing data payload reduction and model accuracy preservation.
  Our approach ensures efficient resource usage by adapting reduction
techniques based on current network conditions, leading to shorter convergence
times and improved training efficiency. We present the design of the NetSenseML
adaptive data reduction function and experimental evaluations show that
NetSenseML can improve training throughput by a factor of 1.55 to 9.84 times
compared to state-of-the-art compression-enabled systems for representative DDL
training jobs in bandwidth-constrained conditions.

</details>


### [74] [A Study of Synchronization Methods for Concurrent Size](https://arxiv.org/abs/2506.16350)
*Hen Kas-Sharir,Gal Sela,Erez Petrank*

Main category: cs.DC

TL;DR: 研究了并发环境下数据结构的同步方法，以减少线性化并发大小操作的性能开销。


<details>
  <summary>Details</summary>
Motivation: 在并发环境中，线性化并发大小的实现会在所有操作中引入显著开销，即使未调用大小方法。

Method: 研究了三种同步方法：握手技术（用于并发垃圾回收）、乐观技术和基于锁的技术。

Result: 通过选择适当的同步方法可以显著减少开销，但无单一最佳方法；低竞争时乐观和锁方法最佳，高竞争时握手和无等待方法更优。

Conclusion: 不同场景需要不同的同步方法，研究结果与并发计算的普遍趋势一致。

Abstract: The size of collections, maps, and data structures in general, constitutes a
fundamental property. An implementation of the size method is required in most
programming environments. Nevertheless, in a concurrent environment,
integrating a linearizable concurrent size introduces a noticeable overhead on
all operations of the data structure, even when the size method is not invoked
during the execution. In this work we present a study of synchronization
methods in an attempt to improve the performance of the data structure. In
particular, we study a handshake technique that is commonly used with
concurrent garbage collection, an optimistic technique, and a lock-based
technique. Evaluation against the state-of-the-art size methodology
demonstrates that the overhead can be significantly reduced by selecting the
appropriate synchronization approach, but there is no one-size-fits-all method.
Different scenarios call for different synchronization methods, as rigorously
shown in this study. Nevertheless, our findings align with general trends in
concurrent computing. In scenarios characterized by low contention, optimistic
and lock-based approaches work best, whereas under high contention, the most
effective solutions are the handshake approach and the wait-free approach.

</details>


### [75] [Parallel Point-to-Point Shortest Paths and Batch Queries](https://arxiv.org/abs/2506.16488)
*Xiaojun Dong,Andy Li,Yan Gu,Yihan Sun*

Main category: cs.DC

TL;DR: Orionet是一种高效的并行点对点最短路径(PPSP)查询实现,结合双向搜索(BiDS)等启发式方法,特别关注批量PPSP查询。其基于现有单源最短路径(SSSP)框架,通过引入剪枝条件,实现了具有简单高效实现的并行PPSP算法。在14个测试图上,双向搜索平均比GraphIt快2.9倍,比MBQ快6.8倍;双向A*搜索比GraphIt和MBQ分别快4.4倍和6.2倍。


<details>
  <summary>Details</summary>
Motivation: 解决点对点最短路径(PPSP)查询的高效并行计算问题,特别是在批量查询场景中,以提高实际应用中的性能。

Method: 基于双向搜索(BiDS)、A*搜索及其双向变体,设计了一个并行PPSP框架,并扩展到批量查询,利用批量查询的共享信息。

Result: 在多种图类型和查询对上,Orionet显著优于现有基线(GraphIt和MBQ),双向搜索和双向A*搜索分别实现了显著的性能提升。

Conclusion: Orionet通过简单高效的实现和批量查询优化,显著提升了PPSP查询的计算效率,适用于实际应用场景。

Abstract: We propose Orionet, efficient parallel implementations of Point-to-Point
Shortest Paths (PPSP) queries using bidirectional search (BiDS) and other
heuristics, with an additional focus on batch PPSP queries. We present a
framework for parallel PPSP built on existing single-source shortest paths
(SSSP) frameworks by incorporating pruning conditions. As a result, we develop
efficient parallel PPSP algorithms based on early termination, bidirectional
search, A$^*$ search, and bidirectional A$^*$ all with simple and efficient
implementations.
  We extend our idea to batch PPSP queries, which are widely used in real-world
scenarios. We first design a simple and flexible abstraction to represent the
batch so PPSP can leverage the shared information of the batch. Orionet
formalizes the batch as a query graph represented by edges between queried
sources and targets. In this way, we directly extended our PPSP framework to
batched queries in a simple and efficient way.
  We evaluate Orionet on both single and batch PPSP queries using various graph
types and distance percentiles of queried pairs, and compare it against two
baselines, GraphIt and MBQ. Both of them support parallel single PPSP and A$^*$
using unidirectional search. On 14 graphs we tested, on average, our
bidirectional search is 2.9$\times$ faster than GraphIt, and 6.8$\times$ faster
than MBQ. Our bidirectional A$^*$ is 4.4$\times$ and 6.2$\times$ faster than
the A$^*$ in GraphIt and MBQ, respectively. For batched PPSP queries, we also
provide in-depth experimental evaluation, and show that Orionet provides strong
performance compared to the plain solutions.

</details>


### [76] [JANUS: Resilient and Adaptive Data Transmission for Enabling Timely and Efficient Cross-Facility Scientific Workflows](https://arxiv.org/abs/2506.17084)
*Vladislav Esaulov,Jieyang Chen,Norbert Podhorszki,Fred Suter,Scott Klasky,Anu G Bourgeois,Lipeng Wan*

Main category: cs.DC

TL;DR: 本文提出了一种名为JANUS的弹性自适应数据传输方法，用于跨设施科学工作流，通过UDP、纠删码和误差有损压缩等技术提升数据传输效率。


<details>
  <summary>Details</summary>
Motivation: 跨设施科学工作流中大规模数据传输面临带宽压力、TCP重传和传统容错方法的高开销问题。

Method: JANUS结合UDP、纠删码和误差有损压缩，动态调整编码参数以适应实时网络条件，并通过优化模型确定最佳配置。

Result: 实验表明，JANUS显著提升了数据传输效率，同时保持了数据保真度。

Conclusion: JANUS为跨设施科学工作流提供了一种高效、弹性的数据传输解决方案。

Abstract: In modern science, the growing complexity of large-scale projects has
increased reliance on cross-facility workflows, where institutions share
resources and expertise to accelerate discovery. These workflows often involve
transferring massive data over wide-area networks. While high-speed networks
like ESnet and data transfer services like Globus have improved data mobility,
challenges remain. Large data volumes can strain bandwidth, TCP suffers from
retransmissions due to packet loss, and traditional fault-tolerance methods
like erasure coding introduce significant overhead.
  This paper presents JANUS, a resilient and adaptive data transmission
approach for cross-facility scientific workflows. JANUS uses UDP, integrates
erasure coding for fault tolerance, and applies error-bounded lossy compression
to reduce overhead. This design enables users to balance transmission time and
accuracy based on specific needs. JANUS also adapts coding parameters to
real-time network conditions and uses optimization models to determine ideal
configurations. Experiments show that JANUS significantly improves data
transfer efficiency while preserving fidelity.

</details>


### [77] [Enabling Blockchain Interoperability Through Network Discovery Services](https://arxiv.org/abs/2506.16611)
*Khalid Hassan,Amirreza Sokhankhosh,Sara Rouhani*

Main category: cs.DC

TL;DR: 论文提出一种去中心化的区块链网络发现架构，解决现有方案未解决的初始网络发现问题，并通过激励机制鼓励节点参与网络维护，实验证明其具有高扩展性和低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着Web3技术的快速发展，区块链网络的互操作性已有进展，但初始网络发现问题仍未被解决，阻碍了去中心化服务的全面实现。

Method: 设计并实现了一种去中心化的区块链网络发现架构，包含资产和服务发现机制，并通过激励机制促进节点参与。采用Substrate框架进行评估。

Result: 在实验网络中，该架构能处理13万并发请求，中位响应时间为5.5毫秒，且能通过扩展网络进一步提升处理能力。

Conclusion: 提出的去中心化发现架构具有高扩展性和低延迟，为区块链网络的初始发现问题提供了可行的解决方案。

Abstract: Web3 technologies have experienced unprecedented growth in the last decade,
achieving widespread adoption. As various blockchain networks continue to
evolve, we are on the cusp of a paradigm shift in which they could provide
services traditionally offered by the Internet, but in a decentralized manner,
marking the emergence of the Internet of Blockchains. While significant
progress has been achieved in enabling interoperability between blockchain
networks, existing solutions often assume that networks are already mutually
aware. This reveals a critical gap: the initial discovery of blockchain
networks remains largely unaddressed. This paper proposes a decentralized
architecture for blockchain network discovery that operates independently of
any centralized authority. We also introduce a mechanism for discovering assets
and services within a blockchain from external networks. Given the
decentralized nature of the proposed discovery architecture, we design an
incentive mechanism to encourage nodes to actively participate in maintaining
the discovery network. The proposed architecture implemented and evaluated,
using the Substrate framework, demonstrates its resilience and scalability,
effectively handling up to 130,000 concurrent requests under the tested network
configurations, with a median response time of 5.5 milliseconds, demonstrating
the ability to scale its processing capacity further by increasing its network
size.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [78] [Adaptive Anomaly Detection in the Presence of Concept Drift: Extended Report](https://arxiv.org/abs/2506.15831)
*Jongjun Park,Fei Chiang,Mostafa Milani*

Main category: cs.DB

TL;DR: AnDri系统用于在数据漂移存在的情况下进行异常检测，通过调整时间上的正常模式并区分异常子序列和新概念，同时提出了一种新的聚类方法AHC。


<details>
  <summary>Details</summary>
Motivation: 数据分布的变化（概念漂移）和异常变化给时间序列中的异常检测带来挑战。现有工作通常孤立地研究这两个问题，缺乏综合解决方案。

Method: 开发了AnDri系统，通过时间调整正常模式区分异常和新概念，并引入Adjacent Hierarchical Clustering（AHC）聚类方法。

Result: AnDri能够有效区分概念漂移和异常，提高检测准确性并减少不必要的模型更新开销。

Conclusion: AnDri系统提供了一种综合解决方案，解决了概念漂移和异常检测的联合问题，具有实用价值。

Abstract: Data changes to reflect evolving user behaviour, preferences, and changes in
the environment. Such changes may occur due to expected shifts in the data
distribution, i.e., concept drift, or unexpected anomalous changes. The
presence of concept drift poses challenges for anomaly detection in time
series. While anomalies are caused by undesirable changes in the data,
differentiating abnormal changes from varying normal behaviours is difficult
due to differing frequencies of occurrence, varying time intervals when normal
patterns occur. Differentiating between concept drift and anomalies is critical
for accurate analysis as studies have shown that the compounding effects of
error propagation in downstream data analysis tasks lead to lower detection
accuracy and increased overhead due to unnecessary model updates.
Unfortunately, existing work has largely explored anomaly detection and concept
drift detection in isolation. We develop AnDri, a system for Anomaly detection
in the presence of Drift, which adjusts the normal patterns temporally, and
distinguish abnormal subsequences and new concepts. Moreover, it introduces a
new clustering method, Adjacent Hierarchical Clustering (AHC), which groups
similar subsequences while respecting their temporal locality.

</details>


### [79] [Delta: A Learned Mixed Cost-based Query Optimization Framework](https://arxiv.org/abs/2506.15848)
*Jiazhen Peng,Zheng Qu,Xiaoye Miao,Rong Zhu*

Main category: cs.DB

TL;DR: Delta是一个混合成本查询优化框架，通过兼容查询检测器和两阶段规划器，解决了传统优化器的不足，提供更高效的查询计划。


<details>
  <summary>Details</summary>
Motivation: 现有查询优化器存在搜索空间爆炸、启发式剪枝限制、训练成本高和准确性低的问题，且缺乏性能不佳查询的检测机制。

Method: Delta采用兼容查询检测器预筛性能可能不佳的查询，两阶段规划器结合值网络和成本模型生成高效计划。

Result: 在三个工作负载上，Delta平均性能提升2.34倍，优于最先进学习方法。

Conclusion: Delta通过混合方法有效解决了传统优化器的缺陷，显著提升了查询优化性能。

Abstract: Query optimizer is a crucial module for database management systems. Existing
optimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic
programming with cost models but face search space explosion and heuristic
pruning constraints; (2) value-based ones train value networks to enable
efficient beam search, but incur higher training costs and lower accuracy. They
also lack mechanisms to detect queries where they may perform poorly. To
determine more efficient plans, we propose Delta, a mixed cost-based query
optimization framework that consists of a compatible query detector and a
two-stage planner. Delta first employs a Mahalanobis distancebased detector to
preemptively filter out incompatible queries where the planner might perform
poorly. For compatible queries, Delta activates its two-stage mixed cost-based
planner. Stage I serves as a coarse-grained filter to generate high-quality
candidate plans based on the value network via beam search, relaxing precision
requirements and narrowing the search space. Stage II employs a fine-grained
ranker to determine the best plan from the candidate plans based on a learned
cost model. Moreover, to reduce training costs, we reuse and augment the
training data from stage I to train the model in stage II. Experimental results
on three workloads demonstrate that Delta identifies higher-quality plans,
achieving an average 2.34x speedup over PostgreSQL and outperforming the
state-of-the-art learned methods by 2.21x.

</details>


### [80] [Empowering Graph-based Approximate Nearest Neighbor Search with Adaptive Awareness Capabilities](https://arxiv.org/abs/2506.15986)
*Jiancheng Ruan,Tingyang Chen,Renchi Yang,Xiangyu Ke,Yunjun Gao*

Main category: cs.DB

TL;DR: 论文提出了一种名为GATE的轻量级自适应模块，用于加速高维空间中的近似最近邻搜索（ANNS），通过优化入口点选择和利用对比学习模型，显著提升了查询性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于图的ANNS方法在拓扑信息利用不足和分布失配方面的挑战。

Method: 提出GATE模块，包括提取中心节点、使用对比学习模型编码结构和查询特征，并构建导航图索引。

Result: 实验表明，GATE比现有方法在查询性能上提升了1.2-2.0倍。

Conclusion: GATE通过自适应优化入口点选择和高效编码，显著提升了高维ANNS的效率。

Abstract: Approximate Nearest Neighbor Search (ANNS) in high-dimensional spaces finds
extensive applications in databases, information retrieval, recommender
systems, etc. While graph-based methods have emerged as the leading solution
for ANNS due to their superior query performance, they still face several
challenges, such as struggling with local optima and redundant computations.
These issues arise because existing methods (i) fail to fully exploit the
topological information underlying the proximity graph G, and (ii) suffer from
severe distribution mismatches between the base data and queries in practice.
  To this end, this paper proposes GATE, high-tier proximity Graph with
Adaptive Topology and Query AwarEness, as a lightweight and adaptive module
atop the graph-based indexes to accelerate ANNS. Specifically, GATE formulates
the critical problem to identify an optimal entry point in the proximity graph
for a given query, facilitating faster online search. By leveraging the
inherent clusterability of high-dimensional data, GATE first extracts a small
set of hub nodes V as candidate entry points. Then, resorting to a contrastive
learning-based two-tower model, GATE encodes both the structural semantics
underlying G and the query-relevant features into the latent representations of
these hub nodes V. A navigation graph index on V is further constructed to
minimize the model inference overhead. Extensive experiments demonstrate that
GATE achieves a 1.2-2.0X speed-up in query performance compared to
state-of-the-art graph-based indexes.

</details>


### [81] [Filter-Centric Vector Indexing: Geometric Transformation for Efficient Filtered Vector Search](https://arxiv.org/abs/2506.15987)
*Alireza Heidari,Wei Zhang*

Main category: cs.DB

TL;DR: FCVI框架通过将过滤条件直接编码到向量空间中，显著提高了向量搜索的效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前向量搜索方法在性能和准确性之间难以平衡，而FCVI旨在解决这一挑战。

Method: FCVI通过数学变换$ψ(v, f, α)$将过滤条件编码到向量空间，兼容现有向量索引。

Result: FCVI的吞吐量比现有方法高2.6-3.0倍，且在分布变化时表现稳定。

Conclusion: FCVI为需要灵活过滤功能的生产系统提供了高效且兼容的解决方案。

Abstract: The explosive growth of vector search applications demands efficient handling
of combined vector similarity and attribute filtering; a challenge where
current approaches force an unsatisfying choice between performance and
accuracy. We introduce Filter-Centric Vector Indexing (FCVI), a novel framework
that transforms this fundamental trade-off by directly encoding filter
conditions into the vector space through a mathematically principled
transformation $\psi(v, f, \alpha)$. Unlike specialized solutions, FCVI works
with any existing vector index (HNSW, FAISS, ANNOY) while providing theoretical
guarantees on accuracy. Our comprehensive evaluation demonstrates that FCVI
achieves 2.6-3.0 times higher throughput than state-of-the-art methods while
maintaining comparable recall. More remarkably, FCVI exhibits exceptional
stability under distribution shifts; maintaining consistent performance when
filter patterns or vector distributions change, unlike traditional approaches
that degrade significantly. This combination of performance, compatibility, and
resilience positions FCVI as an immediately applicable solution for production
vector search systems requiring flexible filtering capabilities.

</details>


### [82] [Data-Agnostic Cardinality Learning from Imperfect Workloads](https://arxiv.org/abs/2506.16007)
*Peizhi Wu,Rong Kang,Tieying Zhang,Jianjun Chen,Ryan Marcus,Zachary G. Ives*

Main category: cs.DB

TL;DR: GRASP是一种数据无关的基数估计系统，适用于真实世界中不完整和不平衡的查询负载，性能优于现有查询驱动模型，甚至接近传统基于数据的模型。


<details>
  <summary>Details</summary>
Motivation: 传统基数估计依赖数据访问，但在合规要求下受限；现有查询驱动模型需要数据或完美训练负载，而真实场景往往不满足。

Method: GRASP采用组合设计，包括新每表基数模型处理区间谓词分布变化，及学习型计数草图模型捕获跨表连接相关性。

Result: 在三个数据库实例中，GRASP在不完美负载下估计精度和查询延迟均优于现有模型，在CEB-IMDb-full基准上媲美传统数据驱动方法。

Conclusion: GRASP在无数据访问且仅用10%连接模板的情况下，展现了强泛化能力和鲁棒性，适用于真实约束场景。

Abstract: Cardinality estimation (CardEst) is a critical aspect of query optimization.
Traditionally, it leverages statistics built directly over the data. However,
organizational policies (e.g., regulatory compliance) may restrict global data
access. Fortunately, query-driven cardinality estimation can learn CardEst
models using query workloads. However, existing query-driven models often
require access to data or summaries for best performance, and they assume
perfect training workloads with complete and balanced join templates (or join
graphs). Such assumptions rarely hold in real-world scenarios, in which join
templates are incomplete and imbalanced. We present GRASP, a data-agnostic
cardinality learning system designed to work under these real-world
constraints. GRASP's compositional design generalizes to unseen join templates
and is robust to join template imbalance. It also introduces a new per-table
CardEst model that handles value distribution shifts for range predicates, and
a novel learned count sketch model that captures join correlations across base
relations. Across three database instances, we demonstrate that GRASP
consistently outperforms existing query-driven models on imperfect workloads,
both in terms of estimation accuracy and query latency. Remarkably, GRASP
achieves performance comparable to, or even surpassing, traditional approaches
built over the underlying data on the complex CEB-IMDb-full benchmark --
despite operating without any data access and using only 10% of all possible
join templates.

</details>


### [83] [PBench: Workload Synthesizer with Real Statistics for Cloud Analytics Benchmarking](https://arxiv.org/abs/2506.16379)
*Yan Zhou,Chunwei Liu,Bhuvan Urgaonkar,Zhengle Wang,Magnus Mueller,Chao Zhang,Songyue Zhang,Pascal Pfeil,Dominik Horn,Zhengchun Liu,Davide Pagano,Tim Kraska,Samuel Madden,Ju Fan*

Main category: cs.DB

TL;DR: 本文提出PBench，一种通过组合现有基准测试组件生成近似真实云工作负载的合成工作负载，解决了传统基准测试无法捕捉真实执行统计的问题。


<details>
  <summary>Details</summary>
Motivation: 云服务提供商常用标准基准测试评估系统，但这些测试无法真实反映生产工作负载的统计特性，需要一种新的合成工作负载方法。

Method: PBench采用多目标优化选择组件、渐进式时间戳分配和基于LLM的组件增强，以平衡性能指标和操作符分布。

Result: 在真实工作负载上的评估显示，PBench将近似误差降低至现有方法的1/6。

Conclusion: PBench有效生成了统计近似真实的合成工作负载，填补了标准基准与实际工作负载间的差距。

Abstract: Cloud service providers commonly use standard benchmarks like TPC-H and
TPC-DS to evaluate and optimize cloud data analytics systems. However, these
benchmarks rely on fixed query patterns and fail to capture the real execution
statistics of production cloud workloads. Although some cloud database vendors
have recently released real workload traces, these traces alone do not qualify
as benchmarks, as they typically lack essential components like the original
SQL queries and their underlying databases. To overcome this limitation, this
paper introduces a new problem of workload synthesis with real statistics,
which aims to generate synthetic workloads that closely approximate real
execution statistics, including key performance metrics and operator
distributions, in real cloud workloads. To address this problem, we propose
PBench, a novel workload synthesizer that constructs synthetic workloads by
judiciously selecting and combining workload components (i.e., queries and
databases) from existing benchmarks. This paper studies the key challenges in
PBench. First, we address the challenge of balancing performance metrics and
operator distributions by introducing a multi-objective optimization-based
component selection method. Second, to capture the temporal dynamics of real
workloads, we design a timestamp assignment method that progressively refines
workload timestamps. Third, to handle the disparity between the original
workload and the candidate workload, we propose a component augmentation
approach that leverages large language models (LLMs) to generate additional
workload components while maintaining statistical fidelity. We evaluate PBench
on real cloud workload traces, demonstrating that it reduces approximation
error by up to 6x compared to state-of-the-art methods.

</details>


### [84] [LDI: Localized Data Imputation](https://arxiv.org/abs/2506.16616)
*Soroush Omidvartehrani,Davood Rafiei*

Main category: cs.DB

TL;DR: LDI框架通过本地化提示提升LLM在数据填充中的准确性和透明度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM数据填充方法因宽泛提示导致的准确性、可扩展性和可解释性问题。

Method: LDI选择上下文相关的属性和元组子集，进行本地化提示，减少噪声并提供可追溯性。

Result: LDI在四个真实数据集上表现优于现有方法，准确率提升高达8%（托管LLM）和97%（轻量本地模型）。

Conclusion: LDI在准确性、可解释性和数据一致性方面表现优异，适用于高风险和隐私敏感场景。

Abstract: Missing values are a common challenge in real-world tabular data and can
significantly impair downstream analysis. While Large Language Models (LLMs)
have recently shown promise in data imputation, existing methods often rely on
broad, unfiltered prompts that compromise accuracy, scalability, and
explainability. We introduce LDI (Localized Data Imputation), a novel framework
that improves both the accuracy and transparency of LLM-based imputation by
selecting a compact, contextually relevant subset of attributes and tuples for
each missing value. This localized prompting reduces noise, enables
traceability by revealing which data influenced each prediction, and is
effective across both hosted LLMs and lightweight local models. Our extensive
experiments on four real-world datasets show that LDI outperforms
state-of-the-art methods, achieving up to 8% higher accuracy when using hosted
LLMs. The gains are more substantial with lightweight local models, reaching
nearly 17% and 97% accuracy on some datasets when using 3 and 10 examples,
respectively. In addition to higher accuracy, LDI offers improved
interpretability and robustness to data inconsistencies, making it well-suited
for high-stakes and privacy-sensitive applications.

</details>


### [85] [Advancing Fact Attribution for Query Answering: Aggregate Queries and Novel Algorithms](https://arxiv.org/abs/2506.16923)
*Omer Abramovich,Daniel Deutch,Nave Frost,Ahmet Kara,Dan Olteanu*

Main category: cs.DB

TL;DR: 提出了一种新方法计算输入元组对查询结果的贡献，首次适用于聚合查询，并优化了性能，实现了3个数量级的运行时间提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有工作无法处理聚合查询的问题，同时提升查询计算的性能。

Method: 采用两种新颖优化：1. 计算相同贡献的元组之一；2. 利用查询谱系的梯度快速计算所有元组贡献。

Result: 实验表明，无聚合查询性能提升3个数量级，且首次实用于聚合查询。

Conclusion: 该方法显著提升了计算效率和实用性，为聚合查询提供了解决方案。

Abstract: In this paper, we introduce a novel approach to computing the contribution of
input tuples to the result of the query, quantified by the Banzhaf and Shapley
values. In contrast to prior algorithmic work that focuses on
Select-Project-Join-Union queries, ours is the first practical approach for
queries with aggregates. It relies on two novel optimizations that are
essential for its practicality and significantly improve the runtime
performance already for queries without aggregates. The first optimization
exploits the observation that many input tuples have the same contribution to
the query result, so it is enough to compute the contribution of one of them.
The second optimization uses the gradient of the query lineage to compute the
contributions of all tuples with the same complexity as for one of them.
Experiments with a million instances over 3 databases show that our approach
achieves up to 3 orders of magnitude runtime improvements over the
state-of-the-art for queries without aggregates, and that it is practical for
aggregate queries.

</details>


### [86] [PUL: Pre-load in Software for Caches Wouldn't Always Play Along](https://arxiv.org/abs/2506.16976)
*Arthur Bernhardt,Sajjad Tamimi,Florian Stock,Andreas Koch,Ilia Petrov*

Main category: cs.DB

TL;DR: 该论文研究了基于软件的预取技术，特别是在近数据处理环境中如何通过计算与I/O交织最大化计算利用率。


<details>
  <summary>Details</summary>
Motivation: 内存延迟和带宽是限制系统性能和可扩展性的主要因素，而软件预取技术具有更高的效率，尤其在新型CPU中表现更佳。

Method: 研究了软件驱动的、后摩尔时代的系统，将操作卸载到智能内存中，并通过计算与I/O的交织优化计算利用率。

Result: 软件预取在近数据处理环境中展现出更高的潜力。

Conclusion: 软件预取技术在未来计算系统中具有重要应用前景，尤其是在近数据处理方面。

Abstract: Memory latencies and bandwidth are major factors, limiting system performance
and scalability. Modern CPUs aim at hiding latencies by employing large caches,
out-of-order execution, or complex hardware prefetchers. However,
software-based prefetching exhibits higher efficiency, improving with newer CPU
generations.
  In this paper we investigate software-based, post-Moore systems that offload
operations to intelligent memories. We show that software-based prefetching has
even higher potential in near-data processing settings by maximizing compute
utilization through compute/IO interleaving.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [87] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Main category: cs.AR

TL;DR: DeepRTL2通过统一生成和嵌入任务，为EDA领域提供了全面的LLM解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在LLM的生成任务上，忽略了嵌入任务在EDA中的重要性，如代码搜索和性能预测。DeepRTL2填补了这一空白。

Method: 提出DeepRTL2，一个多功能LLM家族，统一处理RTL相关的生成和嵌入任务。

Result: 实验表明，DeepRTL2在所有评估任务中均达到最先进的性能。

Conclusion: DeepRTL2首次为EDA中的多样化挑战提供了全面的解决方案。

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


### [88] [Profile-Guided Temporal Prefetching](https://arxiv.org/abs/2506.15985)
*Mengming Li,Qijun Zhang,Yichuan Gao,Wenji Fang,Yao Lu,Yongqing Ren,Zhiyao Xie*

Main category: cs.AR

TL;DR: 提出Prophet，一种硬件-软件协同设计框架，通过基于计数器的方法优化元数据存储管理，显著提升时空预测的效率。


<details>
  <summary>Details</summary>
Motivation: 现有时空预测方案无法高效利用有限的片上存储，而软件间接访问预测对时空预测的优化效果有限。

Method: 使用基于计数器而非追踪的程序分析方法，注入动态调整的提示以指导元数据存储管理，并与现有硬件预测器共存。

Result: Prophet比当前最佳时空预测器Triangel性能提升14.23%，在所有测试负载中表现优异，且开销极小。

Conclusion: Prophet通过硬件-软件协同设计，成功解决了复杂时空模式的预测难题，显著提升了性能和适应性。

Abstract: Temporal prefetching shows promise for handling irregular memory access
patterns, which are common in data-dependent and pointer-based data structures.
Recent studies introduced on-chip metadata storage to reduce the memory traffic
caused by accessing metadata from off-chip DRAM. However, existing prefetching
schemes struggle to efficiently utilize the limited on-chip storage. An
alternative solution, software indirect access prefetching, remains ineffective
for optimizing temporal prefetching.
  In this work, we propose Prophet--a hardware-software co-designed framework
that leverages profile-guided methods to optimize metadata storage management.
Prophet profiles programs using counters instead of traces, injects hints into
programs to guide metadata storage management, and dynamically tunes these
hints to enable the optimized binary to adapt to different program inputs.
Prophet is designed to coexist with existing hardware temporal prefetchers,
delivering efficient, high-performance solutions for frequently executed
workloads while preserving the original runtime scheme for less frequently
executed workloads. Prophet outperforms the state-of-the-art temporal
prefetcher, Triangel, by 14.23%, effectively addressing complex temporal
patterns where prior profile-guided solutions fall short (only achieving 0.1%
performance gain). Prophet delivers superior performance across all evaluated
workload inputs, introducing negligible profiling, analysis, and instruction
overhead.

</details>


### [89] [HetGPU: The pursuit of making binary compatibility towards GPUs](https://arxiv.org/abs/2506.15993)
*Yiwei Yang,Yusheng Zheng,Tong Yu,Andi Quinn*

Main category: cs.AR

TL;DR: hetGPU是一个新系统，通过编译器、运行时和抽象层，使单一GPU二进制文件能在不同厂商的硬件上运行。


<details>
  <summary>Details</summary>
Motivation: 解决异构GPU基础设施因指令集、执行模型和驱动堆栈不同导致的二进制兼容性问题。

Method: 使用架构无关的GPU中间表示（IR）和运行时动态翻译技术，统一线程、内存和同步操作的抽象。

Result: 初步评估显示，未修改的GPU二进制文件能在不同GPU间迁移，开销极小。

Conclusion: hetGPU为实现厂商无关的GPU计算提供了可能。

Abstract: Heterogeneous GPU infrastructures present a binary compatibility challenge:
code compiled for one vendor's GPU will not run on another due to divergent
instruction sets, execution models, and driver stacks . We propose hetGPU, a
new system comprising a compiler, runtime, and abstraction layer that together
enable a single GPU binary to execute on NVIDIA, AMD, Intel, and Tenstorrent
hardware. The hetGPU compiler emits an architecture-agnostic GPU intermediate
representation (IR) and inserts metadata for managing execution state. The
hetGPU runtime then dynamically translates this IR to the target GPU's native
code and provides a uniform abstraction of threads, memory, and
synchronization. Our design tackles key challenges: differing SIMT vs. MIMD
execution (warps on NVIDIA/AMD vs. many-core RISC-V on Tenstorrent), varied
instruction sets, scheduling and memory model discrepancies, and the need for
state serialization for live migration. We detail the hetGPU architecture,
including the IR transformation pipeline, a state capture/reload mechanism for
live GPU migration, and an abstraction layer that bridges warp-centric and
core-centric designs. Preliminary evaluation demonstrates that unmodified GPU
binaries compiled with hetGPU can be migrated across disparate GPUs with
minimal overhead, opening the door to vendor-agnostic GPU computing.

</details>


### [90] [SparseDPD: A Sparse Neural Network-based Digital Predistortion FPGA Accelerator for RF Power Amplifier Linearization](https://arxiv.org/abs/2506.16591)
*Manno Versluis,Yizhuo Wu,Chang Gao*

Main category: cs.AR

TL;DR: SparseDPD是一种基于FPGA的加速器，通过稀疏神经网络优化实现低功耗高性能的数字预失真，提升无线通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统多项式模型在数字预失真(DPD)中性能有限，而神经网络方法虽优但计算复杂度高，难以实际部署。

Method: 提出SparseDPD，采用空间稀疏的相位归一化时延神经网络(PNTDNN)，通过非结构化剪枝优化计算负载。

Result: 在Xilinx Zynq-7Z010 FPGA上实现170 MHz运行，ACPR达-59.4 dBc，动态功耗仅241 mW，参数64个且稀疏度74%。

Conclusion: SparseDPD证明了FPGA加速的可行性，使神经网络DPD在实时无线通信中更实用高效。

Abstract: Digital predistortion (DPD) is crucial for linearizing radio frequency (RF)
power amplifiers (PAs), improving signal integrity and efficiency in wireless
systems. Neural network (NN)-based DPD methods surpass traditional polynomial
models but face computational challenges limiting their practical deployment.
This paper introduces SparseDPD, an FPGA accelerator employing a spatially
sparse phase-normalized time-delay neural network (PNTDNN), optimized through
unstructured pruning to reduce computational load without accuracy loss.
Implemented on a Xilinx Zynq-7Z010 FPGA, SparseDPD operates at 170 MHz,
achieving exceptional linearization performance (ACPR: -59.4 dBc, EVM: -54.0
dBc, NMSE: -48.2 dB) with only 241 mW dynamic power, using 64 parameters with
74% sparsity. This work demonstrates FPGA-based acceleration, making NN-based
DPD practical and efficient for real-time wireless communication applications.
Code is publicly available at https://github.com/MannoVersluis/SparseDPD.

</details>


### [91] [Lookup Table-based Multiplication-free All-digital DNN Accelerator Featuring Self-Synchronous Pipeline Accumulation](https://arxiv.org/abs/2506.16800)
*Hiroto Tagata,Takashi Sato,Hiromitsu Awano*

Main category: cs.AR

TL;DR: 论文提出了一种基于MADDNESS的全数字加速器，采用自同步流水线累加器，显著提升了能量效率和面积效率。


<details>
  <summary>Details</summary>
Motivation: 减少大规模矩阵计算带来的高能耗是深度神经网络应用中的关键挑战，现有的MADDNESS方法存在面积效率和计算精度的问题。

Method: 提出了一种新型的基于MADDNESS的全数字加速器，利用自同步流水线累加器技术。

Result: 后仿真结果显示，与传统加速器相比，能量效率提升了2.5倍（174 TOPS/W），面积效率提升了5倍（2.01 TOPS/mm²）。

Conclusion: 该全数字加速器在能量效率和面积效率上具有显著优势，适用于实际应用场景。

Abstract: Deep neural networks (DNNs) have been widely applied in our society, yet
reducing power consumption due to large-scale matrix computations remains a
critical challenge. MADDNESS is a known approach to improving energy efficiency
by substituting matrix multiplication with table lookup operations. Previous
research has employed large analog computing circuits to convert inputs into
LUT addresses, which presents challenges to area efficiency and computational
accuracy. This paper proposes a novel MADDNESS-based all-digital accelerator
featuring a self-synchronous pipeline accumulator, resulting in a compact,
energy-efficient, and PVT-invariant computation. Post-layout simulation using a
commercial 22nm process showed that 2.5 times higher energy efficiency (174
TOPS/W) and 5 times higher area efficiency (2.01 TOPS/mm2) can be achieved
compared to the conventional accelerator.

</details>


### [92] [RCNet: $ΔΣ$ IADCs as Recurrent AutoEncoders](https://arxiv.org/abs/2506.16903)
*Arnaud Verdant,William Guicquero,Jérôme Chossat*

Main category: cs.AR

TL;DR: 论文提出了RCNet深度学习模型用于ΔΣ ADC，结合RNN和硬件约束优化，展示了在特定硬件复杂度下SNR与面积的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用深度学习模型优化ΔΣ ADC的设计，特别是如何在硬件约束下提升性能。

Method: 采用RNN描述调制器和滤波器，结合高端优化器和自定义损失函数，考虑量化权重、信号饱和等硬件限制。

Result: 在DC转换中，RCNet成功在SNR（>13bit）和面积约束（<14pF）之间找到平衡，且优化架构不一定依赖高阶调制器。

Conclusion: RCNet为ADC设计提供了新的自由度，证明了深度学习在硬件优化中的潜力。

Abstract: This paper proposes a deep learning model (RCNet) for Delta-Sigma
($\Delta\Sigma$) ADCs. Recurrent Neural Networks (RNNs) allow to describe both
modulators and filters. This analogy is applied to Incremental ADCs (IADC).
High-end optimizers combined with full-custom losses are used to define
additional hardware design constraints: quantized weights, signal saturation,
temporal noise injection, devices area. Focusing on DC conversion, our early
results demonstrate that $SNR$ defined as an Effective Number Of Bits (ENOB)
can be optimized under a certain hardware mapping complexity. The proposed
RCNet succeeded to provide design tradeoffs in terms of $SNR$ ($>$13bit) versus
area constraints ($<$14pF total capacitor) at a given $OSR$ (80 samples).
Interestingly, it appears that the best RCNet architectures do not necessarily
rely on high-order modulators, leveraging additional topology exploration
degrees of freedom.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [93] [Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration](https://arxiv.org/abs/2506.15759)
*Siyi Xie,Hanxin Zhu,Tianyu He,Xin Li,Zhibo Chen*

Main category: cs.SD

TL;DR: Sonic4D是一个新颖的框架，用于生成与4D场景对齐的空间音频，填补了现有4D生成技术中音频生成的空白，显著提升了沉浸式体验。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成技术虽然能合成逼真的动态3D场景，但忽视了与场景对齐的空间音频生成，限制了沉浸式体验的真实性。

Method: Sonic4D分为三个阶段：1）从单目视频生成4D场景和单声道音频；2）通过视觉定位策略估计声源的3D坐标；3）基于物理模拟合成与视角和时间变化的空间音频。

Result: 实验表明，Sonic4D无需训练即可生成与4D场景一致的真实空间音频，显著提升了用户的沉浸感。

Conclusion: Sonic4D成功解决了4D场景中空间音频生成的问题，为沉浸式体验提供了重要补充。

Abstract: Recent advancements in 4D generation have demonstrated its remarkable
capability in synthesizing photorealistic renderings of dynamic 3D scenes.
However, despite achieving impressive visual performance, almost all existing
methods overlook the generation of spatial audio aligned with the corresponding
4D scenes, posing a significant limitation to truly immersive audiovisual
experiences. To mitigate this issue, we propose Sonic4D, a novel framework that
enables spatial audio generation for immersive exploration of 4D scenes.
Specifically, our method is composed of three stages: 1) To capture both the
dynamic visual content and raw auditory information from a monocular video, we
first employ pre-trained expert models to generate the 4D scene and its
corresponding monaural audio. 2) Subsequently, to transform the monaural audio
into spatial audio, we localize and track the sound sources within the 4D
scene, where their 3D spatial coordinates at different timestamps are estimated
via a pixel-level visual grounding strategy. 3) Based on the estimated sound
source locations, we further synthesize plausible spatial audio that varies
across different viewpoints and timestamps using physics-based simulation.
Extensive experiments have demonstrated that our proposed method generates
realistic spatial audio consistent with the synthesized 4D scene in a
training-free manner, significantly enhancing the immersive experience for
users. Generated audio and video examples are available at
https://x-drunker.github.io/Sonic4D-project-page.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: 本文介绍了BEWA（贝叶斯认识论加权权威）架构，旨在通过概率一致性和动态更新的科学主张处理科学文献爆炸式增长带来的认知挑战。


<details>
  <summary>Details</summary>
Motivation: 科学文献的快速增长超出了人类和人工智能系统的认知处理能力，需要一种新的方法来结构化和评估科学主张。

Method: BEWA通过复制分数、引用加权和时间衰减对科学主张进行动态评估，使用贝叶斯推理、矛盾处理和认知衰减机制更新信念。

Result: BEWA提供了一个可计算验证的认识论网络，支持科学推理的机器化，促进了真理效用、理性信念收敛和审计韧性。

Conclusion: BEWA为处理动态科学领域中的复杂问题提供了一种新型的计算基础，增强了机器系统的推理能力和科学完整性。

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [95] [Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines](https://arxiv.org/abs/2506.16924)
*Tomoya Kashimata,Yohei Hamakawa,Masaya Yamasaki,Kosuke Tatsumura*

Main category: cs.AI

TL;DR: 本文提出了一种启发式多臂赌博机方法，用于动态离散环境中的优化问题，通过扩展黑盒优化方法，利用Ising机器高效探索动作并适应环境变化。


<details>
  <summary>Details</summary>
Motivation: 实时系统需要优化离散变量，但在动态环境中，传统的多臂赌博机算法由于组合离散优化的复杂性，难以有效优化。

Method: 扩展黑盒优化方法，利用Ising机器探索动作并考虑变量间的交互及动态环境的变化。

Result: 在移动用户的无线通信系统中验证了方法的动态适应性。

Conclusion: 该方法成功解决了动态离散环境中的优化挑战。

Abstract: Many real-time systems require the optimization of discrete variables.
Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms
perform optimization by repeatedly taking actions and observing the
corresponding instant rewards without any prior knowledge. Recently, a BBO
method using an Ising machine has been proposed to find the best action that is
represented by a combination of discrete values and maximizes the instant
reward in static environments. In contrast, dynamic environments, where
real-time systems operate, necessitate MAB algorithms that maximize the average
reward over multiple trials. However, due to the enormous number of actions
resulting from the combinatorial nature of discrete optimization, conventional
MAB algorithms cannot effectively optimize dynamic, discrete environments.
Here, we show a heuristic MAB method for dynamic, discrete environments by
extending the BBO method, in which an Ising machine effectively explores the
actions while considering interactions between variables and changes in dynamic
environments. We demonstrate the dynamic adaptability of the proposed method in
a wireless communication system with moving users.

</details>


### [96] [Approximation Fixpoint Theory with Refined Approximation Spaces](https://arxiv.org/abs/2506.16294)
*Linde Vanbesien,Bart Bogaerts,Marc Denecker*

Main category: cs.AI

TL;DR: 本文扩展了近似不动点理论（AFT），通过引入更精细的近似空间来克服原有局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然AFT在非单调推理形式中广泛应用，但在某些简单例子中存在局限，本文旨在解决这一问题。

Method: 引入更一般的近似空间概念，展示其表达能力，并研究不同近似空间之间的关系。

Result: 新方法提升了AFT的表达能力，扩展了其适用性。

Conclusion: 扩展后的AFT能够处理更复杂的近似问题，为未来研究提供新方向。

Abstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various
semantics of non-monotonic reasoning formalisms in knowledge representation
such as Logic Programming and Answer Set Programming. Many semantics of such
non-monotonic formalisms can be characterized as suitable fixpoints of a
non-monotonic operator on a suitable lattice. Instead of working on the
original lattice, AFT operates on intervals in such lattice to approximate or
construct the fixpoints of interest. While AFT has been applied successfully
across a broad range of non-monotonic reasoning formalisms, it is confronted by
its limitations in other, relatively simple, examples. In this paper, we
overcome those limitations by extending consistent AFT to deal with
approximations that are more refined than intervals. Therefore, we introduce a
more general notion of approximation spaces, showcase the improved
expressiveness and investigate relations between different approximation
spaces.

</details>


### [97] [Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving](https://arxiv.org/abs/2506.17104)
*Chuxue Cao,Mengze Li,Juntao Dai,Jinluan Yang,Zijian Zhao,Shengyu Zhang,Weijie Shi,Chengzhong Liu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 论文提出了DREAM方法，通过增强大语言模型（LLM）在FOL推理中的多样性和合理性，改进了LLM在复杂数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂数学推理任务中表现不佳，尤其是多步FOL推理，亟需一种方法提升其多样性和合理性。

Method: 提出了DREAM方法，包括Axiom-Driven Strategy Diversification和Sub-Proposition Error Feedback机制。

Result: DREAM显著提升了LLM在多步FOL推理任务中的表现，改进幅度为0.6%至6.4%。

Conclusion: DREAM通过多样化策略和错误反馈机制，为LLM在多步FOL推理任务中的性能提供了有效解决方案。

Abstract: Large language models (LLMs) have shown promising first-order logic (FOL)
reasoning capabilities with applications in various areas. However, their
effectiveness in complex mathematical reasoning involving multi-step FOL
deductions is still under-researched. While LLMs perform competitively on
established mathematical reasoning benchmarks, they struggle with multi-step
FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on
our proposed theorem proving dataset. This issue arises from the limited
exploration of diverse proof strategies and the potential for early reasoning
mistakes to undermine entire proofs. To address these issues, we propose DREAM,
a self-adaptive solution that enhances the Diversity and REAsonability of LLMs'
generation strategies. DREAM incorporates an Axiom-Driven Strategy
Diversification mechanism to promote varied strategic outcomes and a
Sub-Proposition Error Feedback to help LLMs reflect on and correct their
proofs. Our contributions include pioneering advancements in LLMs' mathematical
reasoning through FOL theorem proving, introducing a novel inference stage
solution that improves performance by 0.6% to 6.4%, and providing a curated
dataset of 447 mathematical theorems in Lean 4 format for evaluation.

</details>


### [98] [Incentivizing High-quality Participation From Federated Learning Agents](https://arxiv.org/abs/2506.16731)
*Jinlong Pang,Jiaheng Wei,Yifan Hua,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: 联邦学习中的激励机制框架，考虑数据异质性以加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究假设代理自愿无私参与，但实际中代理可能因自私而退出或提供低质量贡献，同时现有方法忽视数据异质性导致的努力差异。

Method: 引入Wasserstein距离量化数据异质性，利用同伴预测机制设计评分函数，提出两阶段Stackelberg博弈模型。

Result: 在真实数据集上验证了所提机制的有效性。

Conclusion: 通过激励感知框架解决了代理参与和数据异质性问题，显著提升了联邦学习的收敛速度。

Abstract: Federated learning (FL) provides a promising paradigm for facilitating
collaboration between multiple clients that jointly learn a global model
without directly sharing their local data. However, existing research suffers
from two caveats: 1) From the perspective of agents, voluntary and unselfish
participation is often assumed. But self-interested agents may opt out of the
system or provide low-quality contributions without proper incentives; 2) From
the mechanism designer's perspective, the aggregated models can be
unsatisfactory as the existing game-theoretical federated learning approach for
data collection ignores the potential heterogeneous effort caused by
contributed data. To alleviate above challenges, we propose an incentive-aware
framework for agent participation that considers data heterogeneity to
accelerate the convergence process. Specifically, we first introduce the notion
of Wasserstein distance to explicitly illustrate the heterogeneous effort and
reformulate the existing upper bound of convergence. To induce truthful
reporting from agents, we analyze and measure the generalization error gap of
any two agents by leveraging the peer prediction mechanism to develop score
functions. We further present a two-stage Stackelberg game model that
formalizes the process and examines the existence of equilibrium. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed mechanism.

</details>


### [99] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Main category: cs.AI

TL;DR: 本文提出了一种用于关键任务谈判场景中智能代理AI系统的评估框架，通过实验研究了人格特质和AI代理特性对谈判结果的影响。


<details>
  <summary>Details</summary>
Motivation: 旨在开发能够适应多样化人类操作者和利益相关者的AI代理，满足跨团队协调和军民互动等高价值应用的需求。

Method: 利用Sotopia模拟测试平台，通过两个实验系统评估人格特质和AI代理特性对谈判结果的影响。实验1采用因果发现方法分析人格特质对价格谈判的影响；实验2评估人类与AI在职位谈判中的互动。

Result: 发现'宜人性'和'外向性'显著影响谈判可信度、目标达成和知识获取结果；AI代理的透明度、能力和适应性对其可信度有重要影响。

Conclusion: 本文为评估AI代理在多样化人格和团队动态中的可靠性提供了可重复的方法，支持复杂任务中AI系统的社会动态评估。

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [100] [The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring](https://arxiv.org/abs/2506.16617)
*Soobin Chae,Suhwan Lee,Hanna Hauptmann,Hajo A. Reijers,Xixi Lu*

Main category: cs.AI

TL;DR: 研究探讨了不同解释风格和AI感知准确性对PPM中决策的影响，发现两者对任务表现和决策信心有显著作用。


<details>
  <summary>Details</summary>
Motivation: 当前PPM中的XAI评估主要关注功能指标，忽视了用户中心的决策影响，因此需要研究解释风格和感知准确性对决策的影响。

Method: 通过决策实验，用户在不同解释风格和感知准确性下进行决策，测量任务表现、一致性和决策信心。

Result: 研究发现，感知准确性和解释风格对决策有显著影响。

Conclusion: 研究强调了用户中心评估的重要性，为PPM中解释工具的设计提供了依据。

Abstract: Predictive Process Monitoring (PPM) often uses deep learning models to
predict the future behavior of ongoing processes, such as predicting process
outcomes. While these models achieve high accuracy, their lack of
interpretability undermines user trust and adoption. Explainable AI (XAI) aims
to address this challenge by providing the reasoning behind the predictions.
However, current evaluations of XAI in PPM focus primarily on functional
metrics (such as fidelity), overlooking user-centered aspects such as their
effect on task performance and decision-making. This study investigates the
effects of explanation styles (feature importance, rule-based, and
counterfactual) and perceived AI accuracy (low or high) on decision-making in
PPM. We conducted a decision-making experiment, where users were presented with
the AI predictions, perceived accuracy levels, and explanations of different
styles. Users' decisions were measured both before and after receiving
explanations, allowing the assessment of objective metrics (Task Performance
and Agreement) and subjective metrics (Decision Confidence). Our findings show
that perceived accuracy and explanation style have a significant effect.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [101] [From Generation to Adaptation: Comparing AI-Assisted Strategies in High School Programming Education](https://arxiv.org/abs/2506.15955)
*Tong Hu,Songzan Wang*

Main category: cs.CY

TL;DR: 研究比较两种LCA辅助编程教学法，发现基于MFU的方法更有效。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过LCA辅助编程教育，帮助学生在WeChat小程序竞赛中成功。

Method: 采用两阶段对比实验（From-Scratch与MFU方法），分析学生表现。

Result: MFU方法完成率100%，From-Scratch仅20%，MFU更符合LCA优势。

Conclusion: LCA的有效整合依赖于教学设计，提出双脚手架模型，为编程教育提供实践指导。

Abstract: This exploratory case study investigated two contrasting pedagogical
approaches for LCA-assisted programming with five novice high school students
preparing for a WeChat Mini Program competition. In Phase 1, students used LCAs
to generate code from abstract specifications (From-Scratch approach),
achieving only 20% MVP completion. In Phase 2, students adapted existing
Minimal Functional Units (MFUs), small, functional code examples, using LCAs,
achieving 100% MVP completion. Analysis revealed that the MFU-based approach
succeeded by aligning with LCA strengths in pattern modification rather than de
novo generation, while providing cognitive scaffolds that enabled students to
navigate complex development tasks. The study introduces a dual-scaffolding
model combining technical support (MFUs) with pedagogical guidance (structured
prompting strategies), demonstrating that effective LCA integration depends
less on AI capabilities than on instructional design. These findings offer
practical guidance for educators seeking to transform AI tools from sources of
frustration into productive learning partners in programming education.

</details>


### [102] [Teaching Complex Systems based on Microservices](https://arxiv.org/abs/2506.16492)
*Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman,Eduardo Guerra*

Main category: cs.CY

TL;DR: 该论文分享了在圣保罗大学教授80多名学生微服务开发的经验，展示了如何向高年级本科生教授复杂系统的开发。


<details>
  <summary>Details</summary>
Motivation: 当前开发微服务系统的复杂性是一个挑战，论文旨在通过教学实践解决这一问题。

Method: 采用团队合作和模拟工业环境的方式，教授高年级本科生微服务开发。

Result: 实践证明，高年级本科生能够掌握微服务开发的复杂概念。

Conclusion: 通过合适的教学方法，可以有效地向高年级本科生传授微服务开发的先进知识和技术。

Abstract: Developing complex systems using microservices is a current challenge. In
this paper, we present our experience with teaching this subject to more than
80 students at the University of S\~ao Paulo (USP), fostering team work and
simulating the industry's environment. We show it is possible to teach such
advanced concepts for senior undergraduate students of Computer Science and
related fields.

</details>


### [103] [AI labeling reduces the perceived accuracy of online content but has limited broader effects](https://arxiv.org/abs/2506.16202)
*Chuyao Wang,Patrick Sturgis,Daniel de Kadt*

Main category: cs.CY

TL;DR: 研究表明，显式标注AI生成的内容会降低公众对其准确性的感知，但对政策支持或虚假信息的整体担忧影响有限。


<details>
  <summary>Details</summary>
Motivation: 探讨AI标注对公众评估内容的影响，以促进透明度和公众信心。

Method: 通过全国代表性调查实验（n=3,861）测试标注对政策兴趣、支持和虚假信息担忧的影响。

Result: AI标注降低政策兴趣但对政策支持无影响；提高AI使用的显著性可减轻准确性感知的负面影响。

Conclusion: AI标注引发的算法厌恶效应影响范围有限。

Abstract: Explicit labeling of online content produced by artificial intelligence (AI)
is a widely mooted policy for ensuring transparency and promoting public
confidence. Yet little is known about the scope of AI labeling effects on
public assessments of labeled content. We contribute new evidence on this
question from a survey experiment using a high-quality nationally
representative probability sample (n = 3,861). First, we demonstrate that
explicit AI labeling of a news article about a proposed public policy reduces
its perceived accuracy. Second, we test whether there are spillover effects in
terms of policy interest, policy support, and general concerns about online
misinformation. We find that AI labeling reduces interest in the policy, but
neither influences support for the policy nor triggers general concerns about
online misinformation. We further find that increasing the salience of AI use
reduces the negative impact of AI labeling on perceived accuracy, while
one-sided versus two-sided framing of the policy has no moderating effect.
Overall, our findings suggest that the effects of algorithm aversion induced by
AI labeling of online content are limited in scope.

</details>


### [104] [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 论文讨论了在心理学中广泛应用的大语言模型（LLM）可能因使用人类测量工具而产生虚假结果的问题，提出了整合可靠测量原则和因果推断标准的框架，以建立稳健的AI心理学科学。


<details>
  <summary>Details</summary>
Motivation: 心理学研究中，LLM作为工具或模型的应用可能导致测量假象，因此需要更严谨的验证方法。

Method: 文章提出了一个双效度框架，根据科学野心调整验证标准，区分不同研究目标所需的证据强度。

Result: 当前实践常将统计模式匹配误认为心理现象证据，需开发计算类比和明确的证据标准。

Conclusion: 建立稳健的AI心理学科学需整合测量与因果推断标准，避免滥用人类测量工具。

Abstract: Large language models (LLMs) are rapidly being adopted across psychology,
serving as research tools, experimental subjects, human simulators, and
computational models of cognition. However, the application of human
measurement tools to these systems can produce contradictory results, raising
concerns that many findings are measurement phantoms--statistical artifacts
rather than genuine psychological phenomena. In this Perspective, we argue that
building a robust science of AI psychology requires integrating two of our
field's foundational pillars: the principles of reliable measurement and the
standards for sound causal inference. We present a dual-validity framework to
guide this integration, which clarifies how the evidence needed to support a
claim scales with its scientific ambition. Using an LLM to classify text may
require only basic accuracy checks, whereas claiming it can simulate anxiety
demands a far more rigorous validation process. Current practice systematically
fails to meet these requirements, often treating statistical pattern matching
as evidence of psychological phenomena. The same model output--endorsing "I am
anxious"--requires different validation strategies depending on whether
researchers claim to measure, characterize, simulate, or model psychological
constructs. Moving forward requires developing computational analogues of
psychological constructs and establishing clear, scalable standards of evidence
rather than the uncritical application of human measurement tools.

</details>


### [105] [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 本文提出了一个框架，利用大语言模型（LLMs）作为心理学模拟器，分为角色模拟和认知建模两部分，并讨论了方法验证、伦理问题及透明性需求。


<details>
  <summary>Details</summary>
Motivation: 为心理学和行为学研究提供方法论指导，利用LLMs模拟多样情境和认知过程。

Method: 开发心理学基础角色模拟方法，结合数据验证；探索认知建模中的内部表征分析和因果干预方法。

Result: 整合了LLMs的性能证据，包括系统性偏差和文化局限性，为研究提供实用策略。

Conclusion: 强调透明性和伦理考量，帮助研究者利用LLMs的独特能力应对心理学研究中的挑战。

Abstract: Large language models (LLMs) offer emerging opportunities for psychological
and behavioral research, but methodological guidance is lacking. This article
provides a framework for using LLMs as psychological simulators across two
primary applications: simulating roles and personas to explore diverse
contexts, and serving as computational models to investigate cognitive
processes. For simulation, we present methods for developing psychologically
grounded personas that move beyond demographic categories, with strategies for
validation against human data and use cases ranging from studying inaccessible
populations to prototyping research instruments. For cognitive modeling, we
synthesize emerging approaches for probing internal representations,
methodological advances in causal interventions, and strategies for relating
model behavior to human cognition. We address overarching challenges including
prompt sensitivity, temporal limitations from training data cutoffs, and
ethical considerations that extend beyond traditional human subjects review.
Throughout, we emphasize the need for transparency about model capabilities and
constraints. Together, this framework integrates emerging empirical evidence
about LLM performance--including systematic biases, cultural limitations, and
prompt brittleness--to help researchers wrangle these challenges and leverage
the unique capabilities of LLMs in psychological research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [106] [BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios](https://arxiv.org/abs/2506.16546)
*Liyang Yu,Tianyi Wang,Junfeng Jiao,Fengwu Shan,Hongqing Chu,Bingzhao Gao*

Main category: cs.RO

TL;DR: 本文提出了一种双层交互决策算法（BIDA），结合交互式蒙特卡洛树搜索（MCTS）和深度强化学习（DRL），以提升自动驾驶车辆在动态交通场景中的交互性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在复杂交通环境中需要实时、安全地与人类交通参与者交互，但人类行为的不可预测性带来了挑战。本文旨在解决这一问题。

Method: 采用三种DRL算法构建可靠的价值网络和策略网络，指导交互式MCTS的在线推导过程，并设计了动态轨迹规划器和跟踪控制器。

Result: 实验表明，BIDA不仅提升了交互效率并降低了计算成本，还在安全性和交互合理性上优于其他最新基准方法。

Conclusion: BIDA在动态交通场景中表现出优越的性能，为自动驾驶车辆的交互决策提供了有效解决方案。

Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to
interact with other traffic participants while making real-time and
safety-critical decisions accordingly. The unpredictability of human behaviors
poses significant challenges, particularly in dynamic scenarios, such as
multi-lane highways and unsignalized T-intersections. To address this gap, we
design a bi-level interaction decision-making algorithm (BIDA) that integrates
interactive Monte Carlo tree search (MCTS) with deep reinforcement learning
(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs
in dynamic key traffic scenarios. Specifically, we adopt three types of DRL
algorithms to construct a reliable value network and policy network, which
guide the online deduction process of interactive MCTS by assisting in value
update and node selection. Then, a dynamic trajectory planner and a trajectory
tracking controller are designed and implemented in CARLA to ensure smooth
execution of planned maneuvers. Experimental evaluations demonstrate that our
BIDA not only enhances interactive deduction and reduces computational costs,
but also outperforms other latest benchmarks, which exhibits superior safety,
efficiency and interaction rationality under varying traffic conditions.

</details>


### [107] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Main category: cs.RO

TL;DR: 论文提出了一种基于视觉语言模型（VLM）的新型机器人操作框架，用于处理自然语言指令中的模糊性问题，并通过生成可解释和可执行的中间代码来解决指令歧义。


<details>
  <summary>Details</summary>
Motivation: 现有语言条件策略通常依赖端到端模型，缺乏模块化和可解释性，导致性能不佳。本文旨在解决自然语言指令中模糊性和歧义性问题。

Method: 使用视觉语言模型（VLM）解释自然语言指令中的抽象概念，生成任务特定代码，并与感知模块结合生成3D注意力图，解决指令歧义。

Result: 实验表明，该方法在语言模糊性、接触丰富的操作和多对象交互等挑战性任务中表现优异。

Conclusion: 提出的框架通过模块化和可解释的中间表示，显著提高了机器人操作任务的性能，解决了现有方法的局限性。

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [108] [Bias Variation Compensation in Perimeter-Gated SPAD TRNGs](https://arxiv.org/abs/2506.15888)
*Md Sakibur Sajal,Hunter Guthrie,Marc Dandin*

Main category: physics.ins-det

TL;DR: 该研究提出了一种利用64x64阵列的pgSPADs作为熵源的方法，通过BV补偿技术生成随机二进制字符串，成功将偏差降至1%以下，并通过了NIST的统计测试。


<details>
  <summary>Details</summary>
Motivation: 为了解决随机数生成器中熵源阵列的偏差变化问题，研究探索了一种能够在宽范围BV下有效工作的硬件友好方案。

Method: 采用0.35微米标准CMOS技术制造64x64阵列的pgSPADs作为熵源，通过调节栅极电压补偿BV，并应用Von Neumann算法去除偏差。

Result: 在室温下，每像素2kHz的原始比特生成率下，BV低于1%，且去偏后的比特通过了NIST的所有16项统计测试。

Conclusion: 该研究成功实现了低偏差的高效随机数生成，为硬件熵源提供了可行的解决方案。

Abstract: Random number generators that utilize arrays of entropy source elements
suffer from bias variation (BV). Despite the availability of efficient
debiasing algorithms, optimized implementations of hardware friendly options
depend on the bit bias in the raw bit streams and cannot accommodate a wide BV.
In this work, we present a 64 x 64 array of perimeter gated single photon
avalanche diodes (pgSPADs), fabricated in a 0.35 {\mu}m standard CMOS
technology, as a source of entropy to generate random binary strings with a BV
compensation technique. By applying proper gate voltages based on the devices'
native dark count rates, we demonstrate less than 1% BV for a raw-bit
generation rate of 2 kHz/pixel at room temperature. The raw bits were debiased
using the classical iterative Von Neumann's algorithm and the debiased bits
were found to pass all of the 16 tests from NIST's Statistical Test Suite.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [109] [Comparison of substructured non-overlapping domain decomposition and overlapping additive Schwarz methods for large-scale Helmholtz problems with multiple sources](https://arxiv.org/abs/2506.16875)
*Boris Martin,Pierre Jolivet,Christophe Geuzaine*

Main category: math.NA

TL;DR: 比较非重叠子结构域分解方法和优化的限制加性Schwarz预条件器在解决大规模Helmholtz问题中的性能，显示适当调整后非重叠方法性能优于重叠方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模Helmholtz问题在高阶有限元离散化下的计算困难，尤其是在3D中直接分解系统矩阵成本高且内存需求大，迭代方法难以稳健收敛。

Method: 比较非重叠子结构域分解方法（DDM）和优化的限制加性Schwarz预条件器（ORAS）在解决多源Helmholtz问题中的性能。

Result: 在现实地球物理测试案例中，适当调整后，非重叠方法能够显著减少收敛差距，性能优于重叠方法。

Conclusion: 非重叠子结构域分解方法在适当调整后，能够成为解决大规模Helmholtz问题的有效策略。

Abstract: Solving large-scale Helmholtz problems discretized with high-order finite
elements is notoriously difficult, especially in 3D where direct factorization
of the system matrix is very expensive and memory demanding, and robust
convergence of iterative methods is difficult to obtain. Domain decomposition
methods (DDM) constitute one of the most promising strategy so far, by
combining direct and iterative approaches: using direct solvers on overlapping
or non-overlapping subdomains, as a preconditioner for a Krylov subspace method
on the original Helmholtz system or as an iterative solver on a substructured
problem involving field values or Lagrange multipliers on the interfaces
between the subdomains. In this work we compare the computational performance
of non-overlapping substructured DDM and Optimized Restricted Additive Schwarz
(ORAS) preconditioners for solving large-scale Helmholtz problems with multiple
sources, as is encountered, e.g., in frequency-domain Full Waveform Inversion.
We show on a realistic geophysical test-case that, when appropriately tuned,
the non-overlapping methods can reduce the convergence gap sufficiently to
significantly outperform the overlapping methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [110] [Floating-Point Neural Networks Are Provably Robust Universal Approximators](https://arxiv.org/abs/2506.16065)
*Geonho Hwang,Wonyeol Lee,Yeachan Park,Sejun Park,Feras Saad*

Main category: cs.LG

TL;DR: 本文提出了首个适用于浮点数神经网络（Floating-Point Neural Networks）的区间通用近似定理（IUA），证明其在浮点数环境下仍能完美逼近目标函数的直接图像映射。


<details>
  <summary>Details</summary>
Motivation: 以往关于神经网络的通用近似定理（UA）和区间通用近似定理（IUA）基于无限精度实数的假设，而实际实现中神经网络使用有限精度的浮点数计算。本文旨在探讨浮点数环境下IUA定理是否依然成立。

Method: 通过理论分析，研究浮点数神经网络的表达能力，提出首个浮点数环境下的IUA定理，并探讨其与实数环境下的差异。

Result: 证明了浮点数神经网络在有限精度下仍能完美逼近目标函数的直接图像映射，且无表达能力的限制。此外，还得出两个重要推论：存在可证明鲁棒的浮点数神经网络；以及仅使用浮点数加法和乘法的直线程序对于所有终止的浮点数程序具有计算完备性。

Conclusion: 本文填补了浮点数环境下IUA定理的理论空白，揭示了浮点数神经网络在表达能力上的潜在优势，并提供了理论支持其在实际应用中的鲁棒性和计算能力。

Abstract: The classical universal approximation (UA) theorem for neural networks
establishes mild conditions under which a feedforward neural network can
approximate a continuous function $f$ with arbitrary accuracy. A recent result
shows that neural networks also enjoy a more general interval universal
approximation (IUA) theorem, in the sense that the abstract interpretation
semantics of the network using the interval domain can approximate the direct
image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with
arbitrary accuracy. These theorems, however, rest on the unrealistic assumption
that the neural network computes over infinitely precise real numbers, whereas
their software implementations in practice compute over finite-precision
floating-point numbers. An open question is whether the IUA theorem still holds
in the floating-point setting.
  This paper introduces the first IUA theorem for floating-point neural
networks that proves their remarkable ability to perfectly capture the direct
image map of any rounded target function $f$, showing no limits exist on their
expressiveness. Our IUA theorem in the floating-point setting exhibits material
differences from the real-valued setting, which reflects the fundamental
distinctions between these two computational models. This theorem also implies
surprising corollaries, which include (i) the existence of provably robust
floating-point neural networks; and (ii) the computational completeness of the
class of straight-line programs that use only floating-point additions and
multiplications for the class of all floating-point programs that halt.

</details>


### [111] [The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation](https://arxiv.org/abs/2506.17016)
*Giulia Bertazzini,Chiara Albisani,Daniele Baracchi,Dasara Shullani,Roberto Verdecchia*

Main category: cs.LG

TL;DR: 该研究通过实证实验比较了17种AI图像生成模型的能源消耗，发现模型间能源差异高达46倍，分辨率影响不一，U-Net模型更节能，量化反而降低效率，图像质量提升未必增加能耗。


<details>
  <summary>Details</summary>
Motivation: 随着AI图像生成的普及及其对资源的需求增长，研究旨在揭示生成每张图像背后的环境影响。

Method: 实验比较了17种先进模型，量化了模型、分辨率、提示长度等因素对能耗的影响，并结合图像质量指标分析。

Result: 模型能耗差异显著，U-Net比Transformer节能，量化降低效率，分辨率影响不一致，图像质量与能耗无直接关联。

Conclusion: 提升图像质量未必增加能耗，部分高效模型同时具有高图像质量。

Abstract: With the growing adoption of AI image generation, in conjunction with the
ever-increasing environmental resources demanded by AI, we are urged to answer
a fundamental question: What is the environmental impact hidden behind each
image we generate? In this research, we present a comprehensive empirical
experiment designed to assess the energy consumption of AI image generation.
Our experiment compares 17 state-of-the-art image generation models by
considering multiple factors that could affect their energy consumption, such
as model quantization, image resolution, and prompt length. Additionally, we
consider established image quality metrics to study potential trade-offs
between energy consumption and generated image quality. Results show that image
generation models vary drastically in terms of the energy they consume, with up
to a 46x difference. Image resolution affects energy consumption
inconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.
U-Net-based models tend to consume less than Transformer-based one. Model
quantization instead results to deteriorate the energy efficiency of most
models, while prompt length and content have no statistically significant
impact. Improving image quality does not always come at the cost of a higher
energy consumption, with some of the models producing the highest quality
images also being among the most energy efficient ones.

</details>


### [112] [PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning](https://arxiv.org/abs/2506.15923)
*Liangyan Li,Yangyi Liu,Yimo Ning,Stefano Rini,Jun Chen*

Main category: cs.LG

TL;DR: 提出了一个利用PNCS改进联邦学习中客户端选择的新框架，通过捕捉高阶梯度矩解决非独立同分布数据问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法忽视了客户端间的梯度相关性，尤其在数据异质性场景下表现不佳。

Method: 提出PNCS方法用于客户端选择，并设计算法通过历史队列保证多样性。

Result: 在不同数据划分下，VGG16模型的实验显示性能优于现有方法。

Conclusion: PNCS框架显著提升了联邦学习的收敛速度和准确性。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging
diverse datasets from multiple sources while preserving data privacy by
avoiding centralized storage. However, many existing approaches fail to account
for the intricate gradient correlations between remote clients, a limitation
that becomes especially problematic in data heterogeneity scenarios. In this
work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity
(PNCS) to improve client selection for model aggregation. By capturing
higher-order gradient moments, PNCS addresses non-IID data challenges,
enhancing convergence speed and accuracy. Additionally, we introduce a simple
algorithm ensuring diverse client selection through a selection history queue.
Experiments with a VGG16 model across varied data partitions demonstrate
consistent improvements over state-of-the-art methods.

</details>


### [113] [From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience](https://arxiv.org/abs/2506.16051)
*Zhiwei Li,Carl Kesselman,Tran Huy Nguyen,Benjamin Yixing Xu,Kyle Bolo,Kimberley Yu*

Main category: cs.LG

TL;DR: 为解决机器学习中的可重现性问题，本文提出了一种数据为中心的框架，通过六种结构化工件支持实验的版本控制和溯源。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习工作流程缺乏透明度且碎片化，导致可重现性和协作决策的适应性受限。

Method: 提出了以六种结构化工件（数据集、特征、工作流、执行、资产和受控词汇）为核心的数据中心框架。

Result: 框架在青光眼检测的临床案例中验证了其支持迭代探索、提升可重现性和保留溯源的能力。

Conclusion: 该框架为机器学习的生命周期管理提供了系统化解决方案，增强了协作项目的透明度和可重现性。

Abstract: Reproducibility remains a central challenge in machine learning (ML),
especially in collaborative eScience projects where teams iterate over data,
features, and models. Current ML workflows are often dynamic yet fragmented,
relying on informal data sharing, ad hoc scripts, and loosely connected tools.
This fragmentation impedes transparency, reproducibility, and the adaptability
of experiments over time. This paper introduces a data-centric framework for
lifecycle-aware reproducibility, centered around six structured artifacts:
Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These
artifacts formalize the relationships between data, code, and decisions,
enabling ML experiments to be versioned, interpretable, and traceable over
time. The approach is demonstrated through a clinical ML use case of glaucoma
detection, illustrating how the system supports iterative exploration, improves
reproducibility, and preserves the provenance of collaborative decisions across
the ML lifecycle.

</details>


### [114] [Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures](https://arxiv.org/abs/2506.16654)
*Vijay Prakash Dwivedi,Charilaos Kanatsoulis,Shenyang Huang,Jure Leskovec*

Main category: cs.LG

TL;DR: 论文综述了关系深度学习（RDL），重点探讨了如何将多表关系数据库表示为关系实体图，并总结了GNN模型在RDL中的应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 多表关系数据库中的数据可以通过关系实体图进行表示，从而支持端到端的表示学习，无需传统特征工程。

Method: 通过将关系数据库表示为关系实体图，利用GNN等图机器学习方法进行建模，并针对大规模多表集成、时间动态性和异构数据等挑战提出解决方案。

Result: 总结了RDL领域的公共基准数据集、关键挑战及最新架构进展，为关系数据的深度学习建模提供了基础。

Conclusion: RDL将图机器学习的多个子领域统一起来，为设计能够处理关系数据的基础模型提供了方向。

Abstract: Graph machine learning has led to a significant increase in the capabilities
of models that learn on arbitrary graph-structured data and has been applied to
molecules, social networks, recommendation systems, and transportation, among
other domains. Data in multi-tabular relational databases can also be
constructed as 'relational entity graphs' for Relational Deep Learning (RDL) -
a new blueprint that enables end-to-end representation learning without
traditional feature engineering. Compared to arbitrary graph-structured data,
relational entity graphs have key properties: (i) their structure is defined by
primary-foreign key relationships between entities in different tables, (ii)
the structural connectivity is a function of the relational schema defining a
database, and (iii) the graph connectivity is temporal and heterogeneous in
nature. In this paper, we provide a comprehensive review of RDL by first
introducing the representation of relational databases as relational entity
graphs, and then reviewing public benchmark datasets that have been used to
develop and evaluate recent GNN-based RDL models. We discuss key challenges
including large-scale multi-table integration and the complexities of modeling
temporal dynamics and heterogeneous data, while also surveying foundational
neural network methods and recent architectural advances specialized for
relational entity graphs. Finally, we explore opportunities to unify these
distinct modeling challenges, highlighting how RDL converges multiple
sub-fields in graph machine learning towards the design of foundation models
that can transform the processing of relational data.

</details>


### [115] [Optimizing Multilingual Text-To-Speech with Accents & Emotions](https://arxiv.org/abs/2506.16310)
*Pranav Pawar,Akshansh Dwivedi,Jenish Boricha,Himanshu Gohil,Aditya Dubey*

Main category: cs.LG

TL;DR: 该论文提出了一种新的TTS架构，结合口音保持和多尺度情感建模，特别针对印地语和印度英语口音优化，显著提升了口音准确性和情感识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前TTS系统在多语言环境（尤其是印度语言）中难以正确合成带有多语言口音和上下文相关情感的语音，这是由于文化差异导致的。

Method: 扩展Parler-TTS模型，引入语言特定音素对齐混合编码器-解码器架构、文化敏感的情感嵌入层，以及动态口音代码切换与残差向量量化。

Result: 口音准确性提升23.7%（WER从15.4%降至11.8%），情感识别准确率达85.3%，用户评价（MOS）为4.2/5。

Conclusion: 该系统展示了可扩展的口音-情感解耦技术，为跨语言合成提供了可行性，特别适用于南亚教育科技和辅助软件。

Abstract: State-of-the-art text-to-speech (TTS) systems realize high naturalness in
monolingual environments, synthesizing speech with correct multilingual accents
(especially for Indic languages) and context-relevant emotions still poses
difficulty owing to cultural nuance discrepancies in current frameworks. This
paper introduces a new TTS architecture integrating accent along with
preserving transliteration with multi-scale emotion modelling, in particularly
tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS
model by integrating A language-specific phoneme alignment hybrid
encoder-decoder architecture, and culture-sensitive emotion embedding layers
trained on native speaker corpora, as well as incorporating a dynamic accent
code switching with residual vector quantization. Quantitative tests
demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction
from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native
listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system
is that it can mix code in real time - generating statements such as "Namaste,
let's talk about <Hindi phrase>" with uninterrupted accent shifts while
preserving emotional consistency. Subjective evaluation with 200 users reported
a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than
existing multilingual systems (p<0.01). This research makes cross-lingual
synthesis more feasible by showcasing scalable accent-emotion disentanglement,
with direct application in South Asian EdTech and accessibility software.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [116] [Cross-Modal Epileptic Signal Harmonization: Frequency Domain Mapping Quantization for Pre-training a Unified Neurophysiological Transformer](https://arxiv.org/abs/2506.17068)
*Runkai Zhang,Hua Yu,John Q. Gan,Haixian Wang*

Main category: q-bio.NC

TL;DR: 本文提出了一种名为EpiNT的Transformer预训练模型，用于统一分析头皮脑电图（EEG）和颅内脑电图（iEEG），以解决两种模态在记录方式、信号质量等方面的差异问题。


<details>
  <summary>Details</summary>
Motivation: EEG和iEEG在癫痫诊断和治疗中具有重要作用，但统一分析这两种模态存在挑战，如记录方式、信号幅值和信噪比的差异。

Method: EpiNT采用基于掩码自编码器（MAE）和向量量化（VQ）的通道独立建模方法，并引入频域映射量化器以捕捉关键频率特征。

Result: 在1,199名患者的2,700多小时多模态临床神经生理数据上预训练后，EpiNT在六项下游分类任务中表现优于随机初始化模型和其他预训练方法。

Conclusion: EpiNT为癫痫神经生理学的统一分析提供了一种有前景的方法。

Abstract: Scalp electroencephalography (EEG) and intracranial EEG (iEEG) are vital for
epilepsy diagnosis and treatment. Their unified analysis offers the potential
to harness the complementary strengths of each modality but is challenging due
to variations in recording montages, amplitude and signal-to-noise ratio (SNR),
and frequency components. To address the aforementioned challenges, this paper
introduces EpiNT, a novel Transformer-based pre-trained model for unified EEG
and iEEG analysis. EpiNT employs channel-independent modeling with masked
autoencoders (MAE) and vector quantization (VQ), along with a frequency domain
mapping quantizer to capture crucial frequency features. Pre-trained on over
2,700 hours of multi-modal clinical neurophysiological data from 1,199
patients, EpiNT outperformed both randomly initialized models and other
pre-trained methods on six downstream classification tasks, demonstrating
robust representation learning capabilities. This work presents a promising
approach for unified epilepsy neurophysiology analysis.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [117] [Mixed-Signal Quantum Circuit Design for Option Pricing Using Design Compiler](https://arxiv.org/abs/2506.15936)
*Yu-Ting Kao,Yeong-Jar Chang,Ying-Wei Tseng*

Main category: quant-ph

TL;DR: 提出一种混合信号量子电路框架，通过三种新方法降低电路复杂度并提高噪声容忍度，显著优于传统设计。


<details>
  <summary>Details</summary>
Motivation: 挑战传统观念，证明量子电路可以结合经典VLSI技术，解决当前量子设计的局限性。

Method: 提出混合信号量子电路框架，包含三种新方法：降低电路复杂度、提高噪声容忍度，并结合模拟与数字优势。

Result: 在12量子比特案例中，门数从4095降至392，深度从2048降至6，错误率从25.86%降至1.64%。

Conclusion: 量子电路可以有效利用经典VLSI技术，展示出在灵活性和可合成性上的优势。

Abstract: Prior studies have largely focused on quantum algorithms, often reducing
parallel computing designs to abstract models or overly simplified circuits.
This has contributed to the misconception that most applications are feasible
only through VLSI circuits and cannot be implemented using quantum circuits. To
challenge this view, we present a mixed-signal quantum circuit framework
incorporating three novel methods that reduce circuit complexity and improve
noise tolerance. In a 12 qubit case study comparing our design with JP Morgan's
option pricing circuit, we reduced the gate count from 4095 to 392, depth from
2048 to 6, and error rate from 25.86\% to 1.64\%. Our design combines analog
simplicity with digital flexibility and synthesizability, demonstrating that
quantum circuits can effectively leverage classical VLSI techniques, such as
those enabled by Synopsys Design Compiler to address current quantum design
limitations.

</details>


### [118] [Enhancing Expressivity of Quantum Neural Networks Based on the SWAP test](https://arxiv.org/abs/2506.16938)
*Sebastian Nagies,Emiliano Tolotti,Davide Pastorello,Enrico Blanzieri*

Main category: quant-ph

TL;DR: 该论文研究了基于SWAP测试电路的量子神经网络（QNN）在数学上等价于具有二次激活函数的经典两层前馈网络，分析了其局限性并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 探讨量子神经网络与经典模型的联系，以提升其在机器学习和量子计算中的表现。

Method: 研究SWAP测试电路的QNN，并通过引入广义SWAP测试电路改进架构。

Result: 原始架构在某些任务中表现受限，但改进后的架构能成功学习高维奇偶校验函数。

Conclusion: 通过分析经典任务增强QNN表达能力，改进的SWAP测试架构展现出广泛的应用潜力。

Abstract: Parameterized quantum circuits represent promising architectures for machine
learning applications, yet many lack clear connections to classical models,
potentially limiting their ability to translate the wide success of classical
neural networks to the quantum realm. We examine a specific type of quantum
neural network (QNN) built exclusively from SWAP test circuits, and discuss its
mathematical equivalence to a classical two-layer feedforward network with
quadratic activation functions under amplitude encoding. Our analysis across
classical real-world and synthetic datasets reveals that while this
architecture can successfully learn many practical tasks, it exhibits
fundamental expressivity limitations due to violating the universal
approximation theorem, particularly failing on harder problems like the parity
check function. To address this limitation, we introduce a circuit modification
using generalized SWAP test circuits that effectively implements classical
neural networks with product layers. This enhancement enables successful
learning of parity check functions in arbitrary dimensions which we
analytically argue to be impossible for the original architecture beyond two
dimensions regardless of network size. Our results establish a framework for
enhancing QNN expressivity through classical task analysis and demonstrate that
our SWAP test-based architecture offers broad representational capacity,
suggesting potential promise also for quantum learning tasks.

</details>


### [119] [No Scratch Quantum Computing by Reducing Qubit Overhead for Efficient Arithmetics](https://arxiv.org/abs/2506.17135)
*Omid Faizy,Norbert Wehn,Paul Lukowicz,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: 量子哈密顿计算（QHC）通过单旋转量子门编码输入，减少量子算逻辑运算所需的量子比特资源，提出优化的半加器和全加器电路。


<details>
  <summary>Details</summary>
Motivation: 传统量子算术运算需要大量辅助量子比特以保持可逆性，而QHC旨在减少资源和提高效率，突破经典CMOS能量限制。

Method: 利用QHC原理设计半加器和全加器电路，将传统多量子比特结构压缩至两量子比特的4×4希尔伯特空间。

Result: QHC显著减少所需量子比特数和门资源，适用于经典逻辑在量子硬件上的优化实现。

Conclusion: QHC为量子电路和光子学提供了资源优化的新途径，尤其适合FPGA应用。

Abstract: Quantum arithmetic computation requires a substantial number of scratch
qubits to stay reversible. These operations necessitate qubit and gate
resources equivalent to those needed for the larger of the input or output
registers due to state encoding. Quantum Hamiltonian Computing (QHC) introduces
a novel approach by encoding input for logic operations within a single
rotating quantum gate. This innovation reduces the required qubit register $ N
$ to the size of the output states $ O $, where $ N = \log_2 O $. Leveraging
QHC principles, we present reversible half-adder and full-adder circuits that
compress the standard Toffoli + CNOT layout [Vedral et al., PRA, 54, 11,
(1996)] from three-qubit and four-qubit formats for the Quantum half-adder
circuit and five sequential Fredkin gates using five qubits [Moutinho et al.,
PRX Energy 2, 033002 (2023)] for full-adder circuit; into a two-qubit, 4$\times
$4 Hilbert space. This scheme, presented here, is optimized for classical logic
evaluated on quantum hardware, which due to unitary evolution can bypass
classical CMOS energy limitations to certain degree. Although we avoid
superposition of input and output states in this manuscript, this remains
feasible in principle. We see the best application for QHC in finding the
minimal qubit and gate resources needed to evaluate any truth table, advancing
FPGA capabilities using integrated quantum circuits or photonics.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [120] [Microcanonical simulated annealing: Massively parallel Monte Carlo simulations with sporadic random-number generation](https://arxiv.org/abs/2506.16240)
*M. Bernaschi,L. A. Fernandez,I. González-Adalid Pemartín,E. Marinari,V. Martin-Mayor,G. Parisi,F. Ricci-Tersenghi,J. J. Ruiz-Lorenzo,D. Yllanes*

Main category: cond-mat.stat-mech

TL;DR: 提出了名为mic.SA的微规范模拟退火方法，显著减少了蒙特卡洛模拟中随机数生成的负担，适用于大规模并行计算。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛模拟在复杂系统数值模拟中耗费大量计算资源于随机数生成，亟需高效解决方案。

Method: 开发了mic.SA方法，通过微规范模拟退火减少随机数需求，适用于并行计算。

Result: 在三维伊辛自旋玻璃模型中验证，mic.SA在热平衡区域与标准模拟结果一致，且动力学行为可通过简单时间缩放匹配。

Conclusion: mic.SA为高效、可扩展的蒙特卡洛模拟提供了一种有前景的替代方案。

Abstract: Numerical simulations of models and theories that describe complex
experimental systems $\unicode{x2014}$in fields like high-energy and
condensed-matter physics$\unicode{x2014}$ are becoming increasingly important.
Examples include lattice gauge theories, which can describe, among others,
quantum chromodynamics (the Standard Model description of strong interactions
between elementary particles), and spin-glass systems. Beyond fundamental
research, these computational methods also find practical applications, among
many others, in optimization, finance, and complex biological problems.
However, Monte Carlo simulations, an important subcategory of these methods,
are plagued by a major drawback: they are extremely greedy for (pseudo) random
numbers. The total fraction of computer time dedicated to random-number
generation increases as the hardware grows more sophisticated, and can get
prohibitive for special-purpose computing platforms. We propose here a
general-purpose microcanonical simulated annealing (mic.SA) formalism that
dramatically reduces such a burden. The algorithm is fully adapted to a
massively parallel computation, as we show in the particularly demanding
benchmark of the three-dimensional Ising spin glass. We carry out very
stringent numerical tests of the new algorithm by comparing our results,
obtained on GPUs, with high-precision standard (i.e., random-number-greedy)
simulations performed on the Janus II custom-built supercomputer. In those
cases where thermal equilibrium is reachable (i.e., in the paramagnetic phase),
both simulations reach compatible values. More significantly, barring
short-time corrections, a simple time rescaling suffices to map the mic.SA
off-equilibrium dynamics onto the results obtained with standard simulations.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [121] [SAFER-D: A Self-Adaptive Security Framework for Distributed Computing Architectures](https://arxiv.org/abs/2506.16545)
*Marco Stadler,Michael Vierhauser,Michael Riegler,Daniel Waghubinger,Johannes Sametinger*

Main category: cs.CR

TL;DR: 论文提出了一种自适应的整体安全框架，用于增强分布式计算架构的安全性，应对物联网和网络物理系统中的复杂网络攻击。


<details>
  <summary>Details</summary>
Motivation: 物联网和网络物理系统的兴起增加了网络复杂性，传统安全措施难以应对新型攻击，亟需创新的防御机制。

Method: 开发了一种结合多种自适应策略的整体安全框架，并将其应用于实际场景中。

Result: 评估显示该框架具有良好的适用性和效率，展现了进一步扩展研究的潜力。

Conclusion: 提出的自适应安全框架为分布式计算架构的安全性提供了有效解决方案，未来研究可进一步优化和扩展其功能。

Abstract: The rise of the Internet of Things and Cyber-Physical Systems has introduced
new challenges on ensuring secure and robust communication. The growing number
of connected devices increases network complexity, leading to higher latency
and traffic. Distributed computing architectures (DCAs) have gained prominence
to address these issues. This shift has significantly expanded the attack
surface, requiring additional security measures to protect all components --
from sensors and actuators to edge nodes and central servers. Recent incidents
highlight the difficulty of this task: Cyberattacks, like distributed denial of
service attacks, continue to pose severe threats and cause substantial damage.
Implementing a holistic defense mechanism remains an open challenge,
particularly against attacks that demand both enhanced resilience and rapid
response. Addressing this gap requires innovative solutions to enhance the
security of DCAs. In this work, we present our holistic self-adaptive security
framework which combines different adaptation strategies to create
comprehensive and efficient defense mechanisms. We describe how to incorporate
the framework into a real-world use case scenario and further evaluate its
applicability and efficiency. Our evaluation yields promising results,
indicating great potential to further extend the research on our framework.

</details>


### [122] [Sudoku: Decomposing DRAM Address Mapping into Component Functions](https://arxiv.org/abs/2506.15918)
*Minbok Wi,Seungmin Baek,Seonyong Park,Mattan Erez,Jung Ho Ahn*

Main category: cs.CR

TL;DR: 提出了一种基于时序的技术Sudoku，能够自动分解DRAM地址映射到各组件功能。


<details>
  <summary>Details</summary>
Motivation: 现有逆向工程方法无法有效分解DRAM地址映射，影响内存行为理解和RowHammer攻击的精准性。

Method: 利用DRAM刷新间隔和连续访问延迟的时序技术，开发软件工具Sudoku。

Result: 成功在Intel和AMD处理器上分解出通道、rank、组和bank等功能的地址映射。

Conclusion: Sudoku解决了DRAM地址映射分解的难题，提升了内存行为的理解和攻击精确性。

Abstract: Decomposing DRAM address mappings into component-level functions is critical
for understanding memory behavior and enabling precise RowHammer attacks, yet
existing reverse-engineering methods fall short. We introduce novel
timing-based techniques leveraging DRAM refresh intervals and consecutive
access latencies to infer component-specific functions. Based on this, we
present Sudoku, the first software-based tool to automatically decompose full
DRAM address mappings into channel, rank, bank group, and bank functions while
identifying row and column bits. We validate Sudoku's effectiveness,
successfully decomposing mappings on recent Intel and AMD processors.

</details>


### [123] [ETrace:Event-Driven Vulnerability Detection in Smart Contracts via LLM-Based Trace Analysis](https://arxiv.org/abs/2506.15790)
*Chenyang Peng,Haijun Wang,Yin Wu,Hao Wu,Ming Fan,Yitao Zhao,Ting Liu*

Main category: cs.CR

TL;DR: 论文提出了一种名为ETrace的新型事件驱动智能合约漏洞检测框架，无需源代码即可通过LLM追踪分析潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着区块链技术应用广泛，智能合约的安全性和稳定性成为关键挑战，传统漏洞检测方法依赖源代码分析，但并非所有合约都提供源代码。

Method: ETrace通过从交易日志中提取细粒度事件序列，利用大型语言模型（LLMs）作为自适应语义解释器，通过链式推理重建事件分析，并采用模式匹配建立交易行为与已知攻击行为的因果关系。

Result: 初步实验结果验证了ETrace的有效性。

Conclusion: ETrace为无需源代码的智能合约漏洞检测提供了一种创新解决方案。

Abstract: With the advance application of blockchain technology in various fields,
ensuring the security and stability of smart contracts has emerged as a
critical challenge. Current security analysis methodologies in vulnerability
detection can be categorized into static analysis and dynamic analysis
methods.However, these existing traditional vulnerability detection methods
predominantly rely on analyzing original contract code, not all smart contracts
provide accessible code.We present ETrace, a novel event-driven vulnerability
detection framework for smart contracts, which uniquely identifies potential
vulnerabilities through LLM-powered trace analysis without requiring source
code access. By extracting fine-grained event sequences from transaction logs,
the framework leverages Large Language Models (LLMs) as adaptive semantic
interpreters to reconstruct event analysis through chain-of-thought reasoning.
ETrace implements pattern-matching to establish causal links between
transaction behavior patterns and known attack behaviors. Furthermore, we
validate the effectiveness of ETrace through preliminary experimental results.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [124] [Fast solvers for Tokamak fluid models with PETSC -- Part I](https://arxiv.org/abs/2506.16676)
*Mark F. Adams,Jin Chen,Benjamin Sturdevant*

Main category: physics.plasm-ph

TL;DR: 论文提出在托卡马克磁流体动力学模型中引入多重网格求解器，以替代直接求解器。


<details>
  <summary>Details</summary>
Motivation: 直接求解器在现代硬件上并行性不足且效率不高，为此提出改进方案。

Method: 研究在M3D-C1代码中，利用PETSC库实现一维多级网格求解器，特别关注速度求解部分。

Result: 实验显示，新方法比原有方法快约5倍。

Conclusion: 多级网格求解器能显著提升求解效率，适用于现代硬件需求。

Abstract: This report develops the first step in adding multigrid solvers to scientific
and engineering-relevant magnetohydrodynamics (MHD) models of Tokamaks. These
models are characterized by a distinguished direction in the toroidal
coordinate that is partially aligned with the magnetic guide field, which
dominates the plasma dynamics. All Tokamak models exploit this structure, for
example, NIMROD (https://nimrodteam.org/) uses $2D$, unstructured, high-order
finite elements in the poloidal plane with Fourier modes in the toroidal
coordinate, and the $3D$, extended MHD code M3D-C1
(https://w3.pppl.gov/~nferraro/m3dc1.html) uses $2D$, unstructured $C^1$
elements in the poloidal plane with cubic Hermite functions in the toroidal
direction. This structure suggests adding toroidal semi-coarsening multigrid to
the existing solver and thereby reducing reliance on direct solvers, which do
not scale optimally and are not well suited to modern hardware that demands
extreme levels of parallelism. This report focuses on the velocity solve in
M3D-C1, using the PETSC -- the Portable, Extensible Toolkit for Scientific
Computation -- numerical library (https://petsc.org), and shows that with
little new application code, one-dimensional multigrid is about $5x$ faster
than the existing one-level method on an MHD disruption, with runaway
electrons, test problem.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [125] [Spatially-Aware Evaluation of Segmentation Uncertainty](https://arxiv.org/abs/2506.16589)
*Tal Zeevi,Eléonore V. Lieffrig,Lawrence H. Staib,John A. Onofrey*

Main category: cs.CV

TL;DR: 本文提出三种考虑空间结构的指标，用于评估医学图像分割中的不确定性模式，相比传统指标更能区分有意义和虚假的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统的不确定性评估指标忽略了空间上下文和解剖结构，导致对不同的不确定性模式（如散点状或边界对齐）评分相同，无法满足临床需求。

Method: 提出了三种结合结构和边界信息的空间感知指标，并在前列腺分区分割数据上进行了验证。

Result: 新指标能更好地与临床重要因素对齐，并有效区分有意义和虚假的不确定性模式。

Conclusion: 空间感知指标在医学图像分割中具有更高的实用性和准确性，适用于临床评估。

Abstract: Uncertainty maps highlight unreliable regions in segmentation predictions.
However, most uncertainty evaluation metrics treat voxels independently,
ignoring spatial context and anatomical structure. As a result, they may assign
identical scores to qualitatively distinct patterns (e.g., scattered vs.
boundary-aligned uncertainty). We propose three spatially aware metrics that
incorporate structural and boundary information and conduct a thorough
validation on medical imaging data from the prostate zonal segmentation
challenge within the Medical Segmentation Decathlon. Our results demonstrate
improved alignment with clinically important factors and better discrimination
between meaningful and spurious uncertainty patterns.

</details>


### [126] [Fine-grained Image Retrieval via Dual-Vision Adaptation](https://arxiv.org/abs/2506.16273)
*Xin Jiang,Meiqi Cao,Hao Tang,Fei Shen,Zechao Li*

Main category: cs.CV

TL;DR: 文章提出了一种名为DVA的双视觉自适应方法，通过样本和特征的协同适应改进细粒度图像检索任务，避免过拟合并保留预训练知识。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度图像检索方法存在过拟合问题，忽视了大模型预训练知识，导致泛化能力下降。DVA旨在解决这一问题。

Method: DVA包含对象感知适应（修改输入样本）和上下文内适应（调整少量参数），并结合知识蒸馏机制提升性能。

Result: 实验表明，DVA参数少，在多个细粒度数据集上表现出色。

Conclusion: DVA通过协同适应和知识蒸馏，有效提升了细粒度图像检索的性能和泛化能力。

Abstract: Fine-Grained Image Retrieval~(FGIR) faces challenges in learning
discriminative visual representations to retrieve images with similar
fine-grained features. Current leading FGIR solutions typically follow two
regimes: enforce pairwise similarity constraints in the semantic embedding
space, or incorporate a localization sub-network to fine-tune the entire model.
However, such two regimes tend to overfit the training data while forgetting
the knowledge gained from large-scale pre-training, thus reducing their
generalization ability. In this paper, we propose a Dual-Vision Adaptation
(DVA) approach for FGIR, which guides the frozen pre-trained model to perform
FGIR through collaborative sample and feature adaptation. Specifically, we
design Object-Perceptual Adaptation, which modifies input samples to help the
pre-trained model perceive critical objects and elements within objects that
are helpful for category prediction. Meanwhile, we propose In-Context
Adaptation, which introduces a small set of parameters for feature adaptation
without modifying the pre-trained parameters. This makes the FGIR task using
these adjusted features closer to the task solved during the pre-training.
Additionally, to balance retrieval efficiency and performance, we propose
Discrimination Perception Transfer to transfer the discriminative knowledge in
the object-perceptual adaptation to the image encoder using the knowledge
distillation mechanism. Extensive experiments show that DVA has fewer learnable
parameters and performs well on three in-distribution and three
out-of-distribution fine-grained datasets.

</details>


### [127] [Class Agnostic Instance-level Descriptor for Visual Instance Search](https://arxiv.org/abs/2506.16745)
*Qi-Ying Sun,Wan-Lei Zhao,Yi-Bo Miao,Chong-Wah Ngo*

Main category: cs.CV

TL;DR: 本文提出一种基于自监督ViT的分层特征子集检测方法，用于解决图像检索中实例级别特征表示不足的问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度特征在图像检索中表现优异，但实例级别特征表示不足，且监督或弱监督方法对未知类别效果不佳。

Method: 利用自监督ViT输出的特征集，通过分层方式检测紧凑特征子集，构建多层次实例区域表示。

Result: 该方法在三个实例搜索基准测试中显著优于现有技术，且对已知和未知类别均有效。

Conclusion: 分层特征子集方法解决了对象嵌入和遮挡问题，为实例搜索提供了全面表示。

Abstract: Despite the great success of the deep features in content-based image
retrieval, the visual instance search remains challenging due to the lack of
effective instance level feature representation. Supervised or weakly
supervised object detection methods are not among the options due to their poor
performance on the unknown object categories. In this paper, based on the
feature set output from self-supervised ViT, the instance level region
discovery is modeled as detecting the compact feature subsets in a hierarchical
fashion. The hierarchical decomposition results in a hierarchy of feature
subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the
various instance regions in an image of different semantic scales. The
hierarchical decomposition well addresses the problem of object embedding and
occlusions, which are widely observed in the real scenarios. The features
derived from the nodes on the hierarchy make up a comprehensive representation
for the latent instances in the image. Our instance-level descriptor remains
effective on both the known and unknown object categories. Empirical studies on
three instance search benchmarks show that it outperforms state-of-the-art
methods considerably.

</details>


### [128] [TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration](https://arxiv.org/abs/2506.16784)
*Xiaoyu Shi,Rahul Kumar Jain,Yinhao Li,Ruibo Hou,Jingliang Cheng,Jie Bai,Guohua Zhao,Lanfen Lin,Rui Xu,Yen-wei Chen*

Main category: cs.CV

TL;DR: 该论文介绍了首个公开的多模态数据集TextBraTS，结合MRI和文本注释，并提出了一种新的文本引导医学图像分割方法，显著提升了脑肿瘤分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前脑肿瘤分析领域缺乏结合影像和文本的全面数据集，限制了多模态方法的发展。为了填补这一空白，研究者推出了TextBraTS数据集。

Method: 基于TextBraTS数据集，提出了一种新的基线框架和序列交叉注意力方法，用于文本引导的医学图像分割。

Result: 通过多种文本-图像融合策略和模板化文本的实验，该方法显著提高了脑肿瘤分割的准确率。

Conclusion: 研究不仅提供了数据集和实现代码，还为多模态集成技术提供了有价值的见解。

Abstract: Deep learning has demonstrated remarkable success in medical image
segmentation and computer-aided diagnosis. In particular, numerous advanced
methods have achieved state-of-the-art performance in brain tumor segmentation
from MRI scans. While recent studies in other medical imaging domains have
revealed that integrating textual reports with visual data can enhance
segmentation accuracy, the field of brain tumor analysis lacks a comprehensive
dataset that combines radiological images with corresponding textual
annotations. This limitation has hindered the exploration of multimodal
approaches that leverage both imaging and textual data.
  To bridge this critical gap, we introduce the TextBraTS dataset, the first
publicly available volume-level multimodal dataset that contains paired MRI
volumes and rich textual annotations, derived from the widely adopted BraTS2020
benchmark. Building upon this novel dataset, we propose a novel baseline
framework and sequential cross-attention method for text-guided volumetric
medical image segmentation. Through extensive experiments with various
text-image fusion strategies and templated text formulations, our approach
demonstrates significant improvements in brain tumor segmentation accuracy,
offering valuable insights into effective multimodal integration techniques.
  Our dataset, implementation code, and pre-trained models are publicly
available at https://github.com/Jupitern52/TextBraTS.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [129] [New Bounds for the Ideal Proof System in Positive Characteristic](https://arxiv.org/abs/2506.16397)
*Amik Raj Behera,Nutan Limaye,Varun Ramanathan,Srikanth Srinivasan*

Main category: cs.CC

TL;DR: 本文证明了正特征域上代数证明系统（IPS）多个子系统的上下界，扩展了之前关于特征0域的研究。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于证明$AC^{0}[p]$-Frege下界的重要性，以及代数复杂性理论中的新进展。

Method: 采用Forbes等人的函数下界方法，证明IPS子系统的指数规模下界，并展示了高效的上界结果。

Result: 证明了正特征域上IPS子系统的指数规模下界，以及高效常数深度IPS反驳能力。

Conclusion: 工作揭示了正特征域上代数证明系统的强大能力，并为后续研究提供了基础。

Abstract: In this work, we prove upper and lower bounds over fields of positive
characteristics for several fragments of the Ideal Proof System (IPS), an
algebraic proof system introduced by Grochow and Pitassi (J. ACM 2018). Our
results extend the works of Forbes, Shpilka, Tzameret, and Wigderson (Theory of
Computing 2021) and also of Govindasamy, Hakoniemi, and Tzameret (FOCS 2022).
These works primarily focused on proof systems over fields of characteristic
$0$, and we are able to extend these results to positive characteristic.
  The question of proving general IPS lower bounds over positive characteristic
is motivated by the important question of proving $AC^{0}[p]$-Frege lower
bounds. This connection was observed by Grochow and Pitassi (J. ACM 2018).
Additional motivation comes from recent developments in algebraic complexity
theory due to Forbes (CCC 2024) who showed how to extend previous lower bounds
over characteristic $0$ to positive characteristic.
  In our work, we adapt the functional lower bound method of Forbes et al.
(Theory of Computing 2021) to prove exponential-size lower bounds for various
subsystems of IPS. Additionally, we derive upper bounds for the instances
presented above. We show that they have efficient constant-depth IPS
refutations. We also show that constant-depth IPS can efficiently refute a
general class of instances, namely all symmetric instances, thereby further
uncovering the strength of these algebraic proofs in positive characteristic.
  Notably, our lower bounds hold for fields of arbitrary characteristic but
require the field size to be $n^{\omega(1)}$. In a concurrent work, Elbaz,
Govindasamy, Lu, and Tzameret have shown lower bounds against restricted
classes of IPS over finite fields of any size by considering different hard
instances.

</details>


### [130] [The Proof Analysis Problem](https://arxiv.org/abs/2506.16956)
*Noel Arteche,Albert Atserias,Susanna F. de Rezende,Erfan Khaniki*

Main category: cs.CC

TL;DR: 论文研究了Resolution证明系统中，短证明是否泄露了满足赋值的问题，并提出了新的算法和概念。


<details>
  <summary>Details</summary>
Motivation: 探讨Resolution短证明是否隐含满足赋值，以及如何通过算法提取这些信息，进一步研究证明复杂性。

Method: 提出多项式时间算法，从短Resolution证明中提取满足赋值，并引入Proof Analysis Problem (PAP)以研究证明系统。

Result: 确认短Resolution证明确实泄露满足赋值，并证明EF的PAP问题是NP完全的。

Conclusion: 研究为证明复杂性提供了新视角，揭示了证明系统自动化和下界问题的联系，并构造了新的硬实例。

Abstract: Atserias and M\"uller (JACM, 2020) proved that for every unsatisfiable CNF
formula $\varphi$, the formula $\operatorname{Ref}(\varphi)$, stating
"$\varphi$ has small Resolution refutations", does not have subexponential-size
Resolution refutations. Conversely, when $\varphi$ is satisfiable, Pudl\'ak
(TCS, 2003) showed how to construct a polynomial-size Resolution refutation of
$\operatorname{Ref}(\varphi)$ given a satisfying assignment of $\varphi$. A
question that remained open is: do all short Resolution refutations of
$\operatorname{Ref}(\varphi)$ explicitly leak a satisfying assignment of
$\varphi$?
  We answer this question affirmatively by giving a polynomial-time algorithm
that extracts a satisfying assignment for $\varphi$ given any short Resolution
refutation of $\operatorname{Ref}(\varphi)$. The algorithm follows from a new
feasibly constructive proof of the Atserias-M\"uller lower bound, formalizable
in Cook's theory $\mathsf{PV_1}$ of bounded arithmetic.
  Motivated by this, we introduce a computational problem concerning Resolution
lower bounds: the Proof Analysis Problem (PAP). For a proof system $Q$, the
Proof Analysis Problem for $Q$ asks, given a CNF formula $\varphi$ and a
$Q$-proof of a Resolution lower bound for $\varphi$, encoded as $\neg
\operatorname{Ref}(\varphi)$, whether $\varphi$ is satisfiable. In contrast to
PAP for Resolution, we prove that PAP for Extended Frege (EF) is NP-complete.
  Our results yield new insights into proof complexity: (i) every proof system
simulating EF is (weakly) automatable if and only if it is (weakly) automatable
on formulas stating Resolution lower bounds; (ii) we provide Ref formulas
exponentially hard for bounded-depth Frege systems; and (iii) for every strong
enough theory of arithmetic $T$ we construct unsatisfiable CNF formulas
exponentially hard for Resolution but for which $T$ cannot prove even a
quadratic lower bound.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [131] [Proofs that Modify Proofs, 1/2](https://arxiv.org/abs/2506.16491)
*Henry Towsner*

Main category: math.LO

TL;DR: 论文介绍了《证明修改证明》的序数分析，通过扩展系统解释证明函数，逐步分析无参数Π₁₂-理解的片段。


<details>
  <summary>Details</summary>
Motivation: 探讨证明修改证明的方法，并分析其在无参数Π₁₂-理解片段中的序数强度。

Method: 采用与《证明修改证明》相同的方法，通过扩展系统解释证明函数，分阶段构建序数分析。

Result: 实现了从无参数Π₁₁-理解到完整Π₁₁-理解的序数分析。

Conclusion: 该方法成功应用于分析无参数Π₁₂-理解片段的序数强度。

Abstract: This paper is a prelude and elaboration on Proofs that Modify Proofs. Here we
present an ordinal analysis of a fragment of the $\mu$-calculus around the
strength of parameter-free $\Pi^1_2$-comprehension using the same approach as
that paper, interpreting functions on proofs as proofs in an expanded system.
We build up the ordinal analysis in several stages, beginning by illustrating
the method systems at the strength of paremeter-free $\Pi^1_1$-comprehension
and full $\Pi^1_1$-comprehension.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [132] [Volumetric Parameterization for 3-Dimensional Simply-Connected Manifolds](https://arxiv.org/abs/2506.17025)
*Zhiyuan Lyu,Qiguang Chen,Gary P. T. Choi,Lok Ming Lui*

Main category: cs.CG

TL;DR: 论文提出了一种新的体积参数化方法，用于3D简单连通流形，旨在平衡几何结构和密度失真，并展示了其有效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以控制3D流形映射的双射性和局部几何失真，且未平衡不同属性。本文旨在解决这些问题。

Method: 提出了几种新颖的体积参数化方法，结合了几何结构保持、密度均衡和失真平衡的模型。

Result: 实验表明，这些方法能够生成具有不同理想属性的3D流形参数化，并在流形重网格应用中表现优异。

Conclusion: 新方法成功解决了传统方法的局限性，为3D流形参数化提供了有效的解决方案。

Abstract: With advances in technology, there has been growing interest in developing
effective mapping methods for 3-dimensional objects in recent years. Volumetric
parameterization for 3D solid manifolds plays an important role in processing
3D data. However, the conventional approaches cannot control the bijectivity
and local geometric distortions of the result mappings due to the complex
structure of the solid manifolds. Moreover, prior methods mainly focus on one
property instead of balancing different properties during the mapping process.
In this paper, we propose several novel methods for computing volumetric
parameterizations for 3D simply-connected manifolds. Analogous to surface
parameterization, our framework incorporates several models designed to
preserve geometric structure, achieve density equalization, and optimally
balance geometric and density distortions. With these methods, various 3D
manifold parameterizations with different desired properties can be achieved.
These methods are tested on different examples and manifold remeshing
applications, demonstrating their effectiveness and accuracy.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [133] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
*Fenghua Cheng,Jinxiang Wang,Sen Wang,Zi Huang,Xue Li*

Main category: cs.CL

TL;DR: 该论文提出了一个名为GeoGuess的多模态推理任务，要求根据街景图像定位并详细解释位置。为解决现有任务在层次化视觉线索推理上的不足，引入了数据集GeoExplain和推理方法SightSense，取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理任务在层次化视觉线索（如局部细节与全局上下文）推理方面存在不足，而GeoGuess任务通过结合多层次视觉信息与地理知识，填补了这一空白。

Method: 提出了GeoGuess任务，并创建数据集GeoExplain（包含街景-坐标-解释三元组）。同时开发了多模态多级推理方法SightSense，基于视觉信息层级和外部知识进行预测与解释。

Result: SightSense在GeoGuess任务中表现出色，验证了其多层次推理能力。

Conclusion: GeoGuess任务和SightSense方法为多模态推理提供了新方向，尤其在层次化视觉信息与知识关联方面具有潜力。

Abstract: Multimodal reasoning is a process of understanding, integrating and inferring
information across different data modalities. It has recently attracted surging
academic attention as a benchmark for Artificial Intelligence (AI). Although
there are various tasks for evaluating multimodal reasoning ability, they still
have limitations. Lack of reasoning on hierarchical visual clues at different
levels of granularity, e.g., local details and global context, is of little
discussion, despite its frequent involvement in real scenarios. To bridge the
gap, we introduce a novel and challenging task for multimodal reasoning, namely
GeoGuess. Given a street view image, the task is to identify its location and
provide a detailed explanation. A system that succeeds in GeoGuess should be
able to detect tiny visual clues, perceive the broader landscape, and associate
with vast geographic knowledge. Therefore, GeoGuess would require the ability
to reason between hierarchical visual information and geographic knowledge. In
this work, we establish a benchmark for GeoGuess by introducing a specially
curated dataset GeoExplain which consists of
panoramas-geocoordinates-explanation tuples. Additionally, we present a
multimodal and multilevel reasoning method, namely SightSense which can make
prediction and generate comprehensive explanation based on hierarchy of visual
information and external knowledge. Our analysis and experiments demonstrate
their outstanding performance in GeoGuess.

</details>


### [134] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.CL

TL;DR: REIS是一种针对RAG改进的ISP系统，通过优化数据库布局、数据放置技术和ANNS引擎，显著提升了检索性能和能效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的知识受限于其训练数据，RAG通过外部知识库补充静态知识，但检索阶段成为瓶颈。现有ISP技术存在算法不匹配、数据检索未加速和硬件修改复杂的问题。

Method: 提出REIS系统，采用优化的数据库布局、ISP量身定制的数据放置技术和利用存储系统现有资源的ANNS引擎。

Result: 相比服务器级系统，REIS在检索性能上平均提升13倍，能效提升55倍。

Conclusion: REIS通过软硬件协同设计，有效解决了RAG检索阶段的瓶颈问题，为未来ISP系统提供了实用且高效的解决方案。

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [135] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
*Taylor Lynn Curtis,Maximilian Puelma Touzel,William Garneau,Manon Gruaz,Mike Pinder,Li Wei Wang,Sukanya Krishna,Luda Cohen,Jean-François Godbout,Reihaneh Rabbany,Kellin Pelrine*

Main category: cs.CL

TL;DR: Veracity是一个开源AI系统，结合大语言模型和网络检索，提供多语言支持、评分和交互式界面，帮助用户对抗虚假信息。


<details>
  <summary>Details</summary>
Motivation: 虚假信息的泛滥对社会构成威胁，尤其是生成式AI的发展加剧了这一现象。

Method: 利用大语言模型和网络检索代理分析用户提交的声明，提供有依据的真实性评估和直观解释。

Result: 系统具备多语言支持、评分功能和交互式界面，不仅能检测虚假信息，还能解释推理过程。

Conclusion: Veracity通过透明的事实核查提升媒体素养，助力构建更知情的社会。

Abstract: The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.

</details>


### [136] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
*Jiaxin Pei,Dustin Wright,Isabelle Augenstin,David Jurgens*

Main category: cs.CL

TL;DR: 论文提出了一个计算框架，用于模拟公众对科学新闻的多维度感知，并开发了NLP模型预测感知分数。研究发现，科学新闻的公众感知与最终参与度直接相关。


<details>
  <summary>Details</summary>
Motivation: 科学传播中，公众对科学信息的感知和互动难以预测，但这对建立信任和理解至关重要。

Method: 使用计算框架构建大规模科学新闻感知数据集（10,489个注释），并开发NLP模型预测感知分数。通过Reddit实验分析感知与参与度的关系。

Result: 科学新闻消费频率是感知的主要驱动因素；公众感知分数高的帖子在Reddit上获得更多评论和点赞。

Conclusion: 强调感知建模在科学传播中的重要性，为预测公众兴趣和参与提供新途径。

Abstract: Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [137] [Autonomous Trajectory Optimization for UAVs in Disaster Zone Using Henry Gas Optimization Scheme](https://arxiv.org/abs/2506.15910)
*Zakria Qadir,Muhammad Bilal,Guoqiang Liu,Xiaolong Xu*

Main category: eess.SY

TL;DR: 提出了一种基于亨利气体优化（HGO）的集群优化方案（COS），用于无人机（UAV）在复杂环境中的轨迹优化，显著降低运输成本和计算时间。


<details>
  <summary>Details</summary>
Motivation: 在灾害多发环境中，无人机（UAV）在救援服务和提供互联网连接中扮演重要角色，但优化轨迹选择是关键挑战。

Method: 采用HGO元启发式算法设计数学模型，并与PSO、GWO、CSA和BMO等现有算法进行比较。

Result: 在多种环境测试中，HGO表现优于其他算法，尤其在常态环境中，运输成本降低39.3%，计算时间减少16.8%。

Conclusion: HGO算法适用于智能城市中无人机的自主轨迹优化。

Abstract: The unmanned aerial vehicles (UAVs) in a disaster-prone environment plays
important role in assisting the rescue services and providing the internet
connectivity with the outside world. However, in such a complex environment the
selection of optimum trajectory of UAVs is of utmost importance. UAV trajectory
optimization deals with finding the shortest path in the minimal possible time.
In this paper, a cluster optimization scheme (COS) is proposed using the Henry
gas optimization (HGO) metaheuristic algorithm to identify the shortest path
having minimal transportation cost and algorithm complexity. The mathematical
model is designed for COS using the HGO algorithm and compared with the
state-of-the-art metaheuristic algorithms such as particle swarm optimization
(PSO), grey wolf optimization (GWO), cuckoo search algorithm (CSA) and
barnacles mating optimizer (BMO). In order to prove the robustness of the
proposed model, four different scenarios are evaluated that includes ambient
environment, constrict environment, tangled environment, and complex
environment. In all the aforementioned scenarios, the HGO algorithm outperforms
the existing algorithms. Particularly, in the ambient environment, the HGO
algorithm achieves a 39.3% reduction in transportation cost and a 16.8%
reduction in computational time as compared to the PSO algorithm. Hence, the
HGO algorithm can be used for autonomous trajectory optimization of UAVs in
smart cities.

</details>


### [138] [Closed-Loop Molecular Communication with Local and Global Degradation: Modeling and ISI Analysis](https://arxiv.org/abs/2506.17112)
*Lukas Brand,Fardad Vakilipoor,Sören Botsch,Timo Jakumeit,Sebastian Lotter,Robert Schober,Maximilian Schäfer*

Main category: eess.SY

TL;DR: 提出了一种新型的基于物理的闭环分子通信系统信号传播模型，特别适用于生物医学应用，如心血管系统中的健康监测或药物输送。


<details>
  <summary>Details</summary>
Motivation: 闭环分子通信系统与开环系统相比，具有不同的信号传播特性，尤其是周期性信号分子到达引起的符号间干扰（ISI）。为了准确建模这些特性，研究团队开发了一个考虑多种环境因素的传播模型。

Method: 提出了一个分析模型，考虑了信号分子的时空释放模式、流体流动、扩散和降解等环境效应，并区分了局部和全局的信号分子清除机制。模型的准确性通过3D粒子模拟进行了验证。

Result: 模型能够准确捕捉闭环系统中的信号传播特性，并用于严格表征不同类型的符号间干扰现象。

Conclusion: 该模型为闭环分子通信系统的设计提供了理论基础，特别是在生物医学应用中具有重要价值。

Abstract: This paper presents a novel physics-based model for signal propagation in
closed-loop molecular communication (MC) systems, which are particularly
relevant for many envisioned biomedical applications, such as health monitoring
or drug delivery within the closed-loop human cardiovascular system (CVS).
Compared to open-loop systems, which are mostly considered in MC, closed-loop
systems exhibit different characteristic effects influencing signaling molecule
(SM) propagation. One key phenomenon are the periodic SM arrivals at the
receiver (RX), leading to various types of inter-symbol interference (ISI)
inherent to closed-loop system. To capture these characteristic effects, we
propose an analytical model for the SM propagation inside closed-loop systems.
The model accounts for arbitrary spatio-temporal SM release patterns at the
transmitter (TX), and incorporates several environmental effects such as fluid
flow, SM diffusion, and SM degradation. Moreover, to capture a wide range of
practically relevant degradation and clearance mechanisms, the model includes
both local removal (e.g., due to SM absorption into organs) and global removal
(e.g., due to chemical degradation) of SMs. The accuracy of the proposed model
is validated with three-dimensional (3-D) particle-based simulations (PBSs).
Moreover, we utilize the proposed model to develop a rigorous characterization
of the various types of ISI encountered in closed-loop MC systems.

</details>
