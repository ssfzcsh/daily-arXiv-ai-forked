<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 24]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 13]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 4]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CV](#cs.CV) [Total: 7]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: TrioXpert是一个端到端的事件管理框架，通过多模态数据和LLM协作推理机制，显著提升了异常检测、故障分类和根因定位的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单模态数据且无法同时处理多个任务，缺乏解释性。

Method: 设计和实现三个独立的数据处理管道，利用LLM进行协作推理。

Result: 在两项任务中性能显著提升：AD（4.7%-57.7%）、FT（2.1%-40.6%）、RCL（1.6%-163.1%）。

Conclusion: TrioXpert通过多模态和协作推理，高效解决了事件管理中的多个任务并提升了可解释性。

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [2] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: 该论文提出了一种流式流程模拟发现技术，通过结合增量流程发现和在线机器学习方法，动态适应业务环境的变化，优先考虑近期数据同时保留历史信息。实验证明该技术在模拟中更具稳定性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在动态业务环境中，传统流程模拟发现技术缺乏实时适应性，无法反映不断优化的业务流程。论文旨在解决这一问题，提出一种能实时适应变化的模拟技术。

Method: 结合增量流程发现和在线机器学习方法，优先利用近期数据，同时保留历史信息，以实现对流程动态变化的适应。

Result: 在四种不同事件日志上的实验表明，该技术能够通过权衡近期数据和历史知识，生成更稳定的模拟结果，并在处理概念漂移时表现出鲁棒性。

Conclusion: 所提出的流式流程模拟发现技术能有效适应动态业务环境，提高模拟的准确性和稳定性，为流程优化提供更好的支持。

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [3] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: 研究探讨了GitHub Copilot对学生在遗留代码基础上编程的影响，发现其显著提高了效率和进度。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI助手在遗留代码开发中对学生编程表现和行为的潜在影响。

Method: 通过对比实验和混合方法分析学生在使用和不使用Copilot时的表现。

Result: 学生使用Copilot时任务完成速度提高35%，方案进度增加50%，但对其建议的理解不足。

Conclusion: 需要开发新的教学方法，以结合AI助手的优势并促进学生对建议的理解。

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [4] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: 论文探讨了在使用LLM解决编程任务时，生成-修剪-排序方法中速度与精度的权衡，发现ORM在提升效率方面非常关键。


<details>
  <summary>Details</summary>
Motivation: 挑战当前关于‘优先使用全面验证器而非ORM’的共识，系统研究速度与精度之间的折衷。

Method: 比较了生成-修剪-排序方法与传统的生成-排序方法，分析了ORM在提升验证速度中的作用。

Result: 生成-修剪-排序方法比传统方法快11.65倍，精度仅降低8.33%，ORM能有效过滤错误解。

Conclusion: 生成-修剪-排序方法通过ORM提升验证效率，为设计高效且准确的程序排序系统提供了新思路。

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [5] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: 该论文提出了一种评估大型语言模型（LLM）在代码生成中对输入变化敏感性的合成评估流水线，以及基于用户背景的系统性角色评估方法。


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成中降低了编程门槛，但生成代码的质量受用户背景和提示质量影响较大，因此需要量化LLM对输入变化的敏感性。

Method: 提出合成评估流水线和系统性角色评估方法，独立于具体编程任务和LLM，广泛适用。

Result: 实验证明了方法的有效性，并分享了代码供社区使用。

Conclusion: 论文提供了一种独立于任务和模型的评估方法，有助于理解LLM在代码生成中的性能差异。

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [6] [AI-Based Software Vulnerability Detection: A Systematic Literature Review](https://arxiv.org/abs/2506.10280)
*Samiha Shimmi,Hamed Okhravi,Mona Rahimi*

Main category: cs.SE

TL;DR: 这篇论文系统地回顾了2018-2023年软件漏洞检测领域的AI驱动方法，发现91%的研究采用AI技术，其中图模型最常用，并指出了数据集质量、可复现性和可解释性等关键问题。


<details>
  <summary>Details</summary>
Motivation: 传统的漏洞检测方法（如静态分析、基于规则的匹配）存在局限性，促使研究转向AI驱动方法，以提高检测效率和准确性。

Method: 对2018-2023年的SVD研究进行系统性综述，涵盖技术分类、特征表示和嵌入方法的综合分类。

Result: 91%的研究使用AI方法，图模型是最主流的技术；同时发现数据集质量、可复现性和可解释性等关键问题。

Conclusion: 研究指出了未来研究方向，如联邦学习和量子神经网络等未充分探索的技术，为领域发展提供了路线图。

Abstract: Software vulnerabilities in source code pose serious cybersecurity risks,
prompting a shift from traditional detection methods (e.g., static analysis,
rule-based matching) to AI-driven approaches. This study presents a systematic
review of software vulnerability detection (SVD) research from 2018 to 2023,
offering a comprehensive taxonomy of techniques, feature representations, and
embedding methods. Our analysis reveals that 91% of studies use AI-based
methods, with graph-based models being the most prevalent. We identify key
limitations, including dataset quality, reproducibility, and interpretability,
and highlight emerging opportunities in underexplored techniques such as
federated learning and quantum neural networks, providing a roadmap for future
research.

</details>


### [7] [Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis](https://arxiv.org/abs/2506.10322)
*Xueying Du,Kai Yu,Chong Wang,Yi Zou,Wentai Deng,Zuoyu Ou,Xin Peng,Lingming Zhang,Yiling Lou*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的迭代路径可行性分析框架LLM4PFA，有效减少静态Bug检测中的误报率。


<details>
  <summary>Details</summary>
Motivation: 现有静态Bug分析器在大型代码库中误报率高，主要原因是路径可行性验证能力不足。

Method: 采用基于LLM代理的定向约束推理和上下文感知分析，增强复杂路径可行性验证。

Result: LLM4PFA能过滤72%-96%的误报，优于基线41.1%-105.7%，仅漏报3个真实Bug。

Conclusion: LLM4PFA显著提升了静态Bug检测的精度和可扩展性。

Abstract: Static bug analyzers play a crucial role in ensuring software quality.
However, existing analyzers for bug detection in large codebases often suffer
from high false positive rates. This is primarily due to the limited
capabilities of analyzers in path feasibility validation with multiple
conditional branches and complex data dependencies. While current LLM-based
approaches attempt to address this issue, their effectiveness remains limited
due to insufficient constraint cascade analysis and scalability challenges in
large projects. To address this challenge, we propose an iterative path
feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted
constraint reasoning, and key context-aware analysis driven by agent planning,
LLM4PFA effectively enhances complex inter-procedural path feasibility analysis
for minimizing false positives in static bug detection. Evaluation results show
that LLM4PFA precisely filters out 72% to 96% false positives reported during
static bug detection, significantly outperforming all the baselines by 41.1% -
105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true
positives.

</details>


### [8] [Solving Package Management via Hypergraph Dependency Resolution](https://arxiv.org/abs/2506.10803)
*Ryan Gibb,Patrick Ferris,David Allsopp,Michael Winston Dales,Mark Elvers,Thomas Gazagnaire,Sadiq Jaffer,Thomas Leonard,Jon Ludlam,Anil Madhavapeddy*

Main category: cs.SE

TL;DR: 论文提出HyperRes，一个跨语言生态系统的依赖解析系统，通过超图建模解决不同包管理器间的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 多语言项目无法跨生态系统表达精确依赖，现有包管理器互操作性不足。

Method: 定义HyperRes系统，将多种包管理器翻译为超图模型，实现跨生态系统依赖解析。

Result: 证明依赖解析可在当前独立的生态系统中实现，无需用户更换包管理器。

Conclusion: HyperRes通过翻译包装元数据，为特定部署环境提供精确的依赖解决方案。

Abstract: Package managers are everywhere, with seemingly every language and operating
system implementing their own solution. The lack of interoperability between
these systems means that multi-lingual projects are unable to express precise
dependencies across language ecosystems, and external system and hardware
dependencies are typically implicit and unversioned. We define HyperRes, a
formal system for describing versioned dependency resolution using a hypergraph
that is expressive enough to model many ecosystems and solve dependency
constraints across them. We define translations from dozens of existing package
managers to HyperRes and comprehensively demonstrate that dependency resolution
can work across ecosystems that are currently distinct. This does not require
users to shift their choice of package managers; instead, HyperRes allows for
the translation of packaging metadata between ecosystems, and for solving to be
precisely specialised to a particular deployment environment.

</details>


### [9] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: 利用大型语言模型（如 GPT-3.5 Turbo 和 GPT-4o）结合静态代码分析和检索增强生成（RAG）技术，自动检测和修复代码问题，显著提高代码质量并优化开发流程。


<details>
  <summary>Details</summary>
Motivation: 通过自动化工具减少软件开发中的人工干预，提高代码质量和效率。

Method: 结合静态代码分析框架检测问题，利用LLMs进行自动化修复，并通过RAG和迭代提示工程优化输出。

Result: 代码问题显著减少，开发流程得到优化。

Conclusion: LLMs与静态分析及RAG技术的结合有效提高了代码质量和开发效率。

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [10] [Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization](https://arxiv.org/abs/2506.10624)
*Lukas Jünger,Jan Henrik Weinstock,Tim Kraus*

Main category: cs.SE

TL;DR: 提出一种基于容器化的虚拟平台（VP）方法，以降低环境依赖性并支持云部署，加速硬件/软件协同开发。


<details>
  <summary>Details</summary>
Motivation: 解决硬件/软件系统复杂性带来的安全关键领域（如汽车）中测试挑战，尤其是硬件可用性滞后问题。

Method: 利用SystemC TLM-2.0标准的虚拟平台，结合容器化技术封装VP，支持云部署和并行测试，采用开源VP技术（如QEMU、VCML）。

Result: 通过AI加速器VP案例研究验证了方法的有效性，提供了加速硬件/软件协同开发的解决方案。

Conclusion: 该方法为解决硬件/软件系统复杂性提供了实用方案，支持早期软件开发和测试。

Abstract: The ever-increasing complexity of HW/SW systems presents a persistent
challenge, particularly in safety-critical domains like automotive, where
extensive testing is imperative. However, the availability of hardware often
lags behind, hindering early-stage software development. To address this,
Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a
pivotal solution, enabling pre-silicon execution and testing of unmodified
target software. In this study, we propose an approach leveraging
containerization to encapsulate VPs in order to reduce environment dependencies
and enable cloud deployment for fast, parallelized test execution, as well as
open-source VP technologies such as QEMU and VCML to obviate the need for seat
licenses. To demonstrate the efficacy of our approach, we present an Artificial
Intelligence (AI) accelerator VP case study. Through our research, we offer a
robust solution to address the challenges posed by the complexity of HW/SW
systems, with practical implications for accelerating HW/SW co-development.

</details>


### [11] [AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine](https://arxiv.org/abs/2506.10365)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Haoyue Jiao,Ziqi Liu,Lutong Xie,Chang Liu,Jianyuan Liang,Yaxian Qing,Xiaopu Zhang,Dehua Peng,Zhipeng Gui,Xuefeng Guan*

Main category: cs.SE

TL;DR: AutoGEEval++是一个用于评估生成Google Earth Engine (GEE)地理空间代码的大型语言模型(LLMs)的自动化框架，支持多维指标评估和边界测试。


<details>
  <summary>Details</summary>
Motivation: 缺乏标准化的自动化工具来评估生成地理空间代码的LLMs。

Method: 基于GEE Python API构建AutoGEEval++框架，包含6,365个测试案例和自动化评估模块。

Result: 评估24种LLMs，发现其在性能、稳定性和错误类型上的显著差异。

Conclusion: AutoGEEval++为GEE代码生成提供了标准化评估基准，具有实用性和可扩展性。

Abstract: Geospatial code generation is becoming a key frontier in integrating
artificial intelligence with geo-scientific analysis, yet standardised
automated evaluation tools for this task remain absent. This study presents
AutoGEEval++, an enhanced framework building on AutoGEEval, and the first
automated assessment system for large language models (LLMs) generating
geospatial code on Google Earth Engine (GEE). It supports diverse data
modalities and varying task complexities. Built on the GEE Python API,
AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test
cases across 26 data types and three task categories: unit, combo, and theme
tests. It includes a submission programme and a judge module to realise an
end-to-end automated evaluation pipeline from code generation to
execution-based validation. The framework adopts multi-dimensional
metrics-accuracy, resource usage, run-time efficiency, and error
types-balancing hallucination control and efficiency, and enabling boundary
testing and error pattern analysis. Using AutoGEEval++, we evaluate 24
state-of-the-art LLMs (as of June 2025), including general-purpose,
reasoning-enhanced, code-centric, and geoscience-specific models. Results
reveal clear performance, stability, and error differences across task types,
model designs, and deployment settings, confirming AutoGEEval++'s practical
value and scalability in vertical-domain code generation. This work establishes
the first standardised evaluation protocol and foundational benchmark for
GEE-based LLM code generation, providing a unified basis for performance
comparison and a methodological framework for systematic, domain-specific code
evaluation.

</details>


### [12] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/abs/2506.10376)
*Fan Wu,Cuiyun Gao,Shuqing Li,Xin-Cheng Wen,Qing Liao*

Main category: cs.SE

TL;DR: 论文提出LayoutCoder框架，利用多模态大语言模型解决UI2Code任务中的布局理解和代码生成问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化UI2Code任务对提高开发效率至关重要，但现有方法依赖大量标注数据且泛化能力不足。多模态大语言模型虽潜力大，但难以理解复杂UI布局并生成准确代码。

Method: LayoutCoder包含三个模块：元素关系构建、UI布局解析和布局引导的代码融合，用于从网页图像生成UI代码。

Result: 在Snap2Code和Design2Code数据集上，LayoutCoder性能最优，BLEU和CLIP分数分别平均提升10.14%和3.95%。

Conclusion: LayoutCoder通过布局引导的多模块设计，有效解决了UI2Code任务中的布局保持和代码生成问题，性能显著提升。

Abstract: Converting user interfaces into code (UI2Code) is a crucial step in website
development, which is time-consuming and labor-intensive. The automation of
UI2Code is essential to streamline this task, beneficial for improving the
development efficiency. There exist deep learning-based methods for the task;
however, they heavily rely on a large amount of labeled training data and
struggle with generalizing to real-world, unseen web page designs. The advent
of Multimodal Large Language Models (MLLMs) presents potential for alleviating
the issue, but they are difficult to comprehend the complex layouts in UIs and
generate the accurate code with layout preserved. To address these issues, we
propose LayoutCoder, a novel MLLM-based framework generating UI code from
real-world webpage images, which includes three key modules: (1) Element
Relation Construction, which aims at capturing UI layout by identifying and
grouping components with similar structures; (2) UI Layout Parsing, which aims
at generating UI layout trees for guiding the subsequent code generation
process; and (3) Layout-Guided Code Fusion, which aims at producing the
accurate code with layout preserved. For evaluation, we build a new benchmark
dataset which involves 350 real-world websites named Snap2Code, divided into
seen and unseen parts for mitigating the data leakage issue, besides the
popular dataset Design2Code. Extensive evaluation shows the superior
performance of LayoutCoder over the state-of-the-art approaches. Compared with
the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and
3.95% in the CLIP score on average across all datasets.

</details>


### [13] [Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation](https://arxiv.org/abs/2506.10397)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 提出了一种基于规则的自动化框架，用于分类量子软件仓库中的问题，重点针对量子特定错误类型。框架通过关键词和启发式技术实现，验证显示其分类准确性高，但仍存在改进空间。


<details>
  <summary>Details</summary>
Motivation: 准确的错误分类对提高软件质量至关重要，尤其是量子软件中的特定错误类型。

Method: 采用基于关键词和启发式的规则框架，对量子软件仓库中的问题进行自动分类，并通过手动分类样本验证其可靠性。

Result: 框架在准确性（85.21%）和F1分数（0.7075-0.8393）方面表现良好，但严重性分类仍需改进。量子特定错误占27.3%，多为量子电路级问题。

Conclusion: 该框架在量子软件错误分类方面表现出色，但严重性分类需进一步优化，同时揭示了量子特定错误的分布特征。

Abstract: Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen's
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.

</details>


### [14] [Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models](https://arxiv.org/abs/2506.10426)
*Xiao Yu,Haoxuan Chen,Feifei Niu,Xing Hu,Jacky Wai Keung,Xin Xia*

Main category: cs.SE

TL;DR: 论文对分布式训练与推理框架中的软件缺陷进行了首次大规模实证分析，涵盖了308个已修复缺陷，并探讨了其症状、根因及修复策略。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的发展，分布式训练框架的复杂性导致软件缺陷频发，亟需研究其特性以提高框架可靠性。

Method: 分析了DeepSpeed、Megatron-LM和Colossal-AI三个框架中的308个缺陷，研究了症状、根因、修复难度及常见修复策略。

Result: 发现48%的缺陷可通过少量代码修改（≤10行）解决，且存在独特的分布式根因（如分配策略错误、通信错误）。

Conclusion: 研究结果为提升框架可靠性及自动化调试提供了实用建议，并探讨了基于LLM的工具的潜在应用。

Abstract: With the rapid development of large language models (LLMs), distributed
training and inference frameworks like DeepSpeed have become essential for
scaling model training and inference across multiple GPUs or nodes. However,
the increasing complexity of these frameworks brings non-trivial software bugs,
which may degrade training performance, cause unexpected failures, and result
in significant resource waste. Understanding framework bugs' characteristics is
fundamental for quality assurance, allowing the design of more effective
debugging and repair methods. Thus, our paper conducts the first large-scale
empirical analysis of 308 fixed bugs across three popular distributed
training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We
examine bug symptoms, root causes, bug identification and fixing efforts, and
common low-effort fixing strategies. Additionally, the distributed nature of
these frameworks introduces unique bug root causes, such as allocation strategy
error and distributed communication error. Diagnosing and fixing complex bugs
remains challenging due to factors like the disconnect between symptoms and
root causes, high bug reproduction costs, and low-level or cross-component
interactions. Interestingly, we observe that 48% of bug fixes require minimal
code changes (<=10 LOC) and follow simple strategies such as conditional logic
optimization, parameter handling enhancement, or version compatibility
handling, indicating potential for automation. Based on these insights, we
offer several implications for improving the reliability of both distributed
training and inference frameworks and their dependent LLM projects, while also
identifying opportunities to leverage LLM-based tools for automated debugging
and repair.

</details>


### [15] [EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair](https://arxiv.org/abs/2506.10484)
*Fangwen Mu,Junjie Wang,Lin Shi,Song Wang,Shoubin Li,Qing Wang*

Main category: cs.SE

TL;DR: ExpeRepair是一种基于LLM的新型方法，通过双通道知识积累从历史修复经验中学习，优于现有开源方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件修复任务中存在孤立处理问题和静态提示策略的局限性，需改进。

Method: ExpeRepair利用双通道记忆系统（情景记忆和语义记忆），动态整合修复经验和抽象见解。

Result: 在SWE-bench Lite基准测试中，ExpeRepair以49.3%的pass@1分数表现最佳。

Conclusion: ExpeRepair通过动态学习和双记忆系统显著提升LLM在软件修复中的表现。

Abstract: Automatically repairing software issues remains a fundamental challenge at
the intersection of software engineering and AI. Although recent advancements
in Large Language Models (LLMs) have demonstrated potential for
repository-level repair tasks, current methodologies exhibit two notable
limitations: (1) they often address issues in isolation, neglecting to
incorporate insights from previously resolved issues, and (2) they rely on
static and rigid prompting strategies, which constrain their ability to
generalize across diverse and evolving issue scenarios. Inspired by the dual
memory systems of human cognition, where episodic and semantic memories work
synergistically to support human reasoning and decision-making, we propose
ExpeRepair, a novel LLM-based approach that continuously learns from historical
repair experiences through dual-channel knowledge accumulation. ExpeRepair
organizes historical repair experiences into two complementary memories: an
episodic memory that stores concrete repair demonstrations, and a semantic
memory that encodes abstract reflective insights. At inference time, ExpeRepair
activates both memory systems by retrieving relevant demonstrations from
episodic memory and recalling high-level repair insights from semantic memory.
It further enhances adaptability through dynamic prompt composition,
synergistically integrating both memory types to replace static prompts with
context-aware, experience-driven prompts. Experiments on the SWE-bench Lite
benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with
Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.

</details>


### [16] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: BugGen是一个利用LLM自动生成和验证RTL功能错误的创新方法，显著提升了验证效率和ML调试能力。


<details>
  <summary>Details</summary>
Motivation: 硬件复杂性增加了验证负担，现有方法无法可靠生成多样化的错误数据集，促使开发一种自动化的替代方案。

Method: BugGen通过多智能体架构，结合模块分区、目标选择和迭代优化，生成并验证错误。

Result: BugGen在OpenTitan上生成了500个独特错误，功能准确率达94%，验证效率是人工的5倍以上，并暴露了104个未检测错误。

Conclusion: BugGen提供了高质量错误数据集的生成方案，对验证和ML辅助调试有显著帮助。

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [17] [AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length](https://arxiv.org/abs/2506.10525)
*Junhang Cheng,Fang Liu,Chengru Wu,Li Zhang*

Main category: cs.SE

TL;DR: 该论文提出了AdaptiveLLM框架，通过动态选择最佳LLM优化代码生成任务中的性能与成本平衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在代码生成中面临性能与成本不平衡的挑战，现有方法资源密集且依赖人工标注难度标签。

Method: AdaptiveLLM通过Chain-of-Thought长度估计任务难度，聚类后使用CodeBERT嵌入难度特征，XGBoost选择最优模型。

Result: 实验显示AdaptiveLLM在pass@1上提升7.86%，资源消耗降低88.9%，比单个模型提升15%准确率。

Conclusion: AdaptiveLLM通过自动评估难度和动态选择模型，实现了性能与成本的高效平衡。

Abstract: While Large Language Models (LLMs) have significantly advanced code
generation efficiency, they face inherent challenges in balancing performance
and inference costs across diverse programming tasks. Dynamically selecting the
optimal LLM based on task difficulty and resource constraints offers a
promising approach to achieve an optimal balance between efficiency and
performance. However, existing model selection methods are resource-intensive
and often neglect cost efficiency. Moreover, these approaches rely on
human-annotated difficulty labels that are frequently inaccessible in
real-world settings and may not align with the LLM's own assessment of task
difficulty. In this paper, we introduce AdaptiveLLM, a framework that
dynamically selects optimal LLMs for a given coding task by automatically
assessing task difficulty. Our framework first estimates task difficulty using
Chain-of-Thought lengths generated by reasoning model, clusters these into
three difficulty levels via k-means, and fine-tunes CodeBERT to embed
difficulty-aware features. A trained XGBoost classifier then selects the best
model for each problem, optimizing the performance-cost trade-off. Experimental
results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score
while reducing resource consumption by 88.9% compared to baseline method
ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an
approximately 15% accuracy improvement, while maintaining the same level of
cost consumption. Apart from that, the difficulty assessment using CoT provides
more reliable selection criteria than human evaluation. Our replication package
is available at https://github.com/cjhCoder7/AdaptiveLLM.

</details>


### [18] [Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub](https://arxiv.org/abs/2506.10654)
*Abir Bouraffa,Carolin Brandt,Andy Zaidmann,Walid Maalej*

Main category: cs.SE

TL;DR: 研究分析了代码审查中开发者的导航顺序，发现44.6%的Pull请求评论顺序非字母序，部分遵循有意义顺序；非字母序审查更高效，但获批率略低。


<details>
  <summary>Details</summary>
Motivation: 探讨代码审查中开发者导航顺序的多样性和其对审查效率的影响。

Method: 分析了23,241个GitHub Pull请求中的评论顺序，识别出非字母序的导航模式及其类型。

Result: 44.6%的Pull请求评论顺序非字母序，包括最大差异优先、标题相似度优先、测试优先等模式；非字母序审查覆盖率更高但获批率略低。

Conclusion: 需要为大型Pull请求提供更多审查支持，开发者倾向于复杂策略而非固定顺序。

Abstract: Developers use tools such as GitHub pull requests to review code, discuss
proposed changes, and request modifications. While changed files are commonly
presented in alphabetical order, this does not necessarily coincide with the
reviewer's preferred navigation sequence. This study investigates the different
navigation orders developers follow while commenting on changes submitted in
pull requests. We mined code review comments from 23,241 pull requests in 100
popular Java and Python repositories on GitHub to analyze the order in which
the reviewers commented on the submitted changes. Our analysis shows that for
44.6% of pull requests, the reviewers comment in a non-alphabetical order.
Among these pull requests, we identified traces of alternative meaningful
orders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were
commented in the order of the files' similarity to the pull request's title and
description, and 29% (1,188) of pull requests containing changes to both
production and test files adhered to a test-first order. We also observed that
the proportion of reviewed files to total submitted files was significantly
higher in non-alphabetically ordered reviews, which also received slightly
fewer approvals from reviewers, on average. Our findings highlight the need for
additional support during code reviews, particularly for larger pull requests,
where reviewers are more likely to adopt complex strategies rather than
following a single predefined order.

</details>


### [19] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目旨在通过自然语言处理和人工智能技术，自动化生成形式化规范并追踪需求，从软件设计阶段到系统实现和验证。


<details>
  <summary>Details</summary>
Motivation: 解决形式化规范追踪和验证中的挑战。

Method: 结合自然语言处理、本体论、相似性重用、大语言模型和人工智能技术。

Result: 自动化生成形式化规范及需求追踪。

Conclusion: VERIFAI项目为软件开发的追踪和验证提供了创新方法。

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


### [20] [From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models](https://arxiv.org/abs/2506.10770)
*Joran Leest,Claudia Raibulet,Patricia Lago,Ilias Gerostathopoulos*

Main category: cs.SE

TL;DR: 论文提出了一种新的视角，通过引入C-SAR框架，系统地整理和分析了机器学习监控中的上下文信息。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习监控研究缺乏对上下文信息的统一理解，导致监控过程中难以进行有效的根因分析和警报生成。

Method: 论文对94项研究进行了系统性回顾，提出C-SAR框架，分类和结构化上下文信息，并识别了20种可复用的模式。

Result: 研究引入了C-SAR框架，为机器学习监控提供了一种新的系统性方法，强调了上下文信息的重要性。

Conclusion: 该研究为机器学习监控提供了一种系统化的方法，通过上下文信息的整合，提升了监控的可靠性和系统性。

Abstract: Machine learning (ML) models in production do not fail due to statistical
anomalies in their input data; they fail due to contextual misalignment -- when
their environment deviates from training assumptions, leading to unreliable
predictions. Effective ML monitoring requires rich contextual information to
move beyond detecting statistical shifts toward meaningful alerts and
systematic root-cause analysis. Yet, surprisingly, despite extensive research
in ML monitoring and related disciplines (drift detection, data validation,
out-of-distribution detection), there is no shared understanding of how to use
contextual information -- striking, given that monitoring involves
interpretation of information in context. In response, this paper presents a
systematic review to characterize and structure the various types of contextual
information in this domain. Our analysis examines 94 primary studies across
data mining, databases, software engineering, and ML. We introduce the
Contextual System--Aspect--Representation (C-SAR) framework, a conceptual model
that synthesizes our findings. We also identify 20 recurring and potentially
reusable patterns of specific system, aspect, and representation combinations,
and map them to the monitoring activities they support. This study provides a
new perspective on ML monitoring: from interpreting "tea leaves" of
observational statistics into constructing and managing "system maps" that
enable systematic and reliable ML monitoring practices.

</details>


### [21] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 本文首次对AI驱动的移动应用用户反馈进行了大规模研究，利用894K条Google Play评论，构建了一个多阶段分析管道，揭示了用户对AI功能的满意度与不满的主要主题。


<details>
  <summary>Details</summary>
Motivation: 尽管AI功能在移动应用中快速普及，但用户对这些功能的感知、评价和批评尚未被充分研究，主要原因是用户反馈量过大。

Method: 研究通过构建一个多阶段分析管道（包括评论分类、情感方面提取和聚类），利用人类标注基准和大型语言模型（LLMs）对292个AI驱动的应用进行了系统分析。

Result: 提取了100多万条情感方面对，聚类为18个正面和15个负面主题，揭示了用户关注的核心问题（如生产力、可靠性、技术故障和定价）。

Conclusion: 该方法比传统方法更准确地反映了用户对AI应用的体验，并为类别分析提供了新的见解。

Abstract: Artificial Intelligence (AI)-powered features have rapidly proliferated
across mobile apps in various domains, including productivity, education,
entertainment, and creativity. However, how users perceive, evaluate, and
critique these AI features remains largely unexplored, primarily due to the
overwhelming volume of user feedback. In this work, we present the first
comprehensive, large-scale study of user feedback on AI-powered mobile apps,
leveraging a curated dataset of 292 AI-driven apps across 14 categories with
894K AI-specific reviews from Google Play. We develop and validate a
multi-stage analysis pipeline that begins with a human-labeled benchmark and
systematically evaluates large language models (LLMs) and prompting strategies.
Each stage, including review classification, aspect-sentiment extraction, and
clustering, is validated for accuracy and consistency. Our pipeline enables
scalable, high-precision analysis of user feedback, extracting over one million
aspect-sentiment pairs clustered into 18 positive and 15 negative user topics.
Our analysis reveals that users consistently focus on a narrow set of themes:
positive comments emphasize productivity, reliability, and personalized
assistance, while negative feedback highlights technical failures (e.g.,
scanning and recognition), pricing concerns, and limitations in language
support. Our pipeline surfaces both satisfaction with one feature and
frustration with another within the same review. These fine-grained,
co-occurring sentiments are often missed by traditional approaches that treat
positive and negative feedback in isolation or rely on coarse-grained analysis.
To this end, our approach provides a more faithful reflection of the real-world
user experiences with AI-powered apps. Category-aware analysis further uncovers
both universal drivers of satisfaction and domain-specific frustrations.

</details>


### [22] [Evaluating Large Language Models on Non-Code Software Engineering Tasks](https://arxiv.org/abs/2506.10833)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: 论文提出SELU基准，评估大语言模型在17种非代码软件工程任务上的表现，并测试了22个开源模型和2个专有模型。中等规模的解码器模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码任务上表现优异，但在非代码软件工程任务上的效果尚未充分探索，因此需要建立基准评估其能力。

Method: 构建SELU基准，涵盖17种非代码任务，包括分类、回归等，使用开源和专有模型进行测试，并通过多种指标和贝叶斯排名比较性能。

Result: 中等规模的解码器模型表现最稳定，代码预训练的领域适应效果有限。

Conclusion: SELU为模型选择提供指导，并指出未来可扩展至生成和设计任务的潜力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code understanding and generation; however, their effectiveness on non-code
Software Engineering (SE) tasks remains underexplored. We present the first
comprehensive benchmark, which we name `Software Engineering Language
Understanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from
identifying whether a requirement is functional or non-functional to estimating
the effort and complexity of backlog items. SELU covers classification,
regression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)
targets, with data drawn from diverse sources such as code repositories, issue
tracking systems, and developer forums. We fine-tune 22 open-source LLMs,
prompt two proprietary alternatives, and train two baselines. Performance is
measured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and
compared via the Bayesian signed-rank test. Our results show that
moderate-scale decoder-only models consistently form a top-tier, exhibiting
high mean performance and low across-task variance, while domain adaptation via
code-focused pre-training might yield only modest improvements. These insights
guide model selection for non-code SE workflows and highlight directions for
expanding SELU to generative and design-oriented scenarios.

</details>


### [23] [MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework](https://arxiv.org/abs/2506.10869)
*Quinn Thibeault,Giulia Pedrielli*

Main category: cs.SE

TL;DR: 这篇论文介绍了MultiCoSim，一个基于Python的仿真框架，用于解决复杂Cyber-Physical Systems (CPS)仿真中的灵活性和模块化挑战。


<details>
  <summary>Details</summary>
Motivation: 现有仿真工具在配置和灵活性方面存在局限性，阻碍了复杂CPS的研究和开发，因此需要一种更灵活的仿真框架。

Method: 提出了MultiCoSim框架，支持程序化定义和配置仿真组件，分布式组件协同仿真，并能够无缝替换和重新配置组件。

Result: 通过案例研究（如定制自动控制器和PX4自动驾驶仪的集成）验证了MultiCoSim的灵活性和实用性。

Conclusion: MultiCoSim能够简化CPS仿真流程，为研究和开发提供支持。

Abstract: Simulation is a foundational tool for the analysis and testing of
cyber-physical systems (CPS), underpinning activities such as algorithm
development, runtime monitoring, and system verification. As CPS grow in
complexity and scale, particularly in safety-critical and learning-enabled
settings, accurate analysis and synthesis increasingly rely on the rapid use of
simulation experiments. Because CPS inherently integrate hardware, software,
and physical processes, simulation platforms must support co-simulation of
heterogeneous components at varying levels of fidelity. Despite recent advances
in high-fidelity modeling of hardware, firmware, and physics, co-simulation in
diverse environments remains challenging. These limitations hinder the
development of reusable benchmarks and impede the use of simulation for
automated and comparative evaluation.
  Existing simulation tools often rely on rigid configurations, lack automation
support, and present obstacles to portability and modularity. Many are
configured through static text files or impose constraints on how simulation
components are represented and connected, making it difficult to flexibly
compose systems or integrate components across platforms.
  To address these challenges, we introduce MultiCoSim, a Python-based
simulation framework that enables users to define, compose, and configure
simulation components programmatically. MultiCoSim supports distributed,
component-based co-simulation and allows seamless substitution and
reconfiguration of components. We demonstrate the flexibility of MultiCoSim
through case studies that include co-simulations involving custom
automaton-based controllers, as well as integration with off-the-shelf
platforms like the PX4 autopilot for aerial robotics. These examples highlight
MultiCoSim's capability to streamline CPS simulation pipelines for research and
development.

</details>


### [24] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出自动化工具SWE-Factory，解决GitHub问题数据集构建中的挑战，包括环境搭建、评分和验证，实验显示高效低成本。


<details>
  <summary>Details</summary>
Motivation: 传统构建大规模GitHub问题数据集方法耗时耗力，需自动化解决方案。

Method: SWE-Factory包含三个核心组件：SWE-Builder（多代理系统）、标准化评分方法和自动化验证流程。

Result: 在671个问题上验证，成本低至0.024美元/实例，评分准确率100%，验证精度0.92、召回率1.00。

Conclusion: SWE-Factory能高效构建高质量数据集，推动LLMs在软件工程中的应用。

Abstract: Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [25] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: 提出一种将大型语言模型（LLM）与持久性Lisp环境结合的新架构，支持动态工具的创建与调用。


<details>
  <summary>Details</summary>
Motivation: 通过程序化交互实现LLM的工具定义与进化，结合符号编程与神经语言生成。

Method: 利用Lisp表达式嵌入生成层，并通过中间件拦截，实现状态化外部内存与动态工具创建。

Result: 提出设计框架与架构原则，为未来交互式AI系统开发提供指导。

Conclusion: 该架构为LLM与符号编程的结合提供了实用路径，支持动态工具开发与交互。

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


### [26] [A Language-Agnostic Logical Relation for Message-Passing Protocols](https://arxiv.org/abs/2506.10026)
*Tesla Zhang,Sonya Simkin,Rui Li,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: 提出了一种用于验证异构消息传递系统中协议合规性的框架，首次实现了与语言无关的逻辑关系，并通过Coq定理证明器实现。


<details>
  <summary>Details</summary>
Motivation: 由于分布式和异构系统中的应用通常涉及并发和消息传递，且与外部对象交互，传统的基于共同语言或类型系统的验证方法不再适用，因此需要新的验证框架。

Method: 开发了一个完全基于标记过渡语义的语言无关逻辑关系框架，能够验证任意对象（包括外部对象）的协议合规性。通过Coq定理证明器实现了该框架及两个案例研究。

Result: 成功验证了两种场景：(1)特定应用或硬件设备的单次验证，(2)给定类型系统的多应用程序一次性验证。

Conclusion: 该框架为异构系统中的协议合规性验证提供了通用且灵活的解决方案，适用于广泛的应用和设备。

Abstract: Today's computing landscape has been gradually shifting to applications
targeting distributed and *heterogeneous* systems, such as cloud computing and
Internet of Things (IoT) applications. These applications are predominantly
*concurrent*, employ *message-passing*, and interface with *foreign objects*,
ranging from externally implemented code to actual physical devices such as
sensors. Verifying that the resulting systems adhere to the intended protocol
of interaction is challenging -- the usual assumption of a common
implementation language, let alone a type system, no longer applies, ruling out
any verification method based on them. This paper develops a framework for
certifying *protocol compliance* of heterogeneous message-passing systems. It
contributes the first mechanization of a *language-agnostic logical relation*,
asserting that its inhabitants comply with the protocol specified. This
definition relies entirely on a labelled transition-based semantics,
accommodating arbitrary inhabitants, typed and untyped alike, including foreign
objects. As a case study, the paper considers two scenarios: (1) *per-instance
verification* of a specific application or hardware device, and (2)
*once-and-for-all verification* of well-typed applications for a given type
system. The logical relation and both scenarios are mechanized in the Coq
theorem prover.

</details>


### [27] [Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations](https://arxiv.org/abs/2506.10781)
*Zhiyao Zhong,Cyrus Omar*

Main category: cs.PL

TL;DR: Hazel Deriver是一款基于网络的实时编辑器，旨在通过多层支持帮助学生构建规则推导树，减少难度并提高理解与参与度。


<details>
  <summary>Details</summary>
Motivation: 学生在构建规则推导树时面临困难，如推理规则复杂、缺乏即时反馈和手写证明工作量大。

Method: 基于Hazel实时编程环境，设计交互式编辑器，提供分层支持和实时反馈。

Result: 初步研究表明，该工具降低了任务难度，同时提升了概念理解和参与度。

Conclusion: 讨论了分层设计功能，并提出在系统指导与学习者自主性之间平衡的问题。

Abstract: Students in programming languages and formal logic courses often struggle
with constructing rule-based derivation trees due to the complexity of applying
inference rules, the lack of immediate feedback, and the manual effort required
for handwritten proofs. We present Hazel Deriver, a live, web-based editor
designed to scaffold derivation construction through multiple layers of
support. Built on the Hazel live programming environment, it provides a
structured, interactive experience that encourages iterative exploration and
real-time feedback. A preliminary user study with former students suggests that
Hazel Deriver reduces the perceived difficulty of derivation tasks while
improving conceptual understanding and engagement. We discuss the design of its
layered scaffolding features and raise questions about balancing system
guidance with learner autonomy.

</details>


### [28] [Choreographic Quick Changes: First-Class Location (Set) Polymorphism](https://arxiv.org/abs/2506.10913)
*Ashley Samuelson,Andrew K. Hirsch,Ethan Cecchetti*

Main category: cs.PL

TL;DR: 提出了首个支持一等进程名和多态型的类型化编排语言$λ_{QC}$，填补现有编排语言的不足。


<details>
  <summary>Details</summary>
Motivation: 现有编排语言缺乏现代系统所需的关键功能，如动态计算节点分配等。

Method: 设计并实现了$λ_{QC}$，支持一等进程名、多态型、代数及递归数据类型、多重定位值，并在Rocq中形式化验证。

Result: $λ_{QC}$显著提升了表达能力，且确保无死锁。

Conclusion: $λ_{QC}$为并发系统编程提供了更强大的工具。

Abstract: Choreographic programming is a promising new paradigm for programming
concurrent systems where a developer writes a single centralized program that
compiles to individual programs for each node. Existing choreographic
languages, however, lack critical features integral to modern systems, like the
ability of one node to dynamically compute who should perform a computation and
send that decision to others. This work addresses this gap with $\lambda_{QC}$,
the first typed choreographic language with \emph{first class process names}
and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also
improves expressive power over previous work by supporting algebraic and
recursive data types as well as multiply-located values. We formalize and
mechanically verify our results in Rocq, including the standard choreographic
guarantee of deadlock freedom.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [29] [Playing in the Sandbox: A Study on the Usability of Seccomp](https://arxiv.org/abs/2506.10234)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.OS

TL;DR: 研究探讨了为何尽管沙盒技术能限制应用程序行为并防止进程被滥用，但实际被沙盒化的应用却相对较少。通过对7名有经验的Seccomp开发者的试用研究，发现他们在沙盒化应用时面临多种挑战，且各自采取了不同的解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索为何沙盒技术（如Seccomp）未广泛应用，尽管它在安全方面具有明显优势。

Method: 研究方法包括对7名有Seccomp开发经验的开发者进行试用研究，观察他们在沙盒化应用时的不同方法和面临的挑战。

Result: 研究结果显示，开发者在使用Seccomp时面临诸多困难，且各自采取了不同的解决方案。开发者还提出了改进建议，以简化沙盒化过程。

Conclusion: 研究结论指出，当前Seccomp的使用存在较高的复杂性，需进一步优化工具和指导，以提升开发者的沙盒化效率和效果。

Abstract: Sandboxing restricts what applications do, and prevents exploited processes
being abused; yet relatively few applications get sandboxed: why? We report a
usability trial with 7 experienced Seccomp developers exploring how they
approached sandboxing an application and the difficulties they faced. The
developers each approached sandboxing the application differently and each came
to different solutions. We highlight many challenges of using Seccomp, the
sandboxing designs by the participants, and what developers think would make it
easier for them to sandbox applications effectively.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [30] [AI5GTest: AI-Driven Specification-Aware Automated Testing and Validation of 5G O-RAN Components](https://arxiv.org/abs/2506.10111)
*Abiodun Ganiyu,Pranshav Gajjar,Vijay K Shah*

Main category: cs.NI

TL;DR: AI5GTest是一种基于AI的测试框架，用于自动化验证O-RAN组件的合规性，通过结合LLM和人工审核提高测试效率和准确性。


<details>
  <summary>Details</summary>
Motivation: O-RAN的分解架构带来复杂的测试挑战，现有手动测试框架存在碎片化和人为错误问题，亟需自动化解决方案。

Method: AI5GTest利用Gen-LLM生成测试流程，Val-LLM验证信号消息，Debug-LLM分析异常，并结合人工审核确保透明度。

Result: 相比传统方法，AI5GTest显著减少测试时间，同时保持高验证准确性。

Conclusion: AI5GTest为O-RAN测试提供了高效、准确的自动化解决方案，解决了现有框架的局限性。

Abstract: The advent of Open Radio Access Networks (O-RAN) has transformed the
telecommunications industry by promoting interoperability, vendor diversity,
and rapid innovation. However, its disaggregated architecture introduces
complex testing challenges, particularly in validating multi-vendor components
against O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as
those provided by Open Testing and Integration Centres (OTICs), rely heavily on
manual processes, are fragmented and prone to human error, leading to
inconsistency and scalability issues. To address these limitations, we present
AI5GTest -- an AI-powered, specification-aware testing framework designed to
automate the validation of O-RAN components. AI5GTest leverages a cooperative
Large Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and
Debug-LLM. Gen-LLM automatically generates expected procedural flows for test
cases based on 3GPP and O-RAN specifications, while Val-LLM cross-references
signaling messages against these flows to validate compliance and detect
deviations. If anomalies arise, Debug-LLM performs root cause analysis,
providing insight to the failure cause. To enhance transparency and
trustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the
Gen-LLM presents top-k relevant official specifications to the tester for
approval before proceeding with validation. Evaluated using a range of test
cases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest
demonstrates a significant reduction in overall test execution time compared to
traditional manual methods, while maintaining high validation accuracy.

</details>


### [31] [Large Language Models-Empowered Wireless Networks: Fundamentals, Architecture, and Challenges](https://arxiv.org/abs/2506.10651)
*Latif U. Khan,Maher Guizani,Sami Muhaidat,Choong Seon Hong*

Main category: cs.NI

TL;DR: 论文探讨了将大型语言模型（LLMs）与无线网络结合的概念，提出了一种基于电信LLMs的理性无线网络（LLM-native wireless systems），并通过案例研究展示了其分布式实现的优越性。


<details>
  <summary>Details</summary>
Motivation: 无线网络的快速发展带来了服务质量与用户体验需求的新挑战，同时LLMs在复杂任务中展现出潜力，促使研究者探索将两者结合的可行性。

Method: 提出了LLM-native wireless systems的概念，并通过基于双深度Q学习（DDQN）的解决方案进行了案例研究。

Result: 提出的DDQN解决方案在性能上优于现有的DDQN方案。

Conclusion: 论文总结了LLM与无线网络结合的潜力，并提出了未来研究的开放性问题。

Abstract: The rapid advancement of wireless networks has resulted in numerous
challenges stemming from their extensive demands for quality of service towards
innovative quality of experience metrics (e.g., user-defined metrics in terms
of sense of physical experience for haptics applications). In the meantime,
large language models (LLMs) emerged as promising solutions for many difficult
and complex applications/tasks. These lead to a notion of the integration of
LLMs and wireless networks. However, this integration is challenging and needs
careful attention in design. Therefore, in this article, we present a notion of
rational wireless networks powered by \emph{telecom LLMs}, namely,
\emph{LLM-native wireless systems}. We provide fundamentals, vision, and a case
study of the distributed implementation of LLM-native wireless systems. In the
case study, we propose a solution based on double deep Q-learning (DDQN) that
outperforms existing DDQN solutions. Finally, we provide open challenges.

</details>


### [32] [Energy-Efficient Deep Learning for Traffic Classification on Microcontrollers](https://arxiv.org/abs/2506.10851)
*Adel Chehade,Edoardo Ragusa,Paolo Gastaldo,Rodolfo Zunino*

Main category: cs.NI

TL;DR: 该论文提出了一种基于深度学习的轻量级1D-CNN，用于资源有限的微控制器上的高效流量分类，通过硬件感知的神经架构搜索优化，实现了高精度和低能耗。


<details>
  <summary>Details</summary>
Motivation: 解决物联网智能系统中资源受限设备上的流量分类问题，平衡准确性、计算效率和实际部署需求。

Method: 开发轻量级1D-CNN，采用硬件感知神经架构搜索优化，并进行INT8量化以减少能耗。

Result: 模型在ISCX VPN-NonVPN数据集上达到96.59%准确率，参数量仅88.26K，FLOPs为10.08M；在现实设备上部署时，能耗和延迟表现优秀。

Conclusion: 证明了在设备端进行加密流量分析的可行性，为低功耗物联网安全解决方案提供了新途径。

Abstract: In this paper, we present a practical deep learning (DL) approach for
energy-efficient traffic classification (TC) on resource-limited
microcontrollers, which are widely used in IoT-based smart systems and
communication networks. Our objective is to balance accuracy, computational
efficiency, and real-world deployability. To that end, we develop a lightweight
1D-CNN, optimized via hardware-aware neural architecture search (HW-NAS), which
achieves 96.59% accuracy on the ISCX VPN-NonVPN dataset with only 88.26K
parameters, a 20.12K maximum tensor size, and 10.08M floating-point operations
(FLOPs). Moreover, it generalizes across various TC tasks, with accuracies
ranging from 94% to 99%. To enable deployment, the model is quantized to INT8,
suffering only a marginal 1-2% accuracy drop relative to its Float32
counterpart. We evaluate real-world inference performance on two
microcontrollers: the high-performance STM32F746G-DISCO and the cost-sensitive
Nucleo-F401RE. The deployed model achieves inference latencies of 31.43ms and
115.40ms, with energy consumption of 7.86 mJ and 29.10 mJ per inference,
respectively. These results demonstrate the feasibility of on-device encrypted
traffic analysis, paving the way for scalable, low-power IoT security
solutions.

</details>


### [33] [Dynamic Beyond 5G and 6G Connectivity: Leveraging NTN and RIS Synergies for Optimized Coverage and Capacity in High-Density Environments](https://arxiv.org/abs/2506.10900)
*Valdemar Farré,Juan Estrada,David Vega,Luis F Urquiza-Aguiar,Juan A. Vásquez Peralvo,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 论文提出了一种结合非地面网络（NTN）和可重构智能表面（RIS）的6G网络规划框架，解决了高密度环境下传统地面网络的覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 大型户外活动中，传统地面网络在高密度环境下难以提供稳定的覆盖和容量，需新的解决方案。

Method: 通过整合LEO卫星、RIS平台和B5G技术（如mMIMO和波束成形），优化频谱利用，并基于动态SINR模型管理干扰。

Result: 仿真验证表明，该框架显著提升了高密度场景下的覆盖、可靠性和成本效益。

Conclusion: 该框架是满足未来6G网络需求的潜力解决方案。

Abstract: The increasing demand for reliable, high-capacity communication during
large-scale outdoor events poses significant challenges for traditional
Terrestrial Networks (TNs), which often struggle to provide consistent coverage
in high-density environments. This paper presents a novel 6G radio network
planning framework that integrates Non-Terrestrial Networks (NTNs) with
Reconfigurable Intelligent Surfaces (RISs) to deliver ubiquitous coverage and
enhanced network capacity. Our framework overcomes the limitations of
conventional deployable base stations by leveraging NTN architectures,
including Low Earth Orbit (LEO) satellites and passive RIS platforms seamlessly
integrated with Beyond 5G (B5G) TNs. By incorporating advanced B5G technologies
such as Massive Multiple Input Multiple Output (mMIMO) and beamforming, and by
optimizing spectrum utilization across the C, S, and Ka bands, we implement a
rigorous interference management strategy based on a dynamic SINR model.
Comprehensive calculations and simulations validate the proposed framework,
demonstrating significant improvements in connectivity, reliability, and
cost-efficiency in crowded scenarios. This integration strategy represents a
promising solution for meeting the evolving demands of future 6G networks.

</details>


### [34] [Agentic Semantic Control for Autonomous Wireless Space Networks: Extending Space-O-RAN with MCP-Driven Distributed Intelligence](https://arxiv.org/abs/2506.10925)
*Eduardo Baena,Paolo Testolina,Michele Polese,Sergi Aliaga,Andrew Benincasa,Dimitrios Koutsonikolas,Josep Jornet,Tommaso Melodia*

Main category: cs.NI

TL;DR: 提出了一种结合语义代理层的扩展方案，通过MCP和A2A协议实现上下文感知决策，优化月球表面无线通信系统。


<details>
  <summary>Details</summary>
Motivation: 解决月球表面无线通信系统在自主性、鲁棒性和适应性方面的严格需求，弥补Space-O-RAN在动态策略和语义集成上的不足。

Method: 引入语义代理层，利用MCP和A2A协议实现多层级上下文感知决策，部署分布式认知代理以协调无线通信策略。

Result: 实现了延迟自适应推理和带宽感知语义压缩，支持多层级交互和任务约束推理。

Conclusion: 该方案提升了月球表面无线通信系统的自适应能力，为任务执行提供了更高效的支持。

Abstract: Lunar surface operations impose stringent requirements on wireless
communication systems, including autonomy, robustness to disruption, and the
ability to adapt to environmental and mission-driven context. While Space-O-RAN
provides a distributed orchestration model aligned with 3GPP standards, its
decision logic is limited to static policies and lacks semantic integration. We
propose a novel extension incorporating a semantic agentic layer enabled by the
Model Context Protocol (MCP) and Agent-to-Agent (A2A) communication protocols,
allowing context-aware decision making across real-time, near-real-time, and
non-real-time control layers. Distributed cognitive agents deployed in rovers,
landers, and lunar base stations implement wireless-aware coordination
strategies, including delay-adaptive reasoning and bandwidth-aware semantic
compression, while interacting with multiple MCP servers to reason over
telemetry, locomotion planning, and mission constraints.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [35] [Semantic Communication-Enabled Cloud-Edge-End-collaborative Metaverse Services Architecure](https://arxiv.org/abs/2506.10001)
*Yuxuan Li,Sheng Jinag,Bizhu Wang*

Main category: cs.MM

TL;DR: 论文提出了一种基于语义通信的云-边-端协作沉浸式元宇宙架构（SC-CEE-Meta），通过语义传输和模块化设计解决VR数据传输中的延迟和带宽问题，显著提升了用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着技术发展，元宇宙需求增长，但高分辨率虚拟场景的无线传输面临带宽不足和延迟问题，用户体验不佳。

Method: 设计了SC-CEE-Meta架构，包含语义传输、视频合成和3D场景重建模块，将语义信息而非原始数据传输，优化资源分配。

Result: 在Meta Quest Pro上验证，无线传输延迟降低96.05%，图像质量提升43.99%。

Conclusion: SC-CEE-Meta通过语义通信和协同计算，有效解决了VR传输的带宽和延迟问题，显著改善用户体验。

Abstract: With technology advancing and the pursuit of new audiovisual experiences
strengthening, the metaverse has gained surging enthusiasm. However, it faces
practical hurdles as substantial data like high-resolution virtual scenes must
be transmitted between cloud platforms and VR devices. Specifically, the VR
device's wireless transmission hampered by insufficient bandwidth, causes speed
and delay problems. Meanwhile, poor channel quality leads to data errors and
worsens user experience. To solve this, we've proposed the Semantic
Communication-Enabled Cloud-Edge-End Collaborative Immersive Metaverse Service
(SC-CEE-Meta) Architecture, which includes three modules: VR video semantic
transmission, video synthesis, and 3D virtual scene reconstruction. By
deploying semantic modules on VR devices and edge servers and sending key
semantic info instead of focusing on bit-level reconstruction, it can cut
latency, resolve the resource-bandwidth conflict, and better withstand channel
interference. Also, the cloud deploys video synthesis and 3D scene
reconstruction preprocessing, while edge devices host 3D reconstruction
rendering modules, all for immersive services. Verified on Meta Quest Pro, the
SC-CEE-Meta can reduce wireless transmission delay by 96.05\% and boost image
quality by 43.99\% under poor channel condition.

</details>


### [36] [Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming](https://arxiv.org/abs/2506.10004)
*Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: 概述了扩展现实(XR)流媒体的最新技术，包括XR的定义、设备、流量特征、体验质量优化方法及未来挑战。


<details>
  <summary>Details</summary>
Motivation: XR技术快速发展，但流媒体领域的研究尚未系统化，需要全面梳理以推动技术进步。

Method: 通过分析XR流量特性、用户体验因素及视觉注意力优化方法，提出改进流媒体效率的策略。

Result: 总结了XR流媒体的关键技术、优化方法及现有应用，并指出了未来发展的主要挑战。

Conclusion: XR流媒体在技术和用户体验方面仍有提升空间，需要进一步研究以应对新兴需求。

Abstract: Extended reality (XR) is rapidly advancing, and poised to revolutionize
content creation and consumption. In XR, users integrate various sensory inputs
to form a cohesive perception of the virtual environment. This survey reviews
the state-of-the-art in XR streaming, focusing on multiple paradigms. To begin,
we define XR and introduce various XR headsets along with their multimodal
interaction methods to provide a foundational understanding. We then analyze XR
traffic characteristics to highlight the unique data transmission requirements.
We also explore factors that influence the quality of experience in XR systems,
aiming to identify key elements for enhancing user satisfaction. Following
this, we present visual attention-based optimization methods for XR streaming
to improve efficiency and performance. Finally, we examine current applications
and highlight challenges to provide insights into ongoing and future
developments of XR.

</details>


### [37] [EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis](https://arxiv.org/abs/2506.10002)
*Jianwu Fang,Lei-Lei Li,Zhedong Zheng,Hongkai Yu,Jianru Xue,Zhengguo Li,Tat-Seng Chua*

Main category: cs.MM

TL;DR: 论文提出了一种注意力视频扩散（AVD）模型，用于合成交通事故视频片段，解决交通场景中因果部分难以识别的问题，并通过生成的数据训练无监督的等效TAA（EQ-TAA）方法。实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 交通场景中交通事故预防（TAA）是一个关键问题，但由于数据偏倚和背景干扰，现有监督学习方法效果有限。论文希望通过生成因果视频片段来改进TAA。

Method: 提出AVD模型，通过文本提示生成交通事故或正常视频片段，同时保持帧的风格和内容。训练时无需额外标注，并设计EQ-TAA方法，利用生成的数据进行对比学习。

Result: 实验显示AVD和EQ-TAA在性能上优于现有方法，特别是在处理数据偏倚和背景干扰时表现更优。

Conclusion: AVD和EQ-TAA通过生成因果视频片段和进行对比学习，显著提升了TAA任务的性能，为无监督学习提供了新思路。

Abstract: Traffic Accident Anticipation (TAA) in traffic scenes is a challenging
problem for achieving zero fatalities in the future. Current approaches
typically treat TAA as a supervised learning task needing the laborious
annotation of accident occurrence duration. However, the inherent long-tailed,
uncertain, and fast-evolving nature of traffic scenes has the problem that real
causal parts of accidents are difficult to identify and are easily dominated by
data bias, resulting in a background confounding issue. Thus, we propose an
Attentive Video Diffusion (AVD) model that synthesizes additional accident
video clips by generating the causal part in dashcam videos, i.e., from normal
clips to accident clips. AVD aims to generate causal video frames based on
accident or accident-free text prompts while preserving the style and content
of frames for TAA after video generation. This approach can be trained using
datasets collected from various driving scenes without any extra annotations.
Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant
triple loss for an anchor accident-free video clip, along with the generated
pair of contrastive pseudo-normal and pseudo-accident clips. Extensive
experiments have been conducted to evaluate the performance of AVD and EQ-TAA,
and competitive performance compared to state-of-the-art methods has been
obtained.

</details>


### [38] [Integrating multimedia documents in 3D city models for a better understanding of territories](https://arxiv.org/abs/2506.10003)
*C. Gautier,J. Delanoy,G. Gesquière*

Main category: cs.MM

TL;DR: 论文探讨了如何通过整合多媒体文档和3D城市模型，以增强对城市环境的理解。


<details>
  <summary>Details</summary>
Motivation: 目前3D城市模型缺乏建筑的历史或功能等上下文信息，而多媒体文档（如图像、视频、文本）含有这些信息。两者的结合有助于城市分析。

Method: 提出了四种方法将多媒体文档集成到3D城市场景中，并结合用户引导模式，帮助用户理解城市环境。

Result: 在法国里昂地区的多个项目中验证了这些技术的实用性，例如通过数字导览为标志性建筑提供上下文或展示城市的演变。

Conclusion: 整合多媒体文档和3D场景是一种有效的城市理解工具，可用于空间导航和文档搜索。

Abstract: Digital 3D representations of urban areas, through their growing
availability, are a helpful tool to better understand a territory. However,
they lack contextual information about, for example, the history or
functionality of buildings. On another side, multimedia documents like images,
videos or texts usually contain such information. Crossing these two types of
data can therefore help in the analysis and understanding of the organization
of our cities. This could also be used to develop document search based on
spatial navigation, instead of the classical textual query. In this paper, we
propose four approaches to integrate multimedia documents in a 3D urban scene,
allowing to contextualize the scene with any type of media. We combine these
integration approaches with user guidance modes that allows to guide the user
through the consumption of these media and support its understanding of the
territory. We demonstrate the usefulness of these techniques in the context of
different projects within the Lyon area (France). The use of multimedia
documents integrated into a digital tour allows, for example, the iconic
buildings to be contextualised or to understand the evolution of a territory
through time.

</details>


### [39] [HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction](https://arxiv.org/abs/2506.10006)
*Jie Qin,Wei Yang,Yan Su,Yiran Zhu,Weizhen Li,Yunyue Pan,Chengchang Pan,Honggang Qi*

Main category: cs.MM

TL;DR: 提出了一种自适应双模态框架，通过动态分支选择、双向跨模态GAN和混合训练协议，显著提升了HER2评估的准确性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 当前HER2评估模型通常单独分析H&E或IHC图像，而临床实践中需要二者的协同解读，但双模态数据采集常受限于复杂性和成本。

Method: 1) 动态分支选择器；2) 双向跨模态GAN；3) 混合训练协议。

Result: 单模态H&E预测准确率从71.44%提升至94.25%，双模态准确率达95.09%，跨模态重建显著提升F1分数。

Conclusion: 该框架在资源有限的环境中具有潜力，能在不要求同步采集的情况下提供接近双模态的性能。

Abstract: Current HER2 assessment models for breast cancer predominantly analyze H&E or
IHC images in isolation,despite clinical reliance on their synergistic
interpretation. However, concurrent acquisition of both modalities is often
hindered by workflow complexity and cost constraints. We propose an adaptive
bimodal framework enabling flexible single-/dual-modality HER2 prediction
through three innovations: 1) A dynamic branch selector that activates either
single-modality reconstruction or dual-modality joint inference based on input
completeness; 2) A bidirectional cross-modal GAN performing context-aware
feature-space reconstruction of missing modalities; 3) A hybrid training
protocol integrating adversarial learning and multi-task optimization. This
architecture elevates single-modality H&E prediction accuracy from 71.44% to
94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28%
reliability with sole IHC inputs. The framework's "dual-preferred,
single-compatible" design delivers near-bimodal performance without requiring
synchronized acquisition, particularly benefiting resource-limited settings
through IHC infrastructure cost reduction. Experimental validation confirms
22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with
cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251
(IHC to HE). By dynamically routing inputs through reconstruction-enhanced or
native fusion pathways, the system mitigates performance degradation from
missing data while preserving computational efficiency (78.55% parameter
reduction in lightweight variant). This elastic architecture demonstrates
significant potential for democratizing precise HER2 assessment across diverse
healthcare settings.

</details>


### [40] [Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space](https://arxiv.org/abs/2506.10007)
*Kangwei Liu,Junwu Liu,Xiaowei Yi,Jinlin Guo,Yun Cao*

Main category: cs.MM

TL;DR: 本文提出了一种基于扩散的框架，用于可控的3D面部表情动画，解决了单一模态控制信号和确定性回归映射的限制。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于单一模态控制信号（如视频、文本或情感标签），且采用确定性回归映射，限制了表情和行为的多样性与自然性。

Method: 提出了一种基于FLAME的多模态情感绑定策略，通过对比学习对齐文本、音频和情感标签；同时设计了注意力机制的潜在扩散模型，增强运动的多样性和时间一致性。

Result: 实验表明，该方法在多数指标上优于现有方法，情感相似性提高了21.6%，同时保持了生理上合理的面部动态。

Conclusion: 该框架通过多模态信号和扩散模型，显著提升了3D面部动画的表现力和控制灵活性。

Abstract: Audio-driven emotional 3D facial animation encounters two significant
challenges: (1) reliance on single-modal control signals (videos, text, or
emotion labels) without leveraging their complementary strengths for
comprehensive emotion manipulation, and (2) deterministic regression-based
mapping that constrains the stochastic nature of emotional expressions and
non-verbal behaviors, limiting the expressiveness of synthesized animations. To
address these challenges, we present a diffusion-based framework for
controllable expressive 3D facial animation. Our approach introduces two key
innovations: (1) a FLAME-centered multimodal emotion binding strategy that
aligns diverse modalities (text, audio, and emotion labels) through contrastive
learning, enabling flexible emotion control from multiple signal sources, and
(2) an attention-based latent diffusion model with content-aware attention and
emotion-guided layers, which enriches motion diversity while maintaining
temporal coherence and natural facial dynamics. Extensive experiments
demonstrate that our method outperforms existing approaches across most
metrics, achieving a 21.6\% improvement in emotion similarity while preserving
physiologically plausible facial dynamics. Project Page:
https://kangweiiliu.github.io/Control_3D_Animation.

</details>


### [41] [Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics](https://arxiv.org/abs/2506.10008)
*Yi-Chun Chen*

Main category: cs.MM

TL;DR: 本文提出了一种分层知识图谱框架，用于结构化理解视觉叙事（如漫画），通过多模态图谱捕捉语义、空间和时间关系，并在Manga109数据集中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为解决视觉叙事中多模态媒体的结构化理解问题，提出一种分层知识图谱框架以支持复杂推理任务。

Method: 将叙事内容分解为多个层次，构造多模态图谱，整合视觉元素与文本组件，并在不同叙事层次间建立关联。

Result: 在Manga109数据集上的实验表明，该方法在动作检索、对话追踪等任务中展示了高精度和高召回率。

Conclusion: 该框架为视觉媒体的内容分析、交互叙事和多模态推理提供了可扩展的基础。

Abstract: This paper presents a hierarchical knowledge graph framework for the
structured understanding of visual narratives, focusing on multimodal media
such as comics. The proposed method decomposes narrative content into multiple
levels, from macro-level story arcs to fine-grained event segments. It
represents them through integrated knowledge graphs that capture semantic,
spatial, and temporal relationships. At the panel level, we construct
multimodal graphs that link visual elements such as characters, objects, and
actions with corresponding textual components, including dialogue and captions.
These graphs are integrated across narrative levels to support reasoning over
story structure, character continuity, and event progression.
  We apply our approach to a manually annotated subset of the Manga109 dataset
and demonstrate its ability to support symbolic reasoning across diverse
narrative tasks, including action retrieval, dialogue tracing, character
appearance mapping, and panel timeline reconstruction. Evaluation results show
high precision and recall across tasks, validating the coherence and
interpretability of the framework. This work contributes a scalable foundation
for narrative-based content analysis, interactive storytelling, and multimodal
reasoning in visual media.

</details>


### [42] [Multimodal Emotion Coupling via Speech-to-Facial and Bodily Gestures in Dyadic Interaction](https://arxiv.org/abs/2506.10010)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: cs.MM

TL;DR: 研究探讨了情感表达中语音、面部和手势的协调动态，分析了非重叠和重叠对话对情绪表达的影响，并发现不同情绪状态下动作活跃度和预测准确性的差异。


<details>
  <summary>Details</summary>
Motivation: 深入了解情感表达中语音、面部和手势的协调机制，以提高实时情感检测的准确性。

Method: 通过IEMOCAP数据集的多人互动数据，使用区域运动捕捉技术分析了语音特征（如MFCCs）与3D面部及手部标记的位移关系。

Result: 非重叠对话下情绪表达更活跃，愤怒在重叠对话中抑制手势。预测模型中，音调和MFCCs在发音区域准确率最高，而唤醒度和效价相关性较低。

Conclusion: 情绪表达的多模态协调受对话结构影响，不同情绪状态下面部和手势的动态特征对情感检测有重要意义。

Abstract: Human emotional expression emerges through coordinated vocal, facial, and
gestural signals. While speech face alignment is well established, the broader
dynamics linking emotionally expressive speech to regional facial and hand
motion remains critical for gaining a deeper insight into how emotional and
behavior cues are communicated in real interactions. Further modulating the
coordination is the structure of conversational exchange like sequential turn
taking, which creates stable temporal windows for multimodal synchrony, and
simultaneous speech, often indicative of high arousal moments, disrupts this
alignment and impacts emotional clarity. Understanding these dynamics enhances
realtime emotion detection by improving the accuracy of timing and synchrony
across modalities in both human interactions and AI systems. This study
examines multimodal emotion coupling using region specific motion capture from
dyadic interactions in the IEMOCAP corpus. Speech features included low level
prosody, MFCCs, and model derived arousal, valence, and categorical emotions
(Happy, Sad, Angry, Neutral), aligned with 3D facial and hand marker
displacements. Expressive activeness was quantified through framewise
displacement magnitudes, and speech to gesture prediction mapped speech
features to facial and hand movements. Nonoverlapping speech consistently
elicited greater activeness particularly in the lower face and mouth. Sadness
showed increased expressivity during nonoverlap, while anger suppressed
gestures during overlaps. Predictive mapping revealed highest accuracy for
prosody and MFCCs in articulatory regions while arousal and valence had lower
and more context sensitive correlations. Notably, hand speech synchrony was
enhanced under low arousal and overlapping speech, but not for valence.

</details>


### [43] [WDMIR: Wavelet-Driven Multimodal Intent Recognition](https://arxiv.org/abs/2506.10011)
*Weiyin Gong,Kai Zhang,Yanghai Zhang,Qi Liu,Xinjie Sun,Junyu Lu,Linbo Zhu*

Main category: cs.MM

TL;DR: 本文提出了一种基于小波变换的多模态意图识别框架（WDMIR），通过频域分析增强对非语言信息的理解，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于依赖文本分析，忽略了视频和音频中的非语言语义信息，导致意图识别不够全面。

Method: 1. 小波驱动的融合模块，在频域同步分解和集成视频-音频特征；2. 跨模态交互机制，逐步从双模态提升到三模态整合。

Result: 在MIntRec数据集上表现最优，准确率提升1.13%；消融实验显示小波模块对非语言信息提取效果显著。

Conclusion: WDMIR框架通过频域分析和跨模态交互，有效填补了语言与非语言信息之间的语义鸿沟，提升了意图识别的准确性。

Abstract: Multimodal intent recognition (MIR) seeks to accurately interpret user
intentions by integrating verbal and non-verbal information across video, audio
and text modalities. While existing approaches prioritize text analysis, they
often overlook the rich semantic content embedded in non-verbal cues. This
paper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR)
framework that enhances intent understanding through frequency-domain analysis
of non-verbal information. To be more specific, we propose: (1) a
wavelet-driven fusion module that performs synchronized decomposition and
integration of video-audio features in the frequency domain, enabling
fine-grained analysis of temporal dynamics; (2) a cross-modal interaction
mechanism that facilitates progressive feature enhancement from bimodal to
trimodal integration, effectively bridging the semantic gap between verbal and
non-verbal information. Extensive experiments on MIntRec demonstrate that our
approach achieves state-of-the-art performance, surpassing previous methods by
1.13% on accuracy. Ablation studies further verify that the wavelet-driven
fusion module significantly improves the extraction of semantic information
from non-verbal sources, with a 0.41% increase in recognition accuracy when
analyzing subtle emotional cues.

</details>


### [44] [Thief of Truth: VR comics about the relationship between AI and humans](https://arxiv.org/abs/2506.10012)
*Joonhyung Bae*

Main category: cs.MM

TL;DR: 一篇探讨VR漫画扩展性的研究，通过VR视角控制和交互设计提升沉浸感与可访问性。


<details>
  <summary>Details</summary>
Motivation: 探索人类与AI的关系，并通过VR漫画的创新形式展现。

Method: 采用VR视角控制、交互设计提升沉浸感，并设计提升可访问性的方法。

Result: 提出了一种实验性的VR漫画尝试范例。

Conclusion: 该研究展示了VR漫画在沉浸感和可访问性上的潜力。

Abstract: Thief of Truth is a first-person perspective Virtual Reality (VR) comic that
explores the relationship between humans and artificial intelligence (AI). The
work tells the story of a mind-uploaded human being reborn as a new subject
while interacting with an AI that is looking for the meaning of life. In order
to experiment with the expandability of VR comics, the work was produced by
focusing on three problems. First, the comic is designed using the viewing
control effect of VR. Second, through VR controller-based interaction, the
player's immersion in the work is increased. Third, a method for increasing
accessibility to VR comics was devised. This work aims to present an example of
an experimental attempt in VR Comics.

</details>


### [45] [Immersive Fantasy Based on Digital Nostalgia: Environmental Narratives for the Korean Millennials and Gen Z](https://arxiv.org/abs/2506.10013)
*Yerin Doh,Joonhyung Bae*

Main category: cs.MM

TL;DR: 研究介绍了媒体艺术作品《亲爱的乘客，请戴上口罩》，通过多层次探讨COVID-19疫情中激增的一次性口罩浪费问题，结合数字化怀旧与航空旅行回忆，提出生态担忧。


<details>
  <summary>Details</summary>
Motivation: 探讨疫情中一次性口罩浪费的生态问题，并结合年轻一代的数字化怀旧与旅行记忆，提升环保意识。

Method: 通过点击式游戏和沉浸式展览，参与者穿梭于虚拟与现实领域，面对伦理与环境困境。

Result: 作品激发了共情和潜在行动，但资源使用和后续参与度问题仍需解决。

Conclusion: 该艺术项目有效唤起环保意识，但仍需优化资源利用和参与体验。

Abstract: This study introduces the media artwork Dear Passenger, Please Wear a Mask,
designed to offer a layered exploration of single-use mask waste, which
escalated during the COVID-19 pandemic. The piece reframes underappreciated
ecological concerns by interweaving digital nostalgia and airline travel
recollections of Millennials and Gen Z with a unique fantasy narrative. Via a
point-and-click game and an immersive exhibition, participants traverse both
virtual and real domains, facing ethical and environmental dilemmas. While it
fosters empathy and potential action, resource use and post-experience
engagement challenges persist.

</details>


### [46] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2506.10016)
*Longzhen Han,Awes Mubarak,Almas Baimagambetov,Nikolaos Polatidis,Thar Baker*

Main category: cs.MM

TL;DR: 多模态大语言模型（MLLMs）通过统一架构整合语言与其他感官模态，扩展了生成能力。本文调查了六种主要生成模态，并分析了关键技术如何支持跨模态能力。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型如何通过跨模态技术实现多样化输出，并总结其发展趋势与挑战。

Method: 综述了自监督学习、混合专家、人类反馈强化学习和思维链等技术在跨模态能力中的作用。

Result: 揭示了跨模态协同效应和关键模型架构趋势，同时提出了未解决的挑战。

Conclusion: 本文为MLLM发展提供了统一视角，并指出了实现更通用、自适应和可解释系统的关键路径。

Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text
generation, now spanning diverse output modalities including images, music,
video, human motion, and 3D objects, by integrating language with other sensory
modalities under unified architectures. This survey categorises six primary
generative modalities and examines how foundational techniques, namely
Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement
Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,
enable cross-modal capabilities. We analyze key models, architectural trends,
and emergent cross-modal synergies, while highlighting transferable techniques
and unresolved challenges. Architectural innovations like transformers and
diffusion models underpin this convergence, enabling cross-modal transfer and
modular specialization. We highlight emerging patterns of synergy, and identify
open challenges in evaluation, modularity, and structured reasoning. This
survey offers a unified perspective on MLLM development and identifies critical
paths toward more general-purpose, adaptive, and interpretable multimodal
systems.

</details>


### [47] [Can Sound Replace Vision in LLaVA With Token Substitution?](https://arxiv.org/abs/2506.10416)
*Ali Vosoughi,Jing Bi,Pinxin Liu,Yunlong Tang,Chenliang Xu*

Main category: cs.MM

TL;DR: SoundCLIP 探索了在 MLLM 中直接集成音频和视觉输入的方法，发现音频检索性能提升但文本生成质量下降，提出权衡方案。


<details>
  <summary>Details</summary>
Motivation: 传统多模态系统依赖文本对齐表示，限制了音频信息的利用。SoundCLIP 旨在直接集成音频和视觉输入，提升任务的音频理解能力。

Method: 1. 通过多层感知机将音频特征投影到 CLIP 视觉空间；2. 保留原始音频嵌入。实验使用五种音频编码器，并提出 WhisperCLIP 架构和新数据集 AVE-2。

Result: 音频检索性能显著提升（Top-1 准确率提高 44%），但文本生成质量下降。语言监督预训练的编码器（如 CLAP、Whisper）表现更好。

Conclusion: 发现跨模态对齐并非对所有任务都有益，需要在检索准确性和文本生成质量之间权衡。

Abstract: While multimodal systems have achieved impressive advances, they typically
rely on text-aligned representations rather than directly integrating audio and
visual inputs. This reliance can limit the use of acoustic information in tasks
requiring nuanced audio understanding. In response, SoundCLIP explores direct
audio-visual integration within multimodal large language models (MLLMs) by
substituting CLIP's visual tokens with audio representations and selecting
sound-relevant patch tokens in models such as LLaVA. We investigate two
configurations: (1) projecting audio features into CLIP's visual manifold via a
multilayer perceptron trained with InfoNCE on paired audio-video segments, and
(2) preserving raw audio embeddings with minimal dimensional adjustments.
Experiments with five state-of-the-art audio encoders reveal a fundamental
trade-off. While audio-to-video retrieval performance increases dramatically
(up to 44 percentage points in Top-1 accuracy) when audio is projected into
CLIP's space, text generation quality declines. Encoders pre-trained with text
supervision (CLAP, Whisper, ImageBind) maintain stronger generative
capabilities than those focused primarily on audiovisual alignment (Wav2CLIP,
AudioCLIP), highlighting the value of language exposure for generation tasks.
We introduce WhisperCLIP, an architecture that fuses intermediate
representations from Whisper, as well as AudioVisual Event Evaluation (AVE-2),
a dataset of 580,147 three-second audiovisual clips with fine-grained alignment
annotations. Our findings challenge the assumption that stronger cross-modal
alignment necessarily benefits all multimodal tasks; instead, a Pareto frontier
emerges wherein optimal performance depends on balancing retrieval accuracy
with text generation quality. Codes and datasets:
https://github.com/ali-vosoughi/SoundCLIP.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [48] [Growing a Modular Framework for Modal Systems- HOLMS: a HOL Light Library](https://arxiv.org/abs/2506.10048)
*Antonella Bilotta*

Main category: cs.LO

TL;DR: 这篇论文介绍了HOLMS项目，一个在HOL Light证明助手中用于模态推理的模块化框架，重点关注模态系统的充足性定理证明和自动化决策过程。


<details>
  <summary>Details</summary>
Motivation: 旨在为HOL Light提供一个模块化框架，支持模态逻辑的推理，并通过充足性定理建立语法证明系统与关系模型之间的形式连接。

Method: 开发了统一的模块化策略，直接在HOL Light中为多个模态系统（K, T, K4, GL）证明充足性定理，并集成了自动化决策和反模型构造工具。

Result: 展示了如何在通用证明助手中灵活且稳健地实现模态推理，同时扩展了HOLMS框架的功能和适用范围。

Conclusion: HOLMS框架的成功实现为模态逻辑的进一步研究和应用奠定了基础，并证明了其灵活性和实用性。

Abstract: The present dissertation introduces the research project on HOLMS
(\textbf{HOL} Light Library for \textbf{M}odal \textbf{S}ystems), a growing
modular framework for modal reasoning within the HOL Light proof assistant. To
provide an accessible introduction to the library, the fundamentals of modal
logic are outlined first, followed by a concise manual for the proof assistant
itself. The core contribution of this work on HOLMS is the development of a
unified and modular strategy for proving adequacy theorems with respect to
relational semantics directly within HOL Light for several normal modal
systems, currently including K, T, K4, and GL. Adequacy theorems establish a
formal connection between syntactic proof systems and their intended relational
models, ensuring that derivable statements align with valid ones. This approach
extends previous research on G\"odel-L\"ob logic (GL) by two HOLMS developers.
It also assesses the generality and compositionality of the completeness proofs
in George Boolos' monograph \textit{The logic of provability}. Beyond
theoretical contributions, HOLMS incorporates automated decision procedures and
a countermodel constructor for K, T, K4, and GL, illustrating how
general-purpose proof assistants can be effectively combined with research on
labelled sequent calculi and key insights from correspondence and bisimulation
theories. The implementation in HOL Light demonstrates the feasibility of
mechanising modal reasoning in a flexible and robust manner, paving the way for
further developments of the HOLMS framework.

</details>


### [49] [Notes on applicative matching logic](https://arxiv.org/abs/2506.10088)
*Laurentiu Leustean*

Main category: cs.LO

TL;DR: 这些讲义介绍了应用匹配逻辑（AML）的基本定义和结果，这是Xiaohong Chen和Grigore Roșu最近引入的一种功能变体的匹配逻辑，适合作为AML理论的入门文本。


<details>
  <summary>Details</summary>
Motivation: AML旨在提供一种逻辑框架，用于定义编程语言的正式语义，并用于规范和推理程序行为。

Method: 讲义基于Monk的数理逻辑教科书，系统地介绍了AML的基本定义和理论结果。

Result: 讲义可以作为AML理论的入门材料，帮助读者理解和应用AML。

Conclusion: AML是匹配逻辑的一种功能变体，具有理论和实用价值，适合用于编程语言的语义定义和程序行为分析。

Abstract: Matching logic (ML) was developed by Grigore Ro\c{s}u and collaborators as a
logic for defining the formal semantics of programming languages and for
specifying and reasoning about the behavior of programs. These lecture notes
present basic definitions and results on applicative matching logic (AML), a
functional variant of ML introduced recently by Xiaohong Chen and Grigore
Ro\c{s}u. They can be used as an introductory text in the theory of AML. Monk's
textbook on mathematical logic has an enormous influence on the notes.

</details>


### [50] [StepProof: Step-by-step verification of natural language mathematical proofs](https://arxiv.org/abs/2506.10558)
*Xiaolin Hu,Qinghua Zhou,Bogdan Grechuk,Ivan Y. Tyukin*

Main category: cs.LO

TL;DR: StepProof是一种新型的自动形式化方法，专注于分步验证自然语言证明，显著提高了验证效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 交互式定理证明器虽然在形式化验证数学证明方面强大，但缺乏自然语言接口。现有自动形式化方法仅支持完整证明验证，缺少句子级验证能力。

Method: StepProof将完整证明分解为多个可验证的子证明，实现句子级验证，并通过小规模人工调整自然语言证明优化性能。

Result: 实验结果表明，StepProof显著提高了验证成功率和效率。人工调整进一步提升了其性能。

Conclusion: StepProof填补了现有自动形式化方法的不足，为更细粒度的证明验证提供了有效工具。

Abstract: Interactive theorem provers (ITPs) are powerful tools for the formal
verification of mathematical proofs down to the axiom level. However, their
lack of a natural language interface remains a significant limitation. Recent
advancements in large language models (LLMs) have enhanced the understanding of
natural language inputs, paving the way for autoformalization - the process of
translating natural language proofs into formal proofs that can be verified.
Despite these advancements, existing autoformalization approaches are limited
to verifying complete proofs and lack the capability for finer, sentence-level
verification. To address this gap, we propose StepProof, a novel
autoformalization method designed for granular, step-by-step verification.
StepProof breaks down complete proofs into multiple verifiable subproofs,
enabling sentence-level verification. Experimental results demonstrate that
StepProof significantly improves proof success rates and efficiency compared to
traditional methods. Additionally, we found that minor manual adjustments to
the natural language proofs, tailoring them for step-level verification,
further enhanced StepProof's performance in autoformalization.

</details>


### [51] [Encoding call-by-push-value in the pi-calculus](https://arxiv.org/abs/2506.10584)
*Benjamin Bennetzen,Nikolaj Rossander Kristensen,Peter Buus Steffensen*

Main category: cs.LO

TL;DR: 该论文提出了一种将Levy的按名调用的值传递lambda演算（CBPV）编码到π演算的方法，并在π-演算中证明了其编码的完备性和正确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索CBPV和π演算之间的关系，并提供一种有效的编码方法，同时验证其正确性和完备性。

Method: 采用了π-i演算进行编码，避免了de Bruijn索引的形式化挑战，并通过手工证明验证了完备性和正确性。

Result: 证明了编码的完备性和正确性，并满足Gorla提出的五项编码标准。同时在Coq中开始形式化验证。

Conclusion: 该编码方法在理论和实践中均具有潜力，未来的工作可以进一步完善形式化验证。

Abstract: In this report we define an encoding of Levys call-by-push-value
lambda-calculus (CBPV) in the pi-calculus, and prove that our encoding is both
sound and complete. We present informal (by-hand) proofs of soundness,
completeness, and all required lemmas. The encoding is specialized to the
internal pi-calculus (pi-i-calculus) to circumvent certain challenges
associated with using de Bruijn index in a formalization, and it also helps
with bisimulation as early-, late- and open-bisimulation coincide in this
setting, furthermore bisimulation is a congruence. Additionally, we argue that
our encoding also satisfies the five criteria for good encodings proposed by
Gorla, as well as show similarities between Milners and our encoding. This
paper includes encodings from CBPV in the pi-i-calculus, asynchronous polyadic
pi-calculus and the local pi-calculus. We begin a formalization of the proof in
Coq for the soundness and completeness of the encoding in the pi-i-calculus.
Not all lemmas used in the formalization are themselves formally proven.
However, we argue that the non-proven lemmas are reasonable, as they are proven
by hand, or amount to Coq formalities that are straightforward given informal
arguments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [52] [Cybernetic Marionette: Channeling Collective Agency Through a Wearable Robot in a Live Dancer-Robot Duet](https://arxiv.org/abs/2506.10079)
*Anup Sathya,Jiasheng Li,Zeyu Yan,Adriane Fang,Bill Kules,Jonathan David Martin,Huaishu Peng*

Main category: cs.HC

TL;DR: DANCE^2是一种互动舞蹈表演，观众通过投票控制舞者身上的可穿戴机器人行为，尽管投票数据一致，但观众认为自己的选择影响了表演。


<details>
  <summary>Details</summary>
Motivation: 探索观众在表演中的集体代理感与行为之间的复杂关系，类比算法系统中代理感的体验与实际的差异。

Method: 通过实时投票让观众决定舞者与机器人的互动，结合后表演调查分析观众感知与实际行为。

Result: 观众认为自己的选择有意义，但投票数据却显示高度一致，揭示了代理感与行为的脱节。

Conclusion: 舞蹈编排和互动设计可以调解代理感与实际行为之间的关系，为算法系统提供现场类比。

Abstract: We describe DANCE^2, an interactive dance performance in which audience
members channel their collective agency into a dancer-robot duet by voting on
the behavior of a wearable robot affixed to the dancer's body. At key moments
during the performance, the audience is invited to either continue the
choreography or override it, shaping the unfolding interaction through
real-time collective input. While post-performance surveys revealed that
participants felt their choices meaningfully influenced the performance, voting
data across four public performances exhibited strikingly consistent patterns.
This tension between what audience members do, what they feel, and what
actually changes highlights a complex interplay between agentive behavior, the
experience of agency, and power. We reflect on how choreography, interaction
design, and the structure of the performance mediate this relationship,
offering a live analogy for algorithmically curated digital systems where
agency is felt, but not exercised.

</details>


### [53] [Mastery Learning Improves Performance on Complex Tasks on PCP Literacy Test](https://arxiv.org/abs/2506.10164)
*Chandana Srinivas,Elif E. Firat,Robert S. Laramee,Alark Joshi*

Main category: cs.HC

TL;DR: 探讨了如何通过修订版布鲁姆分类法和Mastery Learning教学法帮助学生学习平行坐标图（PCPs），研究发现Mastery Learning组在高级思维模块表现更佳。


<details>
  <summary>Details</summary>
Motivation: 学生在学习不熟悉的数据可视化技术（如平行坐标图）时面临挑战，需要有效的教学方法提升其理解能力。

Method: 采用修订版布鲁姆分类法和Mastery Learning教学法进行课堂干预，分两组对比教学效果。

Result: Mastery Learning组在高级思维模块（分析、评估）表现更好，对PCPs的理解更深入。

Conclusion: Mastery Learning结合修订版布鲁姆分类法能有效提升学生在高级数据可视化技术中的学习效果。

Abstract: Developing literacy with unfamiliar data visualization techniques such as
Parallel Coordinate Plots (PCPs) can be a significant challenge for students.
We adopted the Revised Bloom's taxonomy to instruct students on Parallel
Coordinate Plots (PCPs) using Mastery Learning in the classroom. To evaluate
Mastery Learning's impact, we conducted an intervention in a Data Visualization
course to teach students about PCPs using the Revised Bloom's taxonomy with and
without Mastery Learning. Based on our intervention, we found that while
students in both groups performed similarly on the first two (Remember,
Understand) modules, the students in the Mastery Learning group performed
better on modules that required more advanced thinking (Analyze, Evaluate) and
demonstrated a better comprehension of PCPs. We provide all the materials
developed including the six-module Bloom's Taxonomy PCP literacy (BTPL) test
for full reproducibility on our website at
https://vis-graphics.github.io/PCP-Literacy-Test/.

</details>


### [54] [Intergenerational AI Literacy in Korean Immigrant Families: Interpretive Gatekeeping Meets Convenient Critical Deferment](https://arxiv.org/abs/2506.10197)
*Jeongone Seo,Ryan Womack,Tawfiq Ammari*

Main category: cs.HC

TL;DR: 研究探讨了韩国移民家庭在美国如何使用AI工具（如ChatGPT和智能助手），揭示了两种关键实践：父母的‘解释性把关’和青少年的‘便利性批判延迟’，挑战了传统的AI素养模式，指出AI素养是动态且关系性的。


<details>
  <summary>Details</summary>
Motivation: 随着AI深度融入家庭生活，移民家庭面临独特的代际、语言和文化挑战，研究旨在探索他们如何应对这些挑战。

Method: 通过对20名家长和青少年进行半结构化访谈，研究分析了家庭中AI工具的使用实践。

Result: 发现两种主要实践：‘解释性把关’（父母通过文化和伦理价值观调解子女的AI使用）和‘便利性批判延迟’（青少年为即时学术和社交效用推迟批判性评估）。

Conclusion: 研究扩展了代际AI素养的概念，并提出了更公平、文化敏感的家庭中心化AI设计建议。

Abstract: As artificial intelligence (AI) becomes deeply integrated into family life,
immigrant families must navigate unique intergenerational, linguistic, and
cultural challenges. This study examines how Korean immigrant families in the
United States negotiate the use of AI tools such as ChatGPT and smart
assistants in their homes. Through 20 semi-structured interviews with parents
and teens, we identify two key practices that shape their engagement:
interpretive gatekeeping, where parents mediate their children's AI use through
a lens of cultural and ethical values, and convenient critical deferment, where
teens strategically postpone critical evaluation of AI for immediate academic
and social utility. These intertwined practices challenge conventional,
skills-based models of AI literacy, revealing it instead as a dynamic and
relational practice co-constructed through ongoing family negotiation. We
contribute to information science and HCI by offering a new conceptual
extension for intergenerational AI literacy and providing design implications
for more equitable, culturally attuned, and family-centered AI systems.

</details>


### [55] [Speculative Design in Spiraling Time: Methods and Indigenous HCI](https://arxiv.org/abs/2506.10229)
*James Eschrich,Cole McMullen,Sarah Sterman*

Main category: cs.HC

TL;DR: 探讨了推测性设计在土著人机交互中的适用性，指出了时间性假设的局限性，并提出了基于“螺旋时间”概念的替代方案。


<details>
  <summary>Details</summary>
Motivation: 研究推测性设计是否适用于土著人机交互，并解决其时间性假设带来的问题。

Method: 通过分析时间性假设的局限性，并提出基于“螺旋时间”的替代理解。

Result: 发现时间性假设可能削弱推测性设计的实用性，螺旋时间概念更适合土著人机交互。

Conclusion: 提出螺旋时间概念作为推测性设计的替代框架，以更好地服务于土著人机交互需求。

Abstract: In this position paper, we first discuss the uptake of speculative design as
a method for Indigenous HCI. Then, we outline how a key assumption about
temporality threatens to undermine the usefulness of speculative design in this
context. Finally, we briefly sketch out a possible alternative understanding of
speculative design, based on the concept of "spiraling time," which could be
better suited for Indigenous HCI.

</details>


### [56] [Extended Creativity: A Conceptual Framework for Understanding Human-AI Creative Relations](https://arxiv.org/abs/2506.10249)
*Andrea Gaggioli,Sabrina Bartolotta,Andrea Ubaldi,Katusha Gerardini,Eleonora Diletta Sarcinella,Alice Chirico*

Main category: cs.HC

TL;DR: AI可通过支持、协同和共生三种模式增强人类创造力，这些模式基于技术自主性和感知代理程度，影响从日常问题解决到范式转变的创新。


<details>
  <summary>Details</summary>
Motivation: 探索AI如何有效增强人类创造力。

Method: 从分布式创造力视角，提出AI支持、协同和共生三种模式，并分析其技术自主性和感知代理的影响。

Result: 不同模式对不同层次的创造力有独特影响，涉及理论、伦理和设计问题。

Conclusion: AI在创造力中的作用需综合考虑技术、感知和多层次影响，以实现最佳协同。

Abstract: Artificial Intelligence holds significant potential to enhance human
creativity. However, achieving this vision requires a clearer understanding of
how such enhancement can be effectively realized. Adopting the perspective of
distributed creativity, we identify three primary modes through which AI can
contribute to creative processes: Support, where AI acts as a tool; Synergy,
where AI and humans collaborate in complementary ways; and Symbiosis, where
human and AI cognition become so integrated that they form a unified creative
system. These modes are defined along two key dimensions: the level of
technical autonomy exhibited by the AI system and the degree of perceived
agency attributed to it. We examine how each configuration influences different
levels of creativity - from everyday problem-solving to paradigm-shifting
innovation - and discuss the theoretical, ethical, and design implications.

</details>


### [57] [Beyond Compliance: A User-Autonomy Framework for Inclusive and Customizable Web Accessibility](https://arxiv.org/abs/2506.10324)
*Lalitha A R*

Main category: cs.HC

TL;DR: 论文提出从合规驱动的网页可访问性转向以用户为中心的关怀驱动模型，强调用户自主权，以神经多样性用户为例展示个性化需求。


<details>
  <summary>Details</summary>
Motivation: 当前的可访问性标准常被静态化为合规清单，缺乏灵活性；研究旨在通过关怀驱动模型提升用户体验和自主性。

Method: 引入可定制的Comfort Mode框架，支持用户根据需求调整界面设置（如对比度、排版等），保持品牌视觉一致性。

Result: 提出了最小化和高级两种实现模型，展示低成本无缝集成包容性设计，提升数字包容性。

Conclusion: 该框架灵活、可扩展，实现了用户自主权、美学完整性和可访问性的统一，为数字包容性提供了新范式。

Abstract: This paper proposes a shift from compliance-centered web accessibility to a
care-driven model that prioritizes user autonomy, using neurodivergent users as
a catalyst case for broader personalization needs. While accessibility
standards offer a flexible framework, they are often interpreted and
implemented as static compliance checklists, our approach reframes it as a
flexible, user-centered process. We introduce a customizable Comfort Mode
framework that allows users to adapt interface settings, such as contrast,
typography, motion, and scaling, according to their individual needs, while
retaining the brand's core visual identity. Grounded in psychological and
cognitive accessibility principles, our design supports personalization without
sacrificing creative freedom. We present both minimal and advanced
implementation models with mock-ups, demonstrating how inclusive design can be
seamlessly integrated at minimal cost. This approach aims to broaden digital
inclusivity by offering autonomy to those who require it, without imposing
changes on those who do not. The proposed system is adaptable, scalable, and
suitable for a wide range of users and brands, offering a new paradigm where
user autonomy, aesthetic integrity, and accessibility converge not through
compromise, but through choice.

</details>


### [58] [IDEA: Augmenting Design Intelligence through Design Space Exploration](https://arxiv.org/abs/2506.10587)
*Chuer Chen,Xiaoke Yan,Xiaoyu Qi,Nan Cao*

Main category: cs.HC

TL;DR: 提出了一种名为IDEA的决策框架，通过结构化表示设计空间，利用LLM生成约束，并结合MCTS算法高效探索设计空间。该框架在数据驱动的文章撰写和可视化生成中验证了其有效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前设计空间的决策高度依赖设计师经验，缺乏数学形式化支持自动化设计。

Method: 引入结构化表示建模设计空间，利用LLM生成约束，结合MCTS算法探索设计空间，并实例化为具体实现。

Result: 在文章撰写和可视化生成中验证了IDEA的跨领域适应性和高效设计能力。

Conclusion: IDEA框架能够有效提升设计智能，生成更优的设计成果。

Abstract: Design spaces serve as a conceptual framework that enables designers to
explore feasible solutions through the selection and combination of design
elements. However, effective decision-making remains heavily dependent on the
designer's experience, and the absence of mathematical formalization prevents
computational support for automated design processes. To bridge this gap, we
introduce a structured representation that models design spaces with orthogonal
dimensions and discrete selectable elements. Building on this model, we present
IDEA, a decision-making framework for augmenting design intelligence through
design space exploration to generate effective outcomes. Specifically, IDEA
leverages large language models (LLMs) for constraint generation, incorporates
a Monte Carlo Tree Search (MCTS) algorithm guided by these constraints to
explore the design space efficiently, and instantiates abstract decisions into
domain-specific implementations. We validate IDEA in two design scenarios:
data-driven article composition and pictorial visualization generation,
supported by example results, expert interviews, and a user study. The
evaluation demonstrates the IDEA's adaptability across domains and its
capability to produce superior design outcomes.

</details>


### [59] [Accessible Design in Integrated Development Environments: A Think Aloud Study Exploring the Experiences of Students with ADHD](https://arxiv.org/abs/2506.10598)
*Luke Halpin,Phillip Benachour,Tracy Hall,Ann-Marie Houghton,Emily Winter*

Main category: cs.HC

TL;DR: 该研究探讨了集成开发环境（IDE）界面对注意力缺陷多动障碍（ADHD）学生的学习影响，通过访谈和观察分析，揭示了当前设计中的问题并提出改进需求。


<details>
  <summary>Details</summary>
Motivation: 探讨IDE界面对ADHD学生的影响，填补了该领域的研究空白，旨在改善这些学生的学习体验。

Method: 采用“有声思维”研究和定性观察访谈，分析九名学生在Visual Studio Code中的学习和互动体验，并通过主题分析整理数据。

Result: 识别出三个主题（自信心、互动和学习）及其子主题，揭示了当前IDE设计对学生造成的挫败感和障碍。

Conclusion: 需改进IDE的可用性和可访问性设计，以支持ADHD学生，研究提供了宝贵的用户体验见解。

Abstract: Coding forms a key part of computer science education in universities. As
part of this education, Integrated Development Environments (IDEs) are
essential tools for coding. However, it is currently unknown how the design of
an IDE's interface impacts on students with Attention Deficit Hyperactivity
Disorder (ADHD).
  In this study we investigated the use of IDEs by students with ADHD. We
conducted a think aloud study with nine university computing students, followed
by qualitative observational interviews to analyse their learning and
engagement with the Visual Studio Code IDE. The paper reports on these
experiences and seeks to understand the role IDEs play in the educational
setting.
  Our work also examines how digital accessibility and usability are considered
in the current design of IDEs. We analysed the qualitative data using a
thematic analysis and identified three primary themes: self-confidence,
interaction, and learning as well as various sub-themes.
  The themes and their sub-themes illustrate key areas of consideration when
designing IDEs for students with ADHD. The primary findings highlight
experiences of frustration and barriers in the current design and layout of
IDEs.
  Through our participatory approach we provide a rare insight into ADHD user
experiences around usability and accessibility, and describe the need for
better design of development environments to ensure a positive learning
experience for the students.

</details>


### [60] [Integrating Large Language Models into Text Animation: An Intelligent Editing System with Inline and Chat Interaction](https://arxiv.org/abs/2506.10762)
*Bao Zhang,Zihan Li,Zhenglei Liu,Huanchen Wang,Yuxin Ma*

Main category: cs.HC

TL;DR: 本文提出了一种基于大型语言模型（LLM）的文本动画编辑系统，旨在降低非专业人士的使用门槛，提升创意生产效率。


<details>
  <summary>Details</summary>
Motivation: 传统动画工作流程对非专业人士存在操作复杂、效率低下的问题，亟需一种更易用的解决方案。

Method: 采用基于代理的双流管道，结合上下文内联建议和对话引导，并通过语义-动画映射实现LLM驱动的创意意图转换。系统还支持同步预览和统一参数调整。

Result: 用户研究表明，该系统能有效帮助非专业人士完成动画工作流程，验证了管道的可行性。

Conclusion: 研究鼓励进一步探索将LLM集成到更广泛的视频创作工作流程中。

Abstract: Text animation, a foundational element in video creation, enables efficient
and cost-effective communication, thriving in advertisements, journalism, and
social media. However, traditional animation workflows present significant
usability barriers for non-professionals, with intricate operational procedures
severely hindering creative productivity. To address this, we propose a Large
Language Model (LLM)-aided text animation editing system that enables real-time
intent tracking and flexible editing. The system introduces an agent-based
dual-stream pipeline that integrates context-aware inline suggestions and
conversational guidance as well as employs a semantic-animation mapping to
facilitate LLM-driven creative intent translation. Besides, the system supports
synchronized text-animation previews and parametric adjustments via unified
controls to improve editing workflow. A user study evaluates the system,
highlighting its ability to help non-professional users complete animation
workflows while validating the pipeline. The findings encourage further
exploration of integrating LLMs into a comprehensive video creation workflow.

</details>


### [61] [Grasp Prediction based on Local Finger Motion Dynamics](https://arxiv.org/abs/2506.10818)
*Dimitar Valkov,Pascal Kockwelp,Florian Daiber,Antonio Krüger*

Main category: cs.HC

TL;DR: 利用手部运动学实时预测用户意图抓取的物体，实验表明LSTM网络能高精度预测抓取时间和距离。


<details>
  <summary>Details</summary>
Motivation: 预测用户意图抓取的物体可为交互环境提供上下文信息，帮助缓解点对点延迟的影响。

Method: 通过记录16名参与者在抓取真实和合成物体时的手部运动数据，使用LSTM网络进行实时识别。

Result: LSTM网络能精准预测抓取时间（误差<21毫秒）和距离（误差<1厘米），目标大小预测准确率>97%。

Conclusion: 该方法对自适应和细粒度交互用户界面的设计具有重要启示，适用于普适和混合现实环境。

Abstract: The ability to predict the object the user intends to grasp offers essential
contextual information and may help to leverage the effects of point-to-point
latency in interactive environments. This paper explores the feasibility and
accuracy of real-time recognition of uninstrumented objects based on hand
kinematics during reach-to-grasp actions. In a data collection study, we
recorded the hand motions of 16 participants while reaching out to grasp and
then moving real and synthetic objects. Our results demonstrate that even a
simple LSTM network can predict the time point at which the user grasps an
object with a precision better than 21 ms and the current distance to this
object with a precision better than 1 cm. The target's size can be determined
in advance with an accuracy better than 97%. Our results have implications for
designing adaptive and fine-grained interactive user interfaces in ubiquitous
and mixed-reality environments.

</details>


### [62] [(De)composing Craft: An Elementary Grammar for Sharing Expertise in Craft Workflows](https://arxiv.org/abs/2506.10891)
*Ritik Batra,Lydia Kim,Ilan Mandel,Amritansh Kwatra,Jane L. E.,Steven J. Jackson,Thijs Roumen*

Main category: cs.HC

TL;DR: 提出了一种记录手工艺实践中即兴行为的基本语法，并通过CraftLink工具验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统手工艺记录的局限性在于忽视了实践中的隐性知识，阻碍了知识的共享与传承。

Method: 结合HCI、CSCW和社会科学的文献与专家访谈，开发记录即兴行为的语法，并通过CraftLink接口分析专家视频。

Result: 用户研究表明，该语法能有效捕捉并分享专业知识，支持社区内的知识协作存档。

Conclusion: 为计算系统支持手工艺社区的协作知识存档提供了新途径。

Abstract: Craft practices rely on evolving archives of skill and knowledge, developed
through generations of craftspeople experimenting with designs, materials, and
techniques. Better documentation of these practices enables the sharing of
knowledge and expertise between sites and generations. However, most
documentation focuses solely on the linear steps leading to final artifacts,
neglecting the tacit knowledge necessary to improvise, or adapt workflows to
meet the unique demands of each craft project. This omission limits knowledge
sharing and reduces craft to a mechanical endeavor, rather than a sophisticated
way of seeing, thinking, and doing. Drawing on expert interviews and literature
from HCI, CSCW and the social sciences, we develop an elementary grammar to
document improvisational actions of real-world craft practices. We demonstrate
the utility of this grammar with an interface called CraftLink that can be used
to analyze expert videos and semi-automatically generate documentation to
convey material and contextual variations of craft practices. Our user study
with expert crocheters (N=7) using this interface evaluates our grammar's
effectiveness in capturing and sharing expert knowledge with other
craftspeople, offering new pathways for computational systems to support
collaborative archives of knowledge and practice within communities.

</details>


### [63] [The Role of Generative AI in Facilitating Social Interactions: A Scoping Review](https://arxiv.org/abs/2506.10927)
*T. T. J. E. Arets,G. Perugia,M. Houben,W. A. IJsselsteijn*

Main category: cs.HC

TL;DR: 这篇论文综述了生成式AI（GAI）技术如何影响社交互动，分析了30项研究，总结了应用趋势和设计方法，并呼吁更多关注公平设计实践。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI技术对人类社交互动的影响，填补当前研究空白。

Method: 通过分析2020年以来的30项研究，采用范围综述方法。

Result: 识别了多种应用领域，如故事讲述、情感技能训练等，并强调参与式设计方法。

Conclusion: GAI有潜力支持动态个性化互动，但需更多关注公平设计和包容性评估策略。

Abstract: Reduced social connectedness increasingly poses a threat to mental health,
life expectancy, and general well-being. Generative AI (GAI) technologies, such
as large language models (LLMs) and image generation tools, are increasingly
integrated into applications aimed at enhancing human social experiences.
Despite their growing presence, little is known about how these technologies
influence social interactions. This scoping review investigates how GAI-based
applications are currently designed to facilitate social interaction, what
forms of social engagement they target, and which design and evaluation
methodologies designers use to create and evaluate them. Through an analysis of
30 studies published since 2020, we identify key trends in application domains
including storytelling, socio-emotional skills training, reminiscence,
collaborative learning, music making, and general conversation. We highlight
the role of participatory and co-design approaches in fostering both effective
technology use and social engagement, while also examining socio-ethical
concerns such as cultural bias and accessibility. This review underscores the
potential of GAI to support dynamic and personalized interactions, but calls
for greater attention to equitable design practices and inclusive evaluation
strategies.

</details>


### [64] [Video-Mediated Emotion Disclosure: A Study of Mental Health Vlogging by People with Schizophrenia on YouTube](https://arxiv.org/abs/2506.10932)
*Jiaying Lizzy Liu,Yan Zhang*

Main category: cs.HC

TL;DR: 研究分析了精神分裂症患者通过视频博客表达情感的多样方式，发现视觉元素对观众回应有积极影响。


<details>
  <summary>Details</summary>
Motivation: 填补关于精神分裂症患者在视频博客中如何构建情感叙事的研究空白。

Method: 分析200个由精神分裂症患者制作的YouTube视频，基于媒体研究和自我呈现理论构建视觉分析框架。

Result: 揭示了通过语言和视觉渠道表达情感的多样化实践，视觉元素增强了观众的支持性回应。

Conclusion: 未来需开展大规模定量研究，探讨视觉特征如何影响社交媒体上的视频交流，以优化支持患者表达的平台设计。

Abstract: Individuals with schizophrenia frequently experience intense emotions and
often turn to vlogging as a medium for emotional expression. While previous
research has predominantly focused on text based disclosure, little is known
about how individuals construct narratives around emotions and emotional
experiences in video blogs. Our study addresses this gap by analyzing 200
YouTube videos created by individuals with schizophrenia. Drawing on media
research and self presentation theories, we developed a visual analysis
framework to disentangle these videos. Our analysis revealed diverse practices
of emotion disclosure through both verbal and visual channels, highlighting the
dynamic interplay between these modes of expression. We found that the
deliberate construction of visual elements, including environmental settings
and specific aesthetic choices, appears to foster more supportive and engaged
viewer responses. These findings underscore the need for future large scale
quantitative research examining how visual features shape video mediated
communication on social media platforms. Such investigations would inform the
development of care centered video sharing platforms that better support
individuals managing illness experiences.

</details>


### [65] [Instance-Based Transfer Learning with Similarity-Aware Subject Selection for Cross-Subject SSVEP-Based BCIs](https://arxiv.org/abs/2506.10933)
*Ziwen Wang,Yue Zhang,Zhiqiang Zhang,Sheng Quan Xie,Alexander Lanzon,William P. Heath,Zhenhong Li*

Main category: cs.HC

TL;DR: 提出了一种名为iTRCA的新迁移学习框架，通过结合源受试者的通用特征和目标受试者的特定特征，有效降低SSVEP-BCI的数据需求。进一步改进的SS-iTRCA通过相似性选择源受试者以提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决SSVEP-BCI中因个体差异导致的迁移学习效果不佳问题，减少目标受试者所需训练数据。

Method: iTRCA提取受试者通用特征和特定特征；SS-iTRCA引入相似性筛选策略以优化源受试者选择。

Result: 在多个数据集上验证了iTRCA和SS-iTRCA的有效性，显著提升了性能。

Conclusion: 该研究为开发高性能SSVEP-BCI提供了可行方案，并减少了数据依赖性。

Abstract: Steady-state visual evoked potential (SSVEP)-based brain-computer interfaces
(BCIs) can achieve high recognition accuracy with sufficient training data.
Transfer learning presents a promising solution to alleviate data requirements
for the target subject by leveraging data from source subjects; however,
effectively addressing individual variability among both target and source
subjects remains a challenge. This paper proposes a novel transfer learning
framework, termed instance-based task-related component analysis (iTRCA), which
leverages knowledge from source subjects while considering their individual
contributions. iTRCA extracts two types of features: (1) the subject-general
feature, capturing shared information between source and target subjects in a
common latent space, and (2) the subject-specific feature, preserving the
unique characteristics of the target subject. To mitigate negative transfer, we
further design an enhanced framework, subject selection-based iTRCA (SS-iTRCA),
which integrates a similarity-based subject selection strategy to identify
appropriate source subjects for transfer based on their task-related components
(TRCs). Comparative evaluations on the Benchmark, BETA, and a self-collected
dataset demonstrate the effectiveness of the proposed iTRCA and SS-iTRCA
frameworks. This study provides a potential solution for developing
high-performance SSVEP-based BCIs with reduced target subject data.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [66] [Learning-based density-equalizing map](https://arxiv.org/abs/2506.10027)
*Yanwen Huang,Lok Ming Lui,Gary P. T. Choi*

Main category: cs.GR

TL;DR: 作者提出了一种基于深度学习的新框架（LDEM），用于密度均衡映射，克服了传统方法在精度、重叠问题和2D到3D扩展上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统密度均衡映射方法在精度、极端情况下的重叠问题以及从2D扩展到3D时存在挑战，需要一种更灵活和鲁棒的新方法。

Method: 使用深度学习框架，引入一种损失函数以确保密度均匀性和几何规则性，并采用分层方法预测粗粒度和密集级别的变换。

Result: LDEM在密度均衡性和双射性方面优于现有方法，且能无缝扩展到3D领域，适用于表面重新网格化等应用。

Conclusion: LDEM为密度均衡映射提供了一种可扩展且鲁棒的计算方法，为实际应用开辟了新可能性。

Abstract: Density-equalizing map (DEM) serves as a powerful technique for creating
shape deformations with the area changes reflecting an underlying density
function. In recent decades, DEM has found widespread applications in fields
such as data visualization, geometry processing, and medical imaging.
Traditional approaches to DEM primarily rely on iterative numerical solvers for
diffusion equations or optimization-based methods that minimize handcrafted
energy functionals. However, these conventional techniques often face several
challenges: they may suffer from limited accuracy, produce overlapping
artifacts in extreme cases, and require substantial algorithmic redesign when
extended from 2D to 3D, due to the derivative-dependent nature of their energy
formulations. In this work, we propose a novel learning-based
density-equalizing mapping framework (LDEM) using deep neural networks.
Specifically, we introduce a loss function that enforces density uniformity and
geometric regularity, and utilize a hierarchical approach to predict the
transformations at both the coarse and dense levels. Our method demonstrates
superior density-equalizing and bijectivity properties compared to prior
methods for a wide range of simple and complex density distributions, and can
be easily applied to surface remeshing with different effects. Also, it
generalizes seamlessly from 2D to 3D domains without structural changes to the
model architecture or loss formulation. Altogether, our work opens up new
possibilities for scalable and robust computation of density-equalizing maps
for practical applications.

</details>


### [67] [FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training](https://arxiv.org/abs/2506.10035)
*Fuhan Cai,Yong Guo,Jie Li,Wenbo Li,Xiangzhong Fang,Jian Chen*

Main category: cs.GR

TL;DR: FastFLUX是一种架构级剪枝框架，旨在提升FLUX推理效率，通过BRLL方法替换复杂结构块为轻量线性层，并引入Sandwich Training进行局部微调，显著提升速度且保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型（如DiTs）参数量大，导致推理慢、内存消耗高、部署困难，现有加速方法性能下降明显且训练成本高。

Method: 提出FastFLUX框架，采用BRLL方法替换复杂残差分支为线性层，保留原始捷径连接；引入Sandwich Training局部微调策略，使用LoRA监督邻近块。

Result: 实验表明，FastFLUX在剪枝20%层级后仍保持高质量图像生成，同时显著提升推理速度。

Conclusion: FastFLUX有效解决了DiTs推理效率问题，平衡了性能与速度，具有实际应用潜力。

Abstract: Recent advancements in text-to-image (T2I) generation have led to the
emergence of highly expressive models such as diffusion transformers (DiTs),
exemplified by FLUX. However, their massive parameter sizes lead to slow
inference, high memory usage, and poor deployability. Existing acceleration
methods (e.g., single-step distillation and attention pruning) often suffer
from significant performance degradation and incur substantial training costs.
To address these limitations, we propose FastFLUX, an architecture-level
pruning framework designed to enhance the inference efficiency of FLUX. At its
core is the Block-wise Replacement with Linear Layers (BRLL) method, which
replaces structurally complex residual branches in ResBlocks with lightweight
linear layers while preserving the original shortcut connections for stability.
Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning
strategy that leverages LoRA to supervise neighboring blocks, mitigating
performance drops caused by structural replacement. Experiments show that our
FastFLUX maintains high image quality under both qualitative and quantitative
evaluations, while significantly improving inference speed, even with 20\% of
the hierarchy pruned. Our code will be available soon.

</details>


### [68] [Token Perturbation Guidance for Diffusion Models](https://arxiv.org/abs/2506.10036)
*Javad Rajabi,Soroush Mehraban,Seyedmorteza Sadat,Babak Taati*

Main category: cs.GR

TL;DR: TPG是一种无需训练、适用于条件和非条件生成的通用引导方法，通过扰动中间令牌表示提升生成质量。


<details>
  <summary>Details</summary>
Motivation: CFG需要特定训练且局限于条件生成，TPG旨在解决这些限制。

Method: TPG通过规范保留的扰动矩阵直接作用于扩散网络中的中间令牌表示。

Result: 在SDXL和Stable Diffusion 2.1上，TPG在非条件生成中FID提升近2倍，同时保持与CFG相似的提示对齐效果。

Conclusion: TPG是一种通用且条件无关的引导方法，为更广泛的扩散模型带来CFG类似的优势。

Abstract: Classifier-free guidance (CFG) has become an essential component of modern
diffusion models to enhance both generation quality and alignment with input
conditions. However, CFG requires specific training procedures and is limited
to conditional generation. To address these limitations, we propose Token
Perturbation Guidance (TPG), a novel method that applies perturbation matrices
directly to intermediate token representations within the diffusion network.
TPG employs a norm-preserving shuffling operation to provide effective and
stable guidance signals that improve generation quality without architectural
changes. As a result, TPG is training-free and agnostic to input conditions,
making it readily applicable to both conditional and unconditional generation.
We further analyze the guidance term provided by TPG and show that its effect
on sampling more closely resembles CFG compared to existing training-free
guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1
show that TPG achieves nearly a 2$\times$ improvement in FID for unconditional
generation over the SDXL baseline, while closely matching CFG in prompt
alignment. These results establish TPG as a general, condition-agnostic
guidance method that brings CFG-like benefits to a broader class of diffusion
models. The code is available at
https://github.com/TaatiTeam/Token-Perturbation-Guidance

</details>


### [69] [Ambient Diffusion Omni: Training Good Models with Bad Data](https://arxiv.org/abs/2506.10038)
*Giannis Daras,Adrian Rodriguez-Munoz,Adam Klivans,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.GR

TL;DR: 该论文展示了如何利用低质量、合成和分布外图像提升扩散模型的性能，提出了Ambient Diffusion Omni框架，通过利用自然图像的光谱功率定律衰减和局部性特性，显著提高了图像质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖高质量、经过筛选的数据集，但大量低质量图像被忽视。论文旨在挖掘这些被丢弃图像的价值，设计了一种能够从所有可用图像中提取信号的训练框架。

Method: 提出Ambient Diffusion Omni框架，利用自然图像的光谱功率定律衰减和局部性特性，在训练中从模糊、压缩和运动伪影等合成损坏图像中提取信号。

Result: 该框架在ImageNet FID上达到最佳水平，并在文本到图像生成任务中显著提升图像质量和多样性。

Conclusion: 通过噪声平衡初始偏差与有限无偏数据之间的权衡，该框架为扩散模型训练提供了新思路，实现了更好的性能。

Abstract: We show how to use low-quality, synthetic, and out-of-distribution images to
improve the quality of a diffusion model. Typically, diffusion models are
trained on curated datasets that emerge from highly filtered data pools from
the Web and other sources. We show that there is immense value in the
lower-quality images that are often discarded. We present Ambient Diffusion
Omni, a simple, principled framework to train diffusion models that can extract
signal from all available images during training. Our framework exploits two
properties of natural images -- spectral power law decay and locality. We first
validate our framework by successfully training diffusion models with images
synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We
then use our framework to achieve state-of-the-art ImageNet FID, and we show
significant improvements in both image quality and diversity for text-to-image
generative modeling. The core insight is that noise dampens the initial skew
between the desired high-quality distribution and the mixed distribution we
actually observe. We provide rigorous theoretical justification for our
approach by analyzing the trade-off between learning from biased data versus
limited unbiased data across diffusion times.

</details>


### [70] [Low-Barrier Dataset Collection with Real Human Body for Interactive Per-Garment Virtual Try-On](https://arxiv.org/abs/2506.10468)
*Zaiqiang Wu,Yechen Li,Jingyuan Liu,Yuki Shibata,Takayuki Hori,I-Chao Shen,Takeo Igarashi*

Main category: cs.GR

TL;DR: 提出一种低成本的人体虚拟试衣方法，解决了传统方法依赖昂贵机器人模型和服装与人体对齐不准的问题。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试衣方法受限于前视图和实时性，且依赖昂贵设备和机器人模型，无法准确模拟人体变形，服装与人体对齐不佳。

Method: 采用真实人体收集服装数据集，引入混合人物表示方法，结合简化DensePose图提升服装与人体对齐效果。

Result: 实验表明，该方法在图像质量和时间一致性上优于现有技术，用户研究证实其对服装购买决策有帮助。

Conclusion: 该方法降低了虚拟试衣的门槛，提升了服装合成的真实性和实用性。

Abstract: Existing image-based virtual try-on methods are often limited to the front
view and lack real-time performance. While per-garment virtual try-on methods
have tackled these issues by capturing per-garment datasets and training
per-garment neural networks, they still encounter practical limitations: (1)
the robotic mannequin used to capture per-garment datasets is prohibitively
expensive for widespread adoption and fails to accurately replicate natural
human body deformation; (2) the synthesized garments often misalign with the
human body. To address these challenges, we propose a low-barrier approach for
collecting per-garment datasets using real human bodies, eliminating the
necessity for a customized robotic mannequin. We also introduce a hybrid person
representation that enhances the existing intermediate representation with a
simplified DensePose map. This ensures accurate alignment of synthesized
garment images with the human body and enables human-garment interaction
without the need for customized wearable devices. We performed qualitative and
quantitative evaluations against other state-of-the-art image-based virtual
try-on methods and conducted ablation studies to demonstrate the superiority of
our method regarding image quality and temporal consistency. Finally, our user
study results indicated that most participants found our virtual try-on system
helpful for making garment purchasing decisions.

</details>


### [71] [Edit360: 2D Image Edits to 3D Assets from Any Angle](https://arxiv.org/abs/2506.10507)
*Junchao Huang,Xinting Hu,Zhuotao Tian,Shaoshuai Shi,Li Jiang*

Main category: cs.GR

TL;DR: Edit360是一个无需调整的框架，将2D图像编辑扩展到多视角一致的3D编辑，通过视频扩散模型实现任意视角的用户定制编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常限制在预定视角进行3D编辑，缺乏灵活性和实用性，难以满足多视角一致性的精细编辑需求。

Method: 基于视频扩散模型，Edit360采用锚点视角编辑传播机制，在潜在空间和注意力空间中对齐并融合多视角信息。

Result: 框架实现了高质量3D内容的重建，支持用户从任意视角进行定制化编辑，同时确保多视角一致性。

Conclusion: Edit360为3D资产编辑提供了灵活且高效的新方法，扩展了扩散模型在3D领域的应用潜力。

Abstract: Recent advances in diffusion models have significantly improved image
generation and editing, but extending these capabilities to 3D assets remains
challenging, especially for fine-grained edits that require multi-view
consistency. Existing methods typically restrict editing to predetermined
viewing angles, severely limiting their flexibility and practical applications.
We introduce Edit360, a tuning-free framework that extends 2D modifications to
multi-view consistent 3D editing. Built upon video diffusion models, Edit360
enables user-specific editing from arbitrary viewpoints while ensuring
structural coherence across all views. The framework selects anchor views for
2D modifications and propagates edits across the entire 360-degree range. To
achieve this, Edit360 introduces a novel Anchor-View Editing Propagation
mechanism, which effectively aligns and merges multi-view information within
the latent and attention spaces of diffusion models. The resulting edited
multi-view sequences facilitate the reconstruction of high-quality 3D assets,
enabling customizable 3D content creation.

</details>


### [72] [Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture](https://arxiv.org/abs/2506.10580)
*Chengxu Zuo,Jiawei Huang,Xiao Jiang,Yuan Yao,Xiangren Shi,Rui Cao,Xinyu Yi,Feng Xu,Shihui Guo,Yipeng Qin*

Main category: cs.GR

TL;DR: 提出一种新颖的动态校准方法，打破IMU校准的绝对静态假设限制，显著扩大应用场景。


<details>
  <summary>Details</summary>
Motivation: 解决传统IMU校准中绝对静态假设的限制，使其适用于更广泛的应用场景。

Method: 通过两个松弛假设实时估计RG'G和RBS，使用Transformer模型学习映射关系，并设计校准触发器。

Result: 首次实现隐式IMU校准和长期高精度运动捕捉。

Conclusion: 该方法显著提升了稀疏IMU系统的实用性和适应性。

Abstract: In this paper, we propose a novel dynamic calibration method for sparse
inertial motion capture systems, which is the first to break the restrictive
absolute static assumption in IMU calibration, i.e., the coordinate drift RG'G
and measurement offset RBS remain constant during the entire motion, thereby
significantly expanding their application scenarios. Specifically, we achieve
real-time estimation of RG'G and RBS under two relaxed assumptions: i) the
matrices change negligibly in a short time window; ii) the human movements/IMU
readings are diverse in such a time window. Intuitively, the first assumption
reduces the number of candidate matrices, and the second assumption provides
diverse constraints, which greatly reduces the solution space and allows for
accurate estimation of RG'G and RBS from a short history of IMU readings in
real time. To achieve this, we created synthetic datasets of paired RG'G, RBS
matrices and IMU readings, and learned their mappings using a Transformer-based
model. We also designed a calibration trigger based on the diversity of IMU
readings to ensure that assumption ii) is met before applying our method. To
our knowledge, we are the first to achieve implicit IMU calibration (i.e.,
seamlessly putting IMUs into use without the need for an explicit calibration
process), as well as the first to enable long-term and accurate motion capture
using sparse IMUs. The code and dataset are available at
https://github.com/ZuoCX1996/TIC.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [73] [Is Sparse Matrix Reordering Effective for Sparse Matrix-Vector Multiplication?](https://arxiv.org/abs/2506.10356)
*Omid Asudeh,Sina Mahdipour Saravani,Gerald Sabin,Fabrice Rastello,P Sadayappan*

Main category: cs.DC

TL;DR: 评估稀疏矩阵重排序在不同多核CPU平台上对稀疏矩阵-向量乘性能的影响，发现重排序通过优化非零元素模式提升性能并改善负载均衡。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏矩阵重排序对性能的提升效果，特别是跨不同CPU平台的适用性和稳定性。

Method: 比较不同重排序策略对性能的影响，重点关注顺序和并行执行时的表现，并分析测量方法和负载不均衡的影响。

Result: 重排序能显著提升性能，但效果因策略和CPU平台而异，并行执行时负载均衡尤为关键。

Conclusion: 稀疏矩阵重排序是一种有效的性能优化手段，但需根据具体硬件和执行模式选择合适的策略。

Abstract: This work evaluates the impact of sparse matrix reordering on the performance
of sparse matrix-vector multiplication across different multicore CPU
platforms. Reordering can significantly enhance performance by optimizing the
non-zero element patterns to reduce total data movement and improve the
load-balancing. We examine how these gains vary over different CPUs for
different reordering strategies, focusing on both sequential and parallel
execution. We address multiple aspects, including appropriate measurement
methodology, comparison across different kinds of reordering strategies,
consistency across machines, and impact of load imbalance.

</details>


### [74] [Resilience through Automated Adaptive Configuration for Distribution and Replication](https://arxiv.org/abs/2506.10248)
*Scott D. Stoller,Balaji Jayasankar,Yanhong A. Liu*

Main category: cs.DC

TL;DR: 本文提出了一个自动化框架，用于优化分布式软件在异构硬件上的配置，以提高系统在故障下的弹性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂系统在故障下的弹性问题，通过优化软件组件的分布和复制。

Method: 基于状态空间探索的模型发现算法，结合了优化技术（如基于等价关系的商约简）。

Result: 成功应用于自动驾驶系统模型，生成了弹性配置和重新配置策略。

Conclusion: 该框架能有效提高复杂系统在故障下的弹性和适应性。

Abstract: This paper presents a powerful automated framework for making complex systems
resilient under failures, by optimized adaptive distribution and replication of
interdependent software components across heterogeneous hardware components
with widely varying capabilities. A configuration specifies how software is
distributed and replicated: which software components to run on each computer,
which software components to replicate, which replication protocols to use,
etc. We present an algorithm that, given a system model and resilience
requirements, (1) determines initial configurations of the system that are
resilient, and (2) generates a reconfiguration policy that determines
reconfiguration actions to execute in response to failures and recoveries. This
model-finding algorithm is based on state-space exploration and incorporates
powerful optimizations, including a quotient reduction based on a novel
equivalence relation between states. We present experimental results from
successfully applying a prototype implementation of our framework to a model of
an autonomous driving system.

</details>


### [75] [HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration](https://arxiv.org/abs/2506.10401)
*Jiaqi Lv,Xufeng He,Yanchen Liu,Xu Dai,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 本文提出了一种利用AI编译器和自动优化技术生成高性能CUDA及对应平台代码对的框架，解决CUDA代码跨平台翻译的挑战，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型的快速发展，CUDA生态系统在并行计算领域占据主导地位，但将CUDA代码翻译到其他平台面临编程范式和硬件架构差异的挑战。现有方法覆盖范围有限且开发成本高。

Method: 提出了一种结合AI编译器和自动优化技术的框架，采用基于图的数据增强方法，并引入HPCTransEval基准测试工具评估LLM在CUDA翻译中的表现。

Result: 实验结果表明，该框架显著提高了CUDA代码翻译的性能，展示了LLM在解决CUDA生态系统兼容性问题中的潜力。

Conclusion: 该框架通过AI和自动优化技术有效解决了CUDA代码跨平台翻译的挑战，为LLM在并行计算领域的应用提供了新思路。

Abstract: The rapid growth of deep learning has driven exponential increases in model
parameters and computational demands. NVIDIA GPUs and their CUDA-based software
ecosystem provide robust support for parallel computing, significantly
alleviating computational bottlenecks. Meanwhile, due to the cultivation of
user programming habits and the high performance of GPUs, the CUDA ecosystem
has established a dominant position in the field of parallel software. This
dominance requires other hardware platforms to support CUDA-based software with
performance portability. However, translating CUDA code to other platforms
poses significant challenges due to differences in parallel programming
paradigms and hardware architectures. Existing approaches rely on language
extensions, domain-specific languages (DSLs), or compilers but face limitations
in workload coverage and generalizability. Moreover, these methods often incur
substantial development costs. Recently, LLMs have demonstrated extraordinary
potential in various vertical domains, especially in code-related tasks.
However, the performance of existing LLMs in CUDA transpilation, particularly
for high-performance code, remains suboptimal. The main reason for this
limitation lies in the lack of high-quality training datasets. To address these
challenges, we propose a novel framework for generating high-performance CUDA
and corresponding platform code pairs, leveraging AI compiler and automatic
optimization technology. We further enhance the framework with a graph-based
data augmentation method and introduce HPCTransEval, a benchmark for evaluating
LLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU
transpilation as a case study on leading LLMs. The result demonstrates that our
framework significantly improves CUDA transpilation, highlighting the potential
of LLMs to address compatibility challenges within the CUDA ecosystem.

</details>


### [76] [Federated Learning within Global Energy Budget over Heterogeneous Edge Accelerators](https://arxiv.org/abs/2506.10413)
*Roopkatha Banerjee,Tejus Chandrashekar,Ananth Eswar,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 该论文提出了一种基于能量预算的联邦学习客户端选择优化方法（FedJoule），旨在在限制总能量消耗的同时最大化模型精度并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在设备异构性和非独立同分布数据下，优化能量效率和模型精度的挑战，同时引入全局能量限制以实现可持续AI。

Method: 采用双层整数线性规划（ILP）方法，结合近似Shapley值和能量-时间预测模型，实现高效的客户端选择。

Result: FedJoule在多样能量预算、非独立同分布数据和实际配置下，模型精度和时间效率分别比现有技术高出15%和48%。

Conclusion: FedJoule在能量使用与性能之间实现了有效平衡，为联邦学习的可持续性提供了可行方案。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. However, optimizing both
energy efficiency and model accuracy remains a challenge, given device and data
heterogeneity. Further, sustainable AI through a global energy budget for FL
has not been explored. We propose a novel optimization problem for client
selection in FL that maximizes the model accuracy within an overall energy
limit and reduces training time. We solve this with a unique bi-level ILP
formulation that leverages approximate Shapley values and energy-time
prediction models to efficiently solve this. Our FedJoule framework achieves
superior training accuracies compared to SOTA and simple baselines for diverse
energy budgets, non-IID distributions, and realistic experiment configurations,
performing 15% and 48% better on accuracy and time, respectively. The results
highlight the effectiveness of our method in achieving a viable trade-off
between energy usage and performance in FL environments.

</details>


### [77] [Automating Multi-Tenancy Performance Evaluation on Edge Compute Nodes](https://arxiv.org/abs/2506.10461)
*Joanna Georgiou,Moysis Symeonides,George Pallis,Marios D. Dikaiakos*

Main category: cs.DC

TL;DR: 论文提出了一种自动基准测试框架，用于分析边缘计算环境中的多租户性能，以解决资源争用导致的安全和性能问题。


<details>
  <summary>Details</summary>
Motivation: 边缘计算作为云计算的替代方案，多租户虽然必要，但会引发安全和性能问题，需要有效的工具来简化分析和优化部署。

Method: 研究团队开发了一个包含内置监控堆栈的自动基准测试框架，集成了多种常用测试负载，如流分析、数据库操作和机器学习应用。

Result: 通过案例驱动分析，提供了多租户对不同硬件配置和多样化负载的影响的深入见解。

Conclusion: 该框架的实现和实验所用的容器化负载已公开可用，为边缘计算环境的多租户性能优化提供了实用工具。

Abstract: Edge Computing emerges as a promising alternative of Cloud Computing, with
scalable compute resources and services deployed in the path between IoT
devices and Cloud. Since virtualization techniques can be applied on Edge
compute nodes, administrators can share their Edge infrastructures among
multiple users, providing the so-called multi-tenancy. Even though
multi-tenancy is unavoidable, it raises concerns about security and performance
degradation due to resource contention in Edge Computing. For that,
administrators need to deploy services with non-antagonizing profiles and
explore workload co-location scenarios to enhance performance and energy
consumption. Achieving this, however, requires extensive configuration,
deployment, iterative testing, and analysis, an effort-intensive and
time-consuming process. To address this challenge, we introduce an
auto-benchmarking framework designed to streamline the analysis of
multi-tenancy performance in Edge environments. Our framework includes a
built-in monitoring stack and integrates with widely used benchmarking
workloads, such as streaming analytics, database operations, machine learning
applications, and component-based stress testing. We perform a case-driven
analysis and provide valuable insights into the impact of multi-tenancy on Edge
environments with different hardware configurations and diverse workloads.
Finally, the implementation of our framework, along with the containerized
workloads used for experimentation, is publicly available.

</details>


### [78] [TD-Pipe: Temporally-Disaggregated Pipeline Parallelism Architecture for High-Throughput LLM Inference](https://arxiv.org/abs/2506.10470)
*Hongbin Zhang,Taosheng Wei,Zhenyi Zheng,Jiangsu Du,Zhiguang Chen,Yutong Lu*

Main category: cs.DC

TL;DR: TD-Pipe通过时间解耦的管道并行架构解决LLM推理中的管道气泡问题，提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 模型规模增大时，管道并行在LLM推理中因通信需求低而受青睐，但预填充和解码阶段的不平衡工作负载和数据依赖导致管道气泡和性能下降。

Method: TD-Pipe采用时间解耦的管道并行架构，并通过层次控制器结构、AI贪婪预填充、批间工作窃取和时空强度比较等方法优化。

Result: 实验显示TD-Pipe在GPU节点上的吞吐量比现有张量并行方法提升1.91倍，比现有管道并行方法提升2.73倍。

Conclusion: TD-Pipe有效解决了管道气泡问题，显著提升了LLM推理的吞吐量。

Abstract: As the model size continuously increases, pipeline parallelism shows great
promise in throughput-oriented LLM inference due to its low demand on
communications. However, imbalanced pipeline workloads and complex data
dependencies in the prefill and decode phases result in massive pipeline
bubbles and further severe performance reduction. To better exploit the
pipeline parallelism for high-throughput LLM inference, we propose TD-Pipe,
with the key idea lies in the temporally-disaggregated pipeline parallelism
architecture. Specifically, this architecture disaggregates the prefill and
decode phases in the temporal dimension, so as to eliminate pipeline bubbles
caused by the phase switching. TD-Pipe identifies potential issues of
exploiting the novel architecture and provides solutions. First, a
hierarchy-controller structure is used to better coordinate devices in pipeline
parallelism by decoupling the scheduling from execution. Second, the AI-based
greedy prefill approach aggressively performs more prefills by predicting the
output length and simulating the memory usage. Third, the inter-batch work
stealing approach dynamically balances decode phase workloads between different
batches to reduce bubbles. Forth, the spatial-temporal intensity comparison
approach determines the optimal switch from decode to prefill by comparing the
performance drop from reduced computational intensity with that from phase
switching bubbles. Extensive experiments show that TD-Pipe effectively
increases the throughput of LLM inference by up to 1.91x over the existing
tensor parallel approach and 2.73x over the existing pipeline parallel approach
on GPU nodes with only PCIe interconnection.

</details>


### [79] [HP2C-DT: High-Precision High-Performance Computer-enabled Digital Twin](https://arxiv.org/abs/2506.10523)
*E. Iraola,M. García-Lorenzo,F. Lordan-Gomis,F. Rossi,E. Prieto-Araujo,R. M. Badia*

Main category: cs.DC

TL;DR: 该论文提出了一种名为HP2C-DT的高性能计算数字孪生架构，通过动态分配任务到边缘、云或HPC资源，解决了实时响应与高计算需求之间的平衡问题，并在电网用例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在监控、分析和控制物理系统方面具有潜力，但现有架构难以平衡实时性和计算需求，云方案存在延迟和资源限制，边缘方案则缺乏复杂计算能力。

Method: 提出HP2C-DT参考架构，将高性能计算（HPC）融入计算连续体，并使用COMPSs实现跨基础设施的无缝工作负载分配。

Result: 在电网用例中，HP2C-DT通过边缘数据聚合减少通信带宽一个数量级，动态卸载优化响应时间提升2倍，并在实际资源范围内保持接近理想的计算密集型工作负载扩展性。

Conclusion: HPC驱动的HP2C-DT方法能够突破数字孪生当前限制，使其更智能、快速，并更好地处理现实世界的复杂性。

Abstract: Digital twins are transforming the way we monitor, analyze, and control
physical systems, but designing architectures that balance real-time
responsiveness with heavy computational demands remains a challenge.
Cloud-based solutions often struggle with latency and resource constraints,
while edge-based approaches lack the processing power for complex simulations
and data-driven optimizations.
  To address this problem, we propose the High-Precision High-Performance
Computer-enabled Digital Twin (HP2C-DT) reference architecture, which
integrates High-Performance Computing (HPC) into the computing continuum.
Unlike traditional setups that use HPC only for offline simulations, HP2C-DT
makes it an active part of digital twin workflows, dynamically assigning tasks
to edge, cloud, or HPC resources based on urgency and computational needs.
  Furthermore, to bridge the gap between theory and practice, we introduce the
HP2C-DT framework, a working implementation that uses COMPSs for seamless
workload distribution across diverse infrastructures. We test it in a power
grid use case, showing how it reduces communication bandwidth by an order of
magnitude through edge-side data aggregation, improves response times by up to
2x via dynamic offloading, and maintains near-ideal strong scaling for
compute-intensive workflows across a practical range of resources. These
results demonstrate how an HPC-driven approach can push digital twins beyond
their current limitations, making them smarter, faster, and more capable of
handling real-world complexity.

</details>


### [80] [GPU-Accelerated Distributed QAOA on Large-scale HPC Ecosystems](https://arxiv.org/abs/2506.10531)
*Zhihao Xu,Srikar Chundury,Seongmin Kim,Amir Shehata,Xinyi Li,Ang Li,Tengfei Luo,Frank Mueller,In-Saeng Suh*

Main category: cs.DC

TL;DR: DQAOA算法通过高级问题分解和并行执行提高了性能和可扩展性，在GPU加速下实现了10倍速度提升。


<details>
  <summary>Details</summary>
Motivation: 解决复杂组合优化问题，提升量子计算的实用性和可扩展性。

Method: 采用高级问题分解和并行执行策略，结合HPC系统，优化量子-经典混合工作负载管理。

Result: GPU加速的量子模拟显著提升DQAOA性能，速度提升达10倍。

Conclusion: 该研究为大规模问题实例提供了更高效的支持，推动了混合量子-经典计算的实际应用。

Abstract: Quantum computing holds great potential to accelerate the process of solving
complex combinatorial optimization problems. The Distributed Quantum
Approximate Optimization Algorithm (DQAOA) addresses high-dimensional, dense
problems using current quantum computing techniques and high-performance
computing (HPC) systems. In this work, we improve the scalability and
efficiency of DQAOA through advanced problem decomposition and parallel
execution using message passing on the Frontier CPU/GPU supercomputer. Our
approach ensures efficient quantum-classical workload management by
distributing large problem instances across classical and quantum resources.
Experimental results demonstrate that enhanced decomposition strategies and
GPU-accelerated quantum simulations significantly improve DQAOA's performance,
achieving up to 10x speedup over CPU-based simulations. This advancement
enables better scalability for large problem instances, supporting the
practical deployment of GPU systems for hybrid quantum-classical applications.
We also highlight ongoing integration efforts using the Quantum Framework (QFw)
to support future HPC-quantum computing systems.

</details>


### [81] [6G Infrastructures for Edge AI: An Analytical Perspective](https://arxiv.org/abs/2506.10570)
*Kurt Horvath,Shpresa Tuda,Blerta Idrizi,Stojan Kitanov,Fisnik Doko,Dragi Kimovski*

Main category: cs.DC

TL;DR: 论文探讨了AI与物联网结合对网络性能的需求，分析了5G在支持AI边缘应用时的不足，并提出了优化6G生态的建议。


<details>
  <summary>Details</summary>
Motivation: AI与物联网的发展需要超低延迟、高吞吐和实时处理的网络性能，但5G在实际部署中仍存在性能差距。

Method: 通过实证评估中欧5G网络基础设施的延迟性能，并与AI应用需求对比。

Result: 5G网络的延迟（61ms至110ms）超出AI应用需求约270%，显示当前部署存在显著不足。

Conclusion: 研究提出了优化6G生态的建议，以满足下一代AI应用的性能需求。

Abstract: The convergence of Artificial Intelligence (AI) and the Internet of Things
has accelerated the development of distributed, network-sensitive applications,
necessitating ultra-low latency, high throughput, and real-time processing
capabilities. While 5G networks represent a significant technological
milestone, their ability to support AI-driven edge applications remains
constrained by performance gaps observed in real-world deployments. This paper
addresses these limitations and highlights critical advancements needed to
realize a robust and scalable 6G ecosystem optimized for AI applications.
Furthermore, we conduct an empirical evaluation of 5G network infrastructure in
central Europe, with latency measurements ranging from 61 ms to 110 ms across
different close geographical areas. These values exceed the requirements of
latency-critical AI applications by approximately 270%, revealing significant
shortcomings in current deployments. Building on these findings, we propose a
set of recommendations to bridge the gap between existing 5G performance and
the requirements of next-generation AI applications.

</details>


### [82] [Graph-based Gossiping for Communication Efficiency in Decentralized Federated Learning](https://arxiv.org/abs/2506.10607)
*Huong Nguyen,Hong-Tri Nguyen,Praveen Kumar Donta,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.DC

TL;DR: 该论文提出了一种基于图的机制（如最小生成树和图着色）来优化去中心化学习中的通信效率，解决了现有方法在大规模环境中通信效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化学习方法因模拟环境不真实导致性能评估不可靠，且在大规模网络中通信效率低下。本工作旨在优化通信效率。

Method: 采用基于图的机制（最小生成树和图着色）优化网络结构和调度，并在真实物理设备上配置子网络以模拟实际分布式环境。

Result: 实验表明，该方法显著提升了通信效率，带宽和传输时间分别减少了约8倍和4.4倍。

Conclusion: 该方法在优化去中心化学习通信方面具有显著效果，适应不同拓扑和数据规模，解决了现有方法的局限性。

Abstract: Federated learning has emerged as a privacy-preserving technique for
collaborative model training across heterogeneously distributed silos. Yet, its
reliance on a single central server introduces potential bottlenecks and risks
of single-point failure. Decentralizing the server, often referred to as
decentralized learning, addresses this problem by distributing the server role
across nodes within the network. One drawback regarding this pure
decentralization is it introduces communication inefficiencies, which arise
from increased message exchanges in large-scale setups. However, existing
proposed solutions often fail to simulate the real-world distributed and
decentralized environment in their experiments, leading to unreliable
performance evaluations and limited applicability in practice. Recognizing the
lack from prior works, this work investigates the correlation between model
size and network latency, a critical factor in optimizing decentralized
learning communication. We propose a graph-based gossiping mechanism, where
specifically, minimum spanning tree and graph coloring are used to optimize
network structure and scheduling for efficient communication across various
network topologies and message capacities. Our approach configures and manages
subnetworks on real physical routers and devices and closely models real-world
distributed setups. Experimental results demonstrate that our method
significantly improves communication, compatible with different topologies and
data sizes, reducing bandwidth and transfer time by up to circa 8 and 4.4
times, respectively, compared to naive flooding broadcasting methods.

</details>


### [83] [Deployment of Containerized Simulations in an API-Driven Distributed Infrastructure](https://arxiv.org/abs/2506.10642)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: SUNRISE是一个统一的虚拟原型基础设施，旨在整合多种仿真技术，促进协作，并通过分布式计算资源和开放API提升效率。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统市场的动态性使得虚拟原型工具在硬件/软件协同设计中变得不可或缺，但目前存在多样化的解决方案，缺乏统一的方式。

Method: 提出SUNRISE基础设施，通过分布式计算资源和开放API，整合多种虚拟原型技术，为用户提供统一的访问方式。

Result: SUNRISE能够促进协作并高效部署仿真任务，解决了现有虚拟原型工具的分散问题。

Conclusion: SUNRISE为虚拟原型设计提供了一个高效的统一平台，推动了硬件/软件协同设计的进步。

Abstract: The increasingly dynamic market for embedded systems makes virtual prototypes
an indispensable tool for hardware/software codesign. The broad acceptance of
the methodology has led to a diverse range of solutions: from open-source, pure
console-based simulators to highly capable commercial simulation tools. In this
work we present SUNRISE, an infrastructure to provide users a unified approach
to utilizing virtual prototyping solutions, facilitate access to various
simulation technologies and boost cooperation by leveraging decentralized
compute resources for deployment of simulation workloads and definition of open
APIs.

</details>


### [84] [Towards Sustainable Computing: Exploring Energy Consumption Efficiency of Alternative Configurations and Workloads in an Open Source Messaging System](https://arxiv.org/abs/2506.10693)
*Maria Voreakou,George Kousiouris,Mara Nikolaidou*

Main category: cs.DC

TL;DR: 论文探讨了在大型计算基础设施（如云环境）中，消息系统（如RabbitMQ）的能源消耗问题，通过实验比较不同架构的能耗差异，最高可减少31%的功耗。


<details>
  <summary>Details</summary>
Motivation: 由于微服务架构和物联网的发展，消息系统在现代计算基础设施中扮演重要角色，但其能源消耗问题日益突出，亟待优化。

Method: 通过实验性过程，测试RabbitMQ在不同工作负载和配置下的能源消耗，比较不同架构的能耗表现。

Result: 研究发现，架构选择对能耗有显著影响，最高可减少31%的功耗，相关数据集已公开。

Conclusion: 研究为架构选择和能源成本建模提供了实用参考，有助于优化消息系统的能源效率。

Abstract: Energy consumption in current large scale computing infrastructures is
becoming a critical issue, especially with the growing demand for centralized
systems such as cloud environments. With the advancement of microservice
architectures and the Internet of Things, messaging systems have become an
integral and mainstream part of modern computing infrastructures, carrying out
significant workload in a majority of applications. In this paper, we describe
an experimental process to explore energy-based benchmarking for RabbitMQ, one
of the main open source messaging frameworks. The involved system is described,
as well as required components, and setup scenarios, involving different
workloads and configurations among the tests as well as messaging system use
cases. Alternative architectures are investigated and compared from an energy
consumption point of view, for different message rates and consumer numbers.
Differences in architectural selection have been quantified and can lead to up
to 31\% reduction in power consumption. The resulting dataset is made publicly
available and can thus prove helpful for architectures' comparison,
energy-based cost modeling, and beyond.

</details>


### [85] [The Impact of Partial Computations on the Red-Blue Pebble Game](https://arxiv.org/abs/2506.10854)
*Pál András Papp,Aleksandros Sobczyk,A. N. Yzelman*

Main category: cs.DC

TL;DR: 该论文研究了带有部分计算步骤的红蓝鹅卵石游戏（PRBP），证明了在允许逐步聚合输入的情况下，I/O成本可以显著降低，但同时指出优化成本和近似最优解仍然是NP难问题。


<details>
  <summary>Details</summary>
Motivation: 传统红蓝鹅卵石游戏（RBP）假设所有输入必须同时存在于快速内存中，但实际计算中可能允许逐步聚合输入。因此，研究PRBP可以更真实地反映I/O成本。

Method: 通过扩展RBP模型，引入部分计算步骤（PRBP），并分析其基本性质。利用$S$-partitions工具推导I/O成本下界，并比较PRBP与RBP的差异。

Result: 在允许部分计算时，I/O成本可降低至线性因子，但优化成本和近似最优解是NP难问题。PRBP为某些计算任务提供了更精确的I/O成本模型。

Conclusion: PRBP是RBP的一个更现实的扩展模型，适用于逐步聚合输入的场景，但其优化问题仍具挑战性，表明在实际应用中需要权衡策略复杂性与成本收益。

Abstract: We study an extension of the well-known red-blue pebble game (RBP) with
partial computation steps, inspired by the recent work of Sobczyk. While the
original RBP assumes that we need to have all the inputs of an operation in
fast memory at the same time, in many concrete computations, the inputs can be
aggregated one by one into the final output value. These partial computation
steps can enable pebbling strategies with much smaller I/O cost, and in
settings where such a step-by-step aggregation is possible, this extended
red-blue pebble game offers a much more realistic cost model.
  We establish the fundamental properties of this partial-computing red-blue
pebble game (PRBP), and compare it to the original RBP. We begin with some
simple examples where allowing partial computations can decrease the optimal
I/O cost. It is also shown that the cost can decrease by up to a linear factor
this way, but in general, it is NP-hard to decide whether partial computations
allow for a smaller cost in a specific DAG. We then discuss how $S$-partitions,
a crucial tool for deriving I/O lower bounds in RBP, can be adapted to the PRBP
model. These new tools are then used to establish lower bounds on the I/O cost
of some prominent computational tasks. Finally, we also adapt a hardness result
from RBP, showing that the optimum cost is still NP-hard to approximate in PRBP
to any reasonable factor.

</details>


### [86] [Adaptive Job Scheduling in Quantum Clouds Using Reinforcement Learning](https://arxiv.org/abs/2506.10889)
*Waylon Luo,Jiapeng Zhao,Tong Zhan,Qiang Guan*

Main category: cs.DC

TL;DR: 论文提出了一种仿真工具，用于在联网量子处理器（QPU）上调度和执行分布式量子任务，以解决当前量子系统在扩展性和抗噪性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 当前量子系统面临比特数有限、相干时间短和易受噪声影响的瓶颈，导致执行大型复杂电路困难。需要通过任务调度和资源协调优化量子工作负载。

Method: 开发了一种基于仿真的工具，支持分布式调度和并行执行量子任务，模拟电路分解并通过处理器间通信实现并行执行。比较了四种调度策略，包括强化学习模型。

Result: 分析表明并行化且考虑噪声的调度策略能显著提高分布式量子基础设施的计算吞吐量，揭示了不同策略在运行效率、保真度和通信成本上的权衡。

Conclusion: 通过仿真工具展示了分布式量子计算的潜力，噪声感知调度策略为未来量子系统的规模化提供了实用解决方案。

Abstract: Present-day quantum systems face critical bottlenecks, including limited
qubit counts, brief coherence intervals, and high susceptibility to errors-all
of which obstruct the execution of large and complex circuits. The advancement
of quantum algorithms has outpaced the capabilities of existing quantum
hardware, making it difficult to scale computations effectively. Additionally,
inconsistencies in hardware performance and pervasive quantum noise undermine
system stability and computational accuracy. To optimize quantum workloads
under these constraints, strategic approaches to task scheduling and resource
coordination are essential. These methods must aim to accelerate processing,
retain operational fidelity, and reduce the communication burden inherent to
distributed setups. One of the persistent challenges in this domain is how to
efficiently divide and execute large circuits across multiple quantum
processors (QPUs), especially in error-prone environments. In response, we
introduce a simulation-based tool that supports distributed scheduling and
concurrent execution of quantum jobs on networked QPUs connected via real-time
classical channels. The tool models circuit decomposition for workloads that
surpass individual QPU limits, allowing for parallel execution through
inter-processor communication. Using this simulation environment, we compare
four distinct scheduling techniques-among them, a model informed by
reinforcement learning. These strategies are evaluated across multiple metrics,
including runtime efficiency, fidelity preservation, and communication costs.
Our analysis underscores the trade-offs inherent in each approach and
highlights how parallelized, noise-aware scheduling can meaningfully improve
computational throughput in distributed quantum infrastructures.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [87] [GPU Acceleration of SQL Analytics on Compressed Data](https://arxiv.org/abs/2506.10092)
*Zezhou Huang,Krystian Sakowski,Hans Lehnert,Wei Cui,Carlo Curino,Matteo Interlandi,Marius Dumitru,Rathijit Sen*

Main category: cs.DB

TL;DR: 论文提出了一种直接在轻量压缩数据上执行查询的新方法，解决了GPU HBM容量有限的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: GPU HBM容量有限，传统方法（如数据分区或混合执行）因带宽限制或I/O开销难以高效处理大数据集。需要通过压缩和直接在压缩数据上运算来缓解瓶颈。

Method: 采用轻量压缩方案（如RLE、索引编码等），支持多列RLE操作和异构列编码，并利用PyTorch张量运算实现跨设备兼容性。

Result: 在无法装入GPU内存的生产数据集上，比商用CPU系统快一个数量级。

Conclusion: 该方法拓展了GPU在大数据集上的应用范围，是其他扩展或回退机制的有力补充。

Abstract: GPUs are uniquely suited to accelerate (SQL) analytics workloads thanks to
their massive compute parallelism and High Bandwidth Memory (HBM) -- when
datasets fit in the GPU HBM, performance is unparalleled. Unfortunately, GPU
HBMs remain typically small when compared with lower-bandwidth CPU main memory.
Besides brute-force scaling across many GPUs, current solutions to accelerate
queries on large datasets include leveraging data partitioning and loading
smaller data batches in GPU HBM, and hybrid execution with a connected device
(e.g., CPUs). Unfortunately, these approaches are exposed to the limitations of
lower main memory and host-to-device interconnect bandwidths, introduce
additional I/O overheads, or incur higher costs. This is a substantial problem
when trying to scale adoption of GPUs on larger datasets. Data compression can
alleviate this bottleneck, but to avoid paying for costly
decompression/decoding, an ideal solution must include computation primitives
to operate directly on data in compressed form.
  This is the focus of our paper: a set of new methods for running queries
directly on light-weight compressed data using schemes such as Run-Length
Encoding (RLE), index encoding, bit-width reductions, and dictionary encoding.
Our novelty includes operating on multiple RLE columns without decompression,
handling heterogeneous column encodings, and leveraging PyTorch tensor
operations for portability across devices. Experimental evaluations show
speedups of an order of magnitude compared to state-of-the-art commercial
CPU-only analytics systems, for real-world queries on a production dataset that
would not fit into GPU memory uncompressed. This work paves the road for GPU
adoption in a much broader set of use cases, and it is complementary to most
other scale-out or fallback mechanisms.

</details>


### [88] [A Hybrid Heuristic Framework for Resource-Efficient Querying of Scientific Experiments Data](https://arxiv.org/abs/2506.10422)
*Mayank Patel,Minal Bhise*

Main category: cs.DB

TL;DR: 论文提出了一种轻量级资源感知框架RAW-HF，用于优化查询原始数据的效率，显著减少执行时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统数据库系统和原位引擎在资源利用和数据加载效率上存在不足，导致高成本和低效。

Method: 设计了资源感知混合框架RAW-HF，通过优化资源分配和工作负载管理提升效率。

Result: 在SDSS和LOD数据集上，执行时间减少超过85%，资源利用率也有显著提升。

Conclusion: RAW-HF在资源优化和工作负载执行效率上优于传统方法和现有混合系统技术。

Abstract: Scientific experiments and modern applications are generating large amounts
of data every day. Most organizations utilize In-house servers or Cloud
resources to manage application data and workload. The traditional database
management system (DBMS) and HTAP systems spend significant time & resources to
load the entire dataset into DBMS before starting query execution. On the other
hand, in-situ engines may reparse required data multiple times, increasing
resource utilization and data processing costs. Additionally, over or
under-allocation of resources also increases application running costs. This
paper proposes a lightweight Resource Availability &Workload aware Hybrid
Framework (RAW-HF) to optimize querying raw data by utilizing existing finite
resources efficiently. RAW-HF includes modules that help optimize the resources
required to execute a given workload and maximize the utilization of existing
resources. The impact of applying RAW-HF to real-world scientific dataset
workloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data
(LOD) presented over 90% and 85% reduction in workload execution time (WET)
compared to widely used traditional DBMS PostgreSQL. The overall CPU, IO
resource utilization, and WET have been reduced by 26%, 25%, and 26%,
respectively, while improving memory utilization by 33%, compared to the
state-of-the-art workload-aware partial loading technique (WA) proposed for
hybrid systems. A comparison of MUAR technique used by RAW-HF with machine
learning based resource allocation techniques like PCC is also presented.

</details>


### [89] [A Unifying Algorithm for Hierarchical Queries](https://arxiv.org/abs/2506.10238)
*Mahmoud Abo Khamis,Jesse Comer,Phokion Kolaitis,Sudeepa Roy,Val Tannen*

Main category: cs.DB

TL;DR: 本文确定了层次查询在三个不同问题中的可解性边界，提出了一种统一的2-幺半群算法。


<details>
  <summary>Details</summary>
Motivation: 研究层次查询在不同计算问题中的边界，探索统一的算法框架。

Method: 定义了“bag-set最大化”问题，使用2-幺半群抽象结构设计统一算法。

Result: 非层次查询的bag-set最大化问题是NP完全的；层次查询的三类问题均可用多项式算法解决。

Conclusion: 层次查询是三类问题的分界点，2-幺半群提供了一种统一的高效算法框架。

Abstract: The class of hierarchical queries is known to define the boundary of the
dichotomy between tractability and intractability for the following two
extensively studied problems about self-join free Boolean conjunctive queries
(SJF-BCQ): (i) evaluating a SJF-BCQ on a tuple-independent probabilistic
database; (ii) computing the Shapley value of a fact in a database on which a
SJF-BCQ evaluates to true. Here, we establish that hierarchical queries define
also the boundary of the dichotomy between tractability and intractability for
a different natural algorithmic problem, which we call the "bag-set
maximization" problem. The bag-set maximization problem associated with a
SJF-BCQ $Q$ asks: given a database $\cal D$, find the biggest value that $Q$
takes under bag semantics on a database $\cal D'$ obtained from $\cal D$ by
adding at most $\theta$ facts from another given database $\cal D^r$.
  For non-hierarchical queries, we show that the bag-set maximization problem
is an NP-complete optimization problem. More significantly, for hierarchical
queries, we show that all three aforementioned problems (probabilistic query
evaluation, Shapley value computation, and bag-set maximization) admit a single
unifying polynomial-time algorithm that operates on an abstract algebraic
structure, called a "2-monoid". Each of the three problems requires a different
instantiation of the 2-monoid tailored for the problem at hand.

</details>


### [90] [S3 Mirror: S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable with DBOS](https://arxiv.org/abs/2506.10886)
*Steven Vasquez-Grinnell,Alex Poliakov*

Main category: cs.DB

TL;DR: S3Mirror是一个用于快速、可靠且可观察地传输大型基因组测序数据的开源应用，比AWS DataSync快40倍且成本更低。


<details>
  <summary>Details</summary>
Motivation: 满足大型制药组织对高效、低成本、可靠的数据传输需求。

Method: 使用DBOSTransact持久执行框架开发，并在多种环境（包括DBOS Cloud Pro）中测试性能和成本。

Result: 性能比AWS DataSync快40倍，成本更低，具备故障恢复能力和实时监测功能。

Conclusion: S3Mirror是高效且经济的数据传输解决方案。

Abstract: To meet the needs of a large pharmaceutical organization, we set out to
create S3Mirror - an application for transferring large genomic sequencing
datasets between S3 buckets quickly, reliably, and observably. We used the
DBOSTransact durable execution framework to achieve these goals and benchmarked
the performance and cost of the application. S3Mirror is an open source DBOS
Python application that can run in a variety of environments, including DBOS
Cloud Pro where it runs as much as 40x faster than AWS DataSync at a fraction
of the cost. Moreover, S3Mirror is resilient to failures and allows for
real-time filewise observability of ongoing and past transfers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [91] [CarbonSet: A Dataset to Analyze Trends and Benchmark the Sustainability of CPUs and GPUs](https://arxiv.org/abs/2506.10373)
*Jiajun Hu,Chetan Choppali Sudarshan,Vidya A. Chhabria,Aman Arora*

Main category: cs.AR

TL;DR: 论文提出了CarbonSet数据集，用于分析过去十年CPU和GPU的可持续性趋势，指出现代处理器设计在环保方面仍有不足，碳排放因AI需求激增而大幅上升。


<details>
  <summary>Details</summary>
Motivation: 芯片行业快速发展导致碳排放激增，但缺乏对整个芯片生命周期可持续性的全面分析。研究旨在填补这一空白，为下一代处理器设计提供基准。

Method: 通过集成可持续性和性能指标的CarbonSet数据集，分析旗舰处理器过去十年的可持续性趋势。

Result: 研究发现现代处理器设计未达可持续标准，过去三年碳排放因AI需求增长超过50倍，能效和先进制程是主要挑战。

Conclusion: 需优化处理器设计以缓解碳排放问题，特别是在AI需求持续增长的背景下。

Abstract: Over the years, the chip industry has consistently developed high-performance
processors to address the increasing demands across diverse applications.
However, the rapid expansion of chip production has significantly increased
carbon emissions, raising critical concerns about environmental sustainability.
While researchers have previously modeled the carbon footprint (CFP) at both
system and processor levels, a holistic analysis of sustainability trends
encompassing the entire chip lifecycle remains lacking. This paper presents
CarbonSet, a comprehensive dataset integrating sustainability and performance
metrics for CPUs and GPUs over the past decade. CarbonSet aims to benchmark and
assess the design of next-generation processors. Leveraging this dataset, we
conducted detailed analysis of flagship processors' sustainability trends over
the last decade. This paper further highlights that modern processors are not
yet sustainably designed, with total carbon emissions increasing more than
50$\times$ in the past three years due to the surging demand driven by the AI
boom. Power efficiency remains a significant concern, while advanced process
nodes pose new challenges requiring to effectively amortize the dramatically
increased manufacturing carbon emissions.

</details>


### [92] [EasyDRAM: An FPGA-based Infrastructure for Fast and Accurate End-to-End Evaluation of Emerging DRAM Techniques](https://arxiv.org/abs/2506.10441)
*Oğuzhan Canpolat,Ataberk Olgun,David Novo,Oğuz Ergin,Onur Mutlu*

Main category: cs.AR

TL;DR: EasyDRAM是一个基于FPGA的框架，用于快速且准确地评估DRAM技术，克服了现有平台在可访问性和建模现代系统方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的DRAM技术评估平台需要硬件描述语言的专业知识，且难以准确建模现代计算系统。

Method: EasyDRAM通过使用C++高语言实现DRAM技术，并采用时间缩放技术来模拟处理器与DRAM之间的时钟频率差异。

Result: EasyDRAM提供了一个无需硬件描述语言专业知识的平台，并能准确建模现代系统的时序行为。

Conclusion: EasyDRAM旨在促进内存系统设计的创新，并已开源以便未来研究使用。

Abstract: DRAM is a critical component of modern computing systems. Recent works
propose numerous techniques (that we call DRAM techniques) to enhance
DRAM-based computing systems' throughput, reliability, and computing
capabilities (e.g., in-DRAM bulk data copy). Evaluating the system-wide
benefits of DRAM techniques is challenging as they often require modifications
across multiple layers of the computing stack. Prior works propose FPGA-based
platforms for rapid end-to-end evaluation of DRAM techniques on real DRAM
chips. Unfortunately, existing platforms fall short in two major aspects: (1)
they require deep expertise in hardware description languages, limiting
accessibility; and (2) they are not designed to accurately model modern
computing systems.
  We introduce EasyDRAM, an FPGA-based framework for rapid and accurate
end-to-end evaluation of DRAM techniques on real DRAM chips. EasyDRAM overcomes
the main drawbacks of prior FPGA-based platforms with two key ideas. First,
EasyDRAM removes the need for hardware description language expertise by
enabling developers to implement DRAM techniques using a high-level language
(C++). At runtime, EasyDRAM executes the software-defined memory system design
in a programmable memory controller. Second, EasyDRAM tackles a fundamental
challenge in accurately modeling modern systems: real processors typically
operate at higher clock frequencies than DRAM, a disparity that is difficult to
replicate on FPGA platforms. EasyDRAM addresses this challenge by decoupling
the processor-DRAM interface and advancing the system state using a novel
technique we call time scaling, which faithfully captures the timing behavior
of the modeled system.
  We believe and hope that EasyDRAM will enable innovative ideas in memory
system design to rapidly come to fruition. To aid future research EasyDRAM
implementation is open sourced at https://github.com/CMU-SAFARI/EasyDRAM.

</details>


### [93] [Towards Zero-Stall Matrix Multiplication on Energy-Efficient RISC-V Clusters for Machine Learning Acceleration](https://arxiv.org/abs/2506.10921)
*Luca Colagrande,Lorenzo Leone,Maximilian Coco,Andrei Deaconeasa,Luca Benini*

Main category: cs.AR

TL;DR: 论文提出了一种优化的微架构，通过消除控制流和内存访问的低效，实现了近乎理想的利用率，提升了性能和能源效率，同时保持了通用性和可编程性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习计算需求的增长，需要设计高效的加速器，平衡效率与灵活性。现有架构存在控制流和内存访问的低效问题。

Method: 采用零开销循环嵌套和零冲突内存子系统，通过新型双缓冲感知互连消除L1内存的银行冲突。

Result: 实现了96.1%-99.4%的利用率，性能提升11%，能效提升8%，接近专用加速器的性能。

Conclusion: 优化后的架构在保持通用性和可编程性的同时，显著提升了效率和性能。

Abstract: The growing computational demands of machine learning (ML) workloads have
driven the design of ML accelerators aiming at an optimal tradeoff between
efficiency and flexibility. A widely explored architecture for flexible ML
accelerators is based on clusters of lightweight instruction processors sharing
multi-banked L1 memory, augmented with specialized instruction extensions for
key ML-related computations, such as matrix multiplication (matmul). However,
instruction extensions should be coupled with microarchitectural optimizations
that remove inefficiencies due to control flow (loop handling) and memory
access, without drastically increasing processor complexity. Moving from a
state-of-the-art (SoA) ML accelerator cluster based on RISC-V processors, we
propose a low-overhead optimized microarchitecture that eliminates these
inefficiencies almost entirely while retaining programmability. We introduce
"zero-overhead loop nests" to remove control overheads, and a "zero-conflict
memory subsystem", leveraging a novel double-buffering-aware interconnect, to
eliminate bank conflicts in L1 memory. With these enhancements, we attain
near-ideal utilizations between 96.1% and 99.4%, achieving 11% performance and
8% energy efficiency improvements over the baseline SoA RISC-V cluster. We
demonstrate comparable utilizations and performance to a specialized SoA
accelerator, with only 12% difference in energy efficiency, while providing a
fully-programmable general-purpose solution supporting a significantly wider
range of workloads.

</details>


### [94] [MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis Inside the Storage Subsystem](https://arxiv.org/abs/2506.10931)
*Melina Soysal,Konstantina Koliogeorgi,Can Firtina,Nika Mansouri Ghiasi,Rakesh Nadig,Haiyu Mayo,Geraldo F. Oliveira,Yu Liang,Klea Zambaku,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.AR

TL;DR: MARS系统通过硬件/软件协同设计，显著提升了RSGA的性能和能效，减少了数据移动瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着测序技术发展，软件基础的RSGA难以匹配高吞吐量数据，硬件加速虽有效但I/O成为新瓶颈。

Method: MARS结合过滤机制、量化方案和存储内计算，优化RSGA流程并减少数据移动。

Result: MARS平均性能提升93倍，能效提升427倍。

Conclusion: MARS为RSGA提供了一种高性能、低能耗的解决方案。

Abstract: Raw signal genome analysis (RSGA) has emerged as a promising approach to
enable real-time genome analysis by directly analyzing raw electrical signals.
However, rapid advancements in sequencing technologies make it increasingly
difficult for software-based RSGA to match the throughput of raw signal
generation. This paper demonstrates that while hardware acceleration techniques
can significantly accelerate RSGA, the high volume of genomic data shifts the
performance and energy bottleneck from computation to I/O data movement. As
sequencing throughput increases, I/O overhead becomes the main contributor to
both runtime and energy consumption. Therefore, there is a need to design a
high-performance, energy-efficient system for RSGA that can both alleviate the
data movement bottleneck and provide large acceleration capabilities. We
propose MARS, a storage-centric system that leverages the heterogeneous
resources within modern storage systems (e.g., storage-internal DRAM, storage
controller, flash chips) alongside their large storage capacity to tackle both
data movement and computational overheads of RSGA in an area-efficient and
low-cost manner. MARS accelerates RSGA through a novel hardware/software
co-design approach. First, MARS modifies the RSGA pipeline via two filtering
mechanisms and a quantization scheme, reducing hardware demands and optimizing
for in-storage execution. Second, MARS accelerates the RSGA steps directly
within the storage by leveraging both Processing-Near-Memory and
Processing-Using-Memory paradigms. Third, MARS orchestrates the execution of
all steps to fully exploit in-storage parallelism and minimize data movement.
Our evaluation shows that MARS outperforms basecalling-based software and
hardware-accelerated state-of-the-art read mapping pipelines by 93x and 40x, on
average across different datasets, while reducing their energy consumption by
427x and 72x.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [95] [Mind the Gap: Revealing Security Barriers through Situational Awareness of Small and Medium Business Key Decision-Makers](https://arxiv.org/abs/2506.10025)
*Yuanhaur Chang,Oren Heller,Yaniv Shlomo,Iddo Bar-Noy,Ella Bokobza,Michal Grinstein-Weiss,Ning Zhang*

Main category: cs.CR

TL;DR: 本文研究了中小型企业（SMB）决策者在网络安全决策中的认知和行为，通过混合方法揭示其风险感知及其影响因素，并提出了提升意识和应对挑战的干预措施。


<details>
  <summary>Details</summary>
Motivation: SMB决策者普遍缺乏网络安全意识和知识，导致防御措施实施不力。研究旨在深入理解其决策行为及其影响因素。

Method: 采用混合方法，包括半结构化访谈（21人）和在线调查（322人），结合主题分析和情境意识模型。

Result: 揭示了SMB决策者的风险感知、防御措施选择及其影响因素；构建了结构方程模型，阐明了意识提升的路径。

Conclusion: 研究为SMB提供针对性干预措施，以提升网络安全意识和应对能力。

Abstract: Key decision-makers in small and medium businesses (SMBs) often lack the
awareness and knowledge to implement cybersecurity measures effectively. To
gain a deeper understanding of how SMB executives navigate cybersecurity
decision-making, we deployed a mixed-method approach, conducting
semi-structured interviews (n=21) and online surveys (n=322) with SMB key
decision-makers. Using thematic analysis, we revealed SMB decision-makers'
perceived risks in terms of the digital assets they valued, and found reasons
for their choice of defense measures and factors impacting security perception.
We employed the situational awareness model to characterize decision-makers
based on cybersecurity awareness, identifying those who have comparatively low
awareness in the fight against adversaries. We further explored the
relationship between awareness and business attributes, and constructed a
holistic structural equation model to understand how awareness can be improved.
Finally, we proposed interventions to help SMBs overcome potential challenges.

</details>


### [96] [Multiverse Privacy Theory for Contextual Risks in Complex User-AI Interactions](https://arxiv.org/abs/2506.10042)
*Ece Gumusel*

Main category: cs.CR

TL;DR: 本文提出了一种名为“多元宇宙隐私理论”的新框架，通过模拟用户隐私决策的平行宇宙，从情境完整性、动态偏好和概率决策的角度理解隐私问题。


<details>
  <summary>Details</summary>
Motivation: 在人工智能交互日益增多的时代，用户面临复杂且不确定的隐私决策，亟需新的理论框架来理解和应对这些挑战。

Method: 该理论通过模拟每个隐私决策导致的平行宇宙（即不同潜在结果），结合情境完整性、动态偏好和概率决策，提供了一种新的隐私分析框架。

Result: 初步理论框架为隐私研究提供了新的视角，未来工作将基于实际场景调查数据进一步验证和应用。

Conclusion: 多元宇宙隐私理论为复杂隐私决策提供了创新的分析工具，有望在未来的隐私研究中发挥重要作用。

Abstract: In an era of increasing interaction with artificial intelligence (AI), users
face evolving privacy decisions shaped by complex, uncertain factors. This
paper introduces Multiverse Privacy Theory, a novel framework in which each
privacy decision spawns a parallel universe, representing a distinct potential
outcome based on user choices over time. By simulating these universes, this
theory provides a foundation for understanding privacy through the lens of
contextual integrity, evolving preferences, and probabilistic decision-making.
Future work will explore its application using real-world, scenario-based
survey data.

</details>


### [97] [Expert-in-the-Loop Systems with Cross-Domain and In-Domain Few-Shot Learning for Software Vulnerability Detection](https://arxiv.org/abs/2506.10104)
*David Farr,Kevin Talty,Alexandra Farr,John Stockdale,Iain Cruickshank,Jevin West*

Main category: cs.CR

TL;DR: 论文探讨了使用大型语言模型（LLM）检测Python代码漏洞的效果，比较了零样本、少样本跨域和少样本同域提示策略，发现少样本提示效果更佳，并结合置信度路由策略优化自动化与人工监督的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，快速准确的漏洞检测对系统安全至关重要。研究旨在探索LLM在漏洞评估中的潜力。

Method: 通过模拟识别带有已知CWE的Python代码，比较零样本、少样本跨域和少样本同域提示策略，并结合置信度路由策略优化性能。

Result: 零样本提示效果不佳，少样本提示显著提升分类性能，尤其在结合置信度路由策略后，模型能高效分配专家资源处理高不确定性案例。LLM在少量示例下可跨漏洞类别泛化。

Conclusion: LLM有望成为可扩展的网络安全工具，但模型可靠性、可解释性和对抗鲁棒性仍需研究。结合AI与专家决策可提升网络安全效率。

Abstract: As cyber threats become more sophisticated, rapid and accurate vulnerability
detection is essential for maintaining secure systems. This study explores the
use of Large Language Models (LLMs) in software vulnerability assessment by
simulating the identification of Python code with known Common Weakness
Enumerations (CWEs), comparing zero-shot, few-shot cross-domain, and few-shot
in-domain prompting strategies. Our results indicate that while zero-shot
prompting performs poorly, few-shot prompting significantly enhances
classification performance, particularly when integrated with confidence-based
routing strategies that improve efficiency by directing human experts to cases
where model uncertainty is high, optimizing the balance between automation and
expert oversight. We find that LLMs can effectively generalize across
vulnerability categories with minimal examples, suggesting their potential as
scalable, adaptable cybersecurity tools in simulated environments. However,
challenges such as model reliability, interpretability, and adversarial
robustness remain critical areas for future research. By integrating AI-driven
approaches with expert-in-the-loop (EITL) decision-making, this work highlights
a pathway toward more efficient and responsive cybersecurity workflows. Our
findings provide a foundation for deploying AI-assisted vulnerability detection
systems in both real and simulated environments that enhance operational
resilience while reducing the burden on human analysts.

</details>


### [98] [D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning](https://arxiv.org/abs/2506.10125)
*Muqi Zou,Hongyu Cai,Hongwei Wu,Zion Leonahenahe Basque,Arslan Khan,Berkay Celik,Dave,Tian,Antonio Bianchi,Ruoyu,Wang,Dongyan Xu*

Main category: cs.CR

TL;DR: D-LiFT利用强化学习训练的LLMs提升反编译代码质量，通过D-SCORE系统综合评估准确性和可读性。


<details>
  <summary>Details</summary>
Motivation: 反编译器的输出常存在错误且难以阅读，现有LLM方法有限，D-LiFT旨在解决这一问题。

Method: D-LiFT结合RL训练的LLMs，使用D-SCORE系统验证准确性并评估可读性，优化输出。

Result: D-LiFT显著提升反编译代码质量，比基线LLMs多改进55.3%的函数。

Conclusion: D-LiFT通过D-SCORE驱动LLM微调，实现了反编译代码的准确性和可读性双重提升。

Abstract: Decompilers, which reconstruct human-readable source code from binary
executables, are vital to many security tasks. Yet, despite recent advances,
their output often suffers from syntactic and semantic errors and remains
difficult to read. Recently, with the advent of large language models (LLMs),
researchers began to explore the potential of LLMs to refine decompiler output.
Nevertheless, our study of these approaches reveals significant limitations,
such as introducing new errors and relying on unreliable accuracy validation.
In this paper, we present D-LiFT, an automated decompiler backend that
harnesses and further trains LLMs to improve the quality of decompiled code via
reinforcement learning (RL). Unlike prior work that overlooks preserving
accuracy, D-LiFT adheres to a key principle for enhancing the quality of
decompiled code: \textit{preserving accuracy while improving readability}.
Central to D-LiFT, we propose D-SCORE, an integrated quality assessment system
to score the decompiled code from multiple aspects. In line with our principle,
D-SCORE assigns low scores to any inaccurate output and only awards higher
scores for readability to code that passes the accuracy check. Specifically,
D-SCORE first verifies the syntactic and semantic correctness via the compiler
and symbolic execution; only if a candidate is deemed accurate, it then
evaluates readability using established metrics to compare the LLM output with
the original decompiled code. The score will then be fed back to the LLM for
fine-tuning. Our implementation, based on Ghidra and a range of LLMs,
demonstrates significant improvements for the accurate decompiled code from the
coreutils and util-linux projects. Compared to baseline LLMs without
D-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled
functions, as measured by D-SCORE.

</details>


### [99] [ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space](https://arxiv.org/abs/2506.10323)
*Chuyang Chen,Brendan Dolan-Gavitt,Zhiqiang Lin*

Main category: cs.CR

TL;DR: ELFuzz利用LLM自动合成生成式模糊测试器，显著提升了代码覆盖率和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试需要手动编写输入语法和语义约束，耗费大量人力。

Method: ELFuzz通过LLM驱动的最小种子模糊测试器演化，实现全自动合成。

Result: ELFuzz在真实世界规模的系统中表现优异，代码覆盖率提升434.8%，漏洞检测提升174.0%。

Conclusion: ELFuzz展示了自动化、高效和可扩展的模糊测试输入的潜力。

Abstract: Generation-based fuzzing produces appropriate testing cases according to
specifications of input grammars and semantic constraints to test systems and
software. However, these specifications require significant manual efforts to
construct. This paper proposes a new approach, ELFuzz (Evolution Through Large
Language Models for Fuzzing), that automatically synthesizes generation-based
fuzzers tailored to a system under test (SUT) via LLM-driven synthesis over
fuzzer space. At a high level, it starts with minimal seed fuzzers and propels
the synthesis by fully automated LLM-driven evolution with coverage guidance.
Compared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of
real-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)
synthesize efficient fuzzers that catch interesting grammatical structures and
semantic constraints in a human-understandable way. Our evaluation compared
ELFuzz with specifications manually written by domain experts and synthesized
by state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more
coverage and triggers up to 174.0% more artificially injected bugs. We also
used ELFuzz to conduct a real-world fuzzing campaign on the newest version of
cvc5 for 14 days, and encouragingly, it found five 0-day bugs (three are
exploitable). Moreover, we conducted an ablation study, which shows that the
fuzzer space model, the key component of ELFuzz, contributes the most (up to
62.5%) to the effectiveness of ELFuzz. Further analysis of the fuzzers
synthesized by ELFuzz confirms that they catch interesting grammatical
structures and semantic constraints in a human-understandable way. The results
present the promising potential of ELFuzz for more automated, efficient, and
extensible input generation for fuzzing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [100] [LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs](https://arxiv.org/abs/2506.10527)
*Yanan Cai,Ahmed Salem,Besmira Nushi,Mark Russinovich*

Main category: cs.AI

TL;DR: LogiPlan是一个新颖的基准测试，用于评估大语言模型在逻辑规划和复杂关系结构推理方面的能力，涵盖任务生成、一致性检测和关系比较三个任务。


<details>
  <summary>Details</summary>
Motivation: 为了测试LLMs在生成和查询关系结构图（如网络基础设施、知识库或业务流程模式）中的逻辑推理能力，设计了LogiPlan基准。

Method: 通过动态调整任务复杂度（如对象数量、关系链深度），评估模型在三个任务中的表现：计划生成、一致性检测和比较问题。

Result: 实验显示，当前最先进的模型在简单任务上表现良好，但在需要深度逻辑规划的复杂配置中表现较差，性能与模型的规模和架构相关。

Conclusion: LogiPlan揭示了LLMs在逻辑规划方面的局限性，为未来模型优化提供了方向。

Abstract: We introduce LogiPlan, a novel benchmark designed to evaluate the
capabilities of large language models (LLMs) in logical planning and reasoning
over complex relational structures. Logical relational reasoning is important
for applications that may rely on LLMs to generate and query structured graphs
of relations such as network infrastructure, knowledge bases, or business
process schema. Our framework allows for dynamic variation of task complexity
by controlling the number of objects, relations, and the minimum depth of
relational chains, providing a fine-grained assessment of model performance
across difficulty levels. LogiPlan encompasses three complementary tasks: (1)
Plan Generation, where models must construct valid directed relational graphs
meeting specified structural constraints; (2) Consistency Detection, testing
models' ability to identify inconsistencies in relational structures; and (3)
Comparison Question, evaluating models' capacity to determine the validity of
queried relationships within a given graph. Additionally, we assess models'
self-correction capabilities by prompting them to verify and refine their
initial solutions. We evaluate state-of-the-art models including DeepSeek R1,
Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,
O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant
performance gaps that correlate with model scale and architecture. Our analysis
demonstrates that while recent reasoning-enhanced models show promising results
on simpler instances, they struggle with more complex configurations requiring
deeper logical planning.

</details>


### [101] [Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods](https://arxiv.org/abs/2506.10420)
*Boris Sedlak,Alireza Furutanpey,Zihang Wang,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.AI

TL;DR: 论文介绍了一种基于代理的自动扩展框架，用于在资源受限的边缘计算环境中动态调整硬件资源和服务配置，四种代理类型在实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 边缘计算因资源限制需要更灵活的扩展行为，传统自动扩展方法不再适用。

Method: 采用基于代理的框架，动态调整资源和服务配置，比较了四种代理类型：Active Inference、Deep Q Network、Analysis of Structural Knowledge和Deep Active Inference。

Result: 所有代理均能满足性能要求，其中Deep Q Network得益于预训练，结构分析收敛快，Deep Active Inference兼具理论和实践优势。

Conclusion: 研究证明多维代理自动扩展在边缘环境中可行，为未来研究提供了方向。

Abstract: Edge computing breaks with traditional autoscaling due to strict resource
constraints, thus, motivating more flexible scaling behaviors using multiple
elasticity dimensions. This work introduces an agent-based autoscaling
framework that dynamically adjusts both hardware resources and internal service
configurations to maximize requirements fulfillment in constrained
environments. We compare four types of scaling agents: Active Inference, Deep Q
Network, Analysis of Structural Knowledge, and Deep Active Inference, using two
real-world processing services running in parallel: YOLOv8 for visual
recognition and OpenCV for QR code detection. Results show all agents achieve
acceptable SLO performance with varying convergence patterns. While the Deep Q
Network benefits from pre-training, the structural analysis converges quickly,
and the deep active inference agent combines theoretical foundations with
practical scalability advantages. Our findings provide evidence for the
viability of multi-dimensional agent-based autoscaling for edge environments
and encourage future work in this research direction.

</details>


### [102] [System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers](https://arxiv.org/abs/2506.10708)
*Michael Bartholomew,Joohyung Lee*

Main category: cs.AI

TL;DR: ASPMT结合了答案集编程和可满足性模理论，提出了一种称为aspsmt2smt的编译器，将ASPMT程序转换为SMT实例，利用gringo和z3求解器处理连续变化。


<details>
  <summary>Details</summary>
Motivation: 研究ASPMT的目的是为了结合答案集编程和可满足性模理论，以解决涉及连续变化的计算问题。

Method: 通过aspsmt2smt编译器实现ASPMT到SMT的转换，使用gringo部分地处理变量，剩余部分由z3求解器完成。

Result: 系统能有效处理涉及实数计算的连续变化问题，展现了实用性和效率。

Conclusion: ASPMT及其工具aspsmt2smt为连续变化推理提供了可行的解决方案，结合了两种技术的优势。

Abstract: Answer Set Programming Modulo Theories (ASPMT) is an approach to combining
answer set programming and satisfiability modulo theories based on the
functional stable model semantics. It is shown that the tight fragment of ASPMT
programs can be turned into SMT instances, thereby allowing SMT solvers to
compute stable models of ASPMT programs. In this paper we present a compiler
called {\sc aspsmt2smt}, which implements this translation. The system uses ASP
grounder {\sc gringo} and SMT solver {\sc z3}. {\sc gringo} partially grounds
input programs while leaving some variables to be processed by {\sc z3}. We
demonstrate that the system can effectively handle real number computations for
reasoning about continuous changes.

</details>


### [103] [Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning](https://arxiv.org/abs/2506.10585)
*Mohd Anwar Jamal Faiz*

Main category: cs.AI

TL;DR: 本文介绍了Primender序列，一种结合素数特性和模数数字条件的新型整数序列，用于评估大型语言模型的符号推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发可解释、基于规则的测试平台，以评估大型语言模型在推断隐藏规则、验证数学假设和推广符号逻辑方面的能力。

Method: 设计了结构化提示和评估框架，测试多种先进语言模型在识别规则、验证假设和生成序列方面的表现。

Result: 通过比较模型的规则推断准确性、假设验证能力和序列生成质量，评估了模型在符号推理和模式归纳中的表现。

Conclusion: 本研究提出了新的数学构造和可复现的方法论，用于基准测试语言模型在符号推理和假设验证中的能力，连接了数论、人工智能和软件工程的领域。

Abstract: This paper introduces the Primender sequence, a novel integer sequence
defined by a hybrid rule that combines classical primality with modular
digit-based conditions. Specifically, a number n is included in the sequence if
it is prime or ends with a prime number of unit digit or any length. In other
words, numbers which are primes or have at least one prime suffix. The
resulting sequence exhibits a deterministic yet non-trivial structure, blending
number-theoretic properties with symbolic patterning. We propose the Primender
sequence as a benchmark for evaluating the symbolic reasoning capabilities of
Large Language Models (LLMs). The study is motivated by the need for
interpretable, rule-based testbeds that can assess an LLM's ability to infer
hidden rules, validate mathematical hypotheses, and generalize symbolic logic
at scale. A key hypothesis explored is: Whenever a number in the Primender
sequence is exactly one more than the largest prime less than or equal to it,
the difference between it and the previous number in the sequence is also 1. We
design a structured prompt and evaluation framework to test this hypothesis
across multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,
Gemini, Grok, and LLaMA. The models are tasked with identifying the underlying
rule, validating the hypothesis, and generating the next 100,000 terms of the
sequence. Comparative metrics such as rule inference accuracy, hypothesis
evaluation, sequence validity, and symbolic explanation quality are used to
assess model performance. This work contributes a novel mathematical construct
and a reproducible methodology for benchmarking LLMs in symbolic reasoning,
hypothesis testing, and scalable pattern generalization - bridging the domains
of number theory, artificial intelligence, and software engineering.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [104] [The Iris File Extension](https://arxiv.org/abs/2506.10009)
*Ryan Erik Landvater,Michael David Olp,Mustafa Yousif,Ulysses Balis*

Main category: eess.IV

TL;DR: 介绍了一种名为Iris的新型二进制幻灯片格式，旨在解决数字病理学中高效实时传输和显示的需求，支持现代压缩和注释功能。


<details>
  <summary>Details</summary>
Motivation: 现有DICOM标准在存储全片图像时存在尺寸限制，影响检索速度，需要一种性能优化的中间格式。

Method: 提出了Iris文件扩展，提供动态结构、现代压缩支持、文件验证和损坏恢复功能，并开源了相关代码。

Result: 实现了高效的即时访问和跨平台支持，提供多种语言绑定，便于集成到现有系统中。

Conclusion: Iris格式填补了数字病理学领域的空白，为高效数据传输和存储提供了新解决方案。

Abstract: A modern digital pathology vendor-agnostic binary slide format specifically
targeting the unmet need of efficient real-time transfer and display has not
yet been established. Growing adoption of digital pathology only intensifies
the need for an intermediary digital slide format with an emphasis on
performance for use between slide servers and image management software or for
inter-institutional transmission of cases. Although the DICOM standard is a
well-established format widely used for long-term storage of both images and
critically associated metadata, its inherent limitations on maximum image
dimensions can impact retrieval speed, particularly when accessing whole slide
images using a pyramidal structure of slide viewer applications. Here, we
introduce the Iris file extension, a binary file container specification
explicitly designed for whole slide image systems that can abstract the file
structure outline into memory for immediate tile access. The Iris file
extension adds modern compression support, a dynamic structure with optional
file features, computationally trivial deep file validation and corruption
recovery capabilities, and slide annotation support. In addition to the file
specification document, we provide source code to allow for (de)serialization
and validation of a binary stream against the standard and corresponding binary
builds with C++, Python, and JavaScript language bindings. We further provide
full encoder and decoder implementation source code, as well as binary builds
(as part of the separate Iris Codec Community module) with language bindings
for C++ and Python to allow for easy integration with existing WSI solutions.
We provide the Iris File Extension specification openly to the community in the
form of a Creative Commons Attribution-No Derivative 4.0 international license.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [105] [LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation](https://arxiv.org/abs/2506.10235)
*Chen-Chia Chang,Wan-Hsuan Lin,Yikang Shen,Yiran Chen,Xin Zhang*

Main category: cs.LG

TL;DR: LaMAGIC2通过改进标识符表示和降低复杂度，提升了模拟拓扑生成的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现代应用对模拟拓扑设计的需求日益定制化，但现有方法因复杂度高和数值精度问题表现不佳。

Method: 提出SFCI表示法，降低复杂度至O(|V|)，并提升数值精度敏感性。

Result: LaMAGIC2在0.01严格容差下成功率提升34%，MSE降低10倍，且在复杂电路中表现更优。

Conclusion: LaMAGIC2为模拟拓扑生成提供了高效且鲁棒的解决方案。

Abstract: Automation of analog topology design is crucial due to customized
requirements of modern applications with heavily manual engineering efforts.
The state-of-the-art work applies a sequence-to-sequence approach and
supervised finetuning on language models to generate topologies given user
specifications. However, its circuit formulation is inefficient due to O(|V |2)
token length and suffers from low precision sensitivity to numeric inputs. In
this work, we introduce LaMAGIC2, a succinct float-input canonical formulation
with identifier (SFCI) for language model-based analog topology generation.
SFCI addresses these challenges by improving component-type recognition through
identifier-based representations, reducing token length complexity to O(|V |),
and enhancing numeric precision sensitivity for better performance under tight
tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher
success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a
prior method. LaMAGIC2 also exhibits better transferability for circuits with
more vertices with up to 58.5% improvement. These advancements establish
LaMAGIC2 as a robust framework for analog topology generation.

</details>


### [106] [Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion](https://arxiv.org/abs/2506.09999)
*Yukun Chen,Zihuan Qiu,Fanman Meng,Hongliang Li,Linfeng Xu,Qingbo Wu*

Main category: cs.LG

TL;DR: 本文提出一种基于多模态预训练模型的MCIL方法，通过MIFE模块、AAVFM模块、对比训练损失和新评估指标，解决多模态增量学习中的信息整合和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 传统MCIL方法仅限于视觉和文本模态，本文探索跨视觉、音频和文本模态的MCIL，以解决互补信息整合和灾难性遗忘的挑战。

Method: 提出基于MoE结构的MIFE模块进行增量微调，设计AAVFM模块增强特征判别性，引入对比训练损失优化跨模态对齐，并开发两项新评估指标。

Result: 在三个多模态数据集上的实验验证了方法的有效性。

Conclusion: 该方法在多模态增量学习中表现出色，解决了信息整合和灾难性遗忘问题。

Abstract: Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that
focus only on vision and text, this paper explores MCIL across vision, audio
and text modalities, addressing challenges in integrating complementary
information and mitigating catastrophic forgetting. To tackle these issues, we
propose an MCIL method based on multimodal pre-trained models. Firstly, a
Multimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts
(MoE) structure is introduced to achieve effective incremental fine-tuning for
AudioCLIP. Secondly, to enhance feature discriminability and generalization, we
propose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking
threshold mechanism and a dynamic feature fusion mechanism, along with a
strategy to enhance text diversity. Thirdly, a novel multimodal
class-incremental contrastive training loss is proposed to optimize cross-modal
alignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced
for comprehensive assessment. Extensive experiments on three multimodal
datasets validate the effectiveness of our method.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [107] [Collective Bargaining in the Information Economy Can Address AI-Driven Power Concentration](https://arxiv.org/abs/2506.10272)
*Nicholas Vincent,Matthew Prewitt,Hanlin Li*

Main category: cs.CY

TL;DR: 本文主张需要重组AI系统信息市场，提倡信息生产者集体议价以避免市场失败和资源集中，并提出技术和政策支持方案。


<details>
  <summary>Details</summary>
Motivation: AI系统的发展依赖大量信息，但当前市场结构可能导致信息生产者利益受损，甚至引发信息生态崩溃，需重新协调市场机制。

Method: 提出信息生产者通过集体议价与AI开发者谈判，技术上支持联邦数据管理工具和数据估值，政策上推动可信中介组织。

Result: 集体议价和市场化协调可避免资本集中和信息生态崩溃，促进可持续AI发展。

Conclusion: 通过技术和政策支持信息生产者集体议价，是实现AI可持续发展和社会利益最大化的关键。

Abstract: This position paper argues that there is an urgent need to restructure
markets for the information that goes into AI systems. Specifically, producers
of information goods (such as journalists, researchers, and creative
professionals) need to be able to collectively bargain with AI product builders
in order to receive reasonable terms and a sustainable return on the
informational value they contribute. We argue that without increased market
coordination or collective bargaining on the side of these primary information
producers, AI will exacerbate a large-scale "information market failure" that
will lead not only to undesirable concentration of capital, but also to a
potential "ecological collapse" in the informational commons. On the other
hand, collective bargaining in the information economy can create market
frictions and aligned incentives necessary for a pro-social, sustainable AI
future. We provide concrete actions that can be taken to support a
coalition-based approach to achieve this goal. For example, researchers and
developers can establish technical mechanisms such as federated data management
tools and explainable data value estimations, to inform and facilitate
collective bargaining in the information economy. Additionally, regulatory and
policy interventions may be introduced to support trusted data intermediary
organizations representing guilds or syndicates of information producers.

</details>


### [108] [The Urban Model Platform: A Public Backbone for Modeling and Simulation in Urban Digital Twins](https://arxiv.org/abs/2506.10964)
*Rico H Herzog,Till Degkwitz,Trivik Verma*

Main category: cs.CY

TL;DR: 论文探讨了城市数字孪生中开放城市模型平台的重要性，以支持多元化和协作的城市规划。


<details>
  <summary>Details</summary>
Motivation: 当前城市数字孪生模型多为集中式，限制了模型的多元化和开放性，缺乏对复杂城市系统和多方利益的包容性。

Method: 通过参与式设计方法，与汉堡市合作开发了一种开放的Urban Model Platform，采用开放标准和多模型方法。

Result: 该平台既能作为城市数字孪生的技术基础，又能支持协作和多元化的城市过程表示。

Conclusion: 开放城市模型平台是实现可持续和集成城市规划的有效途径。

Abstract: Urban digital twins are increasingly perceived as a way to pool the growing
digital resources of cities for the purpose of a more sustainable and
integrated urban planning. Models and simulations are central to this
undertaking: They enable "what if?" scenarios, create insights and describe
relationships between the vast data that is being collected. However, the
process of integrating and subsequently using models in urban digital twins is
an inherently complex undertaking. It raises questions about how to represent
urban complexity, how to deal with uncertain assUrban Model Platformtions and
modeling paradigms, and how to capture underlying power relations. Existent
approaches in the domain largely focus on monolithic and centralized solutions
in the tradition of neoliberal city-making, oftentimes prohibiting pluralistic
and open interoperable models. Using a participatory design for participatory
systems approach together with the City of Hamburg, Germany, we find that an
open Urban Model Platform can function both as a public technological backbone
for modeling and simulation in urban digital twins and as a socio-technical
framework for a collaborative and pluralistic representation of urban
processes. Such a platform builds on open standards, allows for a decentralized
integration of models, enables communication between models and supports a
multi-model approach to representing urban systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [109] [Quantum resources in resource management systems](https://arxiv.org/abs/2506.10052)
*Iskandar Sitdikov,M. Emre Sahin,Utz Bacher,Aleksander Wennersteen,Andrew Damin,Mark Birmingham,Philippa Rubin,Stefano Mensa,Matthieu Moreau,Aurelien Nober,Hitomi Takahashi,Munetaka Ohtani*

Main category: quant-ph

TL;DR: 该论文讨论了将量子计算机集成到高性能计算（HPC）环境中的设计架构和实现方法，通过Slurm插件实现量子资源的统一调度。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算机的出现，如何将其与现有HPC基础设施集成以支持混合量子-经典应用成为关键问题。

Method: 论文提出了一套Slurm插件，实现本地和云端量子计算资源的集成，并详细介绍了接口设计和实现。

Result: 实现了量子资源与HPC环境的无缝集成，支持统一调度和混合应用。

Conclusion: 通过插件设计的灵活性和兼容性，为量子计算在HPC中的广泛应用奠定了基础。

Abstract: Quantum computers are beginning to operate in high-performance computing
(HPC) environments. Quantum can complement classical resources for specific
workloads, but their adoption depends on integration into existing HPC
infrastructure. Treating quantum devices as first-class resources allows for
unified scheduling, improved usability, and support for hybrid
quantum-classical applications. This paper presents the design architecture and
reference implementation for quantum resources control using existing workload
management systems. We introduce a suite of plugins for Slurm that enable
integration of on-prem and cloud quantum computing resources into existing
high-performance computing centers. The paper details the interface design,
plugin concept and implementation, operational aspects for heterogeneous
compute clusters, as well as considerations for other resource management
systems.

</details>


### [110] [Synchronization for Fault-Tolerant Quantum Computers](https://arxiv.org/abs/2506.10258)
*Satvik Maurya,Swamit Tannu*

Main category: quant-ph

TL;DR: 量子纠错码（QEC）通过将逻辑量子比特编码到更多不可靠的量子比特中来可靠存储信息。表面码因其对物理错误的高容忍度成为容错量子计算（FTQC）的主要候选方案，但由于逻辑量子比特在同步问题上的不同步，可能导致操作效率降低。为了解决同步问题，研究者提出了三种策略：被动、主动和混合策略，其中混合策略效果最佳，能将逻辑错误率降低3.4倍，解码延迟降低2.2倍。


<details>
  <summary>Details</summary>
Motivation: 表面码逻辑量子比特的同步问题可能导致操作效率降低和错误率上升，因此需要研究有效的同步策略以提高系统性能。

Method: 提出三种同步策略（被动、主动和混合策略），通过调整逻辑量子比特的同步周期来减少错误累积，并评估其对逻辑错误率和解码延迟的影响。

Result: 混合策略表现最佳，逻辑错误率降低3.4倍，解码延迟降低2.2倍。

Conclusion: 有效的同步策略可以显著提高容错量子计算的效率和可靠性，混合策略是当前最优解决方案。

Abstract: Quantum Error Correction (QEC) codes store information reliably in logical
qubits by encoding them in a larger number of less reliable qubits. The surface
code, known for its high resilience to physical errors, is a leading candidate
for fault-tolerant quantum computing (FTQC). Logical qubits encoded with the
surface code can be in different phases of their syndrome generation cycle,
thereby introducing desynchronization in the system. This can occur due to the
production of non-Clifford states, dropouts due to fabrication defects, and the
use of other QEC codes with the surface code to reduce resource requirements.
Logical operations require the syndrome generation cycles of the logical qubits
involved to be synchronized. This requires the leading qubit to pause or slow
down its cycle, allowing more errors to accumulate before the next cycle,
thereby increasing the risk of uncorrectable errors.
  To synchronize the syndrome generation cycles of logical qubits, we define
three policies - Passive, Active, and Hybrid. The Passive policy is the
baseline, and the simplest, wherein the leading logical qubits idle until they
are synchronized with the remaining logical qubits. On the other hand, the
Active policy aims to slow the leading logical qubits down gradually, by
inserting short idle periods before multiple code cycles. This approach reduces
the logical error rate (LER) by up to 2.4x compared to the Passive policy. The
Hybrid policy further reduces the LER by up to 3.4x by reducing the
synchronization slack and running a few additional rounds of error correction.
Furthermore, the reduction in the logical error rate with the proposed
synchronization policies enables a speedup in decoding latency of up to 2.2x
with a circuit-level noise model.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [111] [A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild](https://arxiv.org/abs/2506.10117)
*Klim Kireev,Ana-Maria Creţu,Raphael Meier,Sarah Adel Bargal,Elissa Redmiles,Carmela Troncoso*

Main category: cs.CV

TL;DR: 论文介绍了首个多模态环境下检测未成年内容的图像-字幕数据集ICCWD，包含1万条手动标注数据，用于评估检测工具的性能。实验表明当前最佳方法的准确率为75.3%，任务具有一定挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有工具在多模态环境下检测未成年人内容的性能缺乏标准化评估，数据集稀缺。

Method: 发布名为ICCWD的图像-字幕数据集，包含多样化的未成年图像场景，并手动标注。基于该数据集评估了三种检测器，包括商业年龄估计系统。

Result: 最佳检测方法的真阳性率为75.3%，显示任务具有挑战性。

Conclusion: ICCWD数据集有助于未来设计更高效的未成年人内容检测方法。

Abstract: Platforms and the law regulate digital content depicting minors (defined as
individuals under 18 years of age) differently from other types of content.
Given the sheer amount of content that needs to be assessed, machine
learning-based automation tools are commonly used to detect content depicting
minors. To our knowledge, no dataset or benchmark currently exists for
detecting these identification methods in a multi-modal environment. To fill
this gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an
image-caption dataset aimed at benchmarking tools that detect depictions of
minors. Our dataset is richer than previous child image datasets, containing
images of children in a variety of contexts, including fictional depictions and
partially visible bodies. ICCWD contains 10,000 image-caption pairs manually
labeled to indicate the presence or absence of a child in the image. To
demonstrate the possible utility of our dataset, we use it to benchmark three
different detectors, including a commercial age estimation system applied to
images. Our results suggest that child detection is a challenging task, with
the best method achieving a 75.3% true positive rate. We hope the release of
our dataset will aid in the design of better minor detection methods in a wide
range of scenarios.

</details>


### [112] [From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations](https://arxiv.org/abs/2506.10559)
*Yutong Zhou,Masahiro Ryo*

Main category: cs.CV

TL;DR: 提出一种端到端的视觉到因果框架，将物种图像转化为可解释的栖息地偏好因果见解。


<details>
  <summary>Details</summary>
Motivation: 理解生态系统的物种分布对保护生物多样性至关重要，但现有方法分散且不易为非专家所用。

Method: 整合物种识别、全球分布检索、伪缺失采样和气候数据提取，结合因果推理方法分析环境特征的影响。

Result: 通过蜜蜂和花卉的案例展示了框架的潜力，生成人类可理解的因果解释。

Conclusion: 该框架结合多模态AI，为描述物种栖息地提供了统计支持的生态建模实践。

Abstract: Explaining why the species lives at a particular location is important for
understanding ecological systems and conserving biodiversity. However, existing
ecological workflows are fragmented and often inaccessible to non-specialists.
We propose an end-to-end visual-to-causal framework that transforms a species
image into interpretable causal insights about its habitat preference. The
system integrates species recognition, global occurrence retrieval,
pseudo-absence sampling, and climate data extraction. We then discover causal
structures among environmental features and estimate their influence on species
occurrence using modern causal inference methods. Finally, we generate
statistically grounded, human-readable causal explanations from structured
templates and large language models. We demonstrate the framework on a bee and
a flower species and report early results as part of an ongoing project,
showing the potential of the multimodal AI assistant backed up by a recommended
ecological modeling practice for describing species habitat in
human-understandable language.

</details>


### [113] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成式人工智能的方法，通过结合Stable Diffusion、GPT-2和混合音频管道，实现了从文本输入生成高质量60秒电影视频的技术。


<details>
  <summary>Details</summary>
Motivation: 利用生成式人工智能技术提升多媒体创作效率，满足创意、教育和工业领域的需求。

Method: 采用五场景框架，结合线性帧插值、电影级后处理和音视频同步，利用Python 3.11在GPU加速环境中实现。

Result: 实验展示了出色的视觉质量、叙事连贯性和效率，证明了技术的可行性。

Conclusion: 该方法在文本到视频合成领域具有广泛应用潜力。

Abstract: Advances in generative artificial intelligence have altered multimedia
creation, allowing for automatic cinematic video synthesis from text inputs.
This work describes a method for creating 60-second cinematic movies
incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for
narrative structuring, and a hybrid audio pipeline using gTTS and
YouTube-sourced music. It uses a five-scene framework, which is augmented by
linear frame interpolation, cinematic post-processing (e.g., sharpening), and
audio-video synchronization to provide professional-quality results. It was
created in a GPU-accelerated Google Colab environment using Python 3.11. It has
a dual-mode Gradio interface (Simple and Advanced), which supports resolutions
of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA
memory management and error handling ensure reliability. The experiments
demonstrate outstanding visual quality, narrative coherence, and efficiency,
furthering text-to-video synthesis for creative, educational, and industrial
applications.

</details>


### [114] [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/abs/2506.10452)
*Guowei Zhong,Ruohong Huan,Mingzhen Wu,Ronghua Liang,Peng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为CIDer的多模态情感识别框架，通过随机模态特征缺失任务和因果推断模块，解决了模态缺失和分布外数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前多模态情感识别方法在处理模态缺失和分布外数据时存在局限性，需要更鲁棒的解决方案。

Method: CIDer包含模型特定的自蒸馏模块和模型无关的因果推断模块，结合注意力机制和模态融合策略。

Result: 实验表明，CIDer在模态缺失和分布外场景下具有鲁棒性，且参数少、训练快。

Conclusion: CIDer为多模态情感识别提供了一种高效且鲁棒的解决方案。

Abstract: Recent advancements in Multimodal Emotion Recognition (MER) face challenges
in addressing both modality missing and Out-Of-Distribution (OOD) data
simultaneously. Existing methods often rely on specific models or introduce
excessive parameters, which limits their practicality. To address these issues,
we propose a novel robust MER framework, Causal Inference Distiller (CIDer),
and introduce a new task, Random Modality Feature Missing (RMFM), to generalize
the definition of modality missing. CIDer integrates two key components: a
Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal
Inference (MACI) module. MSSD enhances robustness under the RMFM task through a
weight-sharing self-distillation approach applied across low-level features,
attention maps, and high-level representations. Additionally, a Word-level
Self-aligned Attention Module (WSAM) reduces computational complexity, while a
Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.
To tackle OOD challenges, MACI employs a tailored causal graph to mitigate
label and language biases using a Multimodal Causal Module (MCM) and
fine-grained counterfactual texts. Notably, MACI can independently enhance OOD
generalization with minimal additional parameters. Furthermore, we also
introduce the new repartitioned MER OOD datasets. Experimental results
demonstrate that CIDer achieves robust performance in both RMFM and OOD
scenarios, with fewer parameters and faster training compared to
state-of-the-art methods. The implementation of this work is publicly
accessible at https://github.com/gw-zhong/CIDer.

</details>


### [115] [DanceChat: Large Language Model-Guided Music-to-Dance Generation](https://arxiv.org/abs/2506.10574)
*Qing Wang,Xiaohang Yang,Yilan Dong,Naveen Raj Govindaraj,Gregory Slabaugh,Shanxin Yuan*

Main category: cs.CV

TL;DR: Music-to-dance generation faces challenges due to the semantic gap between music and dance, and the scarcity of paired data. DanceChat uses an LLM as a choreographer to provide textual motion instructions, improving diversity and musical alignment.


<details>
  <summary>Details</summary>
Motivation: The semantic gap and one-to-many mapping between music and dance require additional guidance beyond music alone. Existing methods lack diversity and alignment with musical styles due to limited data.

Method: DanceChat includes an LLM-based pseudo instruction generator, multi-modal feature fusion, and a diffusion-based motion synthesis module with alignment loss.

Result: Experiments on AIST++ and human evaluations show DanceChat outperforms state-of-the-art methods in quality and diversity.

Conclusion: DanceChat successfully bridges the music-dance semantic gap by leveraging LLM-guided textual instructions, enhancing both diversity and alignment in generated dance motions.

Abstract: Music-to-dance generation aims to synthesize human dance motion conditioned
on musical input. Despite recent progress, significant challenges remain due to
the semantic gap between music and dance motion, as music offers only abstract
cues, such as melody, groove, and emotion, without explicitly specifying the
physical movements. Moreover, a single piece of music can produce multiple
plausible dance interpretations. This one-to-many mapping demands additional
guidance, as music alone provides limited information for generating diverse
dance movements. The challenge is further amplified by the scarcity of paired
music and dance data, which restricts the model\^a\u{A}\'Zs ability to learn
diverse dance patterns. In this paper, we introduce DanceChat, a Large Language
Model (LLM)-guided music-to-dance generation approach. We use an LLM as a
choreographer that provides textual motion instructions, offering explicit,
high-level guidance for dance generation. This approach goes beyond implicit
learning from music alone, enabling the model to generate dance that is both
more diverse and better aligned with musical styles. Our approach consists of
three components: (1) an LLM-based pseudo instruction generation module that
produces textual dance guidance based on music style and structure, (2) a
multi-modal feature extraction and fusion module that integrates music, rhythm,
and textual guidance into a shared representation, and (3) a diffusion-based
motion synthesis module together with a multi-modal alignment loss, which
ensures that the generated dance is aligned with both musical and textual cues.
Extensive experiments on AIST++ and human evaluations show that DanceChat
outperforms state-of-the-art methods both qualitatively and quantitatively.

</details>


### [116] [VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos](https://arxiv.org/abs/2506.10857)
*Jiashuo Yu,Yue Wu,Meng Chu,Zhifei Ren,Zizheng Huang,Pei Chu,Ruijie Zhang,Yinan He,Qirui Li,Songze Li,Zhenxiang Li,Zhongying Tu,Conghui He,Yu Qiao,Yali Wang,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: VRBench是首个专注于评估大模型多步推理能力的长期叙事视频基准，包含大量带时间戳的视频和问题对，通过多维评估提供深入分析。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法忽视了时间推理和程序有效性，因此设计了VRBench以解决这一问题。

Method: 开发了包含长视频和多步问题对的基准，采用多阶段过滤和专家评审确保质量，并设计了多维度评估流程。

Result: 评估了12个LLM和16个VLM，提供了关于多步推理的重要见解。

Conclusion: VRBench为多步推理领域提供了全面的评估工具和宝贵的分析结果。

Abstract: We present VRBench, the first long narrative video benchmark crafted for
evaluating large models' multi-step reasoning capabilities, addressing
limitations in existing evaluations that overlook temporal reasoning and
procedural validity. It comprises 1,010 long videos (with an average duration
of 1.6 hours), along with 9,468 human-labeled multi-step question-answering
pairs and 30,292 reasoning steps with timestamps. These videos are curated via
a multi-stage filtering process including expert inter-rater reviewing to
prioritize plot coherence. We develop a human-AI collaborative framework that
generates coherent reasoning chains, each requiring multiple temporally
grounded steps, spanning seven types (e.g., event attribution, implicit
inference). VRBench designs a multi-phase evaluation pipeline that assesses
models at both the outcome and process levels. Apart from the MCQs for the
final results, we propose a progress-level LLM-guided scoring metric to
evaluate the quality of the reasoning chain from multiple dimensions
comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on
VRBench, we undertake a thorough analysis and provide valuable insights that
advance the field of multi-step reasoning.

</details>


### [117] [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
*Leigang Qu,Feng Cheng,Ziyan Yang,Qi Zhao,Shanchuan Lin,Yichun Shi,Yicong Li,Wenjie Wang,Tat-Seng Chua,Lu Jiang*

Main category: cs.CV

TL;DR: 本文探索了通过视频数据直接训练上下文图像编辑模型的方法，提出了一种可扩展的多模态序列标注方法和块因果扩散变换器，并在多轮图像编辑任务中取得了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文图像编辑方法依赖任务特定流程和专家模型，限制了灵活性和可扩展性。本文旨在直接从视频数据中学习模型，以提高通用性和性能。

Method: 提出了一种可扩展的方法，将视频标注为多模态序列，并设计了一个块因果扩散变换器，通过预测下一图像、当前分割和下一分割三个代理任务进行训练。

Result: 模型在上下文图像编辑任务中表现优异，在两个多轮图像编辑基准测试中达到最佳效果，同时在多概念合成、故事生成和链式编辑中展示了潜力。

Conclusion: 直接从视频中学习的模型不仅能够高效完成图像编辑任务，还具有广泛的应用前景，为上下文图像编辑提供了新的研究方向。

Abstract: In-context image editing aims to modify images based on a contextual sequence
comprising text and previously generated images. Existing methods typically
depend on task-specific pipelines and expert models (e.g., segmentation and
inpainting) to curate training data. In this work, we explore whether an
in-context image editing model can be learned directly from videos. We
introduce a scalable approach to annotate videos as interleaved multimodal
sequences. To effectively learn from this data, we design a block-causal
diffusion transformer trained on three proxy tasks: next-image prediction,
current segmentation prediction, and next-segmentation prediction.
Additionally, we propose a novel multi-turn image editing benchmark to advance
research in this area. Extensive experiments demonstrate that our model
exhibits strong in-context image editing capabilities and achieves
state-of-the-art results on two multi-turn image editing benchmarks. Despite
being trained exclusively on videos, our model also shows promising abilities
in multi-concept composition, story generation, and chain-of-editing
applications.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [118] [Receiving RISs: Enabling Channel Estimation and Autonomous Configuration](https://arxiv.org/abs/2506.10662)
*George C. Alexandropoulos,Konstantinos D. Katsanos,Evangelos Vlachos*

Main category: eess.SP

TL;DR: 提出了一种半无源可重构智能表面（RIS）硬件架构，用于提升MIMO通信系统性能，包括非正交训练导频序列的通道估计算法和优化反射系数的控制器设计。


<details>
  <summary>Details</summary>
Motivation: 提升MIMO系统性能，尤其是FR3及以上频段的通信系统，通过RIS的动态反射系数优化和高效通道估计。

Method: 采用基于ADMM的通道估计算法，利用RIS随机空间吸收采样和MIMO通道的稀疏性及低秩特性。

Result: 数值实验表明，提出的通道估计技术优于基准方案，且动态优化RIS反射系数有效。

Conclusion: 所提架构和算法显著提升MIMO系统性能，特别适用于高频段通信。

Abstract: This chapter focuses on a hardware architecture for semi-passive
Reconfigurable Intelligent Surfaces (RISs) and investigates its consideration
for boosting the performance of Multiple-Input Multiple-Output (MIMO)
communication systems. The architecture incorporates a single or multiple
radio-frequency chains to receive pilot signals via tunable absorption phase
profiles realized by the metasurface front end, as well as a controller
encompassing a baseband processing unit to carry out channel estimation, and
consequently, the optimization of the RIS reflection coefficients. A novel
channel estimation protocol, according to which the RIS receives non-orthogonal
training pilot sequences from two multi-antenna terminals via tunable
absorption phase profiles, and then, estimates the respective channels via its
signal processing unit, is presented. The channel estimates are particularly
used by the RIS controller to design the capacity-achieving reflection phase
configuration of the metasurface front end. The proposed channel estimation
algorithm, which is based on the Alternating Direction Method of Multipliers
(ADMM), profits from the RIS random spatial absorption sampling to capture the
entire signal space, and exploits the beamspace sparsity and low-rank
properties of extremely large MIMO channels, which is particularly relevant for
communication systems at the FR3 band and above. Our extensive numerical
investigations showcase the superiority of the proposed channel estimation
technique over benchmark schemes for various system and RIS hardware
configuration parameters, as well as the effectiveness of using channel
estimates at the RIS side to dynamically optimize the possibly phase-quantized
reflection coefficients of its unit elements.

</details>


### [119] [Ground Reaction Force Estimation via Time-aware Knowledge Distillation](https://arxiv.org/abs/2506.10265)
*Eun Som Jeon,Sinjini Mitra,Jisoo Lee,Omik M. Save,Ankita Shukla,Hyunglae Lee,Pavan Turaga*

Main category: eess.SP

TL;DR: 该论文提出了一种时间感知的知识蒸馏框架，用于从鞋垫传感器数据中估计地面反作用力（GRF），解决了便携式传感器噪声和准确性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 便携式鞋垫传感器虽然成本低且便携，但易受噪声干扰且准确性不如传统仪器跑步机测量。因此，需要一种新方法来提高其GRF估计的准确性。

Method: 提出时间感知知识蒸馏框架，利用小批量中的相似性和时间特征，捕捉目标数据与输入数据之间的互补关系和序列特性。

Result: 实验表明，该框架在GRF估计上优于现有基线方法，验证了其有效性。

Conclusion: 时间感知知识蒸馏为便携式传感器提供了一种高精度的GRF估计方案，扩展了其应用潜力。

Abstract: Human gait analysis with wearable sensors has been widely used in various
applications, such as daily life healthcare, rehabilitation, physical therapy,
and clinical diagnostics and monitoring. In particular, ground reaction force
(GRF) provides critical information about how the body interacts with the
ground during locomotion. Although instrumented treadmills have been widely
used as the gold standard for measuring GRF during walking, their lack of
portability and high cost make them impractical for many applications. As an
alternative, low-cost, portable, wearable insole sensors have been utilized to
measure GRF; however, these sensors are susceptible to noise and disturbance
and are less accurate than treadmill measurements. To address these challenges,
we propose a Time-aware Knowledge Distillation framework for GRF estimation
from insole sensor data. This framework leverages similarity and temporal
features within a mini-batch during the knowledge distillation process,
effectively capturing the complementary relationships between features and the
sequential properties of the target and input data. The performance of the
lightweight models distilled through this framework was evaluated by comparing
GRF estimations from insole sensor data against measurements from an
instrumented treadmill. Empirical results demonstrated that Time-aware
Knowledge Distillation outperforms current baselines in GRF estimation from
wearable sensor data.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [120] [Faster CONGEST Approximation Algorithms for Maximum Weighted Independent Set in Sparse Graphs](https://arxiv.org/abs/2506.10845)
*Salwa Faour,Fabian Kuhn*

Main category: cs.DS

TL;DR: 该论文研究了加权最大独立集问题在树和有界树性图中的确定性分布式CONGEST算法，给出了紧的复杂度界和改进的近似算法。


<details>
  <summary>Details</summary>
Motivation: 最大独立集问题是经典的优化问题，但在分布式环境下研究较少。论文旨在填补这一空白，特别是在树和有界树性图中的加权版本。

Method: 对于树，证明了确定性计算（1-ε）-近似解具有紧复杂度Θ(log*(n)/ε)。对于有界树性图，提出了两种近似算法，分别基于局部舍入框架和改进的Gil算法。

Result: 在树中，算法复杂度是紧的；在有界树性图中，提出的算法优于近期结果，提供了更高的近似比和更低的复杂度。

Conclusion: 论文为加权最大独立集问题提供了高效的确定性分布式算法，扩展了稀疏图上的近似解方法。

Abstract: The maximum independent set problem is a classic optimization problem that
has also been studied quite intensively in the distributed setting. While the
problem is hard to approximate in general, there are good approximation
algorithms known for several sparse graph families. In this paper, we consider
deterministic distributed CONGEST algorithms for the weighted version of the
problem in trees and graphs of bounded arboricity.
  For trees, we prove that the task of deterministically computing a
$(1-\epsilon)$-approximate solution to the maximum weight independent set
(MWIS) problem has a tight $\Theta(\log^*(n) / \epsilon)$ complexity. The lower
bound already holds on unweighted oriented paths. On the upper bound side, we
show that the bound can be achieved even in unrooted trees.
  For graphs $G=(V,E)$ of arboricity $\beta>1$, we give two algorithms. If the
sum of all node weights is $w(V)$, we show that for any $\epsilon>0$, an
independent set of weight at least $(1-\epsilon)\cdot \frac{w(V)}{4\beta}$ can
be computed in $O(\log^2(\beta/\epsilon)/\epsilon + \log^* n)$ rounds. This
result is obtained by a direct application of the local rounding framework of
Faour, Ghaffari, Grunau, Kuhn, and Rozho\v{n} [SODA '23]. We further show that
for any $\epsilon>0$, an independent set of weight at least
$(1-\epsilon)\cdot\frac{w(V)}{2\beta+1}$ can be computed in
$O(\log^3(\beta)\cdot\log(1/\epsilon)/\epsilon^2 \cdot\log n)$ rounds. This
improves on a recent result of Gil [OPODIS '23], who showed that a
$1/\lfloor(2+\epsilon)\beta\rfloor$-approximation to the MWIS problem can be
computed in $O(\beta\cdot\log n)$ rounds. As an intermediate step, we design an
algorithm to compute an independent set of total weight at least
$(1-\epsilon)\cdot\sum_{v\in V}\frac{w(v)}{deg(v)+1}$ in time
$O(\log^3(\Delta)\cdot\log(1/\epsilon)/\epsilon + \log^* n)$, where $\Delta$ is
the maximum degree of the graph.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [121] [The Gittins Index: A Design Principle for Decision-Making Under Uncertainty](https://arxiv.org/abs/2506.10872)
*Ziv Scully,Alexander Terenin*

Main category: math.OC

TL;DR: 该教程旨在展示Gittins指数在实际问题中的应用，通过示例介绍其功能，并展示其在最优和次优问题中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管Gittins指数在理论上被认为是一个重要概念，但其在实际问题中的应用潜力未被充分挖掘。本文旨在证明其实际价值。

Method: 通过示例驱动的介绍，展示Gittins指数在多臂老虎机问题和队列优化等问题中的应用，包括最优和次优场景。

Result: Gittins指数在贝叶斯优化和队列尾延迟最小化等实际问题中表现优异。

Conclusion: Gittins指数不仅具有理论价值，还能有效解决实际问题，尤其是在次优场景中表现突出。

Abstract: The Gittins index is a tool that optimally solves a variety of
decision-making problems involving uncertainty, including multi-armed bandit
problems, minimizing mean latency in queues, and search problems like the
Pandora's box model. However, despite the above examples and later extensions
thereof, the space of problems that the Gittins index can solve perfectly
optimally is limited, and its definition is rather subtle compared to those of
other multi-armed bandit algorithms. As a result, the Gittins index is often
regarded as being primarily a concept of theoretical importance, rather than a
practical tool for solving decision-making problems.
  The aim of this tutorial is to demonstrate that the Gittins index can be
fruitfully applied to practical problems. We start by giving an example-driven
introduction to the Gittins index, then walk through several examples of
problems it solves - some optimally, some suboptimally but still with excellent
performance. Two practical highlights in the latter category are applying the
Gittins index to Bayesian optimization, and applying the Gittins index to
minimizing tail latency in queues.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [122] [Learning Chaotic Dynamics with Neuromorphic Network Dynamics](https://arxiv.org/abs/2506.10773)
*Yinhao Xu,Georg A. Gottwald,Zdenka Kuncic*

Main category: cond-mat.dis-nn

TL;DR: 该研究探讨如何利用自身为动态系统的神经形态网络学习和建模动态系统，通过基于忆阻器的复杂电路实现非线性响应，并在储备计算框架下模拟和评估网络对多元混沌时间序列的自主预测能力。研究发现输入电压最大化忆阻器动态范围时能优化非线性响应，同时输入电极覆盖范围增加会抑制不利于学习的响应，为优化外部控制参数提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何利用神经形态网络的动态特性来学习和建模复杂动态系统，特别关注如何通过外部控制参数优化网络性能。

Method: 采用基于忆阻器的复杂电路构建神经形态网络，并在储备计算框架下模拟该网络对多元混沌时间序列的自主预测能力，通过调整输入电极和电压研究其非线性响应。

Result: 研究发现，当输入电压最大化忆阻器动态范围时，网络表现出最佳非线性响应，而输入电极覆盖范围增加会抑制不利于学习的响应。

Conclusion: 研究结果为利用外部控制参数优化神经形态网络设备提供了实用指导，有助于学习复杂动态系统。

Abstract: This study investigates how dynamical systems may be learned and modelled
with a neuromorphic network which is itself a dynamical system. The
neuromorphic network used in this study is based on a complex electrical
circuit comprised of memristive elements that produce neuro-synaptic nonlinear
responses to input electrical signals. To determine how computation may be
performed using the physics of the underlying system, the neuromorphic network
was simulated and evaluated on autonomous prediction of a multivariate chaotic
time series, implemented with a reservoir computing framework. Through
manipulating only input electrodes and voltages, optimal nonlinear dynamical
responses were found when input voltages maximise the number of memristive
components whose internal dynamics explore the entire dynamical range of the
memristor model. Increasing the network coverage with the input electrodes was
found to suppress other nonlinear responses that are less conducive to
learning. These results provide valuable insights into how a practical
neuromorphic network device can be optimised for learning complex dynamical
systems using only external control parameters.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [123] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
*Aakriti Kumar,Nalin Poungpeth,Diyi Yang,Erina Farrell,Bruce Lambert,Matthew Groh*

Main category: cs.CL

TL;DR: LLMs在评估共情沟通方面的表现接近专家水平，超过了众包工作者。研究通过四种心理学框架比较了专家、众包人员和LLM的注释结果，表明LLM在特定任务中可以提供透明和监督。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在评估共情沟通中的可靠性，是否能够作为情感敏感任务（如对话伴侣）的支持工具。

Method: 使用四种心理学框架，对200个真实对话进行注释，比较专家（3,150条）、众包（2,844条）和LLM（3,150条）的一致性。

Result: 专家一致性高但受框架子组分的清晰度和主观性影响；LLM表现接近专家水平，优于众包。

Conclusion: LLM在特定任务中表现优异，可作为共情沟通的可靠评估工具，支持透明和监督。

Abstract: Large language models (LLMs) excel at generating empathic responses in
text-based conversations. But, how reliably do they judge the nuances of
empathic communication? We investigate this question by comparing how experts,
crowdworkers, and LLMs annotate empathic communication across four evaluative
frameworks drawn from psychology, natural language processing, and
communications applied to 200 real-world conversations where one speaker shares
a personal problem and the other offers support. Drawing on 3,150 expert
annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess
inter-rater reliability between these three annotator groups. We find that
expert agreement is high but varies across the frameworks' sub-components
depending on their clarity, complexity, and subjectivity. We show that expert
agreement offers a more informative benchmark for contextualizing LLM
performance than standard classification metrics. Across all four frameworks,
LLMs consistently approach this expert level benchmark and exceed the
reliability of crowdworkers. These results demonstrate how LLMs, when validated
on specific tasks with appropriate benchmarks, can support transparency and
oversight in emotionally sensitive applications including their use as
conversational companions.

</details>


### [124] [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
*Yixin Ou,Yujie Luo,Jingsheng Zheng,Lanning Wei,Shuofei Qiao,Jintian Zhang,Da Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: AutoMind是一个新型的LLM代理框架，通过专家知识库、树搜索算法和自适应编码策略，显著提升自动化数据科学任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理框架在处理复杂任务时效果有限，缺乏人类专家的灵活性。

Method: 引入专家知识库、树搜索算法和自适应编码策略。

Result: 在两个基准测试中表现优于现有方法，展现高效和鲁棒性。

Conclusion: AutoMind是迈向全自动化数据科学的重要一步。

Abstract: Large Language Model (LLM) agents have shown great potential in addressing
real-world data science problems. LLM-driven data science agents promise to
automate the entire machine learning pipeline, yet their real-world
effectiveness remains limited. Existing frameworks depend on rigid, pre-defined
workflows and inflexible coding strategies; consequently, they excel only on
relatively simple, classical problems and fail to capture the empirical
expertise that human practitioners bring to complex, innovative tasks. In this
work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework
that overcomes these deficiencies through three key advances: (1) a curated
expert knowledge base that grounds the agent in domain expert knowledge, (2) an
agentic knowledgeable tree search algorithm that strategically explores
possible solutions, and (3) a self-adaptive coding strategy that dynamically
tailors code generation to task complexity. Evaluations on two automated data
science benchmarks demonstrate that AutoMind delivers superior performance
versus state-of-the-art baselines. Additional analyses confirm favorable
effectiveness, efficiency, and qualitative solution quality, highlighting
AutoMind as an efficient and robust step toward fully automated data science.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [125] [FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio Classification](https://arxiv.org/abs/2506.10207)
*Jun Bai,Rajib Rana,Di Wu,Youyang Qu,Xiaohui Tao,Ji Zhang*

Main category: cs.SD

TL;DR: 联邦音频分类（FedAC）面临数据异构、模型异构和数据中毒三大挑战，本文提出FedMLAC框架，通过双向知识蒸馏和层剪枝聚合策略，提升分类精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在音频分类中虽保护隐私，但面临数据异构、模型异构和数据中毒等问题，现有方法缺乏统一可靠的解决方案。

Method: 提出FedMLAC框架，采用双模型架构（个性化本地模型和全局共享插件模型），结合双向知识蒸馏和层剪枝聚合策略。

Result: 在四种音频分类基准测试中，FedMLAC在分类精度和抗噪声能力上均优于现有方法。

Conclusion: FedMLAC为联邦音频分类提供了统一且高效的解决方案，显著提升了性能并增强了鲁棒性。

Abstract: Federated Learning (FL) provides a privacy-preserving paradigm for training
audio classification (AC) models across distributed clients without sharing raw
data. However, Federated Audio Classification (FedAC) faces three critical
challenges that substantially hinder performance: data heterogeneity, model
heterogeneity, and data poisoning. While prior works have attempted to address
these issues, they are typically treated independently, lacking a unified and
robust solution suited to real-world federated audio scenarios. To bridge this
gap, we propose FedMLAC, a unified mutual learning framework designed to
simultaneously tackle these challenges in FedAC. Specifically, FedMLAC
introduces a dual-model architecture on each client, comprising a personalized
local AC model and a lightweight, globally shared Plug-in model. Through
bidirectional knowledge distillation, the Plug-in model enables global
knowledge transfer while adapting to client-specific data distributions, thus
supporting both generalization and personalization. To further enhance
robustness against corrupted audio data, we develop a Layer-wise Pruning
Aggregation (LPA) strategy that filters unreliable Plug-in model updates based
on parameter deviations during server-side aggregation. Extensive experiments
on four diverse audio classification benchmarks, spanning both speech and
non-speech tasks, demonstrate that FedMLAC consistently outperforms existing
state-of-the-art methods in terms of classification accuracy and robustness to
noisy data.

</details>
