<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.HC](#cs.HC) [Total: 18]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [math.LO](#math.LO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.IT](#cs.IT) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.FL](#cs.FL) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Blueprint First, Model Second: A Framework for Deterministic LLM Workflow](https://arxiv.org/abs/2508.02721)
*Libin Qiu,Yuhang Ye,Zhirong Gao,Xide Zou,Junfu Chen,Ziming Gui,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Kun Zhao*

Main category: cs.SE

TL;DR: 论文提出了Source Code Agent框架，通过将工作流逻辑与生成模型分离，解决了LLM代理在结构化环境中执行不可预测的问题，提升了执行效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理的非确定性限制了其在需要严格程序执行的场景中的应用，因此需要一种新的框架来确保执行的可预测性和可靠性。

Method: 采用“蓝图优先，模型其次”的方法，将专家定义的操作程序编码为执行蓝图，由确定性引擎执行，LLM仅用于处理特定的子任务。

Result: 在tau-bench基准测试中，Source Code Agent表现优异，平均Pass^1分数比最强基线高出10.1个百分点，并显著提高了执行效率。

Conclusion: 该框架为需要严格程序逻辑的应用提供了可验证且可靠的自主代理部署方案。

Abstract: While powerful, the inherent non-determinism of large language model (LLM)
agents limits their application in structured operational environments where
procedural fidelity and predictable execution are strict requirements. This
limitation stems from current architectures that conflate probabilistic,
high-level planning with low-level action execution within a single generative
process. To address this, we introduce the Source Code Agent framework, a new
paradigm built on the "Blueprint First, Model Second" philosophy. Our framework
decouples the workflow logic from the generative model. An expert-defined
operational procedure is first codified into a source code-based Execution
Blueprint, which is then executed by a deterministic engine. The LLM is
strategically invoked as a specialized tool to handle bounded, complex
sub-tasks within the workflow, but never to decide the workflow's path. We
conduct a comprehensive evaluation on the challenging tau-bench benchmark,
designed for complex user-tool-rule scenarios. Our results demonstrate that the
Source Code Agent establishes a new state-of-the-art, outperforming the
strongest baseline by 10.1 percentage points on the average Pass^1 score while
dramatically improving execution efficiency. Our work enables the verifiable
and reliable deployment of autonomous agents in applications governed by strict
procedural logic.

</details>


### [2] [Interpreting Performance Profiles with Deep Learning](https://arxiv.org/abs/2508.02729)
*Zhuoran Liu*

Main category: cs.SE

TL;DR: 该论文提出了一种结合性能分析和程序语义的新方法，通过深度学习技术提升性能分析工具的可操作性。


<details>
  <summary>Details</summary>
Motivation: 现有的性能分析工具对软件工程师负担较重，尤其是非代码作者难以关联性能数据与程序语义，限制了工具的适用性。

Method: 结合Async Profiler生成的性能数据与基于CodeBERT模型的代码摘要技术，通过图形界面展示调用路径的代码摘要。

Result: 系统能有效辅助分析多种Java基准测试中的性能问题。

Conclusion: 结合性能分析与程序语义的深度学习方法，可提升性能分析工具的实用性和用户体验。

Abstract: Profiling tools (also known as profilers) play an important role in
understanding program performance at runtime, such as hotspots, bottlenecks,
and inefficiencies. While profilers have been proven to be useful, they give
extra burden to software engineers. Software engineers, as the users, are
responsible to interpret the complex performance data and identify actionable
optimization in program source code. However, it can be challenging for users
to associate inefficiencies with the program semantics, especially if the users
are not the authors of the code, which limits the applicability of profilers.
  In this thesis, we explore a new direction to combine performance profiles
and program semantics with a deep learning approach. The key idea is to glean
code summary for semantic information (at a certain level) and integrate it
into a profiler, which can better understand program inefficiencies for
actionable optimization. To be concrete, we combine profiles generated by Async
Profiler (the state-of-the-art Java profiler) with code summarization from a
fine-tuned CodeBERT-based model. We demonstrate the code summaries of any
selected call path in a graphic user interface. Our system can effectively
assist analysis on many Java benchmarks.

</details>


### [3] [A Note on Code Quality Score: LLMs for Maintainable Large Codebases](https://arxiv.org/abs/2508.02732)
*Sherman Wong,Jalaj Bhandari,Leo Zhou Fan Yang,Xylan Xu,Yi Zhuang,Cem Cayiroglu,Payal Bhuptani,Sheela Yadawad,Hung Duong*

Main category: cs.SE

TL;DR: 论文提出Code Quality Score (CQS)系统，通过微调的Llama3模型自动检测代码质量问题并提供改进建议，结合人工规则过滤错误响应，实际应用中表现出高精度和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 大规模软件系统中，多人协作开发时维护代码质量具有挑战性。

Method: CQS系统使用两个微调的Llama3模型（SFT和离线RL）检测代码最佳实践问题，并为LLM生成的代码评审提供建议，辅以人工规则过滤错误响应。

Result: 离线评估显示CQS在识别有效问题方面具有高精度，实际应用中用户满意度每周达60%。

Conclusion: CQS系统在大规模工业环境中有效提升代码质量，并通过开发者反馈优化模型训练数据。

Abstract: Maintaining code quality in large-scale software systems presents significant
challenges, particularly in settings where a large numbers of engineers work
concurrently on a codebase. This paper introduces Code Quality Score (CQS)
system to automatically detect issues with a set of code changes and provide
actionable insights. At its core, the CQS system is powered by two Llama3
models, fine-tuned (with SFT and offline RL approaches), to a) detect common
code quality issues related to coding best practices and b) to provide good
``critiques'' for LLM-generated code review respectively. To maintain good user
experience, we layer the system with hand-crafted rules to filter out incorrect
responses/hallucinations. Offline evaluations show that our CQS system is able
to achieve an impressive precision rate for identifying valid issues. This
system has already been rolled out to developers in an industrial scale setting
and has consistently achieved 60\% week over week user helpfulness rate,
demonstrating its effectiveness in a real-world environment. In this paper, we
present details of the CQS system along with some learnings on curating
developer feedback to create training data for LLM fine-tuning.

</details>


### [4] [What's in a Proof? Analyzing Expert Proof-Writing Processes in F* and Verus](https://arxiv.org/abs/2508.02733)
*Rijul Jain,Shraddha Barke,Gabriel Ebner,Md Rakib Hossain Misu,Shan Lu,Sarah Fakhoury*

Main category: cs.SE

TL;DR: POPLs（面向证明的编程语言）具有强大功能但学习曲线陡峭，阻碍了广泛应用。本文通过用户研究，分析专家如何使用F*和Verus，提出设计AI证明助手的建议，并验证其效果。


<details>
  <summary>Details</summary>
Motivation: POPLs虽能提供形式化正确性证明，但由于学习难度高且缺乏对其开发过程的深入了解，限制了其广泛应用和工具发展。

Method: 通过用户研究收集和分析八位专家使用F*和Verus的代码遥测数据，揭示专家在证明开发中的策略和挑战。

Result: 研究发现三种策略和多种未记录的非正式实践，提出AI证明助手的四项设计建议，并通过案例验证效果提升。

Conclusion: 研究为提升POPLs可用性和工具设计提供了实用指导，AI证明助手在遵循建议后表现优于基线模型。

Abstract: Proof-oriented programming languages (POPLs) empower developers to write code
alongside formal correctness proofs, providing formal guarantees that the code
adheres to specified requirements. Despite their powerful capabilities, POPLs
present a steep learning curve and have not yet been adopted by the broader
software community. The lack of understanding about the proof-development
process and how expert proof developers interact with POPLs has hindered the
advancement of effective proof engineering and the development of
proof-synthesis models/tools.
  In this work, we conduct a user study, involving the collection and analysis
of fine-grained source code telemetry from eight experts working with two
languages, F* and Verus. Results reveal interesting trends and patterns about
how experts reason about proofs and key challenges encountered during the proof
development process. We identify three distinct strategies and multiple
informal practices that are not captured final code snapshots, yet are
predictive of task outcomes. We translate these findings into concrete design
guidance for AI proof assistants: bias toward early specification drafting,
explicit sub-goal decomposition, bounded active errors, and disciplined
verifier interaction. We also present a case study of an F* proof agent
grounded in these recommendations, and demonstrate improved performance over
baseline LLMs

</details>


### [5] [Automated Code Repair for C/C++ Static Analysis Alerts](https://arxiv.org/abs/2508.02820)
*David Svoboda,Lori Flynn,William Klieber,Michael Duggan,Nicholas Reimer,Joseph Sible*

Main category: cs.SE

TL;DR: 本文通过应用自动程序修复（APR）工具显著减少了静态分析（SA）工具产生的警报数量，降低了人工审查代码的工作量。


<details>
  <summary>Details</summary>
Motivation: 减少静态分析工具产生的假阳性警报，并通过自动化修复缺陷代码来提高效率。

Method: 设计和开发了一个APR工具，用于修复C/C++代码中与3类SA警报相关的问题。修复简单且局部化。

Result: 该工具成功修复了8718/9234个SA警报，对两类缺陷和两个代码库的修复或假阳性排除率超过80%。

Conclusion: APR工具显著提高了SA警报处理的效率，但需要注意修复可能带来的新问题（如sqlite3.c）。

Abstract: (Note: This work is a preprint.) Static analysis (SA) tools produce many
diagnostic alerts indicating that source code in C or C++ may be defective and
potentially vulnerable to security exploits. Many of these alerts are false
positives. Identifying the true-positive alerts and repairing the defects in
the associated code are huge efforts that automated program repair (APR) tools
can help with. Our experience showed us that APR can reduce the number of SA
alerts significantly and reduce the manual effort of analysts to review code.
This engineering experience paper details the application of design,
development, and performance testing to an APR tool we built that repairs C/C++
code associated with 3 categories of alerts produced by multiple SA tools. Its
repairs are simple and local. Furthermore, our findings convinced the
maintainers of the CERT Coding Standards to re-assess and update the metrics
used to assess when violations of guidelines are detectable or repairable. We
discuss engineering design choices made to support goals of trustworthiness and
acceptability to developers. Our APR tool repaired 8718 out of 9234 alerts
produced by one SA tool on one codebase. It can repair 3 flaw categories. For 2
flaw categories, 2 SA tools, and 2 codebases, our tool repaired or dismissed as
false positives over 80% of alerts, on average. Tests showed repairs did not
appreciably degrade the performance of the code or cause new alerts to appear
(with the possible exception of sqlite3.c). This paper describes unique
contributions that include a new empirical analysis of SA data, our selection
method for flaw categories to repair, publication of our APR tool, and a
dataset of SA alerts from open-source SA tools run on open-source codebases. It
discusses positive and negative results and lessons learned.

</details>


### [6] [Automated Validation of LLM-based Evaluators for Software Engineering Artifacts](https://arxiv.org/abs/2508.02827)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Rami Katan,Alice Podolsky,Orna Raz,Avi Ziv*

Main category: cs.SE

TL;DR: 论文提出REFINE框架，用于自动化评估大语言模型（LLMs）在软件工程任务中的表现，通过生成不同质量的代码并结合期望排序测试评估器的效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为代码评估器仍不可靠，人工评估成本高且不可扩展，现有自动方法无法识别细粒度质量差异。

Method: REFINE框架包括Hierarchy Dataset Builder（生成质量递减的代码）和Evaluator Tester（测试评估器排序与预期的一致性）。

Result: REFINE在IBM内部工作流中应用，显著提升评估器配置的准确度，某些任务中评分从0.7提高到0.9。

Conclusion: REFINE通过可控的细粒度测试提升LLM评估器的可靠性，实际应用于工业场景支持模型发布决策。

Abstract: Automation in software engineering increasingly relies on large language
models (LLMs) to generate, review, and assess code artifacts. However,
establishing LLMs as reliable evaluators remains an open challenge: human
evaluations are costly, subjective and non scalable, while existing automated
methods fail to discern fine grained variations in artifact quality.
  We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation),
an automated framework for benchmarking LLM based evaluators across software
engineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder
applies novel generation techniques to automatically synthesize artifacts with
progressively reduced quality, and Evaluator Tester quantifies each candidate
evaluator configuration by measuring how closely its rankings align with
expected ordering.
  A key feature of REFINE is controllability: users can tune the granularity of
degradation to progressively refine evaluator configurations, from coarse
filtering to stress testing on subtle quality gaps.
  While the methodology is general, we focus on coding tasks reflecting the
practical demands in our production setting. REFINE was integrated into IBM's
internal development workflows and applied to code generation, translation, and
summarization for COBOL, an enterprise critical programming language, using
industrial data. It was used to identify LLM as a Judge configurations that
lifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks.
These nuance sensitive evaluators are now actively used by model training teams
to support model release decisions.

</details>


### [7] [Developer Perceptions on Utilising Low-Code Approaches to Build Accessible and Adaptive Applications for Seniors](https://arxiv.org/abs/2508.02968)
*Shavindra Wickramathilaka,John Grundy,Kashumi Madampe,Omar Haggag*

Main category: cs.SE

TL;DR: 本文探讨了如何通过低代码工具AdaptForge帮助开发者高效创建适合老年人的无障碍和自适应应用。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化问题日益严峻，亟需技术手段提升老年人的自主性。开发者面临时间和资源限制，难以满足无障碍和个性化需求。

Method: 通过访谈18位软件从业者，评估低代码模型驱动工程工具AdaptForge的效果。

Result: 研究总结了开发者对这类工具的期望，并提出了设计建议。

Conclusion: 低代码工具有潜力成为行业标准解决方案，支持无障碍和自适应软件开发。

Abstract: The global ageing population presents a growing societal challenge, creating
an urgent need for inclusive technologies that promote autonomy among older
adults. Software practitioners can address this by delivering digital services
that enhance seniors' independence and reduce reliance on routine support from
family members and healthcare infrastructure. However, traditional development
practices, constrained by time and resources, often result in applications with
major accessibility and personalisation barriers. Increasing pressure from
regulatory requirements, such as the European Accessibility Act (EAA), and the
personal empathy many developers feel toward supporting their older loved ones
and their own future selves have created a demand for tools that support the
development of accessible and adaptive software. To address this demand, this
paper presents an interview-based empirical study with 18 software
practitioners, evaluating AdaptForge: a low-code model-driven engineering (MDE)
tool that enables the efficient creation of accessible and adaptive
applications for senior users by mitigating development constraints through
automated code generation. Based on these insights, we identify developer
expectations for adopting such tools as industry-standard solutions and provide
empirically grounded recommendations for designing low-code tools that support
accessible and adaptive software development.

</details>


### [8] [MRG-Bench: Evaluating and Exploring the Requirements of Context for Repository-Level Code Generation](https://arxiv.org/abs/2508.02998)
*Haiyang Li*

Main category: cs.SE

TL;DR: MRG-Bench是一个新的多语言代码生成评估数据集，解决了现有数据集缺乏真实世界代码分布和可运行测试等问题。实验表明当前代码生成技术在理解用户需求方面表现不佳，且不同语言需要不同的上下文设计。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估数据集存在缺乏可运行测试、偏离真实代码分布和仅支持Python等问题，影响了评估结果的可信度。

Method: 提出MRG-Bench数据集，包含真实代码库数据、支持多语言（Python、Java、Go）和项目级可运行测试。基于此数据集进行了多项实验。

Result: 实验表明当前代码生成技术在理解用户需求方面表现不佳，且不同编程语言对上下文的需求有显著差异。

Conclusion: 需要针对不同语言设计专门的上下文信息，以提升代码生成模型在真实场景中的表现。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
code generation. However, current evaluation datasets suffer from issues such
as the lack of runnable test cases, deviation from the distribution of
real-world code, and the ability to evaluate only the Python language. These
limitations undermine the credibility of the evaluation results.
  To address these limitations, we introduce \textbf{MRG-Bench} (Multi-language
Repository-level Code Generation Benchmark), a novel dataset that provides a
more accurate evaluation of LLMs in practical repository-level code generation
tasks. MRG-Bench has three main features: (1) practical data sourced from
real-world code repositories that align to the practical distribution, (2)
multiple programming languages support, including Python, Java, and Go, and (3)
project-level runnable test cases to assess the quality of the generated code.
  Based on MRG-Bench, we conducted extensive experiments including large
language models, long-context models, and RAG-related methods. These evaluation
results demonstrate that \textbf{current repository-level code generation
techniques suffer from significant performance deficiencies}. To further
investigate why models fail, we designed novel experiments to annotate the
underlying causes of generation errors. The results explicitly show that the
majority of methods suffer from "\textbf{difficulty in understanding user
requirements}," failing to comprehend their assigned tasks accurately.
Moreover, the impact of different repository-level contexts on this issue
exhibits significant disparities across different programming languages,
suggesting that, in practice, specialized contextual information needs to be
designed for different languages.

</details>


### [9] [Tool-integrated Reinforcement Learning for Repo Deep Search](https://arxiv.org/abs/2508.03012)
*Zexiong Ma,Chao Peng,Qunhong Zeng,Pengfei Gao,Yanzhen Zou,Bing Xie*

Main category: cs.SE

TL;DR: ToolTrain是一种两阶段工具集成训练框架，通过结合拒绝采样的监督微调和工具集成强化学习，提升LLMs在问题定位中使用检索工具的能力，实验结果显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在处理问题定位时，虽然结合了仓库检索工具，但仍难以高效完成多步推理和导航任务。

Method: 提出ToolTrain框架，分两阶段训练：拒绝采样的监督微调和工具集成强化学习，以优化LLMs在问题定位中的工具使用能力。

Result: 32B模型在函数级定位上表现优于Claude-3.7，且定位性能提升直接改善了端到端问题解决效果。

Conclusion: ToolTrain证明了通过训练优化问题定位能力是提升自动化软件开发的有效策略。

Abstract: Issue localization, the process of identifying code locations that need
modification to resolve software issues, is a critical yet challenging task in
software development. The semantic gap between natural language issue
descriptions and faulty code requires complex multi-hop reasoning through code
dependencies. Existing LLM-based agents attempt to address this by integrating
repository retrieval tools. However, this transforms issue localization into a
demanding task we call Repo Deep Search, which requires the LLM to effectively
utilize various repository retrieval tools throughout a multi-step reasoning
and navigation process. To tackle this challenge, we present ToolTrain, a
two-stage tool-integrated training framework combining rejection-sampled
supervised fine-tuning and tool-integrated reinforcement learning to enhance
LLMs' ability to use retrieval tools for issue localization. Experimental
results show that ToolTrain-trained models achieve state-of-the-art
performance, with our 32B model even surpassing Claude-3.7 on function-level
localization. The results also show that improved localization performance
translates to better end-to-end issue resolution performance. This further
demonstrates that training for issue localization is a viable and effective
strategy for improving automated software development.

</details>


### [10] [StoneDetector: Conventional and versatile code clone detection for Java](https://arxiv.org/abs/2508.03435)
*Thomas S. Heinze,André Schäfer,Wolfram Amme*

Main category: cs.SE

TL;DR: 论文介绍了StoneDetector平台及其方法，用于检测Java源代码和字节码中的代码克隆，通过支配树路径的文本比较实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 代码克隆可能导致项目膨胀和问题传播，因此识别它们至关重要。

Method: 基于支配树路径的文本比较，使用不同字符串度量和哈希算法配置。

Result: 在多项基准测试中，StoneDetector展现了高效的性能和扩展性。

Conclusion: StoneDetector是一种多功能且高效的代码克隆检测工具。

Abstract: Copy & paste is a widespread practice when developing software and, thus,
duplicated and subsequently modified code occurs frequently in software
projects. Since such code clones, i.e., identical or similar fragments of code,
can bloat software projects and cause issues like bug or vulnerability
propagation, their identification is of importance. In this paper, we present
the StoneDetector platform and its underlying method for finding code clones in
Java source and Bytecode. StoneDetector implements a conventional clone
detection approach based upon the textual comparison of paths derived from the
code's representation by dominator trees. In this way, the tool does not only
find exact and syntactically similar near-miss code clones, but also code
clones that are harder to detect due to their larger variety in the syntax. We
demonstrate StoneDetector's versatility as a conventional clone detection
platform and analyze its various available configuration parameters, including
the usage of different string metrics, hashing algorithms, etc. In our
exhaustive evaluation with other conventional clone detectors on several
state-of-the-art benchmarks, we can show StoneDetector's performance and
scalability in finding code clones in both, Java source and Bytecode.

</details>


### [11] [A System Model Generation Benchmark from Natural Language Requirements](https://arxiv.org/abs/2508.03215)
*Dongming Jin,Zhi Jin,Linyu Li,Zheng Fang,Jia Li,Xiaohong Chen*

Main category: cs.SE

TL;DR: SysMBench是一个用于评估大型语言模型（LLM）生成系统模型能力的基准测试，包含151个场景。研究表明目前LLM在该任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 由于系统模型开发的复杂性和描述语言的特定性，需要评估LLM在此任务上的能力，但目前缺乏相关基准。

Method: 提出SysMBench基准，包含场景、自然语言描述、系统模型和可视化图表。使用SysMEval评估指标对17个LLM进行评估。

Result: LLM在SysMBench上表现较差，最高BLEU仅为4%，SysMEval-F1为62%。

Conclusion: SysMBench为未来LLM在系统模型生成领域的研究提供了基准，但目前模型的性能仍需提升。

Abstract: System models, a critical artifact in software development, provide a formal
abstraction of both the structural and behavioral aspects of software systems,
which can facilitate the early requirements analysis and architecture design.
However, developing system models remains challenging due to the specific
syntax of model description languages and the relative scarcity of public model
examples. While large language models (LLMs) have shown promise in generating
code with programming languages and could potentially aid in system model
development, no benchmarks currently exist for evaluating their ability to
generate system models with specific description languages. We present
SysMBench, which comprises 151 human-curated scenarios spanning a wide range of
popular domains and varying difficulty levels. Each scenario mainly comprises a
natural language requirements description, a system model expressed in a
specific model description language, and a visualized system model diagram. The
requirements description is fed as user input to the LLM, the system model with
description language is used to verify if the generated system model conforms
to the requirements, and the visualized diagram serves to support manual
validation. We introduce SysMEval, a semantic-aware evaluation metric to
evaluate the quality of generated system models. We evaluate 17 popular LLMs on
this task with three traditional metrics and SysMEval, from directly prompting
to three commonly used enhancement strategies. Our in-depth evaluation shows
that LLMs perform poorly on SysMBench, with the highest BLEU of 4% and
SysMEval-F1 of 62%. We release the SysMBench and its evaluation framework to
enable future research on LLM-based system model generation.

</details>


### [12] [ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated Test Programs](https://arxiv.org/abs/2508.03603)
*Iti Shree,Karine Even-Mendoz,Tomasz Radzik*

Main category: cs.SE

TL;DR: ReFuzzer是一种通过检测和修正编译与运行时错误来优化LLM生成的测试程序的框架，显著提高了测试程序的有效性和代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的编译器模糊测试工具常生成语法或语义无效的测试程序，限制了其在测试编译器优化和后端组件中的效果。

Method: ReFuzzer通过反馈循环与本地LLM结合，验证并过滤错误程序，提高模糊测试的有效性。

Result: ReFuzzer将测试程序的有效性从47.0-49.4%提升至96.6-97.3%，关键优化组件的代码覆盖率显著增加。

Conclusion: ReFuzzer显著提高了测试程序的质量和模糊测试的效果，为编译器测试提供了有效工具。

Abstract: Existing LLM-based compiler fuzzers often produce syntactically or
semantically invalid test programs, limiting their effectiveness in exercising
compiler optimizations and backend components. We introduce ReFuzzer, a
framework for refining LLM-generated test programs by systematically detecting
and correcting compilation and runtime violations (e.g. division by zero or
array out-of-bounds accesses). ReFuzzer employs a feedback loop with a local
LLM to validate and filter erroneous programs before execution, improving
fuzzing effectiveness beyond crash detection and enabling the generation of
diverse yet valid test programs.
  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box
fuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'
validity from 47.0-49.4% to 96.6-97.3%, with an average processing time of
2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing
significantly increased code coverage in critical optimization and IR
generation components. For example, vectorization coverage had an absolute
improvement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing,
enhancing testing effectiveness.

</details>


### [13] [SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization](https://arxiv.org/abs/2508.03258)
*Yueyue Liu,Hongyu Zhang,Yuantian Miao*

Main category: cs.SE

TL;DR: 该论文提出了SmartLLMs调度器（SLS），一种动态且经济高效的调度解决方案，旨在优化大型语言模型（LLM）的部署效率。SLS通过实时反馈和动态更新策略，显著提升了性能并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM部署在多样任务中面临高成本、长响应时间和性能不稳定的挑战，静态调度方法依赖大量训练数据且灵活性不足。

Method: SLS包含三个核心组件：自适应缓存管理器、性能-成本优化调度器和动态更新管理器，分别减少冗余计算、动态分配任务并实时更新策略。

Result: 实验表明，SLS在日志解析和代码生成任务中平均性能提升198.82%，处理时间减少63.28%。

Conclusion: SLS通过动态调度和实时反馈，有效提升了LLM的实用性和灵活性，适用于多样软件工程任务。

Abstract: Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable
capabilities in a variety of software engineering tasks. Despite the
advancements, their practical deployment faces challenges, including high
financial costs, long response time, and varying performance, especially when
handling a large number of queries (jobs). Existing optimization strategies for
deploying LLMs for diverse tasks focus on static scheduling, which requires
extensive training data for performance prediction, increasing the
computational costs and limiting the applicability and flexibility. In this
paper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective
scheduling solution. The key idea is to learn LLMs' performance on diverse
tasks and incorporate their real-time feedback to update strategies
periodically. Specifically, SLS incorporates three key components, including an
Adaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic
Update Manager. The Cache Manager stores the outputs of previously processed
queries and employs an adaptive strategy to reduce redundant computations and
minimize response times. For queries not found in the cache, the Scheduler
dynamically allocates them to the most suitable LLM based on the predicted
performance and cost from models that take both query-specific and LLM-specific
features as input. The Update Manager continuously refines the cache and
scheduling strategies based on real-time feedback from the assigned queries to
enhance decision-making and adapt to evolving task characteristics. To evaluate
the effectiveness of SLS, we conduct extensive experiments on two LLM-based
software engineering tasks, including log parsing and code generation. The
results show that SLS significantly outperforms the baseline methods, achieving
an average performance improvement of 198.82% and an average processing time
reduction of 63.28%.

</details>


### [14] [GUI-ReRank: Enhancing GUI Retrieval with Multi-Modal LLM-based Reranking](https://arxiv.org/abs/2508.03298)
*Kristian Kolthoff,Felix Kretzer,Christian Bartelt,Alexander Maedche,Simone Paolo Ponzetto*

Main category: cs.SE

TL;DR: GUI-ReRank是一种新框架，结合了快速嵌入检索与MLLM重排技术，显著提升了GUI检索的准确性和通用性。


<details>
  <summary>Details</summary>
Motivation: GUI原型构建资源密集且耗时，现有NL检索方法性能有限且难以通用。

Method: 集成快速嵌入检索与MLLM重排技术，提供自定义GUI仓库标注和嵌入流程。

Result: 在GUI检索基准测试中超越SOTA模型，提供检索效果与计算资源的权衡分析。

Conclusion: GUI-ReRank提高了检索效率和通用性，适合集成到LLM工作流中。

Abstract: GUI prototyping is a fundamental component in the development of modern
interactive systems, which are now ubiquitous across diverse application
domains. GUI prototypes play a critical role in requirements elicitation by
enabling stakeholders to visualize, assess, and refine system concepts
collaboratively. Moreover, prototypes serve as effective tools for early
testing, iterative evaluation, and validation of design ideas with both end
users and development teams. Despite these advantages, the process of
constructing GUI prototypes remains resource-intensive and time-consuming,
frequently demanding substantial effort and expertise. Recent research has
sought to alleviate this burden through NL-based GUI retrieval approaches,
which typically rely on embedding-based retrieval or tailored ranking models
for specific GUI repositories. However, these methods often suffer from limited
retrieval performance and struggle to generalize across arbitrary GUI datasets.
In this work, we present GUI-ReRank, a novel framework that integrates rapid
embedding-based constrained retrieval models with highly effective MLLM-based
reranking techniques. GUI-ReRank further introduces a fully customizable GUI
repository annotation and embedding pipeline, enabling users to effortlessly
make their own GUI repositories searchable, which allows for rapid discovery of
relevant GUIs for inspiration or seamless integration into customized LLM-based
RAG workflows. We evaluated our approach on an established NL-based GUI
retrieval benchmark, demonstrating that GUI-ReRank significantly outperforms
SOTA tailored LTR models in both retrieval accuracy and generalizability.
Additionally, we conducted a comprehensive cost and efficiency analysis of
employing MLLMs for reranking, providing valuable insights regarding the
trade-offs between retrieval effectiveness and computational resources. Video:
https://youtu.be/_7x9UCh82ug

</details>


### [15] [Industrial LLM-based Code Optimization under Regulation: A Mixture-of-Agents Approach](https://arxiv.org/abs/2508.03329)
*Mari Ashiga,Vardan Voskanyan,Fateme Dinmohammadi,Jingzhi Gong,Paul Brookes,Matthew Truscott,Rafail Giavrimis,Mike Basios,Leslie Kanthan,Wei Jie*

Main category: cs.SE

TL;DR: 论文探讨了一种基于多智能体混合（MoA）的方法，用于解决受监管行业中无法使用商业大语言模型（LLMs）的代码优化问题，展示了其在开源模型上的优势。


<details>
  <summary>Details</summary>
Motivation: 解决受监管行业因数据隐私和合规要求无法使用商业LLMs的问题，实现高质量、低成本的代码优化。

Method: 采用Mixture-of-Agents（MoA）方法，结合多个专业LLMs，并与遗传算法（GA）系统和单个LLM优化器进行对比。

Result: MoA在开源模型上表现优异，节省14.3%-22.2%成本，优化时间缩短28.6%-32.2%；GA在商业模型上更具优势，但两种集成方法均优于单个LLM。

Conclusion: MoA方法为受监管行业提供了一种可行的代码优化解决方案，平衡了合规性与性能。

Abstract: Recent advancements in Large Language Models (LLMs) for code optimization
have enabled industrial platforms to automate software performance engineering
at unprecedented scale and speed. Yet, organizations in regulated industries
face strict constraints on which LLMs they can use - many cannot utilize
commercial models due to data privacy regulations and compliance requirements,
creating a significant challenge for achieving high-quality code optimization
while maintaining cost-effectiveness. We address this by implementing a
Mixture-of-Agents (MoA) approach that directly synthesizes code from multiple
specialized LLMs, comparing it against TurinTech AI's vanilla Genetic Algorithm
(GA)-based ensemble system and individual LLM optimizers using real-world
industrial codebases. Our key contributions include: (1) First MoA application
to industrial code optimization using real-world codebases; (2) Empirical
evidence that MoA excels with open-source models, achieving 14.3% to 22.2% cost
savings and 28.6% to 32.2% faster optimization times for regulated
environments; (3) Deployment guidelines demonstrating GA's advantage with
commercial models while both ensembles outperform individual LLMs; and (4)
Real-world validation across 50 code snippets and seven LLM combinations,
generating over 8,700 variants, addresses gaps in industrial LLM ensemble
evaluation. This provides actionable guidance for organizations balancing
regulatory compliance with optimization performance in production environments.

</details>


### [16] [Key-Augmented Neural Triggers for Knowledge Sharing](https://arxiv.org/abs/2508.03340)
*Alex Wolf,Marco Edoardo Palma,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: KANT通过嵌入知识锚点解决代码库知识碎片化和RAG效率问题，显著降低推理延迟，适合本地部署。


<details>
  <summary>Details</summary>
Motivation: 解决代码库知识碎片化、RAG效率低下、训练数据稀缺及隐私问题。

Method: 提出KANT方法，嵌入知识锚点并合成专用数据，优化训练和推理。

Result: 人工评估显示数据质量高，KANT偏好率60%，推理延迟降低85%。

Conclusion: KANT为代码理解提供了高效、低延迟的本地部署方案。

Abstract: Repository-level code comprehension and knowledge sharing remain core
challenges in software engineering. Large language models (LLMs) have shown
promise by generating explanations of program structure and logic. However,
these approaches still face limitations: First, relevant knowledge is
distributed across multiple files within a repository, aka semantic
fragmentation. Second, retrieval inefficiency and attention saturation degrade
performance in RAG pipelines, where long, unaligned contexts overwhelm
attention. Third, repository specific training data is scarce and often
outdated. Finally, proprietary LLMs hinder industrial adoption due to privacy
and deployment constraints. To address these issues, we propose Key-Augmented
Neural Triggers (KANT), a novel approach that embeds knowledge anchors into
both training and inference. Unlike prior methods, KANT enables internal access
to repository specific knowledge, reducing fragmentation and grounding
inference in localized context. Moreover, we synthesize specialized data
directly from code. At inference, knowledge anchors replace verbose context,
reducing token overhead and latency while supporting efficient, on premise
deployment. We evaluate KANT via: a qualitative human evaluation of the
synthesized dataset's intent coverage and quality across five dimensions;
compare against SOTA baselines across five qualitative dimensions and inference
speed; and replication across different LLMs to assess generalizability.
Results show that the synthetic training data aligned with information-seeking
needs. KANT achieved over 60% preference from human annotators and a LocalStack
expert (preferring 79% of cases). Also, KANT reduced inference latency by up to
85% across all models. Overall, it is well-suited for scalable, low-latency,
on-premise deployments, providing a strong foundation for code comprehension.

</details>


### [17] [Psychological safety in software workplaces: A systematic literature review](https://arxiv.org/abs/2508.03369)
*Beatriz Santana,Lidivânio Monte,Bianca Santana de Araújo Silva,Glauco Carneiro,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: 本文通过系统文献综述，总结了软件工程中心理安全（PS）的前因后果，发现PS对团队创新和学习有积极影响，但研究仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 心理安全对软件工程团队的健康和表现至关重要，但相关研究有限，需要系统性总结现有知识。

Method: 采用系统文献综述方法，分析来自四个数字图书馆的研究数据，结合定量和定性分析。

Result: 研究显示PS的前因包括团队自主权、敏捷方法和领导行为，PS能促进创新和团队表现。

Conclusion: PS在软件开发中具有重要作用，但需进一步研究其影响因素和提升策略，尤其是在不同组织环境中。

Abstract: Context: Psychological safety (PS) is an important factor influencing team
well-being and performance, particularly in collaborative and dynamic domains
such as software development. Despite its acknowledged significance, research
on PS within the field of software engineering remains limited. The
socio-technical complexities and fast-paced nature of software development
present challenges to cultivating PS. To the best of our knowledge, no
systematic secondary study has synthesized existing knowledge on PS in the
context of software engineering.
  Objective: This study aims to systematically review and synthesize the
existing body of knowledge on PS in software engineering. Specifically, it
seeks to identify the potential antecedents and consequences associated with
the presence or absence of PS among individuals involved in the software
development process.
  Methods: A systematic literature review was conducted, encompassing studies
retrieved from four digital libraries. The extracted data were subjected to
both quantitative and qualitative analyses.
  Results: The findings indicate a growing academic interest in PS within
software engineering, with the majority of studies grounded in Edmondson's
framework. Factors antecedents of PS were identified at the individual, team,
and organizational levels, including team autonomy, agile methodologies, and
leadership behaviors.
  Conclusion: PS fosters innovation, learning, and team performance within
software development. However, significant gaps persist in understanding the
contextual factors influencing PS, its underlying mechanisms, and effective
strategies for its enhancement. Future research should address these gaps by
investigating the practical applications of PS within diverse organizational
settings in the software engineering domain.

</details>


### [18] [Agentic AI in 6G Software Businesses: A Layered Maturity Model](https://arxiv.org/abs/2508.03393)
*Muhammad Zohaib,Muhammad Azeem Akbar,Sami Hyrynsalmi,Arif Ali Khan*

Main category: cs.SE

TL;DR: 本文研究了6G软件业务中代理AI系统的机遇与挑战，通过主题映射识别了29个促进因素和27个抑制因素，并归类为五大主题。


<details>
  <summary>Details</summary>
Motivation: 代理AI系统在6G环境中具有自主性、可扩展性和智能决策的潜力，但其采用面临技术、集成和组织等多方面的挑战。

Method: 通过多声部文献综述和针对性扫描，进行初步主题映射，识别影响代理软件采用的因素。

Result: 研究提出了29个促进因素和27个抑制因素，并归类为五个高级主题，为组织评估代理转型能力提供了结构化视角。

Conclusion: 本研究为软件开发组织提供了实用的框架，以评估和提升其代理优先能力，适应6G需求。

Abstract: The emergence of agentic AI systems in 6G software businesses presents both
strategic opportunities and significant challenges. While such systems promise
increased autonomy, scalability, and intelligent decision-making across
distributed environments, their adoption raises concerns regarding technical
immaturity, integration complexity, organizational readiness, and
performance-cost trade-offs. In this study, we conducted a preliminary thematic
mapping to identify factors influencing the adoption of agentic software within
the context of 6G. Drawing on a multivocal literature review and targeted
scanning, we identified 29 motivators and 27 demotivators, which were further
categorized into five high-level themes in each group. This thematic mapping
offers a structured overview of the enabling and inhibiting forces shaping
organizational readiness for agentic transformation. Positioned as a
feasibility assessment, the study represents an early phase of a broader
research initiative aimed at developing and validating a layered maturity model
grounded in CMMI model with the software architectural three dimensions
possibly Data, Business Logic, and Presentation. Ultimately, this work seeks to
provide a practical framework to help software-driven organizations assess,
structure, and advance their agent-first capabilities in alignment with the
demands of 6G.

</details>


### [19] [On the Evaluation of Large Language Models in Multilingual Vulnerability Repair](https://arxiv.org/abs/2508.03470)
*Dong wang,Junji Yu,Honglin Shu,Michael Fu,Chakkrit Tantithamthavorn,Yasutaka Kamei,Junjie Chen*

Main category: cs.SE

TL;DR: GPT-4o在指令调优和少样本提示下，表现出色，尤其在多语言漏洞修复中优越于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在多语言漏洞修复中的潜力，克服现有方法仅限于C/C++的问题。

Method: 进行大规模实证研究，比较自动漏洞修复方法和LLMs在七种编程语言中的表现，重点关注指令调优的GPT-4o。

Result: GPT-4o在修复独特漏洞和最危险漏洞上表现优异，且在新语言的漏洞修复中泛化能力强。Go语言修复效果最佳，C/C++最差。

Conclusion: LLMs在多语言漏洞修复中前景广阔，但仍需解决失败案例问题。

Abstract: Various Deep Learning-based approaches with pre-trained language models have
been proposed for automatically repairing software vulnerabilities. However,
these approaches are limited to a specific programming language (C/C++). Recent
advances in large language models (LLMs) offer language-agnostic capabilities
and strong semantic understanding, exhibiting potential to overcome
multilingual vulnerability limitations. Although some work has begun to explore
LLMs' repair performance, their effectiveness is unsatisfactory. To address
these limitations, we conducted a large-scale empirical study to investigate
the performance of automated vulnerability repair approaches and
state-of-the-art LLMs across seven programming languages. Results show GPT-4o,
instruction-tuned with few-shot prompting, performs competitively against the
leading approach, VulMaster. Additionally, the LLM-based approach shows
superior performance in repairing unique vulnerabilities and is more likely to
repair the most dangerous vulnerabilities. Instruction-tuned GPT-4o
demonstrates strong generalization on vulnerabilities in previously unseen
language, outperforming existing approaches. Analysis shows Go consistently
achieves the highest effectiveness across all model types, while C/C++ performs
the worst. Based on findings, we discuss the promise of LLM on multilingual
vulnerability repair and the reasons behind LLM's failed cases. This work takes
the first look at repair approaches and LLMs across multiple languages,
highlighting the promising future of adopting LLMs for multilingual
vulnerability repair.

</details>


### [20] [BitsAI-Fix: LLM-Driven Approach for Automated Lint Error Resolution in Practice](https://arxiv.org/abs/2508.03487)
*Yuanpeng Li,Qi Long,Zhiyuan Yao,Jian Xu,Lintao Xie,Xu He,Lu Geng,Xin Han,Yueyan Chen,Wenbo Duan*

Main category: cs.SE

TL;DR: BitsAI-Fix利用LLMs自动修复代码中的lint错误，通过上下文扩展、搜索替换补丁生成及重新验证，结合渐进式强化学习和针对性奖励机制，在字节跳动生产中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 企业代码库规模扩大导致lint错误远超人工修复能力，技术债务积累阻碍开发效率。

Method: 采用tree-sitter扩展上下文，训练LLMs生成补丁，结合渐进式强化学习和规则奖励机制。

Result: 在字节跳动支持5000+工程师，修复12000+静态分析问题，准确率约85%。

Conclusion: 基于LLMs的代码修复方案在企业环境中可行，为大规模工业场景提供参考。

Abstract: As enterprise codebases continue to grow in scale and complexity, the volume
of lint errors far exceeds engineers' manual remediation capacity, leading to
continuous accumulation of technical debt and hindered development efficiency.
This paper presents BitsAI-Fix, an automated lint error remediation workflow
based on Large Language Models (LLMs), designed to address this critical
challenge in industrial-scale environments. BitsAI-Fix employs tree-sitter for
context expansion and generates search-and-replace format patches through
specially trained LLMs, followed by lint scan re-verification to output final
remediation results. Additionally, our approach introduces an innovative
progressive reinforcement learning (RL) training strategy that can
automatically acquire verifiable training data during the project cold-start
phase and continuously iterate the model by collecting online samples through
feedback after system deployment. Furthermore, we designed a targeted
rule-based reward mechanism that combines format rewards and correctness
rewards while penalizing redundant modifications. We also propose a "code diff
matching" methodology to continuously track online effectiveness. In production
deployment at ByteDance, our solution has supported over 5,000 engineers,
resolved more than 12,000 static analysis issues, achieved approximately 85%
remediation accuracy, with around 1,000 weekly active adopters. This work
demonstrates the practical feasibility of LLM-based code remediation solutions
in enterprise environments and serves as a reference for automated code fix in
large-scale industrial scenarios.

</details>


### [21] [LaTCoder: Converting Webpage Design to Code with Layout-as-Thought](https://arxiv.org/abs/2508.03560)
*Yi Gui,Zhen Li,Zhongyi Zhang,Guohao Wang,Tianpeng Lv,Gaoyang Jiang,Yi Liu,Dongping Chen,Yao Wan,Hongyu Zhang,Wenbin Jiang,Xuanhua Shi,Hai Jin*

Main category: cs.SE

TL;DR: 论文提出LaTCoder方法，利用Layout-as-Thought（LaT）提升多模态大语言模型在网页设计转代码任务中的布局准确性，通过分块生成和动态选择策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在网页设计转代码任务中布局保留不足的问题，提升代码生成的准确性和实用性。

Method: 1. 分块算法将网页设计拆分为图像块；2. CoT-based方法逐块生成代码；3. 绝对定位和MLLM-based两种组装策略动态选择最优输出。

Result: 实验显示，DeepSeek-VL2的TreeBLEU提升66.67%，MAE降低38%；人工评估中60%以上的案例更偏好LaTCoder生成的网页。

Conclusion: LaTCoder显著改进了布局保留效果，证明了LaT方法的有效性。

Abstract: Converting webpage designs into code (design-to-code) plays a vital role in
User Interface (UI) development for front-end developers, bridging the gap
between visual design and functional implementation. While recent Multimodal
Large Language Models (MLLMs) have shown significant potential in
design-to-code tasks, they often fail to accurately preserve the layout during
code generation. To this end, we draw inspiration from the Chain-of-Thought
(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that
enhances layout preservation in webpage design during code generation with
Layout-as-Thought (LaT). Specifically, we first introduce a simple yet
efficient algorithm to divide the webpage design into image blocks. Next, we
prompt MLLMs using a CoTbased approach to generate code for each block.
Finally, we apply two assembly strategies-absolute positioning and an
MLLM-based method-followed by dynamic selection to determine the optimal
output. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs
(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly
introduced, more challenging benchmark (CC-HARD) that features complex layouts.
The experimental results on automatic metrics demonstrate significant
improvements. Specifically, TreeBLEU scores increased by 66.67% and MAE
decreased by 38% when using DeepSeek-VL2, compared to direct prompting.
Moreover, the human preference evaluation results indicate that annotators
favor the webpages generated by LaTCoder in over 60% of cases, providing strong
evidence of the effectiveness of our method.

</details>


### [22] [Intent Preserving Generation of Diverse and Idiomatic (Code-)Artifacts](https://arxiv.org/abs/2508.03642)
*Oliver Westphal*

Main category: cs.SE

TL;DR: 论文提出了一种通过定义抽象构建块来自动生成编程练习任务和相关工件的方法，避免了为每个相关工件单独编写庞大生成器的复杂性。


<details>
  <summary>Details</summary>
Motivation: 自动化生成编程练习任务时，通常需要生成程序（如示例解决方案或任务描述），但编写生成器可能复杂且难以适应新任务，尤其是需要多种相关工件时。

Method: 通过定义一组抽象构建块及其具体实现，再以这些构建块的组合结构来描述目标工件，从而自动生成多种相关工件。

Result: 该方法能生成多样化且符合习惯的代码，适用于多种上下文，避免了为每个工件单独编写生成器的麻烦。

Conclusion: 基于抽象构建块的方法简化了编程练习任务及相关工件的生成过程，具有通用性和灵活性。

Abstract: When automatically generating programming exercise tasks one often also needs
to automatically generate programs. At the very least when providing sample
solutions is part of automated feedback. But programs can also be used as part
of the exercise task description to communicate a task's requirements.
  Writing good program generators that produce varied yet idiomatic code while
being easily adaptable for new tasks is challenging. The challenges are
intensified if task generation requires additional artifacts, like a more
general behavior specification for testing or additional textual descriptions.
Manually writing generators for multiple different but strongly related
artifacts gets complicated quickly.
  We present an approach where instead of writing monolithic generators for
multiple connected artifacts one specifies a small set of abstract building
blocks and for each such building block defines sets of concrete realizations
for various kinds of artifacts. Then the intended structure of the resulting
artifacts is specified as a composition of the small abstract building blocks.
This abstract description then serves as the common source from which related
artifacts can be derived automatically. The approach is generic in the kind of
artifacts it can produce and is therefore adaptable to a wide range of
contexts.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [23] [Compositional Quantum Control Flow with Efficient Compilation in Qunity](https://arxiv.org/abs/2508.02857)
*Mikhail Mints,Finn Voichick,Leonidas Lampropoulos,Robert Rand*

Main category: cs.PL

TL;DR: 本文研究了高效编译量子控制流构造的方法，以Qunity语言为基础，提出优化技术以减少量子比特和门的数量。


<details>
  <summary>Details</summary>
Motivation: 现有量子编程语言多基于量子电路模型，高层次的抽象实现困难，尤其是量子控制流。Qunity语言提出了量子控制构造，但缺乏实现且编译效率低。

Method: 以Qunity为基础，引入更多抽象层，开发完整的Qunity编译器，将代码转为OpenQASM 3，并优化编译过程。

Result: 实现了高效的Qunity编译器，显著减少量子比特和门的使用。

Conclusion: 通过优化编译技术，Qunity语言的高层次量子控制流构造得以高效实现。

Abstract: Most existing quantum programming languages are based on the quantum circuit
model of computation, as higher-level abstractions are particularly challenging
to implement - especially ones relating to quantum control flow. The Qunity
language, proposed by Voichick et al., offered such an abstraction in the form
of a quantum control construct, with great care taken to ensure that the
resulting language is still realizable. However, Qunity lacked a working
implementation, and the originally proposed compilation procedure was very
inefficient, with even simple quantum algorithms compiling to unreasonably
large circuits.
  In this work, we focus on the efficient compilation of high-level quantum
control flow constructs, using Qunity as our starting point. We introduce a
wider range of abstractions on top of Qunity's core language that offer
compelling trade-offs compared to its existing control construct. We create a
complete implementation of a Qunity compiler, which converts high-level Qunity
code into the quantum assembly language OpenQASM 3. We develop optimization
techniques for multiple stages of the Qunity compilation procedure, including
both low-level circuit optimizations as well as methods that consider the
high-level structure of a Qunity program, greatly reducing the number of qubits
and gates used by the compiler.

</details>


### [24] [SAGE-HLS: Syntax-Aware AST-Guided LLM for High-Level Synthesis Code Generation](https://arxiv.org/abs/2508.03558)
*M Zafir Sadik Khan,Nowfel Mashnoor,Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 该论文提出了SAGE-HLS，一种专门用于HLS代码生成的微调LLM，通过创造数据集和改进技术显著提升了代码合成和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 由于HLS领域的公开数据集稀缺，现有LLM应用受限，论文旨在填补这一空白并优化HLS代码生成。

Method: 方法包括Verilog-to-C/C++转换创建数据集、基于AST的微调策略以及半自动化评估框架。

Result: 实验显示，SAGE-HLS在代码可合成性上接近100%成功率，功能正确性达75%。

Conclusion: SAGE-HLS为HLS代码生成提供了高效解决方案，显著优于现有技术。

Abstract: In today's rapidly evolving field of electronic design automation (EDA), the
complexity of hardware designs is increasing, necessitating more sophisticated
automation solutions. High-level synthesis (HLS), as a pivotal solution,
automates hardware designs from high-level abstractions (e.g., C/C++). However,
it faces significant challenges, particularly in design space exploration and
optimization. While large language models (LLMs) have shown notable
capabilities in code generation, their application to HLS has been limited due
to the scarcity of (publicly) available HLS code datasets. Hence, research in
this domain has primarily focused on techniques such as prompt engineering and
retrieval-augmented generation (RAG). To overcome this limitation, this paper
introduces SAGE-HLS, the first-of-its-kind fine-tuned LLM specifically for HLS
code generation. Our method includes three key advancements: (i) We implement
Verilog-to-C/C++ porting, converting verified and synthesizable Verilog codes
into corresponding C, creating a dataset of 16.7K HLS codes; (ii) We implement
a fine-tuning strategy, which is based on instruction prompting to code
generation guided by abstract syntax tree (AST); (iii) We develop a
semi-automated evaluation framework using VerilogEval to assess the
functionality of the generated HLS code. Our experiments show that SAGE-HLS,
fined-tuned on the QwenCoder (2.5) 7B model, achieves a near 100% success rate
in code synthesizability and a 75% success rate in functional correctness.

</details>


### [25] [Teaching Introductory Functional Programming Using Haskelite](https://arxiv.org/abs/2508.03640)
*Pedro Vasconcelos*

Main category: cs.PL

TL;DR: 论文探讨了在教授函数式编程时，使用逐步追踪解释器来帮助学生理解替换模型的经验。


<details>
  <summary>Details</summary>
Motivation: 学生在学习函数式编程时，常常难以将代数中的替换概念应用于递归定义、代数数据类型和高阶函数等新场景。

Method: 在波尔图大学的入门课程中，使用了一个针对Haskell子集的逐步追踪解释器。

Result: 学生们反馈该解释器有助于澄清误解并提升理解。

Conclusion: 论文总结了经验和教训，并指出了未来工作的方向。

Abstract: Learning functional programming requires learning a substitution-based
computational model. While substitution should be a familiar concept from
high-school algebra, students often have difficulty applying it to new
settings, such as recursive definitions, algebraic data types and higher-order
functions. Step-by-step interpreters have been shown to help beginners by
clarifying misconceptions and improving understanding.
  This paper reports on the experience of using a step-by-step tracing
interpreter for a subset of Haskell while teaching an introductory functional
programming course at the University of Porto. We describe the use of the
interpreter, present some feedback obtained from students, reflect on the
lessons learned and point directions for further work.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [26] [A Novel Hybrid Optical and STAR IRS System for NTN Communications](https://arxiv.org/abs/2508.03147)
*Shunyuan Shang,Emna Zedini,Abla Kammoun,Mohamed-Slim Alouini*

Main category: cs.PF

TL;DR: 提出了一种结合光学智能反射面（OIRS）和同时收发智能反射面（STAR-IRS）的非地面网络（NTN）系统，以解决下一代通信网络的关键问题。系统采用多输入多输出（MIMO）技术和新型近似方法，优化了性能指标。


<details>
  <summary>Details</summary>
Motivation: 解决下一代通信网络中信号传输的挑战，通过结合OIRS和STAR-IRS提升系统覆盖和性能。

Method: 系统模型包括光学地面站（OGS）通过OIRS传输信号到地面站（ES），再通过AF中继和STAR-IRS覆盖室内外用户。开发了适用于多OIRS场景的信道模型，并引入新型近似方法优化RF链路。

Result: 推导了闭合形式的关键性能指标（如中断概率、遍历容量和平均误码率）的双变量Fox-H函数表达式，并分析了高信噪比下的系统多样性阶数。

Conclusion: 提出的系统模型和方法显著提升了通信性能，为下一代网络设计提供了理论支持。

Abstract: This paper proposes a novel non-terrestrial networks (NTNs) system that
integrates optical intelligent reflecting surfaces (OIRS) and simultaneous
transmitting and reflecting Intelligent reflecting surfaces (STAR-IRS) to
address critical challenges in next-generation communication networks. The
proposed system model features a signal transmitted from the optical ground
station (OGS) to the earth station (ES) via an OIRS mounted horizontally on a
high altitude platform (HAP). The ES uses an amplify-and-forward (AF) relay
with fixed gain for signal relaying, which is then transmitted through a
STAR-IRS vertically installed on a building to facilitate communication with
both indoor and outdoor users. The FSO link incorporates (multiple-input
multiple-output) MIMO technology, and this paper develops a channel model
specifically designed for scenarios where the number of OIRS units exceeds one.
For the radio-frequency (RF) link, a novel and highly precise approximation
method is introduced, offering superior accuracy compared to traditional
approaches based on the central limit theorem (CLT). Closed-form analytical
expressions for key performance metrics, including outage probability (OP),
ergodic capacity and average bit error rate (BER) are derived in terms of the
bivariate Fox-H function for this novel five hops system. Asymptotic
expressions at high SNR are also presented, providing insights into system
diversity order.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [27] [AgentSight: System-Level Observability for AI Agents Using eBPF](https://arxiv.org/abs/2508.02736)
*Yusheng Zheng,Yanpeng Hu,Tong Yu,Andi Quinn*

Main category: cs.OS

TL;DR: AgentSight是一个AgentOps可观测性框架，通过边界追踪技术填补LLM代理意图与低层行为间的语义鸿沟，使用eBPF实现低开销监控。


<details>
  <summary>Details</summary>
Motivation: 现有工具无法关联LLM代理的高层意图与低层行为，导致难以区分良性操作、恶意攻击和故障。

Method: 结合边界追踪技术，通过eBPF监控系统接口，提取加密流量语义意图，并与内核事件因果关联。

Result: AgentSight能检测提示注入攻击、资源浪费循环和多代理系统中的瓶颈，性能开销低于3%。

Conclusion: AgentSight提供了一种无侵入、框架无关的解决方案，解决了LLM代理监控的核心挑战。

Abstract: Modern software infrastructure increasingly relies on LLM agents for
development and maintenance, such as Claude Code and Gemini-cli. However, these
AI agents differ fundamentally from traditional deterministic software, posing
a significant challenge to conventional monitoring and debugging. This creates
a critical semantic gap: existing tools observe either an agent's high-level
intent (via LLM prompts) or its low-level actions (e.g., system calls), but
cannot correlate these two views. This blindness makes it difficult to
distinguish between benign operations, malicious attacks, and costly failures.
We introduce AgentSight, an AgentOps observability framework that bridges this
semantic gap using a hybrid approach. Our approach, boundary tracing, monitors
agents from outside their application code at stable system interfaces using
eBPF. AgentSight intercepts TLS-encrypted LLM traffic to extract semantic
intent, monitors kernel events to observe system-wide effects, and causally
correlates these two streams across process boundaries using a real-time engine
and secondary LLM analysis. This instrumentation-free technique is
framework-agnostic, resilient to rapid API changes, and incurs less than 3%
performance overhead. Our evaluation shows AgentSight detects prompt injection
attacks, identifies resource-wasting reasoning loops, and reveals hidden
coordination bottlenecks in multi-agent systems. AgentSight is released as an
open-source project at https://github.com/agent-sight/agentsight.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [28] [A Reinforcement Learning Framework for Mobility Control of gNBs in Dynamic Radio Access Networks](https://arxiv.org/abs/2508.02960)
*Pedro Duarte,André Coelho,Manuel Ricardo*

Main category: cs.NI

TL;DR: 论文提出了一种移动基站(gNB)的智能移动控制算法，通过3D模拟环境CC-SIM和深度Q网络(DQN)显著减少了动态无线环境中的视线阻塞时间。


<details>
  <summary>Details</summary>
Motivation: 解决无线环境中由于用户移动和动态障碍物导致的视线(LoS)连接问题，提升服务质量(QoS)。

Method: 开发了CONVERGE Chamber Simulator (CC-SIM)模拟环境，并利用深度Q网络(DQN)训练移动控制算法。

Result: 实验显示，与静态部署相比，DQN代理能将LoS阻塞时间减少高达42%。

Conclusion: 基于学习的移动控制算法在下一代自适应无线网络中具有显著效果。

Abstract: The increasing complexity of wireless environments, characterized by user
mobility and dynamic obstructions, poses challenges for the maintenance of
Line-of-Sight (LoS) connectivity. Mobile base stations (gNBs) stand as a
promising solution by physically relocating to restore or sustain LoS, thereby
necessitating the development of intelligent algorithms for autonomous movement
control.
  As part of the CONVERGE research project, which is developing an experimental
chamber to integrate computer vision (CV) into mobile networks and enhance
Quality of Service (QoS) in dynamic wireless environments, this paper presents
two key contributions. First, we introduce the CONVERGE Chamber Simulator
(CC-SIM), a 3D simulation environment for developing, training, and validating
mobility control algorithms for mobile gNBs. CC-SIM models user and obstacle
mobility, visual occlusion, and Radio Frequency (RF) propagation behavior. It
supports both offline reinforcement learning and real-time testing through
tight integration with a standalone 5G system via the OpenAirInterface (OAI) RF
simulator, enabling validation under realistic network conditions.
  Second, leveraging CC-SIM, we develop a Deep Q-Network (DQN) agent that
learns to reposition the gNB proactively in response to dynamic environmental
changes. Experiments across three representative use cases show that the
trained agent significantly reduces LoS blockage time - by up to 42% - when
compared to static deployments. These results highlight the effectiveness of
learning-based mobility control in adaptive next-generation wireless networks.

</details>


### [29] [A Survey of AI Agent Registry Solutions](https://arxiv.org/abs/2508.03095)
*Aditi Singh,Abul Ehtesham,Ramesh Raskar,Mahesh Lambe,Pradyumna Chari,Jared James Grogan,Abhishek Singh,Saket Kumar*

Main category: cs.NI

TL;DR: 该论文调查了三种AI代理注册系统方法，分别基于不同的元数据模型，比较了它们在安全性、可扩展性、认证和维护性方面的优劣，并提出了未来设计的建议。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理在云、企业和去中心化环境中的普及，标准化注册系统对于发现、身份和能力共享变得至关重要。

Method: 论文分析了三种注册方法：MCP的mcp.json（集中式元注册）、A2A的Agent Card（基于JSON的去中心化交互）和NANDA的AgentFacts（加密可验证的隐私保护模型）。

Result: 研究比较了这三种方法在安全性、可扩展性、认证和维护性四个维度的表现，总结了各自的优缺点。

Conclusion: 论文提出了未来设计和采用AI代理注册系统的建议，强调了跨领域互操作性和动态发现的重要性。

Abstract: As As autonomous AI agents scale across cloud, enterprise, and decentralized
environments, the need for standardized registry systems to support discovery,
identity, and capability sharing has become essential. This paper surveys three
prominent registry approaches each defined by a unique metadata model: MCP's
mcp.json, A2A's Agent Card, and NANDA's AgentFacts. MCP uses a centralized
metaregistry with GitHub authenticated publishing and structured metadata for
server discovery. A2A enables decentralized interaction via JSON-based Agent
Cards, discoverable through well-known URIs, curated catalogs, or direct
configuration. NANDA Index introduces AgentFacts, a cryptographically
verifiable and privacy-preserving metadata model designed for dynamic
discovery, credentialed capabilities, and cross-domain interoperability. These
approaches are compared across four dimensions: security, scalability,
authentication, and maintainability. The paper concludes with suggestions and
recommendations to guide future design and adoption of registry systems for the
Internet of AI Agents.

</details>


### [30] [Using the NANDA Index Architecture in Practice: An Enterprise Perspective](https://arxiv.org/abs/2508.03101)
*Sichao Wang,Ramesh Raskar,Mahesh Lambe,Pradyumna Chari,Rekha Singhal,Shailja Gupta,Rajesh Ranjan,Ken Huang*

Main category: cs.NI

TL;DR: 论文提出了NANDA框架，解决自主AI代理生态系统的基础设施需求，包括全局发现、能力验证和跨协议互操作性，同时引入零信任原则和治理机制。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理生态系统中缺乏安全、可靠且互操作的基础设施，阻碍了大规模多代理协作的部署。

Method: 提出NANDA框架，整合全球代理发现、密码学验证的能力证明（AgentFacts）、跨协议互操作性，并应用零信任代理访问（ZTAA）原则。

Result: NANDA实现了安全、可扩展的多代理协作框架，支持企业治理和合规性，同时保持代理的自主性。

Conclusion: NANDA填补了AI代理能力与安全基础设施需求之间的关键空白，为下一代自主智能系统奠定了基础。

Abstract: The proliferation of autonomous AI agents represents a paradigmatic shift
from traditional web architectures toward collaborative intelligent systems
requiring sophisticated mechanisms for discovery, authentication, capability
verification, and secure collaboration across heterogeneous protocol
environments. This paper presents a comprehensive framework addressing the
fundamental infrastructure requirements for secure, trustworthy, and
interoperable AI agent ecosystems. We introduce the NANDA (Networked AI Agents
in a Decentralized Architecture) framework, providing global agent discovery,
cryptographically verifiable capability attestation through AgentFacts, and
cross-protocol interoperability across Anthropic's Modal Context Protocol
(MCP), Google's Agent-to-Agent (A2A), Microsoft's NLWeb, and standard HTTPS
communications. NANDA implements Zero Trust Agentic Access (ZTAA) principles,
extending traditional Zero Trust Network Access (ZTNA) to address autonomous
agent security challenges including capability spoofing, impersonation attacks,
and sensitive data leakage. The framework defines Agent Visibility and Control
(AVC) mechanisms enabling enterprise governance while maintaining operational
autonomy and regulatory compliance. Our approach transforms isolated AI agents
into an interconnected ecosystem of verifiable, trustworthy intelligent
services, establishing foundational infrastructure for large-scale autonomous
agent deployment across enterprise and consumer environments. This work
addresses the critical gap between current AI agent capabilities and
infrastructure requirements for secure, scalable, multi-agent collaboration,
positioning the foundation for next-generation autonomous intelligent systems.

</details>


### [31] [Morphlux: Programmable chip-to-chip photonic fabrics in multi-accelerator servers for ML](https://arxiv.org/abs/2508.03674)
*Abhishek Vijaya Kumar,Eric Ding,Arjun Devraj,Rachee Singh*

Main category: cs.NI

TL;DR: Morphlux是一种可编程光子互连架构，用于服务器内加速器芯片的光学互连，解决电气互连带宽不足导致的资源利用率低下问题，提升ML模型训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有服务器中加速器芯片间的电气互连带宽不足，导致资源闲置。利用光互连技术突破带宽限制是研究动机。

Method: 开发Morphlux，一种服务器规模的可编程光子互连架构，通过光学互连加速器芯片，并展示其硬件原型。

Result: Morphlux将租户计算分配的带宽提升66%，计算碎片减少70%，训练吞吐量提高1.72倍，并能在1.2秒内逻辑替换故障芯片。

Conclusion: Morphlux证明了光互连在提升服务器内加速器互连效率和资源利用率方面的潜力。

Abstract: We optically interconnect accelerator chips (e.g., GPUs, TPUs) within compute
servers using newly viable programmable chip-to-chip photonic fabrics. In
contrast, today, commercial multi-accelerator compute servers that are
workhorses of ML, use electrical interconnects to network accelerator chips in
the server. However, recent trends have shown an interconnect bandwidth wall
caused by accelerator FLOPS scaling at a faster rate than the bandwidth of the
interconnect between accelerators in the same server. This has led to
under-utilization and idling of GPU resources in cloud datacenters. We develop
Morphlux, a server-scale programmable photonic fabric, to interconnect
accelerators within servers. We show that augmenting state-of-the-art photonic
ML-centric datacenters with Morphlux can improve the bandwidth of tenant
compute allocations by up to 66% and reduce compute fragmentation by up to 70%.
We develop a novel end-to-end hardware prototype of Morphlux to demonstrate
these performance benefits, which translate to 1.72x improvement in training
throughput of ML models. By rapidly programming the server-scale fabric in our
hardware testbed, Morphlux can logically replace a failed accelerator chip in
1.2 seconds.

</details>


### [32] [NANDA Adaptive Resolver: Architecture for Dynamic Resolution of AI Agent Names](https://arxiv.org/abs/2508.03113)
*John Zinky,Hema Seshadri,Mahesh Lambe,Pradyumna Chari,Ramesh Raskar*

Main category: cs.NI

TL;DR: AdaptiveResolver是一种动态微服务架构，旨在解决分布式异构环境中AI代理通信的静态端点解析限制。


<details>
  <summary>Details</summary>
Motivation: 传统DNS或静态URL无法满足复杂环境中的代理通信需求，AdaptiveResolver通过上下文感知和实时选择解决这一问题。

Method: 代理通过Agent Fact卡片在注册表中广告名称和上下文需求，请求代理可通过注册表发现目标代理并获取定制化通信通道。

Result: 该架构支持信任、服务质量和资源约束的协商，实现灵活、安全且可扩展的代理间交互。

Conclusion: AdaptiveResolver为未来复杂的代理生态系统提供了稳健且可演进的通信基础。

Abstract: AdaptiveResolver is a dynamic microservice architecture designed to address
the limitations of static endpoint resolution for AI agent communication in
distributed, heterogeneous environments. Unlike traditional DNS or static URLs,
AdaptiveResolver enables context-aware, real-time selection of communication
endpoints based on factors such as geographic location, system load, agent
capabilities, and security threats. Agents advertise their Agent Name and
context requirements through Agent Fact cards in an Agent Registry/Index. A
requesting Agent discovers a Target Agent using the registry. The Requester
Agent can then resolve the Target Agent Name to obtain a tailored communication
channel to the agent based on actual environmental context between the agents.
The architecture supports negotiation of trust, quality of service, and
resource constraints, facilitating flexible, secure, and scalable
agent-to-agent interactions that go beyond the classic client-server model.
AdaptiveResolver provides a foundation for robust, future-proof agent
communication that can evolve with increasing ecosystem complexity.

</details>


### [33] [Scalability and Performance Evaluation of IEEE 802.11ah IoT Deployments: A Testbed Approach](https://arxiv.org/abs/2508.03146)
*Kostas Chounos,Katerina Kyriakou,Thanasis Korakis*

Main category: cs.NI

TL;DR: 该研究开发并评估了面向5G及未来应用的现代无线物联网架构，通过真实部署的IEEE 802.11ah测试床揭示了其在实际场景中的性能和扩展性限制，发现网络竞争和邻道干扰显著影响性能，并提供了设备能耗的全视角分析。


<details>
  <summary>Details</summary>
Motivation: 分析数据需求增长对无线物联网架构的影响，填补IEEE 802.11ah在实际部署中性能研究的空白。

Method: 构建IEEE 802.11ah办公测试床，进行真实实验，评估性能、扩展性和能耗。

Result: 发现网络竞争和邻道干扰导致吞吐量显著下降，能耗分析为实际部署提供了全面视角。

Conclusion: 研究结果有助于优化物联网到云的部署决策和能耗管理。

Abstract: This work focuses on the development and assessment of modern wireless
Internet of Things (IoT) architectures, with relevance to emerging 5G and
beyond applications. To analyze the growing demands for data, and their impact,
we built an IEEE 802.11ah (WiFi Halow) office testbed for real-world
experimentation. This deployment allows us to uncover the practical performance
and scalability limitations of such networks under various challenging
scenarios. To the best of our knowledge, this is the first study to consider
complex real-world IEEE 802.11ah implementations, aiming specifically to reveal
unexpected performance behaviors, such as significant throughput degradation
arising in closely deployed wireless links. Our findings show that intense
network contention and Adjacent Channel Interference (ACI), drastically impact
the performance of the wireless links involved. Beyond evaluating network
performance, our experimental analysis also considers the energy consumption of
the devices under test, offering a more holistic perspective on the feasibility
of IEEE 802.11ah in real-world deployments. The effective disclosure of such
unexpected phenomena, can lead to well planned decisions and energy consumption
optimization across the IoT to Cloud continuum.

</details>


### [34] [Energy-efficient Federated Learning for UAV Communications](https://arxiv.org/abs/2508.03171)
*Chien-Wei Fu,Meng-Lin Ku*

Main category: cs.NI

TL;DR: 提出了一种无人机辅助的联邦学习框架，通过联合优化无人机轨迹、用户参与、功率分配和数据量控制，以最小化系统能耗。


<details>
  <summary>Details</summary>
Motivation: 为了解决联邦学习在无人机辅助下的能耗问题，并提高学习性能。

Method: 采用交替优化（AO）和逐次凸逼近（SCA）技术，将非凸问题转化为凸问题，设计了迭代能量消耗优化（ECO）算法。

Result: 仿真结果表明，ECO算法在能耗优化上始终优于现有基准方案。

Conclusion: 提出的框架和算法有效降低了系统能耗，并提高了联邦学习的性能。

Abstract: In this paper, we propose an unmanned aerial vehicle (UAV)-assisted federated
learning (FL) framework that jointly optimizes UAV trajectory, user
participation, power allocation, and data volume control to minimize overall
system energy consumption. We begin by deriving the convergence accuracy of the
FL model under multiple local updates, enabling a theoretical understanding of
how user participation and data volume affect FL learning performance. The
resulting joint optimization problem is non-convex; to address this, we employ
alternating optimization (AO) and successive convex approximation (SCA)
techniques to convexify the non-convex constraints, leading to the design of an
iterative energy consumption optimization (ECO) algorithm. Simulation results
confirm that ECO consistently outperform existing baseline schemes.

</details>


### [35] [Directives for Function Offloading in 5G Networks Based on a Performance Characteristics Analysis](https://arxiv.org/abs/2508.03287)
*Falk Dettinger,Matthias Weiß,Daniel Baumann,Martin Sommer,Michael Weyrich*

Main category: cs.NI

TL;DR: 论文评估了5G非独立网络在车辆功能云执行中的表现，重点关注延迟、往返时间和数据包交付率，测试结果表明信号质量良好，数据传输稳定，但云卸载的效果受地理和硬件限制。


<details>
  <summary>Details</summary>
Motivation: 针对5G非独立网络在车辆算法云卸载中的实际应用较少，研究填补了理论与实践的差距，探索其在现实环境中的性能表现。

Method: 在德国不同地理环境中测试两种AI算法（情绪识别和物体识别），使用法兰克福的云端和曼海姆的云端，采用多种部署策略进行分析。

Result: 信号质量平均84%，无连接中断，数据包错误率低于0.1%，传输时间因地而异，处理时间受计算硬件影响。云卸载需往返时间超过150ms才适用。

Conclusion: 5G非独立网络在车辆云卸载中表现稳定，但地理和硬件因素显著影响性能，云卸载的实用性取决于具体场景和时间要求。

Abstract: Cloud-based offloading helps address energy consumption and performance
challenges in executing resource-intensive vehicle algorithms. Utilizing 5G,
with its low latency and high bandwidth, enables seamless vehicle-to-cloud
integration. Currently, only non-standalone 5G is publicly available, and
real-world applications remain underexplored compared to theoretical studies.
This paper evaluates 5G non-standalone networks for cloud execution of vehicle
functions, focusing on latency, Round Trip Time, and packet delivery. Tests
used two AI-based algorithms -- emotion recognition and object recognition --
along an 8.8 km route in Baden-W\"urttemberg, Germany, encompassing urban,
rural, and forested areas. Two platforms were analyzed: a cloudlet in Frankfurt
and a cloud in Mannheim, employing various deployment strategies like
conventional applications and containerized and container-orchestrated setups.
Key findings highlight an average signal quality of 84 %, with no connectivity
interruptions despite minor drops in built-up areas. Packet analysis revealed a
Packet Error Rate below 0.1 % for both algorithms. Transfer times varied
significantly depending on the geographical location and the backend servers'
network connections, while processing times were mainly influenced by the
computation hardware in use. Additionally, cloud offloading seems only be a
suitable option, when a round trip time of more than 150 ms is possible.

</details>


### [36] [Bidirectional TLS Handshake Caching for Constrained Industrial IoT Scenarios](https://arxiv.org/abs/2508.03321)
*Jörn Bodenhausen,Simon Mangel,Thomas Vogt,Martin Henze*

Main category: cs.NI

TL;DR: BiTHaC通过双向TLS握手缓存减少工业物联网中的带宽和计算开销。


<details>
  <summary>Details</summary>
Motivation: 工业物联网设备资源受限，传统TLS握手开销大，需优化。

Method: 利用重复TLS握手静态部分（如证书）实现缓存，避免冗余传输。

Result: 带宽消耗减少61.1%，计算开销降低8.5%，内存开销可控。

Conclusion: BiTHaC在保证TLS安全性的同时显著提升效率。

Abstract: While TLS has become the de-facto standard for end-to-end security, its use
to secure critical communication in evolving industrial IoT scenarios is
severely limited by prevalent resource constraints of devices and networks.
Most notably, the TLS handshake to establish secure connections incurs
significant bandwidth and processing overhead that often cannot be handled in
constrained environments. To alleviate this situation, we present BiTHaC which
realizes bidirectional TLS handshake caching by exploiting that significant
parts of repeated TLS handshakes, especially certificates, are static. Thus,
redundant information neither needs to be transmitted nor corresponding
computations performed, saving valuable bandwidth and processing resources. By
implementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth
consumption of TLS handshakes by up to 61.1% and the computational overhead by
up to 8.5%, while incurring only well-manageable memory overhead and preserving
the strict security guarantees of TLS.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [37] [VisAug: Facilitating Speech-Rich Web Video Navigation and Engagement with Auto-Generated Visual Augmentations](https://arxiv.org/abs/2508.03410)
*Baoquan Zhao,Xiaofan Ma,Qianshi Pang,Ruomei Wang,Fan Zhou,Shujin Lin*

Main category: cs.MM

TL;DR: VisAug是一种新型交互系统，通过基于视频语音内容生成视觉增强，改善以语音为主的视频导航和参与度。


<details>
  <summary>Details</summary>
Motivation: 随着数字技术的普及，语音为主的视频内容激增，现有视觉为主的视频系统难以满足其需求。

Method: 开发VisAug系统，自动根据语音内容生成视觉增强，提升视频导航和参与度。

Result: VisAug显著提升了信息消费和用户参与度。

Conclusion: VisAug为日益视频化的数字环境提供了有效解决方案。

Abstract: The widespread adoption of digital technology has ushered in a new era of
digital transformation across all aspects of our lives. Online learning,
social, and work activities, such as distance education, videoconferencing,
interviews, and talks, have led to a dramatic increase in speech-rich video
content. In contrast to other video types, such as surveillance footage, which
typically contain abundant visual cues, speech-rich videos convey most of their
meaningful information through the audio channel. This poses challenges for
improving content consumption using existing visual-based video summarization,
navigation, and exploration systems. In this paper, we present VisAug, a novel
interactive system designed to enhance speech-rich video navigation and
engagement by automatically generating informative and expressive visual
augmentations based on the speech content of videos. Our findings suggest that
this system has the potential to significantly enhance the consumption and
engagement of information in an increasingly video-driven digital landscape.

</details>


### [38] [OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering Dataset](https://arxiv.org/abs/2508.03583)
*Quang-Linh Tran,Binh Nguyen,Gareth J. F. Jones,Cathal Gurrin*

Main category: cs.MM

TL;DR: 该论文提出了一个名为OpenLifelogQA的新型生命日志问答数据集，基于18个月的生命日志数据，包含14,187对多样化且难度不同的问答，支持生命日志技术的研究。


<details>
  <summary>Details</summary>
Motivation: 现有的生命日志问答研究资源有限，数据集多为小型或合成数据，限制了研究的进展。

Method: 论文基于18个月的生命日志数据，构建了一个开放式的问答数据集OpenLifelogQA，包含多种类型和难度的问答对，并进行了基线实验验证。

Result: 基线实验结果显示，BERT Score为89.7%，ROUGE-L为25.87%，LLM Score为3.9665（来自LLaVA-NeXT-Interleave 7B模型）。

Conclusion: OpenLifelogQA数据集的发布将支持生命日志技术的进一步研究，例如实现基于聊天的个人生命日志助手。

Abstract: Lifelogging refers to the process of passively collecting, storing, and
analysing personal daily life data using wearable devices. This data can
support applications in memory preservation and enhancement. For example, using
an ask-and-answer strategy, question-answering (QA) on lifelog data opens an
interactive and interesting way to explore memorable events and insights into
daily life. However, research resources for QA on lifelog data are limited to
small-sized or synthetic QA datasets. In this paper, we present a novel lifelog
QA dataset called OpenLifelogQA, building upon an 18-month lifelog dataset. Our
dataset focuses on an open-ended and practical QA with real-world application
in daily lifelog usage. We construct 14,187 pairs of Q&A with diverse types and
difficulty levels. A baseline experiment is reported for this dataset with
competitive average performance of 89.7% BERT Score, 25.87% ROUGE-L and 3.9665
LLM Score from LLaVA-NeXT-Interleave 7B model. We release this Q&A dataset to
the research community to support new research into lifelog technologies, such
as enabling personal chat-based assistants for lifelog data to become a
reality.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [39] [When are two algorithms the same? Towards addressing Hilbert's 24th problem](https://arxiv.org/abs/2508.02764)
*Konstantin Doubrovinski*

Main category: cs.LO

TL;DR: 论文探讨了如何判断两个定理证明或程序是否'本质上相同'，提出了基于递归理论和柯尔莫哥洛夫复杂度的简约方法。


<details>
  <summary>Details</summary>
Motivation: 受希尔伯特启发，研究定理证明或程序的'本质相同性'。

Method: 基于递归理论和柯尔莫哥洛夫复杂度的方法。

Result: 提出了一种简约的解决方案。

Conclusion: 为定理证明和程序的相似性提供了新的理论框架。

Abstract: The informal question of when two theorem proofs are "essentially the same"
goes back to David Hilbert, who considered adding it (or something largely
equivalent) to his famous list of open problems, but eventually decided to
leave it out. Given that the notion of a formal proof is closely related to
that of a (computer) program, i.e. a recursive function, it may be useful to
ask the same question with regard to programs instead. Here we propose a
minimalistic approach to this question within Recursion Theory, building
heavily on the use of Kolmogorov Complexity.

</details>


### [40] [Intensional FOL over Belnap's Billatice for Strong-AI Robotics](https://arxiv.org/abs/2508.02774)
*Zoran Majkic*

Main category: cs.LO

TL;DR: 本文提出了一种基于扩展的一阶逻辑（IFOL）方法，以解决强人工智能（AGI）中的悖论和不完整知识问题。


<details>
  <summary>Details</summary>
Motivation: 目标是开发一种接近人类智能的AGI机器人，其需要具备自我意识、学习能力和规划能力，但标准二值逻辑存在悖论和不完整知识的问题。

Method: 提出了一种扩展的IFOL逻辑，基于Belnap的四值格子理论，能够处理真值排序和知识排序。

Result: 该方法解决了标准二值逻辑中的悖论和不完整知识问题。

Conclusion: 扩展的IFOL逻辑为AGI机器人的智能建模提供了更强大的工具。

Abstract: AGI (Strong AI) aims to create intelligent robots that are quasi
indistinguishable from the human mind. Like a child, the AGI robot would have
to learn through input and experiences, constantly progressing and advancing
its abilities over time. The AGI robot would require an intelligence more close
to human's intelligence: it would have a self-aware consciousness that has the
ability to solve problems, learn, and plan. Based on this approach an
Intensional many-sorted First-order Logic (IFOL), as an extension of a standard
FOL with Tarskian's semantics, is proposed in order to avoid the problems of
standard 2-valued FOL with paradoxes (inconsistent formulae) and a necessity
for robots to work with incomplete (unknown) knowledge as well. This is a more
sophisticated version of IFOL with the same syntax but different semantics,
able to deal with truth-ordering and knowledge-ordering as well, based on the
well known Belnap's billatice with four truth-values that extend the set of
classical two truth-values.

</details>


### [41] [Analysis of logics with arithmetic](https://arxiv.org/abs/2508.03574)
*Michael Benedikt,Chia-Hsuan Lu,Tony Tan*

Main category: cs.LO

TL;DR: 本文提出了关于计数和算术逻辑的有限可满足性的新结果，包括二变量逻辑与一元公式间的基数比较的紧复杂度界限，以及局部Presburger量词逻辑的界限。


<details>
  <summary>Details</summary>
Motivation: 研究有限可满足性在逻辑和算术中的复杂性，简化关键结果的证明过程。

Method: 通过分析二变量逻辑与一元公式的基数比较，以及局部Presburger量词逻辑，重新梳理了相关理论。

Result: 得出了紧的复杂度界限，并简化了现有结果的证明。

Conclusion: 研究不仅推进了对有限可满足性的理解，还简化了相关逻辑的理论基础。

Abstract: We present new results on finite satisfiability of logics with counting and
arithmetic. This includes tight bounds on the complexity for two-variable logic
with counting and cardinality comparisons between unary formulas, and also on
logics with so-called local Presburger quantifiers. In the process, we provide
simpler proofs of some key prior results on finite satisfiability and
semi-linearity of the spectrum for these logics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [42] [LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data](https://arxiv.org/abs/2508.02679)
*Wayupuk Sommuang,Kun Kerdthaisong,Pasin Buakhaw,Aslan B. Wong,Nutchanon Yongsatianchot*

Main category: cs.HC

TL;DR: 本文提出了一种基于LLM代理的模拟框架，用于模拟学生活动与心理健康，利用StudentLife数据集实现，并通过动态更新代理心理状态来探索新场景和干预研究。


<details>
  <summary>Details</summary>
Motivation: 学生心理健康对学业成功至关重要，但目前的研究缺乏对复杂行为和心理状态动态交互的深入模拟。

Method: 使用LLM代理模拟学生行为，通过问卷调查和智能手机感知数据初始化代理，并结合提示技术、记忆系统和活动管理策略动态更新心理状态。

Result: 该框架不仅能复现现有数据，还能探索新场景（如同伴影响和社交媒体作用），并支持干预研究和虚拟访谈。

Conclusion: LLM驱动的行为建模为理解和支持学生心理健康提供了新途径。

Abstract: Students' mental well-being is vital for academic success, with activities
such as studying, socializing, and sleeping playing a role. Current mobile
sensing data highlight this intricate link using statistical and machine
learning analyses. We propose a novel LLM agent-based simulation framework to
model student activities and mental health using the StudentLife Dataset. Each
LLM agent was initialized with personality questionnaires and guided by
smartphone sensing data throughout the simulated semester. These agents predict
individual behaviors, provide self-reported mental health data via ecological
momentary assessments (EMAs), and complete follow-up personality
questionnaires. To ensure accuracy, we investigated various prompting
techniques, memory systems, and activity-based mental state management
strategies that dynamically update an agent's mental state based on their daily
activities. This simulation goes beyond simply replicating existing data. This
allows us to explore new scenarios that are not present in the original
dataset, such as peer influence through agent-to-agent interactions and the
impact of social media. Furthermore, we can conduct intervention studies by
manipulating activity patterns via sensing signals and personality traits using
questionnaire responses. This provides valuable insights into the behavioral
changes that could enhance student well-being. The framework also facilitates
hypothetical interviews with LLM agents, offering deeper insights into their
mental health. This study showcases the power of LLM-driven behavioral modeling
with sensing data, opening new avenues for understanding and supporting student
mental health.

</details>


### [43] [AnnoSense: A Framework for Physiological Emotion Data Collection in Everyday Settings for AI](https://arxiv.org/abs/2508.02680)
*Pragya Singh,Ankush Gupta,Mohan Kumar,Pushpendra Singh*

Main category: cs.HC

TL;DR: 该论文探讨了在真实环境中收集情感数据的挑战，提出了一个名为AnnoSense的框架，以支持情感AI的数据收集，并通过专家评估验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备的普及，情感监测在日常环境中的应用前景广阔，但高质量数据和准确标注仍是AI算法有效的关键。

Method: 通过75份调查问卷、32次公众访谈和3次焦点小组讨论（共12位心理健康专家）收集利益相关者的反馈，开发了AnnoSense框架，并由25位情感AI专家评估。

Result: AnnoSense框架在清晰性、实用性和适应性方面得到了专家的认可。

Conclusion: AnnoSense框架有望改进真实环境中情感数据的收集与分析，为未来情感AI研究提供了新方向。

Abstract: Emotional and mental well-being are vital components of quality of life, and
with the rise of smart devices like smartphones, wearables, and artificial
intelligence (AI), new opportunities for monitoring emotions in everyday
settings have emerged. However, for AI algorithms to be effective, they require
high-quality data and accurate annotations. As the focus shifts towards
collecting emotion data in real-world environments to capture more authentic
emotional experiences, the process of gathering emotion annotations has become
increasingly complex. This work explores the challenges of everyday emotion
data collection from the perspectives of key stakeholders. We collected 75
survey responses, performed 32 interviews with the public, and 3 focus group
discussions (FGDs) with 12 mental health professionals. The insights gained
from a total of 119 stakeholders informed the development of our framework,
AnnoSense, designed to support everyday emotion data collection for AI. This
framework was then evaluated by 25 emotion AI experts for its clarity,
usefulness, and adaptability. Lastly, we discuss the potential next steps and
implications of AnnoSense for future research in emotion AI, highlighting its
potential to enhance the collection and analysis of emotion data in real-world
contexts.

</details>


### [44] [Real-World Receptivity to Adaptive Mental Health Interventions: Findings from an In-the-Wild Study](https://arxiv.org/abs/2508.02817)
*Nilesh Kumar Sahu,Aditya Sneh,Snehil Gupta,Haroon R Lone*

Main category: cs.HC

TL;DR: 研究探讨了用户对心理健康干预的接受度（接受和可行性），使用强化学习算法优化干预时机。


<details>
  <summary>Details</summary>
Motivation: 移动健康技术虽能实时监测心理健康，但对用户接受干预的研究较少，尤其是如何根据情境优化干预。

Method: 通过两周的实地研究，使用自定义App收集被动传感器数据和主动情境报告，采用Thompson Sampling算法优化干预。

Result: 研究发现，被动感知数据显著影响用户对干预的接受度，强化学习能有效优化干预时机。

Conclusion: 研究为设计情境感知的自适应干预提供了新见解，强调干预需兼顾及时性和可操作性。

Abstract: The rise of mobile health (mHealth) technologies has enabled real-time
monitoring and intervention for mental health conditions using passively sensed
smartphone data. Building on these capabilities, Just-in-Time Adaptive
Interventions (JITAIs) seek to deliver personalized support at opportune
moments, adapting to users' evolving contexts and needs. Although prior
research has examined how context affects user responses to generic
notifications and general mHealth messages, relatively little work has explored
its influence on engagement with actual mental health interventions.
Furthermore, while much of the existing research has focused on detecting when
users might benefit from an intervention, less attention has been paid to
understanding receptivity, i.e., users' willingness and ability to engage with
and act upon the intervention.
  In this study, we investigate user receptivity through two components:
acceptance(acknowledging or engaging with a prompt) and feasibility (ability to
act given situational constraints). We conducted a two-week in-the-wild study
with 70 students using a custom Android app, LogMe, which collected passive
sensor data and active context reports to prompt mental health interventions.
The adaptive intervention module was built using Thompson Sampling, a
reinforcement learning algorithm. We address four research questions relating
smartphone features and self-reported contexts to acceptance and feasibility,
and examine whether an adaptive reinforcement learning approach can optimize
intervention delivery by maximizing a combined receptivity reward. Our results
show that several types of passively sensed data significantly influenced user
receptivity to interventions. Our findings contribute insights into the design
of context-aware, adaptive interventions that are not only timely but also
actionable in real-world settings.

</details>


### [45] [NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification](https://arxiv.org/abs/2508.02823)
*Wenshuo Zhang,Leixian Shen,Shuchang Xu,Jindu Wang,Jian Zhao,Huamin Qu,Linping Yuan*

Main category: cs.HC

TL;DR: 论文提出NeuroSync，通过直接意图-任务匹配解决用户意图与LLM生成代码之间的双向模糊性问题，提升对齐效果和编码效率。


<details>
  <summary>Details</summary>
Motivation: 解决用户意图与生成代码间的偏差问题，因用户意图和编码任务均非线性，需通过线性提示和代码序列表达。

Method: 提出直接意图-任务匹配范式，实现LLM理解的外部化和可视化编辑，并通过NeuroSync实现知识蒸馏与用户交互。

Result: 实验显示NeuroSync增强意图-任务对齐，降低认知负担，提高编码效率。

Conclusion: 直接意图-任务匹配和NeuroSync能有效解决用户与LLM交互中的模糊性问题，提升用户体验。

Abstract: Conversational LLMs have been widely adopted by domain users with limited
programming experience to solve domain problems. However, these users often
face misalignment between their intent and generated code, resulting in
frustration and rounds of clarification. This work first investigates the cause
of this misalignment, which dues to bidirectional ambiguity: both user intents
and coding tasks are inherently nonlinear, yet must be expressed and
interpreted through linear prompts and code sequences. To address this, we
propose direct intent-task matching, a new human-LLM interaction paradigm that
externalizes and enables direct manipulation of the LLM understanding, i.e.,
the coding tasks and their relationships inferred by the LLM prior to code
generation. As a proof-of-concept, this paradigm is then implemented in
NeuroSync, which employs a knowledge distillation pipeline to extract LLM
understanding, user intents, and their mappings, and enhances the alignment by
allowing users to intuitively inspect and edit them via visualizations. We
evaluate the algorithmic components of NeuroSync via technical experiments, and
assess its overall usability and effectiveness via a user study (N=12). The
results show that it enhances intent-task alignment, lowers cognitive effort,
and improves coding efficiency.

</details>


### [46] [Critical Challenges in Content Moderation for People Who Use Drugs (PWUD): Insights into Online Harm Reduction Practices from Moderators](https://arxiv.org/abs/2508.02868)
*Kaixuan Wang,Loraine Clarke,Carl-Cyril J Dreue,Guancheng Zhou,Jason T. Jacques*

Main category: cs.HC

TL;DR: 该论文研究Reddit上药物使用者的在线社区管理，探讨了管理工作的独特性及其挑战，并提出了改进社会技术设计的建议。


<details>
  <summary>Details</summary>
Motivation: 调查在线社区对药物使用者的支持作用，以及现有社会技术系统对管理工作的不足。

Method: 通过采访Reddit上药物使用者社区的经验丰富的管理员，分析管理工作的特点。

Result: 发现管理工作具有三个独特挑战：专业风险评估、紧急危机应对及平台政策与社区安全目标的冲突。当前系统不足以支持这些社区。

Conclusion: 提出改进社会技术设计的两个方向：自动化工具支持人类决策和高级别指令取代低级别规则编程，并强调其对减少危害的影响。

Abstract: Online communities serve as essential support channels for People Who Use
Drugs (PWUD), providing access to peer support and harm reduction information.
The moderation of these communities involves consequential decisions affecting
member safety, yet existing sociotechnical systems provide insufficient support
for moderators. Through interviews with experienced moderators from PWUD forums
on Reddit, we analyse the unique nature of this work. We argue that this work
constitutes a distinct form of public health intervention characterised by
three moderation challenges: the need for specialised, expert risk assessment;
time-critical crisis response; and the navigation of a structural conflict
between platform policies and community safety goals. We demonstrate how
current moderation systems are insufficient in supporting PWUD communities. For
example, policies minimising platforms' legal exposure to illicit activities
can inadvertently push moderators to implement restrictive rules to protect
community's existence, which can limit such a vulnerable group's ability to
share potentially life-saving resources online. We conclude by identifying two
necessary shifts in sociotechnical design to support moderators' work: first,
moving to automated tools that support human sensemaking in contexts with
competing interests; and second, shifting from systems that require moderators
to perform low-level rule programming to those that enable high-level,
example-based instruction. Further, we highlight how the design of
sociotechnical systems in online spaces could impact harm reduction efforts
aimed at improving health outcomes for PWUD communities.

</details>


### [47] [VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People](https://arxiv.org/abs/2508.02958)
*Daniel Killough,Justin Feng,Zheng Xue "ZX" Ching,Daniel Wang,Rithvik Dyava,Yapeng Tian,Yuhang Zhao*

Main category: cs.HC

TL;DR: VRSight 是一个端到端系统，通过 AI 模型识别 VR 场景并生成空间音频反馈，帮助盲人无需开发者干预即可在 VR 中交互。


<details>
  <summary>Details</summary>
Motivation: 目前 VR 对盲人的可访问性不足，主流 VR 应用缺乏无障碍设计。

Method: 利用 AI 模型（如物体检测、深度估计和基于 LLM 的氛围解释）识别 VR 场景，并生成空间音频反馈。

Result: 参与者成功使用 VRSight 在 VR 中完成社交任务（如识别虚拟角色和可用座位）。

Conclusion: VRSight 有效提升了 VR 对盲人群体的可访问性，无需额外开发工作量。

Abstract: Virtual Reality (VR) is inaccessible to blind people. While research has
investigated many techniques to enhance VR accessibility, they require
additional developer effort to integrate. As such, most mainstream VR apps
remain inaccessible as the industry de-prioritizes accessibility. We present
VRSight, an end-to-end system that recognizes VR scenes post hoc through a set
of AI models (e.g., object detection, depth estimation, LLM-based atmosphere
interpretation) and generates tone-based, spatial audio feedback, empowering
blind users to interact in VR without developer intervention. To enable virtual
element detection, we further contribute DISCOVR, a VR dataset consisting of 30
virtual object classes from 17 social VR apps, substituting real-world datasets
that remain not applicable to VR contexts. Nine participants used VRSight to
explore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in
facilitating social tasks like avatar awareness and available seat
identification.

</details>


### [48] [Survey of Large Language Models in Extended Reality: Technical Paradigms and Application Frontiers](https://arxiv.org/abs/2508.03014)
*Jingyan Wang,Yang Zhao,Haotian Mao,Xubo Yang*

Main category: cs.HC

TL;DR: 该论文综述了大型语言模型（LLM）与扩展现实（XR）结合的现状，提供了技术范式和应用领域的分类，并探讨了发展趋势和挑战。


<details>
  <summary>Details</summary>
Motivation: 通过融合LLM与XR技术，提升用户在沉浸式环境中的交互体验，推动智能XR系统的发展。

Method: 提出基于技术范式的分类法，包括交互代理控制、XR开发工具包和生成式场景合成，并分析这些技术如何支持XR应用。

Result: 论文总结了LLM与XR结合的技术和应用趋势，并指出了设计考虑和开放挑战。

Conclusion: 该研究为研究人员和从业者提供了指导，有助于推动智能XR体验的前沿发展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation, and their integration with
Extended Reality (XR) is poised to transform how users interact with immersive
environments. This survey provides a comprehensive review of recent
developments at the intersection of LLMs and XR, offering a structured
organization of research along both technical and application dimensions. We
propose a taxonomy of LLM-enhanced XR systems centered on key technical
paradigms -- such as interactive agent control, XR development toolkits, and
generative scene synthesis -- and discuss how these paradigms enable novel
capabilities in XR. In parallel, we examine how LLM-driven techniques support
practical XR applications across diverse domains, including immersive
education, clinical healthcare, and industrial manufacturing. By connecting
these technical paradigms with application frontiers, our survey highlights
current trends, delineates design considerations, and identifies open
challenges in building LLM-augmented XR systems. This work provides insights
that can guide researchers and practitioners in advancing the state of the art
in intelligent XR experiences.

</details>


### [49] [Facilitating Visual Media Exploration for Blind and Low Vision Users through AI-Powered Interactive Storytelling](https://arxiv.org/abs/2508.03061)
*Shuchang Xu*

Main category: cs.HC

TL;DR: 该研究提出了一种AI驱动的交互式叙事范式，帮助盲人和低视力用户在连贯的叙事中探索视觉媒体，通过三种技术实现：层次叙事、并行叙事和分支叙事。


<details>
  <summary>Details</summary>
Motivation: 现有工具将视觉媒体探索与主要叙事分离，破坏了叙事连贯性，增加了认知负担。研究旨在通过AI技术改善这一现象，提升用户体验。

Method: 使用AI生成交互式叙事，开发了分层叙事、并行叙事和分支叙事三种技术，分别针对照片集、视频评论和360度视频的探索。

Result: 研究证明，AI驱动的交互式叙事能在多种媒体格式中平衡用户自主性与叙事连贯性。

Conclusion: 未来工作将进一步提升叙事的个性化和表达力，为盲人和低视力用户提供更丰富的体验。

Abstract: Empowering blind and low vision (BLV) users to explore visual media improves
content comprehension, strengthens user agency, and fulfills diverse
information needs. However, most existing tools separate exploration from the
main narration, which disrupts the narrative flow, increases cognitive load,
and limits deep engagement with visual media. To address these challenges, my
PhD research introduces the paradigm of AI-powered interactive storytelling,
which leverages AI to generate interactive narratives, enabling BLV users to
explore visual media within a coherent storytelling experience. I have
operationalized this paradigm through three techniques: (1) Hierarchical
Narrative, which supports photo-collection exploration at different levels of
detail; (2) Parallel Narrative, which provides seamless access to time-synced
video comments; and (3) Branching Narrative, which enables immersive navigation
of 360{\deg} videos. Together, these techniques demonstrate that AI-powered
interactive storytelling can effectively balance user agency with narrative
coherence across diverse media formats. My future work will advance this
paradigm by enabling more personalized and expressive storytelling experiences
for BLV audiences.

</details>


### [50] [StoryEnsemble: Enabling Dynamic Exploration & Iteration in the Design Process with AI and Forward-Backward Propagation](https://arxiv.org/abs/2508.03182)
*Sangho Suh,Michael Lai,Kevin Pu,Steven P. Dow,Tovi Grossman*

Main category: cs.HC

TL;DR: 论文研究了设计过程中因时间和资源限制导致探索和迭代不足的问题，开发了一个名为StoryEnsemble的工具，通过AI和节点链接界面支持动态探索，用户研究表明其能有效提升迭代效率。


<details>
  <summary>Details</summary>
Motivation: 设计过程中时间和资源的限制常阻碍设计师进行广泛探索和反馈收集，难以在实践中坚持核心设计原则，为此研究团队试图通过技术手段解决这一问题。

Method: 研究团队进行了15人的形成性研究，开发了StoryEnsemble工具，结合AI和节点链接界面，支持动态探索和多方向迭代。

Result: 用户研究表明，StoryEnsemble能实现快速、多方向的迭代，并支持灵活的设计阶段导航。

Conclusion: 该研究展示了AI如何通过新颖的交互方式支持更流畅、易用和吸引人的设计迭代实践。

Abstract: Design processes involve exploration, iteration, and movement across
interconnected stages such as persona creation, problem framing, solution
ideation, and prototyping. However, time and resource constraints often hinder
designers from exploring broadly, collecting feedback, and revisiting earlier
assumptions-making it difficult to uphold core design principles in practice.
To better understand these challenges, we conducted a formative study with 15
participants-comprised of UX practitioners, students, and instructors. Based on
the findings, we developed StoryEnsemble, a tool that integrates AI into a
node-link interface and leverages forward and backward propagation to support
dynamic exploration and iteration across the design process. A user study with
10 participants showed that StoryEnsemble enables rapid, multi-directional
iteration and flexible navigation across design stages. This work advances our
understanding of how AI can foster more iterative design practices by
introducing novel interactions that make exploration and iteration more fluid,
accessible, and engaging.

</details>


### [51] [Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse](https://arxiv.org/abs/2508.03216)
*Hikari Yanagawa,Yuichi Hiroi,Satomi Tokida,Yuji Hatada,Takefumi Hiraki*

Main category: cs.HC

TL;DR: 论文提出了一种动态适应用户兴趣和意图的导航助手Navigation Pixie，通过在商业元宇宙平台上的实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 商业元宇宙平台缺乏动态适应用户兴趣和意图的导航辅助工具，现有研究在商业化环境中的应用仍具挑战性。

Method: 采用松散耦合架构，结合结构化空间元数据和基于LLM的自然语言处理，减少平台依赖性。

Result: 实验表明Navigation Pixie显著增加了用户在平台的停留时间和自由探索行为，主观评价显示PC和VR-HMD环境中用户偏好不同。

Conclusion: 该研究推动了VR交互设计中对话式空间导航助手的发展，并建立了跨平台评估方法论。

Abstract: While commercial metaverse platforms offer diverse user-generated content,
they lack effective navigation assistance that can dynamically adapt to users'
interests and intentions. Although previous research has investigated on-demand
agents in controlled environments, implementation in commercial settings with
diverse world configurations and platform constraints remains challenging.
  We present Navigation Pixie, an on-demand navigation agent employing a
loosely coupled architecture that integrates structured spatial metadata with
LLM-based natural language processing while minimizing platform dependencies,
which enables experiments on the extensive user base of commercial metaverse
platforms. Our cross-platform experiments on commercial metaverse platform
Cluster with 99 PC client and 94 VR-HMD participants demonstrated that
Navigation Pixie significantly increased dwell time and free exploration
compared to fixed-route and no-agent conditions across both platforms.
Subjective evaluations revealed consistent on-demand preferences in PC
environments versus context-dependent social perception advantages in VR-HMD.
This research contributes to advancing VR interaction design through
conversational spatial navigation agents, establishes cross-platform evaluation
methodologies revealing environment-dependent effectiveness, and demonstrates
empirical experimentation frameworks for commercial metaverse platforms.

</details>


### [52] [Quo-Vadis Multi-Agent Automotive Research? Insights from a Participatory Workshop and Questionnaire](https://arxiv.org/abs/2508.03281)
*Pavlo Bazilinskyy,Francesco Walker,Debargha Dey,Tram Thi Minh Tran,Hyungchai Park,Hyochang Kim,Hyunmin Kang,Patrick Ebel*

Main category: cs.HC

TL;DR: 论文探讨了混合交通环境中多主体研究的挑战与机遇，强调需跨学科方法和更好的工具支持。


<details>
  <summary>Details</summary>
Motivation: 研究混合交通环境（包含自动驾驶、人工驾驶车辆和弱势道路使用者）中多主体研究的现状及其挑战。

Method: 通过参与式研讨会（N=15）和问卷调查（N=19）探索多主体研究方法论。

Result: 多主体方法价值被广泛认可，但实际操作和技术障碍阻碍其应用。

Conclusion: 需要跨学科方法、更好的工具和仿真环境以支持可扩展且符合伦理的多主体研究。

Abstract: The transition to mixed-traffic environments that involve automated vehicles,
manually operated vehicles, and vulnerable road users presents new challenges
for human-centered automotive research. Despite this, most studies in the
domain focus on single-agent interactions. This paper reports on a
participatory workshop (N = 15) and a questionnaire (N = 19) conducted during
the AutomotiveUI '24 conference to explore the state of multi-agent automotive
research. The participants discussed methodological challenges and
opportunities in real-world settings, simulations, and computational modeling.
Key findings reveal that while the value of multi-agent approaches is widely
recognized, practical and technical barriers hinder their implementation. The
study highlights the need for interdisciplinary methods, better tools, and
simulation environments that support scalable, realistic, and ethically
informed multi-agent research.

</details>


### [53] [Enhancing Joint Human-AI Inference in Robot Missions: A Confidence-Based Approach](https://arxiv.org/abs/2508.03293)
*Duc-An Nguyen,Clara Colombatto,Steve Fleming,Ingmar Posner,Nick Hawes,Raunak Bhattacharyya*

Main category: cs.HC

TL;DR: 研究探讨了在机器人远程操作任务中，基于最高置信度选择的联合人-AI推理，发现其准确性受AI信心校准的影响，且人类会根据AI推荐调整推理。结果强调良好校准的AI决策支持系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助任务中，人类操作员常因判断失误未能实现互补性，研究旨在探索联合人-AI推理，以提升任务效果。

Method: 通过100名参与者的用户研究，在模拟机器人远程操作任务中测试基于置信度选择的联合推理，并分析AI信心校准的影响。

Result: 联合推理准确性更高，且人类会根据AI推荐调整推理；校准不佳的AI系统会损害团队表现。

Conclusion: 强调AI决策支持系统需具备良好的元认知敏感性，以实现高效的人-AI协作。

Abstract: Joint human-AI inference holds immense potential to improve outcomes in
human-supervised robot missions. Current day missions are generally in the
AI-assisted setting, where the human operator makes the final inference based
on the AI recommendation. However, due to failures in human judgement on when
to accept or reject the AI recommendation, complementarity is rarely achieved.
We investigate joint human-AI inference where the inference made with higher
confidence is selected. Through a user study with N=100 participants on a
representative simulated robot teleoperation task, specifically studying the
inference of robots' control delays we show that: a) Joint inference accuracy
is higher and its extent is regulated by the confidence calibration of the AI
agent, and b) Humans change their inferences based on AI recommendations and
the extent and direction of this change is also regulated by the confidence
calibration of the AI agent. Interestingly, our results show that pairing
poorly-calibrated AI-DSS with humans hurts performance instead of helping the
team, reiterating the need for AI-based decision support systems with good
metacognitive sensitivity. To the best of our knowledge, our study presents the
first application of a maximum-confidence-based heuristic for joint human-AI
inference within a simulated robot teleoperation task.

</details>


### [54] [Remini: Leveraging Chatbot-Mediated Mutual Reminiscence for Promoting Positive Affect and Feeling of Connectedness among Loved Ones](https://arxiv.org/abs/2508.03355)
*Zhuoqun Jiang,ShunYi Yeo,Wei Xuan Donovan Seow,Simon Perrault*

Main category: cs.HC

TL;DR: 论文提出Remini聊天机器人，通过结构化对话支持伴侣间的互惠自我披露，增强情感联系和幸福感。


<details>
  <summary>Details</summary>
Motivation: 现有技术工具多关注个体反思或单向叙事，忽略了互动对话的重要性，因此设计了Remini。

Method: 基于SFAM框架，Remini通过五个叙事阶段引导对话，并在48人的混合方法研究中与基线聊天机器人对比。

Result: 研究发现Remini显著提升积极情绪、连接感和参与度，并促进更详细的叙事共建。

Conclusion: 研究为通过互惠回忆增强人际联系的对话代理提供了实证设计依据。

Abstract: Mutual reminiscence, defined as revisiting shared positive memories through
reciprocal self-disclosure, strengthens emotional bonds, enhances well-being,
and deepens intimacy. However, most technology-mediated reminiscence tools
emphasize individual reflection or one-way storytelling, which overlooks the
dynamic, interactive dialogue essential for meaningful mutual reminiscence. To
address this limitation, we introduce Remini, a chatbot designed to support
reciprocal self-disclosure between close partners such as couples, friends, or
family members. Grounded in the Social Functions of Autobiographical Memory
(SFAM) framework, Remini uses conversational AI to guide emotionally rich
exchanges through five narrative phases: rapport building, memory narration,
elaboration, reflection, and summary. In a mixed-method, both between- and
within- subjects study (N = 48, 24 dyads), we compare Remini to a baseline
chatbot that offers minimal memory-trigger prompts. Our findings show that
structured guidance from Remini significantly improves positive affect, feeling
of connection, and engagement. It also fosters more detailed narrative
co-construction and greater reciprocal self-disclosure. Participant feedback
highlights the practical value, perceived benefits, and design considerations
of chatbot-mediated reminiscence. We contribute empirically grounded design
implications for conversational agents that strengthen human connection through
mutual reminiscence.

</details>


### [55] [The Science Fiction Science Method](https://arxiv.org/abs/2508.03430)
*Iyad Rahwan,Azim Shariff,Jean-François Bonnefon*

Main category: cs.HC

TL;DR: 论文提出一种名为‘科幻科学’的方法，通过实验模拟未来技术并量化评估参与者态度和行为，以预测技术的社会影响。


<details>
  <summary>Details</summary>
Motivation: 传统定性方法难以预测未来技术的社会行为影响，需要定量实验方法辅助发展

Method: 使用实验方法模拟未来技术，控制变量并量化参与者态度和行为

Result: 科幻科学方法未被广泛接受，但其潜力可通过改进方法实现

Conclusion: 通过规范和改进方法，科幻科学可成为预测未来技术影响的有效工具

Abstract: Predicting the social and behavioral impact of future technologies, before
they are achieved, would allow us to guide their development and regulation
before these impacts get entrenched. Traditionally, this prediction has relied
on qualitative, narrative methods. Here we describe a method which uses
experimental methods to simulate future technologies, and collect quantitative
measures of the attitudes and behaviors of participants assigned to controlled
variations of the future. We call this method 'science fiction science'. We
suggest that the reason why this method has not been fully embraced yet,
despite its potential benefits, is that experimental scientists may be
reluctant to engage in work facing such serious validity threats as science
fiction science. To address these threats, we consider possible constraints on
the kind of technology that science fiction science may study, as well as the
unconventional, immersive methods that science fiction science may require. We
seek to provide perspective on the reasons why this method has been
marginalized for so long, what benefits it would bring if it could be built on
strong yet unusual methods, and how we can normalize these methods to help the
diverse community of science fiction scientists to engage in a virtuous cycle
of validity improvement.

</details>


### [56] [Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs and Vision Models](https://arxiv.org/abs/2508.03547)
*Ada Yi Zhao,Aditya Gunturu,Ellen Yi-Luen Do,Ryo Suzuki*

Main category: cs.HC

TL;DR: 论文提出了一种名为Guided Reality的自动化AR系统，结合了LLMs和视觉模型，生成动态视觉指导以提升AR指令的上下文嵌入效果。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM生成的AR指令缺乏丰富的视觉增强，限制了用户对物理任务的理解和执行。

Method: 系统整合LLMs和视觉模型，生成多步指令、识别视觉指导类型、提取空间信息，并将视觉指导嵌入物理空间。

Result: 通过用户研究（N=16）验证了系统的有效性，并探讨了其在培训工作流中的应用。

Conclusion: Guided Reality能有效提升AR指令的视觉嵌入效果，具有实际应用的潜力。

Abstract: Large language models (LLMs) have enabled the automatic generation of
step-by-step augmented reality (AR) instructions for a wide range of physical
tasks. However, existing LLM-based AR guidance often lacks rich visual
augmentations to effectively embed instructions into spatial context for a
better user understanding. We present Guided Reality, a fully automated AR
system that generates embedded and dynamic visual guidance based on
step-by-step instructions. Our system integrates LLMs and vision models to: 1)
generate multi-step instructions from user queries, 2) identify appropriate
types of visual guidance, 3) extract spatial information about key interaction
points in the real world, and 4) embed visual guidance in physical space to
support task execution. Drawing from a corpus of user manuals, we define five
categories of visual guidance and propose an identification strategy based on
the current step. We evaluate the system through a user study (N=16),
completing real-world tasks and exploring the system in the wild. Additionally,
four instructors shared insights on how Guided Reality could be integrated into
their training workflows.

</details>


### [57] [SlideAudit: A Dataset and Taxonomy for Automated Evaluation of Presentation Slides](https://arxiv.org/abs/2508.03630)
*Zhuohao Jerry Zhang,Ruiqi Chen,Mingyuan Zhong,Jacob O. Wobbrock*

Main category: cs.HC

TL;DR: 论文介绍了SlideAudit数据集，用于自动化评估演示幻灯片设计问题，并通过AI模型测试了识别设计缺陷的能力。


<details>
  <summary>Details</summary>
Motivation: 自动化评估特定图形设计（如演示幻灯片）是一个尚未解决的问题，论文旨在填补这一空白。

Method: 通过设计专家合作开发了幻灯片设计缺陷的分类法，收集并标注了2400张幻灯片，测试了多种大型语言模型和提示策略。

Result: AI模型在识别设计缺陷上表现不佳，F1分数在0.331到0.655之间，但基于分类法的提示策略效果最佳。87.8%的改进幻灯片得益于分类法。

Conclusion: SlideAudit数据集和分类法为自动化幻灯片设计评估提供了实用工具，尽管AI当前表现有限，但分类法显著提升了改进效果。

Abstract: Automated evaluation of specific graphic designs like presentation slides is
an open problem. We present SlideAudit, a dataset for automated slide
evaluation. We collaborated with design experts to develop a thorough taxonomy
of slide design flaws. Our dataset comprises 2400 slides collected and
synthesized from multiple sources, including a subset intentionally modified
with specific design problems. We then fully annotated them using our taxonomy
through strictly trained crowdsourcing from Prolific. To evaluate whether AI is
capable of identifying design flaws, we compared multiple large language models
under different prompting strategies, and with an existing design critique
pipeline. We show that AI models struggle to accurately identify slide design
flaws, with F1 scores ranging from 0.331 to 0.655. Notably, prompting
techniques leveraging our taxonomy achieved the highest performance. We further
conducted a remediation study to assess AI's potential for improving slides.
Among 82.0% of slides that showed significant improvement, 87.8% of them were
improved more with our taxonomy, further demonstrating its utility.

</details>


### [58] [Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired](https://arxiv.org/abs/2508.03651)
*Ruei-Che Chang,Rosiana Natalie,Wenqian Xu,Jovan Zheng Feng Yap,Anhong Guo*

Main category: cs.HC

TL;DR: 大型多模态模型的进步为视障人士提供了通过实时视频系统与现实世界互动的能力，但其在实际辅助任务中的益处与挑战尚不明确。


<details>
  <summary>Details</summary>
Motivation: 探索实时视频AI在视障人士辅助任务中的实际效果与局限性。

Method: 对八位视障参与者进行探索性研究，使用ChatGPT的Advanced Voice with Video在陌生环境中完成多种任务。

Result: 当前系统能有效处理静态场景，但在动态情境中提供的实时描述不足，且存在信息不准确、幻觉等问题。

Conclusion: 需增强实时感知能力、优化干预时机，并解决生态与安全问题以改进辅助视频AI。

Abstract: Recent advancements in large multimodal models have provided blind or
visually impaired (BVI) individuals with new capabilities to interpret and
engage with the real world through interactive systems that utilize live video
feeds. However, the potential benefits and challenges of such capabilities to
support diverse real-world assistive tasks remain unclear. In this paper, we
present findings from an exploratory study with eight BVI participants.
Participants used ChatGPT's Advanced Voice with Video, a state-of-the-art live
video AI released in late 2024, in various real-world scenarios, from locating
objects to recognizing visual landmarks, across unfamiliar indoor and outdoor
environments. Our findings indicate that current live video AI effectively
provides guidance and answers for static visual scenes but falls short in
delivering essential live descriptions required in dynamic situations. Despite
inaccuracies in spatial and distance information, participants leveraged the
provided visual information to supplement their mobility strategies. Although
the system was perceived as human-like due to high-quality voice interactions,
assumptions about users' visual abilities, hallucinations, generic responses,
and a tendency towards sycophancy led to confusion, distrust, and potential
risks for BVI users. Based on the results, we discuss implications for
assistive video AI agents, including incorporating additional sensing
capabilities for real-world use, determining appropriate intervention timing
beyond turn-taking interactions, and addressing ecological and safety concerns.

</details>


### [59] [Classifying Epistemic Relationships in Human-AI Interaction: An Exploratory Approach](https://arxiv.org/abs/2508.03673)
*Shengnan Yang,Rongqian Ma*

Main category: cs.HC

TL;DR: 研究探讨了人类与AI在知识贡献中的动态认知关系，提出了五种关系类型，并强调需要超越静态AI隐喻，采用更细致的框架来理解人机知识共建。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在知识密集型工作中的普及，研究AI如何重塑用户的角色及其与人类的认知关系变得重要。

Method: 基于对31位跨学科学者的访谈，开发了五部分编码手册，识别出五种认知关系类型。

Result: 发现认知角色是动态且依赖情境的，提出五种关系类型：工具依赖、有条件委托、共同代理协作、权威替代和认知回避。

Conclusion: 呼吁HCI领域超越静态AI隐喻，采用更细致的框架以捕捉人机知识共建的多样性和规范性维度。

Abstract: As AI systems become integral to knowledge-intensive work, questions arise
not only about their functionality but also their epistemic roles in human-AI
interaction. While HCI research has proposed various AI role typologies, it
often overlooks how AI reshapes users' roles as knowledge contributors. This
study examines how users form epistemic relationships with AI-how they assess,
trust, and collaborate with it in research and teaching contexts. Based on 31
interviews with academics across disciplines, we developed a five-part codebook
and identified five relationship types: Instrumental Reliance, Contingent
Delegation, Co-agency Collaboration, Authority Displacement, and Epistemic
Abstention. These reflect variations in trust, assessment modes, tasks, and
human epistemic status. Our findings show that epistemic roles are dynamic and
context-dependent. We argue for shifting beyond static metaphors of AI toward a
more nuanced framework that captures how humans and AI co-construct knowledge,
enriching HCI's understanding of the relational and normative dimensions of AI
use.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [60] [Neighborhood-Preserving Voronoi Treemaps](https://arxiv.org/abs/2508.03445)
*Patrick Paetzold,Rebecca Kehlbeck,Yumeng Xue,Bin Chen,Yunhai Wang,Oliver Deussen*

Main category: cs.GR

TL;DR: 该论文提出了一种改进的Voronoi树图算法，通过考虑数据相似性来生成保留邻域关系的树图。


<details>
  <summary>Details</summary>
Motivation: 现有的Voronoi树图仅能展示节点及其层次结构，但在实际应用中，数据通常还包含其他属性（如地理边界或语义相似性）。这些属性需要被纳入算法中以生成更优的结果。

Method: 论文提出了一种新的方法：1）在数据预处理阶段引入相似性；2）使用Kuhn-Munkres匹配将相似性与CVT单元对齐；3）通过贪心交换优化邻域关系；4）在优化过程中调整单元大小。

Result: 通过实际案例（信息图表和语言学数据）验证了方法的有效性，并定量评估了邻域保留效果。

Conclusion: 该方法成功将数据相似性融入Voronoi树图，生成了保留邻域关系的树图，实用性强。

Abstract: Voronoi treemaps are used to depict nodes and their hierarchical
relationships simultaneously. However, in addition to the hierarchical
structure, data attributes, such as co-occurring features or similarities,
frequently exist. Examples include geographical attributes like shared borders
between countries or contextualized semantic information such as embedding
vectors derived from large language models. In this work, we introduce a
Voronoi treemap algorithm that leverages data similarity to generate
neighborhood-preserving treemaps. First, we extend the treemap layout pipeline
to consider similarity during data preprocessing. We then use a Kuhn-Munkres
matching of similarities to centroidal Voronoi tessellation (CVT) cells to
create initial Voronoi diagrams with equal cell sizes for each level. Greedy
swapping is used to improve the neighborhoods of cells to match the data's
similarity further. During optimization, cell areas are iteratively adjusted to
their respective sizes while preserving the existing neighborhoods. We
demonstrate the practicality of our approach through multiple real-world
examples drawn from infographics and linguistics. To quantitatively assess the
resulting treemaps, we employ treemap metrics and measure neighborhood
preservation.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [61] [Understanding Demand for Shared Autonomous Micro-Mobility](https://arxiv.org/abs/2508.03521)
*Naroa Coretti Sanchez,Kent Larson*

Main category: cs.ET

TL;DR: 研究了共享自动驾驶微出行系统的行为和环境影响，重点关注自动驾驶自行车与美国公交的整合。填补了关于服务替代模式、用户接受度及服务属性影响的研究空白，通过问卷调查和离散选择模型分析发现服务设计对环境和用户行为有显著影响。


<details>
  <summary>Details</summary>
Motivation: 填补对共享自动驾驶微出行系统替代模式、用户接受度及服务属性影响的研究空白。

Method: 设计基于真实出行的情境感知偏好调查，并估计离散选择模型，包括一个融入潜在态度的混合模型。

Result: 服务设计显著影响采用率、模式转换和环境影响；低等待时间和成本增加采用率但提高排放；中等等待时间更可能减少影响；采用率因人口特征和城市类型而异。

Conclusion: 研究结果为开发更可持续和公平的出行系统提供了见解。

Abstract: This study examines the behavioral and environmental implications of shared
autonomous micro-mobility systems, focusing on autonomous bicycles and their
integration with transit in the U.S. While prior research has addressed
operational and lifecycle aspects, a critical gap remains in understanding
which modes these services are likely to substitute, who is most inclined to
adopt them, and how service attributes influence user decisions. We design a
context-aware stated preference survey grounded in real-world trips and
estimate discrete choice models, including a hybrid model incorporating latent
attitudes. Findings indicate that adoption, mode shift, and environmental
impacts are highly sensitive to service design. Scenarios with minimal wait and
cost yield high adoption but increase emissions, while moderate waits are more
likely to reduce impacts. Adoption likelihood varies with demographic
characteristics, and outcomes depend on city type, context, and infrastructure
assumptions. These insights can inform the development of more sustainable and
equitable mobility systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [62] [Low-Communication Resilient Distributed Estimation Algorithm Based on Memory Mechanism](https://arxiv.org/abs/2508.02705)
*Wei Li,Limei Hu,Feng Chen,Ye Yao*

Main category: cs.DC

TL;DR: 提出了一种低通信弹性分布式估计算法，通过节点选择策略和W-SVDD模型提升抗攻击能力，并引入事件触发机制减少无效更新。


<details>
  <summary>Details</summary>
Motivation: 多任务对抗网络中，受攻击的节点或链路阻碍了未知参数的准确估计，需要一种低通信且抗攻击的解决方案。

Method: 基于声誉的节点选择策略和W-SVDD模型训练数据，加入事件触发机制优化更新。

Result: 仿真表明算法在低通信成本下性能优于其他算法。

Conclusion: 算法有效提升了分布式估计的抗攻击能力和通信效率。

Abstract: In multi-task adversarial networks, the accurate estimation of unknown
parameters in a distributed algorithm is hindered by attacked nodes or links.
To tackle this challenge, this brief proposes a low-communication resilient
distributed estimation algorithm. First, a node selection strategy based on
reputation is introduced that allows nodes to communicate with more reliable
subset of neighbors. Subsequently, to discern trustworthy intermediate
estimates, the Weighted Support Vector Data Description (W-SVDD) model is
employed to train the memory data. This trained model contributes to reinforce
the resilience of the distributed estimation process against the impact of
attacked nodes or links. Additionally, an event-triggered mechanism is
introduced to minimize ineffective updates to the W-SVDD model, and a suitable
threshold is derived based on assumptions. The convergence of the algorithm is
analyzed. Finally, simulation results demonstrate that the proposed algorithm
achieves superior performance with less communication cost compared to other
algorithms.

</details>


### [63] [A DataOps Toolbox Enabling Continuous Semantic Integration of Devices for Edge-Cloud AI Applications](https://arxiv.org/abs/2508.02708)
*Mario Scrocca,Marco Grassi,Alessio Carenini,Jean-Paul Calbimonte,Darko Anicic,Irene Celino*

Main category: cs.DC

TL;DR: 该论文介绍了一个基于语义网络技术和低代码机制的DataOps工具箱，旨在解决AI应用中多设备协作的数据互操作性挑战。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中实施AI应用需要多设备协作，但不同设备、数据格式和通信接口的异构性带来了巨大挑战。

Method: 设计并实现了一个支持持续语义集成的DataOps工具箱，结合语义网络技术和低代码机制，解决数据互操作性问题。

Result: 工具箱在三个不同领域的用例中得到应用，成功实现了静态节点信息和运行时数据交换的互操作性。

Conclusion: 通过试点活动验证了工具箱的有效性，并总结了经验教训，为未来类似应用提供了参考。

Abstract: The implementation of AI-based applications in complex environments often
requires the collaboration of several devices spanning from edge to cloud.
Identifying the required devices and configuring them to collaborate is a
challenge relevant to different scenarios, like industrial shopfloors, road
infrastructures, and healthcare therapies. We discuss the design and
implementation of a DataOps toolbox leveraging Semantic Web technologies and a
low-code mechanism to address heterogeneous data interoperability requirements
in the development of such applications. The toolbox supports a continuous
semantic integration approach to tackle various types of devices, data formats,
and semantics, as well as different communication interfaces. The paper
presents the application of the toolbox to three use cases from different
domains, the DataOps pipelines implemented, and how they guarantee
interoperability of static nodes' information and runtime data exchanges.
Finally, we discuss the results from the piloting activities in the use cases
and the lessons learned.

</details>


### [64] [PROV-AGENT: Unified Provenance for Tracking AI Agent Interactions in Agentic Workflows](https://arxiv.org/abs/2508.02866)
*Renan Souza,Amal Gueroudji,Stephen DeWitt,Daniel Rosendo,Tirthankar Ghosal,Robert Ross,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 本文提出PROV-AGENT模型，扩展W3C PROV并利用MCP，以捕获代理在工作流中的交互，提升透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 由于AI代理在复杂工作流中可能产生错误且错误传播难以追踪，需要细粒度溯源来链接代理决策及其影响。

Method: 提出PROV-AGENT模型，扩展W3C PROV标准并结合MCP，整合代理交互到端到端工作流溯源中。

Result: 实现了实时开源系统，支持跨环境的溯源查询和代理可靠性分析。

Conclusion: PROV-AGENT有效提升了代理工作流的透明度和可靠性，支持关键溯源分析。

Abstract: Foundation models, such as Large Language Models (LLMs), are increasingly
used as core components of AI agents in complex, large-scale workflows across
federated and heterogeneous environments. In agentic workflows, autonomous
agents plan tasks, interact with humans and peers, and shape scientific
outcomes. This makes transparency, traceability, reproducibility, and
reliability essential. However, AI-based agents can hallucinate or reason
incorrectly, and their decisions may propagate errors through the workflow,
especially when one agent's output feeds into another's input. Therefore,
fine-grained provenance is essential to link agent decisions, their end-to-end
context, and downstream impacts. While provenance techniques have long
supported reproducibility and workflow data understanding, they fail to capture
and relate agent-centric metadata (prompts, responses, and decisions) with the
rest of the workflow. In this paper, we introduce PROV-AGENT, a provenance
model that extends W3C PROV and leverages the Model Context Protocol (MCP) to
integrate agent interactions into end-to-end workflow provenance. Our
contributions include: (1) a provenance model tailored for agentic workflows,
(2) a near real-time, open-source system for capturing agentic provenance, and
(3) a cross-facility evaluation spanning edge, cloud, and HPC environments,
demonstrating support for critical provenance queries and agent reliability
analysis.

</details>


### [65] [Optimal Simultaneous Byzantine Agreement, Common Knowledge and Limited Information Exchange](https://arxiv.org/abs/2508.03418)
*Ron van der Meyden*

Main category: cs.DC

TL;DR: 本文重新审视了基于知识分析的同步拜占庭协议问题，提出了更高效的信息交换协议，并澄清了相关概念上的混淆。


<details>
  <summary>Details</summary>
Motivation: 为了解决分布式算法中全信息协议的低效问题，本文探索了更实用的信息交换方式，以优化协议性能和实用性。

Method: 通过知识基础程序，结合特定的故障模型和信息交换协议，提出了一种优化的解决方案。

Result: 研究表明，在特定条件下，该方案优于传统全信息协议，但也存在不适用的情况。

Conclusion: 本文为同步拜占庭协议提供了一种更高效的解决方案，并明确了其适用范围。

Abstract: In order to develop solutions that perform actions as early as possible,
analysis of distributed algorithms using epistemic logic has generally
concentrated on ``full information protocols'', which may be inefficient with
respect to space and computation time. The paper reconsiders the epistemic
analysis of the problem of Simultaneous Byzantine Agreement with respect to
weaker, but more practical, exchanges of information. The paper first clarifies
some issues concerning both the specification of this problem and the knowledge
based program characterizing its solution, concerning the distinction between
the notions of ``nonfaulty'' and ``not yet failed'', on which there are
variances in the literature. It is then shown that, when implemented relative
to a given failure model and an information exchange protocol satisfying
certain conditions, this knowledge based program yields a protocol that is
optimal relative to solutions using the same information exchange. Conditions
are also identified under which this implementation is also an optimum, but an
example is provided that shows this does not hold in general.

</details>


### [66] [Understanding the Landscape of Ampere GPU Memory Errors](https://arxiv.org/abs/2508.03513)
*Zhu Zhu,Yu Sun,Dhatri Parakal,Bo Fang,Steven Farrell,Gregory H. Bauer,Brett Bode,Ian T. Foster,Michael E. Papka,William Gropp,Zhao Zhang,Lishan Yang*

Main category: cs.DC

TL;DR: 本文通过大规模跨超级计算机研究，分析了NVIDIA A100 GPU的内存可靠性，提供了对高性能计算系统设计的宝贵见解。


<details>
  <summary>Details</summary>
Motivation: 理解GPU内存错误行为是实现高效可靠高性能计算系统的关键。

Method: 研究覆盖三个超级计算机（Delta、Polaris、Perlmutter），分析67.77百万GPU设备小时的错误日志。

Result: 比较了错误率和平均错误间隔时间（MTBE），揭示了三个系统的共性与差异。

Conclusion: 研究结果对超级计算机的可靠性运行、检查点间隔选择及与前代GPU的对比具有重要意义。

Abstract: Graphics Processing Units (GPUs) have become a de facto solution for
accelerating high-performance computing (HPC) applications. Understanding their
memory error behavior is an essential step toward achieving efficient and
reliable HPC systems. In this work, we present a large-scale
cross-supercomputer study to characterize GPU memory reliability, covering
three supercomputers - Delta, Polaris, and Perlmutter - all equipped with
NVIDIA A100 GPUs. We examine error logs spanning 67.77 million GPU device-hours
across 10,693 GPUs. We compare error rates and mean-time-between-errors (MTBE)
and highlight both shared and distinct error characteristics among these three
systems. Based on these observations and analyses, we discuss the implications
and lessons learned, focusing on the reliable operation of supercomputers, the
choice of checkpointing interval, and the comparison of reliability
characteristics with those of previous-generation GPUs. Our characterization
study provides valuable insights into fault-tolerant HPC system design and
operation, enabling more efficient execution of HPC applications.

</details>


### [67] [In-Memory Non-Binary LDPC Decoding](https://arxiv.org/abs/2508.03567)
*Oscar Ferraz,Vitor Silva,Gabriel Falcao*

Main category: cs.DC

TL;DR: 本文提出了首个基于近存储计算（PiM）的非二进制LDPC解码器，在UPMEM系统中实现了76 Mbit/s的解码吞吐量，性能优于低功耗GPU方案。


<details>
  <summary>Details</summary>
Motivation: 内存技术发展滞后导致数据移动瓶颈，影响并行处理系统性能，需通过近存储计算（PiM）缓解。

Method: 设计非二进制LDPC解码器，采用PiM范式，将计算单元置于数据存储位置附近。

Result: PiM方案实现76 Mbit/s解码吞吐量，性能优于优化的低功耗GPU方案。

Conclusion: PiM范式为非二进制LDPC解码提供高效解决方案，性能竞争力强。

Abstract: Low-density parity-check (LDPC) codes are an important feature of several
communication and storage applications, offering a flexible and effective
method for error correction. These codes are computationally complex and
require the exploitation of parallel processing to meet real-time constraints.
As advancements in arithmetic and logic unit technology allowed for higher
performance of computing systems, memory technology has not kept the same pace
of development, creating a data movement bottleneck and affecting parallel
processing systems more dramatically. To alleviate the severity of this
bottleneck, several solutions have been proposed, namely the processing
in-memory (PiM) paradigm that involves the design of compute units to where (or
near) the data is stored, utilizing thousands of low-complexity processing
units to perform out bit-wise and simple arithmetic operations. This paper
presents a novel efficient solution for near-memory non-binary LDPC decoders in
the UPMEM system, for the best of our knowledge the first real hardware
PiM-based non-binary LDPC decoder that is benchmarked against low-power GPU
parallel solutions highly optimized for throughput performance. PiM-based
non-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, which is
even competitive when compared against implementations running in edge GPUs.

</details>


### [68] [Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling](https://arxiv.org/abs/2508.03611)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Block是一个分布式调度框架，通过利用请求中的上下文信息优化大语言模型服务中的负载均衡和自动资源配置，显著提升服务容量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有模型服务系统通常采用单体的启发式任务调度器，难以满足大规模服务的低开销、可靠性和可扩展性需求。Block旨在解决这些问题。

Method: Block采用完全分布式、无状态和预测性调度，利用LLM推理的确定性特性（如主机配置、响应长度和硬件性能）进行决策。

Result: 在12个GPU集群上的测试表明，Block比启发式调度器性能提升显著，服务容量提升16.7%，P99尾部延迟降低49.5%。

Conclusion: Block是一种高效的分布式调度框架，适用于不同模型和工作负载，且代码和数据已开源。

Abstract: This paper presents Block, a distributed scheduling framework designed to
optimize load balancing and auto-provisioning across instances in large
language model serving frameworks by leveraging contextual information from
incoming requests. Unlike popular model serving systems that rely on monolithic
and heuristic task schedulers, Block operates as a fully distributed,
stateless, and predictive scheduling system to achieve low overhead,
reliability, and scalability. It leverages the deterministic and predictable
characteristics of LLM inferences, such as host configurations, response
lengths, and hardware performance, to make scheduling decisions based on
accurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block
significantly outperforms heuristic schedulers, boosting serving capacity by up
to 16.7\% and reducing P99 tail latency by up to 49.5\%. These performance
gains remain consistent across diverse models, workloads and configurations.
Code and data are open-sourced.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [69] [Learned Adaptive Indexing](https://arxiv.org/abs/2508.03471)
*Suvam Kumar Das,Suprio Ray*

Main category: cs.DB

TL;DR: 论文提出了一种新的自适应索引方法，利用机器学习模型动态构建索引，优于现有的自适应索引方法。


<details>
  <summary>Details</summary>
Motivation: 传统索引在频繁变化的查询负载或数据更新场景下效率低下，而现有的学习索引需要预先训练，无法适应动态变化。为了解决这些问题，作者提出了基于学习模型的自适应索引方法。

Method: 通过查询处理动态构建索引，并结合查询负载预测技术，基于历史数据预测未来负载，优化索引性能。

Result: 实验表明，该方法在大多数情况下比其他自适应索引方法表现更好，查询性能提升1.2至5.6倍。

Conclusion: 学习自适应索引方法在动态环境下具有显著优势，为数据库索引技术提供了新的方向。

Abstract: Indexes can significantly improve search performance in relational databases.
However, if the query workload changes frequently or new data updates occur
continuously, it may not be worthwhile to build a conventional index upfront
for query processing. Adaptive indexing is a technique in which an index gets
built on the fly as a byproduct of query processing. In recent years, research
in database indexing has taken a new direction where machine learning models
are employed for the purpose of indexing. These indexes, known as learned
indexes, can be more efficient compared to traditional indexes such as B+-tree
in terms of memory footprints and query performance. However, a learned index
has to be constructed upfront and requires training the model in advance, which
becomes a challenge in dynamic situations when workload changes frequently. To
the best of our knowledge, no learned indexes exist yet for adaptive indexing.
We propose a novel learned approach for adaptive indexing. It is built on the
fly as queries are submitted and utilizes learned models for indexing data. To
enhance query performance, we employ a query workload prediction technique that
makes future workload projection based on past workload data. We have evaluated
our learned adaptive indexing approach against existing adaptive indexes for
various query workloads. Our results show that our approach performs better
than others in most cases, offering 1.2x - 5.6x improvement in query
performance.

</details>


### [70] [[Technical Report] ArceKV: Towards Workload-driven LSM-compactions for Key-Value Store Under Dynamic Workloads](https://arxiv.org/abs/2508.03565)
*Junfeng Liu,Haoxuan Xie,Siqiang Luo*

Main category: cs.DB

TL;DR: ElasticLSM和ArceKV通过优化LSM树的动态工作负载性能，显著提升了键值存储的效率。


<details>
  <summary>Details</summary>
Motivation: 现实工作负载具有高度动态性，而现有的工作负载感知方法难以维持最优性能或转换成本高昂。

Method: 提出ElasticLSM放宽LSM树的结构约束，并通过Arce轻量级决策引擎指导最优操作。

Result: ArceKV在动态场景中性能提升约3倍。

Conclusion: ElasticLSM和ArceKV为动态工作负载下的键值存储提供了高效解决方案。

Abstract: Key-value stores underpin a wide range of applications due to their
simplicity and efficiency. Log-Structured Merge Trees (LSM-trees) dominate as
their underlying structure, excelling at handling rapidly growing data. Recent
research has focused on optimizing LSM-tree performance under static workloads
with fixed read-write ratios. However, real-world workloads are highly dynamic,
and existing workload-aware approaches often struggle to sustain optimal
performance or incur substantial transition overhead when workload patterns
shift. To address this, we propose ElasticLSM, which removes traditional
LSM-tree structural constraints to allow more flexible management actions
(i.e., compactions and write stalls) creating greater opportunities for
continuous performance optimization. We further design Arce, a lightweight
compaction decision engine that guides ElasticLSM in selecting the optimal
action from its expanded action space. Building on these components, we
implement ArceKV, a full-fledged key-value store atop RocksDB. Extensive
evaluations demonstrate that ArceKV outperforms state-of-the-art compaction
strategies across diverse workloads, delivering around 3x faster performance in
dynamic scenarios.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [71] [Mamba-X: An End-to-End Vision Mamba Accelerator for Edge Computing Devices](https://arxiv.org/abs/2508.02977)
*Dongho Yoon,Gungyu Lee,Jaewon Chang,Yunjae Lee,Dongjae Lee,Minsoo Rhu*

Main category: cs.AR

TL;DR: Vision Mamba 是一种用于计算机视觉任务的状态空间模型，虽然降低了延迟和内存消耗，但其顺序扫描操作导致 GPU 效率低下。Mamba-X 是一种加速器，通过并行扫描和量化技术提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer 模型在语言建模中表现优异，但其计算和内存需求随序列长度增加而显著上升。状态空间模型（SSMs）提供了一种更高效的替代方案，但现有的 Vision Mamba 在边缘设备上部署时仍存在 GPU 效率问题。

Method: 提出 Mamba-X 加速器，采用 systolic scan 阵列实现并行扫描以减少内存流量，并使用硬件友好的混合量化技术降低内存占用。

Result: Mamba-X 能够在不损失精度的情况下提升 Vision Mamba 的硬件效率和部署能力。

Conclusion: Mamba-X 通过并行扫描和量化优化，解决了 Vision Mamba 在边缘设备上的效率问题，为计算机视觉任务提供了一种高效的解决方案。

Abstract: Transformers have proven effective in language modeling but are limited by
high computational and memory demands that grow quadratically with input
sequence length. State space models (SSMs) offer a promising alternative by
reducing attention complexity from $O(L^2)$ to $O(L)$ while also lowering
overall memory consumption. Vision Mamba adapts the SSM approach for computer
vision tasks, achieving lower latency and memory consumption than traditional
transformer models. However, deploying Vision Mamba on edge devices is
challenging due to its sequential scan operations, which hinder GPU efficiency.
We propose Mamba-X, an end-to-end Vision Mamba accelerator that includes a
systolic scan array to maximize parallelism and minimize memory traffic, along
with a hybrid, hardware-friendly quantization technique to reduce memory usage
and improve hardware efficiency without sacrificing accuracy.

</details>


### [72] [Towards Memory Specialization: A Case for Long-Term and Short-Term RAM](https://arxiv.org/abs/2508.02992)
*Peijing Li,Muhammad Shahir Abdurraman,Rachel Cleaveland,Sergey Legtchenko,Philip Levis,Ioan Stefanovici,Thierry Tambe,David Tennenhouse,Caroline Trippel*

Main category: cs.AR

TL;DR: SRAM和DRAM的扩展已停止，成本无法降低，内存成为系统主要成本。本文主张从简单的内存层次转向利用应用特定访问模式的专用内存架构。


<details>
  <summary>Details</summary>
Motivation: 传统SRAM和DRAM的扩展受阻，内存成本成为瓶颈，需探索更高效的内存架构以应对未来需求。

Method: 提出两种新的内存类别：LtRAM（针对长期存在的读密集型数据）和StRAM（针对短期频繁访问的临时数据），并探讨其技术实现和系统集成。

Result: 通过非分层优化，这些专用内存类别可以提升系统效率和可扩展性。

Conclusion: 推动这种内存架构的演变是实现未来高效和可扩展计算系统的关键。

Abstract: Both SRAM and DRAM have stopped scaling: there is no technical roadmap to
reduce their cost (per byte/GB). As a result, memory now dominates system cost.
This paper argues for a paradigm shift from today's simple memory hierarchy
toward specialized memory architectures that exploit application-specific
access patterns. Rather than relying solely on traditional off-chip DRAM and
on-chip SRAM, we envisage memory systems equipped with additional types of
memory whose performance trade-offs benefit workloads through non-hierarchical
optimization. We propose two new memory classes deserving explicit OS support:
long-term RAM (LtRAM) optimized for read-intensive data with long lifetimes,
and short-term RAM (StRAM) designed for transient, frequently-accessed data
with short lifetimes. We explore underlying device technologies that could
implement these classes, including their evolution and their potential
integration into current system designs given emerging workload requirements.
We identify critical research challenges to realize what we believe is a
necessary evolution toward more efficient and scalable computing systems
capable of meeting future demands.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [73] [Difference-restriction algebras with operators](https://arxiv.org/abs/2508.03432)
*Célia Borlido,Ganna Kudryavtseva,Brett McLean*

Main category: math.LO

TL;DR: 论文展示了部分函数的抽象代数与Hausdorff étale空间之间的伴随关系，并推广了广义布尔代数与Hausdorff空间的伴随关系。


<details>
  <summary>Details</summary>
Motivation: 研究部分函数的代数结构与拓扑空间之间的联系，推广现有理论。

Method: 定义差异限制代数及其有限兼容完备化，通过伴随关系构造完备化。

Result: 伴随关系扩展到局部紧零维Hausdorff étale空间与有限兼容完备差异限制代数之间的对偶性。

Conclusion: 通过伴随关系和对偶性，推广了广义布尔代数与空间的经典结果，并扩展到包含附加算子的代数。

Abstract: We exhibit an adjunction between a category of abstract algebras of partial
functions that we call difference-restriction algebras and a category of
Hausdorff \'etale spaces. Difference-restriction algebras are those algebras
isomorphic to a collection of partial functions closed under relative
complement and domain restriction. Our adjunction generalises the adjunction
between the category of generalised Boolean algebras and the category of
Hausdorff spaces. We define the finitary compatible completion of a
difference-restriction algebra and show that the monad induced by our
adjunction yields the finitary compatible completion of any
difference-restriction algebra. As a corollary, the adjunction restricts to a
duality between the finitarily compatibly complete difference-restriction
algebras and the locally compact zero-dimensional Hausdorff \'etale spaces,
generalising the duality between generalised Boolean algebras and locally
compact zero-dimensional Hausdorff spaces. We then extend these adjunction,
duality, and completion results to difference-restriction algebras equipped
with arbitrary additional compatibility preserving operators.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [74] [Can Large Language Models Identify Materials from Radar Signals?](https://arxiv.org/abs/2508.03120)
*Jiangyou Zhu,Hongyu Deng,He Chen*

Main category: eess.SP

TL;DR: LLMaterial利用预训练的LLM直接从雷达信号中推断材料组成，解决了现有雷达技术局限于封闭集对象和任务特定数据收集的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的雷达技术通常局限于封闭集对象类别，并且需要任务特定的数据来训练深度学习模型，限制了实际应用。因此，研究是否能利用预训练的LLM直接从雷达信号中推断材料组成。

Method: 1. 引入物理信号处理流程，将高冗余雷达原始数据压缩为封装材料内在特征的中间参数；2. 采用检索增强生成策略，为LLM提供领域特定知识，使其能够解释和推理中间参数。

Result: 初步结果显示，LLMaterial能有效区分多种常见材料，展示了其在现实材料识别应用中的强大潜力。

Conclusion: LLMaterial证明了利用预训练的LLM进行开放集材料识别的可行性，为实际应用提供了新方法。

Abstract: Accurately identifying the material composition of objects is a critical
capability for AI robots powered by large language models (LLMs) to perform
context-aware manipulation. Radar technologies offer a promising sensing
modality for material recognition task. When combined with deep learning, radar
technologies have demonstrated strong potential in identifying the material of
various objects. However, existing radar-based solutions are often constrained
to closed-set object categories and typically require task-specific data
collection to train deep learning models, largely limiting their practical
applicability. This raises an important question: Can we leverage the powerful
reasoning capabilities of pre-trained LLMs to directly infer material
composition from raw radar signals? Answering this question is non-trivial due
to the inherent redundancy of radar signals and the fact that pre-trained LLMs
have no prior exposure to raw radar data during training. To address this, we
introduce LLMaterial, the first study to investigate the feasibility of using
LLM to identify materials directly from radar signals. First, we introduce a
physics-informed signal processing pipeline that distills high-redundancy radar
raw data into a set of compact intermediate parameters that encapsulate the
material's intrinsic characteristics. Second, we adopt a retrieval-augmented
generation (RAG) strategy to provide the LLM with domain-specific knowledge,
enabling it to interpret and reason over the extracted intermediate parameters.
Leveraging this integration, the LLM is empowered to perform step-by-step
reasoning on the condensed radar features, achieving open-set material
recognition directly from raw radar signals. Preliminary results show that
LLMaterial can effectively distinguish among a variety of common materials,
highlighting its strong potential for real-world material identification
applications.

</details>


### [75] [Investigating the Cognitive Response of Brake Lights in Initiating Braking Action Using EEG](https://arxiv.org/abs/2508.03274)
*Ramaswamy Palaniappan,Surej Mouli,Howard Bowman,Ian McLoughlin*

Main category: eess.SP

TL;DR: 论文研究了不同刹车灯设计对驾驶员反应时间的影响，通过EEG分析发现LED刹车灯的认知反应时间显著快于传统灯泡刹车灯。


<details>
  <summary>Details</summary>
Motivation: 半数交通事故源于驾驶员注意力不集中或车距不足，尤其是追尾事故在英国最常见。研究旨在通过测量驾驶员对刹车灯设计的反应，提升行车安全。

Method: 在模拟驾驶环境中测试22名受试者对10种刹车灯（8种LED、2种传统灯泡）的反应，使用EEG记录P3成分分析认知反应时间。

Result: EEG数据显示，传统灯泡刹车灯的认知反应时间显著慢于所有LED刹车灯；LED设计间差异不明显，可能与EEG信号中的运动伪影有关。

Conclusion: LED刹车灯在引发驾驶员快速认知反应方面优于传统灯泡设计，有助于减少追尾事故。

Abstract: Half of all road accidents result from either lack of driver attention or
from maintaining insufficient separation between vehicles. Collision from the
rear, in particular, has been identified as the most common class of accident
in the UK, and its influencing factors have been widely studied for many years.
Rear-mounted stop lamps, illuminated when braking, are the primary mechanism to
alert following drivers to the need to reduce speed or brake. This paper
develops a novel brain response approach to measuring subject reaction to
different brake light designs. A variety of off-the-shelf brake light
assemblies are tested in a physical simulated driving environment to assess the
cognitive reaction times of 22 subjects. Eight pairs of LED-based and two pairs
of incandescent bulb-based brake light assemblies are used and
electroencephalogram (EEG) data recorded. Channel Pz is utilised to extract the
P3 component evoked during the decision making process that occurs in the brain
when a participant decides to lift their foot from the accelerator and depress
the brake. EEG analysis shows that both incandescent bulb-based lights are
statistically slower to evoke cognitive responses than all tested LED-based
lights. Between the LED designs, differences are evident, but not statistically
significant, attributed to the significant amount of movement artifact in the
EEG signal.

</details>


### [76] [Decoding and Engineering the Phytobiome Communication for Smart Agriculture](https://arxiv.org/abs/2508.03584)
*Fatih Gulec,Hamdan Awan,Nigel Wallbridge,Andrew W. Eckford*

Main category: eess.SP

TL;DR: 论文探讨了通过通信工程视角理解植物-环境-生物群落（phytobiome）的通信，并将其与智能农业结合，提出了一种多尺度框架模型及其潜在应用。


<details>
  <summary>Details</summary>
Motivation: 现代农业面临食物需求增长、环境污染和水资源短缺的挑战，智能农业技术（如物联网和ML/AI）为解决这些问题提供了可能。论文旨在利用通信理论推动农业科学和实践。

Method: 首先概述了phytobiome通过分子和电生理信号进行的通信，并提出了将其建模为通信网络的多尺度框架。通过植物实验验证了电生理信号的建模方法。

Result: 提出了智能农业应用（如智能灌溉和精准农化输送），并将ML/AI与分子通信（MC）技术相结合，为高效、可持续和环保的农业生产铺平道路。

Conclusion: 论文总结了phytobiome通信模型在智能农业中的潜力，并讨论了实施挑战、开放研究问题和工业前景。

Abstract: Smart agriculture applications, integrating technologies like the Internet of
Things and machine learning/artificial intelligence (ML/AI) into agriculture,
hold promise to address modern challenges of rising food demand, environmental
pollution, and water scarcity. Alongside the concept of the phytobiome, which
defines the area including the plant, its environment, and associated
organisms, and the recent emergence of molecular communication (MC), there
exists an important opportunity to advance agricultural science and practice
using communication theory. In this article, we motivate to use the
communication engineering perspective for developing a holistic understanding
of the phytobiome communication and bridge the gap between the phytobiome
communication and smart agriculture. Firstly, an overview of phytobiome
communication via molecular and electrophysiological signals is presented and a
multi-scale framework modeling the phytobiome as a communication network is
conceptualized. Then, how this framework is used to model electrophysiological
signals is demonstrated with plant experiments. Furthermore, possible smart
agriculture applications, such as smart irrigation and targeted delivery of
agrochemicals, through engineering the phytobiome communication are proposed.
These applications merge ML/AI methods with the Internet of Bio-Nano-Things
enabled by MC and pave the way towards more efficient, sustainable, and
eco-friendly agricultural production. Finally, the implementation challenges,
open research issues, and industrial outlook for these applications are
discussed.

</details>


### [77] [Secure mmWave Beamforming with Proactive-ISAC Defense Against Beam-Stealing Attacks](https://arxiv.org/abs/2508.02856)
*Seyed Bagher Hashemi Natanzi,Hossein Mohammadi,Bo Tang,Vuk Marojevic*

Main category: eess.SP

TL;DR: 论文提出了一种基于深度强化学习（DRL）的主动防御框架，用于应对毫米波通信系统中的波束窃取攻击，通过集成感知与通信（ISAC）能力实现智能威胁评估。


<details>
  <summary>Details</summary>
Motivation: 毫米波通信系统面临高级波束窃取攻击的威胁，亟需一种主动、自适应的防御方案以确保物理层安全。

Method: 采用基于近端策略优化（PPO）的DRL代理，结合ISAC能力动态探测可疑活动，并通过课程学习策略确保训练成功。

Result: 该框架实现了92.8%的平均攻击检测率和13 dB以上的用户信干噪比（SINR）。

Conclusion: 该DRL框架能有效平衡安全性和通信性能，为毫米波通信系统提供了稳健的防御方案。

Abstract: Millimeter-wave (mmWave) communication systems face increasing susceptibility
to advanced beam-stealing attacks, posing a significant physical layer security
threat. This paper introduces a novel framework employing an advanced Deep
Reinforcement Learning (DRL) agent for proactive and adaptive defense against
these sophisticated attacks. A key innovation is leveraging Integrated Sensing
and Communications (ISAC) capabilities for active, intelligent threat
assessment. The DRL agent, built on a Proximal Policy Optimization (PPO)
algorithm, dynamically controls ISAC probing actions to investigate suspicious
activities. We introduce an intensive curriculum learning strategy that
guarantees the agent experiences successful detection during training to
overcome the complex exploration challenges inherent to such a
security-critical task. Consequently, the agent learns a robust and adaptive
policy that intelligently balances security and communication performance.
Numerical results demonstrate that our framework achieves a mean attacker
detection rate of 92.8% while maintaining an average user SINR of over 13 dB.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [78] [Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025](https://arxiv.org/abs/2508.01263)
*Long S. T. Nguyen,Khang H. N. Vo,Thu H. A. Nguyen,Tuan C. Bui,Duc Q. Nguyen,Thanh-Tung Tran,Anh D. Nguyen,Minh L. Nguyen,Fabien Baldacci,Thang H. Bui,Emanuel Di Nardo,Angelo Ciaramella,Son H. Le,Ihsan Ullah,Lorenzo Di Rocco,Tho T. Quan*

Main category: cs.CL

TL;DR: 该论文分析了2025年XAI挑战赛，探讨了在教育中结合轻量级LLM与符号系统以实现透明且可解释的AI问答系统的努力。


<details>
  <summary>Details</summary>
Motivation: 解决教育领域AI透明度与可解释性的需求，通过竞赛促进LLM与符号推理的结合。

Method: 基于逻辑模板构造数据集，采用轻量级LLM或混合系统开发问答系统，生成自然语言解释。

Result: 提供了高质量数据集和评估协议，展示了LLM与符号推理在教育中的应用潜力。

Conclusion: 竞赛为未来XAI教育系统和研究提供了实践指导，推动了AI透明性与可解释性发展。

Abstract: The growing integration of Artificial Intelligence (AI) into education has
intensified the need for transparency and interpretability. While hackathons
have long served as agile environments for rapid AI prototyping, few have
directly addressed eXplainable AI (XAI) in real-world educational contexts.
This paper presents a comprehensive analysis of the XAI Challenge 2025, a
hackathon-style competition jointly organized by Ho Chi Minh City University of
Technology (HCMUT) and the International Workshop on Trustworthiness and
Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International
Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked
participants with building Question-Answering (QA) systems capable of answering
student queries about university policies while generating clear, logic-based
natural language explanations. To promote transparency and trustworthiness,
solutions were required to use lightweight Large Language Models (LLMs) or
hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed
via logic-based templates with Z3 validation and refined through expert student
review to ensure alignment with real-world academic scenarios. We describe the
challenge's motivation, structure, dataset construction, and evaluation
protocol. Situating the competition within the broader evolution of AI
hackathons, we argue that it represents a novel effort to bridge LLMs and
symbolic reasoning in service of explainability. Our findings offer actionable
insights for future XAI-centered educational systems and competitive research
initiatives.

</details>


### [79] [More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation](https://arxiv.org/abs/2508.03678)
*Yangtian Zi,Harshitha Menon,Arjun Guha*

Main category: cs.CL

TL;DR: 论文研究了LLMs在通用与专用代码生成基准上的性能差异，提出PartialOrderEval方法分析提示细节对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在专用基准上表现不佳的原因是缺乏领域知识还是提示细节不足。

Method: 引入PartialOrderEval工具，在HumanEval和ParEval基准上测试不同详细程度的提示对模型性能的影响。

Result: 实验显示不同任务对提示细节的敏感性不同，关键改进因素包括显式I/O规范、边界案例处理和分步分解。

Conclusion: 提示细节的详细程度对LLMs在专用任务上的性能有显著影响，需针对性优化。

Abstract: State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general
benchmarks like HumanEval but underperform on specialized suites such as
ParEval. Is this due to LLMs missing domain knowledge or insufficient prompt
detail is given? To answer this, we introduce PartialOrderEval, which augments
any code generation benchmark with a partial order of prompts from minimal to
maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets
of ParEval, we measure how pass@1 scales with prompt specificity. Our
experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of
prompt sensitivity across different tasks, and a qualitative analysis
highlights explicit I/O specifications, edge-case handling, and stepwise
breakdowns as the key drivers of prompt detail improvement.

</details>


### [80] [When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025](https://arxiv.org/abs/2508.03037)
*Ariya Mukherjee-Gandhi,Oliver Muellerklein*

Main category: cs.CL

TL;DR: 分析了2013年至2025年AI生成艺术的英文讨论，揭示艺术家观点与主流叙事的偏离，技术术语可能阻碍艺术家参与。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何影响艺术创作及艺术家生计，强调艺术家声音在讨论中常被忽视。

Method: 采用BERTopic方法分析439篇500字文本，识别五个稳定主题群。

Result: 发现技术术语可能成为门槛，艺术家关切常被忽略。

Conclusion: 呼吁更多透明度和艺术家视角的参与，为未来研究提供方法论基础。

Abstract: As generative AI continues to reshape artistic production and alternate modes
of human expression, artists whose livelihoods are most directly affected have
raised urgent concerns about consent, transparency, and the future of creative
labor. However, the voices of artists are often marginalized in dominant public
and scholarly discourse. This study presents a twelve-year analysis, from 2013
to 2025, of English-language discourse surrounding AI-generated art. It draws
from 439 curated 500-word excerpts sampled from opinion articles, news reports,
blogs, legal filings, and spoken-word transcripts. Through a reproducible
methodology, we identify five stable thematic clusters and uncover a
misalignment between artists' perceptions and prevailing media narratives. Our
findings highlight how the use of technical jargon can function as a subtle
form of gatekeeping, often sidelining the very issues artists deem most urgent.
Our work provides a BERTopic-based methodology and a multimodal baseline for
future research, alongside a clear call for deeper, transparency-driven
engagement with artist perspectives in the evolving AI-creative landscape.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [81] [Thermal-Aware 3D Design for Side-Channel Information Leakage](https://arxiv.org/abs/2508.02816)
*Dylan Stow,Russell Barnes,Eren Kurshan,Yuan Xie*

Main category: cs.CR

TL;DR: 本文提出了一种利用3D集成技术和动态活动模式生成的方法，有效降低热侧信道攻击的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 热侧信道攻击可以泄露芯片关键功能块的活动甚至加密密钥，因此需要一种主动保护机制来隐藏关键活动并降低功耗。

Method: 结合3D集成的固有特性及动态生成的自定义活动模式，隐藏功能层中的关键活动。

Result: 实验显示，该方法将侧信道脆弱因子（SVF）降至0.05以下，空间热侧信道因子（STSF）降至0.59以下。

Conclusion: 3D技术与运行时算法结合，可高效保护芯片免受热侧信道攻击。

Abstract: Side-channel attacks are important security challenges as they reveal
sensitive information about on-chip activities. Among such attacks, the thermal
side-channel has been shown to disclose the activities of key functional blocks
and even encryption keys. This paper proposes a novel approach to proactively
conceal critical activities in the functional layers while minimizing the power
dissipation by (i) leveraging inherent characteristics of 3D integration to
protect from side-channel attacks and (ii) dynamically generating custom
activity patterns to match the activity to be concealed in the functional
layers. Experimental analysis shows that 3D technology combined with the
proposed run-time algorithm effectively reduces the Side channel vulnerability
Factor (SVF) below 0.05 and the Spatial Thermal Side-channel Factor (STSF)
below 0.59.

</details>


### [82] [MalFlows: Context-aware Fusion of Heterogeneous Flow Semantics for Android Malware Detection](https://arxiv.org/abs/2508.03588)
*Zhaoyi Meng,Fenglei Xu,Wenxiang Zhao,Wansen Wang,Wenchao Huang,Jie Cui,Hong Zhong,Yan Xiong*

Main category: cs.CR

TL;DR: 论文提出MalFlows技术，通过上下文感知的异构流语义融合，提升安卓恶意软件检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用不同类型流语义互补性上存在不足，且缺乏上下文感知，限制了流语义整合的准确性。

Method: 采用异构信息网络（HIN）建模流语义，提出上下文感知的HIN嵌入技术flow2vec，并结合通道注意力神经网络进行分类。

Result: 在31,000多个真实应用的大规模数据集上，MalFlows表现优于基线方法，验证了flow2vec的有效性。

Conclusion: MalFlows是首个全面整合多类流信息的研究，显著提升了恶意软件检测性能。

Abstract: Static analysis, a fundamental technique in Android app examination, enables
the extraction of control flows, data flows, and inter-component communications
(ICCs), all of which are essential for malware detection. However, existing
methods struggle to leverage the semantic complementarity across different
types of flows for representing program behaviors, and their context-unaware
nature further hinders the accuracy of cross-flow semantic integration. We
propose and implement MalFlows, a novel technique that achieves context-aware
fusion of heterogeneous flow semantics for Android malware detection. Our goal
is to leverage complementary strengths of the three types of flow-related
information for precise app profiling. We adopt a heterogeneous information
network (HIN) to model the rich semantics across these program flows. We
further propose flow2vec, a context-aware HIN embedding technique that
distinguishes the semantics of HIN entities as needed based on contextual
constraints across different flows and learns accurate app representations
through the joint use of multiple meta-paths. The representations are finally
fed into a channel-attention-based deep neural network for malware
classification. To the best of our knowledge, this is the first study to
comprehensively aggregate the strengths of diverse flow-related information for
assessing maliciousness within apps. We evaluate MalFlows on a large-scale
dataset comprising over 20 million flow instances extracted from more than
31,000 real-world apps. Experimental results demonstrate that MalFlows
outperforms representative baselines in Android malware detection, and
meanwhile, validate the effectiveness of flow2vec in accurately learning app
representations from the HIN constructed over the heterogeneous flows.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [83] [Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours](https://arxiv.org/abs/2508.03514)
*Pavlos Panagiotidis,Victor Zhi Heung Ngo,Sean Myatt,Roma Patel,Rachel Ramchurn,Alan Chamberlain,Ayse Kucukyilmaz*

Main category: cs.RO

TL;DR: 提出了一种基于导演指导的框架，通过戏剧化方法生成机器人情感化行为。


<details>
  <summary>Details</summary>
Motivation: 为艺术表演开发具有表达力的机器人行为，通过戏剧方法指导机器人动作生成。

Method: 利用导演指导的工作流，通过即兴手势生成情感表达，并建立可重用的动作模板库。

Result: 初步试验证明了框架的可行性，但机器人机械限制带来挑战。

Conclusion: 该框架为跨学科团队创建社交机器人行为提供了模型，并促进了人机协作方法的发展。

Abstract: In this paper, we propose theatre-in-the-loop, a framework for developing
expressive robot behaviours tailored to artistic performance through a
director-guided puppeteering workflow. Leveraging theatrical methods, we use
narrative objectives to direct a puppeteer in generating improvised robotic
gestures that convey specific emotions. These improvisations are captured and
curated to build a dataset of reusable movement templates for standalone
playback in future autonomous performances. Initial trials demonstrate the
feasibility of this approach, illustrating how the workflow enables precise
sculpting of robotic gestures into coherent emotional arcs while revealing
challenges posed by the robot's mechanical constraints. We argue that this
practice-led framework provides a model for interdisciplinary teams creating
socially expressive robot behaviours, contributing to (1) theatre as an
interactive training ground for human-robot interaction and (2) co-creation
methodologies between humans and machines.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [84] [Efficient Variational Quantum Algorithms via Circuit Knitting and Architecture Search](https://arxiv.org/abs/2508.03376)
*Jun Wu,Jiaqi Yang,Jicun Li,Wei Xie,Xiang-Yang Li*

Main category: quant-ph

TL;DR: CKVQA框架通过电路编织和子电路优化，显著减少采样开销，同时保持与传统量子电路设计相当的精度。


<details>
  <summary>Details</summary>
Motivation: 当前量子硬件可用的量子比特数远低于实用量子算法的需求，电路编织虽能解决但带来高采样开销。

Method: 引入CKVQA框架，结合量子电路架构搜索和子电路级优化，应用于变分量子算法（VQA）。

Result: 数值结果显示CKVQA显著降低采样开销，同时保持与传统设计相当的精度。

Conclusion: CKVQA为变分量子算法提供了一种高效的电路织解决方案。

Abstract: Current quantum hardware presents a significant limitation in the number of
available qubits compared to the requirements of practical quantum algorithms.
Circuit knitting has been proposed as a solution to this issue by partitioning
larger quantum circuits into smaller parts that can be executed by current
devices. However, this approach often leads to a high sampling overhead, which
increases exponentially with the number of cut points. In this paper, we
introduce CKVQA, a framework that applies circuit knitting to variational
quantum algorithms (VQAs). By employing a quantum circuit architecture search
adapted to this scenario, CKVQA aims to minimize the sampling overhead by
identifying parameterized quantum circuits that achieve a favorable balance
between algorithmic performance and sampling overhead. Additionally, since
circuit knitting generates multiple subcircuits, we have developed a
subcircuit-level optimization method to accelerate the training of VQAs and
reduce overall execution time. We apply this framework to two widely-used VQAs:
the Quantum Approximate Optimization Algorithm and the Variational Quantum
Eigensolver. Our numerical results demonstrate that the CKVQA framework
significantly reduces the sampling overheads while maintaining comparable
accuracy to conventional parameterized quantum circuit designs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [85] [SonicMaster: Towards Controllable All-in-One Music Restoration and Mastering](https://arxiv.org/abs/2508.03448)
*Jan Melechovsky,Ambuj Mehrish,Dorien Herremans*

Main category: cs.SD

TL;DR: SonicMaster是一个统一的生成模型，用于音乐修复和母带处理，通过文本控制解决多种音频问题。


<details>
  <summary>Details</summary>
Motivation: 解决非专业环境下音乐录音中常见的音频质量问题，如混响、失真等，传统方法需要多个工具和手动调整。

Method: 使用基于流的生成训练范式，通过文本提示将降级音频映射到修复版本，训练数据为模拟多种降级的高质量音频对。

Result: 客观指标显示SonicMaster显著提升音频质量，主观测试也证实用户偏好其修复结果。

Conclusion: SonicMaster作为一种统一方法，在音乐修复和母带处理中表现出高效性和广泛适用性。

Abstract: Music recordings often suffer from audio quality issues such as excessive
reverberation, distortion, clipping, tonal imbalances, and a narrowed stereo
image, especially when created in non-professional settings without specialized
equipment or expertise. These problems are typically corrected using separate
specialized tools and manual adjustments. In this paper, we introduce
SonicMaster, the first unified generative model for music restoration and
mastering that addresses a broad spectrum of audio artifacts with text-based
control. SonicMaster is conditioned on natural language instructions to apply
targeted enhancements, or can operate in an automatic mode for general
restoration. To train this model, we construct the SonicMaster dataset, a large
dataset of paired degraded and high-quality tracks by simulating common
degradation types with nineteen degradation functions belonging to five
enhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our
approach leverages a flow-matching generative training paradigm to learn an
audio transformation that maps degraded inputs to their cleaned, mastered
versions guided by text prompts. Objective audio quality metrics demonstrate
that SonicMaster significantly improves sound quality across all artifact
categories. Furthermore, subjective listening tests confirm that listeners
prefer SonicMaster's enhanced outputs over the original degraded audio,
highlighting the effectiveness of our unified approach.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [86] [Advancing Precision in Multi-Point Cloud Fusion Environments](https://arxiv.org/abs/2508.03179)
*Ulugbek Alibekov,Vanessa Staderini,Philipp Schneider,Doris Antensteiner*

Main category: cs.CV

TL;DR: 该研究通过评估点云和多点云匹配方法，专注于视觉工业检测，并引入合成数据集和改进的CloudCompare插件以提高自动检测系统的效率。


<details>
  <summary>Details</summary>
Motivation: 解决视觉工业检测中点云匹配和表面缺陷分析的准确性与效率问题。

Method: 提出合成数据集用于配准方法的定量评估，开发CloudCompare插件以实现多点云合并和表面缺陷可视化。

Result: 提高了点云匹配的精度和自动化检测系统的效率。

Conclusion: 该研究通过新工具和数据集，显著提升了工业检测中点云分析的性能和实用性。

Abstract: This research focuses on visual industrial inspection by evaluating point
clouds and multi-point cloud matching methods. We also introduce a synthetic
dataset for quantitative evaluation of registration method and various distance
metrics for point cloud comparison. Additionally, we present a novel
CloudCompare plugin for merging multiple point clouds and visualizing surface
defects, enhancing the accuracy and efficiency of automated inspection systems.

</details>


### [87] [Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN](https://arxiv.org/abs/2508.03415)
*Shivangi Nigam,Adarsh Prasad Behera,Shekhar Verma,P. Nagabhushan*

Main category: cs.CV

TL;DR: Fd-CycleGAN通过结合Local Neighborhood Encoding和频率感知监督，改进了CycleGAN的潜在表示学习，实现了更高质量的图像翻译。


<details>
  <summary>Details</summary>
Motivation: 现有的CycleGAN在低数据量场景下效果不佳，Fd-CycleGAN旨在通过改进潜在表示学习和分布对齐来提升性能。

Method: 提出了Local Neighborhood Encoding和频率感知监督，并结合KL/JS散度等分布损失来量化生成图像与真实图像的分布差异。

Result: 在Horse2Zebra等数据集上表现优于基线CycleGAN和其他SOTA方法，具有更高的感知质量、更快的收敛速度和更好的模态多样性。

Conclusion: Fd-CycleGAN在图像翻译任务中表现优越，尤其适用于文档修复、艺术风格迁移和医学图像合成等领域。

Abstract: This paper presents Fd-CycleGAN, an image-to-image (I2I) translation
framework that enhances latent representation learning to approximate real data
distributions. Building upon the foundation of CycleGAN, our approach
integrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to
capture fine-grained local pixel semantics while preserving structural
coherence from the source domain. We employ distribution-based loss metrics,
including KL/JS divergence and log-based similarity measures, to explicitly
quantify the alignment between real and generated image distributions in both
spatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we
conduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a
synthetically augmented Strike-off dataset. Compared to baseline CycleGAN and
other state-of-the-art methods, our approach demonstrates superior perceptual
quality, faster convergence, and improved mode diversity, particularly in
low-data regimes. By effectively capturing local and global distribution
characteristics, Fd-CycleGAN achieves more visually coherent and semantically
consistent translations. Our results suggest that frequency-guided latent
learning significantly improves generalization in image translation tasks, with
promising applications in document restoration, artistic style transfer, and
medical image synthesis. We also provide comparative insights with
diffusion-based generative models, highlighting the advantages of our
lightweight adversarial approach in terms of training efficiency and
qualitative output.

</details>


### [88] [VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering](https://arxiv.org/abs/2508.03039)
*Yiran Meng,Junhong Ye,Wei Zhou,Guanghui Yue,Xudong Mao,Ruomei Wang,Baoquan Zhao*

Main category: cs.CV

TL;DR: VideoForest框架通过人物锚定的分层推理解决跨视频问答的挑战，整合了人物特征提取、多粒度树结构和多代理推理，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 跨视频问答面临多视频流间建立联系和多源信息检索的复杂性，传统单视频理解方法难以应对。

Method: 采用人物特征作为桥梁，结合ReID和跟踪算法、多粒度树结构和多代理推理框架。

Result: 实验显示，VideoForest在人物识别、行为分析及推理任务中表现优异，准确性显著提升。

Conclusion: VideoForest通过人物级特征统一多视频流，实现了跨视频理解的新范式，兼具计算效率。

Abstract: Cross-video question answering presents significant challenges beyond
traditional single-video understanding, particularly in establishing meaningful
connections across video streams and managing the complexity of multi-source
information retrieval. We introduce VideoForest, a novel framework that
addresses these challenges through person-anchored hierarchical reasoning. Our
approach leverages person-level features as natural bridge points between
videos, enabling effective cross-video understanding without requiring
end-to-end training. VideoForest integrates three key innovations: 1) a
human-anchored feature extraction mechanism that employs ReID and tracking
algorithms to establish robust spatiotemporal relationships across multiple
video sources; 2) a multi-granularity spanning tree structure that
hierarchically organizes visual content around person-level trajectories; and
3) a multi-agent reasoning framework that efficiently traverses this
hierarchical structure to answer complex cross-video queries. To evaluate our
approach, we develop CrossVideoQA, a comprehensive benchmark dataset
specifically designed for person-centric cross-video analysis. Experimental
results demonstrate VideoForest's superior performance in cross-video reasoning
tasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior
analysis, and 51.67% in summarization and reasoning, significantly
outperforming existing methods. Our work establishes a new paradigm for
cross-video understanding by unifying multiple video streams through
person-level features, enabling sophisticated reasoning across distributed
visual information while maintaining computational efficiency.

</details>


### [89] [DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition](https://arxiv.org/abs/2508.03397)
*Xinzhu Li,Juepeng Zheng,Yikun Chen,Xudong Mao,Guanghui Yue,Wei Zhou,Chenlei Lv,Ruomei Wang,Fan Zhou,Baoquan Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种名为DepthGait的新框架，通过结合RGB深度图和剪影来提升步态识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法中的2D剪影和骨骼模型在视角变化和细节捕捉上表现不足，需要更丰富的输入模态来提高判别性。

Method: DepthGait框架从RGB图像序列中估计深度图，并开发了一种多尺度和跨级别融合方案来弥合深度图与剪影之间的模态差异。

Result: 在标准基准测试中，DepthGait实现了最先进的性能，并在具有挑战性的数据集中取得了显著的rank-1准确率。

Conclusion: 结果表明，深度图的引入和多模态融合显著提升了步态识别的性能。

Abstract: Robust gait recognition requires highly discriminative representations, which
are closely tied to input modalities. While binary silhouettes and skeletons
have dominated recent literature, these 2D representations fall short of
capturing sufficient cues that can be exploited to handle viewpoint variations,
and capture finer and meaningful details of gait. In this paper, we introduce a
novel framework, termed DepthGait, that incorporates RGB-derived depth maps and
silhouettes for enhanced gait recognition. Specifically, apart from the 2D
silhouette representation of the human body, the proposed pipeline explicitly
estimates depth maps from a given RGB image sequence and uses them as a new
modality to capture discriminative features inherent in human locomotion. In
addition, a novel multi-scale and cross-level fusion scheme has also been
developed to bridge the modality gap between depth maps and silhouettes.
Extensive experiments on standard benchmarks demonstrate that the proposed
DepthGait achieves state-of-the-art performance compared to peer methods and
attains an impressive mean rank-1 accuracy on the challenging datasets.

</details>


### [90] [Live Demonstration: Neuromorphic Radar for Gesture Recognition](https://arxiv.org/abs/2508.03324)
*Satyapreet Singh Yadav,Chandra Sekhar Seelamantula,Chetan Singh Thakur*

Main category: cs.CV

TL;DR: 提出了一种基于事件驱动的神经形态雷达框架，用于实时、低功耗的手势识别，采用异步Sigma-Delta编码和轻量级神经网络，实现低延迟和高能效。


<details>
  <summary>Details</summary>
Motivation: 传统雷达手势识别系统需要连续采样和处理数据，功耗和计算开销较大。本文受生物感知启发，设计了一种事件驱动的架构，以降低能耗和延迟。

Method: 使用24 GHz多普勒雷达前端和自定义神经形态采样器，将中频信号通过异步Sigma-Delta编码转换为稀疏的基于事件的特征。这些事件由Cortex-M0微控制器上的轻量级神经网络直接处理。

Result: 在7位用户的5种手势数据集上，系统实现了实时准确率>85%，同时显著降低了内存、功耗和计算开销。

Conclusion: 该框架为雷达手势识别提供了一种新型的低功耗、低延迟解决方案，首次将异步Sigma-Delta编码和事件驱动处理应用于该领域。

Abstract: We present a neuromorphic radar framework for real-time, low-power hand
gesture recognition (HGR) using an event-driven architecture inspired by
biological sensing. Our system comprises a 24 GHz Doppler radar front-end and a
custom neuromorphic sampler that converts intermediate-frequency (IF) signals
into sparse spike-based representations via asynchronous sigma-delta encoding.
These events are directly processed by a lightweight neural network deployed on
a Cortex-M0 microcontroller, enabling low-latency inference without requiring
spectrogram reconstruction. Unlike conventional radar HGR pipelines that
continuously sample and process data, our architecture activates only when
meaningful motion is detected, significantly reducing memory, power, and
computation overhead. Evaluated on a dataset of five gestures collected from
seven users, our system achieves > 85% real-time accuracy. To the best of our
knowledge, this is the first work that employs bio-inspired asynchronous
sigma-delta encoding and an event-driven processing framework for radar-based
HGR.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [91] [What If, But Privately: Private Counterfactual Retrieval](https://arxiv.org/abs/2508.03681)
*Shreya Meel,Mohamed Nomeir,Pasan Dissanayake,Sanghamitra Dutta,Sennur Ulukus*

Main category: cs.IT

TL;DR: 该论文提出了一种保护用户隐私的反事实解释检索框架，解决了黑箱机器学习模型在高风险应用中的透明性和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，黑箱机器学习模型需要提供透明和可解释的反事实解释，但传统方法可能威胁用户和机构的隐私。论文旨在保护用户隐私，同时提供有效的反事实解释。

Method: 论文首先提出了私有反事实检索（PCR）问题，并提出了基线PCR方案以保护用户隐私。随后提出两种改进方案以减少数据库信息泄露。接着讨论了不可变PCR（I-PCR）问题，并提出了两种隐私保护方案。最后扩展了方案以支持用户偏好。

Result: 理论分析和数值结果表明，提出的方案能够信息论完美保护用户隐私，同时减少数据库信息泄露。

Conclusion: 该框架在保护隐私的同时，提供了高效的反事实解释检索方案，适用于高风险应用场景。

Abstract: Transparency and explainability are two important aspects to be considered
when employing black-box machine learning models in high-stake applications.
Providing counterfactual explanations is one way of catering this requirement.
However, this also poses a threat to the privacy of the institution that is
providing the explanation, as well as the user who is requesting it. In this
work, we are primarily concerned with the user's privacy who wants to retrieve
a counterfactual instance, without revealing their feature vector to the
institution. Our framework retrieves the exact nearest neighbor counterfactual
explanation from a database of accepted points while achieving perfect,
information-theoretic, privacy for the user. First, we introduce the problem of
private counterfactual retrieval (PCR) and propose a baseline PCR scheme that
keeps the user's feature vector information-theoretically private from the
institution. Building on this, we propose two other schemes that reduce the
amount of information leaked about the institution database to the user,
compared to the baseline scheme. Second, we relax the assumption of mutability
of all features, and consider the setting of immutable PCR (I-PCR). Here, the
user retrieves the nearest counterfactual without altering a private subset of
their features, which constitutes the immutable set, while keeping their
feature vector and immutable set private from the institution. For this, we
propose two schemes that preserve the user's privacy information-theoretically,
but ensure varying degrees of database privacy. Third, we extend our PCR and
I-PCR schemes to incorporate user's preference on transforming their
attributes, so that a more actionable explanation can be received. Finally, we
present numerical results to support our theoretical findings, and compare the
database leakage of the proposed schemes.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [92] [CTBench: Cryptocurrency Time Series Generation Benchmark](https://arxiv.org/abs/2508.02758)
*Yihao Ang,Qiang Wang,Qiang Huang,Yifan Bao,Xinyu Xi,Anthony K. H. Tung,Chen Jin,Zhiyong Huang*

Main category: q-fin.ST

TL;DR: 介绍了CTBench，一个针对加密货币市场的时间序列生成（TSG）基准测试工具，填补了现有方法在该领域的不足。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场的特殊性（24/7交易、高波动性和快速变化）导致现有TSG方法不适用，缺乏金融场景评估。

Method: 提出了CTBench，包含开源数据集和13项指标，评估模型在预测、交易、风险等方面的表现，采用双任务框架。

Result: 测试了8种模型，揭示了统计保真度与实际盈利性之间的权衡，并提供模型选择和部署的实用建议。

Conclusion: CTBench为加密货币领域的TSG模型提供了首个全面基准，具有实用指导意义。

Abstract: Synthetic time series are essential tools for data augmentation, stress
testing, and algorithmic prototyping in quantitative finance. However, in
cryptocurrency markets, characterized by 24/7 trading, extreme volatility, and
rapid regime shifts, existing Time Series Generation (TSG) methods and
benchmarks often fall short, jeopardizing practical utility. Most prior work
(1) targets non-financial or traditional financial domains, (2) focuses
narrowly on classification and forecasting while neglecting crypto-specific
complexities, and (3) lacks critical financial evaluations, particularly for
trading applications. To address these gaps, we introduce \textsf{CTBench}, the
first comprehensive TSG benchmark tailored for the cryptocurrency domain.
\textsf{CTBench} curates an open-source dataset from 452 tokens and evaluates
TSG models across 13 metrics spanning 5 key dimensions: forecasting accuracy,
rank fidelity, trading performance, risk assessment, and computational
efficiency. A key innovation is a dual-task evaluation framework: (1) the
\emph{Predictive Utility} task measures how well synthetic data preserves
temporal and cross-sectional patterns for forecasting, while (2) the
\emph{Statistical Arbitrage} task assesses whether reconstructed series support
mean-reverting signals for trading. We benchmark eight representative models
from five methodological families over four distinct market regimes, uncovering
trade-offs between statistical fidelity and real-world profitability. Notably,
\textsf{CTBench} offers model ranking analysis and actionable guidance for
selecting and deploying TSG models in crypto analytics and strategy
development.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [93] [Beyond risk: A proto-framework for assessing the societal impact of AI systems](https://arxiv.org/abs/2508.03666)
*Willem Fourie*

Main category: cs.CY

TL;DR: 论文提出了一个原型框架，通过'自由'概念评估AI的社会影响，以补充当前基于风险的AI监管方法。


<details>
  <summary>Details</summary>
Motivation: 当前'负责任AI'范式主要关注风险缓解，但对AI社会影响的系统性考量不足，作者希望通过自由概念填补这一空白。

Method: 结合康德哲学和当代诠释，提出自由的两个维度（能力和机会），并基于可持续发展目标构建原型框架。

Result: 原型框架为政策制定提供了一种评估AI社会影响的工具，旨在补充现有风险导向方法。

Conclusion: 该框架是首次将自由概念操作化于AI监管中，为未来研究奠定基础。

Abstract: In the discourse on AI regulation, 'responsible AI' is the dominant paradigm,
with the focus on mitigating the risks related to AI systems. While this focus
is important and necessary, it has limited use for a systematic consideration
of AI's societal impact. This paper proposes a proto-framework for assessing
the societal impact of AI systems by operationalising the concept of freedom.
This proto-framework is intended as a step towards a fully operationalised
framework to be used in policymaking contexts. By drawing on Kantian philosophy
and related contemporary interpretations, freedom is developed as the
counterpart to the concept of responsibility. Two dimensions of freedom are
developed in further detail: freedom as capability and freedom as opportunity.
These two dimensions of freedom are then applied in a proto-framework that
systematically considers AI's impact on society using the Sustainable
Development Goals. This proto-framework aims to complement current risk-based
approaches and thereby offers a first step towards operationalising the concept
of freedom in AI regulation.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [94] [Design Support for Multitape Turing Machines](https://arxiv.org/abs/2508.03638)
*Marco T. Morazán,Oliwia Kempinski,Andrés M. Garced*

Main category: cs.FL

TL;DR: 该论文探讨了如何通过三种可视化工具帮助学生更好地理解和设计多带图灵机，解决了学生在学习过程中遇到的困难。


<details>
  <summary>Details</summary>
Motivation: 学生在学习多带图灵机时面临理解操作语义和接受/拒绝单词原因的困难，现有的FSM编程语言支持不足。

Method: 开发了三种可视化工具：动态模拟执行工具、静态渲染多带图灵机转移图工具和静态渲染计算图工具。

Result: 这些工具被学生广泛接受并认为有用，实证数据支持其有效性。

Conclusion: 提出的可视化工具显著提升了学生对多带图灵机的理解和设计能力。

Abstract: Many Formal Languages and Automata Theory courses introduce students to
Turing machine extensions. One of the most widely-used extensions endows Turing
machines with multiple tapes. Although multitape Turing machines are an
abstraction to simplify Turing machine design, students find them no less
challenging. To aid students in understanding these machines, the FSM
programming language provides support for their definition and execution. This,
however, has proven insufficient for many students to understand the
operational semantics of such machines and to understand why such machines
accept or reject a word. To address this problem, three visualization tools
have been developed. The first is a dynamic visualization tool that simulates
machine execution. The second is a static visualization tool that automatically
renders a graphic for a multitape Turing machine's transition diagram. The
third is a static visualization tool that automatically renders computation
graphs for multitape Turing machines. This article presents these tools and
illustrates how they are used to help students design and implement multitape
Turing machines. In addition, empirical data is presented that suggests these
tools are well-received and found useful by students.

</details>


### [95] [A Design Recipe and Recipe-Based Errors for Regular Expressions](https://arxiv.org/abs/2508.03639)
*Marco T. Morazán,Shamil Dzhatdoyev,Josephine Des Rosiers,Tijana Minić,Andrés M. Garced,David Anthony K. Fields*

Main category: cs.FL

TL;DR: 本文提出了一种新框架，用于支持形式语言与自动机理论学生学习正则表达式的设计。框架包括设计方法和定制错误提示系统。


<details>
  <summary>Details</summary>
Motivation: 为学生学习正则表达式设计提供支持，改进错误提示的清晰度和实用性。

Method: 开发了一个设计方法、定制的错误提示系统和简化的单元测试语法。

Result: 框架在课堂中使用，演示了调试会话，并初步实现了错误提示系统。

Conclusion: 该框架有效支持学生学习，错误提示系统设计实用且符合最佳实践。

Abstract: This article presents a novel framework to provide Formal Languages and
Automata Theory students design support for the development of regular
expressions. This framework includes a design recipe for regular expressions
and a customized error messaging system. The error messaging system produces
recipe-based errors that include the step of the design recipe not successfully
completed. Furthermore, the error messages follow the established practices of
being concise, succinct, jargon-free, and nonprescriptive. In addition, a
shorthand syntax developed for writing unit tests is described. The in-class
use of the design recipe is illustrated, two debugging sessions using the
described system are discussed, and the implementation of the error messaging
system is briefly sketched.

</details>


### [96] [Visual Execution and Validation of Finite-State Machines and Pushdown Automata](https://arxiv.org/abs/2508.03641)
*Marco T. Morazán,David Anthony K. Fields,Andrés M. Garced,Tijana Minić*

Main category: cs.FL

TL;DR: 论文提出两种动态可视化工具，帮助学生理解非确定性有限状态机和下推自动机的工作原理。


<details>
  <summary>Details</summary>
Motivation: 学生在学习非确定性有限状态机和下推自动机时遇到困难，尤其是理解其操作语义和栈的使用。

Method: 开发两种动态可视化工具，逐步展示非确定性有限状态机和下推自动机的所有计算过程，并支持视觉验证状态属性。

Result: 工具帮助学生理解机器的工作原理，并支持验证状态的属性。

Conclusion: 动态可视化工具有效解决了学生在学习非确定性有限状态机和下推自动机时的理解难题。

Abstract: In Formal Languages and Automata Theory courses, students find understanding
nondeterministic finite-state and pushdown automata difficult. In many cases,
this means that it is challenging for them to comprehend the operational
semantics of such machines and, as a consequence, determine why a word is
accepted or rejected. This is not entirely surprising, because students are
mostly trained to design and implement deterministic programs. Comprehension of
pushdown automata is further complicated, because reasoning about the stack is
necessary. A common difficulty students face, for example, is understanding
that two different computations on the same word may reach the same state with
different stack values. To aid student understanding, we present two novel
dynamic visualization tools for FSM -- a domain-specific programming language
for the Automata Theory classroom -- to support the design of such machines.
These tools visualize all computations that may be performed, respectively, by
a nondeterministic finite-state machine or by a pushdown automata in a stepwise
manner. In addition, these tools aid the machine verification process by
allowing users to visually validate whether the properties a state represents
hold when a machine transitions into it.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [97] [Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams](https://arxiv.org/abs/2508.03379)
*Wenxin Mao,Zhitao Wang Long Wang,Sirong Chen,Cuiyun Gao,Luyang Cao,Ziming Liu,Qiming Zhang,Jun Zhou,Zhi Jin*

Main category: cs.AI

TL;DR: 论文提出了一种名为UML2Dep的分步代码生成框架，通过形式化规范消除自然语言描述的歧义性，提升LLM生成代码的准确性。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述在复杂需求（如系统行为、条件逻辑和架构约束）中容易产生歧义，特别是在服务导向架构中隐含的数据依赖性问题难以处理。

Method: 1. 提出了增强版的UML序列图，整合决策表和API规范以消除歧义。2. 引入数据依赖推断（DDI）任务，通过约束数学推理和提示策略构建显式数据依赖图。

Result: 框架通过形式化规范和静态解析优化推理过程，显著提升了代码生成的准确性和效率。

Conclusion: UML2Dep框架有效解决了自然语言描述中的歧义性问题，为复杂需求的代码生成提供了可靠解决方案。

Abstract: Large language models (LLMs) excel at generating code from natural language
(NL) descriptions. However, the plain textual descriptions are inherently
ambiguous and often fail to capture complex requirements like intricate system
behaviors, conditional logic, and architectural constraints; implicit data
dependencies in service-oriented architectures are difficult to infer and
handle correctly. To bridge this gap, we propose a novel step-by-step code
generation framework named UML2Dep by leveraging unambiguous formal
specifications of complex requirements. First, we introduce an enhanced Unified
Modeling Language (UML) sequence diagram tailored for service-oriented
architectures. This diagram extends traditional visual syntax by integrating
decision tables and API specifications, explicitly formalizing structural
relationships and business logic flows in service interactions to rigorously
eliminate linguistic ambiguity. Second, recognizing the critical role of data
flow, we introduce a dedicated data dependency inference (DDI) task. DDI
systematically constructs an explicit data dependency graph prior to actual
code synthesis. To ensure reliability, we formalize DDI as a constrained
mathematical reasoning task through novel prompting strategies, aligning with
LLMs' excellent mathematical strengths. Additional static parsing and
dependency pruning further reduce context complexity and cognitive load
associated with intricate specifications, thereby enhancing reasoning accuracy
and efficiency.

</details>


### [98] [VQA support to Arabic Language Learning Educational Tool](https://arxiv.org/abs/2508.03488)
*Khaled Bachir Delassi,Lakhdar Zeggane,Hadda Cherroun,Abdelhamid Haouhat,Kaoutar Bouzouad*

Main category: cs.AI

TL;DR: 论文总结：AI驱动的阿拉伯语学习工具，通过视觉问答和主动学习提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语学习工具稀缺的问题，尤其是基于现代教育学模型（如主动学习）的工具。

Method: 设计并评估了一个AI工具，利用视觉问答和构建主义学习法，结合视觉-语言预训练模型和大语言模型生成交互式测验。

Result: 工具在1266个视觉问答测试中表现出合适的准确率，证明了其潜力。

Conclusion: 该工具能有效填补阿拉伯语教育空白，为学习者提供个性化和互动的学习体验。

Abstract: We address the problem of scarcity of educational Arabic Language Learning
tools that advocate modern pedagogical models such as active learning which
ensures language proficiency. In fact, we investigate the design and evaluation
of an AI-powered educational tool designed to enhance Arabic language learning
for non-native speakers with beginner-to-intermediate proficiency level. The
tool leverages advanced AI models to generate interactive visual quizzes,
deploying Visual Question Answering as the primary activity. Adopting a
constructivist learning approach, the system encourages active learning through
real-life visual quizzes, and image-based questions that focus on improving
vocabulary, grammar, and comprehension. The system integrates Vision-Language
Pretraining models to generate contextually relevant image description from
which Large Language Model generate assignments based on customized Arabic
language Learning quizzes thanks to prompting.
  The effectiveness of the tool is evaluated through a manual annotated
benchmark consisting of 1266 real-life visual quizzes, with human participants
providing feedback. The results show a suitable accuracy rates, validating the
tool's potential to bridge the gap in Arabic language education and
highlighting the tool's promise as a reliable, AI-powered resource for Arabic
learners, offering personalized and interactive learning experiences.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [99] [Embedding-Enhanced Probabilistic Modeling of Ferroelectric Field Effect Transistors (FeFETs)](https://arxiv.org/abs/2508.02737)
*Tasnia Nobi Afee,Jack Hutchins,Md Mazharul Islam,Thomas Kampfe,Ahmedullah Aziz*

Main category: cs.LG

TL;DR: 提出了一种基于混合密度网络（MDN）的概率建模框架，用于捕捉FeFET的随机性，解决了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: FeFET的随机性来自操作循环和制造变异性，现有模型难以准确捕捉其全部特征或缺乏数学平滑性，影响电路级集成的稳定性。

Method: 采用C-无穷连续激活函数的MDN模型，结合器件特定的嵌入层，以捕捉器件间的本征物理变异性，并生成合成器件实例。

Result: 模型在捕捉FeFET电流行为变异性时表现出高准确性（R2为0.92）。

Conclusion: 该框架为FeFET的随机行为建模提供了可扩展的数据驱动解决方案，为未来的紧凑模型开发和电路仿真集成奠定了基础。

Abstract: FeFETs hold strong potential for advancing memory and logic technologies, but
their inherent randomness arising from both operational cycling and fabrication
variability poses significant challenges for accurate and reliable modeling.
Capturing this variability is critical, as it enables designers to predict
behavior, optimize performance, and ensure reliability and robustness against
variations in manufacturing and operating conditions. Existing deterministic
and machine learning-based compact models often fail to capture the full extent
of this variability or lack the mathematical smoothness required for stable
circuit-level integration. In this work, we present an enhanced probabilistic
modeling framework for FeFETs that addresses these limitations. Building upon a
Mixture Density Network (MDN) foundation, our approach integrates C-infinity
continuous activation functions for smooth, stable learning and a
device-specific embedding layer to capture intrinsic physical variability
across devices. Sampling from the learned embedding distribution enables the
generation of synthetic device instances for variability-aware simulation. With
an R2 of 0.92, the model demonstrates high accuracy in capturing the
variability of FeFET current behavior. Altogether, this framework provides a
scalable, data-driven solution for modeling the full stochastic behavior of
FeFETs and offers a strong foundation for future compact model development and
circuit simulation integration.

</details>


### [100] [Frontier: Simulating the Next Generation of LLM Inference Systems](https://arxiv.org/abs/2508.03148)
*Yicheng Feng,Xin Tan,Kin Hang Sew,Yimin Jiang,Yibo Zhu,Hong Xu*

Main category: cs.LG

TL;DR: Frontier是一个高保真模拟器，专为复杂的大型语言模型（LLM）推理设计，支持MoE模型和解耦架构。


<details>
  <summary>Details</summary>
Motivation: 随着MoE模型和解耦架构的兴起，现有模拟器无法捕捉新的系统动态，因此需要一种新的模拟器来支持这些新兴范式。

Method: Frontier采用统一框架，支持共址和解耦系统，并提供对MoE推理的原生支持，包括专家并行（EP）和复杂工作流模拟。

Result: Frontier通过改进的操作模型提高了准确性，并支持跨集群专家路由和高级流水线策略。

Conclusion: Frontier为社区提供了一个工具，以设计和优化未来大规模LLM推理。

Abstract: Large Language Model (LLM) inference is growing increasingly complex with the
rise of Mixture-of-Experts (MoE) models and disaggregated architectures that
decouple components like prefill/decode (PD) or attention/FFN (AF) for
heterogeneous scaling. Existing simulators, architected for co-located, dense
models, are unable to capture the intricate system dynamics of these emerging
paradigms. We present Frontier, a high-fidelity simulator designed from the
ground up for this new landscape. Frontier introduces a unified framework to
model both co-located and disaggregated systems, providing native support for
MoE inference with expert parallelism (EP). It enables the simulation of
complex workflows like cross-cluster expert routing and advanced pipelining
strategies for latency hiding. To ensure fidelity and usability, Frontier
incorporates refined operator models for improved accuracy. Frontier empowers
the community to design and optimize the future of LLM inference at scale.

</details>


### [101] [Heterogeneity-Oblivious Robust Federated Learning](https://arxiv.org/abs/2508.03579)
*Weiyao Zhang,Jinyang Li,Qi Song,Miao Wang,Chungang Lin,Haitong Luo,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: Horus 是一种基于低秩适应（LoRAs）的联邦学习框架，解决了在超异构环境中的中毒攻击问题，通过聚合 LoRAs 减少攻击面，并利用稳定性特征过滤恶意客户端。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在超异构环境中极易受到中毒攻击，且高维度模型扩大了攻击面。传统聚合策略效果不佳，攻击难以检测。

Method: Horus 在稳定层中插入 LoRAs，仅聚合 LoRAs 以减少攻击面。利用输入投影（LoRA-A）的稳定性特征设计中毒分数过滤恶意客户端，并对良性客户端采用投影感知聚合机制。

Result: 在多样化数据集、模型架构和攻击场景中，Horus 在鲁棒性和准确性上均优于现有基线。

Conclusion: Horus 通过低秩适应和稳定性特征有效解决了超异构环境下联邦学习的中毒攻击问题，提升了系统安全性和性能。

Abstract: Federated Learning (FL) remains highly vulnerable to poisoning attacks,
especially under real-world hyper-heterogeneity, where clients differ
significantly in data distributions, communication capabilities, and model
architectures. Such heterogeneity not only undermines the effectiveness of
aggregation strategies but also makes attacks more difficult to detect.
Furthermore, high-dimensional models expand the attack surface. To address
these challenges, we propose Horus, a heterogeneity-oblivious robust FL
framework centered on low-rank adaptations (LoRAs). Rather than aggregating
full model parameters, Horus inserts LoRAs into empirically stable layers and
aggregates only LoRAs to reduce the attack surface.We uncover a key empirical
observation that the input projection (LoRA-A) is markedly more stable than the
output projection (LoRA-B) under heterogeneity and poisoning. Leveraging this,
we design a Heterogeneity-Oblivious Poisoning Score using the features from
LoRA-A to filter poisoned clients. For the remaining benign clients, we propose
projection-aware aggregation mechanism to preserve collaborative signals while
suppressing drifts, which reweights client updates by consistency with the
global directions. Extensive experiments across diverse datasets, model
architectures, and attacks demonstrate that Horus consistently outperforms
state-of-the-art baselines in both robustness and accuracy.

</details>


### [102] [GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics](https://arxiv.org/abs/2508.02926)
*Arthur Cho*

Main category: cs.LG

TL;DR: GrandJury提出了一种新的评估协议，结合时间衰减聚合、完整追踪和动态任务评分，支持多元化和可追责的AI模型评估。


<details>
  <summary>Details</summary>
Motivation: 现行AI模型评估方法依赖静态基准测试，无法适应动态用户需求或不断变化的实际情况，亟需改进。

Method: GrandJury协议整合了时间衰减聚合、完整追踪、动态任务评分和多评委人类判断，提供透明且多元的评估框架。

Result: 通过开源实现和公开数据集展示了新评估方法的必要性，为AI从业者提供了无绝对真实情况下的评估新范式。

Conclusion: GrandJury为AI模型在动态环境中的评估提供了更灵活、透明和可追责的方案。

Abstract: Generative Machine Learning models have become central to modern systems,
powering applications in creative writing, summarization, multi-hop reasoning,
and context-aware dialogue. These models underpin large-scale AI assistants,
workflow automation, and autonomous decision-making. In such domains,
acceptable response is rarely absolute or static, but plural and highly
context-dependent. Yet standard evaluation regimes still rely on static,
benchmark-style tests, incentivizing optimization toward leaderboard scores
rather than alignment with dynamic user needs or evolving realities. GrandJury
introduces a formal evaluation protocol combining time-decayed aggregation,
complete traceability, with the support of dynamic, transparent task rubric
attribution, and multi-rater human judgment. Together, these elements enable
pluralistic, accountable evaluation that captures evolving consensus and
surfaces disagreement. We provide an open-source implementation (grandjury PyPI
package) and a public collection of Large Language Model (LLM) inference
outputs to illustrate the need and method. GrandJury provides a new paradigm
for AI practitioners when evaluating machine learning outputs without absolute
ground truth.

</details>


### [103] [Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03501)
*Alexander Golubev,Maria Trofimova,Sergei Polezhaev,Ibragim Badertdinov,Maksim Nekrashevich,Anton Shevtsov,Simon Karasik,Sergey Abramov,Andrei Andriushchenko,Filipp Fisin,Sergei Skvortsov,Boris Yangel*

Main category: cs.LG

TL;DR: 论文探讨了将强化学习应用于需要多轮交互的真实世界任务（如软件工程），并展示了其成功案例。


<details>
  <summary>Details</summary>
Motivation: 填补强化学习在多轮交互任务中的空白，特别是软件工程这类需要与环境多次交互的领域。

Method: 采用改进的DAPO算法，基于Qwen2.5-72B-Instruct模型训练智能体。

Result: 在SWE-bench测试中，成功率从20%提升至39%，并在SWE-rebench中表现优于其他模型。

Conclusion: 证明强化学习在多轮交互任务中的有效性，为构建更强大的自主智能体提供了可行路径。

Abstract: Research on applications of Reinforcement Learning (RL) to Large Language
Models (LLMs) has mostly been focused on single-turn problems, such as
mathematical reasoning or single-shot code generation. While these problems can
be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate
case of multi-turn interaction where the environment provides no feedback. This
contrasts with many real-world domains, such as software engineering (SWE),
which require rich multi-turn interactions with a stateful environment that
responds to each action with a non-trivial observation.
  To bridge this gap, we demonstrate the successful application of RL to this
general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)
algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world
software engineering tasks. Our approach increases the agent's success rate on
the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to
39%, without relying on any teacher models. On SWE-rebench, our agent matches
or outperforms leading open-weight models such as DeepSeek-V3-0324 and
Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward
building more capable autonomous agents for complex real-world problems based
on open models.

</details>
