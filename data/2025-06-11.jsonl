{"id": "2506.08055", "pdf": "https://arxiv.org/pdf/2506.08055", "abs": "https://arxiv.org/abs/2506.08055", "authors": ["Sabbir M. Saleh", "Nazim Madhavji", "John Steinbacher"], "title": "A Systematic Literature Review on Continuous Integration and Deployment (CI/CD) for Secure Cloud Computing", "categories": ["cs.SE", "cs.CR", "D.2.11"], "comment": "11 pages, 3 figures", "summary": "As cloud environments become widespread, cybersecurity has emerged as a top\npriority across areas such as networks, communication, data privacy, response\ntimes, and availability. Various sectors, including industries, healthcare, and\ngovernment, have recently faced cyberattacks targeting their computing systems.\nEnsuring secure app deployment in cloud environments requires substantial\neffort. With the growing interest in cloud security, conducting a systematic\nliterature review (SLR) is critical to identifying research gaps. Continuous\nSoftware Engineering, which includes continuous integration (CI), delivery\n(CDE), and deployment (CD), is essential for software development and\ndeployment. In our SLR, we reviewed 66 papers, summarising tools, approaches,\nand challenges related to the security of CI/CD in the cloud. We addressed key\naspects of cloud security and CI/CD and reported on tools such as Harbor,\nSonarQube, and GitHub Actions. Challenges such as image manipulation,\nunauthorised access, and weak authentication were highlighted. The review also\nuncovered research gaps in how tools and practices address these security\nissues in CI/CD pipelines, revealing a need for further study to improve\ncloud-based security solutions."}
{"id": "2506.08153", "pdf": "https://arxiv.org/pdf/2506.08153", "abs": "https://arxiv.org/abs/2506.08153", "authors": ["Renato Cordeiro Ferreira"], "title": "A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.8; I.2.0"], "comment": "4 pages, 3 figures (2 diagrams, 1 table), to be published in CAIN\n  2025", "summary": "How can the complexity of ML-enabled systems be managed effectively? The goal\nof this research is to investigate how complexity affects ML-Enabled Systems\n(MLES). To address this question, this research aims to introduce a\nmetrics-based architectural model to characterize the complexity of MLES. The\ngoal is to support architectural decisions, providing a guideline for the\ninception and growth of these systems. This paper showcases the first step for\ncreating the metrics-based architectural model: an extension of a reference\narchitecture that can describe MLES to collect their metrics."}
{"id": "2506.08171", "pdf": "https://arxiv.org/pdf/2506.08171", "abs": "https://arxiv.org/abs/2506.08171", "authors": ["Daniel Koh", "Yannic Noller", "Corina S. Pasareanu", "Adrians Skapars", "Youcheng Sun"], "title": "Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been successfully applied to a variety of\ncoding tasks, including code generation, completion, and repair. However, more\ncomplex symbolic reasoning tasks remain largely unexplored by LLMs. This paper\ninvestigates the capacity of LLMs to reason about worst-case executions in\nprograms through symbolic constraints analysis, aiming to connect LLMs and\nsymbolic reasoning approaches. Specifically, we define and address the problem\nof worst-case symbolic constraints analysis as a measure to assess the\ncomprehension of LLMs. We evaluate the performance of existing LLMs on this\nnovel task and further improve their capabilities through symbolic\nreasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)\nconstraint solving and supported by a specially designed dataset of symbolic\nconstraints. Experimental results show that our solver-aligned model,\nWARP-1.0-3B, consistently surpasses size-matched and even much larger\nbaselines, demonstrating that a 3B LLM can recover the very constraints that\npin down an algorithm's worst-case behaviour through reinforcement learning\nmethods. These findings suggest that LLMs are capable of engaging in deeper\nsymbolic reasoning, supporting a closer integration between neural\nnetwork-based learning and formal methods for rigorous program analysis."}
{"id": "2506.08173", "pdf": "https://arxiv.org/pdf/2506.08173", "abs": "https://arxiv.org/abs/2506.08173", "authors": ["Nguyen Phu Vinh", "Anh Chung Hoang", "Chris Ngo", "Truong-Son Hy"], "title": "Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong capabilities in code\ngeneration and comprehension, yet their application to complex software\nengineering tasks often suffers from low precision and limited\ninterpretability. We present Repeton, a fully open-source framework that\nleverages LLMs for precise and automated code manipulation in real-world Git\nrepositories. Rather than generating holistic fixes, Repeton operates through a\nstructured patch-and-test pipeline: it iteratively diagnoses issues, proposes\ncode changes, and validates each patch through automated testing. This stepwise\nprocess is guided by lightweight heuristics and development tools, avoiding\nreliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite\nbenchmark, our method shows good performance compared to RAG-based methods in\nboth patch validity and interpretability. By decomposing software engineering\ntasks into modular, verifiable stages, Repeton provides a practical path toward\nscalable and transparent autonomous debugging."}
{"id": "2506.08200", "pdf": "https://arxiv.org/pdf/2506.08200", "abs": "https://arxiv.org/abs/2506.08200", "authors": ["Kat R. Agres", "Adyasha Dash", "Phoebe Chua", "Stefan K. Ehrlich"], "title": "AffectMachine-Pop: A controllable expert system for real-time pop music generation", "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Music is a powerful medium for influencing listeners' emotional states, and\nthis capacity has driven a surge of research interest in AI-based affective\nmusic generation in recent years. Many existing systems, however, are a black\nbox which are not directly controllable, thus making these systems less\nflexible and adaptive to users. We present \\textit{AffectMachine-Pop}, an\nexpert system capable of generating retro-pop music according to arousal and\nvalence values, which can either be pre-determined or based on a listener's\nreal-time emotion states. To validate the efficacy of the system, we conducted\na listening study demonstrating that AffectMachine-Pop is capable of generating\naffective music at target levels of arousal and valence. The system is tailored\nfor use either as a tool for generating interactive affective music based on\nuser input, or for incorporation into biofeedback or neurofeedback systems to\nassist users with emotion self-regulation."}
{"id": "2506.08238", "pdf": "https://arxiv.org/pdf/2506.08238", "abs": "https://arxiv.org/abs/2506.08238", "authors": ["Parosh Abdulla", "Elli Anastasiadi", "Mohamed Faouzi Atig", "Samuel Grahn"], "title": "Verification of the Release-Acquire Semantics", "categories": ["cs.PL"], "comment": null, "summary": "The Release-Acquire (RA) semantics and its variants are some of the most\nfundamental models of concurrent semantics for architectures, programming\nlanguages, and distributed systems. Several steps have been taken in the\ndirection of testing such semantics, where one is interested in whether a\nsingle program execution is consistent with a memory model. The more general\nverification problem, i.e., checking whether all allowed program runs are\nconsistent with a memory model, has still not been studied as much. The purpose\nof this work is to bridge this gap. We tackle the verification problem, where,\ngiven an implementation described as a register machine, we check if any of its\nruns violates the RA semantics or its Strong (SRA) and Weak (WRA) variants. We\nshow that verifying WRA in this setup is in O([)n5 ], while verifying the RA\nand SRA is in both NP- and coNP-hard, and provide a PSPACE upper bound. This\nboth answers some fundamental questions about the complexity of these problems,\nbut also provides insights on the expressive power of register machines as a\nmodel."}
{"id": "2506.08043", "pdf": "https://arxiv.org/pdf/2506.08043", "abs": "https://arxiv.org/abs/2506.08043", "authors": ["Ashkan Shahbazi", "Kyvia Pereira", "Jon S. Heiselman", "Elaheh Akbari", "Annie C. Benson", "Sepehr Seifi", "Xinyuan Liu", "Garrison L. Johnston", "Erwin Terpstra", "Anne Draaisma", "Jan-Jaap Severes", "Jie Ying Wu", "Nabil Simaan", "Michael L. Miga", "Soheil Kolouri"], "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Fast and accurate simulation of soft tissue deformation is a critical factor\nfor surgical robotics and medical training. In this paper, we introduce a novel\nphysics-informed neural simulator that approximates soft tissue deformations in\na realistic and real-time manner. Our framework integrates Kelvinlet-based\npriors into neural simulators, making it the first approach to leverage\nKelvinlets for residual learning and regularization in data-driven soft tissue\nmodeling. By incorporating large-scale Finite Element Method (FEM) simulations\nof both linear and nonlinear soft tissue responses, our method improves neural\nnetwork predictions across diverse architectures, enhancing accuracy and\nphysical consistency while maintaining low latency for real-time performance.\nWe demonstrate the effectiveness of our approach by performing accurate\nsurgical maneuvers that simulate the use of standard laparoscopic tissue\ngrasping tools with high fidelity. These results establish Kelvinlet-augmented\nlearning as a powerful and efficient strategy for real-time, physics-aware soft\ntissue simulation in surgical applications."}
{"id": "2506.08132", "pdf": "https://arxiv.org/pdf/2506.08132", "abs": "https://arxiv.org/abs/2506.08132", "authors": ["Erfan Nosrati", "Majid Ghaderi"], "title": "Congestion-Aware Path Selection for Load Balancing in AI Clusters", "categories": ["cs.NI"], "comment": null, "summary": "Fast training of large machine learning models requires distributed training\non AI clusters consisting of thousands of GPUs. The efficiency of distributed\ntraining crucially depends on the efficiency of the network interconnecting\nGPUs in the cluster. These networks are commonly built using RDMA following a\nClos-like datacenter topology. To efficiently utilize the network bandwidth,\nload balancing is employed to distribute traffic across multiple redundant\npaths. While there exists numerous techniques for load-balancing in traditional\ndatacenters, these are often either optimized for TCP traffic or require\nspecialized network hardware, thus limiting their utility in AI clusters.\n  This paper presents the design and evaluation of Hopper, a new load-balancing\ntechnique optimized for RDMA traffic in AI clusters. Operating entirely at the\nhost level, Hopper requires no specialized hardware or modifications to network\nswitches. It continuously monitors the current path for congestion and\ndynamically switches traffic to a less congested path when congestion is\ndetected. Furthermore, it incorporates a lightweight mechanism to identify\nalternative paths and carefully controls the timing of path switching to\nprevent excessive out-of-order packets.\n  We evaluated Hopper using ns-3 simulations and a testbed implementation. Our\nevaluations show that Hopper reduces the average and 99-percentile tail flow\ncompletion time by up to 20% and 14%, respectively, compared to\nstate-of-the-art host-based load balancing techniques."}
{"id": "2506.08200", "pdf": "https://arxiv.org/pdf/2506.08200", "abs": "https://arxiv.org/abs/2506.08200", "authors": ["Kat R. Agres", "Adyasha Dash", "Phoebe Chua", "Stefan K. Ehrlich"], "title": "AffectMachine-Pop: A controllable expert system for real-time pop music generation", "categories": ["cs.HC", "cs.MM"], "comment": null, "summary": "Music is a powerful medium for influencing listeners' emotional states, and\nthis capacity has driven a surge of research interest in AI-based affective\nmusic generation in recent years. Many existing systems, however, are a black\nbox which are not directly controllable, thus making these systems less\nflexible and adaptive to users. We present \\textit{AffectMachine-Pop}, an\nexpert system capable of generating retro-pop music according to arousal and\nvalence values, which can either be pre-determined or based on a listener's\nreal-time emotion states. To validate the efficacy of the system, we conducted\na listening study demonstrating that AffectMachine-Pop is capable of generating\naffective music at target levels of arousal and valence. The system is tailored\nfor use either as a tool for generating interactive affective music based on\nuser input, or for incorporation into biofeedback or neurofeedback systems to\nassist users with emotion self-regulation."}
{"id": "2506.08231", "pdf": "https://arxiv.org/pdf/2506.08231", "abs": "https://arxiv.org/abs/2506.08231", "authors": ["Melissa Estevez", "Nisha Singh", "Lauren Dyson", "Blythe Adamson", "Qianyu Yuan", "Megan W. Hildner", "Erin Fidyk", "Olive Mbah", "Farhad Khan", "Kathi Seidl-Rathkopf", "Aaron B. Cohen"], "title": "Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": "18 pages, 3 tables, 1 figure", "summary": "Large language models (LLMs) are increasingly used to extract clinical data\nfrom electronic health records (EHRs), offering significant improvements in\nscalability and efficiency for real-world data (RWD) curation in oncology.\nHowever, the adoption of LLMs introduces new challenges in ensuring the\nreliability, accuracy, and fairness of extracted data, which are essential for\nresearch, regulatory, and clinical applications. Existing quality assurance\nframeworks for RWD and artificial intelligence do not fully address the unique\nerror modes and complexities associated with LLM-extracted data. In this paper,\nwe propose a comprehensive framework for evaluating the quality of clinical\ndata extracted by LLMs. The framework integrates variable-level performance\nbenchmarking against expert human abstraction, automated verification checks\nfor internal consistency and plausibility, and replication analyses comparing\nLLM-extracted data to human-abstracted datasets or external standards. This\nmultidimensional approach enables the identification of variables most in need\nof improvement, systematic detection of latent errors, and confirmation of\ndataset fitness-for-purpose in real-world research. Additionally, the framework\nsupports bias assessment by stratifying metrics across demographic subgroups.\nBy providing a rigorous and transparent method for assessing LLM-extracted RWD,\nthis framework advances industry standards and supports the trustworthy use of\nAI-powered evidence generation in oncology research and practice."}
{"id": "2506.08233", "pdf": "https://arxiv.org/pdf/2506.08233", "abs": "https://arxiv.org/abs/2506.08233", "authors": ["André Platzer", "Long Qian"], "title": "Approximate Axiomatization for Differentially-Defined Functions", "categories": ["cs.LO", "math.LO", "03B70, 03F03, 65L70, 65G20", "F.4.1; F.3.1; G.1.7; I.2.3"], "comment": null, "summary": "This article establishes a complete approximate axiomatization for the\nreal-closed field $\\mathbb{R}$ expanded with all differentially-defined\nfunctions, including special functions such as $\\sin(x), \\cos(x), e^x, \\dots$.\nEvery true sentence is provable up to some numerical approximation, and the\ntruth of such approximations converge under mild conditions. Such an\naxiomatization is a fragment of the axiomatization for differential dynamic\nlogic, and is therefore a finite extension of the axiomatization of real-closed\nfields. Furthermore, the numerical approximations approximate formulas\ncontaining special function symbols by $\\text{FOL}_{\\mathbb{R}}$ formulas,\nimproving upon earlier decidability results only concerning closed sentences."}
{"id": "2506.08528", "pdf": "https://arxiv.org/pdf/2506.08528", "abs": "https://arxiv.org/abs/2506.08528", "authors": ["Yu Guan", "Zhiyu Yin", "Haoyu Chen", "Sheng Cheng", "Chaojie Yang", "Tianyin Xu", "Yang Zhang", "Hanyu Zhao", "Yong Li", "Dennis Cai", "Ennan Zhai"], "title": "PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production", "categories": ["cs.DC", "cs.LG", "cs.OS"], "comment": null, "summary": "Troubleshooting performance problems of large model training (LMT) is\nimmensely challenging, due to unprecedented scales of modern GPU clusters, the\ncomplexity of software-hardware interactions, and the data intensity of the\ntraining process. Existing troubleshooting approaches designed for traditional\ndistributed systems or datacenter networks fall short and can hardly apply to\nreal-world training systems. In this paper, we present PerfTracker, the first\nonline troubleshooting system utilizing fine-grained profiling, to diagnose\nperformance issues of large-scale model training in production. PerfTracker can\ndiagnose performance issues rooted in both hardware (e.g., GPUs and their\ninterconnects) and software (e.g., Python functions and GPU operations). It\nscales to LMT on modern GPU clusters. PerfTracker effectively summarizes\nruntime behavior patterns of fine-grained LMT functions via online profiling,\nand leverages differential observability to localize the root cause with\nminimal production impact. PerfTracker has been deployed as a production\nservice for large-scale GPU clusters of O(10, 000) GPUs (product homepage\nhttps://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).\nIt has been used to diagnose a variety of difficult performance issues."}
{"id": "2506.08461", "pdf": "https://arxiv.org/pdf/2506.08461", "abs": "https://arxiv.org/abs/2506.08461", "authors": ["Sungwoong Yune", "Hyojeong Lee", "Adiwena Putra", "Hyunjun Cho", "Cuong Duong Manh", "Jaeho Jeon", "Joo-Young Kim"], "title": "ABC-FHE : A Resource-Efficient Accelerator Enabling Bootstrappable Parameters for Client-Side Fully Homomorphic Encryption", "categories": ["cs.AR", "cs.CR", "cs.ET"], "comment": "7 pages, 6 figures, DAC 2025: 62st IEEE/ACM Design Automation\n  Conference. (DAC'25)", "summary": "As the demand for privacy-preserving computation continues to grow, fully\nhomomorphic encryption (FHE)-which enables continuous computation on encrypted\ndata-has become a critical solution. However, its adoption is hindered by\nsignificant computational overhead, requiring 10000-fold more computation\ncompared to plaintext processing. Recent advancements in FHE accelerators have\nsuccessfully improved server-side performance, but client-side computations\nremain a bottleneck, particularly under bootstrappable parameter\nconfigurations, which involve combinations of encoding, encrypt, decoding, and\ndecrypt for large-sized parameters. To address this challenge, we propose\nABC-FHE, an area- and power-efficient FHE accelerator that supports\nbootstrappable parameters on the client side. ABC-FHE employs a streaming\narchitecture to maximize performance density, minimize area usage, and reduce\noff-chip memory access. Key innovations include a reconfigurable Fourier engine\ncapable of switching between NTT and FFT modes. Additionally, an on-chip\npseudo-random number generator and a unified on-the-fly twiddle factor\ngenerator significantly reduce memory demands, while optimized task scheduling\nenhances the CKKS client-side processing, achieving reduced latency. Overall,\nABC-FHE occupies a die area of 28.638 mm2 and consumes 5.654 W of power in 28\nnm technology. It delivers significant performance improvements, achieving a\n1112x speed-up in encoding and encryption execution time compared to a CPU, and\n214x over the state-of-the-art client-side accelerator. For decoding and\ndecryption, it achieves a 963x speed-up over the CPU and 82x over the\nstate-of-the-art accelerator."}
{"id": "2506.08249", "pdf": "https://arxiv.org/pdf/2506.08249", "abs": "https://arxiv.org/abs/2506.08249", "authors": ["Ken Gu", "Zhihan Zhang", "Kate Lin", "Yuwei Zhang", "Akshay Paruchuri", "Hong Yu", "Mehran Kazemi", "Kumar Ayush", "A. Ali Heydari", "Maxwell A. Xu", "Girish Narayanswamy", "Yun Liu", "Ming-Zher Poh", "Yuzhe Yang", "Mark Malhotra", "Shwetak Patel", "Hamid Palangi", "Xuhai Xu", "Daniel McDuff", "Tim Althoff", "Xin Liu"], "title": "RADAR: Benchmarking Language Models on Imperfect Tabular Data", "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "Language models (LMs) are increasingly being deployed to perform autonomous\ndata analyses. However, their data awareness -- the ability to recognize,\nreason over, and appropriately handle data artifacts such as missing values,\noutliers, and logical inconsistencies -- remains underexplored. These artifacts\nare especially common in real-world tabular data and, if mishandled, can\nsignificantly compromise the validity of analytical conclusions. To address\nthis gap, we present RADAR, a benchmark for systematically evaluating\ndata-aware reasoning on tabular data. We develop a framework to simulate data\nartifacts via programmatic perturbations to enable targeted evaluation of model\nbehavior. RADAR comprises 2980 table query pairs, grounded in real-world data\nspanning 9 domains and 5 data artifact types. In addition to evaluating\nartifact handling, RADAR systematically varies table size to study how\nreasoning performance holds when increasing table size. Our evaluation reveals\nthat, despite decent performance on tables without data artifacts, frontier\nmodels degrade significantly when data artifacts are introduced, exposing\ncritical gaps in their capacity for robust, data-aware analysis. Designed to be\nflexible and extensible, RADAR supports diverse perturbation types and\ncontrollable table sizes, offering a valuable resource for advancing tabular\nreasoning."}
{"id": "2506.08053", "pdf": "https://arxiv.org/pdf/2506.08053", "abs": "https://arxiv.org/abs/2506.08053", "authors": ["Yuhao Lian", "Xiao Han", "Xinmao Deng"], "title": "Power Domain Sparse Dimensional Constellation Multiple Access (PD-SDCMA) for Enabled Flexible PONs", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.16271 by\n  other authors", "summary": "With the commercial deployment of 5G and the in-depth research of 6G, the\ndemand for high-speed data services in the next-generation fiber optic access\nsystems is growing increasingly. Passive optical networks (PONs) have become a\nresearch hotspot due to their characteristics of low loss, high bandwidth, and\nlow cost. However, the traditional orthogonal multiple access (OMA-PON) has\ndifficulty meeting the requirements of the next-generation PON for high\nspectral efficiency and flexibility. In this paper, a novel transmission\ntechnology, namely power-domain sparse dimension constellation multiple access\n(PD-SDCMA), is proposed for the first time. Through the signal space dimension\nselection strategy (S2D-strategy) in the high-dimensional signal space, the\nlow-dimensional constellation is sparsely superimposed into the\nhigh-dimensional space, thereby reducing multi-user interference and enhancing\nthe system capacity. PD-SDCMA supports higher-order modulation formats and more\naccess groups, and is also compatible with the existing orthogonal frequency\ndivision multiplexing (OFDM) architecture. The simulation results show that in\na 25 km single-mode fiber system, compared with PD-NOMA and 3D-NOMA, PD-SDCMA\ncan support more users and significantly reduce BER. This technology provides\nan efficient and low-cost solution for the evolution of Flexible PONs."}
{"id": "2506.08179", "pdf": "https://arxiv.org/pdf/2506.08179", "abs": "https://arxiv.org/abs/2506.08179", "authors": ["Sasidhar Matta", "Vahid Garousi"], "title": "MBTModelGenerator: A software tool for reverse engineering of Model-based Testing (MBT) models from clickstream data of web applications", "categories": ["cs.SE"], "comment": null, "summary": "Automated testing has become a standard practice in software engineering, yet\nthe creation of test models and suites remains labor-intensive. To reduce this\neffort, we developed an open-source tool that automatically generates\nModel-Based Testing (MBT) models from clickstream data collected during user\ninteraction with web applications. The tool captures UI events, transforms them\ninto state-transition models, and exports the result in a format compatible\nwith the GraphWalker MBT tool. This enables immediate test execution without\nthe need for manual model creation. The approach lowers the barrier to MBT\nadoption by leveraging actual usage behavior and reducing the reliance on\nupfront modeling. This technical report documents the system requirements,\ndesign decisions, implementation details, testing process, and empirical\nevaluation of the tool, which is publicly available as open-source."}
{"id": "2506.08493", "pdf": "https://arxiv.org/pdf/2506.08493", "abs": "https://arxiv.org/abs/2506.08493", "authors": ["Qilin Yin", "Wei Lu", "Xiangyang Luo", "Xiaochun Cao"], "title": "Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Most research efforts in the multimedia forensics domain have focused on\ndetecting forgery audio-visual content and reached sound achievements. However,\nthese works only consider deepfake detection as a classification task and\nignore the case where partial segments of the video are tampered with. Temporal\nforgery localization (TFL) of small fake audio-visual clips embedded in real\nvideos is still challenging and more in line with realistic application\nscenarios. To resolve this issue, we propose a universal context-aware\ncontrastive learning framework (UniCaCLF) for TFL. Our approach leverages\nsupervised contrastive learning to discover and identify forged instants by\nmeans of anomaly detection, allowing for the precise localization of temporal\nforged segments. To this end, we propose a novel context-aware perception layer\nthat utilizes a heterogeneous activation operation and an adaptive context\nupdater to construct a context-aware contrastive objective, which enhances the\ndiscriminability of forged instant features by contrasting them with genuine\ninstant features in terms of their distances to the global context. An\nefficient context-aware contrastive coding is introduced to further push the\nlimit of instant feature distinguishability between genuine and forged instants\nin a supervised sample-by-sample manner, suppressing the cross-sample influence\nto improve temporal forgery localization performance. Extensive experimental\nresults over five public datasets demonstrate that our proposed UniCaCLF\nsignificantly outperforms the state-of-the-art competing algorithms."}
{"id": "2506.08396", "pdf": "https://arxiv.org/pdf/2506.08396", "abs": "https://arxiv.org/abs/2506.08396", "authors": ["Lifan Hu"], "title": "Linguine: A Natural-Language Programming Language with Formal Semantics and a Clean Compiler Pipeline", "categories": ["cs.PL"], "comment": null, "summary": "Linguine is a natural-language-inspired programming language that enables\nusers to write programs in a fluent, controlled subset of English while\npreserving formal semantics. The language introduces anaphoric constructs, such\nas pronoun variables (e.g., \"it\", \"them\"), that are statically resolved through\nreferent-tracking analysis combined with a Hindley-Milner-style type system.\nEach pronoun is guaranteed to be unambiguous and well-typed at compile time.\n  The Linguine compiler pipeline includes lexing, parsing, clause graph\nconstruction, desugaring into a typed intermediate representation, type\ninference, and abstract interpretation. This enables the early detection of\nsemantic errors, such as undefined or type-inconsistent references. A\nlightweight backend currently generates Python code.\n  This paper formalizes the core language, defines its typing and operational\nsemantics, and proves the soundness of its pronoun resolution mechanism. An\ninitial evaluation shows that Linguine allows the expression of concise and\nreadable programs while supporting static verification.\n  Linguine represents a step toward programming systems that prioritize human\nlinguistic intuition while remaining grounded in formal methods and\ntype-theoretic rigor."}
{"id": "2506.08064", "pdf": "https://arxiv.org/pdf/2506.08064", "abs": "https://arxiv.org/abs/2506.08064", "authors": ["Livio Tenze", "Enrique Canessa"], "title": "A Real-time 3D Desktop Display", "categories": ["cs.GR", "cs.CV"], "comment": "10 pages, 5 figures", "summary": "A new extended version of the altiro3D C++ Library -- initially developed to\nget glass-free holographic displays starting from 2D images -- is here\nintroduced aiming to deal with 3D video streams from either 2D webcam images or\nflat video files. These streams are processed in real-time to synthesize\nlight-fields (in Native format) and feed realistic 3D experiences. The core\nfunction needed to recreate multiviews consists on the use of MiDaS\nConvolutional Neural Network (CNN), which allows to extract a depth map from a\nsingle 2D image. Artificial Intelligence (AI) computing techniques are applied\nto improve the overall performance of the extended altiro3D Library. Thus,\naltiro3D can now treat standard images, video streams or screen portions of a\nDesktop where other apps may be also running (like web browsers, video chats,\netc) and render them into 3D. To achieve the latter, a screen region need to be\nselected in order to feed the output directly into a light-field 3D device such\nas Looking Glass (LG) Portrait. In order to simplify the acquisition of a\nDesktop screen area by the user, a multi-platform Graphical User Interface has\nbeen also implemented. Sources available at:\nhttps://github.com/canessae/altiro3D/releases/tag/2.0.0"}
{"id": "2506.08386", "pdf": "https://arxiv.org/pdf/2506.08386", "abs": "https://arxiv.org/abs/2506.08386", "authors": ["Matteo Bordin", "Madhukara S. Holla", "Sakthivel Velumani", "Salvatore D'Oro", "Tommaso Melodia"], "title": "5G Aero: A Prototyping Platform for Evaluating Aerial 5G Communications", "categories": ["cs.NI", "eess.SP"], "comment": "IEEE International Symposium on Personal, Indoor and Mobile Radio\n  Communications (PIMRC) 2025 - Track 3: Mobile and Wireless Networks", "summary": "The application of small-factor, 5G-enabled Unmanned Aerial Vehicles (UAVs)\nhas recently gained significant interest in various aerial and Industry 4.0\napplications. However, ensuring reliable, high-throughput, and low-latency 5G\ncommunication in aerial applications remains a critical and underexplored\nproblem. This paper presents the 5th generation (5G) Aero, a compact UAV\noptimized for 5G connectivity, aimed at fulfilling stringent 3rd Generation\nPartnership Project (3GPP) requirements. We conduct a set of experiments in an\nindoor environment, evaluating the UAV's ability to establish high-throughput,\nlow-latency communications in both Line-of-Sight (LoS) and Non-Line-of-Sight\n(NLoS) conditions. Our findings demonstrate that the 5G Aero meets the required\n3GPP standards for Command and Control (C2) packets latency in both LoS and\nNLoS, and video latency in LoS communications and it maintains acceptable\nlatency levels for video transmission in NLoS conditions. Additionally, we show\nthat the 5G module installed on the UAV introduces a negligible 1% decrease in\nflight time, showing that 5G technologies can be integrated into commercial\noff-the-shelf UAVs with minimal impact on battery lifetime. This paper\ncontributes to the literature by demonstrating the practical capabilities of\ncurrent 5G networks to support advanced UAV operations in telecommunications,\noffering insights into potential enhancements and optimizations for UAV\nperformance in 5G networks"}
{"id": "2506.08294", "pdf": "https://arxiv.org/pdf/2506.08294", "abs": "https://arxiv.org/abs/2506.08294", "authors": ["Ruanqianqian Huang", "Ayana Monroe", "Peli de Halleux", "Sorin Lerner", "Nikolaj Bjørner"], "title": "Z3Guide: A Scalable, Student-Centered, and Extensible Educational Environment for Logic Modeling", "categories": ["cs.HC", "cs.LO"], "comment": null, "summary": "Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from\nbudgeting for grocery shopping to verifying software behavior. Logic modeling\nhelps solve CSPs programmatically using SMT solvers. Despite its importance in\nmany Computer Science disciplines, resources for teaching and learning logic\nmodeling are scarce and scattered, and challenges remain in designing\neducational environments for logic modeling that are accessible and meet the\nneeds of teachers and students. This paper explores how to design such an\nenvironment and probes the impact of the design on the learning experience.\nFrom a need-finding interview study and a design iteration with teachers of\nlogic modeling, we curated 10 design guidelines spanning three main\nrequirements: providing easy access, supporting various educational modalities,\nand allowing extensions for customized pedagogical needs. We implemented nine\nguidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a\nlogic modeling learning workshop with more than 100 students, we gathered\npositive feedback on its support for learning and identified opportunities for\nfuture improvements."}
{"id": "2506.08437", "pdf": "https://arxiv.org/pdf/2506.08437", "abs": "https://arxiv.org/abs/2506.08437", "authors": ["Chris Chen", "Annabelle McIver", "Carroll Morgan"], "title": "Forward and Backward Simulations for Partially Observable Probability", "categories": ["cs.LO"], "comment": null, "summary": "Data refinement is the standard extension of a refinement relation from\nprograms to datatypes (i.e. a behavioural subtyping relation). Forward/backward\nsimulations provide a tractable method for establishing data refinement, and\nhave been thoroughly studied for nondeterministic programs. However, for\nstandard models of mixed probability and nondeterminism, ordinary assignment\nstatements may not commute with (variable-disjoint) program fragments. This (1)\ninvalidates a key assumption underlying the soundness of simulations, and (2)\nprevents modelling probabilistic datatypes with encapsulated state.\n  We introduce a weakest precondition semantics for Kuifje$_\\sqcap$, a language\nfor partially observable Markov decision processes, using so-called loss\n(function) transformers. We prove soundness of forward/backward simulations in\nthis richer setting, modulo healthiness conditions with a remarkable duality:\nforward simulations cannot leak information, and backward simulations cannot\nexploit leaked information."}
{"id": "2506.08597", "pdf": "https://arxiv.org/pdf/2506.08597", "abs": "https://arxiv.org/abs/2506.08597", "authors": ["H. Omidi", "L. Sacco", "V. Hutter", "G. Irsiegler", "M. Claus", "M. Schobben", "A. Jacob", "M. Schramm", "S. Fiore"], "title": "Towards Provenance-Aware Earth Observation Workflows: the openEO Case Study", "categories": ["cs.DC"], "comment": null, "summary": "Capturing the history of operations and activities during a computational\nworkflow is significantly important for Earth Observation (EO). The data\nprovenance helps to collect the metadata that records the lineage of data\nproducts, providing information about how data are generated, transferred,\nmanipulated, by whom all these operations are performed and through which\nprocesses, parameters, and datasets. This paper presents an approach to improve\nthose aspects, by integrating the data provenance library yProv4WFs within\nopenEO, a platform to let users connect to Earth Observation cloud back-ends in\na simple and unified way. In addition, it is demonstrated how the integration\nof data provenance concepts across EO processing chains enables researchers and\nstakeholders to better understand the flow, the dependencies, and the\ntransformations involved in analytical workflows."}
{"id": "2506.08496", "pdf": "https://arxiv.org/pdf/2506.08496", "abs": "https://arxiv.org/abs/2506.08496", "authors": ["Jiale Dong", "Hao Wu", "Zihao Wang", "Wenqi Lou", "Zhendong Zheng", "Lei Gong", "Chao Wang", "Xuehai Zhou"], "title": "CoQMoE: Co-Designed Quantization and Computation Orchestration for Mixture-of-Experts Vision Transformer on FPGA", "categories": ["cs.AR"], "comment": "Accepted by Euro-Par 2025 (oral)", "summary": "Vision Transformers (ViTs) exhibit superior performance in computer vision\ntasks but face deployment challenges on resource-constrained devices due to\nhigh computational/memory demands. While Mixture-of-Experts Vision Transformers\n(MoE-ViTs) mitigate this through a scalable architecture with sub-linear\ncomputational growth, their hardware implementation on FPGAs remains\nconstrained by resource limitations. This paper proposes a novel accelerator\nfor efficiently implementing quantized MoE models on FPGAs through two key\ninnovations: (1) A dual-stage quantization scheme combining\nprecision-preserving complex quantizers with hardware-friendly simplified\nquantizers via scale reparameterization, with only 0.28 $\\%$ accuracy loss\ncompared to full precision; (2) A resource-aware accelerator architecture\nfeaturing latency-optimized streaming attention kernels and reusable linear\noperators, effectively balancing performance and resource consumption.\nExperimental results demonstrate that our accelerator achieves nearly 155\nframes per second, a 5.35$\\times$ improvement in throughput, and over $80\\%$\nenergy reduction compared to state-of-the-art (SOTA) FPGA MoE accelerators,\nwhile maintaining $<1\\%$ accuracy loss across vision benchmarks. Our\nimplementation is available at https://github.com/DJ000011/CoQMoE."}
{"id": "2506.08276", "pdf": "https://arxiv.org/pdf/2506.08276", "abs": "https://arxiv.org/abs/2506.08276", "authors": ["Yichuan Wang", "Shu Liu", "Zhifei Li", "Yongji Wu", "Ziming Mao", "Yilong Zhao", "Xiao Yan", "Zhiying Xu", "Yang Zhou", "Ion Stoica", "Sewon Min", "Matei Zaharia", "Joseph E. Gonzalez"], "title": "LEANN: A Low-Storage Vector Index", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Embedding-based search is widely used in applications such as recommendation\nand retrieval-augmented generation (RAG). Recently, there is a growing demand\nto support these capabilities over personal data stored locally on devices.\nHowever, maintaining the necessary data structure associated with the\nembedding-based search is often infeasible due to its high storage overhead.\nFor example, indexing 100 GB of raw data requires 150 to 700 GB of storage,\nmaking local deployment impractical. Reducing this overhead while maintaining\nsearch quality and latency becomes a critical challenge. In this paper, we\npresent LEANN, a storage-efficient approximate nearest neighbor (ANN) search\nindex optimized for resource-constrained personal devices. LEANN combines a\ncompact graph-based structure with an efficient on-the-fly recomputation\nstrategy to enable fast and accurate retrieval with minimal storage overhead.\nOur evaluation shows that LEANN reduces index size to under 5% of the original\nraw data, achieving up to 50 times smaller storage than standard indexes, while\nmaintaining 90% top-3 recall in under 2 seconds on real-world question\nanswering benchmarks."}
{"id": "2506.08702", "pdf": "https://arxiv.org/pdf/2506.08702", "abs": "https://arxiv.org/abs/2506.08702", "authors": ["Sankalan Pal Chowdhury", "Terry Jingchen Zhang", "Donya Rooein", "Dirk Hovy", "Tanja Käser", "Mrinmaya Sachan"], "title": "Educators' Perceptions of Large Language Models as Tutors: Comparing Human and AI Tutors in a Blind Text-only Setting", "categories": ["cs.ET"], "comment": "Accepted to BEA@ACL 2025", "summary": "The rapid development of Large Language Models (LLMs) opens up the\npossibility of using them as personal tutors. This has led to the development\nof several intelligent tutoring systems and learning assistants that use LLMs\nas back-ends with various degrees of engineering. In this study, we seek to\ncompare human tutors with LLM tutors in terms of engagement, empathy,\nscaffolding, and conciseness. We ask human tutors to annotate and compare the\nperformance of an LLM tutor with that of a human tutor in teaching grade-school\nmath word problems on these qualities. We find that annotators with teaching\nexperience perceive LLMs as showing higher performance than human tutors in all\n4 metrics. The biggest advantage is in empathy, where 80% of our annotators\nprefer the LLM tutor more often than the human tutors. Our study paints a\npositive picture of LLMs as tutors and indicates that these models can be used\nto reduce the load on human teachers in the future."}
{"id": "2506.08311", "pdf": "https://arxiv.org/pdf/2506.08311", "abs": "https://arxiv.org/abs/2506.08311", "authors": ["Ira Ceka", "Saurabh Pujar", "Shyam Ramji", "Luca Buratti", "Gail Kaiser", "Baishakhi Ray"], "title": "Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "With the advent of large language models (LLMs), software engineering agents\n(SWE agents) have emerged as a powerful paradigm for automating a range of\nsoftware tasks -- from code generation and repair to test case synthesis. These\nagents operate autonomously by interpreting user input and responding to\nenvironmental feedback. While various agent architectures have demonstrated\nstrong empirical performance, the internal decision-making worfklows that drive\ntheir behavior remain poorly understood. Deeper insight into these workflows\nhold promise for improving both agent reliability and efficiency. In this work,\nwe present the first systematic study of SWE agent behavior through the lens of\nexecution traces. Our contributions are as follows: (1) we propose the first\ntaxonomy of decision-making pathways across five representative agents; (2)\nusing this taxonomy, we identify three core components essential to agent\nsuccess -- bug localization, patch generation, and reproduction test generation\n-- and study each in depth; (3) we study the impact of test generation on\nsuccessful patch production; and analyze strategies that can lead to successful\ntest generation; (4) we further conduct the first large-scale code clone\nanalysis comparing agent-generated and developer-written patches and provide a\nqualitative study revealing structural and stylistic differences in patch\ncontent. Together, these findings offer novel insights into agent design and\nopen avenues for building agents that are both more effective and more aligned\nwith human development practices."}
{"id": "2506.08524", "pdf": "https://arxiv.org/pdf/2506.08524", "abs": "https://arxiv.org/abs/2506.08524", "authors": ["Weiguo Wang", "Andy Nie", "Wenrui Zhou", "Yi Kai", "Chengchen Hu"], "title": "Teaching Physical Awareness to LLMs through Sounds", "categories": ["cs.SD", "cs.AI", "cs.MM", "cs.RO", "eess.AS"], "comment": "ICML 2025", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in text and\nmultimodal processing, yet they fundamentally lack physical\nawareness--understanding of real-world physical phenomena. In this work, we\npresent ACORN, a framework that teaches LLMs physical awareness through sound,\nfocusing on fundamental physical phenomena like the Doppler effect, multipath\neffect, and spatial relationships. To overcome data scarcity, ACORN introduce a\nphysics-based simulator combining real-world sound sources with controlled\nphysical channels to generate diverse training data. Using this simulator, we\nbuild AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an\naudio encoder that processes both magnitude and phase information. By\nconnecting our audio encoder to state-of-the-art LLMs, we demonstrate\nreasonable results in both simulated and real-world tasks, such as\nline-of-sight detection, Doppler effect estimation, and Direction-of-Arrival\nestimation, paving the way for enabling LLMs to understand physical world."}
{"id": "2506.09043", "pdf": "https://arxiv.org/pdf/2506.09043", "abs": "https://arxiv.org/abs/2506.09043", "authors": ["Tianyu Chen", "Darshal Shetty", "Jeremy G. Siek", "Chao-Hong Chen", "Weixi Ma", "Arnaud Venet", "Rocky Liu"], "title": "Gradual Metaprogramming", "categories": ["cs.PL", "D.3"], "comment": "13 pages, 10 figures", "summary": "Data engineers increasingly use domain-specific languages (DSLs) to generate\nthe code for data pipelines. Such DSLs are often embedded in Python.\nUnfortunately, there are challenges in debugging the generation of data\npipelines: an error in a Python DSL script is often detected too late, after\nthe execution of the script, and the source code location that triggers the\nerror is hard to pinpoint.\n  In this paper, we focus on the F3 DSL of Meta (Facebook), which is a DSL\nembedded in Python (so it is dynamically-typed) to generate data pipeline\ndescription code that is statically-typed. We propose gradual metaprogramming\nto (1) provide a migration path toward statically typed DSLs, (2) immediately\nprovide earlier detection of code generation type errors, and (3) report the\nsource code location responsible for the type error. Gradual metaprogramming\naccomplishes this by type checking code fragments and incrementally performing\nruntime checks as they are spliced together. We define MetaGTLC, a\nmetaprogramming calculus in which a gradually-typed metalanguage manipulates a\nstatically-typed object language, and give semantics to it by translation to\nthe cast calculus MetaCC. We prove that successful metaevaluation always\ngenerates a well-typed object program and mechanize the proof in Agda."}
{"id": "2506.08161", "pdf": "https://arxiv.org/pdf/2506.08161", "abs": "https://arxiv.org/abs/2506.08161", "authors": ["Jakub Bokšanský", "Daniel Meister", "Carsten Benthin"], "title": "GATE: Geometry-Aware Trained Encoding", "categories": ["cs.GR"], "comment": null, "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."}
{"id": "2506.08408", "pdf": "https://arxiv.org/pdf/2506.08408", "abs": "https://arxiv.org/abs/2506.08408", "authors": ["Haoyang Wang", "Jingao Xu", "Chenyu Zhao", "Yuhan Cheng", "Xuecheng Chen", "Chaopeng Hong", "Xiao-Ping Zhang", "Yunhao Liu", "Xinlei Chen"], "title": "Aerial Shepherds: Enabling Hierarchical Localization in Heterogeneous MAV Swarms", "categories": ["cs.NI"], "comment": "18 pages", "summary": "A heterogeneous micro aerial vehicles (MAV) swarm consists of\nresource-intensive but expensive advanced MAVs (AMAVs) and resource-limited but\ncost-effective basic MAVs (BMAVs), offering opportunities in diverse fields.\nAccurate and real-time localization is crucial for MAV swarms, but current\npractices lack a low-cost, high-precision, and real-time solution, especially\nfor lightweight BMAVs. We find an opportunity to accomplish the task by\ntransforming AMAVs into mobile localization infrastructures for BMAVs. However,\ntranslating this insight into a practical system is challenging due to issues\nin estimating locations with diverse and unknown localization errors of BMAVs,\nand allocating resources of AMAVs considering interconnected influential\nfactors. This work introduces TransformLoc, a new framework that transforms\nAMAVs into mobile localization infrastructures, specifically designed for\nlow-cost and resource-constrained BMAVs. We design an error-aware joint\nlocation estimation model to perform intermittent joint estimation for BMAVs\nand introduce a similarity-instructed adaptive grouping-scheduling strategy to\nallocate resources of AMAVs dynamically. TransformLoc achieves a collaborative,\nadaptive, and cost-effective localization system suitable for large-scale\nheterogeneous MAV swarms. We implement and validate TransformLoc on industrial\ndrones. Results show it outperforms all baselines by up to 68\\% in localization\nperformance, improving navigation success rates by 60\\%. Extensive robustness\nand ablation experiments further highlight the superiority of its design."}
{"id": "2506.08303", "pdf": "https://arxiv.org/pdf/2506.08303", "abs": "https://arxiv.org/abs/2506.08303", "authors": ["Thomas M. Kwok", "Hilary HY Cheng", "Wai Tuck Chow"], "title": "EMG-Driven Stiffness-Modulating Palpation for Telerehabilitation", "categories": ["cs.HC"], "comment": "Accepted by the Workshop on Human-Robot Contact and Manipulation\n  (HRCM 2025) at RSS Conference 2025", "summary": "In this work, we introduce HJ-Pal, a lightweight wearable haptic device that\nleverages EMG-driven honeycomb jamming to render muscle activation as\nkinesthetic feedback, enabling remote palpation for small muscle assessment in\ntelerehabilitation."}
{"id": "2506.08525", "pdf": "https://arxiv.org/pdf/2506.08525", "abs": "https://arxiv.org/abs/2506.08525", "authors": ["Hannah Mertens", "Tim Quatmann", "Joost-Pieter Katoen"], "title": "Compositional Reasoning for Parametric Probabilistic Automata", "categories": ["cs.LO", "math.PR"], "comment": "Full version of a paper accepted for publication at CONCUR 2025", "summary": "We establish an assume-guarantee (AG) framework for compositional reasoning\nabout multi-objective queries in parametric probabilistic automata (pPA) - an\nextension to probabilistic automata (PA), where transition probabilities are\nfunctions over a finite set of parameters. We lift an existing framework for PA\nto the pPA setting, incorporating asymmetric, circular, and interleaving proof\nrules. Our approach enables the verification of a broad spectrum of\nmulti-objective queries for pPA, encompassing probabilistic properties and\n(parametric) expected total rewards. Additionally, we introduce a rule for\nreasoning about monotonicity in composed pPAs."}
{"id": "2506.08636", "pdf": "https://arxiv.org/pdf/2506.08636", "abs": "https://arxiv.org/abs/2506.08636", "authors": ["Zeinab Nezami", "Zhuolun Li", "Chuhao Qin", "Fatemeh Banaie", "Rabiya Khalid", "Evangelos Pournaras"], "title": "Blockchain and Edge Computing Nexus: A Large-scale Systematic Literature Review", "categories": ["cs.DC"], "comment": null, "summary": "Blockchain and edge computing are two instrumental paradigms of decentralized\ncomputation, driving key advancements in Smart Cities applications such as\nsupply chain, energy and mobility. Despite their unprecedented impact on\nsociety, they remain significantly fragmented as technologies and research\nareas, while they share fundamental principles of distributed systems and\ndomains of applicability. This paper introduces a novel and large-scale\nsystematic literature review on the nexus of blockchain and edge computing with\nthe aim to unravel a new understanding of how the interfacing of the two\ncomputing paradigms can boost innovation to provide solutions to timely but\nalso long-standing research challenges. By collecting almost 6000 papers from 3\ndatabases and putting under scrutiny almost 1000 papers, we build a novel\ntaxonomy and classification consisting of 22 features with 287 attributes that\nwe study using quantitative and machine learning methods. They cover a broad\nspectrum of technological, design, epistemological and sustainability aspects.\nResults reveal 4 distinguishing patterns of interplay between blockchain and\nedge computing with key determinants the public (permissionless) vs. private\n(permissioned) design, technology and proof of concepts. They also demonstrate\nthe prevalence of blockchain-assisted edge computing for improving privacy and\nsecurity, in particular for mobile computing applications."}
{"id": "2506.08785", "pdf": "https://arxiv.org/pdf/2506.08785", "abs": "https://arxiv.org/abs/2506.08785", "authors": ["Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration", "categories": ["cs.AR", "cs.AI", "cs.CC", "eess.IV"], "comment": null, "summary": "The increasing complexity of AI models requires flexible hardware capable of\nsupporting diverse precision formats, particularly for energy-constrained edge\nplatforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC\nengine that performs efficient multiply-accumulate operations using a unified\ndata-path for 4/8/16-bit fixed-point, floating point, and posit formats. The\narchitecture incorporates a layer adaptive precision strategy to align\ncomputational accuracy with workload sensitivity, optimizing both performance\nand energy usage. PARV-CE integrates quantization-aware execution with a\nreconfigurable SIMD pipeline, enabling high-throughput processing with minimal\noverhead through hardware-software co-design. The results demonstrate up to 2x\nimprovement in PDP and 3x reduction in resource usage compared to SoTA designs,\nwhile retaining accuracy within 1.8% FP32 baseline. The architecture supports\nboth on-device training and inference across a range of workloads, including\nDNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE\nincorporated POLARON as a scalable and energy-efficient solution for\nprecision-adaptive AI acceleration at edge."}
{"id": "2506.08671", "pdf": "https://arxiv.org/pdf/2506.08671", "abs": "https://arxiv.org/abs/2506.08671", "authors": ["Junfeng Liu", "Jiarui Ye", "Mengshi Chen", "Meng Li", "Siqiang Luo"], "title": "Evaluating Learned Indexes in LSM-tree Systems: Benchmarks,Insights and Design Choices", "categories": ["cs.DB", "cs.DS", "E.1"], "comment": "14 pages,12 figures", "summary": "LSM-tree-based data stores are widely used in industry due to their\nexceptional performance. However, as data volumes grow, efficiently querying\nlarge-scale databases becomes increasingly challenging. To address this, recent\nstudies attempted to integrate learned indexes into LSM-trees to enhance lookup\nperformance, which has demonstrated promising improvements. Despite this, only\na limited range of learned index types has been considered, and the strengths\nand weaknesses of different learned indexes remain unclear, making them\ndifficult for practical use. To fill this gap, we provide a comprehensive and\nsystematic benchmark to pursue an in-depth understanding of learned indexes in\nLSM-tree systems. In this work, we summarize the workflow of 8 existing learned\nindexes and analyze the associated theoretical cost. We also identify several\nkey factors that significantly influence the performance of learned indexes and\nconclude them with a novel configuration space, including various index types,\nboundary positions, and granularity. Moreover, we implement different learned\nindex designs on a unified platform to evaluate across various configurations.\nSurprisingly, our experiments reveal several unexpected insights, such as the\nmarginal lookup enhancement when allocating a large memory budget to learned\nindexes and modest retraining overhead of learned indexes. Besides, we also\noffer practical guidelines to help developers intelligently select and tune\nlearned indexes for custom use cases."}
{"id": "2506.06353", "pdf": "https://arxiv.org/pdf/2506.06353", "abs": "https://arxiv.org/abs/2506.06353", "authors": ["Naseem Babu", "Jimson Mathew", "A. P. Vinod"], "title": "Large Language Models for EEG: A Comprehensive Survey and Taxonomy", "categories": ["eess.SP", "cs.AI", "cs.ET", "cs.HC", "cs.LG"], "comment": null, "summary": "The growing convergence between Large Language Models (LLMs) and\nelectroencephalography (EEG) research is enabling new directions in neural\ndecoding, brain-computer interfaces (BCIs), and affective computing. This\nsurvey offers a systematic review and structured taxonomy of recent\nadvancements that utilize LLMs for EEG-based analysis and applications. We\norganize the literature into four domains: (1) LLM-inspired foundation models\nfor EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal\ngeneration including image and 3D object synthesis, and (4) clinical\napplications and dataset management tools. The survey highlights how\ntransformer-based architectures adapted through fine-tuning, few-shot, and\nzero-shot learning have enabled EEG-based models to perform complex tasks such\nas natural language generation, semantic interpretation, and diagnostic\nassistance. By offering a structured overview of modeling strategies, system\ndesigns, and application areas, this work serves as a foundational resource for\nfuture work to bridge natural language processing and neural signal analysis\nthrough language models."}
{"id": "2506.08561", "pdf": "https://arxiv.org/pdf/2506.08561", "abs": "https://arxiv.org/abs/2506.08561", "authors": ["Hao Wu", "Haijun Wang", "Shangwang Li", "Yin Wu", "Ming Fan", "Yitao Zhao", "Ting Liu"], "title": "Detecting State Manipulation Vulnerabilities in Smart Contracts Using LLM and Static Analysis", "categories": ["cs.SE"], "comment": null, "summary": "An increasing number of DeFi protocols are gaining popularity, facilitating\ntransactions among multiple anonymous users. State Manipulation is one of the\nnotorious attacks in DeFi smart contracts, with price variable being the most\ncommonly exploited state variable-attackers manipulate token prices to gain\nillicit profits. In this paper, we propose PriceSleuth, a novel method that\nleverages the Large Language Model (LLM) and static analysis to detect Price\nManipulation (PM) attacks proactively. PriceSleuth firstly identifies core\nlogic function related to price calculation in DeFi contracts. Then it guides\nLLM to locate the price calculation code statements. Secondly, PriceSleuth\nperforms backward dependency analysis of price variables, instructing LLM in\ndetecting potential price manipulation. Finally, PriceSleuth utilizes\npropagation analysis of price variables to assist LLM in detecting whether\nthese variables are maliciously exploited. We presented preliminary\nexperimental results to substantiate the effectiveness of PriceSleuth . And we\noutline future research directions for PriceSleuth."}
{"id": "2506.08591", "pdf": "https://arxiv.org/pdf/2506.08591", "abs": "https://arxiv.org/abs/2506.08591", "authors": ["Chengchao Shen", "Hourun Zhu", "Gongfan Fang", "Jianxin Wang", "Xinchao Wang"], "title": "Diversity-Guided MLP Reduction for Efficient Large Vision Transformers", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Transformer models achieve excellent scaling property, where the performance\nis improved with the increment of model capacity. However, large-scale model\nparameters lead to an unaffordable cost of computing and memory. We analyze\npopular transformer architectures and find that multilayer perceptron (MLP)\nmodules take up the majority of model parameters. To this end, we focus on the\nrecoverability of the compressed models and propose a Diversity-Guided MLP\nReduction (DGMR) method to significantly reduce the parameters of large vision\ntransformers with only negligible performance degradation. Specifically, we\nconduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons\nof MLP hidden layer, while preserving weight diversity for better performance\nrecover during distillation. Compared to the model trained from scratch, our\npruned model only requires 0.06\\% data of LAION-2B (for the training of large\nvision transformers) without labels (ImageNet-1K) to recover the original\nperformance. Experimental results on several state-of-the-art large vision\ntransformers demonstrate that our method achieves a more than 57.0\\% parameter\nand FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),\nour method accomplishes a 71.5\\% parameter and FLOPs reduction without\nperformance degradation. The source code and trained weights are available at\nhttps://github.com/visresearch/DGMR."}
{"id": "2506.08237", "pdf": "https://arxiv.org/pdf/2506.08237", "abs": "https://arxiv.org/abs/2506.08237", "authors": ["Bailey Miller", "Rohan Sawhney", "Keenan Crane", "Ioannis Gkioulekas"], "title": "Solving partial differential equations in participating media", "categories": ["cs.GR", "cs.NA", "math.NA"], "comment": "SIGGRAPH 2025. Project page\n  https://imaging.cs.cmu.edu/volumetric_walk_on_spheres", "summary": "We consider the problem of solving partial differential equations (PDEs) in\ndomains with complex microparticle geometry that is impractical, or\nintractable, to model explicitly. Drawing inspiration from volume rendering, we\npropose tackling this problem by treating the domain as a participating medium\nthat models microparticle geometry stochastically, through aggregate\nstatistical properties (e.g., particle density). We first introduce the problem\nsetting of PDE simulation in participating media. We then specialize to\nexponential media and describe the properties that make them an attractive\nmodel of microparticle geometry for PDE simulation problems. We use these\nproperties to develop two new algorithms, volumetric walk on spheres and\nvolumetric walk on stars, that generalize previous Monte Carlo algorithms to\nenable efficient and discretization-free simulation of linear elliptic PDEs\n(e.g., Laplace) in participating media. We demonstrate experimentally that our\nalgorithms can solve Laplace boundary value problems with complex microparticle\ngeometry more accurately and more efficiently than previous approaches, such as\nensemble averaging and homogenization."}
{"id": "2506.09039", "pdf": "https://arxiv.org/pdf/2506.09039", "abs": "https://arxiv.org/abs/2506.09039", "authors": ["Abderrahime Filali", "Diala Naboulsi", "Georges Kaddoum"], "title": "Deep Reinforcement Learning-Based RAN Slicing with Efficient Inter-Slice Isolation in Tactical Wireless Networks", "categories": ["cs.NI"], "comment": null, "summary": "The next generation of tactical networks (TNs) is poised to further leverage\nthe key enablers of 5G and beyond 5G (B5G) technology, such as radio access\nnetwork (RAN) slicing and the open RAN (O-RAN) paradigm, to unlock multiple\narchitectural options and opportunities for a wide range of innovative\napplications. RAN slicing and the O-RAN paradigm are considered game changers\nin TNs, where the former makes it possible to tailor user services to users\nrequirements, and the latter brings openness and intelligence to the management\nof the RAN. In TNs, bandwidth scarcity requires a dynamic bandwidth slicing\nstrategy. Although this type of strategy ensures efficient bandwidth\nutilization, it compromises RAN slicing isolation in terms of quality of\nservice (QoS) performance. To deal with this challenge, we propose a deep\nreinforcement learning (DRL)-based RAN slicing mechanism that achieves a\ntrade-off between efficient RAN bandwidth sharing and appropriate inter- and\nintra-slice isolation. The proposed mechanism performs bandwidth allocation in\ntwo stages. In the first stage, the bandwidth is allocated to the RAN slices.\nIn the second stage, each slice partitions its bandwidth among its associated\nusers. In both stages, the slicing operation is constrained by several\nconsiderations related to improving the QoS of slices and users that in turn\nfoster inter- and intra-slice isolation. The proposed RAN slicing mechanism is\nbased on DRL algorithms to perform the bandwidth sharing operation in each\nstage. We propose to deploy the mechanism in an O-RAN architecture and describe\nthe O-RAN functional blocks and the main DRL model lifecycle management phases\ninvolved. We also develop three different implementations of the proposed\nmechanism, each based on a different DRL algorithm, and evaluate their\nperformance against multiple baselines across various parameters."}
{"id": "2506.08443", "pdf": "https://arxiv.org/pdf/2506.08443", "abs": "https://arxiv.org/abs/2506.08443", "authors": ["Kazuki Kawamura", "Jun Rekimoto"], "title": "SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills", "categories": ["cs.HC", "cs.CV", "68T05", "H.5.2; K.3; I.2.7"], "comment": "5 pages, 1 figure; accepted as a paper to the Generative AI and HCI\n  (GenAICHI) workshop at CHI 2025 (Yokohama, 27 Apr 2025)", "summary": "While current AI illustration tools can generate high-quality images from\ntext prompts, they rarely reveal the step-by-step procedure that human artists\nfollow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based\nimage generation with a large-language-model tutor. At each stage, novices\nreceive real-time feedback on anatomy, perspective, and composition, revise any\nstep non-linearly, and branch alternative versions. By exposing intermediate\noutputs and embedding pedagogical dialogue, SakugaFlow turns a black-box\ngenerator into a scaffolded learning environment that supports both creative\nexploration and skills acquisition."}
{"id": "2506.08588", "pdf": "https://arxiv.org/pdf/2506.08588", "abs": "https://arxiv.org/abs/2506.08588", "authors": ["Liesbeth De Mol", "Yuri V. Matiyasevich", "Eugenio G. Omodeo", "Alberto Policriti", "Wilfried Sieg", "Elaine J. Weyuker"], "title": "Martin Davis: An Overview of his Work in Logic, Computer Science, and Philosophy", "categories": ["cs.LO"], "comment": null, "summary": "In his autobiographic essay written in 1999, ``From logic to computer science\nand back'', Martin David Davis (3/8/1928--1/1/2023) indicated that he viewed\nhimself as a logician \\emph{and} a computer scientist. He expanded the essay in\n2016 and expressed a new perspective through a changed title, ``My life as a\nlogician''. He points out that logic was the unifying theme underlying his\nscientific career. Our paper attempts to provide a consistent vision that\nilluminates Davis' successive contributions leading to his landmark writings on\ncomputability, unsolvable problems, automated reasoning, as well as the history\nand philosophy of computing."}
{"id": "2506.08653", "pdf": "https://arxiv.org/pdf/2506.08653", "abs": "https://arxiv.org/abs/2506.08653", "authors": ["Alexander Strack", "Christopher Taylor", "Dirk Pflüger"], "title": "Parallel FFTW on RISC-V: A Comparative Study including OpenMP, MPI, and HPX", "categories": ["cs.DC", "cs.ET"], "comment": "12 pages, 9 figures, Fifth International workshop on RISC-V for HPC\n  co-located with ISC 2025", "summary": "Rapid advancements in RISC-V hardware development shift the focus from\nlow-level optimizations to higher-level parallelization. Recent RISC-V\nprocessors, such as the SOPHON SG2042, have 64 cores. RISC-V processors with\ncore counts comparable to the SG2042, make efficient parallelization as crucial\nfor RISC-V as the more established processors such as x86-64. In this work, we\nevaluate the parallel scaling of the widely used FFTW library on RISC-V for MPI\nand OpenMP. We compare it to a 64-core AMD EPYC 7742 CPU side by side for\ndifferent types of FFTW planning. Additionally, we investigate the effect of\nmemory optimization on RISC-V in HPX-FFT, a parallel FFT library based on the\nasynchronous many-task runtime HPX using an FFTW backend. We generally observe\na performance delta between the x86-64 and RISC-V chips of factor eight for\ndouble-precision 2D FFT. Effective memory optimizations in HPX-FFT on x86-64 do\nnot translate to the RISC-V chip. FFTW with MPI shows good scaling up to 64\ncores on x86-64 and RISC-V regardless of planning. In contrast, FFTW with\nOpenMP requires measured planning on both architectures to achieve good scaling\nup to 64 cores. The results of our study mark an early step on the journey to\nlarge-scale parallel applications running on RISC-V."}
{"id": "2506.08842", "pdf": "https://arxiv.org/pdf/2506.08842", "abs": "https://arxiv.org/abs/2506.08842", "authors": ["Kainan Wang", "Chengyi Yang", "Chengting Yu", "Yee Sin Ang", "Bo Wang", "Aili Wang"], "title": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN Accelerator with Algorithm and Hardware Co-Design", "categories": ["cs.AR"], "comment": null, "summary": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ..."}
{"id": "2506.08743", "pdf": "https://arxiv.org/pdf/2506.08743", "abs": "https://arxiv.org/abs/2506.08743", "authors": ["Michael Färber", "David Lamprecht", "Yuni Susanti"], "title": "Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems", "categories": ["cs.IR", "cs.AI", "cs.DB", "cs.LG"], "comment": "Accepted at DASFAA 2025", "summary": "Graph Neural Networks (GNNs) have substantially advanced the field of\nrecommender systems. However, despite the creation of more than a thousand\nknowledge graphs (KGs) under the W3C standard RDF, their rich semantic\ninformation has not yet been fully leveraged in GNN-based recommender systems.\nTo address this gap, we propose a comprehensive integration of RDF KGs with\nGNNs that utilizes both the topological information from RDF object properties\nand the content information from RDF datatype properties. Our main focus is an\nin-depth evaluation of various GNNs, analyzing how different semantic feature\ninitializations and types of graph structure heterogeneity influence their\nperformance in recommendation tasks. Through experiments across multiple\nrecommendation scenarios involving multi-million-node RDF graphs, we\ndemonstrate that harnessing the semantic richness of RDF KGs significantly\nimproves recommender systems and lays the groundwork for GNN-based recommender\nsystems for the Linked Open Data cloud. The code and data are available on our\nGitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation"}
{"id": "2506.08350", "pdf": "https://arxiv.org/pdf/2506.08350", "abs": "https://arxiv.org/abs/2506.08350", "authors": ["Yicheng Zhan", "Dong-Ha Shin", "Seung-Hwan Baek", "Kaan Akşit"], "title": "Complex-Valued Holographic Radiance Fields", "categories": ["cs.GR", "cs.CV", "cs.ET"], "comment": "28 pages, 21 figures", "summary": "Modeling the full properties of light, including both amplitude and phase, in\n3D representations is crucial for advancing physically plausible rendering,\nparticularly in holographic displays. To support these features, we propose a\nnovel representation that optimizes 3D scenes without relying on\nintensity-based intermediaries. We reformulate 3D Gaussian splatting with\ncomplex-valued Gaussian primitives, expanding support for rendering with light\nwaves. By leveraging RGBD multi-view images, our method directly optimizes\ncomplex-valued Gaussians as a 3D holographic scene representation. This\neliminates the need for computationally expensive hologram re-optimization.\nCompared with state-of-the-art methods, our method achieves 30x-10,000x speed\nimprovements while maintaining on-par image quality, representing a first step\ntowards geometrically aligned, physically plausible holographic scene\nrepresentations."}
{"id": "2506.08581", "pdf": "https://arxiv.org/pdf/2506.08581", "abs": "https://arxiv.org/abs/2506.08581", "authors": ["Fabian C. Peña", "Steffen Herbold"], "title": "Evaluating the Performance and Efficiency of Sentence-BERT for Code Comment Classification", "categories": ["cs.SE"], "comment": "4th Intl. Workshop on NL-based Software Engineering (NLBSE)", "summary": "This work evaluates Sentence-BERT for a multi-label code comment\nclassification task seeking to maximize the classification performance while\ncontrolling efficiency constraints during inference. Using a dataset of 13,216\nlabeled comment sentences, Sentence-BERT models are fine-tuned and combined\nwith different classification heads to recognize comment types. While larger\nmodels outperform smaller ones in terms of F1, the latter offer outstanding\nefficiency, both in runtime and GFLOPS. As result, a balance between a\nreasonable F1 improvement (+0.0346) and a minimal efficiency degradation (+1.4x\nin runtime and +2.1x in GFLOPS) is reached."}
{"id": "2506.08334", "pdf": "https://arxiv.org/pdf/2506.08334", "abs": "https://arxiv.org/abs/2506.08334", "authors": ["Weikun Peng", "Jun Lv", "Cewu Lu", "Manolis Savva"], "title": "Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos", "categories": ["cs.GR", "cs.CV"], "comment": "Project website can be found at\n  https://3dlg-hcvc.github.io/video2articulation/", "summary": "Articulated objects are prevalent in daily life. Understanding their\nkinematic structure and reconstructing them have numerous applications in\nembodied AI and robotics. However, current methods require carefully captured\ndata for training or inference, preventing practical, scalable, and\ngeneralizable reconstruction of articulated objects. We focus on reconstruction\nof an articulated object from a casually captured RGBD video shot with a\nhand-held camera. A casually captured video of an interaction with an\narticulated object is easy to acquire at scale using smartphones. However, this\nsetting is quite challenging, as the object and camera move simultaneously and\nthere are significant occlusions as the person interacts with the object. To\ntackle these challenges, we introduce a coarse-to-fine framework that infers\njoint parameters and segments movable parts of the object from a dynamic RGBD\nvideo. To evaluate our method under this new setting, we build a 20$\\times$\nlarger synthetic dataset of 784 videos containing 284 objects across 11\ncategories. We compare our approach with existing methods that also take video\nas input. Experiments show that our method can reconstruct synthetic and real\narticulated objects across different categories from dynamic RGBD videos,\noutperforming existing methods significantly."}
{"id": "2506.08053", "pdf": "https://arxiv.org/pdf/2506.08053", "abs": "https://arxiv.org/abs/2506.08053", "authors": ["Yuhao Lian", "Xiao Han", "Xinmao Deng"], "title": "Power Domain Sparse Dimensional Constellation Multiple Access (PD-SDCMA) for Enabled Flexible PONs", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": "arXiv admin note: substantial text overlap with arXiv:2502.16271 by\n  other authors", "summary": "With the commercial deployment of 5G and the in-depth research of 6G, the\ndemand for high-speed data services in the next-generation fiber optic access\nsystems is growing increasingly. Passive optical networks (PONs) have become a\nresearch hotspot due to their characteristics of low loss, high bandwidth, and\nlow cost. However, the traditional orthogonal multiple access (OMA-PON) has\ndifficulty meeting the requirements of the next-generation PON for high\nspectral efficiency and flexibility. In this paper, a novel transmission\ntechnology, namely power-domain sparse dimension constellation multiple access\n(PD-SDCMA), is proposed for the first time. Through the signal space dimension\nselection strategy (S2D-strategy) in the high-dimensional signal space, the\nlow-dimensional constellation is sparsely superimposed into the\nhigh-dimensional space, thereby reducing multi-user interference and enhancing\nthe system capacity. PD-SDCMA supports higher-order modulation formats and more\naccess groups, and is also compatible with the existing orthogonal frequency\ndivision multiplexing (OFDM) architecture. The simulation results show that in\na 25 km single-mode fiber system, compared with PD-NOMA and 3D-NOMA, PD-SDCMA\ncan support more users and significantly reduce BER. This technology provides\nan efficient and low-cost solution for the evolution of Flexible PONs."}
{"id": "2506.08467", "pdf": "https://arxiv.org/pdf/2506.08467", "abs": "https://arxiv.org/abs/2506.08467", "authors": ["Prakash Shukla", "Suchismita Naik", "Ike Obi", "Jessica Backus", "Nancy Rasche", "Paul Parson"], "title": "Rethinking Citation of AI Sources in Student-AI Collaboration within HCI Design Education", "categories": ["cs.HC"], "comment": "8 pages, EduCHI 2025: 7th Annual Symposium on HCI Education,\n  Bloomington, IN, USA, July 2025", "summary": "The growing integration of AI tools in student design projects presents an\nunresolved challenge in HCI education: how should AI-generated content be cited\nand documented? Traditional citation frameworks -- grounded in credibility,\nretrievability, and authorship -- struggle to accommodate the dynamic and\nephemeral nature of AI outputs. In this paper, we examine how undergraduate\nstudents in a UX design course approached AI usage and citation when given the\nfreedom to integrate generative tools into their design process. Through\nqualitative analysis of 35 team projects and reflections from 175 students, we\nidentify varied citation practices ranging from formal attribution to indirect\nor absent acknowledgment. These inconsistencies reveal gaps in existing\nframeworks and raise questions about authorship, assessment, and pedagogical\ntransparency. We argue for rethinking AI citation as a reflective and\npedagogical practice; one that supports metacognitive engagement by prompting\nstudents to critically evaluate how and why they used AI throughout the design\nprocess. We propose alternative strategies -- such as AI contribution\nstatements and process-aware citation models that better align with the\niterative and reflective nature of design education. This work invites\neducators to reconsider how citation practices can support meaningful\nstudent--AI collaboration."}
{"id": "2506.08150", "pdf": "https://arxiv.org/pdf/2506.08150", "abs": "https://arxiv.org/abs/2506.08150", "authors": ["Arvid Becker", "Pedro Cabalar", "Martin Diéguez", "Javier Romero", "Susana Hahn", "Torsten Schaub"], "title": "Compiling Metric Temporal Answer Set Programming", "categories": ["cs.AI", "cs.LO", "I.2.4; I.2.8"], "comment": null, "summary": "We develop a computational approach to Metric Answer Set Programming (ASP) to\nallow for expressing quantitative temporal constrains, like durations and\ndeadlines. A central challenge is to maintain scalability when dealing with\nfine-grained timing constraints, which can significantly exacerbate ASP's\ngrounding bottleneck. To address this issue, we leverage extensions of ASP with\ndifference constraints, a simplified form of linear constraints, to handle\ntime-related aspects externally. Our approach effectively decouples metric ASP\nfrom the granularity of time, resulting in a solution that is unaffected by\ntime precision."}
{"id": "2506.08661", "pdf": "https://arxiv.org/pdf/2506.08661", "abs": "https://arxiv.org/abs/2506.08661", "authors": ["Rida Bazzi", "Anya Chaturvedi", "Andréa W. Richa", "Peter Vargas"], "title": "Synchronization in Anonymous Networks Under Continuous Dynamics", "categories": ["cs.DC"], "comment": "18 pages, 1 figure, 2 tables", "summary": "We present the $\\kappa$-Synchronizer that works in non-synchronous dynamic\nnetworks under minimal assumptions. Our model allows continuous topological\nchanges without any guarantee of eventual global or partial stabilization and\nassumes that nodes are anonymous. This deterministic synchronizer is the first\nto enable nodes to simulate a dynamic network synchronous algorithm for\nexecutions in a semi-synchronous dynamic environment under a weakly-fair node\nactivation scheduler, despite the absence of a global clock, node ids,\npersistent connectivity or any assumptions about the edge dynamics (in both the\nsynchronous and semi-synchronous environments). In summary, we make the\nfollowing contributions: (1) we extend the definition of synchronizers to\nnetworks with continuous arbitrary edge dynamics; (2) we present the first\nsynchronizer from the semi-synchronous to the synchronous model in a network\nwith continuous arbitrary edge dynamics; and (3) we present non-trivial\napplications of the proposed synchronizer to existing algorithms. We assume an\nextension of the Pull communication model by adding a single 1-bit multi-writer\natomic register at each edge-port of a node, since we show that the standard\nPull model is not sufficient to allow for non-trivial synchronization in our\nscenario. The $\\kappa$-Synchronizer operates with memory overhead at the nodes\nthat is linear on the maximum node degree and logarithmic on the runtime of the\nunderlying synchronous algorithm being simulated."}
{"id": "2506.08252", "pdf": "https://arxiv.org/pdf/2506.08252", "abs": "https://arxiv.org/abs/2506.08252", "authors": ["Amisha Srivastava", "Samit S. Miftah", "Hyunmin Kim", "Debjit Pal", "Kanad Basu"], "title": "PoSyn: Secure Power Side-Channel Aware Synthesis", "categories": ["cs.CR", "cs.AR"], "comment": null, "summary": "Power Side-Channel (PSC) attacks exploit power consumption patterns to\nextract sensitive information, posing risks to cryptographic operations crucial\nfor secure systems. Traditional countermeasures, such as masking, face\nchallenges including complex integration during synthesis, substantial area\noverhead, and susceptibility to optimization removal during logic synthesis. To\naddress these issues, we introduce PoSyn, a novel logic synthesis framework\ndesigned to enhance cryptographic hardware resistance against PSC attacks. Our\nmethod centers on optimal bipartite mapping of vulnerable RTL components to\nstandard cells from the technology library, aiming to minimize PSC leakage. By\nutilizing a cost function integrating critical characteristics from both the\nRTL design and the standard cell library, we strategically modify mapping\ncriteria during RTL-to-netlist conversion without altering design\nfunctionality. Furthermore, we theoretically establish that PoSyn minimizes\nmutual information leakage, strengthening its security against PSC\nvulnerabilities. We evaluate PoSyn across various cryptographic hardware\nimplementations, including AES, RSA, PRESENT, and post-quantum cryptographic\nalgorithms such as Saber and CRYSTALS-Kyber, at technology nodes of 65nm, 45nm,\nand 15nm. Experimental results demonstrate a substantial reduction in success\nrates for Differential Power Analysis (DPA) and Correlation Power Analysis\n(CPA) attacks, achieving lows of 3% and 6%, respectively. TVLA analysis further\nconfirms that synthesized netlists exhibit negligible leakage. Additionally,\ncompared to conventional countermeasures like masking and shuffling, PoSyn\nsignificantly lowers attack success rates, achieving reductions of up to 72%,\nwhile simultaneously enhancing area efficiency by as much as 3.79 times."}
{"id": "2506.08759", "pdf": "https://arxiv.org/pdf/2506.08759", "abs": "https://arxiv.org/abs/2506.08759", "authors": ["Tim Littau", "Rihan Hai"], "title": "Qymera: Simulating Quantum Circuits using RDBMS", "categories": ["quant-ph", "cs.DB", "cs.ET"], "comment": null, "summary": "Quantum circuit simulation is crucial for quantum computing such as\nvalidating quantum algorithms. We present Qymera, a system that repurposes\nrelational database management systems (RDBMSs) for simulation by translating\ncircuits into SQL queries, allowing quantum operations to run natively within\nan RDBMS. Qymera supports a wide range of quantum circuits, offering a\ngraphical circuit builder and code-based interfaces to input circuits. With a\nbenchmarking framework, Qymera facilitates comparison of RDBMS-based simulation\nagainst state-of-the-art simulation methods. Our demonstration showcases\nQymera's end-to-end SQL-based execution, seamless integration with classical\nworkflows, and its utility for development, benchmarking, and education in\nquantum computing and data management."}
{"id": "2506.08461", "pdf": "https://arxiv.org/pdf/2506.08461", "abs": "https://arxiv.org/abs/2506.08461", "authors": ["Sungwoong Yune", "Hyojeong Lee", "Adiwena Putra", "Hyunjun Cho", "Cuong Duong Manh", "Jaeho Jeon", "Joo-Young Kim"], "title": "ABC-FHE : A Resource-Efficient Accelerator Enabling Bootstrappable Parameters for Client-Side Fully Homomorphic Encryption", "categories": ["cs.AR", "cs.CR", "cs.ET"], "comment": "7 pages, 6 figures, DAC 2025: 62st IEEE/ACM Design Automation\n  Conference. (DAC'25)", "summary": "As the demand for privacy-preserving computation continues to grow, fully\nhomomorphic encryption (FHE)-which enables continuous computation on encrypted\ndata-has become a critical solution. However, its adoption is hindered by\nsignificant computational overhead, requiring 10000-fold more computation\ncompared to plaintext processing. Recent advancements in FHE accelerators have\nsuccessfully improved server-side performance, but client-side computations\nremain a bottleneck, particularly under bootstrappable parameter\nconfigurations, which involve combinations of encoding, encrypt, decoding, and\ndecrypt for large-sized parameters. To address this challenge, we propose\nABC-FHE, an area- and power-efficient FHE accelerator that supports\nbootstrappable parameters on the client side. ABC-FHE employs a streaming\narchitecture to maximize performance density, minimize area usage, and reduce\noff-chip memory access. Key innovations include a reconfigurable Fourier engine\ncapable of switching between NTT and FFT modes. Additionally, an on-chip\npseudo-random number generator and a unified on-the-fly twiddle factor\ngenerator significantly reduce memory demands, while optimized task scheduling\nenhances the CKKS client-side processing, achieving reduced latency. Overall,\nABC-FHE occupies a die area of 28.638 mm2 and consumes 5.654 W of power in 28\nnm technology. It delivers significant performance improvements, achieving a\n1112x speed-up in encoding and encryption execution time compared to a CPU, and\n214x over the state-of-the-art client-side accelerator. For decoding and\ndecryption, it achieves a 963x speed-up over the CPU and 82x over the\nstate-of-the-art accelerator."}
{"id": "2506.08606", "pdf": "https://arxiv.org/pdf/2506.08606", "abs": "https://arxiv.org/abs/2506.08606", "authors": ["Radoslaw Klimek"], "title": "RE-oriented Model Development with LLM Support and Deduction-based Verification", "categories": ["cs.SE"], "comment": "The paper has been peer-reviewed and accepted for publication to the\n  1st International Workshop on Artificial Intelligence for Integrated\n  Development Environments (AI-IDE) of the 33rd ACM Symposium on the\n  Foundations of Software Engineering (FSE '25), June 23--27, 2025, Trondheim,\n  Norway", "summary": "The requirements engineering (RE) phase is pivotal in developing high-quality\nsoftware. Integrating advanced modelling techniques with large language models\n(LLMs) and formal verification in a logical style can significantly enhance\nthis process. We propose a comprehensive framework that focuses on specific\nUnified Modelling Language (UML) diagrams for preliminary system development.\nThis framework offers visualisations at various modelling stages and seamlessly\nintegrates large language models and logical reasoning engines. The behavioural\nmodels generated with the assistance of LLMs are automatically translated into\nformal logical specifications. Deductive formal verification ensures that\nlogical requirements and interrelations between software artefacts are\nthoroughly addressed. Ultimately, the framework facilitates the automatic\ngeneration of program skeletons, streamlining the transition from design to\nimplementation."}
{"id": "2506.08350", "pdf": "https://arxiv.org/pdf/2506.08350", "abs": "https://arxiv.org/abs/2506.08350", "authors": ["Yicheng Zhan", "Dong-Ha Shin", "Seung-Hwan Baek", "Kaan Akşit"], "title": "Complex-Valued Holographic Radiance Fields", "categories": ["cs.GR", "cs.CV", "cs.ET"], "comment": "28 pages, 21 figures", "summary": "Modeling the full properties of light, including both amplitude and phase, in\n3D representations is crucial for advancing physically plausible rendering,\nparticularly in holographic displays. To support these features, we propose a\nnovel representation that optimizes 3D scenes without relying on\nintensity-based intermediaries. We reformulate 3D Gaussian splatting with\ncomplex-valued Gaussian primitives, expanding support for rendering with light\nwaves. By leveraging RGBD multi-view images, our method directly optimizes\ncomplex-valued Gaussians as a 3D holographic scene representation. This\neliminates the need for computationally expensive hologram re-optimization.\nCompared with state-of-the-art methods, our method achieves 30x-10,000x speed\nimprovements while maintaining on-par image quality, representing a first step\ntowards geometrically aligned, physically plausible holographic scene\nrepresentations."}
{"id": "2506.08655", "pdf": "https://arxiv.org/pdf/2506.08655", "abs": "https://arxiv.org/abs/2506.08655", "authors": ["Kamil Jerabek", "Jan Luxemburk", "Richard Plny", "Josef Koumar", "Jaroslav Pesek", "Karel Hynek"], "title": "When Simple Model Just Works: Is Network Traffic Classification in Crisis?", "categories": ["cs.LG", "cs.NI"], "comment": null, "summary": "Machine learning has been applied to network traffic classification (TC) for\nover two decades. While early efforts used shallow models, the latter 2010s saw\na shift toward complex neural networks, often reporting near-perfect accuracy.\nHowever, it was recently revealed that a simple k-NN baseline using packet\nsequences metadata (sizes, times, and directions) can be on par or even\noutperform more complex methods. In this paper, we investigate this phenomenon\nfurther and evaluate this baseline across 12 datasets and 15 TC tasks, and\ninvestigate why it performs so well. Our analysis shows that most datasets\ncontain over 50% redundant samples (identical packet sequences), which\nfrequently appear in both training and test sets due to common splitting\npractices. This redundancy can lead to overestimated model performance and\nreduce the theoretical maximum accuracy when identical flows have conflicting\nlabels. Given its distinct characteristics, we further argue that standard\nmachine learning practices adapted from domains like NLP or computer vision may\nbe ill-suited for TC. Finally, we propose new directions for task formulation\nand evaluation to address these challenges and help realign the field."}
{"id": "2506.08517", "pdf": "https://arxiv.org/pdf/2506.08517", "abs": "https://arxiv.org/abs/2506.08517", "authors": ["Mayar Elfares", "Salma Younis", "Pascal Reisert", "Ralf Küsters", "Tobias Renner", "Andreas Bulling"], "title": "Guidelines for Gaze-based Neural Preliminary Diagnosis", "categories": ["cs.HC", "q-bio.NC"], "comment": null, "summary": "Neural disorders refer to any condition affecting the nervous system and that\ninfluence how individuals perceive and interact with the world. Traditional\nneural diagnoses rely on cumbersome, time-consuming, or subjective methods,\nsuch as clinical interviews, behavioural observations, or medical imaging. Eye\ntracking is an attractive alternative because analysing eye movements, such as\nfixations and saccades, can provide more objective insights into brain function\nand cognitive processing by capturing non-verbal and unconscious responses.\nDespite its potential, existing gaze-based studies presented seemingly\ncontradictory findings. They are dispersed across diverse fields, requiring\nfurther research to standardise protocols and expand their application,\nparticularly as a preliminary indicator of neural processes for differential\ndiagnosis. Therefore, this paper outlines the main agreed-upon findings and\nprovides a systematisation of knowledge and key guidelines towards advancing\ngaze-based neural preliminary diagnosis."}
{"id": "2506.08216", "pdf": "https://arxiv.org/pdf/2506.08216", "abs": "https://arxiv.org/abs/2506.08216", "authors": ["Shahaf Bassan", "Guy Amir", "Meirav Zehavi", "Guy Katz"], "title": "What makes an Ensemble (Un) Interpretable?", "categories": ["cs.LG", "cs.CC", "cs.LO"], "comment": "To appear in ICML 2025", "summary": "Ensemble models are widely recognized in the ML community for their limited\ninterpretability. For instance, while a single decision tree is considered\ninterpretable, ensembles of trees (e.g., boosted trees) are often treated as\nblack-boxes. Despite this folklore recognition, there remains a lack of\nrigorous mathematical understanding of what particularly makes an ensemble\n(un)-interpretable, including how fundamental factors like the (1) *number*,\n(2) *size*, and (3) *type* of base models influence its interpretability. In\nthis work, we seek to bridge this gap by applying concepts from computational\ncomplexity theory to study the challenges of generating explanations for\nvarious ensemble configurations. Our analysis uncovers nuanced complexity\npatterns influenced by various factors. For example, we demonstrate that under\nstandard complexity assumptions like P$\\neq$NP, interpreting ensembles remains\nintractable even when base models are of constant size. Surprisingly, the\ncomplexity changes drastically with the number of base models: small ensembles\nof decision trees are efficiently interpretable, whereas interpreting ensembles\nwith even a constant number of linear models remains intractable. We believe\nthat our findings provide a more robust foundation for understanding the\ninterpretability of ensembles, emphasizing the benefits of examining it through\na computational complexity lens."}
{"id": "2506.08715", "pdf": "https://arxiv.org/pdf/2506.08715", "abs": "https://arxiv.org/abs/2506.08715", "authors": ["Paritosh Ranjan", "Surajit Majumder", "Prodip Roy", "Bhuban Padhan"], "title": "Balancing Fixed Number of Nodes Among Multiple Fixed Clusters", "categories": ["cs.DC"], "comment": "9 pages, 2 figures", "summary": "Cloud infrastructure users often allocate a fixed number of nodes to\nindividual container clusters (e.g., Kubernetes, OpenShift), resulting in\nunderutilization of computing resources due to asynchronous and variable\nworkload peaks across clusters. This research proposes a novel system and\nmethod for dynamic rebalancing of a fixed total number of nodes among multiple\nfixed clusters based on real-time resource utilization thresholds. By\nintroducing a Node Balancing Cluster Group (NBCG), clusters are grouped and\nallowed to dynamically share nodes through a controlled reallocation mechanism,\nmanaged by a Node Balancing Cluster Balancer and a Resizing Rule Engine. The\nsystem identifies overutilized and underutilized clusters using threshold\nparameters, and reassigns nodes without incurring additional provisioning\ncosts. If reallocation causes a violation of utilization thresholds, the system\nreverses the operation to maintain cluster stability. The proposed architecture\nnot only optimizes resource utilization and operational cost but also\nintroduces a strategic advantage for cloud service providers like IBM Cloud.\nUnlike existing solutions, this approach enables intra-account node sharing\nacross clusters with strict adherence to user-defined constraints and ensures\nconsistent cluster state management. This invention has the potential to\nsignificantly reduce computing resource waste and position IBM Cloud services\nas more efficient and competitive."}
{"id": "2506.08653", "pdf": "https://arxiv.org/pdf/2506.08653", "abs": "https://arxiv.org/abs/2506.08653", "authors": ["Alexander Strack", "Christopher Taylor", "Dirk Pflüger"], "title": "Parallel FFTW on RISC-V: A Comparative Study including OpenMP, MPI, and HPX", "categories": ["cs.DC", "cs.ET"], "comment": "12 pages, 9 figures, Fifth International workshop on RISC-V for HPC\n  co-located with ISC 2025", "summary": "Rapid advancements in RISC-V hardware development shift the focus from\nlow-level optimizations to higher-level parallelization. Recent RISC-V\nprocessors, such as the SOPHON SG2042, have 64 cores. RISC-V processors with\ncore counts comparable to the SG2042, make efficient parallelization as crucial\nfor RISC-V as the more established processors such as x86-64. In this work, we\nevaluate the parallel scaling of the widely used FFTW library on RISC-V for MPI\nand OpenMP. We compare it to a 64-core AMD EPYC 7742 CPU side by side for\ndifferent types of FFTW planning. Additionally, we investigate the effect of\nmemory optimization on RISC-V in HPX-FFT, a parallel FFT library based on the\nasynchronous many-task runtime HPX using an FFTW backend. We generally observe\na performance delta between the x86-64 and RISC-V chips of factor eight for\ndouble-precision 2D FFT. Effective memory optimizations in HPX-FFT on x86-64 do\nnot translate to the RISC-V chip. FFTW with MPI shows good scaling up to 64\ncores on x86-64 and RISC-V regardless of planning. In contrast, FFTW with\nOpenMP requires measured planning on both architectures to achieve good scaling\nup to 64 cores. The results of our study mark an early step on the journey to\nlarge-scale parallel applications running on RISC-V."}
{"id": "2506.08628", "pdf": "https://arxiv.org/pdf/2506.08628", "abs": "https://arxiv.org/abs/2506.08628", "authors": ["Radoslaw Klimek", "Julia Witek"], "title": "Logic Mining from Process Logs: Towards Automated Specification and Verification", "categories": ["cs.SE"], "comment": "This is a draft version of the article submitted to a journal from\n  the JCR list", "summary": "Logical specifications play a key role in the formal analysis of behavioural\nmodels. Automating the derivation of such specifications is particularly\nvaluable in complex systems, where manual construction is time-consuming and\nerror-prone. This article presents an approach for generating logical\nspecifications from process models discovered via workflow mining, combining\npattern-based translation with automated reasoning techniques. In contrast to\nearlier work, we evaluate the method on both general-purpose and real-case\nevent logs, enabling a broader empirical assessment. The study examines the\nimpact of data quality, particularly noise, on the structure and testability of\ngenerated specifications. Using automated theorem provers, we validate a\nvariety of logical properties, including satisfiability, internal consistency,\nand alignment with predefined requirements. The results support the\napplicability of the approach in realistic settings and its potential\nintegration into empirical software engineering practices."}
{"id": "2506.09023", "pdf": "https://arxiv.org/pdf/2506.09023", "abs": "https://arxiv.org/abs/2506.09023", "authors": ["Julia Guerrero-Viu", "Michael Fischer", "Iliyan Georgiev", "Elena Garces", "Diego Gutierrez", "Belen Masia", "Valentin Deschaintre"], "title": "Fine-Grained Spatially Varying Material Selection in Images", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Selection is the first step in many image editing processes, enabling faster\nand simpler modifications of all pixels sharing a common modality. In this\nwork, we present a method for material selection in images, robust to lighting\nand reflectance variations, which can be used for downstream editing tasks. We\nrely on vision transformer (ViT) models and leverage their features for\nselection, proposing a multi-resolution processing strategy that yields finer\nand more stable selection results than prior methods. Furthermore, we enable\nselection at two levels: texture and subtexture, leveraging a new two-level\nmaterial selection (DuMaS) dataset which includes dense annotations for over\n800,000 synthetic images, both on the texture and subtexture levels."}
{"id": "2506.08549", "pdf": "https://arxiv.org/pdf/2506.08549", "abs": "https://arxiv.org/abs/2506.08549", "authors": ["Rajan Das Gupta", "Ashikur Rahman", "Md Imrul Hasan Showmick", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "title": "Exploring the Convergence of HCI and Evolving Technologies in Information Systems", "categories": ["cs.HC"], "comment": "Accepted in CITIC 2025", "summary": "Modern technology driven information systems are part of our daily lives.\nHowever, this deep integration poses new challenges to the human computer\ninteraction (HCI) professionals. With the rapid growth of mobile and cloud\ncomputing and the Internet of Things (IoT), the demand for HCI specialists to\ndesign user-friendly and adaptable interfaces has never been more pressing.\nEspecially for diverse user groups such as children, the elderly and people\nwith disabilities who need interfaces tailored to their needs regardless of\ntime and location. This study reviewed 50 recent papers on HCI interface design\nfor modern information systems. The goal is to see how well these methods\naddress the demands of current technology. The findings show that most HCI\ndesign methods are still based on old desktop models and do not support mobile\nusers and location-based services well. Most existing interface design\nguidelines do not align with the flexibility and dynamism of emerging\ntechnologies. The goal of this study is to improve interface design by\ncombining agile methodologies with human-centered design principles. Future\nstudies should also incorporate both qualitative and quantitative approaches,\nparticularly in the context of cloud-based technologies and organizational\ninformation systems. This approach aims to bridge the gap between current\ninterface design practices and the changing technological landscape."}
{"id": "2506.08294", "pdf": "https://arxiv.org/pdf/2506.08294", "abs": "https://arxiv.org/abs/2506.08294", "authors": ["Ruanqianqian Huang", "Ayana Monroe", "Peli de Halleux", "Sorin Lerner", "Nikolaj Bjørner"], "title": "Z3Guide: A Scalable, Student-Centered, and Extensible Educational Environment for Logic Modeling", "categories": ["cs.HC", "cs.LO"], "comment": null, "summary": "Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from\nbudgeting for grocery shopping to verifying software behavior. Logic modeling\nhelps solve CSPs programmatically using SMT solvers. Despite its importance in\nmany Computer Science disciplines, resources for teaching and learning logic\nmodeling are scarce and scattered, and challenges remain in designing\neducational environments for logic modeling that are accessible and meet the\nneeds of teachers and students. This paper explores how to design such an\nenvironment and probes the impact of the design on the learning experience.\nFrom a need-finding interview study and a design iteration with teachers of\nlogic modeling, we curated 10 design guidelines spanning three main\nrequirements: providing easy access, supporting various educational modalities,\nand allowing extensions for customized pedagogical needs. We implemented nine\nguidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a\nlogic modeling learning workshop with more than 100 students, we gathered\npositive feedback on its support for learning and identified opportunities for\nfuture improvements."}
{"id": "2506.08923", "pdf": "https://arxiv.org/pdf/2506.08923", "abs": "https://arxiv.org/abs/2506.08923", "authors": ["Holly Casaletto", "Jeff Lefevre", "Aldrin Montana", "Peter Alvaro"], "title": "Mycelium: A Transformation-Embedded LSM-Tree", "categories": ["cs.DC"], "comment": null, "summary": "Compaction is a necessary, but often costly background process in\nwrite-optimized data structures like LSM-trees that reorganizes incoming data\nthat is sequentially appended to logs. In this paper, we introduce\nTransformation-Embedded LSM-trees (TE-LSM), a novel approach that transparently\nembeds a variety of data transformations into the compaction process. While\nmany others have sought to reduce the high cost of compaction, TE-LSMs leverage\nthe opportunity to embed other useful work to amortize IO costs and\namplification. We illustrate the use of a TE-LSM in Mycelium, our prototype\nbuilt on top of RocksDB that extends the compaction process through a\ncross-column-family merging mechanism. Mycelium enables seamless integration of\na transformer interface and aims to better prepare data for future accesses\nbased on access patterns. We use Mycelium to explore three types of\ntransformations: splitting column groups, converting data formats, and index\nbuilding. In addition to providing a cost model analysis, we evaluate\nMycelium's write and read performance using YCSB workloads. Our results show\nthat Mycelium incurs a 20% write throughput overhead - significantly lower than\nthe 35% to 60% overhead observed in naive approaches that perform data\ntransformations outside of compaction-while achieving up to 425% improvements\nin read latency compared to RocksDB baseline."}
{"id": "2506.08749", "pdf": "https://arxiv.org/pdf/2506.08749", "abs": "https://arxiv.org/abs/2506.08749", "authors": ["Viktoria Patapovich", "Mo Kordzanganeh", "Alexey Melnikov"], "title": "Superposed Parameterised Quantum Circuits", "categories": ["quant-ph", "cs.ET", "cs.LG", "cs.NE"], "comment": "20 pages, 6 figures, 3 tables", "summary": "Quantum machine learning has shown promise for high-dimensional data\nanalysis, yet many existing approaches rely on linear unitary operations and\nshared trainable parameters across outputs. These constraints limit\nexpressivity and scalability relative to the multi-layered, non-linear\narchitectures of classical deep networks. We introduce superposed parameterised\nquantum circuits to overcome these limitations. By combining flip-flop quantum\nrandom-access memory with repeat-until-success protocols, a superposed\nparameterised quantum circuit embeds an exponential number of parameterised\nsub-models in a single circuit and induces polynomial activation functions\nthrough amplitude transformations and post-selection. We provide an analytic\ndescription of the architecture, showing how multiple parameter sets are\ntrained in parallel while non-linear amplitude transformations broaden\nrepresentational power beyond conventional quantum kernels. Numerical\nexperiments underscore these advantages: on a 1D step-function regression a\ntwo-qubit superposed parameterised quantum circuit cuts the mean-squared error\nby three orders of magnitude versus a parameter-matched variational baseline;\non a 2D star-shaped two-dimensional classification task, introducing a\nquadratic activation lifts accuracy to 81.4% and reduces run-to-run variance\nthree-fold. These results position superposed parameterised quantum circuits as\na hardware-efficient route toward deeper, more versatile parameterised quantum\ncircuits capable of learning complex decision boundaries."}
{"id": "2506.08680", "pdf": "https://arxiv.org/pdf/2506.08680", "abs": "https://arxiv.org/abs/2506.08680", "authors": ["Hugo Daniel Macedo", "Ken Pierce"], "title": "Proceedings of the 23rd International Overture Workshop", "categories": ["cs.SE"], "comment": null, "summary": "This volume contains the papers presented at the 23rd International Overture\nWorkshop, held on the 11th of June 2025. This event was the latest in a series\nof workshops around the Vienna Development Method (VDM), the open-source\nproject Overture, and related tools and formalisms. VDM is one of the longest\nestablished formal methods for systems development. A lively community of\nresearchers and practitioners has grown up in academia and industry has grown\naround the modelling languages (VDM-SL, VDM++, VDM-RT, CML) and tools\n(VDMTools, Overture, Crescendo, Symphony, the INTO-CPS chain, and ViennaTalk).\nTogether, these provide a platform for work on modelling and analysis\ntechnology that includes static and dynamic analysis, test generation,\nexecution support, and model checking. This workshop provided updates on the\nemerging technology of VDM/Overture, including collaboration infrastructure,\ncollaborative modelling and co-simulation for Cyber-Physical Systems."}
{"id": "2506.08634", "pdf": "https://arxiv.org/pdf/2506.08634", "abs": "https://arxiv.org/abs/2506.08634", "authors": ["Alvaro Becerra", "Daniel Andres", "Pablo Villegas", "Roberto Daza", "Ruth Cobos"], "title": "MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "Accepted in LASI Spain 25: Learning Analytics Summer Institute Spain\n  2025", "summary": "In this article, we present a novel multimodal feedback framework called\nMOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal\nLearning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),\nand Collaborative assessments for generating personalized feedback on student\nlearning activities. This framework consists of four key steps. First, peers\nand professors' assessments are conducted through standardized rubrics (that\ninclude both quantitative and qualitative evaluations). Second, multimodal data\nare collected during learning activities, including video recordings, audio\ncapture, gaze tracking, physiological signals (heart rate, motion data), and\nbehavioral interactions. Third, personalized feedback is generated using AI,\nsynthesizing human-based evaluations and data-based multimodal insights such as\nposture, speech patterns, stress levels, and cognitive load, among others.\nFinally, students review their own performance through video recordings and\nengage in self-assessment and feedback visualization, comparing their own\nevaluations with peers and professors' assessments, class averages, and\nAI-generated recommendations. By combining human-based and data-based\nevaluation techniques, this framework enables more accurate, personalized and\nactionable feedback. We tested MOSAIC-F in the context of improving oral\npresentation skills."}
{"id": "2506.08321", "pdf": "https://arxiv.org/pdf/2506.08321", "abs": "https://arxiv.org/abs/2506.08321", "authors": ["Manooshree Patel", "Rayna Bhattacharyya", "Thomas Lu", "Arnav Mehta", "Niels Voss", "Narges Norouzi", "Gireeja Ranade"], "title": "LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs", "categories": ["cs.AI", "cs.HC", "cs.LO"], "comment": null, "summary": "We present LeanTutor, a Large Language Model (LLM)-based tutoring system for\nmath proofs. LeanTutor interacts with the student in natural language, formally\nverifies student-written math proofs in Lean, generates correct next steps, and\nprovides the appropriate instructional guidance. LeanTutor is composed of three\nmodules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and\n(iii) a natural language feedback generator. The first module faithfully\nautoformalizes student proofs into Lean and verifies proof accuracy via\nsuccessful code compilation. If the proof has an error, the incorrect step is\nidentified. The next-step generator module outputs a valid next Lean tactic for\nincorrect proofs via LLM-based candidate generation and proof search. The\nfeedback generator module leverages Lean data to produce a\npedagogically-motivated natural language hint for the student user. To evaluate\nour system, we introduce PeanoBench, a human-written dataset derived from the\nNatural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each\nnatural language proof step is paired with the corresponding logically\nequivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of\ntactics in correct proofs and accurately identifies the incorrect step in 30%\nof incorrect proofs. In generating natural language hints for erroneous proofs,\nLeanTutor outperforms a simple baseline on accuracy and relevance metrics."}
{"id": "2210.16402", "pdf": "https://arxiv.org/pdf/2210.16402", "abs": "https://arxiv.org/abs/2210.16402", "authors": ["Artavazd Maranjyan", "Mher Safaryan", "Peter Richtárik"], "title": "GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity", "categories": ["cs.LG", "cs.DC", "math.OC", "stat.ML"], "comment": null, "summary": "We study a class of distributed optimization algorithms that aim to alleviate\nhigh communication costs by allowing clients to perform multiple local\ngradient-type training steps before communication. In a recent breakthrough,\nMishchenko et al. (2022) proved that local training, when properly executed,\nleads to provable communication acceleration, and this holds in the strongly\nconvex regime without relying on any data similarity assumptions. However,\ntheir ProxSkip method requires all clients to take the same number of local\ntraining steps in each communication round. We propose a redesign of the\nProxSkip method, allowing clients with ``less important'' data to get away with\nfewer local training steps without impacting the overall communication\ncomplexity of the method. In particular, we prove that our modified method,\nGradSkip, converges linearly under the same assumptions and has the same\naccelerated communication complexity, while the number of local gradient steps\ncan be reduced relative to a local condition number. We further generalize our\nmethod by extending the randomness of probabilistic alternations to arbitrary\nunbiased compression operators and by considering a generic proximable\nregularizer. This generalization, which we call GradSkip+, recovers several\nrelated methods in the literature as special cases. Finally, we present an\nempirical study on carefully designed toy problems that confirm our theoretical\nclaims."}
{"id": "2506.08759", "pdf": "https://arxiv.org/pdf/2506.08759", "abs": "https://arxiv.org/abs/2506.08759", "authors": ["Tim Littau", "Rihan Hai"], "title": "Qymera: Simulating Quantum Circuits using RDBMS", "categories": ["quant-ph", "cs.DB", "cs.ET"], "comment": null, "summary": "Quantum circuit simulation is crucial for quantum computing such as\nvalidating quantum algorithms. We present Qymera, a system that repurposes\nrelational database management systems (RDBMSs) for simulation by translating\ncircuits into SQL queries, allowing quantum operations to run natively within\nan RDBMS. Qymera supports a wide range of quantum circuits, offering a\ngraphical circuit builder and code-based interfaces to input circuits. With a\nbenchmarking framework, Qymera facilitates comparison of RDBMS-based simulation\nagainst state-of-the-art simulation methods. Our demonstration showcases\nQymera's end-to-end SQL-based execution, seamless integration with classical\nworkflows, and its utility for development, benchmarking, and education in\nquantum computing and data management."}
{"id": "2506.08688", "pdf": "https://arxiv.org/pdf/2506.08688", "abs": "https://arxiv.org/abs/2506.08688", "authors": ["Wenbing Tang", "Mingfei Cheng", "Renzhi Wang", "Yuan Zhou", "Chengwei Liu", "Yang Liu", "Zuohua Ding"], "title": "Causality-aware Safety Testing for Autonomous Driving Systems", "categories": ["cs.SE"], "comment": null, "summary": "Simulation-based testing is essential for evaluating the safety of Autonomous\nDriving Systems (ADSs). Comprehensive evaluation requires testing across\ndiverse scenarios that can trigger various types of violations under different\nconditions. While existing methods typically focus on individual diversity\nmetrics, such as input scenarios, ADS-generated motion commands, and system\nviolations, they often fail to capture the complex interrelationships among\nthese elements. This oversight leads to gaps in testing coverage, potentially\nmissing critical issues in the ADS under evaluation. However, quantifying these\ninterrelationships presents a significant challenge. In this paper, we propose\na novel causality-aware fuzzing technique, Causal-Fuzzer, to enable efficient\nand comprehensive testing of ADSs by exploring causally diverse scenarios. The\ncore of Causal-Fuzzer is constructing a causal graph to model the\ninterrelationships among the diversities of input scenarios, ADS motion\ncommands, and system violations. Then the causal graph will guide the process\nof critical scenario generation. Specifically, Causal-Fuzzer proposes (1) a\ncausality-based feedback mechanism that quantifies the combined diversity of\ntest scenarios by assessing whether they activate new causal relationships, and\n(2) a causality-driven mutation strategy that prioritizes mutations on input\nscenario elements with higher causal impact on ego action changes and violation\noccurrence, rather than treating all elements equally. We evaluated\nCausal-Fuzzer on an industry-grade ADS Apollo, with a high-fidelity. Our\nempirical results demonstrate that Causal-Fuzzer significantly outperforms\nexisting methods in (1) identifying a greater diversity of violations, (2)\nproviding enhanced testing sufficiency with improved coverage of causal\nrelationships, and (3) achieving greater efficiency in detecting the first\ncritical scenarios."}
{"id": "2506.08725", "pdf": "https://arxiv.org/pdf/2506.08725", "abs": "https://arxiv.org/abs/2506.08725", "authors": ["Hyeon Jeon", "Jeongin Park", "Sungbok Shin", "Jinwook Seo"], "title": "Stop Misusing t-SNE and UMAP for Visual Analytics", "categories": ["cs.HC", "cs.LG"], "comment": "9 pages", "summary": "Misuses of t-SNE and UMAP in visual analytics have become increasingly\ncommon. For example, although t-SNE and UMAP projections often do not\nfaithfully reflect true distances between clusters, practitioners frequently\nuse them to investigate inter-cluster relationships. In this paper, we bring\nthis issue to the surface and comprehensively investigate why such misuse\noccurs and how to prevent it. We conduct a literature review of 114 papers to\nverify the prevalence of the misuse and analyze the reasonings behind it. We\nthen execute an interview study to uncover practitioners' implicit motivations\nfor using these techniques -- rationales often undisclosed in the literature.\nOur findings indicate that misuse of t-SNE and UMAP primarily stems from\nlimited discourse on their appropriate use in visual analytics. We conclude by\nproposing future directions and concrete action items to promote more\nreasonable use of DR."}
{"id": "2506.08505", "pdf": "https://arxiv.org/pdf/2506.08505", "abs": "https://arxiv.org/abs/2506.08505", "authors": ["Shahaf Bassan", "Yizhak Yisrael Elboher", "Tobias Ladner", "Matthias Althoff", "Guy Katz"], "title": "Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": "To appear in ICML 2025", "summary": "Despite significant advancements in post-hoc explainability techniques for\nneural networks, many current methods rely on heuristics and do not provide\nformally provable guarantees over the explanations provided. Recent work has\nshown that it is possible to obtain explanations with formal guarantees by\nidentifying subsets of input features that are sufficient to determine that\npredictions remain unchanged using neural network verification techniques.\nDespite the appeal of these explanations, their computation faces significant\nscalability challenges. In this work, we address this gap by proposing a novel\nabstraction-refinement technique for efficiently computing provably sufficient\nexplanations of neural network predictions. Our method abstracts the original\nlarge neural network by constructing a substantially reduced network, where a\nsufficient explanation of the reduced network is also provably sufficient for\nthe original network, hence significantly speeding up the verification process.\nIf the explanation is in sufficient on the reduced network, we iteratively\nrefine the network size by gradually increasing it until convergence. Our\nexperiments demonstrate that our approach enhances the efficiency of obtaining\nprovably sufficient explanations for neural network predictions while\nadditionally providing a fine-grained interpretation of the network's\npredictions across different abstraction levels."}
{"id": "2506.08027", "pdf": "https://arxiv.org/pdf/2506.08027", "abs": "https://arxiv.org/abs/2506.08027", "authors": ["Asit Mishra", "Dusan Stosic", "Simon Layton"], "title": "Recipes for Pre-training LLMs with MXFP8", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Precision scaling - using fewer bits to represent model parameters and\nrelated tensors during pre-training - has emerged as a compelling technique for\nimproving GPU efficiency without sacrificing accuracy. Microscaling (MX)\nformats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling\nthis precision scaling aspect. These formats combine narrow floating-point data\ntypes with per-block scaling factors, offering a fine-grained approach to\nquantizing tensors.\n  Although MX-formats offer the promise of improved numeric stability compared\nto other reduced-precision representations, in practice they must be used\ncarefully in order to successfully converge an LLM on a multi-trillion token\ndataset. In this paper, we show that the rounding mode suggested in OCP\nspecification can lead to divergence when pre-training an LLM. We show an\nimproved rounding mode, which uses round-to-infinity to compute scaling\nfactors, enables successful pre-training in MXFP8 for an 8B model on 15T\ntokens."}
{"id": "2506.08790", "pdf": "https://arxiv.org/pdf/2506.08790", "abs": "https://arxiv.org/abs/2506.08790", "authors": ["Samarth Sikand", "Rohit Mehra", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "title": "Do Generative AI Tools Ensure Green Code? An Investigative Study", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": "4 pages. To be published in the proceedings of 2nd International\n  Workshop on Responsible AI Engineering (RAIE '24), co-located with ICSE '24,\n  Lisbon, Portugal", "summary": "Software sustainability is emerging as a primary concern, aiming to optimize\nresource utilization, minimize environmental impact, and promote a greener,\nmore resilient digital ecosystem. The sustainability or \"greenness\" of software\nis typically determined by the adoption of sustainable coding practices. With a\nmaturing ecosystem around generative AI, many software developers now rely on\nthese tools to generate code using natural language prompts. Despite their\npotential advantages, there is a significant lack of studies on the\nsustainability aspects of AI-generated code. Specifically, how environmentally\nfriendly is the AI-generated code based upon its adoption of sustainable coding\npractices? In this paper, we present the results of an early investigation into\nthe sustainability aspects of AI-generated code across three popular generative\nAI tools - ChatGPT, BARD, and Copilot. The results highlight the default\nnon-green behavior of tools for generating code, across multiple rules and\nscenarios. It underscores the need for further in-depth investigations and\neffective remediation strategies."}
{"id": "2506.08805", "pdf": "https://arxiv.org/pdf/2506.08805", "abs": "https://arxiv.org/abs/2506.08805", "authors": ["Stina Klein", "Pooja Prajod", "Katharina Weitz", "Matteo Lavit Nicora", "Dimitra Tsovaltzi", "Elisabeth André"], "title": "Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration", "categories": ["cs.HC", "cs.RO"], "comment": "Accepted LBW at CHIWORK 2025", "summary": "The integration of collaborative robots (cobots) in industrial settings\nraises concerns about worker well-being, particularly due to reduced social\ninteractions. Avatars - designed to facilitate worker interactions and\nengagement - are promising solutions to enhance the human-robot collaboration\n(HRC) experience. However, real-world perspectives on avatar-supported HRC\nremain unexplored. To address this gap, we conducted a focus group study with\nemployees from a German manufacturing company that uses cobots. Before the\ndiscussion, participants engaged with a scripted, industry-like HRC demo in a\nlab setting. This qualitative approach provided valuable insights into the\navatar's potential roles, improvements to its behavior, and practical\nconsiderations for deploying them in industrial workcells. Our findings also\nemphasize the importance of personalized communication and task assistance.\nAlthough our study's limitations restrict its generalizability, it serves as an\ninitial step in recognizing the potential of adaptive, context-aware avatar\ninteractions in real-world industrial environments."}
{"id": "2506.08585", "pdf": "https://arxiv.org/pdf/2506.08585", "abs": "https://arxiv.org/abs/2506.08585", "authors": ["Petr Hliněný", "Jan Jedelský"], "title": "k-Planar and Fan-Crossing Drawings and Transductions of Planar Graphs", "categories": ["cs.CG", "cs.LO", "math.CO"], "comment": null, "summary": "We introduce a two-way connection between FO transductions (logical\ntransformations) of planar graphs, and a certain variant of fan-crossing\n(fan-planar) drawings of graphs which for bounded-degree graphs essentially\nreduces to being k-planar for fixed k. For graph classes, this connection\nallows to derive non-transducibility results from nonexistence of the said\ndrawings and, conversely, from nonexistence of a transduction to derive\nnonexistence of the said drawings. For example, the class of 3D-grids is not\nk-planar for any fixed k. We hope that this connection will help to draw a path\nto a possible proof that not all toroidal graphs are transducible from planar\ngraphs.\n  Our characterization can be extended to any fixed surface instead of the\nplane. The result is based on a very recent characterization of weakly sparse\nFO transductions of classes of bounded expansion by [Gajarsk\\'y, G{\\l}adkowski,\nJedelsk\\'y, Pilipczuk and Toru\\'nczyk, arXiv:2505.15655]."}
{"id": "2506.08167", "pdf": "https://arxiv.org/pdf/2506.08167", "abs": "https://arxiv.org/abs/2506.08167", "authors": ["Sunny Gupta", "Nikita Jangid", "Amit Sethi"], "title": "UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"], "comment": null, "summary": "Federated Learning (FL) often suffers from severe performance degradation\nwhen faced with non-IID data, largely due to local classifier bias. Traditional\nremedies such as global model regularization or layer freezing either incur\nhigh computational costs or struggle to adapt to feature shifts. In this work,\nwe propose UniVarFL, a novel FL framework that emulates IID-like training\ndynamics directly at the client level, eliminating the need for global model\ndependency. UniVarFL leverages two complementary regularization strategies\nduring local training: Classifier Variance Regularization, which aligns\nclass-wise probability distributions with those expected under IID conditions,\neffectively mitigating local classifier bias; and Hyperspherical Uniformity\nRegularization, which encourages a uniform distribution of feature\nrepresentations across the hypersphere, thereby enhancing the model's ability\nto generalize under diverse data distributions. Extensive experiments on\nmultiple benchmark datasets demonstrate that UniVarFL outperforms existing\nmethods in accuracy, highlighting its potential as a highly scalable and\nefficient solution for real-world FL deployments, especially in\nresource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL"}
{"id": "2506.08812", "pdf": "https://arxiv.org/pdf/2506.08812", "abs": "https://arxiv.org/abs/2506.08812", "authors": ["Priyavanshi Pathania", "Rohit Mehra", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "title": "Towards a Knowledge Base of Common Sustainability Weaknesses in Green Software Development", "categories": ["cs.SE", "cs.CY"], "comment": "3 pages. To be published in the proceedings of 38th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE 2023),\n  Kirchberg, Luxembourg", "summary": "With the climate crisis looming, engineering sustainable software systems\nbecome crucial to optimize resource utilization, minimize environmental impact,\nand foster a greener, more resilient digital ecosystem. For developers, getting\naccess to automated tools that analyze code and suggest sustainabilityrelated\noptimizations becomes extremely important from a learning and implementation\nperspective. However, there is currently a dearth of such tools due to the lack\nof standardized knowledge, which serves as the foundation of these tools. In\nthis paper, we motivate the need for the development of a standard knowledge\nbase of commonly occurring sustainability weaknesses in code, and propose an\ninitial way of doing that. Furthermore, through preliminary experiments, we\ndemonstrate why existing knowledge regarding software weaknesses cannot be\nre-tagged \"as is\" to sustainability without significant due diligence, thereby\nurging further explorations in this ecologically significant domain."}
{"id": "2506.08881", "pdf": "https://arxiv.org/pdf/2506.08881", "abs": "https://arxiv.org/abs/2506.08881", "authors": ["Nicolas Grelier", "Johannes Pfau", "Nicolas Mathieu", "Stéphane Kaufmann"], "title": "From Fads to Classics -- Analyzing Video Game Trend Evolutions through Steam Tags", "categories": ["cs.HC"], "comment": null, "summary": "The video game industry deals with a fast-paced, competitive and almost\nunpredictable market. Trends of genres, settings and modalities change on a\nperpetual basis, studios are often one big hit or miss away from surviving or\nperishing, and hitting the pulse of the time has become one of the greatest\nchallenges for industrials, investors and other stakeholders. In this work, we\naim to support the understanding of video game trends over time based on\ndata-driven analysis, visualization and interpretation of Steam tag evolutions.\nWe confirm underlying groundwork that trends can be categorized in short-lived\nfads, contemporary fashions, or stable classics, and derived that the surge of\na trend averages at about four years in the realm of video games. After using\nindustrial experts to validate our findings, we deliver visualizations,\ninsights and an open approach of deciphering shifts in video game trends."}
{"id": "2506.08899", "pdf": "https://arxiv.org/pdf/2506.08899", "abs": "https://arxiv.org/abs/2506.08899", "authors": ["Elias Horner", "Cristinel Mateis", "Guido Governatori", "Agata Ciabattoni"], "title": "From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LO"], "comment": null, "summary": "We present a novel approach to the automated semantic analysis of legal texts\nusing large language models (LLMs), targeting their transformation into formal\nrepresentations in Defeasible Deontic Logic (DDL). We propose a structured\npipeline that segments complex normative language into atomic snippets,\nextracts deontic rules, and evaluates them for syntactic and semantic\ncoherence. Our methodology is evaluated across various LLM configurations,\nincluding prompt engineering strategies, fine-tuned models, and multi-stage\npipelines, focusing on legal norms from the Australian Telecommunications\nConsumer Protections Code. Empirical results demonstrate promising alignment\nbetween machine-generated and expert-crafted formalizations, showing that LLMs\n- particularly when prompted effectively - can significantly contribute to\nscalable legal informatics."}
{"id": "2506.08169", "pdf": "https://arxiv.org/pdf/2506.08169", "abs": "https://arxiv.org/abs/2506.08169", "authors": ["Jingqiao Tang", "Ryan Bausback", "Feng Bao", "Richard Archibald"], "title": "Federated Learning on Stochastic Neural Networks", "categories": ["cs.LG", "cs.DC"], "comment": "25 pages, 19 figures, Submitted to Journal of Machine Learning for\n  Modeling and Computing", "summary": "Federated learning is a machine learning paradigm that leverages edge\ncomputing on client devices to optimize models while maintaining user privacy\nby ensuring that local data remains on the device. However, since all data is\ncollected by clients, federated learning is susceptible to latent noise in\nlocal datasets. Factors such as limited measurement capabilities or human\nerrors may introduce inaccuracies in client data. To address this challenge, we\npropose the use of a stochastic neural network as the local model within the\nfederated learning framework. Stochastic neural networks not only facilitate\nthe estimation of the true underlying states of the data but also enable the\nquantification of latent noise. We refer to our federated learning approach,\nwhich incorporates stochastic neural networks as local models, as Federated\nstochastic neural networks. We will present numerical experiments demonstrating\nthe performance and effectiveness of our method, particularly in handling\nnon-independent and identically distributed data."}
{"id": "2506.08860", "pdf": "https://arxiv.org/pdf/2506.08860", "abs": "https://arxiv.org/abs/2506.08860", "authors": ["Samah Kansab", "Francis Bordeleau", "Ali Tizghadam"], "title": "On The Impact of Merge Request Deviations on Code Review Practices", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": null, "summary": "Code review is a key practice in software engineering, ensuring quality and\ncollaboration. However, industrial Merge Request (MR) workflows often deviate\nfrom standardized review processes, with many MRs serving non-review purposes\n(e.g., drafts, rebases, or dependency updates). We term these cases deviations\nand hypothesize that ignoring them biases analytics and undermines ML models\nfor review analysis.\n  We identify seven deviation categories, occurring in 37.02% of MRs, and\npropose a few-shot learning detection method (91% accuracy). By excluding\ndeviations, ML models predicting review completion time improve performance in\n53.33% of cases (up to 2.25x) and exhibit significant shifts in feature\nimportance (47% overall, 60% top-*k*).\n  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven\ndetection approach, and (3) empirical evidence of their impact on ML-based\nreview analytics. This work aids practitioners in optimizing review efforts and\nensuring reliable insights."}
{"id": "2506.08892", "pdf": "https://arxiv.org/pdf/2506.08892", "abs": "https://arxiv.org/abs/2506.08892", "authors": ["Tauhid Tanjim", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams", "categories": ["cs.HC", "cs.RO"], "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "The human-robot interaction (HRI) field has recognized the importance of\nenabling robots to interact with teams. Human teams rely on effective\ncommunication for successful collaboration in time-sensitive environments.\nRobots can play a role in enhancing team coordination through real-time\nassistance. Despite significant progress in human-robot teaming research, there\nremains an essential gap in how robots can effectively communicate with action\nteams using multimodal interaction cues in time-sensitive environments. This\nstudy addresses this knowledge gap in an experimental in-lab study to\ninvestigate how multimodal robot communication in action teams affects workload\nand human perception of robots. We explore team collaboration in a medical\ntraining scenario where a robotic crash cart (RCC) provides verbal and\nnon-verbal cues to help users remember to perform iterative tasks and search\nfor supplies. Our findings show that verbal cues for object search tasks and\nvisual cues for task reminders reduce team workload and increase perceived ease\nof use and perceived usefulness more effectively than a robot with no feedback.\nOur work contributes to multimodal interaction research in the HRI field,\nhighlighting the need for more human-robot teaming research to understand best\npractices for integrating collaborative robots in time-sensitive environments\nsuch as in hospitals, search and rescue, and manufacturing applications."}
{"id": "2506.08426", "pdf": "https://arxiv.org/pdf/2506.08426", "abs": "https://arxiv.org/abs/2506.08426", "authors": ["Zheng Lin", "Zhe Chen", "Xianhao Chen", "Wei Ni", "Yue Gao"], "title": "HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "16 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:2403.13101", "summary": "Split federated learning (SFL) has emerged as a promising paradigm to\ndemocratize machine learning (ML) on edge devices by enabling layer-wise model\npartitioning. However, existing SFL approaches suffer significantly from the\nstraggler effect due to the heterogeneous capabilities of edge devices. To\naddress the fundamental challenge, we propose adaptively controlling batch\nsizes (BSs) and model splitting (MS) for edge devices to overcome resource\nheterogeneity. We first derive a tight convergence bound of SFL that quantifies\nthe impact of varied BSs and MS on learning performance. Based on the\nconvergence bound, we propose HASFL, a heterogeneity-aware SFL framework\ncapable of adaptively controlling BS and MS to balance communication-computing\nlatency and training convergence in heterogeneous edge networks. Extensive\nexperiments with various datasets validate the effectiveness of HASFL and\ndemonstrate its superiority over state-of-the-art benchmarks."}
{"id": "2506.08980", "pdf": "https://arxiv.org/pdf/2506.08980", "abs": "https://arxiv.org/abs/2506.08980", "authors": ["Kaifeng He", "Mingwei Liu", "Chong Wang", "Zike Li", "Yanlin Wang", "Xin Peng", "Zibin Zheng"], "title": "AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code Generation", "categories": ["cs.SE", "I.2.7; I.2.2"], "comment": null, "summary": "Code generation with large language models (LLMs) is highly sensitive to\ntoken selection during decoding, particularly at uncertain decision points that\ninfluence program logic. While standard strategies like greedy and beam search\ntreat all tokens uniformly, they overlook code-specific uncertainty patterns,\nleading to suboptimal performance. This paper presents an empirical study\nrevealing that many generation errors stem from ranking mistakes at\nhigh-uncertainty steps, where the correct token is present but not top-ranked.\n  Motivated by these findings, we propose AdaDec, an uncertainty-guided\nadaptive decoding framework that integrates a token-level pause-then-rerank\nmechanism driven by token uncertainty (Shannon entropy). AdaDec learns\nmodel-specific uncertainty thresholds and applies a lookahead-based reranking\nstrategy when uncertainty is high. Experiments on HumanEval and MBPP benchmarks\nshow that AdaDec improves Pass@1 accuracy by up to 15.5% over greedy decoding,\noutperforms or matches beam search, and reduces computational cost and latency\nthrough efficient, selective pausing. Our results highlight the promise of\nuncertainty-aware adaptive decoding for improving the reliability and\nefficiency of LLM-based code generation."}
{"id": "2506.08911", "pdf": "https://arxiv.org/pdf/2506.08911", "abs": "https://arxiv.org/abs/2506.08911", "authors": ["Petar Jakuš", "Hrvoje Džapo"], "title": "Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU", "categories": ["cs.HC", "cs.LG", "cs.SD", "eess.AS"], "comment": "4 pages", "summary": "This paper presents a keyword spotting (KWS) system implemented on the NXP\nMCXN947 microcontroller with an integrated Neural Processing Unit (NPU),\nenabling real-time voice interaction on resource-constrained devices. The\nsystem combines MFCC feature extraction with a CNN classifier, optimized using\nQuantization Aware Training to reduce model size with minimal accuracy drop.\nExperimental results demonstrate a 59x speedup in inference time when\nleveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy\nwith a model size of 30.58 KB, demonstrating the feasibility of efficient,\nlow-power voice interfaces on embedded platforms."}
{"id": "2506.08433", "pdf": "https://arxiv.org/pdf/2506.08433", "abs": "https://arxiv.org/abs/2506.08433", "authors": ["Hernán Maina", "Nicolás Wolovick", "Luciana Benotti"], "title": "Low-resource domain adaptation while minimizing energy and hardware resource consumption", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "A shorter version of this work was accepted as a two-page abstract\n  for presentation at the Widening Natural Language Processing (WiNLP) 2023\n  Workshop. That version was not publicly released, and this is the first\n  public version of the work", "summary": "Training Large Language Models (LLMs) is costly in terms of energy, hardware,\nand annotated data, often resulting in a positionality rooted in predominant\ncultures and values (Santy et al., 2023). Domain adaptation has emerged as a\npromising strategy to better align models with diverse cultural and value\ncontexts (Hershcovich et al., 2022), but its computational cost remains a\nsignificant barrier, particularly for research groups lacking access to\nlarge-scale infrastructure. In this paper, we evaluate how the use of different\nnumerical precisions and data parallelization strategies impacts both training\nspeed (as a proxy to energy and hardware consumption) and model accuracy, with\nthe goal of facilitating domain adaptation in low-resource environments. Our\nfindings are relevant to any setting where energy efficiency, accessibility, or\nlimited hardware availability are key concerns."}
{"id": "2506.09002", "pdf": "https://arxiv.org/pdf/2506.09002", "abs": "https://arxiv.org/abs/2506.09002", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Hange Shi", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "title": "Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models", "categories": ["cs.SE"], "comment": "13 pages, 5 figures", "summary": "Unit testing is essential for ensuring software reliability and correctness.\nClassic Search-Based Software Testing (SBST) methods and concolic\nexecution-based approaches for generating unit tests often fail to achieve high\ncoverage due to difficulties in handling complex program units, such as\nbranching conditions and external dependencies. Recent work has increasingly\nutilized large language models (LLMs) to generate test cases, improving the\nquality of test generation by providing better context and correcting errors in\nthe model's output. However, these methods rely on fixed prompts, resulting in\nrelatively low compilation success rates and coverage. This paper presents\nPALM, an approach that leverages large language models (LLMs) to enhance the\ngeneration of high-coverage unit tests. PALM performs program analysis to\nidentify branching conditions within functions, which are then combined into\npath constraints. These constraints and relevant contextual information are\nused to construct prompts that guide the LLMs in generating unit tests. We\nimplement the approach and evaluate it in 10 open-source Rust crates.\nExperimental results show that within just two or three hours, PALM can\nsignificantly improves test coverage compared to classic methods, with\nincreases in overall project coverage exceeding 50% in some instances and its\ngenerated tests achieving an average coverage of 75.77%, comparable to human\neffort (71.30%), highlighting the potential of LLMs in automated test\ngeneration. We submitted 91 PALM-generated unit tests targeting new code. Of\nthese submissions, 80 were accepted, 5 were rejected, and 6 remain pending\nreview. The results demonstrate the effectiveness of integrating program\nanalysis with AI and open new avenues for future research in automated software\ntesting."}
{"id": "2506.06353", "pdf": "https://arxiv.org/pdf/2506.06353", "abs": "https://arxiv.org/abs/2506.06353", "authors": ["Naseem Babu", "Jimson Mathew", "A. P. Vinod"], "title": "Large Language Models for EEG: A Comprehensive Survey and Taxonomy", "categories": ["eess.SP", "cs.AI", "cs.ET", "cs.HC", "cs.LG"], "comment": null, "summary": "The growing convergence between Large Language Models (LLMs) and\nelectroencephalography (EEG) research is enabling new directions in neural\ndecoding, brain-computer interfaces (BCIs), and affective computing. This\nsurvey offers a systematic review and structured taxonomy of recent\nadvancements that utilize LLMs for EEG-based analysis and applications. We\norganize the literature into four domains: (1) LLM-inspired foundation models\nfor EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal\ngeneration including image and 3D object synthesis, and (4) clinical\napplications and dataset management tools. The survey highlights how\ntransformer-based architectures adapted through fine-tuning, few-shot, and\nzero-shot learning have enabled EEG-based models to perform complex tasks such\nas natural language generation, semantic interpretation, and diagnostic\nassistance. By offering a structured overview of modeling strategies, system\ndesigns, and application areas, this work serves as a foundational resource for\nfuture work to bridge natural language processing and neural signal analysis\nthrough language models."}
{"id": "2506.08713", "pdf": "https://arxiv.org/pdf/2506.08713", "abs": "https://arxiv.org/abs/2506.08713", "authors": ["Fariz Ikhwantri", "Dusica Marijan"], "title": "Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process."}
{"id": "2506.08462", "pdf": "https://arxiv.org/pdf/2506.08462", "abs": "https://arxiv.org/abs/2506.08462", "authors": ["Christos Margadji", "Sebastian W. Pattinson"], "title": "Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing", "categories": ["cs.AI", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Industrial processes must be robust and adaptable, as environments and tasks\nare often unpredictable, while operational errors remain costly and difficult\nto detect. AI-based control systems offer a path forward, yet typically depend\non supervised learning with extensive labelled datasets, which limits their\nability to generalize across variable and data-scarce industrial settings.\nFoundation models could enable broader reasoning and knowledge integration, but\nrarely deliver the quantitative precision demanded by engineering applications.\nHere, we introduceControl and Interpretation of Production via Hybrid Expertise\nand Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming\nto replicate human-like reasoning for industrial control, instantiated in a\ncommercial-grade 3D printer. It integrates a process expert, a regression model\nenabling quantitative characterization of system states required for\nengineering tasks. CIPHER also incorporates retrieval-augmented generation to\naccess external expert knowledge and support physics-informed, chain-of-thought\nreasoning. This hybrid architecture exhibits strong generalization to\nout-of-distribution tasks. It interprets visual or textual inputs from process\nmonitoring, explains its decisions, and autonomously generates precise machine\ninstructions, without requiring explicit annotations. CIPHER thus lays the\nfoundations for autonomous systems that act with precision, reason with\ncontext, and communicate decisions transparently, supporting safe and trusted\ndeployment in industrial settings."}
{"id": "2506.08727", "pdf": "https://arxiv.org/pdf/2506.08727", "abs": "https://arxiv.org/abs/2506.08727", "authors": ["Samarth Sikand", "Rohit Mehra", "Priyavanshi Pathania", "Nikhil Bamby", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder", "Adam P. Burden"], "title": "Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.SE"], "comment": "5 pages. To be published in the proceedings of 9th International\n  Workshop on Green and Sustainable Software (GREENS '25), April 29, 2025,\n  Ottawa, Canada (Co-located with ICSE 2025)", "summary": "While Generative AI stands to be one of the fastest adopted technologies\never, studies have made evident that the usage of Large Language Models (LLMs)\nputs significant burden on energy grids and our environment. It may prove a\nhindrance to the Sustainability goals of any organization. A crucial step in\nany Sustainability strategy is monitoring or estimating the energy consumption\nof various components. While there exist multiple tools for monitoring energy\nconsumption, there is a dearth of tools/frameworks for estimating the\nconsumption or carbon emissions. Current drawbacks of both monitoring and\nestimation tools include high input data points, intrusive nature, high error\nmargin, etc. We posit that leveraging emerging LLM benchmarks and related data\npoints can help overcome aforementioned challenges while balancing accuracy of\nthe emission estimations. To that extent, we discuss the challenges of current\napproaches and present our evolving framework, R-ICE, which estimates prompt\nlevel inference carbon emissions by leveraging existing state-of-the-art(SOTA)\nbenchmark. This direction provides a more practical and non-intrusive way to\nenable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our\npromising validation results suggest that benchmark-based modelling holds great\npotential for inference emission estimation and warrants further exploration\nfrom the scientific community."}
{"id": "2506.08555", "pdf": "https://arxiv.org/pdf/2506.08555", "abs": "https://arxiv.org/abs/2506.08555", "authors": ["Xinyue Niu", "Akira Furui"], "title": "Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement", "categories": ["cs.CV", "cs.HC"], "comment": "6 pages, 3 figures. This work has been accepted for presentation at\n  the IEEE Engineering in Medicine and Biology Conference (EMBC) 2025", "summary": "Cross-subject electromyography (EMG) pattern recognition faces significant\nchallenges due to inter-subject variability in muscle anatomy, electrode\nplacement, and signal characteristics. Traditional methods rely on\nsubject-specific calibration data to adapt models to new users, an approach\nthat is both time-consuming and impractical for large-scale, real-world\ndeployment. This paper presents an approach to eliminate calibration\nrequirements through feature disentanglement, enabling effective cross-subject\ngeneralization. We propose an end-to-end dual-branch adversarial neural network\nthat simultaneously performs pattern recognition and individual identification\nby disentangling EMG features into pattern-specific and subject-specific\ncomponents. The pattern-specific components facilitate robust pattern\nrecognition for new users without model calibration, while the subject-specific\ncomponents enable downstream applications such as task-invariant biometric\nidentification. Experimental results demonstrate that the proposed model\nachieves robust performance on data from unseen users, outperforming various\nbaseline methods in cross-subject scenarios. Overall, this study offers a new\nperspective for cross-subject EMG pattern recognition without model calibration\nand highlights the proposed model's potential for broader applications, such as\ntask-independent biometric systems."}
{"id": "2506.08838", "pdf": "https://arxiv.org/pdf/2506.08838", "abs": "https://arxiv.org/abs/2506.08838", "authors": ["Yuchong Xie", "Wenhui Zhang", "Dongdong She"], "title": "ZTaint-Havoc: From Havoc Mode to Zero-Execution Fuzzing-Driven Taint Inference", "categories": ["cs.CR", "cs.SE"], "comment": "To appear on 34th ISSTA", "summary": "Fuzzing is a widely used technique for discovering software vulnerabilities,\nbut identifying hot bytes that influence program behavior remains challenging.\nTraditional taint analysis can track such bytes white-box, but suffers from\nscalability issue. Fuzzing-Driven Taint Inference (FTI) offers a black-box\nalternative, yet typically incurs significant runtime overhead due to extra\nprogram executions. We observe that the commonly used havoc mutation scheme in\nfuzzing can be adapted for lightweight FTI with zero extra executions. We\npresent a computational model of havoc mode, demonstrating that it can perform\nFTI while generating new test cases. Building on this, we propose ZTaint-Havoc,\na novel, efficient FTI with minimal overhead (3.84% on UniBench, 12.58% on\nFuzzBench). We further design an effective mutation algorithm utilizing the\nidentified hot bytes. Our comprehensive evaluation shows that ZTaint-Havoc,\nimplemented in AFL++, improves edge coverage by up to 33.71% on FuzzBench and\n51.12% on UniBench over vanilla AFL++, with average gains of 2.97% and 6.12% in\n24-hour fuzzing campaigns."}
{"id": "2506.08836", "pdf": "https://arxiv.org/pdf/2506.08836", "abs": "https://arxiv.org/abs/2506.08836", "authors": ["Flavio D'Intino", "Hans-Peter Hutter"], "title": "Advancing STT for Low-Resource Real-World Speech", "categories": ["cs.CL", "cs.HC"], "comment": "Conference: HCI International 2025, 20 pages, 4 figures", "summary": "Swiss German is a low-resource language represented by diverse dialects that\ndiffer significantly from Standard German and from each other, lacking a\nstandardized written form. As a result, transcribing Swiss German involves\ntranslating into Standard German. Existing datasets have been collected in\ncontrolled environments, yielding effective speech-to-text (STT) models, but\nthese models struggle with spontaneous conversational speech.\n  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour\nannotated speech corpus featuring real-world long-audio recordings from 39\nSwiss German radio and TV stations. It captures spontaneous speech across all\nmajor Swiss dialects recorded in various realistic environments and overcomes\nthe limitation of prior sentence-level corpora.\n  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,\nachieving notable enhancements over previous zero-shot performance metrics.\nImprovements in word error rate (WER) ranged from 19% to 33%, while BLEU scores\nincreased between 8% and 40%. The best fine-tuned model, large-v3, achieved a\nWER of 17.1% and a BLEU score of 74.8. This advancement is crucial for\ndeveloping effective and robust STT systems for Swiss German and other\nlow-resource languages in real-world contexts."}
{"id": "2506.08890", "pdf": "https://arxiv.org/pdf/2506.08890", "abs": "https://arxiv.org/abs/2506.08890", "authors": ["Tauhid Tanjim", "Promise Ekpo", "Huajie Cao", "Jonathan St. George", "Kevin Ching", "Hee Rin Lee", "Angelique Taylor"], "title": "Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication", "categories": ["cs.RO", "cs.HC"], "comment": "This is the author's original submitted version of the paper accepted\n  to the 2025 IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN). \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. For any other use, please contact IEEE", "summary": "Healthcare workers (HCWs) encounter challenges in hospitals, such as\nretrieving medical supplies quickly from crash carts, which could potentially\nresult in medical errors and delays in patient care. Robotic crash carts (RCCs)\nhave shown promise in assisting healthcare teams during medical tasks through\nguided object searches and task reminders. Limited exploration has been done to\ndetermine what communication modalities are most effective and least disruptive\nto patient care in real-world settings. To address this gap, we conducted a\nbetween-subjects experiment comparing the RCC's verbal and non-verbal\ncommunication of object search with a standard crash cart in resuscitation\nscenarios to understand the impact of robot communication on workload and\nattitudes toward using robots in the workplace. Our findings indicate that\nverbal communication significantly reduced mental demand and effort compared to\nvisual cues and with a traditional crash cart. Although frustration levels were\nslightly higher during collaborations with the robot compared to a traditional\ncart, these research insights provide valuable implications for human-robot\nteamwork in high-stakes environments."}
{"id": "2506.08962", "pdf": "https://arxiv.org/pdf/2506.08962", "abs": "https://arxiv.org/abs/2506.08962", "authors": ["Liangliang Chen", "Huiru Xie", "Jacqueline Rohde", "Ying Zhang"], "title": "WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Accepted to 2025 Frontiers in Education (FIE) Conference", "summary": "This research-to-practice work-in-progress (WIP) paper presents an AI-enabled\nsmart tutor designed to provide homework assessment and feedback for students\nin an undergraduate circuit analysis course. We detail the tutor's design\nphilosophy and core components, including open-ended question answering and\nhomework feedback generation. The prompts are carefully crafted to optimize\nresponses across different problems. The smart tutor was deployed on the\nMicrosoft Azure platform and is currently in use in an undergraduate circuit\nanalysis course at the School of Electrical and Computer Engineering in a\nlarge, public, research-intensive institution in the Southeastern United\nStates. Beyond offering personalized instruction and feedback, the tutor\ncollects student interaction data, which is summarized and shared with the\ncourse instructor. To evaluate its effectiveness, we collected student\nfeedback, with 90.9% of responses indicating satisfaction with the tutor.\nAdditionally, we analyze a subset of collected data on preliminary circuit\nanalysis topics to assess tutor usage frequency for each problem and identify\nfrequently asked questions. These insights help instructors gain real-time\nawareness of student difficulties, enabling more targeted classroom\ninstruction. In future work, we will release a full analysis once the complete\ndataset is available after the Spring 2025 semester. We also explore the\npotential applications of this smart tutor across a broader range of\nengineering disciplines by developing improved prompts, diagram-recognition\nmethods, and database management strategies, which remain ongoing areas of\nresearch."}
