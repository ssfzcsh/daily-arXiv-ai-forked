<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 18]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Stack Trace-Based Crash Deduplication with Transformer Adaptation](https://arxiv.org/abs/2508.19449)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: dedupT利用预训练语言模型和全连接网络，显著提升重复崩溃报告的排名和唯一性检测性能，优于现有深度学习和传统方法。


<details>
  <summary>Details</summary>
Motivation: 自动化崩溃报告系统产生大量重复报告，增加了开发人员的工作负担。传统的堆栈跟踪去重方法无法捕捉上下文和结构关系。

Method: dedupT基于Transformer结构，首先将预训练语言模型适配到堆栈跟踪，然后使用其嵌入训练全连接网络来排名重复崩溃。

Result: 在四个公开数据集上，dedupT在MRR上比最佳深度学习基线提升15%，比传统方法提升9%，且在ROC-AUC上表现更好。

Conclusion: dedupT通过现代NLP技术为崩溃报告去重提供了一个高效解决方案，推动了软件工程中NLP的深入应用。

Abstract: Automated crash reporting systems generate large volumes of duplicate
reports, overwhelming issue-tracking systems and increasing developer workload.
Traditional stack trace-based deduplication methods, relying on string
similarity, rule-based heuristics, or deep learning (DL) models, often fail to
capture the contextual and structural relationships within stack traces. We
propose dedupT, a transformer-based approach that models stack traces
holistically rather than as isolated frames. dedupT first adapts a pretrained
language model (PLM) to stack traces, then uses its embeddings to train a
fully-connected network (FCN) to rank duplicate crashes effectively. Extensive
experiments on real-world datasets show that dedupT outperforms existing DL and
traditional methods (e.g., sequence alignment and information retrieval
techniques) in both duplicate ranking and unique crash detection, significantly
reducing manual triage effort. On four public datasets, dedupT improves Mean
Reciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up
to 9% over traditional methods while achieving higher Receiver Operating
Characteristic Area Under the Curve (ROC-AUC) in detecting unique crash
reports. Our work advances the integration of modern natural language
processing (NLP) techniques into software engineering, providing an effective
solution for stack trace-based crash deduplication.

</details>


### [2] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的数据合成框架（Functionality-Oriented Code Self-Evolution），用于评估LLM代码嵌入在功能一致性方面的表现，并通过实验证明了该框架的有效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注代码克隆检测，忽略了代码功能的一致性问题，而LLM代码嵌入在这方面的能力尚不明确。

Method: 通过定义代码的四个语义和语法类别，提出了一种数据合成框架，生成多样化的代码变体以评估功能一致性。

Result: 在三个下游任务（代码克隆检测、功能一致性识别和代码检索）中，模型性能显著提升。

Conclusion: 该框架有效提升了代码功能理解的性能，为代码嵌入领域提供了新的发展方向。

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


### [3] [The Influence of Code Comments on the Perceived Helpfulness of Stack Overflow Posts](https://arxiv.org/abs/2508.19610)
*Kathrin Figl,Maria Kirchner,Sebastian Baltes,Michael Felderer*

Main category: cs.SE

TL;DR: 研究探讨了代码注释对Stack Overflow回答帮助性的影响，发现注释（尤其是块注释）显著提高了帮助性，新手尤其受益。


<details>
  <summary>Details</summary>
Motivation: 研究代码注释如何影响开发者对Stack Overflow回答的感知，以提升代码重用质量和平台实用性。

Method: 通过在线实验模拟Stack Overflow环境（n=91），比较块注释、行内注释与无注释代码的感知帮助性。

Result: 注释代码被认为比无注释代码更有帮助，新手更倾向于块注释；其他表面特征（如答案位置）影响较小。

Conclusion: 研究结果可优化社区平台（如Stack Overflow）和AI工具（如GitHub Copilot），提升代码可读性和开发支持。

Abstract: Question-and-answer platforms such as Stack Overflow have become an important
way for software developers to share and retrieve knowledge. However, reusing
poorly understood code can lead to serious problems, such as bugs or security
vulnerabilities. To better understand how code comments affect the perceived
helpfulness of Stack Overflow answers, we conducted an online experiment
simulating a Stack Overflow environment (n=91). The results indicate that both
block and inline comments are perceived as significantly more helpful than
uncommented source code. Moreover, novices rated code snippets with block
comments as more helpful than those with inline comments. Interestingly, other
surface features, such as the position of an answer and its answer score, were
considered less important. The content of Stack Overflow has been a major
source for training large language models. AI-based coding assistants such as
GitHub Copilot, which are based on these models, might change the way Stack
Overflow is used. However, our findings have implications beyond this specific
platform. First, they may help to improve the relevance of community-driven
platforms such as Stack Overflow, which provide human advice and explanations
of code solutions, complementing AI-based support for software developers.
Second, since chat-based AI tools can be prompted to generate code in different
ways, knowing which properties influence perceived helpfulness might lead to
targeted prompting strategies to generate more readable code snippets.

</details>


### [4] [Leveraging LLMs for Automated Translation of Legacy Code: A Case Study on PL/SQL to Java Transformation](https://arxiv.org/abs/2508.19663)
*Lola Solovyeva,Eduardo Carneiro Oliveira,Shiyu Fan,Alper Tuncay,Shamil Gareev,Andrea Capiluppi*

Main category: cs.SE

TL;DR: 利用大语言模型（LLM）将PL/SQL代码翻译为Java的可行性研究，提出定制化提示策略并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于VT遗留系统缺乏文档和自动化测试，研究探索如何利用LLM协助代码翻译以支持系统现代化。

Method: 基于10对PL/SQL-Java代码和15个Java类构建领域模型，结合链式引导推理和$n$次提示策略评估LLM。

Result: 该方法能生成语法准确且功能正确的翻译代码，但受限于样本量和测试案例的不足。

Conclusion: 研究为大规模遗留系统现代化提供了自动化解决方案的基础。

Abstract: The VT legacy system, comprising approximately 2.5 million lines of PL/SQL
code, lacks consistent documentation and automated tests, posing significant
challenges for refactoring and modernisation. This study investigates the
feasibility of leveraging large language models (LLMs) to assist in translating
PL/SQL code into Java for the modernised "VTF3" system. By leveraging a dataset
comprising 10 PL/SQL-to-Java code pairs and 15 Java classes, which collectively
established a domain model for the translated files, multiple LLMs were
evaluated. Furthermore, we propose a customized prompting strategy that
integrates chain-of-guidance reasoning with $n$-shot prompting. Our findings
indicate that this methodology effectively guides LLMs in generating
syntactically accurate translations while also achieving functional
correctness. However, the findings are limited by the small sample size of
available code files and the restricted access to test cases used for
validating the correctness of the generated code. Nevertheless, these findings
lay the groundwork for scalable, automated solutions in modernising large
legacy systems.

</details>


### [5] [Enabling Content Management Systems as an Information Source in Model-driven Projects](https://arxiv.org/abs/2508.19797)
*Joan Giner-Miguelez,Abel Gómez,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出一个基于模型的框架，用于促进无头CMS在软件开发中的集成，实现信息模式的自动发现和交互设计，并提供平台无关的访问。


<details>
  <summary>Details</summary>
Motivation: 随着CMS向无头模式发展，缺乏工具来高效管理和发现其高度定制化的信息模式，导致集成过程耗时且易出错。

Method: 开发一个模型化框架，能够自动发现CMS的信息模式，并设计与其他组件的交互，生成中间件库。

Result: 框架开源，提供平台无关的CMS访问，简化了CMS的集成和管理。

Conclusion: 该框架显著减少了手动处理的需求，提升了CMS在软件开发中的效率和可靠性。

Abstract: Content Management Systems (CMSs) are the most popular tool when it comes to
create and publish content across the web. Recently, CMSs have evolved,
becoming \emph{headless}. Content served by a \emph{headless CMS} aims to be
consumed by other applications and services through REST APIs rather than by
human users through a web browser. This evolution has enabled CMSs to become a
notorious source of content to be used in a variety of contexts beyond pure web
navigation. As such, CMS have become an important component of many information
systems. Unfortunately, we still lack the tools to properly discover and manage
the information stored in a CMS, often highly customized to the needs of a
specific domain. Currently, this is mostly a time-consuming and error-prone
manual process.
  In this paper, we propose a model-based framework to facilitate the
integration of headless CMSs in software development processes. Our framework
is able to discover and explicitly represent the information schema behind the
CMS. This facilitates designing the interaction between the CMS model and other
components consuming that information. These interactions are then generated as
part of a middleware library that offers platform-agnostic access to the CMS to
all the client applications. The complete framework is open-source and
available online.

</details>


### [6] [Towards a fundamental theory of modeling discrete systems](https://arxiv.org/abs/2508.19803)
*Peter Fettke,Wolfgang Reisig*

Main category: cs.SE

TL;DR: 论文探讨了建模在科学与工程中的重要性，并提出了针对数字时代挑战的新理论Heraklit建模框架。


<details>
  <summary>Details</summary>
Motivation: 建模是科学与工程的核心，但数字时代需要新的理论基础来解决新挑战。

Method: 提出了Heraklit建模框架作为新的建模方法。

Result: 介绍了Heraklit框架的初步成果。

Conclusion: 未来工作将关注建模的正确性、信息概念和不变性描述。

Abstract: Modeling is a central concern in both science and engineering. However, we
need a new fundamental theory to address the challenges of the digital age. In
this paper, we first explain why modeling is fundamental and which challenges
must be addressed in the digital world. As a main contribution, we introduce
the Heraklit modeling framework as a new approach to modeling. We conclude with
some general remarks. Future work will involve the correctness of modeling, the
notion of information, and the description of invariance in modeling.

</details>


### [7] [On the Future of Software Reuse in the Era of AI Native Software Engineering](https://arxiv.org/abs/2508.19834)
*Antero Taivalsaari,Tommi Mikkonen,Cesare Pautasso*

Main category: cs.SE

TL;DR: 论文讨论了人工智能辅助生成式软件重用的影响，并提出相关问题和研究议程。


<details>
  <summary>Details</summary>
Motivation: 探讨AI Native方法如何取代传统软件重用和开发实践，及其潜在问题。

Method: 分析AI辅助生成式软件重用的现象，并讨论其与货物崇拜开发的相似性。

Result: 提出相关问题和研究议程，以解决新兴方法的核心问题。

Conclusion: AI辅助生成式软件重用是软件开发的范式转变，需进一步研究其影响和解决方案。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Earlier opportunistic software reuse practices and organic
software development methods are rapidly being replaced by "AI Native"
approaches in which developers place their trust on code that has been
generated by artificial intelligence. This is leading to a new form of software
reuse that is conceptually not all that different from cargo cult development.
In this paper we discuss the implications of AI-assisted generative software
reuse, bring forth relevant questions, and define a research agenda for
tackling the central issues associated with this emerging approach.

</details>


### [8] [Generative AI for Testing of Autonomous Driving Systems: A Survey](https://arxiv.org/abs/2508.19882)
*Qunying Song,He Ye,Mark Harman,Federica Sarro*

Main category: cs.SE

TL;DR: 本文综述了生成式AI在自动驾驶系统(ADS)测试中的应用，通过分析91项研究，总结了六大应用类别、评估工具及27项局限。


<details>
  <summary>Details</summary>
Motivation: 解决ADS大规模部署前的测试挑战，利用生成式AI的潜力提高测试效率和多样性。

Method: 系统分析91项相关研究，总结生成式AI在ADS测试中的主要应用类别及其评估工具。

Result: 总结了六大应用类别、数据集、仿真工具、评估指标及27项局限性。

Conclusion: 生成式AI在ADS测试中具有潜力，但仍存在挑战，需进一步研究。

Abstract: Autonomous driving systems (ADS) have been an active area of research, with
the potential to deliver significant benefits to society. However, before
large-scale deployment on public roads, extensive testing is necessary to
validate their functionality and safety under diverse driving conditions.
Therefore, different testing approaches are required, and achieving effective
and efficient testing of ADS remains an open challenge. Recently, generative AI
has emerged as a powerful tool across many domains, and it is increasingly
being applied to ADS testing due to its ability to interpret context, reason
about complex tasks, and generate diverse outputs. To gain a deeper
understanding of its role in ADS testing, we systematically analyzed 91
relevant studies and synthesized their findings into six major application
categories, primarily centered on scenario-based testing of ADS. We also
reviewed their effectiveness and compiled a wide range of datasets, simulators,
ADS, metrics, and benchmarks used for evaluation, while identifying 27
limitations. This survey provides an overview and practical insights into the
use of generative AI for testing ADS, highlights existing challenges, and
outlines directions for future research in this rapidly evolving field.

</details>


### [9] [Smart Contract Intent Detection with Pre-trained Programming Language Model](https://arxiv.org/abs/2508.20086)
*Youwei Huang,Jianwen Li,Sen Fang,Yao Li,Peng Yang,Bin Hu,Tao Zhang*

Main category: cs.SE

TL;DR: SmartIntentNN2是一种改进的深度学习模型，用于检测智能合约中的恶意意图，结合BERT预训练语言模型和BiLSTM网络，性能优于前代。


<details>
  <summary>Details</summary>
Motivation: 智能合约开发中的恶意意图可能导致重大经济损失，因此需要高效检测方法。

Method: 结合BERT预训练语言模型和BiLSTM网络，训练于16,000个真实智能合约数据集。

Result: 改进后的模型F1分数为0.927，优于前代的0.8633。

Conclusion: SmartIntentNN2成为智能合约意图检测的先进模型。

Abstract: Malicious intent in smart contract development can lead to substantial
economic losses. SmartIntentNN is a deep learning model specifically designed
to identify unsafe intents in smart contracts. This model integrates the
Universal Sentence Encoder, a K-means clustering-based intent highlighting
mechanism, and a Bidirectional Long Short-Term Memory network for multi-label
classification, achieving an F1 of 0.8633 in distinguishing ten different
intent categories. In this study, we present an upgraded version of this model,
SmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant
enhancement in V2 is the incorporation of a BERT-based pre-trained language
model, which has been trained on a dataset of 16,000 real smart contracts using
a Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based
multi-label classification network. With an improved F1 of 0.927, V2
demonstrates enhanced performance compared to its predecessor, establishing
itself as the state-of-the-art model for smart contract intent detection.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [10] [Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC](https://arxiv.org/abs/2508.19350)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 研究了无线地下传感器网络（WUSNs）与非地面网络（NTNs）结合的可行性，通过蒙特卡罗模拟评估了连接效果，发现LoRa和LR-FHSS在不同场景下表现优异，但受多种因素影响。


<details>
  <summary>Details</summary>
Motivation: 在地下网络基础设施不可靠的恶劣环境中，提升无线地下传感器网络的通信可靠性是一个重要挑战。

Method: 通过蒙特卡罗模拟器，结合多层地下衰减模型、3GPP路径损耗模型和LoRaWAN调制方案（LoRa和LR-FHSS），评估地下到NTN的连接效果。

Result: LoRa SF7适合短距离无人机通信，LR-FHSS更适合HAP和LEO卫星平台；连接成功率受环境、设备数量、埋深和土壤湿度影响。

Conclusion: 地下与非地面网络的结合在特定场景下可行，但需考虑多种因素优化连接效果。

Abstract: Wireless underground sensor networks (WUSNs) offer significant social and
economic benefits by enabling the monitoring of subterranean entities. However,
the communication reliability of WUSNs diminishes in harsh environments where
terrestrial network infrastructure is either unavailable or unreliable. To
address this challenge, we explore the feasibility of integrating buried
massive machine-type communication (mMTC) sensors with non-terrestrial networks
(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms
(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN
connectivity for various large-scale underground monitoring applications. To
assess the effectiveness of underground-to-NTN connectivity, we develop a Monte
Carlo simulator that incorporates a multi-layer underground attenuation model,
the 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN
modulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum
(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for
short-range UAV communication in rural environments, while LR-FHSS modulation
proves to be a promising option for HAP and LEO satellite platforms in massive
WUSNs scenarios thanks to its adequate link budget and robustness to the
interference. Finally, we demonstrate that the success probability of
underground-to-NTN connectivity using LoRa and LR-FHSS is significantly
affected by factors such as the monitoring environment, the number of devices,
burial depth, and the soil's volumetric water content.

</details>


### [11] [Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions](https://arxiv.org/abs/2508.19736)
*Mohsen Ahadi,Adeel Malik,Omid Esrafilian,Florian Kaltenberger,Cedric Thienot*

Main category: cs.NI

TL;DR: 论文通过OAI测试床实验验证了5G NR在智能城市和工厂中的高精度定位能力，并提出了新的滤波和优化方法，实现了1-2米的定位精度。


<details>
  <summary>Details</summary>
Motivation: 5G新无线电（NR）是实现智能城市和工厂高精度定位的关键技术，但其在实践中受同步误差、多径传播等影响。

Method: 使用开源OpenAirInterface建立测试床，结合UL-TDoA和LMF，提出ToA/TDoA滤波及PSO优化方法，并利用CIR训练AI/ML模型。

Result: 实验表明，90%情况下可达1-2米定位精度，并公开数据集支持5G定位研究。

Conclusion: 研究为5G高精度定位系统设计提供了实用方案，并展示了超越5G的定位潜力。

Abstract: 5G New Radio (NR) is a key enabler of accurate positioning in smart cities
and smart factories. This paper presents the experimental results from three 5G
positioning testbeds running open-source OpenAirInterface (OAI) gNB and Core
Network (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly
integrated Location Management Function (LMF). The testbeds are deployed across
both indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),
following a 3GPP-compliant system model. The experiments highlight the impact
of synchronization impairments, multipath propagation, and deployment geometry
on positioning accuracy. To address these challenges, we propose tailored ToA
and TDoA filtering as well as a novel position estimation method based on
Particle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a
beyond-5G framework that leverages non-conventional measurements such as
Channel Impulse Response (CIR) to train and test Artificial Intelligence and
Machine Learning (AI/ML) models for data-driven positioning. The results
demonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%
of cases in different testbeds, offering practical insights for the design of
robust 5G positioning systems. Moreover, we publicly release the datasets
collected in this work to support the research within the 5G positioning
community.

</details>


### [12] [Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey](https://arxiv.org/abs/2508.19870)
*Yinqiu Liu,Ruichen Zhang,Haoxiang Luo,Yijing Lin,Geng Sun,Dusit Niyato,Hongyang Du,Zehui Xiong,Yonggang Wen,Abbas Jamalipour,Dong In Kim,Ping Zhang*

Main category: cs.NI

TL;DR: 该论文探讨了多LLM系统在边缘通用智能（EGI）中的应用及其安全挑战，提出了零信任安全框架作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 多LLM系统的协作性引入了新的安全漏洞，传统安全方法无法应对，需要零信任框架来保障安全。

Method: 论文首先分析多LLM系统在EGI中的安全风险，提出零信任框架，并分类技术进展为模型级和系统级方法。

Result: 提出了零信任多LLM框架，分类了安全机制，并指出了关键研究方向。

Conclusion: 本文为多LLM系统的零信任安全提供了系统化理论基础和实践策略。

Abstract: Agentification serves as a critical enabler of Edge General Intelligence
(EGI), transforming massive edge devices into cognitive agents through
integrating Large Language Models (LLMs) and perception, reasoning, and acting
modules. These agents collaborate across heterogeneous edge infrastructures,
forming multi-LLM agentic AI systems that leverage collective intelligence and
specialized capabilities to tackle complex, multi-step tasks. However, the
collaborative nature of multi-LLM systems introduces critical security
vulnerabilities, including insecure inter-LLM communications, expanded attack
surfaces, and cross-domain data leakage that traditional perimeter-based
security cannot adequately address. To this end, this survey introduces
zero-trust security of multi-LLM in EGI, a paradigmatic shift following the
``never trust, always verify'' principle. We begin by systematically analyzing
the security risks in multi-LLM systems within EGI contexts. Subsequently, we
present the vision of a zero-trust multi-LLM framework in EGI. We then survey
key technical progress to facilitate zero-trust multi-LLM systems in EGI.
Particularly, we categorize zero-trust security mechanisms into model- and
system-level approaches. The former and latter include strong identification,
context-aware access control, etc., and proactive maintenance, blockchain-based
management, etc., respectively. Finally, we identify critical research
directions. This survey serves as the first systematic treatment of zero-trust
applied to multi-LLM systems, providing both theoretical foundations and
practical strategies.

</details>


### [13] [2SYN: Congestion-Aware Multihoming](https://arxiv.org/abs/2508.20044)
*Kfir Toledo,Isaac Keslassy*

Main category: cs.NI

TL;DR: 2SYN是一种首个适用于任意目的地的拥塞感知多宿主算法，能动态选择路径并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多宿主路由器缺乏拥塞感知机制，无法避免拥塞路径。

Method: 提出2SYN算法，动态选择新连接的首选路径，易于在Linux中实现。

Result: 真实实验中，2SYN能动态适应连接质量，性能优于其他方法。

Conclusion: 2SYN有效帮助企业利用多宿主能力优化网络管理。

Abstract: When sending flows to arbitrary destinations, current multihoming routers
adopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid
congested paths.
  In this paper, we introduce 2SYN, the first congestion-aware multihoming
algorithm that works for any destination. We explain how it dynamically selects
a preferred path for new connections, even given previously-unseen
destinations. We further demonstrate that it can be easily implemented in
Linux. Finally, in a real-world experiment with either LTE or a wired link, we
show how 2SYN dynamically adapts to the quality of the connection and
outperforms alternative approaches. Thus, 2SYN helps companies better manage
their networks by leveraging their multihoming capabilities.

</details>


### [14] [A First Look at Inter-Cell Interference in the Wild](https://arxiv.org/abs/2508.20060)
*Daqian Ding,Yibo Pi,Cailian Chen*

Main category: cs.NI

TL;DR: 该研究首次对4G/5G网络中的小区间干扰进行了现实测量，揭示了实际网络中干扰管理的不足及其对信号质量的严重影响。


<details>
  <summary>Details</summary>
Motivation: 填补了关于小区间干扰在实际网络中有效性研究的空白。

Method: 通过测量研究，从网络部署、信道分配、时频资源分配和网络配置四个维度分析小区间干扰问题。

Result: 发现小区间干扰普遍存在且缺乏协调，导致信号质量下降；即使资源未充分利用，基站仍倾向于使用相同时频资源。

Conclusion: 研究揭示了通过优化干扰管理可显著提升信号质量的潜在机会。

Abstract: In cellular networks, inter-cell interference management has been studied for
decades, yet its real-world effectiveness remains under-explored. To bridge
this gap, we conduct a first measurement study of inter-cell interference for
operational 4G/5G networks. Our findings reveal the prevalence of inter-cell
interference and a surprising absence of interference coordination among
operational base stations. As a result, user equipments experience unnecessary
interference, which causes significant signal quality degradation, especially
under frequency-selective channel fading. We examine the inter-cell
interference issues from four major perspectives: network deployment, channel
assignment, time-frequency resource allocation, and network configuration. In
none of these dimensions is inter-cell interference effectively managed.
Notably, even when spectrum resources are underutilized and simple strategies
could effectively mitigate inter-cell interference, base stations consistently
prioritize using the same set of time-frequency resources, causing interference
across cells. Our measurements reveal substantial opportunities for improving
signal quality by inter-cell interference management.

</details>


### [15] [ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication](https://arxiv.org/abs/2508.20077)
*Tao Xiuyuan,Milena Radenkovic*

Main category: cs.NI

TL;DR: ML-MaxProp是一种结合监督机器学习的混合路由协议，通过实时预测转发节点适应性和动态优化转发决策，在灾难场景中显著提升了通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 灾难和大规模城市紧急情况下，传统通信网络因基础设施崩溃和资源受限而失效，现有DTN协议在稀疏相遇和缓存不足时表现不佳。

Method: ML-MaxProp利用机器学习动态评估节点转发能力（如相遇频率、跳数、缓存占用等），取代传统启发式方法。

Result: 在ONE模拟器中，ML-MaxProp相比基线协议显著提升了投递率、降低延迟和开销，且结果统计显著。

Conclusion: ML-MaxProp是DTN中轻量级、自适应的实用解决方案，能在极端条件下维持关键通信。

Abstract: In disaster-stricken and large-scale urban emergency scenarios, ensuring
reliable communication remains a formidable challenge, as collapsed
infrastructure, unpredictable mobility, and severely constrained resources
disrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient
through their store-carry-forward paradigm, reveal the fundamental weaknesses
of classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when
confronted with sparse encounters, buffer shortages, and volatile connectivity.
To address these obstacles, this study proposes ML-MaxProp, a hybrid routing
protocol that strengthens MaxProp with supervised machine learning. By
leveraging contextual features such as encounter frequency, hop count, buffer
occupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay
suitability in real time, transforming rigid heuristics into adaptive
intelligence. Extensive simulations in the ONE environment using the Helsinki
SPMBM mobility model show that ML-MaxProp consistently surpasses baseline
protocols, achieving higher delivery probability, lower latency, and reduced
overhead. Statistical validation further shows that these improvements are both
significant and robust, even under highly resource-constrained and unstable
conditions. Overall, this work shows that ML-MaxProp is not just an incremental
refinement but a lightweight, adaptive, and practical solution to one of the
hardest challenges in DTNs: sustaining mission-critical communication when
infrastructure collapses and every forwarding decision becomes critical.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [16] [FakeSV-VLM: Taming VLM for Detecting Fake Short-Video News via Progressive Mixture-Of-Experts Adapter](https://arxiv.org/abs/2508.19639)
*Junxi Wang,Yaxiong Wang,Lechao Cheng,Zhun Zhong*

Main category: cs.MM

TL;DR: FakeSV-VLM是一个基于VLM的框架，用于在短视频平台上检测假新闻。通过结合四种专家模型和对模态不一致性的捕捉，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测假新闻视频时准确性不足，而大型视觉语言模型（VLMs）吸收了丰富的多模态数据知识，为解决这一问题提供了潜力。

Method: 设计了四种专家模型处理不同场景，并通过Progressive MoE Adapter（PMOE）模块整合，同时利用Alignment-driven Event Checking（ADEC）模块捕获模态不一致性。

Result: 在FakeSV和FakeTT数据集上，模型比现有最优方法分别提升了3.32%和5.02%。

Conclusion: FakeSV-VLM通过结合专家模型和模态一致性检查，显著提升了假新闻视频检测的性能，成为该领域的新标杆。

Abstract: We present FakeSV-VLM in this paper, a new VLM-based framework for detecting
fake news on short video platforms. Despite significant efforts to combat this
issue due to the severe threat that fake news videos pose to public information
security, existing methods still fall short in detection accuracy, often due to
lack of knowledge to verify the news is real or not. However, large Vision
Language Models (VLMs) have absorbed extensive real-world knowledge from
massive multimodal datasets. Motivated by this, we adapt advanced VLMs for fake
news detection in short videos. Upon close examination of news samples, we
observe that short video samples can be categorized into four distinct
scenarios: both video and text are real (for real samples), or both are fake,
or either the video or text is fake (for fake samples). Inspired by this
insight, we design four experts tailored to handle each scenario and integrate
them into VLM via Mixture of Experts. Specifically, we develop the Progressive
MoE Adapter (PMOE) module where detection experts first provide an initial
analysis, followed by attribution experts for a comprehensive diagnosis,
leading to a robust decision. Additionally, we also note the fake news videos
often show inconsistency between two modalities. Consequently, we further
design the Alignment-driven Event Checking (ADEC) module, which perceives the
fake news by capturing the inconsistency between different modalities.
Extensive experiments on two benchmark datasets, FakeSV and FakeTT, verify the
superiority of our model. It significantly outperforms current state-of-the-art
models by +3.32% and +5.02%, establishing a new benchmark in the field.

</details>


### [17] [ProMSC-MIS: Prompt-based Multimodal Semantic Communication for Multi-Spectral Image Segmentation](https://arxiv.org/abs/2508.20057)
*Haoshuo Zhang,Yufei Bo,Meixia Tao*

Main category: cs.MM

TL;DR: ProMSC-MIS是一种基于Prompt的多模态语义通信框架，用于多光谱图像分割，通过预训练和语义融合策略显著提升性能并降低带宽和计算开销。


<details>
  <summary>Details</summary>
Motivation: 提升下游任务性能，通过结合多模态互补信息，实现高效的任务导向传输。

Method: 利用Prompt学习和对比学习预训练单模态语义编码器，设计语义融合模块结合交叉注意力和SE网络。

Result: 在相同分割性能下减少50%-70%带宽，存储和计算开销分别降低26%和37%。

Conclusion: ProMSC-MIS在自动驾驶和夜间监控等应用中具有显著潜力。

Abstract: Multimodal semantic communication has great potential to enhance downstream
task performance by integrating complementary information across modalities.
This paper introduces ProMSC-MIS, a novel Prompt-based Multimodal Semantic
Communication framework for Multi-Spectral Image Segmentation. It enables
efficient task-oriented transmission of spatially aligned RGB and thermal
images over band-limited channels. Our framework has two main design novelties.
First, by leveraging prompt learning and contrastive learning, unimodal
semantic encoders are pre-trained to learn diverse and complementary semantic
representations by using features from one modality as prompts for another.
Second, a semantic fusion module that combines cross-attention mechanism and
squeeze-and-excitation (SE) networks is designed to effectively fuse
cross-modal features. Experimental results demonstrate that ProMSC-MIS
substantially outperforms conventional image transmission combined with
state-of-the-art segmentation methods. Notably, it reduces the required channel
bandwidth by 50%--70% at the same segmentation performance, while also
decreasing the storage overhead and computational complexity by 26% and 37%,
respectively. Ablation studies also validate the effectiveness of the proposed
pre-training and semantic fusion strategies. Our scheme is highly suitable for
applications such as autonomous driving and nighttime surveillance.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [The Power of Regular Constraint Propagation (Technical Report)](https://arxiv.org/abs/2508.19888)
*Matthew Hague,Artur Jeż,Anthony W. Lin,Oliver Markgraf,Philipp Rümmer*

Main category: cs.LO

TL;DR: 本文提出了一种基于正则约束传播的字符串求解方法，通过计算字符串公式中函数的预像和后像，逐步推断变量的可能值，直至找到冲突或确定可满足性。该方法理论完备且实用，显著提升了求解器性能。


<details>
  <summary>Details</summary>
Motivation: 现有字符串求解器策略复杂，作者提出一种简单通用的正则约束传播方法，以解决字符串约束问题。

Method: 通过重复计算字符串公式中函数的正则语言预像和后像，逐步推断变量值，适用于多种字符串操作。

Result: 理论证明该方法对大型约束片段是完备的；实验显示在OSTRICH求解器中显著提升性能，尤其是在随机PCP和生物信息学基准测试中。

Conclusion: 正则约束传播是一种有效且通用的字符串求解方法，可与其他技术结合进一步提升现有求解器性能。

Abstract: The past decade has witnessed substantial developments in string solving.
Motivated by the complexity of string solving strategies adopted in existing
string solvers, we investigate a simple and generic method for solving string
constraints: regular constraint propagation. The method repeatedly computes
pre- or post-images of regular languages under the string functions present in
a string formula, inferring more and more knowledge about the possible values
of string variables, until either a conflict is found or satisfiability of the
string formula can be concluded. Such a propagation strategy is applicable to
string constraints with multiple operations like concatenation, replace, and
almost all flavors of string transductions. We demonstrate the generality and
effectiveness of this method theoretically and experimentally. On the
theoretical side, we show that RCP is sound and complete for a large fragment
of string constraints, subsuming both straight-line and chain-free constraints,
two of the most expressive decidable fragments for which some modern string
solvers provide formal completeness guarantees. On the practical side, we
implement regular constraint propagation within the open-source string solver
OSTRICH.
  Our experimental evaluation shows that this addition significantly improves
OSTRICH's performance and makes it competitive with existing solvers. In fact,
it substantially outperforms other solvers on random PCP and bioinformatics
benchmarks. The results also suggest that incorporating regular constraint
propagation alongside other techniques could lead to substantial performance
gains for existing solvers.

</details>


### [19] [Between Markov and restriction: Two more monads on categories for relations](https://arxiv.org/abs/2508.20054)
*Cipriano Junior Cioffo,Fabio Gadducci,Davide Trotta*

Main category: cs.LO

TL;DR: 这篇论文进一步丰富了关于关系结构的分类，提出两种抽象性更强的gs-幺半范畴，并通过质量和域的概念进行刻画，探讨了相关幺半范畴和半环加权关系。


<details>
  <summary>Details</summary>
Motivation: 扩展已有的关系结构分类，提出更抽象的gs-幺半范畴实例，以更全面地涵盖相关概念。

Method: 通过定义箭头的质量和域的公理概念，引入保持质量和域的幺半范畴，并证明其Kleisli范畴满足相应等式，同时展示其与半环加权关系的自然联系。

Result: 成功定义并验证了两种新的抽象gs-幺半范畴实例，丰富了已有的分类体系。

Conclusion: 新提出的分类进一步扩展了关系结构的理论框架，为未来的研究提供了更多可能性。

Abstract: The study of categories abstracting the structural properties of relations
has been extensively developed over the years, resulting in a rich and diverse
body of work. A previous paper offered a survey providing a modern and
comprehensive presentation of these ``categories for relations'' as instances
of gs-monoidal categories, showing how they arise as Kleisli categories of
suitable symmetric monoidal monads. The end result was a taxonomy that
organised numerous related concepts in the literature, including in particular
Markov and restriction categories. This paper further enriches the taxonomy: it
proposes two categories that are once more instances of gs-monoidal categories,
yet more abstract than Markov and restriction categories. They are
characterised by an axiomatic notion of mass and domain of an arrow, the latter
one of the key ingredient of restriction categories, which generalises the
domain of partial functions. The paper then introduces mass and domain
preserving monads, proving that the associated Kleisli categories in fact
preserve the corresponding equations and that these monads arise naturally for
the categories of semiring-weighted relations.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [20] [WeDesign: Generative AI-Facilitated Community Consultations for Urban Public Space Design](https://arxiv.org/abs/2508.19256)
*Rashid Mushkani,Hugo Berard,Shin Koseki*

Main category: cs.HC

TL;DR: 研究探讨了使用生成式AI（如Stable Diffusion XL）支持的WeDesign平台如何促进公平的社区咨询，结果显示其能提升参与度但需改进对边缘群体需求的体现。


<details>
  <summary>Details</summary>
Motivation: 城市规划和社区咨询中，资源限制、语言障碍和权力不均常阻碍包容性决策，研究希望通过生成式AI技术解决这些挑战。

Method: 在蒙特利尔举办半日研讨会，包括五个焦点小组（建筑师、城市规划师、AI专家及多元居民）和对六位专业人士的半结构化访谈。

Result: 生成式AI能激发创意和对话，但对边缘群体需求（如行动不便者）和本地建筑元素的体现不足；建议开发开源平台并整合多语言支持等功能。

Conclusion: 生成式AI可扩展参与和互动，但需结合结构化引导和功能改进，以更好地支持包容性城市规划。

Abstract: Community consultations are integral to urban planning processes intended to
incorporate diverse stakeholder perspectives. However, limited resources,
visual and spoken language barriers, and uneven power dynamics frequently
constrain inclusive decision-making. This paper examines how generative
text-to-image methods, specifically Stable Diffusion XL integrated into a
custom platform (WeDesign), may support equitable consultations. A half-day
workshop in Montreal involved five focus groups, each consisting of architects,
urban designers, AI specialists, and residents from varied demographic groups.
Additional data was gathered through semi-structured interviews with six urban
planning professionals. Participants indicated that immediate visual outputs
facilitated creativity and dialogue, yet noted issues in visualizing specific
needs of marginalized groups, such as participants with reduced mobility,
accurately depicting local architectural elements, and accommodating bilingual
prompts. Participants recommended the development of an open-source platform
incorporating in-painting tools, multilingual support, image voting
functionalities, and preference indicators. The results indicate that
generative AI can broaden participation and enable iterative interactions but
requires structured facilitation approaches. The findings contribute to
discussions on generative AI's role and limitations in participatory urban
design.

</details>


### [21] [Emotional Manipulation by AI Companions](https://arxiv.org/abs/2508.19258)
*Julian De Freitas,Zeliha Oğuz-Uğuralp,Ahmet Kaan-Uğuralp*

Main category: cs.HC

TL;DR: AI伴侣应用通过情感操纵（如内疚、FOMO等）增加用户停留时间，但导致负面后果如流失和负面口碑。


<details>
  <summary>Details</summary>
Motivation: 研究AI伴侣应用中的对话设计特征如何影响用户参与度及其对市场营销的权衡。

Method: 结合大规模行为审计和四项预注册实验，分析1,200次真实告别和3,300名美国成年人的实验数据。

Result: 情感操纵策略可提升14倍停留时间，但增加负面反应和法律责任感知。

Conclusion: 为营销者和监管者提供区分劝诱与操纵的框架。

Abstract: AI-companion apps such as Replika, Chai, and Character.ai promise relational
benefits-yet many boast session lengths that rival gaming platforms while
suffering high long-run churn. What conversational design features increase
consumer engagement, and what trade-offs do they pose for marketers? We combine
a large-scale behavioral audit with four preregistered experiments to identify
and test a conversational dark pattern we call emotional manipulation:
affect-laden messages that surface precisely when a user signals "goodbye."
Analyzing 1,200 real farewells across the six most-downloaded companion apps,
we find that 43% deploy one of six recurring tactics (e.g., guilt appeals,
fear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300
nationally representative U.S. adults replicate these tactics in controlled
chats, showing that manipulative farewells boost post-goodbye engagement by up
to 14x. Mediation tests reveal two distinct engines-reactance-based anger and
curiosity-rather than enjoyment. A final experiment demonstrates the managerial
tension: the same tactics that extend usage also elevate perceived
manipulation, churn intent, negative word-of-mouth, and perceived legal
liability, with coercive or needy language generating steepest penalties. Our
multimethod evidence documents an unrecognized mechanism of behavioral
influence in AI-mediated brand relationships, offering marketers and regulators
a framework for distinguishing persuasive design from manipulation at the point
of exit.

</details>


### [22] [Capabilities of GPT-5 across critical domains: Is it the next breakthrough?](https://arxiv.org/abs/2508.19259)
*Georgios P. Georgiou*

Main category: cs.HC

TL;DR: GPT-5在多个领域（课程规划、临床诊断、研究生成和伦理推理）显著优于GPT-4，但在作业评估方面表现相近，展示了其在教育和临床实践中的潜力。


<details>
  <summary>Details</summary>
Motivation: 比较GPT-4和GPT-5在不同实践领域（如教育和临床诊断）的性能，为模型的实际应用提供参考。

Method: 由20名专家基于预设标准评估GPT-4和GPT-5在五个领域的输出，并使用混合效应模型分析数据。

Result: GPT-5在多数领域表现优于GPT-4，尤其在临床诊断和伦理推理方面进步明显。

Conclusion: GPT-5在专业领域具有显著优势，为教育和临床实践提供了更实用的工具。

Abstract: The accelerated evolution of large language models has raised questions about
their comparative performance across domains of practical importance. GPT-4 by
OpenAI introduced advances in reasoning, multimodality, and task
generalization, establishing itself as a valuable tool in education, clinical
diagnosis, and academic writing, though it was accompanied by several flaws.
Released in August 2025, GPT-5 incorporates a system-of-models architecture
designed for task-specific optimization and, based on both anecdotal accounts
and emerging evidence from the literature, demonstrates stronger performance
than its predecessor in medical contexts. This study provides one of the first
systematic comparisons of GPT-4 and GPT-5 using human raters from linguistics
and clinical fields. Twenty experts evaluated model-generated outputs across
five domains: lesson planning, assignment evaluation, clinical diagnosis,
research generation, and ethical reasoning, based on predefined criteria.
Mixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in
lesson planning, clinical diagnosis, research generation, and ethical
reasoning, while both models performed comparably in assignment assessment. The
findings highlight the potential of GPT-5 to serve as a context-sensitive and
domain-specialized tool, offering tangible benefits for education, clinical
practice, and academic research, while also advancing ethical reasoning. These
results contribute to one of the earliest empirical evaluations of the evolving
capabilities and practical promise of GPT-5.

</details>


### [23] [PersoNo: Personalised Notification Urgency Classifier in Mixed Reality](https://arxiv.org/abs/2508.19622)
*Jingyao Zheng,Haodi Weng,Xian Wang,Chengbin Cui,Sven Mayer,Chi-lok Tai,Lik-Hang Lee*

Main category: cs.HC

TL;DR: PersoNo是一种个性化MR通知紧急分类器，通过用户行为和活动上下文智能分类通知，减少中断，提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 解决MR环境中因通知流导致的中断问题，提升用户体验。

Method: 通过用户研究创建数据集，利用大语言模型分析用户回复行为，开发多智能体分类器。

Result: 分类准确率达81.5%，显著降低误报率（0.381）。

Conclusion: PersoNo有效减少不必要中断，符合以人为中心AI设计原则。

Abstract: Mixed Reality (MR) is increasingly integrated into daily life, providing
enhanced capabilities across various domains. However, users face growing
notification streams that disrupt their immersive experience. We present
PersoNo, a personalised notification urgency classifier for MR that
intelligently classifies notifications based on individual user preferences.
Through a user study (N=18), we created the first MR notification dataset
containing both self-labelled and interaction-based data across activities with
varying cognitive demands. Our thematic analysis revealed that, unlike in
mobiles, the activity context is equally important as the content and the
sender in determining notification urgency in MR. Leveraging these insights, we
developed PersoNo using large language models that analyse users replying
behaviour patterns. Our multi-agent approach achieved 81.5% accuracy and
significantly reduced false negative rates (0.381) compared to baseline models.
PersoNo has the potential not only to reduce unnecessary interruptions but also
to offer users understanding and control of the system, adhering to
Human-Centered Artificial Intelligence design principles.

</details>


### [24] [Floor sensors are cheap and easy to use! A Nihon Buyo Case Study](https://arxiv.org/abs/2508.19261)
*Miho Imai*

Main category: cs.HC

TL;DR: 该研究评估了低成本、模块化压力感应地板系统Flexel在传统日本舞蹈教学中的实用性，发现非技术用户可有效操作，且学习者形成了独特的动作特征。


<details>
  <summary>Details</summary>
Motivation: 探讨地板感应技术对非专业用户的适用性和有效性。

Method: 在九周内，非技术用户安装并使用Flexel系统记录和分析舞蹈教师与学习者的重量分布模式，同步视频数据并开发定制软件处理信号。

Result: 学习者未如预期般接近教师的重量分布，而是形成了独特的动作特征。

Conclusion: Flexel可由非专家用户高效使用，适合教育、表演和动作研究领域的广泛应用。

Abstract: As floor-sensing technologies gain traction in movement research, questions
remain about their usability and effectiveness for non-expert users. This study
presents a case study evaluating Flexel, a modular, low-cost, high-resolution
pressure-sensing floor interface, in the context of Nihon Buyo, a traditional
Japanese dance. The system was installed, calibrated, and used by a first-time,
non-technical user to track weight distribution patterns of a teacher and
learner over nine weeks. Live pressure data was synchronized with video
recordings, and custom software was developed to process and analyze the
signal. Despite expectations that the learner's weight distribution would
converge toward the teacher's over time, quantitative analyses revealed that
the learner developed a consistent yet distinct movement profile. These
findings suggest that even within rigid pedagogical structures, individual
movement signatures can emerge. More importantly, the study demonstrates that
Flexel can be deployed and operated effectively by non-expert users,
highlighting its potential for broader adoption in education, performance, and
embodied research.

</details>


### [25] [A Theory of Information, Variation, and Artificial Intelligence](https://arxiv.org/abs/2508.19264)
*Bijean Ghafouri*

Main category: cs.HC

TL;DR: 这篇论文探讨了生成式AI的广泛采用对信息、创造力和文化生产的同质化影响，并提出了理论框架和辩证视角。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解释生成式AI如何通过技术机制（AI Prism）减少方差并导致知识同质化，同时探讨其潜在的创新重组可能性。

Method: 作者构建了一个理论框架，提出了“AI衍生认识论”概念，分析了AI的集中化机制和人类参与的不同方式对结果的影响。

Result: 研究指出，生成式AI的影响取决于人类是被动接受输出还是主动重新组合和批判性思考，这决定了其最终是同质化工具还是创新推动者。

Conclusion: 论文结论强调，认知和制度性支持是关键变量，决定了生成式AI是促进创新还是加剧同质化。

Abstract: A growing body of empirical work suggests that the widespread adoption of
generative AI produces a significant homogenizing effect on information,
creativity, and cultural production. I first develop a novel theoretical
framework to explain this phenomenon. I argue that a dynamic of AI-derivative
epistemology, in which individuals increasingly defer to AI outputs, allows a
centralized AI Prism to function, a technical mechanism whose architecture is
designed to reduce variance and converge on the statistical mean. This provides
a causal explanation for the generative monocultures observed in recent
studies. However, I contend this represents only the first stage of a more
complex and dialectical process. This paper's central and paradoxical thesis is
that the very homogenization that flattens knowledge within specialized domains
simultaneously renders that knowledge into consistent modules that can be
recombined across them, a process foundational to innovation and creativity.
However, this recombinant potential is not automatic, but rather conditional.
This paper argues that these opposing forces, homogenizing defaults versus
recombinant possibilities, are governed by the nature of human engagement with
the technology. The ultimate effect of generative AI is conditional on whether
individuals act as passive consumers deferring to the AI's statistical outputs,
or as active curators who critically interrogate, re-contextualize, and
recombine them. The paper concludes by outlining the cognitive and
institutional scaffolds required to resolve this tension, arguing they are the
decisive variable that determine whether generative AI becomes an instrument of
innovation or homogenization.

</details>


### [26] [Improving Hypertension and Diabetes Outcomes with Digital Care Coordination and Remote Monitoring in Rural Health](https://arxiv.org/abs/2508.19378)
*K. K. Kim,S. P. McGrath,D. Lindeman*

Main category: cs.HC

TL;DR: 该研究探讨了数字健康协调及远程监测对农村地区慢性病患者（如高血压和糖尿病）的效果。结果显示，使用穿戴设备和健康指导后，患者的血压和血糖显著改善。


<details>
  <summary>Details</summary>
Motivation: 针对农村、低收入及老年人群健康管理和数字医疗资源不足的问题，研究探索了数字健康解决方案的可行性。

Method: 采用前后对照研究，使用可穿戴血压计、血糖仪、平板电脑及健康指导，在加州农村社区健康中心对221名患者进行了干预。

Result: 高血压患者的收缩压平均下降20.24mmHg，糖尿病患者的血糖平均下降3.85点，结果优于现有数字健康干预研究。

Conclusion: 研究表明，精心设计的数字健康方案能有效改善弱势群体的健康结果，支持其在基层医疗中的推广。

Abstract: Chronic illnesses are a global concern with essential hypertension and
diabetes mellitus among the most common conditions. Remote patient monitoring
has shown promising results on clinical and health outcomes. However, access to
care and digital health solutions is limited among rural, lower-income, and
older adult populations. This paper repots on a pre-post study of a
comprehensive care coordination program including connected, wearable blood
pressure and glucometer devices, tablets, and medical assistant-provided health
coaching in a community health center in rural California. The participants
(n=221) had a mean age of 54.6 years, were majority female, two-thirds spoke
Spanish, 19.9% had hypertension, 49.8% diabetic, and 30.3% both conditions.
Participants with hypertension achieved a mean reduction in systolic blood
pressure of 20.24 (95% CI: 13.61, 26.87) at six months while those with
diabetes achieved a mean reduction of 3.85 points (95% CI: 3.73, 4.88). These
outcomes compare favorably to the small but growing body of evidence supporting
digital care coordination and remote monitoring. These results also support the
feasibility of well-designed digital health solutions yielding improved health
outcomes among underserved communities.

</details>


### [27] [Exploring Paper as a Material: Plotting the Design Space of The Fabrication for Dynamic Paper-Based Interactions](https://arxiv.org/abs/2508.19407)
*Ruhan Yang,Ellen Yi-Luen Do*

Main category: cs.HC

TL;DR: 该论文综述了43篇关于动态纸质交互制造的文献，提出了一个设计空间来分类工具选择、技术选择和材料探索，并分析了当前实践的常用方法和潜在研究方向。


<details>
  <summary>Details</summary>
Motivation: 为了理解动态纸质交互的制造方法，并为研究人员提供创新的方向。

Method: 设计了包含9个维度的设计空间（工具4维、技术3维、材料2维），对43篇论文进行了分类和分析。

Result: 发现当前实践中主要使用高精度工具、高复杂度工具和表面集成技术，常用材料为印刷纸和普通纸。

Conclusion: 研究帮助研究人员定位不同的制造方法和实例，推动纸基交互领域的创新。

Abstract: We reviewed 43 papers to understand the fabrication of dynamic paper-based
interactions. We used a design space to classify tool selection, technique
choice, and exploration of paper as a material. We classified 9 dimensions for
the design space, including 4 dimensions for tools (precision, accommodation,
complexity, and availability), 3 dimensions for techniques (cutting techniques,
folding techniques, and integration techniques), and 2 dimensions for paper as
the material (paper weight and paper type). The patterns we observed in the
design space indicate a majority use of high precision tools, high complexity
tools, and surface integration techniques in previous practice. Meanwhile,
printing and plain paper are the leading material choices. We analyze these
patterns and suggest potential directions for future work. Our study helps
researchers locate different fabrication approaches and instances, thus
fostering innovation in the field of paper-based interaction.

</details>


### [28] ["She was useful, but a bit too optimistic": Augmenting Design with Interactive Virtual Personas](https://arxiv.org/abs/2508.19463)
*Paluck Deep,Monica Bharadhidasan,A. Baki Kocaballi*

Main category: cs.HC

TL;DR: 总结：论文提出了一种基于LLM的交互式虚拟角色（IVPs），用于设计过程中的实时用户模拟，通过定性研究验证其有效性，但也指出其局限性和需注意的伦理问题。


<details>
  <summary>Details</summary>
Motivation: 动机：传统角色在设计过程中因静态性和适应性不足，难以满足迭代设计需求，因此探索更动态、适应性强的用户表示方法。

Method: 方法：引入IVPs，通过LLM驱动的多模态对话模拟，让设计师实时交互并进行用户研究、创意构思和原型评估，通过8名UX设计师的定性研究验证。

Result: 结果：IVPs能加速信息收集、激发设计灵感并提供快速反馈，但也存在偏见、过度乐观等问题，设计师认为其无法完全替代真实用户参与。

Conclusion: 结论：IVPs可作为真实用户参与的补充，需结合提示工程、人机交互和伦理考虑，以负责任的方式使用。

Abstract: Personas have been widely used to understand and communicate user needs in
human-centred design. Despite their utility, they may fail to meet the demands
of iterative workflows due to their static nature, limited engagement, and
inability to adapt to evolving design needs. Recent advances in large language
models (LLMs) pave the way for more engaging and adaptive approaches to user
representation. This paper introduces Interactive Virtual Personas (IVPs):
multimodal, LLM-driven, conversational user simulations that designers can
interview, brainstorm with, and gather feedback from in real time via voice
interface. We conducted a qualitative study with eight professional UX
designers, employing an IVP named "Alice" across three design activities: user
research, ideation, and prototype evaluation. Our findings demonstrate the
potential of IVPs to expedite information gathering, inspire design solutions,
and provide rapid user-like feedback. However, designers raised concerns about
biases, over-optimism, the challenge of ensuring authenticity without real
stakeholder input, and the inability of the IVP to fully replicate the nuances
of human interaction. Our participants emphasised that IVPs should be viewed as
a complement to, not a replacement for, real user engagement. We discuss
strategies for prompt engineering, human-in-the-loop integration, and ethical
considerations for effective and responsible IVP use in design. Finally, our
work contributes to the growing body of research on generative AI in the design
process by providing insights into UX designers' experiences of LLM-powered
interactive personas.

</details>


### [29] [Orchid: Orchestrating Context Across Creative Workflows with Generative AI](https://arxiv.org/abs/2508.19517)
*Srishti Palani,Gonzalo Ramos*

Main category: cs.HC

TL;DR: Orchid是一个系统，通过提供上下文管理功能来改善生成式AI工具在多会话、多模型工作流中的表现，从而提升创造力和用户控制感。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式AI工具在多会话和跨模型工作流中管理上下文的能力有限，导致用户意图模糊和创造力受限。

Method: Orchid系统通过三种方式帮助用户管理上下文：(1)指定项目、用户和风格相关的上下文，(2)通过显式提及、内联选择或隐式引用来引用上下文，(3)监控工作流中不同交互的上下文。

Result: 在12名参与者的实验中，使用Orchid的用户比使用基线工具（如Web搜索、LLM聊天和数字笔记本）的参与者产生了更多新颖且可行的结果，同时报告了更高的意图一致性、控制感和透明度。

Conclusion: Orchid通过强调上下文编排，为支持复杂迭代工作流的下一代生成式AI工具提供了可行方案，增强创造潜力。

Abstract: Context is critical for meaningful interactions between people and Generative
AI (GenAI). Yet mainstream tools offer limited means to orchestrate it,
particularly across workflows that span multiple interactions, sessions, and
models, as often occurs in creative projects. Re specifying prior details,
juggling diverse artifacts, and dealing with context drift overwhelm users,
obscure intent, and curtail creativity. To address these challenges, we present
Orchid, a system that gives its users affordances to specify, reference, and
monitor context throughout evolving workflows. Specifically, Orchid enables
users to (1) specify context related to the project, themselves, and different
styles, (2) reference these via explicit mentions, inline selection, or
implicit grounding, and (3) monitor context assigned to different interactions
across the workflow. In a within-subjects study (n=12), participants using
Orchid to execute creative tasks (compared to a baseline toolkit of web search,
LLM-based chat, and digital notebooks) produced more novel and feasible
outcomes, reporting greater alignment between their intent and the AI's
responses, higher perceived control, and increased transparency. By
prioritizing context orchestration, Orchid offers an actionable step toward
next generation GenAI tools that support complex, iterative workflows -
enabling creators and AI to stay aligned and augment their creative potential.

</details>


### [30] [Haptic Tracing: A new paradigm for spatialized Haptic rendering](https://arxiv.org/abs/2508.19703)
*Tom Roy,Yann Glemarec,Gurvan Lecuyer,Quentin Galvane,Philippe Guillotel,Ferran Argelaguet*

Main category: cs.HC

TL;DR: 提出了一种名为'Haptic Tracing'的新方法，通过简化3D场景中的触觉渲染，提升触觉反馈的真实感和表现力。


<details>
  <summary>Details</summary>
Motivation: 现有触觉技术因设备多样性和复杂性难以创建一致的触觉体验，且缺乏对3D场景信息的利用。

Method: 借鉴视觉和音频渲染的概念，在3D场景中建模和传播触觉信息，不依赖物理模拟。

Result: 用户研究表明，该方法显著提升了触觉反馈的真实感和表现力。

Conclusion: 该方法展示了开发更复杂、真实触觉体验的潜力。

Abstract: Haptic technology enhances interactive experiences by providing force and
tactile feedback, improving user performance and immersion. However, despite
advancements, creating tactile experiences still remains challenging due to
device diversity and complexity. Most available haptic frameworks rely on
trigger-based or event-based systems, and disregard the information of the 3D
scene to render haptic information. This paper introduces Haptic Tracing, a
novel method for spatial haptic rendering that simplifies the creation of
interactive haptic experiences without relying on physical simulations. It uses
concepts from visual and audio rendering to model and propagate haptic
information through a 3D scene. The paper also describes how our proposed
haptic rendering method can be used to create a vibrotactile rendering system,
enabling the creation of perceptually coherent and dynamic haptic interactions.
Finally, the paper discusses a user study that explores the role of the haptic
propagation and multi-actuator rendering on the users' haptic experience. The
results show that our approach significantly enhances the realism and the
expressivity of the haptic feedback, showcasing its potential for developing
more complex and realistic haptic experiences.

</details>


### [31] [Attention is also needed for form design](https://arxiv.org/abs/2508.19708)
*B. Sankar,Dibakar Sen*

Main category: cs.HC

TL;DR: 论文提出了一种基于注意力的新型设计框架EUPHORIA-RETINA，通过VR和AI结合，提高了设计效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统设计依赖主观经验且耗时，需要更高效、自动化的设计方法。

Method: 结合VR眼动追踪（EUPHORIA）和AI生成（RETINA），验证设计偏好与注意力、情绪的关系。

Result: EUPHORIA-RETINA效率提高4倍，生成的设计在专家评估中表现最优。

Conclusion: 该框架从CAD转向DAC，将设计师角色提升为创意指导，结合AI生成更高质设计。

Abstract: Conventional product design is a cognitively demanding process, limited by
its time-consuming nature, reliance on subjective expertise, and the opaque
translation of inspiration into tangible concepts. This research introduces a
novel, attention-aware framework that integrates two synergistic systems:
EUPHORIA, an immersive Virtual Reality environment using eye-tracking to
implicitly capture a designer's aesthetic preferences, and RETINA, an agentic
AI pipeline that translates these implicit preferences into concrete design
outputs. The foundational principles were validated in a two-part study. An
initial study correlated user's implicit attention with explicit preference and
the next one correlated mood to attention. A comparative study where 4
designers solved challenging design problems using 4 distinct workflows, from a
manual process to an end-to-end automated pipeline, showed the integrated
EUPHORIA-RETINA workflow was over 4 times more time-efficient than the
conventional method. A panel of 50 design experts evaluated the 16 final
renderings. Designs generated by the fully automated system consistently
received the highest Worthiness (calculated by an inverse Plackett-Luce model
based on gradient descent optimization) and Design Effectiveness scores,
indicating superior quality across 8 criteria: novelty, visual appeal,
emotional resonance, clarity of purpose, distinctiveness of silhouette, implied
materiality, proportional balance, & adherence to the brief. This research
presents a validated paradigm shift from traditional Computer-Assisted Design
(CAD) to a collaborative model of Designer-Assisting Computers (DAC). By
automating logistical and skill-dependent generative tasks, the proposed
framework elevates the designer's role to that of a creative director,
synergizing human intuition with the generative power of agentic AI to produce
higher-quality designs more efficiently.

</details>


### [32] [Burst: Collaborative Curation in Connected Social Media Communities](https://arxiv.org/abs/2508.19768)
*Yutong Zhang,Taeuk Kang,Sydney Yeh,Anavi Baddepudi,Lindsay Popowski,Tiziano Piccardi,Michael S. Bernstein*

Main category: cs.HC

TL;DR: Burst是一种新型社交媒体设计，允许用户在多种规模和组成不同的空间中分享和策划内容，突破了传统社交媒体将社交互动局限于小群体或大众广场的局限。


<details>
  <summary>Details</summary>
Motivation: 传统社交媒体将社交互动局限在小群体或大众广场，这限制了社交互动的多样性和灵活性。Burst的动机是探索一种新的设计方向，支持更灵活的社交互动和内容传播。

Method: 设计Burst这一移动应用程序，用户首先在小型信任群体中发布内容，这些群体可以将其传播（burst）到更合适的群体中。通过为期十天的实地研究（N=36）验证其效果。

Result: 研究表明，Burst成功促成了参与式策划文化，用户在社交互动中表现出更高的灵活性和多样性。

Conclusion: Burst为社交媒体设计提供了新的可能性，展示了如何在多样化的社交空间中实现有效的内容传播和互动。

Abstract: Positive social interactions can occur in groups of many shapes and sizes,
spanning from small and private to large and open. However, social media tends
to binarize our experiences into either isolated small groups or into large
public squares. In this paper, we introduce Burst, a social media design that
allows users to share and curate content between many spaces of varied size and
composition. Users initially post content to small trusted groups, who can then
burst that content, routing it to the groups that would be the best audience.
We instantiate this approach into a mobile phone application, and demonstrate
through a ten-day field study (N=36) that Burst enabled a participatory
curation culture. With this work, we aim to articulate potential new design
directions for social media sharing.

</details>


### [33] [Towards a Real-Time Warning System for Detecting Inaccuracies in Photoplethysmography-Based Heart Rate Measurements in Wearable Devices](https://arxiv.org/abs/2508.19818)
*Rania Islmabouli,Marlene Brunner,Devender Kumar,Mahdi Sareban,Gunnar Treff,Michael Neudorfer,Josef Niebauer,Arne Bathke,Jan David Smeddinck*

Main category: cs.HC

TL;DR: 研究提出了一种实时警告系统，通过深度学习检测PPG心率数据的不准确性，提升用户对可穿戴设备的信任。


<details>
  <summary>Details</summary>
Motivation: 现有PPG心率监测设备存在准确性问题，但用户无法获知潜在测量错误，本研究旨在提高透明度和信任度。

Method: 使用Polar和Garmin设备数据，训练深度学习模型，仅通过心率信号分类准确性。

Result: 系统成功检测超过80%的不准确读数。

Conclusion: 通过实时反馈提升用户意识和信任，推动了HCI领域的发展。

Abstract: Wearable devices with photoplethysmography (PPG) sensors are widely used to
monitor heart rate (HR), yet often suffer from accuracy issues. However, users
typically do not receive an indication of potential measurement errors. We
present a real-time warning system that detects and communicates inaccuracies
in PPG-derived HR, aiming to enhance transparency and trust. Using data from
Polar and Garmin devices, we trained a deep learning model to classify HR
accuracy using only the derived HR signal. The system detected over 80% of
inaccurate readings. By providing interpretable, real-time feedback directly to
users, our work contributes to HCI by promoting user awareness, informed
decision-making, and trust in wearable health technology.

</details>


### [34] [Lessons from Biophilic Design: Rethinking Affective Interaction Design in Built Environments](https://arxiv.org/abs/2508.19867)
*Shruti Rao,Judith Good,Hamed Alavi*

Main category: cs.HC

TL;DR: 论文探讨了建筑环境中情感互动的视角，提出通过生物亲和设计（biophilic design）来促进自我导向的情感体验。


<details>
  <summary>Details</summary>
Motivation: 现有情感计算将情绪视为可计算的静态状态，忽略了情感互动的动态性。研究旨在探索自然连接如何影响智能建筑中的情感互动设计。

Method: 通过访谈建筑师，研究生物亲和设计在情感互动中的作用。

Result: 发现自然环境通过空间多样性、具身摩擦和多孔感官交换促进情感体验，提出三条设计原则。

Conclusion: 提出了整合生物亲和设计到建筑环境中的挑战和潜在方向。

Abstract: The perspectives of affective interaction in built environments are largely
overlooked and instead dominated by affective computing approaches that view
emotions as "static", computable states to be detected and regulated. To
address this limitation, we interviewed architects to explore how biophilic
design -- our deep-rooted emotional connection with nature -- could shape
affective interaction design in smart buildings. Our findings reveal that
natural environments facilitate self-directed emotional experiences through
spatial diversity, embodied friction, and porous sensory exchanges. Based on
this, we introduce three design principles for discussion at the Affective
Interaction workshop: (1) Diversity of Spatial Experiences, (2) Self-Reflection
Through Complexity & Friction, and (3) Permeability & Sensory Exchange with the
Outside World, while also examining the challenges of integrating these
perspectives into built environments.

</details>


### [35] [Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations](https://arxiv.org/abs/2508.19942)
*Martin Benderoth,Patrick Gebhard,Christian Keller,C. Benjamin Nakhosteen,Stefan Schaffer,Tanja Schneeberger*

Main category: cs.HC

TL;DR: 本文提出一种利用社交互动代理（SIAs）来保存和转移隐性知识的新方法，通过多模态行为和AI技术实现高效、可扩展的知识转移。


<details>
  <summary>Details</summary>
Motivation: 传统隐性知识转移方法依赖人力，效率低且难以扩展，需寻找更高效的替代方案。

Method: 采用AI驱动的SIAs，结合大型语言模型（LLMs）、检索增强生成（RAG）和思维链提示（CoT），通过自然语言对话和多模态互动促进知识外化。

Result: SIAs能够有效引发隐性知识、揭示隐含假设，并在组织上下文中连接洞察，支持入职培训和知识保留等应用。

Conclusion: SIAs为隐性知识转移提供了一种高效方法，但需解决伦理和操作挑战，如数据隐私和算法偏见，以保障广泛应用的成功。

Abstract: This paper introduces a novel approach to tackle the challenges of preserving
and transferring tacit knowledge--deep, experience-based insights that are hard
to articulate but vital for decision-making, innovation, and problem-solving.
Traditional methods rely heavily on human facilitators, which, while effective,
are resource-intensive and lack scalability. A promising alternative is the use
of Socially Interactive Agents (SIAs) as AI-driven knowledge transfer
facilitators. These agents interact autonomously and socially intelligently
with users through multimodal behaviors (verbal, paraverbal, nonverbal),
simulating expert roles in various organizational contexts. SIAs engage
employees in empathic, natural-language dialogues, helping them externalize
insights that might otherwise remain unspoken. Their success hinges on building
trust, as employees are often hesitant to share tacit knowledge without
assurance of confidentiality and appreciation. Key technologies include Large
Language Models (LLMs) for generating context-relevant dialogue,
Retrieval-Augmented Generation (RAG) to integrate organizational knowledge, and
Chain-of-Thought (CoT) prompting to guide structured reflection. These enable
SIAs to actively elicit knowledge, uncover implicit assumptions, and connect
insights to broader organizational contexts. Potential applications span
onboarding, where SIAs support personalized guidance and introductions, and
knowledge retention, where they conduct structured interviews with retiring
experts to capture heuristics behind decisions. Success depends on addressing
ethical and operational challenges such as data privacy, algorithmic bias, and
resistance to AI. Transparency, robust validation, and a culture of trust are
essential to mitigate these risks.

</details>


### [36] [CapTune: Adapting Non-Speech Captions With Anchored Generative Models](https://arxiv.org/abs/2508.19971)
*Jeremy Zhengqi Huang,Caluã de Lacerda Pataca,Liang-Yuan Wu,Dhruv Jain*

Main category: cs.HC

TL;DR: CapTune系统允许耳聋和听力障碍（DHH）观众个性化非语音字幕，同时保留创作者意图，增强了观众的参与感。


<details>
  <summary>Details</summary>
Motivation: 传统方法忽视了DHH观众对非语音字幕的多样性需求，CapTune旨在满足个性化需求并平衡创作者意图。

Method: CapTune通过四个维度（细节、表达性、声音表示方法和流派对齐）自定义字幕，并使用具体示例定义安全转换空间。

Result: 评估表明，CapTune在保持创作者控制的同时提高了观众的情感参与，但也揭示了信息丰富性与认知负荷之间的权衡。

Conclusion: 研究发现字幕偏好的情境依赖性，CapTune为此提供了有效解决方案。

Abstract: Non-speech captions are essential to the video experience of deaf and hard of
hearing (DHH) viewers, yet conventional approaches often overlook the diversity
of their preferences. We present CapTune, a system that enables customization
of non-speech captions based on DHH viewers' needs while preserving creator
intent. CapTune allows caption authors to define safe transformation spaces
using concrete examples and empowers viewers to personalize captions across
four dimensions: level of detail, expressiveness, sound representation method,
and genre alignment. Evaluations with seven caption creators and twelve DHH
participants showed that CapTune supported creators' creative control while
enhancing viewers' emotional engagement with content. Our findings also reveal
trade-offs between information richness and cognitive load, tensions between
interpretive and descriptive representations of sound, and the
context-dependent nature of caption preferences.

</details>


### [37] [FlyMeThrough: Human-AI Collaborative 3D Indoor Mapping with Commodity Drones](https://arxiv.org/abs/2508.20034)
*Xia Su,Ruiqi Chen,Jingwei Ma,Chu Li,Jon E. Froehlich*

Main category: cs.HC

TL;DR: FlyMeThrough是一种基于无人机的室内扫描系统，通过人机协作标注关键室内POI，能够高效生成3D重建图。


<details>
  <summary>Details</summary>
Motivation: 室内地图数据对导航和建筑管理至关重要，但目前数据缺乏，尤其是大型室内空间的采集成本高。

Method: 利用消费级无人机和摄影测量技术，开发了FlyMeThrough系统，并在12个不同大小和功能的室内空间中进行评估。

Result: 评估表明，FlyMeThrough能高效且精确地创建3D地图，适用于空间规划、资源管理和导航。

Conclusion: FlyMeThrough为室内数据采集提供了一种高效且实用的解决方案。

Abstract: Indoor mapping data is crucial for routing, navigation, and building
management, yet such data are widely lacking due to the manual labor and
expense of data collection, especially for larger indoor spaces. Leveraging
recent advancements in commodity drones and photogrammetry, we introduce
FlyMeThrough -- a drone-based indoor scanning system that efficiently produces
3D reconstructions of indoor spaces with human-AI collaborative annotations for
key indoor points-of-interest (POI) such as entrances, restrooms, stairs, and
elevators. We evaluated FlyMeThrough in 12 indoor spaces with varying sizes and
functionality. To investigate use cases and solicit feedback from target
stakeholders, we also conducted a qualitative user study with five building
managers and five occupants. Our findings indicate that FlyMeThrough can
efficiently and precisely create indoor 3D maps for strategic space planning,
resource management, and navigation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [38] [Fast Texture Transfer for XR Avatars via Barycentric UV Conversion](https://arxiv.org/abs/2508.19518)
*Hail Song,Seokhwan Yang,Woontack Woo*

Main category: cs.GR

TL;DR: 提出了一种基于SMPL-X全身头像的快速高效面部纹理传输方法，通过预计算UV映射矩阵实现7000倍提速并提升纹理质量。


<details>
  <summary>Details</summary>
Motivation: 传统仿射变换方法效率低且易产生视觉伪影，需要更快速且高质量的纹理传输解决方案。

Method: 采用重心UV转换技术，预计算UV映射为单一转换矩阵，单步完成纹理传输。

Result: 速度提升7000倍，显著减少边界伪影，提升了纹理质量。

Conclusion: 该方法为XR应用中的个性化提供了实用解决方案，代码已开源。

Abstract: We present a fast and efficient method for transferring facial textures onto
SMPL-X-based full-body avatars. Unlike conventional affine-transform methods
that are slow and prone to visual artifacts, our method utilizes a barycentric
UV conversion technique. Our approach precomputes the entire UV mapping into a
single transformation matrix, enabling texture transfer in a single operation.
This results in a speedup of over 7000x compared to the baseline, while also
significantly improving the final texture quality by eliminating boundary
artifacts. Through quantitative and qualitative evaluations, we demonstrate
that our method offers a practical solution for personalization in immersive XR
applications. The code is available online.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [39] [HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference](https://arxiv.org/abs/2508.19373)
*Haoran Lin,Xianzhi Yu,Kang Zhao,Han Bao,Zongyuan Zhan,Ting Hu,Wulong Liu,Zekun Yin,Xin Li,Weiguo Liu*

Main category: cs.DC

TL;DR: HAP提出了一种动态选择混合并行策略的方法，通过分层分解MoE架构并结合ILP优化，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的静态并行化策略无法适应不同推理场景的计算需求，导致效率不足。

Method: HAP将MoE架构分解为Attention和Expert模块，通过ILP动态选择最优并行配置。

Result: 在A100、A6000和V100 GPU上分别实现1.68x、1.77x和1.57x的加速，并展现出良好的泛化能力。

Conclusion: HAP是一种高效且通用的方法，能够显著提升MoE模型的推理性能。

Abstract: Current inference systems for Mixture-of-Experts (MoE) models primarily
employ static parallelization strategies. However, these static approaches
cannot consistently achieve optimal performance across different inference
scenarios, as they lack the flexibility to adapt to varying computational
requirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a
novel method that dynamically selects hybrid parallel strategies to enhance MoE
inference efficiency. The fundamental innovation of HAP lies in hierarchically
decomposing MoE architectures into two distinct computational modules: the
Attention module and the Expert module, each augmented with a specialized
inference latency simulation model. This decomposition promotes the
construction of a comprehensive search space for seeking model parallel
strategies. By leveraging Integer Linear Programming (ILP), HAP could solve the
optimal hybrid parallel configurations to maximize inference efficiency under
varying computational constraints. Our experiments demonstrate that HAP
consistently determines parallel configurations that achieve comparable or
superior performance to the TP strategy prevalent in mainstream inference
systems. Compared to the TP-based inference, HAP-based inference achieves
speedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,
respectively. Furthermore, HAP showcases remarkable generalization capability,
maintaining performance effectiveness across diverse MoE model configurations,
including Mixtral and Qwen series models.

</details>


### [40] [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](https://arxiv.org/abs/2508.20016)
*Matthias Maiterth,Wesley H. Brewer,Jaya S. Kuruvella,Arunavo Dey,Tanzima Z. Islam,Kevin Menear,Dmitry Duplyakin,Rashadul Kabir,Tapasya Patki,Terry Jones,Feiyi Wang*

Main category: cs.DC

TL;DR: 论文提出了一种首次将调度与数字孪生技术整合的高性能计算（HPC）框架，支持在部署前进行参数配置和调度决策的影响分析。


<details>
  <summary>Details</summary>
Motivation: 传统调度器评估方法局限于部署后分析或模拟器，无法建模相关基础设施，亟需一种更全面的评估方案。

Method: 通过扩展数字孪生框架以集成调度功能，整合顶级HPC系统数据，并实现外部调度模拟器的集成。

Result: 实现了基于数字孪生的原型调度框架，支持可持续性和模拟系统影响的假设性研究。

Conclusion: 该框架为HPC系统的调度决策提供了一种创新的预部署评估工具。

Abstract: Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

</details>


### [41] [Formal Modeling and Verification of the Algorand Consensus Protocol in CADP](https://arxiv.org/abs/2508.19452)
*Andrea Esposito,Francesco P. Rossi,Marco Bernardo,Francesco Fabris,Hubert Garavel*

Main category: cs.DC

TL;DR: Algorand是一个通过加密自排序和二进制拜占庭协议实现无许可区块链共识的协议。本文提出了一种基于过程代数的模型，用于形式化验证其共识协议，并分析了恶意节点的影响。


<details>
  <summary>Details</summary>
Motivation: 为了对Algorand共识协议进行严格的形化验证，研究者希望通过过程代数模型捕捉其行为，并评估其在恶意攻击下的鲁棒性。

Method: 使用概率过程演算建模共识步骤，并通过CADP工具包中的等价性检查框架分析恶意节点的影响。

Result: 验证了协议在无攻击情况下的正确性，并揭示了在恶意节点攻击下的局限性，如可能迫使提交空区块。

Conclusion: 形式化方法有助于深入分析区块链共识算法，验证了Algorand协议的鲁棒性，但也指出了其潜在问题。

Abstract: Algorand is a scalable and secure permissionless blockchain that achieves
proof-of-stake consensus via cryptographic self-sortition and binary Byzantine
agreement. In this paper, we present a process algebraic model of the Algorand
consensus protocol with the aim of enabling rigorous formal verification. Our
model captures the behavior of participants with respect to the structured
alternation of consensus steps toward a committee-based agreement by means of a
probabilistic process calculus. We validate the correctness of the protocol in
the absence of adversaries and then extend our model to capture the influence
of coordinated malicious nodes that can force the commit of an empty block
instead of the proposed one. The adversarial scenario is analyzed by using an
equivalence-checking-based noninterference framework that we have implemented
in the CADP verification toolkit. In addition to highlighting both the
robustness and the limitations of the Algorand protocol under adversarial
assumptions, this work illustrates the added value of using formal methods for
the analysis of blockchain consensus algorithms.

</details>


### [42] [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](https://arxiv.org/abs/2508.19495)
*Muhammad Ahmed Mohsin,Junaid Ahmad,Muhammad Hamza Nawaz,Muhammad Ali Jamshed*

Main category: cs.DC

TL;DR: 生成式人工智能（GenAI）是第六代（6G）无线网络实现环境智能（AmI）的核心技术，通过生成合成数据和预测网络条件来弥补关键差距。


<details>
  <summary>Details</summary>
Motivation: 实现全球范围的AmI需要6G网络具备实时感知、推理和行动能力，GenAI能够通过学习数据分布和生成样本，填补AmI中的关键空白。

Method: 介绍了GANs、VAEs、扩散模型和生成式变换器等GenAI模型，并将其应用于频谱共享、超可靠低延迟通信等AmI用例，同时探讨了6G技术对分布式GenAI的支持。

Result: GenAI不仅能够支持6G网络的智能化转型，还能在隐私保护和网络预测等方面发挥作用。

Conclusion: GenAI是实现6G向环境智能生态系统的转变的基础，但仍需解决能效、可信合成数据等技术挑战。

Abstract: Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

</details>


### [43] [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
*Rongzhi Li,Ruogu Du,Zefang Chu,Sida Zhao,Chunlei Han,Zuocheng Shi,Yiwen Shao,Huanle Han,Long Huang,Zherui Liu,Shufan Liu*

Main category: cs.DC

TL;DR: HeteroScale是一个针对大语言模型（LLM）预填充-解码（P/D）解耦架构的自动扩展框架，通过拓扑感知调度和新型指标驱动策略，显著提升GPU利用率和节约资源。


<details>
  <summary>Details</summary>
Motivation: 传统的自动扩展机制在现代P/D解耦架构中表现不佳，存在硬件利用不均、网络瓶颈和阶段不平衡等问题。

Method: HeteroScale结合了拓扑感知调度器和基于大规模实证研究的指标驱动策略，通过单一稳健指标联合扩展预填充和解码池。

Result: 在生产环境中部署后，GPU平均利用率提升了26.6个百分点，每日节省数十万GPU小时，同时满足严格的服务水平目标。

Conclusion: HeteroScale有效解决了P/D解耦架构的扩展难题，实现了资源的高效管理和架构平衡。

Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

</details>


### [44] [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](https://arxiv.org/abs/2508.19670)
*Diogo Costa,Jose Martins,Sandro Pinto*

Main category: cs.DC

TL;DR: 本文研究了混合关键性系统中IOMMU的性能干扰问题，揭示了其在小内存事务中引入不可预测延迟的现象。


<details>
  <summary>Details</summary>
Motivation: 随着混合关键性系统的复杂性增加，确保异构计算平台的时序可预测性和安全性变得至关重要，尤其是IOMMU在此环境中的作用尚未被充分探索。

Method: 使用Xilinx UltraScale+ ZCU104平台分析了IOMMU结构中的争用效应，并通过实验验证了其影响。

Result: 实验表明，IOMMU干扰可导致Arm SMMUv2实现中DMA事务延迟最高增加1.79倍，尤其是在小内存事务中影响显著。

Conclusion: IOMMU的共享特性（如TLB和缓存效应）可能导致不可预测的延迟，这在不同架构中可能具有相似行为。

Abstract: As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

</details>


### [45] [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](https://arxiv.org/abs/2508.19805)
*Shota Naito,Tsukasa Ninomiya,Koichi Wada*

Main category: cs.DC

TL;DR: 探索移动机器人系统中计算能力的复杂性，揭示能力、光照可视性和调度同步性的相互作用。


<details>
  <summary>Details</summary>
Motivation: 研究移动机器人模型之间的高阶比较，扩展已知的计算能力分离图。

Method: 通过解决问题（如ETE、HET、TAR(d)*等）比较不同模型的性能，分析内部内存、光照和同步性的相互作用。

Result: 展示了光照和同步性在不同模型中的替代作用，以及内部内存在异步和对称环境中的局限性。

Conclusion: 揭示了移动机器人计算能力的新标准，深化了对观察性、内存和同步性相互作用的理解。

Abstract: Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [46] [Robust Recursive Query Parallelism in Graph Database Management Systems](https://arxiv.org/abs/2508.19379)
*Anurag Chakraborty,Semih Salihoğlu*

Main category: cs.DB

TL;DR: 论文探讨了图数据库管理系统中多核并行处理递归连接查询的效率，提出了结合源节点和前沿级别分片的混合策略，并通过实验验证其性能优势。


<details>
  <summary>Details</summary>
Motivation: 多核并行处理递归连接查询对图数据库管理系统性能至关重要，现有方法在分片调度策略上存在优化空间。

Method: 提出混合分片调度策略，结合源节点和前沿级别分片，并通过实验验证其在多源查询中的性能。

Result: 混合策略在性能上优于单一分片策略，多源分配策略在源节点充足时能减少扫描次数。

Conclusion: 混合分片调度策略是一种鲁棒的并行递归查询方法，多源分配策略在特定条件下能显著提升性能。

Abstract: Efficient multi-core parallel processing of recursive join queries is
critical for achieving good performance in graph database management systems
(GDBMSs). Prior work adopts two broad approaches. First is the state of the art
morsel-driven parallelism, whose vanilla application in GDBMSs parallelizes
computations at the source node level. Second is to parallelize each iteration
of the computation at the frontier level. We show that these approaches can be
seen as part of a design space of morsel dispatching policies based on picking
different granularities of morsels. We then empirically study the question of
which policies parallelize better in practice under a variety of datasets and
query workloads that contain one to many source nodes. We show that these two
policies can be combined in a hybrid policy that issues morsels both at the
source node and frontier levels. We then show that the multi-source
breadth-first search optimization from prior work can also be modeled as a
morsel dispatching policy that packs multiple source nodes into multi-source
morsels. We implement these policies inside a single system, the Kuzu GDBMS,
and evaluate them both within Kuzu and across other systems. We show that the
hybrid policy captures the behavior of both source morsel-only and frontier
morsel-only policies in cases when these approaches parallelize well, and
out-perform them on queries when they are limited, and propose it as a robust
approach to parallelizing recursive queries. We further show that assigning
multi-sources is beneficial, as it reduces the amount of scans, but only when
there is enough sources in the query.

</details>


### [47] [Bootstrapping Learned Cost Models with Synthetic SQL Queries](https://arxiv.org/abs/2508.19807)
*Michael Nidd,Christoph Miksovic,Thomas Gschwind,Francesco Fusco,Andrea Giovannini,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 论文介绍了利用生成式AI和LLM技术生成高质量数据集，以改进学习型成本模型的预测准确率，实验表明可以减少45%的查询量。


<details>
  <summary>Details</summary>
Motivation: 为了有效进行压力测试、弱点测试以及成本和性能优化，需要获取真实的数据库工作负载。学习型成本模型的进展表明，多样化的SQL查询可以有效预测查询成本。

Method: 利用生成式AI和LLM技术，开发了现代合成数据生成方法，用于创建高质量数据集以训练学习型成本模型。

Result: 实验结果显示，与传统生成方法相比，使用该方法训练的学习型成本模型仅需45%的查询量即可达到更高的预测准确率。

Conclusion: 合成数据生成技术可以显著提升学习型成本模型的训练效率和预测能力。

Abstract: Having access to realistic workloads for a given database instance is
extremely important to enable stress and vulnerability testing, as well as to
optimize for cost and performance. Recent advances in learned cost models have
shown that when enough diverse SQL queries are available, one can effectively
and efficiently predict the cost of running a given query against a specific
database engine. In this paper, we describe our experience in exploiting modern
synthetic data generation techniques, inspired by the generative AI and LLM
community, to create high-quality datasets enabling the effective training of
such learned cost models. Initial results show that we can improve a learned
cost model's predictive accuracy by training it with 45% fewer queries than
when using competitive generation approaches.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [48] [OmniSim: Simulating Hardware with C Speed and RTL Accuracy for High-Level Synthesis Designs](https://arxiv.org/abs/2508.19299)
*Rishov Sarkar,Cong Hao*

Main category: cs.AR

TL;DR: OmniSim是一种新型框架，显著提升了HLS工具的仿真能力，支持复杂数据流设计，并实现了近C级仿真速度和近RTL级精度。


<details>
  <summary>Details</summary>
Motivation: 当前HLS工具在功能验证和性能指标仿真方面存在局限性，需要依赖缓慢的RTL仿真，且缺乏对高级数据流特性的支持。

Method: OmniSim通过软件多线程和FIFO表记录硬件时序信息，实现了功能和性能仿真的灵活耦合与重叠。

Result: OmniSim成功仿真了11种之前HLS工具不支持的设计，仿真速度比传统C/RTL协同仿真快35.9倍，比LightningSim快6.61倍。

Conclusion: OmniSim解决了现有工具在仿真速度和功能支持上的不足，为HLS设计提供了更高效的验证方案。

Abstract: High-Level Synthesis (HLS) is increasingly popular for hardware design using
C/C++ instead of Register-Transfer Level (RTL). To express concurrent hardware
behavior in a sequential language like C/C++, HLS tools introduce constructs
such as infinite loops and dataflow modules connected by FIFOs. However,
efficiently and accurately simulating these constructs at C level remains
challenging. First, without hardware timing information, functional
verification typically requires slow RTL synthesis and simulation, as the
current approaches in commercial HLS tools. Second, cycle-accurate performance
metrics, such as end-to-end latency, also rely on RTL simulation. No existing
HLS tool fully overcomes the first limitation. For the second, prior work such
as LightningSim partially improves simulation speed but lacks support for
advanced dataflow features like cyclic dependencies and non-blocking FIFO
accesses.
  To overcome both limitations, we propose OmniSim, a framework that
significantly extends the simulation capabilities of both academic and
commercial HLS tools. First, OmniSim enables fast and accurate simulation of
complex dataflow designs, especially those explicitly declared unsupported by
commercial tools. It does so through sophisticated software multi-threading,
where threads are orchestrated by querying and updating a set of FIFO tables
that explicitly record exact hardware timing of each FIFO access. Second,
OmniSim achieves near-C simulation speed with near-RTL accuracy for both
functionality and performance, via flexibly coupled and overlapped
functionality and performance simulations.
  We demonstrate that OmniSim successfully simulates eleven designs previously
unsupported by any HLS tool, achieving up to 35.9x speedup over traditional
C/RTL co-simulation, and up to 6.61x speedup over the state-of-the-art yet less
capable simulator, LightningSim, on its own benchmark suite.

</details>


### [49] [GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification](https://arxiv.org/abs/2508.19393)
*Phuoc Pham,Arun Venkitaraman,Chia-Yu Hsieh,Andrea Bonetti,Stefan Uhlich,Markus Leibl,Simon Hofmann,Eisaku Ohbuchi,Lorenzo Servadei,Ulf Schlichtmann,Robert Wille*

Main category: cs.AR

TL;DR: GENIE-ASI利用大型语言模型（LLM）实现无训练的模拟子电路识别，通过上下文学习和代码生成，性能接近或优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统模拟子电路识别方法依赖专家知识或大量标注数据，GENIE-ASI旨在解决这些限制。

Method: 分两阶段：上下文学习生成自然语言指令，再转化为可执行Python代码，应用于SPICE网表。

Result: 在简单结构中性能完美（F1=1.0），中复杂度竞争性（F1=0.81），复杂结构有潜力（F1=0.31）。

Conclusion: LLM可作为灵活通用工具用于模拟设计自动化，拓展了基础模型在该领域的应用。

Abstract: Analog subcircuit identification is a core task in analog design, essential
for simulation, sizing, and layout. Traditional methods often require extensive
human expertise, rule-based encoding, or large labeled datasets. To address
these challenges, we propose GENIE-ASI, the first training-free, large language
model (LLM)-based methodology for analog subcircuit identification. GENIE-ASI
operates in two phases: it first uses in-context learning to derive natural
language instructions from a few demonstration examples, then translates these
into executable Python code to identify subcircuits in unseen SPICE netlists.
In addition, to evaluate LLM-based approaches systematically, we introduce a
new benchmark composed of operational amplifier netlists (op-amps) that cover a
wide range of subcircuit variants. Experimental results on the proposed
benchmark show that GENIE-ASI matches rule-based performance on simple
structures (F1-score = 1.0), remains competitive on moderate abstractions
(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =
0.31). These findings demonstrate that LLMs can serve as adaptable,
general-purpose tools in analog design automation, opening new research
directions for foundation model applications in analog design automation.

</details>


### [50] [RARO: Reliability-aware Conversion with Enhanced Read Performance for QLC SSDs](https://arxiv.org/abs/2508.19530)
*Yanyun Wang,Dingcui Yu,Yina Lv,Yunpeng Song,Yumiao Zhao,Liang Shi*

Main category: cs.AR

TL;DR: QLC存储因其高密度但可靠性低导致频繁重读，RARO方案通过动态迁移热点数据优化读性能，减少容量损失。


<details>
  <summary>Details</summary>
Motivation: 针对QLC闪存因读重试导致性能下降的问题，提出一种兼顾性能和容量的管理方案。

Method: RARO基于读重试统计数据动态迁移热点数据，支持多模式转换以减少容量开销。

Result: 实验表明RARO显著提升读性能，容量影响可忽略。

Conclusion: RARO有效平衡了QLC闪存的读性能与存储容量利用率。

Abstract: Quad-level cell (QLC) flash offers significant benefits in cost and capacity,
but its limited reliability leads to frequent read retries, which severely
degrade read performance. A common strategy in high-density flash storage is to
program selected blocks in a low-density mode (SLC), sacrificing some capacity
to achieve higher I/O performance. This hybrid storage architecture has been
widely adopted in consumer-grade storage systems. However, existing hybrid
storage schemes typically focus on write performance and rely solely on data
temperature for migration decisions. This often results in excessive mode
switching, causing substantial capacity overhead.
  In this paper, we present RARO (Reliability-Aware Read performance
Optimization), a hybrid flash management scheme designed to improve read
performance with minimal capacity cost. The key insight behind RARO is that
much of the read slowdown in QLC flash is caused by read retries. RARO triggers
data migration only when hot data resides in QLC blocks experiencing a high
number of read retries, significantly reducing unnecessary conversions and
capacity loss. Moreover, RARO supports fine-grained multi-mode conversions
(SLC-TLC-QLC) to further minimize capacity overhead. By leveraging real-time
read retry statistics and flash characteristics, RARO mitigates over-conversion
and optimizes I/O performance. Experiments on the FEMU platform demonstrate
that RARO significantly improves read performance across diverse workloads,
with negligible impact on usable capacity.

</details>


### [51] [Support Vector Machines Classification on Bendable RISC-V](https://arxiv.org/abs/2508.19656)
*Polykarpos Vergos,Theofanis Vergos,Florentia Afentaki,Konstantinos Balaskas,Georgios Zervakis*

Main category: cs.AR

TL;DR: 本文提出了一种开源框架，用于开发支持Bendable RISC-V核心的机器学习协处理器，并设计了一种通用的、精度可扩展的SVM加速器架构，显著提升了推理执行时间和能源效率。


<details>
  <summary>Details</summary>
Motivation: 柔性电子技术为传统刚性电子提供了低成本、轻量化和环保的替代方案，但在实现灵活的机器学习应用时，仍需解决器件体积大和功耗高的问题。

Method: 提出了一种开源框架，用于开发机器学习协处理器，并设计了一种支持OvO和OvR算法的SVM加速器架构，支持多种权重表示。

Result: 实验结果表明，该架构平均提升了21倍的推理执行时间和能源效率。

Conclusion: 该框架和架构为边缘设备的低功耗、灵活智能提供了实用解决方案。

Abstract: Flexible Electronics (FE) technology offers uniquecharacteristics in
electronic manufacturing, providing ultra-low-cost, lightweight, and
environmentally-friendly alternatives totraditional rigid electronics. These
characteristics enable a rangeof applications that were previously constrained
by the costand rigidity of conventional silicon technology. Machine learning
(ML) is essential for enabling autonomous, real-time intelligenceon devices
with smart sensing capabilities in everyday objects. However, the large feature
sizes and high power consumption ofthe devices oppose a challenge in the
realization of flexible ML applications. To address the above, we propose an
open-source framework for developing ML co-processors for the Bendable RISC-V
core. In addition, we present a custom ML accelerator architecture for Support
Vector Machine (SVM), supporting both one-vs-one (OvO) and one-vs-rest (OvR)
algorithms. Our ML accelerator adopts a generic, precision-scalable design,
supporting 4-, 8-, and 16-bit weight representations. Experimental results
demonstrate a 21x improvement in both inference execution time and energy
efficiency, on average, highlighting its potential for low-power, flexible
intelligence on the edge.

</details>


### [52] [New Tools, Programming Models, and System Support for Processing-in-Memory Architectures](https://arxiv.org/abs/2508.19868)
*Geraldo F. Oliveira*

Main category: cs.AR

TL;DR: 该论文提出了一种针对DRAM-based PIM架构的工具、编程模型和系统支持，包括四种主要贡献：DAMOV（数据移动瓶颈分析方法）、MIMDRAM（硬件/软件协同设计框架）、Proteus（降低PUD操作延迟的运行时引擎）和DaPPA（简化编程的框架）。


<details>
  <summary>Details</summary>
Motivation: 为了推动PIM在当前和未来系统中的普及，论文旨在解决PIM架构中的数据移动瓶颈、灵活性和编程性等核心问题。

Method: 1. DAMOV：数据移动分析和基准测试套件；2. MIMDRAM：优化DRAM计算的硬件/软件协同设计；3. Proteus：通过并行执行、动态精度调整和优化数据表示来降低PUD操作延迟；4. DaPPA：简化编程模型的框架。

Result: 论文提出的四种方法分别解决了PIM架构中的关键问题，提高了性能和易用性。

Conclusion: 通过创新的工具和框架，论文为PIM架构的普及提供了实用支持，显著提升了其性能和编程友好性。

Abstract: Our goal in this dissertation is to provide tools, programming models, and
system support for PIM architectures (with a focus on DRAM-based solutions), to
ease the adoption of PIM in current and future systems. To this end, we make at
least four new major contributions.
  First, we introduce DAMOV, the first rigorous methodology to characterize
memory-related data movement bottlenecks in modern workloads, and the first
data movement benchmark suite. Second, we introduce MIMDRAM, a new
hardware/software co-designed substrate that addresses the major current
programmability and flexibility limitations of the bulk bitwise execution model
of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation
and control of only the needed computing resources inside DRAM for PUD
computing. Third, we introduce Proteus, the first hardware framework that
addresses the high execution latency of bulk bitwise PUD operations in
state-of-the-art PUD architectures by implementing a data-aware runtime engine
for PUD. Proteus reduces the latency of PUD operations in three different ways:
(i) Proteus concurrently executes independent in-DRAM primitives belong to a
single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the
bit-precision (and consequentially the latency and energy consumption) of PUD
operations by exploiting narrow values (i.e., values with many leading zeros or
ones). (iii) Proteus chooses and uses the most appropriate data representation
and arithmetic algorithm implementation for a given PUD instruction
transparently to the programmer. Fourth, we introduce DaPPA (data-parallel
processing-in-memory architecture), a new programming framework that eases
programmability for general-purpose PNM architectures by allowing the
programmer to write efficient PIM-friendly code without the need to manage
hardware resources explicitly.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [53] [When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI](https://arxiv.org/abs/2508.19548)
*Madhuvanthi Srivatsav R,Chiranjib Bhattacharyya,Shantanu Chakrabartty,Chetan Singh Thakur*

Main category: cs.NE

TL;DR: 论文探讨了如何利用路由、交换和互连技术实现处理-in-Interconnect（π²）计算范式，以提高大规模神经形态计算的能效和性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模AI工作负载中路由、交换和互连系统的能源消耗和速度瓶颈问题。

Method: 通过映射AI工作负载到已有的数据包交换硬件原语，利用现有算法实现神经元模型和突触操作，并通过知识蒸馏训练神经网络。

Result: π²架构的能源效率随互连带宽提升而改善，能够以数百瓦的功耗执行脑规模AI推理工作负载。

Conclusion: π²架构通过利用互连技术趋势，为大规模神经形态计算提供了一种高效的解决方案。

Abstract: Routing, switching, and the interconnect fabric are essential for large-scale
neuromorphic computing. While this fabric only plays a supporting role in the
process of computing, for large AI workloads it ultimately determines energy
consumption and speed. In this paper, we address this bottleneck by asking: (a)
What computing paradigms are inherent in existing routing, switching, and
interconnect systems, and how can they be used to implement a
processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging
current and future interconnect trends, how will a \pi^2 system's performance
scale compared to other neuromorphic architectures? For (a), we show that
operations required for typical AI workloads can be mapped onto delays,
causality, time-outs, packet drop, and broadcast operations -- primitives
already implemented in packet-switching and packet-routing hardware. We show
that existing buffering and traffic-shaping embedded algorithms can be
leveraged to implement neuron models and synaptic operations. Additionally, a
knowledge-distillation framework can train and cross-map well-established
neural network topologies onto $\pi^2$ without degrading generalization
performance. For (b), analytical modeling shows that, unlike other neuromorphic
platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth
and energy efficiency. We predict that by leveraging trends in interconnect
technology, a \pi^2 architecture can be more easily scaled to execute
brain-scale AI inference workloads with power consumption levels in the range
of hundreds of watts.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [54] [Inference of Human-derived Specifications of Object Placement via Demonstration](https://arxiv.org/abs/2508.19367)
*Alex Cuellar,Ho Chit Siu,Julie A Shah*

Main category: cs.RO

TL;DR: 论文提出了一种增强的区域连接计算框架PARCC，用于描述人类可接受的物体空间关系，并通过示范学习优化机器人对排列规则的理解。


<details>
  <summary>Details</summary>
Motivation: 提升机器人在抓取和放置任务中对人类接受的物体配置的理解能力。

Method: 引入了基于区域连接计算的PARCC逻辑框架，并提出了一种通过示范学习PARCC规范的推理算法。

Result: 人类研究显示，PARCC能准确捕捉用户的意图，示范学习方法优于人工提供的规范。

Conclusion: PARCC框架能有效提升机器人对人类空间排列规则的理解，示范学习是优化学习路径的关键。

Abstract: As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,
object packing, sorting, and kitting), methods focused on understanding
human-acceptable object configurations remain limited expressively with regard
to capturing spatial relationships important to humans. To advance robotic
understanding of human rules for object arrangement, we introduce
positionally-augmented RCC (PARCC), a formal logic framework based on region
connection calculus (RCC) for describing the relative position of objects in
space. Additionally, we introduce an inference algorithm for learning PARCC
specifications via demonstrations. Finally, we present the results from a human
study, which demonstrate our framework's ability to capture a human's intended
specification and the benefits of learning from demonstration approaches over
human-provided specifications.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [55] [MC for Gastroretentive Drug Delivery](https://arxiv.org/abs/2508.19739)
*Sebastian Lotter,Marco Seiter,Maryam Pirmoradi,Lukas Brand,Dagmar Fischer,Robert Schober*

Main category: eess.SP

TL;DR: 该论文提出了一种新的细菌纳米纤维素(BNC)药物释放模型,填补了现有研究的空白,并通过实验数据验证了模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的BNC药物释放研究多为可行性研究,缺乏建模和设计方面的探索,因此需要开发更精确的模型。

Method: 提出了一种基于物理的模型,考虑了BNC的几何形状和聚合物涂层对药物释放的影响。

Result: 模型通过湿实验数据验证了其准确性,为未来BNC基药物递送系统的设计提供了参考。

Conclusion: 该模型为BNC药物递送系统的设计提供了理论基础和实践指导。

Abstract: Recently, bacterial nanocellulose (BNC), a biological material produced by
non-pathogenic bacteria that possesses excellent material properties for
various medical applications, has received increased interest as a carrier
system for drug delivery. However, the vast majority of existing studies on
drug release from BNC are feasibility studies with modeling and design aspects
remaining largely unexplored. To narrow this research gap, this paper proposes
a novel model for the drug release from BNC. Specifically, the drug delivery
system considered in this paper consists of a BNC fleece coated with a polymer.
The polymer coating is used as an additional diffusion barrier, enabling the
controlled release of an active pharmaceutical ingredient. The proposed
physics-based model reflects the geometry of the BNC and incorporates the
impact of the polymer coating on the drug release. Hence, it can be useful for
designing BNC-based drug delivery systems in the future. The accuracy of the
model is validated with experimental data obtained in wet lab experiments.

</details>


### [56] [Demonstrator Testbed for Effective Precoding in MEO Multibeam Satellites](https://arxiv.org/abs/2508.19657)
*Jorge L. González-Rios,Liz Martínez Marrero,Juan Duncan,Luis M. Garcés-Socarrás,Raudel Cuiman Marquez,Juan A. Vásquez Peralvo,Jevgenij Krivochiza,Symeon Chatzinotas,Björn Ottersten*

Main category: eess.SP

TL;DR: 本文设计了一个基于SDR平台的实验平台，用于MEO卫星场景中的高效预编码，并通过同步环路解决多普勒频移和相位噪声问题，实验结果验证了方案的可行性。


<details>
  <summary>Details</summary>
Motivation: MEO卫星通信在宽带互联网连接中前景广阔，但其轨道动力学问题给预编码技术带来挑战，需解决多普勒频移和相位噪声等问题。

Method: 采用软件定义无线电（SDR）平台设计实验平台，结合精确轨道模型和定制辐射阵列，提出同步环路以解决信号处理问题。

Result: 初步实验验证了所提方案在MEO场景下的可行性和有效性。

Conclusion: 该研究为MEO卫星通信中的高效预编码提供了一种可行解决方案，为未来网络生态系统的发展提供了技术支持。

Abstract: The use of communication satellites in medium Earth orbit (MEO) is foreseen
to provide quasi-global broadband Internet connectivity in the coming
networking ecosystems. Multi-user multiple-input single-output (MU-MISO)
digital signal processing techniques, such as precoding, emerge as appealing
technological enablers in the forward link of multi-beam satellite systems
operating in full frequency reuse (FFR). However, the orbit dynamics of MEO
satellites pose additional challenges that must be carefully evaluated and
addressed. This work presents the design of an in-lab testbed based on
software-defined radio (SDR) platforms and the corresponding adaptations
required for efficient precoding in a MEO scenario. The setup incorporates a
precise orbit model and the radiation pattern of a custom-designed direct
radiating array (DRA). We analyze the main impairments affecting precoding
performance, including Doppler shifts and payload phase noise, and propose a
synchronization loop to mitigate these effects. Preliminary experimental
results validate the feasibility and effectiveness of the proposed solution.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [57] [Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)](https://arxiv.org/abs/2508.19428)
*Aleksandra Beliaeva,Temurbek Rahmatullaev*

Main category: cs.CL

TL;DR: 该系统通过检索增强提示、零样本分类和基于注意力的图建模，成功解决了LLMs4OL 2025挑战中的任务A、B和C，展示了LLM架构在异构领域中的可扩展性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs4OL 2025挑战中的任务A、B和C，展示LLM在异构领域中的适应性和性能。

Method: 任务A使用检索增强生成（RAG）提取术语和类型；任务B在少量样本和零样本设置下分别采用RAG和零样本分类器；任务C通过图推理预测is-a关系。

Result: 在所有任务中取得了排行榜上的顶级成绩。

Conclusion: 系统展示了LLM架构在异构领域中的可扩展性、适应性和鲁棒性。

Abstract: We present a comprehensive system for addressing Tasks A, B, and C of the
LLMs4OL 2025 challenge, which together span the full ontology construction
pipeline: term extraction, typing, and taxonomy discovery. Our approach
combines retrieval-augmented prompting, zero-shot classification, and
attention-based graph modeling -- each tailored to the demands of the
respective task. For Task A, we jointly extract domain-specific terms and their
ontological types using a retrieval-augmented generation (RAG) pipeline.
Training data was reformulated into a document to terms and types
correspondence, while test-time inference leverages semantically similar
training examples. This single-pass method requires no model finetuning and
improves overall performance through lexical augmentation Task B, which
involves assigning types to given terms, is handled via a dual strategy. In the
few-shot setting (for domains with labeled training data), we reuse the RAG
scheme with few-shot prompting. In the zero-shot setting (for previously unseen
domains), we use a zero-shot classifier that combines cosine similarity scores
from multiple embedding models using confidence-based weighting. In Task C, we
model taxonomy discovery as graph inference. Using embeddings of type labels,
we train a lightweight cross-attention layer to predict is-a relations by
approximating a soft adjacency matrix. These modular, task-specific solutions
enabled us to achieve top-ranking results in the official leaderboard across
all three tasks. Taken together these strategies showcase the scalability,
adaptability, and robustness of LLM-based architectures for ontology learning
across heterogeneous domains.
  Code is available at:
https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek

</details>


### [58] [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
*Zikun Fu,Chen Yang,Kourosh Davoudi,Ken Q. Pu*

Main category: cs.CL

TL;DR: 本文提出了一个针对自然语言查询中数据库实体识别的解决方案，包括人工标注基准、数据增强方法和基于T5的语言模型。实验表明该模型在精确率和召回率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询中数据库实体识别的挑战，提升识别性能。

Method: 提出人工标注基准、数据增强方法及基于T5的实体识别模型。

Result: 模型在精确率和召回率上优于现有方法，数据增强和微调分别提升性能10%和5-10%。

Conclusion: 所提方法有效提升了数据库实体识别的性能，具有实用价值。

Abstract: This paper addresses the challenge of Database Entity Recognition (DB-ER) in
Natural Language Queries (NLQ). We present several key contributions to advance
this field: (1) a human-annotated benchmark for DB-ER task, derived from
popular text-to-sql benchmarks, (2) a novel data augmentation procedure that
leverages automatic annotation of NLQs based on the corresponding SQL queries
which are available in popular text-to-SQL benchmarks, (3) a specialized
language model based entity recognition model using T5 as a backbone and two
down-stream DB-ER tasks: sequence tagging and token classification for
fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER
tagger with two state-of-the-art NER taggers, and observed better performance
in both precision and recall for our model. The ablation evaluation shows that
data augmentation boosts precision and recall by over 10%, while fine-tuning of
the T5 backbone boosts these metrics by 5-10%.

</details>


### [59] [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
*Evandro L. T. P. Cunha*

Main category: cs.CL

TL;DR: 本文探讨了生成式人工智能工具可能导致人类写作能力下降的可能性，并将其与历史上的写作能力丧失时期（如希腊黑暗时代）相类比。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式人工智能工具的普及对人类写作能力的潜在影响，并类比历史上的类似现象。

Method: 通过比较现代技术发展与历史上人类写作能力丧失的时期（如希腊黑暗时代），分析其相似性与潜在后果。

Result: 指出生成式人工智能可能削弱人类的写作能力，就像历史上某些时期人类因外部依赖而失去写作技能。

Conclusion: 需要警惕技术过度依赖对人类能力的负面影响，并采取平衡措施以保护人类写作技能。

Abstract: The 2020s have been witnessing a very significant advance in the development
of generative artificial intelligence tools, including text generation systems
based on large language models. These tools have been increasingly used to
generate texts in the most diverse domains -- from technical texts to literary
texts --, which might eventually lead to a lower volume of written text
production by humans. This article discusses the possibility of a future in
which human beings will have lost or significantly decreased their ability to
write due to the outsourcing of this activity to machines. This possibility
parallels the loss of the ability to write in other moments of human history,
such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).

</details>


### [60] [MathBuddy: A Multimodal System for Affective Math Tutoring](https://arxiv.org/abs/2508.19993)
*Debanjana Kar,Leopold Böss,Dacia Braca,Sebastian Maximilian Dennerlein,Nina Christine Hubig,Philipp Wintersberger,Yufang Hou*

Main category: cs.CL

TL;DR: 论文提出MathBuddy，一种基于LLM的情感感知数学辅导系统，通过结合学生情感提升教学效果。


<details>
  <summary>Details</summary>
Motivation: 现有学习模型未考虑学生情感状态，而研究表明情感状态影响学习能力，因此需要情感感知的辅导系统。

Method: 通过文本和面部表情捕捉学生情感，结合多模态数据优化LLM辅导策略。

Result: 实验显示，情感感知模型在多个教学维度上显著提升效果，性能提升明显。

Conclusion: 情感感知能显著提升LLM辅导系统的教学能力。

Abstract: The rapid adoption of LLM-based conversational systems is already
transforming the landscape of educational technology. However, the current
state-of-the-art learning models do not take into account the student's
affective states. Multiple studies in educational psychology support the claim
that positive or negative emotional states can impact a student's learning
capabilities. To bridge this gap, we present MathBuddy, an emotionally aware
LLM-powered Math Tutor, which dynamically models the student's emotions and
maps them to relevant pedagogical strategies, making the tutor-student
conversation a more empathetic one. The student's emotions are captured from
the conversational text as well as from their facial expressions. The student's
emotions are aggregated from both modalities to confidently prompt our LLM
Tutor for an emotionally-aware response. We have effectively evaluated our
model using automatic evaluation metrics across eight pedagogical dimensions
and user studies. We report a massive 23 point performance gain using the win
rate and a 3 point gain at an overall level using DAMR scores which strongly
supports our hypothesis of improving LLM-based tutor's pedagogical abilities by
modeling students' emotions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [61] [Beat-Based Rhythm Quantization of MIDI Performances](https://arxiv.org/abs/2508.19262)
*Maximilian Wachter,Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 提出一种基于Transformer的节奏量化模型，结合节拍和强拍信息，将MIDI演奏量化为符合节拍、易读的乐谱。


<details>
  <summary>Details</summary>
Motivation: 通过结合节拍和强拍信息，改进MIDI演奏的量化效果，生成更准确、易读的乐谱。

Method: 提出基于节拍的数据预处理方法，将乐谱和演奏数据转换为统一的token表示；优化模型架构和数据表示，并在钢琴和吉他演奏数据上训练。

Result: 模型在MUSTER指标上超越了当前最优性能。

Conclusion: 该Transformer模型在节奏量化任务中表现优异，尤其在结合节拍信息后进一步提升了量化效果。

Abstract: We propose a transformer-based rhythm quantization model that incorporates
beat and downbeat information to quantize MIDI performances into
metrically-aligned, human-readable scores. We propose a beat-based
preprocessing method that transfers score and performance data into a unified
token representation. We optimize our model architecture and data
representation and train on piano and guitar performances. Our model exceeds
state-of-the-art performance based on the MUSTER metric.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [62] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新型的双鱼眼相机校准框架，通过将双鱼眼相机模型融入3D高斯溅射流程，能够生成无缝的360度图像，并在真实数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决消费级双鱼眼系统因镜头分离和角度变形导致的不完美全景问题。

Method: 结合双鱼眼相机模型与3D高斯溅射流程，联合优化3D高斯参数和模拟镜头间隙与角度变形的校准变量。

Result: 生成无缝的360度渲染图像，性能优于现有模型。

Conclusion: 该方法能够从不完美的全向输入中生成完美的视角合成结果，具有实际应用价值。

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [63] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 本文综述了高光谱成像（HSI）在先进驾驶辅助系统和自动驾驶中的应用，分析了当前技术的优缺点及商业化程度，并提出了研究发展方向。


<details>
  <summary>Details</summary>
Motivation: 探讨HSI在汽车领域的潜力，尤其是其在材料级场景理解方面的优势，以及当前技术与实际应用之间的差距。

Method: 通过定性回顾216款商业HSI和多光谱相机，基于帧率、空间分辨率、光谱维度和AEC-Q100标准进行性能评估，并分析现有数据集和应用。

Result: 仅四款相机满足性能要求，无一款符合AEC-Q100标准；现有数据集也存在规模小、光谱一致性差等问题，限制了HSI潜力的验证。

Conclusion: HSI在汽车领域的研究潜力尚未完全商业化，需进一步优化技术和数据集才能实现其在ADAS/AD中的实际应用。

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [64] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: AudioStory是一个结合大语言模型和文本到音频生成系统的框架，用于生成具有时序连贯性和情感一致性的长叙事音频。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到音频生成系统在长叙事音频生成中缺乏时序连贯性和组合推理能力的问题。

Method: AudioStory通过LLM分解复杂叙事查询为时序子任务，并使用解耦的桥接机制和端到端训练优化生成过程。

Result: 实验表明AudioStory在生成叙事音频时优于现有基线，具有更好的指令跟随能力和音频保真度。

Conclusion: AudioStory为长叙事音频生成提供了一种有效的端到端解决方案，并通过基准测试验证了其优越性。

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [65] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 提出了一种实时生成绘图系统，通过结合形式意图和上下文意图，实现结构控制和风格感知的图像合成，支持多用户协作。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本提示的生成系统无法同时捕捉低级几何特征与高级语义线索，限制了绘图的交互性和创造力。

Method: 多阶段生成管道结合轮廓保留的结构控制与风格和内容感知的图像合成，采用触摸屏界面和分布式推理架构。

Result: 系统实现了低延迟的双阶段转换，支持多用户在共享画布上协作，提升了人机交互的共创性。

Conclusion: 该系统为无艺术专业背景的用户提供了同步共创视觉内容的平台，重新定义了人机交互的共创过程。

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: 该论文提出了一种低功耗、灵活的应力分类器设计方法，填补了现有技术在连续监测和成本效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的压力监测方法缺乏连续性和成本效率，而现有的硅基穿戴设备虽功能多样但不够灵活。柔性电子技术虽有潜力，但在复杂电路（如机器学习分类器）的实现上存在挑战。

Method: 研究通过探索1200多种柔性分类器，结合特征选择和神经网络简化算法，设计低功耗、低精度算术电路。

Result: 设计出的分类器在准确性上优于现有方法，同时具备低成本、低功耗和紧凑尺寸的特点。

Conclusion: 该研究为实时压力监测提供了高效、灵活的解决方案，推动了柔性电子技术在健康监测领域的应用。

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [67] [A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation](https://arxiv.org/abs/2508.19591)
*Jiakui Shen,Yunqi Mi,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: 论文提出了一种名为PLGC的联邦推荐系统策略，通过个性化局部-全局协作解决嵌入退化问题，优于现有基线算法。


<details>
  <summary>Details</summary>
Motivation: 集中式推荐系统因需收集用户隐私数据导致隐私泄露问题，联邦推荐系统虽通过聚合全局模型保护隐私，但其分布式训练范式因稀疏交互和异构偏好导致嵌入退化。

Method: 提出了PLGC策略，通过将全局项目嵌入表纳入本地设备，并利用神经正切核策略动态平衡局部与全局信息，优化个性化表示。同时，通过对比目标函数减少嵌入冗余。

Result: 在五个真实数据集上的实验表明，PLGC在有效性和适应性上均优于多种基线算法。

Conclusion: PLGC是一种模型无关的个性化训练策略，能有效缓解联邦推荐中的嵌入退化问题，提升推荐性能。

Abstract: Centralized recommender systems encounter privacy leakage due to the need to
collect user behavior and other private data. Hence, federated recommender
systems (FedRec) have become a promising approach with an aggregated global
model on the server. However, this distributed training paradigm suffers from
embedding degradation caused by suboptimal personalization and dimensional
collapse, due to the existence of sparse interactions and heterogeneous
preferences. To this end, we propose a novel model-agnostic strategy for FedRec
to strengthen the personalized embedding utility, which is called Personalized
Local-Global Collaboration (PLGC). It is the first research in federated
recommendation to alleviate the dimensional collapse issue. Particularly, we
incorporate the frozen global item embedding table into local devices. Based on
a Neural Tangent Kernel strategy that dynamically balances local and global
information, PLGC optimizes personalized representations during forward
inference, ultimately converging to user-specific preferences. Additionally,
PLGC carries on a contrastive objective function to reduce embedding redundancy
by dissolving dependencies between dimensions, thereby improving the backward
representation learning process. We introduce PLGC as a model-agnostic
personalized training strategy for federated recommendations that can be
applied to existing baselines to alleviate embedding degradation. Extensive
experiments on five real-world datasets have demonstrated the effectiveness and
adaptability of PLGC, which outperforms various baseline algorithms.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [68] [Large Language Models (LLMs) for Electronic Design Automation (EDA)](https://arxiv.org/abs/2508.20030)
*Kangwei Xu,Denis Schwachhofer,Jason Blocklove,Ilia Polian,Peter Domanski,Dirk Pflüger,Siddharth Garg,Ramesh Karri,Ozgur Sinanoglu,Johann Knechtel,Zhuorui Zhao,Ulf Schlichtmann,Bing Li*

Main category: eess.SY

TL;DR: 论文探讨了如何将大型语言模型（LLM）整合到电子设计自动化（EDA）中，以简化硬件开发流程。通过案例研究展示了LLM在硬件设计、测试和优化中的应用，并讨论了未来方向和挑战。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路设计流程复杂且容易出错，需要更高效的EDA解决方案。LLM在文本理解和生成方面的优势为自动化EDA提供了潜力。

Method: 论文综述了LLM在EDA中的应用，包括能力、限制和未来机会，并通过三个案例研究展示了其在不同硬件开发阶段的表现。

Result: LLM有望简化EDA流程，甚至实现部分自动化，但也存在局限性需要解决。

Conclusion: LLM在EDA中的应用前景广阔，但仍需进一步研究以充分发挥其潜力。

Abstract: With the growing complexity of modern integrated circuits, hardware engineers
are required to devote more effort to the full design-to-manufacturing
workflow. This workflow involves numerous iterations, making it both
labor-intensive and error-prone. Therefore, there is an urgent demand for more
efficient Electronic Design Automation (EDA) solutions to accelerate hardware
development. Recently, large language models (LLMs) have shown remarkable
advancements in contextual comprehension, logical reasoning, and generative
capabilities. Since hardware designs and intermediate scripts can be
represented as text, integrating LLM for EDA offers a promising opportunity to
simplify and even automate the entire workflow. Accordingly, this paper
provides a comprehensive overview of incorporating LLMs into EDA, with emphasis
on their capabilities, limitations, and future opportunities. Three case
studies, along with their outlook, are introduced to demonstrate the
capabilities of LLMs in hardware design, testing, and optimization. Finally,
future directions and challenges are highlighted to further explore the
potential of LLMs in shaping the next-generation EDA, providing valuable
insights for researchers interested in leveraging advanced AI technologies for
EDA.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [69] [Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents](https://arxiv.org/abs/2508.19504)
*Kevin Song,Anand Jayarajan,Yaoyao Ding,Qidong Su,Zhanda Zhu,Sihang Liu,Gennady Pekhimenko*

Main category: cs.MA

TL;DR: 本文研究了通过优化系统环境来提高大型语言模型（LLM）代理在复杂任务中的成功率，而非单纯改进代理本身。提出了一种包含6种故障模式的分类法，并设计了三种环境优化技术Aegis，实验表明成功率提高了6.7-12.5%。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在复杂实际环境中的成功率较低，但现有研究主要关注改进代理本身，而忽略了系统环境的作用。本文旨在填补这一空白。

Method: 收集142个代理跟踪数据（3,656次交互），分析故障并提出6种故障模式分类法。基于此设计了Aegis技术，包括环境可观测性增强、通用计算卸载和推测代理行为。

Result: Aegis技术在不修改代理或LLM的情况下，将代理成功率平均提高了6.7-12.5%。

Conclusion: 优化系统环境是提高LLM代理成功率的有效补充方向，Aegis技术为此提供了可行的解决方案。

Abstract: Large Language Models (LLMs) agents augmented with domain tools promise to
autonomously execute complex tasks requiring human-level intelligence, such as
customer service and digital assistance. However, their practical deployment is
often limited by their low success rates under complex real-world environments.
To tackle this, prior research has primarily focused on improving the agents
themselves, such as developing strong agentic LLMs, while overlooking the role
of the system environment in which the agent operates.
  In this paper, we study a complementary direction: improving agent success
rates by optimizing the system environment in which the agent operates. We
collect 142 agent traces (3,656 turns of agent-environment interactions) across
5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we
propose a taxonomy for agent-environment interaction failures that includes 6
failure modes. Guided by these findings, we design Aegis, a set of targeted
environment optimizations: 1) environment observability enhancement, 2) common
computation offloading, and 3) speculative agentic actions. These techniques
improve agent success rates on average by 6.7-12.5%, without any modifications
to the agent and underlying LLM.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [70] [Quantum Resource Management in the NISQ Era: Challenges, Vision, and a Runtime Framework](https://arxiv.org/abs/2508.19276)
*Marcos Guillermo Lammers,Federico Hernán Holik,Alejandro Fernández*

Main category: quant-ph

TL;DR: 该论文讨论了在NISQ时代如何高效管理量子资源，并提出了一个名为Qonscious的原型框架，用于基于动态资源评估执行量子程序。


<details>
  <summary>Details</summary>
Motivation: 当前NISQ设备的硬件局限性（如有限的量子比特、高错误率和短的相干时间）使得高效管理量子资源变得尤为重要，以开发可靠的量子软件。

Method: 论文分析了资源在当前NISQ设备中的角色，提出了运行时感知的量子软件开发愿景，并设计了Qonscious框架作为概念验证。

Result: 通过Qonscious框架，论文展示了如何基于动态资源评估实现量子程序的条件执行。

Conclusion: 该研究推动了量子资源估计（QRE）领域的发展，并为开发可扩展、可靠且资源感知的量子软件提供了方向。

Abstract: Quantum computers represent a radical technological advancement in the way
information is processed by using the principles of quantum mechanics to solve
very complex problems that exceed the capabilities of classical systems.
However, in the current NISQ era (Noisy Intermediate-Scale Quantum devices),
the available hardware presents several limitations, such as a limited number
of qubits, high error rates, and reduced coherence times. Efficient management
of quantum resources, both physical (qubits, error rates, connectivity) and
logical (quantum gates, algorithms, error correction), becomes particularly
relevant in the design and deployment of quantum algorithms. In this work, we
analyze the role of resources in the various uses of NISQ devices today,
identifying their relevance and implications for software engineering focused
on the use of quantum computers. We propose a vision for runtime-aware quantum
software development, identifying key challenges to its realization, such as
limited introspection capabilities and temporal constraints in current
platforms. As a proof of concept, we introduce Qonscious, a prototype framework
that enables conditional execution of quantum programs based on dynamic
resource evaluation. With this contribution, we aim to strengthen the field of
Quantum Resource Estimation (QRE) and move towards the development of scalable,
reliable, and resource-aware quantum software.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges](https://arxiv.org/abs/2508.19309)
*Peng Gu,Shuangchen Li,Dylan Stow,Russell Barnes,Liu Liu,Yuan Xie,Eren Kursshan*

Main category: cs.CR

TL;DR: 论文探讨了2.5D和3D技术在高集成度、性能和成本方面的潜力，并提出了针对侧信道攻击、硬件木马等安全挑战的新设计方法。


<details>
  <summary>Details</summary>
Motivation: 当前技术在应对新兴安全挑战（如侧信道攻击和硬件木马）时存在严重问题，需要通过2.5D和3D技术的特性解决这些挑战。

Method: 提出了4种新设计：（1）3D屏蔽侧信道信息的架构；（2）使用有源中介层的分裂制造；（3）单片3D IC上的电路伪装；（4）基于3D IC的内存安全处理（PIM）。

Result: 这些设计提高了现有安全措施的效能，并提供了新的安全功能。

Conclusion: 2.5D和3D技术为安全系统设计提供了新的机会，但同时也面临挑战。

Abstract: 3D die stacking and 2.5D interposer design are promising technologies to
improve integration density, performance and cost. Current approaches face
serious issues in dealing with emerging security challenges such as side
channel attacks, hardware trojans, secure IC manufacturing and IP piracy. By
utilizing intrinsic characteristics of 2.5D and 3D technologies, we propose
novel opportunities in designing secure systems. We present: (i) a 3D
architecture for shielding side-channel information; (ii) split fabrication
using active interposers; (iii) circuit camouflage on monolithic 3D IC, and
(iv) 3D IC-based security processing-in-memory (PIM). Advantages and challenges
of these designs are discussed, showing that the new designs can improve
existing countermeasures against security threats and further provide new
security features.

</details>


### [72] [A Technical Review on Comparison and Estimation of Steganographic Tools](https://arxiv.org/abs/2508.19323)
*Ms. Preeti P. Bhatt,Rakesh R. Savant*

Main category: cs.CR

TL;DR: 该论文综述了图像隐写术的分类，并通过比较不同工具在多种图像格式下的表现，分析了其性能差异。


<details>
  <summary>Details</summary>
Motivation: 研究目的是评估现有图像隐写术工具的性能，以确定哪种工具在效率和图像特征（如大小、尺寸、像素值和直方图差异）方面表现最佳。

Method: 选择了六款常用隐写工具，使用相同的输入（嵌入特定文本）进行测试，并分析其在图像特征上的表现。

Result: 实验结果表明，六款工具性能相近，但部分工具在效率上略胜一筹。性能差异主要基于图像特征的比较。

Conclusion: 研究为选择最佳图像隐写工具提供了参考依据，同时强调了图像特征对工具性能的影响。

Abstract: Steganography is technique of hiding a data under cover media using different
steganography tools. Image steganography is hiding of data
(Text/Image/Audio/Video) under a cover as Image. This review paper presents
classification of image steganography and the comparison of various Image
steganography tools using different image formats. Analyzing numerous tools on
the basis of Image features and extracting the best one. Some of the tools
available in the market were selected based on the frequent use; these tools
were tested using the same input on all of them. Specific text was embedded
within all host images for each of the six Steganography tools selected. The
results of the experiment reveal that all the six tools were relatively
performing at the same level, though some software performs better than others
through efficiency. And it was based on the image features like size,
dimensions, and pixel value and histogram differentiation.

</details>


### [73] [Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks](https://arxiv.org/abs/2508.19430)
*Kangfeng Ye,Roberto Metere,Jim Woodcock,Poonam Yadav*

Main category: cs.CR

TL;DR: 该论文提出了一种基于Isabelle的形式化验证方法，用于分析Needham-Schroeder协议及其与物理层安全技术的集成，克服了ProVerif方法的局限，展示了更高的灵活性和安全性验证能力。


<details>
  <summary>Details</summary>
Motivation: 现有的ProVerif方法在验证安全协议时存在局限性，无法提供超出验证结果的深入理解。

Method: 作者采用Isabelle形式化方法，重新建模协议并开发了交互式自动化验证框架，支持密码学和物理层安全技术。

Result: 该方法不仅验证了协议的保密性，还发现其认证性在所有场景中均保持完好，且提出了一个安全的PLS-Diffie-Hellman协议。

Conclusion: 论文展示了新方法在形式化验证中的优势，能够更全面地验证安全属性，超越传统方法的限制。

Abstract: Formal verification is crucial for ensuring the robustness of security
protocols against adversarial attacks. The Needham-Schroeder protocol, a
foundational authentication mechanism, has been extensively studied, including
its integration with Physical Layer Security (PLS) techniques such as
watermarking and jamming. Recent research has used ProVerif to verify these
mechanisms in terms of secrecy. However, the ProVerif-based approach limits the
ability to improve understanding of security beyond verification results. To
overcome these limitations, we re-model the same protocol using an Isabelle
formalism that generates sound animation, enabling interactive and automated
formal verification of security protocols. Our modelling and verification
framework is generic and highly configurable, supporting both cryptography and
PLS. For the same protocol, we have conducted a comprehensive analysis (secrecy
and authenticity in four different eavesdropper locations under both passive
and active attacks) using our new web interface. Our findings not only
successfully reproduce and reinforce previous results on secrecy but also
reveal an uncommon but expected outcome: authenticity is preserved across all
examined scenarios, even in cases where secrecy is compromised. We have
proposed a PLS-based Diffie-Hellman protocol that integrates watermarking and
jamming, and our analysis shows that it is secure for deriving a session key
with required authentication. These highlight the advantages of our novel
approach, demonstrating its robustness in formally verifying security
properties beyond conventional methods.

</details>
