<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 24]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 13]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 4]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.CY](#cs.CY) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.SD](#cs.SD) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: 本文提出了一种名为TrioXpert的端到端事件管理框架，通过多模态数据全面分析微服务系统状态，并利用大型语言模型协同推理，显著提升了异常检测、故障分诊和根因定位的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于单模态数据的事件管理方法无法同时处理多个任务且缺乏解释性，因此需要一种能够充分利用多模态数据并提供明确推理证据的解决方案。

Method: TrioXpert设计了三个独立的数据处理管道，基于不同模态的特性，结合大型语言模型的协同推理机制，同时处理多个任务并提供推理证据。

Result: 在两个流行的微服务系统数据集上的实验表明，TrioXpert在异常检测、故障分诊和根因定位任务中分别实现了4.7%至57.7%、2.1%至40.6%和1.6%至163.1%的性能提升。

Conclusion: TrioXpert通过多模态数据和协同推理机制，显著提升了事件管理的效果和解释性，为复杂微服务系统的运维提供了有力工具。

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [2] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: 论文提出了一种流式过程模拟发现技术，结合增量过程发现和在线机器学习方法，以应对动态业务流程的实时变化，提高模拟的稳定性和对概念漂移的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在动态商业环境中，现有业务流程模拟发现技术缺乏对实时操作变化的适应性，因此需要一种能够整合增量过程发现和在线机器学习的方法来优化模拟效果。

Method: 采用流式过程模拟发现技术，结合增量过程发现与在线机器学习，优先考虑近期数据，同时保留历史信息。

Result: 实验表明，该技术在模拟中赋予近期数据更高权重并保留历史知识，能够生成更稳定的模拟，并有效处理概念漂移。

Conclusion: 提出的技术不仅提升了模拟的稳定性，还在处理概念漂移时表现出鲁棒性，适用于动态业务环境。

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [3] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: 论文研究了GitHub Copilot如何影响本科生在遗留代码库中的编程表现、行为和理解，发现使用Copilot时任务完成速度快35%，解决方案进展多50%，但学生对Copilot建议的理解不足，需改进教学方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI编码助手（如GitHub Copilot）正在改变软件开发实践，但其对学生在遗留代码库开发任务中的影响尚不明确。

Method: 通过对照实验，10名本科生在遗留Web应用中完成相似任务（使用与不使用Copilot），结合性能、行为分析和退出访谈进行评估。

Result: 使用Copilot的学生任务完成速度快35%，解决方案进展多50%，手动编码和网络搜索时间分别减少11%和12%，但对Copilot建议的理解不足。

Conclusion: 计算教育者需开发新教学方法，利用GenAI助手优势，同时引导学生反思其建议的原理。

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [4] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: 该论文挑战了在代码生成任务中优先使用全面验证器（如完整测试套件）而忽视速度和准确性权衡的共识，提出通过生成-修剪-排名方法，利用较快的验证器（如ORM）在排名前过滤错误方案，实现速度和准确性的平衡。


<details>
  <summary>Details</summary>
Motivation: 探讨在代码生成任务中，全面验证器与ORM之间的权衡关系，验证ORM在速度和准确性上的价值，尤其是在生成-修剪-排名方法中的作用。

Method: 通过生成-修剪-排名策略，使用较快的验证器（ORM）在最终排名前过滤错误方案，并与全面测试套件进行比较。

Result: 该方法使系统速度提升11.65倍，仅降低8.33%的准确性，且能有效过滤高排名的错误方案。

Conclusion: 研究支持在设计可扩展且准确的程序排名系统时，应考虑验证器的速度和准确性权衡，ORM在特定场景下具有显著价值。

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [5] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: 该论文提出了一种评估大型语言模型（LLMs）在代码生成中对输入变化敏感性的方法，包括合成评估流程和基于角色（persona）的系统评估方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成中降低了开发门槛并加速了开发过程，但生成代码的质量和功能受用户背景和提示质量的影响。因此，需要量化LLMs对输入变化的敏感性，以提升生成代码的可靠性。

Method: 提出了两种评估方法：1）合成评估管道，用于代码生成；2）基于角色的系统评估方法，用于揭示LLMs响应与用户背景的关联性。这些方法不依赖具体编程任务或LLMs，因此具有广泛适用性。

Result: 实验证明了所提方法的有效性，并公开了代码以供社区使用。

Conclusion: 论文提供了独立于任务和模型的评估工具，有助于理解LLMs在代码生成中对输入变化的敏感性，为提升生成代码质量提供了实用方法。

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [6] [AI-Based Software Vulnerability Detection: A Systematic Literature Review](https://arxiv.org/abs/2506.10280)
*Samiha Shimmi,Hamed Okhravi,Mona Rahimi*

Main category: cs.SE

TL;DR: 该研究系统回顾了2018至2023年软件漏洞检测（SVD）领域的研究，揭示了91%的研究采用基于AI的方法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞对网络安全构成严重威胁，传统检测方法（如静态分析、基于规则的匹配）存在局限性，促使转向AI驱动的解决方案。

Method: 通过系统文献综述，总结了技术、特征表示和嵌入方法的分类，特别关注AI和图基模型的应用。

Result: 发现大多数研究使用基于AI的方法，图基模型最为普遍；同时指出了数据集质量、可重复性和可解释性等关键限制。

Conclusion: 研究强调了联合学习和量子神经网络等未充分探索技术的潜力，为未来研究提供了路线图。

Abstract: Software vulnerabilities in source code pose serious cybersecurity risks,
prompting a shift from traditional detection methods (e.g., static analysis,
rule-based matching) to AI-driven approaches. This study presents a systematic
review of software vulnerability detection (SVD) research from 2018 to 2023,
offering a comprehensive taxonomy of techniques, feature representations, and
embedding methods. Our analysis reveals that 91% of studies use AI-based
methods, with graph-based models being the most prevalent. We identify key
limitations, including dataset quality, reproducibility, and interpretability,
and highlight emerging opportunities in underexplored techniques such as
federated learning and quantum neural networks, providing a roadmap for future
research.

</details>


### [7] [Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis](https://arxiv.org/abs/2506.10322)
*Xueying Du,Kai Yu,Chong Wang,Yi Zou,Wentai Deng,Zuoyu Ou,Xin Peng,Lingming Zhang,Yiling Lou*

Main category: cs.SE

TL;DR: 提出LLM4PFA框架，通过迭代路径可行性分析和LLM代理的约束推理，显著降低静态漏洞检测的高误报率。


<details>
  <summary>Details</summary>
Motivation: 现有静态漏洞检测工具在大型代码库中误报率高，现有LLM方法在约束分析和可扩展性上受限，需要更有效的解决方案。

Method: 采用基于LLM代理的迭代路径可行性分析框架，结合上下文感知分析和代理规划，优化复杂路径的可行性验证。

Result: LLM4PFA过滤72%-96%的误报，显著优于基线（提升41.1%-105.7%），且仅遗漏3个真实漏洞。

Conclusion: LLM4PFA有效降低误报率，验证了其在复杂路径分析中的高效性和实用性。

Abstract: Static bug analyzers play a crucial role in ensuring software quality.
However, existing analyzers for bug detection in large codebases often suffer
from high false positive rates. This is primarily due to the limited
capabilities of analyzers in path feasibility validation with multiple
conditional branches and complex data dependencies. While current LLM-based
approaches attempt to address this issue, their effectiveness remains limited
due to insufficient constraint cascade analysis and scalability challenges in
large projects. To address this challenge, we propose an iterative path
feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted
constraint reasoning, and key context-aware analysis driven by agent planning,
LLM4PFA effectively enhances complex inter-procedural path feasibility analysis
for minimizing false positives in static bug detection. Evaluation results show
that LLM4PFA precisely filters out 72% to 96% false positives reported during
static bug detection, significantly outperforming all the baselines by 41.1% -
105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true
positives.

</details>


### [8] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: 研究探索了通过整合大型语言模型（如GPT-3.5 Turbo和GPT-4o）来自动化代码问题检测和修订，结合静态代码分析框架和检索增强生成技术，显著减少了代码问题。


<details>
  <summary>Details</summary>
Motivation: 旨在通过自动化工具提升软件开发效率，减少代码中的缺陷和漏洞，优化开发流程。

Method: 采用静态代码分析框架检测问题，结合LLMs进行自动化修订，并通过检索增强生成（RAG）和代码比较工具减少错误输出。

Result: 实验结果表明代码问题显著减少，证明了该方法的有效性。

Conclusion: 整合LLMs、静态分析和RAG技术可有效提升代码质量，优化开发流程。

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [9] [AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine](https://arxiv.org/abs/2506.10365)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Haoyue Jiao,Ziqi Liu,Lutong Xie,Chang Liu,Jianyuan Liang,Yaxian Qing,Xiaopu Zhang,Dehua Peng,Zhipeng Gui,Xuefeng Guan*

Main category: cs.SE

TL;DR: AutoGEEval++是一个增强的框架，用于自动化评估大语言模型在Google Earth Engine上生成地理空间代码的性能。它支持多种数据类型和任务复杂度，并通过多维度指标评估模型的准确性、资源使用、运行效率和错误类型。


<details>
  <summary>Details</summary>
Motivation: 地理空间代码生成是人工智能与地理科学分析整合的关键前沿，但目前缺乏标准化的自动化评估工具。

Method: AutoGEEval++基于GEE Python API构建，包含6,365个测试案例的基准数据集，支持单元、组合和主题测试，并提供端到端的自动化评估流程。

Result: 评估了24个最先进的大语言模型，揭示了在不同任务类型、模型设计和部署设置下的性能、稳定性和错误差异。

Conclusion: AutoGEEval++为GEE代码生成建立了首个标准化评估协议和基准，为性能比较和领域特定代码评估提供了统一的方法框架。

Abstract: Geospatial code generation is becoming a key frontier in integrating
artificial intelligence with geo-scientific analysis, yet standardised
automated evaluation tools for this task remain absent. This study presents
AutoGEEval++, an enhanced framework building on AutoGEEval, and the first
automated assessment system for large language models (LLMs) generating
geospatial code on Google Earth Engine (GEE). It supports diverse data
modalities and varying task complexities. Built on the GEE Python API,
AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test
cases across 26 data types and three task categories: unit, combo, and theme
tests. It includes a submission programme and a judge module to realise an
end-to-end automated evaluation pipeline from code generation to
execution-based validation. The framework adopts multi-dimensional
metrics-accuracy, resource usage, run-time efficiency, and error
types-balancing hallucination control and efficiency, and enabling boundary
testing and error pattern analysis. Using AutoGEEval++, we evaluate 24
state-of-the-art LLMs (as of June 2025), including general-purpose,
reasoning-enhanced, code-centric, and geoscience-specific models. Results
reveal clear performance, stability, and error differences across task types,
model designs, and deployment settings, confirming AutoGEEval++'s practical
value and scalability in vertical-domain code generation. This work establishes
the first standardised evaluation protocol and foundational benchmark for
GEE-based LLM code generation, providing a unified basis for performance
comparison and a methodological framework for systematic, domain-specific code
evaluation.

</details>


### [10] [Solving Package Management via Hypergraph Dependency Resolution](https://arxiv.org/abs/2506.10803)
*Ryan Gibb,Patrick Ferris,David Allsopp,Michael Winston Dales,Mark Elvers,Thomas Gazagnaire,Sadiq Jaffer,Thomas Leonard,Jon Ludlam,Anil Madhavapeddy*

Main category: cs.SE

TL;DR: 本文提出了HyperRes，一种基于超图的版本依赖解析系统，用于跨语言生态系统的依赖管理，无需改变用户当前的包管理器选择。


<details>
  <summary>Details</summary>
Motivation: 解决多语言项目中依赖无法跨生态系统精确表达的问题，以及外部系统和硬件依赖通常未版本化的局限性。

Method: 定义HyperRes系统，通过超图建模依赖关系，并实现多种包管理器到HyperRes的转换。

Result: 证明依赖解析可以在当前分离的生态系统中跨平台工作。

Conclusion: HyperRes为跨生态系统依赖管理提供了通用解决方案，且灵活适应不同部署环境。

Abstract: Package managers are everywhere, with seemingly every language and operating
system implementing their own solution. The lack of interoperability between
these systems means that multi-lingual projects are unable to express precise
dependencies across language ecosystems, and external system and hardware
dependencies are typically implicit and unversioned. We define HyperRes, a
formal system for describing versioned dependency resolution using a hypergraph
that is expressive enough to model many ecosystems and solve dependency
constraints across them. We define translations from dozens of existing package
managers to HyperRes and comprehensively demonstrate that dependency resolution
can work across ecosystems that are currently distinct. This does not require
users to shift their choice of package managers; instead, HyperRes allows for
the translation of packaging metadata between ecosystems, and for solving to be
precisely specialised to a particular deployment environment.

</details>


### [11] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/abs/2506.10376)
*Fan Wu,Cuiyun Gao,Shuqing Li,Xin-Cheng Wen,Qing Liao*

Main category: cs.SE

TL;DR: LayoutCoder是一个基于MLLM的框架，用于从网页图像生成UI代码，通过元素关系构建、UI布局解析和布局引导的代码融合三个模块，解决了现有方法在布局理解和代码生成上的不足，并在性能上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: UI2Code的任务在网站开发中耗时且劳动密集，现有的深度学习方法依赖大量标注数据且泛化能力有限，Multimodal Large Language Models (MLLMs)虽然有望解决这一问题，但在复杂布局理解和准确代码生成上存在困难。因此，提出了LayoutCoder来改进这些问题。

Method: LayoutCoder包括三个关键模块：(1) 元素关系构建，捕捉UI布局；(2) UI布局解析，生成布局树；(3) 布局引导的代码融合，生成准确且保留布局的代码。

Result: 实验结果表明，LayoutCoder在BLEU和CLIP分数上分别比基线方法提高了10.14%和3.95%。

Conclusion: LayoutCoder在UI2Code任务中表现出色，优于现有方法，尤其是在处理复杂布局和生成准确代码方面。

Abstract: Converting user interfaces into code (UI2Code) is a crucial step in website
development, which is time-consuming and labor-intensive. The automation of
UI2Code is essential to streamline this task, beneficial for improving the
development efficiency. There exist deep learning-based methods for the task;
however, they heavily rely on a large amount of labeled training data and
struggle with generalizing to real-world, unseen web page designs. The advent
of Multimodal Large Language Models (MLLMs) presents potential for alleviating
the issue, but they are difficult to comprehend the complex layouts in UIs and
generate the accurate code with layout preserved. To address these issues, we
propose LayoutCoder, a novel MLLM-based framework generating UI code from
real-world webpage images, which includes three key modules: (1) Element
Relation Construction, which aims at capturing UI layout by identifying and
grouping components with similar structures; (2) UI Layout Parsing, which aims
at generating UI layout trees for guiding the subsequent code generation
process; and (3) Layout-Guided Code Fusion, which aims at producing the
accurate code with layout preserved. For evaluation, we build a new benchmark
dataset which involves 350 real-world websites named Snap2Code, divided into
seen and unseen parts for mitigating the data leakage issue, besides the
popular dataset Design2Code. Extensive evaluation shows the superior
performance of LayoutCoder over the state-of-the-art approaches. Compared with
the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and
3.95% in the CLIP score on average across all datasets.

</details>


### [12] [Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization](https://arxiv.org/abs/2506.10624)
*Lukas Jünger,Jan Henrik Weinstock,Tim Kraus*

Main category: cs.SE

TL;DR: 论文提出利用容器化封装虚拟平台（VPs），以减少环境依赖并支持云部署，加速硬件/软件协同开发。


<details>
  <summary>Details</summary>
Motivation: 解决硬件滞后对早期软件开发的阻碍，特别是在安全关键领域。

Method: 基于SystemC TLM-2.0标准，利用容器化技术封装VPs，结合开源工具如QEMU和VCML，实现云部署和并行测试。

Result: 通过AI加速器VP案例验证了方法的有效性。

Conclusion: 提出的方案为复杂硬件/软件系统开发提供了实际可行的解决方案。

Abstract: The ever-increasing complexity of HW/SW systems presents a persistent
challenge, particularly in safety-critical domains like automotive, where
extensive testing is imperative. However, the availability of hardware often
lags behind, hindering early-stage software development. To address this,
Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a
pivotal solution, enabling pre-silicon execution and testing of unmodified
target software. In this study, we propose an approach leveraging
containerization to encapsulate VPs in order to reduce environment dependencies
and enable cloud deployment for fast, parallelized test execution, as well as
open-source VP technologies such as QEMU and VCML to obviate the need for seat
licenses. To demonstrate the efficacy of our approach, we present an Artificial
Intelligence (AI) accelerator VP case study. Through our research, we offer a
robust solution to address the challenges posed by the complexity of HW/SW
systems, with practical implications for accelerating HW/SW co-development.

</details>


### [13] [Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation](https://arxiv.org/abs/2506.10397)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 本文提出了一种基于规则的自动化框架，用于分类量子软件存储库中的问题，覆盖类型、类别、严重性和影响的质量属性。通过关键词和启发式技术，该框架在准确性和F1分数上表现良好，但严重性分类仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 提高量子软件的质量需要准确的错误分类。研究旨在开发一种自动化框架，专注于量子特定错误类型，以帮助开发者更好地理解和解决问题。

Method: 采用基于关键词和启发式的规则分类方法，对36个Qiskit存储库中的12,910个问题进行了分层抽样（4,984个）的人工与自动化分类对比。

Result: 框架在准确性和F1分数上表现良好（最高85.21%），但对严重性分类的Kappa系数较低（0.162）。大多数问题是低严重性（93.7%），量子特定错误占27.3%。

Conclusion: 该框架在量子软件错误分类中表现可靠，尤其是非严重性分类，但严重性分类需进一步优化。分析结果为量子软件质量改进提供了重要参考。

Abstract: Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen's
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.

</details>


### [14] [Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models](https://arxiv.org/abs/2506.10426)
*Xiao Yu,Haoxuan Chen,Feifei Niu,Xing Hu,Jacky Wai Keung,Xin Xia*

Main category: cs.SE

TL;DR: 本文对DeepSpeed、Megatron-LM和Colossal-AI三个分布式训练/推理框架的308个已修复错误进行了首次大规模实证分析，研究了错误症状、根本原因及修复策略。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的发展，分布式框架的复杂性带来了严重的软件错误问题，亟需分析其特征以提高质量和调试效率。

Method: 通过分析308个固定错误，总结了错误症状、根本原因、识别与修复难度，并探讨了低代码量修复策略的自动化潜力。

Result: 48%的错误修复仅需少量代码更改（<=10 LOC），且可通过简单策略（如条件逻辑优化、参数处理增强等）实现，具备自动化潜力。

Conclusion: 研究为提高分布式框架及LLM项目的可靠性提供了指导，并建议利用LLM工具实现自动化调试与修复。

Abstract: With the rapid development of large language models (LLMs), distributed
training and inference frameworks like DeepSpeed have become essential for
scaling model training and inference across multiple GPUs or nodes. However,
the increasing complexity of these frameworks brings non-trivial software bugs,
which may degrade training performance, cause unexpected failures, and result
in significant resource waste. Understanding framework bugs' characteristics is
fundamental for quality assurance, allowing the design of more effective
debugging and repair methods. Thus, our paper conducts the first large-scale
empirical analysis of 308 fixed bugs across three popular distributed
training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We
examine bug symptoms, root causes, bug identification and fixing efforts, and
common low-effort fixing strategies. Additionally, the distributed nature of
these frameworks introduces unique bug root causes, such as allocation strategy
error and distributed communication error. Diagnosing and fixing complex bugs
remains challenging due to factors like the disconnect between symptoms and
root causes, high bug reproduction costs, and low-level or cross-component
interactions. Interestingly, we observe that 48% of bug fixes require minimal
code changes (<=10 LOC) and follow simple strategies such as conditional logic
optimization, parameter handling enhancement, or version compatibility
handling, indicating potential for automation. Based on these insights, we
offer several implications for improving the reliability of both distributed
training and inference frameworks and their dependent LLM projects, while also
identifying opportunities to leverage LLM-based tools for automated debugging
and repair.

</details>


### [15] [EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair](https://arxiv.org/abs/2506.10484)
*Fangwen Mu,Junjie Wang,Lin Shi,Song Wang,Shoubin Li,Qing Wang*

Main category: cs.SE

TL;DR: ExpeRepair是一种基于LLM的新方法，通过双通道知识积累从历史修复经验中学习，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前软件修复方法存在孤立处理问题和静态提示策略的局限性，未能充分利用历史经验。

Method: 提出ExpeRepair，结合情景记忆和语义记忆的双通道知识，动态生成上下文感知的提示。

Result: 在SWE-bench Lite基准测试中，ExpeRepair以49.3%的pass@1分数超越所有开源方法。

Conclusion: ExpeRepair通过动态学习和记忆整合，显著提升软件修复的通用性和效果。

Abstract: Automatically repairing software issues remains a fundamental challenge at
the intersection of software engineering and AI. Although recent advancements
in Large Language Models (LLMs) have demonstrated potential for
repository-level repair tasks, current methodologies exhibit two notable
limitations: (1) they often address issues in isolation, neglecting to
incorporate insights from previously resolved issues, and (2) they rely on
static and rigid prompting strategies, which constrain their ability to
generalize across diverse and evolving issue scenarios. Inspired by the dual
memory systems of human cognition, where episodic and semantic memories work
synergistically to support human reasoning and decision-making, we propose
ExpeRepair, a novel LLM-based approach that continuously learns from historical
repair experiences through dual-channel knowledge accumulation. ExpeRepair
organizes historical repair experiences into two complementary memories: an
episodic memory that stores concrete repair demonstrations, and a semantic
memory that encodes abstract reflective insights. At inference time, ExpeRepair
activates both memory systems by retrieving relevant demonstrations from
episodic memory and recalling high-level repair insights from semantic memory.
It further enhances adaptability through dynamic prompt composition,
synergistically integrating both memory types to replace static prompts with
context-aware, experience-driven prompts. Experiments on the SWE-bench Lite
benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with
Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.

</details>


### [16] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: BugGen是一种利用大型语言模型（LLM）自主生成、插入和验证RTL中功能性错误的工具，显著提高了验证效率和机器学习辅助调试能力。


<details>
  <summary>Details</summary>
Motivation: 硬件复杂性导致验证资源紧张，需要一种能够生成多样化且可扩展的错误数据集的工具，以改进调试效率。

Method: BugGen采用多代理架构，通过分区模块、选择突变目标、迭代优化和回滚机制，确保生成的错误在语法和功能上可靠。

Result: 在五个OpenTitan IP模块中，BugGen生成了500个独特错误，功能准确率达到94%，每小时生成17.7个已验证错误，并发现了104个之前未检测到的错误。

Conclusion: BugGen为生成高质量错误数据集提供了可扩展的解决方案，显著提升了验证效率和机器学习辅助调试的有效性。

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [17] [AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length](https://arxiv.org/abs/2506.10525)
*Junhang Cheng,Fang Liu,Chengru Wu,Li Zhang*

Main category: cs.SE

TL;DR: AdaptiveLLM是一个动态选择最优大型语言模型（LLM）的框架，通过自动评估任务难度来优化性能与成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在代码生成中性能与成本难以平衡，现有方法资源密集且依赖人工标注的难度标签，缺乏实用性。

Method: 框架通过Chain-of-Thought长度估计任务难度，用k-means聚类为三级，结合CodeBERT嵌入难度特征，XGBoost分类器选择最优模型。

Result: 实验显示，AdaptiveLLM在pass@1得分上提升7.86%，资源消耗减少88.9%；与单一模型相比，精度提高15%且成本持平。

Conclusion: AdaptiveLLM通过自动难度评估优化模型选择，显著提升性能与成本效率，其难度评估比人工更可靠。

Abstract: While Large Language Models (LLMs) have significantly advanced code
generation efficiency, they face inherent challenges in balancing performance
and inference costs across diverse programming tasks. Dynamically selecting the
optimal LLM based on task difficulty and resource constraints offers a
promising approach to achieve an optimal balance between efficiency and
performance. However, existing model selection methods are resource-intensive
and often neglect cost efficiency. Moreover, these approaches rely on
human-annotated difficulty labels that are frequently inaccessible in
real-world settings and may not align with the LLM's own assessment of task
difficulty. In this paper, we introduce AdaptiveLLM, a framework that
dynamically selects optimal LLMs for a given coding task by automatically
assessing task difficulty. Our framework first estimates task difficulty using
Chain-of-Thought lengths generated by reasoning model, clusters these into
three difficulty levels via k-means, and fine-tunes CodeBERT to embed
difficulty-aware features. A trained XGBoost classifier then selects the best
model for each problem, optimizing the performance-cost trade-off. Experimental
results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score
while reducing resource consumption by 88.9% compared to baseline method
ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an
approximately 15% accuracy improvement, while maintaining the same level of
cost consumption. Apart from that, the difficulty assessment using CoT provides
more reliable selection criteria than human evaluation. Our replication package
is available at https://github.com/cjhCoder7/AdaptiveLLM.

</details>


### [18] [Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub](https://arxiv.org/abs/2506.10654)
*Abir Bouraffa,Carolin Brandt,Andy Zaidmann,Walid Maalej*

Main category: cs.SE

TL;DR: 研究发现，代码审查中44.6%的拉取请求评论顺序为非字母顺序，且非字母顺序审查的文件比例更高，但获得批准的评率略低。


<details>
  <summary>Details</summary>
Motivation: 探讨开发者在代码审查中遵循的导航顺序，以优化审查工具支持。

Method: 分析100个Java和Python仓库中23,241个拉取请求的评论顺序。

Result: 44.6%的拉取请求评论顺序为非字母顺序，其中部分遵循特定逻辑顺序（如最大差异优先、与标题相似度、测试优先）。非字母顺序审查的文件比例更高，但批准率略低。

Conclusion: 需要为更大规模的拉取请求提供更多审查支持，尤其是当审查者采用复杂策略时。

Abstract: Developers use tools such as GitHub pull requests to review code, discuss
proposed changes, and request modifications. While changed files are commonly
presented in alphabetical order, this does not necessarily coincide with the
reviewer's preferred navigation sequence. This study investigates the different
navigation orders developers follow while commenting on changes submitted in
pull requests. We mined code review comments from 23,241 pull requests in 100
popular Java and Python repositories on GitHub to analyze the order in which
the reviewers commented on the submitted changes. Our analysis shows that for
44.6% of pull requests, the reviewers comment in a non-alphabetical order.
Among these pull requests, we identified traces of alternative meaningful
orders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were
commented in the order of the files' similarity to the pull request's title and
description, and 29% (1,188) of pull requests containing changes to both
production and test files adhered to a test-first order. We also observed that
the proportion of reviewed files to total submitted files was significantly
higher in non-alphabetically ordered reviews, which also received slightly
fewer approvals from reviewers, on average. Our findings highlight the need for
additional support during code reviews, particularly for larger pull requests,
where reviewers are more likely to adopt complex strategies rather than
following a single predefined order.

</details>


### [19] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目致力于通过自然语言处理、本体论和人工智能技术，解决形式化规范的跟踪与验证问题。


<details>
  <summary>Details</summary>
Motivation: 解决软件设计中形式化规范的跟踪与验证挑战，从设计到实现的全程支持。

Method: 采用自然语言处理、本体论描述、相似性重用、大语言模型和人工智能指导。

Result: 自动生成形式化规范，并提供需求跟踪支持。

Conclusion: VERIFAI项目通过多技术融合，提升了形式化规范跟踪与验证的效率。

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


### [20] [From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models](https://arxiv.org/abs/2506.10770)
*Joran Leest,Claudia Raibulet,Patricia Lago,Ilias Gerostathopoulos*

Main category: cs.SE

TL;DR: 论文提出了一个系统性框架C-SAR，用于分类和结构化机器学习监控中的上下文信息，并识别了20种可重用模式。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习监控研究缺乏对上下文信息的系统性理解，导致难以进行有效的异常检测和根因分析。

Method: 通过系统性综述94项研究，提出了C-SAR框架，对上下文信息进行分类和结构化，并识别20种可重用模式。

Result: 开发了C-SAR框架和20种可重用模式，帮助从统计异常检测转向系统性监控。

Conclusion: 研究为机器学习监控提供了新视角，从解读统计信息转向构建系统化的监控实践。

Abstract: Machine learning (ML) models in production do not fail due to statistical
anomalies in their input data; they fail due to contextual misalignment -- when
their environment deviates from training assumptions, leading to unreliable
predictions. Effective ML monitoring requires rich contextual information to
move beyond detecting statistical shifts toward meaningful alerts and
systematic root-cause analysis. Yet, surprisingly, despite extensive research
in ML monitoring and related disciplines (drift detection, data validation,
out-of-distribution detection), there is no shared understanding of how to use
contextual information -- striking, given that monitoring involves
interpretation of information in context. In response, this paper presents a
systematic review to characterize and structure the various types of contextual
information in this domain. Our analysis examines 94 primary studies across
data mining, databases, software engineering, and ML. We introduce the
Contextual System--Aspect--Representation (C-SAR) framework, a conceptual model
that synthesizes our findings. We also identify 20 recurring and potentially
reusable patterns of specific system, aspect, and representation combinations,
and map them to the monitoring activities they support. This study provides a
new perspective on ML monitoring: from interpreting "tea leaves" of
observational statistics into constructing and managing "system maps" that
enable systematic and reliable ML monitoring practices.

</details>


### [21] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 本文提出了一个多阶段分析管道，用于大规模分析用户对AI移动应用的反馈，揭示了用户的关注焦点和情感主题。


<details>
  <summary>Details</summary>
Motivation: 研究用户如何感知、评价和批评AI移动应用的功能，填补了这一领域的空白。

Method: 利用292个AI驱动的应用和894K条用户评论，开发了一个多阶段分析管道，包括分类、情感提取和聚类，并验证了大型语言模型和提示策略的效果。

Result: 提取了100多万个情感对，发现了18个积极和15个消极主题，用户反馈集中在生产力、可靠性和技术失败等方面。

Conclusion: 该方法能更真实地反映用户对AI应用的实际体验，揭示了满意和不满的具体原因，并发现了跨类别的共性和特异性问题。

Abstract: Artificial Intelligence (AI)-powered features have rapidly proliferated
across mobile apps in various domains, including productivity, education,
entertainment, and creativity. However, how users perceive, evaluate, and
critique these AI features remains largely unexplored, primarily due to the
overwhelming volume of user feedback. In this work, we present the first
comprehensive, large-scale study of user feedback on AI-powered mobile apps,
leveraging a curated dataset of 292 AI-driven apps across 14 categories with
894K AI-specific reviews from Google Play. We develop and validate a
multi-stage analysis pipeline that begins with a human-labeled benchmark and
systematically evaluates large language models (LLMs) and prompting strategies.
Each stage, including review classification, aspect-sentiment extraction, and
clustering, is validated for accuracy and consistency. Our pipeline enables
scalable, high-precision analysis of user feedback, extracting over one million
aspect-sentiment pairs clustered into 18 positive and 15 negative user topics.
Our analysis reveals that users consistently focus on a narrow set of themes:
positive comments emphasize productivity, reliability, and personalized
assistance, while negative feedback highlights technical failures (e.g.,
scanning and recognition), pricing concerns, and limitations in language
support. Our pipeline surfaces both satisfaction with one feature and
frustration with another within the same review. These fine-grained,
co-occurring sentiments are often missed by traditional approaches that treat
positive and negative feedback in isolation or rely on coarse-grained analysis.
To this end, our approach provides a more faithful reflection of the real-world
user experiences with AI-powered apps. Category-aware analysis further uncovers
both universal drivers of satisfaction and domain-specific frustrations.

</details>


### [22] [Evaluating Large Language Models on Non-Code Software Engineering Tasks](https://arxiv.org/abs/2506.10833)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: SELU是首个用于评估大型语言模型（LLM）在17项非代码软件工程任务上表现的基准，涵盖分类、回归等任务。研究发现中等规模的解码器模型表现最佳，代码预训练的改进有限。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在非代码软件工程任务中的表现，填补现有研究的空白。

Method: 构建SELU基准，训练22个开源LLM和2个基线模型，使用贝叶斯符号秩检验比较性能。

Result: 中等规模的解码器模型表现最优，代码预训练的改进效果有限。

Conclusion: SELU为选择非代码任务模型提供指导，未来可扩展至生成和设计场景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code understanding and generation; however, their effectiveness on non-code
Software Engineering (SE) tasks remains underexplored. We present the first
comprehensive benchmark, which we name `Software Engineering Language
Understanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from
identifying whether a requirement is functional or non-functional to estimating
the effort and complexity of backlog items. SELU covers classification,
regression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)
targets, with data drawn from diverse sources such as code repositories, issue
tracking systems, and developer forums. We fine-tune 22 open-source LLMs,
prompt two proprietary alternatives, and train two baselines. Performance is
measured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and
compared via the Bayesian signed-rank test. Our results show that
moderate-scale decoder-only models consistently form a top-tier, exhibiting
high mean performance and low across-task variance, while domain adaptation via
code-focused pre-training might yield only modest improvements. These insights
guide model selection for non-code SE workflows and highlight directions for
expanding SELU to generative and design-oriented scenarios.

</details>


### [23] [MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework](https://arxiv.org/abs/2506.10869)
*Quinn Thibeault,Giulia Pedrielli*

Main category: cs.SE

TL;DR: MultiCoSim是一个基于Python的仿真框架，旨在解决现有仿真工具在自动化、灵活性和模块化方面的不足，支持分布式、组件化的协同仿真。


<details>
  <summary>Details</summary>
Motivation: 随着信息物理系统（CPS）复杂性和规模的增加，尤其是安全关键和基于学习的场景，传统仿真工具在配置和灵活性上的局限阻碍了其发展。

Method: 通过MultiCoSim框架，用户可以以编程方式定义、组合和配置仿真组件，支持分布式协同仿真和组件无缝替换。

Result: 案例研究表明，MultiCoSim能够有效简化CPS仿真流程，支持与现成平台（如PX4自动驾驶仪）的集成。

Conclusion: MultiCoSim为CPS研究和开发提供了更灵活的仿真工具，克服了现有工具的局限性。

Abstract: Simulation is a foundational tool for the analysis and testing of
cyber-physical systems (CPS), underpinning activities such as algorithm
development, runtime monitoring, and system verification. As CPS grow in
complexity and scale, particularly in safety-critical and learning-enabled
settings, accurate analysis and synthesis increasingly rely on the rapid use of
simulation experiments. Because CPS inherently integrate hardware, software,
and physical processes, simulation platforms must support co-simulation of
heterogeneous components at varying levels of fidelity. Despite recent advances
in high-fidelity modeling of hardware, firmware, and physics, co-simulation in
diverse environments remains challenging. These limitations hinder the
development of reusable benchmarks and impede the use of simulation for
automated and comparative evaluation.
  Existing simulation tools often rely on rigid configurations, lack automation
support, and present obstacles to portability and modularity. Many are
configured through static text files or impose constraints on how simulation
components are represented and connected, making it difficult to flexibly
compose systems or integrate components across platforms.
  To address these challenges, we introduce MultiCoSim, a Python-based
simulation framework that enables users to define, compose, and configure
simulation components programmatically. MultiCoSim supports distributed,
component-based co-simulation and allows seamless substitution and
reconfiguration of components. We demonstrate the flexibility of MultiCoSim
through case studies that include co-simulations involving custom
automaton-based controllers, as well as integration with off-the-shelf
platforms like the PX4 autopilot for aerial robotics. These examples highlight
MultiCoSim's capability to streamline CPS simulation pipelines for research and
development.

</details>


### [24] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出SWE-Factory自动化管道，解决GitHub问题解决任务数据集的构建难题，整合环境搭建、评分和验证三个核心组件，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 构建大规模GitHub问题解决数据集对训练和评估语言模型至关重要，但传统方法耗时耗力。

Method: SWE-Factory包含SWE-Builder（多agent环境构建）、标准化退出码评分和自动化fail2pass验证。

Result: 实验显示，SWE-Builder高效构建任务实例，退出码评分100%准确，fail2pass验证精度0.92，召回1.00。

Conclusion: 自动化管道加速高质量数据集构建，代码和数据集已开源。

Abstract: Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [25] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: 提出了一种将大型语言模型（LLM）与持久性交互式Lisp环境整合的新架构，支持LLM通过编程与REPL交互动态定义工具。


<details>
  <summary>Details</summary>
Motivation: 为了增强LLM的能力，使其能够动态创建和使用工具，同时结合符号编程和神经语言生成的交互式AI系统。

Method: 通过在生成中嵌入Lisp表达式并通过中间件拦截，实现状态化外部存储器、反射式编程和动态工具创建。

Result: 设计框架和架构原则为未来交互式AI系统的实现提供了指导。

Conclusion: 该架构为LLM与符号编程的结合提供了新的可能性，推动了交互式AI系统的发展。

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


### [26] [A Language-Agnostic Logical Relation for Message-Passing Protocols](https://arxiv.org/abs/2506.10026)
*Tesla Zhang,Sonya Simkin,Rui Li,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: 论文提出了一种验证异构消息传递系统协议合规性的框架，首次实现了与语言无关的逻辑关系机械化，基于标记转换语义，适用于任意对象。


<details>
  <summary>Details</summary>
Motivation: 随着分布式和异构系统应用的普及，验证其协议合规性成为挑战，传统基于共同语言或类型系统的方法不再适用。

Method: 开发了一种基于标记转换语义的语言无关逻辑关系框架，并在Coq中机械化实现，支持任意对象。

Result: 框架成功应用于单实例验证和类型化应用的一次性验证，展示了其广泛适用性。

Conclusion: 该框架为异构消息传递系统的协议验证提供了通用解决方案，支持灵活的应用场景。

Abstract: Today's computing landscape has been gradually shifting to applications
targeting distributed and *heterogeneous* systems, such as cloud computing and
Internet of Things (IoT) applications. These applications are predominantly
*concurrent*, employ *message-passing*, and interface with *foreign objects*,
ranging from externally implemented code to actual physical devices such as
sensors. Verifying that the resulting systems adhere to the intended protocol
of interaction is challenging -- the usual assumption of a common
implementation language, let alone a type system, no longer applies, ruling out
any verification method based on them. This paper develops a framework for
certifying *protocol compliance* of heterogeneous message-passing systems. It
contributes the first mechanization of a *language-agnostic logical relation*,
asserting that its inhabitants comply with the protocol specified. This
definition relies entirely on a labelled transition-based semantics,
accommodating arbitrary inhabitants, typed and untyped alike, including foreign
objects. As a case study, the paper considers two scenarios: (1) *per-instance
verification* of a specific application or hardware device, and (2)
*once-and-for-all verification* of well-typed applications for a given type
system. The logical relation and both scenarios are mechanized in the Coq
theorem prover.

</details>


### [27] [Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations](https://arxiv.org/abs/2506.10781)
*Zhiyao Zhong,Cyrus Omar*

Main category: cs.PL

TL;DR: Hazel Deriver是一个基于网络的实时编辑器，旨在帮助学生通过多层次支持构建规则推导树，解决推理规则的复杂性和缺乏即时反馈的问题。初步研究表明，它降低了任务的难度并提高了学习效果。


<details>
  <summary>Details</summary>
Motivation: 学生在编程和形式逻辑课程中常因推理规则复杂、缺乏即时反馈和手动证明的繁琐而困扰，因此需要一种工具来辅助学习。

Method: 开发了基于Hazel实时编程环境的Hazel Deriver编辑器，提供结构化交互和实时反馈，支持迭代探索。

Result: 初步用户研究表明，Hazel Deriver降低了任务的感知难度，提升了概念理解和学习参与度。

Conclusion: 论文讨论了分层支持功能的设计，并探讨了系统引导与学习者自主性之间的平衡问题。

Abstract: Students in programming languages and formal logic courses often struggle
with constructing rule-based derivation trees due to the complexity of applying
inference rules, the lack of immediate feedback, and the manual effort required
for handwritten proofs. We present Hazel Deriver, a live, web-based editor
designed to scaffold derivation construction through multiple layers of
support. Built on the Hazel live programming environment, it provides a
structured, interactive experience that encourages iterative exploration and
real-time feedback. A preliminary user study with former students suggests that
Hazel Deriver reduces the perceived difficulty of derivation tasks while
improving conceptual understanding and engagement. We discuss the design of its
layered scaffolding features and raise questions about balancing system
guidance with learner autonomy.

</details>


### [28] [Choreographic Quick Changes: First-Class Location (Set) Polymorphism](https://arxiv.org/abs/2506.10913)
*Ashley Samuelson,Andrew K. Hirsch,Ethan Cecchetti*

Main category: cs.PL

TL;DR: 介绍了λQC，首个具有一流进程名称和类型/位置多态性的类型化编排语言，支持代数/递归数据类型和多位置值。


<details>
  <summary>Details</summary>
Motivation: 现有的编排语言缺乏现代系统所需的关键功能，如动态计算执行者并将其传递给其他节点的能力。

Method: 开发了λQC，引入了一流进程名称、类型/位置多态性，并支持代数/递归数据类型和多位置值。

Result: 在Rocq中形式化并验证了结果，包括无死锁的标准编排保证。

Conclusion: λQC填补了现有编排语言的不足，并提升了表达能力。

Abstract: Choreographic programming is a promising new paradigm for programming
concurrent systems where a developer writes a single centralized program that
compiles to individual programs for each node. Existing choreographic
languages, however, lack critical features integral to modern systems, like the
ability of one node to dynamically compute who should perform a computation and
send that decision to others. This work addresses this gap with $\lambda_{QC}$,
the first typed choreographic language with \emph{first class process names}
and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also
improves expressive power over previous work by supporting algebraic and
recursive data types as well as multiply-located values. We formalize and
mechanically verify our results in Rocq, including the standard choreographic
guarantee of deadlock freedom.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [29] [Playing in the Sandbox: A Study on the Usability of Seccomp](https://arxiv.org/abs/2506.10234)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.OS

TL;DR: 研究探讨了经验丰富的Seccomp开发者在使用沙盒技术时面临的挑战及其解决方法。


<details>
  <summary>Details</summary>
Motivation: 尽管沙盒技术可以限制应用程序行为并防止被利用，但实际应用中沙盒使用率较低。研究旨在找出开发者使用Seccomp时的困难。

Method: 对7名有经验的Seccomp开发者进行可用性试验，观察他们在沙盒化应用程序时的方法和面临的挑战。

Result: 开发者采用了不同的沙盒化方法，并提出多种解决方案。研究总结了Seccomp的使用挑战和开发者的建议。

Conclusion: 研究揭示了Seccomp的实际应用难度，并提出改进建议以提升沙盒技术的可用性。

Abstract: Sandboxing restricts what applications do, and prevents exploited processes
being abused; yet relatively few applications get sandboxed: why? We report a
usability trial with 7 experienced Seccomp developers exploring how they
approached sandboxing an application and the difficulties they faced. The
developers each approached sandboxing the application differently and each came
to different solutions. We highlight many challenges of using Seccomp, the
sandboxing designs by the participants, and what developers think would make it
easier for them to sandbox applications effectively.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [30] [AI5GTest: AI-Driven Specification-Aware Automated Testing and Validation of 5G O-RAN Components](https://arxiv.org/abs/2506.10111)
*Abiodun Ganiyu,Pranshav Gajjar,Vijay K Shah*

Main category: cs.NI

TL;DR: AI5GTest是一个基于AI的自动化测试框架，用于验证O-RAN组件，通过LLM协作提高效率与准确性。


<details>
  <summary>Details</summary>
Motivation: O-RAN的开放架构带来测试复杂性，现有手动方法效率低且易出错。

Method: 采用Gen-LLM、Val-LLM和Debug-LLM协作框架，自动化生成流程、验证合规性及根因分析。

Result: 与传统手动方法相比，显著减少测试时间且保持高准确性。

Conclusion: AI5GTest解决了O-RAN测试的挑战，提升了效率与可靠性。

Abstract: The advent of Open Radio Access Networks (O-RAN) has transformed the
telecommunications industry by promoting interoperability, vendor diversity,
and rapid innovation. However, its disaggregated architecture introduces
complex testing challenges, particularly in validating multi-vendor components
against O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as
those provided by Open Testing and Integration Centres (OTICs), rely heavily on
manual processes, are fragmented and prone to human error, leading to
inconsistency and scalability issues. To address these limitations, we present
AI5GTest -- an AI-powered, specification-aware testing framework designed to
automate the validation of O-RAN components. AI5GTest leverages a cooperative
Large Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and
Debug-LLM. Gen-LLM automatically generates expected procedural flows for test
cases based on 3GPP and O-RAN specifications, while Val-LLM cross-references
signaling messages against these flows to validate compliance and detect
deviations. If anomalies arise, Debug-LLM performs root cause analysis,
providing insight to the failure cause. To enhance transparency and
trustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the
Gen-LLM presents top-k relevant official specifications to the tester for
approval before proceeding with validation. Evaluated using a range of test
cases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest
demonstrates a significant reduction in overall test execution time compared to
traditional manual methods, while maintaining high validation accuracy.

</details>


### [31] [Large Language Models-Empowered Wireless Networks: Fundamentals, Architecture, and Challenges](https://arxiv.org/abs/2506.10651)
*Latif U. Khan,Maher Guizani,Sami Muhaidat,Choong Seon Hong*

Main category: cs.NI

TL;DR: 文章探讨了无线网络与大型语言模型（LLMs）集成的挑战与潜力，提出了LLM原生无线系统的概念，并通过案例展示了其分布式实现的优越性。


<details>
  <summary>Details</summary>
Motivation: 无线网络的快速发展带来了服务质量的需求转向用户体验的新指标，而LLMs为解决复杂问题提供了新思路，因此探讨两者的集成具有重要意义。

Method: 提出了LLM原生无线系统的概念，并通过基于双深度Q学习（DDQN）的分布式实现进行案例研究。

Result: 提出的DDQN解决方案优于现有方法，展示了LLM-native无线系统的潜力。

Conclusion: LLM与无线网络的集成具有广阔前景，但仍需解决开放挑战。

Abstract: The rapid advancement of wireless networks has resulted in numerous
challenges stemming from their extensive demands for quality of service towards
innovative quality of experience metrics (e.g., user-defined metrics in terms
of sense of physical experience for haptics applications). In the meantime,
large language models (LLMs) emerged as promising solutions for many difficult
and complex applications/tasks. These lead to a notion of the integration of
LLMs and wireless networks. However, this integration is challenging and needs
careful attention in design. Therefore, in this article, we present a notion of
rational wireless networks powered by \emph{telecom LLMs}, namely,
\emph{LLM-native wireless systems}. We provide fundamentals, vision, and a case
study of the distributed implementation of LLM-native wireless systems. In the
case study, we propose a solution based on double deep Q-learning (DDQN) that
outperforms existing DDQN solutions. Finally, we provide open challenges.

</details>


### [32] [Energy-Efficient Deep Learning for Traffic Classification on Microcontrollers](https://arxiv.org/abs/2506.10851)
*Adel Chehade,Edoardo Ragusa,Paolo Gastaldo,Rodolfo Zunino*

Main category: cs.NI

TL;DR: 提出了一种用于资源受限微控制器的深度学习流量分类方法，通过轻量级1D-CNN和硬件感知神经架构搜索实现高精度与低能耗。


<details>
  <summary>Details</summary>
Motivation: 在IoT设备上实现高效的加密流量分析，兼顾精度与计算资源限制。

Method: 采用硬件感知神经架构搜索优化的轻量级1D-CNN，并进行INT8量化。

Result: 模型在ISCX数据集上达96.59%精度，在两种微控制器上实现低延迟和低能耗。

Conclusion: 验证了在低功耗设备上部署高效流量分类的可行性，为IoT安全提供新方案。

Abstract: In this paper, we present a practical deep learning (DL) approach for
energy-efficient traffic classification (TC) on resource-limited
microcontrollers, which are widely used in IoT-based smart systems and
communication networks. Our objective is to balance accuracy, computational
efficiency, and real-world deployability. To that end, we develop a lightweight
1D-CNN, optimized via hardware-aware neural architecture search (HW-NAS), which
achieves 96.59% accuracy on the ISCX VPN-NonVPN dataset with only 88.26K
parameters, a 20.12K maximum tensor size, and 10.08M floating-point operations
(FLOPs). Moreover, it generalizes across various TC tasks, with accuracies
ranging from 94% to 99%. To enable deployment, the model is quantized to INT8,
suffering only a marginal 1-2% accuracy drop relative to its Float32
counterpart. We evaluate real-world inference performance on two
microcontrollers: the high-performance STM32F746G-DISCO and the cost-sensitive
Nucleo-F401RE. The deployed model achieves inference latencies of 31.43ms and
115.40ms, with energy consumption of 7.86 mJ and 29.10 mJ per inference,
respectively. These results demonstrate the feasibility of on-device encrypted
traffic analysis, paving the way for scalable, low-power IoT security
solutions.

</details>


### [33] [Dynamic Beyond 5G and 6G Connectivity: Leveraging NTN and RIS Synergies for Optimized Coverage and Capacity in High-Density Environments](https://arxiv.org/abs/2506.10900)
*Valdemar Farré,Juan Estrada,David Vega,Luis F Urquiza-Aguiar,Juan A. Vásquez Peralvo,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 提出一种整合非地面网络（NTN）和可重构智能表面（RIS）的6G网络规划框架，以解决高密度环境中的覆盖与容量问题。


<details>
  <summary>Details</summary>
Motivation: 传统地面网络（TN）在高密度环境中覆盖不足，难以满足大型户外活动的通信需求。

Method: 结合LEO卫星、被动RIS平台及B5G技术（如mMIMO和波束成形），并优化频谱使用，实现动态干扰管理。

Result: 仿真验证显示，该框架显著提升了连接性、可靠性和成本效率。

Conclusion: 该策略为未来6G网络的演进需求提供了有前景的解决方案。

Abstract: The increasing demand for reliable, high-capacity communication during
large-scale outdoor events poses significant challenges for traditional
Terrestrial Networks (TNs), which often struggle to provide consistent coverage
in high-density environments. This paper presents a novel 6G radio network
planning framework that integrates Non-Terrestrial Networks (NTNs) with
Reconfigurable Intelligent Surfaces (RISs) to deliver ubiquitous coverage and
enhanced network capacity. Our framework overcomes the limitations of
conventional deployable base stations by leveraging NTN architectures,
including Low Earth Orbit (LEO) satellites and passive RIS platforms seamlessly
integrated with Beyond 5G (B5G) TNs. By incorporating advanced B5G technologies
such as Massive Multiple Input Multiple Output (mMIMO) and beamforming, and by
optimizing spectrum utilization across the C, S, and Ka bands, we implement a
rigorous interference management strategy based on a dynamic SINR model.
Comprehensive calculations and simulations validate the proposed framework,
demonstrating significant improvements in connectivity, reliability, and
cost-efficiency in crowded scenarios. This integration strategy represents a
promising solution for meeting the evolving demands of future 6G networks.

</details>


### [34] [Agentic Semantic Control for Autonomous Wireless Space Networks: Extending Space-O-RAN with MCP-Driven Distributed Intelligence](https://arxiv.org/abs/2506.10925)
*Eduardo Baena,Paolo Testolina,Michele Polese,Sergi Aliaga,Andrew Benincasa,Dimitrios Koutsonikolas,Josep Jornet,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文提出了一种在月球表面无线通信系统中引入语义代理层的扩展方案，通过动态决策提升系统的自主性和适应性。


<details>
  <summary>Details</summary>
Motivation: 月球表面无线通信系统需满足高自主性、抗干扰性和环境适应性，但现有技术（如Space-O-RAN）仅支持静态策略，缺乏语义集成。

Method: 通过结合MCP和A2A协议，部署分布式认知代理实现上下文感知决策，支持实时与非实时控制层的协调策略（如延迟自适应推理和带宽感知语义压缩）。

Result: 该系统能够在月球探测器、着陆器和基站间实现动态协调，优化通信性能。

Conclusion: 所提方案显著提升了月球通信系统的智能化和适应性。

Abstract: Lunar surface operations impose stringent requirements on wireless
communication systems, including autonomy, robustness to disruption, and the
ability to adapt to environmental and mission-driven context. While Space-O-RAN
provides a distributed orchestration model aligned with 3GPP standards, its
decision logic is limited to static policies and lacks semantic integration. We
propose a novel extension incorporating a semantic agentic layer enabled by the
Model Context Protocol (MCP) and Agent-to-Agent (A2A) communication protocols,
allowing context-aware decision making across real-time, near-real-time, and
non-real-time control layers. Distributed cognitive agents deployed in rovers,
landers, and lunar base stations implement wireless-aware coordination
strategies, including delay-adaptive reasoning and bandwidth-aware semantic
compression, while interacting with multiple MCP servers to reason over
telemetry, locomotion planning, and mission constraints.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [35] [Semantic Communication-Enabled Cloud-Edge-End-collaborative Metaverse Services Architecure](https://arxiv.org/abs/2506.10001)
*Yuxuan Li,Sheng Jinag,Bizhu Wang*

Main category: cs.MM

TL;DR: 提出了基于语义通信的云边端协同沉浸式元宇宙服务架构（SC-CEE-Meta），通过语义传输、视频合成和3D重建模块，显著降低了延迟并提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决元宇宙中高分辨率虚拟场景传输的带宽不足、延迟和数据错误问题，提升用户体验。

Method: 采用云边端协同架构，部署语义传输、视频合成和3D重建模块，利用语义信息替代比特级传输。

Result: 在Meta Quest Pro上测试，无线传输延迟降低96.05%，图像质量提升43.99%。

Conclusion: SC-CEE-Meta有效解决了元宇宙服务的传输瓶颈，提升了服务质量和用户体验。

Abstract: With technology advancing and the pursuit of new audiovisual experiences
strengthening, the metaverse has gained surging enthusiasm. However, it faces
practical hurdles as substantial data like high-resolution virtual scenes must
be transmitted between cloud platforms and VR devices. Specifically, the VR
device's wireless transmission hampered by insufficient bandwidth, causes speed
and delay problems. Meanwhile, poor channel quality leads to data errors and
worsens user experience. To solve this, we've proposed the Semantic
Communication-Enabled Cloud-Edge-End Collaborative Immersive Metaverse Service
(SC-CEE-Meta) Architecture, which includes three modules: VR video semantic
transmission, video synthesis, and 3D virtual scene reconstruction. By
deploying semantic modules on VR devices and edge servers and sending key
semantic info instead of focusing on bit-level reconstruction, it can cut
latency, resolve the resource-bandwidth conflict, and better withstand channel
interference. Also, the cloud deploys video synthesis and 3D scene
reconstruction preprocessing, while edge devices host 3D reconstruction
rendering modules, all for immersive services. Verified on Meta Quest Pro, the
SC-CEE-Meta can reduce wireless transmission delay by 96.05\% and boost image
quality by 43.99\% under poor channel condition.

</details>


### [36] [Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming](https://arxiv.org/abs/2506.10004)
*Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: 综述讨论了XR流媒体的最新进展，涵盖了XR的定义、设备、传输需求、体验质量优化方法及当前应用与挑战。


<details>
  <summary>Details</summary>
Motivation: XR技术快速发展，有望彻底改变内容的创作和消费方式，因此需要全面了解其流媒体技术及优化方法。

Method: 通过分析XR流量特征、体验质量影响因素，并介绍基于视觉注意力的优化方法，对XR流媒体进行系统综述。

Result: 总结了XR流媒体的关键技术和优化方法，揭示了提升用户体验的核心因素。

Conclusion: XR流媒体在多个领域具有巨大潜力，但仍面临技术挑战，未来需要进一步研究和创新。

Abstract: Extended reality (XR) is rapidly advancing, and poised to revolutionize
content creation and consumption. In XR, users integrate various sensory inputs
to form a cohesive perception of the virtual environment. This survey reviews
the state-of-the-art in XR streaming, focusing on multiple paradigms. To begin,
we define XR and introduce various XR headsets along with their multimodal
interaction methods to provide a foundational understanding. We then analyze XR
traffic characteristics to highlight the unique data transmission requirements.
We also explore factors that influence the quality of experience in XR systems,
aiming to identify key elements for enhancing user satisfaction. Following
this, we present visual attention-based optimization methods for XR streaming
to improve efficiency and performance. Finally, we examine current applications
and highlight challenges to provide insights into ongoing and future
developments of XR.

</details>


### [37] [EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis](https://arxiv.org/abs/2506.10002)
*Jianwu Fang,Lei-Lei Li,Zhedong Zheng,Hongkai Yu,Jianru Xue,Zhengguo Li,Tat-Seng Chua*

Main category: cs.MM

TL;DR: 本文提出了一种AVD模型和EQ-TAA方法，通过生成因果视频帧解决交通场景中的TAA问题，无需额外标注，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前TAA方法依赖标注且易受数据偏差影响，本文旨在通过视频合成和对比学习解决这些问题。

Method: 使用AVD模型合成事故视频帧，EQ-TAA方法结合对比损失实现鲁棒预测。

Result: AVD和EQ-TAA在实验中表现优于现有技术。

Conclusion: AVD和EQ-TAA通过合成数据和对比学习有效解决了TAA中的问题。

Abstract: Traffic Accident Anticipation (TAA) in traffic scenes is a challenging
problem for achieving zero fatalities in the future. Current approaches
typically treat TAA as a supervised learning task needing the laborious
annotation of accident occurrence duration. However, the inherent long-tailed,
uncertain, and fast-evolving nature of traffic scenes has the problem that real
causal parts of accidents are difficult to identify and are easily dominated by
data bias, resulting in a background confounding issue. Thus, we propose an
Attentive Video Diffusion (AVD) model that synthesizes additional accident
video clips by generating the causal part in dashcam videos, i.e., from normal
clips to accident clips. AVD aims to generate causal video frames based on
accident or accident-free text prompts while preserving the style and content
of frames for TAA after video generation. This approach can be trained using
datasets collected from various driving scenes without any extra annotations.
Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant
triple loss for an anchor accident-free video clip, along with the generated
pair of contrastive pseudo-normal and pseudo-accident clips. Extensive
experiments have been conducted to evaluate the performance of AVD and EQ-TAA,
and competitive performance compared to state-of-the-art methods has been
obtained.

</details>


### [38] [Integrating multimedia documents in 3D city models for a better understanding of territories](https://arxiv.org/abs/2506.10003)
*C. Gautier,J. Delanoy,G. Gesquière*

Main category: cs.MM

TL;DR: 通过整合多媒体文档到3D城市场景中，提供更丰富的上下文信息，帮助理解城市组织和历史变迁。


<details>
  <summary>Details</summary>
Motivation: 现有的3D城市模型缺乏建筑物历史和功能等上下文信息，而多媒体文档（如图片、视频或文本）通常包含这些信息，整合两者有助于更全面地分析和理解城市。

Method: 提出了四种方法将多媒体文档集成到3D城市场景中，并结合用户引导模式，帮助用户更好地理解城市。

Result: 在法国里昂地区的多个项目中验证了这些技术的实用性，例如通过数字导览展示标志性建筑的背景或区域的历史演变。

Conclusion: 整合多媒体文档的3D城市场景技术能够有效提升城市信息的丰富性和可理解性，适用于多种应用场景。

Abstract: Digital 3D representations of urban areas, through their growing
availability, are a helpful tool to better understand a territory. However,
they lack contextual information about, for example, the history or
functionality of buildings. On another side, multimedia documents like images,
videos or texts usually contain such information. Crossing these two types of
data can therefore help in the analysis and understanding of the organization
of our cities. This could also be used to develop document search based on
spatial navigation, instead of the classical textual query. In this paper, we
propose four approaches to integrate multimedia documents in a 3D urban scene,
allowing to contextualize the scene with any type of media. We combine these
integration approaches with user guidance modes that allows to guide the user
through the consumption of these media and support its understanding of the
territory. We demonstrate the usefulness of these techniques in the context of
different projects within the Lyon area (France). The use of multimedia
documents integrated into a digital tour allows, for example, the iconic
buildings to be contextualised or to understand the evolution of a territory
through time.

</details>


### [39] [HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction](https://arxiv.org/abs/2506.10006)
*Jie Qin,Wei Yang,Yan Su,Yiran Zhu,Weizhen Li,Yunyue Pan,Chengchang Pan,Honggang Qi*

Main category: cs.MM

TL;DR: 提出了一个自适应双模态框架，通过动态分支选择器、双向跨模态GAN和混合训练协议，显著提升了HER2评估的准确性，尤其是在资源有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的HER2评估模型通常单独分析H&E或IHC图像，而临床实践中需要它们的协同解读，但同步获取这两种模态存在复杂性和成本问题。

Method: 提出动态分支选择器、双向跨模态GAN和混合训练协议，支持灵活的单/双模态预测。

Result: 单模态H&E预测准确率从71.44%提升至94.25%，双模态准确率达95.09%，且计算效率高（轻量版参数减少78.55%）。

Conclusion: 该框架在不需同步采集的情况下实现接近双模态的性能，尤其适用于资源有限的医疗环境。

Abstract: Current HER2 assessment models for breast cancer predominantly analyze H&E or
IHC images in isolation,despite clinical reliance on their synergistic
interpretation. However, concurrent acquisition of both modalities is often
hindered by workflow complexity and cost constraints. We propose an adaptive
bimodal framework enabling flexible single-/dual-modality HER2 prediction
through three innovations: 1) A dynamic branch selector that activates either
single-modality reconstruction or dual-modality joint inference based on input
completeness; 2) A bidirectional cross-modal GAN performing context-aware
feature-space reconstruction of missing modalities; 3) A hybrid training
protocol integrating adversarial learning and multi-task optimization. This
architecture elevates single-modality H&E prediction accuracy from 71.44% to
94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28%
reliability with sole IHC inputs. The framework's "dual-preferred,
single-compatible" design delivers near-bimodal performance without requiring
synchronized acquisition, particularly benefiting resource-limited settings
through IHC infrastructure cost reduction. Experimental validation confirms
22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with
cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251
(IHC to HE). By dynamically routing inputs through reconstruction-enhanced or
native fusion pathways, the system mitigates performance degradation from
missing data while preserving computational efficiency (78.55% parameter
reduction in lightweight variant). This elastic architecture demonstrates
significant potential for democratizing precise HER2 assessment across diverse
healthcare settings.

</details>


### [40] [Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space](https://arxiv.org/abs/2506.10007)
*Kangwei Liu,Junwu Liu,Xiaowei Yi,Jinlin Guo,Yun Cao*

Main category: cs.MM

TL;DR: 该论文提出了一种基于扩散模型的可控3D面部动画框架，通过多模态情感绑定策略和注意力潜扩散模型，解决了情感动画的多样性和自然性问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前音频驱动的3D面部动画存在两个主要问题：1）依赖单一模态控制信号，未能利用多模态互补优势；2）确定性回归方法限制了情感表达的随机性。这导致动画表现力受限。

Method: 提出了一种基于扩散模型的框架，包含两个关键技术：1）FLAME中心的多模态情感绑定策略，通过对比学习对齐文本、音频和情感标签；2）注意力潜扩散模型，结合内容感知注意力和情感引导层，增强运动多样性并保持时序一致性。

Result: 实验表明，该方法在大多数指标上优于现有方法，情感相似度提升21.6%，同时保持生理合理的面部动态。

Conclusion: 该研究通过多模态情感绑定和扩散模型，成功提升了3D面部动画的情感表现力和自然性，为相关领域提供了新的技术路径。

Abstract: Audio-driven emotional 3D facial animation encounters two significant
challenges: (1) reliance on single-modal control signals (videos, text, or
emotion labels) without leveraging their complementary strengths for
comprehensive emotion manipulation, and (2) deterministic regression-based
mapping that constrains the stochastic nature of emotional expressions and
non-verbal behaviors, limiting the expressiveness of synthesized animations. To
address these challenges, we present a diffusion-based framework for
controllable expressive 3D facial animation. Our approach introduces two key
innovations: (1) a FLAME-centered multimodal emotion binding strategy that
aligns diverse modalities (text, audio, and emotion labels) through contrastive
learning, enabling flexible emotion control from multiple signal sources, and
(2) an attention-based latent diffusion model with content-aware attention and
emotion-guided layers, which enriches motion diversity while maintaining
temporal coherence and natural facial dynamics. Extensive experiments
demonstrate that our method outperforms existing approaches across most
metrics, achieving a 21.6\% improvement in emotion similarity while preserving
physiologically plausible facial dynamics. Project Page:
https://kangweiiliu.github.io/Control_3D_Animation.

</details>


### [41] [Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics](https://arxiv.org/abs/2506.10008)
*Yi-Chun Chen*

Main category: cs.MM

TL;DR: 提出了一种基于分层知识图谱的视觉叙事理解框架，适用于漫画等多模态媒体。


<details>
  <summary>Details</summary>
Motivation: 为了更好地解析和表示视觉叙事中的多层次语义、空间和时间关系，以支持推理任务。

Method: 通过分解叙事内容为多个层次，构建综合知识图谱，包括宏观故事线和细粒度事件段，并在面板级构建多模态图。

Result: 在Manga109数据集上验证了框架的高精度和高召回率，适用于动作检索、对话追踪等任务。

Conclusion: 该框架为叙事内容分析、交互式故事讲述和多模态推理提供了可扩展的基础。

Abstract: This paper presents a hierarchical knowledge graph framework for the
structured understanding of visual narratives, focusing on multimodal media
such as comics. The proposed method decomposes narrative content into multiple
levels, from macro-level story arcs to fine-grained event segments. It
represents them through integrated knowledge graphs that capture semantic,
spatial, and temporal relationships. At the panel level, we construct
multimodal graphs that link visual elements such as characters, objects, and
actions with corresponding textual components, including dialogue and captions.
These graphs are integrated across narrative levels to support reasoning over
story structure, character continuity, and event progression.
  We apply our approach to a manually annotated subset of the Manga109 dataset
and demonstrate its ability to support symbolic reasoning across diverse
narrative tasks, including action retrieval, dialogue tracing, character
appearance mapping, and panel timeline reconstruction. Evaluation results show
high precision and recall across tasks, validating the coherence and
interpretability of the framework. This work contributes a scalable foundation
for narrative-based content analysis, interactive storytelling, and multimodal
reasoning in visual media.

</details>


### [42] [Multimodal Emotion Coupling via Speech-to-Facial and Bodily Gestures in Dyadic Interaction](https://arxiv.org/abs/2506.10010)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: cs.MM

TL;DR: 该论文研究了情绪表达中语音、面部和手势的多模态协调，探讨了非重叠和重叠对话对情感表达的影响，并通过运动捕捉和语音特征预测情感信号。


<details>
  <summary>Details</summary>
Motivation: 深入理解情绪表达中语音、面部和手势的协调机制，有助于提升实时情感检测的准确性，为人类互动和AI系统提供更精确的多模态同步信息。

Method: 使用IEMOCAP语料库中的二元互动数据，通过区域特异性运动捕捉和语音特征（如基频、MFCCs、模型衍生的唤醒度、效价和分类情绪）分析语音与面部及手势的关联。

Result: 非重叠对话更易引发面部和手势的活跃性，尤其是下部和嘴部区域；悲伤在非重叠时表现更强，愤怒则在重叠时抑制手势；语音特征对预测手势运动的准确性差异显著。

Conclusion: 研究表明，对话结构和情绪类型显著影响多模态信号协调，为进一步优化情感检测和人机交互提供了重要依据。

Abstract: Human emotional expression emerges through coordinated vocal, facial, and
gestural signals. While speech face alignment is well established, the broader
dynamics linking emotionally expressive speech to regional facial and hand
motion remains critical for gaining a deeper insight into how emotional and
behavior cues are communicated in real interactions. Further modulating the
coordination is the structure of conversational exchange like sequential turn
taking, which creates stable temporal windows for multimodal synchrony, and
simultaneous speech, often indicative of high arousal moments, disrupts this
alignment and impacts emotional clarity. Understanding these dynamics enhances
realtime emotion detection by improving the accuracy of timing and synchrony
across modalities in both human interactions and AI systems. This study
examines multimodal emotion coupling using region specific motion capture from
dyadic interactions in the IEMOCAP corpus. Speech features included low level
prosody, MFCCs, and model derived arousal, valence, and categorical emotions
(Happy, Sad, Angry, Neutral), aligned with 3D facial and hand marker
displacements. Expressive activeness was quantified through framewise
displacement magnitudes, and speech to gesture prediction mapped speech
features to facial and hand movements. Nonoverlapping speech consistently
elicited greater activeness particularly in the lower face and mouth. Sadness
showed increased expressivity during nonoverlap, while anger suppressed
gestures during overlaps. Predictive mapping revealed highest accuracy for
prosody and MFCCs in articulatory regions while arousal and valence had lower
and more context sensitive correlations. Notably, hand speech synchrony was
enhanced under low arousal and overlapping speech, but not for valence.

</details>


### [43] [WDMIR: Wavelet-Driven Multimodal Intent Recognition](https://arxiv.org/abs/2506.10011)
*Weiyin Gong,Kai Zhang,Yanghai Zhang,Qi Liu,Xinjie Sun,Junyu Lu,Linbo Zhu*

Main category: cs.MM

TL;DR: 本文提出了一种基于小波的多模态意图识别框架（WDMIR），通过频域分析增强非语言信息的理解，实现了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法多注重文本分析，忽视了非语言线索中的丰富语义信息，本文旨在填补这一空白。

Method: 提出小波驱动的融合模块和跨模态交互机制，实现视频和音频特征的频域同步分析与渐进式特征增强。

Result: 在MIntRec数据集上，WDMIR的准确率比之前方法提高了1.13%，消融实验验证了小波融合模块的有效性。

Conclusion: WDMIR框架通过频域分析和跨模态交互，显著提升了多模态意图识别的性能。

Abstract: Multimodal intent recognition (MIR) seeks to accurately interpret user
intentions by integrating verbal and non-verbal information across video, audio
and text modalities. While existing approaches prioritize text analysis, they
often overlook the rich semantic content embedded in non-verbal cues. This
paper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR)
framework that enhances intent understanding through frequency-domain analysis
of non-verbal information. To be more specific, we propose: (1) a
wavelet-driven fusion module that performs synchronized decomposition and
integration of video-audio features in the frequency domain, enabling
fine-grained analysis of temporal dynamics; (2) a cross-modal interaction
mechanism that facilitates progressive feature enhancement from bimodal to
trimodal integration, effectively bridging the semantic gap between verbal and
non-verbal information. Extensive experiments on MIntRec demonstrate that our
approach achieves state-of-the-art performance, surpassing previous methods by
1.13% on accuracy. Ablation studies further verify that the wavelet-driven
fusion module significantly improves the extraction of semantic information
from non-verbal sources, with a 0.41% increase in recognition accuracy when
analyzing subtle emotional cues.

</details>


### [44] [Thief of Truth: VR comics about the relationship between AI and humans](https://arxiv.org/abs/2506.10012)
*Joonhyung Bae*

Main category: cs.MM

TL;DR: VR漫画《Thief of Truth》探讨人类与AI的关系，实验了VR技术在漫画表现力、交互体验和可访问性上的应用。


<details>
  <summary>Details</summary>
Motivation: 探究人类与人工智能的关系，并通过VR技术实验漫画的可扩展性。

Method: 1. 利用VR的视角控制效果设计漫画；2. 通过VR控制器提升玩家沉浸感；3. 设计提升VR漫画可访问性的方法。

Result: 提出了一种VR漫画的实验性尝试范例。

Conclusion: 该作品展示了VR漫画在技术和叙事上的创新潜力。

Abstract: Thief of Truth is a first-person perspective Virtual Reality (VR) comic that
explores the relationship between humans and artificial intelligence (AI). The
work tells the story of a mind-uploaded human being reborn as a new subject
while interacting with an AI that is looking for the meaning of life. In order
to experiment with the expandability of VR comics, the work was produced by
focusing on three problems. First, the comic is designed using the viewing
control effect of VR. Second, through VR controller-based interaction, the
player's immersion in the work is increased. Third, a method for increasing
accessibility to VR comics was devised. This work aims to present an example of
an experimental attempt in VR Comics.

</details>


### [45] [Immersive Fantasy Based on Digital Nostalgia: Environmental Narratives for the Korean Millennials and Gen Z](https://arxiv.org/abs/2506.10013)
*Yerin Doh,Joonhyung Bae*

Main category: cs.MM

TL;DR: 《Dear Passenger, Please Wear a Mask》通过数字怀旧与航空旅行的幻想叙事，探讨了疫情期间口罩浪费的生态问题，结合点击游戏和沉浸式展览，引发参与者的伦理与环境思考。


<details>
  <summary>Details</summary>
Motivation: 研究旨在引发社会对疫情期间一次性口罩浪费问题的关注，并通过艺术品的形式唤起年轻世代（如千禧一代和Z世代）对生态问题的共情。

Method: 采用数字怀旧与航空旅行叙事的结合，设计了一款点击游戏和沉浸式展览，让参与者在虚拟与现实之间穿梭，体验环境与伦理困境。

Result: 作品成功激发了参与者的共情和潜在行动意愿，但也面临着资源使用和后续参与度不足的挑战。

Conclusion: 虽然作品有效唤起了生态意识，但如何在减少资源消耗的同时维持参与者的长期参与，仍需进一步探索。

Abstract: This study introduces the media artwork Dear Passenger, Please Wear a Mask,
designed to offer a layered exploration of single-use mask waste, which
escalated during the COVID-19 pandemic. The piece reframes underappreciated
ecological concerns by interweaving digital nostalgia and airline travel
recollections of Millennials and Gen Z with a unique fantasy narrative. Via a
point-and-click game and an immersive exhibition, participants traverse both
virtual and real domains, facing ethical and environmental dilemmas. While it
fosters empathy and potential action, resource use and post-experience
engagement challenges persist.

</details>


### [46] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2506.10016)
*Longzhen Han,Awes Mubarak,Almas Baimagambetov,Nikolaos Polatidis,Thar Baker*

Main category: cs.MM

TL;DR: 多模态大语言模型（MLLMs）通过统一架构整合语言与其他感官模态，涵盖图像、音乐、视频等多种输出形式。本文综述了六种主要生成模态，并探讨了自监督学习、专家混合等技术如何实现跨模态能力。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型如何整合不同输出模态并推动跨模态能力的进展。

Method: 通过分析自监督学习、专家混合、强化学习反馈和思维链提示等核心技术，研究跨模态能力的实现方式。

Result: 总结了关键模型、架构趋势和跨模态协同效应，同时指出可转移技术和未解决的问题。

Conclusion: 本文提供了多模态大语言模型发展的统一视角，并指出了实现更通用、自适应和可解释系统的关键路径。

Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text
generation, now spanning diverse output modalities including images, music,
video, human motion, and 3D objects, by integrating language with other sensory
modalities under unified architectures. This survey categorises six primary
generative modalities and examines how foundational techniques, namely
Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement
Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,
enable cross-modal capabilities. We analyze key models, architectural trends,
and emergent cross-modal synergies, while highlighting transferable techniques
and unresolved challenges. Architectural innovations like transformers and
diffusion models underpin this convergence, enabling cross-modal transfer and
modular specialization. We highlight emerging patterns of synergy, and identify
open challenges in evaluation, modularity, and structured reasoning. This
survey offers a unified perspective on MLLM development and identifies critical
paths toward more general-purpose, adaptive, and interpretable multimodal
systems.

</details>


### [47] [Can Sound Replace Vision in LLaVA With Token Substitution?](https://arxiv.org/abs/2506.10416)
*Ali Vosoughi,Jing Bi,Pinxin Liu,Yunlong Tang,Chenliang Xu*

Main category: cs.MM

TL;DR: SoundCLIP研究直接集成音频和视觉输入的多模态大语言模型，探索音频嵌入并揭示检索与生成任务的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 传统多模态系统依赖文本对齐表示，限制了音频信息的利用，因此需要直接集成音频和视觉输入以提升性能。

Method: 采用两种配置：将音频特征投影到CLIP视觉空间或保留原始音频嵌入，并评估五种音频编码器的表现。

Result: 音频投影显著提高检索性能，但降低文本生成质量；语言预训练的编码器在生成任务中表现更好。

Conclusion: 跨模态对齐的强度并非对所有任务都有益，需平衡检索准确性和生成质量以实现最优性能。

Abstract: While multimodal systems have achieved impressive advances, they typically
rely on text-aligned representations rather than directly integrating audio and
visual inputs. This reliance can limit the use of acoustic information in tasks
requiring nuanced audio understanding. In response, SoundCLIP explores direct
audio-visual integration within multimodal large language models (MLLMs) by
substituting CLIP's visual tokens with audio representations and selecting
sound-relevant patch tokens in models such as LLaVA. We investigate two
configurations: (1) projecting audio features into CLIP's visual manifold via a
multilayer perceptron trained with InfoNCE on paired audio-video segments, and
(2) preserving raw audio embeddings with minimal dimensional adjustments.
Experiments with five state-of-the-art audio encoders reveal a fundamental
trade-off. While audio-to-video retrieval performance increases dramatically
(up to 44 percentage points in Top-1 accuracy) when audio is projected into
CLIP's space, text generation quality declines. Encoders pre-trained with text
supervision (CLAP, Whisper, ImageBind) maintain stronger generative
capabilities than those focused primarily on audiovisual alignment (Wav2CLIP,
AudioCLIP), highlighting the value of language exposure for generation tasks.
We introduce WhisperCLIP, an architecture that fuses intermediate
representations from Whisper, as well as AudioVisual Event Evaluation (AVE-2),
a dataset of 580,147 three-second audiovisual clips with fine-grained alignment
annotations. Our findings challenge the assumption that stronger cross-modal
alignment necessarily benefits all multimodal tasks; instead, a Pareto frontier
emerges wherein optimal performance depends on balancing retrieval accuracy
with text generation quality. Codes and datasets:
https://github.com/ali-vosoughi/SoundCLIP.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [48] [Growing a Modular Framework for Modal Systems- HOLMS: a HOL Light Library](https://arxiv.org/abs/2506.10048)
*Antonella Bilotta*

Main category: cs.LO

TL;DR: 该论文介绍了HOLMS（HOL Light的模态系统库），一个在HOL Light证明助手中用于模态推理的模块化框架，重点研究了模态逻辑的统一证明策略。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个模块化和统一的模态推理框架，并验证模态系统的完备性定理，同时扩展了之前对Gödel-Löb逻辑的研究。

Method: 通过在HOL Light中实现统一的模块化策略，证明了多个正常模态系统（如K、T、K4和GL）的完备性定理，并结合了自动化决策过程和反模型构造器。

Result: 成功展示了在HOL Light中机械化模态推理的可行性，并提供了灵活且稳健的实现方法。

Conclusion: HOLMS框架为模态推理的进一步研究提供了基础，展示了通用证明助手与模态逻辑研究结合的潜力。

Abstract: The present dissertation introduces the research project on HOLMS
(\textbf{HOL} Light Library for \textbf{M}odal \textbf{S}ystems), a growing
modular framework for modal reasoning within the HOL Light proof assistant. To
provide an accessible introduction to the library, the fundamentals of modal
logic are outlined first, followed by a concise manual for the proof assistant
itself. The core contribution of this work on HOLMS is the development of a
unified and modular strategy for proving adequacy theorems with respect to
relational semantics directly within HOL Light for several normal modal
systems, currently including K, T, K4, and GL. Adequacy theorems establish a
formal connection between syntactic proof systems and their intended relational
models, ensuring that derivable statements align with valid ones. This approach
extends previous research on G\"odel-L\"ob logic (GL) by two HOLMS developers.
It also assesses the generality and compositionality of the completeness proofs
in George Boolos' monograph \textit{The logic of provability}. Beyond
theoretical contributions, HOLMS incorporates automated decision procedures and
a countermodel constructor for K, T, K4, and GL, illustrating how
general-purpose proof assistants can be effectively combined with research on
labelled sequent calculi and key insights from correspondence and bisimulation
theories. The implementation in HOL Light demonstrates the feasibility of
mechanising modal reasoning in a flexible and robust manner, paving the way for
further developments of the HOLMS framework.

</details>


### [49] [Notes on applicative matching logic](https://arxiv.org/abs/2506.10088)
*Laurentiu Leustean*

Main category: cs.LO

TL;DR: 这篇论文介绍了应用匹配逻辑（AML），这是匹配逻辑（ML）的一个功能变体，用于编程语言的语义定义和程序行为规范。


<details>
  <summary>Details</summary>
Motivation: 开发AML的目的是为了提供一个更功能化的工具，用于编程语言的语义定义和程序行为的规范与推理。

Method: 论文通过基本定义和结果来介绍AML，并参考了Monk的数学逻辑教材。

Result: AML被提出并展示为一种有效的工具，可用于编程语言语义和程序行为的理论研究。

Conclusion: AML是匹配逻辑的一种功能变体，适用于编程语言语义的理论研究，可作为AML理论的入门材料。

Abstract: Matching logic (ML) was developed by Grigore Ro\c{s}u and collaborators as a
logic for defining the formal semantics of programming languages and for
specifying and reasoning about the behavior of programs. These lecture notes
present basic definitions and results on applicative matching logic (AML), a
functional variant of ML introduced recently by Xiaohong Chen and Grigore
Ro\c{s}u. They can be used as an introductory text in the theory of AML. Monk's
textbook on mathematical logic has an enormous influence on the notes.

</details>


### [50] [StepProof: Step-by-step verification of natural language mathematical proofs](https://arxiv.org/abs/2506.10558)
*Xiaolin Hu,Qinghua Zhou,Bogdan Grechuk,Ivan Y. Tyukin*

Main category: cs.LO

TL;DR: StepProof提出了一种新颖的自动形式化方法，能够实现句子级的逐步验证，显著提高了证明成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法仅能验证完整证明，缺乏句子级验证能力，限制了其应用范围。

Method: StepProof将完整证明分解为多个可验证的子证明，支持逐步验证。

Result: 实验证明，StepProof比传统方法更高效且成功率更高，通过微调自然语言证明可进一步提升性能。

Conclusion: StepProof填补了自动形式化在句子级验证方面的空白，为形式化验证工具提供了更灵活的支持。

Abstract: Interactive theorem provers (ITPs) are powerful tools for the formal
verification of mathematical proofs down to the axiom level. However, their
lack of a natural language interface remains a significant limitation. Recent
advancements in large language models (LLMs) have enhanced the understanding of
natural language inputs, paving the way for autoformalization - the process of
translating natural language proofs into formal proofs that can be verified.
Despite these advancements, existing autoformalization approaches are limited
to verifying complete proofs and lack the capability for finer, sentence-level
verification. To address this gap, we propose StepProof, a novel
autoformalization method designed for granular, step-by-step verification.
StepProof breaks down complete proofs into multiple verifiable subproofs,
enabling sentence-level verification. Experimental results demonstrate that
StepProof significantly improves proof success rates and efficiency compared to
traditional methods. Additionally, we found that minor manual adjustments to
the natural language proofs, tailoring them for step-level verification,
further enhanced StepProof's performance in autoformalization.

</details>


### [51] [Encoding call-by-push-value in the pi-calculus](https://arxiv.org/abs/2506.10584)
*Benjamin Bennetzen,Nikolaj Rossander Kristensen,Peter Buus Steffensen*

Main category: cs.LO

TL;DR: 提出了一种将Levy的CBPV lambda演算编码到pi演算中的方法，证明了其完整性和正确性，并讨论了编码的优良性质，初步实现了Coq形式化验证。


<details>
  <summary>Details</summary>
Motivation: 解决在pi演算中使用de Bruijn索引形式化的挑战，同时验证编码的合理性。

Method: 采用内部pi演算（pi-i-calculus）进行编码，解决bisimulation问题，并通过手写证明和部分Coq形式化验证。

Result: 编码满足Gorla提出的五种优秀编码标准，且在pi-i-calculus中bisimulation性质良好。

Conclusion: 编码方法有效且合理，未来的工作可以完善Coq形式化证明。

Abstract: In this report we define an encoding of Levys call-by-push-value
lambda-calculus (CBPV) in the pi-calculus, and prove that our encoding is both
sound and complete. We present informal (by-hand) proofs of soundness,
completeness, and all required lemmas. The encoding is specialized to the
internal pi-calculus (pi-i-calculus) to circumvent certain challenges
associated with using de Bruijn index in a formalization, and it also helps
with bisimulation as early-, late- and open-bisimulation coincide in this
setting, furthermore bisimulation is a congruence. Additionally, we argue that
our encoding also satisfies the five criteria for good encodings proposed by
Gorla, as well as show similarities between Milners and our encoding. This
paper includes encodings from CBPV in the pi-i-calculus, asynchronous polyadic
pi-calculus and the local pi-calculus. We begin a formalization of the proof in
Coq for the soundness and completeness of the encoding in the pi-i-calculus.
Not all lemmas used in the formalization are themselves formally proven.
However, we argue that the non-proven lemmas are reasonable, as they are proven
by hand, or amount to Coq formalities that are straightforward given informal
arguments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [52] [Cybernetic Marionette: Channeling Collective Agency Through a Wearable Robot in a Live Dancer-Robot Duet](https://arxiv.org/abs/2506.10079)
*Anup Sathya,Jiasheng Li,Zeyu Yan,Adriane Fang,Bill Kules,Jonathan David Martin,Huaishu Peng*

Main category: cs.HC

TL;DR: DANCE^2是一个互动舞蹈表演，观众通过投票控制舞者身上的可穿戴机器人行为，虽调查显示观众感到其选择有影响，但投票数据展示了高度一致性，揭示了行为、体验与权力的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 探索观众在互动表演中的集体代理感与实际行为之间的关系，以及技术如何影响这种体验。

Method: 通过实时投票让观众干预舞者与机器人的互动，结合调查和数据分析评估效果。

Result: 观众主观认为其选择有意义，但投票数据趋同，显示代理感与实际行为存在差异。

Conclusion: 表演设计揭示了代理感与真实权力的分离，为算法系统中的类似现象提供了现场类比。

Abstract: We describe DANCE^2, an interactive dance performance in which audience
members channel their collective agency into a dancer-robot duet by voting on
the behavior of a wearable robot affixed to the dancer's body. At key moments
during the performance, the audience is invited to either continue the
choreography or override it, shaping the unfolding interaction through
real-time collective input. While post-performance surveys revealed that
participants felt their choices meaningfully influenced the performance, voting
data across four public performances exhibited strikingly consistent patterns.
This tension between what audience members do, what they feel, and what
actually changes highlights a complex interplay between agentive behavior, the
experience of agency, and power. We reflect on how choreography, interaction
design, and the structure of the performance mediate this relationship,
offering a live analogy for algorithmically curated digital systems where
agency is felt, but not exercised.

</details>


### [53] [Mastery Learning Improves Performance on Complex Tasks on PCP Literacy Test](https://arxiv.org/abs/2506.10164)
*Chandana Srinivas,Elif E. Firat,Robert S. Laramee,Alark Joshi*

Main category: cs.HC

TL;DR: 研究通过修订版布鲁姆分类法和掌握学习法教授平行坐标图（PCPs）的读写能力，发现掌握学习组在高级思维模块（分析、评估）表现更佳。


<details>
  <summary>Details</summary>
Motivation: 学生在学习不熟悉的数据可视化技术（如PCPs）时面临挑战，研究旨在探索掌握学习法的教学效果。

Method: 采用修订版布鲁姆分类法和掌握学习法进行干预实验，比较两组学生在不同思维模块的表现。

Result: 掌握学习组在高级思维模块表现更好，且对PCPs的理解更深入。

Conclusion: 掌握学习法有助于提升学生对PCPs的高级认知能力，提供了可复现的教学材料。

Abstract: Developing literacy with unfamiliar data visualization techniques such as
Parallel Coordinate Plots (PCPs) can be a significant challenge for students.
We adopted the Revised Bloom's taxonomy to instruct students on Parallel
Coordinate Plots (PCPs) using Mastery Learning in the classroom. To evaluate
Mastery Learning's impact, we conducted an intervention in a Data Visualization
course to teach students about PCPs using the Revised Bloom's taxonomy with and
without Mastery Learning. Based on our intervention, we found that while
students in both groups performed similarly on the first two (Remember,
Understand) modules, the students in the Mastery Learning group performed
better on modules that required more advanced thinking (Analyze, Evaluate) and
demonstrated a better comprehension of PCPs. We provide all the materials
developed including the six-module Bloom's Taxonomy PCP literacy (BTPL) test
for full reproducibility on our website at
https://vis-graphics.github.io/PCP-Literacy-Test/.

</details>


### [54] [Intergenerational AI Literacy in Korean Immigrant Families: Interpretive Gatekeeping Meets Convenient Critical Deferment](https://arxiv.org/abs/2506.10197)
*Jeongone Seo,Ryan Womack,Tawfiq Ammari*

Main category: cs.HC

TL;DR: 这篇研究探讨了韩国移民家庭在美国如何使用AI工具（如ChatGPT和智能助手），揭示了父母通过文化和伦理价值观调解子女AI使用的“解释性把关”，以及青少年为学术和社会效用而“便利性批判性延迟”的关键实践。


<details>
  <summary>Details</summary>
Motivation: AI在家庭生活中的深度整合需要研究移民家庭如何应对代际、语言和文化的独特挑战。

Method: 通过20次半结构化访谈，采访了韩国移民家庭的父母和青少年。

Result: 发现了两类关键实践：“解释性把关”和“便利性批判性延迟”，表明AI素养是动态且关系性的，通过家庭协商共同构建。

Conclusion: 研究为信息科学和人机交互提供了新的概念扩展，并提出了更具包容性、文化敏感性和以家庭为中心的AI系统设计建议。

Abstract: As artificial intelligence (AI) becomes deeply integrated into family life,
immigrant families must navigate unique intergenerational, linguistic, and
cultural challenges. This study examines how Korean immigrant families in the
United States negotiate the use of AI tools such as ChatGPT and smart
assistants in their homes. Through 20 semi-structured interviews with parents
and teens, we identify two key practices that shape their engagement:
interpretive gatekeeping, where parents mediate their children's AI use through
a lens of cultural and ethical values, and convenient critical deferment, where
teens strategically postpone critical evaluation of AI for immediate academic
and social utility. These intertwined practices challenge conventional,
skills-based models of AI literacy, revealing it instead as a dynamic and
relational practice co-constructed through ongoing family negotiation. We
contribute to information science and HCI by offering a new conceptual
extension for intergenerational AI literacy and providing design implications
for more equitable, culturally attuned, and family-centered AI systems.

</details>


### [55] [Speculative Design in Spiraling Time: Methods and Indigenous HCI](https://arxiv.org/abs/2506.10229)
*James Eschrich,Cole McMullen,Sarah Sterman*

Main category: cs.HC

TL;DR: 该立场论文探讨了推测性设计在土著人机交互中的适用性，提出了关于时间性的关键假设的潜在问题，并提出了基于‘螺旋时间’概念的替代理解。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索推测性设计在土著人机交互中的有效性，并识别其可能存在的问题。

Method: 通过分析推测性设计的假设，特别是时间性假设，来评估其在土著HCI中的适用性。

Result: 研究表明，当前的推测性设计在土著HCI中存在局限性，并提出了一种基于‘螺旋时间’的替代方法。

Conclusion: 结论是，采用‘螺旋时间’概念的推测性设计可能更适合土著人机交互的应用场景。

Abstract: In this position paper, we first discuss the uptake of speculative design as
a method for Indigenous HCI. Then, we outline how a key assumption about
temporality threatens to undermine the usefulness of speculative design in this
context. Finally, we briefly sketch out a possible alternative understanding of
speculative design, based on the concept of "spiraling time," which could be
better suited for Indigenous HCI.

</details>


### [56] [Extended Creativity: A Conceptual Framework for Understanding Human-AI Creative Relations](https://arxiv.org/abs/2506.10249)
*Andrea Gaggioli,Sabrina Bartolotta,Andrea Ubaldi,Katusha Gerardini,Eleonora Diletta Sarcinella,Alice Chirico*

Main category: cs.HC

TL;DR: AI通过支持、协同和共生三种模式增强人类创造力，技术自主性和感知代理程度是关键维度，影响从日常问题解决到颠覆性创新的创造力层面。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何有效增强人类创造力，通过分布式创造力的视角明确其实现路径。

Method: 定义了AI在创意过程中的三种模式（支持、协同、共生），并基于技术自主性和感知代理程度两个维度进行分析。

Result: 不同配置对创造力的影响不同，涉及日常问题解决到重大创新，同时揭示了理论、伦理和设计层面的意义。

Conclusion: AI与人类创造力的结合模式多样，其成功实现需综合考虑技术、感知和多维度影响。

Abstract: Artificial Intelligence holds significant potential to enhance human
creativity. However, achieving this vision requires a clearer understanding of
how such enhancement can be effectively realized. Adopting the perspective of
distributed creativity, we identify three primary modes through which AI can
contribute to creative processes: Support, where AI acts as a tool; Synergy,
where AI and humans collaborate in complementary ways; and Symbiosis, where
human and AI cognition become so integrated that they form a unified creative
system. These modes are defined along two key dimensions: the level of
technical autonomy exhibited by the AI system and the degree of perceived
agency attributed to it. We examine how each configuration influences different
levels of creativity - from everyday problem-solving to paradigm-shifting
innovation - and discuss the theoretical, ethical, and design implications.

</details>


### [57] [Beyond Compliance: A User-Autonomy Framework for Inclusive and Customizable Web Accessibility](https://arxiv.org/abs/2506.10324)
*Lalitha A R*

Main category: cs.HC

TL;DR: 提出从合规性驱动转向以关怀为中心的网络可访问性模型，强调用户自主权，为神经多样性用户提供个性化需求，并引入可自定义的舒适模式框架。


<details>
  <summary>Details</summary>
Motivation: 现有的可访问性标准常被视为静态合规清单，忽略了用户的个性化需求，尤其是神经多样性用户的特殊需求。

Method: 设计了一个灵活的舒适模式框架，允许用户根据个人需求调整界面设置（如对比度、字体、动画等），同时保留品牌视觉一致性。

Result: 提出了最小和高级实现模型，展示了如何以低成本无缝集成包容性设计，支持用户自主权而不牺牲创意自由。

Conclusion: 这一方法为数字包容性提供了新范例，通过用户选择而非妥协，实现了用户自主权、美学和可访问性的统一。

Abstract: This paper proposes a shift from compliance-centered web accessibility to a
care-driven model that prioritizes user autonomy, using neurodivergent users as
a catalyst case for broader personalization needs. While accessibility
standards offer a flexible framework, they are often interpreted and
implemented as static compliance checklists, our approach reframes it as a
flexible, user-centered process. We introduce a customizable Comfort Mode
framework that allows users to adapt interface settings, such as contrast,
typography, motion, and scaling, according to their individual needs, while
retaining the brand's core visual identity. Grounded in psychological and
cognitive accessibility principles, our design supports personalization without
sacrificing creative freedom. We present both minimal and advanced
implementation models with mock-ups, demonstrating how inclusive design can be
seamlessly integrated at minimal cost. This approach aims to broaden digital
inclusivity by offering autonomy to those who require it, without imposing
changes on those who do not. The proposed system is adaptable, scalable, and
suitable for a wide range of users and brands, offering a new paradigm where
user autonomy, aesthetic integrity, and accessibility converge not through
compromise, but through choice.

</details>


### [58] [IDEA: Augmenting Design Intelligence through Design Space Exploration](https://arxiv.org/abs/2506.10587)
*Chuer Chen,Xiaoke Yan,Xiaoyu Qi,Nan Cao*

Main category: cs.HC

TL;DR: 该论文提出了一种结构化表示方法，用于建模设计空间，并引入IDEA框架，结合LLM和MCTS算法，提升设计智能的决策能力。


<details>
  <summary>Details</summary>
Motivation: 设计空间缺乏数学形式化，限制了自动化设计过程的支持，现有决策仍依赖设计师经验。

Method: 提出了结构化表示方法，结合LLM生成约束，MCTS算法探索空间，并将抽象决策实例化为领域特定实现。

Result: 在文章撰写和可视化生成场景中验证，IDEA展现跨领域适应性和高效设计成果。

Conclusion: IDEA框架通过数学建模和智能算法，显著提升了设计空间探索的效率和成果质量。

Abstract: Design spaces serve as a conceptual framework that enables designers to
explore feasible solutions through the selection and combination of design
elements. However, effective decision-making remains heavily dependent on the
designer's experience, and the absence of mathematical formalization prevents
computational support for automated design processes. To bridge this gap, we
introduce a structured representation that models design spaces with orthogonal
dimensions and discrete selectable elements. Building on this model, we present
IDEA, a decision-making framework for augmenting design intelligence through
design space exploration to generate effective outcomes. Specifically, IDEA
leverages large language models (LLMs) for constraint generation, incorporates
a Monte Carlo Tree Search (MCTS) algorithm guided by these constraints to
explore the design space efficiently, and instantiates abstract decisions into
domain-specific implementations. We validate IDEA in two design scenarios:
data-driven article composition and pictorial visualization generation,
supported by example results, expert interviews, and a user study. The
evaluation demonstrates the IDEA's adaptability across domains and its
capability to produce superior design outcomes.

</details>


### [59] [Accessible Design in Integrated Development Environments: A Think Aloud Study Exploring the Experiences of Students with ADHD](https://arxiv.org/abs/2506.10598)
*Luke Halpin,Phillip Benachour,Tracy Hall,Ann-Marie Houghton,Emily Winter*

Main category: cs.HC

TL;DR: 研究探讨了集成开发环境（IDE）对ADHD学生的影响，发现当前设计可能导致挫败感和学习障碍，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 探究IDE界面设计对ADHD学生在计算机科学教育中的影响，现有研究缺乏相关数据。

Method: 采用有声思维研究和定性观察访谈，分析9名ADHD学生在Visual Studio Code中的学习与互动。

Result: 识别出三个主题（自信心、交互、学习）及其子主题，揭示了当前IDE设计中的不足。

Conclusion: 需改进IDE的可访问性和可用性设计，以提升ADHD学生的学习体验。

Abstract: Coding forms a key part of computer science education in universities. As
part of this education, Integrated Development Environments (IDEs) are
essential tools for coding. However, it is currently unknown how the design of
an IDE's interface impacts on students with Attention Deficit Hyperactivity
Disorder (ADHD).
  In this study we investigated the use of IDEs by students with ADHD. We
conducted a think aloud study with nine university computing students, followed
by qualitative observational interviews to analyse their learning and
engagement with the Visual Studio Code IDE. The paper reports on these
experiences and seeks to understand the role IDEs play in the educational
setting.
  Our work also examines how digital accessibility and usability are considered
in the current design of IDEs. We analysed the qualitative data using a
thematic analysis and identified three primary themes: self-confidence,
interaction, and learning as well as various sub-themes.
  The themes and their sub-themes illustrate key areas of consideration when
designing IDEs for students with ADHD. The primary findings highlight
experiences of frustration and barriers in the current design and layout of
IDEs.
  Through our participatory approach we provide a rare insight into ADHD user
experiences around usability and accessibility, and describe the need for
better design of development environments to ensure a positive learning
experience for the students.

</details>


### [60] [Integrating Large Language Models into Text Animation: An Intelligent Editing System with Inline and Chat Interaction](https://arxiv.org/abs/2506.10762)
*Bao Zhang,Zihan Li,Zhenglei Liu,Huanchen Wang,Yuxin Ma*

Main category: cs.HC

TL;DR: 提出了一种基于大型语言模型（LLM）的文本动画编辑系统，旨在为非专业人士降低动画制作的复杂性，并通过实时意图跟踪和灵活编辑提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统动画工作流对非专业人士操作复杂，阻碍创作效率。

Method: 采用基于代理的双流管道，结合上下文感知的内联建议和对话引导，并通过语义-动画映射实现LLM驱动的意图翻译。系统还支持同步文本-动画预览和统一控制参数调整。

Result: 用户研究表明，系统能帮助非专业人士完成动画工作流，验证了管道的有效性。

Conclusion: 结果表明，将LLM整合到视频创作流程中具有潜力，值得进一步探索。

Abstract: Text animation, a foundational element in video creation, enables efficient
and cost-effective communication, thriving in advertisements, journalism, and
social media. However, traditional animation workflows present significant
usability barriers for non-professionals, with intricate operational procedures
severely hindering creative productivity. To address this, we propose a Large
Language Model (LLM)-aided text animation editing system that enables real-time
intent tracking and flexible editing. The system introduces an agent-based
dual-stream pipeline that integrates context-aware inline suggestions and
conversational guidance as well as employs a semantic-animation mapping to
facilitate LLM-driven creative intent translation. Besides, the system supports
synchronized text-animation previews and parametric adjustments via unified
controls to improve editing workflow. A user study evaluates the system,
highlighting its ability to help non-professional users complete animation
workflows while validating the pipeline. The findings encourage further
exploration of integrating LLMs into a comprehensive video creation workflow.

</details>


### [61] [Grasp Prediction based on Local Finger Motion Dynamics](https://arxiv.org/abs/2506.10818)
*Dimitar Valkov,Pascal Kockwelp,Florian Daiber,Antonio Krüger*

Main category: cs.HC

TL;DR: 本研究探索了基于手部运动学实时识别无仪器目标物体的可行性，通过实验验证了一种简单LSTM网络的高精度预测能力。


<details>
  <summary>Details</summary>
Motivation: 预测用户意图抓取的物体可提供关键上下文信息，有助于缓解交互环境中的点对点延迟问题。

Method: 记录16名参与者在抓取真实和合成物体时的手部运动数据，并使用LSTM网络进行分析。

Result: LSTM网络能高精度预测抓取时间点（误差<21ms）、距离（误差<1cm）及目标大小（准确率>97%）。

Conclusion: 研究结果为无处不在和混合现实环境中设计自适应精细交互界面提供了重要依据。

Abstract: The ability to predict the object the user intends to grasp offers essential
contextual information and may help to leverage the effects of point-to-point
latency in interactive environments. This paper explores the feasibility and
accuracy of real-time recognition of uninstrumented objects based on hand
kinematics during reach-to-grasp actions. In a data collection study, we
recorded the hand motions of 16 participants while reaching out to grasp and
then moving real and synthetic objects. Our results demonstrate that even a
simple LSTM network can predict the time point at which the user grasps an
object with a precision better than 21 ms and the current distance to this
object with a precision better than 1 cm. The target's size can be determined
in advance with an accuracy better than 97%. Our results have implications for
designing adaptive and fine-grained interactive user interfaces in ubiquitous
and mixed-reality environments.

</details>


### [62] [(De)composing Craft: An Elementary Grammar for Sharing Expertise in Craft Workflows](https://arxiv.org/abs/2506.10891)
*Ritik Batra,Lydia Kim,Ilan Mandel,Amritansh Kwatra,Jane L. E.,Steven J. Jackson,Thijs Roumen*

Main category: cs.HC

TL;DR: 论文提出了一种记录手工艺中即兴行为的基础语法，并通过CraftLink界面验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 手工艺知识的传统记录方式过于线性，忽略了即兴与适应性的隐性知识，限制了知识的共享。

Method: 结合专家访谈与多学科文献，开发了一种记录手工艺即兴行为的基础语法，并通过CraftLink界面分析专家视频，半自动生成文档。

Result: 用户研究（N=7）表明，该语法能有效捕捉并分享专家知识，为计算系统支持社区知识共享提供了新途径。

Conclusion: 研究为手工艺知识的协作式记录与共享提供了新的工具和方法，强调了隐性知识的重要性。

Abstract: Craft practices rely on evolving archives of skill and knowledge, developed
through generations of craftspeople experimenting with designs, materials, and
techniques. Better documentation of these practices enables the sharing of
knowledge and expertise between sites and generations. However, most
documentation focuses solely on the linear steps leading to final artifacts,
neglecting the tacit knowledge necessary to improvise, or adapt workflows to
meet the unique demands of each craft project. This omission limits knowledge
sharing and reduces craft to a mechanical endeavor, rather than a sophisticated
way of seeing, thinking, and doing. Drawing on expert interviews and literature
from HCI, CSCW and the social sciences, we develop an elementary grammar to
document improvisational actions of real-world craft practices. We demonstrate
the utility of this grammar with an interface called CraftLink that can be used
to analyze expert videos and semi-automatically generate documentation to
convey material and contextual variations of craft practices. Our user study
with expert crocheters (N=7) using this interface evaluates our grammar's
effectiveness in capturing and sharing expert knowledge with other
craftspeople, offering new pathways for computational systems to support
collaborative archives of knowledge and practice within communities.

</details>


### [63] [The Role of Generative AI in Facilitating Social Interactions: A Scoping Review](https://arxiv.org/abs/2506.10927)
*T. T. J. E. Arets,G. Perugia,M. Houben,W. A. IJsselsteijn*

Main category: cs.HC

TL;DR: 综述探讨生成式AI如何设计以促进社交互动，分析其应用领域与方法，强调参与式设计和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 社交连接减少威胁心理健康，生成式AI在社交互动中的影响尚不明确，需系统研究。

Method: 分析2020年以来30项研究，总结应用领域、设计方法及评估策略。

Result: 识别故事讲述、情感训练等应用趋势，发现参与式设计有效，但存在文化偏见等问题。

Conclusion: 生成式AI有望支持个性化互动，但需关注公平设计和包容性评估。

Abstract: Reduced social connectedness increasingly poses a threat to mental health,
life expectancy, and general well-being. Generative AI (GAI) technologies, such
as large language models (LLMs) and image generation tools, are increasingly
integrated into applications aimed at enhancing human social experiences.
Despite their growing presence, little is known about how these technologies
influence social interactions. This scoping review investigates how GAI-based
applications are currently designed to facilitate social interaction, what
forms of social engagement they target, and which design and evaluation
methodologies designers use to create and evaluate them. Through an analysis of
30 studies published since 2020, we identify key trends in application domains
including storytelling, socio-emotional skills training, reminiscence,
collaborative learning, music making, and general conversation. We highlight
the role of participatory and co-design approaches in fostering both effective
technology use and social engagement, while also examining socio-ethical
concerns such as cultural bias and accessibility. This review underscores the
potential of GAI to support dynamic and personalized interactions, but calls
for greater attention to equitable design practices and inclusive evaluation
strategies.

</details>


### [64] [Video-Mediated Emotion Disclosure: A Study of Mental Health Vlogging by People with Schizophrenia on YouTube](https://arxiv.org/abs/2506.10932)
*Jiaying Lizzy Liu,Yan Zhang*

Main category: cs.HC

TL;DR: 该研究填补了关于精神分裂症患者如何在视频博客中构建情绪叙述的研究空白，分析了200个YouTube视频，揭示了情绪表达的多模式动态互动。


<details>
  <summary>Details</summary>
Motivation: 以往研究多集中于文本表达，忽略了视频博客中情绪叙述的构建，因此研究者希望通过视觉分析框架探索这一问题。

Method: 研究通过媒体研究和自我呈现理论开发视觉分析框架，分析了200个精神分裂症患者制作的YouTube视频。

Result: 研究发现，视觉元素的刻意构建（如环境和美学选择）能促进更支持性的观众回应，突显了情绪表达的多样性。

Conclusion: 研究呼吁未来开展大规模定量研究，探讨视觉特征如何影响社交媒体上的视频传播，以开发更支持患者的视频分享平台。

Abstract: Individuals with schizophrenia frequently experience intense emotions and
often turn to vlogging as a medium for emotional expression. While previous
research has predominantly focused on text based disclosure, little is known
about how individuals construct narratives around emotions and emotional
experiences in video blogs. Our study addresses this gap by analyzing 200
YouTube videos created by individuals with schizophrenia. Drawing on media
research and self presentation theories, we developed a visual analysis
framework to disentangle these videos. Our analysis revealed diverse practices
of emotion disclosure through both verbal and visual channels, highlighting the
dynamic interplay between these modes of expression. We found that the
deliberate construction of visual elements, including environmental settings
and specific aesthetic choices, appears to foster more supportive and engaged
viewer responses. These findings underscore the need for future large scale
quantitative research examining how visual features shape video mediated
communication on social media platforms. Such investigations would inform the
development of care centered video sharing platforms that better support
individuals managing illness experiences.

</details>


### [65] [Instance-Based Transfer Learning with Similarity-Aware Subject Selection for Cross-Subject SSVEP-Based BCIs](https://arxiv.org/abs/2506.10933)
*Ziwen Wang,Yue Zhang,Zhiqiang Zhang,Sheng Quan Xie,Alexander Lanzon,William P. Heath,Zhenhong Li*

Main category: cs.HC

TL;DR: 提出了基于实例的任务相关成分分析（iTRCA）和基于主题选择的iTRCA（SS-iTRCA）两种迁移学习框架，通过结合源主体的共同特征和目标主体的独特特征，减少SSVEP-BCI对目标主体数据的需求。


<details>
  <summary>Details</summary>
Motivation: 解决SSVEP-BCI中个体差异导致的迁移学习效果不佳问题，减少目标主体的数据需求。

Method: 开发了iTRCA和SS-iTRCA框架，提取主体通用特征和主体特有特征，并通过相似度选择源主体。

Result: 在多个数据集上验证了iTRCA和SS-iTRCA框架的有效性。

Conclusion: 该方法为开发高性能SSVEP-BCI提供了一种减少数据需求的解决方案。

Abstract: Steady-state visual evoked potential (SSVEP)-based brain-computer interfaces
(BCIs) can achieve high recognition accuracy with sufficient training data.
Transfer learning presents a promising solution to alleviate data requirements
for the target subject by leveraging data from source subjects; however,
effectively addressing individual variability among both target and source
subjects remains a challenge. This paper proposes a novel transfer learning
framework, termed instance-based task-related component analysis (iTRCA), which
leverages knowledge from source subjects while considering their individual
contributions. iTRCA extracts two types of features: (1) the subject-general
feature, capturing shared information between source and target subjects in a
common latent space, and (2) the subject-specific feature, preserving the
unique characteristics of the target subject. To mitigate negative transfer, we
further design an enhanced framework, subject selection-based iTRCA (SS-iTRCA),
which integrates a similarity-based subject selection strategy to identify
appropriate source subjects for transfer based on their task-related components
(TRCs). Comparative evaluations on the Benchmark, BETA, and a self-collected
dataset demonstrate the effectiveness of the proposed iTRCA and SS-iTRCA
frameworks. This study provides a potential solution for developing
high-performance SSVEP-based BCIs with reduced target subject data.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [66] [Learning-based density-equalizing map](https://arxiv.org/abs/2506.10027)
*Yanwen Huang,Lok Ming Lui,Gary P. T. Choi*

Main category: cs.GR

TL;DR: 提出了一种基于深度学习的密度均衡映射框架（LDEM），解决了传统方法在精度、重叠问题和2D到3D扩展上的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统密度均衡映射方法存在精度有限、极端情况下产生重叠以及难以从2D扩展到3D等问题，需要一种更高效和通用的解决方案。

Method: 利用深度神经网络构建LDEM框架，设计了强制密度均匀和几何规律性的损失函数，并采用分层方法预测粗粒度和密集级别的变换。

Result: LDEM在广泛分布的密度情况下表现出更优的密度均衡和双射特性，且能无缝扩展到3D领域。

Conclusion: 该框架为密度均衡映射的实用化提供了可扩展和鲁棒的解决方案。

Abstract: Density-equalizing map (DEM) serves as a powerful technique for creating
shape deformations with the area changes reflecting an underlying density
function. In recent decades, DEM has found widespread applications in fields
such as data visualization, geometry processing, and medical imaging.
Traditional approaches to DEM primarily rely on iterative numerical solvers for
diffusion equations or optimization-based methods that minimize handcrafted
energy functionals. However, these conventional techniques often face several
challenges: they may suffer from limited accuracy, produce overlapping
artifacts in extreme cases, and require substantial algorithmic redesign when
extended from 2D to 3D, due to the derivative-dependent nature of their energy
formulations. In this work, we propose a novel learning-based
density-equalizing mapping framework (LDEM) using deep neural networks.
Specifically, we introduce a loss function that enforces density uniformity and
geometric regularity, and utilize a hierarchical approach to predict the
transformations at both the coarse and dense levels. Our method demonstrates
superior density-equalizing and bijectivity properties compared to prior
methods for a wide range of simple and complex density distributions, and can
be easily applied to surface remeshing with different effects. Also, it
generalizes seamlessly from 2D to 3D domains without structural changes to the
model architecture or loss formulation. Altogether, our work opens up new
possibilities for scalable and robust computation of density-equalizing maps
for practical applications.

</details>


### [67] [FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training](https://arxiv.org/abs/2506.10035)
*Fuhan Cai,Yong Guo,Jie Li,Wenbo Li,Xiangzhong Fang,Jian Chen*

Main category: cs.GR

TL;DR: FastFLUX 是一个针对FLUX模型的架构级剪枝框架，通过BRLL方法和ST训练策略，显著提升了推理效率同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型（如FLUX）参数量大，推理速度慢且部署困难。现有加速方法往往导致性能下降和训练成本高。

Method: 提出了Block-wise Replacement with Linear Layers（BRLL）方法替换复杂残差分支，并结合Sandwich Training（ST）策略进行局部微调。

Result: 实验显示FastFLUX在剪枝20%的情况下仍能保持高质量的图像，同时显著提升了推理速度。

Conclusion: FastFLUX是一种高效的剪枝框架，解决了FLUX模型的推理和部署问题。

Abstract: Recent advancements in text-to-image (T2I) generation have led to the
emergence of highly expressive models such as diffusion transformers (DiTs),
exemplified by FLUX. However, their massive parameter sizes lead to slow
inference, high memory usage, and poor deployability. Existing acceleration
methods (e.g., single-step distillation and attention pruning) often suffer
from significant performance degradation and incur substantial training costs.
To address these limitations, we propose FastFLUX, an architecture-level
pruning framework designed to enhance the inference efficiency of FLUX. At its
core is the Block-wise Replacement with Linear Layers (BRLL) method, which
replaces structurally complex residual branches in ResBlocks with lightweight
linear layers while preserving the original shortcut connections for stability.
Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning
strategy that leverages LoRA to supervise neighboring blocks, mitigating
performance drops caused by structural replacement. Experiments show that our
FastFLUX maintains high image quality under both qualitative and quantitative
evaluations, while significantly improving inference speed, even with 20\% of
the hierarchy pruned. Our code will be available soon.

</details>


### [68] [Token Perturbation Guidance for Diffusion Models](https://arxiv.org/abs/2506.10036)
*Javad Rajabi,Soroush Mehraban,Seyedmorteza Sadat,Babak Taati*

Main category: cs.GR

TL;DR: 论文提出了一种名为Token Perturbation Guidance (TPG)的新方法，用于改进扩散模型的生成质量和输入条件对齐，无需额外训练且适用于条件和非条件生成。


<details>
  <summary>Details</summary>
Motivation: 现有的Classifier-free guidance (CFG)虽然有效，但需要特定训练且仅适用于条件生成。TPG旨在通过直接扰动中间标记表示来解决这些问题。

Method: TPG通过对扩散网络中间标记表示应用范数保持的扰动矩阵，提供稳定且有效的引导信号，无需改变模型结构。

Result: 实验表明，TPG在SDXL和Stable Diffusion 2.1上显著提升了非条件生成的FID，同时实现了与CFG相当的提示对齐效果。

Conclusion: TPG作为一种通用且条件无关的引导方法，能够为更广泛的扩散模型带来类似CFG的优势。

Abstract: Classifier-free guidance (CFG) has become an essential component of modern
diffusion models to enhance both generation quality and alignment with input
conditions. However, CFG requires specific training procedures and is limited
to conditional generation. To address these limitations, we propose Token
Perturbation Guidance (TPG), a novel method that applies perturbation matrices
directly to intermediate token representations within the diffusion network.
TPG employs a norm-preserving shuffling operation to provide effective and
stable guidance signals that improve generation quality without architectural
changes. As a result, TPG is training-free and agnostic to input conditions,
making it readily applicable to both conditional and unconditional generation.
We further analyze the guidance term provided by TPG and show that its effect
on sampling more closely resembles CFG compared to existing training-free
guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1
show that TPG achieves nearly a 2$\times$ improvement in FID for unconditional
generation over the SDXL baseline, while closely matching CFG in prompt
alignment. These results establish TPG as a general, condition-agnostic
guidance method that brings CFG-like benefits to a broader class of diffusion
models. The code is available at
https://github.com/TaatiTeam/Token-Perturbation-Guidance

</details>


### [69] [Ambient Diffusion Omni: Training Good Models with Bad Data](https://arxiv.org/abs/2506.10038)
*Giannis Daras,Adrian Rodriguez-Munoz,Adam Klivans,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.GR

TL;DR: 通过低质量、合成和非分布图像提升扩散模型质量的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖高质量数据集，但低质量图像中潜藏大量有用信息，被浪费。

Method: 提出Ambient Diffusion Omni框架，利用图像的光谱幂律衰减和局部性，从所有可用图像中提取信号。

Result: 在合成和真实数据上验证，实现最佳ImageNet FID，提升文本到图像生成的多样性和质量。

Conclusion: 噪声平衡了高质量目标分布与混合分布间的偏差，理论分析支持该方法的高效性。

Abstract: We show how to use low-quality, synthetic, and out-of-distribution images to
improve the quality of a diffusion model. Typically, diffusion models are
trained on curated datasets that emerge from highly filtered data pools from
the Web and other sources. We show that there is immense value in the
lower-quality images that are often discarded. We present Ambient Diffusion
Omni, a simple, principled framework to train diffusion models that can extract
signal from all available images during training. Our framework exploits two
properties of natural images -- spectral power law decay and locality. We first
validate our framework by successfully training diffusion models with images
synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We
then use our framework to achieve state-of-the-art ImageNet FID, and we show
significant improvements in both image quality and diversity for text-to-image
generative modeling. The core insight is that noise dampens the initial skew
between the desired high-quality distribution and the mixed distribution we
actually observe. We provide rigorous theoretical justification for our
approach by analyzing the trade-off between learning from biased data versus
limited unbiased data across diffusion times.

</details>


### [70] [Low-Barrier Dataset Collection with Real Human Body for Interactive Per-Garment Virtual Try-On](https://arxiv.org/abs/2506.10468)
*Zaiqiang Wu,Yechen Li,Jingyuan Liu,Yuki Shibata,Takayuki Hori,I-Chao Shen,Takeo Igarashi*

Main category: cs.GR

TL;DR: 论文提出了一种低成本的人体数据采集方法替代昂贵的机器人模特，并引入混合人物表示方法，解决了现有虚拟试衣技术在实时性和图像对齐上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟试衣方法通常仅限于正面视角且缺乏实时性，而基于单一服装的方法虽有所改进，但仍面临数据采集成本高及图像对齐不准的问题。

Method: 该方法采用真实人体采集服装数据，无需机器人模特，并结合简化的DensePose映射改进中间表示，确保服装与人体准确对齐。

Result: 实验表明，该方法在图像质量和时间一致性上优于现有技术，用户研究也证实其对服装购买决策有帮助。

Conclusion: 该研究提供了一种低成本的虚拟试衣解决方案，解决了现有技术的局限性，显著提升了用户体验。

Abstract: Existing image-based virtual try-on methods are often limited to the front
view and lack real-time performance. While per-garment virtual try-on methods
have tackled these issues by capturing per-garment datasets and training
per-garment neural networks, they still encounter practical limitations: (1)
the robotic mannequin used to capture per-garment datasets is prohibitively
expensive for widespread adoption and fails to accurately replicate natural
human body deformation; (2) the synthesized garments often misalign with the
human body. To address these challenges, we propose a low-barrier approach for
collecting per-garment datasets using real human bodies, eliminating the
necessity for a customized robotic mannequin. We also introduce a hybrid person
representation that enhances the existing intermediate representation with a
simplified DensePose map. This ensures accurate alignment of synthesized
garment images with the human body and enables human-garment interaction
without the need for customized wearable devices. We performed qualitative and
quantitative evaluations against other state-of-the-art image-based virtual
try-on methods and conducted ablation studies to demonstrate the superiority of
our method regarding image quality and temporal consistency. Finally, our user
study results indicated that most participants found our virtual try-on system
helpful for making garment purchasing decisions.

</details>


### [71] [Edit360: 2D Image Edits to 3D Assets from Any Angle](https://arxiv.org/abs/2506.10507)
*Junchao Huang,Xinting Hu,Zhuotao Tian,Shaoshuai Shi,Li Jiang*

Main category: cs.GR

TL;DR: Edit360是一个无需调整的框架，将2D图像编辑扩展到多视角一致的3D编辑，基于视频扩散模型，支持任意视角的用户定制编辑，并确保多视角的结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D编辑方法通常局限于预定的视角，缺乏灵活性和实际应用价值。Edit360旨在解决这一问题，提供更灵活的多视角编辑能力。

Method: 基于视频扩散模型，Edit360通过对选定锚定视角的2D修改，利用Anchor-View Editing Propagation机制在潜在和注意力空间中对齐和合并多视角信息。

Result: Edit360能够生成高质量的多视角序列，支持自定义3D内容创建。

Conclusion: Edit360为3D编辑提供了更灵活和一致的解决方案，推动了3D内容创作的发展。

Abstract: Recent advances in diffusion models have significantly improved image
generation and editing, but extending these capabilities to 3D assets remains
challenging, especially for fine-grained edits that require multi-view
consistency. Existing methods typically restrict editing to predetermined
viewing angles, severely limiting their flexibility and practical applications.
We introduce Edit360, a tuning-free framework that extends 2D modifications to
multi-view consistent 3D editing. Built upon video diffusion models, Edit360
enables user-specific editing from arbitrary viewpoints while ensuring
structural coherence across all views. The framework selects anchor views for
2D modifications and propagates edits across the entire 360-degree range. To
achieve this, Edit360 introduces a novel Anchor-View Editing Propagation
mechanism, which effectively aligns and merges multi-view information within
the latent and attention spaces of diffusion models. The resulting edited
multi-view sequences facilitate the reconstruction of high-quality 3D assets,
enabling customizable 3D content creation.

</details>


### [72] [Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture](https://arxiv.org/abs/2506.10580)
*Chengxu Zuo,Jiawei Huang,Xiao Jiang,Yuan Yao,Xiangren Shi,Rui Cao,Xinyu Yi,Feng Xu,Shihui Guo,Yipeng Qin*

Main category: cs.GR

TL;DR: 提出一种新型动态校准方法，打破传统IMU校准中的静态假设，实现实时估计RG'G和RBS，扩展了稀疏惯性运动捕捉系统的应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统IMU校准方法依赖静态假设（RG'G和RBS恒定不变），限制了其在动态场景中的应用。新方法旨在通过松弛假设实现动态校准。

Method: 采用两个松弛假设：短时间内RG'G和RBS变化可忽略；IMU读数具有多样性。基于Transformer的模型学习RG'G、RBS与IMU读数的映射，并通过校准触发机制确保条件满足。

Result: 首次实现隐式IMU校准（无需显式校准过程），并支持稀疏IMU的长期精准运动捕捉。

Conclusion: 该方法突破传统限制，为稀疏IMU在动态场景中的应用提供了新思路。

Abstract: In this paper, we propose a novel dynamic calibration method for sparse
inertial motion capture systems, which is the first to break the restrictive
absolute static assumption in IMU calibration, i.e., the coordinate drift RG'G
and measurement offset RBS remain constant during the entire motion, thereby
significantly expanding their application scenarios. Specifically, we achieve
real-time estimation of RG'G and RBS under two relaxed assumptions: i) the
matrices change negligibly in a short time window; ii) the human movements/IMU
readings are diverse in such a time window. Intuitively, the first assumption
reduces the number of candidate matrices, and the second assumption provides
diverse constraints, which greatly reduces the solution space and allows for
accurate estimation of RG'G and RBS from a short history of IMU readings in
real time. To achieve this, we created synthetic datasets of paired RG'G, RBS
matrices and IMU readings, and learned their mappings using a Transformer-based
model. We also designed a calibration trigger based on the diversity of IMU
readings to ensure that assumption ii) is met before applying our method. To
our knowledge, we are the first to achieve implicit IMU calibration (i.e.,
seamlessly putting IMUs into use without the need for an explicit calibration
process), as well as the first to enable long-term and accurate motion capture
using sparse IMUs. The code and dataset are available at
https://github.com/ZuoCX1996/TIC.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [73] [Is Sparse Matrix Reordering Effective for Sparse Matrix-Vector Multiplication?](https://arxiv.org/abs/2506.10356)
*Omid Asudeh,Sina Mahdipour Saravani,Gerald Sabin,Fabrice Rastello,P Sadayappan*

Main category: cs.DC

TL;DR: 研究了稀疏矩阵重排序对不同多核CPU平台上稀疏矩阵-向量乘法性能的影响，通过优化非零元素模式提升性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵重排序可以减少数据移动和改善负载均衡，从而提升性能，但不同CPU上的效果差异尚未系统评估。

Method: 比较了不同重排序策略在顺序和并行执行时的表现，关注测量方法、策略对比、机器间一致性及负载不平衡的影响。

Result: 发现稀疏矩阵重排序能显著提升性能，但其效果因CPU和策略而异。

Conclusion: 稀疏矩阵重排序是优化性能的有效手段，但需结合具体硬件和策略选择。

Abstract: This work evaluates the impact of sparse matrix reordering on the performance
of sparse matrix-vector multiplication across different multicore CPU
platforms. Reordering can significantly enhance performance by optimizing the
non-zero element patterns to reduce total data movement and improve the
load-balancing. We examine how these gains vary over different CPUs for
different reordering strategies, focusing on both sequential and parallel
execution. We address multiple aspects, including appropriate measurement
methodology, comparison across different kinds of reordering strategies,
consistency across machines, and impact of load imbalance.

</details>


### [74] [Resilience through Automated Adaptive Configuration for Distribution and Replication](https://arxiv.org/abs/2506.10248)
*Scott D. Stoller,Balaji Jayasankar,Yanhong A. Liu*

Main category: cs.DC

TL;DR: 本文提出了一种自动化框架，通过优化自适应分发和复制软件组件，使复杂系统在故障下具有弹性。


<details>
  <summary>Details</summary>
Motivation: 为了提高复杂系统在异构硬件环境下的容错能力，确保系统在故障发生时能够快速恢复和重新配置。

Method: 使用状态空间探索算法和基于新型状态等价关系的商约简优化技术，生成初始弹性配置和故障恢复策略。

Result: 通过实验验证，该框架成功应用于自动驾驶系统的模型，展示了其有效性。

Conclusion: 该框架为复杂系统提供了一种高效的容错解决方案，适用于异构硬件环境。

Abstract: This paper presents a powerful automated framework for making complex systems
resilient under failures, by optimized adaptive distribution and replication of
interdependent software components across heterogeneous hardware components
with widely varying capabilities. A configuration specifies how software is
distributed and replicated: which software components to run on each computer,
which software components to replicate, which replication protocols to use,
etc. We present an algorithm that, given a system model and resilience
requirements, (1) determines initial configurations of the system that are
resilient, and (2) generates a reconfiguration policy that determines
reconfiguration actions to execute in response to failures and recoveries. This
model-finding algorithm is based on state-space exploration and incorporates
powerful optimizations, including a quotient reduction based on a novel
equivalence relation between states. We present experimental results from
successfully applying a prototype implementation of our framework to a model of
an autonomous driving system.

</details>


### [75] [HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration](https://arxiv.org/abs/2506.10401)
*Jiaqi Lv,Xufeng He,Yanchen Liu,Xu Dai,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 提出新框架，利用AI编译器和自动优化技术生成高性能CUDA代码及对应平台代码，提升CUDA转译性能。


<details>
  <summary>Details</summary>
Motivation: 解决CUDA代码转译到其他平台时的性能可移植性问题，克服现有方法的局限性和高开发成本。

Method: 采用AI编译器和自动优化技术构建框架，结合图增强数据方法，并引入HPCTransEval评估标准。

Result: 实验表明框架显著提升CUDA转译性能，展示LLMs解决CUDA生态兼容性问题的潜力。

Conclusion: 新框架有效提升CUDA转译效率，为LLMs在多平台代码转换中的应用开辟道路。

Abstract: The rapid growth of deep learning has driven exponential increases in model
parameters and computational demands. NVIDIA GPUs and their CUDA-based software
ecosystem provide robust support for parallel computing, significantly
alleviating computational bottlenecks. Meanwhile, due to the cultivation of
user programming habits and the high performance of GPUs, the CUDA ecosystem
has established a dominant position in the field of parallel software. This
dominance requires other hardware platforms to support CUDA-based software with
performance portability. However, translating CUDA code to other platforms
poses significant challenges due to differences in parallel programming
paradigms and hardware architectures. Existing approaches rely on language
extensions, domain-specific languages (DSLs), or compilers but face limitations
in workload coverage and generalizability. Moreover, these methods often incur
substantial development costs. Recently, LLMs have demonstrated extraordinary
potential in various vertical domains, especially in code-related tasks.
However, the performance of existing LLMs in CUDA transpilation, particularly
for high-performance code, remains suboptimal. The main reason for this
limitation lies in the lack of high-quality training datasets. To address these
challenges, we propose a novel framework for generating high-performance CUDA
and corresponding platform code pairs, leveraging AI compiler and automatic
optimization technology. We further enhance the framework with a graph-based
data augmentation method and introduce HPCTransEval, a benchmark for evaluating
LLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU
transpilation as a case study on leading LLMs. The result demonstrates that our
framework significantly improves CUDA transpilation, highlighting the potential
of LLMs to address compatibility challenges within the CUDA ecosystem.

</details>


### [76] [Federated Learning within Global Energy Budget over Heterogeneous Edge Accelerators](https://arxiv.org/abs/2506.10413)
*Roopkatha Banerjee,Tejus Chandrashekar,Ananth Eswar,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 联邦学习（FL）在保护数据隐私的同时实现了跨分布式客户端的协作模型训练。然而，如何在设备与数据异构性下同时优化能源效率和模型精度仍是一个挑战。本文提出了一种新的客户端选择优化问题，以在总能源限制内最大化模型精度并减少训练时间，通过双级ILP公式实现。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中能源效率与模型精度难以兼顾的问题，并探索在全局能源预算下的可持续AI方案。

Method: 提出了一种基于双级整数线性规划（ILP）的客户端选择优化方法，结合近似Shapley值和能源-时间预测模型。

Result: 提出的FedJoule框架在多样化能源预算、非独立同分布数据和实际实验配置下，其训练精度和时间分别比现有方法和简单基线提升了15%和48%。

Conclusion: FedJoule在能源使用和性能之间实现了有效的权衡，为联邦学习环境提供了可行的解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. However, optimizing both
energy efficiency and model accuracy remains a challenge, given device and data
heterogeneity. Further, sustainable AI through a global energy budget for FL
has not been explored. We propose a novel optimization problem for client
selection in FL that maximizes the model accuracy within an overall energy
limit and reduces training time. We solve this with a unique bi-level ILP
formulation that leverages approximate Shapley values and energy-time
prediction models to efficiently solve this. Our FedJoule framework achieves
superior training accuracies compared to SOTA and simple baselines for diverse
energy budgets, non-IID distributions, and realistic experiment configurations,
performing 15% and 48% better on accuracy and time, respectively. The results
highlight the effectiveness of our method in achieving a viable trade-off
between energy usage and performance in FL environments.

</details>


### [77] [Automating Multi-Tenancy Performance Evaluation on Edge Compute Nodes](https://arxiv.org/abs/2506.10461)
*Joanna Georgiou,Moysis Symeonides,George Pallis,Marios D. Dikaiakos*

Main category: cs.DC

TL;DR: 该论文提出了一种自动化基准测试框架，用于分析边缘计算环境中的多租户性能问题，帮助管理员优化资源分配和性能。


<details>
  <summary>Details</summary>
Motivation: 边缘计算的多租户需求引发了安全和性能问题，传统手动配置和测试方法效率低，亟需自动化工具来解决。

Method: 设计了包含监控堆栈和集成常用测试负载的框架，用于分析多租户对边缘环境的影响。

Result: 通过案例驱动分析，提供了关于多租户对不同硬件配置和负载的影响的见解。

Conclusion: 该框架公开可用，能够高效分析多租户性能问题，优化边缘计算环境。

Abstract: Edge Computing emerges as a promising alternative of Cloud Computing, with
scalable compute resources and services deployed in the path between IoT
devices and Cloud. Since virtualization techniques can be applied on Edge
compute nodes, administrators can share their Edge infrastructures among
multiple users, providing the so-called multi-tenancy. Even though
multi-tenancy is unavoidable, it raises concerns about security and performance
degradation due to resource contention in Edge Computing. For that,
administrators need to deploy services with non-antagonizing profiles and
explore workload co-location scenarios to enhance performance and energy
consumption. Achieving this, however, requires extensive configuration,
deployment, iterative testing, and analysis, an effort-intensive and
time-consuming process. To address this challenge, we introduce an
auto-benchmarking framework designed to streamline the analysis of
multi-tenancy performance in Edge environments. Our framework includes a
built-in monitoring stack and integrates with widely used benchmarking
workloads, such as streaming analytics, database operations, machine learning
applications, and component-based stress testing. We perform a case-driven
analysis and provide valuable insights into the impact of multi-tenancy on Edge
environments with different hardware configurations and diverse workloads.
Finally, the implementation of our framework, along with the containerized
workloads used for experimentation, is publicly available.

</details>


### [78] [TD-Pipe: Temporally-Disaggregated Pipeline Parallelism Architecture for High-Throughput LLM Inference](https://arxiv.org/abs/2506.10470)
*Hongbin Zhang,Taosheng Wei,Zhenyi Zheng,Jiangsu Du,Zhiguang Chen,Yutong Lu*

Main category: cs.DC

TL;DR: TD-Pipe是一种新型的管道并行架构，通过临时解耦预填充和解码阶段来提高LLM推理的吞吐量，解决了传统管道并行中的负载不平衡和数据依赖问题，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模增大，管道并行在LLM推理中显示出潜力，但预填充和解码阶段的不均衡负载和复杂数据依赖导致管道气泡和性能下降，需要一种更高效的并行架构。

Method: TD-Pipe采用时间解耦的管道并行架构，通过层次控制器解耦调度与执行，基于AI的贪婪预填充预测输出长度和模拟内存使用，动态平衡解码阶段负载，并通过时空强度比较优化阶段切换。

Result: 实验表明，TD-Pipe在仅使用PCIe互连的GPU节点上，吞吐量比现有张量并行方法高1.91倍，比管道并行方法高2.73倍。

Conclusion: TD-Pipe通过创新架构和优化策略，显著提升了LLM推理的吞吐量，为大规模模型的高效推理提供了有效解决方案。

Abstract: As the model size continuously increases, pipeline parallelism shows great
promise in throughput-oriented LLM inference due to its low demand on
communications. However, imbalanced pipeline workloads and complex data
dependencies in the prefill and decode phases result in massive pipeline
bubbles and further severe performance reduction. To better exploit the
pipeline parallelism for high-throughput LLM inference, we propose TD-Pipe,
with the key idea lies in the temporally-disaggregated pipeline parallelism
architecture. Specifically, this architecture disaggregates the prefill and
decode phases in the temporal dimension, so as to eliminate pipeline bubbles
caused by the phase switching. TD-Pipe identifies potential issues of
exploiting the novel architecture and provides solutions. First, a
hierarchy-controller structure is used to better coordinate devices in pipeline
parallelism by decoupling the scheduling from execution. Second, the AI-based
greedy prefill approach aggressively performs more prefills by predicting the
output length and simulating the memory usage. Third, the inter-batch work
stealing approach dynamically balances decode phase workloads between different
batches to reduce bubbles. Forth, the spatial-temporal intensity comparison
approach determines the optimal switch from decode to prefill by comparing the
performance drop from reduced computational intensity with that from phase
switching bubbles. Extensive experiments show that TD-Pipe effectively
increases the throughput of LLM inference by up to 1.91x over the existing
tensor parallel approach and 2.73x over the existing pipeline parallel approach
on GPU nodes with only PCIe interconnection.

</details>


### [79] [HP2C-DT: High-Precision High-Performance Computer-enabled Digital Twin](https://arxiv.org/abs/2506.10523)
*E. Iraola,M. García-Lorenzo,F. Lordan-Gomis,F. Rossi,E. Prieto-Araujo,R. M. Badia*

Main category: cs.DC

TL;DR: 论文提出了HP2C-DT架构，将高性能计算（HPC）融入数字孪生工作流，通过动态任务分配解决实时性和计算需求平衡问题，并在电网案例中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生在实时响应与计算需求之间存在矛盾，云端和边缘计算方案各有不足。

Method: 提出HP2C-DT架构，整合HPC作为动态任务分配的核心，并开发框架通过COMPSs实现跨基础设施的工作负载分发。

Result: 在电网案例中，HP2C-DT减少了通信带宽一个数量级，响应时间提升2倍，计算密集型任务扩展性接近理想状态。

Conclusion: HPC驱动的数字孪生方案能够突破现有局限性，提升智能性、速度和复杂场景处理能力。

Abstract: Digital twins are transforming the way we monitor, analyze, and control
physical systems, but designing architectures that balance real-time
responsiveness with heavy computational demands remains a challenge.
Cloud-based solutions often struggle with latency and resource constraints,
while edge-based approaches lack the processing power for complex simulations
and data-driven optimizations.
  To address this problem, we propose the High-Precision High-Performance
Computer-enabled Digital Twin (HP2C-DT) reference architecture, which
integrates High-Performance Computing (HPC) into the computing continuum.
Unlike traditional setups that use HPC only for offline simulations, HP2C-DT
makes it an active part of digital twin workflows, dynamically assigning tasks
to edge, cloud, or HPC resources based on urgency and computational needs.
  Furthermore, to bridge the gap between theory and practice, we introduce the
HP2C-DT framework, a working implementation that uses COMPSs for seamless
workload distribution across diverse infrastructures. We test it in a power
grid use case, showing how it reduces communication bandwidth by an order of
magnitude through edge-side data aggregation, improves response times by up to
2x via dynamic offloading, and maintains near-ideal strong scaling for
compute-intensive workflows across a practical range of resources. These
results demonstrate how an HPC-driven approach can push digital twins beyond
their current limitations, making them smarter, faster, and more capable of
handling real-world complexity.

</details>


### [80] [GPU-Accelerated Distributed QAOA on Large-scale HPC Ecosystems](https://arxiv.org/abs/2506.10531)
*Zhihao Xu,Srikar Chundury,Seongmin Kim,Amir Shehata,Xinyi Li,Ang Li,Tengfei Luo,Frank Mueller,In-Saeng Suh*

Main category: cs.DC

TL;DR: 研究改进了分布式量子近似优化算法（DQAOA）的可扩展性和效率，利用高性能计算和GPU加速量子模拟，实现了比CPU模拟快10倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 量子计算在解决复杂组合优化问题方面潜力巨大，但现有技术在处理高维、密集问题时面临挑战。DQAOA旨在利用当前量子计算技术和高性能计算系统来解决这些问题。

Method: 通过高级问题分解和并行执行（在Frontier CPU/GPU超级计算机上使用消息传递），优化DQAOA的可扩展性和效率，确保量子-经典工作负载的高效管理。

Result: 实验结果表明，改进的问题分解策略和GPU加速量子模拟显著提升了DQAOA的性能，比CPU模拟快10倍，并支持大规模问题的实际部署。

Conclusion: 该研究为混合量子-经典应用部署提供了更高效的解决方案，同时展示了通过量子框架（QFw）集成未来HPC-量子计算系统的潜力。

Abstract: Quantum computing holds great potential to accelerate the process of solving
complex combinatorial optimization problems. The Distributed Quantum
Approximate Optimization Algorithm (DQAOA) addresses high-dimensional, dense
problems using current quantum computing techniques and high-performance
computing (HPC) systems. In this work, we improve the scalability and
efficiency of DQAOA through advanced problem decomposition and parallel
execution using message passing on the Frontier CPU/GPU supercomputer. Our
approach ensures efficient quantum-classical workload management by
distributing large problem instances across classical and quantum resources.
Experimental results demonstrate that enhanced decomposition strategies and
GPU-accelerated quantum simulations significantly improve DQAOA's performance,
achieving up to 10x speedup over CPU-based simulations. This advancement
enables better scalability for large problem instances, supporting the
practical deployment of GPU systems for hybrid quantum-classical applications.
We also highlight ongoing integration efforts using the Quantum Framework (QFw)
to support future HPC-quantum computing systems.

</details>


### [81] [6G Infrastructures for Edge AI: An Analytical Perspective](https://arxiv.org/abs/2506.10570)
*Kurt Horvath,Shpresa Tuda,Blerta Idrizi,Stojan Kitanov,Fisnik Doko,Dragi Kimovski*

Main category: cs.DC

TL;DR: 本文分析了5G网络在支持AI驱动的边缘应用中的性能不足，提出向6G发展的建议。


<details>
  <summary>Details</summary>
Motivation: AI与物联网的融合需要超低延迟和高吞吐量，但现有5G在实际部署中的性能无法满足需求，需技术升级。

Method: 对中欧5G网络基础设施进行实证评估，测量延迟并分析性能差距。

Result: 测量显示延迟为61-110毫秒，超过AI应用需求的270%，揭示5G的显著不足。

Conclusion: 提出建议以缩小5G性能与下一代AI应用需求之间的差距，推动6G生态系统的建设。

Abstract: The convergence of Artificial Intelligence (AI) and the Internet of Things
has accelerated the development of distributed, network-sensitive applications,
necessitating ultra-low latency, high throughput, and real-time processing
capabilities. While 5G networks represent a significant technological
milestone, their ability to support AI-driven edge applications remains
constrained by performance gaps observed in real-world deployments. This paper
addresses these limitations and highlights critical advancements needed to
realize a robust and scalable 6G ecosystem optimized for AI applications.
Furthermore, we conduct an empirical evaluation of 5G network infrastructure in
central Europe, with latency measurements ranging from 61 ms to 110 ms across
different close geographical areas. These values exceed the requirements of
latency-critical AI applications by approximately 270%, revealing significant
shortcomings in current deployments. Building on these findings, we propose a
set of recommendations to bridge the gap between existing 5G performance and
the requirements of next-generation AI applications.

</details>


### [82] [Graph-based Gossiping for Communication Efficiency in Decentralized Federated Learning](https://arxiv.org/abs/2506.10607)
*Huong Nguyen,Hong-Tri Nguyen,Praveen Kumar Donta,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.DC

TL;DR: 本文探讨了联邦学习中的通信效率问题，提出了一种基于图论的gossiping机制，通过最小生成树和图着色优化网络结构和调度，显著提升了通信效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习依赖单一中心服务器可能导致性能瓶颈和单点故障，而完全去中心化又引入通信效率问题。现有解决方案未能真实模拟分布式环境，导致性能评估不可靠。

Method: 提出基于最小生成树和图着色的图论gossiping机制，优化网络结构和调度，并在真实物理设备上配置子网络以模拟真实环境。

Result: 实验显示该方法显著提升通信效率，与不同拓扑结构和数据大小兼容，带宽和传输时间分别减少约8倍和4.4倍。

Conclusion: 通过优化网络通信机制，能有效提升去中心化学习的效率，适应实际应用需求。

Abstract: Federated learning has emerged as a privacy-preserving technique for
collaborative model training across heterogeneously distributed silos. Yet, its
reliance on a single central server introduces potential bottlenecks and risks
of single-point failure. Decentralizing the server, often referred to as
decentralized learning, addresses this problem by distributing the server role
across nodes within the network. One drawback regarding this pure
decentralization is it introduces communication inefficiencies, which arise
from increased message exchanges in large-scale setups. However, existing
proposed solutions often fail to simulate the real-world distributed and
decentralized environment in their experiments, leading to unreliable
performance evaluations and limited applicability in practice. Recognizing the
lack from prior works, this work investigates the correlation between model
size and network latency, a critical factor in optimizing decentralized
learning communication. We propose a graph-based gossiping mechanism, where
specifically, minimum spanning tree and graph coloring are used to optimize
network structure and scheduling for efficient communication across various
network topologies and message capacities. Our approach configures and manages
subnetworks on real physical routers and devices and closely models real-world
distributed setups. Experimental results demonstrate that our method
significantly improves communication, compatible with different topologies and
data sizes, reducing bandwidth and transfer time by up to circa 8 and 4.4
times, respectively, compared to naive flooding broadcasting methods.

</details>


### [83] [Deployment of Containerized Simulations in an API-Driven Distributed Infrastructure](https://arxiv.org/abs/2506.10642)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: 文章介绍了一种名为SUNRISE的基础设施，旨在统一虚拟原型设计解决方案，促进访问多种仿真技术，并利用分布式计算资源提升协作效率。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式系统市场的动态发展，虚拟原型成为硬件/软件协同设计的重要工具。但当前的解决方案多样且分散，亟需一种统一的方法来整合技术和资源。

Method: 提出了SUNRISE基础设施，通过提供统一的访问方式、开放的API和分布式计算资源，实现仿真技术的整合与协作。

Result: SUNRISE为用户提供了一个高效的平台，支持多样化的仿真技术和分布式资源利用。

Conclusion: SUNRISE为虚拟原型设计领域提供了一种创新的统一解决方案，显著提升了协作和技术整合的效率。

Abstract: The increasingly dynamic market for embedded systems makes virtual prototypes
an indispensable tool for hardware/software codesign. The broad acceptance of
the methodology has led to a diverse range of solutions: from open-source, pure
console-based simulators to highly capable commercial simulation tools. In this
work we present SUNRISE, an infrastructure to provide users a unified approach
to utilizing virtual prototyping solutions, facilitate access to various
simulation technologies and boost cooperation by leveraging decentralized
compute resources for deployment of simulation workloads and definition of open
APIs.

</details>


### [84] [Towards Sustainable Computing: Exploring Energy Consumption Efficiency of Alternative Configurations and Workloads in an Open Source Messaging System](https://arxiv.org/abs/2506.10693)
*Maria Voreakou,George Kousiouris,Mara Nikolaidou*

Main category: cs.DC

TL;DR: 研究探索了RabbitMQ消息框架的能源基准测试，比较不同架构和工作负载下的能耗，最高可减少31%的电力消耗。


<details>
  <summary>Details</summary>
Motivation: 随着大规模计算基础设施能源消耗问题日益突出，尤其是云计算和微服务架构的普及，研究消息系统的能耗优化变得至关重要。

Method: 通过实验测试不同工作负载和配置下的RabbitMQ系统，比较替代架构的能源消耗，量化差异。

Result: 不同架构选择可显著影响能耗，最高节能达31%，数据集公开可供进一步研究。

Conclusion: 研究为消息系统能源优化提供了数据支持，有助于架构比较和成本建模。

Abstract: Energy consumption in current large scale computing infrastructures is
becoming a critical issue, especially with the growing demand for centralized
systems such as cloud environments. With the advancement of microservice
architectures and the Internet of Things, messaging systems have become an
integral and mainstream part of modern computing infrastructures, carrying out
significant workload in a majority of applications. In this paper, we describe
an experimental process to explore energy-based benchmarking for RabbitMQ, one
of the main open source messaging frameworks. The involved system is described,
as well as required components, and setup scenarios, involving different
workloads and configurations among the tests as well as messaging system use
cases. Alternative architectures are investigated and compared from an energy
consumption point of view, for different message rates and consumer numbers.
Differences in architectural selection have been quantified and can lead to up
to 31\% reduction in power consumption. The resulting dataset is made publicly
available and can thus prove helpful for architectures' comparison,
energy-based cost modeling, and beyond.

</details>


### [85] [The Impact of Partial Computations on the Red-Blue Pebble Game](https://arxiv.org/abs/2506.10854)
*Pál András Papp,Aleksandros Sobczyk,A. N. Yzelman*

Main category: cs.DC

TL;DR: 本文研究了带部分计算步骤的红蓝鹅卵石游戏（PRBP）的扩展模型，展示了在允许逐步聚合输入时，I/O成本可以显著降低，但确定最优成本是NP难问题。


<details>
  <summary>Details</summary>
Motivation: 传统红蓝鹅卵石游戏（RBP）假设操作的所有输入必须同时存在于快速内存中，而在实际计算中，输入可以逐步聚合。PRBP模型更贴近现实，提供了更精确的I/O成本分析。

Method: 引入带部分计算的PRBP模型，通过简单例子展示其优势，分析I/O成本降低的潜在因素，并证明其NP难性质。同时，将RBP中的$S$-分区工具扩展到PRBP模型，用于推导I/O成本下界。

Result: PRBP模型中，I/O成本可降低至线性因子，但确定最优成本是NP难问题。此外，文中还展示了如何利用扩展的工具为常见计算任务提供I/O成本下界。

Conclusion: PRBP模型通过允许部分计算步骤显著提升了I/O成本分析的准确性，但其最优成本的确定和近似仍具有挑战性，为未来的研究提供了方向。

Abstract: We study an extension of the well-known red-blue pebble game (RBP) with
partial computation steps, inspired by the recent work of Sobczyk. While the
original RBP assumes that we need to have all the inputs of an operation in
fast memory at the same time, in many concrete computations, the inputs can be
aggregated one by one into the final output value. These partial computation
steps can enable pebbling strategies with much smaller I/O cost, and in
settings where such a step-by-step aggregation is possible, this extended
red-blue pebble game offers a much more realistic cost model.
  We establish the fundamental properties of this partial-computing red-blue
pebble game (PRBP), and compare it to the original RBP. We begin with some
simple examples where allowing partial computations can decrease the optimal
I/O cost. It is also shown that the cost can decrease by up to a linear factor
this way, but in general, it is NP-hard to decide whether partial computations
allow for a smaller cost in a specific DAG. We then discuss how $S$-partitions,
a crucial tool for deriving I/O lower bounds in RBP, can be adapted to the PRBP
model. These new tools are then used to establish lower bounds on the I/O cost
of some prominent computational tasks. Finally, we also adapt a hardness result
from RBP, showing that the optimum cost is still NP-hard to approximate in PRBP
to any reasonable factor.

</details>


### [86] [Adaptive Job Scheduling in Quantum Clouds Using Reinforcement Learning](https://arxiv.org/abs/2506.10889)
*Waylon Luo,Jiapeng Zhao,Tong Zhan,Qiang Guan*

Main category: cs.DC

TL;DR: 提出了一个模拟工具，用于在分布式量子处理器上优化任务调度和资源协调，以提高计算效率和保真度。


<details>
  <summary>Details</summary>
Motivation: 当前量子系统面临硬件限制（如比特数少、相干时间短、易受噪声影响），影响复杂量子电路的执行能力。需要解决如何在多量子处理器上高效分配和执行任务的问题。

Method: 开发了一个模拟工具，支持分布式调度和并行执行量子任务，并对比了四种调度策略，包括基于强化学习的模型。

Result: 分析表明，并行化且考虑噪声的调度策略能显著提升分布式量子基础设施的计算吞吐量。

Conclusion: 通过模拟工具和优化调度策略，可以有效克服硬件限制，提升量子计算效率。

Abstract: Present-day quantum systems face critical bottlenecks, including limited
qubit counts, brief coherence intervals, and high susceptibility to errors-all
of which obstruct the execution of large and complex circuits. The advancement
of quantum algorithms has outpaced the capabilities of existing quantum
hardware, making it difficult to scale computations effectively. Additionally,
inconsistencies in hardware performance and pervasive quantum noise undermine
system stability and computational accuracy. To optimize quantum workloads
under these constraints, strategic approaches to task scheduling and resource
coordination are essential. These methods must aim to accelerate processing,
retain operational fidelity, and reduce the communication burden inherent to
distributed setups. One of the persistent challenges in this domain is how to
efficiently divide and execute large circuits across multiple quantum
processors (QPUs), especially in error-prone environments. In response, we
introduce a simulation-based tool that supports distributed scheduling and
concurrent execution of quantum jobs on networked QPUs connected via real-time
classical channels. The tool models circuit decomposition for workloads that
surpass individual QPU limits, allowing for parallel execution through
inter-processor communication. Using this simulation environment, we compare
four distinct scheduling techniques-among them, a model informed by
reinforcement learning. These strategies are evaluated across multiple metrics,
including runtime efficiency, fidelity preservation, and communication costs.
Our analysis underscores the trade-offs inherent in each approach and
highlights how parallelized, noise-aware scheduling can meaningfully improve
computational throughput in distributed quantum infrastructures.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [87] [GPU Acceleration of SQL Analytics on Compressed Data](https://arxiv.org/abs/2506.10092)
*Zezhou Huang,Krystian Sakowski,Hans Lehnert,Wei Cui,Carlo Curino,Matteo Interlandi,Marius Dumitru,Rathijit Sen*

Main category: cs.DB

TL;DR: 该论文提出了一种直接在轻量压缩数据上运行查询的新方法，解决了GPU内存有限的问题，提升了SQL分析性能。


<details>
  <summary>Details</summary>
Motivation: 由于GPU内存容量有限，现有解决方案如数据分区或混合执行在大数据集上存在性能瓶颈，因此需要一种直接在压缩数据上操作的方法。

Method: 论文提出了一系列方法，包括在RLE编码数据上直接查询、处理异构列编码以及利用PyTorch张量操作实现跨设备兼容性。

Result: 实验结果表明，与最先进的CPU分析系统相比，该方法在实际生产数据集上实现了数量级的加速。

Conclusion: 该研究为GPU在更广泛场景中的应用铺平了道路，并与其他扩展或回退机制互补。

Abstract: GPUs are uniquely suited to accelerate (SQL) analytics workloads thanks to
their massive compute parallelism and High Bandwidth Memory (HBM) -- when
datasets fit in the GPU HBM, performance is unparalleled. Unfortunately, GPU
HBMs remain typically small when compared with lower-bandwidth CPU main memory.
Besides brute-force scaling across many GPUs, current solutions to accelerate
queries on large datasets include leveraging data partitioning and loading
smaller data batches in GPU HBM, and hybrid execution with a connected device
(e.g., CPUs). Unfortunately, these approaches are exposed to the limitations of
lower main memory and host-to-device interconnect bandwidths, introduce
additional I/O overheads, or incur higher costs. This is a substantial problem
when trying to scale adoption of GPUs on larger datasets. Data compression can
alleviate this bottleneck, but to avoid paying for costly
decompression/decoding, an ideal solution must include computation primitives
to operate directly on data in compressed form.
  This is the focus of our paper: a set of new methods for running queries
directly on light-weight compressed data using schemes such as Run-Length
Encoding (RLE), index encoding, bit-width reductions, and dictionary encoding.
Our novelty includes operating on multiple RLE columns without decompression,
handling heterogeneous column encodings, and leveraging PyTorch tensor
operations for portability across devices. Experimental evaluations show
speedups of an order of magnitude compared to state-of-the-art commercial
CPU-only analytics systems, for real-world queries on a production dataset that
would not fit into GPU memory uncompressed. This work paves the road for GPU
adoption in a much broader set of use cases, and it is complementary to most
other scale-out or fallback mechanisms.

</details>


### [88] [A Hybrid Heuristic Framework for Resource-Efficient Querying of Scientific Experiments Data](https://arxiv.org/abs/2506.10422)
*Mayank Patel,Minal Bhise*

Main category: cs.DB

TL;DR: 论文提出了一种轻量级资源感知混合框架（RAW-HF），以优化原始数据查询并高效利用有限资源，显著减少了工作负载执行时间和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 传统数据库管理系统和HTAP系统在加载全部数据时耗费大量时间和资源，而原地查询引擎可能多次重新解析数据，导致资源浪费和成本增加。

Method: 提出RAW-HF框架，包含优化资源分配和工作负载执行的模块，结合资源可用性感知和机器学习技术。

Result: 在实际科学数据集（如SDSS和LOD）上，RAW-HF显著减少了工作负载执行时间和资源消耗，优于传统DBMS和现有混合系统技术。

Conclusion: RAW-HF在资源利用率和查询性能上取得了显著改进，为高效处理大规模数据提供了新思路。

Abstract: Scientific experiments and modern applications are generating large amounts
of data every day. Most organizations utilize In-house servers or Cloud
resources to manage application data and workload. The traditional database
management system (DBMS) and HTAP systems spend significant time & resources to
load the entire dataset into DBMS before starting query execution. On the other
hand, in-situ engines may reparse required data multiple times, increasing
resource utilization and data processing costs. Additionally, over or
under-allocation of resources also increases application running costs. This
paper proposes a lightweight Resource Availability &Workload aware Hybrid
Framework (RAW-HF) to optimize querying raw data by utilizing existing finite
resources efficiently. RAW-HF includes modules that help optimize the resources
required to execute a given workload and maximize the utilization of existing
resources. The impact of applying RAW-HF to real-world scientific dataset
workloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data
(LOD) presented over 90% and 85% reduction in workload execution time (WET)
compared to widely used traditional DBMS PostgreSQL. The overall CPU, IO
resource utilization, and WET have been reduced by 26%, 25%, and 26%,
respectively, while improving memory utilization by 33%, compared to the
state-of-the-art workload-aware partial loading technique (WA) proposed for
hybrid systems. A comparison of MUAR technique used by RAW-HF with machine
learning based resource allocation techniques like PCC is also presented.

</details>


### [89] [A Unifying Algorithm for Hierarchical Queries](https://arxiv.org/abs/2506.10238)
*Mahmoud Abo Khamis,Jesse Comer,Phokion Kolaitis,Sudeepa Roy,Val Tannen*

Main category: cs.DB

TL;DR: 研究证明，层次化查询定义了‘bag-set maximization’问题的易处理性边界，并与概率查询评估和Shapley值计算问题共享统一的算法框架。


<details>
  <summary>Details</summary>
Motivation: 探索层次化查询在不同计算问题中的作用，特别是新的‘bag-set maximization’问题，以理解其计算复杂性边界。

Method: 通过引入‘2-monoid’抽象代数结构，提出一种统一的算法框架，适用于层次化查询的三大问题：概率查询评估、Shapley值计算和bag-set maximization。

Result: 非层次化查询的bag-set maximization问题是NP难问题；层次化查询的三大问题均可通过2-monoid的统一算法在多项式时间内解决。

Conclusion: 层次化查询在多个自然算法问题中定义了易处理性边界，统一的2-monoid框架为解决这些问题提供了高效算法。

Abstract: The class of hierarchical queries is known to define the boundary of the
dichotomy between tractability and intractability for the following two
extensively studied problems about self-join free Boolean conjunctive queries
(SJF-BCQ): (i) evaluating a SJF-BCQ on a tuple-independent probabilistic
database; (ii) computing the Shapley value of a fact in a database on which a
SJF-BCQ evaluates to true. Here, we establish that hierarchical queries define
also the boundary of the dichotomy between tractability and intractability for
a different natural algorithmic problem, which we call the "bag-set
maximization" problem. The bag-set maximization problem associated with a
SJF-BCQ $Q$ asks: given a database $\cal D$, find the biggest value that $Q$
takes under bag semantics on a database $\cal D'$ obtained from $\cal D$ by
adding at most $\theta$ facts from another given database $\cal D^r$.
  For non-hierarchical queries, we show that the bag-set maximization problem
is an NP-complete optimization problem. More significantly, for hierarchical
queries, we show that all three aforementioned problems (probabilistic query
evaluation, Shapley value computation, and bag-set maximization) admit a single
unifying polynomial-time algorithm that operates on an abstract algebraic
structure, called a "2-monoid". Each of the three problems requires a different
instantiation of the 2-monoid tailored for the problem at hand.

</details>


### [90] [S3 Mirror: S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable with DBOS](https://arxiv.org/abs/2506.10886)
*Steven Vasquez-Grinnell,Alex Poliakov*

Main category: cs.DB

TL;DR: 开发了S3Mirror应用，用于高效、可靠地传输大型基因组数据，性能优于AWS DataSync且成本更低。


<details>
  <summary>Details</summary>
Motivation: 满足制药组织对快速、可靠和可观测的大型基因组数据传输的需求。

Method: 使用了DBOSTransact持久执行框架，并在多种环境中进行性能与成本测试。

Result: S3Mirror在DBOS Cloud Pro中的运行速度比AWS DataSync快40倍，且成本更低，同时具备高容错性和实时可观测性。

Conclusion: S3Mirror是一个开源的高效、低成本解决方案，适用于大型数据传输需求。

Abstract: To meet the needs of a large pharmaceutical organization, we set out to
create S3Mirror - an application for transferring large genomic sequencing
datasets between S3 buckets quickly, reliably, and observably. We used the
DBOSTransact durable execution framework to achieve these goals and benchmarked
the performance and cost of the application. S3Mirror is an open source DBOS
Python application that can run in a variety of environments, including DBOS
Cloud Pro where it runs as much as 40x faster than AWS DataSync at a fraction
of the cost. Moreover, S3Mirror is resilient to failures and allows for
real-time filewise observability of ongoing and past transfers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [91] [CarbonSet: A Dataset to Analyze Trends and Benchmark the Sustainability of CPUs and GPUs](https://arxiv.org/abs/2506.10373)
*Jiajun Hu,Chetan Choppali Sudarshan,Vidya A. Chhabria,Aman Arora*

Main category: cs.AR

TL;DR: 本文介绍了CarbonSet，一个整合了过去十年CPU和GPU的可持续性和性能指标的全面数据集，用于评估下一代处理器的设计，并揭示了现代处理器在可持续性设计上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着芯片产业的快速发展，碳排放大增，但缺乏对整个芯片生命周期可持续性的综合分析，因此需要建立数据集以评估和改进处理器设计的可持续性。

Method: 构建CarbonSet数据集，整合CPU和GPU的可持续性和性能指标，并分析过去十年旗舰处理器的可持续性趋势。

Result: 现代处理器设计尚未实现可持续性，过去三年总碳排放量因AI需求激增而增长超过50倍，能效和制造碳排放问题突出。

Conclusion: 研究强调芯片行业需改进设计和制造工艺，以降低碳排放并实现可持续性目标。

Abstract: Over the years, the chip industry has consistently developed high-performance
processors to address the increasing demands across diverse applications.
However, the rapid expansion of chip production has significantly increased
carbon emissions, raising critical concerns about environmental sustainability.
While researchers have previously modeled the carbon footprint (CFP) at both
system and processor levels, a holistic analysis of sustainability trends
encompassing the entire chip lifecycle remains lacking. This paper presents
CarbonSet, a comprehensive dataset integrating sustainability and performance
metrics for CPUs and GPUs over the past decade. CarbonSet aims to benchmark and
assess the design of next-generation processors. Leveraging this dataset, we
conducted detailed analysis of flagship processors' sustainability trends over
the last decade. This paper further highlights that modern processors are not
yet sustainably designed, with total carbon emissions increasing more than
50$\times$ in the past three years due to the surging demand driven by the AI
boom. Power efficiency remains a significant concern, while advanced process
nodes pose new challenges requiring to effectively amortize the dramatically
increased manufacturing carbon emissions.

</details>


### [92] [EasyDRAM: An FPGA-based Infrastructure for Fast and Accurate End-to-End Evaluation of Emerging DRAM Techniques](https://arxiv.org/abs/2506.10441)
*Oğuzhan Canpolat,Ataberk Olgun,David Novo,Oğuz Ergin,Onur Mutlu*

Main category: cs.AR

TL;DR: EasyDRAM是一个基于FPGA的框架，用于快速准确地评估DRAM技术，解决了现有平台在硬件描述语言要求和现代系统建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基于FPGA的DRAM技术评估平台存在两大问题：需要深厚的硬件描述语言知识，以及对现代计算系统建模不准确。这限制了技术的可访问性和评估的准确性。

Method: EasyDRAM通过两个关键创新解决这些问题：（1）允许开发者使用高级语言（C++）实现DRAM技术；（2）采用时间缩放技术精确建模处理器和DRAM之间的时钟频率差异。

Result: EasyDRAM能够快速且准确地评估DRAM技术的系统级优势，同时降低了开发门槛。

Conclusion: EasyDRAM有望促进内存系统设计的创新，并已开源以支持未来研究。

Abstract: DRAM is a critical component of modern computing systems. Recent works
propose numerous techniques (that we call DRAM techniques) to enhance
DRAM-based computing systems' throughput, reliability, and computing
capabilities (e.g., in-DRAM bulk data copy). Evaluating the system-wide
benefits of DRAM techniques is challenging as they often require modifications
across multiple layers of the computing stack. Prior works propose FPGA-based
platforms for rapid end-to-end evaluation of DRAM techniques on real DRAM
chips. Unfortunately, existing platforms fall short in two major aspects: (1)
they require deep expertise in hardware description languages, limiting
accessibility; and (2) they are not designed to accurately model modern
computing systems.
  We introduce EasyDRAM, an FPGA-based framework for rapid and accurate
end-to-end evaluation of DRAM techniques on real DRAM chips. EasyDRAM overcomes
the main drawbacks of prior FPGA-based platforms with two key ideas. First,
EasyDRAM removes the need for hardware description language expertise by
enabling developers to implement DRAM techniques using a high-level language
(C++). At runtime, EasyDRAM executes the software-defined memory system design
in a programmable memory controller. Second, EasyDRAM tackles a fundamental
challenge in accurately modeling modern systems: real processors typically
operate at higher clock frequencies than DRAM, a disparity that is difficult to
replicate on FPGA platforms. EasyDRAM addresses this challenge by decoupling
the processor-DRAM interface and advancing the system state using a novel
technique we call time scaling, which faithfully captures the timing behavior
of the modeled system.
  We believe and hope that EasyDRAM will enable innovative ideas in memory
system design to rapidly come to fruition. To aid future research EasyDRAM
implementation is open sourced at https://github.com/CMU-SAFARI/EasyDRAM.

</details>


### [93] [Towards Zero-Stall Matrix Multiplication on Energy-Efficient RISC-V Clusters for Machine Learning Acceleration](https://arxiv.org/abs/2506.10921)
*Luca Colagrande,Lorenzo Leone,Maximilian Coco,Andrei Deaconeasa,Luca Benini*

Main category: cs.AR

TL;DR: 论文摘要介绍了一种优化的微架构，旨在消除控制和内存访问的低效问题，同时保持可编程性，实现了高性能和能效提升。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习计算需求增长，需要设计在效率和灵活性之间取得平衡的加速器。

Method: 提出了零开销循环嵌套和零冲突内存子系统，结合双缓冲感知互联。

Result: 实现了96.1%到99.4%的近理想利用率，性能和能效分别提升11%和8%。

Conclusion: 优化后的架构在保持可编程性的同时，接近专用加速器的性能。

Abstract: The growing computational demands of machine learning (ML) workloads have
driven the design of ML accelerators aiming at an optimal tradeoff between
efficiency and flexibility. A widely explored architecture for flexible ML
accelerators is based on clusters of lightweight instruction processors sharing
multi-banked L1 memory, augmented with specialized instruction extensions for
key ML-related computations, such as matrix multiplication (matmul). However,
instruction extensions should be coupled with microarchitectural optimizations
that remove inefficiencies due to control flow (loop handling) and memory
access, without drastically increasing processor complexity. Moving from a
state-of-the-art (SoA) ML accelerator cluster based on RISC-V processors, we
propose a low-overhead optimized microarchitecture that eliminates these
inefficiencies almost entirely while retaining programmability. We introduce
"zero-overhead loop nests" to remove control overheads, and a "zero-conflict
memory subsystem", leveraging a novel double-buffering-aware interconnect, to
eliminate bank conflicts in L1 memory. With these enhancements, we attain
near-ideal utilizations between 96.1% and 99.4%, achieving 11% performance and
8% energy efficiency improvements over the baseline SoA RISC-V cluster. We
demonstrate comparable utilizations and performance to a specialized SoA
accelerator, with only 12% difference in energy efficiency, while providing a
fully-programmable general-purpose solution supporting a significantly wider
range of workloads.

</details>


### [94] [MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis Inside the Storage Subsystem](https://arxiv.org/abs/2506.10931)
*Melina Soysal,Konstantina Koliogeorgi,Can Firtina,Nika Mansouri Ghiasi,Rakesh Nadig,Haiyu Mayo,Geraldo F. Oliveira,Yu Liang,Klea Zambaku,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.AR

TL;DR: MARS系统通过异构资源优化数据移动和计算，显著提升基因组信号分析性能。


<details>
  <summary>Details</summary>
Motivation: 测序技术快速发展导致基因组分析中I/O成为性能瓶颈，需设计高效系统解决。

Method: 提出MARS系统，结合软硬件协同设计，利用存储内部资源加速分析流程。

Result: MARS性能优于现有方案93倍和40倍，能耗降低427倍和72倍。

Conclusion: MARS以低成本和高效方式解决了基因组分析中的数据移动和计算瓶颈。

Abstract: Raw signal genome analysis (RSGA) has emerged as a promising approach to
enable real-time genome analysis by directly analyzing raw electrical signals.
However, rapid advancements in sequencing technologies make it increasingly
difficult for software-based RSGA to match the throughput of raw signal
generation. This paper demonstrates that while hardware acceleration techniques
can significantly accelerate RSGA, the high volume of genomic data shifts the
performance and energy bottleneck from computation to I/O data movement. As
sequencing throughput increases, I/O overhead becomes the main contributor to
both runtime and energy consumption. Therefore, there is a need to design a
high-performance, energy-efficient system for RSGA that can both alleviate the
data movement bottleneck and provide large acceleration capabilities. We
propose MARS, a storage-centric system that leverages the heterogeneous
resources within modern storage systems (e.g., storage-internal DRAM, storage
controller, flash chips) alongside their large storage capacity to tackle both
data movement and computational overheads of RSGA in an area-efficient and
low-cost manner. MARS accelerates RSGA through a novel hardware/software
co-design approach. First, MARS modifies the RSGA pipeline via two filtering
mechanisms and a quantization scheme, reducing hardware demands and optimizing
for in-storage execution. Second, MARS accelerates the RSGA steps directly
within the storage by leveraging both Processing-Near-Memory and
Processing-Using-Memory paradigms. Third, MARS orchestrates the execution of
all steps to fully exploit in-storage parallelism and minimize data movement.
Our evaluation shows that MARS outperforms basecalling-based software and
hardware-accelerated state-of-the-art read mapping pipelines by 93x and 40x, on
average across different datasets, while reducing their energy consumption by
427x and 72x.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [95] [Receiving RISs: Enabling Channel Estimation and Autonomous Configuration](https://arxiv.org/abs/2506.10662)
*George C. Alexandropoulos,Konstantinos D. Katsanos,Evangelos Vlachos*

Main category: eess.SP

TL;DR: 本文提出了一种半被动RIS硬件架构，通过非正交训练序列和ADMM算法进行信道估计，优化RIS反射系数以提升MIMO系统性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过半被动RIS架构提升MIMO通信系统的性能，特别是在高频段（FR3及以上）的应用场景。

Method: 采用非正交训练序列和ADMM算法进行信道估计，结合RIS的空间吸收采样和MIMO信道的稀疏性与低秩特性。

Result: 仿真结果表明，所提信道估计技术在多种系统和RIS硬件配置参数下优于基准方案，并能有效优化RIS单元的反射相位。

Conclusion: 所提架构和算法在高频通信系统中具有显著性能优势，为RIS的动态优化提供了有效解决方案。

Abstract: This chapter focuses on a hardware architecture for semi-passive
Reconfigurable Intelligent Surfaces (RISs) and investigates its consideration
for boosting the performance of Multiple-Input Multiple-Output (MIMO)
communication systems. The architecture incorporates a single or multiple
radio-frequency chains to receive pilot signals via tunable absorption phase
profiles realized by the metasurface front end, as well as a controller
encompassing a baseband processing unit to carry out channel estimation, and
consequently, the optimization of the RIS reflection coefficients. A novel
channel estimation protocol, according to which the RIS receives non-orthogonal
training pilot sequences from two multi-antenna terminals via tunable
absorption phase profiles, and then, estimates the respective channels via its
signal processing unit, is presented. The channel estimates are particularly
used by the RIS controller to design the capacity-achieving reflection phase
configuration of the metasurface front end. The proposed channel estimation
algorithm, which is based on the Alternating Direction Method of Multipliers
(ADMM), profits from the RIS random spatial absorption sampling to capture the
entire signal space, and exploits the beamspace sparsity and low-rank
properties of extremely large MIMO channels, which is particularly relevant for
communication systems at the FR3 band and above. Our extensive numerical
investigations showcase the superiority of the proposed channel estimation
technique over benchmark schemes for various system and RIS hardware
configuration parameters, as well as the effectiveness of using channel
estimates at the RIS side to dynamically optimize the possibly phase-quantized
reflection coefficients of its unit elements.

</details>


### [96] [Ground Reaction Force Estimation via Time-aware Knowledge Distillation](https://arxiv.org/abs/2506.10265)
*Eun Som Jeon,Sinjini Mitra,Jisoo Lee,Omik M. Save,Ankita Shukla,Hyunglae Lee,Pavan Turaga*

Main category: eess.SP

TL;DR: 提出了一个基于时间感知知识蒸馏的框架，用于从鞋垫传感器数据中估计地面反作用力（GRF），优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 便携式鞋垫传感器测量GRF成本低但准确性不足，需要提高其估计精度以替代昂贵且不便的跑步机测量方法。

Method: 采用时间感知知识蒸馏框架，利用小批量数据中的相似性和时间特征，捕捉特征间的互补关系和数据的序列特性。

Result: 实验结果表明，该框架在GRF估计中性能优于现有方法。

Conclusion: 时间感知知识蒸馏框架为便携式GRF测量提供了一种高精度且实用的解决方案。

Abstract: Human gait analysis with wearable sensors has been widely used in various
applications, such as daily life healthcare, rehabilitation, physical therapy,
and clinical diagnostics and monitoring. In particular, ground reaction force
(GRF) provides critical information about how the body interacts with the
ground during locomotion. Although instrumented treadmills have been widely
used as the gold standard for measuring GRF during walking, their lack of
portability and high cost make them impractical for many applications. As an
alternative, low-cost, portable, wearable insole sensors have been utilized to
measure GRF; however, these sensors are susceptible to noise and disturbance
and are less accurate than treadmill measurements. To address these challenges,
we propose a Time-aware Knowledge Distillation framework for GRF estimation
from insole sensor data. This framework leverages similarity and temporal
features within a mini-batch during the knowledge distillation process,
effectively capturing the complementary relationships between features and the
sequential properties of the target and input data. The performance of the
lightweight models distilled through this framework was evaluated by comparing
GRF estimations from insole sensor data against measurements from an
instrumented treadmill. Empirical results demonstrated that Time-aware
Knowledge Distillation outperforms current baselines in GRF estimation from
wearable sensor data.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [97] [LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation](https://arxiv.org/abs/2506.10235)
*Chen-Chia Chang,Wan-Hsuan Lin,Yikang Shen,Yiran Chen,Xin Zhang*

Main category: cs.LG

TL;DR: 论文提出LaMAGIC2，一种基于语言模型的模拟拓扑生成方法，改进了之前方法的低精度和低效率问题。


<details>
  <summary>Details</summary>
Motivation: 现代应用中模拟拓扑设计的自动化需求迫切，但现有方法在数值输入精度和效率上存在不足。

Method: 采用SFCI（简洁浮点输入规范表示法），通过标识符优化组件类型识别，降低计算复杂度，提高数值精度敏感性。

Result: LaMAGIC2在0.01的严格公差下成功率提高34%，MSE降低10倍，对更多顶点的电路适应性提升58.5%。

Conclusion: LaMAGIC2成为模拟拓扑生成的稳健框架，显著提升了性能和适用性。

Abstract: Automation of analog topology design is crucial due to customized
requirements of modern applications with heavily manual engineering efforts.
The state-of-the-art work applies a sequence-to-sequence approach and
supervised finetuning on language models to generate topologies given user
specifications. However, its circuit formulation is inefficient due to O(|V |2)
token length and suffers from low precision sensitivity to numeric inputs. In
this work, we introduce LaMAGIC2, a succinct float-input canonical formulation
with identifier (SFCI) for language model-based analog topology generation.
SFCI addresses these challenges by improving component-type recognition through
identifier-based representations, reducing token length complexity to O(|V |),
and enhancing numeric precision sensitivity for better performance under tight
tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher
success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a
prior method. LaMAGIC2 also exhibits better transferability for circuits with
more vertices with up to 58.5% improvement. These advancements establish
LaMAGIC2 as a robust framework for analog topology generation.

</details>


### [98] [Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion](https://arxiv.org/abs/2506.09999)
*Yukun Chen,Zihuan Qiu,Fanman Meng,Hongliang Li,Linfeng Xu,Qingbo Wu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多模态预训练模型的多模态类增量学习方法，涵盖了视觉、听觉和文本模态，通过引入增量特征提取器、自适应融合模块和对比训练损失来提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统多模态类增量学习（MCIL）方法仅关注视觉和文本模态的局限性，探索视觉、听觉和文本模态的整合，同时应对互补信息融合和灾难性遗忘的挑战。

Method: 1. 基于Mixture-of-Experts（MoE）结构的多模态增量特征提取器（MIFE）用于增量微调。2. 自适应视听融合模块（AAVFM）增强特征判别性和泛化能力。3. 提出多模态类增量对比训练损失优化跨模态对齐。

Result: 在三个多模态数据集上的大量实验验证了方法的有效性。

Conclusion: 该方法在多模态类增量学习中取得了显著效果，解决了多模态整合和灾难性遗忘的问题。

Abstract: Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that
focus only on vision and text, this paper explores MCIL across vision, audio
and text modalities, addressing challenges in integrating complementary
information and mitigating catastrophic forgetting. To tackle these issues, we
propose an MCIL method based on multimodal pre-trained models. Firstly, a
Multimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts
(MoE) structure is introduced to achieve effective incremental fine-tuning for
AudioCLIP. Secondly, to enhance feature discriminability and generalization, we
propose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking
threshold mechanism and a dynamic feature fusion mechanism, along with a
strategy to enhance text diversity. Thirdly, a novel multimodal
class-incremental contrastive training loss is proposed to optimize cross-modal
alignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced
for comprehensive assessment. Extensive experiments on three multimodal
datasets validate the effectiveness of our method.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [99] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
*Aakriti Kumar,Nalin Poungpeth,Diyi Yang,Erina Farrell,Bruce Lambert,Matthew Groh*

Main category: cs.CL

TL;DR: 该研究比较了专家、众包工作者和LLMs在四种心理学、NLP和交流学评估框架中对共情沟通的标注能力，发现LLMs在共情任务中表现接近专家水平。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在共情沟通评估中的可靠性，尤其在情感敏感应用中的透明度和监督。

Method: 通过比较专家、众包工作者和LLMs在200个真实交流中的共情标注（共9,144个标注），评估它们的评分一致性。

Result: 专家一致性高，LLMs表现接近专家水平，优于众包工作者。

Conclusion: LLMs在特定任务中经过适当验证，可支持情感敏感应用的透明度和监督。

Abstract: Large language models (LLMs) excel at generating empathic responses in
text-based conversations. But, how reliably do they judge the nuances of
empathic communication? We investigate this question by comparing how experts,
crowdworkers, and LLMs annotate empathic communication across four evaluative
frameworks drawn from psychology, natural language processing, and
communications applied to 200 real-world conversations where one speaker shares
a personal problem and the other offers support. Drawing on 3,150 expert
annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess
inter-rater reliability between these three annotator groups. We find that
expert agreement is high but varies across the frameworks' sub-components
depending on their clarity, complexity, and subjectivity. We show that expert
agreement offers a more informative benchmark for contextualizing LLM
performance than standard classification metrics. Across all four frameworks,
LLMs consistently approach this expert level benchmark and exceed the
reliability of crowdworkers. These results demonstrate how LLMs, when validated
on specific tasks with appropriate benchmarks, can support transparency and
oversight in emotionally sensitive applications including their use as
conversational companions.

</details>


### [100] [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
*Yixin Ou,Yujie Luo,Jingsheng Zheng,Lanning Wei,Shuofei Qiao,Jintian Zhang,Da Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: AutoMind是一个自适应、智能的LLM代理框架，通过专家知识库、智能树搜索算法和自适应编码策略提升数据科学自动化效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在数据科学中依赖固定工作流，难以处理复杂任务，需要改进以适应实际需求。

Method: 提出AutoMind框架，包括专家知识库、智能树搜索算法和自适应编码策略。

Result: 在基准测试中表现优于现有方法，具备高效性和鲁棒性。

Conclusion: AutoMind为数据科学自动化提供了有效且稳健的解决方案。

Abstract: Large Language Model (LLM) agents have shown great potential in addressing
real-world data science problems. LLM-driven data science agents promise to
automate the entire machine learning pipeline, yet their real-world
effectiveness remains limited. Existing frameworks depend on rigid, pre-defined
workflows and inflexible coding strategies; consequently, they excel only on
relatively simple, classical problems and fail to capture the empirical
expertise that human practitioners bring to complex, innovative tasks. In this
work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework
that overcomes these deficiencies through three key advances: (1) a curated
expert knowledge base that grounds the agent in domain expert knowledge, (2) an
agentic knowledgeable tree search algorithm that strategically explores
possible solutions, and (3) a self-adaptive coding strategy that dynamically
tailors code generation to task complexity. Evaluations on two automated data
science benchmarks demonstrate that AutoMind delivers superior performance
versus state-of-the-art baselines. Additional analyses confirm favorable
effectiveness, efficiency, and qualitative solution quality, highlighting
AutoMind as an efficient and robust step toward fully automated data science.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [101] [Faster CONGEST Approximation Algorithms for Maximum Weighted Independent Set in Sparse Graphs](https://arxiv.org/abs/2506.10845)
*Salwa Faour,Fabian Kuhn*

Main category: cs.DS

TL;DR: 论文研究了确定性分布式CONGEST算法在树和有界树性图中解决加权最大独立集问题的复杂度，提出了上下界结果，并改进了现有算法。


<details>
  <summary>Details</summary>
Motivation: 最大独立集问题在分布式环境中尤为重要，但现有研究集中在稀疏图上。本文旨在通过确定性算法在树和有界树性图中提供高效的近似解决方案。

Method: 通过理论分析和算法设计，分别在树和有界树性图中提出确定性算法，利用局部舍入框架和改进的近似方法。

Result: 在树中证明了复杂度为Θ(log*(n)/ε)，在有界树性图中提供了两种算法，显著改进了现有结果的近似比和运行时间。

Conclusion: 本文提出的算法在稀疏图中实现了高效近似，为分布式环境中的最大独立集问题提供了新的理论支持。

Abstract: The maximum independent set problem is a classic optimization problem that
has also been studied quite intensively in the distributed setting. While the
problem is hard to approximate in general, there are good approximation
algorithms known for several sparse graph families. In this paper, we consider
deterministic distributed CONGEST algorithms for the weighted version of the
problem in trees and graphs of bounded arboricity.
  For trees, we prove that the task of deterministically computing a
$(1-\epsilon)$-approximate solution to the maximum weight independent set
(MWIS) problem has a tight $\Theta(\log^*(n) / \epsilon)$ complexity. The lower
bound already holds on unweighted oriented paths. On the upper bound side, we
show that the bound can be achieved even in unrooted trees.
  For graphs $G=(V,E)$ of arboricity $\beta>1$, we give two algorithms. If the
sum of all node weights is $w(V)$, we show that for any $\epsilon>0$, an
independent set of weight at least $(1-\epsilon)\cdot \frac{w(V)}{4\beta}$ can
be computed in $O(\log^2(\beta/\epsilon)/\epsilon + \log^* n)$ rounds. This
result is obtained by a direct application of the local rounding framework of
Faour, Ghaffari, Grunau, Kuhn, and Rozho\v{n} [SODA '23]. We further show that
for any $\epsilon>0$, an independent set of weight at least
$(1-\epsilon)\cdot\frac{w(V)}{2\beta+1}$ can be computed in
$O(\log^3(\beta)\cdot\log(1/\epsilon)/\epsilon^2 \cdot\log n)$ rounds. This
improves on a recent result of Gil [OPODIS '23], who showed that a
$1/\lfloor(2+\epsilon)\beta\rfloor$-approximation to the MWIS problem can be
computed in $O(\beta\cdot\log n)$ rounds. As an intermediate step, we design an
algorithm to compute an independent set of total weight at least
$(1-\epsilon)\cdot\sum_{v\in V}\frac{w(v)}{deg(v)+1}$ in time
$O(\log^3(\Delta)\cdot\log(1/\epsilon)/\epsilon + \log^* n)$, where $\Delta$ is
the maximum degree of the graph.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [102] [LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs](https://arxiv.org/abs/2506.10527)
*Yanan Cai,Ahmed Salem,Besmira Nushi,Mark Russinovich*

Main category: cs.AI

TL;DR: LogiPlan是一个新基准，用于评估大语言模型（LLM）在复杂关系结构中的逻辑规划和推理能力。


<details>
  <summary>Details</summary>
Motivation: 逻辑关系推理对于依赖LLM生成和查询关系结构的应用（如网络基础设施、知识库或业务流程模式）至关重要。

Method: LogiPlan通过动态调整任务复杂度（如对象数量、关系深度）来评估模型性能，包含三个任务：计划生成、一致性检测和比较问题。

Result: 评估了多个先进模型，发现性能差异与模型规模和架构相关，复杂逻辑任务仍是挑战。

Conclusion: LogiPlan揭示了LLM在复杂逻辑规划中的局限性，尤其是深度推理任务。

Abstract: We introduce LogiPlan, a novel benchmark designed to evaluate the
capabilities of large language models (LLMs) in logical planning and reasoning
over complex relational structures. Logical relational reasoning is important
for applications that may rely on LLMs to generate and query structured graphs
of relations such as network infrastructure, knowledge bases, or business
process schema. Our framework allows for dynamic variation of task complexity
by controlling the number of objects, relations, and the minimum depth of
relational chains, providing a fine-grained assessment of model performance
across difficulty levels. LogiPlan encompasses three complementary tasks: (1)
Plan Generation, where models must construct valid directed relational graphs
meeting specified structural constraints; (2) Consistency Detection, testing
models' ability to identify inconsistencies in relational structures; and (3)
Comparison Question, evaluating models' capacity to determine the validity of
queried relationships within a given graph. Additionally, we assess models'
self-correction capabilities by prompting them to verify and refine their
initial solutions. We evaluate state-of-the-art models including DeepSeek R1,
Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,
O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant
performance gaps that correlate with model scale and architecture. Our analysis
demonstrates that while recent reasoning-enhanced models show promising results
on simpler instances, they struggle with more complex configurations requiring
deeper logical planning.

</details>


### [103] [Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods](https://arxiv.org/abs/2506.10420)
*Boris Sedlak,Alireza Furutanpey,Zihang Wang,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.AI

TL;DR: 提出了一种基于代理的边缘计算弹性扩展框架，通过动态调整硬件资源和内部配置来优化受限环境下的性能，四种代理均表现出可接受的SLO性能。


<details>
  <summary>Details</summary>
Motivation: 边缘计算因严格的资源限制需要更灵活的弹性扩展行为，以充分利用多维度弹性。

Method: 采用基于代理的框架，动态调整硬件资源和内部服务配置，比较四种代理（Active Inference、Deep Q Network等）。

Result: 所有代理均实现可接受的SLO性能，Deep Q Network受益于预训练，结构分析快速收敛，深度主动推理代理结合理论和实践优势。

Conclusion: 验证了基于代理的多维度弹性扩展在边缘环境中的可行性，鼓励未来进一步研究。

Abstract: Edge computing breaks with traditional autoscaling due to strict resource
constraints, thus, motivating more flexible scaling behaviors using multiple
elasticity dimensions. This work introduces an agent-based autoscaling
framework that dynamically adjusts both hardware resources and internal service
configurations to maximize requirements fulfillment in constrained
environments. We compare four types of scaling agents: Active Inference, Deep Q
Network, Analysis of Structural Knowledge, and Deep Active Inference, using two
real-world processing services running in parallel: YOLOv8 for visual
recognition and OpenCV for QR code detection. Results show all agents achieve
acceptable SLO performance with varying convergence patterns. While the Deep Q
Network benefits from pre-training, the structural analysis converges quickly,
and the deep active inference agent combines theoretical foundations with
practical scalability advantages. Our findings provide evidence for the
viability of multi-dimensional agent-based autoscaling for edge environments
and encourage future work in this research direction.

</details>


### [104] [System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers](https://arxiv.org/abs/2506.10708)
*Michael Bartholomew,Joohyung Lee*

Main category: cs.AI

TL;DR: ASPMT是一种结合逻辑编程与理论求解的方法，本文提出了一个名为aspsmt2smt的编译器，实现ASPMT程序到SMT实例的转换，使用Gringo和Z3工具，支持实数计算。


<details>
  <summary>Details</summary>
Motivation: 将逻辑编程与理论求解结合，处理连续变化的推理问题。

Method: 开发编译器aspsmt2smt，利用Gringo部分接地程序，并使用Z3完成剩余求解。

Result: 系统能有效处理实数计算，适用于连续变化的推理。

Conclusion: ASPMT结合SMT的方法可行且实用，工具链Gringo和Z3支持高效实现。

Abstract: Answer Set Programming Modulo Theories (ASPMT) is an approach to combining
answer set programming and satisfiability modulo theories based on the
functional stable model semantics. It is shown that the tight fragment of ASPMT
programs can be turned into SMT instances, thereby allowing SMT solvers to
compute stable models of ASPMT programs. In this paper we present a compiler
called {\sc aspsmt2smt}, which implements this translation. The system uses ASP
grounder {\sc gringo} and SMT solver {\sc z3}. {\sc gringo} partially grounds
input programs while leaving some variables to be processed by {\sc z3}. We
demonstrate that the system can effectively handle real number computations for
reasoning about continuous changes.

</details>


### [105] [Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning](https://arxiv.org/abs/2506.10585)
*Mohd Anwar Jamal Faiz*

Main category: cs.AI

TL;DR: 提出了一种名为Primender的新整数序列，结合了经典素数和模数条件，用于评估大型语言模型（LLMs）的符号推理能力。


<details>
  <summary>Details</summary>
Motivation: 需要一个可解释的、基于规则的测试集，以评估LLMs推断隐藏规则、验证数学假设和规模化符号逻辑的能力。

Method: 设计了结构化提示和评估框架，测试多个LLMs（如ChatGPT、Copilot等）在识别规则、验证假设和生成序列方面的性能。

Result: 提出了一种新的数学构造和可重复的方法论，用于评估LLMs在符号推理、假设测试和模式泛化方面的表现。

Conclusion: Primender序列和方法论为数论、人工智能和软件工程的交叉领域提供了新的研究工具。

Abstract: This paper introduces the Primender sequence, a novel integer sequence
defined by a hybrid rule that combines classical primality with modular
digit-based conditions. Specifically, a number n is included in the sequence if
it is prime or ends with a prime number of unit digit or any length. In other
words, numbers which are primes or have at least one prime suffix. The
resulting sequence exhibits a deterministic yet non-trivial structure, blending
number-theoretic properties with symbolic patterning. We propose the Primender
sequence as a benchmark for evaluating the symbolic reasoning capabilities of
Large Language Models (LLMs). The study is motivated by the need for
interpretable, rule-based testbeds that can assess an LLM's ability to infer
hidden rules, validate mathematical hypotheses, and generalize symbolic logic
at scale. A key hypothesis explored is: Whenever a number in the Primender
sequence is exactly one more than the largest prime less than or equal to it,
the difference between it and the previous number in the sequence is also 1. We
design a structured prompt and evaluation framework to test this hypothesis
across multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,
Gemini, Grok, and LLaMA. The models are tasked with identifying the underlying
rule, validating the hypothesis, and generating the next 100,000 terms of the
sequence. Comparative metrics such as rule inference accuracy, hypothesis
evaluation, sequence validity, and symbolic explanation quality are used to
assess model performance. This work contributes a novel mathematical construct
and a reproducible methodology for benchmarking LLMs in symbolic reasoning,
hypothesis testing, and scalable pattern generalization - bridging the domains
of number theory, artificial intelligence, and software engineering.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [106] [Learning Chaotic Dynamics with Neuromorphic Network Dynamics](https://arxiv.org/abs/2506.10773)
*Yinhao Xu,Georg A. Gottwald,Zdenka Kuncic*

Main category: cond-mat.dis-nn

TL;DR: 该研究探讨了如何利用神经形态网络（一种动态系统）来学习和建模动态系统，通过优化外部控制参数提高性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用神经形态网络的动态特性，学习复杂的动态系统，尤其是通过外部控制参数优化网络性能。

Method: 使用基于忆阻元件的复杂电路构建神经形态网络，并通过储层计算框架模拟其对多元混沌时间序列的自主预测能力。

Result: 发现输入电压最大化忆阻元件动态范围时，能够优化非线性动态响应；而增加输入电极覆盖会抑制不利于学习的非线性响应。

Conclusion: 研究揭示了如何通过外部控制参数优化神经形态网络的性能，为学习复杂动态系统提供了实用指导。

Abstract: This study investigates how dynamical systems may be learned and modelled
with a neuromorphic network which is itself a dynamical system. The
neuromorphic network used in this study is based on a complex electrical
circuit comprised of memristive elements that produce neuro-synaptic nonlinear
responses to input electrical signals. To determine how computation may be
performed using the physics of the underlying system, the neuromorphic network
was simulated and evaluated on autonomous prediction of a multivariate chaotic
time series, implemented with a reservoir computing framework. Through
manipulating only input electrodes and voltages, optimal nonlinear dynamical
responses were found when input voltages maximise the number of memristive
components whose internal dynamics explore the entire dynamical range of the
memristor model. Increasing the network coverage with the input electrodes was
found to suppress other nonlinear responses that are less conducive to
learning. These results provide valuable insights into how a practical
neuromorphic network device can be optimised for learning complex dynamical
systems using only external control parameters.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [107] [Mind the Gap: Revealing Security Barriers through Situational Awareness of Small and Medium Business Key Decision-Makers](https://arxiv.org/abs/2506.10025)
*Yuanhaur Chang,Oren Heller,Yaniv Shlomo,Iddo Bar-Noy,Ella Bokobza,Michal Grinstein-Weiss,Ning Zhang*

Main category: cs.CR

TL;DR: 论文研究了中小企业决策者在网络安全决策中的认知和行为，通过混合方法揭示了他们的风险感知、防御措施选择动机及安全意识水平，并提出了改进措施。


<details>
  <summary>Details</summary>
Motivation: 中小企业决策者在网络安全措施实施中缺乏意识和知识，研究旨在深入了解其决策过程和改进空间。

Method: 采用混合方法，包括半结构化访谈（21人）和在线调查（322人），结合主题分析和情境感知模型，构建结构方程模型。

Result: 发现了决策者对数字资产的风险感知、防御措施选择的原因及安全认知的影响因素，并识别出安全意识较低的人群。

Conclusion: 提出了干预措施，帮助中小企业克服网络安全挑战，并提出了提高安全意识的途径。

Abstract: Key decision-makers in small and medium businesses (SMBs) often lack the
awareness and knowledge to implement cybersecurity measures effectively. To
gain a deeper understanding of how SMB executives navigate cybersecurity
decision-making, we deployed a mixed-method approach, conducting
semi-structured interviews (n=21) and online surveys (n=322) with SMB key
decision-makers. Using thematic analysis, we revealed SMB decision-makers'
perceived risks in terms of the digital assets they valued, and found reasons
for their choice of defense measures and factors impacting security perception.
We employed the situational awareness model to characterize decision-makers
based on cybersecurity awareness, identifying those who have comparatively low
awareness in the fight against adversaries. We further explored the
relationship between awareness and business attributes, and constructed a
holistic structural equation model to understand how awareness can be improved.
Finally, we proposed interventions to help SMBs overcome potential challenges.

</details>


### [108] [Multiverse Privacy Theory for Contextual Risks in Complex User-AI Interactions](https://arxiv.org/abs/2506.10042)
*Ece Gumusel*

Main category: cs.CR

TL;DR: 论文提出了'多元隐私理论'，通过模拟用户隐私决策产生的平行宇宙来理解隐私问题。


<details>
  <summary>Details</summary>
Motivation: 在AI互动日益增多的时代，用户的隐私决策受复杂和不确定因素影响，需要新的理论框架。

Method: 引入'多元隐私理论'，模拟隐私决策的不同潜在结果，基于上下文完整性、偏好演变和概率决策。

Result: 理论为理解隐私提供了新视角，未来将基于真实场景调查数据进一步应用。

Conclusion: 该框架为隐私研究提供了创新的理论基础，未来可通过实际数据验证其有效性。

Abstract: In an era of increasing interaction with artificial intelligence (AI), users
face evolving privacy decisions shaped by complex, uncertain factors. This
paper introduces Multiverse Privacy Theory, a novel framework in which each
privacy decision spawns a parallel universe, representing a distinct potential
outcome based on user choices over time. By simulating these universes, this
theory provides a foundation for understanding privacy through the lens of
contextual integrity, evolving preferences, and probabilistic decision-making.
Future work will explore its application using real-world, scenario-based
survey data.

</details>


### [109] [Expert-in-the-Loop Systems with Cross-Domain and In-Domain Few-Shot Learning for Software Vulnerability Detection](https://arxiv.org/abs/2506.10104)
*David Farr,Kevin Talty,Alexandra Farr,John Stockdale,Iain Cruickshank,Jevin West*

Main category: cs.CR

TL;DR: 本文研究了利用大型语言模型（LLMs）进行软件漏洞检测的方法，发现通过多示例提示和置信度路由策略可以显著提升分类性能，同时提出了未来需要解决的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，快速准确的漏洞检测对系统安全至关重要。本文旨在探索LLMs在漏洞评估中的潜力。

Method: 通过比较零示例、跨领域多示例和领域内多示例提示策略，模拟识别Python代码中的已知漏洞。

Result: 研究表明，多示例提示显著提升了性能，结合置信度路由策略可优化自动化与专家监督的平衡。LLMs能够通过少量示例泛化漏洞类别。

Conclusion: LLMs在模拟环境中展现出作为可扩展安全工具的潜力，但模型可靠性、可解释性和对抗鲁棒性仍需进一步研究。AI与专家协作的路径为提高网络安全效率提供了方向。

Abstract: As cyber threats become more sophisticated, rapid and accurate vulnerability
detection is essential for maintaining secure systems. This study explores the
use of Large Language Models (LLMs) in software vulnerability assessment by
simulating the identification of Python code with known Common Weakness
Enumerations (CWEs), comparing zero-shot, few-shot cross-domain, and few-shot
in-domain prompting strategies. Our results indicate that while zero-shot
prompting performs poorly, few-shot prompting significantly enhances
classification performance, particularly when integrated with confidence-based
routing strategies that improve efficiency by directing human experts to cases
where model uncertainty is high, optimizing the balance between automation and
expert oversight. We find that LLMs can effectively generalize across
vulnerability categories with minimal examples, suggesting their potential as
scalable, adaptable cybersecurity tools in simulated environments. However,
challenges such as model reliability, interpretability, and adversarial
robustness remain critical areas for future research. By integrating AI-driven
approaches with expert-in-the-loop (EITL) decision-making, this work highlights
a pathway toward more efficient and responsive cybersecurity workflows. Our
findings provide a foundation for deploying AI-assisted vulnerability detection
systems in both real and simulated environments that enhance operational
resilience while reducing the burden on human analysts.

</details>


### [110] [D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning](https://arxiv.org/abs/2506.10125)
*Muqi Zou,Hongyu Cai,Hongwei Wu,Zion Leonahenahe Basque,Arslan Khan,Berkay Celik,Dave,Tian,Antonio Bianchi,Ruoyu,Wang,Dongyan Xu*

Main category: cs.CR

TL;DR: D-LiFT利用强化学习训练LLMs改进反编译代码质量，提出D-SCORE评估系统，确保准确性同时提升可读性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs改进反编译代码的方法存在引入新错误和验证不靠谱的问题，D-LiFT旨在解决这些局限。

Method: 结合强化学习训练LLMs，开发D-SCORE系统，先验证准确性再评估可读性，并将评分反馈给LLMs进行微调。

Result: D-LiFT在coreutils和util-linux项目上显著提升反编译代码质量，比基线LLMs多改进55.3%的函数。

Conclusion: D-LiFT通过D-SCORE驱动的微调，有效提升反编译代码的准确性和可读性。

Abstract: Decompilers, which reconstruct human-readable source code from binary
executables, are vital to many security tasks. Yet, despite recent advances,
their output often suffers from syntactic and semantic errors and remains
difficult to read. Recently, with the advent of large language models (LLMs),
researchers began to explore the potential of LLMs to refine decompiler output.
Nevertheless, our study of these approaches reveals significant limitations,
such as introducing new errors and relying on unreliable accuracy validation.
In this paper, we present D-LiFT, an automated decompiler backend that
harnesses and further trains LLMs to improve the quality of decompiled code via
reinforcement learning (RL). Unlike prior work that overlooks preserving
accuracy, D-LiFT adheres to a key principle for enhancing the quality of
decompiled code: \textit{preserving accuracy while improving readability}.
Central to D-LiFT, we propose D-SCORE, an integrated quality assessment system
to score the decompiled code from multiple aspects. In line with our principle,
D-SCORE assigns low scores to any inaccurate output and only awards higher
scores for readability to code that passes the accuracy check. Specifically,
D-SCORE first verifies the syntactic and semantic correctness via the compiler
and symbolic execution; only if a candidate is deemed accurate, it then
evaluates readability using established metrics to compare the LLM output with
the original decompiled code. The score will then be fed back to the LLM for
fine-tuning. Our implementation, based on Ghidra and a range of LLMs,
demonstrates significant improvements for the accurate decompiled code from the
coreutils and util-linux projects. Compared to baseline LLMs without
D-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled
functions, as measured by D-SCORE.

</details>


### [111] [ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space](https://arxiv.org/abs/2506.10323)
*Chuyang Chen,Brendan Dolan-Gavitt,Zhiqiang Lin*

Main category: cs.CR

TL;DR: ELFuzz是一种通过LLM驱动合成的自动生成模糊测试工具的新方法，无需手动构建输入规范，能在真实规模系统上高效工作。


<details>
  <summary>Details</summary>
Motivation: 传统基于生成的模糊测试需要手动构造输入规范和语义约束，耗费大量人工。

Method: ELFuzz从最小种子模糊器出发，利用LLM驱动的进化和覆盖率指导，自动合成针对测试系统的定制模糊测试工具。

Result: ELFuzz在覆盖率和触发漏洞方面显著优于人工和现有方法，并在真实系统中发现了多个0-day漏洞。

Conclusion: ELFuzz展示了在模糊测试中实现更自动化、高效和可扩展输入生成的潜力。

Abstract: Generation-based fuzzing produces appropriate testing cases according to
specifications of input grammars and semantic constraints to test systems and
software. However, these specifications require significant manual efforts to
construct. This paper proposes a new approach, ELFuzz (Evolution Through Large
Language Models for Fuzzing), that automatically synthesizes generation-based
fuzzers tailored to a system under test (SUT) via LLM-driven synthesis over
fuzzer space. At a high level, it starts with minimal seed fuzzers and propels
the synthesis by fully automated LLM-driven evolution with coverage guidance.
Compared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of
real-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)
synthesize efficient fuzzers that catch interesting grammatical structures and
semantic constraints in a human-understandable way. Our evaluation compared
ELFuzz with specifications manually written by domain experts and synthesized
by state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more
coverage and triggers up to 174.0% more artificially injected bugs. We also
used ELFuzz to conduct a real-world fuzzing campaign on the newest version of
cvc5 for 14 days, and encouragingly, it found five 0-day bugs (three are
exploitable). Moreover, we conducted an ablation study, which shows that the
fuzzer space model, the key component of ELFuzz, contributes the most (up to
62.5%) to the effectiveness of ELFuzz. Further analysis of the fuzzers
synthesized by ELFuzz confirms that they catch interesting grammatical
structures and semantic constraints in a human-understandable way. The results
present the promising potential of ELFuzz for more automated, efficient, and
extensible input generation for fuzzing.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [112] [Collective Bargaining in the Information Economy Can Address AI-Driven Power Concentration](https://arxiv.org/abs/2506.10272)
*Nicholas Vincent,Matthew Prewitt,Hanlin Li*

Main category: cs.CY

TL;DR: 本文主张通过集体谈判重构AI信息市场，以避免信息市场失灵和资本集中，确保信息生产者可持续回报。


<details>
  <summary>Details</summary>
Motivation: AI系统依赖的信息市场存在不公平现象，信息生产者（如记者、研究人员）需要集体谈判以获取合理回报，避免资本集中和信息公地生态崩溃。

Method: 提出通过技术机制（如联合数据管理工具、可解释的数据价值评估）和政策干预（如支持数据中介组织）促进集体谈判。

Result: 集体谈判能为信息经济创造市场摩擦和激励，推动可持续的AI发展。

Conclusion: 集体谈判和协调是解决信息市场问题的关键，需技术与政策共同支持。

Abstract: This position paper argues that there is an urgent need to restructure
markets for the information that goes into AI systems. Specifically, producers
of information goods (such as journalists, researchers, and creative
professionals) need to be able to collectively bargain with AI product builders
in order to receive reasonable terms and a sustainable return on the
informational value they contribute. We argue that without increased market
coordination or collective bargaining on the side of these primary information
producers, AI will exacerbate a large-scale "information market failure" that
will lead not only to undesirable concentration of capital, but also to a
potential "ecological collapse" in the informational commons. On the other
hand, collective bargaining in the information economy can create market
frictions and aligned incentives necessary for a pro-social, sustainable AI
future. We provide concrete actions that can be taken to support a
coalition-based approach to achieve this goal. For example, researchers and
developers can establish technical mechanisms such as federated data management
tools and explainable data value estimations, to inform and facilitate
collective bargaining in the information economy. Additionally, regulatory and
policy interventions may be introduced to support trusted data intermediary
organizations representing guilds or syndicates of information producers.

</details>


### [113] [The Urban Model Platform: A Public Backbone for Modeling and Simulation in Urban Digital Twins](https://arxiv.org/abs/2506.10964)
*Rico H Herzog,Till Degkwitz,Trivik Verma*

Main category: cs.CY

TL;DR: 城市数字孪生用于整合城市数字资源，支持可持续规划，但模型集成复杂。开源的Urban Model Platform可作为技术与社会协作框架。


<details>
  <summary>Details</summary>
Motivation: 解决城市数字孪生中模型集成的复杂性问题，促进多元化和开放的协作建模。

Method: 采用参与式设计方法，开发开放的Urban Model Platform，支持分散模型集成和多模型协作。

Result: 平台成功作为技术和社会协作框架，支持多元城市模型表示。

Conclusion: 开放平台和多元模型协作是解决城市数字孪生复杂性的有效途径。

Abstract: Urban digital twins are increasingly perceived as a way to pool the growing
digital resources of cities for the purpose of a more sustainable and
integrated urban planning. Models and simulations are central to this
undertaking: They enable "what if?" scenarios, create insights and describe
relationships between the vast data that is being collected. However, the
process of integrating and subsequently using models in urban digital twins is
an inherently complex undertaking. It raises questions about how to represent
urban complexity, how to deal with uncertain assUrban Model Platformtions and
modeling paradigms, and how to capture underlying power relations. Existent
approaches in the domain largely focus on monolithic and centralized solutions
in the tradition of neoliberal city-making, oftentimes prohibiting pluralistic
and open interoperable models. Using a participatory design for participatory
systems approach together with the City of Hamburg, Germany, we find that an
open Urban Model Platform can function both as a public technological backbone
for modeling and simulation in urban digital twins and as a socio-technical
framework for a collaborative and pluralistic representation of urban
processes. Such a platform builds on open standards, allows for a decentralized
integration of models, enables communication between models and supports a
multi-model approach to representing urban systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [114] [The Gittins Index: A Design Principle for Decision-Making Under Uncertainty](https://arxiv.org/abs/2506.10872)
*Ziv Scully,Alexander Terenin*

Main category: math.OC

TL;DR: 本教程旨在展示Gittins索引在实际问题中的应用，不仅限于理论意义，而是作为一种实用工具。通过实例驱动的介绍，探讨了其在不同问题中的表现，包括最优和次优但高效的解决案例。


<details>
  <summary>Details</summary>
Motivation: 文章的目的是突破Gittins索引在理论上的局限性，展示其在实际问题中的实用价值，特别是在贝叶斯优化和队列尾延迟最小化等领域的应用。

Method: 通过一系列实例介绍Gittins索引的应用，包括其在最优和次优情况下的表现，重点探讨了贝叶斯优化和队列尾延迟最小化等实际问题。

Result: Gittins索引不仅在某些问题上能实现最优解，还能在次优情况下表现出色，展示了其作为实用工具的潜力。

Conclusion: 尽管Gittins索引的定义复杂且应用范围受限，但它在实际问题中的应用显示出其不仅具有理论意义，还能作为有效的决策工具。

Abstract: The Gittins index is a tool that optimally solves a variety of
decision-making problems involving uncertainty, including multi-armed bandit
problems, minimizing mean latency in queues, and search problems like the
Pandora's box model. However, despite the above examples and later extensions
thereof, the space of problems that the Gittins index can solve perfectly
optimally is limited, and its definition is rather subtle compared to those of
other multi-armed bandit algorithms. As a result, the Gittins index is often
regarded as being primarily a concept of theoretical importance, rather than a
practical tool for solving decision-making problems.
  The aim of this tutorial is to demonstrate that the Gittins index can be
fruitfully applied to practical problems. We start by giving an example-driven
introduction to the Gittins index, then walk through several examples of
problems it solves - some optimally, some suboptimally but still with excellent
performance. Two practical highlights in the latter category are applying the
Gittins index to Bayesian optimization, and applying the Gittins index to
minimizing tail latency in queues.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [115] [A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild](https://arxiv.org/abs/2506.10117)
*Klim Kireev,Ana-Maria Creţu,Raphael Meier,Sarah Adel Bargal,Elissa Redmiles,Carmela Troncoso*

Main category: cs.CV

TL;DR: 该论文介绍了Image-Caption Children in the Wild Dataset（ICCWD），一个用于评估未成年人检测工具的多模态数据集，填补了当前缺乏此类数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 由于数字平台上未成年人内容的特殊性，需要有效的自动化工具进行检测，但目前缺乏多模态环境下的基准数据集。

Method: 研究人员构建了包含10,000张图像-标题对的ICCWD数据集，并通过人工标注标明图像中是否包含未成年人。

Result: 通过测试三种检测器，包括商业年龄估计系统，最佳方法的真正例率为75.3%，显示出未成年人检测的挑战性。

Conclusion: ICCWD数据集的发布有望支持更有效的未成年人检测方法的开发，适用于多种场景。

Abstract: Platforms and the law regulate digital content depicting minors (defined as
individuals under 18 years of age) differently from other types of content.
Given the sheer amount of content that needs to be assessed, machine
learning-based automation tools are commonly used to detect content depicting
minors. To our knowledge, no dataset or benchmark currently exists for
detecting these identification methods in a multi-modal environment. To fill
this gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an
image-caption dataset aimed at benchmarking tools that detect depictions of
minors. Our dataset is richer than previous child image datasets, containing
images of children in a variety of contexts, including fictional depictions and
partially visible bodies. ICCWD contains 10,000 image-caption pairs manually
labeled to indicate the presence or absence of a child in the image. To
demonstrate the possible utility of our dataset, we use it to benchmark three
different detectors, including a commercial age estimation system applied to
images. Our results suggest that child detection is a challenging task, with
the best method achieving a 75.3% true positive rate. We hope the release of
our dataset will aid in the design of better minor detection methods in a wide
range of scenarios.

</details>


### [116] [From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations](https://arxiv.org/abs/2506.10559)
*Yutong Zhou,Masahiro Ryo*

Main category: cs.CV

TL;DR: 提出了一种端到端的视觉到因果框架，将物种图像转化为可解释的栖息地偏好因果见解，结合了物种识别、全球分布检索和因果推断方法。


<details>
  <summary>Details</summary>
Motivation: 理解物种为何生活在特定地点对生态研究和保护生物多样性至关重要，但现有生态工作流程碎片化且难以被非专业人士使用。

Method: 整合物种识别、全球分布检索、伪缺失采样和气候数据提取，使用因果推断方法发现环境特征的因果结构并评估其对物种分布的影响。

Result: 通过蜜蜂和花卉物种的案例展示了框架的潜力，生成了统计基础和人类可读的因果解释。

Conclusion: 该框架展示了多模态AI助手在结合生态建模实践后，能够以人类可理解的语言描述物种栖息地的潜力。

Abstract: Explaining why the species lives at a particular location is important for
understanding ecological systems and conserving biodiversity. However, existing
ecological workflows are fragmented and often inaccessible to non-specialists.
We propose an end-to-end visual-to-causal framework that transforms a species
image into interpretable causal insights about its habitat preference. The
system integrates species recognition, global occurrence retrieval,
pseudo-absence sampling, and climate data extraction. We then discover causal
structures among environmental features and estimate their influence on species
occurrence using modern causal inference methods. Finally, we generate
statistically grounded, human-readable causal explanations from structured
templates and large language models. We demonstrate the framework on a bee and
a flower species and report early results as part of an ongoing project,
showing the potential of the multimodal AI assistant backed up by a recommended
ecological modeling practice for describing species habitat in
human-understandable language.

</details>


### [117] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

TL;DR: 该研究提出了一种利用生成式AI技术（如Stable Diffusion和GPT-2）从文本自动合成60秒电影视频的方法，结合音频视频同步及后处理，实现高质量输出。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI技术的进步，实现从文本输入自动生成高质量电影视频，扩展多媒体创作的可能性。

Method: 结合Stable Diffusion（图像合成）、GPT-2（叙事结构）、gTTS和YouTube音乐（音频），并采用五场景框架、帧插值和后处理技术。

Result: 实验表明，该方法在视觉质量、叙事连贯性和效率方面表现出色，适用于创意、教育和工业应用。

Conclusion: 研究证明了文本到视频合成的可行性，为多领域应用提供了高效工具。

Abstract: Advances in generative artificial intelligence have altered multimedia
creation, allowing for automatic cinematic video synthesis from text inputs.
This work describes a method for creating 60-second cinematic movies
incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for
narrative structuring, and a hybrid audio pipeline using gTTS and
YouTube-sourced music. It uses a five-scene framework, which is augmented by
linear frame interpolation, cinematic post-processing (e.g., sharpening), and
audio-video synchronization to provide professional-quality results. It was
created in a GPU-accelerated Google Colab environment using Python 3.11. It has
a dual-mode Gradio interface (Simple and Advanced), which supports resolutions
of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA
memory management and error handling ensure reliability. The experiments
demonstrate outstanding visual quality, narrative coherence, and efficiency,
furthering text-to-video synthesis for creative, educational, and industrial
applications.

</details>


### [118] [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/abs/2506.10452)
*Guowei Zhong,Ruohong Huan,Mingzhen Wu,Ronghua Liang,Peng Chen*

Main category: cs.CV

TL;DR: 提出了一种新型稳健的多模态情感识别框架CIDer，结合MSSD和MACI模块，解决了模态缺失和OOD数据问题，并通过实验验证了其高效性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态情感识别方法难以同时处理模态缺失和OOD数据，且存在参数过多或依赖特定模型的局限性。

Method: CIDer框架包含Model-Specific Self-Distillation (MSSD)模块和Model-Agnostic Causal Inference (MACI)模块。MSSD通过权重共享自蒸馏提升RMFM任务的鲁棒性，MACI通过因果图减少标签和语言偏见，并引入多模态因果模块和反事实文本。

Result: 实验表明，CIDer在RMFM和OOD场景下均表现出色，参数更少，训练更快，优于现有方法。

Conclusion: CIDer是一种高效的稳健多模态情感识别框架，解决了模态缺失和OOD数据的挑战，代码已开源。

Abstract: Recent advancements in Multimodal Emotion Recognition (MER) face challenges
in addressing both modality missing and Out-Of-Distribution (OOD) data
simultaneously. Existing methods often rely on specific models or introduce
excessive parameters, which limits their practicality. To address these issues,
we propose a novel robust MER framework, Causal Inference Distiller (CIDer),
and introduce a new task, Random Modality Feature Missing (RMFM), to generalize
the definition of modality missing. CIDer integrates two key components: a
Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal
Inference (MACI) module. MSSD enhances robustness under the RMFM task through a
weight-sharing self-distillation approach applied across low-level features,
attention maps, and high-level representations. Additionally, a Word-level
Self-aligned Attention Module (WSAM) reduces computational complexity, while a
Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.
To tackle OOD challenges, MACI employs a tailored causal graph to mitigate
label and language biases using a Multimodal Causal Module (MCM) and
fine-grained counterfactual texts. Notably, MACI can independently enhance OOD
generalization with minimal additional parameters. Furthermore, we also
introduce the new repartitioned MER OOD datasets. Experimental results
demonstrate that CIDer achieves robust performance in both RMFM and OOD
scenarios, with fewer parameters and faster training compared to
state-of-the-art methods. The implementation of this work is publicly
accessible at https://github.com/gw-zhong/CIDer.

</details>


### [119] [DanceChat: Large Language Model-Guided Music-to-Dance Generation](https://arxiv.org/abs/2506.10574)
*Qing Wang,Xiaohang Yang,Yilan Dong,Naveen Raj Govindaraj,Gregory Slabaugh,Shanxin Yuan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的音乐到舞蹈生成方法DanceChat，通过文本动作指令生成多样且与音乐风格一致的舞蹈动作。


<details>
  <summary>Details</summary>
Motivation: 音乐与舞蹈之间存在语义鸿沟，且音乐到舞蹈的映射是一对多的，需要额外指导来生成多样化的舞蹈动作。

Method: DanceChat由三部分组成：LLM生成伪指令、多模态特征提取与融合模块、基于扩散的动作合成模块。

Result: 在AIST++数据集和人类评估中，DanceChat在质量和多样性上均优于现有方法。

Conclusion: LLM的引入有效解决了音乐到舞蹈生成的多样性与一致性难题。

Abstract: Music-to-dance generation aims to synthesize human dance motion conditioned
on musical input. Despite recent progress, significant challenges remain due to
the semantic gap between music and dance motion, as music offers only abstract
cues, such as melody, groove, and emotion, without explicitly specifying the
physical movements. Moreover, a single piece of music can produce multiple
plausible dance interpretations. This one-to-many mapping demands additional
guidance, as music alone provides limited information for generating diverse
dance movements. The challenge is further amplified by the scarcity of paired
music and dance data, which restricts the model\^a\u{A}\'Zs ability to learn
diverse dance patterns. In this paper, we introduce DanceChat, a Large Language
Model (LLM)-guided music-to-dance generation approach. We use an LLM as a
choreographer that provides textual motion instructions, offering explicit,
high-level guidance for dance generation. This approach goes beyond implicit
learning from music alone, enabling the model to generate dance that is both
more diverse and better aligned with musical styles. Our approach consists of
three components: (1) an LLM-based pseudo instruction generation module that
produces textual dance guidance based on music style and structure, (2) a
multi-modal feature extraction and fusion module that integrates music, rhythm,
and textual guidance into a shared representation, and (3) a diffusion-based
motion synthesis module together with a multi-modal alignment loss, which
ensures that the generated dance is aligned with both musical and textual cues.
Extensive experiments on AIST++ and human evaluations show that DanceChat
outperforms state-of-the-art methods both qualitatively and quantitatively.

</details>


### [120] [VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos](https://arxiv.org/abs/2506.10857)
*Jiashuo Yu,Yue Wu,Meng Chu,Zhifei Ren,Zizheng Huang,Pei Chu,Ruijie Zhang,Yinan He,Qirui Li,Songze Li,Zhenxiang Li,Zhongying Tu,Conghui He,Yu Qiao,Yali Wang,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: VRBench是一个用于评估大型模型多步推理能力的长叙事视频基准，填补了现有评估中忽略时间推理和程序有效性的空白。


<details>
  <summary>Details</summary>
Motivation: 解决现有评估方法在时间推理和程序有效性方面的不足，提升多步推理能力的评测标准。

Method: 通过多阶段筛选流程（包括专家评审）构建包含1,010个长视频的数据集，并提供人类标注的多步问答对和推理步骤；设计了人机协作框架生成连贯推理链，并采用多阶段评估流程。

Result: 在VRBench上对12个LLM和16个VLM进行了广泛评估，提供了对多步推理能力的深入分析。

Conclusion: VRBench为多步推理领域提供了先进的评估工具，填补了现有评测的空白。

Abstract: We present VRBench, the first long narrative video benchmark crafted for
evaluating large models' multi-step reasoning capabilities, addressing
limitations in existing evaluations that overlook temporal reasoning and
procedural validity. It comprises 1,010 long videos (with an average duration
of 1.6 hours), along with 9,468 human-labeled multi-step question-answering
pairs and 30,292 reasoning steps with timestamps. These videos are curated via
a multi-stage filtering process including expert inter-rater reviewing to
prioritize plot coherence. We develop a human-AI collaborative framework that
generates coherent reasoning chains, each requiring multiple temporally
grounded steps, spanning seven types (e.g., event attribution, implicit
inference). VRBench designs a multi-phase evaluation pipeline that assesses
models at both the outcome and process levels. Apart from the MCQs for the
final results, we propose a progress-level LLM-guided scoring metric to
evaluate the quality of the reasoning chain from multiple dimensions
comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on
VRBench, we undertake a thorough analysis and provide valuable insights that
advance the field of multi-step reasoning.

</details>


### [121] [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
*Leigang Qu,Feng Cheng,Ziyan Yang,Qi Zhao,Shanchuan Lin,Yichun Shi,Yicong Li,Wenjie Wang,Tat-Seng Chua,Lu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种基于视频学习的上下文图像编辑模型，通过多模态序列标注和块因果扩散变换器，实现了高效的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以直接从视频中学习上下文图像编辑模型，避免依赖任务特定的专家模型和流水线。

Method: 设计了一种块因果扩散变换器，通过三种代理任务（下一图像预测、当前分割预测、下一分割预测）从视频中学习。

Result: 模型在多种上下文图像编辑任务中表现优异，并在两个多轮图像编辑基准上达到最优性能。

Conclusion: 该模型展现了强大的上下文图像编辑能力，并在多概念组合、故事生成和链式编辑应用中显示出潜力。

Abstract: In-context image editing aims to modify images based on a contextual sequence
comprising text and previously generated images. Existing methods typically
depend on task-specific pipelines and expert models (e.g., segmentation and
inpainting) to curate training data. In this work, we explore whether an
in-context image editing model can be learned directly from videos. We
introduce a scalable approach to annotate videos as interleaved multimodal
sequences. To effectively learn from this data, we design a block-causal
diffusion transformer trained on three proxy tasks: next-image prediction,
current segmentation prediction, and next-segmentation prediction.
Additionally, we propose a novel multi-turn image editing benchmark to advance
research in this area. Extensive experiments demonstrate that our model
exhibits strong in-context image editing capabilities and achieves
state-of-the-art results on two multi-turn image editing benchmarks. Despite
being trained exclusively on videos, our model also shows promising abilities
in multi-concept composition, story generation, and chain-of-editing
applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [122] [FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio Classification](https://arxiv.org/abs/2506.10207)
*Jun Bai,Rajib Rana,Di Wu,Youyang Qu,Xiaohui Tao,Ji Zhang*

Main category: cs.SD

TL;DR: FedMLAC是一个统一的联邦音频分类框架，通过双向知识蒸馏和层剪枝聚合策略解决数据异构、模型异构和数据中毒问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦音频分类中数据异构、模型异构和数据中毒这三项关键挑战，提供一个统一的解决方案。

Method: 采用双模型架构（个性化本地模型和轻量级全局插件模型），结合双向知识蒸馏和层剪枝聚合策略。

Result: 在四个音频分类基准测试中，FedMLAC在分类准确性和抗噪性方面均优于现有方法。

Conclusion: FedMLAC为联邦音频分类提供了一个高效且鲁棒的解决方案。

Abstract: Federated Learning (FL) provides a privacy-preserving paradigm for training
audio classification (AC) models across distributed clients without sharing raw
data. However, Federated Audio Classification (FedAC) faces three critical
challenges that substantially hinder performance: data heterogeneity, model
heterogeneity, and data poisoning. While prior works have attempted to address
these issues, they are typically treated independently, lacking a unified and
robust solution suited to real-world federated audio scenarios. To bridge this
gap, we propose FedMLAC, a unified mutual learning framework designed to
simultaneously tackle these challenges in FedAC. Specifically, FedMLAC
introduces a dual-model architecture on each client, comprising a personalized
local AC model and a lightweight, globally shared Plug-in model. Through
bidirectional knowledge distillation, the Plug-in model enables global
knowledge transfer while adapting to client-specific data distributions, thus
supporting both generalization and personalization. To further enhance
robustness against corrupted audio data, we develop a Layer-wise Pruning
Aggregation (LPA) strategy that filters unreliable Plug-in model updates based
on parameter deviations during server-side aggregation. Extensive experiments
on four diverse audio classification benchmarks, spanning both speech and
non-speech tasks, demonstrate that FedMLAC consistently outperforms existing
state-of-the-art methods in terms of classification accuracy and robustness to
noisy data.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [123] [Quantum resources in resource management systems](https://arxiv.org/abs/2506.10052)
*Iskandar Sitdikov,M. Emre Sahin,Utz Bacher,Aleksander Wennersteen,Andrew Damin,Mark Birmingham,Philippa Rubin,Stefano Mensa,Matthieu Moreau,Aurelien Nober,Hitomi Takahashi,Munetaka Ohtani*

Main category: quant-ph

TL;DR: 论文探讨了如何将量子计算机集成到高性能计算（HPC）环境中，提出了一种基于Slurm的插件架构设计，支持统一调度和混合量子-经典应用。


<details>
  <summary>Details</summary>
Motivation: 量子计算机可以补充经典计算资源，但需要与现有HPC基础设施集成以实现统一调度和用户友好性。

Method: 设计并实现了一套基于Slurm的插件，支持本地和云量子计算资源的集成和管理。

Result: 提出了接口设计、插件概念及实现方案，并探讨了异构计算集群的操作考虑。

Conclusion: 该研究为量子计算资源纳入HPC提供了可行方案，适用于多种资源管理系统。

Abstract: Quantum computers are beginning to operate in high-performance computing
(HPC) environments. Quantum can complement classical resources for specific
workloads, but their adoption depends on integration into existing HPC
infrastructure. Treating quantum devices as first-class resources allows for
unified scheduling, improved usability, and support for hybrid
quantum-classical applications. This paper presents the design architecture and
reference implementation for quantum resources control using existing workload
management systems. We introduce a suite of plugins for Slurm that enable
integration of on-prem and cloud quantum computing resources into existing
high-performance computing centers. The paper details the interface design,
plugin concept and implementation, operational aspects for heterogeneous
compute clusters, as well as considerations for other resource management
systems.

</details>


### [124] [Synchronization for Fault-Tolerant Quantum Computers](https://arxiv.org/abs/2506.10258)
*Satvik Maurya,Swamit Tannu*

Main category: quant-ph

TL;DR: 论文研究了量子纠错码中的逻辑量子比特同步问题，提出了三种同步策略（被动、主动和混合），有效降低了逻辑错误率并提升了解码速度。


<details>
  <summary>Details</summary>
Motivation: 由于逻辑量子比特在表面码中的不同步问题会导致纠错效率下降，作者希望通过优化同步策略来提高量子计算系统的可靠性和性能。

Method: 定义了三种同步策略：被动策略（简单等待同步）、主动策略（逐步减速以减少错误累积）和混合策略（结合主动策略和额外纠错轮次）。

Result: 主动策略将逻辑错误率降低2.4倍，混合策略进一步降低3.4倍，同时解码延迟提升2.2倍。

Conclusion: 提出的同步策略显著优化了逻辑量子比特的错误率和系统性能，为容错量子计算提供了有效解决方案。

Abstract: Quantum Error Correction (QEC) codes store information reliably in logical
qubits by encoding them in a larger number of less reliable qubits. The surface
code, known for its high resilience to physical errors, is a leading candidate
for fault-tolerant quantum computing (FTQC). Logical qubits encoded with the
surface code can be in different phases of their syndrome generation cycle,
thereby introducing desynchronization in the system. This can occur due to the
production of non-Clifford states, dropouts due to fabrication defects, and the
use of other QEC codes with the surface code to reduce resource requirements.
Logical operations require the syndrome generation cycles of the logical qubits
involved to be synchronized. This requires the leading qubit to pause or slow
down its cycle, allowing more errors to accumulate before the next cycle,
thereby increasing the risk of uncorrectable errors.
  To synchronize the syndrome generation cycles of logical qubits, we define
three policies - Passive, Active, and Hybrid. The Passive policy is the
baseline, and the simplest, wherein the leading logical qubits idle until they
are synchronized with the remaining logical qubits. On the other hand, the
Active policy aims to slow the leading logical qubits down gradually, by
inserting short idle periods before multiple code cycles. This approach reduces
the logical error rate (LER) by up to 2.4x compared to the Passive policy. The
Hybrid policy further reduces the LER by up to 3.4x by reducing the
synchronization slack and running a few additional rounds of error correction.
Furthermore, the reduction in the logical error rate with the proposed
synchronization policies enables a speedup in decoding latency of up to 2.2x
with a circuit-level noise model.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [125] [The Iris File Extension](https://arxiv.org/abs/2506.10009)
*Ryan Erik Landvater,Michael David Olp,Mustafa Yousif,Ulysses Balis*

Main category: eess.IV

TL;DR: 本文介绍了一种名为Iris的新型数字化病理学二进制幻灯片格式，旨在解决现有DICOM标准在实时传输和显示效率上的不足，并提供了高性能的中间文件格式。


<details>
  <summary>Details</summary>
Motivation: 随着数字化病理学的普及，亟需一种高性能的中间文件格式，以解决DICOM标准在图像尺寸限制和检索速度上的不足。

Method: 开发了Iris文件扩展名，这是一种专为全幻灯片图像系统设计的二进制文件容器规范，支持现代压缩、动态结构、文件验证和注释功能。

Result: Iris提供了高效的实时传输和显示性能，并提供了多语言支持的源代码和二进制构建，便于集成到现有解决方案中。

Conclusion: Iris是一种创新的数字化病理学文件格式，填补了现有标准的不足，并以开源方式提供给社区。

Abstract: A modern digital pathology vendor-agnostic binary slide format specifically
targeting the unmet need of efficient real-time transfer and display has not
yet been established. Growing adoption of digital pathology only intensifies
the need for an intermediary digital slide format with an emphasis on
performance for use between slide servers and image management software or for
inter-institutional transmission of cases. Although the DICOM standard is a
well-established format widely used for long-term storage of both images and
critically associated metadata, its inherent limitations on maximum image
dimensions can impact retrieval speed, particularly when accessing whole slide
images using a pyramidal structure of slide viewer applications. Here, we
introduce the Iris file extension, a binary file container specification
explicitly designed for whole slide image systems that can abstract the file
structure outline into memory for immediate tile access. The Iris file
extension adds modern compression support, a dynamic structure with optional
file features, computationally trivial deep file validation and corruption
recovery capabilities, and slide annotation support. In addition to the file
specification document, we provide source code to allow for (de)serialization
and validation of a binary stream against the standard and corresponding binary
builds with C++, Python, and JavaScript language bindings. We further provide
full encoder and decoder implementation source code, as well as binary builds
(as part of the separate Iris Codec Community module) with language bindings
for C++ and Python to allow for easy integration with existing WSI solutions.
We provide the Iris File Extension specification openly to the community in the
form of a Creative Commons Attribution-No Derivative 4.0 international license.

</details>
