<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.HC](#cs.HC) [Total: 11]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How (Not) To Write a Software Engineering Abstract](https://arxiv.org/abs/2506.21634)
*Lutz Prechelt,Lloyd Montgomery,Julian Frattini,Franz Zieris*

Main category: cs.SE

TL;DR: 该论文分析了高质量软件工程会议摘要的结构，发现大多数摘要不完整或存在问题，并提出改进指南。


<details>
  <summary>Details</summary>
Motivation: 研究旨在量化高质量软件工程会议摘要的结构缺陷，并提出改进建议。

Method: 通过定性开放编码和定量内容分析，分析362篇高影响力会议摘要的结构和信息完整性。

Result: 仅29%的摘要完整，4%符合理想标准；结构化摘要表现更优。

Conclusion: 多数摘要未达理想标准，结构化格式更有效，建议社区强制要求结论部分。

Abstract: Background: Abstracts are a particularly valuable element in a software
engineering research article. However, not all abstracts are as informative as
they could be. Objective: Characterize the structure of abstracts in
high-quality software engineering venues. Observe and quantify deficiencies.
Suggest guidelines for writing informative abstracts. Methods: Use qualitative
open coding to derive concepts that explain relevant properties of abstracts.
Identify the archetypical structure of abstracts. Use quantitative content
analysis to objectively characterize abstract structure of a sample of 362
abstracts from five presumably high-quality venues. Use exploratory data
analysis to find recurring issues in abstracts. Compare the archetypical
structure to actual structures. Infer guidelines for producing informative
abstracts. Results: Only 29% of the sampled abstracts are complete, i.e.,
provide background, objective, method, result, and conclusion information. For
structured abstracts, the ratio is twice as big. Only 4% of the abstracts are
proper, i.e., they also have good readability (Flesch-Kincaid score) and have
no informativeness gaps, understandability gaps, nor highly ambiguous
sentences. Conclusions: (1) Even in top venues, a large majority of abstracts
are far from ideal. (2) Structured abstracts tend to be better than
unstructured ones. (3) Artifact-centric works need a different structured
format. (4) The community should start requiring conclusions that generalize,
which currently are often missing in abstracts.

</details>


### [2] [Experience converting a large mathematical software package written in C++ to C++20 modules](https://arxiv.org/abs/2506.21654)
*Wolfgang Bangerth*

Main category: cs.SE

TL;DR: 该论文研究了如何将基于C++的大型数学软件包（如deal.II有限元库）转换为C++20的模块系统，探讨了兼容性和实际应用效果。转换后库自身的编译时间减少，但对下游项目的编译时间影响不一。


<details>
  <summary>Details</summary>
Motivation: 传统的C++头文件接口方式存在笨拙、不可靠和慢的问题，C++20引入模块系统以改善此问题。论文旨在探讨如何将大型数学软件包迁移到这一新系统。

Method: 采用deal.II库（约80万行代码）作为案例，提出了一种同时支持头文件和模块接口的转换方法，并分析了技术与人因方面的挑战。

Result: 转换后库自身的编译时间减少，但下游项目的编译时间未显示明确趋势。转换工作量虽不小，但并非不可行。

Conclusion: 论文提出长期策略，逐步将整个数学软件生态系统迁移到模块系统，预计需要数年或数十年时间。

Abstract: Mathematical software has traditionally been built in the form of "packages"
that build on each other. A substantial fraction of these packages is written
in C++ and, as a consequence, the interface of a package is described in the
form of header files that downstream packages and applications can then
#include. C++ has inherited this approach towards exporting interfaces from C,
but the approach is clunky, unreliable, and slow. As a consequence, C++20 has
introduced a "module" system in which packages explicitly export declarations
and code that compilers then store in machine-readable form and that downstream
users can "import" -- a system in line with what many other programming
languages have used for decades.
  Herein, I explore how one can convert large mathematical software packages
written in C++ to this system, using the deal.II finite element library with
its around 800,000 lines of code as an example. I describe an approach that
allows providing both header-based and module-based interfaces from the same
code base, discuss the challenges one encounters, and how modules actually work
in practice in a variety of technical and human metrics. The results show that
with a non-trivial, but also not prohibitive effort, the conversion to modules
is possible, resulting in a reduction in compile time for the converted library
itself; on the other hand, for downstream projects, compile times show no clear
trend. I end with thoughts about long-term strategies for converting the entire
ecosystem of mathematical software over the coming years or decades.

</details>


### [3] [The DevSafeOps Dilemma: A Systematic Literature Review on Rapidity in Safe Autonomous Driving Development and Operation](https://arxiv.org/abs/2506.21693)
*Ali Nouri,Beatriz Cabrero-Daniel,Fredrik Törner,Christian Berger*

Main category: cs.SE

TL;DR: 本文通过系统文献综述，探讨了DevOps在自动驾驶开发中的应用，总结了挑战与解决方案，并指出仍需解决的安全问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统开发复杂且需确保安全可靠，DevOps的持续开发和监控特性为此提供了潜力。

Method: 通过系统文献综述，识别、分析和综合DevOps在自动驾驶开发中的相关文献。

Result: 总结了应用DevOps于安全相关AI功能时的挑战和解决方案。

Conclusion: 研究发现，实现安全的DevOps用于自动驾驶开发仍需解决多个开放问题。

Abstract: Developing autonomous driving (AD) systems is challenging due to the
complexity of the systems and the need to assure their safe and reliable
operation. The widely adopted approach of DevOps seems promising to support the
continuous technological progress in AI and the demand for fast reaction to
incidents, which necessitate continuous development, deployment, and
monitoring. We present a systematic literature review meant to identify,
analyse, and synthesise a broad range of existing literature related to usage
of DevOps in autonomous driving development. Our results provide a structured
overview of challenges and solutions, arising from applying DevOps to
safety-related AI-enabled functions. Our results indicate that there are still
several open topics to be addressed to enable safe DevOps for the development
of safe AD.

</details>


### [4] [Using Generative AI in Software Design Education: An Experience Report](https://arxiv.org/abs/2506.21703)
*Victoria Jackson,Susannah Liu,Andre van der Hoek*

Main category: cs.SE

TL;DR: 论文探讨了在本科软件设计课程中引入生成式AI（如ChatGPT）的实践，分析了学生使用AI的体验，并总结了对教育者的启示。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育领域的应用日益广泛，但关于其在软件设计课程中的使用研究较少。本文旨在填补这一空白。

Method: 学生在团队作业中使用ChatGPT，收集对话记录和反思，进行定性分析。

Result: 学生发现ChatGPT在设计过程中有帮助，但也意识到需要对其回答进行批判性评估。

Conclusion: 生成式AI在软件设计教育中具有潜力，能帮助学生设计并了解其优缺点。

Abstract: With the rapid adoption of Generative AI (GenAI) tools, software engineering
educators have grappled with how best to incorporate them into the classroom.
While some research discusses the use of GenAI in the context of learning to
code, there is little research that explores the use of GenAI in the classroom
for other areas of software development. This paper provides an experience
report on introducing GenAI into an undergraduate software design class.
Students were required to use GenAI (in the form of ChatGPT) to help complete a
team-based assignment. The data collected consisted of the ChatGPT conversation
logs and students' reflections on using ChatGPT for the assignment.
Subsequently, qualitative analysis was undertaken on the data. Students
identified numerous ways ChatGPT helped them in their design process while
recognizing the need to critique the response before incorporating it into
their design. At the same time, we identified several key lessons for educators
in how to deploy GenAI in a software design class effectively. Based on our
experience, we believe students can benefit from using GenAI in software design
education as it helps them design and learn about the strengths and weaknesses
of GenAI.

</details>


### [5] [KARMA Approach supporting Development Process Reconstruction in Model-based Systems Engineering](https://arxiv.org/abs/2506.22037)
*Jiawei Li,Zan Liang,Guoxin Wang,Jinzhi Lu,Yan Yan,Shouxuan Wu,Hao Wang*

Main category: cs.SE

TL;DR: 提出了一种基于KARMA语言和自然语言处理的模型重构方法，显著提升开发流程设计效率。


<details>
  <summary>Details</summary>
Motivation: 当前系统开发过程中缺乏有效方法管理需求变更和实现模型重构，为此提出一种支持开发流程模型的模型重构方法。

Method: 利用基于GOPPRR-E的KARMA语言统一形式化流程模型，引入模型重构框架，通过自然语言处理分析需求文本并提取信息，经过结构重组和算法优化生成满足需求的流程模型。

Result: 以飞机机载维护系统开发为例，验证了该方法能显著提升设计效率。

Conclusion: 该方法有效解决了开发流程模型重构问题，提升了设计效率。

Abstract: Model reconstruction is a method used to drive the development of complex
system development processes in model-based systems engineering. Currently,
during the iterative design process of a system, there is a lack of an
effective method to manage changes in development requirements, such as
development cycle requirements and cost requirements, and to realize the
reconstruction of the system development process model. To address these
issues, this paper proposes a model reconstruction method to support the
development process model. Firstly, the KARMA language, based on the GOPPRR-E
metamodeling method, is utilized to uniformly formalize the process models
constructed based on different modeling languages. Secondly, a model
reconstruction framework is introduced. This framework takes a structured
development requirements based natural language as input, employs natural
language processing techniques to analyze the development requirements text,
and extracts structural and optimization constraint information. Then, after
structural reorganization and algorithm optimization, a development process
model that meets the development requirements is obtained. Finally, as a case
study, the development process of the aircraft onboard maintenance system is
reconstructed. The results demonstrate that this method can significantly
enhance the design efficiency of the development process.

</details>


### [6] [Autonomic Microservice Management via Agentic AI and MAPE-K Integration](https://arxiv.org/abs/2506.22185)
*Matteo Esposito,Alexander Bakhtin,Noman Ahmad,Mikel Robredo,Ruoyu Su,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: 提出基于MAPE-K和智能代理的框架，用于微服务的自主异常检测与修复，提升系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 微服务的去中心化特性带来安全和管理挑战，威胁系统稳定性。

Method: 采用MAPE-K框架，结合智能代理，实现自主异常检测与修复。

Result: 为行业提供实用解决方案，增强系统稳定性和安全性。

Conclusion: 该框架可定制化，适用于提升微服务的性能和安全性。

Abstract: While microservices are revolutionizing cloud computing by offering
unparalleled scalability and independent deployment, their decentralized nature
poses significant security and management challenges that can threaten system
stability. We propose a framework based on MAPE-K, which leverages agentic AI,
for autonomous anomaly detection and remediation to address the daunting task
of highly distributed system management. Our framework offers practical,
industry-ready solutions for maintaining robust and secure microservices.
Practitioners and researchers can customize the framework to enhance system
stability, reduce downtime, and monitor broader system quality attributes such
as system performance level, resilience, security, and anomaly management,
among others.

</details>


### [7] [Can Large Language Models Help Students Prove Software Correctness? An Experimental Study with Dafny](https://arxiv.org/abs/2506.22370)
*Carolina Carreira,Álvaro Silva,Alexandre Abreu,Alexandra Mendes*

Main category: cs.SE

TL;DR: 这篇论文研究了学生在使用大型语言模型（如ChatGPT）解决形式验证问题时的表现及其策略，发现学生在使用ChatGPT时表现更好，但效果与提示质量相关。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（如ChatGPT）在支持学生完成高认知需求任务（如程序形式验证）中的潜力及其影响。

Method: 通过混合方法研究，让硕士生在形式方法课程中完成两个验证问题，一个使用定制的ChatGPT界面，另一个不使用，并记录所有交互。

Result: 使用ChatGPT的学生表现显著更好，但表现提升与提示质量密切相关。

Conclusion: 提出了将大型语言模型更有效地融入形式方法课程的实际建议，包括设计促进学习的LLM感知挑战。

Abstract: Students in computing education increasingly use large language models (LLMs)
such as ChatGPT. Yet, the role of LLMs in supporting cognitively demanding
tasks, like deductive program verification, remains poorly understood. This
paper investigates how students interact with an LLM when solving formal
verification exercises in Dafny, a language that supports functional
correctness, by allowing programmers to write formal specifications and
automatically verifying that the implementation satisfies the specification. We
conducted a mixed-methods study with master's students enrolled in a formal
methods course. Each participant completed two verification problems, one with
access to a custom ChatGPT interface, that logged all interactions, and the
other without. We identified strategies used by successful students and
assessed the level of trust students place in LLMs. %\todo{Our findings show
that something here} Our findings show that students perform significantly
better when using ChatGPT; however, performance gains are tied to prompt
quality. We conclude with practical recommendations for integrating LLMs into
formal methods courses more effectively, including designing LLM-aware
challenges that promote learning rather than substitution.

</details>


### [8] [What Makes ChatGPT Effective for Software Issue Resolution? An Empirical Study of Developer-ChatGPT Conversations in GitHub](https://arxiv.org/abs/2506.22390)
*Ramtin Ehsani,Sakshi Pathak,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: 研究分析了686个开发者与ChatGPT的对话，发现62%的对话对问题解决有帮助，ChatGPT在代码生成和工具推荐上表现最佳，但在代码解释上较弱。


<details>
  <summary>Details</summary>
Motivation: 了解ChatGPT在开发者问题解决中的有效性，以指导开发者和工具设计。

Method: 分析GitHub上的开发者-ChatGPT对话，分类任务类型和评估对话、项目及问题相关指标。

Result: ChatGPT对62%的对话有帮助，擅长代码生成和工具推荐，短且易读的对话更有效。

Conclusion: 研究为开发者提供了交互策略建议，并为优化LLM设计提供了指导。

Abstract: Conversational large-language models are extensively used for issue
resolution tasks. However, not all developer-LLM conversations are useful for
effective issue resolution. In this paper, we analyze 686 developer-ChatGPT
conversations shared within GitHub issue threads to identify characteristics
that make these conversations effective for issue resolution. First, we analyze
the conversations and their corresponding issues to distinguish helpful from
unhelpful conversations. We begin by categorizing the types of tasks developers
seek help with to better understand the scenarios in which ChatGPT is most
effective. Next, we examine a wide range of conversational, project, and
issue-related metrics to uncover factors associated with helpful conversations.
Finally, we identify common deficiencies in unhelpful ChatGPT responses to
highlight areas that could inform the design of more effective developer-facing
tools. We found that only 62% of the ChatGPT conversations were helpful for
successful issue resolution. ChatGPT is most effective for code generation and
tools/libraries/APIs recommendations, but struggles with code explanations.
Helpful conversations tend to be shorter, more readable, and exhibit stronger
semantic and linguistic alignment. Larger, more popular projects and more
experienced developers benefit more from ChatGPT. At the issue level, ChatGPT
performs best on simpler problems with limited developer activity and faster
resolution, typically well-scoped tasks like compilation errors. The most
common deficiencies in unhelpful ChatGPT responses include incorrect
information and lack of comprehensiveness. Our findings have wide implications
including guiding developers on effective interaction strategies for issue
resolution, informing the development of tools or frameworks to support optimal
prompt design, and providing insights on fine-tuning LLMs for issue resolution
tasks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [9] [Redundant Array Computation Elimination](https://arxiv.org/abs/2506.21960)
*Zixuan Wang,Liang Yuan,Xianmeng Jiang,Kun Li,Junmin Xiao,Yunquan Zhang*

Main category: cs.PF

TL;DR: 提出了一种通用的冗余数组计算消除技术RACE，通过两层方案高效识别数组引用间数据重用和表达式间计算冗余，并生成优化代码。


<details>
  <summary>Details</summary>
Motivation: 现有冗余消除方法缺乏普适性，要么针对特定模式，要么无法识别复杂结构冗余。

Method: RACE采用两层方案遍历循环嵌套中的表达式树，线性时间分层检测冗余，并支持表达式重关联增强冗余机会。

Result: 实验证明RACE的有效性。

Conclusion: RACE是一种通用且高效的冗余数组计算消除技术。

Abstract: Redundancy elimination is a key optimization direction, and loop nests are
the main optimization target in modern compilers. Previous work on redundancy
elimination of array computations in loop nests lacks universality. These
approaches either focus on specific computation patterns or fail to recognize
redundancies with complex structures. This paper proposes RACE (Redundant Array
Computation Elimination), a more general redundancy elimination technique. RACE
utilizes a novel two-level scheme to identify the data reuse between array
references and the computation redundancies between expressions. It traverses
the expression trees in loop nests to detect redundancies hierarchically in
linear time and generates efficient code with optimized auxiliary arrays that
store redundant computation results. Furthermore, RACE supports the expression
reassociation with various aggressive strategies to improve the redundancy
opportunities. Experimental results demonstrate the effectiveness of RACE.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [10] [Joint Task Offloading and Resource Allocation in Low-Altitude MEC via Graph Attention Diffusion](https://arxiv.org/abs/2506.21933)
*Yifan Xue,Ruihuai Liang,Bo Yang,Xuelin Cao,Zhiwen Yu,Mérouane Debbah,Chau Yuen*

Main category: cs.NI

TL;DR: 针对低空经济网络中的任务调度问题，提出了一种基于图注意扩散的解决方案生成器（GADSG），有效解决了异构MEC系统中的任务卸载和资源分配挑战。


<details>
  <summary>Details</summary>
Motivation: 低空经济的快速发展对多接入边缘计算系统提出了实时和智能任务调度的需求，但节点异构性、通信链路不稳定和任务动态性等问题带来了挑战。

Method: 构建了一个三层的异构MEC系统架构，结合图注意力网络和扩散模型，提出了GADSG方法，对卸载决策和资源分配进行联合优化。

Result: 通过多组仿真数据实验，GADSG在优化性能、稳健性和泛化性上显著优于现有基线方法。

Conclusion: GADSG展现出了在动态复杂的低空经济网络环境中高效任务调度的潜力。

Abstract: With the rapid development of the low-altitude economy, air-ground integrated
multi-access edge computing (MEC) systems are facing increasing demands for
real-time and intelligent task scheduling. In such systems, task offloading and
resource allocation encounter multiple challenges, including node
heterogeneity, unstable communication links, and dynamic task variations. To
address these issues, this paper constructs a three-layer heterogeneous MEC
system architecture for low-altitude economic networks, encompassing aerial and
ground users as well as edge servers. The system is systematically modeled from
the perspectives of communication channels, computational costs, and constraint
conditions, and the joint optimization problem of offloading decisions and
resource allocation is uniformly abstracted into a graph-structured modeling
task. On this basis, we propose a graph attention diffusion-based solution
generator (GADSG). This method integrates the contextual awareness of graph
attention networks with the solution distribution learning capability of
diffusion models, enabling joint modeling and optimization of discrete
offloading variables and continuous resource allocation variables within a
high-dimensional latent space. We construct multiple simulation datasets with
varying scales and topologies. Extensive experiments demonstrate that the
proposed GADSG model significantly outperforms existing baseline methods in
terms of optimization performance, robustness, and generalization across task
structures, showing strong potential for efficient task scheduling in dynamic
and complex low-altitude economic network environments.

</details>


### [11] [Resilient Communication For Avalanche Response in Infrastructure-Limited Environments](https://arxiv.org/abs/2506.22148)
*Joshua Goulton,Milena Radenkovic*

Main category: cs.NI

TL;DR: DTN框架通过瑞士铁路网络传输雪崩警报的有效性分析，对比Epidemic和PROPHET协议，验证其在城市和偏远地区的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在基础设施有限的灾害环境中（如雪崩警报）利用现有交通网络实现可靠的通信。

Method: 使用ONE模拟器建模瑞士铁路网络，对比Epidemic和PROPHET两种DTN路由协议，分别在城市和偏远山区场景测试。

Result: 铁路网络在两种环境下均能提供稳定的机会性通信，支持DTN技术在偏远地区的应用。

Conclusion: 瑞士铁路网络作为DTN数据传输骨干具有潜力，特别是在灾害通信中。

Abstract: Delay Tolerant Networks (DTNs) offer a promising paradigm for maintaining
communication in infrastructure limited environments, such as those encountered
during natural disasters. This paper investigates the viability of leveraging
an existing national transport system - the Swiss rail network - as a data mule
backbone for disseminating critical avalanche alerts. Using The Opportunistic
Network Environment (ONE) simulator, we model the entire Swiss rail network and
conduct a rigorous comparative analysis of two seminal DTN routing protocols:
Epidemic and PROPHET. Experiments are performed in two distinct scenarios:
alerts originating from dense urban centres and from sparse, remote mountainous
regions. Our results demonstrate that the rail network provides robust
connectivity for opportunistic communication in both environments thus
validating the integration of DTN principles in remote scenarios.

</details>


### [12] [V2X Intention Sharing for Cooperative Electrically Power-Assisted Cycles](https://arxiv.org/abs/2506.22223)
*Felipe Valle Quiroz,Johan Elfing,Joel Pålsson,Elena Haller,Oscar Amador Molina*

Main category: cs.NI

TL;DR: 本文提出了一种新颖的意图共享机制，用于V2X通信框架中的电动助力自行车（EPAC），通过椭圆区域表示和最小二乘法优化轨迹预测，提升网络可靠性和传输频率。


<details>
  <summary>Details</summary>
Motivation: 旨在为EPACs在V2X通信中提供高效、低延迟的意图共享，以提升弱势道路使用者的安全和协作感知能力。

Method: 采用最小二乘法拟合二次多项式，将离散预测轨迹点替换为紧凑的椭圆地理区域表示，固定数据负载以实现高频传输。

Result: 仿真显示在受限通信条件下优于标准ETSI VAMs，实验验证了嵌入式系统实时部署的可行性。

Conclusion: 该方法支持可扩展、低延迟的意图共享，为开放式预测方法提供了研究基础。

Abstract: This paper introduces a novel intention-sharing mechanism for Electrically
Power-Assisted Cycles (EPACs) within V2X communication frameworks, enhancing
the ETSI VRU Awareness Message (VAM) protocol. The method replaces discrete
predicted trajectory points with a compact elliptical geographical area
representation derived via quadratic polynomial fitting and Least Squares
Method (LSM). This approach encodes trajectory predictions with fixed-size data
payloads, independent of the number of forecasted points, enabling
higher-frequency transmissions and improved network reliability. Simulation
results demonstrate superior inter-packet gap (IPG) performance compared to
standard ETSI VAMs, particularly under constrained communication conditions. A
physical experiment validates the feasibility of real-time deployment on
embedded systems. The method supports scalable, low-latency intention sharing,
contributing to cooperative perception and enhanced safety for vulnerable road
users in connected and automated mobility ecosystems. Finally, we discuss the
viability of LSM and open the door to other methods for prediction.

</details>


### [13] [Design and Evaluation of IEEE 802.11ax Uplink Orthogonal Frequency Division Multiple Random Access in ns-3](https://arxiv.org/abs/2506.22260)
*Douglas Dziedzorm Agbeve,Andrey Belogaev,Jeroen Famaey*

Main category: cs.NI

TL;DR: 本文提出了一种完全符合标准且开源的UORA实现，解决了现有模拟器的局限性，提升了资源分配的效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着Wi-Fi网络密度增加和新兴应用对低延迟、高可靠性的需求，EDCA机制的局限性日益显现，现有的UORA研究多依赖不公开的模拟器，限制了结果的可复现性和验证。

Method: 开发了一个与ns-3版本3.38兼容的完全标准化的开源UORA实现，改进了资源分配的调度逻辑和配置信号。

Result: 该实现显著提高了资源分配的效率和灵活性，支持更准确的UORA评估。

Conclusion: 这一开源工具有助于推动未来Wi-Fi资源分配策略的研究。

Abstract: Wi-Fi networks have long relied on the Enhanced Distributed Channel Access
(EDCA) mechanism, allowing stations to compete for transmission opportunities.
However, as networks become denser and emerging applications demand lower
latency and higher reliability, the limitations of EDCA such as overhead due to
contention and collisions have become more pronounced. To address these
challenges, Orthogonal Frequency Division Multiple Access (OFDMA) has been
introduced in Wi-Fi, enabling more efficient channel utilization through
scheduled resource allocation. Furthermore, Wi-Fi 6 defines Uplink Orthogonal
Frequency Division Multiple Random Access (UORA), a hybrid mechanism that
combines both scheduled and random access, balancing efficiency and
responsiveness in resource allocation. Despite significant research on UORA,
most studies rely on custom simulators that are not publicly available,
limiting reproducibility and preventing validation of the presented results.
The only known open-source UORA implementation in the ns-3 simulator exhibits
key limitations, such as usage of the same trigger frame (TF) to schedule
resources for buffer status reports and data transmissions, and lack of
signaling for UORA configuration. In this paper, we present a fully
standard-compliant and open source UORA implementation that is compatible with
ns-3 version 3.38, addressing these limitations to improve resource allocation
efficiency and adaptability. This implementation enables more accurate and
flexible evaluation of UORA, fostering future research on Wi-Fi resource
allocation strategies.

</details>


### [14] [Concept-Level AI for Telecom: Moving Beyond Large Language Models](https://arxiv.org/abs/2506.22359)
*Viswanath Kumarskandpriya,Abdulhalim Dandoush,Abbas Bradai,Ali Belgacem*

Main category: cs.NI

TL;DR: 论文探讨了LCMs在电信领域的优势，认为其能克服LLMs在处理跨层依赖和实时协调等问题上的不足，是实现高效AI驱动的电信管理的关键。


<details>
  <summary>Details</summary>
Motivation: 电信网络管理面临复杂多层次和多运营商系统的挑战，需要更高效的技术支持。

Method: 通过LCMs利用双曲隐空间进行层次表示，替代LLMs的逐词处理，以概念嵌入解决复杂网络交互问题。

Result: LCMs在内存效率、跨层相关性和多模态集成方面表现优于LLMs。

Conclusion: 采用LCMs是电信领域AI管理的必要进化，而非简单改进。

Abstract: The telecommunications and networking domain stands at the precipice of a
transformative era, driven by the necessity to manage increasingly complex,
hierarchical, multi administrative domains (i.e., several operators on the same
path) and multilingual systems. Recent research has demonstrated that Large
Language Models (LLMs), with their exceptional general-purpose text analysis
and code generation capabilities, can be effectively applied to certain telecom
problems (e.g., auto-configuration of data plan to meet certain application
requirements). However, due to their inherent token-by-token processing and
limited capacity for maintaining extended context, LLMs struggle to fulfill
telecom-specific requirements such as cross-layer dependency cascades (i.e.,
over OSI), temporal-spatial fault correlation, and real-time distributed
coordination. In contrast, Large Concept Models (LCMs), which reason at the
abstraction level of semantic concepts rather than individual lexical tokens,
offer a fundamentally superior approach for addressing these telecom
challenges. By employing hyperbolic latent spaces for hierarchical
representation and encapsulating complex multi-layered network interactions
within concise concept embeddings, LCMs overcome critical shortcomings of LLMs
in terms of memory efficiency, cross-layer correlation, and native multimodal
integration. This paper argues that adopting LCMs is not simply an incremental
step, but a necessary evolutionary leap toward achieving robust and effective
AI-driven telecom management.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture](https://arxiv.org/abs/2506.21865)
*Haofeng Wang,Yilin Guo,Zehao Li,Tong Yue,Yizong Wang,Enci Zhang,Rongqun Lin,Feng Gao,Shiqi Wang,Siwei Ma*

Main category: cs.MM

TL;DR: RiverEcho是一个基于大型语言模型和文化知识数据集的实时交互系统，用于保护和传承古老的黄河文化，通过语音查询和数字人提供专业回答。


<details>
  <summary>Details</summary>
Motivation: 保护和传承黄河文化，丰富其推广手段，并为用户提供更深入的文化见解。

Method: 构建黄河文化知识数据库，结合检索增强生成技术（RAG），通过语音交互和数字人展示结果。

Result: RAG技术提升了大型语言模型的回答质量，生成更专业、信息丰富的回应。

Conclusion: RiverEcho不仅丰富了黄河文化的推广方式，还增强了用户的文化体验和理解。

Abstract: The Yellow River is China's mother river and a cradle of human civilization.
The ancient Yellow River culture is, moreover, an indispensable part of human
art history. To conserve and inherit the ancient Yellow River culture, we
designed RiverEcho, a real-time interactive system that responds to voice
queries using a large language model and a cultural knowledge dataset,
delivering explanations through a talking-head digital human. Specifically, we
built a knowledge database focused on the ancient Yellow River culture,
including the collection of historical texts and the processing pipeline.
Experimental results demonstrate that leveraging Retrieval-Augmented Generation
(RAG) on the proposed dataset enhances the response quality of the Large
Language Model(LLM), enabling the system to generate more professional and
informative responses. Our work not only diversifies the means of promoting
Yellow River culture but also provides users with deeper cultural insights.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [On the role of connectivity in Linear Logic proofs](https://arxiv.org/abs/2506.21678)
*Raffaele Di Donna,Lorenzo Tortora de Falco*

Main category: cs.LO

TL;DR: 该论文探讨了一种扩展线性逻辑证明结构的Danos-Regnier正确性准则的性质。该性质应用于证明结构的正确性图，要求图无环且连通分量数比底部或弱化节点数多一。在MELL中，这一性质是必要条件但非充分条件。作者提出了一个几何条件，使其成为充分条件，从而隔离了MELL的几个片段。


<details>
  <summary>Details</summary>
Motivation: 研究扩展Danos-Regnier准则的性质，以更全面地验证线性逻辑证明结构的正确性。

Method: 引入几何条件，将原本仅为必要条件的性质转化为充分条件，用于MELL片段的正确性判定。

Result: 成功隔离了MELL的几个片段，并验证了这一性质的充分性。

Conclusion: 几何条件的引入扩展了正确性准则的应用范围，并重新证实了一些已知结果。

Abstract: We investigate a property that extends the Danos-Regnier correctness
criterion for linear logic proof-structures. The property applies to the
correctness graphs of a proof-structure: it states that any such graph is
acyclic and that the number of its connected components is exactly one more
than the number of nodes bottom or weakening. This is known to be necessary but
not sufficient in multiplicative exponential linear logic (MELL) to recover a
sequent calculus proof from a proof-structure. We present a geometric condition
on untyped proof-structures allowing us to turn this necessary property into a
sufficient one: we can thus isolate several fragments of MELL for which this
property is indeed a correctness criterion. We also recover as by-product some
known results.

</details>


### [17] [Negated String Containment is Decidable (Technical Report)](https://arxiv.org/abs/2506.22061)
*Vojtěch Havlena,Michal Hečko,Lukáš Holík,Ondřej Lengál*

Main category: cs.LO

TL;DR: 论文证明了字符串不包含谓词的不可判定性问题，展示了其在实际应用中的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究字符串不包含谓词的不可判定性，解决符号执行等领域中的实际问题。

Method: 分析正则语言约束下的字符串变量序列，并结合链自由字方程和正则成员约束。

Result: 证明了字符串不包含谓词的可判定性。

Conclusion: 研究结果为符号执行等领域的应用提供了理论基础。

Abstract: We provide a positive answer to a long-standing open question of the
decidability of the not-contains string predicate. Not-contains is practically
relevant, for instance in symbolic execution of string manipulating programs.
Particularly, we show that the predicate notContains(x1 ... xn, y1 ... ym),
where x1 ... xn and y1 ... ym are sequences of string variables constrained by
regular languages, is decidable. Decidability of a not-contains predicate
combined with chain-free word equations and regular membership constraints
follows.

</details>


### [18] [Wait-Only Broadcast Protocols are Easier to Verify](https://arxiv.org/abs/2506.22144)
*Lucie Guillou,Arnaud Sangnier,Nathalie Sznajder*

Main category: cs.LO

TL;DR: 研究了执行相同有限状态协议并通过广播通信的进程网络，重点分析同步问题和重复覆盖问题的可解性。在Wait-Only协议下，同步问题为Ackermann-Complete，重复覆盖问题在EXPSPACE中且PSPACE-hard。


<details>
  <summary>Details</summary>
Motivation: 探索参数化进程数量下的同步和重复覆盖问题的可解性，尤其在协议限制为Wait-Only时的复杂性。

Method: 通过理论分析，研究Wait-Only协议下同步和重复覆盖问题的计算复杂性。

Result: 同步问题为Ackermann-Complete，重复覆盖问题在EXPSPACE中且PSPACE-hard。

Conclusion: Wait-Only协议限制了问题的复杂性，使得同步和重复覆盖问题在某些情况下可解或部分可解。

Abstract: We study networks of processes that all execute the same finite-state
protocol and communicate via broadcasts. We are interested in two problems with
a parameterized number of processes: the synchronization problem which asks
whether there is an execution which puts all processes on a given state; and
the repeated coverability problem which asks if there is an infinite execution
where a given transition is taken infinitely often. Since both problems are
undecidable in the general case, we investigate those problems when the
protocol is Wait-Only, i.e., it has no state from which a process can both
broadcast and receive messages. We establish that the synchronization problem
becomes Ackermann-complete, and the repeated coverability problem is in
EXPSPACE, and PSPACE-hard.

</details>


### [19] [Scott's Representation Theorem and the Univalent Karoubi Envelope](https://arxiv.org/abs/2506.22196)
*Arnoud van der Leer,Kobe Wullaert,Benedikt Ahrens*

Main category: cs.LO

TL;DR: 该论文在统一数学基础中形式化了Scott的表示定理，实现了Scott和Hyland的两种证明，并探讨了Karoubi包络的作用及基础选择的影响。


<details>
  <summary>Details</summary>
Motivation: 研究目的是在一致型数学基础上形式化Scott的表示定理，并探讨其在范畴构造中的作用及基础选择的影响。

Method: 在UniMath库中实现了Scott和Hyland的两种证明方法，讨论了Karoubi包络的作用，并自动化了λ-项的简化。

Result: 成功形式化了Scott定理，展示了两种证明方法，并实现了λ-项的自动化简化。

Conclusion: 论文实现了定理的形式化，为研究无类型λ-演算提供了新工具，并探讨了数学基础对构造的影响。

Abstract: Lambek and Scott constructed a correspondence between simply-typed lambda
calculi and Cartesian closed categories. Scott's Representation Theorem is a
cousin to this result for untyped lambda calculi. It states that every untyped
lambda calculus arises from a reflexive object in some category. We present a
formalization of Scott's Representation Theorem in univalent foundations, in
the (Rocq-)UniMath library. Specifically, we implement two proofs of that
theorem, one by Scott and one by Hyland. We also explain the role of the
Karoubi envelope -- a categorical construction -- in the proofs and the impact
the chosen foundation has on this construction. Finally, we report on some
automation we have implemented for the reduction of $\lambda$-terms.

</details>


### [20] [Computation by infinite descent made explicit](https://arxiv.org/abs/2506.22206)
*Sebastian Enqvist*

Main category: cs.LO

TL;DR: 介绍了基于带序数变量的不动点公式的直觉逻辑非良基证明系统,探索了其计算内容,并展示了证明的计算性。


<details>
  <summary>Details</summary>
Motivation: 扩展直觉逻辑的证明系统,以支持归纳和共归纳定义,并探索其计算含义。

Method: 使用带序数变量标注的不动点公式,引入计算性概念,并验证证明的可计算性。

Result: 证明了每个有效证明都是可计算的,并展示了规范化和唯一函数表示结果。

Conclusion: 通过范畴模型验证了不动点公式与代数和余代数的对应关系。

Abstract: We introduce a non-wellfounded proof system for intuitionistic logic extended
with ordinary inductive and co-inductive definitions, based on a syntax in
which fixpoint formulas are annotated with explicit variables for ordinals. We
explore the computational content of this system, in particular we introduce a
notion of computability and show that every valid proof is computable. As a
consequence, we obtain a normalization result for proofs of what we call
finitary formulas. A special case of this result is that every proof of a
sequent of the appropriate form represents a unique function on natural
numbers. Finally, we derive a categorical model from the proof system and show
that least and greatest fixpoint formulas correspond to initial algebras and
final coalgebras respectively.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [21] [ViStruct: Simulating Expert-Like Reasoning Through Task Decomposition and Visual Attention Cues](https://arxiv.org/abs/2506.21762)
*Oliver Huang,Carolina Nobre*

Main category: cs.HC

TL;DR: ViStruct是一个自动化系统，通过分解高阶视觉问题为结构化分析步骤，并突出显示关键图表区域，模拟专家的数据可视化解释行为。


<details>
  <summary>Details</summary>
Motivation: 专家在多步数据可视化任务中使用的解释策略（如分解复杂目标或选择性关注关键区域）通常不明确，ViStruct旨在通过模型模拟这些策略。

Method: 利用大型语言和视觉语言模型，ViStruct识别图表组件，将子任务映射到空间区域，并展示视觉注意力线索。

Result: 在12种图表类型的45个任务中，ViStruct生成的解释序列与专家一致，且验证了其可解释性。

Conclusion: ViStruct提供了专家解释的可复制模型，为未来视觉素养工具开发提供了参考。

Abstract: Data visualization tasks often require multi-step reasoning, and the
interpretive strategies experts use, such as decomposing complex goals into
smaller subtasks and selectively attending to key chart regions are rarely made
explicit. ViStruct is an automated pipeline that simulates these expert
behaviours by breaking high-level visual questions into structured analytic
steps and highlighting semantically relevant chart areas. Leveraging large
language and vision-language models, ViStruct identifies chart components, maps
subtasks to spatial regions, and presents visual attention cues to externalize
expert-like reasoning flows. While not designed for direct novice instruction,
ViStruct provides a replicable model of expert interpretation that can inform
the development of future visual literacy tools. We evaluate the system on 45
tasks across 12 chart types and validate its outputs with trained visualization
users, confirming its ability to produce interpretable and expert-aligned
reasoning sequences.

</details>


### [22] [Avatars and Environments for Meetings in Social VR: What Styles and Choices Matter to People in Group Creativity Tasks?](https://arxiv.org/abs/2506.21780)
*Anya Osborne,Sabrina Fielder,Lee Taber,Tara Lamb,Joshua McVeigh-Schultz,Katherine Isbister*

Main category: cs.HC

TL;DR: 研究探讨了社交虚拟现实（VR）平台在远程会议中的潜力，分析了不同虚拟环境和头像风格对团队创造力的影响。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情促使远程协作工具需求激增，社交VR平台可能提供比视频会议更具临场感的替代方案，减少面对面会议的碳足迹。

Method: 通过两项连续研究：研究一调查了87名用户对VR会议中头像和环境的偏好；研究二在40名参与者中测试了不同虚拟环境和头像对创造力任务的影响。

Result: 研究结果为VR会议中头像和虚拟环境的设计提供了实践指导，以优化团队协作体验。

Conclusion: 社交VR平台在增强团队协作和减少碳排放方面具有潜力，头像和环境设计是关键因素。

Abstract: Due to the COVID-19 pandemic, many professional entities shifted toward
remote collaboration and video conferencing (VC) tools. Social virtual reality
(VR) platforms present an alternative to VC for meetings and collaborative
activities. Well-crafted social VR environments could enhance feelings of
co-presence and togetherness at meetings, helping reduce the need for
carbon-intensive travel to face-to-face meetings. This research contributes to
creating meeting tools in VR by exploring the effects of avatar styles and
virtual environments on groups creative performance using the Mozilla Hubs
platform. We present the results of two sequential studies. Study One surveys
avatar and environment preferences in various VR meeting contexts (N=87). Study
Two applies these findings to the design of a between-subjects and
within-subjects research where participants (N=40) perform creativity tasks in
pairs as embodied avatars in different virtual settings using VR headsets. We
discuss the design implications of avatar appearances and meeting settings on
teamwork.

</details>


### [23] [Validation of the MySurgeryRisk Algorithm for Predicting Complications and Death after Major Surgery: A Retrospective Multicenter Study Using OneFlorida Data Trust](https://arxiv.org/abs/2506.21814)
*Yuanfang Ren,Esra Adiyeke,Ziyuan Guan,Zhenhong Hu,Mackenzie J Meni,Benjamin Shickel,Parisa Rashidi,Tezcan Ozrazgat-Baslanti,Azra Bihorac*

Main category: cs.HC

TL;DR: 本研究旨在开发和验证预测重大手术后并发症和死亡的模型，使用多中心数据进行重新开发，结果显示模型性能优异且具有普适性。


<details>
  <summary>Details</summary>
Motivation: 术后并发症普遍且影响显著，需要通过模型预测以提高患者管理。

Method: 采用MySurgeryRisk算法，结合XGBoost模型，对多中心数据进行分析。

Result: 模型在预测ICU需求、机械通气、急性肾损伤和院内死亡率方面表现优异（AUC 0.92-0.95）。

Conclusion: 模型性能与先前验证一致，普适性强，手术代码和医生专业是关键影响因素。

Abstract: Despite advances in surgical techniques and care, postoperative complications
are prevalent and effects up to 15% of the patients who underwent a major
surgery. The objective of this study is to develop and validate models for
predicting postoperative complications and death after major surgery on a large
and multicenter dataset, following the previously validated MySurgeryRisk
algorithm. This retrospective, longitudinal and multicenter cohort analysis
included 508,097 encounters from 366,875 adult inpatients who underwent major
surgeries and were admitted to healthcare institutions within the OneFlorida+
network between 01/01/2012 and 04/29/2023. We applied the validated feature
selection and transformation approach in MySurgeryRisk models and redeveloped
eXtreme Gradient Boosting (XGBoost) models for predicting risk of postoperative
acute kidney injury (AKI), need for intensive care unit (ICU) admission, need
for mechanical ventilation (MV) therapy and in-hospital mortality on a
development set and evaluated the model performance on a validation set. Area
under the receiver operating characteristics curve values were obtained for
need for ICU admission, 0.93 (95% Confidence Interval [CI], 0.93-0.93); need
for MV, 0.94 (95% CI, 0.94-0.94); AKI, 0.92 (95% CI, 0.92-0.92); and
in-hospital mortality, 0.95 (95% CI, 0.94-0.95). Area under the
precision-recall curve values were computed for need for ICU admission, 0.62
(95% CI, 0.62-0.63); need for MV, 0.51 (95% CI, 0.49-0.52); AKI, 0.53 (95% CI,
0.53-0.54); and in-hospital mortality, 0.26 (95% CI, 0.24-0.29). The
performance of these models is comparable to that of the previously validated
MySurgeryRisk models, suggesting the enhanced generalizability of the models.
Primary procedure code and provider specialty consistently appeared as the top
influential variables, providing valuable insights into the factors influencing
surgical outcomes.

</details>


### [24] [3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach](https://arxiv.org/abs/2506.21845)
*Zhuodi Cai*

Main category: cs.HC

TL;DR: 3Description是一个实验性的人机协作3D建模方法，通过语言和手势描述让非专业人士也能参与建模，结合AI技术提高可访问性和易用性。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D建模对非专业人士的复杂性和可用性问题，推动人机协作的普及，避免技术主导的同时保留人类创造力。

Method: 结合定性研究、产品分析和用户测试，整合NLP和计算机视觉（OpenAI和MediaPipe）技术，通过网页实现语音和手势输入的3D建模调整。

Result: 开发出基于网页的3Description工具，支持非专业人士通过语言和手势协作创建3D模型。

Conclusion: 3Description不仅提升了3D建模的包容性和易用性，还强调了人机协作中人类创造力的重要性，为未来3D世界的构建提供了新途径。

Abstract: This paper presents 3Description, an experimental human-AI collaborative
approach for intuitive 3D modeling. 3Description aims to address accessibility
and usability challenges in traditional 3D modeling by enabling
non-professional individuals to co-create 3D models using verbal and gesture
descriptions. Through a combination of qualitative research, product analysis,
and user testing, 3Description integrates AI technologies such as Natural
Language Processing and Computer Vision, powered by OpenAI and MediaPipe.
Recognizing the web has wide cross-platform capabilities, 3Description is
web-based, allowing users to describe the desired model and subsequently adjust
its components using verbal and gestural inputs. In the era of AI and emerging
media, 3Description not only contributes to a more inclusive and user-friendly
design process, empowering more people to participate in the construction of
the future 3D world, but also strives to increase human engagement in
co-creation with AI, thereby avoiding undue surrender to technology and
preserving human creativity.

</details>


### [25] [Focus on the Experts: Co-designing an Augmented Reality Eye-Gaze Tracking System with Surgical Trainees to Improve Endoscopic Instruction](https://arxiv.org/abs/2506.21896)
*Jumanh Atoum,Jinkyung Park,Mamtaj Akter,Nicholas Kavoussi,Pamela Wisniewski,Jie Ying Wu*

Main category: cs.HC

TL;DR: 通过增强现实（AR）技术改进内窥镜手术培训，研究人员与18名外科学员合作，共同设计了一个基于AR的眼球追踪系统，以满足学员对2D到3D映射训练的需求，同时不影响患者护理。


<details>
  <summary>Details</summary>
Motivation: 传统的内窥镜手术培训模型依赖高强度的监督，难以满足日益增长的外科医生需求。AR技术有望提高培训效率，但需深入了解学员需求以设计合适的AR培训系统。

Method: 研究人员与18名外科学员合作，分析当前培训环境的优缺点，并共同设计了一个基于AR的眼球追踪系统，重点关注2D到3D映射训练需求。

Result: 学员认为AR眼球追踪系统是一种有用的补充培训方法，能够在不影响患者护理的情况下提高学习效果。研究结果为设计AR培训系统提供了用户指导。

Conclusion: AR眼球追踪系统有望改进内窥镜手术培训，未来需进一步优化系统功能以满足学员需求。

Abstract: The current apprenticeship model for surgical training requires a high level
of supervision, which does not scale well to meet the growing need for more
surgeons. Many endoscopic procedures are directly taught in the operating room
(OR) while the attending surgeon and trainee operate on patients. The need to
prioritize patient care limits the trainees' opportunities to experiment and
receive feedback on their performance. Augmented reality (AR) has the potential
to increase efficiency in endoscopic surgical training, but additional research
is critical to understanding the needs of surgical trainees to inform the
design of AR training systems. Therefore, we worked with 18 surgical trainees
to understand the strengths, limitations, and unmet needs of their current
training environment and to co-design an AR eye-gaze tracking system based on
their preferences. Trainees emphasized the need to practice the 2D to 3D
mapping needed to properly familiarize oneself with the anatomy of patients to
prepare for real surgery. The trainees felt that an AR-based eye gaze tracking
system would be a useful supplemental training method that would improve their
learning in OR cases without detracting from patient care. To tailor the AR
system to their needs, they co-designed features to improve their ability to
track the attending surgeon's eye gaze and to provide a real-time, interactive
system. Our results are valuable in shaping the endoscopic training modules by
generating user-informed guidelines to design future collaborative AR-based
eye-gaze tracking systems.

</details>


### [26] [Bias, Accuracy, and Trust: Gender-Diverse Perspectives on Large Language Models](https://arxiv.org/abs/2506.21898)
*Aimen Gaba,Emily Wall,Tejas Ramkumar Babu,Yuriy Brun,Kyle Hall,Cindy Xiong Bearfield*

Main category: cs.HC

TL;DR: 研究探讨了不同性别群体对ChatGPT的偏见、准确性和信任度的看法，发现非二元性别参与者更容易受到刻板印象的影响，而男性对模型的信任度更高。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的普及，对其偏见的担忧日益增加，本研究旨在了解性别多样化群体对LLMs的看法。

Method: 通过对25名非二元/跨性别者、男性和女性的深入访谈，观察性别化与中性提示对模型响应的影响及其评价。

Result: 性别化提示引发更多特定身份响应，非二元参与者更易受到刻板印象影响；准确度感知在性别间一致，技术性和创意任务错误较多；信任度因性别而异。

Conclusion: 研究强调在LLM开发中纳入性别多样化视角，以提升包容性和信任度，建议增加训练数据多样性、平衡性别响应深度并加入澄清问题。

Abstract: Large language models (LLMs) are becoming increasingly ubiquitous in our
daily lives, but numerous concerns about bias in LLMs exist. This study
examines how gender-diverse populations perceive bias, accuracy, and
trustworthiness in LLMs, specifically ChatGPT. Through 25 in-depth interviews
with non-binary/transgender, male, and female participants, we investigate how
gendered and neutral prompts influence model responses and how users evaluate
these responses. Our findings reveal that gendered prompts elicit more
identity-specific responses, with non-binary participants particularly
susceptible to condescending and stereotypical portrayals. Perceived accuracy
was consistent across gender groups, with errors most noted in technical topics
and creative tasks. Trustworthiness varied by gender, with men showing higher
trust, especially in performance, and non-binary participants demonstrating
higher performance-based trust. Additionally, participants suggested improving
the LLMs by diversifying training data, ensuring equal depth in gendered
responses, and incorporating clarifying questions. This research contributes to
the CSCW/HCI field by highlighting the need for gender-diverse perspectives in
LLM development in particular and AI in general, to foster more inclusive and
trustworthy systems.

</details>


### [27] [AnyAni: An Interactive System with Generative AI for Animation Effect Creation and Code Understanding in Web Development](https://arxiv.org/abs/2506.21962)
*Tianrun Qiu,Yuxin Ma*

Main category: cs.HC

TL;DR: AnyAni 是一个帮助前端开发者设计动画的人机协作系统，通过生成式 AI 和非线性工作流支持动画创作。


<details>
  <summary>Details</summary>
Motivation: 开发者缺乏专业设计背景，难以描述和实现动画效果，需要工具辅助。

Method: 通过形成性研究（N=6）识别问题，开发 AnyAni 系统，结合生成式 AI 和非线性工作流，支持动画设计。

Result: 用户研究（N=9）验证了系统的可用性。

Conclusion: AnyAni 是有效的动画设计助手，帮助开发者克服创意和实现难题。

Abstract: Generative AI assistants have been widely used in front-end programming.
However, besides code writing, developers often encounter the need to generate
animation effects. As novices in creative design without the assistance of
professional designers, developers typically face difficulties in describing,
designing, and implementing desired animations. To address this issue, we
conducted a formative study (N=6) to identify the challenges that code
developers face when dealing with animation design issues. Then, we introduce
AnyAni, a human-AI collaborative system that supports front-end developers in
the ideation, manipulation, and implementation of animation effects. The system
combines the assistance of generative AI in creative design by adopting a
nonlinear workflow for iterative animation development. In addition, developers
can understand and learn the code generated for implementing animations through
various interactive methods. A user study (N=9) demonstrated the usability of
AnyAni in animation effect creation support for developers.

</details>


### [28] [Building Trustworthy Cognitive Monitoring for Safety-Critical Human Tasks: A Phased Methodological Approach](https://arxiv.org/abs/2506.22066)
*Maciej Grzeszczuk,Grzegorz Pochwatko,Barbara Karpowicz,Stanisław Knapiński,Wiesław Kopeć*

Main category: cs.HC

TL;DR: 论文提出了一种分阶段构建认知监控系统的方法，旨在通过结合多种技术手段，实时评估高风险任务中操作员的认知表现，以减少人为错误并提升安全性。


<details>
  <summary>Details</summary>
Motivation: 高风险任务（如空中交通管制、手术等）中的操作员需要在多变且压力大的条件下保持出色认知表现，亟需一种实时监控方法以支持其工作。

Method: 采用分阶段方法，结合人因研究、模拟训练、传感器技术和心理学原理，构建从简化模拟到实际操作的认知监控系统。

Result: 该系统能实时评估操作员表现，适应工作负荷变化和压力疲劳影响，同时支持决策而不损害操作员自主性。

Conclusion: 该方法为开发透明且适应性强的系统提供了基础，有助于在关键安全领域中提升人机协作效能。

Abstract: Operators performing high-stakes, safety-critical tasks - such as air traffic
controllers, surgeons, or mission control personnel - must maintain exceptional
cognitive performance under variable and often stressful conditions. This paper
presents a phased methodological approach to building cognitive monitoring
systems for such environments. By integrating insights from human factors
research, simulation-based training, sensor technologies, and fundamental
psychological principles, the proposed framework supports real-time performance
assessment with minimum intrusion. The approach begins with simplified
simulations and evolves towards operational contexts. Key challenges addressed
include variability in workload, the effects of fatigue and stress, thus the
need for adaptive monitoring for early warning support mechanisms. The
methodology aims to improve situational awareness, reduce human error, and
support decision-making without undermining operator autonomy. Ultimately, the
work contributes to the development of resilient and transparent systems in
domains where human performance is critical to safety.

</details>


### [29] [NoticeLight: Embracing Socio-Technical Asymmetry through Tangible Peripheral Robotic Embodiment in Hybrid Collaboration](https://arxiv.org/abs/2506.22125)
*Marie Altmann,Kimberly Hegemann,Ali Askari,Vineetha Rallabandi,Max Pascher,Jens Gerken*

Main category: cs.HC

TL;DR: NoticeLight是一个有形、外围的机器人装置，旨在通过将远程参与者的数字存在转化为环境物理信号（如情绪动态、语言贡献马赛克和注意力提示）来增强混合会议，从而促进外围意识和平衡参与。


<details>
  <summary>Details</summary>
Motivation: 现代工作场所中的混合协作模式带来了持续的社会技术不对称，尤其是远程参与者在存在差异、可见性降低和非语言沟通受限方面处于劣势。传统方法试图消除这些不对称，但近期研究建议将其作为设计约束。

Method: 引入NoticeLight，它通过抽象群体状态为微弱的光模式，在共处空间中展现远程参与者的数字化存在，从而在不中断会议流程或增加认知负担的情况下促进平衡参与。

Result: NoticeLight通过其物理信号提升了外围意识和参与平衡，支持了人类与机器人协同的新视角，即机器人作为重塑而非复制人类存在的媒介。

Conclusion: 这项工作推动了关于机器人装置如何在工作场所中赋能公平、动态协作的讨论。

Abstract: Hybrid collaboration has become a fixture in modern workplaces, yet it
introduces persistent socio-technical asymmetries-especially disadvantaging
remote participants, who struggle with presence disparity, reduced visibility,
and limited non-verbal communication. Traditional solutions often seek to erase
these asymmetries, but recent research suggests embracing them as productive
design constraints. In this context, we introduce NoticeLight: a tangible,
peripheral robotic embodiment designed to augment hybrid meetings. NoticeLight
transforms remote participants' digital presence into ambient, physical signals
-- such as mood dynamics, verbal contribution mosaics, and attention cues --
within the co-located space. By abstracting group states into subtle light
patterns, NoticeLight fosters peripheral awareness and balanced participation
without disrupting meeting flow or demanding cognitive overload. This approach
aligns with emerging perspectives in human-robot synergy, positioning robots as
mediators that reshape, rather than replicate, human presence. Our work thereby
advances the discourse on how robotic embodiments can empower equitable,
dynamic collaboration in the workplace.

</details>


### [30] [Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education](https://arxiv.org/abs/2506.22231)
*Russell Beale*

Main category: cs.HC

TL;DR: 生成式AI（如ChatGPT）在高等教育中的快速普及带来机遇与挑战，需制定政策以平衡技术潜力与学术诚信。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在高等教育中的应用及其对学术诚信、伦理和公平的影响。

Method: 综合近期研究和案例，分析AI的机遇与挑战，并提出政策建议。

Result: 研究表明47%的学生使用LLM完成作业，检测工具准确率88%，需改进评估和培训。

Conclusion: 需主动调整政策，以利用AI潜力，同时保障学术诚信和公平。

Abstract: The rapid proliferation of generative artificial intelligence (AI) tools -
especially large language models (LLMs) such as ChatGPT - has ushered in a
transformative era in higher education. Universities in developed regions are
increasingly integrating these technologies into research, teaching, and
assessment. On one hand, LLMs can enhance productivity by streamlining
literature reviews, facilitating idea generation, assisting with coding and
data analysis, and even supporting grant proposal drafting. On the other hand,
their use raises significant concerns regarding academic integrity, ethical
boundaries, and equitable access. Recent empirical studies indicate that nearly
47% of students use LLMs in their coursework - with 39% using them for exam
questions and 7% for entire assignments - while detection tools currently
achieve around 88% accuracy, leaving a 12% error margin. This article
critically examines the opportunities offered by generative AI, explores the
multifaceted challenges it poses, and outlines robust policy solutions.
Emphasis is placed on redesigning assessments to be AI-resilient, enhancing
staff and student training, implementing multi-layered enforcement mechanisms,
and defining acceptable use. By synthesizing data from recent research and case
studies, the article argues that proactive policy adaptation is imperative to
harness AI's potential while safeguarding the core values of academic integrity
and equity.

</details>


### [31] [How to Evaluate the Accuracy of Online and AI-Based Symptom Checkers: A Standardized Methodological Framework](https://arxiv.org/abs/2506.22379)
*Marvin Kopka,Markus A. Feufel*

Main category: cs.HC

TL;DR: 该论文提出了一种标准化评估在线和AI症状检查工具的方法框架，以提高未来研究的质量和可比性。


<details>
  <summary>Details</summary>
Motivation: 由于现有评估方法缺乏质量控制且已过时，亟需一种高质量的方法框架来标准化症状检查工具的评估。

Method: 通过综合实证研究，提出基于代表性案例选择、内外有效评估设计以及提高跨研究可比性指标的框架，并提供开源资源辅助实施。

Result: 该框架旨在提升未来对在线和AI症状检查工具评估的质量和可比性，支持元分析并帮助利益相关者做出更明智的决策。

Conclusion: 论文提出的标准化评估框架填补了长期存在的空白，为症状检查工具的研究和实际应用提供了更科学的方法。

Abstract: Online and AI-based symptom checkers are applications that assist medical
laypeople in diagnosing their symptoms and determining which course of action
to take. When evaluating these tools, previous studies primarily used an
approach introduced a decade ago that lacked any type of quality control.
Numerous studies have criticized this approach, and several empirical studies
have sought to improve specific aspects of evaluations. However, even after a
decade, a high-quality methodological framework for standardizing the
evaluation of symptom checkers remains missing. This article synthesizes
empirical studies to outline a framework for standardized evaluations based on
representative case selection, an externally and internally valid evaluation
design, and metrics that increase cross-study comparability. This approach is
backed up by several open-access resources to facilitate implementation.
Ultimately, this approach should enhance the quality and comparability of
future evaluations of online and AI-based symptom checkers to enable
meta-analyses and help stakeholders make more informed decisions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [32] [ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes](https://arxiv.org/abs/2506.21629)
*Chenhao Zhang,Yezhi Shen,Fengqing Zhu*

Main category: cs.GR

TL;DR: 论文提出了一种结合迭代最近点（ICP）和优化的方法，用于改进大范围运动下的相机姿态估计，并通过体素化场景增强来指导大规模场景重建，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法（如NeRF和3DGS）严重依赖预处理相机姿态和3D结构先验，这在户外场景中难以获取，因此需要一种更鲁棒的相机姿态估计和大规模场景重建方法。

Method: 结合ICP和优化方法进行相机姿态估计，并使用体素化场景增强来指导重建。

Result: ICP-3DGS在室内外场景的相机姿态估计和新视角合成方面均优于现有方法。

Conclusion: 该方法解决了神经渲染中依赖预处理相机姿态的局限性，为大规模场景重建提供了有效解决方案。

Abstract: In recent years, neural rendering methods such as NeRFs and 3D Gaussian
Splatting (3DGS) have made significant progress in scene reconstruction and
novel view synthesis. However, they heavily rely on preprocessed camera poses
and 3D structural priors from structure-from-motion (SfM), which are
challenging to obtain in outdoor scenarios. To address this challenge, we
propose to incorporate Iterative Closest Point (ICP) with optimization-based
refinement to achieve accurate camera pose estimation under large camera
movements. Additionally, we introduce a voxel-based scene densification
approach to guide the reconstruction in large-scale scenes. Experiments
demonstrate that our approach ICP-3DGS outperforms existing methods in both
camera pose estimation and novel view synthesis across indoor and outdoor
scenes of various scales. Source code is available at
https://github.com/Chenhao-Z/ICP-3DGS.

</details>


### [33] [SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model](https://arxiv.org/abs/2506.21632)
*Da Li,Donggang Jia,Markus Hadwiger,Ivan Viola*

Main category: cs.GR

TL;DR: 该论文提出了一种通过点云解耦和联合优化策略，从单目视频中重建交互式人体和背景的方法，实现了高质量重建和实时渲染。


<details>
  <summary>Details</summary>
Motivation: 从单目动态场景视频中重建交互式人体和背景是一个高度挑战性的问题，传统方法在重建质量和计算效率上存在不足。

Method: 采用点云解耦和联合优化策略，引入位置纹理细分SMPL模型表面生成人体点云，并结合卷积神经网络预测人体点云特征。

Result: 该方法在重建指标上优于HUGS，实现了100 FPS的实时渲染速度，并在GPU资源消耗上更高效。

Conclusion: 该方法不仅提高了重建质量和效率，还展示了可扩展性，适用于动物场景重建。

Abstract: Reconstructing an interactive human avatar and the background from a
monocular video of a dynamic human scene is highly challenging. In this work we
adopt a strategy of point cloud decoupling and joint optimization to achieve
the decoupled reconstruction of backgrounds and human bodies while preserving
the interactivity of human motion. We introduce a position texture to subdivide
the Skinned Multi-Person Linear (SMPL) body model's surface and grow the human
point cloud. To capture fine details of human dynamics and deformations, we
incorporate a convolutional neural network structure to predict human body
point cloud features based on texture. This strategy makes our approach free of
hyperparameter tuning for densification and efficiently represents human points
with half the point cloud of HUGS. This approach ensures high-quality human
reconstruction and reduces GPU resource consumption during training. As a
result, our method surpasses the previous state-of-the-art HUGS in
reconstruction metrics while maintaining the ability to generalize to novel
poses and views. Furthermore, our technique achieves real-time rendering at
over 100 FPS, $\sim$6$\times$ the HUGS speed using only Linear Blend Skinning
(LBS) weights for human transformation. Additionally, this work demonstrates
that this framework can be extended to animal scene reconstruction when an
accurately-posed model of an animal is available.

</details>


### [34] [SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target Reconstruction](https://arxiv.org/abs/2506.21633)
*Aobo Li,Zhengxin Lei,Jiangtao Wei,Feng Xu*

Main category: cs.GR

TL;DR: 本文提出了一种新型SAR目标重建方法SDGR，结合高斯溅射与映射投影算法，通过优化高斯基元参数实现高效3D重建。


<details>
  <summary>Details</summary>
Motivation: 解决SAR图像中复杂电磁散射机制带来的重建挑战，受光学域3D高斯溅射成功的启发。

Method: 结合高斯溅射与映射投影算法，计算高斯基元的散射强度，通过SDGR生成模拟SAR图像，并通过自定义CUDA梯度流优化参数。

Result: 实验验证了SDGR在模拟和真实SAR数据上的有效性，能准确重建目标的几何结构和散射特性。

Conclusion: SDGR为SAR图像3D重建提供了一种新方案，效果显著。

Abstract: Three-dimensional target reconstruction from synthetic aperture radar (SAR)
imagery is crucial for interpreting complex scattering information in SAR data.
However, the intricate electromagnetic scattering mechanisms inherent to SAR
imaging pose significant reconstruction challenges. Inspired by the remarkable
success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this
paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR)
specifically designed for SAR target reconstruction. Our approach combines
Gaussian splatting with the Mapping and Projection Algorithm to compute
scattering intensities of Gaussian primitives and generate simulated SAR images
through SDGR. Subsequently, the loss function between the rendered image and
the ground truth image is computed to optimize the Gaussian primitive
parameters representing the scene, while a custom CUDA gradient flow is
employed to replace automatic differentiation for accelerated gradient
computation. Through experiments involving the rendering of simplified
architectural targets and SAR images of multiple vehicle targets, we validate
the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the
effectiveness of our method for target reconstruction is demonstrated on both
simulated and real-world datasets containing multiple vehicle targets, with
quantitative evaluations conducted to assess its reconstruction performance.
Experimental results indicate that our approach can effectively reconstruct the
geometric structures and scattering properties of targets, thereby providing a
novel solution for 3D reconstruction in the field of SAR imaging.

</details>


### [35] [A Design Space for Visualization Transitions of 3D Spatial Data in Hybrid AR-Desktop Environments](https://arxiv.org/abs/2506.22250)
*Yucheng Lu,Tobias Rau,Benjamin Lee,Andreas Köhn,Michael Sedlmair,Christian Sandor,Tobias Isenberg*

Main category: cs.GR

TL;DR: 提出了一种在混合AR-桌面环境中为3D空间数据集的外观设计过渡动画的方法，以降低认知负荷并提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 混合界面结合传统和沉浸式显示，需要过渡动画来连接不同维度的数据表示，减少用户认知负荷。

Method: 设计了一个过渡设计空间，基于数据和应用需求选择过渡动画。讨论了3D可视化的空间编码流程，并提出了在两个空间编码流程之间插值的过渡方法。

Result: 通过天文、放射学和化学三个案例研究验证了设计空间的实用性，并总结了经验教训。

Conclusion: 该设计空间为混合AR-桌面环境中3D数据的过渡动画提供了决策支持和设计灵感，改善了用户体验。

Abstract: We present a design space for animated transitions of the appearance of 3D
spatial datasets in a hybrid Augmented Reality (AR)-desktop context. Such
hybrid interfaces combine both traditional and immersive displays to facilitate
the exploration of 2D and 3D data representations in the environment in which
they are best displayed. One key aspect is to introduce transitional animations
that change between the different dimensionalities to illustrate the connection
between the different representations and to reduce the potential cognitive
load on the user. The specific transitions to be used depend on the type of
data, the needs of the application domain, and other factors. We summarize
these as a transition design space to simplify the decision-making process and
provide inspiration for future designs. First, we discuss 3D visualizations
from a spatial perspective: a spatial encoding pipeline, where 3D data sampled
from the physical world goes through various transformations, being mapped to
visual representations, and then being integrated into a hybrid AR-desktop
environment. The transition design then focuses on interpolating between two
spatial encoding pipelines to provide a smooth experience. To illustrate the
use of our design space, we apply it to three case studies that focus on
applications in astronomy, radiology, and chemistry; we then discuss lessons
learned from these applications.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [36] [Evaluating Redundancy Mitigation in Vulnerable Road User Awareness Messages for Bicycles](https://arxiv.org/abs/2506.22052)
*Nico Ostendorf,Keno Garlichs,Lars Wolf*

Main category: cs.ET

TL;DR: 该研究评估了冗余缓解（RM）在V2X通信中对骑行者的安全性影响，发现标准化RM机制使用过时信息，提出改进方法以减少信道负载并提升安全性。


<details>
  <summary>Details</summary>
Motivation: 随着V2X通信设备增多，信道负载增加，冗余信息传输可能导致安全问题，需优化RM机制以平衡信道负载和VRU（弱势道路使用者）的感知。

Method: 通过模拟高自行车密度的城市场景，评估RM对信道繁忙比（CBR）和VRU感知率（VPR）的影响，分析位置、速度和方向的实际差异。

Result: 标准化RM虽减少信道拥塞，但降低了VPR；改进的RM显著降低最大CBR，且减少对安全的影响，位置、速度和方向差异更小。

Conclusion: 改进的RM机制在减少信道负载的同时保持了更高的安全性，需进一步研究优化RM技术以确保VRU安全。

Abstract: V2X communication has become crucial for enhancing road safety, especially
for Vulnerable Road Users (VRU) such as pedestrians and cyclists. However, the
increasing number of devices communicating on the same channels will lead to
significant channel load. To address this issue this study evaluates the
effectiveness of Redundancy Mitigation (RM) for VRU Awareness Messages (VAM),
focusing specifically on cyclists. The objective of RM is to minimize the
transmission of redundant information. We conducted a simulation study using a
urban scenario with a high bicycle density based on traffic data from Hannover,
Germany. This study assessed the impact of RM on channel load, measured by
Channel Busy Ratio (CBR), and safety, measured by VRU Perception Rate (VPR) in
simulation. To evaluate the accuracy and reliability of the RM mechanisms, we
analyzed the actual differences in position, speed, and heading between the ego
VRU and the VRU, which was assumed to be redundant. Our findings indicate that
while RM can reduce channel congestion, it also leads to a decrease in VPR. The
analysis of actual differences revealed that the RM mechanism standardized by
ETSI often uses outdated information, leading to significant discrepancies in
position, speed, and heading, which could result in dangerous situations. To
address these limitations, we propose an adapted RM mechanism that improves the
balance between reducing channel load and maintaining VRU awareness. The
adapted approach shows a significant reduction in maximum CBR and a less
significant decrease in VPR compared to the standardized RM. Moreover, it
demonstrates better performance in the actual differences in position, speed,
and heading, thereby enhancing overall safety. Our results highlight the need
for further research to optimize RM techniques and ensure they effectively
enhance V2X communication without compromising the safety of VRUs.

</details>


### [37] [Unified Memcapacitor-Memristor Memory for Synaptic Weights and Neuron Temporal Dynamics](https://arxiv.org/abs/2506.22227)
*Simone D'Agostino,Marco Massarotto,Tristan Torchet,Filippo Moro,Niccolò Castellani,Laurent Grenouillet,Yann Beilliard,David Esseni,Melika Payvand,Elisa Vianello*

Main category: cs.ET

TL;DR: 论文提出了一种结合忆阻和忆容行为的记忆堆栈，设计了能够同时控制RSNN时空动态的电路，硬件模拟显示了其在神经形态处理中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索忆阻和忆容行为的统一，以实现更高效的神经形态计算。

Method: 设计并实验表征了结合忆阻和忆容行为的记忆堆栈，并开发了控制RSNN时空动态的电路。

Result: 硬件模拟表明该电路在神经形态处理中具有高效性。

Conclusion: 通过忆阻和忆容行为的结合，为神经形态计算提供了一种新的高效方案。

Abstract: We present a fabricated and experimentally characterized memory stack that
unifies memristive and memcapacitive behavior. Exploiting this dual
functionality, we design a circuit enabling simultaneous control of spatial and
temporal dynamics in recurrent spiking neural networks (RSNNs). Hardware-aware
simulations highlight its promise for efficient neuromorphic processing.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [38] [SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference](https://arxiv.org/abs/2506.22033)
*Yongchao He,Bohan Zhao,Zheng Cao*

Main category: cs.DC

TL;DR: SiPipe通过利用未充分利用的CPU资源来卸载辅助计算和通信，从而提高LLM推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理工作负载的增长，管道并行(PP)在多GPU部署中存在效率低下问题，SiPipe旨在解决这些问题。

Method: 采用CPU采样、令牌安全执行模型和结构感知传输三种关键技术，优化管道气泡问题。

Result: SiPipe在相同PP配置下，吞吐量提升2.1倍，延迟降低43%，GPU利用率提高23%。

Conclusion: SiPipe通过智能资源调度显著提升了LLM推理效率，具有广泛适用性。

Abstract: As inference workloads for large language models (LLMs) scale to meet growing
user demand, pipeline parallelism (PP) has become a widely adopted strategy for
multi-GPU deployment, particularly in cross-node setups, to improve key-value
(KV) cache capacity and inference throughput. However, PP suffers from inherent
inefficiencies caused by three types of execution bubbles-load-imbalance,
intra-stage, and inter-stage-which limit pipeline saturation. We present
SiPipe, a heterogeneous pipeline design that improves throughput by leveraging
underutilized CPU resources to offload auxiliary computation and communication.
SiPipe incorporates three key techniques-CPU sampling, a token-safe execution
model, and structure-aware transmission-to mitigate pipeline bubbles and
improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1
times higher throughput, 43% lower per-token latency, and up to 23% higher
average GPU utilization compared to the state-of-the-art vLLM under the same PP
configuration, demonstrating its generality across LLMs and deployment
scenarios.

</details>


### [39] [MCFuser: High-Performance and Rapid Fusion of Memory-Bound Compute-Intensive Operators](https://arxiv.org/abs/2506.22169)
*Zheng Zhang,Donglin Yang,Xiaobo Zhou,Dazhao Cheng*

Main category: cs.DC

TL;DR: MCFuser框架通过高级分块表达式和DAG分析优化计算密集型操作链的融合，显著提高性能并减少调优时间。


<details>
  <summary>Details</summary>
Motivation: 解决计算密集型操作链在动态张量维度下由于内存瓶颈和冗余访问导致的性能不佳问题。

Method: 利用高级分块表达式定义搜索空间，结合DAG分析消除冗余内存访问，并通过启发式搜索快速优化。

Result: 在NVIDIA GPU上性能最高提升5.9倍，调优时间减少70倍以上。

Conclusion: MCFuser高效解决了计算密集型操作链的融合难题，显著提升性能与部署效率。

Abstract: Operator fusion, a key technique to improve data locality and alleviate GPU
memory bandwidth pressure, often fails to extend to the fusion of multiple
compute-intensive operators due to saturated computation throughput. However,
the dynamicity of tensor dimension sizes could potentially lead to these
operators becoming memory-bound, necessitating the generation of fused kernels,
a task hindered by limited search spaces for fusion strategies, redundant
memory access, and prolonged tuning time, leading to sub-optimal performance
and inefficient deployment.
  We introduce MCFuser, a pioneering framework designed to overcome these
obstacles by generating high-performance fused kernels for what we define as
memory-bound compute-intensive (MBCI) operator chains. Leveraging high-level
tiling expressions to delineate a comprehensive search space, coupled with
Directed Acyclic Graph (DAG) analysis to eliminate redundant memory accesses,
MCFuser streamlines kernel optimization. By implementing guidelines to prune
the search space and incorporating an analytical performance model with a
heuristic search, MCFuser not only significantly accelerates the tuning process
but also demonstrates superior performance. Benchmarked against leading
compilers like Ansor on NVIDIA A100 and RTX3080 GPUs, MCFuser achieves up to a
5.9x speedup in kernel performance and outpaces other baselines while reducing
tuning time by over 70-fold, showcasing its agility.

</details>


### [40] [SPTCStencil: Unleashing Sparse Tensor Cores for Stencil Computation via Strided Swap](https://arxiv.org/abs/2506.22035)
*Qiqi GU,Chenpeng Wu,Heng Shi,Jianguo Yao*

Main category: cs.DC

TL;DR: 论文提出了一种名为SPTCStencil的高性能模板计算系统，利用稀疏张量核心（SpTCs）加速传统模板计算，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将模板计算转换为矩阵乘法时因冗余零填充导致效率低下，稀疏计算范式可解决这一问题。

Method: 采用稀疏化策略将模板计算高效转换为矩阵乘法，并设计了针对SpTC优化的GPU内核。

Result: 实验表明，SPTCStencil平均性能提升了5.46倍，优于基于张量核心的方法。

Conclusion: SPTCStencil首次将SpTC技术应用于深度学习之外的领域，显著提升了模板计算效率。

Abstract: Stencil computation, a pivotal numerical method in science and engineering,
iteratively updates grid points using weighted neighbor contributions and
exhibits strong parallelism for multi-core processors. Current optimization
techniques targeting conducting stencil computation on tensor core accelerators
incur substantial overheads due to redundant zero-padding during the
transformation to matrix multiplication. To address this, we introduce a sparse
computation paradigm that eliminates inefficiencies by exploiting specialized
hardware units.
  This paper exploits the sparsity in these matrices as a feature and presents
SPTCStencil, a high-performance stencil computation system accelerated by
Sparse Tensor Core (SpTCs). SPTCStencil is the first to harness SpTCs for
acceleration beyond deep learning domains. First, Our approach generalizes an
efficient transformation of stencil computation into matrix multiplications and
specializes this conversion for SpTC compatibility through a novel
sparsification strategy. Furthermore, SPTCStencil incorporates a
high-performance GPU kernel with systematic optimizations designed to maximize
efficiency on SpTCs. Experimental evaluations demonstrate that SPTCStencil
5.46$\times$ and Tensor Core-based approaches by 2.00$\times$ on average.

</details>


### [41] [Proof-of-Behavior: Behavior-Driven Consensus for Trustworthy Decentralized Finance](https://arxiv.org/abs/2506.22171)
*Ailiya Borjigin,Wei Zhou,Cong He*

Main category: cs.DC

TL;DR: PoB（行为证明）通过动态调整验证者权重和去中心化验证，显著减少DeFi中的欺诈行为，并提高公平性。


<details>
  <summary>Details</summary>
Motivation: 现有区块链协议（如PoW和PoS）无法衡量验证者的可信度，导致DeFi环境中的隐蔽不当行为。

Method: 提出PoB模型，包括行为分层评分、动态权重调整和去中心化验证，并设计激励兼容的奖励机制。

Result: 模拟实验显示PoB将欺诈接受率降低90%以上，快速识别恶意验证者，且吞吐量开销不超过5%。

Conclusion: PoB为区块链金融应用提供了可扩展且符合监管的安全治理基础。

Abstract: Current blockchain protocols (e.g., Proof-of-Work and Proof-of-Stake) secure
the ledger yet cannot measure validator trustworthiness, allowing subtle
misconduct that is especially damaging in decentralized-finance (DeFi)
settings. We introduce Proof-of-Behavior (PoB), a consensus model that (i)
gives each action a layered utility score -- covering motivation and outcome,
(ii) adapts validator weights using recent scores, and (iii) applies
decentralized verification with proportional slashing. The reward design is
incentive-compatible, yielding a Nash equilibrium in which honest behavior
maximizes long-run pay-offs. Simulated DeFi experiments (loan-fraud detection,
reputation-weighted validation) show that PoB cuts fraud acceptance by more
than 90%, demotes malicious validators within two rounds, and improves proposer
fairness versus standard PoS, all with no more than a 5% throughput overhead.
By linking consensus influence to verifiably trustworthy conduct, PoB offers a
scalable, regulation-friendly foundation for secure and fair blockchain
governance in financial applications.

</details>


### [42] [MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism](https://arxiv.org/abs/2506.22175)
*Zheng Zhang,Donglin Yang,Yaqi Xia,Liang Ding,Dacheng Tao,Xiaobo Zhou,Dazhao Cheng*

Main category: cs.DC

TL;DR: MPipeMoE是一个高性能库，通过自适应和高效内存的管道并行技术加速MoE训练，实现了2.8倍的速度提升和47%的内存消耗减少。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE技术在扩展预训练模型方面表现出色，但其在通信和内存消耗方面的低效问题亟待解决。

Method: 设计了自适应管道并行技术，并提出了内存重用策略以减少冗余，同时开发了自适应选择组件来优化运行时策略。

Result: 在8台NVIDIA DGX A100服务器集群中测试，MPipeMoE比现有技术快2.8倍，内存消耗减少47%。

Conclusion: MPipeMoE显著提升了MoE训练的效率和内存利用率，为大规模模型训练提供了实用解决方案。

Abstract: Recently, Mixture-of-Experts (MoE) has become one of the most popular
techniques to scale pre-trained models to extraordinarily large sizes. Dynamic
activation of experts allows for conditional computation, increasing the number
of parameters of neural networks, which is critical for absorbing the vast
amounts of knowledge available in many deep learning areas. However, despite
the existing system and algorithm optimizations, there are significant
challenges to be tackled when it comes to the inefficiencies of communication
and memory consumption.
  In this paper, we present the design and implementation of MPipeMoE, a
high-performance library that accelerates MoE training with adaptive and
memory-efficient pipeline parallelism. Inspired by that the MoE training
procedure can be divided into multiple independent sub-stages, we design
adaptive pipeline parallelism with an online algorithm to configure the
granularity of the pipelining. Further, we analyze the memory footprint
breakdown of MoE training and identify that activations and temporary buffers
are the primary contributors to the overall memory footprint. Toward memory
efficiency, we propose memory reusing strategies to reduce memory requirements
by eliminating memory redundancies, and develop an adaptive selection component
to determine the optimal strategy that considers both hardware capacities and
model characteristics at runtime. We implement MPipeMoE upon PyTorch and
evaluate it with common MoE models in a physical cluster consisting of 8 NVIDIA
DGX A100 servers. Compared with the state-of-art approach, MPipeMoE achieves up
to 2.8x speedup and reduces memory footprint by up to 47% in training large
models.

</details>


### [43] [Towards Operational Data Analytics Chatbots -- Virtual Knowledge Graph is All You Need](https://arxiv.org/abs/2506.22267)
*Junaid Ahmed Khan,Hiari Pizzini Cavagna,Andrea Proia,Andrea Bartolini*

Main category: cs.DC

TL;DR: 论文提出了一种基于大型语言模型(LLM)和虚拟知识图谱(VKG)的端到端ODA聊天机器人系统，显著提高了查询准确率和效率。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能对科学计算的挑战，数据中心的规模和容量激增，计算效率变得至关重要。传统ODA方法难以应对无模式NoSQL数据库的查询问题。

Method: 采用LLM生成SPARQL查询，结合VKG进行实时数据检索，优化VKG构建和LLM推理效率。

Result: 系统查询准确率提升至92.5%，较直接NoSQL查询(25%)显著提高；查询延迟从20.36秒降至3.03秒，VKG大小控制在179 MiB内。

Conclusion: 该方法适合部署并与ODA终端用户实时交互，为数据中心效率提升提供了可行方案。

Abstract: With generative artificial intelligence challenging computational scientific
computing, data centers are experiencing unprecedented growth in both scale and
volume. As a result, computing efficiency has become more critical than ever.
Operational Data Analytics (ODA) relies on the collection of data center
telemetry to improve efficiency, but so far has been focusing on real-time
telemetry data visualization and post-mortem analysis. However, with NoSQL
databases now serving as the default storage backend to support scalability,
querying this data is challenging due to its schema-less nature, which requires
domain knowledge to traverse relationships between data sources. Ontologies and
Knowledge Graphs (KGs) can capture these relationships, but traditional KGs are
costly to scale and have not been widely applied to multivariate timeseries.
Virtual Knowledge Graphs (VKGs) offer a lightweight alternative by generating
query-specific graphs at runtime. In this work, we present a full end-to-end
ODA chatbot system that uses a Large Language Model (LLM) to generate SPARQL
queries, utilizing VKG for data retrieval. This approach achieves 92.5%
accuracy compared to 25% with direct NoSQL queries. The proposed methodology
optimizes VKG construction and LLM inference, cutting previous work average
query latency by 85% (from 20.36s to 3.03s) and keeping VKG sizes under 179
MiB. This performance makes the tool suitable for deployment and real-time
interaction with ODA end-users.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [44] [Revisiting Graph Analytics Benchmark](https://arxiv.org/abs/2506.21811)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Xue Li,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Main category: cs.DB

TL;DR: 本文提出了一种新型图分析基准测试，通过精选核心算法、设计灵活的数据生成器并引入基于LLM的API可用性评估框架，改进了现有基准测试的不足。实验结果验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有图分析基准测试在核心算法选择、数据生成和API可用性评估方面存在局限，亟需改进。

Method: 1. 精选8种核心算法；2. 设计高效灵活的数据生成器并生成8个新数据集；3. 引入基于LLM的多层次API可用性评估框架。

Result: 在多个现有图分析平台上的实验结果表明，新基准测试表现优越。

Conclusion: 新基准测试解决了现有工具的不足，为图分析平台性能评估提供了更全面的方案。

Abstract: The rise of graph analytics platforms has led to the development of various
benchmarks for evaluating and comparing platform performance. However, existing
benchmarks often fall short of fully assessing performance due to limitations
in core algorithm selection, data generation processes (and the corresponding
synthetic datasets), as well as the neglect of API usability evaluation. To
address these shortcomings, we propose a novel graph analytics benchmark.
First, we select eight core algorithms by extensively reviewing both academic
and industrial settings. Second, we design an efficient and flexible data
generator and produce eight new synthetic datasets as the default datasets for
our benchmark. Lastly, we introduce a multi-level large language model
(LLM)-based framework for API usability evaluation-the first of its kind in
graph analytics benchmarks. We conduct comprehensive experimental evaluations
on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and
G-thinker). The experimental results demonstrate the superiority of our
proposed benchmark.

</details>


### [45] [A Survey of LLM Inference Systems](https://arxiv.org/abs/2506.21901)
*James Pan,Guoliang Li*

Main category: cs.DB

TL;DR: 这篇论文综述了大型语言模型(LLM)推理系统的技术，包括请求处理、模型优化、内存管理等，并探讨了这些技术如何通过负载预测和自适应机制克服自回归生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，设计高性能推理系统变得至关重要，但目前缺乏对这些技术的系统化分析和比较。

Method: 论文从请求处理的算法和操作符开始，逐步探讨模型优化、执行技术（如内核设计、批处理和调度）以及内存管理技术（如分页内存、量化等）。

Result: 研究表明，这些技术依赖于负载预测、自适应机制和成本降低，以实现高性能推理。

Conclusion: 论文总结了单副本和多副本推理系统的构建方法，并指出了现有挑战和未来研究方向。

Abstract: The past few years has witnessed specialized large language model (LLM)
inference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside
rapid LLM adoption via services like ChatGPT. Driving these system design
efforts is the unique autoregressive nature of LLM request processing,
motivating new techniques for achieving high performance while preserving high
inference quality over high-volume and high-velocity workloads. While many of
these techniques are discussed across the literature, they have not been
analyzed under the framework of a complete inference system, nor have the
systems themselves been analyzed and compared.
  In this survey, we review these techniques, starting from operators and
algorithms for request processing, then moving on to techniques for model
optimization and execution, including kernel design, batching, and scheduling,
before ending with techniques for memory management, including paged memory,
eviction and offloading techniques, quantization, and cache persistence.
Through these discussions, we show that these techniques fundamentally rely on
load prediction, adaptive mechanisms, and cost reduction in order to overcome
the challenges introduced by autoregressive generation and achieve the goals of
the system. We then discuss how these techniques can be combined to form
single-replica and multi-replica inference systems, including disaggregated
inference systems that offer more control over resource allocation and
serverless systems that can be deployed over shared hardware infrastructure. We
end with a discussion of remaining challenges.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [46] [Power- and Area-Efficient Unary Sorting Architecture Using FSM-Based Unary Number Generator](https://arxiv.org/abs/2506.22107)
*Amir Hossein Jalilvand,M. Hassan Najafi*

Main category: cs.AR

TL;DR: 提出了一种基于有限状态机的升序一元排序模块，显著降低了实现成本，相比现有设计面积减少82%，功耗降低70%。


<details>
  <summary>Details</summary>
Motivation: 一元计算作为一种低成本和高效的硬件排序范式，现有设计因复杂的一元数生成器导致面积和功耗开销大。

Method: 通过两状态有限状态机生成右对齐一元流，迭代识别最小值，无需传统比较器。

Result: 在45nm技术节点上，面积和功耗分别减少82%和70%。

Conclusion: 该排序器为资源受限的硬件系统提供了高效解决方案。

Abstract: Sorting is a fundamental operation in computer systems and is widely used in
applications such as databases, data analytics, and hardware accelerators.
Unary computing has recently emerged as a low-cost and power-efficient paradigm
for implementing hardware sorters by eliminating the need for complex
arithmetic operations. However, existing comparison-free unary computing-based
designs suffer from significant area and power overhead due to costly unary
number generators.
  In this paper, we present a novel ascending-order unary sorting module
featuring a finite-state-machine-based unary number generator that
significantly reduces implementation costs. By generating right-aligned unary
streams using a two-state finite-state machine, our architecture iteratively
identifies the minimum input value in each cycle without conventional
comparators. Synthesis results in a 45nm technology node demonstrate up to 82%
reduction in area and 70% reduction in power consumption compared to
state-of-the-art unary designs. The proposed sorter offers a promising solution
for energy-constrained and resource-limited hardware systems.

</details>


### [47] [Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction](https://arxiv.org/abs/2506.22156)
*Mattia Ricchi,Fabrizio Alfonsi,Camilla Marella,Marco Barbieri,Alessandra Retico,Leonardo Brizi,Alessandro Gabrielli,Claudia Testa*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的神经网络方法，用于从磁共振指纹数据中实时重建大脑参数，训练时间显著缩短。


<details>
  <summary>Details</summary>
Motivation: 传统MRF技术需要大量资源训练神经网络，限制了实时性应用。

Method: 采用FPGA加速神经网络的训练和推理过程。

Result: 训练时间降至200秒，比传统CPU方法快250倍。

Conclusion: 该方法有望实现移动设备上的实时大脑分析，推动临床决策和远程医疗的发展。

Abstract: Magnetic Resonance Fingerprinting (MRF) is a fast quantitative MR Imaging
technique that provides multi-parametric maps with a single acquisition. Neural
Networks (NNs) accelerate reconstruction but require significant resources for
training. We propose an FPGA-based NN for real-time brain parameter
reconstruction from MRF data. Training the NN takes an estimated 200 seconds,
significantly faster than standard CPU-based training, which can be up to 250
times slower. This method could enable real-time brain analysis on mobile
devices, revolutionizing clinical decision-making and telemedicine.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [48] [In situ fine-tuning of in silico trained Optical Neural Networks](https://arxiv.org/abs/2506.22122)
*Gianluca Kosmella,Ripalta Stabile,Jaron Sanders*

Main category: cs.NE

TL;DR: 论文提出了一种名为GIFT的轻量级算法，用于解决光学神经网络(ONN)训练中因噪声不匹配导致的性能下降问题。GIFT利用梯度信息优化预训练参数，无需复杂重新训练。实验显示，在MNIST任务中，GIFT相对基线提升了28%的准确率。


<details>
  <summary>Details</summary>
Motivation: 由于物理实现中的噪声和制造缺陷，光学神经网络的数字模型与实际硬件之间存在不匹配问题，导致性能下降。本文旨在解决这一挑战。

Method: 提出Gradient-Informed Fine-Tuning (GIFT)算法，利用ONN的噪声结构梯度信息，直接在现场调整预训练参数，避免昂贵的重新训练。

Result: 在五层前馈ONN和MNIST任务上的仿真实验中，GIFT在噪声不匹配情况下将准确率相对提升了28%。

Conclusion: GIFT为弥补数字模型与实际ONN实现之间的差距提供了一种实用且高效的解决方案。

Abstract: Optical Neural Networks (ONNs) promise significant advantages over
traditional electronic neural networks, including ultrafast computation, high
bandwidth, and low energy consumption, by leveraging the intrinsic capabilities
of photonics. However, training ONNs poses unique challenges, notably the
reliance on simplified in silico models whose trained parameters must
subsequently be mapped to physical hardware. This process often introduces
inaccuracies due to discrepancies between the idealized digital model and the
physical ONN implementation, particularly stemming from noise and fabrication
imperfections.
  In this paper, we analyze how noise misspecification during in silico
training impacts ONN performance and we introduce Gradient-Informed Fine-Tuning
(GIFT), a lightweight algorithm designed to mitigate this performance
degradation. GIFT uses gradient information derived from the noise structure of
the ONN to adapt pretrained parameters directly in situ, without requiring
expensive retraining or complex experimental setups. GIFT comes with formal
conditions under which it improves ONN performance.
  We also demonstrate the effectiveness of GIFT via simulation on a five-layer
feed forward ONN trained on the MNIST digit classification task. GIFT achieves
up to $28\%$ relative accuracy improvement compared to the baseline performance
under noise misspecification, without resorting to costly retraining. Overall,
GIFT provides a practical solution for bridging the gap between simplified
digital models and real-world ONN implementations.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [49] [Asymptotic analysis and design of shell-based thermal lattice metamaterials](https://arxiv.org/abs/2506.22319)
*Di Zhang,Ligang Liu*

Main category: math.AP

TL;DR: 本文提出了一种严谨的渐近分析框架，用于研究壳格超材料的导热性能，并引入了一种新指标——渐近方向导热性（ADC），用于捕捉几何形状对导热性能的影响。


<details>
  <summary>Details</summary>
Motivation: 扩展先前关于机械刚度的研究，探讨壳格超材料在热传导方面的性能，为其优化设计提供理论依据。

Method: 通过渐近分析和新指标ADC的引入，建立了评估ADC的收敛定理、上界及其实现条件。

Result: 首次为三重周期极小曲面的最优导热性能提供了理论证明，并证明了ADC在低体积分数下对有效导热性的三阶近似。

Conclusion: 本研究不仅为壳格超材料的热传导性能提供了理论支撑，还开发了一种实用的优化算法，数值结果验证了其有效性和鲁棒性。

Abstract: We present a rigorous asymptotic analysis framework for investigating the
thermal conductivity of shell lattice metamaterials, extending prior work from
mechanical stiffness to heat transfer. Central to our analysis is a new metric,
the asymptotic directional conductivity (ADC), which captures the leading-order
influence of the middle surface geometry on the effective thermal conductivity
in the vanishing-thickness limit. A convergence theorem is established for
evaluating ADC, along with a sharp upper bound and the necessary and sufficient
condition for achieving this bound. These results provide the first theoretical
justification for the optimal thermal conductivity of triply periodic minimal
surfaces. Furthermore, we show that ADC yields a third-order approximation to
the effective conductivity of shell lattices at low volume fractions. To
support practical design applications, we develop a discrete algorithm for
computing and optimizing ADC over arbitrary periodic surfaces. Numerical
results confirm the theoretical predictions and demonstrate the robustness and
effectiveness of the proposed optimization algorithm.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [50] [Nets-within-Nets through the Lens of Data Nets](https://arxiv.org/abs/2506.22344)
*Francesco Di Cosmo,Soumodev Mal,Tephilla Prince*

Main category: cs.CC

TL;DR: 研究在非确定性令牌丢失情况下基本对象系统（EOSs）的可达性问题的复杂性，并将其与数据网络的覆盖性问题联系起来。


<details>
  <summary>Details</summary>
Motivation: 探索基本对象系统（EOSs）在令牌丢失情况下的复杂性，并将其与数据网络的表达能力和复杂性进行比较，以建立两种扩展Petri网方法之间的联系。

Method: 通过将保守EOSs（cEOSs）的覆盖性问题转化为数据网络的一个片段，利用已知的数据网络结果分析cEOSs的复杂性。

Result: 证明了cEOSs的覆盖性问题与数据网络中一个有趣片段的覆盖性问题等价，其复杂性介于Fω2和Fωω之间。

Conclusion: 通过建立cEOSs与数据网络之间的联系，不仅揭示了两种Petri网扩展方法的共性，还利用数据网络的结果分析了cEOSs的复杂性。

Abstract: Elementary Object Systems (EOSs) are a model in the nets-within-nets (NWNs)
paradigm, where tokens in turn can host standard Petri nets. We study the
complexity of the reachability problem of EOSs when subjected to
non-deterministic token losses. It is known that this problem is equivalent to
the coverability problem with no lossiness of conservative EOSs (cEOSs). We
precisely characterize cEOS coverability into the framework of data nets, whose
tokens carry data from an infinite domain. Specifically, we show that cEOS
coverability is equivalent to the coverability of an interesting fragment of
data nets that extends beyond $\nu$PNs (featuring globally fresh name
creation), yet remains less expressive than Unordered Data Nets (featuring
lossy name creation as well as powerful forms of whole-place operations and
broadcasts). This insight bridges two apparently orthogonal approaches to PN
extensions, namely data nets and NWNs. At the same time, it enables us to
analyze cEOS coverability taking advantage of known results on data nets. As a
byproduct, we immediately get that the complexity of cEOS coverability lies
between $\mathbf{F}_{\omega 2}$ and $\mathbf{F}_{\omega^\omega}$, two classes
beyond Primitive Recursive.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [51] [FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction](https://arxiv.org/abs/2506.21562)
*Jun Yin,Pengyu Zeng,Jing Zhong,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.CL

TL;DR: 该论文提出了一种基于自回归机制的渐进式平面图生成方法，以替代传统的端到端生成模型，更符合实际建筑设计的迭代工作流程。


<details>
  <summary>Details</summary>
Motivation: 现有的平面图生成模型多为端到端生成，与实际建筑设计的渐进式工作流程不匹配，因此需要一种更符合实际需求的方法。

Method: 借鉴大型语言模型中的自回归'下一个令牌预测'机制，提出了一种'下一个房间预测'范式，用于建筑平面图建模。

Result: 实验评估表明，FPDS在文本到平面图任务中表现优于扩散模型和Tell2Design。

Conclusion: FPDS展示了在支持未来智能建筑设计中的潜在适用性。

Abstract: In the architectural design process, floor plan generation is inherently
progressive and iterative. However, existing generative models for floor plans
are predominantly end-to-end generation that produce an entire pixel-based
layout in a single pass. This paradigm is often incompatible with the
incremental workflows observed in real-world architectural practice. To address
this issue, we draw inspiration from the autoregressive 'next token prediction'
mechanism commonly used in large language models, and propose a novel 'next
room prediction' paradigm tailored to architectural floor plan modeling.
Experimental evaluation indicates that FPDS demonstrates competitive
performance in comparison to diffusion models and Tell2Design in the
text-to-floorplan task, indicating its potential applicability in supporting
future intelligent architectural design.

</details>


### [52] [VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents](https://arxiv.org/abs/2506.21582)
*Sam Yu-Te Lee,Chengyang Ji,Shicheng Wen,Lifu Huang,Dongyi Liu,Kwan-Liu Ma*

Main category: cs.CL

TL;DR: VIDEE系统通过智能代理帮助非专家用户进行高级文本分析，包括分解、执行和评估三阶段流程，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统文本分析需要NLP专业知识，限制了入门级分析师的使用。大型语言模型（LLMs）的发展为自动化文本分析提供了可能。

Method: VIDEE采用人机协作流程：分解（蒙特卡洛树搜索结合人工反馈）、执行（生成可执行分析管道）、评估（基于LLM的验证与可视化）。

Result: 定量实验验证了VIDEE的有效性，用户研究表明系统对非专家用户友好，并揭示了用户行为模式。

Conclusion: VIDEE为非专家用户提供了实用工具，未来可进一步优化智能文本分析系统的人机协作设计。

Abstract: Text analytics has traditionally required specialized knowledge in Natural
Language Processing (NLP) or text analysis, which presents a barrier for
entry-level analysts. Recent advances in large language models (LLMs) have
changed the landscape of NLP by enabling more accessible and automated text
analysis (e.g., topic detection, summarization, information extraction, etc.).
We introduce VIDEE, a system that supports entry-level data analysts to conduct
advanced text analytics with intelligent agents. VIDEE instantiates a
human-agent collaroration workflow consisting of three stages: (1)
Decomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search
algorithm to support generative reasoning with human feedback, (2) Execution,
which generates an executable text analytics pipeline, and (3) Evaluation,
which integrates LLM-based evaluation and visualizations to support user
validation of execution results. We conduct two quantitative experiments to
evaluate VIDEE's effectiveness and analyze common agent errors. A user study
involving participants with varying levels of NLP and text analytics experience
-- from none to expert -- demonstrates the system's usability and reveals
distinct user behavior patterns. The findings identify design implications for
human-agent collaboration, validate the practical utility of VIDEE for
non-expert users, and inform future improvements to intelligent text analytics
systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model](https://arxiv.org/abs/2506.21851)
*Haofeng Wang,Fangtao Zhou,Qi Zhang,Zeyuan Chen,Enci Zhang,Zhao Wang,Xiaofeng Huang,Siwei Ma*

Main category: cs.CV

TL;DR: 提出了一种用于RGB-IR图像对联合压缩的框架，通过跨模态熵模型和低频信息提取块优化压缩效率。


<details>
  <summary>Details</summary>
Motivation: 随着RGB和红外图像在多模态应用中的广泛使用，存储和传输成本增加，需要高效的数据压缩方法。

Method: 设计了通道式跨模态熵模型（CCEM），包含低频上下文提取块（LCEB）和低频上下文融合块（LCFB），用于跨模态信息建模。

Result: 实验表明，该方法在LLVIP和KAIST数据集上优于现有方法，例如在LLVIP数据集上比特率节省了23.1%。

Conclusion: 该框架通过跨模态信息利用显著提升了RGB-IR图像对的压缩效率。

Abstract: RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in
various applications like intelligent surveillance. However, as the number of
modalities increases, the required data storage and transmission costs also
double. Therefore, efficient RGB-IR data compression is essential. This work
proposes a joint compression framework for RGB-IR image pair. Specifically, to
fully utilize cross-modality prior information for accurate context probability
modeling within and between modalities, we propose a Channel-wise
Cross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context
Extraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are
designed for extracting and aggregating the global low-frequency information
from both modalities, which assist the model in predicting entropy parameters
more accurately. Experimental results demonstrate that our approach outperforms
existing RGB-IR image pair and single-modality compression methods on LLVIP and
KAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate
saving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec
presented at CVPR 2022.

</details>


### [54] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Main category: cs.CV

TL;DR: LLaVA-Scissor是一种无需训练的token压缩策略，用于视频多模态大语言模型，通过基于语义连接组件（SCC）的方法实现高效压缩，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有token压缩方法主要基于注意力分数，无法有效捕获所有语义区域且易导致冗余。本文旨在解决这一问题。

Method: 提出两步骤时空token压缩策略，利用SCC在空间和时间域分配token到不同语义区域，确保全面语义覆盖。

Result: 在视频问答、长视频理解等多项评测中，LLaVA-Scissor表现优异，尤其在低token保留率下。

Conclusion: LLaVA-Scissor通过SCC实现高效token压缩，提升视频理解性能，优于现有方法。

Abstract: In this paper, we present LLaVA-Scissor, a training-free token compression
strategy designed for video multimodal large language models. Previous methods
mostly attempt to compress tokens based on attention scores, but fail to
effectively capture all semantic regions and often lead to token redundancy.
Differently, we propose to leverage the Semantic Connected Components (SCC)
approach that assigns tokens to distinct semantic regions within the token set,
ensuring comprehensive semantic coverage. The outcome is a two-step
spatio-temporal token compression strategy that utilizes SCC in both spatial
and temporal domains. This strategy can effectively compress tokens by
representing the entire video with a set of non-overlapping semantic tokens. We
conduct extensive evaluations of the token compression capabilities of
LLaVA-Scissor across diverse video understanding benchmarks, including video
question answering, long video understanding, and comprehensive multi-choices
benchmarks. Experimental results show that the proposed LLaVA-Scissor
outperforms other token compression methods, achieving superior performance in
various video understanding benchmarks, particularly at low token retention
ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.

</details>


### [55] [Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles](https://arxiv.org/abs/2506.21885)
*Chuheng Wei,Ziye Qin,Ziyan Zhang,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: 该论文系统性地综述了多传感器融合在自动驾驶中的应用，包括数据级、特征级和决策级策略，并探讨了深度学习方法、多模态数据集及新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 多传感器融合对于克服单一传感器的局限并提升自动驾驶的环境感知能力至关重要。

Method: 论文提出了数据级、特征级和决策级融合策略，并回顾了基于深度学习的方法。

Result: 提供了多模态数据集的应用分析，并探讨了视觉-语言模型和大语言模型的融合潜力。

Conclusion: 论文总结了当前方法，并展望了多传感器融合在提升自动驾驶系统适应性和鲁棒性方面的未来方向。

Abstract: Multi-sensor fusion plays a critical role in enhancing perception for
autonomous driving, overcoming individual sensor limitations, and enabling
comprehensive environmental understanding. This paper first formalizes
multi-sensor fusion strategies into data-level, feature-level, and
decision-level categories and then provides a systematic review of deep
learning-based methods corresponding to each strategy. We present key
multi-modal datasets and discuss their applicability in addressing real-world
challenges, particularly in adverse weather conditions and complex urban
environments. Additionally, we explore emerging trends, including the
integration of Vision-Language Models (VLMs), Large Language Models (LLMs), and
the role of sensor fusion in end-to-end autonomous driving, highlighting its
potential to enhance system adaptability and robustness. Our work offers
valuable insights into current methods and future directions for multi-sensor
fusion in autonomous driving.

</details>


### [56] [Generating Attribute-Aware Human Motions from Textual Prompt](https://arxiv.org/abs/2506.21912)
*Xinghan Wang,Kun Xu,Fei Li,Cao Sheng,Jiazhong Yu,Yadong Mu*

Main category: cs.CV

TL;DR: 本文提出了一种新的文本驱动人体动作生成框架，考虑了人体属性（如年龄、性别、体重和身高）的影响，通过解耦动作语义和属性信息，实现了属性和文本双重控制下的动作生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动人体动作生成方法忽略了人体属性对动作模式的影响，而作者认为这是关键因素。因此，本文旨在填补这一空白。

Method: 基于结构性因果模型（SCM）的框架，解耦动作语义与人体属性，实现文本到语义的预测和属性控制的动作生成。

Result: 提出的模型能够生成符合用户文本和属性输入的逼真动作，并通过新提出的数据集HumanAttr验证了其有效性。

Conclusion: 本文首次将人体属性纳入文本驱动动作生成，提出了新的框架和数据集，为未来研究奠定了基础。

Abstract: Text-driven human motion generation has recently attracted considerable
attention, allowing models to generate human motions based on textual
descriptions. However, current methods neglect the influence of human
attributes (such as age, gender, weight, and height) which are key factors
shaping human motion patterns. This work represents a pilot exploration for
bridging this gap. We conceptualize each motion as comprising both attribute
information and action semantics, where textual descriptions align exclusively
with action semantics. To achieve this, a new framework inspired by Structural
Causal Models is proposed to decouple action semantics from human attributes,
enabling text-to-semantics prediction and attribute-controlled generation. The
resulting model is capable of generating realistic, attribute-aware motion
aligned with the user's text and attribute inputs. For evaluation, we introduce
HumanAttr, a comprehensive dataset containing attribute annotations for
text-motion pairs, setting the first benchmark for attribute-aware
text-to-motion generation. Extensive experiments on the new dataset validate
our model's effectiveness.

</details>


### [57] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Main category: cs.CV

TL;DR: 该论文提出了一种针对人脸对齐中误差分布偏差的新方法，通过各向异性方向损失（ADL）和各向异性注意力模块（AAM）提升CNN模型的收敛性。


<details>
  <summary>Details</summary>
Motivation: 人脸对齐中的误差分布存在偏差，表现为沿地标曲线的切线方向扩散。这一偏差与模糊的地标标注任务相关，因此需要一种方法优化误差分布以实现更好的模型收敛。

Method: 论文提出了ADL和AAM，分别用于坐标回归和热图回归。ADL在法向施加强约束力，AAM则通过注意力模块在切线方向放松约束，两者互补学习。这些方法整合为ADNet训练框架。

Result: ADNet在300W、WFLW和COFW数据集上取得了最先进的结果，验证了其有效性和鲁棒性。

Conclusion: ADL和AAM的互补设计有效解决了人脸对齐中的误差偏差问题，为相关任务提供了新的优化方向。

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [58] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种通过现有稀疏地标数据集增强地标密度的框架，用于面部对齐任务。


<details>
  <summary>Details</summary>
Motivation: 针对稀疏面部地标在美容医学等场景中不足的问题，研究者希望提高地标的密度。

Method: 利用稀疏地标数据的局部相似性，提出弱监督学习方法来优化地标密度，设计了多个操作符实现该目标。

Result: 在密集和稀疏地标数据集上均达到了最先进的准确率，且无需额外成本。

Conclusion: 该方法为面部对齐任务提供了高效且可扩展的解决方案。

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [59] [PEACE: Empowering Geologic Map Holistic Understanding with MLLMs](https://arxiv.org/abs/2501.06184)
*Yangyu Huang,Tianyi Gao,Haoran Xu,Qihao Zhao,Yang Song,Zhipeng Gui,Tengchao Lv,Hao Chen,Lei Cui,Scarlett Li,Furu Wei*

Main category: cs.CV

TL;DR: 论文提出了GeoMap-Bench（首个地质图理解评估基准）和GeoMap-Agent（首个地质图理解智能体），通过多模块设计显著提升了MLLMs在地质图任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在地质图理解方面表现不足，主要由于地图通用的高分辨率、多组件和领域知识需求。

Method: 构建GeoMap-Bench基准，设计GeoMap-Agent（含HIE、DKI、PEQA模块）并引入AI专家组协作工具。

Result: GeoMap-Agent在基准测试中得分0.811，远超GPT-4o的0.369。

Conclusion: 工作为地质学AI应用铺路，提升了地质研究的效率和准确性。

Abstract: Geologic map, as a fundamental diagram in geology science, provides critical
insights into the structure and composition of Earth's subsurface and surface.
These maps are indispensable in various fields, including disaster detection,
resource exploration, and civil engineering. Despite their significance,
current Multimodal Large Language Models (MLLMs) often fall short in geologic
map understanding. This gap is primarily due to the challenging nature of
cartographic generalization, which involves handling high-resolution map,
managing multiple associated components, and requiring domain-specific
knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever
benchmark for evaluating MLLMs in geologic map understanding, which assesses
the full-scale abilities in extracting, referring, grounding, reasoning, and
analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent
designed for geologic map understanding, which features three modules:
Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),
and Prompt-enhanced Question Answering (PEQA). Inspired by the
interdisciplinary collaboration among human scientists, an AI expert group acts
as consultants, utilizing a diverse tool pool to comprehensively analyze
questions. Through comprehensive experiments, GeoMap-Agent achieves an overall
score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.
Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,
paves the way for advanced AI applications in geology, enhancing the efficiency
and accuracy of geological investigations.

</details>


### [60] [Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD](https://arxiv.org/abs/2506.22111)
*Ruthvik Bokkasam,Shankar Gangisetty,A. H. Abdul Hafez,C. V. Jawahar*

Main category: cs.CV

TL;DR: 该论文介绍了一个印度驾驶行人数据集，旨在解决非结构化环境中建模行人行为的复杂性，现有预测方法在新数据集上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中准确预测行人行为对安全至关重要，而现有数据集难以覆盖非结构化环境的复杂性。

Method: 提出一个包含高水平和低水平注释的印度驾驶行人数据集，涵盖光照变化、遮挡等挑战。

Result: 现有意图预测方法性能下降15%，轨迹预测方法的MSE增加1208，表现显著劣于标准数据集。

Conclusion: 新数据集为行人行为研究提供了新挑战，推动开发更鲁棒的模型。

Abstract: With the rapid advancements in autonomous driving, accurately predicting
pedestrian behavior has become essential for ensuring safety in complex and
unpredictable traffic conditions. The growing interest in this challenge
highlights the need for comprehensive datasets that capture unstructured
environments, enabling the development of more robust prediction models to
enhance pedestrian safety and vehicle navigation. In this paper, we introduce
an Indian driving pedestrian dataset designed to address the complexities of
modeling pedestrian behavior in unstructured environments, such as illumination
changes, occlusion of pedestrians, unsignalized scene types and
vehicle-pedestrian interactions. The dataset provides high-level and detailed
low-level comprehensive annotations focused on pedestrians requiring the
ego-vehicle's attention. Evaluation of the state-of-the-art intention
prediction methods on our dataset shows a significant performance drop of up to
$\mathbf{15\%}$, while trajectory prediction methods underperform with an
increase of up to $\mathbf{1208}$ MSE, defeating standard pedestrian datasets.
Additionally, we present exhaustive quantitative and qualitative analysis of
intention and trajectory baselines. We believe that our dataset will open new
challenges for the pedestrian behavior research community to build robust
models. Project Page:
https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [61] [Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations](https://arxiv.org/abs/2506.22237)
*Sebastian Murgul,Moritz Reiser,Michael Heizmann,Christoph Seibert*

Main category: cs.SD

TL;DR: 提出了一种基于卷积循环神经网络（CRNN）的音频与MIDI同步方法，比传统DTW方法准确率提升20%，结合DTW进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 解决钢琴演奏录音与MIDI文件松散对齐的问题，提升同步精度。

Method: 使用CRNN处理未对齐的钢琴卷和频谱图，训练时通过增强的MIDI模拟人为时间误差。

Result: CRNN比DTW准确率高20%，结合两者效果更佳。

Conclusion: 神经网络可显著提升MIDI与音频对齐技术，展现了其潜力。

Abstract: In this paper, we present a neural network approach for synchronizing audio
recordings of human piano performances with their corresponding loosely aligned
MIDI files. The task is addressed using a Convolutional Recurrent Neural
Network (CRNN) architecture, which effectively captures spectral and temporal
features by processing an unaligned piano roll and a spectrogram as inputs to
estimate the aligned piano roll. To train the network, we create a dataset of
piano pieces with augmented MIDI files that simulate common human timing
errors. The proposed model achieves up to 20% higher alignment accuracy than
the industry-standard Dynamic Time Warping (DTW) method across various
tolerance windows. Furthermore, integrating DTW with the CRNN yields additional
improvements, offering enhanced robustness and consistency. These findings
demonstrate the potential of neural networks in advancing state-of-the-art
MIDI-to-audio alignment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [62] [Under the Hood of BlotchyQuasar: DLL-Based RAT Campaigns Against Latin America](https://arxiv.org/abs/2506.22323)
*Alessio Di Santo*

Main category: cs.CR

TL;DR: 一篇论文分析了针对拉丁美洲（尤其是巴西）的恶意软件活动，使用钓鱼邮件和DLL侧加载技术传播一种名为BlotchyQuasar的恶意软件变种，具备多种恶意功能，包括窃取银行信息。


<details>
  <summary>Details</summary>
Motivation: 揭示并分析近期针对拉丁美洲的复杂恶意软件活动，以增强对此类威胁的认知和防御能力。

Method: 研究恶意软件的分发方式（钓鱼邮件和多阶段感染）、加载技术（DLL侧加载）以及恶意功能（如凭证窃取和键盘记录）。

Result: 发现BlotchyQuasar虽然开发仓促，但功能强大，威胁严重。

Conclusion: 强调需加强网络安全防御以应对此类高级威胁。

Abstract: A sophisticated malspam campaign was recently uncovered targeting Latin
American countries, with a particular focus on Brazil. This operation utilizes
a highly deceptive phishing email to trick users into executing a malicious MSI
file, initiating a multi-stage infection. The core of the attack leverages DLL
side-loading, where a legitimate executable from Valve Corporation is used to
load a trojanized DLL, thereby bypassing standard security defenses.
  Once active, the malware, a variant of QuasarRAT known as BlotchyQuasar, is
capable of a wide range of malicious activities. It is designed to steal
sensitive browser-stored credentials and banking information, the latter
through fake login windows mimicking well-known Brazilian banks. The threat
establishes persistence by modifying the Windows registry , captures user
keystrokes through keylogging , and exfiltrates stolen data to a
Command-and-Control (C2) server using encrypted payloads. Despite its advanced
capabilities, the malware code exhibits signs of rushed development, with
inefficiencies and poor error handling that suggest the threat actors
prioritized rapid deployment over meticulous design. Nonetheless, the campaign
extensive reach and sophisticated mechanisms pose a serious and immediate
threat to the targeted regions, underscoring the need for robust cybersecurity
defenses.

</details>


### [63] [Reliability Analysis of Smart Contract Execution Architectures: A Comparative Simulation Study](https://arxiv.org/abs/2506.22180)
*Önder Gürcan*

Main category: cs.CR

TL;DR: 该研究评估了智能合约执行架构的安全性，并通过仿真验证了Execute-Order-Validate架构在可靠性和安全性上的优势。


<details>
  <summary>Details</summary>
Motivation: 自主系统日益复杂和互联，需要可靠的安全解决方案，智能合约因其自动化和去中介化特性成为潜在方案。

Method: 开发了智能合约执行安全性评估模型，并通过物联网能源案例进行仿真，分析文献中的安全漏洞。

Result: Execute-Order-Validate架构在可靠性和安全性上表现更优。

Conclusion: 研究证明Execute-Order-Validate架构是更可靠的智能合约执行方案。

Abstract: The industrial market continuously needs reliable solutions to secure
autonomous systems. Especially as these systems become more complex and
interconnected, reliable security solutions are becoming increasingly
important. One promising solution to tackle this challenge is using smart
contracts designed to meet contractual conditions, avoid malicious errors,
secure exchanges, and minimize the need for reliable intermediaries. However,
smart contracts are immutable. Moreover, there are different smart contract
execution architectures (namely Order-Execute and Execute-Order-Validate) that
have different throughputs. In this study, we developed an evaluation model for
assessing the security of reliable smart contract execution. We then developed
a realistic smart contract enabled IoT energy case study. Finally, we simulate
the developed case study to evaluate several smart contract security
vulnerabilities reported in the literature. Our results show that the
Execute-Order-Validate architecture is more promising regarding reliability and
security.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [64] [SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge](https://arxiv.org/abs/2506.21819)
*Lena John,Kheir Eddine Farfar,Sören Auer,Oliver Karras*

Main category: cs.DL

TL;DR: 论文提出了一种基于5星LOD模型的知识表示进化模型SciMantify，逐步将PDF转换为语义化的知识图谱表示，通过人机协作优化语义标注，并在ORKG平台验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 科学出版物多为PDF格式，缺乏结构化与语义表示，限制了知识的可访问性与重用性。

Method: 提出五阶段进化模型SciMantify，结合人机协作进行语义标注，逐步优化科学知识的语义表示。

Result: 初步用户实验显示，该方法简化了科学知识预处理，降低了语义化工作量，并提升了与知识图谱结构的对齐。

Conclusion: SciMantify为科学知识提供了更灵活的语义表示方法，增强了其在知识图谱中的可访问性与重用性。

Abstract: Scientific publications, primarily digitized as PDFs, remain static and
unstructured, limiting the accessibility and reusability of the contained
knowledge. At best, scientific knowledge from publications is provided in
tabular formats, which lack semantic context. A more flexible, structured, and
semantic representation is needed to make scientific knowledge understandable
and processable by both humans and machines. We propose an evolution model of
knowledge representation, inspired by the 5-star Linked Open Data (LOD) model,
with five stages and defined criteria to guide the stepwise transition from a
digital artifact, such as a PDF, to a semantic representation integrated in a
knowledge graph (KG). Based on an exemplary workflow implementing the entire
model, we developed a hybrid approach, called SciMantify, leveraging tabular
formats of scientific knowledge, e.g., results from secondary studies, to
support its evolving semantification. In the approach, humans and machines
collaborate closely by performing semantic annotation tasks (SATs) and refining
the results to progressively improve the semantic representation of scientific
knowledge. We implemented the approach in the Open Research Knowledge Graph
(ORKG), an established platform for improving the findability, accessibility,
interoperability, and reusability of scientific knowledge. A preliminary user
experiment showed that the approach simplifies the preprocessing of scientific
knowledge, reduces the effort for the evolving semantification, and enhances
the knowledge representation through better alignment with the KG structures.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [65] [Single-shot HDR using conventional image sensor shutter functions and optical randomization](https://arxiv.org/abs/2506.22426)
*Xiang Dai,Kyrollos Yanny,Kristina Monakhova,Nicholas Antipa*

Main category: eess.IV

TL;DR: 论文提出了一种基于全局复位释放（GRR）快门模式和光学随机置换的单次曝光高动态范围（HDR）成像方法，解决了传统方法在高光区域和运动场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 克服传统多曝光HDR成像在动态场景中的运动伪影问题，以及现有单次曝光方法在高光区域的恢复能力不足。

Method: 利用GRR快门模式为不同行分配不同曝光时间，结合光学随机置换技术，通过优化问题和总变差图像先验恢复HDR数据。

Result: 仿真表明，该方法在像素饱和率高时（10%以上）优于其他单次曝光方法；实验原型使用低成本商用传感器和随机光纤束，动态范围达到73dB。

Conclusion: 该方法通过创新性的硬件设计和简单的计算先验，显著提升了单次曝光HDR成像的性能，尤其在像素饱和率高的场景下。

Abstract: High-dynamic-range (HDR) imaging is an essential technique for overcoming the
dynamic range limits of image sensors. The classic method relies on multiple
exposures, which slows capture time, resulting in motion artifacts when imaging
dynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR
data into a single exposure, then computationally recovering it. Many
established methods use strong image priors to recover improperly exposed image
detail. These approaches struggle with extended highlight regions. We utilize
the global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR
shutter mode applies a longer exposure time to rows closer to the bottom of the
sensor. We use optics that relay a randomly permuted (shuffled) image onto the
sensor, effectively creating spatially randomized exposures across the scene.
The exposure diversity allows us to recover HDR data by solving an optimization
problem with a simple total variation image prior. In simulation, we
demonstrate that our method outperforms other single-shot methods when many
sensor pixels are saturated (10% or more), and is competitive at a modest
saturation (1%). Finally, we demonstrate a physical lab prototype that uses an
off-the-shelf random fiber bundle for the optical shuffling. The fiber bundle
is coupled to a low-cost commercial sensor operating in GRR shutter mode. Our
prototype achieves a dynamic range of up to 73dB using an 8-bit sensor with
48dB dynamic range.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [66] [On Drug Delivery System Parameter Optimisation via Semantic Information Theory](https://arxiv.org/abs/2506.22137)
*Milica Lekić,Mohammad Zoofaghari,Ilangko Balasingham,Mladen Veletić*

Main category: cs.IT

TL;DR: 研究了基于语义信息理论的药物递送系统（DDS）设计，通过分子通信框架量化信息需求，优化DDS参数。


<details>
  <summary>Details</summary>
Motivation: 探讨如何在动态环境中利用语义信息理论指导DDS的设计与优化，以实现更高效的药物递送。

Method: 将DDS视为分子浓度通道，引入干预、生存函数及系统-环境相关性，量化语义信息需求。

Result: 提供了定量分析工具，可优化DDS参数，平衡成本、效果和准确性。

Conclusion: 该框架为DDS设计与优化提供了新工具，有望提升药物递送效率。

Abstract: We investigate the application of semantic information theory to drug
delivery systems (DDS) within the molecular communication (MC) framework. To
operationalise this, we observe a DDS as a molecular concentration-based
channel. Semantic information is defined as the amount of information required
for a DDS to achieve its therapeutic goal in a dynamic environment. We derive
it by introducing interventions, defined as modifications to DDS parameters, a
viability function, and system-environment correlations quantified via the
channel capacity. Here, the viability function represents DDS performance based
on a drug dose-response relationship. Our model considers a system capable of
inducing functional changes in a receiver cancer cell, where exceeding critical
DDS parameter values can significantly reduce performance or
cost-effectiveness. By analysing the MC-based DDS model through a semantic
information perspective, we examine how correlations between the internalised
particle concentration $(Y)$ and the particle concentration in the
extracellular environment $(X)$ evolve under interventions. The final catalogue
of results provides a quantitative basis for DDS design and optimisation,
offering a method to determine optimal DDS parameter values under constraints
such as chemical budget, desired effect and accuracy. Thus, the proposed
framework can serve as a novel tool for guiding DDS design and optimisation.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [67] [PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM Applications](https://arxiv.org/abs/2506.21593)
*Abu Hanif Muhammad Syarubany,Chang Dong Yoo*

Main category: cs.IR

TL;DR: PentaRAG通过五层模块优化LLM在企业文档部署中的响应速度和GPU成本，显著提升缓存效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 企业部署大语言模型需要快速响应和成本可控，现有RAG技术未能完全满足这些需求。

Method: 采用五层模块（缓存、语义缓存、记忆召回、会话记忆和传统检索层），结合Mistral-8B等技术实现。

Result: 测试显示，PentaRAG提升答案相似度8%，准确性16%，查询延迟降至0.248秒，吞吐量达10万QPS。

Conclusion: 分层路由策略能同时实现新鲜度、速度和效率，适用于生产级RAG系统。

Abstract: Enterprise deployments of large-language model (LLM) demand continuously
changing document collections with sub-second latency and predictable GPU cost
requirements that classical Retrieval-Augmented Generation (RAG) pipelines only
partially satisfy. We present PentaRAG, a five-layer module that routes each
query through two instant caches (fixed key-value and semantic), a
memory-recall mode that exploits the LLM's own weights, an adaptive session
memory, and a conventional retrieval-augmentation layer. Implemented with
Mistral-8B, Milvus and vLLM, the system can answer most repeated or
semantically similar questions from low-latency caches while retaining full
retrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined
with the memory-recall layer raises answer similarity by approximately 8% and
factual correctness by approximately 16% over the base model. Under a
nine-session runtime simulation, cache warming reduces mean latency from
several seconds to well below one second and shifts traffic toward the fast
paths. Resource-efficiency tests show that PentaRAG cuts average GPU time to
0.248 seconds per query, roughly half that of a naive RAG baseline, and
sustains an aggregate throughput of approximately 100,000 queries per second on
our setup. These results demonstrate that a layered routing strategy can
deliver freshness, speed, and efficiency simultaneously in production-grade RAG
systems.

</details>


### [68] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Main category: cs.IR

TL;DR: 提出了一种系统性、定量化的多模态生成AI评估框架，通过优化模态权重提升企业文档智能的可信度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态生成AI的评估框架难以建立可信度，影响了可靠性和企业采用率。

Method: 引入了一种定量基准测试框架，结合文本、图像、标题和OCR等跨模态输入，建立技术指标与用户信任度的定量关系。

Result: 优化模态权重（30%文本、15%图像、25%标题、30%OCR）比纯文本基线性能提升57.3%，同时保持计算效率。

Conclusion: 为多模态RAG在关键企业应用中的可信度提供了严格框架，推动了负责任AI的部署。

Abstract: Current evaluation frameworks for multimodal generative AI struggle to
establish trustworthiness, hindering enterprise adoption where reliability is
paramount. We introduce a systematic, quantitative benchmarking framework to
measure the trustworthiness of progressively integrating cross-modal inputs
such as text, images, captions, and OCR within VisualRAG systems for enterprise
document intelligence. Our approach establishes quantitative relationships
between technical metrics and user-centric trust measures. Evaluation reveals
that optimal modality weighting with weights of 30% text, 15% image, 25%
caption, and 30% OCR improves performance by 57.3% over text-only baselines
while maintaining computational efficiency. We provide comparative assessments
of foundation models, demonstrating their differential impact on
trustworthiness in caption generation and OCR extraction-a vital consideration
for reliable enterprise AI. This work advances responsible AI deployment by
providing a rigorous framework for quantifying and enhancing trustworthiness in
multimodal RAG for critical enterprise applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [REDELEX: A Framework for Relational Deep Learning Exploration](https://arxiv.org/abs/2506.22199)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: 本文介绍了REDELEX框架，用于评估不同复杂度的关系深度学习模型在70多个RDB上的表现，分析影响性能的因素。


<details>
  <summary>Details</summary>
Motivation: 关系数据库（RDB）是存储结构化信息的黄金标准，但缺乏对关系深度学习模型性能与RDB特性之间关系的分析。

Method: 提出REDELEX框架，评估多种复杂度RDL模型在70多个RDB上的性能，并与经典方法对比。

Result: 验证了RDL的优越性能，并揭示了模型复杂度、数据库大小和结构特性是影响性能的主要因素。

Conclusion: REDELEX为RDL模型性能分析提供了全面工具，并开源了RDB集合，推动研究发展。

Abstract: Relational databases (RDBs) are widely regarded as the gold standard for
storing structured information. Consequently, predictive tasks leveraging this
data format hold significant application promise. Recently, Relational Deep
Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized
as graph structures, enabling the application of various graph neural
architectures to effectively address these tasks. However, given its novelty,
there is a lack of analysis into the relationships between the performance of
various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for
evaluating RDL models of varying complexity on the most diverse collection of
over 70 RDBs, which we make available to the community. Benchmarked alongside
key representatives of classic methods, we confirm the generally superior
performance of RDL while providing insights into the main factors shaping
performance, including model complexity, database sizes and their structural
properties.

</details>


### [70] [Performance Prediction for Large Systems via Text-to-Text Regression](https://arxiv.org/abs/2506.21718)
*Yash Akhauri,Bryan Lewandowski,Cheng-Hsi Lin,Adrian N. Reyes,Grant C. Forbes,Arissa Wongpanich,Bangding Yang,Mohamed S. Abdelfattah,Sagi Perel,Xingyou Song*

Main category: cs.LG

TL;DR: 提出了一种基于文本到文本回归的通用、可扩展方法，用于预测复杂系统数据的指标结果，优于传统表格回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统表格回归方法在复杂系统数据（如配置文件或系统日志）中效果有限，需要更通用的解决方案。

Method: 使用60M参数的编码器-解码器模型，从随机初始化开始训练，进行文本到文本回归。

Result: 在Google的Borg系统上，模型实现了0.99的排名相关性（平均0.9），均方误差比传统方法低100倍，并能轻松适应新任务。

Conclusion: 该方法为复杂系统结果的预测提供了新的可能性，展示了通用模拟器的潜力。

Abstract: In many industries, predicting metric outcomes of large systems is a
fundamental problem, driven largely by traditional tabular regression. However,
such methods struggle on complex systems data in the wild such as configuration
files or system logs, where feature engineering is often infeasible. We propose
text-to-text regression as a general, scalable alternative. For predicting
resource efficiency on Borg, Google's massive compute cluster scheduling
system, a 60M parameter encoder-decoder, trained from random initialization,
achieves up to a near perfect 0.99 (0.9 average) rank correlation across the
entire fleet, and 100x lower MSE than tabular approaches. The model also easily
adapts to new tasks in only 500 few-shot examples and captures the densities of
complex outcome distributions. Ablation studies highlight the importance of
using encoders, increasing sequence length, and the model's inherent
uncertainty quantification. These findings pave the way for universal
simulators of real-world outcomes.

</details>


### [71] [Hyper-modal Imputation Diffusion Embedding with Dual-Distillation for Federated Multimodal Knowledge Graph Completion](https://arxiv.org/abs/2506.22036)
*Ying Zhang,Yu Zhao,Xuhui Sui,Baohang Zhou,Xiangrui Cai,Li Shen,Xiaojie Yuan,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出了一个名为FedMKGC的任务，通过联合训练多模态知识图谱（MKGs）来预测缺失链接，同时保护敏感知识不外泄。提出的MMFeD3-HidE框架解决了多模态不确定性和异构性问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态知识图谱分散且缺乏协作的问题，同时确保推理能力和传输安全。

Method: 提出HidE模型恢复多模态分布，并设计MMFeD3框架实现客户端与服务器间的互蒸馏。

Result: 实验证明MMFeD3-HidE在有效性、语义一致性和收敛鲁棒性方面表现优异。

Conclusion: MMFeD3-HidE成功解决了FedMKGC任务中的挑战，并提供了基准测试支持。

Abstract: With the increasing multimodal knowledge privatization requirements,
multimodal knowledge graphs in different institutes are usually decentralized,
lacking of effective collaboration system with both stronger reasoning ability
and transmission safety guarantees. In this paper, we propose the Federated
Multimodal Knowledge Graph Completion (FedMKGC) task, aiming at training over
federated MKGs for better predicting the missing links in clients without
sharing sensitive knowledge. We propose a framework named MMFeD3-HidE for
addressing multimodal uncertain unavailability and multimodal client
heterogeneity challenges of FedMKGC. (1) Inside the clients, our proposed
Hyper-modal Imputation Diffusion Embedding model (HidE) recovers the complete
multimodal distributions from incomplete entity embeddings constrained by
available modalities. (2) Among clients, our proposed Multimodal FeDerated Dual
Distillation (MMFeD3) transfers knowledge mutually between clients and the
server with logit and feature distillation to improve both global convergence
and semantic consistency. We propose a FedMKGC benchmark for a comprehensive
evaluation, consisting of a general FedMKGC backbone named MMFedE, datasets
with heterogeneous multimodal information, and three groups of constructed
baselines. Experiments conducted on our benchmark validate the effectiveness,
semantic consistency, and convergence robustness of MMFeD3-HidE.

</details>
