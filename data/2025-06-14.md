<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 24]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 13]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.LG](#cs.LG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CY](#cs.CY) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [eess.SP](#eess.SP) [Total: 2]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CV](#cs.CV) [Total: 7]
- [math.OC](#math.OC) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [TrioXpert: An automated incident management framework for microservice system](https://arxiv.org/abs/2506.10043)
*Yongqian Sun,Yu Luo,Xidao Wen,Yuan Yuan,Xiaohui Nie,Shenglin Zhang,Tong Liu,Xi Luo*

Main category: cs.SE

TL;DR: 论文提出TrioXpert，一种基于多模态数据和LLMs的端到端事件管理框架，显著提升了异常检测、故障分类和根因定位的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单模态数据且难以同时处理多个下游任务，且缺乏解释性。

Method: TrioXpert设计多模态数据处理管道，利用LLMs协作推理机制，结合数值和文本数据。

Result: 在两个数据集上，TrioXpert在AD、FT和RCL任务上性能显著提升，最高达163.1%。

Conclusion: TrioXpert通过多模态和数据驱动的推理机制，有效提升事件管理的性能和解释性。

Abstract: Automated incident management plays a pivotal role in large-scale
microservice systems. However, many existing methods rely solely on
single-modal data (e.g., metrics, logs, and traces) and struggle to
simultaneously address multiple downstream tasks, including anomaly detection
(AD), failure triage (FT), and root cause localization (RCL). Moreover, the
lack of clear reasoning evidence in current techniques often leads to
insufficient interpretability. To address these limitations, we propose
TrioXpert, an end-to-end incident management framework capable of fully
leveraging multimodal data. TrioXpert designs three independent data processing
pipelines based on the inherent characteristics of different modalities,
comprehensively characterizing the operational status of microservice systems
from both numerical and textual dimensions. It employs a collaborative
reasoning mechanism using large language models (LLMs) to simultaneously handle
multiple tasks while providing clear reasoning evidence to ensure strong
interpretability. We conducted extensive evaluations on two popular
microservice system datasets, and the experimental results demonstrate that
TrioXpert achieves outstanding performance in AD (improving by 4.7% to 57.7%),
FT (improving by 2.1% to 40.6%), and RCL (improving by 1.6% to 163.1%) tasks.

</details>


### [2] [Reward Models Enable Scalable Code Verification by Trading Accuracy for Throughput](https://arxiv.org/abs/2506.10056)
*Gabriel Orlanski,Nicholas Roberts,Aws Albarghouthi,Frederic Sala*

Main category: cs.SE

TL;DR: 论文挑战了优先使用全面验证器而非结果奖励模型(ORM)的共识，系统探索了速度与准确性的权衡。研究发现，ORM在通过牺牲部分准确性来提升验证速度方面至关重要，尤其在生成-剪枝-排序方法中表现突出。


<details>
  <summary>Details</summary>
Motivation: 旨在挑战当前关于在代码任务中优先使用全面验证器而非ORM的共识，探索速度与准确性之间的权衡。

Method: 采用生成-剪枝-排序方法，利用快速但不完全准确的验证器在排序前移除错误解决方案。

Result: 该系统比全面测试套件快11.65倍，仅牺牲8.33%的准确性。

Conclusion: ORM在可扩展和准确的程序排序系统设计中具有重要价值，尤其在生成-剪枝-排序方法中效果显著。

Abstract: The standard paradigm for solving coding tasks via large language models
(LLMs) is to generate-then-rank programs, where the latter step uses a verifier
in the ranking process. The growing consensus is that a comprehensive verifier
(e.g., a full test suite) should be prioritized over an outcome reward model
(ORM) whenever possible, with little consideration given to the trade-offs
involved. We aim to challenge this assumption by systematically exploring the
tradeoff between speed and accuracy. We find that ORMs play a crucial role in
scaling verification through trading accuracy for speed, even when a
comprehensive verifier is available. Their value becomes especially apparent
when used in a generate-prune-then-rank approach, where a faster but less
accurate verifier removes incorrect solutions prior to ranking -- leading to a
system that is 11.65x faster while only being 8.33% less accurate than the full
test suite. We analyze the generate-prune-then-rank approach and show that it
works by filtering out incorrect but highly ranked solutions. These findings
enable the design of scalable and accurate program ranking systems.

</details>


### [3] [Online Discovery of Simulation Models for Evolving Business Processes (Extended Version)](https://arxiv.org/abs/2506.10049)
*Francesco Vinci,Gyunam Park,Wil van der Aalst,Massimiliano de Leoni*

Main category: cs.SE

TL;DR: 提出了一种流式业务流程仿真发现技术，结合增量流程发现与在线机器学习方法，以适应动态业务环境。


<details>
  <summary>Details</summary>
Motivation: 传统业务流程仿真发现技术缺乏对实时操作变化的适应性，无法满足动态业务环境中持续优化的需求。

Method: 集成增量流程发现与在线机器学习方法，优先考虑近期数据同时保留历史信息。

Result: 在四种不同事件日志上的实验表明，该技术能生成更稳定的仿真模型，并有效处理概念漂移。

Conclusion: 该技术通过动态调整数据权重，提升了仿真模型的适应性和稳定性。

Abstract: Business Process Simulation (BPS) refers to techniques designed to replicate
the dynamic behavior of a business process. Many approaches have been proposed
to automatically discover simulation models from historical event logs,
reducing the cost and time to manually design them. However, in dynamic
business environments, organizations continuously refine their processes to
enhance efficiency, reduce costs, and improve customer satisfaction. Existing
techniques to process simulation discovery lack adaptability to real-time
operational changes. In this paper, we propose a streaming process simulation
discovery technique that integrates Incremental Process Discovery with Online
Machine Learning methods. This technique prioritizes recent data while
preserving historical information, ensuring adaptation to evolving process
dynamics. Experiments conducted on four different event logs demonstrate the
importance in simulation of giving more weight to recent data while retaining
historical knowledge. Our technique not only produces more stable simulations
but also exhibits robustness in handling concept drift, as highlighted in one
of the use cases.

</details>


### [4] [Solving Package Management via Hypergraph Dependency Resolution](https://arxiv.org/abs/2506.10803)
*Ryan Gibb,Patrick Ferris,David Allsopp,Michael Winston Dales,Mark Elvers,Thomas Gazagnaire,Sadiq Jaffer,Thomas Leonard,Jon Ludlam,Anil Madhavapeddy*

Main category: cs.SE

TL;DR: HyperRes是一种超图形式化系统，解决了多语言项目依赖管理的互操作性问题，支持跨生态系统依赖解析。


<details>
  <summary>Details</summary>
Motivation: 多语言项目中依赖管理跨生态系统的问题存在，现有包管理器缺乏互操作性。

Method: 定义HyperRes系统，通过超图建模依赖关系，将多种包管理器翻译为HyperRes表达。

Result: 实现了跨生态系统依赖解析，无需用户切换包管理器。

Conclusion: HyperRes提供了一种高效解决多生态系统依赖管理的方法。

Abstract: Package managers are everywhere, with seemingly every language and operating
system implementing their own solution. The lack of interoperability between
these systems means that multi-lingual projects are unable to express precise
dependencies across language ecosystems, and external system and hardware
dependencies are typically implicit and unversioned. We define HyperRes, a
formal system for describing versioned dependency resolution using a hypergraph
that is expressive enough to model many ecosystems and solve dependency
constraints across them. We define translations from dozens of existing package
managers to HyperRes and comprehensively demonstrate that dependency resolution
can work across ecosystems that are currently distinct. This does not require
users to shift their choice of package managers; instead, HyperRes allows for
the translation of packaging metadata between ecosystems, and for solving to be
precisely specialised to a particular deployment environment.

</details>


### [5] [The Effects of GitHub Copilot on Computing Students' Programming Effectiveness, Efficiency, and Processes in Brownfield Programming Tasks](https://arxiv.org/abs/2506.10051)
*Md Istiak Hossain Shihab,Christopher Hundhausen,Ahsun Tariq,Summit Haque,Yunhan Qiao,Brian Mulanda*

Main category: cs.SE

TL;DR: GitHub Copilot显著提高了学生在遗留代码库中的编程效率和进度，但也引发了对其建议理解的担忧。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI编码助手（如GitHub Copilot）在学生处理遗留代码库任务中的影响。

Method: 通过实验对比10名学生在有/无Copilot情况下完成相似任务的性能、行为和理解。

Result: 使用Copilot时，学生任务完成快35%，进度多50%，代码手动编写时间少11%，搜索时间少12%。

Conclusion: 需开发新教学方法，利用AI助手优势并促进对其建议的反思。

Abstract: When graduates of computing degree programs enter the software industry, they
will most likely join teams working on legacy code bases developed by people
other than themselves. In these so-called brownfield software development
settings, generative artificial intelligence (GenAI) coding assistants like
GitHub Copilot are rapidly transforming software development practices, yet the
impact of GenAI on student programmers performing brownfield development tasks
remains underexplored. This paper investigates how GitHub Copilot influences
undergraduate students' programming performance, behaviors, and understanding
when completing brownfield programming tasks in which they add new code to an
unfamiliar code base. We conducted a controlled experiment in which 10
undergraduate computer science students completed highly similar brownfield
development tasks with and without Copilot in a legacy web application. Using a
mixed-methods approach combining performance analysis, behavioral analysis, and
exit interviews, we found that students completed tasks 35% faster (p < 0.05)
and made 50% more solution progress p (< 0.05) when using Copilot. Moreover,
our analysis revealed that, when using Copilot, students spent 11% less time
manually writing code (p < 0.05), and 12% less time conducting web searches (p
< 0.05), providing evidence of a fundamental shift in how they engaged in
programming. In exit interviews, students reported concerns about not
understanding how or why Copilot suggestions work. This research suggests the
need for computing educators to develop new pedagogical approaches that
leverage GenAI assistants' benefits while fostering reflection on how and why
GenAI suggestions address brownfield programming tasks. Complete study results
and analysis are presented at https://ghcopilot-icer.github.io/.

</details>


### [6] [Prompt Variability Effects On LLM Code Generation](https://arxiv.org/abs/2506.10204)
*Andrei Paleyes,Radzim Sendyka,Diana Robinson,Christian Cabrera,Neil D. Lawrence*

Main category: cs.SE

TL;DR: 该论文提出了一种独立于具体编程任务和LLM的代码生成评估方法，包括合成评估流程和基于角色的系统评估。


<details>
  <summary>Details</summary>
Motivation: 量化LLM对输入变化的敏感性，揭示生成代码的质量如何随用户背景变化。

Method: 提出合成评估流程和基于角色的评估方法，不依赖具体任务和模型。

Result: 实验证明了方法的有效性，并公开了代码。

Conclusion: 方法广泛适用，有助于评估和改进LLM的代码生成质量。

Abstract: Code generation is one of the most active areas of application of Large
Language Models (LLMs). While LLMs lower barriers to writing code and
accelerate development process, the overall quality of generated programs
depends on the quality of given prompts. Specifically, functionality and
quality of generated code can be sensitive to user's background and familiarity
with software development. It is therefore important to quantify LLM's
sensitivity to variations in the input. To this end we propose a synthetic
evaluation pipeline for code generation with LLMs, as well as a systematic
persona-based evaluation approach to expose qualitative differences of LLM
responses dependent on prospective user background. Both proposed methods are
completely independent from specific programming tasks and LLMs, and thus are
widely applicable. We provide experimental evidence illustrating utility of our
methods and share our code for the benefit of the community.

</details>


### [7] [AI-Based Software Vulnerability Detection: A Systematic Literature Review](https://arxiv.org/abs/2506.10280)
*Samiha Shimmi,Hamed Okhravi,Mona Rahimi*

Main category: cs.SE

TL;DR: 该论文系统综述了2018至2023年软件漏洞检测研究，发现91%的研究采用AI方法，图模型最为常见，并指出数据集质量、可复现性等局限性。


<details>
  <summary>Details</summary>
Motivation: 传统软件漏洞检测方法存在不足，AI驱动的方法成为研究趋势，需系统梳理当前研究进展与未来方向。

Method: 通过系统综述，对技术、特征表示和嵌入方法进行分类分析。

Result: 91%研究使用AI方法，图模型最流行；发现数据集质量、可复现性等关键问题。

Conclusion: 未来研究方向包括联邦学习、量子神经网络等未充分探索技术。

Abstract: Software vulnerabilities in source code pose serious cybersecurity risks,
prompting a shift from traditional detection methods (e.g., static analysis,
rule-based matching) to AI-driven approaches. This study presents a systematic
review of software vulnerability detection (SVD) research from 2018 to 2023,
offering a comprehensive taxonomy of techniques, feature representations, and
embedding methods. Our analysis reveals that 91% of studies use AI-based
methods, with graph-based models being the most prevalent. We identify key
limitations, including dataset quality, reproducibility, and interpretability,
and highlight emerging opportunities in underexplored techniques such as
federated learning and quantum neural networks, providing a roadmap for future
research.

</details>


### [8] [Minimizing False Positives in Static Bug Detection via LLM-Enhanced Path Feasibility Analysis](https://arxiv.org/abs/2506.10322)
*Xueying Du,Kai Yu,Chong Wang,Yi Zou,Wentai Deng,Zuoyu Ou,Xin Peng,Lingming Zhang,Yiling Lou*

Main category: cs.SE

TL;DR: 论文提出了一种名为LLM4PFA的迭代路径可行性分析框架，通过基于LLM的约束推理和上下文感知分析，显著降低了静态错误检测中的误报率。


<details>
  <summary>Details</summary>
Motivation: 现有静态错误检测工具在大型代码库中误报率高，尤其是路径可行性验证能力不足。

Method: 提出LLM4PFA框架，利用LLM代理进行约束推理和上下文感知分析，优化复杂路径可行性验证。

Result: 评估表明，LLM4PFA能过滤72%-96%的误报，性能提升41.1%-105.7%，且仅漏检3个真实错误。

Conclusion: LLM4PFA框架在减少静态错误检测误报方面表现优异，具有实际应用潜力。

Abstract: Static bug analyzers play a crucial role in ensuring software quality.
However, existing analyzers for bug detection in large codebases often suffer
from high false positive rates. This is primarily due to the limited
capabilities of analyzers in path feasibility validation with multiple
conditional branches and complex data dependencies. While current LLM-based
approaches attempt to address this issue, their effectiveness remains limited
due to insufficient constraint cascade analysis and scalability challenges in
large projects. To address this challenge, we propose an iterative path
feasibility analysis framework LLM4PFA. By leveraging LLM agent based targeted
constraint reasoning, and key context-aware analysis driven by agent planning,
LLM4PFA effectively enhances complex inter-procedural path feasibility analysis
for minimizing false positives in static bug detection. Evaluation results show
that LLM4PFA precisely filters out 72% to 96% false positives reported during
static bug detection, significantly outperforming all the baselines by 41.1% -
105.7% improvements; meanwhile LLM4PFA only misses 3 real bugs of 45 true
positives.

</details>


### [9] [Scalable Software Testing in Fast Virtual Platforms: Leveraging SystemC, QEMU and Containerization](https://arxiv.org/abs/2506.10624)
*Lukas Jünger,Jan Henrik Weinstock,Tim Kraus*

Main category: cs.SE

TL;DR: 提出了一种基于容器化的虚拟平台（VP）解决方案，用于解决硬件/软件系统复杂性带来的测试挑战，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 硬件/软件系统日益复杂，尤其是在安全关键领域，硬件可用性滞后阻碍了软件早期开发，需要一种高效解决方案。

Method: 利用SystemC TLM-2.0标准的虚拟平台，结合容器化技术，支持云端部署和并行化测试，并采用开源技术如QEMU和VCML。

Result: 通过AI加速器的案例研究验证了该方法的有效性，为加速硬件/软件协同开发提供了实用方案。

Conclusion: 容器化的虚拟平台能够有效解决硬件/软件系统复杂性带来的挑战，具有加速开发的潜力。

Abstract: The ever-increasing complexity of HW/SW systems presents a persistent
challenge, particularly in safety-critical domains like automotive, where
extensive testing is imperative. However, the availability of hardware often
lags behind, hindering early-stage software development. To address this,
Virtual Platforms (VPs) based on the SystemC TLM-2.0 standard have emerged as a
pivotal solution, enabling pre-silicon execution and testing of unmodified
target software. In this study, we propose an approach leveraging
containerization to encapsulate VPs in order to reduce environment dependencies
and enable cloud deployment for fast, parallelized test execution, as well as
open-source VP technologies such as QEMU and VCML to obviate the need for seat
licenses. To demonstrate the efficacy of our approach, we present an Artificial
Intelligence (AI) accelerator VP case study. Through our research, we offer a
robust solution to address the challenges posed by the complexity of HW/SW
systems, with practical implications for accelerating HW/SW co-development.

</details>


### [10] [Augmenting Large Language Models with Static Code Analysis for Automated Code Quality Improvements](https://arxiv.org/abs/2506.10330)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: 该研究通过将LLMs（如GPT-3.5 Turbo和GPT-4o）集成到软件开发流程中，实现了代码问题的自动检测与修订。结合静态代码分析和检索增强生成（RAG），显著减少了代码问题，提升了开发效率。


<details>
  <summary>Details</summary>
Motivation: 旨在利用LLMs的能力自动化检测和修复代码问题，从而提升代码质量、优化开发流程并减少资源消耗。

Method: 采用静态代码分析框架检测问题，利用LLMs生成修订建议，并通过RAG和“代码对比应用”提高准确性和减少幻觉。

Result: 结果显示，结合LLMs、静态分析和RAG的方法显著减少了代码问题，验证了其有效性。

Conclusion: LLMs与静态代码分析和RAG的结合，能有效提升代码质量和开发效率，具有实际应用价值。

Abstract: This study examined code issue detection and revision automation by
integrating Large Language Models (LLMs) such as OpenAI's GPT-3.5 Turbo and
GPT-4o into software development workflows. A static code analysis framework
detects issues such as bugs, vulnerabilities, and code smells within a
large-scale software project. Detailed information on each issue was extracted
and organized to facilitate automated code revision using LLMs. An iterative
prompt engineering process is applied to ensure that prompts are structured to
produce accurate and organized outputs aligned with the project requirements.
Retrieval-augmented generation (RAG) is implemented to enhance the relevance
and precision of the revisions, enabling LLM to access and integrate real-time
external knowledge. The issue of LLM hallucinations - where the model generates
plausible but incorrect outputs - is addressed by a custom-built "Code
Comparison App," which identifies and corrects erroneous changes before
applying them to the codebase. Subsequent scans using the static code analysis
framework revealed a significant reduction in code issues, demonstrating the
effectiveness of combining LLMs, static analysis, and RAG to improve code
quality, streamline the software development process, and reduce time and
resource expenditure.

</details>


### [11] [AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine](https://arxiv.org/abs/2506.10365)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Haoyue Jiao,Ziqi Liu,Lutong Xie,Chang Liu,Jianyuan Liang,Yaxian Qing,Xiaopu Zhang,Dehua Peng,Zhipeng Gui,Xuefeng Guan*

Main category: cs.SE

TL;DR: AutoGEEval++是一个用于评估大语言模型在Google Earth Engine上生成地理空间代码的自动化框架，支持多种数据类型和任务复杂度，并提供了标准化的评估协议和基准数据集。


<details>
  <summary>Details</summary>
Motivation: 地理空间代码生成在人工智能与地理科学分析结合中日益重要，但目前缺乏标准化的自动化评估工具。

Method: 基于GEE Python API构建AutoGEEval++框架，包含6,365个测试用例的基准数据集和端到端评估流程，采用多维度指标。

Result: 评估了24种先进的LLM模型，揭示了不同任务类型和模型设计的性能差异，验证了框架的实用性和可扩展性。

Conclusion: AutoGEEval++为GEE代码生成提供了首个标准化评估协议和基准，为领域特定代码评估奠定了基础。

Abstract: Geospatial code generation is becoming a key frontier in integrating
artificial intelligence with geo-scientific analysis, yet standardised
automated evaluation tools for this task remain absent. This study presents
AutoGEEval++, an enhanced framework building on AutoGEEval, and the first
automated assessment system for large language models (LLMs) generating
geospatial code on Google Earth Engine (GEE). It supports diverse data
modalities and varying task complexities. Built on the GEE Python API,
AutoGEEval++ features a benchmark dataset-AutoGEEval++-Bench-with 6,365 test
cases across 26 data types and three task categories: unit, combo, and theme
tests. It includes a submission programme and a judge module to realise an
end-to-end automated evaluation pipeline from code generation to
execution-based validation. The framework adopts multi-dimensional
metrics-accuracy, resource usage, run-time efficiency, and error
types-balancing hallucination control and efficiency, and enabling boundary
testing and error pattern analysis. Using AutoGEEval++, we evaluate 24
state-of-the-art LLMs (as of June 2025), including general-purpose,
reasoning-enhanced, code-centric, and geoscience-specific models. Results
reveal clear performance, stability, and error differences across task types,
model designs, and deployment settings, confirming AutoGEEval++'s practical
value and scalability in vertical-domain code generation. This work establishes
the first standardised evaluation protocol and foundational benchmark for
GEE-based LLM code generation, providing a unified basis for performance
comparison and a methodological framework for systematic, domain-specific code
evaluation.

</details>


### [12] [MLLM-Based UI2Code Automation Guided by UI Layout Information](https://arxiv.org/abs/2506.10376)
*Fan Wu,Cuiyun Gao,Shuqing Li,Xin-Cheng Wen,Qing Liao*

Main category: cs.SE

TL;DR: LayoutCoder是一种基于MLLM的新框架，用于从网页图像生成UI代码，通过三个关键模块解决现有方法在布局理解和代码生成上的不足，并在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动化UI2Code能显著提高开发效率，但现有深度学习方法依赖大量标注数据且泛化能力有限，MLLMs虽有潜力但难以理解复杂布局。

Method: 提出LayoutCoder框架，包括元素关系构建、UI布局解析和布局引导代码融合三个模块。

Result: 在Snap2Code和Design2Code数据集上，LayoutCoder优于现有方法，BLEU和CLIP分数平均提升10.14%和3.95%。

Conclusion: LayoutCoder通过多模态大语言模型有效解决了UI2Code中的布局理解和代码生成问题，显著提升了性能。

Abstract: Converting user interfaces into code (UI2Code) is a crucial step in website
development, which is time-consuming and labor-intensive. The automation of
UI2Code is essential to streamline this task, beneficial for improving the
development efficiency. There exist deep learning-based methods for the task;
however, they heavily rely on a large amount of labeled training data and
struggle with generalizing to real-world, unseen web page designs. The advent
of Multimodal Large Language Models (MLLMs) presents potential for alleviating
the issue, but they are difficult to comprehend the complex layouts in UIs and
generate the accurate code with layout preserved. To address these issues, we
propose LayoutCoder, a novel MLLM-based framework generating UI code from
real-world webpage images, which includes three key modules: (1) Element
Relation Construction, which aims at capturing UI layout by identifying and
grouping components with similar structures; (2) UI Layout Parsing, which aims
at generating UI layout trees for guiding the subsequent code generation
process; and (3) Layout-Guided Code Fusion, which aims at producing the
accurate code with layout preserved. For evaluation, we build a new benchmark
dataset which involves 350 real-world websites named Snap2Code, divided into
seen and unseen parts for mitigating the data leakage issue, besides the
popular dataset Design2Code. Extensive evaluation shows the superior
performance of LayoutCoder over the state-of-the-art approaches. Compared with
the best-performing baseline, LayoutCoder improves 10.14% in the BLEU score and
3.95% in the CLIP score on average across all datasets.

</details>


### [13] [Bug Classification in Quantum Software: A Rule-Based Framework and Its Evaluation](https://arxiv.org/abs/2506.10397)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 提出一个基于规则的自动化框架，用于量子软件问题分类，验证显示高准确度，但严重性分类有待改进。


<details>
  <summary>Details</summary>
Motivation: 提高软件质量需要对软件缺陷进行准确分类，特别是在量子计算领域。

Method: 基于关键词和启发式技术的规则框架，用于分类量子软件问题，并进行统计验证。

Result: 框架准确率达85.21%，F1分数0.7075-0.8393；量子特定问题占总问题的27.3%。

Conclusion: 框架在多数分类任务中表现出色，但严重性分类需进一步优化。

Abstract: Accurate classification of software bugs is essential for improving software
quality. This paper presents a rule-based automated framework for classifying
issues in quantum software repositories by bug type, category, severity, and
impacted quality attributes, with additional focus on quantum-specific bug
types. The framework applies keyword and heuristic-based techniques tailored to
quantum computing. To assess its reliability, we manually classified a
stratified sample of 4,984 issues from a dataset of 12,910 issues across 36
Qiskit repositories. Automated classifications were compared with ground truth
using accuracy, precision, recall, and F1-score. The framework achieved up to
85.21% accuracy, with F1-scores ranging from 0.7075 (severity) to 0.8393
(quality attribute). Statistical validation via paired t-tests and Cohen's
Kappa showed substantial to almost perfect agreement for bug type (k = 0.696),
category (k = 0.826), quality attribute (k = 0.818), and quantum-specific bug
type (k = 0.712). Severity classification showed slight agreement (k = 0.162),
suggesting room for improvement. Large-scale analysis revealed that classical
bugs dominate (67.2%), with quantum-specific bugs at 27.3%. Frequent bug
categories included compatibility, functional, and quantum-specific defects,
while usability, maintainability, and interoperability were the most impacted
quality attributes. Most issues (93.7%) were low severity; only 4.3% were
critical. A detailed review of 1,550 quantum-specific bugs showed that over
half involved quantum circuit-level problems, followed by gate errors and
hardware-related issues.

</details>


### [14] [Towards Understanding Bugs in Distributed Training and Inference Frameworks for Large Language Models](https://arxiv.org/abs/2506.10426)
*Xiao Yu,Haoxuan Chen,Feifei Niu,Xing Hu,Jacky Wai Keung,Xin Xia*

Main category: cs.SE

TL;DR: 本文通过对DeepSpeed、Megatron-LM和Colossal-AI三大分布式训练/推理框架的308个已修复错误进行分析，揭示了错误症状、根本原因及修复策略，发现48%的错误可通过简单代码更改修复，并提出了改进框架可靠性的建议。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）和分布式训练/推理框架的快速发展，框架中的软件错误可能严重影响性能和资源利用。了解这些错误的特性对于设计有效的调试和修复方法至关重要。

Method: 对DeepSpeed、Megatron-LM和Colossal-AI三大框架中的308个已修复错误进行大规模实证分析，涵盖错误症状、根本原因、识别与修复过程及常见低复杂度修复策略。

Result: 发现分布式框架中的独特错误原因（如分配策略错误和分布式通信错误），且48%的错误修复仅需少量代码更改（≤10行），并遵循简单策略。

Conclusion: 研究结果为改进分布式训练/推理框架的可靠性提供了实用建议，并指出了利用LLM工具实现自动化调试和修复的潜力。

Abstract: With the rapid development of large language models (LLMs), distributed
training and inference frameworks like DeepSpeed have become essential for
scaling model training and inference across multiple GPUs or nodes. However,
the increasing complexity of these frameworks brings non-trivial software bugs,
which may degrade training performance, cause unexpected failures, and result
in significant resource waste. Understanding framework bugs' characteristics is
fundamental for quality assurance, allowing the design of more effective
debugging and repair methods. Thus, our paper conducts the first large-scale
empirical analysis of 308 fixed bugs across three popular distributed
training/inference frameworks: DeepSpeed, Megatron-LM, and Colossal-AI. We
examine bug symptoms, root causes, bug identification and fixing efforts, and
common low-effort fixing strategies. Additionally, the distributed nature of
these frameworks introduces unique bug root causes, such as allocation strategy
error and distributed communication error. Diagnosing and fixing complex bugs
remains challenging due to factors like the disconnect between symptoms and
root causes, high bug reproduction costs, and low-level or cross-component
interactions. Interestingly, we observe that 48% of bug fixes require minimal
code changes (<=10 LOC) and follow simple strategies such as conditional logic
optimization, parameter handling enhancement, or version compatibility
handling, indicating potential for automation. Based on these insights, we
offer several implications for improving the reliability of both distributed
training and inference frameworks and their dependent LLM projects, while also
identifying opportunities to leverage LLM-based tools for automated debugging
and repair.

</details>


### [15] [EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair](https://arxiv.org/abs/2506.10484)
*Fangwen Mu,Junjie Wang,Lin Shi,Song Wang,Shoubin Li,Qing Wang*

Main category: cs.SE

TL;DR: ExpeRepair是一种基于LLM的新方法，通过双通道知识积累从历史修复经验中学习，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决当前软件修复方法中孤立处理问题和静态提示策略的局限性。

Method: 利用双记忆系统（情景记忆和语义记忆）结合动态提示组合，改进通用性。

Result: 在SWE-bench Lite基准测试中，pass@1得分49.3%，优于开源方法。

Conclusion: ExpeRepair通过经验驱动提示提升了修复效果，验证了双记忆系统的有效性。

Abstract: Automatically repairing software issues remains a fundamental challenge at
the intersection of software engineering and AI. Although recent advancements
in Large Language Models (LLMs) have demonstrated potential for
repository-level repair tasks, current methodologies exhibit two notable
limitations: (1) they often address issues in isolation, neglecting to
incorporate insights from previously resolved issues, and (2) they rely on
static and rigid prompting strategies, which constrain their ability to
generalize across diverse and evolving issue scenarios. Inspired by the dual
memory systems of human cognition, where episodic and semantic memories work
synergistically to support human reasoning and decision-making, we propose
ExpeRepair, a novel LLM-based approach that continuously learns from historical
repair experiences through dual-channel knowledge accumulation. ExpeRepair
organizes historical repair experiences into two complementary memories: an
episodic memory that stores concrete repair demonstrations, and a semantic
memory that encodes abstract reflective insights. At inference time, ExpeRepair
activates both memory systems by retrieving relevant demonstrations from
episodic memory and recalling high-level repair insights from semantic memory.
It further enhances adaptability through dynamic prompt composition,
synergistically integrating both memory types to replace static prompts with
context-aware, experience-driven prompts. Experiments on the SWE-bench Lite
benchmark demonstrate that ExpeRepair achieves a pass@1 score of 49.3% with
Claude 3.7 Sonnet, outperforming all state-of-the-art open-source methods.

</details>


### [16] [BugGen: A Self-Correcting Multi-Agent LLM Pipeline for Realistic RTL Bug Synthesis](https://arxiv.org/abs/2506.10501)
*Surya Jasper,Minh Luu,Evan Pan,Aakash Tyagi,Michael Quinn,Jiang Hu,David Kebo Houngninou*

Main category: cs.SE

TL;DR: BugGen是一种全自动多代理方法，利用LLM生成、插入和验证RTL中的功能错误，显著提升验证效率和ML辅助调试。


<details>
  <summary>Details</summary>
Motivation: 硬件复杂性增加导致验证资源紧张，现有方法无法可靠生成多样化且可扩展的错误数据集。

Method: BugGen通过分区模块、闭环代理架构选择目标，并迭代优化与回滚机制，确保语法正确和功能可检测性。

Result: 在OpenTitan IP块中生成500个独特错误，功能准确率94%，吞吐量17.7个/小时，优于手动方法。

Conclusion: BugGen为高质量错误数据集提供可扩展解决方案，显著提升ML辅助调试的实用性与效率。

Abstract: Hardware complexity continues to strain verification resources, motivating
the adoption of machine learning (ML) methods to improve debug efficiency.
However, ML-assisted debugging critically depends on diverse and scalable bug
datasets, which existing manual or automated bug insertion methods fail to
reliably produce. We introduce BugGen, a first of its kind, fully autonomous,
multi-agent pipeline leveraging Large Language Models (LLMs) to systematically
generate, insert, and validate realistic functional bugs in RTL. BugGen
partitions modules, selects mutation targets via a closed-loop agentic
architecture, and employs iterative refinement and rollback mechanisms to
ensure syntactic correctness and functional detectability. Evaluated across
five OpenTitan IP blocks, BugGen produced 500 unique bugs with 94% functional
accuracy and achieved a throughput of 17.7 validated bugs per hour-over five
times faster than typical manual expert insertion. Additionally, BugGen
identified 104 previously undetected bugs in OpenTitan regressions,
highlighting its utility in exposing verification coverage gaps. Compared
against Certitude, BugGen demonstrated over twice the syntactic accuracy,
deeper exposure of testbench blind spots, and more functionally meaningful and
complex bug scenarios. Furthermore, when these BugGen-generated datasets were
employed to train ML-based failure triage models, we achieved high
classification accuracy (88.1%-93.2%) across different IP blocks, confirming
the practical utility and realism of generated bugs. BugGen thus provides a
scalable solution for generating high-quality bug datasets, significantly
enhancing verification efficiency and ML-assisted debugging.

</details>


### [17] [AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length](https://arxiv.org/abs/2506.10525)
*Junhang Cheng,Fang Liu,Chengru Wu,Li Zhang*

Main category: cs.SE

TL;DR: AdaptiveLLM动态选择最适合的LLM进行代码生成，通过自动评估任务难度，优化性能与成本的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有模型选择方法资源密集且忽视成本效率，且依赖人工标注的难度标签，而AdaptiveLLM解决了这些限制。

Method: 使用Chain-of-Thought长度评估任务难度，k-means聚类为三个级别，CodeBERT嵌入难度特征，XGBoost分类器选择最优模型。

Result: 实验显示，AdaptiveLLM在pass@1分数上提升7.86%，资源消耗减少88.9%，且比单模型准确率提高15%。

Conclusion: AdaptiveLLM提供了一种高效、成本优化的动态模型选择框架，且难度评估比人工标注更可靠。

Abstract: While Large Language Models (LLMs) have significantly advanced code
generation efficiency, they face inherent challenges in balancing performance
and inference costs across diverse programming tasks. Dynamically selecting the
optimal LLM based on task difficulty and resource constraints offers a
promising approach to achieve an optimal balance between efficiency and
performance. However, existing model selection methods are resource-intensive
and often neglect cost efficiency. Moreover, these approaches rely on
human-annotated difficulty labels that are frequently inaccessible in
real-world settings and may not align with the LLM's own assessment of task
difficulty. In this paper, we introduce AdaptiveLLM, a framework that
dynamically selects optimal LLMs for a given coding task by automatically
assessing task difficulty. Our framework first estimates task difficulty using
Chain-of-Thought lengths generated by reasoning model, clusters these into
three difficulty levels via k-means, and fine-tunes CodeBERT to embed
difficulty-aware features. A trained XGBoost classifier then selects the best
model for each problem, optimizing the performance-cost trade-off. Experimental
results show that AdaptiveLLM achieves a 7.86% improvement in pass@1 score
while reducing resource consumption by 88.9% compared to baseline method
ComplexityNet. When compared to a single model, AdaptiveLLM demonstrates an
approximately 15% accuracy improvement, while maintaining the same level of
cost consumption. Apart from that, the difficulty assessment using CoT provides
more reliable selection criteria than human evaluation. Our replication package
is available at https://github.com/cjhCoder7/AdaptiveLLM.

</details>


### [18] [Not One to Rule Them All: Mining Meaningful Code Review Orders From GitHub](https://arxiv.org/abs/2506.10654)
*Abir Bouraffa,Carolin Brandt,Andy Zaidmann,Walid Maalej*

Main category: cs.SE

TL;DR: 研究发现，44.6%的GitHub拉取请求中评论者不会按字母顺序查看文件，而是采用多种有意义顺序，如最大差异优先或测试优先等。


<details>
  <summary>Details</summary>
Motivation: 探讨代码审查中开发者如何选择导航顺序，以优化审查工具支持。

Method: 分析100个热门Java和Python仓库的23,241个拉取请求中的评论顺序。

Result: 44.6%的拉取请求评论顺序非字母序，且非字母顺序审查的审查率更高，但略少获批准。

Conclusion: 大型拉取请求需要更灵活的审查顺序支持工具。

Abstract: Developers use tools such as GitHub pull requests to review code, discuss
proposed changes, and request modifications. While changed files are commonly
presented in alphabetical order, this does not necessarily coincide with the
reviewer's preferred navigation sequence. This study investigates the different
navigation orders developers follow while commenting on changes submitted in
pull requests. We mined code review comments from 23,241 pull requests in 100
popular Java and Python repositories on GitHub to analyze the order in which
the reviewers commented on the submitted changes. Our analysis shows that for
44.6% of pull requests, the reviewers comment in a non-alphabetical order.
Among these pull requests, we identified traces of alternative meaningful
orders: 20.6% (2,134) followed a largest-diff-first order, 17.6% (1,827) were
commented in the order of the files' similarity to the pull request's title and
description, and 29% (1,188) of pull requests containing changes to both
production and test files adhered to a test-first order. We also observed that
the proportion of reviewed files to total submitted files was significantly
higher in non-alphabetically ordered reviews, which also received slightly
fewer approvals from reviewers, on average. Our findings highlight the need for
additional support during code reviews, particularly for larger pull requests,
where reviewers are more likely to adopt complex strategies rather than
following a single predefined order.

</details>


### [19] [Formalising Software Requirements using Large Language Models](https://arxiv.org/abs/2506.10704)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: VERIFAI项目旨在通过自然语言处理、本体论和AI技术，解决形式化规范的追踪和验证问题。


<details>
  <summary>Details</summary>
Motivation: 解决软件设计到实现过程中形式化规范的追踪和验证挑战。

Method: 结合自然语言处理、本体论、相似性重用和大型语言模型，自动生成形式化规范。

Result: 提出一种综合方法支持规范生成和需求追踪。

Conclusion: VERIFAI为形式化规范的自动生成和验证提供了创新解决方案。

Abstract: This paper is a brief introduction to our recently initiated project named
VERIFAI: Traceability and verification of natural language requirements. The
project addresses the challenges in the traceability and verification of formal
specifications through providing support for the automatic generation of the
formal specifications and the traceability of the requirements from the initial
software design stage through the systems implementation and verification.
Approaches explored in this project include Natural Language Processing, use of
ontologies to describe the software system domain, reuse of existing software
artefacts from similar systems (i.e. through similarity based reuse) and large
language models to identify and declare the specifications as well as use of
artificial intelligence to guide the process.

</details>


### [20] [From Tea Leaves to System Maps: Context-awareness in Monitoring Operational Machine Learning Models](https://arxiv.org/abs/2506.10770)
*Joran Leest,Claudia Raibulet,Patricia Lago,Ilias Gerostathopoulos*

Main category: cs.SE

TL;DR: 该论文提出了一个系统性综述，探讨机器学习监控中上下文信息的应用，并提出了C-SAR框架，旨在提升监控的可靠性和系统性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习监控研究缺乏对上下文信息的共享理解，导致监控难以从统计异常转向有意义的警报和根因分析。

Method: 通过分析94项跨领域研究，提出C-SAR框架，识别20种可复用的上下文模式。

Result: 研究发现上下文信息可通过C-SAR框架有效分类，并支持监控活动。

Conclusion: 研究为机器学习监控提供了新视角，从统计异常转向系统化的上下文管理。

Abstract: Machine learning (ML) models in production do not fail due to statistical
anomalies in their input data; they fail due to contextual misalignment -- when
their environment deviates from training assumptions, leading to unreliable
predictions. Effective ML monitoring requires rich contextual information to
move beyond detecting statistical shifts toward meaningful alerts and
systematic root-cause analysis. Yet, surprisingly, despite extensive research
in ML monitoring and related disciplines (drift detection, data validation,
out-of-distribution detection), there is no shared understanding of how to use
contextual information -- striking, given that monitoring involves
interpretation of information in context. In response, this paper presents a
systematic review to characterize and structure the various types of contextual
information in this domain. Our analysis examines 94 primary studies across
data mining, databases, software engineering, and ML. We introduce the
Contextual System--Aspect--Representation (C-SAR) framework, a conceptual model
that synthesizes our findings. We also identify 20 recurring and potentially
reusable patterns of specific system, aspect, and representation combinations,
and map them to the monitoring activities they support. This study provides a
new perspective on ML monitoring: from interpreting "tea leaves" of
observational statistics into constructing and managing "system maps" that
enable systematic and reliable ML monitoring practices.

</details>


### [21] [What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps](https://arxiv.org/abs/2506.10785)
*Vinaik Chhetri,Krishna Upadhyay,A. B. Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 该研究首次大规模分析用户对AI驱动移动应用的反馈，通过多阶段分析流程提取了超过一百万条细粒度情感观点，揭示了用户关注的核心主题。


<details>
  <summary>Details</summary>
Motivation: 用户对AI功能的看法和评价尚未被充分研究，主要由于海量用户反馈难以处理。

Method: 研究利用292个AI应用的894K条评论，开发并验证了一个多阶段分析流程，包括分类、情感提取和聚类。

Result: 用户反馈集中在少数主题上：正面关注生产力和可靠性，负面关注技术故障和定价。

Conclusion: 该研究提供了更真实的用户体验分析，揭示了通用满意度和领域特定问题的驱动因素。

Abstract: Artificial Intelligence (AI)-powered features have rapidly proliferated
across mobile apps in various domains, including productivity, education,
entertainment, and creativity. However, how users perceive, evaluate, and
critique these AI features remains largely unexplored, primarily due to the
overwhelming volume of user feedback. In this work, we present the first
comprehensive, large-scale study of user feedback on AI-powered mobile apps,
leveraging a curated dataset of 292 AI-driven apps across 14 categories with
894K AI-specific reviews from Google Play. We develop and validate a
multi-stage analysis pipeline that begins with a human-labeled benchmark and
systematically evaluates large language models (LLMs) and prompting strategies.
Each stage, including review classification, aspect-sentiment extraction, and
clustering, is validated for accuracy and consistency. Our pipeline enables
scalable, high-precision analysis of user feedback, extracting over one million
aspect-sentiment pairs clustered into 18 positive and 15 negative user topics.
Our analysis reveals that users consistently focus on a narrow set of themes:
positive comments emphasize productivity, reliability, and personalized
assistance, while negative feedback highlights technical failures (e.g.,
scanning and recognition), pricing concerns, and limitations in language
support. Our pipeline surfaces both satisfaction with one feature and
frustration with another within the same review. These fine-grained,
co-occurring sentiments are often missed by traditional approaches that treat
positive and negative feedback in isolation or rely on coarse-grained analysis.
To this end, our approach provides a more faithful reflection of the real-world
user experiences with AI-powered apps. Category-aware analysis further uncovers
both universal drivers of satisfaction and domain-specific frustrations.

</details>


### [22] [Evaluating Large Language Models on Non-Code Software Engineering Tasks](https://arxiv.org/abs/2506.10833)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: 提出首个针对非代码软件工程（SE）任务的综合基准SELU，评估LLMs在17项任务中的表现，结果显示中等规模的解码器模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码任务中表现出色，但其在非代码SE任务中的有效性尚未充分研究。

Method: 创建SELU基准，覆盖分类、回归等任务，评估22个开源LLMs和2个专有模型，使用多种指标衡量性能。

Result: 中等规模解码器模型表现最优，领域适应性预训练仅带来有限改进。

Conclusion: 研究为SE任务模型选择提供指导，并建议将SELU扩展至生成和设计导向场景。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code understanding and generation; however, their effectiveness on non-code
Software Engineering (SE) tasks remains underexplored. We present the first
comprehensive benchmark, which we name `Software Engineering Language
Understanding' (SELU), for evaluating LLMs on 17 non-code tasks, spanning from
identifying whether a requirement is functional or non-functional to estimating
the effort and complexity of backlog items. SELU covers classification,
regression, Named Entity Recognition (NER), and Masked Language Modeling (MLM)
targets, with data drawn from diverse sources such as code repositories, issue
tracking systems, and developer forums. We fine-tune 22 open-source LLMs,
prompt two proprietary alternatives, and train two baselines. Performance is
measured using metrics such as F1-macro, SMAPE, F1-micro, and accuracy, and
compared via the Bayesian signed-rank test. Our results show that
moderate-scale decoder-only models consistently form a top-tier, exhibiting
high mean performance and low across-task variance, while domain adaptation via
code-focused pre-training might yield only modest improvements. These insights
guide model selection for non-code SE workflows and highlight directions for
expanding SELU to generative and design-oriented scenarios.

</details>


### [23] [MultiCoSim: A Python-based Multi-Fidelity Co-Simulation Framework](https://arxiv.org/abs/2506.10869)
*Quinn Thibeault,Giulia Pedrielli*

Main category: cs.SE

TL;DR: MultiCoSim是一个基于Python的仿真框架，解决了现有仿真工具在灵活性、自动化和可移植性方面的不足，支持分布式、组件化的协同仿真。


<details>
  <summary>Details</summary>
Motivation: 随着CPS复杂性和规模的增长，特别是在安全关键和学习增强的背景下，现有仿真工具的刚性配置和缺乏自动化支持阻碍了灵活组合系统和跨平台集成组件的能力。

Method: MultiCoSim通过Python程序化定义和配置仿真组件，支持分布式协同仿真和组件的无缝替换与重新配置。

Result: 案例研究表明MultiCoSim能够灵活地支持定制控制器与现成平台（如PX4自动驾驶仪）的协同仿真，简化CPS仿真流程。

Conclusion: MultiCoSim为CPS的研究和开发提供了高效的仿真支持，解决了现有工具的局限性。

Abstract: Simulation is a foundational tool for the analysis and testing of
cyber-physical systems (CPS), underpinning activities such as algorithm
development, runtime monitoring, and system verification. As CPS grow in
complexity and scale, particularly in safety-critical and learning-enabled
settings, accurate analysis and synthesis increasingly rely on the rapid use of
simulation experiments. Because CPS inherently integrate hardware, software,
and physical processes, simulation platforms must support co-simulation of
heterogeneous components at varying levels of fidelity. Despite recent advances
in high-fidelity modeling of hardware, firmware, and physics, co-simulation in
diverse environments remains challenging. These limitations hinder the
development of reusable benchmarks and impede the use of simulation for
automated and comparative evaluation.
  Existing simulation tools often rely on rigid configurations, lack automation
support, and present obstacles to portability and modularity. Many are
configured through static text files or impose constraints on how simulation
components are represented and connected, making it difficult to flexibly
compose systems or integrate components across platforms.
  To address these challenges, we introduce MultiCoSim, a Python-based
simulation framework that enables users to define, compose, and configure
simulation components programmatically. MultiCoSim supports distributed,
component-based co-simulation and allows seamless substitution and
reconfiguration of components. We demonstrate the flexibility of MultiCoSim
through case studies that include co-simulations involving custom
automaton-based controllers, as well as integration with off-the-shelf
platforms like the PX4 autopilot for aerial robotics. These examples highlight
MultiCoSim's capability to streamline CPS simulation pipelines for research and
development.

</details>


### [24] [SWE-Factory: Your Automated Factory for Issue Resolution Training Data and Evaluation Benchmarks](https://arxiv.org/abs/2506.10954)
*Lianghong Guo,Yanlin Wang,Caihua Li,Pengyu Yang,Jiachi Chen,Wei Tao,Yingtian Zou,Duyu Tang,Zibin Zheng*

Main category: cs.SE

TL;DR: SWE-Factory是一个自动化流水线，用于高效构建和验证GitHub问题解决任务的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 构建大规模数据集对训练和评估大型语言模型的软件工程能力至关重要，但传统方法费时费力。

Method: SWE-Factory包含三个核心组件：SWE-Builder（多代理系统自动构建评估环境）、标准化退出码评分方法和自动化fail2pass验证。

Result: 实验表明，流水线能高效构建任务实例（如GPT-4.1-mini下每个实例成本$0.045），评分精度达100%，验证准确率达0.92。

Conclusion: 该自动化流水线可加速高质量数据集构建，促进模型训练与评估。

Abstract: Constructing large-scale datasets for the GitHub issue resolution task is
crucial for both training and evaluating the software engineering capabilities
of Large Language Models (LLMs). However, the traditional process for creating
such benchmarks is notoriously challenging and labor-intensive, particularly in
the stages of setting up evaluation environments, grading test outcomes, and
validating task instances. In this paper, we propose SWE-Factory, an automated
pipeline designed to address these challenges. To tackle these issues, our
pipeline integrates three core automated components. First, we introduce
SWE-Builder, a multi-agent system that automates evaluation environment
construction, which employs four specialized agents that work in a
collaborative, iterative loop and leverages an environment memory pool to
enhance efficiency. Second, we introduce a standardized, exit-code-based
grading method that eliminates the need for manually writing custom parsers.
Finally, we automate the fail2pass validation process using these reliable exit
code signals. Experiments on 671 issues across four programming languages show
that our pipeline can effectively construct valid task instances; for example,
with GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at $0.045 per
instance, while with Gemini-2.5-flash, it achieves comparable performance at
the lowest cost of $0.024 per instance. We also demonstrate that our
exit-code-based grading achieves 100% accuracy compared to manual inspection,
and our automated fail2pass validation reaches a precision of 0.92 and a recall
of 1.00. We hope our automated pipeline will accelerate the collection of
large-scale, high-quality GitHub issue resolution datasets for both training
and evaluation. Our code and datasets are released at
https://github.com/DeepSoftwareAnalytics/swe-factory.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [25] [From Tool Calling to Symbolic Thinking: LLMs in a Persistent Lisp Metaprogramming Loop](https://arxiv.org/abs/2506.10021)
*Jordi de la Torre*

Main category: cs.PL

TL;DR: 研究提出了一种将大语言模型（LLM）与交互式Lisp环境结合的架构，支持动态工具创建和状态记忆。


<details>
  <summary>Details</summary>
Motivation: 旨在结合符号编程与神经语言生成，实现更灵活的AI系统交互。

Method: 通过Lisp表达式嵌入和中间件拦截，实现状态记忆和动态工具创建。

Result: 提出了一种框架和设计原则，为未来交互式AI系统提供指导。

Conclusion: 该架构为结合符号编程与神经语言的AI系统提供了新思路。

Abstract: We propose a novel architecture for integrating large language models (LLMs)
with a persistent, interactive Lisp environment. This setup enables LLMs to
define, invoke, and evolve their own tools through programmatic interaction
with a live REPL. By embedding Lisp expressions within generation and
intercepting them via a middleware layer, the system allows for stateful
external memory, reflective programming, and dynamic tool creation. We present
a design framework and architectural principles to guide future implementations
of interactive AI systems that integrate symbolic programming with neural
language generation.

</details>


### [26] [A Language-Agnostic Logical Relation for Message-Passing Protocols](https://arxiv.org/abs/2506.10026)
*Tesla Zhang,Sonya Simkin,Rui Li,Yue Yao,Stephanie Balzer*

Main category: cs.PL

TL;DR: 提出了一种用于验证异构消息传递系统协议遵从性的框架，支持语言无关的逻辑关系，适用于多种应用场景。


<details>
  <summary>Details</summary>
Motivation: 由于现代分布式和异构系统中应用程序的复杂性，以及缺乏共同实现语言和类型系统的支持，验证协议遵从性变得极具挑战性。

Method: 开发了一个基于标签转换语义的语言无关逻辑关系框架，支持任意对象，并在Coq中进行了机械化验证。

Result: 框架成功支持两类场景的验证：特定实例的验证和类型系统的通用验证。

Conclusion: 该框架为异构系统的协议验证提供了有效的工具，填补了现有方法的不足。

Abstract: Today's computing landscape has been gradually shifting to applications
targeting distributed and *heterogeneous* systems, such as cloud computing and
Internet of Things (IoT) applications. These applications are predominantly
*concurrent*, employ *message-passing*, and interface with *foreign objects*,
ranging from externally implemented code to actual physical devices such as
sensors. Verifying that the resulting systems adhere to the intended protocol
of interaction is challenging -- the usual assumption of a common
implementation language, let alone a type system, no longer applies, ruling out
any verification method based on them. This paper develops a framework for
certifying *protocol compliance* of heterogeneous message-passing systems. It
contributes the first mechanization of a *language-agnostic logical relation*,
asserting that its inhabitants comply with the protocol specified. This
definition relies entirely on a labelled transition-based semantics,
accommodating arbitrary inhabitants, typed and untyped alike, including foreign
objects. As a case study, the paper considers two scenarios: (1) *per-instance
verification* of a specific application or hardware device, and (2)
*once-and-for-all verification* of well-typed applications for a given type
system. The logical relation and both scenarios are mechanized in the Coq
theorem prover.

</details>


### [27] [Hazel Deriver: A Live Editor for Constructing Rule-Based Derivations](https://arxiv.org/abs/2506.10781)
*Zhiyao Zhong,Cyrus Omar*

Main category: cs.PL

TL;DR: Hazel Deriver是一个基于网络的编辑器，旨在帮助编程语言和形式逻辑课程的学生更轻松地构建基于规则的推导树，提供实时反馈和多层次支持。初步研究显示其降低了任务难度并提升了学习效果。


<details>
  <summary>Details</summary>
Motivation: 学生在构建推导树时面临应用推理规则的复杂性、缺乏即时反馈以及手工证明的繁琐问题。

Method: 基于Hazel实时编程环境开发的Hazel Deriver，提供结构化、交互式体验，支持迭代探索和实时反馈。

Result: 初步用户研究表明，Hazel Deriver降低了任务难度，提升了概念理解和学习参与度。

Conclusion: 讨论了多层次支持功能的设计，并探讨了系统指导与学习者自主性之间的平衡问题。

Abstract: Students in programming languages and formal logic courses often struggle
with constructing rule-based derivation trees due to the complexity of applying
inference rules, the lack of immediate feedback, and the manual effort required
for handwritten proofs. We present Hazel Deriver, a live, web-based editor
designed to scaffold derivation construction through multiple layers of
support. Built on the Hazel live programming environment, it provides a
structured, interactive experience that encourages iterative exploration and
real-time feedback. A preliminary user study with former students suggests that
Hazel Deriver reduces the perceived difficulty of derivation tasks while
improving conceptual understanding and engagement. We discuss the design of its
layered scaffolding features and raise questions about balancing system
guidance with learner autonomy.

</details>


### [28] [Choreographic Quick Changes: First-Class Location (Set) Polymorphism](https://arxiv.org/abs/2506.10913)
*Ashley Samuelson,Andrew K. Hirsch,Ethan Cecchetti*

Main category: cs.PL

TL;DR: 提出了一种新型编排语言 λQC，支持头等进程名和多态性，改进了表达能力和功能。


<details>
  <summary>Details</summary>
Motivation: 现有编排语言缺乏关键的现代系统功能，如动态计算节点并发送决策。

Method: 通过引入 λQC，支持头等进程名、多态性和递归数据类型。

Result: λQC 显著提高了表达能力和功能性，并通过 Rocq 形式化验证了无死锁性。

Conclusion: λQC 填补了现有编排语言的不足，为并发系统编程提供了更强大的工具。

Abstract: Choreographic programming is a promising new paradigm for programming
concurrent systems where a developer writes a single centralized program that
compiles to individual programs for each node. Existing choreographic
languages, however, lack critical features integral to modern systems, like the
ability of one node to dynamically compute who should perform a computation and
send that decision to others. This work addresses this gap with $\lambda_{QC}$,
the first typed choreographic language with \emph{first class process names}
and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also
improves expressive power over previous work by supporting algebraic and
recursive data types as well as multiply-located values. We formalize and
mechanically verify our results in Rocq, including the standard choreographic
guarantee of deadlock freedom.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [29] [Playing in the Sandbox: A Study on the Usability of Seccomp](https://arxiv.org/abs/2506.10234)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.OS

TL;DR: 论文研究了Seccomp开发者在使用沙盒技术时面临的挑战，分析了他们的不同方法和建议。


<details>
  <summary>Details</summary>
Motivation: 探讨为什么沙盒技术在实际应用中较少被采用，以及开发者在沙盒化应用程序时遇到的困难。

Method: 通过对7位有经验的Seccomp开发者进行可用性试验，记录他们的沙盒化方法和遇到的难题。

Result: 开发者们采取了不同的沙盒化方法，遇到了许多挑战，并提出了改进建议。

Conclusion: Seccomp的使用存在显著挑战，开发者需要更多支持以有效沙盒化应用程序。

Abstract: Sandboxing restricts what applications do, and prevents exploited processes
being abused; yet relatively few applications get sandboxed: why? We report a
usability trial with 7 experienced Seccomp developers exploring how they
approached sandboxing an application and the difficulties they faced. The
developers each approached sandboxing the application differently and each came
to different solutions. We highlight many challenges of using Seccomp, the
sandboxing designs by the participants, and what developers think would make it
easier for them to sandbox applications effectively.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [30] [AI5GTest: AI-Driven Specification-Aware Automated Testing and Validation of 5G O-RAN Components](https://arxiv.org/abs/2506.10111)
*Abiodun Ganiyu,Pranshav Gajjar,Vijay K Shah*

Main category: cs.NI

TL;DR: AI5GTest是一个基于AI的自动化测试框架，用于解决O-RAN多厂商组件测试的复杂性问题，通过LLM模型提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: O-RAN的开放架构带来测试复杂性，现有手动方法效率低且易出错，亟需自动化解决方案。

Method: 利用Gen-LLM、Val-LLM和Debug-LLM的协作框架，自动化生成测试流程、验证合规性及分析故障，并引入人为监督。

Result: 相比传统方法，AI5GTest显著减少测试时间，同时保持高准确性。

Conclusion: AI5GTest为O-RAN测试提供了高效、可靠的自动化工具，推动行业标准化进程。

Abstract: The advent of Open Radio Access Networks (O-RAN) has transformed the
telecommunications industry by promoting interoperability, vendor diversity,
and rapid innovation. However, its disaggregated architecture introduces
complex testing challenges, particularly in validating multi-vendor components
against O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as
those provided by Open Testing and Integration Centres (OTICs), rely heavily on
manual processes, are fragmented and prone to human error, leading to
inconsistency and scalability issues. To address these limitations, we present
AI5GTest -- an AI-powered, specification-aware testing framework designed to
automate the validation of O-RAN components. AI5GTest leverages a cooperative
Large Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and
Debug-LLM. Gen-LLM automatically generates expected procedural flows for test
cases based on 3GPP and O-RAN specifications, while Val-LLM cross-references
signaling messages against these flows to validate compliance and detect
deviations. If anomalies arise, Debug-LLM performs root cause analysis,
providing insight to the failure cause. To enhance transparency and
trustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the
Gen-LLM presents top-k relevant official specifications to the tester for
approval before proceeding with validation. Evaluated using a range of test
cases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest
demonstrates a significant reduction in overall test execution time compared to
traditional manual methods, while maintaining high validation accuracy.

</details>


### [31] [Large Language Models-Empowered Wireless Networks: Fundamentals, Architecture, and Challenges](https://arxiv.org/abs/2506.10651)
*Latif U. Khan,Maher Guizani,Sami Muhaidat,Choong Seon Hong*

Main category: cs.NI

TL;DR: 论文探讨了无线网络与大型语言模型（LLMs）的集成，提出了‘LLM原生无线系统’的概念，并通过案例研究展示了其分布式实现。


<details>
  <summary>Details</summary>
Motivation: 无线网络的快速发展带来了服务质量和用户体验指标的新挑战，而LLMs为复杂任务提供了潜在的解决方案，因此研究两者的集成具有重要意义。

Method: 提出了‘LLM原生无线系统’的概念，并通过案例研究中的双深度Q学习（DDQN）方法实现。

Result: 提出的基于DDQN的解决方案优于现有的DDQN方法。

Conclusion: LLM与无线网络的集成是一个有前景的研究方向，但仍面临诸多挑战。

Abstract: The rapid advancement of wireless networks has resulted in numerous
challenges stemming from their extensive demands for quality of service towards
innovative quality of experience metrics (e.g., user-defined metrics in terms
of sense of physical experience for haptics applications). In the meantime,
large language models (LLMs) emerged as promising solutions for many difficult
and complex applications/tasks. These lead to a notion of the integration of
LLMs and wireless networks. However, this integration is challenging and needs
careful attention in design. Therefore, in this article, we present a notion of
rational wireless networks powered by \emph{telecom LLMs}, namely,
\emph{LLM-native wireless systems}. We provide fundamentals, vision, and a case
study of the distributed implementation of LLM-native wireless systems. In the
case study, we propose a solution based on double deep Q-learning (DDQN) that
outperforms existing DDQN solutions. Finally, we provide open challenges.

</details>


### [32] [Energy-Efficient Deep Learning for Traffic Classification on Microcontrollers](https://arxiv.org/abs/2506.10851)
*Adel Chehade,Edoardo Ragusa,Paolo Gastaldo,Rodolfo Zunino*

Main category: cs.NI

TL;DR: 提出了一种用于资源有限单片机的轻量级1D-CNN方法，通过硬件感知神经架构搜索优化，实现了高效的能源消耗和准确性平衡。


<details>
  <summary>Details</summary>
Motivation: 解决物联网智能系统中资源受限微控制器上的能耗与准确性平衡问题。

Method: 开发了一种轻量级1D-CNN，并通过硬件感知神经架构搜索优化，支持INT8量化部署。

Result: 在ISCX VPN-NonVPN数据集上达到96.59%的准确率，参数和计算量极低，适用于多种流量分类任务。

Conclusion: 证明了在设备端进行加密流量分析的可行性，为低功耗物联网安全解决方案铺平了道路。

Abstract: In this paper, we present a practical deep learning (DL) approach for
energy-efficient traffic classification (TC) on resource-limited
microcontrollers, which are widely used in IoT-based smart systems and
communication networks. Our objective is to balance accuracy, computational
efficiency, and real-world deployability. To that end, we develop a lightweight
1D-CNN, optimized via hardware-aware neural architecture search (HW-NAS), which
achieves 96.59% accuracy on the ISCX VPN-NonVPN dataset with only 88.26K
parameters, a 20.12K maximum tensor size, and 10.08M floating-point operations
(FLOPs). Moreover, it generalizes across various TC tasks, with accuracies
ranging from 94% to 99%. To enable deployment, the model is quantized to INT8,
suffering only a marginal 1-2% accuracy drop relative to its Float32
counterpart. We evaluate real-world inference performance on two
microcontrollers: the high-performance STM32F746G-DISCO and the cost-sensitive
Nucleo-F401RE. The deployed model achieves inference latencies of 31.43ms and
115.40ms, with energy consumption of 7.86 mJ and 29.10 mJ per inference,
respectively. These results demonstrate the feasibility of on-device encrypted
traffic analysis, paving the way for scalable, low-power IoT security
solutions.

</details>


### [33] [Dynamic Beyond 5G and 6G Connectivity: Leveraging NTN and RIS Synergies for Optimized Coverage and Capacity in High-Density Environments](https://arxiv.org/abs/2506.10900)
*Valdemar Farré,Juan Estrada,David Vega,Luis F Urquiza-Aguiar,Juan A. Vásquez Peralvo,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 提出了一种6G无线网络规划框架，结合非地面网络和可重构智能表面，以解决高密度环境中传统地面网络的覆盖与容量问题。


<details>
  <summary>Details</summary>
Motivation: 高密度大型户外活动对通信的高可靠性与大容量需求日益增长，传统地面网络难以满足这些需求。

Method: 整合低地球轨道卫星、被动RIS平台与B5G技术（如mMIMO和波束赋形），通过动态SINR模型优化频谱利用。

Result: 模拟验证显示，该框架显著提升了拥挤场景下的连接性、可靠性和成本效益。

Conclusion: 该集成策略为未来6G网络的演进需求提供了有效解决方案。

Abstract: The increasing demand for reliable, high-capacity communication during
large-scale outdoor events poses significant challenges for traditional
Terrestrial Networks (TNs), which often struggle to provide consistent coverage
in high-density environments. This paper presents a novel 6G radio network
planning framework that integrates Non-Terrestrial Networks (NTNs) with
Reconfigurable Intelligent Surfaces (RISs) to deliver ubiquitous coverage and
enhanced network capacity. Our framework overcomes the limitations of
conventional deployable base stations by leveraging NTN architectures,
including Low Earth Orbit (LEO) satellites and passive RIS platforms seamlessly
integrated with Beyond 5G (B5G) TNs. By incorporating advanced B5G technologies
such as Massive Multiple Input Multiple Output (mMIMO) and beamforming, and by
optimizing spectrum utilization across the C, S, and Ka bands, we implement a
rigorous interference management strategy based on a dynamic SINR model.
Comprehensive calculations and simulations validate the proposed framework,
demonstrating significant improvements in connectivity, reliability, and
cost-efficiency in crowded scenarios. This integration strategy represents a
promising solution for meeting the evolving demands of future 6G networks.

</details>


### [34] [Agentic Semantic Control for Autonomous Wireless Space Networks: Extending Space-O-RAN with MCP-Driven Distributed Intelligence](https://arxiv.org/abs/2506.10925)
*Eduardo Baena,Paolo Testolina,Michele Polese,Sergi Aliaga,Andrew Benincasa,Dimitrios Koutsonikolas,Josep Jornet,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文提出了一种在月球表面无线通信系统中引入语义代理层的扩展方案，以增强上下文感知决策能力。


<details>
  <summary>Details</summary>
Motivation: 月球表面作业对无线通信系统提出了高要求，现有Space-O-RAN模型在决策逻辑上缺乏语义集成和动态适应能力。

Method: 通过Model Context Protocol (MCP)和Agent-to-Agent (A2A)协议实现语义代理层，支持多时间尺度的上下文感知决策。

Result: 部署在月球设备上的分布式认知代理实现了延迟自适应推理和带宽感知的语义压缩。

Conclusion: 该扩展方案显著提升了无线通信系统的适应性和鲁棒性，适用于月球任务。

Abstract: Lunar surface operations impose stringent requirements on wireless
communication systems, including autonomy, robustness to disruption, and the
ability to adapt to environmental and mission-driven context. While Space-O-RAN
provides a distributed orchestration model aligned with 3GPP standards, its
decision logic is limited to static policies and lacks semantic integration. We
propose a novel extension incorporating a semantic agentic layer enabled by the
Model Context Protocol (MCP) and Agent-to-Agent (A2A) communication protocols,
allowing context-aware decision making across real-time, near-real-time, and
non-real-time control layers. Distributed cognitive agents deployed in rovers,
landers, and lunar base stations implement wireless-aware coordination
strategies, including delay-adaptive reasoning and bandwidth-aware semantic
compression, while interacting with multiple MCP servers to reason over
telemetry, locomotion planning, and mission constraints.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [35] [Immersive Multimedia Communication: State-of-the-Art on eXtended Reality Streaming](https://arxiv.org/abs/2506.10004)
*Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: 该论文综述了扩展现实（XR）流媒体的最新进展，涵盖定义、设备、交互方法、数据传输要求、体验质量优化及当前应用与挑战。


<details>
  <summary>Details</summary>
Motivation: XR技术快速发展，有望彻底改变内容创作和消费方式，因此需要对其流媒体技术进行全面回顾和分析。

Method: 通过定义XR、分析XR设备与交互方法、研究数据传输特性和体验质量影响因素，提出基于视觉注意力的优化方法。

Result: 综述了XR流媒体的现状，识别了优化用户体验的关键因素，并指出了未来研究方向。

Conclusion: XR流媒体在多个领域有广泛应用前景，但仍面临技术挑战，需进一步研究和优化。

Abstract: Extended reality (XR) is rapidly advancing, and poised to revolutionize
content creation and consumption. In XR, users integrate various sensory inputs
to form a cohesive perception of the virtual environment. This survey reviews
the state-of-the-art in XR streaming, focusing on multiple paradigms. To begin,
we define XR and introduce various XR headsets along with their multimodal
interaction methods to provide a foundational understanding. We then analyze XR
traffic characteristics to highlight the unique data transmission requirements.
We also explore factors that influence the quality of experience in XR systems,
aiming to identify key elements for enhancing user satisfaction. Following
this, we present visual attention-based optimization methods for XR streaming
to improve efficiency and performance. Finally, we examine current applications
and highlight challenges to provide insights into ongoing and future
developments of XR.

</details>


### [36] [Semantic Communication-Enabled Cloud-Edge-End-collaborative Metaverse Services Architecure](https://arxiv.org/abs/2506.10001)
*Yuxuan Li,Sheng Jinag,Bizhu Wang*

Main category: cs.MM

TL;DR: 论文提出了一种基于语义通信的云边端协同架构（SC-CEE-Meta），用于解决元宇宙中高分辨率虚拟场景传输的带宽和延迟问题，显著提升了用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着技术发展和新视听体验需求的增长，元宇宙面临高分辨率数据在云平台与VR设备间传输的带宽不足和延迟问题。

Method: 提出了SC-CEE-Meta架构，包括VR视频语义传输、视频合成和3D虚拟场景重建三个模块，通过语义通信减少数据传输量并优化资源分配。

Result: 在Meta Quest Pro上验证表明，无线传输延迟降低了96.05%，图像质量在弱信道条件下提升了43.99%。

Conclusion: SC-CEE-Meta架构有效解决了带宽和延迟问题，提升了元宇宙服务的沉浸感和用户体验。

Abstract: With technology advancing and the pursuit of new audiovisual experiences
strengthening, the metaverse has gained surging enthusiasm. However, it faces
practical hurdles as substantial data like high-resolution virtual scenes must
be transmitted between cloud platforms and VR devices. Specifically, the VR
device's wireless transmission hampered by insufficient bandwidth, causes speed
and delay problems. Meanwhile, poor channel quality leads to data errors and
worsens user experience. To solve this, we've proposed the Semantic
Communication-Enabled Cloud-Edge-End Collaborative Immersive Metaverse Service
(SC-CEE-Meta) Architecture, which includes three modules: VR video semantic
transmission, video synthesis, and 3D virtual scene reconstruction. By
deploying semantic modules on VR devices and edge servers and sending key
semantic info instead of focusing on bit-level reconstruction, it can cut
latency, resolve the resource-bandwidth conflict, and better withstand channel
interference. Also, the cloud deploys video synthesis and 3D scene
reconstruction preprocessing, while edge devices host 3D reconstruction
rendering modules, all for immersive services. Verified on Meta Quest Pro, the
SC-CEE-Meta can reduce wireless transmission delay by 96.05\% and boost image
quality by 43.99\% under poor channel condition.

</details>


### [37] [EQ-TAA: Equivariant Traffic Accident Anticipation via Diffusion-Based Accident Video Synthesis](https://arxiv.org/abs/2506.10002)
*Jianwu Fang,Lei-Lei Li,Zhedong Zheng,Hongkai Yu,Jianru Xue,Zhengguo Li,Tat-Seng Chua*

Main category: cs.MM

TL;DR: 论文提出了一种注意力视频扩散（AVD）模型，用于生成事故视频片段以解决交通场景中的长尾、不确定性问题，同时提出了等变TAA（EQ-TAA）方法，实验结果表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前交通事故预测（TAA）中因数据偏差导致的背景混淆问题，提高预测准确性。

Method: 提出了AVD模型，通过生成事故视频片段来增强数据；结合EQ-TAA方法，使用等变三重损失进行训练。

Result: AVD和EQ-TAA在实验中表现优异，优于现有方法。

Conclusion: AVD和EQ-TAA能有效解决数据偏差问题，提升事故预测性能。

Abstract: Traffic Accident Anticipation (TAA) in traffic scenes is a challenging
problem for achieving zero fatalities in the future. Current approaches
typically treat TAA as a supervised learning task needing the laborious
annotation of accident occurrence duration. However, the inherent long-tailed,
uncertain, and fast-evolving nature of traffic scenes has the problem that real
causal parts of accidents are difficult to identify and are easily dominated by
data bias, resulting in a background confounding issue. Thus, we propose an
Attentive Video Diffusion (AVD) model that synthesizes additional accident
video clips by generating the causal part in dashcam videos, i.e., from normal
clips to accident clips. AVD aims to generate causal video frames based on
accident or accident-free text prompts while preserving the style and content
of frames for TAA after video generation. This approach can be trained using
datasets collected from various driving scenes without any extra annotations.
Additionally, AVD facilitates an Equivariant TAA (EQ-TAA) with an equivariant
triple loss for an anchor accident-free video clip, along with the generated
pair of contrastive pseudo-normal and pseudo-accident clips. Extensive
experiments have been conducted to evaluate the performance of AVD and EQ-TAA,
and competitive performance compared to state-of-the-art methods has been
obtained.

</details>


### [38] [Integrating multimedia documents in 3D city models for a better understanding of territories](https://arxiv.org/abs/2506.10003)
*C. Gautier,J. Delanoy,G. Gesquière*

Main category: cs.MM

TL;DR: 提出四种方法将多媒体文档集成到3D城市场景中，以增强场景的上下文信息，并结合用户引导模式提升理解效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D城市场景缺乏建筑历史或功能等上下文信息，而多媒体文档通常包含此类信息，将两者结合有助于城市分析和理解。

Method: 提出了四种方法将多媒体文档集成到3D场景中，并设计用户引导模式以辅助媒体消费和空间导航。

Result: 在法国里昂地区的多个项目中验证了这些技术的实用性，例如通过多媒体文档增强对标志性建筑或城市演变的解读。

Conclusion: 该研究通过结合多媒体和3D场景，为城市分析和空间导航提供了新的工具，效果显著。

Abstract: Digital 3D representations of urban areas, through their growing
availability, are a helpful tool to better understand a territory. However,
they lack contextual information about, for example, the history or
functionality of buildings. On another side, multimedia documents like images,
videos or texts usually contain such information. Crossing these two types of
data can therefore help in the analysis and understanding of the organization
of our cities. This could also be used to develop document search based on
spatial navigation, instead of the classical textual query. In this paper, we
propose four approaches to integrate multimedia documents in a 3D urban scene,
allowing to contextualize the scene with any type of media. We combine these
integration approaches with user guidance modes that allows to guide the user
through the consumption of these media and support its understanding of the
territory. We demonstrate the usefulness of these techniques in the context of
different projects within the Lyon area (France). The use of multimedia
documents integrated into a digital tour allows, for example, the iconic
buildings to be contextualised or to understand the evolution of a territory
through time.

</details>


### [39] [HER2 Expression Prediction with Flexible Multi-Modal Inputs via Dynamic Bidirectional Reconstruction](https://arxiv.org/abs/2506.10006)
*Jie Qin,Wei Yang,Yan Su,Yiran Zhu,Weizhen Li,Yunyue Pan,Chengchang Pan,Honggang Qi*

Main category: cs.MM

TL;DR: 提出了一种自适应双模态框架，通过动态分支选择、双向跨模态GAN和混合训练协议，显著提升HER2预测准确性，同时降低资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决HER2评估中H&E和IHC图像单独分析的局限性，克服双模态获取的复杂性和成本问题。

Method: 提出三种创新：动态分支选择器、双向跨模态GAN和混合训练协议，支持灵活的单/双模态预测。

Result: 单模态H&E预测准确率从71.44%提升至94.25%，双模态准确率达95.09%，同时减少IHC基础设施成本。

Conclusion: 该框架在资源有限的环境中具有广泛应用潜力，能显著提升HER2评估的准确性和效率。

Abstract: Current HER2 assessment models for breast cancer predominantly analyze H&E or
IHC images in isolation,despite clinical reliance on their synergistic
interpretation. However, concurrent acquisition of both modalities is often
hindered by workflow complexity and cost constraints. We propose an adaptive
bimodal framework enabling flexible single-/dual-modality HER2 prediction
through three innovations: 1) A dynamic branch selector that activates either
single-modality reconstruction or dual-modality joint inference based on input
completeness; 2) A bidirectional cross-modal GAN performing context-aware
feature-space reconstruction of missing modalities; 3) A hybrid training
protocol integrating adversarial learning and multi-task optimization. This
architecture elevates single-modality H&E prediction accuracy from 71.44% to
94.25% while achieving 95.09% dual-modality accuracy, maintaining 90.28%
reliability with sole IHC inputs. The framework's "dual-preferred,
single-compatible" design delivers near-bimodal performance without requiring
synchronized acquisition, particularly benefiting resource-limited settings
through IHC infrastructure cost reduction. Experimental validation confirms
22.81%/12.90% accuracy improvements over H&E/IHC baselines respectively, with
cross-modal reconstruction enhancing F1-scores to 0.9609 (HE to IHC) and 0.9251
(IHC to HE). By dynamically routing inputs through reconstruction-enhanced or
native fusion pathways, the system mitigates performance degradation from
missing data while preserving computational efficiency (78.55% parameter
reduction in lightweight variant). This elastic architecture demonstrates
significant potential for democratizing precise HER2 assessment across diverse
healthcare settings.

</details>


### [40] [Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space](https://arxiv.org/abs/2506.10007)
*Kangwei Liu,Junwu Liu,Xiaowei Yi,Jinlin Guo,Yun Cao*

Main category: cs.MM

TL;DR: 论文提出了一个基于扩散的框架，用于可控的情绪化3D面部动画，解决了单模态控制和确定性映射的问题，通过多模态情感绑定和注意力潜扩散模型提高了动画的表现力和多样性。


<details>
  <summary>Details</summary>
Motivation: 为了解决音频驱动的情绪化3D面部动画在单模态控制和确定性映射方面的不足，作者提出了一个多模态情感绑定和注意力扩散模型的新方法。

Method: 采用了FLAME中心的多模态情感绑定策略和注意力潜扩散模型，通过对比学习对齐文本、音频和情感标签，并使用内容感知注意力和情感引导层增强动态多样性。

Result: 实验表明，该方法在大多数指标上优于现有方法，情感相似性提高了21.6%，同时保持了生理上合理的面部动态。

Conclusion: 提出的方法通过多模态控制和扩散模型显著提升了3D面部动画的情绪表现力和动态多样性，具有广泛的应用潜力。

Abstract: Audio-driven emotional 3D facial animation encounters two significant
challenges: (1) reliance on single-modal control signals (videos, text, or
emotion labels) without leveraging their complementary strengths for
comprehensive emotion manipulation, and (2) deterministic regression-based
mapping that constrains the stochastic nature of emotional expressions and
non-verbal behaviors, limiting the expressiveness of synthesized animations. To
address these challenges, we present a diffusion-based framework for
controllable expressive 3D facial animation. Our approach introduces two key
innovations: (1) a FLAME-centered multimodal emotion binding strategy that
aligns diverse modalities (text, audio, and emotion labels) through contrastive
learning, enabling flexible emotion control from multiple signal sources, and
(2) an attention-based latent diffusion model with content-aware attention and
emotion-guided layers, which enriches motion diversity while maintaining
temporal coherence and natural facial dynamics. Extensive experiments
demonstrate that our method outperforms existing approaches across most
metrics, achieving a 21.6\% improvement in emotion similarity while preserving
physiologically plausible facial dynamics. Project Page:
https://kangweiiliu.github.io/Control_3D_Animation.

</details>


### [41] [Structured Graph Representations for Visual Narrative Reasoning: A Hierarchical Framework for Comics](https://arxiv.org/abs/2506.10008)
*Yi-Chun Chen*

Main category: cs.MM

TL;DR: 本文提出了一种层级知识图谱框架，用于结构化理解视觉叙事（如漫画），通过多模态图谱捕捉语义、空间和时间关系。


<details>
  <summary>Details</summary>
Motivation: 旨在解决多模态媒体（如漫画）中叙事内容的结构化理解问题，支持符号推理和内容分析。

Method: 将叙事内容分解为多个层次，构建集成知识图谱，链接视觉元素与文本组件。

Result: 在Manga109数据集上验证了框架的高精度和高召回率，支持多种叙事任务。

Conclusion: 该框架为基于叙事的内容分析、交互式故事讲述和多模态推理提供了可扩展的基础。

Abstract: This paper presents a hierarchical knowledge graph framework for the
structured understanding of visual narratives, focusing on multimodal media
such as comics. The proposed method decomposes narrative content into multiple
levels, from macro-level story arcs to fine-grained event segments. It
represents them through integrated knowledge graphs that capture semantic,
spatial, and temporal relationships. At the panel level, we construct
multimodal graphs that link visual elements such as characters, objects, and
actions with corresponding textual components, including dialogue and captions.
These graphs are integrated across narrative levels to support reasoning over
story structure, character continuity, and event progression.
  We apply our approach to a manually annotated subset of the Manga109 dataset
and demonstrate its ability to support symbolic reasoning across diverse
narrative tasks, including action retrieval, dialogue tracing, character
appearance mapping, and panel timeline reconstruction. Evaluation results show
high precision and recall across tasks, validating the coherence and
interpretability of the framework. This work contributes a scalable foundation
for narrative-based content analysis, interactive storytelling, and multimodal
reasoning in visual media.

</details>


### [42] [Multimodal Emotion Coupling via Speech-to-Facial and Bodily Gestures in Dyadic Interaction](https://arxiv.org/abs/2506.10010)
*Von Ralph Dane Marquez Herbuela,Yukie Nagai*

Main category: cs.MM

TL;DR: 研究探讨了情感表达中语音、面部和手势的动态协调，发现非重叠语音增强面部和手势活跃度，而愤怒情绪在重叠语音时抑制手势。预测模型中低频音高和MFCCs在发音区域准确性最高。


<details>
  <summary>Details</summary>
Motivation: 探索情感表达中语音与面部、手势的动态协调，以提高实时情感检测的准确性。

Method: 使用IEMOCAP语料库的双人互动数据，分析语音特征（如低频音高、MFCCs）与3D面部、手部标记位移的关系。

Result: 非重叠语音显著增强面部和手势活跃度，愤怒情绪抑制手势；低频音高和MFCCs预测准确性最高。

Conclusion: 情感表达中语音与面部、手势的协调模式受情绪类型和语音重叠影响，对情感检测系统优化有重要启示。

Abstract: Human emotional expression emerges through coordinated vocal, facial, and
gestural signals. While speech face alignment is well established, the broader
dynamics linking emotionally expressive speech to regional facial and hand
motion remains critical for gaining a deeper insight into how emotional and
behavior cues are communicated in real interactions. Further modulating the
coordination is the structure of conversational exchange like sequential turn
taking, which creates stable temporal windows for multimodal synchrony, and
simultaneous speech, often indicative of high arousal moments, disrupts this
alignment and impacts emotional clarity. Understanding these dynamics enhances
realtime emotion detection by improving the accuracy of timing and synchrony
across modalities in both human interactions and AI systems. This study
examines multimodal emotion coupling using region specific motion capture from
dyadic interactions in the IEMOCAP corpus. Speech features included low level
prosody, MFCCs, and model derived arousal, valence, and categorical emotions
(Happy, Sad, Angry, Neutral), aligned with 3D facial and hand marker
displacements. Expressive activeness was quantified through framewise
displacement magnitudes, and speech to gesture prediction mapped speech
features to facial and hand movements. Nonoverlapping speech consistently
elicited greater activeness particularly in the lower face and mouth. Sadness
showed increased expressivity during nonoverlap, while anger suppressed
gestures during overlaps. Predictive mapping revealed highest accuracy for
prosody and MFCCs in articulatory regions while arousal and valence had lower
and more context sensitive correlations. Notably, hand speech synchrony was
enhanced under low arousal and overlapping speech, but not for valence.

</details>


### [43] [WDMIR: Wavelet-Driven Multimodal Intent Recognition](https://arxiv.org/abs/2506.10011)
*Weiyin Gong,Kai Zhang,Yanghai Zhang,Qi Liu,Xinjie Sun,Junyu Lu,Linbo Zhu*

Main category: cs.MM

TL;DR: 论文提出了一种基于小波的多模态意图识别框架WDMIR，通过频域分析提升非语言信息的语义理解，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于依赖文本分析，忽视了非语言信息中的丰富语义内容，影响意图识别的准确性。

Method: 提出小波驱动融合模块进行视频-音频特征的频域同步分解与集成，以及跨模态交互机制实现双模态到三模态的渐进特征增强。

Result: 在MIntRec数据集上取得最优性能，准确率提升1.13%，小波融合模块对非语言信息提取效果显著（准确率提升0.41%）。

Conclusion: WDMIR框架通过频域分析和跨模态交互，有效提升了多模态意图识别的性能，验证了非语言信息的重要性。

Abstract: Multimodal intent recognition (MIR) seeks to accurately interpret user
intentions by integrating verbal and non-verbal information across video, audio
and text modalities. While existing approaches prioritize text analysis, they
often overlook the rich semantic content embedded in non-verbal cues. This
paper presents a novel Wavelet-Driven Multimodal Intent Recognition(WDMIR)
framework that enhances intent understanding through frequency-domain analysis
of non-verbal information. To be more specific, we propose: (1) a
wavelet-driven fusion module that performs synchronized decomposition and
integration of video-audio features in the frequency domain, enabling
fine-grained analysis of temporal dynamics; (2) a cross-modal interaction
mechanism that facilitates progressive feature enhancement from bimodal to
trimodal integration, effectively bridging the semantic gap between verbal and
non-verbal information. Extensive experiments on MIntRec demonstrate that our
approach achieves state-of-the-art performance, surpassing previous methods by
1.13% on accuracy. Ablation studies further verify that the wavelet-driven
fusion module significantly improves the extraction of semantic information
from non-verbal sources, with a 0.41% increase in recognition accuracy when
analyzing subtle emotional cues.

</details>


### [44] [Thief of Truth: VR comics about the relationship between AI and humans](https://arxiv.org/abs/2506.10012)
*Joonhyung Bae*

Main category: cs.MM

TL;DR: 《Thief of Truth》是第一人称VR漫画，探讨人类与AI的关系，通过VR技术和交互设计提升沉浸感与可访问性。


<details>
  <summary>Details</summary>
Motivation: 研究VR漫画的可扩展性，探索人类与AI互动的主题。

Method: 采用VR观看控制效果、基于控制器的交互设计，以及提高VR漫画可访问性的方法。

Result: 展示了一种实验性的VR漫画尝试。

Conclusion: 为VR漫画的未来发展提供了创新的设计思路。

Abstract: Thief of Truth is a first-person perspective Virtual Reality (VR) comic that
explores the relationship between humans and artificial intelligence (AI). The
work tells the story of a mind-uploaded human being reborn as a new subject
while interacting with an AI that is looking for the meaning of life. In order
to experiment with the expandability of VR comics, the work was produced by
focusing on three problems. First, the comic is designed using the viewing
control effect of VR. Second, through VR controller-based interaction, the
player's immersion in the work is increased. Third, a method for increasing
accessibility to VR comics was devised. This work aims to present an example of
an experimental attempt in VR Comics.

</details>


### [45] [Immersive Fantasy Based on Digital Nostalgia: Environmental Narratives for the Korean Millennials and Gen Z](https://arxiv.org/abs/2506.10013)
*Yerin Doh,Joonhyung Bae*

Main category: cs.MM

TL;DR: 介绍了媒体艺术作品《亲爱的乘客，请戴口罩》，通过游戏和展览探索口罩废弃物问题，结合数字怀旧与环保议题，展现了虚实结合的体验，但也存在资源使用和后期参与挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在通过艺术手段唤起人们对COVID-19期间一次性口罩废弃物的生态关注，并激发年轻一代的共鸣与行动。

Method: 采用点按游戏和沉浸式展览的形式，结合数字怀旧和航班旅行回忆，构建虚拟与现实交织的体验。

Result: 作品成功引发参与者对环保议题的共情和潜在行动，但资源利用和体验后的持续参与仍需改进。

Conclusion: 《亲爱的乘客，请戴口罩》通过艺术与科技的融合，为环保议题提供了新颖的表达方式，但仍需解决实践中的挑战以实现更广泛的影响。

Abstract: This study introduces the media artwork Dear Passenger, Please Wear a Mask,
designed to offer a layered exploration of single-use mask waste, which
escalated during the COVID-19 pandemic. The piece reframes underappreciated
ecological concerns by interweaving digital nostalgia and airline travel
recollections of Millennials and Gen Z with a unique fantasy narrative. Via a
point-and-click game and an immersive exhibition, participants traverse both
virtual and real domains, facing ethical and environmental dilemmas. While it
fosters empathy and potential action, resource use and post-experience
engagement challenges persist.

</details>


### [46] [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2506.10016)
*Longzhen Han,Awes Mubarak,Almas Baimagambetov,Nikolaos Polatidis,Thar Baker*

Main category: cs.MM

TL;DR: 本文综述了多模态大语言模型(MLLMs)的发展，探讨了其跨模态能力的生成技术和关键模型，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究MLLMs的跨模态能力及其统一架构，以推动更通用、自适应和可解释的多模态系统发展。

Method: 通过分类六种主要生成模态，分析自监督学习、混合专家、人类反馈强化学习和思维链提示等基础技术的应用。

Result: 总结了跨模态协同效应和未解决的问题，如评估、模块化和结构化推理。

Conclusion: MLLMs的技术融合和未来发展需要更通用的架构和更高效的跨模态方法。

Abstract: Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text
generation, now spanning diverse output modalities including images, music,
video, human motion, and 3D objects, by integrating language with other sensory
modalities under unified architectures. This survey categorises six primary
generative modalities and examines how foundational techniques, namely
Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement
Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,
enable cross-modal capabilities. We analyze key models, architectural trends,
and emergent cross-modal synergies, while highlighting transferable techniques
and unresolved challenges. Architectural innovations like transformers and
diffusion models underpin this convergence, enabling cross-modal transfer and
modular specialization. We highlight emerging patterns of synergy, and identify
open challenges in evaluation, modularity, and structured reasoning. This
survey offers a unified perspective on MLLM development and identifies critical
paths toward more general-purpose, adaptive, and interpretable multimodal
systems.

</details>


### [47] [Can Sound Replace Vision in LLaVA With Token Substitution?](https://arxiv.org/abs/2506.10416)
*Ali Vosoughi,Jing Bi,Pinxin Liu,Yunlong Tang,Chenliang Xu*

Main category: cs.MM

TL;DR: SoundCLIP研究直接在多模态大语言模型中整合音频和视觉输入，发现音频到视频检索性能提升但文本生成质量下降，提出WhisperCLIP架构和AVE-2数据集。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态系统依赖文本对齐表示而忽视音频信息直接整合的问题。

Method: 利用CLIP的视觉令牌替换为音频表示，探索两种配置：音频特征投影到CLIP视觉流形和保留原始音频嵌入。

Result: 音频投影提升检索性能（Top-1准确率提高44百分点），但文本生成质量下降；WhisperCLIP在生成任务中表现更优。

Conclusion: 强跨模态对齐并非对所有任务有益，需在检索准确率和文本生成质量间权衡。

Abstract: While multimodal systems have achieved impressive advances, they typically
rely on text-aligned representations rather than directly integrating audio and
visual inputs. This reliance can limit the use of acoustic information in tasks
requiring nuanced audio understanding. In response, SoundCLIP explores direct
audio-visual integration within multimodal large language models (MLLMs) by
substituting CLIP's visual tokens with audio representations and selecting
sound-relevant patch tokens in models such as LLaVA. We investigate two
configurations: (1) projecting audio features into CLIP's visual manifold via a
multilayer perceptron trained with InfoNCE on paired audio-video segments, and
(2) preserving raw audio embeddings with minimal dimensional adjustments.
Experiments with five state-of-the-art audio encoders reveal a fundamental
trade-off. While audio-to-video retrieval performance increases dramatically
(up to 44 percentage points in Top-1 accuracy) when audio is projected into
CLIP's space, text generation quality declines. Encoders pre-trained with text
supervision (CLAP, Whisper, ImageBind) maintain stronger generative
capabilities than those focused primarily on audiovisual alignment (Wav2CLIP,
AudioCLIP), highlighting the value of language exposure for generation tasks.
We introduce WhisperCLIP, an architecture that fuses intermediate
representations from Whisper, as well as AudioVisual Event Evaluation (AVE-2),
a dataset of 580,147 three-second audiovisual clips with fine-grained alignment
annotations. Our findings challenge the assumption that stronger cross-modal
alignment necessarily benefits all multimodal tasks; instead, a Pareto frontier
emerges wherein optimal performance depends on balancing retrieval accuracy
with text generation quality. Codes and datasets:
https://github.com/ali-vosoughi/SoundCLIP.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [48] [Growing a Modular Framework for Modal Systems- HOLMS: a HOL Light Library](https://arxiv.org/abs/2506.10048)
*Antonella Bilotta*

Main category: cs.LO

TL;DR: HOLMS是一个在HOL Light证明助手中用于模态推理的模块化框架，支持多个模态系统的充足性定理证明，并提供自动化决策工具。


<details>
  <summary>Details</summary>
Motivation: 为模态逻辑在HOL Light中的实现提供统一且模块化的方法，扩展之前对Gödel-Löb逻辑的研究。

Method: 开发了一种模块化策略，直接在HOL Light中证明多个模态系统（如K、T、K4、GL）的充足性定理，并集成自动化工具。

Result: 成功实现了模态推理的机械化，验证了模块化方法的可行性，并为HOLMS框架的进一步发展奠定了基础。

Conclusion: HOLMS展示了通用证明助手与模态逻辑研究的有效结合，为未来扩展提供了灵活且坚实的基础。

Abstract: The present dissertation introduces the research project on HOLMS
(\textbf{HOL} Light Library for \textbf{M}odal \textbf{S}ystems), a growing
modular framework for modal reasoning within the HOL Light proof assistant. To
provide an accessible introduction to the library, the fundamentals of modal
logic are outlined first, followed by a concise manual for the proof assistant
itself. The core contribution of this work on HOLMS is the development of a
unified and modular strategy for proving adequacy theorems with respect to
relational semantics directly within HOL Light for several normal modal
systems, currently including K, T, K4, and GL. Adequacy theorems establish a
formal connection between syntactic proof systems and their intended relational
models, ensuring that derivable statements align with valid ones. This approach
extends previous research on G\"odel-L\"ob logic (GL) by two HOLMS developers.
It also assesses the generality and compositionality of the completeness proofs
in George Boolos' monograph \textit{The logic of provability}. Beyond
theoretical contributions, HOLMS incorporates automated decision procedures and
a countermodel constructor for K, T, K4, and GL, illustrating how
general-purpose proof assistants can be effectively combined with research on
labelled sequent calculi and key insights from correspondence and bisimulation
theories. The implementation in HOL Light demonstrates the feasibility of
mechanising modal reasoning in a flexible and robust manner, paving the way for
further developments of the HOLMS framework.

</details>


### [49] [Notes on applicative matching logic](https://arxiv.org/abs/2506.10088)
*Laurentiu Leustean*

Main category: cs.LO

TL;DR: 本文介绍了功能化匹配逻辑（AML）的基础定义与成果，AML是Grigore Roșu及其合作者近期提出的一种功能化匹配逻辑变体，可用于程序的形式语义定义与行为推理。


<details>
  <summary>Details</summary>
Motivation: 开发功能化匹配逻辑（AML）的目的是为了提供一个更高效的逻辑框架，用于定义编程语言的形式语义，并规范和推理程序行为。

Method: 通过介绍AML的基础定义和成果，结合数学逻辑的理论，提供一个AML的入门文本。

Result: AML作为一个功能化的匹配逻辑变体，展示了其在程序形式语义和行为推理中的适用性。

Conclusion: AML为程序的形式语义和行为推理提供了一个有效的逻辑框架，Monk的数学逻辑教科书对其有重要影响。

Abstract: Matching logic (ML) was developed by Grigore Ro\c{s}u and collaborators as a
logic for defining the formal semantics of programming languages and for
specifying and reasoning about the behavior of programs. These lecture notes
present basic definitions and results on applicative matching logic (AML), a
functional variant of ML introduced recently by Xiaohong Chen and Grigore
Ro\c{s}u. They can be used as an introductory text in the theory of AML. Monk's
textbook on mathematical logic has an enormous influence on the notes.

</details>


### [50] [StepProof: Step-by-step verification of natural language mathematical proofs](https://arxiv.org/abs/2506.10558)
*Xiaolin Hu,Qinghua Zhou,Bogdan Grechuk,Ivan Y. Tyukin*

Main category: cs.LO

TL;DR: StepProof提出了一种新型的自动形式化方法，支持逐步验证自然语言证明，显著提升了验证成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 当前交互式定理证明器缺乏自然语言接口，且现有自动形式化方法只能验证完整证明，无法实现句子级别的验证。StepProof旨在填补这一空白。

Method: StepProof将完整证明分解为多个可验证的子证明，实现句子级别的逐步验证，并结合少量人工调整优化性能。

Result: 实验表明，StepProof在验证成功率和效率上均优于传统方法。人工调整进一步提升了性能。

Conclusion: StepProof为自动形式化提供了更细粒度的验证能力，显著提升了交互式定理证明的实用性和效率。

Abstract: Interactive theorem provers (ITPs) are powerful tools for the formal
verification of mathematical proofs down to the axiom level. However, their
lack of a natural language interface remains a significant limitation. Recent
advancements in large language models (LLMs) have enhanced the understanding of
natural language inputs, paving the way for autoformalization - the process of
translating natural language proofs into formal proofs that can be verified.
Despite these advancements, existing autoformalization approaches are limited
to verifying complete proofs and lack the capability for finer, sentence-level
verification. To address this gap, we propose StepProof, a novel
autoformalization method designed for granular, step-by-step verification.
StepProof breaks down complete proofs into multiple verifiable subproofs,
enabling sentence-level verification. Experimental results demonstrate that
StepProof significantly improves proof success rates and efficiency compared to
traditional methods. Additionally, we found that minor manual adjustments to
the natural language proofs, tailoring them for step-level verification,
further enhanced StepProof's performance in autoformalization.

</details>


### [51] [Encoding call-by-push-value in the pi-calculus](https://arxiv.org/abs/2506.10584)
*Benjamin Bennetzen,Nikolaj Rossander Kristensen,Peter Buus Steffensen*

Main category: cs.LO

TL;DR: 该论文提出了一种将Levy的按值调用的push-value lambda演算(CBPV)编码到π演算的方法，并证明了其编码的可靠性和完整性。同时，该编码满足Gorla提出的五项标准，并与Milner的编码进行了比较。


<details>
  <summary>Details</summary>
Motivation: 目的是在π演算中实现CBPV的编码，并验证其可靠性与完整性，同时满足编码质量的标准。

Method: 使用内部π演算(π-i-calculus)来简化编码和验证，避免了de Bruijn索引的复杂性，并通过手工证明验证了相关引理和定理。

Result: 编码被证明可靠且完整，满足Gorla的编码标准，同时在Coq中部分实现了形式化验证。

Conclusion: 该编码方法在理论和实践中均表现出色，为CBPV与π演算的交互提供了可行方案。

Abstract: In this report we define an encoding of Levys call-by-push-value
lambda-calculus (CBPV) in the pi-calculus, and prove that our encoding is both
sound and complete. We present informal (by-hand) proofs of soundness,
completeness, and all required lemmas. The encoding is specialized to the
internal pi-calculus (pi-i-calculus) to circumvent certain challenges
associated with using de Bruijn index in a formalization, and it also helps
with bisimulation as early-, late- and open-bisimulation coincide in this
setting, furthermore bisimulation is a congruence. Additionally, we argue that
our encoding also satisfies the five criteria for good encodings proposed by
Gorla, as well as show similarities between Milners and our encoding. This
paper includes encodings from CBPV in the pi-i-calculus, asynchronous polyadic
pi-calculus and the local pi-calculus. We begin a formalization of the proof in
Coq for the soundness and completeness of the encoding in the pi-i-calculus.
Not all lemmas used in the formalization are themselves formally proven.
However, we argue that the non-proven lemmas are reasonable, as they are proven
by hand, or amount to Coq formalities that are straightforward given informal
arguments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [52] [Cybernetic Marionette: Channeling Collective Agency Through a Wearable Robot in a Live Dancer-Robot Duet](https://arxiv.org/abs/2506.10079)
*Anup Sathya,Jiasheng Li,Zeyu Yan,Adriane Fang,Bill Kules,Jonathan David Martin,Huaishu Peng*

Main category: cs.HC

TL;DR: DANCE^2研究观众通过投票控制舞者身上穿戴的机器人行为，探讨了观众行为、感知与实际影响之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 探索观众在互动表演中的集体行为如何影响其对自主权的感知与实际表现的关系。

Method: 通过实时投票系统让观众控制舞者和机器人的互动，结合后表演调查分析观众体验。

Result: 观众认为自己的选择有意义，但投票数据显示行为模式高度一致，揭示了自主感与实际行为的差异。

Conclusion: 表演揭示了自主权感知与行为的矛盾，为算法系统的自主权体验提供了类比。

Abstract: We describe DANCE^2, an interactive dance performance in which audience
members channel their collective agency into a dancer-robot duet by voting on
the behavior of a wearable robot affixed to the dancer's body. At key moments
during the performance, the audience is invited to either continue the
choreography or override it, shaping the unfolding interaction through
real-time collective input. While post-performance surveys revealed that
participants felt their choices meaningfully influenced the performance, voting
data across four public performances exhibited strikingly consistent patterns.
This tension between what audience members do, what they feel, and what
actually changes highlights a complex interplay between agentive behavior, the
experience of agency, and power. We reflect on how choreography, interaction
design, and the structure of the performance mediate this relationship,
offering a live analogy for algorithmically curated digital systems where
agency is felt, but not exercised.

</details>


### [53] [Mastery Learning Improves Performance on Complex Tasks on PCP Literacy Test](https://arxiv.org/abs/2506.10164)
*Chandana Srinivas,Elif E. Firat,Robert S. Laramee,Alark Joshi*

Main category: cs.HC

TL;DR: 论文研究了如何使用修订版布鲁姆分类法和掌握学习法提升学生对平行坐标图（PCPs）的读写能力，发现掌握学习法在高级思考模块中效果更好。


<details>
  <summary>Details</summary>
Motivation: 学生对不熟悉的数据可视化技术（如PCPs）的读写能力存在挑战，需要有效的教学方法。

Method: 采用修订版布鲁姆分类法和掌握学习法进行课堂干预，并通过六模块的BTPL测试评估效果。

Result: 掌握学习组学生在高级思考模块（分析、评估）表现更好，且对PCPs的理解更深入。

Conclusion: 掌握学习法在提升学生数据可视化高级能力方面更有效，研究提供了完整的教学材料供进一步验证。

Abstract: Developing literacy with unfamiliar data visualization techniques such as
Parallel Coordinate Plots (PCPs) can be a significant challenge for students.
We adopted the Revised Bloom's taxonomy to instruct students on Parallel
Coordinate Plots (PCPs) using Mastery Learning in the classroom. To evaluate
Mastery Learning's impact, we conducted an intervention in a Data Visualization
course to teach students about PCPs using the Revised Bloom's taxonomy with and
without Mastery Learning. Based on our intervention, we found that while
students in both groups performed similarly on the first two (Remember,
Understand) modules, the students in the Mastery Learning group performed
better on modules that required more advanced thinking (Analyze, Evaluate) and
demonstrated a better comprehension of PCPs. We provide all the materials
developed including the six-module Bloom's Taxonomy PCP literacy (BTPL) test
for full reproducibility on our website at
https://vis-graphics.github.io/PCP-Literacy-Test/.

</details>


### [54] [Intergenerational AI Literacy in Korean Immigrant Families: Interpretive Gatekeeping Meets Convenient Critical Deferment](https://arxiv.org/abs/2506.10197)
*Jeongone Seo,Ryan Womack,Tawfiq Ammari*

Main category: cs.HC

TL;DR: 研究了韩国移民家庭如何通过文化视角调解AI工具的使用，揭示了AI素养是动态的家庭协商过程。


<details>
  <summary>Details</summary>
Motivation: 探索AI在移民家庭中的独特挑战，特别是文化和代际因素如何影响AI使用。

Method: 通过20个半结构化访谈，分析了父母和青少年在AI使用中的实践。

Result: 发现两种核心实践：父母的文化调解和青少年的实用主义策略，挑战了传统AI素养模型。

Conclusion: 为信息科学和HCI提供了新的概念框架，强调AI系统应更关注家庭和文化需求。

Abstract: As artificial intelligence (AI) becomes deeply integrated into family life,
immigrant families must navigate unique intergenerational, linguistic, and
cultural challenges. This study examines how Korean immigrant families in the
United States negotiate the use of AI tools such as ChatGPT and smart
assistants in their homes. Through 20 semi-structured interviews with parents
and teens, we identify two key practices that shape their engagement:
interpretive gatekeeping, where parents mediate their children's AI use through
a lens of cultural and ethical values, and convenient critical deferment, where
teens strategically postpone critical evaluation of AI for immediate academic
and social utility. These intertwined practices challenge conventional,
skills-based models of AI literacy, revealing it instead as a dynamic and
relational practice co-constructed through ongoing family negotiation. We
contribute to information science and HCI by offering a new conceptual
extension for intergenerational AI literacy and providing design implications
for more equitable, culturally attuned, and family-centered AI systems.

</details>


### [55] [Speculative Design in Spiraling Time: Methods and Indigenous HCI](https://arxiv.org/abs/2506.10229)
*James Eschrich,Cole McMullen,Sarah Sterman*

Main category: cs.HC

TL;DR: 探讨将推测性设计作为原住民HCI方法的问题，提出基于‘螺旋时间’概念的替代方案。


<details>
  <summary>Details</summary>
Motivation: 研究推测性设计在原住民HCI中的适用性，指出其时间假设的局限性。

Method: 分析推测性设计的问题，提出‘螺旋时间’作为替代框架。

Result: 发现传统推测性设计不适合原住民HCI，新框架更贴合需求。

Conclusion: ‘螺旋时间’概念有望改进原住民HCI中的推测性设计应用。

Abstract: In this position paper, we first discuss the uptake of speculative design as
a method for Indigenous HCI. Then, we outline how a key assumption about
temporality threatens to undermine the usefulness of speculative design in this
context. Finally, we briefly sketch out a possible alternative understanding of
speculative design, based on the concept of "spiraling time," which could be
better suited for Indigenous HCI.

</details>


### [56] [Extended Creativity: A Conceptual Framework for Understanding Human-AI Creative Relations](https://arxiv.org/abs/2506.10249)
*Andrea Gaggioli,Sabrina Bartolotta,Andrea Ubaldi,Katusha Gerardini,Eleonora Diletta Sarcinella,Alice Chirico*

Main category: cs.HC

TL;DR: AI通过支持、协同和共生三种模式增强人类创造力，研究探讨了这些模式在不同创造力层次的影响及理论、伦理和设计意义。


<details>
  <summary>Details</summary>
Motivation: 探索AI如何有效增强人类创造力，需要更清晰的理解。

Method: 采用分布式创造力的视角，识别AI在创造力过程中的三种主要模式：支持、协同和共生，并分析其在技术自主性和感知代理两个维度上的表现。

Result: 研究揭示了不同AI配置如何影响从日常问题解决到范式转换创新的创造力层次。

Conclusion: 研究为AI在创造力增强中的理论、伦理和设计提供了重要启示。

Abstract: Artificial Intelligence holds significant potential to enhance human
creativity. However, achieving this vision requires a clearer understanding of
how such enhancement can be effectively realized. Adopting the perspective of
distributed creativity, we identify three primary modes through which AI can
contribute to creative processes: Support, where AI acts as a tool; Synergy,
where AI and humans collaborate in complementary ways; and Symbiosis, where
human and AI cognition become so integrated that they form a unified creative
system. These modes are defined along two key dimensions: the level of
technical autonomy exhibited by the AI system and the degree of perceived
agency attributed to it. We examine how each configuration influences different
levels of creativity - from everyday problem-solving to paradigm-shifting
innovation - and discuss the theoretical, ethical, and design implications.

</details>


### [57] [Beyond Compliance: A User-Autonomy Framework for Inclusive and Customizable Web Accessibility](https://arxiv.org/abs/2506.10324)
*Lalitha A R*

Main category: cs.HC

TL;DR: 提出从合规驱动的网页无障碍转向以用户自主为核心的关怀模型，强调个性化需求，尤其是对神经多样性用户的支持。


<details>
  <summary>Details</summary>
Motivation: 当前无障碍标准虽灵活但常被静态化执行，需要更动态、用户中心的方法。

Method: 引入可定制的“舒适模式”框架，允许用户调整界面设置（如对比度、字体、动画等），同时保持品牌视觉一致性。

Result: 展示了低成本实现无缝包容性设计的两种模型，证明个性化与创意自由可兼顾。

Conclusion: 新框架通过选择而非妥协，实现用户自主、美学与无障碍的统一，适用于广泛用户和品牌。

Abstract: This paper proposes a shift from compliance-centered web accessibility to a
care-driven model that prioritizes user autonomy, using neurodivergent users as
a catalyst case for broader personalization needs. While accessibility
standards offer a flexible framework, they are often interpreted and
implemented as static compliance checklists, our approach reframes it as a
flexible, user-centered process. We introduce a customizable Comfort Mode
framework that allows users to adapt interface settings, such as contrast,
typography, motion, and scaling, according to their individual needs, while
retaining the brand's core visual identity. Grounded in psychological and
cognitive accessibility principles, our design supports personalization without
sacrificing creative freedom. We present both minimal and advanced
implementation models with mock-ups, demonstrating how inclusive design can be
seamlessly integrated at minimal cost. This approach aims to broaden digital
inclusivity by offering autonomy to those who require it, without imposing
changes on those who do not. The proposed system is adaptable, scalable, and
suitable for a wide range of users and brands, offering a new paradigm where
user autonomy, aesthetic integrity, and accessibility converge not through
compromise, but through choice.

</details>


### [58] [IDEA: Augmenting Design Intelligence through Design Space Exploration](https://arxiv.org/abs/2506.10587)
*Chuer Chen,Xiaoke Yan,Xiaoyu Qi,Nan Cao*

Main category: cs.HC

TL;DR: 提出了IDEA框架，通过设计空间探索和LLMs增强设计决策，结合MCTS算法生成有效设计成果。


<details>
  <summary>Details</summary>
Motivation: 设计空间的决策过程依赖于设计师经验，缺乏数学形式化，限制了自动化设计支持。

Method: 引入结构化表示模型，结合LLMs约束生成和MCTS算法探索设计空间，并将抽象决策转化为具体实现。

Result: 在文章撰写和图像可视化生成中验证了IDEA的跨领域适应性和卓越性能。

Conclusion: IDEA通过自动化探索和约束优化，显著提升了设计成果的质量和效率。

Abstract: Design spaces serve as a conceptual framework that enables designers to
explore feasible solutions through the selection and combination of design
elements. However, effective decision-making remains heavily dependent on the
designer's experience, and the absence of mathematical formalization prevents
computational support for automated design processes. To bridge this gap, we
introduce a structured representation that models design spaces with orthogonal
dimensions and discrete selectable elements. Building on this model, we present
IDEA, a decision-making framework for augmenting design intelligence through
design space exploration to generate effective outcomes. Specifically, IDEA
leverages large language models (LLMs) for constraint generation, incorporates
a Monte Carlo Tree Search (MCTS) algorithm guided by these constraints to
explore the design space efficiently, and instantiates abstract decisions into
domain-specific implementations. We validate IDEA in two design scenarios:
data-driven article composition and pictorial visualization generation,
supported by example results, expert interviews, and a user study. The
evaluation demonstrates the IDEA's adaptability across domains and its
capability to produce superior design outcomes.

</details>


### [59] [Accessible Design in Integrated Development Environments: A Think Aloud Study Exploring the Experiences of Students with ADHD](https://arxiv.org/abs/2506.10598)
*Luke Halpin,Phillip Benachour,Tracy Hall,Ann-Marie Houghton,Emily Winter*

Main category: cs.HC

TL;DR: 研究探讨了集成开发环境（IDE）对ADHD学生的影响，发现当前设计存在不足，需改进以提升学习体验。


<details>
  <summary>Details</summary>
Motivation: 了解IDE界面设计如何影响ADHD学生的学习，填补相关研究空白。

Method: 采用‘有声思维’实验和定性观察访谈，分析学生在Visual Studio Code IDE中的学习与互动。

Result: 发现三大主题：自信心、互动和学习，揭示了当前IDE设计对ADHD学生的障碍。

Conclusion: 需优化IDE设计，以更好地支持ADHD学生的教育需求。

Abstract: Coding forms a key part of computer science education in universities. As
part of this education, Integrated Development Environments (IDEs) are
essential tools for coding. However, it is currently unknown how the design of
an IDE's interface impacts on students with Attention Deficit Hyperactivity
Disorder (ADHD).
  In this study we investigated the use of IDEs by students with ADHD. We
conducted a think aloud study with nine university computing students, followed
by qualitative observational interviews to analyse their learning and
engagement with the Visual Studio Code IDE. The paper reports on these
experiences and seeks to understand the role IDEs play in the educational
setting.
  Our work also examines how digital accessibility and usability are considered
in the current design of IDEs. We analysed the qualitative data using a
thematic analysis and identified three primary themes: self-confidence,
interaction, and learning as well as various sub-themes.
  The themes and their sub-themes illustrate key areas of consideration when
designing IDEs for students with ADHD. The primary findings highlight
experiences of frustration and barriers in the current design and layout of
IDEs.
  Through our participatory approach we provide a rare insight into ADHD user
experiences around usability and accessibility, and describe the need for
better design of development environments to ensure a positive learning
experience for the students.

</details>


### [60] [Integrating Large Language Models into Text Animation: An Intelligent Editing System with Inline and Chat Interaction](https://arxiv.org/abs/2506.10762)
*Bao Zhang,Zihan Li,Zhenglei Liu,Huanchen Wang,Yuxin Ma*

Main category: cs.HC

TL;DR: 提出了一种基于大语言模型的文本动画编辑系统，通过实时意图追踪和灵活编辑降低非专业人士的使用门槛。


<details>
  <summary>Details</summary>
Motivation: 传统动画流程对非专业人士不友好，操作复杂阻碍创意生产力。

Method: 系统采用基于代理的双流管道，结合上下文感知内联建议和对话引导，通过语义-动画映射实现LLM驱动的创意翻译。

Result: 用户研究表明系统能帮助非专业人士完成动画流程，验证了管道的有效性。

Conclusion: 鼓励进一步探索将LLM整合到完整视频创作流程中。

Abstract: Text animation, a foundational element in video creation, enables efficient
and cost-effective communication, thriving in advertisements, journalism, and
social media. However, traditional animation workflows present significant
usability barriers for non-professionals, with intricate operational procedures
severely hindering creative productivity. To address this, we propose a Large
Language Model (LLM)-aided text animation editing system that enables real-time
intent tracking and flexible editing. The system introduces an agent-based
dual-stream pipeline that integrates context-aware inline suggestions and
conversational guidance as well as employs a semantic-animation mapping to
facilitate LLM-driven creative intent translation. Besides, the system supports
synchronized text-animation previews and parametric adjustments via unified
controls to improve editing workflow. A user study evaluates the system,
highlighting its ability to help non-professional users complete animation
workflows while validating the pipeline. The findings encourage further
exploration of integrating LLMs into a comprehensive video creation workflow.

</details>


### [61] [Grasp Prediction based on Local Finger Motion Dynamics](https://arxiv.org/abs/2506.10818)
*Dimitar Valkov,Pascal Kockwelp,Florian Daiber,Antonio Krüger*

Main category: cs.HC

TL;DR: 论文研究了基于手部运动学实时预测用户抓取目标的可行性，展示了LSTM网络在预测抓取时间和目标尺寸上的高精度。


<details>
  <summary>Details</summary>
Motivation: 预测用户抓取目标的意图可以为交互环境提供关键上下文信息，并缓解点对点延迟的影响。

Method: 通过记录16名参与者的手部运动数据，使用简单的LSTM网络进行实时识别。

Result: LSTM网络预测抓取时间精度优于21毫秒，距离精度优于1厘米，目标尺寸预测准确率超过97%。

Conclusion: 研究结果为设计自适应细粒度的交互用户界面提供了重要参考。

Abstract: The ability to predict the object the user intends to grasp offers essential
contextual information and may help to leverage the effects of point-to-point
latency in interactive environments. This paper explores the feasibility and
accuracy of real-time recognition of uninstrumented objects based on hand
kinematics during reach-to-grasp actions. In a data collection study, we
recorded the hand motions of 16 participants while reaching out to grasp and
then moving real and synthetic objects. Our results demonstrate that even a
simple LSTM network can predict the time point at which the user grasps an
object with a precision better than 21 ms and the current distance to this
object with a precision better than 1 cm. The target's size can be determined
in advance with an accuracy better than 97%. Our results have implications for
designing adaptive and fine-grained interactive user interfaces in ubiquitous
and mixed-reality environments.

</details>


### [62] [(De)composing Craft: An Elementary Grammar for Sharing Expertise in Craft Workflows](https://arxiv.org/abs/2506.10891)
*Ritik Batra,Lydia Kim,Ilan Mandel,Amritansh Kwatra,Jane L. E.,Steven J. Jackson,Thijs Roumen*

Main category: cs.HC

TL;DR: 本文提出了一种记录手工艺实践中即兴行为的基础语法，并开发了一个名为CraftLink的界面，用于分析专家视频并生成文档。


<details>
  <summary>Details</summary>
Motivation: 传统的手工艺知识记录方式忽视了即兴和适应性的隐性知识，限制了知识的共享。

Method: 通过专家访谈和跨学科文献研究，开发了一种记录即兴行为的基础语法，并设计了CraftLink界面。

Result: 用户研究（N=7）验证了该语法在捕捉和分享专家知识方面的有效性。

Conclusion: 该方法为计算系统支持社区内的知识共享提供了新途径。

Abstract: Craft practices rely on evolving archives of skill and knowledge, developed
through generations of craftspeople experimenting with designs, materials, and
techniques. Better documentation of these practices enables the sharing of
knowledge and expertise between sites and generations. However, most
documentation focuses solely on the linear steps leading to final artifacts,
neglecting the tacit knowledge necessary to improvise, or adapt workflows to
meet the unique demands of each craft project. This omission limits knowledge
sharing and reduces craft to a mechanical endeavor, rather than a sophisticated
way of seeing, thinking, and doing. Drawing on expert interviews and literature
from HCI, CSCW and the social sciences, we develop an elementary grammar to
document improvisational actions of real-world craft practices. We demonstrate
the utility of this grammar with an interface called CraftLink that can be used
to analyze expert videos and semi-automatically generate documentation to
convey material and contextual variations of craft practices. Our user study
with expert crocheters (N=7) using this interface evaluates our grammar's
effectiveness in capturing and sharing expert knowledge with other
craftspeople, offering new pathways for computational systems to support
collaborative archives of knowledge and practice within communities.

</details>


### [63] [The Role of Generative AI in Facilitating Social Interactions: A Scoping Review](https://arxiv.org/abs/2506.10927)
*T. T. J. E. Arets,G. Perugia,M. Houben,W. A. IJsselsteijn*

Main category: cs.HC

TL;DR: 这篇综述探讨了生成式AI（GAI）在增强社交互动中的应用，分析了30项研究，总结了关键趋势和社会伦理问题，并呼吁更公平的设计和评估策略。


<details>
  <summary>Details</summary>
Motivation: 社交连接减少对心理健康和幸福感构成威胁，而GAI技术在社交应用中的影响力尚未充分了解，因此需要系统研究。

Method: 通过对2020年以来的30项研究进行范围综述，分析GAI应用的设计目标、社交互动形式及设计评估方法。

Result: 发现GAI在多个领域（如讲故事、协作学习等）的应用趋势，并强调参与式设计和文化偏见等社会伦理问题。

Conclusion: GAI具有支持个性化互动的潜力，但需关注公平设计和包容性评估。

Abstract: Reduced social connectedness increasingly poses a threat to mental health,
life expectancy, and general well-being. Generative AI (GAI) technologies, such
as large language models (LLMs) and image generation tools, are increasingly
integrated into applications aimed at enhancing human social experiences.
Despite their growing presence, little is known about how these technologies
influence social interactions. This scoping review investigates how GAI-based
applications are currently designed to facilitate social interaction, what
forms of social engagement they target, and which design and evaluation
methodologies designers use to create and evaluate them. Through an analysis of
30 studies published since 2020, we identify key trends in application domains
including storytelling, socio-emotional skills training, reminiscence,
collaborative learning, music making, and general conversation. We highlight
the role of participatory and co-design approaches in fostering both effective
technology use and social engagement, while also examining socio-ethical
concerns such as cultural bias and accessibility. This review underscores the
potential of GAI to support dynamic and personalized interactions, but calls
for greater attention to equitable design practices and inclusive evaluation
strategies.

</details>


### [64] [Video-Mediated Emotion Disclosure: A Study of Mental Health Vlogging by People with Schizophrenia on YouTube](https://arxiv.org/abs/2506.10932)
*Jiaying Lizzy Liu,Yan Zhang*

Main category: cs.HC

TL;DR: 研究分析了200个由精神分裂症患者制作的YouTube视频，揭示了通过视觉和语言渠道表达情绪的多样性，并发现视觉元素的精心设计有助于引发更支持的观众反应。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究对视频博客中情绪表达方式的空白，尤其是精神分裂症患者通过视觉和语言渠道表达情绪的多样性。

Method: 基于媒体研究和自我呈现理论，开发视觉分析框架，分析200个YouTube视频。

Result: 发现视觉元素的刻意构建（如环境设置和审美选择）能促进更多支持的观众反应。

Conclusion: 未来需要大规模定量研究视频特征如何影响社交媒体上的视频传播，以开发更好的支持患者的视频共享平台。

Abstract: Individuals with schizophrenia frequently experience intense emotions and
often turn to vlogging as a medium for emotional expression. While previous
research has predominantly focused on text based disclosure, little is known
about how individuals construct narratives around emotions and emotional
experiences in video blogs. Our study addresses this gap by analyzing 200
YouTube videos created by individuals with schizophrenia. Drawing on media
research and self presentation theories, we developed a visual analysis
framework to disentangle these videos. Our analysis revealed diverse practices
of emotion disclosure through both verbal and visual channels, highlighting the
dynamic interplay between these modes of expression. We found that the
deliberate construction of visual elements, including environmental settings
and specific aesthetic choices, appears to foster more supportive and engaged
viewer responses. These findings underscore the need for future large scale
quantitative research examining how visual features shape video mediated
communication on social media platforms. Such investigations would inform the
development of care centered video sharing platforms that better support
individuals managing illness experiences.

</details>


### [65] [Instance-Based Transfer Learning with Similarity-Aware Subject Selection for Cross-Subject SSVEP-Based BCIs](https://arxiv.org/abs/2506.10933)
*Ziwen Wang,Yue Zhang,Zhiqiang Zhang,Sheng Quan Xie,Alexander Lanzon,William P. Heath,Zhenhong Li*

Main category: cs.HC

TL;DR: 提出了一种新的迁移学习框架iTRCA及其改进版SS-iTRCA，用于解决SSVEP-BCI中个体差异问题，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: SSVEP-BCIs需要大量训练数据，迁移学习可减轻数据需求，但个体差异导致效果不佳。

Method: iTRCA结合通用和特定特征，SS-iTRCA通过相似性选择源主体。

Result: 在多个数据集上验证了iTRCA和SS-iTRCA的有效性。

Conclusion: 该框架为减少目标主体数据需求的高性能SSVEP-BCI提供了解决方案。

Abstract: Steady-state visual evoked potential (SSVEP)-based brain-computer interfaces
(BCIs) can achieve high recognition accuracy with sufficient training data.
Transfer learning presents a promising solution to alleviate data requirements
for the target subject by leveraging data from source subjects; however,
effectively addressing individual variability among both target and source
subjects remains a challenge. This paper proposes a novel transfer learning
framework, termed instance-based task-related component analysis (iTRCA), which
leverages knowledge from source subjects while considering their individual
contributions. iTRCA extracts two types of features: (1) the subject-general
feature, capturing shared information between source and target subjects in a
common latent space, and (2) the subject-specific feature, preserving the
unique characteristics of the target subject. To mitigate negative transfer, we
further design an enhanced framework, subject selection-based iTRCA (SS-iTRCA),
which integrates a similarity-based subject selection strategy to identify
appropriate source subjects for transfer based on their task-related components
(TRCs). Comparative evaluations on the Benchmark, BETA, and a self-collected
dataset demonstrate the effectiveness of the proposed iTRCA and SS-iTRCA
frameworks. This study provides a potential solution for developing
high-performance SSVEP-based BCIs with reduced target subject data.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [66] [Learning-based density-equalizing map](https://arxiv.org/abs/2506.10027)
*Yanwen Huang,Lok Ming Lui,Gary P. T. Choi*

Main category: cs.GR

TL;DR: 本文提出了一种基于深度学习的新型密度均衡映射框架（LDEM），通过神经网络解决了传统方法在精度和扩展性上的局限，实现了从2D到3D的无缝推广。


<details>
  <summary>Details</summary>
Motivation: 传统密度均衡映射（DEM）方法面临精度有限、易产生重叠伪影，以及从2D扩展到3D时需重新设计算法的挑战。

Method: 提出LDEM，利用深度学习网络设计损失函数以强制密度均匀和几何规则，并采用层次化方法预测变换。

Result: 新方法在密度均衡和双射性上优于现有方法，并可轻松应用于表面重网格化，且无需修改即可从2D推广到3D。

Conclusion: LDEM为密度均衡映射的应用提供了可扩展且鲁棒的新解决方案。

Abstract: Density-equalizing map (DEM) serves as a powerful technique for creating
shape deformations with the area changes reflecting an underlying density
function. In recent decades, DEM has found widespread applications in fields
such as data visualization, geometry processing, and medical imaging.
Traditional approaches to DEM primarily rely on iterative numerical solvers for
diffusion equations or optimization-based methods that minimize handcrafted
energy functionals. However, these conventional techniques often face several
challenges: they may suffer from limited accuracy, produce overlapping
artifacts in extreme cases, and require substantial algorithmic redesign when
extended from 2D to 3D, due to the derivative-dependent nature of their energy
formulations. In this work, we propose a novel learning-based
density-equalizing mapping framework (LDEM) using deep neural networks.
Specifically, we introduce a loss function that enforces density uniformity and
geometric regularity, and utilize a hierarchical approach to predict the
transformations at both the coarse and dense levels. Our method demonstrates
superior density-equalizing and bijectivity properties compared to prior
methods for a wide range of simple and complex density distributions, and can
be easily applied to surface remeshing with different effects. Also, it
generalizes seamlessly from 2D to 3D domains without structural changes to the
model architecture or loss formulation. Altogether, our work opens up new
possibilities for scalable and robust computation of density-equalizing maps
for practical applications.

</details>


### [67] [FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training](https://arxiv.org/abs/2506.10035)
*Fuhan Cai,Yong Guo,Jie Li,Wenbo Li,Xiangzhong Fang,Jian Chen*

Main category: cs.GR

TL;DR: FastFLUX是用于提升FLUX模型推理效率的架构级剪枝框架，采用BRLL方法和ST训练策略，显著加快推理速度并保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有T2I生成模型如FLUX参数庞大，推理慢且部署困难，现有加速方法性能下降严重且训练成本高。

Method: FastFLUX结合BRLL（用轻量线性层替换复杂残差分支）和ST（局部微调策略）优化模型结构。

Result: 实验表明FastFLUX在剪除20%层级后仍保持高质量图像生成，并显著提升推理速度。

Conclusion: FastFLUX为大规模T2I模型提供高效推理解决方案，平衡性能与效率。

Abstract: Recent advancements in text-to-image (T2I) generation have led to the
emergence of highly expressive models such as diffusion transformers (DiTs),
exemplified by FLUX. However, their massive parameter sizes lead to slow
inference, high memory usage, and poor deployability. Existing acceleration
methods (e.g., single-step distillation and attention pruning) often suffer
from significant performance degradation and incur substantial training costs.
To address these limitations, we propose FastFLUX, an architecture-level
pruning framework designed to enhance the inference efficiency of FLUX. At its
core is the Block-wise Replacement with Linear Layers (BRLL) method, which
replaces structurally complex residual branches in ResBlocks with lightweight
linear layers while preserving the original shortcut connections for stability.
Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning
strategy that leverages LoRA to supervise neighboring blocks, mitigating
performance drops caused by structural replacement. Experiments show that our
FastFLUX maintains high image quality under both qualitative and quantitative
evaluations, while significantly improving inference speed, even with 20\% of
the hierarchy pruned. Our code will be available soon.

</details>


### [68] [Token Perturbation Guidance for Diffusion Models](https://arxiv.org/abs/2506.10036)
*Javad Rajabi,Soroush Mehraban,Seyedmorteza Sadat,Babak Taati*

Main category: cs.GR

TL;DR: 提出了Token Perturbation Guidance (TPG)，一种无需训练的方法，通过扰动中间令牌表示来提升扩散模型的生成质量，适用于条件和无条件生成。


<details>
  <summary>Details</summary>
Motivation: Classifier-free guidance (CFG)需要特定的训练过程且仅限于条件生成。为解决这些限制，TPG被提出。

Method: TPG直接扰动扩散网络中的中间令牌表示，使用保持范数的扰动操作提供稳定指导信号。

Result: TPG在无条件生成中显著提升了FID（近2倍），并在提示对齐上与CFG表现接近。

Conclusion: TPG是一种通用且条件无关的指导方法，扩展了CFG的优势到更广泛的扩散模型中。

Abstract: Classifier-free guidance (CFG) has become an essential component of modern
diffusion models to enhance both generation quality and alignment with input
conditions. However, CFG requires specific training procedures and is limited
to conditional generation. To address these limitations, we propose Token
Perturbation Guidance (TPG), a novel method that applies perturbation matrices
directly to intermediate token representations within the diffusion network.
TPG employs a norm-preserving shuffling operation to provide effective and
stable guidance signals that improve generation quality without architectural
changes. As a result, TPG is training-free and agnostic to input conditions,
making it readily applicable to both conditional and unconditional generation.
We further analyze the guidance term provided by TPG and show that its effect
on sampling more closely resembles CFG compared to existing training-free
guidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1
show that TPG achieves nearly a 2$\times$ improvement in FID for unconditional
generation over the SDXL baseline, while closely matching CFG in prompt
alignment. These results establish TPG as a general, condition-agnostic
guidance method that brings CFG-like benefits to a broader class of diffusion
models. The code is available at
https://github.com/TaatiTeam/Token-Perturbation-Guidance

</details>


### [69] [Ambient Diffusion Omni: Training Good Models with Bad Data](https://arxiv.org/abs/2506.10038)
*Giannis Daras,Adrian Rodriguez-Munoz,Adam Klivans,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.GR

TL;DR: 通过利用低质量、合成和分布外图像，提出Ambient Diffusion Omni框架，显著提升扩散模型的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖高质量过滤数据，但低质量图像往往被丢弃。本文证明这些图像中仍蕴含重要信息，提出一种新框架以利用所有可用图像。

Method: 通过分析自然图像的频谱幂律衰减和局部性特性，开发Ambient Diffusion Omni框架，利用合成损坏图像（如高斯模糊、JPEG压缩）训练模型。

Result: 实验表明，该框架在ImageNet FID上达到SOTA，并在文本到图像生成任务中显著提升图像质量和多样性。

Conclusion: 噪声可缓解高质量分布与混合分布之间的初始偏差，为利用低质量数据提供了理论依据和实践方法。

Abstract: We show how to use low-quality, synthetic, and out-of-distribution images to
improve the quality of a diffusion model. Typically, diffusion models are
trained on curated datasets that emerge from highly filtered data pools from
the Web and other sources. We show that there is immense value in the
lower-quality images that are often discarded. We present Ambient Diffusion
Omni, a simple, principled framework to train diffusion models that can extract
signal from all available images during training. Our framework exploits two
properties of natural images -- spectral power law decay and locality. We first
validate our framework by successfully training diffusion models with images
synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We
then use our framework to achieve state-of-the-art ImageNet FID, and we show
significant improvements in both image quality and diversity for text-to-image
generative modeling. The core insight is that noise dampens the initial skew
between the desired high-quality distribution and the mixed distribution we
actually observe. We provide rigorous theoretical justification for our
approach by analyzing the trade-off between learning from biased data versus
limited unbiased data across diffusion times.

</details>


### [70] [Low-Barrier Dataset Collection with Real Human Body for Interactive Per-Garment Virtual Try-On](https://arxiv.org/abs/2506.10468)
*Zaiqiang Wu,Yechen Li,Jingyuan Liu,Yuki Shibata,Takayuki Hori,I-Chao Shen,Takeo Igarashi*

Main category: cs.GR

TL;DR: 提出一种低成本、基于真实人体的衣物虚拟试穿方法，提高了图像质量和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有虚拟试穿方法成本高、人体变形不自然以及衣物与身体对齐不准的问题。

Method: 采用真实人体采集数据，结合混合人物表示和简化DensePose映射。

Result: 在图像质量和时间一致性上优于现有方法，用户研究表明对购买决策有帮助。

Conclusion: 该方法降低了成本并提升了效果，适用于实际应用。

Abstract: Existing image-based virtual try-on methods are often limited to the front
view and lack real-time performance. While per-garment virtual try-on methods
have tackled these issues by capturing per-garment datasets and training
per-garment neural networks, they still encounter practical limitations: (1)
the robotic mannequin used to capture per-garment datasets is prohibitively
expensive for widespread adoption and fails to accurately replicate natural
human body deformation; (2) the synthesized garments often misalign with the
human body. To address these challenges, we propose a low-barrier approach for
collecting per-garment datasets using real human bodies, eliminating the
necessity for a customized robotic mannequin. We also introduce a hybrid person
representation that enhances the existing intermediate representation with a
simplified DensePose map. This ensures accurate alignment of synthesized
garment images with the human body and enables human-garment interaction
without the need for customized wearable devices. We performed qualitative and
quantitative evaluations against other state-of-the-art image-based virtual
try-on methods and conducted ablation studies to demonstrate the superiority of
our method regarding image quality and temporal consistency. Finally, our user
study results indicated that most participants found our virtual try-on system
helpful for making garment purchasing decisions.

</details>


### [71] [Edit360: 2D Image Edits to 3D Assets from Any Angle](https://arxiv.org/abs/2506.10507)
*Junchao Huang,Xinting Hu,Zhuotao Tian,Shaoshuai Shi,Li Jiang*

Main category: cs.GR

TL;DR: Edit360是一个基于视频扩散模型的无需调优框架，用于将2D编辑扩展到多视角一致的3D编辑，支持任意视角的用户定制编辑，保证结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D编辑方法受限于预定义视角，灵活性不足。Edit360旨在实现多视角一致的3D编辑，提升实用性和灵活性。

Method: 利用视频扩散模型，通过Anchor-View Editing Propagation机制在潜在和注意力空间中对齐和合并多视角信息。

Result: 生成的编辑多视角序列可重建高质量3D资产，支持自定义3D内容生成。

Conclusion: Edit360实现了无需调优的多视角一致3D编辑，扩展了3D生成的实用性。

Abstract: Recent advances in diffusion models have significantly improved image
generation and editing, but extending these capabilities to 3D assets remains
challenging, especially for fine-grained edits that require multi-view
consistency. Existing methods typically restrict editing to predetermined
viewing angles, severely limiting their flexibility and practical applications.
We introduce Edit360, a tuning-free framework that extends 2D modifications to
multi-view consistent 3D editing. Built upon video diffusion models, Edit360
enables user-specific editing from arbitrary viewpoints while ensuring
structural coherence across all views. The framework selects anchor views for
2D modifications and propagates edits across the entire 360-degree range. To
achieve this, Edit360 introduces a novel Anchor-View Editing Propagation
mechanism, which effectively aligns and merges multi-view information within
the latent and attention spaces of diffusion models. The resulting edited
multi-view sequences facilitate the reconstruction of high-quality 3D assets,
enabling customizable 3D content creation.

</details>


### [72] [Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture](https://arxiv.org/abs/2506.10580)
*Chengxu Zuo,Jiawei Huang,Xiao Jiang,Yuan Yao,Xiangren Shi,Rui Cao,Xinyu Yi,Feng Xu,Shihui Guo,Yipeng Qin*

Main category: cs.GR

TL;DR: 提出一种新型动态校准方法，首次突破IMU校准中的绝对静态假设限制，显著扩展其应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统IMU校准依赖绝对静态假设，限制了其应用范围。本文旨在通过动态校准方法解决这一问题。

Method: 采用两种松弛假设，结合Transformer模型学习RG'G和RBS矩阵与IMU读数的映射，并设计校准触发机制。

Result: 实现了无需显式校准过程的隐式校准，并首次实现稀疏IMU的长期高精度运动捕捉。

Conclusion: 新方法有效扩展了IMU校准的应用场景，为未来动态校准技术提供了新思路。

Abstract: In this paper, we propose a novel dynamic calibration method for sparse
inertial motion capture systems, which is the first to break the restrictive
absolute static assumption in IMU calibration, i.e., the coordinate drift RG'G
and measurement offset RBS remain constant during the entire motion, thereby
significantly expanding their application scenarios. Specifically, we achieve
real-time estimation of RG'G and RBS under two relaxed assumptions: i) the
matrices change negligibly in a short time window; ii) the human movements/IMU
readings are diverse in such a time window. Intuitively, the first assumption
reduces the number of candidate matrices, and the second assumption provides
diverse constraints, which greatly reduces the solution space and allows for
accurate estimation of RG'G and RBS from a short history of IMU readings in
real time. To achieve this, we created synthetic datasets of paired RG'G, RBS
matrices and IMU readings, and learned their mappings using a Transformer-based
model. We also designed a calibration trigger based on the diversity of IMU
readings to ensure that assumption ii) is met before applying our method. To
our knowledge, we are the first to achieve implicit IMU calibration (i.e.,
seamlessly putting IMUs into use without the need for an explicit calibration
process), as well as the first to enable long-term and accurate motion capture
using sparse IMUs. The code and dataset are available at
https://github.com/ZuoCX1996/TIC.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [73] [Resilience through Automated Adaptive Configuration for Distribution and Replication](https://arxiv.org/abs/2506.10248)
*Scott D. Stoller,Balaji Jayasankar,Yanhong A. Liu*

Main category: cs.DC

TL;DR: 摘要介绍了一种自动框架，用于通过优化自适应分布和复制软件组件，提升复杂系统在故障下的韧性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在异构硬件上部署的复杂系统如何在故障时保持韧性，满足系统模型的弹性需求。

Method: 提出一种基于状态空间探索的算法，结合等价关系优化的商约简，生成初始韧性配置和动态重配置策略。

Result: 通过在自动驾驶系统模型上的原型实现验证了框架的有效性。

Conclusion: 该框架为复杂系统提供了自动化、高效的韧性管理方案。

Abstract: This paper presents a powerful automated framework for making complex systems
resilient under failures, by optimized adaptive distribution and replication of
interdependent software components across heterogeneous hardware components
with widely varying capabilities. A configuration specifies how software is
distributed and replicated: which software components to run on each computer,
which software components to replicate, which replication protocols to use,
etc. We present an algorithm that, given a system model and resilience
requirements, (1) determines initial configurations of the system that are
resilient, and (2) generates a reconfiguration policy that determines
reconfiguration actions to execute in response to failures and recoveries. This
model-finding algorithm is based on state-space exploration and incorporates
powerful optimizations, including a quotient reduction based on a novel
equivalence relation between states. We present experimental results from
successfully applying a prototype implementation of our framework to a model of
an autonomous driving system.

</details>


### [74] [Is Sparse Matrix Reordering Effective for Sparse Matrix-Vector Multiplication?](https://arxiv.org/abs/2506.10356)
*Omid Asudeh,Sina Mahdipour Saravani,Gerald Sabin,Fabrice Rastello,P Sadayappan*

Main category: cs.DC

TL;DR: 研究评估了稀疏矩阵重排序对不同多核CPU平台上稀疏矩阵向量乘法性能的影响，发现重排序能通过优化非零元素模式提升性能。


<details>
  <summary>Details</summary>
Motivation: 探讨稀疏矩阵重排序如何通过减少数据移动和改善负载均衡来提升性能，并比较不同策略在不同CPU上的表现。

Method: 研究采用了顺序和并行执行策略，分析了重排序策略的多样性、测量方法的适用性及负载不均衡的影响。

Result: 结果表明，重排序能显著提升性能，但其效果因策略和CPU平台而异。

Conclusion: 稀疏矩阵重排序在不同CPU平台上能有效优化性能，但需根据具体场景选择合适的策略。

Abstract: This work evaluates the impact of sparse matrix reordering on the performance
of sparse matrix-vector multiplication across different multicore CPU
platforms. Reordering can significantly enhance performance by optimizing the
non-zero element patterns to reduce total data movement and improve the
load-balancing. We examine how these gains vary over different CPUs for
different reordering strategies, focusing on both sequential and parallel
execution. We address multiple aspects, including appropriate measurement
methodology, comparison across different kinds of reordering strategies,
consistency across machines, and impact of load imbalance.

</details>


### [75] [HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration](https://arxiv.org/abs/2506.10401)
*Jiaqi Lv,Xufeng He,Yanchen Liu,Xu Dai,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 论文提出了一种利用AI编译器和自动优化技术的新框架，用于生成高性能CUDA和对应平台代码对，并通过图数据增强方法和HPCTransEval基准测试提升LLM在CUDA转换上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的快速发展，模型参数和计算需求激增。尽管NVIDIA GPU及其CUDA生态系统在并行计算中占据主导地位，但将CUDA代码移植到其他平台仍面临挑战。现有方法存在覆盖范围有限、开发成本高等问题，而LLM在高性能代码转换方面表现不佳，主要原因是缺乏高质量训练数据。

Method: 作者提出了一个结合AI编译器和自动优化技术的新框架，生成高性能CUDA与目标平台代码对。同时，采用图数据增强方法提升数据质量，并引入HPCTransEval基准测试评估LLM在CUDA转换上的性能。

Result: 实验以CUDA-to-CPU转换为例，验证了该框架显著提升了LLM在CUDA代码转换上的表现，展示了LLM解决CUDA生态系统兼容性问题的潜力。

Conclusion: 该研究通过新框架和基准测试，为解决CUDA生态系统中的兼容性问题提供了有效途径，并展示了LLM在高性能代码转换领域的潜力。

Abstract: The rapid growth of deep learning has driven exponential increases in model
parameters and computational demands. NVIDIA GPUs and their CUDA-based software
ecosystem provide robust support for parallel computing, significantly
alleviating computational bottlenecks. Meanwhile, due to the cultivation of
user programming habits and the high performance of GPUs, the CUDA ecosystem
has established a dominant position in the field of parallel software. This
dominance requires other hardware platforms to support CUDA-based software with
performance portability. However, translating CUDA code to other platforms
poses significant challenges due to differences in parallel programming
paradigms and hardware architectures. Existing approaches rely on language
extensions, domain-specific languages (DSLs), or compilers but face limitations
in workload coverage and generalizability. Moreover, these methods often incur
substantial development costs. Recently, LLMs have demonstrated extraordinary
potential in various vertical domains, especially in code-related tasks.
However, the performance of existing LLMs in CUDA transpilation, particularly
for high-performance code, remains suboptimal. The main reason for this
limitation lies in the lack of high-quality training datasets. To address these
challenges, we propose a novel framework for generating high-performance CUDA
and corresponding platform code pairs, leveraging AI compiler and automatic
optimization technology. We further enhance the framework with a graph-based
data augmentation method and introduce HPCTransEval, a benchmark for evaluating
LLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU
transpilation as a case study on leading LLMs. The result demonstrates that our
framework significantly improves CUDA transpilation, highlighting the potential
of LLMs to address compatibility challenges within the CUDA ecosystem.

</details>


### [76] [Federated Learning within Global Energy Budget over Heterogeneous Edge Accelerators](https://arxiv.org/abs/2506.10413)
*Roopkatha Banerjee,Tejus Chandrashekar,Ananth Eswar,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 论文提出了FedJoule框架，通过优化客户端选择和能量预算，在联邦学习中实现高模型精度和低能耗的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中设备与数据异构性导致的能量效率与模型精度难以兼顾的问题，同时探索全局能量预算下的可持续AI。

Method: 使用双层整数线性规划（ILP）结合近似Shapley值和能量-时间预测模型，优化客户端选择与能量分配。

Result: FedJoule在多种能量预算、非独立同分布数据和实际配置下，模型精度比现有方法提升15%，训练时间减少48%。

Conclusion: FedJoule在联邦学习中有效实现了能耗与性能的平衡，为可持续AI提供了可行方案。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. However, optimizing both
energy efficiency and model accuracy remains a challenge, given device and data
heterogeneity. Further, sustainable AI through a global energy budget for FL
has not been explored. We propose a novel optimization problem for client
selection in FL that maximizes the model accuracy within an overall energy
limit and reduces training time. We solve this with a unique bi-level ILP
formulation that leverages approximate Shapley values and energy-time
prediction models to efficiently solve this. Our FedJoule framework achieves
superior training accuracies compared to SOTA and simple baselines for diverse
energy budgets, non-IID distributions, and realistic experiment configurations,
performing 15% and 48% better on accuracy and time, respectively. The results
highlight the effectiveness of our method in achieving a viable trade-off
between energy usage and performance in FL environments.

</details>


### [77] [Automating Multi-Tenancy Performance Evaluation on Edge Compute Nodes](https://arxiv.org/abs/2506.10461)
*Joanna Georgiou,Moysis Symeonides,George Pallis,Marios D. Dikaiakos*

Main category: cs.DC

TL;DR: 提出一个自动基准测试框架，简化边缘计算中多租户性能分析，集成内置监控和常用工作负载测试。


<details>
  <summary>Details</summary>
Motivation: 边缘计算多租户化虽必要，但可能引发安全与性能问题，需优化资源配置以减少资源竞争的影响。

Method: 开发自动基准测试框架，结合监控栈和多样化工作负载（如流分析、数据库操作、机器学习）。

Result: 通过案例分析了多租户对不同硬件配置和负载的影响，框架及实验容器化工具已开源。

Conclusion: 该框架为边缘计算多租户性能优化提供实用工具，支持高效部署与测试。

Abstract: Edge Computing emerges as a promising alternative of Cloud Computing, with
scalable compute resources and services deployed in the path between IoT
devices and Cloud. Since virtualization techniques can be applied on Edge
compute nodes, administrators can share their Edge infrastructures among
multiple users, providing the so-called multi-tenancy. Even though
multi-tenancy is unavoidable, it raises concerns about security and performance
degradation due to resource contention in Edge Computing. For that,
administrators need to deploy services with non-antagonizing profiles and
explore workload co-location scenarios to enhance performance and energy
consumption. Achieving this, however, requires extensive configuration,
deployment, iterative testing, and analysis, an effort-intensive and
time-consuming process. To address this challenge, we introduce an
auto-benchmarking framework designed to streamline the analysis of
multi-tenancy performance in Edge environments. Our framework includes a
built-in monitoring stack and integrates with widely used benchmarking
workloads, such as streaming analytics, database operations, machine learning
applications, and component-based stress testing. We perform a case-driven
analysis and provide valuable insights into the impact of multi-tenancy on Edge
environments with different hardware configurations and diverse workloads.
Finally, the implementation of our framework, along with the containerized
workloads used for experimentation, is publicly available.

</details>


### [78] [TD-Pipe: Temporally-Disaggregated Pipeline Parallelism Architecture for High-Throughput LLM Inference](https://arxiv.org/abs/2506.10470)
*Hongbin Zhang,Taosheng Wei,Zhenyi Zheng,Jiangsu Du,Zhiguang Chen,Yutong Lu*

Main category: cs.DC

TL;DR: TD-Pipe提出了一种时间解耦的管道并行架构，通过分离预填充和解码阶段来消除管道气泡，显著提升了LLM推理的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模增大，管道并行因其低通信需求在面向吞吐量的推理中表现出潜力，但工作负载不平衡和数据依赖导致严重的管道气泡和性能下降。

Method: 采用时间解耦管道并行架构，通过层级控制器、AI贪婪预填充、批次间工作窃取和时空强度比较方法优化性能。

Result: 实验显示TD-Pipe在仅PCIe互联的GPU节点上，吞吐量比现有张量并行方法提高1.91倍，比管道并行方法提高2.73倍。

Conclusion: TD-Pipe通过时间解耦架构和多项优化技术，有效提升了LLM推理的吞吐量和性能。

Abstract: As the model size continuously increases, pipeline parallelism shows great
promise in throughput-oriented LLM inference due to its low demand on
communications. However, imbalanced pipeline workloads and complex data
dependencies in the prefill and decode phases result in massive pipeline
bubbles and further severe performance reduction. To better exploit the
pipeline parallelism for high-throughput LLM inference, we propose TD-Pipe,
with the key idea lies in the temporally-disaggregated pipeline parallelism
architecture. Specifically, this architecture disaggregates the prefill and
decode phases in the temporal dimension, so as to eliminate pipeline bubbles
caused by the phase switching. TD-Pipe identifies potential issues of
exploiting the novel architecture and provides solutions. First, a
hierarchy-controller structure is used to better coordinate devices in pipeline
parallelism by decoupling the scheduling from execution. Second, the AI-based
greedy prefill approach aggressively performs more prefills by predicting the
output length and simulating the memory usage. Third, the inter-batch work
stealing approach dynamically balances decode phase workloads between different
batches to reduce bubbles. Forth, the spatial-temporal intensity comparison
approach determines the optimal switch from decode to prefill by comparing the
performance drop from reduced computational intensity with that from phase
switching bubbles. Extensive experiments show that TD-Pipe effectively
increases the throughput of LLM inference by up to 1.91x over the existing
tensor parallel approach and 2.73x over the existing pipeline parallel approach
on GPU nodes with only PCIe interconnection.

</details>


### [79] [HP2C-DT: High-Precision High-Performance Computer-enabled Digital Twin](https://arxiv.org/abs/2506.10523)
*E. Iraola,M. García-Lorenzo,F. Lordan-Gomis,F. Rossi,E. Prieto-Araujo,R. M. Badia*

Main category: cs.DC

TL;DR: 提出了一种名为HP2C-DT的数字孪生架构，将高性能计算（HPC）融入计算连续体，动态分配任务到边缘、云或HPC资源，提高响应速度和计算能力。


<details>
  <summary>Details</summary>
Motivation: 目前数字孪生在实时响应与计算需求间的平衡是一个挑战，云方案延迟高，边缘方案计算能力不足。

Method: 提出HP2C-DT架构和框架，整合HPC，动态分配任务，并用COMPSs实现跨基础设施的无缝工作负载分配。

Result: 在电网用例中测试，带宽降低10倍，响应速度提升2倍，计算密集型任务实现接近理想的强扩展性。

Conclusion: HPC驱动的数字孪生架构能突破现有局限，使其更智能、快速，适应复杂实际场景。

Abstract: Digital twins are transforming the way we monitor, analyze, and control
physical systems, but designing architectures that balance real-time
responsiveness with heavy computational demands remains a challenge.
Cloud-based solutions often struggle with latency and resource constraints,
while edge-based approaches lack the processing power for complex simulations
and data-driven optimizations.
  To address this problem, we propose the High-Precision High-Performance
Computer-enabled Digital Twin (HP2C-DT) reference architecture, which
integrates High-Performance Computing (HPC) into the computing continuum.
Unlike traditional setups that use HPC only for offline simulations, HP2C-DT
makes it an active part of digital twin workflows, dynamically assigning tasks
to edge, cloud, or HPC resources based on urgency and computational needs.
  Furthermore, to bridge the gap between theory and practice, we introduce the
HP2C-DT framework, a working implementation that uses COMPSs for seamless
workload distribution across diverse infrastructures. We test it in a power
grid use case, showing how it reduces communication bandwidth by an order of
magnitude through edge-side data aggregation, improves response times by up to
2x via dynamic offloading, and maintains near-ideal strong scaling for
compute-intensive workflows across a practical range of resources. These
results demonstrate how an HPC-driven approach can push digital twins beyond
their current limitations, making them smarter, faster, and more capable of
handling real-world complexity.

</details>


### [80] [GPU-Accelerated Distributed QAOA on Large-scale HPC Ecosystems](https://arxiv.org/abs/2506.10531)
*Zhihao Xu,Srikar Chundury,Seongmin Kim,Amir Shehata,Xinyi Li,Ang Li,Tengfei Luo,Frank Mueller,In-Saeng Suh*

Main category: cs.DC

TL;DR: 通过改进分布式量子近似优化算法（DQAOA），结合高性能计算（HPC）系统，实现了更高的可扩展性和效率。实验结果显示，GPU加速的量子模拟比传统CPU模拟快10倍。


<details>
  <summary>Details</summary>
Motivation: 解决高维度、密集型组合优化问题的量子计算方法需要更高的可扩展性和效率。

Method: 采用高级问题分解和并行执行策略，利用消息传递技术在Frontier CPU/GPU超级计算机上优化DQAOA。

Result: 实验结果显示，GPU加速的量子模拟比CPU快10倍，显著提升了DQAOA的性能和可扩展性。

Conclusion: 改进后的DQAOA为大规模问题的量子-经典混合计算提供了实用解决方案，同时为未来的HPC-量子计算系统奠定了基础。

Abstract: Quantum computing holds great potential to accelerate the process of solving
complex combinatorial optimization problems. The Distributed Quantum
Approximate Optimization Algorithm (DQAOA) addresses high-dimensional, dense
problems using current quantum computing techniques and high-performance
computing (HPC) systems. In this work, we improve the scalability and
efficiency of DQAOA through advanced problem decomposition and parallel
execution using message passing on the Frontier CPU/GPU supercomputer. Our
approach ensures efficient quantum-classical workload management by
distributing large problem instances across classical and quantum resources.
Experimental results demonstrate that enhanced decomposition strategies and
GPU-accelerated quantum simulations significantly improve DQAOA's performance,
achieving up to 10x speedup over CPU-based simulations. This advancement
enables better scalability for large problem instances, supporting the
practical deployment of GPU systems for hybrid quantum-classical applications.
We also highlight ongoing integration efforts using the Quantum Framework (QFw)
to support future HPC-quantum computing systems.

</details>


### [81] [6G Infrastructures for Edge AI: An Analytical Perspective](https://arxiv.org/abs/2506.10570)
*Kurt Horvath,Shpresa Tuda,Blerta Idrizi,Stojan Kitanov,Fisnik Doko,Dragi Kimovski*

Main category: cs.DC

TL;DR: 论文探讨了AI与物联网结合对网络性能的需求，指出5G在AI边缘应用中的局限性，并通过实证评估揭示了5G在欧洲的延迟问题，提出了优化6G生态系统的建议。


<details>
  <summary>Details</summary>
Motivation: AI与物联网的融合需要低延迟、高吞吐量的网络支持，而5G在实际部署中表现不足，无法满足AI应用的需求。

Method: 通过对欧洲5G网络基础设施的实证评估，测量延迟，并与AI应用的需求对比。

Result: 5G的延迟（61ms至110ms）超过了AI应用需求270%，显示当前部署存在明显不足。

Conclusion: 提出了一系列建议，以缩小5G性能与下一代AI应用需求之间的差距，推动6G生态系统的优化。

Abstract: The convergence of Artificial Intelligence (AI) and the Internet of Things
has accelerated the development of distributed, network-sensitive applications,
necessitating ultra-low latency, high throughput, and real-time processing
capabilities. While 5G networks represent a significant technological
milestone, their ability to support AI-driven edge applications remains
constrained by performance gaps observed in real-world deployments. This paper
addresses these limitations and highlights critical advancements needed to
realize a robust and scalable 6G ecosystem optimized for AI applications.
Furthermore, we conduct an empirical evaluation of 5G network infrastructure in
central Europe, with latency measurements ranging from 61 ms to 110 ms across
different close geographical areas. These values exceed the requirements of
latency-critical AI applications by approximately 270%, revealing significant
shortcomings in current deployments. Building on these findings, we propose a
set of recommendations to bridge the gap between existing 5G performance and
the requirements of next-generation AI applications.

</details>


### [82] [Graph-based Gossiping for Communication Efficiency in Decentralized Federated Learning](https://arxiv.org/abs/2506.10607)
*Huong Nguyen,Hong-Tri Nguyen,Praveen Kumar Donta,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.DC

TL;DR: 针对联邦学习中的中心化服务器问题，本文研究了去中心化学习中的通信效率，并提出了一种基于图的gossip机制，显著提升了通信效率。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化学习中因消息交换增加导致的通信效率问题，并弥补现有方法在真实分布式环境模拟上的不足。

Method: 提出基于最小生成树和图着色的图gossip机制，优化网络结构和调度，并在真实物理设备上验证。

Result: 实验显示该方法在不同拓扑和数据量下均有效，带宽和传输时间分别减少了约8倍和4.4倍。

Conclusion: 该方法显著提升了通信效率，适用于真实去中心化学习场景。

Abstract: Federated learning has emerged as a privacy-preserving technique for
collaborative model training across heterogeneously distributed silos. Yet, its
reliance on a single central server introduces potential bottlenecks and risks
of single-point failure. Decentralizing the server, often referred to as
decentralized learning, addresses this problem by distributing the server role
across nodes within the network. One drawback regarding this pure
decentralization is it introduces communication inefficiencies, which arise
from increased message exchanges in large-scale setups. However, existing
proposed solutions often fail to simulate the real-world distributed and
decentralized environment in their experiments, leading to unreliable
performance evaluations and limited applicability in practice. Recognizing the
lack from prior works, this work investigates the correlation between model
size and network latency, a critical factor in optimizing decentralized
learning communication. We propose a graph-based gossiping mechanism, where
specifically, minimum spanning tree and graph coloring are used to optimize
network structure and scheduling for efficient communication across various
network topologies and message capacities. Our approach configures and manages
subnetworks on real physical routers and devices and closely models real-world
distributed setups. Experimental results demonstrate that our method
significantly improves communication, compatible with different topologies and
data sizes, reducing bandwidth and transfer time by up to circa 8 and 4.4
times, respectively, compared to naive flooding broadcasting methods.

</details>


### [83] [Deployment of Containerized Simulations in an API-Driven Distributed Infrastructure](https://arxiv.org/abs/2506.10642)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: SUNRISE是一个统一利用虚拟原型解决方案的基础设施，旨在简化访问多种仿真技术并提升合作效率。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统市场的动态性使得虚拟原型成为硬件/软件协同设计的重要工具，但现有的解决方案多样且分散。

Method: 通过提供统一的基础设施（SUNRISE），整合多种仿真技术，利用分散计算资源，并定义开放的API。

Result: 用户能够更高效地利用虚拟原型解决方案，促进技术合作与资源共享。

Conclusion: SUNRISE为虚拟原型领域提供了一种高效且开放的统一方法，有助于推动硬件/软件协同设计的发展。

Abstract: The increasingly dynamic market for embedded systems makes virtual prototypes
an indispensable tool for hardware/software codesign. The broad acceptance of
the methodology has led to a diverse range of solutions: from open-source, pure
console-based simulators to highly capable commercial simulation tools. In this
work we present SUNRISE, an infrastructure to provide users a unified approach
to utilizing virtual prototyping solutions, facilitate access to various
simulation technologies and boost cooperation by leveraging decentralized
compute resources for deployment of simulation workloads and definition of open
APIs.

</details>


### [84] [Towards Sustainable Computing: Exploring Energy Consumption Efficiency of Alternative Configurations and Workloads in an Open Source Messaging System](https://arxiv.org/abs/2506.10693)
*Maria Voreakou,George Kousiouris,Mara Nikolaidou*

Main category: cs.DC

TL;DR: 研究探讨了RabbitMQ的能量基准测试，比较了不同架构的能耗，最高可节省31%的电力消耗。


<details>
  <summary>Details</summary>
Motivation: 随着大规模计算基础设施能耗问题日益突出，尤其是云环境等集中式系统的需求增长，需优化消息系统的能源效率。

Method: 通过实验测试RabbitMQ在不同负载和配置下的能耗，比较不同架构的能源消耗表现。

Result: 不同架构选择可导致能耗差异，最高节省31%的电力消耗，数据集已公开。

Conclusion: 研究表明架构选择对能耗有显著影响，公开数据集有助于进一步研究和建模。

Abstract: Energy consumption in current large scale computing infrastructures is
becoming a critical issue, especially with the growing demand for centralized
systems such as cloud environments. With the advancement of microservice
architectures and the Internet of Things, messaging systems have become an
integral and mainstream part of modern computing infrastructures, carrying out
significant workload in a majority of applications. In this paper, we describe
an experimental process to explore energy-based benchmarking for RabbitMQ, one
of the main open source messaging frameworks. The involved system is described,
as well as required components, and setup scenarios, involving different
workloads and configurations among the tests as well as messaging system use
cases. Alternative architectures are investigated and compared from an energy
consumption point of view, for different message rates and consumer numbers.
Differences in architectural selection have been quantified and can lead to up
to 31\% reduction in power consumption. The resulting dataset is made publicly
available and can thus prove helpful for architectures' comparison,
energy-based cost modeling, and beyond.

</details>


### [85] [The Impact of Partial Computations on the Red-Blue Pebble Game](https://arxiv.org/abs/2506.10854)
*Pál András Papp,Aleksandros Sobczyk,A. N. Yzelman*

Main category: cs.DC

TL;DR: 本文扩展了经典的红蓝鹅卵石游戏（RBP），引入部分计算步骤（PRBP），证明其能在某些情况下显著降低I/O成本，并讨论了相关工具和下限。


<details>
  <summary>Details</summary>
Motivation: 原RBP模型要求操作的所有输入必须同时存在于快速内存中，而实际计算中可以通过逐步聚合输入来实现部分计算。PRBP提供了更现实的成本模型。

Method: 通过比较PRBP与原RBP，分析部分计算步骤对降低I/O成本的影响，并扩展$S$-分区工具以推导下限。

Result: 部分计算可线性降低I/O成本，但在特定DAG中判断是否适用是NP难问题。此外，PRBP的最优成本仍是NP难近似。

Conclusion: PRBP通过部分计算步骤提供更优的I/O成本模型，但其复杂性仍高，需要进一步研究。

Abstract: We study an extension of the well-known red-blue pebble game (RBP) with
partial computation steps, inspired by the recent work of Sobczyk. While the
original RBP assumes that we need to have all the inputs of an operation in
fast memory at the same time, in many concrete computations, the inputs can be
aggregated one by one into the final output value. These partial computation
steps can enable pebbling strategies with much smaller I/O cost, and in
settings where such a step-by-step aggregation is possible, this extended
red-blue pebble game offers a much more realistic cost model.
  We establish the fundamental properties of this partial-computing red-blue
pebble game (PRBP), and compare it to the original RBP. We begin with some
simple examples where allowing partial computations can decrease the optimal
I/O cost. It is also shown that the cost can decrease by up to a linear factor
this way, but in general, it is NP-hard to decide whether partial computations
allow for a smaller cost in a specific DAG. We then discuss how $S$-partitions,
a crucial tool for deriving I/O lower bounds in RBP, can be adapted to the PRBP
model. These new tools are then used to establish lower bounds on the I/O cost
of some prominent computational tasks. Finally, we also adapt a hardness result
from RBP, showing that the optimum cost is still NP-hard to approximate in PRBP
to any reasonable factor.

</details>


### [86] [Adaptive Job Scheduling in Quantum Clouds Using Reinforcement Learning](https://arxiv.org/abs/2506.10889)
*Waylon Luo,Jiapeng Zhao,Tong Zhan,Qiang Guan*

Main category: cs.DC

TL;DR: 当前量子系统面临量子比特数有限、相干时间短和易受噪声影响等问题，阻碍了大规模复杂电路的有效执行。论文提出了一种基于模拟的工具，支持分布式调度和在联网量子处理器上并行执行任务，并通过模拟比较了四种调度策略，强调了并行、噪声感知调度对提升分布式量子计算性能的价值。


<details>
  <summary>Details</summary>
Motivation: 量子硬件的发展滞后于量子算法的进步，且硬件性能不一致和量子噪声问题影响了计算的稳定性和准确性。因此，如何在资源受限和噪声环境下优化量子任务的调度和资源协调成为关键。

Method: 开发了一种模拟工具，用于分布式调度和并行执行量子任务，支持将超限电路分解并在多个量子处理器上通过经典通信并行执行。比较了四种调度策略，包括基于强化学习的模型。

Result: 通过模拟评估，论文展示了不同调度策略在运行效率、保真度和通信开销上的权衡，并表明并行、噪声感知调度能显著提升分布式计算吞吐量。

Conclusion: 分布式量子计算需结合并行调度和噪声管理，论文提出的工具和方法为优化资源受限的量子系统提供了有效解决方案。

Abstract: Present-day quantum systems face critical bottlenecks, including limited
qubit counts, brief coherence intervals, and high susceptibility to errors-all
of which obstruct the execution of large and complex circuits. The advancement
of quantum algorithms has outpaced the capabilities of existing quantum
hardware, making it difficult to scale computations effectively. Additionally,
inconsistencies in hardware performance and pervasive quantum noise undermine
system stability and computational accuracy. To optimize quantum workloads
under these constraints, strategic approaches to task scheduling and resource
coordination are essential. These methods must aim to accelerate processing,
retain operational fidelity, and reduce the communication burden inherent to
distributed setups. One of the persistent challenges in this domain is how to
efficiently divide and execute large circuits across multiple quantum
processors (QPUs), especially in error-prone environments. In response, we
introduce a simulation-based tool that supports distributed scheduling and
concurrent execution of quantum jobs on networked QPUs connected via real-time
classical channels. The tool models circuit decomposition for workloads that
surpass individual QPU limits, allowing for parallel execution through
inter-processor communication. Using this simulation environment, we compare
four distinct scheduling techniques-among them, a model informed by
reinforcement learning. These strategies are evaluated across multiple metrics,
including runtime efficiency, fidelity preservation, and communication costs.
Our analysis underscores the trade-offs inherent in each approach and
highlights how parallelized, noise-aware scheduling can meaningfully improve
computational throughput in distributed quantum infrastructures.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [87] [GPU Acceleration of SQL Analytics on Compressed Data](https://arxiv.org/abs/2506.10092)
*Zezhou Huang,Krystian Sakowski,Hans Lehnert,Wei Cui,Carlo Curino,Matteo Interlandi,Marius Dumitru,Rathijit Sen*

Main category: cs.DB

TL;DR: 提出了在GPU上直接操作轻量级压缩数据的新方法，显著提升了大规模数据集的查询性能。


<details>
  <summary>Details</summary>
Motivation: GPU的高带宽内存（HBM）适合加速SQL分析工作负载，但HBM容量有限，现有解决方案在扩展到大规模数据集时面临性能和成本挑战。

Method: 开发了一套新方法，直接在压缩数据上运行查询，包括RLE、字典编码等技术，并利用PyTorch张量操作实现跨设备兼容性。

Result: 实验显示，相比CPU-only系统，性能提升了一个数量级，适用于无法完整加载到GPU内存的生产数据集。

Conclusion: 该方法为GPU在更广泛的应用场景中采用铺平了道路，并与其他扩展机制互补。

Abstract: GPUs are uniquely suited to accelerate (SQL) analytics workloads thanks to
their massive compute parallelism and High Bandwidth Memory (HBM) -- when
datasets fit in the GPU HBM, performance is unparalleled. Unfortunately, GPU
HBMs remain typically small when compared with lower-bandwidth CPU main memory.
Besides brute-force scaling across many GPUs, current solutions to accelerate
queries on large datasets include leveraging data partitioning and loading
smaller data batches in GPU HBM, and hybrid execution with a connected device
(e.g., CPUs). Unfortunately, these approaches are exposed to the limitations of
lower main memory and host-to-device interconnect bandwidths, introduce
additional I/O overheads, or incur higher costs. This is a substantial problem
when trying to scale adoption of GPUs on larger datasets. Data compression can
alleviate this bottleneck, but to avoid paying for costly
decompression/decoding, an ideal solution must include computation primitives
to operate directly on data in compressed form.
  This is the focus of our paper: a set of new methods for running queries
directly on light-weight compressed data using schemes such as Run-Length
Encoding (RLE), index encoding, bit-width reductions, and dictionary encoding.
Our novelty includes operating on multiple RLE columns without decompression,
handling heterogeneous column encodings, and leveraging PyTorch tensor
operations for portability across devices. Experimental evaluations show
speedups of an order of magnitude compared to state-of-the-art commercial
CPU-only analytics systems, for real-world queries on a production dataset that
would not fit into GPU memory uncompressed. This work paves the road for GPU
adoption in a much broader set of use cases, and it is complementary to most
other scale-out or fallback mechanisms.

</details>


### [88] [A Unifying Algorithm for Hierarchical Queries](https://arxiv.org/abs/2506.10238)
*Mahmoud Abo Khamis,Jesse Comer,Phokion Kolaitis,Sudeepa Roy,Val Tannen*

Main category: cs.DB

TL;DR: 层次查询定义了SJF-BCQ在概率数据库评估、Shapley值计算和bag-set最大化问题中的可解性边界。


<details>
  <summary>Details</summary>
Motivation: 旨在确定层次查询在不同自然算法问题（如bag-set最大化）中的可解性边界，并提供一个统一的算法框架。

Method: 利用2-monoid代数结构设计多项式时间算法，适用于层次查询的三种不同问题。

Result: 非层次查询的bag-set最大化问题是NP完全优化问题，层次查询则可通过统一算法高效解决。

Conclusion: 层次查询是多种算法问题的可解性边界，统一的2-monoid框架为解决这些问题提供了简洁的途径。

Abstract: The class of hierarchical queries is known to define the boundary of the
dichotomy between tractability and intractability for the following two
extensively studied problems about self-join free Boolean conjunctive queries
(SJF-BCQ): (i) evaluating a SJF-BCQ on a tuple-independent probabilistic
database; (ii) computing the Shapley value of a fact in a database on which a
SJF-BCQ evaluates to true. Here, we establish that hierarchical queries define
also the boundary of the dichotomy between tractability and intractability for
a different natural algorithmic problem, which we call the "bag-set
maximization" problem. The bag-set maximization problem associated with a
SJF-BCQ $Q$ asks: given a database $\cal D$, find the biggest value that $Q$
takes under bag semantics on a database $\cal D'$ obtained from $\cal D$ by
adding at most $\theta$ facts from another given database $\cal D^r$.
  For non-hierarchical queries, we show that the bag-set maximization problem
is an NP-complete optimization problem. More significantly, for hierarchical
queries, we show that all three aforementioned problems (probabilistic query
evaluation, Shapley value computation, and bag-set maximization) admit a single
unifying polynomial-time algorithm that operates on an abstract algebraic
structure, called a "2-monoid". Each of the three problems requires a different
instantiation of the 2-monoid tailored for the problem at hand.

</details>


### [89] [A Hybrid Heuristic Framework for Resource-Efficient Querying of Scientific Experiments Data](https://arxiv.org/abs/2506.10422)
*Mayank Patel,Minal Bhise*

Main category: cs.DB

TL;DR: 本文提出了一种轻量级资源感知框架RAW-HF，用于高效查询原始数据，显著减少工作负载执行时间和资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统数据库和HTAP系统在加载数据时消耗大量资源，而原位引擎可能多次重新解析数据，增加了资源利用率和成本。

Method: RAW-HF框架通过优化资源分配和最大化现有资源利用率，提高查询效率。

Result: 在SDSS和LOD数据集上，RAW-HF减少了90%和85%的工作负载执行时间，CPU和IO资源消耗分别降低26%和25%。

Conclusion: RAW-HF在资源利用和性能提升方面优于现有技术，如传统DBMS和WA技术。

Abstract: Scientific experiments and modern applications are generating large amounts
of data every day. Most organizations utilize In-house servers or Cloud
resources to manage application data and workload. The traditional database
management system (DBMS) and HTAP systems spend significant time & resources to
load the entire dataset into DBMS before starting query execution. On the other
hand, in-situ engines may reparse required data multiple times, increasing
resource utilization and data processing costs. Additionally, over or
under-allocation of resources also increases application running costs. This
paper proposes a lightweight Resource Availability &Workload aware Hybrid
Framework (RAW-HF) to optimize querying raw data by utilizing existing finite
resources efficiently. RAW-HF includes modules that help optimize the resources
required to execute a given workload and maximize the utilization of existing
resources. The impact of applying RAW-HF to real-world scientific dataset
workloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data
(LOD) presented over 90% and 85% reduction in workload execution time (WET)
compared to widely used traditional DBMS PostgreSQL. The overall CPU, IO
resource utilization, and WET have been reduced by 26%, 25%, and 26%,
respectively, while improving memory utilization by 33%, compared to the
state-of-the-art workload-aware partial loading technique (WA) proposed for
hybrid systems. A comparison of MUAR technique used by RAW-HF with machine
learning based resource allocation techniques like PCC is also presented.

</details>


### [90] [S3 Mirror: S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable with DBOS](https://arxiv.org/abs/2506.10886)
*Steven Vasquez-Grinnell,Alex Poliakov*

Main category: cs.DB

TL;DR: S3Mirror是一款用于快速、可靠且可观察地在S3存储桶之间传输大型基因组测序数据集的开源应用，基于DBOSTransact框架，性能优于AWS DataSync且成本更低。


<details>
  <summary>Details</summary>
Motivation: 满足大型制药组织对快速、可靠且可观察的大数据集传输需求。

Method: 使用DBOSTransact持久执行框架开发S3Mirror，并在多种环境中进行性能与成本测试。

Result: 在DBOS Cloud Pro中，S3Mirror比AWS DataSync快40倍且成本更低，同时具备故障恢复和实时传输监控能力。

Conclusion: S3Mirror是一款高效、经济且可靠的大数据集传输工具，适用于多种环境。

Abstract: To meet the needs of a large pharmaceutical organization, we set out to
create S3Mirror - an application for transferring large genomic sequencing
datasets between S3 buckets quickly, reliably, and observably. We used the
DBOSTransact durable execution framework to achieve these goals and benchmarked
the performance and cost of the application. S3Mirror is an open source DBOS
Python application that can run in a variety of environments, including DBOS
Cloud Pro where it runs as much as 40x faster than AWS DataSync at a fraction
of the cost. Moreover, S3Mirror is resilient to failures and allows for
real-time filewise observability of ongoing and past transfers.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [91] [CarbonSet: A Dataset to Analyze Trends and Benchmark the Sustainability of CPUs and GPUs](https://arxiv.org/abs/2506.10373)
*Jiajun Hu,Chetan Choppali Sudarshan,Vidya A. Chhabria,Aman Arora*

Main category: cs.AR

TL;DR: 芯片产业持续发展高性能处理器，但快速扩张导致碳排放增加，环境可持续性问题凸显。本文提出CarbonSet数据集，整合过去十年CPU和GPU的可持续性与性能指标，分析旗舰处理器的可持续性趋势，揭示现代处理器设计仍不环保，碳排放显著增加。


<details>
  <summary>Details</summary>
Motivation: 芯片生产碳排放激增引发环境可持续性问题，现有研究缺乏对整个芯片生命周期的可持续性趋势分析。

Method: 提出CarbonSet数据集，整合CPU和GPU的可持续性与性能指标，并对其进行分析。

Result: 现代处理器设计未实现可持续性，过去三年碳排放增加超过50倍，能效和先进制程是主要挑战。

Conclusion: 需优化设计和制造，以减少碳排放，实现芯片产业的可持续发展。

Abstract: Over the years, the chip industry has consistently developed high-performance
processors to address the increasing demands across diverse applications.
However, the rapid expansion of chip production has significantly increased
carbon emissions, raising critical concerns about environmental sustainability.
While researchers have previously modeled the carbon footprint (CFP) at both
system and processor levels, a holistic analysis of sustainability trends
encompassing the entire chip lifecycle remains lacking. This paper presents
CarbonSet, a comprehensive dataset integrating sustainability and performance
metrics for CPUs and GPUs over the past decade. CarbonSet aims to benchmark and
assess the design of next-generation processors. Leveraging this dataset, we
conducted detailed analysis of flagship processors' sustainability trends over
the last decade. This paper further highlights that modern processors are not
yet sustainably designed, with total carbon emissions increasing more than
50$\times$ in the past three years due to the surging demand driven by the AI
boom. Power efficiency remains a significant concern, while advanced process
nodes pose new challenges requiring to effectively amortize the dramatically
increased manufacturing carbon emissions.

</details>


### [92] [EasyDRAM: An FPGA-based Infrastructure for Fast and Accurate End-to-End Evaluation of Emerging DRAM Techniques](https://arxiv.org/abs/2506.10441)
*Oğuzhan Canpolat,Ataberk Olgun,David Novo,Oğuz Ergin,Onur Mutlu*

Main category: cs.AR

TL;DR: EasyDRAM是一个基于FPGA的框架，通过高语言（如C++）实现DRAM技术评估，解决了现有平台需要硬件描述语言和难以准确模拟现代系统的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的FPGA平台在评估DRAM技术时存在两大局限：一是需要硬件描述语言的专业知识，二是难以准确模拟现代计算系统。

Method: EasyDRAM采用C++实现DRAM技术，并通过时间缩放技术解耦处理器与DRAM接口，准确模拟系统时序行为。

Result: EasyDRAM显著降低了评估DRAM技术的门槛，并提升了模拟准确性。

Conclusion: EasyDRAM为内存系统设计的创新提供了更快速、准确的评估工具，并开源以支持未来研究。

Abstract: DRAM is a critical component of modern computing systems. Recent works
propose numerous techniques (that we call DRAM techniques) to enhance
DRAM-based computing systems' throughput, reliability, and computing
capabilities (e.g., in-DRAM bulk data copy). Evaluating the system-wide
benefits of DRAM techniques is challenging as they often require modifications
across multiple layers of the computing stack. Prior works propose FPGA-based
platforms for rapid end-to-end evaluation of DRAM techniques on real DRAM
chips. Unfortunately, existing platforms fall short in two major aspects: (1)
they require deep expertise in hardware description languages, limiting
accessibility; and (2) they are not designed to accurately model modern
computing systems.
  We introduce EasyDRAM, an FPGA-based framework for rapid and accurate
end-to-end evaluation of DRAM techniques on real DRAM chips. EasyDRAM overcomes
the main drawbacks of prior FPGA-based platforms with two key ideas. First,
EasyDRAM removes the need for hardware description language expertise by
enabling developers to implement DRAM techniques using a high-level language
(C++). At runtime, EasyDRAM executes the software-defined memory system design
in a programmable memory controller. Second, EasyDRAM tackles a fundamental
challenge in accurately modeling modern systems: real processors typically
operate at higher clock frequencies than DRAM, a disparity that is difficult to
replicate on FPGA platforms. EasyDRAM addresses this challenge by decoupling
the processor-DRAM interface and advancing the system state using a novel
technique we call time scaling, which faithfully captures the timing behavior
of the modeled system.
  We believe and hope that EasyDRAM will enable innovative ideas in memory
system design to rapidly come to fruition. To aid future research EasyDRAM
implementation is open sourced at https://github.com/CMU-SAFARI/EasyDRAM.

</details>


### [93] [Towards Zero-Stall Matrix Multiplication on Energy-Efficient RISC-V Clusters for Machine Learning Acceleration](https://arxiv.org/abs/2506.10921)
*Luca Colagrande,Lorenzo Leone,Maximilian Coco,Andrei Deaconeasa,Luca Benini*

Main category: cs.AR

TL;DR: 本文提出了一种低开销优化的微架构，通过消除控制流和内存访问的低效性，显著提升ML加速器的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 为满足机器学习工作负载日益增长的计算需求，需要设计在效率和灵活性之间取得最佳平衡的ML加速器。

Method: 引入零开销循环嵌套和零冲突内存子系统，优化控制流和内存访问；提出一种新型双缓冲感知互连技术。

Result: 实现了接近理想的利用率（96.1%-99.4%），性能提升11%，能效提升8%，与专用加速器相比能效差距仅12%。

Conclusion: 该微架构在保持完全可编程性的同时，显著提升了ML加速器的性能和能效，适用于更广泛的工作负载。

Abstract: The growing computational demands of machine learning (ML) workloads have
driven the design of ML accelerators aiming at an optimal tradeoff between
efficiency and flexibility. A widely explored architecture for flexible ML
accelerators is based on clusters of lightweight instruction processors sharing
multi-banked L1 memory, augmented with specialized instruction extensions for
key ML-related computations, such as matrix multiplication (matmul). However,
instruction extensions should be coupled with microarchitectural optimizations
that remove inefficiencies due to control flow (loop handling) and memory
access, without drastically increasing processor complexity. Moving from a
state-of-the-art (SoA) ML accelerator cluster based on RISC-V processors, we
propose a low-overhead optimized microarchitecture that eliminates these
inefficiencies almost entirely while retaining programmability. We introduce
"zero-overhead loop nests" to remove control overheads, and a "zero-conflict
memory subsystem", leveraging a novel double-buffering-aware interconnect, to
eliminate bank conflicts in L1 memory. With these enhancements, we attain
near-ideal utilizations between 96.1% and 99.4%, achieving 11% performance and
8% energy efficiency improvements over the baseline SoA RISC-V cluster. We
demonstrate comparable utilizations and performance to a specialized SoA
accelerator, with only 12% difference in energy efficiency, while providing a
fully-programmable general-purpose solution supporting a significantly wider
range of workloads.

</details>


### [94] [MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis Inside the Storage Subsystem](https://arxiv.org/abs/2506.10931)
*Melina Soysal,Konstantina Koliogeorgi,Can Firtina,Nika Mansouri Ghiasi,Rakesh Nadig,Haiyu Mayo,Geraldo F. Oliveira,Yu Liang,Klea Zambaku,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.AR

TL;DR: MARS是一种存储为中心的系统，通过优化数据处理和计算，显著提升了RSGA的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 由于测序技术快速发展，软件基础的RSGA难以匹配原始信号生成的吞吐量，且I/O数据移动成为性能和能耗瓶颈。

Method: MARS通过硬件/软件协同设计，包括过滤机制、量化方案及利用存储内部资源加速RSGA步骤。

Result: MARS比现有软件和硬件加速方案分别快93倍和40倍，能耗降低427倍和72倍。

Conclusion: MARS通过存储内部资源的高效利用，有效解决了RSGA的数据移动和计算瓶颈问题。

Abstract: Raw signal genome analysis (RSGA) has emerged as a promising approach to
enable real-time genome analysis by directly analyzing raw electrical signals.
However, rapid advancements in sequencing technologies make it increasingly
difficult for software-based RSGA to match the throughput of raw signal
generation. This paper demonstrates that while hardware acceleration techniques
can significantly accelerate RSGA, the high volume of genomic data shifts the
performance and energy bottleneck from computation to I/O data movement. As
sequencing throughput increases, I/O overhead becomes the main contributor to
both runtime and energy consumption. Therefore, there is a need to design a
high-performance, energy-efficient system for RSGA that can both alleviate the
data movement bottleneck and provide large acceleration capabilities. We
propose MARS, a storage-centric system that leverages the heterogeneous
resources within modern storage systems (e.g., storage-internal DRAM, storage
controller, flash chips) alongside their large storage capacity to tackle both
data movement and computational overheads of RSGA in an area-efficient and
low-cost manner. MARS accelerates RSGA through a novel hardware/software
co-design approach. First, MARS modifies the RSGA pipeline via two filtering
mechanisms and a quantization scheme, reducing hardware demands and optimizing
for in-storage execution. Second, MARS accelerates the RSGA steps directly
within the storage by leveraging both Processing-Near-Memory and
Processing-Using-Memory paradigms. Third, MARS orchestrates the execution of
all steps to fully exploit in-storage parallelism and minimize data movement.
Our evaluation shows that MARS outperforms basecalling-based software and
hardware-accelerated state-of-the-art read mapping pipelines by 93x and 40x, on
average across different datasets, while reducing their energy consumption by
427x and 72x.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [95] [Faster CONGEST Approximation Algorithms for Maximum Weighted Independent Set in Sparse Graphs](https://arxiv.org/abs/2506.10845)
*Salwa Faour,Fabian Kuhn*

Main category: cs.DS

TL;DR: 论文研究了分布式环境下带权最大独立集问题的确定性算法，针对树和有界树性图，提出了紧的时间复杂度上下界和改进的近似算法。


<details>
  <summary>Details</summary>
Motivation: 最大独立集问题是经典的优化问题，但在分布式环境中研究较少，尤其是带权版本。论文旨在填补这一空白，针对特定图类提供高效的确定性算法。

Method: 论文使用确定性分布式CONGEST算法，针对树和有界树性图，设计了不同的近似算法，包括直接应用局部舍入框架和改进现有结果的算法。

Result: 在树结构中，证明了确定性算法的紧时间复杂度；在有界树性图中，提出了两种不同近似比的算法，改进了现有结果。

Conclusion: 论文为分布式环境下带权最大独立集问题提供了紧的时间复杂度界限和高效的近似算法，为相关研究提供了新的理论支持。

Abstract: The maximum independent set problem is a classic optimization problem that
has also been studied quite intensively in the distributed setting. While the
problem is hard to approximate in general, there are good approximation
algorithms known for several sparse graph families. In this paper, we consider
deterministic distributed CONGEST algorithms for the weighted version of the
problem in trees and graphs of bounded arboricity.
  For trees, we prove that the task of deterministically computing a
$(1-\epsilon)$-approximate solution to the maximum weight independent set
(MWIS) problem has a tight $\Theta(\log^*(n) / \epsilon)$ complexity. The lower
bound already holds on unweighted oriented paths. On the upper bound side, we
show that the bound can be achieved even in unrooted trees.
  For graphs $G=(V,E)$ of arboricity $\beta>1$, we give two algorithms. If the
sum of all node weights is $w(V)$, we show that for any $\epsilon>0$, an
independent set of weight at least $(1-\epsilon)\cdot \frac{w(V)}{4\beta}$ can
be computed in $O(\log^2(\beta/\epsilon)/\epsilon + \log^* n)$ rounds. This
result is obtained by a direct application of the local rounding framework of
Faour, Ghaffari, Grunau, Kuhn, and Rozho\v{n} [SODA '23]. We further show that
for any $\epsilon>0$, an independent set of weight at least
$(1-\epsilon)\cdot\frac{w(V)}{2\beta+1}$ can be computed in
$O(\log^3(\beta)\cdot\log(1/\epsilon)/\epsilon^2 \cdot\log n)$ rounds. This
improves on a recent result of Gil [OPODIS '23], who showed that a
$1/\lfloor(2+\epsilon)\beta\rfloor$-approximation to the MWIS problem can be
computed in $O(\beta\cdot\log n)$ rounds. As an intermediate step, we design an
algorithm to compute an independent set of total weight at least
$(1-\epsilon)\cdot\sum_{v\in V}\frac{w(v)}{deg(v)+1}$ in time
$O(\log^3(\Delta)\cdot\log(1/\epsilon)/\epsilon + \log^* n)$, where $\Delta$ is
the maximum degree of the graph.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [96] [Mind the Gap: Revealing Security Barriers through Situational Awareness of Small and Medium Business Key Decision-Makers](https://arxiv.org/abs/2506.10025)
*Yuanhaur Chang,Oren Heller,Yaniv Shlomo,Iddo Bar-Noy,Ella Bokobza,Michal Grinstein-Weiss,Ning Zhang*

Main category: cs.CR

TL;DR: 研究了中小型企业高管在网络安全决策中的认知和行为，通过访谈和调查分析其风险感知与防御措施选择。


<details>
  <summary>Details</summary>
Motivation: 中小型企业高管缺乏网络安全意识和知识，导致决策不足，需要研究其行为以提高安全水平。

Method: 采用混合方法，包括21次半结构化访谈与322份在线调查，结合主题分析和情境感知模型。

Result: 揭示了高管的风险感知与防御选择，构建了结构方程模型以提升安全意识。

Conclusion: 提出干预措施，帮助中小型企业克服网络安全挑战。

Abstract: Key decision-makers in small and medium businesses (SMBs) often lack the
awareness and knowledge to implement cybersecurity measures effectively. To
gain a deeper understanding of how SMB executives navigate cybersecurity
decision-making, we deployed a mixed-method approach, conducting
semi-structured interviews (n=21) and online surveys (n=322) with SMB key
decision-makers. Using thematic analysis, we revealed SMB decision-makers'
perceived risks in terms of the digital assets they valued, and found reasons
for their choice of defense measures and factors impacting security perception.
We employed the situational awareness model to characterize decision-makers
based on cybersecurity awareness, identifying those who have comparatively low
awareness in the fight against adversaries. We further explored the
relationship between awareness and business attributes, and constructed a
holistic structural equation model to understand how awareness can be improved.
Finally, we proposed interventions to help SMBs overcome potential challenges.

</details>


### [97] [Multiverse Privacy Theory for Contextual Risks in Complex User-AI Interactions](https://arxiv.org/abs/2506.10042)
*Ece Gumusel*

Main category: cs.CR

TL;DR: 该论文介绍了多元隐私理论（Multiverse Privacy Theory），通过模拟用户隐私决策的平行宇宙来研究隐私问题。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能交互的增加，用户面临复杂的隐私决策，传统的隐私理论难以应对这种不确定性。

Method: 提出了多元隐私理论框架，模拟用户隐私决策的平行宇宙，结合情境完整性、动态偏好和概率决策进行分析。

Result: 该理论为理解隐私问题提供了新视角，尤其是通过平行宇宙的模拟。

Conclusion: 未来将基于真实场景调查数据进一步探索该理论的应用。

Abstract: In an era of increasing interaction with artificial intelligence (AI), users
face evolving privacy decisions shaped by complex, uncertain factors. This
paper introduces Multiverse Privacy Theory, a novel framework in which each
privacy decision spawns a parallel universe, representing a distinct potential
outcome based on user choices over time. By simulating these universes, this
theory provides a foundation for understanding privacy through the lens of
contextual integrity, evolving preferences, and probabilistic decision-making.
Future work will explore its application using real-world, scenario-based
survey data.

</details>


### [98] [Expert-in-the-Loop Systems with Cross-Domain and In-Domain Few-Shot Learning for Software Vulnerability Detection](https://arxiv.org/abs/2506.10104)
*David Farr,Kevin Talty,Alexandra Farr,John Stockdale,Iain Cruickshank,Jevin West*

Main category: cs.CR

TL;DR: 论文研究了使用大型语言模型（LLM）检测Python代码漏洞的策略，比较了零样本、少样本跨域和少样本域内提示方法，发现少样本方法表现更优，结合置信度路由可提升效率。


<details>
  <summary>Details</summary>
Motivation: 随着网络威胁日益复杂，需要快速准确的漏洞检测方法。

Method: 通过模拟检测Python代码中的已知CWEs，比较零样本和两种少样本提示策略，结合置信度路由优化专家投入。

Result: 少样本提示显著优于零样本，结合置信路由可优化自动化与专家监督的平衡。LLM在模拟环境中展现跨漏洞类别的泛化能力。

Conclusion: LLM在漏洞检测中具有潜力，但模型可靠性、可解释性和对抗鲁棒性仍需研究。结合专家决策可提升网络安全效率。

Abstract: As cyber threats become more sophisticated, rapid and accurate vulnerability
detection is essential for maintaining secure systems. This study explores the
use of Large Language Models (LLMs) in software vulnerability assessment by
simulating the identification of Python code with known Common Weakness
Enumerations (CWEs), comparing zero-shot, few-shot cross-domain, and few-shot
in-domain prompting strategies. Our results indicate that while zero-shot
prompting performs poorly, few-shot prompting significantly enhances
classification performance, particularly when integrated with confidence-based
routing strategies that improve efficiency by directing human experts to cases
where model uncertainty is high, optimizing the balance between automation and
expert oversight. We find that LLMs can effectively generalize across
vulnerability categories with minimal examples, suggesting their potential as
scalable, adaptable cybersecurity tools in simulated environments. However,
challenges such as model reliability, interpretability, and adversarial
robustness remain critical areas for future research. By integrating AI-driven
approaches with expert-in-the-loop (EITL) decision-making, this work highlights
a pathway toward more efficient and responsive cybersecurity workflows. Our
findings provide a foundation for deploying AI-assisted vulnerability detection
systems in both real and simulated environments that enhance operational
resilience while reducing the burden on human analysts.

</details>


### [99] [D-LiFT: Improving LLM-based Decompiler Backend via Code Quality-driven Fine-tuning](https://arxiv.org/abs/2506.10125)
*Muqi Zou,Hongyu Cai,Hongwei Wu,Zion Leonahenahe Basque,Arslan Khan,Berkay Celik,Dave,Tian,Antonio Bianchi,Ruoyu,Wang,Dongyan Xu*

Main category: cs.CR

TL;DR: D-LiFT利用强化学习和大语言模型改进反编译器输出，通过D-SCORE系统确保准确性并提升可读性，显著提升反编译代码质量。


<details>
  <summary>Details</summary>
Motivation: 现有反编译器输出常存在语法和语义错误，可读性差；虽然大语言模型（LLMs）被尝试用于改进代码，但其存在引入新错误和验证不可靠的问题。

Method: 提出D-LiFT框架，基于强化学习优化LLMs，并设计D-SCORE系统综合评估代码准确性和可读性，仅对通过准确性检查的代码奖励可读性分数。

Result: 实验显示，D-LiFT相比未优化的LLMs，提升了55.3%的反编译函数质量，显著改进coreutils和util-linux项目的代码。

Conclusion: D-LiFT通过结合强化学习和多维度评估，显著提升了反编译代码的准确性和可读性，为安全任务提供了更可靠的工具。

Abstract: Decompilers, which reconstruct human-readable source code from binary
executables, are vital to many security tasks. Yet, despite recent advances,
their output often suffers from syntactic and semantic errors and remains
difficult to read. Recently, with the advent of large language models (LLMs),
researchers began to explore the potential of LLMs to refine decompiler output.
Nevertheless, our study of these approaches reveals significant limitations,
such as introducing new errors and relying on unreliable accuracy validation.
In this paper, we present D-LiFT, an automated decompiler backend that
harnesses and further trains LLMs to improve the quality of decompiled code via
reinforcement learning (RL). Unlike prior work that overlooks preserving
accuracy, D-LiFT adheres to a key principle for enhancing the quality of
decompiled code: \textit{preserving accuracy while improving readability}.
Central to D-LiFT, we propose D-SCORE, an integrated quality assessment system
to score the decompiled code from multiple aspects. In line with our principle,
D-SCORE assigns low scores to any inaccurate output and only awards higher
scores for readability to code that passes the accuracy check. Specifically,
D-SCORE first verifies the syntactic and semantic correctness via the compiler
and symbolic execution; only if a candidate is deemed accurate, it then
evaluates readability using established metrics to compare the LLM output with
the original decompiled code. The score will then be fed back to the LLM for
fine-tuning. Our implementation, based on Ghidra and a range of LLMs,
demonstrates significant improvements for the accurate decompiled code from the
coreutils and util-linux projects. Compared to baseline LLMs without
D-SCORE-driven fine-tuning, D-LiFT produces 55.3% more improved decompiled
functions, as measured by D-SCORE.

</details>


### [100] [ELFuzz: Efficient Input Generation via LLM-driven Synthesis Over Fuzzer Space](https://arxiv.org/abs/2506.10323)
*Chuyang Chen,Brendan Dolan-Gavitt,Zhiqiang Lin*

Main category: cs.CR

TL;DR: ELFuzz 是一种通过大语言模型自动生成定制化模糊测试工具的新方法，显著提高了覆盖率和漏洞检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统的手动构建模糊测试规范耗时耗力，ELFuzz 旨在通过自动化合成减少人力投入。

Method: ELFuzz 利用 LLM 驱动的合成技术，从最小种子模糊器开始，通过覆盖引导的进化生成高效模糊测试工具。

Result: 在评估中，ELFuzz 覆盖率提升 434.8%，检测漏洞能力提升 174.0%，并在实际测试中发现多个 0-day 漏洞。

Conclusion: ELFuzz 展示了自动化、高效和可扩展的模糊测试潜力，特别适用于大规模系统。

Abstract: Generation-based fuzzing produces appropriate testing cases according to
specifications of input grammars and semantic constraints to test systems and
software. However, these specifications require significant manual efforts to
construct. This paper proposes a new approach, ELFuzz (Evolution Through Large
Language Models for Fuzzing), that automatically synthesizes generation-based
fuzzers tailored to a system under test (SUT) via LLM-driven synthesis over
fuzzer space. At a high level, it starts with minimal seed fuzzers and propels
the synthesis by fully automated LLM-driven evolution with coverage guidance.
Compared to previous approaches, ELFuzz can 1) seamlessly scale to SUTs of
real-world sizes -- up to 1,791,104 lines of code in our evaluation -- and 2)
synthesize efficient fuzzers that catch interesting grammatical structures and
semantic constraints in a human-understandable way. Our evaluation compared
ELFuzz with specifications manually written by domain experts and synthesized
by state-of-the-art approaches. It shows that ELFuzz achieves up to 434.8% more
coverage and triggers up to 174.0% more artificially injected bugs. We also
used ELFuzz to conduct a real-world fuzzing campaign on the newest version of
cvc5 for 14 days, and encouragingly, it found five 0-day bugs (three are
exploitable). Moreover, we conducted an ablation study, which shows that the
fuzzer space model, the key component of ELFuzz, contributes the most (up to
62.5%) to the effectiveness of ELFuzz. Further analysis of the fuzzers
synthesized by ELFuzz confirms that they catch interesting grammatical
structures and semantic constraints in a human-understandable way. The results
present the promising potential of ELFuzz for more automated, efficient, and
extensible input generation for fuzzing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [101] [LaMAGIC2: Advanced Circuit Formulations for Language Model-Based Analog Topology Generation](https://arxiv.org/abs/2506.10235)
*Chen-Chia Chang,Wan-Hsuan Lin,Yikang Shen,Yiran Chen,Xin Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为LaMAGIC2的新框架，通过改进电路描述的效率和数值精度，显著提升了模拟拓扑生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现代应用对模拟拓扑设计的定制化需求高，而现有方法的电路描述效率低且对数值输入不敏感。

Method: 提出SFCI（简洁浮点输入规范表示法），通过标识符优化组件类型识别，降低复杂度并提升数值精度。

Result: 实验表明，LaMAGIC2在严苛容差下成功率高34%，均方误差降低10倍，且在更大电路中适应性更好。

Conclusion: LaMAGIC2为模拟拓扑生成提供了高效、精确且适应性强的解决方案。

Abstract: Automation of analog topology design is crucial due to customized
requirements of modern applications with heavily manual engineering efforts.
The state-of-the-art work applies a sequence-to-sequence approach and
supervised finetuning on language models to generate topologies given user
specifications. However, its circuit formulation is inefficient due to O(|V |2)
token length and suffers from low precision sensitivity to numeric inputs. In
this work, we introduce LaMAGIC2, a succinct float-input canonical formulation
with identifier (SFCI) for language model-based analog topology generation.
SFCI addresses these challenges by improving component-type recognition through
identifier-based representations, reducing token length complexity to O(|V |),
and enhancing numeric precision sensitivity for better performance under tight
tolerances. Our experiments demonstrate that LaMAGIC2 achieves 34% higher
success rates under a tight tolerance of 0.01 and 10X lower MSEs compared to a
prior method. LaMAGIC2 also exhibits better transferability for circuits with
more vertices with up to 58.5% improvement. These advancements establish
LaMAGIC2 as a robust framework for analog topology generation.

</details>


### [102] [Leveraging Pre-Trained Models for Multimodal Class-Incremental Learning under Adaptive Fusion](https://arxiv.org/abs/2506.09999)
*Yukun Chen,Zihuan Qiu,Fanman Meng,Hongliang Li,Linfeng Xu,Qingbo Wu*

Main category: cs.LG

TL;DR: 提出了基于多模态预训练模型的MCIL方法，解决传统MCIL仅关注视觉和文本的局限性，整合视听文本模态，通过特征提取、动态融合和对比训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统MCIL方法仅针对视觉和文本，本研究探索视听文本多模态的MCIL，解决信息互补和灾难性遗忘问题。

Method: 1. 基于MoE的多模态增量特征提取器(MIFE)；2. 自适应视听融合模块(AAVFM)及文本多样性增强；3. 多模态对比训练损失；4. 新评估指标。

Result: 在三个多模态数据集上的实验验证了方法的有效性。

Conclusion: 所提方法在多模态增量学习中表现出色，解决了跨模态对齐和信息融合问题。

Abstract: Unlike traditional Multimodal Class-Incremental Learning (MCIL) methods that
focus only on vision and text, this paper explores MCIL across vision, audio
and text modalities, addressing challenges in integrating complementary
information and mitigating catastrophic forgetting. To tackle these issues, we
propose an MCIL method based on multimodal pre-trained models. Firstly, a
Multimodal Incremental Feature Extractor (MIFE) based on Mixture-of-Experts
(MoE) structure is introduced to achieve effective incremental fine-tuning for
AudioCLIP. Secondly, to enhance feature discriminability and generalization, we
propose an Adaptive Audio-Visual Fusion Module (AAVFM) that includes a masking
threshold mechanism and a dynamic feature fusion mechanism, along with a
strategy to enhance text diversity. Thirdly, a novel multimodal
class-incremental contrastive training loss is proposed to optimize cross-modal
alignment in MCIL. Finally, two MCIL-specific evaluation metrics are introduced
for comprehensive assessment. Extensive experiments on three multimodal
datasets validate the effectiveness of our method.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [103] [Quantum resources in resource management systems](https://arxiv.org/abs/2506.10052)
*Iskandar Sitdikov,M. Emre Sahin,Utz Bacher,Aleksander Wennersteen,Andrew Damin,Mark Birmingham,Philippa Rubin,Stefano Mensa,Matthieu Moreau,Aurelien Nober,Hitomi Takahashi,Munetaka Ohtani*

Main category: quant-ph

TL;DR: 该论文探讨了将量子计算资源集成到高性能计算（HPC）环境中的方法，提出了一种基于Slurm的插件设计，以实现对量子设备的统一调度和管理。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在高性能计算环境中的应用潜力巨大，但其广泛采用需要与现有HPC基础设施的无缝集成。

Method: 通过设计一套Slurm插件，将本地和云量子计算资源集成到HPC中心，并详细介绍了接口设计、实现及异构集群操作。

Result: 提出了一种有效的量子资源控制架构，支持混合量子-经典应用的调度和管理。

Conclusion: 该研究为量子计算资源在HPC环境中的集成提供了实用解决方案，并具有广泛的应用前景。

Abstract: Quantum computers are beginning to operate in high-performance computing
(HPC) environments. Quantum can complement classical resources for specific
workloads, but their adoption depends on integration into existing HPC
infrastructure. Treating quantum devices as first-class resources allows for
unified scheduling, improved usability, and support for hybrid
quantum-classical applications. This paper presents the design architecture and
reference implementation for quantum resources control using existing workload
management systems. We introduce a suite of plugins for Slurm that enable
integration of on-prem and cloud quantum computing resources into existing
high-performance computing centers. The paper details the interface design,
plugin concept and implementation, operational aspects for heterogeneous
compute clusters, as well as considerations for other resource management
systems.

</details>


### [104] [Synchronization for Fault-Tolerant Quantum Computers](https://arxiv.org/abs/2506.10258)
*Satvik Maurya,Swamit Tannu*

Main category: quant-ph

TL;DR: 论文提出了三种同步策略（被动、主动和混合）来解决表面码逻辑量子比特的同步问题，并显著降低了逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 为了解决逻辑量子比特在表面码中因同步问题导致的错误累积风险，提高量子计算的可靠性和效率。

Method: 定义了三种同步策略：被动策略（基线）、主动策略（逐步减慢）和混合策略（减少同步间隙并执行额外纠错轮次）。

Result: 主动策略将逻辑错误率降低2.4倍，混合策略降低3.4倍，同时解码延迟速度提升2.2倍。

Conclusion: 提出的同步策略显著改善了表面码逻辑量子比特的同步问题，降低了错误率，同时提升了系统性能。

Abstract: Quantum Error Correction (QEC) codes store information reliably in logical
qubits by encoding them in a larger number of less reliable qubits. The surface
code, known for its high resilience to physical errors, is a leading candidate
for fault-tolerant quantum computing (FTQC). Logical qubits encoded with the
surface code can be in different phases of their syndrome generation cycle,
thereby introducing desynchronization in the system. This can occur due to the
production of non-Clifford states, dropouts due to fabrication defects, and the
use of other QEC codes with the surface code to reduce resource requirements.
Logical operations require the syndrome generation cycles of the logical qubits
involved to be synchronized. This requires the leading qubit to pause or slow
down its cycle, allowing more errors to accumulate before the next cycle,
thereby increasing the risk of uncorrectable errors.
  To synchronize the syndrome generation cycles of logical qubits, we define
three policies - Passive, Active, and Hybrid. The Passive policy is the
baseline, and the simplest, wherein the leading logical qubits idle until they
are synchronized with the remaining logical qubits. On the other hand, the
Active policy aims to slow the leading logical qubits down gradually, by
inserting short idle periods before multiple code cycles. This approach reduces
the logical error rate (LER) by up to 2.4x compared to the Passive policy. The
Hybrid policy further reduces the LER by up to 3.4x by reducing the
synchronization slack and running a few additional rounds of error correction.
Furthermore, the reduction in the logical error rate with the proposed
synchronization policies enables a speedup in decoding latency of up to 2.2x
with a circuit-level noise model.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [105] [FedMLAC: Mutual Learning Driven Heterogeneous Federated Audio Classification](https://arxiv.org/abs/2506.10207)
*Jun Bai,Rajib Rana,Di Wu,Youyang Qu,Xiaohui Tao,Ji Zhang*

Main category: cs.SD

TL;DR: FedMLAC是一个统一的联邦音频分类框架，通过双向知识蒸馏和层间剪枝聚合解决了数据异质性、模型异质性和数据中毒问题。


<details>
  <summary>Details</summary>
Motivation: 联邦音频分类面临数据异质性、模型异质性和数据中毒三大挑战，现有方法缺乏统一解决方案。

Method: 提出FedMLAC框架，采用双模型架构（个性化本地模型和全局共享Plug-in模型）和层间剪枝聚合策略。

Result: 在多个音频分类基准测试中，FedMLAC在分类准确性和抗噪性上优于现有方法。

Conclusion: FedMLAC有效解决了联邦音频分类的核心挑战，兼具泛化性和个性化能力。

Abstract: Federated Learning (FL) provides a privacy-preserving paradigm for training
audio classification (AC) models across distributed clients without sharing raw
data. However, Federated Audio Classification (FedAC) faces three critical
challenges that substantially hinder performance: data heterogeneity, model
heterogeneity, and data poisoning. While prior works have attempted to address
these issues, they are typically treated independently, lacking a unified and
robust solution suited to real-world federated audio scenarios. To bridge this
gap, we propose FedMLAC, a unified mutual learning framework designed to
simultaneously tackle these challenges in FedAC. Specifically, FedMLAC
introduces a dual-model architecture on each client, comprising a personalized
local AC model and a lightweight, globally shared Plug-in model. Through
bidirectional knowledge distillation, the Plug-in model enables global
knowledge transfer while adapting to client-specific data distributions, thus
supporting both generalization and personalization. To further enhance
robustness against corrupted audio data, we develop a Layer-wise Pruning
Aggregation (LPA) strategy that filters unreliable Plug-in model updates based
on parameter deviations during server-side aggregation. Extensive experiments
on four diverse audio classification benchmarks, spanning both speech and
non-speech tasks, demonstrate that FedMLAC consistently outperforms existing
state-of-the-art methods in terms of classification accuracy and robustness to
noisy data.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [106] [LogiPlan: A Structured Benchmark for Logical Planning and Relational Reasoning in LLMs](https://arxiv.org/abs/2506.10527)
*Yanan Cai,Ahmed Salem,Besmira Nushi,Mark Russinovich*

Main category: cs.AI

TL;DR: LogiPlan是一个用于评估大语言模型在逻辑规划和关系推理方面能力的新基准，包含三个任务：计划生成、一致性检测和比较问题。


<details>
  <summary>Details</summary>
Motivation: 逻辑关系推理对于依赖大语言模型生成和查询关系结构的应用（如网络基础设施、知识库或业务流程模式）至关重要。

Method: LogiPlan通过控制对象数量、关系和关系链的最小深度动态调整任务复杂度，评估模型在不同难度下的表现。

Result: 评估显示，尽管推理增强模型在简单任务上表现良好，但在需要深度逻辑规划的复杂配置中表现不佳。

Conclusion: LogiPlan揭示了模型规模和架构与性能之间的显著关联，为未来研究提供了重要参考。

Abstract: We introduce LogiPlan, a novel benchmark designed to evaluate the
capabilities of large language models (LLMs) in logical planning and reasoning
over complex relational structures. Logical relational reasoning is important
for applications that may rely on LLMs to generate and query structured graphs
of relations such as network infrastructure, knowledge bases, or business
process schema. Our framework allows for dynamic variation of task complexity
by controlling the number of objects, relations, and the minimum depth of
relational chains, providing a fine-grained assessment of model performance
across difficulty levels. LogiPlan encompasses three complementary tasks: (1)
Plan Generation, where models must construct valid directed relational graphs
meeting specified structural constraints; (2) Consistency Detection, testing
models' ability to identify inconsistencies in relational structures; and (3)
Comparison Question, evaluating models' capacity to determine the validity of
queried relationships within a given graph. Additionally, we assess models'
self-correction capabilities by prompting them to verify and refine their
initial solutions. We evaluate state-of-the-art models including DeepSeek R1,
Gemini 2.0 Pro, Gemini 2 Flash Thinking, GPT-4.5, GPT-4o, Llama 3.1 405B,
O3-mini, O1, and Claude 3.7 Sonnet across these tasks, revealing significant
performance gaps that correlate with model scale and architecture. Our analysis
demonstrates that while recent reasoning-enhanced models show promising results
on simpler instances, they struggle with more complex configurations requiring
deeper logical planning.

</details>


### [107] [Multi-dimensional Autoscaling of Processing Services: A Comparison of Agent-based Methods](https://arxiv.org/abs/2506.10420)
*Boris Sedlak,Alireza Furutanpey,Zihang Wang,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.AI

TL;DR: 论文提出了一种基于代理的边缘计算弹性扩展框架，通过动态调整硬件资源和服务配置，在受限环境中最大化需求满足度。


<details>
  <summary>Details</summary>
Motivation: 边缘计算因严格的资源限制需要更灵活的扩展行为，传统扩展方法无法满足需求。

Method: 引入基于代理的扩展框架，比较了四种代理类型（Active Inference、Deep Q Network、Analysis of Structural Knowledge、Deep Active Inference），并使用YOLOv8和OpenCV进行实验验证。

Result: 所有代理均能满足性能指标，但收敛模式各异；Deep Q Network需预训练，结构分析快速收敛，深度Active Inference兼具理论和实践优势。

Conclusion: 研究证明了基于代理的多维扩展在边缘环境中的可行性，鼓励未来进一步探索。

Abstract: Edge computing breaks with traditional autoscaling due to strict resource
constraints, thus, motivating more flexible scaling behaviors using multiple
elasticity dimensions. This work introduces an agent-based autoscaling
framework that dynamically adjusts both hardware resources and internal service
configurations to maximize requirements fulfillment in constrained
environments. We compare four types of scaling agents: Active Inference, Deep Q
Network, Analysis of Structural Knowledge, and Deep Active Inference, using two
real-world processing services running in parallel: YOLOv8 for visual
recognition and OpenCV for QR code detection. Results show all agents achieve
acceptable SLO performance with varying convergence patterns. While the Deep Q
Network benefits from pre-training, the structural analysis converges quickly,
and the deep active inference agent combines theoretical foundations with
practical scalability advantages. Our findings provide evidence for the
viability of multi-dimensional agent-based autoscaling for edge environments
and encourage future work in this research direction.

</details>


### [108] [System ASPMT2SMT:Computing ASPMT Theories by SMT Solvers](https://arxiv.org/abs/2506.10708)
*Michael Bartholomew,Joohyung Lee*

Main category: cs.AI

TL;DR: ASPMT结合了答案集编程和满足模理论，通过编译器aspsmt2smt将其转化为SMT实例，利用gringo和z3处理连续变化推理。


<details>
  <summary>Details</summary>
Motivation: 结合答案集编程与满足模理论，解决连续变化的推理问题。

Method: 开发编译器aspsmt2smt，利用gringo部分接地程序，z3处理剩余变量。

Result: 系统能有效处理实数计算，支持连续变化推理。

Conclusion: ASPMT通过编译器实现高效推理，扩展了答案集编程的应用范围。

Abstract: Answer Set Programming Modulo Theories (ASPMT) is an approach to combining
answer set programming and satisfiability modulo theories based on the
functional stable model semantics. It is shown that the tight fragment of ASPMT
programs can be turned into SMT instances, thereby allowing SMT solvers to
compute stable models of ASPMT programs. In this paper we present a compiler
called {\sc aspsmt2smt}, which implements this translation. The system uses ASP
grounder {\sc gringo} and SMT solver {\sc z3}. {\sc gringo} partially grounds
input programs while leaving some variables to be processed by {\sc z3}. We
demonstrate that the system can effectively handle real number computations for
reasoning about continuous changes.

</details>


### [109] [Primender Sequence: A Novel Mathematical Construct for Testing Symbolic Inference and AI Reasoning](https://arxiv.org/abs/2506.10585)
*Mohd Anwar Jamal Faiz*

Main category: cs.AI

TL;DR: 论文介绍了Primender序列，一种结合素数性和模数条件的整数序列，旨在评估大型语言模型（LLMs）的符号推理能力。


<details>
  <summary>Details</summary>
Motivation: 需要可解释的、基于规则的测试平台来评估LLMs推断隐藏规则、验证数学假设和扩规模符号逻辑的能力。

Method: 设计Primender序列作为测试基准，通过结构化提示和评估框架测试LLMs，并比较其表现。

Result: 提出了新颖的数学构造和可重复的方法论，用于评估LLMs在符号推理、假设测试和模式推广中的表现。

Conclusion: Primender序列和研究方法为数论、人工智能和软件工程的交叉领域提供了新的工具和见解。

Abstract: This paper introduces the Primender sequence, a novel integer sequence
defined by a hybrid rule that combines classical primality with modular
digit-based conditions. Specifically, a number n is included in the sequence if
it is prime or ends with a prime number of unit digit or any length. In other
words, numbers which are primes or have at least one prime suffix. The
resulting sequence exhibits a deterministic yet non-trivial structure, blending
number-theoretic properties with symbolic patterning. We propose the Primender
sequence as a benchmark for evaluating the symbolic reasoning capabilities of
Large Language Models (LLMs). The study is motivated by the need for
interpretable, rule-based testbeds that can assess an LLM's ability to infer
hidden rules, validate mathematical hypotheses, and generalize symbolic logic
at scale. A key hypothesis explored is: Whenever a number in the Primender
sequence is exactly one more than the largest prime less than or equal to it,
the difference between it and the previous number in the sequence is also 1. We
design a structured prompt and evaluation framework to test this hypothesis
across multiple state-of-the-art LLMs, including ChatGPT, Copilot, DeepSeek,
Gemini, Grok, and LLaMA. The models are tasked with identifying the underlying
rule, validating the hypothesis, and generating the next 100,000 terms of the
sequence. Comparative metrics such as rule inference accuracy, hypothesis
evaluation, sequence validity, and symbolic explanation quality are used to
assess model performance. This work contributes a novel mathematical construct
and a reproducible methodology for benchmarking LLMs in symbolic reasoning,
hypothesis testing, and scalable pattern generalization - bridging the domains
of number theory, artificial intelligence, and software engineering.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [110] [Collective Bargaining in the Information Economy Can Address AI-Driven Power Concentration](https://arxiv.org/abs/2506.10272)
*Nicholas Vincent,Matthew Prewitt,Hanlin Li*

Main category: cs.CY

TL;DR: 该立场文件主张需重组AI系统信息市场，强调信息生产者需集体协商以获取合理回报，避免市场失灵和生态崩溃。


<details>
  <summary>Details</summary>
Motivation: AI发展加剧了信息市场的不平等，导致资本集中和生态风险，需通过集体协商实现可持续AI未来。

Method: 提出技术机制（如联合数据管理工具和数据价值评估）和政策干预（支持数据中介组织）以促进集体协商。

Result: 若成功实施集体协商，可实现市场摩擦减少、激励机制对齐，推动社会友好型AI发展。

Conclusion: 通过集体协商和技术及政策支持，可重构信息市场，实现可持续的AI生态。

Abstract: This position paper argues that there is an urgent need to restructure
markets for the information that goes into AI systems. Specifically, producers
of information goods (such as journalists, researchers, and creative
professionals) need to be able to collectively bargain with AI product builders
in order to receive reasonable terms and a sustainable return on the
informational value they contribute. We argue that without increased market
coordination or collective bargaining on the side of these primary information
producers, AI will exacerbate a large-scale "information market failure" that
will lead not only to undesirable concentration of capital, but also to a
potential "ecological collapse" in the informational commons. On the other
hand, collective bargaining in the information economy can create market
frictions and aligned incentives necessary for a pro-social, sustainable AI
future. We provide concrete actions that can be taken to support a
coalition-based approach to achieve this goal. For example, researchers and
developers can establish technical mechanisms such as federated data management
tools and explainable data value estimations, to inform and facilitate
collective bargaining in the information economy. Additionally, regulatory and
policy interventions may be introduced to support trusted data intermediary
organizations representing guilds or syndicates of information producers.

</details>


### [111] [The Urban Model Platform: A Public Backbone for Modeling and Simulation in Urban Digital Twins](https://arxiv.org/abs/2506.10964)
*Rico H Herzog,Till Degkwitz,Trivik Verma*

Main category: cs.CY

TL;DR: 该论文探讨了如何通过开放式城市模型平台解决城市数字孪生中模型集成的复杂性问题，强调多方协作和去中心化方法。


<details>
  <summary>Details</summary>
Motivation: 城市数字孪生旨在整合城市数字资源以实现更可持续和综合的规划，但现有方法多集中于单一化解决方案，无法满足多元化和开放互操作的需求。

Method: 采用参与式设计与多方协作方法，开发了一种开放式城市模型平台，支持去中心化模型集成和多模型通信。

Result: 该平台可作为城市数字孪生的公共技术框架，支持协作式城市过程表征，并基于开放标准实现模型间的互通。

Conclusion: 开放式城市模型平台为城市数字孪生提供了一种多元化和去中心化的解决方案，推动了更开放的协作式规划实践。

Abstract: Urban digital twins are increasingly perceived as a way to pool the growing
digital resources of cities for the purpose of a more sustainable and
integrated urban planning. Models and simulations are central to this
undertaking: They enable "what if?" scenarios, create insights and describe
relationships between the vast data that is being collected. However, the
process of integrating and subsequently using models in urban digital twins is
an inherently complex undertaking. It raises questions about how to represent
urban complexity, how to deal with uncertain assUrban Model Platformtions and
modeling paradigms, and how to capture underlying power relations. Existent
approaches in the domain largely focus on monolithic and centralized solutions
in the tradition of neoliberal city-making, oftentimes prohibiting pluralistic
and open interoperable models. Using a participatory design for participatory
systems approach together with the City of Hamburg, Germany, we find that an
open Urban Model Platform can function both as a public technological backbone
for modeling and simulation in urban digital twins and as a socio-technical
framework for a collaborative and pluralistic representation of urban
processes. Such a platform builds on open standards, allows for a decentralized
integration of models, enables communication between models and supports a
multi-model approach to representing urban systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [112] [The Iris File Extension](https://arxiv.org/abs/2506.10009)
*Ryan Erik Landvater,Michael David Olp,Mustafa Yousif,Ulysses Balis*

Main category: eess.IV

TL;DR: 提出了一种名为Iris的新型二进制幻灯片格式，旨在解决数字病理学中实时传输和显示的效率问题，支持现代压缩和动态结构等特点。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一种高效的中间数字幻灯片格式，以满足快速传输和显示的需求，尤其是在跨机构病例传输中。

Method: 设计了Iris文件扩展，提供现代压缩、动态结构、文件验证和恢复功能，并提供了多语言实现的开源代码。

Result: Iris格式能够抽象文件结构以实现即时瓦片访问，并支持幻灯片注释，适用于现有WSI解决方案的集成。

Conclusion: 通过开源Iris文件扩展，为数字病理学社区提供了一种高效、灵活的解决方案。

Abstract: A modern digital pathology vendor-agnostic binary slide format specifically
targeting the unmet need of efficient real-time transfer and display has not
yet been established. Growing adoption of digital pathology only intensifies
the need for an intermediary digital slide format with an emphasis on
performance for use between slide servers and image management software or for
inter-institutional transmission of cases. Although the DICOM standard is a
well-established format widely used for long-term storage of both images and
critically associated metadata, its inherent limitations on maximum image
dimensions can impact retrieval speed, particularly when accessing whole slide
images using a pyramidal structure of slide viewer applications. Here, we
introduce the Iris file extension, a binary file container specification
explicitly designed for whole slide image systems that can abstract the file
structure outline into memory for immediate tile access. The Iris file
extension adds modern compression support, a dynamic structure with optional
file features, computationally trivial deep file validation and corruption
recovery capabilities, and slide annotation support. In addition to the file
specification document, we provide source code to allow for (de)serialization
and validation of a binary stream against the standard and corresponding binary
builds with C++, Python, and JavaScript language bindings. We further provide
full encoder and decoder implementation source code, as well as binary builds
(as part of the separate Iris Codec Community module) with language bindings
for C++ and Python to allow for easy integration with existing WSI solutions.
We provide the Iris File Extension specification openly to the community in the
form of a Creative Commons Attribution-No Derivative 4.0 international license.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [113] [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
*Aakriti Kumar,Nalin Poungpeth,Diyi Yang,Erina Farrell,Bruce Lambert,Matthew Groh*

Main category: cs.CL

TL;DR: 论文研究了专家、众包工作者和大语言模型（LLM）在四种心理学框架下对共情交流的标注可靠性，发现LLM的表现接近专家水平，优于众包工作者。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在共情交流中的标注可靠性，验证其在情感敏感应用中的潜力。

Method: 通过比较专家、众包工作者和LLM在四种心理学框架下对200个真实对话的标注数据（共9,144条标注），评估其一致性和可靠性。

Result: 专家间一致性强但受框架子组分的清晰度和复杂性影响；LLM表现接近专家水平，优于众包工作者。

Conclusion: LLM在特定任务中表现可靠，适合作为情感敏感应用的工具，需以专家标准为基准进行验证。

Abstract: Large language models (LLMs) excel at generating empathic responses in
text-based conversations. But, how reliably do they judge the nuances of
empathic communication? We investigate this question by comparing how experts,
crowdworkers, and LLMs annotate empathic communication across four evaluative
frameworks drawn from psychology, natural language processing, and
communications applied to 200 real-world conversations where one speaker shares
a personal problem and the other offers support. Drawing on 3,150 expert
annotations, 2,844 crowd annotations, and 3,150 LLM annotations, we assess
inter-rater reliability between these three annotator groups. We find that
expert agreement is high but varies across the frameworks' sub-components
depending on their clarity, complexity, and subjectivity. We show that expert
agreement offers a more informative benchmark for contextualizing LLM
performance than standard classification metrics. Across all four frameworks,
LLMs consistently approach this expert level benchmark and exceed the
reliability of crowdworkers. These results demonstrate how LLMs, when validated
on specific tasks with appropriate benchmarks, can support transparency and
oversight in emotionally sensitive applications including their use as
conversational companions.

</details>


### [114] [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
*Yixin Ou,Yujie Luo,Jingsheng Zheng,Lanning Wei,Shuofei Qiao,Jintian Zhang,Da Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: AutoMind是一种自适应、知识丰富的LLM代理框架，通过专家知识库、树搜索算法和动态代码生成策略，显著提升了数据科学任务的自动化效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的数据科学代理框架依赖于僵化的工作流程和代码策略，难以应对复杂任务，AutoMind旨在克服这些局限。

Method: AutoMind采用三大创新：知识库支持、树搜索算法和自适应代码生成策略。

Result: 在自动化数据科学基准测试中，AutoMind表现优于现有先进方法，展现出高效性和稳健性。

Conclusion: AutoMind为完全自动化的数据科学迈出了重要一步，展示了其在复杂任务中的潜力。

Abstract: Large Language Model (LLM) agents have shown great potential in addressing
real-world data science problems. LLM-driven data science agents promise to
automate the entire machine learning pipeline, yet their real-world
effectiveness remains limited. Existing frameworks depend on rigid, pre-defined
workflows and inflexible coding strategies; consequently, they excel only on
relatively simple, classical problems and fail to capture the empirical
expertise that human practitioners bring to complex, innovative tasks. In this
work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework
that overcomes these deficiencies through three key advances: (1) a curated
expert knowledge base that grounds the agent in domain expert knowledge, (2) an
agentic knowledgeable tree search algorithm that strategically explores
possible solutions, and (3) a self-adaptive coding strategy that dynamically
tailors code generation to task complexity. Evaluations on two automated data
science benchmarks demonstrate that AutoMind delivers superior performance
versus state-of-the-art baselines. Additional analyses confirm favorable
effectiveness, efficiency, and qualitative solution quality, highlighting
AutoMind as an efficient and robust step toward fully automated data science.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [115] [Receiving RISs: Enabling Channel Estimation and Autonomous Configuration](https://arxiv.org/abs/2506.10662)
*George C. Alexandropoulos,Konstantinos D. Katsanos,Evangelos Vlachos*

Main category: eess.SP

TL;DR: 本文提出了一种用于半被动可重构智能表面（RIS）的硬件架构，并通过一种新颖的信道估计协议优化RIS反射系数，以提升MIMO通信系统性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过RIS硬件架构和信道估计协议解决MIMO通信系统中信道估计和反射系数优化的问题。

Method: 采用基于ADMM的信道估计算法，利用RIS随机空间吸收采样和MIMO信道的稀疏性及低秩特性。

Result: 数值实验表明，该方案在多种系统及RIS硬件配置下优于基准方案，并有效优化RIS单元反射系数。

Conclusion: 所提方案在FR3及以上频段的通信系统中具有显著优势，能动态优化RIS反射系数。

Abstract: This chapter focuses on a hardware architecture for semi-passive
Reconfigurable Intelligent Surfaces (RISs) and investigates its consideration
for boosting the performance of Multiple-Input Multiple-Output (MIMO)
communication systems. The architecture incorporates a single or multiple
radio-frequency chains to receive pilot signals via tunable absorption phase
profiles realized by the metasurface front end, as well as a controller
encompassing a baseband processing unit to carry out channel estimation, and
consequently, the optimization of the RIS reflection coefficients. A novel
channel estimation protocol, according to which the RIS receives non-orthogonal
training pilot sequences from two multi-antenna terminals via tunable
absorption phase profiles, and then, estimates the respective channels via its
signal processing unit, is presented. The channel estimates are particularly
used by the RIS controller to design the capacity-achieving reflection phase
configuration of the metasurface front end. The proposed channel estimation
algorithm, which is based on the Alternating Direction Method of Multipliers
(ADMM), profits from the RIS random spatial absorption sampling to capture the
entire signal space, and exploits the beamspace sparsity and low-rank
properties of extremely large MIMO channels, which is particularly relevant for
communication systems at the FR3 band and above. Our extensive numerical
investigations showcase the superiority of the proposed channel estimation
technique over benchmark schemes for various system and RIS hardware
configuration parameters, as well as the effectiveness of using channel
estimates at the RIS side to dynamically optimize the possibly phase-quantized
reflection coefficients of its unit elements.

</details>


### [116] [Ground Reaction Force Estimation via Time-aware Knowledge Distillation](https://arxiv.org/abs/2506.10265)
*Eun Som Jeon,Sinjini Mitra,Jisoo Lee,Omik M. Save,Ankita Shukla,Hyunglae Lee,Pavan Turaga*

Main category: eess.SP

TL;DR: 提出了一种基于时间感知知识蒸馏的框架，用于从便携式鞋垫传感器数据中估计地面反作用力（GRF），解决了传统传感器噪声大、精度低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GRF测量方法（如跑步机）昂贵且不便携，而低成本鞋垫传感器易受干扰且不准确。需提升便携传感器的GRF估计精度。

Method: 采用时间感知知识蒸馏，利用小批量数据的相似性和时序特征，捕捉目标与输入数据的互补关系和时序特性。

Result: 实验表明，该框架在GRF估计上优于现有基线，验证了其有效性。

Conclusion: 时间感知知识蒸馏为便携式GRF测量提供了可行的高精度解决方案。

Abstract: Human gait analysis with wearable sensors has been widely used in various
applications, such as daily life healthcare, rehabilitation, physical therapy,
and clinical diagnostics and monitoring. In particular, ground reaction force
(GRF) provides critical information about how the body interacts with the
ground during locomotion. Although instrumented treadmills have been widely
used as the gold standard for measuring GRF during walking, their lack of
portability and high cost make them impractical for many applications. As an
alternative, low-cost, portable, wearable insole sensors have been utilized to
measure GRF; however, these sensors are susceptible to noise and disturbance
and are less accurate than treadmill measurements. To address these challenges,
we propose a Time-aware Knowledge Distillation framework for GRF estimation
from insole sensor data. This framework leverages similarity and temporal
features within a mini-batch during the knowledge distillation process,
effectively capturing the complementary relationships between features and the
sequential properties of the target and input data. The performance of the
lightweight models distilled through this framework was evaluated by comparing
GRF estimations from insole sensor data against measurements from an
instrumented treadmill. Empirical results demonstrated that Time-aware
Knowledge Distillation outperforms current baselines in GRF estimation from
wearable sensor data.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [117] [Learning Chaotic Dynamics with Neuromorphic Network Dynamics](https://arxiv.org/abs/2506.10773)
*Yinhao Xu,Georg A. Gottwald,Zdenka Kuncic*

Main category: cond-mat.dis-nn

TL;DR: 本研究探讨了如何通过神经形态网络学习和建模动态系统，该网络本身是一个动态系统，基于包含忆阻器元件的复杂电路。研究通过模拟和评估在混沌时间序列预测中的表现，发现输入电压最大化忆阻器动态范围时，非线性动态响应最优。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用神经形态网络的物理特性学习和建模复杂动态系统。

Method: 使用基于忆阻器的复杂电路模拟神经形态网络，并在混沌时间序列预测中应用储层计算框架进行评估。

Result: 输入电压最大化忆阻器动态范围时，非线性动态响应最优；网络覆盖增加会抑制不利于学习的响应。

Conclusion: 研究为优化神经形态网络设备的外部控制参数提供了实用见解。

Abstract: This study investigates how dynamical systems may be learned and modelled
with a neuromorphic network which is itself a dynamical system. The
neuromorphic network used in this study is based on a complex electrical
circuit comprised of memristive elements that produce neuro-synaptic nonlinear
responses to input electrical signals. To determine how computation may be
performed using the physics of the underlying system, the neuromorphic network
was simulated and evaluated on autonomous prediction of a multivariate chaotic
time series, implemented with a reservoir computing framework. Through
manipulating only input electrodes and voltages, optimal nonlinear dynamical
responses were found when input voltages maximise the number of memristive
components whose internal dynamics explore the entire dynamical range of the
memristor model. Increasing the network coverage with the input electrodes was
found to suppress other nonlinear responses that are less conducive to
learning. These results provide valuable insights into how a practical
neuromorphic network device can be optimised for learning complex dynamical
systems using only external control parameters.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [118] [A Manually Annotated Image-Caption Dataset for Detecting Children in the Wild](https://arxiv.org/abs/2506.10117)
*Klim Kireev,Ana-Maria Creţu,Raphael Meier,Sarah Adel Bargal,Elissa Redmiles,Carmela Troncoso*

Main category: cs.CV

TL;DR: 本文介绍了新发布的多模态数据集ICCWD，用于检测未成年人内容，填补了现有空白，并展示了其在基准测试中的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有工具缺乏多模态环境下检测未成年人内容的基准数据集，本文旨在填补这一空白。

Method: 发布ICCWD数据集，包含10,000张手动标注的图像-标题对，并测试了三种检测器的性能。

Result: 最佳检测器的真阳性率为75.3%，表明未成年人检测具有挑战性。

Conclusion: ICCWD数据集有望促进更优的未成年人检测方法的设计。

Abstract: Platforms and the law regulate digital content depicting minors (defined as
individuals under 18 years of age) differently from other types of content.
Given the sheer amount of content that needs to be assessed, machine
learning-based automation tools are commonly used to detect content depicting
minors. To our knowledge, no dataset or benchmark currently exists for
detecting these identification methods in a multi-modal environment. To fill
this gap, we release the Image-Caption Children in the Wild Dataset (ICCWD), an
image-caption dataset aimed at benchmarking tools that detect depictions of
minors. Our dataset is richer than previous child image datasets, containing
images of children in a variety of contexts, including fictional depictions and
partially visible bodies. ICCWD contains 10,000 image-caption pairs manually
labeled to indicate the presence or absence of a child in the image. To
demonstrate the possible utility of our dataset, we use it to benchmark three
different detectors, including a commercial age estimation system applied to
images. Our results suggest that child detection is a challenging task, with
the best method achieving a 75.3% true positive rate. We hope the release of
our dataset will aid in the design of better minor detection methods in a wide
range of scenarios.

</details>


### [119] [From Images to Insights: Explainable Biodiversity Monitoring with Plain Language Habitat Explanations](https://arxiv.org/abs/2506.10559)
*Yutong Zhou,Masahiro Ryo*

Main category: cs.CV

TL;DR: 提出了一种端到端的视觉-因果框架，通过图像识别和因果推断方法，生成物种栖息地偏好的可解释因果解释。


<details>
  <summary>Details</summary>
Motivation: 理解物种在特定位置生存的原因对生态研究和生物多样性保护至关重要，但现有方法对非专业人士不友好。

Method: 整合物种识别、全球分布数据检索、伪缺失采样和气候数据提取，利用现代因果推断方法发现环境特征间的因果关系。

Result: 通过对蜜蜂和花的物种进行测试，展示了该框架在生成人类可理解的栖息地描述方面的潜力。

Conclusion: 该框架结合AI技术和生态建模实践，为非专业人士提供了直观的物种栖息地因果解释。

Abstract: Explaining why the species lives at a particular location is important for
understanding ecological systems and conserving biodiversity. However, existing
ecological workflows are fragmented and often inaccessible to non-specialists.
We propose an end-to-end visual-to-causal framework that transforms a species
image into interpretable causal insights about its habitat preference. The
system integrates species recognition, global occurrence retrieval,
pseudo-absence sampling, and climate data extraction. We then discover causal
structures among environmental features and estimate their influence on species
occurrence using modern causal inference methods. Finally, we generate
statistically grounded, human-readable causal explanations from structured
templates and large language models. We demonstrate the framework on a bee and
a flower species and report early results as part of an ongoing project,
showing the potential of the multimodal AI assistant backed up by a recommended
ecological modeling practice for describing species habitat in
human-understandable language.

</details>


### [120] [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
*Sridhar S,Nithin A,Shakeel Rifath,Vasantha Raj*

Main category: cs.CV

TL;DR: 提出了一种结合Stable Diffusion、GPT-2和混合音频管道的60秒电影生成方法，具有高质量视觉效果和叙事连贯性。


<details>
  <summary>Details</summary>
Motivation: 利用生成式AI改进多媒体创作，实现从文本到专业级视频的自动生成。

Method: 采用五场景框架，结合图像合成、叙事结构、线性插值及音视频同步技术。

Result: 在GPU环境中实验证明了出色的视觉质量、叙事连贯性和效率。

Conclusion: 该方法为创意、教育及工业应用提供了高效的文本到视频生成解决方案。

Abstract: Advances in generative artificial intelligence have altered multimedia
creation, allowing for automatic cinematic video synthesis from text inputs.
This work describes a method for creating 60-second cinematic movies
incorporating Stable Diffusion for high-fidelity image synthesis, GPT-2 for
narrative structuring, and a hybrid audio pipeline using gTTS and
YouTube-sourced music. It uses a five-scene framework, which is augmented by
linear frame interpolation, cinematic post-processing (e.g., sharpening), and
audio-video synchronization to provide professional-quality results. It was
created in a GPU-accelerated Google Colab environment using Python 3.11. It has
a dual-mode Gradio interface (Simple and Advanced), which supports resolutions
of up to 1024x768 and frame rates of 15-30 FPS. Optimizations such as CUDA
memory management and error handling ensure reliability. The experiments
demonstrate outstanding visual quality, narrative coherence, and efficiency,
furthering text-to-video synthesis for creative, educational, and industrial
applications.

</details>


### [121] [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/abs/2506.10452)
*Guowei Zhong,Ruohong Huan,Mingzhen Wu,Ronghua Liang,Peng Chen*

Main category: cs.CV

TL;DR: 提出了一种名为CIDer的新型多模态情绪识别框架，通过自蒸馏和因果推断技术解决了模态缺失和OOD数据问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多模态情绪识别中的模态缺失和OOD数据时存在不足，限制了实用性。

Method: 提出了CIDer框架，包含模型特定自蒸馏模块（MSSD）和模型无关因果推断模块（MACI），以增强鲁棒性和OOD泛化能力。

Result: CIDer在RMFM和OOD场景中表现优异，参数更少且训练更快。

Conclusion: CIDer通过创新的模块设计，在性能和效率上均优于现有方法。

Abstract: Recent advancements in Multimodal Emotion Recognition (MER) face challenges
in addressing both modality missing and Out-Of-Distribution (OOD) data
simultaneously. Existing methods often rely on specific models or introduce
excessive parameters, which limits their practicality. To address these issues,
we propose a novel robust MER framework, Causal Inference Distiller (CIDer),
and introduce a new task, Random Modality Feature Missing (RMFM), to generalize
the definition of modality missing. CIDer integrates two key components: a
Model-Specific Self-Distillation (MSSD) module and a Model-Agnostic Causal
Inference (MACI) module. MSSD enhances robustness under the RMFM task through a
weight-sharing self-distillation approach applied across low-level features,
attention maps, and high-level representations. Additionally, a Word-level
Self-aligned Attention Module (WSAM) reduces computational complexity, while a
Multimodal Composite Transformer (MCT) facilitates efficient multimodal fusion.
To tackle OOD challenges, MACI employs a tailored causal graph to mitigate
label and language biases using a Multimodal Causal Module (MCM) and
fine-grained counterfactual texts. Notably, MACI can independently enhance OOD
generalization with minimal additional parameters. Furthermore, we also
introduce the new repartitioned MER OOD datasets. Experimental results
demonstrate that CIDer achieves robust performance in both RMFM and OOD
scenarios, with fewer parameters and faster training compared to
state-of-the-art methods. The implementation of this work is publicly
accessible at https://github.com/gw-zhong/CIDer.

</details>


### [122] [DanceChat: Large Language Model-Guided Music-to-Dance Generation](https://arxiv.org/abs/2506.10574)
*Qing Wang,Xiaohang Yang,Yilan Dong,Naveen Raj Govindaraj,Gregory Slabaugh,Shanxin Yuan*

Main category: cs.CV

TL;DR: DanceChat利用大型语言模型（LLM）作为编舞指导，通过文本动作指令生成多样且与音乐风格匹配的舞蹈动作，解决了音乐与舞蹈之间存在的一对多映射问题。


<details>
  <summary>Details</summary>
Motivation: 音乐与舞蹈间的语义鸿沟及一对多映射关系使得仅靠音乐生成多样化且匹配的舞蹈动作具有挑战性。DanceChat旨在通过LLM提供的显式文本指导弥补这一缺陷。

Method: 1. LLM生成文本舞蹈指导；2. 多模态特征提取与融合模块；3. 基于扩散模型的动作合成与对齐损失。

Result: 在AIST++数据集和人工评估中，DanceChat在质量和多样性上均优于现有方法。

Conclusion: DanceChat通过LLM的显式指导，显著提升了音乐到舞蹈生成的多样性和风格匹配性。

Abstract: Music-to-dance generation aims to synthesize human dance motion conditioned
on musical input. Despite recent progress, significant challenges remain due to
the semantic gap between music and dance motion, as music offers only abstract
cues, such as melody, groove, and emotion, without explicitly specifying the
physical movements. Moreover, a single piece of music can produce multiple
plausible dance interpretations. This one-to-many mapping demands additional
guidance, as music alone provides limited information for generating diverse
dance movements. The challenge is further amplified by the scarcity of paired
music and dance data, which restricts the model\^a\u{A}\'Zs ability to learn
diverse dance patterns. In this paper, we introduce DanceChat, a Large Language
Model (LLM)-guided music-to-dance generation approach. We use an LLM as a
choreographer that provides textual motion instructions, offering explicit,
high-level guidance for dance generation. This approach goes beyond implicit
learning from music alone, enabling the model to generate dance that is both
more diverse and better aligned with musical styles. Our approach consists of
three components: (1) an LLM-based pseudo instruction generation module that
produces textual dance guidance based on music style and structure, (2) a
multi-modal feature extraction and fusion module that integrates music, rhythm,
and textual guidance into a shared representation, and (3) a diffusion-based
motion synthesis module together with a multi-modal alignment loss, which
ensures that the generated dance is aligned with both musical and textual cues.
Extensive experiments on AIST++ and human evaluations show that DanceChat
outperforms state-of-the-art methods both qualitatively and quantitatively.

</details>


### [123] [VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos](https://arxiv.org/abs/2506.10857)
*Jiashuo Yu,Yue Wu,Meng Chu,Zhifei Ren,Zizheng Huang,Pei Chu,Ruijie Zhang,Yinan He,Qirui Li,Songze Li,Zhenxiang Li,Zhongying Tu,Conghui He,Yu Qiao,Yali Wang,Yi Wang,Limin Wang*

Main category: cs.CV

TL;DR: VRBench是一个针对大型模型多步推理能力的长叙事视频基准测试，填补了现有评估中忽视时序推理和程序有效性的空白。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在时序推理和程序有效性方面存在不足，需要更全面的多步推理评估工具。

Method: 开发VRBench基准测试，包含1,010个长视频和9,468个人工标注的问答对，采用多阶段过滤和专家评审确保视频质量，并通过人机协作框架生成多步推理链。

Result: 对12种语言模型和16种视觉语言模型的评估显示，VRBench在多步推理评估方面提供了全面而有价值的分析。

Conclusion: VRBench为多步推理领域提供了新的评估标准，并通过多维度的评分指标推动了该领域的进步。

Abstract: We present VRBench, the first long narrative video benchmark crafted for
evaluating large models' multi-step reasoning capabilities, addressing
limitations in existing evaluations that overlook temporal reasoning and
procedural validity. It comprises 1,010 long videos (with an average duration
of 1.6 hours), along with 9,468 human-labeled multi-step question-answering
pairs and 30,292 reasoning steps with timestamps. These videos are curated via
a multi-stage filtering process including expert inter-rater reviewing to
prioritize plot coherence. We develop a human-AI collaborative framework that
generates coherent reasoning chains, each requiring multiple temporally
grounded steps, spanning seven types (e.g., event attribution, implicit
inference). VRBench designs a multi-phase evaluation pipeline that assesses
models at both the outcome and process levels. Apart from the MCQs for the
final results, we propose a progress-level LLM-guided scoring metric to
evaluate the quality of the reasoning chain from multiple dimensions
comprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on
VRBench, we undertake a thorough analysis and provide valuable insights that
advance the field of multi-step reasoning.

</details>


### [124] [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
*Leigang Qu,Feng Cheng,Ziyan Yang,Qi Zhao,Shanchuan Lin,Yichun Shi,Yicong Li,Wenjie Wang,Tat-Seng Chua,Lu Jiang*

Main category: cs.CV

TL;DR: 该论文研究了如何通过视频学习实现上下文图像编辑，提出了一种基于块因果扩散Transformer的方法，并在多个任务中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务专用管道和专家模型来生成训练数据，该研究探索是否可以直接从视频中学习上下文图像编辑模型。

Method: 提出了一种可扩展的视频标注方法，将其转化为交错多模态序列，并设计了块因果扩散Transformer，通过三个代理任务进行训练。

Result: 模型在上下文图像编辑任务中表现优异，在两个多轮图像编辑基准测试中达到了最先进的水平，同时展现出多概念合成、故事生成和编辑链应用的能力。

Conclusion: 研究表明，直接从视频中学习上下文图像编辑是可行的，且模型在多种应用中表现出了潜力。

Abstract: In-context image editing aims to modify images based on a contextual sequence
comprising text and previously generated images. Existing methods typically
depend on task-specific pipelines and expert models (e.g., segmentation and
inpainting) to curate training data. In this work, we explore whether an
in-context image editing model can be learned directly from videos. We
introduce a scalable approach to annotate videos as interleaved multimodal
sequences. To effectively learn from this data, we design a block-causal
diffusion transformer trained on three proxy tasks: next-image prediction,
current segmentation prediction, and next-segmentation prediction.
Additionally, we propose a novel multi-turn image editing benchmark to advance
research in this area. Extensive experiments demonstrate that our model
exhibits strong in-context image editing capabilities and achieves
state-of-the-art results on two multi-turn image editing benchmarks. Despite
being trained exclusively on videos, our model also shows promising abilities
in multi-concept composition, story generation, and chain-of-editing
applications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [125] [The Gittins Index: A Design Principle for Decision-Making Under Uncertainty](https://arxiv.org/abs/2506.10872)
*Ziv Scully,Alexander Terenin*

Main category: math.OC

TL;DR: 本文教程旨在展示Gittins指数如何实际应用于决策问题，通过示例介绍其用途，包括最优和次优但高效的情况。


<details>
  <summary>Details</summary>
Motivation: 尽管Gittins指数在理论上很重要，但其应用范围有限且定义复杂，本文试图证明其在实际问题中的实用性。

Method: 通过示例驱动的方式介绍Gittins指数，并展示其在贝叶斯优化和队列尾部延迟最小化等实际问题中的应用。

Result: Gittins指数不仅能最优解决某些问题，还能在其他问题上表现出色。

Conclusion: Gittins指数是一个强大的工具，不仅具有理论意义，还能实际应用于多种决策问题。

Abstract: The Gittins index is a tool that optimally solves a variety of
decision-making problems involving uncertainty, including multi-armed bandit
problems, minimizing mean latency in queues, and search problems like the
Pandora's box model. However, despite the above examples and later extensions
thereof, the space of problems that the Gittins index can solve perfectly
optimally is limited, and its definition is rather subtle compared to those of
other multi-armed bandit algorithms. As a result, the Gittins index is often
regarded as being primarily a concept of theoretical importance, rather than a
practical tool for solving decision-making problems.
  The aim of this tutorial is to demonstrate that the Gittins index can be
fruitfully applied to practical problems. We start by giving an example-driven
introduction to the Gittins index, then walk through several examples of
problems it solves - some optimally, some suboptimally but still with excellent
performance. Two practical highlights in the latter category are applying the
Gittins index to Bayesian optimization, and applying the Gittins index to
minimizing tail latency in queues.

</details>
