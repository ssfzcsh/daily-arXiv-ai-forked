<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.HC](#cs.HC) [Total: 11]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 5]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.IR](#cs.IR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: 该研究填补了自动程序修复（APR）工具综合评估框架的空白，并以Sorald为例进行了评估。结果显示，虽然Sorald能修复特定规则违规，但会引入新故障，降低功能正确性并破坏代码结构。


<details>
  <summary>Details</summary>
Motivation: 在LLM时代，高质量软件开发需求迫切，APR工具通过修复代码违规来提升代码质量。但先前研究仅关注其修复能力，忽视了新引入的问题，因此需开发更全面的评估框架。

Method: 研究以Sorald为概念验证工具，评估其在修复2,393个Java代码片段中3,529个SonarQube违规时的表现。

Result: Sorald修复了违规，但引入了2,120个新故障（32个错误，2,088个代码异味），功能正确性下降24%，代码结构退化。

Conclusion: 研究强调需开发涵盖APR工具全面影响的评估方法，以确保其安全有效应用。

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [2] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: 论文提出将生成式AI与传统软件工程结合，以构建更可靠、高效的系统，并提出了五大设计原则和架构模式。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在不可预测性和低效性方面的挑战，以推动更可靠的AI赋能系统发展。

Method: 提出五大设计原则（可靠性、卓越性、可进化性、自依赖性和保障性）及架构模式（如GenAI-native单元）。

Result: 初步框架为构建自适应、高效的系统提供了指导，但需进一步验证。

Conclusion: 未来的研究方向包括实施和优化这一概念框架，以促进技术和社会影响的良性发展。

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [3] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 知识蒸馏（KD）在代码理解任务中表现优异，能显著提升学生模型性能，保留98%教师模型能力且仅需5%参数。


<details>
  <summary>Details</summary>
Motivation: 解决预训练语言模型（PLMs）在代码理解中的计算强度和推理延迟问题。

Method: 系统研究知识蒸馏在代码任务中的效果，实验涵盖两种KD方法（基于logit和特征）、八种学生模型和两种教师模型。

Result: KD能显著提升学生模型性能，特征型KD方法效果最佳，学生架构与教师相似性不保证更好结果。

Conclusion: KD是高效代码理解的可行方案，未来可进一步探索其潜力。

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [4] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: SynthCoder结合行业领先技术，在代码补全任务中取得最优性能，解决了常见模型重复代码的问题。


<details>
  <summary>Details</summary>
Motivation: 由于现有优化方法存在权衡效应，导致性能提升不均衡，甚至低于基线模型，因此需要一种更优的模型。

Method: 通过AST节点提取和开发者行为模拟构建多样化数据集，利用BM25算法和调用图增强上下文信息，采用两阶段训练（课程学习和DPO对齐）。

Result: 在多个代码补全基准测试中表现优异，并有效减少了代码重复现象。

Conclusion: SynthCoder在代码补全任务中实现了最佳性能，并通过高质量数据集缓解了模型重复代码的问题。

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [5] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,João R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: 论文提出了两个数据集和工具支持，用于评估任务型聊天机器人的可靠性、安全和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模、高质量的数据集，任务型聊天机器人的可靠性、安全和鲁棒性评估尚未充分探索。

Method: 提出了两个数据集：TOFU-R（GitHub上的Rasa聊天机器人快照）和BRASATO（精选的复杂聊天机器人集合），并提供了工具支持。

Result: TOFU-R代表了开源Rasa聊天机器人的实践状态，BRASATO则简化了研究复现和可靠性评估。

Conclusion: 这些数据集和工具填补了任务型聊天机器人评估领域的空白，促进了相关研究的发展。

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [6] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin Muñoz Barón,Chunyang Chen,Lukas Böhme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: 总结：大型语言模型（LLM）在软件工程研究中存在复现难题，社区提出了八项准则以提高透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 应对LLM在研究中因非确定性、训练数据不透明和架构演变导致的复现和可复制性问题。

Method: 提出LLM研究的分类法和八项设计及报告准则，强调研究过程的透明性。

Result: 八项准则包括声明使用、报告配置、文档工具、公开提示、人工验证、开放LLM基线、基准和度量标准，以及明确局限性。

Conclusion: 通过分类法和准则，推动LLM研究的可复现性和可复制性，并在线维护资源供社区使用和改进。

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>


### [7] [QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements](https://arxiv.org/abs/2508.15512)
*Markus Borg,Martin Larsson,Philip Breid,Nadim Hagatulah*

Main category: cs.SE

TL;DR: 论文提出QUPER-MAn模型，将可维护性从次要问题转化为主动管理目标。


<details>
  <summary>Details</summary>
Motivation: 现有研究中可维护性常被忽视，需求工程可以弥合这一差距。

Method: 采用设计科学研究方法，开发了QUPER-MAn模型。

Result: 可维护性仍是次要关注点，QUPER-MAn能支持目标设定。

Conclusion: QUPER-MAn可通过需求工程提升可维护性管理。

Abstract: Maintainable source code is essential for sustainable development in any
software organization. Unfortunately, many studies show that maintainability
often receives less attention than its importance warrants. We argue that
requirements engineering can address this gap the problem by fostering
discussions and setting appropriate targets in a responsible manner. In this
preliminary work, we conducted an exploratory study of industry practices
related to requirements engineering for maintainability. Our findings confirm
previous studies: maintainability remains a second-class quality concern.
Explicit requirements often make sweeping references to coding conventions.
Tools providing maintainability proxies are common but typically only used in
implicit requirements related to engineering practices. To address this, we
propose QUPER-MAn, a maintainability adaption of the QUPER model, which was
originally developed to help organizations set targets for performance
requirements. Developed using a design science approach, QUPER-MAn, integrates
maintainability benchmarks and supports target setting. We posit that it can
shift maintainability from an overlooked development consequence to an actively
managed goal driven by informed and responsible engineering decisions.

</details>


### [8] [A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs](https://arxiv.org/abs/2508.15536)
*Yi Zhang,He Jiang,Xiaochen Li,Shikai Guo,Peiyu Zou,Zun Wang*

Main category: cs.SE

TL;DR: VERMEI是一种测试FPGA逻辑综合工具的新方法，通过预处理、等价变异和错误识别模块提高测试效果。


<details>
  <summary>Details</summary>
Motivation: FPGA逻辑综合工具的缺陷可能导致意外行为和安全风险，但现有测试方法在语义和逻辑复杂性上不足。

Method: VERMEI通过预处理识别僵尸逻辑，等价变异生成复杂变体，差分测试识别错误。

Result: 实验显示VERMEI优于现有方法，5个月内报告15个错误，9个为新确认。

Conclusion: VERMEI有效提升FPGA逻辑综合工具的测试能力。

Abstract: FPGA (Field-Programmable Gate Array) logic synthesis tools are key components
in the EDA (Electronic Design Automation) toolchain. They convert hardware
designs written in description languages such as Verilog into gate-level
representations for FPGAs. However, defects in these tools may lead to
unexpected behaviors and pose security risks. Therefore, it is crucial to
harden these tools through testing. Although several methods have been proposed
to automatically test FPGA logic synthesis tools, the challenge remains of
insufficient semantic and logical complexity in test programs. In this paper,
we propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI
consists of three modules: preprocessing, equivalent mutation, and bug
identification. The preprocessing module identifies zombie logic (inactive code
with no impact on the circuit output) in seed programs through simulation and
coverage analysis. The equivalent mutation module generates equivalent variants
of seed programs by pruning or inserting logic fragments in zombie areas. It
uses Bayesian sampling to extract logic fragments from historical Verilog
designs, making the generated variants have complex control flows and
structures. The bug identification module, based on differential testing,
compares the synthesized outputs of seed and variant programs to identify bugs.
Experiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms
the state-of-the-art methods. Within five months, VERMEI reported 15 bugs to
vendors, 9 of which were confirmed as new.

</details>


### [9] [Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study](https://arxiv.org/abs/2508.15570)
*Marion Wiese,Kamila Serwa,Anastasia Besier,Ariane S. Marion-Jetten,Eva Bittner*

Main category: cs.SE

TL;DR: 论文通过行动研究在IT公司中实施了技术债务管理（TDM）流程，探讨了实践者采用的TD活动方法及TDM对技术债务意识的长期影响。


<details>
  <summary>Details</summary>
Motivation: 技术债务管理（TDM）虽被广泛研究，但实践中应用较少。本研究旨在基于预定研讨会概念，在IT公司中建立TDM流程并分析其对TD意识的影响。

Method: 采用行动研究方法（16个月内5个行动周期），通过问卷分析、团队会议观察、心理学方法（TD-SAGAT）和待办事项数据评估TD意识。

Result: 实践者倾向于基于系统演进和成本计算的TD偿还和优先排序。待办事项中的提醒（如复选框或文本模板）可持续提高TD意识。

Conclusion: 研讨会方法可行且能带来可持续的流程变革。研究还提出了适用于其他IT团队的新TDM思路，如重新提交日期、TD讨论复选框和可视化优先排序。

Abstract: Context. Technical debt (TD) items are constructs in a software system
providing short-term benefits but hindering future changes. TD management (TDM)
is frequently researched but rarely adopted in practice. Goal. This study aimed
to establish a TDM process in an IT company based on a predefined workshop
concept. We analyzed which research approaches practitioners adopted for each
TD activity and the TDM's long-term effect on TD awareness. Method. We used
action research (five action cycles in 16 months) with an IT team that creates
IT solutions for signal processing. To examine TD awareness, we (1) analyzed
questionnaires completed during each workshop, (2) observed team meetings, (3)
adopted a method from psychology for measuring awareness in decision-making
situations called TD-SAGAT, and (4) evaluated the backlog data. Results.
Practitioners preferred TD repayment and prioritization based on the system's
evolution and cost calculations, i.e., repayment of so-called low-hanging
fruits. Reminders in the backlog items, such as checkboxes or text templates,
led to a sustainable rise in TD awareness. Conclusions. We showed that a
workshop-based approach is feasible and leads to sustainable process changes.
New ideas for TDM applicable to other IT teams emerged, e.g., using a
re-submission date, using a Talked about TD checkbox, and using visualizations
for TD prioritization.

</details>


### [10] [From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems](https://arxiv.org/abs/2508.15584)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 论文报告了PREVENT和REACT方法在船舶系统中的集成应用，展示了异常检测与故障排除的结合效果。


<details>
  <summary>Details</summary>
Motivation: 针对工业系统因磨损、误用或故障导致的异常行为，需要及时检测、定位并采取应对措施。

Method: 采用PREVENT故障预测方法及其扩展的REACT故障排除模块，应用于Fincantieri的船舶系统。

Result: 成功实现了异常检测与故障排除的集成。

Conclusion: 总结了经验教训，有助于将这些分析方法推广到其他工业产品。

Abstract: Complex and large industrial systems often misbehave, for instance, due to
wear, misuse, or faults. To cope with these incidents, it is important to
timely detect their occurrences, localize the sources of the problems, and
implement the appropriate countermeasures. This paper reports our experience
with a state-of-the-art failure prediction method, PREVENT, and its extension
with a troubleshooting module, REACT, applied to naval systems developed by
Fincantieri. Our results show how to integrate anomaly detection with
troubleshooting procedures. We conclude by discussing a lesson learned, which
may help deploy and extend these analyses to other industrial products.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Homomorphism Calculus for User-Defined Aggregations](https://arxiv.org/abs/2508.15109)
*Ziteng Wang,Ruijie Fang,Linus Zheng,Dixin Tang,Isil Dillig*

Main category: cs.PL

TL;DR: 论文提出了一种新的同态演算，用于验证和反驳用户定义的聚合函数（UDAF）是否满足同态性质，并能构造相应的合并算子以提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有数据处理框架（如Spark和Flink）支持UDAF，但需满足同态性质以实现高效执行，但缺乏自动验证和构造的方法。

Method: 提出了一种同态演算，可验证UDAF的同态性质，并构造合并算子以支持增量计算和并行执行。

Result: 基于该演算实现了算法，并在真实UDAF上验证，显著优于现有合成工具。

Conclusion: 该方法能有效解决UDAF的同态验证和合并算子构造问题，提升了计算效率。

Abstract: Data processing frameworks like Apache Spark and Flink provide built-in
support for user-defined aggregation functions (UDAFs), enabling the
integration of domain-specific logic. However, for these frameworks to support
\emph{efficient} UDAF execution, the function needs to satisfy a
\emph{homomorphism property}, which ensures that partial results from
independent computations can be merged correctly. Motivated by this problem,
this paper introduces a novel \emph{homomorphism calculus} that can both verify
and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also
enables the construction of a corresponding merge operator which can be used
for incremental computation and parallel execution. We have implemented an
algorithm based on our proposed calculus and evaluate it on real-world UDAFs,
demonstrating that our approach significantly outperforms two leading
synthesizers.

</details>


### [12] [Software Model Checking via Summary-Guided Search (Extended Version)](https://arxiv.org/abs/2508.15137)
*Ruijie Fang,Zachary Kincaid,Thomas Reps*

Main category: cs.PL

TL;DR: GPS是一种新的软件模型检查算法，结合静态分析和搜索策略，高效发现程序中的安全证明和错误输入，且性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 提出一种高效的软件模型检查算法，以解决现有工具在处理长且输入相关的错误路径时的效率问题。

Method: GPS通过组合静态分析摘要和分层搜索策略，修剪不可行路径并驱动测试生成，探索新程序状态。

Result: GPS在SV-COMP等基准测试中表现优于现有最先进工具，解决更多问题且运行时间更短。

Conclusion: GPS不仅高效且具有完备性，通过新颖技术实现了性能和功能的平衡。

Abstract: In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a suite of benchmarks including both programs from the Software
Verification Competition (SV-COMP) and from prior literature, and found that
our implementation of GPS outperforms state-of-the-art software model checkers
(including the top performers in SV-COMP ReachSafety-Loops category), both in
terms of the number of benchmarks solved and in terms of running time.

</details>


### [13] [Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment](https://arxiv.org/abs/2508.15157)
*David M Kahn,Jan Hoffmann,Runming Li*

Main category: cs.PL

TL;DR: 大步骤语义扩展为big-stop语义，通过少量额外规则支持发散计算，而不引入错误状态，保持简洁性。


<details>
  <summary>Details</summary>
Motivation: 大步骤语义虽然简洁但无法描述发散计算，需扩展以弥补这一缺陷。

Method: 引入big-stop语义，扩展标准大步骤推理规则，定义与小步骤语义等价的计算判断。

Result: 成功在多种语言（PCF、命令式语言）中实现big-stop语义，且规则简洁。

Conclusion: big-stop语义既能保持大步骤的简洁性，又能描述发散计算，优于其他复杂方案。

Abstract: As evident in the programming language literature, many practitioners favor
specifying dynamic program behavior using big-step over small-step semantics.
Unlike small-step semantics, which must dwell on every intermediate program
state, big-step semantics conveniently jump directly to the ever-important
result of the computation. Big-step semantics also typically involve fewer
inference rules than their small-step counterparts. However, in exchange for
ergonomics, big-step semantics give up power: Small-step semantics describes
program behaviors that are outside the grasp of big-step semantics, notably
divergence. This work presents a little-known extension of big-step semantics
with inductive definitions that captures diverging computations without
introducing error states. This big-stop semantics is illustrated for typed,
untyped, and effectful variants of PCF, as well as a while-loop-based
imperative language. Big-stop semantics extends the standard big-step inference
rules with a few additional rules to define an evaluation judgment that is
equivalent to the reflexive-transitive closure of small-step transitions. This
simple extension contrasts with other solutions in the literature which
sacrifice ergonomics by introducing many additional inference rules, global
state, and/or less-commonly-understood reasoning principles like coinduction.

</details>


### [14] [Probabilistic Inference for Datalog with Correlated Inputs](https://arxiv.org/abs/2508.15166)
*Jingbo Wang,Shashin Halalingaiah,Weiyi Chen,Chao Wang,Isil Dillig*

Main category: cs.PL

TL;DR: 该论文提出了一个名为Praline的新方法，用于在有（部分已知）输入相关性的情况下进行精确的概率推理，解决了传统概率逻辑编程语言无法处理输入事实之间统计相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的概率逻辑编程语言（如ProbLog）无法处理输入事实之间的统计相关性，限制了推理的精确性和适用范围。

Method: 论文将推理任务建模为约束优化问题，并提出了一种基于约束求解、静态分析和迭代优化的高效δ-精确推理算法。

Result: 实验表明，该方法在现实基准测试中能有效扩展并提供紧密的概率界限。

Conclusion: Praline在解决输入相关性问题上表现出色，并具有较高的可扩展性和精确性。

Abstract: Probabilistic extensions of logic programming languages, such as ProbLog,
integrate logical reasoning with probabilistic inference to evaluate
probabilities of output relations; however, prior work does not account for
potential statistical correlations among input facts. This paper introduces
Praline, a new extension to Datalog designed for precise probabilistic
inference in the presence of (partially known) input correlations. We formulate
the inference task as a constrained optimization problem, where the solution
yields sound and precise probability bounds for output facts. However, due to
the complexity of the resulting optimization problem, this approach alone often
does not scale to large programs. To address scalability, we propose a more
efficient $\delta$-exact inference algorithm that leverages constraint solving,
static analysis, and iterative refinement. Our empirical evaluation on
challenging real-world benchmarks, including side-channel analysis,
demonstrates that our method not only scales effectively but also delivers
tight probability bounds.

</details>


### [15] [Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern](https://arxiv.org/abs/2508.15264)
*Patrick Redmond,Jonathan Castello,José Manuel Calderón Trilla,Lindsey Kuper*

Main category: cs.PL

TL;DR: 论文探讨了ECS（实体-组件-系统）设计模式的本质，提出了一个抽象模型Core ECS，揭示了其确定性并发编程的潜力。


<details>
  <summary>Details</summary>
Motivation: ECS模式虽然在游戏开发中广泛应用，但其本质在多领域中未被深入理解，现有解释多基于具体框架或不完美比喻，缺乏理论严谨性。

Method: 通过设计抽象模型Core ECS，剥离具体实现细节，研究ECS程序在调度无关下的确定性行为。

Result: 发现ECS模式可作为确定性并发编程模型，但现有框架未能充分利用这一潜力。

Conclusion: 研究为ECS实现技术开辟了新方向，强调对确定性并发的优化空间。

Abstract: The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.

</details>


### [16] [Fair Termination for Resource-Aware Active Objects](https://arxiv.org/abs/2508.15333)
*Francesco Dagnino,Paola Giannini,Violet Ka I Pun,Ulises Torrella*

Main category: cs.PL

TL;DR: 本文开发了一个资源感知的活动对象核心演算及类型系统，确保程序能公平终止。


<details>
  <summary>Details</summary>
Motivation: 活动对象系统用于分布式系统和业务过程建模，具有并发和资源感知的特点，因此需要资源感知的形式化方法。

Method: 结合分级语义和类型系统技术（适用于顺序程序）与公平终止技术（用于同步会话）。

Result: 提出了一个核心演算和类型系统，确保程序始终能最终终止。

Conclusion: 研究成果为资源感知的活动对象模型提供了形式化保障。

Abstract: Active object systems are a model of distributed computation that has been
adopted for modelling distributed systems and business process workflows. This
field of modelling is, in essence, concurrent and resource-aware, motivating
the development of resource-aware formalisations on the active object model.
The contributions of this work are the development of a core calculus for
resource-aware active objects together with a type system ensuring that
well-typed programs are fairly terminating, i.e., they can always eventually
terminate. To achieve this, we combine techniques from graded semantics and
type systems, which are quite well understood for sequential programs, with
those for fair termination, which have been developed for synchronous~sessions.

</details>


### [17] [Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)](https://arxiv.org/abs/2508.15576)
*Andreas Lööw,Seung Hoon Park,Daniele Nantes-Sobrinho,Sacha-Élie Ayoun,Opale Sjöstedt,Philippa Gardner*

Main category: cs.PL

TL;DR: 该论文为内存模型参数的CSE平台提供了新的形式化基础，通过机械化、验证多种内存模型、涵盖SL和ISL分析，并基于标准定义，提升了现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有CSE平台缺乏对内存模型参数化的满意形式化基础，限制了灵活性和适用范围。

Method: 论文提出新的形式化基础，并在Rocq中机械化，验证了多种内存模型的应用，同时涵盖SL和ISL分析。

Result: 成功实现了对C和CHERI等内存模型的验证，为CSE平台提供了更灵活的形式化支持。

Conclusion: 该研究为内存模型参数的CSE平台建立了全面的形式化基础，提升了工具的可扩展性和适用性。

Abstract: Multiple successful compositional symbolic execution (CSE) tools and
platforms exploit separation logic (SL) for compositional verification and/or
incorrectness separation logic (ISL) for compositional bug-finding, including
VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian
platform, the only CSE platform that is parametric on the memory model, meaning
that it can be instantiated to different memory models, suggests that the
ability to use custom memory models allows for more flexibility in supporting
analysis of a wide range of programming languages, for implementing custom
automation, and for improving performance. However, the literature lacks a
satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for
memory-model-parametric CSE platforms. Our foundation advances the state of the
art in four ways. First, we mechanise our foundation (in the interactive
theorem prover Rocq). Second, we validate our foundation by instantiating it to
a broad range of memory models, including models for C and CHERI. Third,
whereas previous memory-model-parametric work has only covered SL analyses, we
cover both SL and ISL analyses. Fourth, our foundation is based on standard
definitions of SL and ISL (including definitions of function specification
validity, to ensure sound interoperation with other tools and platforms also
based on standard definitions).

</details>


### [18] [Active Learning for Neurosymbolic Program Synthesis](https://arxiv.org/abs/2508.15750)
*Celeste Barnaby,Qiaochu Chen,Ramya Ramalingam,Osbert Bastani,Isil Dillig*

Main category: cs.PL

TL;DR: 论文提出了一种新的主动学习方法SmartLabel，专门针对神经符号程序合成中的神经组件预测错误问题，使用了约束共形评估（CCE）策略，实验显示其效果显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习技术在神经符号程序合成中由于神经组件预测错误导致生成错误程序，因此需要新方法解决这一问题。

Method: 提出了基于约束共形评估（CCE）的新策略，通过迭代优化CCE确保程序观察等价性。

Result: 实验在三个神经符号领域进行，SmartLabel在98%的基准测试中成功识别真实程序，平均仅需不到5轮用户交互，而现有技术仅能覆盖65%。

Conclusion: SmartLabel通过CCE显著提高了神经符号程序合成的准确性和效率，解决了神经预测错误的问题。

Abstract: The goal of active learning for program synthesis is to synthesize the
desired program by asking targeted questions that minimize user interaction.
While prior work has explored active learning in the purely symbolic setting,
such techniques are inadequate for the increasingly popular paradigm of
neurosymbolic program synthesis, where the synthesized program incorporates
neural components. When applied to the neurosymbolic setting, such techniques
can -- and, in practice, do -- return an unintended program due to
mispredictions of neural components. This paper proposes a new active learning
technique that can handle the unique challenges posed by neural network
mispredictions. Our approach is based upon a new evaluation strategy called
constrained conformal evaluation (CCE), which accounts for neural
mispredictions while taking into account user-provided feedback. Our proposed
method iteratively makes CCE more precise until all remaining programs are
guaranteed to be observationally equivalent. We have implemented this method in
a tool called SmartLabel and experimentally evaluated it on three neurosymbolic
domains. Our results demonstrate that SmartLabel identifies the ground truth
program for 98% of the benchmarks, requiring under 5 rounds of user interaction
on average. In contrast, prior techniques for active learning are only able to
converge to the ground truth program for at most 65% of the benchmarks.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [19] [Mitigating context switching in densely packed Linux clusters with Latency-Aware Group Scheduling](https://arxiv.org/abs/2508.15703)
*Al Amjad Tawfiq Isstaif,Evangelia Kalyvianaki,Richard Mortier*

Main category: cs.OS

TL;DR: 论文指出在高密度工作负载（如无服务器应用）中，CPU上下文切换开销会显著降低节点性能，即使调度器理论上是合理的。作者通过修改Linux内核调度器解决了这一问题，显著提升了性能并减少了集群规模需求。


<details>
  <summary>Details</summary>
Motivation: 现有的集群编排器（如Kubernetes）依赖于节点容量和任务需求的准确估计，但实际运行中，尤其是在高密度工作负载下，CPU上下文切换的开销会导致性能下降，通常通过过度配置集群来缓解，造成资源浪费。

Method: 作者分析了上下文切换的开销来源，提出并评估了对标准Linux内核调度器的改进，通过优先完成任务而非低级别的任务公平性，减少上下文切换时间。

Result: 改进后的调度器在性能上达到相同效果，同时集群规模需求减少了28%。

Conclusion: 通过调整调度器的任务完成优先级，可以有效减少上下文切换带来的性能损失，显著提升资源利用率。

Abstract: Cluster orchestrators such as Kubernetes depend on accurate estimates of node
capacity and job requirements. Inaccuracies in either lead to poor placement
decisions and degraded cluster performance. In this paper, we show that in
densely packed workloads, such as serverless applications, CPU context
switching overheads can become so significant that a node's performance is
severely degraded, even when the orchestrator placement is theoretically sound.
In practice this issue is typically mitigated by over-provisioning the cluster,
leading to wasted resources.
  We show that these context switching overhead arise from both an increase in
the average cost of an individual context switch and a higher rate of context
switching, which together amplify overhead multiplicatively when managing large
numbers of concurrent cgroups, Linux's group scheduling mechanism for managing
multi-threaded colocated workloads. We propose and evaluate modifications to
the standard Linux kernel scheduler that mitigate these effects, achieving the
same effective performance with a 28% smaller cluster size. The key insight
behind our approach is to prioritise task completion over low-level per-task
fairness, enabling the scheduler to drain contended CPU run queues more rapidly
and thereby reduce time spent on context switching.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Toward Sustainable Subterranean mMTC: Space-Air-Ground-Underground Networks Powered by LoRaWAN and Wireless Energy Transfer](https://arxiv.org/abs/2508.15058)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 论文提出了一种新型空地地下一体化网络（SAGUIN）架构，结合LoRaWAN和无线能量传输技术，以支持可持续的地下大规模机器类通信（mMTC）。


<details>
  <summary>Details</summary>
Motivation: 解决地下无线传感器网络（WUSNs）在恶劣环境下资源稀缺、通信覆盖受限的问题，特别是在偏远、灾害频发的地区。

Method: 整合卫星系统、空中平台、地面网络和地下通信，采用LoRaWAN和无线能量传输技术，并通过仿真评估其性能。

Result: 结果表明，SAGUIN系统结合优化的时间分配策略和适当配置的扩展因子（SF），可显著延长地下设备的运行寿命。

Conclusion: SAGUIN架构为可持续的地下mMTC提供了有效解决方案，但仍需进一步研究解决关键挑战。

Abstract: Wireless underground sensor networks (WUSNs), which enable real-time sensing
and monitoring of underground resources by underground devices (UDs), hold
great promise for delivering substantial social and economic benefits across
various verticals. However, due to the harsh subterranean environment, scarce
network resources, and restricted communication coverage, WUSNs face
significant challenges in supporting sustainable massive machine-type
communications (mMTC), particularly in remote, disaster-stricken, and
hard-to-reach areas. To complement this, we conceptualize in this study a novel
space-air-ground-underground integrated network (SAGUIN) architecture that
seamlessly incorporates satellite systems, aerial platforms, terrestrial
networks, and underground communications. On this basis, we integrate LoRaWAN
and wireless energy transfer (WET) technologies into SAGUIN to enable
sustainable subterranean mMTC. We begin by reviewing the relevant technical
background and presenting the architecture and implementation challenges of
SAGUIN. Then, we employ simulations to model a remote underground pipeline
monitoring scenario to evaluate the feasibility and performance of SAGUIN based
on LoRaWAN and WET technologies, focusing on the effects of parameters such as
underground conditions, time allocation, LoRaWAN spread factor (SF)
configurations, reporting periods, and harvested energy levels. Our results
evidence that the proposed SAGUIN system, when combined with the derived time
allocation strategy and an appropriate SF, can effectively extend the
operational lifetime of UDs, thereby facilitating sustainable subterranean
mMTC. Finally, we pinpoint key challenges and future research directions for
SAGUIN.

</details>


### [21] [From 5G RAN Queue Dynamics to Playback: A Performance Analysis for QUIC Video Streaming](https://arxiv.org/abs/2508.15087)
*Jashanjot Singh Sidhu,Jorge Ignacio Sandoval,Abdelhak Bentaleb,Sandra Cespedes*

Main category: cs.NI

TL;DR: QUIC在5G网络中优化视频流质量需要跨层协调，主动队列管理策略（如RED和L4S）与QUIC、拥塞控制算法及自适应比特率方案的动态交互是关键。


<details>
  <summary>Details</summary>
Motivation: 5G网络支持超低延迟和高带宽，但移动网络中视频流的质量体验（QoE）优化仍面临挑战，需要综合考虑应用层的ABR方案、传输层的拥塞控制算法及链路层的RLC排队。

Method: 通过综合分析现代主动队列管理策略（如RED和L4S）在5G环境中对视频流的影响，特别是其与QUIC实现、RLC缓冲区、拥塞控制算法及ABR方案的交互作用。

Result: 研究表明，AQM策略的有效性取决于其与QUIC实现、CC算法和ABR方案的动态交互，单一优化效果有限。

Conclusion: 需要跨层的实时协调机制，以充分利用5G网络能力，提供稳健、自适应且高质量的视频流服务。

Abstract: The rapid adoption of QUIC as a transport protocol has transformed content
delivery by reducing latency, enhancing congestion control (CC), and enabling
more efficient multiplexing. With the advent of 5G networks, which support
ultra-low latency and high bandwidth, streaming high-resolution video at 4K and
beyond has become increasingly viable. However, optimizing Quality of
Experience (QoE) in mobile networks remains challenging due to the complex
interactions among Adaptive Bit Rate (ABR) schemes at the application layer, CC
algorithms at the transport layer, and Radio Link Control (RLC) queuing at the
link layer in the 5G network. While prior studies have largely examined these
components in isolation, this work presents a comprehensive analysis of the
impact of modern active queue management (AQM) strategies, such as RED and L4S,
on video streaming over diverse QUIC implementations--focusing particularly on
their interaction with the RLC buffer in 5G environments and the interplay
between CC algorithms and ABR schemes. Our findings demonstrate that the
effectiveness of AQM strategies in improving video streaming QoE is
intrinsically linked to their dynamic interaction with QUIC implementations, CC
algorithms and ABR schemes-highlighting that isolated optimizations are
insufficient. This intricate interdependence necessitates holistic, cross-layer
adaptive mechanisms capable of real-time coordination between network,
transport and application layers, which are crucial for fully leveraging the
capabilities of 5G networks to deliver robust, adaptive, and high-quality video
streaming.

</details>


### [22] [Toward Autonomous Digital Populations for Communication-Sensing-Computation Ecosystem](https://arxiv.org/abs/2508.15268)
*Gaosheng Zhao,Dong In Kim*

Main category: cs.NI

TL;DR: 本文提出了一种基于数字孪生技术的自然启发架构框架，以解决当前网络在集中控制、静态设计和人为干预方面的局限性，旨在构建具备动态协调、分布式决策和自适应能力的下一代通信网络。


<details>
  <summary>Details</summary>
Motivation: 当前通信网络依赖集中控制和静态设计，限制了其在复杂环境中的适应性和韧性，亟需一种更灵活、自适应的解决方案。

Method: 采用数字孪生技术，将边缘设备组织成功能性数字群，并通过云端的多群集成实现数字生态系统的自发演化。

Result: 提出了一个结合工程方法和社会技术洞察的框架，为下一代网络奠定了理论基础。

Conclusion: 该框架为未来通信网络提供了动态协调、分布式决策和持续演化的能力，有助于构建更灵活和韧性的基础设施。

Abstract: Future communication networks are expected to achieve deep integration of
communication, sensing, and computation, forming a tightly coupled and
autonomously operating infrastructure system. However, current reliance on
centralized control, static design, and human intervention continues to
constrain the multidimensional evolution of network functions and applications,
limiting adaptability and resilience in large-scale, layered, and complex
environments. To address these challenges, this paper proposes a
nature-inspired architectural framework that leverages digital twin technology
to organize connected devices at the edge into functional digital populations,
while enabling the emergence of an evolvable digital ecosystem through
multi-population integration at the cloud. We believe that this framework,
which combines engineering methodologies with sociotechnical insights, lays the
theoretical foundation for building next-generation communication networks with
dynamic coordination, distributed decision-making, continuous adaptation, and
evolutionary capabilities.

</details>


### [23] [Unlocking the Performance Potential of Mega-Constellation Networks: An Exploration of Structure-Building Paradigms](https://arxiv.org/abs/2508.15307)
*Xiangtong Wang,Wei Li,Menglong Yang,Songchen Han*

Main category: cs.NI

TL;DR: 论文提出了一种名为SML（Structure = Motif + Lattice）的新范式，用于优化巨型星座网络（MCN）的设计，通过解耦为局部和全局设计，解决高可用性和低延迟问题。


<details>
  <summary>Details</summary>
Motivation: 设计巨型星座网络（MCN）时，如何在有限的平均传输延迟内配置最稳定的星间链路（ISL）是关键挑战。本文旨在解决这一问题。

Method: 提出SML范式，将MCN设计分解为局部Motif和全局Lattice设计，并开发高效启发式算法SMLOP，在多项式时间内求解HALLMD问题。

Result: 在四种先进星座上的实验验证表明，SML显著提升了容量（5~18%）、吞吐量（1~12%），并降低了路径扩展（12~23%）和往返时间RTT（8~77%）。

Conclusion: SML范式能够有效优化MCN的设计，在保持低延迟的同时提高网络可用性，适用于未来空间无线通信网络。

Abstract: The network structure design plays a vital role in the mega-constellation
network (MSN) to coordinate massive network nodes to ensure the effectiveness
and reliability of operations and services for future space wireless
communications networks.
  One of the critical issues in MCN is how to design an optimal network control
structure by configuring the most stable inter-satellite link (ISL) to achieve
high available MCN within a limited average transmission delays.
  To address this problem, this paper introduces a novel MCN structure design
paradigm: Structure = Motif + Lattice (SML), which decouples MCN design into
local motifs design and global lattices design. Specifically, we formulate the
High-Availability and Low-Latency Mega-Constellation Design (HALLMD) problem,
aimed at maximizing ISL availability while minimizing the transmission latency.
To solve HALLMD, we propose SMLOP, a heuristic algorithm that efficiently finds
optimal network structures in polynomial time. Experimental validation on four
public state-of-the-art constellations demonstrates significant improvements,
including enhanced capacity by $5\sim 18\%$, increased throughput by $1\sim
12\%$, reduced path stretch by $12\sim 23\%$, and Round-Trip Time (RTT) by
$8\sim 77\%$.

</details>


### [24] [Interface on demand: Towards AI native Control interfaces for 6G](https://arxiv.org/abs/2508.15595)
*Abhishek Dandekar,Prashiddha D. Thapa,Ashrafur Rahman,Julius Schulz-Zander*

Main category: cs.NI

TL;DR: 提出利用大语言模型的多智能体框架，动态生成网络功能间的控制接口，解决传统接口的局限性问题。


<details>
  <summary>Details</summary>
Motivation: 传统标准化网络接口存在供应商不兼容性、设计僵化和缺乏适应性等问题，亟需创新解决方案。

Method: 采用匹配智能体和对齐能力，代码生成智能体实现接口API服务器，并在多供应商环境中验证。

Result: 性能评估展示了接口生成任务中成本与延迟的权衡，验证了方法的有效性。

Conclusion: 该研究为未来移动网络的AI原生动态接口生成奠定了基础，增强了互操作性和适应性。

Abstract: Traditional standardized network interfaces face significant limitations,
including vendor-specific incompatibilities, rigid design assumptions, and lack
of adaptability for new functionalities. We propose a multi-agent framework
leveraging large language models (LLMs) to generate control interfaces on
demand between network functions (NFs). This includes a matching agent, which
aligns required control functionalities with NF capabilities, and a
code-generation agent, which generates the necessary API server for interface
realization. We validate our approach using simulated multi-vendor gNB and WLAN
AP environments. The performance evaluations highlight the trade-offs between
cost and latency across LLMs for interface generation tasks. Our work sets the
foundation for AI-native dynamic control interface generation, paving the way
for enhanced interoperability and adaptability in future mobile networks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [25] [Robust Symbolic Reasoning for Visual Narratives via Hierarchical and Semantically Normalized Knowledge Graphs](https://arxiv.org/abs/2508.14941)
*Yi-Chun Chen*

Main category: cs.MM

TL;DR: 提出了一种用于层次化叙事知识图谱的语义归一化框架，通过减少标注噪声和语义对齐提升叙事的连贯性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉叙事（如漫画）需要结构化表示，但现有符号叙事图谱常因不一致和冗余限制了推理和泛化的有效性。

Method: 基于认知模型，利用词汇相似性和嵌入聚类整合语义相关动作和事件，实现语义归一化。

Result: 在Manga109数据集上验证，归一化后的图谱在动作检索、角色定位等任务中展现出更高的连贯性和鲁棒性。

Conclusion: 语义归一化是构建可扩展、认知启发的多模态叙事理解模型的关键步骤。

Abstract: Understanding visual narratives such as comics requires structured
representations that capture events, characters, and their relations across
multiple levels of story organization. However, symbolic narrative graphs often
suffer from inconsistency and redundancy, where similar actions or events are
labeled differently across annotations or contexts. Such variance limits the
effectiveness of reasoning and generalization.
  This paper introduces a semantic normalization framework for hierarchical
narrative knowledge graphs. Building on cognitively grounded models of
narrative comprehension, we propose methods that consolidate semantically
related actions and events using lexical similarity and embedding-based
clustering. The normalization process reduces annotation noise, aligns symbolic
categories across narrative levels, and preserves interpretability.
  We demonstrate the framework on annotated manga stories from the Manga109
dataset, applying normalization to panel-, event-, and story-level graphs.
Preliminary evaluations across narrative reasoning tasks, such as action
retrieval, character grounding, and event summarization, show that semantic
normalization improves coherence and robustness, while maintaining symbolic
transparency. These findings suggest that normalization is a key step toward
scalable, cognitively inspired graph models for multimodal narrative
understanding.

</details>


### [26] [Holo-Artisan: A Personalized Multi-User Holographic Experience for Virtual Museums on the Edge Intelligence](https://arxiv.org/abs/2508.14956)
*Nan-Hong Kuo,Hojjat Baghban*

Main category: cs.MM

TL;DR: Holo-Artisan是一个通过全息显示和边缘智能实现虚拟博物馆沉浸式多用户体验的系统，结合生成式AI和联邦学习，提供个性化互动。


<details>
  <summary>Details</summary>
Motivation: 传统博物馆展品静态，缺乏互动性，Holo-Artisan旨在通过技术手段使展品动态化，提供个性化互动体验。

Method: 利用边缘计算节点处理用户实时数据，生成式AI驱动数字艺术品响应，联邦学习保护隐私并优化模型，云平台协调多人共享场景。

Result: 系统实现了低延迟、高保真度的个性化全息显示，使静态展品变为动态互动艺术品。

Conclusion: Holo-Artisan开创了文化遗产互动的新范式，通过技术增强用户体验。

Abstract: We present Holo-Artisan, a novel system architecture enabling immersive
multi-user experiences in virtual museums through true holographic displays and
personalized edge intelligence. In our design, local edge computing nodes
process real-time user data -- including pose, facial expression, and voice --
for multiple visitors concurrently. Generative AI models then drive digital
artworks (e.g., a volumetric Mona Lisa) to respond uniquely to each viewer. For
instance, the Mona Lisa can return a smile to one visitor while engaging in a
spoken Q\&A with another, all in real time. A cloud-assisted collaboration
platform composes these interactions in a shared scene using a universal scene
description, and employs ray tracing to render high-fidelity, personalized
views with a direct pipeline to glasses-free holographic displays. To preserve
user privacy and continuously improve personalization, we integrate federated
learning (FL) -- edge devices locally fine-tune AI models and share only model
updates for aggregation. This edge-centric approach minimizes latency and
bandwidth usage, ensuring a synchronized shared experience with individual
customization. Through Holo-Artisan, static museum exhibits are transformed
into dynamic, living artworks that engage each visitor in a personal dialogue,
heralding a new paradigm of cultural heritage interaction.

</details>


### [27] [\textit{adder-viz}: Real-Time Visualization Software for Transcoding Event Video](https://arxiv.org/abs/2508.14996)
*Andrew C. Freeman,Luke Reinkensmeyer*

Main category: cs.MM

TL;DR: 本文介绍了对事件视频处理软件adder-viz的改进，旨在提升实时事件转码的可视化效果和应用性能。


<details>
  <summary>Details</summary>
Motivation: 现有事件视频表示在灵活性、速度和可压缩性方面存在局限，ADΔER表示法试图解决这些问题。

Method: 通过改进adder-viz软件，实现实时事件转码过程的可视化和应用测试。

Result: 改进后的软件已开源，并可从集中化代码库获取。

Conclusion: 改进的adder-viz软件为事件视频研究提供了更高效的工具，支持实际应用开发。

Abstract: Recent years have brought about a surge in neuromorphic ``event'' video
research, primarily targeting computer vision applications. Event video eschews
video frames in favor of asynchronous, per-pixel intensity samples. While much
work has focused on a handful of representations for specific event cameras,
these representations have shown limitations in flexibility, speed, and
compressibility. We previously proposed the unified AD{\Delta}ER representation
to address these concerns. This paper introduces numerous improvements to the
\textit{adder-viz} software for visualizing real-time event transcode processes
and applications in-the-loop. The MIT-licensed software is available from a
centralized repository at
\href{https://github.com/ac-freeman/adder-codec-rs}{https://github.com/ac-freeman/adder-codec-rs}.

</details>


### [28] [A Low-Latency 3D Live Remote Visualization System for Tourist Sites Integrating Dynamic and Pre-captured Static Point Clouds](https://arxiv.org/abs/2508.15398)
*Takahiro Matsumoto,Masafumi Suzuki,Mariko Yamaguchi,Masakatsu Aoki,Shunsuke Konagai,Kazuhiko Murasaki*

Main category: cs.MM

TL;DR: 提出了一种结合多LiDAR和相机的实时动态点云捕获系统，适用于户外旅游景点，解决了现有技术在复杂光照和传感器放置上的限制。


<details>
  <summary>Details</summary>
Motivation: 针对户外旅游景点因维护和美观限制以及光照变化导致现有3D捕获技术难以应用的问题。

Method: 结合多LiDAR和相机进行实时动态点云捕获，并与预捕获的静态点云整合，通过自动调整静态点云颜色以适应光照变化。

Result: 系统能在100毫秒延迟内保持30 fps的宽场景实时渲染，并在实际旅游景点部署中验证了其有效性。

Conclusion: 系统成功解决了户外复杂环境下的实时3D捕获难题，适合旅游景点的实际应用。

Abstract: Various real-time methods for capturing and transmitting dynamic 3D spaces
have been proposed, including those based on RGB-D cameras and volumetric
capture. However, applying existing methods to outdoor tourist sites remains
difficult because maintenance and aesthetic constraints limit sensor placement,
and daylight variability complicates processing. We propose a system that
combines multiple LiDARs and cameras for live dynamic point cloud capture, and
integrates them with pre-captured static point clouds for wide-area 3D
visualization. The system sustains 30 fps across wide-area scenes while keeping
latency below 100 ms. To mitigate lighting inconsistencies, static point-cloud
colors are automatically adjusted to current lighting. The effectiveness of our
system is demonstrated through real-world deployment in a tourist site.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [29] [LitForager: Exploring Multimodal Literature Foraging Strategies in Immersive Sensemaking](https://arxiv.org/abs/2508.15043)
*Haoyang Yang,Elliott H. Faa,Weijian Liu,Shunan Guo,Duen Horng Chau,Yalong Yang*

Main category: cs.HC

TL;DR: LitForager是一个沉浸式文献探索工具，旨在通过网络可视化和多模态交互提高文献检索效率。


<details>
  <summary>Details</summary>
Motivation: 研究者面对文献快速增长的挑战，需要工具辅助发现和整理相关文献，但现有工具偏重信息整合，忽视了文献检索阶段。

Method: 开发了LitForager，使用WebXR技术，支持3D文献网络的空间组织和多模态交互。

Result: 15位研究者参与的观察性用户研究表明，该工具能有效支持流畅的文献检索策略和空间感知。

Conclusion: LitForager填补了沉浸式环境中文献检索工具的空白，有助于完整的信息感知工作流程。

Abstract: Exploring and comprehending relevant academic literature is a vital yet
challenging task for researchers, especially given the rapid expansion in
research publications. This task fundamentally involves sensemaking -
interpreting complex, scattered information sources to build understanding.
While emerging immersive analytics tools have shown cognitive benefits like
enhanced spatial memory and reduced mental load, they predominantly focus on
information synthesis (e.g., organizing known documents). In contrast, the
equally important information foraging phase - discovering and gathering
relevant literature - remains underexplored within immersive environments,
hindering a complete sensemaking workflow. To bridge this gap, we introduce
LitForager, an interactive literature exploration tool designed to facilitate
information foraging of research literature within an immersive sensemaking
workflow using network-based visualizations and multimodal interactions.
Developed with WebXR and informed by a formative study with researchers,
LitForager supports exploration guidance, spatial organization, and seamless
transition through a 3D literature network. An observational user study with 15
researchers demonstrated LitForager's effectiveness in supporting fluid
foraging strategies and spatial sensemaking through its multimodal interface.

</details>


### [30] [Understanding Accessibility Needs of Blind Authors on CMS-Based Websites](https://arxiv.org/abs/2508.15045)
*Guillermo Vera-Amaro,José Rafael Rojano-Cáceres*

Main category: cs.HC

TL;DR: 本文探讨了盲人用户在CMS中作为内容创作者面临的可访问性问题，强调自动化工具的局限性及用户中心设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究盲人用户在使用CMS时的可访问性障碍，填补现有研究中盲人作为内容创作者角色的不足。

Method: 采用自动化工具和手动可用性测试（参与者为3名盲人和1名视力正常者），并基于Barrier Walkthrough方法进行专家分析。

Result: 结果显示块状编辑器存在严重可用性问题，而文本编辑器、AI生成图像描述及针对性训练显著提高了盲人用户的自主性。

Conclusion: 提升CMS可访问性需减少视觉依赖，采用用户中心设计，并整合AI工具支持盲人内容创作。

Abstract: This paper addresses the limited attention given to blind users as content
creators in Content Management Systems (CMS), a gap that remains under-explored
in web accessibility research. For blind authors, effective interaction with
CMS platforms requires more than technical compliance; it demands interfaces
designed with semantic clarity, predictable navigation, and meaningful feedback
for screen reader users. This study investigates the accessibility barriers
blind users face when performing key tasks, such as page creation, menu
editing, and image publishing, using CMS platforms. A two-fold evaluation was
conducted using automated tools and manual usability testing with three blind
and one sighted participant, complemented by expert analysis based on the
Barrier Walkthrough method. Results showed that block-based interfaces were
particularly challenging, often marked as accessible by automated tools but
resulting in critical usability issues during manual evaluation. The use of a
text-based editor, the integration of AI-generated image descriptions, and
training aligned with screen reader workflows, significantly improved usability
and autonomy. These findings underscore the limitations of automated
assessments and highlight the importance of user-centered design practices.
Enhancing CMS accessibility requires consistent navigation structures, reduced
reliance on visual interaction patterns, and the integration of AI tools that
support blind content authors throughout the content creation process.

</details>


### [31] [QueryGenie: Making LLM-Based Database Querying Transparent and Controllable](https://arxiv.org/abs/2508.15146)
*Longfei Chen,Shenghan Gao,Shiwei Wang,Ken Lin,Yun Wang,Quan Li*

Main category: cs.HC

TL;DR: QueryGenie是一种交互式系统，旨在通过增量推理、实时验证和响应式互动机制，解决LLM驱动数据库查询中的用户意图误解、幻觉内容生成和缺乏反馈机制等问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的数据库查询工具存在用户意图误解、幻觉内容生成和缺乏反馈机制等问题，影响了其可靠性和实用性。

Method: 提出QueryGenie系统，采用增量推理、实时验证和响应式互动机制，使用户能够监控、理解和指导查询生成过程。

Result: 用户可以通过迭代优化查询逻辑，确保其与意图一致，从而提升查询体验。

Conclusion: QueryGenie通过透明和可控的交互设计，显著改善了LLM驱动数据库查询的可靠性和实用性。

Abstract: Conversational user interfaces powered by large language models (LLMs) have
significantly lowered the technical barriers to database querying. However,
existing tools still encounter several challenges, such as misinterpretation of
user intent, generation of hallucinated content, and the absence of effective
mechanisms for human feedback-all of which undermine their reliability and
practical utility. To address these issues and promote a more transparent and
controllable querying experience, we proposed QueryGenie, an interactive system
that enables users to monitor, understand, and guide the LLM-driven query
generation process. Through incremental reasoning, real-time validation, and
responsive interaction mechanisms, users can iteratively refine query logic and
ensure alignment with their intent.

</details>


### [32] [ReviseMate: Exploring Contextual Support for Digesting STEM Paper Reviews](https://arxiv.org/abs/2508.15148)
*Yuansong Xu,Shuhao Zhang,Yijie Fan,Shaohan Shi,Zhenhui Peng,Quan Li*

Main category: cs.HC

TL;DR: 研究提出ReviseMate系统，帮助研究者更高效地消化和整合审稿人反馈，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的审稿反馈消化过程耗时且依赖分析能力，缺乏针对性工具。

Method: 通过访谈和故事板开发交互系统ReviseMate，并进行用户实验和现场部署。

Result: ReviseMate在实验中优于基线方法，现场部署效果良好。

Conclusion: 交互工具能显著提升审稿反馈的消化和整合效率。

Abstract: Effectively assimilating and integrating reviewer feedback is crucial for
researchers seeking to refine their papers and handle potential rebuttal phases
in academic venues. However, traditional review digestion processes present
challenges such as time consumption, reading fatigue, and the requisite for
comprehensive analytical skills. Prior research on review analysis often
provides theoretical guidance with limited targeted support. Additionally,
general text comprehension tools overlook the intricate nature of
comprehensively understanding reviews and lack contextual assistance. To bridge
this gap, we formulated research questions to explore the authors' concerns and
methods for enhancing comprehension during the review digestion phase. Through
interviews and the creation of storyboards, we developed ReviseMate, an
interactive system designed to address the identified challenges. A controlled
user study (N=31) demonstrated the superiority of ReviseMate over baseline
methods, with positive feedback regarding user interaction. Subsequent field
deployment (N=6) further validated the effectiveness of ReviseMate in
real-world review digestion scenarios. These findings underscore the potential
of interactive tools to significantly enhance the assimilation and integration
of reviewer feedback during the manuscript review process.

</details>


### [33] [Evaluating an Immersive Analytics Application at an Enterprise Business Intelligence Customer Conference](https://arxiv.org/abs/2508.15152)
*Matthew Brehmer,Ginger Gloystein,Bailiang Zhou,Abby Gray,Sruthi Pillai,Ben Medina,Vidya Setlur*

Main category: cs.HC

TL;DR: 本文反思了在企业BI会议上对Tableau for visionOS的沉浸式分析应用进行的评估，探讨了在实用性与用户体验之间平衡的挑战，并提出了新的评估方法建议。


<details>
  <summary>Details</summary>
Motivation: 评估Tableau for visionOS在实际企业环境中的可用性和潜在效用，并探索头戴式显示器（HMD）在BI数据交互中的新可能性。

Method: 通过22名参与者的形成性评估，收集了对应用可用性和HMD潜力的反馈，结合定性与定量方法。

Result: 发现需要在评估中综合考虑3D表示和HMD交互模式的独特需求，提出了新的评估考量。

Conclusion: 本文为企业视角下的沉浸式分析评估方法提供了贡献，强调需整合定性与定量指标以适应HMD交互。

Abstract: We reflect on an evaluation of an immersive analytics application (Tableau
for visionOS) conducted at a large enterprise business intelligence (BI)
conference. Conducting a study in such a context offered an opportunistic
setting to gather diverse feedback. However, this setting also highlighted the
challenge of evaluating usability while also assessing potential utility, as
feedback straddled between the novelty of the experience and the practicality
of the application in participants' analytical workflows. This formative
evaluation with 22 participants allowed us to gather insights with respect to
the usability of Tableau for visionOS, along with broader perspectives on the
potential for head-mounted displays (HMDs) to promote new ways to engage with
BI data. Our experience suggests a need for new evaluation considerations that
integrate qualitative and quantitative measures and account for unique
interaction patterns with 3D representations and interfaces accessible via an
HMD. Overall, we contribute an enterprise perspective on evaluation
methodologies for immersive analytics.

</details>


### [34] [GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design](https://arxiv.org/abs/2508.15227)
*Wen-Fan Wang,Ting-Ying Lee,Chien-Ting Lu,Che-Wei Hsu,Nil Ponsa Campany,Yu Chen,Mike Y. Chen,Bing-Yu Chen*

Main category: cs.HC

TL;DR: 论文介绍了一种名为GenTune的系统，帮助设计师更好地理解和控制生成式AI生成的图像内容，解决了当前方法中存在的提示过长、局部与全局调整困难的问题。


<details>
  <summary>Details</summary>
Motivation: 设计师在使用生成式AI时面临两大挑战：一是AI生成的提示过长难以理解和修改，二是局部编辑可能破坏全局一致性。

Method: GenTune通过允许设计师选择图像元素并追溯至提示标签，支持精准且全局一致的图像调整。

Result: 实验表明，GenTune显著提升了提示—图像理解、调整质量和效率，以及用户满意度（所有p值小于0.01）。

Conclusion: GenTune有效地优化了人类与AI的协作，适用于实际设计工作场景。

Abstract: Environment designers in the entertainment industry create imaginative 2D and
3D scenes for games, films, and television, requiring both fine-grained control
of specific details and consistent global coherence. Designers have
increasingly integrated generative AI into their workflows, often relying on
large language models (LLMs) to expand user prompts for text-to-image
generation, then iteratively refining those prompts and applying inpainting.
However, our formative study with 10 designers surfaced two key challenges: (1)
the lengthy LLM-generated prompts make it difficult to understand and isolate
the keywords that must be revised for specific visual elements; and (2) while
inpainting supports localized edits, it can struggle with global consistency
and correctness. Based on these insights, we present GenTune, an approach that
enhances human--AI collaboration by clarifying how AI-generated prompts map to
image content. Our GenTune system lets designers select any element in a
generated image, trace it back to the corresponding prompt labels, and revise
those labels to guide precise yet globally consistent image refinement. In a
summative study with 20 designers, GenTune significantly improved prompt--image
comprehension, refinement quality, and efficiency, and overall satisfaction
(all $p < .01$) compared to current practice. A follow-up field study with two
studios further demonstrated its effectiveness in real-world settings.

</details>


### [35] [Visualization on Smart Wristbands: Results from an In-situ Design Workshop with Four Scenarios](https://arxiv.org/abs/2508.15249)
*Alaul Islam,Fairouz Grioui,Raimund Dachselt,Petra Isenberg*

Main category: cs.HC

TL;DR: 研究通过工作坊探索智能腕带数据可视化的设计，重点关注不同手臂姿势下的显示适应性。


<details>
  <summary>Details</summary>
Motivation: 智能腕带在不同姿势下的可视性问题尚未解决，研究旨在探索其设计方法。

Method: 采用纸面构思工作坊，基于四种使用场景（办公、散步、骑行、驾驶）分析数据布局和可视化设计。

Result: 参与者偏好能适应手臂运动的响应式可视化设计。

Conclusion: 研究为未来智能腕带的可视化设计提供了初步指导。

Abstract: We present the results of an in-situ ideation workshop for designing data
visualizations on smart wristbands that can show data around the entire wrist
of a wearer. Wristbands pose interesting challenges because the visibility of
different areas of the band depends on the wearer's arm posture. We focused on
four usage scenarios that lead to different postures: office work, leisurely
walks, cycling, and driving. As the technology for smart wristbands is not yet
commercially available, we conducted a paper-based ideation exercise that
showed how spatial layout and visualization design on smart wristbands may need
to vary depending on the types of data items of interest and arm postures.
Participants expressed a strong preference for responsive visualization designs
that could adapt to the movement of wearers' arms. Supplemental material from
the study is available here: https://osf.io/4hrca/.

</details>


### [36] [Spatio-Temporal Mixed and Augmented Reality Experience Description for Interactive Playback](https://arxiv.org/abs/2508.15258)
*Dooyoung Kim,Woontack Woo*

Main category: cs.HC

TL;DR: MAR-ED框架通过事件、关键帧和回放三个核心原语，实现了过去事件在用户当前物理空间中的交互式和自适应回放，为训练、文化遗产和交互式叙事提供了新范式。


<details>
  <summary>Details</summary>
Motivation: 当前空间媒体技术主要关注静态内容的捕捉或回放，缺乏与用户环境的连接和交互性，MAR-ED旨在标准化描述体验的语义和交互结构。

Method: MAR-ED基于三个核心原语：事件原语（语义场景图表示）、关键帧原语（高效数据访问）和回放原语（用户驱动的自适应回放），通过三阶段流程将记录的经验转化为自适应体验。

Result: MAR-ED框架能够动态调整空间-时间结构以适应新环境，并通过用户输入改变叙事，实现沉浸式群体体验。

Conclusion: MAR-ED为数字记忆和记录事件提供了超越被动视频的沉浸式体验，为多个领域开辟了新可能性。

Abstract: We propose the Spatio-Temporal Mixed and Augmented Reality Experience
Description (MAR-ED), a novel framework to standardize the representation of
past events for interactive and adaptive playback in a user's present physical
space. While current spatial media technologies have primarily focused on
capturing or replaying content as static assets, often disconnected from the
viewer's environment or offering limited interactivity, the means to describe
an experience's underlying semantic and interactive structure remains
underexplored. We propose a descriptive framework called MAR-ED based on three
core primitives: 1) Event Primitives for semantic scene graph representation,
2) Keyframe Primitives for efficient and meaningful data access, and 3)
Playback Primitives for user-driven adaptive interactive playback of recorded
MAR experience. The proposed flowchart of the three-stage process of the
proposed MAR-ED framework transforms a recorded experience into a unique
adaptive MAR experience during playback, where its spatio-temporal structure
dynamically conforms to a new environment and its narrative can be altered by
live user input. Drawing on this framework, personal digital memories and
recorded events can evolve beyond passive 2D/3D videos into immersive,
spatially-integrated group experiences, opening new paradigms for training,
cultural heritage, and interactive storytelling without requiring complex,
per-user adaptive rendering.

</details>


### [37] [Foundation Models for Cross-Domain EEG Analysis Application: A Survey](https://arxiv.org/abs/2508.15716)
*Hongqi Li,Yitong Chen,Yujuan Wang,Weihang Ni,Haodong Zhang*

Main category: cs.HC

TL;DR: 本文提出了首个全面的面向模态的EEG分析基础模型分类法，系统整理了基于EEG解码、EEG-文本、EEG-视觉、EEG-音频和多模态框架的研究进展，并分析了每类的研究思路、理论基础和架构创新。


<details>
  <summary>Details</summary>
Motivation: EEG分析的基础模型技术快速发展，但研究领域分散，缺乏系统分类，作者旨在填补这一空白。

Method: 通过模态分类法（EEG解码、EEG-文本、EEG-视觉、EEG-音频和多模态框架），系统分析每类的研究进展、理论及架构创新。

Result: 提供了一种参考框架，加速EEG基础模型转化为可扩展、可解释和在线可操作的解决方案。

Conclusion: 该分类法不仅为未来方法开发提供参考，还推动了EEG基础模型在实际应用中的发展。

Abstract: Electroencephalography (EEG) analysis stands at the forefront of neuroscience
and artificial intelligence research, where foundation models are reshaping the
traditional EEG analysis paradigm by leveraging their powerful representational
capacity and cross-modal generalization. However, the rapid proliferation of
these techniques has led to a fragmented research landscape, characterized by
diverse model roles, inconsistent architectures, and a lack of systematic
categorization. To bridge this gap, this study presents the first comprehensive
modality-oriented taxonomy for foundation models in EEG analysis,
systematically organizing research advances based on output modalities of the
native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal
frameworks. We rigorously analyze each category's research ideas, theoretical
foundations, and architectural innovations, while highlighting open challenges
such as model interpretability, cross-domain generalization, and real-world
applicability in EEG-based systems. By unifying this dispersed field, our work
not only provides a reference framework for future methodology development but
accelerates the translation of EEG foundation models into scalable,
interpretable, and online actionable solutions.

</details>


### [38] [Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI](https://arxiv.org/abs/2508.15727)
*Hannah Selder,Florian Fischer,Per Ola Kristensson,Arthur Fleig*

Main category: cs.HC

TL;DR: 论文通过系统分析奖励函数设计中的努力最小化、任务完成奖励和目标接近奖励对HCI任务的影响，提供了实用的设计指南。


<details>
  <summary>Details</summary>
Motivation: 解决HCI中因奖励函数设计不当导致的低效计算问题，提升生物力学模拟的效率和实用性。

Method: 分析了努力最小化、任务完成奖励和目标接近奖励在指向、跟踪和选择反应任务中的作用，并通过权重敏感性分析推导出设计原则。

Result: 研究发现接近奖励对引导运动至关重要，完成奖励确保任务成功，努力项在适当缩放时能优化运动规律。

Conclusion: 论文提出的指南简化了生物力学模拟的设计，无需强化学习专业知识，适用于实际界面开发。

Abstract: Designing effective reward functions is critical for reinforcement
learning-based biomechanical simulations, yet HCI researchers and practitioners
often waste (computation) time with unintuitive trial-and-error tuning. This
paper demystifies reward function design by systematically analyzing the impact
of effort minimization, task completion bonuses, and target proximity
incentives on typical HCI tasks such as pointing, tracking, and choice
reaction. We show that proximity incentives are essential for guiding movement,
while completion bonuses ensure task success. Effort terms, though optional,
help refine motion regularity when appropriately scaled. We perform an
extensive analysis of how sensitive task success and completion time depend on
the weights of these three reward components. From these results we derive
practical guidelines to create plausible biomechanical simulations without the
need for reinforcement learning expertise, which we then validate on remote
control and keyboard typing tasks. This paper advances simulation-based
interaction design and evaluation in HCI by improving the efficiency and
applicability of biomechanical user modeling for real-world interface
development.

</details>


### [39] ["Does the cafe entrance look accessible? Where is the door?" Towards Geospatial AI Agents for Visual Inquiries](https://arxiv.org/abs/2508.15752)
*Jon E. Froehlich,Jared Hwang,Zeyu Wang,John S. O'Meara,Xia Su,William Huang,Yang Zhang,Alex Fiannaca,Philip Nelson,Shaun Kane*

Main category: cs.HC

TL;DR: 提出Geo-Visual Agents，利用多模态AI分析地理空间图像和GIS数据，解决传统地图无法回答的视觉空间问题。


<details>
  <summary>Details</summary>
Motivation: 现有交互式数字地图依赖预结构化GIS数据，无法满足用户对世界外观的视觉空间问题需求。

Method: 结合街道景观、地点照片、航拍图像与传统GIS数据，定义Geo-Visual Agents的传感和交互方法。

Result: 展示了三个范例，验证了Geo-Visual Agents的潜力。

Conclusion: 提出了未来研究的关键挑战和机会，推动地理视觉智能的发展。

Abstract: Interactive digital maps have revolutionized how people travel and learn
about the world; however, they rely on pre-existing structured data in GIS
databases (e.g., road networks, POI indices), limiting their ability to address
geo-visual questions related to what the world looks like. We introduce our
vision for Geo-Visual Agents--multimodal AI agents capable of understanding and
responding to nuanced visual-spatial inquiries about the world by analyzing
large-scale repositories of geospatial images, including streetscapes (e.g.,
Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial
imagery (e.g., satellite photos) combined with traditional GIS data sources. We
define our vision, describe sensing and interaction approaches, provide three
exemplars, and enumerate key challenges and opportunities for future work.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [40] [Hybrelighter: Combining Deep Anisotropic Diffusion and Scene Reconstruction for On-device Real-time Relighting in Mixed Reality](https://arxiv.org/abs/2508.14930)
*Hanwen Zhao,John Akers,Baback Elmieh,Ira Kemelmacher-Shlizerman*

Main category: cs.GR

TL;DR: 提出一种新型混合现实场景重光照方法，结合图像分割与光照传播技术，解决现有深度学习和滤镜方法的性能与精度问题。


<details>
  <summary>Details</summary>
Motivation: 混合现实中实时准确重光照的需求，现有技术或速度不足或精度不够。

Method: 整合图像分割、各向异性扩散的光照传播及基础场景理解，优化设备扫描误差。

Result: 实时边缘设备上实现100 fps的高效重光照，效果视觉上吸引且准确。

Conclusion: 新方法在性能与精度上优于行业标准，适用于如房地产等实际应用。

Abstract: Mixed Reality scene relighting, where virtual changes to lighting conditions
realistically interact with physical objects, producing authentic illumination
and shadows, can be used in a variety of applications. One such application in
real estate could be visualizing a room at different times of day and placing
virtual light fixtures. Existing deep learning-based relighting techniques
typically exceed the real-time performance capabilities of current MR devices.
On the other hand, scene understanding methods, such as on-device scene
reconstruction, often yield inaccurate results due to scanning limitations, in
turn affecting relighting quality. Finally, simpler 2D image filter-based
approaches cannot represent complex geometry and shadows. We introduce a novel
method to integrate image segmentation, with lighting propagation via
anisotropic diffusion on top of basic scene understanding, and the
computational simplicity of filter-based techniques. Our approach corrects
on-device scanning inaccuracies, delivering visually appealing and accurate
relighting effects in real-time on edge devices, achieving speeds as high as
100 fps. We show a direct comparison between our method and the industry
standard, and present a practical demonstration of our method in the
aforementioned real estate example.

</details>


### [41] [Inference Time Debiasing Concepts in Diffusion Models](https://arxiv.org/abs/2508.14933)
*Lucas S. Kupssinskü,Marco N. Bochernitsan,Jordan Kopper,Otávio Parraga,Rodrigo C. Barros*

Main category: cs.GR

TL;DR: 提出了DeCoDi方法，通过改变推理过程减少文本到图像扩散模型的偏见，无需显著改变图像质量或增加计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型中存在的性别、种族和年龄偏见问题，提供一种轻量级的去偏方法。

Method: 调整扩散过程，避免潜在维度中的偏见概念区域，仅需改变推理过程。

Result: 人工评估表明方法在性别、种族和年龄去偏上有效，与GPT4o的自动评估结果一致。

Conclusion: DeCoDi方法能显著提升生成图像的多样性，适合广泛使用。

Abstract: We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based
models that changes the inference procedure, does not significantly change
image quality, has negligible compute overhead, and can be applied in any
diffusion-based image generation model. DeCoDi changes the diffusion process to
avoid latent dimension regions of biased concepts. While most deep learning
debiasing methods require complex or compute-intensive interventions, our
method is designed to change only the inference procedure. Therefore, it is
more accessible to a wide range of practitioners. We show the effectiveness of
the method by debiasing for gender, ethnicity, and age for the concepts of
nurse, firefighter, and CEO. Two distinct human evaluators manually inspect
1,200 generated images. Their evaluation results provide evidence that our
method is effective in mitigating biases based on gender, ethnicity, and age.
We also show that an automatic bias evaluation performed by the GPT4o is not
significantly statistically distinct from a human evaluation. Our evaluation
shows promising results, with reliable levels of agreement between evaluators
and more coverage of protected attributes. Our method has the potential to
significantly improve the diversity of images it generates by diffusion-based
text-to-image generative models.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [42] [Money in Motion: Micro-Velocity and Usage of Ethereums Liquid Staking Tokens](https://arxiv.org/abs/2508.15391)
*Benjamin Kraner,Luca Pennella,Nicolò Vallarano,Claudio J. Tessone*

Main category: cs.ET

TL;DR: 该论文提出了一个微速度框架，用于分析Lido流动性质押代币stETH及其封装形式wstETH的链上流通情况，揭示了高速度和集中化的用户行为，并提供了开源工具和数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管流动性质押代币（LSTs）越来越重要，但其微观货币动态仍未被充分研究。论文旨在填补这一空白，提供对stETH和wstETH流通行为的深入理解。

Method: 通过重建完整的转账和份额会计历史，计算地址级速度，并将其分解为行为组成部分。同时开源了索引事件日志和历史合约状态的管道，并发布了两个公开数据集。

Result: 研究发现两种代币的流通速度持续较高，且活动高度集中化：少数大地址（可能是机构账户）主导了大部分流通，而普通用户较为被动。此外，用户行为逐渐转向非变基形式的wstETH。

Conclusion: 该研究首次大规模实证描述了流动性质押代币的流通特征，为监控质押资产流动提供了可扩展的模板，并向研究社区提供了新的开放资源。

Abstract: We introduce a micro-velocity framework for analysing the on-chain
circulation of Lidos liquid-staking tokens, stETH, and its wrapped ERC-20 form,
wstETH. By reconstructing full transfer and share-based accounting histories,
we compute address-level velocities and decompose them into behavioural
components. Despite their growing importance, the micro-level monetary dynamics
of LSTs remain largely unexplored. Our data reveal persistently high velocity
for both tokens, reflecting intensive reuse within DeFi. Yet activity is highly
concentrated: a small cohort of large addresses, likely institutional accounts,
are responsible for most turnover, while the rest of the users remain largely
passive. We also observe a gradual transition in user behavior, characterized
by a shift toward wstETH, the non-rebasing variant of stETH. This shift appears
to align with DeFi composability trends, as wstETH is more frequently deployed
across protocols such as AAVE, Spark, Balancer, and SkyMoney.
  To make the study fully reproducible, we release (i) an open-source pipeline
that indexes event logs and historical contract state, and (ii) two public
datasets containing every Transfer and TransferShares record for stETH and
wstETH through 2024-11-08. This is the first large-scale empirical
characterisation of liquid-staking token circulation. Our approach offers a
scalable template for monitoring staking asset flows and provides new,
open-access resources to the research community.

</details>


### [43] [Distributed Shared Layered Storage Quantum Simulator: A novel quantum simulation system for efficient scaling and cost optimization](https://arxiv.org/abs/2508.15542)
*Mingyang Yu,Haorui Yang,Donglin Wang,Desheng Kong,Ji Du,Yulong Fu,Wei Wang,Jing Xu*

Main category: cs.ET

TL;DR: 本文提出了一种新型分布式共享分层存储量子模拟器（DSLSQS），通过创新架构和去TCP/IP网络技术解决了量子模拟器的单节点瓶颈问题，显著提高了效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式技术无法满足量子模拟器高频率遍历特性导致的需求，造成单节点瓶颈。

Method: 引入分布式共享数据存储架构和De-TCP/IP网络技术，采用分层存储以减少高性能内存的使用。

Result: 实验证明DSLSQS性能提升350%以上，并显著降低了模拟成本。

Conclusion: DSLSQS为量子计算的实践部署提供了关键见解，并为分布式量子模拟集群的开发提供了有效框架。

Abstract: Quantum simulators are essential tools for developing and testing quantum
algorithms. However, the high-frequency traversal characteristic of quantum
simulators represents an unprecedented demand in the history of IT, and
existing distributed technologies is unable to meet this requirement, resulting
in a single-node bottleneck of quantum simulator. To overcome this limitation,
this paper introduces a novel Distributed Shared Layered Storage Quantum
Simulator (DSLSQS). By leveraging an innovative distributed architecture in
which multiple computational nodes share data storage directly, together with
De-TCP/IP networking technology, DSLSQS effectively eliminates East-West data
flow in distributed systems. This approach mitigates the bottleneck of
distributed quantum simulation clusters and enhances the scalability. Moreover,
the system employs layered storage technology, which reduces usage of expensive
high-performance memory and substantially lowers simulation costs. Furthermore,
this paper systematically analyzes the performance and cost constraints of
distributed quantum simulator cluster, identifying distributed networking as
the primary performance bottleneck and highlighting that minimizing storage
costs is crucial to reducing the total cost. Finally, experimental evaluations
with a 27-qubit simulation confirm the successful implementation of layered
storage within the quantum simulator. DSLSQS significantly enhances simulation
efficiency, yielding a performance improvement of over 350% compared to
existing distributed technologies. These results underscore the superior
performance and scalability of the proposed architecture in managing complex
quantum computing tasks. This paper provides crucial insights for the practical
deployment of quantum computing and presents an effective framework for the
development of distributed quantum simulation clusters.

</details>


### [44] [QVecOpt: An Efficient Storage and Computing Opti-mization Framework for Large-scale Quantum State Simulation](https://arxiv.org/abs/2508.15545)
*Mingyang Yu,Haorui Yang,Donglin Wang,Desheng Kong,Ji Du,Yulong Fu,Jing Xu*

Main category: cs.ET

TL;DR: 针对大规模量子态模拟中的内存限制、频繁磁盘I/O和高计算复杂度问题，本研究提出了量子向量优化框架QVecOpt，通过四项优化策略显著提升了模拟效率。


<details>
  <summary>Details</summary>
Motivation: 解决经典计算平台上大规模量子态模拟的内存瓶颈和计算复杂度问题，提升模拟效率。

Method: 提出QVecOpt框架，整合振幅配对、缓存优化、块存储优化和并行优化四项策略，优化状态向量存储和计算调度。

Result: 将单量子比特门的遍历复杂度从$O(2^n)$降至$O(1)$，模拟16-29量子比特时效率提升近十倍，突破内存瓶颈。

Conclusion: QVecOpt为大规模量子计算经典模拟提供高效、可扩展的解决方案，具有重要学术和实用价值。

Abstract: In response to the challenges in large-scale quantum state simulation on
classical computing platforms, including memory limits, frequent disk I/O, and
high computational complexity, this study builds upon a previously proposed
hierarchical storage-based quantum simulation system and introduces an
optimization framework, the Quantum Vector Optimization Framework (QVecOpt).
QVecOpt integrates four strategies: amplitude pairing, cache optimization,
block storage optimization, and parallel optimization. These collectively
enhance state vector storage and computational scheduling. The amplitude
pairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing
traversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache
optimization pre-allocates buffers and loads only required data, cutting disk
I/O. Block storage optimization partitions the state vector for on-demand
loading and local updates, reducing redundant access. Parallel optimization
distributes the state vector across nodes for collaborative computation,
achieving near-linear speedup. Complexity analysis shows that, compared with
hierarchical storage simulation, the method reduces state vector traversals for
single-qubit gates from $2^n$ to 1, removing the main bottleneck. It also
lowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and
$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,
breaking the memory bottleneck of existing tools and enabling high-bit quantum
circuit simulations beyond traditional methods. This work provides an
efficient, scalable solution for classical simulation of large-scale quantum
computation with significant academic and practical value.

</details>


### [45] [Low-Power Control of Resistance Switching Transitions in First-Order Memristors](https://arxiv.org/abs/2508.15620)
*Valeriy A. Slipko,Alon Ascoli,Fernando Corinto,Yuriy V. Pershin*

Main category: cs.ET

TL;DR: 该研究提出了一种优化一阶忆阻器件低功耗控制的方法，推导了最节能的电阻编程协议，并通过两种电压控制器件的模型展示了不同场景下的最优电压刺激策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了解决忆阻器件编程过程中的电压-时间困境，开发出更节能的编程协议，这在学术界和工业界具有广泛的研究意义。

Method: 采用了一种通用且独特的方法，优化忆阻器件的开关转换，并通过两种数学模型（Kvatinsky和Miranda-Sune模型）验证了最优电压刺激策略。

Result: 研究结果表明，最优编程协议可能包括单一方波电压脉冲或复杂的连续波形，具体取决于器件的物理特性和编程约束。

Conclusion: 该研究为忆阻器件的节能编程提供了重要指导，解决了电压-时间困境，具有重要的实际应用价值。

Abstract: In many cases, the behavior of physical memristive devices can be relatively
well captured by using a single internal state variable. This study
investigates the low-power control of first-order memristive devices to derive
the most energy-efficient protocols for programming their resistances. A unique
yet general approach to optimizing the switching transitions in devices of this
kind is introduced. For pedagogical purposes, without loss of generality, the
proposed control paradigm is applied to a couple of differential algebraic
equation sets for voltage-controlled devices, specifically Kvatinsky's Voltage
ThrEshold Adaptive Memristor mathematical description and Miranda's and Sune's
dynamic balance model. It is demonstrated that, depending upon intrinsic
physical properties of the device, captured in the model formulas and parameter
setting, and upon constraints on programming time and voltages, the optimal
protocol for either of the two switching scenarios may require the application
of a single square voltage pulse of height set to a certain level within the
admissible range across a fraction or entire given programming time interval,
or of some more involved voltage stimulus of unique polarity, including
analogue continuous waveforms that can be approximated by trains of square
voltage pulses of different heights, over the entire programming time interval.
The practical implications of these research findings are significant, as the
development of energy-efficient protocols to program memristive devices,
resolving the so-called voltage-time dilemma in the device physics community,
is a subject under intensive and extensive studies across the academic
community and industry.

</details>


### [46] [Exploration of Evolving Quantum Key Distribution Network Architecture Using Model-Based Systems Engineering](https://arxiv.org/abs/2508.15733)
*Hayato Ishida,Amal Elsokary,Maria Aslam,Catherine White,Michael J. de C. Henshaw,Siyuan Ji*

Main category: cs.ET

TL;DR: 本文探讨利用系统工程方法解决量子安全通信的需求，通过建模量子密钥分发网络架构，提出一个驱动可变性框架，促进模块化架构的复用。


<details>
  <summary>Details</summary>
Motivation: 量子技术的传感器、计算、定时和通信能力进步依赖于将量子设备集成到现有经典基础设施的复杂系统工程，以满足量子计算对加密威胁的需求。

Method: 利用正交可变性建模和系统建模语言，对现有和未来的量子通信网络（尤其是量子密钥分发网络）进行建模，创建可追溯的模块化架构。

Result: 提出了一个驱动可变性框架，支持快速演进的网络架构管理，并促进量子密钥分发网络的系统化开发。

Conclusion: 研究结果为量子密钥分发网络的可行开发提供了支持，并对量子系统工程的更广泛集成挑战提供了借鉴。

Abstract: Realisation of significant advances in capabilities of sensors, computing,
timing, and communication enabled by quantum technologies is dependent on
engineering highly complex systems that integrate quantum devices into existing
classical infrastructure. A systems engineering approach is considered to
address the growing need for quantum-secure telecommunications that overcome
the threat to encryption caused by maturing quantum computation. This work
explores a range of existing and future quantum communication networks,
specifically quantum key distribution network proposals, to model and
demonstrate the evolution of quantum key distribution network architectures.
Leveraging Orthogonal Variability Modelling and Systems Modelling Language as
candidate modelling languages, the study creates traceable artefacts to promote
modular architectures that are reusable for future studies. We propose a
variability-driven framework for managing fast-evolving network architectures
with respect to increasing stakeholder expectations. The result contributes to
the systematic development of viable quantum key distribution networks and
supports the investigation of similar integration challenges relevant to the
broader context of quantum systems engineering.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [47] [Declarative Data Pipeline for Large Scale ML Services](https://arxiv.org/abs/2508.15105)
*Yunzhao Yang,Runhui Wang,Xuanqing Liu,Adit Krishnan,Yefan Tao,Yuqian Deng,Kuangyou Yao,Peiyuan Sun,Henrik Johnson,Aditi sinha,Davor Golac,Gerald Friedland,Usman Shakeel,Daryl Cooke,Joe Sullivan,Chris Kong*

Main category: cs.DC

TL;DR: 论文提出了一种新型的“声明式数据管道”架构，通过模块化框架在Apache Spark中无缝集成机器学习能力，解决了大规模协作环境中系统性能与代码可维护性之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代分布式数据处理系统在大规模集成机器学习能力时，面临系统性能与代码可维护性、开发效率之间的挑战。

Method: 采用模块化框架（Pipes），结合逻辑计算单元，取代传统的微服务方法，建立清晰的组件边界和标准化接口。

Result: 企业案例显示开发效率提升50%，协作/故障排除时间从几周缩短到几天，性能提升500倍（扩展性）和10倍（吞吐量）；学术实验证明吞吐量至少快5.7倍，CPU利用率达99%。

Conclusion: 该架构为构建可扩展、可维护的数据处理系统提供了有效的解决方案，同时平衡了系统性能和开发速度。

Abstract: Modern distributed data processing systems face significant challenges in
balancing system performance with code maintainability and developer
productivity, particularly when integrating machine learning capabilities at
scale. In large collaborative environments, these challenges are amplified by
high communication overhead between teams and the complexity of coordinating
development across multiple groups. This paper presents a novel "Declarative
Data Pipeline" architecture that addresses these challenges while processing
billions of records with high accuracy and efficiency. Our architecture
introduces a modular framework that seamlessly integrates machine learning
capabilities within Apache Spark by combining logical computation units that we
refer as Pipes, departing from traditional microservice-based approaches. By
establishing clear component boundaries and standardized interfaces, we achieve
both modularity and system optimization without sacrificing maintainability.
The enterprise case study demonstrate substantial improvements in multiple
dimensions: development efficiency improved by 50%,
collaboration/troubleshooting efforts compressed from weeks to days,
performance improved by 500x in scalability and by 10x in throughput. The
academic experiment also proves at least 5.7x faster in throughput with 99% CPU
utilization than non-framework implementations. This paper details the
architectural decisions, implementation strategies, and performance
optimizations that enable these improvements, providing insights for building
scalable, maintainable data processing systems that effectively balance system
performance with development velocity.

</details>


### [48] [Efficient Mixed-Precision Large Language Model Inference with TurboMind](https://arxiv.org/abs/2508.15601)
*Li Zhang,Youhe Jiang,Guoliang He,Xin Chen,Han Lv,Qian Yao,Fangcheng Fu,Kai Chen*

Main category: cs.DC

TL;DR: 该论文提出了混合精度推理技术，通过优化内存和计算，显著降低大型语言模型（LLM）的延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 为了减少大型语言模型的内存和计算需求，同时提高推理效率。

Method: 采用两种混合精度流水线（GEMM流水线和注意力流水线），结合硬件感知的权重打包、自适应头对齐等技术。

Result: 在16种流行的LLM和4种GPU架构上测试，延迟降低61%，吞吐量提高156%。

Conclusion: 该技术被集成到开源的高性能推理引擎TurboMind中，显著提升了混合精度工作负载的性能。

Abstract: Mixed-precision inference techniques reduce the memory and computational
demands of Large Language Models (LLMs) by applying hybrid precision formats to
model weights, activations, and KV caches. This work introduces mixed-precision
LLM inference techniques that encompass (i) systematic memory and compute
optimization across hierarchical storage and tensor core architectures, and
(ii) comprehensive end-to-end mixed-precision optimization across diverse
precision formats and hardware configurations. Our approach features two novel
mixed-precision pipelines designed for optimal hardware utilization: a General
Matrix Multiply (GEMM) pipeline that optimizes matrix operations through
offline weight packing and online acceleration, and an attention pipeline that
enables efficient attention computation with arbitrary Query, Key, and Value
precision combinations. The key implementation of the pipelines includes (i)
hardware-aware weight packing for automatic format optimization, (ii) adaptive
head alignment for efficient attention computation, (iii) instruction-level
parallelism for memory hierarchy exploitation, and (iv) KV memory loading
pipeline for enhanced inference efficiency. We conduct comprehensive
evaluations across 16 popular LLMs and 4 representative GPU architectures.
Results demonstrate that our approach achieves up to 61% lower serving latency
(30% on average) and up to 156% higher throughput (58% on average) in
mixed-precision workloads compared to existing mixed-precision frameworks,
establishing consistent performance improvements across all tested
configurations and hardware types. This work is integrated into TurboMind, a
high-performance inference engine of the LMDeploy project, which is
open-sourced and publicly available at https://github.com/InternLM/lmdeploy.

</details>


### [49] [Databelt: A Continuous Data Path for Serverless Workflows in the 3D Compute Continuum](https://arxiv.org/abs/2508.15351)
*Cynthia Marcelino,Leonard Guelmino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Databelt是一个针对动态3D计算连续体的无服务器工作流状态管理框架，通过SLO感知状态传播和函数状态融合机制，显著降低延迟和网络开销。


<details>
  <summary>Details</summary>
Motivation: 在动态的3D计算连续体中，无服务器函数面临高延迟和不必要的网络传输问题，需要高效的状态管理解决方案。

Method: Databelt提出SLO感知状态传播和函数状态融合机制，优化状态访问和共享，减少重复操作。

Result: 实验显示，Databelt将工作流执行时间减少66%，吞吐量提高50%，存储操作延迟降低20%。

Conclusion: Databelt显著提升了无服务器工作流在动态网络环境中的效率和性能。

Abstract: Typically, serverless functions rely on remote storage services for managing
state, which can result in increased latency and network communication
overhead. In a dynamic environment such as the 3D (Edge-Cloud-Space) Compute
Continuum, serverless functions face additional challenges due to frequent
changes in network topology. As satellites move in and out of the range of
ground stations, functions must make multiple hops to access cloud services,
leading to high-latency state access and unnecessary data transfers. In this
paper, we present Databelt, a state management framework for serverless
workflows designed for the dynamic environment of the 3D Compute Continuum.
Databelt introduces an SLO-aware state propagation mechanism that enables the
function state to move continuously in orbit. Databelt proactively offloads
function states to the most suitable node, such that when functions execute,
the data is already present on the execution node or nearby, thus minimizing
state access latency and reducing the number of network hops. Additionally,
Databelt introduces a function state fusion mechanism that abstracts state
management for functions sharing the same serverless runtime. When functions
are fused, Databelt seamlessly retrieves their state as a group, reducing
redundant network and storage operations and improving overall workflow
efficiency. Our experimental results show that Databelt reduces workflow
execution time by up to 66% and increases throughput by 50% compared to the
baselines. Furthermore, our results show that Databelt function state fusion
reduces storage operations latency by up to 20%, by reducing repetitive storage
requests for functions within the same runtime, ensuring efficient execution of
serverless workflows in highly dynamic network environments such as the 3D
Continuum.

</details>


### [50] [Universal Dancing by Luminous Robots under Sequential Schedulers](https://arxiv.org/abs/2508.15484)
*Caterina Feletti,Paola Flocchini,Debasish Pattanayak,Giuseppe Prencipe,Nicola Santoro*

Main category: cs.DC

TL;DR: 本文研究了在LUMI模型下解决通用舞蹈问题的可能性，通过分布式计数器机制实现了任意初始配置下的多机器人编队。


<details>
  <summary>Details</summary>
Motivation: 探索在LUMI模型下放宽现有舞蹈问题中对编队和初始配置的限制条件，实现更通用的舞蹈问题解决方案。

Method: 利用LUMI模型中的颜色灯和顺序调度器，设计分布式计数器机制算法。

Result: 证明了在颜色数量限制下，通用舞蹈问题可解，并提出了一个非刚性移动的算法。

Conclusion: LUMI模型结合顺序调度器能放宽舞蹈问题的必要约束，实现通用多机器人编队。

Abstract: The Dancing problem requires a swarm of $n$ autonomous mobile robots to form
a sequence of patterns, aka perform a choreography. Existing work has proven
that some crucial restrictions on choreographies and initial configurations
(e.g., on repetitions of patterns, periodicity, symmetries,
contractions/expansions) must hold so that the Dancing problem can be solved
under certain robot models. Here, we prove that these necessary constraints can
be dropped by considering the LUMI model (i.e., where robots are endowed with a
light whose color can be chosen from a constant-size palette) under the quite
unexplored sequential scheduler. We formalize the class of Universal Dancing
problems which require a swarm of $n$ robots starting from any initial
configuration to perform a (periodic or finite) sequence of arbitrary patterns,
only provided that each pattern consists of $n$ vertices (including
multiplicities). However, we prove that, to be solvable under LUMI, the length
of the feasible choreographies is bounded by the compositions of $n$ into the
number of colors available to the robots. We provide an algorithm solving the
Universal Dancing problem by exploiting the peculiar capability of sequential
robots to implement a distributed counter mechanism. Even assuming non-rigid
movements, our algorithm ensures spatial homogeneity of the performed
choreography.

</details>


### [51] [Lower Bounds for $k$-Set Agreement in Fault-Prone Networks](https://arxiv.org/abs/2508.15562)
*Pierre Fraigniaud,Minh Hang Nguyen,Ami Paz,Ulrich Schmid,Hugo Rincon Galeana*

Main category: cs.DC

TL;DR: 提出了一种新的k-set agreement下界，适用于任意有向通信网络的同步消息传递系统，扩展了现有关于完全网络和特定网络模型的下界结果。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在任意通信网络中k-set agreement问题的复杂性，尤其是在进程可能崩溃的情况下，提供更通用的理论支持。

Method: 采用了拓扑证明方法，结合shellable carrier maps和Sperner引理，分析了协议复形的演化过程，并提出了一种新的carrier map。

Result: 证明了在特定轮数内k-set agreement的不可能性，并提供了基于通信图半径的简单下界。此外，展示了基于Kuhn三角剖分的更小输入复形的可行性。

Conclusion: 新方法不仅扩展了现有理论，还简化了部分证明过程，同时为更高效的算法设计提供了理论依据。

Abstract: We develop a new lower bound for k-set agreement in synchronous
message-passing systems connected by an arbitrary directed communication
network, where up to t processes may crash. Our result thus generalizes the
t/k+1 lower bound for complete networks in the t-resilient model by Chaudhuri,
Herlihy, Lynch, and Tuttle [JACM'00]. Moreover, it generalizes two lower bounds
for oblivious algorithms in synchronous systems connected by an arbitrary
undirected communication network known to the processes, namely, the domination
number-based lower bound by Castaneda, Fraigniaud, Paz, Rajsbaum, Roy, and
Travers [TCS'21] for failure-free processes, and the radius-based lower bound
in the t-resilient model by Fraigniaud, Nguyen, and Paz [STACS'24].
  Our topological proof non-trivially generalizes and extends the
connectivity-based approach for the complete network, as presented in the book
by Herlihy, Kozlov, and Rajsbaum (2013). It is based on a sequence of shellable
carrier maps that, starting from a shellable input complex, determine the
evolution of the protocol complex: During the first t/k rounds, carrier maps
that crash exactly k processes per round are used, ensuring high connectivity
of their images. A Sperner's lemma style argument is used to prove that k-set
agreement is still impossible by that round. From round t/k+1 up to our lower
bound, we employ a novel carrier map that maintains high connectivity. Our
proof also provides a strikingly simple lower bound for k-set agreement in
synchronous systems with an arbitrary communication network with initial
crashes. We express the resulting additional agreement overhead via an
appropriately defined radius of the communication graphs. Finally, we prove
that the usual input pseudosphere complex for k-set agreement can be replaced
by an exponentially smaller input complex based on Kuhn triangulations, which
we prove to be also shellable.

</details>


### [52] [CausalMesh: A Formally Verified Causal Cache for Stateful Serverless Computing](https://arxiv.org/abs/2508.15647)
*Haoran Zhang,Zihao Zhang,Shuai Mu,Sebastian Angel,Vincent Liu*

Main category: cs.DC

TL;DR: CausalMesh 是一种在无服务器计算环境中实现因果一致性的新型缓存系统，支持无需协调和中止的读写操作，并提高性能。


<details>
  <summary>Details</summary>
Motivation: 在无服务器环境中，多个函数可能分配在不同节点的不同缓存中，导致潜在的不一致问题。

Method: 提出 CausalMesh，支持在计算迁移（如无服务器环境）中实现因果一致性缓存，无需协调和中止。

Result: 通过 Dafny 形式化验证，实验显示 CausalMesh 延迟更低、吞吐量更高。

Conclusion: CausalMesh 是首个支持客户端在多服务器间漫游时仍保持因果一致性的解决方案，性能优于现有方案。

Abstract: Stateful serverless workflows consist of multiple serverless functions that
access state on a remote database. Developers sometimes add a cache layer
between the serverless runtime and the database to improve I/O latency.
However, in a serverless environment, functions in the same workflow may be
scheduled to different nodes with different caches, which can cause
non-intuitive anomalies. This paper presents CausalMesh, a novel approach to
causally consistent caching in environments where a computation may migrate
from one machine to another, such as in serverless computing. CausalMesh is the
first cache system that supports coordination-free and abort-free read/write
operations and read transactions when clients roam among multiple servers.
CausalMesh also supports read-write transactional causal consistency in the
presence of client roaming, but at the cost of abort-freedom.
  We have formally verified CausalMesh's protocol in Dafny, and our
experimental evaluation shows that CausalMesh has lower latency and higher
throughput than existing proposals

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [53] [Random Sampling over Spatial Range Joins](https://arxiv.org/abs/2508.15070)
*Daichi Amagata*

Main category: cs.DB

TL;DR: 该论文解决了空间范围连接中随机采样效率低的问题，提出了一种新的数据结构和算法，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 空间范围连接在多个领域应用广泛，但传统方法计算成本高且结果集庞大。随机采样可缓解这些问题，但如何高效实现随机采样不运行完整连接仍具挑战性。

Method: 设计两种基线算法并证明其低效性，提出新数据结构，预期时间为$	ilde{O}(n + m + t)$，空间为$O(n+m)$。

Result: 实验表明，新算法在大多数测试中明显快于基线方法。

Conclusion: 新算法首次高效解决了空间范围连接的随机采样问题，具有实际应用价值。

Abstract: Spatial range joins have many applications, including geographic information
systems, location-based social networking services, neuroscience, and
visualization. However, joins incur not only expensive computational costs but
also too large result sets. A practical and reasonable approach to alleviating
these issues is to return random samples of the join results. Although this is
promising and sufficient for many applications involving spatial range joins,
efficiently computing random samples is not trivial. This is because we must
obtain random join samples without running spatial range joins. We address this
challenging problem for the first time and aim at designing a time- and
space-efficient algorithm. First, we design two baseline algorithms that employ
existing techniques for random sampling and show that they are not efficient.
Then, we propose a new data structure that can deal with our problem in
$\tilde{O}(n + m + t)$ expected time and $O(n+m)$ space, where $n$ and $m$ are
the sizes of two point sets and $t$ is the required number of samples. We
conduct extensive experiments using four real spatial datasets, and the results
demonstrate that our algorithm is significantly faster than the baselines in
most tests.

</details>


### [54] [Temporal $k$-Core Query, Revisited](https://arxiv.org/abs/2508.15238)
*Yinyu Liu,Kaiqiang Yu,Shengxin Liu,Cheng Long,Zhaoquan Gu*

Main category: cs.DB

TL;DR: 论文分析了动态图（如社交网络）中查询凝聚子图的问题，提出了高效算法CoreT，显著提升了现有方法的计算效率。


<details>
  <summary>Details</summary>
Motivation: 理解动态网络的演化结构（如社交平台的社区变化）需要高效的凝聚子图查询方法，现有算法OTCD存在冗余计算和扩展性问题。

Method: 提出CoreT算法，动态记录顶点或边进入k-core的最早时间戳，通过剪枝冗余计算，仅需一次遍历查询区间。

Result: 实验表明，CoreT在大型数据集上比OTCD快四个数量级，具有高扩展性。

Conclusion: CoreT高效且可扩展，适用于长期时序k-core分析。

Abstract: Querying cohesive subgraphs in temporal graphs is essential for understanding
the dynamic structure of real-world networks, such as evolving communities in
social platforms, shifting hyperlink structures on the Web, and transient
communication patterns in call networks. Recently, research has focused on the
temporal $k$-core query, which aims to identify all $k$-cores across all
possible time sub-intervals within a given query interval. The state-of-the-art
algorithm OTCD mitigates redundant computations over overlapping sub-intervals
by exploiting inclusion relationships among $k$-cores in different time
intervals. Nevertheless, OTCD remains limited in scalability due to the
combinatorial growth in interval enumeration and repeated processing. In this
paper, we revisit the temporal $k$-core query problem and introduce a novel
algorithm CoreT, which dynamically records the earliest timestamp at which each
vertex or edge enters a $k$-core. This strategy enables substantial pruning of
redundant computations. As a result, CoreT requires only a single pass over the
query interval and achieves improved time complexity, which is linear in both
the number of temporal edges within the query interval and the duration of the
interval, making it highly scalable for long-term temporal analysis.
Experimental results on large real-world datasets show that CoreT achieves up
to four orders of magnitude speedup compared to the existing state-of-the-art
OTCD, demonstrating its effectiveness and scalability for temporal $k$-core
analysis.

</details>


### [55] [AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL](https://arxiv.org/abs/2508.15276)
*Zhongjun Ding,Yin Lin,Tianjing Zeng*

Main category: cs.DB

TL;DR: AmbiSQL是一个交互式系统，通过检测查询歧义并引导用户澄清意图，显著提升Text-to-SQL系统的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在Text-to-SQL任务中因查询歧义导致的误解和错误SQL生成问题。

Method: 引入细粒度歧义分类法，自动检测歧义并通过多选问题引导用户反馈，重写模糊问题。

Result: 歧义检测精度达87.2%，SQL准确匹配率提升50%。

Conclusion: AmbiSQL通过用户互动显著提升Text-to-SQL系统的实用性和性能。

Abstract: Text-to-SQL systems translate natural language questions into SQL queries,
providing substantial value for non-expert users. While large language models
(LLMs) show promising results for this task, they remain error-prone. Query
ambiguity has been recognized as a major obstacle for LLM-based Text-to-SQL
systems, leading to misinterpretation of user intent and inaccurate SQL
generation. We demonstrate AmbiSQL, an interactive system that automatically
detects query ambiguities and guides users through intuitive multiple-choice
questions to clarify their intent. Our approach introduces a fine-grained
ambiguity taxonomy for identifying ambiguities that affect database element
mapping and LLM reasoning, then incorporates user feedback to rewrite ambiguous
questions. Evaluation on an ambiguous query dataset shows that AmbiSQL achieves
87.2% precision in ambiguity detection and improves SQL exact match accuracy by
50% when integrated with Text-to-SQL systems. Our demonstration showcases the
significant performance gains and highlights the system's practical usability.
Code repo and demonstration are available at:
https://github.com/JustinzjDing/AmbiSQL.

</details>


### [56] [Efficient Cloud-Edge-Device Query Execution Based on Collaborative Scan Operator](https://arxiv.org/abs/2508.15285)
*Chunyu Zhao,Hongzhi Wang,Kaixin Zhang,Hongliang Li,Yihan Zhang,Jiawei Zhang,Kunkai Gu,Yuan Tian,Xiangdong Huang,Jingyi Xu*

Main category: cs.DB

TL;DR: 该论文提出了一种基于协作扫描算子的云边设备协作查询处理方法，以解决边缘资源瓶颈时的查询性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在云边设备协作查询处理中，边缘资源瓶颈会导致性能下降，而现有方法难以灵活切换执行环境。

Method: 通过建立基于协作扫描算子的云边设备协作框架，实现查询执行在云与边缘间的无缝切换。

Result: 实验表明，在足够网络带宽下，协作扫描算子能有效缓解边缘高负载导致的性能下降，并实现资源均衡调度。

Conclusion: 所提方法能显著提升边缘资源瓶颈时的查询性能。

Abstract: In cloud-edge-device (CED) collaborative query (CQ) processing, by leveraging
CED collaboration, the advantages of both cloud computing and edge resources
can be fully integrated. However, it is difficult to implement collaborative
operators that can flexibly switch between the cloud and the edge during query
execution. Thus, in this paper, we aim to improve the query performance when
the edge resources reach a bottleneck. To achieve seamless switching of query
execution between the cloud and edge, we propose a CQ processing method by
establishing a CED collaborative framework based on the collaborative scan
operator, so that query execution can be transferred to the cloud at any time
when the edge resources are saturated. Extensive experiments show that, under
sufficient network download bandwidth, the CED collaborative scan operator can
effectively alleviate the performance degradation of scan operators caused by
high I/O load and CPU wait time at the edge. It also achieves balanced resource
scheduling between the cloud and edge.

</details>


### [57] [Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional Vector Search](https://arxiv.org/abs/2508.15290)
*Peiqi Yin,Xiao Yan,Qihui Zhou,Hui Li,Xiaolu Li,Lin Zhang,Meiling Wang,Xin Yao,James Cheng*

Main category: cs.DB

TL;DR: 本文提出了Gorgeous系统，通过优先处理图结构而非向量，优化了基于相似性的向量搜索性能。


<details>
  <summary>Details</summary>
Motivation: 大规模向量数据集（如TB级）的处理成本高，现有系统采用SSD存储和邻近图索引，但数据布局不佳，未能有效利用内存空间和局部性。

Method: 设计Gorgeous系统，采用内存缓存存储图节点邻接表以提升缓存命中率，并优化磁盘块格式以增强数据局部性。

Result: 实验证明Gorgeous显著优于现有系统，查询吞吐量提升60%以上，延迟降低35%以上。

Conclusion: Gorgeous通过优化图结构和向量访问实现了高效的向量搜索，为大规模数据处理提供了有效解决方案。

Abstract: Similarity-based vector search underpins many important applications, but a
key challenge is processing massive vector datasets (e.g., in TBs). To reduce
costs, some systems utilize SSDs as the primary data storage. They employ a
proximity graph, which connects similar vectors to form a graph and is the
state-of-the-art index for vector search. However, these systems are hindered
by sub-optimal data layouts that fail to effectively utilize valuable memory
space to reduce disk access and suffer from poor locality for accessing
disk-resident data. Through extensive profiling and analysis, we found that the
structure of the proximity graph index is accessed more frequently than the
vectors themselves, yet existing systems do not distinguish between the two. To
address this problem, we design the Gorgeous system with the principle of
prioritizing graph structure over vectors. Specifically, Gorgeous features a
memory cache that keeps the adjacency lists of graph nodes to improve cache
hits and a disk block format that explicitly stores neighbors' adjacency lists
along with a vector to enhance data locality. Experimental results show that
Gorgeous consistently outperforms two state-of-the-art disk-based systems for
vector search, boosting average query throughput by over 60% and reducing query
latency by over 35%.

</details>


### [58] [GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector Nearest Neighbor Search](https://arxiv.org/abs/2508.15694)
*Yijie Zhou,Shengyuan Lin,Shufeng Gong,Song Yu,Shuhao Fan,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: GoVector是一种针对磁盘图索引的高效缓存策略，结合静态和动态缓存，减少I/O开销，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有静态缓存在第二阶段无法高效处理动态访问的节点，导致高I/O延迟，需要更优策略。

Method: 结合静态缓存存储入口节点与动态缓存捕捉空间局部性高的节点，并优化磁盘存储布局。

Result: 实验显示，GoVector在90%召回率下平均减少46%的I/O操作，提升1.73倍查询吞吐，降低42%延迟。

Conclusion: GoVector显著优化了磁盘图索引的I/O效率，提升了查询性能。

Abstract: Graph-based high-dimensional vector indices have become a mainstream solution
for large-scale approximate nearest neighbor search (ANNS). However, their
substantial memory footprint often requires storage on secondary devices, where
frequent on-demand loading of graph and vector data leads to I/O becoming the
dominant bottleneck, accounting for over 90\% of query latency. Existing static
caching strategies mitigate this issue only in the initial navigation phase by
preloading entry points and multi-hop neighbors, but they fail in the second
phase where query-dependent nodes must be dynamically accessed to achieve high
recall. We propose GoVector, an I/O-efficient caching strategy tailored for
disk-based graph indices. GoVector combines (1) a static cache that stores
entry points and frequently accessed neighbors, and (2) a dynamic cache that
adaptively captures nodes with high spatial locality during the second search
phase. To further align storage layout with similarity-driven search patterns,
GoVector reorders nodes on disk so that similar vectors are colocated on the
same or adjacent pages, thereby improving locality and reducing I/O overhead.
Extensive experiments on multiple public datasets show that GoVector achieves
substantial performance improvements. At 90% recall, it reduces I/O operations
by 46% on average, increases query throughput by 1.73x, and lowers query
latency by 42% compared to state-of-the-art disk-based graph indexing systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [59] [Accelerating GenAI Workloads by Enabling RISC-V Microkernel Support in IREE](https://arxiv.org/abs/2508.14899)
*Adeel Ahmad,Ahmad Tameem Kamal,Nouman Amir,Bilal Zafar,Saad Bin Nasir*

Main category: cs.AR

TL;DR: 该项目在基于MLIR的机器学习编译器和运行时IREE中实现了RISC-V微内核支持，通过优化RISC-V64目标下的MLIR linalg方言操作，并开发专用微内核，对比了性能提升。


<details>
  <summary>Details</summary>
Motivation: 为RISC-V架构提供高效的机器学习编译器支持，满足特定模型（如Llama-3.2-1B-Instruct）的性能需求。

Method: 在IREE流程中，将MLIR linalg方言操作降级为linalg.mmt4d操作，并开发RISC-V优化的微内核。

Result: 与上游IREE和Llama.cpp相比，性能有所提升。

Conclusion: 该项目成功实现了RISC-V微内核支持，优化了特定模型的执行效率。

Abstract: This project enables RISC-V microkernel support in IREE, an MLIR-based
machine learning compiler and runtime. The approach begins by enabling the
lowering of MLIR linalg dialect contraction ops to linalg.mmt4d op for the
RISC-V64 target within the IREE pass pipeline, followed by the development of
optimized microkernels for RISC-V. The performance gains are compared with
upstream IREE and Llama.cpp for the Llama-3.2-1B-Instruct model.

</details>


### [60] [Improving Chip Design Enablement for Universities in Europe -- A Position Paper](https://arxiv.org/abs/2508.14907)
*Lukas Krupp,Ian O'Connor,Luca Benini,Christoph Studer,Joachim Rodrigues,Norbert Wehn*

Main category: cs.AR

TL;DR: 本文探讨欧洲大学和学术机构在提升芯片设计教育和研究中的作用，以应对芯片设计能力的不足。


<details>
  <summary>Details</summary>
Motivation: 欧洲半导体行业面临芯片设计能力严重不足的问题，尤其是人才短缺和设计价值链中的落后。

Method: 综述当前欧洲的芯片设计项目，分析招聘、生产力、技术获取和设计支持等方面的挑战，并提出战略机遇。

Result: 提出一系列建议，强调需要协调努力和战略投资来应对挑战。

Conclusion: 通过学术机构的合作和投资，可以提升欧洲的芯片设计能力。

Abstract: The semiconductor industry is pivotal to Europe's economy, especially within
the industrial and automotive sectors. However, Europe faces a significant
shortfall in chip design capabilities, marked by a severe skilled labor
shortage and lagging contributions in the design value chain segment. This
paper explores the role of European universities and academic initiatives in
enhancing chip design education and research to address these deficits. We
provide a comprehensive overview of current European chip design initiatives,
analyze major challenges in recruitment, productivity, technology access, and
design enablement, and identify strategic opportunities to strengthen chip
design capabilities within academic institutions. Our analysis leads to a
series of recommendations that highlight the need for coordinated efforts and
strategic investments to overcome these challenges.

</details>


### [61] [Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis](https://arxiv.org/abs/2508.14917)
*Weichien Liao*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的可扩展预处理流水线，用于实时去噪，优化了DRAM缓冲，减少了延迟并支持高吞吐量成像工作流。


<details>
  <summary>Details</summary>
Motivation: 解决高通量成像工作流（如PRISM）因数据生成速率超过传统实时处理能力而面临的挑战。

Method: 通过高级综合（HLS）实现FPGA预处理流水线，利用突发模式AXI4接口优化DRAM缓冲，进行帧减法和平均操作。

Result: 该架构在帧间隔内运行，实现了实时去噪并减小了数据集大小，适用于PRISM级采集。

Conclusion: 模块化FPGA框架为光谱学和显微镜学中的延迟敏感成像工作流提供了实用解决方案。

Abstract: High-throughput imaging workflows, such as Parallel Rapid Imaging with
Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional
real-time processing capabilities. We present a scalable FPGA-based
preprocessing pipeline for real-time denoising, implemented via High-Level
Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture
performs frame subtraction and averaging directly on streamed image data,
minimizing latency through burst-mode AXI4 interfaces. The resulting kernel
operates below the inter-frame interval, enabling inline denoising and reducing
dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale
acquisition, this modular FPGA framework offers a practical solution for
latency-sensitive imaging workflows in spectroscopy and microscopy.

</details>


### [62] [Row-Column Hybrid Grouping for Fault-Resilient Multi-Bit Weight Representation on IMC Arrays](https://arxiv.org/abs/2508.15685)
*Kang Eun Jeon,Sangheum Yeon,Jinhee Kim,Hyeonsu Bang,Johnny Rhe,Jong Hwan Ko*

Main category: cs.AR

TL;DR: 提出一种新型的多比特权重表示技术和优化的编译器管道，解决模拟内存计算系统中计算不可靠性和高编译开销问题。


<details>
  <summary>Details</summary>
Motivation: 解决模拟内存计算系统中因固定故障（SAFs）导致的不可靠性以及现有故障缓解算法的高编���开销问题。

Method: 1. 提出行-列混合分组的多比特权重表示技术，增强容错能力；2. 将故障感知权重分解问题转化为整数线性规划任务，优化编译器管道。

Result: 在卷积网络和小型语言模型上，实现了8%的准确率提升、150倍的编译加速和2倍的能效提升。

Conclusion: 提出的技术显著提升了模拟内存计算系统的可靠性、编译效率和能效。

Abstract: This paper addresses two critical challenges in analog In-Memory Computing
(IMC) systems that limit their scalability and deployability: the computational
unreliability caused by stuck-at faults (SAFs) and the high compilation
overhead of existing fault-mitigation algorithms, namely Fault-Free (FF). To
overcome these limitations, we first propose a novel multi-bit weight
representation technique, termed row-column hybrid grouping, which generalizes
conventional column grouping by introducing redundancy across both rows and
columns. This structural redundancy enhances fault tolerance and can be
effectively combined with existing fault-mitigation solutions. Second, we
design a compiler pipeline that reformulates the fault-aware weight
decomposition problem as an Integer Linear Programming (ILP) task, enabling
fast and scalable compilation through off-the-shelf solvers. Further
acceleration is achieved through theoretical insights that identify fault
patterns amenable to trivial solutions, significantly reducing computation.
Experimental results on convolutional networks and small language models
demonstrate the effectiveness of our approach, achieving up to 8%p improvement
in accuracy, 150x faster compilation, and 2x energy efficiency gain compared to
existing baselines.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [63] [Human Feedback Driven Dynamic Speech Emotion Recognition](https://arxiv.org/abs/2508.14920)
*Ilya Fedorov,Dmitry Korobchenko*

Main category: cs.SD

TL;DR: 提出了一种动态语音情感识别新方法，重点研究3D虚拟角色的情感驱动，采用基于Dirichlet分布的多阶段方法并引入人类反馈以提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 探索动态语音情感识别的新领域，不同于传统方法，假设每段音频对应随时间变化的情感序列，特别关注3D虚拟角色的情感动画。

Method: 提出多阶段方法：先训练传统语音情感识别模型，合成情感序列，再通过人类反馈优化模型；引入基于Dirichlet分布的情感混合建模新方法。

Result: 实验表明Dirichlet方法在情感混合建模上更有效，结合人类反馈既简化标注又提升模型质量。

Conclusion: 该方法在动态情感识别和3D动画驱动中表现出色，Dirichlet分布和人类反馈是关键改进点。

Abstract: This work proposes to explore a new area of dynamic speech emotion
recognition. Unlike traditional methods, we assume that each audio track is
associated with a sequence of emotions active at different moments in time. The
study particularly focuses on the animation of emotional 3D avatars. We propose
a multi-stage method that includes the training of a classical speech emotion
recognition model, synthetic generation of emotional sequences, and further
model improvement based on human feedback. Additionally, we introduce a novel
approach to modeling emotional mixtures based on the Dirichlet distribution.
The models are evaluated based on ground-truth emotions extracted from a
dataset of 3D facial animations. We compare our models against the sliding
window approach. Our experimental results show the effectiveness of
Dirichlet-based approach in modeling emotional mixtures. Incorporating human
feedback further improves the model quality while providing a simplified
annotation procedure.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [64] [Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions](https://arxiv.org/abs/2508.15047)
*Yibo Liu,Liam Shatzel,Brandon Haworth,Teseo Schneider*

Main category: cs.AI

TL;DR: 提出了基于大型语言模型（LLM）的新型方法，通过对话系统和语言驱动的导航来模拟更真实的人群行为。


<details>
  <summary>Details</summary>
Motivation: 现有的人群模拟方法缺乏对社会和语言互动的考虑，限制了行为的真实性。

Method: 结合LLM生成对话，并根据个性、情感状态等参数控制导航和转向。

Result: 在复杂场景中观察到自动分组和信息传递行为，模拟效果更真实。

Conclusion: 该方法提升了人群模拟的真实性，能自然产生群体行为。

Abstract: Animating and simulating crowds using an agent-based approach is a
well-established area where every agent in the crowd is individually controlled
such that global human-like behaviour emerges. We observe that human navigation
and movement in crowds are often influenced by complex social and environmental
interactions, driven mainly by language and dialogue. However, most existing
work does not consider these dimensions and leads to animations where
agent-agent and agent-environment interactions are largely limited to steering
and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to
control agents' movement. Our method has two main components: a dialogue system
and language-driven navigation. We periodically query agent-centric LLMs
conditioned on character personalities, roles, desires, and relationships to
control the generation of inter-agent dialogue when necessitated by the spatial
and social relationships with neighbouring agents. We then use the conversation
and each agent's personality, emotional state, vision, and physical state to
control the navigation and steering of each agent. Our model thus enables
agents to make motion decisions based on both their perceptual inputs and the
ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay
between social interactions, steering, and crowding. In these scenarios, we
observe that grouping and ungrouping of agents automatically occur.
Additionally, our experiments show that our method serves as an
information-passing mechanism within the crowd. As a result, our framework
produces more realistic crowd simulations, with emergent group behaviours
arising naturally from any environmental setting.

</details>


### [65] [Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle](https://arxiv.org/abs/2508.15680)
*Mark Cote,Susana Aires*

Main category: cs.AI

TL;DR: 本文主张从技术哲学角度解读《欧盟人工智能法案》，揭示AI系统中数据的长期动态，提出“futurity”概念以分析数据递归价值链，强调需监管AI生命周期的动态性。


<details>
  <summary>Details</summary>
Motivation: 探讨《欧盟人工智能法案》在AI数据动态生命周期中的监管盲点，揭示数据递归价值链对负责任AI框架的挑战。

Method: 结合跨学科方法提出技术基础与哲学一致的分析框架，并以Simondon技术哲学为灵感，构建AI生命周期的形式化模型。

Result: 指出当前政策缺失对AI动态“生成”过程的考虑，提出“futurity”概念，强调数据递归性和权力不对称。

Conclusion: 呼吁监管需关注基础设施与时间动态，提出包括生命周期审计等具体措施，以应对AI系统的递归性与权力集中问题。

Abstract: This paper argues that a techno-philosophical reading of the EU AI Act
provides insight into the long-term dynamics of data in AI systems,
specifically, how the lifecycle from ingestion to deployment generates
recursive value chains that challenge existing frameworks for Responsible AI.
We introduce a conceptual tool to frame the AI pipeline, spanning data,
training regimes, architectures, feature stores, and transfer learning. Using
cross-disciplinary methods, we develop a technically grounded and
philosophically coherent analysis of regulatory blind spots. Our central claim
is that what remains absent from policymaking is an account of the dynamic of
becoming that underpins both the technical operation and economic logic of AI.
To address this, we advance a formal reading of AI inspired by Simondonian
philosophy of technology, reworking his concept of individuation to model the
AI lifecycle, including the pre-individual milieu, individuation, and
individuated AI. To translate these ideas, we introduce futurity: the
self-reinforcing lifecycle of AI, where more data enhances performance, deepens
personalisation, and expands application domains. Futurity highlights the
recursively generative, non-rivalrous nature of data, underpinned by
infrastructures like feature stores that enable feedback, adaptation, and
temporal recursion. Our intervention foregrounds escalating power asymmetries,
particularly the tech oligarchy whose infrastructures of capture, training, and
deployment concentrate value and decision-making. We argue that effective
regulation must address these infrastructural and temporal dynamics, and
propose measures including lifecycle audits, temporal traceability, feedback
accountability, recursion transparency, and a right to contest recursive reuse.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [66] [Transition-based vs stated-based acceptance for automata over infinite words](https://arxiv.org/abs/2508.15402)
*Antonio Casares*

Main category: cs.FL

TL;DR: 该论文探讨了无限对象自动机中基于转移的接受条件替代传统基于状态的接受条件的原因和优势。


<details>
  <summary>Details</summary>
Motivation: 随着研究趋势的转变，论文旨在分析并倡导在无限字自动机中使用基于转移的接受条件，探讨其在不同问题中的显著影响。

Method: 通过调查和分析，比较基于状态和基于转移的接受条件在多种问题中的应用效果。

Result: 发现基于转移的接受条件在某些场景下更具优势，并对选择不同形式化的原因进行了探讨。

Conclusion: 论文总结认为，在无限字自动机中采用基于转移的接受条件是一种值得推广的趋势。

Abstract: Automata over infinite objects are a well-established model with applications
in logic and formal verification. Traditionally, acceptance in such automata is
defined based on the set of states visited infinitely often during a run.
However, there is a growing trend towards defining acceptance based on
transitions rather than states.
  In this survey, we analyse the reasons for this shift and advocate using
transition-based acceptance in the context of automata over infinite words. We
present a collection of problems where the choice of formalism has a major
impact and discuss the causes of these differences.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [67] [A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity](https://arxiv.org/abs/2508.15386)
*Sabine Houy,Bruno Kreyssig,Timothee Riom,Alexandre Bartel,Patrick McDaniel*

Main category: cs.CR

TL;DR: 本文建立了一个分类法，将LLVM的前向边缘控制流完整性（CFI）变体映射到内存损坏漏洞类别，为开发者在现有代码库中逐步部署CFI提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 内存损坏漏洞是软件安全的严重威胁，但目前开发者缺乏将CFI应用于实际软件的指导。

Method: 基于Top 10已知被利用漏洞（KEV）列表，作者选择四个高影响力漏洞类别，并为每类选一个代表性CVE，评估LLVM CFI的防护效果。

Result: CFI在两个案例中成功阻止漏洞利用，但在另外两个案例中失败，展示了其潜力与当前局限性。

Conclusion: 研究结果为开发者提供了部署CFI的决策依据，并为改进CFI在实际系统中的应用奠定了基础。

Abstract: Memory corruption vulnerabilities remain one of the most severe threats to
software security. They often allow attackers to achieve arbitrary code
execution by redirecting a vulnerable program's control flow. While Control
Flow Integrity (CFI) has gained traction to mitigate this exploitation path,
developers are not provided with any direction on how to apply CFI to
real-world software. In this work, we establish a taxonomy mapping LLVM's
forward-edge CFI variants to memory corruption vulnerability classes, offering
actionable guidance for developers seeking to deploy CFI incrementally in
existing codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)
list, we identify four high-impact vulnerability categories and select one
representative CVE for each. We evaluate LLVM's CFI against each CVE and
explain why CFI blocks exploitation in two cases while failing in the other
two, illustrating its potential and current limitations. Our findings support
informed deployment decisions and provide a foundation for improving the
practical use of CFI in production systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [68] [HEAS: Hierarchical Evolutionary Agent Simulation Framework for Cross-Scale Modeling and Multi-Objective Search](https://arxiv.org/abs/2508.15555)
*Ruiyu Zhang,Lin Nie,Xin Zhao*

Main category: cs.MA

TL;DR: HEAS是一个Python框架，结合了基于代理的建模、进化优化和锦标赛评估，提供统一且可重复的工作流，支持多目标优化和PyTorch集成。


<details>
  <summary>Details</summary>
Motivation: 为了解决跨学科、多尺度的建模问题，提供一个工具化的框架，支持可重复性和比较性研究。

Method: 将模型表示为轻量级进程的层次结构，通过确定性调度和共享上下文实现跨尺度耦合。提供API和CLI工具，支持进化优化和锦标赛评估。

Result: 成功构建了一个灵活且标准化的框架，支持多场景应用，并在生态和企业决策示例中验证了其可行性。

Conclusion: HEAS为跨学科研究提供了实用基础，能够生成可靠且可重复的结果。

Abstract: Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that
unifies layered agent-based modeling with evolutionary optimization and
tournament evaluation in a single, reproducible workflow. HEAS represents
models as hierarchies of lightweight processes ("streams") scheduled in
deterministic layers that read and write a shared context, making cross-scale
couplings explicit and auditable. A compact API and CLI-simulate, optimize,
evaluate-expose single- and multi-objective evolution, PyTorch policy
integration via parameter flattening/unflattening, and general tournament
tooling with user-defined scoring and voting rules. The framework standardizes
evaluation through uniform per-step and episode metrics, persists seeds,
logbooks, and hall-of-fame archives, and provides plotting helpers for traces,
Pareto fronts, and comparative outcomes, reducing glue code and improving
comparability across studies. HEAS emphasizes separation of mechanism from
orchestration, allowing exogenous drivers, endogenous agents, and aggregators
to be composed and swapped without refactoring, while the same model can be
used for forward simulation, optimization, or systematic comparison. We
illustrate usage with two compact examples-an ecological system and an
enterprise decision-making setting. HEAS offers a practical foundation for
cross-disciplinary, multi-level inquiry, yielding reliable, reproducible
results.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [69] [JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs](https://arxiv.org/abs/2508.15468)
*Zhiqiang Que,Chang Sun,Sudarshan Paramesvaran,Emyr Clement,Katerina Karakoulaki,Christopher Brown,Lauri Laatu,Arianna Cox,Alexander Tapper,Wayne Luk,Maria Spiropulu*

Main category: hep-ex

TL;DR: 提出了一种线性计算复杂度的GNN架构JEDI-linear，用于HL-LHC的硬件触发系统，显著降低了延迟和资源使用，提升了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在硬件触发系统中因计算复杂性和内存访问模式问题难以满足严格的延迟和资源约束。

Method: 通过共享变换和全局聚合消除显式对相互作用，引入细粒度量化感知训练和基于分布式算术的无乘法器操作。

Result: FPGA实现的JEDI-linear在延迟、启动间隔和LUT使用上均优于现有方案，同时模型准确性更高且无需DSP块。

Conclusion: JEDI-linear为下一代触发系统提供了精准、可扩展且高效的GNN推理解决方案，推动了科学应用的广泛采用。

Abstract: Graph Neural Networks (GNNs), particularly Interaction Networks (INs), have
shown exceptional performance for jet tagging at the CERN High-Luminosity Large
Hadron Collider (HL-LHC). However, their computational complexity and irregular
memory access patterns pose significant challenges for deployment on FPGAs in
hardware trigger systems, where strict latency and resource constraints apply.
In this work, we propose JEDI-linear, a novel GNN architecture with linear
computational complexity that eliminates explicit pairwise interactions by
leveraging shared transformations and global aggregation. To further enhance
hardware efficiency, we introduce fine-grained quantization-aware training with
per-parameter bitwidth optimization and employ multiplier-free
multiply-accumulate operations via distributed arithmetic. Evaluation results
show that our FPGA-based JEDI-linear achieves 3.7 to 11.5 times lower latency,
up to 150 times lower initiation interval, and up to 6.2 times lower LUT usage
compared to state-of-the-art designs while also delivering higher model
accuracy and eliminating the need for DSP blocks entirely. In contrast,
state-of-the-art solutions consume over 8,700 DSPs. This is the first
interaction-based GNN to achieve less than 60~ns latency and currently meets
the requirements for use in the HL-LHC CMS Level-1 trigger system. This work
advances the next-generation trigger systems by enabling accurate, scalable,
and resource-efficient GNN inference in real-time environments. Our
open-sourced templates will further support reproducibility and broader
adoption across scientific applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [70] [Locally Pareto-Optimal Interpretations for Black-Box Machine Learning Models](https://arxiv.org/abs/2508.15220)
*Aniruddha Joshi,Supratik Chakraborty,S Akshay,Shetal Shah,Hazem Torfah,Sanjit Seshia*

Main category: cs.LG

TL;DR: 提出了一种基于局部最优性保证的框架，用于合成黑盒机器学习模型的解释，解决了准确性与可解释性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成多目标解释时缺乏对Pareto最优性的保证，或面临可扩展性问题。

Method: 采用多目标学习或搜索技术生成候选解释，并通过SAT求解器验证局部最优性。

Result: 在基准测试中表现良好，接近全局最优保证方法的解释质量。

Conclusion: 该方法通过局部最优性验证提升了可扩展性，同时保持了解释的质量。

Abstract: Creating meaningful interpretations for black-box machine learning models
involves balancing two often conflicting objectives: accuracy and
explainability. Exploring the trade-off between these objectives is essential
for developing trustworthy interpretations. While many techniques for
multi-objective interpretation synthesis have been developed, they typically
lack formal guarantees on the Pareto-optimality of the results. Methods that do
provide such guarantees, on the other hand, often face severe scalability
limitations when exploring the Pareto-optimal space. To address this, we
develop a framework based on local optimality guarantees that enables more
scalable synthesis of interpretations. Specifically, we consider the problem of
synthesizing a set of Pareto-optimal interpretations with local optimality
guarantees, within the immediate neighborhood of each solution. Our approach
begins with a multi-objective learning or search technique, such as
Multi-Objective Monte Carlo Tree Search, to generate a best-effort set of
Pareto-optimal candidates with respect to accuracy and explainability. We then
verify local optimality for each candidate as a Boolean satisfiability problem,
which we solve using a SAT solver. We demonstrate the efficacy of our approach
on a set of benchmarks, comparing it against previous methods for exploring the
Pareto-optimal front of interpretations. In particular, we show that our
approach yields interpretations that closely match those synthesized by methods
offering global guarantees.

</details>


### [71] [Mini-Batch Robustness Verification of Deep Neural Networks](https://arxiv.org/abs/2508.15454)
*Saar Tzour-Shaday,Dana Drachsler Cohen*

Main category: cs.LG

TL;DR: 论文提出了一种名为BaVerLy的群组局部鲁棒性验证方法，通过动态构建和验证小批量输入，显著提升了神经网络图像分类器对对抗攻击的鲁棒性验证效率。


<details>
  <summary>Details</summary>
Motivation: 神经网络图像分类器在安全关键应用中广泛使用，但易受对抗攻击影响。当前局部鲁棒性验证方法分析时间长或精度不足，效果不佳。

Method: 提出群组局部鲁棒性验证方法BaVerLy，利用相似网络计算的ε-ball构建小批量，动态验证并通过结果指导进一步分析。

Result: 在MNIST和CIFAR-10数据集上的实验表明，BaVerLy平均提升了验证效率2.3倍，最高可达4.1倍，总分析时间从24小时减少到6小时。

Conclusion: BaVerLy通过群组验证显著提高了局部鲁棒性验证的效率，为大规模输入集的鲁棒性分析提供了可行方案。

Abstract: Neural network image classifiers are ubiquitous in many safety-critical
applications. However, they are susceptible to adversarial attacks. To
understand their robustness to attacks, many local robustness verifiers have
been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers
introduce a long analysis time or lose too much precision, making them less
effective for a large set of inputs. In this work, we propose a new approach to
local robustness: group local robustness verification. The key idea is to
leverage the similarity of the network computations of certain $\epsilon$-balls
to reduce the overall analysis time. We propose BaVerLy, a sound and complete
verifier that boosts the local robustness verification of a set of
$\epsilon$-balls by dynamically constructing and verifying mini-batches.
BaVerLy adaptively identifies successful mini-batch sizes, accordingly
constructs mini-batches of $\epsilon$-balls that have similar network
computations, and verifies them jointly. If a mini-batch is verified, all
$\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected
as not being robust, guiding the refinement. In the latter case, BaVerLy
leverages the analysis results to expedite the analysis of that $\epsilon$-ball
as well as the other $\epsilon$-balls in the batch. We evaluate BaVerLy on
fully connected and convolutional networks for MNIST and CIFAR-10. Results show
that BaVerLy scales the common one by one verification by 2.3x on average and
up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6
hours.

</details>


### [72] [Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications](https://arxiv.org/abs/2508.15008)
*Hamza A. Abushahla,Dara Varam,Ariel J. N. Panopio,Mohamed I. AlHajri*

Main category: cs.LG

TL;DR: 该调查报告系统性地综述了在资源受限的嵌入式设备上部署量化神经网络（QNNs）的技术、框架和平台，重点探讨了模型性能与硬件能力之间的权衡，并指出了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的微控制器上部署QNNs时，需要平衡模型性能、计算复杂性和内存限制。TinyML通过机器学习算法、硬件加速和软件优化的结合来解决这些问题。

Method: 硬件为中心的量化介绍，系统性地回顾了加速深度学习模型的量化技术，并评估了支持QNNs执行的软件框架和硬件平台。

Result: 深入分析了模型性能与硬件能力之间的关键权衡，总结了现有技术、框架和平台的优缺点。

Conclusion: 该领域仍存在挑战，但未来在QNNs部署方面有广阔的发展前景。

Abstract: The deployment of Quantized Neural Networks (QNNs) on resource-constrained
devices, such as microcontrollers, has introduced significant challenges in
balancing model performance, computational complexity and memory constraints.
Tiny Machine Learning (TinyML) addresses these issues by integrating
advancements across machine learning algorithms, hardware acceleration, and
software optimization to efficiently run deep neural networks on embedded
systems. This survey presents a hardware-centric introduction to quantization,
systematically reviewing essential quantization techniques employed to
accelerate deep learning models for embedded applications. In particular,
further emphasis is put on critical trade-offs among model performance and
hardware capabilities. The survey further evaluates existing software
frameworks and hardware platforms designed specifically for supporting QNN
execution on microcontrollers. Moreover, we provide an analysis of the current
challenges and an outline of promising future directions in the rapidly
evolving domain of QNN deployment.

</details>


### [73] [Quantum Long Short-term Memory with Differentiable Architecture Search](https://arxiv.org/abs/2508.14955)
*Samuel Yen-Chi Chen,Prayag Tiwari*

Main category: cs.LG

TL;DR: 提出了一种名为DiffQAS-QLSTM的可微分框架，用于优化量子循环模型的电路设计和参数选择，表现优于手工设计的基线。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在序列数据学习中表现出潜力，但设计有效的变分量子电路（VQCs）仍具挑战性且通常需针对特定任务调整。

Method: 提出DiffQAS-QLSTM，一种端到端的可微分框架，可在训练过程中同时优化VQC参数和架构选择。

Result: DiffQAS-QLSTM在多样化测试场景中表现优于手工设计的基线，损失更低。

Conclusion: 该框架为可扩展和自适应的量子序列学习开辟了新途径。

Abstract: Recent advances in quantum computing and machine learning have given rise to
quantum machine learning (QML), with growing interest in learning from
sequential data. Quantum recurrent models like QLSTM are promising for
time-series prediction, NLP, and reinforcement learning. However, designing
effective variational quantum circuits (VQCs) remains challenging and often
task-specific. To address this, we propose DiffQAS-QLSTM, an end-to-end
differentiable framework that optimizes both VQC parameters and architecture
selection during training. Our results show that DiffQAS-QLSTM consistently
outperforms handcrafted baselines, achieving lower loss across diverse test
settings. This approach opens the door to scalable and adaptive quantum
sequence learning.

</details>


### [74] [TOAST: Fast and scalable auto-partitioning based on principled static analysis](https://arxiv.org/abs/2508.15010)
*Sami Alabed,Dominik Grewe,Norman Alexander Rink,Timur Sitdikov,Agnieszka Swietlik,Dimitrios Vytiniotis,Daniel Belov*

Main category: cs.LG

TL;DR: 提出了一种结合静态编译器分析和蒙特卡洛树搜索的系统，用于优化大型机器学习模型在分布式加速器系统中的分区问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动分区器在探索庞大的分区空间时存在内存不足或搜索速度过慢的问题，且因人为限制搜索空间常导致不可行或次优解。

Method: 通过静态编译器分析构建高效的决策空间，结合蒙特卡洛树搜索来优化分区。

Result: 系统在多样化的硬件平台和模型架构上显著优于现有工业方法，发现了更优的解决方案。

Conclusion: 该系统能够全自动化地为复杂大型模型找到高性能的分区方案。

Abstract: Partitioning large machine learning models across distributed accelerator
systems is a complex process, requiring a series of interdependent decisions
that are further complicated by internal sharding ambiguities. Consequently,
existing auto-partitioners often suffer from out-of-memory errors or are
prohibitively slow when exploring the exponentially large space of possible
partitionings. To mitigate this, they artificially restrict the search space,
but this approach frequently yields infeasible solutions that violate device
memory constraints or lead to sub-optimal performance.
  We propose a system that combines a novel static compiler analysis with a
Monte Carlo Tree Search. Our analysis constructs an efficient decision space by
identifying (i) tensor dimensions requiring identical sharding, and (ii)
partitioning "conflicts" that require resolution.
  Our system significantly outperforms state-of-the-art industrial methods
across diverse hardware platforms and model architectures, discovering
previously unknown, superior solutions, and the process is fully automated even
for complex and large models.

</details>


### [75] [A Solvable Molecular Switch Model for Stable Temporal Information Processing](https://arxiv.org/abs/2508.15451)
*H. I. Nurdin,C. A. Nijhuis*

Main category: cs.LG

TL;DR: 研究了一种输入驱动的单状态微分方程模型，展示了其在生物启发行为和数学稳定性上的共存，支持其在神经形态计算中的应用。


<details>
  <summary>Details</summary>
Motivation: 开发一种兼具生物启发行为和数学稳定性的模型，以支持动态分子开关在神经形态计算中的应用。

Method: 使用线性状态和非线性输入的微分方程模型，并验证其收敛性和记忆衰减特性。

Result: 模型能够稳定处理时变输入，并展示了在深层前馈和循环架构中的潜力。

Conclusion: 模型为神经形态计算提供了理论支持，并可能启发更多可求解模型的开发。

Abstract: This paper studies an input-driven one-state differential equation model
initially developed for an experimentally demonstrated dynamic molecular switch
that switches like synapses in the brain do. The linear-in-the-state and
nonlinear-in-the-input model is exactly solvable, and it is shown that it also
possesses mathematical properties of convergence and fading memory that enable
stable processing of time-varying inputs by nonlinear dynamical systems. Thus,
the model exhibits the co-existence of biologically-inspired behavior and
desirable mathematical properties for stable learning on sequential data. The
results give theoretical support for the use of the dynamic molecular switches
as computational units in deep cascaded/layered feedforward and recurrent
architectures as well as other more general structures for neuromorphic
computing. They could also inspire more general exactly solvable models that
can be fitted to emulate arbitrary physical devices which can mimic
brain-inspired behaviour and perform stable computation on input signals.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [76] [Image-Conditioned 3D Gaussian Splat Quantization](https://arxiv.org/abs/2508.15372)
*Xinshuang Liu,Runfa Blark Li,Keito Suzuki,Truong Nguyen*

Main category: cs.CV

TL;DR: ICGS-Quantizer 是一种改进的 3DGS 压缩方法，通过联合利用高斯和属性相关性及共享码书，显著提升压缩效率，并支持场景变化后的适应性更新。


<details>
  <summary>Details</summary>
Motivation: 解决现有 3DGS 压缩方法在大规模场景或长期存档中存储需求高且无法适应场景变化的问题。

Method: 提出 ICGS-Quantizer，通过共享码书和条件解码技术，联合优化压缩和适应性。

Result: 实验证明其在压缩效率和场景更新适应性上优于现有方法，存储需求降至千字节级别。

Conclusion: ICGS-Quantizer 在 3D 场景压缩和更新中表现出色，为长期存档提供了高效且灵活的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for
enabling high-quality real-time rendering. Although 3DGS compression methods
have been proposed for deployment on storage-constrained devices, two
limitations hinder archival use: (1) they compress medium-scale scenes only to
the megabyte range, which remains impractical for large-scale scenes or
extensive scene collections; and (2) they lack mechanisms to accommodate scene
changes after long-term archival. To address these limitations, we propose an
Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially
enhances compression efficiency and provides adaptability to scene changes
after archiving. ICGS-Quantizer improves quantization efficiency by jointly
exploiting inter-Gaussian and inter-attribute correlations and by using shared
codebooks across all training scenes, which are then fixed and applied to
previously unseen test scenes, eliminating the overhead of per-scene codebooks.
This approach effectively reduces the storage requirements for 3DGS to the
kilobyte range while preserving visual fidelity. To enable adaptability to
post-archival scene changes, ICGS-Quantizer conditions scene decoding on images
captured at decoding time. The encoding, quantization, and decoding processes
are trained jointly, ensuring that the codes, which are quantized
representations of the scene, are effective for conditional decoding. We
evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating.
Experimental results show that ICGS-Quantizer consistently outperforms
state-of-the-art methods in compression efficiency and adaptability to scene
changes. Our code, model, and data will be publicly available on GitHub.

</details>


### [77] [Scaling Group Inference for Diverse and High-Quality Generation](https://arxiv.org/abs/2508.15773)
*Gaurav Parmar,Or Patashnik,Daniil Ostashev,Kuan-Chieh Wang,Kfir Aberman,Srinivasa Narasimhan,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出了一种可扩展的群体推理方法，通过优化样本质量和多样性，解决了生成模型中独立采样导致的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型独立采样导致的结果冗余问题，提升用户在多样本选择时的体验。

Method: 将群体推理建模为二次整数分配问题，通过逐步剪枝候选集提高效率。

Result: 在多样性和质量上显著优于独立采样基线和其他推理算法，适用于多种生成任务。

Conclusion: 该方法使生成模型能够将多输出视为一个整体而非独立样本，具有广泛应用潜力。

Abstract: Generative models typically sample outputs independently, and recent
inference-time guidance and scaling algorithms focus on improving the quality
of individual samples. However, in real-world applications, users are often
presented with a set of multiple images (e.g., 4-8) for each prompt, where
independent sampling tends to lead to redundant results, limiting user choices
and hindering idea exploration. In this work, we introduce a scalable group
inference method that improves both the diversity and quality of a group of
samples. We formulate group inference as a quadratic integer assignment
problem: candidate outputs are modeled as graph nodes, and a subset is selected
to optimize sample quality (unary term) while maximizing group diversity
(binary term). To substantially improve runtime efficiency, we progressively
prune the candidate set using intermediate predictions, allowing our method to
scale up to large candidate sets. Extensive experiments show that our method
significantly improves group diversity and quality compared to independent
sampling baselines and recent inference algorithms. Our framework generalizes
across a wide range of tasks, including text-to-image, image-to-image, image
prompting, and video generation, enabling generative models to treat multiple
outputs as cohesive groups rather than independent samples.

</details>


### [78] [Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments](https://arxiv.org/abs/2508.15158)
*Md. Nurul Absur,Abhinav Kumar,Swastik Brahma,Saptarshi Debroy*

Main category: cs.CV

TL;DR: 提出了一种基于投资组合理论的边缘资源管理策略，用于保证多视角3D重建在系统中断情况下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 多视角3D重建在紧急响应等场景中需要实时性，但边缘环境的动态性和复杂性可能导致重建质量下降，因此需要一种可靠的资源管理策略。

Method: 采用投资组合理论优化边缘资源分配，并使用遗传算法快速求解，确保在相机可能发生时空相关中断时仍能满足重建质量要求。

Result: 通过公开和自定义3D数据集验证，该方法相比传统基线策略，能够在时空中断下保证更可靠的3D重建质量。

Conclusion: 提出的策略在动态边缘环境中表现优于传统方法，为多视角3D重建提供了更高的可靠性。

Abstract: Multi-view 3D reconstruction applications are revolutionizing critical use
cases that require rapid situational-awareness, such as emergency response,
tactical scenarios, and public safety. In many cases, their near-real-time
latency requirements and ad-hoc needs for compute resources necessitate
adoption of `Just-in-time' edge environments where the system is set up on the
fly to support the applications during the mission lifetime. However,
reliability issues can arise from the inherent dynamism and operational
adversities of such edge environments, resulting in spatiotemporally correlated
disruptions that impact the camera operations, which can lead to sustained
degradation of reconstruction quality. In this paper, we propose a novel
portfolio theory inspired edge resource management strategy for reliable
multi-view 3D reconstruction against possible system disruptions. Our proposed
methodology can guarantee reconstruction quality satisfaction even when the
cameras are prone to spatiotemporally correlated disruptions. The portfolio
theoretic optimization problem is solved using a genetic algorithm that
converges quickly for realistic system settings. Using publicly available and
customized 3D datasets, we demonstrate the proposed camera selection strategy's
benefits in guaranteeing reliable 3D reconstruction against traditional
baseline strategies, under spatiotemporal disruptions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [79] [Collaborative Filtering using Variational Quantum Hopfield Associative Memory](https://arxiv.org/abs/2508.14906)
*Amir Kermanshahani,Ebrahim Ardeshir-Larijani,Rakesh Saini,Saif Al-Kuwari*

Main category: cs.IR

TL;DR: 该论文提出了一种混合推荐系统，结合量子Hopfield联想记忆（QHAM）与深度神经网络，用于改进MovieLens 1M数据集上的提取和分类性能，并在理想和噪声环境下均表现出色。


<details>
  <summary>Details</summary>
Motivation: 量子计算因其比经典系统更快的计算能力，在机器学习和推荐系统等领域有广泛应用潜力。论文旨在通过结合量子计算和深度学习技术，提升推荐系统的性能。

Method: 使用K-Means算法将用户分为多个独特群组，并通过编码器将这些群组转换为极性模式。随后，将其集成到基于变分QHAM的混合推荐模型中，在理想和噪声环境下进行训练。

Result: 在理想环境中，模型ROC值为0.9795，准确度为0.8841；在噪声环境中，ROC为0.9177，准确度为0.8013，表现出稳定的性能。

Conclusion: 论文提出的框架结合了变分量计算和深度学习，能够处理真实数据集，并在噪声环境中表现优异，为推荐系统的未来应用提供了有前景的方向。

Abstract: Quantum computing, with its ability to do exponentially faster computation
compared to classical systems, has found novel applications in various fields
such as machine learning and recommendation systems. Quantum Machine Learning
(QML), which integrates quantum computing with machine learning techniques,
presents powerful new tools for data processing and pattern recognition. This
paper proposes a hybrid recommendation system that combines Quantum Hopfield
Associative Memory (QHAM) with deep neural networks to improve the extraction
and classification on the MovieLens 1M dataset. User archetypes are clustered
into multiple unique groups using the K-Means algorithm and converted into
polar patterns through the encoder's activation function. These polar patterns
are then integrated into the variational QHAM-based hybrid recommendation
model. The system was trained using the MSE loss over 35 epochs in an ideal
environment, achieving an ROC value of 0.9795, an accuracy of 0.8841, and an
F-1 Score of 0.8786. Trained with the same number of epochs in a noisy
environment using a custom Qiskit AER noise model incorporating bit-flip and
readout errors with the same probabilities as in real quantum hardware, it
achieves an ROC of 0.9177, an accuracy of 0.8013, and an F-1 Score equal to
0.7866, demonstrating consistent performance.
  Additionally, we were able to optimize the qubit overhead present in previous
QHAM architectures by efficiently updating only one random targeted qubit. This
research presents a novel framework that combines variational quantum computing
with deep learning, capable of dealing with real-world datasets with comparable
performance compared to purely classical counterparts. Additionally, the model
can perform similarly well in noisy configurations, showcasing a steady
performance and proposing a promising direction for future usage in
recommendation systems.

</details>


### [80] [On the Effectiveness of Graph Reordering for Accelerating Approximate Nearest Neighbor Search on GPU](https://arxiv.org/abs/2508.15436)
*Yutaro Oguri,Mai Nishimura,Yusuke Matsui*

Main category: cs.IR

TL;DR: 本文首次系统研究了GPU上基于图的近似最近邻搜索（ANNS）中图重排序的效果，提出了一个统一的评估框架和针对性优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的ANNS研究侧重于算法创新，而忽视了内存布局对执行时间的显著影响。

Method: 通过图适配器将不同图拓扑转换为通用表示，并开发GPU优化的图遍历引擎，评估多样化的重排序策略。

Result: 针对GPU的重排序策略在保持搜索精度的同时，实现了高达15%的QPS提升。

Conclusion: 内存布局优化与现有算法创新相互独立，可为性能提升提供额外空间。

Abstract: We present the first systematic investigation of graph reordering effects for
graph-based Approximate Nearest Neighbor Search (ANNS) on a GPU. While
graph-based ANNS has become the dominant paradigm for modern AI applications,
recent approaches focus on algorithmic innovations while neglecting memory
layout considerations that significantly affect execution time. Our unified
evaluation framework enables comprehensive evaluation of diverse reordering
strategies across different graph indices through a graph adapter that converts
arbitrary graph topologies into a common representation and a GPU-optimized
graph traversal engine. We conduct a comprehensive analysis across diverse
datasets and state-of-the-art graph indices, introducing analysis metrics that
quantify the relationship between structural properties and memory layout
effectiveness. Our GPU-targeted reordering achieves up to 15$\%$ QPS
improvements while preserving search accuracy, demonstrating that memory layout
optimization operates orthogonally to existing algorithmic innovations. We will
release all code upon publication to facilitate reproducibility and foster
further research.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [81] [Optimizing Compilation for Distributed Quantum Computing via Clustering and Annealing](https://arxiv.org/abs/2508.15267)
*Ruilin Zhou,Jinglei Cheng,Yuhang Gan,Junyu Liu,Chen Qian*

Main category: quant-ph

TL;DR: 论文提出了一种针对异构分布式量子计算的编译框架，通过利用量子电路的结构模式、聚类初始量子位放置和退火算法优化映射，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 解决异构量子处理单元（QPUs）在分布式量子计算（DQC）中量子程序映射的挑战。

Method: 提出编译框架，包括利用电路结构模式、聚类初始量子位放置和退火算法调整映射。

Result: 实验表明方法有效，比基线最多减少88.40%的目标值。

Conclusion: 该框架能高效处理复杂异构分布式量子系统。

Abstract: Efficiently mapping quantum programs onto Distributed quantum computing (DQC)
are challenging, particularly when considering the heterogeneous quantum
processing units (QPUs) with different structures. In this paper, we present a
comprehensive compilation framework that addresses these challenges with three
key insights: exploiting structural patterns within quantum circuits, using
clustering for initial qubit placement, and adjusting qubit mapping with
annealing algorithms. Experimental results demonstrate the effectiveness of our
methods and the capability to handle complex heterogeneous distributed quantum
systems. Our evaluation shows that our method reduces the objective value at
most 88.40\% compared to the baseline.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [82] [Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging](https://arxiv.org/abs/2508.14931)
*Zahra TehraniNasab,Amar Kumar,Tal Arbel*

Main category: eess.IV

TL;DR: 论文探讨了如何通过微调扩散模型提高高分辨率（512x512像素）图像生成质量，比较了多种微调方法对生成质量的影响，并评估了生成图像在数据稀缺下游任务中的实用性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像合成在许多应用（尤其是医学影像领域）中日益重要，但现有研究多局限于低分辨率。微调成为适应预训练模型的关键机制。

Method: 系统研究了多种微调技术（包括全微调和参数高效微调），分析了它们对生成质量指标（如FID、Vendi分数和提示-图像对齐）的影响，并评估了下游分类任务的性能。

Result: 特定微调策略不仅提高了生成图像的保真度，还改善了在真实图像上训练和评估分类器时的下游性能。

Conclusion: 研究表明，针对高分辨率图像合成的微调方法在提升生成质量和下游任务性能方面具有显著效果，代码已公开。

Abstract: Advancements in diffusion-based foundation models have improved text-to-image
generation, yet most efforts have been limited to low-resolution settings. As
high-resolution image synthesis becomes increasingly essential for various
applications, particularly in medical imaging domains, fine-tuning emerges as a
crucial mechanism for adapting these powerful pre-trained models to
task-specific requirements and data distributions. In this work, we present a
systematic study, examining the impact of various fine-tuning techniques on
image generation quality when scaling to high resolution 512x512 pixels. We
benchmark a diverse set of fine-tuning methods, including full fine-tuning
strategies and parameter-efficient fine-tuning (PEFT). We dissect how different
fine-tuning methods influence key quality metrics, including Fr\'echet
Inception Distance (FID), Vendi score, and prompt-image alignment. We also
evaluate the utility of generated images in a downstream classification task
under data-scarce conditions, demonstrating that specific fine-tuning
strategies improve both generation fidelity and downstream performance when
synthetic images are used for classifier training and evaluation on real
images. Our code is accessible through the project website -
https://tehraninasab.github.io/PixelUPressure/.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [83] [LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa](https://arxiv.org/abs/2508.15110)
*Graham Hill,JingYuan Gong,Thulani Babeli,Moseli Mots'oehli,James Gachomo Wanjiku*

Main category: cs.CE

TL;DR: AI（尤其是大语言模型和代理AI）在保险业具有变革潜力，非洲市场存在独特机遇与挑战，需多方合作推动包容性AI解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索AI在保险业的应用潜力，特别关注非洲市场的独特需求和机遇。

Method: 分析AI在保险业的机会与挑战，识别非洲市场的差距和本地努力。

Result: 提出需要多方合作以制定包容、可持续的AI策略。

Conclusion: 呼吁各方合作，为非洲市场开发定制化AI解决方案。

Abstract: In this work, we highlight the transformative potential of Artificial
Intelligence (AI), particularly Large Language Models (LLMs) and agentic AI, in
the insurance sector. We consider and emphasize the unique opportunities,
challenges, and potential pathways in insurance amid rapid performance
improvements, increased open-source access, decreasing deployment costs, and
the complexity of LLM or agentic AI frameworks. To bring it closer to home, we
identify critical gaps in the African insurance market and highlight key local
efforts, players, and partnership opportunities. Finally, we call upon
actuaries, insurers, regulators, and tech leaders to a collaborative effort
aimed at creating inclusive, sustainable, and equitable AI strategies and
solutions: by and for Africans.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models](https://arxiv.org/abs/2508.15357)
*Haji Gul,Abul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.CL

TL;DR: 论文提出了一个统一的元度量标准EDAS，用于综合评估知识图谱补全（KGC）模型在多数据集和多指标下的性能，解决了现有评估方法不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有KGC模型评估方法在不同数据集和指标下表现不一致，导致难以全面比较模型性能，需要一个统一的评估框架。

Method: 提出基于平均解距离的EDAS元度量标准，整合多数据集和多指标的性能，生成一个标准化分数。

Result: 在FB15k-237和WN18RR等基准数据集上的实验表明，EDAS能有效统一模型排名，提供一致的评估结果。

Conclusion: EDAS为KGC模型提供了一个全面、鲁棒且可解释的评估框架，支持更公平的跨数据集比较。

Abstract: Knowledge Graphs (KGs) enable applications in various domains such as
semantic search, recommendation systems, and natural language processing. KGs
are often incomplete, missing entities and relations, an issue addressed by
Knowledge Graph Completion (KGC) methods that predict missing elements.
Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank
(MR), and Hit@k, are commonly used to assess the performance of such KGC
models. A major challenge in evaluating KGC models, however, lies in comparing
their performance across multiple datasets and metrics. A model may outperform
others on one dataset but underperform on another, making it difficult to
determine overall superiority. Moreover, even within a single dataset,
different metrics such as MRR and Hit@1 can yield conflicting rankings, where
one model excels in MRR while another performs better in Hit@1, further
complicating model selection for downstream tasks. These inconsistencies hinder
holistic comparisons and highlight the need for a unified meta-metric that
integrates performance across all metrics and datasets to enable a more
reliable and interpretable evaluation framework. To address this need, we
propose KG Evaluation based on Distance from Average Solution (EDAS), a robust
and interpretable meta-metric that synthesizes model performance across
multiple datasets and diverse evaluation criteria into a single normalized
score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated
aspects of performance, EDAS offers a global perspective that supports more
informed model selection and promotes fairness in cross-dataset evaluation.
Experimental results on benchmark datasets such as FB15k-237 and WN18RR
demonstrate that EDAS effectively integrates multi-metric, multi-dataset
performance into a unified ranking, offering a consistent, robust, and
generalizable framework for evaluating KGC models.

</details>


### [85] [SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version](https://arxiv.org/abs/2508.15478)
*Nghiem Thanh Pham,Tung Kieu,Duc-Manh Nguyen,Son Ha Xuan,Nghia Duong-Trung,Danh Le-Phuoc*

Main category: cs.CL

TL;DR: SLM-Bench是首个专门评估小型语言模型（SLM）性能和环境影响的基准测试，涵盖准确性、计算效率和可持续性指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对小型语言模型的系统性评估，尤其是其性能和环境影响之间的权衡。

Method: 使用23个数据集在4种硬件配置上评估15个SLM，量化11项指标，确保公平比较。

Result: 不同SLM在准确性和能源效率上有显著差异，SLM-Bench为未来的研究提供了标准化评估工具。

Conclusion: SLM-Bench填补了资源效率和实际应用之间的评估空白，为SLM的多维度评估设定了新标准。

Abstract: Small Language Models (SLMs) offer computational efficiency and
accessibility, yet a systematic evaluation of their performance and
environmental impact remains lacking. We introduce SLM-Bench, the first
benchmark specifically designed to assess SLMs across multiple dimensions,
including accuracy, computational efficiency, and sustainability metrics.
SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14
domains. The evaluation is conducted on 4 hardware configurations, providing a
rigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench
quantifies 11 metrics across correctness, computation, and consumption,
enabling a holistic assessment of efficiency trade-offs. Our evaluation
considers controlled hardware conditions, ensuring fair comparisons across
models. We develop an open-source benchmarking pipeline with standardized
evaluation protocols to facilitate reproducibility and further research. Our
findings highlight the diverse trade-offs among SLMs, where some models excel
in accuracy while others achieve superior energy efficiency. SLM-Bench sets a
new standard for SLM evaluation, bridging the gap between resource efficiency
and real-world applicability.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [86] [Neural Robot Dynamics](https://arxiv.org/abs/2508.15755)
*Jie Xu,Eric Heiden,Iretiayo Akinola,Dieter Fox,Miles Macklin,Yashraj Narang*

Main category: cs.RO

TL;DR: NeRD提出了一种可泛化的神经模拟器，通过机器人中心和空间不变的表示，解决了现有神经模拟器泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现代机器人高自由度和复杂机制使得准确高效模拟成为挑战，现有神经模拟器需要专门训练且泛化能力不足。

Method: 提出NeRD，学习机器人特定的动力学模型，用于预测刚性关节体的未来状态，并替换传统模拟器中的低层动力学和接触求解器。

Result: NeRD在数千步模拟中稳定准确，能泛化到新任务和环境，支持仅在神经引擎中学习策略，并可从现实数据微调以减少仿真与现实差距。

Conclusion: NeRD为机器人模拟提供了一种可泛化、可微调的神经动力学模型，显著提升了模拟的准确性和适应性。

Abstract: Accurate and efficient simulation of modern robots remains challenging due to
their high degrees of freedom and intricate mechanisms. Neural simulators have
emerged as a promising alternative to traditional analytical simulators,
capable of efficiently predicting complex dynamics and adapting to real-world
data; however, existing neural simulators typically require
application-specific training and fail to generalize to novel tasks and/or
environments, primarily due to inadequate representations of the global state.
In this work, we address the problem of learning generalizable neural
simulators for robots that are structured as articulated rigid bodies. We
propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models
for predicting future states for articulated rigid bodies under contact
constraints. NeRD uniquely replaces the low-level dynamics and contact solvers
in an analytical simulator and employs a robot-centric and spatially-invariant
simulation state representation. We integrate the learned NeRD models as an
interchangeable backend solver within a state-of-the-art robotics simulator. We
conduct extensive experiments to show that the NeRD simulators are stable and
accurate over a thousand simulation steps; generalize across tasks and
environment configurations; enable policy learning exclusively in a neural
engine; and, unlike most classical simulators, can be fine-tuned from
real-world data to bridge the gap between simulation and reality.

</details>
