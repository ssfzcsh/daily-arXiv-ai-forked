<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CG](#cs.CG) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Systematic Literature Review on Continuous Integration and Deployment (CI/CD) for Secure Cloud Computing](https://arxiv.org/abs/2506.08055)
*Sabbir M. Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: 该论文通过系统文献综述分析了云环境中持续软件工程（CI/CD）的安全问题，总结了工具、方法和挑战，并指出研究空白。


<details>
  <summary>Details</summary>
Motivation: 随着云环境的普及，网络安全成为重要议题。行业、医疗和政府等领域频繁遭遇网络攻击，亟需系统研究以填补安全研究空白。

Method: 通过系统文献综述（SLR）分析了66篇论文，总结了云环境中CI/CD安全相关的工具（如Harbor、SonarQube）和挑战（如图像篡改、弱认证）。

Result: 研究发现现有工具和实践在解决CI/CD管道安全问题方面存在不足，揭示了进一步研究的必要性。

Conclusion: 论文呼吁更多研究以改进云环境中的安全解决方案，填补CI/CD安全的研究空白。

Abstract: As cloud environments become widespread, cybersecurity has emerged as a top
priority across areas such as networks, communication, data privacy, response
times, and availability. Various sectors, including industries, healthcare, and
government, have recently faced cyberattacks targeting their computing systems.
Ensuring secure app deployment in cloud environments requires substantial
effort. With the growing interest in cloud security, conducting a systematic
literature review (SLR) is critical to identifying research gaps. Continuous
Software Engineering, which includes continuous integration (CI), delivery
(CDE), and deployment (CD), is essential for software development and
deployment. In our SLR, we reviewed 66 papers, summarising tools, approaches,
and challenges related to the security of CI/CD in the cloud. We addressed key
aspects of cloud security and CI/CD and reported on tools such as Harbor,
SonarQube, and GitHub Actions. Challenges such as image manipulation,
unauthorised access, and weak authentication were highlighted. The review also
uncovered research gaps in how tools and practices address these security
issues in CI/CD pipelines, revealing a need for further study to improve
cloud-based security solutions.

</details>


### [2] [A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.08153)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 该研究旨在通过基于指标的架构模型来管理和评估机器学习赋能系统（MLES）的复杂性，目标是支持架构决策并提供系统设计和发展的指南。


<details>
  <summary>Details</summary>
Motivation: 探讨复杂性对机器学习赋能系统（MLES）的影响，并寻找有效的管理方法。

Method: 扩展参考架构以描述MLES并收集相关指标，作为构建基于指标的架构模型的第一步。

Result: 提出了一个扩展的参考架构，用于量化和分析MLES的复杂性。

Conclusion: 研究为MLES的复杂性管理奠定了基础，未来可进一步发展完整的指标模型以支持架构决策。

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper showcases the first step for
creating the metrics-based architectural model: an extension of a reference
architecture that can describe MLES to collect their metrics.

</details>


### [3] [Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models](https://arxiv.org/abs/2506.08171)
*Daniel Koh,Yannic Noller,Corina S. Pasareanu,Adrians Skapars,Youcheng Sun*

Main category: cs.SE

TL;DR: 该论文研究了大型语言模型（LLMs）在符号推理任务中的表现，特别是针对程序的最坏情况执行约束分析，通过SMT约束求解和专门设计的符号约束数据集优化模型能力。实验显示，3B规模的WARP-1.0模型能超越更大规模的基线模型。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在复杂符号推理任务中的应用潜力，特别是程序的最坏情况分析，以弥补LLMs与符号推理方法之间的鸿沟。

Method: 定义最坏情况符号约束分析任务，通过SMT约束求解和专门设计的符号约束数据集进行模型微调，评估和改进LLMs能力。

Result: 实验证明3B规模的WARP-1.0模型优于同规模及更大规模的基线模型，能够通过强化学习方法恢复算法最坏行为的约束条件。

Conclusion: LLMs具备深层符号推理能力，支持神经网络学习与形式化方法在程序分析中的进一步结合。

Abstract: Large language models (LLMs) have been successfully applied to a variety of
coding tasks, including code generation, completion, and repair. However, more
complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper
investigates the capacity of LLMs to reason about worst-case executions in
programs through symbolic constraints analysis, aiming to connect LLMs and
symbolic reasoning approaches. Specifically, we define and address the problem
of worst-case symbolic constraints analysis as a measure to assess the
comprehension of LLMs. We evaluate the performance of existing LLMs on this
novel task and further improve their capabilities through symbolic
reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)
constraint solving and supported by a specially designed dataset of symbolic
constraints. Experimental results show that our solver-aligned model,
WARP-1.0-3B, consistently surpasses size-matched and even much larger
baselines, demonstrating that a 3B LLM can recover the very constraints that
pin down an algorithm's worst-case behaviour through reinforcement learning
methods. These findings suggest that LLMs are capable of engaging in deeper
symbolic reasoning, supporting a closer integration between neural
network-based learning and formal methods for rigorous program analysis.

</details>


### [4] [Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles](https://arxiv.org/abs/2506.08173)
*Nguyen Phu Vinh,Anh Chung Hoang,Chris Ngo,Truong-Son Hy*

Main category: cs.SE

TL;DR: Repeton是一个基于LLM的开源框架，通过分步的补丁与测试流程，提高代码操作的精确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂软件工程任务中精度和可解释性不足，因此开发Repeton以解决这一问题。

Method: 采用分步诊断、提出代码更改并通过自动化测试验证的流程，结合轻量级启发式方法和开发工具。

Result: 在SWE-bench Lite基准测试中，Repeton的表现优于基于RAG的方法，特别是在补丁有效性和可解释性方面。

Conclusion: Repeton通过模块化和可验证的任务分解，为可扩展且透明的自主调试提供了实用路径。

Abstract: Large Language Models (LLMs) have shown strong capabilities in code
generation and comprehension, yet their application to complex software
engineering tasks often suffers from low precision and limited
interpretability. We present Repeton, a fully open-source framework that
leverages LLMs for precise and automated code manipulation in real-world Git
repositories. Rather than generating holistic fixes, Repeton operates through a
structured patch-and-test pipeline: it iteratively diagnoses issues, proposes
code changes, and validates each patch through automated testing. This stepwise
process is guided by lightweight heuristics and development tools, avoiding
reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite
benchmark, our method shows good performance compared to RAG-based methods in
both patch validity and interpretability. By decomposing software engineering
tasks into modular, verifiable stages, Repeton provides a practical path toward
scalable and transparent autonomous debugging.

</details>


### [5] [MBTModelGenerator: A software tool for reverse engineering of Model-based Testing (MBT) models from clickstream data of web applications](https://arxiv.org/abs/2506.08179)
*Sasidhar Matta,Vahid Garousi*

Main category: cs.SE

TL;DR: 论文提出了一个开源工具，通过用户点击流数据自动生成基于模型的测试（MBT）模型，降低了MBT应用的门槛。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程中测试模型和套件创建的高成本问题。

Method: 使用点击流数据生成状态转换模型，并兼容GraphWalker MBT工具。

Result: 工具成功实现了自动生成MBT模型，减少了手动建模的依赖。

Conclusion: 该工具通过利用实际用户行为降低了MBT的采用难度，并公开为开源项目。

Abstract: Automated testing has become a standard practice in software engineering, yet
the creation of test models and suites remains labor-intensive. To reduce this
effort, we developed an open-source tool that automatically generates
Model-Based Testing (MBT) models from clickstream data collected during user
interaction with web applications. The tool captures UI events, transforms them
into state-transition models, and exports the result in a format compatible
with the GraphWalker MBT tool. This enables immediate test execution without
the need for manual model creation. The approach lowers the barrier to MBT
adoption by leveraging actual usage behavior and reducing the reliance on
upfront modeling. This technical report documents the system requirements,
design decisions, implementation details, testing process, and empirical
evaluation of the tool, which is publicly available as open-source.

</details>


### [6] [Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study](https://arxiv.org/abs/2506.08311)
*Ira Ceka,Saurabh Pujar,Shyam Ramji,Luca Buratti,Gail Kaiser,Baishakhi Ray*

Main category: cs.SE

TL;DR: 该论文系统地研究了软件工程代理（SWE代理）的决策行为，通过执行轨迹提出了首个分类法，并深入分析了核心组件及其对代理成功的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的出现，SWE代理在自动化软件任务中表现出强大能力，但其内部决策机制尚不清晰，本研究旨在揭示这些机制以提高代理的可靠性和效率。

Method: 研究通过执行轨迹分析，提出了首个SWE代理决策路径分类法，并深入研究了三个核心组件：错误定位、补丁生成和测试生成，同时对代理生成与开发者补丁进行了克隆分析。

Result: 研究发现测试生成对补丁成功率有显著影响，并揭示了代理生成补丁与开发者补丁在结构和风格上的差异。

Conclusion: 研究为代理设计提供了新见解，有助于开发更有效且更符合人类开发习惯的代理。

Abstract: With the advent of large language models (LLMs), software engineering agents
(SWE agents) have emerged as a powerful paradigm for automating a range of
software tasks -- from code generation and repair to test case synthesis. These
agents operate autonomously by interpreting user input and responding to
environmental feedback. While various agent architectures have demonstrated
strong empirical performance, the internal decision-making worfklows that drive
their behavior remain poorly understood. Deeper insight into these workflows
hold promise for improving both agent reliability and efficiency. In this work,
we present the first systematic study of SWE agent behavior through the lens of
execution traces. Our contributions are as follows: (1) we propose the first
taxonomy of decision-making pathways across five representative agents; (2)
using this taxonomy, we identify three core components essential to agent
success -- bug localization, patch generation, and reproduction test generation
-- and study each in depth; (3) we study the impact of test generation on
successful patch production; and analyze strategies that can lead to successful
test generation; (4) we further conduct the first large-scale code clone
analysis comparing agent-generated and developer-written patches and provide a
qualitative study revealing structural and stylistic differences in patch
content. Together, these findings offer novel insights into agent design and
open avenues for building agents that are both more effective and more aligned
with human development practices.

</details>


### [7] [Detecting State Manipulation Vulnerabilities in Smart Contracts Using LLM and Static Analysis](https://arxiv.org/abs/2506.08561)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: 本文提出了一种名为PriceSleuth的新方法，结合大语言模型（LLM）和静态分析，用于主动检测DeFi智能合约中的价格操纵攻击。


<details>
  <summary>Details</summary>
Motivation: 随着DeFi协议的普及，匿名用户间的交易增加，价格变量成为攻击者常用的操纵目标，以获取非法利润。因此，需要一种有效的方法来检测此类攻击。

Method: PriceSleuth通过识别与价格计算相关的核心逻辑函数，引导LLM定位代码语句，并执行价格变量的依赖分析和传播分析，以检测潜在的价格操纵。

Result: 初步实验结果验证了PriceSleuth的有效性。

Conclusion: PriceSleuth展示了一种结合LLM和静态分析的新方法，为未来研究提供了方向。

Abstract: An increasing number of DeFi protocols are gaining popularity, facilitating
transactions among multiple anonymous users. State Manipulation is one of the
notorious attacks in DeFi smart contracts, with price variable being the most
commonly exploited state variable-attackers manipulate token prices to gain
illicit profits. In this paper, we propose PriceSleuth, a novel method that
leverages the Large Language Model (LLM) and static analysis to detect Price
Manipulation (PM) attacks proactively. PriceSleuth firstly identifies core
logic function related to price calculation in DeFi contracts. Then it guides
LLM to locate the price calculation code statements. Secondly, PriceSleuth
performs backward dependency analysis of price variables, instructing LLM in
detecting potential price manipulation. Finally, PriceSleuth utilizes
propagation analysis of price variables to assist LLM in detecting whether
these variables are maliciously exploited. We presented preliminary
experimental results to substantiate the effectiveness of PriceSleuth . And we
outline future research directions for PriceSleuth.

</details>


### [8] [Evaluating the Performance and Efficiency of Sentence-BERT for Code Comment Classification](https://arxiv.org/abs/2506.08581)
*Fabian C. Peña,Steffen Herbold*

Main category: cs.SE

TL;DR: 评估Sentence-BERT在多标签代码注释分类任务中的性能与效率平衡。


<details>
  <summary>Details</summary>
Motivation: 研究如何在分类性能与推理效率之间找到最佳平衡。

Method: 使用13,216条标记的注释句子数据集，微调Sentence-BERT并结合不同分类头进行实验。

Result: 大模型F1更高，但小模型效率更优；最终找到性能提升（F1 +0.0346）与效率损失（运行时间+1.4倍，GFLOPS+2.1倍）的平衡点。

Conclusion: 证明了在高效推理约束下微调Sentence-BERT的可行性，并实现了性能与效率的权衡。

Abstract: This work evaluates Sentence-BERT for a multi-label code comment
classification task seeking to maximize the classification performance while
controlling efficiency constraints during inference. Using a dataset of 13,216
labeled comment sentences, Sentence-BERT models are fine-tuned and combined
with different classification heads to recognize comment types. While larger
models outperform smaller ones in terms of F1, the latter offer outstanding
efficiency, both in runtime and GFLOPS. As result, a balance between a
reasonable F1 improvement (+0.0346) and a minimal efficiency degradation (+1.4x
in runtime and +2.1x in GFLOPS) is reached.

</details>


### [9] [RE-oriented Model Development with LLM Support and Deduction-based Verification](https://arxiv.org/abs/2506.08606)
*Radoslaw Klimek*

Main category: cs.SE

TL;DR: 提出一个结合UML、LLM和形式化验证的框架，用于增强需求工程阶段。


<details>
  <summary>Details</summary>
Motivation: 通过整合先进建模技术和形式化验证，提高软件开发质量。

Method: 利用特定UML图和LLM生成行为模型，并通过形式化逻辑规范进行验证。

Result: 实现从设计到实现的自动化程序骨架生成，提升效率。

Conclusion: 该框架显著优化了需求工程过程，为软件开发提供高效支持。

Abstract: The requirements engineering (RE) phase is pivotal in developing high-quality
software. Integrating advanced modelling techniques with large language models
(LLMs) and formal verification in a logical style can significantly enhance
this process. We propose a comprehensive framework that focuses on specific
Unified Modelling Language (UML) diagrams for preliminary system development.
This framework offers visualisations at various modelling stages and seamlessly
integrates large language models and logical reasoning engines. The behavioural
models generated with the assistance of LLMs are automatically translated into
formal logical specifications. Deductive formal verification ensures that
logical requirements and interrelations between software artefacts are
thoroughly addressed. Ultimately, the framework facilitates the automatic
generation of program skeletons, streamlining the transition from design to
implementation.

</details>


### [10] [Logic Mining from Process Logs: Towards Automated Specification and Verification](https://arxiv.org/abs/2506.08628)
*Radoslaw Klimek,Julia Witek*

Main category: cs.SE

TL;DR: 本文提出了一种从工作流挖掘生成的流程模型自动推导逻辑规范的方法，结合模式翻译与自动推理技术，并在通用和真实事件日志上评估其效果。


<details>
  <summary>Details</summary>
Motivation: 在复杂系统中，手动构建逻辑规范耗时且易错，因此自动化生成逻辑规范具有重要价值。

Method: 结合模式翻译与自动推理技术，从流程模型生成逻辑规范，并使用自动定理证明器验证各类逻辑属性。

Result: 方法在通用和真实事件日志上验证了适用性，并评估了数据质量对规范生成的影响。

Conclusion: 该方法的实际应用潜力得到了验证，有望集成到实证软件工程实践中。

Abstract: Logical specifications play a key role in the formal analysis of behavioural
models. Automating the derivation of such specifications is particularly
valuable in complex systems, where manual construction is time-consuming and
error-prone. This article presents an approach for generating logical
specifications from process models discovered via workflow mining, combining
pattern-based translation with automated reasoning techniques. In contrast to
earlier work, we evaluate the method on both general-purpose and real-case
event logs, enabling a broader empirical assessment. The study examines the
impact of data quality, particularly noise, on the structure and testability of
generated specifications. Using automated theorem provers, we validate a
variety of logical properties, including satisfiability, internal consistency,
and alignment with predefined requirements. The results support the
applicability of the approach in realistic settings and its potential
integration into empirical software engineering practices.

</details>


### [11] [Proceedings of the 23rd International Overture Workshop](https://arxiv.org/abs/2506.08680)
*Hugo Daniel Macedo,Ken Pierce*

Main category: cs.SE

TL;DR: 第23届国际Overture研讨会的论文集，主题涉及VDM方法及其相关工具和形式化方法的进展。


<details>
  <summary>Details</summary>
Motivation: 推动VDM及相关工具在学术界和工业界的应用与发展。

Method: 通过研讨会分享VDM建模语言（如VDM-SL、VDM++等）和工具（如Overture、Crescendo等）的最新进展。

Result: 展示了VDM/Overture在静态和动态分析、测试生成、模型检查等领域的应用。

Conclusion: 研讨会促进了VDM技术社区的交流与合作，推动了其在Cyber-Physical Systems等领域的应用。

Abstract: This volume contains the papers presented at the 23rd International Overture
Workshop, held on the 11th of June 2025. This event was the latest in a series
of workshops around the Vienna Development Method (VDM), the open-source
project Overture, and related tools and formalisms. VDM is one of the longest
established formal methods for systems development. A lively community of
researchers and practitioners has grown up in academia and industry has grown
around the modelling languages (VDM-SL, VDM++, VDM-RT, CML) and tools
(VDMTools, Overture, Crescendo, Symphony, the INTO-CPS chain, and ViennaTalk).
Together, these provide a platform for work on modelling and analysis
technology that includes static and dynamic analysis, test generation,
execution support, and model checking. This workshop provided updates on the
emerging technology of VDM/Overture, including collaboration infrastructure,
collaborative modelling and co-simulation for Cyber-Physical Systems.

</details>


### [12] [Causality-aware Safety Testing for Autonomous Driving Systems](https://arxiv.org/abs/2506.08688)
*Wenbing Tang,Mingfei Cheng,Renzhi Wang,Yuan Zhou,Chengwei Liu,Yang Liu,Zuohua Ding*

Main category: cs.SE

TL;DR: 本文提出了Causal-Fuzzer技术，通过因果图建模输入场景、ADS运动命令和系统违规之间的复杂关系，实现高效全面的自动驾驶系统测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常关注单个多样性指标，忽视了这些元素间的复杂关系，导致测试覆盖不足，可能遗漏关键问题。

Method: Causal-Fuzzer构建因果图模型，并提出基于因果关系的反馈机制和突变策略，优先考虑对因果关系影响更大的突变。

Result: 在Apollo ADS上的实验表明，Causal-Fuzzer在发现更多违规、提高测试充分性和检测效率方面显著优于现有方法。

Conclusion: Causal-Fuzzer通过建模复杂因果关系，实现了更全面高效的自动驾驶系统测试。

Abstract: Simulation-based testing is essential for evaluating the safety of Autonomous
Driving Systems (ADSs). Comprehensive evaluation requires testing across
diverse scenarios that can trigger various types of violations under different
conditions. While existing methods typically focus on individual diversity
metrics, such as input scenarios, ADS-generated motion commands, and system
violations, they often fail to capture the complex interrelationships among
these elements. This oversight leads to gaps in testing coverage, potentially
missing critical issues in the ADS under evaluation. However, quantifying these
interrelationships presents a significant challenge. In this paper, we propose
a novel causality-aware fuzzing technique, Causal-Fuzzer, to enable efficient
and comprehensive testing of ADSs by exploring causally diverse scenarios. The
core of Causal-Fuzzer is constructing a causal graph to model the
interrelationships among the diversities of input scenarios, ADS motion
commands, and system violations. Then the causal graph will guide the process
of critical scenario generation. Specifically, Causal-Fuzzer proposes (1) a
causality-based feedback mechanism that quantifies the combined diversity of
test scenarios by assessing whether they activate new causal relationships, and
(2) a causality-driven mutation strategy that prioritizes mutations on input
scenario elements with higher causal impact on ego action changes and violation
occurrence, rather than treating all elements equally. We evaluated
Causal-Fuzzer on an industry-grade ADS Apollo, with a high-fidelity. Our
empirical results demonstrate that Causal-Fuzzer significantly outperforms
existing methods in (1) identifying a greater diversity of violations, (2)
providing enhanced testing sufficiency with improved coverage of causal
relationships, and (3) achieving greater efficiency in detecting the first
critical scenarios.

</details>


### [13] [Do Generative AI Tools Ensure Green Code? An Investigative Study](https://arxiv.org/abs/2506.08790)
*Samarth Sikand,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 该论文探讨了AI生成代码的可持续性问题，发现当前主流工具（如ChatGPT、BARD、Copilot）生成的代码普遍不符合绿色编码标准，亟需进一步研究和改进策略。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的普及，其代码生成的可持续性问题尚未被充分研究，论文旨在填补这一空白，评估AI生成代码的环保性。

Method: 研究分析了三种流行生成式AI工具（ChatGPT、BARD、Copilot）生成的代码，评估其是否符合可持续编码实践的标准。

Result: 研究发现，这些工具生成的代码在多数情况下不符合绿色编码规则，表现出默认的非环保行为。

Conclusion: 论文呼吁对AI生成代码的可持续性问题进行更深入的研究，并制定有效的改进策略。

Abstract: Software sustainability is emerging as a primary concern, aiming to optimize
resource utilization, minimize environmental impact, and promote a greener,
more resilient digital ecosystem. The sustainability or "greenness" of software
is typically determined by the adoption of sustainable coding practices. With a
maturing ecosystem around generative AI, many software developers now rely on
these tools to generate code using natural language prompts. Despite their
potential advantages, there is a significant lack of studies on the
sustainability aspects of AI-generated code. Specifically, how environmentally
friendly is the AI-generated code based upon its adoption of sustainable coding
practices? In this paper, we present the results of an early investigation into
the sustainability aspects of AI-generated code across three popular generative
AI tools - ChatGPT, BARD, and Copilot. The results highlight the default
non-green behavior of tools for generating code, across multiple rules and
scenarios. It underscores the need for further in-depth investigations and
effective remediation strategies.

</details>


### [14] [Towards a Knowledge Base of Common Sustainability Weaknesses in Green Software Development](https://arxiv.org/abs/2506.08812)
*Priyavanshi Pathania,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 论文探讨了构建标准化知识库以解决代码中可持续性弱点的必要性，并提出了初步方法，强调现有知识不能直接重标为可持续性。


<details>
  <summary>Details</summary>
Motivation: 应对气候危机，优化资源利用，减少环境影响，需要开发支持可持续性优化的自动化工具，但目前缺乏标准化知识基础。

Method: 提出构建常见代码可持续性弱点的标准知识库的初步方法，并进行初步实验验证。

Result: 实验表明现有软件弱点知识不能直接应用于可持续性领域，需进一步探索。

Conclusion: 呼吁在生态重要领域深入研究和开发标准化知识库，以支持可持续软件的开发。

Abstract: With the climate crisis looming, engineering sustainable software systems
become crucial to optimize resource utilization, minimize environmental impact,
and foster a greener, more resilient digital ecosystem. For developers, getting
access to automated tools that analyze code and suggest sustainabilityrelated
optimizations becomes extremely important from a learning and implementation
perspective. However, there is currently a dearth of such tools due to the lack
of standardized knowledge, which serves as the foundation of these tools. In
this paper, we motivate the need for the development of a standard knowledge
base of commonly occurring sustainability weaknesses in code, and propose an
initial way of doing that. Furthermore, through preliminary experiments, we
demonstrate why existing knowledge regarding software weaknesses cannot be
re-tagged "as is" to sustainability without significant due diligence, thereby
urging further explorations in this ecologically significant domain.

</details>


### [15] [On The Impact of Merge Request Deviations on Code Review Practices](https://arxiv.org/abs/2506.08860)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 论文研究了代码审查中的工业合并请求(MR)流程偏离标准化审查的问题，提出了一种检测方法并证明了其对机器学习模型的正面影响。


<details>
  <summary>Details</summary>
Motivation: 工业MR流程常偏离标准审查，导致部分MR用于非审查目的，这可能影响分析结果和机器学习模型的准确性。

Method: 识别了七种偏离类别，提出了一种few-shot学习检测方法。通过排除偏离案例，改进ML模型的性能。

Result: 偏离案例占37.02%的MR；检测方法准确率达91%。排除偏离后，ML模型在53.33%的情况下性能提升，最大提升2.25倍。

Conclusion: 研究表明，识别和处理MR偏离有助于优化代码审查效率，并为ML模型提供更可靠的数据基础。

Abstract: Code review is a key practice in software engineering, ensuring quality and
collaboration. However, industrial Merge Request (MR) workflows often deviate
from standardized review processes, with many MRs serving non-review purposes
(e.g., drafts, rebases, or dependency updates). We term these cases deviations
and hypothesize that ignoring them biases analytics and undermines ML models
for review analysis.
  We identify seven deviation categories, occurring in 37.02% of MRs, and
propose a few-shot learning detection method (91% accuracy). By excluding
deviations, ML models predicting review completion time improve performance in
53.33% of cases (up to 2.25x) and exhibit significant shifts in feature
importance (47% overall, 60% top-*k*).
  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven
detection approach, and (3) empirical evidence of their impact on ML-based
review analytics. This work aids practitioners in optimizing review efforts and
ensuring reliable insights.

</details>


### [16] [AdaDec: Uncertainty-Guided Adaptive Decoding for LLM-based Code Generation](https://arxiv.org/abs/2506.08980)
*Kaifeng He,Mingwei Liu,Chong Wang,Zike Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 本文提出了一种不确定性引导的自适应解码框架AdaDec，通过暂停-重排机制提升代码生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有解码策略（如贪婪搜索和束搜索）在处理代码生成时忽视了代码特有的不确定性模式，导致性能不佳。本文发现许多错误源于高不确定性步骤中的排名错误。

Method: 提出AdaDec框架，利用Shannon熵作为不确定性指标，学习模型特定的不确定性阈值，并在高不确定性时应用前瞻性重排策略。

Result: 在HumanEval和MBPP基准测试中，AdaDec将Pass@1准确率提升至多15.5%，优于或匹配束搜索，同时降低计算成本和延迟。

Conclusion: 不确定性感知的自适应解码可以显著提升基于大语言模型的代码生成的可靠性和效率。

Abstract: Code generation with large language models (LLMs) is highly sensitive to
token selection during decoding, particularly at uncertain decision points that
influence program logic. While standard strategies like greedy and beam search
treat all tokens uniformly, they overlook code-specific uncertainty patterns,
leading to suboptimal performance. This paper presents an empirical study
revealing that many generation errors stem from ranking mistakes at
high-uncertainty steps, where the correct token is present but not top-ranked.
  Motivated by these findings, we propose AdaDec, an uncertainty-guided
adaptive decoding framework that integrates a token-level pause-then-rerank
mechanism driven by token uncertainty (Shannon entropy). AdaDec learns
model-specific uncertainty thresholds and applies a lookahead-based reranking
strategy when uncertainty is high. Experiments on HumanEval and MBPP benchmarks
show that AdaDec improves Pass@1 accuracy by up to 15.5% over greedy decoding,
outperforms or matches beam search, and reduces computational cost and latency
through efficient, selective pausing. Our results highlight the promise of
uncertainty-aware adaptive decoding for improving the reliability and
efficiency of LLM-based code generation.

</details>


### [17] [Boosting Rust Unit Test Coverage through Hybrid Program Analysis and Large Language Models](https://arxiv.org/abs/2506.09002)
*Bei Chu,Yang Feng,Kui Liu,Hange Shi,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: PALM利用大型语言模型（LLMs）通过程序分析和路径约束生成高覆盖率的单元测试，显著提高了测试覆盖率和编译成功率。


<details>
  <summary>Details</summary>
Motivation: 传统的SBST和concolic执行方法在处理复杂程序单元（如分支条件和外部依赖）时覆盖率低，而现有的LLM方法因固定提示词导致覆盖率有限。

Method: PALM通过程序分析提取分支条件并构建路径约束，结合上下文生成动态提示词，指导LLMs生成单元测试。

Result: 在10个Rust项目中，PALM在2-3小时内显著提高了覆盖率（部分项目超过50%），生成的测试平均覆盖率达75.77%，接近人工水平（71.30%）。

Conclusion: PALM展示了LLMs与程序分析结合的潜力，为自动化测试开辟了新方向。

Abstract: Unit testing is essential for ensuring software reliability and correctness.
Classic Search-Based Software Testing (SBST) methods and concolic
execution-based approaches for generating unit tests often fail to achieve high
coverage due to difficulties in handling complex program units, such as
branching conditions and external dependencies. Recent work has increasingly
utilized large language models (LLMs) to generate test cases, improving the
quality of test generation by providing better context and correcting errors in
the model's output. However, these methods rely on fixed prompts, resulting in
relatively low compilation success rates and coverage. This paper presents
PALM, an approach that leverages large language models (LLMs) to enhance the
generation of high-coverage unit tests. PALM performs program analysis to
identify branching conditions within functions, which are then combined into
path constraints. These constraints and relevant contextual information are
used to construct prompts that guide the LLMs in generating unit tests. We
implement the approach and evaluate it in 10 open-source Rust crates.
Experimental results show that within just two or three hours, PALM can
significantly improves test coverage compared to classic methods, with
increases in overall project coverage exceeding 50% in some instances and its
generated tests achieving an average coverage of 75.77%, comparable to human
effort (71.30%), highlighting the potential of LLMs in automated test
generation. We submitted 91 PALM-generated unit tests targeting new code. Of
these submissions, 80 were accepted, 5 were rejected, and 6 remain pending
review. The results demonstrate the effectiveness of integrating program
analysis with AI and open new avenues for future research in automated software
testing.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [18] [Verification of the Release-Acquire Semantics](https://arxiv.org/abs/2506.08238)
*Parosh Abdulla,Elli Anastasiadi,Mohamed Faouzi Atig,Samuel Grahn*

Main category: cs.PL

TL;DR: 本文研究了RA及其变种SRA和WRA的一致性验证问题，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 填补对内存模型一致性验证问题的研究空白，特别是检查所有允许的程序运行是否一致。

Method: 通过寄存器机器描述实现，验证其运行是否违反RA、SRA或WRA语义。

Result: 验证WRA的复杂度为O(n5)，而RA和SRA的验证是NP-和coNP难的，且具有PSPACE上界。

Conclusion: 研究不仅回答了这些问题的基础复杂性，还揭示了寄存器机器模型的表达能力。

Abstract: The Release-Acquire (RA) semantics and its variants are some of the most
fundamental models of concurrent semantics for architectures, programming
languages, and distributed systems. Several steps have been taken in the
direction of testing such semantics, where one is interested in whether a
single program execution is consistent with a memory model. The more general
verification problem, i.e., checking whether all allowed program runs are
consistent with a memory model, has still not been studied as much. The purpose
of this work is to bridge this gap. We tackle the verification problem, where,
given an implementation described as a register machine, we check if any of its
runs violates the RA semantics or its Strong (SRA) and Weak (WRA) variants. We
show that verifying WRA in this setup is in O([)n5 ], while verifying the RA
and SRA is in both NP- and coNP-hard, and provide a PSPACE upper bound. This
both answers some fundamental questions about the complexity of these problems,
but also provides insights on the expressive power of register machines as a
model.

</details>


### [19] [Linguine: A Natural-Language Programming Language with Formal Semantics and a Clean Compiler Pipeline](https://arxiv.org/abs/2506.08396)
*Lifan Hu*

Main category: cs.PL

TL;DR: Linguine是一种受自然语言启发的编程语言，允许用户用流畅的英语子集编写程序，同时保留形式语义。其通过代词变量和类型系统确保编译时的清晰性和类型安全。


<details>
  <summary>Details</summary>
Motivation: 旨在结合人类语言直觉与形式化方法，提升编程的可读性和静态验证能力。

Method: 引入代词变量，采用Hindley-Milner类型系统进行静态解析，编译流程包括词法分析、类型推断等步骤。

Result: 语言在保持简洁可读的同时支持静态验证，证明了代词解析机制的合理性。

Conclusion: Linguine为结合语言直觉与形式化方法的编程系统提供了新的方向。

Abstract: Linguine is a natural-language-inspired programming language that enables
users to write programs in a fluent, controlled subset of English while
preserving formal semantics. The language introduces anaphoric constructs, such
as pronoun variables (e.g., "it", "them"), that are statically resolved through
referent-tracking analysis combined with a Hindley-Milner-style type system.
Each pronoun is guaranteed to be unambiguous and well-typed at compile time.
  The Linguine compiler pipeline includes lexing, parsing, clause graph
construction, desugaring into a typed intermediate representation, type
inference, and abstract interpretation. This enables the early detection of
semantic errors, such as undefined or type-inconsistent references. A
lightweight backend currently generates Python code.
  This paper formalizes the core language, defines its typing and operational
semantics, and proves the soundness of its pronoun resolution mechanism. An
initial evaluation shows that Linguine allows the expression of concise and
readable programs while supporting static verification.
  Linguine represents a step toward programming systems that prioritize human
linguistic intuition while remaining grounded in formal methods and
type-theoretic rigor.

</details>


### [20] [Gradual Metaprogramming](https://arxiv.org/abs/2506.09043)
*Tianyu Chen,Darshal Shetty,Jeremy G. Siek,Chao-Hong Chen,Weixi Ma,Arnaud Venet,Rocky Liu*

Main category: cs.PL

TL;DR: 本文提出了


<details>
  <summary>Details</summary>
Motivation: 数据工程师使用嵌入Python的DSL生成数据管道代码时，面临错误检测延迟和定位困难的问题。针对这一问题，本文聚焦于Meta的F3 DSL，提出了一种渐进式元编程方法。

Method: 提出渐进式元编程方法，通过静态类型检查代码片段并在拼接时进行运行时检查，实现早期错误检测和错误定位。定义MetaGTLC元编程演算，并将其翻译到MetaCC演算。

Result: 证明了成功元评估始终生成类型良好的对象程序，并在Agda中机械化验证。

Conclusion: 渐进式元编程为静态类型DSL提供了迁移路径，提高了错误检测的及时性和定位准确性。

Abstract: Data engineers increasingly use domain-specific languages (DSLs) to generate
the code for data pipelines. Such DSLs are often embedded in Python.
Unfortunately, there are challenges in debugging the generation of data
pipelines: an error in a Python DSL script is often detected too late, after
the execution of the script, and the source code location that triggers the
error is hard to pinpoint.
  In this paper, we focus on the F3 DSL of Meta (Facebook), which is a DSL
embedded in Python (so it is dynamically-typed) to generate data pipeline
description code that is statically-typed. We propose gradual metaprogramming
to (1) provide a migration path toward statically typed DSLs, (2) immediately
provide earlier detection of code generation type errors, and (3) report the
source code location responsible for the type error. Gradual metaprogramming
accomplishes this by type checking code fragments and incrementally performing
runtime checks as they are spliced together. We define MetaGTLC, a
metaprogramming calculus in which a gradually-typed metalanguage manipulates a
statically-typed object language, and give semantics to it by translation to
the cast calculus MetaCC. We prove that successful metaevaluation always
generates a well-typed object program and mechanize the proof in Agda.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [21] [Congestion-Aware Path Selection for Load Balancing in AI Clusters](https://arxiv.org/abs/2506.08132)
*Erfan Nosrati,Majid Ghaderi*

Main category: cs.NI

TL;DR: Hopper是一种专门为AI集群中的RDMA流量优化的负载均衡技术，无需专用硬件或交换机修改，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 分布式训练大型机器学习模型需要高效的GPU集群网络，现有负载均衡技术无法满足RDMA流量的需求。

Method: Hopper通过主机级别的设计，动态监测路径拥塞并切换流量，同时控制切换时间以减少乱序包。

Result: 测试显示，Hopper相比现有技术，平均和尾部流完成时间分别降低了20%和14%。

Conclusion: Hopper为AI集群提供了一种高效的RDMA负载均衡解决方案。

Abstract: Fast training of large machine learning models requires distributed training
on AI clusters consisting of thousands of GPUs. The efficiency of distributed
training crucially depends on the efficiency of the network interconnecting
GPUs in the cluster. These networks are commonly built using RDMA following a
Clos-like datacenter topology. To efficiently utilize the network bandwidth,
load balancing is employed to distribute traffic across multiple redundant
paths. While there exists numerous techniques for load-balancing in traditional
datacenters, these are often either optimized for TCP traffic or require
specialized network hardware, thus limiting their utility in AI clusters.
  This paper presents the design and evaluation of Hopper, a new load-balancing
technique optimized for RDMA traffic in AI clusters. Operating entirely at the
host level, Hopper requires no specialized hardware or modifications to network
switches. It continuously monitors the current path for congestion and
dynamically switches traffic to a less congested path when congestion is
detected. Furthermore, it incorporates a lightweight mechanism to identify
alternative paths and carefully controls the timing of path switching to
prevent excessive out-of-order packets.
  We evaluated Hopper using ns-3 simulations and a testbed implementation. Our
evaluations show that Hopper reduces the average and 99-percentile tail flow
completion time by up to 20% and 14%, respectively, compared to
state-of-the-art host-based load balancing techniques.

</details>


### [22] [5G Aero: A Prototyping Platform for Evaluating Aerial 5G Communications](https://arxiv.org/abs/2506.08386)
*Matteo Bordin,Madhukara S. Holla,Sakthivel Velumani,Salvatore D'Oro,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本文介绍了5G Aero无人机，专为5G连接优化，并通过实验验证其在视距和非视距条件下的高吞吐量、低延迟通信性能，符合3GPP标准。


<details>
  <summary>Details</summary>
Motivation: 确保5G无人机在高吞吐量和低延迟通信中的可靠性，尤其是在视距和非视距条件下，是当前尚未充分探索的问题。

Method: 设计了一款紧凑型5G无人机5G Aero，并在室内环境中对其实验，评估其在视距和非视距下的通信性能。

Result: 5G Aero在视距和非视距条件下均满足3GPP标准；5G模块对飞行时间影响极小（仅1%）。

Conclusion: 研究表明，当前5G网络能有效支持无人机高级操作，为无人机在5G网络中的性能优化提供了参考。

Abstract: The application of small-factor, 5G-enabled Unmanned Aerial Vehicles (UAVs)
has recently gained significant interest in various aerial and Industry 4.0
applications. However, ensuring reliable, high-throughput, and low-latency 5G
communication in aerial applications remains a critical and underexplored
problem. This paper presents the 5th generation (5G) Aero, a compact UAV
optimized for 5G connectivity, aimed at fulfilling stringent 3rd Generation
Partnership Project (3GPP) requirements. We conduct a set of experiments in an
indoor environment, evaluating the UAV's ability to establish high-throughput,
low-latency communications in both Line-of-Sight (LoS) and Non-Line-of-Sight
(NLoS) conditions. Our findings demonstrate that the 5G Aero meets the required
3GPP standards for Command and Control (C2) packets latency in both LoS and
NLoS, and video latency in LoS communications and it maintains acceptable
latency levels for video transmission in NLoS conditions. Additionally, we show
that the 5G module installed on the UAV introduces a negligible 1% decrease in
flight time, showing that 5G technologies can be integrated into commercial
off-the-shelf UAVs with minimal impact on battery lifetime. This paper
contributes to the literature by demonstrating the practical capabilities of
current 5G networks to support advanced UAV operations in telecommunications,
offering insights into potential enhancements and optimizations for UAV
performance in 5G networks

</details>


### [23] [Aerial Shepherds: Enabling Hierarchical Localization in Heterogeneous MAV Swarms](https://arxiv.org/abs/2506.08408)
*Haoyang Wang,Jingao Xu,Chenyu Zhao,Yuhan Cheng,Xuecheng Chen,Chaopeng Hong,Xiao-Ping Zhang,Yunhao Liu,Xinlei Chen*

Main category: cs.NI

TL;DR: 提出了一种名为TransformLoc的新框架，将高级微型飞行器（AMAVs）转变为移动定位基础设施，为低成本、资源受限的基本微型飞行器（BMAVs）提供实时高精度定位。


<details>
  <summary>Details</summary>
Motivation: 当前微型飞行器群体缺乏低成本、高精度、实时的定位解决方案，特别是对轻量级BMAVs。通过利用AMAVs的资源优势，可以为其提供定位支持。

Method: 设计了错误感知的联合位置估计模型和相似性指导的自适应分组调度策略，动态分配AMAVs资源。

Result: 实验显示，TransformLoc在定位性能上优于基线方法68%，导航成功率提升60%，并通过了广泛的鲁棒性和消融实验验证。

Conclusion: TransformLoc为大规模异构微型飞行器群体提供了一种协作、自适应且经济高效的定位系统。

Abstract: A heterogeneous micro aerial vehicles (MAV) swarm consists of
resource-intensive but expensive advanced MAVs (AMAVs) and resource-limited but
cost-effective basic MAVs (BMAVs), offering opportunities in diverse fields.
Accurate and real-time localization is crucial for MAV swarms, but current
practices lack a low-cost, high-precision, and real-time solution, especially
for lightweight BMAVs. We find an opportunity to accomplish the task by
transforming AMAVs into mobile localization infrastructures for BMAVs. However,
translating this insight into a practical system is challenging due to issues
in estimating locations with diverse and unknown localization errors of BMAVs,
and allocating resources of AMAVs considering interconnected influential
factors. This work introduces TransformLoc, a new framework that transforms
AMAVs into mobile localization infrastructures, specifically designed for
low-cost and resource-constrained BMAVs. We design an error-aware joint
location estimation model to perform intermittent joint estimation for BMAVs
and introduce a similarity-instructed adaptive grouping-scheduling strategy to
allocate resources of AMAVs dynamically. TransformLoc achieves a collaborative,
adaptive, and cost-effective localization system suitable for large-scale
heterogeneous MAV swarms. We implement and validate TransformLoc on industrial
drones. Results show it outperforms all baselines by up to 68\% in localization
performance, improving navigation success rates by 60\%. Extensive robustness
and ablation experiments further highlight the superiority of its design.

</details>


### [24] [Deep Reinforcement Learning-Based RAN Slicing with Efficient Inter-Slice Isolation in Tactical Wireless Networks](https://arxiv.org/abs/2506.09039)
*Abderrahime Filali,Diala Naboulsi,Georges Kaddoum*

Main category: cs.NI

TL;DR: 本文提出了一种基于深度强化学习（DRL）的RAN切片机制，以解决战术网络中带宽共享与切片隔离之间的平衡问题，并在O-RAN架构中实现。


<details>
  <summary>Details</summary>
Motivation: 战术网络（TNs）需要动态带宽切片策略，但现有方法在高效带宽利用的同时可能牺牲QoS性能。本文旨在通过DRL机制实现带宽共享与切片隔离的平衡。

Method: 采用基于DRL的两阶段带宽分配机制：第一阶段分配带宽给RAN切片，第二阶段切片内部分配带宽给用户。该机制部署在O-RAN架构中，并开发了三种不同的DRL算法实现。

Result: 通过多种基线测试，评估了三种DRL算法实现的性能，证明其在带宽共享和切片隔离方面的有效性。

Conclusion: 提出的DRL机制在战术网络中能够高效平衡带宽共享与切片隔离，并在O-RAN架构中具有实际应用价值。

Abstract: The next generation of tactical networks (TNs) is poised to further leverage
the key enablers of 5G and beyond 5G (B5G) technology, such as radio access
network (RAN) slicing and the open RAN (O-RAN) paradigm, to unlock multiple
architectural options and opportunities for a wide range of innovative
applications. RAN slicing and the O-RAN paradigm are considered game changers
in TNs, where the former makes it possible to tailor user services to users
requirements, and the latter brings openness and intelligence to the management
of the RAN. In TNs, bandwidth scarcity requires a dynamic bandwidth slicing
strategy. Although this type of strategy ensures efficient bandwidth
utilization, it compromises RAN slicing isolation in terms of quality of
service (QoS) performance. To deal with this challenge, we propose a deep
reinforcement learning (DRL)-based RAN slicing mechanism that achieves a
trade-off between efficient RAN bandwidth sharing and appropriate inter- and
intra-slice isolation. The proposed mechanism performs bandwidth allocation in
two stages. In the first stage, the bandwidth is allocated to the RAN slices.
In the second stage, each slice partitions its bandwidth among its associated
users. In both stages, the slicing operation is constrained by several
considerations related to improving the QoS of slices and users that in turn
foster inter- and intra-slice isolation. The proposed RAN slicing mechanism is
based on DRL algorithms to perform the bandwidth sharing operation in each
stage. We propose to deploy the mechanism in an O-RAN architecture and describe
the O-RAN functional blocks and the main DRL model lifecycle management phases
involved. We also develop three different implementations of the proposed
mechanism, each based on a different DRL algorithm, and evaluate their
performance against multiple baselines across various parameters.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [25] [Approximate Axiomatization for Differentially-Defined Functions](https://arxiv.org/abs/2506.08233)
*André Platzer,Long Qian*

Main category: cs.LO

TL;DR: 论文提出了实数闭合域与微分定义函数的近似公理化方法，并证明了其完备性。


<details>
  <summary>Details</summary>
Motivation: 研究如何将实数闭合域与特殊函数（如正弦、余弦、指数函数等）结合，并为其建立近似公理化系统，以提高理论的可处理性和计算效率。

Method: 通过扩展实数闭合域的公理化系统，并引入微分动态逻辑的片段，构建了一个包含数值近似的公理化框架。

Result: 证明了该公理化系统在数值近似下具有完备性，且能够将含特殊函数的公式近似为一阶逻辑公式。

Conclusion: 该研究为实数闭合域与微分定义函数的公理化提供了有效方法，扩展了理论边界并提升了计算能力。

Abstract: This article establishes a complete approximate axiomatization for the
real-closed field $\mathbb{R}$ expanded with all differentially-defined
functions, including special functions such as $\sin(x), \cos(x), e^x, \dots$.
Every true sentence is provable up to some numerical approximation, and the
truth of such approximations converge under mild conditions. Such an
axiomatization is a fragment of the axiomatization for differential dynamic
logic, and is therefore a finite extension of the axiomatization of real-closed
fields. Furthermore, the numerical approximations approximate formulas
containing special function symbols by $\text{FOL}_{\mathbb{R}}$ formulas,
improving upon earlier decidability results only concerning closed sentences.

</details>


### [26] [Forward and Backward Simulations for Partially Observable Probability](https://arxiv.org/abs/2506.08437)
*Chris Chen,Annabelle McIver,Carroll Morgan*

Main category: cs.LO

TL;DR: 本文介绍了一种用于部分可观察马尔可夫决策过程的最弱前提语义，证明了在该复杂场景下，前向/后向模拟的合理性。


<details>
  <summary>Details</summary>
Motivation: 传统的数据精化方法在混合概率和非确定性的模型中存在不足，影响了模拟的正确性和状态封装能力。

Method: 使用损失函数转换器，为Kuifje$_{\sqcap}$语言设计最弱前提语义。

Result: 在前向模拟不泄露信息且后向模拟不利用泄露信息的健康条件下，证明了模拟的合理性。

Conclusion: 该方法扩展了模拟的适用范围，解决了概率数据类型的建模问题。

Abstract: Data refinement is the standard extension of a refinement relation from
programs to datatypes (i.e. a behavioural subtyping relation). Forward/backward
simulations provide a tractable method for establishing data refinement, and
have been thoroughly studied for nondeterministic programs. However, for
standard models of mixed probability and nondeterminism, ordinary assignment
statements may not commute with (variable-disjoint) program fragments. This (1)
invalidates a key assumption underlying the soundness of simulations, and (2)
prevents modelling probabilistic datatypes with encapsulated state.
  We introduce a weakest precondition semantics for Kuifje$_\sqcap$, a language
for partially observable Markov decision processes, using so-called loss
(function) transformers. We prove soundness of forward/backward simulations in
this richer setting, modulo healthiness conditions with a remarkable duality:
forward simulations cannot leak information, and backward simulations cannot
exploit leaked information.

</details>


### [27] [Compositional Reasoning for Parametric Probabilistic Automata](https://arxiv.org/abs/2506.08525)
*Hannah Mertens,Tim Quatmann,Joost-Pieter Katoen*

Main category: cs.LO

TL;DR: 本文提出了一种参数化概率自动机（pPA）的多目标查询的组合推理框架，扩展了现有PA框架，并引入了新的证明规则。


<details>
  <summary>Details</summary>
Motivation: 为了在参数化概率自动机（pPA）中验证多样化的多目标查询，包括概率属性和期望总奖励，需要一种灵活的推理框架。

Method: 将现有PA框架扩展到pPA，引入不对称、循环和交错证明规则，并添加了关于单调性的推理规则。

Result: 实现了对pPA中广泛的多目标查询的验证能力。

Conclusion: 该框架为pPA的多目标查询提供了一种有效的组合推理方法，扩展了现有技术的能力。

Abstract: We establish an assume-guarantee (AG) framework for compositional reasoning
about multi-objective queries in parametric probabilistic automata (pPA) - an
extension to probabilistic automata (PA), where transition probabilities are
functions over a finite set of parameters. We lift an existing framework for PA
to the pPA setting, incorporating asymmetric, circular, and interleaving proof
rules. Our approach enables the verification of a broad spectrum of
multi-objective queries for pPA, encompassing probabilistic properties and
(parametric) expected total rewards. Additionally, we introduce a rule for
reasoning about monotonicity in composed pPAs.

</details>


### [28] [Martin Davis: An Overview of his Work in Logic, Computer Science, and Philosophy](https://arxiv.org/abs/2506.08588)
*Liesbeth De Mol,Yuri V. Matiyasevich,Eugenio G. Omodeo,Alberto Policriti,Wilfried Sieg,Elaine J. Weyuker*

Main category: cs.LO

TL;DR: 本文探讨了Martin Davis如何将逻辑学贯穿其科学生涯，并重点分析了他对计算理论、不可解问题等领域的重要贡献。


<details>
  <summary>Details</summary>
Motivation: 研究Martin Davis的科学贡献及其如何将逻辑学作为统一主题贯穿其职业生涯，以揭示其思想的一致性。

Method: 通过分析Davis的自传体文章及其标题变化，探讨他的学术视角和贡献。

Result: Davis的逻辑学视角是其科学成就的核心，尤其在计算理论和哲学领域表现突出。

Conclusion: Davis的科学贡献体现了逻辑学在计算机科学中的深刻影响，其思想对相关领域有重要启发。

Abstract: In his autobiographic essay written in 1999, ``From logic to computer science
and back'', Martin David Davis (3/8/1928--1/1/2023) indicated that he viewed
himself as a logician \emph{and} a computer scientist. He expanded the essay in
2016 and expressed a new perspective through a changed title, ``My life as a
logician''. He points out that logic was the unifying theme underlying his
scientific career. Our paper attempts to provide a consistent vision that
illuminates Davis' successive contributions leading to his landmark writings on
computability, unsolvable problems, automated reasoning, as well as the history
and philosophy of computing.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [29] [AffectMachine-Pop: A controllable expert system for real-time pop music generation](https://arxiv.org/abs/2506.08200)
*Kat R. Agres,Adyasha Dash,Phoebe Chua,Stefan K. Ehrlich*

Main category: cs.HC

TL;DR: 本文介绍了一个名为AffectMachine-Pop的专家系统，能够根据预定义或实时情绪状态生成具有特定唤醒度和效价值的复古流行音乐。该系统旨在提供可控的音乐生成，适用于情感自我调节。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐生成系统多为黑箱，缺乏直接控制性，限制了其灵活性和适应性。因此，需要一种能够根据用户需求或实时情绪状态生成情感音乐的可控系统。

Method: 开发了AffectMachine-Pop专家系统，通过唤醒度和效价值生成复古流行音乐，并进行了听音实验验证其效果。

Result: 实验证明AffectMachine-Pop能够根据目标唤醒度和效价值生成情感音乐，适用于交互式音乐生成或生物反馈系统。

Conclusion: AffectMachine-Pop为情感音乐生成提供了可控且灵活的解决方案，适用于用户交互和情感调节。

Abstract: Music is a powerful medium for influencing listeners' emotional states, and
this capacity has driven a surge of research interest in AI-based affective
music generation in recent years. Many existing systems, however, are a black
box which are not directly controllable, thus making these systems less
flexible and adaptive to users. We present \textit{AffectMachine-Pop}, an
expert system capable of generating retro-pop music according to arousal and
valence values, which can either be pre-determined or based on a listener's
real-time emotion states. To validate the efficacy of the system, we conducted
a listening study demonstrating that AffectMachine-Pop is capable of generating
affective music at target levels of arousal and valence. The system is tailored
for use either as a tool for generating interactive affective music based on
user input, or for incorporation into biofeedback or neurofeedback systems to
assist users with emotion self-regulation.

</details>


### [30] [Z3Guide: A Scalable, Student-Centered, and Extensible Educational Environment for Logic Modeling](https://arxiv.org/abs/2506.08294)
*Ruanqianqian Huang,Ayana Monroe,Peli de Halleux,Sorin Lerner,Nikolaj Bjørner*

Main category: cs.HC

TL;DR: 该论文探讨如何设计一个逻辑建模的教育环境，并验证其对学生学习体验的影响。通过需求访谈与教师合作迭代设计，提出了10条设计指南，开发了基于浏览器的开源工具Z3Guide，并在100多名学生的工作坊中获得积极反馈。


<details>
  <summary>Details</summary>
Motivation: 尽管逻辑建模在计算机科学中广泛应用，但其教育资源稀缺且分散，设计一个能满足师生需求的易访问教育环境仍是挑战。

Method: 通过需求访谈研究和教师合作的迭代设计，提炼出10条设计指南，并在开源工具Z3Guide中实现了9条。

Result: 在100多名学生的逻辑建模工作坊中，Z3Guide获得了对学生学习的积极支持反馈，并发现了未来改进的机会。

Conclusion: 通过设计和实施Z3Guide，论文验证了其有效性，并为进一步优化逻辑建模教育环境提供了方向。

Abstract: Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from
budgeting for grocery shopping to verifying software behavior. Logic modeling
helps solve CSPs programmatically using SMT solvers. Despite its importance in
many Computer Science disciplines, resources for teaching and learning logic
modeling are scarce and scattered, and challenges remain in designing
educational environments for logic modeling that are accessible and meet the
needs of teachers and students. This paper explores how to design such an
environment and probes the impact of the design on the learning experience.
From a need-finding interview study and a design iteration with teachers of
logic modeling, we curated 10 design guidelines spanning three main
requirements: providing easy access, supporting various educational modalities,
and allowing extensions for customized pedagogical needs. We implemented nine
guidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a
logic modeling learning workshop with more than 100 students, we gathered
positive feedback on its support for learning and identified opportunities for
future improvements.

</details>


### [31] [EMG-Driven Stiffness-Modulating Palpation for Telerehabilitation](https://arxiv.org/abs/2506.08303)
*Thomas M. Kwok,Hilary HY Cheng,Wai Tuck Chow*

Main category: cs.HC

TL;DR: HJ-Pal 是一款轻量级可穿戴触觉设备，利用 EMG 驱动的蜂窝堵塞技术实现肌肉活动的动觉反馈，用于远程康复中的小肌肉评估。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过触觉反馈技术改善远程康复中的小肌肉评估功能，提升远程医疗的实用性和效果。

Method: 采用了 EMG 驱动的蜂窝堵塞技术，通过可穿戴设备将肌肉活动转化为动觉反馈。

Result: 成功开发出 HJ-Pal 设备，能够实现远程肌肉评估的触觉反馈。

Conclusion: HJ-Pal 为远程康复提供了有效的解决方案，具有实际应用潜力。

Abstract: In this work, we introduce HJ-Pal, a lightweight wearable haptic device that
leverages EMG-driven honeycomb jamming to render muscle activation as
kinesthetic feedback, enabling remote palpation for small muscle assessment in
telerehabilitation.

</details>


### [32] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: SakugaFlow 是一个四阶段流程，结合扩散图像生成和大型语言模型指导，为新手提供实时反馈并支持非线性修改，将黑盒生成器转变为学习工具。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 插图工具虽能生成高质量图像，但缺乏人类艺术家的分步流程，限制了学习和创作探索。

Method: 采用四阶段流程，结合扩散模型和语言模型，提供实时反馈（如解剖、透视、构图），支持非线性修改和分支版本。

Result: SakugaFlow 通过展示中间输出和嵌入教学对话，成为一个支持创意探索和技能获取的学习环境。

Conclusion: 该工具将黑盒 AI 生成器转变为支持学习和创作的工具，填补了当前 AI 插画工具的不足。

Abstract: While current AI illustration tools can generate high-quality images from
text prompts, they rarely reveal the step-by-step procedure that human artists
follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based
image generation with a large-language-model tutor. At each stage, novices
receive real-time feedback on anatomy, perspective, and composition, revise any
step non-linearly, and branch alternative versions. By exposing intermediate
outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box
generator into a scaffolded learning environment that supports both creative
exploration and skills acquisition.

</details>


### [33] [Rethinking Citation of AI Sources in Student-AI Collaboration within HCI Design Education](https://arxiv.org/abs/2506.08467)
*Prakash Shukla,Suchismita Naik,Ike Obi,Jessica Backus,Nancy Rasche,Paul Parson*

Main category: cs.HC

TL;DR: 论文探讨了HCI教育中AI生成内容引用的问题，提出重新思考AI引用作为反思性教学实践的重要性。


<details>
  <summary>Details</summary>
Motivation: AI工具在教学设计中的广泛应用使得传统引用框架无法适应AI输出的动态性，亟需新的引用方法。

Method: 通过分析35个团队项目和175名学生的反思，研究了学生在设计过程中对AI工具的使用和引用方式。

Result: 发现学生在AI引用上存在不一致性，现有框架不完善，提出了如AI贡献声明等替代策略。

Conclusion: 建议教育者重新设计引用实践，以支持学生与AI的有意义协作，并促进反思性学习。

Abstract: The growing integration of AI tools in student design projects presents an
unresolved challenge in HCI education: how should AI-generated content be cited
and documented? Traditional citation frameworks -- grounded in credibility,
retrievability, and authorship -- struggle to accommodate the dynamic and
ephemeral nature of AI outputs. In this paper, we examine how undergraduate
students in a UX design course approached AI usage and citation when given the
freedom to integrate generative tools into their design process. Through
qualitative analysis of 35 team projects and reflections from 175 students, we
identify varied citation practices ranging from formal attribution to indirect
or absent acknowledgment. These inconsistencies reveal gaps in existing
frameworks and raise questions about authorship, assessment, and pedagogical
transparency. We argue for rethinking AI citation as a reflective and
pedagogical practice; one that supports metacognitive engagement by prompting
students to critically evaluate how and why they used AI throughout the design
process. We propose alternative strategies -- such as AI contribution
statements and process-aware citation models that better align with the
iterative and reflective nature of design education. This work invites
educators to reconsider how citation practices can support meaningful
student--AI collaboration.

</details>


### [34] [Guidelines for Gaze-based Neural Preliminary Diagnosis](https://arxiv.org/abs/2506.08517)
*Mayar Elfares,Salma Younis,Pascal Reisert,Ralf Küsters,Tobias Renner,Andreas Bulling*

Main category: cs.HC

TL;DR: 本文概述了眼动追踪技术在神经障碍诊断中的应用、现有研究的矛盾点，并提出了标准化协议和进一步研究的建议。


<details>
  <summary>Details</summary>
Motivation: 传统的神经诊断方法费时且主观，而眼动追踪能提供更客观的神经功能洞察，但现有研究结果矛盾且分散，需要标准化和扩展应用。

Method: 通过系统化总结现有知识，提出关键指南，推动眼动追踪作为神经初步诊断工具的标准化协议。

Result: 总结了已被认可的主要发现，并为未来的研究提供了规范化的方向。

Conclusion: 眼动追踪技术有潜力成为神经障碍初步诊断的工具，但需要进一步研究以实现标准化和广泛应用。

Abstract: Neural disorders refer to any condition affecting the nervous system and that
influence how individuals perceive and interact with the world. Traditional
neural diagnoses rely on cumbersome, time-consuming, or subjective methods,
such as clinical interviews, behavioural observations, or medical imaging. Eye
tracking is an attractive alternative because analysing eye movements, such as
fixations and saccades, can provide more objective insights into brain function
and cognitive processing by capturing non-verbal and unconscious responses.
Despite its potential, existing gaze-based studies presented seemingly
contradictory findings. They are dispersed across diverse fields, requiring
further research to standardise protocols and expand their application,
particularly as a preliminary indicator of neural processes for differential
diagnosis. Therefore, this paper outlines the main agreed-upon findings and
provides a systematisation of knowledge and key guidelines towards advancing
gaze-based neural preliminary diagnosis.

</details>


### [35] [Exploring the Convergence of HCI and Evolving Technologies in Information Systems](https://arxiv.org/abs/2506.08549)
*Rajan Das Gupta,Ashikur Rahman,Md Imrul Hasan Showmick,Md. Yeasin Rahat,Md. Jakir Hossen*

Main category: cs.HC

TL;DR: 研究分析了50篇HCI界面设计论文，指出现有方法仍基于传统桌面模型，无法满足移动和位置服务需求，需结合敏捷方法和以人为中心的设计原则进行改进。


<details>
  <summary>Details</summary>
Motivation: 现代技术与信息系统的深度整合对HCI提出新挑战，需设计适应移动、云端和物联网的友好界面，尤其是面向儿童、老年人和残疾人等多样化用户群体。

Method: 通过回顾50篇近期HCI界面设计论文，评估现有方法对当前技术的适应性。

Result: 研究发现多数HCI设计方法仍基于传统桌面模型，无法满足移动用户和位置服务的灵活性和动态性需求。

Conclusion: 未来研究需结合敏捷方法和以人为中心的设计原则，并纳入定性与定量方法，以弥合界面设计与技术发展间的差距。

Abstract: Modern technology driven information systems are part of our daily lives.
However, this deep integration poses new challenges to the human computer
interaction (HCI) professionals. With the rapid growth of mobile and cloud
computing and the Internet of Things (IoT), the demand for HCI specialists to
design user-friendly and adaptable interfaces has never been more pressing.
Especially for diverse user groups such as children, the elderly and people
with disabilities who need interfaces tailored to their needs regardless of
time and location. This study reviewed 50 recent papers on HCI interface design
for modern information systems. The goal is to see how well these methods
address the demands of current technology. The findings show that most HCI
design methods are still based on old desktop models and do not support mobile
users and location-based services well. Most existing interface design
guidelines do not align with the flexibility and dynamism of emerging
technologies. The goal of this study is to improve interface design by
combining agile methodologies with human-centered design principles. Future
studies should also incorporate both qualitative and quantitative approaches,
particularly in the context of cloud-based technologies and organizational
information systems. This approach aims to bridge the gap between current
interface design practices and the changing technological landscape.

</details>


### [36] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: 提出了一种名为MOSAIC-F的新型多模态反馈框架，用于生成个性化学习活动反馈，结合了人类评估和基于数据的多模态分析。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多模态数据（如视频、音频、生理信号等）和AI技术，为学生提供更准确、个性化的学习反馈。

Method: 框架分为四步：1）通过标准化量表进行同伴和教师评估；2）收集多模态数据；3）利用AI生成个性化反馈；4）学生通过视频和自我评估比较反馈。

Result: 在改善口头表达技能的测试中证明了框架的有效性。

Conclusion: 通过结合人类和数据驱动的评估技术，MOSAIC-F能够提供更精准和可操作的个性化反馈。

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


### [37] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)
*Hyeon Jeon,Jeongin Park,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 论文探讨了t-SNE和UMAP在可视化分析中的常见误用，分析了原因并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 由于t-SNE和UMAP投影常被错误用于研究聚类间关系，作者希望揭示问题并预防误用。

Method: 通过文献综述（114篇论文）和访谈研究，验证误用的普遍性并分析原因。

Result: 研究发现误用主要源于对这两种技术适用范围的讨论不足。

Conclusion: 作者提出了未来方向和具体措施，以推动降维技术的合理使用。

Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly
common. For example, although t-SNE and UMAP projections often do not
faithfully reflect true distances between clusters, practitioners frequently
use them to investigate inter-cluster relationships. In this paper, we bring
this issue to the surface and comprehensively investigate why such misuse
occurs and how to prevent it. We conduct a literature review of 114 papers to
verify the prevalence of the misuse and analyze the reasonings behind it. We
then execute an interview study to uncover practitioners' implicit motivations
for using these techniques -- rationales often undisclosed in the literature.
Our findings indicate that misuse of t-SNE and UMAP primarily stems from
limited discourse on their appropriate use in visual analytics. We conclude by
proposing future directions and concrete action items to promote more
reasonable use of DR.

</details>


### [38] [Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration](https://arxiv.org/abs/2506.08805)
*Stina Klein,Pooja Prajod,Katharina Weitz,Matteo Lavit Nicora,Dimitra Tsovaltzi,Elisabeth André*

Main category: cs.HC

TL;DR: 研究探讨了在工业环境中使用协作机器人时，工人社交互动减少的问题，提出通过虚拟化身（avatar）提升人机协作体验，并通过焦点小组研究验证了其潜力。


<details>
  <summary>Details</summary>
Motivation: 协作机器人在工业环境中的应用可能导致工人社交互动减少，影响其福祉，虚拟化身被认为是一种潜在的解决方案以改善人机协作体验，但目前缺乏实际应用的视角。

Method: 研究采用焦点小组方法，邀请德国制造公司员工参与，先通过实验室模拟工业场景的人机协作演示，再讨论虚拟化身的应用。

Result: 研究发现虚拟化身在工业工作单元中有潜在价值，提出了对其行为的改进建议，并强调了个性化沟通和任务支持的重要性。

Conclusion: 尽管研究结果难以推广，但为探索适应性强、上下文感知的虚拟化身在真实工业环境中的应用迈出了第一步。

Abstract: The integration of collaborative robots (cobots) in industrial settings
raises concerns about worker well-being, particularly due to reduced social
interactions. Avatars - designed to facilitate worker interactions and
engagement - are promising solutions to enhance the human-robot collaboration
(HRC) experience. However, real-world perspectives on avatar-supported HRC
remain unexplored. To address this gap, we conducted a focus group study with
employees from a German manufacturing company that uses cobots. Before the
discussion, participants engaged with a scripted, industry-like HRC demo in a
lab setting. This qualitative approach provided valuable insights into the
avatar's potential roles, improvements to its behavior, and practical
considerations for deploying them in industrial workcells. Our findings also
emphasize the importance of personalized communication and task assistance.
Although our study's limitations restrict its generalizability, it serves as an
initial step in recognizing the potential of adaptive, context-aware avatar
interactions in real-world industrial environments.

</details>


### [39] [From Fads to Classics -- Analyzing Video Game Trend Evolutions through Steam Tags](https://arxiv.org/abs/2506.08881)
*Nicolas Grelier,Johannes Pfau,Nicolas Mathieu,Stéphane Kaufmann*

Main category: cs.HC

TL;DR: 论文通过数据驱动的分析、可视化和解释Steam标签的演变，帮助理解视频游戏趋势的变化，并将趋势分为短时效的潮流、当代时尚和稳定经典。


<details>
  <summary>Details</summary>
Motivation: 视频游戏市场竞争激烈且不可预测，行业需要更好地把握趋势以应对快速变化的市场需求。

Method: 利用Steam标签的数据进行时间演化分析，并邀请工业专家验证结果。

Result: 趋势可分为三类，其平均周期约为四年，可视化工具和开放方法可用于解读趋势变化。

Conclusion: 该研究为视频游戏行业提供了数据驱动的趋势分析工具，帮助行业更好地应对市场挑战。

Abstract: The video game industry deals with a fast-paced, competitive and almost
unpredictable market. Trends of genres, settings and modalities change on a
perpetual basis, studios are often one big hit or miss away from surviving or
perishing, and hitting the pulse of the time has become one of the greatest
challenges for industrials, investors and other stakeholders. In this work, we
aim to support the understanding of video game trends over time based on
data-driven analysis, visualization and interpretation of Steam tag evolutions.
We confirm underlying groundwork that trends can be categorized in short-lived
fads, contemporary fashions, or stable classics, and derived that the surge of
a trend averages at about four years in the realm of video games. After using
industrial experts to validate our findings, we deliver visualizations,
insights and an open approach of deciphering shifts in video game trends.

</details>


### [40] [Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams](https://arxiv.org/abs/2506.08892)
*Tauhid Tanjim,Jonathan St. George,Kevin Ching,Hee Rin Lee,Angelique Taylor*

Main category: cs.HC

TL;DR: 研究探索了多模态机器人通信在医疗团队中如何影响工作负荷和人类对机器人的感知，验证了语音和视觉提示的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在时间敏感环境中如何通过多模态交互提示有效沟通的空白。

Method: 通过实验研究，机器人使用语音和非语音提示在医疗训练场景中辅助团队完成任务。

Result: 语音提示用于物品搜索任务和视觉提示用于任务提醒，能更有效地减少工作负荷并提升感知易用性和实用性。

Conclusion: 多模态交互提升了人机团队协作效率，强调在时间敏感环境中进一步研究人机团队协作的重要性。

Abstract: The human-robot interaction (HRI) field has recognized the importance of
enabling robots to interact with teams. Human teams rely on effective
communication for successful collaboration in time-sensitive environments.
Robots can play a role in enhancing team coordination through real-time
assistance. Despite significant progress in human-robot teaming research, there
remains an essential gap in how robots can effectively communicate with action
teams using multimodal interaction cues in time-sensitive environments. This
study addresses this knowledge gap in an experimental in-lab study to
investigate how multimodal robot communication in action teams affects workload
and human perception of robots. We explore team collaboration in a medical
training scenario where a robotic crash cart (RCC) provides verbal and
non-verbal cues to help users remember to perform iterative tasks and search
for supplies. Our findings show that verbal cues for object search tasks and
visual cues for task reminders reduce team workload and increase perceived ease
of use and perceived usefulness more effectively than a robot with no feedback.
Our work contributes to multimodal interaction research in the HRI field,
highlighting the need for more human-robot teaming research to understand best
practices for integrating collaborative robots in time-sensitive environments
such as in hospitals, search and rescue, and manufacturing applications.

</details>


### [41] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: 论文展示了一种在NXP MCXN947微控制器上实现的关键词检测系统，通过集成神经处理单元，实现在资源受限设备上的实时语音交互，结合MFCC特征提取和CNN分类器，优化模型大小并保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的嵌入式设备提供高效的实时语音交互解决方案。

Method: 采用MFCC特征提取和CNN分类器，结合量化感知训练优化模型大小。

Result: 实验结果显示，利用NPU实现59倍推理速度提升，准确率达97.06%，模型大小为30.58 KB。

Conclusion: 系统证明了在嵌入式平台上实现高效、低功耗语音交互的可行性。

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [42] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: 提出了一种基于物理信息的神经模拟器，利用Kelvinlet先验和FEM模拟，实现实时、精确的软组织变形模拟。


<details>
  <summary>Details</summary>
Motivation: 手术机器人和医学训练需要快速且精确的软组织变形模拟。

Method: 将Kelvinlet先验引入神经模拟器，结合线性和非线性FEM模拟，进行残差学习和正则化。

Result: 方法在多架构中提升网络预测的准确性和物理一致性，同时保持低延迟。

Conclusion: Kelvinlet增强学习是手术应用中高效实时的软组织模拟策略。

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


### [43] [A Real-time 3D Desktop Display](https://arxiv.org/abs/2506.08064)
*Livio Tenze,Enrique Canessa*

Main category: cs.GR

TL;DR: altiro3D C++库的新版本，支持从2D图像或视频流实时合成光场，用于3D显示，采用AI技术优化性能，新增多平台GUI。


<details>
  <summary>Details</summary>
Motivation: 扩展altiro3D库的功能，使其能够处理3D视频流并提供更真实的3D体验。

Method: 利用MiDaS CNN从单张2D图像提取深度图，结合AI技术优化处理性能，并实现多平台GUI。

Result: altiro3D能够处理标准图像、视频流或桌面屏幕部分，支持实时渲染到3D光场设备。

Conclusion: 新版本的altiro3D提升了3D内容的实时处理能力，扩展了应用场景。

Abstract: A new extended version of the altiro3D C++ Library -- initially developed to
get glass-free holographic displays starting from 2D images -- is here
introduced aiming to deal with 3D video streams from either 2D webcam images or
flat video files. These streams are processed in real-time to synthesize
light-fields (in Native format) and feed realistic 3D experiences. The core
function needed to recreate multiviews consists on the use of MiDaS
Convolutional Neural Network (CNN), which allows to extract a depth map from a
single 2D image. Artificial Intelligence (AI) computing techniques are applied
to improve the overall performance of the extended altiro3D Library. Thus,
altiro3D can now treat standard images, video streams or screen portions of a
Desktop where other apps may be also running (like web browsers, video chats,
etc) and render them into 3D. To achieve the latter, a screen region need to be
selected in order to feed the output directly into a light-field 3D device such
as Looking Glass (LG) Portrait. In order to simplify the acquisition of a
Desktop screen area by the user, a multi-platform Graphical User Interface has
been also implemented. Sources available at:
https://github.com/canessae/altiro3D/releases/tag/2.0.0

</details>


### [44] [GATE: Geometry-Aware Trained Encoding](https://arxiv.org/abs/2506.08161)
*Jakub Bokšanský,Daniel Meister,Carsten Benthin*

Main category: cs.GR

TL;DR: 论文提出了一种基于几何感知的编码方法GATE，将特征向量存储在三角网格表面，解决了哈希编码方案的局限性，提升了神经网络的训练控制和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有哈希编码方案存在哈希冲突、分辨率与场景大小选择困难以及内存访问不一致等问题，影响了神经网络算法的效率和近似质量。

Method: 提出GATE编码方法，将特征向量存储在三角网格表面，通过网格颜色解耦特征向量密度与几何密度，并支持自适应细节层次控制。

Result: GATE方法避免了哈希编码的局限性，适用于神经渲染相关算法（如神经辐射缓存），并提供了更精细的训练控制和自适应细节管理。

Conclusion: GATE是一种高效且灵活的编码方法，解决了哈希编码的不足，适用于神经渲染等领域，具有潜在的应用价值。

Abstract: The encoding of input parameters is one of the fundamental building blocks of
neural network algorithms. Its goal is to map the input data to a
higher-dimensional space, typically supported by trained feature vectors. The
mapping is crucial for the efficiency and approximation quality of neural
networks. We propose a novel geometry-aware encoding called GATE that stores
feature vectors on the surface of triangular meshes. Our encoding is suitable
for neural rendering-related algorithms, for example, neural radiance caching.
It also avoids limitations of previous hash-based encoding schemes, such as
hash collisions, selection of resolution versus scene size, and divergent
memory access. Our approach decouples feature vector density from geometry
density using mesh colors, while allowing for finer control over neural network
training and adaptive level-of-detail.

</details>


### [45] [Solving partial differential equations in participating media](https://arxiv.org/abs/2506.08237)
*Bailey Miller,Rohan Sawhney,Keenan Crane,Ioannis Gkioulekas*

Main category: cs.GR

TL;DR: 提出了两种新的算法，用于在复杂微粒子几何的域中高效求解线性椭圆偏微分方程（PDE），比传统方法更准确和高效。


<details>
  <summary>Details</summary>
Motivation: 解决在难以显式建模的复杂微粒子几何域中求解PDE的问题，利用统计特性简化模型。

Method: 提出了volumetric walk on spheres和volumetric walk on stars两种算法，基于指数介质特性，推广了蒙特卡罗方法。

Result: 新算法在解决Laplace边界值问题时，比传统方法（如集合平均和均质化）更准确和高效。

Conclusion: 通过统计建模和新型算法，能够在复杂几何域中高效且准确地求解PDE。

Abstract: We consider the problem of solving partial differential equations (PDEs) in
domains with complex microparticle geometry that is impractical, or
intractable, to model explicitly. Drawing inspiration from volume rendering, we
propose tackling this problem by treating the domain as a participating medium
that models microparticle geometry stochastically, through aggregate
statistical properties (e.g., particle density). We first introduce the problem
setting of PDE simulation in participating media. We then specialize to
exponential media and describe the properties that make them an attractive
model of microparticle geometry for PDE simulation problems. We use these
properties to develop two new algorithms, volumetric walk on spheres and
volumetric walk on stars, that generalize previous Monte Carlo algorithms to
enable efficient and discretization-free simulation of linear elliptic PDEs
(e.g., Laplace) in participating media. We demonstrate experimentally that our
algorithms can solve Laplace boundary value problems with complex microparticle
geometry more accurately and more efficiently than previous approaches, such as
ensemble averaging and homogenization.

</details>


### [46] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akşit*

Main category: cs.GR

TL;DR: 提出一种新型3D全息场景表示方法，利用复值高斯基元优化3D场景，显著提升渲染速度且保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 为推进物理逼真渲染（尤其是全息显示），需全面建模光的振幅和相位特性。

Method: 基于复值高斯基元的3D高斯泼溅技术，通过RGBD多视角图像直接优化3D全息场景表示。

Result: 相比现有方法，速度提升30x-10,000x，同时保持图像质量。

Conclusion: 该方法首次实现了几何对齐的物理逼真全息场景表示，为未来研究奠定基础。

Abstract: Modeling the full properties of light, including both amplitude and phase, in
3D representations is crucial for advancing physically plausible rendering,
particularly in holographic displays. To support these features, we propose a
novel representation that optimizes 3D scenes without relying on
intensity-based intermediaries. We reformulate 3D Gaussian splatting with
complex-valued Gaussian primitives, expanding support for rendering with light
waves. By leveraging RGBD multi-view images, our method directly optimizes
complex-valued Gaussians as a 3D holographic scene representation. This
eliminates the need for computationally expensive hologram re-optimization.
Compared with state-of-the-art methods, our method achieves 30x-10,000x speed
improvements while maintaining on-par image quality, representing a first step
towards geometrically aligned, physically plausible holographic scene
representations.

</details>


### [47] [Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos](https://arxiv.org/abs/2506.08334)
*Weikun Peng,Jun Lv,Cewu Lu,Manolis Savva*

Main category: cs.GR

TL;DR: 论文提出了一种从动态RGBD视频中重建关节物体的方法，解决了现有方法对数据采集要求高的问题，通过粗到精的框架推断关节参数和分割可动部分，并在更大规模合成数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 日常生活中的关节物体广泛存在，但现有方法需要精心采集的数据，限制了实用性和可扩展性。论文旨在通过普通手持相机拍摄的视频实现关节物体的重建。

Method: 提出了一种粗到精的框架，从动态RGBD视频中推断关节参数和可动部分的分割。

Result: 在构建的784个视频的合成数据集上，该方法显著优于现有方法，并能泛化到真实场景中的不同类别关节物体。

Conclusion: 论文的方法为从动态RGBD视频中重建关节物体提供了一种实用、可扩展且泛化性强的解决方案。

Abstract: Articulated objects are prevalent in daily life. Understanding their
kinematic structure and reconstructing them have numerous applications in
embodied AI and robotics. However, current methods require carefully captured
data for training or inference, preventing practical, scalable, and
generalizable reconstruction of articulated objects. We focus on reconstruction
of an articulated object from a casually captured RGBD video shot with a
hand-held camera. A casually captured video of an interaction with an
articulated object is easy to acquire at scale using smartphones. However, this
setting is quite challenging, as the object and camera move simultaneously and
there are significant occlusions as the person interacts with the object. To
tackle these challenges, we introduce a coarse-to-fine framework that infers
joint parameters and segments movable parts of the object from a dynamic RGBD
video. To evaluate our method under this new setting, we build a 20$\times$
larger synthetic dataset of 784 videos containing 284 objects across 11
categories. We compare our approach with existing methods that also take video
as input. Experiments show that our method can reconstruct synthetic and real
articulated objects across different categories from dynamic RGBD videos,
outperforming existing methods significantly.

</details>


### [48] [Fine-Grained Spatially Varying Material Selection in Images](https://arxiv.org/abs/2506.09023)
*Julia Guerrero-Viu,Michael Fischer,Iliyan Georgiev,Elena Garces,Diego Gutierrez,Belen Masia,Valentin Deschaintre*

Main category: cs.GR

TL;DR: 提出了一种基于ViT模型的多分辨率处理策略，用于图像中的材料选择，并引入了包含密集标注的两级数据集DuMaS。


<details>
  <summary>Details</summary>
Motivation: 图像编辑中材料选择的鲁棒性和准确性需求，尤其是在光照和反射变化的情况下。

Method: 利用ViT模型特征和多分辨率处理策略，结合DuMaS数据集的两级标注。

Result: 比现有方法更精细和稳定的选择结果。

Conclusion: 该方法在材料选择任务中表现出色，为下游编辑任务提供了可靠支持。

Abstract: Selection is the first step in many image editing processes, enabling faster
and simpler modifications of all pixels sharing a common modality. In this
work, we present a method for material selection in images, robust to lighting
and reflectance variations, which can be used for downstream editing tasks. We
rely on vision transformer (ViT) models and leverage their features for
selection, proposing a multi-resolution processing strategy that yields finer
and more stable selection results than prior methods. Furthermore, we enable
selection at two levels: texture and subtexture, leveraging a new two-level
material selection (DuMaS) dataset which includes dense annotations for over
800,000 synthetic images, both on the texture and subtexture levels.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [49] [Power Domain Sparse Dimensional Constellation Multiple Access (PD-SDCMA) for Enabled Flexible PONs](https://arxiv.org/abs/2506.08053)
*Yuhao Lian,Xiao Han,Xinmao Deng*

Main category: cs.ET

TL;DR: 提出了一种新型传输技术PD-SDCMA，通过高维信号空间中的稀疏叠加，减少多用户干扰并提升系统容量，适用于下一代光纤接入系统。


<details>
  <summary>Details</summary>
Motivation: 解决传统正交多址技术（OMA-PON）在下一代PON中频谱效率和灵活性不足的问题。

Method: 采用高维信号空间中的S2D策略，将低维星座稀疏叠加至高维空间，支持高阶调制格式和多用户接入。

Result: 在25公里单模光纤系统中，PD-SDCMA相比PD-NOMA和3D-NOMA支持更多用户且显著降低误码率。

Conclusion: PD-SDCMA为柔性PON的演进提供了一种高效低成本的解决方案。

Abstract: With the commercial deployment of 5G and the in-depth research of 6G, the
demand for high-speed data services in the next-generation fiber optic access
systems is growing increasingly. Passive optical networks (PONs) have become a
research hotspot due to their characteristics of low loss, high bandwidth, and
low cost. However, the traditional orthogonal multiple access (OMA-PON) has
difficulty meeting the requirements of the next-generation PON for high
spectral efficiency and flexibility. In this paper, a novel transmission
technology, namely power-domain sparse dimension constellation multiple access
(PD-SDCMA), is proposed for the first time. Through the signal space dimension
selection strategy (S2D-strategy) in the high-dimensional signal space, the
low-dimensional constellation is sparsely superimposed into the
high-dimensional space, thereby reducing multi-user interference and enhancing
the system capacity. PD-SDCMA supports higher-order modulation formats and more
access groups, and is also compatible with the existing orthogonal frequency
division multiplexing (OFDM) architecture. The simulation results show that in
a 25 km single-mode fiber system, compared with PD-NOMA and 3D-NOMA, PD-SDCMA
can support more users and significantly reduce BER. This technology provides
an efficient and low-cost solution for the evolution of Flexible PONs.

</details>


### [50] [Educators' Perceptions of Large Language Models as Tutors: Comparing Human and AI Tutors in a Blind Text-only Setting](https://arxiv.org/abs/2506.08702)
*Sankalan Pal Chowdhury,Terry Jingchen Zhang,Donya Rooein,Dirk Hovy,Tanja Käser,Mrinmaya Sachan*

Main category: cs.ET

TL;DR: LLMs在辅导学生解决数学应用题时，在同理心等方面优于人类导师。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs作为个人导师的潜力，并与人类导师在多个维度上进行比较。

Method: 让人类导师对LLM导师和人类导师在同理心等方面的表现进行标注和比较。

Result: LLM导师在同理心等方面表现更佳，80%的标注者更偏好LLM导师。

Conclusion: LLMs有望成为高效导师，减轻人类教师的负担。

Abstract: The rapid development of Large Language Models (LLMs) opens up the
possibility of using them as personal tutors. This has led to the development
of several intelligent tutoring systems and learning assistants that use LLMs
as back-ends with various degrees of engineering. In this study, we seek to
compare human tutors with LLM tutors in terms of engagement, empathy,
scaffolding, and conciseness. We ask human tutors to annotate and compare the
performance of an LLM tutor with that of a human tutor in teaching grade-school
math word problems on these qualities. We find that annotators with teaching
experience perceive LLMs as showing higher performance than human tutors in all
4 metrics. The biggest advantage is in empathy, where 80% of our annotators
prefer the LLM tutor more often than the human tutors. Our study paints a
positive picture of LLMs as tutors and indicates that these models can be used
to reduce the load on human teachers in the future.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [51] [PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production](https://arxiv.org/abs/2506.08528)
*Yu Guan,Zhiyu Yin,Haoyu Chen,Sheng Cheng,Chaojie Yang,Tianyin Xu,Yang Zhang,Hanyu Zhao,Yong Li,Dennis Cai,Ennan Zhai*

Main category: cs.DC

TL;DR: 论文提出了PerfTracker，首个利用细粒度分析的在线故障排查系统，用于诊断大规模模型训练中的性能问题。


<details>
  <summary>Details</summary>
Motivation: 由于现代GPU集群规模庞大、软硬件交互复杂且训练过程数据密集，解决大型模型训练（LMT）性能问题极具挑战性，现有方法难以适用。

Method: PerfTracker通过在线细粒度分析，总结LMT函数的运行时行为模式，利用差异可观测性定位根本原因，同时对生产影响最小化。

Result: PerfTracker已部署在规模达O(10,000) GPU的生产集群中，成功诊断多种复杂性能问题。

Conclusion: PerfTracker为大规模模型训练性能问题的诊断提供了高效且实用的解决方案。

Abstract: Troubleshooting performance problems of large model training (LMT) is
immensely challenging, due to unprecedented scales of modern GPU clusters, the
complexity of software-hardware interactions, and the data intensity of the
training process. Existing troubleshooting approaches designed for traditional
distributed systems or datacenter networks fall short and can hardly apply to
real-world training systems. In this paper, we present PerfTracker, the first
online troubleshooting system utilizing fine-grained profiling, to diagnose
performance issues of large-scale model training in production. PerfTracker can
diagnose performance issues rooted in both hardware (e.g., GPUs and their
interconnects) and software (e.g., Python functions and GPU operations). It
scales to LMT on modern GPU clusters. PerfTracker effectively summarizes
runtime behavior patterns of fine-grained LMT functions via online profiling,
and leverages differential observability to localize the root cause with
minimal production impact. PerfTracker has been deployed as a production
service for large-scale GPU clusters of O(10, 000) GPUs (product homepage
https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).
It has been used to diagnose a variety of difficult performance issues.

</details>


### [52] [Towards Provenance-Aware Earth Observation Workflows: the openEO Case Study](https://arxiv.org/abs/2506.08597)
*H. Omidi,L. Sacco,V. Hutter,G. Irsiegler,M. Claus,M. Schobben,A. Jacob,M. Schramm,S. Fiore*

Main category: cs.DC

TL;DR: 论文提出了一种通过在openEO平台集成数据溯源库yProv4WFs来改进地球观测数据工作流历史记录的方法。


<details>
  <summary>Details</summary>
Motivation: 地球观测（EO）工作流中操作和活动历史的记录对数据溯源至关重要，但目前缺乏有效的工具来记录数据的生成、传输和处理过程。

Method: 将数据溯源库yProv4WFs集成到openEO平台中，以统一和简化用户与地球观测云后端的连接方式。

Result: 集成数据溯源概念使研究人员和利益相关者能够更好地理解分析工作流中的数据流、依赖关系和转换。

Conclusion: 通过yProv4WFs在openEO中的集成，显著提升了地球观测数据处理流程的可追溯性和透明度。

Abstract: Capturing the history of operations and activities during a computational
workflow is significantly important for Earth Observation (EO). The data
provenance helps to collect the metadata that records the lineage of data
products, providing information about how data are generated, transferred,
manipulated, by whom all these operations are performed and through which
processes, parameters, and datasets. This paper presents an approach to improve
those aspects, by integrating the data provenance library yProv4WFs within
openEO, a platform to let users connect to Earth Observation cloud back-ends in
a simple and unified way. In addition, it is demonstrated how the integration
of data provenance concepts across EO processing chains enables researchers and
stakeholders to better understand the flow, the dependencies, and the
transformations involved in analytical workflows.

</details>


### [53] [Blockchain and Edge Computing Nexus: A Large-scale Systematic Literature Review](https://arxiv.org/abs/2506.08636)
*Zeinab Nezami,Zhuolun Li,Chuhao Qin,Fatemeh Banaie,Rabiya Khalid,Evangelos Pournaras*

Main category: cs.DC

TL;DR: 论文通过系统文献综述探讨区块链与边缘计算的结合，揭示了两种技术交互的四种模式及其在隐私和安全性方面的应用。


<details>
  <summary>Details</summary>
Motivation: 区块链与边缘计算虽同为去中心化计算范式，但在技术和研究领域上仍显碎片化，论文旨在探索二者结合如何推动创新并解决研究挑战。

Method: 通过收集和分析近6000篇论文，构建包含22个特征和287个属性的分类体系，并采用定量和机器学习方法进行研究。

Result: 研究发现区块链与边缘计算交互的四种模式，重点关注公共（无许可）与私有（有许可）设计，区块链辅助边缘计算在移动计算中提升隐私和安全性。

Conclusion: 区块链与边缘计算的结合具有显著潜力，尤其是在隐私和安全领域，为智能城市等应用提供了创新解决方案。

Abstract: Blockchain and edge computing are two instrumental paradigms of decentralized
computation, driving key advancements in Smart Cities applications such as
supply chain, energy and mobility. Despite their unprecedented impact on
society, they remain significantly fragmented as technologies and research
areas, while they share fundamental principles of distributed systems and
domains of applicability. This paper introduces a novel and large-scale
systematic literature review on the nexus of blockchain and edge computing with
the aim to unravel a new understanding of how the interfacing of the two
computing paradigms can boost innovation to provide solutions to timely but
also long-standing research challenges. By collecting almost 6000 papers from 3
databases and putting under scrutiny almost 1000 papers, we build a novel
taxonomy and classification consisting of 22 features with 287 attributes that
we study using quantitative and machine learning methods. They cover a broad
spectrum of technological, design, epistemological and sustainability aspects.
Results reveal 4 distinguishing patterns of interplay between blockchain and
edge computing with key determinants the public (permissionless) vs. private
(permissioned) design, technology and proof of concepts. They also demonstrate
the prevalence of blockchain-assisted edge computing for improving privacy and
security, in particular for mobile computing applications.

</details>


### [54] [Parallel FFTW on RISC-V: A Comparative Study including OpenMP, MPI, and HPX](https://arxiv.org/abs/2506.08653)
*Alexander Strack,Christopher Taylor,Dirk Pflüger*

Main category: cs.DC

TL;DR: 该论文研究在RISC-V架构上广泛使用的FFTW库的并行扩展性，比较了其与x86-64架构在MPI和OpenMP下的性能差异，并探讨了内存优化对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着RISC-V硬件的发展，核心数量增加，高效的并行化变得尤为重要。本研究旨在评估RISC-V在并行计算中的表现，并与x86-64架构进行对比。

Method: 使用FFTW库在RISC-V和x86-64架构上测试MPI和OpenMP的并行扩展性，并分析HPX-FFT库中的内存优化效果。

Result: 在双精度2D FFT中，x86-64的性能优于RISC-V约8倍；MPI在两种架构上均表现良好，OpenMP则需优化规划；内存优化在RISC-V上效果不明显。

Conclusion: 本研究为RISC-V在大规模并行应用中的发展提供了早期参考，展示了其潜力与当前的性能差距。

Abstract: Rapid advancements in RISC-V hardware development shift the focus from
low-level optimizations to higher-level parallelization. Recent RISC-V
processors, such as the SOPHON SG2042, have 64 cores. RISC-V processors with
core counts comparable to the SG2042, make efficient parallelization as crucial
for RISC-V as the more established processors such as x86-64. In this work, we
evaluate the parallel scaling of the widely used FFTW library on RISC-V for MPI
and OpenMP. We compare it to a 64-core AMD EPYC 7742 CPU side by side for
different types of FFTW planning. Additionally, we investigate the effect of
memory optimization on RISC-V in HPX-FFT, a parallel FFT library based on the
asynchronous many-task runtime HPX using an FFTW backend. We generally observe
a performance delta between the x86-64 and RISC-V chips of factor eight for
double-precision 2D FFT. Effective memory optimizations in HPX-FFT on x86-64 do
not translate to the RISC-V chip. FFTW with MPI shows good scaling up to 64
cores on x86-64 and RISC-V regardless of planning. In contrast, FFTW with
OpenMP requires measured planning on both architectures to achieve good scaling
up to 64 cores. The results of our study mark an early step on the journey to
large-scale parallel applications running on RISC-V.

</details>


### [55] [Synchronization in Anonymous Networks Under Continuous Dynamics](https://arxiv.org/abs/2506.08661)
*Rida Bazzi,Anya Chaturvedi,Andréa W. Richa,Peter Vargas*

Main category: cs.DC

TL;DR: 提出了κ-Synchronizer，用于在非同步动态网络中实现节点间的同步，适用于拓扑持续变化且节点匿名的环境。


<details>
  <summary>Details</summary>
Motivation: 解决在缺乏全局时钟、节点ID或稳定连接的动态网络中实现同步的问题。

Method: 扩展了Pull通信模型，在每个节点的边端口添加1位多写入原子寄存器，设计确定性同步器。

Result: 首次在半同步动态网络中实现同步算法模拟，内存开销与节点最大度数线性相关。

Conclusion: κ-Synchronizer在极端动态网络中有效，为现有算法提供了非平凡应用。

Abstract: We present the $\kappa$-Synchronizer that works in non-synchronous dynamic
networks under minimal assumptions. Our model allows continuous topological
changes without any guarantee of eventual global or partial stabilization and
assumes that nodes are anonymous. This deterministic synchronizer is the first
to enable nodes to simulate a dynamic network synchronous algorithm for
executions in a semi-synchronous dynamic environment under a weakly-fair node
activation scheduler, despite the absence of a global clock, node ids,
persistent connectivity or any assumptions about the edge dynamics (in both the
synchronous and semi-synchronous environments). In summary, we make the
following contributions: (1) we extend the definition of synchronizers to
networks with continuous arbitrary edge dynamics; (2) we present the first
synchronizer from the semi-synchronous to the synchronous model in a network
with continuous arbitrary edge dynamics; and (3) we present non-trivial
applications of the proposed synchronizer to existing algorithms. We assume an
extension of the Pull communication model by adding a single 1-bit multi-writer
atomic register at each edge-port of a node, since we show that the standard
Pull model is not sufficient to allow for non-trivial synchronization in our
scenario. The $\kappa$-Synchronizer operates with memory overhead at the nodes
that is linear on the maximum node degree and logarithmic on the runtime of the
underlying synchronous algorithm being simulated.

</details>


### [56] [Balancing Fixed Number of Nodes Among Multiple Fixed Clusters](https://arxiv.org/abs/2506.08715)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy,Bhuban Padhan*

Main category: cs.DC

TL;DR: 提出了一种动态节点平衡系统，通过实时资源利用率阈值在多集群间共享节点，优化资源利用并降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决云基础设施中固定节点分配导致的资源利用率低下问题。

Method: 引入节点平衡集群组（NBCG），通过节点平衡集群平衡器和调整规则引擎动态管理节点分配。

Result: 优化资源利用率，降低运营成本，并提升云服务提供商的竞争力。

Conclusion: 该系统显著减少计算资源浪费，为云服务提供商带来战略优势。

Abstract: Cloud infrastructure users often allocate a fixed number of nodes to
individual container clusters (e.g., Kubernetes, OpenShift), resulting in
underutilization of computing resources due to asynchronous and variable
workload peaks across clusters. This research proposes a novel system and
method for dynamic rebalancing of a fixed total number of nodes among multiple
fixed clusters based on real-time resource utilization thresholds. By
introducing a Node Balancing Cluster Group (NBCG), clusters are grouped and
allowed to dynamically share nodes through a controlled reallocation mechanism,
managed by a Node Balancing Cluster Balancer and a Resizing Rule Engine. The
system identifies overutilized and underutilized clusters using threshold
parameters, and reassigns nodes without incurring additional provisioning
costs. If reallocation causes a violation of utilization thresholds, the system
reverses the operation to maintain cluster stability. The proposed architecture
not only optimizes resource utilization and operational cost but also
introduces a strategic advantage for cloud service providers like IBM Cloud.
Unlike existing solutions, this approach enables intra-account node sharing
across clusters with strict adherence to user-defined constraints and ensures
consistent cluster state management. This invention has the potential to
significantly reduce computing resource waste and position IBM Cloud services
as more efficient and competitive.

</details>


### [57] [Mycelium: A Transformation-Embedded LSM-Tree](https://arxiv.org/abs/2506.08923)
*Holly Casaletto,Jeff Lefevre,Aldrin Montana,Peter Alvaro*

Main category: cs.DC

TL;DR: 本文提出了一种名为TE-LSM的新型LSM树方法，通过在压缩过程中嵌入数据转换来降低IO成本，并展示了原型系统Mycelium的性能优势。


<details>
  <summary>Details</summary>
Motivation: 传统的LSM树压缩过程成本高，但未被充分利用以嵌入其他有用的数据转换工作，从而提升整体效率。

Method: 提出了TE-LSM方法，并在RocksDB基础上构建了原型系统Mycelium，支持跨列族合并和多种数据转换（如列组拆分、数据格式转换和索引构建）。

Result: 实验显示，Mycelium的写入吞吐量开销仅为20%（低于传统方法的35%至60%），同时读取延迟最高提升425%。

Conclusion: TE-LSM和Mycelium显著提高了数据访问效率，同时减少了压缩成本，展示了在LSM树中嵌入数据转换的潜力。

Abstract: Compaction is a necessary, but often costly background process in
write-optimized data structures like LSM-trees that reorganizes incoming data
that is sequentially appended to logs. In this paper, we introduce
Transformation-Embedded LSM-trees (TE-LSM), a novel approach that transparently
embeds a variety of data transformations into the compaction process. While
many others have sought to reduce the high cost of compaction, TE-LSMs leverage
the opportunity to embed other useful work to amortize IO costs and
amplification. We illustrate the use of a TE-LSM in Mycelium, our prototype
built on top of RocksDB that extends the compaction process through a
cross-column-family merging mechanism. Mycelium enables seamless integration of
a transformer interface and aims to better prepare data for future accesses
based on access patterns. We use Mycelium to explore three types of
transformations: splitting column groups, converting data formats, and index
building. In addition to providing a cost model analysis, we evaluate
Mycelium's write and read performance using YCSB workloads. Our results show
that Mycelium incurs a 20% write throughput overhead - significantly lower than
the 35% to 60% overhead observed in naive approaches that perform data
transformations outside of compaction-while achieving up to 425% improvements
in read latency compared to RocksDB baseline.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [58] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Main category: cs.DB

TL;DR: RADAR是一个系统性评估语言模型在表格数据上进行数据感知推理能力的基准，揭示了前沿模型在处理数据瑕疵时的表现缺陷。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型的数据感知能力（如识别和处理缺失值、异常值等数据瑕疵），这些瑕疵在真实表格数据中常见但未被充分研究，可能影响分析结论的有效性。

Method: 开发RADAR基准，通过程序化扰动模拟数据瑕疵，评估模型行为；包含2980个表格查询对，覆盖9个领域和5种瑕疵类型。

Result: 前沿模型在无瑕疵数据上表现良好，但在引入数据瑕疵后性能显著下降，显示其数据感知分析能力存在关键缺陷。

Conclusion: RADAR为提升表格推理提供了灵活性强的基准资源，支持多样化的瑕疵类型和可控的表格大小，有助于推动稳健的数据感知分析研究。

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


### [59] [LEANN: A Low-Storage Vector Index](https://arxiv.org/abs/2506.08276)
*Yichuan Wang,Shu Liu,Zhifei Li,Yongji Wu,Ziming Mao,Yilong Zhao,Xiao Yan,Zhiying Xu,Yang Zhou,Ion Stoica,Sewon Min,Matei Zaharia,Joseph E. Gonzalez*

Main category: cs.DB

TL;DR: LEANN是一种存储高效的近似最近邻搜索索引，专为资源受限的个人设备设计，大幅降低存储开销，同时保持检索性能和速度。


<details>
  <summary>Details</summary>
Motivation: 随着个性化数据本地存储需求的增加，传统基于嵌入的搜索方法由于高存储开销难以实现，因此需要开发更高效的解决方案。

Method: LEANN结合紧凑的图基结构和高效的动态重新计算策略，实现低存储开销下的快速准确检索。

Result: LEANN将索引大小降至原始数据的5%，存储开销比标准索引小50倍，且在真实问答基准测试中保持90%的top-3召回率，响应时间低于2秒。

Conclusion: LEANN有效解决了本地设备上高存储开销的问题，为嵌入搜索提供了实用的低资源解决方案。

Abstract: Embedding-based search is widely used in applications such as recommendation
and retrieval-augmented generation (RAG). Recently, there is a growing demand
to support these capabilities over personal data stored locally on devices.
However, maintaining the necessary data structure associated with the
embedding-based search is often infeasible due to its high storage overhead.
For example, indexing 100 GB of raw data requires 150 to 700 GB of storage,
making local deployment impractical. Reducing this overhead while maintaining
search quality and latency becomes a critical challenge. In this paper, we
present LEANN, a storage-efficient approximate nearest neighbor (ANN) search
index optimized for resource-constrained personal devices. LEANN combines a
compact graph-based structure with an efficient on-the-fly recomputation
strategy to enable fast and accurate retrieval with minimal storage overhead.
Our evaluation shows that LEANN reduces index size to under 5% of the original
raw data, achieving up to 50 times smaller storage than standard indexes, while
maintaining 90% top-3 recall in under 2 seconds on real-world question
answering benchmarks.

</details>


### [60] [Evaluating Learned Indexes in LSM-tree Systems: Benchmarks,Insights and Design Choices](https://arxiv.org/abs/2506.08671)
*Junfeng Liu,Jiarui Ye,Mengshi Chen,Meng Li,Siqiang Luo*

Main category: cs.DB

TL;DR: 研究了LSM-tree中不同类型学习索引的性能，提出了系统性基准测试，发现内存分配对查找性能提升有限，并提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 随着数据量增长，LSM-tree的查询效率面临挑战，学习索引虽有潜力，但类型有限且优劣不明。

Method: 总结了8种现有学习索引的工作流程及理论成本，提出配置空间，并在统一平台上实现和评估。

Result: 实验发现大内存分配对查找性能提升有限，且学习索引的重新训练开销较小。

Conclusion: 研究为开发者提供了选择和学习索引调优的实用指南。

Abstract: LSM-tree-based data stores are widely used in industry due to their
exceptional performance. However, as data volumes grow, efficiently querying
large-scale databases becomes increasingly challenging. To address this, recent
studies attempted to integrate learned indexes into LSM-trees to enhance lookup
performance, which has demonstrated promising improvements. Despite this, only
a limited range of learned index types has been considered, and the strengths
and weaknesses of different learned indexes remain unclear, making them
difficult for practical use. To fill this gap, we provide a comprehensive and
systematic benchmark to pursue an in-depth understanding of learned indexes in
LSM-tree systems. In this work, we summarize the workflow of 8 existing learned
indexes and analyze the associated theoretical cost. We also identify several
key factors that significantly influence the performance of learned indexes and
conclude them with a novel configuration space, including various index types,
boundary positions, and granularity. Moreover, we implement different learned
index designs on a unified platform to evaluate across various configurations.
Surprisingly, our experiments reveal several unexpected insights, such as the
marginal lookup enhancement when allocating a large memory budget to learned
indexes and modest retraining overhead of learned indexes. Besides, we also
offer practical guidelines to help developers intelligently select and tune
learned indexes for custom use cases.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [61] [ABC-FHE : A Resource-Efficient Accelerator Enabling Bootstrappable Parameters for Client-Side Fully Homomorphic Encryption](https://arxiv.org/abs/2506.08461)
*Sungwoong Yune,Hyojeong Lee,Adiwena Putra,Hyunjun Cho,Cuong Duong Manh,Jaeho Jeon,Joo-Young Kim*

Main category: cs.AR

TL;DR: ABC-FHE是一种高效的全同态加密加速器，专为解决客户端计算瓶颈而设计，通过流式架构和创新的密钥技术显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 全同态加密（FHE）由于计算开销大，客户端处理成为瓶颈，尤其是在需要支持可自举参数的情况下，亟需高效解决方案。

Method: 提出ABC-FHE，采用流式架构、可重构傅里叶引擎、片上伪随机数生成器和统一即时旋转因子生成器，优化任务调度以减少内存需求和延迟。

Result: 在28nm技术下，ABC-FHE的芯片面积为28.638 mm²，功耗5.654 W，编码和加密速度比CPU快1112倍，比现有客户端加速器快214倍。解密和解码速度分别快963倍和82倍。

Conclusion: ABC-FHE在客户端显著提升了全同态加密的性能和效率，为隐私保护计算提供了可行的解决方案。

Abstract: As the demand for privacy-preserving computation continues to grow, fully
homomorphic encryption (FHE)-which enables continuous computation on encrypted
data-has become a critical solution. However, its adoption is hindered by
significant computational overhead, requiring 10000-fold more computation
compared to plaintext processing. Recent advancements in FHE accelerators have
successfully improved server-side performance, but client-side computations
remain a bottleneck, particularly under bootstrappable parameter
configurations, which involve combinations of encoding, encrypt, decoding, and
decrypt for large-sized parameters. To address this challenge, we propose
ABC-FHE, an area- and power-efficient FHE accelerator that supports
bootstrappable parameters on the client side. ABC-FHE employs a streaming
architecture to maximize performance density, minimize area usage, and reduce
off-chip memory access. Key innovations include a reconfigurable Fourier engine
capable of switching between NTT and FFT modes. Additionally, an on-chip
pseudo-random number generator and a unified on-the-fly twiddle factor
generator significantly reduce memory demands, while optimized task scheduling
enhances the CKKS client-side processing, achieving reduced latency. Overall,
ABC-FHE occupies a die area of 28.638 mm2 and consumes 5.654 W of power in 28
nm technology. It delivers significant performance improvements, achieving a
1112x speed-up in encoding and encryption execution time compared to a CPU, and
214x over the state-of-the-art client-side accelerator. For decoding and
decryption, it achieves a 963x speed-up over the CPU and 82x over the
state-of-the-art accelerator.

</details>


### [62] [CoQMoE: Co-Designed Quantization and Computation Orchestration for Mixture-of-Experts Vision Transformer on FPGA](https://arxiv.org/abs/2506.08496)
*Jiale Dong,Hao Wu,Zihao Wang,Wenqi Lou,Zhendong Zheng,Lei Gong,Chao Wang,Xuehai Zhou*

Main category: cs.AR

TL;DR: 提出了一种新的FPGA加速器，通过双阶段量化方案和资源感知架构，显著提升Mixture-of-Experts Vision Transformers (MoE-ViTs)的性能和能效。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViTs)在计算和内存需求上较高，难以在资源受限设备上部署，而MoE-ViTs虽有助缓解，但FPGA上的实现仍受资源限制。

Method: 采用双阶段量化方案（结合精度保持量化器和硬件友好量化器）和资源感知加速器架构（优化注意力内核和可重用线性操作符）。

Result: 实验显示，加速器实现了155帧/秒的吞吐量，比现有FPGA MoE加速器提升5.35倍，能耗降低80%以上，且精度损失<1%。

Conclusion: 该方法有效平衡了性能与资源消耗，为资源受限设备上的ViTs部署提供了高效解决方案。

Abstract: Vision Transformers (ViTs) exhibit superior performance in computer vision
tasks but face deployment challenges on resource-constrained devices due to
high computational/memory demands. While Mixture-of-Experts Vision Transformers
(MoE-ViTs) mitigate this through a scalable architecture with sub-linear
computational growth, their hardware implementation on FPGAs remains
constrained by resource limitations. This paper proposes a novel accelerator
for efficiently implementing quantized MoE models on FPGAs through two key
innovations: (1) A dual-stage quantization scheme combining
precision-preserving complex quantizers with hardware-friendly simplified
quantizers via scale reparameterization, with only 0.28 $\%$ accuracy loss
compared to full precision; (2) A resource-aware accelerator architecture
featuring latency-optimized streaming attention kernels and reusable linear
operators, effectively balancing performance and resource consumption.
Experimental results demonstrate that our accelerator achieves nearly 155
frames per second, a 5.35$\times$ improvement in throughput, and over $80\%$
energy reduction compared to state-of-the-art (SOTA) FPGA MoE accelerators,
while maintaining $<1\%$ accuracy loss across vision benchmarks. Our
implementation is available at https://github.com/DJ000011/CoQMoE.

</details>


### [63] [POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration](https://arxiv.org/abs/2506.08785)
*Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: 提出PARV-CE，一种支持多精度格式的统一数据路径MAC引擎，通过硬件-软件协同设计优化性能和能耗。


<details>
  <summary>Details</summary>
Motivation: AI模型复杂性增加，需要灵活支持多精度格式的硬件，尤其是面向边缘平台。

Method: 采用分层自适应精度策略，结合可重构SIMD流水线，实现量化感知执行。

Result: 相比现有设计，PDP提升2倍，资源使用减少3倍，精度损失仅1.8%。

Conclusion: PARV-CE是面向边缘的高效可扩展AI加速解决方案。

Abstract: The increasing complexity of AI models requires flexible hardware capable of
supporting diverse precision formats, particularly for energy-constrained edge
platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC
engine that performs efficient multiply-accumulate operations using a unified
data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The
architecture incorporates a layer adaptive precision strategy to align
computational accuracy with workload sensitivity, optimizing both performance
and energy usage. PARV-CE integrates quantization-aware execution with a
reconfigurable SIMD pipeline, enabling high-throughput processing with minimal
overhead through hardware-software co-design. The results demonstrate up to 2x
improvement in PDP and 3x reduction in resource usage compared to SoTA designs,
while retaining accuracy within 1.8% FP32 baseline. The architecture supports
both on-device training and inference across a range of workloads, including
DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE
incorporated POLARON as a scalable and energy-efficient solution for
precision-adaptive AI acceleration at edge.

</details>


### [64] [STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN Accelerator with Algorithm and Hardware Co-Design](https://arxiv.org/abs/2506.08842)
*Kainan Wang,Chengyi Yang,Chengting Yu,Yee Sin Ang,Bo Wang,Aili Wang*

Main category: cs.AR

TL;DR: STI-SNN加速器通过算法与硬件协同设计，解决了SNN的时间依赖性和不规则性问题，实现了高能效、低延迟和灵活性。


<details>
  <summary>Details</summary>
Motivation: 由于SNN的时间依赖性和尖峰不规则性，现有硬件在并行处理和数据重用方面表现不足，影响延迟和能效。

Method: 采用TET损失函数进行时间剪枝，结合OS数据流优化硬件设计，支持多种卷积方法和并行处理。

Result: STI-SNN实现了单时间步推理，减少了内存访问成本，提升了数据重用效率，同时支持灵活的计算模式。

Conclusion: STI-SNN为资源受限应用提供了一种高效、灵活且低延迟的加速器解决方案。

Abstract: Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for
their event-driven characteristics and high energy efficiency. However, the
temporal dependency and irregularity of spikes present significant challenges
for hardware parallel processing and data reuse, leading to some existing
accelerators falling short in processing latency and energy efficiency. To
overcome these challenges, we introduce the STI-SNN accelerator, designed for
resource-constrained applications with high energy efficiency, flexibility, and
low latency. The accelerator is designed through algorithm and hardware
co-design. Firstly, STI-SNN can perform inference in a single timestep. At the
algorithm level, we introduce a temporal pruning approach based on the temporal
efficient training (TET) loss function. This approach alleviates spike
disappearance during timestep reduction, maintains inference accuracy, and
expands TET's application. In hardware design, we analyze data access patterns
and adopt the output stationary (OS) dataflow, eliminating the need to store
membrane potentials and access memory operations. Furthermore, based on the OS
dataflow, we propose a compressed and sorted representation of spikes, then
cached in the line buffer to reduce the memory access cost and improve reuse
efficiency. Secondly, STI-SNN supports different convolution methods. By
adjusting the computation mode of processing elements (PEs) and parameterizing
the computation array, STI-SNN can accommodate lightweight models based on
depthwise separable convolutions (DSCs), further enhancing hardware
flexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer
parallel processing. For inter-layer parallelism, we ...

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [65] [k-Planar and Fan-Crossing Drawings and Transductions of Planar Graphs](https://arxiv.org/abs/2506.08585)
*Petr Hliněný,Jan Jedelský*

Main category: cs.CG

TL;DR: 论文探讨了平面图的FO转导与某种fan-crossing绘图之间的双向联系，并展示了如何利用这种联系推导非转导性和绘图不存在的结论。


<details>
  <summary>Details</summary>
Motivation: 研究平面图的逻辑转换与绘图之间的关系，以解决3D网格等图的k-平面性问题，并探索环面图的转导可能性。

Method: 通过FO转导与fan-planar绘图的关联，分析图类的非转导性和绘图不存在的条件。

Result: 证明了3D网格不是k-平面的，并展示了这种联系可以扩展到任意固定曲面。

Conclusion: 该研究为证明并非所有环面图都能从平面图转导提供了新思路，并扩展了最近的弱稀疏FO转导表征结果。

Abstract: We introduce a two-way connection between FO transductions (logical
transformations) of planar graphs, and a certain variant of fan-crossing
(fan-planar) drawings of graphs which for bounded-degree graphs essentially
reduces to being k-planar for fixed k. For graph classes, this connection
allows to derive non-transducibility results from nonexistence of the said
drawings and, conversely, from nonexistence of a transduction to derive
nonexistence of the said drawings. For example, the class of 3D-grids is not
k-planar for any fixed k. We hope that this connection will help to draw a path
to a possible proof that not all toroidal graphs are transducible from planar
graphs.
  Our characterization can be extended to any fixed surface instead of the
plane. The result is based on a very recent characterization of weakly sparse
FO transductions of classes of bounded expansion by [Gajarsk\'y, G{\l}adkowski,
Jedelsk\'y, Pilipczuk and Toru\'nczyk, arXiv:2505.15655].

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [66] [Qymera: Simulating Quantum Circuits using RDBMS](https://arxiv.org/abs/2506.08759)
*Tim Littau,Rihan Hai*

Main category: quant-ph

TL;DR: Qymera是一种利用关系数据库管理系统（RDBMS）进行量子电路模拟的系统，通过将量子电路翻译为SQL查询实现高效模拟。


<details>
  <summary>Details</summary>
Motivation: 量子电路模拟在验证量子算法方面至关重要，传统方法可能效率不足，因此需要创新的解决方案。

Method: 将量子电路转化为SQL查询，利用RDBMS进行模拟，并提供图形化电路构建器和代码接口。

Result: Qymera支持多种量子电路，并能够与传统方法进行性能对比，展现了其在开发和教育中的实用性。

Conclusion: Qymera通过SQL实现量子模拟，为量子计算和数据管理提供了高效、集成的解决方案。

Abstract: Quantum circuit simulation is crucial for quantum computing such as
validating quantum algorithms. We present Qymera, a system that repurposes
relational database management systems (RDBMSs) for simulation by translating
circuits into SQL queries, allowing quantum operations to run natively within
an RDBMS. Qymera supports a wide range of quantum circuits, offering a
graphical circuit builder and code-based interfaces to input circuits. With a
benchmarking framework, Qymera facilitates comparison of RDBMS-based simulation
against state-of-the-art simulation methods. Our demonstration showcases
Qymera's end-to-end SQL-based execution, seamless integration with classical
workflows, and its utility for development, benchmarking, and education in
quantum computing and data management.

</details>


### [67] [Superposed Parameterised Quantum Circuits](https://arxiv.org/abs/2506.08749)
*Viktoria Patapovich,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: 量子机器学习在高维数据分析中有潜力，但现有方法受限于线性酉操作和共享可训练参数。论文提出叠加参数化量子电路，结合flip-flop量子随机存储和RUS协议，实现指数级参数化子模型和多激活函数，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决量子机器学习中线性酉操作和共享参数限制表达性和可扩展性的问题，提出更高性能的量子电路架构。

Method: 引入叠加参数化量子电路，结合flip-flop量子随机存储和RUS协议，通过振幅变换和后选择实现多参数并行训练和非线性激活。

Result: 数值实验表明，在1D阶跃函数回归中，MSE降低三个数量级；在2D星形分类任务中，精度提升至81.4%，方差减少三倍。

Conclusion: 叠加参数化量子电路为实现硬件高效、更深层次的量子机器学习提供了新途径。

Abstract: Quantum machine learning has shown promise for high-dimensional data
analysis, yet many existing approaches rely on linear unitary operations and
shared trainable parameters across outputs. These constraints limit
expressivity and scalability relative to the multi-layered, non-linear
architectures of classical deep networks. We introduce superposed parameterised
quantum circuits to overcome these limitations. By combining flip-flop quantum
random-access memory with repeat-until-success protocols, a superposed
parameterised quantum circuit embeds an exponential number of parameterised
sub-models in a single circuit and induces polynomial activation functions
through amplitude transformations and post-selection. We provide an analytic
description of the architecture, showing how multiple parameter sets are
trained in parallel while non-linear amplitude transformations broaden
representational power beyond conventional quantum kernels. Numerical
experiments underscore these advantages: on a 1D step-function regression a
two-qubit superposed parameterised quantum circuit cuts the mean-squared error
by three orders of magnitude versus a parameter-matched variational baseline;
on a 2D star-shaped two-dimensional classification task, introducing a
quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance
three-fold. These results position superposed parameterised quantum circuits as
a hardware-efficient route toward deeper, more versatile parameterised quantum
circuits capable of learning complex decision boundaries.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [68] [Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization](https://arxiv.org/abs/2506.08493)
*Qilin Yin,Wei Lu,Xiangyang Luo,Xiaochun Cao*

Main category: cs.CV

TL;DR: 该论文提出了一种通用的上下文感知对比学习框架（UniCaCLF），用于解决视频中部分片段被篡改的时间伪造定位（TFL）问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多媒体取证研究主要关注真假视频分类，而忽略了视频部分片段被篡改的情况，时间伪造定位（TFL）更具现实意义但更具挑战性。

Method: 采用基于监督对比学习的框架，通过异常检测识别伪造片段；提出上下文感知感知层和自适应上下文更新器，构建对比目标，增强伪造与真实片段特征的区分度。

Result: 在五个公开数据集上的实验表明，UniCaCLF显著优于现有算法。

Conclusion: UniCaCLF通过上下文感知对比学习有效提升了时间伪造定位的性能，解决了现有方法的局限性。

Abstract: Most research efforts in the multimedia forensics domain have focused on
detecting forgery audio-visual content and reached sound achievements. However,
these works only consider deepfake detection as a classification task and
ignore the case where partial segments of the video are tampered with. Temporal
forgery localization (TFL) of small fake audio-visual clips embedded in real
videos is still challenging and more in line with realistic application
scenarios. To resolve this issue, we propose a universal context-aware
contrastive learning framework (UniCaCLF) for TFL. Our approach leverages
supervised contrastive learning to discover and identify forged instants by
means of anomaly detection, allowing for the precise localization of temporal
forged segments. To this end, we propose a novel context-aware perception layer
that utilizes a heterogeneous activation operation and an adaptive context
updater to construct a context-aware contrastive objective, which enhances the
discriminability of forged instant features by contrasting them with genuine
instant features in terms of their distances to the global context. An
efficient context-aware contrastive coding is introduced to further push the
limit of instant feature distinguishability between genuine and forged instants
in a supervised sample-by-sample manner, suppressing the cross-sample influence
to improve temporal forgery localization performance. Extensive experimental
results over five public datasets demonstrate that our proposed UniCaCLF
significantly outperforms the state-of-the-art competing algorithms.

</details>


### [69] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: 提出了DGMR方法，通过Gram-Schmidt修剪策略减少视觉Transformer中MLP模块的参数，减少了57%以上的参数和计算量，性能损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 大模型参数导致计算和内存成本过高，研究发现MLP模块占用了大部分参数，因此需要压缩模型以降低成本。

Method: 采用多样性指导的MLP压缩方法（DGMR），通过Gram-Schmidt权重修剪策略消除MLP隐藏层的冗余神经元，同时保留权重多样性以提高性能恢复能力。

Result: 实验表明，DGMR方法在多个先进视觉Transformer上实现了57%以上的参数和计算量减少，性能损失极小。对于EVA-CLIP-E模型，参数和计算量减少了71.5%，且性能未下降。

Conclusion: DGMR方法能够高效压缩视觉Transformer，显著减少参数和计算量，同时保持模型性能。

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [70] [Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement](https://arxiv.org/abs/2506.08555)
*Xinyue Niu,Akira Furui*

Main category: cs.CV

TL;DR: 论文提出了一种通过特征解耦消除校准需求的方法，实现了跨使用者肌肉电信号（EMG）模式识别。


<details>
  <summary>Details</summary>
Motivation: 跨使用者EMG模式识别因肌肉解剖、电极放置和信号特征的个体差异而面临挑战，传统方法需大量校准数据，不适用于大规模实际部署。

Method: 提出了一种端到端双分支对抗神经网络，通过解耦EMG特征为模式特定和个体特定成分，实现无校准的识别。

Result: 实验表明，该方法对未见使用者的数据表现优异，优于多种基线方法。

Conclusion: 研究为无校准的跨使用者EMG模式识别提供了新思路，并展示了其在任务无关生物识别系统中的潜力。

Abstract: Cross-subject electromyography (EMG) pattern recognition faces significant
challenges due to inter-subject variability in muscle anatomy, electrode
placement, and signal characteristics. Traditional methods rely on
subject-specific calibration data to adapt models to new users, an approach
that is both time-consuming and impractical for large-scale, real-world
deployment. This paper presents an approach to eliminate calibration
requirements through feature disentanglement, enabling effective cross-subject
generalization. We propose an end-to-end dual-branch adversarial neural network
that simultaneously performs pattern recognition and individual identification
by disentangling EMG features into pattern-specific and subject-specific
components. The pattern-specific components facilitate robust pattern
recognition for new users without model calibration, while the subject-specific
components enable downstream applications such as task-invariant biometric
identification. Experimental results demonstrate that the proposed model
achieves robust performance on data from unseen users, outperforming various
baseline methods in cross-subject scenarios. Overall, this study offers a new
perspective for cross-subject EMG pattern recognition without model calibration
and highlights the proposed model's potential for broader applications, such as
task-independent biometric systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez,Nisha Singh,Lauren Dyson,Blythe Adamson,Qianyu Yuan,Megan W. Hildner,Erin Fidyk,Olive Mbah,Farhad Khan,Kathi Seidl-Rathkopf,Aaron B. Cohen*

Main category: cs.LG

TL;DR: 本文提出了一个综合框架，用于评估大型语言模型（LLMs）从电子健康记录（EHRs）中提取临床数据的质量，解决现有质量保证框架未完全涵盖的独特问题。


<details>
  <summary>Details</summary>
Motivation: 确保LLMs提取数据的可靠性、准确性和公平性对肿瘤学研究至关重要，但现有框架未能充分应对LLMs的独特挑战。

Method: 提出多维度评估框架，包括变量水平性能基准测试、自动一致性检查和复制分析，并支持偏差评估。

Result: 该框架能识别需改进的变量，系统检测潜在错误，并确认数据集在真实世界研究中的适用性。

Conclusion: 此框架提升行业标准，支持在肿瘤学研究和实践中可信赖地使用AI生成的证据。

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [72] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek,Jan Luxemburk,Richard Plny,Josef Koumar,Jaroslav Pesek,Karel Hynek*

Main category: cs.LG

TL;DR: 论文发现k-NN在流量分类中表现出色，原因在于数据集冗余和常见的数据拆分实践导致性能高估，并建议调整任务制定和评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究k-NN在流量分类中的优异表现背后的原因，以及探索机器学习在网络流量分类中的适用性。

Method: 通过分析12个数据集和15个任务，评估k-NN的性能，并调查冗余数据对结果的影响。

Result: 大多数数据集包含50%以上的冗余样本，导致性能被高估，且标准机器学习方法可能不适合流量分类。

Conclusion: 建议重新制定任务和评估方法，以解决冗余数据问题并调整领域研究方向。

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [73] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan,Guy Amir,Meirav Zehavi,Guy Katz*

Main category: cs.LG

TL;DR: 本文通过计算复杂性理论探讨了集成模型的解释性问题，分析了基础模型的数量、大小和类型如何影响解释性，并发现在一定条件下即使基础模型规模固定，集成解释依然困难。


<details>
  <summary>Details</summary>
Motivation: 集成模型（如提升树）常被视为黑箱，缺乏关于其解释性的严格数学理解。研究旨在填补这一空白，探讨影响集成解释性的关键因素。

Method: 应用计算复杂性理论分析不同集成配置的解释挑战，特别关注基础模型数量、大小和类型的影响。

Result: 研究发现，即使基础模型规模固定，集成解释仍可能难以处理（如P≠NP假设下）；小型决策树集成易解释，而少量线性模型集成仍困难。

Conclusion: 研究为理解集成解释性提供了更坚实的基础，强调通过计算复杂性视角分析的重要性。

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [74] [GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity](https://arxiv.org/abs/2210.16402)
*Artavazd Maranjyan,Mher Safaryan,Peter Richtárik*

Main category: cs.LG

TL;DR: 论文提出了一种改进的分布式优化算法GradSkip，通过减少不重要数据的本地训练步骤，保持通信加速效果，并推广到更通用的压缩和正则化方法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决现有分布式优化算法（如ProxSkip）要求所有客户端在每轮通信中执行相同数量本地训练步骤的问题，以提高效率。

Method: 提出GradSkip方法，允许数据重要性较低的客户端减少本地训练步骤，并推广到支持任意无偏压缩算子和通用正则化器的GradSkip+。

Result: 理论证明GradSkip在强凸条件下线性收敛且通信复杂度不变，实验验证了其有效性。

Conclusion: GradSkip在保持通信加速的同时提高了灵活性，其推广形式GradSkip+可涵盖多种现有方法。

Abstract: We study a class of distributed optimization algorithms that aim to alleviate
high communication costs by allowing clients to perform multiple local
gradient-type training steps before communication. In a recent breakthrough,
Mishchenko et al. (2022) proved that local training, when properly executed,
leads to provable communication acceleration, and this holds in the strongly
convex regime without relying on any data similarity assumptions. However,
their ProxSkip method requires all clients to take the same number of local
training steps in each communication round. We propose a redesign of the
ProxSkip method, allowing clients with ``less important'' data to get away with
fewer local training steps without impacting the overall communication
complexity of the method. In particular, we prove that our modified method,
GradSkip, converges linearly under the same assumptions and has the same
accelerated communication complexity, while the number of local gradient steps
can be reduced relative to a local condition number. We further generalize our
method by extending the randomness of probabilistic alternations to arbitrary
unbiased compression operators and by considering a generic proximable
regularizer. This generalization, which we call GradSkip+, recovers several
related methods in the literature as special cases. Finally, we present an
empirical study on carefully designed toy problems that confirm our theoretical
claims.

</details>


### [75] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Matthias Althoff,Guy Katz*

Main category: cs.LG

TL;DR: 提出了一种基于抽象-精炼的新方法，用于高效计算神经网络预测的可证明充分解释。通过构建简化网络，显著加快验证过程，并迭代优化网络规模以提高解释准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的事后解释方法大多依赖启发式且缺乏形式化保证，现有可证明解释方法在计算上面临可扩展性挑战。本文旨在弥补这一差距。

Method: 提出抽象-精炼技术，通过构建简化网络并迭代调整其规模，高效计算可证明的充分解释。

Result: 实验表明，该方法在提升解释计算效率的同时，还提供了对不同抽象层级网络预测的细粒度解释。

Conclusion: 本文方法在保证解释的充分性前提下，显著提升了计算效率，为神经网络解释提供了新视角。

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [76] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra,Dusan Stosic,Simon Layton*

Main category: cs.LG

TL;DR: 论文探讨了在预训练中使用低精度（MX格式）表示模型参数的技术，改进了舍入模式以实现成功训练。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过精确缩放技术（如MX格式）提高GPU效率，同时避免精度损失，但现有的舍入模式可能导致模型训练发散。

Method: 提出了一种改进的舍入模式，使用“向无穷舍入”计算缩放因子，成功在MXFP8格式下预训练了一个8B参数的模型。

Result: 改进后的方法在15T tokens的数据集上成功预训练了模型，避免了发散问题。

Conclusion: 通过优化舍入模式，MX格式可以在不牺牲准确性的情况下高效支持大规模语言模型的预训练。

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [77] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL提出了一种新的联邦学习框架，通过两种正则化策略模拟IID训练，有效解决了非IID数据下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非IID数据下表现不佳，传统方法成本高或适应性差。

Method: UniVarFL结合Classifier Variance Regularization和Hyperspherical Uniformity Regularization，在客户端层面模拟IID训练。

Result: 在多个基准数据集上，UniVarFL的准确率优于现有方法。

Conclusion: UniVarFL是一种高效且可扩展的解决方案，特别适用于资源受限的环境。

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [78] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang,Ryan Bausback,Feng Bao,Richard Archibald*

Main category: cs.LG

TL;DR: 提出了一种在联邦学习框架下使用随机神经网络作为局部模型的方法，以应对本地数据中的潜在噪声问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中本地数据可能因测量能力有限或人为错误引入噪声，影响模型性能。

Method: 采用随机神经网络作为局部模型，估计数据的真实状态并量化潜在噪声。

Result: 数值实验验证了该方法在处理非独立同分布数据时的性能和有效性。

Conclusion: 联邦随机神经网络能有效处理本地数据中的潜在噪声问题。

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [79] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin,Zhe Chen,Xianhao Chen,Wei Ni,Yue Gao*

Main category: cs.LG

TL;DR: 该论文提出了一种基于自适应控制批量大小和模型分割的分割联邦学习框架HASFL，以解决边缘设备异构性导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有分割联邦学习方法因边缘设备异构性而显著受制于滞后效应，亟需解决资源异构性问题。

Method: 通过推导紧致的收敛边界量化批量大小和模型分割对学习性能的影响，提出自适应控制这些参数以平衡通信计算延迟与训练收敛的HASFL框架。

Result: 在多种数据集上的实验验证了HASFL的有效性，并显示其优于现有最佳方法。

Conclusion: HASFL通过自适应控制策略有效解决了分割联邦学习中的异构性问题，提升了学习性能。

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [80] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand,Rohit Mehra,Priyavanshi Pathania,Nikhil Bamby,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.LG

TL;DR: 论文探讨了大型语言模型（LLMs）的高能耗问题，并提出了基于基准的框架R-ICE来估算推理碳排放。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展带来了巨大的能源消耗问题，可能阻碍可持续发展目标。目前缺乏有效的能源消耗或碳排放估算工具。

Method: 作者提出利用现有的LLM基准数据，开发了一个名为R-ICE的框架，用于估算推理碳排放。该方法更实用且非侵入性。

Result: 验证结果表明，基于基准的建模在碳排放估算方面具有潜力。

Conclusion: R-ICE框架为解决LLM的碳排放问题提供了可行方案，值得进一步研究。

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [81] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/abs/2506.08524)
*Weiguo Wang,Andy Nie,Wenrui Zhou,Yi Kai,Chengchen Hu*

Main category: cs.SD

TL;DR: 论文提出ACORN框架，通过声音训练大型语言模型（LLMs）的物理感知能力，解决其缺乏物理世界理解的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在物理现象理解上的不足，通过声音实现物理感知能力的提升。

Method: 引入基于物理的模拟器生成多样化训练数据，构建AQA-PHY数据集，并提出能处理幅度和相位信息的音频编码器。

Result: 在模拟和真实任务（如直线检测、多普勒效应估计和到达方向估计）中取得合理结果。

Conclusion: ACORN框架为LLMs理解物理世界提供了可行路径。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [82] [Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication](https://arxiv.org/abs/2506.08890)
*Tauhid Tanjim,Promise Ekpo,Huajie Cao,Jonathan St. George,Kevin Ching,Hee Rin Lee,Angelique Taylor*

Main category: cs.RO

TL;DR: 对比了机器人推车（RCC）在紧急场景中的语言与非语言交流方式对医护人员的影响，发现语言交流显著降低心智能耗且效果优于视觉提示与传统推车。


<details>
  <summary>Details</summary>
Motivation: 探讨在真实医疗环境中，机器人推车（RCC）的哪种交流方式（语言或非语言）对医护人员工作负荷和态度影响最小且最有效。

Method: 采用组间实验设计，比较RCC的语言与非语言交流方式与传统推车在急救场景中的表现。

Result: 语言交流显著减少医护人员的心智能耗和工作努力程度，但使用机器人稍增挫败感。

Conclusion: 研究为高风险环境中人机协作提供了重要设计启示，语言交流更优。

Abstract: Healthcare workers (HCWs) encounter challenges in hospitals, such as
retrieving medical supplies quickly from crash carts, which could potentially
result in medical errors and delays in patient care. Robotic crash carts (RCCs)
have shown promise in assisting healthcare teams during medical tasks through
guided object searches and task reminders. Limited exploration has been done to
determine what communication modalities are most effective and least disruptive
to patient care in real-world settings. To address this gap, we conducted a
between-subjects experiment comparing the RCC's verbal and non-verbal
communication of object search with a standard crash cart in resuscitation
scenarios to understand the impact of robot communication on workload and
attitudes toward using robots in the workplace. Our findings indicate that
verbal communication significantly reduced mental demand and effort compared to
visual cues and with a traditional crash cart. Although frustration levels were
slightly higher during collaborations with the robot compared to a traditional
cart, these research insights provide valuable implications for human-robot
teamwork in high-stakes environments.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [83] [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)
*Naseem Babu,Jimson Mathew,A. P. Vinod*

Main category: eess.SP

TL;DR: 综述探讨了大型语言模型（LLMs）与脑电图（EEG）研究的融合，总结了LLMs在EEG分析中的四大应用领域及其技术进展。


<details>
  <summary>Details</summary>
Motivation: LLMs与EEG研究的结合为神经解码、脑机接口和情感计算开辟了新方向，需要系统梳理相关技术和应用。

Method: 通过文献调研和分类，将LLMs在EEG中的应用划分为四大领域：EEG表示学习、EEG到语言解码、跨模态生成及临床应用与管理工具。

Result: 研究发现基于Transformer架构的LLMs通过微调、少样本和零样本学习，能够完成自然语言生成、语义解释和诊断辅助等复杂任务。

Conclusion: 该综述为未来自然语言处理与神经信号分析的结合提供了基础资源，并指出了研究方向的潜力。

Abstract: The growing convergence between Large Language Models (LLMs) and
electroencephalography (EEG) research is enabling new directions in neural
decoding, brain-computer interfaces (BCIs), and affective computing. This
survey offers a systematic review and structured taxonomy of recent
advancements that utilize LLMs for EEG-based analysis and applications. We
organize the literature into four domains: (1) LLM-inspired foundation models
for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal
generation including image and 3D object synthesis, and (4) clinical
applications and dataset management tools. The survey highlights how
transformer-based architectures adapted through fine-tuning, few-shot, and
zero-shot learning have enabled EEG-based models to perform complex tasks such
as natural language generation, semantic interpretation, and diagnostic
assistance. By offering a structured overview of modeling strategies, system
designs, and application areas, this work serves as a foundational resource for
future work to bridge natural language processing and neural signal analysis
through language models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [84] [PoSyn: Secure Power Side-Channel Aware Synthesis](https://arxiv.org/abs/2506.08252)
*Amisha Srivastava,Samit S. Miftah,Hyunmin Kim,Debjit Pal,Kanad Basu*

Main category: cs.CR

TL;DR: PoSyn是一个新型逻辑综合框架，通过优化RTL组件到标准单元的映射来降低功耗侧信道攻击的泄露风险，显著减少攻击成功率并提高面积效率。


<details>
  <summary>Details</summary>
Motivation: 功耗侧信道攻击威胁加密系统的安全性，传统对策如掩码面临集成复杂、面积开销大等问题，需要更高效的解决方案。

Method: PoSyn采用二分图映射方法，结合RTL设计和标准单元库的关键特性，通过优化映射标准减少泄露。

Result: 实验显示，PoSyn在多种加密硬件上显著降低了DPA和CPA攻击成功率（分别降至3%和6%），同时面积效率提升最多3.79倍。

Conclusion: PoSyn在提升安全性的同时优化了硬件性能，是应对功耗侧信道攻击的有效解决方案。

Abstract: Power Side-Channel (PSC) attacks exploit power consumption patterns to
extract sensitive information, posing risks to cryptographic operations crucial
for secure systems. Traditional countermeasures, such as masking, face
challenges including complex integration during synthesis, substantial area
overhead, and susceptibility to optimization removal during logic synthesis. To
address these issues, we introduce PoSyn, a novel logic synthesis framework
designed to enhance cryptographic hardware resistance against PSC attacks. Our
method centers on optimal bipartite mapping of vulnerable RTL components to
standard cells from the technology library, aiming to minimize PSC leakage. By
utilizing a cost function integrating critical characteristics from both the
RTL design and the standard cell library, we strategically modify mapping
criteria during RTL-to-netlist conversion without altering design
functionality. Furthermore, we theoretically establish that PoSyn minimizes
mutual information leakage, strengthening its security against PSC
vulnerabilities. We evaluate PoSyn across various cryptographic hardware
implementations, including AES, RSA, PRESENT, and post-quantum cryptographic
algorithms such as Saber and CRYSTALS-Kyber, at technology nodes of 65nm, 45nm,
and 15nm. Experimental results demonstrate a substantial reduction in success
rates for Differential Power Analysis (DPA) and Correlation Power Analysis
(CPA) attacks, achieving lows of 3% and 6%, respectively. TVLA analysis further
confirms that synthesized netlists exhibit negligible leakage. Additionally,
compared to conventional countermeasures like masking and shuffling, PoSyn
significantly lowers attack success rates, achieving reductions of up to 72%,
while simultaneously enhancing area efficiency by as much as 3.79 times.

</details>


### [85] [ZTaint-Havoc: From Havoc Mode to Zero-Execution Fuzzing-Driven Taint Inference](https://arxiv.org/abs/2506.08838)
*Yuchong Xie,Wenhui Zhang,Dongdong She*

Main category: cs.CR

TL;DR: ZTaint-Havoc通过利用havoc变异方案实现轻量级Fuzz驱动污点推断，显著提升了模糊测试的效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统的污点分析方法在可扩展性和运行时开销方面存在局限，需要一种更高效的替代方案。

Method: 提出了ZTaint-Havoc，基于havoc变异模式的计算模型，实现了零额外执行开销的污点推断。

Result: 在AFL++中实现后，显著提升了边缘覆盖率（最高达51.12%），且运行时开销极低（3.84%-12.58%）。

Conclusion: ZTaint-Havoc是一种高效且低开销的污点推断方法，适用于模糊测试。

Abstract: Fuzzing is a widely used technique for discovering software vulnerabilities,
but identifying hot bytes that influence program behavior remains challenging.
Traditional taint analysis can track such bytes white-box, but suffers from
scalability issue. Fuzzing-Driven Taint Inference (FTI) offers a black-box
alternative, yet typically incurs significant runtime overhead due to extra
program executions. We observe that the commonly used havoc mutation scheme in
fuzzing can be adapted for lightweight FTI with zero extra executions. We
present a computational model of havoc mode, demonstrating that it can perform
FTI while generating new test cases. Building on this, we propose ZTaint-Havoc,
a novel, efficient FTI with minimal overhead (3.84% on UniBench, 12.58% on
FuzzBench). We further design an effective mutation algorithm utilizing the
identified hot bytes. Our comprehensive evaluation shows that ZTaint-Havoc,
implemented in AFL++, improves edge coverage by up to 33.71% on FuzzBench and
51.12% on UniBench over vanilla AFL++, with average gains of 2.97% and 6.12% in
24-hour fuzzing campaigns.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [86] [Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems](https://arxiv.org/abs/2506.08743)
*Michael Färber,David Lamprecht,Yuni Susanti*

Main category: cs.IR

TL;DR: 该论文提出了一种将RDF知识图谱（KGs）与图神经网络（GNNs）全面整合的方法，利用RDF的拓扑信息和内容信息，显著提升了推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量RDF知识图谱，但其丰富的语义信息在GNN-based推荐系统中尚未充分利用，论文旨在填补这一空白。

Method: 通过深入评估多种GNNs，分析不同语义特征初始化和图结构异质性对推荐任务的影响。

Result: 实验表明，利用RDF KGs的语义丰富性显著提升了推荐系统的性能。

Conclusion: 该方法为基于GNN的推荐系统在Linked Open Data云中的应用奠定了基础。

Abstract: Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [87] [WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis](https://arxiv.org/abs/2506.08962)
*Liangliang Chen,Huiru Xie,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: 本文介绍了一款AI驱动的智能导师，用于本科生电路分析课程的作业评估与反馈，展示其设计、部署效果及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为了解决电路分析课程中个性化作业反馈的不足，并帮助教师实时掌握学生难点，提升教学效果。

Method: 设计了具备开放式问答和作业反馈功能的智能导师，优化提示词，部署于Microsoft Azure平台，并收集学生交互数据供分析。

Result: 90.9%的学生对导师表示满意；初步数据分析揭示了常见问题和学生难点，帮助教学调整。

Conclusion: 智能导师在电路分析课程中表现良好，未来将扩展至其他工程学科，并优化功能。

Abstract: This research-to-practice work-in-progress (WIP) paper presents an AI-enabled
smart tutor designed to provide homework assessment and feedback for students
in an undergraduate circuit analysis course. We detail the tutor's design
philosophy and core components, including open-ended question answering and
homework feedback generation. The prompts are carefully crafted to optimize
responses across different problems. The smart tutor was deployed on the
Microsoft Azure platform and is currently in use in an undergraduate circuit
analysis course at the School of Electrical and Computer Engineering in a
large, public, research-intensive institution in the Southeastern United
States. Beyond offering personalized instruction and feedback, the tutor
collects student interaction data, which is summarized and shared with the
course instructor. To evaluate its effectiveness, we collected student
feedback, with 90.9% of responses indicating satisfaction with the tutor.
Additionally, we analyze a subset of collected data on preliminary circuit
analysis topics to assess tutor usage frequency for each problem and identify
frequently asked questions. These insights help instructors gain real-time
awareness of student difficulties, enabling more targeted classroom
instruction. In future work, we will release a full analysis once the complete
dataset is available after the Spring 2025 semester. We also explore the
potential applications of this smart tutor across a broader range of
engineering disciplines by developing improved prompts, diagram-recognition
methods, and database management strategies, which remain ongoing areas of
research.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [88] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Main category: cs.CL

TL;DR: 论文提出了一种利用大语言模型（LLMs）自动分析法律文本语义的新方法，将其转化为可废止道义逻辑（DDL）的形式化表示，并通过结构化流程评估规则一致性，最终验证了LLMs在法律信息学中的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决法律文本语义分析的自动化问题，利用LLMs提升法律信息处理的可扩展性。

Method: 采用结构化流程，将法律文本分割为原子片段，提取道义规则，并通过不同LLM配置（如提示工程、微调模型等）评估规则的一致性。

Result: 实验结果表明，机器生成的形式化表示与专家手工结果高度一致，有效提示的LLMs表现尤为突出。

Conclusion: LLMs在法律文本的形式化表示中具有重要价值，尤其是在提示工程的支持下，能够显著提升法律信息学的效率和规模。

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [89] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: 该论文探讨了在使用不同数值精度和数据并行化策略时，如何平衡训练速度（代表能源和硬件消耗）与模型准确性，以促进低资源环境中的领域适应。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的训练成本高昂，且通常反映主流文化和价值观。领域适应是一种有前景的策略，但其计算成本仍是障碍，尤其是在资源有限的环境中。

Method: 研究评估了不同数值精度和数据并行化策略对训练速度和模型准确性的影响。

Result: 研究结果为关注能效、可访问性或硬件限制的场景提供了实用建议。

Conclusion: 该工作为低资源环境下的领域适应提供了技术支持，有助于提升模型的能源效率和可访问性。

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [90] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Main category: cs.CL

TL;DR: 提出了一种基于自然语言推理的多跳推理方法EXCLAIM，用于自动化检测和解释合规性，利用大型语言模型生成保证案例，并通过GDPR案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于法律和技术文本的复杂性、模型解释需求的增加以及保证案例数据的有限性，自动化合规检测面临挑战。

Method: 采用自然语言推理(NLI)和多跳推理框架，生成保证案例并评估其覆盖率和结构一致性。

Result: 实验表明，基于NLI的方法在自动化合规检测中具有潜力，特别是在GDPR要求的案例中表现良好。

Conclusion: EXCLAIM方法为自动化合规检测提供了可解释和可追溯的解决方案，展示了NLI在实际应用中的价值。

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [91] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
*Flavio D'Intino,Hans-Peter Hutter*

Main category: cs.CL

TL;DR: 该论文介绍了SRB-300数据集，一个包含300小时瑞士德语自发语音的标注语料库，并通过微调OpenAI Whisper模型显著提升了语音转文本的性能。


<details>
  <summary>Details</summary>
Motivation: 瑞士德语作为一种低资源语言，缺乏标准化书写形式，现有语音转文本模型在自发对话语音中表现不佳。

Method: 作者构建了SRB-300数据集并基于其微调了多个OpenAI Whisper模型。

Result: 微调后的模型在词错误率（WER）和BLEU分数上均有显著提升，最佳模型的WER为17.1%，BLEU分数为74.8%。

Conclusion: 该研究为瑞士德语及其他低资源语言的实际应用提供了有效的语音转文本解决方案。

Abstract: Swiss German is a low-resource language represented by diverse dialects that
differ significantly from Standard German and from each other, lacking a
standardized written form. As a result, transcribing Swiss German involves
translating into Standard German. Existing datasets have been collected in
controlled environments, yielding effective speech-to-text (STT) models, but
these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from 39
Swiss German radio and TV stations. It captures spontaneous speech across all
major Swiss dialects recorded in various realistic environments and overcomes
the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance metrics.
Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores
increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a
WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for
developing effective and robust STT systems for Swiss German and other
low-resource languages in real-world contexts.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [Compiling Metric Temporal Answer Set Programming](https://arxiv.org/abs/2506.08150)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Javier Romero,Susana Hahn,Torsten Schaub*

Main category: cs.AI

TL;DR: 开发了一种计算方法来支持度量ASP，解决定量时间约束问题，同时通过差异约束扩展保持可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决度量ASP中定量时间约束（如持续时间和截止时间）的表达问题，尤其是在细粒度时间约束下避免可扩展性问题。

Method: 利用ASP的差异约束扩展，将时间相关部分外部化处理，从而解耦度量ASP与时间粒度。

Result: 提出的方法不受时间精度影响，有效解决了细粒度时间约束下的可扩展性问题。

Conclusion: 该方法成功将度量ASP与时间粒度解耦，为处理定量时间约束提供了高效解决方案。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to
allow for expressing quantitative temporal constrains, like durations and
deadlines. A central challenge is to maintain scalability when dealing with
fine-grained timing constraints, which can significantly exacerbate ASP's
grounding bottleneck. To address this issue, we leverage extensions of ASP with
difference constraints, a simplified form of linear constraints, to handle
time-related aspects externally. Our approach effectively decouples metric ASP
from the granularity of time, resulting in a solution that is unaffected by
time precision.

</details>


### [93] [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/abs/2506.08321)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.AI

TL;DR: LeanTutor是一个基于大型语言模型的数学证明辅导系统，通过自然语言交互和形式化验证帮助学生撰写和修正数学证明。系统包含三个模块：自动形式化/证明检查、下一步生成和自然语言反馈生成。评估结果显示，系统在形式化和错误识别上有一定效果，并在自然语言提示生成上优于简单基线。


<details>
  <summary>Details</summary>
Motivation: 为了解决学生在数学证明中可能遇到的困难，提供一个交互式辅导系统，利用大型语言模型和形式化验证工具（如Lean）帮助学生学习和修正证明。

Method: 系统分为三个模块：(i) 自动形式化和证明检查模块，将学生证明转为Lean代码并验证；(ii) 下一步生成模块，为错误证明生成有效下一步；(iii) 反馈生成模块，提供自然语言提示。评估使用了人写的数据集PeanoBench。

Result: 自动形式化模块正确形式化了57%的正确证明步骤，并在30%的错误证明中准确识别错误步骤。自然语言提示生成在准确性和相关性上优于基线。

Conclusion: LeanTutor展示了基于LLM和形式化验证的数学辅导系统的潜力，但在形式化和错误识别上仍有改进空间。

Abstract: We present LeanTutor, a Large Language Model (LLM)-based tutoring system for
math proofs. LeanTutor interacts with the student in natural language, formally
verifies student-written math proofs in Lean, generates correct next steps, and
provides the appropriate instructional guidance. LeanTutor is composed of three
modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and
(iii) a natural language feedback generator. The first module faithfully
autoformalizes student proofs into Lean and verifies proof accuracy via
successful code compilation. If the proof has an error, the incorrect step is
identified. The next-step generator module outputs a valid next Lean tactic for
incorrect proofs via LLM-based candidate generation and proof search. The
feedback generator module leverages Lean data to produce a
pedagogically-motivated natural language hint for the student user. To evaluate
our system, we introduce PeanoBench, a human-written dataset derived from the
Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each
natural language proof step is paired with the corresponding logically
equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of
tactics in correct proofs and accurately identifies the incorrect step in 30%
of incorrect proofs. In generating natural language hints for erroneous proofs,
LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

</details>


### [94] [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/abs/2506.08462)
*Christos Margadji,Sebastian W. Pattinson*

Main category: cs.AI

TL;DR: CIPHER是一个结合视觉-语言-动作（VLA）的框架，用于工业控制，模仿人类推理能力，支持无监督学习和透明决策。


<details>
  <summary>Details</summary>
Motivation: 工业过程需要鲁棒性和适应性，传统AI依赖标注数据且泛化能力有限，CIPHER旨在解决这一局限。

Method: CIPHER整合过程专家、回归模型和检索增强生成技术，支持物理驱动的链式推理。

Result: 该框架在分布外任务中表现出强泛化能力，能解释决策并生成精确指令，无需显式标注。

Conclusion: CIPHER为工业环境中的自主系统提供了高精度、透明推理的基础。

Abstract: Industrial processes must be robust and adaptable, as environments and tasks
are often unpredictable, while operational errors remain costly and difficult
to detect. AI-based control systems offer a path forward, yet typically depend
on supervised learning with extensive labelled datasets, which limits their
ability to generalize across variable and data-scarce industrial settings.
Foundation models could enable broader reasoning and knowledge integration, but
rarely deliver the quantitative precision demanded by engineering applications.
Here, we introduceControl and Interpretation of Production via Hybrid Expertise
and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming
to replicate human-like reasoning for industrial control, instantiated in a
commercial-grade 3D printer. It integrates a process expert, a regression model
enabling quantitative characterization of system states required for
engineering tasks. CIPHER also incorporates retrieval-augmented generation to
access external expert knowledge and support physics-informed, chain-of-thought
reasoning. This hybrid architecture exhibits strong generalization to
out-of-distribution tasks. It interprets visual or textual inputs from process
monitoring, explains its decisions, and autonomously generates precise machine
instructions, without requiring explicit annotations. CIPHER thus lays the
foundations for autonomous systems that act with precision, reason with
context, and communicate decisions transparently, supporting safe and trusted
deployment in industrial settings.

</details>
