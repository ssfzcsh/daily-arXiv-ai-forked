<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]
- [cs.PL](#cs.PL) [Total: 6]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 19]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 11]
- [cs.HC](#cs.HC) [Total: 19]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [math.CT](#math.CT) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.CE](#cs.CE) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Git Context Controller: Manage the Context of LLM-based Agents like Git](https://arxiv.org/abs/2508.00031)
*Junde Wu*

Main category: cs.SE

TL;DR: 介绍了一个名为Git-Context-Controller (GCC)的结构化上下文管理框架，用于提升大型语言模型代理在长期任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型的代理被部署在长期任务中，上下文管理成为关键瓶颈。

Method: GCC框架通过类似Git的版本控制机制，将代理内存结构化为持久文件系统，支持COMMIT、BRANCH、MERGE等操作。

Result: 实验显示，配备GCC的代理在SWE-Bench-Lite基准测试中表现优异，解决了48.00%的软件错误，并在自复制案例中显著提升任务完成率。

Conclusion: GCC框架有效支持代理管理长期目标、探索替代方案，并在跨会话和跨代理场景中实现内存恢复或传递。

Abstract: Large language model (LLM) based agents have shown impressive capabilities by
interleaving internal reasoning with external tool use. However, as these
agents are deployed in long-horizon workflows, such as coding for a big,
long-term project, context management becomes a critical bottleneck. We
introduce Git-Context-Controller (GCC), a structured context management
framework inspired by software version control systems. GCC elevates context as
versioned memory hierarchy like Git. It structures agent memory as a persistent
file system with explicit operations: COMMIT, BRANCH, MERGE, and CONTEXT,
enabling milestone-based checkpointing, exploration of alternative plans, and
structured reflection. Our approach empowers agents to manage long-term goals,
isolate architectural experiments, and recover or hand off memory across
sessions and agents. Empirically, agents equipped with GCC achieve
state-of-the-art performance on the SWE-Bench-Lite benchmark, resolving 48.00
of software bugs, outperforming 26 competitive systems. In a self-replication
case study, a GCC-augmented agent builds a new CLI agent from scratch,
achieving 40.7 task resolution, compared to only 11.7 without GCC. The code is
released at: https://github.com/theworldofagents/GCC

</details>


### [2] [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,Bruno D. Ferreira-Saraiva,João P. Matos-Carvalho*

Main category: cs.SE

TL;DR: 研究了大型语言模型（LLM）在自动化代码生成中的表现，评估了它们在复杂场景下的功能正确性，发现GPT-4.1表现最佳，但也揭示了LLM和第三方库的局限性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在科学研究中生成Python代码的能力，特别是在面对不熟悉的API时。

Method: 通过结构化零示例提示，测试LLM在数据分析和合成数据生成任务中的表现，并进行功能和提示符合性评估。

Result: 仅有少数模型能生成正确代码，GPT-4.1表现最为稳定；同时也发现了第三方库文档和实现的不足。

Conclusion: 当前LLM在科学自动化中存在局限性，需要改进提示设计、文档和模型能力。

Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating
code generation in scientific research, yet their ability to interpret and use
unfamiliar Python APIs for complex computational experiments remains poorly
characterized. This study systematically benchmarks a selection of
state-of-the-art LLMs in generating functional Python code for two increasingly
challenging scenarios: conversational data analysis with the \textit{ParShift}
library, and synthetic data generation and clustering using \textit{pyclugen}
and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts
specifying detailed requirements but omitting in-context examples. Model
outputs are evaluated quantitatively for functional correctness and prompt
compliance over multiple runs, and qualitatively by analyzing the errors
produced when code execution fails. Results show that only a small subset of
models consistently generate correct, executable code, with GPT-4.1 standing
out as the only model to always succeed in both tasks. In addition to
benchmarking LLM performance, this approach helps identify shortcomings in
third-party libraries, such as unclear documentation or obscure implementation
bugs. Overall, these findings highlight current limitations of LLMs for
end-to-end scientific automation and emphasize the need for careful prompt
design, comprehensive library documentation, and continued advances in language
model capabilities.

</details>


### [3] [Machine Learning Pipeline for Software Engineering: A Systematic Literature Review](https://arxiv.org/abs/2508.00045)
*Samah Kansab*

Main category: cs.SE

TL;DR: 这是一篇关于机器学习在软件工程中应用的系统文献综述，总结了当前ML管道的优缺点及最佳实践，并提出了改进软件质量与效率的实用建议。


<details>
  <summary>Details</summary>
Motivation: 由于软件工程系统复杂度的增加，传统方法难以应对调试时间长、缺陷检测效率低等问题，机器学习成为潜在解决方案。但ML的效果依赖于其管道的鲁棒性。

Method: 本文通过系统文献综述（SLR）分析了ML在软件工程中的最新管道，整理了预处理、特征选择、算法选择、验证和评估等方面的最佳实践与挑战。

Result: 研究发现，鲁棒的预处理（如SMOTE）和特征选择（如SZZ）能提高模型可靠性。集成方法（如随机森林、梯度提升）表现优异，而简单模型（如朴素贝叶斯）适合高效和可解释性场景。

Conclusion: 本文强调了设计良好的ML管道对解决软件工程挑战的重要性，为研究者和实践者提供了优化软件质量与效率的行动指南。通过识别差距与趋势，为复杂开发环境中ML的进一步应用和创新奠定了基础。

Abstract: The rapid advancement of software development practices has introduced
challenges in ensuring quality and efficiency across the software engineering
(SE) lifecycle. As SE systems grow in complexity, traditional approaches often
fail to scale, resulting in longer debugging times, inefficient defect
detection, and resource-heavy development cycles. Machine Learning (ML) has
emerged as a key solution, enabling automation in tasks such as defect
prediction, code review, and release quality estimation. However, the
effectiveness of ML in SE depends on the robustness of its pipeline, including
data collection, preprocessing, feature engineering, algorithm selection,
validation, and evaluation.
  This systematic literature review (SLR) examines state-of-the-art ML
pipelines designed for SE, consolidating best practices, challenges, and gaps.
Our findings show that robust preprocessing, such as SMOTE for data balancing
and SZZ-based algorithms for feature selection, improves model reliability.
Ensemble methods like Random Forest and Gradient Boosting dominate performance
across tasks, while simpler models such as Naive Bayes remain valuable for
efficiency and interpretability. Evaluation metrics including AUC, F1-score,
and precision are most common, with new metrics like Best Arithmetic Mean (BAM)
emerging in niche applications. Validation techniques such as bootstrapping are
widely used to ensure model stability and generalizability.
  This SLR highlights the importance of well-designed ML pipelines for
addressing SE challenges and provides actionable insights for researchers and
practitioners seeking to optimize software quality and efficiency. By
identifying gaps and trends, this study sets a foundation for advancing ML
adoption and fostering innovation in increasingly complex development
environments.

</details>


### [4] [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
*Yihong Dong,Xue Jiang,Jiaru Qian,Tian Wang,Kechi Zhang,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: 本文系统性综述了基于大语言模型（LLM）的代码生成代理技术，分析了其自主性、任务扩展性和工程实用性，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代码生成代理的核心特性及其在软件开发中的潜力。

Method: 系统调查了该领域的技术发展、核心架构（单代理与多代理）、应用场景、评测基准和工具。

Result: 总结了LLM代码生成代理的现状，并提供了未来研究的长期方向。

Conclusion: 该领域应用潜力巨大，需进一步解决工程挑战以推动发展。

Abstract: Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

</details>


### [5] [How Quantization Impacts Privacy Risk on LLMs for Code?](https://arxiv.org/abs/2508.00128)
*Md Nazmul Haque,Hua Yang,Zhou Yang,Bowen Xu*

Main category: cs.SE

TL;DR: 该论文研究了量化技术对代码大语言模型（LLMs4Code）的任务性能和隐私风险的影响，发现量化能显著降低隐私风险，并与任务性能存在权衡关系。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs4Code在训练中可能包含敏感数据（如云服务凭证和个人信息），隐私风险备受关注。量化技术广泛用于降低模型计算成本，但其对隐私保护的影响尚不明确。

Method: 研究采用静态和动态量化技术，应用于Pythia、CodeGen和GPTNeo三种代表性模型家族，通过成员推断（MI）评估隐私风险。

Result: 量化显著降低了隐私风险，且任务性能与隐私风险呈正相关。研究发现，量化后的较大模型可能在任务性能和隐私保护之间取得更好平衡。

Conclusion: 量化是降低LLMs4Code隐私风险的有效方法，研究结果为实际部署中隐私保护提供了实用指导。

Abstract: Large language models for code (LLMs4Code) rely heavily on massive training
data, including sensitive data, such as cloud service credentials of the
projects and personal identifiable information of the developers, raising
serious privacy concerns. Membership inference (MI) has recently emerged as an
effective tool for assessing privacy risk by identifying whether specific data
belong to a model's training set. In parallel, model compression techniques,
especially quantization, have gained traction for reducing computational costs
and enabling the deployment of large models. However, while quantized models
still retain knowledge learned from the original training data, it remains
unclear whether quantization affects their ability to retain and expose privacy
information. Answering this question is of great importance to understanding
privacy risks in real-world deployments. In this work, we conduct the first
empirical study on how quantization influences task performance and privacy
risk simultaneously in LLMs4Code. To do this, we implement widely used
quantization techniques (static and dynamic) to three representative model
families, namely Pythia, CodeGen, and GPTNeo. Our results demonstrate that
quantization has a significant impact on reducing the privacy risk relative to
the original model. We also uncover a positive correlation between task
performance and privacy risk, indicating an underlying tradeoff. Moreover, we
reveal the possibility that quantizing larger models could yield better balance
than using full-precision small models. Finally, we demonstrate that these
findings generalize across different architectures, model sizes and MI methods,
offering practical guidance for safeguarding privacy when deploying compressed
LLMs4Code.

</details>


### [6] [Testing the Untestable? An Empirical Study on the Testing Process of LLM-Powered Software Systems](https://arxiv.org/abs/2508.00198)
*Cleyton Magalhaes,Italo Santos,Brody Stuart-Verner,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文探讨了大型语言模型（LLM）驱动的系统在真实软件开发中的测试方法，提出需要结合手动与自动化策略，并适应传统验证方法的调整。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注LLM在代码生成等任务中的应用，而缺乏对LLM系统开发中测试方法的系统研究。

Method: 通过99份学生报告，采用主题分析和结构化编码进行探索性案例研究。

Result: 测试策略结合手动与自动化方法，涵盖系统逻辑和模型行为，常见挑战包括输出不稳定和提示敏感性。

Conclusion: 测试LLM系统需融合源码推理和行为评估，为生成式组件的软件测试提供了实践依据。

Abstract: Background: Software systems powered by large language models are becoming a
routine part of everyday technologies, supporting applications across a wide
range of domains. In software engineering, many studies have focused on how
LLMs support tasks such as code generation, debugging, and documentation.
However, there has been limited focus on how full systems that integrate LLMs
are tested during development. Aims: This study explores how LLM-powered
systems are tested in the context of real-world application development.
Method: We conducted an exploratory case study using 99 individual reports
written by students who built and deployed LLM-powered applications as part of
a university course. Each report was independently analyzed using thematic
analysis, supported by a structured coding process. Results: Testing strategies
combined manual and automated methods to evaluate both system logic and model
behavior. Common practices included exploratory testing, unit testing, and
prompt iteration. Reported challenges included integration failures,
unpredictable outputs, prompt sensitivity, hallucinations, and uncertainty
about correctness. Conclusions: Testing LLM-powered systems required
adaptations to traditional verification methods, blending source-level
reasoning with behavior-aware evaluations. These findings provide evidence on
the practical context of testing generative components in software systems.

</details>


### [7] [Functional vs. Object-Oriented: Comparing How Programming Paradigms Affect the Architectural Characteristics of Systems](https://arxiv.org/abs/2508.00244)
*Briza Mel Dias de Sousa,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: 比较面向对象编程（OOP）和函数式编程（FP）对软件系统架构特性的影响，通过定性定量分析两种语言实现数字钱包系统的差异。


<details>
  <summary>Details</summary>
Motivation: 随着函数式编程在工业界的关注度上升，研究旨在比较OOP和FP对系统架构的影响，帮助开发者和组织选择更适合的范式。

Method: 使用Kotlin（OOP代表）和Scala（FP代表）实现数字钱包系统，通过定性（自我民族志）和定量（开发者调查）方法分析两种范式的差异。

Result: 定性和定量分析揭示了两种编程范式在系统架构特性上的差异，以及开发者的编写和阅读体验。

Conclusion: 研究结果为开发者和组织在选择编程范式时提供了参考依据，有助于更明智的决策。

Abstract: After decades of dominance by object-oriented programming (OOP), functional
programming (FP) is gaining increasing attention in the software industry. This
study compares the impact of OOP and FP on the architectural characteristics of
software systems. For that, it examines the design and implementation of a
Digital Wallet system, developed in Kotlin (representing OOP) and Scala
(representing FP). The comparison is made through both qualitative and
quantitative analyses to explore how each paradigm influences the system's
architectural characteristics. The self-ethnographic qualitative analysis
provides a side-by-side comparison of both implementations, revealing the
perspective of those writing such code. The survey-based quantitative analysis
gathers feedback from developers with diverse backgrounds, showing their
impressions of those reading this code. Hopefully, these results may be useful
for developers or organizations seeking to make more informed decisions about
which paradigm is best suited for their next project.

</details>


### [8] [Leveraging Large Language Model for Information Retrieval-based Bug Localization](https://arxiv.org/abs/2508.00253)
*Moumita Asad,Rafed Muhammad Yasir,Armin Geramirad,Sam Malek*

Main category: cs.SE

TL;DR: 本文提出了一种基于大型语言模型（LLM）的缺陷定位方法GenLoc，通过代码探索功能和向量嵌入技术，有效解决了缺陷报告与源代码之间的词汇不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有缺陷定位方法因词汇不匹配问题效果有限，GenLoc旨在通过LLM和代码探索技术提升定位准确率。

Method: GenLoc结合LLM的代码探索功能和向量嵌入技术，迭代分析代码库以定位潜在缺陷文件。

Result: 在6个大型Java项目的9000多个实际缺陷报告中，GenLoc在多个指标上优于五种先进技术，Accuracy@1平均提升超过60%。

Conclusion: GenLoc通过LLM和上下文增强技术显著提升了缺陷定位的准确性。

Abstract: Information Retrieval-based Bug Localization aims to identify buggy source
files for a given bug report. While existing approaches -- ranging from vector
space models to deep learning models -- have shown potential in this domain,
their effectiveness is often limited by the vocabulary mismatch between bug
reports and source code. To address this issue, we propose a novel Large
Language Model (LLM) based bug localization approach, called GenLoc. Given a
bug report, GenLoc leverages an LLM equipped with code-exploration functions to
iteratively analyze the code base and identify potential buggy files. To gather
better context, GenLoc may optionally retrieve semantically relevant files
using vector embeddings. GenLoc has been evaluated on over 9,000 real-world bug
reports from six large-scale Java projects. Experimental results show that
GenLoc outperforms five state-of-the-art bug localization techniques across
multiple metrics, achieving an average improvement of more than 60\% in
Accuracy@1.

</details>


### [9] [Accurate and Consistent Graph Model Generation from Text with Large Language Models](https://arxiv.org/abs/2508.00255)
*Boqi Chen,Ou Wei,Bingzhou Zheng,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 论文提出了一种基于抽象-具体化框架的方法，通过聚合LLM的多个输出来提升图模型生成的准确性和一致性，解决了语法错误和约束不一致等问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM生成的图模型中存在的语法违规、约束不一致和准确性不足三大问题。

Method: 采用抽象-具体化框架，首先生成概率性部分模型聚合候选输出，再精炼为满足所有约束的具体模型。

Result: 实验表明，该方法显著提高了生成图模型的一致性和质量。

Conclusion: 该框架有效解决了LLM生成图模型的关键问题，为这一任务提供了新的解决方案。

Abstract: Graph model generation from natural language description is an important task
with many applications in software engineering. With the rise of large language
models (LLMs), there is a growing interest in using LLMs for graph model
generation. Nevertheless, LLM-based graph model generation typically produces
partially correct models that suffer from three main issues: (1) syntax
violations: the generated model may not adhere to the syntax defined by its
metamodel, (2) constraint inconsistencies: the structure of the model might not
conform to some domain-specific constraints, and (3) inaccuracy: due to the
inherent uncertainty in LLMs, the models can include inaccurate, hallucinated
elements. While the first issue is often addressed through techniques such as
constraint decoding or filtering, the latter two remain largely unaddressed.
Motivated by recent self-consistency approaches in LLMs, we propose a novel
abstraction-concretization framework that enhances the consistency and quality
of generated graph models by considering multiple outputs from an LLM. Our
approach first constructs a probabilistic partial model that aggregates all
candidate outputs and then refines this partial model into the most appropriate
concrete model that satisfies all constraints. We evaluate our framework on
several popular open-source and closed-source LLMs using diverse datasets for
model generation tasks. The results demonstrate that our approach significantly
improves both the consistency and quality of the generated graph models.

</details>


### [10] [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
*Dong Huang,Jie M. Zhang,Mark Harman,Qianru Zhang,Mingzhe Du,See-Kiong Ng*

Main category: cs.SE

TL;DR: 论文介绍了ULT基准测试，用于评估大语言模型在单元测试生成中的能力，解决了现有基准的数据污染和结构简单问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型测试生成基准存在数据污染和结构简单性两大问题，可能导致研究结论的偏差和泛化能力不足。因此，需要一个更真实且具有挑战性的新基准。

Method: ULT通过多阶段筛选构建，包含3,909个高复杂度的Python函数任务，同时设计了PLT基准用于分析记忆与推理的差异。

Result: ULT的测试结果显著低于其他基准（如TestEval和PLT），表明其更具挑战性。例如，LLMs在准确率、语句覆盖率、分支覆盖率和变异评分上表现较差。

Conclusion: ULT为评估LLMs在单元测试生成中的能力提供了更真实的标准，同时PLT辅助分析了模型的记忆效应。

Abstract: Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

</details>


### [11] [Managing Power Gaps as a Topic of Pair Programming Skill: A Grounded Theory](https://arxiv.org/abs/2508.00462)
*Linus Ververs,Lutz Prechelt*

Main category: cs.SE

TL;DR: 论文研究了结对编程中的权力差距现象及其影响，并提出了改善建议。


<details>
  <summary>Details</summary>
Motivation: 探究工业中结对编程的权力相关现象，为从业者提供改进建议。

Method: 分析22个工业结对编程会话，采用扎根理论方法，调查292名参与者验证理论。

Result: 提出权力差距理论，指出其对知识传递、代码质量和效率的负面影响，调查证实理论常见。

Conclusion: 避免权力差距是结对编程的重要技能，需减少层级行为，增加平等行为。

Abstract: Context: Pair Programming as a work mode is used (occasionally or frequently)
throughout professional software development. Objective: Understand what
power-related phenomena occur in pair programming as it is used in industry;
give advice to practitioners on how to do better pair programming. Method:
Analyze 22 industrial pair programming sessions using Grounded Theory
Methodology. Formulate a Grounded Theory on power-related behaviors. Run a
survey with 292 participants about that theory. Use it to demonstrate that the
phenomena are common. Results: Our theory describes the phenomenon of Power
Gap: a perceived difference in participation opportunities. The theory shows
the behaviors that create a Power Gap or result from it. Power Gaps tend to
damage knowledge transfer, code quality, and process effi ciency. The survey
results show that all concepts from our theory are frequent in practice. They
also provide more grounding for concepts that are observable only indirectly.
Conclusions: It is a valuable component of pair programming skill to be able to
avoid Power Gaps. Specifically, pair partners need to avoid Hierarchical
Behavior (which tends to create or increase a Power Gap) and should perform
enough Equalizing Behavior (which prevents or reduces a Power Gap).

</details>


### [12] [Desyan: A Platform for Seamless Value-Flow and Symbolic Analysis](https://arxiv.org/abs/2508.00508)
*Panagiotis Diamantakis,Thanassis Avgerinos,Yannis Smaragdakis*

Main category: cs.SE

TL;DR: Desyan是一个整合值流分析和符号推理的程序分析平台，基于Soufflé Datalog引擎，支持高效SMT求解和本地符号推理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的值流分析和符号分析缺乏统一的平台，导致两种技术难以高效集成，限制了程序分析的潜力和效率。

Method: Desyan扩展了Soufflé Datalog引擎，集成SMT求解器，并提供自动处理典型模式的支持，同时支持Datalog本地符号推理。

Result: Desyan在值流分析中性能领先（提升20倍以上），在需要复杂SMT求解时表现优异，轻量级符号推理速度提升2倍以上。

Conclusion: Desyan成功填补了值流分析和符号分析的鸿沟，提供了一个高效且灵活的集成平台。

Abstract: Over the past two decades, two different types of static analyses have
emerged as dominant paradigms both in academia and industry: value-flow
analysis (e.g., data-flow analysis or points-to analysis) and symbolic analysis
(e.g., symbolic execution). Despite their individual successes in numerous
application fields, the two approaches have remained largely separate; an
artifact of the simple reality that there is no broadly adopted unifying
platform for effortless and efficient integration of symbolic techniques with
high-performance data-flow reasoning.
  To bridge this gap, we introduce Desyan: a platform for writing program
analyses with seamless integration of value-flow and symbolic reasoning. Desyan
expands a production-ready Datalog fixpoint engine (Souffl\'e) with
full-fledged SMT solving invoking industry-leading SMT engines. Desyan provides
constructs for automatically (and efficiently!) handling typical patterns that
come up in program analysis. At the same time, the integration is agnostic with
respect to the solving technology, and supports Datalog-native symbolic
reasoning, via a bottom-up algebraic reasoning module.
  The result is an engine that allows blending different kinds of reasoning, as
needed for the underlying analysis. For value-flow analysis, the engine is the
best-in-class Datalog evaluator (often by a factor of over 20x in execution
time); for applications that require full SMT (e.g., a concolic execution
engine or other symbolic evaluator that needs to solve arbitrarily complex
conditions), the engine is leveraging the leading SMT solvers; for lightweight
symbolic evaluation (e.g., solving simple conditionals in the context of a
path-sensitive analysis), the engine can use Datalog-native symbolic reasoning,
achieving large speedups (often of over 2x) compared to eagerly appealing to an
SMT solver.

</details>


### [13] [SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval](https://arxiv.org/abs/2508.00546)
*Wenchao Gu,Zongyi Lyu,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出了一种名为SPENCER的框架，通过结合双编码器和交叉编码器提升代码检索性能，并采用自适应模型蒸馏技术优化效率。


<details>
  <summary>Details</summary>
Motivation: 传统的双编码器缺乏底层模型训练时代码与描述的交互，限制了性能，因此需要一种既能提高效果又保持效率的方法。

Method: SPENCER结合双编码器缩小搜索空间，再使用交叉编码器提高准确性，并通过自适应模型蒸馏技术减少推理时间。

Result: 实验表明，结合双编码器和交叉编码器提升了性能，模型蒸馏技术减少70%推理时间的同时保持了98%的性能。

Conclusion: SPENCER在性能和效率上均优于传统双编码器方法，为代码检索任务提供了有效的解决方案。

Abstract: Code retrieval aims to provide users with desired code snippets based on
users' natural language queries. With the development of deep learning
technologies, adopting pre-trained models for this task has become mainstream.
Considering the retrieval efficiency, most of the previous approaches adopt a
dual-encoder for this task, which encodes the description and code snippet into
representation vectors, respectively. However, the model structure of the
dual-encoder tends to limit the model's performance, since it lacks the
interaction between the code snippet and description at the bottom layer of the
model during training. To improve the model's effectiveness while preserving
its efficiency, we propose a framework, which adopts Self-AdaPtive Model
Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts
the dual-encoder to narrow the search space and then adopts the cross-encoder
to improve accuracy. To improve the efficiency of SPENCER, we propose a novel
model distillation technique, which can greatly reduce the inference time of
the dual-encoder while maintaining the overall performance. We also propose a
teaching assistant selection strategy for our model distillation, which can
adaptively select the suitable teaching assistant models for different
pre-trained models during the model distillation to ensure the model
performance. Extensive experiments demonstrate that the combination of
dual-encoder and cross-encoder improves overall performance compared to solely
dual-encoder-based models for code retrieval. Besides, our model distillation
technique retains over 98% of the overall performance while reducing the
inference time of the dual-encoder by 70%.

</details>


### [14] [From Code to Career: Assessing Competitive Programmers for Industry Placement](https://arxiv.org/abs/2508.00772)
*Md Imranur Rahman Akib,Fathima Binthe Muhammed,Umit Saha,Md Fazlul Karim Patwary,Mehrin Anannya,Md Alomgeer Hussein,Md Biplob Hosen*

Main category: cs.SE

TL;DR: 研究提出了一种基于Codeforces用户编程表现预测其就业能力的模型，采用随机森林分类器将用户分为四个就业能力等级，并通过Flask部署实现实时预测。


<details>
  <summary>Details</summary>
Motivation: 当前科技行业需要评估程序员就业能力的工具，研究旨在分析竞争编程活动与求职成功之间的相关性。

Method: 使用Codeforces API收集用户数据，处理关键性能指标，构建随机森林分类器模型，并通过Flask部署。

Result: 模型能有效区分不同技能水平，预测用户就业能力，为职业评估提供机器学习基础。

Conclusion: 研究为机器学习在技术职业评估中的应用奠定了基础，并可扩展到更广泛的领域。

Abstract: In today's fast-paced tech industry, there is a growing need for tools that
evaluate a programmer's job readiness based on their coding performance. This
study focuses on predicting the potential of Codeforces users to secure various
levels of software engineering jobs. The primary objective is to analyze how a
user's competitive programming activity correlates with their chances of
obtaining positions, ranging from entry-level roles to jobs at major tech
companies. We collect user data using the Codeforces API, process key
performance metrics, and build a prediction model using a Random Forest
classifier. The model categorizes users into four levels of employability,
ranging from those needing further development to those ready for top-tier tech
jobs. The system is implemented using Flask and deployed on Render for
real-time predictions. Our evaluation demonstrates that the approach
effectively distinguishes between different skill levels based on coding
proficiency and participation. This work lays a foundation for the use of
machine learning in career assessment and could be extended to predict job
readiness in broader technical fields.

</details>


### [15] [Can User Feedback Help Issue Detection? An Empirical Study on a One-billion-user Online Service System](https://arxiv.org/abs/2508.00593)
*Shuyao Jiang,Jiazhen Gu,Wujie Zheng,Yangfan Zhou,Michael R. Lyu*

Main category: cs.SE

TL;DR: 研究用户反馈在大型服务系统中用于问题检测的有效性，发现需过滤无关信息且机器学习是可行方向。


<details>
  <summary>Details</summary>
Motivation: 了解用户反馈特性以改进问题检测方法。

Method: 对超过5000万条用户反馈进行实证研究，分析反馈内容和特征。

Result: 大部分反馈与问题无关，机器学习方法是合理的。

Conclusion: 研究结果为设计和实现实用的反馈问题检测方法提供了基础。

Abstract: Background: It has long been suggested that user feedback, typically written
in natural language by end-users, can help issue detection. However, for
large-scale online service systems that receive a tremendous amount of
feedback, it remains a challenging task to identify severe issues from user
feedback. Aims: To develop a better feedback-based issue detection approach, it
is crucial first to gain a comprehensive understanding of the characteristics
of user feedback in real production systems. Method: In this paper, we conduct
an empirical study on 50,378,766 user feedback items from six real-world
services in a one-billion-user online service system. We first study what users
provide in their feedback. We then examine whether certain features of feedback
items can be good indicators of severe issues. Finally, we investigate whether
adopting machine learning techniques to analyze user feedback is reasonable.
Results: Our results show that a large proportion of user feedback provides
irrelevant information about system issues. As a result, it is crucial to
filter out issue-irrelevant information when processing user feedback.
Moreover, we find severe issues that cannot be easily detected based solely on
user feedback characteristics. Finally, we find that the distributions of the
feedback topics in different time intervals are similar. This confirms that
designing machine learning-based approaches is a viable direction for better
analyzing user feedback. Conclusions: We consider that our findings can serve
as an empirical foundation for feedback-based issue detection in large-scale
service systems, which sheds light on the design and implementation of
practical issue detection approaches.

</details>


### [16] [MCeT: Behavioral Model Correctness Evaluation using Large Language Models](https://arxiv.org/abs/2508.00630)
*Khaled Ahmed,Jialing Song,Boqi Chen,Ou Wei,Bingzhou Zheng*

Main category: cs.SE

TL;DR: 本文提出MCeT工具，通过多视角细粒度方法结合LLM，自动评估行为模型（如顺序图）的正确性，相比直接方法显著提升问题发现率和精度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在模型生成中的广泛应用，需开发自动评估工具以提升模型正确性，并为工程师和AI助手提供反馈。

Method: MCeT将图表分解为原子交互，需求文本分解为原子项，进行细粒度对比，并结合自一致性检查减少LLM幻觉问题。

Result: 方法在真实需求数据集上，将精度从0.58提升至0.81，问题发现率提高90%，每张图表平均多发现6个新问题。

Conclusion: 多视角细粒度方法显著优于直接LLM评估，为行为模型正确性检查提供了高效自动化工具。

Abstract: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of
documentation that are typically designed by system engineers from requirements
documentation, either fully manually or assisted by design tools. With the
growing use of Large Language Models (LLM) as AI modeling assistants, more
automation will be involved in generating diagrams. This necessitates the
advancement of automatic model correctness evaluation tools. Such a tool can be
used to evaluate both manually and AI automatically generated models; to
provide feedback to system engineers, and enable AI assistants to self-evaluate
and self-enhance their generated models.
  In this paper, we propose MCeT, the first fully automated tool to evaluate
the correctness of a behavioral model, sequence diagrams in particular, against
its corresponding requirements text and produce a list of issues that the model
has. We utilize LLMs for the correctness evaluation tasks as they have shown
outstanding natural language understanding ability. However, we show that
directly asking an LLM to compare a diagram to requirements finds less than 35%
of issues that experienced engineers can find. We propose to supplement the
direct check with a fine-grained, multi-perspective approach; we split the
diagram into atomic, non-divisible interactions, and split the requirements
text into atomic, self-contained items. We compare the diagram with atomic
requirements and each diagram-atom with the requirements. We also propose a
self-consistency checking approach that combines perspectives to mitigate LLM
hallucinated issues. Our combined approach improves upon the precision of the
direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover,
the approach finds 90% more issues that the experienced engineers found than
the direct approach, and reports an average of 6 new issues per diagram.

</details>


### [17] [Is LLM-Generated Code More Maintainable \& Reliable than Human-Written Code?](https://arxiv.org/abs/2508.00700)
*Alfred Santa Molison,Marcia Moraes,Glaucia Melo,Fabio Santos,Wesley K. G. Assuncao*

Main category: cs.SE

TL;DR: 比较了LLM生成代码与人类编写代码的质量，发现LLM代码整体缺陷更少，但需注意复杂场景下的严重问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成代码与人类编写代码在软件质量上的差异，以评估LLM的实际应用潜力。

Method: 结合编码任务数据集、三种LLM配置（零样本、少样本和微调）及SonarQube工具分析Python代码质量指标。

Result: LLM代码缺陷较少且修复成本低，但微调后模型可能降低性能，且复杂任务中易出现结构性缺陷。

Conclusion: LLM生成代码有优势但也存在问题，需系统性评估以提高复杂场景下的可靠性。

Abstract: Background: The rise of Large Language Models (LLMs) in software development
has opened new possibilities for code generation. Despite the widespread use of
this technology, it remains unclear how well LLMs generate code solutions in
terms of software quality and how they compare to human-written code. Aims:
This study compares the internal quality attributes of LLM-generated and
human-written code. Method: Our empirical study integrates datasets of coding
tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and
SonarQube to assess software quality. The dataset comprises Python code
solutions across three difficulty levels: introductory, interview, and
competition. We analyzed key code quality metrics, including maintainability
and reliability, and the estimated effort required to resolve code issues.
Results: Our analysis shows that LLM-generated code has fewer bugs and requires
less effort to fix them overall. Interestingly, fine-tuned models reduced the
prevalence of high-severity issues, such as blocker and critical bugs, and
shifted them to lower-severity categories, but decreased the model's
performance. In competition-level problems, the LLM solutions sometimes
introduce structural issues that are not present in human-written code.
Conclusion: Our findings provide valuable insights into the quality of
LLM-generated code; however, the introduction of critical issues in more
complex scenarios highlights the need for a systematic evaluation and
validation of LLM solutions. Our work deepens the understanding of the
strengths and limitations of LLMs for code generation.

</details>


### [18] [Tool-Assisted Conformance Checking to Reference Process Models](https://arxiv.org/abs/2508.00738)
*Bernhard Rumpe,Max Stachon,Sebastian Stüber,Valdes Voufo*

Main category: cs.SE

TL;DR: 本文探讨了通过任务和事件的因果依赖分析，实现自动化一致性检查，以验证具体流程模型是否参考了参考模型。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性检查方法缺乏对语义模型比较的表达能力和自动化支持，导致这一问题未解决。

Method: 提出了一种算法，基于因果依赖分析，并集成到一个更广泛的语义框架中。

Result: 通过案例研究评估了该算法，展示了其在提高准确性和灵活性方面的效果。

Conclusion: 研究提供了一种工具辅助的解决方案，增强了流程模型一致性验证的准确性和灵活性。

Abstract: Reference models convey best practices and standards. The reference
frameworks necessitate conformance checks to ensure adherence to established
guidelines and principles, which is crucial for maintaining quality and
consistency in various processes. This paper explores automated conformance
checks for concrete process models against reference models using causal
dependency analysis of tasks and events. Existing notions of conformance
checking for process models focus on verifying process execution traces and
lack the expressiveness and automation needed for semantic model comparison,
leaving this question unresolved. We integrate our approach into a broader
semantic framework for defining reference model conformance. We outline an
algorithm for reference process model conformance checking, evaluate it through
a case study, and discuss its strengths and limitations. Our research provides
a tool-assisted solution enhancing accuracy and flexibility in process model
conformance verification.

</details>


### [19] [Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures](https://arxiv.org/abs/2508.00749)
*Johanna Grahl,Bernhard Rumpe,Max Stachon,Sebastian Stüber*

Main category: cs.SE

TL;DR: 该论文研究了如何利用动态符号执行（DSE）进行组件-连接器架构的语义差异分析，重点关注MontiArc模型。通过增强MontiArc到Java的生成器收集运行时数据，评估了DSE在语义差异分析中的适用性和局限性。


<details>
  <summary>Details</summary>
Motivation: 在模型驱动开发中，确保演化的模型的正确性和一致性至关重要，因此需要高效的方法来分析语义差异。

Method: 增强MontiArc-to-Java生成器以收集运行时符号和具体执行数据，包括过渡条件、访问状态和自动机内部变量，从而识别关键执行轨迹。

Result: 研究发现DSE在组件-连接器架构分析中具有潜力，但主要局限是可扩展性，需要进一步研究以提升其在大系统中的实用性。

Conclusion: DSE在语义差异分析中有前景，但在处理大规模系统时需要进一步优化以克服可扩展性问题。

Abstract: In the context of model-driven development, ensuring the correctness and
consistency of evolving models is paramount. This paper investigates the
application of Dynamic Symbolic Execution (DSE) for semantic difference
analysis of component-and-connector architectures, specifically utilizing
MontiArc models. We have enhanced the existing MontiArc-to-Java generator to
gather both symbolic and concrete execution data at runtime, encompassing
transition conditions, visited states, and internal variables of automata. This
data facilitates the identification of significant execution traces that
provide critical insights into system behavior. We evaluate various execution
strategies based on the criteria of runtime efficiency, minimality, and
completeness, establishing a framework for assessing the applicability of DSE
in semantic difference analysis. Our findings indicate that while DSE shows
promise for analyzing component and connector architectures, scalability
remains a primary limitation, suggesting further research is needed to enhance
its practical utility in larger systems.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [20] [Modelling Program Spaces in Program Synthesis with Constraints](https://arxiv.org/abs/2508.00005)
*Tilman Hinnerichs,Bart Swinkels,Jaap de Jong,Reuben Gardos Reid,Tudor Magirescu,Neil Yorke-Smith,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: 论文提出了一种利用语法约束优化程序合成空间的方法，通过BART求解器高效消除不必要程序，显著提升枚举效率。


<details>
  <summary>Details</summary>
Motivation: 程序合成的核心挑战在于庞大的程序空间。传统方法依赖组合约束求解器表达程序语义，但未充分利用约束移除无用程序。

Method: 引入语法约束对程序空间建模，开发BART求解器高效传播和解决这些约束。

Result: 约束可消除高达99%的程序空间，显著减少枚举时间。

Conclusion: 语法约束是一种高效工具，能有效缩小程序合成空间并提升枚举效率。

Abstract: A core challenge in program synthesis is taming the large space of possible
programs. Since program synthesis is essentially a combinatorial search, the
community has sought to leverage powerful combinatorial constraint solvers.
Here, constraints are used to express the program semantics, but not as a
potentially potent tool to remove unwanted programs. Recent inductive logic
programming approaches introduce constraints on the program's syntax to be
synthesized. These syntactic constraints allow for checking and propagating a
constraint without executing the program, and thus for arbitrary operators. In
this work, we leverage syntactic constraints to model program spaces, defining
not just solutions that are feasible, but also ones that are likely useful. To
demonstrate this idea, we introduce BART, a solver that efficiently propagates
and solves these constraints. We evaluate BART on program space enumeration
tasks, finding that the constraints eliminate up to 99 percent of the program
space, and that modeling program spaces significantly reduces enumeration time.

</details>


### [21] [From Provable Correctness to Probabilistic Generation: A Comparative Review of Program Synthesis Paradigms](https://arxiv.org/abs/2508.00013)
*Zurabi Kobaladze,Anna Arnania,Tamar Sanikidze*

Main category: cs.PL

TL;DR: 论文综述了程序合成的五种主要方法，并分析了其进展与挑战，强调了从符号方法到神经符号混合方法的转变。


<details>
  <summary>Details</summary>
Motivation: 探究程序合成领域的发展历程和主要方法，比较不同范式的优缺点。

Method: 对五种主要方法（基于逻辑、归纳、草图/模式、大型语言模型和神经符号混合）进行文献综述，分析其原理、系统和应用。

Result: 总结了不同方法的权衡（如正确性保证和搜索复杂性），并展示了从符号到神经符号混合方法的演变。

Conclusion: 未来发展方向是实现可靠且可扩展的程序合成，需要进一步研究神经符号混合方法。

Abstract: Program synthesis--the automated generation of executable code from
high-level specifications--has been a central goal of computer science for over
fifty years. This thesis provides a comparative literature review of the main
paradigms that have shaped the field, tracing its evolution from formal logic
based methods to recent advances using large scale neural models. We examine
five key approaches: logic based (deductive) synthesis, inductive (example
based) synthesis, sketch/schema based synthesis, large language model based
synthesis, and neuro-symbolic hybrids. For each, we analyze foundational
principles, notable systems, and practical applications, highlighting trade
offs between correctness guarantees, specification requirements, search
complexity, and expressive power. By reviewing developments from formally
verified synthesis tools such as KIDS and Coq to data driven models generating
probabilistic code from natural language like Codex, we present a comprehensive
narrative of progress and ongoing challenges. This work emphasizes the
transition from symbolic to hybrid neuro-symbolic methods and outlines future
directions for reliable and scalable program synthesis.

</details>


### [22] [Extended Abstract: Mutable Objects with Several Implementations](https://arxiv.org/abs/2508.00016)
*Matt Kaufmann,Yahya Sohail,Warren A. Hunt Jr*

Main category: cs.PL

TL;DR: 摘要介绍了ACL2的新功能attach-stobj，允许在不重新认证的情况下为抽象stobj提供不同的可执行操作。


<details>
  <summary>Details</summary>
Motivation: 解决抽象stobj的灵活性问题，避免频繁的重新认证。

Method: 通过attach-stobj功能实现不同的可执行操作，并提供背景、用户概述和实现说明。

Result: 增强了ACL2的功能灵活性，减少了认证负担。

Conclusion: attach-stobj为ACL2用户提供了更高效的开发体验。

Abstract: This extended abstract outlines an ACL2 feature, attach-stobj, that first
appeared in ACL2 Version 8.6 (October, 2024). This feature supports different
executable operations for a given abstract stobj, without requiring
recertification of the book that introduces that stobj or theorems about it.
The paper provides background as well as a user-level overview and some
implementation notes.

</details>


### [23] [Automated Type Annotation in Python Using Large Language Models](https://arxiv.org/abs/2508.00422)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.PL

TL;DR: LLMs用于自动生成Python类型注释，无需额外训练即可与传统深度学习竞争。


<details>
  <summary>Details</summary>
Motivation: 手动生成Python类型注释费时且易错，传统自动化方法存在局限。

Method: 使用LLM生成注释，结合语法树和类型检查器进行迭代修复。

Result: GPT4.1mini和O3Mini表现最佳，准确率分别达70.5%和79.1%。

Conclusion: LLMs在生成类型注释中有效且可扩展至其他语言。

Abstract: Type annotations in Python enhance maintainability and error detection.
However, generating these annotations manually is error prone and requires
extra effort. Traditional automation approaches like static analysis, machine
learning, and deep learning struggle with limited type vocabularies, behavioral
over approximation, and reliance on large labeled datasets. In this work, we
explore the use of LLMs for generating type annotations in Python. We develop a
generate check repair pipeline: the LLM proposes annotations guided by a
Concrete Syntax Tree representation, a static type checker (Mypy) verifies
them, and any errors are fed back for iterative refinement. We evaluate four
LLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini
(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.
We first measure the proportion of code snippets annotated by LLMs for which
MyPy reported no errors (i.e., consistent results): GPT 4oMini achieved
consistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,
and O4Mini each reached approximately 88.6% consistency (around 11.4%
failures). To measure annotation quality, we then compute exact-match and
base-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini
perform the best, achieving up to 70.5% exact match and 79.1% base type
accuracy, requiring under one repair iteration on average. Our results
demonstrate that general-purpose and reasoning optimized LLMs, without any task
specific fine tuning or additional training can be effective in generating
consistent type annotations.They perform competitively with traditional deep
learning techniques which require large labeled dataset for training. While our
work focuses on Python, the pipeline can be extended to other optionally typed
imperative languages like Ruby

</details>


### [24] [Semantic Subtyping for Maps in Erlang](https://arxiv.org/abs/2508.00482)
*Erdem Yildirim,Albert Schimpf,Stefan Wehr,Annette Bieniusa*

Main category: cs.PL

TL;DR: 构建了一个包含类型变量、基本类型、集合类型和映射类型的集合论模型，定义了基于集合包含的语义子类型关系，重点是参数化映射类型的子类型定义。


<details>
  <summary>Details</summary>
Motivation: 为Erlang中的映射类型提供一个集合论模型，并定义其子类型关系，特别是参数化映射类型的子类型关系。

Method: 通过构建一个包含多种类型（如类型变量、基本类型等）的集合论模型，定义基于集合包含的语义子类型关系。

Result: 成功定义了一个适用于Erlang映射类型的子类型关系，尤其是解决了参数化映射类型的子类型问题。

Conclusion: 该工作为Erlang的类型系统提供了理论基础，特别是扩展了参数化映射类型的子类型定义能力。

Abstract: In this paper we will construct a set-theoretic model of types featuring type
variables, base types, set-theoretic types and map types. Syntax of map types
spans all the map types available in Erlang. The model of types is used to
define a semantic subtyping relation based on set containment. The novelty of
this work is the definition of subtyping over parameteric map types.

</details>


### [25] [Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations](https://arxiv.org/abs/2508.00534)
*Mikel Vandeloise*

Main category: cs.PL

TL;DR: 本文系统性地综述了多范式编程语言的分类问题，指出现有分类法的不足，并提出基于组合重构的新方法。


<details>
  <summary>Details</summary>
Motivation: 多范式语言的兴起使得传统分类方法面临挑战，导致软件工程中的互操作性缺陷等问题。

Method: 通过对74项主要研究的综合，分析现有分类法的局限性，并提出基于组合重构的原子基元和数学框架的新方法。

Result: 发现现有分类法缺乏概念细粒度和统一的形式基础，而组合重构方法（基于类型论、范畴论和统一编程理论）更具前景。

Conclusion: 研究指出文献中出现了从分类向形式重构框架的显著转变，并提出统一研究的议程。

Abstract: The rise of multi-paradigm languages challenges traditional classification
methods, leading to practical software engineering issues like interoperability
defects. This systematic literature review (SLR) maps the formal foundations of
programming paradigms. Our objective is twofold: (1) to assess the state of the
art of classification formalisms and their limitations, and (2) to identify the
conceptual primitives and mathematical frameworks for a more powerful,
reconstructive approach.
  Based on a synthesis of 74 primary studies, we find that existing taxonomies
lack conceptual granularity, a unified formal basis, and struggle with hybrid
languages. In response, our analysis reveals a strong convergence toward a
compositional reconstruction of paradigms. This approach identifies a minimal
set of orthogonal, atomic primitives and leverages mathematical frameworks,
predominantly Type theory, Category theory and Unifying Theories of Programming
(UTP), to formally guarantee their compositional properties.
  We conclude that the literature reflects a significant intellectual shift
away from classification towards these promising formal, reconstructive
frameworks. This review provides a map of this evolution and proposes a
research agenda for their unification.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [26] [DGEMM without FP64 Arithmetic -- using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme](https://arxiv.org/abs/2508.00441)
*Daichi Mukunoki*

Main category: cs.PF

TL;DR: 本文重新探讨了Ozaki方案在最新AI硬件中使用低精度浮点运算的有效性，特别关注FP6和FP8张量核心，以及基于整数运算的FP64模拟。


<details>
  <summary>Details</summary>
Motivation: 随着AI计算需求增长，低精度矩阵乘法加速硬件普及，但难以直接用于科学计算。Ozaki方案虽能用低精度运算实现高精度矩阵乘法，但最新硬件更偏向低精度浮点而非整数运算。

Method: 研究了FP6和FP8张量核心的应用，以及FP64模拟和新的分块策略。

Result: 在黑莓架构GPU上验证了使用FP8张量核心和FP64模拟进行DGEMM的高效性。

Conclusion: 结合低精度浮点运算和FP64模拟的Ozaki方案在新硬件上仍具潜力。

Abstract: Since AI computations require low-precision matrix multiplications,
processors with enhanced performance for these operations are increasing along
with the growing demand for AI computations. However, it is difficult to use
these operations directly for scientific computations. The Ozaki scheme, an
accurate matrix multiplication method proposed by Ozaki et al. in 2012, enables
FP64 matrix multiplication (DGEMM) using low-precision floating-point
operations such as FP16. The method was subsequently extended to utilize
integer arithmetic. The use of integer operations reduces computational cost
compared to the floating-point based approach. It has also demonstrated higher
performance than hardware FP64 operations on GPUs with fast INT8 Tensor Cores
for AI workloads. However, the latest hardware tends to enhance low-precision
floating-point operation performance such as FP8 instead of INT8. This study
revisits the utilization of low-precision floating-point operations in the
Ozaki scheme, considering the latest AI hardware. Specifically, we consider the
use of FP6 and FP8 Tensor Cores. Moreover, for processors that support very
slow FP64 operations or do not support them at all, we consider the use of the
FP64 emulation based on integer arithmetic. We also examine a new blocking
strategy. We demonstrate the effectiveness of these methods by evaluating the
performance of DGEMM using FP8 Tensor Cores and FP64 emulation on a Blackwell
architecture GPU.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [27] [Composable OS Kernel Architectures for Autonomous Intelligence](https://arxiv.org/abs/2508.00604)
*Rajpreet Singh,Vidhi Kothari*

Main category: cs.OS

TL;DR: 该研究提出了一种新的智能系统操作系统内核架构，将内核从静态资源管理器转变为自适应、集成AI的平台。


<details>
  <summary>Details</summary>
Motivation: 随着智能系统在边缘设备、云基础设施和嵌入式实时环境中的普及，需要一种新的内核架构来支持这些系统的需求。

Method: (1) 将可加载内核模块（LKM）视为面向AI的计算单元；(2) 将Linux内核扩展为支持深度学习推理、浮点加速和实时自适应调度的AI原生环境；(3) 引入基于范畴论和同伦类型理论的神经符号内核设计。

Result: 操作系统能够主动预测并适应自主智能应用的认知需求。

Conclusion: 该研究为智能系统提供了一种创新的内核架构，提升了其适应性和性能。

Abstract: As intelligent systems permeate edge devices, cloud infrastructure, and
embedded real-time environments, this research proposes a new OS kernel
architecture for intelligent systems, transforming kernels from static resource
managers to adaptive, AI-integrated platforms. Key contributions include: (1)
treating Loadable Kernel Modules (LKMs) as AI-oriented computation units for
fast sensory and cognitive processing in kernel space; (2) expanding the Linux
kernel into an AI-native environment with built-in deep learning inference,
floating-point acceleration, and real-time adaptive scheduling for efficient ML
workloads; and (3) introducing a Neurosymbolic kernel design leveraging
Category Theory and Homotopy Type Theory to unify symbolic reasoning and
differentiable logic within OS internals. Together, these approaches enable
operating systems to proactively anticipate and adapt to the cognitive needs of
autonomous intelligent applications.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [28] [Agent Network Protocol Technical White Paper](https://arxiv.org/abs/2508.00007)
*Gaowei Chang,Eidan Lin,Chengxuan Yuan,Rizhao Cai,Binbin Chen,Xuan Xie,Yin Zhang*

Main category: cs.NI

TL;DR: ANP提出了一种新的Agent通信协议，旨在解决大规模Agent互联与协作中的身份验证、动态协商和能力发现等问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的普及，现有互联网基础设施难以满足大规模代理互联与协作的需求，需要设计新的通信协议。

Method: ANP采用AI原生的模块化架构，分为身份与加密通信层、元协议协商层和应用协议层。

Result: ANP系统性地解决了代理身份认证、动态协商和能力发现互操作性等问题。

Conclusion: ANP是适应未来Agent互联网趋势的新一代通信协议。

Abstract: With the development of large models and autonomous decision-making AI,
agents are rapidly becoming the new entities of the internet, following mobile
apps. However, existing internet infrastructure is primarily designed for human
interaction, creating data silos, unfriendly interfaces, and high collaboration
costs among agents, making it difficult to support the needs for large-scale
agent interconnection and collaboration. The internet is undergoing a profound
transformation, showing four core trends: agents replacing traditional
software, universal agent interconnection, native protocol-based connections,
and autonomous agent organization and collaboration. To align with these
trends, Agent Network Protocol (ANP) proposes a new generation of communication
protocols for the Agentic Web. ANP adheres to AI-native design, maintains
compatibility with existing internet protocols, adopts a modular composable
architecture, follows minimalist yet extensible principles, and enables rapid
deployment based on existing infrastructure. Through a three-layer protocol
system--identity and encrypted communication layer, meta-protocol negotiation
layer, and application protocol layer--ANP. systematically solves the problems
of agent identity authentication, dynamic negotiation, and capability discovery
interoperability.

</details>


### [29] [Enabling Immersive XR Collaborations over FTTR Networks (Invited)](https://arxiv.org/abs/2508.00009)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 本文探讨了FTTR（光纤到房间）下的预测带宽分配和无缝切换方案，以实现高质量的沉浸式协作体验。


<details>
  <summary>Details</summary>
Motivation: 研究FTTR作为实现室内扩展现实协作的潜在解决方案，以提升协作体验。

Method: 提出了预测带宽分配和无缝切换方案。

Result: 通过FTTR实现了高质量的沉浸式协作体验。

Conclusion: FTTR结合预测带宽分配和无缝切换方案，是提升室内协作体验的有效方法。

Abstract: Fiber-To-The-Room is a potential solution to achieve in-premise extended
reality collaborations. This paper explores predictive bandwidth allocation and
seamless handover schemes over FTTR, showing high-quality immersive experience
for in-premise collaborations can be achieved. \c{opyright} 2025 The Author(s).

</details>


### [30] [Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight Approach](https://arxiv.org/abs/2508.00629)
*Francisco Crespo,Javier Villegas,Carlos Baena,Eduardo Baena,Sergio Fortes,Raquel Barco*

Main category: cs.NI

TL;DR: 该论文提出了一种轻量级、可编程的分布式应用（dApp），用于在软化的无线电接入网络（RAN）中动态管理CPU资源，提高能效和利用率。


<details>
  <summary>Details</summary>
Motivation: 随着Open RAN（O-RAN）的发展，RAN的软件化带来的灵活性和多供应商部署面临着在严格实时约束下高效管理CPU资源的新挑战。传统操作系统调度器与延迟敏感的RAN工作负载交互时，往往导致性能下降和能耗增加。

Method: 论文提出了一种部署在分布式单元（DU）层级的dApp，通过与操作系统闭环协作，利用线程级遥测数据（如上下文切换、每周期指令数IPC和缓存指标），实时调整CPU线程亲和性、核心隔离和频率缩放。该解决方案无需访问专有RAN软件、硬件特定功能或内核修改。

Result: 在商用级srsRAN部署中的实验结果表明，该解决方案能够在保证实时处理性能的同时显著节省能耗，同时提升CPU利用率，且系统开销极低。

Conclusion: 该研究展示了低延迟dApp在下一代网络中实现细粒度资源控制的潜力，为O-RAN架构中的能效和性能优化提供了一种新途径。

Abstract: The transition toward softwarized Radio Access Networks (RANs), driven by the
Open RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through
disaggregation and virtualization of base station functions. However, this
shift introduces new challenges in managing CPU resources efficiently under
strict real-time constraints. In particular, the interplay between
latency-sensitive RAN workloads and general-purpose Operating System (OS)
schedulers often leads to sub-optimal performance and unnecessary energy
consumption. This work proposes a lightweight, programmable distributed
application (dApp) deployed at the Distributed Unit (DU) level to dynamically
orchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging
thread-level telemetry like context switches, Instructions Per Cycle (IPC), and
cache metrics, to adapt CPU thread affinity, core isolation, and frequency
scaling in real time. Unlike existing solutions, it requires no access to
proprietary RAN software, hardware-specific features, or kernel modifications.
Fully compliant with the O-RAN architecture and agnostic to the underlying RAN
stack, the proposed solution introduces negligible overhead while improving
energy efficiency and CPU utilization. Experimental results using a
commercial-grade srsRAN deployment demonstrate consistent power savings without
compromising real-time processing performance, highlighting the potential of
low-latency dApps for fine-grained resource control in next-generation networks

</details>


### [31] [Non-Terrestrial Network Models Using Stochastic Geometry: Planar or Spherical?](https://arxiv.org/abs/2508.00010)
*Ruibo Wang,Baha Eddine Youcef Belmekki,Howard H. Yang,Mohamed Slim Alouini*

Main category: cs.NI

TL;DR: 该论文分析了在非地面网络（NTN）性能分析中，平面与球形模型的相对误差，并提出了一种生成相似平面和球形点过程的算法以及相对误差估计方法，以确定平面模型的适用性。


<details>
  <summary>Details</summary>
Motivation: 随着非地面网络的快速发展，网络性能分析的计算复杂度急剧增加。平面模型因简化计算而被广泛使用，但其忽略地球曲率会导致高海拔NTN分析偏差，因此需要量化平面与球形模型的差距。

Method: 提出了一种同时生成平面和球形点过程的算法，并基于拓扑和网络级指标定义了相似性度量，进而开发了相对误差估计算法。此外，推导了最优平面高度的解析表达式。

Result: 数值结果表明，部署高度和区域对NTN建模有显著影响，并通过HAP和LEO卫星星座的案例研究验证了方法的有效性。

Conclusion: 论文为平面模型在高海拔NTN分析中的适用性提供了理论支持，并降低了计算复杂度。

Abstract: With the explosive deployment of non-terrestrial networks (NTNs), the
computational complexity of network performance analysis is rapidly escalating.
As one of the most suitable mathematical tools for analyzing large-scale
network topologies, stochastic geometry (SG) enables the representation of
network performance metrics as functions of network parameters, thus offering
low-complexity performance analysis solutions. However, choosing between planar
and spherical models remains challenging. Planar models neglect Earth's
curvature, causing deviations in high-altitude NTN analysis, yet are still
often used for simplicity. This paper introduces relative error to quantify the
gap between planar and spherical models, helping determine when planar modeling
is sufficient. To calculate the relative error, we first propose a point
process (PP) generation algorithm that simultaneously generates a pair of
homogeneous and asymptotically similar planar and spherical PPs. We then
introduce several typical similarity metrics, including topology-related and
network-level metrics, and further develop a relative error estimation
algorithm based on these metrics. In addition, we derive an analytical
expression for the optimal planar altitude, which reduces computational
complexity and provides theoretical support for planar approximation. Finally,
numerical results investigate how deployment altitude and region affect NTN
modeling, with case studies on HAP and LEO satellite constellations.

</details>


### [32] [AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks](https://arxiv.org/abs/2508.00011)
*Ahmet Melih Ince,Ayse Elif Canbilen,Halim Yanikomeroglu*

Main category: cs.NI

TL;DR: 论文摘要概述了6G网络如何结合非地面网络（NTN）和高空平台站（HAPS），通过深度确定性策略梯度（DDPG）强化学习方法优化V2X网络中的信息新鲜度（AoI），提升网络可靠性和信息时效性。


<details>
  <summary>Details</summary>
Motivation: 为了满足6G网络对超高可靠性和低延迟通信（HRLLC）的需求，尤其是在自动驾驶等安全关键应用中，需要探索新技术以增强网络的冗余性和可靠性。

Method: 提出了一种基于深度确定性策略梯度（DDPG）的强化学习方法，用于动态优化HAPS支持的V2X网络中的信息新鲜度（AoI），实现去中心化的资源分配。

Result: 研究发现，结合HAPS和基于DDPG的学习方法，可以有效提升信息新鲜度和网络总体可靠性，尤其是在车队自动驾驶系统中。

Conclusion: HAPS支持的解决方案与DDPG强化学习相结合，为6G网络中的资源分配提供了一种高效且去中心化的方法，为自动驾驶系统的通信需求提供了潜在解决方案。

Abstract: Sixth-generation (6G) networks are designed to meet the hyper-reliable and
low-latency communication (HRLLC) requirements of safety-critical applications
such as autonomous driving. Integrating non-terrestrial networks (NTN) into the
6G infrastructure brings redundancy to the network, ensuring continuity of
communications even under extreme conditions. In particular, high-altitude
platform stations (HAPS) stand out for their wide coverage and low latency
advantages, supporting communication reliability and enhancing information
freshness, especially in rural areas and regions with infrastructure
constraints. In this paper, we present reinforcement learning-based approaches
using deep deterministic policy gradient (DDPG) to dynamically optimize the
age-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.
The proposed method improves information freshness and overall network
reliability by enabling independent learning without centralized coordination.
The findings reveal the potential of HAPS-supported solutions, combined with
DDPG-based learning, for efficient AoI-aware resource allocation in
platoon-based autonomous vehicle systems.

</details>


### [33] [Performance Analysis of SAGIN from the Relay Perspective: A Spherical Stochastic Geometry Approach](https://arxiv.org/abs/2508.00020)
*Ferdaous Tarhouni,Ruibo Wang,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 该论文研究了卫星-空中-地面一体化网络（SAGIN）中高空平台（HAPs）作为中继的性能评估，提出了三种性能指标，并使用球形随机几何（SSG）工具进行分析。同时还研究了卫星网络拓扑对性能的影响以及HAPs的最小传输功率需求。


<details>
  <summary>Details</summary>
Motivation: 随着全球无线通信需求的增长，SAGIN的重要性日益凸显。HAPs作为中继节点可以提升通信性能，因此需要从这一独特视角评估其性能。

Method: 研究采用了球形随机几何（SSG）工具，提出了三种性能指标：平均接入数据速率、平均回程数据速率和回程速率超出概率（BREP），并推导了其解析表达式。

Result: 论文提供了BREP的闭式表达式，并通过数值结果展示了卫星网络拓扑对性能的影响以及HAPs的最小传输功率需求。

Conclusion: 通过SSG框架，论文为SAGIN中HAPs的中继性能提供了低复杂度的评估方法，并验证了SSG在此类研究中的优势。

Abstract: In recent years, the satellite-aerial-ground integrated network (SAGIN) has
become essential in meeting the increasing demands for global wireless
communications. In SAGIN, high-altitude platforms (HAPs) can serve as
communication hubs and act as relays to enhance communication performance. In
this paper, we evaluate network performance and analyze the role of HAPs in
SAGIN from the relay perspective. Based on this unique perspective, we
introduce three metrics to evaluate the performance, named the average access
data rate, the average backhaul data rate, and the backhaul rate exceedance
probability (BREP). Considering the need for dynamic topology and interference
analysis, we choose spherical stochastic geometry (SSG) as a tool and derive
analytical expressions for the above metrics to achieve low-complexity
performance evaluation. Specifically, we provide a closed-form expression for
the end-to-end performance metric BREP. Given that there is no existing
literature in the SSG field studying networks from a relay perspective, we
specifically investigate the impact of satellite network topology on
performance in our numerical results to further highlight the advantages of the
SSG framework. Additionally, we analyze the minimum HAP transmission power
required to maintain both short-term and long-term data rate demands.

</details>


### [34] [Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts](https://arxiv.org/abs/2508.00234)
*Jin Yang,Qiong Wu,Zhiying Feng,Zhi Zhou,Deke Guo,Xu Chen*

Main category: cs.NI

TL;DR: 论文提出了一种基于深度强化学习的QoS感知LLM路由框架，以提高边缘LLM服务的实时响应和数据隐私保护。


<details>
  <summary>Details</summary>
Motivation: 云LLM服务存在高延迟、不稳定响应和隐私问题，部署边缘LLM是解决方案，但需优化请求路由以保障QoS。

Method: 采用深度强化学习（DRL），结合动态状态抽象技术和异构图注意力网络（HAN），以及动作影响估计器和定制奖励函数。

Result: 实验显示，相比现有基线，所提算法显著提升平均QoS和计算资源效率。

Conclusion: 该框架能有效解决LLM服务异构性、请求干扰和动态负载问题，确保长期稳定QoS。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
leading to a significant increase in user demand for LLM services. However,
cloud-based LLM services often suffer from high latency, unstable
responsiveness, and privacy concerns. Therefore, multiple LLMs are usually
deployed at the network edge to boost real-time responsiveness and protect data
privacy, particularly for many emerging smart mobile and IoT applications.
Given the varying response quality and latency of LLM services, a critical
issue is how to route user requests from mobile and IoT devices to an
appropriate LLM service (i.e., edge LLM expert) to ensure acceptable
quality-of-service (QoS). Existing routing algorithms fail to simultaneously
address the heterogeneity of LLM services, the interference among requests, and
the dynamic workloads necessary for maintaining long-term stable QoS. To meet
these challenges, in this paper we propose a novel deep reinforcement learning
(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM
services. Due to the dynamic nature of the global state, we propose a dynamic
state abstraction technique to compactly represent global state features with a
heterogeneous graph attention network (HAN). Additionally, we introduce an
action impact estimator and a tailored reward function to guide the DRL agent
in maximizing QoS and preventing latency violations. Extensive experiments on
both Poisson and real-world workloads demonstrate that our proposed algorithm
significantly improves average QoS and computing resource efficiency compared
to existing baselines.

</details>


### [35] [Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models](https://arxiv.org/abs/2508.00028)
*Abir Ray*

Main category: cs.NI

TL;DR: 提出了一种结合马尔可夫链模型和ITU-R传播模型的可扩展频谱可用性预测框架，以动态识别频谱机会。


<details>
  <summary>Details</summary>
Motivation: 频谱资源在时间和空间上常被低效利用，需动态频谱接入策略以允许次级用户使用空闲频率。

Method: 结合两状态马尔可夫链模型（捕捉时间占用模式）和ITU-R传播模型（路径损耗和杂波效应），预测频谱可用性。

Result: 该方法能高效预测时空频谱机会，计算成本低，适用于实时频谱管理。

Conclusion: 框架灵活，可适配不同频段和场景，为认知无线电网络提供实用解决方案。

Abstract: Spectrum resources are often underutilized across time and space, motivating
dynamic spectrum access strategies that allow secondary users to exploit unused
frequencies. A key challenge is predicting when and where spectrum will be
available (i.e., unused by primary licensed users) in order to enable proactive
and interference-free access. This paper proposes a scalable framework for
spectrum availability prediction that combines a two-state Markov chain model
of primary user activity with high-fidelity propagation models from the ITU-R
(specifically Recommendations P.528 and P.2108). The Markov chain captures
temporal occupancy patterns, while the propagation models incorporate path loss
and clutter effects to determine if primary signals exceed interference
thresholds at secondary user locations. By integrating these components, the
proposed method can predict spectrum opportunities both in time and space with
improved accuracy. We develop the system model and algorithm for the approach,
analyze its scalability and computational efficiency, and discuss assumptions,
limitations, and potential applications. The framework is flexible and can be
adapted to various frequency bands and scenarios. The results and analysis show
that the proposed approach can effectively identify available spectrum with low
computational cost, making it suitable for real-time spectrum management in
cognitive radio networks and other dynamic spectrum sharing systems.

</details>


### [36] [Towards Reliable AI in 6G: Detecting Concept Drift in Wireless Network](https://arxiv.org/abs/2508.00042)
*Athanasios Tziouvaras,Carolina Fortuna,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Marko Grobelnik,Blaž Bertalanič*

Main category: cs.NI

TL;DR: 该论文提出两种无监督、模型无关的批量概念漂移检测方法，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 针对6G网络中AI模型因无线环境动态变化导致的性能退化问题，研究通用且高效的概念漂移检测方法。

Method: 设计两种基于期望效用评分的无监督概念漂移检测器，无需部署后真实标签即可触发模型再训练。

Result: 方法在真实无线场景中表现优异，F1分数达0.94-1.00，假警报率降低20%。

Conclusion: 提出的方法有效解决了概念漂移检测的挑战，显著提升了6G网络中AI模型的适应性。

Abstract: AI-native 6G networks promise unprecedented automation and performance by
embedding machine-learning models throughout the radio access and core segments
of the network. However, the non-stationary nature of wireless environments due
to infrastructure changes, user mobility, and emerging traffic patterns,
induces concept drifts that can quickly degrade these model accuracies.
Existing methods in general are very domain specific, or struggle with certain
type of concept drift. In this paper, we introduce two unsupervised,
model-agnostic, batch concept drift detectors. Both methods compute an
expected-utility score to decide when concept drift occurred and if model
retraining is warranted, without requiring ground-truth labels after
deployment. We validate our framework on two real-world wireless use cases in
outdoor fingerprinting for localization and for link-anomaly detection, and
demonstrate that both methods are outperforming classical detectors such as
ADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an
F1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus
reducing the false alarm rate by up to 20 percentage points compared to the
best classical detectors.

</details>


### [37] [Benchmarking XRootD-HTTPS on 400Gbps Links with Variable Latencies](https://arxiv.org/abs/2508.00228)
*Aashay Arora,Diego Davila,Frank Würthwein,John Graham,Dima Mishin,Justas Balcas,Tom Lehman,Xi Yang,Chin Guok,Harvey Newman*

Main category: cs.NI

TL;DR: 论文研究了在高亮度LHC时代下，US-CMS Tier-2站点如何提升软硬件以应对400 Gbps带宽需求，重点测试了XRootD HTTP第三方拷贝的性能。


<details>
  <summary>Details</summary>
Motivation: 为应对未来网络流量的增长，确保生产与用户数据分析访问的软件准备就绪。

Method: 通过系统测试，模拟真实网络条件，探索不同主机和传输配置对XRootD HTTP第三方拷贝性能的影响。

Result: 研究成功复现了真实网络条件，并测试了多台来源主机和CPU分配的效果。

Conclusion: 研究成果为未来网络升级提供了重要参考，支持高带宽需求的实现。

Abstract: In anticipation of the High Luminosity-LHC era, there is a critical need to
oversee software readiness for upcoming growth in network traffic for
production and user data analysis access. This paper looks into software and
hardware required improvements in US-CMS Tier-2 sites to be able to sustain and
meet the projected 400 Gbps bandwidth demands while tackling the challenge
posed by varying latencies between sites. Specifically, our study focuses on
identifying the performance of XRootD HTTP third-party copies across multiple
400 Gbps links and exploring different host and transfer configurations. Our
approach involves systematic testing with variations in the number of origins
per cluster and CPU allocations for each origin. By replicating real network
conditions and creating network "loops" that traverse multiple switches across
the wide area network, we are able to replicate authentic network conditions

</details>


### [38] [Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study](https://arxiv.org/abs/2508.00256)
*Chuang Zhang,Geng Sun,Jiacheng Wang,Yijing Lin,Weijie Yuan,Sinem Coleri,Dusit Niyato,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 本文探讨了低空无线网络（LAWNs）的安全挑战，提出了基于大型人工智能模型（LAM）的优化框架，通过强化学习提升安全通信性能，并通过仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于低空操作、频繁移动和依赖未授权频段的特性，LAWNs面临独特的安全挑战，传统AI方法存在局限性，因此研究LAMs的解决方案具有重要意义。

Method: 提出了一种基于LAM的优化框架，利用大语言模型（LLMs）生成增强状态特征并设计内在奖励，以改进强化学习在安全通信任务中的表现。

Result: 通过案例研究仿真，验证了所提出框架在LAWNs安全通信中的有效性。

Conclusion: LAMs在LAWNs安全应用中展现出潜力，未来需进一步探索其集成方向。

Abstract: Low-altitude wireless networks (LAWNs) have the potential to revolutionize
communications by supporting a range of applications, including urban parcel
delivery, aerial inspections and air taxis. However, compared with traditional
wireless networks, LAWNs face unique security challenges due to low-altitude
operations, frequent mobility and reliance on unlicensed spectrum, making it
more vulnerable to some malicious attacks. In this paper, we investigate some
large artificial intelligence model (LAM)-enabled solutions for secure
communications in LAWNs. Specifically, we first explore the amplified security
risks and important limitations of traditional AI methods in LAWNs. Then, we
introduce the basic concepts of LAMs and delve into the role of LAMs in
addressing these challenges. To demonstrate the practical benefits of LAMs for
secure communications in LAWNs, we propose a novel LAM-based optimization
framework that leverages large language models (LLMs) to generate enhanced
state features on top of handcrafted representations, and to design intrinsic
rewards accordingly, thereby improving reinforcement learning performance for
secure communication tasks. Through a typical case study, simulation results
validate the effectiveness of the proposed framework. Finally, we outline
future directions for integrating LAMs into secure LAWN applications.

</details>


### [39] [Energy Efficient Trajectory Control and Resource Allocation in Multi-UAV-assisted MEC via Deep Reinforcement Learning](https://arxiv.org/abs/2508.00261)
*Saichao Liu,Geng Sun,Chuang Zhang,Xuejie Liu,Jiacheng Wang,Changyuan Zhao,Dusit Niyato*

Main category: cs.NI

TL;DR: 研究了无人机辅助的移动边缘计算系统，通过优化无人机飞行路径和资源分配，提升系统性能。提出了结合模仿学习的增强深度强化学习算法DPPOIL，效果优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 移动边缘计算由于位置固定和服务范围有限，性能受限。无人机辅助的MEC系统有望提升智能设备的计算能力。

Method: 提出多目标优化问题TCRAMOP，优化无人机飞行路径和资源分配；设计DPPOIL算法，结合模仿学习提升策略性能。

Result: 仿真显示DPPOIL算法优于其他基线方法。

Conclusion: 无人机辅助MEC系统及DPPOIL算法能显著提升计算服务性能。

Abstract: Mobile edge computing (MEC) is a promising technique to improve the
computational capacity of smart devices (SDs) in Internet of Things (IoT).
However, the performance of MEC is restricted due to its fixed location and
limited service scope. Hence, we investigate an unmanned aerial vehicle
(UAV)-assisted MEC system, where multiple UAVs are dispatched and each UAV can
simultaneously provide computing service for multiple SDs. To improve the
performance of system, we formulated a UAV-based trajectory control and
resource allocation multi-objective optimization problem (TCRAMOP) to
simultaneously maximize the offloading number of UAVs and minimize total
offloading delay and total energy consumption of UAVs by optimizing the flight
paths of UAVs as well as the computing resource allocated to served SDs. Then,
consider that the solution of TCRAMOP requires continuous decision-making and
the system is dynamic, we propose an enhanced deep reinforcement learning (DRL)
algorithm, namely, distributed proximal policy optimization with imitation
learning (DPPOIL). This algorithm incorporates the generative adversarial
imitation learning technique to improve the policy performance. Simulation
results demonstrate the effectiveness of our proposed DPPOIL and prove that the
learned strategy of DPPOIL is better compared with other baseline methods.

</details>


### [40] [Mamba for Wireless Communications and Networking: Principles and Opportunities](https://arxiv.org/abs/2508.00403)
*Rongsheng Zhang,Ruichen Zhang,Yang Lu,Wei Chen,Bo Ai,Dusit Niyato*

Main category: cs.NI

TL;DR: Mamba模型在时空数据处理任务中展现出高效性，有望通过平衡计算效率与效果，革新无线通信和网络设计。文章综述了Mamba在无线系统中的潜在应用，并提出两种应用框架，通过案例研究展示了其在特征增强和计算效率方面的改进，同时指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 无线网络的异构性和动态性日益增加，亟需高效的模型来优化通信和网络设计。Mamba模型因其在时空数据任务中的高效性，被选为潜在的解决方案。

Method: 文章首先从长程依赖建模和空间特征提取的角度分析Mamba在无线信号处理任务中的潜力，随后提出两种应用框架（替代传统算法和实现新范式），并通过案例研究验证其效果。

Result: 案例研究表明，Mamba在智能资源分配和联合源与信道解码等任务中，显著提升了特征增强和计算效率。

Conclusion: Mamba在无线通信和网络设计中具有广阔的应用前景，但仍需解决一些关键挑战，未来研究可围绕这些挑战展开。

Abstract: Mamba has emerged as a powerful model for efficiently addressing tasks
involving temporal and spatial data. Regarding the escalating heterogeneity and
dynamics in wireless networks, Mamba holds the potential to revolutionize
wireless communication and networking designs by balancing the trade-off
between computational efficiency and effectiveness. This article presents a
comprehensive overview of Mamba' applications in wireless systems.
Specifically, we first analyze the potentials of Mamba for wireless signal
processing tasks from the perspectives of long-range dependency modeling and
spatial feature extraction. Then we propose two application frameworks for
Mamba in wireless communications, i.e., replacement of traditional algorithms,
and enabler of novel paradigms. Guided by the two frameworks, we conduct case
studies on intelligent resource allocation and joint source and channel
decoding to demonstrate Mamba's improvements in both feature enhancement and
computational efficiency. Finally, we highlight critical challenges and outline
potential research directions for Mamba in wireless communications and
networking.

</details>


### [41] [Enhancing Wireless Networks for IoT with Large Vision Models: Foundations and Applications](https://arxiv.org/abs/2508.00583)
*Yunting Xu,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Deepu Rajan,Liang Yu,Haibo Zhou,Abbas Jamalipour,Xianbin Wang*

Main category: cs.NI

TL;DR: 论文探讨了大型视觉模型（LVMs）在视觉智能和物联网（IoT）中的应用，并提出了一种渐进式微调框架以优化多任务性能。


<details>
  <summary>Details</summary>
Motivation: 研究LVMs在视觉任务中的卓越表现及其在无线通信领域的潜在应用，解决LVMs模型大和重训练难的问题。

Method: 提出渐进式微调框架，逐步调整预训练的LVMs以适应无线领域的多任务联合优化。

Result: 在低空经济网络（LAENets）中的案例研究表明，该框架在联合波束成形和定位任务中优于传统CNN。

Conclusion: LVMs为智能无线系统的集成提供了有前景的方向，尤其是在多任务优化的IoT场景中。

Abstract: Large vision models (LVMs) have emerged as a foundational paradigm in visual
intelligence, achieving state-of-the-art performance across diverse visual
tasks. Recent advances in LVMs have facilitated their integration into Internet
of Things (IoT) scenarios, offering superior generalization and adaptability
for vision-assisted network optimization. In this paper, we first investigate
the functionalities and core architectures of LVMs, highlighting their
capabilities across classification, segmentation, generation, and multimodal
visual processing. We then explore a variety of LVM applications in wireless
communications, covering representative tasks across the physical layer,
network layer, and application layer. Furthermore, given the substantial model
size of LVMs and the challenges of model retraining in wireless domains, we
propose a progressive fine-tuning framework that incrementally adapts
pretrained LVMs for joint optimization of multiple IoT tasks. A case study in
low-altitude economy networks (LAENets) demonstrates the effectiveness of the
proposed framework over conventional CNNs in joint beamforming and positioning
tasks for Internet of drones, underscoring a promising direction for
integrating LVMs into intelligent wireless systems.

</details>


### [42] [Joint Association and Phase Shifts Design for UAV-mounted Stacked Intelligent Metasurfaces-assisted Communications](https://arxiv.org/abs/2508.00616)
*Mingzhe Fan,Geng Sun,Hongyang Pan,Jiacheng Wang,Jiancheng An,Hongyang Du,Chau Yuen*

Main category: cs.NI

TL;DR: 论文提出了一种基于无人机搭载智能超表面（UAV-SIMs）的通信系统，通过联合优化无人机与用户关联、无人机位置和超表面相位偏移，以最大化网络容量。采用了交替优化策略解决非凸和NP难问题，并通过仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 固定智能超表面（SIMs）会限制通信性能，而移动SIMs可以灵活部署以提升系统性能。因此，研究无人机搭载SIMs（UAV-SIMs）的通信系统成为一种有前景的解决方案。

Method: 将联合优化问题（USBJOP）分解为三个子问题：无人机与用户关联优化（AUUOP）、无人机位置优化（ULOP）和超表面相位偏移优化（USPSOP）。采用交替优化（AO）策略，前两者通过CVX工具箱求解，后者通过逐层迭代优化方法解决。

Result: 仿真结果表明，所提出的策略在不同设置下均能有效提升网络容量。

Conclusion: 无人机搭载智能超表面的通信系统通过联合优化能够显著提升通信性能，验证了该方法的可行性和有效性。

Abstract: Stacked intelligent metasurfaces (SIMs) have emerged as a promising
technology for realizing wave-domain signal processing, while the fixed SIMs
will limit the communication performance of the system compared to the mobile
SIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted
communication system, where UAVs as base stations (BSs) can cache the data
processed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance
the communication performance. To this end, we formulate a UAV-SIM-based joint
optimization problem (USBJOP) to comprehensively consider the association
between UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of
UAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and
NP-hardness of USBJOP, we decompose it into three sub-optimization problems,
which are the association between UAV-SIMs and users optimization problem
(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase
shifts optimization problem (USPSOP). Then, these three sub-optimization
problems are solved by an alternating optimization (AO) strategy. Specifically,
AUUOP and ULOP are transformed to a convex form and then solved by the CVX
tool, while we employ a layer-by-layer iterative optimization method for
USPSOP. Simulation results verify the effectiveness of the proposed strategy
under different simulation setups.

</details>


### [43] [Criticality-Based Dynamic Topology Optimization for Enhancing Aerial-Marine Swarm Resilience](https://arxiv.org/abs/2508.00688)
*Ruiyang Huang,Haocheng Wang,Yixuan Shen,Ning Gao,Qiang Ni,Shi Jin,Yifan Wu*

Main category: cs.NI

TL;DR: 论文提出了一种两步框架，结合节点关键性排序和多目标拓扑优化，以增强异构海洋-空中群网络在对抗环境中的韧性。


<details>
  <summary>Details</summary>
Motivation: 异构海洋-空中群网络在对抗环境中面临通信中断和结构脆弱的挑战，需要提升网络韧性。

Method: 提出三层架构表示群网络结构、通信和任务依赖关系，设计SurBi-Ranking方法实时评估节点和边的重要性，并使用NSGA-III算法优化拓扑。

Result: 实验显示SurBi-Ranking比传统方法更准确定位关键节点和边，优化方法显著减少自然连接退化30%，提高任务成功率。

Conclusion: 框架能有效维持多阶段操作的连接性和任务效果，具备实际应用潜力。

Abstract: Heterogeneous marine-aerial swarm networks encounter substantial difficulties
due to targeted communication disruptions and structural weaknesses in
adversarial environments. This paper proposes a two-step framework to
strengthen the network's resilience. Specifically, our framework combines the
node prioritization based on criticality with multi-objective topology
optimization. First, we design a three-layer architecture to represent
structural, communication, and task dependencies of the swarm networks. Then,
we introduce the SurBi-Ranking method, which utilizes graph convolutional
networks, to dynamically evaluate and rank the criticality of nodes and edges
in real time. Next, we apply the NSGA-III algorithm to optimize the network
topology, aiming to balance communication efficiency, global connectivity, and
mission success rate. Experiments demonstrate that compared to traditional
methods like K-Shell, our SurBi-Ranking method identifies critical nodes and
edges with greater accuracy, as deliberate attacks on these components cause
more significant connectivity degradation. Furthermore, our optimization
approach, when prioritizing SurBi-Ranked critical components under attack,
reduces the natural connectivity degradation by around 30%, achieves higher
mission success rates, and incurs lower communication reconfiguration costs,
ensuring sustained connectivity and mission effectiveness across multi-phase
operations.

</details>


### [44] [Deep Joint Source-Channel Coding for Small Satellite Applications](https://arxiv.org/abs/2508.00715)
*Olga Kondrateva,Grace Li Zhang,Julian Zobel,Björn Scheuermann,Stefan Dietzel*

Main category: cs.NI

TL;DR: 本文提出了一种适用于卫星通信的自适应深度联合源信道编码（DJSCC）框架，通过注意力模块实现单一网络适应多种信道状态，显著减少模型存储需求并提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决小卫星在地球观测中产生的高维数据传输瓶颈问题，适应复杂且多变的卫星通信环境。

Method: 提出DJSCC-SAT基本系统和ADJSCC-SAT自适应架构，利用多状态统计信道模型和注意力模块实现网络泛化能力。

Result: 在Sentinel-2多光谱数据上验证，自适应方法性能接近多专用网络，同时减少存储需求并增强对信道误差的鲁棒性。

Conclusion: 该框架为实际卫星任务中部署鲁棒性自适应DJSCC系统提供了实用高效的解决方案。

Abstract: Small satellites used for Earth observation generate vast amounts of
high-dimensional data, but their operation in low Earth orbit creates a
significant communication bottleneck due to limited contact times and harsh,
varying channel conditions. While deep joint source-channel coding (DJSCC) has
emerged as a promising technique, its practical application to the complex
satellite environment remains an open question. This paper presents a
comprehensive DJSCC framework tailored for satellite communications. We first
establish a basic system, DJSCC-SAT, and integrate a realistic, multi-state
statistical channel model to guide its training and evaluation. To overcome the
impracticality of using separate models for every channel condition, we then
introduce an adaptable architecture, ADJSCC-SAT, which leverages attention
modules to allow a single neural network to adjust to a wide range of channel
states with minimal overhead. Through extensive evaluation on Sentinel-2
multi-spectral data, we demonstrate that our adaptable approach achieves
performance comparable to using multiple specialized networks while
significantly reducing model storage requirements. Furthermore, the adaptable
model shows enhanced robustness to channel estimation errors, outperforming the
non-adaptable baseline. The proposed framework is a practical and efficient
step toward deploying robust, adaptive DJSCC systems for real-world satellite
missions.

</details>


### [45] [Overlapping IPv4, IPv6, and TCP data: exploring errors, test case context and multiple overlaps inside network stacks and NIDSes with PYROLYSE](https://arxiv.org/abs/2508.00735)
*Lucas Aubard,Johan Mazel,Gilles Guette,Pierre Chifflier*

Main category: cs.NI

TL;DR: 论文提出PYROLYSE工具，用于测试IP和TCP重组策略的多样性及潜在漏洞，发现实现间策略差异大且现有NIDS方法存在问题。


<details>
  <summary>Details</summary>
Motivation: 研究IP和TCP重组策略的多样性及其对网络安全的影响，特别是NIDS与主机OS解释不一致导致的漏洞。

Method: 开发PYROLYSE工具，全面测试不同协议实现的重组策略，并通过实验分析多样性和错误。

Result: 发现14至20种不同重组行为，报告了8个安全相关错误，并指出NIDS在重叠块超过两个时不适用现有策略。

Conclusion: NIDS需改进重组策略以避免安全风险，PYROLYSE为相关测试提供了有效工具。

Abstract: IP fragmentation and TCP segmentation allow for splitting large data packets
into smaller ones, e.g., for transmission across network links of limited
capacity. These mechanisms permit complete or partial overlaps with different
data on the overlapping portions. IPv4, IPv6, and TCP reassembly policies,
i.e., the data chunk preferences that depend on the overlap types, differ
across protocol implementations. This leads to vulnerabilities, as NIDSes may
interpret the packet differently from the monitored host OSes. Some NIDSes,
such as Suricata or Snort, can be configured so that their policies are
consistent with the monitored OSes. The first contribution of the paper is
PYROLYSE, an audit tool that exhaustively tests and describes the reassembly
policies of various IP and TCP implementation types. This tool ensures that
implementations reassemble overlapping chunk sequences without errors. The
second contribution is the analysis of PYROLYSE artifacts. We first show that
the reassembly policies are much more diverse than previously thought. Indeed,
by testing all the overlap possibilities for n <= 3 test case chunks and
different testing scenarios, we observe from 14 to 20 different behaviors out
of 23 tested implementations depending on the protocol. Second, we report eight
errors impacting one OS, two NIDSes, and two embedded stacks, which can lead to
security issues such as NIDS pattern-matching bypass or DoS attacks. A CVE was
assigned to a NIDS error. Finally, we show that implemented IP and TCP policies
obtained through chunk pair testing are usually inconsistent with the observed
triplet reassemblies. Therefore, contrarily to what they currently do, NIDSes
or other network traffic analysis tools should not apply n = 2 pair policies
when the number of overlapping chunks exceeds two.

</details>


### [46] [Data Movement Manager (DMM) for the SENSE-Rucio Interoperation Prototype](https://arxiv.org/abs/2508.00792)
*Aashay Arora,Diego Davila,Jonathan Guiang,Frank Würthwein,Harvey Newman,Justas Balcas,Tom Lehman,Xi Yang*

Main category: cs.NI

TL;DR: DMM是一个连接Rucio和SENSE的接口原型，支持基于SDN的高能物理数据流，并通过优先级带宽分配优化网络使用。


<details>
  <summary>Details</summary>
Motivation: 为了解决高能物理数据流在网络中的高效传输问题，DMM连接了CERN的数据管理软件Rucio和SDN服务SENSE。

Method: DMM利用主机级吞吐量指标和FTS数据传输任务级指标，实现基于优先级的带宽分配和细粒度监控。

Result: DMM成功优化了网络使用，并提供了对低效数据流的细粒度监控能力。

Conclusion: DMM的设计和实现为高能物理数据流的高效传输和监控提供了可行的解决方案。

Abstract: The Data Movement Manager (DMM) is a prototype interface that connects CERN's
data management software, Rucio, with the Sofware-Defined Networking (SDN)
service SENSE by ESNet. It enables SDN-enabled high-energy physics data flows
using the existing worldwide LHC computing grid infrastructure. A key feature
of DMM is transfer priority-based bandwidth allocation, optimizing network
usage. Additionally, it provides fine-grained monitoring of underperforming
flows by leveraging end-to-end data flow monitoring. This is achieved through
access to host-level (network interface) throughput metrics and transfer-tool
(FTS) data transfer job-level metrics. This paper details the design and
implementation of DMM.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [47] [MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval](https://arxiv.org/abs/2508.00579)
*Ziyu Gong,Yihua Huang,Chengcheng Mai*

Main category: cs.MM

TL;DR: 介绍了一种新型多模态RAG模型MMRAG-DocQA，用于解决多模态长上下文文档问答任务中的幻觉问题和多模态间断性，通过分层索引和多粒度语义检索提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LVLM和RAG的方法存在幻觉问题及多模态间不连贯，需要新方法整合跨页多模态信息以实现准确问答。

Method: 提出MMRAG-DocQA模型，采用分层索引结合页面内和跨页拓扑块，多粒度语义检索及LLM重排序以优化证据连接和推理。

Result: 在MMLongBench-Doc和LongDocURL数据集上，MMRAG-DocQA表现优于现有方法，有效处理多页多模态文档问答。

Conclusion: MMRAG-DocQA通过分层索引和多粒度检索解决了多模态长文档问答中的关键挑战，展示了显著性能提升。

Abstract: The multi-modal long-context document question-answering task aims to locate
and integrate multi-modal evidences (such as texts, tables, charts, images, and
layouts) distributed across multiple pages, for question understanding and
answer generation. The existing methods can be categorized into Large
Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation
(RAG)-based methods. However, the former were susceptible to hallucinations,
while the latter struggled for inter-modal disconnection and cross-page
fragmentation. To address these challenges, a novel multi-modal RAG model,
named MMRAG-DocQA, was proposed, leveraging both textual and visual information
across long-range pages to facilitate accurate question answering. A
hierarchical indexing method with the integration of flattened in-page chunks
and topological cross-page chunks was designed to jointly establish in-page
multi-modal associations and long-distance cross-page dependencies. By means of
joint similarity evaluation and large language model (LLM)-based re-ranking, a
multi-granularity semantic retrieval method, including the page-level parent
page retrieval and document-level summary retrieval, was proposed to foster
multi-modal evidence connection and long-distance evidence integration and
reasoning. Experimental results performed on public datasets, MMLongBench-Doc
and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in
understanding and answering modality-rich and multi-page documents.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [48] [Building Bigraphs of the real world](https://arxiv.org/abs/2508.00003)
*Kang Rong Roy Ang*

Main category: cs.LO

TL;DR: 提出了一种使用OpenStreetMap数据将全球建筑、街道和行政区组织成层次空间分区树的规范，并将其编码为反映街道连接性的数字孪生大图，并开发了一个高效工具。


<details>
  <summary>Details</summary>
Motivation: 通过层次化结构和大图技术，构建一个能够完整表示全球街道连接性的数字孪生模型。

Method: 利用OpenStreetMap数据，设计并实现了一个基于OCaml的工具，构建全球区域的大图，并对开源工具进行算法改进。

Result: 实现了对超大图的快速构建和转换，性能提升高达97倍。

Conclusion: 该研究为全球空间数据的层次化建模提供了高效的技术方案。

Abstract: This report proposes a formal specification for organising all buildings,
streets and administrative areas in the world into a hierarchical
space-partitioning tree using data from OpenStreetMap. This hierarchical
structure is encoded into a bigraph, serving as a digital twin of the world and
capturing complete street connectivity. It presents a tool implemented in OCaml
(source code at https://github.com/royangkr/bigraph-of-the-world ) that
constructs bigraphs for regions from any part of the world. In addition, it
contributes algorithmic improvements to open-source bigraph-building tools that
enable them to efficiently construct and transform extremely large bigraphs,
achieving up to a 97x speedup among other gains.

</details>


### [49] [Reasoning under uncertainty in the game of Cops and Robbers](https://arxiv.org/abs/2508.00004)
*Dazhu Li,Sujata Ghosh,Fenrong Liu*

Main category: cs.LO

TL;DR: 该论文提出了一种新的形式框架ELCR，用于分析Cops and Robbers游戏中玩家的不完全信息，并研究其逻辑性质和更新机制。


<details>
  <summary>Details</summary>
Motivation: 研究Cops and Robbers游戏中玩家不完全信息的影响，填补形式化分析的空白。

Method: 提出Epistemic Logic of Cops and Robbers (ELCR)框架，纳入玩家位置、观察能力和推理等核心概念，并通过动态操作符定义信息更新机制。

Result: 实现了自动化追踪玩家互动和信息更新，研究了ELCR的多种性质（如公理化和可判定性），并与相关逻辑和游戏范式进行了比较。

Conclusion: ELCR框架首次从形式化角度探讨了Cops and Robbers游戏中的不完全信息问题，为相关研究提供了新的理论基础。

Abstract: The game of Cops and Robbers is an important model for studying computational
queries in pursuit-evasion environments, among others. As recent logical
explorations have shown, its structure exhibits appealing analogies with modal
logic. In this paper, we enrich the game with a setting in which players may
have imperfect information. We propose a new formal framework, Epistemic Logic
of Cops and Robbers (ELCR), to make the core notions of the game precise, for
instance, players' positions, observational power and inference. Applying ELCR
to analyze the game, we obtain an automated way to track interactions between
players and characterize their information updates during the game. The update
mechanism is defined by a novel dynamic operator, and we compare it with some
relevant paradigms from the game and logic perspectives. We study various
properties of ELCR including axiomatization and decidability. To our knowledge,
this is the first attempt to explore these games from a formal point of view
where (partial) information available to players is taken into account.

</details>


### [50] [Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation](https://arxiv.org/abs/2508.00017)
*Nikolai Sergeev*

Main category: cs.LO

TL;DR: 生成逻辑（GL）是一种基于用户提供的公理化定义的确定性架构，通过逻辑块（LBs）系统探索其演绎邻域，自动生成可验证的数学定理证明。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个可审计、可重现的证明系统，自动生成和验证数学定理，支持硬件-软件协同设计。

Method: 将公理化定义编译为逻辑块网格，通过消息交换和统一推理规则生成新事实，实现机器可检查的证明。

Result: 成功从皮亚诺公理自动生成了加法、乘法等基础算术定律的可验证证明，并导出为可交互的HTML格式。

Conclusion: GL展示了自动定理证明的潜力，为与概率模型（如LLMs）的集成和硬件加速提供了方向。

Abstract: We present Generative Logic (GL), a deterministic architecture that begins
from user-supplied axiomatic definitions -- written in a minimalist
Mathematical Programming Language (MPL) -- and systematically explores their
deductive neighborhood. Definitions are compiled into a distributed grid of
simple Logic Blocks (LBs) that exchange messages; any time several expressions
unify under an inference rule, a new fact is emitted with full provenance to
its sources, yielding replayable, auditable proof graphs.
  A prototype software implementation instantiates the workflow on first-order
Peano arithmetic. Starting only from the Peano axioms, GL enumerates candidate
implications, applies normalization and type filters, and automatically
reconstructs machine-checkable proofs of foundational arithmetic laws including
associativity and commutativity of addition, associativity and commutativity of
multiplication, and distributivity. Generated proofs export to navigable HTML
so that every inference step can be inspected independently.
  We outline a hardware-software co-design path toward massively parallel
realizations and describe prospective integration with probabilistic models
(e.g., Large Language Models (LLMs)) for autoformalization and conjecture
seeding. The Python and MPL code to reproduce the Peano experiments, along with
the full HTML proof graphs, are available in the project's GitHub repository at
https://github.com/Generative-Logic/GL/tree/35a111ea9ba53afe051703d6050be0c3923e9724
and are permanently archived at https://doi.org/10.5281/zenodo.16408441. We
invite community feedback and collaboration.

</details>


### [51] [Deciding the Value of Two-Clock Almost Non-Zeno Weighted Timed Games](https://arxiv.org/abs/2508.00014)
*Isa Vialard*

Main category: cs.LO

TL;DR: 本文解决了带有两时钟且几乎非Zeno的加权定时游戏的价值问题，表明其具有可判定性。


<details>
  <summary>Details</summary>
Motivation: 研究加权定时游戏（wtgs）的价值问题，特别是针对两时钟且几乎非Zeno的游戏，填补了这一领域长期存在的空白。

Method: 通过数学和算法分析，研究两时钟且几乎非Zeno的wtgs的性质，并证明其价值问题的可判定性。

Result: 证明了在两时钟且几乎非Zeno的wtgs中，价值问题是可判定的。

Conclusion: 本文解决了加权定时游戏领域的开放性问题，扩展了对这类游戏可判定性的理解。

Abstract: The Value Problem for weighted timed games (wtgs) consists in determining,
given a two-player weighted timed game with a reachability objective and a
rational threshold, whether or not the value of the game exceeds the threshold.
When restrained to wtgs with non-negative weight, this problem is known to be
undecidable for weighted timed games with three or more clocks, and decidable
for one-clock wtgs. The Value Problem for two-clock non-negative wtgs, which
remained stubbornly open for a decade, was recently shown to be undecidable. In
this article, we show that the Value Problem is decidable when considering
two-clock almost non-Zeno wtgs.

</details>


### [52] [Extended Abstract: Partial-encapsulate and Its Support for Floating-point Operations in ACL2](https://arxiv.org/abs/2508.00015)
*Matt Kaufmann,J Strother Moore*

Main category: cs.LO

TL;DR: 论文展示了 partial-encapsulate 的强大功能，并通过 ACL2 中浮点运算的实现来说明其应用。


<details>
  <summary>Details</summary>
Motivation: 通过 partial-encapsulate 来展示其在实际应用中的潜力，尤其是在 ACL2 中实现浮点运算时的高效性。

Method: 使用 partial-encapsulate 功能，结合 ACL2 实现浮点运算的具体操作。

Result: 成功展示了 partial-encapsulate 在 ACL2 中实现浮点运算的有效性和实用性。

Conclusion: partial-encapsulate 是一种强大的工具，能够在 ACL2 等系统中高效实现复杂的浮点运算。

Abstract: We illustrate the power of partial-encapsulate, showing how it is used in the
implementation of floating-point operations in ACL2.

</details>


### [53] [Alignment Monitoring](https://arxiv.org/abs/2508.00021)
*Thomas A. Henzinger,Konstantin Kueffner,Vasu Singh,I Sun*

Main category: cs.LO

TL;DR: 提出一种对齐监控方法，用于验证概率模型与实际系统的预测对齐情况，并提供了多类监控器以评估模型对齐性。


<details>
  <summary>Details</summary>
Motivation: 确保概率模型与现实的系统行为对齐是形式化验证的前提，但传统方法缺乏动态监测机制。

Method: 利用顺序预测工具构建对齐监控器，包括基础对齐监控器、差分对齐监控器和加权对齐监控器。

Result: 实验在PRISM基准套件上验证，监控器快速、内存高效且能早期检测不对齐。

Conclusion: 对齐监控器为模型与现实的动态对齐提供了有效工具，适用于实际应用场景。

Abstract: Formal verification provides assurances that a probabilistic system satisfies
its specification--conditioned on the system model being aligned with reality.
We propose alignment monitoring to watch that this assumption is justified. We
consider a probabilistic model well aligned if it accurately predicts the
behaviour of an uncertain system in advance. An alignment score measures this
by quantifying the similarity between the model's predicted and the system's
(unknown) actual distributions. An alignment monitor observes the system at
runtime; at each point in time it uses the current state and the model to
predict the next state. After the next state is observed, the monitor updates
the verdict, which is a high-probability interval estimate for the true
alignment score. We utilize tools from sequential forecasting to construct our
alignment monitors. Besides a monitor for measuring the expected alignment
score, we introduce a differential alignment monitor, designed for comparing
two models, and a weighted alignment monitor, which permits task-specific
alignment monitoring. We evaluate our monitors experimentally on the PRISM
benchmark suite. They are fast, memory-efficient, and detect misalignment
early.

</details>


### [54] [Ordinal Folding Index: A Computable Metric for Self-Referential Semantics](https://arxiv.org/abs/2508.00151)
*Faruk Alpay,Hamdi Al Alakkad*

Main category: cs.LO

TL;DR: OFI是一种新的、完全可计算的度量标准，用于衡量语句、协议或观点需要经过多少轮自引用才能稳定其真实性或结果。


<details>
  <summary>Details</summary>
Motivation: OFI旨在建立固定点逻辑、无限奇偶游戏和形式理论强度之间的直接联系，提供统一的视角。

Method: 通过将自引用深度转化为单一序数，OFI结合了游戏理论和逻辑度量，并证明其可算法枚举。

Result: OFI改进了所有经典游戏理论和逻辑度量，同时在有限竞技场上提供了多项式时间近似方案。

Conclusion: OFI为游戏理论家和逻辑学家提供了统一的工具，为跨领域技术开辟了共同研究基础。

Abstract: The Ordinal Folding Index (OFI) is a new, fully computable yard-stick that
measures how many rounds of self-reference a statement, protocol or position
must unfold before its truth or outcome stabilises. By turning this abstract
'fold-back' depth into a single ordinal number, OFI forges a direct link
between areas that are usually studied in isolation: the closure stages of
fixed-point logics, the time-to-win values of infinite parity games, and the
ordinal progressions that calibrate the strength of formal theories. We prove
that OFI refines all classical game-theoretic and logical metrics while
remaining algorithmically enumerable, supply a polynomial-time approximation
scheme on finite arenas, and show how the index coincides exactly with the
length of the shortest winning strategy in the associated evaluation game.
Alongside the theory we outline five open problems from the completeness of the
computable-ordinal spectrum to the possibility of 'compressing' deep
self-reference that chart a research programme at the intersection of
computer-aided logic, algorithmic game theory and ordinal analysis. OFI thus
invites game theorists and logicians alike to view infinite play, transfinite
induction and reflective reasoning through a single, intuitive lens, opening
common ground for techniques.

</details>


### [55] [Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers](https://arxiv.org/abs/2508.00419)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.LO

TL;DR: 论文研究了利用大型语言模型（如OpenAI的O1系列）与Z3求解器结合，自动化生成循环不变式的方法，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 循环不变式的自动生成具有挑战性，现有方法无法覆盖所有程序。本文探索大型语言模型是否能在这一任务中表现更好。

Method: 将OpenAI的O1系列模型与Z3求解器紧密耦合，通过生成-检查流程迭代优化循环不变式。使用Code2Inv基准测试133个任务。

Result: 框架覆盖所有133个任务（100%），优于之前的最佳107个任务。每次实例仅需1-2次模型提议，耗时14-55秒。

Conclusion: 大型语言模型具备潜在的逻辑推理能力，可用于自动化循环不变式生成，且方法可推广到其他命令式语言。

Abstract: Loop invariants are essential for proving the correctness of programs with
loops. Developing loop invariants is challenging, and fully automatic synthesis
cannot be guaranteed for arbitrary programs. Some approaches have been proposed
to synthesize loop invariants using symbolic techniques and more recently using
neural approaches. These approaches are able to correctly synthesize loop
invariants only for subsets of standard benchmarks. In this work, we
investigate whether modern, reasoning-optimized large language models can do
better. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled
generate-and-check pipeline with the Z3 SMT solver, using solver
counterexamples to iteratively guide invariant refinement. We use Code2Inv
benchmark, which provides C programs along with their formal preconditions and
postconditions. On this benchmark of 133 tasks, our framework achieves 100%
coverage (133 out of 133), outperforming the previous best of 107 out of 133,
while requiring only 1-2 model proposals per instance and 14-55 seconds of
wall-clock time. These results demonstrate that LLMs possess latent logical
reasoning capabilities which can help automate loop invariant synthesis. While
our experiments target C-specific programs, this approach should be
generalizable to other imperative languages.

</details>


### [56] [Analysing Temporal Reasoning in Description Logics Using Formal Grammars](https://arxiv.org/abs/2508.00575)
*Camille Bourgaux,Anton Gnatenko,Michaël Thomazo*

Main category: cs.LO

TL;DR: 该论文建立了特定形式文法与时态描述逻辑 $\mathcal{TEL}^\bigcirc$ 片段之间的对应关系，揭示了其模型非周期性及查询回答不可判定性，同时发现了某些片段的可判定性。


<details>
  <summary>Details</summary>
Motivation: 探索 $\mathcal{TEL}^\bigcirc$ 与形式文法（如连接文法）的联系，解决其模型周期性和查询回答可判定性问题。

Method: 通过将 $\mathcal{TEL}^\bigcirc$ 片段与连接文法建立对应关系，分析其性质。

Result: 证明 $\mathcal{TEL}^\bigcirc$ 不具有模型周期性，查询回答不可判定，但发现了某些片段的可判定性。

Conclusion: 该研究解决了 $\mathcal{TEL}^\bigcirc$ 的开放性问题，并为其他片段的可判定性提供了工具和方法。

Abstract: We establish a correspondence between (fragments of)
$\mathcal{TEL}^\bigcirc$, a temporal extension of the $\mathcal{EL}$
description logic with the LTL operator $\bigcirc^k$, and some specific kinds
of formal grammars, in particular, conjunctive grammars (context-free grammars
equipped with the operation of intersection). This connection implies that
$\mathcal{TEL}^\bigcirc$ does not possess the property of ultimate periodicity
of models, and further leads to undecidability of query answering in
$\mathcal{TEL}^\bigcirc$, closing a question left open since the introduction
of $\mathcal{TEL}^\bigcirc$. Moreover, it also allows to establish decidability
of query answering for some new interesting fragments of
$\mathcal{TEL}^\bigcirc$, and to reuse for this purpose existing tools and
algorithms for conjunctive grammars.

</details>


### [57] [Parameterized Infinite-State Reactive Synthesis](https://arxiv.org/abs/2508.00613)
*Benedikt Maderbacher,Roderick Bloem*

Main category: cs.LO

TL;DR: 提出了一种合成参数化无限状态系统的方法，可针对不同参数值实例化。方法通过反例引导循环，包括合成具体系统、通用化程序、生成证明候选和验证证明四个步骤。


<details>
  <summary>Details</summary>
Motivation: 为了支持参数化系统的合成，特别是能够适应不同环境参数的需求。

Method: 采用反例引导的循环方法，结合反通用化和语法引导的合成技术，将程序中的语法差异表达为参数的函数。

Result: 在文献中的示例和新问题上进行了评估，验证了方法的有效性。

Conclusion: 该方法能够有效合成参数化系统，并通过结合多种技术确保其正确性。

Abstract: We propose a method to synthesize a parameterized infinite-state systems that
can be instantiated for different parameter values. The specification is given
in a parameterized temporal logic that allows for data variables as well as
parameter variables that encode properties of the environment. Our synthesis
method runs in a counterexample-guided loop consisting of four main steps:
First, we use existing techniques to synthesize concrete systems for some small
parameter instantiations. Second, we generalize the concrete systems into a
parameterized program. Third, we create a proof candidate consisting of an
invariant and a ranking function. Fourth, we check the proof candidate for
consistency with the program. If the proof succeeds, the parameterized program
is valid. Otherwise, we identify a parameter value for which the proof fails
and add a new concrete instance to step one. To generalize programs and create
proof candidates, we use a combination of anti-unification and syntax-guided
synthesis to express syntactic differences between programs as functions of the
parameters. We evaluate our approach on examples from the literature that have
been extended with parameters as well as new problems.

</details>


### [58] [Putting Perspective into OWL [sic]: Complexity-Neutral Standpoint Reasoning for Ontology Languages via Monodic S5 over Counting Two-Variable First-Order Logic (Extended Version with Appendix)](https://arxiv.org/abs/2508.00653)
*Lucía Gómez Álvarez,Sebastian Rudolph*

Main category: cs.LO

TL;DR: 本文研究了C2逻辑的单调立场扩展，通过多项式时间翻译将扩展后的形式转化为标准C2逻辑，保持了NExpTime完全性，并探讨了描述逻辑的扩展限制。


<details>
  <summary>Details</summary>
Motivation: 多视角建模和推理在知识表示中具有重要意义，但如何在保持高效推理的同时实现这一目标是一个挑战。

Method: 通过多项式时间翻译将单调立场扩展的C2逻辑公式转化为标准C2逻辑，并利用模型论论证展示其可行性。

Result: 扩展后的逻辑保持了NExpTime完全性，且在描述逻辑中扩展单调立场不会增加推理复杂度。

Conclusion: 单调立场在保持高效推理的同时为知识表示提供了强大工具，但限制其扩展条件以避免不可判定性至关重要。

Abstract: Standpoint extensions of knowledge representation formalisms have been
recently introduced as a means to incorporate multi-perspective modelling and
reasoning through modal operators that attribute pieces of knowledge to
specific entities or agents. In these extensions, the integration between
conceptual modelling and perspective annotations can vary in strength, with
monodic standpoint extensions offering a well-balanced approach. They allow for
advanced modelling features, such as the expression of rigid concepts, while
maintaining desirable reasoning complexity.
  We consider the extension of C2--the counting two-variable fragment of
first-order logic--by monodic standpoints. At the heart of our work is a
polynomial-time translation of formulas in this extended formalism into
standard, standpoint-free C2, a result that relies on intricate model-theoretic
arguments. Thanks to this translation, the satisfiability problem remains at
the same complexity level: NExpTime-complete, as in plain C2. Since our
formalism subsumes monodic S5 over C2, this result also marks a substantial
advancement in the study of first-order modal logics.
  From a practical standpoint, this means that highly expressive description
logics such as SHOIQBs and SROIQBs--which underpin the widely adopted OWL 1 and
OWL 2 ontology languages standardised by the W3C--can be extended with monodic
standpoints without increasing the standard reasoning complexity.
  We further prove that NExpTime-hardness arises even in significantly less
expressive description logics, as long as they include both nominals and
monodic standpoints. Moreover, we show that if the monodicity restriction is
relaxed even slightly in the presence of inverse roles, functionality, and
nominals, the satisfiability problem becomes undecidable.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [59] [ReVise: A Human-AI Interface for Incremental Algorithmic Recourse](https://arxiv.org/abs/2508.00002)
*Kaustav Bhattacharjee,Jun Yuan,Aritra Dasgupta*

Main category: cs.HC

TL;DR: 论文提出了一种可视化的分析方法，用于帮助数据主体在算法决策中规划渐进的补救路径，以避免误导性行动。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在社会技术系统中的广泛应用，其黑箱特性引发了担忧。数据主体（如求职者、贷款申请者）在不完全理解如何改变自身状况时，可能采取错误行动，导致长期不良后果。现有方法仅关注最终补救目标，忽视了逐步实现的可行性和用户偏好。

Method: 作者与AI/ML专家合作，设计了一种可视化分析工作流，并开发了一个交互式可视化界面，帮助数据主体高效探索补救方案。

Result: 通过真实数据集的使用场景和12名研究生的主观反馈，验证了该方法的有效性。

Conclusion: 该交互式工具能够有效帮助数据主体选择合适的渐进补救路径，弥补了现有方法的不足。

Abstract: The recent adoption of artificial intelligence in socio-technical systems
raises concerns about the black-box nature of the resulting decisions in fields
such as hiring, finance, admissions, etc. If data subjects -- such as job
applicants, loan applicants, and students -- receive an unfavorable outcome,
they may be interested in algorithmic recourse, which involves updating certain
features to yield a more favorable result when re-evaluated by algorithmic
decision-making. Unfortunately, when individuals do not fully understand the
incremental steps needed to change their circumstances, they risk following
misguided paths that can lead to significant, long-term adverse consequences.
Existing recourse approaches focus exclusively on the final recourse goal but
neglect the possible incremental steps to reach the goal with real-life
constraints, user preferences, and model artifacts. To address this gap, we
formulate a visual analytic workflow for incremental recourse planning in
collaboration with AI/ML experts and contribute an interactive visualization
interface that helps data subjects efficiently navigate the recourse
alternatives and make an informed decision. We present a usage scenario and
subjective feedback from observational studies with twelve graduate students
using a real-world dataset, which demonstrates that our approach can be
instrumental for data subjects in choosing a suitable recourse path.

</details>


### [60] [Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web](https://arxiv.org/abs/2507.23585)
*Sophia Liu,Shm Garanganao Almeda*

Main category: cs.HC

TL;DR: 论文提出‘超文本摩擦’概念，旨在通过重新应用超文本原则（摩擦、可追溯性和结构）来恢复算法驱动界面中被削弱的用户控制权。


<details>
  <summary>Details</summary>
Motivation: 当前算法驱动的界面（如推荐系统和生成式AI工具）以牺牲用户控制权为代价优先考虑参与度和效率，导致用户对内容的可见性和意义构建失去控制。

Method: 通过对Wikipedia与Instagram Explore以及Are.na与生成式AI图像工具的比较分析，研究不同系统如何构建用户体验、导航和创作。

Result: 超文本系统强调来源、关联思维和用户驱动的意义构建，而算法系统则模糊过程并削弱参与感。

Conclusion: 论文提出‘超文本摩擦’作为设计立场，为在算法主导的网络环境中恢复用户控制权提供了潜在解决方案。

Abstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI
tools, often prioritize engagement and efficiency at the expense of user
agency. As systems take on more decision-making, users have less control over
what they see and how meaning or relationships between content are constructed.
This paper introduces "Hypertextual Friction," a conceptual design stance that
repositions classical hypertext principles--friction, traceability, and
structure--as actionable values for reclaiming agency in algorithmically
mediated environments. Through a comparative analysis of real-world
interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image
tools--we examine how different systems structure user experience, navigation,
and authorship. We show that hypertext systems emphasize provenance,
associative thinking, and user-driven meaning-making, while algorithmic systems
tend to obscure process and flatten participation. We contribute: (1) a
comparative analysis of how interface structures shape agency in user-driven
versus agent-driven systems, and (2) a conceptual stance that offers
hypertextual values as design commitments for reclaiming agency in an
increasingly algorithmic web.

</details>


### [61] [A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app](https://arxiv.org/abs/2508.00103)
*Guilherme Guerino,Luiz Rodrigues,Luana Bianchiniand Mariana Alves,Marcelo Marinho,Thomaz Veloso,Valmir Macario,Diego Dermeval,Thales Vieira,Ig Bittencourt,Seiji Isotani*

Main category: cs.HC

TL;DR: 研究提出MathAIde系统，结合计算机视觉和AI纠正数学练习，并通过反馈提升学习效果，强调教师在设计中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 解决AI教育中教师参与不足、AI工具限制及资源可及性问题，通过增强智能（AuI）提升系统实用性。

Method: 采用头脑风暴、高保真原型、A/B测试及真实课堂案例研究，设计并评估MathAIde系统。

Result: 研究提出了教师为中心的AuI实现方案，实际部署验证了系统的实用性和高采纳潜力。

Conclusion: 用户中心设计提升AIED系统的实用性和采纳率，尤其在资源有限环境中。

Abstract: Integrating Artificial Intelligence in Education (AIED) aims to enhance
learning experiences through technologies like Intelligent Tutoring Systems
(ITS), offering personalized learning, increased engagement, and improved
retention rates. However, AIED faces three main challenges: the critical role
of teachers in the design process, the limitations and reliability of AI tools,
and the accessibility of technological resources. Augmented Intelligence (AuI)
addresses these challenges by enhancing human capabilities rather than
replacing them, allowing systems to suggest solutions. In contrast, humans
provide final assessments, thus improving AI over time. In this sense, this
study focuses on designing, developing, and evaluating MathAIde, an ITS that
corrects mathematics exercises using computer vision and AI and provides
feedback based on photos of student work. The methodology included
brainstorming sessions with potential users, high-fidelity prototyping, A/B
testing, and a case study involving real-world classroom environments for
teachers and students. Our research identified several design possibilities for
implementing AuI in ITSs, emphasizing a balance between user needs and
technological feasibility. Prioritization and validation through prototyping
and testing highlighted the importance of efficiency metrics, ultimately
leading to a solution that offers pre-defined remediation alternatives for
teachers. Real-world deployment demonstrated the usefulness of the proposed
solution. Our research contributes to the literature by providing a usable,
teacher-centered design approach that involves teachers in all design phases.
As a practical implication, we highlight that the user-centered design approach
increases the usefulness and adoption potential of AIED systems, especially in
resource-limited environments.

</details>


### [62] [Decoupling Data and Tooling in Interactive Visualization](https://arxiv.org/abs/2508.00107)
*Jan Simson*

Main category: cs.HC

TL;DR: 论文提出了一种模块化方法，将数据整理与加载功能与可视化组件分离，以减少开发冗余并提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前可视化工具缺乏对数据整理的支持，导致开发成本高、用户学习曲线陡峭，用户体验较差。

Method: 通过模块化架构分离数据处理与可视化功能，开发了一个基于网络技术的原型。

Result: 展示了模块化方法的可行性，并允许多种工具并行使用。

Conclusion: 未来研究方向包括与其他工具集成和采用新技术优化数据转换，希望获得社区反馈。

Abstract: Interactive data visualization is a major part of modern exploratory data
analysis, with web-based technologies enabling a rich ecosystem of both
specialized and general tools. However, current visualization tools often lack
support for transformation or wrangling of data and are forced to re-implement
their own solutions to load and ingest data. This redundancy creates
substantial development overhead for tool creators, steeper learning curves for
users who must master different data handling interfaces across tools and a
degraded user experience as data handling is usually seen as an after-thought.
  We propose a modular approach that separates data wrangling and loading
capabilities from visualization components. This architecture allows
visualization tools to concentrate on their core strengths while providing the
opportunity to develop a unified, powerful interface for data handling. An
additional benefit of this approach is that it allows for multiple tools to
exist and be used side by side. We demonstrate the feasibility of this approach
by building an early prototype using web technologies to encapsulate
visualization tools and manage data flow between them.
  We discuss future research directions, including downstream integrations with
other tooling, such as IDEs, literate programming notebooks and applications,
as well as incorporation of new technologies for efficient data
transformations. We seek input from the community to better understand the
requirements towards this approach.

</details>


### [63] [Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models](https://arxiv.org/abs/2508.00140)
*Zhanna Kaufman,Madeline Endres,Cindy Xiong Bearfield,Yuriy Brun*

Main category: cs.HC

TL;DR: 论文研究了ML模型的可解释性可视化对用户理解和信任的影响，发现理解与信任呈负相关，因为更易理解的视觉化会增加对偏见的感知，从而降低信任。


<details>
  <summary>Details</summary>
Motivation: ML系统中的偏见行为普遍存在，影响利益相关者的信任和使用方式，不同背景的用户对同一系统的看法和信任度不同。研究旨在探讨可解释性可视化如何影响理解和信任。

Method: 通过用户研究评估五种最先进的可视化工具（LIME、SHAP、CP、Anchors和ELI5），测量设计特征对非专家用户的理解、偏见感知和信任的影响。

Result: 研究发现理解与信任呈负相关，更易理解的视觉化会增加偏见感知，从而降低信任。通过设计干预，可以显著提高理解、增加偏见感知并降低信任。

Conclusion: 研究揭示了理解与信任的关系，强调可视化设计在促进负责任ML应用中的作用。

Abstract: Systems relying on ML have become ubiquitous, but so has biased behavior
within them. Research shows that bias significantly affects stakeholders' trust
in systems and how they use them. Further, stakeholders of different
backgrounds view and trust the same systems differently. Thus, how ML models'
behavior is explained plays a key role in comprehension and trust. We survey
explainability visualizations, creating a taxonomy of design characteristics.
We conduct user studies to evaluate five state-of-the-art visualization tools
(LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how
taxonomy characteristics affect comprehension, bias perception, and trust for
non-expert ML users. Surprisingly, we find an inverse relationship between
comprehension and trust: the better users understand the models, the less they
trust them. We investigate the cause and find that this relationship is
strongly mediated by bias perception: more comprehensible visualizations
increase people's perception of bias, and increased bias perception reduces
trust. We confirm this relationship is causal: Manipulating explainability
visualizations to control comprehension, bias perception, and trust, we show
that visualization design can significantly (p < 0.001) increase comprehension,
increase perceived bias, and reduce trust. Conversely, reducing perceived model
bias, either by improving model fairness or by adjusting visualization design,
significantly increases trust even when comprehension remains high. Our work
advances understanding of how comprehension affects trust and systematically
investigates visualization's role in facilitating responsible ML applications.

</details>


### [64] [DeformTune: A Deformable XAI Music Prototype for Non-Musicians](https://arxiv.org/abs/2508.00160)
*Ziqing Xu,Nick Bryan-Kinns*

Main category: cs.HC

TL;DR: DeformTune 是一个结合触觉变形界面与 MeasureVAE 模型的系统，旨在为无音乐背景的用户提供更直观的 AI 音乐生成体验。初步研究揭示了用户在使用过程中遇到的挑战，并提出了提升 AI 可解释性的设计机会。


<details>
  <summary>Details</summary>
Motivation: 现有 AI 音乐生成工具依赖文本提示或复杂界面，限制了非音乐背景用户的使用。论文旨在探索更直观、易用的交互方式。

Method: 开发 DeformTune 原型系统，结合触觉变形界面与 MeasureVAE 模型，并通过 11 名非专业音乐用户的初步研究收集反馈。

Result: 研究发现用户面临控制映射不明确、表达范围有限等问题，但提出了通过多模态反馈和渐进式交互支持提升 AI 可解释性的机会。

Conclusion: DeformTune 为 AI 音乐系统的可解释性和用户友好性提供了早期研究基础，有助于未来设计更易用的工具。

Abstract: Many existing AI music generation tools rely on text prompts, complex
interfaces, or instrument-like controls, which may require musical or technical
knowledge that non-musicians do not possess. This paper introduces DeformTune,
a prototype system that combines a tactile deformable interface with the
MeasureVAE model to explore more intuitive, embodied, and explainable AI
interaction. We conducted a preliminary study with 11 adult participants
without formal musical training to investigate their experience with
AI-assisted music creation. Thematic analysis of their feedback revealed
recurring challenge--including unclear control mappings, limited expressive
range, and the need for guidance throughout use. We discuss several design
opportunities for enhancing explainability of AI, including multimodal feedback
and progressive interaction support. These findings contribute early insights
toward making AI music systems more explainable and empowering for novice
users.

</details>


### [65] [The SPACE of AI: Real-World Lessons on AI's Impact on Developers](https://arxiv.org/abs/2508.00178)
*Brian Houck,Travis Lowdermilk,Cody Beyer,Steven Clarke,Ben Hanrahan*

Main category: cs.HC

TL;DR: AI工具在软件开发中被广泛采纳，主要提升例行任务的效率，但对协作影响有限。团队文化和支持结构是关键。


<details>
  <summary>Details</summary>
Motivation: 探讨AI对开发者生产力和体验的实际影响。

Method: 采用混合研究方法，包括500多份开发者调查和定性访谈与观察。

Result: AI显著提升效率和满意度，但对协作影响较小；组织支持对最大化AI价值至关重要。

Conclusion: AI是开发者的补充，有效集成需团队文化和结构支持。

Abstract: As artificial intelligence (AI) tools become increasingly embedded in
software development workflows, questions persist about their true impact on
developer productivity and experience. This paper presents findings from a
mixed-methods study examining how developers perceive AI's influence across the
dimensions of the SPACE framework: Satisfaction, Performance, Activity,
Collaboration and Efficiency. Drawing on survey responses from over 500
developers and qualitative insights from interviews and observational studies,
we find that AI is broadly adopted and widely seen as enhancing productivity,
particularly for routine tasks. However, the benefits vary, depending on task
complexity, individual usage patterns, and team-level adoption. Developers
report increased efficiency and satisfaction, with less evidence of impact on
collaboration. Organizational support and peer learning play key roles in
maximizing AI's value. These findings suggest that AI is augmenting developers
rather than replacing them, and that effective integration depends as much on
team culture and support structures as on the tools themselves. We conclude
with practical recommendations for teams, organizations and researchers seeking
to harness AI's potential in software engineering.

</details>


### [66] [HandOver: Enabling Precise Selection & Manipulation of 3D Objects with Mouse and Hand Tracking](https://arxiv.org/abs/2508.00211)
*Esen K. Tütüncü,Mar Gonzalez-Franco,Eric J. Gonzalez*

Main category: cs.HC

TL;DR: HandOver结合鼠标精确选择和手势追踪的3D操作，提高XR交互的精度和表现力。用户研究显示其优于传统光线投射方法，降低任务误差并提升操作舒适性。


<details>
  <summary>Details</summary>
Motivation: 传统XR交互中，光线投射技术难以兼顾对象选择的精度和手势操作的灵活性。HandOver旨在结合鼠标的精确性和手势的表现力，提供更优的交互方案。

Method: HandOver利用鼠标驱动深度感知3D光标进行精确选择，用户通过手势悬停过渡到直接3D操作目标对象。在研究中，与两种光线投射技术（传统Ray和混合Ray+Hand）进行比较。

Result: HandOver在所有距离的任务误差上表现更优，并通过RULA姿势分析和NASA-TLX自评指标显著提升交互舒适性。

Conclusion: HandOver展示了一种结合传统输入设备和手势追踪的XR交互范式，显著提升用户舒适度和任务性能，为沉浸式环境提供统一的工作流。

Abstract: We present HandOver, an extended reality (XR) interaction technique designed
to unify the precision of traditional mouse input for object selection with the
expressiveness of hand-tracking for object manipulation. With HandOver, the
mouse is used to drive a depth-aware 3D cursor enabling precise and restful
targeting -by hovering their hand over the mouse, the user can then seamlessly
transition into direct 3D manipulation of the target object. In a formal user
study, we compare HandOver against two raybased techniques: traditional
raycasting (Ray) and a hybrid method (Ray+Hand) in a 3D docking task. Results
show HandOver yields lower task errors across all distances, and moreover
improves interaction ergonomics as highlighted by a RULA posture analysis and
self-reported measures (NASA-TLX). These findings illustrate the benefits of
blending traditional precise input devices with the expressive gestural inputs
afforded by hand-tracking in XR, leading to improved user comfort and task
performance. This blended paradigm yields a unified workflow allowing users to
leverage the best of each input modality as they interact in immersive
environments.

</details>


### [67] [Correcting Misperceptions at a Glance: Using Data Visualizations to Reduce Political Sectarianism](https://arxiv.org/abs/2508.00233)
*Douglas Markant,Subham Sah,Alireza Karduni,Milad Rogha,My Thai,Wenwen Dou*

Main category: cs.HC

TL;DR: 研究发现，政治派别的极端主义部分源于对政治对手的错误认知，纠正这些错误认知能减少极端主义支持。数据可视化的方式影响纠正效果。


<details>
  <summary>Details</summary>
Motivation: 研究探讨如何通过数据可视化纠正对政治对手的误解，以减少极端主义支持。

Method: 实验设计比较不同数据可视化方式（仅平均值、平均值加区间、平均值加点）对纠正效果的影响。

Result: 仅显示平均值和显示全分布的纠正效果最强，而显示区间的效果较弱。显示全分布还提高了记忆准确性。

Conclusion: 数据可视化是纠正群体误解的有效工具，但其呈现方式显著影响效果。

Abstract: Political sectarianism is fueled in part by misperceptions of political
opponents: People commonly overestimate the support for extreme policies among
members of the other party. Research suggests that correcting partisan
misperceptions by informing people about the actual views of outparty members
may reduce one's own expressed support for political extremism, including
partisan violence and anti-democratic actions. The present study investigated
how correction effects depend on different representations of outparty views
communicated through data visualizations. We conducted an experiment with U.S.
based participants from Prolific (N=239 Democrats, N=244 Republicans).
Participants made predictions about support for political violence and
undemocratic practices among members of their political outparty. They were
then presented with data from an earlier survey on the actual views of outparty
members. Some participants viewed only the average response (Mean-Only
condition), while other groups were shown visual representations of the range
of views from 75% of the outparty (Mean+Interval condition) or the full
distribution of responses (Mean+Points condition). Compared to a control group
that was not informed about outparty views, we observed the strongest
correction effects among participants in the Mean-only and Mean+Points
condition, while correction effects were weaker in the Mean+Interval condition.
In addition, participants who observed the full distribution of out-party views
(Mean+Points condition) were most accurate at later recalling the degree of
support among the outparty. Our findings suggest that data visualizations can
be an important tool for correcting pervasive distortions in beliefs about
other groups. However, the way in which variability in outparty views is
visualized can significantly shape how people interpret and respond to
corrective information.

</details>


### [68] [What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance](https://arxiv.org/abs/2508.00239)
*Jacqueline Elise Bruen,Myounghoon Jeon*

Main category: cs.HC

TL;DR: 研究探讨了观众对AI生成艺术作品的评价差异，尤其是对舞蹈表演的评价，结果显示观众在不了解AI参与时更容易认可其艺术价值。


<details>
  <summary>Details</summary>
Motivation: 探讨人工智能生成艺术的价值争议，尤其是观众对不同技术参与的表演的感知差异。

Method: 开发了两个版本的舞蹈表演（含或不含GenAI），对观众进行前后问卷调查，共39名参与者。

Result: 观众在不了解AI参与时更倾向于认可其艺术价值。

Conclusion: 强调社会背景和用户解释的重要性，呼吁更广泛的讨论以弥合理解差距。

Abstract: With the development of generative artificial intelligence (GenAI) tools to
create art, stakeholders cannot come to an agreement on the value of these
works. In this study we uncovered the mixed opinions surrounding art made by
AI. We developed two versions of a dance performance augmented by technology
either with or without GenAI. For each version we informed audiences of the
performance's development either before or after a survey on their perceptions
of the performance. There were thirty-nine participants (13 males, 26 female)
divided between the four performances. Results demonstrated that individuals
were more inclined to attribute artistic merit to works made by GenAI when they
were unaware of its use. We present this case study as a call to address the
importance of utilizing the social context and the users' interpretations of
GenAI in shaping a technical explanation, leading to a greater discussion that
can bridge gaps in understanding.

</details>


### [69] [TofuML: A Spatio-Physical Interactive Machine Learning Device for Interactive Exploration of Machine Learning for Novices](https://arxiv.org/abs/2508.00252)
*Wataru Kawabe,Hiroto Fukuda,Akihisa Shitara,Yuri Nakao,Yusuke Sugano*

Main category: cs.HC

TL;DR: TofuML 是一个互动系统，通过物理空间界面使非专家用户更容易接触和理解机器学习概念，提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 为了使非专家用户更容易理解和参与机器学习模型的设计与评估过程。

Method: 使用物理设备与纸质界面，让用户通过直觉操作训练声音分类模型，并与传统 GUI 系统对比。

Result: TofuML 比 GUI 更能吸引用户，降低非专家参与机器学习的门槛，并激发用户创新的应用想法。

Conclusion: 该研究为开发面向广泛用户的互动机器学习系统提供了实用参考。

Abstract: We introduce TofuML, an interactive system designed to make machine learning
(ML) concepts more accessible and engaging for non-expert users. Unlike
conventional GUI-based systems, TofuML employs a physical and spatial interface
consisting of a small device and a paper mat, allowing users to train and
evaluate sound classification models through intuitive, toy-like interactions.
Through two user studies -- a comparative study against a GUI-based version and
a public event deployment -- we investigated how TofuML impacts users'
engagement in the ML model creation process, their ability to provide
appropriate training data, and their conception of potential applications. Our
results indicated that TofuML enhanced user engagement compared to a GUI while
lowering barriers for non-experts to engage with ML. Users demonstrated
creativity in conceiving diverse ML applications, revealing opportunities to
optimize between conceptual understanding and user engagement. These findings
contribute to developing interactive ML systems/frameworks designed for a wide
range of users.

</details>


### [70] [MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems](https://arxiv.org/abs/2508.00300)
*Shruthi Chari,Oshani Seneviratne,Prithwish Chakraborty,Pablo Meyer,Deborah L. McGuinness*

Main category: cs.HC

TL;DR: MetaExplainer是一个神经符号框架，通过三步流程生成用户中心的AI解释，显著提升解释的个性化和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统提供的解释与用户需求之间存在差距，需要一种更贴近用户的解释方法。

Method: 采用三阶段流程：分解用户问题、生成模型推荐、合成自然语言解释，并结合解释本体指导LLMs。

Result: 实验显示高绩效（F1-score 59.06%），用户研究证实解释的创造性和全面性。

Conclusion: MetaExplainer在糖尿病数据集中表现优异，具备广泛应用潜力。

Abstract: Explanations are crucial for building trustworthy AI systems, but a gap often
exists between the explanations provided by models and those needed by users.
To address this gap, we introduce MetaExplainer, a neuro-symbolic framework
designed to generate user-centered explanations. Our approach employs a
three-stage process: first, we decompose user questions into machine-readable
formats using state-of-the-art large language models (LLM); second, we delegate
the task of generating system recommendations to model explainer methods; and
finally, we synthesize natural language explanations that summarize the
explainer outputs. Throughout this process, we utilize an Explanation Ontology
to guide the language models and explainer methods. By leveraging LLMs and a
structured approach to explanation generation, MetaExplainer aims to enhance
the interpretability and trustworthiness of AI systems across various
applications, providing users with tailored, question-driven explanations that
better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate
a step towards evaluating and utilizing current state-of-the-art explanation
frameworks. Our results show high performance across all stages, with a 59.06%
F1-score in question reframing, 70% faithfulness in model explanations, and 67%
context-utilization in natural language synthesis. User studies corroborate
these findings, highlighting the creativity and comprehensiveness of generated
explanations. Tested on the Diabetes (PIMA Indian) tabular dataset,
MetaExplainer supports diverse explanation types, including Contrastive,
Counterfactual, Rationale, Case-Based, and Data explanations. The framework's
versatility and traceability from using ontology to guide LLMs suggest broad
applicability beyond the tested scenarios, positioning MetaExplainer as a
promising tool for enhancing AI explainability across various domains.

</details>


### [71] [Evaluating the Efficacy of Large Language Models for Generating Fine-Grained Visual Privacy Policies in Homes](https://arxiv.org/abs/2508.00321)
*Shuning Zhang,Ying Ma,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 论文探讨在智能家居环境中利用LLM实现动态隐私控制的可行性，提出了一种多维度分类框架，并通过实验验证其效果。


<details>
  <summary>Details</summary>
Motivation: 智能家居中视觉传感器的普及带来了隐私挑战，现有隐私控制方法静态且粗糙，无法适应动态和社交复杂的环境。

Method: 提出多维度分类框架（数据敏感性、空间上下文、社交存在），利用LLM实时推理并执行细粒度隐私规则。

Result: 实验证实该方法可行，LLM引擎的机器评估得分为3.99/5，人类评估得分为4.00/5。

Conclusion: LLM可以作为动态隐私政策引擎核心，实现高效且社会化的隐私保护。

Abstract: The proliferation of visual sensors in smart home environments, particularly
through wearable devices like smart glasses, introduces profound privacy
challenges. Existing privacy controls are often static and coarse-grained,
failing to accommodate the dynamic and socially nuanced nature of home
environments. This paper investigates the viability of using Large Language
Models (LLMs) as the core of a dynamic and adaptive privacy policy engine. We
propose a conceptual framework where visual data is classified using a
multi-dimensional schema that considers data sensitivity, spatial context, and
social presence. An LLM then reasons over this contextual information to
enforce fine-grained privacy rules, such as selective object obfuscation, in
real-time. Through a comparative evaluation of state-of-the-art Vision Language
Models (including GPT-4o and the Qwen-VL series) in simulated home settings ,
our findings show the feasibility of this approach. The LLM-based engine
achieved a top machine-evaluated appropriateness score of 3.99 out of 5, and
the policies generated by the models received a top human-evaluated score of
4.00 out of 5.

</details>


### [72] [From Patient Burdens to User Agency: Designing for Real-Time Protection Support in Online Health Consultations](https://arxiv.org/abs/2508.00328)
*Shuning Zhang,Ying Ma,Yongquan `Owen' Hu,Ting Dang,Hong Jia,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 本文探讨了在线医疗咨询平台的隐私风险，提出了SafeShare技术以平衡隐私与实用性，通过本地化LLM实时匿名化信息。


<details>
  <summary>Details</summary>
Motivation: 用户对匿名性和控制权的需求与平台现实的隐私保护措施存在脱节，亟需解决以重建信任。

Method: 通过半结构化访谈了解用户需求和挑战，开发了SafeShare技术，采用本地化LLM实时匿名化咨询内容。

Result: SafeShare的PII检测模块在三个数据集上表现出色，IMCS21数据集上准确率达89.64%。

Conclusion: SafeShare通过技术手段有效平衡了隐私保护与信息实用性，为在线医疗平台的隐私问题提供了可行解决方案。

Abstract: Online medical consultation platforms, while convenient, are undermined by
significant privacy risks that erode user trust. We first conducted in-depth
semi-structured interviews with 12 users to understand their perceptions of
security and privacy landscapes on online medical consultation platforms, as
well as their practices, challenges and expectation. Our analysis reveals a
critical disconnect between users' desires for anonymity and control, and
platform realities that offload the responsibility of ``privacy labor''. To
bridge this gap, we present SafeShare, an interaction technique that leverages
localized LLM to redact consultations in real-time. SafeShare balances utility
and privacy through selectively anonymize private information. A technical
evaluation of SafeShare's core PII detection module on 3 dataset demonstrates
high efficacy, achieving 89.64\% accuracy with Qwen3-4B on IMCS21 dataset.

</details>


### [73] [HateBuffer: Safeguarding Content Moderators' Mental Well-Being through Hate Speech Content Modification](https://arxiv.org/abs/2508.00439)
*Subin Park,Jeonghyun Kim,Jeanne Choi,Joseph Seering,Uichin Lee,Sung-Ju Lee*

Main category: cs.HC

TL;DR: HateBuffer是一款旨在保护内容审核员心理健康的工具，通过匿名化仇恨言论目标和改写攻击性表达，但其实际效果与预期不符。


<details>
  <summary>Details</summary>
Motivation: 在线平台上的仇恨言论对内容审核员的心理健康造成了严重负担。

Method: 设计了HateBuffer工具，通过匿名化目标、改写攻击性表达等方式减轻审核员的心理负担。

Result: 使用HateBuffer后，参与者对仇恨言论严重性的评分降低，但在情绪和疲劳方面未见明显改善；不过，HateBuffer提高了审核准确性并略微提升了召回率。

Conclusion: HateBuffer虽未直接改善心理健康，但显示出文本内容修改技术在健康内容审核环境中的潜力。

Abstract: Hate speech remains a persistent and unresolved challenge in online
platforms. Content moderators, working on the front lines to review
user-generated content and shield viewers from hate speech, often find
themselves unprotected from the mental burden as they continuously engage with
offensive language. To safeguard moderators' mental well-being, we designed
HateBuffer, which anonymizes targets of hate speech, paraphrases offensive
expressions into less offensive forms, and shows the original expressions when
moderators opt to see them. Our user study with 80 participants consisted of a
simulated hate speech moderation task set on a fictional news platform,
followed by semi-structured interviews. Although participants rated the hate
severity of comments lower while using HateBuffer, contrary to our
expectations, they did not experience improved emotion or reduced fatigue
compared with the control group. In interviews, however, participants described
HateBuffer as an effective buffer against emotional contagion and the
normalization of biased opinions in hate speech. Notably, HateBuffer did not
compromise moderation accuracy and even contributed to a slight increase in
recall. We explore possible explanations for the discrepancy between the
perceived benefits of HateBuffer and its measured impact on mental well-being.
We also underscore the promise of text-based content modification techniques as
tools for a healthier content moderation environment.

</details>


### [74] [Pull Requests From The Classroom: Co-Developing Curriculum And Code](https://arxiv.org/abs/2508.00646)
*Dennis Zyska,Ilia Kuznetsov,Florian Müller,Iryna Gurevych*

Main category: cs.HC

TL;DR: 教育技术与教师教学目标常不匹配，导致教学效果受损。本文通过大学科学写作课程的案例，探讨了课程与技术的共同开发。研究发现共同开发虽能提高软件功能与课程目标的契合度，但也揭示了可用性和基础设施问题，强调教学与技术团队需更紧密协作。


<details>
  <summary>Details</summary>
Motivation: 解决教育技术与教师教学目标之间的不匹配问题，以提高教学效果。

Method: 通过大学科学写作课程的案例研究，共同开发定制化的同行反馈系统，支持注释、反馈交流和修订。

Result: 共同开发增强了软件功能与课程目标的一致性，但也暴露了可用性和基础设施的局限。

Conclusion: 教学与技术团队需更紧密协作，以优化教育技术的开发与使用。

Abstract: Educational technologies often misalign with instructors' pedagogical goals,
forcing adaptations that compromise teaching efficacy. In this paper, we
present a case study on the co-development of curriculum and technology in the
context of a university course on scientific writing. Specifically, we examine
how a custom-built peer feedback system was iteratively developed alongside the
course to support annotation, feedback exchange, and revision. Results show
that while co-development fostered stronger alignment between software features
and course goals, it also exposed usability limitations and
infrastructure-related frustrations, emphasizing the need for closer
coordination between teaching and technical teams.

</details>


### [75] [The Manipulative Power of Voice Characteristics: Investigating Deceptive Patterns in Mandarin Chinese Female Synthetic Speech](https://arxiv.org/abs/2508.00652)
*Shuning Zhang,Han Chen,Yabo Wang,Yiqun Xu,Jiaqi Bai,Yuanyuan Wu,Shixuan Li,Xin Yi,Chunhui Wang,Hewu Li*

Main category: cs.HC

TL;DR: 该研究首次系统探讨了中文普通话中女性合成声音的语音特征如何影响用户行为，揭示了在不同场景下语音特征对用户决策的显著影响，为道德设计提供了依据。


<details>
  <summary>Details</summary>
Motivation: 现有研究对语音交互中的欺骗模式（尤其是非英语语境）关注不足，特别是在中文背景下。研究旨在填补这一空白，探讨语音特征如何影响用户行为。

Method: 通过操纵五种语音特征（三种强度）和两种场景（购物与问答），结合初步研究（N=24）和主研究（N=36），评估语音特征对用户行为的影响。

Result: 主研究显示语音特征显著影响用户行为（最高达+2027.6%），效果因特征、场景及用户感知（如音调、音色）和人口统计学差异而变化。

Conclusion: 研究强调了语音特征在用户行为中的关键作用，为语音交互的道德设计提供了实证支持。

Abstract: Pervasive voice interaction enables deceptive patterns through subtle voice
characteristics, yet empirical investigation into this manipulation lags
behind, especially within major non-English language contexts. Addressing this
gap, our study presents the first systematic investigation into voice
characteristic-based dark patterns employing female synthetic voices in
Mandarin Chinese. This focus is crucial given the prevalence of female personas
in commercial assistants and the prosodic significance in the Chinese language.
Guided by the conceptual framework identifying key influencing factors, we
systematically evaluate effectiveness variations by manipulating voice
characteristics (five characteristics, three intensities) across different
scenarios (shopping vs. question-answering) with different commercial aims. A
preliminary study (N=24) validated the experimental materials and the main
study (N=36) revealed significant behavioral manipulation (up to +2027.6%).
Crucially, the analysis showed that effectiveness varied significantly with
voice characteristics and scenario, mediated by user perception (of tone,
intonation, timbre) and user demographics (individual preferences, though
limited demographic impact). These interconnected findings offer evidence-based
insights for ethical design.

</details>


### [76] [Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors Impacting AI Adoption](https://arxiv.org/abs/2508.00723)
*Rebecca Yu,Valerie Chen,Ameet Talwalkar,Hoda Heidari*

Main category: cs.HC

TL;DR: 研究探讨了决策者在不同领域中自愿采用AI工具的关键因素，并提出了AI采用表以分析跨领域差异。


<details>
  <summary>Details</summary>
Motivation: 评估AI工具如何被决策者自愿采用，以支持AI的负责任部署。

Method: 采访了医学、法律、新闻和公共部门的专家，分析AI使用案例和采用感知。

Result: 确定了影响AI采用的四个关键因素，并将其转化为AI采用表，用于跨领域比较分析。

Conclusion: 研究结果提供了实用的指南，有助于更好地从决策者角度部署AI。

Abstract: Growing excitement around deploying AI across various domains calls for a
careful assessment of how human decision-makers interact with AI-powered
systems. In particular, it is essential to understand when decision-makers
voluntarily choose to consult AI tools, which we term decision-maker adoption.
We interviewed experts across four domains -- medicine, law, journalism, and
the public sector -- to explore current AI use cases and perceptions of
adoption. From these interviews, we identify key factors that shape
decision-maker adoption of AI tools: the decision-maker's background,
perceptions of the AI, consequences for the decision-maker, and perceived
implications for other stakeholders. We translate these factors into an AI
adoption sheet to analyze how decision-makers approach adoption choices through
comparative, cross-domain case studies, highlighting how our factors help
explain inter-domain differences in adoption. Our findings offer practical
guidance for supporting the responsible and context-aware deployment of AI by
better accounting for the decision-maker's perspective.

</details>


### [77] [How LLMs are Shaping the Future of Virtual Reality](https://arxiv.org/abs/2508.00737)
*Süeda Özkaya,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: 本文综述了大型语言模型（LLMs）在虚拟现实（VR）游戏中的应用，探讨了其在叙事生成、NPC互动、可访问性和个性化等方面的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合LLMs提升VR游戏的沉浸感、适应性和智能性，推动更真实的用户体验。

Method: 分析了2018-2025年间62项同行评议研究，总结了LLMs在VR中的应用领域和设计策略。

Result: 研究表明，LLMs能显著增强VR的逼真度和用户参与度，但需解决实时性能、伦理等问题。

Conclusion: 未来需关注多模态AI、情感计算等领域，以促进智能且包容的VR系统发展。

Abstract: The integration of Large Language Models (LLMs) into Virtual Reality (VR)
games marks a paradigm shift in the design of immersive, adaptive, and
intelligent digital experiences. This paper presents a comprehensive review of
recent research at the intersection of LLMs and VR, examining how these models
are transforming narrative generation, non-player character (NPC) interactions,
accessibility, personalization, and game mastering. Drawing from an analysis of
62 peer reviewed studies published between 2018 and 2025, we identify key
application domains ranging from emotionally intelligent NPCs and procedurally
generated storytelling to AI-driven adaptive systems and inclusive gameplay
interfaces. We also address the major challenges facing this convergence,
including real-time performance constraints, memory limitations, ethical risks,
and scalability barriers. Our findings highlight that while LLMs significantly
enhance realism, creativity, and user engagement in VR environments, their
effective deployment requires robust design strategies that integrate
multimodal interaction, hybrid AI architectures, and ethical safeguards. The
paper concludes by outlining future research directions in multimodal AI,
affective computing, reinforcement learning, and open-source development,
aiming to guide the responsible advancement of intelligent and inclusive VR
systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [78] [Occlusion-robust Stylization for Drawing-based 3D Animation](https://arxiv.org/abs/2508.00398)
*Sunjae Yoon,Gwanhyeong Koo,Younghwan Lee,Ji Woo Hong,Chang D. Yoo*

Main category: cs.GR

TL;DR: 论文提出了Occlusion-robust Stylization Framework (OSF)，用于解决基于手绘的3D动画中因遮挡导致风格退化的问题，通过光学流提供遮挡鲁棒的边缘引导，显著提升了风格一致性，并实现了更高效的推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在基于手绘的3D动画中，因遮挡导致风格属性退化（如轮廓闪烁和笔触模糊），尤其是在动态运动中。这是由于训练与推理时的姿势差距（stylization pose gap）问题所致。

Method: 提出OSF框架，利用光学流提供遮挡鲁棒的边缘引导，确保即使在遮挡情况下也能保持一致的风格化。OSF单阶段运行，比两阶段方法更快且更节省内存。

Result: OSF显著提升了风格一致性，推理速度提升2.4倍，内存占用减少2.1倍。

Conclusion: OSF为基于手绘的3D动画提供了高效且鲁棒的解决方案，解决了风格退化问题，同时优化了计算效率。

Abstract: 3D animation aims to generate a 3D animated video from an input image and a
target 3D motion sequence. Recent advances in image-to-3D models enable the
creation of animations directly from user-hand drawings. Distinguished from
conventional 3D animation, drawing-based 3D animation is crucial to preserve
artist's unique style properties, such as rough contours and distinct stroke
patterns. However, recent methods still exhibit quality deterioration in style
properties, especially under occlusions caused by overlapping body parts,
leading to contour flickering and stroke blurring. This occurs due to a
`stylization pose gap' between training and inference in stylization networks
designed to preserve drawing styles in drawing-based 3D animation systems. The
stylization pose gap denotes that input target poses used to train the
stylization network are always in occlusion-free poses, while target poses
encountered in an inference include diverse occlusions under dynamic motions.
To this end, we propose Occlusion-robust Stylization Framework (OSF) for
drawing-based 3D animation. We found that while employing object's edge can be
effective input prior for guiding stylization, it becomes notably inaccurate
when occlusions occur at inference. Thus, our proposed OSF provides
occlusion-robust edge guidance for stylization network using optical flow,
ensuring a consistent stylization even under occlusions. Furthermore, OSF
operates in a single run instead of the previous two-stage method, achieving
2.4x faster inference and 2.1x less memory.

</details>


### [79] [CrossSet: Unveiling the Complex Interplay of Two Set-typed Dimensions in Multivariate Data](https://arxiv.org/abs/2508.00424)
*Kresimir Matkovic,Rainer Splechtna,Denis Gracanin,Helwig Hauser*

Main category: cs.GR

TL;DR: 提出了一种名为CrossSet的新方法，用于交互式可视分析双变量集合类型数据，通过多尺度方法联合研究两个集合类型的维度及其相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要集中在单个集合类型维度的分析，而缺乏对两个集合类型维度及其交互作用的联合研究。

Method: 基于任务分析，采用分层矩阵布局实现双维度集合类型数据的多尺度交互式可视化，支持从全局概览到细节分析。

Result: 通过多个应用场景验证了CrossSet的有效性和高效性。

Conclusion: CrossSet为双变量集合类型数据的交互式可视分析提供了一种有效且高效的新方法。

Abstract: The interactive visual analysis of set-typed data, i.e., data with attributes
that are of type set, is a rewarding area of research and applications.
Valuable prior work has contributed solutions that enable the study of such
data with individual set-typed dimensions. In this paper, we present CrossSet,
a novel method for the joint study of two set-typed dimensions and their
interplay. Based on a task analysis, we describe a new, multi-scale approach to
the interactive visual exploration and analysis of such data. Two set-typed
data dimensions are jointly visualized using a hierarchical matrix layout,
enabling the analysis of the interactions between two set-typed attributes at
several levels, in addition to the analysis of individual such dimensions.
CrossSet is anchored at a compact, large-scale overview that is complemented by
drill-down opportunities to study the relations between and within the
set-typed dimensions, enabling an interactive visual multi-scale exploration
and analysis of bivariate set-typed data. Such an interactive approach makes it
possible to study single set-typed dimensions in detail, to gain an overview of
the interaction and association between two such dimensions, to refine one of
the dimensions to gain additional details at several levels, and to drill down
to the specific interactions of individual set-elements from the set-typed
dimensions. To demonstrate the effectiveness and efficiency of CrossSet, we
have evaluated the new method in the context of several application scenarios.

</details>


### [80] [Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation](https://arxiv.org/abs/2508.00428)
*Nan Xiang,Tianyi Liang,Haiwen Huang,Shiqi Jiang,Hao Huang,Yifei Huang,Liangyu Chen,Changbo Wang,Chenhui Li*

Main category: cs.GR

TL;DR: Sel3DCraft是一个用于文本到3D生成的视觉提示工程系统，通过双分支结构、多视图混合评分和视觉分析套件解决了传统盲试问题。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到3D生成技术依赖于盲目的试错提示过程，导致结果不可预测，亟需一种更直观的引导方法。

Method: Sel3DCraft采用双分支结构（检索与生成结合）、多视图混合评分以及视觉分析工具，以提升3D模型的评估和优化。

Result: 实验和用户研究表明，Sel3DCraft在支持设计师创造力方面优于其他文本到3D生成系统。

Conclusion: Sel3DCraft通过视觉提示工程改进了文本到3D生成的效率和可控性，为设计领域提供了实用工具。

Abstract: Text-to-3D (T23D) generation has transformed digital content creation, yet
remains bottlenecked by blind trial-and-error prompting processes that yield
unpredictable results. While visual prompt engineering has advanced in
text-to-image domains, its application to 3D generation presents unique
challenges requiring multi-view consistency evaluation and spatial
understanding. We present Sel3DCraft, a visual prompt engineering system for
T23D that transforms unstructured exploration into a guided visual process. Our
approach introduces three key innovations: a dual-branch structure combining
retrieval and generation for diverse candidate exploration; a multi-view hybrid
scoring approach that leverages MLLMs with innovative high-level metrics to
assess 3D models with human-expert consistency; and a prompt-driven visual
analytics suite that enables intuitive defect identification and refinement.
Extensive testing and user studies demonstrate that Sel3DCraft surpasses other
T23D systems in supporting creativity for designers.

</details>


### [81] [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation](https://arxiv.org/abs/2508.00782)
*Kien T. Pham,Yingqing He,Yazhou Xing,Qifeng Chen,Long Chen*

Main category: cs.GR

TL;DR: 论文提出了一种名为SpA2V的框架，通过空间听觉线索从音频生成语义和空间对齐的视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注音频的语义信息，忽略了声音的空间属性（如位置和方向），而SpA2V填补了这一空白。

Method: SpA2V分为两阶段：1）音频引导的视频规划，利用MLLM提取空间和语义线索构建视频场景布局（VSL）；2）基于布局的视频生成，将VSL作为条件输入预训练的扩散模型。

Result: 实验表明SpA2V能生成与输入音频语义和空间对齐的高质量视频。

Conclusion: SpA2V通过利用空间听觉线索，显著提升了视频生成的空间和语义准确性。

Abstract: Audio-driven video generation aims to synthesize realistic videos that align
with input audio recordings, akin to the human ability to visualize scenes from
auditory input. However, existing approaches predominantly focus on exploring
semantic information, such as the classes of sounding sources present in the
audio, limiting their ability to generate videos with accurate content and
spatial composition. In contrast, we humans can not only naturally identify the
semantic categories of sounding sources but also determine their deeply encoded
spatial attributes, including locations and movement directions. This useful
information can be elucidated by considering specific spatial indicators
derived from the inherent physical properties of sound, such as loudness or
frequency. As prior methods largely ignore this factor, we present SpA2V, the
first framework explicitly exploits these spatial auditory cues from audios to
generate videos with high semantic and spatial correspondence. SpA2V decomposes
the generation process into two stages: 1) Audio-guided Video Planning: We
meticulously adapt a state-of-the-art MLLM for a novel task of harnessing
spatial and semantic cues from input audio to construct Video Scene Layouts
(VSLs). This serves as an intermediate representation to bridge the gap between
the audio and video modalities. 2) Layout-grounded Video Generation: We develop
an efficient and effective approach to seamlessly integrate VSLs as conditional
guidance into pre-trained diffusion models, enabling VSL-grounded video
generation in a training-free manner. Extensive experiments demonstrate that
SpA2V excels in generating realistic videos with semantic and spatial alignment
to the input audios.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [82] [Reimagining Voltage-Controlled Cryogenic Boolean Logic Paradigm with Quantum-Enhanced Josephson Junction FETs](https://arxiv.org/abs/2508.00295)
*Md Mazharul Islam,Diego Ferrer,Shamiul Alam,Juan P. Mendez,Denis Mamaluy,Wei Pan,Ahmedullah Aziz*

Main category: cs.ET

TL;DR: 论文提出了基于量子增强JJFET的超导逻辑电路设计，解决了现有超导器件在级联性和增益方面的挑战，并通过仿真验证了基本逻辑门和级联电路的可行性。


<details>
  <summary>Details</summary>
Motivation: 超低温电子器件在低功耗计算和量子技术中具有潜力，但现有超导器件在级联性和增益方面存在不足，限制了其在复杂逻辑架构中的应用。

Method: 采用量子增强JJFET（基于InAs和GaSb异质结构）设计电压控制逻辑电路，并结合多层加热纳米超导热开关（nTron）确保级联性。通过Verilog A模型模拟器件特性。

Result: 仿真展示了NOT、NAND和NOR等基本逻辑门以及3输入多数门的成功实现，并通过2输入XOR门验证了级联性。

Conclusion: 量子增强JJFET及nTron的结合为超导逻辑电路提供了可行的解决方案，有望推动超低温电子器件的发展。

Abstract: The growing demand for ultra low power computing and the emergence of quantum
technologies have intensified interest in cryogenic electronics, particularly
superconducting devices.Despite their promise, current controlled
superconducting components face fundamental challenges in cascadability,
limiting their effectiveness in complex logic architectures.To overcome this,
recent efforts have focused on developing gate tunable superconducting devices,
such as Josephson Junction Field Effect Transistors (JJFETs).However, achieving
robust control and sufficient supercurrent gain, both critical for
transistor-like performance in logic circuits remains a key challenge.A recent
advancement in JJFET design, based on InAs and GaSb heterostructures,
demonstrates enhanced gain and favorable device characteristics suitable for
circuit integration.Building on this innovation, we propose and analyze
fundamental voltage controlled logic topologies using the quantum enhanced
JJFET. We develop a Verilog A based circuit compatible compact model of the
quantum enhanced JJFET which accurately captures the experimentally observed
device characteristics.To ensure cascadability, our logic circuits incorporate
the multilayered Heater Nanocryotron (nTron), a superconducting nanowire-based
thermal switch.Through simulation based analysis, we demonstrate the successful
implementation of fundamental logic gates, including NOT, NAND, and NOR.
Furthermore, we design a 3 input majority gate, which plays a pivotal role in
quantum and reversible computing due to its universality.Finally, to
demonstrate the cascadability of our proposed logic topology, we demonstrate
the operation of a 2 input XOR gate based on our designed JJFET based NOT,
NAND, and NOR gate.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [83] [Integrated user scheduling and beam steering in over-the-air federated learning for mobile IoT](https://arxiv.org/abs/2508.00341)
*Shengheng Liu,Ningning Fu,Zhonghao Zhang,Yongming Huang,Tony Q. S. Quek*

Main category: cs.DC

TL;DR: 提出了一种结合空中计算的联邦学习框架，通过优化用户调度和接收波束成形策略，提升了大规模网络中模型聚合的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）的普及导致大量敏感数据的收集，隐私问题日益突出。联邦学习（FL）作为一种去中心化解决方案，需要解决无线资源稀缺导致的用户参与限制问题。

Method: 引入空中计算到FL框架，通过差分凸技术分解非凸优化问题，提出低复杂度用户调度策略以直接确定用户子集。

Result: 实验证明，所提方法在聚合误差和学习性能上优于现有方法。

Conclusion: 通过优化调度和传输策略，显著提升了大规模FL网络的效率和准确性，同时降低了计算复杂度。

Abstract: The rising popularity of Internet of things (IoT) has spurred technological
advancements in mobile internet and interconnected systems. While offering
flexible connectivity and intelligent applications across various domains, IoT
service providers must gather vast amounts of sensitive data from users, which
nonetheless concomitantly raises concerns about privacy breaches. Federated
learning (FL) has emerged as a promising decentralized training paradigm to
tackle this challenge. This work focuses on enhancing the aggregation
efficiency of distributed local models by introducing over-the-air computation
into the FL framework. Due to radio resource scarcity in large-scale networks,
only a subset of users can participate in each training round. This highlights
the need for effective user scheduling and model transmission strategies to
optimize communication efficiency and inference accuracy. To address this, we
propose an integrated approach to user scheduling and receive beam steering,
subject to constraints on the number of selected users and transmit power.
Leveraging the difference-of-convex technique, we decompose the primal
non-convex optimization problem into two sub-problems, yielding an iterative
solution. While effective, the computational load of the iterative method
hampers its practical implementation. To overcome this, we further propose a
low-complexity user scheduling policy based on characteristic analysis of the
wireless channel to directly determine the user subset without iteration.
Extensive experiments validate the superiority of the proposed method in terms
of aggregation error and learning performance over existing approaches.

</details>


### [84] [Tetris: Efficient Intra-Datacenter Calls Packing for Large Conferencing Services](https://arxiv.org/abs/2508.00426)
*Rohan Gandhi,Ankur Mallick,Ken Sueda,Rui Liang*

Main category: cs.DC

TL;DR: Tetris框架通过利用历史数据和定期迁移优化呼叫分配，显著降低热点MP的负载。


<details>
  <summary>Details</summary>
Motivation: 当前算法在高负载时易导致MP服务器热点问题，影响性能和成本。

Method: 提出Tetris框架，分两步优化呼叫分配：基于历史数据的初始分配和定期迁移。

Result: 在24小时超过1000万次呼叫的测试中，Tetris将热点MP上的参与者数减少至少2.5倍。

Conclusion: Tetris有效解决了MP服务器热点问题，提升了性能和成本效益。

Abstract: Conference services like Zoom, Microsoft Teams, and Google Meet facilitate
millions of daily calls, yet ensuring high performance at low costs remains a
significant challenge. This paper revisits the problem of packing calls across
Media Processor (MP) servers that host the calls within individual datacenters
(DCs). We show that the algorithm used in Teams -- a large scale conferencing
service as well as other state-of-art algorithms are prone to placing calls
resulting in some of the MPs becoming hot (high CPU utilization) that leads to
degraded performance and/or elevated hosting costs. The problem arises from
disregarding the variability in CPU usage among calls, influenced by
differences in participant numbers and media types (audio/video), compounded by
bursty call arrivals. To tackle this, we propose Tetris, a multi-step framework
which (a) optimizes initial call assignments by leveraging historical data and
(b) periodically migrates calls from hot MPs using linear optimization, aiming
to minimize hot MP usage. Evaluation based on a 24-hour trace of over 10
million calls in one DC shows that Tetris reduces participant numbers on hot
MPs by at least 2.5X.

</details>


### [85] [SwarnRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments](https://arxiv.org/abs/2508.00622)
*Kapel Dev,Yash Madhwal,Sofia Shevelo,Pavel Osinenko,Yury Yanovich*

Main category: cs.DC

TL;DR: SwarnRaft是一个基于区块链的定位与共识框架，用于在GNSS信号缺失时维持无人机群的协作与数据完整性。


<details>
  <summary>Details</summary>
Motivation: 无人机群在关键应用中依赖GNSS信号，但信号可能因干扰或攻击中断，导致任务失败。

Method: 利用Raft共识算法，通过分布式节点（无人机）达成状态更新共识，即使部分节点无法接收GNSS信号。

Result: 原型系统通过轻量级通信模型展示了在信号缺失时的鲁棒性和容错能力。

Conclusion: 该工作为不可预测环境中的分散式无人机操作提供了实用且安全的基础。

Abstract: Unmanned aerial vehicle (UAV) swarms are increasingly used in critical
applications such as aerial mapping, environmental monitoring, and autonomous
delivery. However, the reliability of these systems is highly dependent on
uninterrupted access to the Global Navigation Satellite Systems (GNSS) signals,
which can be disrupted in real-world scenarios due to interference,
environmental conditions, or adversarial attacks, causing disorientation,
collision risks, and mission failure. This paper proposes SwarnRaft, a
blockchain-inspired positioning and consensus framework for maintaining
coordination and data integrity in UAV swarms operating under GNSS-denied
conditions. SwarnRaft leverages the Raft consensus algorithm to enable
distributed drones (nodes) to agree on state updates such as location and
heading, even in the absence of GNSS signals for one or more nodes. In our
prototype, each node uses GNSS and local sensing, and communicates over WiFi in
a simulated swarm. Upon signal loss, consensus is used to reconstruct or verify
the position of the failed node based on its last known state and trajectory.
Our system demonstrates robustness in maintaining swarm coherence and fault
tolerance through a lightweight, scalable communication model. This work offers
a practical and secure foundation for decentralized drone operation in
unpredictable environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [86] [E2ATST: A Temporal-Spatial Optimized Energy-Efficient Architecture for Training Spiking Transformer](https://arxiv.org/abs/2508.00475)
*Yunhao Ma,Yanyu Lin,Mingjing Li,Puli Quan,Chenlin Zhou,Wenyue Zhang,Zhiwei Zhong,Wanyi Jia,Xueke Zhu,Qingyan Meng,Huihui Zhou,Fengwei An*

Main category: cs.AR

TL;DR: 研究来自深圳的高校和研究机构合作完成。


<details>
  <summary>Details</summary>
Motivation: 探讨深圳地区高校和研究机构之间的合作模式和成果。

Method: 分析多个机构的联合研究情况。

Result: 展示了深圳地区学术合作的潜力与成效。

Conclusion: 深圳的研究机构和高校通过合作提升了研究水平和影响力。

Abstract: (1) Pengcheng Laboratory, (2) Southern University of Science and Technology,
(3) Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,
(4) University of Chinese Academy of Sciences

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [87] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型和多模态大型语言模型中表格处理的挑战，并提出了一种分类法和任务框架，指出了当前研究中的关键问题。


<details>
  <summary>Details</summary>
Motivation: 由于表格结构的复杂性和多样性，现有方法多为专门化而非通用，研究者希望通过系统化的分类和任务框架，推动该领域的进一步发展。

Method: 通过分类法描述表格输入表示形式，并介绍表格理解任务，梳理当前研究的不足。

Result: 指出了三个关键研究空白：任务偏重于检索而非推理；处理复杂表格结构和大规模表格时模型表现不佳；模型泛化能力有限。

Conclusion: 论文呼吁进一步研究以解决表格处理领域的关键挑战，特别是推理能力、复杂结构处理和多表格场景的建模。

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [88] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: 本文系统评估了大型语言模型（LLM）的优化技术（如剪枝、量化和标记丢弃）在长上下文场景中的效果，揭示了这些方法对生成质量的影响及其在更大模型上的扩展性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种NLP任务中表现出色，但其高资源需求和有限上下文窗口问题尚未得到充分研究。本文旨在填补这一空白。

Method: 通过系统基准测试，分析两种支持长上下文的LLM架构的优化方法（单独及组合使用），并扩展评估到700亿参数的更大模型。

Result: 研究发现，优化方法的简单组合会因累积近似误差而对较大模型产生负面影响，且仅依赖F1分数会掩盖问答任务中的精度-召回权衡。

Conclusion: 本研究为LLM实践者和研究者提供了平衡效率、准确性和扩展性的实用指导。

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [89] [AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation](https://arxiv.org/abs/2508.00733)
*Le Wang,Jun Wang,Feng Deng,Chen Zhang,Kun Gai,Di Zhang*

Main category: cs.SD

TL;DR: AudioGen-Omni 是一种基于多模态扩散变换器（MMDit）的统一方法，能够根据输入视频生成高质量音频、语音和歌曲，并通过联合训练实现多任务适应。


<details>
  <summary>Details</summary>
Motivation: 为克服传统文本固定范式的语义限制，提升跨模态条件生成的效果，设计了一种能够处理多模态输入的统一模型。

Method: 采用联合训练范式，整合视频-文本-音频数据，通过歌词-转录编码器和PAAPI增强的注意力机制实现精准跨模态对齐。

Result: 在音频质量、语义对齐和唇同步准确度上表现优异，并在多项音频生成任务中达到最先进水平，推理时间大幅缩短。

Conclusion: AudioGen-Omni 通过联合训练和高效架构，显著提升了多模态音频生成的性能与效率。

Abstract: We present AudioGen-Omni - a unified approach based on multimodal diffusion
transformers (MMDit), capable of generating high-fidelity audio, speech, and
songs coherently synchronized with the input video. AudioGen-Omni introduces a
novel joint training paradigm that seamlessly integrates large-scale
video-text-audio corpora, enabling a model capable of generating semantically
rich, acoustically diverse audio conditioned on multimodal inputs and adaptable
to a wide range of audio generation tasks. AudioGen-Omni employs a unified
lyrics-transcription encoder that encodes graphemes and phonemes from both sung
and spoken inputs into dense frame-level representations. Dense frame-level
representations are fused using an AdaLN-based joint attention mechanism
enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein
RoPE is selectively applied to temporally structured modalities to ensure
precise and robust cross-modal alignment. By unfreezing all modalities and
masking missing inputs, AudioGen-Omni mitigates the semantic constraints of
text-frozen paradigms, enabling effective cross-modal conditioning. This joint
training approach enhances audio quality, semantic alignment, and lip-sync
accuracy, while also achieving state-of-the-art results on
Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8
seconds of audio, it offers substantial improvements in both efficiency and
generality.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [90] [FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning with Major Malicious Clients](https://arxiv.org/abs/2508.00636)
*Haocheng Jiang,Hua Shen,Jixin Zhang,Willy Susilo,Mingwu Zhang*

Main category: cs.CR

TL;DR: FedGuard是一种新型联邦学习机制，通过利用成员推断对模型偏差的高敏感性，有效识别并排除中毒模型，显著优于现有防御方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习框架易受拜占庭攻击，尤其是当大多数客户端为恶意或数据高度非独立同分布时。现有防御机制仅针对特定攻击类型，效果有限。

Method: FedGuard要求客户端在训练中包含服务器指定的额外小批量数据，利用中毒模型在这批数据上置信度显著下降的特性进行识别。

Result: 在三种高度非独立同分布数据集、90%客户端为拜占庭且每轮七种攻击情况下，FedGuard显著优于现有方案。

Conclusion: FedGuard通过创新性方法有效解决了联邦学习中的拜占庭攻击问题，具备广泛适用性和高效性。

Abstract: Federated learning is a distributed training framework vulnerable to
Byzantine attacks, particularly when over 50% of clients are malicious or when
datasets are highly non-independent and identically distributed (non-IID).
Additionally, most existing defense mechanisms are designed for specific attack
types (e.g., gradient similarity-based schemes can only defend against outlier
model poisoning), limiting their effectiveness. In response, we propose
FedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the
aforementioned issues by leveraging the high sensitivity of membership
inference to model bias. By requiring clients to include an additional
mini-batch of server-specified data in their training, FedGuard can identify
and exclude poisoned models, as their confidence in the mini-batch will drop
significantly. Our comprehensive evaluation unequivocally shows that, under
three highly non-IID datasets, with 90% of clients being Byzantine and seven
different types of Byzantine attacks occurring in each round, FedGuard
significantly outperforms existing robust federated learning schemes in
mitigating various types of Byzantine attacks.

</details>


### [91] [Unveiling Dynamic Binary Instrumentation Techniques](https://arxiv.org/abs/2508.00682)
*Oscar Llorente-Vazquez,Xabier Ugarte-Pedrero,Igor Santos-Grueiro,Pablo Garcia Bringas*

Main category: cs.CR

TL;DR: 对动态二进制插桩（DBI）技术进行了全面的分析，比较了不同技术的优缺点，并指出没有单一技术在所有情况下都是最优的。


<details>
  <summary>Details</summary>
Motivation: 研究DBI技术的多样性和复杂性，整合过程级和全系统级别的分析方法，以帮助选择最适合的工具和技术。

Method: 分析DBI的构建模块和底层技术，比较不同技术对运行事件的插桩能力，并评估其性能表现。

Result: 研究发现，没有一种技术在所有情况下都优于其他技术。

Conclusion: DBI技术的选择应基于具体应用场景的需求。

Abstract: Dynamic Binary Instrumentation (DBI) is the set of techniques that enable
instrumentation of programs at run-time, making it possible to monitor and
modify the execution of compiled binaries or entire systems. DBI is used for
countless security applications and analyses, and is extensively used across
many fields in both industry and academia. Over the years, several DBI
approaches have been proposed based on different technologies and implementing
diverse techniques. Every solution tries to overcome certain limitations, but
they sometimes bring other shortcomings. Some are specialized for one
particular domain or task, while others have a wider scope.
  In this paper, we shed light into the labyrinth of DBI, bringing together
process-level and whole-system approaches. We depict their building blocks and
analyze the underlying instrumentation techniques, comparing their ability to
instrument different primitives and run-time events. Then, we evaluate their
performance when implementing each primitive, and highlight relevant
observations. Our results show that no single technique is better than the rest
in all circumstances.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [92] [Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process](https://arxiv.org/abs/2508.00816)
*Youssef Ait El Mahjoub,Jean-Michel Fourneau,Salma Alouah*

Main category: math.OC

TL;DR: 论文提出了一种基于单输入超状态可分解MDP（SISDMDP）的高效策略评估方法，适用于大规模状态空间和长期优化的MDP问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模状态空间和长期优化MDP问题时，传统策略评估方法的计算复杂度高，尤其在无限时域设置下。

Method: 结合Chiu的单输入分解和Robertazzi的单周期递归属性，定义了SISDMDP，并利用其结构开发了精确且高效的策略评估方法。

Result: 该方法能够有效降低计算复杂度，适用于平均和折扣奖励MDP的规模化求解。

Conclusion: 通过结构分解，论文提出了一种可扩展的MDP解决方案，为复杂决策问题提供了新的计算工具。

Abstract: Solving Markov Decision Processes (MDPs) remains a central challenge in
sequential decision-making, especially when dealing with large state spaces and
long-term optimization criteria. A key step in Bellman dynamic programming
algorithms is the policy evaluation, which becomes computationally demanding in
infinite-horizon settings such as average-reward or discounted-reward
formulations. In the context of Markov chains, aggregation and disaggregation
techniques have for a long time been used to reduce complexity by exploiting
structural decompositions. In this work, we extend these principles to a
structured class of MDPs. We define the Single-Input Superstate Decomposable
Markov Decision Process (SISDMDP), which combines Chiu's single-input
decomposition with Robertazzi's single-cycle recurrence property. When a policy
induces this structure, the resulting transition graph can be decomposed into
interacting components with centralized recurrence. We develop an exact and
efficient policy evaluation method based on this structure. This yields a
scalable solution applicable to both average and discounted reward MDPs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [93] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: DevFT是一种受认知发展启发的资源高效方法，通过分阶段的微调过程，逐步构建强大的大型语言模型，显著提高性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 为了解决联邦微调在边缘设备上资源消耗大的问题，同时保持数据隐私，提出了DevFT方法。

Method: DevFT将微调过程分解为多个发展阶段，每个阶段优化具有递增参数容量的子模型。通过知识迁移和优化的初始化参数，加速训练并避免局部最小值。

Result: DevFT在多个基准测试中显著优于现有方法，实现了更快的收敛速度、更低的通信开销和更高的性能。

Conclusion: DevFT不仅资源高效，还能与现有方法兼容，是一种有前景的联邦微调解决方案。

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [94] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: Adacc是一个新颖的内存管理框架，结合自适应压缩和激活检查点技术，减少GPU内存占用，提升大型语言模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中，重新计算会带来高达30%的开销，Adacc旨在通过优化内存管理减少这种开销。

Method: 包括三层设计：针对异常的层特定压缩算法、基于MILP的最优调度策略和自适应策略演化机制。

Result: 实验显示Adacc能加速训练1.01x至1.37x，同时保持与基线相当的模型精度。

Conclusion: Adacc有效解决了内存开销问题，显著提升了训练效率。

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [95] [Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience](https://arxiv.org/abs/2508.00596)
*Xiang Zhang,Zhou Li,Shuangyang Li,Kai Wan,Derrick Wing Kwan Ng,Giuseppe Caire*

Main category: cs.IT

TL;DR: 该论文研究了去中心化联邦学习中的安全聚合问题，从信息论角度探索了最小通信和密钥使用率，并建立了性能极限。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注协议设计和计算保证，但对信息论极限和去中心化环境下的通信与密钥使用最优界限了解不足。

Method: 通过分析$K$个全连接用户的安全聚合问题，要求每个用户不泄露除输入和之外的任何信息，并推导了最优速率区域。

Result: 每用户需传输至少一个符号、持有至少一个密钥符号，且所有用户总密钥量不少于$K - 1$个独立符号。

Conclusion: 结果为设计安全高效的分布式学习协议提供了理论依据。

Abstract: In decentralized federated learning (FL), multiple clients collaboratively
learn a shared machine learning (ML) model by leveraging their privately held
datasets distributed across the network, through interactive exchange of the
intermediate model updates. To ensure data security, cryptographic techniques
are commonly employed to protect model updates during aggregation. Despite
growing interest in secure aggregation, existing works predominantly focus on
protocol design and computational guarantees, with limited understanding of the
fundamental information-theoretic limits of such systems. Moreover, optimal
bounds on communication and key usage remain unknown in decentralized settings,
where no central aggregator is available. Motivated by these gaps, we study the
problem of decentralized secure aggregation (DSA) from an information-theoretic
perspective. Specifically, we consider a network of $K$ fully-connected users,
each holding a private input -- an abstraction of local training data -- who
aim to securely compute the sum of all inputs. The security constraint requires
that no user learns anything beyond the input sum, even when colluding with up
to $T$ other users. We characterize the optimal rate region, which specifies
the minimum achievable communication and secret key rates for DSA. In
particular, we show that to securely compute one symbol of the desired input
sum, each user must (i) transmit at least one symbol to others, (ii) hold at
least one symbol of secret key, and (iii) all users must collectively hold no
fewer than $K - 1$ independent key symbols. Our results establish the
fundamental performance limits of DSA, providing insights for the design of
provably secure and communication-efficient protocols in distributed learning
systems.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [96] [Dynamics and Coherence for the Free Cornering with Protocol Choice](https://arxiv.org/abs/2508.00633)
*Chad Nester,Niels Voorneveld*

Main category: math.CT

TL;DR: 本文提出了一个术语重写系统，用于模拟具有协议选择的自由角动的动态特性，并通过该系统证明了自由角动与协议选择的相干性定理。


<details>
  <summary>Details</summary>
Motivation: 研究自由角动与协议选择的动态特性，提供一个形式化的模型来理解过程交互的范畴行为。

Method: 开发了一个术语重写系统，该系统在适当的意义上是合流和终止的。

Result: 证明了自由角动与协议选择的相干性定理。

Conclusion: 术语重写系统为自由角动与协议选择提供了一个有效的模型，并验证了其相干性。

Abstract: We present a term rewriting system that models the dynamic aspects of the
free cornering with protocol choice of a monoidal category, which has been
proposed as a categorical model of process interaction. This term rewriting
system is confluent and terminating in an appropriate sense. We use this
machinery to prove a coherence theorem for the free cornering with protocol
choice.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [97] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

TL;DR: 论文提出了一种新框架，通过基于语言指令的视觉投影器混合和专家推荐策略，解决持续学习中视觉-语言模型忽视语言指令的问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的持续学习方法在更新视觉投影器时可能导致模型忽视语言指令，尤其是对重复类型的文本指令。为了解决这一问题，作者提出了新的框架。

Method: 使用混合视觉投影器，每个投影器作为基于指令上下文的专业视觉-语言翻译专家；引入专家推荐策略和专家剪枝技术以避免干扰。

Result: 在多样化的视觉-语言任务中，该方法优于现有持续学习方法，生成更符合指令的响应。

Conclusion: 提出的方法有效解决了视觉-语言模型中语言指令被忽视的问题，提升了持续学习能力。

Abstract: Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [98] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: 论文探讨了在虚拟化身中使用面部运动模式作为行为生物特征进行身份验证的可能性，提出了新的数据集和轻量级网络架构，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着逼真虚拟化身的普及，其带来的身份冒用风险日益严重。研究旨在验证面部运动模式能否作为可靠的行为生物特征来检测冒用行为。

Method: 通过GAGAvatar生成了真实与冒用的虚拟化身视频数据集，并提出了一种轻量级的时空图卷积网络架构，利用面部关键点建模动态表情。

Result: 实验表明，面部运动特征可实现有效的身份验证，AUC值接近80%。

Conclusion: 研究强调了在虚拟化身通信系统中加强行为生物特征防御的紧迫性，并提供了相关数据集和系统供社区研究。

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [99] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 论文提出AVR-Eval评价标准和AVR-Agent多智能体系统，用于评估和生成交互式多媒体内容，但发现当前模型难以有效利用高质量资产和反馈。


<details>
  <summary>Details</summary>
Motivation: 解决生成复杂交互式内容（如视频游戏）时自动化评估和质量提升的挑战。

Method: 提出AVR-Eval作为多媒体内容质量相对评价标准，并开发AVR-Agent多智能体系统，通过迭代优化生成JavaScript代码。

Result: AVR-Agent生成的内容在实验中表现优于一次性生成的内容，但模型未能有效利用自定义资产和反馈。

Conclusion: 揭示了人类与机器在内容创作方法上的根本差异，当前模型在利用高质量资产和反馈方面仍需改进。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [100] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出了一种基于HyperTWTL的安全强化学习方法（SecRL），用于在满足HyperTWTL约束的条件下学习安全感知的最优策略，并通过案例研究验证了其有效性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有许多关于时序逻辑约束的安全强化学习（SRL）的研究，但在利用超属性探索安全感知强化学习（RL）方面存在显著的研究空白。本文旨在填补这一空白。

Method: 提出了一种结合动态Boltzmann softmax强化学习和HyperTWTL约束的方法，用于在马尔可夫决策过程（MDP）框架下学习安全感知的最优策略。

Result: 通过一个拾取和交付机器人任务的案例研究，验证了所提方法的有效性和扩展性，并与其他两种基线强化学习算法相比表现更优。

Conclusion: 本文的方法为安全感知强化学习提供了新的解决方案，展示了超属性在机器人应用中的潜力。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [101] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: LLM代理在多个领域表现出强大的自主能力，但其随机性行为带来了难以预测的安全风险。现有规则系统反应滞后，提出Pro2Guard框架，通过概率可达性分析预测风险，提前干预，验证效果显著。


<details>
  <summary>Details</summary>
Motivation: LLM代理的随机行为存在安全隐患，现有规则系统缺乏前瞻性，难以处理长期依赖和分布偏移，需提出更高效的主动安全框架。

Method: Pro2Guard将代理行为抽象为符号状态，从执行轨迹学习DTMC模型，运行时通过概率预测不安全状态，结合语义检查和PAC保证，配置阈值提前干预。

Result: 在家庭代理任务中，Pro2Guard阻止了93.6%的不安全行为，保持80.4%的任务完成率；在自动驾驶中，100%预测违规和碰撞，提前38.66秒预警。

Conclusion: Pro2Guard通过主动风险预测有效提升LLM代理的安全性，平衡安全与任务完成率，适用于关键领域。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [102] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出了一种结合传统可解释AI技术与生成式AI模型的混合框架，旨在生成多模态且个性化的解释，以适应不同用户的需求。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的自适应学习系统缺乏透明度，且现有XAI技术忽视了用户角色和理解能力。

Method: 通过整合传统XAI技术、生成式AI模型和用户个性化，设计了一个动态的、用户为中心的通信框架。

Result: 提出了一个新的解释性框架，重新定义了解释性为动态通信过程，并指出了教育中XAI的局限性。

Conclusion: 目标是开发出既提升透明度又支持以用户为中心的体验的可解释AI。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [103] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出了一种用户分段的上下文感知解释系统，通过可视化方法提升社交媒体推荐的可解释性，适应不同用户需求。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体推荐的解释性普遍缺乏个性化，导致用户不理解推荐原因，降低其价值。

Method: 提出一种结合视觉与数字解释风格的框架，根据用户需求调整解释粒度（专家与普通用户），并在单一流程中实现。

Result: 将通过30名X用户的公开试点验证框架对决策和信任的影响。

Conclusion: 该系统首次在单一流程中联合调整解释风格和粒度，有望提升用户对推荐的理解和信任。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [104] [LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources](https://arxiv.org/abs/2508.00654)
*Rodrigo Escobar Díaz Guerrero,Jamile Mohammad Jafari,Tobias Meyer-Zedler,Michael Schmitt,Juergen Popp,Thomas Bocklitz*

Main category: cs.CE

TL;DR: LEO是一个基于网络的平台，用于连接和管理不同数据系统之间的联系，尤其在显微镜研究中整合分布式数据。


<details>
  <summary>Details</summary>
Motivation: 显微镜研究中，多个分散平台上的大量数据难以整合和链接，不符合FAIR数据管理原则，缺乏有效工具。

Method: 开发了LEO平台，通过插件架构链接电子实验室笔记本（ELNs）和OMERO等数据源。

Result: LEO成为可扩展且灵活的解决方案，适用于广泛的显微镜研究工作流。

Conclusion: LEO解决了数据整合的挑战，支持FAIR原则，提升了数据的可访问性和复用性。

Abstract: In the interdisciplinary field of microscopy research, managing and
integrating large volumes of data stored across disparate platforms remains a
major challenge. Data types such as bioimages, experimental records, and
spectral information are often maintained in separate repositories, each
following different management standards. However, linking these data sources
across the research lifecycle is essential to align with the FAIR principles of
data management: Findability, Accessibility, Interoperability, and Reusability.
Despite this need, there is a notable lack of tools capable of effectively
integrating and linking data from heterogeneous sources. To address this gap,
we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based
platform designed to create and manage links between distributed data systems.
LEO was initially developed to link objects between Electronic Lab Notebooks
(ELNs) and OMERO, but its functionality has since been extended through a
plugin-based architecture, allowing the integration of additional data sources.
This extensibility makes LEO a scalable and flexible solution for a wide range
of microscopy research workflows.

</details>


### [105] [Contact Sensors to Remote Cameras: Quantifying Cardiorespiratory Coupling in High-Altitude Exercise Recovery](https://arxiv.org/abs/2508.00773)
*Jiankai Tang,Meng Kang,Yiru Zhang,Kegang Wang,Daniel Mcduff,Xin Liu,Yuanchun Shi,Yuntao Wang*

Main category: cs.CE

TL;DR: 研究表明心肺耦合（CRC）可作为自主神经调节的敏感标志，并在非接触式监测中有潜力。高海拔环境下，恢复阶段的CRC同步更频繁但不稳定，且rPPG与血氧仪指标强相关。


<details>
  <summary>Details</summary>
Motivation: 探索心肺耦合在高海拔下的动态变化及其作为生理功能指标的潜力，同时验证非接触式测量的可行性。

Method: 在高海拔地区测量静息和运动恢复状态下的CRC，并利用远程光电容积描记术（rPPG）进行非接触式测量，与血氧仪数据对比。

Result: 恢复阶段的CRC同步频率更高但稳定性较低（p < 0.05）。rPPG与血氧仪数据的相关性极强（Pearson r = 0.96）。

Conclusion: CRC在高海拔环境中显示出动态变化特征，且非接触式rPPG测量方法可行，为未来无接触监测提供了可能。

Abstract: Cardiorespiratory coupling (CRC) captures the dynamic interaction between the
cardiac and respiratory systems--an interaction strengthened by physical
exercise and linked to improved physiological function. We examined CRC at high
altitude in two states, rest and post-exercise recovery, and found significant
differences (p < 0.05). Quantitative analysis revealed that recovery involved
more frequent yet less stable episodes of synchronization between respiration
and pulse. Furthermore, we explored the feasibility of non-contact CRC
measurement with remote photoplethysmography (rPPG), observing a strong
correlation with oximeter-based metrics (Pearson r = 0.96). These findings
highlight the potential of CRC as a sensitive marker for autonomic regulation
and its future application in contactless monitoring. Source code is available
at GitHub: https://github.com/McJackTang/CRC.

</details>
