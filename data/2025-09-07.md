<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LO](#cs.LO) [Total: 3]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Towards the Datasets Used in Requirements Engineering of Mobile Apps: Preliminary Findings from a Systematic Mapping Study](https://arxiv.org/abs/2509.03541)
*Chong Wang,Haoning Wu,Peng Liang,Maya Daneva,Marten van Sinderen*

Main category: cs.SE

TL;DR: 该论文通过系统映射研究发现，移动应用需求工程研究的数据集主要来自Google Play和Apple App Store，且集中在需求获取和分析活动上，提出了数据集来源多样化的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究移动应用需求工程中数据集的使用现状，揭示现有研究的局限性。

Method: 采用Kitchenham等的系统映射研究指南，分析了43篇相关论文。

Result: 发现90%以上的研究使用Google Play和Apple App Store数据集，且需求获取和分析活动为主。

Conclusion: 需扩展数据集来源和研究活动范围以提高结果的普适性。

Abstract: [Background] Research on requirements engineering (RE) for mobile apps
employs datasets formed by app users, developers or vendors. However, little is
known about the sources of these datasets in terms of platforms and the RE
activities that were researched with the help of the respective datasets.
[Aims] The goal of this paper is to investigate the state-of-the-art of the
datasets of mobile apps used in existing RE research. [Method] We carried out a
systematic mapping study by following the guidelines of Kitchenham et al.
[Results] Based on 43 selected papers, we found that Google Play and Apple App
Store provide the datasets for more than 90% of published research in RE for
mobile apps. We also found that the most investigated RE activities - based on
datasets, are requirements elicitation and requirements analysis. [Conclusions]
Our most important conclusions are: (1) there is a growth in the use of
datasets for RE research of mobile apps since 2012, (2) the RE knowledge for
mobile apps might be skewed due to the overuse of Google Play and Apple App
Store, (3) there are attempts to supplement reviews of apps from repositories
with other data sources, (4) there is a need to expand the alternative sources
and experiments with complimentary use of multiple sources, if the community
wants more generalizable results. Plus, it is expected to expand the research
on other RE activities, beyond elicitation and analysis.

</details>


### [2] [A Multi-stage Error Diagnosis for APB Transaction](https://arxiv.org/abs/2509.03554)
*Cheng-Yang Tsai,Tzu-Wei Huang,Jen-Wei Shih,I-Hsiang Wang,Yu-Cheng Lin,Rung-Bin Lin*

Main category: cs.SE

TL;DR: 本文提出了一种基于分层随机森林的自动化框架，用于高效检测SoC设计中的APB事务错误，实验结果显示91.36%的整体准确率。


<details>
  <summary>Details</summary>
Motivation: 现代SoC设计中，功能验证和调试是主要瓶颈，手动检测VCD文件中的APB事务错误效率低且易出错，亟需自动化解决方案。

Method: 采用分层随机森林架构，通过四个预训练的二分类器依次检测范围外访问、地址损坏和数据损坏错误，优先处理高确信度地址错误。

Result: 实验结果显示整体准确率为91.36%，地址错误检测近乎完美，数据错误表现稳健，团队在ICCAD 2025测试阶段获得第一名。

Conclusion: 分层机器学习在EDA硬件调试中具有强大潜力，验证了其作为自动化工具的有效性。

Abstract: Functional verification and debugging are critical bottlenecks in modern
System-on-Chip (SoC) design, with manual detection of Advanced Peripheral Bus
(APB) transaction errors in large Value Change Dump (VCD) files being
inefficient and error-prone. Addressing the 2025 ICCAD Contest Problem D, this
study proposes an automated error diagnosis framework using a hierarchical
Random Forest-based architecture. The multi-stage error diagnosis employs four
pre-trained binary classifiers to sequentially detect Out-of-Range Access,
Address Corruption, and Data Corruption errors, prioritizing high-certainty
address-related faults before tackling complex data errors to enhance
efficiency. Experimental results show an overall accuracy of 91.36%, with
near-perfect precision and recall for address errors and robust performance for
data errors. Although the final results of the ICCAD 2025 CAD Contest are yet
to be announced as of the submission date, our team achieved first place in the
beta stage, highlighting the method's competitive strength. This research
validates the potential of hierarchical machine learning as a powerful
automated tool for hardware debugging in Electronic Design Automation (EDA).

</details>


### [3] [Parse Tree Tracking Through Time for Programming Process Analysis at Scale](https://arxiv.org/abs/2509.03668)
*Matt Rau,Chris Brown,John Edwards*

Main category: cs.SE

TL;DR: 该论文提出了一种跟踪抽象语法树节点的算法，用于分析学生在编程过程中的行为，并揭示了一些新的统计发现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过跟踪抽象语法树节点来更深入地理解学生的编程行为，填补现有方法在高层次代码表示自动化跟踪方面的不足。

Method: 提出了两种算法，用于跟踪语法树节点并为不可解析的代码状态构建树表示，应用于公开的学生编程数据进行分析。

Result: 发现了一些新的统计现象，例如代码删除率在条件语句内外相似，以及学生代码跳转行为不一定反映困难。

Conclusion: 该研究为量化学生编程行为提供了新维度，有助于理解代码结构发展和学生编程困难。

Abstract: Background and Context: Programming process data can be utilized to
understand the processes students use to write computer programming
assignments. Keystroke- and line-level event logs have been used in the past in
various ways, primarily in high-level descriptive statistics (e.g., timings,
character deletion rate, etc). Analysis of behavior in context (e.g., how much
time students spend working on loops) has been cumbersome because of our
inability to automatically track high-level code representations, such as
abstract syntax trees, through time and unparseable states.
  Objective: Our study has two goals. The first is to design the first
algorithm that tracks parse tree nodes through time. Second, we utilize this
algorithm to perform a partial replication study of prior work that used manual
tracking of code representations, as well as other novel analyses of student
programming behavior that can now be done at scale.
  Method: We use two algorithms presented in this paper to track parse tree
nodes through time and construct tree representations for unparseable code
states. We apply these algorithms to a public keystroke data from student
coursework in a 2021 CS1 course and conduct analysis on the resulting parse
trees.
  Findings: We discover newly observable statistics at scale, including that
code is deleted at similar rates inside and outside of conditionals and loops,
a third of commented out code is eventually restored, and that frequency with
which students jump around in their code may not be indicative of struggle.
  Implications: The ability to track parse trees through time opens the door to
understanding new dimensions of student programming, such as best practices of
structural development of code over time, quantitative measurement of what
syntactic constructs students struggle most with, refactoring behavior, and
attention shifting within the code.

</details>


### [4] [Towards an Understanding of Developer Experience-Driven Transparency in Software Ecosystems](https://arxiv.org/abs/2509.03848)
*Rodrigo Oliveira Zacarias,Rodrigo Pereira dos Santos,Patricia Lago*

Main category: cs.SE

TL;DR: 该研究提出了SECO-TransDX模型，从开发者体验（DX）角度探讨软件生态系统（SECO）中的透明度问题，旨在系统性理解其对开发者感知和互动的影响。


<details>
  <summary>Details</summary>
Motivation: 透明度是软件生态系统中未被充分研究的关键因素，现有研究虽承认其对信任、公平和参与的重要性，但缺乏系统性概念化与开发者体验的关系。

Method: 研究提出了SECO-TransDX概念模型，基于先前研究并通过德尔菲法（Delphi study）与学术界和工业界专家共同完善。模型包含63个相关概念。

Result: SECO-TransDX模型揭示了透明度如何通过技术、社会和组织层面中介开发者体验，为研究和实践提供了结构化视角。

Conclusion: 该模型为未来研究奠定基础，同时帮助实践者设计更具透明度的开发者中心平台，促进长期参与。

Abstract: Software ecosystems (SECO) have become a dominant paradigm in the software
industry, enabling third-party developers to co-create value through
complementary components and services. While Developer Experience (DX) is
increasingly recognized as critical for sustainable SECO, transparency remains
an underexplored factor shaping how developers perceive and interact with
ecosystems. Existing studies acknowledge transparency as essential for trust,
fairness, and engagement, yet its relationship with DX has not been
systematically conceptualized. Hence, this work aims to advance the
understanding of transparency in SECO from a developer-centered perspective. To
this end, we propose SECO-TransDX (Transparency in Software Ecosystems from a
Developer Experience Perspective), a conceptual model that introduces the
notion of DX-driven transparency. The model identifies 63 interrelated
concepts, including conditioning factors, ecosystem procedures, artifacts, and
relational dynamics that influence how transparency is perceived and
constructed during developer interactions. SECO-TransDX was built upon prior
research and refined through a Delphi study with experts from academia and
industry. It offers a structured lens to examine how transparency mediates DX
across technical, social, and organizational layers. For researchers, it lays
the groundwork for future studies and tool development; for practitioners, it
supports the design of trustworthy, developer-centered platforms that improve
transparency and foster long-term engagement in SECO.

</details>


### [5] [Design and Development of a Web Platform for Blood Donation Management](https://arxiv.org/abs/2509.04423)
*Fatima Zulfiqar Ali,Atrooba Ilyas*

Main category: cs.SE

TL;DR: 开发了一个基于网络的献血平台，通过集中化管理连接患者、献血者和管理员，优化了献血服务的效率和及时性。


<details>
  <summary>Details</summary>
Motivation: 解决紧急情况下寻找合适献血者的挑战，提高献血服务的效率和可及性。

Method: 使用PHP (Laravel框架)、HTML、CSS、Bootstrap和MySQL等技术，通过用例、数据库、类和序列图设计开发平台。

Result: 平台实现了献血者的注册、患者的搜索功能，减少了紧急情况下的延误和复杂性。

Conclusion: 该平台通过数字化管理显著提高了献血服务的效率和及时性。

Abstract: Blood donation is a critical component of healthcare, yet locating suitable
donors in emergencies often presents significant challenges. This paper
presents the design and development of a Blood Donation Web Platform, a
web-based system that connects patients, donors, and administrators within a
centralized digital space. The platform allows interested donors to register
their personal information, including blood group, contact details, and
availability. Patients can search for donors based on blood group and location,
and the system provides a list of nearby donors who are ready to donate. The
platform design was guided by use case, database, class, and sequence diagrams
to ensure a well-structured and efficient system architecture. Modern web
technologies, including PHP (Laravel framework), HTML, CSS, Bootstrap, and
MySQL, supported by XAMPP and Visual Studio Code, were employed to implement a
dynamic, interactive, and user-friendly platform. By streamlining donor
refgistration, blood requests, and communication, the proposed system reduces
delays and complexities in emergencies, improving timely accessibility of blood
and enhancing overall efficiency in blood donation services.

</details>


### [6] [VulRTex: A Reasoning-Guided Approach to Identify Vulnerabilities from Rich-Text Issue Report](https://arxiv.org/abs/2509.03875)
*Ziyou Jiang,Mingyang Li,Guowei Yang,Lin Shi,Qing Wang*

Main category: cs.SE

TL;DR: 研究人员提出VulRTex，一种基于大语言模型（LLM）和丰富文本信息的推理指导方法，用于自动识别开源软件（OSS）中的漏洞相关报告（IR），显著提升性能并降低时间成本。


<details>
  <summary>Details</summary>
Motivation: 当前安全从业者需手动筛选漏洞相关IR，效率低且可能被攻击者利用。现有方法仅关注文本描述，忽视IR的丰富文本信息。

Method: VulRTex利用LLM的推理能力构建漏洞推理数据库，并通过历史案例生成推理指导，结合目标IR的丰富文本信息进行推理分析。

Result: 在973,572个IR上的实验显示，VulRTex在识别漏洞相关IR和预测CWE-ID上性能最佳，F1提升11.0%，AUPRC提升20.2%，时间成本降低50%。

Conclusion: VulRTex在真实场景中成功识别30个新兴漏洞，其中11个被分配CVE-ID，验证了其实用性和高效性。

Abstract: Software vulnerabilities exist in open-source software (OSS), and the
developers who discover these vulnerabilities may submit issue reports (IRs) to
describe their details. Security practitioners need to spend a lot of time
manually identifying vulnerability-related IRs from the community, and the time
gap may be exploited by attackers to harm the system. Previously, researchers
have proposed automatic approaches to facilitate identifying these
vulnerability-related IRs, but these works focus on textual descriptions but
lack the comprehensive analysis of IR's rich-text information. In this paper,
we propose VulRTex, a reasoning-guided approach to identify
vulnerability-related IRs with their rich-text information. In particular,
VulRTex first utilizes the reasoning ability of the Large Language Model (LLM)
to prepare the Vulnerability Reasoning Database with historical IRs. Then, it
retrieves the relevant cases from the prepared reasoning database to generate
reasoning guidance, which guides LLM to identify vulnerabilities by reasoning
analysis on target IRs' rich-text information. To evaluate the performance of
VulRTex, we conduct experiments on 973,572 IRs, and the results show that
VulRTex achieves the highest performance in identifying the
vulnerability-related IRs and predicting CWE-IDs when the dataset is
imbalanced, outperforming the best baseline with +11.0% F1, +20.2% AUPRC, and
+10.5% Macro-F1, and 2x lower time cost than baseline reasoning approaches.
Furthermore, VulRTex has been applied to identify 30 emerging vulnerabilities
across 10 representative OSS projects in 2024's GitHub IRs, and 11 of them are
successfully assigned CVE-IDs, which illustrates VulRTex's practicality.

</details>


### [7] [Vulnerability-Affected Versions Identification: How Far Are We?](https://arxiv.org/abs/2509.03876)
*Xingchu Chen,Chengwei Liu,Jialun Cao,Yang Xiao,Xinyue Cai,Yeting Li,Jingyi Shi,Tianqi Sun,Haiming Chen ang Wei Huo*

Main category: cs.SE

TL;DR: 本文首次全面研究了漏洞影响版本的识别问题，评估了12种工具的性能，发现其准确率普遍低于45.0%，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 识别受漏洞影响的软件版本对于补丁和风险管理至关重要，现有工具的评估范围有限，亟需更全面的研究。

Method: 研究通过整理1128个C/C++漏洞的高质量基准，从四个维度系统评估12种代表性工具的性能。

Result: 结果显示工具的最高准确率仅为45.0%，主要挑战来自启发式依赖、语义推理不足和匹配逻辑僵化。

Conclusion: 研究强调了开发新方法的必要性，并提供了改进工具开发和组合策略的实用建议。

Abstract: Identifying which software versions are affected by a vulnerability is
critical for patching, risk mitigation.Despite a growing body of tools, their
real-world effectiveness remains unclear due to narrow evaluation scopes often
limited to early SZZ variants, outdated techniques, and small or
coarse-graineddatasets. In this paper, we present the first comprehensive
empirical study of vulnerability affected versions identification. We curate a
high quality benchmark of 1,128 real-world C/C++ vulnerabilities and
systematically evaluate 12 representative tools from both tracing and matching
paradigms across four dimensions: effectiveness at both vulnerability and
version levels, root causes of false positives and negatives, sensitivity to
patch characteristics, and ensemble potential. Our findings reveal fundamental
limitations: no tool exceeds 45.0% accuracy, with key challenges stemming from
heuristic dependence, limited semantic reasoning, and rigid matching logic.
Patch structures such as add-only and cross-file changes further hinder
performance. Although ensemble strategies can improve results by up to 10.1%,
overall accuracy remains below 60.0%, highlighting the need for fundamentally
new approaches. Moreover, our study offers actionable insights to guide tool
development, combination strategies, and future research in this critical area.
Finally, we release the replicated code and benchmark on our website to
encourage future contributions.outdated techniques, and small or coarse grained
datasets.

</details>


### [8] [Analyzing Variations in Dependency Distributions Due to Code Smell Interactions](https://arxiv.org/abs/2509.03896)
*Zushuai Zhang,Elliott Wen,Ewan Tempero*

Main category: cs.SE

TL;DR: 研究发现代码异味之间的相互作用会增加模块间的依赖关系，开发者应优先解决相互作用的代码异味而非孤立存在的异味。


<details>
  <summary>Details</summary>
Motivation: 模块间的依赖关系会增加维护的复杂性和成本，因此有必要了解导致依赖增加的代码异味互动情况。

Method: 对116个开源Java系统进行依赖分析，量化代码异味之间的相互作用及其与静态依赖分布的关系。

Result: 代码异味对的交互会导致某些依赖增加（如Feature Envy与Data Class的依赖中位数从1增至7），总体上增加总依赖。

Conclusion: 开发者应优先处理相互作用的代码异味以减少模块间的依赖。

Abstract: The existence of dependencies between modules, such as classes, can mean that
changing a module triggers ripple effects that make maintenance complex and
costly, so the advice is to minimize dependencies between modules. It is
therefore important to understand the circumstances that can lead to increased
dependencies. Recent studies suggest that code smells, which are
characteristics of code that indicate potential design issues, may interact in
ways that increase dependencies between modules. In this study, we aim to
confirm previous observations and investigate whether and how the distribution
of static dependencies changes in the presence of code smell interactions. We
conducted a dependency analysis on 116 open-source Java systems to quantify the
interactions, comparing interactions among code smells and interactions between
code smells and non-code smells. Our results suggest that while interactions
between code smell pairs are associated with increases in certain dependencies
and decreases in others, overall, they are associated with an increase in total
dependencies. For example, the median number of dependencies between Feature
Envy methods and Data Classes is seven times as many as when the methods are
non-Feature Envy methods, increasing from 1 to 7. This implies that developers
should prioritize addressing code smells that interact with each other, rather
than code smells that exist only in isolation.

</details>


### [9] [The Auth Shim: A Lightweight Architectural Pattern for Integrating Enterprise SSO with Standalone Open-Source Applications](https://arxiv.org/abs/2509.03900)
*Yuvraj Agrawal*

Main category: cs.SE

TL;DR: 本文介绍了Auth Shim，一种轻量级架构模式，用于解决开源软件（OSS）在企业环境中缺乏原生SAML或OIDC协议支持的安全集成问题。


<details>
  <summary>Details</summary>
Motivation: 企业广泛采用开源软件，但这些工具通常缺乏对SAML或OIDC等协议的原生支持，导致安全集成缺口。

Method: Auth Shim是一种外部代理服务，作为兼容层，将企业身份提供商（IdP）的请求转换为目标应用的原生会话管理机制。目标应用需提供安全的编程管理API。

Result: 通过在Adobe实施该模式，成功将一款流行的开源BI工具与Okta SAML集成，实现了基于IAM组的自动化RBAC并消除了手动用户配置。

Conclusion: Auth Shim为将任何独立OSS工具集成到企业SSO生态系统中提供了可重复使用、安全且经济高效的蓝图，使企业能够在保障安全治理的同时拥抱开源创新。

Abstract: Open-source software OSS is widely adopted in enterprise settings, but
standalone tools often lack native support for protocols like SAML or OIDC,
creating a critical security integration gap. This paper introduces and
formalizes the Auth Shim, a lightweight architectural pattern designed to solve
this problem. The Auth Shim is a minimal, external proxy service that acts as a
compatibility layer, translating requests from an enterprise Identity Provider
IdP into the native session management mechanism of a target application. A key
prerequisite for this pattern is that the target application must expose a
programmatic, secure administrative API. We present a case study of the
pattern's implementation at Adobe to integrate a popular OSS BI tool with Okta
SAML, which enabled automated Role-Based Access Control RBAC via IAM group
mapping and eliminated manual user provisioning. By defining its components,
interactions, and production deployment considerations, this paper provides a
reusable, secure, and cost-effective blueprint for integrating any standalone
OSS tool into an enterprise SSO ecosystem, thereby enabling organizations to
embrace open-source innovation without compromising on security governance.

</details>


### [10] [RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models](https://arxiv.org/abs/2509.04078)
*Jingjing Liu,Zeming Liu,Zihao Cheng,Mengliang He,Xiaoming Shi,Yuhang Guo,Xiangrong Zhu,Yuanfang Guo,Yunhong Wang,Haifeng Wang*

Main category: cs.SE

TL;DR: 该论文介绍了RepoDebug，一个多任务、多语言的仓库级代码调试数据集，旨在解决现有数据集在任务、语言和错误类型多样性上的不足，并评估了10个大型语言模型在仓库级调试中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注函数级代码修复，忽略了更复杂和现实的仓库级场景，导致对大型语言模型在仓库级调试中的挑战理解不完整。

Method: 提出了一个名为RepoDebug的数据集，支持8种编程语言、3种调试任务和22种错误子类型。

Result: 评估实验显示，即使是表现最好的模型Claude 3.5 Sonnect，在仓库级调试中仍表现不佳。

Conclusion: RepoDebug填补了仓库级调试数据集的空白，但大型语言模型在这一领域的表现仍有提升空间。

Abstract: Large Language Models (LLMs) have exhibited significant proficiency in code
debugging, especially in automatic program repair, which may substantially
reduce the time consumption of developers and enhance their efficiency.
Significant advancements in debugging datasets have been made to promote the
development of code debugging. However, these datasets primarily focus on
assessing the LLM's function-level code repair capabilities, neglecting the
more complex and realistic repository-level scenarios, which leads to an
incomplete understanding of the LLM's challenges in repository-level debugging.
While several repository-level datasets have been proposed, they often suffer
from limitations such as limited diversity of tasks, languages, and error
types. To mitigate this challenge, this paper introduces RepoDebug, a
multi-task and multi-language repository-level code debugging dataset with 22
subtypes of errors that supports 8 commonly used programming languages and 3
debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,
where Claude 3.5 Sonnect, the best-performing model, still cannot perform well
in repository-level debugging.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [When Lifetimes Liberate: A Type System for Arenas with Higher-Order Reachability Tracking](https://arxiv.org/abs/2509.04253)
*Siyuan He,Songlin Jia,Yuyan Bao,Tiark Rompf*

Main category: cs.PL

TL;DR: 该论文提出了一种统一静态资源管理的方法，结合区域系统和所有权类型的优点，支持灵活的资源共享和静态生命周期控制。


<details>
  <summary>Details</summary>
Motivation: 解决高阶函数语言中静态资源管理的难题，结合区域系统和所有权类型的优势，提供灵活的资源管理方式。

Method: 提出了两种新的扩展：A<:和{A}<:，分别支持粗粒度可达性跟踪和词法生命周期控制，并在Rocq中形式化验证。

Result: 实现了静态安全保障，同时保留了语言的表达能力和简单性。

Conclusion: 该方法成功统一了区域系统和所有权类型的优势，为高阶语言中的资源管理提供了灵活且安全的解决方案。

Abstract: Static resource management in higher-order functional languages remains
elusive due to tensions between control, expressiveness, and flexibility.
Region-based systems [Grossman et al. 2002; Tofte et al. 2001] offer control
over lifetimes and expressive in-region sharing, but restrict resources to
lexical scopes. Rust, an instance of ownership types [Clarke et al. 2013],
offers non-lexical lifetimes and robust safety guarantees, yet its global
invariants make common sharing patterns hard to express. Reachability types
[Wei et al. 2024] enable reasoning about sharing and separation, but lack
practical tools for controlling resource lifetimes.
  In this work, we try to unify their strengths. Our solution enables grouping
resources as arenas for arbitrary sharing and static guarantees of lexically
scoped lifetimes. Crucially, arenas and lexical lifetimes are not the only
choice: users may also manage resources individually, with non-lexical
lifetimes. Regardless of mode, resources share the same type, preserving the
higher-order parametric nature of the language.
  Obtaining static safety guarantee in a higher-order language with flexible
sharing is nontrivial. To this end, we propose two new extensions atop
reachability types [Wei et al. 2024]. First, A<: features a novel
two-dimensional store model to enable coarse-grained reachability tracking for
arbitrarily shared resources within arenas. Building on this, {A}<: establishes
lexical lifetime control with static guarantees. As the first reachability
formalism presented for lifetime control, {A}<: avoids the complication of
flow-sensitive reasoning and retains expressive power and simplicity. Both
calculi are formalized and proven type safe in Rocq.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [12] [Towards Deterministic Sub-0.5 us Response on Linux through Interrupt Isolation](https://arxiv.org/abs/2509.03855)
*Zhouyi Zhou,Zhili Liu,Shancong Zhang,Jiemin Li,Dengke Du,Mengke Sun,Zhiqiang Wang,Hongyan Liu,Guogai Xu*

Main category: cs.OS

TL;DR: 论文提出了一种中断隔离方法，通过集中和最小化计时器中断干扰，显著降低抖动和响应延迟，实验证明其能在ARM多核平台上实现低于0.5微秒的响应时间。


<details>
  <summary>Details</summary>
Motivation: Linux系统的实时响应能力常受中断冲突和计时器处理开销的限制，难以实现亚微秒级延迟。

Method: 设计了一种中断隔离方法，通过专用API选择性调用计时器处理程序并抑制非关键的跨处理器中断。

Result: 在ARM多核平台上实验表明，该机制能稳定实现低于0.5微秒的响应时间，优于传统的Linux PREEMPT-RT配置。

Conclusion: 中断隔离是一种轻量且有效的策略，适用于通用操作系统中实现确定性实时工作负载的潜在方案。

Abstract: Real-time responsiveness in Linux is often constrained by interrupt
contention and timer handling overhead, making it challenging to achieve
sub-microsecond latency. This work introduces an interrupt isolation approach
that centralizes and minimizes timer interrupt interference across CPU cores.
By enabling a dedicated API to selectively invoke timer handling routines and
suppress non-critical inter-processor interrupts, our design significantly
reduces jitter and response latency. Experiments conducted on an ARM-based
multicore platform demonstrate that the proposed mechanism consistently
achieves sub-0.5 us response times, outperforming conventional Linux PREEMPT-RT
configurations. These results highlight the potential of interrupt isolation as
a lightweight and effective strategy for deterministic real-time workloads in
general-purpose operating systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [Entanglement Purification With Finite Latency Classical Communication in Quantum Networks](https://arxiv.org/abs/2509.03667)
*Vivek Vasan,Alexander Nico-Katz,Boulat A. Bash,Daniel C. Kilper,Marco Ruffini*

Main category: cs.NI

TL;DR: 论文研究了在IP通信网络中非即时经典协调下，纠缠纯化协议（BBPSSW、DEJMPS）的实际可行性，评估了它们在不同网络条件和量子存储技术下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决量子网络中纠缠对在存储期间由于环境退相干导致保真度下降的问题，同时克服经典通信延迟带来的额外退相干挑战。

Method: 采用微观Lindblad方法分析量子动力学，结合当前城域IP网络延迟统计和量子存储测试平台参数，评估协议性能，并确定成功与失败的区域。

Result: 确定了纠缠纯化成功的临界保真度等值线，计算了完成多轮纯化所需的纠缠对数量，以及满足应用特定阈值的高保真纠缠对的稳态吞吐量。

Conclusion: 为在当前和未来网络中部署纠缠纯化提供了延迟预算、存储质量目标和资源开销估计。

Abstract: Quantum networks rely on high fidelity entangled pairs distributed to nodes,
but maintaining their fidelity is challenged by environmental decoherence
during storage. Entanglement purification is used to restore fidelity, but the
idle periods imposed by the associated classical communication delays
counteract this goal by exposing the states to further decoherence. In this
work, we analyze the practical viability of entanglement purification protocols
(BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet
protocol (IP) communications networks. We present a comprehensive performance
evaluation of these protocols in various network conditions for a range of
quantum memory technologies. We employ a microscopic Lindblad treatment of the
underlying quantum dynamics, and use current-generation metropolitan IP network
latency statistics and parameters drawn from quantum memory testbeds. In doing
so we identify the regions in which entanglement purification succeeds and
fails, delineated by break-even iso-fidelity contours in the phase space. We
then determine the total number of entangled pairs required to complete a
multi-round purification protocol, and the steady-state throughput of entangled
pairs with purified fidelities that exceed application-specific thresholds.
This provides latency budgets, memory quality targets, and resource-overhead
estimates for deploying purification on current and near-future networks.

</details>


### [14] [Drift Plus Optimistic Penalty -- A Learning Framework for Stochastic Network Optimization](https://arxiv.org/abs/2509.03762)
*Sathwik Chadaga,Eytan Modiano*

Main category: cs.NI

TL;DR: 该论文研究了队列网络中路由和调度的联合优化问题，其中边缘传输成本未知。控制器需通过噪声观测动态调整决策，以最小化总期望成本，并确保网络稳定性。


<details>
  <summary>Details</summary>
Motivation: 在队列网络中，未知的边缘传输成本和网络稳定性需求增加了路由和调度问题的复杂性，需要同时优化吞吐量和成本。

Method: 结合Lyapunov漂移加惩罚优化和多臂老虎机技术，提出了一种网络控制策略。

Result: 策略实现了相对于完全了解到达和成本的最佳策略的$O(\sqrt{T}\log T)$次线性遗憾。

Conclusion: 仿真验证了该策略的有效性，其遗憾确实为次线性。

Abstract: We consider the problem of joint routing and scheduling in queueing networks,
where the edge transmission costs are unknown. At each time-slot, the network
controller receives noisy observations of transmission costs only for those
edges it selects for transmission. The network controller's objective is to
make routing and scheduling decisions so that the total expected cost is
minimized. This problem exhibits an exploration-exploitation trade-off,
however, previous bandit-style solutions cannot be directly applied to this
problem due to the queueing dynamics. In order to ensure network stability, the
network controller needs to optimize throughput and cost simultaneously. We
show that the best achievable cost is lower bounded by the solution to a static
optimization problem, and develop a network control policy using techniques
from Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show
that the policy achieves a sub-linear regret of order $O(\sqrt{T}\log T)$, as
compared to the best policy that has complete knowledge of arrivals and costs.
Finally, we evaluate the proposed policy using simulations and show that its
regret is indeed sub-linear.

</details>


### [15] [A Versatile and Programmable UAV Platform for Radio Access Network and End-to-End Cellular Measurements](https://arxiv.org/abs/2509.03818)
*Sherwan Jalal Abdullah,Sravan Reddy Chintareddy,Victor S. Frost,Shawn Keshmiri,Morteza Hashemi*

Main category: cs.NI

TL;DR: 本文开发了一种基于无人机的移动网络性能测量平台，适用于传统方法难以覆盖的地区，通过空中操作和数据分析提供网络性能指标。


<details>
  <summary>Details</summary>
Motivation: 传统的人群众包方法在人口稀少和地形复杂的农村地区效果不佳，需要一种更高效、安全的网络测量解决方案。

Method: 平台整合了机载计算单元和商用蜂窝调制解调器，通过无人机收集RAN信号和端到端网络性能数据，并利用地理空间映射和统计分析展示结果。

Result: 实验表明，高海拔下信号功率因视线条件改善而增强，但信号质量因邻近小区干扰而下降；尽管如此，系统在大多数区域保持了可接受的信号质量和吞吐量。

Conclusion: 无人机平台为农村和复杂地形区域的网络性能测量提供了有效工具，但信号强度与覆盖范围并非完全相关。

Abstract: In this work, we develop a measurement platform to capture mobile network
performance metrics including coverage and quality of service in regions where
conventional coverage testing approaches are frequently time-intensive,
labor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing
methods are used to collect cellular network performance metrics. However,
these approaches are inadequate in rural areas due to low-density population,
and difficult terrain. The platform described here is a UAV-based and is
designed to investigate the mobile network performance through aerial
operations and gather Radio Access Network (RAN) signal alongside end-to-end
network performance metrics. Our platform gathers metrics through the
integration of an onboard computation unit and commercial off-the-shelf
cellular modem. The gathered data are subsequently analyzed and displayed using
geospatial mapping utilities and statistical techniques to deliver key
observations on cellular network performance. Experimental results showed that
the received signal power improves at higher altitudes due to enhanced
line-of-sight (LoS) conditions as expected. However, the signal quality
degrades as a result of increased interference from neighboring cells. The
analysis reveals that for most of the geographic area covered in the initial
experiments the system maintained acceptable signal quality, with adequate
throughput performance for both uplink and downlink communications, while
maintaining satisfactory round-trip time characteristics. Notably, the
experiment showed that a strong radio signal metric for a given cell does not
necessarily translate to consistent spatial coverage across the tested region.

</details>


### [16] [Indoor Positioning with Wi-Fi Location: A Survey of IEEE 802.11mc/az/bk Fine Timing Measurement Research](https://arxiv.org/abs/2509.03901)
*Katarzyna Kosek-Szott,Szymon Szott,Wojciech Ciezobka,Maksymilian Wojnar,Krzysztof Rusek,Jonathan Segev*

Main category: cs.NI

TL;DR: 本文填补了关于IEEE 802.11mc FTM协议及其最新增强技术在室内定位中应用的文献综述空白，分类并回顾了180多篇相关研究论文。


<details>
  <summary>Details</summary>
Motivation: 室内定位技术在家庭、办公室和工业网络中具有广泛的应用需求，IEEE 802.11mc FTM协议因其高精度和设备支持显示出巨大潜力，但其相关综述仍缺乏。

Method: 通过分类和回顾180多篇研究论文，分析FTM在实际精度、机器学习改进方法、与其他系统的结合、应用及安全问题等方面的研究成果。

Result: 总结了最重要的研究成果，并提出了未解决的问题和未来的研究方向。

Conclusion: 该综述为FTM在室内定位中的应用提供了全面参考，并指出了进一步研究的开放领域。

Abstract: Indoor positioning is an enabling technology for home, office, and industrial
network users because it provides numerous information and communication
technology (ICT) and Internet of things (IoT) functionalities such as indoor
navigation, smart meter localization, asset tracking, support for emergency
services, and detection of hazardous situations. The IEEE 802.11mc fine timing
measurement (FTM) protocol (commercially known as Wi-Fi Location) has great
potential to enable indoor positioning in future generation devices, primarily
because of the high availability of Wi-Fi networks, FTM's high accuracy and
device support. Furthermore, new FTM enhancements are available in the released
(802.11az) and recently completed (802.11bk) amendments. Despite the multitude
of literature reviews on indoor positioning, a survey dedicated to FTM and its
recent enhancements has so far been lacking. We fill this gap by classifying
and reviewing over 180 research papers related to the practical accuracy
achieved with FTM, methods for improving its accuracy (also with machine
learning), combining FTM with other indoor positioning systems, FTM-based
applications, and security issues. Based on the conducted survey, we summarize
the most important research achievements and formulate open areas for further
research.

</details>


### [17] [Autonomous Task Offloading of Vehicular Edge Computing with Parallel Computation Queues](https://arxiv.org/abs/2509.03935)
*Sungho Cho,Sung Il Choi,Seung Hyun Oh,Ian P. Roberts,Sang Hyun Lee*

Main category: cs.NI

TL;DR: 该论文提出了一种车联网边缘计算（VEC）中的并行任务卸载策略，旨在通过资源协作平衡减少用户等待延迟，理论与数值验证表明其全局最优性。


<details>
  <summary>Details</summary>
Motivation: 现有车联网边缘计算中，任务卸载常因资源利用不均或负载拥塞导致延迟，需一种高效策略以优化性能。

Method: 基于网络协作平衡资源利用与负载，预测服务器瞬时处理能力并分析队列离散变量，以实现精确估计和任务分配。

Result: 理论与数值验证证明，该方案在延迟减少上优于现有方法，真实地图虚拟环境测试也验证了其可行性。

Conclusion: 通过预测服务器处理能力和精确队列分析，该策略能有效识别过载服务器并优化延迟，为车联网边缘计算提供了高效解决方案。

Abstract: This work considers a parallel task execution strategy in vehicular edge
computing (VEC) networks, where edge servers are deployed along the roadside to
process offloaded computational tasks of vehicular users. To minimize the
overall waiting delay among vehicular users, a novel task offloading solution
is implemented based on the network cooperation balancing resource
under-utilization and load congestion. Dual evaluation through theoretical and
numerical ways shows that the developed solution achieves a globally optimal
delay reduction performance compared to existing methods, which is also
approved by the feasibility test over a real-map virtual environment. The
in-depth analysis reveals that predicting the instantaneous processing power of
edge servers facilitates the identification of overloaded servers, which is
critical for determining network delay. By considering discrete variables of
the queue, the proposed technique's precise estimation can effectively address
these combinatorial challenges to achieve optimal performance.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [18] [A Cegar-centric Bounded Reachability Analysis for Compositional Affine Hybrid Systems](https://arxiv.org/abs/2509.03560)
*Atanu Kundu,Pratyay Sarkar,Rajarshi Ray*

Main category: cs.LO

TL;DR: 提出了一种基于CEGAR的组合混合系统有界可达性分析算法，避免显式计算产品自动机，提升了效率。


<details>
  <summary>Details</summary>
Motivation: 解决组合混合系统可达性分析中的语义保留和状态空间爆炸问题。

Method: 结合CEGAR原则，在离散抽象中搜索反例并通过符号可达性分析验证；优化包括缓存结果和混合不同组合语义。

Result: 在工具SAT-Reach中实现了算法，展示了可扩展性优势。

Conclusion: 提出的算法高效且可扩展，适用于组合混合系统的可达性分析。

Abstract: Reachability analysis of compositional hybrid systems, where individual
components are modeled as hybrid automata, poses unique challenges. In addition
to preserving the compositional semantics while computing system behaviors,
algorithms have to cater to the explosion in the number of locations in the
parallel product automaton. In this paper, we propose a bounded reachability
analysis algorithm for compositional hybrid systems with piecewise affine
dynamics, based on the principle of counterexample guided abstraction
refinement (CEGAR). In particular, the algorithm searches for a counterexample
in the discrete abstraction of the composition model, without explicitly
computing a product automaton. When a counterexample is discovered in the
abstraction, its validity is verified by a refinement of the state-space guided
by the abstract counterexample. The state-space refinement is through a
symbolic reachability analysis, particularly using a state-of-the-art algorithm
with support functions as the continuous state representation. In addition, the
algorithm mixes different semantics of composition with the objective of
improved efficiency. Step compositional semantics is followed while exploring
the abstract (discrete) state-space, while shallow compositional semantics is
followed during state-space refinement with symbolic reachability analysis.
Optimizations such as caching the results of the symbolic reachability
analysis, which can be later reused, have been proposed. We implement this
algorithm in the tool SAT-Reach and demonstrate the scalability benefits.

</details>


### [19] [Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on Controllers in Reactive Synthesis](https://arxiv.org/abs/2509.04129)
*Mickael Randour*

Main category: cs.LO

TL;DR: 该论文探讨了控制器合成中策略的复杂性，讨论了不同合成环境下记忆和随机性的影响。


<details>
  <summary>Details</summary>
Motivation: 研究控制器合成中简单策略的优势，以及策略复杂性对实际控制器设计和维护的影响。

Method: 采用博弈论方法，分析系统与环境交互时的策略复杂性问题。

Result: 总结了关于记忆和随机性在策略中作用的最新研究成果。

Conclusion: 强调了对策略复杂性传统概念的进一步研究需求。

Abstract: In the game-theoretic approach to controller synthesis, we model the
interaction between a system to be controlled and its environment as a game
between these entities, and we seek an appropriate (e.g., winning or optimal)
strategy for the system. This strategy then serves as a formal blueprint for a
real-world controller. A common belief is that simple (e.g., using limited
memory) strategies are better: corresponding controllers are easier to conceive
and understand, and cheaper to produce and maintain.
  This invited contribution focuses on the complexity of strategies in a
variety of synthesis contexts. We discuss recent results concerning memory and
randomness, and take a brief look at what lies beyond our traditional notions
of complexity for strategies.

</details>


### [20] [Janus-faces of temporal constraint languages: a dichotomy of expressivity](https://arxiv.org/abs/2509.04347)
*Johanna Brunar,Michael Pinsker,Moritz Schöbi*

Main category: cs.LO

TL;DR: 该论文揭示了时间约束语言在多项式时间内可解的情况下，其表达能力极其有限，并提出了新的代数结果和一致性证明。


<details>
  <summary>Details</summary>
Motivation: 研究时间约束语言的复杂性分类，特别关注其在可解情况下的表达能力和代数特性。

Method: 通过分析时间约束语言的pp-解释能力，证明了其表达能力的局限性，并利用伪Siggers多态性等工具进行研究。

Result: 发现这些语言不仅表达能力有限，还支持4元伪Siggers多态性，为更广泛的Bodirsky-Pinsker猜想提供了支持。

Conclusion: 研究结果为无限域约束满足问题的复杂性分类提供了新的见解和潜在的统一证明方法。

Abstract: The Bodirsky-K\'ara classification of temporal constraint languages stands as
one of the earliest and most seminal complexity classifications within
infinite-domain Constraint Satisfaction Problems (CSPs), yet it remains one of
the most mysterious in terms of algorithms and algebraic invariants for the
tractable cases. We show that those temporal languages which do not
pp-construct EVERYTHING (and thus by the classification are solvable in
polynomial time) have, in fact, very limited expressive power as measured by
the graphs and hypergraphs they can pp-interpret. This limitation yields many
previously unknown algebraic consequences, while also providing new, uniform
proofs for known invariance properties. In particular, we show that such
temporal constraint languages admit $4$-ary pseudo-Siggers polymorphisms -- a
result that sustains the possibility that the existence of such polymorphisms
extends to the much broader context of the Bodirsky-Pinsker conjecture.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [21] [Promisedland: An XR Narrative Attraction Integrating Diorama-to-Virtual Workflow and Elemental Storytelling](https://arxiv.org/abs/2509.03678)
*Xianghan Wang,Chingshuan Hsiao,Shimei Qiu*

Main category: cs.HC

TL;DR: Promisedland是一个混合现实（MR）叙事体验项目，结合了文化叙事、生态教育及创新的混合生产流程，通过交互式旅程恢复元素平衡。


<details>
  <summary>Details</summary>
Motivation: 旨在通过混合现实技术提供一个沉浸式的叙事体验，结合文化和生态教育，同时展示低成本、高保真的原型设计方法。

Method: 采用Diorama-to-Virtual流程，包括手工制作物理模型、3D扫描后集成到Unreal Engine，结合Stewart平台提供运动反馈。

Result: 最终原型在Meta Quest上运行，支持动态交互和实时视觉反馈，提升了沉浸感和情感参与度。

Conclusion: Promisedland为未来XR叙事装置提供了可复制的设计蓝图，提出了物理与数字元素融合的新框架。

Abstract: Promisedland is a mixed-reality (MR) narrative attraction that combines
cultural storytelling, ecological education, and an innovative hybrid
production workflow. Set in a future Earth suffering from elemental imbalance,
users embark on an interactive journey guided by symbolic characters to restore
harmony through the collection of five classical elements: metal, wood, water,
fire, and earth. To prototype this experience, we introduce a low-cost,
high-fidelity Diorama-to-Virtual pipeline - handcrafting physical scale models,
3D scanning, and integrating them into Unreal Engine. This process enables
rapid spatial prototyping while preserving the material expressiveness and
narrative consistency of the physical environment. To further enhance
immersion, the experience incorporates a Stewart Platform to provide motion
feedback synchronized with the virtual ride dynamics, reinforcing spatial
presence and embodied engagement. The final prototype runs on Meta Quest,
supporting dynamic interactions and real-time visual feedback. Promisedland
offers a replicable design blueprint for future XR narrative installations
across museums, cultural exhibitions, and themed entertainment. It proposes a
new framework for XR Narrative Attractions - where physical and digital
elements converge to deepen immersion, agency, and emotional engagement.

</details>


### [22] [Designing Effective AI Explanations for Misinformation Detection: A Comparative Study of Content, Social, and Combined Explanations](https://arxiv.org/abs/2509.03693)
*Yeaeun Gong,Yifan Liu,Lanyu Shang,Na Wei,Dong Wang*

Main category: cs.HC

TL;DR: 研究AI解释对用户识别虚假信息的帮助，探讨了内容解释和社会解释的效果及其组合的影响，结果表明解释类型和顺序对检测准确性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI（XAI）方法主要关注内容解释，忽略了社会背景，因此研究其他解释形式（如社会解释及其组合）的效果。

Method: 通过在线众包实验（COVID-19和政治领域）比较内容解释、社会解释及其组合的效果。

Result: AI解释能有效帮助用户识别虚假信息，效果受内容与社会解释的一致性及呈现顺序影响。

Conclusion: 研究为设计更有效的AI解释提供了指导，强调了社会背景与内容结合的重要性。

Abstract: In this paper, we study the problem of AI explanation of misinformation,
where the goal is to identify explanation designs that help improve users'
misinformation detection abilities and their overall user experiences. Our work
is motivated by the limitations of current Explainable AI (XAI) approaches,
which predominantly focus on content explanations that elucidate the linguistic
features and sentence structures of the misinformation. To address this
limitation, we explore various explanations beyond content explanation, such as
"social explanation" that considers the broader social context surrounding
misinformation, as well as a "combined explanation" where both the content and
social explanations are presented in scenarios that are either aligned or
misaligned with each other. To evaluate the comparative effectiveness of these
AI explanations, we conduct two online crowdsourcing experiments in the
COVID-19 (Study 1 on Prolific) and Politics domains (Study 2 on MTurk). Our
results show that AI explanations are generally effective in aiding users to
detect misinformation, with effectiveness significantly influenced by the
alignment between content and social explanations. We also find that the order
in which explanation types are presented - specifically, whether a content or
social explanation comes first - can influence detection accuracy, with
differences found between the COVID-19 and Political domains. This work
contributes towards more effective design of AI explanations, fostering a
deeper understanding of how different explanation types and their combinations
influence misinformation detection.

</details>


### [23] [Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support](https://arxiv.org/abs/2509.03741)
*Eduardo Davalos,Yike Zhang,Shruti Jain,Namrata Srivastava,Trieu Truong,Nafees-ul Haque,Tristan Van,Jorge Salas,Sara McFadden,Sun-Joo Cho,Gautam Biswas,Amanda Goodwin*

Main category: cs.HC

TL;DR: 论文提出了一种基于眼动数据的学习分析仪，通过用户中心设计和数据叙事原则，帮助学生和教师更好地理解学习过程。


<details>
  <summary>Details</summary>
Motivation: 眼动数据在教学中未被充分利用，存在数据解释和可访问性挑战，论文旨在解决这些问题。

Method: 通过五项研究，结合教师和学生反馈，设计并评估了一个支持英语语言艺术的仪表盘，利用可视化、分层解释和LLM驱动的对话代理。

Result: 研究发现，结合熟悉的可视化和自然语言交互的眼动分析可以提高数据的可理解和教学价值。

Conclusion: 未来教育技术系统应整合新颖数据模态，并注重用户友好的交互设计。

Abstract: Eye-tracking offers rich insights into student cognition and engagement, but
remains underutilized in classroom-facing educational technology due to
challenges in data interpretation and accessibility. In this paper, we present
the iterative design and evaluation of a gaze-based learning analytics
dashboard for English Language Arts (ELA), developed through five studies
involving teachers and students. Guided by user-centered design and data
storytelling principles, we explored how gaze data can support reflection,
formative assessment, and instructional decision-making. Our findings
demonstrate that gaze analytics can be approachable and pedagogically valuable
when supported by familiar visualizations, layered explanations, and narrative
scaffolds. We further show how a conversational agent, powered by a large
language model (LLM), can lower cognitive barriers to interpreting gaze data by
enabling natural language interactions with multimodal learning analytics. We
conclude with design implications for future EdTech systems that aim to
integrate novel data modalities in classroom contexts.

</details>


### [24] [Map as a By-product: Collective Landmark Mapping from IMU Data and User-provided Texts in Situated Tasks](https://arxiv.org/abs/2509.03792)
*Ryo Yonetani,Kotaro Hara*

Main category: cs.HC

TL;DR: Collective Landmark Mapper利用智能手机IMU数据和用户文本输入生成语义地标地图，适用于零售和非零售场景。


<details>
  <summary>Details</summary>
Motivation: 解决用户在室内导航和任务中记录地标的需求，生成统一的语义地图，满足零售和非零售应用的实际需求。

Method: 利用智能手机IMU数据和用户自由文本输入识别地标，并通过多用户数据聚合生成统一地图。

Result: 用户研究表明系统可行且在零售和非零售应用中表现优越。

Conclusion: 该系统具有广泛的应用潜力，尤其在零售环境中能有效生成语义地标地图。

Abstract: This paper presents Collective Landmark Mapper, a novel map-as-a-by-product
system for generating semantic landmark maps of indoor environments. Consider
users engaged in situated tasks that require them to navigate these
environments and regularly take notes on their smartphones. Collective Landmark
Mapper exploits the smartphone's IMU data and the user's free text input during
these tasks to identify a set of landmarks encountered by the user. The
identified landmarks are then aggregated across multiple users to generate a
unified map representing the positions and semantic information of all
landmarks. In developing the proposed system, we focused specifically on retail
applications and conducted a formative interview with stakeholders to confirm
their practical needs that motivate the map-as-a-byproduct approach. Our user
study demonstrates the feasibility of the proposed system and its superior
mapping performance in two different setups: creating a product availability
map from restocking checklist tasks at a retail store and constructing a room
usage map from office inspection tasks, further demonstrating the potential
applicability to non-retail applications.

</details>


### [25] [Exploring the Integration of Extended Reality and Artificial Intelligence (AI) for Remote STEM Education and Assessment](https://arxiv.org/abs/2509.03812)
*Shadeeb Hossain,Natalie Sommer,Neda Adib*

Main category: cs.HC

TL;DR: 本文提出了一种动态游戏化架构，用于扩展现实人工智能虚拟培训环境，旨在通过沉浸式、自适应和动觉学习增强STEM教育。系统分为四个阶段引入，并讨论了安全性和隐私保护措施。


<details>
  <summary>Details</summary>
Motivation: 通过沉浸式和自适应的学习方式提升STEM教育效果，同时解决安全和隐私问题。

Method: 提出四阶段动态游戏化架构，采用分层安全措施（AES 256加密、多因素认证等）。

Result: 识别了潜在风险并提出了缓解策略，同时讨论了大规模采用的障碍。

Conclusion: 该系统展示了提升STEM教育的潜力，但需克服技术和成本等挑战。

Abstract: This paper presents a dynamic gamification architecture for an Extended
Reality Artificial Intelligence virtual training environment designed to
enhance STEM education through immersive adaptive, and kinesthetic learning.
The proposed system can be introduced in four phases: Introduction Phase,
Component Development Phase, Fault Introduction and Correction Phase and
Generative AI XR scenarios Phase. Security and privacy are discussed via a
defense-in-depth approach spanning client, middleware, and backend layers,
incorporating AES 256 encryption, multi-factor authentication, role-based
access control and GDPR or FERPA compliance. Risks such as sensor exploitation,
perceptual manipulation, and virtual physical harm are identified, with
mitigation strategies embedded at the design stage. Potential barriers to large
scale adoption-including technical complexity, cost of deployment, and need for
cybersecurity expertise are discussed.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [26] [LuxDiT: Lighting Estimation with Video Diffusion Transformer](https://arxiv.org/abs/2509.03680)
*Ruofan Liang,Kai He,Zan Gojcic,Igor Gilitschenski,Sanja Fidler,Nandita Vijaykumar,Zian Wang*

Main category: cs.GR

TL;DR: LuxDiT通过微调视频扩散Transformer生成HDR环境光贴图，解决了从单图像或视频估计光照的难题，并在合成和真实场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决从单图像或视频估计光照的挑战，克服现有方法因缺乏多样化和昂贵的真实HDR数据而受限的问题。

Method: 使用视频扩散Transformer进行微调，结合合成数据集和低秩适应策略，预测HDR环境光贴图。

Result: LuxDiT在定量和定性评估中优于现有技术，生成的光照预测具有真实的高频细节。

Conclusion: LuxDiT展示了数据驱动方法在光照估计任务中的潜力，为计算机视觉和图形学提供了有效解决方案。

Abstract: Estimating scene lighting from a single image or video remains a longstanding
challenge in computer vision and graphics. Learning-based approaches are
constrained by the scarcity of ground-truth HDR environment maps, which are
expensive to capture and limited in diversity. While recent generative models
offer strong priors for image synthesis, lighting estimation remains difficult
due to its reliance on indirect visual cues, the need to infer global
(non-local) context, and the recovery of high-dynamic-range outputs. We propose
LuxDiT, a novel data-driven approach that fine-tunes a video diffusion
transformer to generate HDR environment maps conditioned on visual input.
Trained on a large synthetic dataset with diverse lighting conditions, our
model learns to infer illumination from indirect visual cues and generalizes
effectively to real-world scenes. To improve semantic alignment between the
input and the predicted environment map, we introduce a low-rank adaptation
finetuning strategy using a collected dataset of HDR panoramas. Our method
produces accurate lighting predictions with realistic angular high-frequency
details, outperforming existing state-of-the-art techniques in both
quantitative and qualitative evaluations.

</details>


### [27] [Memory Optimization for Convex Hull Support Point Queries](https://arxiv.org/abs/2509.03753)
*Michael Greer*

Main category: cs.GR

TL;DR: 本文评估了几种凸包内存布局改进方案，以提升支持点查询的计算速度。


<details>
  <summary>Details</summary>
Motivation: 支持点查询是常见碰撞算法的核心部分，本文旨在通过优化内存布局来提高其计算效率。

Method: 研究提出了几种凸包内存布局的改进方案，以优化支持点查询的性能。

Result: 结果表明，改进方案显著提升了计算速度，具体效果取决于凸包的顶点数量。

Conclusion: 通过优化凸包的内存布局，可以有效加速支持点查询，从而提升碰撞算法的整体效率。

Abstract: This paper evaluates several improvements to the memory layout of convex
hulls to improve computation times for support point queries. The support point
query is a fundamental part of common collision algorithms, and the work
presented achieves a significant speedup depending on the number of vertices of
the convex hull.

</details>


### [28] [ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast, Memory-Efficient Reconstruction](https://arxiv.org/abs/2509.03775)
*Sankeerth Durvasula,Sharanshangar Muhunthan,Zain Moustafa,Richard Chen,Ruofan Liang,Yushi Guan,Nilesh Ahuja,Nilesh Jain,Selvakumar Panneer,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: 3D高斯泼溅（3DGS）是一种高质量的实时渲染技术，但高模型质量需要大量3D高斯分布，导致GPU内存消耗大、训练速度慢。ContraGS通过代码簿压缩存储高斯参数向量，解决了直接训练压缩表示的难题，显著降低内存使用并加速训练和渲染。


<details>
  <summary>Details</summary>
Motivation: 高内存消耗和低效的数据移动限制了3DGS在实际应用中的可行性，尤其是在资源有限的设备上。

Method: ContraGS利用代码簿紧凑存储高斯参数，并通过贝叶斯推断和非可微分参数的马尔可夫链蒙特卡罗采样来训练压缩表示。

Result: ContraGS平均减少3.49倍峰值内存，训练加速1.36倍，渲染加速1.88倍，同时保持接近SOTA的质量。

Conclusion: ContraGS为3DGS模型提供了一种高效的内存优化方法，解决了压缩表示直接训练的难题。

Abstract: 3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world
scenes with high quality and real-time rendering. Typically, a higher quality
representation can be achieved by using a large number of 3D Gaussians.
However, using large 3D Gaussian counts significantly increases the GPU device
memory for storing model parameters. A large model thus requires powerful GPUs
with high memory capacities for training and has slower training/rendering
latencies due to the inefficiencies of memory access and data movement. In this
work, we introduce ContraGS, a method to enable training directly on compressed
3DGS representations without reducing the Gaussian Counts, and thus with a
little loss in model quality. ContraGS leverages codebooks to compactly store a
set of Gaussian parameter vectors throughout the training process, thereby
significantly reducing memory consumption. While codebooks have been
demonstrated to be highly effective at compressing fully trained 3DGS models,
directly training using codebook representations is an unsolved challenge.
ContraGS solves the problem of learning non-differentiable parameters in
codebook-compressed representations by posing parameter estimation as a
Bayesian inference problem. To this end, ContraGS provides a framework that
effectively uses MCMC sampling to sample over a posterior distribution of these
compressed representations. With ContraGS, we demonstrate that ContraGS
significantly reduces the peak memory during training (on average 3.49X) and
accelerated training and rendering (1.36X and 1.88X on average, respectively),
while retraining close to state-of-art quality.

</details>


### [29] [TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface Scattering for Perlin Distributed Heterogeneous Media](https://arxiv.org/abs/2509.04047)
*Ashish Tiwari,Satyam Bhardwaj,Yash Bachwana,Parag Sarvoday Sahu,T. M. Feroz Ali,Bhargava Chintalapati,Shanmuganathan Raman*

Main category: cs.GR

TL;DR: 该论文提出了一种基于学习的方法TensoIS，用于从稀疏多视角图像中估计异质散射参数，并创建了合成数据集HeteroSynth。


<details>
  <summary>Details</summary>
Motivation: 解决异质介质散射参数估计这一严重受限且具有挑战性的问题，利用Perlin噪声模拟真实世界中的异质性。

Method: 通过Fractal Perlin噪声创建合成数据集HeteroSynth，并提出基于学习的TensoIS框架，用低秩张量分量表示散射体积。

Result: TensoIS在HeteroSynth测试集、烟雾和云几何体以及真实样本中表现出有效性。

Conclusion: 该方法探索了Perlin噪声分布，为真实世界中的异质散射参数估计提供了潜在解决方案。

Abstract: Estimating scattering parameters of heterogeneous media from images is a
severely under-constrained and challenging problem. Most of the existing
approaches model BSSRDF either through an analysis-by-synthesis approach,
approximating complex path integrals, or using differentiable volume rendering
techniques to account for heterogeneity. However, only a few studies have
applied learning-based methods to estimate subsurface scattering parameters,
but they assume homogeneous media. Interestingly, no specific distribution is
known to us that can explicitly model the heterogeneous scattering parameters
in the real world. Notably, procedural noise models such as Perlin and Fractal
Perlin noise have been effective in representing intricate heterogeneities of
natural, organic, and inorganic surfaces. Leveraging this, we first create
HeteroSynth, a synthetic dataset comprising photorealistic images of
heterogeneous media whose scattering parameters are modeled using Fractal
Perlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a
learning-based feed-forward framework to estimate these Perlin-distributed
heterogeneous scattering parameters from sparse multi-view image observations.
Instead of directly predicting the 3D scattering parameter volume, TensoIS uses
learnable low-rank tensor components to represent the scattering volume. We
evaluate TensoIS on unseen heterogeneous variations over shapes from the
HeteroSynth test set, smoke and cloud geometries obtained from open-source
realistic volumetric simulations, and some real-world samples to establish its
effectiveness for inverse scattering. Overall, this study is an attempt to
explore Perlin noise distribution, given the lack of any such well-defined
distribution in literature, to potentially model real-world heterogeneous
scattering in a feed-forward manner.

</details>


### [30] [SMooGPT: Stylized Motion Generation using Large Language Models](https://arxiv.org/abs/2509.04058)
*Lei Zhong,Yi Yang,Changjian Li*

Main category: cs.GR

TL;DR: 论文提出了一种基于LLM的新方法SMooGPT，通过身体部分文本空间生成风格化动作，解决了现有方法在可解释性、控制和泛化性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有风格化动作生成方法存在可解释性差、控制有限、泛化性不足等问题，尤其是在公开数据集偏好的动作（如行走）之外表现不佳。基于人类动作可通过自然语言描述、LLM对动作的理解能力及动作的组合性特点，提出了新的解决方案。

Method: 采用推理-组合-生成的框架，利用身体部分文本空间作为中间表示，通过微调的LLM（SMooGPT）作为推理器、组合器和生成器，生成风格化动作。

Result: 实验和用户感知研究表明，该方法在纯文本驱动的风格化动作生成中表现优异，具有更高的可解释性、细粒度控制能力和对新风格的泛化能力。

Conclusion: 通过LLM结合身体部分文本空间的表示，显著提升了风格化动作生成的可控性、泛化性和多样性。

Abstract: Stylized motion generation is actively studied in computer graphics,
especially benefiting from the rapid advances in diffusion models. The goal of
this task is to produce a novel motion respecting both the motion content and
the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing
research attempts to address this problem via motion style transfer or
conditional motion generation. They typically embed the motion style into a
latent space and guide the motion implicitly in a latent space as well. Despite
the progress, their methods suffer from low interpretability and control,
limited generalization to new styles, and fail to produce motions other than
``walking'' due to the strong bias in the public stylization dataset. In this
paper, we propose to solve the stylized motion generation problem from a new
perspective of reasoning-composition-generation, based on our observations: i)
human motion can often be effectively described using natural language in a
body-part centric manner, ii) LLMs exhibit a strong ability to understand and
reason about human motion, and iii) human motion has an inherently
compositional nature, facilitating the new motion content or style generation
via effective recomposing. We thus propose utilizing body-part text space as an
intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a
reasoner, composer, and generator when generating the desired stylized motion.
Our method executes in the body-part text space with much higher
interpretability, enabling fine-grained motion control, effectively resolving
potential conflicts between motion content and style, and generalizes well to
new styles thanks to the open-vocabulary ability of LLMs. Comprehensive
experiments and evaluations, and a user perceptual study, demonstrate the
effectiveness of our approach, especially under the pure text-driven stylized
motion generation.

</details>


### [31] [Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network Weight Space Diffusion](https://arxiv.org/abs/2509.04145)
*Dongliang Cao,Guoxing Sun,Marc Habermann,Florian Bernard*

Main category: cs.GR

TL;DR: 提出一种结合个性化渲染与扩散生成模型的新方法，以高真实感和姿态相关变形生成动态人像。


<details>
  <summary>Details</summary>
Motivation: 现有方法在泛化性和渲染质量上存在局限，无法同时实现高真实感和跨身份的动态人像生成。

Method: 采用两阶段流程：优化个性化UNet捕捉姿态细节，再训练超扩散模型生成网络权重。

Result: 在大规模多视角数据集上验证了方法的优越性。

Conclusion: 通过结合两种模型的优势，实现了高质量的动态人像生成。

Abstract: Creating human avatars is a highly desirable yet challenging task. Recent
advancements in radiance field rendering have achieved unprecedented
photorealism and real-time performance for personalized dynamic human avatars.
However, these approaches are typically limited to person-specific rendering
models trained on multi-view video data for a single individual, limiting their
ability to generalize across different identities. On the other hand,
generative approaches leveraging prior knowledge from pre-trained 2D diffusion
models can produce cartoonish, static human avatars, which are animated through
simple skeleton-based articulation. Therefore, the avatars generated by these
methods suffer from lower rendering quality compared to person-specific
rendering methods and fail to capture pose-dependent deformations such as cloth
wrinkles. In this paper, we propose a novel approach that unites the strengths
of person-specific rendering and diffusion-based generative modeling to enable
dynamic human avatar generation with both high photorealism and realistic
pose-dependent deformations. Our method follows a two-stage pipeline: first, we
optimize a set of person-specific UNets, with each network representing a
dynamic human avatar that captures intricate pose-dependent deformations. In
the second stage, we train a hyper diffusion model over the optimized network
weights. During inference, our method generates network weights for real-time,
controllable rendering of dynamic human avatars. Using a large-scale,
cross-identity, multi-view video dataset, we demonstrate that our approach
outperforms state-of-the-art human avatar generation methods.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [Trustworthy Second-hand Marketplace for Built Environment](https://arxiv.org/abs/2509.04085)
*Stanly Wilson,Kwabena Adu-Duodu,Yinhao Li,Ringo Sham,Yingli Wang,Ellis Solaiman,Charith Perera,Rajiv Ranjan,Omer Rana*

Main category: cs.DC

TL;DR: 本文提出了一种基于区块链的数字市场框架，旨在通过透明和可追溯的机制促进可持续建筑材料的再利用，解决了工业自动化和循环供应链中的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 建筑行业在材料浪费和可持续实践方面面临重大挑战，需要创新的解决方案来整合自动化、可追溯性和分散决策以实现高效的材料再利用。

Method: 采用区块链技术和IPFS（星际文件系统）构建了一个数字市场框架，确保材料交换的透明性和可追溯性。

Result: 开发的框架展示了市场的操作流程，证明了其实际应用和有效性，能够高效且可信地促进可再利用材料的交换。

Conclusion: 该数字市场框架为更可持续的建筑实践提供了重要支持，改善了材料再利用的效率与信任问题。

Abstract: The construction industry faces significant challenges regarding material
waste and sustainable practices, necessitating innovative solutions that
integrate automation, traceability, and decentralised decision-making to enable
efficient material reuse. This paper presents a blockchain-enabled digital
marketplace for sustainable construction material reuse, ensuring transparency
and traceability using InterPlanetary File System (IPFS). The proposed
framework enhances trust and accountability in material exchange, addressing
key challenges in industrial automation and circular supply chains. A framework
has been developed to demonstrate the operational processes of the marketplace,
illustrating its practical application and effectiveness. Our contributions
show how the marketplace can facilitate the efficient and trustworthy exchange
of reusable materials, representing a substantial step towards more sustainable
construction practices.

</details>


### [33] [Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software](https://arxiv.org/abs/2509.03653)
*Siddharth Samsi,Dan Campbell,Emanuel Scoullos,Oded Green*

Main category: cs.DC

TL;DR: HPEC Graph Challenge的新基准测试利用GraphBLAS和数据科学语言，展示了通过开源工具（如NVIDIA RAPIDS）实现高效加速，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统HPC基准测试（如LINPACK）无法覆盖复杂工作负载，新基准测试Anonymized Network Sensing Graph Challenge需要更多软件组件和端到端工作流。

Method: 使用GraphBLAS和数据科学语言重新解释问题，利用NVIDIA RAPIDS生态系统中的现成ETL工具（如cuDF和cupy）实现加速。

Result: 在NVIDIA A100、H100和H200 GPU上分别实现了147x-509x、243x-1269x和332x-2185x的加速。

Conclusion: 开源企业工具能在无需专用HPC代码的情况下显著提升性能，为新基准测试提供了高效解决方案。

Abstract: The HPEC Graph Challenge is a collection of benchmarks representing complex
workloads that test the hardware and software components of HPC systems, which
traditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph
Isomorphism, focused on several compute-bound and memory-bound kernels. The
most recent of the challenges, the Anonymized Network Sensing Graph Challenge,
represents a shift in direction, as it represents a longer end-to-end workload
that requires many more software components, including, but not limited to,
data I/O, data structures for representing graph data, and a wide range of
functions for data preparation and network analysis. A notable feature of this
new graph challenge is the use of GraphBLAS to represent the computational
aspects of the problem statement. In this paper, we show an alternative
interpretation of the GraphBLAS formulations using the language of data
science. With this formulation, we show that the new graph challenge can be
implemented using off-the-shelf ETL tools available in open-source, enterprise
software such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software,
RAPIDS cuDF and cupy, we enable significant software acceleration without
requiring any specific HPC code and show speedups, over the same code running
with Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an
NVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.

</details>


### [34] [Distributed Download from an External Data Source in Asynchronous Faulty Settings](https://arxiv.org/abs/2509.03755)
*John Augustine,Soumyottam Chatterjee,Valerie King,Manish Kumar,Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 该论文研究了异步通信网络中的分布式数据检索问题，重点关注在崩溃和拜占庭故障模型下的查询复杂度优化。


<details>
  <summary>Details</summary>
Motivation: 扩展分布式数据检索的研究至异步网络，解决现有同步网络研究的局限性。

Method: 提出确定性解决方案以容忍崩溃故障，并在拜占庭故障模型中分析随机协议的查询复杂度。

Result: 在崩溃故障模型中，提出最优查询复杂度的解决方案；在拜占庭故障模型中，展示了随机协议的潜力。

Conclusion: 该研究首次在异步网络中解决了数据检索问题，为未来分布式系统设计提供了新方向。

Abstract: The distributedData Retrieval (DR) model consists of $k$ peers connected by a
complete peer-to-peer communication network, and a trusted external data source
that stores an array $\textbf{X}$ of $n$ bits ($n \gg k$). Up to $\beta k$ of
the peers might fail in any execution (for $\beta \in [0, 1)$). Peers can
obtain the information either by inexpensive messages passed among themselves
or through expensive queries to the source array $\textbf{X}$. In the DR model,
we focus on designing protocols that minimize the number of queries performed
by any nonfaulty peer (a measure referred to as query complexity) while
maximizing the resilience parameter $\beta$.
  The Download problem requires each nonfaulty peer to correctly learn the
entire array $\textbf{X}$. Earlier work on this problem focused on synchronous
communication networks and established several deterministic and randomized
upper and lower bounds. Our work is the first to extend the study of
distributed data retrieval to asynchronous communication networks. We address
the Download problem under both the Byzantine and crash failure models. We
present query-optimal deterministic solutions in an asynchronous model that can
tolerate any fixed fraction $\beta<1$ of crash faults. In the Byzantine failure
model, it is known that deterministic protocols incur a query complexity of
$\Omega(n)$ per peer, even under synchrony. We extend this lower bound to
randomized protocols in the asynchronous model for $\beta \geq 1/2$, and
further show that for $\beta < 1/2$, a randomized protocol exists with
near-optimal query complexity. To the best of our knowledge, this is the first
work to address the Download problem in asynchronous communication networks.

</details>


### [35] [Gathering of asynchronous robots on circle with limited visibility using finite communication](https://arxiv.org/abs/2509.04004)
*Avisek Sharma,Satakshi Ghosh,Buddhadeb Sau*

Main category: cs.DC

TL;DR: 本文提出了一种解决有限可见性（π-可见性）机器人聚集问题的算法，机器人具有有限通信能力（FCOM），运动是非刚性的，且在全异步调度器下工作。


<details>
  <summary>Details</summary>
Motivation: 针对有限可见性机器人难以在圆上聚集的问题，过去研究主要集中在刚性运动和特定调度条件下。本文旨在放宽这些限制，探索非刚性运动和全异步条件下的解决方案。

Method: 提出了一种算法，适用于具有有限通信能力（FCOM）的机器人，在π-可见性模型下运行，采用非刚性运动和全异步调度器。

Result: 成功解决了π-可见性模型下的聚集问题，机器人能够在非刚性运动和全异步条件下完成任务。

Conclusion: 本研究扩展了有限可见性机器人聚集问题的解决范围，为非刚性和全异步条件下的类似问题提供了新的思路。

Abstract: This work addresses the gathering problem for a set of autonomous, anonymous,
and homogeneous robots with limited visibility operating in a continuous
circle. The robots are initially placed at distinct positions, forming a
rotationally asymmetric configuration. The robots agree on the clockwise
direction. In the $\theta$-visibility model, a robot can only see those robots
on the circle that are at an angular distance $<\theta$ from it. Di Luna
\textit{et. al.} [DISC'20] have shown that, in $\pi/2$ visibility, gathering is
impossible. In addition, they provided an algorithm for robots with $\pi$
visibility, operating under a semi-synchronous scheduler. In the $\pi$
visibility model, only one point, the point at the angular distance $\pi$ is
removed from the visibility. Ghosh \textit{et. al.} [SSS'23] provided a
gathering algorithm for $\pi$ visibility model with robot having finite memory
($\mathcal{FSTA}$), operating under a special asynchronous scheduler.
  If the robots can see all points on the circle, then the gathering can be
done by electing a leader in the weakest robot model under a fully asynchronous
scheduler. However, previous works have shown that even the removal of one
point from the visibility makes gathering difficult. In both works, the robots
had rigid movement. In this work, we propose an algorithm that solves the
gathering problem under the $\pi$-visibility model for robots that have finite
communication ability ($\mathcal{FCOM}$). In this work the robot movement is
non-rigid and the robots work under a fully asynchronous scheduler.

</details>


### [36] [Counterfactual simulations for large scale systems with burnout variables](https://arxiv.org/abs/2509.04038)
*Benjamin Heymann*

Main category: cs.DC

TL;DR: 论文提出了一种基于不确定性松弛的新算法，用于高效并行计算，显著提高了具有“burnout变量”的系统在反事实估计中的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在大规模系统中，burnout变量的存在使得反事实分析的计算成本高昂，尤其是在需要顺序处理的场景下，如在线广告中的预算管理。

Method: 引入了不确定性松弛算法，通过并行计算来提高效率。

Result: 该方法显著提升了计算的可扩展性，适用于具有burnout变量的系统。

Conclusion: 不确定性松弛算法为解决反事实分析中的计算瓶颈提供了有效的解决方案。

Abstract: We consider large-scale systems influenced by burnout variables - state
variables that start active, shape dynamics, and irreversibly deactivate once
certain conditions are met. Simulating what-if scenarios in such systems is
computationally demanding, as alternative trajectories often require sequential
processing, which does not scale very well. This challenge arises in settings
like online advertising, because of campaigns budgets, complicating
counterfactual analysis despite rich data availability. We introduce a new type
of algorithms based on what we refer to as uncertainty relaxation, that enables
efficient parallel computation, significantly improving scalability for
counterfactual estimation in systems with burnout variables.

</details>


### [37] [LowDiff: Efficient Frequent Checkpointing via Low-Cost Differential for High-Performance Distributed Training Systems](https://arxiv.org/abs/2509.04084)
*Chenxuan Yao,Yuchong Hu,Feifan Liu,Zhengyu Liu,Dan Feng*

Main category: cs.DC

TL;DR: LowDiff是一种高效的频繁检查点框架，通过重用压缩梯度作为差分检查点降低成本，并结合批量梯度写入优化，动态调整检查点频率和批量大小以最大化性能。


<details>
  <summary>Details</summary>
Motivation: 当前分布式训练中的频繁检查点生成大量检查点，导致成本高昂且性能下降，而差分检查点方法仅适用于推荐系统，未能广泛用于通用分布式训练系统。

Method: 提出了LowDiff框架，通过重用压缩梯度作为差分检查点，并结合批量梯度写入优化和动态调整策略。还引入了分层梯度重用和CPU异步持久化策略。

Result: 实验表明，LowDiff能在每次迭代中实现检查点，运行时开销低于3.1%。

Conclusion: LowDiff是一种高效的频繁检查点解决方案，适用于广泛的工作负载，显著降低了成本并提升了性能。

Abstract: Distributed training of large deep-learning models often leads to failures,
so checkpointing is commonly employed for recovery. State-of-the-art studies
focus on frequent checkpointing for fast recovery from failures. However, it
generates numerous checkpoints, incurring substantial costs and thus degrading
training performance. Recently, differential checkpointing has been proposed to
reduce costs, but it is limited to recommendation systems, so its application
to general distributed training systems remains unexplored.
  This paper proposes LowDiff, an efficient frequent checkpointing framework
that \textit{reuses} compressed gradients, serving as differential checkpoints
to reduce cost. Furthermore, LowDiff incorporates a batched gradient write
optimization to persist these differentials to storage efficiently. It also
dynamically tunes both the checkpoint frequency and the batching size to
maximize performance. We further enhance LowDiff with a layer-wise gradient
reusing and snapshotting approach and a CPU-based asynchronous persistence
strategy, enabling frequent checkpointing without gradient compression.
Experiments on various workloads show that LowDiff can achieve checkpointing
frequency up to per iteration with less than 3.1\% runtime overhead.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [38] [Hardware-Aware Data and Instruction Mapping for AI Tasks: Balancing Parallelism, I/O and Memory Tradeoffs](https://arxiv.org/abs/2509.03846)
*Md Rownak Hossain Chowdhury,Mostafizur Rahman*

Main category: cs.AR

TL;DR: 提出一种深度学习推理映射框架，通过预测神经网络行为提前规划计算和通信，减少主机干预和片外内存使用。


<details>
  <summary>Details</summary>
Motivation: 为了解决深度学习推理中对主机控制、I/O和片外内存的依赖问题，提出一种通过预测行为提前规划的框架。

Method: 框架生成统一指令和数据流，利用消息传递架构实现数据局部性和计算协调，如权重复用和多播技术。

Result: 在VGG-19上，框架保持88-92%的高利用率，97%的消息内部生成，计算吞吐量超1TFLOP/s。

Conclusion: 该框架通过流式计算和紧密协调数据与指令流，显著提升了深度学习的执行效率。

Abstract: We introduce a mapping framework for deep learning inference that takes
advantage of predictable neural network behavior to plan both computation and
communication ahead of time. The framework generates a unified stream of
instructions and data, enabling the hardware to execute operations and route
information on its own, without frequent involvement from the host and with
minimal off-chip memory use. This naturally reduces reliance on I/O, off-chip
memory, and host control. By leveraging fine-grained message passing on a
programmable, message-based compute architecture, the framework keeps data
movement local and coordinates computation across the array using techniques
such as stationary-weight reuse, in-array multicasting, and staged reductions.
Applied to VGG-19, the framework sustains high utilization (88 to 92 percent),
with over 97 percent of messages generated internally and nearly 89 percent of
time consumed on-chip transfers. Computation throughput scales beyond 1 TFLOP/s
on larger arrays, while traffic reductions from reuse and local aggregation
reach up to 100 MB per layer. Overall, the results highlight the effectiveness
of streaming-based computation and show how our mapper enables this execution
style by tightly coordinating data and instruction flow across the hardware.

</details>


### [39] [Real Time FPGA Based CNNs for Detection, Classification, and Tracking in Autonomous Systems: State of the Art Designs and Optimizations](https://arxiv.org/abs/2509.04153)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 该论文综述了FPGA上部署CNN用于实时视觉任务的最新进展，涵盖算法、硬件加速及优化策略，并探讨了FPGA平台和开发工具，为高效边缘计算提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶、机器人等实时视觉应用需求的增长，FPGA因其低功耗和确定性延迟成为GPU和ASIC的有力替代，亟需系统性的技术总结。

Method: 通过分析FPGA上的CNN实现技术（如剪枝、量化等）、硬件平台（如SoC FPGA、ACAPs）及开发工具（如Vitis AI），并探讨混合架构（GPU+FPGA）和硬件-软件协同设计。

Result: 总结了FPGA部署CNN的关键技术与优化策略，为资源受限设备的实时推理提供了高效解决方案，支持边缘应用的开发。

Conclusion: 该综述为研究者提供了FPGA部署高效视觉系统的全面指南，推动下一代低功耗高性能边缘计算的发展。

Abstract: This paper presents a comprehensive review of recent advances in deploying
convolutional neural networks (CNNs) for object detection, classification, and
tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand
for real-time computer vision applications in domains such as autonomous
vehicles, robotics, and surveillance, FPGAs have emerged as a powerful
alternative to GPUs and ASICs due to their reconfigurability, low power
consumption, and deterministic latency. We critically examine state-of-the-art
FPGA implementations of CNN-based vision tasks, covering algorithmic
innovations, hardware acceleration techniques, and the integration of
optimization strategies like pruning, quantization, and sparsity-aware methods
to maximize performance within hardware constraints. This survey also explores
the landscape of modern FPGA platforms, including classical LUT-DSP based
architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration
Platforms (ACAPs), comparing their capabilities in handling deep learning
workloads. Furthermore, we review available software development tools such as
Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the
design and deployment of AI models on FPGAs. The paper uniquely discusses
hybrid architecture that combine GPUs and FPGAs for collaborative acceleration
of AI inference, addressing challenges related to energy efficiency and
throughput. Additionally, we highlight hardware-software co-design practices,
dataflow optimizations, and pipelined processing techniques essential for
real-time inference on resource-constrained devices. Through this survey,
researchers and engineers are equipped with insights to develop
next-generation, power-efficient, and high-performance vision systems optimized
for FPGA deployment in edge and embedded applications.

</details>


### [40] [Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations](https://arxiv.org/abs/2509.04162)
*Safa Mohammed Sali,Mahmoud Meribout,Ashiyana Abdul Majeed*

Main category: cs.AR

TL;DR: 论文综述了FPGA在Transformer和视觉语言模型推理中的设计权衡、优化策略及实现挑战，探讨了硬件-算法协同设计的新趋势，旨在提高部署效率和适应性。


<details>
  <summary>Details</summary>
Motivation: Transformer和视觉语言模型的高计算复杂度和内存需求，使其在低延迟和低功耗环境中的部署面临挑战。FPGA因其可重构性和高效并行性成为理想解决方案。

Method: 通过分析设备类别选择、内存约束、数据流编排、量化策略等关键因素，结合硬件-算法协同设计，探索FPGA的高效推理实现。

Result: 论文提供了全面的技术基础，并展望了可扩展、可移植的FPGA解决方案，以支持不断演进的模型架构。

Conclusion: FPGA在Transformer和视觉语言模型的高效部署中展现出潜力，未来需解决标准基准缺失和运行时灵活性等问题。

Abstract: Transformers and vision-language models (VLMs) have emerged as dominant
architectures in computer vision and multimodal AI, offering state-of-the-art
performance in tasks such as image classification, object detection, visual
question answering, and caption generation. However, their high computational
complexity, large memory footprints, and irregular data access patterns present
significant challenges for deployment in latency- and power-constrained
environments. Field-programmable gate arrays (FPGAs) provide an attractive
hardware platform for such workloads due to their reconfigurability,
fine-grained parallelism, and potential for energy-efficient acceleration. This
paper presents a comprehensive review of design trade-offs, optimization
strategies, and implementation challenges for FPGA-based inference of
transformers and VLMs. We examine critical factors such as device-class
selection, memory subsystem constraints, dataflow orchestration, quantization
strategies, sparsity exploitation, and toolchain choices, alongside
modality-specific issues unique to VLMs, including heterogeneous compute
balancing and cross-attention memory management. Additionally, we discuss
emerging trends in hardware-algorithm co-design, highlighting innovations in
attention mechanisms, compression, and modular overlays to improve efficiency
and adaptability. Practical issues such as runtime flexibility, verification
overhead, and the absence of standardized FPGA multimodal benchmarks are also
considered. Finally, we outline future directions toward scalable, portable,
and reconfigurable FPGA solutions that adapt to evolving model architectures
while sustaining high utilization and predictable performance. This synthesis
offers both a technical foundation and a forward-looking perspective to help
bridge the gap between advanced multimodal AI models and efficient FPGA
deployment.

</details>


### [41] [Real-time Object Detection and Associated Hardware Accelerators Targeting Autonomous Vehicles: A Review](https://arxiv.org/abs/2509.04173)
*Safa Sali,Anis Meribout,Ashiyana Majeed,Mahmoud Meribout,Juan Pablo,Varun Tiwari,Asma Baobaid*

Main category: cs.AR

TL;DR: 这篇综述论文回顾了实时目标检测算法及其硬件加速器在自动驾驶汽车中的应用，强调了算法与商业技术之间的差距。


<details>
  <summary>Details</summary>
Motivation: 研究实时目标检测算法的效率及其在自动驾驶汽车中的应用，旨在填补学术界与商业技术之间的鸿沟。

Method: 综述了当前最先进的实时目标检测算法，尤其是基于CNN的模型，以及它们在GPU和ASIC等硬件加速器上的部署。

Result: 尽管已有算法能达到每秒数百帧的处理速度，但在多摄像头场景下仍需硬件和算法改进。

Conclusion: 论文为未来全自动驾驶汽车的设计提供了参考，并指出商业系统的保密性是当前研究的挑战。

Abstract: The efficiency of object detectors depends on factors like detection
accuracy, processing time, and computational resources. Processing time is
crucial for real-time applications, particularly for autonomous vehicles (AVs),
where instantaneous responses are vital for safety. This review paper provides
a concise yet comprehensive survey of real-time object detection (OD)
algorithms for autonomous cars delving into their hardware accelerators (HAs).
Non-neural network-based algorithms, which use statistical image processing,
have been entirely substituted by AI algorithms, such as different models of
convolutional neural networks (CNNs). Their intrinsically parallel features led
them to be deployable into edge-based HAs of various types, where GPUs and, to
a lesser extent, ASIC (application-specific integrated circuit) remain the most
widely used. Throughputs of hundreds of frames/s (fps) could be reached;
however, handling object detection for all the cameras available in a typical
AV requires further hardware and algorithmic improvements. The intensive
competition between AV providers has limited the disclosure of algorithms,
firmware, and even hardware platform details. This remains a hurdle for
researchers, as commercial systems provide valuable insights while academics
undergo lengthy training and testing on restricted datasets and road scenarios.
Consequently, many AV research papers may not be reflected in end products,
being developed under limited conditions. This paper surveys state-of-the-art
OD algorithms and aims to bridge the gap with technologies in commercial AVs.
To our knowledge, this aspect has not been addressed in earlier surveys. Hence,
the paper serves as a tangible reference for researchers designing future
generations of vehicles, expected to be fully autonomous for comfort and
safety.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [42] [Error Detection Schemes for Barrett Reduction of CT-BU on FPGA in Post Quantum Cryptography](https://arxiv.org/abs/2509.04070)
*Paresh Baidya,Rourab Paul,Vikas Srivastava,Sumit Kumar Debnath*

Main category: cs.CR

TL;DR: 论文提出了三种轻量级且高效的基于重计算的故障检测方法（RENO、RESO和RESWO），用于Kyber算法的Barrett Reduction模块，其中RESWO是新方法，效率接近100%。


<details>
  <summary>Details</summary>
Motivation: 量子后加密（PQC）算法在硬件加速器中可能因故意注入的故障而泄露敏感信息，影响可靠性，因此需要高效的故障检测方法。

Method: 提出了RESWO算法（重计算交换操作数），并结合现有RENO和RESO方法，应用于Kyber的Barrett Reduction模块。

Result: RESWO延迟低于RENO和RESO，三者故障检测效率均接近100%。

Conclusion: RESWO是一个高效且轻量级的故障检测方法，适用于PQC算法的硬件实现。

Abstract: A fault can occur naturally or intentionally. However, intentionally
injecting faults into hardware accelerators of Post-Quantum Cryptographic (PQC)
algorithms may leak sensitive information. This intentional fault injection in
side-channel attacks compromises the reliability of PQC implementations. The
recently NIST-standardized key encapsulation mechanism (KEM), Kyber may also
leak information at the hardware implementation level. This work proposes three
efficient and lightweight recomputation-based fault detection methods for
Barrett Reduction in the Cooley-Tukey Butterfly Unit (CT-BU) of Kyber on a
Field Programmable Gate Array (FPGA). The CT-BU and Barrett Reduction are
fundamental components in structured lattice-based PQC algorithms, including
Kyber, NTRU, Falcon, CRYSTALS-Dilithium, etc. This paper introduces a new
algorithm, Recomputation with Swapped Operand (RESWO), for fault detection.
While Recomputation with Negated Operand (RENO) and Recomputation with Shifted
Operand (RESO) are existing methods used in other PQC hardware algorithms. To
the best of our knowledge, RENO and RESO have never been used in Barrett
Reduction before. The proposed RESWO method consumes a similar number of slices
compared to RENO and RESO. However, RESWO shows lesser delay compared to both
RENO and RESO. The fault detection efficiency of RESWO, RENO, and RESO is
nearly 100%.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [43] [ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference](https://arxiv.org/abs/2509.03565)
*Qi Chen,Jingxuan Wei,Zhuoya Yao,Haiguang Wang,Gaowei Wu,Bihui Yu,Siyuan Li,Cheng Tan*

Main category: cs.CL

TL;DR: 提出ResearchPulse框架，通过多代理系统实现跨文档科学推理任务，提取并对齐相关论文中的动机、方法和实验结果，优于GPT-4o等基线方法。


<details>
  <summary>Details</summary>
Motivation: 科学思想的演变需要通过跨文档推理来理解相关研究的主题发展。

Method: 提出了ResearchPulse框架，包含任务分解的Plan Agent、构建动机-方法思维导图的Mmap-Agent和合成实验线图的Lchart-Agent。

Result: 实验表明，该系统在语义对齐、结构一致性和可视化保真度方面优于GPT-4o等基线方法。

Conclusion: ResearchPulse为科学推理提供了一种新的结构化方法，支持研究发展链的重建。

Abstract: Understanding how scientific ideas evolve requires more than summarizing
individual papers-it demands structured, cross-document reasoning over
thematically related research. In this work, we formalize multi-document
scientific inference, a new task that extracts and aligns motivation,
methodology, and experimental results across related papers to reconstruct
research development chains. This task introduces key challenges, including
temporally aligning loosely structured methods and standardizing heterogeneous
experimental tables. We present ResearchPulse, an agent-based framework that
integrates instruction planning, scientific content extraction, and structured
visualization. It consists of three coordinated agents: a Plan Agent for task
decomposition, a Mmap-Agent that constructs motivation-method mind maps, and a
Lchart-Agent that synthesizes experimental line charts. To support this task,
we introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper
clusters. Experiments show that our system, despite using 7B-scale agents,
consistently outperforms strong baselines like GPT-4o in semantic alignment,
structural consistency, and visual fidelity. The dataset are available in
https://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [44] [Oruga: An Avatar of Representational Systems Theory](https://arxiv.org/abs/2509.04041)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.AI

TL;DR: 论文提出了一种名为Oruga的系统，基于Representational Systems Theory (RST)，旨在通过数据结构和语言工具实现灵活表示转换。


<details>
  <summary>Details</summary>
Motivation: 目标是让机器具备人类灵活使用表示（如图表、类比）的能力，以增强与人类的兼容性。

Method: 开发Oruga系统，包含RST核心数据结构、通信语言及基于结构转换的引擎。

Result: 展示了Oruga的核心与语言，并通过结构转换示例验证其能力。

Conclusion: Oruga系统为机器实现灵活表示转换提供了初步框架，为未来研究奠定基础。

Abstract: Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.

</details>


### [45] [Domain size asymptotics for Markov logic networks](https://arxiv.org/abs/2509.04192)
*Vera Koponen*

Main category: cs.AI

TL;DR: 该论文研究了马尔科夫逻辑网络（MLN）在域大小趋于无穷时分布性质的极限行为，通过三类具体示例展示了MLN的随机结构行为的多样性，并比较了MLN与提升贝叶斯网络的渐近不可比性。


<details>
  <summary>Details</summary>
Motivation: 探索MLN在不同域大小下的分布行为，特别是极限情况，以揭示软约束对随机结构影响的多样性。

Method: 通过三类MLN具体示例（一元关系符号的量化自由MLN、减少三角形或k-clique的MLN、限制高顶点度的MLN），分析域大小趋于无穷时的分布性质。

Result: 展示了MLN的极限行为因软约束而异，部分情况下权重影响显著；同时证明了MLN与提升贝叶斯网络的渐近不可比性。

Conclusion: MLN的极限行为复杂多样，软约束的选择和权重可能显著影响分布，且在大型域中MLN分布与均匀分布差异显著。

Abstract: A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [46] [lifeXplore at the Lifelog Search Challenge 2021](https://arxiv.org/abs/2509.03692)
*Andreas Leibetseder,Klaus Schoeffmann*

Main category: cs.IR

TL;DR: Lifelog Search Challenge (LSC)是一项交互式生命日志数据检索竞赛，本文介绍了改进的lifeXplore系统，结合时间线摘要浏览和交互式概念过滤功能。


<details>
  <summary>Details</summary>
Motivation: 提升生命日志数据的检索效率和用户体验，满足LSC竞赛中对快速检索特定记忆的需求。

Method: 改进的lifeXplore系统，结合时间线摘要浏览、交互式概念过滤，并新增时间查询、高级摘要功能和可用性优化。

Result: 系统功能更强大，检索效率更高，用户体验更优。

Conclusion: 改进的lifeXplore系统能更好地满足LSC竞赛需求，展现了生命日志检索技术的进步。

Abstract: Since its first iteration in 2018, the Lifelog Search Challenge (LSC)
continues to rise in popularity as an interactive lifelog data retrieval
competition, co-located at the ACM International Conference on Multimedia
Retrieval (ICMR). The goal of this annual live event is to search a large
corpus of lifelogging data for specifically announced memories using a
purposefully developed tool within a limited amount of time. As long-standing
participants, we present our improved lifeXplore - a retrieval system combining
chronologic day summary browsing with interactive combinable concept filtering.
Compared to previous versions, the tool is improved by incorporating temporal
queries, advanced day summary features as well as usability improvements.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

TL;DR: 本文综述了人类运动视频生成领域的最新进展，首次全面梳理了生成过程的五个关键阶段，并探讨了大语言模型在该领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有综述仅关注个别方法，缺乏对整个生成过程的全面概述，本论文旨在填补这一空白。

Method: 基于三种主要模态（视觉、文本、音频），详细分析了十个子任务和五个生成阶段：输入、运动规划、运动视频生成、优化和输出。

Result: 综述覆盖200多篇论文，梳理了技术发展趋势并突出了里程碑工作。

Conclusion: 本文为数字人类综合应用提供了宝贵资源，并展望了人类运动视频生成的未来前景。

Abstract: Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [48] [TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph](https://arxiv.org/abs/2509.04086)
*Yaru Chen,Faegheh Sardari,Peiliang Zhang,Ruohao Guo,Yang Xiang,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合双向文本融合（BiT）模块和类别感知时序图（CATS）模块的方法，以解决现有音频-视觉视频解析（AVVP）任务中噪声标签和注意力分散的问题，实现了在多个关键指标上的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有AVVP方法在噪声标签处理和注意力机制上存在不足，导致训练过程中误差被放大。本文旨在通过结合两种研究方向的优势，提升解析精度。

Method: 通过BiT模块对音频和视觉模态特征进行语义注入和动态校准，定位并净化语义线索；利用CATS模块进行语义传播和连接，实现跨时间的精确语义信息分发。

Result: 提出的方法在LLP和UnAV-100两个基准数据集上实现了SOTA性能。

Conclusion: 结合BiT和CATS模块的方法有效解决了现有问题，提升了AVVP任务的性能。

Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [49] [How many patients could we save with LLM priors?](https://arxiv.org/abs/2509.04250)
*Shota Arai,David Selby,Andrew Vargo,Sebastian Vollmer*

Main category: stat.ME

TL;DR: 提出了一种利用大型语言模型（LLM）生成先验分布的分层贝叶斯建模框架，以提高临床试验中的数据统计效率。


<details>
  <summary>Details</summary>
Motivation: 减少临床试验所需患者数量，同时保持统计效能，通过结合外部临床专业知识改进安全性评估。

Method: 利用预训练LLM直接生成分层贝叶斯模型的超参数先验分布，取代传统的数据增强方法。

Result: LLM生成的先验分布显著提升了预测性能，优于传统元分析方法。

Conclusion: 该方法能够显著提高临床试验效率，减少患者数量，并可能改变药物安全监测和监管决策的方式。

Abstract: Imagine a world where clinical trials need far fewer patients to achieve the
same statistical power, thanks to the knowledge encoded in large language
models (LLMs). We present a novel framework for hierarchical Bayesian modeling
of adverse events in multi-center clinical trials, leveraging LLM-informed
prior distributions. Unlike data augmentation approaches that generate
synthetic data points, our methodology directly obtains parametric priors from
the model. Our approach systematically elicits informative priors for
hyperparameters in hierarchical Bayesian models using a pre-trained LLM,
enabling the incorporation of external clinical expertise directly into
Bayesian safety modeling. Through comprehensive temperature sensitivity
analysis and rigorous cross-validation on real-world clinical trial data, we
demonstrate that LLM-derived priors consistently improve predictive performance
compared to traditional meta-analytical approaches. This methodology paves the
way for more efficient and expert-informed clinical trial design, enabling
substantial reductions in the number of patients required to achieve robust
safety assessment and with the potential to transform drug safety monitoring
and regulatory decision making.

</details>
