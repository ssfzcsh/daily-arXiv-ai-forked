<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [math.LO](#math.LO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 该论文引入了一个德语开发者情感分析数据集，填补了德语领域数据缺失的空白，并验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析工具主要依赖英语数据，德语领域缺乏相关资源，影响了德语开发者社区的情感分析研究。

Method: 从德国开发者论坛提取5,949条语句，由四位德语学生基于Shaver等人的情感模型标注六种基本情感。

Result: 标注过程显示出高评分者一致性和可靠性，验证了数据集的有效性；现有德语工具无法满足软件工程领域需求。

Conclusion: 提出的德语数据集为德语开发者社区的情感分析提供了可靠资源，并指出了未来优化标注和应用的方向。

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [2] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: 论文提出了一种自动化工具，用于从用户反馈中提取可解释性需求并生成相应解释，评估显示AI生成的解释在清晰度和风格上更受青睐，但正确性仍需人工验证。


<details>
  <summary>Details</summary>
Motivation: 增强透明度、建立用户信任及确保合规性需要可解释性，但现有方法缺乏从用户反馈到结构化需求的系统化途径。

Method: 提出工具支持的方法，与工业自动化制造商合作创建标注数据集，评估AI生成需求与解释的效果。

Result: AI生成的解释在清晰度和风格上优于人工，但需求的相关性和正确性不足，需人工验证。

Conclusion: 研究推动了可解释性需求自动化处理的进展，提供了数据集支持未来研究，并强调了人工验证的重要性。

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [3] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: 工业4.0技术与工程工作流的融合为工厂和流程工程的自动化与优化提供了关键支持，其中AAS是实现数字孪生互操作性的核心工具。本文结合BPMN探讨了AAS在工程工作流中的应用，提出了一种分布式AAS基础设施以提升安全性和扩展性，并开发了一个原型系统以提高效率与可追溯性。


<details>
  <summary>Details</summary>
Motivation: 工业4.0技术的应用需要高效、安全的工程自动化解决方案，AAS作为一种数字孪生技术的核心组件，能够促进跨组织协作和数据交换。

Method: 本文提出了一种分布式AAS copy-on-write基础设施，结合BPMN实现结构化与自动化流程，并开发了一个工作流管理原型系统。

Result: 通过分布式AAS基础设施和原型系统，提高了工程工作流的效率、安全性和可追溯性。

Conclusion: AAS与BPMN的结合为工业4.0技术支持下的工程自动化提供了可行且高效的解决方案。

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [4] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 研究表明，当前的需求文档对直接用于LLM生成代码过于抽象，需要先手动分解为编程任务并补充设计决策。


<details>
  <summary>Details</summary>
Motivation: 探讨开发者如何结合需求和设计工件使用LLM生成代码，评估LLM是否可能取代传统软件工程。

Method: 访谈14家公司的18名从业者，分析其使用需求和设计工件的过程。

Result: 需求需分解并补充后输入LLM，传统需求工程在LLM时代仍不可或缺。

Conclusion: LLM时代仍需基础需求工程工作，科学方法自动化需求中心任务需结合这一背景。

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [5] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: 论文提出了一种系统性的Prompt Engineering（PE）在需求工程（RE）中的应用综述（PE4RE），分析了35项研究，提出了一个混合分类法，并指出了当前局限和研究空白，最后给出了一个逐步发展路线图。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在需求工程中的应用存在不确定性和不可控性，缺乏有效的提示指导，阻碍了其可信赖的实现。

Method: 基于Kitchenham和Petersen的二级研究协议，搜索了6个数字图书馆，筛选了867条记录，分析了35项主要研究，提出了混合分类法。

Result: 分类法将技术导向的模式与任务导向的RE角色联系起来，揭示了当前局限和研究空白。

Conclusion: 论文为从临时PE原型向可复现、适合实践的工作流程的演变提供了路线图。

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [6] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: 论文探讨了如何利用检索增强生成（RAG）模型支持太空领域需求的半自动化生成，通过模块化的AI方法处理文档、分类内容并合成需求草案，初步结果显示可减少人工工作量和提升需求覆盖率。


<details>
  <summary>Details</summary>
Motivation: 小规模太空组织和新兴企业在从大量非结构化文档中提取可操作需求时面临困难，需要高精度且符合严格标准的方法。

Method: 采用RAG模型和大型语言模型（LLMs），对原始太空任务文档进行预处理、分类，检索领域标准相关内容并合成需求草案。

Result: 初步应用显示，该方法能减少人工工作量、提升需求覆盖率，并支持轻量级合规性对齐。

Conclusion: 研究为AI在需求工程中的广泛应用提供了路线图，旨在降低小组织参与大规模安全关键任务的障碍。

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: 程序的一般等价性不可判定，但忽略语句语义后问题可判定且实用。命题等价性在(G)KAT框架下的新进展。


<details>
  <summary>Details</summary>
Motivation: 探讨在抽象语句语义后，程序等价性问题如何变得可判定及实用。

Method: 基于(Guarded) Kleene Algebra with Tests ((G)KAT)的理论框架。

Result: 展示命题程序等价性在实际中的可行性及其理论支持。

Conclusion: 在(G)KAT框架下，命题程序等价性成为可判定且有实用价值的工具。

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [8] [Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning](https://arxiv.org/abs/2507.07118)
*Zelin Zhu,Kai Yang,Rui Zhang*

Main category: cs.NI

TL;DR: 论文提出了一种结合无线定位和感知的深度学习优化算法（SPG-MIBO），适用于MIMO-OFDM系统的高维数据。


<details>
  <summary>Details</summary>
Motivation: 无线定位和感知技术在现代网络中至关重要，但现有方法在高维CSI特性下的联合建模研究不足。

Method: 提出了基于随机近端梯度的混合整数双层优化算法（SPG-MIBO），适用于高维和大规模数据。

Result: 多数据集实验验证了算法的有效性，显示了联合优化的性能提升。

Conclusion: SPG-MIBO在联合优化无线定位和感知方面表现优异，具备理论和实验支持。

Abstract: Wireless localization and sensing technologies are essential in modern
wireless networks, supporting applications in smart cities, the Internet of
Things (IoT), and autonomous systems. High-performance localization and sensing
systems are critical for both network efficiency and emerging intelligent
applications. Integrating channel state information (CSI) with deep learning
has recently emerged as a promising solution. Recent works have leveraged the
spatial diversity of multiple input multiple output (MIMO) systems and the
frequency granularity of orthogonal frequency division multiplexing (OFDM)
waveforms to improve spatial resolution. Nevertheless, the joint modeling of
localization and sensing under the high-dimensional CSI characteristics of
MIMO-OFDM systems remains insufficiently investigated. This work aims to
jointly model and optimize localization and sensing tasks to harness their
potential synergy. We first formulate localization and sensing as a
mixed-integer bilevel deep learning problem and then propose a novel stochastic
proximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)
algorithm. SPG-MIBO is well-suited for high-dimensional and large-scale
datasets, leveraging mini-batch training at each step for computational and
memory efficiency. The algorithm is also supported by theoretical convergence
guarantees. Extensive experiments on multiple datasets validate its
effectiveness and highlight the performance gains from joint localization and
sensing optimization.

</details>


### [9] [DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training](https://arxiv.org/abs/2507.07149)
*Renyuan Liu,Yuyang Leng,Kaiyan Liu,Shaohan Hu,Chun-Fu,Chen,Peijun Zhao,Heechul Yun,Shuochao Yao*

Main category: cs.NI

TL;DR: 论文提出了DAF框架，通过系统级优化实现动态激活量化，显著降低内存占用并加速训练，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 解决移动和边缘设备在深度神经网络训练中因激活占用内存过高而面临的内存限制问题。

Method: 引入DAF框架，包括混合归约操作、CPU-GPU协作位压缩和重要性感知分页内存管理。

Result: 在多种深度学习模型上实现22.9倍内存节省和3.2倍加速，且不影响训练精度。

Conclusion: DAF为资源受限环境提供了一种高效、可扩展的解决方案。

Abstract: Recent advancements in on-device training for deep neural networks have
underscored the critical need for efficient activation compression to overcome
the memory constraints of mobile and edge devices. As activations dominate
memory usage during training and are essential for gradient computation,
compressing them without compromising accuracy remains a key research
challenge. While existing methods for dynamic activation quantization promise
theoretical memory savings, their practical deployment is impeded by
system-level challenges such as computational overhead and memory
fragmentation.
  To address these challenges, we introduce DAF, a Dynamic Activation Framework
that enables scalable and efficient on-device training through system-level
optimizations. DAF achieves both memory- and time-efficient dynamic
quantization training by addressing key system bottlenecks. It develops hybrid
reduction operations tailored to the memory hierarchies of mobile and edge
SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic
quantization, and implements an importance-aware paging memory management
scheme to reduce fragmentation and support dynamic memory adjustments.
  These optimizations collectively enable DAF to achieve substantial memory
savings and speedup without compromising model training accuracy. Evaluations
on various deep learning models across embedded and mobile platforms
demonstrate up to a $22.9\times$ reduction in memory usage and a $3.2\times$
speedup, making DAF a scalable and practical solution for resource-constrained
environments.

</details>


### [10] [PHandover: Parallel Handover in Mobile Satellite Network](https://arxiv.org/abs/2507.07437)
*Jiasheng Wu,Shaojie Su,Wenjun Zhu,Xiong Wang,Jingjing Zhang,Xingqiu He,Yue Gao*

Main category: cs.NI

TL;DR: 提出了一种并行切换机制，通过基于计划的切换和卫星同步功能（SSF），显著降低了LEO卫星网络中的切换延迟，实验表明延迟减少了21倍。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络因其高速移动导致频繁高延迟切换，影响时延敏感应用性能，亟需解决方案。

Method: 采用基于计划的切换替代基于测量的切换，引入SSF功能，结合信号强度预测的机器学习模型和高效切换调度算法。

Result: 实验显示，方案较标准NTN切换和其他现有方法减少21倍延迟，网络稳定性和用户性能显著提升。

Conclusion: 并行切换机制有效解决了LEO卫星网络的高延迟问题，具备5G核心网兼容性，性能优越。

Abstract: The construction of Low Earth Orbit (LEO) satellite constellations has
recently attracted tremendous attention from both academia and industry. The 5G
and 6G standards have identified LEO satellite networks as a key component of
future mobile networks. However, due to the high-speed movement of satellites,
ground terminals often experience frequent and high-latency handovers, which
significantly deteriorate the performance of latency-sensitive applications. To
address this challenge, we propose a parallel handover mechanism for mobile
satellite networks that can considerably reduce handover latency. The main idea
is to employ plan-based handovers instead of measurement-based handovers to
avoid interactions between the access and core networks, thereby eliminating
the significant time overhead associated with traditional handover procedures.
Specifically, we introduce a novel network function named the Satellite
Synchronized Function (SSF), which is designed to be fully compliant with the
standard 5G core network. In addition, we propose a machine learning model for
signal strength prediction, coupled with an efficient handover scheduling
algorithm. We have conducted extensive experiments, and the results demonstrate
that our proposed handover scheme can reduce handover latency by 21\times
compared to the standard NTN handover scheme and two other existing handover
approaches, along with significant improvements in network stability and
user-level performance.

</details>


### [11] [Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks](https://arxiv.org/abs/2507.07481)
*Wen Zhang,Aimin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Weijie Yuan,Dusit Niyato*

Main category: cs.NI

TL;DR: 提出了一种无人机辅助的无电池传感器网络数据采集与无线供电框架，并通过改进的强化学习算法优化能量消耗与数据采集公平性。


<details>
  <summary>Details</summary>
Motivation: 解决高温等极端环境下无线供电传感器网络的部署与数据采集难题。

Method: 结合无人机供电与OFDMA数据传输，并采用改进的SAC-PPV算法优化功率分配与飞行轨迹。

Result: 仿真表明所提方法在多种网络配置下优于基准算法。

Conclusion: 无人机辅助框架及改进算法有效解决了极端环境下的无线供电与数据采集问题。

Abstract: The integration of wireless power transfer (WPT) with Internet of Things
(IoT) offers promising solutions for sensing applications, but faces
significant challenges when deployed in hard-to-access areas such as
high-temperature environments. In such extreme conditions, traditional fixed
WPT infrastructure cannot be safely installed, and batteries rapidly degrade
due to hardware failures. In this paper, we propose an uncrewed aerial vehicle
(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)
networks deployed in these challenging environments. Specifically, we consider
a practical scenario where a UAV first transfers energy to BLS nodes via WPT,
enabling these nodes to subsequently transmit their collected data to the UAV
through orthogonal frequency-division multiple access (OFDMA). Then, we
formulate a multi-objective optimization problem that aims to maximize the fair
data collection volume while minimizing the UAV energy consumption through
joint optimization of transmit power allocation and flight trajectory planning.
Due to the non-convex nature and dynamic characteristics of this problem,
conventional optimization methods prove inadequate. To address these
challenges, we propose an enhanced soft actor-critic algorithm with
parameter-free attention, prioritized experience replay, and value-based reward
centering (SAC-PPV), thereby improving the exploration efficiency and learning
stability of the algorithm in complex WPT scenarios. Simulation results
demonstrate that the proposed approach consistently outperforms benchmark
algorithms under various network configurations.

</details>


### [12] [A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks](https://arxiv.org/abs/2507.07535)
*Jingzhao Xie,Zhenglian Li,Gang Sun,Long Luo,Hongfang Yu,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文提出了一种名为ABS的模块化框架，用于解决计算能力网络（CPN）中的服务映射问题，显著提升了资源利用率和服务接受率。


<details>
  <summary>Details</summary>
Motivation: 计算能力网络（CPN）旨在通过网络协调统一广域计算资源，但现有方法未能充分整合资源并优化服务映射。

Method: 提出ABS框架，包括基于图分割的重构、双层优化架构和碎片感知评估，采用分布式粒子群优化实现。

Result: 在复杂场景中，ABS比现有基线方法提升73.2%的计算资源利用率和60.2%的服务接受率。

Conclusion: ABS成功解决了CPN中的服务映射问题，显著提升了资源效率和服务质量。

Abstract: Computing Power Network (CPN) unifies wide-area computing resources through
coordinated network control, while cloud-native abstractions enable flexible
resource orchestration and on-demand service provisioning atop the elastic
infrastructure CPN provides. However, current approaches fall short of fully
integrating computing resources via network-enabled coordination as envisioned
by CPN. In particular, optimally mapping services to an underlying
infrastructure to maximize resource efficiency and service satisfaction remains
challenging. To overcome this challenge, we formally define the service mapping
problem in CPN, establish its theoretical intractability, and identify key
challenges in practical optimization. We propose Adaptive Bilevel Search (ABS),
a modular framework featuring (1) graph partitioning-based reformulation to
capture variable coupling, (2) a bilevel optimization architecture for
efficient global exploration with local optimality guarantees, and (3)
fragmentation-aware evaluation for global performance guidance. Implemented
using distributed particle swarm optimization, ABS is extensively evaluated
across diverse CPN scenarios, consistently outperforming existing approaches.
Notably, in complex scenarios, ABS achieves up to 73.2% higher computing
resource utilization and a 60.2% higher service acceptance ratio compared to
the best-performing baseline.

</details>


### [13] [Can cloud-based VR streaming handle Wi-Fi OBSS contention?](https://arxiv.org/abs/2507.07677)
*Miguel Casasnovas,Marc Carrascosa-Zamacois,Boris Bellalta*

Main category: cs.NI

TL;DR: 研究了Wi-Fi网络中重叠信道对VR流的负面影响，发现OBSS数量和位置显著影响性能，并提出NeSt-VR算法有效缓解了这一问题。


<details>
  <summary>Details</summary>
Motivation: 探索Wi-Fi网络中重叠信道（尤其是80 MHz信道内的部分和完全重叠）对VR流性能的负面影响，并提出解决方案。

Method: 通过实验分析不同OBSS配置下VR流的性能变化，并测试NeSt-VR算法的有效性。

Result: OBSS数量和位置对VR流性能有显著影响，NeSt-VR能有效减轻性能下降。

Conclusion: 重叠信道对VR流有显著负面影响，但NeSt-VR算法能改善性能，尤其在OBSS密集环境中。

Abstract: This paper experimentally analyzes the negative impact of contention caused
by neighboring Wi-Fi networks operating on overlapping channels on Virtual
Reality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full
channel overlap within an 80 MHz channel. Our results show that (i) increasing
the number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies
contention and degrades VR streaming performance; (ii) OBSS activity on the
secondary-sided 40 MHz portion degrades performance more than activity on the
primary-sided 40 MHz portion; (iii) for the same aggregate load, full channel
overlap with two 40 MHz OBSS contenders is less detrimental than partial
overlap with a single high-load 40 MHz contender, but more disruptive than full
overlap with two 80 MHz contenders; and (iv) full channel overlap with two 40
MHz OBSS contenders has a smaller impact on VR streaming under symmetric
traffic loads than under asymmetric loads. Moreover, our results demonstrate
that our previously proposed Network-aware Step-wise adaptive bitrate algorithm
for VR streaming (NeSt-VR) effectively mitigates performance degradation in
OBSS environments, enabling VR streaming under heavier OBSS traffic conditions.

</details>


### [14] [HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN](https://arxiv.org/abs/2507.07841)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.NI

TL;DR: 论文提出了一种基于Wi-Fi HaLow IEEE 802.11s网状网络的智能城市应急通信架构HaLert，支持文本、位置、图像、音频和视频的通信，并结合SDN和LoRa网络实现远程监控和配置。


<details>
  <summary>Details</summary>
Motivation: 为了解决灾难事件中基础设施受损导致通信中断的问题，利用智能城市中密集分布的IoT网络资源，开发一种可快速重分配的应急通信系统。

Method: 提出HaLert架构，结合Wi-Fi HaLow网状网络和SDN范式，并通过LoRa控制的泛洪网状网络实现远程监控和配置。

Result: 在真实城市环境中测试显示，Wi-Fi HaLow网络平均延迟15-54.8 ms，上传速率134-726 Kbps，下载速率117-682 Kbps，LoRa网络消息成功率达94.96%，系统稳定且功能完整。

Conclusion: HaLert架构在障碍物、非视距和地形坡度等条件下表现出强韧性和稳定性，适合作为智能城市的应急通信解决方案。

Abstract: Events such as catastrophes and disasters are, in most cases, unpredictable.
Consequently, reusing existing infrastructures to develop alternative
communication strategies after disasters is essential to minimise the impact of
these events on the population's ability to communicate and promptly receive
alerts from authorities. In this context, the emergence of smart cities,
characterised by dense and geographically distributed IoT networks, presents
significant potential for such reuse. This work proposes HaLert, a resilient
architecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,
whose resources can be readily reallocated to support a emergency communication
system to exchange messages (including text, location, image, audio, and video)
between citizens, authorities, and between both parties. To facilitate remote
monitoring and configuration of the network, the architecture incorporates the
SDN (Software-Defined Networking) paradigm, supported by a LoRa controlled
flooding mesh network. A prototype was developed based on this architecture and
tested in a real urban scenario comprising both indoor and outdoor
environments. The results demonstrated that, despite the significant impact of
obstacles, lack of line-of-sight, and terrain slopes on the latency (average
latency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and
726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow
network, it remained stable and resilient, successfully providing all
functionalities associated with the HaLert architecture. The tests conducted on
the LoRa network revealed a high average message success rate of 94.96%.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech Processing](https://arxiv.org/abs/2507.07396)
*Zeyang Song,Shimin Zhang,Yuhong Chou,Jibin Wu,Haizhou Li*

Main category: cs.MM

TL;DR: 提出了一种名为IML-Spikeformer的新型脉冲神经网络架构，针对大规模语音处理任务设计了输入感知多级脉冲机制，显著提升了性能并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在能源效率上优于传统人工神经网络（ANNs），但在大规模语音处理任务中性能不足，主要受限于训练时的高计算开销和缺乏针对语音任务的大规模架构。

Method: 设计了输入感知多级脉冲（IMLS）机制和带有层次衰减掩码的重新参数化脉冲自注意力模块（HD-RepSSA），以模拟多时间步脉冲并在单时间步内实现高效计算。

Result: 在AiShell-1和Librispeech-960数据集上的词错误率分别为6.0%和3.4%，与ANN变压器相当，同时理论推理能耗降低了4.64倍和4.32倍。

Conclusion: IML-Spikeformer在大规模语音处理任务中实现了性能和能源效率的双重突破，为SNN架构的可扩展性提供了新思路。

Abstract: Spiking Neural Networks (SNNs), inspired by biological neural mechanisms,
represent a promising neuromorphic computing paradigm that offers
energy-efficient alternatives to traditional Artificial Neural Networks (ANNs).
Despite proven effectiveness, SNN architectures have struggled to achieve
competitive performance on large-scale speech processing task. Two key
challenges hinder progress: (1) the high computational overhead during training
caused by multi-timestep spike firing, and (2) the absence of large-scale SNN
architectures tailored to speech processing tasks. To overcome the issues, we
introduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking
Transformer architecture specifically designed for large-scale speech
processing. Central to our design is the Input-aware Multi-Level Spike (IMLS)
mechanism, which simulate multi-timestep spike firing within a single timestep
using an adaptive, input-aware thresholding scheme. IML-Spikeformer further
integrates a Reparameterized Spiking Self-Attention (RepSSA) module with a
Hierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module
enhances the precision of attention maps and enables modeling of multi-scale
temporal dependencies in speech signals. Experiments demonstrate that
IML-Spikeformer achieves word error rates of 6.0\% on AiShell-1 and 3.4\% on
Librispeech-960, comparable to conventional ANN transformers while reducing
theoretical inference energy consumption by 4.64$\times$ and 4.32$\times$
respectively. IML-Spikeformer marks an advance of scalable SNN architectures
for large-scale speech processing in both task performance and energy
efficiency.

</details>


### [16] [The Potential of Olfactory Stimuli in Stress Reduction through Virtual Reality](https://arxiv.org/abs/2507.07911)
*Yasmin Elsaddik Valdivieso,Mohd Faisal,Karim Alghoul,Monireh,Vahdati,Kamran Gholizadeh Hamlabadi,Fedwa Laamarti,Hussein Al Osman,Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: 研究表明，嗅觉刺激（如海滩精油气味）在虚拟现实中虽然未显著提升自我报告的放松感，但通过生理指标（HRV）显著降低了压力，且多数参与者认可其实际吸引力。


<details>
  <summary>Details</summary>
Motivation: 探讨嗅觉刺激在虚拟现实中对压力缓解和放松的影响，以弥补传统VR仅依赖视觉和听觉的不足。

Method: 采用随机组内设计，30名参与者体验带有和不带有海滩精油气味的VR场景，通过自我报告和心电图（HRV）测量效果。

Result: 嗅觉刺激显著降低了生理压力（HRV指标提高108%），但未显著影响自我报告的放松感；71.4%的参与者愿意使用嗅觉增强VR。

Conclusion: 嗅觉刺激可能通过潜意识增强放松效果，突显多感官整合在VR中的重要性。未来可研究个性化气味和长期效果。

Abstract: Immersive virtual reality (VR) is a promising tool for stress reduction and
relaxation, traditionally relying on visual and auditory stimuli. This study
examines the role of olfactory stimuli in enhancing these effects, using a
randomized within-subject design. Thirty participants aged 18-60 experienced VR
scenarios simulating a calming seaside environment, with sessions lasting 45
minutes, in two conditions: with and without a "Beach" essential oil scent
(Yankee Candle) administered via diffuser. Stress and relaxation were assessed
through self-reported surveys and physiological measures, specifically
ECG-based heart rate variability (HRV). Results showed no significant
difference in self-reported relaxation scores (p=0.371) between conditions, but
HRV analysis revealed a significant stress reduction (p=0.002) with olfactory
input, with HF increasing 108% from the Math Stress Test to the scented
relaxation condition, compared to 44% without scent. Additionally, 71.4% of
participants expressed willingness to use olfactory-enhanced VR for relaxation,
suggesting practical appeal. These findings indicate that olfactory stimuli may
enhance relaxation subconsciously, underscoring the importance of multisensory
integration in VR. Future work could explore personalized scents and long-term
effects to optimize VR- based interventions for emotional and physical
well-being.

</details>


### [17] [Multimodal Framework for Explainable Autonomous Driving: Integrating Video, Sensor, and Textual Data for Enhanced Decision-Making and Transparency](https://arxiv.org/abs/2507.07938)
*Abolfazl Zarghani,Amirhossein Ebrahimi,Amir Malekesfandiari*

Main category: cs.MM

TL;DR: 本文提出了一种结合视频、传感器和文本数据的新型多模态框架，用于自动驾驶车辆的行为预测和可解释性输出，显著提升了性能和透明度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的成功依赖于对复杂动态环境的准确理解和决策透明度，但多模态数据的无缝整合和AI决策的可解释性仍是重大挑战。

Method: 结合VideoMAE进行时空视频分析，定制传感器融合模块实时处理数据，以及BERT理解文本，构建了一个多模态框架。

Result: 在BDD-X和nuScenes数据集上，模型训练损失从5.7231降至0.0187，动作预测准确率达92.5%，解释质量BLEU-4得分0.75，优于现有方法。

Conclusion: 多模态集成和可解释性为构建安全、透明且可信赖的自动驾驶系统提供了突破，推动了自动驾驶技术的广泛应用。

Abstract: Autonomous vehicles (AVs) are poised to redefine transportation by enhancing
road safety, minimizing human error, and optimizing traffic efficiency. The
success of AVs depends on their ability to interpret complex, dynamic
environments through diverse data sources, including video streams, sensor
measurements, and contextual textual information. However, seamlessly
integrating these multimodal inputs and ensuring transparency in AI-driven
decisions remain formidable challenges. This study introduces a novel
multimodal framework that synergistically combines video, sensor, and textual
data to predict driving actions while generating human-readable explanations,
fostering trust and regulatory compliance. By leveraging VideoMAE for
spatiotemporal video analysis, a custom sensor fusion module for real-time data
processing, and BERT for textual comprehension, our approach achieves robust
decision-making and interpretable outputs. Evaluated on the BDD-X (21113
samples) and nuScenes (1000 scenes) datasets, our model reduces training loss
from 5.7231 to 0.0187 over five epochs, attaining an action prediction accuracy
of 92.5% and a BLEU-4 score of 0.75 for explanation quality, outperforming
state-of-the-art methods. Ablation studies confirm the critical role of each
modality, while qualitative analyses and human evaluations highlight the
model's ability to produce contextually rich, user-friendly explanations. These
advancements underscore the transformative potential of multimodal integration
and explainability in building safe, transparent, and trustworthy AV systems,
paving the way for broader societal adoption of autonomous driving
technologies.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [18] [Dirty Data in the Newsroom: Comparing Data Preparation in Journalism and Data Science](https://arxiv.org/abs/2507.07238)
*Stephen Kasica,Charles Berret,Tamara Munzner*

Main category: cs.HC

TL;DR: 该研究填补了数据新闻中数据准备研究的空白，通过混合主题分析方法扩展了数据科学工作模型，并提出了一种新的脏数据问题分类法。


<details>
  <summary>Details</summary>
Motivation: 尽管数据科学工作流程中的数据准备已有研究，但数据新闻领域的数据准备尚未得到充分关注。

Method: 采用混合主题分析方法，结合演绎编码（来自现有数据科学工作流程）和归纳编码（来自36名数据新闻记者的访谈）。

Result: 扩展了数据科学工作模型，总结了60种脏数据问题，并提出了基于心理模型差异的新分类法。

Conclusion: 研究揭示了数据新闻记者面临的四大挑战（历时性、区域性、碎片化和数据源分散），并提供了理论支持。

Abstract: The work involved in gathering, wrangling, cleaning, and otherwise preparing
data for analysis is often the most time consuming and tedious aspect of data
work. Although many studies describe data preparation within the context of
data science workflows, there has been little research on data preparation in
data journalism. We address this gap with a hybrid form of thematic analysis
that combines deductive codes derived from existing accounts of data science
workflows and inductive codes arising from an interview study with 36
professional data journalists. We extend a previous model of data science work
to incorporate detailed activities of data preparation. We synthesize 60 dirty
data issues from 16 taxonomies on dirty data and our interview data, and we
provide a novel taxonomy to characterize these dirty data issues as
discrepancies between mental models. We also identify four challenges faced by
journalists: diachronic, regional, fragmented, and disparate data sources.

</details>


### [19] [FLoRA: An Advanced AI-Powered Engine to Facilitate Hybrid Human-AI Regulated Learning](https://arxiv.org/abs/2507.07362)
*Xinyu Li,Tongguang Li,Lixiang Yan,Yuheng Li,Linxuan Zhao,Mladen Raković,Inge Molenaar,Dragan Gašević,Yizhou Fan*

Main category: cs.HC

TL;DR: 论文探讨了AI如何影响学习者的自我调节学习（SRL），提出混合人机调节学习（HHAIRL）框架，并介绍了增强版\flora Engine工具，结合生成式AI和学习分析技术，以动态支持SRL。


<details>
  <summary>Details</summary>
Motivation: 现有数字工具在适应性、支持SRL全阶段及人机互动方面不足，需开发更灵活的解决方案。

Method: 提出增强版\flora Engine，集成生成式AI和学习分析，提供协作写作、多代理聊天机器人等工具。

Result: 研究验证了\flora Engine在支持SRL和HHAIRL方面的有效性。

Conclusion: \flora Engine为AI增强学习提供了理论和实践支持，推动了SRL和HHAIRL的发展。

Abstract: SRL, defined as learners' ability to systematically plan, monitor, and
regulate their learning activities, is crucial for sustained academic
achievement and lifelong learning competencies. Emerging Artificial
Intelligence (AI) developments profoundly influence SRL interactions by
potentially either diminishing or strengthening learners' opportunities to
exercise their own regulatory skills. Recent literature emphasizes a balanced
approach termed Hybrid Human-AI Regulated Learning (HHAIRL), in which AI
provides targeted, timely scaffolding while preserving the learners' role as
active decision-makers and reflective monitors of their learning process.
Nevertheless, existing digital tools frequently fall short, lacking
adaptability, focusing narrowly on isolated SRL phases, and insufficiently
support meaningful human-AI interactions. In response, this paper introduces
the enhanced \flora Engine, which incorporates advanced Generative Artificial
Intelligence (GenAI) features and state-of-the-art learning analytics,
explicitly grounded in SRL and HHAIRL theories. The \flora Engine offers
instrumentation tools such as collaborative writing, multi-agents chatbot, and
detailed learning trace logging to support dynamic, adaptive scaffolding
tailored to individual needs in real time. We further present a summary of
several research studies that provide the validations for and illustrate how
these instrumentation tools can be utilized in real-world educational and
experimental contexts. These studies demonstrate the effectiveness of \flora
Engine in fostering SRL and HHAIRL, providing both theoretical insights and
practical solutions for the future of AI-enhanced learning context.

</details>


### [20] [Pluri-perspectivism in Human-robot Co-creativity with Older Adults](https://arxiv.org/abs/2507.07550)
*Marianne Bossema,Rob Saunders,Aske Plaat,Somaya Ben Allouch*

Main category: cs.HC

TL;DR: 探讨多视角主义在人类-机器人协同创作中的重要性，并提出一个五维模型来指导设计和分析交互动态。


<details>
  <summary>Details</summary>
Motivation: 研究多视角主义如何支持创造性实践，并探索机器人在人类创造力中的潜在增强作用。

Method: 基于文献和与视觉艺术家及艺术教育者的访谈研究，提出五维模型。

Result: 研究发现多视角主义能通过自适应、情境敏感的行为增强人类创造力。

Conclusion: 未来可将多视角主义与视觉语言模型结合，进一步提升协同创作机器人的情境敏感性。

Abstract: This position paper explores pluriperspectivism as a core element of human
creative experience and its relevance to humanrobot cocreativity We propose a
layered fivedimensional model to guide the design of cocreative behaviors and
the analysis of interaction dynamics This model is based on literature and
results from an interview study we conducted with 10 visual artists and 8 arts
educators examining how pluriperspectivism supports creative practice The
findings of this study provide insight in how robots could enhance human
creativity through adaptive contextsensitive behavior demonstrating the
potential of pluriperspectivism This paper outlines future directions for
integrating pluriperspectivism with visionlanguage models VLMs to support
context sensitivity in cocreative robots

</details>


### [21] [ArchiveGPT: A human-centered evaluation of using a vision language model for image cataloguing](https://arxiv.org/abs/2507.07551)
*Line Abele,Gerrit Anders,Tolgahan Aydın,Jürgen Buder,Helen Fischer,Dominik Kimmel,Markus Huff*

Main category: cs.HC

TL;DR: 研究探讨了AI生成的目录描述是否能媲美人工质量，以及AI如何融入档案和博物馆的编目工作流程，指出AI需人类审核以确保准确性和质量。


<details>
  <summary>Details</summary>
Motivation: 摄影收藏快速增长，手动编目难以跟上，因此探索使用视觉语言模型（VLMs）自动生成元数据。

Method: 利用VLM（InternVL2）为考古照片生成目录描述，由专家和非专家评估分类、质量及使用意愿。

Result: AI生成的描述质量受OCR错误和幻觉限制，但准确性高的描述更难被分类；专家对AI工具的接受度较低。

Conclusion: 建议采用协作模式，AI生成初稿后由人类审核，建立透明可解释的流程以增强专业人士信任。

Abstract: The accelerating growth of photographic collections has outpaced manual
cataloguing, motivating the use of vision language models (VLMs) to automate
metadata generation. This study examines whether Al-generated catalogue
descriptions can approximate human-written quality and how generative Al might
integrate into cataloguing workflows in archival and museum collections. A VLM
(InternVL2) generated catalogue descriptions for photographic prints on
labelled cardboard mounts with archaeological content, evaluated by archive and
archaeology experts and non-experts in a human-centered, experimental
framework. Participants classified descriptions as AI-generated or
expert-written, rated quality, and reported willingness to use and trust in AI
tools. Classification performance was above chance level, with both groups
underestimating their ability to detect Al-generated descriptions. OCR errors
and hallucinations limited perceived quality, yet descriptions rated higher in
accuracy and usefulness were harder to classify, suggesting that human review
is necessary to ensure the accuracy and quality of catalogue descriptions
generated by the out-of-the-box model, particularly in specialized domains like
archaeological cataloguing. Experts showed lower willingness to adopt AI tools,
emphasizing concerns on preservation responsibility over technical performance.
These findings advocate for a collaborative approach where AI supports draft
generation but remains subordinate to human verification, ensuring alignment
with curatorial values (e.g., provenance, transparency). The successful
integration of this approach depends not only on technical advancements, such
as domain-specific fine-tuning, but even more on establishing trust among
professionals, which could both be fostered through a transparent and
explainable AI pipeline.

</details>


### [22] [Conjugated Capabilities: Interrelations of Elementary Human Capabilities and Their Implication on Human-Machine Task Allocation and Capability Testing Procedures](https://arxiv.org/abs/2507.07560)
*Nils Mandischer,Larissa Füller,Torsten Alles,Frank Flemisch,Lars Mikelsons*

Main category: cs.HC

TL;DR: 论文提出了一种称为“共轭能力”的概念，用于解决人机交互中人能力不足的问题，并通过数据分析和网络建模展示了其应用潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是让人机和自动化系统能够通过共轭能力优化任务分配和行为适应，以弥补人类的能力限制。

Method: 方法包括分析基本能力的相互关系、建立共轭能力网络图，并通过IMBA标准中的康复患者数据验证。

Result: 结果表明，共轭能力网络可用于优化测试设计和任务分配，展示了实际应用的可能性。

Conclusion: 论文结论指出，共轭能力的概念及其网络模型为人机交互的优化提供了新的工具和方法。

Abstract: Human and automation capabilities are the foundation of every human-autonomy
interaction and interaction pattern. Therefore, machines need to understand the
capacity and performance of human doing, and adapt their own behavior,
accordingly. In this work, we address the concept of conjugated capabilities,
i.e. capabilities that are dependent or interrelated and between which effort
can be distributed. These may be used to overcome human limitations, by
shifting effort from a deficient to a conjugated capability with performative
resources. For example: A limited arm's reach may be compensated by tilting the
torso forward. We analyze the interrelation between elementary capabilities
within the IMBA standard to uncover potential conjugation, and show evidence in
data of post-rehabilitation patients. From the conjugated capabilities, within
the example application of stationary manufacturing, we create a network of
interrelations. With this graph, a manifold of potential uses is enabled. We
showcase the graph's usage in optimizing IMBA test design to accelerate data
recordings, and discuss implications of conjugated capabilities on task
allocation between the human and an autonomy.

</details>


### [23] [Probing Experts' Perspectives on AI-Assisted Public Speaking Training](https://arxiv.org/abs/2507.07930)
*Nesrine Fourati,Alisa Barkar,Marion Dragée,Liv Danthon-Lefebvre,Mathieu Chollet*

Main category: cs.HC

TL;DR: 本文评价了专家对商业AI演讲训练工具的看法，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 探索专家对AI演讲训练工具的看法，以改进其设计和效果。

Method: 通过16次半结构化访谈和2次焦点小组，收集专家意见。

Result: 专家认同AI在技术训练中的价值，但指出需个性化反馈和清晰设计。

Conclusion: 支持传统教练与AI结合的混合模式。

Abstract: Background: Public speaking is a vital professional skill, yet it remains a
source of significant anxiety for many individuals. Traditional training relies
heavily on expert coaching, but recent advances in AI has led to novel types of
commercial automated public speaking feedback tools. However, most research has
focused on prototypes rather than commercial applications, and little is known
about how public speaking experts perceive these tools.
  Objectives: This study aims to evaluate expert opinions on the efficacy and
design of commercial AI-based public speaking training tools and to propose
guidelines for their improvement.
  Methods: The research involved 16 semi-structured interviews and 2 focus
groups with public speaking experts. Participants discussed their views on
current commercial tools, their potential integration into traditional
coaching, and suggestions for enhancing these systems.
  Results and Conclusions: Experts acknowledged the value of AI tools in
handling repetitive, technical aspects of training, allowing coaches to focus
on higher-level skills. However they found key issues in current tools,
emphasising the need for personalised, understandable, carefully selected
feedback and clear instructional design. Overall, they supported a hybrid model
combining traditional coaching with AI-supported exercises.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [24] [Generative Panoramic Image Stitching](https://arxiv.org/abs/2507.07133)
*Mathieu Tuli,Kaveh Kamali,David B. Lindell*

Main category: cs.GR

TL;DR: 提出了一种生成全景图像拼接的方法，通过微调扩散模型实现无缝全景合成。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接方法在处理视差、光线变化等问题时效果不佳，现有生成模型也无法合成大型连贯的全景图。

Method: 微调基于扩散的去噪模型，以保留场景内容和布局，从单张参考图像生成全景。

Result: 方法在图像质量和结构一致性上显著优于基线，适用于复杂场景。

Conclusion: 该方法有效解决了传统和现有生成模型的局限性，实现了高质量全景合成。

Abstract: We introduce the task of generative panoramic image stitching, which aims to
synthesize seamless panoramas that are faithful to the content of multiple
reference images containing parallax effects and strong variations in lighting,
camera capture settings, or style. In this challenging setting, traditional
image stitching pipelines fail, producing outputs with ghosting and other
artifacts. While recent generative models are capable of outpainting content
consistent with multiple reference images, they fail when tasked with
synthesizing large, coherent regions of a panorama. To address these
limitations, we propose a method that fine-tunes a diffusion-based inpainting
model to preserve a scene's content and layout based on multiple reference
images. Once fine-tuned, the model outpaints a full panorama from a single
reference image, producing a seamless and visually coherent result that
faithfully integrates content from all reference images. Our approach
significantly outperforms baselines for this task in terms of image quality and
the consistency of image structure and scene layout when evaluated on captured
datasets.

</details>


### [25] [LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS](https://arxiv.org/abs/2507.07136)
*Wanhua Li,Yujie Zhao,Minghan Qin,Yang Liu,Yuanhao Cai,Chuang Gan,Hanspeter Pfister*

Main category: cs.GR

TL;DR: LangSplatV2通过稀疏编码和高斯溅射技术，显著提升了3D语言场的高维特征处理速度和查询效率，实现了476.2 FPS的高维特征溅射和384.6 FPS的3D开放词汇查询速度。


<details>
  <summary>Details</summary>
Motivation: 现有LangSplat在高级GPU上仍无法实现实时推理性能（8.2 FPS），限制了其广泛应用。识别了重量级解码器为主要速度瓶颈，旨在通过稀疏编码技术消除这一瓶颈。

Method: 假设每个高斯作为全局字典中的稀疏编码，学习3D稀疏系数场，完全消除重量级解码器需求，并结合CUDA优化的高效稀疏系数溅射方法。

Result: LangSplatV2实现了42倍速度提升和47倍查询加速，同时保持了竞争性的查询精度。

Conclusion: LangSplatV2通过稀疏编码和高效溅射技术，显著提升了3D语言场的处理速度和实用性，为复杂场景中的语言交互应用提供了更高效的解决方案。

Abstract: In this paper, we introduce LangSplatV2, which achieves high-dimensional
feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6
FPS for high-resolution images, providing a 42 $\times$ speedup and a 47
$\times$ boost over LangSplat respectively, along with improved query accuracy.
LangSplat employs Gaussian Splatting to embed 2D CLIP language features into
3D, significantly enhancing speed and learning a precise 3D language field with
SAM semantics. Such advancements in 3D language fields are crucial for
applications that require language interaction within complex scenes. However,
LangSplat does not yet achieve real-time inference performance (8.2 FPS), even
with advanced A100 GPUs, severely limiting its broader application. In this
paper, we first conduct a detailed time analysis of LangSplat, identifying the
heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2
assumes that each Gaussian acts as a sparse code within a global dictionary,
leading to the learning of a 3D sparse coefficient field that entirely
eliminates the need for a heavyweight decoder. By leveraging this sparsity, we
further propose an efficient sparse coefficient splatting method with CUDA
optimization, rendering high-dimensional feature maps at high quality while
incurring only the time cost of splatting an ultra-low-dimensional feature. Our
experimental results demonstrate that LangSplatV2 not only achieves better or
competitive query accuracy but is also significantly faster. Codes and demos
are available at our project page: https://langsplat-v2.github.io.

</details>


### [26] [Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation](https://arxiv.org/abs/2507.07387)
*Chengan He,Jorge Alejandro Amador Herrera,Zhixin Shu,Xin Sun,Yao Feng,Sören Pirk,Dominik L. Michels,Meng Zhang,Tuanfeng Y. Wang,Julie Dorsey,Holly Rushmeier,Yi Zhou*

Main category: cs.GR

TL;DR: Digital Salon 是一个全面的3D头发创作系统，通过自然语言交互降低技术门槛，支持实时生成、模拟和渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法专注于3D头发建模的孤立部分，且计算量大或需要网络训练，Digital Salon旨在提供更整体和交互式的解决方案。

Method: 系统包含四个关键阶段：文本引导的头发检索、实时头发模拟、交互式头发细化及头发条件图像生成。

Result: 用户研究表明，系统在快速原型设计上优于传统工作流程，未来可能在真实沙龙环境中部署。

Conclusion: Digital Salon为不同技能水平的用户提供了高效、直观的头发建模工具，极大简化了数字媒体中的创意流程。

Abstract: We introduce Digital Salon, a comprehensive hair authoring system that
supports real-time 3D hair generation, simulation, and rendering. Unlike
existing methods that focus on isolated parts of 3D hair modeling and involve a
heavy computation process or network training, Digital Salon offers a holistic
and interactive system that lowers the technical barriers of 3D hair modeling
through natural language-based interaction. The system guides users through
four key stages: text-guided hair retrieval, real-time hair simulation,
interactive hair refinement, and hair-conditioned image generation. This
cohesive workflow makes advanced hair design accessible to users of varying
skill levels and dramatically streamlines the creative process in digital media
with an intuitive, versatile, and efficient solution for hair modeling. User
studies show that our system can outperform traditional hair modeling workflows
for rapid prototyping. Furthermore, we provide insights into the benefits of
our system with future potential of deploying our system in real salon
environments. More details can be found on our project page:
https://digital-salon.github.io/.

</details>


### [27] [Self-supervised Learning of Latent Space Dynamics](https://arxiv.org/abs/2507.07440)
*Yue Li,Gene Wei-Chin Lin,Egor Larionov,Aljaz Bozic,Doug Roble,Ladislav Kavan,Stelian Coros,Bernhard Thomaszewski,Tuur Stuyck,Hsiao-yu Chen*

Main category: cs.GR

TL;DR: 提出了一种基于神经潜在空间积分器的新型子空间模拟框架，以高效模拟变形物体的动态行为，适用于便携设备。


<details>
  <summary>Details</summary>
Motivation: 传统模拟方法计算成本高，子空间方法虽能提升性能，但仍难以满足便携设备的严格性能需求。

Method: 利用自监督学习增强推理稳定性和泛化能力，完全在潜在空间操作，避免全空间计算。

Result: 在杆、壳体和固体等复杂示例中展示了方法的有效性和广泛适用性。

Conclusion: 该方法高效且适合便携设备，具有广泛的应用潜力。

Abstract: Modeling the dynamic behavior of deformable objects is crucial for creating
realistic digital worlds. While conventional simulations produce high-quality
motions, their computational costs are often prohibitive. Subspace simulation
techniques address this challenge by restricting deformations to a
lower-dimensional space, improving performance while maintaining visually
compelling results. However, even subspace methods struggle to meet the
stringent performance demands of portable devices such as virtual reality
headsets and mobile platforms. To overcome this limitation, we introduce a
novel subspace simulation framework powered by a neural latent-space
integrator. Our approach leverages self-supervised learning to enhance
inference stability and generalization. By operating entirely within latent
space, our method eliminates the need for full-space computations, resulting in
a highly efficient method well-suited for deployment on portable devices. We
demonstrate the effectiveness of our approach on challenging examples involving
rods, shells, and solids, showcasing its versatility and potential for
widespread adoption.

</details>


### [28] [SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction](https://arxiv.org/abs/2507.07465)
*Wei Yao,Shuzhao Xie,Letian Li,Weixiang Zhang,Zhixin Lai,Shiqi Dai,Ke Zhang,Zhi Wang*

Main category: cs.GR

TL;DR: SD-GS 是一种高效动态高斯溅射框架，通过引入可变形锚点网格和变形感知的密集化策略，显著减小模型大小并提升渲染速度。


<details>
  <summary>Details</summary>
Motivation: 当前4D高斯框架在动态场景重建中存在存储成本与复杂运动表征能力的折中问题，限制了其实际应用。

Method: 提出了可变形锚点网格和变形感知的密集化策略，优化场景表示和运动建模。

Result: 相比现有方法，SD-GS 模型大小平均减少60%，FPS提升100%，计算效率显著提升且视觉质量更优。

Conclusion: SD-GS 通过高效表示和自适应策略，在动态场景重建中实现了存储和性能的显著优化。

Abstract: Current 4D Gaussian frameworks for dynamic scene reconstruction deliver
impressive visual fidelity and rendering speed, however, the inherent trade-off
between storage costs and the ability to characterize complex physical motions
significantly limits the practical application of these methods. To tackle
these problems, we propose SD-GS, a compact and efficient dynamic Gaussian
splatting framework for complex dynamic scene reconstruction, featuring two key
contributions. First, we introduce a deformable anchor grid, a hierarchical and
memory-efficient scene representation where each anchor point derives multiple
3D Gaussians in its local spatiotemporal region and serves as the geometric
backbone of the 3D scene. Second, to enhance modeling capability for complex
motions, we present a deformation-aware densification strategy that adaptively
grows anchors in under-reconstructed high-dynamic regions while reducing
redundancy in static areas, achieving superior visual quality with fewer
anchors. Experimental results demonstrate that, compared to state-of-the-art
methods, SD-GS achieves an average of 60\% reduction in model size and an
average of 100\% improvement in FPS, significantly enhancing computational
efficiency while maintaining or even surpassing visual quality.

</details>


### [29] [Capture Stage Environments: A Guide to Better Matting](https://arxiv.org/abs/2507.07623)
*Hannah Dröge,Janelle Pfeifer,Saskia Rabich,Markus Plack,Reinhard Klein,Matthias B. Hullin*

Main category: cs.GR

TL;DR: 该研究探讨了捕获舞台内容中图像抠图的挑战，提出了改进工作流程的指南，并展示了无需大量标注的高效管道。


<details>
  <summary>Details</summary>
Motivation: 现有的抠图算法在捕获舞台内容中表现不佳，研究旨在解决这些问题并分享改进方法。

Method: 提出了一个高效管道，利用现有算法适应捕获舞台的特殊需求，无需大量标注，同时提出基于扩散模型的验证方法。

Result: 验证方法显著改善了捕获舞台内容的抠图效果。

Conclusion: 研究为捕获舞台内容的抠图问题提供了实用指南和高效解决方案，适用于离线和实时场景。

Abstract: Capture stages are high-end sources of state-of-the-art recordings for
downstream applications in movies, games, and other media. One crucial step in
almost all pipelines is the matting of images to isolate the captured
performances from the background. While common matting algorithms deliver
remarkable performance in other applications like teleconferencing and mobile
entertainment, we found that they struggle significantly with the peculiarities
of capture stage content. The goal of our work is to share insights into those
challenges as a curated list of those characteristics along with a constructive
discussion for proactive intervention and present a guideline to practitioners
for an improved workflow to mitigate unresolved challenges. To this end, we
also demonstrate an efficient pipeline to adapt state-of-the-art approaches to
such custom setups without the need of extensive annotations, both offline and
real-time. For an objective evaluation, we propose a validation methodology
based on a leading diffusion model that highlights the benefits of our
approach.

</details>


### [30] [RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection](https://arxiv.org/abs/2507.07733)
*Yongyang Zhou,Fang-Lue Zhang,Zichen Wang,Lei Zhang*

Main category: cs.GR

TL;DR: 3DGS在反射物体渲染上存在挑战，RTR-GS提出了一种新框架，通过混合渲染模型有效解决此问题，并提升多种渲染任务的效果。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS在渲染反射物体时的局限性，特别是在逆渲染和重新光照方面。

Method: 提出了RTR-GS框架，结合前向渲染和延迟渲染，分离高频和低频外观，并细化BRDF和光照分解。

Result: 实验表明，该方法在视角合成、法线估计、分解和重新光照方面表现优异，且效率高。

Conclusion: RTR-GS显著提升了反射物体渲染的多任务表现，是一种高效且强大的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in
novel view synthesis. However, rendering reflective objects remains a
significant challenge, particularly in inverse rendering and relighting. We
introduce RTR-GS, a novel inverse rendering framework capable of robustly
rendering objects with arbitrary reflectance properties, decomposing BRDF and
lighting, and delivering credible relighting results. Given a collection of
multi-view images, our method effectively recovers geometric structure through
a hybrid rendering model that combines forward rendering for radiance transfer
with deferred rendering for reflections. This approach successfully separates
high-frequency and low-frequency appearances, mitigating floating artifacts
caused by spherical harmonic overfitting when handling high-frequency details.
We further refine BRDF and lighting decomposition using an additional
physically-based deferred rendering branch. Experimental results show that our
method enhances novel view synthesis, normal estimation, decomposition, and
relighting while maintaining efficient training inference process.

</details>


### [31] [Hi-d maps: An interactive visualization technique for multi-dimensional categorical data](https://arxiv.org/abs/2507.07890)
*Radi Muhammad Reza,Benjamin A Watson*

Main category: cs.GR

TL;DR: Hi-D maps是一种新颖的多维分类数据可视化方法，通过分层切割多边形并结合多种视觉线索，有效展示高维数据。


<details>
  <summary>Details</summary>
Motivation: 解决现有技术在高维数据可视化中效果不佳和空间效率低的问题。

Method: 将数据空间映射到2D多边形区域，分层切割并结合视觉线索（如方向、颜色、文本等）及交互浏览功能。

Result: 实现了对高维数据的清晰展示，并在交互中提供动态效果。

Conclusion: Hi-D maps在多维数据可视化中表现优异，但在维度过多时效果会下降。

Abstract: In this paper, we present Hi-D maps, a novel method for the visualization of
multi-dimensional categorical data. Our work addresses the scarcity of
techniques for visualizing a large number of data-dimensions in an effective
and space-efficient manner. We have mapped the full data-space onto a 2D
regular polygonal region. The polygon is cut hierarchically with lines parallel
to a user-controlled, ordered sequence of sides, each representing a dimension.
We have used multiple visual cues such as orientation, thickness, color,
countable glyphs, and text to depict cross-dimensional information. We have
added interactivity and hierarchical browsing to facilitate flexible
exploration of the display: small areas can be scrutinized for details. Thus,
our method is also easily extendable to visualize hierarchical information. Our
glyph animations add an engaging aesthetic during interaction. Like many
visualizations, Hi-D maps become less effective when a large number of
dimensions stresses perceptual limits, but Hi-D maps may add clarity before
those limits are reached.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [Distributed Training under Packet Loss](https://arxiv.org/abs/2507.07114)
*Erez Weintraub,Ron Banner,Ariel Orda*

Main category: cs.DC

TL;DR: 本文提出了一种新的分布式训练框架，能够在不可靠的网络连接下运行，保证梯度聚合的无偏性和参数漂移的有界性，从而解决传统框架在丢包时模型精度和收敛性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分布式框架依赖可靠网络连接，导致高延迟和可扩展性受限。不可靠连接虽降低延迟，但丢包会损害模型精度和收敛性，亟需一种端到端解决方案。

Method: 采用两阶段防御机制：1）无偏梯度聚合，通过重构梯度估计保证期望正确性；2）有界参数广播，确保模型差异有界。

Result: 在LLAMA2 7B模型和64 GPU实验中，容忍10%丢包时困惑度变化不超过0.8%。

Conclusion: 该框架填补了高效通信协议与大规模模型训练需求之间的鸿沟，为在普通或广域网络上实现鲁棒学习提供了可能。

Abstract: State-of-the-art language and vision models are routinely trained across
thousands of GPUs, often spanning multiple data-centers, yet today's
distributed frameworks still assume reliable connections (e.g., InfiniBand or
RoCE). The resulting acknowledgment traffic and retransmissions inflate tail
latencies and limit scalability. Leveraging unreliable connections will reduce
latency but may sacrifice model accuracy and convergence once packets are
dropped. A principled, end-to-end solution that preserves accuracy and
convergence guarantees under genuine packet loss has previously been missing.
We address this critical gap by introducing a novel distributed training
framework capable of operating over unreliable connections, offering unbiased
gradient aggregation and bounded parameter drift without modifying model code
or optimizers. The key insight is a two-stage defense against missing messages:
(i) Unbiased gradient aggregation: each worker reconstructs a consistent
gradient estimate from whatever packets arrive, guaranteeing expectation-level
correctness; and (ii) Bounded-drift parameter broadcasts: we prove the
inter-worker model discrepancy remains O(1) even after arbitrarily many
iterations, preventing the unbounded divergence typical of asynchronous setups.
Analytical bounds are matched by experiments on the LLAMA2 7B model with 64
GPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.
This work bridges the gap between communication-efficient datacenter protocols
and the accuracy and generalization guarantees demanded by modern large-model
training, enabling robust, high-throughput learning on commodity or wide-area
networks.

</details>


### [33] [Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces](https://arxiv.org/abs/2507.07116)
*Juan Cano-Benito,Andrea Cimmino,Sven Hertling,Heiko Paulheim,Raúl García-Castro*

Main category: cs.DC

TL;DR: 论文评估了不同分布式账本技术（DLT）在语义数据存储中的性能，发现私有DLT效率最高，混合DLT在可审计性和效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决去中心化数据空间中语义数据存储效率低的问题，以支持安全、可信的数据交换。

Method: 方法是通过实验对比公有、私有和混合DLT在存储、更新和查询语义数据时的性能和效率。

Result: 结果显示私有DLT最有效，混合DLT则在可审计性和效率间提供平衡。

Conclusion: 结论是为去中心化数据生态系统选择DLT基础设施时，需根据数据主权需求权衡效率和可审计性。

Abstract: Data spaces are emerging as decentralised infrastructures that enable
sovereign, secure, and trustworthy data exchange among multiple participants.
To achieve semantic interoperability within these environments, the use of
semantic web technologies and knowledge graphs has been proposed. Although
distributed ledger technologies (DLT) fit as the underlying infrastructure for
data spaces, there remains a significant gap in terms of the efficient storage
of semantic data on these platforms. This paper presents a systematic
evaluation of semantic data storage across different types of DLT (public,
private, and hybrid), using a real-world knowledge graph as an experimental
basis. The study compares performance, storage efficiency, resource
consumption, and the capabilities to update and query semantic data. The
results show that private DLTs are the most efficient for storing and managing
semantic content, while hybrid DLTs offer a balanced trade-off between public
auditability and operational efficiency. This research leads to a discussion on
the selection of the most appropriate DLT infrastructure based on the data
sovereignty requirements of decentralised data ecosystems.

</details>


### [34] [Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](https://arxiv.org/abs/2507.07223)
*Myoungsoo Jung*

Main category: cs.DC

TL;DR: 论文提出了一种基于CXL的模块化数据中心架构，以解决现代AI工作负载（如LLMs和RAG）在内存、通信带宽和资源灵活性方面的需求。通过优化XLink互连和分层内存模型，提升了AI基础设施的可扩展性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统GPU-centric架构因GPU间通信开销增加而难以扩展，无法满足现代AI工作负载的需求。

Method: 提出基于CXL的模块化架构，结合XLink优化互连和分层内存模型（本地与池化内存结合），并评估轻量级CXL实现、HBM和硅光子技术。

Result: 通过混合CXL-over-XLink设计减少长距离数据传输，同时保持内存一致性，提升了可扩展性、吞吐量和灵活性。

Conclusion: 新型架构有效解决了AI基础设施的扩展问题，为未来大规模AI应用提供了高效解决方案。

Abstract: Modern AI workloads such as large language models (LLMs) and
retrieval-augmented generation (RAG) impose severe demands on memory,
communication bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU communication
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in LLMs. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.

</details>


### [35] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: 分析了分布式高性能系统中机器学习任务的集体通信行为，提出了网络资源优化建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习任务中的集体通信操作可能引发高带宽和突发性流量，导致网络拥塞和性能下降，需要分析以优化资源分配。

Method: 利用Nvidia Collective Communication Library记录通信行为，调整并行度、节点数和模型类型等参数进行分析。

Result: 展示了DeepSeek V3推理模型的通信行为数据，包括操作类型、传输大小等，表明需改进通信框架和网络拓扑。

Conclusion: 现有集体通信框架和网络拓扑需调整以适应机器学习工作负载的网络异常影响。

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>


### [36] [Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](https://arxiv.org/abs/2507.07120)
*Nidhi Bhatia,Ankit More,Ritika Borkar,Tiyasa Mitra,Ramon Matas,Ritchie Zhao,Maximilian Golub,Dheevatsa Mudigere,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: Helix Parallelism通过混合执行策略优化LLM中的KV缓存和FFN计算，减少延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决长KV缓存和FFN权重读取的瓶颈问题，提高实时解码效率。

Method: 结合KV并行和TP/EP，通过轻量通信和批处理重叠优化。

Result: 在固定批量和延迟预算下，延迟降低1.5倍，批量支持32倍增长。

Conclusion: Helix显著提升了长序列实时推理的效率和可行性。

Abstract: As LLMs scale to multi-million-token KV histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long KV caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of KV heads, it leads to inefficient KV
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long KV histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies KV
parallelism during attention to shard KV caches across GPUs, then reuses the
same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
communication step. To minimize the exposed communication cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through
batchwise overlap, preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.

</details>


### [37] [Ampere: Communication-Efficient and High-Accuracy Split Federated Learning](https://arxiv.org/abs/2507.07130)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: Ampere是一种新型的联邦学习系统，通过减少设备计算和通信开销，同时提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 传统的分块联邦学习（SFL）系统虽然减轻了设备计算负担，但引入了大量通信开销，且对非独立同分布（non-IID）数据表现不佳。

Method: Ampere采用单向块间训练和轻量级辅助网络生成，减少梯度传输和中间交换，同时通过整合激活数据应对数据异构性。

Result: 实验表明，Ampere在模型精度、训练时间、通信和计算效率上均显著优于SFL基线系统。

Conclusion: Ampere在联邦学习中实现了高效性和鲁棒性的平衡，尤其适合处理异构数据。

Abstract: A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large communication overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server communication while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the communication overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and transformers show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server communication overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.

</details>


### [38] [M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure](https://arxiv.org/abs/2507.07144)
*Hongyi Xie,Min Zhou,Qiao Yu,Jialiang Yu,Zhenli Sheng,Hong Xie,Defu Lian*

Main category: cs.DC

TL;DR: 提出了一种多尺度分层内存故障预测框架M2-MFP，通过二进制空间特征提取器和双路径时序建模架构提升预测性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内存故障对云计算基础设施稳定性构成威胁，现有预测方法在泛化性和性能上存在不足。

Method: 将内存错误日志转换为多级二进制矩阵表示，使用二进制空间特征提取器和双路径时序建模架构（时间块模块和时间点模块）进行分析。

Result: 在基准数据集和实际部署中表现优异，显著超越现有方法。

Conclusion: M2-MFP框架能有效提升内存故障预测的可靠性和云计算基础设施的可用性。

Abstract: As cloud services become increasingly integral to modern IT infrastructure,
ensuring hardware reliability is essential to sustain high-quality service.
Memory failures pose a significant threat to overall system stability, making
accurate failure prediction through the analysis of memory error logs (i.e.,
Correctable Errors) imperative. Existing memory failure prediction approaches
have notable limitations: rule-based expert models suffer from limited
generalizability and low recall rates, while automated feature extraction
methods exhibit suboptimal performance. To address these limitations, we
propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction
framework designed to enhance the reliability and availability of cloud
infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level
binary matrix representations and introduces a Binary Spatial Feature Extractor
(BSFE) to automatically extract high-order features at both DIMM-level and
bit-level. Building upon the BSFE outputs, we develop a dual-path temporal
modeling architecture: 1) a time-patch module that aggregates multi-level
features within observation windows, and 2) a time-point module that employs
interpretable rule-generation trees trained on bit-level patterns. Experiments
on both benchmark datasets and real-world deployment show the superiority of
M$^2$-MFP as it outperforms existing state-of-the-art methods by significant
margins. Code and data are available at this repository:
https://github.com/hwcloud-RAS/M2-MFP.

</details>


### [39] [Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience](https://arxiv.org/abs/2507.07352)
*Loïc Pottier,Konstantia Georgouli,Timothy S. Carpenter,Fikret Aydin,Jeremy O. B. Tempkin,Dwight V. Nissley,Frederick H. Streitz,Thomas R. W. Scogland,Peer-Timo Bremer,Felice C. Lightstone,Helgi I. Ingólfsson*

Main category: cs.DC

TL;DR: 该论文介绍了多尺度机器学习建模基础设施（MuMMI）的轻量版mini-MuMMI，能够在小规模HPC系统或笔记本电脑上运行，用于管理多尺度分子动力学模拟工作流，并展示了其在RAS-RAF膜相互作用研究中的应用。


<details>
  <summary>Details</summary>
Motivation: 多尺度模型是模拟复杂现象的重要工具，但在不同时间和长度尺度之间建立桥梁一直是挑战。随着机器学习的发展，这一任务有了新的解决方案。MuMMI通过并行系统管理大规模模拟工作流，但其对高性能计算系统的依赖限制了其应用范围。

Method: 提出了mini-MuMMI，是MuMMI的轻量版本，适用于小规模HPC系统或笔记本电脑。通过优化工作流管理功能，mini-MuMMI能够高效地协调多尺度分子动力学模拟，同时在资源有限的系统中运行。

Result: mini-MuMMI成功应用于RAS-RAF膜相互作用的研究，展示了其在多尺度模拟中的实用性。研究还讨论了如何推广多尺度工作流并将其扩展到分子动力学以外的更广泛应用中。

Conclusion: mini-MuMMI为解决多尺度建模中的计算资源限制问题提供了可行方案，扩展了MuMMI的适用范围，为更广泛的应用领域提供了支持。

Abstract: Computational models have become one of the prevalent methods to model
complex phenomena. To accurately model complex interactions, such as detailed
biomolecular interactions, scientists often rely on multiscale models comprised
of several internal models operating at difference scales, ranging from
microscopic to macroscopic length and time scales. Bridging the gap between
different time and length scales has historically been challenging but the
advent of newer machine learning (ML) approaches has shown promise for tackling
that task. Multiscale models require massive amounts of computational power and
a powerful workflow management system. Orchestrating ML-driven multiscale
studies on parallel systems with thousands of nodes is challenging, the
workflow must schedule, allocate and control thousands of simulations operating
at different scales. Here, we discuss the massively parallel Multiscale
Machine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow
management infrastructure, that can orchestrate thousands of molecular dynamics
(MD) simulations operating at different timescales, spanning from millisecond
to nanosecond. More specifically, we introduce a novel version of MuMMI called
"mini-MuMMI". Mini-MuMMI is a curated version of MuMMI designed to run on
modest HPC systems or even laptops whereas MuMMI requires larger HPC systems.
We demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions
and discuss the different challenges behind the generalization of multiscale
workflows and how mini-MuMMI can be leveraged to target a broader range of
applications outside of MD and RAS-RAF interactions.

</details>


### [40] [KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](https://arxiv.org/abs/2507.07400)
*Zaifeng Pan,Ajjkumar Patel,Zhengding Hu,Yipeng Shen,Yue Guan,Wan-Lu Li,Lianhui Qin,Yida Wang,Yufei Ding*

Main category: cs.DC

TL;DR: KVFlow是一个针对基于大型语言模型的多智能体工作流的KV缓存管理框架，通过智能预测和预加载提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有系统使用LRU策略管理KV缓存，无法预测未来使用导致频繁缓存未命中和计算开销。

Method: KVFlow通过Agent Step Graph预测智能体激活时间，并引入重叠KV预取机制。

Result: 相比现有方法，KVFlow在单工作流和多并发场景下分别实现1.83倍和2.19倍加速。

Conclusion: KVFlow显著提升了基于LLM的多智能体工作流的服务效率。

Abstract: Large language model (LLM) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing LLM systems employ prefix caching to
reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict KV caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards KV caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present KVFlow, a workflow-aware KV
cache management framework tailored for agentic workloads. KVFlow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the KV node
level, allowing KVFlow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a
fully overlapped KV prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.

</details>


### [41] [Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems](https://arxiv.org/abs/2507.07671)
*Jovan Prodanov,Blaž Bertalanič,Carolina Fortuna,Shih-Kai Chou,Matjaž Branko Jurič,Ramon Sanchez-Iborra,Jernej Hribar*

Main category: cs.DC

TL;DR: 论文提出了一种基于多智能体强化学习的资源动态扩展引擎（MARLISE），相较于传统静态阈值方法，能更高效地优化边缘-云系统的资源利用和性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统静态阈值方法在动态负载环境下资源扩展效率低下的问题。

Method: 采用深度强化学习算法（DQN和PPO）开发MARLISE，实现动态、反应式的资源扩展。

Result: 实验表明，MARLISE在处理动态负载时，能保证微服务的低响应时间，同时显著提高资源效率。

Conclusion: MARLISE在资源动态扩展方面优于传统启发式方法，适合边缘-云系统的需求。

Abstract: Modern edge-cloud systems face challenges in efficiently scaling resources to
handle dynamic and unpredictable workloads. Traditional scaling approaches
typically rely on static thresholds and predefined rules, which are often
inadequate for optimizing resource utilization and maintaining performance in
distributed and dynamic environments. This inefficiency hinders the
adaptability and performance required in edge-cloud infrastructures, which can
only be achieved through the newly proposed in-place scaling. To address this
problem, we propose the Multi-Agent Reinforcement Learning-based In-place
Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with
in-place resource scaling. We develop our solution using two Deep Reinforcement
Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization
(PPO). We analyze each version of the proposed MARLISE solution using dynamic
workloads, demonstrating their ability to ensure low response times of
microservices and scalability. Our results show that MARLISE-based approaches
outperform heuristic method in managing resource elasticity while maintaining
microservice response times and achieving higher resource efficiency.

</details>


### [42] [KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling](https://arxiv.org/abs/2507.07932)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Qiang Guan,Hailong Jiang*

Main category: cs.DC

TL;DR: KIS-S框架通过结合GPU感知的模拟器和PPO驱动的自动缩放器，优化Kubernetes中GPU推理工作负载的动态扩展表现。


<details>
  <summary>Details</summary>
Motivation: 解决Kubernetes默认Horizontal Pod Autoscaler（HPA）在面对动态突发流量和缺乏GPU指标时的不足。

Method: KIS-S结合KISim（GPU感知模拟器）和KIScaler（基于PPO的自动缩放器），在模拟中学习延迟感知且资源高效的扩展策略。

Result: 实验显示，KIScaler平均奖励提高75.2%，P95延迟降低6.7倍，且在无需重新训练的情况下通用性强。

Conclusion: 研究填补了反应式自动扩展与智能编排在GPU加速环境中的差距。

Abstract: Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [43] [Algorithmic Complexity Attacks on All Learned Cardinality Estimators: A Data-centric Approach](https://arxiv.org/abs/2507.07438)
*Yingze Li,Xianglong Liu,Dong Wang,Zixuan Wang,Hongzhi Wang,Kaixing Zhang,Yiming Guan*

Main category: cs.DB

TL;DR: 论文研究了学习型基数估计器在数据漂移下的脆弱性，提出了数据级的黑盒攻击方法，并设计了一种多项式时间近似算法。实验证明攻击效果显著，同时也提出了防御措施。


<details>
  <summary>Details</summary>
Motivation: 学习型基数估计器在实际部署中易受数据漂移影响，本研究旨在揭示其脆弱性并为开发稳健优化器提供理论支持。

Method: 提出了一种数据级黑盒攻击方法，证明最优攻击策略是NP-Hard问题，并设计了一种多项式时间近似算法。

Result: 实验表明，仅修改0.8%的训练数据即可使90分位数Qerror增加三个数量级，端到端处理时间最多增加20倍。

Conclusion: 研究揭示了学习型基数估计器的关键漏洞，首次提供了其数据更新脆弱性的统一分析，并提出了两种防御措施。

Abstract: Learned cardinality estimators show promise in query cardinality prediction,
yet they universally exhibit fragility to training data drifts, posing risks
for real-world deployment. This work is the first to theoretical investigate
how minimal data-level drifts can maximally degrade the accuracy of learned
estimators. We propose data-centric algorithmic complexity attacks against
learned estimators in a black-box setting, proving that finding the optimal
attack strategy is NP-Hard. To address this, we design a polynomial-time
approximation algorithm with a $(1-\kappa)$ approximation ratio. Extensive
experiments demonstrate our attack's effectiveness: on STATS-CEB and IMDB-JOB
benchmarks, modifying just 0.8\% of training tuples increases the 90th
percentile Qerror by three orders of magnitude and raises end-to-end processing
time by up to 20$\times$. Our work not only reveals critical vulnerabilities in
deployed learned estimators but also provides the first unified worst-case
theoretical analysis of their fragility under data updates. Additionally, we
identify two countermeasures to mitigate such black-box attacks, offering
insights for developing robust learned database optimizers.

</details>


### [44] [JOB-Complex: A Challenging Benchmark for Traditional & Learned Query Optimization](https://arxiv.org/abs/2507.07471)
*Johannes Wehrstein,Timo Eckmann,Roman Heinrich,Carsten Binnig*

Main category: cs.DB

TL;DR: 该论文提出现有的查询优化基准测试（如JOB基准）存在局限性，无法反映真实世界的复杂性，因此作者提出了新的基准测试JOB-Complex，以挑战传统和学习的查询优化器。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试未能体现真实世界中查询优化的复杂特性（如字符串列连接或复杂过滤条件），导致对传统和学习优化器性能的高估。

Method: 作者提出JOB-Complex基准测试，包含30个SQL查询和近6000个执行计划，旨在模拟真实场景的复杂性。

Result: 实验显示，传统和学习的成本模型在JOB-Complex上表现不佳，运行时间比最优计划慢11倍。

Conclusion: JOB-Complex为评估查询优化器和成本模型提供了更真实的标准，揭示了现有方法的局限性。

Abstract: Query optimization is a fundamental task in database systems that is crucial
to providing high performance. To evaluate learned and traditional optimizer's
performance, several benchmarks, such as the widely used JOB benchmark, are
used. However, in this paper, we argue that existing benchmarks are inherently
limited, as they do not reflect many real-world properties of query
optimization, thus overstating the performance of both traditional and learned
optimizers. In fact, simple but realistic properties, such as joins over string
columns or complex filter predicates, can drastically reduce the performance of
existing query optimizers. Thus, we introduce JOB-Complex, a new benchmark
designed to challenge traditional and learned query optimizers by reflecting
real-world complexity. Overall, JOB-Complex contains 30 SQL queries and comes
together with a plan-selection benchmark containing nearly 6000 execution
plans, making it a valuable resource to evaluate the performance of query
optimizers and cost models in real-world scenarios. In our evaluation, we show
that traditional and learned cost models struggle to achieve high performance
on JOB-Complex, providing a runtime of up to 11x slower compared to the optimal
plans.

</details>


### [45] [A Service Architecture for Dataspaces](https://arxiv.org/abs/2507.07979)
*Benedikt T. Arnold,Christoph Lange,Christina Gillmann,Stefan Decker*

Main category: cs.DB

TL;DR: 论文提出了一种在数据空间中提供通用服务的抽象层，并展示了其在EDC Connector中的实现。


<details>
  <summary>Details</summary>
Motivation: 尽管数据空间主要关注数据资产交换，但实际上对基于数据空间的服务有需求，概念上也需要将其纳入数据空间。

Method: 提出一个抽象层，支持在数据空间中轻松开发与现有技术无缝集成的服务，并以EDC Connector为例进行实现。

Result: 展示了该服务架构的初步实现及实际适用性。

Conclusion: 抽象层的提出为数据空间服务的开发提供了便利，扩展了数据空间的功能。

Abstract: Dataspaces are designed to support sovereign, trusted and decentralized data
exchange between participants forming an ecosystem. They are standardized by
initiatives such as the International Data Spaces Association or Gaia-X and
have gained adoption in several domains such as mobility, manufacturing,
tourism or culture. In dataspaces, participants use connectors to communicate
peer-to-peer. The Eclipse Dataspace Components (EDC) Connector is a broadly
adopted, open-source implementation that adheres to the standards and is
supported by a large community. As dataspaces in general, it focuses on the
exchange of data assets with associated usage policies and does not support
services. In practice, however, there is demand for dataspace-based services
and conceptual arguments support their inclusion in dataspaces. In this paper,
we propose an abstraction layer for providing generic services within
dataspaces. Adopters can use this layer to easily develop own services,
seamlessly integrated with the existing dataspace technology. Besides, we
present an initial implementation of this service architecture for the EDC
Connector and demonstrate its practical applicability.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [46] [Accelerating Transposed Convolutions on FPGA-based Edge Devices](https://arxiv.org/abs/2507.07683)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 论文提出了一种硬件-软件协同设计的加速器MM2IM，通过结合矩阵乘法与col2IM优化转置卷积层的计算效率，在资源受限的边缘设备上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的输入导向映射方法在转置卷积层中存在输出映射复杂、计算重叠和无效计算等问题，导致性能瓶颈，尤其是在资源受限的边缘设备上。

Method: 提出名为MM2IM的加速器，结合矩阵乘法与col2IM方法，针对转置卷积层进行优化，并通过SECDA-TFLite设计工具包实现和评估。

Result: 在261种转置卷积配置中，MM2IM平均加速比达1.9倍，部分生成模型加速比高达4.2倍，且能效比显著优于其他加速器。

Conclusion: MM2IM显著提升了资源受限设备上生成模型的性能，验证了其高效性与实用性。

Abstract: Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Bias-Aware Mislabeling Detection via Decoupled Confident Learning](https://arxiv.org/abs/2507.07216)
*Yunyi Li,Maria De-Arteaga,Maytal Saar-Tsechansky*

Main category: cs.LG

TL;DR: 本文提出了一种名为DeCoLe的新框架，用于检测标签偏见引起的数据标记错误，特别适用于仇恨言论检测等领域。


<details>
  <summary>Details</summary>
Motivation: 标签偏见是数据完整性的重要挑战，现有方法稀缺，亟需有效解决方案。

Method: 提出了基于机器学习的DeCoLe框架，理论证明其有效性，并在仇恨言论检测中评估性能。

Result: DeCoLe在偏见感知的标记错误检测方面表现优异，优于其他方法。

Conclusion: DeCoLe为数据质量管理提供了强大工具，可提升数据可靠性。

Abstract: Reliable data is a cornerstone of modern organizational systems. A notable
data integrity challenge stems from label bias, which refers to systematic
errors in a label, a covariate that is central to a quantitative analysis, such
that its quality differs across social groups. This type of bias has been
conceptually and empirically explored and is widely recognized as a pressing
issue across critical domains. However, effective methodologies for addressing
it remain scarce. In this work, we propose Decoupled Confident Learning
(DeCoLe), a principled machine learning based framework specifically designed
to detect mislabeled instances in datasets affected by label bias, enabling
bias aware mislabelling detection and facilitating data quality improvement. We
theoretically justify the effectiveness of DeCoLe and evaluate its performance
in the impactful context of hate speech detection, a domain where label bias is
a well documented challenge. Empirical results demonstrate that DeCoLe excels
at bias aware mislabeling detection, consistently outperforming alternative
approaches for label error detection. Our work identifies and addresses the
challenge of bias aware mislabeling detection and offers guidance on how DeCoLe
can be integrated into organizational data management practices as a powerful
tool to enhance data reliability.

</details>


### [48] [CHOMET: Conditional Handovers via Meta-Learning](https://arxiv.org/abs/2507.07581)
*Michail Kalntis,Fernando A. Kuipers,George Iosifidis*

Main category: cs.LG

TL;DR: 论文提出了一种基于O-RAN范式的元学习框架，用于优化3GPP提出的条件切换（CHO）技术，显著提高了在信号波动条件下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 随着移动网络复杂化和小区小型化，传统切换技术面临延迟增加和失败率升高的问题。3GPP引入条件切换（CHO）来缓解这些问题，但CHO仍存在资源分配效率和信令开销等新挑战。

Method: 论文提出了一种基于O-RAN范式的元学习框架，通过动态优化CHO的资源分配和信令管理，解决了CHO的挑战。

Result: 该框架在信号波动条件下表现出色，性能比3GPP基准方案提升了至少180%，并提供了鲁棒的动态后悔保证。

Conclusion: 该研究为CHO的优化提供了一种高效且可扩展的解决方案，显著提升了移动网络的切换性能和用户体验。

Abstract: Handovers (HOs) are the cornerstone of modern cellular networks for enabling
seamless connectivity to a vast and diverse number of mobile users. However, as
mobile networks become more complex with more diverse users and smaller cells,
traditional HOs face significant challenges, such as prolonged delays and
increased failures. To mitigate these issues, 3GPP introduced conditional
handovers (CHOs), a new type of HO that enables the preparation (i.e., resource
allocation) of multiple cells for a single user to increase the chance of HO
success and decrease the delays in the procedure. Despite its advantages, CHO
introduces new challenges that must be addressed, including efficient resource
allocation and managing signaling/communication overhead from frequent cell
preparations and releases. This paper presents a novel framework aligned with
the O-RAN paradigm that leverages meta-learning for CHO optimization, providing
robust dynamic regret guarantees and demonstrating at least 180% superior
performance than other 3GPP benchmarks in volatile signal conditions.

</details>


### [49] [Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data](https://arxiv.org/abs/2507.07589)
*Arpana Sinhal,Anay Sinhal,Amit Sinhal*

Main category: cs.LG

TL;DR: 该研究通过多模态数据集和机器学习模型解决了医疗工作者压力监测的不足，提出了可部署的压力监测系统。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情期间，医护人员压力问题突出，而现有压力监测技术因数据集和分析框架不足难以应对。

Method: 结合SMOTE预处理技术，使用随机森林、XGBoost和MLP模型，并整合为堆叠分类器。

Result: 通过公开数据集和可复现分析流程，开发了实用的压力监测系统。

Conclusion: 未来需扩大样本多样性并探索边缘计算以实现低延迟压力预警。

Abstract: Healthcare professionals, particularly nurses, face elevated occupational
stress, a concern amplified during the COVID-19 pandemic. While wearable
sensors offer promising avenues for real-time stress monitoring, existing
studies often lack comprehensive datasets and robust analytical frameworks.
This study addresses these gaps by introducing a multimodal dataset comprising
physiological signals, electrodermal activity, heart rate and skin temperature.
A systematic literature review identified limitations in prior stress-detection
methodologies, particularly in handling class imbalance and optimizing model
generalizability. To overcome these challenges, the dataset underwent
preprocessing with the Synthetic Minority Over sampling Technique (SMOTE),
ensuring balanced representation of stress states. Advanced machine learning
models including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were
evaluated and combined into a Stacking Classifier to leverage their collective
predictive strengths. By using a publicly accessible dataset and a reproducible
analytical pipeline, this work advances the development of deployable
stress-monitoring systems, offering practical implications for safeguarding
healthcare workers' mental health. Future research directions include expanding
demographic diversity and exploring edge-computing implementations for low
latency stress alerts.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [50] [Discrete Beamforming Optimization for RISs with a Limited Phase Range and Amplitude Attenuation](https://arxiv.org/abs/2507.07342)
*Dogan Kutay Pekcan,Hongyi Liao,Ender Ayanoglu*

Main category: eess.SP

TL;DR: 论文研究了通过RIS最大化用户设备接收功率的问题，提出了最优搜索算法和量化框架，分析了离散相位数和相位范围对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 研究通过RIS优化接收功率的问题，特别是在存在PDA和离散相位限制的情况下，需要找到最优解的条件和方法。

Method: 提出了最优搜索算法和APQ量化框架，并扩展为EAPQ算法，通过几何投影和封闭式表达式分析性能。

Result: 发现增加离散相位数超过K=4时收益甚微，性能受相位范围R和衰减水平的复杂影响，且最优算法可作为基准。

Conclusion: 研究为RIS的离散波束成形提供了理论支持和实用工具，揭示了性能与相位范围及衰减的关系。

Abstract: This paper addresses the problem of maximizing the received power at a user
equipment via reconfigurable intelligent surface (RIS) characterized by
phase-dependent amplitude (PDA) and discrete phase shifts over a limited phase
range. Given complex RIS coefficients, that is, discrete phase shifts and PDAs,
we derive the necessary and sufficient conditions to achieve the optimal
solution. To this end, we propose an optimal search algorithm that is proven to
converge in linear time within at most NK steps, significantly outperforming
the exhaustive search approach that would otherwise be needed for RISs with
amplitude attenuation. Furthermore, we introduce a practical quantization
framework for PDA-introduced RISs termed amplitude-introduced polar
quantization (APQ), and extend it to a novel algorithm named extended
amplitude-introduced polar quantization (EAPQ) that works with geometric
projections. We derive closed-form expressions to assess how closely the
performance of the proposed RIS configuration can approximate the ideal case
with continuous phases and no attenuation. Our analysis reveals that increasing
the number of discrete phases beyond K = 4 yields only marginal gains,
regardless of attenuation levels, provided the RIS has a sufficiently wide
phase range R. Furthermore, we also show and quantify that when the phase range
R is limited, the performance is sensitive to attenuation for larger R, and
sensitive to R when there is less attenuation. Finally, the proposed optimal
algorithm provides a generic upper bound that could serve as a benchmark for
discrete beamforming in RISs with amplitude constraints.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 论文探讨了使用神经符号方法检测供应链中的非法活动，比较了手动和自动特征提取的新闻文章分类效果。


<details>
  <summary>Details</summary>
Motivation: 供应链中的非法活动（如假冒零件、强迫劳动）数据稀疏且易被篡改，传统机器学习方法难以应对，需要新的解决方案。

Method: 研究了神经符号方法，并提出了一种基于大型语言模型（LLM）的问题树方法，用于识别和量化新闻文章的相关性。

Result: 比较了人类和机器对新闻文章分类的差异，为供应链中的非法活动检测提供了系统性评估。

Conclusion: 神经符号方法结合LLM可有效解决数据稀疏问题，提升供应链非法活动的检测能力。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [52] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 论文分析了规则型机器学习模型在解释预测时的潜在问题，如负面重叠和冗余，并开发了算法来识别这些问题。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，错误的解释可能误导决策者，因此需要严格评估规则型模型的解释质量。

Method: 开发算法分析规则型模型中的负面重叠和冗余等不受欢迎的特征。

Result: 研究发现，广泛使用的规则型学习工具会导致规则集出现这些负面特征。

Conclusion: 现有规则型学习工具可能导致解释质量不佳，需进一步改进。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

TL;DR: 本文提出一种多模态实体链接模型MMoE，通过多级专家混合机制动态选择模态信息，解决现有方法中的提及模糊性和模态内容动态选择问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能解决提及模糊性和模态内容动态选择问题，阻碍了多模态实体链接的性能提升。

Method: MMoE模型利用大语言模型增强提及描述，提取多模态特征，并通过多级专家混合机制动态选择特征。

Result: 实验表明，MMoE在性能上显著优于现有方法。

Conclusion: MMoE通过动态选择和增强多模态信息，有效提升了多模态实体链接的准确性。

Abstract: Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [54] [T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates](https://arxiv.org/abs/2507.07633)
*Zhitao Wang,Hengyu Man,Wenrui Li,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于轨迹引导的生成式视频编码框架（T-GVC），通过语义感知的稀疏运动采样和轨迹对齐的损失约束，在超低码率（ULB）下实现高质量视频重建。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频编码方法受限于领域特异性或对高级文本引导的过度依赖，难以捕捉运动细节。

Method: T-GVC采用语义感知稀疏运动采样管道，结合轨迹对齐的损失约束，在不牺牲生成模型能力的情况下实现物理合理的运动模式。

Result: 实验表明，T-GVC在ULB条件下优于传统编解码器和端到端压缩方法，且运动控制更精确。

Conclusion: T-GVC为基于几何运动建模的生成式视频编码开辟了新方向。

Abstract: Recent advances in video generation techniques have given rise to an emerging
paradigm of generative video coding, aiming to achieve semantically accurate
reconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong
generative priors. However, most existing methods are limited by domain
specificity (e.g., facial or human videos) or an excessive dependence on
high-level text guidance, which often fails to capture motion details and
results in unrealistic reconstructions. To address these challenges, we propose
a Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC
employs a semantic-aware sparse motion sampling pipeline to effectively bridge
low-level motion tracking with high-level semantic understanding by extracting
pixel-wise motion as sparse trajectory points based on their semantic
importance, not only significantly reducing the bitrate but also preserving
critical temporal semantic information. In addition, by incorporating
trajectory-aligned loss constraints into diffusion processes, we introduce a
training-free latent space guidance mechanism to ensure physically plausible
motion patterns without sacrificing the inherent capabilities of generative
models. Experimental results demonstrate that our framework outperforms both
traditional codecs and state-of-the-art end-to-end video compression methods
under ULB conditions. Furthermore, additional experiments confirm that our
approach achieves more precise motion control than existing text-guided
methods, paving the way for a novel direction of generative video coding guided
by geometric motion modeling.

</details>


### [55] [SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs](https://arxiv.org/abs/2507.07610)
*Siting Wang,Luoyang Sun,Cheng Deng,Kun Shao,Minnan Pei,Zheng Tian,Haifeng Zhang,Jun Wang*

Main category: cs.CV

TL;DR: 提出了SpatialViz-Bench，一个评估空间可视化能力的多模态基准测试，表明当前多模态大语言模型在此任务上仍存在缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法通常将空间可视化能力嵌入到更广泛的数学和逻辑测试中，且可能受训练数据重叠影响，因此需要更专门的评估工具。

Method: 开发了包含12个任务和1180个自动生成问题的SpatialViz-Bench基准测试，用于全面评估空间可视化能力。

Result: 评估33个最先进的多模态大语言模型，发现了性能差异大、与人类直觉不符的困难感知、2D到3D性能骤降等问题。

Conclusion: SpatialViz-Bench填补了领域空白，展示了当前模型在空间可视化任务中的不足。

Abstract: Humans can directly imagine and manipulate visual images in their minds, a
capability known as spatial visualization. While multi-modal Large Language
Models (MLLMs) support imagination-based reasoning, spatial visualization
remains insufficiently evaluated, typically embedded within broader
mathematical and logical assessments. Existing evaluations often rely on IQ
tests or math competitions that may overlap with training data, compromising
assessment reliability. To this end, we introduce SpatialViz-Bench, a
comprehensive multi-modal benchmark for spatial visualization with 12 tasks
across 4 sub-abilities, comprising 1,180 automatically generated problems. Our
evaluation of 33 state-of-the-art MLLMs not only reveals wide performance
variations and demonstrates the benchmark's strong discriminative power, but
also uncovers counter-intuitive findings: models exhibit unexpected behaviors
by showing difficulty perception that misaligns with human intuition,
displaying dramatic 2D-to-3D performance cliffs, and defaulting to formula
derivation despite spatial tasks requiring visualization alone. SpatialVizBench
empirically demonstrates that state-of-the-art MLLMs continue to exhibit
deficiencies in spatial visualization tasks, thereby addressing a significant
lacuna in the field. The benchmark is publicly available.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [56] [A 2-categorical approach to the semantics of dependent type theory with computation axioms](https://arxiv.org/abs/2507.07208)
*Matteo Spadetto*

Main category: math.LO

TL;DR: 论文提出了一种基于2维范畴语义的方法来描述公理类型论的语义，扩展了Garner对内涵类型的处理，证明其解释是合理的，并通过语义方法验证了内涵恒等类型的计算规则在公理类型论中不可成立。


<details>
  <summary>Details</summary>
Motivation: 公理类型论缺乏计算规则，现有的范畴语义方法难以直接应用。本文旨在解决这一挑战，从2维范畴视角提供其语义的有效描述。

Method: 采用Richard Garner的2维方法，将公理类型论的语义编码为自然的2维范畴数据，定义了显示映射2-范畴模型，并放宽了对内涵类型的限制。

Result: 证明了公理类型论在2维范畴模型中的解释是合理的，且具有完备性。通过重构Hofmann和Streicher的群模型，验证了内涵恒等类型计算规则的不可行性。

Conclusion: 研究扩展了Garner的工作，为公理类型论提供了新的语义框架，并通过具体模型验证了其独特性。

Abstract: Axiomatic type theory is a dependent type theory without computation rules.
The term equality judgements that usually characterise these rules are replaced
by computation axioms, i.e., additional term judgements that are typed by
identity types. This paper is devoted to providing an effective description of
its semantics, from a higher categorical perspective: given the challenge of
encoding intensional type formers into 1-dimensional categorical terms and
properties, a challenge that persists even for axiomatic type formers, we adopt
Richard Garner's approach in the 2-dimensional study of dependent types. We
prove that the type formers of axiomatic theories can be encoded into natural
2-dimensional category theoretic data, obtaining a presentation of the
semantics of axiomatic type theory via 2-categorical models called display map
2-categories. In the axiomatic case, the 2-categorical requirements identified
by Garner for interpreting intensional type formers are relaxed. Therefore, we
obtain a presentation of the semantics of the axiomatic theory that generalises
Garner's one for the intensional case. Our main result states that the
interpretation of axiomatic theories within display map 2-categories is
well-defined and enjoys the soundness property. We use this fact to provide a
semantic proof that the computation rule of intensional identity types is not
admissible in axiomatic type theory. This is achieved via a revisitation of
Hofmann and Streicher's groupoid model that believes axiomatic identity types
but does not believe intensional ones.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [57] [Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance](https://arxiv.org/abs/2507.07881)
*Roberto Ulloa,Juhi Kulshrestha,Celina Kacperski*

Main category: cs.CY

TL;DR: 研究发现，教育水平较低的人群更容易回避使用对话式AI工具，强调了包容性设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨对话式AI工具是否会因教育水平差异导致数字不平等加剧。

Method: 通过在线实验（N=1,636）和行为数据分析，结合UTAUT2理论和LASSO回归。

Result: 对话式AI组的任务回避率（51%）显著高于传统搜索组（30.9%）和对照组（16.8%），教育水平较低的参与者回避率最高（74.4%）。

Conclusion: 教育水平对AI工具的采纳有显著影响，需通过包容性设计减少不平等。

Abstract: The rise of conversational AI (CAI), powered by large language models, is
transforming how individuals access and interact with digital information.
However, these tools may inadvertently amplify existing digital inequalities.
This study investigates whether differences in formal education are associated
with CAI avoidance, leveraging behavioral data from an online experiment (N =
1,636). Participants were randomly assigned to a control or an
information-seeking task, either a traditional online search or a CAI
(Perplexity AI). Task avoidance (operationalized as survey abandonment or
providing unrelated responses during task assignment) was significantly higher
in the CAI group (51%) compared to the search (30.9%) and control (16.8%)
groups, with the highest CAI avoidance among participants with lower education
levels (~74.4%). Structural equation modeling based on the theoretical
framework UTAUT2 and LASSO regressions reveal that education is strongly
associated with CAI avoidance, even after accounting for various cognitive and
affective predictors of technology adoption. These findings underscore
education's central role in shaping AI adoption and the role of self-selection
biases in AI-related research, stressing the need for inclusive design to
ensure equitable access to emerging technologies.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [58] [Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task](https://arxiv.org/abs/2507.07327)
*Brian B. Vuong,Josie Davidson,Sangheui Cheon,Kyujin Cho,Allison M. Okamura*

Main category: cs.RO

TL;DR: 研究提出将触觉反馈从手部移至手腕，以避免与手术机器人操纵杆的直接交互冲突，并验证了手腕触觉反馈在提高操作力准确性上的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决触觉反馈与手术机器人操纵杆交互冲突的问题，探索手腕触觉反馈在手术中的潜在优势。

Method: 使用软气动手腕触觉装置，通过实验比较有无触觉反馈时参与者操作达芬奇手术机器人执行组织触诊任务的力准确性。

Result: 提供手腕触觉反馈时，参与者施加的力误差显著降低，但操作时间延长。

Conclusion: 手腕触觉反馈能有效提高力准确性，但需权衡操作速度与准确性的关系。

Abstract: Previous work has shown that the addition of haptic feedback to the hands can
improve awareness of tool-tissue interactions and enhance performance of
teleoperated tasks in robot-assisted minimally invasive surgery. However,
hand-based haptic feedback occludes direct interaction with the manipulanda of
surgeon console in teleoperated surgical robots. We propose relocating haptic
feedback to the wrist using a wearable haptic device so that haptic feedback
mechanisms do not need to be integrated into the manipulanda. However, it is
unknown if such feedback will be effective, given that it is not co-located
with the finger movements used for manipulation. To test if relocated haptic
feedback improves force application during teleoperated tasks using da Vinci
Research Kit (dVRK) surgical robot, participants learned to palpate a phantom
tissue to desired forces. A soft pneumatic wrist-worn haptic device with an
anchoring system renders tool-tissue interaction forces to the wrist of the
user. Participants performed the palpation task with and without wrist-worn
haptic feedback and were evaluated for the accuracy of applied forces.
Participants demonstrated statistically significant lower force error when
wrist-worn haptic feedback was provided. Participants also performed the
palpation task with longer movement times when provided wrist-worn haptic
feedback, indicating that the haptic feedback may have caused participants to
operate at a different point in the speed-accuracy tradeoff curve.

</details>


### [59] [FiDTouch: A 3D Wearable Haptic Display for the Finger Pad](https://arxiv.org/abs/2507.07661)
*Daria Trinitatova,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: FiDTouch是一种3D可穿戴触觉设备，通过微型倒立Delta机器人提供精确的触觉反馈，提升人机交互体验。


<details>
  <summary>Details</summary>
Motivation: 指尖触觉设备在虚拟现实、医疗训练和远程机器人操作等领域有广泛应用潜力，可提升用户体验和培训效果。

Method: 采用微型倒立Delta机器人设计，提供精确的接触和快速变化的动态刺激。通过用户研究评估其静态空间接触和皮肤拉伸刺激的感知效果。

Result: FiDTouch能提供精准的触觉和力刺激，增强人机和人机交互的沉浸感和效率。

Conclusion: FiDTouch展示了在高精度触觉反馈领域的潜力，适用于多种交互场景。

Abstract: The applications of fingertip haptic devices have spread to various fields
from revolutionizing virtual reality and medical training simulations to
facilitating remote robotic operations, proposing great potential for enhancing
user experiences, improving training outcomes, and new forms of interaction. In
this work, we present FiDTouch, a 3D wearable haptic device that delivers
cutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin
stretch, and vibrotactile feedback. The application of a tiny inverted Delta
robot in the mechanism design allows providing accurate contact and fast
changing dynamic stimuli to the finger pad surface. The performance of the
developed display was evaluated in a two-stage user study of the perception of
static spatial contact stimuli and skin stretch stimuli generated on the finger
pad. The proposed display, by providing users with precise touch and force
stimuli, can enhance user immersion and efficiency in the fields of
human-computer and human-robot interactions.

</details>


### [60] [A Graph Isomorphism-based Decentralized Algorithm for Modular Robot Configuration Formation](https://arxiv.org/abs/1602.03104)
*Ayan Dutta,Prithviraj Dasgupta,Carl Nelson*

Main category: cs.RO

TL;DR: 提出了一种基于图同构的算法，用于模块化机器人系统中的配置形成问题，通过效用框架选择目标位置，减少时间和能耗，并证明了算法的完备性和帕累托最优性。


<details>
  <summary>Details</summary>
Motivation: 解决模块化机器人系统中初始配置不同的模块如何高效移动到用户指定目标配置的问题。

Method: 基于图同构和效用框架的算法，模块选择目标位置时尽可能保留原始配置。

Result: 算法具有完备性和帕累托最优性，实验显示规划时间短（100模块仅需毫秒级），性能优于市场分配算法。

Conclusion: 提出的算法在时间和通信效率上优于现有方法，适用于模块化机器人系统的高效配置形成。

Abstract: We consider the problem of configuration formation in modular robot systems
where a set of modules that are initially in different configurations and
located at different locations are required to assume appropriate positions so
that they can get into a new, user-specified, target configuration. We propose
a novel algorithm based on graph isomorphism, where the modules select
locations or spots in the target configuration using a utility-based framework,
while retaining their original configuration to the greatest extent possible,
to reduce the time and energy required by the modules to assume the target
configuration. We have shown analytically that our proposed algorithm is
complete and guarantees a Pareto-optimal allocation. Experimental simulations
of our algorithm with different number of modules in different initial
configurations and located initially at different locations, show that the
planning time of our algorithm is nominal (order of msec. for 100 modules). We
have also compared our algorithm against a market-based allocation algorithm
and shown that our proposed algorithm performs better in terms of time and
number of messages exchanged.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [61] [The Richness of CSP Non-redundancy](https://arxiv.org/abs/2507.07942)
*Joshua Brakensiek,Venkatesan Guruswami,Bart M. P. Jansen,Victor Lagerkvist,Magnus Wahlström*

Main category: cs.DM

TL;DR: 该论文研究了约束满足问题（CSP）中的非冗余性（NRD），展示了如何通过有限CSP谓词表达不同复杂度的非冗余性，并探索了条件非冗余性的分类与代数理论。


<details>
  <summary>Details</summary>
Motivation: 非冗余性在计算机科学与数学中具有重要地位，与稀疏化、核化、查询复杂度等问题紧密相关。论文旨在深入理解非冗余性及其应用。

Method: 通过构造有限CSP谓词展示非冗余性的多项式复杂度，并基于极值组合学中的高围长图理论对二元谓词的条件非冗余性进行分类。进一步，发展了条件非冗余性的代数理论。

Result: 证明了存在有限CSP谓词的非冗余性为Θ(n^r)，且分类了所有二元谓词的条件非冗余性。代数理论的应用揭示了Mal'tsev嵌入的新例子。

Conclusion: 非冗余性是多个重要问题的核心，论文通过理论与代数方法深化了对其的理解，并提供了新的应用例子。

Abstract: In the field of constraint satisfaction problems (CSP), a clause is called
redundant if its satisfaction is implied by satisfying all other clauses. An
instance of CSP$(P)$ is called non-redundant if it does not contain any
redundant clause. The non-redundancy (NRD) of a predicate $P$ is the maximum
number of clauses in a non-redundant instance of CSP$(P)$, as a function of the
number of variables $n$. Recent progress has shown that non-redundancy is
crucially linked to many other important questions in computer science and
mathematics including sparsification, kernelization, query complexity,
universal algebra, and extremal combinatorics. Given that non-redundancy is a
nexus for many of these important problems, the central goal of this paper is
to more deeply understand non-redundancy.
  Our first main result shows that for every rational number $r \ge 1$, there
exists a finite CSP predicate $P$ such that the non-redundancy of $P$ is
$\Theta(n^r)$. Our second main result explores the concept of conditional
non-redundancy first coined by Brakensiek and Guruswami [STOC 2025]. We
completely classify the conditional non-redundancy of all binary predicates
(i.e., constraints on two variables) by connecting these non-redundancy
problems to the structure of high-girth graphs in extremal combinatorics.
  Inspired by these concrete results, we build off the work of Carbonnel [CP
2022] to develop an algebraic theory of conditional non-redundancy. As an
application of this algebraic theory, we revisit the notion of Mal'tsev
embeddings, which is the most general technique known to date for establishing
that a predicate has linear non-redundancy. For example, we provide the first
example of predicate with a Mal'tsev embedding that cannot be attributed to the
structure of an Abelian group, but rather to the structure of the quantum Pauli
group.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [62] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj,Brindha Raghuraman,Nagarani Gopalakrishnan,Yazan Otoum*

Main category: cs.CR

TL;DR: 本文提出了一种混合AI驱动的网络安全框架，用于增强关键基础设施的实时漏洞检测、威胁建模和自动修复能力。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施系统对社会稳定和经济韧性至关重要，但其日益互联的特性使其面临多种网络威胁，如勒索软件、拒绝服务攻击和高级持续性威胁。

Method: 研究分析了网络安全的脆弱性，包括威胁态势、攻击途径，并提出了一种混合AI驱动的网络安全框架。

Result: 研究提供了可操作的见解，以增强关键基础设施系统对新兴网络威胁的安全性和韧性。

Conclusion: AI在关键基础设施网络安全中具有重要作用，但也需解决对抗性AI、法规遵从性和集成复杂性等问题。

Abstract: Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [63] [Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling](https://arxiv.org/abs/2507.07250)
*Jordi Serra-Ruiz,David Megías*

Main category: cs.CR

TL;DR: 提出了一种多波段图像的半脆弱水印方案，采用树状结构矢量量化方法嵌入水印，通过迭代算法检测图像篡改。


<details>
  <summary>Details</summary>
Motivation: 为了防止遥感图像在传输或存储过程中被篡改，需要一种既能抵抗有损压缩又能检测篡改的水印方案。

Method: 将多光谱或高光谱图像的像素签名通过树状结构矢量量化处理，分割为三维块，并迭代调整以满足水印嵌入条件。

Result: 该方案能够在有损压缩（超过阈值）时保留水印，同时检测出篡改区域及其位置。

Conclusion: 所提方法有效实现了对遥感图像的半脆弱水印保护，兼具抗压缩和篡改检测能力。

Abstract: A semi-fragile watermarking scheme for multiple band images is presented in
this article. We propose to embed a mark into remote sensing images applying a
tree-structured vector quantization approach to the pixel signatures instead of
processing each band separately. The signature of the multispectral or
hyperspectral image is used to embed the mark in it order to detect any
significant modification of the original image. The image is segmented into
three-dimensional blocks, and a tree-structured vector quantizer is built for
each block. These trees are manipulated using an iterative algorithm until the
resulting block satisfies a required criterion, which establishes the embedded
mark. The method is shown to be able to preserve the mark under lossy
compression (above a given threshold) but, at the same time, it detects
possibly forged blocks and their position in the whole image.

</details>


### [64] [Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations](https://arxiv.org/abs/2507.07916)
*Federico Maria Cau,Giuseppe Desolda,Francesco Greco,Lucio Davide Spano,Luca Viganò*

Main category: cs.CR

TL;DR: 论文研究了大型语言模型（LLMs）能否生成清晰、简洁且可扩展的网络钓鱼警告解释，并通过用户实验验证其效果优于人工生成内容。


<details>
  <summary>Details</summary>
Motivation: 现有网络钓鱼警告对话因解释不清晰和内容静态导致效果有限，需要更有效的解决方案。

Method: 进行大规模用户实验（N=750），比较人工生成与LLM（Claude 3.5 Sonnet和Llama 3.3 70B）生成的两种解释风格对用户行为的影响。

Result: LLM生成的解释在降低钓鱼攻击易感性上表现优异，Claude效果尤为突出；特征解释对真实钓鱼更有效，反事实解释减少误报。

Conclusion: LLM可用于自动生成高效的钓鱼警告解释，具有可扩展性和适应性，符合以人为本的价值观。

Abstract: Phishing has become a prominent risk in modern cybersecurity, often used to
bypass technological defences by exploiting predictable human behaviour.
Warning dialogues are a standard mitigation measure, but the lack of
explanatory clarity and static content limits their effectiveness. In this
paper, we report on our research to assess the capacity of Large Language
Models (LLMs) to generate clear, concise, and scalable explanations for
phishing warnings. We carried out a large-scale between-subjects user study (N
= 750) to compare the influence of warning dialogues supplemented with manually
generated explanations against those generated by two LLMs, Claude 3.5 Sonnet
and Llama 3.3 70B. We investigated two explanatory styles (feature-based and
counterfactual) for their effects on behavioural metrics (click-through rate)
and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that
well-constructed LLM-generated explanations can equal or surpass manually
crafted explanations in reducing susceptibility to phishing; Claude-generated
warnings exhibited particularly robust performance. Feature-based explanations
were more effective for genuine phishing attempts, whereas counterfactual
explanations diminished false-positive rates. Other variables such as workload,
gender, and prior familiarity with warning dialogues significantly moderated
warning effectiveness. These results indicate that LLMs can be used to
automatically build explanations for warning users against phishing, and that
such solutions are scalable, adaptive, and consistent with human-centred
values.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [65] [Audio-Visual Speech Separation via Bottleneck Iterative Network](https://arxiv.org/abs/2507.07270)
*Sidong Zhang,Shiv Shankar,Trang Nguyen,Andrea Fanelli,Madalina Fiterau*

Main category: cs.SD

TL;DR: 提出了Bottleneck Iterative Network (BIN)，通过轻量级融合块和瓶颈融合表示提升语音分离模型性能，减少训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 整合非听觉线索信息可提升语音分离模型性能，但现有方法模型过大或性能不足，需要平衡性能与成本。

Method: 提出BIN方法，通过轻量级融合块和瓶颈融合表示迭代优化模型。

Result: 在NTCD-TIMIT和LRS3+WHAM!数据集上，BIN在SI-SDRi指标上优于现有模型，训练和推理时间减少50%以上。

Conclusion: BIN在性能与成本间取得平衡，是一种高效的语音分离方法。

Abstract: Integration of information from non-auditory cues can significantly improve
the performance of speech-separation models. Often such models use deep
modality-specific networks to obtain unimodal features, and risk being too
costly or lightweight but lacking capacity. In this work, we present an
iterative representation refinement approach called Bottleneck Iterative
Network (BIN), a technique that repeatedly progresses through a lightweight
fusion block, while bottlenecking fusion representations by fusion tokens. This
helps improve the capacity of the model, while avoiding major increase in model
size and balancing between the model performance and training cost. We test BIN
on challenging noisy audio-visual speech separation tasks, and show that our
approach consistently outperforms state-of-the-art benchmark models with
respect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously
achieving a reduction of more than 50% in training and GPU inference time
across nearly all settings.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [66] [Toolchain for Faster Iterations in Quantum Software Development](https://arxiv.org/abs/2507.07448)
*Otso Kinanen,Andrés D. Muñoz-Moller,Vlad Stirbu,Tommi Mikkonen*

Main category: quant-ph

TL;DR: 论文探讨了如何利用远程计算能力优化量子软件开发工作流，通过降低本地执行与远程硬件之间的转换门槛，提升执行速度。


<details>
  <summary>Details</summary>
Motivation: 量子计算潜力巨大，但开发者在开发量子软件时面临硬件限制、计算需求高和技术栈复杂等挑战。

Method: 提出利用远程计算能力，优化工作流，支持更复杂的量子电路设计和迭代开发。

Result: 实验结果显示，方案实现了高达5倍的电路执行加速，支持21至29量子位的开发。

Conclusion: 远程计算能力能显著提升量子软件开发效率，支持更复杂的应用开发。

Abstract: Quantum computing proposes a revolutionary paradigm that can radically
transform numerous scientific and industrial application domains. To realize
this promise, these new capabilities need software solutions that are able to
effectively harness its power. However, developers may face significant
challenges when developing and executing quantum software due to the limited
availability of quantum computer hardware, high computational demands of
simulating quantum computers on classical systems, and complicated technology
stack to enable currently available accelerators into development environments.
These limitations make it difficult for the developer to create an efficient
workflow for quantum software development. In this paper, we investigate the
potential of using remote computational capabilities in an efficient manner to
improve the workflow of quantum software developers, by lowering the barrier of
moving between local execution and computationally more efficient remote
hardware and offering speedup in execution with simulator surroundings. The
goal is to allow the development of more complex circuits and to support an
iterative software development approach. In our experiment, with the solution
presented in this paper, we have obtained up to 5 times faster circuit
execution runtime, and enabled qubit ranges from 21 to 29 qubits with a simple
plug-and-play kernel for the Jupyter notebook.

</details>


### [67] [Quantum Executor: A Unified Interface for Quantum Computing](https://arxiv.org/abs/2507.07597)
*Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: quant-ph

TL;DR: Quantum Executor是一个后端无关的执行引擎，支持跨异构平台进行量子实验，提供声明式和模块化接口，解耦实验设计与后端执行。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算从理论走向实践，需要更强大、可移植且可扩展的量子软件实验工具。

Method: 设计了Quantum Executor，支持异步和分布式执行，提供统一的API和管理工具。

Result: 通过自动基准测试和混合验证等场景展示了其适用性，能够简化量子开发流程。

Conclusion: 讨论了当前局限性，并提出了未来改进的路线图。

Abstract: As quantum computing evolves from theoretical promise to practical
deployment, the demand for robust, portable, and scalable tools for quantum
software experimentation is growing. This paper introduces Quantum Executor, a
backend-agnostic execution engine designed to orchestrate quantum experiments
across heterogeneous platforms. Quantum Executor provides a declarative and
modular interface that decouples experiment design from backend execution,
enabling seamless interoperability and code reuse across diverse quantum and
classical resources. Key features include support for asynchronous and
distributed execution, customizable execution strategies and a unified API for
managing quantum experiments. We illustrate its applicability through two
life-like usage scenarios such as automated benchmarking and hybrid validation,
discussing its capacity to streamline quantum development. We conclude by
discussing current limitations and outlining a roadmap for future enhancements.

</details>


### [68] [ProvideQ: A Quantum Optimization Toolbox](https://arxiv.org/abs/2507.07649)
*Domenik Eichhorn,Nick Poser,Maximilian Schweikart,Ina Schaefer*

Main category: quant-ph

TL;DR: ProvideQ工具箱通过元求解器策略实现了经典与量子计算的混合求解器，解决了技术栈整合的难题，但其性能还需更先进的硬件支持。


<details>
  <summary>Details</summary>
Motivation: 混合求解器在理论上性能优越，但实际应用中缺乏整合经典与量子计算的技术栈。

Method: 提供ProvideQ工具箱，支持用户通过元求解器配置工具动态分解问题为经典与量子子程序。

Result: 概念验证展示了量子子程序的可行性，但性能仍需提升硬件支持。

Conclusion: ProvideQ工具箱为混合求解提供了实用工具，但硬件发展是其性能提升的关键。

Abstract: Hybrid solvers for combinatorial optimization problems combine the advantages
of classical and quantum computing to overcome difficult computational
challenges. Although their theoretical performance seems promising, their
practical applicability is challenging due to the lack of a technological stack
that can seamlessly integrate quantum solutions with existing classical
optimization frameworks. We tackle this challenge by introducing the ProvideQ
toolbox, a software tool that enables users to easily adapt and configure
hybrid solvers via Meta-Solver strategies. A Meta-Solver strategy implements
decomposition techniques, which splits problems into classical and quantum
subroutines. The ProvideQ toolbox enables the interactive creation of such
decompositions via a Meta-Solver configuration tool. It combines
well-established classical optimization techniques with quantum circuits that
are seamlessly executable on multiple backends. This paper introduces the
technical details of the ProvideQ toolbox, explains its architecture, and
demonstrates possible applications for several real-world use cases. Our proof
of concept shows that Meta-Solver strategies already enable the application of
quantum subroutines today, however, more sophisticated hardware is required to
make their performance competitive.

</details>
