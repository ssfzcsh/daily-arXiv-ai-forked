<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 12]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 9]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 9]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [eess.IV](#eess.IV) [Total: 1]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [You Don't Know Until You Click:Automated GUI Testing for Production-Ready Software Evaluation](https://arxiv.org/abs/2508.14104)
*Yutong Bian,Xianhao Lin,Yupeng Xie,Tianyang Liu,Mingchen Zhuge,Siyuan Lu,Haoming Tang,Jinlin Wang,Jiayi Zhang,Jiaqi Chen,Xiangru Tang,Yongxin Ni,Sirui Hong,Chenglin Wu*

Main category: cs.SE

TL;DR: 介绍了一个名为RealDevWorld的新评估框架，用于自动化端到端评估LLMs生成生产级软件的能力，包含任务集和交互式评估系统。


<details>
  <summary>Details</summary>
Motivation: 当前评测方法无法全面评估LLMs生成的交互式软件的实际可用性，需填补这一空白。

Method: 提出RealDevBench任务集和AppEvalPilot交互评估系统，模拟真实用户行为进行自动评估。

Result: 框架评估准确率达0.92，与专家评估相关性为0.85，显著减少人工审查需求。

Conclusion: RealDevWorld实现了高效、自动化且与人类评估一致的软件评测，支持LLMs生成生产级软件的规模评估。

Abstract: Large Language Models (LLMs) and code agents in software development are
rapidly evolving from generating isolated code snippets to producing
full-fledged software applications with graphical interfaces, interactive
logic, and dynamic behaviors. However, current benchmarks fall short in
evaluating such production-ready software, as they often rely on static checks
or binary pass/fail scripts, failing to capture the interactive behaviors and
runtime dynamics that define real-world usability - qualities that only emerge
when an application is actively used. This is the blind spot of current
evaluation: you don't know if an app works until you click through it, interact
with it, and observe how it responds. To bridge this gap, we introduce
RealDevWorld, a novel evaluation framework for automated end-to-end assessment
of LLMs' ability to generate production-ready repositories from scratch. It
features two key components: (1) RealDevBench, a diverse collection of 194
open-ended software engineering tasks across multiple domains, incorporating
multimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a
new agent-as-a-judge evaluation system that simulates realistic, GUI-based user
interactions to automatically and holistically assess software functional
correctness, visual fidelity, and runtime behavior. The framework delivers
fine-grained, task-specific diagnostic feedback, supporting nuanced evaluation
beyond simple success/failure judgments. Empirical results show that
RealDevWorld delivers effective, automatic, and human-aligned evaluations,
achieving an accuracy of 0.92 and a correlation of 0.85 with expert human
assessments, while significantly reducing the reliance on manual review. This
enables scalable, human-aligned assessment of production-level software
generated by LLMs. Our code is available on GitHub.

</details>


### [2] [Ambiguity Resolution with Human Feedback for Code Writing Tasks](https://arxiv.org/abs/2508.14114)
*Aditey Nandan,Viraj Kumar*

Main category: cs.SE

TL;DR: 论文提出了一种基于ARHF技术的系统，帮助程序员识别和解决代码任务描述中的歧义性问题。


<details>
  <summary>Details</summary>
Motivation: 由于代码任务描述通常以自然语言表达且可能存在歧义，程序员需学会识别并解决这些歧义，系统旨在辅助这一过程。

Method: 系统通过ARHF技术：(1)识别可能引发歧义的输入，(2)获取有限的人类反馈，(3)利用反馈生成解决歧义的代码。

Result: 评估了系统原型的效果。

Conclusion: 讨论了此类辅助系统对计算机科学教育的潜在影响。

Abstract: Specifications for code writing tasks are usually expressed in natural
language and may be ambiguous. Programmers must therefore develop the ability
to recognize ambiguities in task specifications and resolve them by asking
clarifying questions. We present and evaluate a prototype system, based on a
novel technique (ARHF: Ambiguity Resolution with Human Feedback), that (1)
suggests specific inputs on which a given task specification may be ambiguous,
(2) seeks limited human feedback about the code's desired behavior on those
inputs, and (3) uses this feedback to generate code that resolves these
ambiguities. We evaluate the efficacy of our prototype, and we discuss the
implications of such assistive systems on Computer Science education.

</details>


### [3] [Measuring LLM Code Generation Stability via Structural Entropy](https://arxiv.org/abs/2508.14288)
*Yewei Song,Tiezhu Sun,Xunzhu Tang,Prateek Rajput,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.SE

TL;DR: 该论文提出了一种基于抽象语法树（AST）和熵的方法，用于评估大型语言模型（LLMs）在代码生成中的稳定性。通过两种互补的度量方式（Jensen-Shannon散度和结构交叉熵比），该方法无需参考样本或执行代码，即可分析模型的鲁棒性和一致性。


<details>
  <summary>Details</summary>
Motivation: 为了在真实开发中判断LLMs生成代码的可靠性，需要评估其生成代码的稳定性。现有的评价指标（如pass@k、BLEU、CodeBLEU）依赖参考样本或执行结果，无法完全满足需求，因此作者提出了一个新的评估框架。

Method: 利用抽象语法树（AST）的深度限制子树作为输入源，计算其相对频率分布，并引入Jensen-Shannon散度和结构交叉熵比两种度量指标。方法支持仅结构分析和标记级分析，运行时间为O(n,d)。

Result: 论文在标准代码生成任务上测试了多个主流LLMs，结果表明AST驱动的结构熵能揭示模型的一致性和鲁棒性细节。

Conclusion: 该方法无需外部参考或执行代码，提供了一种轻量级的代码生成稳定性评估工具，适用于多种编程语言。

Abstract: Assessing the stability of code generation from large language models (LLMs)
is essential for judging their reliability in real-world development. We extend
prior "structural-entropy concepts" to the program domain by pairing entropy
with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the
multiset of depth-bounded subtrees of AST in each generated program and treat
their relative frequencies as a probability distribution. We then measure
stability in two complementary ways: (i) Jensen-Shannon divergence, a
symmetric, bounded indicator of structural overlap, and (ii) a Structural
Cross-Entropy ratio that highlights missing high-probability patterns. Both
metrics admit structural-only and token-aware variants, enabling separate views
on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or
CodeBLEU, our metrics are reference-free, language-agnostic, and
execution-independent. We benchmark several leading LLMs on standard code
generation tasks, demonstrating that AST-driven structural entropy reveals
nuances in model consistency and robustness. The method runs in O(n,d) time
with no external tests, providing a lightweight addition to the code-generation
evaluation toolkit.

</details>


### [4] [Static Analysis as a Feedback Loop: Enhancing LLM-Generated Code Beyond Correctness](https://arxiv.org/abs/2508.14419)
*Scott Blyth,Sherlock A. Licorish,Christoph Treude,Markus Wagner*

Main category: cs.SE

TL;DR: 论文通过静态分析驱动的迭代提示方法，显著提升了大型语言模型在生成高质量代码方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的代码生成基准测试（如HumanEval和MBPP）主要关注功能正确性，忽视了代码安全性、可靠性、可读性和可维护性等更广泛的维度。

Method: 提出了一种基于Bandit和Pylint的迭代静态分析驱动提示算法，用于识别和修复代码质量问题。

Result: 实验表明，GPT-4o在十次迭代后显著改善了代码质量：安全性问题从>40%降至13%，可读性违规从>80%降至11%，可靠性警告从>50%降至11%。

Conclusion: 研究表明，通过静态分析反馈引导，大型语言模型可以显著提升代码质量，而不仅仅是功能正确性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
code generation, achieving high scores on benchmarks such as HumanEval and
MBPP. However, these benchmarks primarily assess functional correctness and
neglect broader dimensions of code quality, including security, reliability,
readability, and maintainability. In this work, we systematically evaluate the
ability of LLMs to generate high-quality code across multiple dimensions using
the PythonSecurityEval benchmark. We introduce an iterative static
analysis-driven prompting algorithm that leverages Bandit and Pylint to
identify and resolve code quality issues. Our experiments with GPT-4o show
substantial improvements: security issues reduced from >40% to 13%, readability
violations from >80% to 11%, and reliability warnings from >50% to 11% within
ten iterations. These results demonstrate that LLMs, when guided by static
analysis feedback, can significantly enhance code quality beyond functional
correctness.

</details>


### [5] [Design and Evaluation of a Scalable Data Pipeline for AI-Driven Air Quality Monitoring in Low-Resource Settings](https://arxiv.org/abs/2508.14451)
*Richard Sserujongi,Daniel Ogenrwot,Nicholas Niwamanya,Noah Nsimbe,Martin Bbaale,Benjamin Ssempala,Noble Mutabazi,Raja Fidel Wabinyai,Deo Okure,Engineer Bainomugisha*

Main category: cs.SE

TL;DR: 论文介绍了AirQo数据管道的设计、实现与评估，这是一个模块化、云原生的ETL系统，支持非洲城市中异构空气质量数据的实时和批量处理。


<details>
  <summary>Details</summary>
Motivation: 低成本环境传感器和AI应用的普及推动了可扩展、弹性数据基础设施的需求，尤其是在数据稀缺和资源受限的地区。

Method: 采用Apache Airflow、Apache Kafka和Google BigQuery等开源技术构建模块化ETL系统，整合多种数据流，实现自动校准、预测和分析。

Result: 每月处理来自400多个设备的海量数据，实现低延迟、高吞吐和强健的数据可用性。

Conclusion: 通过开源平台和部署经验分享，为资源受限环境下的环境数据工程提供了可复用的参考方案。

Abstract: The increasing adoption of low-cost environmental sensors and AI-enabled
applications has accelerated the demand for scalable and resilient data
infrastructures, particularly in data-scarce and resource-constrained regions.
This paper presents the design, implementation, and evaluation of the AirQo
data pipeline: a modular, cloud-native Extract-Transform-Load (ETL) system
engineered to support both real-time and batch processing of heterogeneous air
quality data across urban deployments in Africa. It is Built using open-source
technologies such as Apache Airflow, Apache Kafka, and Google BigQuery. The
pipeline integrates diverse data streams from low-cost sensors, third-party
weather APIs, and reference-grade monitors to enable automated calibration,
forecasting, and accessible analytics. We demonstrate the pipeline's ability to
ingest, transform, and distribute millions of air quality measurements monthly
from over 400 monitoring devices while achieving low latency, high throughput,
and robust data availability, even under constrained power and connectivity
conditions. The paper details key architectural features, including workflow
orchestration, decoupled ingestion layers, machine learning-driven sensor
calibration, and observability frameworks. Performance is evaluated across
operational metrics such as resource utilization, ingestion throughput,
calibration accuracy, and data availability, offering practical insights into
building sustainable environmental data platforms. By open-sourcing the
platform and documenting deployment experiences, this work contributes a
reusable blueprint for similar initiatives seeking to advance environmental
intelligence through data engineering in low-resource settings.

</details>


### [6] [What You See Is What It Does: A Structural Pattern for Legible Software](https://arxiv.org/abs/2508.14511)
*Eagon Meng,Daniel Jackson*

Main category: cs.SE

TL;DR: 本文探讨了LLM编码器带来的机遇和局限性，提出了新的软件结构模式以提高代码的可读性和模块化。


<details>
  <summary>Details</summary>
Motivation: 当前软件常因缺乏直接对应性和模块化不足，导致增量性、完整性和透明性等关键需求无法满足。

Method: 提出了一种新结构模式，基于概念和同步机制（独立服务和事件规则），并设计了领域特定语言以实现声明式行为表达。

Result: 通过RealWorld基准案例研究验证了该方法的有效性。

Conclusion: 新结构模式能显著提升软件的清晰性和模块化，适应LLM生成的需求。

Abstract: The opportunities offered by LLM coders (and their current limitations)
demand a reevaluation of how software is structured. Software today is often
"illegible" - lacking a direct correspondence between code and observed
behavior - and insufficiently modular, leading to a failure of three key
requirements of robust coding: incrementality (the ability to deliver small
increments by making localized changes), integrity (avoiding breaking prior
increments) and transparency (making clear what has changed at build time, and
what actions have happened at runtime).
  A new structural pattern offers improved legibility and modularity. Its
elements are concepts and synchronizations: fully independent services and
event-based rules that mediate between them. A domain-specific language for
synchronizations allows behavioral features to be expressed in a granular and
declarative way (and thus readily generated by an LLM). A case study of the
RealWorld benchmark is used to illustrate and evaluate the approach.

</details>


### [7] [Preguss: It Analyzes, It Specifies, It Verifies](https://arxiv.org/abs/2508.14532)
*Zhongyi Wang,Tengjie Lin,Mingshuai Chen,Mingqi Yang,Haokun Li,Xiao Yi,Shengchao Qin,Jianwei Yin*

Main category: cs.SE

TL;DR: Preguss是一个模块化框架，结合静态分析和演绎验证，通过LLM辅助生成和优化形式化规范，旨在提升大规模程序验证的自动化程度。


<details>
  <summary>Details</summary>
Motivation: 目标是实现大规模软件和硬件系统的全自动化验证，当前LLM在形式化验证中的应用因上下文限制和复杂规范推断困难而难以扩展。

Method: Preguss框架结合静态分析和演绎验证，通过RTE引导的验证单元构建与优先级排序，以及LLM辅助的单元级规范合成。

Result: Preguss为大规模程序自动化验证提供了一条可行路径。

Conclusion: Preguss通过模块化和细粒度方法，显著提升了形式化规范生成和验证的自动化水平。

Abstract: Fully automated verification of large-scale software and hardware systems is
arguably the holy grail of formal methods. Large language models (LLMs) have
recently demonstrated their potential for enhancing the degree of automation in
formal verification by, e.g., generating formal specifications as essential to
deductive verification, yet exhibit poor scalability due to context-length
limitations and, more importantly, the difficulty of inferring complex,
interprocedural specifications. This paper outlines Preguss - a modular,
fine-grained framework for automating the generation and refinement of formal
specifications. Preguss synergizes between static analysis and deductive
verification by orchestrating two components: (i) potential runtime error
(RTE)-guided construction and prioritization of verification units, and (ii)
LLM-aided synthesis of interprocedural specifications at the unit level. We
envisage that Preguss paves a compelling path towards the automated
verification of large-scale programs.

</details>


### [8] [Post-hoc LLM-Supported Debugging of Distributed Processes](https://arxiv.org/abs/2508.14540)
*Dennis Schiese,Andreas Both*

Main category: cs.SE

TL;DR: 提出了一种利用生成式AI和系统过程数据生成自然语言解释的方法，以简化复杂分布式系统中的手动调试过程。


<details>
  <summary>Details</summary>
Motivation: 解决手动调试资源密集且部分过时的问题，特别是在复杂分布式系统中。

Method: 结合系统过程数据和生成式AI，生成自然语言解释，帮助开发者理解系统行为和潜在错误。

Result: 开发了一个基于组件化Java系统的演示工具，方法语言无关，并提供开源Web应用。

Conclusion: 该方法为开发者提供了高效理解系统的方式，即使不熟悉系统细节也能有效调试。

Abstract: In this paper, we address the problem of manual debugging, which nowadays
remains resource-intensive and in some parts archaic. This problem is
especially evident in increasingly complex and distributed software systems.
Therefore, our objective of this work is to introduce an approach that can
possibly be applied to any system, at both the macro- and micro-level, to ease
this debugging process. This approach utilizes a system's process data, in
conjunction with generative AI, to generate natural-language explanations.
These explanations are generated from the actual process data, interface
information, and documentation to guide the developers more efficiently to
understand the behavior and possible errors of a process and its sub-processes.
Here, we present a demonstrator that employs this approach on a component-based
Java system. However, our approach is language-agnostic. Ideally, the generated
explanations will provide a good understanding of the process, even if
developers are not familiar with all the details of the considered system. Our
demonstrator is provided as an open-source web application that is freely
accessible to all users.

</details>


### [9] [Towards LLM-generated explanations for Component-based Knowledge Graph Question Answering Systems](https://arxiv.org/abs/2508.14553)
*Dennis Schiese,Aleksandr Perevalov,Andreas Both*

Main category: cs.SE

TL;DR: 论文提出了一种基于组件输入输出数据流的解释方法，使用SPARQL和RDF作为背景，通过大型语言模型（LLMs）生成高质量的解释，优于模板方法。


<details>
  <summary>Details</summary>
Motivation: 解决组件式问答系统中因AI方法复杂导致的决策难以解释的问题。

Method: 利用组件的数据流（SPARQL输入和RDF输出）生成解释，比较模板和LLMs两种方法。

Result: LLMs生成的解释质量更高，用户评价优于模板方法。

Conclusion: 通过LLMs自动解释问答组件的行为和决策，提升用户理解。

Abstract: Over time, software systems have reached a level of complexity that makes it
difficult for their developers and users to explain particular decisions made
by them. In this paper, we focus on the explainability of component-based
systems for Question Answering (QA). These components often conduct processes
driven by AI methods, in which behavior and decisions cannot be clearly
explained or justified, s.t., even for QA experts interpreting the executed
process and its results is hard. To address this challenge, we present an
approach that considers the components' input and output data flows as a source
for representing the behavior and provide explanations for the components,
enabling users to comprehend what happened. In the QA framework used here, the
data flows of the components are represented as SPARQL queries (inputs) and RDF
triples (outputs). Hence, we are also providing valuable insights on
verbalization regarding these data types. In our experiments, the approach
generates explanations while following template-based settings (baseline) or
via the use of Large Language Models (LLMs) with different configurations
(automatic generation). Our evaluation shows that the explanations generated
via LLMs achieve high quality and mostly outperform template-based approaches
according to the users' ratings. Therefore, it enables us to automatically
explain the behavior and decisions of QA components to humans while using RDF
and SPARQL as a context for explanations.

</details>


### [10] [Towards a DSL to Formalize Multimodal Requirements](https://arxiv.org/abs/2508.14631)
*Marcos Gomez-Vazquez,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出了一种名为MERLAN的领域特定语言（DSL），用于定义多模态交互系统的需求，并提供了工具原型来自动生成符合需求的系统实现。


<details>
  <summary>Details</summary>
Motivation: 随着多模态系统（处理文本、音频、图像等多种输入）的普及，需要一种方法来定义这些新型交互的需求，但现有方法不足可能导致系统无法满足用户需求。

Method: 设计了MERLAN DSL，包括其元模型和基于ANTLR的文本语法，并开发了一个原型工具，用于编写需求并自动生成基于代理框架的系统实现。

Result: 提供了MERLAN语言和工具原型，支持多模态系统需求的规范和实现。

Conclusion: MERLAN为多模态系统需求的规范提供了有效的解决方案，填补了当前语言和方法的不足。

Abstract: Multimodal systems, which process multiple input types such as text, audio,
and images, are becoming increasingly prevalent in software systems, enabled by
the huge advancements in Machine Learning. This triggers the need to easily
define the requirements linked to these new types of user interactions,
potentially involving more than one modality at the same time. This remains an
open challenge due to the lack of languages and methods adapted to the diverse
nature of multimodal interactions, with the risk of implementing AI-enhanced
systems that do not properly satisfy the user needs.
  In this sense, this paper presents MERLAN, a Domain-Specific Language (DSL)
to specify the requirements for these new types of multimodal interfaces. We
present the metamodel for such language together with a textual syntax
implemented as an ANTLR grammar. A prototype tool enabling requirements
engineers to write such requirements and automatically generate a possible
implementation of a system compliant with them on top of an agentic framework
is also provided.

</details>


### [11] [Assessing the Quality and Security of AI-Generated Code: A Quantitative Analysis](https://arxiv.org/abs/2508.14727)
*Abbas Sabra,Olivier Schmitt,Joseph Tyler*

Main category: cs.SE

TL;DR: 该研究定量评估了五种大型语言模型（LLMs）的代码质量和安全性，发现尽管其代码功能正常，但存在多种缺陷，需验证后才能用于生产环境。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs生成的代码质量与安全性，弥补之前研究中仅关注功能性能的不足。

Method: 通过SonarQube对4,442份Java代码进行静态分析，评估缺陷类型。

Result: 发现LLMs生成的代码普遍存在漏洞和弱点，功能性测试分数不能反映代码质量。

Conclusion: 静态分析是检测潜在缺陷的有效工具，LLMs生成的代码需额外验证。

Abstract: This study presents a quantitative evaluation of the code quality and
security of five prominent Large Language Models (LLMs): Claude Sonnet 4,
Claude 3.7 Sonnet, GPT-4o, Llama 3.2 90B, and OpenCoder 8B. While prior
research has assessed the functional performance of LLM-generated code, this
research tested LLM output from 4,442 Java coding assignments through
comprehensive static analysis using SonarQube. The findings suggest that
although LLMs can generate functional code, they also introduce a range of
software defects, including bugs, security vulnerabilities, and code smells.
These defects do not appear to be isolated; rather, they may represent shared
weaknesses stemming from systemic limitations within current LLM code
generation methods. In particular, critically severe issues, such as hard-coded
passwords and path traversal vulnerabilities, were observed across multiple
models. These results indicate that LLM-generated code requires verification in
order to be considered production-ready. This study found no direct correlation
between a model's functional performance (measured by Pass@1 rate of unit
tests) and the overall quality and security of its generated code, measured by
the number of SonarQube issues in benchmark solutions that passed the
functional tests. This suggests that functional benchmark performance score is
not a good indicator of overall code quality and security. The goal of this
study is not to rank LLM performance but to highlight that all evaluated models
appear to share certain weaknesses. Consequently, these findings support the
view that static analysis can be a valuable instrument for detecting latent
defects and an important safeguard for organizations that deploy AI in software
development.

</details>


### [12] [Challenges of Virtual Validation and Verification for Automotive Functions](https://arxiv.org/abs/2508.14747)
*Beatriz Cabrero-Daniel,Mazen Mohamad*

Main category: cs.SE

TL;DR: 论文探讨了车辆验证与验证过程中的挑战，通过专家研讨会和调查总结了17个关键挑战及其解决方案，并指出了下一步研究方向。


<details>
  <summary>Details</summary>
Motivation: 车辆验证与验证的复杂性及其对安全的重要性，促使研究团队探索模拟中的挑战和未解决问题。

Method: 通过专家研讨会和问卷调查，汇总了车辆模拟验证中的关键挑战和潜在解决方案。

Result: 专家们提出了17个关键挑战，并讨论了解决方案和下一步研究方向，同时发现资源不足是实现解决方案的障碍。

Conclusion: 许多挑战已有解决方案，未来研究应集中在未解决问题上，并将研究成果分享给更广泛的社区。

Abstract: Verification and validation of vehicles is a complex yet critical process,
particularly for ensuring safety and coverage through simulations. However,
achieving realistic and useful simulations comes with significant challenges.
To explore these challenges, we conducted a workshop with experts in the field,
allowing them to brainstorm key obstacles. Following this, we distributed a
survey to consolidate findings and gain further insights into potential
solutions. The experts identified 17 key challenges, along with proposed
solutions, an assessment of whether they represent next steps for research, and
the roadblocks to their implementation. While a lack of resources was not
initially highlighted as a major challenge, utilizing more resources emerged as
a critical necessity when experts discussed solutions. Interestingly, we
expected some of these challenges to have already been addressed or to have
systematic solutions readily available, given the collective expertise in the
field. Many of the identified problems already have known solutions, allowing
us to shift focus towards unresolved challenges and share the next steps with
the broader community.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Tuning Random Generators: Property-Based Testing as Probabilistic Programming](https://arxiv.org/abs/2508.14394)
*Ryan Tjoa,Poorva Garg,Harrison Goldstein,Todd Millstein,Benjamin Pierce,Guy Van den Broeck*

Main category: cs.PL

TL;DR: 本文提出自动优化生成器权重的技术，以改进属性测试中的输入分布。通过离散概率编程系统Loaded Dice实现，提升测试用例多样性和有效性。


<details>
  <summary>Details</summary>
Motivation: 属性测试中生成器的权重调整过程繁琐且难以控制输入分布，限制了测试效果。

Method: 开发自动离线调整生成器权重的技术，利用目标函数优化符号权重，并通过Loaded Dice实现。

Result: 实验表明优化后的生成器在多样性和有效性上有显著提升，bug发现速度提高3.1-7.4倍。

Conclusion: 自动优化生成器权重能有效改善测试输入分布并加速bug发现。

Abstract: Property-based testing validates software against an executable specification
by evaluating it on randomly generated inputs. The standard way that PBT users
generate test inputs is via generators that describe how to sample test inputs
through random choices. To achieve a good distribution over test inputs, users
must tune their generators, i.e., decide on the weights of these individual
random choices. Unfortunately, it is very difficult to understand how to choose
individual generator weights in order to achieve a desired distribution, so
today this process is tedious and limits the distributions that can be
practically achieved.
  In this paper, we develop techniques for the automatic and offline tuning of
generators. Given a generator with undetermined symbolic weights and an
objective function, our approach automatically learns values for these weights
that optimize for the objective. We describe useful objective functions that
allow users to (1) target desired distributions and (2) improve the diversity
and validity of their test cases. We have implemented our approach in a novel
discrete probabilistic programming system, Loaded Dice, that supports
differentiation and parameter learning, and use it as a language for
generators. We empirically demonstrate that our approach is effective at
optimizing generator distributions according to the specified objective
functions. We also perform a thorough evaluation on PBT benchmarks,
demonstrating that, when automatically tuned for diversity and validity, the
generators exhibit a 3.1-7.4x speedup in bug finding.

</details>


### [14] [Close is Good Enough: Component-Based Synthesis Modulo Logical Similarity](https://arxiv.org/abs/2508.14614)
*Ashish Mishra,Suresh Jagannathan*

Main category: cs.PL

TL;DR: 该论文提出了一种基于逻辑相似性分析的组件合成方法，通过细化类型规范和树自动机变体优化搜索过程，显著提升了复杂查询下的合成能力。


<details>
  <summary>Details</summary>
Motivation: 传统的组件合成算法（CBS）在处理约束严格的查询时效率低下，因为严格的约束使得可行解空间稀疏。论文旨在通过分析执行路径间的逻辑相似性，减少冗余搜索。

Method: 论文提出了一种新的搜索方法，利用细化类型规范（refinement-type specifications）和合格树自动机（Qualified Tree Automata），通过记录路径信息并利用子类型约束进行相似性推理，避免探索语义相似的路径。

Result: 通过工具\name的实现和评估，展示了该方法在解决复杂CBS查询时的卓越能力，远超现有技术。

Conclusion: 通过引入逻辑相似性分析和细化类型规范，论文显著提升了组件合成的效率和能力，为复杂查询的解决提供了新思路。

Abstract: Component-based synthesis (CBS) aims to generate loop-free programs from a
set of libraries whose methods are annotated with specifications and whose
output must satisfy a set of logical constraints, expressed as a query. The
effectiveness of a CBS algorithm critically depends on the severity of the
constraints imposed by the query. The more exact these constraints are, the
sparser the space of feasible solutions. This maxim also applies when we enrich
the expressiveness of the specifications affixed to library methods. In both
cases, the search must now contend with constraints that may only hold over a
small number of the possible execution paths that can be enumerated by a CBS
procedure.
  In this paper, we address this challenge by equipping CBS search with the
ability to reason about logical similarities among the paths it explores. Our
setting considers library methods equipped with refinement-type specifications
that enrich ordinary base types with a set of rich logical qualifiers to
constrain the set of values accepted by that type. We perform a search over a
tree automata variant called Qualified Tree Automata that intelligently records
information about enumerated terms, leveraging subtyping constraints over the
refinement types associated with these terms to enable reasoning about
similarity among candidate solutions as search proceeds, thereby avoiding
exploration of semantically similar paths.
  We present an implementation of this idea in a tool called \name, and provide
a comprehensive evaluation that demonstrates \name's ability to synthesize
solutions to complex CBS queries that go well-beyond the capabilities of the
existing state-of-the-art.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [15] [StarStream: Live Video Analytics over Space Networking](https://arxiv.org/abs/2508.14222)
*Miao Zhang,Jiaxing Li,Haoyuan Zhao,Linfeng Shen,Jiangchuan Liu*

Main category: cs.NI

TL;DR: 论文探讨了在现代低地球轨道（LEO）卫星网络（LSN）上实现实时视频分析（LVA）的挑战与潜力，并提出了一种名为StarStream的自适应流媒体框架，以应对LSN中的网络瓶颈和波动。


<details>
  <summary>Details</summary>
Motivation: 现有LVA系统主要依赖地面网络，限制了在自然灾害或偏远地区的应用。随着LEO卫星网络（如Starlink）的出现，全球高速互联网接入成为可能，但网络性能不稳定成为新挑战。

Method: 研究通过在Starlink上进行广泛实测，提出StarStream框架，包括基于Transformer的网络性能预测器和内容感知配置优化器，以自适应LSN的波动条件。

Result: 实验表明，StarStream在真实网络和视频处理数据中表现出色，有效解决了LSN中的上行链路瓶颈和性能波动问题。

Conclusion: StarStream为LEO卫星网络上的LVA提供了高效解决方案，未来可进一步优化以适应更复杂的网络环境。

Abstract: Streaming videos from resource-constrained front-end devices over networks to
resource-rich cloud servers has long been a common practice for surveillance
and analytics. Most existing live video analytics (LVA) systems, however, have
been built over terrestrial networks, limiting their applications during
natural disasters and in remote areas that desperately call for real-time
visual data delivery and scene analysis. With the recent advent of space
networking, in particular, Low Earth Orbit (LEO) satellite constellations such
as Starlink, high-speed truly global Internet access is becoming available and
affordable. This paper examines the challenges and potentials of LVA over
modern LEO satellite networking (LSN). Using Starlink as the testbed, we have
carried out extensive in-the-wild measurements to gain insights into its
achievable performance for LVA. The results reveal that the uplink bottleneck
in today's LSN, together with the volatile network conditions, can
significantly affect the service quality of LVA and necessitate prompt
adaptation. We accordingly develop StarStream, a novel LSN-adaptive streaming
framework for LVA. At its core, StarStream is empowered by a Transformer-based
network performance predictor tailored for LSN and a content-aware
configuration optimizer. We discuss a series of key design and implementation
issues of StarStream and demonstrate its effectiveness and superiority through
trace-driven experiments with real-world network and video processing data.

</details>


### [16] [OmniSense: Towards Edge-Assisted Online Analytics for 360-Degree Videos](https://arxiv.org/abs/2508.14237)
*Miao Zhang,Yifei Zhu,Linfeng Shen,Fangxin Wang,Jiangchuan Liu*

Main category: cs.NI

TL;DR: OmniSense是一个边缘辅助框架，旨在通过轻量级球形兴趣区域预测和智能视觉模型缩放，高效分析360度视频，实现高精度和低延迟。


<details>
  <summary>Details</summary>
Motivation: 随着全向相机硬件成本的降低和扩展现实应用的普及，360度视频的捕获需求增加，需要一种高效的分析方法以提取无盲区的可操作信息。

Method: OmniSense通过轻量级球形兴趣区域（SRoI）预测算法修剪冗余信息，并结合视频内容和网络动态，智能调整视觉模型以优化资源利用。

Result: 与资源无关的基线相比，OmniSense将准确率提高了19.8%至114.6%，同时在类似延迟下实现2.0到2.4倍的速度提升。

Conclusion: OmniSense在资源受限的环境中实现了高效的360度视频分析，为动态场景下的视频处理提供了实用解决方案。

Abstract: With the reduced hardware costs of omnidirectional cameras and the
proliferation of various extended reality applications, more and more
$360^\circ$ videos are being captured. To fully unleash their potential,
advanced video analytics is expected to extract actionable insights and
situational knowledge without blind spots from the videos. In this paper, we
present OmniSense, a novel edge-assisted framework for online immersive video
analytics. OmniSense achieves both low latency and high accuracy, combating the
significant computation and network resource challenges of analyzing
$360^\circ$ videos. Motivated by our measurement insights into $360^\circ$
videos, OmniSense introduces a lightweight spherical region of interest (SRoI)
prediction algorithm to prune redundant information in $360^\circ$ frames.
Incorporating the video content and network dynamics, it then smartly scales
vision models to analyze the predicted SRoIs with optimized resource
utilization. We implement a prototype of OmniSense with commodity devices and
evaluate it on diverse real-world collected $360^\circ$ videos. Extensive
evaluation results show that compared to resource-agnostic baselines, it
improves the accuracy by $19.8\%$ -- $114.6\%$ with similar end-to-end
latencies. Meanwhile, it hits $2.0\times$ -- $2.4\times$ speedups while keeping
the accuracy on par with the highest accuracy of baselines.

</details>


### [17] [A Distributed Learned Hash Table](https://arxiv.org/abs/2508.14239)
*Shengze Wang,Yi Liu,Xiaoxue Zhang,Liting Hu,Chen Qian*

Main category: cs.NI

TL;DR: LEAD是一种新型分布式哈希表系统，通过学习模型优化范围查询性能，显著降低延迟和消息成本。


<details>
  <summary>Details</summary>
Motivation: 尽管分布式哈希表（DHTs）在分布式系统中应用广泛，但其处理范围查询的能力有限，影响了如LLM服务、区块链等应用。

Method: LEAD通过递归机器学习模型映射和检索数据，同时保持数据固有顺序，并设计了降低延迟和消息成本的方案。

Result: 实验显示，LEAD在大规模系统中将查询延迟和消息成本降低80%至90%以上，同时具备良好的可扩展性和鲁棒性。

Conclusion: LEAD为分布式键值系统提供了高效、可扩展且健壮的数据检索解决方案。

Abstract: Distributed Hash Tables (DHTs) are pivotal in numerous high-impact key-value
applications built on distributed networked systems, offering a decentralized
architecture that avoids single points of failure and improves data
availability. Despite their widespread utility, DHTs face substantial
challenges in handling range queries, which are crucial for applications such
as LLM serving, distributed storage, databases, content delivery networks, and
blockchains. To address this limitation, we present LEAD, a novel system
incorporating learned models within DHT structures to significantly optimize
range query performance. LEAD utilizes a recursive machine learning model to
map and retrieve data across a distributed system while preserving the inherent
order of data. LEAD includes the designs to minimize range query latency and
message cost while maintaining high scalability and resilience to network
churn. Our comprehensive evaluations, conducted in both testbed implementation
and simulations, demonstrate that LEAD achieves tremendous advantages in system
efficiency compared to existing range query methods in large-scale distributed
systems, reducing query latency and message cost by 80% to 90%+. Furthermore,
LEAD exhibits remarkable scalability and robustness against system churn,
providing a robust, scalable solution for efficient data retrieval in
distributed key-value systems.

</details>


### [18] [DeeP-TE: Data-enabled Predictive Traffic Engineering](https://arxiv.org/abs/2508.14281)
*Zhun Yin,Xiaotian Li,Lifan Mei,Yong Liu,Zhong-Ping Jiang*

Main category: cs.NI

TL;DR: DeeP-TE算法通过历史数据动态调整路由配置，有效降低网络拥塞，且路由变更频率低于基准方法。


<details>
  <summary>Details</summary>
Motivation: 自适应路由需应对流量变化和频繁路由变更带来的性能挑战。

Method: 利用历史路由和链路速率数据，无需直接测量或估计流量矩阵，动态生成路由更新。

Result: 在实际网络中，DeeP-TE实现了接近最优的控制效果，且路由变化显著减少。

Conclusion: DeeP-TE算法结合数据驱动方法，有效解决了自适应路由的挑战。

Abstract: Routing configurations of a network should constantly adapt to traffic
variations to achieve good network performance. Adaptive routing faces two main
challenges: 1) how to accurately measure/estimate time-varying traffic
matrices? 2) how to control the network and application performance degradation
caused by frequent route changes? In this paper, we develop a novel
data-enabled predictive traffic engineering (DeeP-TE) algorithm that minimizes
the network congestion by gracefully adapting routing configurations over time.
Our control algorithm can generate routing updates directly from the historical
routing data and the corresponding link rate data, without direct traffic
matrix measurement or estimation. Numerical experiments on real network
topologies with real traffic matrices demonstrate that the proposed DeeP-TE
routing adaptation algorithm can achieve close-to-optimal control effectiveness
with significantly lower routing variations than the baseline methods.

</details>


### [19] [Design and Simulation of Fault-Tolerant Network Switching System Using Python-Based Algorithms](https://arxiv.org/abs/2508.14305)
*Terlumun Gbaden,Mterorga Ukor,Grace Erdoo Ateata*

Main category: cs.NI

TL;DR: 本文设计并模拟了一种基于Python的容错网络交换系统，通过冗余链路和自动故障转移提高网络可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 现代网络需要高可靠性和快速响应，特别是在关键环境中，容错机制至关重要。

Method: 使用NetworkX建模LAN和冗余链路，通过Scapy注入故障，并利用自定义Python逻辑实现自动故障转移和重路由。

Result: 系统显著提高了数据包传输连续性，缩短了恢复时间，并减少了数据包丢失。

Conclusion: 该方法为中等规模网络提供了一种轻量级且可扩展的容错解决方案，适用于企业和学术环境。

Abstract: Ensuring uninterrupted data flow in modern networks requires robust
fault-tolerant mechanisms, especially in environments where reliability and
responsiveness are critical. This paper presents the design and simulation of a
fault-tolerant network switching system using Python-based algorithms. A
simulated enterprise-level Local Area Network (LAN) was modeled using NetworkX
to represent switch-router interconnectivity with redundant links. Fault
scenarios, including link failure and congestion, were injected using Scapy,
while automatic failover and rerouting were implemented via custom Python
logic. The system demonstrates resilience by dynamically detecting path
failures, redistributing network traffic through redundant links, and
minimizing downtime. Performance evaluations reveal significant improvements in
packet delivery continuity, faster recovery times, and reduced packet loss
compared to non-fault-tolerant baselines. The implementation provides a
scalable and lightweight approach to integrating fault-tolerance features into
mid-scale networks, with potential application in enterprise information
technology infrastructures and academic simulations.

</details>


### [20] [The Small-World Beneath LEO Satellite Coverage: Ground Hubs in Multi-Shell Constellations](https://arxiv.org/abs/2508.14335)
*Hailong Su,Jinshu Su,Yusheng Xia,Haibin Li*

Main category: cs.NI

TL;DR: 该论文研究了由10,956颗卫星和198个地面站组成的六层巨型星座的结构特性和网络动态，发现其具有小世界特性，地面站中继和馈线链路显著提升了通信效率，但也存在负载不平衡问题。这些发现为未来卫星网络设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 研究低地球轨道（LEO）卫星巨型星座的路由效率和层间通信问题，以提升全球连接性能。

Method: 利用复杂网络分析工具，分析六层星座的结构特性和动态行为，重点关注小世界特性、地面站中继和馈线链路的作用。

Result: 星座具有强小世界特性，地面站中继提升层间连接性，馈线链路减少路径长度，但地面站间存在负载不平衡问题。

Conclusion: 研究结果为当前和未来卫星网络设计提供了重要参考，尤其在提升效率和可靠性方面。

Abstract: In recent years, the emergence of large-scale Low-Earth-Orbit (LEO) satellite
constellations has introduced unprecedented opportunities for global
connectivity. However, routing efficiency and inter-shell communication remain
key challenges in multi-shell architectures. This paper investigates the
structural properties and network dynamics of a representative six-shell
mega-constellation composed of 10,956 satellites and 198 gateway stations
(GSs). Leveraging tools from complex network analysis, we identify several
critical findings: (1) the constellation exhibits strong small-world
characteristics, enabling efficient routing despite large network diameters;
(2) GS relays play a pivotal role in enhancing inter-shell connectivity by
bridging otherwise disconnected components; (3) feeder links significantly
reduce average path length, making long-haul communication more feasible; (4)
betweenness analysis reveals load imbalances among GSs, indicating the need for
traffic-aware management strategies; (5) the architecture offers excellent
spatial coverage and resilience, maintaining connectivity and low routing costs
even under GS failures. These insights not only explain the design rationale
behind current mega-constellations like SpaceX Starlink, but also provide
valuable guidance for the evolution of future satellite network
infrastructures.

</details>


### [21] [Availability-Aware VNF Placement and Request Routing in MEC-Enabled 5G Networks](https://arxiv.org/abs/2508.14435)
*Aqsa Sayeed,Samaresh Bera*

Main category: cs.NI

TL;DR: 该论文研究了在移动边缘计算（MEC）支持的5G网络中虚拟网络功能（VNF）放置问题，针对超高可靠低时延通信（uRLLC）应用的需求，提出了一种随机化舍入方法和贪心启发式方法来解决NP难问题，并通过仿真和原型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了满足uRLLC应用对高可靠性和低时延的严格要求，研究如何在5G网络中高效放置VNF，以最大化网络服务提供商的收益。

Method: 提出了一种近似随机化舍入方法，能够在多项式时间内解决NP难优化问题，并结合贪心启发式方法处理资源约束的违规问题。

Result: 仿真表明，随机化舍入和贪心方法的收益分别接近最优解的5%和10%以内；原型实现也验证了冗余VNF放置对提升分组交付率和降低时延的有效性。

Conclusion: 该研究揭示了uRLLC应用中可用性与资源效率之间的权衡，并提出了可行的解决方案。

Abstract: In this paper, we study the virtual network function (VNF) placement problem
in mobile edge computing (MEC)-enabled 5G networks to meet the stringent
reliability and latency requirements of uRLLC applications. We pose it as a
constrained optimization problem, which is NP-hard, to maximize the total
reward obtained by a network service provider by serving uRLLC service
requests. We propose an approximated randomized rounding approach to solve the
NP-hard optimization problem in polynomial time. We prove that the proposed
randomized approach achieves performance guarantees while violating the
resource constraints boundedly. Furthermore, we present a greedy-heuristic
approach to tackle the violations of resource constraints.
  Simulation results show that the proposed randomized rounding and greedy
approaches achieve a total reward which is within 5% and 10% of the optimal
solution, respectively. Furthermore, we compare the proposed greedy approach
with the existing schemes that do not consider the availability requirements.
We observe that the existing schemes perform poorly in terms of total reward,
as negligence to the availability requirements negatively impacts the number of
successfully served requests. These findings highlight the trade-off between
availability and resource efficiency in latency-sensitive uRLLC applications.
We also implement a software prototype of a 5G network using open-source
software platforms with redundant placement of VNFs. The results on packet
delivery ratio and latency obtained from the prototype implementation are also
improved in the redundant VNFs with different failure probabilities.

</details>


### [22] [Transforming Next-generation Network Planning assisted by Data Acquisition of Top Three Spanish MNOs](https://arxiv.org/abs/2508.14445)
*M. Umar Khan*

Main category: cs.NI

TL;DR: 论文提出利用传统基础设施的移动流量数据进行5G网络规划，通过清理和分析西班牙三大运营商的开源数据库，提取有用信息并可视化流量密度模式。


<details>
  <summary>Details</summary>
Motivation: 为了更高效地进行5G网络的拓扑和成本规划，需要利用传统移动流量数据提取有用信息。

Method: 使用西班牙三大运营商的开源数据库，清理数据并提取流量信息，可视化流量密度模式以支持新gNB部署。

Result: 通过数据清理和分析，识别了用户密度最高的区域，为5G新服务部署提供了依据。

Conclusion: 该方法有效支持了5G网络的规划和部署，尤其是通过传统数据分析和可视化来实现高效网络设计。

Abstract: In this paper, we address the necessity of data related to mobile traffic of
the legacy infrastructure to extract useful information and perform network
dimensioning for 5G. These data can help us achieve a more efficient network
planning design, especially in terms of topology and cost. To that end, a real
open database of top three Spanish mobile network operators (MNOs) is used to
estimate the traffic and to identify the area of highest user density for the
deployment of new services. We propose the data acquisition procedure described
to clean the database, to extract meaningful traffic information and to
visualize traffic density patterns for new gNB deployments. We present the
state of the art in Network Data. We describe the considered network database
in detail. The Network Data Acquisition entity along with the proposed
procedure is explained. The corresponding results are discussed, following the
conclusions.

</details>


### [23] [Adaptive Network Selection for Latency-Aware V2X Systems under Varying Network and Vehicle Densities](https://arxiv.org/abs/2508.14471)
*Muhammad Z. Haq,Nadia N. Qadri,Omer Chughtai,Sadiq A. Ahmad,Waqas Khalid,Heejung Yu*

Main category: cs.NI

TL;DR: ANS-V2X是一种自适应网络选择框架，针对延迟敏感的V2X系统设计，能够在不同车辆密度和网络条件下高效运行。


<details>
  <summary>Details</summary>
Motivation: 现代车辆通信环境对低延迟和高吞吐量通信有高需求，但实时网络选择受到多种应用需求和异构无线接入技术（如4G、5G和ad hoc）的挑战。

Method: 采用启发式驱动的网络选择方法，综合考虑应用敏感性、延迟、计算负载和方向性约束，并与MILP和Q学习方法进行对比。

Result: ANS-V2X在性能上接近最优（与MILP相比差5-10%），但决策时间减少85%，执行时间低于15毫秒，优于其他方法。

Conclusion: ANS-V2X适合实时且延迟敏感的V2X系统部署，在动态环境中表现优异。

Abstract: This paper presents ANS-V2X, an Adaptive Network Selection framework tailored
for latency-aware V2X systems operating under varying vehicle densities and
heterogeneous network conditions. Modern vehicular environments demand
low-latency and high-throughput communication, yet real-time network selection
is hindered by diverse application requirements and the coexistence of multiple
Radio Access Technologies (RATs) such as 4G, 5G, and ad hoc links. ANS-V2X
employs a heuristic-driven approach to assign vehicles to networks by
considering application sensitivity, latency, computational load, and
directionality constraints. The framework is benchmarked against a
Mixed-Integer Linear Programming (MILP) formulation for optimal solutions and a
Q-learning-based method representing reinforcement learning. Simulation results
demonstrate that ANS-V2X achieves near-optimal performance, typically within 5
to 10% of the utility achieved by MILP-V2X, while reducing execution time by
more than 85%. Although MILP-V2X offers globally optimal results, its
computation time often exceeds 100 milliseconds, making it unsuitable for
real-time applications. The Q-learning-based method is more adaptable but
requires extensive training and converges slowly in dynamic scenarios. In
contrast, ANS-V2X completes decisions in under 15 milliseconds and consistently
delivers lower latency than both alternatives. This confirms its suitability
for real-time, edge-level deployment in latency-critical V2X systems

</details>


### [24] [Multi-Tier UAV Edge Computing for Low Altitude Networks Towards Long-Term Energy Stability](https://arxiv.org/abs/2508.14601)
*Yufei Ye,Shijian Gao,Xinhu Zheng,Liuqing Yang*

Main category: cs.NI

TL;DR: 论文提出了一种新型多层级无人机辅助边缘计算系统，旨在优化低空网络中的任务延迟和能量稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决低空网络中任务执行延迟高和轻型无人机能量与计算资源有限的问题。

Method: 使用Lyapunov优化将问题分解为时间槽确定的子问题，动态调整任务优先级，优化任务分配、资源分配和无人机轨迹规划。

Result: 模拟结果显示，传输能量降低至少26%，且能量稳定性优于现有基准。

Conclusion: 系统能有效减少延迟并提升能量稳定性，适用于低空网络场景。

Abstract: This paper presents a novel multi-tier UAV-assisted edge computing system
designed for low-altitude networks. The system comprises vehicle users,
lightweight Low-Tier UAVs (L-UAVs), and High-Tier UAV (H-UAV). L-UAVs function
as small-scale edge servers positioned closer to vehicle users, while the
H-UAV, equipped with more powerful server and larger-capacity battery, serves
as mobile backup server to address the limitations in endurance and computing
resources of L-UAVs. The primary objective is to minimize task execution delays
while ensuring long-term energy stability for L-UAVs. To address this
challenge, the problem is first decoupled into a series of deterministic
problems for each time slot using Lyapunov optimization. The priorities of task
delay and energy consumption for L-UAVs are adaptively adjusted based on
real-time energy status. The optimization tasks include assignment of tasks,
allocation of computing resources, and trajectory planning for both L-UAVs and
H-UAV. Simulation results demonstrate that the proposed approach achieves a
reduction of at least 26% in transmission energy for L-UAVs and exhibits
superior energy stability compared to existing benchmarks.

</details>


### [25] [Adaptive Vision-Based Coverage Optimization in Mobile Wireless Sensor Networks: A Multi-Agent Deep Reinforcement Learning Approach](https://arxiv.org/abs/2508.14676)
*Parham Soltani,Mehrshad Eskandarpour,Sina Heidari,Farnaz Alizadeh,Hossein Soleimani*

Main category: cs.NI

TL;DR: 提出一种基于深度强化学习的移动无线传感器网络自主部署策略，提升覆盖率和能效。


<details>
  <summary>Details</summary>
Motivation: 解决传统静态部署中传感器能量耗尽和覆盖冗余问题。

Method: 结合摄像头和深度强化学习，实现多智能体协同控制传感器移动。

Result: 覆盖提升26.5%，能耗降低32%，网络寿命延长45%。

Conclusion: 该方法显著提升移动传感器网络的适应性和能效。

Abstract: Traditional Wireless Sensor Networks (WSNs) typically rely on pre-analysis of
the target area, network size, and sensor coverage to determine initial
deployment. This often results in significant overlap to ensure continued
network operation despite sensor energy depletion. With the emergence of Mobile
Wireless Sensor Networks (MWSNs), issues such as sensor failure and static
coverage limitations can be more effectively addressed through mobility. This
paper proposes a novel deployment strategy in which mobile sensors autonomously
position themselves to maximize area coverage, eliminating the need for
predefined policies. A live camera system, combined with deep reinforcement
learning (DRL), monitors the network by detecting sensor LED indicators and
evaluating real-time coverage. Rewards based on coverage efficiency and sensor
movement are computed at each learning step and shared across the network
through a Multi-Agent Reinforcement Learning (MARL) framework, enabling
decentralized, cooperative sensor control. Key contributions include a
vision-based, low-cost coverage evaluation method; a scalable MARL-DRL
framework for autonomous deployment; and a self-reconfigurable system that
adjusts sensor positioning in response to energy depletion. Compared to
traditional distance-based localization, the proposed method achieves a 26.5%
improvement in coverage, a 32% reduction in energy consumption, and a 22%
decrease in redundancy, extending network lifetime by 45%. This approach
significantly enhances adaptability, energy efficiency, and robustness in
MWSNs, offering a practical deployment solution within the IoT framework.

</details>


### [26] [Energy-Efficient Routing Algorithm for Wireless Sensor Networks: A Multi-Agent Reinforcement Learning Approach](https://arxiv.org/abs/2508.14679)
*Parham Soltani,Mehrshad Eskandarpour,Amir Ahmadizad,Hossein Soleimani*

Main category: cs.NI

TL;DR: 本文提出了一种基于强化学习的簇头选择和混合多跳路由算法，通过Q学习和多智能体系统动态调整传输路径，优化无线传感器网络的能源效率。


<details>
  <summary>Details</summary>
Motivation: 提高无线传感器网络的能源管理效率，延长网络生命周期并确保可靠的数据传输。

Method: 结合Q学习和多智能体系统，设计动态路由算法，融合MERA和MST经典方法，优化能源消耗和负载均衡。

Result: 仿真表明，该方法显著提高了节点存活率，减少了SoC方差，增强了网络韧性。

Conclusion: 该方案为动态传感器部署和物联网应用提供了可扩展且适应性强的能源管理解决方案。

Abstract: Efficient energy management is essential in Wireless Sensor Networks (WSNs)
to extend network lifetime and ensure reliable data transmission. This paper
presents a novel method using reinforcement learning-based cluster-head
selection and a hybrid multi-hop routing algorithm, which leverages Q-learning
within a multi-agent system to dynamically adapt transmission paths based on
the energy distribution across sensor nodes. Each sensor node is modeled as an
autonomous agent that observes local state parameters, such as residual energy,
distance to sink, hop count, and hotspot proximity, and selects routing actions
that maximize long-term energy efficiency. After computing the optimal paths,
each sensor aggregates sensed data and forwards it through intermediate nodes
to a selected transmitter node, chosen based on the highest remaining State of
Charge (SoC), thereby avoiding premature node depletion. To promote efficient
learning, a carefully designed reward function incentivizes balanced load
distribution, hotspot avoidance, and energy-aware forwarding while maintaining
signal quality. The learning process occurs either in a decentralized manner or
via a cloud-based controller that offloads computation in large-scale
deployments. Moreover, the RL-driven routing decisions are fused with classical
graph-based methods, Minimum Energy Routing Algorithm (MERA) and Minimum
Spanning Tree (MST), to optimize energy consumption and load balancing.
Simulations confirm that the proposed approach significantly improves node
survival rate, reduces SoC variance, and enhances network resilience, making it
a scalable and adaptive solution for energy-constrained WSNs in dynamic sensor
deployments and IoT applications.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [27] [FakeHunter: Multimodal Step-by-Step Reasoning for Explainable Video Forensics](https://arxiv.org/abs/2508.14581)
*Chen Chen,Runze Li,Zejun Zhang,Pukun Zhao,Fanqing Zhou,Longxiang Wang,Haojian Huang*

Main category: cs.MM

TL;DR: FakeHunter 是一种多模态深度伪造检测框架，结合了记忆引导检索、链式推理和工具增强验证，提供准确且可解释的视频取证。


<details>
  <summary>Details</summary>
Motivation: 为了提供更准确且可解释的深度伪造视频检测方法，FakeHunter 结合多种技术以实现高精度和实用性。

Method: 使用 CLIP 和 CLAP 编码视听内容，通过 FAISS 索引内存库检索真实样本进行上下文对比，并通过链式推理和工具验证定位和解释篡改。

Result: 在 X-AVFake 基准测试中，FakeHunter 的准确率达 34.75%（比基线高 16.87%），工具验证将低置信度案例的准确率提升至 46.50%。

Conclusion: FakeHunter 展示了多阶段设计的有效性，同时在实际部署中表现出较高的效率。

Abstract: FakeHunter is a multimodal deepfake detection framework that combines
memory-guided retrieval, chain-of-thought (Observation-Thought-Action)
reasoning, and tool-augmented verification to provide accurate and
interpretable video forensics. FakeHunter encodes visual content using CLIP and
audio using CLAP, generating joint audio-visual embeddings that retrieve
semantically similar real exemplars from a FAISS-indexed memory bank for
contextual grounding. Guided by the retrieved context, the system iteratively
reasons over evidence to localize manipulations and explain them. When
confidence is low, it automatically invokes specialized tools-such as zoom-in
image forensics or mel-spectrogram inspection-for fine-grained verification.
Built on Qwen2.5-Omni-7B, FakeHunter produces structured JSON verdicts that
specify what was modified, where it occurs, and why it is judged fake. We also
introduce X-AVFake, a benchmark comprising 5.7k+ manipulated and real videos
(950+ min) annotated with manipulation type, region/entity, violated reasoning
category, and free-form justification. On X-AVFake, FakeHunter achieves an
accuracy of 34.75%, outperforming the vanilla Qwen2.5-Omni-7B by 16.87
percentage points and MiniCPM-2.6 by 25.56 percentage points. Ablation studies
reveal that memory retrieval contributes a 7.75 percentage point gain, and
tool-based inspection improves low-confidence cases to 46.50%. Despite its
multi-stage design, the pipeline processes a 10-minute clip in 8 minutes on a
single NVIDIA A800 (0.8x real-time) or 2 minutes on four GPUs (0.2x),
demonstrating practical deployability.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [28] [To Zip Through the Cost Analysis of Probabilistic Programs](https://arxiv.org/abs/2508.14249)
*Matthias Hetzenberger,Georg Moser,Florian Zuleger*

Main category: cs.LO

TL;DR: 提出了一个基于Liquid Haskell的细化类型概率单子，用于自动化分析概率算法的期望运行时，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决概率算法中自动化分析期望运行时的局限性问题。

Method: 引入细化类型概率单子，并扩展支持无限分布，利用Liquid Haskell的SMT细化类型检查实现自动化分析。

Result: 成功应用于四种案例研究，包括梅尔德堆、优惠券收集、随机快速排序和zip树，验证了方法的可行性。

Conclusion: 提出的框架在自动化分析概率算法的期望运行时具有实际应用价值，特别是结合交互式证明时效果显著。

Abstract: Probabilistic programming and the formal analysis of probabilistic algorithms
are active areas of research, driven by the widespread use of randomness to
improve performance. While functional correctness has seen substantial
progress, automated reasoning about expected runtime remains comparatively
limited. In this work, we address this challenge by introducing a
refinement-typed probability monad in Liquid Haskell. Our monad enables
automated reasoning about expected values and costs by encoding probabilistic
behaviour directly in types. Initially defined for discrete distributions over
finite support, it is extended to support infinite distributions via an
axiomatic approach. By leveraging Liquid Haskell's SMT-based refinement type
checking, our framework provides a high degree of automation. We evaluate our
approach through four case studies: meldable heaps, coupon collector,
randomised quicksort, and zip trees. The first two demonstrate automation with
minimal annotation overhead. The latter two showcase how our monad integrates
with interactive proofs, including the first formal verification of the
expected runtime of zip trees.

</details>


### [29] [Quantum Petri Nets with Event Structures semantics](https://arxiv.org/abs/2508.14531)
*Julien Saan Joachim,Marc de Visme,Stefan Haar*

Main category: cs.LO

TL;DR: 该论文提出了量子佩特里网（QPNs），填补了量子并发领域缺乏严格语义和分析工具的空白。


<details>
  <summary>Details</summary>
Motivation: 现有'量子佩特里网'缺乏严格的并发和量子语义、分析工具及展开理论，因此需要建立一个有理论基础的量子并发模型。

Method: 通过引入量子佩特里网（QPNs），并与量子事件结构语义兼容，定义了局部量子发生网（LQONs），并构建了具有展开语义和组合框架的QPNs。

Result: 成功建立了具有严格语义基础的量子并发模型，连接了佩特里网理论与量子编程。

Conclusion: QPNs为量子并发提供了一个语义严谨的模型，解决了现有研究的缺陷。

Abstract: Classical Petri nets provide a canonical model of concurrency, with unfolding
semantics linking nets, occurrence nets, and event structures. No comparable
framework exists for quantum concurrency: existing ''quantum Petri nets'' lack
rigorous concurrent and sound quantum semantics, analysis tools, and unfolding
theory. We introduce Quantum Petri Nets (QPNs), Petri nets equipped with a
quantum valuation compatible with the quantum event structure semantics of
Clairambault, De Visme, and Winskel (2019). Our contributions are: (i) a local
definition of Quantum Occurrence Nets (LQONs) compatible with quantum event
structures, (ii) a construction of QPNs with a well-defined unfolding
semantics, (iii) a compositional framework for QPNs. This establishes a
semantically well grounded model of quantum concurrency, bridging Petri net
theory and quantum programming.

</details>


### [30] [Correct Black-Box Monitors for Distributed Deadlock Detection: Formalisation and Implementation (Technical Report)](https://arxiv.org/abs/2508.14851)
*Radosław Jan Rowicki,Adrian Francalanza,Alceste Scalas*

Main category: cs.LO

TL;DR: 提出了一种分布式黑盒监控器DDMon，用于检测微服务系统中的死锁问题，通过观察消息交换和发送探针来准确识别死锁服务，无需误报或漏报。


<details>
  <summary>Details</summary>
Motivation: 随着并发和分布式系统的规模扩大，死锁风险增加且诊断困难，需要一种有效的黑盒监控解决方案。

Method: 开发分布式黑盒监控器，通过观察消息交互和交换探针检测死锁，并使用形式化模型证明算法正确性。

Result: 实现了工具DDMon，能够准确识别死锁服务，性能良好，并在Erlang/OTP应用中验证。

Conclusion: 首次提出并实现了分布式黑盒死锁检测监控器，形式化证明其正确性，工具DDMon具有实用价值。

Abstract: Many software applications rely on concurrent and distributed (micro)services
that interact via message-passing and various forms of remote procedure calls
(RPC). As these systems organically evolve and grow in scale and complexity,
the risk of introducing deadlocks increases and their impact may worsen: even
if only a few services deadlock, many other services may block while awaiting
responses from the deadlocked ones. As a result, the "core" of the deadlock can
be obfuscated by its consequences on the rest of the system, and diagnosing and
fixing the problem can be challenging.
  In this work we tackle the challenge by proposing distributed black-box
monitors that are deployed alongside each service and detect deadlocks by only
observing the incoming and outgoing messages, and exchanging probes with other
monitors. We present a formal model that captures popular RPC-based application
styles (e.g., gen_servers in Erlang/OTP), and a distributed black-box
monitoring algorithm that we prove sound and complete (i.e., identifies
deadlocked services with neither false positives nor false negatives). We
implement our results in a tool called DDMon for the monitoring of Erlang/OTP
applications, and we evaluate its performance.
  This is the first work that formalises, proves the correctness, and
implements distributed black-box monitors for deadlock detection. Our results
are mechanised in Coq. DDMon is the companion artifact of this paper.

</details>


### [31] [A Complete and Natural Rule Set for Multi-Qutrit Clifford Circuits](https://arxiv.org/abs/2508.14670)
*Sarah Meng Li,Michele Mosca,Neil J. Ross,John van de Wetering,Yuming Zhao*

Main category: cs.LO

TL;DR: 提出了一个完整的n-qutrit Clifford电路的规则重写系统，这是首次在奇数质数维度上实现量子电路片段的完整性。


<details>
  <summary>Details</summary>
Motivation: 研究目标是扩展Selinger的n-qubit Clifford电路正规形式到qutrit（三态量子比特）环境，并建立一套完整的重写规则。

Method: 首先将Selinger的正规形式推广到qutrit场景，然后设计重写系统将任何Clifford电路化简为该正规形式，最终简化规则为一个简洁的自然集。

Result: 成功展示了qutrit Clifford幺正群的生成元与关系，提供了该群的清晰表示。

Conclusion: 该研究填补了奇数质数维度量子电路规则完整性的空白，为qutrit Clifford电路提供了理论基础。

Abstract: We present a complete set of rewrite rules for n-qutrit Clifford circuits
where n is any non-negative integer. This is the first completeness result for
any fragment of quantum circuits in odd prime dimensions. We first generalize
Selinger's normal form for n-qubit Clifford circuits to the qutrit setting.
Then, we present a rewrite system by which any Clifford circuit can be reduced
to this normal form. We then simplify the rewrite rules in this procedure to a
small natural set of rules, giving a clean presentation of the group of qutrit
Clifford unitaries in terms of generators and relations.

</details>


### [32] [Emerson-Lei and Manna-Pnueli Games for LTLf+ and PPLTL+ Synthesis](https://arxiv.org/abs/2508.14725)
*Daniel Hausmann,Shufang Zhu,Gianmarco Parretti,Christoph Weinhuber,Giuseppe De Giacomo,Nir Piterman*

Main category: cs.LO

TL;DR: 论文介绍了两种针对LTLfp和PPLTLp逻辑的反应性综合求解器，分别基于符号化求解器和Manna-Pnueli游戏，后者通过组合更简单的游戏实现效率提升。


<details>
  <summary>Details</summary>
Motivation: 为了在无限轨迹设置中应用有限轨迹的LTLf/PPLTL技术，并在表达力上达到完整LTL的水平，需要开发新的求解器。

Method: 论文提出了两种方法：一种基于符号化求解器，通过降低属性复杂性；另一种基于Manna-Pnueli游戏，通过组合简单游戏实现高效求解。

Result: 实验结果显示，Manna-Pnueli游戏在多数情况下表现更优，但并非所有情况，表明结合两种方法可能进一步提升性能。

Conclusion: 两种求解器各有优势，结合使用可能达到更好的实际效果。

Abstract: Recently, the Manna-Pnueli Hierarchy has been used to define the temporal
logics LTLfp and PPLTLp, which allow to use finite-trace LTLf/PPLTL techniques
in infinite-trace settings while achieving the expressiveness of full LTL. In
this paper, we present the first actual solvers for reactive synthesis in these
logics. These are based on games on graphs that leverage DFA-based techniques
from LTLf/PPLTL to construct the game arena. We start with a symbolic solver
based on Emerson-Lei games, which reduces lower-class properties (guarantee,
safety) to higher ones (recurrence, persistence) before solving the game. We
then introduce Manna-Pnueli games, which natively embed Manna-Pnueli objectives
into the arena. These games are solved by composing solutions to a DAG of
simpler Emerson-Lei games, resulting in a provably more efficient approach. We
implemented the solvers and practically evaluated their performance on a range
of representative formulas. The results show that Manna-Pnueli games often
offer significant advantages, though not universally, indicating that combining
both approaches could further enhance practical performance.

</details>


### [33] [Constraint satisfaction problems, compactness and non-measurable sets](https://arxiv.org/abs/2508.14838)
*Claude Tardif*

Main category: cs.LO

TL;DR: 论文探讨了有限关系结构A的紧凑性及其与Zermelo-Fraenkel公理系统的关系，指出A在宽度为一时可在该系统中证明，否则会导致3空间中非可测集的存在。


<details>
  <summary>Details</summary>
Motivation: 研究有限关系结构的紧凑性及其证明条件的限制，探讨其与公理系统和数学基础理论的关系。

Method: 通过分类A的宽度，讨论其在Zermelo-Fraenkel公理系统中的可证明性及其对非可测集存在的影响。

Result: 若A宽度为一，其紧凑性可在Zermelo-Fraenkel公理系统中证明；否则，A的紧凑性暗示3空间中存在非可测集。

Conclusion: 有限关系结构的紧凑性与A的宽度及数学基础理论中的选择公理密切相关，揭示了其深层次的理论联系。

Abstract: A finite relational structure A is called compact if for any infinite
relational structure B of the same type, the existence of a homomorphism from B
to A is equivalent to the existence of homomorphisms from all finite
substructures of B to A. We show that if A has width one, then the compactness
of A can be proved in the axiom system of Zermelo and Fraenkel, but otherwise,
the compactness of A implies the existence of non-measurable sets in 3-space.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [34] [Using Real Names of Disabled Participant-Contributors to Practice Citational Justice in Accessibility](https://arxiv.org/abs/2508.14257)
*Jonathan Zong*

Main category: cs.HC

TL;DR: 该论文提出在有残疾研究参与者同意的情况下，公开其姓名，以实践引用公正，强调残疾人的知识和贡献。


<details>
  <summary>Details</summary>
Motivation: 传统的匿名化做法可能导致残疾人的贡献被忽视，论文旨在通过引用公正来承认残疾人的知识和贡献。

Method: 研究者通过分析无障碍可视化研究中的实例，探讨何时适合公开参与者身份，并提出相关考虑。

Result: 研究发现，有意引用残疾人的贡献可以增强其社会认可和学术影响力。

Conclusion: 论文鼓励研究者有意识地引用和承认残疾人的知识产权，以促进社会公正。

Abstract: In accessibility research involving human subjects, researchers
conventionally anonymize their research participants to protect privacy.
However, a lack of intentionality about who to publicly acknowledge for
intellectual contributions to research can lead to the erasure of disabled
individuals' work and knowledge. In this paper, I propose identifying disabled
research participants by name (with consent) as a practice of citational
justice. I share observations from examples of this practice in accessible
visualization research, and offer considerations for when it may be appropriate
to de-anonymize. Intentional practices of citation offer researchers an
opportunity to acknowledge the expertise and intellectual contributions of
disabled people in our communities.

</details>


### [35] ["They Aren't Built For Me": An Exploratory Study of Strategies for Measurement of Graphical Primitives in Tactile Graphics](https://arxiv.org/abs/2508.14289)
*Areen Khalaila,Lane Harrison,Nam Wook Kim,Dylan Cashman*

Main category: cs.HC

TL;DR: 研究探讨了基于视觉感知的设计指南是否适合触觉感知系统，发现触觉图形设计中存在感知预期与实际工具的差距，并提出了一套新的设计指南。


<details>
  <summary>Details</summary>
Motivation: 盲人或低视力人群通过触觉技术分析数据时，视觉感知的设计指南可能不适用，需要探索更合适的触觉图形设计方法。

Method: 通过复制Cleveland和McGill的图形感知研究，结合群体访谈，分析盲人或低视力人群在触觉图形中的测量策略。

Result: 研究发现视觉感知的编码在触觉图形中有用，但参与者更倾向于专为触觉设计的编码，同时揭示了感知预期与实际工具的差距。

Conclusion: 研究提出了一套针对触觉图形的设计指南，以期弥补视觉与触觉感知间的差距，提升盲人或低视力人群的数据分析体验。

Abstract: Advancements in accessibility technologies such as low-cost swell form
printers or refreshable tactile displays promise to allow blind or low-vision
(BLV) people to analyze data by transforming visual representations directly to
tactile representations. However, it is possible that design guidelines derived
from experiments on the visual perception system may not be suited for the
tactile perception system. We investigate the potential mismatch between
familiar visual encodings and tactile perception in an exploratory study into
the strategies employed by BLV people to measure common graphical primitives
converted to tactile representations. First, we replicate the Cleveland and
McGill study on graphical perception using swell form printing with eleven BLV
subjects. Then, we present results from a group interview in which we describe
the strategies used by our subjects to read four common chart types. While our
results suggest that familiar encodings based on visual perception studies can
be useful in tactile graphics, our subjects also expressed a desire to use
encodings designed explicitly for BLV people. Based on this study, we identify
gaps between the perceptual expectations of common charts and the perceptual
tools available in tactile perception. Then, we present a set of guidelines for
the design of tactile graphics that accounts for these gaps. Supplemental
material is available at
https://osf.io/3nsfp/?view_only=7b7b8dcbae1d4c9a8bb4325053d13d9f.

</details>


### [36] [Exploring Organizational Strategies in Immersive Computational Notebooks](https://arxiv.org/abs/2508.14346)
*Sungwon In,Ayush Roy,Eric Krokos,Kirsten Whitley,Chris North,Yalong Yang*

Main category: cs.HC

TL;DR: 沉浸式计算笔记本通过增强交互性提升数据分析效率，但目前对其组织策略的研究不足。本研究探索了空间结构和执行顺序的可视化，发现用户偏爱半圆柱形结构和非线性分析。


<details>
  <summary>Details</summary>
Motivation: 虽然沉浸式计算笔记本提升了导航性能，但其组织策略尚未充分研究，特别是在空间结构和执行顺序的可视化方面。

Method: 通过探索性用户研究，调查用户对笔记本空间结构和执行顺序的偏好，并记录其分析行为。

Result: 用户偏好半圆柱形结构，并在笔记本规模增大时更倾向于采用非线性分析方法。

Conclusion: 沉浸式计算笔记本的空间结构和可视化执行顺序对数据科学流程的效率和用户行为有显著影响。

Abstract: Computational notebooks, which integrate code, documentation, tags, and
visualizations into a single document, have become increasingly popular for
data analysis tasks. With the advent of immersive technologies, these notebooks
have evolved into a new paradigm, enabling more interactive and intuitive ways
to perform data analysis. An immersive computational notebook, which integrates
computational notebooks within an immersive environment, significantly enhances
navigation performance with embodied interactions. However, despite recognizing
the significance of organizational strategies in the immersive data science
process, the organizational strategies for using immersive notebooks remain
largely unexplored. In response, our research aims to deepen our understanding
of organizations, especially focusing on spatial structures for computational
notebooks, and to examine how various execution orders can be visualized in an
immersive context. Through an exploratory user study, we found participants
preferred organizing notebooks in half-cylindrical structures and engaged
significantly more in non-linear analysis. Notably, as the scale of the
notebooks increased (i.e., more code cells), users increasingly adopted
multiple, concurrent non-linear analytical approaches.

</details>


### [37] [NoteIt: A System Converting Instructional Videos to Interactable Notes Through Multimodal Video Understanding](https://arxiv.org/abs/2508.14395)
*Running Zhao,Zhihan Jiang,Xinchen Zhang,Chirui Chang,Handi Chen,Weipeng Deng,Luyao Jin,Xiaojuan Qi,Xun Qian,Edith C. H. Ngai*

Main category: cs.HC

TL;DR: NoteIt是一个自动将教学视频转换为交互式笔记的系统，通过提取视频的层次结构和多模态信息，并允许用户自定义内容和格式，效果优。


<details>
  <summary>Details</summary>
Motivation: 现有工具生成的笔记无法全面保留视频信息或满足用户对多样化展示和交互的需求。

Method: NoteIt采用新管道提取视频的层次结构和多模态关键信息，并通过界面实现用户定制。

Result: 技术评估和用户研究（N=36）显示其在客观指标和用户体验上表现优异。

Conclusion: NoteIt的管道和界面设计有效提升了教学视频笔记的质量和可用性。

Abstract: Users often take notes for instructional videos to access key knowledge later
without revisiting long videos. Automated note generation tools enable users to
obtain informative notes efficiently. However, notes generated by existing
research or off-the-shelf tools fail to preserve the information conveyed in
the original videos comprehensively, nor can they satisfy users' expectations
for diverse presentation formats and interactive features when using notes
digitally. In this work, we present NoteIt, a system, which automatically
converts instructional videos to interactable notes using a novel pipeline that
faithfully extracts hierarchical structure and multimodal key information from
videos. With NoteIt's interface, users can interact with the system to further
customize the content and presentation formats of the notes according to their
preferences. We conducted both a technical evaluation and a comparison user
study (N=36). The solid performance in objective metrics and the positive user
feedback demonstrated the effectiveness of the pipeline and the overall
usability of NoteIt. Project website: https://zhaorunning.github.io/NoteIt/

</details>


### [38] [Detecting Reading-Induced Confusion Using EEG and Eye Tracking](https://arxiv.org/abs/2508.14442)
*Haojun Zhuang,Dünya Baradari,Nataliya Kosmyna,Arnav Balyan,Constanze Albrecht,Stephanie Chen,Pattie Maes*

Main category: cs.HC

TL;DR: 该研究通过结合EEG和眼动追踪技术，分析了阅读中产生的困惑的神经和行为特征，发现多模态模型能显著提高分类准确率，为开发实时监测困惑的自适应系统奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索阅读中困惑的神经和行为特征，以便开发能够动态检测和响应用户困惑的自适应系统，应用于个性化学习和人机交互等领域。

Method: 研究方法包括使用EEG和眼动追踪收集11名成年人的神经和注视数据，通过N400事件相关电位和行为标记分析困惑特征，并用机器学习评估多模态模型的分类效果。

Result: 多模态模型（EEG+眼动）比单模态模型分类准确率提高了4-22%，最高准确率达89.6%；结果还表明困惑的神经特征主要与大脑颞区相关。

Conclusion: 该研究为开发实时监测困惑的自适应系统提供了基础，未来可应用于个性化学习、人机交互和无障碍技术等领域。

Abstract: Humans regularly navigate an overwhelming amount of information via text
media, whether reading articles, browsing social media, or interacting with
chatbots. Confusion naturally arises when new information conflicts with or
exceeds a reader's comprehension or prior knowledge, posing a challenge for
learning. In this study, we present a multimodal investigation of
reading-induced confusion using EEG and eye tracking. We collected neural and
gaze data from 11 adult participants as they read short paragraphs sampled from
diverse, real-world sources. By isolating the N400 event-related potential
(ERP), a well-established neural marker of semantic incongruence, and
integrating behavioral markers from eye tracking, we provide a detailed
analysis of the neural and behavioral correlates of confusion during
naturalistic reading. Using machine learning, we show that multimodal (EEG +
eye tracking) models improve classification accuracy by 4-22% over unimodal
baselines, reaching an average weighted participant accuracy of 77.3% and a
best accuracy of 89.6%. Our results highlight the dominance of the brain's
temporal regions in these neural signatures of confusion, suggesting avenues
for wearable, low-electrode brain-computer interfaces (BCI) for real-time
monitoring. These findings lay the foundation for developing adaptive systems
that dynamically detect and respond to user confusion, with potential
applications in personalized learning, human-computer interaction, and
accessibility.

</details>


### [39] [Towards AI-based Sustainable and XR-based human-centric manufacturing: Implementation of ISO 23247 for digital twins of production systems](https://arxiv.org/abs/2508.14580)
*Huizhong Cao,Henrik Söderlund,Qi Fang,Siyuan Chen,Lejla Erdal,Ammar Gubartalla,Paulo Victor Lopes,Guodong Shao,Per Lonnehed,Henri Putto,Abbe Ahmed,Sven Ekered,Björn Johansson*

Main category: cs.HC

TL;DR: 本文探讨了数字孪生技术在工业5.0中的挑战及其解决方案，基于ISO 23247标准实现了实时数字孪生系统，并结合VR和AI提升操作员认知体验。


<details>
  <summary>Details</summary>
Motivation: 工业5.0强调以人为中心、可持续性和韧性，数字孪生技术为实时生产系统提供了效率、韧性和可持续性，但环境性能和与VR、AI的整合仍具挑战。

Method: 通过基于ISO 23247标准的实时数字孪生系统，连接物理工厂和仿真软件，结合VR能力，提供认知辅助和用户友好界面，并通过IoT平台实现双向通信。

Result: 在实验室规模的无人机工厂中测试了ISO 23247标准的潜力，展示了VR整合数字孪生的实际效益，并提出了AI整合和环境性能KPI的下一步方向。

Conclusion: 本文通过理论和实践验证了VR整合数字孪生的可行性，解决了技术整合问题，并推动了基于ISO 23247的数字孪生框架发展。

Abstract: Since the introduction of Industry 4.0, digital twin technology has
significantly evolved, laying the groundwork for a transition toward Industry
5.0 principles centered on human-centricity, sustainability, and resilience.
Through digital twins, real-time connected production systems are anticipated
to be more efficient, resilient, and sustainable, facilitating communication
and connectivity between digital and physical systems. However, environmental
performance and integration with virtual reality (VR) and artificial
intelligence (AI) of such systems remain challenging. Further exploration of
digital twin technologies is needed to validate the real-world impact and
benefits. This paper investigates these challenges by implementing a real-time
digital twin based on the ISO 23247 standard, connecting the physical factory
and simulation software with VR capabilities. This digital twin system provides
cognitive assistance and a user-friendly interface for operators, thereby
improving cognitive ergonomics. The connection of the Internet of Things (IoT)
platform allows the digital twin to have real-time bidirectional communication,
collaboration, monitoring, and assistance. A lab-scale drone factory was used
as the digital twin application to test and evaluate the ISO 23247 standard and
its potential benefits. Additionally, AI integration and environmental
performance Key Performance Indicators (KPIs) have been considered as the next
stages in improving VR-integrated digital twins. With a solid theoretical
foundation and a demonstration of the VR-integrated digital twins, this paper
addresses integration issues between various technologies and advances the
framework of digital twins based on ISO 23247.

</details>


### [40] [Topology-Aware Volume Fusion for Spectral Computed Tomography via Histograms and Extremum Graph](https://arxiv.org/abs/2508.14719)
*Mohit Sharma,Emma Nilsson,Martin Falk,Talha Bin Masood,Lee Jollans,Anders Persson,Tino Ebbers,Ingrid Hotz*

Main category: cs.HC

TL;DR: 该论文提出了一种将光子计数计算机断层扫描（PCCT）的多能量数据融合为单一代表性体积的方法，通过分析2D直方图和构建极值图，显著简化了数据分析和可视化过程。


<details>
  <summary>Details</summary>
Motivation: PCCT生成的多能量数据虽然能增强组织和材料的区分能力，但其复杂性和冗余性使得可视化和解释变得困难，因此需要一种有效的数据融合方法。

Method: 方法包括计算成对体积的2D直方图，构建极值图来捕捉关键结构特征，并通过降维生成一个代表性体积，融合多能量数据中的共享和互补信息。

Result: 生成的代表性体积保留了原始数据的结构和材料特性，同时将分析范围从多个体积缩减为一个，从而提高了可视化和分割的效率。

Conclusion: 该研究提出的拓扑感知的多能量数据融合方法为PCCT数据的可视化和分析提供了更高效的解决方案。

Abstract: Photon-Counting Computed Tomography (PCCT) is a novel imaging modality that
simultaneously acquires volumetric data at multiple X-ray energy levels,
generating separate volumes that capture energy-dependent attenuation
properties. Attenuation refers to the reduction in X-ray intensity as it passes
through different tissues or materials. This spectral information enhances
tissue and material differentiation, enabling more accurate diagnosis and
analysis. However, the resulting multivolume datasets are often complex and
redundant, making visualization and interpretation challenging. To address
these challenges, we propose a method for fusing spectral PCCT data into a
single representative volume that enables direct volume rendering and
segmentation by leveraging both shared and complementary information across
different channels. Our approach starts by computing 2D histograms between
pairs of volumes to identify those that exhibit prominent structural features.
These histograms reveal relationships and variations that may be difficult to
discern from individual volumes alone. Next, we construct an extremum graph
from the 2D histogram of two minimally correlated yet complementary
volumes-selected to capture both shared and distinct features-thereby
maximizing the information content. The graph captures the topological
distribution of histogram extrema. By extracting prominent structure within
this graph and projecting each grid point in histogram space onto it, we reduce
the dimensionality to one, producing a unified volume. This representative
volume retains key structural and material characteristics from the original
spectral data while significantly reducing the analysis scope from multiple
volumes to one. The result is a topology-aware, information-rich fusion of
multi-energy CT datasets that facilitates more effective visualization and
segmentation.

</details>


### [41] [Challenges and Opportunities for Participatory Design of Conversational Agents for Young People's Wellbeing](https://arxiv.org/abs/2508.14787)
*Natalia Kucirkova,Alexis Hiniker,Megumi Ishikawa,Sho Tsuji,Aayushi Dangol,Robert Wolfe*

Main category: cs.HC

TL;DR: 探讨了在四种国家背景下，研究儿童和青少年与对话代理互动的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 研究旨在展示人工智能技术如何在不同社会文化背景下支持儿童的福祉。

Method: 通过跨国研究分析对话代理与儿童互动的可能性。

Result: 揭示了对话代理在支持儿童福祉方面的潜力。

Conclusion: 研究表明，对话代理在不同文化背景下都能为儿童提供支持，但需进一步研究以优化其应用。

Abstract: This paper outlines the challenges and opportunities of research on
conversational agents with children and young people across four countries,
exploring the ways AI technologies can support children's well-being across
social and cultural contexts.

</details>


### [42] [From Passive Tool to Socio-cognitive Teammate: A Conceptual Framework for Agentic AI in Human-AI Collaborative Learning](https://arxiv.org/abs/2508.14825)
*Lixiang Yan*

Main category: cs.HC

TL;DR: 论文提出APCP框架，将AI从教育工具提升为协作伙伴，分为四个层级，并讨论AI在协作学习中的潜力与限制。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏理解人机协作在教育中的概念框架，AI正从工具演变为主动参与者。

Method: 基于社会文化学习理论，提出四层AI代理模型（从工具到协作伙伴）。

Result: 区分了AI的功能协作与真实协作能力，为教学设计提供了新方向。

Conclusion: 尽管AI无法真正协作，但可作为高效功能伙伴，设计互补学习环境是未来关键。

Abstract: The role of Artificial Intelligence (AI) in education is undergoing a rapid
transformation, moving beyond its historical function as an instructional tool
towards a new potential as an active participant in the learning process. This
shift is driven by the emergence of agentic AI, autonomous systems capable of
proactive, goal-directed action. However, the field lacks a robust conceptual
framework to understand, design, and evaluate this new paradigm of human-AI
interaction in learning. This paper addresses this gap by proposing a novel
conceptual framework (the APCP framework) that charts the transition from AI as
a tool to AI as a collaborative partner. We present a four-level model of
escalating AI agency within human-AI collaborative learning: (1) the AI as an
Adaptive Instrument, (2) the AI as a Proactive Assistant, (3) the AI as a
Co-Learner, and (4) the AI as a Peer Collaborator. Grounded in sociocultural
theories of learning and Computer-Supported Collaborative Learning (CSCL), this
framework provides a structured vocabulary for analysing the shifting roles and
responsibilities between human and AI agents. The paper further engages in a
critical discussion of the philosophical underpinnings of collaboration,
examining whether an AI, lacking genuine consciousness or shared
intentionality, can be considered a true collaborator. We conclude that while
AI may not achieve authentic phenomenological partnership, it can be designed
as a highly effective functional collaborator. This distinction has significant
implications for pedagogy, instructional design, and the future research agenda
for AI in education, urging a shift in focus towards creating learning
environments that harness the complementary strengths of both human and AI.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [43] [A Real-world Display Inverse Rendering Dataset](https://arxiv.org/abs/2508.14411)
*Seokjun Choi,Hoon-Gyu Chung,Yujin Jeon,Giljoo Nam,Seung-Hwan Baek*

Main category: cs.GR

TL;DR: 该论文提出了首个基于显示器的逆向渲染真实世界数据集，填补了相关领域的空白，并提出了一个简单有效的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏使用显示-相机系统捕获的真实世界数据集，这限制了基于显示器的逆向渲染方法的发展和评估。

Method: 构建了一个由LCD显示器和立体偏振相机组成的成像系统，捕获了多种几何和反射特性物体，使用了逐点亮光模式（OLAT）。

Result: 提供了一个高质量的真实数据集，并提出的基线方法在性能上超越了现有最先进的逆向渲染方法。

Conclusion: 该数据集和基线方法为基于显示器的逆向渲染研究提供了重要资源，推动了相关技术的发展。

Abstract: Inverse rendering aims to reconstruct geometry and reflectance from captured
images. Display-camera imaging systems offer unique advantages for this task:
each pixel can easily function as a programmable point light source, and the
polarized light emitted by LCD displays facilitates diffuse-specular
separation. Despite these benefits, there is currently no public real-world
dataset captured using display-camera systems, unlike other setups such as
light stages. This absence hinders the development and evaluation of
display-based inverse rendering methods. In this paper, we introduce the first
real-world dataset for display-based inverse rendering. To achieve this, we
construct and calibrate an imaging system comprising an LCD display and stereo
polarization cameras. We then capture a diverse set of objects with diverse
geometry and reflectance under one-light-at-a-time (OLAT) display patterns. We
also provide high-quality ground-truth geometry. Our dataset enables the
synthesis of captured images under arbitrary display patterns and different
noise levels. Using this dataset, we evaluate the performance of existing
photometric stereo and inverse rendering methods, and provide a simple, yet
effective baseline for display inverse rendering, outperforming
state-of-the-art inverse rendering methods. Code and dataset are available on
our project page at https://michaelcsj.github.io/DIR/

</details>


### [44] [MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds](https://arxiv.org/abs/2508.14879)
*Bingquan Dai,Li Ray Luo,Qihong Tang,Jie Wang,Xinyu Lian,Hao Xu,Minghan Qin,Xudong Xu,Bo Dai,Haoqian Wang,Zhaoyang Lyu,Jiangmiao Pang*

Main category: cs.GR

TL;DR: MeshCoder框架将3D点云重建为可编辑的Blender Python脚本，支持复杂几何和直观编辑。通过大型数据集和LLM，提升了3D形状理解能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖有限领域特定语言和小数据集的问题，支持复杂几何结构的建模和编辑。

Method: 开发Blender Python API构建数据集，训练多模态LLM将点云转换为可执行脚本。

Result: 在形状到代码重建任务中表现优异，支持通过代码修改实现几何和拓扑编辑。

Conclusion: MeshCoder为程序化3D形状重建和理解提供了强大灵活的解决方案。

Abstract: Reconstructing 3D objects into editable programs is pivotal for applications
like reverse engineering and shape editing. However, existing methods often
rely on limited domain-specific languages (DSLs) and small-scale datasets,
restricting their ability to model complex geometries and structures. To
address these challenges, we introduce MeshCoder, a novel framework that
reconstructs complex 3D objects from point clouds into editable Blender Python
scripts. We develop a comprehensive set of expressive Blender Python APIs
capable of synthesizing intricate geometries. Leveraging these APIs, we
construct a large-scale paired object-code dataset, where the code for each
object is decomposed into distinct semantic parts. Subsequently, we train a
multimodal large language model (LLM) that translates 3D point cloud into
executable Blender Python scripts. Our approach not only achieves superior
performance in shape-to-code reconstruction tasks but also facilitates
intuitive geometric and topological editing through convenient code
modifications. Furthermore, our code-based representation enhances the
reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these
contributions establish MeshCoder as a powerful and flexible solution for
programmatic 3D shape reconstruction and understanding.

</details>


### [45] [Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds](https://arxiv.org/abs/2508.14892)
*Jia Lu,Taoran Yi,Jiemin Fang,Chen Yang,Chuiyun Wu,Wei Shen,Wenyu Liu,Qi Tian,Xinggang Wang*

Main category: cs.GR

TL;DR: 该论文提出了一种仅需正面和背面两张图像即可重建3D人体的方法，通过几何重建模型和增强算法实现快速且高质量的重建。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图下的3D人体重建对扩展应用至关重要，但目前方法需要多张图像，用户门槛较高。

Method: 重新设计基于基础重建模型的几何重建模型，预测一致的点云；应用增强算法补充缺失的颜色信息。

Result: 实验表明，该方法在单张NVIDIA RTX 4090上仅需190毫秒即可完成重建，性能优于现有技术。

Conclusion: 该方法降低了数据采集要求，适用于低成本设备，展示了在大规模数据集上的优越表现。

Abstract: Reconstructing 3D human bodies from sparse views has been an appealing topic,
which is crucial to broader the related applications. In this paper, we propose
a quite challenging but valuable task to reconstruct the human body from only
two images, i.e., the front and back view, which can largely lower the barrier
for users to create their own 3D digital humans. The main challenges lie in the
difficulty of building 3D consistency and recovering missing information from
the highly sparse input. We redesign a geometry reconstruction model based on
foundation reconstruction models to predict consistent point clouds even input
images have scarce overlaps with extensive human data training. Furthermore, an
enhancement algorithm is applied to supplement the missing color information,
and then the complete human point clouds with colors can be obtained, which are
directly transformed into 3D Gaussians for better rendering quality.
Experiments show that our method can reconstruct the entire human in 190 ms on
a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,
demonstrating state-of-the-art performance on the THuman2.0 and cross-domain
datasets. Additionally, our method can complete human reconstruction even with
images captured by low-cost mobile devices, reducing the requirements for data
collection. Demos and code are available at
https://hustvl.github.io/Snap-Snap/.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [46] [Time-optimal Asynchronous Minimal Vertex Covering by Myopic Robots](https://arxiv.org/abs/2508.14247)
*Saswata Jana,Subhajit Pramanick,Adri Bhattacharya,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 论文研究了有限视野的自主机器人群体如何在图中部署以满足最小顶点覆盖（MVC）的局部知识问题，提出了针对树和一般图的不同算法，并分析了其时间和空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决有限视野机器人群体如何通过局部知识在图中实现最小顶点覆盖的部署问题。

Method: 提出了针对树结构的两种算法（单门和多门）以及针对一般图的两种算法，分析了可见性范围和时间复杂度。

Result: 在树上实现了最小顶点覆盖（MinVC），在一般图中提出了时间复杂度为O(|E|)的算法。

Conclusion: 即使在有限视野和局部知识下，机器人群体可以实现最小顶点覆盖部署，算法在特定条件下达到最优。

Abstract: In a connected graph with an autonomous robot swarm with limited visibility,
it is natural to ask whether the robots can be deployed to certain vertices
satisfying a given property using only local knowledge. This paper
affirmatively answers the question with a set of \emph{myopic} (finite
visibility range) luminous robots with the aim of \emph{filling a minimal
vertex cover} (MVC) of a given graph $G = (V, E)$. The graph has special
vertices, called \emph{doors}, through which robots enter sequentially.
Starting from the doors, the goal of the robots is to settle on a set of
vertices that forms a minimal vertex cover of $G$ under the asynchronous
($\mathcal{ASYNC}$) scheduler. We are also interested in achieving the
\emph{minimum vertex cover} (MinVC, which is NP-hard \cite{Karp1972} for
general graphs) for a specific graph class using the myopic robots. We
establish lower bounds on the visibility range for the robots and on the time
complexity (which is $\Omega(|E|)$). We present two algorithms for trees: one
for single door, which is both time and memory-optimal, and the other for
multiple doors, which is memory-optimal and achieves time-optimality when the
number of doors is a constant. Interestingly, our technique achieves MinVC on
trees with a single door. We then move to the general graph, where we present
two algorithms, one for the single door and the other for the multiple doors
with an extra memory of $O(\log \Delta)$ for the robots, where $\Delta$ is the
maximum degree of $G$. All our algorithms run in $O(|E|)$ epochs.

</details>


### [47] [Pure Data Spaces](https://arxiv.org/abs/2508.14271)
*Saul Youssef*

Main category: cs.DC

TL;DR: 该论文提出了一种基于“纯数据”作为数学和计算基础的框架，以“有限序列”为核心概念，而非逻辑或类型。通过“空间”这一基本集合概念，研究了其半环结构，并通过实例展示了框架的潜力。


<details>
  <summary>Details</summary>
Motivation: 目的是建立一个基于数据的数学和计算基础，摆脱传统逻辑或类型的限制，探索新的理论和研究方向。

Method: 通过定义“空间”作为基本集合，研究其自同态半环，并通过“有机生长”的方式从纯数据中构建数学对象。

Result: 从框架中自然衍生出经典数学对象，如自然数、矩阵代数、四元数等，并讨论了其对理论的新启示。

Conclusion: 该框架为数学和计算提供了新的基础，展示了其在理论和实践中的潜力，值得进一步探索。

Abstract: In a previous work, "pure data" is proposed as an axiomatic foundation for
mathematics and computing, based on "finite sequence" as the foundational
concept rather than based on logic or type. Within this framework, objects with
mathematical meaning are "data" and collections of mathematical objects must
then be associative data, called a "space." A space is then the basic
collection in this framework analogous to sets in Set Theory or objects in
Category Theory. A theory of spaces is developed,where spaces are studied via
their semiring of endomorphisms. To illustrate these concepts, and as a way of
exploring the implications of the framework, pure data spaces are "grown
organically" from the substrate of pure data with minimal combinatoric
definitions. Familiar objects from classical mathematics emerge this way,
including natural numbers, integers, rational numbers, boolean spaces, matrix
algebras, Gaussian Integers, Quaternions, and non-associative algebras like the
Integer Octonions. Insights from these examples are discussed with a view
towards new directions in theory and new exploration.

</details>


### [48] [SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path](https://arxiv.org/abs/2508.14319)
*Parshan Javanrood,Matei Ripeanu*

Main category: cs.DC

TL;DR: 论文提出了一种名为SSSP-Del的动态SSSP算法，解决了大规模动态图中SSSP问题的高延迟和分布内存处理问题。


<details>
  <summary>Details</summary>
Motivation: 现代图数据规模大且动态变化，现有动态SSSP算法难以同时处理边增减、分布内存和低延迟查询需求。

Method: 提出SSSP-Del算法，采用顶点中心、异步和完全分布式的架构，处理边插入和删除的流数据。

Result: 在百万级真实和合成图上进行了全面评估，分析了延迟、解决方案稳定性和吞吐量。

Conclusion: SSSP-Del算法能够有效支持大规模动态图的SSSP查询需求，解决了现有技术的不足。

Abstract: Modern graphs are both large and dynamic, presenting significant challenges
for fundamental queries, such as the Single-Source Shortest Path (SSSP)
problem. Naively recomputing the SSSP tree after each topology change is
prohibitively expensive, causing on-demand computation to suffer from high
latency. Existing dynamic SSSP algorithms often cannot simultaneously handle
both edge additions and deletions, operate in distributed memory, and provide
low-latency query results. To address these challenges, this paper presents
SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm
for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm
processes streams of both edge insertions and deletions. We conduct a
comprehensive evaluation on large real-world and synthetic graphs with millions
of vertices, and provide a thorough analysis by evaluating result latency,
solution stability, and throughput.

</details>


### [49] [A Hierarchical Sharded Blockchain Balancing Performance and Availability](https://arxiv.org/abs/2508.14457)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: PyloChain是一种分层分片区块链，通过在多个本地链上并行执行本地事务，并在主链上使用DAG-based mempool确保数据可用性和高效BFT共识，平衡了性能与可用性。其性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决区块链分片技术中性能与可用性的不平衡问题，尤其是在少数服务器故障导致数据不可用的情况下。

Method: PyloChain采用分层结构，包括多个本地链和一个主链，利用DAG-based mempool和BFT共识处理全局事务，并通过调度技术减少本地事务的中断。

Result: PyloChain性能优越，吞吐量提升1.49倍，延迟降低2.63倍。

Conclusion: PyloChain通过分层设计和高效共识机制，成功平衡了区块链的可用性与性能。

Abstract: Blockchain networks offer decentralization, transparency, and immutability
for managing critical data but encounter scalability problems as the number of
network members and transaction issuers grows. Sharding is considered a
promising solution to enhance blockchain scalability. However, most existing
blockchain sharding techniques prioritize performance at the cost of
availability (e.g., a failure in a few servers holding a shard leads to data
unavailability). In this paper, we propose PyloChain, a hierarchical sharded
blockchain that balances availability and performance. PyloChain consists of
multiple lower-level local chains and one higher-level main chain. Each local
chain speculatively executes local transactions to achieve high parallelism
across multiple local chains. The main chain leverages a directed-acyclic-graph
(DAG)-based mempool to guarantee local block availability and to enable
efficient Byzantine Fault Tolerance (BFT) consensus to execute global (or
cross-shard) transactions within a collocated sharding. PyloChain speculatively
executes local transactions across multiple local chains to achieve high
parallelism. In order to reduce the number of aborted local transactions,
PyloChain applies a simple scheduling technique to handle global transactions
in the main chain. PyloChain provides a fine-grained auditing mechanism to
mitigate faulty higher-level members by externalizing main chain operations to
lower-level local members. We implemented and evaluated PyloChain,
demonstrating its performance scalability with 1.49x higher throughput and
2.63x faster latency compared to the state-of-the-art balanced hierarchical
sharded blockchain.

</details>


### [50] [Auditable Shared Objects: From Registers to Synchronization Primitives](https://arxiv.org/abs/2508.14506)
*Hagit Attiya,Antonio Fernández Anta,Alessia Milani,Alexandre Rapetti,Corentin Travers*

Main category: cs.DC

TL;DR: 本文研究了可审计共享对象的扩展，从单写扩展到多写寄存器，并实现了高效的n写m读寄存器。方法包括滑动寄存器和LL/SC原语。结果表明共识数是必要的，并展示了其在访问控制中的应用。


<details>
  <summary>Details</summary>
Motivation: 扩展可审计性到多写共享对象，增强数据所有者对数据的控制能力。

Method: 从单写扩展到多写寄存器，实现n写m读寄存器，使用滑动寄存器和LL/SC原语。

Result: 证明共识数的必要性，并展示可审计寄存器在访问控制中的应用（如反闪烁拒绝列表）。

Conclusion: 可审计共享对象的扩展是可行的，并通过高效实现展示了其实际应用潜力。

Abstract: Auditability allows to track operations performed on a shared object,
recording who accessed which information. This gives data owners more control
on their data. Initially studied in the context of single-writer registers,
this work extends the notion of auditability to other shared objects, and
studies their properties.
  We start by moving from single-writer to multi-writer registers, and provide
an implementation of an auditable $n$-writer $m$-reader read / write register,
with $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding
registers, which have consensus number $m+n$. We show that this consensus
number is necessary. The implementation extends naturally to support an
auditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a
primitive that supports efficient implementation of many shared objects.
Finally, we relate auditable registers to other access control objects, by
implementing an anti-flickering deny list from auditable registers.

</details>


### [51] [Boosting Payment Channel Network Liquidity with Topology Optimization and Transaction Selection](https://arxiv.org/abs/2508.14524)
*Krishnendu Chatterjee,Jan Matyáš Křišťan,Stefan Schmid,Jakub Svoboda,Michelle Yeo*

Main category: cs.DC

TL;DR: 摘要讨论了支付通道网络（PCN）如何通过将交易负载从区块链转移到PCN来缓解可扩展性问题，并强调了网络拓扑设计和交易决策的重要性。作者提出了一种近似算法以最小化建通道和拒绝交易的成本，并在理论上和实践中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 支付通道网络（PCN）能解决区块链的可扩展性问题，但需要优化网络拓扑和交易决策以提高吞吐量并延长通道寿命。本文旨在设计一种方法，使其在成本和效率上表现更好。

Method: 提出了一个基于输入交易序列的PCN拓扑和通道容量设计方法，并开发了一种近似算法来最小化成本和拒绝交易的损失。理论上分析了算法的近似比，并在特定假设下进一步优化。

Result: 提出的算法在理论上实现了O(p)和O(√p)的近似比，同时通过实证研究验证了其在闪电网络中的实用性。

Conclusion: 本研究为PCN中的拓扑设计和交易决策提供了一种高效且成本优化的解决方案，并通过理论和实验验证了其有效性。

Abstract: Payment channel networks (PCNs) are a promising technology that alleviates
blockchain scalability by shifting the transaction load from the blockchain to
the PCN. Nevertheless, the network topology has to be carefully designed to
maximise the transaction throughput in PCNs. Additionally, users in PCNs also
have to make optimal decisions on which transactions to forward and which to
reject to prolong the lifetime of their channels. In this work, we consider an
input sequence of transactions over $p$ parties. Each transaction consists of a
transaction size, source, and target, and can be either accepted or rejected
(entailing a cost). The goal is to design a PCN topology among the $p$
cooperating parties, along with the channel capacities, and then output a
decision for each transaction in the sequence to minimise the cost of creating
and augmenting channels, as well as the cost of rejecting transactions. Our
main contribution is an $\mathcal{O}(p)$ approximation algorithm for the
problem with $p$ parties. We further show that with some assumptions on the
distribution of transactions, we can reduce the approximation ratio to
$\mathcal{O}(\sqrt{p})$. We complement our theoretical analysis with an
empirical study of our assumptions and approach in the context of the Lightning
Network.

</details>


### [52] [A Systematic Evaluation of the Potential of Carbon-Aware Execution for Scientific Workflows](https://arxiv.org/abs/2508.14625)
*Kathleen West,Youssef Moawad,Fabian Lehmann,Vasilis Bountris,Ulf Leser,Yehia Elkhatib,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 总结：科学工作流在计算集群上的运行通常能耗高且碳排放多，但碳感知计算可显著减少排放。


<details>
  <summary>Details</summary>
Motivation: 动机：科学工作流在计算集群上的执行能耗高，碳排放问题严重，需要碳感知计算来优化。

Method: 方法：评估七种真实工作流的碳足迹，分析时间转移、暂停恢复及资源扩展对碳减排的影响。

Result: 结果：时间转移可减少80%排放，资源扩展可减少67%排放。

Conclusion: 结论：碳感知计算能显著降低科学工作流的碳排放，时间转移和资源扩展是有效手段。

Abstract: Scientific workflows are widely used to automate scientific data analysis and
often involve computationally intensive processing of large datasets on compute
clusters. As such, their execution tends to be long-running and
resource-intensive, resulting in substantial energy consumption and, depending
on the energy mix, carbon emissions. Meanwhile, a wealth of carbon-aware
computing methods have been proposed, yet little work has focused specifically
on scientific workflows, even though they present a substantial opportunity for
carbon-aware computing because they are often significantly delay tolerant,
efficiently interruptible, highly scalable and widely heterogeneous. In this
study, we first exemplify the problem of carbon emissions associated with
running scientific workflows, and then show the potential for carbon-aware
workflow execution. For this, we estimate the carbon footprint of seven
real-world Nextflow workflows executed on different cluster infrastructures
using both average and marginal carbon intensity data. Furthermore, we
systematically evaluate the impact of carbon-aware temporal shifting, and the
pausing and resuming of the workflow. Moreover, we apply resource scaling to
workflows and workflow tasks. Finally, we report the potential reduction in
overall carbon emissions, with temporal shifting capable of decreasing
emissions by over 80%, and resource scaling capable of decreasing emissions by
67%.

</details>


### [53] [DAG it off: Latency Prefers No Common Coins](https://arxiv.org/abs/2508.14716)
*Amores-Sesar Ignacio,Grøndal Viktor,Holmgård Adam,Ottendal Mads*

Main category: cs.DC

TL;DR: Black Marlin是首个在部分同步设置中基于DAG的拜占庭原子广播协议，无需可靠广播和公共硬币原语，实现了最优延迟和通信复杂度。


<details>
  <summary>Details</summary>
Motivation: 减少拜占庭原子广播协议对可靠广播和公共硬币原语的依赖，同时优化延迟和通信效率。

Method: 基于DAG设计协议，并通过形式化安全分析和实验验证其性能。

Result: 达到3轮通信的最优延迟（拜占庭故障下为4.25轮），并在吞吐量和延迟上优于现有DAG协议。

Conclusion: Black Marlin协议在性能和安全性上均表现优异，为拜占庭容错系统提供了高效解决方案。

Abstract: We introduce Black Marlin, the first Directed Acyclic Graph (DAG)-based
Byzantine atomic broadcast protocol in a partially synchronous setting that
successfully forgoes the reliable broadcast and common coin primitives. Black
Marlin achieves the optimal latency of 3 rounds of communication (4.25 with
Byzantine faults) while maintaining optimal communication and amortized
communication complexities. We present a formal security analysis of the
protocol, accompanied by empirical evidence that Black Marlin outperforms
state-of-the-art DAG-based protocols in both throughput and latency.

</details>


### [54] [MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems](https://arxiv.org/abs/2508.14830)
*Kushagra Agrawal,Polat Goktas,Anjan Bandopadhyay,Debolina Ghosh,Junali Jasmine Jena,Mahendra Kumar Gourisaria*

Main category: cs.DC

TL;DR: MOHAF是一种多目标分层拍卖框架，用于分布式资源分配，优化成本、QoS、能效和公平性，实验显示其效率优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决物联网中动态分布式环境下的资源分配问题，传统集中式和单目标拍卖模型难以平衡系统性能。

Method: 结合分层聚类降低计算复杂度，采用贪婪次模优化策略和动态定价机制。

Result: 在Google Cluster Data上的实验显示，MOHAF效率最高（0.263），公平性完美（Jain's指数=1.000）。

Conclusion: MOHAF为大规模物联网提供高效、公平且可持续的资源分配方案。

Abstract: The rapid growth of Internet of Things (IoT) ecosystems has intensified the
challenge of efficiently allocating heterogeneous resources in highly dynamic,
distributed environments. Conventional centralized mechanisms and
single-objective auction models, focusing solely on metrics such as cost
minimization or revenue maximization, struggle to deliver balanced system
performance. This paper proposes the Multi-Objective Hierarchical Auction
Framework (MOHAF), a distributed resource allocation mechanism that jointly
optimizes cost, Quality of Service (QoS), energy efficiency, and fairness.
MOHAF integrates hierarchical clustering to reduce computational complexity
with a greedy, submodular optimization strategy that guarantees a (1-1/e)
approximation ratio. A dynamic pricing mechanism adapts in real time to
resource utilization, enhancing market stability and allocation quality.
Extensive experiments on the Google Cluster Data trace, comprising 3,553
requests and 888 resources, demonstrate MOHAF's superior allocation efficiency
(0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101)
auctions, while achieving perfect fairness (Jain's index = 1.000). Ablation
studies reveal the critical influence of cost and QoS components in sustaining
balanced multi-objective outcomes. With near-linear scalability, theoretical
guarantees, and robust empirical performance, MOHAF offers a practical and
adaptable solution for large-scale IoT deployments, effectively reconciling
efficiency, equity, and sustainability in distributed resource coordination.

</details>


### [55] [Leveraging Hardware-Aware Computation in Mixed-Precision Matrix Multiply: A Tile-Centric Approach](https://arxiv.org/abs/2508.14848)
*Qiao Zhang,Rabab Alomairy,Dali Wang,Zhuowei Gu,Qinglei Cao*

Main category: cs.DC

TL;DR: 该研究提出了一种自适应混合精度的GEMM框架，支持细粒度块级别的不同精度格式，旨在通过结合算法进步和硬件创新提升计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着低精度算术硬件优化的兴起，需要重新评估数值算法以利用混合精度计算，从而改善性能和能效。

Method: 利用PaRSEC运行时系统在不同架构上平衡工作负载，并在ARM CPU、Nvidia GPU和AMD GPU等多种硬件上进行测试。

Result: 在Fugaku、A100 DGX和Frontier等超级计算机上表现良好，性能扩展性强。

Conclusion: 该研究通过混合精度计算和硬件适配，推动了高性能计算和人工智能领域的进步。

Abstract: General Matrix Multiplication (GEMM) is a critical operation underpinning a
wide range of applications in high-performance computing (HPC) and artificial
intelligence (AI). The emergence of hardware optimized for low-precision
arithmetic necessitates a reevaluation of numerical algorithms to leverage
mixed-precision computations, achieving improved performance and energy
efficiency. This research introduces an adaptive mixed-precision GEMM framework
that supports different precision formats at fine-grained tile/block levels. We
utilize the PaRSEC runtime system to balance workloads across various
architectures. The performance scales well on ARM CPU-based Fugaku
supercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier
supercomputer. This research aims to enhance computational efficiency and
accuracy by bridging algorithmic advancements and hardware innovations, driving
transformative progress in various applications.

</details>


### [56] [The Cost Advantage of Virtual Machine Migrations: Empirical Insights into Amazon's EC2 Marketspace](https://arxiv.org/abs/2508.14883)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.DC

TL;DR: 论文分析了云计算中虚拟机的交易和成本优化，特别关注了异构云组合的成本效益和迁移策略。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过不同市场和提供商的虚拟机组合来优化云计算成本，填补了行业研究的空白。

Method: 利用亚马逊的定价数据和Bitbrains数据中心的使用数据集，进行成本分析。

Result: 异构云组合能实现成本最优，虚拟机迁移在特定运行时间内尤其经济。

Conclusion: 用户未充分利用虚拟机资源，未来成本优化潜力巨大。

Abstract: In recent years, cloud providers have introduced novel approaches for trading
virtual machines. For example, Virtustream introduced so-called muVMs to charge
cloud computing resources while other providers such as Google, Microsoft, or
Amazon re-invented their marketspaces. Today, the market leader Amazon runs six
marketspaces for trading virtual machines. Consumers can purchase bundles of
virtual machines, which are called cloud-portfolios, from multiple marketspaces
and providers. An industry-relevant field of research is to identify best
practices and guidelines on how such optimal portfolios are created. In the
paper at hand, a cost analysis of cloud portfolios is presented. Therefore,
pricing data from Amazon was used as well as a real virtual machine utilization
dataset from the Bitbrains datacenter. The results show that a cost optimum can
only be reached if heterogeneous portfolios are created where virtual machines
are purchased from different marketspaces. Additionally, the cost-benefit of
migrating virtual machines to different marketplaces during runtime is
presented. Such migrations are especially cost-effective for virtual machines
of cloud-portfolios which run between 6 hours and 1 year. The paper further
shows that most of the resources of virtual machines are never utilized by
consumers, which represents a significant future potential for cost
optimization. For the validation of the results, a second dataset of the
Bitbrains datacenter was used, which contains utility data of virtual machines
from a different domain of application.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [57] [Accelerating K-Core Computation in Temporal Graphs](https://arxiv.org/abs/2508.14147)
*Zhuo Ma,Dong Wen,Hanchen Wang,Wentao Li,Wenjie Zhang,Xuemin Lin*

Main category: cs.DB

TL;DR: 提出一种基于核心时间的新算法，高效枚举时态k-核，运行时间与结果规模相关。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法效率低、扩展性差的问题。

Method: 基于核心时间概念设计新算法。

Result: 算法运行时间与结果规模相关，且计算核心时间的代价更低。

Conclusion: 新算法在效率和可扩展性上优于现有方法。

Abstract: We address the problem of enumerating all temporal k-cores given a query time
range and a temporal graph, which suffers from poor efficiency and scalability
in the state-of-the-art solution. Motivated by an existing concept called core
times, we propose a novel algorithm to compute all temporal $k$-cores based on
core times and prove that the algorithmic running time is bounded by the size
of all resulting temporal k-cores, which is optimal in this scenario.
Meanwhile, we show that the cost of computing core times is much lower, which
demonstrates the close relevance between our overall running time and the
result size.

</details>


### [58] [Efficient Size Constraint Community Search over Heterogeneous Information Networks](https://arxiv.org/abs/2508.14356)
*Xinjian Zhang,Lu Chen,Chengfei Liu,Rui Zhou,Bo Ning*

Main category: cs.DB

TL;DR: 该论文提出了一个在异构信息网络（HINs）中进行社区搜索的问题，考虑了社区大小的限制。通过改进的(k, P)-truss模型衡量社区凝聚力，开发了B\&B框架和启发式算法来解决问题。实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有HIN社区搜索工作大多忽略了对社区大小的限制，实践中这种限制是必要的。

Method: 提出改进的(k, P)-truss模型衡量社区凝聚力，开发B\&B框架及启发式算法，并设计了两种精确算法。

Result: 证明了问题的NP难度，实验验证了所提方法的有效性和高效性。

Conclusion: 提出的方法能够高效地解决带大小限制的HIN社区搜索问题，优于现有工作。

Abstract: The goal of community search in heterogeneous information networks (HINs) is
to identify a set of closely related target nodes that includes a query target
node. In practice, a size constraint is often imposed due to limited resources,
which has been overlooked by most existing HIN community search works. In this
paper, we introduce the size-bounded community search problem to HIN data.
Specifically, we propose a refined (k, P)-truss model to measure community
cohesiveness, aiming to identify the most cohesive community of size s that
contains the query node. We prove that this problem is NP-hard. To solve this
problem, we develop a novel B\&B framework that efficiently generates target
node sets of size s. We then tailor novel bounding, branching, total ordering,
and candidate reduction optimisations, which enable the framework to
efficiently lead to an optimum result. We also design a heuristic algorithm
leveraging structural properties of HINs to efficiently obtain a high-quality
initial solution, which serves as a global lower bound to further enhance the
above optimisations. Building upon these, we propose two exact algorithms that
enumerate combinations of edges and nodes, respectively. Extensive experiments
on real-world datasets demonstrate the effectiveness and efficiency of the
proposed methods.

</details>


### [59] [A DBMS-independent approach for capturing provenance polynomials through query rewriting](https://arxiv.org/abs/2508.14608)
*Paulo Pintor,Rogério Costa,José Moreira*

Main category: cs.DB

TL;DR: 本文提出了一种基于查询重写的方法，用于在SQL查询中标注溯源多项式，支持SPJUA操作和嵌套查询，并通过实验验证了其性能和可扩展性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前溯源多项式在数据库查询中的应用存在局限性，特别是在处理聚合和嵌套查询时，且多依赖于单一数据库管理系统，影响了通用性。

Method: 采用查询重写技术，递归传播溯源标注，实现了DBMS无关的SPJUA操作和嵌套查询支持。

Result: 实验证明该方法完整实现了理论形式化，性能与可扩展性优于现有方法。

Conclusion: 该方法为溯源多项式理论提供了首个完整实现，提升了实际应用的可行性和效率。

Abstract: In today's data-driven ecosystems, ensuring data integrity, traceability and
accountability is important. Provenance polynomials constitute a powerful
formalism for tracing the origin and the derivations made to produce database
query results. Despite their theoretical expressiveness, current
implementations have limitations in handling aggregations and nested queries,
and some of them and tightly coupled to a single Database Management System
(DBMS), hindering interoperability and broader applicability.
  This paper presents a query rewriting-based approach for annotating
Structured Query Language (SQL) queries with provenance polynomials. The
proposed methods are DBMS-independent and support
Select-Projection-Join-Union-Aggregation (SPJUA) operations and nested queries,
through recursive propagation of provenance annotations. This constitutes the
first full implementation of semiring-based theory for provenance polynomials
extended with semimodule structures. It also presents an experimental
evaluation to assess the validity of the proposed methods and compare the
performance against state-of-the-art systems using benchmark data and queries.
The results indicate that our solution delivers a comprehensive implementation
of the theoretical formalisms proposed in the literature, and demonstrates
improved performance and scalability, outperforming existing methods.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [60] [MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging](https://arxiv.org/abs/2508.14053)
*Jinwei Tang,Jiayin Qin,Nuo Xu,Pragnya Sudershan Nalla,Yu Cao,Yang,Zhao,Caiwen Ding*

Main category: cs.AR

TL;DR: MAHL 是一个基于分层 LLM 的小芯片设计生成框架，通过多智能体协作优化 Power、Performance 和 Area (PPA)，显著提升了生成精度。


<details>
  <summary>Details</summary>
Motivation: 随着程序工作负载（如 AI）规模和算法复杂度的增加，高维度问题（如计算核心、数组大小和内存层次）成为主要挑战。需要创新方法解决传统小芯片设计中的扁平化设计、验证成本高和参数优化不精确等问题。

Method: 提出 MAHL 框架，包含六个协同工作的智能体，支持分层描述生成、检索增强代码生成、多样化验证和多粒度设计空间探索。

Result: 实验表明，MAHL 显著提升了 RTL 设计的生成精度，并在真实小芯片设计中将 Pass@5 从 0 提升至 0.72，部分优化目标下 PPA 结果优于专家系统 CLARIE。

Conclusion: MAHL 通过分层 LLM 和多智能体协作，有效解决了小芯片设计中的挑战，并在生成精度和 PPA 优化方面表现出色。

Abstract: As program workloads (e.g., AI) increase in size and algorithmic complexity,
the primary challenge lies in their high dimensionality, encompassing computing
cores, array sizes, and memory hierarchies. To overcome these obstacles,
innovative approaches are required. Agile chip design has already benefited
from machine learning integration at various stages, including logic synthesis,
placement, and routing. With Large Language Models (LLMs) recently
demonstrating impressive proficiency in Hardware Description Language (HDL)
generation, it is promising to extend their abilities to 2.5D integration, an
advanced technique that saves area overhead and development costs. However,
LLM-driven chiplet design faces challenges such as flatten design, high
validation cost and imprecise parameter optimization, which limit its chiplet
design capability. To address this, we propose MAHL, a hierarchical LLM-based
chiplet design generation framework that features six agents which
collaboratively enable AI algorithm-hardware mapping, including hierarchical
description generation, retrieval-augmented code generation, diverseflow-based
validation, and multi-granularity design space exploration. These components
together enhance the efficient generation of chiplet design with optimized
Power, Performance and Area (PPA). Experiments show that MAHL not only
significantly improves the generation accuracy of simple RTL design, but also
increases the generation accuracy of real-world chiplet design, evaluated by
Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case
scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves
comparable or even superior PPA results under certain optimization objectives.

</details>


### [61] [Revisit Choice Network for Synthesis and Technology Mapping](https://arxiv.org/abs/2508.14068)
*Chen Chen,Jiaqi Yin,Cunxi Yu*

Main category: cs.AR

TL;DR: Cristal是一种新型布尔选择网络构建方法，通过逻辑锥搜索、结构变异和优先级选择生成高质量选择，显著减少面积/延迟并提高效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在选择网络构建中对选择质量的忽视问题，以提高技术在布尔优化和等效检查中的映射效果。

Method: Cristal结合逻辑锥搜索、结构变异（通过等式饱和生成多样结构）和优先级选择，构建高质量选择网络。

Result: 实验显示，Cristal在延迟/面积优化的后映射阶段显著优于ABC，平均减少3.85%/8.35%（延迟/面积），并减少63.77%运行时间。

Conclusion: Cristal通过优化选择网络构建流程，显著提升了布尔网络的质量和效率。

Abstract: Choice network construction is a critical technique for alleviating
structural bias issues in Boolean optimization, equivalence checking, and
technology mapping. Previous works on lossless synthesis utilize independent
optimization to generate multiple snapshots, and use simulation and SAT solvers
to identify functionally equivalent nodes. These nodes are then merged into a
subject graph with choice nodes. However, such methods often neglect the
quality of these choices, raising the question of whether they truly contribute
to effective technology mapping.
  This paper introduces Cristal, a novel methodology and framework for
constructing Boolean choice networks. Specifically, Cristal introduces a new
flow of choice network-based synthesis and mapping, including representative
logic cone search, structural mutation for generating diverse choice structures
via equality saturation, and priority-ranking choice selection along with
choice network construction and validation. Through these techniques, Cristal
constructs fewer but higher-quality choices.
  Our experimental results demonstrate that Cristal outperforms the
state-of-the-art Boolean choice network construction implemented in ABC in the
post-mapping stage, achieving average reductions of 3.85%/8.35% (area/delay) in
delay-oriented mode, 0.11%/2.74% in area-oriented mode, and a 63.77% runtime
reduction on large-scale cases across a diverse set of combinational circuits
from the IWLS 2005, ISCAS'89, and EPFL benchmark suites.

</details>


### [62] [AI Agents for Photonic Integrated Circuit Design Automation](https://arxiv.org/abs/2508.14123)
*Ankita Sharma,YuQi Fu,Vahid Ansari,Rishabh Iyer,Fiona Kuang,Kashish Mistry,Raisa Islam Aishy,Sara Ahmad,Joaquin Matres,Dirk R. Englund,Joyce K. S. Poon*

Main category: cs.AR

TL;DR: PhIDO是一个多智能体框架，能将自然语言的光子集成电路设计请求转换为布局掩模文件。测试了7种大型语言模型，最高成功率达91%。


<details>
  <summary>Details</summary>
Motivation: 目标是实现光子集成电路设计的自然语言驱动，提高设计效率和自动化程度。

Method: 使用多智能体框架PhIDO，对比7种大型语言模型的性能，测试102个设计描述。

Result: 单器件设计成功率高达91%，15组件以下设计中，o1、Gemini-2.5-pro和Claude Opus 4表现最佳。

Conclusion: 未来需标准化知识表示、扩展数据集、完善验证和引入机器人自动化。

Abstract: We present Photonics Intelligent Design and Optimization (PhIDO), a
multi-agent framework that converts natural-language photonic integrated
circuit (PIC) design requests into layout mask files. We compare 7 reasoning
large language models for PhIDO using a testbench of 102 design descriptions
that ranged from single devices to 112-component PICs. The success rate for
single-device designs was up to 91%. For design queries with less than or equal
to 15 components, o1, Gemini-2.5-pro, and Claude Opus 4 achieved the highest
end-to-end pass@5 success rates of approximately 57%, with Gemini-2.5-pro
requiring the fewest output tokens and lowest cost. The next steps toward
autonomous PIC development include standardized knowledge representations,
expanded datasets, extended verification, and robotic automation.

</details>


### [63] [Cross-Layer Design of Vector-Symbolic Computing: Bridging Cognition and Brain-Inspired Hardware Acceleration](https://arxiv.org/abs/2508.14245)
*Shuting Du,Mohamed Ibrahim,Zishen Wan,Luqi Zheng,Boheng Zhao,Zhenkun Fan,Che-Kai Liu,Tushar Krishna,Arijit Raychowdhury,Haitong Li*

Main category: cs.AR

TL;DR: 该论文探讨了向量符号架构（VSA）在硬件与算法协同设计中的应用，提出了跨层设计方法，并展示了一个高效的内存计算系统。


<details>
  <summary>Details</summary>
Motivation: 尽管VSA在认知应用中被广泛采用，但硬件与算法的协同设计仍缺乏全面讨论。论文旨在填补这一空白，为研究者提供协同设计的见解。

Method: 1. 介绍VSA的核心数学操作和学习范式；2. 分析模拟、混合信号和数字电路设计；3. 提出跨层设计方法；4. 提出首个内存计算层次认知硬件系统。

Result: 通过详细分析和比较硬件实现，提取了设计指南，并展示了协同设计的高效性、灵活性和可扩展性。

Conclusion: 论文为VSA的硬件/软件协同设计提供了框架，并指出了未来研究的开放性问题。

Abstract: Vector Symbolic Architectures (VSAs) have been widely deployed in various
cognitive applications due to their simple and efficient operations. The
widespread adoption of VSAs has, in turn, spurred the development of numerous
hardware solutions aimed at optimizing their performance. Despite these
advancements, a comprehensive and unified discourse on the convergence of
hardware and algorithms in the context of VSAs remains somewhat limited. The
paper aims to bridge the gap between theoretical software-level explorations
and the development of efficient hardware architectures and emerging technology
fabrics for VSAs, providing insights from the co-design aspect for researchers
from either side. First, we introduce the principles of vector-symbolic
computing, including its core mathematical operations and learning paradigms.
Second, we provide an in-depth discussion on hardware technologies for VSAs,
analyzing analog, mixed-signal, and digital circuit design styles. We compare
hardware implementations of VSAs by carrying out detailed analysis of their
performance characteristics and tradeoffs, allowing us to extract design
guidelines for the development of arbitrary VSA formulations. Third, we discuss
a methodology for cross-layer design of VSAs that identifies synergies across
layers and explores key ingredients for hardware/software co-design of VSAs.
Finally, as a concrete demonstration of this methodology, we propose the first
in-memory computing hierarchical cognition hardware system, showcasing the
efficiency, flexibility, and scalability of this co-design approach. The paper
concludes with a discussion of open research challenges for future
explorations.

</details>


### [64] [Power Stabilization for AI Training Datacenters](https://arxiv.org/abs/2508.14318)
*Esha Choukse,Brijesh Warrier,Scot Heath,Luz Belmont,April Zhao,Hassan Ali Khan,Brian Harry,Matthew Kappel,Russell J. Hewett,Kushal Datta,Yu Pei,Caroline Lichtenberger,John Siegler,David Lukofsky,Zaid Kahn,Gurpreet Sahota,Andy Sullivan,Charles Frederick,Hien Thai,Rebecca Naughton,Daniel Jurnove,Justin Harp,Reid Carper,Nithish Mahalingam,Srini Varkala,Alok Gautam Kumbhare,Satyajit Desai,Venkatesh Ramamurthy,Praneeth Gottumukkala,Girish Bhatia,Kelsey Wildstone,Laurentiu Olariu,Mohammed Ayna,Mike Kendrick,Ricardo Bianchini,Aaron Hurst,Reza Zamani,Xin Li,Gene Oden,Rory Carmichael,Tom Li,Apoorv Gupta,Nilesh Dattani,Lawrence Marwong,Rob Nertney,Jeff Liott,Miro Enev,Divya Ramakrishnan,Ian Buck,Jonah Alben*

Main category: cs.AR

TL;DR: 论文分析了大规模AI训练中电力波动的挑战，并提出跨软件、硬件和数据中心的解决方案。


<details>
  <summary>Details</summary>
Motivation: 大规模AI训练中电力消耗的高波动性可能对电网基础设施造成物理损害，需要稳定电力供应。

Method: 研究提出跨软件、GPU硬件和数据中心基础设施的多层次解决方案，并通过真实硬件和云端模拟器测试。

Result: 结合真实测试和模拟，验证了解决方案在现实条件下的有效性。

Conclusion: 多管齐下的方法能有效解决大规模AI训练的电力波动问题，确保安全扩展。

Abstract: Large Artificial Intelligence (AI) training workloads spanning several tens
of thousands of GPUs present unique power management challenges. These arise
due to the high variability in power consumption during the training. Given the
synchronous nature of these jobs, during every iteration there is a
computation-heavy phase, where each GPU works on the local data, and a
communication-heavy phase where all the GPUs synchronize on the data. Because
compute-heavy phases require much more power than communication phases, large
power swings occur. The amplitude of these power swings is ever increasing with
the increase in the size of training jobs. An even bigger challenge arises from
the frequency spectrum of these power swings which, if harmonized with critical
frequencies of utilities, can cause physical damage to the power grid
infrastructure. Therefore, to continue scaling AI training workloads safely, we
need to stabilize the power of such workloads. This paper introduces the
challenge with production data and explores innovative solutions across the
stack: software, GPU hardware, and datacenter infrastructure. We present the
pros and cons of each of these approaches and finally present a multi-pronged
approach to solving the challenge. The proposed solutions are rigorously tested
using a combination of real hardware and Microsoft's in-house cloud power
simulator, providing critical insights into the efficacy of these interventions
under real-world conditions.

</details>


### [65] [Computing-In-Memory Dataflow for Minimal Buffer Traffic](https://arxiv.org/abs/2508.14375)
*Choongseok Song,Doo Seok Jeong*

Main category: cs.AR

TL;DR: 提出了一种新型CIM数据流，显著减少了深度卷积中的缓冲区流量，提高了内存利用率和数据重用，从而降低了能量消耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决CIM宏在加速深度卷积时面临的内存利用率不足和缓冲区流量大的问题，尤其是后者对延迟和能耗的显著影响。

Method: 设计了一种新颖的CIM数据流，通过最大化数据重用和提高内存利用率来优化深度卷积。

Result: 在MobileNet和EfficientNet模型上，缓冲流量减少了77.4-87.0%，数据流量能量和延迟分别降低了10.1-17.9%和15.6-27.8%。

Conclusion: 新型CIM数据流有效解决了深度卷积中的缓冲区流量问题，显著提升了性能和能效。

Abstract: Computing-In-Memory (CIM) offers a potential solution to the memory wall
issue and can achieve high energy efficiency by minimizing data movement,
making it a promising architecture for edge AI devices. Lightweight models like
MobileNet and EfficientNet, which utilize depthwise convolution for feature
extraction, have been developed for these devices. However, CIM macros often
face challenges in accelerating depthwise convolution, including
underutilization of CIM memory and heavy buffer traffic. The latter, in
particular, has been overlooked despite its significant impact on latency and
energy consumption. To address this, we introduce a novel CIM dataflow that
significantly reduces buffer traffic by maximizing data reuse and improving
memory utilization during depthwise convolution. The proposed dataflow is
grounded in solid theoretical principles, fully demonstrated in this paper.
When applied to MobileNet and EfficientNet models, our dataflow reduces buffer
traffic by 77.4-87.0%, leading to a total reduction in data traffic energy and
latency by 10.1-17.9% and 15.6-27.8%, respectively, compared to the baseline
(conventional weight-stationary dataflow).

</details>


### [66] [Wit-HW: Bug Localization in Hardware Design Code via Witness Test Case Generation](https://arxiv.org/abs/2508.14414)
*Ruiyang Ma,Daikang Kuang,Ziqian Liu,Jiaxi Zhang,Ping Fan,Guojie Luo*

Main category: cs.AR

TL;DR: 本文提出了一种自动化硬件 bug 定位框架 Wit-HW，通过生成有效的见证测试用例增强 bug 定位能力，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有硬件 bug 定位技术仅利用单一测试用例，无法有效处理大规模设计和深层 bug，因此需要一种新方法。

Method: 将 bug 定位问题转化为测试生成问题，通过频谱分析差异和突变策略生成有效见证测试用例。

Result: 在 41 个硬件 bug 上，Wit-HW 在 Top-1、5、10 中分别定位了 49%、73%、88% 的 bug，并在开源自测案例中表现优异。

Conclusion: Wit-HW 通过多测试用例分析和突变策略显著提升了硬件 bug 定位的准确性和效率。

Abstract: Debugging hardware designs requires significant manual effort during hardware
development. After engineers identify a bug-triggering test case in
simulation-based hardware verification, they usually spend considerable time
analyzing the execution trace to localize the bug. Although numerous automated
hardware debugging techniques exist, they are not applicable to large designs
and deep bugs. A primary reason for their limitations is that these techniques
only utilize the information of a single bug-triggering test case for bug
localization, which prevents them from effectively analyzing intricate hardware
systems and figure out the root cause of bugs. To solve this problem, in this
paper, we transform the hardware bug localization problem into a test
generation problem, aiming to find a set of effective witness test cases beyond
the initial bug-triggering test case to enhance hardware bug localization.
Witness test cases refer to the cases that do not trigger the bug in the faulty
design. By analyzing the execution differences between passing and failing test
cases with spectrum-based method, we can eliminate innocent design statements
and localize the buggy ones. To further refine the suspicious area, we define
the criteria for effective witness test cases and use a mutation-based strategy
to generate such test cases. Based on this approach, we propose an automated
hardware bug localization framework named Wit-HW. We evaluate Wit-HW on 41 bugs
from various hardware designs. The experimental results show that Wit-HW
effectively localize 49%, 73%, 88% bugs within Top-1, Top-5, Top-10 ranks,
significantly outperforming state-of-the-art bug localization techniques.
Additionally, we evaluate Wit-HW on 13 real-world bugs collected from
open-source hardware projects, showcasing the robust performance of our method.

</details>


### [67] [An Open-Source HW-SW Co-Development Framework Enabling Efficient Multi-Accelerator Systems](https://arxiv.org/abs/2508.14582)
*Ryan Albert Antonio,Joren Dumoulin,Xiaoling Yi,Josse Van Delm,Yunhao Deng,Guilherme Paim,Marian Verhelst*

Main category: cs.AR

TL;DR: SNAX是一个开源硬件-软件框架，采用新颖的混合耦合方案，提升多加速器平台的数据访问效率和兼容性，显著提高神经网络性能。


<details>
  <summary>Details</summary>
Motivation: 当前异构加速器集群的集成策略存在数据移动效率低和兼容性问题，需要统一的解决方案以平衡性能和易用性。

Method: SNAX结合松散耦合的异步控制和紧密耦合的数据访问，提供可重用硬件模块和基于MLIR的编译器，支持快速开发和部署定制化多加速器集群。

Result: 实验表明，SNAX在低功耗异构SoC中实现了>10倍的神经网络性能提升，且加速器利用率保持在>90%。

Conclusion: SNAX框架通过高效的硬件-软件协同设计，显著提升了异构加速器集群的整合性和性能表现。

Abstract: Heterogeneous accelerator-centric compute clusters are emerging as efficient
solutions for diverse AI workloads. However, current integration strategies
often compromise data movement efficiency and encounter compatibility issues in
hardware and software. This prevents a unified approach that balances
performance and ease of use. To this end, we present SNAX, an open-source
integrated HW-SW framework enabling efficient multi-accelerator platforms
through a novel hybrid-coupling scheme, consisting of loosely coupled
asynchronous control and tightly coupled data access. SNAX brings reusable
hardware modules designed to enhance compute accelerator utilization, and its
customizable MLIR-based compiler to automate key system management tasks,
jointly enabling rapid development and deployment of customized
multi-accelerator compute clusters. Through extensive experimentation, we
demonstrate SNAX's efficiency and flexibility in a low-power heterogeneous SoC.
Accelerators can easily be integrated and programmed to achieve > 10x
improvement in neural network performance compared to other accelerator systems
while maintaining accelerator utilization of > 90% in full system operation.

</details>


### [68] [ListenToJESD204B: A Lightweight Open-Source JESD204B IP Core for FPGA-Based Ultrasound Acquisition systems](https://arxiv.org/abs/2508.14798)
*Soumyo Bhattacharjee,Federico Villani,Christian Vogt,Andrea Cossettini,Luca Benini*

Main category: cs.AR

TL;DR: 论文提出了一个开源的ListenToJESD204B接收器IP核心，用于解决超声波系统中多通道同步数据传输的需求，相比商业IP核心显著减少了资源占用。


<details>
  <summary>Details</summary>
Motivation: 超声波系统需要数百个紧密同步的通道，传统的数据传输方式在带宽、引脚数和延迟方面无法满足需求。

Method: 开发了一个开源的接收器IP核心，基于SystemVerilog实现，支持12.8 Gb/s的数据速率，并通过模块化设计实现了高效资源利用。

Result: 该IP核心资源占用仅为商业产品的21%，并且在硬件测试中表现稳定，无错误传输数据30分钟。

Conclusion: ListenToJESD204B是一个高效且开源的解决方案，适用于高通道数的超声波系统。

Abstract: The demand for hundreds of tightly synchronized channels operating at tens of
MSPS in ultrasound systems exceeds conventional low-voltage differential
signaling links' bandwidth, pin count, and latency. Although the JESD204B
serial interface mitigates these limitations, commercial FPGA IP cores are
proprietary, costly, and resource-intensive. We present ListenToJESD204B, an
open-source receiver IP core released under a permissive Solderpad 0.51 license
for AMD Xilinx Zynq UltraScale+ devices. Written in synthesizable
SystemVerilog, the core supports four GTH/GTY lanes at 12.8 Gb/s and provides
cycle-accurate AXI-Stream data alongside deterministic Subclass~1 latency. It
occupies only 107 configurable logic blocks (approximately 437 LUTs),
representing a 79\% reduction compared to comparable commercially available IP.
A modular data path featuring per-lane elastic buffers, SYSREF-locked LMFC
generation, and optional LFSR descrambling facilitates scaling to high lane
counts. We verified protocol compliance through simulation against the Xilinx
JESD204C IP in JESD204B mode and on hardware using TI AFE58JD48 ADCs. Block
stability was verified by streaming 80 MSPS, 16-bit samples over two 12.8 Gb/s
links for 30 minutes with no errors.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [69] [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
*Luca Annese,Sabrina Patania,Silvia Serino,Tom Foulsham,Silvia Rossi,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.AI

TL;DR: 研究探讨了通过结构化示例提升LLM智能体在ReAct框架中的表现，但发现仅靠结构化示例不足以实现稳健的视角采择能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）和推理框架取得进展，但在主动感知、协作推理和视角采择任务中仍存在挑战。

Method: 提出结构化解决方案处理流程，生成三类示例（G型、E型、L型），并将其转化为“思考-行动”示例。

Result: L型示例略微减少了澄清请求和行动步骤，但未带来一致改进；智能体在基础任务中表现良好，但在复杂场景中仍有困难。

Conclusion: 需结合显式信念跟踪、成本建模和更丰富环境，以提升LLM智能体的协作能力。

Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have
opened new possibilities for improving the perspective -taking capabilities of
autonomous agents. However, tasks that involve active perception, collaborative
reasoning, and perspective taking (understanding what another agent can see or
knows) pose persistent challenges for current LLM-based systems. This study
investigates the potential of structured examples derived from transformed
solution graphs generated by the Fast Downward planner to improve the
performance of LLM-based agents within a ReAct framework. We propose a
structured solution-processing pipeline that generates three distinct
categories of examples: optimal goal paths (G-type), informative node paths
(E-type), and step-by-step optimal decision sequences contrasting alternative
actions (L-type). These solutions are further converted into ``thought-action''
examples by prompting an LLM to explicitly articulate the reasoning behind each
decision. While L-type examples slightly reduce clarification requests and
overall action steps, they do not yield consistent improvements. Agents are
successful in tasks requiring basic attentional filtering but struggle in
scenarios that required mentalising about occluded spaces or weighing the costs
of epistemic actions. These findings suggest that structured examples alone are
insufficient for robust perspective-taking, underscoring the need for explicit
belief tracking, cost modelling, and richer environments to enable socially
grounded collaboration in LLM-based agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [70] [Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions](https://arxiv.org/abs/2508.14091)
*Matthew Morris,David J. Tena Cucala,Bernardo Cuenca Grau*

Main category: cs.LG

TL;DR: 该论文探讨了如何在知识图谱中使用单调性图神经网络（GNN）和评分函数提取可解释的Datalog规则，以提升GNN的可解释性和表达能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决GNN在知识图谱中缺乏可解释性的问题，并扩展GNN在链接预测任务中的应用范围，论文提出了一种更通用且高表达性的方法。

Method: 论文通过将GNN和评分函数调整为单调性形式，利用单调性提取可靠的规则，并定义了转换为等效Datalog程序的步骤。

Result: 实验表明，单调性GNN和评分函数在实际链接预测任务中表现良好，并能够生成大量可靠的规则。

Conclusion: 该研究表明，单调性GNN和评分函数不仅能有效用于链接预测，还能提供可解释的规则，进一步扩展了GNN的实用性和理论基础。

Abstract: Graph neural networks (GNNs) are often used for the task of link prediction:
predicting missing binary facts in knowledge graphs (KGs). To address the lack
of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs
with provable correspondence guarantees. The extracted rules can be used to
explain the GNN's predictions; furthermore, they can help characterise the
expressive power of various GNN models. However, these works address only a
form of link prediction based on a restricted, low-expressivity graph
encoding/decoding method. In this paper, we consider a more general and popular
approach for link prediction where a scoring function is used to decode the GNN
output into fact predictions. We show how GNNs and scoring functions can be
adapted to be monotonic, use the monotonicity to extract sound rules for
explaining predictions, and leverage existing results about the kind of rules
that scoring functions can capture. We also define procedures for obtaining
equivalent Datalog programs for certain classes of monotonic GNNs with scoring
functions. Our experiments show that, on link prediction benchmarks, monotonic
GNNs and scoring functions perform well in practice and yield many sound rules.

</details>


### [71] [FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning](https://arxiv.org/abs/2508.14539)
*Tao Shen,Zexi Li,Didi Zhu,Ziyu Zhao,Chao Wu,Fei Wu*

Main category: cs.LG

TL;DR: 联邦学习（FL）中，数据异构性和客户端漂移是主要挑战。本文提出FedEve框架，通过预测-观察方法补偿两种漂移，提升非独立同分布数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习中因部分客户端参与导致的周期漂移问题，及其与客户端漂移的交互，以优化模型性能。

Method: 提出预测-观察框架，实例化为FedEve方法，利用两种漂移的相互补偿降低模型更新方差。

Result: 理论证明FedEve可减少模型更新方差，实验验证其优于其他方法。

Conclusion: FedEve能有效缓解周期漂移和客户端漂移对联邦学习的负面影响，提升非独立同分布数据下的模型表现。

Abstract: Federated learning (FL) is a machine learning paradigm that allows multiple
clients to collaboratively train a shared model without exposing their private
data. Data heterogeneity is a fundamental challenge in FL, which can result in
poor convergence and performance degradation. Client drift has been recognized
as one of the factors contributing to this issue resulting from the multiple
local updates in FedAvg. However, in cross-device FL, a different form of drift
arises due to the partial client participation, but it has not been studied
well. This drift, we referred as period drift, occurs as participating clients
at each communication round may exhibit distinct data distribution that
deviates from that of all clients. It could be more harmful than client drift
since the optimization objective shifts with every round.
  In this paper, we investigate the interaction between period drift and client
drift, finding that period drift can have a particularly detrimental effect on
cross-device FL as the degree of data heterogeneity increases. To tackle these
issues, we propose a predict-observe framework and present an instantiated
method, FedEve, where these two types of drift can compensate each other to
mitigate their overall impact. We provide theoretical evidence that our
approach can reduce the variance of model updates. Extensive experiments
demonstrate that our method outperforms alternatives on non-iid data in
cross-device settings.

</details>


### [72] [Cooperative SGD with Dynamic Mixing Matrices](https://arxiv.org/abs/2508.14565)
*Soumya Sarkar,Shweta Jain*

Main category: cs.LG

TL;DR: 本文提出了一种统一框架，用于动态拓扑结构的分布式SGD算法，比现有工作提供更好或相当的收敛理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有分布式SGD假设固定拓扑和均匀节点贡献，实验表明这种假设不理想，动态拓扑和非均匀聚合策略能显著提升性能。

Method: 基于Local-Update SGD的统一框架，支持动态拓扑和客户端选择。

Result: 框架提供了改进或匹配现有工作的收敛理论保证。

Conclusion: 动态拓扑和非均匀聚合策略对分布式SGD的性能有显著提升，统一框架为相关算法提供了理论支持。

Abstract: One of the most common methods to train machine learning algorithms today is
the stochastic gradient descent (SGD). In a distributed setting, SGD-based
algorithms have been shown to converge theoretically under specific
circumstances. A substantial number of works in the distributed SGD setting
assume a fixed topology for the edge devices. These papers also assume that the
contribution of nodes to the global model is uniform. However, experiments have
shown that such assumptions are suboptimal and a non uniform aggregation
strategy coupled with a dynamically shifting topology and client selection can
significantly improve the performance of such models. This paper details a
unified framework that covers several Local-Update SGD-based distributed
algorithms with dynamic topologies and provides improved or matching
theoretical guarantees on convergence compared to existing work.

</details>


### [73] [Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data](https://arxiv.org/abs/2508.14769)
*Ahmed Mujtaba,Gleb Radchenko,Radu Prodan,Marc Masana*

Main category: cs.LG

TL;DR: 本文提出了一种名为EdgeFD的高效联邦蒸馏方法，通过简化客户端密度比估计并消除服务器端过滤需求，显著提升了知识共享的质量和系统效率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦蒸馏方法存在计算复杂度高（依赖密度比估计）和服务器端延迟（过滤模糊知识）的问题，限制了其在实际资源受限场景中的应用。

Method: 提出EdgeFD方法，采用基于KMeans的高效密度比估计器，过滤客户端数据的分布内和分布外代理数据，无需预训练教师模型。

Result: 实验显示，EdgeFD在多种数据分布（强非IID、弱非IID、IID）下均优于现有方法，计算开销显著降低，适用于边缘设备。

Conclusion: EdgeFD通过高效密度比估计和去服务器过滤，提升了联邦蒸馏的实用性，为资源受限场景提供了可行解决方案。

Abstract: Federated distillation has emerged as a promising collaborative machine
learning approach, offering enhanced privacy protection and reduced
communication compared to traditional federated learning by exchanging model
outputs (soft logits) rather than full model parameters. However, existing
methods employ complex selective knowledge-sharing strategies that require
clients to identify in-distribution proxy data through computationally
expensive statistical density ratio estimators. Additionally, server-side
filtering of ambiguous knowledge introduces latency to the process. To address
these challenges, we propose a robust, resource-efficient EdgeFD method that
reduces the complexity of the client-side density ratio estimation and removes
the need for server-side filtering. EdgeFD introduces an efficient KMeans-based
density ratio estimator for effectively filtering both in-distribution and
out-of-distribution proxy data on clients, significantly improving the quality
of knowledge sharing. We evaluate EdgeFD across diverse practical scenarios,
including strong non-IID, weak non-IID, and IID data distributions on clients,
without requiring a pre-trained teacher model on the server for knowledge
distillation. Experimental results demonstrate that EdgeFD outperforms
state-of-the-art methods, consistently achieving accuracy levels close to IID
scenarios even under heterogeneous and challenging conditions. The
significantly reduced computational overhead of the KMeans-based estimator is
suitable for deployment on resource-constrained edge devices, thereby enhancing
the scalability and real-world applicability of federated distillation. The
code is available online for reproducibility.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [74] [Fine-grained Image Quality Assessment for Perceptual Image Restoration](https://arxiv.org/abs/2508.14475)
*Xiangfei Sheng,Xiaofeng Pan,Zhichao Yang,Pengfei Chen,Leida Li*

Main category: eess.IV

TL;DR: 作者提出了一种针对图像恢复任务的细粒度图像质量评估（IQA）数据集FGRestore，并在此基础上设计了新的IQA模型FGResQ，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有IQA指标在图像恢复任务中表现不佳，尤其是难以区分细粒度的质量差异。因此，需要更准确的IQA方法来支持性能比较和算法优化。

Method: 构建了包含18,408张恢复图像和30,886对细粒度偏好标注的数据集FGRestore，并提出了结合粗粒度评分和细粒度排序的模型FGResQ。

Result: 实验表明，FGResQ在图像恢复任务中显著优于现有的IQA指标。

Conclusion: FGRestore数据集和FGResQ模型填补了图像恢复领域细粒度IQA的空白，为未来研究提供了重要基础。

Abstract: Recent years have witnessed remarkable achievements in perceptual image
restoration (IR), creating an urgent demand for accurate image quality
assessment (IQA), which is essential for both performance comparison and
algorithm optimization. Unfortunately, the existing IQA metrics exhibit
inherent weakness for IR task, particularly when distinguishing fine-grained
quality differences among restored images. To address this dilemma, we
contribute the first-of-its-kind fine-grained image quality assessment dataset
for image restoration, termed FGRestore, comprising 18,408 restored images
across six common IR tasks. Beyond conventional scalar quality scores,
FGRestore was also annotated with 30,886 fine-grained pairwise preferences.
Based on FGRestore, a comprehensive benchmark was conducted on the existing IQA
metrics, which reveal significant inconsistencies between score-based IQA
evaluations and the fine-grained restoration quality. Motivated by these
findings, we further propose FGResQ, a new IQA model specifically designed for
image restoration, which features both coarse-grained score regression and
fine-grained quality ranking. Extensive experiments and comparisons demonstrate
that FGResQ significantly outperforms state-of-the-art IQA metrics. Codes and
model weights have been released in https://pxf0429.github.io/FGResQ/

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [75] [Q-BEAST: A Practical Course on Experimental Evaluation and Characterization of Quantum Computing Systems](https://arxiv.org/abs/2508.14084)
*Minh Chung,Yaknan Gambo,Burak Mete,Xiao-Ting Michelle To,Florian Krötz,Korbinian Staudacher,Martin Letras,Xiaolong Deng,Mounika Vavilala,Amir Raoofy,Jorge Echavarria,Luigi Iapichino,Laura Schulz,Josef Weidendorfer,Martin Schulz*

Main category: physics.ed-ph

TL;DR: 论文提出了Q-BEAST课程，旨在通过理论和实践结合的方式，帮助学生和新人学习量子计算系统的实验分析和性能评估，填补教育空白。


<details>
  <summary>Details</summary>
Motivation: 当前量子计算技术学习曲线陡峭，实际评估和表征量子系统复杂且具有挑战性，尤其是对学生和计算机科学领域的新人。Q-BEAST课程旨在填补这一教育空白。

Method: Q-BEAST课程结合量子计算的基础概念与实际方法，提供真实的量子系统上进行基准测试和性能评估的案例，通过理论教学和动手实验相结合的方式。

Result: 学生通过课程获得了评估真实量子技术优势和局限性的经验，Q-BEAST还推动了高性能计算（HPC）与量子计算在研究和教育中的深度融合。

Conclusion: Q-BEAST是一种有效的教育工具，有助于培养未来量子计算用户和开发者，并促进HPC与QC的更紧密结合。

Abstract: Quantum computing (QC) promises to be a transformative technology with impact
on various application domains, such as optimization, cryptography, and
material science. However, the technology has a sharp learning curve, and
practical evaluation and characterization of quantum systems remains complex
and challenging, particularly for students and newcomers from computer science
to the field of quantum computing. To address this educational gap, we
introduce Q-BEAST, a practical course designed to provide structured training
in the experimental analysis of quantum computing systems. Q-BEAST offers a
curriculum that combines foundational concepts in quantum computing with
practical methodologies and use cases for benchmarking and performance
evaluation on actual quantum systems. Through theoretical instruction and
hands-on experimentation, students gain experience in assessing the advantages
and limitations of real quantum technologies. With that, Q-BEAST supports the
education of a future generation of quantum computing users and developers.
Furthermore, it also explicitly promotes a deeper integration of High
Performance Computing (HPC) and QC in research and education.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [76] [A High Performance GPU CountSketch Implementation and Its Application to Multisketching and Least Squares Problems](https://arxiv.org/abs/2508.14209)
*Andrew J. Higgins,Erik G. Boman,Ichitaro Yamazaki*

Main category: math.NA

TL;DR: 本文提出了一种高效的GPU实现CountSketch方法，并结合多速写技术展示了其在快速降维中的性能优势，同时探讨了其在最小二乘求解器中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究高性能CountSketch实现的不足，并通过GPU优化提升其效率，结合多速写技术进一步验证其在实际应用中的潜力。

Method: 开发了一种高效的GPU实现CountSketch方法，并展示了其与高斯速写结合的多速写技术在矩阵降维中的性能。

Result: 多速写技术显著提升了最小二乘求解器的速度（快77%），且具有更好的数值稳定性，但引入了一个O(1)的相对残差范数乘性因子。

Conclusion: 优化的CountSketch和多速写技术在提升计算效率的同时保持了良好的数值稳定性，为大规模矩阵处理提供了新的解决方案。

Abstract: Random sketching is a dimensionality reduction technique that approximately
preserves norms and singular values up to some $O(1)$ distortion factor with
high probability. The most popular sketches in literature are the Gaussian
sketch and the subsampled randomized Hadamard transform, while the CountSketch
has lower complexity. Combining two sketches, known as multisketching, offers
an inexpensive means of quickly reducing the dimension of a matrix by combining
a CountSketch and Gaussian sketch.
  However, there has been little investigation into high performance
CountSketch implementations. In this work, we develop an efficient GPU
implementation of the CountSketch, and demonstrate the performance of
multisketching using this technique. We also demonstrate the potential for
using this implementation within a multisketched least squares solver that is
up to $77\%$ faster than the normal equations with significantly better
numerical stability, at the cost of an $O(1)$ multiplicative factor introduced
into the relative residual norm.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [77] [BioSonix: Can Physics-Based Sonification Perceptualize Tissue Deformations From Tool Interactions?](https://arxiv.org/abs/2508.14688)
*Veronica Ruozzi,Sasan Matinfar,Laura Schütz,Benedikt Wiestler,Alberto Redaelli,Emiliano Votta,Nassir Navab*

Main category: cs.SD

TL;DR: 该论文提出了一种在混合现实环境中通过听觉反馈增强手术工具导航的方法，利用物理建模和声音模型表达软组织动态特性，得到了专家验证。


<details>
  <summary>Details</summary>
Motivation: 传统视觉化方法在捕捉手术工具与软组织交互的复杂性时存在局限性，如遮挡和深度感知不足，因此需要多感官反馈提升理解。

Method: 使用生物力学模拟建模工具与组织的交互位移，通过优化方法捕捉多样交互场景，并结合物理信息设计声音模型BioSonix。

Result: 实验验证了声音-位移映射的准确性，用户研究表明临床专家能高精度完成任务，且声音反馈显著提升了组织区分和目标任务的性能。

Conclusion: 工具-组织动态与其对应的听觉特征密切相关，证明了多感官反馈在增强复杂交互直觉理解中的潜力。

Abstract: Perceptualizing tool interactions with deformable structures in surgical
procedures remains challenging, as unimodal visualization techniques often fail
to capture the complexity of these interactions due to constraints such as
occlusion and limited depth perception. This paper presents a novel approach to
augment tool navigation in mixed reality environments by providing auditory
representations of tool-tissue dynamics, particularly for interactions with
soft tissue. BioSonix, a physics-informed design framework, utilizes tissue
displacements in 3D space to compute excitation forces for a sound model
encoding tissue properties such as stiffness and density. Biomechanical
simulations were employed to model particle displacements resulting from
tool-tissue interactions, establishing a robust foundation for the method. An
optimization approach was used to define configurations for capturing diverse
interaction scenarios with varying tool trajectories. Experiments were
conducted to validate the accuracy of the sound-displacement mappings.
Additionally, two user studies were performed: the first involved two clinical
professionals (a neuroradiologist and a cardiologist), who confirmed the
method's impact and achieved high task accuracy; the second included 22
biomedical experts, who demonstrated high discrimination accuracy in tissue
differentiation and targeting tasks. The results revealed a strong correlation
between tool-tissue dynamics and their corresponding auditory profiles,
highlighting the potential of these sound representations to enhance the
intuitive understanding of complex interactions.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [78] [Activity Coefficient-based Channel Selection for Electroencephalogram: A Task-Independent Approach](https://arxiv.org/abs/2508.14060)
*Kartik Pandey,Arun Balasubramanian,Debasis Samanta*

Main category: q-bio.NC

TL;DR: 论文提出了一种任务无关的通道选择方法ACCS，利用通道活动系数（CAC）量化通道效用，适用于多样化EEG应用。


<details>
  <summary>Details</summary>
Motivation: 高密度电极阵列在脑机接口（BCI）中带来交叉通道干扰和计算开销等问题，现有通道选择方法需要针对不同任务重新优化。

Method: 提出ACCS方法，基于CAC指标量化通道活动水平，选择排名前16的通道。

Result: ACCS在多类分类任务中最高提升34.97%的准确率，且通道选择独立于下游任务或模型。

Conclusion: ACCS是一种可重用且高效的通道选择方法，适用于多样化EEG应用。

Abstract: Electroencephalogram (EEG) signals have gained widespread adoption in
brain-computer interface (BCI) applications due to their non-invasive,
low-cost, and relatively simple acquisition process. The demand for higher
spatial resolution, particularly in clinical settings, has led to the
development of high-density electrode arrays. However, increasing the number of
channels introduces challenges such as cross-channel interference and
computational overhead. To address these issues, modern BCI systems often
employ channel selection algorithms. Existing methods, however, are typically
task-specific and require re-optimization for each new application. This work
proposes a task-agnostic channel selection method, Activity Coefficient-based
Channel Selection (ACCS), which uses a novel metric called the Channel Activity
Coefficient (CAC) to quantify channel utility based on activity levels. By
selecting the top 16 channels ranked by CAC, ACCS achieves up to 34.97%
improvement in multi-class classification accuracy. Unlike traditional
approaches, ACCS identifies a reusable set of informative channels independent
of the downstream task or model, making it highly adaptable for diverse
EEG-based applications.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [79] [Lagrangian Simulation Volume-Based Contour Tree Simplification](https://arxiv.org/abs/2508.14339)
*Domantas Dilys,Hamish Carr,Steven Boeing*

Main category: cs.CG

TL;DR: 该论文提出了一种基于体积的轮廓树简化方法，适用于粒子模拟（Lagrangian）数据，避免了传统方法中因网格重采样导致的数据膨胀问题，并通过图形适配器在Viskores中实现，显著提高了计算速度和质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决粒子模拟数据中轮廓树简化因体积计算困难而导致的网格重采样问题，作者提出了一种直接基于粒子的体积计算方法。

Method: 使用Delaunay四面体化粒子中心点作为输入，扩展体积基轮廓树简化方法。通过适配2D面积样条方法，实现了并行高效的积分属性计算。

Result: 实验结果表明，直接在粒子上计算轮廓树比在重采样网格上计算快多个数量级，同时避免了插值伪影，提供更好的分割质量。

Conclusion: 该方法有效解决了粒子模拟数据中轮廓树简化的效率和质量问题，为科学和工程模拟数据的分析与可视化提供了新思路。

Abstract: Many scientific and engineering problems are modelled by simulating scalar
fields defined either on space-filling meshes (Eulerian) or as particles
(Lagrangian). For analysis and visualization, topological primitives such as
contour trees can be used, but these often need simplification to filter out
small-scale features. For parcel-based convective cloud simulations,
simplification of the contour tree requires a volumetric measure rather than
persistence. Unlike for cubic meshes, volume cannot be approximated by counting
regular vertices. Typically, this is addressed by resampling irregular data
onto a uniform grid. Unfortunately, the spatial proximity of parcels requires a
high sampling frequency, resulting in a massive increase in data size for
processing. We therefore extend volume-based contour tree simplification to
parcel-in-cell simulations with a graph adaptor in Viskores (VTK-m), using
Delaunay tetrahedralization of the parcel centroids as input. Instead of
relying on a volume approximation by counting regular vertices -- as was done
for cubic meshes -- we adapt the 2D area splines reported by Bajaj et al.
10.1145/259081.259279, and Zhou et al. 10.1109/TVCG.2018.2796555. We implement
this in Viskores (formerly called VTK-m) as prefix-sum style hypersweeps for
parallel efficiency and show how it can be generalized to compute any
integrable property. Finally, our results reveal that contour trees computed
directly on the parcels are orders of magnitude faster than computing them on a
resampled grid, while also arguably offering better quality segmentation,
avoiding interpolation artifacts.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [80] [Trace-Based Reconstruction of Quantum Circuit Dataflow in Surface Codes](https://arxiv.org/abs/2508.14533)
*Theodoros Trochatos,Christopher Kang,Andrew Wang,Frederic T. Chong,Jakub Szefer*

Main category: quant-ph

TL;DR: 论文提出了TraceQ框架，通过观察量子计算中的访问轨迹重建量子电路数据流，并展示其在高精度恢复量子程序中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决量子计算中如何通过访问轨迹分析量子程序的执行和硬件运行情况。

Method: 提出TraceQ框架，基于访问轨迹重建量子电路，并使用启发式方法和子图匹配算法处理轨迹中的模糊性。

Result: 验证了TraceQ能高精度恢复量子子程序甚至整个量子程序，仅需单次轨迹且支持离线处理。

Conclusion: 访问轨迹可作为有效工具分析量子程序执行，TraceQ框架展现出在实际应用中的潜力。

Abstract: Practical applications of quantum computing depend on fault-tolerant devices
that employ error correction. A promising quantum error-correcting code for
large-scale quantum computing is the surface code. For this code,
Fault-Tolerant Quantum Computing (FTQC) can be performed via lattice surgery,
i.e. merging and splitting of encoded qubit patches on a 2D grid. Lattice
surgery operations result in space-time patterns of activity that are defined
in this work as access traces. This work demonstrates that the access traces
reveal when, where, and how logical qubits interact. Leveraging this
formulation, this work further introduces TraceQ, a trace-based reconstruction
framework that is able to reconstruct the quantum circuit dataflow just by
observing the patch activity at each trace entry. The framework is supported by
heuristics for handling inherent ambiguity in the traces, and demonstrates its
effectiveness on a range of synthetic fault-tolerant quantum benchmarks. The
access traces can have applications in a wide range of scenarios, enabling
analysis and profiling of execution of quantum programs and the hardware they
run on. As one example use of TraceQ, this work investigates whether such
traces can act as a side channel through which an observer can recover the
circuit's structure and identify known subroutines in a larger program or even
whole programs. The findings show that indeed the minimal access traces can be
used to recover subroutines or even whole quantum programs with very high
accuracy. Only a single trace per program execution is needed and the
processing can be done fully offline. Along with the custom heuristics,
advanced subgraph matching algorithms used in this work enable a high rate of
locating the subroutines while executing in minimal time.

</details>


### [81] [Inserting Planar-Measured Qubits into MBQC Patterns while Preserving Flow](https://arxiv.org/abs/2508.14671)
*Miriam Backens,Thomas Perez*

Main category: quant-ph

TL;DR: 本文扩展了MBQC中的流条件定义，支持YZ和XZ平面测量，并推导了插入这些测量时保持流不变的条件，推动了MBQC模式的优化和理解。


<details>
  <summary>Details</summary>
Motivation: 为了在MBQC中支持通用的平面测量（如YZ和XZ），并确保计算确定性，需要扩展流条件的定义，以便模式重写（如插入测量）时不破坏流。

Method: 通过扩展因果流的定义，并推导YZ和XZ插入时保持流不变的条件，同时展示了‘顶点分裂’规则可从YZ插入和旋转推导。

Result: 成功定义了支持YZ和XZ测量的流条件，并验证了插入这些测量时流的保持性，提升了MBQC模式重写的灵活性。

Conclusion: 该研究扩展了MBQC中流条件的适用范围，为模式和ZX演算的优化、混淆及路由提供了更高效的理论基础。

Abstract: In the one-way model of measurement-based quantum computation (MBQC),
computation proceeds via single-qubit measurements on a resource state. Flow
conditions ensure that the overall computation is deterministic in a suitable
sense, and are required for efficient translation into quantum circuits.
Procedures that rewrite MBQC patterns -- e.g. for optimisation, or adapting to
hardware constraints -- thus need to preserve the existence of flow. Most
previous work has focused on rewrites that reduce the number of qubits in the
computation, or that introduce new Pauli-measured qubits. Here, we consider the
insertion of planar-measured qubits into MBQC patterns, i.e. arbitrary
measurements in a plane of the Bloch sphere spanned by a pair of Pauli
operators; such measurements are necessary for universal MBQC. We extend the
definition of causal flow, previously restricted to XY -measurements only, to
also permit YZ-measurements and derive the conditions under which a
YZ-insertion preserves causal flow. Then we derive conditions for YZ-insertion
into patterns with gflow or Pauli flow, in which case the argument
straightforwardly extends to XZ-insertions as well. We also show that the
'vertex splitting' or 'neighbour unfusion' rule previously used in the
literature can be derived from YZ-insertion and pivoting. This work contributes
to understanding the broad properties of flow-preserving rewriting in MBQC and
in the ZX-calculus more broadly, and it will enable more efficient
optimisation, obfuscation, or routing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [82] [Confidence Estimation for Text-to-SQL in Large Language Models](https://arxiv.org/abs/2508.14056)
*Sepideh Entezari Maleki,Mohammadreza Pourreza,Davood Rafiei*

Main category: cs.CL

TL;DR: 研究了在无法访问黄金答案的情况下估计文本到SQL生成模型可靠性的方法，重点讨论了黑盒和白盒策略，并验证了一致性方法和SQL语法感知方法的优势。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型(LLMs)的权重和梯度访问受限，需要开发有效的置信度估计方法以评估模型生成的SQL查询的可靠性。

Method: 探索了黑盒和白盒置信度估计策略，包括一致性方法和SQL语法感知方法，并结合执行反馈优化结果。

Result: 一致性方法在黑盒模型中表现优越，SQL语法感知方法在白盒模型中更有效，执行反馈进一步提升了两种方法的有效性。

Conclusion: 结合一致性方法和SQL语法感知方法，辅以执行反馈，可以显著提升文本到SQL任务的可靠性评估。

Abstract: Confidence estimation for text-to-SQL aims to assess the reliability of
model-generated SQL queries without having access to gold answers. We study
this problem in the context of large language models (LLMs), where access to
model weights and gradients is often constrained. We explore both black-box and
white-box confidence estimation strategies, evaluating their effectiveness on
cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior
performance of consistency-based methods among black-box models and the
advantage of SQL-syntax-aware approaches for interpreting LLM logits in
white-box settings. Furthermore, we show that execution-based grounding of
queries provides a valuable supplementary signal, improving the effectiveness
of both approaches.

</details>


### [83] [ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](https://arxiv.org/abs/2508.14706)
*Junying Chen,Zhenyang Cai,Zhiheng Liu,Yunjin Yang,Rongsheng Wang,Qingying Xiao,Xiangyi Feng,Zhan Su,Jing Guo,Xiang Wan,Guangjun Yu,Haizhou Li,Benyou Wang*

Main category: cs.CL

TL;DR: 论文提出了ShizhenGPT，首个针对传统中医（TCM）的多模态大语言模型（LLM），解决了中医领域数据稀缺和多模态诊断的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在传统中医领域的潜力，解决高质量数据稀缺和多模态诊断的难题。

Method: 构建大规模TCM数据集（文本和多模态数据），预训练和指令调优ShizhenGPT，实现多模态推理。

Result: ShizhenGPT在TCM任务中优于同类规模的LLMs，并在多模态感知方面表现突出。

Conclusion: ShizhenGPT为TCM领域的研究和应用提供了新的可能性，推动了多模态感知和诊断的发展。

Abstract: Despite the success of large language models (LLMs) in various domains, their
potential in Traditional Chinese Medicine (TCM) remains largely underexplored
due to two critical barriers: (1) the scarcity of high-quality TCM data and (2)
the inherently multimodal nature of TCM diagnostics, which involve looking,
listening, smelling, and pulse-taking. These sensory-rich modalities are beyond
the scope of conventional LLMs. To address these challenges, we present
ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data
scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text
and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and
physiological signals. ShizhenGPT is pretrained and instruction-tuned to
achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect
recent national TCM qualification exams and build a visual benchmark for
Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that
ShizhenGPT outperforms comparable-scale LLMs and competes with larger
proprietary models. Moreover, it leads in TCM visual understanding among
existing multimodal LLMs and demonstrates unified perception across modalities
like sound, pulse, smell, and vision, paving the way toward holistic multimodal
perception and diagnosis in TCM. Datasets, models, and code are publicly
available. We hope this work will inspire further exploration in this field.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [84] [Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer](https://arxiv.org/abs/2508.14187)
*Md Ashiqur Rahman,Chiao-An Yang,Michael N. Cheng,Lim Jun Hao,Jeremiah Jiang,Teck-Yian Lim,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 论文提出了一种深度平衡标准化器（DEC），用于处理计算机视觉中的尺度变化问题，提升模型的局部尺度等变性。


<details>
  <summary>Details</summary>
Motivation: 解决同一类别物体因尺寸差异或距离导致的尺度变化问题，这些变化在同一图像中可能不同。

Method: 提出DEC，可轻松整合到现有网络架构中，并可适配预训练模型。

Result: 在ImageNet基准测试中，DEC提升了ViT、DeiT等四种预训练模型的性能和局部尺度一致性。

Conclusion: DEC是一种有效提升模型对局部尺度变化处理能力的方法。

Abstract: Scale variation is a fundamental challenge in computer vision. Objects of the
same class can have different sizes, and their perceived size is further
affected by the distance from the camera. These variations are local to the
objects, i.e., different object sizes may change differently within the same
image. To effectively handle scale variations, we present a deep equilibrium
canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can
be easily incorporated into existing network architectures and can be adapted
to a pre-trained model. Notably, we show that on the competitive ImageNet
benchmark, DEC improves both model performance and local scale consistency
across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our
code is available at https://github.com/ashiq24/local-scale-equivariance.

</details>


### [85] [Federated Action Recognition for Smart Worker Assistance Using FastPose](https://arxiv.org/abs/2508.14113)
*Vinit Hegiste,Vidit Goyal,Tatjana Legler,Martin Ruskowski*

Main category: cs.CV

TL;DR: 该论文提出了一种基于联合学习的姿态动作识别框架，在隐私敏感的工业场景中提升了准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在智能制造环境中，实时准确识别工人动作对生产效率、安全和人机协作至关重要，但传统集中式方法在隐私保护上存在不足。

Method: 使用自定义骨骼数据集，结合改进的FastPose模型，对比了LSTM和Transformer编码器在集中式、本地化、FedAvg和FedEnsemble四种范式下的性能。

Result: FL Transformer在全局测试集上比集中式训练提升12.4个百分点，FedEnsemble提升16.3个百分点；在外部客户端上，FL和FedEnsemble分别提升52.6和58.3个百分点。

Conclusion: 联合学习不仅保护隐私，还显著提升了跨用户泛化能力，是工业场景中可扩展、隐私敏感的HAR实用解决方案。

Abstract: In smart manufacturing environments, accurate and real-time recognition of
worker actions is essential for productivity, safety, and human-machine
collaboration. While skeleton-based human activity recognition (HAR) offers
robustness to lighting, viewpoint, and background variations, most existing
approaches rely on centralized datasets, which are impractical in
privacy-sensitive industrial scenarios. This paper presents a federated
learning (FL) framework for pose-based HAR using a custom skeletal dataset of
eight industrially relevant upper-body gestures, captured from five
participants and processed using a modified FastPose model. Two temporal
backbones, an LSTM and a Transformer encoder, are trained and evaluated under
four paradigms: centralized, local (per-client), FL with weighted federated
averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the
global test set, the FL Transformer improves over centralized training by +12.4
percentage points, with FedEnsemble delivering a +16.3 percentage points gain.
On an unseen external client, FL and FedEnsemble exceed centralized accuracy by
+52.6 and +58.3 percentage points, respectively. These results demonstrate that
FL not only preserves privacy but also substantially enhances cross-user
generalization, establishing it as a practical solution for scalable,
privacy-aware HAR in heterogeneous industrial settings.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [86] [Minimizing Task-Oriented Age of Information for Remote Monitoring with Pre-Identification](https://arxiv.org/abs/2508.14575)
*Shuying Gan,Xijun Wang,Chao Xu,Xiang Chen*

Main category: cs.IT

TL;DR: 本文提出了任务导向信息时效性（TAoI）作为衡量信息内容相关性的新指标，并通过SMDP建模与MDP转换，设计了一种低复杂度的最优传输策略，实验证明了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 新兴智能应用需要任务导向通信范式的通用指标，以提升系统任务完成效率。

Method: 引入TAoI指标，通过SMDP建模并转换为MDP，提出基于阈值的相对值迭代算法和简化单阈值策略。

Result: 最优策略在实验中表现优于基准方法，低复杂度算法保持高性能。

Conclusion: TAoI与阈值策略能高效优化任务导向通信系统，兼顾性能与复杂度。

Abstract: The emergence of new intelligent applications has fostered the development of
a task-oriented communication paradigm, where a comprehensive, universal, and
practical metric is crucial for unleashing the potential of this paradigm. To
this end, we introduce an innovative metric, the Task-oriented Age of
Information (TAoI), to measure whether the content of information is relevant
to the system task, thereby assisting the system in efficiently completing
designated tasks. We apply TAoI to a wireless monitoring system tasked with
identifying targets and transmitting their images for subsequent analysis. To
minimize TAoI and determine the optimal transmission policy, we formulate the
dynamic transmission problem as a Semi-Markov Decision Process (SMDP) and
transform it into an equivalent Markov Decision Process (MDP). Our analysis
demonstrates that the optimal policy is threshold-based with respect to TAoI.
Building on this, we propose a low-complexity relative value iteration
algorithm tailored to this threshold structure to derive the optimal
transmission policy. Additionally, we introduce a simpler single-threshold
policy, which, despite a slight performance degradation, offers faster
convergence. Comprehensive experiments and simulations validate the superior
performance of our optimal transmission policy compared to two established
baseline approaches.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [87] [A Guide to Stakeholder Analysis for Cybersecurity Researchers](https://arxiv.org/abs/2508.14796)
*James C Davis,Sophie Chen,Huiyun Peng,Paschal C Amusuo,Kelechi G Kalu*

Main category: cs.CR

TL;DR: 为网络安全研究者提供实用指南，帮助其进行利益相关者伦理分析，以满足学术出版的要求。


<details>
  <summary>Details</summary>
Motivation: 网络安全研究需关注潜在危害，但许多研究者对伦理分析的方法不确定，需要具体指导。

Method: 列举利益相关者类型，并将其与常见实证研究方法对应，提供实际案例分析。

Result: 研究者能更清楚地识别和应对伦理问题，满足伦理审查要求。

Conclusion: 该指南助力研究团队自信、明确地完成伦理分析，避免困惑。

Abstract: Stakeholder-based ethics analysis is now a formal requirement for submissions
to top cybersecurity research venues. This requirement reflects a growing
consensus that cybersecurity researchers must go beyond providing capabilities
to anticipating and mitigating the potential harms thereof. However, many
cybersecurity researchers may be uncertain about how to proceed in an ethics
analysis. In this guide, we provide practical support for that requirement by
enumerating stakeholder types and mapping them to common empirical research
methods. We also offer worked examples to demonstrate how researchers can
identify likely stakeholder exposures in real-world projects. Our goal is to
help research teams meet new ethics mandates with confidence and clarity, not
confusion.

</details>


### [88] [MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing](https://arxiv.org/abs/2508.14300)
*Youssef Maklad,Fares Wael,Ali Hamdi,Wael Elsersy,Khaled Shaban*

Main category: cs.CR

TL;DR: MultiFuzz是一种基于密集检索的多智能体系统，通过整合语义感知上下文检索、专用智能体和结构化工具辅助推理，解决了传统协议模糊测试技术的局限性，显著提高了分支覆盖率和协议状态探索能力。


<details>
  <summary>Details</summary>
Motivation: 传统协议模糊测试技术（如AFL-based系统）因缺乏对复杂协议语法的语义理解和僵化的种子变异策略而效果有限。尽管ChatAFL引入了大型语言模型（LLMs），但仍存在输出不可靠、LLM幻觉和假设LLM了解协议规范等问题。

Method: MultiFuzz利用协议文档（RFC）构建向量数据库的嵌入，通过检索增强生成（RAG）管道，使智能体生成更可靠和结构化的输出。系统将模糊测试过程分解为模块化智能体组，通过链式思考推理协作，动态调整模糊策略。

Result: 实验评估表明，MultiFuzz在实时流协议（RTSP）上显著提高了分支覆盖率，并深入探索协议状态和转换，优于NSFuzz、AFLNet和ChatAFL等先进模糊测试工具。

Conclusion: MultiFuzz通过结合密集检索、智能体协调和语言模型推理，为自主协议模糊测试建立了新范式，为未来智能体模糊测试系统提供了可扩展和可扩展的基础。

Abstract: Traditional protocol fuzzing techniques, such as those employed by AFL-based
systems, often lack effectiveness due to a limited semantic understanding of
complex protocol grammars and rigid seed mutation strategies. Recent works,
such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol
fuzzing and address these limitations, pushing protocol fuzzers to wider
exploration of the protocol state space. But ChatAFL still faces issues like
unreliable output, LLM hallucinations, and assumptions of LLM knowledge about
protocol specifications. This paper introduces MultiFuzz, a novel dense
retrieval-based multi-agent system designed to overcome these limitations by
integrating semantic-aware context retrieval, specialized agents, and
structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of
protocol documentation (RFC Documents) to build embeddings in a vector database
for a retrieval-augmented generation (RAG) pipeline, enabling agents to
generate more reliable and structured outputs, enhancing the fuzzer in mutating
protocol messages with enhanced state coverage and adherence to syntactic
constraints. The framework decomposes the fuzzing process into modular groups
of agents that collaborate through chain-of-thought reasoning to dynamically
adapt fuzzing strategies based on the retrieved contextual knowledge.
Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate
that MultiFuzz significantly improves branch coverage and explores deeper
protocol states and transitions over state-of-the-art (SOTA) fuzzers such as
NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic
coordination, and language model reasoning, MultiFuzz establishes a new
paradigm in autonomous protocol fuzzing, offering a scalable and extensible
foundation for future research in intelligent agentic-based fuzzing systems.

</details>


### [89] [CoFacS -- Simulating a Complete Factory to Study the Security of Interconnected Production](https://arxiv.org/abs/2508.14526)
*Stefan Lenz,David Schachtschneider,Simon Jonas,Liam Tirpitz,Sandra Geisler,Martin Henze*

Main category: cs.CR

TL;DR: CoFacS是一个完整的工厂模拟系统，用于研究工业网络安全措施，能够模拟整条生产线并集成实际工业应用，验证其准确性后进行网络安全研究。


<details>
  <summary>Details</summary>
Motivation: 工业工厂的数字化使其面临严重的网络攻击风险，现有测试平台无法全面模拟复杂生产线，难以评估攻击影响和防护措施的有效性。

Method: 提出CoFacS系统，模拟整条生产线并支持实际工业应用集成，通过与物理模型工厂对比验证其准确性。

Result: CoFacS的最大偏差为0.11%，可研究物理或网络攻击的影响，并通过案例展示了其在攻击检测和5G通信抗干扰研究中的应用。

Conclusion: CoFacS为工业网络安全研究提供了全面且准确的模拟平台，填补了现有测试平台的不足。

Abstract: While the digitization of industrial factories provides tremendous
improvements for the production of goods, it also renders such systems
vulnerable to serious cyber-attacks. To research, test, and validate security
measures protecting industrial networks against such cyber-attacks, the
security community relies on testbeds to simulate industrial systems, as
utilizing live systems endangers costly components or even human life. However,
existing testbeds focus on individual parts of typically complex production
lines in industrial factories. Consequently, the impact of cyber-attacks on
industrial networks as well as the effectiveness of countermeasures cannot be
evaluated in an end-to-end manner. To address this issue and facilitate
research on novel security mechanisms, we present CoFacS, the first COmplete
FACtory Simulation that replicates an entire production line and affords the
integration of real-life industrial applications. To showcase that CoFacS
accurately captures real-world behavior, we validate it against a physical
model factory widely used in security research. We show that CoFacS has a
maximum deviation of 0.11% to the physical reference, which enables us to study
the impact of physical attacks or network-based cyber-attacks. Moreover, we
highlight how CoFacS enables security research through two cases studies
surrounding attack detection and the resilience of 5G-based industrial
communication against jamming.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [90] [PAPPL: Personalized AI-Powered Progressive Learning Platform](https://arxiv.org/abs/2508.14109)
*Shayan Bafandkar,Sungyong Chung,Homa Khosravian,Alireza Talebpour*

Main category: cs.CY

TL;DR: 本文提出个性化AI驱动的渐进学习平台(PAPPL)，用于解决工程教育中标准化框架的问题，通过AI技术支持个性化学习。


<details>
  <summary>Details</summary>
Motivation: 工程教育长期受限于标准化框架，缺乏对学生多样化需求的支持。

Method: 开发PAPPL平台，整合智能辅导系统(ITS)组件和GPT-4o技术，提供个性化反馈和分析。

Result: PAPPL能动态适应学生学习需求，提供针对性帮助，并为教师提供数据分析。

Conclusion: PAPPL为生成式ITS在各级教育中的扩展提供了基础框架，展示了个性化学习和生成式AI在教育中的潜力。

Abstract: Engineering education has historically been constrained by rigid,
standardized frameworks, often neglecting students' diverse learning needs and
interests. While significant advancements have been made in online and
personalized education within K-12 and foundational sciences, engineering
education at both undergraduate and graduate levels continues to lag in
adopting similar innovations. Traditional evaluation methods, such as exams and
homework assignments, frequently overlook individual student requirements,
impeding personalized educational experiences. To address these limitations,
this paper introduces the Personalized AI-Powered Progressive Learning (PAPPL)
platform, an advanced Intelligent Tutoring System (ITS) designed specifically
for engineering education. It highlights the development of a scalable,
data-driven tutoring environment leveraging cutting-edge AI technology to
enhance personalized learning across diverse academic disciplines, particularly
in STEM fields. PAPPL integrates core ITS components including the expert
module, student module, tutor module, and user interface, and utilizes GPT-4o,
a sophisticated large language model (LLM), to deliver context-sensitive and
pedagogically sound hints based on students' interactions. The system uniquely
records student attempts, detects recurring misconceptions, and generates
progressively targeted feedback, providing personalized assistance that adapts
dynamically to each student's learning profile. Additionally, PAPPL offers
instructors detailed analytics, empowering evidence-based adjustments to
teaching strategies. This study provides a fundamental framework for the
progression of Generative ITSs scalable to all education levels, delivering
important perspectives on personalized progressive learning and the wider
possibilities of Generative AI in the field of education.

</details>


### [91] [Documenting Deployment with Fabric: A Repository of Real-World AI Governance](https://arxiv.org/abs/2508.14119)
*Mackenzie Jorgensen,Kendall Brogle,Katherine M. Collins,Lujain Ibrahim,Arina Shah,Petra Ivanovic,Noah Broestl,Gabriel Piles,Paul Dongha,Hatim Abdulhussein,Adrian Weller,Jillian Powers,Umang Bhatt*

Main category: cs.CY

TL;DR: 论文介绍了一个公开的AI用例仓库Fabric，旨在研究AI治理机制，通过访谈和实践者合作设计AI流程图，并分析治理中的常见模式和差距。


<details>
  <summary>Details</summary>
Motivation: AI在社会中的应用越来越广泛，但现有研究多关注其风险和危害，缺乏对其治理机制的系统研究。

Method: 通过半结构化访谈收集20个AI用例，并与实践者共同设计AI流程图，分析治理机制和监督措施。

Result: Fabric仓库包含AI用例的视觉化图表和描述，揭示了治理中的差距和常见的人类监督模式。

Conclusion: Fabric旨在为研究者提供一个可扩展的工具，以研究AI治理的有效性。

Abstract: Artificial intelligence (AI) is increasingly integrated into society, from
financial services and traffic management to creative writing. Academic
literature on the deployment of AI has mostly focused on the risks and harms
that result from the use of AI. We introduce Fabric, a publicly available
repository of deployed AI use cases to outline their governance mechanisms.
Through semi-structured interviews with practitioners, we collect an initial
set of 20 AI use cases. In addition, we co-design diagrams of the AI workflow
with the practitioners. We discuss the oversight mechanisms and guardrails used
in practice to safeguard AI use. The Fabric repository includes visual diagrams
of AI use cases and descriptions of the deployed systems. Using the repository,
we surface gaps in governance and find common patterns in human oversight of
deployed AI systems. We intend for Fabric to serve as an extendable, evolving
tool for researchers to study the effectiveness of AI governance.

</details>
