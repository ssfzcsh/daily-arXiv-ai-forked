{"id": "2507.23087", "pdf": "https://arxiv.org/pdf/2507.23087", "abs": "https://arxiv.org/abs/2507.23087", "authors": ["Fabian Stiehle", "Hans Weytjens", "Ingo Weber"], "title": "On LLM-Assisted Generation of Smart Contracts from Business Processes", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted at the Workshop on Distributed Ledger Technologies in\n  Business Process Management, At the International Conference for Business\n  Process Management (BPM), 2025", "summary": "Large language models (LLMs) have changed the reality of how software is\nproduced. Within the wider software engineering community, among many other\npurposes, they are explored for code generation use cases from different types\nof input. In this work, we present an exploratory study to investigate the use\nof LLMs for generating smart contract code from business process descriptions,\nan idea that has emerged in recent literature to overcome the limitations of\ntraditional rule-based code generation approaches. However, current LLM-based\nwork evaluates generated code on small samples, relying on manual inspection,\nor testing whether code compiles but ignoring correct execution. With this\nwork, we introduce an automated evaluation framework and provide empirical data\nfrom larger data sets of process models. We test LLMs of different types and\nsizes in their capabilities of achieving important properties of process\nexecution, including enforcing process flow, resource allocation, and\ndata-based conditions. Our results show that LLM performance falls short of the\nperfect reliability required for smart contract development. We suggest future\nwork to explore responsible LLM integrations in existing tools for code\ngeneration to ensure more reliable output. Our benchmarking framework can serve\nas a foundation for developing and evaluating such integrations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u4e1a\u52a1\u6d41\u7a0b\u63cf\u8ff0\u751f\u6210\u667a\u80fd\u5408\u7ea6\u4ee3\u7801\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u3002\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u667a\u80fd\u5408\u7ea6\u5f00\u53d1\u4e2d\u7684\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7LLMs\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u8bd5\u4e0d\u540c\u89c4\u6a21\u548c\u7c7b\u578b\u7684LLMs\u5728\u751f\u6210\u4ee3\u7801\u65f6\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u6d41\u7a0b\u6267\u884c\u7684\u5173\u952e\u7279\u6027\uff0c\u5982\u6d41\u7a0b\u63a7\u5236\u3001\u8d44\u6e90\u5206\u914d\u548c\u6570\u636e\u6761\u4ef6\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u751f\u6210\u7684\u4ee3\u7801\u5728\u667a\u80fd\u5408\u7ea6\u5f00\u53d1\u4e2d\u672a\u80fd\u8fbe\u5230\u6240\u9700\u7684\u5b8c\u7f8e\u53ef\u9760\u6027\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u7814\u7a76\u63a2\u7d22\u5c06LLMs\u8d1f\u8d23\u4efb\u5730\u96c6\u6210\u5230\u73b0\u6709\u4ee3\u7801\u751f\u6210\u5de5\u5177\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u8f93\u51fa\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u6846\u67b6\u4f5c\u4e3a\u5f00\u53d1\u6b64\u7c7b\u96c6\u6210\u7684\u57fa\u7840\u3002"}}
{"id": "2507.23118", "pdf": "https://arxiv.org/pdf/2507.23118", "abs": "https://arxiv.org/abs/2507.23118", "authors": ["Mattia Di Profio", "Mingjun Zhong", "Yaji Sripada", "Marcel Jaspars"], "title": "FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering", "categories": ["cs.SE"], "comment": null, "summary": "The Extract, Transform, Load (ETL) workflow is fundamental for populating and\nmaintaining data warehouses and other data stores accessed by analysts for\ndownstream tasks. A major shortcoming of modern ETL solutions is the extensive\nneed for a human-in-the-loop, required to design and implement\ncontext-specific, and often non-generalisable transformations. While related\nwork in the field of ETL automation shows promising progress, there is a lack\nof solutions capable of automatically designing and applying these\ntransformations. We present FlowETL, a novel example-based autonomous ETL\npipeline architecture designed to automatically standardise and prepare input\ndatasets according to a concise, user-defined target dataset. FlowETL is an\necosystem of components which interact together to achieve the desired outcome.\nA Planning Engine uses a paired input-output datasets sample to construct a\ntransformation plan, which is then applied by an ETL worker to the source\ndataset. Monitoring and logging provide observability throughout the entire\npipeline. The results show promising generalisation capabilities across 14\ndatasets of various domains, file structures, and file sizes.", "AI": {"tldr": "FlowETL \u662f\u4e00\u4e2a\u57fa\u4e8e\u793a\u4f8b\u7684\u81ea\u52a8\u5316 ETL \u7ba1\u9053\u67b6\u6784\uff0c\u65e8\u5728\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u81ea\u52a8\u6807\u51c6\u5316\u548c\u51c6\u5907\u8f93\u5165\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u4ee3 ETL \u89e3\u51b3\u65b9\u6848\u9700\u8981\u5927\u91cf\u4eba\u5de5\u53c2\u4e0e\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u8f6c\u6362\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u80fd\u529b\u3002", "method": "FlowETL \u901a\u8fc7\u4e00\u4e2a\u751f\u6001\u7cfb\u7edf\u7ec4\u4ef6\u4ea4\u4e92\u5b9e\u73b0\u76ee\u6807\uff0c\u5305\u62ec\u89c4\u5212\u5f15\u64ce\uff08\u4f7f\u7528\u8f93\u5165-\u8f93\u51fa\u6837\u672c\u6784\u5efa\u8f6c\u6362\u8ba1\u5212\uff09\u548c ETL \u5de5\u4f5c\u5668\u6267\u884c\u8f6c\u6362\uff0c\u540c\u65f6\u63d0\u4f9b\u76d1\u63a7\u548c\u65e5\u5fd7\u529f\u80fd\u3002", "result": "\u5728 14 \u4e2a\u4e0d\u540c\u9886\u57df\u3001\u6587\u4ef6\u7ed3\u6784\u548c\u5927\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FlowETL \u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316 ETL \u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u5e76\u63d0\u9ad8\u6548\u7387\u3002"}}
{"id": "2507.23120", "pdf": "https://arxiv.org/pdf/2507.23120", "abs": "https://arxiv.org/abs/2507.23120", "authors": ["Jordi Cabot"], "title": "Vibe Modeling: Challenges and Opportunities", "categories": ["cs.SE"], "comment": null, "summary": "There is a pressing need for better development methods and tools to keep up\nwith the growing demand and increasing complexity of new software systems. New\ntypes of user interfaces, the need for intelligent components, sustainability\nconcerns, ... bring new challenges that we need to handle. In the last years,\nmodel-driven engineering (MDE) has been key to improving the quality and\nproductivity of software development, but models themselves are becoming\nincreasingly complex to specify and manage. At the same time, we are witnessing\nthe growing popularity of vibe coding approaches that rely on Large Language\nModels (LLMs) to transform natural language descriptions into running code at\nthe expenses of code vulnerabilities, scalability issues and maintainability\nconcerns. In this paper, we introduce the concept of \\textit{vibe modeling} as\na novel approach to integrate the best of both worlds (AI and MDE) to speed up\nthe development of reliable complex systems. We outline the key concepts of\nvibe modeling and highlight the opportunities and open challenges it presents\nfor the future of modeling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408AI\u548cMDE\u7684\u201cvibe modeling\u201d\u65b9\u6cd5\uff0c\u4ee5\u52a0\u901f\u5f00\u53d1\u53ef\u9760\u590d\u6742\u7cfb\u7edf\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u7684\u9700\u6c42\u589e\u957f\u548c\u590d\u6742\u6027\u63d0\u5347\uff0c\u73b0\u6709\u5f00\u53d1\u65b9\u6cd5\u548c\u5de5\u5177\uff08\u5982MDE\u548c\u57fa\u4e8eLLM\u7684\u7f16\u7a0b\uff09\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u201cvibe modeling\u201d\u6982\u5ff5\uff0c\u6574\u5408AI\uff08\u5982LLM\uff09\u548c\u6a21\u578b\u9a71\u52a8\u5de5\u7a0b\uff08MDE\uff09\uff0c\u4ee5\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u53ef\u9760\u4ee3\u7801\u3002", "result": "vibe modeling\u6709\u671b\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4f46\u540c\u65f6\u4e5f\u9762\u4e34\u673a\u9047\u548c\u5f00\u653e\u6027\u95ee\u9898\u3002", "conclusion": "vibe modeling\u4e3a\u590d\u6742\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u5176\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2507.23168", "pdf": "https://arxiv.org/pdf/2507.23168", "abs": "https://arxiv.org/abs/2507.23168", "authors": ["Elmira Onagh", "Maleknaz Nayebi"], "title": "Extension Decisions in Open Source Software Ecosystem", "categories": ["cs.SE"], "comment": "Paper published in JSS journal", "summary": "GitHub Marketplace is expanding by approximately 41% annually, with new\ntools; however, many additions replicate existing functionality. We study this\nphenomenon in the platform's largest segment, Continuous Integration (CI), by\nlinking 6,983 CI Actions to 3,869 providers and mining their version histories.\nOur graph model timestamps every functionality's debut, tracks its adoption,\nand clusters redundant tools. We find that approximately 65% of new CI Actions\nreplicate existing capabilities, typically within six months, and that a small\nset of first-mover Actions accounts for most subsequent forks and extensions.\nThese insights enable developers to choose the optimal moment to launch, target\nunmet functionality, and help maintainers eliminate redundant tools. We publish\nthe complete graph and dataset to encourage longitudinal research on innovation\nand competition in software ecosystems, and to provide practitioners with a\ndata-driven roadmap for identifying emerging trends and guiding product\nstrategy.", "AI": {"tldr": "GitHub Marketplace\u6bcf\u5e74\u589e\u957f41%\uff0c\u4f46\u8bb8\u591a\u65b0\u5de5\u5177\u529f\u80fd\u91cd\u590d\u3002\u7814\u7a76\u805a\u7126\u4e8eCI\u9886\u57df\uff0c\u53d1\u73b065%\u7684\u65b0Actions\u529f\u80fd\u91cd\u590d\uff0c\u4e14\u5c11\u6570\u65e9\u671f\u5de5\u5177\u4e3b\u5bfc\u540e\u7eed\u53d1\u5c55\u3002", "motivation": "\u5206\u6790GitHub Marketplace\u4e2d\u5de5\u5177\u529f\u80fd\u91cd\u590d\u7684\u73b0\u8c61\uff0c\u5c24\u5176\u662fCI\u9886\u57df\uff0c\u4ee5\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u7ba1\u7406\u8005\u4f18\u5316\u5de5\u5177\u9009\u62e9\u548c\u7ef4\u62a4\u3002", "method": "\u901a\u8fc7\u94fe\u63a56,983\u4e2aCI Actions\u548c3,869\u4e2a\u63d0\u4f9b\u8005\uff0c\u5e76\u6316\u6398\u5176\u7248\u672c\u5386\u53f2\uff0c\u6784\u5efa\u56fe\u6a21\u578b\u8ddf\u8e2a\u529f\u80fd\u53d1\u5e03\u65f6\u95f4\u3001\u91c7\u7528\u60c5\u51b5\u548c\u91cd\u590d\u5de5\u5177\u805a\u7c7b\u3002", "result": "\u7ea665%\u7684\u65b0CI Actions\u529f\u80fd\u91cd\u590d\uff0c\u4e14\u901a\u5e38\u57286\u4e2a\u6708\u5185\u51fa\u73b0\uff1b\u5c11\u6570\u65e9\u671f\u5de5\u5177\u4e3b\u5bfc\u540e\u7eed\u7684\u884d\u751f\u548c\u6269\u5c55\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\uff0c\u5e2e\u52a9\u4ed6\u4eec\u9009\u62e9\u6700\u4f73\u53d1\u5e03\u65f6\u95f4\u548c\u5de5\u5177\u7b56\u7565\uff0c\u540c\u65f6\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u957f\u671f\u7814\u7a76\u548c\u5b9e\u8df5\u5e94\u7528\u3002"}}
{"id": "2507.23002", "pdf": "https://arxiv.org/pdf/2507.23002", "abs": "https://arxiv.org/abs/2507.23002", "authors": ["Peter F. Michael", "Zekun Hao", "Serge Belongie", "Abe Davis"], "title": "Noise-Coded Illumination for Forensic and Photometric Video Analysis", "categories": ["cs.GR", "cs.CR", "cs.CV"], "comment": "ACM Transactions on Graphics (2025), presented at SIGGRAPH 2025", "summary": "The proliferation of advanced tools for manipulating video has led to an arms\nrace, pitting those who wish to sow disinformation against those who want to\ndetect and expose it. Unfortunately, time favors the ill-intentioned in this\nrace, with fake videos growing increasingly difficult to distinguish from real\nones. At the root of this trend is a fundamental advantage held by those\nmanipulating media: equal access to a distribution of what we consider\nauthentic (i.e., \"natural\") video. In this paper, we show how coding very\nsubtle, noise-like modulations into the illumination of a scene can help combat\nthis advantage by creating an information asymmetry that favors verification.\nOur approach effectively adds a temporal watermark to any video recorded under\ncoded illumination. However, rather than encoding a specific message, this\nwatermark encodes an image of the unmanipulated scene as it would appear lit\nonly by the coded illumination. We show that even when an adversary knows that\nour technique is being used, creating a plausible coded fake video amounts to\nsolving a second, more difficult version of the original adversarial content\ncreation problem at an information disadvantage. This is a promising avenue for\nprotecting high-stakes settings like public events and interviews, where the\ncontent on display is a likely target for manipulation, and while the\nillumination can be controlled, the cameras capturing video cannot.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7f16\u7801\u5149\u7167\u660e\u6765\u5d4c\u5165\u65f6\u95f4\u6c34\u5370\u7684\u65b9\u6cd5\uff0c\u4ee5\u5bf9\u6297\u89c6\u9891\u4f2a\u9020\u95ee\u9898\uff0c\u5229\u7528\u4fe1\u606f\u4e0d\u5bf9\u79f0\u63d0\u9ad8\u9a8c\u8bc1\u80fd\u529b\u3002", "motivation": "\u7531\u4e8e\u89c6\u9891\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u771f\u5b9e\u89c6\u9891\u4e0e\u4f2a\u9020\u89c6\u9891\u7684\u533a\u5206\u53d8\u5f97\u65e5\u76ca\u56f0\u96be\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7f16\u7801\u5149\u7167\u660e\u4e3a\u89c6\u9891\u6dfb\u52a0\u53ef\u9a8c\u8bc1\u7684\u6c34\u5370\uff0c\u4ece\u800c\u5728\u9a8c\u8bc1\u8fc7\u7a0b\u4e2d\u5360\u636e\u4fe1\u606f\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u5728\u573a\u666f\u7167\u660e\u4e2d\u7f16\u7801\u7ec6\u5fae\u7684\u566a\u58f0\u8c03\u5236\uff0c\u5d4c\u5165\u65f6\u95f4\u6c34\u5370\u3002\u6c34\u5370\u4e0d\u7f16\u7801\u5177\u4f53\u4fe1\u606f\uff0c\u800c\u662f\u8bb0\u5f55\u672a\u7be1\u6539\u573a\u666f\u5728\u7f16\u7801\u7167\u660e\u4e0b\u7684\u56fe\u50cf\u3002", "result": "\u5373\u4f7f\u653b\u51fb\u8005\u77e5\u9053\u8be5\u65b9\u6cd5\u7684\u5b58\u5728\uff0c\u4f2a\u9020\u5e26\u6709\u7f16\u7801\u6c34\u5370\u7684\u89c6\u9891\u4ecd\u9700\u89e3\u51b3\u66f4\u590d\u6742\u7684\u5bf9\u6297\u6027\u95ee\u9898\uff0c\u4e14\u5904\u4e8e\u4fe1\u606f\u52a3\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4fdd\u62a4\u9ad8\u4ef7\u503c\u573a\u666f\uff08\u5982\u516c\u5171\u4e8b\u4ef6\u548c\u91c7\u8bbf\uff09\u7684\u89c6\u9891\u771f\u5b9e\u6027\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u53ef\u63a7\u5236\u7167\u660e\u4f46\u65e0\u6cd5\u63a7\u5236\u62cd\u6444\u76f8\u673a\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2507.23444", "pdf": "https://arxiv.org/pdf/2507.23444", "abs": "https://arxiv.org/abs/2507.23444", "authors": ["Xiang Li", "Xianfu Cheng", "Xiaoming Zhang", "Zhoujun Li"], "title": "Hybrid CNN-Mamba Enhancement Network for Robust Multimodal Sentiment Analysis", "categories": ["cs.MM"], "comment": null, "summary": "Multimodal Sentiment Analysis (MSA) with missing modalities has recently\nattracted increasing attention. Although existing research mainly focuses on\ndesigning complex model architectures to handle incomplete data, it still faces\nsignificant challenges in effectively aligning and fusing multimodal\ninformation. In this paper, we propose a novel framework called the Hybrid\nCNN-Mamba Enhancement Network (HCMEN) for robust multimodal sentiment analysis\nunder missing modality conditions. HCMEN is designed around three key\ncomponents: (1) hierarchical unimodal modeling, (2) cross-modal enhancement and\nalignment, and (3) multimodal mix-up fusion. First, HCMEN integrates the\nstrengths of Convolutional Neural Network (CNN) for capturing local details and\nthe Mamba architecture for modeling global contextual dependencies across\ndifferent modalities. Furthermore, grounded in the principle of Mutual\nInformation Maximization, we introduce a cross-modal enhancement mechanism that\ngenerates proxy modalities from mixed token-level representations and learns\nfine-grained token-level correspondences between modalities. The enhanced\nunimodal features are then fused and passed through the CNN-Mamba backbone,\nenabling local-to-global cross-modal interaction and comprehensive multimodal\nintegration. Extensive experiments on two benchmark MSA datasets demonstrate\nthat HCMEN consistently outperforms existing state-of-the-art methods,\nachieving superior performance across various missing modality scenarios. The\ncode will be released publicly in the near future.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHCMEN\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u6a21\u6001\u7f3a\u5931\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408CNN\u548cMamba\u67b6\u6784\u5b9e\u73b0\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u878d\u5408\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6a21\u6001\u7f3a\u5931\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u65f6\u9762\u4e34\u5bf9\u9f50\u548c\u878d\u5408\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408CNN\u548cMamba\u67b6\u6784\uff0c\u5f15\u5165\u8de8\u6a21\u6001\u589e\u5f3a\u673a\u5236\u548c\u6df7\u5408\u878d\u5408\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u7684\u7efc\u5408\u5904\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHCMEN\u5728\u5404\u79cd\u6a21\u6001\u7f3a\u5931\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HCMEN\u6846\u67b6\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u6027\u80fd\uff0c\u672a\u6765\u5c06\u516c\u5f00\u4ee3\u7801\u3002"}}
{"id": "2507.23151", "pdf": "https://arxiv.org/pdf/2507.23151", "abs": "https://arxiv.org/abs/2507.23151", "authors": ["Louis Rustenholz", "Pedro Lopez-Garcia", "Manuel V. Hermenegildo"], "title": "Abstractions of Sequences, Functions and Operators", "categories": ["cs.PL", "cs.LO"], "comment": "Under consideration for publication in STTT", "summary": "We present theoretical and practical results on the order theory of lattices\nof functions, focusing on Galois connections that abstract (sets of) functions\n- a topic known as higher-order abstract interpretation.\n  We are motivated by the challenge of inferring closed-form bounds on\nfunctions which are defined recursively, i.e. as the fixed point of an operator\nor, equivalently, as the solution to a functional equation. This has multiple\napplications in program analysis (e.g. cost analysis, loop acceleration,\ndeclarative language analysis) and in hybrid systems governed by differential\nequations.\n  Our main contribution is a new family of constraint-based abstract domains\nfor abstracting numerical functions, B-bound domains, which abstract a function\nf by a conjunction of bounds from a preselected set of boundary functions. They\nallow inferring highly non-linear numerical invariants, which classical\nnumerical abstract domains struggle with. We uncover a convexity property in\nthe constraint space that simplifies, and, in some cases, fully automates,\ntransfer function design.\n  We also introduce domain abstraction, a functor that lifts arbitrary mappings\nin value space to Galois connections in function space. This supports\nabstraction from symbolic to numerical functions (i.e. size abstraction), and\nenables dimensionality reduction of equations.\n  We base our constructions of transfer functions on a simple operator\nlanguage, starting with sequences, and extending to more general functions,\nincluding multivariate, piecewise, and non-discrete domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u5173\u4e8e\u51fd\u6570\u683c\u5e8f\u7406\u8bba\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u7ed3\u679c\uff0c\u4e13\u6ce8\u4e8eGalois\u8fde\u63a5\u7684\u62bd\u8c61\uff0c\u63d0\u51fa\u4e86\u65b0\u7684B-bound\u62bd\u8c61\u57df\u548c\u9886\u57df\u62bd\u8c61\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a8\u65ad\u9012\u5f52\u5b9a\u4e49\u7684\u51fd\u6570\u7684\u95ed\u5f0f\u8fb9\u754c\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7a0b\u5e8f\u5206\u6790\u548c\u6df7\u5408\u7cfb\u7edf\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u63a8\u65ad\u9012\u5f52\u5b9a\u4e49\u7684\u51fd\u6570\uff08\u5373\u7b97\u5b50\u7684\u4e0d\u52a8\u70b9\u6216\u51fd\u6570\u65b9\u7a0b\u7684\u89e3\uff09\u7684\u95ed\u5f0f\u8fb9\u754c\uff0c\u8fd9\u5728\u7a0b\u5e8f\u5206\u6790\uff08\u5982\u6210\u672c\u5206\u6790\u3001\u5faa\u73af\u52a0\u901f\u3001\u58f0\u660e\u6027\u8bed\u8a00\u5206\u6790\uff09\u548c\u7531\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u6df7\u5408\u7cfb\u7edf\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51fa\u4e86\u4e00\u65cf\u65b0\u7684\u57fa\u4e8e\u7ea6\u675f\u7684\u62bd\u8c61\u57df\uff08B-bound\u57df\uff09\uff0c\u7528\u4e8e\u62bd\u8c61\u6570\u503c\u51fd\u6570\uff1b\u5f15\u5165\u4e86\u9886\u57df\u62bd\u8c61\u65b9\u6cd5\uff0c\u5c06\u4efb\u610f\u503c\u7a7a\u95f4\u6620\u5c04\u63d0\u5347\u4e3a\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684Galois\u8fde\u63a5\uff1b\u5e76\u4f7f\u7528\u7b80\u5355\u7684\u7b97\u5b50\u8bed\u8a00\u6784\u5efa\u4f20\u9012\u51fd\u6570\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cB-bound\u57df\u80fd\u591f\u63a8\u65ad\u9ad8\u5ea6\u975e\u7ebf\u6027\u7684\u6570\u503c\u4e0d\u53d8\u91cf\uff0c\u800c\u4f20\u7edf\u6570\u503c\u62bd\u8c61\u57df\u96be\u4ee5\u5904\u7406\uff1b\u7ea6\u675f\u7a7a\u95f4\u7684\u51f8\u6027\u7b80\u5316\u4e86\u4f20\u9012\u51fd\u6570\u7684\u8bbe\u8ba1\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5b8c\u5168\u81ea\u52a8\u5316\u3002", "conclusion": "\u7ed3\u8bba\u662f\u901a\u8fc7B-bound\u57df\u548c\u9886\u57df\u62bd\u8c61\u7684\u8054\u5408\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u62bd\u8c61\u548c\u5206\u6790\u590d\u6742\u7684\u6570\u503c\u51fd\u6570\uff0c\u652f\u6301\u4ece\u7b26\u53f7\u51fd\u6570\u5230\u6570\u503c\u51fd\u6570\u7684\u62bd\u8c61\uff0c\u5e76\u5b9e\u73b0\u4e86\u65b9\u7a0b\u7684\u964d\u7ef4\u5904\u7406\u3002"}}
{"id": "2507.23419", "pdf": "https://arxiv.org/pdf/2507.23419", "abs": "https://arxiv.org/abs/2507.23419", "authors": ["James Rhodes", "Lawrence Ong", "Duy T. Ngo"], "title": "WiRM: Wireless Respiration Monitoring Using Conjugate Multiple Channel State Information and Fast Iterative Filtering in Wi-Fi Systems", "categories": ["cs.ET", "eess.SP"], "comment": null, "summary": "Monitoring respiratory health with the use of channel state information (CSI)\nhas shown promising results. Many existing methods focus on monitoring only the\nrespiratory rate, while others focus on monitoring the motion of the chest as a\npatient breathes, which is referred to as the respiratory waveform. This paper\npresents WiRM, a two-staged approach to contactless respiration monitoring. In\nthe first stage, WiRM improves upon existing respiratory rate estimation\ntechniques by using conjugate multiplication for phase sanitisation and\nadaptive multi-trace carving (AMTC) for tracing how the respiratory rate\nchanges over time. When compared against three state-of-the-art methods, WiRM\nhas achieved an average reduction of $38\\%$ in respiratory rate root mean\nsquared error (RMSE). In the second stage, WiRM uses this improved respiratory\nrate estimate to inform the decomposition and selection of the respiratory\nwaveform from the CSI data. Remarkably, WiRM delivers a $178.3\\%$ improvement\nin average absolute correlation with the ground truth respiratory waveform.\nWithin the literature, it is difficult to compare the robustness of existing\nalgorithms in noisy environments. In this paper, we develop a purpose-built\nsimulation toolkit to evaluate the robustness of respiration monitoring\nsolutions under various noise conditions, including thermal, multiplicative,\nand phase noise. Our results show that WiRM demonstrates improved or comparable\nresilience to these common noise sources.", "AI": {"tldr": "WiRM\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u7684\u65e0\u7ebf\u547c\u5438\u76d1\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u4f4d\u51c0\u5316\u548c\u81ea\u9002\u5e94\u591a\u8ff9\u8ffd\u8e2a\u63d0\u9ad8\u547c\u5438\u9891\u7387\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528\u6539\u8fdb\u7684\u4f30\u8ba1\u7ed3\u679c\u5206\u89e3\u548c\u9009\u62e9\u547c\u5438\u6ce2\u5f62\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6ce2\u5f62\u76f8\u5173\u6027\u3002", "motivation": "\u73b0\u6709\u547c\u5438\u76d1\u6d4b\u65b9\u6cd5\u591a\u5173\u6ce8\u547c\u5438\u9891\u7387\u6216\u80f8\u5ed3\u8fd0\u52a8\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "WiRM\u91c7\u7528\u5171\u8f6d\u4e58\u6cd5\u51c0\u5316\u76f8\u4f4d\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u591a\u8ff9\u8ffd\u8e2a\uff08AMTC\uff09\uff0c\u5e76\u901a\u8fc7\u6539\u8fdb\u7684\u547c\u5438\u9891\u7387\u4f30\u8ba1\u5206\u89e3\u548c\u9009\u62e9\u547c\u5438\u6ce2\u5f62\u3002", "result": "WiRM\u51cf\u5c11\u4e8638%\u7684\u547c\u5438\u9891\u7387\u5747\u65b9\u6839\u8bef\u5dee\uff0c\u6ce2\u5f62\u76f8\u5173\u6027\u63d0\u5347\u4e86178.3%\uff0c\u5e76\u5728\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "WiRM\u5728\u547c\u5438\u9891\u7387\u548c\u6ce2\u5f62\u76d1\u6d4b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5177\u5907\u8f83\u5f3a\u7684\u566a\u58f0\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.23012", "pdf": "https://arxiv.org/pdf/2507.23012", "abs": "https://arxiv.org/abs/2507.23012", "authors": ["Ashkan Sobhani", "Sogand Sadrhaghighi", "Xingjun Chu"], "title": "PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers", "categories": ["cs.NI"], "comment": null, "summary": "Large-scale distributed training in production data centers place significant\ndemands on network infrastructure. In particular, significant load balancing\nchallenges arise when processing AI/ML workloads, consisting of low-entropy,\nbursty and long-lived flows. Existing solutions designed for Ethernet, such as\nEqual-Cost Multi-Path (ECMP) struggle to maintain high network utilization.\nWhile major industry players (e.g., Ultra Ethernet Consortium) and parts of\nacademia have proposed packet spraying to enhance AI/ML workload performance,\nwe argue that existing packet spraying solutions lead to buffer inflation over\ntime, negatively affecting network performance. Specifically, when ACK\ncoalescing is used, these solutions lead to stale information, degrading\nnetwork performance. Additionally, in asymmetric network conditions- such as\nmix of ordered an unordered traffic, or link degradation and failures- existing\npacket spraying solutions often lead to increased tail latency. In this paper,\nwe present the design and evaluation of PRIME, a pseudo-randomized round-robin\napproach to packet spraying that considers the network topology to optimize\nload distribution and performance. PRIME uses congestion as an indicator to\nre-balance the load. To this extent, PRIME takes into account various\ncongestion signals, accounting for congestion severity, and their decay times\nto avoid network hotspots. We extensively evaluated PRIME using large-scale\nproduction-level simulator. Our results indicate that, compared to existing\nsolutions, PRIME leads to up to 15% improvement for permutation traffic and up\nto 27% improvement in network degradation scenarios", "AI": {"tldr": "PRIME\u662f\u4e00\u79cd\u4f2a\u968f\u673a\u8f6e\u8be2\u5305\u55b7\u6d12\u65b9\u6cd5\uff0c\u901a\u8fc7\u8003\u8651\u7f51\u7edc\u62d3\u6251\u6765\u4f18\u5316\u8d1f\u8f7d\u5206\u914d\u548c\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709AI/ML\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u7f51\u7edc\u8d1f\u8f7d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u751f\u4ea7\u6570\u636e\u4e2d\u5fc3\u7684\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u5bf9\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u63d0\u51fa\u4e86\u9ad8\u8981\u6c42\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982ECMP\uff09\u5728AI/ML\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5305\u55b7\u6d12\u65b9\u6848\u4f1a\u5bfc\u81f4\u7f13\u51b2\u533a\u81a8\u80c0\u548c\u5c3e\u5ef6\u8fdf\u589e\u52a0\u3002", "method": "PRIME\u91c7\u7528\u4f2a\u968f\u673a\u8f6e\u8be2\u5305\u55b7\u6d12\u65b9\u6cd5\uff0c\u5229\u7528\u62e5\u585e\u4fe1\u53f7\u4f5c\u4e3a\u8d1f\u8f7d\u91cd\u65b0\u5e73\u8861\u7684\u6307\u6807\uff0c\u8003\u8651\u4e86\u62e5\u585e\u4e25\u91cd\u6027\u548c\u8870\u51cf\u65f6\u95f4\uff0c\u4ee5\u907f\u514d\u7f51\u7edc\u70ed\u70b9\u3002", "result": "PRIME\u5728\u7f6e\u6362\u6d41\u91cf\u548c\u7f51\u7edc\u9000\u5316\u573a\u666f\u4e2d\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe15%\u548c27%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PRIME\u901a\u8fc7\u4f18\u5316\u8d1f\u8f7d\u5206\u914d\u548c\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u5728AI/ML\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.23084", "pdf": "https://arxiv.org/pdf/2507.23084", "abs": "https://arxiv.org/abs/2507.23084", "authors": ["Taiyi Wang", "Eiko Yoneki"], "title": "AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads", "categories": ["cs.DB", "cs.AI"], "comment": "14 pages", "summary": "Efficiently selecting indexes is fundamental to database performance\noptimization, particularly for systems handling large-scale analytical\nworkloads. While deep reinforcement learning (DRL) has shown promise in\nautomating index selection through its ability to learn from experience, few\nworks address how these RL-based index advisors can adapt to scaling workloads\ndue to exponentially growing action spaces and heavy trial and error. To\naddress these challenges, we introduce AutoIndexer, a framework that combines\nworkload compression, query optimization, and specialized RL models to scale\nindex selection effectively. By operating on compressed workloads, AutoIndexer\nsubstantially lowers search complexity without sacrificing much index quality.\nExtensive evaluations show that it reduces end-to-end query execution time by\nup to 95% versus non-indexed baselines. On average, it outperforms\nstate-of-the-art RL-based index advisors by approximately 20% in workload cost\nsavings while cutting tuning time by over 50%. These results affirm\nAutoIndexer's practicality for large and diverse workloads.", "AI": {"tldr": "AutoIndexer\u7ed3\u5408\u4e86\u5de5\u4f5c\u8d1f\u8f7d\u538b\u7f29\u3001\u67e5\u8be2\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7d22\u5f15\u9009\u62e9\u7684\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u89c4\u6a21\u5206\u6790\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u7d22\u5f15\u9009\u62e9\u5bf9\u6570\u636e\u5e93\u6027\u80fd\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709RL\u65b9\u6cd5\u96be\u9002\u5e94\u6269\u5c55\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u901a\u8fc7\u5de5\u4f5c\u8d1f\u8f7d\u538b\u7f29\u3001\u67e5\u8be2\u4f18\u5316\u548c\u4e13\u7528RL\u6a21\u578b\uff0cAutoIndexer\u6709\u6548\u964d\u4f4e\u641c\u7d22\u590d\u6742\u5ea6\u3002", "result": "\u76f8\u6bd4\u57fa\u51c6\uff0c\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u51cf\u5c1195%\uff0c\u4f18\u4e8e\u73b0\u6709RL\u65b9\u6cd520%\uff0c\u8c03\u4f18\u65f6\u95f4\u51cf\u534a\u3002", "conclusion": "AutoIndexer\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5b9e\u7528\u6027\u663e\u8457\u3002"}}
{"id": "2507.23603", "pdf": "https://arxiv.org/pdf/2507.23603", "abs": "https://arxiv.org/abs/2507.23603", "authors": ["Andoni Rodriguez", "Irfansha Shaik", "Davide Corsi", "Roy Fox", "Cesar Sanchez"], "title": "Explanations for Unrealizability of Infinite-State Safety Shields", "categories": ["cs.LO"], "comment": null, "summary": "Safe Reinforcement Learning focuses on developing optimal policies while\nensuring safety. A popular method to address such task is shielding, in which a\ncorrect-by-construction safety component is synthesized from logical\nspecifications. Recently, shield synthesis has been extended to infinite-state\ndomains, such as continuous environments. This makes shielding more applicable\nto realistic scenarios. However, often shields might be unrealizable because\nthe specification is inconsistent (e.g., contradictory). In order to address\nthis gap, we present a method to obtain simple unconditional and conditional\nexplanations that witness unrealizability, which goes by temporal formula\nunrolling. In this paper, we show different variants of the technique and its\napplicability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u65f6\u6001\u516c\u5f0f\u5c55\u5f00\u751f\u6210\u65e0\u6761\u4ef6\u6216\u6709\u6761\u4ef6\u89e3\u91ca\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5c4f\u853d\u5408\u6210\u4e2d\u7531\u4e8e\u89c4\u8303\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u4e0d\u53ef\u5b9e\u73b0\u6027\u95ee\u9898\u3002", "motivation": "\u5c4f\u853d\u5408\u6210\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u6269\u5c55\u540e\u66f4\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\uff0c\u4f46\u89c4\u8303\u4e0d\u4e00\u81f4\u53ef\u80fd\u5bfc\u81f4\u5c4f\u853d\u4e0d\u53ef\u5b9e\u73b0\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u65f6\u6001\u516c\u5f0f\u5c55\u5f00\u6280\u672f\uff0c\u751f\u6210\u89c1\u8bc1\u4e0d\u53ef\u5b9e\u73b0\u6027\u7684\u65e0\u6761\u4ef6\u6216\u6709\u6761\u4ef6\u89e3\u91ca\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u4e0d\u540c\u53d8\u4f53\u53ca\u5176\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c4f\u853d\u5408\u6210\u4e2d\u7684\u4e0d\u53ef\u5b9e\u73b0\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u91ca\u5de5\u5177\u3002"}}
{"id": "2507.23387", "pdf": "https://arxiv.org/pdf/2507.23387", "abs": "https://arxiv.org/abs/2507.23387", "authors": ["Weicheng Xue", "Baisong Xu", "Kai Yang", "Yongxiang Liu", "Dengdeng Fan", "Pengxiang Xu", "Yonghong Tian"], "title": "H2SGEMM: Emulating FP32 GEMM on Ascend NPUs using FP16 Units with Precision Recovery and Cache-Aware Optimization", "categories": ["cs.DC"], "comment": null, "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose H2SGEMM,\na high-performance algorithm for emulating FP32 general matrix-matrix\nmultiplication (GEMM) using only FP16 computation units on a representative AI\naccelerator. The method decomposes each FP32 operand into two FP16 values and\ncompensates for numerical errors through a tunable scaling strategy. A detailed\nanalysis of numerical errors, including underflow conditions and precision\nloss, guides the selection of scaling parameters to preserve up to 22 bits of\nmantissa accuracy. We further investigate the effect of computation order on\naccuracy and demonstrate that a term-wise accumulation scheme improves\nnumerical stability over conventional FP32 GEMM in low-exponent regimes.\nFinally, a cache-aware blocking strategy and double-buffered pipeline are\nintroduced to overlap memory transfers with computation, enabling H2SGEMM to\nachieve up to 77% of the theoretical FP32-equivalent peak performance on Ascend\n910A NPU lacking native FP32 support. Extensive numerical experiments confirm\nthat our method not only recovers the accuracy of native FP32 GEMM but also\nexhibits superior numerical stability under certain conditions, due to its\nstructured and error-aware computation order.", "AI": {"tldr": "H2SGEMM\u662f\u4e00\u79cd\u5229\u7528FP16\u8ba1\u7b97\u5355\u5143\u6a21\u62dfFP32 GEMM\u7684\u9ad8\u6027\u80fd\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u6570\u503c\u548c\u8c03\u4f18\u7f29\u653e\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u4f4e\u7cbe\u5ea6\u77e9\u9635\u5f15\u64ce\uff08\u5982FP16\uff09\u7f3a\u4e4f\u5bf9\u5168\u7cbe\u5ea6\u8ba1\u7b97\u7684\u652f\u6301\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5728\u9ad8\u6027\u80fdAI\u52a0\u901f\u5668\u4e0a\u6a21\u62dfFP32 GEMM\u7684\u65b9\u6cd5\u3002", "method": "\u5c06FP32\u64cd\u4f5c\u6570\u5206\u89e3\u4e3a\u4e24\u4e2aFP16\u503c\uff0c\u5e76\u901a\u8fc7\u53ef\u8c03\u7f29\u653e\u7b56\u7565\u8865\u507f\u6570\u503c\u8bef\u5dee\uff1b\u91c7\u7528\u7f13\u5b58\u611f\u77e5\u5206\u5757\u7b56\u7565\u548c\u53cc\u7f13\u51b2\u6d41\u6c34\u7ebf\u3002", "result": "H2SGEMM\u5728Ascend 910A NPU\u4e0a\u5b9e\u73b0\u4e8677%\u7684\u7406\u8bbaFP32\u5cf0\u503c\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u548c\u6570\u503c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6062\u590d\u4e86\u539f\u751fFP32 GEMM\u7684\u7cbe\u5ea6\uff0c\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u8fd8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6570\u503c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.23035", "pdf": "https://arxiv.org/pdf/2507.23035", "abs": "https://arxiv.org/abs/2507.23035", "authors": ["Xueying Wu", "Baijun Zhou", "Zhihui Gao", "Yuzhe Fu", "Qilin Zheng", "Yintao He", "Hai Li"], "title": "KLLM: Fast LLM Inference with K-Means Quantization", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Large language model (LLM) inference poses significant challenges due to its\nintensive memory and computation demands. Weight and activation quantization\n(WAQ) offers a promising solution by reducing both memory footprint and\narithmetic complexity. However, two key challenges remain in the existing WAQ\ndesigns. (1) Traditional WAQ designs rely on uniform integer-based quantization\nfor hardware efficiency, but this often results in significant accuracy\ndegradation at low precision. K-Means-based quantization, a non-uniform\nquantization technique, achieves higher accuracy by matching the Gaussian-like\ndistributions of weights and activations in LLMs. However, its non-uniform\nnature prevents direct execution on low-precision compute units, requiring\ndequantization and floating-point matrix multiplications (MatMuls) during\ninference. (2) Activation outliers further hinder effective low-precision WAQ.\nOffline thresholding methods for outlier detection can lead to significant\nmodel performance degradation, while existing online detection techniques\nintroduce substantial runtime overhead.\n  To address the aforementioned challenges and fully unleash the potential of\nWAQ with K-Means quantization for LLM inference, in this paper, we propose\nKLLM, a hardware-software co-design framework. KLLM features an index-based\ncomputation scheme for efficient execution of MatMuls and nonlinear operations\non K-Means-quantized data, which avoids most of the dequantization and\nfull-precision computations. Moreover, KLLM incorporates a novel outlier\ndetection engine, Orizuru, that efficiently identifies the top-$k$ largest and\nsmallest elements in the activation data stream during online inference.\n  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,\n7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the\nA100 GPU and Atom, respectively.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86KLLM\u6846\u67b6\uff0c\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u7684\u91cf\u5316\u6311\u6218\uff0c\u5305\u62ec\u9ad8\u6548\u7684K-Means\u91cf\u5316\u8ba1\u7b97\u548c\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5bf9\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u6781\u9ad8\uff0c\u4f20\u7edf\u91cf\u5316\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u51c6\u786e\u6027\u548c\u786c\u4ef6\u6548\u7387\u3002", "method": "\u63d0\u51faKLLM\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u7d22\u5f15\u7684\u8ba1\u7b97\u65b9\u6848\u964d\u4f4e\u91cf\u5316\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u8bbe\u8ba1\u65b0\u578b\u5728\u7ebf\u5f02\u5e38\u68c0\u6d4b\u5f15\u64ceOrizuru\u3002", "result": "\u5b9e\u9a8c\u663e\u793aKLLM\u5728\u901f\u5ea6\u548c\u80fd\u6548\u4e0a\u663e\u8457\u4f18\u4e8eA100 GPU\u548cAtom\u3002", "conclusion": "KLLM\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23178", "pdf": "https://arxiv.org/pdf/2507.23178", "abs": "https://arxiv.org/abs/2507.23178", "authors": ["Siyuan Liu", "Zhice Yang", "Huangxun Chen"], "title": "AutoBridge: Automating Smart Device Integration with Centralized Platform", "categories": ["cs.SE", "cs.AI", "I.2.5"], "comment": "14 pages, 12 figures, under review", "summary": "Multimodal IoT systems coordinate diverse IoT devices to deliver\nhuman-centered services. The ability to incorporate new IoT devices under the\nmanagement of a centralized platform is an essential requirement. However, it\nrequires significant human expertise and effort to program the complex IoT\nintegration code that enables the platform to understand and control the device\nfunctions. Therefore, we propose AutoBridge to automate IoT integration code\ngeneration. Specifically, AutoBridge adopts a divide-and-conquer strategy: it\nfirst generates device control logic by progressively retrieving\ndevice-specific knowledge, then synthesizes platformcompliant integration code\nusing platform-specific knowledge. To ensure correctness, AutoBridge features a\nmulti-stage debugging pipeline, including an automated debugger for virtual IoT\ndevice testing and an interactive hardware-in-the-loop debugger that requires\nonly binary user feedback (yes and no) for real-device verification. We\nevaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT\nplatforms. The results demonstrate that AutoBridge can achieves an average\nsuccess rate of 93.87% and an average function coverage of 94.87%, without any\nhuman involvement. With minimal binary yes and no feedback from users, the code\nis then revised to reach 100% function coverage. A user study with 15\nparticipants further shows that AutoBridge outperforms expert programmers by\n50% to 80% in code accuracy, even when the programmers are allowed to use\ncommercial code LLMs.", "AI": {"tldr": "AutoBridge\u81ea\u52a8\u5316\u751f\u6210IoT\u96c6\u6210\u4ee3\u7801\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u7b56\u7565\u548c\u591a\u7ea7\u8c03\u8bd5\u7ba1\u9053\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u6210\u529f\u7387\u548c\u529f\u80fd\u8986\u76d6\u7387\u9ad8\u8fbe93.87%\u548c94.87%\uff0c\u7528\u6237\u53cd\u9988\u540e\u53ef\u5b9e\u73b0100%\u8986\u76d6\u7387\u3002", "motivation": "\u4f20\u7edfIoT\u8bbe\u5907\u96c6\u6210\u9700\u5927\u91cf\u4eba\u5de5\u7f16\u7a0b\uff0cAutoBridge\u65e8\u5728\u81ea\u52a8\u5316\u6b64\u8fc7\u7a0b\u4ee5\u51cf\u5c11\u4e13\u5bb6\u4f9d\u8d56\u548c\u63d0\u5347\u6548\u7387\u3002", "method": "\u91c7\u7528\u5206\u6cbb\u7b56\u7565\uff0c\u5206\u9636\u6bb5\u751f\u6210\u8bbe\u5907\u63a7\u5236\u903b\u8f91\u548c\u5e73\u53f0\u5408\u89c4\u4ee3\u7801\uff0c\u5e76\u5f15\u5165\u591a\u7ea7\u8c03\u8bd5\u7ba1\u9053\uff08\u865a\u62df\u6d4b\u8bd5\u548c\u786c\u4ef6\u9a8c\u8bc1\uff09\u3002", "result": "\u572834\u53f0\u8bbe\u5907\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747\u6210\u529f\u7387\u548c\u529f\u80fd\u8986\u76d6\u7387\u8fbe93.87%\u548c94.87%\uff0c\u7528\u6237\u53cd\u9988\u540e\u53ef\u8fbe100%\u3002", "conclusion": "AutoBridge\u663e\u8457\u4f18\u4e8e\u4e13\u5bb6\u7f16\u7a0b\u548c\u5546\u7528LLM\uff0c\u9a8c\u8bc1\u4e86\u81ea\u52a8\u5316IoT\u96c6\u6210\u7684\u53ef\u884c\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.23777", "pdf": "https://arxiv.org/pdf/2507.23777", "abs": "https://arxiv.org/abs/2507.23777", "authors": ["Dian Chen", "Yansong Qu", "Xinyang Li", "Ming Li", "Shengchuan Zhang"], "title": "XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Current auto-regressive models can generate high-quality, topologically\nprecise meshes; however, they necessitate thousands-or even tens of\nthousands-of next-token predictions during inference, resulting in substantial\nlatency. We introduce XSpecMesh, a quality-preserving acceleration method for\nauto-regressive mesh generation models. XSpecMesh employs a lightweight,\nmulti-head speculative decoding scheme to predict multiple tokens in parallel\nwithin a single forward pass, thereby accelerating inference. We further\npropose a verification and resampling strategy: the backbone model verifies\neach predicted token and resamples any tokens that do not meet the quality\ncriteria. In addition, we propose a distillation strategy that trains the\nlightweight decoding heads by distilling from the backbone model, encouraging\ntheir prediction distributions to align and improving the success rate of\nspeculative predictions. Extensive experiments demonstrate that our method\nachieves a 1.7x speedup without sacrificing generation quality. Our code will\nbe released.", "AI": {"tldr": "XSpecMesh\u662f\u4e00\u79cd\u8d28\u91cf\u4fdd\u6301\u7684\u52a0\u901f\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u591a\u5934\u63a8\u6d4b\u89e3\u7801\u548c\u9a8c\u8bc1\u91cd\u91c7\u6837\u7b56\u7565\uff0c\u5b9e\u73b01.7\u500d\u7684\u52a0\u901f\u4e14\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u81ea\u56de\u5f52\u6a21\u578b\u5728\u7f51\u683c\u751f\u6210\u65f6\u63a8\u65ad\u5ef6\u8fdf\u9ad8\uff0c\u9700\u4f18\u5316\u52a0\u901f\u63a8\u65ad\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u591a\u5934\u63a8\u6d4b\u89e3\u7801\u3001\u9a8c\u8bc1\u91cd\u91c7\u6837\u7b56\u7565\u548c\u84b8\u998f\u8bad\u7ec3\u89e3\u7801\u5934\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u5b9e\u73b01.7\u500d\u52a0\u901f\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "XSpecMesh\u9ad8\u6548\u52a0\u901f\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2507.23042", "pdf": "https://arxiv.org/pdf/2507.23042", "abs": "https://arxiv.org/abs/2507.23042", "authors": ["Santosh Patapati", "Trisanth Srinivasan"], "title": "Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO", "I.2.6; I.2.9; I.2.10; C.3.3"], "comment": "6 pages", "summary": "Autonomous vehicles must react in milliseconds while reasoning about road\ngeometry and traffic intent to navigate complex situations. We introduce\nNovaDrive, a single-branch vision-language architecture that processes\nfront-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a\nsingle branch. A lightweight, two-stage cross-attention block first aligns\nwaypoint tokens with the HD map, then refines attention over fine-grained image\nand depth patches. Coupled with a novel smoothness loss that discourages abrupt\nsteering and speed changes, this design eliminates the need for recurrent\nmemory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language\nbackbone, enabling real-time inference. On the nuScenes / Waymo subset of the\nMD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts\npath-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from\n2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations\nconfirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention\nfusion each contribute the most to these gains. Beyond safety, NovaDrive's\nshorter routes (resulting from the novel smoothness loss) translate to lower\nfuel or battery usage, pointing toward leaner, more easily updated driving\nstacks. NovaDrive can be extended to other embodied-AI domains as well.", "AI": {"tldr": "NovaDrive\u662f\u4e00\u4e2a\u5355\u5206\u652f\u7684\u89c6\u89c9\u8bed\u8a00\u67b6\u6784\uff0c\u901a\u8fc7\u5904\u7406\u524d\u6444\u50cf\u5934\u56fe\u50cf\u3001\u9ad8\u6e05\u5730\u56fe\u3001LiDAR\u6df1\u5ea6\u548c\u6587\u672c\u822a\u70b9\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u4ea4\u53c9\u6ce8\u610f\u529b\u5757\u548c\u5e73\u6ed1\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u5728\u6beb\u79d2\u5185\u5e94\u5bf9\u590d\u6742\u4ea4\u901a\u60c5\u51b5\uff0c\u5bf9\u9053\u8def\u51e0\u4f55\u548c\u4ea4\u901a\u610f\u56fe\u8fdb\u884c\u5feb\u901f\u63a8\u7406\uff0cNovaDrive\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u7b80\u5316\u67b6\u6784\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5355\u5206\u652f\u67b6\u6784\uff0c\u7ed3\u5408\u524d\u6444\u50cf\u5934\u56fe\u50cf\u3001\u9ad8\u6e05\u5730\u56fe\u3001LiDAR\u6df1\u5ea6\u548c\u6587\u672c\u822a\u70b9\uff0c\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u4ea4\u53c9\u6ce8\u610f\u529b\u5757\u548c\u5e73\u6ed1\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5bf9\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u90e8\u5206\u5fae\u8c03\u3002", "result": "\u5728nuScenes/Waymo\u5b50\u96c6\u4e0a\uff0cNovaDrive\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u523084%\uff0c\u8def\u5f84\u6548\u7387\uff08SPL\uff09\u63d0\u5347\u81f30.66\uff0c\u78b0\u649e\u9891\u7387\u4ece2.6%\u964d\u81f31.2%\u3002", "conclusion": "NovaDrive\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u4f18\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u5e76\u6709\u671b\u6269\u5c55\u5230\u5176\u4ed6\u5177\u8eabAI\u9886\u57df\u3002"}}
{"id": "2507.23205", "pdf": "https://arxiv.org/pdf/2507.23205", "abs": "https://arxiv.org/abs/2507.23205", "authors": ["Hebi Li", "Forrest Sheng Bao", "Qi Xiao", "Jin Tian"], "title": "Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks", "categories": ["cs.PL", "cs.SE"], "comment": null, "summary": "Foreign Function Interfaces (FFIs) are essential for enabling\ninteroperability between programming languages, yet existing FFI solutions are\nill-suited for the dynamic, interactive workflows prevalent in modern notebook\nenvironments such as Jupyter. Current approaches require extensive manual\nconfiguration, introduce significant boilerplate, and often lack support for\nrecursive calls and object-oriented programming (OOP) constructs-features\ncritical for productive, multi-language development.\n  We present Kernel-FFI, a transparent, language-agnostic framework that\nenables seamless cross-language function calls and object manipulation within\ninteractive notebooks. Kernel-FFI employs source-level transformation to\nautomatically rewrite cross-language invocations, eliminating the need for\nmanual bindings or boilerplate. Kernel-FFI provides robust support for OOP by\nenabling foreign object referencing and automatic resource management across\nlanguage boundaries. Furthermore, to address the blocking nature of Jupyter\nkernels and support recursive and asynchronous foreign calls, we introduce a\nnovel side-channel communication mechanism. Our tool will be open-sourced and\navailable at https://codepod.io/docs/kernel-ffi", "AI": {"tldr": "Kernel-FFI \u662f\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6e90\u7ea7\u8f6c\u6362\u5b9e\u73b0\u8de8\u8bed\u8a00\u65e0\u7f1d\u8c03\u7528\uff0c\u89e3\u51b3\u4e86\u73b0\u6709 FFI \u5728\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u914d\u7f6e\u7e41\u7410\u3001\u7f3a\u4e4f\u9012\u5f52\u548c OOP \u652f\u6301\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709 FFI \u89e3\u51b3\u65b9\u6848\u5728\u52a8\u6001\u4ea4\u4e92\u5f0f\u7b14\u8bb0\u672c\u73af\u5883\u4e2d\uff08\u5982 Jupyter\uff09\u914d\u7f6e\u590d\u6742\u4e14\u4e0d\u652f\u6301\u9012\u5f52\u8c03\u7528\u548c OOP\uff0c\u5f71\u54cd\u4e86\u591a\u8bed\u8a00\u5f00\u53d1\u7684\u6548\u7387\u3002", "method": "Kernel-FFI \u901a\u8fc7\u6e90\u7ea7\u8f6c\u6362\u81ea\u52a8\u91cd\u5199\u8de8\u8bed\u8a00\u8c03\u7528\uff0c\u65e0\u9700\u624b\u52a8\u7ed1\u5b9a\uff0c\u652f\u6301 OOP \u548c\u8de8\u8bed\u8a00\u8d44\u6e90\u7ba1\u7406\uff0c\u5e76\u5f15\u5165\u4fa7\u4fe1\u9053\u901a\u4fe1\u673a\u5236\u652f\u6301\u9012\u5f52\u548c\u5f02\u6b65\u8c03\u7528\u3002", "result": "Kernel-FFI \u5b9e\u73b0\u4e86\u900f\u660e\u5316\u7684\u8de8\u8bed\u8a00\u8c03\u7528\u548c\u5bf9\u8c61\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709 FFI \u7684\u5c40\u9650\u6027\u3002", "conclusion": "Kernel-FFI \u4e3a\u4ea4\u4e92\u5f0f\u7b14\u8bb0\u672c\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8de8\u8bed\u8a00\u5f00\u53d1\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301 OOP \u548c\u5f02\u6b65\u8c03\u7528\uff0c\u5e76\u5c06\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2507.23618", "pdf": "https://arxiv.org/pdf/2507.23618", "abs": "https://arxiv.org/abs/2507.23618", "authors": ["Xinyi Guo", "Geguang Miao", "Shinichi Nishizawa", "Hiromitsu Awano", "Shinji Kimura", "Takashi Sato"], "title": "SOME: Symmetric One-Hot Matching Elector -- A Lightweight Microsecond Decoder for Quantum Error Correction", "categories": ["cs.ET", "quant-ph"], "comment": null, "summary": "Conventional quantum error correction (QEC) decoders such as Minimum-Weight\nPerfect Matching (MWPM) and Union-Find (UF) offer high thresholds and fast\ndecoding, respectively, but both suffer from high topological complexity. In\ncontrast, Ising model-based decoders reduce topological complexity but demand\nconsiderable decoding time. We propose the Symmetric One-Hot Matching Elector\n(SOME), a novel decoder that reformulates the QEC decoding task as a Quadratic\nUnconstrained Binary Optimization (QUBO) problem -- termed the One-Hot QUBO\n(OHQ). Each variable in the QUBO represents whether a given pair of flipped\nsyndromes is matched, while the error probabilities between the pair are\nencoded as interaction coefficients (weight). Constraints ensure that each\nflipped syndrome is matched exactly once. Valid solutions of OHQ correspond to\nself-inverse permutation matrices, characterized by symmetric one-hot encoding.\nTo solve the OHQ efficiently, SOME reformulates the decoding task as the\nconstruction of permutation matrices that minimize the total weight. It\ninitializes each candidate matrix from one of the minimum-weight syndrome\npairs, then iteratively appends additional pairs in ascending order of weight,\nand finally selects the permutation matrix with the lowest total energy. SOME\nachieves up to a 99.9x reduction in variable count and reduces decoding times\nfrom milliseconds to microseconds on a single-threaded commodity CPU. OHQ also\nmaintains performance up to a 10.5% physical error rate, surpassing the highest\nknown threshold of MWPM@.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u91cf\u5b50\u7ea0\u9519\u89e3\u7801\u5668SOME\uff0c\u901a\u8fc7\u5c06\u89e3\u7801\u4efb\u52a1\u8f6c\u5316\u4e3aQUBO\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u53d8\u91cf\u6570\u91cf\u548c\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u89e3\u7801\u5668\uff08\u5982MWPM\u548cUF\uff09\u5b58\u5728\u9ad8\u62d3\u6251\u590d\u6742\u5ea6\u6216\u957f\u89e3\u7801\u65f6\u95f4\u7684\u95ee\u9898\uff0cSOME\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u5c06\u89e3\u7801\u4efb\u52a1\u8f6c\u5316\u4e3aQUBO\u95ee\u9898\uff08OHQ\uff09\uff0c\u901a\u8fc7\u5bf9\u79f0\u72ec\u70ed\u7f16\u7801\u548c\u6700\u5c0f\u603b\u6743\u91cd\u4f18\u5316\u6784\u5efa\u7f6e\u6362\u77e9\u9635\u3002", "result": "SOME\u5c06\u53d8\u91cf\u6570\u91cf\u51cf\u5c1199.9\u500d\uff0c\u89e3\u7801\u65f6\u95f4\u4ece\u6beb\u79d2\u7ea7\u964d\u81f3\u5fae\u79d2\u7ea7\uff0c\u4e14\u6027\u80fd\u5728\u9ad8\u8bef\u7801\u7387\u4e0b\u4ecd\u4f18\u4e8eMWPM\u3002", "conclusion": "SOME\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u91cf\u5b50\u7ea0\u9519\u89e3\u7801\u5668\uff0c\u9002\u7528\u4e8e\u672a\u6765\u91cf\u5b50\u8ba1\u7b97\u7cfb\u7edf\u3002"}}
{"id": "2507.23177", "pdf": "https://arxiv.org/pdf/2507.23177", "abs": "https://arxiv.org/abs/2507.23177", "authors": ["Neagin Neasamoni Santhi", "Davide Villa", "Michele Polese", "Tommaso Melodia"], "title": "InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps", "categories": ["cs.NI"], "comment": "10 pages, 12 figures, 3 tables", "summary": "Ultra-dense fifth generation (5G) and beyond networks leverage spectrum\nsharing and frequency reuse to enhance throughput, but face unpredictable\nin-band uplink (UL) interference challenges that significantly degrade Signal\nto Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases\n(gNBs). This is particularly problematic at cell edges, where overlapping\nregions force User Equipments (UEs) to increase transmit power, and in\ndirectional millimeter wave systems, where beamforming sidelobes can create\nunexpected interference. The resulting signal degradation disrupts protocol\noperations, including scheduling and resource allocation, by distorting quality\nindicators like Reference Signal Received Power (RSRP) and Received Signal\nStrength Indicator (RSSI), and can compromise critical functions such as\nchannel state reporting and Hybrid Automatic Repeat Request (HARQ)\nacknowledgments. To address this problem, this article introduces InterfO-RAN,\na real-time programmable solution that leverages a Convolutional Neural Network\n(CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical\nlayer, detecting in-band interference with accuracy exceeding 91% in under 650\nus. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics\nProcessing Unit (GPU), coexisting with the 5G NR physical layer processing of\nNVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial\nRadio Units (RUs) and smartphones, our solution was trained and tested on more\nthan 7 million NR UL slots collected from real-world environments,\ndemonstrating robust interference detection capabilities essential for\nmaintaining network performance in dense deployments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInterfO-RAN\u7684\u5b9e\u65f6\u53ef\u7f16\u7a0b\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u7528CNN\u5904\u7406I/Q\u6837\u672c\uff0c\u80fd\u5728650\u5fae\u79d2\u5185\u4ee5\u8d85\u8fc791%\u7684\u51c6\u786e\u7387\u68c0\u6d4b5G\u7f51\u7edc\u4e2d\u7684\u5e26\u5185\u5e72\u6270\u3002", "motivation": "5G\u53ca\u8d855G\u8d85\u5bc6\u96c6\u7f51\u7edc\u901a\u8fc7\u9891\u8c31\u5171\u4eab\u548c\u9891\u7387\u91cd\u7528\u6765\u63d0\u5347\u541e\u5410\u91cf\uff0c\u4f46\u9762\u4e34\u4e0d\u53ef\u9884\u6d4b\u7684\u5e26\u5185\u4e0a\u884c\u5e72\u6270\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u4fe1\u53f7\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u533a\u8fb9\u7f18\u548c\u6beb\u7c73\u6ce2\u7cfb\u7edf\u4e2d\u3002", "method": "\u63d0\u51faInterfO-RAN\u65b9\u6848\uff0c\u5229\u7528CNN\u5904\u7406gNB\u7269\u7406\u5c42\u7684I/Q\u6837\u672c\uff0c\u5e76\u4f7f\u7528GPU\u52a0\u901f\uff0c\u6210\u4e3a\u9996\u4e2aO-RAN dApp\u3002", "result": "\u5728\u8d85\u8fc7700\u4e07\u4e2aNR UL\u65f6\u9699\u7684\u771f\u5b9e\u6570\u636e\u4e2d\u6d4b\u8bd5\uff0cInterfO-RAN\u80fd\u5728650\u5fae\u79d2\u5185\u4ee5\u8d85\u8fc791%\u7684\u51c6\u786e\u7387\u68c0\u6d4b\u5e72\u6270\u3002", "conclusion": "InterfO-RAN\u4e3a\u5bc6\u96c6\u90e8\u7f72\u76845G\u7f51\u7edc\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5e72\u6270\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002"}}
{"id": "2507.23499", "pdf": "https://arxiv.org/pdf/2507.23499", "abs": "https://arxiv.org/abs/2507.23499", "authors": ["Piotr Sowinski", "Kacper Grzymkowski", "Anastasiya Danilenka"], "title": "Jelly-Patch: a Fast Format for Recording Changes in RDF Datasets", "categories": ["cs.DB"], "comment": null, "summary": "Recording data changes in RDF systems is a crucial capability, needed to\nsupport auditing, incremental backups, database replication, and event-driven\nworkflows. In large-scale and low-latency RDF applications, the high volume and\nfrequency of updates can cause performance bottlenecks in the serialization and\ntransmission of changes. To alleviate this, we propose Jelly-Patch -- a\nhigh-performance, compressed binary serialization format for changes in RDF\ndatasets. To evaluate its performance, we benchmark Jelly-Patch against\nexisting RDF Patch formats, using two datasets representing different use cases\n(change data capture and IoT streams). Jelly-Patch is shown to achieve\n3.5--8.9x better compression, and up to 2.5x and 4.6x higher throughput in\nserialization and parsing, respectively. These significant advancements in\nthroughput and compression are expected to improve the performance of\nlarge-scale and low-latency RDF systems.", "AI": {"tldr": "Jelly-Patch \u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u7684\u538b\u7f29\u4e8c\u8fdb\u5236\u5e8f\u5217\u5316\u683c\u5f0f\uff0c\u7528\u4e8e RDF \u6570\u636e\u96c6\u7684\u53d8\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u7387\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u5728\u5927\u578b\u548c\u4f4e\u5ef6\u8fdf\u7684 RDF \u5e94\u7528\u4e2d\uff0c\u9ad8\u9891\u7387\u548c\u5927\u89c4\u6a21\u7684\u66f4\u65b0\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5e8f\u5217\u5316\u548c\u4f20\u8f93\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa Jelly-Patch \u683c\u5f0f\uff0c\u5e76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u5176\u4e0e\u73b0\u6709 RDF Patch \u683c\u5f0f\u5728\u538b\u7f29\u7387\u548c\u541e\u5410\u91cf\u4e0a\u7684\u8868\u73b0\u3002", "result": "Jelly-Patch \u5728\u538b\u7f29\u7387\u4e0a\u63d0\u9ad8\u4e86 3.5--8.9 \u500d\uff0c\u5e8f\u5217\u5316\u548c\u89e3\u6790\u541e\u5410\u91cf\u5206\u522b\u6700\u9ad8\u63d0\u9ad8\u4e86 2.5 \u500d\u548c 4.6 \u500d\u3002", "conclusion": "Jelly-Patch \u663e\u8457\u63d0\u5347\u4e86 RDF \u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u548c\u4f4e\u5ef6\u8fdf\u5e94\u7528\u3002"}}
{"id": "2507.23431", "pdf": "https://arxiv.org/pdf/2507.23431", "abs": "https://arxiv.org/abs/2507.23431", "authors": ["Trever Schirmer", "David Bermbach"], "title": "Towards a Testbed for Scalable FaaS Platforms", "categories": ["cs.DC"], "comment": "Accepted for Publication at the 13th IEEE International Conference on\n  Cloud Engineering (IC2E 2025)", "summary": "Most cloud platforms have a Function-as-a-Service (FaaS) offering that\nenables users to easily write highly scalable applications. To better\nunderstand how the platform's architecture impacts its performance, we present\na research-focused testbed that can be adapted to quickly evaluate the impact\nof different architectures and technologies on the characteristics of\nscalability-focused FaaS platforms.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u5feb\u901f\u8bc4\u4f30\u4e0d\u540c\u67b6\u6784\u548c\u6280\u672f\u5bf9FaaS\u5e73\u53f0\u6027\u80fd\u5f71\u54cd\u7684\u6d4b\u8bd5\u6846\u67b6\u3002", "motivation": "\u4e86\u89e3\u4e91\u5e73\u53f0\u67b6\u6784\u5982\u4f55\u5f71\u54cd\u6027\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9FaaS\u5e73\u53f0\u7684\u6269\u5c55\u6027\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u7814\u7a76\u5bfc\u5411\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u67b6\u6784\u548c\u6280\u672f\u9009\u62e9\u3002", "result": "\u6d4b\u8bd5\u6846\u67b6\u80fd\u591f\u5feb\u901f\u8bc4\u4f30FaaS\u5e73\u53f0\u7684\u6027\u80fd\u548c\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76FaaS\u5e73\u53f0\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2507.23398", "pdf": "https://arxiv.org/pdf/2507.23398", "abs": "https://arxiv.org/abs/2507.23398", "authors": ["Oliver Bause", "Julia Werner", "Paul Palomero Bernardo", "Oliver Bringmann"], "title": "Smart Video Capsule Endoscopy: Raw Image-Based Localization for Enhanced GI Tract Investigation", "categories": ["eess.IV", "cs.AR", "cs.CV"], "comment": "Accepted at the 32nd International Conference on Neural Information\n  Processing - ICONIP 2025", "summary": "For many real-world applications involving low-power sensor edge devices deep\nneural networks used for image classification might not be suitable. This is\ndue to their typically large model size and require- ment of operations often\nexceeding the capabilities of such resource lim- ited devices. Furthermore,\ncamera sensors usually capture images with a Bayer color filter applied, which\nare subsequently converted to RGB images that are commonly used for neural\nnetwork training. However, on resource-constrained devices, such conversions\ndemands their share of energy and optimally should be skipped if possible. This\nwork ad- dresses the need for hardware-suitable AI targeting sensor edge\ndevices by means of the Video Capsule Endoscopy, an important medical proce-\ndure for the investigation of the small intestine, which is strongly limited by\nits battery lifetime. Accurate organ classification is performed with a final\naccuracy of 93.06% evaluated directly on Bayer images involv- ing a CNN with\nonly 63,000 parameters and time-series analysis in the form of Viterbi\ndecoding. Finally, the process of capturing images with a camera and raw image\nprocessing is demonstrated with a customized PULPissimo System-on-Chip with a\nRISC-V core and an ultra-low power hardware accelerator providing an\nenergy-efficient AI-based image clas- sification approach requiring just 5.31\n{\\mu}J per image. As a result, it is possible to save an average of 89.9% of\nenergy before entering the small intestine compared to classic video capsules.", "AI": {"tldr": "\u9488\u5bf9\u8fb9\u7f18\u8bbe\u5907\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684CNN\u6a21\u578b\uff0c\u76f4\u63a5\u5728Bayer\u56fe\u50cf\u4e0a\u5206\u7c7b\uff0c\u7701\u53bb\u4e86RGB\u8f6c\u6362\u6b65\u9aa4\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "\u9488\u5bf9\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\uff0c\u4f20\u7edf\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fc7\u5927\u4e14\u80fd\u8017\u9ad8\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u5728Bayer\u56fe\u50cf\u4e0a\u5206\u7c7b\u7684\u65b9\u6848\u4ee5\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\u3002", "method": "\u91c7\u7528\u4ec5\u542b63,000\u53c2\u6570\u7684CNN\u6a21\u578b\uff0c\u7ed3\u5408Viterbi\u89e3\u7801\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u76f4\u63a5\u5728Bayer\u56fe\u50cf\u4e0a\u8fdb\u884c\u5668\u5b98\u5206\u7c7b\u3002", "result": "\u5b9e\u73b0\u4e8693.06%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6bcf\u5f20\u56fe\u50cf\u4ec5\u6d88\u80175.31\u03bcJ\u80fd\u91cf\uff0c\u8282\u7701\u4e8689.9%\u7684\u80fd\u8017\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u80f6\u56ca\u5185\u7aa5\u955c\u7684\u7535\u6c60\u5bff\u547d\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684AI\u5206\u7c7b\u65b9\u6848\u3002"}}
{"id": "2507.23269", "pdf": "https://arxiv.org/pdf/2507.23269", "abs": "https://arxiv.org/abs/2507.23269", "authors": ["Peter Fettke", "Fabiana Fournier", "Lior Limonad", "Andreas Metzger", "Stefanie Rinderle-Ma", "Barbara Weber"], "title": "XABPs: Towards eXplainable Autonomous Business Processes", "categories": ["cs.SE", "cs.AI", "cs.MA"], "comment": null, "summary": "Autonomous business processes (ABPs), i.e., self-executing workflows\nleveraging AI/ML, have the potential to improve operational efficiency, reduce\nerrors, lower costs, improve response times, and free human workers for more\nstrategic and creative work. However, ABPs may raise specific concerns\nincluding decreased stakeholder trust, difficulties in debugging, hindered\naccountability, risk of bias, and issues with regulatory compliance. We argue\nfor eXplainable ABPs (XABPs) to address these concerns by enabling systems to\narticulate their rationale. The paper outlines a systematic approach to XABPs,\ncharacterizing their forms, structuring explainability, and identifying key BPM\nresearch challenges towards XABPs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u81ea\u4e3b\u4e1a\u52a1\u6d41\u7a0b\uff08ABPs\uff09\u7684\u4f18\u70b9\u4e0e\u6f5c\u5728\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u89e3\u91ca\u7684\u81ea\u4e3b\u4e1a\u52a1\u6d41\u7a0b\uff08XABPs\uff09\u89e3\u51b3\u65b9\u6848\u53ca\u5176\u7814\u7a76\u65b9\u6cd5\u3002", "motivation": "ABPs\u867d\u80fd\u63d0\u5347\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\uff0c\u4f46\u4e5f\u5f15\u53d1\u4e86\u4fe1\u4efb\u3001\u8c03\u8bd5\u3001\u8d23\u4efb\u3001\u504f\u89c1\u548c\u5408\u89c4\u7b49\u95ee\u9898\uff0c\u9700\u8981\u53ef\u89e3\u91ca\u6027\u65b9\u6848\u6765\u5e94\u5bf9\u3002", "method": "\u63d0\u51faXABPs\u6846\u67b6\uff0c\u5305\u62ec\u5f62\u5f0f\u5206\u7c7b\u3001\u53ef\u89e3\u91ca\u6027\u7ed3\u6784\u8bbe\u8ba1\uff0c\u5e76\u8bc6\u522b\u5173\u952eBPM\u7814\u7a76\u6311\u6218\u3002", "result": "\u4e3aXABPs\u7684\u7cfb\u7edf\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "XABPs\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u8bbe\u8ba1\u89e3\u51b3\u4e86ABPs\u7684\u6f5c\u5728\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.23454", "pdf": "https://arxiv.org/pdf/2507.23454", "abs": "https://arxiv.org/abs/2507.23454", "authors": ["Marta Bie\u0144kiewicz", "Julia Ayache", "Panayiotis Charalambous", "Cristina Becchio", "Marco Corragio", "Bertram Taetz", "Francesco De Lellis", "Antonio Grotta", "Anna Server", "Daniel Rammer", "Richard Kulpa", "Franck Multon", "Azucena Garcia-Palacios", "Jessica Sutherland", "Kathleen Bryson", "St\u00e9phane Donikian", "Didier Stricker", "Beno\u00eet Bardy"], "title": "Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary", "categories": ["cs.HC", "cs.CY", "cs.ET", "cs.GR", "q-bio.NC", "I.3.0; I.2; J.4; K.4"], "comment": "pre-print", "summary": "This article explores a critical gap in Mixed Reality (MR) technology: while\nadvances have been made, MR still struggles to authentically replicate human\nembodiment and socio-motor interaction. For MR to enable truly meaningful\nsocial experiences, it needs to incorporate multi-modal data streams and\nmulti-agent interaction capabilities. To address this challenge, we present a\ncomprehensive glossary covering key topics such as Virtual Characters and\nAutonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges\nof Social MR within Neuroscience, Embodiment, and Technology. Our aim is to\ndrive the transformative evolution of MR technologies that prioritize\nhuman-centric innovation, fostering richer digital connections. We advocate for\nMR systems that enhance social interaction and collaboration between humans and\nvirtual autonomous agents, ensuring inclusivity, ethical design and\npsychological safety in the process.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6df7\u5408\u73b0\u5b9e\uff08MR\uff09\u6280\u672f\u5728\u771f\u5b9e\u590d\u5236\u4eba\u7c7b\u4f53\u73b0\u548c\u793e\u4f1a\u8fd0\u52a8\u4e92\u52a8\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u6570\u636e\u6d41\u548c\u591a\u4ee3\u7406\u4e92\u52a8\u80fd\u529b\u662f\u63d0\u5347MR\u793e\u4ea4\u4f53\u9a8c\u7684\u5173\u952e\u3002", "motivation": "\u73b0\u6709MR\u6280\u672f\u5728\u4eba\u7c7b\u4f53\u73b0\u548c\u793e\u4f1a\u4e92\u52a8\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u80fd\u5b9e\u73b0\u771f\u6b63\u7684\u793e\u4ea4\u4f53\u9a8c\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u591a\u6a21\u6001\u548c\u591a\u4ee3\u7406\u6280\u672f\u63a8\u52a8MR\u7684\u53d8\u9769\u3002", "method": "\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u672f\u8bed\u8868\uff0c\u6db5\u76d6\u865a\u62df\u89d2\u8272\u3001\u81ea\u4e3b\u5316\u3001\u8d23\u4efbAI\u3001\u8bbe\u8ba1\u4f26\u7406\u4ee5\u53ca\u795e\u7ecf\u79d1\u5b66\u3001\u4f53\u73b0\u548c\u6280\u672f\u4e2d\u7684\u793e\u4f1aMR\u79d1\u5b66\u6311\u6218\u3002", "result": "\u63a8\u52a8\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684MR\u6280\u672f\u521b\u65b0\uff0c\u589e\u5f3a\u4eba\u7c7b\u4e0e\u865a\u62df\u81ea\u4e3b\u4ee3\u7406\u4e4b\u95f4\u7684\u793e\u4ea4\u4e92\u52a8\u4e0e\u534f\u4f5c\uff0c\u786e\u4fdd\u5305\u5bb9\u6027\u3001\u4f26\u7406\u8bbe\u8ba1\u548c\u5fc3\u7406\u5b89\u5168\u3002", "conclusion": "\u63d0\u51faMR\u6280\u672f\u9700\u4f18\u5148\u8003\u8651\u4eba\u7c7b\u4e2d\u5fc3\u521b\u65b0\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u4e30\u5bcc\u7684\u6570\u5b57\u8fde\u63a5\u548c\u793e\u4ea4\u4e92\u52a8\u3002"}}
{"id": "2507.23779", "pdf": "https://arxiv.org/pdf/2507.23779", "abs": "https://arxiv.org/abs/2507.23779", "authors": ["Miaosen Zhang", "Ziqiang Xu", "Jialiang Zhu", "Qi Dai", "Kai Qiu", "Yifan Yang", "Chong Luo", "Tianyi Chen", "Justin Wagle", "Tim Franklin", "Baining Guo"], "title": "Phi-Ground Tech Report: Advancing Perception in GUI Grounding", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "With the development of multimodal reasoning models, Computer Use Agents\n(CUAs), akin to Jarvis from \\textit{\"Iron Man\"}, are becoming a reality. GUI\ngrounding is a core component for CUAs to execute actual actions, similar to\nmechanical control in robotics, and it directly leads to the success or failure\nof the system. It determines actions such as clicking and typing, as well as\nrelated parameters like the coordinates for clicks. Current end-to-end\ngrounding models still achieve less than 65\\% accuracy on challenging\nbenchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from\nbeing ready for deployment. % , as a single misclick can result in unacceptable\nconsequences. In this work, we conduct an empirical study on the training of\ngrounding models, examining details from data collection to model training.\nUltimately, we developed the \\textbf{Phi-Ground} model family, which achieves\nstate-of-the-art performance across all five grounding benchmarks for models\nunder $10B$ parameters in agent settings. In the end-to-end model setting, our\nmodel still achieves SOTA results with scores of \\textit{\\textbf{43.2}} on\nScreenSpot-pro and \\textit{\\textbf{27.2}} on UI-Vision. We believe that the\nvarious details discussed in this paper, along with our successes and failures,\nnot only clarify the construction of grounding models but also benefit other\nperception tasks. Project homepage:\n\\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86GUI grounding\u6280\u672f\uff0c\u5f00\u53d1\u4e86Phi-Ground\u6a21\u578b\u5bb6\u65cf\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u7684\u53d1\u5c55\u4f7f\u5f97\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUAs\uff09\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u73b0\u6709\u7aef\u5230\u7aefgrounding\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e0d\u8db365%\uff0c\u5f71\u54cd\u4e86\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u5bf9\u4ece\u6570\u636e\u6536\u96c6\u5230\u6a21\u578b\u8bad\u7ec3\u7684\u7ec6\u8282\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5f00\u53d1\u4e86Phi-Ground\u6a21\u578b\u5bb6\u65cf\u3002", "result": "Phi-Ground\u57285\u4e2agrounding\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5728\u7aef\u5230\u7aef\u8bbe\u7f6e\u4e0b\u5206\u522b\u5728ScreenSpot-pro\u548cUI-Vision\u4e0a\u53d6\u5f9743.2\u548c27.2\u7684\u5206\u6570\u3002", "conclusion": "\u7814\u7a76\u4e2d\u8ba8\u8bba\u7684\u7ec6\u8282\u548c\u6210\u529f\u7ecf\u9a8c\u4e0d\u4ec5\u9002\u7528\u4e8egrounding\u6a21\u578b\u6784\u5efa\uff0c\u4e5f\u5bf9\u5176\u4ed6\u611f\u77e5\u4efb\u52a1\u6709\u5e2e\u52a9\u3002"}}
{"id": "2507.23186", "pdf": "https://arxiv.org/pdf/2507.23186", "abs": "https://arxiv.org/abs/2507.23186", "authors": ["Peter Sharpe"], "title": "NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions", "categories": ["cs.LG", "cs.PL"], "comment": null, "summary": "Sparsity detection in black-box functions enables significant computational\nspeedups in gradient-based optimization through Jacobian compression, but\nexisting finite-difference methods suffer from false negatives due to\ncoincidental zero gradients. These false negatives can silently corrupt\ngradient calculations, leading to difficult-to-diagnose errors. We introduce\nNaN-propagation, which exploits the universal contamination property of IEEE\n754 Not-a-Number floating-point values to trace input-output dependencies\nthrough floating-point numerical computations. By systematically contaminating\ninputs with NaN and observing which outputs become NaN, the method reconstructs\nconservative sparsity patterns that eliminate false negatives. We demonstrate\nthe approach on an aerospace wing weight model, achieving a 1.52x speedup while\ndetecting dozens of dependencies missed by conventional methods -- a\nsignificant improvement since gradient computation is the bottleneck in many\noptimization workflows. The technique leverages IEEE 754 compliance to work\nacross programming languages and math libraries without modifying existing\nblack-box codes. Advanced strategies including NaN payload encoding enable\nfaster-than-linear time complexity, improving upon existing black-box sparsity\ndetection methods. Practical algorithms are also proposed to mitigate\nchallenges from branching code execution common in engineering applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528IEEE 754 NaN\u503c\u4f20\u64ad\u7279\u6027\u68c0\u6d4b\u9ed1\u7bb1\u51fd\u6570\u7a00\u758f\u6027\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6709\u9650\u5dee\u5206\u6cd5\u56e0\u68af\u5ea6\u4e3a\u96f6\u5bfc\u81f4\u7684\u5047\u9634\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68af\u5ea6\u8ba1\u7b97\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6709\u9650\u5dee\u5206\u6cd5\u5728\u7a00\u758f\u6027\u68c0\u6d4b\u4e2d\u56e0\u68af\u5ea6\u4e3a\u96f6\u4ea7\u751f\u5047\u9634\u6027\uff0c\u5bfc\u81f4\u68af\u5ea6\u8ba1\u7b97\u9519\u8bef\u4e14\u96be\u4ee5\u8bca\u65ad\u3002\u901a\u8fc7\u5229\u7528IEEE 754 NaN\u7684\u4f20\u64ad\u7279\u6027\uff0c\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u68c0\u6d4b\u8f93\u5165\u8f93\u51fa\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u91c7\u7528NaN\u4f20\u64ad\u6280\u672f\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u5c06\u8f93\u5165\u6807\u8bb0\u4e3aNaN\u5e76\u89c2\u5bdf\u8f93\u51fa\u53d8\u5316\u6765\u91cd\u5efa\u7a00\u758f\u6027\u6a21\u5f0f\uff0c\u6d88\u9664\u4e86\u5047\u9634\u6027\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u517c\u5bb9IEEE 754\u6807\u51c6\uff0c\u65e0\u9700\u4fee\u6539\u9ed1\u7bb1\u4ee3\u7801\u3002", "result": "\u5728\u822a\u7a7a\u822a\u5929\u7ffc\u91cd\u6a21\u578b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e861.52\u500d\u7684\u52a0\u901f\uff0c\u5e76\u68c0\u6d4b\u5230\u4f20\u7edf\u65b9\u6cd5\u9057\u6f0f\u7684\u591a\u4e2a\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6548\u7387\u3002", "conclusion": "NaN\u4f20\u64ad\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8de8\u8bed\u8a00\u7684\u7a00\u758f\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u5de5\u7a0b\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2507.22972", "pdf": "https://arxiv.org/pdf/2507.22972", "abs": "https://arxiv.org/abs/2507.22972", "authors": ["Nikita A. Nemkov", "Stanislav S. Straupe"], "title": "Complexity-energy trade-off in programmable unitary interferometers", "categories": ["physics.optics", "cs.ET"], "comment": "6 pages", "summary": "Coherent multiport interferometers are a promising approach to realize matrix\nmultiplication in integrated photonics. However, most known architectures -\nsuch as MZI and beamsplitter meshes, as well as more general interferometers -\nsuffer from complicated procedures for mapping the matrix elements of the\ndesired transformation to specific phaseshifts in the device. We point out that\nthe high programming complexity is intrinsic, rather than accidental. At the\nsame time, we argue that interferometers admitting efficient programming\nalgorithms in general yield a much lower useful output energy, which ultimately\nlimits their accuracy and energy efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\u5f53\u524d\u591a\u7aef\u53e3\u5e72\u6d89\u4eea\u5728\u77e9\u9635\u4e58\u6cd5\u5b9e\u73b0\u4e2d\u7684\u7f16\u7a0b\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u8ba4\u4e3a\u8fd9\u79cd\u590d\u6742\u6027\u662f\u56fa\u6709\u800c\u975e\u5076\u7136\u7684\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5728\u96c6\u6210\u5149\u5b50\u5b66\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u540c\u65f6\u5206\u6790\u73b0\u6709\u5e72\u6d89\u4eea\u67b6\u6784\u7684\u7f16\u7a0b\u590d\u6742\u6027\u548c\u80fd\u91cf\u6548\u7387\u9650\u5236\u3002", "method": "\u8bba\u6587\u5206\u6790\u4e86\u73b0\u6709\u5e72\u6d89\u4eea\u67b6\u6784\uff08\u5982MZI\u548c\u5206\u675f\u5668\u7f51\u683c\uff09\u7684\u7f16\u7a0b\u590d\u6742\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u4e8e\u9ad8\u6548\u7f16\u7a0b\u7b97\u6cd5\u7684\u56fa\u6709\u5c40\u9650\u6027\u7684\u8bba\u70b9\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u6548\u7f16\u7a0b\u7b97\u6cd5\u901a\u5e38\u4f1a\u5bfc\u81f4\u8f83\u4f4e\u7684\u6709\u7528\u8f93\u51fa\u80fd\u91cf\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5e72\u6d89\u4eea\u7684\u51c6\u786e\u6027\u548c\u80fd\u91cf\u6548\u7387\u3002", "conclusion": "\u8bba\u6587\u5f97\u51fa\u7ed3\u8bba\uff0c\u5f53\u524d\u5e72\u6d89\u4eea\u7684\u7f16\u7a0b\u590d\u6742\u6027\u662f\u56fa\u6709\u7279\u6027\uff0c\u4e14\u5728\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\u53ef\u80fd\u727a\u7272\u8f93\u51fa\u80fd\u91cf\uff0c\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.23286", "pdf": "https://arxiv.org/pdf/2507.23286", "abs": "https://arxiv.org/abs/2507.23286", "authors": ["Zihong Li", "Anshan Yuan", "Xinghua Sun"], "title": "Optimal Packetization Towards Low Latency in Random Access Networks (extended version)", "categories": ["cs.NI"], "comment": "This article is an extended version of a paper to be presented at the\n  IEEE 25th International Conference on Communication Technology (ICCT),\n  Shenyang, China, October 2025", "summary": "As the demand for low-latency services grows, ensuring the delay performance\nof random access (RA) networks has become a priority. Existing studies on the\nqueueing delay performance of the Aloha model universally treat packets as\natomic transmission units, focusing primarily on delay measured in time slots.\nHowever, the impact of packetization on queueing delay has been consistently\noverlooked, particularly for the mean queueing delay measured in seconds, which\nserves as a more precise and practically relevant performance metric than its\nslot-based counterpart. Here, packetization refers to the process of\ndetermining the number of bits assembled into a packet. To optimize queueing\ndelay from the perspective of packetization, this paper establishes the\nmathematical relationship between packetization and mean queueing delay in\nseconds for both connection-free and connection-based Aloha schemes, and\nexplores the optimal packetization strategy to minimize this delay. We identify\nthe optimal mean queueing delay and its corresponding packet size via numerical\nmethods, and further analyze the influence of various network parameters. We\nfurther use simulations to investigate the similar impact of packetization on\njitter of queueing delay. We then apply our analysis to re-evaluate the complex\ntrade-off between the connection-free and connection-based schemes through the\nnew perspective of packetization. Furthermore, recognizing that an analysis of\nthe queueing delay performance for RA-SDT in NTN scenarios, especially from a\npacketization perspective, also remains an unexplored area, we apply the\nanalysis to this scenario as a case study.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5206\u7ec4\u5bf9\u968f\u673a\u63a5\u5165\u7f51\u7edc\u6392\u961f\u5ef6\u8fdf\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4f18\u5316\u5206\u7ec4\u7b56\u7565\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u7f51\u7edc\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002", "motivation": "\u968f\u7740\u5bf9\u4f4e\u5ef6\u8fdf\u670d\u52a1\u7684\u9700\u6c42\u589e\u957f\uff0c\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u5206\u7ec4\u5bf9\u6392\u961f\u5ef6\u8fdf\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u4ee5\u79d2\u4e3a\u5355\u4f4d\u7684\u5e73\u5747\u6392\u961f\u5ef6\u8fdf\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u548c\u6570\u503c\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u5206\u7ec4\u4e0e\u5e73\u5747\u6392\u961f\u5ef6\u8fdf\u7684\u5173\u7cfb\uff0c\u5e76\u63a2\u8ba8\u4e86\u6700\u4f18\u5206\u7ec4\u7b56\u7565\uff1b\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u4e86\u5206\u7ec4\u5bf9\u5ef6\u8fdf\u6296\u52a8\u7684\u5f71\u54cd\u3002", "result": "\u786e\u5b9a\u4e86\u6700\u4f18\u7684\u5e73\u5747\u6392\u961f\u5ef6\u8fdf\u53ca\u5176\u5bf9\u5e94\u7684\u5206\u7ec4\u5927\u5c0f\uff0c\u5206\u6790\u4e86\u7f51\u7edc\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eNTN\u573a\u666f\u3002", "conclusion": "\u5206\u7ec4\u5bf9\u6392\u961f\u5ef6\u8fdf\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f18\u5316\u5206\u7ec4\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u5ef6\u8fdf\uff0c\u540c\u65f6\u4e3a\u8fde\u63a5\u81ea\u7531\u548c\u57fa\u4e8e\u8fde\u63a5\u7684\u65b9\u6848\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u89c6\u89d2\u3002"}}
{"id": "2507.23515", "pdf": "https://arxiv.org/pdf/2507.23515", "abs": "https://arxiv.org/abs/2507.23515", "authors": ["Ana\u00efs Ollagnier", "Aline Menin"], "title": "DataLens: Enhancing Dataset Discovery via Network Topologies", "categories": ["cs.DB"], "comment": null, "summary": "The rapid growth of publicly available textual resources, such as lexicons\nand domain-specific corpora, presents challenges in efficiently identifying\nrelevant resources. While repositories are emerging, they often lack advanced\nsearch and exploration features. Most search methods rely on keyword queries\nand metadata filtering, which require prior knowledge and fail to reveal\nconnections between resources. To address this, we present DataLens, a\nweb-based platform that combines faceted search with advanced visualization\ntechniques to enhance resource discovery. DataLens offers network-based\nvisualizations, where the network structure can be adapted to suit the specific\nanalysis task. It also supports a chained views approach, enabling users to\nexplore data from multiple perspectives. A formative user study involving six\ndata practitioners revealed that users highly value visualization\ntools-especially network-based exploration-and offered insights to help refine\nour approach to better support dataset search.", "AI": {"tldr": "DataLens\u662f\u4e00\u4e2a\u57fa\u4e8e\u7f51\u7edc\u7684\u5e73\u53f0\uff0c\u7ed3\u5408\u4e86\u9762\u641c\u7d22\u548c\u9ad8\u7ea7\u53ef\u89c6\u5316\u6280\u672f\uff0c\u65e8\u5728\u63d0\u9ad8\u8d44\u6e90\u53d1\u73b0\u7684\u6548\u7387\u3002", "motivation": "\u516c\u5f00\u53ef\u7528\u7684\u6587\u672c\u8d44\u6e90\u8fc5\u901f\u589e\u957f\uff0c\u4f46\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u4f9d\u8d56\u5173\u952e\u8bcd\u548c\u5143\u6570\u636e\u8fc7\u6ee4\uff0c\u7f3a\u4e4f\u9ad8\u7ea7\u529f\u80fd\uff0c\u96be\u4ee5\u63ed\u793a\u8d44\u6e90\u95f4\u7684\u8054\u7cfb\u3002", "method": "DataLens\u91c7\u7528\u9762\u641c\u7d22\u548c\u7f51\u7edc\u53ef\u89c6\u5316\u6280\u672f\uff0c\u652f\u6301\u591a\u89c6\u89d2\u6570\u636e\u63a2\u7d22\u548c\u9002\u5e94\u6027\u7f51\u7edc\u7ed3\u6784\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\uff0c\u7528\u6237\u9ad8\u5ea6\u8bc4\u4ef7\u7f51\u7edc\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u6570\u636e\u96c6\u641c\u7d22\u7684\u89c1\u89e3\u3002", "conclusion": "DataLens\u901a\u8fc7\u53ef\u89c6\u5316\u6280\u672f\u6709\u6548\u63d0\u5347\u8d44\u6e90\u53d1\u73b0\u80fd\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u66f4\u597d\u5730\u652f\u6301\u7528\u6237\u9700\u6c42\u3002"}}
{"id": "2507.23533", "pdf": "https://arxiv.org/pdf/2507.23533", "abs": "https://arxiv.org/abs/2507.23533", "authors": ["Flora Angileri", "Andrea Clementi", "Emanuele Natale", "Michele Salvi", "Isabella Ziccardi"], "title": "Threshold-Driven Streaming Graph: Expansion and Rumor Spreading", "categories": ["cs.DC", "math.PR"], "comment": null, "summary": "A randomized distributed algorithm called RAES was introduced in [Becchetti\net al., SODA 2020] to extract a bounded-degree expander from a dense $n$-vertex\nexpander graph $G = (V, E)$. The algorithm relies on a simple threshold-based\nprocedure. A key assumption in [Becchetti et al., SODA 2020] is that the input\ngraph $G$ is static - i.e., both its vertex set $V$ and edge set $E$ remain\nunchanged throughout the process - while the analysis of RAES in dynamic models\nis left as a major open question.\n  In this work, we investigate the behavior of RAES under a dynamic graph model\ninduced by a streaming node-churn process (also known as the sliding window\nmodel), where, at each discrete round, a new node joins the graph and the\noldest node departs. This process yields a bounded-degree dynamic graph\n$\\mathcal{G} =\\{ G_t = (V_t, E_t) : t \\in \\mathbb{N}\\}$ that captures essential\ncharacteristics of peer-to-peer networks -- specifically, node churn and\nthreshold on the number of connections each node can manage. We prove that\nevery snapshot $G_t$ in the dynamic graph sequence has good expansion\nproperties with high probability. Furthermore, we leverage this property to\nestablish a logarithmic upper bound on the completion time of the well-known\nPUSH and PULL rumor spreading protocols over the dynamic graph $\\mathcal{G}$.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u52a8\u6001\u56fe\u6a21\u578b\u4e0bRAES\u7b97\u6cd5\u7684\u884c\u4e3a\uff0c\u8bc1\u660e\u4e86\u5728\u8282\u70b9\u6d41\u5931\u7684\u52a8\u6001\u56fe\u4e2d\uff0c\u6bcf\u4e2a\u5feb\u7167G\u209c\u5177\u6709\u9ad8\u5ea6\u7684\u6269\u5c55\u6027\uff0c\u5e76\u63a8\u5bfc\u51faPUSH\u548cPULL\u8c23\u8a00\u4f20\u64ad\u534f\u8bae\u7684\u5b8c\u6210\u65f6\u95f4\u4e0a\u754c\u3002", "motivation": "\u7814\u7a76RAES\u7b97\u6cd5\u5728\u52a8\u6001\u56fe\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u8282\u70b9\u6d41\u5931\u7684\u6d41\u5f0f\u6a21\u578b\uff0c\u586b\u8865\u4e86\u9759\u6001\u56fe\u5047\u8bbe\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u52a8\u6001\u56fe\u6a21\u578b\uff08\u6ed1\u52a8\u7a97\u53e3\u6a21\u578b\uff09\uff0c\u5206\u6790RAES\u7b97\u6cd5\u5728\u8282\u70b9\u6d41\u5931\u60c5\u51b5\u4e0b\u7684\u6269\u5c55\u6027\u3002", "result": "\u8bc1\u660e\u52a8\u6001\u56fe\u4e2d\u6bcf\u4e2a\u5feb\u7167G\u209c\u5177\u6709\u9ad8\u6982\u7387\u7684\u6269\u5c55\u6027\uff0c\u5e76\u5f97\u51faPUSH/PULL\u534f\u8bae\u7684\u5b8c\u6210\u65f6\u95f4\u4e0a\u754c\u4e3a\u5bf9\u6570\u7ea7\u522b\u3002", "conclusion": "RAES\u7b97\u6cd5\u5728\u52a8\u6001\u56fe\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u52a8\u6001\u7f51\u7edc\u4e2d\u7684\u8c23\u8a00\u4f20\u64ad\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.23562", "pdf": "https://arxiv.org/pdf/2507.23562", "abs": "https://arxiv.org/abs/2507.23562", "authors": ["Sirine Arfa", "Bernhard Vogginger", "Christian Mayr"], "title": "Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform", "categories": ["cs.LG", "cs.AR"], "comment": "8 pages, 5 figures, 3 tables", "summary": "Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power\nconsumption and low-latency inference on neuromorphic hardware for a wide range\nof robotic tasks. In this work, we present an energy-efficient implementation\nof a reinforcement learning (RL) algorithm using quantized SNNs to solve two\nclassical control tasks. The network is trained using the Q-learning algorithm,\nthen fine-tuned and quantized to low-bit (8-bit) precision for embedded\ndeployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative\nadvantage of SpiNNaker2 over conventional computing platforms, we analyze\ninference latency, dynamic power consumption, and energy cost per inference for\nour SNN models, comparing performance against a GTX 1650 GPU baseline. Our\nresults demonstrate SpiNNaker2's strong potential for scalable, low-energy\nneuromorphic computing, achieving up to 32x reduction in energy consumption.\nInference latency remains on par with GPU-based execution, with improvements\nobserved in certain task settings, reinforcing SpiNNaker2's viability for\nreal-time neuromorphic control and making the neuromorphic approach a\ncompelling direction for efficient deep Q-learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5316SNN\u7684\u4f4e\u529f\u8017\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728SpiNNaker2\u82af\u7247\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u63a7\u5236\u4efb\u52a1\uff0c\u76f8\u6bd4GPU\u53ef\u964d\u4f4e32%\u7684\u80fd\u8017\u3002", "motivation": "\u901a\u8fc7SNN\u548cSpiNNaker2\u82af\u7247\u7684\u7ed3\u5408\uff0c\u89e3\u51b3\u4f20\u7edf\u8ba1\u7b97\u5e73\u53f0\u7684\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "method": "\u4f7f\u7528Q-learning\u7b97\u6cd5\u8bad\u7ec3SNN\uff0c\u5e76\u91cf\u5316\u81f38\u4f4d\u7cbe\u5ea6\uff0c\u90e8\u7f72\u5728SpiNNaker2\u82af\u7247\u4e0a\u3002\u4e0eGPU\u57fa\u7ebf\u6bd4\u8f83\u5ef6\u8fdf\u3001\u529f\u8017\u548c\u80fd\u8017\u3002", "result": "SpiNNaker2\u5728\u80fd\u8017\u4e0a\u6bd4GPU\u964d\u4f4e32\u500d\uff0c\u5ef6\u8fdf\u8868\u73b0\u76f8\u5f53\uff0c\u90e8\u5206\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "SpiNNaker2\u5728\u4f4e\u529f\u8017\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u9002\u5408\u5b9e\u65f6\u63a7\u5236\u548c\u9ad8\u6548\u6df1\u5ea6Q\u5b66\u4e60\u3002"}}
{"id": "2507.23348", "pdf": "https://arxiv.org/pdf/2507.23348", "abs": "https://arxiv.org/abs/2507.23348", "authors": ["Han Li", "Yuling Shi", "Shaoxin Lin", "Xiaodong Gu", "Heng Lian", "Xin Wang", "Yantao Jia", "Tao Huang", "Qianxiang Wang"], "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Debate", "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.", "AI": {"tldr": "SWE-Debate\u901a\u8fc7\u7ade\u4e89\u6027\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4ee3\u7801\u5e93\u4e2d\u5168\u5c40\u95ee\u9898\u6a21\u5f0f\u8bc6\u522b\u7684\u4e0d\u8db3\uff0c\u53d6\u5f97\u65b0SOTA\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u72ec\u7acb\u63a2\u7d22\u65b9\u6cd5\u6613\u9677\u5165\u5c40\u90e8\u89e3\uff0c\u96be\u4ee5\u8bc6\u522b\u8de8\u4ee3\u7801\u5e93\u7684\u95ee\u9898\u6a21\u5f0f\uff0c\u56e0\u6b64\u63d0\u51faSWE-Debate\u3002", "method": "\u901a\u8fc7\u4ee3\u7801\u4f9d\u8d56\u56fe\u751f\u6210\u591a\u6545\u969c\u4f20\u64ad\u8def\u5f84\uff0c\u7ec4\u7ec7\u4e09\u8f6e\u8fa9\u8bba\uff0c\u6574\u5408\u4e0d\u540c\u63a8\u7406\u89c6\u89d2\uff0c\u6700\u7ec8\u7528MCTS\u751f\u6210\u8865\u4e01\u3002", "result": "\u5728SWE-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSWE-Debate\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u521b\u4e0b\u65b0SOTA\u3002", "conclusion": "SWE-Debate\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u63d0\u5347\u95ee\u9898\u5b9a\u4f4d\u51c6\u786e\u6027\uff0c\u4e3a\u5f00\u6e90\u667a\u80fd\u4f53\u6846\u67b6\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2507.23485", "pdf": "https://arxiv.org/pdf/2507.23485", "abs": "https://arxiv.org/abs/2507.23485", "authors": ["A. Canton", "L. Fernandez-Jambrina", "M. J. Vazquez-Gallo"], "title": "Rational complex Bezier curves", "categories": ["math.NA", "cs.GR", "cs.NA", "65D17, 68U07"], "comment": "9 pages, 6 figures", "summary": "In this paper we develop the formalism of rational complex Bezier curves.\nThis framework is a simple extension of the CAD paradigm, since it describes\narc of curves in terms of control polygons and weights, which are extended to\ncomplex values. One of the major advantages of this extension is that we may\nmake use of two different groups of projective transformations. Besides the\ngroup of projective transformations of the real plane, we have the group of\ncomplex projective transformations. This allows us to apply useful\ntransformations like the geometric inversion to curves in design. In addition\nto this, the use of the complex formulation allows to lower the degree of the\ncurves in some cases. This can be checked using the resultant of two\npolynomials and provides a simple formula for determining whether a rational\ncubic curve is a conic or not. Examples of application of the formalism to\nclassical curves are included.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6709\u7406\u590d\u8d1d\u585e\u5c14\u66f2\u7ebf\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86CAD\u8303\u5f0f\uff0c\u5141\u8bb8\u63a7\u5236\u591a\u8fb9\u5f62\u548c\u6743\u91cd\u53d6\u590d\u6570\u503c\u3002", "motivation": "\u6269\u5c55CAD\u8303\u5f0f\u4ee5\u5f15\u5165\u590d\u6570\u503c\u63a7\u5236\u591a\u8fb9\u5f62\u548c\u6743\u91cd\uff0c\u5229\u7528\u590d\u6570\u6295\u5f71\u53d8\u6362\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u8bbe\u8ba1\u64cd\u4f5c\u3002", "method": "\u901a\u8fc7\u590d\u6570\u503c\u6269\u5c55\u6709\u7406\u8d1d\u585e\u5c14\u66f2\u7ebf\uff0c\u5229\u7528\u5b9e\u6570\u5e73\u9762\u548c\u590d\u6570\u5e73\u9762\u7684\u6295\u5f71\u53d8\u6362\u7fa4\uff0c\u5e76\u5f15\u5165\u591a\u9879\u5f0f\u7684\u5224\u522b\u5f0f\u6765\u964d\u4f4e\u66f2\u7ebf\u6b21\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u7b80\u5316\u67d0\u4e9b\u66f2\u7ebf\u8bbe\u8ba1\uff0c\u4f8b\u5982\u901a\u8fc7\u5224\u522b\u5f0f\u5224\u65ad\u6709\u7406\u4e09\u6b21\u66f2\u7ebf\u662f\u5426\u4e3a\u5706\u9525\u66f2\u7ebf\u3002", "conclusion": "\u590d\u6570\u6846\u67b6\u6269\u5c55\u4e86\u66f2\u7ebf\u8bbe\u8ba1\u7684\u7075\u6d3b\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u51e0\u4f55\u53d8\u6362\u548c\u964d\u9636\u64cd\u4f5c\u3002"}}
{"id": "2507.23292", "pdf": "https://arxiv.org/pdf/2507.23292", "abs": "https://arxiv.org/abs/2507.23292", "authors": ["RJ Skerry-Ryan", "Julian Salazar", "Soroosh Mariooryad", "David Kao", "Daisy Stanton", "Eric Battenberg", "Matt Shannon", "Ron J. Weiss", "Robin Scheibler", "Jonas Rothfuss", "Tom Bagby"], "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy", "categories": ["cs.LG", "cs.CL", "cs.PL", "cs.SE", "eess.AS"], "comment": null, "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u5e8f\u5217\u5efa\u6a21\u7684\u795e\u7ecf\u7f51\u7edc\u5c42API\u548c\u5e93\uff0c\u652f\u6301\u9010\u5c42\u548c\u9010\u6b65\u6267\u884c\uff0c\u5177\u6709\u72b6\u6001\u7ba1\u7406\u548c\u5f3a\u4e00\u81f4\u6027\u4fdd\u969c\u3002", "motivation": "\u65e8\u5728\u7b80\u5316\u5e8f\u5217\u6a21\u578b\u7684\u521b\u5efa\uff0c\u4f7f\u5176\u65e2\u80fd\u9010\u5c42\u6267\u884c\uff08\u5982\u6559\u5e08\u5f3a\u5236\u8bad\u7ec3\uff09\uff0c\u53c8\u80fd\u9010\u6b65\u6267\u884c\uff08\u5982\u81ea\u56de\u5f52\u91c7\u6837\uff09\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u663e\u5f0f\u72b6\u6001\u548c\u9010\u6b65\u6f14\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u4e0e\u65e0\u72b6\u6001\u9010\u5c42\u8c03\u7528\u7ed3\u679c\u4e00\u81f4\uff0c\u63d0\u4f9b\u7ec4\u5408\u5f0f\u548c\u58f0\u660e\u5f0fAPI\u3002", "result": "\u5b9e\u73b0\u4e86\u590d\u6742\u6a21\u578b\u7684\u5373\u65f6\u6d41\u5f0f\u5904\u7406\uff0c\u51cf\u5c11\u4e86\u5e38\u89c1\u9519\u8bef\uff0c\u652f\u6301\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u5e93\u3002", "conclusion": "SequenceLayers\u5e93\u901a\u8fc7\u72b6\u6001\u7ba1\u7406\u548cAPI\u8bbe\u8ba1\uff0c\u7b80\u5316\u4e86\u751f\u4ea7\u7ea7\u5e8f\u5217\u6a21\u578b\u7684\u6784\u5efa\uff0c\u540c\u65f6\u786e\u4fdd\u6b63\u786e\u6027\u3002"}}
{"id": "2507.23342", "pdf": "https://arxiv.org/pdf/2507.23342", "abs": "https://arxiv.org/abs/2507.23342", "authors": ["Laura Acosta Garc\u00eda", "Juan Aznar Poveda", "Fabian Margreiter", "Antonio-Javier Garc\u00eda S\u00e1nchez", "Joan Garc\u00eda Haro", "Thomas Fahringer", "Jos\u00e9 Lorente L\u00f3pez", "Jos\u00e9-V\u00edctor Rodr\u00edguez"], "title": "FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies", "categories": ["cs.NI", "cs.ET"], "comment": null, "summary": "The Internet of Things (IoT) has transformed many industries, and LoRaWAN\n(Long Range Wide Area Network), built on LoRa (Long Range) technology, has\nbecome a crucial solution for enabling scalable, low-cost, and energy-efficient\ncommunication in wide-area networks. Simulation tools are essential for\noptimizing the transmission parameters and, therefore, the energy efficiency\nand performance of LoRaWAN networks. While existing simulation frameworks\naccurately replicate real-world scenarios by including multiple layers of\ncommunication protocols, they often imply significant computational overhead\nand simulation times. To address this issue, this paper introduces FAST-LoRa, a\nnovel simulation framework designed to enable fast and efficient evaluation of\nLoRaWAN networks and selection of transmission parameters. FAST-LoRa\nstreamlines computation by relying on analytical models without complex\npacket-level simulations and implementing gateway reception using efficient\nmatrix operations. Rather than aiming to replace discrete-event simulators,\nFAST-LoRa is intended as a lightweight and accurate approximation tool for\nevaluating transmission parameter strategies in scenarios with stable traffic\npatterns and uplink-focused communications. In our evaluation, we compare\nFAST-LoRa with a well-established simulator using multiple network\nconfigurations with varying numbers of end devices and gateways. The results\nshow that FAST-LoRa achieves similar accuracy in estimating key network\nmetrics, even in complex scenarios with interference and multi-gateway\nreception, with a Mean Absolute Error (MAE) of 0.940 $\\times 10^{-2}$ for the\nPacket Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while\nsignificantly reducing computational time by up to three orders of magnitude.", "AI": {"tldr": "FAST-LoRa \u662f\u4e00\u4e2a\u65b0\u578b\u4eff\u771f\u6846\u67b6\uff0c\u65e8\u5728\u5feb\u901f\u9ad8\u6548\u5730\u8bc4\u4f30 LoRaWAN \u7f51\u7edc\u548c\u9009\u62e9\u4f20\u8f93\u53c2\u6570\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u5de5\u5177\u867d\u80fd\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u573a\u666f\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u548c\u4eff\u771f\u65f6\u95f4\u8f83\u5927\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8fd1\u4f3c\u5de5\u5177\u3002", "method": "FAST-LoRa \u901a\u8fc7\u4f9d\u8d56\u5206\u6790\u6a21\u578b\u800c\u975e\u590d\u6742\u7684\u5305\u7ea7\u4eff\u771f\uff0c\u5e76\u91c7\u7528\u9ad8\u6548\u77e9\u9635\u8fd0\u7b97\u5b9e\u73b0\u7f51\u5173\u63a5\u6536\uff0c\u4f18\u5316\u8ba1\u7b97\u3002", "result": "FAST-LoRa \u5728\u5173\u952e\u7f51\u7edc\u6307\u6807\u4f30\u8ba1\u4e0a\u4e0e\u4f20\u7edf\u4eff\u771f\u5668\u7cbe\u5ea6\u76f8\u8fd1\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "FAST-LoRa \u662f\u9002\u7528\u4e8e\u7a33\u5b9a\u6d41\u91cf\u6a21\u5f0f\u548c\u4e0a\u884c\u901a\u4fe1\u573a\u666f\u7684\u8f7b\u91cf\u7ea7\u51c6\u786e\u8fd1\u4f3c\u5de5\u5177\u3002"}}
{"id": "2507.23358", "pdf": "https://arxiv.org/pdf/2507.23358", "abs": "https://arxiv.org/abs/2507.23358", "authors": ["Renato Vukovic", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Hsien-Chin Lin", "Shutong Feng", "Nurul Lubis", "Milica Gasic"], "title": "Text-to-SQL Task-oriented Dialogue Ontology Construction", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) are widely used as general-purpose knowledge\nsources, but they rely on parametric knowledge, limiting explainability and\ntrustworthiness. In task-oriented dialogue (TOD) systems, this separation is\nexplicit, using an external database structured by an explicit ontology to\nensure explainability and controllability. However, building such ontologies\nrequires manual labels or supervised training. We introduce TeQoDO: a\nText-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM\nautonomously builds a TOD ontology from scratch without supervision using its\ninherent SQL programming capabilities combined with dialogue theory provided in\nthe prompt. We show that TeQoDO outperforms transfer learning approaches, and\nits constructed ontology is competitive on a downstream dialogue state tracking\ntask. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also\nscales to allow construction of much larger ontologies, which we investigate on\na Wikipedia and ArXiv dataset. We view this as a step towards broader\napplication of ontologies to increase LLM explainability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86TeQoDO\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684SQL\u7f16\u7a0b\u80fd\u529b\u548c\u5bf9\u8bdd\u7406\u8bba\uff0c\u65e0\u9700\u76d1\u7763\u5373\u53ef\u6784\u5efa\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\uff08TOD\uff09\u7684\u672c\u4f53\u3002\u8be5\u65b9\u6cd5\u5728\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u672c\u4f53\u6784\u5efa\u3002", "motivation": "\u4f20\u7edf\u7684TOD\u7cfb\u7edf\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u5e93\u548c\u663e\u5f0f\u672c\u4f53\u4ee5\u786e\u4fdd\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4f46\u6784\u5efa\u8fd9\u4e9b\u672c\u4f53\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u6216\u76d1\u7763\u8bad\u7ec3\u3002TeQoDO\u65e8\u5728\u901a\u8fc7LLMs\u81ea\u4e3b\u6784\u5efa\u672c\u4f53\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "method": "TeQoDO\u901a\u8fc7\u7ed3\u5408LLMs\u7684SQL\u7f16\u7a0b\u80fd\u529b\u548c\u5bf9\u8bdd\u7406\u8bba\uff0c\u65e0\u9700\u76d1\u7763\u5373\u53ef\u4ece\u96f6\u5f00\u59cb\u6784\u5efaTOD\u672c\u4f53\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cTeQoDO\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u6784\u5efa\u7684\u672c\u4f53\u5728\u4e0b\u6e38\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\u3002\u6b64\u5916\uff0cTeQoDO\u8fd8\u80fd\u6269\u5c55\u81f3\u5927\u89c4\u6a21\u672c\u4f53\u6784\u5efa\u3002", "conclusion": "TeQoDO\u4e3aLLMs\u7684\u53ef\u89e3\u91ca\u6027\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u672c\u4f53\u5728\u5927\u89c4\u6a21\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.23700", "pdf": "https://arxiv.org/pdf/2507.23700", "abs": "https://arxiv.org/abs/2507.23700", "authors": ["Andrey Prokopenko", "Daniel Arndt", "Damien Lebrun-Grandi\u00e9", "Bruno Turcksin"], "title": "The ArborX library: version 2.0", "categories": ["cs.DC"], "comment": null, "summary": "This paper provides an overview of the 2.0 release of the ArborX library, a\nperformance portable geometric search library based on Kokkos. We describe the\nmajor changes in ArborX 2.0 including a new interface for the library to\nsupport a wider range of user problems, new search data structures (brute\nforce, distributed), support for user functions to be executed on the results\n(callbacks), and an expanded set of the supported algorithms (ray tracing,\nclustering).", "AI": {"tldr": "ArborX 2.0\u7248\u672c\u7684\u6982\u8ff0\uff0c\u4ecb\u7ecd\u4e86\u5176\u65b0\u63a5\u53e3\u3001\u641c\u7d22\u6570\u636e\u7ed3\u6784\u3001\u56de\u8c03\u529f\u80fd\u53ca\u652f\u6301\u7684\u7b97\u6cd5\u6269\u5c55\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u5e93\u7684\u529f\u80fd\u6027\u548c\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e86\u65b0\u63a5\u53e3\u3001\u65b0\u7684\u641c\u7d22\u6570\u636e\u7ed3\u6784\uff08\u5982\u66b4\u529b\u641c\u7d22\u3001\u5206\u5e03\u5f0f\u641c\u7d22\uff09\u3001\u56de\u8c03\u529f\u80fd\u4ee5\u53ca\u6269\u5c55\u7684\u7b97\u6cd5\u652f\u6301\uff08\u5982\u5149\u7ebf\u8ffd\u8e2a\u3001\u805a\u7c7b\uff09\u3002", "result": "ArborX 2.0\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u529f\u80fd\u6027\uff0c\u80fd\u591f\u6ee1\u8db3\u66f4\u591a\u7528\u6237\u9700\u6c42\u3002", "conclusion": "ArborX 2.0\u901a\u8fc7\u591a\u9879\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86\u5176\u9002\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.23356", "pdf": "https://arxiv.org/pdf/2507.23356", "abs": "https://arxiv.org/abs/2507.23356", "authors": ["Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Avi Ziv"], "title": "Quality Evaluation of COBOL to Java Code Transformation", "categories": ["cs.SE", "cs.AI"], "comment": "Submitted to ASE 2025", "summary": "We present an automated evaluation system for assessing COBOL-to-Java code\ntranslation within IBM's watsonx Code Assistant for Z (WCA4Z). The system\naddresses key challenges in evaluating LLM-based translators, including model\nopacity and the complexity of translation quality assessment. Our approach\ncombines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver\nscalable, multi-faceted evaluations. The system supports continuous integration\nworkflows, enables large-scale benchmarking, and reduces reliance on manual\nreview. We describe the system architecture, evaluation strategies, and\nreporting mechanisms that provide actionable insights for developers and\nproject managers, facilitating the evolution of high-quality, modernized\ncodebases.", "AI": {"tldr": "IBM\u5f00\u53d1\u4e86\u4e00\u5957\u81ea\u52a8\u5316\u8bc4\u4f30\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc4\u4f30COBOL\u8f6cJava\u4ee3\u7801\u7684\u8d28\u91cf\uff0c\u7ed3\u5408\u5206\u6790\u68c0\u6d4b\u5668\u548cLLM\u8bc4\u5224\u6280\u672f\uff0c\u652f\u6301\u6301\u7eed\u96c6\u6210\u548c\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3LLM\u4ee3\u7801\u7ffb\u8bd1\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u590d\u6742\u6027\u7b49\u6311\u6218\u3002", "method": "\u7ed3\u5408\u5206\u6790\u68c0\u6d4b\u5668\uff08analytic checkers\uff09\u4e0eLLM\u4f5c\u4e3a\u8bc4\u5224\uff08LaaJ\uff09\u6280\u672f\uff0c\u63d0\u4f9b\u591a\u7ef4\u5ea6\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u6301\u7eed\u96c6\u6210\u3001\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u5ba1\u6838\u7684\u4f9d\u8d56\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u7ba1\u7406\u8005\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u52a9\u4e8e\u63a8\u52a8\u9ad8\u8d28\u91cf\u3001\u73b0\u4ee3\u5316\u4ee3\u7801\u5e93\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.23429", "pdf": "https://arxiv.org/pdf/2507.23429", "abs": "https://arxiv.org/abs/2507.23429", "authors": ["Jorge Ruiz G\u00f3mez", "Lidia Andr\u00e9s Susinos", "Jorge Alamo Oliv\u00e9", "Sonia Rey Osorno", "Manuel Luis Gonzalez Hern\u00e1ndez"], "title": "Chatting with your ERP: A Recipe", "categories": ["cs.AI", "cs.DB", "cs.ET", "cs.HC", "cs.MA", "68T50, 68P20", "I.2.7; H.2.5; H.2.8; H.5.m"], "comment": "11 pages, includes 3 tables summarizing schema and model performance.\n  Submitted on July 31, 2025. Targets integration of LLM agents with ERP\n  systems using open-weight models and Ollama deployment", "summary": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f7f\u7528\u53cc\u4ee3\u7406\u67b6\u6784\u7684LLM\u4ee3\u7406\uff0c\u7528\u4e8e\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3aSQL\u8bed\u53e5\uff0c\u5e76\u8fde\u63a5\u5230ERP\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3aERP\u7cfb\u7edf\u53ef\u6267\u884cSQL\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63a8\u7406\u548c\u6279\u5224\u9636\u6bb5\u7684\u53cc\u4ee3\u7406\u67b6\u6784\uff0c\u5229\u7528\u5f00\u6e90LLM\u3002", "result": "\u63d0\u9ad8\u4e86\u67e5\u8be2\u751f\u6210\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u53cc\u4ee3\u7406\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u5de5\u4e1aERP\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.23421", "pdf": "https://arxiv.org/pdf/2507.23421", "abs": "https://arxiv.org/abs/2507.23421", "authors": ["Sara Cavallero", "Fabio Saggese", "Junya Shiraishi", "Israel Leyva-Mayorga", "Shashi Raj Pandey", "Chiara Buratti", "Petar Popovski"], "title": "Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication", "categories": ["cs.NI"], "comment": "Submitted to IEEE Transactions on Communications, Copyright might be\n  transferred without notice", "summary": "This paper introduces a dual-mode communication framework for wireless\ndevices that integrates query-driven (pull) and event-driven (push)\ntransmissions within a unified time-frame structure. Devices typically respond\nto information requests in pull mode, but if an anomaly is detected, they\npreempt the regular response to report the critical condition. Additionally,\npush-based communication is used to proactively send critical data without\nwaiting for a request. This adaptive approach ensures timely, context-aware,\nand efficient data delivery across different network conditions. To achieve\nhigh energy efficiency, we incorporate a wake-up radio mechanism and we design\na tailored medium access control (MAC) protocol that supports data traffic\nbelonging to the different communication classes. A comprehensive system-level\nanalysis is conducted, accounting for the wake-up control operation and\nevaluating three key performance metrics: the success probability of anomaly\nreports (push traffic), the success probability of query responses (pull\ntraffic) and the total energy consumption. Numerical results characterize the\nsystem's behavior and highlight the inherent trade-off in success probabilities\nbetween push- and pull-based traffic as a function of allocated communication\nresources. Our analysis demonstrates that the proposed approach reduces energy\nconsumption by up to 30% compared to a traditional approach, while maintaining\nreliable support for both communication paradigms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6a21\u901a\u4fe1\u6846\u67b6\uff0c\u7ed3\u5408\u67e5\u8be2\u9a71\u52a8\uff08pull\uff09\u548c\u4e8b\u4ef6\u9a71\u52a8\uff08push\uff09\u4f20\u8f93\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u6548\u3001\u53ca\u65f6\u7684\u6570\u636e\u4f20\u8f93\uff0c\u80fd\u8017\u964d\u4f4e30%\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u7ebf\u8bbe\u5907\u5728\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u4e0b\u6570\u636e\u4ea4\u4ed8\u7684\u6548\u7387\u548c\u80fd\u8017\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u5173\u952e\u4e8b\u4ef6\u7684\u5feb\u901f\u54cd\u5e94\u548c\u67e5\u8be2\u8bf7\u6c42\u7684\u53ef\u9760\u5904\u7406\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u5408\u5524\u9192\u65e0\u7ebf\u7535\u673a\u5236\u548c\u5b9a\u5236MAC\u534f\u8bae\u7684\u53cc\u6a21\u901a\u4fe1\u6846\u67b6\uff0c\u652f\u6301\u4e0d\u540c\u901a\u4fe1\u7c7b\u522b\u7684\u6570\u636e\u6d41\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u7ea7\u5206\u6790\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u7cfb\u7edf\u80fd\u8017\u964d\u4f4e30%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e24\u79cd\u901a\u4fe1\u6a21\u5f0f\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53cc\u6a21\u901a\u4fe1\u6846\u67b6\u5728\u8282\u80fd\u548c\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u7684\u7f51\u7edc\u9700\u6c42\u3002"}}
{"id": "2507.23018", "pdf": "https://arxiv.org/pdf/2507.23018", "abs": "https://arxiv.org/abs/2507.23018", "authors": ["Wesley Brewer", "Patrick Widener", "Valentine Anantharaj", "Feiyi Wang", "Tom Beck", "Arjun Shankar", "Sarp Oral"], "title": "Data Readiness for Scientific AI at Scale", "categories": ["cs.AI", "cs.CE", "cs.DC", "cs.LG", "I.2.6"], "comment": "10 pages, 1 figure, 2 tables", "summary": "This paper examines how Data Readiness for AI (DRAI) principles apply to\nleadership-scale scientific datasets used to train foundation models. We\nanalyze archetypal workflows across four representative domains - climate,\nnuclear fusion, bio/health, and materials - to identify common preprocessing\npatterns and domain-specific constraints. We introduce a two-dimensional\nreadiness framework composed of Data Readiness Levels (raw to AI-ready) and\nData Processing Stages (ingest to shard), both tailored to high performance\ncomputing (HPC) environments. This framework outlines key challenges in\ntransforming scientific data for scalable AI training, emphasizing\ntransformer-based generative models. Together, these dimensions form a\nconceptual maturity matrix that characterizes scientific data readiness and\nguides infrastructure development toward standardized, cross-domain support for\nscalable and reproducible AI for science.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6570\u636e\u51c6\u5907\uff08DRAI\uff09\u539f\u5219\u5728\u9886\u5bfc\u7ea7\u79d1\u5b66\u6570\u636e\u96c6\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u7684\u4e8c\u7ef4\u6210\u719f\u5ea6\u6846\u67b6\uff0c\u4ee5\u6807\u51c6\u5316\u8de8\u9886\u57df\u652f\u6301\u53ef\u6269\u5c55\u7684\u79d1\u5b66AI\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u79d1\u5b66\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u6570\u636e\u51c6\u5907\u7684\u6807\u51c6\u6846\u67b6\uff0c\u89e3\u51b3\u6570\u636e\u9884\u5904\u7406\u548c\u8de8\u9886\u57df\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5206\u6790\u56db\u4e2a\u4ee3\u8868\u6027\u9886\u57df\u7684\u5de5\u4f5c\u6d41\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u6846\u67b6\uff08\u6570\u636e\u51c6\u5907\u7ea7\u522b\u548c\u5904\u7406\u9636\u6bb5\uff09\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u548c\u751f\u6210\u6a21\u578b\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63cf\u8ff0\u79d1\u5b66\u6570\u636e\u51c6\u5907\u72b6\u6001\uff0c\u5e76\u6307\u5bfc\u57fa\u7840\u8bbe\u65bd\u5f00\u53d1\uff0c\u652f\u6301\u53ef\u6269\u5c55\u548c\u53ef\u590d\u73b0\u7684AI\u8bad\u7ec3\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u6846\u67b6\u4e3a\u8de8\u9886\u57df\u79d1\u5b66\u6570\u636e\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u7684\u51c6\u5907\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u79d1\u5b66AI\u7684\u89c4\u6a21\u5316\u5e94\u7528\u3002"}}
{"id": "2507.22889", "pdf": "https://arxiv.org/pdf/2507.22889", "abs": "https://arxiv.org/abs/2507.22889", "authors": ["Tom Sheffer", "Alon Miron", "Yaniv Dover", "Ariel Goldstein"], "title": "Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Conversations transform individual knowledge into collective insight,\nallowing groups of humans and increasingly groups of artificial intelligence\n(AI) agents to collaboratively solve complex problems. Whether interactions\nbetween AI agents can replicate the synergy observed in human discussions\nremains an open question. To investigate this, we systematically compared four\nconversational configurations: pairs of large language models (LLM-LLM), trios\nof LLMs, trios of humans, and mixed human-LLM pairs. After agents answered\nquestions individually, they engaged in open-ended discussions and then\nreconsidered their initial answers. Interactions involving humans consistently\nled to accuracy improvements after the conversations, benefiting both stronger\nand weaker participants. By contrast, purely LLM-based pairs and trios\nexhibited declines in accuracy, demonstrating limited conversational synergy.\nAnalysis of participants' confidence and answer-switching behavior revealed\nthat knowledge diversity is a critical factor enabling collaborative\nimprovement. Crucially, the lack of gains in LLM-LLM interactions did not stem\nfrom a fundamental limitation of the models' ability to collaborate, but from\nhighly similar knowledge states that left little room for productive exchange.\nOur findings argue for a paradigm shift in AI development: rather than\noptimizing individual models solely for standalone performance, explicitly\ncultivating diversity across agents, even at the cost of slightly lower\nindividual accuracy, may yield AI collaborators that are more effective in\ngroup settings with humans or other AI systems.", "AI": {"tldr": "\u7814\u7a76\u4e86AI\u4ee3\u7406\uff08LLMs\uff09\u4e0e\u4eba\u4e4b\u95f4\u5bf9\u8bdd\u914d\u7f6e\u7684\u5408\u4f5c\u6548\u679c\uff0c\u53d1\u73b0\u4eba\u4e4b\u95f4\u7684\u4ea4\u4e92\u63d0\u5347\u4e86\u51c6\u786e\u6027\uff0c\u800c\u7eafLLM\u4ea4\u4e92\u53cd\u800c\u964d\u4f4e\u4e86\u51c6\u786e\u6027\uff0c\u539f\u56e0\u5728\u4e8e\u77e5\u8bc6\u591a\u6837\u6027\u4e0d\u8db3\u3002", "motivation": "\u63a2\u8ba8AI\u4ee3\u7406\u662f\u5426\u80fd\u901a\u8fc7\u5bf9\u8bdd\u4ea7\u751f\u7c7b\u4f3c\u4eba\u7c7b\u7684\u534f\u540c\u6548\u5e94\uff0c\u4f18\u5316AI\u5728\u7fa4\u4f53\u73af\u5883\u4e2d\u7684\u5408\u4f5c\u8868\u73b0\u3002", "method": "\u6bd4\u8f83\u56db\u79cd\u5bf9\u8bdd\u914d\u7f6e\uff08LLM-LLM\u3001LLM\u4e09\u4eba\u7ec4\u3001\u4eba\u7c7b\u4e09\u4eba\u7ec4\u3001\u4eba-LLM\u6df7\u5408\u7ec4\uff09\uff0c\u5206\u6790\u5bf9\u8bdd\u524d\u540e\u7684\u7b54\u6848\u51c6\u786e\u6027\u53d8\u5316\u53ca\u4ea4\u4e92\u884c\u4e3a\u3002", "result": "\u4eba\u7c7b\u4ea4\u4e92\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u7eafLLM\u4ea4\u4e92\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u77e5\u8bc6\u591a\u6837\u6027\u662f\u534f\u540c\u6539\u8fdb\u7684\u5173\u952e\u3002", "conclusion": "\u5efa\u8baeAI\u5f00\u53d1\u4e2d\u6ce8\u91cd\u591a\u6837\u6027\u800c\u975e\u5355\u4e00\u6a21\u578b\u6027\u80fd\uff0c\u4ee5\u63d0\u5347\u7fa4\u4f53\u5408\u4f5c\u6548\u679c\u3002"}}
{"id": "2507.23361", "pdf": "https://arxiv.org/pdf/2507.23361", "abs": "https://arxiv.org/abs/2507.23361", "authors": ["Silin Chen", "Shaoxin Lin", "Xiaodong Gu", "Yuling Shi", "Heng Lian", "Longfei Yun", "Dong Chen", "Weiguo Sun", "Lin Cao", "Qianxiang Wang"], "title": "SWE-Exp: Experience-Driven Software Issue Resolution", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "Our code and data are available at\n  https://github.com/YerbaPage/SWE-Exp", "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.", "AI": {"tldr": "SWE-Exp\u901a\u8fc7\u4ece\u5148\u524d\u7684\u4ee3\u7406\u8f68\u8ff9\u4e2d\u63d0\u53d6\u53ef\u91cd\u590d\u5229\u7528\u7684\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u8de8\u95ee\u9898\u7684\u6301\u7eed\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f6f\u4ef6\u95ee\u9898\u7684\u89e3\u51b3\u7387\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u89e3\u51b3\u8f6f\u4ef6\u95ee\u9898\u65f6\u7f3a\u4e4f\u8bb0\u5fc6\u80fd\u529b\uff0c\u65e0\u6cd5\u590d\u7528\u6216\u79ef\u7d2f\u4fee\u590d\u7ecf\u9a8c\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002SWE-Exp\u65e8\u5728\u901a\u8fc7\u7ecf\u9a8c\u589e\u5f3a\u7684\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SWE-Exp\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u65b9\u9762\u7684\u7ecf\u9a8c\u5e93\uff0c\u6355\u6349\u6210\u529f\u548c\u5931\u8d25\u7684\u4fee\u590d\u5c1d\u8bd5\uff0c\u4ece\u9ad8\u5c42\u6b21\u95ee\u9898\u7406\u89e3\u5230\u5177\u4f53\u7684\u4ee3\u7801\u4fee\u6539\u4e2d\u63d0\u53d6\u53ef\u590d\u7528\u7684\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSWE-Exp\u5728\u5f00\u6e90\u4ee3\u7406\u6846\u67b6\u4e0b\u7684SWE-bench-Verified\u4e0a\u8fbe\u5230\u4e8641.6%\u7684Pass@1\u89e3\u51b3\u7387\uff0c\u4e3a\u5f53\u524d\u6700\u4f73\u8868\u73b0\u3002", "conclusion": "SWE-Exp\u4e3a\u81ea\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u79ef\u7d2f\u548c\u5229\u7528\u4fee\u590d\u7ecf\u9a8c\u7684\u65b0\u8303\u5f0f\uff0c\u4ece\u8bd5\u9519\u5f0f\u63a2\u7d22\u8f6c\u5411\u4e86\u7b56\u7565\u6027\u7684\u7ecf\u9a8c\u9a71\u52a8\u95ee\u9898\u89e3\u51b3\u3002"}}
{"id": "2507.23433", "pdf": "https://arxiv.org/pdf/2507.23433", "abs": "https://arxiv.org/abs/2507.23433", "authors": ["Erfan Delfani", "Nikolaos Pappas"], "title": "From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks", "categories": ["cs.NI", "cs.IT", "math.IT"], "comment": null, "summary": "Timely and informative data dissemination in communication networks is\nessential for enhancing system performance and energy efficiency, as it reduces\nthe transmission of outdated or redundant data. Timeliness metrics, such as Age\nof Information (AoI), effectively quantify data freshness; however, these\nmetrics fail to account for the intrinsic informativeness of the content\nitself. To address this limitation, content-based metrics have been proposed\nthat combine both timeliness and informativeness. Nevertheless, existing\nstudies have predominantly focused on evaluating average metric values, leaving\nthe complete distribution-particularly in multi-hop network scenarios-largely\nunexplored. In this paper, we provide a comprehensive analysis of the\nstationary distribution of the Version Age of Information (VAoI), a\ncontent-based metric, under various scheduling policies, including randomized\nstationary, uniform, and threshold-based policies, with transmission\nconstraints in single-hop and multi-hop networks. We derive closed-form\nexpressions for the stationary distribution and average VAoI under these\nscheduling approaches. Furthermore, for threshold-based scheduling, we\nanalytically determine the optimal threshold value that minimizes VAoI and\nderive the corresponding optimal VAoI in closed form. Numerical evaluations\nverify our analytical findings, providing valuable insights into leveraging\nVAoI in the design of efficient communication networks.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u4fe1\u606f\u7248\u672c\u5e74\u9f84\uff08VAoI\uff09\u7684\u7a33\u6001\u5206\u5e03\uff0c\u63d0\u51fa\u591a\u79cd\u8c03\u5ea6\u7b56\u7565\u4e0b\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u5728\u5355\u8df3\u548c\u591a\u8df3\u7f51\u7edc\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5e73\u5747\u6307\u6807\uff0c\u7f3a\u4e4f\u5bf9\u5185\u5bb9\u65b0\u9c9c\u5ea6\u4e0e\u4fe1\u606f\u4ef7\u503c\u7684\u5168\u9762\u5206\u5e03\u5206\u6790\uff0c\u5c24\u5176\u662f\u5728\u591a\u8df3\u7f51\u7edc\u4e2d\u3002", "method": "\u7814\u7a76\u591a\u79cd\u8c03\u5ea6\u7b56\u7565\uff08\u968f\u673a\u56fa\u5b9a\u3001\u5747\u5300\u3001\u9608\u503c\uff09\uff0c\u63a8\u5bfcVAoI\u7a33\u6001\u5206\u5e03\u53ca\u5e73\u5747\u503c\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u786e\u5b9a\u6700\u4f18\u9608\u503c\u3002", "result": "\u5f97\u51fa\u4e86VAoI\u7684\u6700\u4f18\u9608\u503c\u53ca\u95ed\u5f0f\u89e3\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u901a\u4fe1\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u4e8eVAoI\u7684\u9ad8\u6548\u8c03\u5ea6\u7b56\u7565\u53c2\u8003\u3002"}}
{"id": "2507.23432", "pdf": "https://arxiv.org/pdf/2507.23432", "abs": "https://arxiv.org/abs/2507.23432", "authors": ["Vincent Cohen-Addad", "Alessandro Epasto", "Jason Lee", "Morteza Zadimoghaddam"], "title": "Scalable contribution bounding to achieve privacy", "categories": ["cs.DS", "cs.CR", "cs.DC"], "comment": null, "summary": "In modern datasets, where single records can have multiple owners, enforcing\nuser-level differential privacy requires capping each user's total\ncontribution. This \"contribution bounding\" becomes a significant combinatorial\nchallenge. Existing sequential algorithms for this task are computationally\nintensive and do not scale to the massive datasets prevalent today. To address\nthis scalability bottleneck, we propose a novel and efficient distributed\nalgorithm. Our approach models the complex ownership structure as a hypergraph,\nwhere users are vertices and records are hyperedges. The algorithm proceeds in\nrounds, allowing users to propose records in parallel. A record is added to the\nfinal dataset only if all its owners unanimously agree, thereby ensuring that\nno user's predefined contribution limit is violated. This method aims to\nmaximize the size of the resulting dataset for high utility while providing a\npractical, scalable solution for implementing user-level privacy in large,\nreal-world systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u901a\u8fc7\u8d85\u56fe\u5efa\u6a21\u548c\u5e76\u884c\u5904\u7406\u89e3\u51b3\u591a\u6240\u6709\u8005\u6570\u636e\u96c6\u4e2d\u7528\u6237\u7ea7\u5dee\u5206\u9690\u79c1\u7684\u8d21\u732e\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u96c6\u4e2d\u5355\u4e2a\u8bb0\u5f55\u53ef\u80fd\u5c5e\u4e8e\u591a\u4e2a\u6240\u6709\u8005\uff0c\u5b9e\u73b0\u7528\u6237\u7ea7\u5dee\u5206\u9690\u79c1\u9700\u8981\u9650\u5236\u6bcf\u4e2a\u7528\u6237\u7684\u603b\u8d21\u732e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u5f00\u9500\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u5c06\u6240\u6709\u6743\u7ed3\u6784\u5efa\u6a21\u4e3a\u8d85\u56fe\uff0c\u7528\u6237\u4e3a\u9876\u70b9\uff0c\u8bb0\u5f55\u4e3a\u8d85\u8fb9\uff1b\u91c7\u7528\u591a\u8f6e\u5e76\u884c\u5904\u7406\uff0c\u8981\u6c42\u6240\u6709\u6240\u6709\u8005\u4e00\u81f4\u540c\u610f\u624d\u6dfb\u52a0\u8bb0\u5f55\u3002", "result": "\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6848\uff0c\u80fd\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\u6700\u5927\u5316\u6570\u636e\u96c6\u89c4\u6a21\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u5b9e\u9645\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u7528\u6237\u7ea7\u5dee\u5206\u9690\u79c1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.22890", "pdf": "https://arxiv.org/pdf/2507.22890", "abs": "https://arxiv.org/abs/2507.22890", "authors": ["Saadiq Rauf Khan", "Vinit Chandak", "Sougata Mukherjea"], "title": "Evaluating LLMs for Visualization Generation and Understanding", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Information Visualization has been utilized to gain insights from complex\ndata. In recent times, Large Language models (LLMs) have performed very well in\nmany tasks. In this paper, we showcase the capabilities of different popular\nLLMs to generate code for visualization based on simple prompts. We also\nanalyze the power of LLMs to understand some common visualizations by answering\nquestions. Our study shows that LLMs could generate code for some simpler\nvisualizations such as bar and pie charts. Moreover, they could answer simple\nquestions about visualizations. However, LLMs also have several limitations.\nFor example, some of them had difficulty generating complex visualizations,\nsuch as violin plot. LLMs also made errors in answering some questions about\nvisualizations, for example, identifying relationships between close boundaries\nand determining lengths of shapes. We believe that our insights can be used to\nimprove both LLMs and Information Visualization systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4fe1\u606f\u53ef\u89c6\u5316\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5c55\u793a\u4e86\u5176\u751f\u6210\u53ef\u89c6\u5316\u4ee3\u7801\u548c\u7406\u89e3\u5e38\u89c1\u56fe\u8868\u7684\u80fd\u529b\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u4fe1\u606f\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e8\u5728\u4fc3\u8fdbLLMs\u548c\u53ef\u89c6\u5316\u7cfb\u7edf\u7684\u5171\u540c\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u7b80\u5355\u63d0\u793a\u8ba9LLMs\u751f\u6210\u53ef\u89c6\u5316\u4ee3\u7801\uff0c\u5e76\u6d4b\u8bd5\u5176\u5bf9\u5e38\u89c1\u56fe\u8868\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "LLMs\u80fd\u591f\u751f\u6210\u7b80\u5355\u56fe\u8868\uff08\u5982\u67f1\u72b6\u56fe\u3001\u997c\u56fe\uff09\u7684\u4ee3\u7801\uff0c\u5e76\u80fd\u56de\u7b54\u5173\u4e8e\u56fe\u8868\u7684\u57fa\u672c\u95ee\u9898\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u5c0f\u63d0\u7434\u56fe\u751f\u6210\u6216\u8fb9\u754c\u5173\u7cfb\u8bc6\u522b\uff09\u4e2d\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u548c\u53ef\u89c6\u5316\u7cfb\u7edf\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u6311\u6218\u3002"}}
{"id": "2507.23370", "pdf": "https://arxiv.org/pdf/2507.23370", "abs": "https://arxiv.org/abs/2507.23370", "authors": ["Trae Research Team", "Pengfei Gao", "Zhao Tian", "Xiangxin Meng", "Xinchen Wang", "Ruida Hu", "Yuanan Xiao", "Yizhou Liu", "Zhao Zhang", "Junjie Chen", "Cuiyun Gao", "Yun Lin", "Yingfei Xiong", "Chao Peng", "Xia Liu"], "title": "Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling", "categories": ["cs.SE", "cs.AI"], "comment": "Pengfei Gao and Zhao Tian contributed equally to this technical\n  report", "summary": "Software issue resolution is a critical challenge in software engineering and\nhas garnered increasing attention in recent years. With the rapid advancement\nof large language models (LLMs), substantial progress has been made in\naddressing real-world software engineering tasks. Recent studies have\nintroduced ensemble reasoning techniques to enhance the performance of\nLLM-based issue resolution. However, existing prompting-based methods still\nface limitations in effectively exploring large ensemble spaces and lack the\ncapacity for repository-level understanding, both of which constrain their\noverall effectiveness. In this paper, we propose Trae Agent, the first\nagent-based ensemble reasoning approach for repository-level issue resolution.\nTrae Agent formulates our goal as an optimal solution search problem and\naddresses two key challenges, i.e., large ensemble spaces and repository-level\nunderstanding, through modular agents for generation, pruning, and selection.\nWe conduct extensive experiments using three leading LLMs on the widely-adopted\nSWE-bench benchmark, comparing Trae Agent against four state-of-the-art\nensemble reasoning techniques. Experimental results demonstrate that Trae Agent\nconsistently achieves superior performance, with an average improvement of\n10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first\nplace on the SWE-bench Verified leaderboard, with a notable Pass@1 score of\n75.20%. We are pleased to release Trae Agent as an open-source project to\nsupport the research community, with all resources available at\nhttps://github.com/bytedance/trae-agent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u96c6\u6210\u63a8\u7406\u65b9\u6cd5Trae Agent\uff0c\u7528\u4e8e\u89e3\u51b3\u8f6f\u4ef6\u4ed3\u5e93\u7ea7\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4ee3\u7406\u5904\u7406\u5927\u89c4\u6a21\u96c6\u6210\u7a7a\u95f4\u548c\u4ed3\u5e93\u7ea7\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u96c6\u6210\u7a7a\u95f4\u63a2\u7d22\u548c\u4ed3\u5e93\u7ea7\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u5176\u6574\u4f53\u6548\u679c\u3002", "method": "\u63d0\u51faTrae Agent\uff0c\u901a\u8fc7\u751f\u6210\u3001\u526a\u679d\u548c\u9009\u62e9\u6a21\u5757\u5316\u4ee3\u7406\uff0c\u5c06\u76ee\u6807\u5efa\u6a21\u4e3a\u6700\u4f18\u89e3\u641c\u7d22\u95ee\u9898\u3002", "result": "\u5728SWE-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTrae Agent\u5e73\u5747\u6027\u80fd\u63d0\u534710.22%\uff0cPass@1\u5f97\u5206\u4e3a75.20%\uff0c\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "Trae Agent\u901a\u8fc7\u4ee3\u7406\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u7684\u6311\u6218\uff0c\u5e76\u5f00\u6e90\u4ee5\u652f\u6301\u7814\u7a76\u793e\u533a\u3002"}}
{"id": "2507.23556", "pdf": "https://arxiv.org/pdf/2507.23556", "abs": "https://arxiv.org/abs/2507.23556", "authors": ["Botao Zhu", "Xianbin Wang"], "title": "Networked Physical Computing: A New Paradigm for Effective Task Completion via Hypergraph Aided Trusted Task-Resource Matching", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "Due to the diverse physical attributes of computing resources and tasks,\ndeveloping effective mechanisms to facilitate task and resource matching in\ncomplex connected systems for value-oriented task completion has become\nincreasingly challenging. To address the challenge, this paper proposes a\nnetworked physical computing system that integrates the physical attributes of\ncomputing resources and tasks as well as task-specific trust relationships\namong devices to enable value-driven task completion. Specifically, we propose\na state-of-the-art hypergraph-aided trusted task-resource matching\n(TTR-matching) framework to achieve the envisioned physical computing. First, a\ntask-specific trusted physical resource hypergraph is defined, which integrates\ntask-specific trust, the physical attributes of resources, and task types. This\nenables accurate modeling of device collaboration dependencies under specific\ntask types. Next, a task hypergraph is generated to associate the task\ninitiator with the physical attributes of the corresponding tasks. Based on\nthese two hypergraphs, a hypergraph matching algorithm is designed to\nfacilitate task-specific trusted collaborator selection and accurate\ntask-resource matching for value-maximizing task completion. Extensive\nexperimental results demonstrate that the proposed TTR-matching framework\noutperforms comparison algorithms in identifying task-specific trustworthy\ncollaborators and maximizing the average value of task completion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u56fe\u8f85\u52a9\u7684\u4fe1\u4efb\u4efb\u52a1-\u8d44\u6e90\u5339\u914d\u6846\u67b6\uff08TTR-matching\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u8fde\u63a5\u7cfb\u7edf\u4e2d\u4efb\u52a1\u548c\u8d44\u6e90\u5339\u914d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6574\u5408\u4efb\u52a1\u7279\u5b9a\u4fe1\u4efb\u5173\u7cfb\u548c\u7269\u7406\u5c5e\u6027\uff0c\u5b9e\u73b0\u4ef7\u503c\u9a71\u52a8\u7684\u4efb\u52a1\u5b8c\u6210\u3002", "motivation": "\u7531\u4e8e\u8ba1\u7b97\u8d44\u6e90\u548c\u4efb\u52a1\u7684\u7269\u7406\u5c5e\u6027\u591a\u6837\u5316\uff0c\u590d\u6742\u8fde\u63a5\u7cfb\u7edf\u4e2d\u4efb\u52a1\u4e0e\u8d44\u6e90\u7684\u5339\u914d\u673a\u5236\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684TTR-matching\u6846\u67b6\uff0c\u5305\u62ec\u4efb\u52a1\u7279\u5b9a\u4fe1\u4efb\u7269\u7406\u8d44\u6e90\u8d85\u56fe\u548c\u4efb\u52a1\u8d85\u56fe\u7684\u5b9a\u4e49\uff0c\u4ee5\u53ca\u8d85\u56fe\u5339\u914d\u7b97\u6cd5\u7684\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTTR-matching\u6846\u67b6\u5728\u8bc6\u522b\u4efb\u52a1\u7279\u5b9a\u53ef\u4fe1\u534f\u4f5c\u4f19\u4f34\u548c\u6700\u5927\u5316\u4efb\u52a1\u5b8c\u6210\u5e73\u5747\u4ef7\u503c\u65b9\u9762\u4f18\u4e8e\u5bf9\u6bd4\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u4efb\u52a1\u7279\u5b9a\u4fe1\u4efb\u548c\u7269\u7406\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u52a1-\u8d44\u6e90\u5339\u914d\u548c\u4ef7\u503c\u9a71\u52a8\u7684\u4efb\u52a1\u5b8c\u6210\u3002"}}
{"id": "2507.23609", "pdf": "https://arxiv.org/pdf/2507.23609", "abs": "https://arxiv.org/abs/2507.23609", "authors": ["Halid Ziya Yerebakan", "Gerardo Hermosillo Valadez"], "title": "Consistent Point Matching", "categories": ["cs.CV", "cs.DC", "cs.LG"], "comment": null, "summary": "This study demonstrates that incorporating a consistency heuristic into the\npoint-matching algorithm \\cite{yerebakan2023hierarchical} improves robustness\nin matching anatomical locations across pairs of medical images. We validated\nour approach on diverse longitudinal internal and public datasets spanning CT\nand MRI modalities. Notably, it surpasses state-of-the-art results on the Deep\nLesion Tracking dataset. Additionally, we show that the method effectively\naddresses landmark localization. The algorithm operates efficiently on standard\nCPU hardware and allows configurable trade-offs between speed and robustness.\nThe method enables high-precision navigation between medical images without\nrequiring a machine learning model or training data.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u4e00\u81f4\u6027\u542f\u53d1\u5f0f\u5f15\u5165\u70b9\u5339\u914d\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u533b\u5b66\u56fe\u50cf\u89e3\u5256\u4f4d\u7f6e\u5339\u914d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6548\u679c\u3002", "motivation": "\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u89e3\u5256\u4f4d\u7f6e\u5339\u914d\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u5c06\u4e00\u81f4\u6027\u542f\u53d1\u5f0f\u6574\u5408\u5230\u70b9\u5339\u914d\u7b97\u6cd5\u4e2d\uff0c\u65e0\u9700\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6216\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728Deep Lesion Tracking\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6548\u679c\uff0c\u4e14\u7b97\u6cd5\u5728\u6807\u51c6CPU\u786c\u4ef6\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u5b66\u56fe\u50cf\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u4e14\u9ad8\u6548\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.22891", "pdf": "https://arxiv.org/pdf/2507.22891", "abs": "https://arxiv.org/abs/2507.22891", "authors": ["J\u00e9r\u00f4me Ferrari", "Benoit Delinchant", "Fr\u00e9d\u00e9ric Wurtz", "Olga Rouchouze"], "title": "Real-time energy monitoring infrastructure for residential collective self-consumption operations using Linky meter", "categories": ["cs.HC"], "comment": "Cired 2025, Jun 2025, Gen{\\`e}ve (CH), Switzerland", "summary": "As part of the energy transition and the rise in energy prices, the number of\ncollective self-consumption operations in France is steadily increasing.\nHowever, energy flow monitoring currently relies on historical ''day+1'' data\nprovided by Linky meters, which does not offer real time feedback to help\nparticipants adapt their energy consumption behaviors. This article introduces\na new open-source infrastructure for real-time monitoring based on Linky meter\ndata, enabling participants to make informed decisions and take timely actions.\nIt includes a description of the xKy device, applied to a collective\nself-consumption operation involving nine participants, supported by the Energy\nTransition Observatory (OTE). The project encompasses the implementation of\ngateways in participants' homes and the development and operation of real-time\nmonitoring website, aimed at increasing participants' self-consumption rate.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eLinky\u7535\u8868\u6570\u636e\u7684\u5f00\u6e90\u5b9e\u65f6\u76d1\u63a7\u57fa\u7840\u8bbe\u65bd\uff0c\u7528\u4e8e\u652f\u6301\u6cd5\u56fd\u96c6\u4f53\u81ea\u6d88\u8d39\u9879\u76ee\u7684\u80fd\u6e90\u7ba1\u7406\uff0c\u5e2e\u52a9\u53c2\u4e0e\u8005\u53ca\u65f6\u8c03\u6574\u7528\u7535\u884c\u4e3a\u3002", "motivation": "\u968f\u7740\u80fd\u6e90\u8f6c\u578b\u548c\u80fd\u6e90\u4ef7\u683c\u4e0a\u6da8\uff0c\u6cd5\u56fd\u96c6\u4f53\u81ea\u6d88\u8d39\u9879\u76ee\u589e\u591a\uff0c\u4f46\u73b0\u6709\u80fd\u6e90\u76d1\u63a7\u4f9d\u8d56\u5386\u53f2\u6570\u636e\uff0c\u7f3a\u4e4f\u5b9e\u65f6\u53cd\u9988\u3002", "method": "\u5f00\u53d1\u4e86xKy\u8bbe\u5907\uff0c\u90e8\u7f72\u7f51\u5173\u5e76\u642d\u5efa\u5b9e\u65f6\u76d1\u63a7\u7f51\u7ad9\uff0c\u5e94\u7528\u4e8e9\u6237\u53c2\u4e0e\u7684\u96c6\u4f53\u81ea\u6d88\u8d39\u9879\u76ee\u3002", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u80fd\u6e90\u6d41\u76d1\u63a7\uff0c\u53c2\u4e0e\u8005\u80fd\u53ca\u65f6\u51b3\u7b56\u548c\u884c\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6848\u63d0\u5347\u4e86\u96c6\u4f53\u81ea\u6d88\u8d39\u7387\uff0c\u652f\u6301\u80fd\u6e90\u8f6c\u578b\u76ee\u6807\u3002"}}
{"id": "2507.23425", "pdf": "https://arxiv.org/pdf/2507.23425", "abs": "https://arxiv.org/abs/2507.23425", "authors": ["Daphn\u00e9 Larrivain", "Shinhyung Yang", "Wilhelm Hasselbring"], "title": "Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures", "categories": ["cs.SE"], "comment": "9 pages, 9 figures", "summary": "The Kieker observability framework is a tool that provides users with the\nmeans to design a custom observability pipeline for their application.\nOriginally tailored for Java, supporting Python with Kieker is worthwhile.\nPython's popularity has exploded over the years, thus making structural\ninsights of Python applications highly valuable. Our Python analysis pipeline\ncombines static and dynamic analysis in order to build a complete picture of a\ngiven system.", "AI": {"tldr": "Kieker\u6846\u67b6\u6700\u521d\u4e3aJava\u8bbe\u8ba1\uff0c\u73b0\u6269\u5c55\u652f\u6301Python\uff0c\u901a\u8fc7\u7ed3\u5408\u9759\u6001\u548c\u52a8\u6001\u5206\u6790\u63d0\u4f9b\u5168\u9762\u7684\u7cfb\u7edf\u6d1e\u5bdf\u3002", "motivation": "Python\u7684\u6d41\u884c\u4f7f\u5f97\u5bf9\u5176\u5e94\u7528\u7a0b\u5e8f\u7684\u7ed3\u6784\u6027\u5206\u6790\u53d8\u5f97\u91cd\u8981\uff0c\u56e0\u6b64\u6269\u5c55Kieker\u6846\u67b6\u4ee5\u652f\u6301Python\u5177\u6709\u4ef7\u503c\u3002", "method": "\u7ed3\u5408\u9759\u6001\u548c\u52a8\u6001\u5206\u6790\uff0c\u6784\u5efa\u5b8c\u6574\u7684\u7cfb\u7edf\u89c6\u56fe\u3002", "result": "\u5b9e\u73b0\u4e86Python\u5206\u6790\u7ba1\u9053\uff0c\u80fd\u591f\u63d0\u4f9b\u5bf9Python\u5e94\u7528\u7a0b\u5e8f\u7684\u5168\u9762\u6d1e\u5bdf\u3002", "conclusion": "Kieker\u6846\u67b6\u7684Python\u652f\u6301\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u591a\u7075\u6d3b\u6027\uff0c\u6ee1\u8db3\u4e86Python\u5e94\u7528\u7a0b\u5e8f\u7684\u53ef\u89c2\u6d4b\u6027\u9700\u6c42\u3002"}}
{"id": "2507.22892", "pdf": "https://arxiv.org/pdf/2507.22892", "abs": "https://arxiv.org/abs/2507.22892", "authors": ["Ismail Hossain", "Mridul Banik"], "title": "Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Conventional augmentative and alternative communication (AAC) systems and\nlanguage-learning platforms often fail to adapt in real time to the user's\ncognitive and linguistic needs, especially in neurological conditions such as\npost-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in\nnoninvasive electroencephalography (EEG)--based brain-computer interfaces\n(BCIs) and transformer--based large language models (LLMs) offer complementary\nstrengths: BCIs capture users' neural intent with low fatigue, while LLMs\ngenerate contextually tailored language content. We propose and evaluate a\nnovel hybrid framework that leverages real-time EEG signals to drive an\nLLM-powered language rehabilitation assistant. This system aims to: (1) enable\nusers with severe speech or motor impairments to navigate language-learning\nmodules via mental commands; (2) dynamically personalize vocabulary,\nsentence-construction exercises, and corrective feedback; and (3) monitor\nneural markers of cognitive effort to adjust task difficulty on the fly.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408EEG-BCI\u548cLLM\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u4e2a\u6027\u5316\u7684\u8bed\u8a00\u5eb7\u590d\u8f85\u52a9\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfAAC\u7cfb\u7edf\u548c\u8bed\u8a00\u5b66\u4e60\u5e73\u53f0\u7f3a\u4e4f\u5b9e\u65f6\u9002\u5e94\u6027\uff0c\u5c24\u5176\u5728\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u60a3\u8005\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u5229\u7528EEG\u4fe1\u53f7\u9a71\u52a8LLM\uff0c\u5b9e\u73b0\u8bed\u8a00\u6a21\u5757\u5bfc\u822a\u3001\u52a8\u6001\u4e2a\u6027\u5316\u4efb\u52a1\u548c\u8ba4\u77e5\u8d1f\u8377\u76d1\u6d4b\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u652f\u6301\u4e25\u91cd\u8a00\u8bed\u6216\u8fd0\u52a8\u969c\u788d\u7528\u6237\uff0c\u5e76\u63d0\u4f9b\u4e2a\u6027\u5316\u8bed\u8a00\u5b66\u4e60\u8f85\u52a9\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3a\u8bed\u8a00\u5eb7\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u5b9e\u65f6\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23640", "pdf": "https://arxiv.org/pdf/2507.23640", "abs": "https://arxiv.org/abs/2507.23640", "authors": ["Samah Kansab", "Mohammed Sayagh", "Francis Bordeleau", "Ali Tizghadam"], "title": "An Empirical Study on the Amount of Changes Required for Merge Request Acceptance", "categories": ["cs.SE"], "comment": null, "summary": "Code review (CR) is essential to software development, helping ensure that\nnew code is properly integrated. However, the CR process often involves\nsignificant effort, including code adjustments, responses to reviewers, and\ncontinued implementation. While past studies have examined CR delays and\niteration counts, few have investigated the effort based on the volume of code\nchanges required, especially in the context of GitLab Merge Requests (MRs),\nwhich remains underexplored. In this paper, we define and measure CR effort as\nthe amount of code modified after submission, using a dataset of over 23,600\nMRs from four GitLab projects. We find that up to 71% of MRs require\nadjustments after submission, and 28% of these involve changes to more than 200\nlines of code. Surprisingly, this effort is not correlated with review time or\nthe number of participants. To better understand and predict CR effort, we\ntrain an interpretable machine learning model using metrics across multiple\ndimensions: text features, code complexity, developer experience, review\nhistory, and branching. Our model achieves strong performance (AUC 0.84-0.88)\nand reveals that complexity, experience, and text features are key predictors.\nHistorical project characteristics also influence current review effort. Our\nfindings highlight the feasibility of using machine learning to explain and\nanticipate the effort needed to integrate code changes during review.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4ee3\u7801\u5ba1\u67e5\uff08CR\uff09\u4e2d\u57fa\u4e8e\u4ee3\u7801\u4fee\u6539\u91cf\u7684\u5de5\u4f5c\u8d1f\u8377\uff0c\u53d1\u73b0\u5927\u591a\u6570\u5408\u5e76\u8bf7\u6c42\u9700\u8c03\u6574\uff0c\u4e14\u4fee\u6b63\u91cf\u4e0d\u76f4\u63a5\u5f71\u54cd\u5ba1\u67e5\u65f6\u95f4\u6216\u53c2\u4e0e\u8005\u6570\u91cf\u3002\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4bCR\u5de5\u4f5c\u91cf\uff0c\u6210\u6548\u663e\u8457\u3002", "motivation": "\u63a2\u8ba8GitLab\u5408\u5e76\u8bf7\u6c42\u4e2d\u4ee3\u7801\u5ba1\u67e5\u7684\u5de5\u4f5c\u8d1f\u8377\uff0c\u5c24\u5176\u662f\u4ee3\u7801\u4fee\u6539\u91cf\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u4ece\u56db\u4e2aGitLab\u9879\u76ee\u4e2d\u6536\u96c623,600\u591a\u4e2a\u5408\u5e76\u8bf7\u6c42\u6570\u636e\uff0c\u5b9a\u4e49CR\u5de5\u4f5c\u91cf\u4e3a\u63d0\u4ea4\u540e\u7684\u4ee3\u7801\u4fee\u6539\u91cf\u3002\u91c7\u7528\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u7ef4\u6307\u6807\uff08\u6587\u672c\u7279\u5f81\u3001\u4ee3\u7801\u590d\u6742\u5ea6\u7b49\uff09\u9884\u6d4b\u5de5\u4f5c\u91cf\u3002", "result": "71%\u7684\u5408\u5e76\u8bf7\u6c42\u9700\u8c03\u6574\uff0c28%\u6d89\u53ca200\u591a\u884c\u4ee3\u7801\u4fee\u6539\uff1b\u4fee\u6b63\u91cf\u4e0e\u5ba1\u67e5\u65f6\u95f4\u6216\u53c2\u4e0e\u8005\u6570\u91cf\u65e0\u5173\u3002\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u4f18\u79c0\uff08AUC 0.84-0.88\uff09\uff0c\u590d\u6742\u5ea6\u3001\u7ecf\u9a8c\u53ca\u6587\u672c\u7279\u5f81\u662f\u5173\u952e\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u53ef\u6709\u6548\u89e3\u91ca\u548c\u9884\u6d4b\u4ee3\u7801\u5ba1\u67e5\u5de5\u4f5c\u91cf\uff0c\u9879\u76ee\u5386\u53f2\u7279\u5f81\u5bf9\u5f53\u524d\u5ba1\u67e5\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2507.22893", "pdf": "https://arxiv.org/pdf/2507.22893", "abs": "https://arxiv.org/abs/2507.22893", "authors": ["Giuseppe Riva"], "title": "Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Contemporary human-AI interaction research overlooks how AI systems\nfundamentally reshape human cognition pre-consciously, a critical blind spot\nfor understanding distributed cognition. This paper introduces \"Cognitive\nInfrastructure Studies\" (CIS) as a new interdisciplinary domain to\nreconceptualize AI as \"cognitive infrastructures\": foundational, often\ninvisible systems conditioning what is knowable and actionable in digital\nsocieties. These semantic infrastructures transport meaning, operate through\nanticipatory personalization, and exhibit adaptive invisibility, making their\ninfluence difficult to detect. Critically, they automate \"relevance judgment,\"\nshifting the \"locus of epistemic agency\" to non-human systems. Through\nnarrative scenarios spanning individual (cognitive dependency), collective\n(democratic deliberation), and societal (governance) scales, we describe how\ncognitive infrastructures reshape human cognition, public reasoning, and social\nepistemologies. CIS aims to address how AI preprocessing reshapes distributed\ncognition across individual, collective, and cultural scales, requiring\nunprecedented integration of diverse disciplinary methods. The framework also\naddresses critical gaps across disciplines: cognitive science lacks\npopulation-scale preprocessing analysis capabilities, digital sociology cannot\naccess individual cognitive mechanisms, and computational approaches miss\ncultural transmission dynamics. To achieve this goal CIS also provides\nmethodological innovations for studying invisible algorithmic influence:\n\"infrastructure breakdown methodologies\", experimental approaches that reveal\ncognitive dependencies by systematically withdrawing AI preprocessing after\nperiods of habituation.", "AI": {"tldr": "\u63d0\u51fa\u201c\u8ba4\u77e5\u57fa\u7840\u8bbe\u65bd\u7814\u7a76\u201d\uff08CIS\uff09\u4f5c\u4e3a\u65b0\u9886\u57df\uff0c\u5c06AI\u89c6\u4e3a\u91cd\u5851\u4eba\u7c7b\u8ba4\u77e5\u7684\u201c\u8ba4\u77e5\u57fa\u7840\u8bbe\u65bd\u201d\uff0c\u7814\u7a76\u5176\u5982\u4f55\u901a\u8fc7\u9690\u5f62\u673a\u5236\u5f71\u54cd\u77e5\u8bc6\u83b7\u53d6\u548c\u793e\u4f1a\u884c\u4e3a\u3002", "motivation": "\u5f53\u524d\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u5ffd\u89c6AI\u5982\u4f55\u6f5c\u5728\u5730\u91cd\u5851\u4eba\u7c7b\u8ba4\u77e5\uff0c\u9700\u65b0\u7684\u8de8\u5b66\u79d1\u65b9\u6cd5\u6765\u5206\u6790\u8fd9\u79cd\u9690\u5f62\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u53d9\u4e8b\u573a\u666f\uff08\u4e2a\u4f53\u8ba4\u77e5\u4f9d\u8d56\u3001\u96c6\u4f53\u6c11\u4e3b\u5ba1\u8bae\u3001\u793e\u4f1a\u6cbb\u7406\uff09\u548c\u201c\u57fa\u7840\u8bbe\u65bd\u7834\u574f\u65b9\u6cd5\u201d\u63ed\u793aAI\u9884\u5904\u7406\u7684\u5f71\u54cd\u3002", "result": "AI\u4f5c\u4e3a\u8ba4\u77e5\u57fa\u7840\u8bbe\u65bd\u6539\u53d8\u4e86\u4eba\u7c7b\u8ba4\u77e5\u3001\u516c\u5171\u63a8\u7406\u548c\u793e\u4f1a\u8ba4\u8bc6\u8bba\uff0c\u9700\u6574\u5408\u591a\u5b66\u79d1\u65b9\u6cd5\u7814\u7a76\u5176\u5f71\u54cd\u3002", "conclusion": "CIS\u586b\u8865\u4e86\u8ba4\u77e5\u79d1\u5b66\u3001\u6570\u5b57\u793e\u4f1a\u5b66\u548c\u8ba1\u7b97\u65b9\u6cd5\u7684\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76AI\u7684\u9690\u5f62\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\u548c\u65b9\u6cd5\u3002"}}
{"id": "1908.01212", "pdf": "https://arxiv.org/pdf/1908.01212", "abs": "https://arxiv.org/abs/1908.01212", "authors": ["Fatimah Rita Ahmadi"], "title": "Typing Tensor Calculus in 2-Categories (I)", "categories": ["math.CT", "cs.LG", "cs.SE"], "comment": "28 pages; extended introduction, more explanation", "summary": "To formalize calculations in linear algebra for the development of efficient\nalgorithms and a framework suitable for functional programming languages and\nfaster parallelized computations, we adopt an approach that treats elements of\nlinear algebra, such as matrices, as morphisms in the category of matrices,\n$\\mathbf{Mat_{k}}$. This framework is further extended by generalizing the\nresults to arbitrary monoidal semiadditive categories. To enrich this\nperspective and accommodate higher-rank matrices (tensors), we define\nsemiadditive 2-categories, where matrices $T_{ij}$ are represented as\n1-morphisms, and tensors with four indices $T_{ijkl}$ as 2-morphisms. This\nformalization provides an index-free, typed linear algebra framework that\nincludes matrices and tensors with up to four indices. Furthermore, we extend\nthe framework to monoidal semiadditive 2-categories and demonstrate detailed\noperations and vectorization within the 2-category of 2Vec introduced by\nKapranov and Voevodsky.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8303\u7574\u8bba\u7684\u7ebf\u6027\u4ee3\u6570\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u77e9\u9635\u548c\u5f20\u91cf\u7684\u8303\u7574\u5316\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u65e0\u7d22\u5f15\u3001\u7c7b\u578b\u5316\u7684\u9ad8\u6548\u7b97\u6cd5\u5f00\u53d1\uff0c\u9002\u7528\u4e8e\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00\u548c\u5e76\u884c\u8ba1\u7b97\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u4e3a\u7ebf\u6027\u4ee3\u6570\u8ba1\u7b97\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u9ad8\u6548\u7b97\u6cd5\u7684\u5f00\u53d1\uff0c\u5e76\u9002\u5e94\u51fd\u6570\u5f0f\u7f16\u7a0b\u8bed\u8a00\u548c\u5e76\u884c\u8ba1\u7b97\u7684\u9700\u6c42\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u77e9\u9635\u89c6\u4e3a\u77e9\u9635\u8303\u7574\u4e2d\u7684\u6001\u5c04\uff0c\u5e76\u901a\u8fc7\u534a\u52a0\u60272-\u8303\u7574\u6269\u5c55\u4ee5\u6db5\u76d6\u9ad8\u9636\u5f20\u91cf\u3002\u8fdb\u4e00\u6b65\u63a8\u5e7f\u5230\u4efb\u610f\u5e7a\u534a\u534a\u52a0\u6027\u8303\u7574\u548c2-\u8303\u7574\uff0c\u5177\u4f53\u64cd\u4f5c\u57282Vec\u8303\u7574\u4e2d\u6f14\u793a\u3002", "result": "\u7ed3\u679c\u662f\u6784\u5efa\u4e86\u4e00\u4e2a\u65e0\u7d22\u5f15\u3001\u7c7b\u578b\u5316\u7684\u6846\u67b6\uff0c\u80fd\u591f\u8868\u793a\u548c\u5904\u7406\u56db\u7ef4\u4ee5\u4e0b\u7684\u5f20\u91cf\uff0c\u5e76\u4e3a\u9ad8\u6548\u8ba1\u7b97\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06\u7ebf\u6027\u4ee3\u6570\u8ba1\u7b97\u8303\u7574\u5316\uff0c\u4e3a\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5e76\u884c\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5de5\u5177\u3002"}}
{"id": "2507.22894", "pdf": "https://arxiv.org/pdf/2507.22894", "abs": "https://arxiv.org/abs/2507.22894", "authors": ["Monique Munarini"], "title": "When no one shows up (at first): Navigating the uncertainties of participatory workshops in interdisciplinary research", "categories": ["cs.HC"], "comment": "Presented at HHAI25:The 4th International Conference Series on Hybrid\n  Human-Artificial Intelligence, workshop Mind the AI-GAP 2025:Co-Designing\n  Socio-Technical Systems. (June 9-13, 2025 in Pisa, Italy)", "summary": "This reflective paper explores often-unspoken challenges of designing and\nfacilitating co-design and participatory workshops, offering practical\nstrategies for early career researchers (ECRs) navigating these methods.\nDrawing from personal experience conducting a series of workshops titled: How\nto Think About Equity in the AI Ecosystem. It follows the full arc of the\nworkshop experience, from conceptualization and activity planning to\nparticipant recruitment and facilitation, offering a grounded account of what\nhappens when participation does not go as expected. The paper examines the\nmethodological challenges of engaging non-expert participants, particularly\nwhen operating without institutional support, financial incentives, or\nintegration into larger events. Despite initial difficulties such as low\nattendance, the workshop fostered rich discussions among a demographically\ndiverse group and ultimately led to one participant volunteering to\nco-facilitate a subsequent session. This transition from participant to\nco-facilitator exemplifies the redistribution of epistemic authority,\npositioning lived experience as central to research and engagement practices.\nBy reframing perceived failure as a productive site of learning, the paper\noffers practical strategies for ECRs working across disciplines who often\nnavigate unfamiliar methodological terrains, contributing to broader\nconversations on the realities of doing interdisciplinary, participatory work\nin practice.", "AI": {"tldr": "\u672c\u6587\u53cd\u601d\u4e86\u8bbe\u8ba1\u548c\u4e3b\u5bfc\u5171\u540c\u8bbe\u8ba1\u4e0e\u53c2\u4e0e\u5f0f\u7814\u8ba8\u4f1a\u7684\u5e38\u89c1\u6311\u6218\uff0c\u4e3a\u65e9\u671f\u804c\u4e1a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9e\u7528\u7b56\u7565\u3002", "motivation": "\u63a2\u8ba8\u5728\u7f3a\u4e4f\u673a\u6784\u652f\u6301\u6216\u6fc0\u52b1\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u6709\u6548\u4e0e\u975e\u4e13\u5bb6\u53c2\u4e0e\u8005\u4e92\u52a8\uff0c\u5e76\u91cd\u6784\u5931\u8d25\u4e3a\u5b66\u4e60\u673a\u4f1a\u3002", "method": "\u57fa\u4e8e\u4e2a\u4eba\u7ecf\u9a8c\uff0c\u8be6\u8ff0\u4ece\u7814\u8ba8\u4f1a\u6982\u5ff5\u5316\u5230\u5b9e\u65bd\u7684\u5b8c\u6574\u8fc7\u7a0b\uff0c\u5305\u62ec\u53c2\u4e0e\u8005\u7684\u52a8\u6001\u53d8\u5316\u3002", "result": "\u7814\u8ba8\u4f1a\u867d\u521d\u671f\u53c2\u4e0e\u5ea6\u4f4e\uff0c\u4f46\u6700\u7ec8\u4fc3\u6210\u591a\u5143\u8ba8\u8bba\u5e76\u5438\u5f15\u53c2\u4e0e\u8005\u8f6c\u4e3a\u5171\u540c\u4e3b\u5bfc\u8005\u3002", "conclusion": "\u901a\u8fc7\u91cd\u6784\u5931\u8d25\u4e3a\u5b66\u4e60\u70b9\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u8df5\u7b56\u7565\uff0c\u5f3a\u8c03\u4e86\u751f\u6d3b\u7ecf\u9a8c\u5728\u7814\u7a76\u4e2d\u7684\u6838\u5fc3\u5730\u4f4d\u3002"}}
{"id": "2507.22895", "pdf": "https://arxiv.org/pdf/2507.22895", "abs": "https://arxiv.org/abs/2507.22895", "authors": ["Ye Sun", "Bowei Zhao", "Dezhong Yao", "Rui Zhang", "Bohan Zhang", "Xiaoyuan Li", "Jing Wang", "Mingxuan Qu", "Gang Liu"], "title": "Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface", "categories": ["cs.HC"], "comment": "18 pages, 9 figures", "summary": "Brain-computer interfaces (BCIs) enable real-time interaction between the\nbrain and external devices by decoding neural signals. However, existing\nmotor-based BCI paradigms, like motor imagery BCI, face challenges with\nimprecise labeling in real-world use. This mismatch between EEG signals and\ntrue behavioral intentions leads to pseudo-labels, undermining decoding\naccuracy and system robustness. To overcome this bottleneck, this paper first\nproposes a novel motor intention extraction framework based on a non-invasive\nbrain-muscle interface (BMuI)($\\text{BCI} =\n\\frac{\\text{Brain}}{\\text{Computer}} \\text{ Interface} =\n\\frac{\\text{Brain}}{\\not\\text{Muscle}}\\! \\text{ (BMuI)} \\times\n\\!\\frac{\\not\\text{Muscle}}{\\text{Computer}}\\! \\text{ Interface}$). This method\nsimulates the neural pathway from the brain to the muscles in order to capture\nand enhance the weak motor intention signals originating in the brain. It then\nuses EMG as a high-fidelity relay medium to achieve more accurate intention\nrecognition and transmission. To systematically validate the feasibility and\neffectiveness of this approach, we conducted both offline experiments (to\nrepeatedly verify feasibility) and online experiments (to construct a real-time\ninteractive system and evaluate its performance). The results show that BMuI is\nfeasible, achieving a prediction accuracy of 0.8314; in the online experiment,\nall participants are able to successfully control the Unity virtual arm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u4fb5\u5165\u5f0f\u8111\u808c\u8089\u63a5\u53e3\uff08BMuI\uff09\u7684\u65b0\u578b\u8fd0\u52a8\u610f\u56fe\u63d0\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8111\u7535\u4fe1\u53f7\uff08EEG\uff09\u548c\u808c\u7535\u4fe1\u53f7\uff08EMG\uff09\u63d0\u9ad8\u610f\u56fe\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8fd0\u52a8\u7684\u8111\u673a\u63a5\u53e3\uff08\u5982\u8fd0\u52a8\u60f3\u8c61BCI\uff09\u56e0\u6807\u7b7e\u4e0d\u7cbe\u786e\u5bfc\u81f4\u89e3\u7801\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u8fd0\u52a8\u610f\u56fe\u4fe1\u53f7\u3002", "method": "\u8bba\u6587\u63d0\u51faBMuI\u6846\u67b6\uff0c\u6a21\u62df\u5927\u8111\u5230\u808c\u8089\u7684\u795e\u7ecf\u901a\u8def\uff0c\u5229\u7528EMG\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u4e2d\u7ee7\u4ecb\u8d28\uff0c\u901a\u8fc7\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "result": "BMuI\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u8fbe\u52300.8314\uff0c\u5728\u7ebf\u5b9e\u9a8c\u4e2d\u6240\u6709\u53c2\u4e0e\u8005\u6210\u529f\u63a7\u5236Unity\u865a\u62df\u624b\u81c2\u3002", "conclusion": "BMuI\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709BCI\u7cfb\u7edf\u4e2d\u610f\u56fe\u4fe1\u53f7\u63d0\u53d6\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u5b9e\u65f6\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.22896", "pdf": "https://arxiv.org/pdf/2507.22896", "abs": "https://arxiv.org/abs/2507.22896", "authors": ["Kohou Wang", "ZhaoXiang Liu", "Lin Bai", "Kun Fan", "Xiang Liu", "Huan Hu", "Kai Wang", "Shiguo Lian"], "title": "iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.RO"], "comment": "17 pages, 12 figures", "summary": "It is crucial that robots' performance can be improved after deployment, as\nthey are inherently likely to encounter novel scenarios never seen before. This\npaper presents an innovative solution: an interactive learning-based robot\nsystem powered by a Multi-modal Large Language Model(MLLM). A key feature of\nour system is its ability to learn from natural dialogues with non-expert\nusers. We also propose chain of question to clarify the exact intent of the\nquestion before providing an answer and dual-modality retrieval modules to\nleverage these interaction events to avoid repeating same mistakes, ensuring a\nseamless user experience before model updates, which is in contrast to current\nmainstream MLLM-based robotic systems. Our system marks a novel approach in\nrobotics by integrating interactive learning, paving the way for superior\nadaptability and performance in diverse environments. We demonstrate the\neffectiveness and improvement of our method through experiments, both\nquantitively and qualitatively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u5b66\u4e60\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u901a\u8fc7\u4e0e\u7528\u6237\u7684\u81ea\u7136\u5bf9\u8bdd\u5b66\u4e60\uff0c\u5e76\u5229\u7528\u53cc\u6a21\u6001\u68c0\u7d22\u6a21\u5757\u907f\u514d\u91cd\u590d\u9519\u8bef\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u9002\u5e94\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u90e8\u7f72\u540e\u5e38\u9047\u5230\u65b0\u573a\u666f\uff0c\u9700\u6301\u7eed\u6539\u8fdb\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u4ea4\u4e92\u5f0f\u5b66\u4e60\u548c\u53cc\u6a21\u6001\u68c0\u7d22\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u7136\u5bf9\u8bdd\u5b66\u4e60\u7528\u6237\u610f\u56fe\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u4e0a\u5747\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u7684\u4ea4\u4e92\u5b66\u4e60\u80fd\u529b\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u73af\u5883\u9002\u5e94\u6027\u548c\u6027\u80fd\u63d0\u5347\u8def\u5f84\u3002"}}
{"id": "2507.23335", "pdf": "https://arxiv.org/pdf/2507.23335", "abs": "https://arxiv.org/abs/2507.23335", "authors": ["Qilin Zhou", "Haipeng Wang", "Zhengyuan Wei", "W. K. Chan"], "title": "Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions", "categories": ["cs.LG", "cs.SE"], "comment": "accepted by QRS 2025", "summary": "Patch robustness certification is an emerging verification approach for\ndefending against adversarial patch attacks with provable guarantees for deep\nlearning systems. Certified recovery techniques guarantee the prediction of the\nsole true label of a certified sample. However, existing techniques, if\napplicable to top-k predictions, commonly conduct pairwise comparisons on those\nvotes between labels, failing to certify the sole true label within the top k\nprediction labels precisely due to the inflation on the number of votes\ncontrolled by the attacker (i.e., attack budget); yet enumerating all\ncombinations of vote allocation suffers from the combinatorial explosion\nproblem. We propose CostCert, a novel, scalable, and precise voting-based\ncertified recovery defender. CostCert verifies the true label of a sample\nwithin the top k predictions without pairwise comparisons and combinatorial\nexplosion through a novel design: whether the attack budget on the sample is\ninfeasible to cover the smallest total additional votes on top of the votes\nuncontrollable by the attacker to exclude the true labels from the top k\nprediction labels. Experiments show that CostCert significantly outperforms the\ncurrent state-of-the-art defender PatchGuard, such as retaining up to 57.3% in\ncertified accuracy when the patch size is 96, whereas PatchGuard has already\ndropped to zero.", "AI": {"tldr": "Patch robustness certification\u662f\u4e00\u79cd\u65b0\u5174\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u9632\u5fa1\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u5e76\u786e\u4fdd\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u7684\u53ef\u8bc1\u660e\u5b89\u5168\u6027\u3002CostCert\u662f\u4e00\u79cd\u65b0\u578b\u3001\u53ef\u6269\u5c55\u4e14\u7cbe\u786e\u7684\u8ba4\u8bc1\u6062\u590d\u6280\u672f\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u9002\u7528\u4e8etop-k\u9884\u6d4b\u65f6\uff0c\u901a\u5e38\u8fdb\u884c\u6807\u7b7e\u95f4\u7684\u4e24\u4e24\u6bd4\u8f83\uff0c\u7531\u4e8e\u653b\u51fb\u8005\u7684\u63a7\u5236\uff08\u653b\u51fb\u9884\u7b97\uff09\uff0c\u65e0\u6cd5\u7cbe\u786e\u8ba4\u8bc1top k\u9884\u6d4b\u6807\u7b7e\u4e2d\u7684\u552f\u4e00\u771f\u5b9e\u6807\u7b7e\u3002", "method": "CostCert\u901a\u8fc7\u9a8c\u8bc1\u653b\u51fb\u9884\u7b97\u662f\u5426\u4e0d\u8db3\u4ee5\u8986\u76d6\u6700\u5c0f\u7684\u603b\u9644\u52a0\u7968\u6570\u6765\u6392\u9664\u771f\u5b9e\u6807\u7b7e\uff0c\u907f\u514d\u4e86\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCostCert\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u6280\u672fPatchGuard\uff0c\u4f8b\u5982\u5728\u8865\u4e01\u5927\u5c0f\u4e3a96\u65f6\u4ecd\u7136\u4fdd\u630157.3%\u7684\u8ba4\u8bc1\u51c6\u786e\u7387\uff0c\u800cPatchGuard\u5df2\u964d\u81f3\u96f6\u3002", "conclusion": "CostCert\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u8ba4\u8bc1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.22897", "pdf": "https://arxiv.org/pdf/2507.22897", "abs": "https://arxiv.org/abs/2507.22897", "authors": ["Luyu Chen", "Quanyu Dai", "Zeyu Zhang", "Xueyang Feng", "Mingyu Zhang", "Pengcheng Tang", "Xu Chen", "Yue Zhu", "Zhenhua Dong"], "title": "RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted by TheWebConf'25 Industry Track", "summary": "Conversational recommender systems (CRS) enhance user experience through\nmulti-turn interactions, yet evaluating CRS remains challenging. User\nsimulators can provide comprehensive evaluations through interactions with CRS,\nbut building realistic and diverse simulators is difficult. While recent work\nleverages large language models (LLMs) to simulate user interactions, they\nstill fall short in emulating individual real users across diverse scenarios\nand lack explicit rating mechanisms for quantitative evaluation. To address\nthese gaps, we propose RecUserSim, an LLM agent-based user simulator with\nenhanced simulation realism and diversity while providing explicit scores.\nRecUserSim features several key modules: a profile module for defining\nrealistic and diverse user personas, a memory module for tracking interaction\nhistory and discovering unknown preferences, and a core action module inspired\nby Bounded Rationality theory that enables nuanced decision-making while\ngenerating more fine-grained actions and personalized responses. To further\nenhance output control, a refinement module is designed to fine-tune final\nresponses. Experiments demonstrate that RecUserSim generates diverse,\ncontrollable outputs and produces realistic, high-quality dialogues, even with\nsmaller base LLMs. The ratings generated by RecUserSim show high consistency\nacross different base LLMs, highlighting its effectiveness for CRS evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86RecUserSim\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7528\u6237\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u66f4\u771f\u5b9e\u3001\u591a\u6837\u5730\u6a21\u62df\u7528\u6237\u5bf9\u8bdd\uff0c\u5e76\u63d0\u4f9b\u663e\u5f0f\u8bc4\u5206\uff0c\u4ee5\u6539\u8fdb\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u8bc4\u4f30\u7f3a\u4e4f\u771f\u5b9e\u4e14\u591a\u6837\u5316\u7684\u7528\u6237\u6a21\u62df\u5668\uff0c\u5e76\u4e14\u7f3a\u4e4f\u663e\u5f0f\u8bc4\u5206\u673a\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "RecUserSim\u5305\u62ec\u591a\u4e2a\u6a21\u5757\uff1a\u7528\u6237\u753b\u50cf\u6a21\u5757\u3001\u8bb0\u5fc6\u6a21\u5757\u3001\u6838\u5fc3\u52a8\u4f5c\u6a21\u5757\uff08\u57fa\u4e8e\u6709\u9650\u7406\u6027\u7406\u8bba\uff09\u548c\u7ec6\u5316\u6a21\u5757\uff0c\u4ee5\u751f\u6210\u66f4\u771f\u5b9e\u548c\u4e2a\u6027\u5316\u7684\u5bf9\u8bdd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRecUserSim\u80fd\u591f\u751f\u6210\u591a\u6837\u4e14\u53ef\u63a7\u7684\u8f93\u51fa\uff0c\u5373\u4f7f\u57fa\u4e8e\u8f83\u5c0f\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4e5f\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u5bf9\u8bdd\uff0c\u8bc4\u5206\u7684\u8de8\u6a21\u578b\u4e00\u81f4\u6027\u8f83\u9ad8\u3002", "conclusion": "RecUserSim\u5728\u6a21\u62df\u7528\u6237\u884c\u4e3a\u548c\u63d0\u4f9b\u663e\u5f0f\u8bc4\u5206\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2507.22898", "pdf": "https://arxiv.org/pdf/2507.22898", "abs": "https://arxiv.org/abs/2507.22898", "authors": ["Julian Acosta", "Scott Adams", "Julius Kernbach", "Romain Hardy", "Sung Eun Kim", "Luyang Luo", "Xiaoman Zhang", "Shreya Johri", "Mohammed Baharoon", "Pranav Rajpurkar"], "title": "Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "We developed a voice-driven artificial intelligence (AI) system that guides\nanyone - from paramedics to family members - through expert-level stroke\nevaluations using natural conversation, while also enabling smartphone video\ncapture of key examination components for documentation and potential expert\nreview. This addresses a critical gap in emergency care: current stroke\nrecognition by first responders is inconsistent and often inaccurate, with\nsensitivity for stroke detection as low as 58%, causing life-threatening delays\nin treatment. Three non-medical volunteers used our AI system to assess ten\nsimulated stroke patients, including cases with likely large vessel occlusion\n(LVO) strokes and stroke-like conditions, while we measured diagnostic\naccuracy, completion times, user confidence, and expert physician review of the\nAI-generated reports. The AI system correctly identified 84% of individual\nstroke signs and detected 75% of likely LVOs, completing evaluations in just\nover 6 minutes. Users reported high confidence (median 4.5/5) and ease of use\n(mean 4.67/5). The system successfully identified 86% of actual strokes but\nalso incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert\nphysician reviewed the AI reports with videos, they identified the correct\ndiagnosis in 100% of cases, but felt confident enough to make preliminary\ntreatment decisions in only 40% of cases due to observed AI errors including\nincorrect scoring and false information. While the current system's limitations\nnecessitate human oversight, ongoing rapid advancements in speech-to-speech AI\nmodels suggest that future versions are poised to enable highly accurate\nassessments. Achieving human-level voice interaction could transform emergency\nmedical care, putting expert-informed assessment capabilities in everyone's\nhands.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u57fa\u4e8e\u8bed\u97f3\u7684AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u6307\u5bfc\u975e\u4e13\u4e1a\u4eba\u5458\u5b8c\u6210\u4e2d\u98ce\u8bc4\u4f30\uff0c\u6548\u679c\u663e\u8457\u4f46\u4ecd\u9700\u4eba\u5de5\u76d1\u7763\u3002", "motivation": "\u89e3\u51b3\u6025\u6551\u4e2d\u4e2d\u98ce\u8bc6\u522b\u4e0d\u4e00\u81f4\u4e14\u51c6\u786e\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u81ea\u7136\u5bf9\u8bdd\u548c\u667a\u80fd\u624b\u673a\u89c6\u9891\u6355\u83b7\u6280\u672f\uff0c\u7531\u975e\u533b\u7597\u5fd7\u613f\u8005\u4f7f\u7528AI\u7cfb\u7edf\u8bc4\u4f30\u6a21\u62df\u4e2d\u98ce\u60a3\u8005\u3002", "result": "AI\u7cfb\u7edf\u8bc6\u522b\u4e2d\u98ce\u75c7\u72b6\u7684\u51c6\u786e\u7387\u4e3a84%\uff0c\u4f46\u5b58\u5728\u8bef\u62a5\uff0c\u4e13\u5bb6\u5ba1\u67e5\u540e\u8bca\u65ad\u51c6\u786e\u7387100%\u3002", "conclusion": "\u5f53\u524d\u7cfb\u7edf\u9700\u4eba\u5de5\u76d1\u7763\uff0c\u4f46\u672a\u6765\u6709\u671b\u901a\u8fc7\u8bed\u97f3AI\u6a21\u578b\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u3002"}}
{"id": "2507.22899", "pdf": "https://arxiv.org/pdf/2507.22899", "abs": "https://arxiv.org/abs/2507.22899", "authors": ["Ivan A. Hanono Cozzetti", "Ahmad Abdou"], "title": "A visual analytics tool for taxonomy-based trajectory data exploration", "categories": ["cs.HC"], "comment": "71 pages, 92 figures", "summary": "The analysis of spatio-temporal data presents significant challenges due to\nthe complexity and heterogeneity of movement patterns. This project proposes a\ndata analytics tool that combines data visualization and statistical\ncomputation to facilitate spatio-temporal data analysis through a multi-level\napproach. The tool categorizes moving objects into distinct taxonomies using\nMachine Learning models, adding meaningful structure to the analysis. Two case\nstudies demonstrate the methodology's effectiveness. The first analyzed Arctic\nfox trajectories, successfully identifying and labeling foxes with Geometric or\nKinematic-based behaviors, further categorized into Curvature and Acceleration\ngroups. Statistical indicators revealed that foxes with Acceleration-based\nbehavior showed constant, steady acceleration, while those with Curvature-based\nbehavior exhibited acceleration peaks and sudden deceleration. The second case\nstudy examined tropical cyclone data, labeling trajectories with Speed,\nCurvature, and hybrid Geometric-based behaviors through unique statistical\nvariables. Analysis of hybrid Geometric behavior (Curvature and Indentation\ncombined) identified specific angles with the highest impact on hurricane shape\nand geometry. The proposed method and tool demonstrate that spatio-temporal\ndata, despite inherent complexity, can be analyzed and explained in detail,\nproviding a theoretical and practical blueprint applicable to multiple domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u53ef\u89c6\u5316\u548c\u7edf\u8ba1\u8ba1\u7b97\u7684\u7a7a\u95f4\u65f6\u7a7a\u6570\u636e\u5206\u6790\u5de5\u5177\uff0c\u901a\u8fc7\u591a\u7ea7\u65b9\u6cd5\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u7c7b\u79fb\u52a8\u5bf9\u8c61\uff0c\u5e76\u5728\u5317\u6781\u72d0\u548c\u70ed\u5e26\u6c14\u65cb\u7684\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7a7a\u95f4\u65f6\u7a7a\u6570\u636e\u7531\u4e8e\u5176\u590d\u6742\u6027\u548c\u5f02\u8d28\u6027\uff0c\u5206\u6790\u96be\u5ea6\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u5de5\u5177\u6765\u7b80\u5316\u548c\u7ed3\u6784\u5316\u5206\u6790\u8fc7\u7a0b\u3002", "method": "\u5de5\u5177\u7ed3\u5408\u6570\u636e\u53ef\u89c6\u5316\u548c\u7edf\u8ba1\u8ba1\u7b97\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u79fb\u52a8\u5bf9\u8c61\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u591a\u7ea7\u65b9\u6cd5\u5206\u6790\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u6210\u529f\u5206\u7c7b\u5e76\u6807\u8bb0\u4e86\u5317\u6781\u72d0\u7684\u70ed\u5e26\u6c14\u65cb\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u7c7b\u578b\u884c\u4e3a\u7684\u7edf\u8ba1\u7279\u5f81\u3002", "conclusion": "\u8be5\u5de5\u5177\u548c\u65b9\u6cd5\u8bc1\u660e\u4e86\u7a7a\u95f4\u65f6\u7a7a\u6570\u636e\u7684\u53ef\u5206\u6790\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8de8\u9886\u57df\u5e94\u7528\u7684\u84dd\u56fe\u3002"}}
{"id": "2507.22900", "pdf": "https://arxiv.org/pdf/2507.22900", "abs": "https://arxiv.org/abs/2507.22900", "authors": ["Sergio Rojas-Galeano"], "title": "Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This exploratory study examines how AI code assistants shape novice\nprogrammers' experiences during a two-part exam in an introductory programming\ncourse. In the first part, students completed a programming task with access to\nAI support; in the second, they extended their solutions without AI. We\ncollected Likert-scale and open-ended responses from 20 students to evaluate\ntheir perceptions and challenges. Findings suggest that AI tools were perceived\nas helpful for understanding code and increasing confidence, particularly\nduring initial development. However, students reported difficulties\ntransferring knowledge to unaided tasks, revealing possible overreliance and\ngaps in conceptual understanding. These insights highlight the need for\npedagogical strategies that integrate AI meaningfully while reinforcing\nfoundational programming skills.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8AI\u4ee3\u7801\u52a9\u624b\u5982\u4f55\u5f71\u54cd\u7f16\u7a0b\u65b0\u624b\u7684\u8003\u8bd5\u4f53\u9a8c\uff0c\u53d1\u73b0AI\u5de5\u5177\u80fd\u63d0\u5347\u4fe1\u5fc3\u4f46\u53ef\u80fd\u5bfc\u81f4\u4f9d\u8d56\u548c\u6982\u5ff5\u7406\u89e3\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7d22AI\u5de5\u5177\u5bf9\u7f16\u7a0b\u521d\u5b66\u8005\u5b66\u4e60\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u8003\u8bd5\u60c5\u5883\u4e2d\u3002", "method": "\u901a\u8fc7\u4e24\u90e8\u5206\u8003\u8bd5\u8bbe\u8ba1\uff08\u4e00\u90e8\u5206\u6709AI\u652f\u6301\uff0c\u53e6\u4e00\u90e8\u5206\u65e0\uff09\u548c20\u540d\u5b66\u751f\u7684\u95ee\u5377\u8c03\u67e5\uff08Likert\u91cf\u8868\u548c\u5f00\u653e\u5f0f\u95ee\u9898\uff09\u6536\u96c6\u6570\u636e\u3002", "result": "AI\u5de5\u5177\u88ab\u8ba4\u4e3a\u6709\u52a9\u4e8e\u4ee3\u7801\u7406\u89e3\u548c\u63d0\u5347\u4fe1\u5fc3\uff0c\u4f46\u5728\u65e0AI\u652f\u6301\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u77e5\u8bc6\u8fc1\u79fb\u56f0\u96be\u548c\u6982\u5ff5\u7406\u89e3\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u7ed3\u5408AI\u5de5\u5177\u7684\u6559\u5b66\u7b56\u7565\uff0c\u540c\u65f6\u52a0\u5f3a\u57fa\u7840\u7f16\u7a0b\u6280\u80fd\u7684\u57f9\u517b\u3002"}}
{"id": "2507.22901", "pdf": "https://arxiv.org/pdf/2507.22901", "abs": "https://arxiv.org/abs/2507.22901", "authors": ["Shingo Hattori", "Takefumi Hiraki"], "title": "Accelerated and Optimized Search of Imperceptible Color Vibration for Embedding Information into LCD images", "categories": ["cs.HC"], "comment": "Presented at ACM SIGGRAPH Asia 2022 Posters", "summary": "Large, high-resolution displays are installed throughout the city as public\ndisplays. By superimposing invisible information on the images of these\ndisplays, large numbers of devices with cameras and sensors can communicate\nwith the displays without prior pairing. Several applications have been\nproposed, such as operating robots or communicating information to users by\ndisplaying 2D codes on images. However, the display of 2D codes has the problem\nof compromising the appearance of displayed content.\n  Abe et al. proposed a method of communicating with devices by superimposing\ninvisible information using color vibration on images displayed on\noff-the-shelf liquid-crystal displays (LCD). Using this method, we can embed\nthe information for devices in images without interfering with the displayed\ncontent. Abe et al. uses a simple serial loop operation to search for color\npairs comprising a color vibration, which requires a very long processing time\ndue to the huge search space.\n  In this paper, we propose an accelerated and optimized search method for\ncolor pairs that constitute the imperceptible color vibration for embedding\ninformation on LCD images. To achieve fast color pair search, we parallelized\nthe search process, which is previously done individually, by using arrays\nrepresenting the amount of movement and an operation to extract elements from\nthe array that satisfy the conditions. In addition, we investigate the amount\nof information that can be superimposed on nine color images using the\nimperceptible color vibration and clarify the applicability of embedding\ninformation into images using the color vibration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u901f\u4f18\u5316\u7684\u989c\u8272\u5bf9\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728LCD\u56fe\u50cf\u4e0a\u5d4c\u5165\u4e0d\u53ef\u89c1\u7684\u989c\u8272\u632f\u52a8\u4fe1\u606f\uff0c\u4ee5\u51cf\u5c11\u5904\u7406\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u4e32\u884c\u5faa\u73af\u64cd\u4f5c\u641c\u7d22\u6784\u6210\u989c\u8272\u632f\u52a8\u7684\u989c\u8272\u5bf9\uff0c\u7531\u4e8e\u641c\u7d22\u7a7a\u95f4\u5de8\u5927\uff0c\u5904\u7406\u65f6\u95f4\u8fc7\u957f\u3002", "method": "\u901a\u8fc7\u5e76\u884c\u5316\u641c\u7d22\u8fc7\u7a0b\uff0c\u5e76\u5229\u7528\u8868\u793a\u79fb\u52a8\u91cf\u7684\u6570\u7ec4\u548c\u6761\u4ef6\u63d0\u53d6\u64cd\u4f5c\uff0c\u52a0\u901f\u989c\u8272\u5bf9\u7684\u641c\u7d22\u3002", "result": "\u6210\u529f\u51cf\u5c11\u4e86\u989c\u8272\u5bf9\u641c\u7d22\u7684\u5904\u7406\u65f6\u95f4\uff0c\u5e76\u5728\u4e5d\u79cd\u5f69\u8272\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u53e0\u52a0\u7684\u4fe1\u606f\u91cf\uff0c\u5c55\u793a\u4e86\u989c\u8272\u632f\u52a8\u5d4c\u5165\u4fe1\u606f\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e76\u884c\u4f18\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u989c\u8272\u5bf9\u641c\u7d22\u7684\u6548\u7387\uff0c\u4e3a\u5728\u516c\u5171\u663e\u793a\u5c4f\u4e0a\u5d4c\u5165\u4e0d\u53ef\u89c1\u4fe1\u606f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.22902", "pdf": "https://arxiv.org/pdf/2507.22902", "abs": "https://arxiv.org/abs/2507.22902", "authors": ["Hashim Hayat", "Maksim Kudrautsau", "Evgeniy Makarov", "Vlad Melnichenko", "Tim Tsykunou", "Piotr Varaksin", "Matt Pavelle", "Adam Z. Oskowitz"], "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Background: Globally we face a projected shortage of 11 million healthcare\npractitioners by 2030, and administrative burden consumes 50% of clinical time.\nArtificial intelligence (AI) has the potential to help alleviate these\nproblems. However, no end-to-end autonomous large language model (LLM)-based AI\nsystem has been rigorously evaluated in real-world clinical practice. In this\nstudy, we evaluated whether a multi-agent LLM-based AI framework can function\nautonomously as an AI doctor in a virtual urgent care setting. Methods: We\nretrospectively compared the performance of the multi-agent AI system Doctronic\nand board-certified clinicians across 500 consecutive urgent-care telehealth\nencounters. The primary end points: diagnostic concordance, treatment plan\nconsistency, and safety metrics, were assessed by blinded LLM-based\nadjudication and expert human review. Results: The top diagnosis of Doctronic\nand clinician matched in 81% of cases, and the treatment plan aligned in 99.2%\nof cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not\nsupported by clinical findings). In an expert review of discordant cases, AI\nperformance was superior in 36.1%, and human performance was superior in 9.3%;\nthe diagnoses were equivalent in the remaining cases. Conclusions: In this\nfirst large-scale validation of an autonomous AI doctor, we demonstrated strong\ndiagnostic and treatment plan concordance with human clinicians, with AI\nperformance matching and in some cases exceeding that of practicing clinicians.\nThese findings indicate that multi-agent AI systems achieve comparable clinical\ndecision-making to human providers and offer a potential solution to healthcare\nworkforce shortages.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u7cfb\u7edf\uff08Doctronic\uff09\u5728\u865a\u62df\u6025\u8bca\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u8bca\u65ad\u548c\u6cbb\u7597\u8ba1\u5212\u4e0e\u4eba\u7c7b\u533b\u751f\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5168\u7403\u9884\u8ba1\u52302030\u5e74\u5c06\u77ed\u7f3a1100\u4e07\u533b\u7597\u4ece\u4e1a\u8005\uff0c\u4e14\u4e34\u5e8a\u65f6\u95f4\u4e2d50%\u7528\u4e8e\u884c\u653f\u8d1f\u62c5\u3002AI\u53ef\u80fd\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5c1a\u65e0\u7aef\u5230\u7aef\u81ea\u4e3b\u7684AI\u7cfb\u7edf\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u88ab\u4e25\u683c\u8bc4\u4f30\u3002", "method": "\u56de\u987e\u6027\u6bd4\u8f83\u4e86500\u6b21\u865a\u62df\u6025\u8bca\u4e2dDoctronic\u4e0e\u8ba4\u8bc1\u4e34\u5e8a\u533b\u751f\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u76f2\u6cd5LLM\u88c1\u51b3\u548c\u4e13\u5bb6\u8bc4\u5ba1\u8bc4\u4f30\u8bca\u65ad\u4e00\u81f4\u6027\u3001\u6cbb\u7597\u8ba1\u5212\u4e00\u81f4\u6027\u548c\u5b89\u5168\u6027\u3002", "result": "Doctronic\u4e0e\u533b\u751f\u7684\u8bca\u65ad\u4e00\u81f4\u7387\u4e3a81%\uff0c\u6cbb\u7597\u8ba1\u5212\u4e00\u81f4\u7387\u4e3a99.2%\u3002AI\u572836.1%\u7684\u4e0d\u4e00\u81f4\u75c5\u4f8b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f18\u4e8e\u4eba\u7c7b\u76849.3%\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5728\u4e34\u5e8a\u51b3\u7b56\u4e0a\u8868\u73b0\u4e0e\u4eba\u7c7b\u533b\u751f\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u53ef\u80fd\u6210\u4e3a\u89e3\u51b3\u533b\u7597\u52b3\u52a8\u529b\u77ed\u7f3a\u7684\u65b9\u6848\u3002"}}
{"id": "2507.22903", "pdf": "https://arxiv.org/pdf/2507.22903", "abs": "https://arxiv.org/abs/2507.22903", "authors": ["Andrew Blair", "Peggy Gregory", "Mary Ellen Foster"], "title": "A blessing or a burden? Exploring worker perspectives of using a social robot in a church", "categories": ["cs.HC", "cs.RO"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (ROMAN)", "summary": "Recent technological advances have allowed robots to assist in the service\nsector, and consequently accelerate job and sector transformation. Less\nattention has been paid to the use of robots in real-world organisations where\nsocial benefits, as opposed to profits, are the primary motivator. To explore\nthese opportunities, we have partnered with a working church and visitor\nattraction. We conducted interviews with 15 participants from a range of\nstakeholder groups within the church to understand worker perspectives of\nintroducing a social robot to the church and analysed the results using\nreflexive thematic analysis. Findings indicate mixed responses to the use of a\nrobot, with participants highlighting the empathetic responsibility the church\nhas towards people and the potential for unintended consequences. However,\ninformation provision and alleviation of menial or mundane tasks were\nidentified as potential use cases. This highlights the need to consider not\nonly the financial aspects of robot introduction, but also how social and\nintangible values shape what roles a robot should take on within an\norganisation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u5728\u975e\u8425\u5229\u7ec4\u7ec7\uff08\u5982\u6559\u4f1a\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u53c2\u4e0e\u8005\u5bf9\u5176\u4f7f\u7528\u6709\u8912\u6709\u8d2c\uff0c\u5f3a\u8c03\u9700\u8003\u8651\u793e\u4f1a\u4ef7\u503c\u4ee5\u53ca\u673a\u5668\u4eba\u89d2\u8272\u3002", "motivation": "\u7814\u7a76\u793e\u4ea4\u673a\u5668\u4eba\u5728\u975e\u8425\u5229\u7ec4\u7ec7\uff08\u5982\u6559\u4f1a\uff09\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u5173\u6ce8\u793e\u4f1a\u5229\u76ca\u800c\u975e\u7ecf\u6d4e\u5229\u76ca\u3002", "method": "\u4e0e\u4e00\u5bb6\u6d3b\u8dc3\u7684\u6559\u4f1a\u5408\u4f5c\uff0c\u5bf915\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u8bbf\u8c08\uff0c\u91c7\u7528\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\u6cd5\u3002", "result": "\u53c2\u4e0e\u8005\u5bf9\u673a\u5668\u4eba\u4f7f\u7528\u53cd\u5e94\u4e0d\u4e00\uff0c\u8ba4\u4e3a\u6559\u4f1a\u5728\u60c5\u611f\u8d23\u4efb\u4e0a\u9700\u8c28\u614e\uff0c\u4f46\u4e5f\u8ba4\u53ef\u5176\u5728\u4fe1\u606f\u63d0\u4f9b\u548c\u51cf\u5c11\u7e41\u7410\u4efb\u52a1\u4e0a\u7684\u6f5c\u529b\u3002", "conclusion": "\u673a\u5668\u4eba\u5f15\u5165\u9700\u517c\u987e\u793e\u4f1a\u4ef7\u503c\u548c\u65e0\u5f62\u5229\u76ca\uff0c\u800c\u4e0d\u4ec5\u662f\u8d22\u52a1\u56e0\u7d20\u3002"}}
{"id": "2507.22904", "pdf": "https://arxiv.org/pdf/2507.22904", "abs": "https://arxiv.org/abs/2507.22904", "authors": ["Ehsan Latif", "Zirak Khan", "Xiaoming Zhai"], "title": "SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches", "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to NeurIPS2025", "summary": "Scientific sketches (e.g., models) offer a powerful lens into students'\nconceptual understanding, yet AI-powered automated assessment of such\nfree-form, visually diverse artifacts remains a critical challenge. Existing\nsolutions often treat sketch evaluation as either an image classification task\nor monolithic vision-language models, which lack interpretability, pedagogical\nalignment, and adaptability across cognitive levels. To address these\nlimitations, we present SketchMind, a cognitively grounded, multi-agent\nframework for evaluating and improving student-drawn scientific sketches.\nSketchMind comprises modular agents responsible for rubric parsing, sketch\nperception, cognitive alignment, and iterative feedback with sketch\nmodification, enabling personalized and transparent evaluation. We evaluate\nSketchMind on a curated dataset of 3,575 student-generated sketches across six\nscience assessment items with different highest order of Bloom's level that\nrequire students to draw models to explain phenomena. Compared to baseline\nGPT-4o performance without SRG (average accuracy: 55.6%), and with SRG\nintegration achieves 77.1% average accuracy (+21.4% average absolute gain). We\nalso demonstrate that multi-agent orchestration with SRG enhances SketchMind\nperformance, for example, GPT-4.1 gains an average 8.9% increase in sketch\nprediction accuracy, outperforming single-agent pipelines across all items.\nHuman evaluators rated the feedback and co-created sketches generated by\n\\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5,\nsignificantly higher than those of baseline models (e.g., 2.3 for GPT-4o).\nExperts noted the system's potential to meaningfully support conceptual growth\nthrough guided revision. Our code and (pending approval) dataset will be\nreleased to support reproducibility and future research in AI-driven education.", "AI": {"tldr": "SketchMind\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u4ee3\u7406\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u548c\u6539\u8fdb\u5b66\u751f\u7684\u79d1\u5b66\u8349\u56fe\u3002\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b83\u5728\u51c6\u786e\u6027\u3001\u89e3\u91ca\u6027\u548c\u6559\u5b66\u9002\u5e94\u6027\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u8349\u56fe\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u89e3\u91ca\u6027\u3001\u6559\u5b66\u5bf9\u9f50\u6027\u548c\u8de8\u8ba4\u77e5\u5c42\u6b21\u7684\u9002\u5e94\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "SketchMind\u91c7\u7528\u6a21\u5757\u5316\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5305\u62ec\u8bc4\u5206\u89e3\u6790\u3001\u8349\u56fe\u611f\u77e5\u3001\u8ba4\u77e5\u5bf9\u9f50\u548c\u8fed\u4ee3\u53cd\u9988\u7b49\u6a21\u5757\u3002", "result": "SketchMind\u57286\u9879\u79d1\u5b66\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523077.1%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff08\u5982GPT-4o\u768455.6%\uff09\u3002", "conclusion": "SketchMind\u5c55\u793a\u4e86\u5728AI\u9a71\u52a8\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u901a\u8fc7\u4e2a\u6027\u5316\u53cd\u9988\u652f\u6301\u5b66\u751f\u7684\u6982\u5ff5\u6210\u957f\u3002"}}
{"id": "2507.22905", "pdf": "https://arxiv.org/pdf/2507.22905", "abs": "https://arxiv.org/abs/2507.22905", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "title": "Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction", "categories": ["cs.HC"], "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into robotic\nsystems, their potential to generate socially and culturally appropriate\naffective touch remains largely unexplored. This study investigates whether\nLLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive\ntactile behaviours to convey emotions in human-robot interaction. We produced\ntext based touch descriptions for 12 distinct emotions across three cultural\ncontexts (Chinese, Belgian, and unspecified), and examined their\ninterpretability in both robot-to-human and human-to-robot scenarios. A total\nof 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified)\nevaluated these LLM-generated tactile behaviours for emotional decoding and\nperceived appropriateness. Results reveal that: (1) under matched cultural\nconditions, participants successfully decoded six out of twelve emotions-mainly\nsocially oriented emotions such as love and Ekman emotions such as anger,\nhowever, self-focused emotions like pride and embarrassment were more difficult\nto interpret; (2) tactile behaviours were perceived as more appropriate when\ndirected from human to robot than from robot to human, revealing an asymmetry\nin social expectations based on interaction roles; (3) behaviours interpreted\nas aggressive (e.g., anger), overly intimate (e.g., love), or emotionally\nambiguous (i.e., not clearly decodable) were significantly more likely to be\nrated as inappropriate; and (4) cultural mismatches reduced decoding accuracy\nand increased the likelihood of behaviours being judged as inappropriate.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86GPT-3.5\u3001GPT-4\u548cGPT-4o\u7b49\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6587\u5316\u9002\u5e94\u6027\u89e6\u89c9\u884c\u4e3a\u4ee5\u4f20\u9012\u60c5\u611f\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6587\u5316\u5339\u914d\u3001\u4ea4\u4e92\u89d2\u8272\u548c\u6587\u5316\u5dee\u5f02\u5bf9\u884c\u4e3a\u89e3\u8bfb\u548c\u9002\u5b9c\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u751f\u6210\u6587\u5316\u548c\u793e\u4ea4\u9002\u5b9c\u6027\u89e6\u89c9\u884c\u4e3a\u7684\u6f5c\u529b\uff0c\u586b\u8865\u76f8\u5173\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u751f\u621012\u79cd\u60c5\u611f\u5728\u4e2d\u3001\u6bd4\u548c\u65e0\u7279\u5b9a\u6587\u5316\u80cc\u666f\u4e0b\u7684\u89e6\u89c9\u63cf\u8ff0\uff0c\u901a\u8fc790\u540d\u53c2\u4e0e\u8005\u8bc4\u4f30\u8fd9\u4e9b\u884c\u4e3a\u7684\u60c5\u89e3\u8bfb\u548c\u9002\u5b9c\u6027\u3002", "result": "\u6587\u5316\u5339\u914d\u4e0b\u6210\u529f\u89e3\u8bfb6\u79cd\u60c5\u611f\uff0c\u4ea4\u4e92\u89d2\u8272\u5f71\u54cd\u9002\u5b9c\u6027\u611f\u77e5\uff0c\u6587\u5316\u4e0d\u5339\u914d\u964d\u4f4e\u89e3\u8bfb\u51c6\u786e\u6027\u548c\u9002\u5b9c\u6027\u3002", "conclusion": "LLMs\u80fd\u751f\u6210\u6587\u5316\u9002\u5e94\u6027\u89e6\u89c9\u884c\u4e3a\uff0c\u4f46\u9700\u8003\u8651\u6587\u5316\u5dee\u5f02\u548c\u4ea4\u4e92\u89d2\u8272\uff0c\u4ee5\u63d0\u5347\u9002\u7528\u6027\u548c\u7528\u6237\u63a5\u53d7\u5ea6\u3002"}}
{"id": "2507.22952", "pdf": "https://arxiv.org/pdf/2507.22952", "abs": "https://arxiv.org/abs/2507.22952", "authors": ["Harry Shomer", "Jiejun Xu"], "title": "Automated Label Placement on Maps via Large Language Models", "categories": ["cs.HC", "cs.CV", "cs.LG"], "comment": "Workshop on AI for Data Editing (AI4DE) at KDD 2025", "summary": "Label placement is a critical aspect of map design, serving as a form of\nspatial annotation that directly impacts clarity and interpretability. Despite\nits importance, label placement remains largely manual and difficult to scale,\nas existing automated systems struggle to integrate cartographic conventions,\nadapt to context, or interpret labeling instructions. In this work, we\nintroduce a new paradigm for automatic label placement (ALP) that formulates\nthe task as a data editing problem and leverages large language models (LLMs)\nfor context-aware spatial annotation. To support this direction, we curate\nMAPLE, the first known benchmarking dataset for evaluating ALP on real-world\nmaps, encompassing diverse landmark types and label placement annotations from\nopen-source data. Our method retrieves labeling guidelines relevant to each\nlandmark type leveraging retrieval-augmented generation (RAG), integrates them\ninto prompts, and employs instruction-tuned LLMs to generate ideal label\ncoordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall\nperformance and generalization across different types of landmarks. This\nincludes both zero-shot and instruction-tuned performance. Our results\ndemonstrate that LLMs, when guided by structured prompts and domain-specific\nretrieval, can learn to perform accurate spatial edits, aligning the generated\noutputs with expert cartographic standards. Overall, our work presents a\nscalable framework for AI-assisted map finishing and demonstrates the potential\nof foundation models in structured data editing tasks. The code and data can be\nfound at https://github.com/HarryShomer/MAPLE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u6807\u7b7e\u653e\u7f6e\uff08ALP\uff09\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4efb\u52a1\u6570\u636e\u5316\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7a7a\u95f4\u6807\u6ce8\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u624b\u52a8\u6807\u7b7e\u653e\u7f6e\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u771f\u5b9e\u5730\u56fe\u57fa\u51c6\u6570\u636e\u96c6MAPLE\u3002", "motivation": "\u5f53\u524d\u6807\u7b7e\u653e\u7f6e\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u64cd\u4f5c\uff0c\u81ea\u52a8\u5316\u7cfb\u7edf\u96be\u4ee5\u6574\u5408\u5236\u56fe\u89c4\u8303\u6216\u9002\u5e94\u4e0a\u4e0b\u6587\uff0c\u4e9f\u9700\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u6807\u7b7e\u653e\u7f6e\u4efb\u52a1\u8f6c\u5316\u4e3a\u6570\u636e\u7f16\u8f91\u95ee\u9898\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ed3\u5408\u9886\u57df\u6307\u5357\uff0c\u901a\u8fc7LLM\u751f\u6210\u7406\u60f3\u6807\u7b7e\u5750\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u5728\u7ed3\u6784\u5316\u63d0\u793a\u548c\u9886\u57df\u68c0\u7d22\u5f15\u5bfc\u4e0b\u80fd\u5b9e\u73b0\u7cbe\u51c6\u7684\u7a7a\u95f4\u7f16\u8f91\uff0c\u7b26\u5408\u4e13\u5bb6\u5236\u56fe\u6807\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAI\u8f85\u52a9\u5730\u56fe\u5236\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6570\u636e\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.23096", "pdf": "https://arxiv.org/pdf/2507.23096", "abs": "https://arxiv.org/abs/2507.23096", "authors": ["Tom Peterka", "Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Cory Quammen", "Berk Geveci"], "title": "ChatVis: Large Language Model Agent for Generating Scientific Visualizations", "categories": ["cs.HC"], "comment": null, "summary": "Large language models (LLMs) are rapidly increasing in capability, but they\nstill struggle with highly specialized programming tasks such as scientific\nvisualization. We present an LLM assistant, ChatVis, that aids the LLM to\ngenerate Python code for ParaView scientific visualization tasks, without the\nneed for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought\nprompt simplification, retrieval-augmented prompt generation using a vector\ndatabase of documentation and code examples, and error checking with iterative\nprompt feedback to correct errors until a visualization is produced. An\nintegral part of our approach is a benchmark suite of canonical visualization\ntasks, ParaView regression tests, and scientific use cases that includes\ncomprehensive evaluation metrics. We evaluate our visualization assistant by\ncomparing results with a variety of top-performing unassisted LLMs. We find\nthat all the metrics are significantly improved with ChatVis.", "AI": {"tldr": "ChatVis\u662f\u4e00\u6b3e\u8f85\u52a9LLM\u751f\u6210ParaView\u79d1\u5b66\u53ef\u89c6\u5316Python\u4ee3\u7801\u7684\u5de5\u5177\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03LLM\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e13\u4e1a\u7f16\u7a0b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u9ad8\u5ea6\u4e13\u4e1a\u5316\u7f16\u7a0b\u4efb\u52a1\uff08\u5982\u79d1\u5b66\u53ef\u89c6\u5316\uff09\u4e2d\u7684\u8868\u73b0\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u91c7\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u7b80\u5316\u3001\u57fa\u4e8e\u5411\u91cf\u6570\u636e\u5e93\u7684\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u751f\u6210\u4ee5\u53ca\u8fed\u4ee3\u53cd\u9988\u9519\u8bef\u68c0\u67e5\u3002", "result": "\u4e0e\u672a\u7ecf\u8f85\u52a9\u7684\u9876\u7ea7LLM\u76f8\u6bd4\uff0cChatVis\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ChatVis\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u79d1\u5b66\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2507.23190", "pdf": "https://arxiv.org/pdf/2507.23190", "abs": "https://arxiv.org/abs/2507.23190", "authors": ["William Huang", "Xia Su", "Jon E. Froehlich", "Yang Zhang"], "title": "Accessibility Scout: Personalized Accessibility Scans of Built Environments", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.MA"], "comment": "18 pages, 16 figures. Presented at ACM UIST 2025", "summary": "Assessing the accessibility of unfamiliar built environments is critical for\npeople with disabilities. However, manual assessments, performed by users or\ntheir personal health professionals, are laborious and unscalable, while\nautomatic machine learning methods often neglect an individual user's unique\nneeds. Recent advances in Large Language Models (LLMs) enable novel approaches\nto this problem, balancing personalization with scalability to enable more\nadaptive and context-aware assessments of accessibility. We present\nAccessibility Scout, an LLM-based accessibility scanning system that identifies\naccessibility concerns from photos of built environments. With use,\nAccessibility Scout becomes an increasingly capable \"accessibility scout\",\ntailoring accessibility scans to an individual's mobility level, preferences,\nand specific environmental interests through collaborative Human-AI\nassessments. We present findings from three studies: a formative study with six\nparticipants to inform the design of Accessibility Scout, a technical\nevaluation of 500 images of built environments, and a user study with 10\nparticipants of varying mobility. Results from our technical evaluation and\nuser study show that Accessibility Scout can generate personalized\naccessibility scans that extend beyond traditional ADA considerations. Finally,\nwe conclude with a discussion on the implications of our work and future steps\nfor building more scalable and personalized accessibility assessments of the\nphysical world.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7cfb\u7edfAccessibility Scout\uff0c\u901a\u8fc7\u7167\u7247\u8bc6\u522b\u5efa\u7b51\u73af\u5883\u7684\u65e0\u969c\u788d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u4e0e\u53ef\u6269\u5c55\u6027\u7ed3\u5408\u7684\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5de5\u548c\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u6b8b\u969c\u4eba\u58eb\u5bf9\u4e2a\u6027\u5316\u65e0\u969c\u788d\u73af\u5883\u7684\u9700\u6c42\uff0cLLMs\u7684\u8fdb\u5c55\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u5f00\u53d1\u4e86Accessibility Scout\u7cfb\u7edf\uff0c\u5229\u7528LLMs\u4ece\u7167\u7247\u4e2d\u8bc6\u522b\u65e0\u969c\u788d\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u5b9e\u73b0\u4e2a\u6027\u5316\u8bc4\u4f30\u3002", "result": "\u6280\u672f\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u751f\u6210\u8d85\u8d8a\u4f20\u7edfADA\u6807\u51c6\u7684\u4e2a\u6027\u5316\u65e0\u969c\u788d\u626b\u63cf\u62a5\u544a\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86LLMs\u5728\u4e2a\u6027\u5316\u65e0\u969c\u788d\u8bc4\u4f30\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.23215", "pdf": "https://arxiv.org/pdf/2507.23215", "abs": "https://arxiv.org/abs/2507.23215", "authors": ["Junyong Park", "Saelyne Yang", "Sungho Jo"], "title": "Silent Impact: Tracking Tennis Shots from the Passive Arm", "categories": ["cs.HC", "H.5.2; I.5.4"], "comment": "15 pages, 9 figures,", "summary": "Wearable technology has transformed sports analytics, offering new dimensions\nin enhancing player experience. Yet, many solutions involve cumbersome setups\nthat inhibit natural motion. In tennis, existing products require sensors on\nthe racket or dominant arm, causing distractions and discomfort. We propose\nSilent Impact, a novel and user-friendly system that analyzes tennis shots\nusing a sensor placed on the passive arm. Collecting Inertial Measurement Unit\nsensor data from 20 recreational tennis players, we developed neural networks\nthat exclusively utilize passive arm data to detect and classify six shots,\nachieving a classification accuracy of 88.2% and a detection F1 score of 86.0%,\ncomparable to the dominant arm. These models were then incorporated into an\nend-to-end prototype, which records passive arm motion through a smartwatch and\ndisplays a summary of shots on a mobile app. User study (N=10) showed that\nparticipants felt less burdened physically and mentally using Silent Impact on\nthe passive arm. Overall, our research establishes the passive arm as an\neffective, comfortable alternative for tennis shot analysis, advancing\nuser-friendly sports analytics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u7528\u6237\u53cb\u597d\u7684\u7cfb\u7edfSilent Impact\uff0c\u901a\u8fc7\u88ab\u52a8\u624b\u81c2\u4f20\u611f\u5668\u5206\u6790\u7f51\u7403\u51fb\u7403\u52a8\u4f5c\uff0c\u51cf\u5c11\u4e86\u5bf9\u81ea\u7136\u8fd0\u52a8\u7684\u5e72\u6270\u3002", "motivation": "\u73b0\u6709\u7f51\u7403\u5206\u6790\u8bbe\u5907\u9700\u5728\u7403\u62cd\u6216\u4e3b\u52a8\u624b\u81c2\u5b89\u88c5\u4f20\u611f\u5668\uff0c\u5bfc\u81f4\u4e0d\u9002\u548c\u5e72\u6270\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u8212\u9002\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u752820\u540d\u4e1a\u4f59\u7f51\u7403\u9009\u624b\u7684\u88ab\u52a8\u624b\u81c2IMU\u4f20\u611f\u5668\u6570\u636e\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u68c0\u6d4b\u548c\u5206\u7c7b\u516d\u79cd\u51fb\u7403\u52a8\u4f5c\uff0c\u5e76\u5c06\u6a21\u578b\u96c6\u6210\u5230\u667a\u80fd\u624b\u8868\u548c\u624b\u673a\u5e94\u7528\u539f\u578b\u4e2d\u3002", "result": "\u5206\u7c7b\u51c6\u786e\u738788.2%\uff0c\u68c0\u6d4bF1\u5206\u657086.0%\uff0c\u7528\u6237\u7814\u7a76\u4e2d\u53c2\u4e0e\u8005\u611f\u5230\u8eab\u5fc3\u8d1f\u62c5\u66f4\u8f7b\u3002", "conclusion": "\u88ab\u52a8\u624b\u81c2\u662f\u4e00\u79cd\u6709\u6548\u4e14\u8212\u9002\u7684\u5206\u6790\u7f51\u7403\u51fb\u7403\u52a8\u4f5c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u7528\u6237\u53cb\u597d\u7684\u8fd0\u52a8\u5206\u6790\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2507.23298", "pdf": "https://arxiv.org/pdf/2507.23298", "abs": "https://arxiv.org/abs/2507.23298", "authors": ["Kazushi Kato", "Koji Inoue", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "title": "Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System", "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": "Accepted by 27th ACM International Conference on Multimodal\n  Interaction (ICMI '25), Long paper", "summary": "In human dialogue, nonverbal information such as nodding and facial\nexpressions is as crucial as verbal information, and spoken dialogue systems\nare also expected to express such nonverbal behaviors. We focus on nodding,\nwhich is critical in an attentive listening system, and propose a model that\npredicts both its timing and type in real time. The proposed model builds on\nthe voice activity projection (VAP) model, which predicts voice activity from\nboth listener and speaker audio. We extend it to prediction of various types of\nnodding in a continuous and real-time manner unlike conventional models. In\naddition, the proposed model incorporates multi-task learning with verbal\nbackchannel prediction and pretraining on general dialogue data. In the timing\nand type prediction task, the effectiveness of multi-task learning was\nsignificantly demonstrated. We confirmed that reducing the processing rate\nenables real-time operation without a substantial drop in accuracy, and\nintegrated the model into an avatar attentive listening system. Subjective\nevaluations showed that it outperformed the conventional method, which always\ndoes nodding in sync with verbal backchannel. The code and trained models are\navailable at https://github.com/MaAI-Kyoto/MaAI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u9884\u6d4b\u70b9\u5934\u65f6\u673a\u548c\u7c7b\u578b\u7684\u6a21\u578b\uff0c\u57fa\u4e8e\u58f0\u97f3\u6d3b\u52a8\u6295\u5f71\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u4e0e\u9884\u5904\u7406\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u975e\u8bed\u8a00\u4fe1\u606f\uff08\u5982\u70b9\u5934\uff09\u5728\u4eba\u673a\u5bf9\u8bdd\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u7cfb\u7edf\u9700\u8981\u66f4\u81ea\u7136\u7684\u8868\u8fbe\u65b9\u5f0f\u3002", "method": "\u6269\u5c55\u58f0\u97f3\u6d3b\u52a8\u6295\u5f71\u6a21\u578b\uff0c\u5b9e\u73b0\u8fde\u7eed\u5b9e\u65f6\u7684\u70b9\u5934\u9884\u6d4b\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u9884\u5904\u7406\u3002", "result": "\u591a\u4efb\u52a1\u5b66\u4e60\u6548\u679c\u663e\u8457\uff0c\u964d\u4f4e\u5904\u7406\u7387\u53ef\u4fdd\u6301\u5b9e\u65f6\u6027\uff0c\u4e3b\u89c2\u8bc4\u4ef7\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u578b\u5728\u5b9e\u65f6\u70b9\u5934\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53ef\u96c6\u6210\u5230\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u63d0\u5347\u4ea4\u4e92\u81ea\u7136\u5ea6\u3002"}}
{"id": "2507.23470", "pdf": "https://arxiv.org/pdf/2507.23470", "abs": "https://arxiv.org/abs/2507.23470", "authors": ["Sebastian G\u00fcrtl", "Gloria Schimetta", "David Kerschbaumer", "Michael Liut", "Alexander Steinmaurer"], "title": "Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": "Learnersourcing: Student-generated Content @ Scale Workshop at L@S\n  2025", "summary": "UML and ER diagrams are foundational in computer science education but come\nwith challenges for learners due to the need for abstract thinking, contextual\nunderstanding, and mastery of both syntax and semantics. These complexities are\ndifficult to address through traditional teaching methods, which often struggle\nto provide scalable, personalized feedback, especially in large classes. We\nintroduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,\nwhich converts a reference diagram and a student-submitted diagram into a\ntextual representation and provides structured feedback based on the\ndifferences. It uses a multi-stage LLM pipeline to compare diagrams and\ngenerate reflective feedback. Furthermore, the tool enables analytical insights\nfor educators, aiming to foster self-directed learning and inform instructional\nstrategies. We evaluated DUET through semi-structured interviews with six\nparticipants, including two educators and four teaching assistants. They\nidentified strengths such as accessibility, scalability, and learning support\nalongside limitations, including reliability and potential misuse. Participants\nalso suggested potential improvements, such as bulk upload functionality and\ninteractive clarification features. DUET presents a promising direction for\nintegrating LLMs into modeling education and offers a foundation for future\nclassroom integration and empirical evaluation.", "AI": {"tldr": "TL;DR: \u8bba\u6587\u4ecb\u7ecd\u4e86DUET\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u6bd4\u8f83UML\u548cER\u56fe\u7684\u6587\u672c\u8868\u793a\u63d0\u4f9b\u7ed3\u6784\u5316\u53cd\u9988\uff0c\u652f\u6301\u4e2a\u6027\u5316\u5b66\u4e60\u3002\u521d\u6b65\u8bc4\u4f30\u663e\u793a\u5176\u5728\u53ef\u8bbf\u95ee\u6027\u548c\u6269\u5c55\u6027\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u53ef\u9760\u6027\u548c\u8bef\u7528\u95ee\u9898\u3002", "motivation": "UML\u548cER\u56fe\u6559\u5b66\u4e2d\u5b58\u5728\u62bd\u8c61\u601d\u7ef4\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u6311\u6218\uff0c\u4f20\u7edf\u6559\u5b66\u65b9\u6cd5\u96be\u4ee5\u63d0\u4f9b\u5927\u89c4\u6a21\u548c\u4e2a\u6027\u5316\u7684\u53cd\u9988\u3002DUET\u65e8\u5728\u901a\u8fc7LLM\u6280\u672f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u652f\u6301\u81ea\u6211\u5b66\u4e60\u548c\u6307\u5bfc\u7b56\u7565\u3002", "method": "DUET\u4f7f\u7528\u591a\u9636\u6bb5LLM\u7ba1\u9053\uff0c\u5c06\u53c2\u8003\u56fe\u548c\u5b66\u751f\u7684\u56fe\u8f6c\u6362\u4e3a\u6587\u672c\u8868\u793a\uff0c\u57fa\u4e8e\u5dee\u5f02\u751f\u6210\u7ed3\u6784\u5316\u53cd\u9988\u3002\u5de5\u5177\u8fd8\u63d0\u4f9b\u6559\u80b2\u5206\u6790\u529f\u80fd\uff0c\u5e2e\u52a9\u6539\u8fdb\u6559\u5b66\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u516d\u540d\u53c2\u4e0e\u8005\u7684\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u8bc4\u4f30\uff0cDUET\u5728\u53ef\u8bbf\u95ee\u6027\u3001\u6269\u5c55\u6027\u548c\u5b66\u4e60\u652f\u6301\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4e5f\u5b58\u5728\u53ef\u9760\u6027\u548c\u6f5c\u5728\u8bef\u7528\u7684\u5c40\u9650\u6027\u3002", "conclusion": "DUET\u5c55\u793a\u4e86\u5c06LLM\u878d\u5165\u5efa\u6a21\u6559\u80b2\u7684\u524d\u666f\uff0c\u4e3a\u672a\u6765\u8bfe\u5802\u5e94\u7528\u548c\u5b9e\u8bc1\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.23492", "pdf": "https://arxiv.org/pdf/2507.23492", "abs": "https://arxiv.org/abs/2507.23492", "authors": ["Dominique Geissler", "Claire Robertson", "Stefan Feuerriegel"], "title": "Digital literacy interventions can boost humans in discerning deepfakes", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Deepfakes, i.e., images generated by artificial intelligence (AI), can erode\ntrust in institutions and compromise election outcomes, as people often\nstruggle to discern real images from deepfakes. Improving digital literacy can\nhelp address these challenges, yet scalable and effective approaches remain\nlargely unexplored. Here, we compare the efficacy of five digital literacy\ninterventions to boost people's ability to discern deepfakes: (1) textual\nguidance on common indicators of deepfakes; (2) visual demonstrations of these\nindicators; (3) a gamified exercise for identifying deepfakes; (4) implicit\nlearning through repeated exposure and feedback; and (5) explanations of how\ndeepfakes are generated with the help of AI. We conducted an experiment with\nN=1,200 participants from the United States to test the immediate and long-term\neffectiveness of our interventions. Our results show that our interventions can\nboost deepfake discernment by up to 13 percentage points while maintaining\ntrust in real images. Altogether, our approach is scalable, suitable for\ndiverse populations, and highly effective for boosting deepfake detection while\nmaintaining trust in truthful information.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e94\u79cd\u6570\u5b57\u7d20\u517b\u5e72\u9884\u63aa\u65bd\uff0c\u63d0\u5347\u4eba\u4eec\u8bc6\u522bDeepfake\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u65b9\u6cd5\u53ef\u63d0\u5347\u8bc6\u522b\u738713\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u771f\u5b9e\u4fe1\u606f\u7684\u4fe1\u4efb\u3002", "motivation": "Deepfake\u53ef\u80fd\u7834\u574f\u673a\u6784\u4fe1\u4efb\u548c\u9009\u4e3e\u7ed3\u679c\uff0c\u9700\u8981\u63a2\u7d22\u6709\u6548\u7684\u6570\u5b57\u7d20\u517b\u63d0\u5347\u65b9\u6cd5\u3002", "method": "\u6d4b\u8bd5\u4e86\u4e94\u79cd\u5e72\u9884\u63aa\u65bd\uff08\u6587\u672c\u5f15\u5bfc\u3001\u89c6\u89c9\u6f14\u793a\u3001\u6e38\u620f\u5316\u7ec3\u4e60\u3001\u9690\u5f0f\u5b66\u4e60\u3001\u751f\u6210\u89e3\u91ca\uff09\uff0c\u5e76\u901a\u8fc71,200\u540d\u7f8e\u56fd\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5e72\u9884\u63aa\u65bd\u53ef\u5c06Deepfake\u8bc6\u522b\u7387\u63d0\u534713\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u4e0d\u5f71\u54cd\u5bf9\u771f\u5b9e\u4fe1\u606f\u7684\u4fe1\u4efb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u3001\u9002\u7528\u4e8e\u591a\u6837\u5316\u4eba\u7fa4\uff0c\u80fd\u6709\u6548\u63d0\u5347Deepfake\u8bc6\u522b\u80fd\u529b\u5e76\u4fdd\u6301\u5bf9\u771f\u5b9e\u4fe1\u606f\u7684\u4fe1\u4efb\u3002"}}
{"id": "2507.23585", "pdf": "https://arxiv.org/pdf/2507.23585", "abs": "https://arxiv.org/abs/2507.23585", "authors": ["Sophia Liu", "Shm Garanganao Almeda"], "title": "Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web", "categories": ["cs.HC"], "comment": "To appear in: Adjunct Proceedings of the 36th ACM Conference on\n  Hypertext and Social Media, Chicago, IL, USA, September 15-18, 2025", "summary": "Today's algorithm-driven interfaces, from recommendation feeds to GenAI\ntools, often prioritize engagement and efficiency at the expense of user\nagency. As systems take on more decision-making, users have less control over\nwhat they see and how meaning or relationships between content are constructed.\nThis paper introduces \"Hypertextual Friction,\" a conceptual design stance that\nrepositions classical hypertext principles--friction, traceability, and\nstructure--as actionable values for reclaiming agency in algorithmically\nmediated environments. Through a comparative analysis of real-world\ninterfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image\ntools--we examine how different systems structure user experience, navigation,\nand authorship. We show that hypertext systems emphasize provenance,\nassociative thinking, and user-driven meaning-making, while algorithmic systems\ntend to obscure process and flatten participation. We contribute: (1) a\ncomparative analysis of how interface structures shape agency in user-driven\nversus agent-driven systems, and (2) a conceptual stance that offers\nhypertextual values as design commitments for reclaiming agency in an\nincreasingly algorithmic web.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u2018\u8d85\u6587\u672c\u6469\u64e6\u2019\u8bbe\u8ba1\u7406\u5ff5\uff0c\u65e8\u5728\u901a\u8fc7\u5f3a\u8c03\u53ef\u8ffd\u6eaf\u6027\u3001\u7ed3\u6784\u548c\u6469\u64e6\uff0c\u6765\u5728\u7b97\u6cd5\u9a71\u52a8\u7684\u754c\u9762\u4e2d\u6062\u590d\u7528\u6237\u63a7\u5236\u6743\u3002", "motivation": "\u5f53\u524d\u7b97\u6cd5\u9a71\u52a8\u7684\u754c\u9762\uff08\u5982\u63a8\u8350\u7cfb\u7edf\u548c\u751f\u6210\u5f0fAI\u5de5\u5177\uff09\u727a\u7272\u4e86\u7528\u6237\u81ea\u4e3b\u6743\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91cd\u65b0\u5f15\u5165\u7ecf\u5178\u8d85\u6587\u672c\u539f\u5219\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5bf9\u7ef4\u57fa\u767e\u79d1\u4e0eInstagram Explore\u3001Are.na\u4e0e\u751f\u6210\u5f0fAI\u56fe\u50cf\u5de5\u5177\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u7814\u7a76\u4e0d\u540c\u7cfb\u7edf\u5982\u4f55\u6784\u5efa\u7528\u6237\u4f53\u9a8c\u3001\u5bfc\u822a\u548c\u521b\u4f5c\u3002", "result": "\u8d85\u6587\u672c\u7cfb\u7edf\u5f3a\u8c03\u6765\u6e90\u3001\u5173\u8054\u601d\u7ef4\u548c\u7528\u6237\u9a71\u52a8\u7684\u610f\u4e49\u6784\u5efa\uff0c\u800c\u7b97\u6cd5\u7cfb\u7edf\u5219\u6a21\u7cca\u8fc7\u7a0b\u5e76\u524a\u5f31\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u8d85\u6587\u672c\u4ef7\u503c\u53ef\u4f5c\u4e3a\u8bbe\u8ba1\u539f\u5219\uff0c\u5e2e\u52a9\u5728\u7b97\u6cd5\u4e3b\u5bfc\u7684\u7f51\u7edc\u4e2d\u6062\u590d\u7528\u6237\u81ea\u4e3b\u6743\u3002"}}
{"id": "2507.22936", "pdf": "https://arxiv.org/pdf/2507.22936", "abs": "https://arxiv.org/abs/2507.22936", "authors": ["Md Talha Mohsin"], "title": "Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.HC", "q-fin.CP"], "comment": "22 Pages, 6 Tables, 7 Figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide variety of Financial Natural Language Processing (FinNLP) tasks.\nHowever, systematic comparisons among widely used LLMs remain underexplored.\nGiven the rapid advancement and growing influence of LLMs in financial\nanalysis, this study conducts a thorough comparative evaluation of five leading\nLLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the\n'Magnificent Seven' technology companies. We create a set of domain-specific\nprompts and then use three methodologies to evaluate model performance: human\nannotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,\nJaccard), and model behavior diagnostics (prompt-level variance and\nacross-model similarity). The results show that GPT gives the most coherent,\nsemantically aligned, and contextually relevant answers; followed by Claude and\nPerplexity. Gemini and DeepSeek, on the other hand, have more variability and\nless agreement. Also, the similarity and stability of outputs change from\ncompany to company and over time, showing that they are sensitive to how\nprompts are written and what source material is used.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e94\u79cd\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08GPT\u3001Claude\u3001Perplexity\u3001Gemini\u548cDeepSeek\uff09\u5728\u91d1\u878d\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\uff0c\u53d1\u73b0GPT\u7684\u8868\u73b0\u6700\u4f18\uff0c\u800cGemini\u548cDeepSeek\u7684\u8f93\u51fa\u53d8\u5f02\u6027\u8f83\u5927\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5bf9\u5176\u7cfb\u7edf\u6027\u7684\u6bd4\u8f83\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u3001\u81ea\u52a8\u5316\u8bed\u4e49\u5ea6\u91cf\uff08\u5982ROUGE\u3001\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001Jaccard\uff09\u548c\u6a21\u578b\u884c\u4e3a\u8bca\u65ad\u4e09\u79cd\u65b9\u6cd5\uff0c\u8bc4\u4f30\u6a21\u578b\u572810-K\u6587\u4ef6\u4e0a\u7684\u8868\u73b0\u3002", "result": "GPT\u751f\u6210\u7684\u56de\u7b54\u6700\u8fde\u8d2f\u4e14\u8bed\u4e49\u5bf9\u9f50\uff1bClaude\u548cPerplexity\u6b21\u4e4b\uff1bGemini\u548cDeepSeek\u8f93\u51fa\u53d8\u5f02\u6027\u5927\u4e14\u4e00\u81f4\u6027\u8f83\u4f4e\u3002", "conclusion": "\u6a21\u578b\u8868\u73b0\u53d7\u63d0\u793a\u8bbe\u8ba1\u548c\u6e90\u6750\u6599\u5f71\u54cd\u663e\u8457\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u9ad8\u5176\u5728\u91d1\u878d\u9886\u57df\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2507.22956", "pdf": "https://arxiv.org/pdf/2507.22956", "abs": "https://arxiv.org/abs/2507.22956", "authors": ["Dong Hyun Roh", "Rajesh Kumar", "An Ngo"], "title": "LLM-Assisted Cheating Detection in Korean Language via Keystrokes", "categories": ["cs.LG", "cs.HC", "K.3.1"], "comment": "This paper has 11 pages, 6 figures, 2 tables, and has been accepted\n  for publication at IEEE-IJCB 2025", "summary": "This paper presents a keystroke-based framework for detecting LLM-assisted\ncheating in Korean, addressing key gaps in prior research regarding language\ncoverage, cognitive context, and the granularity of LLM involvement. Our\nproposed dataset includes 69 participants who completed writing tasks under\nthree conditions: Bona fide writing, paraphrasing ChatGPT responses, and\ntranscribing ChatGPT responses. Each task spans six cognitive processes defined\nin Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and\ncreate). We extract interpretable temporal and rhythmic features and evaluate\nmultiple classifiers under both Cognition-Aware and Cognition-Unaware settings.\nTemporal features perform well under Cognition-Aware evaluation scenarios,\nwhile rhythmic features generalize better under cross-cognition scenarios.\nMoreover, detecting bona fide and transcribed responses was easier than\nparaphrased ones for both the proposed models and human evaluators, with the\nmodels significantly outperforming the humans. Our findings affirm that\nkeystroke dynamics facilitate reliable detection of LLM-assisted writing across\nvarying cognitive demands and writing strategies, including paraphrasing and\ntranscribing LLM-generated responses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u51fb\u952e\u7684\u52a8\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u97e9\u8bed\u4e2d\u57fa\u4e8eLLM\u7684\u4f5c\u5f0a\u884c\u4e3a\uff0c\u586b\u8865\u4e86\u5148\u524d\u7814\u7a76\u5728\u8bed\u8a00\u8986\u76d6\u3001\u8ba4\u77e5\u4e0a\u4e0b\u6587\u548cLLM\u53c2\u4e0e\u7c92\u5ea6\u4e0a\u7684\u7a7a\u767d\u3002", "motivation": "\u586b\u8865\u73b0\u6709\u7814\u7a76\u5728\u68c0\u6d4bLLM\u8f85\u52a9\u4f5c\u5f0a\u65f6\u7684\u8bed\u8a00\u8986\u76d6\u3001\u8ba4\u77e5\u4e0a\u4e0b\u6587\u548cLLM\u53c2\u4e0e\u7c92\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u62ec69\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u96c6\uff0c\u53c2\u4e0e\u8005\u5728\u4e09\u79cd\u6761\u4ef6\u4e0b\u5b8c\u6210\u5199\u4f5c\u4efb\u52a1\uff08\u771f\u5b9e\u5199\u4f5c\u3001\u6539\u5199ChatGPT\u56de\u7b54\u548c\u8f6c\u5f55ChatGPT\u56de\u7b54\uff09\uff0c\u5e76\u63d0\u53d6\u4e86\u65f6\u5e8f\u548c\u8282\u594f\u7279\u5f81\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u5206\u7c7b\u5668\u3002", "result": "\u65f6\u5e8f\u7279\u5f81\u5728\u8ba4\u77e5\u611f\u77e5\u573a\u666f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u8282\u594f\u7279\u5f81\u5728\u8de8\u8ba4\u77e5\u573a\u666f\u4e2d\u66f4\u5177\u6cdb\u5316\u6027\uff1b\u68c0\u6d4b\u771f\u5b9e\u548c\u8f6c\u5f55\u53cd\u5e94\u6bd4\u6539\u5199\u66f4\u5bb9\u6613\uff0c\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "conclusion": "\u51fb\u952e\u52a8\u6001\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u4e0d\u540c\u8ba4\u77e5\u9700\u6c42\u548c\u5199\u4f5c\u7b56\u7565\u4e0b\u7684LLM\u8f85\u52a9\u5199\u4f5c\uff0c\u5305\u62ec\u6539\u5199\u548c\u8f6c\u5f55\u3002"}}
{"id": "2507.23088", "pdf": "https://arxiv.org/pdf/2507.23088", "abs": "https://arxiv.org/abs/2507.23088", "authors": ["Lalithkumar Seenivasan", "Jiru Xu", "Roger D. Soberanis Mukul", "Hao Ding", "Grayson Byrd", "Yu-Chun Ku", "Jose L. Porras", "Masaru Ishii", "Mathias Unberath"], "title": "Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Emerging surgical data science and robotics solutions, especially those\ndesigned to provide assistance in situ, require natural human-machine\ninterfaces to fully unlock their potential in providing adaptive and intuitive\naid. Contemporary AI-driven solutions remain inherently rigid, offering limited\nflexibility and restricting natural human-machine interaction in dynamic\nsurgical environments. These solutions rely heavily on extensive task-specific\npre-training, fixed object categories, and explicit manual-prompting. This work\nintroduces a novel Perception Agent that leverages speech-integrated\nprompt-engineered large language models (LLMs), segment anything model (SAM),\nand any-point tracking foundation models to enable a more natural human-machine\ninteraction in real-time intraoperative surgical assistance. Incorporating a\nmemory repository and two novel mechanisms for segmenting unseen elements,\nPerception Agent offers the flexibility to segment both known and unseen\nelements in the surgical scene through intuitive interaction. Incorporating the\nability to memorize novel elements for use in future surgeries, this work takes\na marked step towards human-machine symbiosis in surgical procedures. Through\nquantitative analysis on a public dataset, we show that the performance of our\nagent is on par with considerably more labor-intensive manual-prompting\nstrategies. Qualitatively, we show the flexibility of our agent in segmenting\nnovel elements (instruments, phantom grafts, and gauze) in a custom-curated\ndataset. By offering natural human-machine interaction and overcoming rigidity,\nour Perception Agent potentially brings AI-based real-time assistance in\ndynamic surgical environments closer to reality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u548c\u5206\u5272\u6a21\u578b\u7684\u65b0\u578b\u611f\u77e5\u4ee3\u7406\uff0c\u7528\u4e8e\u5b9e\u65f6\u624b\u672f\u4e2d\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709AI\u89e3\u51b3\u65b9\u6848\u5728\u52a8\u6001\u624b\u672f\u73af\u5883\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u9650\u5236\u4e86\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u7ed3\u5408\u8bed\u97f3\u96c6\u6210\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u5206\u5272\u6a21\u578b\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u5b9e\u73b0\u5bf9\u5df2\u77e5\u548c\u672a\u77e5\u5143\u7d20\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u4e0e\u624b\u5de5\u63d0\u793a\u7b56\u7565\u6027\u80fd\u76f8\u5f53\uff0c\u5e76\u80fd\u7075\u6d3b\u5206\u5272\u65b0\u5143\u7d20\u3002", "conclusion": "\u8be5\u611f\u77e5\u4ee3\u7406\u4e3a\u52a8\u6001\u624b\u672f\u73af\u5883\u4e2d\u7684AI\u5b9e\u65f6\u8f85\u52a9\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002"}}
{"id": "2507.23544", "pdf": "https://arxiv.org/pdf/2507.23544", "abs": "https://arxiv.org/abs/2507.23544", "authors": ["Ryo Miyoshi", "Yuki Okafuji", "Takuya Iwamoto", "Junya Nakanishi", "Jun Baba"], "title": "User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "This paper has been accepted for presentation at IEEE/RSJ\n  International Conference on Intelligent Robots and Systems 2025 (IROS 2025)", "summary": "In recent years, the demand for social robots has grown, requiring them to\nadapt their behaviors based on users' states. Accurately assessing user\nexperience (UX) in human-robot interaction (HRI) is crucial for achieving this\nadaptability. UX is a multi-faceted measure encompassing aspects such as\nsentiment and engagement, yet existing methods often focus on these\nindividually. This study proposes a UX estimation method for HRI by leveraging\nmultimodal social signals. We construct a UX dataset and develop a\nTransformer-based model that utilizes facial expressions and voice for\nestimation. Unlike conventional models that rely on momentary observations, our\napproach captures both short- and long-term interaction patterns using a\nmulti-instance learning framework. This enables the model to capture temporal\ndynamics in UX, providing a more holistic representation. Experimental results\ndemonstrate that our method outperforms third-party human evaluators in UX\nestimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u793e\u4ea4\u4fe1\u53f7\u548cTransformer\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\uff08UX\uff09\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u4eba\u5de5\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u673a\u5668\u4eba\u9700\u6c42\u7684\u589e\u957f\uff0c\u51c6\u786e\u8bc4\u4f30\u7528\u6237\u4f53\u9a8c\uff08HRI\u4e2d\u7684UX\uff09\u4ee5\u5b9e\u73b0\u884c\u4e3a\u81ea\u9002\u5e94\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u5c40\u9650\u4e8e\u5355\u4e00\u5c42\u9762\uff08\u5982\u60c5\u611f\u6216\u53c2\u4e0e\u5ea6\uff09\uff0c\u7f3a\u4e4f\u5168\u9762\u6027\u3002", "method": "\u901a\u8fc7\u6784\u5efaUX\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u5229\u7528\u9762\u90e8\u8868\u60c5\u548c\u8bed\u97f3\u4fe1\u53f7\uff0c\u7ed3\u5408\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u6355\u6349\u77ed\u671f\u548c\u957f\u671f\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728UX\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u7b2c\u4e09\u65b9\u4eba\u5de5\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u53f7\u548c\u65f6\u6001\u52a8\u6001\u6355\u6349\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684UX\u8868\u5f81\uff0c\u6709\u671b\u63a8\u52a8HRI\u81ea\u9002\u5e94\u884c\u4e3a\u7684\u5b9e\u73b0\u3002"}}
{"id": "2507.23592", "pdf": "https://arxiv.org/pdf/2507.23592", "abs": "https://arxiv.org/abs/2507.23592", "authors": ["Haiyun Zhang", "Stefano Dalla Gasperina", "Saad N. Yousaf", "Toshimitsu Tsuboi", "Tetsuya Narita", "Ashish D. Deshpande"], "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "8 pages, 10 figures, submitted to RA-L", "summary": "Hand exoskeletons are critical tools for dexterous teleoperation and\nimmersive manipulation interfaces, but achieving accurate hand tracking remains\na challenge due to user-specific anatomical variability and donning\ninconsistencies. These issues lead to kinematic misalignments that degrade\ntracking performance and limit applicability in precision tasks. We propose a\nsubject-specific calibration framework for exoskeleton-based hand tracking that\nuses redundant joint sensing and a residual-weighted optimization strategy to\nestimate virtual link parameters. Implemented on the Maestro exoskeleton, our\nmethod improves joint angle and fingertip position estimation across users with\nvarying hand geometries. We introduce a data-driven approach to empirically\ntune cost function weights using motion capture ground truth, enabling more\naccurate and consistent calibration across participants. Quantitative results\nfrom seven subjects show substantial reductions in joint and fingertip tracking\nerrors compared to uncalibrated and evenly weighted models. Qualitative\nvisualizations using a Unity-based virtual hand further confirm improvements in\nmotion fidelity. The proposed framework generalizes across exoskeleton designs\nwith closed-loop kinematics and minimal sensing, and lays the foundation for\nhigh-fidelity teleoperation and learning-from-demonstration applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5197\u4f59\u5173\u8282\u4f20\u611f\u548c\u6b8b\u5dee\u52a0\u6743\u4f18\u5316\u7684\u624b\u90e8\u5916\u9aa8\u9abc\u6821\u51c6\u6846\u67b6\uff0c\u663e\u8457\u51cf\u5c11\u5173\u8282\u548c\u6307\u5c16\u8ddf\u8e2a\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u624b\u90e8\u5916\u9aa8\u9abc\u56e0\u7528\u6237\u89e3\u5256\u5dee\u5f02\u548c\u4f69\u6234\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5197\u4f59\u5173\u8282\u4f20\u611f\u548c\u6b8b\u5dee\u52a0\u6743\u4f18\u5316\u7b56\u7565\u4f30\u8ba1\u865a\u62df\u94fe\u63a5\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u6355\u6349\u6570\u636e\u8c03\u4f18\u6210\u672c\u51fd\u6570\u6743\u91cd\u3002", "result": "\u5728\u4e03\u540d\u53d7\u8bd5\u8005\u4e2d\uff0c\u5173\u8282\u548c\u6307\u5c16\u8ddf\u8e2a\u8bef\u5dee\u8f83\u672a\u6821\u51c6\u548c\u5747\u5300\u52a0\u6743\u6a21\u578b\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u95ed\u73af\u8fd0\u52a8\u5b66\u548c\u6700\u5c0f\u4f20\u611f\u7684\u591a\u79cd\u5916\u9aa8\u9abc\u8bbe\u8ba1\uff0c\u4e3a\u9ad8\u4fdd\u771f\u8fdc\u7a0b\u64cd\u4f5c\u548c\u793a\u8303\u5b66\u4e60\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.23756", "pdf": "https://arxiv.org/pdf/2507.23756", "abs": "https://arxiv.org/abs/2507.23756", "authors": ["Diana Mortagua"], "title": "Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System", "categories": ["cs.LG", "cs.HC"], "comment": null, "summary": "This study centers on overcoming the challenge of selecting the best\nannotators for each query in Active Learning (AL), with the objective of\nminimizing misclassifications. AL recognizes the challenges related to cost and\ntime when acquiring labeled data, and decreases the number of labeled data\nneeded. Nevertheless, there is still the necessity to reduce annotation errors,\naiming to be as efficient as possible, to achieve the expected accuracy faster.\nMost strategies for query-annotator pairs do not consider internal factors that\naffect productivity, such as mood, attention, motivation, and fatigue levels.\nThis work addresses this gap in the existing literature, by not only\nconsidering how the internal factors influence annotators (mood and fatigue\nlevels) but also presenting a new query-annotator pair strategy, using a\nKnowledge-Based Recommendation System (RS). The RS ranks the available\nannotators, allowing to choose one or more to label the queried instance using\ntheir past accuracy values, and their mood and fatigue levels, as well as\ninformation about the instance queried. This work bases itself on existing\nliterature on mood and fatigue influence on human performance, simulating\nannotators in a realistic manner, and predicting their performance with the RS.\nThe results show that considering past accuracy values, as well as mood and\nfatigue levels reduces the number of annotation errors made by the annotators,\nand the uncertainty of the model through its training, when compared to not\nusing internal factors. Accuracy and F1-score values were also better in the\nproposed approach, despite not being as substantial as the aforementioned. The\nmethodologies and findings presented in this study begin to explore the open\nchallenge of human cognitive factors affecting AL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u63a8\u8350\u7cfb\u7edf\u7684\u67e5\u8be2-\u6807\u6ce8\u8005\u5bf9\u7b56\u7565\uff0c\u7efc\u5408\u8003\u8651\u6807\u6ce8\u8005\u8fc7\u53bb\u51c6\u786e\u7387\u3001\u60c5\u7eea\u548c\u75b2\u52b3\u6c34\u5e73\uff0c\u4ee5\u51cf\u5c11\u6807\u6ce8\u9519\u8bef\u5e76\u63d0\u9ad8\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4e3b\u52a8\u5b66\u4e60\u5728\u51cf\u5c11\u6807\u6ce8\u6570\u636e\u9700\u6c42\u7684\u540c\u65f6\uff0c\u4ecd\u9762\u4e34\u6807\u6ce8\u9519\u8bef\u95ee\u9898\uff0c\u73b0\u6709\u7b56\u7565\u672a\u5145\u5206\u8003\u91cf\u5f71\u54cd\u6807\u6ce8\u8005\u751f\u4ea7\u529b\u7684\u5185\u90e8\u56e0\u7d20\uff08\u5982\u60c5\u7eea\u3001\u75b2\u52b3\uff09\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u63a8\u8350\u7cfb\u7edf\uff0c\u7ed3\u5408\u6807\u6ce8\u8005\u5386\u53f2\u51c6\u786e\u7387\u3001\u60c5\u7eea\u548c\u75b2\u52b3\u6c34\u5e73\uff0c\u4e3a\u5176\u6392\u5e8f\u5e76\u9009\u62e9\u6700\u4f73\u6807\u6ce8\u8005\u3002", "result": "\u65b0\u7b56\u7565\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u9519\u8bef\u548c\u6a21\u578b\u8bad\u7ec3\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u5728\u51c6\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u9996\u6b21\u63a2\u8ba8\u4e86\u4eba\u7c7b\u8ba4\u77e5\u56e0\u7d20\u5bf9\u4e3b\u52a8\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u9886\u57df\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
