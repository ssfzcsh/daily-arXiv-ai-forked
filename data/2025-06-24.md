<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 29]
- [cs.NI](#cs.NI) [Total: 12]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.HC](#cs.HC) [Total: 22]
- [cs.GR](#cs.GR) [Total: 10]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DB](#cs.DB) [Total: 10]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.CR](#cs.CR) [Total: 10]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.FL](#cs.FL) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.CV](#cs.CV) [Total: 7]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.CL](#cs.CL) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Challenges and Practices in Quantum Software Testing and Debugging: Insights from Practitioners](https://arxiv.org/abs/2506.17306)
*Jake Zappin,Trevor Stalnaker,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 量子软件工程面临测试和调试的独特挑战，开发者多依赖经典方法而非量子专用工具，亟需改进工具和工作流程。


<details>
  <summary>Details</summary>
Motivation: 量子计算从理论转向实现，开发者面临概率执行、有限可观测性等问题，亟需了解当前实践以改进工具。

Method: 通过调查26名量子软件开发者和后续访谈，聚焦测试、调试和常见挑战。

Result: 多数开发者使用经典测试方法，量子专用工具使用率低（31%），常见bug源于经典问题。

Conclusion: 需开发更贴合量子开发者需求的测试和调试工具，并提供实践导向的建议。

Abstract: Quantum software engineering is an emerging discipline with distinct
challenges, particularly in testing and debugging. As quantum computing
transitions from theory to implementation, developers face issues not present
in classical software development, such as probabilistic execution, limited
observability, shallow abstractions, and low awareness of quantum-specific
tools. To better understand current practices, we surveyed 26 quantum software
developers from academia and industry and conducted follow-up interviews
focused on testing, debugging, and recurring challenges. All participants
reported engaging in testing, with unit testing (88%), regression testing
(54%), and acceptance testing (54%) being the most common. However, only 31%
reported using quantum-specific testing tools, relying instead on manual
methods. Debugging practices were similarly grounded in classical strategies,
such as print statements, circuit visualizations, and simulators, which
respondents noted do not scale well. The most frequently cited sources of bugs
were classical in nature-library updates (81%), developer mistakes (68%), and
compatibility issues (62%)-often worsened by limited abstraction in existing
SDKs. These findings highlight the urgent need for better-aligned testing and
debugging tools, integrated more seamlessly into the workflows of quantum
developers. We present these results in detail and offer actionable
recommendations grounded in the real-world needs of practitioners.

</details>


### [2] [An Expert Survey on Models and Digital Twins](https://arxiv.org/abs/2506.17313)
*Jonathan Reif,Daniel Dittler,Milapji Singh Gill,Tamás Farkas,Valentin Stegmaier,Felix Gehlhoff,Tobias Kleinert,Michael Weyrich*

Main category: cs.SE

TL;DR: 研究通过专家调查揭示了数字孪生中数字模型集成的挑战,包括缺乏标准化接口、高人工适配成本和模型重用支持不足,提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数字孪生的广泛应用需集成多种数字模型,但行业对其挑战和研究需求缺乏系统了解。

Method: 通过跨领域的专家调查,分析数字模型在数字孪生中的应用挑战。

Result: 发现标准化接口缺失、手动适配成本高、模型重用支持有限等问题。

Conclusion: 未来需研究自动化模型组合和基于语义的互操作性。

Abstract: Digital Twins (DTs) are becoming increasingly vital for future industrial
applications, enhancing monitoring, control, and optimization of physical
assets. This enhancement is made possible by integrating various Digital Models
(DMs) within DTs, which must interoperate to represent different system aspects
and fulfill diverse application purposes. However, industry perspectives on the
challenges and research needs for integrating these models are rarely obtained.
Thus, this study conducts an expert survey across multiple application domains
to identify and analyze the challenges in utilizing diverse DMs within DTs. The
results reveal missing standardized interfaces, high manual adaptation effort,
and limited support for model reuse across lifecycle phases, highlighting
future research needs in automated model composition and semantics-based
interoperability.

</details>


### [3] [Large Language Models for Spreadsheets: Benchmarking Progress and Evaluating Performance with FLARE](https://arxiv.org/abs/2506.17330)
*Simon Thorne*

Main category: cs.SE

TL;DR: 本文提出了一个用于评估大型语言模型（LLMs）在电子表格任务中表现的基准框架FLARE，发现LLMs在简单任务中表现良好，但在复杂任务中存在不足。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在电子表格任务中的表现，填补现有研究空白。

Method: 设计了一个涵盖从基础公式生成到复杂场景的基准评估框架FLARE。

Result: LLMs在简单任务中表现良好，但在复杂任务中容易产生错误输出。

Conclusion: 当前的LLMs在需要精确逻辑推理的电子表格任务中存在局限性，需要整合符号推理能力。

Abstract: Large Language Models (LLMs) have demonstrated some significant capabilities
across various domains; however, their effectiveness in spreadsheet related
tasks remains underexplored. This study introduces a foundation for a
comprehensive benchmark framework to evaluate the performance of leading LLMs
in executing spreadsheet functions, formula generation and data manipulation
tasks. The benchmark encompasses tasks ranging from basic formula creation to
complex, real world spreadsheet scenarios. Our findings reveal that while LLMs
exhibit proficiency in straightforward tasks, they often falter in complex,
multi step operations, frequently producing plausible yet incorrect outputs.
These results underscore the limitations of current LLMs in handling
spreadsheet tasks that require precise logical reasoning and highlight the need
for integrating symbolic reasoning capabilities into LLM architectures. To
support this, we introduce FLARE (Formula Logic, Auditing, Reasoning and
Evaluation) a new benchmark for evaluating LLM performance on real-world
spreadsheet logic, auditing, and reasoning tasks.

</details>


### [4] [LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research](https://arxiv.org/abs/2506.17335)
*Shuo Yan,Ruochen Li,Ziming Luo,Zimu Wang,Daoyang Li,Liqiang Jing,Kaiyu He,Peilin Wu,George Michalopoulos,Yue Zhang,Ziyang Zhang,Mian Zhang,Zhiyu Chen,Xinya Du*

Main category: cs.SE

TL;DR: 论文提出LMR-BENCH基准，评估大语言模型（LLM）代理从NLP研究论文中复现代码的能力，发现现有模型在科学推理和代码合成方面仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型在科学发现中表现出巨大潜力，但其在从研究论文中复现代码这一基础但关键任务上的能力尚未充分探索，尤其是在NLP领域。

Method: 提出LMR-BENCH基准，包含28项代码复现任务，涵盖23篇过去五年顶级NLP会议论文的九类基础任务。模型需根据论文、含掩码函数的代码仓库和实现指令完成任务。

Result: 实验表明，即使在标准提示和最新LLM代理设置下，最先进的模型在科学推理和代码合成方面仍存在明显局限。

Conclusion: LLM代理在自主复现科学研究方面的能力仍有重大不足，需进一步提升科学推理和代码合成的能力。

Abstract: Large language model (LLM) agents have demonstrated remarkable potential in
advancing scientific discovery. However, their capability in the fundamental
yet crucial task of reproducing code from research papers, especially in the
NLP domain, remains underexplored. This task includes unique complex reasoning
challenges in the intellectual synthesis of abstract concepts and the
comprehension of code repositories with interdependent files. Motivated by this
gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the
capability of LLM agents on code reproduction from Language Modeling Research.
It consists of 28 code reproduction tasks derived from 23 research papers
published in top-tier NLP venues over the past five years, spanning nine
fundamental categories. Models are provided with a research paper, a code
repository containing one or more masked functions, and instructions for
implementing these functions. We conduct extensive experiments in standard
prompting and LLM agent settings with state-of-the-art LLMs, evaluating the
accuracy of unit tests and performing LLM-based evaluation of code correctness.
Experimental results reveal that even the most advanced models still exhibit
persistent limitations in scientific reasoning and code synthesis, highlighting
critical gaps in LLM agents' ability to autonomously reproduce scientific
research

</details>


### [5] [Re-Evaluating Code LLM Benchmarks Under Semantic Mutation](https://arxiv.org/abs/2506.17369)
*Zhiyuan Pan,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 该论文研究了代码基准测试中提示模板的敏感性，发现即使微小的提示变化也会导致模型性能显著波动，影响评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型时代，代码基准测试对评估模型能力至关重要，但现有基准测试通常依赖单一提示模板，可能导致评估结果不可靠。

Method: 作者提出一个通用框架，通过语义和结构保持的提示模板修改方法，在8个代码基准任务上对10个开源LLM进行了实验，每任务使用100个相似提示模板。

Result: 研究发现，即使细微的提示变化也会显著影响性能，并可能导致模型排名不一致。

Conclusion: 建议未来设计代码基准测试时需考虑提示敏感性，以提高评估的可靠性和准确性。

Abstract: In the era of large language models (LLMs), code benchmarks have become an
important research area in software engineering and are widely used by
practitioners. These benchmarks evaluate the performance of LLMs on specific
code-related tasks, such as code understanding and generation. A critical step
in constructing code benchmarks is the design of prompts. However, as existing
code benchmarks typically rely on a single prompt template per task, they are
prone to the issue of prompt sensitivity, where minor prompt variations could
result in substantial performance variations, leading to unreliable evaluations
of model capabilities.
  While previous studies have explored prompt sensitivity, their experimental
designs and findings are limited to traditional natural language processing
(NLP) tasks. In this paper, we present an empirical study to investigate prompt
sensitivity in code benchmarks. We first propose a general framework that
modifies prompt templates in a manner that preserves both their semantics and
their structure as much as possible. Based on the framework, we conduct
extensive experiments across eight code benchmark tasks on 10 representative
open-source LLMs, with each task featuring 100 semantically similar prompt
templates. We then analyze the evaluation results using various statistical
metrics, focusing on both absolute and relative model performance. Our findings
suggest that even slight prompt variations can lead to significant shifts in
performance. Additionally, we observe that such variations can introduce
inconsistencies in the performance rankings across different models. These
insights highlight the need for considering prompt sensitivity when designing
future code benchmarks, to ensure more reliable and accurate evaluation of LLM
capabilities.

</details>


### [6] [Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing](https://arxiv.org/abs/2506.17539)
*Sidong Feng,Changhao Du,Huaxiao Liu,Qingnan Wang,Zhengwei Lv,Mengfei Wang,Chunyang Chen*

Main category: cs.SE

TL;DR: MAdroid是一种基于大型语言模型的多代理方法，用于自动化多用户交互应用测试，解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 移动应用中的多用户交互功能（如视频通话、直播）在测试自动化中存在挑战，传统方法难以满足动态协作需求。

Method: MAdroid采用三种代理（协调者、操作者、观察者）协作完成任务，操作者模拟用户行为，协调者指导任务，观察者监控过程。

Result: 实验表明，MAdroid在41项任务中完成率达82.9%，动作相似度为96.8%，并成功识别11个回归测试中的交互性Bug。

Conclusion: MAdroid验证了多代理方法在自动化测试中的有效性，具有实际应用潜力。

Abstract: The growing dependence on mobile phones and their apps has made multi-user
interactive features, like chat calls, live streaming, and video conferencing,
indispensable for bridging the gaps in social connectivity caused by physical
and situational barriers. However, automating these interactive features for
testing is fraught with challenges, owing to their inherent need for timely,
dynamic, and collaborative user interactions, which current automated testing
methods inadequately address. Inspired by the concept of agents designed to
autonomously and collaboratively tackle problems, we propose MAdroid, a novel
multi-agent approach powered by the Large Language Models (LLMs) to automate
the multi-user interactive task for app feature testing. Specifically, MAdroid
employs two functional types of multi-agents: user agents (Operator) and
supervisor agents (Coordinator and Observer). Each agent takes a specific role:
the Coordinator directs the interactive task; the Operator mimics user
interactions on the device; and the Observer monitors and reviews the task
automation process. Our evaluation, which included 41 multi-user interactive
tasks, demonstrates the effectiveness of our approach, achieving 82.9% of the
tasks with 96.8% action similarity, outperforming the ablation studies and
state-of-the-art baselines. Additionally, a preliminary investigation
underscores MAdroid's practicality by helping identify 11 multi-user
interactive bugs during regression app testing, confirming its potential value
in real-world software development contexts.

</details>


### [7] [CodeMorph: Mitigating Data Leakage in Large Language Model Assessment](https://arxiv.org/abs/2506.17627)
*Hongzhou Rao,Yanjie Zhao,Wenjie Zhu,Ling Xiao,Meizhen Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: 论文提出了CodeMorph方法，通过代码扰动和遗传算法降低数据泄漏问题，提升大语言模型在代码任务中的评估效果。


<details>
  <summary>Details</summary>
Motivation: 由于代码大语言模型中存在基准测试泄漏问题，现有方法难以生成复杂多样的代码变异，且不支持多语言和跨文件依赖。

Method: CodeMorph结合26种语义保持的代码扰动方法和遗传算法PESO，迭代生成多样化且可编译的代码变异。

Result: 实验显示，应用CodeMorph后，模型在五种语言上的代码完成准确率平均下降24.67%，PESO优化的代码相似度平均降低7.01%。

Conclusion: CodeMorph有效缓解数据泄漏问题，提升了代码大语言模型的评估可靠性。

Abstract: Concerns about benchmark leakage in large language models for code (Code
LLMs) have raised issues of data contamination and inflated evaluation metrics.
The diversity and inaccessibility of many training datasets make it difficult
to prevent data leakage entirely, even with time lag strategies. Consequently,
generating new datasets through code perturbation has become essential.
However, existing methods often fail to produce complex and diverse variations,
struggle with complex cross-file dependencies, and lack support for multiple
programming languages, which limits their effectiveness in enhancing LLM
evaluations for coding tasks. To fill this gap, we propose CodeMorph, an
approach designed to support multiple programming languages while preserving
cross-file dependencies to mitigate data leakage. CodeMorph consists of two
main components that work together to enhance the perturbation process. The
first component employs 26 semantic-preserving transformation methods to
iteratively perturb code, generating diverse variations while ensuring that the
modified code remains compilable. The second component introduces a genetic
algorithm-based selection algorithm, PESO, to identify the more effective
perturbation method for each iteration by targeting lower similarity scores
between the perturbed and original code, thereby enhancing overall perturbation
effectiveness. Experimental results demonstrate that after applying CodeMorph,
the accuracy of the LLM on code completion tasks across five programming
languages decreased by an average of 24.67%, with Python showing the most
significant reduction at 45%. The similarity score of code optimized by PESO
is, on average, 7.01% lower than that of randomly perturbed code, peaking at a
reduction of 42.86%.

</details>


### [8] [Deep Learning Framework Testing via Model Mutation: How Far Are We?](https://arxiv.org/abs/2506.17638)
*Yanzhou Mu,Rong Wang,Juan Zhai,Chunrong Fang,Xiang Chen,Zhiyuan Peng,Peiran Yang,Ruixiang Qian,Shaoyu Yang,Zhenyu Chen*

Main category: cs.SE

TL;DR: 重新评估了现有基于变异的深度学习框架缺陷检测方法的有效性，提出了优化策略，并成功识别了多个高优先级缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测深度学习框架缺陷时存在高假阳性率和忽视独特挑战的问题，导致开发者忽略大部分发现的缺陷。

Method: 收集了三个流行框架的缺陷报告并分类，深入分析后发现现有方法的不足，提出了优化策略。

Result: 优化后，识别了39个独特缺陷（其中31个被开发者确认，8个已修复），包括7个新缺陷（4个被标记为高优先级，3个已解决）。

Conclusion: 研究展示了优化策略的有效性，显著提升了缺陷检测能力，且开发者确认和修复率较高。

Abstract: Deep Learning (DL) frameworks are a fundamental component of DL development.
Therefore, the detection of DL framework defects is important and challenging.
As one of the most widely adopted DL testing techniques, model mutation has
recently gained significant attention. In this study, we revisit the defect
detection ability of existing mutation-based testing methods and investigate
the factors that influence their effectiveness. To begin with, we reviewed
existing methods and observed that many of them mutate DL models (e.g.,
changing their parameters) without any customization, ignoring the unique
challenges in framework testing. Another issue with these methods is their
limited effectiveness, characterized by a high rate of false positives caused
by illegal mutations arising from the use of generic, non-customized mutation
operators. Moreover, we tracked the defects identified by these methods and
discovered that most of them were ignored by developers. Motivated by these
observations, we investigate the effectiveness of existing mutation-based
testing methods in detecting important defects that have been authenticated by
framework developers. We begin by collecting defect reports from three popular
frameworks and classifying them based on framework developers' ratings to build
a comprehensive dataset. We then perform an in-depth analysis to uncover
valuable insights. Based on our findings, we propose optimization strategies to
address the shortcomings of existing approaches. Following these optimizations,
we identified seven new defects, four of which were confirmed by developers as
high-priority issues, with three resolved. In summary, we identified 39 unique
defects across just 23 models, of which 31 were confirmed by developers, and
eight have been fixed.

</details>


### [9] [May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs](https://arxiv.org/abs/2506.17642)
*Shaoyu Yang,Chunrong Fang,Haifeng Lin,Xiang Chen,Zhenyu Chen*

Main category: cs.SE

TL;DR: 论文提出了一种名为FUEL的反馈驱动模糊测试方法，针对深度学习框架中的缺陷检测，利用两种基于大语言模型（LLM）的代理分析反馈信息并生成多样化测试用例，显著提升了测试效果。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架中的缺陷可能导致严重后果（如医疗和自动驾驶领域），而现有模糊测试技术未能全面利用多类反馈信息，且基于LLM的方法仅用于生成测试用例，忽视了其分析反馈的潜力。

Method: 提出FUEL框架，包含两个LLM代理：分析LLM从反馈信息中提取分析摘要，生成LLM根据摘要指导测试用例生成。

Result: FUEL在PyTorch和TensorFlow中发现104个缺陷，其中93个为新缺陷，47个已修复，5个获得CVE编号。

Conclusion: 研究表明，综合考虑多类反馈信息能提升模糊测试性能，并证明LLM分析反馈是未来发展方向。

Abstract: Artificial Intelligence (AI) Infrastructures, represented by Deep Learning
(DL) frameworks, have served as fundamental DL systems over the last decade.
However, the bugs in DL frameworks could lead to catastrophic consequences in
some critical scenarios (e.g., healthcare and autonomous driving). A simple yet
effective way to find bugs in DL frameworks is fuzz testing (Fuzzing).
Unfortunately, existing fuzzing techniques have not comprehensively considered
multiple types of feedback. Additionally, they analyze feedback in a
coarse-grained manner, such as mutating the test cases only according to
whether the coverage increases. Recently, researchers introduced Large Language
Models (LLMs) into fuzzing. However, current LLM-based fuzzing techniques only
focus on using LLMs to generate test cases while overlooking their potential to
analyze feedback information, failing to create more valid and diverse test
cases. To fill this gap, we propose FUEL to break the seal of Feedback-driven
fuzzing for DL frameworks. The backbone of FUEL comprises two LLM-based agents,
namely analysis LLM and generation LLM. Analysis LLM agent infers analysis
summaries from feedback information, while the generation LLM agent creates
tests guided by these analysis summaries. So far, FUEL has detected 104 bugs
for PyTorch and TensorFlow, with 93 confirmed as new bugs, 47 already fixed,
and 5 assigned with CVE IDs. Our work indicates that considering multiple types
of feedback is beneficial to fuzzing performance, and leveraging LLMs to
analyze feedback information is a promising direction. Our artifact is
available at https://github.com/NJU-iSE/FUEL

</details>


### [10] [Improving Compiler Bug Isolation by Leveraging Large Language Models](https://arxiv.org/abs/2506.17647)
*Yixian Qi,Jiajun Jiang,Fengjie Li,Bowen Chen,Hongyu Zhang,Junjie Chen*

Main category: cs.SE

TL;DR: AutoCBI利用大型语言模型（LLM）改进编译器错误定位，通过总结文件功能和重新排序可疑文件来提高准确性，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统编译器错误定位技术在可扩展性和有效性上存在局限性，而预训练的大型语言模型为改进这一问题提供了新思路。

Method: AutoCBI结合LLM总结编译器文件功能，并使用专门提示重新排序可疑文件排名，利用四种信息类型优化结果。

Result: 在GCC和LLVM的120个真实错误测试中，AutoCBI在Top-1排名中的表现优于DiWi、RecBi和FuseFL，提升了66.67%到340%。

Conclusion: AutoCBI为编译器错误定位提供了高效解决方案，并通过消融实验验证了各组件的重要性。

Abstract: Compilers play a foundational role in building reliable software systems, and
bugs within them can lead to catastrophic consequences. The compilation process
typically involves hundreds of files, making traditional automated bug
isolation techniques inapplicable due to scalability or effectiveness issues.
Current mainstream compiler bug localization techniques have limitations in
test program mutation and resource consumption. Inspired by the recent advances
of pre-trained Large Language Models (LLMs), we propose an innovative approach
named AutoCBI, which (1) uses LLMs to summarize compiler file functions and (2)
employs specialized prompts to guide LLM in reordering suspicious file
rankings. This approach leverages four types of information: the failing test
program, source file function summaries, lists of suspicious files identified
through analyzing test coverage, as well as compilation configurations with
related output messages, resulting in a refined ranking of suspicious files.
Our evaluation of AutoCBI against state-of-the-art approaches (DiWi, RecBi and
FuseFL) on 120 real-world bugs from the widely-used GCC and LLVM compilers
demonstrates its effectiveness. Specifically, AutoCBI isolates 66.67%/69.23%,
300%/340%, and 100%/57.14% more bugs than RecBi, DiWi, and FuseFL,
respectively, in the Top-1 ranked results for GCC/LLVM. Additionally, the
ablation study underscores the significance of each component in our approach.

</details>


### [11] [PAGENT: Learning to Patch Software Engineering Agents](https://arxiv.org/abs/2506.17772)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.SE

TL;DR: 本文研究了由七种顶级LLM代码代理生成的失败补丁，提出了六类失败原因的分类法，并设计了一个基于程序分析的补丁代理PAGENT，用于解决类型相关错误。


<details>
  <summary>Details</summary>
Motivation: 理解LLM代理生成失败补丁的根本原因，并提出改进方法。

Method: 收集114个未解决的问题，分析769个失败补丁，设计PAGENT进行类型推断和补丁修复。

Result: PAGENT成功修复了127个类型相关失败补丁中的29个。

Conclusion: PAGENT是一种有效的工具，可用于改进LLM代理生成的补丁质量。

Abstract: LLM Agents produce patches automatically to resolve an issue. However, they
can generate inaccurate patches. Little is known about the root causes behind
those failed patches or how those could be fixed. This paper reports an
empirical study of the failed patches generated by seven top LLM code agents.
We collected 114 issues from the SWE-bench Lite dataset that remained
unresolved across the agents. The seven agents produced a total of 769 failed
patches for those issues, which we checked with a combination of GPT-4o and
manual analysis. We present a taxonomy of the failure reasons across the
patches. The taxonomy contains six categories, with several sub-categories
under each category. For example, a frequently observed category is the
inability of an LLM to correctly infer/produce the appropriate variable type in
the produced patch. As a first step towards addressing such type-related
errors, we designed PAGENT (Patch Agent). PAGENT utilizes program analysis
techniques like CFG creation and exploration to infer the type of information
of a patch. PAGENT does this by applying repository-level static code analysis
techniques. Then, PAGENT refines the inferred type by further utilizing an
LLM-based inference technique. We tested PAGENT on all 127 type-related failed
patches from the top three agents in our study. PAGENT could fix 29 of the 127
failed patches.

</details>


### [12] [SAVANT: Vulnerability Detection in Application Dependencies through Semantic-Guided Reachability Analysis](https://arxiv.org/abs/2506.17798)
*Wang Lingxiang,Quanzhi Fu,Wenjia Song,Gelei Deng,Yi Liu,Dan Williams,Ying Zhang*

Main category: cs.SE

TL;DR: SAVANT是一种结合语义预处理和LLM的漏洞检测方法，通过分析依赖库中的API使用上下文，有效提高了Java开发中开源第三方库的安全风险检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有SCA工具因难以理解API使用语义和分析复杂代码库的局限性，导致漏洞检测不准确，增加了开发团队的负担并延误安全修复。

Method: SAVANT结合漏洞测试用例和LLM的代码语义理解能力，通过语义预处理和LLM驱动的上下文分析，准确检测漏洞。

Result: 在55个实际应用中，SAVANT的精确度、召回率、准确率和F1分数均优于现有SCA工具。

Conclusion: SAVANT通过创新的语义和上下文分析方法，显著提升了漏洞检测的效果，解决了现有工具的局限性。

Abstract: The integration of open-source third-party library dependencies in Java
development introduces significant security risks when these libraries contain
known vulnerabilities. Existing Software Composition Analysis (SCA) tools
struggle to effectively detect vulnerable API usage from these libraries due to
limitations in understanding API usage semantics and computational challenges
in analyzing complex codebases, leading to inaccurate vulnerability alerts that
burden development teams and delay critical security fixes.
  To address these challenges, we proposed SAVANT by leveraging two insights:
proof-of-vulnerability test cases demonstrate how vulnerabilities can be
triggered in specific contexts, and Large Language Models (LLMs) can understand
code semantics. SAVANT combines semantic preprocessing with LLM-powered context
analysis for accurate vulnerability detection. SAVANT first segments source
code into meaningful blocks while preserving semantic relationships, then
leverages LLM-based reflection to analyze API usage context and determine
actual vulnerability impacts. Our evaluation on 55 real-world applications
shows that SAVANT achieves 83.8% precision, 73.8% recall, 69.0% accuracy, and
78.5% F1-score, outperforming state-of-the-art SCA tools.

</details>


### [13] [Is Your Automated Software Engineer Trustworthy?](https://arxiv.org/abs/2506.17812)
*Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 摘要介绍了一个名为BouncerBench的基准测试，旨在评估基于LLM的软件代理是否能拒绝处理模糊输入或错误输出，以提高精确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具在处理模糊或错误输入时缺乏拒绝机制，导致不可靠行为，如生成错误代码。

Method: 引入BouncerBench基准测试，重点关注模糊问题和错误补丁，并实现输入输出筛选器。

Result: 结果显示大多数模型无法在需要时拒绝处理，表明LLM在实际应用中仍有改进空间。

Conclusion: BouncerBench为评估和构建更可靠的代码代理提供了初步工具。

Abstract: Large Language Models (LLMs) are being increasingly used in software
engineering tasks, with an increased focus on bug report resolution over the
past year. However, most proposed systems fail to properly handle uncertain or
incorrect inputs and outputs. Existing LLM-based tools and coding agents
respond to every issue and generate a patch for every case, even when the input
is vague or their own output is incorrect. There are no mechanisms in place to
abstain when confidence is low. This leads to unreliable behaviour, such as
hallucinated code changes or responses based on vague issue reports. We
introduce BouncerBench, a benchmark that evaluates whether LLM-based software
agents can refuse to act when inputs are ill-defined or refuse to respond when
their own outputs are likely to be incorrect. Unlike prior benchmarks that
implicitly incentivize models to generate responses even when uncertain,
BouncerBench aims to improve precision by targeting two overlooked failure
points: (1) vague or underspecified issue descriptions in tickets and (2)
logically or functionally incorrect code patches created by the system. It
measures whether proposed systems can distinguish actionable issues from vague
tickets and valid patches from untrustworthy ones. We also implement a basic
input and output bouncer, evaluating how well current LLMs can abstain when
needed. Our results show that most models fail to abstain from underspecified
inputs or incorrect outputs. Hence, we conclude that there is significant room
for improvement before LLMs can be trusted to make correct decisions and
recommendations in real-world software engineering workflows. BouncerBench
provides a first step toward evaluating and building more cautious, trustworthy
code agents. The replication package, dataset, and leaderboard can be found at
bouncerbench.com

</details>


### [14] [The Impact of AI-Generated Solutions on Software Architecture and Productivity: Results from a Survey Study](https://arxiv.org/abs/2506.17833)
*Giorgio Amasanti,Jasmin Jahic*

Main category: cs.SE

TL;DR: AI工具显著提升软件工程师的生产力，但对项目复杂度增加时效果减弱；对软件质量无显著负面影响，但解决复杂问题时需人工干预。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具对软件工程师生产力和软件质量的长期影响。

Method: 通过对使用AI工具的软件从业者进行问卷调查。

Result: AI工具提高生产力，但对复杂项目效果降低；AI生成的代码片段对质量无负面影响，但复杂问题需人工分解和集成。

Conclusion: AI工具在提升生产力和代码质量方面有效，但复杂场景仍需人工干预。

Abstract: AI-powered software tools are widely used to assist software engineers.
However, there is still a need to understand the productivity benefits of such
tools for software engineers. In addition to short-term benefits, there is a
question of how adopting AI-generated solutions affects the quality of software
over time (e.g., maintainability and extendability).
  To provide some insight on these questions, we conducted a survey among
software practitioners who use AI tools. Based on the data collected from our
survey, we conclude that AI tools significantly increase the productivity of
software engineers. However, the productivity benefits of using AI tools reduce
as projects become more complex. The results also show that there are no
significant negative influences of adopting AI-generated solutions on software
quality, as long as those solutions are limited to smaller code snippets.
However, when solving larger and more complex problems, AI tools generate
solutions of a lower quality, indicating the need for architects to perform
problem decomposition and solution integration.

</details>


### [15] [Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering](https://arxiv.org/abs/2506.17937)
*Tommi Mikkonen,Antero Taivalsaari*

Main category: cs.SE

TL;DR: 论文探讨了AI辅助生成软件复用在‘AI原生’软件工程中的影响，提出了相关问题，并定义了研究议程和行动呼吁，以解决相关的核心问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI和生成式软件复用成为软件开发的核心，传统复用方法被AI辅助方法取代，作者希望探讨这种转变的潜在问题及其在‘AI原生’软件开发中的影响。

Method: 通过分析AI辅助生成软件复用的现状，提出相关问题，并定义研究议程和行动呼吁。

Result: 指出AI辅助软件复用可能导致类似‘货物崇拜开发’的问题，并呼吁进一步研究解决这些挑战。

Conclusion: AI辅助生成软件复用带来了新的挑战，需要明确研究方向和行动以应对这些挑战。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Consequently, earlier software reuse practices and methods
are rapidly being replaced by AI-assisted approaches in which developers place
their trust on code that has been generated by artificial intelligence. This is
leading to a new form of software reuse that is conceptually not all that
different from cargo cult development. In this paper we discuss the
implications of AI-assisted generative software reuse in the context of
emerging "AI native" software engineering, bring forth relevant questions, and
define a tentative research agenda and call to action for tackling some of the
central issues associated with this approach.

</details>


### [16] [Build It Clean: Large-Scale Detection of Code Smells in Build Scripts](https://arxiv.org/abs/2506.17948)
*Mahzabin Tamanna,Yash Chandrani,Matthew Burrows,Brandon Wroblewski,Laurie Williams,Dominik Wermke*

Main category: cs.SE

TL;DR: 研究通过分析GitHub上的构建脚本和问题，识别了13类代码异味，并开发了静态分析工具Sniffer，提出了改善构建脚本的建议。


<details>
  <summary>Details</summary>
Motivation: 帮助开发者避免构建脚本中的代码异味，提高软件项目的效率、可靠性和可维护性。

Method: 采用混合方法，定性分析2000个GitHub问题，定量分析5882个构建脚本，开发工具Sniffer识别异味。

Result: 发现13类异味共10895次出现，不同构建工具中异味分布不同，某些异味对强关联。

Conclusion: 基于研究结果，提出了策略以减少构建脚本中的异味，改善开发实践。

Abstract: Build scripts are files that automate the process of compiling source code,
managing dependencies, running tests, and packaging software into deployable
artifacts. These scripts are ubiquitous in modern software development
pipelines for streamlining testing and delivery. While developing build
scripts, practitioners may inadvertently introduce code smells. Code smells are
recurring patterns of poor coding practices that may lead to build failures or
increase risk and technical debt. The goal of this study is to aid
practitioners in avoiding code smells in build scripts through an empirical
study of build scripts and issues on GitHub. We employed a mixed-methods
approach, combining qualitative and quantitative analysis. We conducted a
qualitative analysis of 2000 build-script-related GitHub issues. Next, we
developed a static analysis tool, Sniffer, to identify code smells in 5882
build scripts of Maven, Gradle, CMake, and Make files, collected from 4877
open-source GitHub repositories. We identified 13 code smell categories, with a
total of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,
337 in CMake, and 6160 in Makefiles.
  Our analysis revealed that Insecure URLs were the most prevalent code smell
in Maven build scripts, while Hardcoded Paths/URLs were commonly observed in
both Gradle and CMake scripts. Wildcard Usage emerged as the most frequent
smell in Makefiles. The co-occurrence analysis revealed strong associations
between specific smell pairs of Hardcoded Paths/URLs with Duplicates, and
Inconsistent Dependency Management with Empty or Incomplete Tags, indicating
potential underlying issues in the build script structure and maintenance
practices. Based on our findings, we recommend strategies to mitigate the
existence of code smells in build scripts to improve the efficiency,
reliability, and maintainability of software projects.

</details>


### [17] [VFArchē: A Dual-Mode Framework for Locating Vulnerable Functions in Open-Source Software](https://arxiv.org/abs/2506.18050)
*Lyuye Zhang,Jian Zhang,Kaixuan Li,Chong Wang,Chengwei Liu,Jiahui Wu,Sen Chen,Yaowen Zheng,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出了一种名为VFArch\=e的双模式方法，用于在有无补丁的情况下自动定位软件依赖中的漏洞函数，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代漏洞数据库（如NVD）通常缺少漏洞函数的关键信息，而补丁又可能不可用或不完全覆盖漏洞函数。需要一种综合解决方案来处理这两种情况。

Method: VFArch\=e采用双模式设计，针对公开漏洞在有补丁和无补丁的两种场景下进行漏洞函数定位。

Result: 在基准数据集上的实验显示，VFArch\=e在Patch-present和Patch-absent模式下分别比最佳基线高出1.3倍和1.9倍的Mean Reciprocal Rank，并能显著减少SCA工具的78-89%误报。

Conclusion: VFArch\=e能够高效定位漏洞函数，适用于现实场景，为解决软件依赖漏洞提供了实用工具。

Abstract: Software Composition Analysis (SCA) has become pivotal in addressing
vulnerabilities inherent in software project dependencies. In particular,
reachability analysis is increasingly used in Open-Source Software (OSS)
projects to identify reachable vulnerabilities (e.g., CVEs) through call
graphs, enabling a focus on exploitable risks. Performing reachability analysis
typically requires the vulnerable function (VF) to track the call chains from
downstream applications. However, such crucial information is usually
unavailable in modern vulnerability databases like NVD. While directly
extracting VF from modified functions in vulnerability patches is intuitive,
patches are not always available. Moreover, our preliminary study shows that
over 26% of VF do not exist in the modified functions. Meanwhile, simply
ignoring patches to search vulnerable functions suffers from overwhelming
noises and lexical gaps between descriptions and source code. Given that almost
half of the vulnerabilities are equipped with patches, a holistic solution that
handles both scenarios with and without patches is required. To meet real-world
needs and automatically localize VF, we present VFArch\=e, a dual-mode approach
designed for disclosed vulnerabilities, applicable in scenarios with or without
available patch links. The experimental results of VFArch\=e on our constructed
benchmark dataset demonstrate significant efficacy regarding three metrics,
achieving 1.3x and 1.9x Mean Reciprocal Rank over the best baselines for
Patch-present and Patch-absent modes, respectively. Moreover, VFArch\=e has
proven its applicability in real-world scenarios by successfully locating VF
for 43 out of 50 latest vulnerabilities with reasonable efforts and
significantly reducing 78-89% false positives of SCA tools.

</details>


### [18] [Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks](https://arxiv.org/abs/2506.18191)
*Masudul Hasan Masud Bhuiyan,Gianluca De Stefano,Giancarlo Pellegrino,Cristian-Alexandru Staicu*

Main category: cs.SE

TL;DR: GRAPHIA利用图神经网络改进JavaScript调用图构建，通过丰富的程序图表示和多类型边链接预测，提高了未解析调用边的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript调用图构建工具既不健全也不完整，常遗漏有效边或产生错误边，需要新方法提升准确性。

Method: 提出GRAPHIA，结合语法和语义边表示程序，利用图神经网络建模代码元素间的非局部关系，支持从不完美标签中学习。

Result: 在50个流行JavaScript库上评估，GRAPHIA在42%未解析调用中将正确目标排名第一，72%的情况下位列前五。

Conclusion: 学习型方法能提升调用图构建的召回率，GRAPHIA是首个将GNN链接预测应用于多文件程序图进行过程间分析的工作。

Abstract: Static analysis plays a key role in finding bugs, including security issues.
A critical step in static analysis is building accurate call graphs that model
function calls in a program. However, due to hard-to-analyze language features,
existing call graph construction algorithms for JavaScript are neither sound
nor complete. Prior work shows that even advanced solutions produce false edges
and miss valid ones. In this work, we assist these tools by identifying missed
call edges. Our main idea is to frame the problem as link prediction on full
program graphs, using a rich representation with multiple edge types. Our
approach, GRAPHIA, leverages recent advances in graph neural networks to model
non-local relationships between code elements. Concretely, we propose
representing JavaScript programs using a combination of syntactic- and
semantic-based edges. GRAPHIA can learn from imperfect labels, including static
call edges from existing tools and dynamic edges from tests, either from the
same or different projects. Because call graphs are sparse, standard machine
learning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by
ranking function definitions for each unresolved call site. We conduct a
large-scale evaluation on 50 popular JavaScript libraries with 163K call edges
(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M
structural and 386K semantic edges. It ranks the correct target as the top
candidate in over 42% of unresolved cases and within the top 5 in 72% of cases,
reducing the manual effort needed for analysis. Our results show that
learning-based methods can improve the recall of JavaScript call graph
construction. To our knowledge, this is the first work to apply GNN-based link
prediction to full multi-file program graphs for interprocedural analysis.

</details>


### [19] [Managing Technical Debt in a Multidisciplinary Data Intensive Software Team: an Observational Case Study](https://arxiv.org/abs/2506.18219)
*Ulrike M. Graetsch,Rashina Hoda,Hourieh Khalazjadeh,Mojtaba Shahin,John Grundy*

Main category: cs.SE

TL;DR: 本文研究了多学科数据密集型团队的技债管理实践，通过案例研究揭示了团队如何评估和处理技债，并提出对新模式和工具支持的需求。


<details>
  <summary>Details</summary>
Motivation: 数据密集型系统的投资增加导致技债风险上升，但多学科团队如何管理技债的实证研究有限。

Method: 采用探索性观察案例研究和社会技术扎根理论分析数据。

Result: 识别了数据组件债务和管道债务，描述了团队的技债评估和处理方法。

Conclusion: 研究结果与现有技债分类一致，并强调需要新的实施模式和工具支持。

Abstract: Context: There is an increase in the investment and development of
data-intensive (DI) solutions, systems that manage large amounts of data.
Without careful management, this growing investment will also grow associated
technical debt (TD). Delivery of DI solutions requires a multidisciplinary
skill set, but there is limited knowledge about how multidisciplinary teams
develop DI systems and manage TD.
  Objective: This research contributes empirical, practice based insights about
multidisciplinary DI team TD management practices.
  Method: This research was conducted as an exploratory observation case study.
We used socio-technical grounded theory (STGT) for data analysis to develop
concepts and categories that articulate TD and TDs debt management practices.
  Results: We identify TD that the DI team deals with, in particular technical
data components debt and pipeline debt. We explain how the team manages the TD,
assesses TD, what TD treatments they consider and how they implement TD
treatments to fit sprint capacity constraints.
  Conclusion: We align our findings to existing TD and TDM taxonomies, discuss
their implications and highlight the need for new implementation patterns and
tool support for multidisciplinary DI teams.

</details>


### [20] [Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations](https://arxiv.org/abs/2506.18289)
*Saurabhsingh Rajput,Mootez Saad,Tushar Sharma*

Main category: cs.SE

TL;DR: 论文强调将能源效率作为AI管道的核心设计考虑，通过五个阶段的优化，实现显著的能源节省和性能保留。


<details>
  <summary>Details</summary>
Motivation: AI的计算需求和能源挑战日益增长，现有优化方法多为孤立反应，缺乏对能源效率的系统性考虑。

Method: 在数据、模型、训练、系统和推理五个阶段进行正交组合优化。

Result: 实验验证显示，能源消耗减少高达94.6%，同时保留95.95%的原始性能（F1分数）。

Conclusion: 提出了一种平衡效率、性能和环保的可操作性框架，推动可持续AI的发展。

Abstract: AI's exponential growth intensifies computational demands and energy
challenges. While practitioners employ various optimization techniques, that we
refer as "knobs" in this paper, to tune model efficiency, these are typically
afterthoughts and reactive ad-hoc changes applied in isolation without
understanding their combinatorial effects on energy efficiency. This paper
emphasizes on treating energy efficiency as the first-class citizen and as a
fundamental design consideration for a compute-intensive pipeline. We show that
strategic selection across five AI pipeline phases (data, model, training,
system, inference) creates cascading efficiency. Experimental validation shows
orthogonal combinations reduce energy consumption by up to $94.6$% while
preserving $95.95$% of the original F1 score of non-optimized pipelines. This
curated approach provides actionable frameworks for informed sustainable AI
that balance efficiency, performance, and environmental responsibility.

</details>


### [21] [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315)
*Lehan He,Zeren Chen,Zhe Zhang,Jing Shao,Xiang Gao,Lu Sheng*

Main category: cs.SE

TL;DR: 论文提出了一种名为Property-Generated Solver的新框架，通过基于属性的测试（PBT）验证代码的高层属性，而非具体输入-输出示例，从而提升大语言模型（LLM）生成代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在代码生成中表现出色，但其功能的正确性在复杂任务中仍面临挑战。传统的测试驱动开发（TDD）因高质量测试用例稀缺或自动化测试生成的缺陷（如偏差测试或不准确的输出预测）而效率不足。

Method: 框架采用两个LLM代理：Generator负责代码生成与迭代优化，Tester管理PBT生命周期并通过属性违规生成语义丰富的反馈。这种闭环迭代模式将PBT作为核心验证机制。

Result: 在多个代码生成基准测试中，Property-Generated Solver相较传统TDD方法，取得了显著的性能提升，pass@1相对增益达23.1%至37.3%。

Conclusion: 通过PBT验证高层属性和双代理协作机制，Property-Generated Solver显著提高了LLM生成代码的正确性和泛化能力。

Abstract: Large Language Models (LLMs) excel at code generation, but ensuring their
outputs to be functionally correct, especially in complex programming tasks, is
a persistent challenge. While traditional Test-Driven Development (TDD) offers
a path for code refinement, its efficacy with LLMs is often undermined by the
scarcity of high-quality test cases or the pitfalls of automated test
generation, including biased tests or inaccurate output predictions that can
misdirect the correction process. This paper introduces Property-Generated
Solver, a novel framework that leverages Property-Based Testing (PBT) to
validate high-level program properties or invariants, instead of relying on
specific input-output examples. These properties are often simpler to define
and verify than directly predicting exhaustive test oracles, breaking the
"cycle of self-deception" where tests might share flaws with the code they are
meant to validate. Property-Generated Solver employs two collaborative
LLM-based agents: a Generator dedicated to code generation and iterative
refinement, and a Tester that manages the PBT life-cycle and formulate
semantically rich feedback from property violations. The resulting
comprehensive and actionable feedback then guides the Generator in its
refinement efforts. By establishing PBT as the core validation engine within
this iterative, closed-loop paradigm, Property-Generated Solver provides a
robust mechanism for steering LLMs towards more correct and generalizable code.
Extensive experimental results on multiple code generation benchmarks
demonstrate that Property-Generated Solver achieves substantial pass@1
improvements, ranging from 23.1% to 37.3% relative gains over established TDD
methods.

</details>


### [22] [Predictive Analytics for Collaborators Answers, Code Quality, and Dropout on Stack Overflow](https://arxiv.org/abs/2506.18329)
*Elijah Zolduoarrati,Sherlock A. Licorish,Nigel Stanger*

Main category: cs.SE

TL;DR: 该研究评估了21种算法在三个任务上的表现，通过标准化、变换等方法优化模型性能，发现Bagging和CodeBERT等模型在特定任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 以往研究采用的模型较少或选择方法随意，可能忽视未测试算法的潜力，因此需要扩展基准测试范围。

Method: 使用标准化、变换等方法结合贝叶斯超参数优化和遗传算法，评估21种算法，并微调CodeBERT模型。

Result: Bagging模型在预测用户回答数上表现最佳（R2=0.821），CodeBERT在用户流失分类中F1-score为0.809。

Conclusion: 研究结果为研究者和实践者提供了模型选择和超参数优化的指导，扩展了基准测试的广度。

Abstract: Previous studies that used data from Stack Overflow to develop predictive
models often employed limited benchmarks of 3-5 models or adopted arbitrary
selection methods. Despite being insightful, their limited scope suggests the
need to benchmark more models to avoid overlooking untested algorithms. Our
study evaluates 21 algorithms across three tasks: predicting the number of
question a user is likely to answer, their code quality violations, and their
dropout status. We employed normalisation, standardisation, as well as
logarithmic and power transformations paired with Bayesian hyperparameter
optimisation and genetic algorithms. CodeBERT, a pre-trained language model for
both natural and programming languages, was fine-tuned to classify user dropout
given their posts (questions and answers) and code snippets. We found Bagging
ensemble models combined with standardisation achieved the highest R2 value
(0.821) in predicting user answers. The Stochastic Gradient Descent regressor,
followed by Bagging and Epsilon Support Vector Machine models, consistently
demonstrated superior performance to other benchmarked algorithms in predicting
user code quality across multiple quality dimensions and languages. Extreme
Gradient Boosting paired with log-transformation exhibited the highest F1-score
(0.825) in predicting user dropout. CodeBERT was able to classify user dropout
with a final F1-score of 0.809, validating the performance of Extreme Gradient
Boosting that was solely based on numerical data. Overall, our benchmarking of
21 algorithms provides multiple insights. Researchers can leverage findings
regarding the most suitable models for specific target variables, and
practitioners can utilise the identified optimal hyperparameters to reduce the
initial search space during their own hyperparameter tuning processes.

</details>


### [23] [Recipe for Discovery: A Framework for Systematic Open Source Project Identification](https://arxiv.org/abs/2506.18359)
*Juanita Gomez,Emily Lovell,Stephanie Lieggi,Alvaro A. Cardenas,James Davis*

Main category: cs.SE

TL;DR: 该论文提出了一种系统化识别和分类大学和研究实验室等机构开发的开放源代码软件项目的框架，并以加州大学系统为例，通过GitHub API构建管道，结合机器学习和大语言模型，高效识别了5.2万个相关仓库。


<details>
  <summary>Details</summary>
Motivation: 解决开放源代码开发中因分散性和缺乏可见性导致的项目难以追踪和认可的问题。

Method: 利用GitHub REST API构建管道，结合传统机器学习和大型语言模型对仓库进行分类。

Result: 成功发现5.2万个仓库，并高精度预测了机构从属关系。

Conclusion: 该框架能够有效地规模化识别和分析学术开放源代码项目。

Abstract: Open source software development, particularly within institutions such as
universities and research laboratories, is often decentralized and difficult to
track. Despite producing highly impactful tools in science, these efforts often
go unrecognized due to a lack of visibility and institutional awareness. This
paper addresses the challenge of discovering, classifying, and analyzing open
source software projects developed across distributed institutional systems. We
present a framework for systematically identifying institutional affiliated
repositories, using the University of California (UC) system as a case study.
  Using GitHub's REST API, we build a pipeline to discover relevant
repositories and extract meaningful metadata. We then propose and evaluate
multiple classification strategies, including both traditional machine learning
models and large language models (LLMs), to distinguish affiliated projects
from unrelated repositories and generate accurate insights into the academic
open source landscape. Our results show that the framework is effective at
scale, discovering over 52,000 repositories and predicting institutional
affiliation with high accuracy.

</details>


### [24] [Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair via Typestate-Guided Context Retrieval](https://arxiv.org/abs/2506.18394)
*Xiao Cheng,Zhihao Guo,Huan Huo,Yulei Sui*

Main category: cs.SE

TL;DR: LTFix是一种利用大型语言模型（LLM）自动修复C语言内存错误的新方法，通过有限类型状态自动机追踪错误传播路径和上下文轨迹，解决LLM在跨函数和文件级错误修复中的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于C语言手动内存管理的复杂性，内存相关错误常常导致严重漏洞，而传统自动程序修复（APR）方法依赖专家设计策略，深度学习方法虽能自动提取修复模式但需要大量数据且缺乏可解释性。

Method: LTFix采用有限类型状态自动机，指导追踪错误传播路径和上下文轨迹，结合内存状态和执行历史的时空维度，为LLM提供简洁而语义丰富的信息。

Result: 该方法有效解决了LLM在理解跨过程内存管理模式和存储库范围分析中的上下文窗口限制问题。

Conclusion: LTFix为LLM在复杂存储库级内存错误修复中的应用提供了新思路，平衡了语义丰富性和计算效率。

Abstract: Memory-related errors in C programming continue to pose significant
challenges in software development, primarily due to the complexities of manual
memory management inherent in the language. These errors frequently serve as
vectors for severe vulnerabilities, while their repair requires extensive
knowledge of program logic and C's memory model. Automated Program Repair (APR)
has emerged as a critical research area to address these challenges.
Traditional APR approaches rely on expert-designed strategies and predefined
templates, which are labor-intensive and constrained by the effectiveness of
manual specifications. Deep learning techniques offer a promising alternative
by automatically extracting repair patterns, but they require substantial
training datasets and often lack interpretability.
  This paper introduces LTFix, a novel approach that harnesses the potential of
Large Language Models (LLMs) for automated memory error repair, especially for
complex repository-level errors that span multiple functions and files. We
address two fundamental challenges in LLM-based memory error repair: a limited
understanding of interprocedural memory management patterns and context window
limitations for repository-wide analysis. Our approach utilizes a finite
typestate automaton to guide the tracking of error-propagation paths and
context trace, capturing both spatial (memory states) and temporal (execution
history) dimensions of error behavior. This typestate-guided context retrieval
strategy provides the LLM with concise yet semantically rich information
relevant to erroneous memory management, effectively addressing the token
limitation of LLMs.

</details>


### [25] [Your Token Becomes Worthless: Unveiling Rug Pull Schemes in Crypto Token via Code-and-Transaction Fusion Analysis](https://arxiv.org/abs/2506.18398)
*Hao Wu,Haijun Wang,Shangwang Li,Yin Wu,Ming Fan,Wuxia Jin,Yitao Zhao,Ting Liu*

Main category: cs.SE

TL;DR: RPhunter通过结合代码和交易数据，利用图神经网络检测加密货币中的Rug Pull骗局，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的Rug Pull检测方法仅关注代码或交易行为单一方面，无法有效应对复杂的骗局，因此需要一种综合方法。

Method: RPhunter结合代码风险图（SRCG）和交易行为图（TFBG），通过注意力融合模型提取特征，使用图神经网络进行检测。

Result: 在一组645个真实案例中，RPhunter的精确率达到95.3%，召回率93.8%，F1分数94.5%，并在实际场景中识别出4801个Rug Pull代币。

Conclusion: RPhunter通过整合代码和交易数据，显著提高了Rug Pull检测的准确性，验证了其在实际应用中的高效性。

Abstract: Rug pull scams have emerged as a persistent threat to cryptocurrency, causing
significant financial losses. A typical scenario involves scammers deploying
honeypot contracts to attract investments, restricting token sales, and
draining the funds, which leaves investors with worthless tokens. Current
methods either rely on predefined patterns to detect code risks or utilize
statistical transaction data to train detection models. However, real-world Rug
Pull schemes often involve a complex interplay between malicious code and
suspicious transaction behaviors. These methods, which solely focus on one
aspect, fall short in detecting such schemes effectively.
  In this paper, we propose RPhunter, a novel technique that integrates code
and transaction for Rug Pull detection. First, RPhunter establishes declarative
rules and performs flow analysis to extract code risk information, further
constructing a semantic risk code graph (SRCG). Meanwhile, to leverage
transaction information, RPhunter formulates dynamic token transaction
activities as a token flow behavior graph (TFBG) in which nodes and edges are
characterized from network structure and market manipulation perspectives.
Finally, RPhunter employs graph neural networks to extract complementary
features from SRCG and TFBG, integrating them through an attention fusion model
to enhance the detection of Rug Pull. We manually analyzed 645 Rug Pull
incidents from code and transaction aspects and constructed a ground-truth
dataset. We evaluated RPhunter on our dataset, achieving a precision of 95.3%,
a recall of 93.8% and an F1 score of 94.5%, which highlights superior
performance compared to existing state-of-the-art methods. Furthermore, when
applied to the real-world scenarios, RPhunter has identified 4801 Rug Pull
tokens, achieving a precision of 91%.

</details>


### [26] [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: 论文提出了一种称为Debugging Decay Index (DDI)的数学框架，用于量化AI调试效果随尝试次数衰减的情况，并通过适时干预恢复调试效率。


<details>
  <summary>Details</summary>
Motivation: AI调试能力在短期内显著衰减，这限制了实际代码生成系统的效能，因此需要一种方法量化这种衰减并优化调试策略。

Method: 引入了Debugging Decay Index (DDI)框架，并采用战略性‘重新开始’方法，在调试过程中适时从利用转向探索。

Result: DDI揭示了当前AI调试的根本局限性，并通过适时干预显著提升了调试效率。

Conclusion: DDI为优化迭代代码生成策略提供了首个量化框架，并为未来的AI调试研究奠定了基础。

Abstract: The effectiveness of AI debugging follows a predictable exponential decay
pattern; most models lose 60-80% of their debugging capability within just 2-3
attempts, despite iterative debugging being a critical capability for practical
code generation systems. We introduce the Debugging Decay Index (DDI), a
mathematical framework that quantifies when debugging becomes ineffective and
predicts intervention points. Our strategic fresh start approach shifts from
exploitation to exploration at strategic points in the debugging process,
demonstrating that well-timed interventions can rescue the effectiveness of
debugging. DDI reveals a fundamental limitation in current AI debugging and
provides the first quantitative framework for optimising iterative code
generation strategies.

</details>


### [27] [ModeliHub: A Web-based, Federated Analytics Platform for Modelica-centric, Model-based Systems Engineering](https://arxiv.org/abs/2506.18790)
*Mohamad Omar Nachawati*

Main category: cs.SE

TL;DR: ModeliHub是一个基于Web的联邦分析平台，专为Modelica模型系统工程设计，提供统一的系统模型和实时仿真环境。


<details>
  <summary>Details</summary>
Motivation: 为系统工程师提供一个基于Modelica的统一平台，支持异构工程工件的集成和分析。

Method: 采用中心辐射式联邦架构，通过Modelica编译器前端实现跨浏览器、桌面和服务器的无缝运行。

Result: 实现了实时交互的数字孪生仿真环境，支持工程领域的无缝集成与分析。

Conclusion: ModeliHub在系统工程中实现了严谨与敏捷的平衡，适用于多领域的工程分析。

Abstract: This paper introduces ModeliHub, a Web-based, federated analytics platform
designed specifically for model-based systems engineering with Modelica.
ModeliHub's key innovation lies in its Modelica-centric, hub-and-spoke
federation architecture that provides systems engineers with a Modelica-based,
unified system model of repositories containing heterogeneous engineering
artifacts. From this unified system model, ModeliHub's Virtual Twin engine
provides a real-time, interactive simulation environment for deploying Modelica
simulation models that represent digital twins of the virtual prototype of the
system under development at a particular iteration of the iterative systems
engineering life cycle. The implementation of ModeliHub is centered around its
extensible, Modelica compiler frontend developed in Isomorphic TypeScript that
can run seamlessly across browser, desktop and server environments. This
architecture aims to strike a balance between rigor and agility, enabling
seamless integration and analysis across various engineering domains.

</details>


### [28] [Context-Aware CodeLLM Eviction for AI-assisted Coding](https://arxiv.org/abs/2506.18796)
*Kishanthan Thangarajah,Boyuan Chen,Shi Chang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: CACE是一种针对自托管CodeLLM服务的上下文感知模型驱逐策略，旨在优化资源受限环境下的服务效率。


<details>
  <summary>Details</summary>
Motivation: 解决企业在自托管AI辅助编码工具时的隐私、延迟和模型定制问题，以及在模型管理和服务效率方面的挑战。

Method: 提出CACE策略，利用模型加载时间、任务延迟敏感性、输出长度、近期使用和未来需求等多因素进行上下文感知的模型驱逐。

Result: 实验表明，CACE能减少首次令牌时间（TTFT）和端到端延迟，同时显著降低模型驱逐次数。

Conclusion: CACE为实际软件工程环境中部署可扩展、低延迟的AI编码助手提供了实用策略。

Abstract: AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are
increasingly integrated into modern software development workflows. To address
concerns around privacy, latency, and model customization, many enterprises opt
to self-host these models. However, the diversity and growing number of
CodeLLMs, coupled with limited accelerator memory, introduce practical
challenges in model management and serving efficiency. This paper presents
CACE, a novel context-aware model eviction strategy designed specifically to
optimize self-hosted CodeLLM serving under resource constraints. Unlike
traditional eviction strategies based solely on recency (e.g., Least Recently
Used), CACE leverages multiple context-aware factors, including model load
time, task-specific latency sensitivity, expected output length, and recent
usage and future demand tracked through a sliding window. We evaluate CACE
using realistic workloads that include both latency-sensitive code completion
and throughput-intensive code reasoning tasks. Our experiments show that CACE
reduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while
significantly lowering the number of model evictions compared to
state-of-the-art systems. Ablation studies further demonstrate the importance
of multi-factor eviction in balancing responsiveness and resource efficiency.
This work contributes practical strategies for deploying scalable, low-latency
AI coding assistants in real-world software engineering environments.

</details>


### [29] [Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories](https://arxiv.org/abs/2506.18824)
*Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: 论文通过实证研究分析了三种基于LLM的代理在软件工程任务中的决策过程，揭示了成功与失败的行为模式，并提出了改进设计的方法。


<details>
  <summary>Details</summary>
Motivation: 探索基于LLM的代理在复杂软件工程任务中的内部决策过程，以提高其透明性和鲁棒性。

Method: 统一了三种代理的交互日志格式，定量分析结构特性、行为模式和token使用，定性评估推理一致性和反馈整合。

Result: 识别了成功和失败执行的行为模式，提供了改进代理设计的具体建议。

Conclusion: 研究发现有助于提升代理的透明性和鲁棒性，并支持进一步研究。

Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate
complex software engineering tasks such as program repair and issue resolution.
These agents operate by autonomously generating natural language thoughts,
invoking external tools, and iteratively refining their solutions. Despite
their widespread adoption, the internal decision-making processes of these
agents remain largely unexplored, limiting our understanding of their
operational dynamics and failure modes. In this paper, we present a large-scale
empirical study of the thought-action-result trajectories of three
state-of-the-art LLM-based agents: \textsc{RepairAgent},
\textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs
into a common format, capturing 120 trajectories and 2822 LLM interactions
focused on program repair and issue resolution. Our study combines quantitative
analyses of structural properties, action patterns, and token usage with
qualitative assessments of reasoning coherence and feedback integration. We
identify key trajectory characteristics such as iteration counts and token
consumption, recurring action sequences, and the semantic coherence linking
thoughts, actions, and their results. Our findings reveal behavioral motifs and
anti-patterns that distinguish successful from failed executions, providing
actionable insights for improving agent design, including prompting strategies,
failure diagnosis, and anti-pattern detection. We release our dataset and
annotation framework to support further research on transparent and robust
autonomous software engineering agents.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [30] [The Case for a Horizontal Federated AI operating System for Telcos](https://arxiv.org/abs/2506.17259)
*Sebastian Barros*

Main category: cs.NI

TL;DR: 提出了一种面向电信领域的横向联邦AI操作系统，旨在统一分散的AI应用，支持数据本地化和多厂商扩展。


<details>
  <summary>Details</summary>
Motivation: 电信运营商需要整合分散的AI能力以提升客户体验、网络运营和服务编排。

Method: 设计并部署一个横向联邦AI操作系统，提供通用执行和协调层，支持联邦训练和现有系统的集成。

Result: 系统能够打破现有孤岛，实现跨运营商的智能协同和自动化。

Conclusion: 该操作系统不仅技术上可行，还能重构分布式网络中智能部署与组合的方式。

Abstract: As artificial intelligence capabilities rapidly advance, Telco operators face
a growing need to unify fragmented AI efforts across customer experience,
network operations, and service orchestration. This paper proposes the design
and deployment of a horizontal federated AI operating system tailored for the
telecommunications domain. Unlike vertical vendor-driven platforms, this system
acts as a common execution and coordination layer, enabling Telcos to deploy AI
agents at scale while preserving data locality, regulatory compliance, and
architectural heterogeneity. We argue that such an operating system must expose
tightly scoped abstractions for telemetry ingestion, agent execution, and model
lifecycle management. It should support federated training across sovereign
operators, offer integration hooks into existing OSS and BSS systems, and
comply with TM Forum and O-RAN standards. Importantly, the platform must be
governed through a neutral foundation model to ensure portability,
compatibility, and multi-vendor extensibility. This architecture offers a path
to break the current silos, unlock ecosystem-level intelligence, and provide a
foundation for agent-based automation across the Telco stack. The case for this
horizontal layer is not only technical but structural, redefining how
intelligence is deployed and composed in a distributed network environment.

</details>


### [31] [Consistent Channel Hopping Algorithms for the Multichannel Rendezvous Problem with Heterogeneous Available Channel Sets](https://arxiv.org/abs/2506.18381)
*Yiwei Liu,Yi-Chia Cheng,Cheng-Shang Chang*

Main category: cs.NI

TL;DR: 提出了一种理论框架，通过一致的频道选择算法解决无线网络中的多频道会合问题，证明了其最优性，并提出了高效的实现方法。


<details>
  <summary>Details</summary>
Motivation: 解决异构可用频道集中的多频道会合问题，确保频道选择的稳定性。

Method: 设计一致的频道选择函数，将其表示为排列序列，并提出模算法降低复杂度。

Result: 证明了算法的会合时间界限及最优性，模拟结果验证了其有效性和扩展性。

Conclusion: 框架和算法在多用户场景中表现优异，为无线网络会合问题提供了理论保障和实践方案。

Abstract: We propose a theoretical framework for consistent channel hopping algorithms
to address the multichannel rendezvous problem (MRP) in wireless networks with
heterogeneous available channel sets. A channel selection function is called
consistent if the selected channel remains unchanged when the available channel
set shrinks, provided the selected channel is still available. We show that all
consistent channel selection functions are equivalent to the function that
always selects the smallest-index channel under appropriate channel relabeling.
This leads to a natural representation of a consistent channel hopping
algorithm as a sequence of permutations. For the two-user MRP, we characterize
rendezvous time slots using a fictitious user and derive tight bounds on the
maximum time-to-rendezvous (MTTR) and expected time-to-rendezvous (ETTR).
Notably, the ETTR is shown to be the inverse of the Jaccard index when
permutations are randomly selected. We also prove that consistent channel
hopping algorithms maximize the rendezvous probability. To reduce
implementation complexity, we propose the modulo algorithm, which uses modular
arithmetic with one-cycle permutations and achieves performance comparable to
locality-sensitive hashing (LSH)-based algorithms. The framework is extended to
multiple users, with novel strategies such as stick-together, spread-out, and a
hybrid method that accelerates rendezvous in both synchronous and asynchronous
settings. Simulation results confirm the effectiveness and scalability of the
proposed algorithms.

</details>


### [32] [Solving the Problem of Poor Internet Connectivity in Dhaka: Innovative Solutions Using Advanced WebRTC and Adaptive Streaming Technologies](https://arxiv.org/abs/2506.17343)
*Pavel Malinovskiy*

Main category: cs.NI

TL;DR: 该论文提出了一种通过集成WebRTC技术和自适应流媒体解决方案来改善达卡市移动数据连接性能的框架，显著提升了网络性能。


<details>
  <summary>Details</summary>
Motivation: 达卡作为人口密集城市，网络连接问题严重，需要一种创新的解决方案来提升移动数据的可靠性和速度。

Method: 结合动态转码、实时纠错和优化接口选择的技术，并分析了网络速度、基站密度和社交媒体使用等数据。

Result: 实验结果显示，该方法显著提高了吞吐量、降低了延迟，并提升了整体服务质量。

Conclusion: 该框架为高密度城市环境中的下一代通信系统提供了可扩展的解决方案。

Abstract: Dhaka, Bangladesh, one of the world's most densely populated cities, faces
severe challenges in maintaining reliable, high-speed internet connectivity.
This paper presents an innovative framework that addresses poor mobile data
connections through the integration of advanced WebRTC technology with adaptive
streaming and server-side recording solutions. Focusing on the unique network
conditions in Dhaka in 2025, our approach combines dynamic transcoding,
real-time error correction, and optimized interface selection to enhance
connectivity. We analyze empirical data on connection speeds, mobile tower
density, district-level population statistics, and social media usage.
Extensive mathematical formulations, including novel models for bitrate
estimation, round-trip time optimization, and reliability analysis, are
provided alongside detailed diagrams and multiple examples of code in both
Python and C++. Experimental results demonstrate significant improvements in
throughput, latency reduction, and overall service quality, offering a scalable
blueprint for next-generation communication systems in hyper-dense urban
environments.

</details>


### [33] [Supporting Deterministic Traffic on Standard NICs](https://arxiv.org/abs/2506.17877)
*Chuanyu Xue,Tianyu Zhang,Andrew Loveless,Song Han*

Main category: cs.NI

TL;DR: KeepON提出了一种基于软件的驱动模型，支持在标准网卡上实现确定性包传输，实验表明其调度准确性显著优于默认驱动和硬件解决方案。


<details>
  <summary>Details</summary>
Motivation: 关键任务的网络应用需要确定性包传输，但标准终端设备缺乏原生支持，且专用硬件兼容性有限。

Method: 通过让网卡持续传输固定大小的数据块作为占位符，并在指定位置替换为实时包，确保传输时间的精确性。

Result: KeepON在标准硬件上的调度准确性达到默认驱动的162倍，硬件解决方案的2.6倍。

Conclusion: KeepON能够在低成本标准硬件上实现高精度的确定性包传输，解决了兼容性和成本问题。

Abstract: Networked mission-critical applications (e.g., avionic control and industrial
automation systems) require deterministic packet transmissions to support a
range of sensing and control tasks with stringent timing constraints. While
specialized network infrastructure (e.g., time-sensitive networking (TSN)
switches) provides deterministic data transport across the network, achieving
strict end-to-end timing guarantees requires equally capable end devices to
support deterministic traffic. These end devices, however, often employ
general-purpose computing platforms like standard PCs, which lack native
support for deterministic traffic and suffer from unpredictable delays
introduced by their software stack and system architecture. Although
specialized NICs with hardware scheduling offload can mitigate this problem,
the limited compatibility hinders their widespread adoption, particularly for
cost-sensitive applications or in legacy devices.
  To fill this gap, this paper proposes a novel software-based driver model,
namely KeepON, to enable the support of deterministic packet transmissions on
end devices equipped with standard NICs. The key idea of KeepON is to have the
NIC keep on transmitting fixed-size data chunks as placeholders, thereby
maintaining a predictable temporal transmission pattern. The real-time packets
generated by the mission-critical application(s) will then be precisely
inserted into this stream by replacing placeholders at the designated position
to ensure their accurate transmission time. We implement and evaluate KeepON by
modifying the network driver on a Raspberry Pi using its standard NIC. Our
experiments demonstrate that KeepON can achieve x162 times scheduling accuracy
comparable to its default driver, and x2.6 times compared to hardware-based
solution, thus enabling precise timing control on standard commodity hardware.

</details>


### [34] [VReaves: Eavesdropping on Virtual Reality App Identity and Activity via Electromagnetic Side Channels](https://arxiv.org/abs/2506.17570)
*Sun Wei,Fang Minghong,Li Mengyuan*

Main category: cs.NI

TL;DR: VR设备无意中发出的电磁辐射可被利用进行应用识别和活动追踪，VReaves系统通过信号处理和机器学习实现了这一点。


<details>
  <summary>Details</summary>
Motivation: VR设备的安全性，尤其是硬件层面的电磁辐射泄漏问题尚未被充分研究。

Method: 通过信号处理流程分析VR设备中嵌入式传感器的电磁辐射，并利用机器学习模型识别应用和活动。

Result: 实验证明，VReaves能有效通过电磁辐射侧信道识别VR应用和活动。

Conclusion: VR设备的电磁辐射侧信道存在安全隐患，需引起重视。

Abstract: Virtual reality (VR) has recently proliferated significantly, consisting of
headsets or head-mounted displays (HMDs) and hand controllers for an embodied
and immersive experience. The VR device is usually embedded with different
kinds of IoT sensors, such as cameras, microphones, communication sensors, etc.
However, VR security has not been scrutinized from a physical hardware point of
view, especially electromagnetic emanations (EM) that are automatically and
unintentionally emitted from the VR headset. This paper presents VReaves, a
system that can eavesdrop on the electromagnetic emanation side channel of a VR
headset for VR app identification and activity recognition. To do so, we first
characterize the electromagnetic emanations from the embedded IoT sensors
(e.g., cameras and microphones) in the VR headset through a signal processing
pipeline and further propose machine learning models to identify the VR app and
recognize the VR app activities. Our experimental evaluation with commercial
off-the-shelf VR devices demonstrates the efficiency of VR app identification
and activity recognition via electromagnetic emanation side channel.

</details>


### [35] [Non-Intrusive MLOps-Driven Performance Intelligence in Software Data Planes](https://arxiv.org/abs/2506.17658)
*Qiong Liu,Jianke Lin,Tianzhu Zhang,Leonardo Linguaglossa*

Main category: cs.NI

TL;DR: 论文提出了一个轻量级、非侵入式的在线性能推断和适应框架DRST，用于解决NFV基础设施中资源竞争导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: NFV因其灵活性和成本效益而广泛应用，但资源共享导致的性能问题在高带宽环境下尤为突出，现有方案因集成复杂和系统限制难以适用。

Method: 通过复用底层NFV基础设施的硬件特性，避免了直接数据平面采集，提出DRST框架，支持无需预定义流量模型或VNF定制化的轻量级MLOps流程。

Result: DRST框架在多样化NFV场景中表现出准确的性能推断、运行时瓶颈诊断和自动化适应能力。

Conclusion: 研究补充了现有解决方案，提供了一种低开销、高效的性能管理方法。

Abstract: The last decade has witnessed the proliferation of network function
virtualization (NFV) in the telco industry, thanks to its unparalleled
flexibility, scalability, and cost-effectiveness. However, as the NFV
infrastructure is shared by virtual network functions (VNFs), sporadic resource
contentions are inevitable. Such contention makes it extremely challenging to
guarantee the performance of the provisioned network services, especially in
high-speed regimes (e.g., Gigabit Ethernet). Existing solutions typically rely
on direct traffic analysis (e.g., packet- or flow-level measurements) to detect
performance degradation and identify bottlenecks, which is not always
applicable due to significant integration overhead and system-level
constraints.
  This paper complements existing solutions with a lightweight, non-intrusive
framework for online performance inference and adaptation. Instead of direct
data-plane collection, we reuse hardware features in the underlying NFV
infrastructure, introducing negligible interference in the data plane. This
framework can be integrated into existing NFV systems with minimal engineering
effort and operates without the need for predefined traffic models or
VNF-specific customization. Through comprehensive evaluation across diverse NFV
scenarios, our Drift-Resilient and Self-Tuning (DRST) framework delivers
accurate performance inference, runtime bottleneck diagnose, and automated
adaptation under runtime drift, via a lightweight MLOps pipeline.

</details>


### [36] [Location Information Sharing Using Software Defined Radio in Multi-UAV Systems](https://arxiv.org/abs/2506.17678)
*Mehmet Kaan Erol,Eyup Emre Ulku*

Main category: cs.NI

TL;DR: 研究通过在SDR（软件定义无线电）上建立多通道无线通信网络（FANET），共享位置信息，并在实际测试环境中验证。


<details>
  <summary>Details</summary>
Motivation: 解决现有通信层结构通常在仿真环境中测试，未能充分反映实际硬件、软件及环境因素的局限性。

Method: 采用多通道令牌循环作为信道接入协议，并使用GNU Radio平台进行SDR软件开发。

Result: 开发了清晰的、可复现的块图和代码，并在实际环境中验证了通信性能。

Conclusion: 研究通过实际测试补充了仿真环境的不足，并提供开源实现以促进进一步研究。

Abstract: SDR (Software Defined Radio) provides flexible, reproducible, and
longer-lasting radio tools for military and civilian wireless communications
infrastructure. SDR is a radio communication system whose components are
implemented as software. This study aims to establish multi-channel wireless
communication with FANET between two SDRs to share location information and
examine it in a realistic test environment. We used multi-channel token
circulation as a channel access protocol and GNU Radio platform for SDR
software development. The structures of the communication layer, including the
protocols, communication systems, and network structures suggested in the
studies in the literature, are generally tested in the simulation environment.
The simulation environment provides researchers with fast and easy development
and testing, but disadvantages exist. These cause a product to be isolated from
hardware, software, and cost effects encountered while developing and
environmental factors affecting the communication channel while testing.
Another contribution of the study is to present the developed block diagrams
and codes as clear and reproducible. The developed software and block diagrams
are available at github.com/knrl/uav-in-802.11-gnuradio.

</details>


### [37] [The Blind Spot of BGP Anomaly Detection: Why LSTM Autoencoders Fail on Real-World Outages](https://arxiv.org/abs/2506.17821)
*Samuel Oluwafemi Adebayo*

Main category: cs.NI

TL;DR: 深度学习在检测BGP异常时存在盲点，仅依赖高复杂度异常假设不充分。


<details>
  <summary>Details</summary>
Motivation: 探讨现有BGP异常检测模型是否有效应对多样化真实事件，如信号丢失或低偏差签名。

Method: 使用LSTM自编码器对比真实历史事件（如Slammer蠕虫）和模拟BGP风暴。

Result: 模型识别高复杂度异常但忽略信号丢失或低偏差事件，误将灾难性故障视为稳定信号。

Conclusion: 需混合多模态检测系统以全面覆盖BGP异常类型，确保安全性。

Abstract: Deep learning has significant potential to make the Internet's Border Gateway
Protocol (BGP) secure by detecting anomalous routing activity. However, all but
a few of these approaches rely on the implicit assumption that anomalies
manifest as noisy, high-complexity outliers from some normal baseline. This
work challenges this assumption by investigating if a best-in-class detection
model built on this assumption can effectively deal with real-world security
events' diverse signatures. We employ an LSTM-based autoencoder, a classical
example of a reconstruction-based anomaly detector, as our test vehicle. We
then contrast this model with a representative sampling of historical BGP
anomalies, including the Slammer worm and the Moscow blackout, and with a
simulated 'BGP storm' designed as a positive control. Our experience unveils a
blind spot of our model: the model easily identifies the synthetic anomaly of
high complexity but invariably fails to identify real-world events that
manifest in the form of a "signal loss" (e.g., Slammer, Moscow Blackout) or
"low-deviation" (e.g., WannaCry) signature. We demonstrate that the model
mistakenly recognizes the abrupt cut-off of BGP updates during catastrophic
failures as a signal of extreme stability, leading to reconstruction errors of
virtually zero and total failure to detect. We conclude that the
characterization of BGP anomalies as high-reconstruction-error events alone is
a weak and dangerous oversimplification. Our research provides the data-driven
case for why hybrid, multi-modal detection systems capable of identifying both
high-complexity and signal-loss signatures are required to enable end-to-end
BGP security.

</details>


### [38] [LiSec-RTF: Reinforcing RPL Resilience Against Routing Table Falsification Attack in 6LoWPAN](https://arxiv.org/abs/2506.17911)
*Shefali Goel,Vinod Kumar Verma,Abhishek Verma*

Main category: cs.NI

TL;DR: 本文介绍了一种轻量级安全方案LiSec-RTF，利用物理不可克隆函数（PUFs）生成独特的认证码，以应对RPL协议中的路由表伪造（RTF）攻击，提升了网络性能。


<details>
  <summary>Details</summary>
Motivation: RPL协议在6LoWPAN网络中因控制消息未认证而容易受到路由表伪造攻击，目前缺乏有效的解决方案。

Method: 提出基于PUFs的LiSec-RTF方案，生成独特认证码（Licenses）来验证路由消息。

Result: 实验表明LiSec-RTF能显著提高网络性能，减少RTF攻击的影响。

Conclusion: LiSec-RTF是一种适用于静态和移动场景的轻量级安全解决方案，有效提升了RPL协议的可靠性。

Abstract: Routing Protocol for Low-Power and Lossy Networks (RPL) is an
energy-efficient routing solution for IPv6 over Low-Power Wireless Personal
Area Networks (6LoWPAN), recommended for resource-constrained devices. While
RPL offers significant benefits, its security vulnerabilities pose challenges,
particularly due to unauthenticated control messages used to establish and
maintain routing information. These messages are susceptible to manipulation,
enabling malicious nodes to inject false routing data. A notable security
concern is the Routing Table Falsification (RTF) attack, where attackers forge
Destination Advertisement Object (DAO) messages to promote fake routes via a
parent nodes routing table. Experimental results indicate that RTF attacks
significantly reduce packet delivery ratio, increase end-to-end delay, and
leverage power consumption. Currently, no effective countermeasures exist in
the literature, reinforcing the need for a security solution to prevent network
disruption and protect user applications. This paper introduces a Lightweight
Security Solution against Routing Table Falsification Attack (LiSec-RTF),
leveraging Physical Unclonable Functions (PUFs) to generate unique
authentication codes, termed Licenses. LiSec-RTF mitigates RTF attack impact
while considering the resource limitations of 6LoWPAN devices in both static
and mobile scenarios. Our testbed experiments indicate that LiSec-RTF
significantly improves network performance compared to standard RPL under RTF
attacks, thereby ensuring reliable and efficient operation.

</details>


### [39] [Mapping The Invisible Internet: Framework and Dataset](https://arxiv.org/abs/2506.18159)
*Siddique Abubakr Muntaka,Jacques Bou Abdo,Kemi Akanbi,Sunkanmi Oluwadare,Faiza Hussein,Oliver Konyo,Michael Asante*

Main category: cs.NI

TL;DR: 论文提出了一个聚焦于I2P网络层的新数据集，填补了先前研究主要集中应用层的空白。


<details>
  <summary>Details</summary>
Motivation: 此前研究多关注暗网等应用层，而本研究通过收集I2P网络层数据，提供更全面的网络分析基础。

Method: 使用SWARM-I2P框架部署I2P路由器作为映射代理，结合动态端口映射，通过路由器查询、netDb分析和被动监控收集数据。

Result: 数据集包含50,000多个节点，详尽的带宽、延迟和运行时间指标，以及大量流量记录和地理位置分布数据。

Conclusion: 该数据集可用于隧道对等选择、匿名网络弹性研究和对抗建模等应用。

Abstract: This article presents a novel dataset focusing on the network layer of the
Invisible Internet Project (I2P), where prior research has predominantly
examined application layers like the dark web. Data was collected through the
SWARM- I2P framework, deploying I2P routers as mapping agents, utilizing
dynamic port mapping (30000-50000 range). The dataset documents over 50,000
nodes, including 2,077 FastSet nodes and 2,331 high-capacity nodes
characterized by bandwidth, latency (mean 121.21ms +- 48.50), and uptime
metrics. It contains 1,997 traffic records (1,003,032 packets/bytes) and
4,222,793 records (2,147,585,625 packets/bytes), with geographic distributions
for 3,444 peers showing capacity metrics (mean 8.57 +- 1.20). Collection
methods included router console queries (127.0.0.1:port/tunnels), netDb
analysis, and passive monitoring, with anonymized identifiers. Data is
structured in CSV/TXT formats (Zenodo) with collection scripts (GitHub).
Potential applications include tunnel peer selection analysis, anonymity
network resilience studies, and adversarial modelling.

</details>


### [40] [XR Offloading Across Multiple Time Scales: The Roles of Power, Temperature, and Energy](https://arxiv.org/abs/2506.18584)
*Francesco Malandrino,Olga Chukhno,Alessandro Catania,Antonella Molinaro,Carla Fabiana Chiasserini*

Main category: cs.NI

TL;DR: 该论文提出了一种名为TAO的温度感知卸载策略，用于优化XR设备的计算任务分配，考虑了瞬时功耗、短期温度波动和长期电池寿命，显著降低了卸载成本。


<details>
  <summary>Details</summary>
Motivation: XR设备（如可穿戴设备）需要在严格的延迟限制下处理大量计算任务，而现有卸载策略未能全面考虑不同时间尺度的影响。

Method: 作者引入了一个综合系统模型，捕捉了功耗、温度和能量的动态变化，并提出了一种随机且静态的卸载策略TAO。

Result: 实验证明，TAO相比现有方法能减少超过35%的卸载成本，同时不违反设备运行限制。

Conclusion: TAO通过全面考虑时间尺度的影响，为XR设备的任务卸载提供了一种高效且可靠的解决方案。

Abstract: Extended reality (XR) devices, commonly known as wearables, must handle
significant computational loads under tight latency constraints. To meet these
demands, they rely on a combination of on-device processing and edge
offloading. This letter focuses on offloading strategies for wearables by
considering their impact across three time scales: instantaneous power
consumption, short-term temperature fluctuations, and long-term battery
duration. We introduce a comprehensive system model that captures these
temporal dynamics, and propose a stochastic and stationary offloading strategy,
called TAO (for temperature-aware offloading), designed to minimize the
offloading cost while adhering to power, thermal, and energy constraints. Our
performance evaluation, leveraging COMSOL models of real-world wearables,
confirms that TAO reduces offloading cost by over 35% compared to
state-of-the-art approaches, without violating the wearable operational limits.

</details>


### [41] [RL-Driven Semantic Compression Model Selection and Resource Allocation in Semantic Communication Systems](https://arxiv.org/abs/2506.18660)
*Xinyi Lin,Peizheng Li,Adnan Aijaz*

Main category: cs.NI

TL;DR: 提出了一种基于强化学习的框架，用于多用户语义通信系统中的语义压缩模型选择和资源分配，通过定义RDE指标优化图像重构质量与通信性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有语义通信系统忽视用户间计算和通信能力差异的问题，需在语义准确性、延迟和能耗之间自适应平衡。

Method: 采用PPO强化学习方法动态选择SCM并分配带宽和功率，定义RDE指标作为系统级优化标准。

Result: 仿真表明该方法优于多种基线策略，并讨论了框架的泛化能力、复杂性、可扩展性和实际应用。

Conclusion: 该框架为实际语义通信系统提供了高效的解决方案，未来可进一步推广和应用。

Abstract: Semantic communication (SemCom) is an emerging paradigm that leverages
semantic-level understanding to improve communication efficiency, particularly
in resource-constrained scenarios. However, existing SemCom systems often
overlook diverse computational and communication capabilities and requirements
among different users. Motivated by the need to adaptively balance semantic
accuracy, latency, and energy consumption, this paper presents a reinforcement
learning (RL)-driven framework for semantic compression model (SCM) selection
and resource allocation in multi-user SemCom systems. To address the challenges
of balancing image reconstruction quality and communication performance, a
system-level optimization metric called Rate-Distortion Efficiency (RDE) has
been defined. The framework considers multiple SCMs with varying complexity and
resource requirements. A proximal policy optimization (PPO)-based RL approach
is developed to dynamically select SCMs and allocate bandwidth and power under
non-convex constraints. Simulations demonstrate that the proposed method
outperforms several baseline strategies. This paper also discusses the
generalization ability, computational complexity, scalability, and practical
implications of the framework for real-world SemCom systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [42] [Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?](https://arxiv.org/abs/2506.17623)
*Yuesheng Huang,Peng Zhang,Riliang Liu,Jiaqi Liang*

Main category: cs.MM

TL;DR: 研究表明，文本到图像（T2I）模型生成的图像可以作为文本中心任务的有效补充模态，但效果高度依赖于文本与生成图像的语义对齐、任务的‘视觉可接地性’以及T2I模型的生成保真度。


<details>
  <summary>Details</summary>
Motivation: 弥补文本数据与多模态模型之间的‘模态差距’，探索动态生成的图像是否能为纯文本任务提供额外价值。

Method: 通过文本分类任务的综合评估框架，分析T2I模型质量、提示工程策略和多模态融合架构的影响。

Result: ‘合成感知’方法显著提升了性能，但效果受限于语义对齐、任务视觉可接地性和T2I模型质量。

Conclusion: 该范式为丰富单模态场景下的语言理解提供了可行路径，但需进一步优化语义对齐和生成质量。

Abstract: A significant ``modality gap" exists between the abundance of text-only data
and the increasing power of multimodal models. This work systematically
investigates whether images generated on-the-fly by Text-to-Image (T2I) models
can serve as a valuable complementary modality for text-centric tasks. Through
a comprehensive evaluation framework on text classification, we analyze the
impact of critical variables, including T2I model quality, prompt engineering
strategies, and multimodal fusion architectures. Our findings demonstrate that
this``synthetic perception" can yield significant performance gains, even when
augmenting strong large language model baselines. However, we find the
effectiveness of this approach is highly conditional, depending critically on
the semantic alignment between text and the generated image, the inherent
``visual groundability" of the task, and the generative fidelity of the T2I
model. Our work establishes the first rigorous benchmark for this paradigm,
providing a clear analysis of its potential and current limitations, and
demonstrating its viability as a pathway to enrich language understanding in
traditionally unimodal scenarios.

</details>


### [43] [Face-Voice Association for Audiovisual Active Speaker Detection in Egocentric Recordings](https://arxiv.org/abs/2506.18055)
*Jason Clarke,Yoshihiko Gotoh,Stefan Goetze*

Main category: cs.MM

TL;DR: SL-ASD 是一种基于跨模态人脸-声音关联的主动说话者检测框架，在自我中心视角（egocentric）场景下表现优于或等同于传统同步方法，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 自我中心视角下的音频视觉同步方法因遮挡、运动模糊和恶劣的声学条件效果不佳，需要替代方案。

Method: 整合人脸-声音关联模型与基于Transformer的编码器，动态加权每帧视觉质量，并配合前端语音分段方法。

Result: SL-ASD 在性能上媲美或超过传统方法，且参数更少。

Conclusion: 在挑战性场景中，可用人脸-声音关联替代严格的音视频同步建模。

Abstract: Audiovisual active speaker detection (ASD) is conventionally performed by
modelling the temporal synchronisation of acoustic and visual speech cues. In
egocentric recordings, however, the efficacy of synchronisation-based methods
is compromised by occlusions, motion blur, and adverse acoustic conditions. In
this work, a novel framework is proposed that exclusively leverages cross-modal
face-voice associations to determine speaker activity. An existing face-voice
association model is integrated with a transformer-based encoder that
aggregates facial identity information by dynamically weighting each frame
based on its visual quality. This system is then coupled with a front-end
utterance segmentation method, producing a complete ASD system. This work
demonstrates that the proposed system, Self-Lifting for audiovisual active
speaker detection(SL-ASD), achieves performance comparable to, and in certain
cases exceeding, that of parameter-intensive synchronisation-based approaches
with significantly fewer learnable parameters, thereby validating the
feasibility of substituting strict audiovisual synchronisation modelling with
flexible biometric associations in challenging egocentric scenarios.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [44] [Modal Logic for Stratified Becoming: Actualization Beyond Possible Worlds](https://arxiv.org/abs/2506.17276)
*Alexandre Le Nepvou*

Main category: cs.LO

TL;DR: 本文提出了一种基于分层实现的新模态逻辑框架，替代传统的全局可能世界模型。


<details>
  <summary>Details</summary>
Motivation: 传统克里普克语义学无法捕捉局部、动态和非对称的实际化过程。

Method: 开发了分层实现逻辑（SAL），通过索引模态运算符到本体稳定性层级，构建结构化的可能性层。

Result: 证明了SAL的语法、语义、公理及完备性，并应用于时间演变、量子退相干等领域。

Conclusion: SAL提供了一种不依赖抽象可能世界的分层模态逻辑，丰富了模态实在论。

Abstract: This article develops a novel framework for modal logic based on the idea of
stratified actualization, rather than the classical model of global possible
worlds. Traditional Kripke semantics treat modal operators as quantification
over fully determinate alternatives, neglecting the local, dynamic, and often
asymmetric nature of actualization processes. We propose a system Stratified
Actualization Logic (SAL) in which modalities are indexed by levels of
ontological stability, interpreted as admissibility regimes. Each modality
operates over a structured layer of possibility, grounded in the internal
coherence of transitions between layers. We formally define the syntax and
semantics of SAL, introduce its axioms, and prove soundness and completeness.
Applications are discussed in connection with temporal becoming, quantum
decoherence domains, and modal metaphysics. The result is a logic that captures
the ontological structure of actualization without recourse to abstract
possible worlds, offering a stratified alternative to standard modal realism.

</details>


### [45] [Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems](https://arxiv.org/abs/2506.17331)
*Craig Steven Wright*

Main category: cs.LO

TL;DR: 开发了一个AI系统框架，支持结构化推理和矛盾检测，确保其合理性。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在严格认知约束下的表现问题，超越随机语言预测。

Method: 结合符号推理、知识图谱和区块链技术，形式化信念表示和验证。

Result: 实现了能保持真理且可审计的认知代理。

Conclusion: 该框架为AI系统提供了更高级的认知能力，适用于需严格验证的场景。

Abstract: This paper develops a comprehensive framework for artificial intelligence
systems that operate under strict epistemic constraints, moving beyond
stochastic language prediction to support structured reasoning, propositional
commitment, and contradiction detection. It formalises belief representation,
metacognitive processes, and normative verification, integrating symbolic
inference, knowledge graphs, and blockchain-based justification to ensure
truth-preserving, auditably rational epistemic agents.

</details>


### [46] [ARCH-COMP25 Category Report: Stochastic Models](https://arxiv.org/abs/2506.17602)
*Alessandro Abate,Omid Akbarzadeh,Henk A. P. Blom,Sofie Haesaert,Sina Hassani,Abolfazl Lavaei,Frederik Baymler Mathiesen,Rahul Misra,Amy Nejati,Mathis Niehage,Fie Ørum,Anne Remke,Behrad Samari,Ruohan Wang,Rafal Wisniewski,Ben Wooding,Mahdieh Zaker*

Main category: cs.LO

TL;DR: 本文介绍了一场关于随机模型形式验证和策略合成的友好竞赛，包括新基准、工具和未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 推动随机模型形式验证和策略合成领域的研究，通过引入新基准和工具促进比较和进步。

Method: 介绍了三种新开发的软件工具、一个新的供水网络基准和一组简化基准。

Result: 竞赛在2025年夏季的ARCH研讨会上成功举办，为新工具和基准的比较提供了平台。

Conclusion: 报告总结了竞赛成果，并提出了明年竞赛的改进建议。

Abstract: This report is concerned with a friendly competition for formal verification
and policy synthesis of stochastic models. The main goal of the report is to
introduce new benchmarks and their properties within this category and
recommend next steps toward next year's edition of the competition. In
particular, this report introduces three recently developed software tools, a
new water distribution network benchmark, and a collection of simplified
benchmarks intended to facilitate further comparisons among tools that were
previously not directly comparable. This friendly competition took place as
part of the workshop Applied Verification for Continuous and Hybrid Systems
(ARCH) in Summer 2025.

</details>


### [47] [Computational Complexity of Model-Checking Quantum Pushdown Systems](https://arxiv.org/abs/2506.18439)
*Deren Lin,Tianrong Lin*

Main category: cs.LO

TL;DR: 该论文研究了从计算复杂性角度对量子下推系统进行模型检验的问题，提出了量子下推系统和量子马尔可夫链的新概念，并探讨了其与概率计算树逻辑的关系。研究发现，无状态量子下推系统对PCTL的模型检验通常是不可判定的，但对有界的PCTL是可判定的，且该问题是NP完全的。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展需要新的理论框架来描述和验证量子系统的行为，因此研究量子下推系统的模型检验问题具有重要理论价值。

Method: 通过扩展概率下推系统和马尔可夫链到量子版本，并探讨是否需要定义量子概率计算树逻辑。研究了无状态量子下推系统对PCTL和bPCTL的模型检验问题。

Result: 发现无状态量子下推系统对PCTL的模型检验是不可判定的，但对bPCTL是可判定的且为NP完全问题。

Conclusion: 量子下推系统的模型检验问题在特定条件下是可判定的，但一般情况下不可判定，这为量子计算的理论研究提供了新的复杂性结果。

Abstract: In this paper, we study the problem of model-checking quantum pushdown
systems from a computational complexity point of view. We arrive at the
following equally important, interesting new results:
  We first extend the notions of the {\it probabilistic pushdown systems} and
{\it Markov chains} to their quantum analogues and investigate the question of
whether it is necessary to define a quantum analogue of {\it probabilistic
computational tree logic} to describe the probabilistic and branching-time
properties of the {\it quantum Markov chain}. We study its model-checking
question and show that model-checking of {\it stateless quantum pushdown
systems (qBPA)} against {\it probabilistic computational tree logic (PCTL)} is
generally undecidable, i.e., there exists no algorithm for model-checking {\it
stateless quantum pushdown systems} against {\it probabilistic computational
tree logic}.
  We then study in which case there exists an algorithm for model-checking {\it
stateless quantum pushdown systems} and show that the problem of model-checking
{\it stateless quantum pushdown systems} against {\it bounded probabilistic
computational tree logic} (bPCTL) is decidable, and further show that this
problem is in $NP$-complete. Our reduction is from the {\it bounded Post
Correspondence Problem} for the first time, a well-known $NP$-complete problem.

</details>


### [48] [Deciding Termination of Simple Randomized Loops](https://arxiv.org/abs/2506.18541)
*Éléanore Meyer,Jürgen Giesl*

Main category: cs.LO

TL;DR: 该论文证明了对于一类简单随机化程序的通用正几乎必然终止性（UPAST）是可判定的，即可以判定这些程序在所有输入下的期望运行时是否有限。


<details>
  <summary>Details</summary>
Motivation: 研究随机化程序的UPAST问题是为了扩展对程序终止性的理解，尤其是在输入范围广泛的背景下。

Method: 研究包含单循环、线性循环条件和由两个线性可交换且可对角化更新组成的循环体的程序，其中每次迭代随机选择更新操作。

Result: 证明了这类程序的UPAST是可判定的，且适用于变量的多种子半环范围。

Conclusion: 该研究扩展了Tiwari在2004年提出的研究方向，将结果推广到随机化程序中。

Abstract: We show that universal positive almost sure termination (UPAST) is decidable
for a class of simple randomized programs, i.e., it is decidable whether the
expected runtime of such a program is finite for all inputs. Our class contains
all programs that consist of a single loop, with a linear loop guard and a loop
body composed of two linear commuting and diagonalizable updates. In each
iteration of the loop, the update to be carried out is picked at random,
according to a fixed probability. We show the decidability of UPAST for this
class of programs, where the program's variables and inputs may range over
various sub-semirings of the real numbers. In this way, we extend a line of
research initiated by Tiwari in 2004 into the realm of randomized programs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [49] [Making the Right Thing: Bridging HCI and Responsible AI in Early-Stage AI Concept Selection](https://arxiv.org/abs/2506.17494)
*Ji-Youn Jung,Devansh Saxena,Minjung Park,Jini Kim,Jodi Forlizzi,Kenneth Holstein,John Zimmerman*

Main category: cs.HC

TL;DR: 本文探讨了如何在AI创新的早期阶段通过设计实验和多学科合作，有效地识别并整合负责任AI（RAI）原则，以减少失败风险并提升项目成功率。


<details>
  <summary>Details</summary>
Motivation: AI项目常因早期决策失误而失败，但目前缺乏实用的方法来在早期阶段识别有潜力的概念并融入RAI原则。

Method: 通过三个设计实验（包括与行业从业者的探针研究），采用多学科协作的方法评估AI概念的风险与收益。

Result: 参与者表现出对早期处理RAI问题的高度接受度，并能有效识别低风险、高收益的AI概念。

Conclusion: 设计主导的方法在AI创新前端嵌入伦理和服务设计思维具有潜力，呼吁HCI和RAI社区共同关注早期创新的伦理与商业考量。

Abstract: AI projects often fail due to financial, technical, ethical, or user
acceptance challenges -- failures frequently rooted in early-stage decisions.
While HCI and Responsible AI (RAI) research emphasize this, practical
approaches for identifying promising concepts early remain limited. Drawing on
Research through Design, this paper investigates how early-stage AI concept
sorting in commercial settings can reflect RAI principles. Through three design
experiments -- including a probe study with industry practitioners -- we
explored methods for evaluating risks and benefits using multidisciplinary
collaboration. Participants demonstrated strong receptivity to addressing RAI
concerns early in the process and effectively identified low-risk, high-benefit
AI concepts. Our findings highlight the potential of a design-led approach to
embed ethical and service design thinking at the front end of AI innovation. By
examining how practitioners reason about AI concepts, our study invites HCI and
RAI communities to see early-stage innovation as a critical space for engaging
ethical and commercial considerations together.

</details>


### [50] [Full-body WPT: wireless powering with meandered e-textiles](https://arxiv.org/abs/2506.17606)
*Ryo Takahashi,Takashi Sato,Wakako Yukita,Tomoyuki Yokota,Takao Someya,Yoshihiro Kawahara*

Main category: cs.HC

TL;DR: 提出一种使用蛇形纺织线圈的全身无线电力传输技术，将强磁场限制在皮肤表面，提升安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统感应系统会向体内深部组织发射强磁场，存在安全问题。

Method: 使用蛇形纺织线圈和低损耗导电纱线设计局部感应系统。

Result: 通过仿真和实验验证了高功率传输效率和适应性。

Conclusion: 该系统为可穿戴健康监测、增强现实和人机交互提供了安全高效的分布式电力网络基础。

Abstract: We present Full-body WPT, wireless power networking around the human body
using a meandered textile coil. Unlike traditional inductive systems that emit
strong fields into the deep tissue inside the body, the meander coil enables
localized generation of strong magnetic field constrained to the skin surface,
even when scaled to the size of the human body. Such localized inductive system
enhances both safety and efficiency of wireless power around the body.
Furthermore, the use of low-loss conductive yarn achieve energy-efficient and
lightweight design. We analyze the performance of our design through
simulations and experimental prototypes, demonstrating high power transfer
efficiency and adaptability to user movement and posture. Our system provides a
safe and efficient distributed power network using meandered textile coils
integrated into wearable materials, highlighting the potential of body-centric
wireless power networking as a foundational layer for ubiquitous health
monitoring, augmented reality, and human-machine interaction systems.

</details>


### [51] [One Does Not Simply 'Mm-hmm': Exploring Backchanneling in the AAC Micro-Culture](https://arxiv.org/abs/2506.17890)
*Tobias Weinberg,Claire O'Connor,Ricardo E. Gonzalez Penuela,Stephanie Valencia,Thijs Roumen*

Main category: cs.HC

TL;DR: 研究了辅助与替代沟通（AAC）技术用户的反馈行为，发现他们形成独特的沟通文化，并通过与技术改进的设计建议。


<details>
  <summary>Details</summary>
Motivation: 日常沟通中反馈行为（如点头、附和）对对话至关重要，但AAC用户受限于技术而表现不足，研究探讨其反馈行为的特点。

Method: 通过4名AAC用户的研讨会和4名言语病理学家的访谈，分析AAC用户与非AAC用户对话中反馈行为的差异。

Result: AAC用户形成了独特的反馈沟通方式，反馈行为在AAC用户之间与AAC用户与非AAC用户对话中存在差异。

Conclusion: 反馈行为是一种微观文化实践，需在AAC技术中重新思考具身性和中介性，设计多模态反馈功能时需要尊重不同沟通文化。

Abstract: Backchanneling (e.g., "uh-huh", "hmm", a simple nod) encompasses a big part
of everyday communication; it is how we negotiate the turn to speak, it signals
our engagement, and shapes the flow of our conversations. For people with
speech and motor impairments, backchanneling is limited to a reduced set of
modalities, and their Augmentative and Alternative Communication (AAC)
technology requires visual attention, making it harder to observe non-verbal
cues of conversation partners. We explore how users of AAC technology approach
backchanneling and create their own unique channels and communication culture.
We conducted a workshop with 4 AAC users to understand the unique
characteristics of backchanneling in AAC. We explored how backchanneling
changes when pairs of AAC users communicate vs when an AAC user communicates
with a non-AAC user. We contextualize these findings through four in-depth
interviews with speech-language pathologists (SLPs). We conclude with a
discussion about backchanneling as a micro-cultural practice, rethinking
embodiment and mediation in AAC technology, and providing design
recommendations for timely multi-modal backchanneling while respecting
different communication cultures.

</details>


### [52] [When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?](https://arxiv.org/abs/2506.17936)
*Romy Müller*

Main category: cs.HC

TL;DR: 论文探讨了C-XAI概念的可变性是否被用户识别和欣赏，发现在铁路安全场景中，用户对不相关特征泛化的概念评价较低。


<details>
  <summary>Details</summary>
Motivation: 研究用户是否能够识别和欣赏C-XAI中的泛化能力，尤其是在复杂任务（如安全评估）中。

Method: 通过实验铁路安全场景，参与者评估模拟AI的性能，AI提供的概念以相似图像片段形式呈现，分为高相关特征和低相关特征的匹配。

Result: 用户对不相关特征泛化的概念评价较低，但对相关特征的不精确非常敏感。

Conclusion: 用户可能无法自发识别泛化，质疑C-XAI是否能展示AI对复杂情境的深层理解。

Abstract: Concept-based explainable artificial intelligence (C-XAI) can help reveal the
inner representations of AI models. Understanding these representations is
particularly important in complex tasks like safety evaluation. Such tasks rely
on high-level semantic information (e.g., about actions) to make decisions
about abstract categories (e.g., whether a situation is dangerous). In this
context, it may desirable for C-XAI concepts to show some variability,
suggesting that the AI is capable of generalising beyond the concrete details
of a situation. However, it is unclear whether people recognise and appreciate
such generalisations and can distinguish them from other, less desirable forms
of imprecision. This was investigated in an experimental railway safety
scenario. Participants evaluated the performance of a simulated AI that
evaluated whether traffic scenes involving people were dangerous. To explain
these decisions, the AI provided concepts in the form of similar image
snippets. These concepts differed in their match with the classified image,
either regarding a highly relevant feature (i.e., relation to tracks) or a less
relevant feature (i.e., actions). Contrary to the hypotheses, concepts that
generalised over less relevant features led to ratings that were lower than for
precisely matching concepts and comparable to concepts that systematically
misrepresented these features. Conversely, participants were highly sensitive
to imprecisions in relevant features. These findings cast doubts on whether
people spontaneously recognise generalisations. Accordingly, they might not be
able to infer from C-XAI concepts whether AI models have gained a deeper
understanding of complex situations.

</details>


### [53] [Conceptualization, Operationalization, and Measurement of Machine Companionship: A Scoping Review](https://arxiv.org/abs/2506.18119)
*Jaime Banks,Zhixin Li*

Main category: cs.HC

TL;DR: 这篇论文通过系统性综述，探讨了机器伴侣（MC）作为一个正式概念的定义和测量变量，并提出了文献指导的定义。


<details>
  <summary>Details</summary>
Motivation: 尽管机器伴侣在社交技术想象中广泛存在，但缺乏对其作为正式概念或测量变量的深入研究。

Method: 采用PRISMA指导的范围综述方法，系统筛选、调查和综合了2017至2025年的71篇学术文献。

Result: 研究发现MC的定义和测量变量存在广泛差异，最终提出了MC的定义：一种自主的、协调的人机关系，随时间展开且主观积极。

Conclusion: 论文为机器伴侣的研究提供了理论基础和定义框架，填补了现有研究的空白。

Abstract: The notion of machine companions has long been embedded in
social-technological imaginaries. Recent advances in AI have moved those media
musings into believable sociality manifested in interfaces, robotic bodies, and
devices. Those machines are often referred to colloquially as "companions" yet
there is little careful engagement of machine companionship (MC) as a formal
concept or measured variable. This PRISMA-guided scoping review systematically
samples, surveys, and synthesizes current scholarly works on MC (N = 71;
2017-2025), to that end. Works varied widely in considerations of MC according
to guiding theories, dimensions of a-priori specified properties (subjectively
positive, sustained over time, co-active, autotelic), and in measured concepts
(with more than 50 distinct measured variables). WE ultimately offer a
literature-guided definition of MC as an autotelic, coordinated connection
between human and machine that unfolds over time and is subjectively positive.

</details>


### [54] [AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System](https://arxiv.org/abs/2506.18143)
*Lancelot Blanchard,Cameron Holt,Joseph A. Paradiso*

Main category: cs.HC

TL;DR: AI Harmonizer 是一种能够自动生成四部和声的工具，无需用户输入和声信息，利用先进的AI技术实现。


<details>
  <summary>Details</summary>
Motivation: 旨在为独唱歌手提供一种无需音乐专业知识即可生成丰富和声的工具，简化传统和声生成方法。

Method: 结合生成式AI技术和符号音乐模型，通过音高检测和声音建模自动生成和声。

Result: 系统可离线生成丰富的合唱纹理，未来可能实现实时应用。

Conclusion: AI Harmonizer 为AI辅助音乐表演和创作开辟了新方向。

Abstract: Vocals harmonizers are powerful tools to help solo vocalists enrich their
melodies with harmonically supportive voices. These tools exist in various
forms, from commercially available pedals and software to custom-built systems,
each employing different methods to generate harmonies. Traditional harmonizers
often require users to manually specify a key or tonal center, while others
allow pitch selection via an external keyboard-both approaches demanding some
degree of musical expertise. The AI Harmonizer introduces a novel approach by
autonomously generating musically coherent four-part harmonies without
requiring prior harmonic input from the user. By integrating state-of-the-art
generative AI techniques for pitch detection and voice modeling with
custom-trained symbolic music models, our system arranges any vocal melody into
rich choral textures. In this paper, we present our methods, explore potential
applications in performance and composition, and discuss future directions for
real-time implementations. While our system currently operates offline, we
believe it represents a significant step toward AI-assisted vocal performance
and expressive musical augmentation. We release our implementation on GitHub.

</details>


### [55] [Two Sonification Methods for the MindCube](https://arxiv.org/abs/2506.18196)
*Fangzheng Liu,Lancelot Blanchard,Don D. Haddad,Joseph A. Paradiso*

Main category: cs.HC

TL;DR: 探索MindCube作为音乐界面的潜力，利用其传感器和输入设备设计两种映射（含AI和不含AI），用于情感调节的音乐系统。


<details>
  <summary>Details</summary>
Motivation: MindCube是一款用于研究情感的交互设备，适合作为音乐系统的控制器，帮助调节情绪。

Method: 设计了两种MindCube映射方案，一种包含生成式AI，提出在潜在空间中注入意义并通过外部控制器导航的技术。

Result: 讨论了研究成果，并提出了未来工作的方向。

Conclusion: MindCube在音乐界面和情感调节方面具有潜力，AI技术的应用为未来研究提供了新方向。

Abstract: In this work, we explore the musical interface potential of the MindCube, an
interactive device designed to study emotions. Embedding diverse sensors and
input devices, this interface resembles a fidget cube toy commonly used to help
users relieve their stress and anxiety. As such, it is a particularly
well-suited controller for musical systems that aim to help with emotion
regulation. In this regard, we present two different mappings for the MindCube,
with and without AI. With our generative AI mapping, we propose a way to infuse
meaning within a latent space and techniques to navigate through it with an
external controller. We discuss our results and propose directions for future
work.

</details>


### [56] [Co-persona: Leveraging LLMs and Expert Collaboration to Understand User Personas through Social Media Data Analysis](https://arxiv.org/abs/2506.18269)
*Min Yin,Haoyu Liu,Boyi Lian,Chunlei Chai*

Main category: cs.HC

TL;DR: 该研究提出了一个名为Co-Persona的框架，结合LLMs和专家验证，通过分析小红书上的3800万条帖子，为B.Co公司开发床头灯产品提取了五种用户画像。


<details>
  <summary>Details</summary>
Motivation: 通过大规模社交媒体数据与用户理解的结合，帮助制造商更好地解读用户行为并指导产品设计和营销策略。

Method: 采用多阶段NLP处理技术，结合LLMs和专家验证，分析用户夜间行为并提取用户画像。

Result: 揭示了五种用户画像（如健康爱好者、夜猫子等），每种画像有独特的睡前活动和产品偏好。

Conclusion: Co-Persona框架为制造商提供了基于数据的用户洞察，推动了理论和实践中的用户画像研究。

Abstract: This study introduces \textsc{Co-Persona}, a framework bridging large-scale
social media analysis and user understanding via integration of Large Language
Models (LLMs) and expert validation. Through a case study of B.Co, a Chinese
manufacturer, we applied \textsc{Co-Persona} to bedside lamp development by
analyzing 38 million posts from Xiao Hongshu. Our multi-stage NLP processing
revealed five user personas based on nighttime behaviors: Health Aficionados,
Night Owls, Interior Decorators, Child-care Workers, and Workaholics. These
personas exhibit distinct pre-sleep activities and product preferences. The
method enhances manufacturers' ability to interpret social data while
preserving user-centric insights, offering actionable strategies for targeted
marketing and product design. This work advances both theoretical persona
development and practical consumer-driven innovation.

</details>


### [57] [Supporting Car-Following Behavior through V2V-Based Beyond-Visual-Range Information Display](https://arxiv.org/abs/2506.18308)
*Feiqi Gu,Zhixiong Wang,Zhenyu Wang,Dengbo He*

Main category: cs.HC

TL;DR: 研究探讨了通过车联网（V2V）通信提供超视距（BVR）信息对跟车行为安全的影响，设计四种人机界面（HMI）进行实验，发现BVR信息能提升安全，但不同HMI效果不同。


<details>
  <summary>Details</summary>
Motivation: 追尾事故频发，传统的前方碰撞预警效果有限。研究希望通过扩展驾驶员的感知范围，利用V2V通信提供的BVR信息，提升跟车行为的安全。

Method: 设计了四种HMI（Brake-HMI、Dis-HMI、THW-HMI、Video-HMI），提供不同类型的BVR信息，并通过驾驶模拟器实验（40名参与者）评估其效果。

Result: BVR信息整体能提升安全，尤其是新手驾驶员，Brake-HMI在链式制动事件中表现最佳，而Video-HMI增加了注意力负担但无显著益处。

Conclusion: 利用V2V通信提供BVR信息可以有效提升跟车行为的安全性，但需选择合适的信息呈现方式以避免额外负担。

Abstract: Rear-end collisions constituted a large portion of crashes on the road,
despite efforts to mitigate rear-end collisions, such as forward collision
warnings. The chance of rear-end collisions is closely related to drivers'
car-following (CF) behaviors in the traffic flow. Given that drivers may rely
on more than the information of the direct lead vehicle (DLV) when making CF
decisions, expanding drivers' perceptual range by providing beyond-visual-range
(BVR) information based on vehicle-to-vehicle (V2V) communication may enhance
CF safety. Thus, four different human-machine interfaces (HMIs) providing
various types of BVR information in CF events were designed, including
Brake-HMI showing only brake action of indirect lead vehicles (ILV), Dis-HMI
and THW-HMI showing the relative distance and time headway between the ILV and
DLV, respectively, and Video-HMI showing the live-stream video of ILV from the
perspective of DLV. A driving simulator experiment with 40 participants was
conducted to evaluate the impact of BVR-based HMI on driving safety in CF
events. We found that, in general, BVR information could improve CF safety
without overloading drivers and compromising their visual attention allocation
strategies, particularly among novice drivers, by enabling quicker brake
responses and increasing time headway and time-to-collision in brake events.
The Brake-HMI yielded the safest performance in chain brake events, whereas
Video-HMI increased attentional demands without observable benefits. This
research provides insights into enabling drivers' BVR perception based on V2V
communication to enhance driving safety in CF scenarios.

</details>


### [58] [Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi Ranging](https://arxiv.org/abs/2506.18317)
*Emerson Sie,Enguang Fan,Federico Cifuentes-Urtubey,Deepak Vasisht*

Main category: cs.HC

TL;DR: PeepLoc是一种基于Wi-Fi的室内定位系统，利用现有设备和基础设施，无需额外硬件，实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 现有室内定位方法实用性不足，PeepLoc旨在提供一种可部署、可扩展的解决方案。

Method: 结合非协作飞行时间测量和基于行人航位推算的众包机制，利用现有Wi-Fi设备实现定位。

Result: 在4座校园建筑中测试，平均和中位定位误差分别为3.41米和3.06米。

Conclusion: PeepLoc性能优于现有室内定位系统，且可与室外GPS相媲美。

Abstract: Indoor localization opens the path to potentially transformative
applications. Although many indoor localization methods have been proposed over
the years, they remain too impractical for widespread deployment in the real
world. In this paper, we introduce PeepLoc, a deployable and scalable
Wi-Fi-based solution for indoor localization that relies only on pre-existing
devices and infrastructure. Specifically, PeepLoc works on any mobile device
with an unmodified Wi-Fi transceiver and in any indoor environment with a
sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the
core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain
non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel
bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and
crowdsourcing to opportunistically initialize pre-existing APs as anchor points
within an environment. We implement PeepLoc using commodity hardware and
evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a
mean and median positional error of 3.41 m and 3.06 m respectively, which is
superior to existing deployed indoor localization systems and is competitive
with commodity GPS in outdoor environments.

</details>


### [59] [CODS : A Theoretical Model for Computational Design Based on Design Space](https://arxiv.org/abs/2506.18455)
*Nan Cao,Xiaoyu Qi,Chuer Chen,Xiaoke Yan*

Main category: cs.HC

TL;DR: CODS是一个理论模型，将计算设计视为设计空间中的约束优化问题，通过LLM自动生成约束，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有设计方法依赖手工启发式或领域特定规则，缺乏通用性和可解释性。CODS旨在提供统一的AI设计自动化框架。

Method: 利用大语言模型通过结构化提示工程自动生成软硬约束，指导设计空间中的优化过程。

Result: 在可视化设计和针织品生成领域，CODS在设计质量、意图对齐和用户偏好上表现优于现有LLM方法。

Conclusion: CODS为可扩展、可控的AI设计自动化提供了统一的基础。

Abstract: We introduce CODS (Computational Optimization in Design Space), a theoretical
model that frames computational design as a constrained optimization problem
over a structured, multi-dimensional design space. Unlike existing methods that
rely on handcrafted heuristics or domain-specific rules, CODS provides a
generalizable and interpretable framework that supports diverse design tasks.
Given a user requirement and a well-defined design space, CODS automatically
derives soft and hard constraints using large language models through a
structured prompt engineering pipeline. These constraints guide the
optimization process to generate design solutions that are coherent,
expressive, and aligned with user intent. We validate our approach across two
domains-visualization design and knitwear generation-demonstrating superior
performance in design quality, intent alignment, and user preference compared
to existing LLM-based methods. CODS offers a unified foundation for scalable,
controllable, and AI-powered design automation.

</details>


### [60] [Crowdsourcing eHMI Designs: A Participatory Approach to Autonomous Vehicle-Pedestrian Communication](https://arxiv.org/abs/2506.18605)
*Ronald Cumbal,Didem Gurdur Broo,Ginevra Castellano*

Main category: cs.HC

TL;DR: 通过参与式和众包方法，研究收集用户生成的自动驾驶车辆外部人机界面（eHMI）设计，发现用户偏好灯光、符号和文字等多模态通信方式。


<details>
  <summary>Details</summary>
Motivation: 为确保自动驾驶车辆在共享环境中的安全，需要有效的通信方式。用户早期参与设计可解决技术开发中的关键挑战。

Method: 采用参与式、众包方法进行两阶段研究，首先引入eHMI概念，收集用户设计草图；后续改进任务目标并鼓励深入反思。

Result: 用户偏好多模态通信（如灯光、符号、文本），强调方向性和适应性，同时结合熟悉元素提升直观性。

Conclusion: 用户早期参与设计可优化eHMI的功能与直观性，多模态通信方式被广泛认可为提升自动驾驶车辆与人类交互的有效手段。

Abstract: As autonomous vehicles become more integrated into shared human environments,
effective communication with road users is essential for ensuring safety. While
previous research has focused on developing external Human-Machine Interfaces
(eHMIs) to facilitate these interactions, we argue that involving users in the
early creative stages can help address key challenges in the development of
this technology. To explore this, our study adopts a participatory,
crowd-sourced approach to gather user-generated ideas for eHMI designs.
Participants were first introduced to fundamental eHMI concepts, equipping them
to sketch their own design ideas in response to scenarios with varying levels
of perceived risk. An initial pre-study with 29 participants showed that while
they actively engaged in the process, there was a need to refine task
objectives and encourage deeper reflection. To address these challenges, a
follow-up study with 50 participants was conducted. The results revealed a
strong preference for autonomous vehicles to communicate their awareness and
intentions using lights (LEDs and projections), symbols, and text.
Participants' sketches prioritized multi-modal communication, directionality,
and adaptability to enhance clarity, consistently integrating familiar vehicle
elements to improve intuitiveness.

</details>


### [61] [Deceptive Game Design? Investigating the Impact of Visual Card Style on Player Perception](https://arxiv.org/abs/2506.18648)
*Leonie Kallabis,Timo Bertram,Florian Rupp*

Main category: cs.HC

TL;DR: 研究了游戏卡牌视觉风格（可爱与英雄风）对玩家感知卡牌强度的影响，发现玩家对此的感知存在个体差异。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉风格如何影响玩家对卡牌实际强度的判断。

Method: 通过单盲调查，使用《魔法风云会》的AI生成卡牌，对比可爱与英雄风格。

Result: 玩家对风格影响的感知呈正态分布，但个体差异显著。

Conclusion: 视觉风格对卡牌强度感知有一定影响，但效果因人而异。

Abstract: The visual style of game elements considerably contributes to the overall
experience. Aesthetics influence player appeal, while the abilities of game
pieces define their in-game functionality. In this paper, we investigate how
the visual style of collectible cards influences the players' perception of the
card's actual strength in the game. Using the popular trading card game Magic:
The Gathering, we conduct a single-blind survey study that examines how players
perceive the strength of AI-generated cards that are shown in two contrasting
visual styles: cute and harmless, or heroic and mighty. Our analysis reveals
that some participants are influenced by a card's visual appearance when
judging its in-game strength. Overall, differences in style perception are
normally distributed around a neutral center, but individual participants vary
in both directions: some generally perceive the cute style to be stronger,
whereas others believe that the heroic style is better.

</details>


### [62] [Fanfiction in the Age of AI: Community Perspectives on Creativity, Authenticity and Adoption](https://arxiv.org/abs/2506.18706)
*Roi Alfassi,Angelora Cooper,Zoe Mitchell,Mary Calabro,Orit Shaer,Osnat Mokryn*

Main category: cs.HC

TL;DR: 研究探讨了GenAI在粉丝小说创作中的影响，分析了157名社区成员对AI生成内容的态度，揭示了从谨慎接受到担忧的多方面反应。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI如何影响创意社区的动态，尤其是粉丝小说中的故事创作与共享方式。

Method: 调查了157名活跃粉丝小说社区成员的看法，包括读者和作者。

Result: 发现成员对AI的态度多样，从接受创意增强到担忧真实性、伦理和社交连接。

Conclusion: 研究强调需要设计干预措施，确保AI在创意平台中的伦理整合，保持社区核心价值。

Abstract: The integration of Generative AI (GenAI) into creative communities, like
fanfiction, is reshaping how stories are created, shared, and valued. This
study investigates the perceptions of 157 active fanfiction members, both
readers and writers, regarding AI-generated content in fanfiction. Our research
explores the impact of GenAI on community dynamics, examining how AI affects
the participatory and collaborative nature of these spaces. The findings reveal
responses ranging from cautious acceptance of AI's potential for creative
enhancement to concerns about authenticity, ethical issues, and the erosion of
human-centered values. Participants emphasized the importance of transparency
and expressed worries about losing social connections. Our study highlights the
need for thoughtful AI integration in creative platforms using design
interventions that enable ethical practices, promote transparency, increase
engagement and connection, and preserve the community's core values.

</details>


### [63] [LLM-enhanced Interactions in Human-Robot Collaborative Drawing with Older Adults](https://arxiv.org/abs/2506.18711)
*Marianne Bossema,Somaya Ben Allouch,Aske Plaat,Rob Saunders*

Main category: cs.HC

TL;DR: 研究旨在探索支持老年人参与人机共创创意体验的因素，通过设计“与机器人一起绘画”课程并观察互动，发现老年人更倾向于担任策展人角色，评估机器人的创意建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何利用机器人增强老年人的创造力体验，填补这一领域的研究空白。

Method: 采用参与式方法，与专业艺术教育者合作设计课程“与机器人一起绘画”，观察互动并进行访谈与分析。

Result: 发现老年人更喜欢作为策展人角色评估机器人建议；增强多模态大语言模型的机器人受好评，但反馈缺乏对上下文的理解和艺术目标的敏感性。

Conclusion: 研究表明LLM增强的机器人具有支持创造力的潜力，并为未来人机共创研究提供了方向。

Abstract: The goal of this study is to identify factors that support and enhance older
adults' creative experiences in human-robot co-creativity. Because the research
into the use of robots for creativity support with older adults remains
underexplored, we carried out an exploratory case study. We took a
participatory approach and collaborated with professional art educators to
design a course Drawing with Robots for adults aged 65 and over. The course
featured human-human and human-robot drawing activities with various types of
robots. We observed collaborative drawing interactions, interviewed
participants on their experiences, and analyzed collected data. Findings show
that participants preferred acting as curators, evaluating creative suggestions
from the robot in a teacher or coach role. When we enhanced a robot with a
multimodal Large Language Model (LLM), participants appreciated its spoken
dialogue capabilities. They reported however, that the robot's feedback
sometimes lacked an understanding of the context, and sensitivity to their
artistic goals and preferences. Our findings highlight the potential of
LLM-enhanced robots to support creativity and offer future directions for
advancing human-robot co-creativity with older adults.

</details>


### [64] [AutoGraph: A Knowledge-Graph Framework for Modeling Interface Interaction and Automating Procedure Execution in Digital Nuclear Control Rooms](https://arxiv.org/abs/2506.18727)
*Xingyu Xiao,Jiejuan Tong,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

TL;DR: AutoGraph是一种基于知识图谱的框架，旨在数字化核电厂控制室中自动化执行程序，减少人为错误并提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机化程序（CBPs）缺乏与人类系统界面（HSI）的语义集成，限制了智能自动化的能力并增加了人为错误风险，尤其是在动态或复杂条件下。

Method: AutoGraph通过四个模块实现：HTRPM跟踪模块捕获操作员交互、IE-KG编码HSI属性、自动将文本程序映射为可执行路径、以及执行引擎。

Result: 验证表明，该框架显著减少了任务完成时间，并支持实时人类可靠性评估。

Conclusion: AutoGraph可扩展性高，能增强复杂社会技术系统中的程序安全性和认知性能。

Abstract: Digitalization in nuclear power plant (NPP) control rooms is reshaping how
operators interact with procedures and interface elements. However, existing
computer-based procedures (CBPs) often lack semantic integration with
human-system interfaces (HSIs), limiting their capacity to support intelligent
automation and increasing the risk of human error, particularly under dynamic
or complex operating conditions. In this study, we present AutoGraph, a
knowledge-graph-based framework designed to formalize and automate procedure
execution in digitalized NPP environments.AutoGraph integrates (1) a proposed
HTRPM tracking module to capture operator interactions and interface element
locations; (2) an Interface Element Knowledge Graph (IE-KG) encoding spatial,
semantic, and structural properties of HSIs; (3) automatic mapping from textual
procedures to executable interface paths; and (4) an execution engine that maps
textual procedures to executable interface paths. This enables the
identification of cognitively demanding multi-action steps and supports fully
automated execution with minimal operator input. We validate the framework
through representative control room scenarios, demonstrating significant
reductions in task completion time and the potential to support real-time human
reliability assessment. Further integration into dynamic HRA frameworks (e.g.,
COGMIF) and real-time decision support systems (e.g., DRIF) illustrates
AutoGraph extensibility in enhancing procedural safety and cognitive
performance in complex socio-technical systems.

</details>


### [65] [Conceptual Modelling for Life Sciences Based on Systemist Foundations](https://arxiv.org/abs/2506.18742)
*R. Lukyanenko,O. Pastor,V. C. Storey*

Main category: cs.HC

TL;DR: 该论文提出了一种系统论的视角，用于创建生命科学问题的概念模型，并引入了一种新符号来捕获生命科学领域的重要语义。


<details>
  <summary>Details</summary>
Motivation: 生命科学问题的复杂性和重要性需要一个更好的概念模型来表示物理和数字世界之间的联系。

Method: 论文引入了系统概念，并开发了一种新的符号，结合系统论思维和基于本体论的组件。

Result: 新符号能够更好地表示生命科学领域的语义，支持精准医学和基因组信息系统的建模。

Conclusion: 系统论视角和新符号为生命科学的概念建模提供了更有效的工具，有助于理解和解决问题。

Abstract: All aspects of our society, including the life sciences, need a mechanism for
people working within them to represent the concepts they employ to carry out
their research. For the information systems being designed and developed to
support researchers and scientists in conducting their work, conceptual models
of the relevant domains are usually designed as both blueprints for a system
being developed and as a means of communication between the designer and
developer. Most conceptual modelling concepts are generic in the sense that
they are applied with the same understanding across many applications. Problems
in the life sciences, however, are especially complex and important, because
they deal with humans, their well-being, and their interactions with the
environment as well as other organisms. This work proposes a systemist
perspective for creating a conceptual model of a life scientist's problem. We
introduce the notion of a system and then show how it can be applied to the
development of an information system for handling genomic-related information.
We extend our discussion to show how the proposed systemist perspective can
support the modelling of precision medicine. This research recognizes
challenges in life sciences research of how to model problems to better
represent the connections between physical and digital worlds. We propose a new
notation that explicitly incorporates systemist thinking, as well as the
components of systems based on recent ontological foundations. The new notation
captures important semantics in the domain of life sciences. It may be used to
facilitate understanding, communication and problem-solving more broadly. We
also provide a precise, sound, ontologically supported characterization of the
term system, as a basic construct for conceptual modelling in life sciences.

</details>


### [66] [From Representation to Mediation: A New Agenda for Conceptual Modeling Research in A Digital World](https://arxiv.org/abs/2506.18743)
*J. Recker,R. Lukyanenko,M. A. Jabbari,B. M. Samuel,A. Castellanos*

Main category: cs.HC

TL;DR: 论文认为概念建模在数字化时代的研究价值不仅未减，反而更加重要，并提出新的理论框架以适配数字世界的需求。


<details>
  <summary>Details</summary>
Motivation: 探讨在数字化背景下，概念建模研究的持续重要性与新理论框架的必要性。

Method: 开发新的理论框架，将概念建模脚本作为物理与数字现实的桥梁，并提出相关研究问题。

Result: 框架提升了传统概念建模知识与数字世界需求的契合度，并提出了新的研究方向。

Conclusion: 概念建模研究需要新方法和语法，以应对数字时代的挑战。

Abstract: The role of information systems (IS) as representations of real-world systems
is changing in an increasingly digitalized world, suggesting that conceptual
modeling is losing its relevance to the IS field. We argue the opposite:
Conceptual modeling research is more relevant to the IS field than ever, but it
requires an update with current theory. We develop a new theoretical framework
of conceptual modeling that delivers a fundamental shift in the assumptions
that govern research in this area. This move can make traditional knowledge
about conceptual modeling consistent with the emerging requirements of a
digital world. Our framework draws attention to the role of conceptual modeling
scripts as mediators between physical and digital realities. We identify new
research questions about grammars, methods, scripts, agents, and contexts that
are situated in intertwined physical and digital realities. We discuss several
implications for conceptual modeling scholarship that relate to the necessity
of developing new methods and grammars for conceptual modeling, broadening the
methodological array of conceptual modeling scholarship, and considering new
dependent variables.

</details>


### [67] [BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility](https://arxiv.org/abs/2506.18749)
*Abdul Basit,Maha Nawaz,Muhammad Shafique*

Main category: cs.HC

TL;DR: BRAVE是一种结合脑电图（EEG）和语音控制的假肢系统，通过集成集成学习分类和人机交互（HITL）校正框架，实现高精度分类和低延迟响应。


<details>
  <summary>Details</summary>
Motivation: 现有基于EEG的假肢控制系统存在信号噪声、分类准确性和实时适应性等问题，BRAVE旨在通过非侵入方式实现更直观的控制。

Method: 采用LSTM、CNN和随机森林的集成学习框架处理EEG信号，并结合自动语音识别（ASR）实现模式切换。

Result: 分类准确率达到96%，响应延迟150毫秒，适用于多种用户。

Conclusion: BRAVE为实时、非侵入性假肢控制提供了可行的解决方案。

Abstract: Non-invasive brain-computer interfaces (BCIs) have the potential to enable
intuitive control of prosthetic limbs for individuals with upper limb
amputations. However, existing EEG-based control systems face challenges
related to signal noise, classification accuracy, and real-time adaptability.
In this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic
system that integrates ensemble learning-based EEG classification with a
human-in-the-loop (HITL) correction framework for enhanced responsiveness.
Unlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims
to interpret EEG-driven motor intent, enabling movement control without
reliance on residual muscle activity. To improve classification robustness,
BRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,
achieving a classification accuracy of 96% across test subjects. EEG signals
are preprocessed using a bandpass filter (0.5-45 Hz), Independent Component
Analysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature
extraction to minimize contamination from electromyographic (EMG) and
electrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic
speech recognition (ASR) to facilitate intuitive mode switching between
different degrees of freedom (DOF) in the prosthetic arm. The system operates
in real time, with a response latency of 150 ms, leveraging Lab Streaming Layer
(LSL) networking for synchronized data acquisition. The system is evaluated on
an in-house fabricated prosthetic arm and on multiple participants highlighting
the generalizability across users. The system is optimized for low-power
embedded deployment, ensuring practical real-world application beyond
high-performance computing environments. Our results indicate that BRAVE offers
a promising step towards robust, real-time, non-invasive prosthetic control.

</details>


### [68] [Patient-Centred Explainability in IVF Outcome Prediction](https://arxiv.org/abs/2506.18760)
*Adarsa Sivaprasad,Ehud Reiter,David McLernon,Nava Tintarev,Siladitya Bhattacharya,Nir Oren*

Main category: cs.HC

TL;DR: 论文评估了试管婴儿结果预测工具的用户界面，重点关注患者或潜在患者对其理解的程度。通过分析匿名患者反馈、用户调查和访谈，量化了信任和理解性。结果表明，普通用户需要超出模型特征空间的解释性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提高患者在试管婴儿结果预测工具中的理解和信任，尤其是在高风险医疗背景下，用户需要超出传统模型解释范围的个性化解释。

Method: 研究方法包括分析四年的匿名患者反馈、进行用户调查和访谈，以及提出基于对话的界面以解决用户需求。

Result: 研究发现，用户对数据变化和模型排除的担忧影响了信任，当前的解释性AI研究和设计未能满足用户对预测模型解释性的需求。

Conclusion: 论文结论呼吁在高风险医疗背景下，需要超越模型特征空间和认知假设的解释性设计，并探索个性化解释的用户期望。

Abstract: This paper evaluates the user interface of an in vitro fertility (IVF)
outcome prediction tool, focussing on its understandability for patients or
potential patients. We analyse four years of anonymous patient feedback,
followed by a user survey and interviews to quantify trust and
understandability. Results highlight a lay user's need for prediction model
\emph{explainability} beyond the model feature space. We identify user concerns
about data shifts and model exclusions that impact trust. The results call
attention to the shortcomings of current practices in explainable AI research
and design and the need for explainability beyond model feature space and
epistemic assumptions, particularly in high-stakes healthcare contexts where
users gather extensive information and develop complex mental models. To
address these challenges, we propose a dialogue-based interface and explore
user expectations for personalised explanations.

</details>


### [69] [Importance of User Control in Data-Centric Steering for Healthcare Experts](https://arxiv.org/abs/2506.18770)
*Aditya Bhattacharya,Simone Stumpf,Katrien Verbert*

Main category: cs.HC

TL;DR: 该论文研究了在医疗领域中人机协作中用户控制水平对数据中心转向的影响，发现手动转向能提升模型性能并保持信任，提出了一种结合手动和自动方法的混合转向系统设计。


<details>
  <summary>Details</summary>
Motivation: 随着AI在医疗等高风险领域的应用日益增多，研究用户控制水平如何影响医疗专家在数据中心转向中的作用，以优化人机协作效果。

Method: 通过一项包含74名医疗专家的混合方法用户研究，比较手动和自动转向方法的效果。

Result: 研究发现，手动转向能显著提升模型性能，并保持用户对系统的信任和理解。

Conclusion: 提出了一种结合手动和自动转向的混合系统设计，以增强人机协作中的用户参与度。

Abstract: As Artificial Intelligence (AI) becomes increasingly integrated into
high-stakes domains like healthcare, effective collaboration between healthcare
experts and AI systems is critical. Data-centric steering, which involves
fine-tuning prediction models by improving training data quality, plays a key
role in this process. However, little research has explored how varying levels
of user control affect healthcare experts during data-centric steering. We
address this gap by examining manual and automated steering approaches through
a between-subjects, mixed-methods user study with 74 healthcare experts. Our
findings show that manual steering, which grants direct control over training
data, significantly improves model performance while maintaining trust and
system understandability. Based on these findings, we propose design
implications for a hybrid steering system that combines manual and automated
approaches to increase user involvement during human-AI collaboration.

</details>


### [70] [Flow-Aware Diffusion for Real-Time VR Restoration: Enhancing Spatiotemporal Coherence and Efficiency](https://arxiv.org/abs/2506.18786)
*Yitong Zhu,Guanxuan Jiang,Zhuowen Liang,Yuyang Wang*

Main category: cs.HC

TL;DR: 论文提出了U-MAD，一种轻量级、实时、基于AI的方法，通过抑制图像层面的光学流动来减轻虚拟现实中的晕动症。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实中的过度光学流动与体感输入不匹配导致感官冲突和不适，现有方法依赖场景结构或硬件，难以灵活适用。

Method: 提出U-MAD，通过AI学习直接在渲染帧中减少高强度运动模式，无需网格编辑或场景适配。

Result: 实验证明U-MAD能减少光学流动并提升时间稳定性，用户研究表明其有效缓解了晕动症症状。

Conclusion: 光学流动的感知调制是创造更友好沉浸体验的有效且可扩展的方法。

Abstract: Cybersickness remains a critical barrier to the widespread adoption of
Virtual Reality (VR), particularly in scenarios involving intense or artificial
motion cues. Among the key contributors is excessive optical flow-perceived
visual motion that, when unmatched by vestibular input, leads to sensory
conflict and discomfort. While previous efforts have explored geometric or
hardware based mitigation strategies, such methods often rely on predefined
scene structures, manual tuning, or intrusive equipment. In this work, we
propose U-MAD, a lightweight, real-time, AI-based solution that suppresses
perceptually disruptive optical flow directly at the image level. Unlike prior
handcrafted approaches, this method learns to attenuate high-intensity motion
patterns from rendered frames without requiring mesh-level editing or scene
specific adaptation. Designed as a plug and play module, U-MAD integrates
seamlessly into existing VR pipelines and generalizes well to procedurally
generated environments. The experiments show that U-MAD consistently reduces
average optical flow and enhances temporal stability across diverse scenes. A
user study further confirms that reducing visual motion leads to improved
perceptual comfort and alleviated cybersickness symptoms. These findings
demonstrate that perceptually guided modulation of optical flow provides an
effective and scalable approach to creating more user-friendly immersive
experiences. The code will be released at https://github.com/XXXXX (upon
publication).

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [71] [FramePrompt: In-context Controllable Animation with Zero Structural Changes](https://arxiv.org/abs/2506.17301)
*Guian Fang,Yuchao Gu,Mike Zheng Shou*

Main category: cs.GR

TL;DR: 本文提出了一种名为FramePrompt的简洁框架，通过将参考图像、骨骼引导运动和目标视频片段视为统一的视觉序列，解决了可控角色动画生成的挑战，无需复杂架构或多阶段处理。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法在可控角色动画生成中因复杂架构或额外模块而导致的效率低下问题，本文旨在利用预训练视频扩散模型的视觉上下文建模能力。

Method: 提出了FramePrompt框架，将动画生成重新定义为条件式未来预测任务，统一处理视觉序列，避免使用额外的引导网络或结构修改。

Result: 实验证明，该方法在多种评估指标上显著优于基线模型，同时简化了训练流程。

Conclusion: 研究展示了序列级视觉条件的有效性，证明了预训练模型在不改变架构的情况下实现可控动画的潜力。

Abstract: Generating controllable character animation from a reference image and motion
guidance remains a challenging task due to the inherent difficulty of injecting
appearance and motion cues into video diffusion models. Prior works often rely
on complex architectures, explicit guider modules, or multi-stage processing
pipelines, which increase structural overhead and hinder deployment. Inspired
by the strong visual context modeling capacity of pre-trained video diffusion
transformers, we propose FramePrompt, a minimalist yet powerful framework that
treats reference images, skeleton-guided motion, and target video clips as a
unified visual sequence. By reformulating animation as a conditional future
prediction task, we bypass the need for guider networks and structural
modifications. Experiments demonstrate that our method significantly
outperforms representative baselines across various evaluation metrics while
also simplifying training. Our findings highlight the effectiveness of
sequence-level visual conditioning and demonstrate the potential of pre-trained
models for controllable animation without architectural changes.

</details>


### [72] [BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing](https://arxiv.org/abs/2506.17450)
*Jiacheng Chen,Ramin Mehran,Xuhui Jia,Saining Xie,Sanghyun Woo*

Main category: cs.GR

TL;DR: BlenderFusion是一个生成式视觉合成框架，通过重新组合物体、相机和背景合成新场景，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂场景编辑任务中物体、相机和背景的协同控制问题。

Method: 采用分层-编辑-合成流程，结合预训练扩散模型和关键训练策略（源掩码和模拟物体抖动）。

Result: 在复杂合成场景编辑任务中显著优于先前方法。

Conclusion: BlenderFusion通过创新的流程和训练策略，实现了高效的视觉场景编辑。

Abstract: We present BlenderFusion, a generative visual compositing framework that
synthesizes new scenes by recomposing objects, camera, and background. It
follows a layering-editing-compositing pipeline: (i) segmenting and converting
visual inputs into editable 3D entities (layering), (ii) editing them in
Blender with 3D-grounded control (editing), and (iii) fusing them into a
coherent scene using a generative compositor (compositing). Our generative
compositor extends a pre-trained diffusion model to process both the original
(source) and edited (target) scenes in parallel. It is fine-tuned on video
frames with two key training strategies: (i) source masking, enabling flexible
modifications like background replacement; (ii) simulated object jittering,
facilitating disentangled control over objects and camera. BlenderFusion
significantly outperforms prior methods in complex compositional scene editing
tasks.

</details>


### [73] [3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene](https://arxiv.org/abs/2506.17636)
*Shihan Chen,Zhaojin Li,Zeyu Chen,Qingsong Yan,Gaoyang Shen,Ran Duan*

Main category: cs.GR

TL;DR: 该论文提出了一种新方法，通过粗到精策略和自适应场景分割，结合解耦外观模型和瞬态掩码模型，成功解决了大规模场景下的三维高斯泼溅重建问题，并在公开数据集上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前三维高斯泼溅方法在大规模场景中面临高计算需求和复杂动态外观的挑战，限制了其在大场景应用中的性能。

Method: 采用粗到精策略，先高效构建粗糙模型，再自适应分割场景并从图像片段细化；整合解耦外观模型和瞬态掩码模型以处理全局外观变化和移动对象干扰；扩展多视角约束并引入单视角正则化。

Result: 在GauU-Scene V2数据集上的实验表明，该方法在视觉效果和表面重建准确性上优于现有基于NeRF和高斯泼溅的方法。

Conclusion: 论文提出的方法成功解决了大规模场景重建的难题，并通过开源代码推动进一步研究。

Abstract: Recent developments in 3D Gaussian Splatting have made significant advances
in surface reconstruction. However, scaling these methods to large-scale scenes
remains challenging due to high computational demands and the complex dynamic
appearances typical of outdoor environments. These challenges hinder the
application in aerial surveying and autonomous driving. This paper proposes a
novel solution to reconstruct large-scale surfaces with fine details,
supervised by full-sized images. Firstly, we introduce a coarse-to-fine
strategy to reconstruct a coarse model efficiently, followed by adaptive scene
partitioning and sub-scene refining from image segments. Additionally, we
integrate a decoupling appearance model to capture global appearance variations
and a transient mask model to mitigate interference from moving objects.
Finally, we expand the multi-view constraint and introduce a single-view
regularization for texture-less areas. Our experiments were conducted on the
publicly available dataset GauU-Scene V2, which was captured using unmanned
aerial vehicles. To the best of our knowledge, our method outperforms existing
NeRF-based and Gaussian-based methods, achieving high-fidelity visual results
and accurate surface from full-size image optimization. Open-source code will
be available on GitHub.

</details>


### [74] [Collaborative Texture Filtering](https://arxiv.org/abs/2506.17770)
*Tomas Akenine-Möller,Pontus Ebelin,Matt Pharr,Bartlomiej Wronski*

Main category: cs.GR

TL;DR: 该论文提出了一种新的纹理压缩算法，利用GPU波通信技术避免了重复的纹理解压缩，从而在放大过滤时实现了更高画质和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的纹理压缩技术在放大过滤时会导致视觉质量下降、噪声和闪烁，因此需要一种更高效的解压缩和过滤方法。

Method: 利用GPU波通信技术在着色器中共享已解码的纹理值，避免重复解压缩，并为特殊情况设计了高质量的备用过滤方法。

Result: 在足够大的放大因子下，实现了零误差过滤，且每次像素仅需≤1次纹理评估；在其他情况下，提出的备用方法也优于现有技术。

Conclusion: 新方法显著提升了纹理压缩在放大过滤时的视觉质量和效率，同时避免了传统方法的缺陷。

Abstract: Recent advances in texture compression provide major improvements in
compression ratios, but cannot use the GPU's texture units for decompression
and filtering. This has led to the development of stochastic texture filtering
(STF) techniques to avoid the high cost of multiple texel evaluations with such
formats. Unfortunately, those methods can give undesirable visual appearance
changes under magnification and may contain visible noise and flicker despite
the use of spatiotemporal denoisers. Recent work substantially improves the
quality of magnification filtering with STF by sharing decoded texel values
between nearby pixels (Wronski 2025). Using GPU wave communication intrinsics,
this sharing can be performed inside actively executing shaders without memory
traffic overhead. We take this idea further and present novel algorithms that
use wave communication between lanes to avoid repeated texel decompression
prior to filtering. By distributing unique work across lanes, we can achieve
zero-error filtering using <=1 texel evaluations per pixel given a sufficiently
large magnification factor. For the remaining cases, we propose novel filtering
fallback methods that also achieve higher quality than prior approaches.

</details>


### [75] [Auto-Regressive Surface Cutting](https://arxiv.org/abs/2506.18017)
*Yang Li,Victor Cheung,Xinhai Liu,Yuguang Chen,Zhongjin Luo,Biwen Lei,Haohan Weng,Zibo Zhao,Jingwei Huang,Zhuo Chen,Chunchao Guo*

Main category: cs.GR

TL;DR: SeamGPT是一种自回归模型，通过模拟专业工作流程生成切割线，将表面切割任务转化为token预测问题，在UV展开基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的表面切割方法常产生技术有效但语义不连贯的分割，需要更智能的解决方案。

Method: 将表面切割作为token预测任务，利用GPT风格转换器预测量化3D坐标的切割线段。

Result: SeamGPT在多种UV展开基准测试（包括流形和非流形网格）中表现卓越，并能改善3D分割工具的边界效果。

Conclusion: SeamGPT为表面切割任务提供了高效且语义一致的新方法，适用于多种应用场景。

Abstract: Surface cutting is a fundamental task in computer graphics, with applications
in UV parameterization, texture mapping, and mesh decomposition. However,
existing methods often produce technically valid but overly fragmented atlases
that lack semantic coherence. We introduce SeamGPT, an auto-regressive model
that generates cutting seams by mimicking professional workflows. Our key
technical innovation lies in formulating surface cutting as a next token
prediction task: sample point clouds on mesh vertices and edges, encode them as
shape conditions, and employ a GPT-style transformer to sequentially predict
seam segments with quantized 3D coordinates. Our approach achieves exceptional
performance on UV unwrapping benchmarks containing both manifold and
non-manifold meshes, including artist-created, and 3D-scanned models. In
addition, it enhances existing 3D segmentation tools by providing clean
boundaries for part decomposition.

</details>


### [76] [Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models](https://arxiv.org/abs/2506.18251)
*Chao Li,Jiawei Fan,Anbang Yao*

Main category: cs.GR

TL;DR: Morse是一个简单的双采样框架，通过快速跳跃采样和自适应残差反馈策略，无损加速扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型生成过程迭代耗时，Morse旨在通过双采样策略提高效率而不损失性能。

Method: Morse包含Dash和Dot两个模型：Dash是预训练的扩散模型，用于跳跃采样；Dot快速生成残差反馈，提升噪声估计。通过时间交错输出和权重共享策略优化效率。

Result: 实验表明，Morse在6项图像生成任务中平均无损加速1.78X至3.31X，并可推广到基于一致性蒸馏的模型（如LCM-SDXL）。

Conclusion: Morse通过双采样框架显著提高扩散模型的生成效率，同时保持性能无损。

Abstract: In this paper, we present Morse, a simple dual-sampling framework for
accelerating diffusion models losslessly. The key insight of Morse is to
reformulate the iterative generation (from noise to data) process via taking
advantage of fast jump sampling and adaptive residual feedback strategies.
Specifically, Morse involves two models called Dash and Dot that interact with
each other. The Dash model is just the pre-trained diffusion model of any type,
but operates in a jump sampling regime, creating sufficient space for sampling
efficiency improvement. The Dot model is significantly faster than the Dash
model, which is learnt to generate residual feedback conditioned on the
observations at the current jump sampling point on the trajectory of the Dash
model, lifting the noise estimate to easily match the next-step estimate of the
Dash model without jump sampling. By chaining the outputs of the Dash and Dot
models run in a time-interleaved fashion, Morse exhibits the merit of flexibly
attaining desired image generation performance while improving overall runtime
efficiency. With our proposed weight sharing strategy between the Dash and Dot
models, Morse is efficient for training and inference. Our method shows a
lossless speedup of 1.78X to 3.31X on average over a wide range of sampling
step budgets relative to 9 baseline diffusion models on 6 image generation
tasks. Furthermore, we show that our method can be also generalized to improve
the Latent Consistency Model (LCM-SDXL, which is already accelerated with
consistency distillation technique) tailored for few-step text-to-image
synthesis. The code and models are available at
https://github.com/deep-optimization/Morse.

</details>


### [77] [What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models](https://arxiv.org/abs/2506.18407)
*Yiyao Wang,Bo Pan,Ke Wang,Han Liu,Jinyuan Mao,Yuxin Liu,Minfeng Zhu,Bo Zhang,Weifeng Chen,Xiuqi Huang,Wei Chen*

Main category: cs.GR

TL;DR: 论文提出了一种基于多模态大语言模型（MLLMs）的WYTWYG框架，用于优化直接体渲染（DVR）中的传递函数（TFs）设计，解决了现有方法在探索空间大和泛化能力弱上的问题。


<details>
  <summary>Details</summary>
Motivation: 传递函数（TFs）在体数据渲染中至关重要，但设计高效TFs仍因用户意图与TF参数空间之间的语义差距而困难。现有优化方法面临探索空间大和泛化能力弱的挑战。

Method: 提出了WYTWYG框架，包含两部分：(1)基于进化的探索器用于高效探索TF空间，(2)基于MLLMs的渲染质量评估器提供通用视觉指导。并构建了交互设计系统。

Result: 通过三个案例研究验证了框架的通用性，并通过大量实验证明了各组件有效性。

Conclusion: WYTWYG框架通过结合进化和MLLMs，显著提升了TF优化的效率和泛化能力，为体渲染提供了新工具。

Abstract: Direct volume rendering (DVR) is a fundamental technique for visualizing
volumetric data, with transfer functions (TFs) playing a crucial role in
extracting meaningful structures. However, designing effective TFs remains
unintuitive due to the semantic gap between user intent and TF parameter space.
Researchers have developed numerous TF optimization methods to bridge this gap.
However, existing methods still face two challenges: large exploration space
and weak generalizability. To address these issues, we propose What You Think
is What You Get (WYTWYG) framework, which leveraging Multi-model Large Language
Models (MLLMs) to guide the TF optimization based on user intent. Specifically,
we first introduce a novel TF optimization approach comprising two core
components: (1) an evolution-based explorer for effective exploration of the TF
space, and (2) a volume rendering quality evaluator based on MLLMs to provide
generalizable visual guidance. We further propose a TF interactive design
system based on this approach. We demonstrate the general applicability of our
framework through three case studies, and validate the effectiveness of each
component through extensive experiments. Our code is available at:
https://github.com/wyysteelhead/TFevolve.

</details>


### [78] [BulletGen: Improving 4D Reconstruction with Bullet-Time Generation](https://arxiv.org/abs/2506.18601)
*Denys Rozumnyi,Jonathon Luiten,Numair Khan,Johannes Schönberger,Peter Kontschieder*

Main category: cs.GR

TL;DR: BulletGen利用生成模型校正高斯动态场景表示中的错误并补全缺失信息，通过扩散视频生成模型对齐4D重建，实现新视角合成和2D/3D跟踪的领先性能。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频动态场景重建中未观察区域和深度估计歧义性的挑战。

Method: 结合扩散视频生成模型与4D高斯动态场景表示，生成帧用于优化4D高斯模型。

Result: 在生成内容与静态/动态场景融合方面表现优异，新视角合成和跟踪任务达到SOTA。

Conclusion: BulletGen通过生成内容增强4D重建，显著提升动态场景的沉浸式体验质量。

Abstract: Transforming casually captured, monocular videos into fully immersive dynamic
experiences is a highly ill-posed task, and comes with significant challenges,
e.g., reconstructing unseen regions, and dealing with the ambiguity in
monocular depth estimation. In this work we introduce BulletGen, an approach
that takes advantage of generative models to correct errors and complete
missing information in a Gaussian-based dynamic scene representation. This is
done by aligning the output of a diffusion-based video generation model with
the 4D reconstruction at a single frozen "bullet-time" step. The generated
frames are then used to supervise the optimization of the 4D Gaussian model.
Our method seamlessly blends generative content with both static and dynamic
scene components, achieving state-of-the-art results on both novel-view
synthesis, and 2D/3D tracking tasks.

</details>


### [79] [DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling](https://arxiv.org/abs/2506.18680)
*Anindita Ghosh,Bing Zhou,Rishabh Dabral,Jian Wang,Vladislav Golyanik,Christian Theobalt,Philipp Slusallek,Chuan Guo*

Main category: cs.GR

TL;DR: DuetGen是一个新颖的框架，用于从音乐生成互动双人舞蹈。它通过两阶段方法（编码动作为离散令牌并生成令牌）和分层建模技术（捕捉交互细节），实现了舞蹈与音乐同步及舞伴协调，效果领先。


<details>
  <summary>Details</summary>
Motivation: 解决双人舞蹈生成的复杂挑战，包括舞伴间的互动同步以及与音乐的协调。

Method: 采用两阶段方法：1）用VQ-VAE将双人动作编码为分层离散令牌；2）通过掩码变换器从音乐生成分层令牌。

Result: 在基准数据集上表现出色，动作真实性、音乐舞蹈对齐及舞伴协调均达到最新水平。

Conclusion: DuetGen通过分层建模和交互表示，成功生成了高质量的同步互动双人舞蹈。

Abstract: We present DuetGen, a novel framework for generating interactive two-person
dances from music. The key challenge of this task lies in the inherent
complexities of two-person dance interactions, where the partners need to
synchronize both with each other and with the music. Inspired by the recent
advances in motion synthesis, we propose a two-stage solution: encoding
two-person motions into discrete tokens and then generating these tokens from
music. To effectively capture intricate interactions, we represent both
dancers' motions as a unified whole to learn the necessary motion tokens, and
adopt a coarse-to-fine learning strategy in both the stages. Our first stage
utilizes a VQ-VAE that hierarchically separates high-level semantic features at
a coarse temporal resolution from low-level details at a finer resolution,
producing two discrete token sequences at different abstraction levels.
Subsequently, in the second stage, two generative masked transformers learn to
map music signals to these dance tokens: the first producing high-level
semantic tokens, and the second, conditioned on music and these semantic
tokens, producing the low-level tokens. We train both transformers to learn to
predict randomly masked tokens within the sequence, enabling them to
iteratively generate motion tokens by filling an empty token sequence during
inference. Through the hierarchical masked modeling and dedicated interaction
representation, DuetGen achieves the generation of synchronized and interactive
two-person dances across various genres. Extensive experiments and user studies
on a benchmark duet dance dataset demonstrate state-of-the-art performance of
DuetGen in motion realism, music-dance alignment, and partner coordination.

</details>


### [80] [A B-Spline Finite Element Method for Cloth Simulation](https://arxiv.org/abs/2506.18867)
*Yuqi Meng,Yihao Shi,Kemeng Huang,Ning Guo,Taku Komura,Yin Yang,Minchen Li*

Main category: cs.GR

TL;DR: 提出了一种基于B样条的有限元方法（FEM）用于布料模拟，解决了线性FEM中常见的锁定伪影和网格依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的线性FEM在布料模拟中存在锁定伪影和网格依赖性问题，需要一种更平滑且高效的方法来改进模拟效果。

Method: 使用二次B样条基函数构建全局C1连续位移场，分别优化膜和弯曲能量的积分规则以提高计算效率。

Result: 实验验证表明，该方法在准确性、视觉效果和计算效率上优于线性FEM和近期高阶方法。

Conclusion: 该方法为布料模拟提供了一种新的空间离散化方案，能够真实模拟复杂褶皱动力学。

Abstract: We present a B-spline finite element method (FEM) for cloth simulation.
Building on quadratic B-spline basis functions, our method provides a globally
$C^1$-continuous displacement field, enabling consistent and accurate
discretization of both membrane and bending energies. This smooth
representation effectively mitigates locking artifacts and mesh dependency
issues commonly observed with linear FEM. To further improve efficiency, we
develop a reduced integration scheme that separately optimizes quadrature rules
for membrane and bending energies, further reducing computational overhead
while maintaining accuracy. We validate our approach through extensive
experiments, demonstrating improved accuracy, visual quality, and efficiency
compared to linear FEM and recent higher-order methods. Our method enables
realistic simulation of complex wrinkling dynamics across varying material
parameters, offering a promising new spatial discretization for cloth
simulation.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [81] [Efficient Beam Selection for ISAC in Cell-Free Massive MIMO via Digital Twin-Assisted Deep Reinforcement Learning](https://arxiv.org/abs/2506.18560)
*Jiexin Zhang,Shu Xu,Chunguo Li,Yongming Huang,Luxi Yang*

Main category: cs.ET

TL;DR: 论文提出了一种基于深度强化学习的波束选择方法，用于无蜂窝集成感知与通信系统，通过数字孪生和奖励设计减少实时交互成本，并在多种条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 在无蜂窝集成感知与通信系统中，波束成形对提升信号质量至关重要，但传统在线强化学习方法存在高成本和风险，需要更高效的解决方案。

Method: 通过马尔可夫决策过程建模波束选择，引入深度强化学习框架，设计数字孪生模块生成虚拟数据，并通过条件生成对抗网络解决数据多样性问题。

Result: 数值结果显示，该方法显著减少了在线交互开销，同时在严格误报率控制、低信噪比和高目标速度等条件下保持高效。

Conclusion: 提出的方法成功结合了数字孪生和离线强化学习，为无蜂窝ISAC系统提供了一种高效且低风险的波束选择方案。

Abstract: Beamforming enhances signal strength and quality by focusing energy in
specific directions. This capability is particularly crucial in cell-free
integrated sensing and communication (ISAC) systems, where multiple distributed
access points (APs) collaborate to provide both communication and sensing
services. In this work, we first derive the distribution of joint target
detection probabilities across multiple receiving APs under false alarm rate
constraints, and then formulate the beam selection procedure as a Markov
decision process (MDP). We establish a deep reinforcement learning (DRL)
framework, in which reward shaping and sinusoidal embedding are introduced to
facilitate agent learning. To eliminate the high costs and associated risks of
real-time agent-environment interactions, we further propose a novel digital
twin (DT)-assisted offline DRL approach. Different from traditional online DRL,
a conditional generative adversarial network (cGAN)-based DT module, operating
as a replica of the real world, is meticulously designed to generate virtual
state-action transition pairs and enrich data diversity, enabling offline
adjustment of the agent's policy. Additionally, we address the
out-of-distribution issue by incorporating an extra penalty term into the loss
function design. The convergency of agent-DT interaction and the upper bound of
the Q-error function are theoretically derived. Numerical results demonstrate
the remarkable performance of our proposed approach, which significantly
reduces online interaction overhead while maintaining effective beam selection
across diverse conditions including strict false alarm control, low
signal-to-noise ratios, and high target velocities.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [82] [Code Generation for Near-Roofline Finite Element Actions on GPUs from Symbolic Variational Forms](https://arxiv.org/abs/2506.17471)
*Kaushik Kulkarni,Andreas Klöckner*

Main category: cs.DC

TL;DR: 提出了一种用于GPU上评估有限元方法（FEM）变分形式的并行化策略，通过UFL语言在单纯形网格上实现。


<details>
  <summary>Details</summary>
Motivation: 目标是通过代码变换和启发式成本模型处理多样化的计算负载，以平衡延迟隐藏和状态空间，从而达到接近屋顶线性能。

Method: 设计了一个调度候选空间及其剪枝策略，结合成本模型优化配置选择，并在Firedrake框架中实现原型。

Result: 在Titan V和Tesla K40c GPU上测试，65%的案例中实现了超过50%的屋顶线性能。

Conclusion: 该方法在多种应用中表现优异，证明了其可行性和高效性。

Abstract: We present a novel parallelization strategy for evaluating Finite Element
Method (FEM) variational forms on GPUs, focusing on those that are expressible
through the Unified Form Language (UFL) on simplex meshes. We base our approach
on code transformations, wherein we construct a space of scheduling candidates
and rank them via a heuristic cost model to effectively handle the large
diversity of computational workloads that can be expressed in this way. We
present a design of a search space to which the cost model is applied, along
with an associated pruning strategy to limit the number of configurations that
need to be empirically evaluated. The goal of our design is to strike a balance
between the device's latency-hiding capabilities and the amount of state space,
a key factor in attaining near-roofline performance.
  To make our work widely available, we have prototyped our parallelization
strategy within the \textsc{Firedrake} framework, a UFL-based FEM solver. We
evaluate the performance of our parallelization scheme on two generations of
Nvidia GPUs, specifically the Titan V (Volta architecture) and Tesla K40c
(Kepler architecture), across a range of operators commonly used in
applications, including fluid dynamics, wave propagation, and structural
mechanics, in 2D and 3D geometries. Our results demonstrate that our proposed
algorithm achieves more than $50\%$ roofline performance in $65\%$ of the test
cases on both devices.

</details>


### [83] [PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning](https://arxiv.org/abs/2506.17338)
*Duong Bach*

Main category: cs.DC

TL;DR: 本文提出了一种名为Co-Forgetting Protocol的新框架，用于在多智能体系统中同步管理共享知识的内存修剪。通过上下文感知语义投票、多尺度时间衰减函数和PBFT共识机制，实现了高效的内存管理。实验结果显示协议在减少内存占用、投票准确性和共识成功率等方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，确保分布式内存保持同步、相关性并避免积累过时或无用的数据是一项关键挑战。本文旨在解决这一问题，借鉴生物遗忘机制，提出了一种同步内存修剪的协议。

Method: Co-Forgetting Protocol整合了三个核心组件：1) 上下文感知语义投票，利用轻量级DistilBERT模型评估内存项的相关性；2) 多尺度时间衰减函数，根据内存项的年龄和访问频率动态调整重要性；3) PBFT共识机制，确保决策在恶意或故障条件下仍可靠。

Result: 实验模拟显示，协议在500个周期内减少52%的内存占用，遗忘决策的投票准确率达88%，PBFT共识成功率为92%，内存访问缓存命中率为82%。

Conclusion: Co-Forgetting Protocol在多智能体系统中实现了高效且健壮的内存管理，为解决共享知识的同步和修剪问题提供了可行的解决方案。

Abstract: The proliferation of multi-agent systems (MAS) in complex, dynamic
environments necessitates robust and efficient mechanisms for managing shared
knowledge. A critical challenge is ensuring that distributed memories remain
synchronized, relevant, and free from the accumulation of outdated or
inconsequential data - a process analogous to biological forgetting. This paper
introduces the Co-Forgetting Protocol, a novel, comprehensive framework
designed to address this challenge by enabling synchronized memory pruning in
MAS. The protocol integrates three key components: (1) context-aware semantic
voting, where agents utilize a lightweight DistilBERT model to assess the
relevance of memory items based on their content and the current operational
context; (2) multi-scale temporal decay functions, which assign diminishing
importance to memories based on their age and access frequency across different
time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based
consensus mechanism, ensuring that decisions to retain or discard memory items
are agreed upon by a qualified and fault-tolerant majority of agents, even in
the presence of up to f Byzantine (malicious or faulty) agents in a system of N
greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient
inter-agent communication and Pinecone for scalable vector embedding storage
and similarity search, with SQLite managing metadata. Experimental evaluations
in a simulated MAS environment with four agents demonstrate the protocol's
efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%
voting accuracy in forgetting decisions against human-annotated benchmarks, a
92% PBFT consensus success rate under simulated Byzantine conditions, and an
82% cache hit rate for memory access.

</details>


### [84] [ConsumerBench: Benchmarking Generative AI Applications on End-User Devices](https://arxiv.org/abs/2506.17538)
*Yile Gu,Rohan Kadekodi,Hoang Nguyen,Keisuke Kamahori,Yiyu Liu,Baris Kasikci*

Main category: cs.DC

TL;DR: ConsumerBench是一个用于评估端用户设备上GenAI模型系统效率和响应时间的基准框架，模拟多应用场景并揭示资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 解决GenAI从云端转向端用户设备时在资源管理、系统效率和用户体验方面的新挑战。

Method: 通过模拟多应用并发运行的场景，定制复杂任务工作流，并捕获应用和系统级指标。

Result: 发现资源共享效率低、分配不公平及静态模型服务器配置的性能问题。

Conclusion: 为模型开发者和系统设计者提供实用见解，如定制内核和SLO感知调度策略的价值。

Abstract: The recent shift in Generative AI (GenAI) applications from cloud-only
environments to end-user devices introduces new challenges in resource
management, system efficiency, and user experience. This paper presents
ConsumerBench, a comprehensive benchmarking framework designed to evaluate the
system efficiency and response time of GenAI models running on end-user
devices. Unlike existing benchmarks that assume exclusive model access on
dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios
executing concurrently on constrained hardware. Furthermore, ConsumerBench
supports customizable workflows that simulate complex tasks requiring
coordination among multiple applications. ConsumerBench captures both
application-level metrics, including latency and Service Level Objective (SLO)
attainment, and system-level metrics like CPU/GPU utilization and memory
bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies
in resource sharing, unfair scheduling under greedy allocation, and performance
pitfalls of static model server configurations. The paper also provides
practical insights for model developers and system designers, highlighting the
benefits of custom kernels tailored to consumer-grade GPU architectures and the
value of implementing SLO-aware scheduling strategies.

</details>


### [85] [Speeding up Local Optimization in Vehicle Routing with Tensor-based GPU Acceleration](https://arxiv.org/abs/2506.17357)
*Zhenyu Lei,Jin-Kao Hao,Qinghua Wu*

Main category: cs.DC

TL;DR: 提出一种基于张量的GPU加速方法，优化车辆路径问题中的局部搜索算法，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 局部搜索在车辆路径问题中计算代价高，尤其是大规模或复杂约束问题，需高效加速方法。

Method: 采用基于属性的张量表示和GPU加速，设计低耦合架构，将计算任务完全卸载到GPU。

Result: 在三个路由问题的基准测试中，该方法比传统CPU实现显著提升计算效率，并可能改进解的质量。

Conclusion: 该方法为局部搜索提供高效加速方案，但其性能特点和潜在瓶颈仍需进一步分析优化。

Abstract: Local search plays a central role in many effective heuristic algorithms for
the vehicle routing problem (VRP) and its variants. However, neighborhood
exploration is known to be computationally expensive and time consuming,
especially for large instances or problems with complex constraints. In this
study, we explore a promising direction to address this challenge by
introducing an original tensor-based GPU acceleration method designed to speed
up the commonly used local search operators in vehicle routing. By using an
attribute-based representation, the method offers broad extensibility, making
it applicable to different VRP variants. Its low-coupling architecture, with
intensive computations completely offloaded to the GPU, ensures seamless
integration in various local search-based algorithms and frameworks, leading to
significant improvements in computational efficiency and potentially improved
solution quality. Through comparative experiments on benchmark instances of
three routing problems, we demonstrate the substantial computational advantages
of the proposed approach over traditional CPU-based implementations. We also
provide a detailed analysis of the strengths and limitations of the method,
providing valuable insights into its performance characteristics and
identifying potential bottlenecks in practical applications. These findings
contribute to a better understanding and suggest directions for future
improvements.

</details>


### [86] [Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2506.17551)
*Haowei Yang,Yu Tian,Zhongheng Yang,Zhao Wang,Chengrui Zhou,Dannier Li*

Main category: cs.DC

TL;DR: 这篇论文探讨了在大语言模型（LLMs）推荐系统中，通过模型并行和数据并行优化分布式训练，提出混合并行方案，显著提升了训练效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs参数量巨大和数据量庞大，其在推荐系统中使用时的计算和通信瓶颈日益突出，需要优化分布式训练方法。

Method: 论文研究了模型并行（包括张量并行和管道并行）和数据并行（同步和异步模式），并结合梯度压缩和稀疏化技术，提出自适应负载均衡机制和高效聚合通信框架。

Result: 实验表明，混合并行方案比传统单模式并行训练吞吐量提升30%以上，资源利用率提升约20%，同时保持了良好的扩展性和鲁棒性。

Conclusion: 论文分析了不同并行策略的权衡，并展望了未来在异构硬件集成和自动化调度技术方面的研究方向。

Abstract: With the rapid adoption of large language models (LLMs) in recommendation
systems, the computational and communication bottlenecks caused by their
massive parameter sizes and large data volumes have become increasingly
prominent. This paper systematically investigates two classes of optimization
methods-model parallelism and data parallelism-for distributed training of LLMs
in recommendation scenarios. For model parallelism, we implement both tensor
parallelism and pipeline parallelism, and introduce an adaptive load-balancing
mechanism to reduce cross-device communication overhead. For data parallelism,
we compare synchronous and asynchronous modes, combining gradient compression
and sparsification techniques with an efficient aggregation communication
framework to significantly improve bandwidth utilization. Experiments conducted
on a real-world recommendation dataset in a simulated service environment
demonstrate that our proposed hybrid parallelism scheme increases training
throughput by over 30% and improves resource utilization by approximately 20%
compared to traditional single-mode parallelism, while maintaining strong
scalability and robustness. Finally, we discuss trade-offs among different
parallel strategies in online deployment and outline future directions
involving heterogeneous hardware integration and automated scheduling
technologies.

</details>


### [87] [Distributed Butterfly Analysis using Mobile Agents](https://arxiv.org/abs/2506.17721)
*Prabhat Kumar Chand,Apurba Das,Anisur Rahaman Molla*

Main category: cs.DC

TL;DR: 本文提出了一种分布式、基于代理的算法，用于在二分图中高效计算蝴蝶结构（4-循环），无需预先了解图的结构。


<details>
  <summary>Details</summary>
Motivation: 二分图中的蝴蝶结构对识别密集子图很重要，但目前基于代理的数据挖掘方法在二分网络中的应用较少。

Method: 代理通过协作构建生成树并选举领导者，利用新颖的相邻代理会面机制，高效地完成蝴蝶计数。

Result: 算法在保持低轮数和内存复杂度的同时，实现了蝴蝶计数的分布式计算。

Conclusion: 该算法不仅适用于二分图，还可扩展到一般图，具有较高的效率和扩展性。

Abstract: Butterflies, or 4-cycles in bipartite graphs, are crucial for identifying
cohesive structures and dense subgraphs. While agent-based data mining is
gaining prominence, its application to bipartite networks remains relatively
unexplored. We propose distributed, agent-based algorithms for \emph{Butterfly
Counting} in a bipartite graph $G((A,B),E)$. Agents first determine their
respective partitions and collaboratively construct a spanning tree, electing a
leader within $O(n \log \lambda)$ rounds using only $O(\log \lambda)$ bits per
agent. A novel meeting mechanism between adjacent agents improves efficiency
and eliminates the need for prior knowledge of the graph, requiring only the
highest agent ID $\lambda$ among the $n$ agents. Notably, our techniques
naturally extend to general graphs, where leader election and spanning tree
construction maintain the same round and memory complexities. Building on these
foundations, agents count butterflies per node in $O(\Delta)$ rounds and
compute the total butterfly count of $G$ in $O(\Delta+\min\{|A|,|B|\})$ rounds.

</details>


### [88] [Choosing the Right Battery Model for Data Center Simulations](https://arxiv.org/abs/2506.17739)
*Paul Kilian,Philipp Wiesner,Odej Kao*

Main category: cs.DC

TL;DR: 研究比较了四种电池模型在数据中心场景中的表现，发现线性模型在短期内既能保持高仿真速度，又能接近复杂物理模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着计算资源需求的增长和碳排放法规的严格化，数据中心需要更高效、可控的电力系统设计，而选择合适的电池模型是其中的关键挑战之一。

Method: 在Vessim协同仿真框架中实现了四种不同的电池模型，并通过实验比较了它们的性能表现。

Result: 线性模型在短期内既能保持高仿真速度，又能接近复杂物理模型的准确性，而简单的无损模型则无法准确反映复杂行为且无运行时优势。

Conclusion: 在数据中心的仿真研究中，线性模型是平衡速度和准确性的理想选择。

Abstract: As demand for computing resources continues to rise, the increasing cost of
electricity and anticipated regulations on carbon emissions are prompting
changes in data center power systems. Many providers are now operating compute
nodes in microgrids, close to renewable power generators and energy storage, to
maintain full control over the cost and origin of consumed electricity.
Recently, new co-simulation testbeds have emerged that integrate
domain-specific simulators to support research, development, and testing of
such systems in a controlled environment. Yet, choosing an appropriate battery
model for data center simulations remains challenging, as it requires balancing
simulation speed, realism, and ease of configuration.
  In this paper, we implement four different battery models for data center
scenarios within the co-simulation framework Vessim and analyze their behavior.
The results show that linear models, which consider inefficiencies and power
limits, closely match the behavior of complex physics-based models in
short-term experiments while offering faster execution, and not requiring
knowledge on electrochemical reactions and circuit-level dynamics. In contrast,
simple, lossless models fail to accurately represent complex behavior and
provide no further runtime advantage.

</details>


### [89] [Maintaining a Bounded Degree Expander in Dynamic Peer-to-Peer Networks](https://arxiv.org/abs/2506.17757)
*Antonio Cruciani*

Main category: cs.DC

TL;DR: 研究了在节点频繁加入和退出的完全分布式环境中维护健壮且稀疏的覆盖网络的问题，提出了一种随机连接策略的动态扩展网络协议。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界非结构化P2P网络中维护低度通信图的问题。

Method: 扩展了Becchetti等人的随机连接策略，结合动态网络和节点流失的对抗性模型。

Result: 协议在对抗性流失下仍能高概率维持常数度扩展图。

Conclusion: 提供了一种简单、全分布式且抗流失的协议，解决了先前的开放性问题。

Abstract: We study the problem of maintaining robust and sparse overlay networks in
fully distributed settings where nodes continuously join and leave the system.
This scenario closely models real-world unstructured peer-to-peer networks,
where maintaining a well-connected yet low-degree communication graph is
crucial. We generalize a recent protocol by Becchetti et al. [SODA 2020] that
relies on a simple randomized connection strategy to build an expander topology
with high probability to a dynamic networks with churn setting. In this work,
the network dynamism is governed by an oblivious adversary that controls which
nodes join and leave the system in each round. The adversary has full knowledge
of the system and unbounded computational power, but cannot see the random
choices made by the protocol. Our analysis builds on the framework of Augustine
et al. [FOCS 2015], and shows that our distributed algorithm maintains a
constant-degree expander graph with high probability, despite a continuous
adversarial churn with a rate of up to $\mathcal{O}(n/polylog(n))$ per round,
where $n$ is the stable network size. The protocol and proof techniques are not
new, but together they resolve a specific open problem raised in prior work.
The result is a simple, fully distributed, and churn-resilient protocol with
provable guarantees that align with observed empirical behavior.

</details>


### [90] [Implementation and Evaluation of Fast Raft for Hierarchical Consensus](https://arxiv.org/abs/2506.17793)
*Anton Melnychuk,Bryan SebaRaj*

Main category: cs.DC

TL;DR: Fast Raft 是一种分层共识协议，减少提交日志条目所需的消息轮次，通过快速通道机制和减少对 leader 的依赖。


<details>
  <summary>Details</summary>
Motivation: 旨在解决动态分布式环境中传统 Raft 协议的性能问题，提升共识效率。

Method: 引入快速通道机制，减少 leader 依赖，使用 gRPC 和 Kubernetes 在 AWS 多可用区部署。

Result: 在低丢包条件下，吞吐量提升且提交延迟降低，同时保持 Raft 的安全性和活性。

Conclusion: Fast Raft 在动态环境中显著提升共识性能，适合实际部署。

Abstract: We present the first open-source implementation and evaluation of Fast Raft,
a hierarchical consensus protocol designed for dynamic, distributed
environments. Fast Raft reduces the number of message rounds needed to commit
log entries compared to standard Raft by introducing a fast-track mechanism and
reducing leader dependence. Our implementation uses gRPC and Kubernetes-based
deployment across AWS availability zones. Experimental results demonstrate a
throughput improvement and reduced commit latency under low packet loss
conditions, while maintaining Raft's safety and liveness guarantees.

</details>


### [91] [CFTel: A Practical Architecture for Robust and Scalable Telerobotics with Cloud-Fog Automation](https://arxiv.org/abs/2506.17991)
*Thien Tran,Jonathan Kua,Minh Tran,Honghao Lyu,Thuong Hoang,Jiong Jin*

Main category: cs.DC

TL;DR: 本文提出了Cloud-Fog Telerobotics（CFTel）架构，旨在解决传统云基远程机器人技术的延迟和可靠性问题，通过分布式计算架构实现实时控制与自主性。


<details>
  <summary>Details</summary>
Motivation: 传统云基远程机器人技术在延迟、可靠性和扩展性方面存在问题，限制了实时性能，特别是在关键应用中。CFTel架构旨在通过分布式计算解决这些问题。

Method: CFTel基于Cloud-Fog Automation（CFA）范式，利用分布式Cloud-Edge-Robotics计算架构，结合5G、边缘智能、数字孪生等技术，实现确定性连接和计算。

Result: 研究表明，CFTel能够提升实时控制、扩展性和自主性，并支持面向服务的解决方案。

Conclusion: CFTel为未来远程机器人研究提供了基础参考，同时也需解决延迟、网络安全和标准化等实践挑战。

Abstract: Telerobotics is a key foundation in autonomous Industrial Cyber-Physical
Systems (ICPS), enabling remote operations across various domains. However,
conventional cloud-based telerobotics suffers from latency, reliability,
scalability, and resilience issues, hindering real-time performance in critical
applications. Cloud-Fog Telerobotics (CFTel) builds on the Cloud-Fog Automation
(CFA) paradigm to address these limitations by leveraging a distributed
Cloud-Edge-Robotics computing architecture, enabling deterministic
connectivity, deterministic connected intelligence, and deterministic networked
computing. This paper synthesizes recent advancements in CFTel, aiming to
highlight its role in facilitating scalable, low-latency, autonomous, and
AI-driven telerobotics. We analyze architectural frameworks and technologies
that enable them, including 5G Ultra-Reliable Low-Latency Communication, Edge
Intelligence, Embodied AI, and Digital Twins. The study demonstrates that CFTel
has the potential to enhance real-time control, scalability, and autonomy while
supporting service-oriented solutions. We also discuss practical challenges,
including latency constraints, cybersecurity risks, interoperability issues,
and standardization efforts. This work serves as a foundational reference for
researchers, stakeholders, and industry practitioners in future telerobotics
research.

</details>


### [92] [Leveraging Cloud-Fog Automation for Autonomous Collision Detection and Classification in Intelligent Unmanned Surface Vehicles](https://arxiv.org/abs/2506.18024)
*Thien Tran,Quang Nguyen,Jonathan Kua,Minh Tran,Toan Luu,Thuong Hoang,Jiong Jin*

Main category: cs.DC

TL;DR: 该论文提出了一种针对海上工业信息物理系统（ICPS）的分布式云-边缘-物联网架构，通过分层处理解决了计算和通信延迟问题，显著提升了实时数据处理能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 海上无人船（USVs）的工业信息物理系统（ICPS）因计算资源受限和通信延迟问题，限制了实时数据处理的效率和可扩展性。

Method: 基于Cloud-Fog Automation的设计原则，提出了一种三层分布式架构（云层、边缘层和物联网层），分别负责数据聚合、本地化AI处理以及低延迟数据采集。

Result: 实验结果表明，该架构在计算效率、响应速度和可扩展性方面均有显著提升，分类准确率达到86%，并改善了延迟性能。

Conclusion: 该研究为海上ICPS提供了一种实用、模块化且可扩展的框架，推动了智能USV的自主性和AI决策能力的发展。

Abstract: Industrial Cyber-Physical Systems (ICPS) technologies are foundational in
driving maritime autonomy, particularly for Unmanned Surface Vehicles (USVs).
However, onboard computational constraints and communication latency
significantly restrict real-time data processing, analysis, and predictive
modeling, hence limiting the scalability and responsiveness of maritime ICPS.
To overcome these challenges, we propose a distributed Cloud-Edge-IoT
architecture tailored for maritime ICPS by leveraging design principles from
the recently proposed Cloud-Fog Automation paradigm. Our proposed architecture
comprises three hierarchical layers: a Cloud Layer for centralized and
decentralized data aggregation, advanced analytics, and future model
refinement; an Edge Layer that executes localized AI-driven processing and
decision-making; and an IoT Layer responsible for low-latency sensor data
acquisition. Our experimental results demonstrated improvements in
computational efficiency, responsiveness, and scalability. When compared with
our conventional approaches, we achieved a classification accuracy of 86\%,
with an improved latency performance. By adopting Cloud-Fog Automation, we
address the low-latency processing constraints and scalability challenges in
maritime ICPS applications. Our work offers a practical, modular, and scalable
framework to advance robust autonomy and AI-driven decision-making and autonomy
for intelligent USVs in future maritime ICPS.

</details>


### [93] [Edge Association Strategies for Synthetic Data Empowered Hierarchical Federated Learning with Non-IID Data](https://arxiv.org/abs/2506.18259)
*Jer Shyuan Ng,Aditya Pribadi Kalapaaking,Xiaoyu Xia,Dusit Niyato,Ibrahim Khalil,Iqbal Gondal*

Main category: cs.DC

TL;DR: 论文提出了一种结合合成数据的层次联邦学习（HFL）框架，解决非独立同分布（non-IID）数据和参与者激励问题。


<details>
  <summary>Details</summary>
Motivation: 现有层次联邦学习（HFL）框架仍存在通信轮次多和参与者合作不充分的问题，尤其是在非独立同分布数据场景下。

Method: 通过边缘服务器生成和分发合成数据，激励工人参与并改善模型性能。

Result: 提出的框架减少了通信轮次，提高了模型性能，并解决了参与者的激励问题。

Conclusion: 合成数据赋能的HFL框架有效应对了非独立同分布数据和参与者激励的双重挑战。

Abstract: In recent years, Federated Learning (FL) has emerged as a widely adopted
privacy-preserving distributed training approach, attracting significant
interest from both academia and industry. Research efforts have been dedicated
to improving different aspects of FL, such as algorithm improvement, resource
allocation, and client selection, to enable its deployment in distributed edge
networks for practical applications. One of the reasons for the poor FL model
performance is due to the worker dropout during training as the FL server may
be located far away from the FL workers. To address this issue, an Hierarchical
Federated Learning (HFL) framework has been introduced, incorporating an
additional layer of edge servers to relay communication between the FL server
and workers. While the HFL framework improves the communication between the FL
server and workers, large number of communication rounds may still be required
for model convergence, particularly when FL workers have non-independent and
identically distributed (non-IID) data. Moreover, the FL workers are assumed to
fully cooperate in the FL training process, which may not always be true in
practical situations. To overcome these challenges, we propose a
synthetic-data-empowered HFL framework that mitigates the statistical issues
arising from non-IID local datasets while also incentivizing FL worker
participation. In our proposed framework, the edge servers reward the FL
workers in their clusters for facilitating the FL training process. To improve
the performance of the FL model given the non-IID local datasets of the FL
workers, the edge servers generate and distribute synthetic datasets to FL
workers within their clusters. FL workers determine which edge server to
associate with, considering the computational resources required to train on
both their local datasets and the synthetic datasets.

</details>


### [94] [The Power of Strong Linearizability: the Difficulty of Consistent Refereeing](https://arxiv.org/abs/2506.18401)
*Hagit Attiya,Armando Castañeda,Constantin Enea*

Main category: cs.DC

TL;DR: 本文研究了强线性化实现与一致性协议的关系，发现了非通用原语无法实现强线性化的新结果。


<details>
  <summary>Details</summary>
Motivation: 探讨强线性化和决定性线性化在并发对象实现中的作用及其与一致性协议的关系。

Method: 定义了两种竞赛对象来捕捉强线性化的能力，并分析了其实现限制。

Result: 发现强线性化需要高协调能力，且竞赛对象比共识更弱，但仍具有强大限制性。

Conclusion: 强线性化在非通用原语中难以实现，竞赛对象为研究提供了新工具。

Abstract: This paper studies the relation between agreement and strongly linearizable
implementations of various objects. This leads to new results about
implementations of concurrent objects from various primitives including window
registers and interfering primitives. We consider implementations that provide
both strong linearizability and decisive linearizability.
  We identify that lock-free, respectively, wait-free, strongly linearizable
implementations of several concurrent objects entail a form of agreement that
is weaker than consensus but impossible to strongly-linearizable implement with
combinations of non-universal primitives. In both cases, lock-free and
wait-free, this form of agreement requires a distinguished process to referee a
competition that involves all other processes. Our results show that consistent
refereeing of such competitions (i.e. the outcome of the competition does not
change in extensions of the current execution) requires high coordination
power.
  More specifically, two contest objects are defined and used to capture the
power of strong linearizability in lock-free and wait-free implementations,
respectively. Both objects are strictly weaker than consensus, in the sense
that they have a wait-free linearizable (in fact, decisively linearizable)
implementation from reads and writes. The contest objects capture strong
linearizability since (1) they have strongly linearizable implementations from
several ``high-level'' objects like stacks, queues, snapshots, counters, and
therefore, impossibility results for them carry over to these objects, and (2)
they admit powerful impossibility results for strong linearizability that
involve window registers and interfering primitives, which are non-universal.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [95] [DCMF: A Dynamic Context Monitoring and Caching Framework for Context Management Platforms](https://arxiv.org/abs/2506.17226)
*Ashish Manchanda,Prem Prakash Jayaraman,Abhik Banerjee,Kaneez Fizza,Arkady Zaslavsky*

Main category: cs.DB

TL;DR: 该论文提出了动态上下文监控框架（DCMF），用于提升物联网中上下文缓存的效果，通过动态评估和管理上下文，显著提高了缓存命中率并减少了缓存过期。


<details>
  <summary>Details</summary>
Motivation: 随着上下文感知物联网应用的兴起，对及时准确的上下文信息需求增加，但上下文的高动态性和易变性带来了维护实时性的挑战。传统缓存策略难以应对这些挑战。

Method: DCMF框架包括上下文评估引擎（CEE）和上下文管理模块（CMM）。CEE通过计算访问概率（PoA），结合多个参数评估上下文；CMM采用Dempster-Shafer方法管理上下文新鲜度。

Result: 实验表明，DCMF在智能城市数据（如交通和道路施工场景）中，缓存命中率提高12.5%，缓存过期减少60%，显著降低了延迟。

Conclusion: DCMF在动态上下文感知物联网环境中具有高可扩展性和适应性，能有效提升上下文管理的实时性和效率。

Abstract: The rise of context-aware IoT applications has increased the demand for
timely and accurate context information. Context is derived by aggregating and
inferring from dynamic IoT data, making it highly volatile and posing
challenges in maintaining freshness and real-time accessibility. Caching is a
potential solution, but traditional policies struggle with the transient nature
of context in IoT (e.g., ensuring real-time access for frequent queries or
handling fast-changing data). To address this, we propose the Dynamic Context
Monitoring Framework (DCMF) to enhance context caching in Context Management
Platforms (CMPs) by dynamically evaluating and managing context. DCMF comprises
two core components: the Context Evaluation Engine (CEE) and the Context
Management Module (CMM). The CEE calculates the Probability of Access (PoA)
using parameters such as Quality of Service (QoS), Quality of Context (QoC),
Cost of Context (CoC), timeliness, and Service Level Agreements (SLAs),
assigning weights to assess access likelihood. Based on this, the CMM applies a
hybrid Dempster-Shafer approach to manage Context Freshness (CF), updating
belief levels and confidence scores to determine whether to cache, evict, or
refresh context items. We implemented DCMF in a Context-as-a-Service (CoaaS)
platform and evaluated it using real-world smart city data, particularly
traffic and roadwork scenarios. Results show DCMF achieves a 12.5% higher cache
hit rate and reduces cache expiry by up to 60% compared to the m-CAC technique,
ensuring timely delivery of relevant context and reduced latency. These results
demonstrate DCMF's scalability and suitability for dynamic context-aware IoT
environments.

</details>


### [96] [Transient Concepts in Streaming Graphs](https://arxiv.org/abs/2506.17451)
*Aida Sheshbolouki,M. Tamer Ozsu*

Main category: cs.DB

TL;DR: 论文探讨了流图设置中的概念漂移（CD）问题，提出了两种新框架SGDD和SGDP用于检测和预测CD，分别针对延迟较大的参数变化和提前预测CD。


<details>
  <summary>Details</summary>
Motivation: CD在流数据中普遍存在，当前方法在流图场景中效果不佳，迫切需要新的框架来有效检测和预测CD。

Method: 提出了SGDD和SGDP两种框架，分别用于检测生成参数变化的CD和在数据记录未到达时提前预测CD。

Result: SGDD检测延迟较大的CD，SGDP能在7374至0.19毫秒前预测CD，且无需访问数据记录负载。

Conclusion: 新框架在流图CD问题上表现出色，为数据管理系统提供了更高效的解决方案。

Abstract: Concept Drift (CD) occurs when a change in a hidden context can induce
changes in a target concept. CD is a natural phenomenon in non-stationary
settings such as data streams. Understanding, detection, and adaptation to CD
in streaming data is (i) vital for effective and efficient analytics as
reliable output depends on adaptation to fresh input, (ii) challenging as it
requires efficient operations as well as effective performance evaluations, and
(iii) impactful as it applies to a variety of use cases and is a crucial
initial step for data management systems. Current works are mostly focused on
passive CD detection as part of supervised adaptation, on independently
generated data instances or graph snapshots, on target concepts as a function
of data labels, on static data management, and on specific temporal order of
data record. These methods do not always work. We revisit CD for the streaming
graphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for
streaming graph CD detection and prediction. Both frameworks discern the change
of generative source. SGDD detects the CDs due to the changes of generative
parameters with significant delays such that it is difficult to evaluate the
performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds
ahead of their occurrence, without accessing the payloads of data records.

</details>


### [97] [Lower Bounds for Conjunctive Query Evaluation](https://arxiv.org/abs/2506.17702)
*Stefan Mengel*

Main category: cs.DB

TL;DR: 本次教程综述了不同设置下联查询评估的复杂性结果，着重展示了复杂性理论中的新假设如何与查询回答相关联，并证明某些算法可能无法进一步改进。


<details>
  <summary>Details</summary>
Motivation: 旨在总结联查询评估在不同模型下的复杂性，并探讨复杂性理论中的新假设如何影响查询回答的算法优化。

Method: 通过综述已知结果，分析从布尔查询到计数、枚举和直接访问等复杂模型的复杂性，结合复杂性理论的新假设进行探讨。

Result: 展示了新假设如何证明某些查询回答算法可能已达到最优，难以进一步改进。

Conclusion: 复杂性理论的新假设为联查询评估的算法上限提供了理论基础，对实际应用中的查询优化具有重要意义。

Abstract: In this tutorial, we will survey known results on the complexity of
conjunctive query evaluation in different settings, ranging from Boolean
queries over counting to more complex models like enumeration and direct
access. A particular focus will be on showing how different relatively recent
hypotheses from complexity theory connect to query answering and allow showing
that known algorithms in several cases can likely not be improved.

</details>


### [98] [Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road Networks](https://arxiv.org/abs/2506.18013)
*Muhammad Farhan,Henning Koehler,Qing Wang*

Main category: cs.DB

TL;DR: 提出一种名为Dual-Hierarchy Labelling (DHL)的高效方法，用于动态道路网络中快速计算最短路径距离，通过双重层次结构优化查询和维护性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态道路网络中表现不佳，查询响应慢或维护效率低，尤其是大型网络。

Method: 结合查询层次和更新层次的双重结构，支持高效查询和维护；提出并行算法优化动态更新。

Result: 在10个大型道路网络中测试，DHL显著优于现有方法，查询速度提高2-4倍，构建和维护时间更快，标签空间仅需10-20%。

Conclusion: DHL为动态道路网络提供了一种高效、可扩展的解决方案，优于现有方法。

Abstract: Computing the shortest-path distance between any two given vertices in road
networks is an important problem. A tremendous amount of research has been
conducted to address this problem, most of which are limited to static road
networks. Since road networks undergo various real-time traffic conditions,
there is a pressing need to address this problem for dynamic road networks.
Existing state-of-the-art methods incrementally maintain an indexing structure
to reflect dynamic changes on road networks. However, these methods suffer from
either slow query response time or poor maintenance performance, particularly
when road networks are large. In this work, we propose an efficient solution
\emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road
networks from a novel perspective, which incorporates two hierarchies with
different but complementary data structures to support efficient query and
update processing. Specifically, our proposed solution is comprised of three
main components: \emph{query hierarchy}, \emph{update hierarchy}, and
\emph{hierarchical labelling}, where \emph{query hierarchy} enables efficient
query answering by exploring only a small subset of vertices in the labels of
two query vertices and \emph{update hierarchy} supports efficient maintenance
of distance labelling under edge weight increase or decrease. We further
develop dynamic algorithms to reflect dynamic changes by efficiently
maintaining the update hierarchy and hierarchical labelling. We also propose a
parallel variant of our dynamic algorithms by exploiting labelling structure.
We evaluate our methods on 10 large road networks and it shows that our methods
significantly outperform the state-of-the-art methods, i.e., achieving
considerably faster construction and update time, while being consistently 2-4
times faster in terms of query processing and consuming only 10\%-20\%
labelling space.

</details>


### [99] [Floating-Point Data Transformation for Lossless Compression](https://arxiv.org/abs/2506.18062)
*Samirasadat Jamalidinan,Kazem Cheshmi*

Main category: cs.DB

TL;DR: 提出了一种新的数据转换方法DTT，通过将相关字节分组来提高浮点数据的压缩效率，性能优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 浮点数据存储需求大且精度关键，现有方法未充分利用其内部模式，亟需高效压缩方法。

Method: DTT方法将相关字节分组，提升压缩效率，并在CPU和GPU上实现测试。

Result: DTT比zstd等工具压缩比提升1.16倍，压缩和解压速度提升1.18-3.79倍。

Conclusion: DTT通过利用浮点数据内部相关性，显著提升了压缩效率和速度。

Abstract: Floating-point data is widely used across various domains. Depending on the
required precision, each floating-point value can occupy several bytes.
Lossless storage of this information is crucial due to its critical accuracy,
as seen in applications such as medical imaging and language model weights. In
these cases, data size is often significant, making lossless compression
essential. Previous approaches either treat this data as raw byte streams for
compression or fail to leverage all patterns within the dataset. However,
because multiple bytes represent a single value and due to inherent patterns in
floating-point representations, some of these bytes are correlated. To leverage
this property, we propose a novel data transformation method called Typed Data
Transformation (\DTT{}) that groups related bytes together to improve
compression. We implemented and tested our approach on various datasets across
both CPU and GPU. \DTT{} achieves a geometric mean compression ratio
improvement of 1.16$\times$ over state-of-the-art compression tools such as
zstd, while also improving both compression and decompression throughput by
1.18--3.79$\times$.

</details>


### [100] [Learning Lineage Constraints for Data Science Operations](https://arxiv.org/abs/2506.18252)
*Jinjin Zhao*

Main category: cs.DB

TL;DR: 提出了一种跨库数据血缘的统一表示方法XProv，结合具体转换图和抽象逻辑模式，以解决数据工作流中跨库调试问题。


<details>
  <summary>Details</summary>
Motivation: 数据科学工作流常涉及多种库，而调试需要跨库血缘，当前血缘表示与特定数据模型紧密耦合，缺乏通用性。

Method: 借鉴跨库性能优化的中间表示思想，设计XProv架构，统一参数化表示逻辑血缘，并链接具体转换图和抽象逻辑模式。

Result: 提出XProv框架，支持已知和未知操作的链路，探讨如何从具体图推测逻辑模式。

Conclusion: XProv为跨库数据血缘提供通用解决方案，有望提升调试效率。

Abstract: Data science workflows often integrate functionalities from a diverse set of
libraries and frameworks. Tasks such as debugging require data lineage that
crosses library boundaries. The problem is that the way that "lineage" is
represented is often intimately tied to particular data models and data
manipulation paradigms. Inspired by the use of intermediate representations
(IRs) in cross-library performance optimizations, this vision paper proposes a
similar architecture for lineage - how do we specify logical lineage across
libraries in a common parameterized way? In practice, cross-library workflows
will contain both known operations and unknown operations, so a key design of
XProv to link both materialized lineage graphs of data transformations and the
aforementioned abstracted logical patterns. We further discuss early ideas on
how to infer logical patterns when only the materialized graphs are available.

</details>


### [101] [Fast Capture of Cell-Level Provenance in Numpy](https://arxiv.org/abs/2506.18255)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: 该论文提出了一种针对数组的注释系统原型，旨在解决溯源跟踪中的挑战，特别是在numpy库中捕获细胞级溯源。


<details>
  <summary>Details</summary>
Motivation: 提高数组工作流的可重复性、治理和数据质量。

Method: 设计了一个原型注释系统，探索了内存优化以减少注释延迟。

Result: 系统显著降低了注释延迟。

Conclusion: 该方法可作为结构化数据工作流和多样化数据科学应用的治理系统的一部分。

Abstract: Effective provenance tracking enhances reproducibility, governance, and data
quality in array workflows. However, significant challenges arise in capturing
this provenance, including: (1) rapidly evolving APIs, (2) diverse operation
types, and (3) large-scale datasets. To address these challenges, this paper
presents a prototype annotation system designed for arrays, which captures
cell-level provenance specifically within the numpy library. With this
prototype, we explore straightforward memory optimizations that substantially
reduce annotation latency. We envision this provenance capture approach for
arrays as part of a broader governance system for tracking for structured data
workflows and diverse data science applications.

</details>


### [102] [TableVault: Managing Dynamic Data Collections for LLM-Augmented Workflows](https://arxiv.org/abs/2506.18257)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: TableVault是一款为LLM增强环境设计的数据管理系统，支持并发执行、确保可重复性、数据版本控制和可组合工作流设计。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）在复杂数据工作流中的管理挑战。

Method: 结合传统数据库方法和LLM驱动需求，提供透明平台管理结构化数据和相关数据工件。

Result: TableVault能高效处理动态数据集合，满足复杂工作流需求。

Conclusion: TableVault为LLM增强环境提供了一种有效的数据管理解决方案。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating
and executing complex data tasks. However, their integration into more complex
data workflows introduces significant management challenges. In response, we
present TableVault - a data management system designed to handle dynamic data
collections in LLM-augmented environments. TableVault meets the demands of
these workflows by supporting concurrent execution, ensuring reproducibility,
maintaining robust data versioning, and enabling composable workflow design. By
merging established database methodologies with emerging LLM-driven
requirements, TableVault offers a transparent platform that efficiently manages
both structured data and associated data artifacts.

</details>


### [103] [Patient Journey Ontology: Representing Medical Encounters for Enhanced Patient-Centric Applications](https://arxiv.org/abs/2506.18772)
*Hassan S. Al Khatib,Subash Neupane,Sudip Mittal,Shahram Rahimi,Nina Marhamati,Sean Bozorgzad*

Main category: cs.DB

TL;DR: 提出一种患者旅程本体（PJO）框架，用于整合和管理患者多源数据，支持语义互操作性和临床推理，增强预测分析和个性化医疗。


<details>
  <summary>Details</summary>
Motivation: 医疗行业向患者中心化转型，需要先进方法管理患者数据，提升临床决策和医疗效率。

Method: 基于本体论构建PJO，整合病史、诊断、治疗等数据，捕捉时间、序列和因果关系，支持预测分析。

Result: 专家评估显示PJO在患者历史检索、症状跟踪和提供者交互表示方面性能优越，具有实际应用潜力。

Conclusion: PJO为知识表示和生成AI在医疗中的应用提供了可靠工具，提升患者结果和医疗效率。

Abstract: The healthcare industry is moving towards a patient-centric paradigm that
requires advanced methods for managing and representing patient data. This
paper presents a Patient Journey Ontology (PJO), a framework that aims to
capture the entirety of a patient's healthcare encounters. Utilizing
ontologies, the PJO integrates different patient data sources like medical
histories, diagnoses, treatment pathways, and outcomes; it enables semantic
interoperability and enhances clinical reasoning. By capturing temporal,
sequential, and causal relationships between medical encounters, the PJO
supports predictive analytics, enabling earlier interventions and optimized
treatment plans. The ontology's structure, including its main classes,
subclasses, properties, and relationships, as detailed in the paper,
demonstrates its ability to provide a holistic view of patient care.
Quantitative and qualitative evaluations by Subject Matter Experts (SMEs)
demonstrate strong capabilities in patient history retrieval, symptom tracking,
and provider interaction representation, while identifying opportunities for
enhanced diagnosis-symptom linking. These evaluations reveal the PJO's
reliability and practical applicability, demonstrating its potential to enhance
patient outcomes and healthcare efficiency. This work contributes to the
ongoing efforts of knowledge representation in healthcare, offering a reliable
tool for personalized medicine, patient journey analysis and advancing the
capabilities of Generative AI in healthcare applications.

</details>


### [104] [LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth](https://arxiv.org/abs/2506.18842)
*Patrick Beukema,Henry Herzog,Yawen Zhang,Hunter Pitelka,Favyen Bastani*

Main category: cs.DB

TL;DR: 提出了一种新的数据集和算法，用于全球范围内的高精度海岸距离计算，分辨率提升100倍以上，并开发了一个高效的计算库Lighthouse。


<details>
  <summary>Details</summary>
Motivation: 现有全球海岸数据集分辨率较低（1-4公里），限制了其应用潜力。

Method: 结合公开卫星图像和计算机视觉技术，提供10米分辨率的全球海岸数据集，并开发Lighthouse库以实现高效计算。

Result: Lighthouse在资源受限环境下表现优异，仅需1 CPU和2GB RAM即可实现毫秒级在线推断。

Conclusion: 新数据集和算法显著提升了海岸距离计算的精度和效率，适用于实时应用。

Abstract: We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [105] [AMD Versal Implementations of FAM and SSCA Estimators](https://arxiv.org/abs/2506.18003)
*Carol Jingyi Li,Ruilin Wu,Philip H. W. Leong*

Main category: cs.AR

TL;DR: 论文提出两种优化后的SCD估计方法，通过FPGA实现，显著提升了计算速度和能源效率。


<details>
  <summary>Details</summary>
Motivation: 传统SCD估计方法计算复杂度高，限制了实时应用，需要更高效的实现方案。

Method: 采用FPGA实现FFT累积法（FAM）和条带谱相关分析法（SSCA），并提出了并行化计算的通用方法。

Result: 相较于NVIDIA RTX 3090 GPU，FAM和SSCA分别实现了4.43倍/1.90倍的速度提升和30.5倍/24.5倍的能效改进。

Conclusion: 研究验证了FPGA在高效SCD估计中的潜力，为实时信号处理提供了可行方案。

Abstract: Cyclostationary analysis is widely used in signal processing, particularly in
the analysis of human-made signals, and spectral correlation density (SCD) is
often used to characterise cyclostationarity. Unfortunately, for real-time
applications, even utilising the fast Fourier transform (FFT), the high
computational complexity associated with estimating the SCD limits its
applicability. In this work, we present optimised, high-speed
field-programmable gate array (FPGA) implementations of two SCD estimation
techniques. Specifically, we present an implementation of the FFT accumulation
method (FAM) running entirely on the AMD Versal AI engine (AIE) array. We also
introduce an efficient implementation of the strip spectral correlation
analyser (SSCA) that can be used for window sizes up to $2^{20}$. For both
techniques, a generalised methodology is presented to parallelise the
computation while respecting memory size and data bandwidth constraints.
Compared to an NVIDIA GeForce RTX 3090 graphics processing unit (GPU) which
uses a similar 7nm technology to our FPGA, for the same accuracy, our FAM/SSCA
implementations achieve speedups of 4.43x/1.90x and a 30.5x/24.5x improvement
in energy efficiency.

</details>


### [106] [Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference](https://arxiv.org/abs/2506.18530)
*Muhammad Ihsan Al Hafiz,Naresh Ravichandran,Anders Lansner,Pawel Herman,Artur Podobas*

Main category: cs.AR

TL;DR: 该论文提出了一种基于FPGA的嵌入式加速器，用于实现低功耗的边缘人工智能，特别适用于脑启发神经网络（BLNN）。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型能耗高且依赖云端，而脑启发神经网络（BLNN）如BCPNN具有稀疏架构和局部学习规则，适合边缘设备。但现有实现依赖GPU或数据中心FPGA，限制了在嵌入式系统中的应用。

Method: 本文提出了一种基于Zynq UltraScale+ SoC的嵌入式FPGA加速器，支持在线学习和推断内核，并采用高层次的综合设计方法。

Result: 在MNIST、肺炎和乳腺癌数据集上的实验表明，与ARM基准相比，该加速器实现了17.5倍的延迟降低和94%的能耗节省，且不损失精度。

Conclusion: 这一研究为边缘设备上的实用神经形态计算铺平了道路，缩小了脑启发学习与实际部署之间的差距。

Abstract: Edge AI applications increasingly require models that can learn and adapt
on-device with minimal energy budget. Traditional deep learning models, while
powerful, are often overparameterized, energy-hungry, and dependent on cloud
connectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian
Confidence Propagation Neural Network (BCPNN), propose a neuromorphic
alternative by mimicking cortical architecture and biologically-constrained
learning. They offer sparse architectures with local learning rules and
unsupervised/semi-supervised learning, making them well-suited for low-power
edge intelligence. However, existing BCPNN implementations rely on GPUs or
datacenter FPGAs, limiting their applicability to embedded systems. This work
presents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+
SoC using High-Level Synthesis. We implement both online learning and
inference-only kernels with support for variable and mixed precision. Evaluated
on MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to
17.5x latency and 94% energy savings over ARM baselines, without sacrificing
accuracy. This work enables practical neuromorphic computing on edge devices,
bridging the gap between brain-like learning and real-world deployment.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [107] [Optimal Parallel Algorithms for Convex Hulls in 2D and 3D under Noisy Primitive Operations](https://arxiv.org/abs/2506.17507)
*Michael T. Goodrich,Vinesh Sridhar*

Main category: cs.CG

TL;DR: 研究了在噪声原始模型下的并行计算几何算法，特别是2D和3D凸包问题，提出了首个在CREW PRAM模型中的最优并行算法，通过广义的失败扫描技术来检测和修复中间步骤的错误。


<details>
  <summary>Details</summary>
Motivation: 噪声原始模型中，几何算法的并行化研究较少，现有方法多为顺序算法，因此探索并行算法以提升效率。

Method: 采用广义的失败扫描技术来检测和修复算法中的中间错误，实现在CREW PRAM模型下的并行计算。

Result: 首次提出了在噪声原始模型中针对2D和3D凸包问题的最优并行算法。

Conclusion: 通过技术改进，成功实现了并行算法在噪声原始模型中的应用，为相关领域提供了新的解决方案。

Abstract: In the noisy primitives model, each primitive comparison performed by an
algorithm, e.g., testing whether one value is greater than another, returns the
incorrect answer with random, independent probability p < 1/2 and otherwise
returns a correct answer. This model was first applied in the context of
sorting and searching, and recent work by Eppstein, Goodrich, and Sridhar
extends this model to sequential algorithms involving geometric primitives such
as orientation and sidedness tests. However, their approaches appear to be
inherently sequential; hence, in this paper, we study parallel computational
geometry algorithms for 2D and 3D convex hulls in the noisy primitives model.
We give the first optimal parallel algorithms in the noisy primitives model for
2D and 3D convex hulls in the CREW PRAM model. The main technical contribution
of our work concerns our ability to detect and fix errors during intermediate
steps of our algorithm using a generalization of the failure sweeping
technique.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [108] [Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models](https://arxiv.org/abs/2506.17580)
*Sajratul Y. Rubaiat,Hasan M. Jamil*

Main category: cs.IR

TL;DR: WISE系统通过结构化工作流程解决科学文献知识提取难题，显著减少文本处理量并提高召回率。


<details>
  <summary>Details</summary>
Motivation: 科学文献爆炸式增长，传统搜索引擎和大语言模型在提供详细、深度和最新信息方面存在不足。

Method: 采用基于LLM的树形架构，动态评分和排名，自适应停止标准，提取和组织查询相关知识。

Result: 在HBB基因相关疾病实验中，WISE减少80%文本处理量，召回率显著优于基线方法。

Conclusion: WISE可扩展到药物发现、材料科学等领域，高效提取和合成非结构化科学文献知识。

Abstract: The exponential growth of scientific literature challenges researchers
extracting and synthesizing knowledge. Traditional search engines return many
sources without direct, detailed answers, while general-purpose LLMs may offer
concise responses that lack depth or omit current information. LLMs with search
capabilities are also limited by context window, yielding short, incomplete
answers. This paper introduces WISE (Workflow for Intelligent Scientific
Knowledge Extraction), a system addressing these limits by using a structured
workflow to extract, refine, and rank query-specific knowledge. WISE uses an
LLM-powered, tree-based architecture to refine data, focusing on query-aligned,
context-aware, and non-redundant information. Dynamic scoring and ranking
prioritize unique contributions from each source, and adaptive stopping
criteria minimize processing overhead. WISE delivers detailed, organized
answers by systematically exploring and synthesizing knowledge from diverse
sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces
processed text by over 80% while achieving significantly higher recall over
baselines like search engines and other LLM-based approaches. ROUGE and BLEU
metrics reveal WISE's output is more unique than other systems, and a novel
level-based metric shows it provides more in-depth information. We also explore
how the WISE workflow can be adapted for diverse domains like drug discovery,
material science, and social science, enabling efficient knowledge extraction
and synthesis from unstructured scientific papers and web sources.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [109] [Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM](https://arxiv.org/abs/2506.17351)
*Mostafa Shahin,Beena Ahmed,Julien Epps*

Main category: cs.SD

TL;DR: 本文提出了一种基于零样本学习的语音认知障碍检测方法，利用Qwen2-Audio AudioLLM模型处理语音和文本输入，通过设计提示指令实现分类，无需手动标注且在多语言和多任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 认知障碍（CI）早期检测对公共卫生至关重要，传统方法依赖有监督模型且泛化性差，缺乏多语言和多任务适应性。

Method: 使用Qwen2-Audio AudioLLM模型，设计提示指令实现零样本分类，无需手动提取特征或训练。

Result: 在两个数据集（英语和多语言）上评估，零样本方法性能接近有监督模型，展现出色的泛化性和一致性。

Conclusion: 零样本AudioLLM方法为认知障碍检测提供了高效、通用的解决方案，尤其在多语言和多任务场景中表现优异。

Abstract: Cognitive impairment (CI) is of growing public health concern, and early
detection is vital for effective intervention. Speech has gained attention as a
non-invasive and easily collectible biomarker for assessing cognitive decline.
Traditional CI detection methods typically rely on supervised models trained on
acoustic and linguistic features extracted from speech, which often require
manual annotation and may not generalise well across datasets and languages. In
this work, we propose the first zero-shot speech-based CI detection method
using the Qwen2- Audio AudioLLM, a model capable of processing both audio and
text inputs. By designing prompt-based instructions, we guide the model in
classifying speech samples as indicative of normal cognition or cognitive
impairment. We evaluate our approach on two datasets: one in English and
another multilingual, spanning different cognitive assessment tasks. Our
results show that the zero-shot AudioLLM approach achieves performance
comparable to supervised methods and exhibits promising generalizability and
consistency across languages, tasks, and datasets.

</details>


### [110] [TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography](https://arxiv.org/abs/2506.18671)
*Yuqin Dai,Wanlu Zhu,Ronghui Li,Xiu Li,Zhenyu Zhang,Jun Li,Jian Yang*

Main category: cs.SD

TL;DR: TCDiff++是一个端到端的音乐驱动框架，旨在解决群体舞蹈生成中的碰撞、脚滑动和位置突变问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在群体舞蹈生成中存在多舞者碰撞、单舞者脚滑动以及长序列生成时的位置突然交换问题。

Method: 提出TCDiff++框架，包括舞者定位嵌入、距离一致性损失、交换模式嵌入、脚步适配器和长序列扩散采样策略。

Result: 实验表明，TCDiff++在长时间场景下表现优异，能够生成高质量且连贯的群体舞蹈。

Conclusion: TCDiff++通过多模块协同，有效解决了群体舞蹈生成中的关键问题，实现了行业领先效果。

Abstract: Music-driven dance generation has garnered significant attention due to its
wide range of industrial applications, particularly in the creation of group
choreography. During the group dance generation process, however, most existing
methods still face three primary issues: multi-dancer collisions, single-dancer
foot sliding and abrupt swapping in the generation of long group dance. In this
paper, we propose TCDiff++, a music-driven end-to-end framework designed to
generate harmonious group dance. Specifically, to mitigate multi-dancer
collisions, we utilize a dancer positioning embedding to better maintain the
relative positioning among dancers. Additionally, we incorporate a
distance-consistency loss to ensure that inter-dancer distances remain within
plausible ranges. To address the issue of single-dancer foot sliding, we
introduce a swap mode embedding to indicate dancer swapping patterns and design
a Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For
long group dance generation, we present a long group diffusion sampling
strategy that reduces abrupt position shifts by injecting positional
information into the noisy input. Furthermore, we integrate a Sequence Decoder
layer to enhance the model's ability to selectively process long sequences.
Extensive experiments demonstrate that our TCDiff++ achieves state-of-the-art
performance, particularly in long-duration scenarios, ensuring high-quality and
coherent group dance generation.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [111] [Mapping the Evolution of Research Contributions using KnoVo](https://arxiv.org/abs/2506.17508)
*Sajratul Y. Rubaiat,Syed N. Sakib,Hasan M. Jamil*

Main category: cs.DL

TL;DR: KnoVo是一个智能框架，用于量化分析科研文献中的研究新颖性，通过多层引文网络和大型语言模型提取比较维度，并生成定量新颖性分数。


<details>
  <summary>Details</summary>
Motivation: 传统引文分析主要衡量研究影响，KnoVo旨在通过新颖性分析帮助评估原创性、发现研究空白和跨学科联系。

Method: 利用大型语言模型动态提取比较维度（如方法、应用、数据集），并通过锦标赛式分析生成新颖性分数。

Result: 通过20篇多领域论文的详细分析，展示了KnoVo的能力，并评估了开源大型语言模型的性能。

Conclusion: KnoVo为科研新颖性分析提供了创新工具，支持知识演变的可视化和研究空白的发现。

Abstract: This paper presents KnoVo (Knowledge Evolution), an intelligent framework
designed for quantifying and analyzing the evolution of research novelty in the
scientific literature. Moving beyond traditional citation analysis, which
primarily measures impact, KnoVo determines a paper's novelty relative to both
prior and subsequent work within its multilayered citation network. Given a
target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to
dynamically extract dimensions of comparison (e.g., methodology, application,
dataset). The target paper is then compared to related publications along these
same extracted dimensions. This comparative analysis, inspired by tournament
selection, yields quantitative novelty scores reflecting the relative
improvement, equivalence, or inferiority of the target paper in specific
aspects. By aggregating these scores and visualizing their progression, for
instance, through dynamic evolution graphs and comparative radar charts, KnoVo
facilitates researchers not only to assess originality and identify similar
work, but also to track knowledge evolution along specific research dimensions,
uncover research gaps, and explore cross-disciplinary connections. We
demonstrate these capabilities through a detailed analysis of 20 diverse papers
from multiple scientific fields and report on the performance of various
open-source LLMs within the KnoVo framework.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [112] [Contextual Pattern Mining and Counting](https://arxiv.org/abs/2506.17613)
*Ling Li,Daniel Gibney,Sharma V. Thankachan,Solon P. Pissis,Grigorios Loukides*

Main category: cs.DS

TL;DR: 论文提出了两个与上下文模式相关的问题（CPM和CPC），并分别给出了高效的解决方案和优化策略。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效地挖掘和计数字符串上下文，以处理大规模数据集。

Method: CPM问题使用线性工作算法，支持内存和外部存储；CPC问题提出基于LZ77分解的索引优化。

Result: CPM算法能处理大规模数据集；CPC索引在查询时间、构造时间和空间上优于现有方法。

Conclusion: 提出的算法和索引优化在性能和实用性上表现优异，适用于不同领域的大规模数据处理。

Abstract: Given a string $P$ of length $m$, a longer string $T$ of length $n>m$, and
two integers $l\geq 0$ and $r\geq 0$, the context of $P$ in $T$ is the set of
all string pairs $(L,R)$, with $|L|=l$ and $|R|=r$, such that the string $LPR$
occurs in $T$. We introduce two problems related to the notion of context: (1)
the Contextual Pattern Mining (CPM) problem, which given $T$, $(m,l,r)$, and an
integer $\tau>0$, asks for outputting the context of each substring $P$ of
length $m$ of $T$, provided that the size of the context of $P$ is at least
$\tau$; and (2) the Contextual Pattern Counting (CPC) problem, which asks for
preprocessing $T$ so that the size of the context of a given query string $P$
of length $m$ can be found efficiently.
  For CPM, we propose a linear-work algorithm that either uses only internal
memory, or a bounded amount of internal memory and external memory, which
allows much larger datasets to be handled. For CPC, we propose an
$\widetilde{\mathcal{O}}(n)$-space index that can be constructed in
$\widetilde{\mathcal{O}}n)$ time and answers queries in
$\mathcal{O}(m)+\widetilde{\mathcal{O}}(1)$ time. We further improve the
practical performance of the CPC index by optimizations that exploit the LZ77
factorization of $T$ and an upper bound on the query length. Using
billion-letter datasets from different domains, we show that the external
memory version of our CPM algorithm can deal with very large datasets using a
small amount of internal memory while its runtime is comparable to that of the
internal memory version. Interestingly, we also show that our optimized index
for CPC outperforms an approach based on the state of the art for the reporting
version of CPC [Navarro, SPIRE 2020] in terms of query time, index size,
construction time, and construction space, often by more than an order of
magnitude.

</details>


### [113] [Fully-Dynamic Parallel Algorithms for Single-Linkage Clustering](https://arxiv.org/abs/2506.18384)
*Quinten De Man,Laxman Dhulipala,Kishen N Gowda*

Main category: cs.DS

TL;DR: 论文研究了在完全动态环境下维护单链接层次聚类树（SLD）的问题，提出了比现有静态算法更高效的动态更新算法。


<details>
  <summary>Details</summary>
Motivation: 现有的单链接层次聚类树（SLD）维护算法在动态环境下效率不足，无法高效处理数据的插入和删除操作。

Method: 提出了一系列动态更新算法，包括插入、删除、并行及批量并行版本，重点优化了时间复杂度和并行效率。

Result: 动态插入算法的时间复杂度为O(h)，删除算法为O(h log(1+n/h))，并支持高效并行处理。

Conclusion: 这些算法在动态环境下显著提升了SLD的维护效率，尤其在树低高度或结构变化较小的情况下效果更佳。

Abstract: Single-linkage clustering is a popular form of hierarchical agglomerative
clustering (HAC) where the distance between two clusters is defined as the
minimum distance between any pair of points across the two clusters. In
single-linkage HAC, the output is typically the single-linkage dendrogram
(SLD), which is the binary tree representing the hierarchy of clusters formed
by iteratively contracting the two closest clusters. In the dynamic setting,
prior work has only studied maintaining a minimum spanning forest over the data
since single-linkage HAC reduces to computing the SLD on the minimum spanning
forest of the data.
  In this paper, we study the problem of maintaining the SLD in the
fully-dynamic setting. We assume the input is a dynamic forest $F$
(representing the minimum spanning forest of the data) which receives a
sequence of edge insertions and edge deletions. To our knowledge, no prior work
has provided algorithms to update an SLD asymptotically faster than recomputing
it from scratch. All of our update algorithms are asymptotically faster than
the best known static SLD computation algorithm, which takes $O(n \log h)$ time
where $h$ is the height of the dendrogram ($h \leq n-1$). Furthermore, our
algorithms are much faster in many cases, such as when $h$ is low. Our first
set of results are an insertion algorithm in $O(h)$ time and a deletion
algorithm in $O(h \log (1+n/h))$ time. Next, we describe parallel and
batch-parallel versions of these algorithms which are work-efficient or nearly
work-efficient and have poly-logarithmic depth. Finally, we show how to perform
insertions near-optimally in $O(c \log(1+n/c))$ time, where $c$ is the number
of structural changes in the dendrogram caused by the update, and give a
work-efficient parallel version of this algorithm that has polylogarithmic
depth.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [114] [Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2506.17342)
*Zijian Long,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.LG

TL;DR: ASMS是一种基于F-MAPPO的新型流媒体系统，旨在解决社交元宇宙中的隐私和流媒体质量问题，提升用户体验至少14%。


<details>
  <summary>Details</summary>
Motivation: 社交元宇宙的快速发展带来了隐私和流媒体质量的挑战，需要一种既能保护用户数据又能提供高质量流媒体的解决方案。

Method: 提出ASMS系统，结合联邦学习（FL）和深度强化学习（DRL），动态调整流媒体比特率，同时保护用户隐私。

Result: 实验表明，ASMS在各种网络条件下比其他流媒体方法用户体验提升至少14%。

Conclusion: ASMS能够在资源受限的动态网络中提供无缝和沉浸式的流媒体体验，同时确保用户数据隐私。

Abstract: The social metaverse is a growing digital ecosystem that blends virtual and
physical worlds. It allows users to interact socially, work, shop, and enjoy
entertainment. However, privacy remains a major challenge, as immersive
interactions require continuous collection of biometric and behavioral data. At
the same time, ensuring high-quality, low-latency streaming is difficult due to
the demands of real-time interaction, immersive rendering, and bandwidth
optimization. To address these issues, we propose ASMS (Adaptive Social
Metaverse Streaming), a novel streaming system based on Federated Multi-Agent
Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which
integrates federated learning (FL) and deep reinforcement learning (DRL) to
dynamically adjust streaming bit rates while preserving user privacy.
Experimental results show that ASMS improves user experience by at least 14%
compared to existing streaming methods across various network conditions.
Therefore, ASMS enhances the social metaverse experience by providing seamless
and immersive streaming, even in dynamic and resource-constrained networks,
while ensuring that sensitive user data remains on local devices.

</details>


### [115] [Episode-specific Fine-tuning for Metric-based Few-shot Learners with Optimization-based Training](https://arxiv.org/abs/2506.17499)
*Xuanyu Zhuang,Geoffroy Peeters,Gaël Richard*

Main category: cs.LG

TL;DR: 提出了一系列在推理过程中针对小样本分类任务的微调方法，通过利用支持样本优化度量空间，并结合元学习框架避免过拟合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有度量模型在推理时未充分利用支持样本，仅用于相似性比较，忽略了优化度量空间的潜力。

Method: 提出了RDFT及其变体IDFT和ADFT，通过构建伪支持-查询对进行微调，并结合优化元学习框架。

Result: 在多个音频数据集上验证，性能显著提升，尤其适用于基于注意力的模型。

Conclusion: 方法有效利用了支持样本，通过微调和元学习提高了小样本分类性能并避免过拟合。

Abstract: In few-shot classification tasks (so-called episodes), a small set of labeled
support samples is provided during inference to aid the classification of
unlabeled query samples. Metric-based models typically operate by computing
similarities between query and support embeddings within a learned metric
space, followed by nearest-neighbor classification. However, these labeled
support samples are often underutilized--they are only used for similarity
comparison, despite their potential to fine-tune and adapt the metric space
itself to the classes in the current episode. To address this, we propose a
series of simple yet effective episode-specific, during-inference fine-tuning
methods for metric-based models, including Rotational Division Fine-Tuning
(RDFT) and its two variants, Iterative Division Fine-Tuning (IDFT) and
Augmented Division Fine-Tuning (ADFT). These methods construct pseudo
support-query pairs from the given support set to enable fine-tuning even for
non-parametric models. Nevertheless, the severely limited amount of data in
each task poses a substantial risk of overfitting when applying such
fine-tuning strategies. To mitigate this, we further propose to train the
metric-based model within an optimization-based meta-learning framework. With
the combined efforts of episode-specific fine-tuning and optimization-based
meta-training, metric-based models are equipped with the ability to rapidly
adapt to the limited support samples during inference while avoiding
overfitting. We validate our approach on three audio datasets from diverse
domains, namely ESC-50 (environmental sounds), Speech Commands V2 (spoken
keywords), and Medley-solos-DB (musical instrument). Experimental results
demonstrate that our approach consistently improves performance for all
evaluated metric-based models (especially for attention-based models) and
generalizes well across different audio domains.

</details>


### [116] [AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing](https://arxiv.org/abs/2506.18495)
*Aniss Bessalah,Hatem Mohamed Abdelmoumen,Karima Benatchba,Hadjer Benmeziane*

Main category: cs.LG

TL;DR: 该研究提出了首个针对模拟内存计算（AIMC）的NAS基准测试AnalogNAS-Bench，并揭示了当前量化技术和架构设计在AIMC上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络未针对AIMC的独特非理想特性设计，需通过NAS优化架构以适配AIMC约束。

Method: 开发了专用NAS基准测试AnalogNAS-Bench，分析AIMC特定噪声下的架构表现。

Result: 发现标准量化技术无法捕捉AIMC噪声，稳健架构需更宽、分支化模块和跳跃连接。

Conclusion: AnalogNAS-Bench为未来AIMC优化的NAS研究提供了工具和方向。

Abstract: Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm
for accelerating Deep Neural Networks (DNNs), offering significant energy and
latency benefits over conventional digital hardware. However, state-of-the-art
neural networks are not inherently designed for AIMC, as they fail to account
for its unique non-idealities. Neural Architecture Search (NAS) is thus needed
to systematically discover neural architectures optimized explicitly for AIMC
constraints. However, comparing NAS methodologies and extracting insights about
robust architectures for AIMC requires a dedicated NAS benchmark that
explicitly accounts for AIMC-specific hardware non-idealities. To address this,
we introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for
AIMC. Our study reveals three key insights: (1) standard quantization
techniques fail to capture AIMC-specific noises, (2) robust architectures tend
to feature wider and branched blocks, (3) skip connections improve resilience
to temporal drift noise. These insights highlight the limitations of current
NAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the
implementations used in this paper can be found at
https://github.com/IBM/analog-nas/tree/main/analognasbench.

</details>


### [117] [Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks](https://arxiv.org/abs/2506.17672)
*Weiming Mai,Jie Gao,Oded Cats*

Main category: cs.LG

TL;DR: 本文提出了一种基于超网络和集成学习的个性化效用函数学习方法，用于预测网约车司机的接单决策，解决了传统模型无法捕捉非线性关系和个性化偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 准确预测司机的接单决策对于提高网约车系统的效率和可靠性至关重要。传统线性模型在捕捉非线性关系和个性化偏好方面表现不足，需要改进。

Method: 通过超网络动态生成线性效用函数的权重，结合集成学习从不同数据片段训练多个超网络，以提高模型的适应性和泛化能力。

Result: 模型在实际数据集中表现出高的预测准确性和不确定性估计能力，同时平衡了解释性和不确定性量化需求，并能揭示司机的个性化偏好。

Conclusion: 提出的集成超网络模型在预测司机接单决策方面具有显著优势，能够有效捕捉非线性关系和个性化偏好，为网约车系统优化提供了有力工具。

Abstract: In ride-hailing systems, drivers decide whether to accept or reject ride
requests based on factors such as order characteristics, traffic conditions,
and personal preferences. Accurately predicting these decisions is essential
for improving the efficiency and reliability of these systems. Traditional
models, such as the Random Utility Maximization (RUM) approach, typically
predict drivers' decisions by assuming linear correlations among attributes.
However, these models often fall short because they fail to account for
non-linear interactions between attributes and do not cater to the unique,
personalized preferences of individual drivers. In this paper, we develop a
method for learning personalized utility functions using hypernetwork and
ensemble learning. Hypernetworks dynamically generate weights for a linear
utility function based on trip request data and driver profiles, capturing the
non-linear relationships. An ensemble of hypernetworks trained on different
data segments further improve model adaptability and generalization by
introducing controlled randomness, thereby reducing over-fitting. We validate
the performance of our ensemble hypernetworks model in terms of prediction
accuracy and uncertainty estimation in a real-world dataset. The results
demonstrate that our approach not only accurately predicts each driver's
utility but also effectively balances the needs for explainability and
uncertainty quantification. Additionally, our model serves as a powerful tool
for revealing the personalized preferences of different drivers, clearly
illustrating which attributes largely impact their rider acceptance decisions.

</details>


### [118] [Machine Learning Model Integration with Open World Temporal Logic for Process Automation](https://arxiv.org/abs/2506.17776)
*Dyuman Aditya,Colton Payne,Mario Leiva,Paulo Shakarian*

Main category: cs.LG

TL;DR: 论文提出了一种将机器学习模型输出与PyReason框架结合的新方法，用于实现实时自适应决策。


<details>
  <summary>Details</summary>
Motivation: 解决如何将机器学习的感知与提取能力转化为复杂工作流中的可操作决策。

Method: 通过PyReason框架，将ML模型的实值输出（如概率）转换为逻辑事实，并动态计算最小模型。

Result: 实现了结合ML感知能力和PyReason逻辑推理的自动化系统，支持实时决策和时间敏感分析。

Conclusion: 该方法在制造、医疗等多个领域具有潜在应用价值。

Abstract: Recent advancements in Machine Learning (ML) have yielded powerful models
capable of extracting structured information from diverse and complex data
sources. However, a significant challenge lies in translating these perceptual
or extractive outputs into actionable, reasoned decisions within complex
operational workflows. To address these challenges, this paper introduces a
novel approach that integrates the outputs from various machine learning models
directly with the PyReason framework, an open-world temporal logic programming
reasoning engine. PyReason's foundation in generalized annotated logic allows
for the seamless incorporation of real-valued outputs (e.g., probabilities,
confidence scores) from diverse ML models, treating them as truth intervals
within its logical framework. Crucially, PyReason provides mechanisms,
implemented in Python, to continuously poll ML model outputs, convert them into
logical facts, and dynamically recompute the minimal model, ensuring real-tine
adaptive decision-making. Furthermore, its native support for temporal
reasoning, knowledge graph integration, and fully explainable interface traces
enables sophisticated analysis over time-sensitive process data and existing
organizational knowledge. By combining the strengths of perception and
extraction from ML models with the logical deduction and transparency of
PyReason, we aim to create a powerful system for automating complex processes.
This integration finds utility across numerous domains, including
manufacturing, healthcare, and business operations.

</details>


### [119] [SliceGX: Layer-wise GNN Explanation with Model-slicing](https://arxiv.org/abs/2506.17977)
*Tingting Zhu,Tingyang Chen,Yinghui Wu,Arijit Khan,Xiangyu Ke*

Main category: cs.LG

TL;DR: SliceGX是一种新颖的GNN解释方法，通过分层渐进式分析生成解释，解决了现有方法缺乏细粒度层间分析的不足。


<details>
  <summary>Details</summary>
Motivation: 现有GNN解释方法通常通过输入扰动识别子图，但缺乏对中间表示如何影响最终结果的细粒度分析，这对模型诊断和架构优化至关重要。

Method: SliceGX将GNN自动分割为层块（模型切片），并在每个层块中发现高质量解释子图，通过高效算法和优化技术逐步生成和维持子图，并提供SPARQL-like查询接口。

Result: 实验验证了SliceGX在大型真实图数据和典型GNN架构上的有效性和高效性，展示了其在模型调试中的实际应用价值。

Conclusion: SliceGX通过分层渐进式分析和高效算法，为GNN提供了细粒度的解释能力，支持模型诊断和优化。

Abstract: Ensuring the trustworthiness of graph neural networks (GNNs) as black-box
models requires effective explanation methods. Existing GNN explanations
typically apply input perturbations to identify subgraphs that are responsible
for the occurrence of the final output of GNNs. However, such approaches lack
finer-grained, layer-wise analysis of how intermediate representations
contribute to the final result, capabilities that are crucial for model
diagnosis and architecture optimization. This paper introduces SliceGX, a novel
GNN explanation approach that generates explanations at specific GNN layers in
a progressive manner. Given a GNN M, a set of selected intermediate layers, and
a target layer, SliceGX automatically segments M into layer blocks ("model
slice") and discovers high-quality explanatory subgraphs in each layer block
that clarifies the occurrence of output of M at the targeted layer. Although
finding such layer-wise explanations is computationally challenging, we develop
efficient algorithms and optimization techniques that incrementally generate
and maintain these subgraphs with provable approximation guarantees.
Additionally, SliceGX offers a SPARQL-like query interface, providing
declarative access and search capacities for the generated explanations.
Through experiments on large real-world graphs and representative GNN
architectures, we verify the effectiveness and efficiency of SliceGX, and
illustrate its practical utility in supporting model debugging.

</details>


### [120] [PuckTrick: A Library for Making Synthetic Data More Realistic](https://arxiv.org/abs/2506.18499)
*Alessandra Agostini,Andrea Maurino,Blerina Spahiu*

Main category: cs.LG

TL;DR: 论文介绍了Pucktrick库，用于通过引入受控错误来污染合成数据集，以提高机器学习模型的鲁棒性和泛化能力。实验表明，用污染后的合成数据训练的模型表现优于纯合成数据训练的模型。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和专有限制，真实数据难以获取，而合成数据又过于干净，缺乏真实数据的复杂性。Pucktrick通过引入受控错误，填补了这一空白。

Method: 提出Pucktrick库，支持多种错误类型（如缺失值、噪声、异常值等），提供两种污染模式：对干净数据集或已污染数据集进一步注入错误。

Result: 在真实金融数据集上的实验表明，用受污染合成数据训练的模型（尤其是树模型和线性模型）表现优于纯合成数据训练的模型。

Conclusion: Pucktrick通过增强合成数据的复杂性，为评估和提升模型鲁棒性提供了有效工具。

Abstract: The increasing reliance on machine learning (ML) models for decision-making
requires high-quality training data. However, access to real-world datasets is
often restricted due to privacy concerns, proprietary restrictions, and
incomplete data availability. As a result, synthetic data generation (SDG) has
emerged as a viable alternative, enabling the creation of artificial datasets
that preserve the statistical properties of real data while ensuring privacy
compliance. Despite its advantages, synthetic data is often overly clean and
lacks real-world imperfections, such as missing values, noise, outliers, and
misclassified labels, which can significantly impact model generalization and
robustness. To address this limitation, we introduce Pucktrick, a Python
library designed to systematically contaminate synthetic datasets by
introducing controlled errors. The library supports multiple error types,
including missing data, noisy values, outliers, label misclassification,
duplication, and class imbalance, offering a structured approach to evaluating
ML model resilience under real-world data imperfections. Pucktrick provides two
contamination modes: one for injecting errors into clean datasets and another
for further corrupting already contaminated datasets. Through extensive
experiments on real-world financial datasets, we evaluate the impact of
systematic data contamination on model performance. Our findings demonstrate
that ML models trained on contaminated synthetic data outperform those trained
on purely synthetic, error-free data, particularly for tree-based and linear
models such as SVMs and Extra Trees.

</details>


### [121] [NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN](https://arxiv.org/abs/2506.17870)
*Jianhang Xie,Chuntao Ding,Xiaqing Li,Shenyuan Ren,Yidong Li,Zhichao Lu*

Main category: cs.LG

TL;DR: 论文提出了一种名为NestQuant的资源友好型后训练整数嵌套量化方法，用于在物联网设备上实现量化模型切换，以适应动态资源需求。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有后训练量化方法无法动态适应物联网设备资源变化以及部署多模型带来的存储和切换开销问题。

Method: 通过整数权重分解和嵌套机制，将量化权重按位拆分为高位和低位整数权重，并通过自适应舍入优化高位权重。

Result: 实验表明，NestQuant在ImageNet-1K上表现优异，能显著减少数据传输、存储和切换开销。如ResNet-101在INT8嵌套INT6时，切换开销降低78.1%。

Conclusion: NestQuant是一种高效的资源自适应量化方法，适用于物联网设备部署，具有性能高、开销低的优势。

Abstract: Deploying quantized deep neural network (DNN) models with resource adaptation
capabilities on ubiquitous Internet of Things (IoT) devices to provide
high-quality AI services can leverage the benefits of compression and meet
multi-scenario resource requirements. However, existing dynamic/mixed precision
quantization requires retraining or special hardware, whereas post-training
quantization (PTQ) has two limitations for resource adaptation: (i) The
state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes
it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying
multiple PTQ models with diverse bitwidths consumes large storage resources and
switching overheads. To this end, this paper introduces a resource-friendly
post-training integer-nesting quantization, i.e., NestQuant, for on-device
quantized model switching on IoT devices. The proposed NestQuant incorporates
the integer weight decomposition, which bit-wise splits quantized weights into
higher-bit and lower-bit weights of integer data types. It also contains a
decomposed weights nesting mechanism to optimize the higher-bit weights by
adaptive rounding and nest them into the original quantized weights. In
deployment, we can send and store only one NestQuant model and switch between
the full-bit/part-bit model by paging in/out lower-bit weights to adapt to
resource changes and reduce consumption. Experimental results on the
ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve
high performance in top-1 accuracy, and reduce in terms of data transmission,
storage consumption, and switching overheads. In particular, the ResNet-101
with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and
part-bit models, respectively, and reduce switching overheads by approximately
78.1% compared with diverse bitwidths PTQ models.

</details>


### [122] [DeInfoReg: A Decoupled Learning Framework for Better Training Throughput](https://arxiv.org/abs/2506.18193)
*Zih-Hao Huang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: DeInfoReg通过分解梯度流解决梯度消失问题，并结合并行计算提升训练效率。实验表明其性能优于传统反向传播。


<details>
  <summary>Details</summary>
Motivation: 解决梯度消失问题，并利用并行计算提高训练吞吐量。

Method: 采用分解梯度流和信息正则化的方法，支持多GPU并行训练。

Result: 在多种任务和数据集上表现优于传统方法，且抗噪能力更强。

Conclusion: DeInfoReg是一种有效且高效的训练方法，特别适用于并行计算环境。

Abstract: This paper introduces Decoupled Supervised Learning with Information
Regularization (DeInfoReg), a novel approach that transforms a long gradient
flow into multiple shorter ones, thereby mitigating the vanishing gradient
problem. Integrating a pipeline strategy, DeInfoReg enables model
parallelization across multiple GPUs, significantly improving training
throughput. We compare our proposed method with standard backpropagation and
other gradient flow decomposition techniques. Extensive experiments on diverse
tasks and datasets demonstrate that DeInfoReg achieves superior performance and
better noise resistance than traditional BP models and efficiently utilizes
parallel computing resources. The code for reproducibility is available at:
https://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.

</details>


### [123] [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution](https://arxiv.org/abs/2506.17323)
*Tamas Bisztray,Bilel Cherif,Richard A. Dubniczky,Nils Gruschka,Bertalan Borsos,Mohamed Amine Ferrag,Attila Kovacs,Vasileios Mavroeidis,Norbert Tihanyi*

Main category: cs.LG

TL;DR: 该论文系统地研究了LLM生成的C程序作者归属问题，提出了一种名为CodeT5-Authorship的新模型，并通过LLM-AuthorBench基准测试展示了其卓越的分类性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成的代码越来越普遍，识别代码背后具体模型的需求日益增加，作者旨在填补这一研究空白并提供开源工具。

Method: 作者基于CodeT5的编码器层构建了CodeT5-Authorship模型，舍弃解码器以专注于分类任务，并通过分类头输出作者的概率分布。

Result: 在二进制分类中，模型对GPT-4.1和GPT-4o的区分准确率达97.56%，多分类中五种主流LLM的准确率为95.40%。

Conclusion: CodeT5-Authorship模型在LLM作者归属任务上表现出色，且开源了模型与基准测试，为后续研究提供了资源。

Abstract: Detecting AI-generated code, deepfakes, and other synthetic content is an
emerging research challenge. As code generated by Large Language Models (LLMs)
becomes more common, identifying the specific model behind each sample is
increasingly important. This paper presents the first systematic study of LLM
authorship attribution for C programs. We released CodeT5-Authorship, a novel
model that uses only the encoder layers from the original CodeT5
encoder-decoder architecture, discarding the decoder to focus on
classification. Our model's encoder output (first token) is passed through a
two-layer classification head with GELU activation and dropout, producing a
probability distribution over possible authors. To evaluate our approach, we
introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs
generated by eight state-of-the-art LLMs across diverse tasks. We compare our
model to seven traditional ML classifiers and eight fine-tuned transformer
models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,
Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model
achieves 97.56% accuracy in distinguishing C programs generated by closely
related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class
attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,
GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the
CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant
Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [124] [Cost-Effective Optimization and Implementation of the CRT-Paillier Decryption Algorithm for Enhanced Performance](https://arxiv.org/abs/2506.17935)
*Zhengwu Huang,Ding Deng,Pengyue Sun,Guangfu Sun,Xiaomei Tang*

Main category: cs.CR

TL;DR: 提出了一种改进的eCRT-Paillier解密算法，通过预计算参数和优化Montgomery模乘操作，减少计算链长度和判断操作，提升解密效率，并设计了一种高效的并行架构MESA。


<details>
  <summary>Details</summary>
Motivation: 当前Paillier算法在解密时计算效率受限于复杂的模运算和密码扩展，使用CRT优化虽能加速模运算，但会延长解密计算链，因此需要一种更高效的解法。

Method: 结合预计算参数和消除冗余判断操作，缩短解密链；设计并行化全流水线架构，避免乘法器复用带来的停滞。

Result: MESA在2048位密钥下解密仅需0.577ms（100 MHz），吞吐量提升1.16至313.21倍，资源效率也显著提升。

Conclusion: eCRT-Paillier和MESA架构显著提升了Paillier算法的解密效率和并行能力，适用于高性能隐私保护场景。

Abstract: To address the privacy protection problem in cloud computing, privacy
enhancement techniques such as the Paillier additive homomorphism algorithm are
receiving widespread attention. Paillier algorithm allows addition and scalar
multiplication operations in dencrypted state, which can effectively protect
privacy. However, its computational efficiency is limited by complex modulo
operations due to the ciphertext expansion followed by encryption. To
accelerate its decryption operation, the Chinese Remainder Theorem (CRT) is
often used to optimize these modulo operations, which lengthens the decryption
computation chain in turn. To address this issue, we propose an eCRT-Paillier
decryption algorithm that shortens the decryption computation chain by
combining precomputed parameters and eliminating extra judgment operations
introduced by Montgomery modular multiplications. These two improvements reduce
50% modular multiplications and 60% judgment operations in the postprocessing
of the CRT-Paillier decryption algorithm. Based on these improvements, we
propose a highly parallel full-pipeline architecture to eliminate stalls caused
by multiplier reuse in traditional modular exponentiation operations. This
architecture also adopts some optimizations such as simplifying modular
exponentiation units by dividing the exponent into segments and parallelizing
data flow by multi-core instantiation. Finally, a high-throughput and efficient
Paillier accelerator named MESA was implemented on the Xilinx Virtex-7 FPGA for
evaluation, which can complete a decryption using 2048-bit key within 0.577ms
under 100 MHz clock frequency. Compared to prior works, MESA demonstrates a
throughput improvement of 1.16 to 313.21 under identical conditions, also with
enhancements in area efficiency for LUT, DSP, and FF of 3.32 to 117.55, 1.49 to
1.64, and 2.94 to 9.94, respectively.

</details>


### [125] [Design high-confidence computers using trusted instructional set architecture and emulators](https://arxiv.org/abs/2506.18780)
*Shuangbao Paul Wang*

Main category: cs.CR

TL;DR: 论文提出了一种全面的可信计算机架构设计与仿真方法，解决现代处理器因Spectre和Meltdown漏洞导致的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现代处理器因分支预测和流水线技术存在Spectre和Meltdown漏洞，现有软件补丁无法根本解决问题，因此需要新的可信架构设计。

Method: 采用全面的计算机架构设计与仿真方法，结合可信指令集架构和密封内核等技术。

Result: 提出了一种解决处理器安全漏洞的新方案，避免了禁用分支预测和流水线的性能损失。

Conclusion: 该研究为高性能且安全的计算机架构设计提供了可行路径。

Abstract: High-confidence computing relies on trusted instructional set architecture,
sealed kernels, and secure operating systems. Cloud computing depends on
trusted systems for virtualization tasks. Branch predictions and pipelines are
essential in improving performance of a CPU/GPU. But Spectre and Meltdown make
modern processors vulnerable to be exploited. Disabling the prediction and
pipeline is definitely not a good solution. On the other hand, current software
patches can only address non-essential issues around Meltdown. This paper
introduces a holistic approach in trusted computer architecture design and
emulation.

</details>


### [126] [LASA: Enhancing SoC Security Verification with LLM-Aided Property Generation](https://arxiv.org/abs/2506.17865)
*Dinesh Reddy Ankireddy,Sudipta Paria,Aritra Dasgupta,Sandip Ray,Swarup Bhunia*

Main category: cs.CR

TL;DR: LASA框架利用LLM和RAG技术，自动生成非空洞的安全属性和SystemVerilog断言，提高SoC设计的形式化验证效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代SoC设计复杂度高，传统的FPV方法需要大量人工工作生成安全属性，耗时且易错。LLM和RAG技术为解决这一问题提供了可能。

Method: LASA结合LLM和RAG技术，从设计规范和文档中生成非空洞的安全属性和SVA，集成EDA工具进行FPV，并通过反馈循环优化提示以提升覆盖率。

Result: 在多个开源SoC设计中，LASA平均覆盖率约88%，并在OpenTitan SoC中发现5个独特漏洞。

Conclusion: LASA证明了利用LLM和RAG技术可高效生成安全属性和SVA，显著提升SoC验证的全面性和效率。

Abstract: Ensuring the security of modern System-on-Chip (SoC) designs poses
significant challenges due to increasing complexity and distributed assets
across the intellectual property (IP) blocks. Formal property verification
(FPV) provides the capability to model and validate design behaviors through
security properties with model checkers; however, current practices require
significant manual efforts to create such properties, making them
time-consuming, costly, and error-prone. The emergence of Large Language Models
(LLMs) has showcased remarkable proficiency across diverse domains, including
HDL code generation and verification tasks. Current LLM-based techniques often
produce vacuous assertions and lack efficient prompt generation, comprehensive
verification, and bug detection. This paper presents LASA, a novel framework
that leverages LLMs and retrieval-augmented generation (RAG) to produce
non-vacuous security properties and SystemVerilog Assertions (SVA) from design
specifications and related documentation for bus-based SoC designs. LASA
integrates commercial EDA tool for FPV to generate coverage metrics and
iteratively refines prompts through a feedback loop to enhance coverage. The
effectiveness of LASA is validated through various open-source SoC designs,
demonstrating high coverage values with an average of ~88\%, denoting
comprehensive verification through efficient generation of security properties
and SVAs. LASA also demonstrates bug detection capabilities, identifying five
unique bugs in the buggy OpenTitan SoC from Hack@DAC'24 competition.

</details>


### [127] [Secret Sharing in 5G-MEC: Applicability for joint Security and Dependability](https://arxiv.org/abs/2506.17371)
*Thilina Pathirana,Ruxandra F. Olimid*

Main category: cs.CR

TL;DR: 该论文探讨了在5G-MEC存储中使用阈值秘密共享技术以解决隐私和安全问题。


<details>
  <summary>Details</summary>
Motivation: 5G-MEC具有低延迟和高效率的优势，但也带来了数据隐私和安全问题，尤其是边缘节点的分布特性增加了风险。

Method: 采用(k,n)阈值秘密共享方案，将敏感数据分片存储于多个边缘节点，需至少k个节点协作才能重建数据。同时提出基于节点可信度选择存储节点的方法。

Result: 该方法不仅能防止最多k-1个节点合谋窃取数据，还能容忍最多n-k个节点失败，提升了数据的安全性和可用性。

Conclusion: 阈值秘密共享是一种有效的安全解决方案，适用于5G-MEC环境，并可扩展至其他需要可信节点选择的场景。

Abstract: Multi-access Edge Computing (MEC), an enhancement of 5G, processes data
closer to its generation point, reducing latency and network load. However, the
distributed and edge-based nature of 5G-MEC presents privacy and security
challenges, including data exposure risks. Ensuring efficient manipulation and
security of sensitive data at the edge is crucial. To address these challenges,
we investigate the usage of threshold secret sharing in 5G-MEC storage, an
approach that enhances both security and dependability. A (k,n) threshold
secret sharing scheme splits and stores sensitive data among n nodes, requiring
at least k nodes for reconstruction. The solution ensures confidentiality by
protecting data against fewer than k colluding nodes and enhances availability
by tolerating up to n-k failing nodes. This approach mitigates threats such as
unauthorized access and node failures, whether accidental or intentional. We
further discuss a method for selecting the convenient MEHs to store the shares,
considering the MEHs' trustworthiness level as a main criterion. Although we
define our proposal in the context of secret-shared data storage, it can be
seen as an independent, standalone selection process for 5G-MEC trustworthy
node selection in other scenarios too.

</details>


### [128] [Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems](https://arxiv.org/abs/2506.17236)
*Serdar Metin*

Main category: cs.CR

TL;DR: 本文提出了一种在非商业区块链网络中公平分配共享资源的方法，解决了现有机制（如“faucet”）易受拒绝服务攻击和缺乏公平性的问题。


<details>
  <summary>Details</summary>
Motivation: 在非商业区块链网络中，由于无法使用货币解决方案，现有资源分配机制（如“faucet”）存在公平性和安全性问题，亟需改进。

Method: 通过将“faucet”机制与Max-min Fairness方案结合，提出了6种高效且抗拒绝服务攻击的算法。

Result: 这些算法不仅抗攻击，还具备低成本特性，并支持不同的用户权重策略。

Conclusion: 研究表明，Max-min Fair算法为区块链资源分配提供了更公平和安全的解决方案。

Abstract: The present dissertation addresses the problem of fairly distributing shared
resources in non-commercial blockchain networks. Blockchains are distributed
systems that order and timestamp records of a given network of users, in a
public, cryptographically secure, and consensual way. The records, which may in
kind be events, transaction orders, sets of rules for structured transactions
etc. are placed within well-defined datastructures called blocks, and they are
linked to each other by the virtue of cryptographic pointers, in a total
ordering which represents their temporal relations of succession. The ability
to operate on the blockchain, and/or to contribute a record to the content of a
block are shared resources of the blockchain systems. In commercial networks,
these resources are exchanged in return for fiat money, and consequently,
fairness is not a relevant problem in terms of computer engineering. In
non-commercial networks, however, monetary solutions are not available, by
definition. The present non-commercial blockchain networks employ trivial
distribution mechanisms called faucets, which offer fixed amounts of free
tokens (called cryptocurrencies) specific to the given network. This mechanism,
although simple and efficient, is prone to denial of service (DoS) attacks and
cannot address the fairness problem. In the present dissertation, the faucet
mechanism is adapted for fair distribution, in line with Max-min Fairness
scheme. In total, we contributed 6 distinct Max-min Fair algorithms as
efficient blockchain faucets. The algorithms we contribute are resistant to DoS
attacks, low-cost in terms of blockchain computation economics, and they also
allow for different user weighting policies.

</details>


### [129] [A Locally Differential Private Coding-Assisted Succinct Histogram Protocol](https://arxiv.org/abs/2506.17767)
*Hsuan-Po Liu,Hessam Mahdavifar*

Main category: cs.CR

TL;DR: 论文提出了一种基于误差校正码的实用性$(ϵ,δ)$-LDP协议，用于构建简洁直方图，并通过极化码和软解码算法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在大规模、隐私敏感的机器学习应用中，简洁直方图（capturing frequent items and their frequencies）的重要性日益凸显。然而，在本地差分隐私（LDP）框架下，如何在保护隐私的同时保持数据效用成为一个关键挑战。

Method: 利用极化码及其逐次取消列表（SCL）解码算法作为底层编码方案，并引入基于高斯扰动的软解码机制，实现高效的信息收集。

Result: 实验表明，该方法在低频率项的情况下优于现有方法，同时保持了频率估计的准确性。

Conclusion: 该研究首次将误差校正码与LDP结合，为隐私保护的简洁直方图构建提供了实用的解决方案，尤其在低频项场景中表现优异。

Abstract: A succinct histogram captures frequent items and their frequencies across
clients and has become increasingly important for large-scale,
privacy-sensitive machine learning applications. To develop a rigorous
framework to guarantee privacy for the succinct histogram problem, local
differential privacy (LDP) has been utilized and shown promising results. To
preserve data utility under LDP, which essentially works by intentionally
adding noise to data, error-correcting codes naturally emerge as a promising
tool for reliable information collection. This work presents the first
practical $(\epsilon,\delta)$-LDP protocol for constructing succinct histograms
using error-correcting codes. To this end, polar codes and their
successive-cancellation list (SCL) decoding algorithms are leveraged as the
underlying coding scheme. More specifically, our protocol introduces
Gaussian-based perturbations to enable efficient soft decoding. Experiments
demonstrate that our approach outperforms prior methods, particularly for items
with low true frequencies, while maintaining similar frequency estimation
accuracy.

</details>


### [130] [Tracking GPTs Third Party Service: Automation, Analysis, and Insights](https://arxiv.org/abs/2506.17315)
*Chuan Yan,Liuhuo Wan,Bowei Guan,Fengqi Yu,Guangdong Bai,Jin Song Dong*

Main category: cs.CR

TL;DR: GPTs-ThirdSpy是一个自动化框架，用于提取GPTs的隐私设置，支持学术研究对第三方服务集成在GPTs中的数据隐私影响进行系统评估。


<details>
  <summary>Details</summary>
Motivation: GPTs与第三方服务的集成方式限制了隐私设置信息的可访问性和分析，难以系统评估其数据隐私影响。

Method: 引入GPTs-ThirdSpy框架，自动提取GPTs的隐私设置，提供实时、可靠的第三方服务元数据。

Result: GPTs-ThirdSpy支持大规模研究，分析GPTs生态系统的透明度和监管挑战。

Conclusion: GPTs-ThirdSpy为学术研究提供了工具，促进对GPTs中第三方服务集成的深入分析与监管改进。

Abstract: ChatGPT has quickly advanced from simple natural language processing to
tackling more sophisticated and specialized tasks. Drawing inspiration from the
success of mobile app ecosystems, OpenAI allows developers to create
applications that interact with third-party services, known as GPTs. GPTs can
choose to leverage third-party services to integrate with specialized APIs for
domain-specific applications. However, the way these disclose privacy setting
information limits accessibility and analysis, making it challenging to
systematically evaluate the data privacy implications of third-party integrate
to GPTs. In order to support academic research on the integration of
third-party services in GPTs, we introduce GPTs-ThirdSpy, an automated
framework designed to extract privacy settings of GPTs. GPTs-ThirdSpy provides
academic researchers with real-time, reliable metadata on third-party services
used by GPTs, enabling in-depth analysis of their integration, compliance, and
potential security risks. By systematically collecting and structuring this
data, GPTs-ThirdSpy facilitates large-scale research on the transparency and
regulatory challenges associated with the GPT app ecosystem.

</details>


### [131] [Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection](https://arxiv.org/abs/2506.18245)
*Lei Yu,Zhirong Huang,Hang Yuan,Shiqi Cheng,Li Yang,Fengjun Zhang,Chenjie Shen,Jiajia Ma,Jingyuan Zhang,Junyi Lu,Chun Zuo*

Main category: cs.CR

TL;DR: 该论文提出了Smart-LLaMA-DPO方法，通过构建高质量数据集、持续预训练和定向偏好优化，显著提升了智能合约漏洞检测的准确性和解释能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有智能合约漏洞检测方法中数据集覆盖不全、大语言模型对特定安全概念理解不足的问题。

Method: 基于LLaMA-3.1-8B，构建全面数据集并进行持续预训练、监督微调和定向偏好优化。

Result: 在F1分数和准确率上分别平均提升10.43%和7.87%，且生成的解释更正确、全面和清晰。

Conclusion: Smart-LLaMA-DPO显著提升了智能合约漏洞检测的性能和解释质量。

Abstract: Smart contract vulnerability detection remains a major challenge in
blockchain security. Existing vulnerability detection methods face two main
issues: (1) Existing datasets lack comprehensive coverage and high-quality
explanations for preference learning. (2) Large language models (LLMs) often
struggle with accurately interpreting specific concepts in smart contract
security. Empirical analysis shows that even after continual pre-training (CPT)
and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of
state changes, resulting in incorrect explanations despite making correct
detection decisions. To address these challenges, we propose Smart-LLaMA-DPO
based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major
vulnerability types and machine-unauditable vulnerabilities, including precise
labels, explanations, and locations for SFT, as well as high-quality and
low-quality output pairs for Direct Preference Optimization (DPO). Second, we
perform CPT using large-scale smart contract to enhance the LLM's understanding
of specific security practices in smart contracts. Futhermore, we conduct SFT
with our comprehensive dataset. Finally, we apply DPO, leveraging human
feedback and a specially designed loss function that increases the probability
of preferred explanations while reducing the likelihood of non-preferred
outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:
reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,
as well as machine-unauditable vulnerabilities. Our method significantly
outperforms state-of-the-art baselines, with average improvements of 10.43% in
F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human
evaluation confirm that our method generates more correct, thorough, and clear
explanations.

</details>


### [132] [Automatic Selection of Protections to Mitigate Risks Against Software Applications](https://arxiv.org/abs/2506.18470)
*Daniele Canavese,Leonardo Regano,Bjorn De Sutter,Cataldo Basile*

Main category: cs.CR

TL;DR: 提出一种基于博弈论的新型自动化软件保护选择方法，通过优化防御策略抵抗攻击并保持应用可用性。


<details>
  <summary>Details</summary>
Motivation: 解决软件中关键资产的MATE风险，优化保护决策以抵抗攻击并最小化性能开销。

Method: 使用博弈论模型和启发式最小最大深度优先搜索策略，结合动态编程优化，定义软件保护指数评估保护效果。

Result: 通过概念验证和专家评估证明该方法实用且有效。

Conclusion: 自动化软件保护是风险缓解的有效解决方案。

Abstract: This paper introduces a novel approach for the automated selection of
software protections to mitigate MATE risks against critical assets within
software applications. We formalize the key elements involved in protection
decision-making - including code artifacts, assets, security requirements,
attacks, and software protections - and frame the protection process through a
game-theoretic model. In this model, a defender strategically applies
protections to various code artifacts of a target application, anticipating
repeated attack attempts by adversaries against the confidentiality and
integrity of the application's assets. The selection of the optimal defense
maximizes resistance to attacks while ensuring the application remains usable
by constraining the overhead introduced by protections. The game is solved
through a heuristic based on a mini-max depth-first exploration strategy,
augmented with dynamic programming optimizations for improved efficiency.
Central to our formulation is the introduction of the Software Protection
Index, an original contribution that extends existing notions of potency and
resilience by evaluating protection effectiveness against attack paths using
software metrics and expert assessments. We validate our approach through a
proof-of-concept implementation and expert evaluations, demonstrating that
automated software protection is a practical and effective solution for risk
mitigation in software.

</details>


### [133] [FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction](https://arxiv.org/abs/2506.18795)
*Jiachi Chen,Yiming Shen,Jiashuo Zhang,Zihao Li,John Grundy,Zhenzhe Shao,Yanlin Wang,Jiashui Wang,Ting Chen,Zibin Zheng*

Main category: cs.CR

TL;DR: FORGE是一个自动化构建高质量智能合约漏洞数据集的方法，解决了现有手动标注方法的两大限制：劳动密集和缺乏标准化分类。


<details>
  <summary>Details</summary>
Motivation: 当前手动构建数据集劳动密集且分类不一致，需要自动化和标准化解决方案。

Method: FORGE利用LLM驱动流程从审计报告中提取漏洞，并采用分治策略和树状思维技术进行分类。

Result: 生成的包含27,497个漏洞的数据集在精度（95.6%）和分类一致性（k-α=0.87）上表现优异。

Conclusion: FORGE显著提升了数据集质量，并揭示了当前安全工具的局限性及研究重点与实际漏洞的不一致。

Abstract: High-quality smart contract vulnerability datasets are critical for
evaluating security tools and advancing smart contract security research. Two
major limitations of current manual dataset construction are (1)
labor-intensive and error-prone annotation processes limiting the scale,
quality, and evolution of the dataset, and (2) absence of standardized
classification rules results in inconsistent vulnerability categories and
labeling results across different datasets. To address these limitations, we
present FORGE, the first automated approach for constructing smart contract
vulnerability datasets. FORGE leverages an LLM-driven pipeline to extract
high-quality vulnerabilities from real-world audit reports and classify them
according to the CWE, the most widely recognized classification in software
security. FORGE employs a divide-and-conquer strategy to extract structured and
self-contained vulnerability information from these reports. Additionally, it
uses a tree-of-thoughts technique to classify the vulnerability information
into the hierarchical CWE classification. To evaluate FORGE's effectiveness, we
run FORGE on 6,454 real-world audit reports and generate a dataset comprising
81,390 solidity files and 27,497 vulnerability findings across 296 CWE
categories. Manual assessment of the dataset demonstrates high extraction
precision and classification consistency with human experts (precision of 95.6%
and inter-rater agreement k-$\alpha$ of 0.87). We further validate the
practicality of our dataset by benchmarking 13 existing security tools on our
dataset. The results reveal the significant limitations in current detection
capabilities. Furthermore, by analyzing the severity-frequency distribution
patterns through a unified CWE perspective in our dataset, we highlight
inconsistency between current smart contract research focus and priorities
identified from real-world vulnerabilities...

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [134] [Optimal Operating Strategy for PV-BESS Households: Balancing Self-Consumption and Self-Sufficiency](https://arxiv.org/abs/2506.17268)
*Jun Wook Heo,Raja Jurdak,Sara Khalifa*

Main category: eess.SY

TL;DR: 论文探讨了家庭光伏-储能系统（PV-BESS）中自消费和自给自足的线性关系，并提出了一种优化策略，通过数学模型和分类方法确定最佳PV发电和BESS容量，结合MPC和RL模型进行验证，结果表明自消费与自给自足的比率是优化系统容量的有效指标。


<details>
  <summary>Details</summary>
Motivation: 随着家庭光伏和储能系统的普及，亟需一种方法来确定最优的发电和储能容量，以最大化自消费和自给自足，减少对主电网的依赖。

Method: 通过数学模型分析自消费与自给自足的线性关系，将家庭分类为四种模式，并结合MPC和RL模型进行仿真验证。

Result: 研究发现自消费与自给自足的比率是优化PV-BESS系统容量的关键指标，可有效提升本地光伏电力的利用率。

Conclusion: 论文为家庭光伏-储能系统的优化运行提供了理论支持和实际指导，证明了自消费与自给自足比率的实用性。

Abstract: High penetration of Photovoltaic (PV) generation and Battery Energy Storage
System (BESS) in individual households increases the demand for solutions to
determine the optimal PV generation power and the capacity of BESS.
Self-consumption and self-sufficiency are essential for optimising the
operation of PV-BESS systems in households, aiming to minimise power import
from and export to the main grid. However, self-consumption and
self-sufficiency are not independent; they share a linear relationship. This
paper demonstrates this relationship and proposes an optimal operating strategy
that considers power generation and consumption profiles to maximise
self-consumption and self-sufficiency in households equipped with a PV-BESS. We
classify self-consumption and self-sufficiency patterns into four categories
based on the ratio of self-sufficiency to self-consumption for each household
and determine the optimal PV generation and BESS capacities using both a
mathematical calculation and this ratio. These optimal operation values for
each category are then simulated using Model Predictive Control (MPC) and
Reinforcement Learning (RL)-based battery charging and discharging scheduling
models. The results show that the ratio between self-consumption and
self-sufficiency is a useful metric for determining the optimal capacity of
PV-BESS systems to maximise the local utilisation of PV-generated power.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [135] [Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots](https://arxiv.org/abs/2506.18365)
*Imene Tarakli,Samuele Vinanzi,Richard Moore,Alessandro Di Nuovo*

Main category: cs.RO

TL;DR: 论文研究了通过交互式强化学习（Interactive RL）实现社交机器人在真实课堂中的教学互动，发现儿童在教授机器人学习时比自主学习表现更好，尤其是语法任务和低知识水平学生受益更多。


<details>
  <summary>Details</summary>
Motivation: 当前关于学习-教学（LbT）模式的研究多依赖于脚本化或模拟行为，缺乏对自主社交机器人在真实课堂中实时互动学习的探索。

Method: 通过两个实验，让58名小学生教授交互式RL驱动的机器人或自主学习法语词汇和语法，对比两种学习方式的效果。

Result: 教授机器人的学生在语法任务上表现显著更优，低知识水平学生受益最大；行为数据显示学生逐渐调整教学策略并更深入参与推理任务。

Conclusion: 交互式RL是教学机器人的有效模型，证明了多自主机器人同时在真实课堂中实现的可行性，拓展了LbT理论中机器人作为适应性伙伴的角色。

Abstract: Despite growing interest in Learning-by-Teaching (LbT), few studies have
explored how this paradigm can be implemented with autonomous, peer-like social
robots in real classrooms. Most prior work has relied on scripted or
Wizard-of-Oz behaviors, limiting our understanding of how real-time,
interactive learning can be supported by artificial agents. This study
addresses this gap by introducing Interactive Reinforcement Learning (RL) as a
cognitive model for teachable social robots. We conducted two between-subject
experiments with 58 primary school children, who either taught a robot or
practiced independently on a tablet while learning French vocabulary
(memorization) and grammatical rules (inference). The robot, powered by
Interactive RL, learned from the child's evaluative feedback. Children in the
LbT condition achieved significantly higher retention gains compared to those
in the self-practice condition, especially on the grammar task. Learners with
lower prior knowledge benefited most from teaching the robot. Behavioural
metrics revealed that children adapted their teaching strategies over time and
engaged more deeply during inference tasks. This work makes two contributions:
(1) it introduces Interactive RL as a pedagogically effective and scalable
model for peer-robot learning, and (2) it demonstrates, for the first time, the
feasibility of deploying multiple autonomous robots simultaneously in real
classrooms. These findings extend theoretical understanding of LbT by showing
that social robots can function not only as passive tutees but as adaptive
partners that enhance meta-cognitive engagement and long-term learning
outcomes.

</details>


### [136] [Mirror Eyes: Explainable Human-Robot Interaction at a Glance](https://arxiv.org/abs/2506.18466)
*Matti Krüger,Daniel Tanneberg,Chao Wang,Stephan Hasler,Michael Gienger*

Main category: cs.RO

TL;DR: 该研究探讨了机器人如何使用类似人眼的注视反射功能，以提高人机交互中的合作效果。实验表明，这种设计能增强用户对机器人信息处理的感知，并提升整体体验。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索机器人注视行为是否能像人类一样反映其兴趣，从而提升人机合作的直观性和效率。

Method: 研究设计了一个机器人头部系统，其屏幕眼球能注视物理空间的点，并在眼球上反射被注视区域的图像。通过33名参与者的实验，评估了这种设计在任务执行中的效果。

Result: 实验结果显示，即使没有明确解释眼球功能，带有反射注视功能的机器人能让用户更快发现错误行为，并显著提升用户体验。

Conclusion: 研究表明，机器人眼球反射功能在人机交互中具有直观且有益的作用，可用于提升合作效率。

Abstract: The gaze of a person tends to reflect their interest. This work explores what
happens when this statement is taken literally and applied to robots. Here we
present a robot system that employs a moving robot head with a screen-based eye
model that can direct the robot's gaze to points in physical space and present
a reflection-like mirror image of the attended region on top of each eye. We
conducted a user study with 33 participants, who were asked to instruct the
robot to perform pick-and-place tasks, monitor the robot's task execution, and
interrupt it in case of erroneous actions. Despite a deliberate lack of
instructions about the role of the eyes and a very brief system exposure,
participants felt more aware about the robot's information processing, detected
erroneous actions earlier, and rated the user experience higher when eye-based
mirroring was enabled compared to non-reflective eyes. These results suggest a
beneficial and intuitive utilization of the introduced method in cooperative
human-robot interaction.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [137] [Automata on $S$-adic words](https://arxiv.org/abs/2506.17460)
*Valérie Berthé,Toghrul Karimov,Mihir Vahanwala*

Main category: cs.FL

TL;DR: 研究了在广义自相似性（$S$-adic）下的无限字的自动机接受问题，证明了可以通过计算自动机$B$来回答哪些$S$-adic字被自动机$A$接受。


<details>
  <summary>Details</summary>
Motivation: 探索在逻辑和验证中，哪些一元谓词的二阶理论是可判定的，特别是针对广义自相似性的无限字。

Method: 研究由一组替换$S$生成的$S$-adic字，并构建自动机$B$来判断$A$是否接受这些字。

Result: 证明了可以通过计算自动机$B$来回答哪些$S$-adic字被自动机$A$接受。

Conclusion: 在有限替换集$S$下，可以算法化解决$S$-adic字的自动机接受问题。

Abstract: A fundamental question in logic and verification is the following: for which
unary predicates $P_1, \ldots, P_k$ is the monadic second-order theory of
$\langle \mathbb{N}; <, P_1, \ldots, P_k \rangle$ decidable? Equivalently, for
which infinite words $\alpha$ can we decide whether a given B\"uchi automaton
$A$ accepts $\alpha$? Carton and Thomas showed decidability in case $\alpha$ is
a fixed point of a letter-to-word substitution $\sigma$, i.e., $\sigma(\alpha)
= \alpha$. However, abundantly more words, e.g., Sturmian words, are
characterised by a broader notion of self-similarity that uses a set $S$ of
substitutions. A word $\alpha$ is said to be directed by a sequence $s =
(\sigma_n)_{n \in \mathbb{N}}$ over $S$ if there is a sequence of words
$(\alpha_n)_{n \in \mathbb{N}}$ such that $\alpha_0 = \alpha$ and $\alpha_n =
\sigma_n(\alpha_{n+1})$ for all $n$; such $\alpha$ is called $S$-adic. We study
the automaton acceptance problem for such words and prove, among others, the
following. Given finite $S$ and an automaton $A$, we can compute an automaton
$B$ that accepts $s \in S^\omega$ if and only if $s$ directs a word $\alpha$
accepted by $A$. Thus we can algorithmically answer questions of the form
"Which $S$-adic words are accepted by a given automaton $A$?"

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [138] [JAX-LaB: A High-Performance, Differentiable, Lattice Boltzmann Library for Modeling Multiphase Fluid Dynamics in Geosciences and Engineering](https://arxiv.org/abs/2506.17713)
*Piyush Pradhan,Pierre Gentine,Shaina Kelly*

Main category: physics.comp-ph

TL;DR: JAX-LaB是一个基于Python的可微Lattice Boltzmann库，用于模拟水文、地质和工程多孔介质中的多相和多物理流动。它继承XLB库，利用JAX实现高性能硬件无关计算，适用于机器学习工作流，并支持CPU、GPU和分布式系统。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效、可扩展且易于与机器学习集成的多相多物理流动模拟工具，解决传统模拟方法中的密度比和接触角控制问题。

Method: 使用Shan-Chen伪势方法建模多相交互，结合状态方程和改进的强制方案，确保密度比大时模拟精度；采用改进的虚拟密度方案处理润湿问题。

Result: 通过拉普拉斯定律、毛细上升等基准测试验证了库的准确性，并展示了单GPU和多GPU的性能扩展。

Conclusion: JAX-LaB是一个开源高性能库，适用于复杂流动系统的模拟，并支持机器学习集成和多硬件平台。

Abstract: We present JAX-LaB, a differentiable, Python-based Lattice Boltzmann library
for simulating multiphase and multiphysics flows in hydrologic, geologic, and
engineered porous media. Built as an extension of the XLB library, JAX-LaB
utilizes JAX for computations and offers a performant, hardware-agnostic
implementation that integrates seamlessly with machine learning workflows and
scales efficiently across CPUs, GPUs, and distributed systems. Multiphase
interactions are modeled using the Shan-Chen pseudopotential method, which is
coupled with an equation of state and an improved forcing scheme to obtain
liquid-vapor densities that are consistent with Maxwell's construction,
enabling simulations of systems with very large density ratios while
maintaining minimal spurious currents. Wetting is handled using the "improved"
virtual density scheme, which allows precise control of contact angles and
eliminates non-physical films seen in other Shan-Chen wetting methods. We
validate the library through several analytical benchmarks, such as Laplace's
law, capillary rise, and cocurrent multicomponent flow, and demonstrate some
exemplary use cases for the library. We also report single- and multi-GPU
performance scaling of the library. The library is open-source under the Apache
license and available at https://github.com/piyush-ppradhan/JAX-LaB.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [139] [A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery](https://arxiv.org/abs/2506.17510)
*Rafael Ferreira da Silva,Milad Abolhasani,Dionysios A. Antonopoulos,Laura Biven,Ryan Coffee,Ian T. Foster,Leslie Hamilton,Shantenu Jha,Theresa Mayer,Benjamin Mintz,Robert G. Moore,Salahudin Nimer,Noah Paulson,Woong Shin,Frederic Suter,Mitra Taheri,Michela Taufer,Newell R. Washburn*

Main category: cs.CY

TL;DR: AISLE是一个自主互联的科学实验室生态网络，旨在整合分散的自治实验室资源，加速科学发现过程。


<details>
  <summary>Details</summary>
Motivation: 当前的自治实验室孤立运行，无法跨机构协作，限制了科学发现的效率和范围。

Method: AISLE通过五个关键维度实现协作：跨机构设备协调、智能数据管理、基于科学原理的AI代理驱动、互操作的代理通信接口以及AI/ML集成科学教育。

Result: AISLE能够将科学发现的时间从数十年缩短到数月，并开拓传统方法无法触及的研究领域。

Conclusion: 协作自治科学的范式转变将在可持续能源、材料开发和公共卫生等领域带来突破。

Abstract: Scientific discovery is being revolutionized by AI and autonomous systems,
yet current autonomous laboratories remain isolated islands unable to
collaborate across institutions. We present the Autonomous Interconnected
Science Lab Ecosystem (AISLE), a grassroots network transforming fragmented
capabilities into a unified system that shorten the path from ideation to
innovation to impact and accelerates discovery from decades to months. AISLE
addresses five critical dimensions: (1) cross-institutional equipment
orchestration, (2) intelligent data management with FAIR compliance, (3)
AI-agent driven orchestration grounded in scientific principles, (4)
interoperable agent communication interfaces, and (5) AI/ML-integrated
scientific education. By connecting autonomous agents across institutional
boundaries, autonomous science can unlock research spaces inaccessible to
traditional approaches while democratizing cutting-edge technologies. This
paradigm shift toward collaborative autonomous science promises breakthroughs
in sustainable energy, materials development, and public health.

</details>


### [140] [Automatic Large Language Models Creation of Interactive Learning Lessons](https://arxiv.org/abs/2506.17356)
*Jionghao Lin,Jiarui Rao,Yiyang Zhao,Yuting Wang,Ashish Gurung,Amanda Barany,Jaclyn Ocumpaugh,Ryan S. Baker,Kenneth R. Koedinger*

Main category: cs.CY

TL;DR: 研究探索了自动生成互动式场景课程，用于培训教授初中数学的线上新手导师。通过Retrieval-Augmented Generation和GPT-4o的提示工程，开发了能生成结构化课程的训练系统，评估结果显示任务分解策略优于单步生成。


<details>
  <summary>Details</summary>
Motivation: 旨在利用AI技术提升新手导师的培训效率，尤其是在远程教育中关键教学技能的培养。

Method: 采用Retrieval-Augmented Generation与GPT-4o结合的任务分解提示策略，生成课程并交由人类评估。

Result: 任务分解生成的课程评分更高，具备内容结构清晰和节省时间的优点，但反馈较泛、部分指令模糊。

Conclusion: 混合人类与AI的方法在导师培训课程生成中具有潜力，但需进一步优化细节。

Abstract: We explore the automatic generation of interactive, scenario-based lessons
designed to train novice human tutors who teach middle school mathematics
online. Employing prompt engineering through a Retrieval-Augmented Generation
approach with GPT-4o, we developed a system capable of creating structured
tutor training lessons. Our study generated lessons in English for three key
topics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior,
and Turning on Cameras, using a task decomposition prompting strategy that
breaks lesson generation into sub-tasks. The generated lessons were evaluated
by two human evaluators, who provided both quantitative and qualitative
evaluations using a comprehensive rubric informed by lesson design research.
Results demonstrate that the task decomposition strategy led to higher-rated
lessons compared to single-step generation. Human evaluators identified several
strengths in the LLM-generated lessons, including well-structured content and
time-saving potential, while also noting limitations such as generic feedback
and a lack of clarity in some instructional sections. These findings underscore
the potential of hybrid human-AI approaches for generating effective lessons in
tutor training.

</details>


### [141] [AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning](https://arxiv.org/abs/2506.17364)
*Alvaro Becerra,Roberto Daza,Ruth Cobos,Aythami Morales,Mutlu Cukurova,Julian Fierrez*

Main category: cs.CY

TL;DR: 该研究通过多模态生物特征检测智能手机使用导致的注意力分散，重点关注在线学习场景。通过生理信号和头部姿势数据，结合多种信号的多模态模型准确率达91%。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决学习者在在线学习中因内部、系统和上下文因素（如智能手机使用）导致的注意力分散问题。传统学习平台缺乏详细的行为数据，而多模态学习分析和生物传感器提供了新视角。

Method: 提出一种基于AI的方法，利用生理信号（如脑电波、心率）和头部姿势数据检测手机使用。研究比较了单一信号和多模态模型的性能。

Result: 单一生理信号（如脑电波或心率）准确性有限，而仅头部姿势数据达到87%准确率。多模态模型结合所有信号后准确率提升至91%。

Conclusion: 研究强调了多模态整合的优势，并讨论了在在线学习环境中实时部署此类模型的潜在影响和局限性。

Abstract: This work investigates the use of multimodal biometrics to detect
distractions caused by smartphone use during tasks that require sustained
attention, with a focus on computer-based online learning. Although the methods
are applicable to various domains, such as autonomous driving, we concentrate
on the challenges learners face in maintaining engagement amid internal (e.g.,
motivation), system-related (e.g., course design) and contextual (e.g.,
smartphone use) factors. Traditional learning platforms often lack detailed
behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors
provide new insights into learner attention. We propose an AI-based approach
that leverages physiological signals and head pose data to detect phone use.
Our results show that single biometric signals, such as brain waves or heart
rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal
model combining all signals reaches 91% accuracy, highlighting the benefits of
integration. We conclude by discussing the implications and limitations of
deploying these models for real-time support in online learning environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [142] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/abs/2506.17707)
*Jihyun Kim,Junho Park,Kyeongbo Kong,Suk-Ju Kang*

Main category: cs.CV

TL;DR: Programmable-Room是一个通过自然语言指令交互式生成和编辑3D房间网格的框架，结合了视觉编程和扩散模型以提高生成质量。


<details>
  <summary>Details</summary>
Motivation: 通过自然语言指令实现对3D房间网格的精确控制和编辑，从而简化用户操作。

Method: 将任务分解为多个步骤（如生成3D坐标、全景图像、构造网格等），并利用视觉编程（基于LLM生成Python类程序）和大规模扩散模型实现统一框架。

Result: 框架在生成和编辑3D房间网格方面表现出灵活性和优越性，定量和定性均优于现有模型。

Conclusion: Programmable-Room通过模块化设计和先进模型，实现了高效且高质量的3D房间生成与编辑，为相关领域提供了新的解决方案。

Abstract: We present Programmable-Room, a framework which interactively generates and
edits a 3D room mesh, given natural language instructions. For precise control
of a room's each attribute, we decompose the challenging task into simpler
steps such as creating plausible 3D coordinates for room meshes, generating
panorama images for the texture, constructing 3D meshes by integrating the
coordinates and panorama texture images, and arranging furniture. To support
the various decomposed tasks with a unified framework, we incorporate visual
programming (VP). VP is a method that utilizes a large language model (LLM) to
write a Python-like program which is an ordered list of necessary modules for
the various tasks given in natural language. We develop most of the modules.
Especially, for the texture generating module, we utilize a pretrained
large-scale diffusion model to generate panorama images conditioned on text and
visual prompts (i.e., layout, depth, and semantic map) simultaneously.
Specifically, we enhance the panorama image generation quality by optimizing
the training objective with a 1D representation of a panorama scene obtained
from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in
generating and editing 3D room meshes, and prove our framework's superiority to
an existing model quantitatively and qualitatively. Project page is available
in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [143] [PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis](https://arxiv.org/abs/2506.17912)
*Chuhao Jin,Haosen Li,Bingzi Zhang,Che Liu,Xiting Wang,Ruihua Song,Wenbing Huang,Ying Qin,Fuzheng Zhang,Di Zhang*

Main category: cs.CV

TL;DR: PlanMoGPT通过渐进式规划和流增强细粒度运动标记化，解决了文本到运动生成中的粒度问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在多模态生成任务中表现优异，但在文本到运动生成中表现不佳，主要因运动标记化的粒度问题。

Method: 提出PlanMoGPT框架，结合渐进式规划和流增强细粒度标记化，通过分层生成运动标记和优化解码器细节恢复。

Result: 在文本到运动基准测试中，FID分数提升63.8%，运动多样性增加49.9%，达到最新水平。

Conclusion: PlanMoGPT成功解决了多样性-质量的权衡问题，为文本到运动生成设定了新标准。

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in
many multimodal generation tasks, but a significant performance gap still
exists in text-to-motion generation, where LLM-based methods lag far behind
non-LLM methods. We identify the granularity of motion tokenization as a
critical bottleneck: fine-grained tokenization induces local dependency issues,
where LLMs overemphasize short-term coherence at the expense of global semantic
alignment, while coarse-grained tokenization sacrifices motion details. To
resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating
progressive planning and flow-enhanced fine-grained motion tokenization. First,
our progressive planning mechanism leverages LLMs' autoregressive capabilities
to hierarchically generate motion tokens by starting from sparse global plans
and iteratively refining them into full sequences. Second, our flow-enhanced
tokenizer doubles the downsampling resolution and expands the codebook size by
eight times, minimizing detail loss during discretization, while a
flow-enhanced decoder recovers motion nuances. Extensive experiments on
text-to-motion benchmarks demonstrate that it achieves state-of-the-art
performance, improving FID scores by 63.8% (from 0.380 to 0.141) on
long-sequence generation while enhancing motion diversity by 49.9% compared to
existing methods. The proposed framework successfully resolves the
diversity-quality trade-off that plagues current non-LLM approaches,
establishing new standards for text-to-motion generation.

</details>


### [144] [On the Robustness of Human-Object Interaction Detection against Distribution Shift](https://arxiv.org/abs/2506.18021)
*Chi Xie,Shuang Liang,Jie Li,Feng Zhu,Rui Zhao,Yichen Wei,Shengjie Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种评估和提升人-物交互（HOI）检测模型在分布偏移下稳健性的方法，包括新的自动化评估基准和两种简单有效的增强策略。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测研究忽视实际场景中的分布偏移问题，限制了模型的实用性，因此需要探索其稳健性。

Method: 提出自动化方法创建首个HOI稳健性评估基准；分析40多个现有模型的不足；通过跨域数据增强和特征融合策略提升稳健性。

Result: 实验结果表明，所提方法显著提升了多种模型的稳健性，并在标准基准测试中也表现优异。

Conclusion: 该研究填补了HOI检测稳健性评估的空白，并为实际应用提供了有效的增强策略。

Abstract: Human-Object Interaction (HOI) detection has seen substantial advances in
recent years. However, existing works focus on the standard setting with ideal
images and natural distribution, far from practical scenarios with inevitable
distribution shifts. This hampers the practical applicability of HOI detection.
In this work, we investigate this issue by benchmarking, analyzing, and
enhancing the robustness of HOI detection models under various distribution
shifts. We start by proposing a novel automated approach to create the first
robustness evaluation benchmark for HOI detection. Subsequently, we evaluate
more than 40 existing HOI detection models on this benchmark, showing their
insufficiency, analyzing the features of different frameworks, and discussing
how the robustness in HOI is different from other tasks. With the insights from
such analyses, we propose to improve the robustness of HOI detection methods
through: (1) a cross-domain data augmentation integrated with mixup, and (2) a
feature fusion strategy with frozen vision foundation models. Both are simple,
plug-and-play, and applicable to various methods. Our experimental results
demonstrate that the proposed approach significantly increases the robustness
of various methods, with benefits on standard benchmarks, too. The dataset and
code will be released.

</details>


### [145] [Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster](https://arxiv.org/abs/2506.18034)
*Fenghe Tang,Wenxin Ma,Zhiyang He,Xiaodong Tao,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 论文发现预训练的大型语言模型（LLM）层可用于处理医学图像分割任务，提出了一种名为LLM4Seg的混合结构，性能显著提升且训练参数增加极少。


<details>
  <summary>Details</summary>
Motivation: 探索预训练LLM在医学图像分割任务中的潜力，利用其语义理解能力提升分割性能。

Method: 提出LLM4Seg结构，将预训练且冻结的LLM层集成到CNN编码器-解码器分割框架中。

Result: 该方法在多种医学图像模态上提高了分割性能，且对不同LLM（如LLaMA和DeepSeek）均有效。

Conclusion: LLM的语义理解能力可有效增强医学图像分割任务，为相关研究提供了新思路。

Abstract: With the advancement of Large Language Model (LLM) for natural language
processing, this paper presents an intriguing finding: a frozen pre-trained LLM
layer can process visual tokens for medical image segmentation tasks.
Specifically, we propose a simple hybrid structure that integrates a
pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation
framework (LLM4Seg). Surprisingly, this design improves segmentation
performance with a minimal increase in trainable parameters across various
modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our
in-depth analysis reveals the potential of transferring LLM's semantic
awareness to enhance segmentation tasks, offering both improved global
understanding and better local modeling capabilities. The improvement proves
robust across different LLMs, validated using LLaMA and DeepSeek.

</details>


### [146] [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://arxiv.org/abs/2506.18866)
*Qijun Gan,Ruizi Yang,Jianke Zhu,Shaofei Xue,Steven Hoi*

Main category: cs.CV

TL;DR: OmniAvatar是一种创新的音频驱动全身视频生成模型，通过多层级音频嵌入策略和LoRA训练方法，提升了唇同步和自然动作，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动人体动画方法多关注面部动作，难以生成自然同步的全身动画，且缺乏精确提示控制。

Method: 采用像素级多层级音频嵌入策略和LoRA训练方法，结合音频特征与基础模型的提示控制能力。

Result: 在面部和半身体视频生成中表现优异，支持多领域精确文本控制。

Conclusion: OmniAvatar在音频驱动全身动画中实现了更高准确性和自然性，扩展了应用场景。

Abstract: Significant progress has been made in audio-driven human animation, while
most existing methods focus mainly on facial movements, limiting their ability
to create full-body animations with natural synchronization and fluidity. They
also struggle with precise prompt control for fine-grained generation. To
tackle these challenges, we introduce OmniAvatar, an innovative audio-driven
full-body video generation model that enhances human animation with improved
lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise
multi-hierarchical audio embedding strategy to better capture audio features in
the latent space, enhancing lip-syncing across diverse scenes. To preserve the
capability for prompt-driven control of foundation models while effectively
incorporating audio features, we employ a LoRA-based training approach.
Extensive experiments show that OmniAvatar surpasses existing models in both
facial and semi-body video generation, offering precise text-based control for
creating videos in various domains, such as podcasts, human interactions,
dynamic scenes, and singing. Our project page is
https://omni-avatar.github.io/.

</details>


### [147] [Let Your Video Listen to Your Music!](https://arxiv.org/abs/2506.18881)
*Xinyu Zhang,Dong Gong,Zicheng Duan,Anton van den Hengel,Lingqiao Liu*

Main category: cs.CV

TL;DR: 提出了一种名为MVAA的框架，能自动将视频内容与音乐节拍对齐，同时保留原视频的视觉内容，通过关键帧对齐和节奏感知的视频修复实现高效编辑。


<details>
  <summary>Details</summary>
Motivation: 解决视频与音乐节奏对齐的实际需求，现有方法依赖人工或启发式技术，灵活性不足，希望自动化并保留原视频内容。

Method: 采用两阶段框架：1) 关键帧与音乐节拍对齐；2) 基于帧条件的扩散模型生成中间帧，保持语义内容。预训练后快速微调以适应特定视频。

Result: 在单块NVIDIA 4090 GPU上10分钟内完成适配，实验显示能高质量对齐节拍并保持视觉流畅。

Conclusion: MVAA框架高效灵活，为多媒体制作提供了一种自动化解决方案，优于现有方法。

Abstract: Aligning the rhythm of visual motion in a video with a given music track is a
practical need in multimedia production, yet remains an underexplored task in
autonomous video editing. Effective alignment between motion and musical beats
enhances viewer engagement and visual appeal, particularly in music videos,
promotional content, and cinematic editing. Existing methods typically depend
on labor-intensive manual cutting, speed adjustments, or heuristic-based
editing techniques to achieve synchronization. While some generative models
handle joint video and music generation, they often entangle the two
modalities, limiting flexibility in aligning video to music beats while
preserving the full visual content. In this paper, we propose a novel and
efficient framework, termed MVAA (Music-Video Auto-Alignment), that
automatically edits video to align with the rhythm of a given music track while
preserving the original visual content. To enhance flexibility, we modularize
the task into a two-step process in our MVAA: aligning motion keyframes with
audio beats, followed by rhythm-aware video inpainting. Specifically, we first
insert keyframes at timestamps aligned with musical beats, then use a
frame-conditioned diffusion model to generate coherent intermediate frames,
preserving the original video's semantic content. Since comprehensive test-time
training can be time-consuming, we adopt a two-stage strategy: pretraining the
inpainting module on a small video set to learn general motion priors, followed
by rapid inference-time fine-tuning for video-specific adaptation. This hybrid
approach enables adaptation within 10 minutes with one epoch on a single NVIDIA
4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show
that our approach can achieve high-quality beat alignment and visual
smoothness.

</details>


### [148] [Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations](https://arxiv.org/abs/2506.18898)
*Jiaming Han,Hao Chen,Yang Zhao,Hanyu Wang,Qi Zhao,Ziyan Yang,Hao He,Xiangyu Yue,Lu Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一种多模态框架Tar，通过共享的离散语义表示统一视觉理解和生成，核心是文本对齐分词器（TA-Tok），实现了跨模态输入输出。


<details>
  <summary>Details</summary>
Motivation: 解决视觉与文本模态间的统一表示问题，提升多模态大语言模型（LLM）的效率和性能。

Method: 使用TA-Tok将图像转换为离散令牌，结合比例自适应编码解码及生成去令牌器，利用两种互补的去令牌器优化输出。

Result: 实验表明Tar在多项基准测试中表现优异，收敛更快且训练效率更高。

Conclusion: Tar框架在多模态任务中展现了高效性和性能优势，为未来研究提供了新思路。

Abstract: This paper presents a multimodal framework that attempts to unify visual
understanding and generation within a shared discrete semantic representation.
At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into
discrete tokens using a text-aligned codebook projected from a large language
model's (LLM) vocabulary. By integrating vision and text into a unified space
with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input
and output through a shared interface, without the need for modality-specific
designs. Additionally, we propose scale-adaptive encoding and decoding to
balance efficiency and visual detail, along with a generative de-tokenizer to
produce high-fidelity visual outputs. To address diverse decoding needs, we
utilize two complementary de-tokenizers: a fast autoregressive model and a
diffusion-based model. To enhance modality fusion, we investigate advanced
pre-training tasks, demonstrating improvements in both visual understanding and
generation. Experiments across benchmarks show that Tar matches or surpasses
existing multimodal LLM methods, achieving faster convergence and greater
training efficiency. Code, models, and data are available at
https://tar.csuhan.com

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [149] [Residue Number System (RNS) based Distributed Quantum Multiplication](https://arxiv.org/abs/2506.17588)
*Bhaskar Gaur,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: 该论文提出了基于残数系统（RNS）的分布式量子乘法，以降低量子乘法器的Toffoli深度和T门使用量，并设计了Quantum Diminished-1 Modulo $(2^n+1)$ Multiplier作为其关键组件。


<details>
  <summary>Details</summary>
Motivation: 量子乘法在量子算法中广泛应用，但现有量子乘法器存在高Toffoli深度和T门使用量的问题，限制了其可扩展性和实用性。

Method: 采用RNS进行分布式量子乘法，通过多个量子模乘法电路的并行执行，降低资源消耗。设计了Quantum Diminished-1 Modulo $(2^n+1)$ Multiplier作为核心组件。

Result: 与现有非分布式量子乘法器相比，Toffoli深度降低了46.018%，T门使用量减少了34.483%至86.25%。

Conclusion: 基于RNS的分布式量子乘法能显著降低资源消耗，提升量子乘法器的可扩展性和实用性。

Abstract: Multiplication of quantum states is a frequently used function or subroutine
in quantum algorithms and applications, making quantum multipliers an essential
component of quantum arithmetic. However, quantum multiplier circuits suffer
from high Toffoli depth and T gate usage, which ultimately affects their
scalability and applicability on quantum computers. To address these issues, we
propose utilizing the Residue Number System (RNS) based distributed quantum
multiplication, which executes multiple quantum modulo multiplication circuits
across quantum computers or jobs with lower Toffoli depth and T gate usage.
Towards this end, we propose a design of Quantum Diminished-1 Modulo $(2^n+1)$
Multiplier, an essential component of RNS based distributed quantum
multiplication. We provide estimates of quantum resource usage and compare them
with those of an existing non-distributed quantum multiplier for 6 to 16 qubit
sized output. Our comparative analysis estimates up to 46.018% lower Toffoli
depth, and reduction in T gates of 34.483% to 86.25%.

</details>


### [150] [Bloch Vector Assertions for Debugging Quantum Programs](https://arxiv.org/abs/2506.18458)
*Noah H. Oldfield,Christoph Laaber,Shaukat Ali*

Main category: quant-ph

TL;DR: Bloq提出了一种基于Bloch向量的断言方法，用于自动化定位量子程序中的故障，避免了手动断言和中电路测量的限制，且在噪声环境下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 量子程序调试困难，现有断言方法存在手动生成、中电路测量依赖和可扩展性差的问题。

Method: 引入Bloch-vector-based断言和AutoBloq自动生成断言方案，利用Pauli算子的期望值测量进行低开销故障定位。

Result: 在684432个程序实验中，Bloq性能优于Proq，F1得分显著提升，且减少了运行时和电路深度开销。

Conclusion: Bloq为近量子设备提供了一种可扩展且高效的断言调试方法。

Abstract: Quantum programs must be reliable to ensure trustworthy results, yet
debugging them is notoriously challenging due to quantum-specific faults like
gate misimplementations and hardware noise, as well as their inherently
probabilistic nature. Assertion-based debugging provides a promising solution
by enabling localized correctness checks during execution. However, current
approaches face challenges including manual assertion generation, reliance on
mid-circuit-measurements, and poor scalability. In this paper, we present Bloq,
a scalable, automated fault localization approach introducing
Bloch-vector-based assertions utilizing expectation value measurements of Pauli
operators, enabling low-overhead fault localization without mid-circuit
measurements. In addition, we introduce AutoBloq, a component of Bloq for
automatically generating assertion schemes from quantum algorithms. An
experimental evaluation over 684432 programs using two algorithms (Quantum
Fourier Transform (QFT) and Grover) shows that Bloq consistently outperforms
the state-of-the-art approach Proq, notably as circuit depth and noise
increase. For Grover, Bloq achieves a mean F1 score across all experimental
instances of 0.74 versus 0.38 for Proq under ideal conditions, and maintains
performance under noise (0.43 versus 0.06). Bloq also reduces Proq's runtime by
a factor of 5 and circuit depth overhead by a factor of 23. These results
underline Bloq's potential to make assertion-based debugging scalable and
effective for near-term quantum devices.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [151] [Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation](https://arxiv.org/abs/2506.17442)
*Hao Guan,David Bates,Li Zhou*

Main category: cs.AI

TL;DR: 这是一篇关于医疗人工智能系统性能退化问题的综述，探讨了监控和维护AI系统健康的方法。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统在临床决策中发挥着重要作用，但随着时间的推移，其性能可能因数据分布变化、患者特征改变等因素而下降，影响系统的可靠性和安全性。

Method: 文章回顾了性能退化的常见原因，总结了检测数据漂移和模型漂移的技术，并深入分析了根本原因，同时探讨了从模型重新训练到测试时适应的校正策略。

Result: 综述涵盖了传统机器学习模型和现代大型语言模型（LLMs），分析了它们的优缺点，并提出了应对性能退化的技术方法。

Conclusion: 文章指出了当前的技术挑战，并提出了未来研究方向，旨在指导开发可靠、稳健的医疗AI系统，以应对动态临床环境中的长期需求。

Abstract: Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

</details>


### [152] [Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition](https://arxiv.org/abs/2506.17792)
*Alexandros Evangelidis,Gricel Vázquez,Simos Gerasimou*

Main category: cs.AI

TL;DR: 该论文提出了一种动态优化马尔可夫决策过程（MDP）的策略合成方法，通过迭代选择最脆弱区域进行细化，显著提高了大规模MDP的计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统策略合成方法无法有效处理大规模MDP状态空间，亟需一种兼顾准确性和效率的解决方案。

Method: 动态细化MDP，迭代选择最脆弱区域进行优化，仅在必要时进行细化。

Result: 在包含多种案例和高达1M状态的MDP测试中，性能提升显著，比PRISM快多达2倍。

Conclusion: 该方法为大规模MDP中的实际策略合成任务提供了高效且具有竞争力的解决方案。

Abstract: Software-intensive systems, such as software product lines and robotics,
utilise Markov decision processes (MDPs) to capture uncertainty and analyse
sequential decision-making problems. Despite the usefulness of conventional
policy synthesis methods, they fail to scale to large state spaces. Our
approach addresses this issue and accelerates policy synthesis in large MDPs by
dynamically refining the MDP and iteratively selecting the most fragile MDP
regions for refinement. This iterative procedure offers a balance between
accuracy and efficiency, as refinement occurs only when necessary. Through a
comprehensive empirical evaluation comprising diverse case studies and MDPs up
to 1M states, we demonstrate significant performance improvements yielded by
our approach compared to the leading probabilistic model checker PRISM (up to
2x), thus offering a very competitive solution for real-world policy synthesis
tasks in larger MDPs.

</details>


### [153] [A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction](https://arxiv.org/abs/2506.18424)
*Chengjie Liu,Weiyu Chen,Huiyao Xu,Yuan Du,Jun Yang,Li Du*

Main category: cs.AI

TL;DR: 提出一种基于LLM的多agent框架，用于从学术论文中提取模拟电路的尺寸关系，从而有效修剪搜索空间，提高优化效率。


<details>
  <summary>Details</summary>
Motivation: 在模拟电路预布局阶段，传统方法忽略先验知识的自动引入，导致搜索空间压缩不足，影响优化效率。

Method: 提出基于大语言模型的多agent框架，从学术论文中提取电路的尺寸关系，用于修剪搜索空间。

Result: 在3种电路上测试，优化效率提高2.32至26.6倍。

Conclusion: LLM能有效修剪模拟电路尺寸的搜索空间，为LLM与传统自动化设计方法的结合提供新思路。

Abstract: In the design process of the analog circuit pre-layout phase, device sizing
is an important step in determining whether an analog circuit can meet the
required performance metrics. Many existing techniques extract the circuit
sizing task as a mathematical optimization problem to solve and continuously
improve the optimization efficiency from a mathematical perspective. But they
ignore the automatic introduction of prior knowledge, fail to achieve effective
pruning of the search space, which thereby leads to a considerable compression
margin remaining in the search space. To alleviate this problem, we propose a
large language model (LLM)-based multi-agent framework for analog circuits'
sizing relationships extraction from academic papers. The search space in the
sizing process can be effectively pruned based on the sizing relationship
extracted by this framework. Eventually, we conducted tests on 3 types of
circuits, and the optimization efficiency was improved by $2.32 \sim 26.6
\times$. This work demonstrates that the LLM can effectively prune the search
space for analog circuit sizing, providing a new solution for the combination
of LLMs and conventional analog circuit design automation methods.

</details>


### [154] [T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent](https://arxiv.org/abs/2506.18559)
*Hong Qing Yu*

Main category: cs.AI

TL;DR: 提出了一种名为T-CPDL的新框架，结合时间、因果和概率推理，显著提升了语言模型在结构推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在时间约束、因果关系和概率推理等结构化推理任务中的不足。

Method: 扩展描述逻辑，引入时间区间操作符、显式因果关系和概率标注，提出两种T-CPDL变体。

Result: 在时间推理和因果推断基准测试中，T-CPDL显著提升了推理准确性、可解释性和置信度校准。

Conclusion: T-CPDL增强了语言模型的透明度与可信度，并为Logic-RAG框架的开发奠定了基础。

Abstract: Large language models excel at generating fluent text but frequently struggle
with structured reasoning involving temporal constraints, causal relationships,
and probabilistic reasoning. To address these limitations, we propose Temporal
Causal Probabilistic Description Logic (T-CPDL), an integrated framework that
extends traditional Description Logic with temporal interval operators,
explicit causal relationships, and probabilistic annotations. We present two
distinct variants of T-CPDL: one capturing qualitative temporal relationships
through Allen's interval algebra, and another variant enriched with explicit
timestamped causal assertions. Both variants share a unified logical structure,
enabling complex reasoning tasks ranging from simple temporal ordering to
nuanced probabilistic causation. Empirical evaluations on temporal reasoning
and causal inference benchmarks confirm that T-CPDL substantially improves
inference accuracy, interpretability, and confidence calibration of language
model outputs. By delivering transparent reasoning paths and fine-grained
temporal and causal semantics, T-CPDL significantly enhances the capability of
language models to support robust, explainable, and trustworthy
decision-making. This work also lays the groundwork for developing advanced
Logic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially
boosting the reasoning capabilities and efficiency of knowledge graph-enhanced
RAG systems.

</details>


### [155] [Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms](https://arxiv.org/abs/2506.17900)
*Cheng Ji,Huaiying Luo*

Main category: cs.AI

TL;DR: 论文提出基于大语言模型（LLM）的智能日志处理与自动调试框架LLM-ID，通过多阶段语义推理机制提升故障定位准确率16.2%。


<details>
  <summary>Details</summary>
Motivation: 解决云平台AI系统日志数据量大、非结构化和语义模糊导致的故障定位与自修复难题。

Method: 扩展预训练Transformer模型，结合多阶段语义推理、无监督聚类与嵌入机制、多轮注意力机制及强化学习策略引导的恢复规划器。

Result: 在云平台日志数据集上实验表明，LLM-ID故障定位准确率提升16.2%，优于现有主流方法。

Conclusion: LLM-ID具备更强的语义理解、持续学习和异构环境适应能力，为云环境调试提供有效解决方案。

Abstract: With the increasing complexity and rapid expansion of the scale of AI systems
in cloud platforms, the log data generated during system operation is massive,
unstructured, and semantically ambiguous, which brings great challenges to
fault location and system self-repair. In order to solve this problem, this
paper proposes an intelligent log processing and automatic debugging framework
based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This
method is extended on the basis of the existing pre-trained Transformer model,
and integrates a multi-stage semantic inference mechanism to realize the
context understanding of system logs and the automatic reconstruction of fault
chains. Firstly, the system log is dynamically structured, and the unsupervised
clustering and embedding mechanism is used to extract the event template and
semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round
attention mechanism to perform contextual reasoning on the log sequence to
generate potential fault assumptions and root cause paths. Furthermore, this
paper introduces a reinforcement learning-based policy-guided recovery planner,
which is driven by the remediation strategy generated by LLM to support dynamic
decision-making and adaptive debugging in the cloud environment. Compared with
the existing rule engine or traditional log analysis system, the proposed model
has stronger semantic understanding ability, continuous learning ability and
heterogeneous environment adaptability. Experiments on the cloud platform log
dataset show that LLM-ID improves the fault location accuracy by 16.2%, which
is significantly better than the current mainstream methods

</details>


### [156] [Reflective Verbal Reward Design for Pluralistic Alignment](https://arxiv.org/abs/2506.17834)
*Carter Blair,Kate Larson,Edith Law*

Main category: cs.AI

TL;DR: 论文提出了一种个性化的奖励建模方法，通过语言模型引导用户反思对话，以解决传统RLHF中单一奖励模型忽视少数偏好的问题。


<details>
  <summary>Details</summary>
Motivation: 人类价值观具有多样性，传统RLHF方法通过单一奖励模型聚合反馈可能压制少数偏好，因此需要更个性化的解决方案。

Method: 使用语言模型引导用户进行反思对话，记录个性化对话历史，并以此作为语言模型的输入，构建个性化的“语言奖励模型”。

Result: 在30名参与者的实验中，该方法比非反思性语言奖励模型准确率提高9-12%，且样本效率更高。

Conclusion: 个性化奖励建模方法能更有效地捕捉多样化的人类价值观，优于传统方法。

Abstract: AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [157] [VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM](https://arxiv.org/abs/2506.17506)
*Lesheng Jin,Zhenyuan Ruan,Haohui Mai,Jingbo Shang*

Main category: cs.CL

TL;DR: VeriLocc结合大语言模型（LLMs）与形式编译器技术，实现跨GPU架构的可验证寄存器分配，性能优于人工调优库。


<details>
  <summary>Details</summary>
Motivation: 现代GPU快速发展，但生产编译器仍依赖人工调优的寄存器分配启发式方法，需针对每代硬件重新调优。

Method: VeriLocc通过微调LLM将中间表示（MIRs）转换为目标寄存器分配，辅以静态分析和验证引导的再生循环。

Result: 在GEMM和MHA任务中，VeriLocc单次准确率85-99%，pass@100接近100%，性能比rocBLAS高10%以上。

Conclusion: VeriLocc提供了一种通用且可验证的寄存器分配方法，性能超越专家调优库。

Abstract: Modern GPUs evolve rapidly, yet production compilers still rely on
hand-crafted register allocation heuristics that require substantial re-tuning
for each hardware generation. We introduce VeriLocc, a framework that combines
large language models (LLMs) with formal compiler techniques to enable
generalizable and verifiable register allocation across GPU architectures.
VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)
into target-specific register assignments, aided by static analysis for
cross-architecture normalization and generalization and a verifier-guided
regeneration loop to ensure correctness. Evaluated on matrix multiplication
(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot
accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more
performant assignments than expert-tuned libraries, outperforming rocBLAS by
over 10% in runtime.

</details>


### [158] [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/abs/2506.17314)
*Adnan Qidwai,Srija Mukhopadhyay,Prerana Khatiwada,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: PRAISE系统利用LLMs自动从客户评价和卖家描述中提取、对比和结构化信息，帮助提升电商产品描述的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 改善电商平台上产品描述的不足，利用客户评价中的有价值信息，提升买卖双方的体验。

Method: 使用大型语言模型（LLMs）自动提取并结构化客户评价和卖家描述中的信息，提供界面展示差异。

Result: PRAISE能够生成结构化的、可操作的见解，显著提升产品目录的质量和可信度。

Conclusion: PRAISE通过自动化处理客户评价，有效解决了产品描述不完善的问题，具有广泛的应用潜力。

Abstract: Accurate and complete product descriptions are crucial for e-commerce, yet
seller-provided information often falls short. Customer reviews offer valuable
details but are laborious to sift through manually. We present PRAISE: Product
Review Attribute Insight Structuring Engine, a novel system that uses Large
Language Models (LLMs) to automatically extract, compare, and structure
insights from customer reviews and seller descriptions. PRAISE provides users
with an intuitive interface to identify missing, contradictory, or partially
matching details between these two sources, presenting the discrepancies in a
clear, structured format alongside supporting evidence from reviews. This
allows sellers to easily enhance their product listings for clarity and
persuasiveness, and buyers to better assess product reliability. Our
demonstration showcases PRAISE's workflow, its effectiveness in generating
actionable structured insights from unstructured reviews, and its potential to
significantly improve the quality and trustworthiness of e-commerce product
catalogs.

</details>


### [159] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)
*Weixin Liang*

Main category: cs.CL

TL;DR: 该论文探讨了大语言模型（LLMs）的社会影响，包括AI检测器的偏见、LLMs在各领域的普及及其在科研反馈中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何改变写作、沟通和创作方式，并探索个人和机构如何适应这一新兴技术。

Method: 通过三个研究方向：分析AI检测器的偏见、量化LLMs在各领域的采用情况、评估LLMs在科研反馈中的潜力。

Result: 发现AI检测器存在系统性偏见、LLMs在多个领域广泛使用、LLMs能为科研提供有效反馈。

Conclusion: LLMs的普及引发公平问题，但同时也为科研支持提供了新机会，需谨慎管理其应用。

Abstract: Large language models (LLMs) have shown significant potential to change how
we write, communicate, and create, leading to rapid adoption across society.
This dissertation examines how individuals and institutions are adapting to and
engaging with this emerging technology through three research directions.
First, I demonstrate how the institutional adoption of AI detectors introduces
systematic biases, particularly disadvantaging writers of non-dominant language
varieties, highlighting critical equity concerns in AI governance. Second, I
present novel population-level algorithmic approaches that measure the
increasing adoption of LLMs across writing domains, revealing consistent
patterns of AI-assisted content in academic peer reviews, scientific
publications, consumer complaints, corporate communications, job postings, and
international organization press releases. Finally, I investigate LLMs'
capability to provide feedback on research manuscripts through a large-scale
empirical analysis, offering insights into their potential to support
researchers who face barriers in accessing timely manuscript feedback,
particularly early-career researchers and those from under-resourced settings.

</details>


### [160] [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)
*Bushra Asseri,Estabrag Abdelaziz,Areej Al-Wabil*

Main category: cs.CL

TL;DR: 本文探讨了通过提示工程减少大型语言模型中对阿拉伯人和穆斯林的文化偏见的方法，总结了五种主要策略及其效果。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型表现出强大能力，但其对阿拉伯人和穆斯林的文化偏见引发了伦理问题，且缺乏专门针对这一群体的偏见缓解研究。

Method: 采用混合方法的系统文献综述，分析了2021-2024年间8项关于偏见缓解策略的实证研究。

Result: 研究发现五种提示工程方法可有效减少偏见，其中结构化多步流程效果最佳（偏见减少达87.7%），但文化提示更易于实施。

Conclusion: 提示工程虽有效，但需进一步开发文化适应性技术和特定评价资源，并结合其他去偏方法以维持模型实用性。

Abstract: Large language models have demonstrated remarkable capabilities across
various domains, yet concerns about cultural bias - particularly towards Arabs
and Muslims - pose significant ethical challenges by perpetuating harmful
stereotypes and marginalization. Despite growing recognition of bias in LLMs,
prompt engineering strategies specifically addressing Arab and Muslim
representation remain understudied. This mixed-methods systematic review
examines such techniques, offering evidence-based guidance for researchers and
practitioners. Following PRISMA guidelines and Kitchenham's systematic review
methodology, we analyzed 8 empirical studies published between 2021-2024
investigating bias mitigation strategies. Our findings reveal five primary
prompt engineering approaches: cultural prompting, affective priming,
self-debiasing techniques, structured multi-step pipelines, and
parameter-optimized continuous prompts. Although all approaches show potential
for reducing bias, effectiveness varied substantially across studies and bias
types. Evidence suggests that certain bias types may be more resistant to
prompt-based mitigation than others. Structured multi-step pipelines
demonstrated the highest overall effectiveness, achieving up to 87.7% reduction
in bias, though they require greater technical expertise. Cultural prompting
offers broader accessibility with substantial effectiveness. These results
underscore the accessibility of prompt engineering for mitigating cultural bias
without requiring access to model parameters. The limited number of studies
identified highlights a significant research gap in this critical area. Future
research should focus on developing culturally adaptive prompting techniques,
creating Arab and Muslim-specific evaluation resources, and integrating prompt
engineering with complementary debiasing methods to address deeper stereotypes
while maintaining model utility.

</details>


### [161] [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/abs/2506.18201)
*Bushra Asseri,Estabraq Abdelaziz,Maha Al Mogren,Tayef Alhefdhi,Areej Al-Wabil*

Main category: cs.CL

TL;DR: 该研究评估了GPT-4o和Gemini 1.5 Pro两种多模态大语言模型在识别阿拉伯儿童故事书插图中情感的能力，发现GPT-4o在性能上优于Gemini，但也揭示了模型在文化理解上的局限性和对模糊叙事的处理困难。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语言背景下缺乏文化敏感的、能够识别情感的教育技术工具，这促使研究者探索多模态AI系统在情感识别方面的表现。

Method: 使用75张阿拉伯故事书插图，通过三种提示策略（零样本、少样本和思维链）评估GPT-4o和Gemini 1.5 Pro的情感识别能力，并与基于Plutchik情感框架的人工标注进行对比。

Result: GPT-4o在所有测试条件下表现优于Gemini，其最佳宏F1得分为59%，而Gemini为43%。模型在文化细节情感和模糊叙事中的表现较差。

Conclusion: 当前模型在文化理解方面存在根本性局限，需要更文化敏感的培训方法来开发适用于阿拉伯学习者的情感识别教育技术。

Abstract: Emotion recognition capabilities in multimodal AI systems are crucial for
developing culturally responsive educational technologies, yet remain
underexplored for Arabic language contexts where culturally appropriate
learning tools are critically needed. This study evaluates the emotion
recognition performance of two advanced multimodal large language models,
GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook
illustrations. We assessed both models across three prompting strategies
(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic
storybooks, comparing model predictions with human annotations based on
Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across
all conditions, achieving the highest macro F1-score of 59% with
chain-of-thought prompting compared to Gemini's best performance of 43%. Error
analysis revealed systematic misclassification patterns, with valence
inversions accounting for 60.7% of errors, while both models struggled with
culturally nuanced emotions and ambiguous narrative contexts. These findings
highlight fundamental limitations in current models' cultural understanding and
emphasize the need for culturally sensitive training approaches to develop
effective emotion-aware educational technologies for Arabic-speaking learners.

</details>
