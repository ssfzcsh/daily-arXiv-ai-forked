<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 24]
- [cs.GR](#cs.GR) [Total: 9]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini](https://arxiv.org/abs/2508.04820)
*Mayra Sofia Ruiz Rodriguez,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 研究了GPT-4o mini在机器学习项目中生成文件级日志的能力，发现其日志位置准确率为63.91%，但过度日志率高达82.66%，并揭示了文件级日志生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在机器学习项目中生成文件级日志的潜力，以提升系统可靠性。

Method: 收集171个ML仓库的4,073个Python文件，移除原日志后使用LLM生成日志，并评估日志位置、级别、变量和文本质量。

Result: LLM生成日志的位置准确率为63.91%，但过度日志率为82.66%；手动分析揭示了文件级日志的挑战。

Conclusion: LLM在生成文件级日志方面有潜力，但仍需解决过度日志和对代码块和项目规范的适应性问题。

Abstract: Logging is essential in software development, helping developers monitor
system behavior and aiding in debugging applications. Given the ability of
large language models (LLMs) to generate natural language and code, researchers
are exploring their potential to generate log statements. However, prior work
focuses on evaluating logs introduced in code functions, leaving file-level log
generation underexplored -- especially in machine learning (ML) applications,
where comprehensive logging can enhance reliability. In this study, we evaluate
the capacity of GPT-4o mini as a case study to generate log statements for ML
projects at file level. We gathered a set of 171 ML repositories containing
4,073 Python files with at least one log statement. We identified and removed
the original logs from the files, prompted the LLM to generate logs for them,
and evaluated both the position of the logs and log level, variables, and text
quality of the generated logs compared to human-written logs. In addition, we
manually analyzed a representative sample of generated logs to identify common
patterns and challenges. We find that the LLM introduces logs in the same place
as humans in 63.91% of cases, but at the cost of a high overlogging rate of
82.66%. Furthermore, our manual analysis reveals challenges for file-level
logging, which shows overlogging at the beginning or end of a function,
difficulty logging within large code blocks, and misalignment with
project-specific logging conventions. While the LLM shows promise for
generating logs for complete files, these limitations remain to be addressed
for practical implementation.

</details>


### [2] [Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models](https://arxiv.org/abs/2508.04895)
*Wentao Lu,Alexander Senchenko,Abram Hindle,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 论文提出了一种自动化流程，通过将游戏bug视频缩减为最能匹配bug描述的单帧图像，显著减少开发者的手动检查工作。


<details>
  <summary>Details</summary>
Motivation: 游戏工作室频繁发布新版本和补丁，产生大量bug报告和视频，手动审查效率低且难以扩展。

Method: 使用FFmpeg提取关键帧，结合视觉-语言模型（GPT-4o）匹配bug描述并选择最具代表性的帧。

Result: 方法在真实游戏数据中F1得分为0.79，准确率为0.89，在光照、物理和UI类bug上表现最佳。

Conclusion: 自动化流程能大幅减少手动工作，提升bug分类和回归检查效率，对游戏行业QA团队和开发者具有实际价值。

Abstract: Modern game studios deliver new builds and patches at a rapid pace,
generating thousands of bug reports, many of which embed gameplay videos. To
verify and triage these bug reports, developers must watch the submitted
videos. This manual review is labour-intensive, slow, and hard to scale. In
this paper, we introduce an automated pipeline that reduces each video to a
single frame that best matches the reported bug description, giving developers
instant visual evidence that pinpoints the bug.
  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video
to a median of just 1.90% of its original frames while still capturing bug
moments in 98.79 of cases. These keyframes are then evaluated by a
vision--language model (GPT-4o), which ranks them based on how well they match
the textual bug description and selects the most representative frame. We
evaluated this approach using real-world developer-submitted gameplay videos
and JIRA bug reports from a popular First-Person Shooter (FPS) game. The
pipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the
top-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =
0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and
lowest for Animation & VFX (0.51).
  By replacing video viewing with an immediately informative image, our
approach dramatically reduces manual effort and speeds up triage and regression
checks, offering practical benefits to quality assurance (QA) teams and
developers across the game industry.

</details>


### [3] [Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities](https://arxiv.org/abs/2508.04921)
*Zixuan Feng,Reed Milewicz,Emerson Murphy-Hill,Tyler Menezes,Alexander Serebrenik,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: 开源软件社区面临生成式AI带来的不确定性，本文通过社会技术框架探讨其潜在风险和机遇。


<details>
  <summary>Details</summary>
Motivation: 生成式AI可能颠覆开源软件的开发、维护和治理方式，社区需应对由此带来的复杂性和模糊性。

Method: 采用受McLuhan的Tetrad启发的社会技术框架，通过情景驱动的方法进行概念性探索。

Result: 在软件实践、文档、社区参与和治理四个领域，识别了生成式AI对开源社区的风险与机遇。

Conclusion: 通过主动采用这一框架，开源社区可以更好地应对技术变革，而非被动适应。

Abstract: Open Source Software communities face a wave of uncertainty as Generative AI
rapidly transforms how software is created, maintained, and governed. Without
clear frameworks, communities risk being overwhelmed by the complexity and
ambiguity introduced by GenAI, threatening the collaborative ethos that
underpins OSS. We conduct a scenario-driven, conceptual exploration using a
socio-technical framework inspired by McLuhan's Tetrad to surface both risks
and opportunities for community resilience amid GenAI-driven disruption of OSS
development across four domains: software practices, documentation, community
engagement, and governance. By adopting this lens, OSS leaders and researchers
can proactively shape the future of their ecosystems, rather than simply
reacting to technological upheaval.

</details>


### [4] [Taxonomy of Faults in Attention-Based Neural Networks](https://arxiv.org/abs/2508.04925)
*Sigma Jahan,Saurabh Singh Rajput,Tushar Sharma,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: 论文分析了基于注意力的神经网络（ABNNs）中的故障，提出了七个新的故障类别，并提供了诊断启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习故障分类未能充分捕捉注意力机制引入的独特故障，导致缺乏可操作的诊断指导。

Method: 通过系统性分析555个来自96个项目的真实故障数据，开发了一个新的故障分类法。

Result: 研究表明，超过一半的ABNN故障源于注意力架构的独特性，并提出了四个诊断启发式方法。

Conclusion: 研究填补了注意力机制故障诊断的空白，为实践者提供了系统化的指导。

Abstract: Attention mechanisms are at the core of modern neural architectures, powering
systems ranging from ChatGPT to autonomous vehicles and driving a major
economic impact. However, high-profile failures, such as ChatGPT's nonsensical
outputs or Google's suspension of Gemini's image generation due to attention
weight errors, highlight a critical gap: existing deep learning fault
taxonomies might not adequately capture the unique failures introduced by
attention mechanisms. This gap leaves practitioners without actionable
diagnostic guidance. To address this gap, we present the first comprehensive
empirical study of faults in attention-based neural networks (ABNNs). Our work
is based on a systematic analysis of 555 real-world faults collected from 96
projects across ten frameworks, including GitHub, Hugging Face, and Stack
Overflow. Through our analysis, we develop a novel taxonomy comprising seven
attention-specific fault categories, not captured by existing work. Our results
show that over half of the ABNN faults arise from mechanisms unique to
attention architectures. We further analyze the root causes and manifestations
of these faults through various symptoms. Finally, by analyzing symptom-root
cause associations, we identify four evidence-based diagnostic heuristics that
explain 33.0% of attention-specific faults, offering the first systematic
diagnostic guidance for attention-based models.

</details>


### [5] [Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic](https://arxiv.org/abs/2508.05005)
*Gang Xu,Airong Wang,Yushan Pan*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）与面向对象编程（OOP）的交叉领域，提出了如何通过LLMs提升OOP学习和代码编写的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs如何在OOP中发挥作用的理解有限，研究旨在填补这一空白，探索LLMs在编程流程中的关键应用点。

Method: 从程序员、新手和有经验者的视角出发，识别编码流程中LLMs可以提供显著帮助的关键节点，并提出增强逻辑推理和代码编写的方法。

Result: 提出了LLMs在OOP任务中的潜在应用点，并展示了如何提升编程体验。

Conclusion: 研究为LLMs与OOP的结合提供了初步框架，为未来研究和工具开发指明了方向。

Abstract: We find ourselves in the midst of an explosion in artificial intelligence
research, particularly with large language models (LLMs). These models have
diverse applications spanning finance, commonsense knowledge graphs, medicine,
and visual analysis. In the world of Object-Oriented Programming(OOP), a robust
body of knowledge and methods has been developed for managing complex tasks
through object-oriented thinking. However, the intersection of LLMs with OOP
remains an underexplored territory. Empirically, we currently possess limited
understanding of how LLMs can enhance the effectiveness of OOP learning and
code writing, as well as how we can evaluate such AI-powered tools. Our work
aims to address this gap by presenting a vision from the perspectives of key
stakeholders involved in an OOP task: programmers, mariners, and experienced
programmers. We identify critical junctures within typical coding workflows
where the integration of LLMs can offer significant benefits. Furthermore, we
propose ways to augment existing logical reasoning and code writing, ultimately
enhancing the programming experience.

</details>


### [6] [An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack](https://arxiv.org/abs/2508.05034)
*Arabat,Ali,Sayagh,Mohammed,Hassine,Jameleddine*

Main category: cs.SE

TL;DR: 论文提出了一种半自动化方法，利用两个机器学习模型来帮助开发者提前识别软件变更之间的依赖关系，以解决大规模软件系统中依赖管理的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性增加，准确识别和管理变更之间的依赖关系变得至关重要，但现有的依赖管理方法效率低下，常导致构建失败或功能不完整。

Method: 通过初步研究OpenStack系统，发现依赖关系多在代码审查阶段识别，耗时较长。作者提出两个ML模型：一个预测变更间依赖的可能性，另一个识别具体的依赖对。

Result: 模型表现良好，平均AUC分别为79.33%和91.89%，Brier得分为0.11和0.014。第二个模型在识别具体依赖对时具有较高的召回率，但精确度有待提升。

Conclusion: 论文提出的半自动化方法能有效帮助开发者提前识别依赖关系，减少延迟和错误，尤其适用于大规模分布式系统的开发和管理。

Abstract: As software systems grow in complexity, accurately identifying and managing
dependencies among changes becomes increasingly critical. For instance, a
change that leverages a function must depend on the change that introduces it.
Establishing such dependencies allows CI/CD pipelines to build and orchestrate
changes effectively, preventing build failures and incomplete feature
deployments. In modern software systems, dependencies often span multiple
components across teams, creating challenges for development and deployment.
They serve various purposes, from enabling new features to managing
configurations, and can even involve traditionally independent changes like
documentation updates. To address these challenges, we conducted a preliminary
study on dependency management in OpenStack, a large-scale software system. Our
study revealed that a substantial portion of software changes in OpenStack over
the past 10 years are interdependent. Surprisingly, 51.08% of these
dependencies are identified during the code review phase-after a median delay
of 5.06 hours-rather than at the time of change creation. Developers often
spend a median of 57.12 hours identifying dependencies, searching among a
median of 463 other changes. To help developers proactively identify
dependencies, we propose a semi-automated approach that leverages two ML
models. The first model predicts the likelihood of dependencies among changes,
while the second identifies the exact pairs of dependent changes. Our proposed
models demonstrate strong performance, achieving average AUC scores of 79.33%
and 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the
second model has a good top-k recall across all types of pairs, while the top-k
precision has room for improvement.

</details>


### [7] [LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps](https://arxiv.org/abs/2508.05085)
*Junayed Mahmud,James Chen,Terry Achille,Camilo Alvarez-Velez,Darren Dean Bansil,Patrick Ijieh,Samar Karanch,Nadeeshan De Silva,Oscar Chaparro,Andrian Marcus,Kevin Moran*

Main category: cs.SE

TL;DR: LadyBug是一个GitHub机器人，通过结合UI交互信息和文本检索，自动定位Android应用的bug。


<details>
  <summary>Details</summary>
Motivation: 提高Android应用bug定位的准确性和效率。

Method: 结合bug报告的文本描述和开发者上传的UI交互跟踪信息，生成可能包含bug的文件排名列表。

Result: 在RedWing基准测试中，LadyBug优于基于文本检索的基线方法，UI信息显著提升了定位准确性。

Conclusion: LadyBug是一个有效的开源工具，能显著提升Android应用bug定位的准确性。

Abstract: This paper introduces LadyBug, a GitHub bot that automatically localizes bugs
for Android apps by combining UI interaction information with text retrieval.
LadyBug connects to an Android app's GitHub repository, and is triggered when a
bug is reported in the corresponding issue tracker. Developers can then record
a reproduction trace for the bug on a device or emulator and upload the trace
to LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both
the text from the original bug description, and UI information from the
reproduction trace to accurately retrieve a ranked list of files from the
project that most likely contain the reported bug.
  We empirically evaluated LadyBug using an automated testing pipeline and
benchmark called RedWing that contains 80 fully-localized and reproducible bug
reports from 39 Android apps. Our results illustrate that LadyBug outperforms
text-retrieval-based baselines and that the utilization of UI information leads
to a substantial increase in localization accuracy. LadyBug is an open-source
tool, available at https://github.com/LadyBugML/ladybug.
  A video showing the capabilities of Ladybug can be viewed here:
https://youtu.be/hI3tzbRK0Cw

</details>


### [8] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: 本文提出了一种统一的框架，通过评估和改进中间推理过程来提升强化学习在代码生成中的效果，包括开发新基准、奖励模型和RL方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在代码生成中依赖测试结果奖励，忽视了推理过程质量，容易导致奖励滥用问题。通过监督推理过程可提升效果。

Method: 开发了LCB-RB基准和OD-based奖励模型训练方法；提出了P-GRPO RL方法，结合过程奖励与任务成功条件。

Result: 7B参数模型在代码生成任务中表现优异，超越基线4.5%，接近GPT-4-Turbo，并在数学任务中验证了泛化性。

Conclusion: 框架有效结合推理质量与任务成功，显著提升性能，公开模型和数据推动进一步研究。

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


### [9] [AI-assisted JSON Schema Creation and Mapping](https://arxiv.org/abs/2508.05192)
*Felix Neubauer,Jürgen Pleiss,Benjamin Uekermann*

Main category: cs.SE

TL;DR: 论文提出了一种结合大型语言模型（LLM）和确定性技术的混合方法，用于通过自然语言输入创建和修改JSON Schema，并集成到开源工具MetaConfigurator中，降低了非专家进行结构化数据建模和数据整合的门槛。


<details>
  <summary>Details</summary>
Motivation: 许多领域缺乏标准化模型，且创建模型对非专家来说是一个重大障碍。

Method: 采用LLM与确定性技术结合的混合方法，支持自然语言输入生成JSON Schema和模式映射，并集成到MetaConfigurator工具中。

Result: 通过化学领域的应用示例验证了方法的可行性，显著降低了非专家的使用门槛。

Conclusion: 结合自然语言交互与确定性保障，有效提升了非专家在结构化数据建模和数据整合中的能力。

Abstract: Model-Driven Engineering (MDE) places models at the core of system and data
engineering processes. In the context of research data, these models are
typically expressed as schemas that define the structure and semantics of
datasets. However, many domains still lack standardized models, and creating
them remains a significant barrier, especially for non-experts. We present a
hybrid approach that combines large language models (LLMs) with deterministic
techniques to enable JSON Schema creation, modification, and schema mapping
based on natural language inputs by the user. These capabilities are integrated
into the open-source tool MetaConfigurator, which already provides visual model
editing, validation, code generation, and form generation from models. For data
integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and
YAML data using LLMs, while ensuring scalability and reliability through
deterministic execution of generated mapping rules. The applicability of our
work is demonstrated in an application example in the field of chemistry. By
combining natural language interaction with deterministic safeguards, this work
significantly lowers the barrier to structured data modeling and data
integration for non-experts.

</details>


### [10] [STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning](https://arxiv.org/abs/2508.05193)
*Kaiwen Yan,Yuhang Chang,Zirui Guo,Yaling Mou,Jiang Ming,Jingwei Sun*

Main category: cs.SE

TL;DR: 提出了一个新的基准测试SX-Bench，用于评估大型语言模型在复杂多函数理解和细粒度执行推理方面的能力，弥补了现有基准在功能和推理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准如HumanEval和MBPP主要关注功能正确性，而CRUXEVAL等推理基准仅适用于单函数、低复杂度场景，导致高级模型的评分饱和，缺乏区分能力。

Method: 设计了SX-Bench基准，任务涉及多个子函数的协作（如链式调用、嵌套循环），以“计算步骤”为最小执行单位，要求模型预测推理任务的总步骤数。

Result: 在20多种主流模型上的实验显示，SX-Bench具有高区分度，即使是OpenAI-O3在Hard-Reasoning任务上准确率也仅为78.37%，揭示了复杂推理的瓶颈。

Conclusion: SX-Bench将代码评估从“单函数验证”推进到“多函数动态推理”，为深入评估高级代码智能模型提供了关键工具。

Abstract: In recent years, large language models (LLMs) have made significant progress
in code intelligence, yet systematically evaluating their code understanding
and reasoning abilities remains challenging. Mainstream benchmarks such as
HumanEval and MBPP primarily assess functional correctness, while reasoning
benchmarks like CRUXEVAL are limited to single-function, low-complexity
scenarios. As a result, advanced models achieve nearly saturated scores,
limiting their discriminative power. To address this, we present
STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex
multi-function understanding and fine-grained execution reasoning. SX-Bench
features tasks involving collaboration among multiple sub-functions (e.g.,
chained calls, nested loops), shifting evaluation towards overall control and
data flow modeling. It defines "computation steps" as the minimal execution
unit and requires models to predict the total number of steps in reasoning
tasks, thereby assessing a model's in-depth understanding of dynamic execution
beyond simple I/O matching. Evaluation on over 20 mainstream models (including
14 reasoning-enhanced models) demonstrates that SX-Bench is highly
discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent
accuracy on Hard-Reasoning tasks, much lower than its saturated scores on
previous benchmarks, thereby revealing bottlenecks in complex and fine-grained
reasoning. We also release an automated pipeline combining program synthesis,
symbolic execution, and LLM-aided validation for efficient benchmark generation
and quality assurance. SX-Bench advances code evaluation from "single-function
verification" to "multi-function dynamic reasoning," providing a key tool for
the in-depth assessment of advanced code intelligence models.

</details>


### [11] [EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0](https://arxiv.org/abs/2508.05199)
*Igor Costa,Christopher Baran*

Main category: cs.SE

TL;DR: EvoGraph是一个通过图结构和学习突变操作实现软件系统自我进化的框架，在多任务中表现优异，显著提升效率和降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统软件现代化过程中面临的漏洞修复、语言迁移和文档维护等问题，同时提出一种适应性强且高效的自动化进化方法。

Method: 将软件组件表示为类型有向图，利用小型语言模型驱动突变操作，并通过多目标适应性选择进化方向。

Result: 在安全漏洞修复（83%）、COBOL到Java转换（93%功能对等）和文档维护（两分钟内更新）等任务中表现优异，计算成本降低90%。

Conclusion: EvoGraph为软件3.0时代提供了可测量控制的持续自适应路径，适用于多种语言和任务。

Abstract: We introduce **EvoGraph**, a framework that enables software systems to
evolve their own source code, build pipelines, documentation, and tickets.
EvoGraph represents every artefact in a typed directed graph, applies learned
mutation operators driven by specialized small language models (SLMs), and
selects survivors with a multi-objective fitness. On three benchmarks, EvoGraph
fixes 83% of known security vulnerabilities, translates COBOL to Java with 93%
functional equivalence (test verified), and maintains documentation freshness
within two minutes. Experiments show a 40% latency reduction and a sevenfold
drop in feature lead time compared with strong baselines. We extend our
approach to **evoGraph**, leveraging language-specific SLMs for modernizing
.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%
semantic equivalence across languages while reducing computational costs by 90%
compared to large language models. EvoGraph's design responds to empirical
failure modes in legacy modernization, such as implicit contracts, performance
preservation, and integration evolution. Our results suggest a practical path
toward Software 3.0, where systems adapt continuously yet remain under
measurable control.

</details>


### [12] [A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes](https://arxiv.org/abs/2508.05301)
*Victoria Torres Bosch,Ronny Seiger,Manuela Albert Albiol,Antoni Mestre Gascon,Pedro Jose Valderas Aranda*

Main category: cs.SE

TL;DR: 该论文提出了一个概念模型和方法论，旨在利用物联网（IoT）衡量和改进业务流程（BPs）的可持续性，超越环境维度，实现全面影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注业务流程管理（BPM）中的可持续性环境维度，但缺乏对可持续发展的系统全面分析。物联网的实时数据收集和自动化能力为业务流程的可持续性改进提供了新机遇。

Method: 提出了一个概念模型，将BPM与物联网联系起来，展示了物联网设备如何支持可持续性。同时，设计了一个结构化方法，用于分析现有业务流程，识别改进机会并实施支持可持续性的物联网增强业务流程。

Result: 论文通过旅游领域的示例和医疗保健案例研究验证了所提方法的实用性。

Conclusion: 概念模型和方法论为业务流程的可持续性改进提供了系统化工具，展示了物联网在实现全面可持续发展中的潜力。

Abstract: The real-time data collection and automation capabilities offered by the
Internet of Things (IoT) are revolutionizing and transforming Business
Processes (BPs) into IoT-enhanced BPs, showing high potential for improving
sustainability. Although already studied in Business Process Management (BPM),
sustainability research has primarily focused on environmental concerns.
However, achieving a holistic and lasting impact requires a systematic approach
to address sustainability beyond the environmental dimension. This work
proposes a conceptual model and a structured methodology with the goal of
analyzing the potential of IoT to measure and improve the sustainability of
BPs. The conceptual model formally represents key sustainability concepts,
linking BPM and IoT by highlighting how IoT devices support and contribute to
sustainability. The methodology guides the systematic analysis of existing BPs,
identifies opportunities, and implements sustainability-aware, IoT-enhanced
BPs. The approach is illustrated through a running example from the tourism
domain and a case study in healthcare.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: 本文提出了首个保证混合模式更新一致性的算法，基于服务行为的语义特性（如交换性），并通过理论框架证明了其正确性。


<details>
  <summary>Details</summary>
Motivation: 现有微服务架构在动态更新时面临混合模式不一致的挑战，传统方法要么资源开销大，要么存在风险。

Method: 提出基于服务行为语义特性的算法，并结合理论框架验证其一致性和原子性。

Result: 证明了语义感知是避免不一致的必要条件，并实现了高效的混合模式更新。

Conclusion: 通过语义感知和理论框架，解决了微服务动态更新中的一致性问题，为实际应用提供了可靠的方法。

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [14] [Mapping Sparse Triangular Solves to GPUs via Fine-grained Domain Decomposition](https://arxiv.org/abs/2508.04917)
*Atharva Gondhalekar,Kjetil Haugen,Thomas Gibson,Wu-chun Feng*

Main category: cs.PF

TL;DR: 该论文通过细粒度域分解优化GPU上的稀疏三角求解，虽略微增加迭代次数，但实现了显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 稀疏线性系统的预处理迭代方法中，稀疏三角求解因内存访问不规则和数据依赖成为瓶颈，作者希望通过GPU架构优化解决这一问题。

Method: 提出一种细粒度域分解策略，生成非重叠子域，每个子域分配给线程块，子域向量保存在GPU共享内存中，减少同步和全局内存访问。

Result: 与ROCm™软件栈的现有实现相比，三角求解加速10.7倍，ILU0预处理的BiCGSTAB求解器加速3.2倍。

Conclusion: 细粒度域分解方法显著提升了GPU上稀疏三角求解的效率，为预处理迭代方法提供了高效实现。

Abstract: Sparse linear systems are typically solved using preconditioned iterative
methods, but applying preconditioners via sparse triangular solves introduces
bottlenecks due to irregular memory accesses and data dependencies. This work
leverages fine-grained domain decomposition to adapt triangular solves to the
GPU architecture. We develop a fine-grained domain decomposition strategy that
generates non-overlapping subdomains, increasing parallelism in the application
of preconditioner at the expense of a modest increase in the iteration count
for convergence. Each subdomain is assigned to a thread block and is sized such
that the subdomain vector fits in the GPU shared memory, eliminating the need
for inter-block synchronization and reducing irregular global memory accesses.
Compared to other state-of-the-art implementations using the ROCm$^{\text{TM}}$
software stack, we achieve a 10.7$\times$ speedup for triangular solves and a
3.2$\times$ speedup for the ILU0-preconditioned biconjugate gradient stabilized
(BiCGSTAB) solver on the AMD Instinct$^{\text{TM}}$ MI210 GPU.

</details>


### [15] [Back to Bits: Extending Shannon's communication performance framework to computing](https://arxiv.org/abs/2508.05621)
*Max Hawkins,Richard Vuduc*

Main category: cs.PF

TL;DR: 提出了一种基于信息论的性能评估单元，适用于现代多样化计算系统。


<details>
  <summary>Details</summary>
Motivation: 传统性能指标（如浮点运算）已无法准确反映现代计算系统的复杂性，需新方法衡量信息处理效能。

Method: 通过信息论框架，将计算视为信息在信道中的转换，以输入输出间的互信息定义性能。

Result: 该框架提供了一个与实现无关的理论基础，能够量化计算中编码、操作和保留的有意义信息。

Conclusion: 新方法为评估多样化计算系统的性能提供了更准确的衡量标准。

Abstract: This work proposes a novel computing performance unit grounded in information
theory. Modern computing systems are increasingly diverse, supporting
low-precision formats, hardware specialization, and emerging paradigms such as
analog, quantum, and reversible logic. Traditional metrics like floating-point
operations (flops) no longer accurately capture this complexity. We frame
computing as the transformation of information through a channel and define
performance in terms of the mutual information between a system's inputs and
outputs. This approach measures not just the quantity of data processed, but
the amount of meaningful information encoded, manipulated, and retained through
computation. Our framework provides a principled, implementation-agnostic
foundation for evaluating performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [TeraRIS NOMA-MIMO Communications for 6G and Beyond Industrial Networks](https://arxiv.org/abs/2508.05130)
*Ali Raza,Muhammad Farhan Khan,Zeeshan Alam,Muhammad Saad,Ilyas Saleem,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Main category: cs.NI

TL;DR: 本文提出了一个联合框架，将可重构智能表面（RIS）与太赫兹（THz）通信和非正交多址（NOMA）结合，以增强智能工业通信。通过两种功率分配策略优化系统性能，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了满足未来6G网络及更高需求中的工业自动化和实时通信要求，提升频谱效率、覆盖范围和可靠性。

Method: 设计了RIS辅助的NOMA MIMO框架，并研究了两种功率分配策略：优化近距离和远距离工业节点的功率分配，以及根据网络需求优先分配功率。

Result: 在30 dBm下，相比固定功率分配，系统实现了23%的总速率增益，并通过仿真验证了理论分析。

Conclusion: RIS辅助的NOMA MIMO框架在THz工业通信中表现出高效性和鲁棒性，是未来6G网络的潜在解决方案。

Abstract: This paper presents a joint framework that integrates reconfigurable
intelligent surfaces (RISs) with Terahertz (THz) communications and
non-orthogonal multiple access (NOMA) to enhance smart industrial
communications. The proposed system leverages the advantages of RIS and THz
bands to improve spectral efficiency, coverage, and reliability key
requirements for industrial automation and real-time communications in future
6G networks and beyond. Within this framework, two power allocation strategies
are investigated: the first optimally distributes power between near and far
industrial nodes, and the second prioritizes network demands to enhance system
performance further. A performance evaluation is conducted to compare the sum
rate and outage probability against a fixed power allocation scheme. Our scheme
achieves up to a 23% sum rate gain over fixed PA at 30 dBm. Simulation results
validate the theoretical analysis, demonstrating the effectiveness and
robustness of the RIS-assisted NOMA MIMO framework for THz enabled industrial
communications.

</details>


### [17] [Modular Design and Experimental Evaluation of 5G Mobile Cell Architectures Based on Overlay and Integrated Models](https://arxiv.org/abs/2508.05249)
*José Ruela,Ivan Cojocaru,André Coelho,Rui Campos,Manuel Ricardo*

Main category: cs.NI

TL;DR: 这篇论文介绍了5G移动单元（MC）的概念、架构设计和性能评估，用于为缺乏固定5G基础设施或无线条件较差的地区提供无线连接。


<details>
  <summary>Details</summary>
Motivation: 解决5G覆盖不足或无线条件差的区域连接问题。

Method: 采用两种MC设计模型（覆盖模型和IAB模型），并通过OAI测试床进行性能验证。

Result: MC的部署位置显著影响网络性能，验证了其有效性。

Conclusion: MC有望为临时扩展覆盖和增强容量提供解决方案，适用于港口、工业场景和公共安全等领域。

Abstract: This paper presents the concept, architectural design, and performance
evaluation of a 5G Mobile Cell (MC) used to provide 5G wireless connectivity to
User Equipment (UE) in areas with limited fixed 5G infrastructures or subject
to adverse radio conditions. We consider two main approaches to MC design: an
overlay model, where the MC obtains backhaul connectivity from a 5G overlay
network, and an Integrated Access and Backhaul (IAB)-based model, discussing
their protocol stacks and architectural implications. In order to validate the
MC's performance, we employ an emulation-based testbed using the
OpenAirInterface (OAI) implementation, considering different MC positions. The
results validate the MC concept and demonstrate that MC positioning
significantly influences network performance. This paper has the potential to
aid network operators and service providers in selecting and deploying MC
architectures for temporary coverage extension and capacity reinforcement in
different environments, including seaports, industrial scenarios, and public
safety.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [18] [JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering](https://arxiv.org/abs/2508.05087)
*Renmiao Chen,Shiyao Cui,Xuancheng Huang,Chengwei Pan,Victor Shea-Jay Huang,QingLin Zhang,Xuan Ouyang,Zhexin Zhang,Hongning Wang,Minlie Huang*

Main category: cs.MM

TL;DR: 论文提出了JPS方法，通过视觉扰动和文本引导协同优化，提高了多模态大语言模型(Jailbreak攻击的成功率和恶意意图实现率。


<details>
  <summary>Details</summary>
Motivation: 当前的研究主要关注攻击成功率（ASR），但忽视了生成的响应是否真正满足攻击者的恶意意图，导致输出质量低下。

Method: JPS方法结合了目标引导的视觉图像扰动和由多智能体系统优化的‘引导提示’，并通过迭代协同优化提升性能。

Result: 实验证明，JPS在ASR和恶意意图实现率（MIFR）上均达到最新水平，适用于多种MLLM和基准测试。

Conclusion: JPS通过视觉和文本协同攻击，显著提升了攻击效果，同时提出了MIFR指标来评估攻击成果的质量。

Abstract: Jailbreak attacks against multimodal large language Models (MLLMs) are a
significant research focus. Current research predominantly focuses on
maximizing attack success rate (ASR), often overlooking whether the generated
responses actually fulfill the attacker's malicious intent. This oversight
frequently leads to low-quality outputs that bypass safety filters but lack
substantial harmful content. To address this gap, we propose JPS,
\underline{J}ailbreak MLLMs with collaborative visual \underline{P}erturbation
and textual \underline{S}teering, which achieves jailbreaks via corporation of
visual image and textually steering prompt. Specifically, JPS utilizes
target-guided adversarial image perturbations for effective safety bypass,
complemented by "steering prompt" optimized via a multi-agent system to
specifically guide LLM responses fulfilling the attackers' intent. These visual
and textual components undergo iterative co-optimization for enhanced
performance. To evaluate the quality of attack outcomes, we propose the
Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a
Reasoning-LLM-based evaluator. Our experiments show JPS sets a new
state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with
analyses confirming its efficacy. Codes are available at
\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}.
\color{warningcolor}{Warning: This paper contains potentially sensitive
contents.}

</details>


### [19] [Embedding Alignment in Code Generation for Audio](https://arxiv.org/abs/2508.05473)
*Sam Kouteili,Hiren Madhu,George Typaldos,Mark Santolucito*

Main category: cs.MM

TL;DR: 论文探讨了如何通过分析代码与音频嵌入空间的关系，提升LLM在创意编码（如实时编码）中的多样性输出，并提出了预测模型。


<details>
  <summary>Details</summary>
Motivation: 在创意编码中，用户需要多样化的代码候选以实现音乐意图，但现有LLM生成的代码缺乏多样性，且无法直接关联音频输出。

Method: 研究了代码与音频嵌入空间的拓扑关系，构建了预测模型以学习嵌入对齐映射。

Result: 发现代码与音频嵌入并非简单线性关系，但可以通过模型学习到对齐映射。

Conclusion: 提出的模型能预测代码对应的音频嵌入，有助于提升音乐输出的多样性。

Abstract: LLM-powered code generation has the potential to revolutionize creative
coding endeavors, such as live-coding, by enabling users to focus on structural
motifs over syntactic details. In such domains, when prompting an LLM, users
may benefit from considering multiple varied code candidates to better realize
their musical intentions. Code generation models, however, struggle to present
unique and diverse code candidates, with no direct insight into the code's
audio output. To better establish a relationship between code candidates and
produced audio, we investigate the topology of the mapping between code and
audio embedding spaces. We find that code and audio embeddings do not exhibit a
simple linear relationship, but supplement this with a constructed predictive
model that shows an embedding alignment map could be learned. Supplementing the
aim for musically diverse output, we present a model that given code predicts
output audio embedding, constructing a code-audio embedding alignment map.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [20] [AI Should Be More Human, Not More Complex](https://arxiv.org/abs/2508.04713)
*Carlo Esposito*

Main category: cs.HC

TL;DR: 用户更喜欢简洁且引用的AI回答，而非复杂冗长的解释。研究表明，过度追求AI的“人工复杂化”会降低用户信任。


<details>
  <summary>Details</summary>
Motivation: 当前AI搜索应用中，复杂冗长的回应降低了用户满意度和参与度，研究旨在找出最优的AI沟通方式。

Method: 通过比较10,000名参与者对五个主要AI搜索系统的反馈，分析用户偏好。

Result: 用户明显偏好简洁且引用来源的回应，复杂回答会导致用户信任下降和认知负担增加。

Conclusion: AI的沟通应直接、引用来源并诚实面对局限，简洁和透明才是提高用户体验和可靠性的关键。

Abstract: Large Language Models (LLMs) in search applications increasingly prioritize
verbose, lexically complex responses that paradoxically reduce user
satisfaction and engagement. Through a comprehensive study of 10.000 (est.)
participants comparing responses from five major AI-powered search systems, we
demonstrate that users overwhelmingly prefer concise, source-attributed
responses over elaborate explanations. Our analysis reveals that current AI
development trends toward "artificial sophistication" create an uncanny valley
effect where systems sound knowledgeable but lack genuine critical thinking,
leading to reduced trust and increased cognitive load. We present evidence that
optimal AI communication mirrors effective human discourse: direct, properly
sourced, and honest about limitations. Our findings challenge the prevailing
assumption that more complex AI responses indicate better performance, instead
suggesting that human-like brevity and transparency are key to user engagement
and system reliability.

</details>


### [21] [Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts](https://arxiv.org/abs/2508.04787)
*Vishnu Menon,Andy Cherney,Elizabeth B. Cloude,Li Zhang,Tiffany D. Do*

Main category: cs.HC

TL;DR: 嵌入LLM引导的反思提示在AI播客中对学习和用户体验的影响研究，结果显示反思提示降低了吸引力。


<details>
  <summary>Details</summary>
Motivation: 探讨反思提示在AI生成播客中对学习和用户体验的影响。

Method: 36名本科生参与实验，比较有无反思提示的版本。

Result: 学习效果相近，但反思提示降低了用户的感知吸引力。

Conclusion: 需进一步研究反思交互设计。

Abstract: This study examined whether embedding LLM-guided reflection prompts in an
interactive AI-generated podcast improved learning and user experience compared
to a version without prompts. Thirty-six undergraduates participated, and while
learning outcomes were similar across conditions, reflection prompts reduced
perceived attractiveness, highlighting a call for more research on reflective
interactivity design.

</details>


### [22] [At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp](https://arxiv.org/abs/2508.04821)
*Yang Liu,Thorbjørn Mikkelsen,Zehai Liu,Gengchen Tian,Diako Mardanbegi,Qiushi Zhou,Hans Gellersen,Ken Pfeuffer*

Main category: cs.HC

TL;DR: SightWarp是一种利用眼手协调的新交互技术，通过生成远距离对象的近空间代理，支持直接手势操作，显著提升3D界面中的操作效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D用户界面中远距离对象交互的不便性，尤其是间接技术（如凝视和捏合）缺乏直接手势的即时性和本体反馈。

Method: 引入SightWarp技术，通过用户注视和手部动作触发，生成远距离对象的缩放近空间代理，支持直接手势操作。

Result: 用户研究显示，SightWarp易于使用，且直接操作性能优于凝视和捏合技术。

Conclusion: SightWarp扩展了3D界面中远近空间对象的交互灵活性，提升了用户体验。

Abstract: In 3D user interfaces, reaching out to grab and manipulate something works
great until it is out of reach. Indirect techniques like gaze and pinch offer
an alternative for distant interaction, but do not provide the same immediacy
or proprioceptive feedback as direct gestures. To support direct gestures for
faraway objects, we introduce SightWarp, an interaction technique that exploits
eye-hand coordination to seamlessly summon object proxies to the user's
fingertips. The idea is that after looking at a distant object, users either
shift their gaze to the hand or move their hand into view-triggering the
creation of a scaled near-space proxy of the object and its surrounding
context. The proxy remains active until the eye-hand pattern is released. The
key benefit is that users always have an option to immediately operate on the
distant object through a natural, direct hand gesture. Through a user study of
a 3D object docking task, we show that users can easily employ SightWarp, and
that subsequent direct manipulation improves performance over gaze and pinch.
Application examples illustrate its utility for 6DOF manipulation,
overview-and-detail navigation, and world-in-miniature interaction. Our work
contributes to expressive and flexible object interactions across near and far
spaces.

</details>


### [23] [Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction](https://arxiv.org/abs/2508.04842)
*Amit Kumar Das,Mohammad Tarun,Klaus Mueller*

Main category: cs.HC

TL;DR: 该论文评估了现代大语言模型（LLMs）的可视化素养，并引入了一种名为Charts-of-Thought的新提示技术，显著提升了模型在可视化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs在可视化任务中的能力，并通过结构化提示方法提升其表现，同时为视觉障碍或低可视化素养的人群提供潜在帮助。

Method: 方法是使用标准提示和Charts-of-Thought技术测试三种LLMs（Claude-3.7-sonnet、GPT-4.5 preview和Gemini-2.0-pro）在可视化素养评估测试（VLAT）中的表现。

Result: 结果显示Charts-of-Thought显著提升了所有模型的得分，其中Claude-3.7-sonnet以50.17分远超人类基准28.82分。

Conclusion: 结论证明现代多模态LLMs在结构化提示下能超越人类可视化素养表现，为LLMs的可视化能力设定了新基准。

Abstract: This paper evaluates the visualization literacy of modern Large Language
Models (LLMs) and introduces a novel prompting technique called
Charts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet,
GPT-4.5 preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment
Test (VLAT) using standard prompts and our structured approach. The
Charts-of-Thought method guides LLMs through a systematic data extraction,
verification, and analysis process before answering visualization questions.
Our results show Claude-3.7-sonnet achieved a score of 50.17 using this method,
far exceeding the human baseline of 28.82. This approach improved performance
across all models, with score increases of 21.8% for GPT-4.5, 9.4% for
Gemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The
performance gains were consistent across original and modified VLAT charts,
with Claude correctly answering 100% of questions for several chart types that
previously challenged LLMs. Our study reveals that modern multimodal LLMs can
surpass human performance on visualization literacy tasks when given the proper
analytical framework. These findings establish a new benchmark for LLM
visualization literacy and demonstrate the importance of structured prompting
strategies for complex visual interpretation tasks. Beyond improving LLM
visualization literacy, Charts-of-Thought could also enhance the accessibility
of visualizations, potentially benefiting individuals with visual impairments
or lower visualization literacy.

</details>


### [24] [An Implementation of a Visual Stepper in the GRASP Programming System](https://arxiv.org/abs/2508.04859)
*Panicz Maciej Godek*

Main category: cs.HC

TL;DR: 本文介绍了GRASP编程系统中可视化评估器扩展的实现方法，并围绕GRASP的设计及其扩展机制架构提供了教程。


<details>
  <summary>Details</summary>
Motivation: 展示GRASP系统中可视化评估器扩展的实现，同时探讨GRASP设计中的关键问题和扩展机制架构。

Method: 描述GRASP系统及其扩展机制的设计与实现。

Result: 提供了GRASP系统的当前设计细节，尽管尚未完成，但指出了需要解决的关键问题。

Conclusion: GRASP的设计和扩展机制对Scheme社区可能具有重要参考价值。

Abstract: The direct purpose of this paper - as its title suggests - is to present how
the visual evaluator extension is implemented in the GRASP programming system.
The indirect purpose is to provide a tutorial around the design of GRASP, and
in particular - around the architecture of its extension mechanism. Neither
GRASP nor its extension mechanisms are, at the moment of writing this paper,
final or complete, and we are certain that some details of the solutions
described in here will change even before the first release. What will not
change, though, is the set of problems that need to be solved in order to build
a system with capabilities similar to those of GRASP. We believe that these
problems might be of interest to the Scheme community.

</details>


### [25] [Learning AI Auditing: A Case Study of Teenagers Auditing a Generative AI Model](https://arxiv.org/abs/2508.04902)
*Luis Morales-Navarro,Michelle Gan,Evelyn Yu,Lauren Vogelstein,Yasmin B. Kafai,Danaé Metaxa*

Main category: cs.HC

TL;DR: 该研究探讨了青少年如何通过算法审计识别和理解日常AI/ML工具的偏见，并展示了他们在参与式设计工作坊中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML技术日益融入青少年的生活，培养他们的AI素养（包括技术知识和社会影响意识）变得尤为重要。

Method: 研究者组织了一场为期两周的参与式设计工作坊，14名14-15岁青少年审计了TikTok的生成AI模型Effect House。

Result: 青少年表现出高度的参与性和创造力，独立发现了专业审计中较少关注的年龄偏见问题，且与研究者的结论相似。

Conclusion: 研究表明，算法审计可以作为培养青少年AI素养的有效活动，同时为他们提供批判性审视AI系统的能力，并为算法偏见研究带来新视角。

Abstract: This study investigates how high school-aged youth engage in algorithm
auditing to identify and understand biases in artificial intelligence and
machine learning (AI/ML) tools they encounter daily. With AI/ML technologies
being increasingly integrated into young people's lives, there is an urgent
need to equip teenagers with AI literacies that build both technical knowledge
and awareness of social impacts. Algorithm audits (also called AI audits) have
traditionally been employed by experts to assess potential harmful biases, but
recent research suggests that non-expert users can also participate
productively in auditing. We conducted a two-week participatory design workshop
with 14 teenagers (ages 14-15), where they audited the generative AI model
behind TikTok's Effect House, a tool for creating interactive TikTok filters.
We present a case study describing how teenagers approached the audit, from
deciding what to audit to analyzing data using diverse strategies and
communicating their results. Our findings show that participants were engaged
and creative throughout the activities, independently raising and exploring new
considerations, such as age-related biases, that are uncommon in professional
audits. We drew on our expertise in algorithm auditing to triangulate their
findings as a way to examine if the workshop supported participants to reach
coherent conclusions in their audit. Although the resulting number of changes
in race, gender, and age representation uncovered by the teens were slightly
different from ours, we reached similar conclusions. This study highlights the
potential for auditing to inspire learning activities to foster AI literacies,
empower teenagers to critically examine AI systems, and contribute fresh
perspectives to the study of algorithmic harms.

</details>


### [26] [Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept](https://arxiv.org/abs/2508.04904)
*Yuqi Hu,Qiwen Xiong,Zhenzhen Qin,Brandon Watanabe,Yujing Wang,Mirjana Prpa,Ilmi Yoon*

Main category: cs.HC

TL;DR: AI驱动的3D模拟游戏，帮助医疗专业人员通过互动模拟提升根因分析（RCA）技能，替代高资源需求的传统培训。


<details>
  <summary>Details</summary>
Motivation: 现有RCA培训资源需求高，导致培训不足和实施不一致，需要更高效、可扩展的解决方案。

Method: 开发AI支持的3D模拟游戏，模拟ICU死亡事件的RCA调查，通过虚拟角色互动、LLM驱动的对话及AI反馈机制。

Result: 原型系统结合LLM、情感文本转语音和AI动画，实现自然互动并提供反馈，支持学习者持续改进。

Conclusion: 计划实证评估系统效果，探索其在医疗培训中的潜在价值。

Abstract: Root Cause Analysis (RCA) is a critical tool for investigating adverse events
in healthcare and improving patient safety. However, existing RCA training
programs are often limited by high resource demands, leading to insufficient
training and inconsistent implementation. To address this challenge, we present
an AI-powered 3D simulation game that helps healthcare professionals develop
RCA skills through interactive, immersive simulations. This approach offers a
cost-effective, scalable, and accessible alternative to traditional training.
The prototype simulates an RCA investigation following a death in the ICU,
where learners interview five virtual avatars representing ICU team members to
investigate the incident and complete a written report. The system enables
natural, life-like interactions with avatars via large language models (LLMs),
emotional text-to-speech, and AI-powered animations. An additional LLM
component provides formative and summative feedback to support continual
improvement. We conclude by outlining plans to empirically evaluate the
system's efficacy.

</details>


### [27] [Toward Supporting Narrative-Driven Data Exploration: Barriers and Design Opportunities](https://arxiv.org/abs/2508.04920)
*Oliver Huang,Carolina Nobre*

Main category: cs.HC

TL;DR: 研究探讨了分析师在叙事驱动探索中的数据挑战，提出了设计机会以改善上下文维护和推理路径追踪。


<details>
  <summary>Details</summary>
Motivation: 分析师在探索数据时，问题不断深化和变化，静态仪表板和预定义指标已无法满足需求，导致见解分散，难以维护上下文和理清结论来源。

Method: 通过48名参与者的形成性研究，识别了叙事驱动探索中的关键障碍。

Result: 研究发现，主要障碍包括跨视图的上下文维护困难、推理路径追踪不足以及解释演变的外部化问题。

Conclusion: 研究为支持叙事驱动分析提供了设计机会，有助于更好地维护上下文和追踪推理路径。

Abstract: Analysts increasingly explore data through evolving, narrative-driven
inquiries, moving beyond static dashboards and predefined metrics as their
questions deepen and shift. As these explorations progress, insights often
become dispersed across views, making it challenging to maintain context or
clarify how conclusions arise. Through a formative study with 48 participants,
we identify key barriers that hinder narrative-driven exploration, including
difficulty maintaining context across views, tracing reasoning paths, and
externalizing evolving interpretations. Our findings surface design
opportunities to support narrative-driven analysis better.

</details>


### [28] [Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge](https://arxiv.org/abs/2508.04995)
*Matthew Kelly*

Main category: cs.HC

TL;DR: 该论文提出了Situated Epistemic Infrastructures（SEI）框架，用于分析后连贯性条件下混合人机系统中知识的权威性如何形成，强调协调而非分类。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如ChatGPT）暴露了当代知识基础设施的脆弱性，因为它们通过模拟连贯性绕过了传统的引用、权威和验证模式。作者旨在探讨如何在人机混合系统中建立知识的权威性。

Method: 提出SEI框架，整合基础设施研究、平台理论和认识论，强调协调机制而非分类，并通过追踪制度、计算和时间安排中的可信度中介来分析问题。

Result: SEI框架为学术交流提供了一个非表征主义的替代模型，适用于AI治理、知识生产和信息系统的伦理设计。

Conclusion: SEI框架为后连贯性条件下的知识权威性分析提供了诊断工具，强调了适应性和预见性的认知管理模型的重要性。

Abstract: Large Language Models (LLMs) such as ChatGPT have rendered visible the
fragility of contemporary knowledge infrastructures by simulating coherence
while bypassing traditional modes of citation, authority, and validation. This
paper introduces the Situated Epistemic Infrastructures (SEI) framework as a
diagnostic tool for analyzing how knowledge becomes authoritative across hybrid
human-machine systems under post-coherence conditions. Rather than relying on
stable scholarly domains or bounded communities of practice, SEI traces how
credibility is mediated across institutional, computational, and temporal
arrangements. Integrating insights from infrastructure studies, platform
theory, and epistemology, the framework foregrounds coordination over
classification, emphasizing the need for anticipatory and adaptive models of
epistemic stewardship. The paper contributes to debates on AI governance,
knowledge production, and the ethical design of information systems by offering
a robust alternative to representationalist models of scholarly communication.

</details>


### [29] [Human-AI Schema Discovery and Application for Creative Problem Solving](https://arxiv.org/abs/2508.05045)
*Sitong Wang*

Main category: cs.HC

TL;DR: 研究提出了一个人类与AI共同发现和应用模式（schema）的框架，以支持创造性问题解决，旨在使隐性知识更易获取和应用。


<details>
  <summary>Details</summary>
Motivation: 在复杂或陌生领域中，人们常依赖结构性模式（schema）来组织思路，但这些模式难以发现和应用。

Method: 设计系统，支持用户通过示例抽象出模式，并将模式应用于人机协同创作的工作流中。

Result: 框架使隐性知识更透明、协作更高效，推动了人机系统的进步。

Conclusion: 该研究为模式驱动的交互提供了新方法，提升了人机共创的透明性和协作性。

Abstract: Humans often rely on underlying structural patterns-schemas-to create,
whether by writing stories, designing software, or composing music. Schemas
help organize ideas and guide exploration, but they are often difficult to
discover and apply, especially in complex or unfamiliar domains. My Ph.D.
research develops a framework for human-AI schema discovery and application to
support creative problem solving. I design systems that support users in
sensemaking over examples to abstract schemas, and in operationalizing schemas
into human-AI co-creative workflows for application. This research offers
insights into how schema-guided interaction can make implicit knowledge more
accessible and actionable, advancing more transparent and collaborative
human-AI systems.

</details>


### [30] [Accessibility Beyond Accommodations: A Systematic Redesign of Introduction to Computer Science for Students with Visual Impairments](https://arxiv.org/abs/2508.05056)
*Vaanee Tripathi,Aalok Thakkar*

Main category: cs.HC

TL;DR: 该论文提出一个框架，重新设计计算机科学入门课程，以支持视障学生，强调系统性而非零散的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究为视障学生开发的编程工具和辅助技术零散且技术复杂，未能融入课程设计，造成教育支持的不全面。

Method: 提出一个五部分框架，包括可访问学习资源、课堂学习工具包、结构化支持系统、在线工具库和心理支持，基于通用设计原则并通过专家咨询验证。

Result: 框架为视障学生提供了无需复杂技术基础设施的公平学习体验，并适合标准大学环境。

Conclusion: 该框架通过技术和方法论的结合，为全面教育支持提供了模板，未来需进一步实证评估。

Abstract: Computer science education has evolved extensively; however, systemic
barriers still prevent students with visual impairments from fully
participating. While existing research has developed specialized programming
tools and assistive technologies, these solutions remain fragmented and often
require complex technical infrastructure, which limits their classroom
implementation. Current approaches treat accessibility as individual
accommodations rather than integral curriculum design, creating gaps in
holistic educational support. This paper presents a comprehensive framework for
redesigning introductory computer science curricula to provide equitable
learning experiences for students with visual impairments without requiring
specialized technical infrastructure. The framework outlines five key
components that together contribute a systematic approach to curriculum
accessibility: accessible learning resources with pre-distributed materials and
tactile diagrams, in-class learning kits with hands-on demonstrations,
structured support systems with dedicated teaching assistance, an online tool
repository, and psychosocial support for classroom participation. Unlike
existing tool-focused solutions, this framework addresses both technical and
pedagogical dimensions of inclusive education while emphasizing practical
implementation in standard university settings. The design is grounded in
universal design principles and validated through expert consultation with
accessibility specialists and disability services professionals, establishing
foundations for future empirical evaluation of learning outcomes and student
engagement while serving as a template for broader institutional adoption.

</details>


### [31] [A Desktop-Centric Design Space for Direct Object Examination and Visualization in Mixed-Reality Environments](https://arxiv.org/abs/2508.05088)
*Sam Johnson-Lacoss,Santiago V. Lombeyda,S. George Djorgovski*

Main category: cs.HC

TL;DR: 论文探讨了混合现实（MR）技术如何通过沉浸式3D界面提升科研和临床工作中的数据分析和空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 随着MR技术的轻量化、高分辨率和普及化，研究科学家和临床医生需要从传统的2D界面过渡到沉浸式3D环境，以更好地理解3D现象和患者病理。

Method: 提出了一种设计空间，将MR中的交互区域和模式分解，以优化桌面物理空间内对象为中心的精确数据分析应用。

Result: MR技术能够提供沉浸式3D表示，增强对物理结构、空间关系以及2D数据的3D上下文理解。

Conclusion: MR技术为科研和临床工作提供了更高效的数据分析和交互方式，未来有望成为标准工具。

Abstract: Mixed reality (MR) environments are bound to become ubiquitous as MR
technology becomes lighter, higher resolution, more affordable, and overall
becomes a seamless extension of our current work and living spaces. For
research scientists and clinicians focused on understanding 3D phenomena or
patient pathologies within the context of the larger human anatomy, that means
a necessary evolution of their workstations currently only utilizing 2D
interfaces for everyday communication, logistics and data analysis. MR
technologies bring forth immersive 3D representations coexisting in our natural
spaces, while allowing for richer interconnected information displays, where 3D
representations greatly aid in the detailed understanding of physical
structures, spatial relationships, and 3D contextualization of 2D measurements,
projections, abstractions, and other data details. We present a breakdown of
the different interaction zones and modalities into a design space that best
accommodates the creation of applications for users engaged through MR
technologies in precise object-centric data analysis within the ergonomic
confines of their desktop physical spaces.

</details>


### [32] [SparseEMG: Computational Design of Sparse EMG Layouts for Sensing Gestures](https://arxiv.org/abs/2508.05098)
*Anand Kumar,Antony Albert Raj Irudayaraj,Ishita Chandra,Adwait Sharma,Aditya Shekhar Nittala*

Main category: cs.HC

TL;DR: 研究了电极选择和分类器对EMG手势识别准确性和稀疏性的影响，提出了一种减少电极数量并保持准确性的方法，并开发了SparseEMG设计工具。


<details>
  <summary>Details</summary>
Motivation: 现有工具多关注模型开发，忽视了电极选择对分类准确性的影响，本文旨在填补这一空白。

Method: 通过系统评估28种组合（4种电极选择方案，7种分类器）在六大数据集上的表现，提出最优方案。

Result: Permutation Importance方法和Random Forest分类器能将电极数量减少53.5%，SparseEMG工具在多种应用中验证有效。

Conclusion: SparseEMG工具能生成高效的稀疏电极布局，且性能跨用户可迁移。

Abstract: Gesture recognition with electromyography (EMG) is a complex problem
influenced by gesture sets, electrode count and placement, and machine learning
parameters (e.g., features, classifiers). Most existing toolkits focus on
streamlining model development but overlook the impact of electrode selection
on classification accuracy. In this work, we present the first data-driven
analysis of how electrode selection and classifier choice affect both accuracy
and sparsity. Through a systematic evaluation of 28 combinations (4 selection
schemes, 7 classifiers), across six datasets, we identify an approach that
minimizes electrode count without compromising accuracy. The results show that
Permutation Importance (selection scheme) with Random Forest (classifier)
reduces the number of electrodes by 53.5\%. Based on these findings, we
introduce SparseEMG, a design tool that generates sparse electrode layouts
based on user-selected gesture sets, electrode constraints, and ML parameters
while also predicting classification performance. SparseEMG supports 50+ unique
gestures and is validated in three real-world applications using different
hardware setups. Results from our multi-dataset evaluation show that the
layouts generated from the SparseEMG design tool are transferable across users
with only minimal variation in gesture recognition performance.

</details>


### [33] [Metacognition and self-regulated learning in manipulative robotic problem-solving task](https://arxiv.org/abs/2508.05112)
*Margarida Romero,George Kalmpourtzis*

Main category: cs.HC

TL;DR: 本章分析了元认知在创造性问题解决（CPS）中的作用，特别是在监测学习者推理和CPS活动中的元推理行为。研究通过教育机器人案例展示了探索与利用知识的过程如何受元认知调控。


<details>
  <summary>Details</summary>
Motivation: 探讨元认知如何在CPS中调控学习者的探索与利用行为，特别是在未明确定义的问题中，以帮助学习者更好地解决问题。

Method: 采用案例研究方法，通过参与者在教育机器人问题中的探索过程，分析其元认知对问题空间和解决方案的调控作用。

Result: 研究发现元认知通过调控探索与利用行为，帮助学习者逐步明确问题并找到解决方案。

Conclusion: 元认知在CPS中起着关键作用，尤其是在未明确定义的问题中，它能够有效调控学习者的探索与利用行为以促进问题解决。

Abstract: Metacognition is an important aspect in creative problem solving (CPS) and
through this chapter we analyse the meta-reasoning aspects applied in the
different processes of monitoring the progress of learners' reasoning and CPS
activities. Meta-reasoning monitors the way that problem-solving processes
advance and regulate time and efforts towards a solution. In the context of an
ill-defined problem, exploration is required to develop a better-defined
problem space and advance towards the solution space. The way learners engage
in exploration and exploitations is regulated by the meta-reasoning within the
CPS activity. The objective of this chapter is to examine and identify the CPS
process with educational robots through a metacognitive and interactionist
approach. This chapter presents a case study, where, to solve a problem, a
participant had to explore a set of robot cubes to develop the technological
knowledge associated with each single component of the system, but also
conceptualize a system-level behaviour of the cubes when they are assembled.
The chapter presents the emergence of knowledge through the metacognitive
regulation of the process of exploration and exploitation of prior knowledge
and emergent knowledge until finding a solution

</details>


### [34] [AI Conversational Tutors in Foreign Language Learning: A Mixed-Methods Evaluation Study](https://arxiv.org/abs/2508.05156)
*Nikolaos Avouris*

Main category: cs.HC

TL;DR: 论文研究AI导师在外语学习中的应用，通过混合方法评估最新AI工具的用户体验和对话功能，提出质量评估标准和未来设计建议。


<details>
  <summary>Details</summary>
Motivation: 外语学习需求增长，AI导师在实时语言处理技术支持下发展迅速，需要评估其效果以改进设计。

Method: 采用混合方法，通过用户体验评估和对话记录分析，研究多种先进AI语言学习工具。

Result: 研究为AI导师的质量评估提供标准，并指出未来工具设计中需关注数据隐私和信息安全。

Conclusion: AI导师在外语学习中潜力巨大，研究为其质量提升和设计改进提供了重要依据。

Abstract: This paper focuses on AI tutors in foreign language learning, a field of
application of AI tutors with great development, especially during the last
years, when great advances in natural language understanding and processing in
real time, have been achieved. These tutors attempt to address needs for
improving language skills (speaking, or communicative competence,
understanding). In this paper, a mixed-methos empirical study on the use of
different kinds of state-of-the-art AI tutors for language learning is
reported. This study involves a user experience evaluation of typical such
tools, with special focus in their conversation functionality and an evaluation
of their quality, based on chat transcripts. This study can help establish
criteria for assessing the quality of such systems and inform the design of
future tools, including concerns about data privacy and secure handling of
learner information.

</details>


### [35] [CWEFS: Brain volume conduction effects inspired channel-wise EEG feature selection for multi-dimensional emotion recognition](https://arxiv.org/abs/2508.05228)
*Xueyuan Xu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 本文提出了一种基于通道级脑电特征选择的新方法CWEFS，用于解决高维脑电信号中的冗余与无关信息问题，并提升情绪识别的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于脑内体积传导效应，高维多通道脑电特征常包含大量冗余与无关信息，影响情绪表征的提取与实时性能。

Method: 提出CWEFS方法，通过共享潜在结构模型整合脑电特征选择，并结合自适应通道权重学习，以优化多维度情绪标签的关联性。

Result: 在三个流行的脑电数据集上验证，CWEFS在六项评估指标中表现最优，优于其他19种特征选择方法。

Conclusion: CWEFS方法显著提升了情绪识别的性能与模型的可解释性，为多维度情感计算提供了精确的特征选择方案。

Abstract: Due to the intracranial volume conduction effects, high-dimensional
multi-channel electroencephalography (EEG) features often contain substantial
redundant and irrelevant information. This issue not only hinders the
extraction of discriminative emotional representations but also compromises the
real-time performance. Feature selection has been established as an effective
approach to address the challenges while enhancing the transparency and
interpretability of emotion recognition models. However, existing EEG feature
selection research overlooks the influence of latent EEG feature structures on
emotional label correlations and assumes uniform importance across various
channels, directly limiting the precise construction of EEG feature selection
models for multi-dimensional affective computing. To address these limitations,
a novel channel-wise EEG feature selection (CWEFS) method is proposed for
multi-dimensional emotion recognition. Specifically, inspired by brain volume
conduction effects, CWEFS integrates EEG emotional feature selection into a
shared latent structure model designed to construct a consensus latent space
across diverse EEG channels. To preserve the local geometric structure, this
consensus space is further integrated with the latent semantic analysis of
multi-dimensional emotional labels. Additionally, CWEFS incorporates adaptive
channel-weight learning to automatically determine the significance of
different EEG channels in the emotional feature selection task. The
effectiveness of CWEFS was validated using three popular EEG datasets with
multi-dimensional emotional labels. Comprehensive experimental results,
compared against nineteen feature selection methods, demonstrate that the EEG
feature subsets chosen by CWEFS achieve optimal emotion recognition performance
across six evaluation metrics.

</details>


### [36] [ADSEL: Adaptive dual self-expression learning for EEG feature selection via incomplete multi-dimensional emotional tagging](https://arxiv.org/abs/2508.05229)
*Tianze Yu,Junming Zhang,Wenjia Dong,Xueyuan Xu,Li Zhuo*

Main category: cs.HC

TL;DR: 该论文提出了一种基于EEG的多维度情绪识别特征选择算法，通过自适应双重自表达学习（ADSEL）与最小二乘回归结合，解决标签不完整和样本维度间信息共享的问题。


<details>
  <summary>Details</summary>
Motivation: 现有EEG特征选择方法假设标签完整，但实际中标签常不完整，影响模型泛化能力。此外，现有方法忽视样本间相关性及其与维度的交互。

Method: 提出ADSEL算法，结合样本级和维度级自表达学习，利用最小二乘回归实现标签重建和特征选择。

Result: ADSEL提高了标签恢复精度，并有效识别最优EEG特征子集，为多维度情绪识别提供支持。

Conclusion: 该方法在标签不完整情况下增强了特征选择的有效性和模型泛化能力。

Abstract: EEG based multi-dimension emotion recognition has attracted substantial
research interest in human computer interfaces. However, the high
dimensionality of EEG features, coupled with limited sample sizes, frequently
leads to classifier overfitting and high computational complexity. Feature
selection constitutes a critical strategy for mitigating these challenges. Most
existing EEG feature selection methods assume complete multi-dimensional
emotion labels. In practice, open acquisition environment, and the inherent
subjectivity of emotion perception often result in incomplete label data, which
can compromise model generalization. Additionally, existing feature selection
methods for handling incomplete multi-dimensional labels primarily focus on
correlations among various dimensions during label recovery, neglecting the
correlation between samples in the label space and their interaction with
various dimensions. To address these issues, we propose a novel incomplete
multi-dimensional feature selection algorithm for EEG-based emotion
recognition. The proposed method integrates an adaptive dual self-expression
learning (ADSEL) with least squares regression. ADSEL establishes a
bidirectional pathway between sample-level and dimension-level self-expression
learning processes within the label space. It could facilitate the
cross-sharing of learned information between these processes, enabling the
simultaneous exploitation of effective information across both samples and
dimensions for label reconstruction. Consequently, ADSEL could enhances label
recovery accuracy and effectively identifies the optimal EEG feature subset for
multi-dimensional emotion recognition.

</details>


### [37] [FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing](https://arxiv.org/abs/2508.05231)
*Wenjia Dong,Xueyuan Xu,Tianze Yu,Junming Zhang,Li Zhuo*

Main category: cs.HC

TL;DR: 该论文提出了一种新的框架FDC-Net，用于端到端的噪声鲁棒情感识别，通过深度耦合去噪和情感识别任务，解决了当前方法中任务独立导致的误差累积和缺乏噪声鲁棒性设计的问题。


<details>
  <summary>Details</summary>
Motivation: EEG信号在实际应用中易受生理伪迹影响，而现有方法通常将去噪和情感识别视为独立任务，导致误差累积且未能利用任务间的潜在协同作用。此外，传统方法缺乏对噪声鲁棒性的系统设计。

Method: FDC-Net通过双向梯度传播与联合优化策略建立了动态协作机制，并集成了门控注意力机制和频率自适应的Transformer。

Result: 在去噪任务中，FDC-Net在DEAP和DREAMER数据集上的最高相关系数（CC）分别为96.30%和90.31%。在情感识别任务中，准确率分别为82.3±7.1%和88.1±0.8%。

Conclusion: FDC-Net通过深度耦合去噪和情感识别任务，显著提升了噪声环境下的情感识别性能，为EEG信号的实际应用提供了更鲁棒的解决方案。

Abstract: Electroencephalogram (EEG)-based emotion recognition holds significant value
in affective computing and brain-computer interfaces. However, in practical
applications, EEG recordings are susceptible to the effects of various
physiological artifacts. Current approaches typically treat denoising and
emotion recognition as independent tasks using cascaded architectures, which
not only leads to error accumulation, but also fails to exploit potential
synergies between these tasks. Moreover, conventional EEG-based emotion
recognition models often rely on the idealized assumption of "perfectly
denoised data", lacking a systematic design for noise robustness. To address
these challenges, a novel framework that deeply couples denoising and emotion
recognition tasks is proposed for end-to-end noise-robust emotion recognition,
termed as Feedback-Driven Collaborative Network for Denoising-Classification
Nexus (FDC-Net). Our primary innovation lies in establishing a dynamic
collaborative mechanism between artifact removal and emotion recognition
through: (1) bidirectional gradient propagation with joint optimization
strategies; (2) a gated attention mechanism integrated with frequency-adaptive
Transformer using learnable band-position encoding. Two most popular EEG-based
emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels
were employed to compare the artifact removal and emotion recognition
performance between ASLSL and nine state-of-the-art methods. In terms of the
denoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of
96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the
emotion recognition task under physiological artifact interference, FDC-Net
achieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on
DREAMER.

</details>


### [38] [Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models](https://arxiv.org/abs/2508.05238)
*Wei Xiang,Muchen Li,Jie Yan,Manling Zheng,Hanfei Zhu,Mengyun Jiang,Lingyun Sun*

Main category: cs.HC

TL;DR: Level 3自动驾驶系统允许驾驶员进行次要任务，但降低了对风险的感知。本研究利用大语言模型（LLM），通过‘人性化’建议帮助驾驶员保持对路况的注意力。测试表明，该工具能有效减少认知负担并协调次要任务与接管行为。


<details>
  <summary>Details</summary>
Motivation: 解决Level 3自动驾驶中驾驶员因次要任务导致的注意力分散和认知负担问题。

Method: 利用LLM生成‘人性化’建议，通过视觉和听觉途径主动引导驾驶员行为。

Result: 研究发现该工具能有效维持驾驶员注意力，减少认知负担，并协调次要任务与接管行为。

Conclusion: 展示了LLM在多任务自动驾驶中支持驾驶员的潜力。

Abstract: Level 3 automated driving systems allows drivers to engage in secondary tasks
while diminishing their perception of risk. In the event of an emergency
necessitating driver intervention, the system will alert the driver with a
limited window for reaction and imposing a substantial cognitive burden. To
address this challenge, this study employs a Large Language Model (LLM) to
assist drivers in maintaining an appropriate attention on road conditions
through a "humanized" persuasive advice. Our tool leverages the road conditions
encountered by Level 3 systems as triggers, proactively steering driver
behavior via both visual and auditory routes. Empirical study indicates that
our tool is effective in sustaining driver attention with reduced cognitive
load and coordinating secondary tasks with takeover behavior. Our work provides
insights into the potential of using LLMs to support drivers during multi-task
automated driving.

</details>


### [39] [A Methodological Framework and Questionnaire for Investigating Perceived Algorithmic Fairness](https://arxiv.org/abs/2508.05281)
*Ahmed Abdal Shafi Rasel,Ahmed Mustafa Amlan,Tasmim Shajahan Mim,Tanvir Hasan*

Main category: cs.HC

TL;DR: 研究探讨了孟加拉国用户对算法决策中公平性的感知，结合定量和定性方法，揭示文化和社会因素对理解AI公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 了解非西方背景下用户对算法公平性、透明度和问责制的看法，以促进全球AI伦理讨论。

Method: 采用混合方法，结合定量调查和定性访谈，分析文化和情境因素。

Result: 研究发现用户对人为监督、解释机制和争议性的态度复杂，强调文化意识设计的重要性。

Conclusion: 研究为全球AI伦理对话提供了新视角，强调文化敏感的算法设计对公平和信任的重要性。

Abstract: This study explores perceptions of fairness in algorithmic decision-making
among users in Bangladesh through a comprehensive mixed-methods approach. By
integrating quantitative survey data with qualitative interview insights, we
examine how cultural, social, and contextual factors influence users'
understanding of fairness, transparency, and accountability in AI systems. Our
findings reveal nuanced attitudes toward human oversight, explanation
mechanisms, and contestability, highlighting the importance of culturally aware
design principles for equitable and trustworthy algorithmic systems. These
insights contribute to ongoing discussions on algorithmic fairness by
foregrounding perspectives from a non-Western context, thus broadening the
global dialogue on ethical AI deployment.

</details>


### [40] [Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs](https://arxiv.org/abs/2508.05325)
*Jonathan C. Roberts,Hanan Alnjar,Aron E. Owen,Panagiotis D. Ritsos*

Main category: cs.HC

TL;DR: 介绍了‘关键设计策略’（CDS），一种通过反思与批判性思维改进可视化设计的结构化方法。


<details>
  <summary>Details</summary>
Motivation: 帮助设计师在开发可视化工具时通过批判性思维识别改进点，尤其是对新手设计师。

Method: CDS分为三个阶段：初始印象（标题与形容词选择）、深度批判（30个启发式问题）、综合反思与下一步计划。

Result: 在本科与研究生课程中应用CDS，并通过多年使用持续优化其内容与辅助材料。

Conclusion: CDS为可视化设计提供了实用框架，并适合教育与实践中的广泛应用。

Abstract: We present the Critical Design Strategy (CDS) - a structured method designed
to facilitate the examination of visualisation designs through reflection and
critical thought. The CDS helps designers think critically and make informed
improvements using heuristic evaluation. When developing a visual tool or
pioneering a novel visualisation approach, identifying areas for enhancement
can be challenging. Critical thinking is particularly crucial for visualisation
designers and tool developers, especially those new to the field, such as
studying visualisation in higher education. The CDS consists of three stages
across six perspectives: Stage 1 captures the essence of the idea by assigning
an indicative title and selecting five adjectives (from twenty options) to form
initial impressions of the design. Stage 2 involves an in-depth critique using
30 heuristic questions spanning six key perspectives - user, environment,
interface, components, design, and visual marks. Stage 3 focuses on
synthesising insights, reflecting on design decisions, and determining the next
steps forward. We introduce the CDS and explore its use across three
visualisation modules in both undergraduate and postgraduate courses. Our
longstanding experience with the CDS has allowed us to refine and develop it
over time: from its initial creation through workshops in 2017/18 to
improvements in wording and the development of two applications by 2020,
followed by the expansion of support notes and refinement of heuristics through
2023; while using it in our teaching each year. This sustained use allows us to
reflect on its practical application and offer guidance on how others can
incorporate it into their own work.

</details>


### [41] [Implementation and Application of Multi-Format 3D Data Integration in a Cross-Device Commercial Metaverse Platform](https://arxiv.org/abs/2508.05332)
*Masanori Ibara,Yuichi Hiroi,Takushi Kamegai,Takefumi Hiraki*

Main category: cs.HC

TL;DR: 这篇论文介绍了工业元宇宙平台Cluster如何通过整合3D设计数据（如BIM和CAD）和数字孪生技术，实现跨设备和多用户协作决策支持，突破了传统专家依赖系统的限制。


<details>
  <summary>Details</summary>
Motivation: 传统3D设计数据（如BIM和CAD）仅限专家使用，阻碍了普通用户参与决策。论文探讨如何利用工业元宇宙降低门槛，促进民主化协作。

Method: 分析了工业和建筑领域主要数据格式的特点与限制，组织元宇宙整合工作流，并通过多领域3D数据应用案例展示协作决策支持。

Result: 多设备访问和多用户同时参与能力为工业元宇宙创造了民主化环境，这是传统专家依赖系统难以实现的。

Conclusion: 工业元宇宙平台Cluster能够高效整合3D数据，支持协作决策，为工业和建筑领域提供了更开放的解决方案。

Abstract: Traditionally, specialized 3D design data, such as BIM and CAD, have been
accessible only to a select group of experts, creating significant barriers
that prevent general users from participating in decision-making processes.
This paper provides a systematic overview of practical insights for utilizing
3D data in industrial and architectural domains by presenting implementation
cases of the industrial metaverse on Cluster, a commercial cross-device
metaverse platform. This paper analyzes the characteristics and constraints of
major data formats in the industrial and architectural fields and organizes
integration workflows for the metaverse. Through application cases utilizing 3D
data across multiple domains, we present practical examples of collaborative
decision-making support enabled by the fusion of metaverse and digital twin
technologies. Specifically, we demonstrate that multi-device access and
simultaneous multi-user participation capabilities foster democratic
environments in the industrial metaverse, which are challenging to achieve with
conventional, expert-dependent systems.

</details>


### [42] [Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study](https://arxiv.org/abs/2508.05497)
*Federico Scarì,Olger Siebinga,Arkady Zgonnikov*

Main category: cs.HC

TL;DR: 本文提出了一种结构化评估框架，用于评估自动驾驶车辆（AVs）与人类驾驶车辆（HDVs）的交互，弥补了现有研究中对人机交互维度的忽视，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有AV控制算法评估主要关注技术性能（如避撞或车道保持效率），忽略了人机交互的维度。本文旨在填补这一空白，提供更全面的评估方法。

Method: 提出一个包含四个关键领域的评估框架：交互效果、交互感知、交互努力和交互能力，并通过驾驶模拟器中的并道场景案例研究验证框架的实用性。

Result: 案例研究结果表明，覆盖所有四个领域的指标能够揭示AV与HDV交互中驾驶员体验的显著差异，突出了全面评估的必要性。

Conclusion: 该框架为研究者、开发者和政策制定者提供了一种系统评估AV行为的方法，不仅关注技术性能，还包括人机交互的可理解性、可接受性和安全性。

Abstract: As automated vehicles (AVs) increasingly integrate into mixed-traffic
environments, evaluating their interaction with human-driven vehicles (HDVs)
becomes critical. In most research focused on developing new AV control
algorithms (controllers), the performance of these algorithms is assessed
solely based on performance metrics such as collision avoidance or lane-keeping
efficiency, while largely overlooking the human-centred dimensions of
interaction with HDVs. This paper proposes a structured evaluation framework
that addresses this gap by incorporating metrics grounded in the human-robot
interaction literature. The framework spans four key domains: a) interaction
effect, b) interaction perception, c) interaction effort, and d) interaction
ability. These domains capture both the performance of the AV and its impact on
human drivers around it. To demonstrate the utility of the framework, we apply
it to a case study evaluating how a state-of-the-art AV controller interacts
with human drivers in a merging scenario in a driving simulator. Measuring
HDV-HDV interactions as a baseline, this study included one representative
metric per domain: a) perceived safety, b) subjective ratings, specifically how
participants perceived the other vehicle's driving behaviour (e.g.,
aggressiveness or predictability) , c) driver workload, and d) merging success.
The results showed that incorporating metrics covering all four domains in the
evaluation of AV controllers can illuminate critical differences in driver
experience when interacting with AVs. This highlights the need for a more
comprehensive evaluation approach. Our framework offers researchers,
developers, and policymakers a systematic method for assessing AV behaviour
beyond technical performance, fostering the development of AVs that are not
only functionally capable but also understandable, acceptable, and safe from a
human perspective.

</details>


### [43] [Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2508.05572)
*Yifan Wang,Hongfeng Ai,Ruiqi Li,Maowei Jiang,Ruiyuan Kang,Jiahua Dong,Cheng Jiang,Chenzhong Li*

Main category: cs.HC

TL;DR: 论文解决了医疗时间序列疾病诊断中的两个挑战：高标注成本和传统对比学习的局限性，提出结合AE-GAN和LMCF框架的方法，实验证明其优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 医疗数据标注成本高且传统对比学习方法复杂、泛化性差，需要一种更高效且自适应的诊断方法。

Method: 结合AE-GAN提取先验知识，并提出LMCF框架，通过多头注意力机制和对比学习策略自适应学习特征。

Result: 在三种目标数据集上表现优于七个基线方法，适用于心肌梗塞、阿尔茨海默病和帕金森病的诊断。

Conclusion: 提出的方法有效解决了医疗诊断中的关键问题，具有显著的医疗应用价值。

Abstract: In medical time series disease diagnosis, two key challenges are identified.
First, the high annotation cost of medical data leads to overfitting in models
trained on label-limited, single-center datasets. To address this, we propose
incorporating external data from related tasks and leveraging AE-GAN to extract
prior knowledge, providing valuable references for downstream tasks. Second,
many existing studies employ contrastive learning to derive more generalized
medical sequence representations for diagnostic tasks, usually relying on
manually designed diverse positive and negative sample pairs. However, these
approaches are complex, lack generalizability, and fail to adaptively capture
disease-specific features across different conditions. To overcome this, we
introduce LMCF (Learnable Multi-views Contrastive Framework), a framework that
integrates a multi-head attention mechanism and adaptively learns
representations from different views through inter-view and intra-view
contrastive learning strategies. Additionally, the pre-trained AE-GAN is used
to reconstruct discrepancies in the target data as disease probabilities, which
are then integrated into the contrastive learning process. Experiments on three
target datasets demonstrate that our method consistently outperforms other
seven baselines, highlighting its significant impact on healthcare applications
such as the diagnosis of myocardial infarction, Alzheimer's disease, and
Parkinson's disease. We release the source code at xxxxx.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [44] [Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting](https://arxiv.org/abs/2508.04965)
*Zijian Wang,Beizhen Zhao,Hao Wang*

Main category: cs.GR

TL;DR: 提出了一种新型的perceive-sample-compress框架，用于优化3DGS在大规模场景和存储效率上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS在处理大规模场景或资源受限时表现不佳，需要改进存储和渲染效率。

Method: 采用场景感知补偿算法和金字塔采样表示法，结合广义高斯混合模型压缩算法。

Result: 实验证明，该方法显著提升了内存效率和视觉质量，同时保持实时渲染速度。

Conclusion: 新框架有效解决了3DGS的存储和渲染效率问题，适用于复杂环境。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable
capabilities in real-time and photorealistic novel view synthesis. However,
traditional 3DGS representations often struggle with large-scale scene
management and efficient storage, particularly when dealing with complex
environments or limited computational resources. To address these limitations,
we introduce a novel perceive-sample-compress framework for 3D Gaussian
Splatting. Specifically, we propose a scene perception compensation algorithm
that intelligently refines Gaussian parameters at each level. This algorithm
intelligently prioritizes visual importance for higher fidelity rendering in
critical areas, while optimizing resource usage and improving overall visible
quality. Furthermore, we propose a pyramid sampling representation to manage
Gaussian primitives across hierarchical levels. Finally, to facilitate
efficient storage of proposed hierarchical pyramid representations, we develop
a Generalized Gaussian Mixed model compression algorithm to achieve significant
compression ratios without sacrificing visual fidelity. The extensive
experiments demonstrate that our method significantly improves memory
efficiency and high visual quality while maintaining real-time rendering speed.

</details>


### [45] [Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction](https://arxiv.org/abs/2508.04966)
*Yifan Zhou,Beizhen Zhao,Pengcheng Wu,Hao Wang*

Main category: cs.GR

TL;DR: 该论文提出了一种结合显式与隐式功能的动态3D高斯泼溅框架，通过频谱感知拉普拉斯编码、增强高斯动态属性和自适应高斯分割策略，解决了动态场景建模中的细节保留与变形一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有动态3D高斯泼溅方法因低秩分解导致过度平滑或高维网格采样导致特征冲突，无法平衡运动细节保存与变形一致性，需要提出新方法解决这些挑战。

Method: 结合频谱感知拉普拉斯编码架构、增强高斯动态属性和自适应高斯分割策略，以哈希编码和拉普拉斯模块混合实现灵活频率运动控制，并通过KDTree优化动态区域查询。

Result: 实验表明，该方法在复杂动态场景重建中表现出色，具有更高的重建保真度，达到当前最佳性能。

Conclusion: 该框架成功解决了动态3D高斯泼溅的频谱冲突问题，为复杂动态场景建模提供了有效解决方案。

Abstract: While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its
extension to dynamic scenes introduces significant challenges. Existing dynamic
3DGS methods suffer from either over-smoothing due to low-rank decomposition or
feature collision from high-dimensional grid sampling. This is because of the
inherent spectral conflicts between preserving motion details and maintaining
deformation consistency at different frequency. To address these challenges, we
propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions.
Our approach contains three key innovations: a spectral-aware Laplacian
encoding architecture which merges Hash encoding and Laplacian-based module for
flexible frequency motion control, an enhanced Gaussian dynamics attribute that
compensates for photometric distortions caused by geometric deformation, and an
adaptive Gaussian split strategy guided by KDTree-based primitive control to
efficiently query and optimize dynamic areas. Through extensive experiments,
our method demonstrates state-of-the-art performance in reconstructing complex
dynamic scenes, achieving better reconstruction fidelity.

</details>


### [46] [Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off](https://arxiv.org/abs/2508.04825)
*Seungyong Lee,Jeong-gi Kwak*

Main category: cs.GR

TL;DR: Voost是一个统一的扩散变换器框架，联合学习虚拟试穿和虚拟试脱任务，通过双向监督和灵活的条件生成，提升了衣物-人体关系推理。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟试穿任务中衣物与人体对应关系建模的挑战，尤其是在姿势和外观变化的情况下。

Method: 使用单个扩散变换器联合学习试穿和试脱任务，引入注意力温度缩放和自我校正采样技术。

Result: 在多个基准测试中，Voost在对齐准确性、视觉保真度和泛化性能上均优于现有方法。

Conclusion: Voost通过统一框架和新技术，显著提升了虚拟试穿和试脱的任务性能。

Abstract: Virtual try-on aims to synthesize a realistic image of a person wearing a
target garment, but accurately modeling garment-body correspondence remains a
persistent challenge, especially under pose and appearance variation. In this
paper, we propose Voost - a unified and scalable framework that jointly learns
virtual try-on and try-off with a single diffusion transformer. By modeling
both tasks jointly, Voost enables each garment-person pair to supervise both
directions and supports flexible conditioning over generation direction and
garment category, enhancing garment-body relational reasoning without
task-specific networks, auxiliary losses, or additional labels. In addition, we
introduce two inference-time techniques: attention temperature scaling for
robustness to resolution or mask variation, and self-corrective sampling that
leverages bidirectional consistency between tasks. Extensive experiments
demonstrate that Voost achieves state-of-the-art results on both try-on and
try-off benchmarks, consistently outperforming strong baselines in alignment
accuracy, visual fidelity, and generalization.

</details>


### [47] [A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding](https://arxiv.org/abs/2508.05064)
*Mahmoud Chick Zaouali,Todd Charter,Yehor Karpichev,Brandon Haworth,Homayoun Najjjaran*

Main category: cs.GR

TL;DR: 本文综述了结合语言指导与3D高斯泼溅（Gaussian Splatting）的当前研究，探讨了理论、集成策略和应用示例，同时提出了限制与未来方向。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅作为一种高效的3D场景表示技术，结合语言模型后展现了新的潜力，但目前缺乏对这一交叉领域的系统综述。

Method: 通过调查现有研究，总结了将大语言模型（LLMs）与高斯泼溅结合的理论基础、集成方法及实际用例。

Result: 揭示了当前技术面临的挑战，如计算瓶颈和缺乏语义标注数据，但也展示了其在场景生成与理解中的潜力。

Conclusion: 综述为未来研究方向提供了框架，强调了在语言指导下提升3D高斯泼溅技术的必要性。

Abstract: Gaussian Splatting has rapidly emerged as a transformative technique for
real-time 3D scene representation, offering a highly efficient and expressive
alternative to Neural Radiance Fields (NeRF). Its ability to render complex
scenes with high fidelity has enabled progress across domains such as scene
reconstruction, robotics, and interactive content creation. More recently, the
integration of Large Language Models (LLMs) and language embeddings into
Gaussian Splatting pipelines has opened new possibilities for text-conditioned
generation, editing, and semantic scene understanding. Despite these advances,
a comprehensive overview of this emerging intersection has been lacking. This
survey presents a structured review of current research efforts that combine
language guidance with 3D Gaussian Splatting, detailing theoretical
foundations, integration strategies, and real-world use cases. We highlight key
limitations such as computational bottlenecks, generalizability, and the
scarcity of semantically annotated 3D Gaussian data and outline open challenges
and future directions for advancing language-guided 3D scene understanding
using Gaussian Splatting.

</details>


### [48] [RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer](https://arxiv.org/abs/2508.05115)
*Fangyu Du,Taiqing Li,Ziwei Zhang,Qian Qiao,Tan Yu,Dingcheng Zhen,Xu Jia,Yang Yang,Shunshun Yin,Siyuan Liu*

Main category: cs.GR

TL;DR: RAP是一种实时音频驱动肖像动画框架，通过混合注意力机制和静态-动态训练推理范式，实现高质量动画，同时满足实时性约束。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高质量输出和实时性之间存在矛盾，RAP旨在在保持高视觉质量的同时实现实时推理。

Method: RAP采用混合注意力机制进行精细音频控制，并提出静态-动态训练推理范式，避免显式运动监督。

Result: RAP在实时约束下实现了最先进的性能，精确同步音频与动画，同时减少时间漂移。

Conclusion: RAP通过创新设计解决了实时性与高质量输出的平衡问题，为音频驱动动画提供了高效解决方案。

Abstract: Audio-driven portrait animation aims to synthesize realistic and natural
talking head videos from an input audio signal and a single reference image.
While existing methods achieve high-quality results by leveraging
high-dimensional intermediate representations and explicitly modeling motion
dynamics, their computational complexity renders them unsuitable for real-time
deployment. Real-time inference imposes stringent latency and memory
constraints, often necessitating the use of highly compressed latent
representations. However, operating in such compact spaces hinders the
preservation of fine-grained spatiotemporal details, thereby complicating
audio-visual synchronization RAP (Real-time Audio-driven Portrait animation), a
unified framework for generating high-quality talking portraits under real-time
constraints. Specifically, RAP introduces a hybrid attention mechanism for
fine-grained audio control, and a static-dynamic training-inference paradigm
that avoids explicit motion supervision. Through these techniques, RAP achieves
precise audio-driven control, mitigates long-term temporal drift, and maintains
high visual fidelity. Extensive experiments demonstrate that RAP achieves
state-of-the-art performance while operating under real-time constraints.

</details>


### [49] [Refining Gaussian Splatting: A Volumetric Densification Approach](https://arxiv.org/abs/2508.05187)
*Mohamed Abdul Gafoor,Marius Preda,Titus Zaharia*

Main category: cs.GR

TL;DR: 论文提出了一种新的密度控制方法，通过利用高斯函数的惯性体积来引导优化过程，改进了3D高斯泼溅（3DGS）中的点原语管理，并研究了不同点云初始化方法的效果。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS中自适应密度控制（ADC）的不足，提升新颖视角合成的质量。

Method: 引入基于高斯函数惯性体积的密度控制方法，并比较了传统SfM和深度图像匹配（DIM）的点云初始化效果。

Result: 在Mip-NeRF 360数据集上的实验表明，该方法在重建质量上优于3DGS，且在多种场景中表现优异。

Conclusion: 新方法有效提升了3D高斯泼溅的点原语管理质量，为新颖视角合成提供了更优的性能。

Abstract: Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS)
often depends on effective point primitive management. The underlying Adaptive
Density Control (ADC) process addresses this issue by automating densification
and pruning. Yet, the vanilla 3DGS densification strategy shows key
shortcomings. To address this issue, in this paper we introduce a novel density
control method, which exploits the volumes of inertia associated to each
Gaussian function to guide the refinement process. Furthermore, we study the
effect of both traditional Structure from Motion (SfM) and Deep Image Matching
(DIM) methods for point cloud initialization. Extensive experimental
evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses
3DGS in reconstruction quality, delivering encouraging performance across
diverse scenes.

</details>


### [50] [GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 2-Manifold Reeb Graphs](https://arxiv.org/abs/2508.05524)
*Sefat Rahman,Tushar M. Athawale,Paul Rosen*

Main category: cs.GR

TL;DR: 提出了一种名为GASP的算法，用于生成满足边界约束、紧凑性和梯度对齐的Reeb图可视化，通过与现有算法的对比证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有Reeb图绘制算法忽略了边界约束、紧凑性和梯度对齐这三个关键属性，导致可视化结果不能忠实反映数据。

Method: 提出GASP算法，确保生成的Reeb图满足边界约束、紧凑性和梯度对齐。

Result: 通过与TTK中的几何重心算法对比，定性和定量评估表明GASP算法的结果更优。

Conclusion: GASP算法在Reeb图可视化中更忠实于数据，优于现有方法。

Abstract: Reeb graphs are an important tool for abstracting and representing the
topological structure of a function defined on a manifold. We have identified
three properties for faithfully representing Reeb graphs in a visualization.
Namely, they should be constrained to the boundary, compact, and aligned with
the function gradient. Existing algorithms for drawing Reeb graphs are agnostic
to or violate these properties. In this paper, we introduce an algorithm to
generate Reeb graph visualizations, called \textit{GASP}, that is cognizant of
these properties, thereby producing visualizations that are more representative
of the underlying data. To demonstrate the improvements, the resulting Reeb
graphs are evaluated both qualitatively and quantitatively against the
geometric barycenter algorithm, using its implementation available in the
Topology ToolKit (TTK), a widely adopted tool for calculating and visualizing
Reeb graphs.

</details>


### [51] [Point cloud segmentation for 3D Clothed Human Layering](https://arxiv.org/abs/2508.05531)
*Davide Garavaso,Federico Masi,Pietro Musoni,Umberto Castellani*

Main category: cs.GR

TL;DR: 该论文提出了一种新的3D点云分割范式，用于同时关联不同层次的服装层，以解决服装人体建模中的语义分割问题，并通过合成数据集和神经网络设置验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 3D服装建模在多个领域中至关重要，但现有方法难以处理服装层之间的重叠问题，尤其是缺少语义信息的分割方法。

Method: 作者提出了一种名为“服装人体分层”的新范式，允许3D点同时关联多个层，并通过合成数据集和不同神经网络设置进行验证。

Result: 实验表明，该方法在合成和真实扫描数据集上均能有效提升服装层的分割准确性。

Conclusion: 论文通过新的分割范式和策略，成功解决了服装层重叠的问题，为服装人体建模提供了更准确的语义分割方案。

Abstract: 3D Cloth modeling and simulation is essential for avatars creation in several
fields, such as fashion, entertainment, and animation. Achieving high-quality
results is challenging due to the large variability of clothed body especially
in the generation of realistic wrinkles. 3D scan acquisitions provide more
accuracy in the representation of real-world objects but lack semantic
information that can be inferred with a reliable semantic reconstruction
pipeline. To this aim, shape segmentation plays a crucial role in identifying
the semantic shape parts. However, current 3D shape segmentation methods are
designed for scene understanding and interpretation and only few work is
devoted to modeling. In the context of clothed body modeling the segmentation
is a preliminary step for fully semantic shape parts reconstruction namely the
underlying body and the involved garments. These parts represent several layers
with strong overlap in contrast with standard segmentation methods that provide
disjoint sets. In this work we propose a new 3D point cloud segmentation
paradigm where each 3D point can be simultaneously associated to different
layers. In this fashion we can estimate the underlying body parts and the
unseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer
above. We name this segmentation paradigm clothed human layering. We create a
new synthetic dataset that simulates very realistic 3D scans with the ground
truth of the involved clothing layers. We propose and evaluate different neural
network settings to deal with 3D clothing layering. We considered both coarse
and fine grained per-layer garment identification. Our experiments demonstrates
the benefit in introducing proper strategies for the segmentation on the
garment domain on both the synthetic and real-world scan datasets.

</details>


### [52] [Physically Controllable Relighting of Photographs](https://arxiv.org/abs/2508.05626)
*Chris Careaga,Yağız Aksoy*

Main category: cs.GR

TL;DR: 提出了一种自监督的图像重光照方法，结合传统渲染的物理精度与神经渲染的逼真效果，实现场景的完全可控3D光照编辑。


<details>
  <summary>Details</summary>
Motivation: 将计算机图形工具中的物理光照控制能力扩展到真实场景的图像重光照任务中，实现逼真且可控的光照编辑。

Method: 通过单目估计几何和固有分量推断彩色网格表示，用户可在3D中定义光照配置，结合路径追踪和神经渲染生成最终结果。采用自监督训练策略，利用真实图像集重建光照。

Result: 提出的方法实现了高质量的场景重光照，将3D图形工具中的光照控制能力成功应用于真实图像。

Conclusion: 该方法在真实场景的光照编辑中实现了物理精度与逼真效果的结合，推进了图像重光照技术的发展。

Abstract: We present a self-supervised approach to in-the-wild image relighting that
enables fully controllable, physically based illumination editing. We achieve
this by combining the physical accuracy of traditional rendering with the
photorealistic appearance made possible by neural rendering. Our pipeline works
by inferring a colored mesh representation of a given scene using monocular
estimates of geometry and intrinsic components. This representation allows
users to define their desired illumination configuration in 3D. The scene under
the new lighting can then be rendered using a path-tracing engine. We send this
approximate rendering of the scene through a feed-forward neural renderer to
predict the final photorealistic relighting result. We develop a differentiable
rendering process to reconstruct in-the-wild scene illumination, enabling
self-supervised training of our neural renderer on raw image collections. Our
method represents a significant step in bringing the explicit physical control
over lights available in typical 3D computer graphics tools, such as Blender,
to in-the-wild relighting.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [53] [Injection Locking and Coupling Dynamics in Superconducting Nanowire based Cryogenic Oscillators](https://arxiv.org/abs/2508.04878)
*Md Mazharul Islam,Md Shafayat Hossain,Kathleen E Hamilton,Ahmedullah Aziz*

Main category: cs.ET

TL;DR: 该论文研究了超导纳米线低温振荡器的注入锁定和互耦合动力学，分析了关键设计参数及其对频率同步和信号协调的影响，为低温计算架构提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 低温振荡器在超导电子学和量子计算中至关重要，但对其同步机制的研究仍有不足，尤其是在超导纳米线振荡器中。本文旨在填补这一空白。

Method: 通过数值模拟研究了超导纳米线振荡器的注入锁定和互耦合动力学，分析了关键参数（如分流电阻、纳米线电感和耦合强度）及其对同步行为的影响。

Result: 研究发现注入信号的幅度和耦合强度可以精确控制振荡器的同步行为和相位差，为功率感知同步和可编程相位编码信息处理提供了可能。

Conclusion: 这些成果为构建基于超导纳米线的振荡神经网络、同步低温逻辑块和片上低温谐振器阵列奠定了基础。

Abstract: Oscillators designed to function at cryogenic temperatures play a critical
role in superconducting electronics and quantum computing by providing stable,
low noise signals with minimal energy loss.Here we present a comprehensive
numerical study of injection locking and mutual coupling dynamics in
superconducting nanowire based cryogenic oscillators.Using the design space of
standalone ScNW based oscillator, we investigate two critical mechanisms that
govern frequency synchronization and signal coordination in cryogenic computing
architectures.First, an injection locking induced by an external AC signal with
a frequency near the oscillators natural frequency, and second, the mutual
coupling dynamics between two ScNW oscillators under varying coupling
strengths.We identify key design parameters such as shunt resistance, nanowire
inductance, and coupling strength that govern the locking range.Additionally,
we examine how the amplitude of the injected signal affects the amplitude of
the locked oscillation, offering valuable insights for power aware oscillator
synchronization.Furthermore, we analyze mutual synchronization between coupled
ScNW oscillators using capacitive and resistive coupling elements.Our results
reveal that the phase difference between oscillators can be precisely
controlled by tuning the coupling strength, enabling programmable phase encoded
information processing.These findings could enable building ScNW based
oscillatory neural networks, synchronized cryogenic logic blocks, and on chip
cryogenic resonator arrays.

</details>


### [54] [Wave Computing based on Dynamical Networks: Applications in Optimization Problems](https://arxiv.org/abs/2508.05014)
*Yunwen Liu,Jiang Xiao*

Main category: cs.ET

TL;DR: 开发了一种基于波传播的计算框架，通过节点和边实现多维并行处理，有效解决NP难问题。


<details>
  <summary>Details</summary>
Motivation: 利用波传播的并行特性，扩展至多维空间（空间、时间、频率域），以高效解决NP难问题。

Method: 构建具有波操控能力的网络节点和边，通过SPICE模拟验证架构可行性。

Result: 成功模拟解决了数划分、0/1背包、旅行商问题等NP难问题。

Conclusion: 该框架展现了解决复杂问题的潜力，拓展了并行计算的可能性。

Abstract: We develop a computing framework that leverages wave propagation within an
interconnected network, where nodes and edges possess wave manipulation
capabilities, such as frequency mixing or time delay. This computing paradigm
can not only achieve intrinsic parallelism like existing works by the
exploration of an exponential number of possibilities simultaneously with very
small number of hardware units, but also extend this unique characteristic to a
multidimensional space including spatial, temporal and frequency domains,
making it particularly effective for addressing NP-hard problems. The proposed
architecture has been validated through SPICE simulations, demonstrating its
potential capability in solving several NP-hard problems, such as the Number
Partitioning Problem, the 0/1 Knapsack Problem, and the Traveling Salesman
Problem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [55] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: OPTIMUMP2P是一种基于随机线性网络编码（RLNC）的新型Gossip算法，旨在提升libp2p的性能和可靠性，特别是在存在恶意行为者的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的Gossip算法（如libp2p中的floodsup和gossibsup）在性能和可靠性方面存在局限性，尤其是在区块链协议中传播信息时。

Method: 提出OPTIMUMP2P算法，利用RLNC技术加速P2P网络中的信息传播，并确保可靠交付，即使在数据被恶意篡改的情况下。

Result: 通过实验验证，OPTIMUMP2P在模拟和真实环境中均表现出优于Gossipsub协议的性能提升，尤其是在区块传播时间上的显著改进。

Conclusion: OPTIMUMP2P通过引入RLNC技术，显著提升了Gossip算法的性能和可靠性，为P2P网络中的信息传播提供了更优的解决方案。

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [56] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: 研究两个具有不同通信能力的机器人如何通过线性搜索捕获移动目标，重点关注不对称通信对竞争比的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨在Sender/Receiver通信模型下，机器人如何利用不同通信能力（面对面与无线）优化捕获目标的策略，并分析其对时间效率的影响。

Method: 设计了新的线性搜索算法，考虑了目标移动速度、起始距离及运动方向（远离或接近）等多种情境，以评估捕获时间的最优竞争比。

Result: 通过分析不同情境下机器人的表现，揭示了不对称通信对线性搜索效率的具体影响。

Conclusion: 不对称通信能力在特定情境下可以显著优化线性搜索的竞争比，为多机器人协作搜索提供了理论支持。

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [57] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3是一个开源的数据平台，用于构建数据共享空间，支持管理、分析和共享研究数据，已管理超过28 PB数据和6400万FAIR数据对象。


<details>
  <summary>Details</summary>
Motivation: 构建一个基于云的数据共享平台，以促进研究社区的数据管理和共享。

Method: 用户首先定义数据模型，Gen3自动生成数据门户（用于搜索和提交数据）和FAIR API，基于少量标准化软件服务构建。

Result: 已成功构建十几个数据共享空间，管理海量数据并支持FAIR原则。

Conclusion: Gen3是一个高效、可扩展且兼容性强的数据共享平台，适用于与其他数据生态系统互操作。

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [58] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: 论文提出了Theseus，一个生产级的企业规模分布式加速器原生查询引擎，旨在优化数据移动、内存利用和计算，在GPU加速系统中表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了降低大规模数据集在线分析处理（OLAP）查询的成本并提高吞吐量，利用GPU等加速器成为可能，但需要解决数据移动等挑战。

Method: Theseus引擎通过异步控制机制、固定大小页锁定主机内存分配等技术，优化网络通信、数据预加载、GPU计算任务等。

Result: 在TPC-H基准测试中，Theseus在成本相当的情况下比Databricks Photon快4倍，且能在2个DGX A100 640GB节点上处理100TB规模的数据。

Conclusion: Theseus是一种高效的分布式查询引擎，特别适合基于加速器的大规模数据处理。

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [59] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 本文提出了一种基于图匹配的放置策略，用于优化深度学习集群调度器的资源利用率，从而减少作业迁移开销并提高作业打包效率。


<details>
  <summary>Details</summary>
Motivation: 提高深度学习集群的资源利用率和调度效率是数据中心的关键目标，现有调度策略存在性能不足或扩展性差的问题。

Method: 将许多放置约束建模为图匹配问题，并设计了新的放置策略以减少作业迁移开销和优化作业打包，这些策略被集成到Tesserae调度器中。

Result: 实验结果表明，Tesserae将平均作业完成时间（JCT）提高了1.62倍，将Makespan提高了1.15倍。

Conclusion: 通过图匹配方法设计的放置策略显著提升了调度器的性能和可扩展性。

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [60] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: 该论文提出了一种基于AMR的高阶求解器，利用Regent语言解决动态数据结构、网格有效性和任务开销等问题，展示了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 开发高效的高阶求解器用于可压缩流动科学应用，通过AMR技术降低计算成本。

Method: 使用Regent语言实现AMR求解器，解决动态数据结构、任务融合和GPU自动生成等问题。

Result: 任务融合实现18倍加速，GPU自动生成获得9.7倍加速。

Conclusion: 该方法在欧拉方程控制的经典问题中验证了有效性。

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [61] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: 提出了一种异构感知的分布式LLM模拟器，用于预测训练时间并支持自定义设备和并行映射配置。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练模拟器假设基础设施同质化，但实际中设备异构性不可避免，需要解决这一差距。

Method: 设计了异构感知的分布式ML训练模拟器，包含非均匀工作负载分区等组件。

Result: 初步模拟结果显示异构性对模型计算和通信时间的影响。

Conclusion: 异构感知模拟器填补了现有技术与实际需求之间的差距，为模型优化提供了新工具。

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [62] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: FastBioDL是一种用于大型生物数据集的并行文件下载工具，通过自适应并发控制器动态调整并发流，显著提升下载速度。


<details>
  <summary>Details</summary>
Motivation: 现有下载工具通常使用静态并发设置，无法适应动态网络条件，导致带宽利用不足和下载时间延长，FastBioDL旨在解决这一问题。

Method: FastBioDL将下载过程建模为在线优化问题，使用效用函数和梯度下降实时调整并发流数量。

Result: 在公共基因组数据集上的评估显示，FastBioDL比现有工具快4倍，高速网络中快2.1倍。

Conclusion: FastBioDL通过智能优化标准HTTP/FTP下载，为大规模基因组数据获取提供高效解决方案，无需专用商业软件。

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [63] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: 本文提出了一种名为AutoMDT的新型模块化数据传输架构，利用深度强化学习优化数据传输性能，显著提升了效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 高性能应用需要快速可靠地传输大量数据，但传统工具因配置固定或优化方法单一导致资源利用不足和不稳定。

Method: AutoMDT采用深度强化学习（PPO代理）同时优化读写和网络操作的并发级别，结合轻量级网络系统模拟器进行离线训练。

Result: 在测试中，AutoMDT实现了8倍更快的收敛速度和68%的传输完成时间减少。

Conclusion: AutoMDT通过模块化设计和强化学习，显著提升了数据传输性能，适应性强且高效。

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [64] [AgenticData: An Agentic Data Analytics System for Heterogeneous Data](https://arxiv.org/abs/2508.05002)
*Ji Sun,Guoliang Li,Peiyao Zhou,Yihui Ma,Jingzhe Xu,Yuan Li*

Main category: cs.DB

TL;DR: AgenticData是一个创新的数据自动化分析系统，通过自然语言问题自动分析多领域数据，结合反馈驱动规划和多代理协作策略，显著提升分析效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有非结构化数据分析系统依赖专家编写代码和管理复杂工作流，成本高且耗时。AgenticData旨在通过自动化解决这些问题。

Method: 1. 反馈驱动规划将自然语言查询转为语义计划；2. 多代理协作（数据剖析代理、语义交叉验证代理、智能记忆代理）；3. 语义优化模型优化计划执行。

Result: 在三类基准测试中表现优异，准确性显著优于现有方法。

Conclusion: AgenticData通过自动化规划和优化，高效解决了传统数据分析的复杂性问题。

Abstract: Existing unstructured data analytics systems rely on experts to write code
and manage complex analysis workflows, making them both expensive and
time-consuming. To address these challenges, we introduce AgenticData, an
innovative agentic data analytics system that allows users to simply pose
natural language (NL) questions while autonomously analyzing data sources
across multiple domains, including both unstructured and structured data.
First, AgenticData employs a feedback-driven planning technique that
automatically converts an NL query into a semantic plan composed of relational
and semantic operators. We propose a multi-agent collaboration strategy by
utilizing a data profiling agent for discovering relevant data, a semantic
cross-validation agent for iterative optimization based on feedback, and a
smart memory agent for maintaining short-term context and long-term knowledge.
Second, we propose a semantic optimization model to refine and execute semantic
plans effectively. Our system, AgenticData, has been tested using three
benchmarks. Experimental results showed that AgenticData achieved superior
accuracy on both easy and difficult tasks, significantly outperforming
state-of-the-art methods.

</details>


### [65] [Making Prompts First-Class Citizens for Adaptive LLM Pipelines](https://arxiv.org/abs/2508.05012)
*Ugur Cetintemel,Shu Chen,Alexander W. Lee,Deepti Raghavan*

Main category: cs.DB

TL;DR: SPEAR 是一种语言和运行时，旨在通过将提示结构化、自适应化并作为执行模型的一等公民，解决提示管理的当前局限。


<details>
  <summary>Details</summary>
Motivation: 现代LLM流程越来越像数据为中心的系统，但提示作为核心元素仍是脆弱、不透明的字符串，限制了重用、优化和运行时控制。

Method: SPEAR 提出了一种提示代数，支持动态提示优化（运行时调整）和结构化提示管理（版本化视图）。提供手动、辅助和自动三种优化模式。

Result: 初步实验量化了不同优化模式的效果，并展示了提示级优化（如操作符融合）的影响。

Conclusion: SPEAR 填补了提示管理的空白，使其更结构化、自适应且可优化。

Abstract: Modern LLM pipelines increasingly resemble data-centric systems: they
retrieve external context, compose intermediate outputs, validate results, and
adapt based on runtime feedback. Yet, the central element guiding this process
-- the prompt -- remains a brittle, opaque string, disconnected from the
surrounding dataflow. This disconnect limits reuse, optimization, and runtime
control.
  In this paper, we describe our vision and an initial design for SPEAR, a
language and runtime that fills this prompt management gap by making prompts
structured, adaptive, and first-class components of the execution model. SPEAR
enables (1) runtime prompt refinement -- modifying prompts dynamically in
response to execution-time signals such as confidence, latency, or missing
context; and (2) structured prompt management -- organizing prompt fragments
into versioned views with support for introspection and logging.
  SPEAR defines a prompt algebra that governs how prompts are constructed and
adapted within a pipeline. It supports multiple refinement modes (manual,
assisted, and automatic), giving developers a balance between control and
automation. By treating prompt logic as structured data, SPEAR enables
optimizations such as operator fusion, prefix caching, and view reuse.
Preliminary experiments quantify the behavior of different refinement modes
compared to static prompts and agentic retries, as well as the impact of
prompt-level optimizations such as operator fusion.

</details>


### [66] [Data-Aware Socratic Query Refinement in Database Systems](https://arxiv.org/abs/2508.05061)
*Ruiyuan Zhang,Chrysanthi Kosyfaki,Xiaofang Zhou*

Main category: cs.DB

TL;DR: DASG是一种基于对话的查询增强框架，通过交互澄清解决自然语言查询的歧义，优化执行成本，并提升查询精度。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询中的歧义问题，通过交互澄清优化查询效率。

Method: 使用对话作为优化决策，结合语义相关性、信息增益和成本降低选择最佳澄清问题。

Result: 在三个数据集上的实验表明，DASG提高了查询精度且保持高效。

Conclusion: DASG建立了一种协作式分析范式，系统主动参与查询制定。

Abstract: In this paper, we propose Data-Aware Socratic Guidance (DASG), a
dialogue-based query enhancement framework that embeds \linebreak interactive
clarification as a first-class operator within database systems to resolve
ambiguity in natural language queries. DASG treats dialogue as an optimization
decision, asking clarifying questions only when the expected execution cost
reduction exceeds the interaction overhead. The system quantifies ambiguity
through linguistic fuzziness, schema grounding confidence, and projected costs
across relational and vector backends. Our algorithm selects the optimal
clarifications by combining semantic relevance, catalog-based information gain,
and potential cost reduction. We evaluate our proposed framework on three
datasets. The results show that DASG demonstrates improved query precision
while maintaining efficiency, establishing a cooperative analytics paradigm
where systems actively participate in query formulation rather than passively
translating user requests.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [67] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 论文分析了基于大型语言模型（LLM）的RTL代码生成中的常见错误原因，并提出针对性改进方法，显著提升了生成准确率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的RTL代码生成成功率较低，但错误原因未被充分理解，阻碍了改进。

Method: 通过综合分析错误原因，提出包括构建领域知识库、引入设计规则检查、处理多模态输入和迭代调试等改进措施。

Result: 改进后的框架在VerilogEval基准测试中达到91.0%的准确率，较基线方法提升了32.7%。

Conclusion: 针对错误的系统性改进方法能显著提升LLM在RTL代码生成中的性能。

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>


### [68] [relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication](https://arxiv.org/abs/2508.05354)
*Michael Rogenmoser,Angelo Garofalo,Luca Benini*

Main category: cs.AR

TL;DR: 本文提出relOBI，一种结合三重模块冗余（TMR）和纠错码（ECC）的Open Bus Interface（OBI）扩展，显著提升SoC互连的可靠性，将故障脆弱性从34.85%降至0%，而面积开销仅为文献报道的细粒度三倍复制的1/8。


<details>
  <summary>Details</summary>
Motivation: 在现代SoC中，互连是处理器核心与内存及外设通信的关键部件，但在高辐射环境下易受软错误影响，导致整个SoC功能失效。因此，设计一种完全可靠的互连方案至关重要。

Method: 通过扩展OBI协议，结合TMR保护关键握手信号，以及ECC保护其他信号，实现全面的可靠性。并在交叉开关中实现和测试这一方案。

Result: 测试结果显示，相比参考设计，该方案将故障脆弱性从34.85%降至0%，但面积增加了2.6倍，时序影响为1.4倍。其面积开销比文献中的细粒度三倍复制方案低1.8倍。

Conclusion: relOBI方案在保证高可靠性的同时，显著降低了面积开销，为高辐射环境下的SoC互连设计提供了有效解决方案。

Abstract: On-chip communication is a critical element of modern systems-on-chip (SoCs),
allowing processor cores to interact with memory and peripherals. Interconnects
require special care in radiation-heavy environments, as any soft error within
the SoC interconnect is likely to cause a functional failure of the whole SoC.
This work proposes relOBI, an extension to Open Bus Interface (OBI) combining
triple modular redundancy (TMR) for critical handshake signals with error
correction codes (ECC) protection on other signals for complete reliability.
Implementing and testing a fully reliable crossbar shows improved reliability
to injected faults from a vulnerability of 34.85 % to 0 % compared to a
reference design, with an area increase of 2.6x and 1.4x timing impact. The
area overhead is 1.8x lower than that reported in the literature for
fine-grained triplication and voting.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [69] [Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications](https://arxiv.org/abs/2508.05248)
*Pradeep Kumar Shukla,Tanujit Chakraborty,Mustafa Sari,Joel Sarout,Partha Pratim Mandal*

Main category: physics.geo-ph

TL;DR: 研究分析了盐岩在不同围压条件下的蠕变趋势预测方法，提出N-BEATS和TCN模型在预测性能上优于传统方法，提高了15-20%的准确率。


<details>
  <summary>Details</summary>
Motivation: 盐岩的蠕变评估对地下核废料、氢能等储存设施的设计至关重要，需准确预测其时间依赖性变形。

Method: 使用多阶段三轴蠕变数据，通过STL分解、Granger因果检验和DNN模型（N-BEATS、TCN、RNN、TF）进行分析与比较。

Result: N-BEATS和TCN模型在各应力水平下表现最佳，准确率优于传统模型15-20%。

Conclusion: 深度学习模型（特别是N-BEATS和TCN）能有效捕捉盐岩蠕变的复杂时间依赖性，为工程应用提供更精准的预测工具。

Abstract: This study provides an in-depth analysis of time series forecasting methods
to predict the time-dependent deformation trend (also known as creep) of salt
rock under varying confining pressure conditions. Creep deformation assessment
is essential for designing and operating underground storage facilities for
nuclear waste, hydrogen energy, or radioactive materials. Salt rocks, known for
their mechanical properties like low porosity, low permeability, high
ductility, and exceptional creep and self-healing capacities, were examined
using multi-stage triaxial (MSTL) creep data. After resampling, axial strain
datasets were recorded at 5--10 second intervals under confining pressure
levels ranging from 5 to 35 MPa over 5.8--21 days. Initial analyses, including
Seasonal-Trend Decomposition (STL) and Granger causality tests, revealed
minimal seasonality and causality between axial strain and temperature data.
Further statistical tests, such as the Augmented Dickey-Fuller (ADF) test,
confirmed the stationarity of the data with p-values less than 0.05, and
wavelet coherence plot (WCP) analysis indicated repeating trends. A suite of
deep neural network (DNN) models (Neural Basis Expansion Analysis for Time
Series (N-BEATS), Temporal Convolutional Networks (TCN), Recurrent Neural
Networks (RNN), and Transformers (TF)) was utilized and compared against
statistical baseline models. Predictive performance was evaluated using Root
Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage
Error (MAPE), and Symmetric Mean Absolute Percentage Error (SMAPE). Results
demonstrated that N-BEATS and TCN models outperformed others across various
stress levels, respectively. DNN models, particularly N-BEATS and TCN, showed a
15--20\% improvement in accuracy over traditional analytical models,
effectively capturing complex temporal dependencies and patterns.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [70] [Everything You Need to Know About CS Education: Open Results from a Survey of More Than 18,000 Participants](https://arxiv.org/abs/2508.05286)
*Katsiaryna Dzialets,Aleksandra Makeeva,Ilya Vlasov,Anna Potriasaeva,Aleksei Rostovskii,Yaroslav Golubev,Anastasiia Birillo*

Main category: cs.CY

TL;DR: 论文基于18,032名学习者的调查数据，更新了计算机科学教育领域的研究，涵盖多样主题，并公开数据集以推动未来研究。


<details>
  <summary>Details</summary>
Motivation: 现有大规模调查未能涵盖新兴趋势（如AI、IDE内学习）和学习者多样性，需要更新的全面研究。

Method: 对来自173个国家的18,032名学习者进行问卷调查，研究内容包括形式教育、学习方式、AI使用等。

Result: 提供了开放数据集，展示了学习挑战、新兴学习形式等多个研究方向的初步结果。

Conclusion: 数据集支持计算机教育领域的进一步研究，并推动技术进步。

Abstract: Computer science education is a dynamic field with many aspects that
influence the learner's path. While these aspects are usually studied in depth
separately, it is also important to carry out broader large-scale studies that
touch on many topics, because they allow us to put different results into each
other's perspective. Past large-scale surveys have provided valuable insights,
however, the emergence of new trends (e.g., AI), new learning formats (e.g.,
in-IDE learning), and the increasing learner diversity highlight the need for
an updated comprehensive study. To address this, we conducted a survey with
18,032 learners from 173 countries, ensuring diverse representation and
exploring a wide range of topics - formal education, learning formats, AI
usage, challenges, motivation, and more. This paper introduces the results of
this survey as an open dataset, describes our methodology and the survey
questions, and highlights, as a motivating example, three possible research
directions within this data: challenges in learning, emerging formats, and
insights into the in-IDE format. The dataset aims to support further research
and foster advancements in computer education.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [71] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
*Fenya Wasserroth,Eleftherios Avramidis,Vera Czehmann,Tanja Kojic,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 研究探讨在微软Hololens 2设备上为现有手语（DGS）虚拟形象添加调整功能的影响，发现尽管用户偏好可调整设置，但用户体验和理解性未显著提升。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解可调整虚拟形象对手语用户的理解性、用户体验及接受度的影响，并探索功能设计的改进空间。

Method: 通过分析专家用户与非可调虚拟形象的交互，评估理解性、用户体验和接受度。

Result: 用户偏好可调设置，但理解和用户体验未提升。系统在情感满意度上优于功能性，且可调虚拟形象带来更高压力。

Conclusion: 个性化不足，手语虚拟形象需默认易理解。建议改进嘴部和面部动画及交互界面，采用参与式设计。

Abstract: This paper presents an investigation into the impact of adding adjustment
features to an existing sign language (SL) avatar on a Microsoft Hololens 2
device. Through a detailed analysis of interactions of expert German Sign
Language (DGS) users with both adjustable and non-adjustable avatars in a
specific use case, this study identifies the key factors influencing the
comprehensibility, the user experience (UX), and the acceptability of such a
system. Despite user preference for adjustable settings, no significant
improvements in UX or comprehensibility were observed, which remained at low
levels, amid missing SL elements (mouthings and facial expressions) and
implementation issues (indistinct hand shapes, lack of feedback and menu
positioning). Hedonic quality was rated higher than pragmatic quality,
indicating that users found the system more emotionally or aesthetically
pleasing than functionally useful. Stress levels were higher for the adjustable
avatar, reflecting lower performance, greater effort and more frustration.
Additionally, concerns were raised about whether the Hololens adjustment
gestures are intuitive and easy to familiarise oneself with. While
acceptability of the concept of adjustability was generally positive, it was
strongly dependent on usability and animation quality. This study highlights
that personalisation alone is insufficient, and that SL avatars must be
comprehensible by default. Key recommendations include enhancing mouthing and
facial animation, improving interaction interfaces, and applying participatory
design.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [72] [Minimal Model Reasoning in Description Logics: Don't Try This at Home!](https://arxiv.org/abs/2508.05350)
*Federica Di Stefano,Quentin Manière,Magdalena Ortiz,Mantas Šimkus*

Main category: cs.AI

TL;DR: 论文研究了描述逻辑（DLs）中最小模型的推理问题，发现概念满足性在最小模型中是不可判定的，即使在简单的$​​\mathcal{EL}$中也成立。通过引入非循环条件，可以恢复可判定性并降低复杂度，同时对DL-Lite家族中的扩展进行了分析。


<details>
  <summary>Details</summary>
Motivation: 最小模型推理在知识表示中具有核心地位，但目前对描述逻辑中的最小模型问题理解有限。尤其是对所有谓词的最小扩展（纯最小模型）的研究较少，本文旨在填补这一空白。

Method: 研究了描述逻辑中纯最小模型的概念满足性问题，通过理论分析和复杂度证明，探讨了不可判定性及通过引入非循环条件恢复可判定的方法。

Result: 发现即使在简单的$​​\mathcal{EL}$中，概念满足性在最小模型中也是不可判定的。通过引入非循环条件，可以在双重指数时间内恢复可判定性。DL-Lite$_{\text{horn}}$的扩展被证明是ExpSpace难的。

Conclusion: 纯最小模型在描述逻辑中的研究揭示了其本质的高复杂度。尽管不可判定性普遍存在，但通过特定条件可以实现可判定性，扩展了对DL-Lite家族的理解。

Abstract: Reasoning with minimal models has always been at the core of many knowledge
representation techniques, but we still have only a limited understanding of
this problem in Description Logics (DLs). Minimization of some selected
predicates, letting the remaining predicates vary or be fixed, as proposed in
circumscription, has been explored and exhibits high complexity. The case of
`pure' minimal models, where the extension of all predicates must be minimal,
has remained largely uncharted. We address this problem in popular DLs and
obtain surprisingly negative results: concept satisfiability in minimal models
is undecidable already for $\mathcal{EL}$. This undecidability also extends to
a very restricted fragment of tuple-generating dependencies. To regain
decidability, we impose acyclicity conditions on the TBox that bring the
worst-case complexity below double exponential time and allow us to establish a
connection with the recently studied pointwise circumscription; we also derive
results in data complexity. We conclude with a brief excursion to the DL-Lite
family, where a positive result was known for DL-Lite$_{\text{core}}$, but our
investigation establishes ExpSpace-hardness already for its extension
DL-Lite$_{\text{horn}}$.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [73] [Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications](https://arxiv.org/abs/2508.04889)
*Theia Henderson,David R. Karger,David D. Clark*

Main category: cs.SI

TL;DR: Graffiti是一个可以轻松构建多样化社交应用并实现互操作的系统，支持用户在不同设计间自由切换而不丢失数据或朋友。


<details>
  <summary>Details</summary>
Motivation: 现有社交应用设计僵化且孤立，构建新应用技术难度高且难以与现有社区互通。

Method: 提出Graffiti系统，采用完全具体化概念和通道机制，支持客户端API和Vue.js插件开发多样化应用。

Result: 成功构建了类似Twitter、Messenger和Wikipedia的应用，并验证了互操作性和生态系统潜力。

Conclusion: Graffiti为社交应用设计提供了灵活且互操作的解决方案，支持多样化和创新。

Abstract: Most social applications, from Twitter to Wikipedia, have rigid
one-size-fits-all designs, but building new social applications is both
technically challenging and results in applications that are siloed away from
existing communities. We present Graffiti, a system that can be used to build a
wide variety of personalized social applications with relative ease that also
interoperate with each other. People can freely move between a plurality of
designs -- each with its own aesthetic, feature set, and moderation -- all
without losing their friends or data.
  Our concept of total reification makes it possible for seemingly
contradictory designs, including conflicting moderation rules, to interoperate.
Conversely, our concept of channels prevents interoperation from occurring by
accident, avoiding context collapse.
  Graffiti applications interact through a minimal client-side API, which we
show admits at least two decentralized implementations. Above the API, we built
a Vue.js plugin, which we use to develop applications similar to Twitter,
Messenger, and Wikipedia using only client-side code. Our case studies explore
how these and other novel applications interoperate, as well as the broader
ecosystem that Graffiti enables.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [74] [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
*Albert Yu,Chengshu Li,Luca Macesanu,Arnav Balaji,Ruchira Ray,Raymond Mooney,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: MICoBot是一个用于人机协作的系统，通过混合主动对话范式，实现任务分配和协作策略的动态调整，显著提高了任务成功率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 为适应不同人类伙伴的行为和需求变化，需要一个灵活的沟通机制来协调任务完成。

Method: 采用三层次决策：元规划器制定协作策略，规划器优化任务分配，执行器决定具体行动或对话。

Result: 在仿真和真实环境中验证，MICoBot显著优于纯LLM基准和其他分配模型。

Conclusion: MICoBot能有效协作多样化用户，提升任务成功率和用户体验。

Abstract: Effective robotic systems for long-horizon human-robot collaboration must
adapt to a wide range of human partners, whose physical behavior, willingness
to assist, and understanding of the robot's capabilities may change over time.
This demands a tightly coupled communication loop that grants both agents the
flexibility to propose, accept, or decline requests as they coordinate toward
completing the task effectively. We apply a Mixed-Initiative dialog paradigm to
Collaborative human-roBot teaming and propose MICoBot, a system that handles
the common scenario where both agents, using natural language, take initiative
in formulating, accepting, or rejecting proposals on who can best complete
different steps of a task. To handle diverse, task-directed dialog, and find
successful collaborative strategies that minimize human effort, MICoBot makes
decisions at three levels: (1) a meta-planner considers human dialog to
formulate and code a high-level collaboration strategy, (2) a planner optimally
allocates the remaining steps to either agent based on the robot's capabilities
(measured by a simulation-pretrained affordance model) and the human's
estimated availability to help, and (3) an action executor decides the
low-level actions to perform or words to say to the human. Our extensive
evaluations in simulation and real-world -- on a physical robot with 18 unique
human participants over 27 hours -- demonstrate the ability of our method to
effectively collaborate with diverse human users, yielding significantly
improved task success and user experience than a pure LLM baseline and other
agent allocation models. See additional videos and materials at
https://robin-lab.cs.utexas.edu/MicoBot/.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [75] [Does Multimodality Improve Recommender Systems as Expected? A Critical Analysis and Future Directions](https://arxiv.org/abs/2508.05377)
*Hongyu Zhou,Yinan Zhang,Aixin Sun,Zhiqi Shen*

Main category: cs.IR

TL;DR: 本文提出了一种评估框架，系统分析多模态推荐系统的性能，发现多模态数据在稀疏交互和召回阶段特别有效，且不同模态的重要性因任务而异。集成学习优于融合学习，大模型不一定更好。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态推荐系统何时及如何真正提升推荐效果，解决多模态数据整合的实际收益不明确的问题。

Method: 提出结构化评估框架，从四个维度（比较效率、推荐任务、推荐阶段、多模态数据整合）评估多模态模型，并与其他基线模型对比。

Result: 多模态数据在稀疏交互和召回阶段更有效；文本模态在电商中更重要，视觉模态在短视频推荐中更有效；集成学习表现优于融合学习，大模型未必更好。

Conclusion: 研究为构建高效多模态推荐系统提供实践指导，强调需谨慎选择模态、整合策略及模型设计。

Abstract: Multimodal recommendation systems are increasingly popular for their
potential to improve performance by integrating diverse data types. However,
the actual benefits of this integration remain unclear, raising questions about
when and how it truly enhances recommendations. In this paper, we propose a
structured evaluation framework to systematically assess multimodal
recommendations across four dimensions: Comparative Efficiency, Recommendation
Tasks, Recommendation Stages, and Multimodal Data Integration. We benchmark a
set of reproducible multimodal models against strong traditional baselines and
evaluate their performance on different platforms. Our findings show that
multimodal data is particularly beneficial in sparse interaction scenarios and
during the recall stage of recommendation pipelines. We also observe that the
importance of each modality is task-specific, where text features are more
useful in e-commerce and visual features are more effective in short-video
recommendations. Additionally, we explore different integration strategies and
model sizes, finding that Ensemble-Based Learning outperforms Fusion-Based
Learning, and that larger models do not necessarily deliver better results. To
deepen our understanding, we include case studies and review findings from
other recommendation domains. Our work provides practical insights for building
efficient and effective multimodal recommendation systems, emphasizing the need
for thoughtful modality selection, integration strategies, and model design.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [76] [A Design for an Early Quantum Network](https://arxiv.org/abs/2508.04967)
*Yuan Li,Chen Zhang,Hao Zhang,Tao Huang,Yunjie Liu*

Main category: quant-ph

TL;DR: 本文提出了一种兼容现有三种量子中继技术的早期量子网络设计方案，旨在满足多样化量子应用的需求，并通过模拟评估了噪声和参数不完美对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着量子信息技术的发展，量子网络需要满足多样化应用对保真度和请求完成时间等关键指标的严格要求，尤其是在量子资源有限和网络性能不理想的情况下。

Method: 提出了一个兼容三种量子中继技术的网络设计，描述了所需标识符和量子请求实现的具体过程，并通过基于离散事件建模的模拟验证设计的可行性。

Result: 模拟考虑了早期网络中可能存在的噪声和不完美参数，分析了这些参数对纠缠态保真度和请求完成时间的影响，并探讨了中央控制器额外的决策选项。

Conclusion: 该设计在资源有限的早期量子网络中具备可行性，并能通过优化控制器决策进一步提升性能。

Abstract: With the rapid advancement of quantum information technology, quantum
networks have become essential for supporting diverse applications, which often
have stringent demands for key metrics such as fidelity and request completion
time. In this work, we propose a design for early-stage quantum networks that
is compatible with the three existing quantum repeater technologies. The design
aims to maximize the ability of the network to accommodate the diverse needs of
quantum applications, even under conditions of limited quantum resources and
suboptimal network performance. We have also described the required identifiers
in the quantum network and the specific process for implementing quantum
requests. To assess the feasibility of our design, we conduct simulations based
on discrete-event modeling of quantum networks. The simulations consider
various types of noise and imperfect parameters that might exist in early-stage
networks. We analyze the impact of these parameters on the fidelity of the
generated entangled states and the request completion time. Furthermore, we
investigated additional decisions that the central controller can make beyond
path selection, such as the choice of cutoff time and the allocation of network
resources to requests.

</details>


### [77] [QFOR: A Fidelity-aware Orchestrator for Quantum Computing Environments using Deep Reinforcement Learning](https://arxiv.org/abs/2508.04974)
*Hoa T. Nguyen,Muhammad Usman,Rajkumar Buyya*

Main category: quant-ph

TL;DR: 量子云计算允许远程访问量子处理器，但量子硬件的异构性和噪声对资源调度提出了挑战。QFOR通过深度强化学习实现异构量子节点间的任务调度，平衡执行保真度和时间。


<details>
  <summary>Details</summary>
Motivation: 量子硬件的异构性和噪声导致现有启发式方法难以动态适应或优化任务分配，需要一种更高效的方法。

Method: 将量子任务调度建模为马尔可夫决策过程，并使用近端策略优化算法学习自适应调度策略，结合IBM量子处理器数据估计噪声。

Result: QFOR在保真度性能上相比启发式基线提升29.5-84%，同时保持可比的执行时间。

Conclusion: QFOR为量子云计算提供了一种自适应且高效的资源调度框架，平衡了保真度和成本效率。

Abstract: Quantum cloud computing enables remote access to quantum processors, yet the
heterogeneity and noise of available quantum hardware create significant
challenges for efficient resource orchestration. These issues complicate the
optimization of quantum task allocation and scheduling, as existing heuristic
methods fall short in adapting to dynamic conditions or effectively balancing
execution fidelity and time. Here, we propose QFOR, a Quantum Fidelity-aware
Orchestration of tasks across heterogeneous quantum nodes in cloud-based
environments using Deep Reinforcement learning. We model the quantum task
orchestration as a Markov Decision Process and employ the Proximal Policy
Optimization algorithm to learn adaptive scheduling policies, using IBM quantum
processor calibration data for noise-aware performance estimation. Our
configurable framework balances overall quantum task execution fidelity and
time, enabling adaptation to different operational priorities. Extensive
evaluation demonstrates that QFOR is adaptive and achieves significant
performance with 29.5-84% improvements in relative fidelity performance over
heuristic baselines. Furthermore, it maintains comparable quantum execution
times, contributing to cost-efficient use of quantum computation resources.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment](https://arxiv.org/abs/2508.04865)
*Aleksander Boruch-Gruszecki,Yangtian Zi,Zixuan Wu,Tejas Oberoi,Carolyn Jane Anderson,Joydeep Biswas,Arjun Guha*

Main category: cs.LG

TL;DR: Agnostics是一种语言无关的后训练流程，通过基于代码行为的通用验证器解决了低资源语言训练瓶颈，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源编程语言中训练不足的问题，避免为每种语言单独定制数据集和基础设施。

Method: 1. 用LLM将单元测试数据集转为I/O格式；2. 提供配置文件指导验证器编译和运行目标语言；3. 应用带验证奖励的强化学习（RLVR）。

Result: 在五种低资源语言中，Agnostics显著提升了模型性能，并为16B以下参数模型创造了新的SOTA结果。

Conclusion: Agnostics简化了多语言训练流程，提供了开源数据集和工具，推动低资源语言代码生成的发展。

Abstract: Large language models (LLMs) already excel at writing code in high-resource
languages such as Python and JavaScript, yet stumble on low-resource languages
that remain essential to science and engineering. Besides the obvious shortage
of pre-training data, post-training itself is a bottleneck: every new language
seems to require new datasets, test harnesses, and reinforcement-learning (RL)
infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that
eliminates this per-language engineering. The key idea is to judge code solely
by its externally observable behavior, so a single verifier can test solutions
written in any language. Concretely, we (i) use an LLM to rewrite existing
unit-test datasets into an I/O format, (ii) supply a short configuration that
tells the verifier how to compile and run a target language, and (iii) apply
reinforcement learning with verifiable rewards (RLVR) in a robust code
execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and
Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other
16B-70B open-weight models; (2) scales cleanly to larger and diverse model
families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for
${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on
MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X,
Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use
configurations, making RL post-training in any programming language as simple
as editing a short YAML file.

</details>


### [79] [HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation](https://arxiv.org/abs/2508.05135)
*Thinh Nguyen,Trung Phan,Binh T. Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: HFedATM proposes一种分层联邦域泛化方法，解决分层联邦学习中领域偏移问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 分层联邦学习（HFL）在解决可扩展性问题的同时，仍面临客户端和站点间数据分布差异（领域偏移）导致模型性能下降的问题。

Method: 提出了HFedATM方法，通过过滤器最优传输对齐（Filter-wise Optimal Transport Alignment）对齐不同站点的模型卷积核，再使用收缩感知正则化均值聚合（Shrinkage-aware Regularized Mean Aggregation）合并对齐的模型。

Result: 实验表明，HFedATM在多数据集上显著提升了联邦域泛化基线的性能，同时保持了计算和通信效率。

Conclusion: HFedATM不仅性能优越，理论分析还表明其收敛更快、训练更稳定，为分层联邦学习中的领域偏移问题提供了有效解决方案。

Abstract: Federated Learning (FL) is a decentralized approach where multiple clients
collaboratively train a shared global model without sharing their raw data.
Despite its effectiveness, conventional FL faces scalability challenges due to
excessive computational and communication demands placed on a single central
server as the number of participating devices grows. Hierarchical Federated
Learning (HFL) addresses these issues by distributing model aggregation tasks
across intermediate nodes (stations), thereby enhancing system scalability and
robustness against single points of failure. However, HFL still suffers from a
critical yet often overlooked limitation: domain shift, where data
distributions vary significantly across different clients and stations,
reducing model performance on unseen target domains. While Federated Domain
Generalization (FedDG) methods have emerged to improve robustness to domain
shifts, their integration into HFL frameworks remains largely unexplored. In
this paper, we formally introduce Hierarchical Federated Domain Generalization
(HFedDG), a novel scenario designed to investigate domain shift within
hierarchical architectures. Specifically, we propose HFedATM, a hierarchical
aggregation method that first aligns the convolutional filters of models from
different stations through Filter-wise Optimal Transport Alignment and
subsequently merges aligned models using a Shrinkage-aware Regularized Mean
Aggregation. Our extensive experimental evaluations demonstrate that HFedATM
significantly boosts the performance of existing FedDG baselines across
multiple datasets and maintains computational and communication efficiency.
Moreover, theoretical analyses indicate that HFedATM achieves tighter
generalization error bounds compared to standard hierarchical averaging,
resulting in faster convergence and stable training behavior.

</details>


### [80] [LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation](https://arxiv.org/abs/2508.04732)
*Xiaoqi Dong,Xiangyu Zhou,Nicholas Evans,Yujia Lin*

Main category: cs.LG

TL;DR: LumiGen是一个基于LVLM的迭代框架，通过智能提示解析和视觉反馈循环提升T2I模型的复杂指令处理和细粒度控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在复杂指令处理、细粒度控制和语义一致性方面存在不足，而LVLMs在多模态理解和指令跟随方面表现出色，因此提出结合两者的LumiGen框架。

Method: LumiGen包含智能提示解析与增强（IPPA）模块和迭代视觉反馈与优化（IVFR）模块，通过闭环反馈机制优化图像生成。

Result: 在LongBench-T2I Benchmark上，LumiGen平均得分为3.08，优于现有基线模型，尤其在文本渲染和姿态生成方面表现突出。

Conclusion: LVLM的集成显著提升了T2I模型的可控性和生成质量，验证了LumiGen框架的有效性。

Abstract: Text-to-Image (T2I) generation has made significant advancements with
diffusion models, yet challenges persist in handling complex instructions,
ensuring fine-grained content control, and maintaining deep semantic
consistency. Existing T2I models often struggle with tasks like accurate text
rendering, precise pose generation, or intricate compositional coherence.
Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in cross-modal understanding and instruction following. We propose
LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I
model performance, particularly in areas requiring fine-grained control,
through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an
Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt
enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which
acts as a "visual critic" to iteratively correct and optimize generated images.
Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a
superior average score of 3.08, outperforming state-of-the-art baselines.
Notably, our framework demonstrates significant improvements in critical
dimensions such as text rendering and pose expression, validating the
effectiveness of LVLM integration for more controllable and higher-quality
image generation.

</details>


### [81] [X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment](https://arxiv.org/abs/2508.05568)
*Qinghua Yao,Xiangrui Xu,Zhize Li*

Main category: cs.LG

TL;DR: X-VFL 是一个新的垂直联邦学习框架，解决了数据样本不对齐和缺失特征问题，并支持本地独立推理。


<details>
  <summary>Details</summary>
Motivation: 传统的垂直联邦学习（VFL）面临数据样本对齐和联合推理的限制，X-VFL旨在解决这些问题。

Method: 引入了两个新模块：Cross Completion (XCom) 完成缺失特征，Decision Subspace Alignment (DS-Align) 对齐决策子空间，支持本地推理。

Result: 实验表明，X-VFL在CIFAR-10和MIMIC-III数据集上分别提高了15%和43%的准确率。

Conclusion: X-VFL在部分缺失特征和本地推理场景中表现优异，优于现有方法。

Abstract: Vertical Federated Learning (VFL) enables collaborative learning by
integrating disjoint feature subsets from multiple clients/parties. However,
VFL typically faces two key challenges: i) the requirement for perfectly
aligned data samples across all clients (missing features are not allowed); ii)
the requirement for joint collaborative inference/prediction involving all
clients (it does not support locally independent inference on a single client).
To address these challenges, we propose X-VFL, a new VFL framework designed to
deal with the non-aligned data samples with (partially) missing features and to
support locally independent inference of new data samples for each client. In
particular, we design two novel modules in X-VFL: Cross Completion (XCom) and
Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other
clients. DS-Align aligns local features with completed and global features
across all clients within the decision subspace, thus enabling locally
independent inference at each client. Moreover, we provide convergence theorems
for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$
convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type
algorithms, where $T$ denotes the number of training update steps. Extensive
experiments on real-world datasets demonstrate that X-VFL significantly
outperforms existing methods, e.g., achieving a 15% improvement in accuracy on
the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III
dataset. These results validate the practical effectiveness and superiority of
X-VFL, particularly in scenarios involving partially missing features and
locally independent inference.

</details>


### [82] [Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality](https://arxiv.org/abs/2508.05025)
*Zhehan Qu,Tianyi Hu,Christian Fronk,Maria Gorlatova*

Main category: cs.LG

TL;DR: 论文研究了AR系统在急救场景中对认知隧道效应的影响，开发了AR应用评估情境意识（SA），并通过眼动追踪和机器学习模型预测SA水平。


<details>
  <summary>Details</summary>
Motivation: AR系统在提供实时指导的同时可能导致认知隧道效应，降低用户在安全关键场景中的情境意识，研究旨在解决这一问题。

Method: 开发了AR应用用于心肺复苏（CPR）指导，模拟突发事件并通过眼动追踪和问卷评估SA，提出FixGraphPool模型分析动态注意模式。

Result: 实验表明，SA水平与眼动特征相关，FixGraphPool模型在预测SA时表现优异（准确率83.0%，F1=81.0%）。

Conclusion: 研究证明了眼动追踪在AR系统中建模SA的潜力，为设计兼顾用户安全和情境意识的AR系统提供了参考。

Abstract: Augmented Reality (AR) systems, while enhancing task performance through
real-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on
virtual content that compromises situational awareness (SA) in safety-critical
scenarios. This paper investigates SA in AR-guided cardiopulmonary
resuscitation (CPR), where responders must balance effective compressions with
vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR
app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth
and rate) and conducted a user study with simulated unexpected incidents (e.g.,
bleeding) to evaluate SA, in which SA metrics were collected via observation
and questionnaires administered during freeze-probe events. Eye tracking
analysis revealed that higher SA levels were associated with greater saccadic
amplitude and velocity, and with reduced proportion and frequency of fixations
on virtual content. To predict SA, we propose FixGraphPool, a graph neural
network that structures gaze events (fixations, saccades) into spatiotemporal
graphs, effectively capturing dynamic attentional patterns. Our model achieved
83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and
state-of-the-art time-series models by leveraging domain knowledge and
spatial-temporal information encoded in ET data. These findings demonstrate the
potential of eye tracking for SA modeling in AR and highlight its utility in
designing AR systems that ensure user safety and situational awareness.

</details>


### [83] [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](https://arxiv.org/abs/2508.05310)
*Jelle Luijkx,Zlatan Ajanović,Laura Ferranti,Jens Kober*

Main category: cs.LG

TL;DR: ASkDAgger框架通过利用新手计划中的信息改进交互式模仿学习，减少人类教师的查询次数。


<details>
  <summary>Details</summary>
Motivation: 人类教学投入是交互式模仿学习推广的瓶颈，现有方法未能充分利用新手计划中的信息。

Method: ASkDAgger包含SAG、FIER和PIER三个组件，分别用于调整阈值、转化演示和优先重放。

Result: 实验验证了ASkDAgger在模拟和实际环境中能减少查询次数并提升性能。

Conclusion: ASkDAgger通过利用新手计划中的信息，提升了学习效率和适应性。

Abstract: Human teaching effort is a significant bottleneck for the broader
applicability of interactive imitation learning. To reduce the number of
required queries, existing methods employ active learning to query the human
teacher only in uncertain, risky, or novel situations. However, during these
queries, the novice's planned actions are not utilized despite containing
valuable information, such as the novice's capabilities, as well as
corresponding uncertainty levels. To this end, we allow the novice to say: "I
plan to do this, but I am uncertain." We introduce the Active Skill-level Data
Aggregation (ASkDAgger) framework, which leverages teacher feedback on the
novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating
threshold to track sensitivity, specificity, or a minimum success rate; (2)
Foresight Interactive Experience Replay (FIER), which recasts valid and
relabeled novice action plans into demonstrations; and (3) Prioritized
Interactive Experience Replay (PIER), which prioritizes replay based on
uncertainty, novice success, and demonstration age. Together, these components
balance query frequency with failure incidence, reduce the number of required
demonstration annotations, improve generalization, and speed up adaptation to
changing domains. We validate the effectiveness of ASkDAgger through
language-conditioned manipulation tasks in both simulation and real-world
environments. Code, data, and videos are available at
https://askdagger.github.io.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [84] [Probabilistic Alternating Simulations for Policy Synthesis in Uncertain Stochastic Dynamical Systems](https://arxiv.org/abs/2508.05062)
*Thom Badings,Alessandro Abate*

Main category: eess.SY

TL;DR: 扩展概率模拟关系以处理同时具有随机和非确定性扰动的系统，用于政策合成。


<details>
  <summary>Details</summary>
Motivation: 现有的概率模拟关系不足以处理同时具有随机和非确定性扰动的系统，因此需要扩展这些关系。

Method: 提出了一种受交替模拟启发的扩展概率模拟关系，用于处理随机和非确定性扰动。

Result: 通过实验验证了该方法在4D状态Dubins车辆中政策合成的适用性。

Conclusion: 新提出的关系扩展了现有的验证和政策合成方法，为复杂扰动系统的分析提供了工具。

Abstract: A classical approach to formal policy synthesis in stochastic dynamical
systems is to construct a finite-state abstraction, often represented as a
Markov decision process (MDP). The correctness of these approaches hinges on a
behavioural relation between the dynamical system and its abstraction, such as
a probabilistic simulation relation. However, probabilistic simulation
relations do not suffice when the system dynamics are, next to being
stochastic, also subject to nondeterministic (i.e., set-valued) disturbances.
In this work, we extend probabilistic simulation relations to systems with both
stochastic and nondeterministic disturbances. Our relation, which is inspired
by a notion of alternating simulation, generalises existing relations used for
verification and policy synthesis used in several works. Intuitively, our
relation allows reasoning probabilistically over stochastic uncertainty, while
reasoning robustly (i.e., adversarially) over nondeterministic disturbances. We
experimentally demonstrate the applicability of our relations for policy
synthesis in a 4D-state Dubins vehicle.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [85] [Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework](https://arxiv.org/abs/2508.04962)
*Peng Zhang,Songru Yang,Jinsheng Sun,Weiqing Li,Zhiyong Su*

Main category: cs.CV

TL;DR: HOW-Seg是一个基于人机协作的开世界点云语义分割框架，通过稀疏标注和分层原型消歧实现高质量分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法需依赖密集标注或离线增量学习，实际应用受限。旨在提出更实用的解决方案。

Method: 构建查询数据上的类原型，利用稀疏标注指导分割，引入分层消歧机制和密集CRF优化标签分配。

Result: 稀疏标注下性能匹配或超越现有方法；密集标注下，在S3DIS和ScanNetv2上取得显著提升。

Conclusion: HOW-Seg通过人机协作和动态优化，显著提升了开世界点云分割的实用性和性能。

Abstract: Open-world point cloud semantic segmentation (OW-Seg) aims to predict point
labels of both base and novel classes in real-world scenarios. However,
existing methods rely on resource-intensive offline incremental learning or
densely annotated support data, limiting their practicality. To address these
limitations, we propose HOW-Seg, the first human-in-the-loop framework for
OW-Seg. Specifically, we construct class prototypes, the fundamental
segmentation units, directly on the query data, avoiding the prototype bias
caused by intra-class distribution shifts between the support and query data.
By leveraging sparse human annotations as guidance, HOW-Seg enables
prototype-based segmentation for both base and novel classes. Considering the
lack of granularity of initial prototypes, we introduce a hierarchical
prototype disambiguation mechanism to refine ambiguous prototypes, which
correspond to annotations of different classes. To further enrich contextual
awareness, we employ a dense conditional random field (CRF) upon the refined
prototypes to optimize their label assignments. Through iterative human
feedback, HOW-Seg dynamically improves its predictions, achieving high-quality
segmentation for both base and novel classes. Experiments demonstrate that with
sparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches or
surpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg)
method under the 5-shot setting. When using advanced backbones (e.g.,
Stratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene),
HOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2,
significantly outperforming alternatives.

</details>
