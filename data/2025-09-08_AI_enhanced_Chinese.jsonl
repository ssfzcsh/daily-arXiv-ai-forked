{"id": "2509.04644", "pdf": "https://arxiv.org/pdf/2509.04644", "abs": "https://arxiv.org/abs/2509.04644", "authors": ["Subhang Boorlagadda", "Nitya Naga Sai Atluri", "Muhammet Mustafa Olmez", "Edward F. Gehringer"], "title": "Comparative Evaluation of Large Language Models for Test-Skeleton Generation", "categories": ["cs.SE"], "comment": "Forthcoming in Frontiers in Education (FIE 2025), Nashville,\n  Tennessee, USA, Nov 2-5, 2025", "summary": "This paper explores the use of Large Language Models (LLMs) to automate the\ngeneration of test skeletons -- structural templates that outline unit test\ncoverage without implementing full test logic. Test skeletons are especially\nimportant in test-driven development (TDD), where they provide an early\nframework for systematic verification. Traditionally authored manually, their\ncreation can be time-consuming and error-prone, particularly in educational or\nlarge-scale development settings. We evaluate four LLMs -- GPT-4,\nDeepSeek-Chat, Llama4-Maverick, and Gemma2-9B -- on their ability to generate\nRSpec skeletons for a real-world Ruby class developed in a university software\nengineering course. Each model's output is assessed using static analysis and a\nblind expert review to measure structural correctness, clarity,\nmaintainability, and conformance to testing best practices. The study reveals\nkey differences in how models interpret code structure and testing conventions,\noffering insights into the practical challenges of using LLMs for automated\ntest scaffolding. Our results show that DeepSeek generated the most\nmaintainable and well-structured skeletons, while GPT-4 produced more complete\nbut conventionally inconsistent output. The study reveals prompt design and\ncontextual input as key quality factors.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u9aa8\u67b6\u7684\u53ef\u884c\u6027\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u6a21\u578b\u5728\u751f\u6210RSpec\u6d4b\u8bd5\u9aa8\u67b6\u65f6\u7684\u6027\u80fd\uff0c\u53d1\u73b0DeepSeek\u8868\u73b0\u6700\u4f73\uff0c\u800cGPT-4\u8f93\u51fa\u5b8c\u6574\u4f46\u4e00\u81f4\u6027\u8f83\u5dee\u3002", "motivation": "\u6d4b\u8bd5\u9aa8\u67b6\u5728\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u7f16\u5199\u8017\u65f6\u4e14\u5bb9\u6613\u51fa\u9519\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLM\u5728\u8fd9\u4e00\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u8bc4\u4f30\u4e86GPT-4\u3001DeepSeek-Chat\u3001Llama4-Maverick\u548cGemma2-9B\u56db\u79cdLLM\u751f\u6210RSpec\u6d4b\u8bd5\u9aa8\u67b6\u7684\u80fd\u529b\uff0c\u91c7\u7528\u9759\u6001\u5206\u6790\u548c\u4e13\u5bb6\u76f2\u5ba1\u8861\u91cf\u8d28\u91cf\u3002", "result": "DeepSeek\u751f\u6210\u7684\u9aa8\u67b6\u6700\u6613\u7ef4\u62a4\u4e14\u7ed3\u6784\u826f\u597d\uff0cGPT-4\u8f93\u51fa\u5b8c\u6574\u4f46\u4e0e\u4f20\u7edf\u6d4b\u8bd5\u5b9e\u8df5\u4e0d\u4e00\u81f4\u3002\u63d0\u793a\u8bbe\u8ba1\u548c\u4e0a\u4e0b\u6587\u8f93\u5165\u662f\u5f71\u54cd\u8d28\u91cf\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "LLM\u53ef\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u9aa8\u67b6\uff0c\u4f46\u6548\u679c\u56e0\u6a21\u578b\u548c\u8bbe\u8ba1\u800c\u5f02\uff0c\u63d0\u793a\u8bbe\u8ba1\u548c\u4e0a\u4e0b\u6587\u4f18\u5316\u5bf9\u63d0\u5347\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.04721", "pdf": "https://arxiv.org/pdf/2509.04721", "abs": "https://arxiv.org/abs/2509.04721", "authors": ["Abhishek Dey", "Saurabh Srivastava", "Gaurav Singh", "Robert G. Pettit"], "title": "Real-Time Performance Benchmarking of TinyML Models in Embedded Systems (PICO: Performance of Inference, CPU, and Operations)", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "This paper presents PICO-TINYML-BENCHMARK, a modular and platform-agnostic\nframework for benchmarking the real-time performance of TinyML models on\nresource-constrained embedded systems. Evaluating key metrics such as inference\nlatency, CPU utilization, memory efficiency, and prediction stability, the\nframework provides insights into computational trade-offs and platform-specific\noptimizations. We benchmark three representative TinyML models -- Gesture\nClassification, Keyword Spotting, and MobileNet V2 -- on two widely adopted\nplatforms, BeagleBone AI64 and Raspberry Pi 4, using real-world datasets.\nResults reveal critical trade-offs: the BeagleBone AI64 demonstrates consistent\ninference latency for AI-specific tasks, while the Raspberry Pi 4 excels in\nresource efficiency and cost-effectiveness. These findings offer actionable\nguidance for optimizing TinyML deployments, bridging the gap between\ntheoretical advancements and practical applications in embedded systems.", "AI": {"tldr": "PICO-TINYML-BENCHMARK\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u4e14\u5e73\u53f0\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30TinyML\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3TinyML\u6a21\u578b\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u6027\u80fd\u8bc4\u4f30\u95ee\u9898\uff0c\u4e3a\u4f18\u5316\u90e8\u7f72\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u63a8\u7406\u5ef6\u8fdf\u3001CPU\u5229\u7528\u7387\u3001\u5185\u5b58\u6548\u7387\u548c\u9884\u6d4b\u7a33\u5b9a\u6027\u7b49\u5173\u952e\u6307\u6807\uff0c\u5bf9\u6bd4\u4e24\u79cd\u5e73\u53f0\uff08BeagleBone AI64\u548cRaspberry Pi 4\uff09\u4e0a\u4e09\u4e2a\u4ee3\u8868\u6027TinyML\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "BeagleBone AI64\u5728AI\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u800cRaspberry Pi 4\u5728\u8d44\u6e90\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u4e0a\u66f4\u4f18\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aTinyML\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u586b\u8865\u4e86\u7406\u8bba\u8fdb\u5c55\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.04763", "pdf": "https://arxiv.org/pdf/2509.04763", "abs": "https://arxiv.org/abs/2509.04763", "authors": ["Tiancheng Jin", "Shangzhou Xia", "Jianjun Zhao"], "title": "NovaQ: Improving Quantum Program Testing through Diversity-Guided Test Case Generation", "categories": ["cs.SE"], "comment": "5 pages", "summary": "Quantum programs are designed to run on quantum computers, leveraging quantum\ncircuits to solve problems that are intractable for classical machines. As\nquantum computing advances, ensuring the reliability of quantum programs has\nbecome increasingly important. This paper introduces NovaQ, a diversity-guided\ntesting framework for quantum programs. NovaQ combines a distribution-based\ntest case generator with a novelty-driven evaluation module. The generator\nproduces diverse quantum state inputs by mutating circuit parameters, while the\nevaluator quantifies behavioral novelty based on internal circuit state\nmetrics, including magnitude, phase, and entanglement. By selecting inputs that\nmap to infrequently covered regions in the metric space, NovaQ effectively\nexplores under-tested program behaviors. We evaluate NovaQ on quantum programs\nof varying sizes and complexities. Experimental results show that NovaQ\nconsistently achieves higher test input diversity and detects more bugs than\nexisting baseline approaches.", "AI": {"tldr": "NovaQ\u662f\u4e00\u4e2a\u7528\u4e8e\u91cf\u5b50\u7a0b\u5e8f\u7684\u591a\u6837\u6027\u5f15\u5bfc\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u91cf\u5b50\u72b6\u6001\u8f93\u5165\u5e76\u7ed3\u5408\u65b0\u9896\u6027\u8bc4\u4f30\u6a21\u5757\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u7684\u53d1\u5c55\uff0c\u786e\u4fdd\u91cf\u5b50\u7a0b\u5e8f\u7684\u53ef\u9760\u6027\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7531\u4e8e\u91cf\u5b50\u7a0b\u5e8f\u7684\u7279\u6b8a\u6027\uff0c\u4f20\u7edf\u7684\u6d4b\u8bd5\u65b9\u6cd5\u53ef\u80fd\u4e0d\u8db3\u4ee5\u8986\u76d6\u5176\u590d\u6742\u884c\u4e3a\u3002", "method": "NovaQ\u7ed3\u5408\u4e86\u57fa\u4e8e\u5206\u5e03\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5668\u548c\u65b0\u9896\u6027\u9a71\u52a8\u7684\u8bc4\u4f30\u6a21\u5757\u3002\u751f\u6210\u5668\u901a\u8fc7\u53d8\u5f02\u7535\u8def\u53c2\u6570\u4ea7\u751f\u591a\u6837\u5316\u7684\u91cf\u5b50\u72b6\u6001\u8f93\u5165\uff0c\u8bc4\u4f30\u5668\u5219\u6839\u636e\u5185\u90e8\u7535\u8def\u72b6\u6001\u6307\u6807\uff08\u5982\u5e45\u5ea6\u3001\u76f8\u4f4d\u548c\u7ea0\u7f20\uff09\u91cf\u5316\u884c\u4e3a\u65b0\u9896\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cNovaQ\u5728\u591a\u6837\u6027\u548c\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u548c\u590d\u6742\u5ea6\u7684\u91cf\u5b50\u7a0b\u5e8f\u3002", "conclusion": "NovaQ\u901a\u8fc7\u591a\u6837\u6027\u5f15\u5bfc\u7684\u6d4b\u8bd5\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5b50\u7a0b\u5e8f\u7684\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.04810", "pdf": "https://arxiv.org/pdf/2509.04810", "abs": "https://arxiv.org/abs/2509.04810", "authors": ["Yogev Cohen", "Dudi Ohayon", "Romy Somkin", "Yehudit Aperstein", "Alexander Apartsin"], "title": "Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "4 pages, 1 figure", "summary": "Automating the decision of whether a code change requires manual review is\nvital for maintaining software quality in modern development workflows.\nHowever, the emergence of new programming languages and frameworks creates a\ncritical bottleneck: while large volumes of unlabelled code are readily\navailable, there is an insufficient amount of labelled data to train supervised\nmodels for review classification. We address this challenge by leveraging Large\nLanguage Models (LLMs) to translate code changes from well-resourced languages\ninto equivalent changes in underrepresented or emerging languages, generating\nsynthetic training data where labelled examples are scarce. We assume that\nalthough LLMs have learned the syntax and semantics of new languages from\navailable unlabelled code, they have yet to fully grasp which code changes are\nconsidered significant or review-worthy within these emerging ecosystems. To\novercome this, we use LLMs to generate synthetic change examples and train\nsupervised classifiers on them. We systematically compare the performance of\nthese classifiers against models trained on real labelled data. Our experiments\nacross multiple GitHub repositories and language pairs demonstrate that\nLLM-generated synthetic data can effectively bootstrap review recommendation\nsystems, narrowing the performance gap even in low-resource settings. This\napproach provides a scalable pathway to extend automated code review\ncapabilities to rapidly evolving technology stacks, even in the absence of\nannotated data.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u65b0\u5174\u7f16\u7a0b\u8bed\u8a00\u4e2d\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ece\u800c\u8bad\u7ec3\u4ee3\u7801\u5ba1\u67e5\u63a8\u8350\u7cfb\u7edf\u3002", "motivation": "\u73b0\u4ee3\u5f00\u53d1\u6d41\u7a0b\u4e2d\uff0c\u81ea\u52a8\u5316\u51b3\u5b9a\u4ee3\u7801\u53d8\u66f4\u662f\u5426\u9700\u8981\u4eba\u5de5\u5ba1\u67e5\u5bf9\u7ef4\u62a4\u8f6f\u4ef6\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u65b0\u5174\u7f16\u7a0b\u8bed\u8a00\u548c\u6846\u67b6\u7f3a\u4e4f\u8db3\u591f\u7684\u6807\u8bb0\u6570\u636e\u6765\u8bad\u7ec3\u76d1\u7763\u6a21\u578b\u3002", "method": "\u901a\u8fc7LLMs\u5c06\u8d44\u6e90\u4e30\u5bcc\u8bed\u8a00\u7684\u4ee3\u7801\u53d8\u66f4\u8f6c\u6362\u4e3a\u65b0\u5174\u8bed\u8a00\u7684\u7b49\u6548\u53d8\u66f4\uff0c\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002\u540c\u65f6\u8bad\u7ec3\u76d1\u7763\u5206\u7c7b\u5668\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u80fd\u6709\u6548\u63d0\u5347\u5ba1\u67e5\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u7f29\u5c0f\u4e0e\u771f\u5b9e\u6807\u8bb0\u6570\u636e\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6269\u5c55\u81ea\u52a8\u5316\u4ee3\u7801\u5ba1\u67e5\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u6807\u8bb0\u6570\u636e\u7684\u5feb\u901f\u6f14\u53d8\u7684\u6280\u6727\u6808\u4e2d\u3002"}}
{"id": "2509.04625", "pdf": "https://arxiv.org/pdf/2509.04625", "abs": "https://arxiv.org/abs/2509.04625", "authors": ["Zhenzhou Qi", "Chung-Hsuan Tung", "Zhihui Gao", "Tingjun Chen"], "title": "NEXUS: Efficient and Scalable Multi-Cell mmWave Baseband Processing with Heterogeneous Compute", "categories": ["cs.NI"], "comment": null, "summary": "The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave\n(mmWave) spectrum, imposes stringent demands on the flexibility, scalability,\nand efficiency of baseband processing. While virtualized Radio Access Networks\n(vRANs) enable dynamic spectrum sharing across cells, compute resource\nallocation for baseband processing, especially in multi-cell deployments with\nheterogeneous workloads, remains underexplored. In this paper, we present\nNEXUS, the first system to realize real-time, virtualized multi-cell mmWave\nbaseband processing on a single server with heterogeneous compute resources.\nNEXUS integrates software-based digital signal processing pipelines with\nhardware-accelerated LDPC decoding, and introduces a novel framework for\nsharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions\n(VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based\nmodel that predicts the most energy-efficient resource allocation for the given\ncell configuration with microsecond-level inference latency and high accuracy.\nFor multi-cell scenarios, NEXUS introduces a power-aware scheduler that\nincorporates a lightweight contention model to adjust resource allocation\nstrategies under concurrent execution. Through extensive evaluation across\nvarious Frequency Range 2 (FR2) cell configurations, we show that NEXUS\nsupports up to 16 concurrent cells under full load, achieving 5.37Gbps\naggregate throughput, while reducing the multi-cell scheduling search space by\norders of magnitude. These results demonstrate that virtualized, resource-aware\nbaseband processing is both practical and efficient for next-generation vRAN\nsystems.", "AI": {"tldr": "NEXUS\u662f\u4e00\u4e2a\u5728\u5355\u670d\u52a1\u5668\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u3001\u865a\u62df\u5316\u591a\u5c0f\u533a\u6beb\u7c73\u6ce2\u57fa\u5e26\u5904\u7406\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\u548c\u521b\u65b0\u7684\u8c03\u5ea6\u6846\u67b6\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "5G NR\u7684\u9ad8\u901f\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u6beb\u7c73\u6ce2\u9891\u8c31\u7684\u5e94\u7528\uff0c\u5bf9\u57fa\u5e26\u5904\u7406\u7684\u7075\u6d3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u591a\u5c0f\u533a\u90e8\u7f72\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002", "method": "NEXUS\u6574\u5408\u4e86\u8f6f\u4ef6\u6570\u5b57\u4fe1\u53f7\u5904\u7406\u7ba1\u9053\u548c\u786c\u4ef6\u52a0\u901f\u7684LDPC\u89e3\u7801\uff0c\u5e76\u5229\u7528\u865a\u62df\u529f\u80fd\uff08VFs\uff09\u5728\u591a\u4e2aCPU\u6838\u5fc3\u95f4\u5171\u4eabACC100 eASIC\u3002\u91c7\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\u9884\u6d4b\u5355\u5c0f\u533a\u8d44\u6e90\u5206\u914d\uff0c\u5f00\u53d1\u8f7b\u91cf\u7ea7\u7ade\u4e89\u6a21\u578b\u8c03\u5ea6\u591a\u5c0f\u533a\u8d44\u6e90\u3002", "result": "NEXUS\u652f\u6301\u6700\u591a16\u4e2a\u5e76\u53d1\u5c0f\u533a\uff0c\u603b\u541e\u5410\u91cf\u8fbe5.37Gbps\uff0c\u540c\u65f6\u5c06\u591a\u5c0f\u533a\u8c03\u5ea6\u7684\u641c\u7d22\u7a7a\u95f4\u51cf\u5c11\u591a\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "NEXUS\u8bc1\u660e\u4e86\u865a\u62df\u5316\u3001\u8d44\u6e90\u611f\u77e5\u7684\u57fa\u5e26\u5904\u7406\u5bf9\u4e0b\u4e00\u4ee3vRAN\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2509.04481", "pdf": "https://arxiv.org/pdf/2509.04481", "abs": "https://arxiv.org/abs/2509.04481", "authors": ["Yi-Chun Chen", "Arnav Jhala"], "title": "Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments", "categories": ["cs.GR", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Recent advances in large language models(LLMs) enable compelling story\ngeneration, but connecting narrative text to playable visual environments\nremains an open challenge in procedural content generation(PCG). We present a\nlightweight pipeline that transforms short narrative prompts into a sequence of\n2D tile-based game scenes, reflecting the temporal structure of stories. Given\nan LLM-generated narrative, our system identifies three key time frames,\nextracts spatial predicates in the form of \"Object-Relation-Object\" triples,\nand retrieves visual assets using affordance-aware semantic embeddings from the\nGameTileNet dataset. A layered terrain is generated using Cellular Automata,\nand objects are placed using spatial rules grounded in the predicate structure.\nWe evaluated our system in ten diverse stories, analyzing tile-object matching,\naffordance-layer alignment, and spatial constraint satisfaction across frames.\nThis prototype offers a scalable approach to narrative-driven scene generation\nand lays the foundation for future work on multi-frame continuity, symbolic\ntracking, and multi-agent coordination in story-centered PCG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6d41\u7a0b\uff0c\u5c06\u77ed\u6587\u672c\u6545\u4e8b\u8f6c\u6362\u4e3a2D\u6e38\u620f\u573a\u666f\u5e8f\u5217\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u6a21\u578b\u548c\u7a0b\u5e8f\u751f\u6210\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u53d9\u4e8b\u6587\u672c\u4e0e\u53ef\u73a9\u89c6\u89c9\u73af\u5883\u8fde\u63a5\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u7a0b\u5e8f\u5316\u5185\u5bb9\u751f\u6210\u7684\u53d1\u5c55\u3002", "method": "\u7cfb\u7edf\u4eceLLM\u751f\u6210\u7684\u6545\u4e8b\u4e2d\u63d0\u53d6\u5173\u952e\u65f6\u95f4\u5e27\u548c\u7a7a\u95f4\u8c13\u8bcd\uff08\u4e09\u5143\u7ec4\uff09\uff0c\u5229\u7528GameTileNet\u6570\u636e\u96c6\u68c0\u7d22\u89c6\u89c9\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u5143\u80de\u81ea\u52a8\u673a\u751f\u6210\u5730\u5f62\u548c\u5bf9\u8c61\u5e03\u5c40\u3002", "result": "\u5728\u5341\u4e2a\u591a\u6837\u5316\u6545\u4e8b\u4e2d\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u74e6\u7247\u5bf9\u8c61\u5339\u914d\u3001\u529f\u80fd\u5c42\u5bf9\u9f50\u548c\u7a7a\u95f4\u7ea6\u675f\u6ee1\u8db3\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u539f\u578b\u4e3a\u53d9\u4e8b\u9a71\u52a8\u7684\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u591a\u5e27\u8fde\u7eed\u6027\u3001\u7b26\u53f7\u8ffd\u8e2a\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.04719", "pdf": "https://arxiv.org/pdf/2509.04719", "abs": "https://arxiv.org/abs/2509.04719", "authors": ["Han Liang", "Jiahui Zhou", "Zicheng Zhou", "Xiaoxi Zhang", "Xu Chen"], "title": "STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs", "categories": ["cs.DC", "cs.CV"], "comment": null, "summary": "The escalating adoption of diffusion models for applications such as image\ngeneration demands efficient parallel inference techniques to manage their\nsubstantial computational cost. However, existing diffusion parallelism\ninference schemes often underutilize resources in heterogeneous multi-GPU\nenvironments, where varying hardware capabilities or background tasks cause\nworkload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion\nInference (STADI), a novel framework to accelerate diffusion model inference in\nsuch settings. At its core is a hybrid scheduler that orchestrates fine-grained\nparallelism across both temporal and spatial dimensions. Temporally, STADI\nintroduces a novel computation-aware step allocator applied after warmup\nphases, using a least-common-multiple-minimizing quantization technique to\nreduce denoising steps on slower GPUs and execution synchronization. To further\nminimize GPU idle periods, STADI executes an elastic patch parallelism\nmechanism that allocates variably sized image patches to GPUs according to\ntheir computational capability, ensuring balanced workload distribution through\na complementary spatial mechanism. Extensive experiments on both\nload-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,\ndemonstrating improved load balancing and mitigation of performance\nbottlenecks. Compared to patch parallelism, a state-of-the-art diffusion\ninference framework, our method significantly reduces end-to-end inference\nlatency by up to 45% and significantly improves resource utilization on\nheterogeneous GPUs.", "AI": {"tldr": "STADI\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u7a7a\u81ea\u9002\u5e94\u8c03\u5ea6\u52a0\u901f\u5f02\u6784\u591aGPU\u73af\u5883\u4e2d\u7684\u6269\u6563\u6a21\u578b\u63a8\u7406\uff0c\u51cf\u5c11\u5ef6\u8fdf45%\u5e76\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u5e76\u884c\u63a8\u7406\u65b9\u6cd5\u5728\u5f02\u6784\u591aGPU\u73af\u5883\u4e0b\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u548c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8c03\u5ea6\u5668\uff0c\u7ed3\u5408\u8ba1\u7b97\u611f\u77e5\u7684\u6b65\u9aa4\u5206\u914d\u5668\u548c\u5f39\u6027\u8865\u4e01\u5e76\u884c\u673a\u5236\uff0c\u4f18\u5316\u65f6\u7a7a\u7ef4\u5ea6\u7684\u8d1f\u8f7d\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSTADI\u663e\u8457\u63d0\u5347\u8d1f\u8f7d\u5747\u8861\uff0c\u51cf\u5c11\u7aef\u5230\u7aef\u63a8\u7406\u5ef6\u8fdf45%\uff0c\u5e76\u6539\u5584\u5f02\u6784GPU\u7684\u8d44\u6e90\u5229\u7528\u7387\u3002", "conclusion": "STADI\u5728\u5f02\u6784\u73af\u5883\u4e2d\u9ad8\u6548\u52a0\u901f\u6269\u6563\u6a21\u578b\u63a8\u7406\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.04632", "pdf": "https://arxiv.org/pdf/2509.04632", "abs": "https://arxiv.org/abs/2509.04632", "authors": ["Zhenyu Wu", "Jiaoyan Chen", "Norman W. Paton"], "title": "Schema Inference for Tabular Data Repositories Using Large Language Models", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Minimally curated tabular data often contain representational inconsistencies\nacross heterogeneous sources, and are accompanied by sparse metadata. Working\nwith such data is intimidating. While prior work has advanced dataset discovery\nand exploration, schema inference remains difficult when metadata are limited.\nWe present SI-LLM (Schema Inference using Large Language Models), which infers\na concise conceptual schema for tabular data using only column headers and cell\nvalues. The inferred schema comprises hierarchical entity types, attributes,\nand inter-type relationships. In extensive evaluation on two datasets from web\ntables and open data, SI-LLM achieves promising end-to-end results, as well as\nbetter or comparable results to state-of-the-art methods at each step. All\nsource code, full prompts, and datasets of SI-LLM are available at\nhttps://github.com/PierreWoL/SILLM.", "AI": {"tldr": "SI-LLM\uff08\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u5f0f\u63a8\u65ad\uff09\u901a\u8fc7\u5217\u6807\u9898\u548c\u5355\u5143\u683c\u503c\u63a8\u65ad\u8868\u683c\u6570\u636e\u7684\u7b80\u660e\u6982\u5ff5\u6a21\u5f0f\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u7531\u4e8e\u5f02\u6784\u6570\u636e\u6e90\u7684\u8868\u793a\u4e0d\u4e00\u81f4\u548c\u5143\u6570\u636e\u7a00\u758f\uff0c\u5904\u7406\u6b64\u7c7b\u8868\u683c\u6570\u636e\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7814\u7a76\u5728\u6a21\u5f0f\u63a8\u65ad\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "method": "SI-LLM\u4ec5\u5229\u7528\u5217\u6807\u9898\u548c\u5355\u5143\u683c\u503c\uff0c\u63a8\u65ad\u51fa\u5305\u542b\u5c42\u6b21\u5316\u5b9e\u4f53\u7c7b\u578b\u3001\u5c5e\u6027\u548c\u7c7b\u578b\u95f4\u5173\u7cfb\u7684\u6982\u5ff5\u6a21\u5f0f\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cSI-LLM\u5728\u7aef\u5230\u7aef\u7ed3\u679c\u548c\u5404\u6b65\u9aa4\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "SI-LLM\u4e3a\u7a00\u758f\u5143\u6570\u636e\u4e0b\u7684\u8868\u683c\u6570\u636e\u6a21\u5f0f\u63a8\u65ad\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u6709\u8d44\u6e90\u548c\u4ee3\u7801\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.04506", "pdf": "https://arxiv.org/pdf/2509.04506", "abs": "https://arxiv.org/abs/2509.04506", "authors": ["Zacharia A. Rudge", "Dominik Dold", "Moritz Fieback", "Dario Izzo", "Said Hamdioui"], "title": "Memristor-Based Neural Network Accelerators for Space Applications: Enhancing Performance with Temporal Averaging and SIRENs", "categories": ["eess.SY", "cs.AI", "cs.AR", "cs.SY"], "comment": "21 pages, IAA acta astronautica. arXiv admin note: text overlap with\n  arXiv:2509.02369", "summary": "Memristors are an emerging technology that enables artificial intelligence\n(AI) accelerators with high energy efficiency and radiation robustness --\nproperties that are vital for the deployment of AI on-board spacecraft.\nHowever, space applications require reliable and precise computations, while\nmemristive devices suffer from non-idealities, such as device variability,\nconductance drifts, and device faults. Thus, porting neural networks (NNs) to\nmemristive devices often faces the challenge of severe performance degradation.\nIn this work, we show in simulations that memristor-based NNs achieve\ncompetitive performance levels on on-board tasks, such as navigation \\& control\nand geodesy of asteroids. Through bit-slicing, temporal averaging of NN layers,\nand periodic activation functions, we improve initial results from around\n$0.07$ to $0.01$ and $0.3$ to $0.007$ for both tasks using RRAM devices, coming\nclose to state-of-the-art levels ($0.003-0.005$ and $0.003$, respectively). Our\nresults demonstrate the potential of memristors for on-board space\napplications, and we are convinced that future technology and NN improvements\nwill further close the performance gap to fully unlock the benefits of\nmemristors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5fc6\u963b\u5668\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u822a\u5929\u5668\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u6280\u672f\u6539\u8fdb\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5fc6\u963b\u5668\u56e0\u5176\u9ad8\u80fd\u6548\u548c\u6297\u8f90\u5c04\u7279\u6027\uff0c\u9002\u5408\u7528\u4e8e\u822a\u5929\u5668\u4e0a\u7684\u4eba\u5de5\u667a\u80fd\u52a0\u901f\u5668\uff0c\u4f46\u5176\u975e\u7406\u60f3\u6027\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4f4d\u5207\u7247\u3001NN\u5c42\u7684\u65f6\u95f4\u5e73\u5747\u548c\u5468\u671f\u6027\u6fc0\u6d3b\u51fd\u6570\u7b49\u6280\u672f\uff0c\u5bf9\u5fc6\u963b\u5668\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u521d\u59cb\u6027\u80fd\u4ece0.07\u548c0.3\u5206\u522b\u63d0\u5347\u81f30.01\u548c0.007\uff0c\u63a5\u8fd1\u73b0\u6709\u6280\u672f\u7684\u6700\u4f18\u6c34\u5e73\uff080.003-0.005\u548c0.003\uff09\u3002", "conclusion": "\u5fc6\u963b\u5668\u5728\u822a\u5929\u5e94\u7528\u4e2d\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u7684\u6280\u672f\u548c\u795e\u7ecf\u7f51\u7edc\u6539\u8fdb\u5c06\u8fdb\u4e00\u6b65\u7f29\u5c0f\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2509.04936", "pdf": "https://arxiv.org/pdf/2509.04936", "abs": "https://arxiv.org/abs/2509.04936", "authors": ["Andrea Gilot", "Tobias Wrigstad", "Eva Darulova"], "title": "A Large-Scale Study of Floating-Point Usage in Statically Typed Languages", "categories": ["cs.PL", "cs.SE"], "comment": null, "summary": "Reasoning about floating-point arithmetic is notoriously hard. While static\nand dynamic analysis techniques or program repair have made significant\nprogress, more work is still needed to make them relevant to real-world code.\nOn the critical path to that goal is understanding what real-world\nfloating-point code looks like. To close that knowledge gap, this paper\npresents the first large-scale empirical study of floating-point arithmetic\nusage in statically typed languages across public GitHub repositories. We\nfollow state-of the art mining practices including random sampling and\nfiltering based on only intrinsic properties to avoid bias, and identify\nfloating-point usage by searching for keywords in the source code, and\nprogramming language constructs (e.g., loops) by parsing the code. Our\nevaluation supports the claim often made in papers that floating-point\narithmetic is widely used. Comparing statistics such as size and usage of\ncertain constructs and functions, we find that benchmarks used in literature to\nevaluate automated reasoning techniques for floating-point arithmetic are in\ncertain aspects representative of 'real-world' code, but not in all. We aim for\nour study and dataset to help future techniques for floating-point arithmetic\nto be designed and evaluated to match actual users' expectations.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u6d6e\u70b9\u7b97\u672f\u5728\u9759\u6001\u7c7b\u578b\u8bed\u8a00\u4e2d\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u7684\u8bba\u6587\uff0c\u65e8\u5728\u586b\u8865\u5bf9\u73b0\u5b9e\u4e16\u754c\u4ee3\u7801\u4e2d\u6d6e\u70b9\u8fd0\u7b97\u4f7f\u7528\u60c5\u51b5\u7684\u77e5\u8bc6\u7a7a\u767d\u3002", "motivation": "\u7406\u89e3\u73b0\u5b9e\u4e16\u754c\u6d6e\u70b9\u4ee3\u7801\u7684\u771f\u5b9e\u9762\u8c8c\uff0c\u4ee5\u63a8\u52a8\u9759\u6001\u548c\u52a8\u6001\u5206\u6790\u6216\u7a0b\u5e8f\u4fee\u590d\u6280\u672f\u5728\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u968f\u673a\u62bd\u6837\u548c\u57fa\u4e8e\u5185\u5728\u5c5e\u6027\u7684\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u907f\u514d\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u5173\u952e\u8bcd\u641c\u7d22\u548c\u4ee3\u7801\u89e3\u6790\u8bc6\u522b\u6d6e\u70b9\u8fd0\u7b97\u548c\u4f7f\u7528\u6784\u9020\uff08\u5982\u5faa\u73af\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6d6e\u70b9\u7b97\u672f\u5e7f\u6cdb\u4f7f\u7528\uff0c\u540c\u65f6\u6587\u732e\u4e2d\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u63a8\u7406\u6280\u672f\u7684\u57fa\u51c6\u5728\u67d0\u4e9b\u65b9\u9762\u5177\u6709\u4ee3\u8868\u6027\uff0c\u4f46\u5e76\u975e\u6240\u6709\u65b9\u9762\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u53ca\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u672a\u6765\u6d6e\u70b9\u7b97\u672f\u6280\u672f\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\uff0c\u4ee5\u66f4\u8d34\u8fd1\u5b9e\u9645\u7528\u6237\u7684\u9700\u6c42\u3002"}}
{"id": "2509.04594", "pdf": "https://arxiv.org/pdf/2509.04594", "abs": "https://arxiv.org/abs/2509.04594", "authors": ["Ethan Davis"], "title": "High Performance Matrix Multiplication", "categories": ["cs.PF"], "comment": null, "summary": "Matrix multiplication is the foundation from much of the success from high\nperformance technologies like deep learning, scientific simulations, and video\ngraphics. High level programming languages like Python and R rely on highly\noptimized low level libraries for performing core linear algebra operations\nlike matrix multiplication from Basic Linear Algebra Subprograms (BLAS). This\npaper compares the performance of five different matrix multiplication\nalgorithms using CuBLAS, CUDA, BLAS, OpenMP, and C++ Threads. We find\nstatistical significance with a p-value below 5e-12 to support the hypothesis\nthat for square $N \\times N$ matrices where $N$ is at least 10,000 then the in\norder performance as measured in floating point operations per second (FLOPS)\nfor these matrix multiplication algorithms is CuBLAS, CUDA, BLAS, OpenMP, and\nC++ Threads.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e94\u79cd\u77e9\u9635\u4e58\u6cd5\u7b97\u6cd5\uff08CuBLAS\u3001CUDA\u3001BLAS\u3001OpenMP\u548cC++ Threads\uff09\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5bf9\u4e8e\u81f3\u5c1110,000\u00d710,000\u7684\u65b9\u9635\uff0c\u6027\u80fd\u6392\u5e8f\u4e3aCuBLAS > CUDA > BLAS > OpenMP > C++ Threads\u3002", "motivation": "\u77e9\u9635\u4e58\u6cd5\u662f\u6df1\u5ea6\u5b66\u4e60\u3001\u79d1\u5b66\u6a21\u62df\u548c\u89c6\u9891\u56fe\u5f62\u7b49\u9ad8\u6027\u80fd\u6280\u672f\u7684\u57fa\u7840\u3002\u7814\u7a76\u4e0d\u540c\u7b97\u6cd5\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u7684\u8868\u73b0\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u8fd9\u4e9b\u5e94\u7528\u3002", "method": "\u4f7f\u7528CuBLAS\u3001CUDA\u3001BLAS\u3001OpenMP\u548cC++ Threads\u4e94\u79cd\u7b97\u6cd5\uff0c\u5bf9\u5927\u89c4\u6a21\u65b9\u9635\uff08N \u2265 10,000\uff09\u8fdb\u884c\u4e58\u6cd5\u8fd0\u7b97\uff0c\u5e76\u4ee5FLOPS\u8861\u91cf\u6027\u80fd\u3002", "result": "\u7edf\u8ba1\u663e\u8457\u6027\u663e\u793a\uff08p\u503c < 5e-12\uff09\uff0c\u6027\u80fd\u6392\u5e8f\u4e3aCuBLAS > CUDA > BLAS > OpenMP > C++ Threads\u3002", "conclusion": "CuBLAS\u5728\u77e9\u9635\u4e58\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u9009\u62e9\u4f9d\u636e\u3002"}}
{"id": "2509.04844", "pdf": "https://arxiv.org/pdf/2509.04844", "abs": "https://arxiv.org/abs/2509.04844", "authors": ["Xinkui Lin", "Yongxiu Xu", "Minghao Tang", "Shilong Zhang", "Hongbo Xu", "Hao Xu", "Yubin Wang"], "title": "REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts", "categories": ["cs.MM", "cs.AI", "cs.IR"], "comment": "ACM MM 2025", "summary": "Multimodal relation extraction (MRE) is a crucial task in the fields of\nKnowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge\ngraph construction. However, existing methods are typically limited to\nextracting a single type of relational triplet, which restricts their ability\nto extract triplets beyond the specified types. Directly combining these\nmethods fails to capture dynamic cross-modal interactions and introduces\nsignificant computational redundancy. Therefore, we propose a novel\n\\textit{unified multimodal Relation Extraction framework with Multilevel\nOptimal Transport and mixture-of-Experts}, termed REMOTE, which can\nsimultaneously extract intra-modal and inter-modal relations between textual\nentities and visual objects. To dynamically select optimal interaction features\nfor different types of relational triplets, we introduce mixture-of-experts\nmechanism, ensuring the most relevant modality information is utilized.\nAdditionally, considering that the inherent property of multilayer sequential\nencoding in existing encoders often leads to the loss of low-level information,\nwe adopt a multilevel optimal transport fusion module to preserve low-level\nfeatures while maintaining multilayer encoding, yielding more expressive\nrepresentations. Correspondingly, we also create a Unified Multimodal Relation\nExtraction (UMRE) dataset to evaluate the effectiveness of our framework,\nencompassing diverse cases where the head and tail entities can originate from\neither text or image. Extensive experiments show that REMOTE effectively\nextracts various types of relational triplets and achieves state-of-the-art\nperformanc on almost all metrics across two other public MRE datasets. We\nrelease our resources at https://github.com/Nikol-coder/REMOTE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aREMOTE\u7684\u591a\u6a21\u6001\u5173\u7cfb\u63d0\u53d6\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u7ea7\u6700\u4f18\u4f20\u8f93\u548c\u4e13\u5bb6\u6df7\u5408\u673a\u5236\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u53d6\u6587\u672c\u5b9e\u4f53\u548c\u89c6\u89c9\u5bf9\u8c61\u4e4b\u95f4\u7684\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u80fd\u63d0\u53d6\u5355\u4e00\u7c7b\u578b\u7684\u5173\u7cfb\u4e09\u5143\u7ec4\uff0c\u4e14\u65e0\u6cd5\u52a8\u6001\u6355\u6349\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u3002", "method": "\u5f15\u5165\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u52a8\u6001\u9009\u62e9\u6700\u4f18\u4ea4\u4e92\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u591a\u7ea7\u6700\u4f18\u4f20\u8f93\u878d\u5408\u6a21\u5757\u4fdd\u7559\u4f4e\u5c42\u4fe1\u606f\u3002", "result": "REMOTE\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "REMOTE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5173\u7cfb\u63d0\u53d6\u7684\u591a\u6837\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2509.04777", "pdf": "https://arxiv.org/pdf/2509.04777", "abs": "https://arxiv.org/abs/2509.04777", "authors": ["Ramana Nagasamudram", "Anindya Banerjee", "David A. Naumann"], "title": "Forall-Exists Relational Verification by Filtering to Forall-Forall", "categories": ["cs.LO", "cs.PL", "F.3.1; F.3.2"], "comment": null, "summary": "Relational verification encompasses research directions such as reasoning\nabout data abstraction, reasoning about security and privacy, secure\ncompilation, and functional specificaton of tensor programs, among others.\nSeveral relational Hoare logics exist, with accompanying tool support for\ncompositional reasoning of $\\forall\\forall$ (2-safety) properties and,\ngenerally, k-safety properties of product programs. In contrast, few logics and\ntools exist for reasoning about $\\forall\\exists$ properties which are critical\nin the context of nondeterminism.\n  This paper's primary contribution is a methodology for verifying a\n$\\forall\\exists$ judgment by way of a novel filter-adequacy transformation.\nThis transformation adds assertions to a product program in such a way that the\ndesired $\\forall\\exists$ property (of a pair of underlying unary programs) is\nimplied by a $\\forall\\forall$ property of the transformed product. The paper\ndevelops a program logic for the basic $\\forall\\exists$ judgement extended with\nassertion failures; develops bicoms, a form of product programs that represents\npairs of executions and that caters for direct translation of $\\forall\\forall$\nproperties to unary correctness; proves (using the logic) a soundness theorem\nthat says successful $\\forall\\forall$ verification of a transformed bicom\nimplies the $\\forall\\exists$ spec for its underlying unary commands; and\nimplements a proof of principle prototype for auto-active relational\nverification which has been used to verify all examples in the paper. The\nmethodology thereby enables a user to work with ordinary assertions and\nassumptions, and a standard assertion language, so that existing tools\nincluding auto-active verifiers can be used.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8fc7\u6ee4\u5668-\u5145\u5206\u6027\u8f6c\u6362\u9a8c\u8bc1\u2200\u2203\u5c5e\u6027\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4ea7\u54c1\u7a0b\u5e8f\u7684\u2200\u2200\u5c5e\u6027\u95f4\u63a5\u8bc1\u660e\u539f\u59cb\u7a0b\u5e8f\u7684\u2200\u2203\u6027\u8d28\uff0c\u5e76\u5f00\u53d1\u4e86\u76f8\u5e94\u7684\u903b\u8f91\u548c\u5de5\u5177\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u548c\u5de5\u5177\u591a\u96c6\u4e2d\u5728\u5904\u7406\u2200\u2200\u5c5e\u6027\uff08\u59822-safety\uff09\uff0c\u4f46\u5bf9\u6d89\u53ca\u975e\u786e\u5b9a\u6027\u7684\u5173\u952e\u2200\u2203\u5c5e\u6027\u7f3a\u4e4f\u6709\u6548\u652f\u6301\u3002", "method": "\u901a\u8fc7\u8fc7\u6ee4\u5668-\u5145\u5206\u6027\u8f6c\u6362\u5728\u4ea7\u54c1\u7a0b\u5e8f\u4e2d\u6dfb\u52a0\u65ad\u8a00\uff0c\u5c06\u2200\u2203\u95ee\u9898\u8f6c\u5316\u4e3a\u2200\u2200\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u652f\u6301\u76f4\u63a5\u7ffb\u8bd1\u7684\u903b\u8f91\uff08bicoms\uff09\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u8bba\u6587\u4e2d\u7684\u6240\u6709\u793a\u4f8b\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u901a\u8fc7\u6807\u51c6\u5de5\u5177\u5b9e\u73b0\u81ea\u52a8\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u73b0\u6709\u5de5\u5177\u7684\u80fd\u529b\uff0c\u4f7f\u5f97\u7528\u6237\u80fd\u591f\u5229\u7528\u5e38\u89c4\u65ad\u8a00\u8bed\u8a00\u548c\u5904\u7406\u2200\u2200\u5c5e\u6027\u7684\u5de5\u5177\u6765\u9a8c\u8bc1\u2200\u2203\u5c5e\u6027\u3002"}}
{"id": "2509.04636", "pdf": "https://arxiv.org/pdf/2509.04636", "abs": "https://arxiv.org/abs/2509.04636", "authors": ["Swapnika Dulam", "Christopher L Dancy"], "title": "Computational Cognitive Modeling to understand the effects of Racializing AI on Human-AI cooperation with PigChase Task", "categories": ["cs.HC"], "comment": null, "summary": "Despite the continued anthropomorphization of AI systems, the potential\nimpact of racialization during human-AI interaction is understudied. This study\nexplores how human-AI cooperation may be impacted by the belief that data used\nto train an AI system is racialized, that is, it was trained on data from a\nspecific group of people. During this study, participants completed a human-AI\ncooperation task using the Pig Chase game. Participants of different\nself-identified demographics interacted with AI agents whose perceived racial\nidentities were manipulated, allowing us to assess how sociocultural\nperspectives influence the decision-making of participants in the game. After\nthe game, participants completed a survey questionnaire to explain the\nstrategies they used while playing the game and to understand the perceived\nintelligence of their AI teammates. Statistical analysis of task behavior data\nrevealed a statistically significant effect of the participant's demographic,\nas well as the interaction between this self-identified demographic and the\ntreatment condition (i.e., the perceived demographic of the agent). The results\nindicated that Non-White participants viewed AI agents racialized as White in a\npositive way compared to AI agents racialized as Black. Both Black and White\nparticipants viewed the AI agent in the control treatment in a negative way. A\nbaseline cognitive model of the task using ACT-R cognitive architecture was\nused to understand a cognitive-level, process-based explanation of the\nparticipants' perspectives based on results found from the study. This model\nhelps us better understand the factors affecting the decision-making strategies\nof the game participants. Results from analysis of these data, as well as\ncognitive modeling, indicate a need to expand understanding of the ways\nracialization (whether implicit or explicit) impacts interaction with AI\nsystems.", "AI": {"tldr": "\u7814\u7a76\u4e86AI\u7cfb\u7edf\u8bad\u7ec3\u6570\u636e\u7684\u79cd\u65cf\u5316\u5bf9\u4eba\u7c7b\u4e0eAI\u5408\u4f5c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e0d\u540c\u79cd\u65cf\u53c2\u4e0e\u8005\u5bf9\u79cd\u65cf\u5316AI\u4ee3\u7406\u7684\u6001\u5ea6\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u4e0eAI\u5408\u4f5c\u4e2d\uff0c\u8bad\u7ec3\u6570\u636e\u7684\u79cd\u65cf\u5316\u5982\u4f55\u5f71\u54cd\u4e92\u52a8\uff0c\u586b\u8865\u76f8\u5173\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7Pig Chase\u6e38\u620f\u5b9e\u9a8c\u548c\u8ba4\u77e5\u6a21\u578b\uff08ACT-R\uff09\u5206\u6790\u53c2\u4e0e\u8005\u884c\u4e3a\u53ca\u7b56\u7565\u3002", "result": "\u4e0d\u540c\u79cd\u65cf\u53c2\u4e0e\u8005\u5bf9\u79cd\u65cf\u5316AI\u4ee3\u7406\u6709\u663e\u8457\u4e0d\u540c\u6001\u5ea6\uff0c\u975e\u767d\u4eba\u5bf9\u767d\u4eba\u5316AI\u4ee3\u7406\u66f4\u79ef\u6781\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u79cd\u65cf\u5316\u5bf9AI\u4e92\u52a8\u7684\u5f71\u54cd\uff0c\u4ee5\u5b8c\u5584\u4eba\u7c7b\u4e0eAI\u5408\u4f5c\u7684\u7406\u89e3\u3002"}}
{"id": "2509.05116", "pdf": "https://arxiv.org/pdf/2509.05116", "abs": "https://arxiv.org/abs/2509.05116", "authors": ["Jialin Chen", "Jeremie Clos", "Dominic Price", "Praminda Caleb-Solly"], "title": "Analyzing Gait Adaptation with Hemiplegia Simulation Suits and Digital Twins", "categories": ["cs.ET", "cs.RO"], "comment": "7 pages, accepted at EMBC 2025, presented at the conference", "summary": "To advance the development of assistive and rehabilitation robots, it is\nessential to conduct experiments early in the design cycle. However, testing\nearly prototypes directly with users can pose safety risks. To address this, we\nexplore the use of condition-specific simulation suits worn by healthy\nparticipants in controlled environments as a means to study gait changes\nassociated with various impairments and support rapid prototyping. This paper\npresents a study analyzing the impact of a hemiplegia simulation suit on gait.\nWe collected biomechanical data using a Vicon motion capture system and Delsys\nTrigno EMG and IMU sensors under four walking conditions: with and without a\nrollator, and with and without the simulation suit. The gait data was\nintegrated into a digital twin model, enabling machine learning analyses to\ndetect the use of the simulation suit and rollator, identify turning behavior,\nand evaluate how the suit affects gait over time. Our findings show that the\nsimulation suit significantly alters movement and muscle activation patterns,\nprompting users to compensate with more abrupt motions. We also identify key\nfeatures and sensor modalities that are most informative for accurately\ncapturing gait dynamics and modeling human-rollator interaction within the\ndigital twin framework.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5065\u5eb7\u53c2\u4e0e\u8005\u7a7f\u6234\u6a21\u62df\u7279\u5b9a\u75be\u75c5\u7684\u670d\u88c5\uff0c\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u7814\u7a76\u6b65\u6001\u53d8\u5316\uff0c\u4ee5\u652f\u6301\u5eb7\u590d\u673a\u5668\u4eba\u7684\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3002\u8bba\u6587\u5206\u6790\u4e86\u504f\u762b\u6a21\u62df\u670d\u88c5\u5bf9\u6b65\u6001\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8bc4\u4f30\u5176\u6548\u679c\u3002", "motivation": "\u65e9\u671f\u6d4b\u8bd5\u5eb7\u590d\u673a\u5668\u4eba\u539f\u578b\u65f6\u76f4\u63a5\u63a5\u89e6\u7528\u6237\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Vicon\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u548cDelsys Trigno EMG/IMU\u4f20\u611f\u5668\u6536\u96c6\u56db\u79cd\u6b65\u884c\u6761\u4ef6\u4e0b\u7684\u751f\u7269\u529b\u5b66\u6570\u636e\uff0c\u5e76\u96c6\u6210\u5230\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u4e2d\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6a21\u62df\u670d\u88c5\u663e\u8457\u6539\u53d8\u4e86\u8fd0\u52a8\u548c\u808c\u8089\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u4f7f\u7528\u8005\u9700\u8981\u66f4\u7a81\u7136\u7684\u52a8\u4f5c\u6765\u8865\u507f\u3002\u7814\u7a76\u8fd8\u786e\u5b9a\u4e86\u6700\u80fd\u6355\u6349\u6b65\u6001\u52a8\u6001\u548c\u4eba\u4e0e\u52a9\u884c\u5668\u4e92\u52a8\u7684\u5173\u952e\u7279\u5f81\u548c\u4f20\u611f\u5668\u7c7b\u578b\u3002", "conclusion": "\u6a21\u62df\u7279\u5b9a\u75be\u75c5\u7684\u670d\u88c5\u662f\u7814\u7a76\u6b65\u6001\u53d8\u5316\u7684\u5b89\u5168\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u4e5f\u4e3a\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6b65\u6001\u6570\u636e\u3002"}}
{"id": "2509.04877", "pdf": "https://arxiv.org/pdf/2509.04877", "abs": "https://arxiv.org/abs/2509.04877", "authors": ["Maryam Khan", "Muhammad Azeem Akbar", "Jussi Kasurinen"], "title": "Integrating Large Language Models in Software Engineering Education: A Pilot Study through GitHub Repositories Mining", "categories": ["cs.SE"], "comment": null, "summary": "Context: Large Language Models (LLMs) such as ChatGPT are increasingly\nadopted in software engineering (SE) education, offering both opportunities and\nchallenges. Their adoption requires systematic investigation to ensure\nresponsible integration into curricula. Objective: This doctoral research aims\nto develop a validated framework for integrating LLMs into SE education through\na multi-phase process, including taxonomies development, empirical\ninvestigation, and case studies. This paper presents the first empirical step.\nMethod: We conducted a pilot repository mining study of 400 GitHub projects,\nanalyzing README files and issues discussions to identify the presence of\nmotivator and demotivator previously synthesized in our literature review [ 8]\nstudy. Results: Motivators such as engagement and motivation (227 hits),\nsoftware engineering process understanding (133 hits), and programming\nassistance and debugging support (97 hits) were strongly represented.\nDemotivators, including plagiarism and IP concerns (385 hits), security,\nprivacy and data integrity (87 hits), and over-reliance on AI in learning (39\nhits), also appeared prominently. In contrast, demotivators such as challenges\nin evaluating learning outcomes and difficulty in curriculum redesign recorded\nno hits across the repositories. Conclusion: The study provides early empirical\nvalidation of motivators/demotivators taxonomies with respect to their themes,\nhighlights research practice gaps, and lays the foundation for developing a\ncomprehensive framework to guide the responsible adoption of LLMs in SE\neducation.", "AI": {"tldr": "\u8be5\u535a\u58eb\u7814\u7a76\u901a\u8fc7\u5206\u6790400\u4e2aGitHub\u9879\u76ee\uff0c\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u4e2d\u7684\u6fc0\u52b1\u56e0\u7d20\uff08\u5982\u5b66\u4e60\u52a8\u529b\uff09\u548c\u963b\u788d\u56e0\u7d20\uff08\u5982\u6284\u88ad\u95ee\u9898\uff09\uff0c\u4e3a\u540e\u7eed\u6846\u67b6\u5f00\u53d1\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u5728\u6559\u80b2\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u5176\u8d23\u4efb\u6027\u6574\u5408\uff0c\u4ee5\u907f\u514d\u6f5c\u5728\u95ee\u9898\u5e76\u53d1\u6325\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u8bd5\u70b9\u4ed3\u5e93\u6316\u6398\u65b9\u6cd5\uff0c\u5206\u6790GitHub\u9879\u76ee\u7684README\u6587\u4ef6\u548c\u8bae\u9898\u8ba8\u8bba\uff0c\u8bc6\u522b\u5df2\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u5408\u6210\u7684\u6fc0\u52b1\u4e0e\u963b\u788d\u56e0\u7d20\u3002", "result": "\u6fc0\u52b1\u56e0\u7d20\uff08\u5982\u5b66\u4e60\u52a8\u529b\uff09\u548c\u963b\u788d\u56e0\u7d20\uff08\u5982\u6284\u88ad\u95ee\u9898\uff09\u5747\u663e\u8457\u5b58\u5728\uff0c\u4f46\u90e8\u5206\u963b\u788d\u56e0\u7d20\uff08\u5982\u8bfe\u7a0b\u8bbe\u8ba1\u95ee\u9898\uff09\u672a\u4f53\u73b0\u3002", "conclusion": "\u7814\u7a76\u521d\u6b65\u9a8c\u8bc1\u4e86\u6fc0\u52b1/\u963b\u788d\u56e0\u7d20\u5206\u7c7b\uff0c\u63ed\u793a\u4e86\u5b9e\u8df5\u4e0e\u7814\u7a76\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u4e3a\u672a\u6765\u6846\u67b6\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.04695", "pdf": "https://arxiv.org/pdf/2509.04695", "abs": "https://arxiv.org/abs/2509.04695", "authors": ["Lars Herschbach", "Damien Rossi", "Sina Keshvadi"], "title": "Path Dynamics in a Deployed Path-Aware Network: A Measurement Study of SCIONLab", "categories": ["cs.NI", "C.2.1; C.2"], "comment": "19 pages, 8 figures. Submitted to the Computer Communications journal", "summary": "Path-aware networks promise enhanced performance and resilience through\nmultipath transport, but a lack of empirical data on their real-world dynamics\nhinders the design of effective protocols. This paper presents a longitudinal\nmeasurement study of the SCION architecture on the global SCIONLab testbed,\ncharacterizing the path stability, diversity, and performance crucial for\nprotocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic\nenvironment, with significant control-plane churn and short path lifetimes in\nparts of the testbed. We identify and characterize path discrepancy, a\nphenomenon where routing policies create asymmetric path availability between\nendpoints. Furthermore, we observe a performance trade-off where concurrent\nmultipath transmissions can improve aggregate throughput but may degrade the\nlatency and reliability of individual paths. These findings demonstrate that\nprotocols such as MPQUIC should explicitly account for high churn and path\nasymmetry, challenging common assumptions in multipath protocol design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5168\u7403SCIONLab\u6d4b\u8bd5\u5e8a\u5bf9SCION\u67b6\u6784\u8fdb\u884c\u7eb5\u5411\u6d4b\u91cf\u7814\u7a76\uff0c\u53d1\u73b0\u8def\u5f84\u7a33\u5b9a\u6027\u3001\u591a\u6837\u6027\u548c\u6027\u80fd\u5bf9\u591a\u8def\u5f84\u534f\u8bae\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u63ed\u793a\u4e86\u9ad8\u52a8\u6001\u6027\u548c\u8def\u5f84\u4e0d\u5bf9\u79f0\u6027\u5bf9\u591a\u8def\u5f84QUIC\u7b49\u534f\u8bae\u7684\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u586b\u8865\u5173\u4e8e\u8def\u5f84\u611f\u77e5\u7f51\u7edc\u5b9e\u65f6\u52a8\u6001\u6570\u636e\u7684\u7a7a\u767d\uff0c\u4ee5\u652f\u6301\u66f4\u6709\u6548\u7684\u591a\u8def\u5f84\u534f\u8bae\u8bbe\u8ba1\u3002", "method": "\u65b9\u6cd5\u662f\u5728\u5168\u7403SCIONLab\u6d4b\u8bd5\u5e8a\u4e0a\u5bf9SCION\u67b6\u6784\u8fdb\u884c\u7eb5\u5411\u6d4b\u91cf\u7814\u7a76\uff0c\u5206\u6790\u8def\u5f84\u7a33\u5b9a\u6027\u3001\u591a\u6837\u6027\u548c\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6d4b\u8bd5\u5e8a\u5177\u6709\u9ad8\u52a8\u6001\u6027\uff0c\u63a7\u5236\u5e73\u9762\u53d8\u5316\u5927\uff0c\u8def\u5f84\u5bff\u547d\u77ed\uff0c\u5b58\u5728\u8def\u5f84\u4e0d\u5bf9\u79f0\u73b0\u8c61\uff0c\u5e76\u53d1\u591a\u8def\u5f84\u4f20\u8f93\u53ef\u80fd\u5f71\u54cd\u5355\u4e00\u8def\u5f84\u7684\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u591a\u8def\u5f84\u534f\u8bae\u8bbe\u8ba1\u9700\u660e\u786e\u8003\u8651\u9ad8\u52a8\u6001\u6027\u548c\u8def\u5f84\u4e0d\u5bf9\u79f0\u6027\uff0c\u6311\u6218\u4e86\u73b0\u6709\u8bbe\u8ba1\u4e2d\u7684\u5e38\u89c1\u5047\u8bbe\u3002"}}
{"id": "2509.04513", "pdf": "https://arxiv.org/pdf/2509.04513", "abs": "https://arxiv.org/abs/2509.04513", "authors": ["Ming Du", "Volker Rose", "Junjing Deng", "Dileep Singh", "Si Chen", "Mathew J. Cherukara"], "title": "Fidelity-preserving enhancement of ptychography with foundational text-to-image models", "categories": ["cs.GR", "cs.NA", "math.NA", "physics.app-ph", "65Kxx"], "comment": null, "summary": "Ptychographic phase retrieval enables high-resolution imaging of complex\nsamples but often suffers from artifacts such as grid pathology and multislice\ncrosstalk, which degrade reconstructed images. We propose a plug-and-play (PnP)\nframework that integrates physics model-based phase retrieval with text-guided\nimage editing using foundational diffusion models. By employing the alternating\ndirection method of multipliers (ADMM), our approach ensures consensus between\ndata fidelity and artifact removal subproblems, maintaining physics consistency\nwhile enhancing image quality. Artifact removal is achieved using a text-guided\ndiffusion image editing method (LEDITS++) with a pre-trained foundational\ndiffusion model, allowing users to specify artifacts for removal in natural\nlanguage. Demonstrations on simulated and experimental datasets show\nsignificant improvements in artifact suppression and structural fidelity,\nvalidated by metrics such as peak signal-to-noise ratio (PSNR) and diffraction\npattern consistency. This work highlights the combination of text-guided\ngenerative models and model-based phase retrieval algorithms as a transferable\nand fidelity-preserving method for high-quality diffraction imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u6a21\u578b\u76f8\u4f4d\u68c0\u7d22\u4e0e\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u7f16\u8f91\u7684\u63d2\u4ef6\uff08PnP\uff09\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u5e76\u51cf\u5c11\u4f2a\u5f71\u3002", "motivation": "\u89e3\u51b3ptychographic\u76f8\u4f4d\u68c0\u7d22\u4e2d\u5b58\u5728\u7684\u7f51\u683c\u75c5\u7406\u548c\u591a\u5c42\u4e32\u6270\u7b49\u4f2a\u5f71\u95ee\u9898\uff0c\u63d0\u5347\u91cd\u5efa\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u6574\u5408\u6570\u636e\u4fdd\u771f\u5ea6\u4e0e\u4f2a\u5f71\u53bb\u9664\u5b50\u95ee\u9898\uff0c\u7ed3\u5408\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u56fe\u50cf\u7f16\u8f91\u6280\u672f\uff08LEDITS++\uff09\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u4f2a\u5f71\u6291\u5236\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u63d0\u5347\uff0c\u901a\u8fc7PSNR\u7b49\u6307\u6807\u9a8c\u8bc1\u3002", "conclusion": "\u7ed3\u5408\u6587\u672c\u5f15\u5bfc\u751f\u6210\u6a21\u578b\u4e0e\u6a21\u578b\u9a71\u52a8\u7684\u76f8\u4f4d\u68c0\u7d22\u7b97\u6cd5\uff0c\u4e3a\u9ad8\u8d28\u91cf\u884d\u5c04\u6210\u50cf\u63d0\u4f9b\u4e86\u53ef\u8fc1\u79fb\u4e14\u4fdd\u771f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.04827", "pdf": "https://arxiv.org/pdf/2509.04827", "abs": "https://arxiv.org/abs/2509.04827", "authors": ["Jiahuan Yu", "Aryan Taneja", "Junfeng Lin", "Minjia Zhang"], "title": "VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving", "categories": ["cs.DC"], "comment": null, "summary": "Modern Large Language Model (LLM) serving systems increasingly support\ninteractive applications, like real-time chat assistants, code generation\ntools, and agentic workflows. However, the soaring energy cost of LLM inference\npresents a growing challenge for sustainable and cost-effective deployment.\nThis paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM\nserving, built from a control theory perspective. VoltanaLLM co-designs\nfrequency scaling and request routing in emerging prefill/decode disaggregated\narchitectures, leveraging their decoupled execution to enable fine-grained\nphase-specific control. It consists of a feedback-driven frequency controller\nthat dynamically adapts GPU frequency for prefill and decode phases, and a\nstate-space router that explores routing decisions across frequency-scaled\ninstances to minimize energy under latency constraints. We implement VoltanaLLM\nin SGLang and evaluate its performance over multiple state-of-the-art LLMs and\nreal-world datasets. The results demonstrate that VoltanaLLM achieves up to\n36.3% energy savings while maintaining near-perfect SLO attainment rate, paving\nthe way for sustainable and intelligent LLM serving.", "AI": {"tldr": "VoltanaLLM\u662f\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u7406\u8bba\u7684SLO\u611f\u77e5\u3001\u9ad8\u6548\u8282\u80fdLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u8bbe\u8ba1\u9891\u7387\u7f29\u653e\u548c\u8bf7\u6c42\u8def\u7531\uff0c\u5b9e\u73b0\u9ad8\u8fbe36.3%\u7684\u8282\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u8fd1\u4e4e\u5b8c\u7f8e\u7684SLO\u8fbe\u6807\u7387\u3002", "motivation": "\u73b0\u4ee3LLM\u670d\u52a1\u7cfb\u7edf\u7684\u80fd\u6e90\u6210\u672c\u6301\u7eed\u6500\u5347\uff0c\u5bf9\u53ef\u6301\u7eed\u548c\u6210\u672c\u6548\u76ca\u90e8\u7f72\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "VoltanaLLM\u7ed3\u5408\u53cd\u9988\u9a71\u52a8\u7684\u9891\u7387\u63a7\u5236\u5668\u548c\u72b6\u6001\u7a7a\u95f4\u8def\u7531\u5668\uff0c\u52a8\u6001\u8c03\u6574GPU\u9891\u7387\u5e76\u4f18\u5316\u8def\u7531\u51b3\u7b56\uff0c\u4ee5\u6700\u5c0f\u5316\u80fd\u6e90\u6d88\u8017\u3002", "result": "\u5728\u591a\u4e2a\u5148\u8fdbLLM\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cVoltanaLLM\u5b9e\u73b0\u9ad8\u8fbe36.3%\u7684\u8282\u80fd\uff0c\u540c\u65f6\u7ef4\u6301\u9ad8SLO\u8fbe\u6807\u7387\u3002", "conclusion": "VoltanaLLM\u4e3a\u53ef\u6301\u7eed\u548c\u667a\u80fdLLM\u670d\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05129", "pdf": "https://arxiv.org/pdf/2509.05129", "abs": "https://arxiv.org/abs/2509.05129", "authors": ["Meihao Liao", "Yueyang Pan", "Rong-Hua Li", "Guoren Wang"], "title": "Efficient Exact Resistance Distance Computation on Small-Treewidth Graphs: a Labelling Approach", "categories": ["cs.DB", "cs.DM", "cs.DS", "cs.LG"], "comment": "Accepted by SIGMOD 2026", "summary": "Resistance distance computation is a fundamental problem in graph analysis,\nyet existing random walk-based methods are limited to approximate solutions and\nsuffer from poor efficiency on small-treewidth graphs (e.g., road networks). In\ncontrast, shortest-path distance computation achieves remarkable efficiency on\nsuch graphs by leveraging cut properties and tree decompositions. Motivated by\nthis disparity, we first analyze the cut property of resistance distance. While\na direct generalization proves impractical due to costly matrix operations, we\novercome this limitation by integrating tree decompositions, revealing that the\nresistance distance $r(s,t)$ depends only on labels along the paths from $s$\nand $t$ to the root of the decomposition. This insight enables compact\nlabelling structures. Based on this, we propose \\treeindex, a novel index\nmethod that constructs a resistance distance labelling of size $O(n \\cdot\nh_{\\mathcal{G}})$ in $O(n \\cdot h_{\\mathcal{G}}^2 \\cdot d_{\\max})$ time, where\n$h_{\\mathcal{G}}$ (tree height) and $d_{\\max}$ (maximum degree) behave as small\nconstants in many real-world small-treewidth graphs (e.g., road networks). Our\nlabelling supports exact single-pair queries in $O(h_{\\mathcal{G}})$ time and\nsingle-source queries in $O(n \\cdot h_{\\mathcal{G}})$ time. Extensive\nexperiments show that TreeIndex substantially outperforms state-of-the-art\napproaches. For instance, on the full USA road network, it constructs a $405$\nGB labelling in $7$ hours (single-threaded) and answers exact single-pair\nqueries in $10^{-3}$ seconds and single-source queries in $190$ seconds--the\nfirst exact method scalable to such large graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6811\u5206\u89e3\u7684\u7d22\u5f15\u65b9\u6cd5TreeIndex\uff0c\u7528\u4e8e\u9ad8\u6548\u8ba1\u7b97\u5c0f\u6811\u5bbd\u56fe\uff08\u5982\u9053\u8def\u7f51\u7edc\uff09\u4e0a\u7684\u7535\u963b\u8ddd\u79bb\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u6e38\u8d70\u65b9\u6cd5\u5728\u5c0f\u6811\u5bbd\u56fe\u4e0a\u6548\u7387\u4f4e\u4e0b\u4e14\u53ea\u80fd\u63d0\u4f9b\u8fd1\u4f3c\u89e3\uff0c\u800c\u6700\u77ed\u8def\u5f84\u8ddd\u79bb\u901a\u8fc7\u6811\u5206\u89e3\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\uff0c\u4f46\u7535\u963b\u8ddd\u79bb\u7684\u76f4\u63a5\u63a8\u5e7f\u56e0\u77e9\u9635\u64cd\u4f5c\u6210\u672c\u9ad8\u800c\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7535\u963b\u8ddd\u79bb\u7684\u5272\u6027\u8d28\uff0c\u5e76\u7ed3\u5408\u6811\u5206\u89e3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u6807\u7b7e\u7ed3\u6784TreeIndex\uff0c\u5176\u6784\u5efa\u65f6\u95f4\u548c\u6807\u7b7e\u5927\u5c0f\u5747\u4e0e\u5c0f\u6811\u5bbd\u56fe\u7684\u5b9e\u9645\u53c2\u6570\u76f8\u5173\u3002", "result": "TreeIndex\u5728\u5c0f\u6811\u5bbd\u56fe\uff08\u5982\u7f8e\u56fd\u9053\u8def\u7f51\u7edc\uff09\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u6784\u5efa\u65f6\u95f4\u77ed\u4e14\u67e5\u8be2\u901f\u5ea6\u5feb\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u5927\u89c4\u6a21\u56fe\u7684\u53ef\u6269\u5c55\u7cbe\u786e\u8ba1\u7b97\u3002", "conclusion": "TreeIndex\u5b9e\u73b0\u4e86\u7535\u963b\u8ddd\u79bb\u7684\u9ad8\u6548\u7cbe\u786e\u8ba1\u7b97\uff0c\u4e3a\u89e3\u51b3\u5c0f\u6811\u5bbd\u56fe\u4e0a\u7684\u8be5\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2509.04528", "pdf": "https://arxiv.org/pdf/2509.04528", "abs": "https://arxiv.org/abs/2509.04528", "authors": ["Juan Sapriza", "Beatrice Grassano", "Alessio Naclerio", "Filippo Quadri", "Tommaso Terzano", "David Mallas\u00e9n", "Davide Schiavone", "Robin Leplae", "J\u00e9r\u00e9mie Moullet", "Alexandre Levisse", "Christoph M\u00fcller", "Mariagrazia Graziano", "Mat\u00edas Miguez", "David Atienza"], "title": "HEEPidermis: a versatile SoC for BioZ recording", "categories": ["physics.ins-det", "cs.AR", "physics.med-ph"], "comment": null, "summary": "Biological impedance (BioZ) is an information-packed modality that allows for\nnon-invasive monitoring of health and emotional state. Currently, most research\ninvolving tissue impedance is based on bulky or fixed-purpose hardware, which\nlimits the scope of research and the possibilities of experiments. In this\nwork, we present HEEPidermis: a System-on-Chip (SoC) which integrates all the\nblocks needed for tissue impedance measurement, including two 8-bit,\narbitrary-signal current DACs, two VCO-based ADCs, and a RISC-V CPU to enable\non-chip feature extraction for closed-loop operation. An event-based\nsub-sampler improves storage and energy efficiency for long-term recording. In\naddition to the versatile SoC, the digital back-end and behavioral models of\nthe analog front-end are open-source, allowing fast system-level simulations or\nrepurposing. The SoC was taped out on TSMC 65 nm LP process.", "AI": {"tldr": "HEEPidermis\u662f\u4e00\u4e2a\u96c6\u6210\u4e86\u7ec4\u7ec7\u963b\u6297\u6d4b\u91cf\u6240\u6709\u5173\u952e\u6a21\u5757\u7684SoC\uff0c\u5305\u62ec\u7535\u6d41DAC\u3001VCO-based ADC\u548cRISC-V CPU\uff0c\u652f\u6301\u95ed\u73af\u64cd\u4f5c\u548c\u9ad8\u6548\u957f\u671f\u8bb0\u5f55\u3002", "motivation": "\u73b0\u6709\u7ec4\u7ec7\u963b\u6297\u6d4b\u91cf\u8bbe\u5907\u4f53\u79ef\u5927\u6216\u529f\u80fd\u5355\u4e00\uff0c\u9650\u5236\u4e86\u7814\u7a76\u548c\u5b9e\u9a8c\u7684\u53ef\u80fd\u6027\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u4e86\u6a21\u62df\u524d\u7aef\u548c\u6570\u5b57\u540e\u7aef\u7684SoC\uff0c\u91c7\u7528\u5f00\u6e90\u8bbe\u8ba1\u548cTSMC 65 nm\u5de5\u827a\u5236\u9020\u3002", "result": "HEEPidermis\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u963b\u6297\u6d4b\u91cf\uff0c\u652f\u6301\u95ed\u73af\u64cd\u4f5c\u548c\u957f\u671f\u8bb0\u5f55\u3002", "conclusion": "HEEPidermis\u4e3a\u751f\u7269\u963b\u6297\u7814\u7a76\u63d0\u4f9b\u4e86\u5c0f\u578b\u5316\u3001\u9ad8\u6548\u7684\u786c\u4ef6\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.05160", "pdf": "https://arxiv.org/pdf/2509.05160", "abs": "https://arxiv.org/abs/2509.05160", "authors": ["Steven Smyth", "Daniel Busch", "Moez Ben Haj Hmida", "Edward A. Lee", "Bernhard Steffen"], "title": "AI-Assisted Modeling: DSL-Driven AI Interactions", "categories": ["cs.PL", "cs.SE"], "comment": "7 pages, 4 figures", "summary": "AI-assisted programming greatly increases software development performance.\nWe enhance this potential by integrating transparency through domain-specific\nmodeling techniques and providing instantaneous, graphical visualizations that\naccurately represent the semantics of AI-generated code. This approach\nfacilitates visual inspection and formal verification, such as model checking.\n  Formal models can be developed using programming, natural language prompts,\nvoice commands, and stage-wise refinement, with immediate feedback after each\ntransformation step. This support can be tailored to specific domains or\nintended purposes, improving both code generation and subsequent validation\nprocesses.\n  To demonstrate the effectiveness of this approach, we have developed a\nprototype as a Visual Studio Code extension for the Lingua Franca language.\nThis prototype showcases the potential for novel domain-specific modeling\npractices, offering an advancement in how models are created, visualized, and\nverified.", "AI": {"tldr": "AI\u8f85\u52a9\u7f16\u7a0b\u901a\u8fc7\u900f\u660e\u5316\u548c\u5373\u65f6\u56fe\u5f62\u53ef\u89c6\u5316\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u9a8c\u8bc1\u80fd\u529b\u3002", "motivation": "\u589e\u5f3aAI\u751f\u6210\u4ee3\u7801\u7684\u900f\u660e\u5ea6\u548c\u9a8c\u8bc1\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u8f6f\u4ef6\u5f00\u53d1\u7684\u6027\u80fd\u548c\u8d28\u91cf\u3002", "method": "\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u5efa\u6a21\u6280\u672f\u548c\u5373\u65f6\u56fe\u5f62\u53ef\u89c6\u5316\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\u65b9\u5f0f\uff08\u7f16\u7a0b\u3001\u81ea\u7136\u8bed\u8a00\u3001\u8bed\u97f3\uff09\uff0c\u5e76\u5206\u6b65\u63d0\u4f9b\u53cd\u9988\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2aVisual Studio Code\u6269\u5c55\u539f\u578b\uff0c\u5c55\u793a\u5982\u4f55\u901a\u8fc7\u65b0\u65b9\u6cd5\u6539\u8fdb\u5efa\u6a21\u3001\u53ef\u89c6\u5316\u548c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728AI\u8f85\u52a9\u7f16\u7a0b\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u548c\u9a8c\u8bc1\u7684\u6548\u7387\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.04884", "pdf": "https://arxiv.org/pdf/2509.04884", "abs": "https://arxiv.org/abs/2509.04884", "authors": ["Raul Singh", "Nicolo Brunello", "Vincenzo Scotti", "Mark James Carman"], "title": "L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning", "categories": ["cs.CL", "cs.PF"], "comment": "Work published at ICNLSP 2025, waiting for publication link", "summary": "The ability of Large Language Models (LLMs) to solve complex tasks has made\nthem crucial in the development of AI-based applications. However, the high\ncomputational requirements to fine-tune these LLMs on downstream tasks pose\nsignificant challenges, particularly when resources are limited. In response to\nthis challenge, we introduce L1RA, a novel technique aimed at dynamically\ndistributing the rank of low-rank adapters during fine-tuning using LoRA. Given\na rank budget (i.e., total sum of adapters rank), L1RA leverages L1\nregularisation to prune redundant ranks and redistribute them across adapters,\nthereby optimising resource utilisation. Through a series of comprehensive\nexperiments, we empirically demonstrate that L1RA maintains comparable or even\nreduced computational overhead compared to other LoRA variants, including the\nvanilla approach, while achieving same or better performances. Moreover, the\npost-training analysis of rank distribution unveiled insights into the specific\nmodel components requiring the most adaptation to align with the task\nobjective: the feed-forward layers and the attention output projection. These\nresults highlight the efficacy of L1RA in not only enhancing the efficiency of\nLLM fine-tuning, but also in providing valuable diagnostic information for\nmodel refinement and customisation. In conclusion, L1RA stands as a promising\ntechnique for advancing the performance and interpretability of LLM adaptation,\nparticularly in scenarios where computational resources are constrained.", "AI": {"tldr": "L1RA\u662f\u4e00\u79cd\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f4e\u79e9\u9002\u914d\u5668\u7684\u79e9\u5206\u914d\u6765\u4f18\u5316\u8d44\u6e90\u5229\u7528\uff0c\u63d0\u9ad8LLM\u5fae\u8c03\u6548\u7387\u3002", "motivation": "\u7531\u4e8eLLMs\u5fae\u8c03\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u5728\u8d44\u6e90\u53d7\u9650\u65f6\u5b58\u5728\u6311\u6218\uff0cL1RA\u65e8\u5728\u4f18\u5316\u8d44\u6e90\u5206\u914d\u4ee5\u51cf\u5c11\u5f00\u9500\u3002", "method": "\u4f7f\u7528L1\u6b63\u5219\u5316\u52a8\u6001\u4fee\u526a\u548c\u91cd\u65b0\u5206\u914d\u4f4e\u79e9\u9002\u914d\u5668\u7684\u79e9\uff0c\u786e\u4fdd\u5728\u7ed9\u5b9a\u79e9\u9884\u7b97\u4e0b\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660eL1RA\u5728\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u597d\u65f6\uff0c\u8ba1\u7b97\u5f00\u9500\u66f4\u4f4e\uff0c\u5e76\u63d0\u4f9b\u6a21\u578b\u6539\u8fdb\u7684\u8bca\u65ad\u4fe1\u606f\u3002", "conclusion": "L1RA\u662f\u4e00\u79cd\u9ad8\u6548\u7684LLM\u5fae\u8c03\u6280\u672f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2509.04938", "pdf": "https://arxiv.org/pdf/2509.04938", "abs": "https://arxiv.org/abs/2509.04938", "authors": ["Jianlu Wang", "Yanan Wang", "Tong Liu"], "title": "An Emotion Recognition Framework via Cross-modal Alignment of EEG and Eye Movement Data", "categories": ["cs.MM"], "comment": null, "summary": "Emotion recognition is essential for applications in affective computing and\nbehavioral prediction, but conventional systems relying on single-modality data\noften fail to capture the complexity of affective states. To address this\nlimitation, we propose an emotion recognition framework that achieves accurate\nmultimodal alignment of Electroencephalogram (EEG) and eye movement data\nthrough a hybrid architecture based on cross-modal attention mechanism.\nExperiments on the SEED-IV dataset demonstrate that our method achieve 90.62%\naccuracy. This work provides a promising foundation for leveraging multimodal\ndata in emotion recognition", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u73b0EEG\u548c\u773c\u52a8\u6570\u636e\u7684\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u4ee5\u63d0\u9ad8\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5355\u6a21\u6001\u6570\u636e\u7684\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u60c5\u611f\u72b6\u6001\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u5229\u7528\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u9f50EEG\u548c\u773c\u52a8\u6570\u636e\u3002", "result": "\u5728SEED-IV\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u53d6\u5f97\u4e8690.62%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002"}}
{"id": "2509.04922", "pdf": "https://arxiv.org/pdf/2509.04922", "abs": "https://arxiv.org/abs/2509.04922", "authors": ["S\u00e9bastien Gou\u00ebzel"], "title": "Higher order differential calculus in mathlib", "categories": ["cs.LO"], "comment": null, "summary": "We report on the higher-order differential calculus library developed inside\nthe Lean mathematical library mathlib. To support a broad range of\napplications, we depart in several ways from standard textbook definitions: we\nallow arbitrary fields of scalars, we work with functions defined on domains\nrather than full spaces, and we integrate analytic functions in the broader\nscale of smooth functions. These generalizations introduce significant\nchallenges, which we address from both the mathematical and the formalization\nperspectives.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728Lean\u6570\u5b66\u5e93mathlib\u4e2d\u5f00\u53d1\u7684\u9ad8\u9636\u5fae\u5206\u8ba1\u7b97\u5e93\uff0c\u5176\u7279\u70b9\u5305\u62ec\u652f\u6301\u4efb\u610f\u6807\u91cf\u57df\u3001\u5728\u5b9a\u4e49\u57df\u800c\u975e\u5168\u7a7a\u95f4\u4e0a\u5de5\u4f5c\uff0c\u5e76\u6574\u5408\u89e3\u6790\u51fd\u6570\u5230\u66f4\u5e7f\u6cdb\u7684\u5e73\u6ed1\u51fd\u6570\u4e2d\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\uff0c\u6587\u7ae0\u5bf9\u4f20\u7edf\u6559\u6750\u5b9a\u4e49\u8fdb\u884c\u4e86\u591a\u9879\u6269\u5c55\uff0c\u5305\u62ec\u6807\u91cf\u57df\u7684\u7075\u6d3b\u6027\u3001\u5b9a\u4e49\u57df\u7684\u9650\u5236\u4ee5\u53ca\u51fd\u6570\u7c7b\u578b\u7684\u6574\u5408\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u548c\u5f62\u5f0f\u5316\u89d2\u5ea6\u89e3\u51b3\u5e7f\u4e49\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u5f00\u53d1\u4e86\u9ad8\u9636\u5fae\u5206\u8ba1\u7b97\u5e93\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u4e2a\u9002\u5e94\u6027\u5f3a\u3001\u529f\u80fd\u5e7f\u6cdb\u7684\u9ad8\u9636\u5fae\u5206\u8ba1\u7b97\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u5e7f\u4e49\u5316\u5f15\u5165\u7684\u95ee\u9898\u3002", "conclusion": "\u8be5\u5e93\u4e3a\u6570\u5b66\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u5e7f\u4e49\u5316\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.04705", "pdf": "https://arxiv.org/pdf/2509.04705", "abs": "https://arxiv.org/abs/2509.04705", "authors": ["Nusrat Jahan Lamia", "Sadia Afrin Mim"], "title": "Transforming Fashion with AI: A Comparative Study of Large Language Models in Apparel Design", "categories": ["cs.HC"], "comment": null, "summary": "Fashion has evolved from handcrafted designs to automated production over the\nyears, where AI has added another dimension to it. Nowadays, practically every\nindustry uses artificial models to automate their operations. To explore their\nrole, we examined three prominent LLMs (OpenAI, GeminiAI, Deepseek) in multiple\nstages of textile manufacturing (e.g., sustainable choice, cost effectiveness,\nproduction planning, etc.). We assessed the models' ability to replicate\ngarment design using certain parameters (fabric construction, shade, weave,\nsilhouette, etc.). We compared the models in terms of different body types and\nfunctional purposes (e.g., fashionwear, sportswear) so that designers could\nevaluate effectiveness before developing actual patterns, make necessary\nmodifications, and conduct fashion forecasting beforehand. To facilitate deeper\nanalysis, we created a custom dataset specifically for fabric image generation\nand classification. Our analysis revealed that, in terms of fabric\nconstruction, the OpenAI DALL-E model integrated with ChatGPT outperformed\nother models, achieving a lower LPIPS (Learned Perceptual Image Patch\nSimilarity) score of approximately $0.2$. In fabric classification from images,\nwe found OpenAI offered the best results by breaking down certain factors\n(e.g., breathability, moisture-wicking, and tactile comfort), achieving\napproximately $80\\%$ accuracy for base construction and $55\\%$ for detailed\nconstruction. However, our results indicate that Deepseek faced significant\nchallenges in generating and recognizing fabric images. Overall, all the models\nstruggled to recognize complex fabric constructions and intricate designs from\nimages, and relying too much on AI might hinder human creativity. We also\nobserved that all three models performed effectively in providing\nrecommendations and insights for fabric design in textual form.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4e09\u5927LLM\uff08OpenAI\u3001GeminiAI\u3001Deepseek\uff09\u5728\u7eba\u7ec7\u5236\u9020\u591a\u9636\u6bb5\uff08\u5982\u53ef\u6301\u7eed\u9009\u62e9\u3001\u751f\u4ea7\u89c4\u5212\u7b49\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u670d\u88c5\u8bbe\u8ba1\u590d\u5236\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u521b\u5efa\u4e86\u5b9a\u5236\u6570\u636e\u96c6\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002", "motivation": "\u63a2\u7d22AI\u5728\u7eba\u7ec7\u5236\u9020\u548c\u670d\u88c5\u8bbe\u8ba1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5e2e\u52a9\u8bbe\u8ba1\u5e08\u63d0\u524d\u8bc4\u4f30\u6548\u679c\u5e76\u8fdb\u884c\u4fee\u6539\u548c\u9884\u6d4b\u3002", "method": "\u901a\u8fc7\u5b9a\u5236\u6570\u636e\u96c6\u6d4b\u8bd5\u4e09\u5927LLM\u5728\u7ec7\u7269\u56fe\u50cf\u751f\u6210\u548c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u8bbe\u8ba1\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002", "result": "OpenAI\u5728\u7ec7\u7269\u6784\u9020\u548c\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\uff08LPIPS\u7ea60.2\uff0c\u5206\u7c7b\u51c6\u786e\u738780%\uff09\uff0c\u800cDeepseek\u8868\u73b0\u8f83\u5dee\u3002\u6240\u6709\u6a21\u578b\u5728\u590d\u6742\u7ec7\u7269\u8bc6\u522b\u548c\u8bbe\u8ba1\u751f\u6210\u4e2d\u5747\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "AI\u5728\u7eba\u7ec7\u5236\u9020\u4e2d\u5177\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u53ef\u80fd\u6291\u5236\u4eba\u7c7b\u521b\u9020\u529b\u3002\u6a21\u578b\u5728\u6587\u672c\u63a8\u8350\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u56fe\u50cf\u4efb\u52a1\u4e2d\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2509.04624", "pdf": "https://arxiv.org/pdf/2509.04624", "abs": "https://arxiv.org/abs/2509.04624", "authors": ["Ali Khanpour", "Tianyi Wang", "Afra Vahidi-Shams", "Wim Ectors", "Farzam Nakhaie", "Amirhossein Taheri", "Christian Claudel"], "title": "UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis", "categories": ["cs.CV", "cs.ET", "cs.RO", "cs.SY", "eess.IV", "eess.SY"], "comment": "15 pages, 8 figures, 2 tables", "summary": "Traffic congestion and violations pose significant challenges for urban\nmobility and road safety. Traditional traffic monitoring systems, such as fixed\ncameras and sensor-based methods, are often constrained by limited coverage,\nlow adaptability, and poor scalability. To address these challenges, this paper\nintroduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance\nsystem capable of accurate vehicle detection, classification, tracking, and\nbehavioral analysis in real-world, unconstrained urban environments. The system\nleverages multi-scale and multi-angle template matching, Kalman filtering, and\nhomography-based calibration to process aerial video data collected from\naltitudes of approximately 200 meters. A case study in urban area demonstrates\nrobust performance, achieving a detection precision of 91.8%, an F1-score of\n90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.\nBeyond precise detection, the system classifies five vehicle types and\nautomatically detects critical traffic violations, including unsafe lane\nchanges, illegal double parking, and crosswalk obstructions, through the fusion\nof geofencing, motion filtering, and trajectory deviation analysis. The\nintegrated analytics module supports origin-destination tracking, vehicle count\nvisualization, inter-class correlation analysis, and heatmap-based congestion\nmodeling. Additionally, the system enables entry-exit trajectory profiling,\nvehicle density estimation across road segments, and movement direction\nlogging, supporting comprehensive multi-scale urban mobility analytics.\nExperimental results confirms the system's scalability, accuracy, and practical\nrelevance, highlighting its potential as an enforcement-aware,\ninfrastructure-independent traffic monitoring solution for next-generation\nsmart cities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u4ea4\u901a\u76d1\u63a7\u7cfb\u7edf\uff0c\u5b9e\u73b0\u9ad8\u6548\u8f66\u8f86\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u884c\u4e3a\u5206\u6790\uff0c\u89e3\u51b3\u4f20\u7edf\u76d1\u63a7\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u76d1\u63a7\u7cfb\u7edf\u8986\u76d6\u8303\u56f4\u6709\u9650\uff0c\u9002\u5e94\u6027\u5dee\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3\u57ce\u5e02\u4ea4\u901a\u9700\u6c42\u3002", "method": "\u5229\u7528\u591a\u5c3a\u5ea6\u591a\u89d2\u5ea6\u6a21\u677f\u5339\u914d\u3001\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u5355\u5e94\u6027\u6821\u51c6\u5904\u7406\u65e0\u4eba\u673a\u822a\u62cd\u89c6\u9891\u3002", "result": "\u7cfb\u7edf\u68c0\u6d4b\u7cbe\u5ea6\u8fbe91.8%\uff0cF1\u5206\u657090.5%\uff0c\u5e76\u80fd\u5206\u7c7b\u8f66\u8f86\u53ca\u68c0\u6d4b\u4ea4\u901a\u8fdd\u89c4\u884c\u4e3a\u3002", "conclusion": "\u7cfb\u7edf\u5177\u6709\u9ad8\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u5408\u4f5c\u4e3a\u667a\u80fd\u57ce\u5e02\u7684\u72ec\u7acb\u4ea4\u901a\u76d1\u63a7\u65b9\u6848\u3002"}}
{"id": "2509.04967", "pdf": "https://arxiv.org/pdf/2509.04967", "abs": "https://arxiv.org/abs/2509.04967", "authors": ["Kai Feng", "Jeremy Singer", "Angelos K Marnerides"], "title": "FuzzRDUCC: Fuzzing with Reconstructed Def-Use Chain Coverage", "categories": ["cs.SE", "cs.CR", "D.2.5"], "comment": null, "summary": "Binary-only fuzzing often struggles with achieving thorough code coverage and\nuncovering hidden vulnerabilities due to limited insight into a program's\ninternal dataflows. Traditional grey-box fuzzers guide test case generation\nprimarily using control flow edge coverage, which can overlook bugs not easily\nexposed through control flow analysis alone. We argue that integrating dataflow\nanalysis into the fuzzing process can enhance its effectiveness by revealing\nhow data propagates through the program, thereby enabling the exploration of\nexecution paths that control flow-based methods might miss. In this context, we\nintroduce FuzzRDUCC, a novel fuzzing framework that employs symbolic execution\nto reconstruct definition-use (def-use) chains directly from binary\nexecutables. FuzzRDUCC identifies crucial dataflow paths and exposes security\nvulnerabilities without incurring excessive computational overhead, due to a\nnovel heuristic algorithm that selects relevant def-use chains without\naffecting the thoroughness of the fuzzing process. We evaluate FuzzRDUCC using\nthe binutils benchmark and demonstrate that it can identify unique crashes not\nfound by state-of-the-art fuzzers. Hence, establishing FuzzRDUCC as a feasible\nsolution for next generation vulnerability detection and discovery mechanisms.", "AI": {"tldr": "FuzzRDUCC\u662f\u4e00\u79cd\u65b0\u7684\u6a21\u7cca\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u6d41\u5206\u6790\u589e\u5f3a\u8986\u76d6\u7387\uff0c\u7ed3\u5408\u7b26\u53f7\u6267\u884c\u91cd\u5efa\u5b9a\u4e49-\u4f7f\u7528\u94fe\uff0c\u663e\u8457\u63d0\u5347\u6f0f\u6d1e\u53d1\u73b0\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6a21\u7cca\u6d4b\u8bd5\u4f9d\u8d56\u63a7\u5236\u6d41\u8fb9\u7f18\u8986\u76d6\u7387\uff0c\u53ef\u80fd\u5ffd\u7565\u9690\u85cf\u6f0f\u6d1e\u3002\u6570\u636e\u6d41\u5206\u6790\u7684\u52a0\u5165\u80fd\u66f4\u597d\u5730\u63ed\u793a\u7a0b\u5e8f\u5185\u90e8\u6570\u636e\u4f20\u64ad\u8def\u5f84\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u6548\u679c\u3002", "method": "FuzzRDUCC\u5229\u7528\u7b26\u53f7\u6267\u884c\u76f4\u63a5\u4ece\u4e8c\u8fdb\u5236\u53ef\u6267\u884c\u6587\u4ef6\u91cd\u5efa\u5b9a\u4e49-\u4f7f\u7528\u94fe\uff0c\u5e76\u901a\u8fc7\u542f\u53d1\u5f0f\u7b97\u6cd5\u9009\u62e9\u5173\u952e\u6570\u636e\u6d41\u8def\u5f84\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728binutils\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFuzzRDUCC\u80fd\u591f\u53d1\u73b0\u672a\u88ab\u5176\u4ed6\u5148\u8fdb\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u68c0\u6d4b\u5230\u7684\u72ec\u7279\u5d29\u6e83\u3002", "conclusion": "FuzzRDUCC\u662f\u4e00\u79cd\u53ef\u884c\u7684\u4e0b\u4e00\u4ee3\u6f0f\u6d1e\u68c0\u6d4b\u548c\u53d1\u73b0\u65b9\u6848\uff0c\u5c24\u5176\u5728\u6570\u636e\u6d41\u8def\u5f84\u5206\u6790\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2509.04792", "pdf": "https://arxiv.org/pdf/2509.04792", "abs": "https://arxiv.org/abs/2509.04792", "authors": ["Erik Rye", "Dave Levin", "Robert Beverly"], "title": "Where Have All the Firewalls Gone? Security Consequences of Residential IPv6 Transition", "categories": ["cs.NI", "cs.CR"], "comment": null, "summary": "IPv4 NAT has limited the spread of IoT botnets considerably by\ndefault-denying bots' incoming connection requests to in-home devices unless\nthe owner has explicitly allowed them. As the Internet transitions to majority\nIPv6, however, residential connections no longer require the use of NAT. This\npaper therefore asks: has the transition from IPv4 to IPv6 ultimately made\nresidential networks more vulnerable to attack, thereby empowering the next\ngeneration of IPv6-based IoT botnets? To answer this question, we introduce a\nlarge-scale IPv6 scanning methodology that, unlike those that rely on AI, can\nbe run on low-resource devices common in IoT botnets. We use this methodology\nto perform the largest-scale measurement of IPv6 residential networks to date,\nand compare which devices are publicly accessible to comparable IPv4 networks.\nWe were able to receive responses from 14.0M distinct IPv6 addresses inside of\nresidential networks (i.e., not the external-facing gateway), in 2,436 ASes\nacross 118 countries. These responses come from protocols commonly exploited by\nIoT botnets (including telnet and FTP), as well as protocols typically\nassociated with end-user devices (including iPhone-Sync and IPP). Comparing to\nIPv4, we show that we are able to reach more printers, iPhones, and smart\nlights over IPv6 than full IPv4-wide scans could. Collectively, our results\nshow that NAT has indeed acted as the de facto firewall of the Internet, and\nthe v4-to-v6 transition of residential networks is opening up new devices to\nattack.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86IPv6\u7684\u666e\u53ca\u662f\u5426\u589e\u52a0\u4e86\u4f4f\u5b85\u7f51\u7edc\u5bf9IoT\u50f5\u5c38\u7f51\u7edc\u7684\u653b\u51fb\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21IPv6\u626b\u63cf\u53d1\u73b0IPv6\u6bd4IPv4\u66f4\u5bb9\u6613\u8ba9IoT\u8bbe\u5907\u66b4\u9732\u5728\u653b\u51fb\u4e0b\u3002", "motivation": "\u968f\u7740\u4e92\u8054\u7f51\u5411IPv6\u8fc7\u6e21\uff0c\u4f4f\u5b85\u7f51\u7edc\u4e0d\u518d\u4f9d\u8d56NAT\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u66f4\u591a\u8bbe\u5907\u66b4\u9732\u5728\u653b\u51fb\u4e0b\uff0c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u9002\u5408\u4f4e\u8d44\u6e90\u8bbe\u5907\u7684\u5927\u89c4\u6a21IPv6\u626b\u63cf\u65b9\u6cd5\uff0c\u5bf9\u6bd4IPv6\u548cIPv4\u4f4f\u5b85\u7f51\u7edc\u4e2d\u53ef\u516c\u5f00\u8bbf\u95ee\u7684\u8bbe\u5907\u6570\u91cf\u3002", "result": "\u626b\u63cf\u7ed3\u679c\u663e\u793a\uff0cIPv6\u7f51\u7edc\u4e2d\u66b4\u9732\u7684\u8bbe\u5907\uff08\u5982\u6253\u5370\u673a\u3001iPhone\u3001\u667a\u80fd\u706f\uff09\u6bd4IPv4\u66f4\u591a\uff0c\u8868\u660eNAT\u66fe\u662f\u4e92\u8054\u7f51\u7684\u9ed8\u8ba4\u9632\u706b\u5899\u3002", "conclusion": "IPv6\u7684\u8fc7\u6e21\u4f7f\u4f4f\u5b85\u7f51\u7edc\u66f4\u5bb9\u6613\u53d7\u5230\u653b\u51fb\uff0cNAT\u7684\u7f3a\u5931\u5bfc\u81f4\u66f4\u591a\u8bbe\u5907\u66b4\u9732\u5728IoT\u50f5\u5c38\u7f51\u7edc\u7684\u5a01\u80c1\u4e0b\u3002"}}
{"id": "2509.05285", "pdf": "https://arxiv.org/pdf/2509.05285", "abs": "https://arxiv.org/abs/2509.05285", "authors": ["Haruo Fujiwara", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in text-driven 3D scene editing and stylization, which\nleverage the powerful capabilities of 2D generative models, have demonstrated\npromising outcomes. However, challenges remain in ensuring high-quality\nstylization and view consistency simultaneously. Moreover, applying style\nconsistently to different regions or objects in the scene with semantic\ncorrespondence is a challenging task. To address these limitations, we\nintroduce techniques that enhance the quality of 3D stylization while\nmaintaining view consistency and providing optional region-controlled style\ntransfer. Our method achieves stylization by re-training an initial 3D\nrepresentation using stylized multi-view 2D images of the source views.\nTherefore, ensuring both style consistency and view consistency of stylized\nmulti-view images is crucial. We achieve this by extending the style-aligned\ndepth-conditioned view generation framework, replacing the fully shared\nattention mechanism with a single reference-based attention-sharing mechanism,\nwhich effectively aligns style across different viewpoints. Additionally,\ninspired by recent 3D inpainting methods, we utilize a grid of multiple depth\nmaps as a single-image reference to further strengthen view consistency among\nstylized images. Finally, we propose Multi-Region Importance-Weighted Sliced\nWasserstein Distance Loss, allowing styles to be applied to distinct image\nregions using segmentation masks from off-the-shelf models. We demonstrate that\nthis optional feature enhances the faithfulness of style transfer and enables\nthe mixing of different styles across distinct regions of the scene.\nExperimental evaluations, both qualitative and quantitative, demonstrate that\nour pipeline effectively improves the results of text-driven 3D stylization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u6587\u672c\u9a71\u52a83D\u573a\u666f\u7f16\u8f91\u548c\u98ce\u683c\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u8bad\u7ec33D\u8868\u793a\u5e76\u7ed3\u5408\u591a\u89c6\u56fe\u98ce\u683c\u5316\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u98ce\u683c\u4e00\u81f4\u6027\u548c\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u73b0\u9ad8\u8d28\u91cf\u98ce\u683c\u5316\u548c\u89c6\u89d2\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u533a\u57df\u6216\u5bf9\u8c61\u4e0a\u5e94\u7528\u8bed\u4e49\u4e00\u81f4\u7684\u98ce\u683c\u65f6\u3002", "method": "\u91c7\u7528\u53c2\u8003\u5f0f\u6ce8\u610f\u529b\u5171\u4eab\u673a\u5236\u589e\u5f3a\u98ce\u683c\u5bf9\u9f50\uff0c\u5229\u7528\u591a\u6df1\u5ea6\u56fe\u7f51\u683c\u63d0\u5347\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u52a0\u6743\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u591a\u533a\u57df\u98ce\u683c\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u98ce\u683c\u5316\u7684\u6548\u679c\uff0c\u652f\u6301\u591a\u533a\u57df\u98ce\u683c\u6df7\u5408\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u98ce\u683c\u548c\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u533a\u57df\u63a7\u5236\u98ce\u683c\u8fc1\u79fb\u80fd\u529b\u3002"}}
{"id": "2509.05216", "pdf": "https://arxiv.org/pdf/2509.05216", "abs": "https://arxiv.org/abs/2509.05216", "authors": ["Mengjiao Han", "Andres Sewell", "Joseph Insley", "Janet Knowles", "Victor A. Mateevitsi", "Michael E. Papka", "Steve Petruzza", "Silvio Rizzi"], "title": "Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization", "categories": ["cs.DC"], "comment": null, "summary": "We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)\npipeline for scientific visualization. Building on previous work that\ndemonstrated high-fidelity isosurface reconstruction using Gaussian primitives,\nwe incorporate a multi-GPU training backend adapted from Grendel-GS to enable\nscalable processing of large datasets. By distributing optimization across\nGPUs, our method improves training throughput and supports high-resolution\nreconstructions that exceed single-GPU capacity. In our experiments, the system\nachieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs\ncompared to a single-GPU baseline, and successfully trains the Miranda dataset\n(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays\nthe groundwork for integrating 3D-GS into HPC-based scientific workflows,\nenabling real-time post hoc and in situ visualization of complex simulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e863D\u9ad8\u65af\u6563\u70b9\uff083D-GS\uff09\u7684\u591aGPU\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u79d1\u5b66\u53ef\u89c6\u5316\u3002\u901a\u8fc7\u591aGPU\u8bad\u7ec3\u540e\u7aef\uff0c\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u6548\u7387\u548c\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5bf9\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u7684\u9ad8\u5206\u8fa8\u7387\u5b9e\u65f6\u53ef\u89c6\u5316\uff0c\u514b\u670d\u5355GPU\u5728\u5904\u7406\u9ad8\u590d\u6742\u5ea6\u6570\u636e\u96c6\u65f6\u7684\u6027\u80fd\u74f6\u9888\u3002", "method": "\u57283D-GS\u7684\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u591aGPU\u8bad\u7ec3\u540e\u7aef\uff08Grendel-GS\uff09\uff0c\u5206\u5e03\u5f0f\u4f18\u5316\u5904\u7406\u6570\u636e\uff0c\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u548c\u6269\u5c55\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u56dbGPU\u5904\u7406Kingsnake\u6570\u636e\u96c6\uff084M\u9ad8\u65af\uff09\u65f6\u63d0\u901f5.6\u500d\uff1bMiranda\u6570\u636e\u96c6\uff0818M\u9ad8\u65af\uff09\u5728\u5355GPU\u65e0\u6cd5\u5b8c\u6210\u7684\u4efb\u52a1\u4e0b\u4e5f\u80fd\u6210\u529f\u8bad\u7ec3\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5c063D-GS\u96c6\u6210\u5230\u9ad8\u6027\u80fd\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u652f\u6301\u590d\u6742\u6a21\u62df\u7684\u5b9e\u65f6\u53ef\u89c6\u5316\u3002"}}
{"id": "2509.04657", "pdf": "https://arxiv.org/pdf/2509.04657", "abs": "https://arxiv.org/abs/2509.04657", "authors": ["Mohammadtaher Safarzadeh", "Afshin Oroojlooyjadid", "Dan Roth"], "title": "Evaluating NL2SQL via SQL2NL", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "comment": "Accepted to EMNLP 2025", "summary": "Robust evaluation in the presence of linguistic variation is key to\nunderstanding the generalization capabilities of Natural Language to SQL\n(NL2SQL) models, yet existing benchmarks rarely address this factor in a\nsystematic or controlled manner. We propose a novel schema-aligned paraphrasing\nframework that leverages SQL-to-NL (SQL2NL) to automatically generate\nsemantically equivalent, lexically diverse queries while maintaining alignment\nwith the original schema and intent. This enables the first targeted evaluation\nof NL2SQL robustness to linguistic variation in isolation-distinct from prior\nwork that primarily investigates ambiguity or schema perturbations. Our\nanalysis reveals that state-of-the-art models are far more brittle than\nstandard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop\nin execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries,\nwhile LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to\n42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We\nalso find that robustness degradation varies significantly with query\ncomplexity, dataset, and domain -- highlighting the need for evaluation\nframeworks that explicitly measure linguistic generalization to ensure reliable\nperformance in real-world settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSQL2NL\u7684\u6a21\u5f0f\u5bf9\u9f50\u590d\u8ff0\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30NL2SQL\u6a21\u578b\u5728\u8bed\u8a00\u591a\u6837\u6027\u4e0b\u7684\u7a33\u5065\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u8106\u5f31\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u7cfb\u7edf\u6216\u53ef\u63a7\u5730\u8bc4\u4f30NL2SQL\u6a21\u578b\u5728\u8bed\u8a00\u591a\u6837\u6027\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6a21\u5f0f\u5bf9\u9f50\u7684\u590d\u8ff0\u6846\u67b6\uff0c\u5229\u7528SQL2NL\u81ea\u52a8\u751f\u6210\u8bed\u4e49\u76f8\u540c\u4f46\u8bcd\u6c47\u591a\u6837\u7684\u67e5\u8be2\uff0c\u4ee5\u9694\u79bb\u8bc4\u4f30\u8bed\u8a00\u53d8\u5f02\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8bed\u8a00\u53d8\u5f02\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u5982LLaMa3.3-70B\u4e0b\u964d10.23%\uff09\uff0c\u4e14\u4e0e\u67e5\u8be2\u590d\u6742\u5ea6\u3001\u6570\u636e\u96c6\u548c\u9886\u57df\u76f8\u5173\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u5f00\u53d1\u660e\u786e\u8861\u91cf\u8bed\u8a00\u6cdb\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u80fd\u3002"}}
{"id": "2509.04798", "pdf": "https://arxiv.org/pdf/2509.04798", "abs": "https://arxiv.org/abs/2509.04798", "authors": ["Yilun Zhao", "Kangding Zhao", "Peng Zhou", "Dingdong Liu", "Tingyu Luo", "Yuzhen Zheng", "Peng Luo", "Shun Hu", "Jin Lin", "Cheng Guo", "Yinhe Han", "Ying Wang", "Mingtang Deng", "Junjie Wu", "X. Fu"], "title": "Distributed-HISQ: A Distributed Quantum Control Architecture", "categories": ["quant-ph", "cs.AR"], "comment": null, "summary": "The design of a scalable Quantum Control Architecture (QCA) faces two primary\nchallenges. First, the continuous growth in qubit counts has rendered\ndistributed QCA inevitable, yet the nondeterministic latencies inherent in\nfeedback loops demand cycleaccurate synchronization across multiple\ncontrollers. Existing synchronization strategies -- whether lock-step or\ndemand-driven -- introduce significant performance penalties. Second, existing\nquantum instruction set architectures are polarized, being either too abstract\nor too granular. This lack of a unifying design necessitates recurrent hardware\ncustomization for each new control requirement, which limits the system's\nreconfigurability and impedes the path toward a scalable and unified digital\nmicroarchitecture.\n  Addressing these challenges, we propose Distributed-HISQ, featuring: (i)\nHISQ, A universal instruction set that redefines quantum control with a\nhardware-agnostic design. By decoupling from quantum operation semantics, HISQ\nprovides a unified language for control sequences, enabling a single\nmicroarchitecture to support various control methods and enhancing system\nreconfigurability. (ii) BISP, a booking-based synchronization protocol that can\npotentially achieve zero-cycle synchronization overhead. The feasibility and\nadaptability of Distributed-HISQ are validated through its implementation on a\ncommercial quantum control system targeting superconducting qubits. We\nperformed a comprehensive evaluation using a customized quantum software stack.\nOur results show that BISP effectively synchronizes multiple control boards,\nleading to a 22.8% reduction in average program execution time and a\n$\\sim$5$\\times$ reduction in infidelity when compared to an existing lock-step\nsynchronization scheme.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Distributed-HISQ\uff0c\u89e3\u51b3\u4e86\u91cf\u5b50\u63a7\u5236\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u901a\u8fc7HISQ\u6307\u4ee4\u96c6\u548cBISP\u540c\u6b65\u534f\u8bae\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u4e0d\u65ad\u589e\u52a0\uff0c\u5206\u5e03\u5f0f\u91cf\u5b50\u63a7\u5236\u67b6\u6784\u4e0d\u53ef\u907f\u514d\uff0c\u4f46\u73b0\u6709\u540c\u6b65\u7b56\u7565\u6027\u80fd\u5f00\u9500\u5927\uff0c\u91cf\u5b50\u6307\u4ee4\u96c6\u8bbe\u8ba1\u7f3a\u4e4f\u7edf\u4e00\u6027\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u53ef\u91cd\u6784\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faDistributed-HISQ\uff0c\u5305\u62ecHISQ\uff08\u786c\u4ef6\u65e0\u5173\u7684\u901a\u7528\u6307\u4ee4\u96c6\uff09\u548cBISP\uff08\u57fa\u4e8e\u9884\u8ba2\u7684\u540c\u6b65\u534f\u8bae\uff09\uff0c\u5e76\u5728\u5546\u7528\u91cf\u5b50\u63a7\u5236\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u9a8c\u8bc1\u3002", "result": "BISP\u663e\u8457\u51cf\u5c11\u4e86\u540c\u6b65\u5f00\u9500\uff0c\u7a0b\u5e8f\u6267\u884c\u65f6\u95f4\u5e73\u5747\u51cf\u5c1122.8%\uff0c\u4fdd\u771f\u5ea6\u63d0\u5347\u7ea65\u500d\u3002", "conclusion": "Distributed-HISQ\u901a\u8fc7\u7edf\u4e00\u7684\u6307\u4ee4\u96c6\u548c\u9ad8\u6548\u7684\u540c\u6b65\u534f\u8bae\uff0c\u4e3a\u91cf\u5b50\u63a7\u5236\u67b6\u6784\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.05293", "pdf": "https://arxiv.org/pdf/2509.05293", "abs": "https://arxiv.org/abs/2509.05293", "authors": ["Julien Vanegue", "Jules Villard", "Peter O'Hearn", "Azalea Raad"], "title": "Non-Termination Proving: 100 Million LoC and Beyond", "categories": ["cs.PL", "cs.CL", "cs.SE", "D.3; F.3"], "comment": "14 pages, 4 figures", "summary": "We report on our tool, Pulse Infinite, that uses proof techniques to show\nnon-termination (divergence) in large programs. Pulse Infinite works\ncompositionally and under-approximately: the former supports scale, and the\nlatter ensures soundness for proving divergence. Prior work focused on small\nbenchmarks in the tens or hundreds of lines of code (LoC), and scale limits\ntheir practicality: a single company may have tens of millions, or even\nhundreds of millions of LoC or more. We report on applying Pulse Infinite to\nover a hundred million lines of open-source and proprietary software written in\nC, C++, and Hack, identifying over 30 previously unknown issues, establishing a\nnew state of the art for detecting divergence in real-world codebases.", "AI": {"tldr": "Pulse Infinite \u662f\u4e00\u6b3e\u5de5\u5177\uff0c\u91c7\u7528\u8bc1\u660e\u6280\u672f\u68c0\u6d4b\u5927\u578b\u7a0b\u5e8f\u4e2d\u7684\u975e\u7ec8\u6b62\uff08\u53d1\u6563\uff09\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u3002", "motivation": "\u73b0\u6709\u7684\u5de5\u5177\u4ec5\u9002\u7528\u4e8e\u5c0f\u578b\u4ee3\u7801\u5e93\uff08\u6570\u5341\u81f3\u6570\u767e\u884c\u4ee3\u7801\uff09\uff0c\u800c\u5b9e\u9645\u4f01\u4e1a\u4ee3\u7801\u5e93\u53ef\u80fd\u8fbe\u5230\u6570\u5343\u4e07\u751a\u81f3\u6570\u4ebf\u884c\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5e94\u5bf9\u5927\u89c4\u6a21\u4ee3\u7801\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "Pulse Infinite \u91c7\u7528\u7ec4\u5408\u548c\u6b20\u8fd1\u4f3c\u7684\u65b9\u6cd5\uff0c\u7ec4\u5408\u6027\u652f\u6301\u89c4\u6a21\u6269\u5c55\uff0c\u6b20\u8fd1\u4f3c\u786e\u4fdd\u53d1\u6563\u8bc1\u660e\u7684\u53ef\u9760\u6027\u3002", "result": "\u8be5\u5de5\u5177\u5728\u8d85\u8fc7\u4e00\u4ebf\u884c\u7684 C\u3001C++ \u548c Hack \u4ee3\u7801\u4e2d\u5e94\u7528\uff0c\u53d1\u73b0 30 \u591a\u4e2a\u672a\u77e5\u95ee\u9898\uff0c\u6210\u4e3a\u68c0\u6d4b\u771f\u5b9e\u4ee3\u7801\u5e93\u53d1\u6563\u95ee\u9898\u7684\u65b0\u6807\u6746\u3002", "conclusion": "Pulse Infinite \u5b9e\u73b0\u4e86\u5bf9\u5927\u89c4\u6a21\u4ee3\u7801\u5e93\u7684\u9ad8\u6548\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u7ec8\u6b62\u95ee\u9898\u68c0\u6d4b\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.05273", "pdf": "https://arxiv.org/pdf/2509.05273", "abs": "https://arxiv.org/abs/2509.05273", "authors": ["Jason Gardner", "Ayan Dutta", "Swapnoneel Roy", "O. Patrick Kreidl", "Ladislau Boloni"], "title": "Greener Deep Reinforcement Learning: Analysis of Energy and Carbon Efficiency Across Atari Benchmarks", "categories": ["cs.LG", "cs.PF"], "comment": "Submitted to a journal - under review", "summary": "The growing computational demands of deep reinforcement learning (DRL) have\nraised concerns about the environmental and economic costs of training\nlarge-scale models. While algorithmic efficiency in terms of learning\nperformance has been extensively studied, the energy requirements, greenhouse\ngas emissions, and monetary costs of DRL algorithms remain largely unexplored.\nIn this work, we present a systematic benchmarking study of the energy\nconsumption of seven state-of-the-art DRL algorithms, namely DQN, TRPO, A2C,\nARS, PPO, RecurrentPPO, and QR-DQN, implemented using Stable Baselines. Each\nalgorithm was trained for one million steps each on ten Atari 2600 games, and\npower consumption was measured in real-time to estimate total energy usage,\nCO2-Equivalent emissions, and electricity cost based on the U.S. national\naverage electricity price. Our results reveal substantial variation in energy\nefficiency and training cost across algorithms, with some achieving comparable\nperformance while consuming up to 24% less energy (ARS vs. DQN), emitting\nnearly 68% less CO2, and incurring almost 68% lower monetary cost (QR-DQN vs.\nRecurrentPPO) than less efficient counterparts. We further analyze the\ntrade-offs between learning performance, training time, energy use, and\nfinancial cost, highlighting cases where algorithmic choices can mitigate\nenvironmental and economic impact without sacrificing learning performance.\nThis study provides actionable insights for developing energy-aware and\ncost-efficient DRL practices and establishes a foundation for incorporating\nsustainability considerations into future algorithmic design and evaluation.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f837\u79cdDRL\u7b97\u6cd5\u7684\u80fd\u6e90\u6d88\u8017\u3001\u78b3\u6392\u653e\u548c\u6210\u672c\uff0c\u63ed\u793a\u4e86\u7b97\u6cd5\u6548\u7387\u7684\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u6307\u51fa\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u548c\u7ecf\u6d4e\u6210\u672c\u3002", "motivation": "\u7814\u7a76DRL\u7b97\u6cd5\u7684\u73af\u5883\u4e0e\u7ecf\u6d4e\u6210\u672c\uff0c\u586b\u8865\u80fd\u6e90\u6548\u7387\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u572810\u6b3eAtari 2600\u6e38\u620f\u4e0a\u8bad\u7ec37\u79cdDRL\u7b97\u6cd5\u5404100\u4e07\u6b65\uff0c\u5b9e\u65f6\u6d4b\u91cf\u80fd\u8017\u5e76\u8ba1\u7b97\u78b3\u6392\u653e\u548c\u6210\u672c\u3002", "result": "\u4e0d\u540c\u7b97\u6cd5\u6548\u7387\u5dee\u5f02\u5927\uff0c\u5982ARS\u6bd4DQN\u8282\u80fd24%\uff0cQR-DQN\u6bd4RecurrentPPO\u51cf\u5c1168%\u78b3\u6392\u653e\u548c\u6210\u672c\u3002", "conclusion": "\u4e3a\u5f00\u53d1\u9ad8\u6548\u4e14\u53ef\u6301\u7eed\u7684DRL\u5b9e\u8df5\u63d0\u4f9b\u4f9d\u636e\uff0c\u5021\u5bfc\u672a\u6765\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u8003\u8651\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2509.05044", "pdf": "https://arxiv.org/pdf/2509.05044", "abs": "https://arxiv.org/abs/2509.05044", "authors": ["Filip Jankovec"], "title": "Subvarieties of pointed Abelian l-groups", "categories": ["math.LO", "cs.LO"], "comment": null, "summary": "This paper provides a complete classification of the subvarieties and\nsubquasivarieties of pointed Abelian lattice-ordered groups ($\\ell$-groups)\nthat are generated by chains. We present two complementary approaches to\nachieve this classification.\n  First, using purely $\\ell$-group-theoretic methods, we analyze the structure\nof lexicographic products and radicals to identify all join-irreducible members\nof the lattice of subvarieties of positively pointed $\\ell$-groups. We provide\na novel equational basis for each of these subvarieties, leading to a complete\ndescription of the entire subvariety lattice. As a direct application, our\n$\\ell$-group-theoretic classification yields an alternative, self-contained\nproof of Komori's celebrated classification of subvarieties of MV-algebras.\n  Second, we explore the connection to MV-algebras via Mundici's $\\Gamma$\nfunctor. We prove that this functor preserves universal classes, a result of\nindependent model-theoretic interest. This allows us to lift the classification\nof universal classes of MV-chains, due to Gispert, to a complete classification\nof universal classes of totally ordered pointed Abelian $\\ell$-groups. As a\ndirect consequence, we obtain a full description of the corresponding lattice\nof subquasivarieties. These results offer a comprehensive structural\nunderstanding of one of the most fundamental classes of ordered algebraic\nstructures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b8c\u5168\u5206\u7c7b\u4e86\u7531\u94fe\u751f\u6210\u7684\u6307\u5411\u963f\u8d1d\u5c14\u683c\u5e8f\u7fa4\u7684\u5b50\u7c07\u548c\u5b50\u62df\u7c07\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u6cd5\uff1a\u683c\u7fa4\u8bba\u5206\u6790\u548cMV\u4ee3\u6570\u7684\u0393\u51fd\u5b50\u8fde\u63a5\uff0c\u63d0\u4f9b\u4e86\u7ed3\u6784\u7684\u5168\u9762\u63cf\u8ff0\u3002", "motivation": "\u7814\u7a76\u6307\u5411\u963f\u8d1d\u5c14\u683c\u5e8f\u7fa4\u7684\u5b50\u7c07\u548c\u5b50\u62df\u7c07\u7ed3\u6784\uff0c\u901a\u8fc7\u4e0d\u540c\u65b9\u6cd5\u63d0\u4f9b\u5b8c\u6574\u7684\u5206\u7c7b\uff0c\u5e76\u63a8\u5e7f\u81f3MV\u4ee3\u6570\u9886\u57df\u3002", "method": "1. \u4f7f\u7528\u683c\u7fa4\u8bba\u65b9\u6cd5\u5206\u6790\u8bcd\u5178\u79ef\u548c\u6839\u57fa\uff0c\u8bc6\u522b\u5b50\u7c07\u683c\u4e2d\u7684\u4e0d\u53ef\u7ea6\u6210\u5458\uff1b2. \u901a\u8fc7MV\u4ee3\u6570\u7684\u0393\u51fd\u5b50\u8fde\u63a5\u6a21\u578b\u7406\u8bba\uff0c\u63a8\u5e7f\u5206\u7c7b\u7ed3\u679c\u3002", "result": "\u83b7\u5f97\u4e86\u5b50\u7c07\u683c\u7684\u5b8c\u6574\u63cf\u8ff0\uff0c\u5e76\u63a8\u5e7f\u4e86Komori\u548cGispert\u7684\u5148\u9a71\u6027\u5206\u7c7b\u7ed3\u679c\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff0c\u4e3a\u6307\u5411\u963f\u8d1d\u5c14\u683c\u5e8f\u7fa4\u7684\u7ed3\u6784\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u5206\u7c7b\u548c\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2509.04752", "pdf": "https://arxiv.org/pdf/2509.04752", "abs": "https://arxiv.org/abs/2509.04752", "authors": ["Melik Ozolcer", "Sang Won Bae"], "title": "SePA: A Search-enhanced Predictive Agent for Personalized Health Coaching", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "Accepted at IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI'25). 7 pages, 5 figures, 3 tables", "summary": "This paper introduces SePA (Search-enhanced Predictive AI Agent), a novel LLM\nhealth coaching system that integrates personalized machine learning and\nretrieval-augmented generation to deliver adaptive, evidence-based guidance.\nSePA combines: (1) Individualized models predicting daily stress, soreness, and\ninjury risk from wearable sensor data (28 users, 1260 data points); and (2) A\nretrieval module that grounds LLM-generated feedback in expert-vetted web\ncontent to ensure contextual relevance and reliability. Our predictive models,\nevaluated with rolling-origin cross-validation and group k-fold\ncross-validation show that personalized models outperform generalized\nbaselines. In a pilot expert study (n=4), SePA's retrieval-based advice was\npreferred over a non-retrieval baseline, yielding meaningful practical effect\n(Cliff's $\\delta$=0.3, p=0.05). We also quantify latency performance trade-offs\nbetween response quality and speed, offering a transparent blueprint for\nnext-generation, trustworthy personal health informatics systems.", "AI": {"tldr": "SePA\u662f\u4e00\u79cd\u7ed3\u5408\u4e2a\u6027\u5316\u673a\u5668\u5b66\u4e60\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684LLM\u5065\u5eb7\u6559\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u6d4b\u6a21\u578b\u548c\u68c0\u7d22\u6a21\u5757\u63d0\u4f9b\u4e2a\u6027\u5316\u5efa\u8bae\uff0c\u4f18\u4e8e\u901a\u7528\u57fa\u7ebf\uff0c\u5e76\u5728\u4e13\u5bb6\u7814\u7a76\u4e2d\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7ed3\u5408\u4e2a\u6027\u5316\u9884\u6d4b\u548c\u53ef\u9760\u68c0\u7d22\u7684\u5065\u5eb7\u6559\u7ec3\u7cfb\u7edf\uff0c\u4ee5\u63d0\u4f9b\u66f4\u7cbe\u51c6\u3001\u53ef\u4fe1\u7684\u5065\u5eb7\u5efa\u8bae\u3002", "method": "\u7ed3\u5408\u4e2a\u6027\u5316\u9884\u6d4b\u6a21\u578b\uff08\u57fa\u4e8e\u53ef\u7a7f\u6234\u6570\u636e\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08\u4f7f\u7528\u4e13\u5bb6\u5ba1\u9605\u7684\u7f51\u7edc\u5185\u5bb9\uff09\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u548c\u4e13\u5bb6\u7814\u7a76\u8bc4\u4f30\u6548\u679c\u3002", "result": "\u4e2a\u6027\u5316\u6a21\u578b\u4f18\u4e8e\u901a\u7528\u57fa\u7ebf\uff0c\u68c0\u7d22\u5efa\u8bae\u66f4\u53d7\u4e13\u5bb6\u9752\u7750\uff08Cliff's \u03b4=0.3\uff0cp=0.05\uff09\uff1b\u540c\u65f6\u5e73\u8861\u4e86\u54cd\u5e94\u8d28\u91cf\u4e0e\u901f\u5ea6\u3002", "conclusion": "SePA\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u4fe1\u4e2a\u4eba\u5065\u5eb7\u4fe1\u606f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04646", "pdf": "https://arxiv.org/pdf/2509.04646", "abs": "https://arxiv.org/abs/2509.04646", "authors": ["Philippe J. Giabbanelli", "Ameeta Agrawal"], "title": "Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization", "categories": ["cs.AI", "cs.ET"], "comment": "Accepted at the AAAI 2025 Fall Symposium Series. November 6-8, 2025,\n  Arlington, VA, USA", "summary": "Modeling & Simulation (M&S) approaches such as agent-based models hold\nsignificant potential to support decision-making activities in health, with\nrecent examples including the adoption of vaccines, and a vast literature on\nhealthy eating behaviors and physical activity behaviors. These models are\npotentially usable by different stakeholder groups, as they support\npolicy-makers to estimate the consequences of potential interventions and they\ncan guide individuals in making healthy choices in complex environments.\nHowever, this potential may not be fully realized because of the models'\ncomplexity, which makes them inaccessible to the stakeholders who could benefit\nthe most. While Large Language Models (LLMs) can translate simulation outputs\nand the design of models into text, current approaches typically rely on\none-size-fits-all summaries that fail to reflect the varied informational needs\nand stylistic preferences of clinicians, policymakers, patients, caregivers,\nand health advocates. This limitation stems from a fundamental gap: we lack a\nsystematic understanding of what these stakeholders need from explanations and\nhow to tailor them accordingly. To address this gap, we present a step-by-step\nframework to identify stakeholder needs and guide LLMs in generating tailored\nexplanations of health simulations. Our procedure uses a mixed-methods design\nby first eliciting the explanation needs and stylistic preferences of diverse\nhealth stakeholders, then optimizing the ability of LLMs to generate tailored\noutputs (e.g., via controllable attribute tuning), and then evaluating through\na comprehensive range of metrics to further improve the tailored generation of\nsummaries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u4e0d\u540c\u5065\u5eb7\u9886\u57df\u7684\u5229\u76ca\u76f8\u5173\u8005\u5b9a\u5236\u4eff\u771f\u6a21\u578b\u7684\u89e3\u91ca\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u4eff\u771f\u6a21\u578b\uff08\u5982\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\uff09\u5728\u652f\u6301\u5065\u5eb7\u51b3\u7b56\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7531\u4e8e\u6a21\u578b\u590d\u6742\u6027\u548c\u89e3\u91ca\u7684\u901a\u7528\u6027\u4e0d\u8db3\uff0c\u5229\u76ca\u76f8\u5173\u8005\u96be\u4ee5\u5145\u5206\u5229\u7528\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u9996\u5148\u6536\u96c6\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u548c\u98ce\u683c\u504f\u597d\uff0c\u7136\u540e\u4f18\u5316LLMs\u4ee5\u751f\u6210\u5b9a\u5236\u5316\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u591a\u6307\u6807\u8bc4\u4f30\u6539\u8fdb\u3002", "result": "\u6846\u67b6\u6210\u529f\u8bc6\u522b\u4e86\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\uff0c\u5e76\u4f18\u5316\u4e86LLMs\u7684\u751f\u6210\u80fd\u529b\uff0c\u4ee5\u63d0\u4f9b\u66f4\u5177\u9488\u5bf9\u6027\u7684\u89e3\u91ca\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5065\u5eb7\u4eff\u771f\u6a21\u578b\u7684\u5b9a\u5236\u89e3\u91ca\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u6709\u671b\u63d0\u5347\u51b3\u7b56\u652f\u6301\u7684\u6548\u679c\u3002"}}
{"id": "2509.05112", "pdf": "https://arxiv.org/pdf/2509.05112", "abs": "https://arxiv.org/abs/2509.05112", "authors": ["Denesa Zyberaj", "Lukasz Mazur", "Nenad Petrovic", "Pankhuri Verma", "Pascal Hirmer", "Dirk Slama", "Xiangwei Cheng", "Alois Knoll"], "title": "GenAI-based test case generation and execution in SDV platform", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This paper introduces a GenAI-driven approach for automated test case\ngeneration, leveraging Large Language Models and Vision-Language Models to\ntranslate natural language requirements and system diagrams into structured\nGherkin test cases. The methodology integrates Vehicle Signal Specification\nmodeling to standardize vehicle signal definitions, improve compatibility\nacross automotive subsystems, and streamline integration with third-party\ntesting tools. Generated test cases are executed within the digital.auto\nplayground, an open and vendor-neutral environment designed to facilitate rapid\nvalidation of software-defined vehicle functionalities. We evaluate our\napproach using the Child Presence Detection System use case, demonstrating\nsubstantial reductions in manual test specification effort and rapid execution\nof generated tests. Despite significant automation, the generation of test\ncases and test scripts still requires manual intervention due to current\nlimitations in the GenAI pipeline and constraints of the digital.auto platform.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGenAI\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u548c\u7cfb\u7edf\u56fe\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u7684Gherkin\u6d4b\u8bd5\u7528\u4f8b\u3002\u65b9\u6cd5\u901a\u8fc7\u8f66\u8f86\u4fe1\u53f7\u89c4\u8303\u5efa\u6a21\u6807\u51c6\u5316\u4fe1\u53f7\u5b9a\u4e49\uff0c\u63d0\u5347\u517c\u5bb9\u6027\u5e76\u7b80\u5316\u96c6\u6210\u3002\u6d4b\u8bd5\u7528\u4f8b\u5728\u6570\u5b57\u6c7d\u8f66\u73af\u5883\u4e2d\u5feb\u901f\u9a8c\u8bc1\uff0c\u4f46GenAI\u6d41\u7a0b\u548c\u5e73\u53f0\u9650\u5236\u4ecd\u9700\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6c7d\u8f66\u8f6f\u4ef6\u6d4b\u8bd5\u7684\u6548\u7387\uff0c\u51cf\u5c11\u4eba\u5de5\u7f16\u5199\u6d4b\u8bd5\u7528\u4f8b\u7684\u5de5\u4f5c\u91cf\uff0c\u5e76\u5b9e\u73b0\u8de8\u5b50\u7cfb\u7edf\u517c\u5bb9\u6027\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u9700\u6c42\u4e0e\u7cfb\u7edf\u56fe\u8f6c\u6362\u4e3aGherkin\u6d4b\u8bd5\u7528\u4f8b\uff0c\u7ed3\u5408\u8f66\u8f86\u4fe1\u53f7\u89c4\u8303\u5efa\u6a21\u6807\u51c6\u5316\u4fe1\u53f7\u5b9a\u4e49\uff0c\u5e76\u5728\u6570\u5b57\u6c7d\u8f66\u5e73\u53f0\u4e2d\u6267\u884c\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u513f\u7ae5\u5b58\u5728\u68c0\u6d4b\u7cfb\u7edf\u7528\u4f8b\u9a8c\u8bc1\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u6d4b\u8bd5\u7528\u4f8b\u7f16\u5199\u5de5\u4f5c\u91cf\uff0c\u5e76\u5feb\u901f\u6267\u884c\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u3002", "conclusion": "\u5c3d\u7ba1\u81ea\u52a8\u5316\u7a0b\u5ea6\u9ad8\uff0c\u4f46\u7531\u4e8eGenAI\u548c\u5e73\u53f0\u9650\u5236\uff0c\u4ecd\u9700\u4eba\u5de5\u5e72\u9884\u3002"}}
{"id": "2509.05248", "pdf": "https://arxiv.org/pdf/2509.05248", "abs": "https://arxiv.org/abs/2509.05248", "authors": ["Iker Mart\u00edn-\u00c1lvarez", "Jos\u00e9 I. Aliaga", "Maribel Castillo"], "title": "Dynamic reconfiguration for malleable applications using RMA", "categories": ["cs.DC"], "comment": "Sumbitted in Workshop DynRes25. 12 pages, 6 Figures, 3 Algorithm, 1\n  Listing", "summary": "This paper investigates the novel one-sided communication methods based on\nremote memory access (RMA) operations in MPI for dynamic resizing of malleable\napplications, enabling data redistribution with minimal impact on application\nexecution. After their integration into the MaM library, these methods are\ncompared with traditional collective-based approaches. In addition, the\nexisting strategy Wait Drains is extended to support efficient background\nreconfiguration. Results show comparable performance, though high\ninitialization costs currently limit their advantage.", "AI": {"tldr": "\u7814\u7a76\u57fa\u4e8eMPI\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee(RMA)\u7684\u5355\u8fb9\u901a\u4fe1\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u53ef\u6269\u5c55\u5e94\u7528\u89c4\u6a21\uff0c\u4e0e\u4f20\u7edf\u96c6\u4f53\u901a\u4fe1\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u6027\u80fd\u76f8\u5f53\u4f46\u521d\u59cb\u5316\u6210\u672c\u8f83\u9ad8\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u5355\u8fb9\u901a\u4fe1\u65b9\u6cd5\u4f18\u5316\u52a8\u6001\u8c03\u6574\u5e94\u7528\u89c4\u6a21\u65f6\u7684\u6570\u636e\u91cd\u65b0\u5206\u914d\uff0c\u51cf\u5c11\u5bf9\u5e94\u7528\u6267\u884c\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u57fa\u4e8eRMA\u7684\u5355\u8fb9\u901a\u4fe1\u65b9\u6cd5\u96c6\u6210\u5230MaM\u5e93\u4e2d\uff0c\u5e76\u4e0e\u4f20\u7edf\u96c6\u4f53\u901a\u4fe1\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u540c\u65f6\u6269\u5c55Wait Drains\u7b56\u7565\u4ee5\u652f\u6301\u9ad8\u6548\u540e\u53f0\u91cd\u65b0\u914d\u7f6e\u3002", "result": "\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u9ad8\u521d\u59cb\u5316\u6210\u672c\u9650\u5236\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "\u5355\u8fb9\u901a\u4fe1\u65b9\u6cd5\u5728\u52a8\u6001\u8c03\u6574\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u964d\u4f4e\u521d\u59cb\u5316\u6210\u672c\u4ee5\u53d1\u6325\u66f4\u5927\u4f18\u52bf\u3002"}}
{"id": "2509.05023", "pdf": "https://arxiv.org/pdf/2509.05023", "abs": "https://arxiv.org/abs/2509.05023", "authors": ["Eneko Atxa Landa", "Elena Lazkano", "Igor Rodriguez", "Itsaso Rodr\u00edguez-Moreno", "Itziar Irigoien"], "title": "Evaluating Idle Animation Believability: a User Perspective", "categories": ["cs.HC", "cs.DB"], "comment": "11 pages, 12 figures", "summary": "Animating realistic avatars requires using high quality animations for every\npossible state the avatar can be in. This includes actions like walking or\nrunning, but also subtle movements that convey emotions and personality. Idle\nanimations, such as standing, breathing or looking around, are crucial for\nrealism and believability. In games and virtual applications, these are often\nhandcrafted or recorded with actors, but this is costly. Furthermore, recording\nrealistic idle animations can be very complex, because the actor must not know\nthey are being recorded in order to make genuine movements. For this reasons\nidle animation datasets are not widely available. Nevertheless, this paper\nconcludes that both acted and genuine idle animations are perceived as real,\nand that users are not able to distinguish between them. It also states that\nhandmade and recorded idle animations are perceived differently. These two\nconclusions mean that recording idle animations should be easier than it is\nthought to be, meaning that actors can be specifically told to act the\nmovements, significantly simplifying the recording process. These conclusions\nshould help future efforts to record idle animation datasets. Finally, we also\npublish ReActIdle, a 3 dimensional idle animation dataset containing both real\nand acted idle motions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u865a\u62df\u89d2\u8272\u7a7a\u95f2\u52a8\u753b\u7684\u751f\u6210\u95ee\u9898\uff0c\u6307\u51fa\u65e0\u8bba\u662f\u6f14\u5458\u8868\u6f14\u8fd8\u662f\u771f\u5b9e\u8bb0\u5f55\u7684\u7a7a\u95f2\u52a8\u753b\uff0c\u7528\u6237\u90fd\u65e0\u6cd5\u533a\u5206\u5176\u771f\u5b9e\u6027\uff0c\u4e14\u624b\u5de5\u5236\u4f5c\u548c\u8bb0\u5f55\u7684\u52a8\u753b\u611f\u77e5\u4e0d\u540c\uff0c\u8fd9\u7b80\u5316\u4e86\u52a8\u753b\u5f55\u5236\u8fc7\u7a0b\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u7a7a\u95f2\u52a8\u753b\u5bf9\u865a\u62df\u89d2\u8272\u7684\u771f\u5b9e\u611f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5e7f\u6cdb\u53ef\u7528\u7684\u6570\u636e\u96c6\uff0c\u4e14\u5f55\u5236\u8fc7\u7a0b\u590d\u6742\u6602\u8d35\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u624b\u5de5\u5236\u4f5c\u548c\u8bb0\u5f55\u7684\u7a7a\u95f2\u52a8\u753b\u7684\u611f\u77e5\u5dee\u5f02\uff0c\u63a2\u8ba8\u4e86\u5f55\u5236\u8fc7\u7a0b\u7684\u7b80\u5316\u53ef\u80fd\u3002", "result": "\u7528\u6237\u65e0\u6cd5\u533a\u5206\u6f14\u5458\u8868\u6f14\u548c\u771f\u5b9e\u8bb0\u5f55\u7684\u7a7a\u95f2\u52a8\u753b\u7684\u771f\u5b9e\u6027\uff0c\u4e14\u624b\u5de5\u5236\u4f5c\u548c\u8bb0\u5f55\u7684\u52a8\u753b\u611f\u77e5\u4e0d\u540c\u3002", "conclusion": "\u6f14\u5458\u53ef\u4ee5\u88ab\u544a\u77e5\u8868\u6f14\u52a8\u4f5c\uff0c\u4ece\u800c\u7b80\u5316\u5f55\u5236\u8fc7\u7a0b\uff0c\u4e3a\u672a\u6765\u5f55\u5236\u7a7a\u95f2\u52a8\u753b\u6570\u636e\u96c6\u63d0\u4f9b\u5e2e\u52a9\u3002"}}
{"id": "2509.04957", "pdf": "https://arxiv.org/pdf/2509.04957", "abs": "https://arxiv.org/abs/2509.04957", "authors": ["Gehui Chen", "Guan'an Wang", "Xiaowen Huang", "Jitao Sang"], "title": "Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent Video-to-Audio (V2A) generation relies on extracting semantic and\ntemporal features from video to condition generative models. Training these\nmodels from scratch is resource intensive. Consequently, leveraging foundation\nmodels (FMs) has gained traction due to their cross-modal knowledge transfer\nand generalization capabilities. One prior work has explored fine-tuning a\nlightweight mapper network to connect a pre-trained visual encoder with a\ntext-to-audio generation model for V2A. Inspired by this, we introduce the\nMultiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper\napproach, MFM-Mapper benefits from richer semantic and temporal information by\nfusing features from dual visual encoders. Furthermore, by replacing a linear\nmapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels\nbetween cross-modal features mapping and autoregressive translation tasks. Our\nMFM-Mapper exhibits remarkable training efficiency. It achieves better\nperformance in semantic and temporal consistency with fewer training consuming,\nrequiring only 16\\% of the training scale compared to previous mapper-based\nwork, yet achieves competitive performance with models trained on a much larger\nscale.", "AI": {"tldr": "MFM-Mapper\u901a\u8fc7\u878d\u5408\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u7279\u5f81\u548c\u4f7f\u7528GPT-2\u66ff\u4ee3\u7ebf\u6027\u6620\u5c04\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\uff0c\u4ec5\u970016%\u7684\u8bad\u7ec3\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u6a21\u578b\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u4e14\u4f9d\u8d56\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002MFM-Mapper\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u6620\u5c04\u673a\u5236\uff0c\u63d0\u5347\u7279\u5f81\u5bf9\u9f50\u548c\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u63d0\u51faMFM-Mapper\uff0c\u878d\u5408\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u7279\u5f81\uff0c\u4f7f\u7528GPT-2\u66ff\u4ee3\u7ebf\u6027\u6620\u5c04\uff0c\u589e\u5f3a\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u3002", "result": "MFM-Mapper\u5728\u8bed\u4e49\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u8bad\u7ec3\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u4ec5\u970016%\u7684\u8bad\u7ec3\u8d44\u6e90\u3002", "conclusion": "MFM-Mapper\u5728\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u7279\u5f81\u6620\u5c04\u3002"}}
{"id": "2509.05094", "pdf": "https://arxiv.org/pdf/2509.05094", "abs": "https://arxiv.org/abs/2509.05094", "authors": ["Areeb Shah Mohammed"], "title": "Partializations of Markov categories", "categories": ["math.CT", "cs.LO", "math.PR", "18B10 (Primary) 18M05, 18M30, 60A05 (Secondary)"], "comment": "62 pages", "summary": "The present work develops a construction of a CD category of partial kernels\nfrom a particular type of Markov category called a partializable Markov\ncategory. These are a generalization of earlier models of categories of partial\nmorphisms such as p-categories, dominical categories, restriction categories,\netc. to a non-deterministic/non-cartesian setting. Here all morphisms are\nquasi-total, with a natural poset enrichment corresponding to one morphism\nbeing a restriction of the other. Furthermore, various properties important to\ncategorical probability are preserved, such as positivity, representability,\nconditionals, Kolmogorov products, and splittings of idempotents. We\nadditionally discuss an alternative notion of Kolmogorov product suitable for\npartial maps, as well as partial algebras for probability monads.\n  The primary example is that of the partialization of the category of standard\nBorel spaces and Markov kernels. Other examples include variants where the\ndistributions are finitely supported, or where one considers multivalued maps\ninstead.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u79cd\u4ece\u7279\u5b9a\u7c7b\u578b\u7684\u9a6c\u5c14\u53ef\u592b\u7c7b\u522b\uff08\u79f0\u4e3a\u53ef\u90e8\u5206\u5316\u9a6c\u5c14\u53ef\u592b\u7c7b\u522b\uff09\u63a8\u5bfc\u51fa\u7684\u90e8\u5206\u6838\u7684CD\u7c7b\u522b\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u90e8\u5206\u6001\u5c04\u6a21\u578b\uff08\u5982p-\u7c7b\u522b\u3001\u652f\u914d\u7c7b\u522b\u7b49\uff09\u5230\u975e\u786e\u5b9a\u6027/\u975e\u7b1b\u5361\u5c14\u8bbe\u7f6e\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u975e\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u63a8\u5e7f\u4f20\u7edf\u7684\u90e8\u5206\u6001\u5c04\u6a21\u578b\uff0c\u5e76\u4fdd\u7559\u6982\u7387\u8bba\u4e2d\u7684\u91cd\u8981\u6027\u8d28\uff08\u5982\u6b63\u6027\u3001\u6761\u4ef6\u6027\u3001Kolmogorov\u79ef\u7b49\uff09\u3002", "method": "\u5229\u7528\u53ef\u90e8\u5206\u5316\u9a6c\u5c14\u53ef\u592b\u7c7b\u522b\u6784\u9020\u90e8\u5206\u6838\u7684CD\u7c7b\u522b\uff0c\u63d0\u51fa\u9002\u7528\u4e8e\u90e8\u5206\u6620\u5c04\u7684Kolmogorov\u79ef\u6982\u5ff5\uff0c\u5e76\u8ba8\u8bba\u6982\u7387\u5e7a\u534a\u7fa4\u7684\u504f\u4ee3\u6570\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u90e8\u5206\u6838\u7684CD\u7c7b\u522b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u6982\u7387\u8bba\u4e2d\u7684\u5173\u952e\u6027\u8d28\u3002\u4e3b\u8981\u5e94\u7528\u5305\u62ec\u6807\u51c6Borel\u7a7a\u95f4\u4e0e\u9a6c\u5c14\u53ef\u592b\u6838\u7c7b\u522b\u7684\u90e8\u5206\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u975e\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u7684\u90e8\u5206\u6001\u5c04\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u6982\u7387\u8bba\u4e2d\u7684\u90e8\u5206\u6620\u5c04\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.04849", "pdf": "https://arxiv.org/pdf/2509.04849", "abs": "https://arxiv.org/abs/2509.04849", "authors": ["Sahil Tomar", "Sandeep Kumar"], "title": "Histogram Driven Amplitude Embedding for Qubit Efficient Quantum Image Compression", "categories": ["quant-ph", "cs.CV", "cs.ET", "cs.IT", "math.IT"], "comment": "7 pages", "summary": "This work introduces a compact and hardware efficient method for compressing\ncolor images using near term quantum devices. The approach segments the image\ninto fixed size blocks called bixels, and computes the total intensity within\neach block. A global histogram with B bins is then constructed from these block\nintensities, and the normalized square roots of the bin counts are encoded as\namplitudes into an n qubit quantum state. Amplitude embedding is performed\nusing PennyLane and executed on real IBM Quantum hardware. The resulting state\nis measured to reconstruct the histogram, enabling approximate recovery of\nblock intensities and full image reassembly. The method maintains a constant\nqubit requirement based solely on the number of histogram bins, independent of\nthe resolution of the image. By adjusting B, users can control the trade off\nbetween fidelity and resource usage. Empirical results demonstrate high quality\nreconstructions using as few as 5 to 7 qubits, significantly outperforming\nconventional pixel level encodings in terms of qubit efficiency and validating\nthe practical application of the method for current NISQ era quantum systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u538b\u7f29\u5f69\u8272\u56fe\u50cf\u7684\u7d27\u51d1\u4e14\u786c\u4ef6\u9ad8\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5757\u548c\u76f4\u65b9\u56fe\u7f16\u7801\u5b9e\u73b0\u9ad8\u6548\u91cf\u5b50\u6001\u5d4c\u5165\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u50cf\u7d20\u7ea7\u7f16\u7801\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u5728\u91cf\u5b50\u8bbe\u5907\u4e0a\u7684\u8d44\u6e90\u6548\u7387\u95ee\u9898\uff0c\u63a2\u7d22\u5728NISQ\u65f6\u4ee3\u91cf\u5b50\u786c\u4ef6\u4e0a\u5b9e\u73b0\u9ad8\u6548\u56fe\u50cf\u538b\u7f29\u7684\u53ef\u884c\u6027\u3002", "method": "\u5c06\u56fe\u50cf\u5206\u5757\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684bixels\uff0c\u8ba1\u7b97\u6bcf\u5757\u603b\u5f3a\u5ea6\uff0c\u6784\u5efa\u5168\u5c40\u76f4\u65b9\u56fe\u5e76\u5c06\u5176\u5f52\u4e00\u5316\u5e73\u65b9\u6839\u7f16\u7801\u4e3a\u91cf\u5b50\u6001\u632f\u5e45\uff0c\u4f7f\u7528PennyLane\u5728IBM\u91cf\u5b50\u786c\u4ef6\u4e0a\u6267\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u97005-7\u4e2a\u91cf\u5b50\u6bd4\u7279\u5373\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c\u8d44\u6e90\u6548\u7387\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728NISQ\u65f6\u4ee3\u91cf\u5b50\u8bbe\u5907\u4e0a\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u901a\u8fc7\u8c03\u8282\u76f4\u65b9\u56fe\u5206\u7bb1\u6570\u5e73\u8861\u4fdd\u771f\u5ea6\u4e0e\u8d44\u6e90\u4f7f\u7528\u3002"}}
{"id": "2509.05197", "pdf": "https://arxiv.org/pdf/2509.05197", "abs": "https://arxiv.org/abs/2509.05197", "authors": ["Naimeng Ye", "Xiao Yu", "Ruize Xu", "Tianyi Peng", "Zhou Yu"], "title": "AI Agents for Web Testing: A Case Study in the Wild", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks.", "AI": {"tldr": "WebProber\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u4ee3\u7406\u7684\u7f51\u9875\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u7528\u6237\u884c\u4e3a\u53d1\u73b0\u4f20\u7edf\u6d4b\u8bd5\u5de5\u5177\u9057\u6f0f\u7684\u53ef\u7528\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7f51\u9875\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u7528\u6237\u884c\u4e3a\uff0c\u5bfc\u81f4\u53ef\u7528\u6027\u95ee\u9898\u672a\u88ab\u53d1\u73b0\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548cAI\u4ee3\u7406\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002", "method": "\u5f00\u53d1WebProber\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165URL\u81ea\u52a8\u63a2\u7d22\u7f51\u7ad9\u3001\u6a21\u62df\u7528\u6237\u4ea4\u4e92\u3001\u8bc6\u522b\u95ee\u9898\u5e76\u751f\u6210\u62a5\u544a\u3002", "result": "\u5728120\u4e2a\u5b66\u672f\u4e2a\u4eba\u7f51\u7ad9\u7684\u6d4b\u8bd5\u4e2d\uff0cWebProber\u53d1\u73b029\u4e2a\u53ef\u7528\u6027\u95ee\u9898\uff0c\u8bb8\u591a\u662f\u4f20\u7edf\u5de5\u5177\u672a\u80fd\u68c0\u6d4b\u5230\u7684\u3002", "conclusion": "\u57fa\u4e8eAI\u4ee3\u7406\u7684\u6d4b\u8bd5\u662f\u672a\u6765\u7528\u6237\u4e2d\u5fc3\u6d4b\u8bd5\u6846\u67b6\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2509.05258", "pdf": "https://arxiv.org/pdf/2509.05258", "abs": "https://arxiv.org/abs/2509.05258", "authors": ["Alexander Interrante-Grant", "Carla Varela-Rosa", "Suhaas Narayan", "Chris Connelly", "Albert Reuther"], "title": "Scaling Performance of Large Language Model Pretraining", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) show best-in-class performance across a wide\nrange of natural language processing applications. Training these models is an\nextremely computationally expensive task; frontier Artificial Intelligence (AI)\nresearch companies are investing billions of dollars into supercomputing\ninfrastructure to train progressively larger models on increasingly massive\ndatasets. Unfortunately, information about the scaling performance and training\nconsiderations of these large training pipelines is scarce in public\nliterature. Working with large-scale datasets and models can be complex and\npractical recommendations are scarce in the public literature for tuning\ntraining performance when scaling up large language models. In this paper, we\naim to demystify the large language model pretraining pipeline somewhat - in\nparticular with respect to distributed training, managing large datasets across\nhundreds of nodes, and scaling up data parallelism with an emphasis on fully\nleveraging available GPU compute capacity.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u65e8\u5728\u63ed\u793a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u6d41\u7a0b\u7684\u6838\u5fc3\u5185\u5bb9\uff0c\u7279\u522b\u662f\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7ba1\u7406\u4ee5\u53ca\u6570\u636e\u5e76\u884c\u6269\u5c55\u7684\u6280\u672f\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\uff0c\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u9700\u8981\u5de8\u5927\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u516c\u5f00\u6587\u732e\u4e2d\u5173\u4e8e\u5176\u6269\u5c55\u6027\u80fd\u548c\u8bad\u7ec3\u4f18\u5316\u7684\u4fe1\u606f\u7a00\u7f3a\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u672c\u6587\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u5b9e\u9645\u6307\u5bfc\u3002", "method": "\u8bba\u6587\u91cd\u70b9\u8ba8\u8bba\u4e86\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u6280\u672f\u3001\u8de8\u6570\u767e\u4e2a\u8282\u70b9\u7ba1\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u6570\u636e\u5e76\u884c\u5145\u5206\u5229\u7528GPU\u8ba1\u7b97\u80fd\u529b\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u548c\u6280\u672f\u7ec6\u8282\uff0c\u5e2e\u52a9\u4f18\u5316\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u672c\u6587\u7684\u7814\u7a76\uff0c\u53ef\u4ee5\u4e3a\u672a\u6765\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u91cd\u8981\u7684\u6280\u672f\u53c2\u8003\uff0c\u586b\u8865\u4e86\u516c\u5f00\u6587\u732e\u4e2d\u7684\u7a7a\u767d\u3002"}}
{"id": "2509.05067", "pdf": "https://arxiv.org/pdf/2509.05067", "abs": "https://arxiv.org/abs/2509.05067", "authors": ["Verena Biener", "Florian Jack Winston", "Dieter Schmalstieg", "Alexander Plopski"], "title": "Long-Term Experiences From Working with Extended Reality in the Wild", "categories": ["cs.HC"], "comment": null, "summary": "Extended Reality (XR) is increasingly used as a productivity tool and recent\ncommercial XR devices have even been specifically designed as productivity\ntools, or, at least, are heavily advertised for such purposes, such as the\nApple Vision Pro (AVP), which has now been available for more than one year. In\nspite of what marketing suggests, research still lacks an understanding of the\nlong-term usage of such devices in ecologically valid everyday settings, as\nmost studies are conducted in very controlled environments. Therefore, we\nconducted interviews with ten AVP users to better understand how experienced\nusers engage with the device, and which limitations persist. Our participants\nreport that XR can increase productivity and that they got used to the device\nafter some time. Yet, a range of limitations persist that might hinder the\nwidespread use of XR as a productivity tool, such as a lack of native\napplications, difficulties when integrating XR into current workflows, and\nlimited possibilities to adapt and customize the XR experience.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86Apple Vision Pro\uff08AVP\uff09\u4f5c\u4e3a\u751f\u4ea7\u529b\u5de5\u5177\u7684\u957f\u671f\u4f7f\u7528\u60c5\u51b5\uff0c\u53d1\u73b0\u5c3d\u7ba1XR\u80fd\u63d0\u5347\u6548\u7387\uff0c\u4f46\u4ecd\u5b58\u5728\u5e94\u7528\u7a0b\u5e8f\u4e0d\u8db3\u3001\u5de5\u4f5c\u6d41\u6574\u5408\u56f0\u96be\u7b49\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1XR\u8bbe\u5907\u4f5c\u4e3a\u751f\u4ea7\u529b\u5de5\u5177\u88ab\u5e7f\u6cdb\u5ba3\u4f20\uff0c\u4f46\u7f3a\u4e4f\u5728\u65e5\u5e38\u73af\u5883\u4e2d\u957f\u671f\u4f7f\u7528\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u91c7\u8bbf\u5341\u4f4dAVP\u7528\u6237\uff0c\u4e86\u89e3\u5176\u4f7f\u7528\u4f53\u9a8c\u548c\u5c40\u9650\u6027\u3002", "result": "\u7528\u6237\u8ba4\u4e3aXR\u80fd\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u4f46\u5b58\u5728\u5e94\u7528\u4e0d\u8db3\u3001\u5de5\u4f5c\u6d41\u6574\u5408\u56f0\u96be\u7b49\u95ee\u9898\u3002", "conclusion": "XR\u4f5c\u4e3a\u751f\u4ea7\u529b\u5de5\u5177\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u89e3\u51b3\u5e94\u7528\u548c\u5de5\u4f5c\u6d41\u6574\u5408\u95ee\u9898\u3002"}}
{"id": "2509.04851", "pdf": "https://arxiv.org/pdf/2509.04851", "abs": "https://arxiv.org/abs/2509.04851", "authors": ["Rajeshwar Tripathi", "Sahil Tomar", "Sandeep Kumar", "Monika Aggarwal"], "title": "Quantum Fourier Transform Based Denoising: Unitary Filtering for Enhanced Speech Clarity", "categories": ["cs.SD", "cs.ET", "eess.AS"], "comment": "8 pages", "summary": "This paper introduces a quantum-inspired denoising framework that integrates\nthe Quantum Fourier Transform (QFT) into classical audio enhancement pipelines.\nUnlike conventional Fast Fourier Transform (FFT) based methods, QFT provides a\nunitary transformation with global phase coherence and energy preservation,\nenabling improved discrimination between speech and noise. The proposed\napproach replaces FFT in Wiener and spectral subtraction filters with a QFT\noperator, ensuring consistent hyperparameter settings for fair comparison.\nExperiments on clean speech, synthetic tones, and noisy mixtures across diverse\nsignal to noise ratio (SNR) conditions, demonstrate statistically significant\ngains in SNR, with up to 15 dB improvement and reduced artifact generation.\nResults confirm that QFT based denoising offers robustness under low SNR and\nnonstationary noise scenarios without additional computational overhead,\nhighlighting its potential as a scalable pathway toward quantum-enhanced speech\nprocessing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u542f\u53d1\u7684\u53bb\u566a\u6846\u67b6\uff0c\u5c06\u91cf\u5b50\u5085\u91cc\u53f6\u53d8\u6362\uff08QFT\uff09\u878d\u5165\u4f20\u7edf\u97f3\u9891\u589e\u5f3a\u6d41\u7a0b\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684FFT\u65b9\u6cd5\uff0cQFT\u63d0\u4f9b\u4e86\u5168\u5c40\u76f8\u4f4d\u4e00\u81f4\u6027\u548c\u80fd\u91cf\u4fdd\u6301\u6548\u679c\uff0c\u4ece\u800c\u63d0\u5347\u8bed\u97f3\u548c\u566a\u58f0\u7684\u533a\u5206\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cQFT\u5728\u53bb\u566a\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8eFFT\u7684\u53bb\u566a\u65b9\u6cd5\u5728\u5904\u7406\u4f4e\u4fe1\u566a\u6bd4\u548c\u975e\u5e73\u7a33\u566a\u58f0\u65f6\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u91cf\u5b50\u4fe1\u53f7\u5904\u7406\u6280\u672f\u5728\u97f3\u9891\u589e\u5f3a\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5c06Wiener\u548c\u8c31\u51cf\u6cd5\u4e2d\u7684FFT\u66ff\u6362\u4e3aQFT\u64cd\u4f5c\u7b26\uff0c\u4fdd\u6301\u8d85\u53c2\u6570\u4e00\u81f4\u4ee5\u516c\u5e73\u6bd4\u8f83\u3002\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u5bf9\u5e72\u51c0\u8bed\u97f3\u3001\u5408\u6210\u97f3\u548c\u566a\u58f0\u6df7\u5408\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cQFT\u53bb\u566a\u5728\u4fe1\u566a\u6bd4\u4e0a\u63d0\u5347\u6700\u9ad8\u8fbe15 dB\uff0c\u4e14\u51cf\u5c11\u4e86\u4f2a\u5f71\u751f\u6210\uff0c\u5c24\u5176\u5728\u4f4e\u4fe1\u566a\u6bd4\u548c\u975e\u5e73\u7a33\u566a\u58f0\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "QFT\u4e3a\u91cf\u5b50\u589e\u5f3a\u7684\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u4fe1\u53f7\u5904\u7406\u5728\u97f3\u9891\u589e\u5f3a\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.04642", "pdf": "https://arxiv.org/pdf/2509.04642", "abs": "https://arxiv.org/abs/2509.04642", "authors": ["Wenxiao Wang", "Priyatham Kattakinda", "Soheil Feizi"], "title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": "Technical Report by RELAI.ai", "summary": "Building reliable LLM agents requires decisions at two levels: the graph\n(which modules exist and how information flows) and the configuration of each\nnode (models, prompts, tools, control knobs). Most existing optimizers tune\nconfigurations while holding the graph fixed, leaving structural failure modes\nunaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for\nLLM agents that jointly searches over graphs and configurations to maximize\nagent quality, subject to explicit rollout/token budgets. Beyond numeric\nmetrics, Maestro leverages reflective textual feedback from traces to\nprioritize edits, improving sample efficiency and targeting specific failure\nmodes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses\nleading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,\n4.9%, and 4.86%, respectively; even when restricted to prompt-only\noptimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these\nresults with far fewer rollouts than GEPA. We further show large gains on two\napplications (interviewer & RAG agents), highlighting that joint graph &\nconfiguration search addresses structural failure modes that prompt tuning\nalone cannot fix.", "AI": {"tldr": "Maestro\u662f\u4e00\u4e2a\u6574\u4f53\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u641c\u7d22\u56fe\u7ed3\u6784\u548c\u8282\u70b9\u914d\u7f6e\u6765\u63d0\u5347LLM\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4f18\u5316\u5668\u4ec5\u8c03\u6574\u8282\u70b9\u914d\u7f6e\uff0c\u800c\u5ffd\u7565\u4e86\u56fe\u7ed3\u6784\u7684\u4f18\u5316\uff0c\u5bfc\u81f4\u7ed3\u6784\u6027\u5931\u8d25\u6a21\u5f0f\u65e0\u6cd5\u89e3\u51b3\u3002", "method": "Maestro\u6846\u67b6\u540c\u65f6\u4f18\u5316\u56fe\u7ed3\u6784\u548c\u8282\u70b9\u914d\u7f6e\uff0c\u5229\u7528\u6587\u672c\u53cd\u9988\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5e76\u9488\u5bf9\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u5728IFBench\u548cHotpotQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMaestro\u5e73\u5747\u4f18\u4e8e\u5176\u4ed6\u4f18\u5316\u56684.86%-12%\uff0c\u4e14\u5728\u63d0\u793a\u4f18\u5316\u65b9\u9762\u4e5f\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8054\u5408\u4f18\u5316\u56fe\u7ed3\u6784\u548c\u914d\u7f6e\u80fd\u89e3\u51b3\u4ec5\u4f18\u5316\u63d0\u793a\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u8868\u73b0\u3002"}}
{"id": "2509.05213", "pdf": "https://arxiv.org/pdf/2509.05213", "abs": "https://arxiv.org/abs/2509.05213", "authors": ["Jiaojiao Zhang", "Yuqi Xu", "Kun Yuan"], "title": "An Efficient Subspace Algorithm for Federated Learning on Heterogeneous Data", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "This work addresses the key challenges of applying federated learning to\nlarge-scale deep neural networks, particularly the issue of client drift due to\ndata heterogeneity across clients and the high costs of communication,\ncomputation, and memory. We propose FedSub, an efficient subspace algorithm for\nfederated learning on heterogeneous data. Specifically, FedSub utilizes\nsubspace projection to guarantee local updates of each client within\nlow-dimensional subspaces, thereby reducing communication, computation, and\nmemory costs. Additionally, it incorporates low-dimensional dual variables to\nmitigate client drift. We provide convergence analysis that reveals the impact\nof key factors such as step size and subspace projection matrices on\nconvergence. Experimental results demonstrate its efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86FedSub\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u6295\u5f71\u964d\u4f4e\u8054\u90a6\u5b66\u4e60\u5728\u5f02\u6784\u6570\u636e\u4e0a\u7684\u901a\u4fe1\u3001\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u540c\u65f6\u7f13\u89e3\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u56e0\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u4ee5\u53ca\u9ad8\u901a\u4fe1\u3001\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "method": "\u5229\u7528\u5b50\u7a7a\u95f4\u6295\u5f71\u786e\u4fdd\u5ba2\u6237\u7aef\u66f4\u65b0\u5728\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u5185\uff0c\u5e76\u7ed3\u5408\u4f4e\u7ef4\u5bf9\u5076\u53d8\u91cf\u7f13\u89e3\u5ba2\u6237\u7aef\u6f02\u79fb\u3002", "result": "\u6536\u655b\u5206\u6790\u63ed\u793a\u4e86\u6b65\u957f\u548c\u5b50\u7a7a\u95f4\u6295\u5f71\u77e9\u9635\u7684\u5f71\u54cd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6548\u7387\u3002", "conclusion": "FedSub\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5f02\u6784\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2509.05145", "pdf": "https://arxiv.org/pdf/2509.05145", "abs": "https://arxiv.org/abs/2509.05145", "authors": ["B\u0142a\u017cej Kotowski", "Nicholas Evans", "Behzad Haki", "Frederic Font", "Sergi Jord\u00e0"], "title": "Exploring Situated Stabilities of a Rhythm Generation System through Variational Cross-Examination", "categories": ["cs.HC", "cs.AI", "cs.SD", "eess.AS"], "comment": "AI Music Creativity 2025", "summary": "This paper investigates GrooveTransformer, a real-time rhythm generation\nsystem, through the postphenomenological framework of Variational\nCross-Examination (VCE). By reflecting on its deployment across three distinct\nartistic contexts, we identify three stabilities: an autonomous drum\naccompaniment generator, a rhythmic control voltage sequencer in Eurorack\nformat, and a rhythm driver for a harmonic accompaniment system. The\nversatility of its applications was not an explicit goal from the outset of the\nproject. Thus, we ask: how did this multistability emerge? Through VCE, we\nidentify three key contributors to its emergence: the affordances of system\ninvariants, the interdisciplinary collaboration, and the situated nature of its\ndevelopment. We conclude by reflecting on the viability of VCE as a descriptive\nand analytical method for Digital Musical Instrument (DMI) design, emphasizing\nits value in uncovering how technologies mediate, co-shape, and are co-shaped\nby users and contexts.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86GrooveTransformer\u7cfb\u7edf\u5728\u827a\u672f\u573a\u666f\u4e2d\u7684\u591a\u7a33\u5b9a\u6027\u53ca\u5176\u6210\u56e0\uff0c\u5e76\u901a\u8fc7VCE\u65b9\u6cd5\u5206\u6790\u4e86\u5176\u5728DMI\u8bbe\u8ba1\u4e2d\u7684\u4ef7\u503c\u3002", "motivation": "\u7814\u7a76GrooveTransformer\u7cfb\u7edf\u5982\u4f55\u5728\u4e0d\u540c\u7684\u827a\u672f\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u591a\u7a33\u5b9a\u6027\uff0c\u4ee5\u53ca\u8fd9\u79cd\u591a\u7a33\u5b9a\u6027\u662f\u5982\u4f55\u4ea7\u751f\u7684\u3002", "method": "\u91c7\u7528Variational Cross- Examination\uff08VCE\uff09\u6846\u67b6\uff0c\u5206\u6790\u7cfb\u7edf\u5728\u4e09\u4e2a\u827a\u672f\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u53d1\u73b0\u7cfb\u7edf\u7684\u591a\u7a33\u5b9a\u6027\u6e90\u4e8e\u7cfb\u7edf\u4e0d\u53d8\u6027\u7684\u529f\u80fd\u3001\u8de8\u5b66\u79d1\u5408\u4f5c\u53ca\u5176\u5f00\u53d1\u7684\u60c5\u5883\u6027\u3002", "conclusion": "VCE\u662f\u63cf\u8ff0\u548c\u5206\u6790DMI\u8bbe\u8ba1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6280\u672f\u4e0e\u7528\u6237\u53ca\u73af\u5883\u4e4b\u95f4\u7684\u76f8\u4e92\u5851\u9020\u5173\u7cfb\u3002"}}
{"id": "2509.05166", "pdf": "https://arxiv.org/pdf/2509.05166", "abs": "https://arxiv.org/abs/2509.05166", "authors": ["Sujit Kumar Sikder", "Jyotirmaya Ijaradar", "Hao Li", "Hichem Omrani"], "title": "Transition of car-based human-mobility in the pandemic era: Data insight from a cross-border region in Europe", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Many transport authorities are collecting and publishing almost real-time\nroad traffic data to meet the growing trend of massive open data, a vital\nresource for foresight decision support systems considering deep data insights.\nUsing such a traffic count dataset, we explored the spatio-temporal transitions\nin the cross-country road traffic volumes in the context of modelling\nbehavioural transitions in car-based human mobility. We developed a\nreproducible workflow for computing multi-dimensional variables of traffic\nflow. This study reports on individual car-based daily travel behaviour\ndetected, before (2016-2018) and during the COVID pandemic (2019-2021), between\nGermany and neighbouring countries (Luxembourg, France and Belgium). In\nrelevance to the net-zero carbon transition, further study should shed light on\nthe interpolation and downscaling approaches at the comprehensive road-network\nlevel for identifying pollution hot spots, causal link to functional landuse\npatterns and calculation of spatial influence area. In the case of Luxembourg,\nthe Bridges and Roads Authority has installed a large digital traffic\nobservatory infrastructure through the adoption of sensor-based IoT\ntechnologies, like other European member states. Since 2016, they have provided\nhigh-performance data processing and published open data on the country's road\ntraffic. The dataset contains an hourly traffic count for different vehicle\ntypes, daily for representative observation points, followed by a major road\nnetwork. The original dataset contains significant missing entries, so\ncomprehensive data harmonization was performed.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u5fb7\u56fd\u4e0e\u90bb\u56fd\u5728COVID\u75ab\u60c5\u524d\u540e\u7684\u8f66\u8f86\u4ea4\u901a\u91cf\u65f6\u7a7a\u53d8\u5316\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u590d\u7684\u5de5\u4f5c\u6d41\u8ba1\u7b97\u4ea4\u901a\u6d41\u591a\u7ef4\u53d8\u91cf\uff0c\u5e76\u4e3a\u78b3\u51c0\u96f6\u8f6c\u578b\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5f00\u653e\u5f0f\u6570\u636e\u7684\u5174\u8d77\uff0c\u5b9e\u65f6\u4ea4\u901a\u6570\u636e\u6210\u4e3a\u652f\u6301\u524d\u77bb\u6027\u51b3\u7b56\u7684\u91cd\u8981\u8d44\u6e90\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528\u6b64\u7c7b\u6570\u636e\u63a2\u7d22\u4eba\u7c7b\u6c7d\u8f66\u51fa\u884c\u884c\u4e3a\u7684\u65f6\u7a7a\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u5362\u68ee\u5821\u7b49\u56fd\u7684\u4ea4\u901a\u8ba1\u6570\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u5de5\u4f5c\u6d41\u8ba1\u7b97\u4ea4\u901a\u6d41\u591a\u7ef4\u53d8\u91cf\uff0c\u5e76\u8fdb\u884c\u4e86\u6570\u636e\u534f\u8c03\u4ee5\u5904\u7406\u7f3a\u5931\u6570\u636e\u3002", "result": "\u7814\u7a76\u62a5\u544a\u4e86\u75ab\u60c5\u524d\u540e\uff082016-2018\u4e0e2019-2021\uff09\u5fb7\u56fd\u4e0e\u90bb\u56fd\u4e4b\u95f4\u7684\u4e2a\u4f53\u6c7d\u8f66\u51fa\u884c\u884c\u4e3a\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u78b3\u51c0\u96f6\u8f6c\u578b\u80cc\u666f\u4e0b\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9053\u8def\u7f51\u7edc\u7ea7\u522b\u7684\u63d2\u503c\u548c\u5c0f\u89c4\u6a21\u65b9\u6cd5\uff0c\u4ee5\u786e\u5b9a\u6c61\u67d3\u70ed\u70b9\u53ca\u5176\u4e0e\u529f\u80fd\u6027\u5730\u7528\u6a21\u5f0f\u7684\u56e0\u679c\u5173\u7cfb\u3002"}}
{"id": "2509.05219", "pdf": "https://arxiv.org/pdf/2509.05219", "abs": "https://arxiv.org/abs/2509.05219", "authors": ["Lennart Luettgau", "Hannah Rose Kirk", "Kobi Hackenburg", "Jessica Bergs", "Henry Davidson", "Henry Ogden", "Divya Siddarth", "Saffron Huang", "Christopher Summerfield"], "title": "Conversational AI increases political knowledge as effectively as self-directed internet search", "categories": ["cs.HC"], "comment": null, "summary": "Conversational AI systems are increasingly being used in place of traditional\nsearch engines to help users complete information-seeking tasks. This has\nraised concerns in the political domain, where biased or hallucinated outputs\ncould misinform voters or distort public opinion. However, in spite of these\nconcerns, the extent to which conversational AI is used for political\ninformation-seeking, as well the potential impact of this use on users'\npolitical knowledge, remains uncertain. Here, we address these questions:\nFirst, in a representative national survey of the UK public (N = 2,499), we\nfind that in the week before the 2024 election as many as 32% of chatbot users\n- and 13% of eligible UK voters - have used conversational AI to seek political\ninformation relevant to their electoral choice. Second, in a series of\nrandomised controlled trials (N = 2,858 total) we find that across issues,\nmodels, and prompting strategies, conversations with AI increase political\nknowledge (increase belief in true information and decrease belief in\nmisinformation) to the same extent as self-directed internet search. Taken\ntogether, our results suggest that although people in the UK are increasingly\nturning to conversational AI for information about politics, this shift may not\nlead to increased public belief in political misinformation.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c32%\u7684\u804a\u5929\u673a\u5668\u4eba\u7528\u6237\u57282024\u5e74\u82f1\u56fd\u5927\u9009\u524d\u4f7f\u7528\u5bf9\u8bdd\u5f0fAI\u83b7\u53d6\u653f\u6cbb\u4fe1\u606f\uff0c\u4f46\u5bf9\u8bdd\u5f0fAI\u7684\u4f7f\u7528\u5e76\u672a\u589e\u52a0\u516c\u4f17\u5bf9\u653f\u6cbb\u9519\u8bef\u4fe1\u606f\u7684\u4fe1\u4efb\uff0c\u6548\u679c\u4e0e\u81ea\u4e3b\u4e92\u8054\u7f51\u641c\u7d22\u76f8\u5f53\u3002", "motivation": "\u968f\u7740\u5bf9\u8bdd\u5f0fAI\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u666e\u53ca\uff0c\u4eba\u4eec\u62c5\u5fe7\u5176\u5728\u653f\u6cbb\u9886\u57df\u53ef\u80fd\u4f20\u64ad\u504f\u89c1\u6216\u865a\u5047\u4fe1\u606f\uff0c\u5f71\u54cd\u9009\u6c11\u51b3\u7b56\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u62c5\u5fe7\u7684\u5b9e\u9645\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u82f1\u56fd\u5168\u56fd\u4ee3\u8868\u6027\u8c03\u67e5\uff08N=2,499\uff09\u548c\u968f\u673a\u5bf9\u7167\u5b9e\u9a8c\uff08N=2,858\uff09\uff0c\u5206\u6790\u5bf9\u8bdd\u5f0fAI\u7684\u4f7f\u7528\u60c5\u51b5\u53ca\u5bf9\u653f\u6cbb\u77e5\u8bc6\u7684\u5f71\u54cd\u3002", "result": "32%\u7684\u804a\u5929\u673a\u5668\u4eba\u7528\u6237\u5728\u9009\u4e3e\u524d\u4f7f\u7528AI\u83b7\u53d6\u653f\u6cbb\u4fe1\u606f\uff1b\u5bf9\u8bdd\u5f0fAI\u63d0\u5347\u653f\u6cbb\u77e5\u8bc6\uff08\u589e\u5f3a\u771f\u5b9e\u4fe1\u606f\u4fe1\u5ff5\uff0c\u51cf\u5c11\u9519\u8bef\u4fe1\u606f\u4fe1\u5ff5\uff09\u7684\u6548\u679c\u4e0e\u4e92\u8054\u7f51\u641c\u7d22\u76f8\u5f53\u3002", "conclusion": "\u5c3d\u7ba1\u5bf9\u8bdd\u5f0fAI\u5728\u82f1\u56fd\u653f\u6cbb\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u4f7f\u7528\u589e\u52a0\uff0c\u4f46\u5176\u53ef\u80fd\u4e0d\u4f1a\u5bfc\u81f4\u516c\u4f17\u5bf9\u653f\u6cbb\u9519\u8bef\u4fe1\u606f\u7684\u4fe1\u4efb\u4e0a\u5347\u3002"}}
{"id": "2509.04510", "pdf": "https://arxiv.org/pdf/2509.04510", "abs": "https://arxiv.org/abs/2509.04510", "authors": ["Michele Materazzini", "Gianluca Morciano", "Jose Manuel Alcalde-Llergo", "Enrique Yeguas-Bolivar", "Giuseppe Calabro", "Andrea Zingoni", "Juri Taborri"], "title": "Combine Virtual Reality and Machine-Learning to Identify the Presence of Dyslexia: A Cross-Linguistic Approach", "categories": ["cs.CL", "cs.HC"], "comment": "22 pages, 10 figures, 5 tables", "summary": "This study explores the use of virtual reality (VR) and artificial\nintelligence (AI) to predict the presence of dyslexia in Italian and Spanish\nuniversity students. In particular, the research investigates whether\nVR-derived data from Silent Reading (SR) tests and self-esteem assessments can\ndifferentiate between students that are affected by dyslexia and students that\nare not, employing machine learning (ML) algorithms. Participants completed\nVR-based tasks measuring reading performance and self-esteem. A preliminary\nstatistical analysis (t tests and Mann Whitney tests) on these data was\nperformed, to compare the obtained scores between individuals with and without\ndyslexia, revealing significant differences in completion time for the SR test,\nbut not in accuracy, nor in self esteem. Then, supervised ML models were\ntrained and tested, demonstrating an ability to classify the presence/absence\nof dyslexia with an accuracy of 87.5 per cent for Italian, 66.6 per cent for\nSpanish, and 75.0 per cent for the pooled group. These findings suggest that VR\nand ML can effectively be used as supporting tools for assessing dyslexia,\nparticularly by capturing differences in task completion speed, but\nlanguage-specific factors may influence classification accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u865a\u62df\u73b0\u5b9e(VR)\u548c\u4eba\u5de5\u667a\u80fd(AI)\u9884\u6d4b\u610f\u5927\u5229\u548c\u897f\u73ed\u7259\u5927\u5b66\u751f\u662f\u5426\u60a3\u6709\u9605\u8bfb\u969c\u788d\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60(ML)\u7b97\u6cd5\u5206\u6790VR\u6570\u636e\u3002", "motivation": "\u63a2\u8ba8VR\u548cAI\u6280\u672f\u5728\u9605\u8bfb\u969c\u788d\u8bca\u65ad\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\uff0c\u4ee5\u63d0\u4f9b\u4e00\u79cd\u65b0\u578b\u7684\u652f\u6301\u5de5\u5177\u3002", "method": "\u53c2\u4e0e\u8005\u5b8c\u6210\u57fa\u4e8eVR\u7684\u9605\u8bfb\u8868\u73b0\u548c\u81ea\u5c0a\u6d4b\u8bd5\uff0c\u901a\u8fc7t\u68c0\u9a8c\u548cMann Whitney\u68c0\u9a8c\u8fdb\u884c\u521d\u6b65\u5206\u6790\uff0c\u968f\u540e\u8bad\u7ec3\u76d1\u7763ML\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "ML\u6a21\u578b\u5728\u610f\u5927\u5229\u3001\u897f\u73ed\u7259\u548c\u5408\u5e76\u7ec4\u4e2d\u7684\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u4e3a87.5%\u300166.6%\u548c75.0%\uff0c\u663e\u793aVR\u548cML\u5728\u6355\u6349\u4efb\u52a1\u5b8c\u6210\u901f\u5ea6\u5dee\u5f02\u65b9\u9762\u6709\u6548\u3002", "conclusion": "VR\u548cML\u53ef\u4f5c\u4e3a\u9605\u8bfb\u969c\u788d\u8bc4\u4f30\u7684\u652f\u6301\u5de5\u5177\uff0c\u4f46\u8bed\u8a00\u56e0\u7d20\u53ef\u80fd\u5f71\u54cd\u5206\u7c7b\u51c6\u786e\u6027\u3002"}}
{"id": "2509.04676", "pdf": "https://arxiv.org/pdf/2509.04676", "abs": "https://arxiv.org/abs/2509.04676", "authors": ["Sasha Mitts"], "title": "An Approach to Grounding AI Model Evaluations in Human-derived Criteria", "categories": ["cs.AI", "cs.HC"], "comment": "4 figures, 6 pages, presented at CHI 2025 Workshop on Human-AI\n  Interaction for Augmented Reasoning", "summary": "In the rapidly evolving field of artificial intelligence (AI), traditional\nbenchmarks can fall short in attempting to capture the nuanced capabilities of\nAI models. We focus on the case of physical world modeling and propose a novel\napproach to augment existing benchmarks with human-derived evaluation criteria,\naiming to enhance the interpretability and applicability of model behaviors.\nGrounding our study in the Perception Test and OpenEQA benchmarks, we conducted\nin-depth interviews and large-scale surveys to identify key cognitive skills,\nsuch as Prioritization, Memorizing, Discerning, and Contextualizing, that are\ncritical for both AI and human reasoning. Our findings reveal that participants\nperceive AI as lacking in interpretive and empathetic skills yet hold high\nexpectations for AI performance. By integrating insights from our findings into\nbenchmark design, we offer a framework for developing more human-aligned means\nof defining and measuring progress. This work underscores the importance of\nuser-centered evaluation in AI development, providing actionable guidelines for\nresearchers and practitioners aiming to align AI capabilities with human\ncognitive processes. Our approach both enhances current benchmarking practices\nand sets the stage for future advancements in AI model evaluation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u6807\u51c6\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3aAI\u6a21\u578b\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u96be\u4ee5\u6355\u6349AI\u6a21\u578b\u7684\u7ec6\u5fae\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7269\u7406\u4e16\u754c\u5efa\u6a21\u9886\u57df\u3002", "method": "\u57fa\u4e8ePerception Test\u548cOpenEQA\u57fa\u51c6\uff0c\u901a\u8fc7\u6df1\u5ea6\u8bbf\u8c08\u548c\u5927\u89c4\u6a21\u8c03\u67e5\uff0c\u8bc6\u522b\u5173\u952e\u8ba4\u77e5\u6280\u80fd\u3002", "result": "\u53c2\u4e0e\u8005\u8ba4\u4e3aAI\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u540c\u7406\u5fc3\u6280\u80fd\uff0c\u4f46\u5bf9AI\u8868\u73b0\u6709\u9ad8\u671f\u671b\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u7528\u6237\u4e2d\u5fc3\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765AI\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u6846\u67b6\u3002"}}
{"id": "2509.04809", "pdf": "https://arxiv.org/pdf/2509.04809", "abs": "https://arxiv.org/abs/2509.04809", "authors": ["Haechang Kim", "Hao Chen", "Can Li", "Jong Min Lee"], "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models", "categories": ["cs.AI", "cs.HC"], "comment": "31 pages total", "summary": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain.", "AI": {"tldr": "TalkToAgent\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u4ea4\u4e92\u5f0f\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u900f\u660e\u5ea6\uff0c\u5f25\u8865XRL\u65b9\u6cd5\u4e0e\u7528\u6237\u9700\u6c42\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u73b0\u6709XRL\u65b9\u6cd5\u7684\u89e3\u91ca\u7ed3\u679c\u4e0d\u591f\u76f4\u89c2\u4e14\u5de5\u5177\u8986\u76d6\u4e0d\u5168\u9762\uff0c\u5bfc\u81f4\u7528\u6237\u96be\u4ee5\u7406\u89e3\u590d\u6742\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "method": "\u63d0\u51faTalkToAgent\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u4e13\u7528LLM\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u81ea\u52a8\u5339\u914d\u7528\u6237\u67e5\u8be2\u4e0eXRL\u5de5\u5177\uff0c\u5e76\u63d0\u4f9b\u72b6\u6001\u53d8\u91cf\u3001\u9884\u671f\u7ed3\u679c\u6216\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "result": "\u5728\u56db\u91cd\u6c34\u7bb1\u8fc7\u7a0b\u63a7\u5236\u95ee\u9898\u4e0a\u9a8c\u8bc1\u4e86TalkToAgent\u7684\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u6620\u5c04\u80fd\u529b\uff0c\u4e14\u53cd\u4e8b\u5b9e\u751f\u6210\u7684\u5931\u8d25\u7387\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "TalkToAgent\u80fd\u6709\u6548\u89e3\u91ca\u548c\u60c5\u5883\u5316\u667a\u80fd\u4f53\u7684\u884c\u4e3a\uff0c\u63d0\u5347\u4e86\u900f\u660e\u5ea6\u548c\u7528\u6237\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.04889", "pdf": "https://arxiv.org/pdf/2509.04889", "abs": "https://arxiv.org/abs/2509.04889", "authors": ["Dominik Pegler", "David Steyrl", "Mengfan Zhang", "Alexander Karner", "Jozsef Arato", "Frank Scharnowski", "Filip Melinscak"], "title": "SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": "60 pages (30 main text, 30 appendix), 20 figures (5 in main text, 15\n  in appendix)", "summary": "Advances in computer vision have opened new avenues for clinical\napplications, particularly in computerized exposure therapy where visual\nstimuli can be dynamically adjusted based on patient responses. As a critical\nstep toward such adaptive systems, we investigated whether pretrained computer\nvision models can accurately predict fear levels from spider-related images. We\nadapted three diverse models using transfer learning to predict human fear\nratings (on a 0-100 scale) from a standardized dataset of 313 images. The\nmodels were evaluated using cross-validation, achieving an average mean\nabsolute error (MAE) between 10.1 and 11.0. Our learning curve analysis\nrevealed that reducing the dataset size significantly harmed performance,\nthough further increases yielded no substantial gains. Explainability\nassessments showed the models' predictions were based on spider-related\nfeatures. A category-wise error analysis further identified visual conditions\nassociated with higher errors (e.g., distant views and artificial/painted\nspiders). These findings demonstrate the potential of explainable computer\nvision models in predicting fear ratings, highlighting the importance of both\nmodel explainability and a sufficient dataset size for developing effective\nemotion-aware therapeutic technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u9884\u8bad\u7ec3\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u662f\u5426\u80fd\u51c6\u786e\u9884\u6d4b\u8718\u86db\u76f8\u5173\u56fe\u50cf\u7684\u6050\u60e7\u7b49\u7ea7\uff0c\u91c7\u7528\u4e09\u79cd\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u9884\u6d4b313\u5f20\u6807\u51c6\u5316\u56fe\u50cf\u7684\u4eba\u7c7b\u6050\u60e7\u8bc4\u5206\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a10.1-11.0\u3002\u7814\u7a76\u53d1\u73b0\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6a21\u578b\u7684\u9884\u6d4b\u57fa\u4e8e\u8718\u86db\u76f8\u5173\u7279\u5f81\uff0c\u8868\u660e\u53ef\u89e3\u91ca\u6a21\u578b\u5728\u60c5\u611f\u6cbb\u7597\u6280\u672f\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u8fdb\u6b65\u4e3a\u4e34\u5e8a\u5e94\u7528\uff08\u5982\u8ba1\u7b97\u673a\u5316\u66b4\u9732\u7597\u6cd5\uff09\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u9884\u8bad\u7ec3\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u662f\u5426\u80fd\u51c6\u786e\u9884\u6d4b\u6050\u60e7\u7b49\u7ea7\uff0c\u4e3a\u5f00\u53d1\u9002\u5e94\u6027\u60c5\u611f\u6cbb\u7597\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e09\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u9884\u6d4b\u8718\u86db\u76f8\u5173\u56fe\u50cf\u7684\u6050\u60e7\u8bc4\u5206\uff080-100\uff09\u3002\u4f7f\u7528313\u5f20\u6807\u51c6\u5316\u56fe\u50cf\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u5e76\u5206\u6790\u5b66\u4e60\u66f2\u7ebf\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6a21\u578b\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a10.1-11.0\u3002\u5b66\u4e60\u66f2\u7ebf\u663e\u793a\u6570\u636e\u96c6\u5927\u5c0f\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u4f46\u589e\u52a0\u6570\u636e\u91cf\u540e\u65e0\u663e\u8457\u63d0\u5347\u3002\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8868\u660e\u6a21\u578b\u4f9d\u8d56\u8718\u86db\u76f8\u5173\u7279\u5f81\u3002\u9519\u8bef\u5206\u6790\u53d1\u73b0\u67d0\u4e9b\u89c6\u89c9\u6761\u4ef6\uff08\u5982\u8fdc\u89c6\u56fe\u548c\u4eba\u5de5\u8718\u86db\uff09\u8bef\u5dee\u8f83\u9ad8\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u53ef\u89e3\u91ca\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u9884\u6d4b\u6050\u60e7\u7b49\u7ea7\u4e2d\u7684\u6f5c\u529b\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u8db3\u591f\u6570\u636e\u96c6\u5927\u5c0f\u5bf9\u5f00\u53d1\u6709\u6548\u60c5\u611f\u6cbb\u7597\u6280\u672f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.04908", "pdf": "https://arxiv.org/pdf/2509.04908", "abs": "https://arxiv.org/abs/2509.04908", "authors": ["Hongyi Jing", "Jiafu Chen", "Chen Rao", "Ziqiang Dang", "Jiajie Teng", "Tianyi Chu", "Juncheng Mo", "Shuo Fang", "Huaizhong Lin", "Rui Lv", "Chenguang Ma", "Lei Zhao"], "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser.", "AI": {"tldr": "SparkUI-Parser\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u8fde\u7eed\u5efa\u6a21\u5750\u6807\u548c\u5f15\u5165\u62d2\u7edd\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MLLMs\u5728GUI\u89e3\u6790\u4e2d\u7684\u7cbe\u5ea6\u548c\u901f\u5ea6\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728GUI\u89e3\u6790\u4e2d\u5b58\u5728\u5750\u6807\u79bb\u6563\u5efa\u6a21\u5bfc\u81f4\u7684\u4f4e\u7cbe\u5ea6\u548c\u6162\u63a8\u7406\u901f\u5ea6\uff0c\u4e14\u65e0\u6cd5\u89e3\u6790\u6574\u4e2a\u754c\u9762\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u9884\u8bad\u7ec3MLLM\u7684\u8fde\u7eed\u5750\u6807\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u4ee4\u724c\u8def\u7531\u5668\u548c\u5750\u6807\u89e3\u7801\u5668\uff0c\u5e76\u5f15\u5165\u4e86\u6539\u8fdb\u7684\u5308\u7259\u5229\u5339\u914d\u7b97\u6cd5\u4f5c\u4e3a\u62d2\u7edd\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSparkUI-Parser\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "SparkUI-Parser\u4e3aGUI\u89e3\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u754c\u9762\u89e3\u6790\u9700\u6c42\u3002"}}
