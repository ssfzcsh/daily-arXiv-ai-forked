<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 15]
- [cs.MM](#cs.MM) [Total: 8]
- [cs.LO](#cs.LO) [Total: 5]
- [cs.HC](#cs.HC) [Total: 46]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 16]
- [cs.DB](#cs.DB) [Total: 11]
- [cs.AR](#cs.AR) [Total: 8]
- [hep-ex](#hep-ex) [Total: 1]
- [math.RA](#math.RA) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.IR](#cs.IR) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [math.LO](#math.LO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.AI](#cs.AI) [Total: 11]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Refactoring-Aware Patch Integration Across Structurally Divergent Java Forks](https://arxiv.org/abs/2508.06718)
*Daniel Ogenrwot,John Businge*

Main category: cs.SE

TL;DR: 研究探讨GitHub长期分叉（变体）的代码补丁集成问题，提出RePatch工具，通过重构感知解决结构漂移问题，成功集成52.8%原本失败的补丁。


<details>
  <summary>Details</summary>
Motivation: GitHub上长期分叉（变体）的开发轨迹独立，代码结构漂移导致补丁集成困难，需语义推理工具支持。

Method: 基于RefMerge框架扩展的RePatch系统，支持非对称补丁传输，通过逆向重构对齐补丁上下文并保留变体意图。

Result: 在478个补丁请求中，Git cherry-pick失败率为64.4%，而RePatch成功集成52.8%的失败补丁。

Conclusion: 语法工具局限性明显，语义推理在补丁传播中至关重要，RePatch为变体开发提供有效支持。

Abstract: While most forks on platforms like GitHub are short-lived and used for social
collaboration, a smaller but impactful subset evolve into long-lived forks,
referred to here as variants, that maintain independent development
trajectories. Integrating bug-fix patches across such divergent variants poses
challenges due to structural drift, including refactorings that rename,
relocate, or reorganize code elements and obscure semantic correspondence. This
paper presents an empirical study of patch integration failures in 14 divergent
pair of variants and introduces RePatch, a refactoring-aware integration system
for Java repositories. RePatch extends the RefMerge framework, originally
designed for symmetric merges, by supporting asymmetric patch transfer. RePatch
inverts refactorings in both the source and target to realign the patch
context, applies the patch, and replays the transformations to preserve the
intent of the variant. In our evaluation of 478 bug-fix pull requests, Git
cherry-pick fails in 64.4% of cases due to structural misalignments, while
RePatch successfully integrates 52.8% of the previously failing patches. These
results highlight the limitations of syntax-based tools and the need for
semantic reasoning in variant-aware patch propagation.

</details>


### [2] [Quo Vadis, Code Review? Exploring the Future of Code Review](https://arxiv.org/abs/2508.06879)
*Michael Dorner,Andreas Bauer,Darja Šmite,Lukas Thode,Daniel Mendez,Ricardo Britto,Stephan Lukasczyk,Ehsan Zabardast,Michael Kormann*

Main category: cs.SE

TL;DR: 论文探讨了从业者对当前代码审查的反思及对未来变化的预期，并分析了这些变化对代码审查演变的潜在长期风险。


<details>
  <summary>Details</summary>
Motivation: 研究从业者对代码审查的现状反思和未来预期，以理解其对协作软件工程的影响。

Method: 通过调查和分析从业者的观点，探讨代码审查的未来变化及其潜在影响。

Result: 揭示了从业者对代码审查的反思和预期变化，以及这些变化可能带来的长期风险。

Conclusion: 代码审查的未来变化可能对其在协作软件工程中的角色产生深远影响，需谨慎评估潜在风险。

Abstract: Code review has long been a core practice in collaborative software
engineering. In this research, we explore how practitioners reflect on code
review today and what changes they anticipate in the near future. We then
discuss the potential long-term risks of these anticipated changes for the
evolution of code review and its role in collaborative software engineering.

</details>


### [3] [Multi-Modal Requirements Data-based Acceptance Criteria Generation using LLMs](https://arxiv.org/abs/2508.06888)
*Fanyu Wang,Chetan Arora,Yonghui Liu,Kaicheng Huang,Chakkrit Tantithamthavorn,Aldeida Aleti,Dishan Sambathkumar,David Lo*

Main category: cs.SE

TL;DR: 提出了一种基于检索增强生成（RAG）的多模态方法（RAGcceptance M2RE），用于从文本和视觉UI数据中自动生成准确且全面的验收标准（ACs），工业案例研究证实其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 手动创建验收标准（ACs）在涉及用户界面的应用中尤其困难，因需结合领域知识与视觉上下文，但文本需求常无法充分捕获这些信息。

Method: 利用检索增强生成（RAG）技术，结合多模态需求数据（文本和UI视觉信息）自动生成ACs。

Result: 工业案例研究表明，多模态数据整合显著提升了生成ACs的相关性、正确性和可理解性，减少了人工努力。

Conclusion: 多模态RAG技术在优化软件验证流程和提高开发效率方面具有显著潜力，具备行业应用价值。

Abstract: Acceptance criteria (ACs) play a critical role in software development by
clearly defining the conditions under which a software feature satisfies
stakeholder expectations. However, manually creating accurate, comprehensive,
and unambiguous acceptance criteria is challenging, particularly in user
interface-intensive applications, due to the reliance on domain-specific
knowledge and visual context that is not always captured by textual
requirements alone. To address these challenges, we propose RAGcceptance M2RE,
a novel approach that leverages Retrieval-Augmented Generation (RAG) to
generate acceptance criteria from multi-modal requirements data, including both
textual documentation and visual UI information. We systematically evaluated
our approach in an industrial case study involving an education-focused
software system used by approximately 100,000 users. The results indicate that
integrating multi-modal information significantly enhances the relevance,
correctness, and comprehensibility of the generated ACs. Moreover, practitioner
evaluations confirm that our approach effectively reduces manual effort,
captures nuanced stakeholder intent, and provides valuable criteria that domain
experts may overlook, demonstrating practical utility and significant potential
for industry adoption. This research underscores the potential of multi-modal
RAG techniques in streamlining software validation processes and improving
development efficiency. We also make our implementation and a dataset
available.

</details>


### [4] [Integrating Rules and Semantics for LLM-Based C-to-Rust Translation](https://arxiv.org/abs/2508.06926)
*Feng Luo,Kexing Ji,Cuiyun Gao,Shuzheng Gao,Jia Feng,Kui Liu,Xin Xia,Michael R. Lyu*

Main category: cs.SE

TL;DR: IRENE是一个基于LLM的框架，通过整合规则和语义提升C代码到Rust的自动翻译效果，解决了现有方法在语法规则和语义一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在C代码到Rust的翻译中存在语法规则适应性和语义一致性不足的问题，IRENE旨在通过规则增强、语义总结和错误驱动翻译来改进这些方面。

Method: IRENE包含三个模块：规则增强检索模块、结构化总结模块和错误驱动翻译模块，分别用于提升Rust规则处理、增强C代码语义理解和迭代优化翻译结果。

Result: 在两个数据集（xCodeEval和HW-Bench）和多个LLM上评估，IRENE在翻译准确性和安全性上表现优异。

Conclusion: IRENE通过整合规则和语义，显著提升了C到Rust代码翻译的质量和安全性，为自动迁移工具提供了新思路。

Abstract: Automated translation of legacy C code into Rust aims to ensure memory safety
while reducing the burden of manual migration. Early approaches in code
translation rely on static rule-based methods, but they suffer from limited
coverage due to dependence on predefined rule patterns. Recent works regard the
task as a sequence-to-sequence problem by leveraging large language models
(LLMs). Although these LLM-based methods are capable of reducing unsafe code
blocks, the translated code often exhibits issues in following Rust rules and
maintaining semantic consistency. On one hand, existing methods adopt a direct
prompting strategy to translate the C code, which struggles to accommodate the
syntactic rules between C and Rust. On the other hand, this strategy makes it
difficult for LLMs to accurately capture the semantics of complex code. To
address these challenges, we propose IRENE, an LLM-based framework that
Integrates RulEs aNd sEmantics to enhance translation. IRENE consists of three
modules: 1) a rule-augmented retrieval module that selects relevant translation
examples based on rules generated from a static analyzer developed by us,
thereby improving the handling of Rust rules; 2) a structured summarization
module that produces a structured summary for guiding LLMs to enhance the
semantic understanding of C code; 3) an error-driven translation module that
leverages compiler diagnostics to iteratively refine translations. We evaluate
IRENE on two datasets (xCodeEval, a public dataset, and HW-Bench, an industrial
dataset provided by Huawei) and eight LLMs, focusing on translation accuracy
and safety.

</details>


### [5] [When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs'' for Human-AI Interaction](https://arxiv.org/abs/2508.06942)
*Zhenchang Xing,Yang Liu,Zhuo Cheng,Qing Huang,Dehai Zhao,Daniel Sun,Chenhua Liu*

Main category: cs.SE

TL;DR: 论文提出了一种名为CNL-P的控制性自然语言提示方法，结合了提示工程和软件工程的最佳实践，以减少自然语言的歧义，提升大语言模型（LLM）的输出质量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的增强，其应用领域日益广泛，但自然语言提示的歧义性可能影响模型输出的质量和一致性。作者希望通过引入软件工程的原则，优化提示设计。

Method: 提出CNL-P方法，通过精确的语法结构和语义规范消除自然语言的歧义，并开发了NL2CNL-P转换工具及静态检查工具。

Result: 实验表明，CNL-P显著提升了LLM输出的质量和一致性，实现了提示工程与软件工程的有机结合。

Conclusion: CNL-P弥补了提示工程与传统软件工程间的鸿沟，为以自然语言为中心的新编程范式奠定了基础。

Abstract: With the growing capabilities of large language models (LLMs), they are
increasingly applied in areas like intelligent customer service, code
generation, and knowledge management. Natural language (NL) prompts act as the
``APIs'' for human-LLM interaction. To improve prompt quality, best practices
for prompt engineering (PE) have been developed, including writing guidelines
and templates. Building on this, we propose Controlled NL for Prompt (CNL-P),
which not only incorporates PE best practices but also draws on key principles
from software engineering (SE). CNL-P introduces precise grammar structures and
strict semantic norms, further eliminating NL's ambiguity, allowing for a
declarative but structured and accurate expression of user intent. This helps
LLMs better interpret and execute the prompts, leading to more consistent and
higher-quality outputs. We also introduce an NL2CNL-P conversion tool based on
LLMs, enabling users to write prompts in NL, which are then transformed into
CNL-P format, thus lowering the learning curve of CNL-P. In particular, we
develop a linting tool that checks CNL-P prompts for syntactic and semantic
accuracy, applying static analysis techniques to NL for the first time.
Extensive experiments demonstrate that CNL-P enhances the quality of LLM
responses through the novel and organic synergy of PE and SE. We believe that
CNL-P can bridge the gap between emerging PE and traditional SE, laying the
foundation for a new programming paradigm centered around NL.

</details>


### [6] [An Empirical Study on Method-Level Performance Evolution in Open-Source Java Projects](https://arxiv.org/abs/2508.07084)
*Kaveh Shahedi,Nana Gyambrah,Heng Li,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: 该研究通过大规模实证分析，探讨了方法级代码变更对性能演化的影响，发现32.7%的变更对性能有显著影响，其中性能退步比改进更常见，挑战了传统的风险分层开发策略。


<details>
  <summary>Details</summary>
Motivation: 性能是软件开发中的关键质量属性，但方法级代码变更对性能演化的影响缺乏细粒度的实证验证，开发者的直觉假设往往未经证实。

Method: 研究分析了15个成熟的开源Java项目的739次提交中的1,499个方法级变更，使用JMH进行精确性能测量，通过字节码插桩捕获方法执行指标，并系统分析了时间模式、变更类型、开发者与复杂性因素以及领域规模交互。

Result: 研究发现32.7%的变更对性能有显著影响，性能退步比改进多1.3倍；算法变更潜力大但风险高；资深开发者的变更更稳定；代码复杂性与退步概率正相关；小型Web服务器项目性能最不稳定。

Conclusion: 研究表明自动性能测试应集成到持续集成流水线中，以捕捉性能变化并指导开发实践。

Abstract: Performance is a critical quality attribute in software development, yet the
impact of method-level code changes on performance evolution remains poorly
understood. While developers often make intuitive assumptions about which types
of modifications are likely to cause performance regressions or improvements,
these beliefs lack empirical validation at a fine-grained level. We conducted a
large-scale empirical study analyzing performance evolution in 15 mature
open-source Java projects hosted on GitHub. Our analysis encompassed 739
commits containing 1,499 method-level code changes, using Java Microbenchmark
Harness (JMH) for precise performance measurement and rigorous statistical
analysis to quantify both the significance and magnitude of performance
variations. We employed bytecode instrumentation to capture method-specific
execution metrics and systematically analyzed four key aspects: temporal
performance patterns, code change type correlations, developer and complexity
factors, and domain-size interactions. Our findings reveal that 32.7% of
method-level changes result in measurable performance impacts, with regressions
occurring 1.3 times more frequently than improvements. Contrary to conventional
wisdom, we found no significant differences in performance impact distributions
across code change categories, challenging risk-stratified development
strategies. Algorithmic changes demonstrate the highest improvement potential
but carry substantial regression risk. Senior developers produce more stable
changes with fewer extreme variations, while code complexity correlates with
increased regression likelihood. Domain-size interactions reveal significant
patterns, with web server + small projects exhibiting the highest performance
instability. Our study provides empirical evidence for integrating automated
performance testing into continuous integration pipelines.

</details>


### [7] [From Noise to Knowledge: Interactive Summaries for Developer Alerts](https://arxiv.org/abs/2508.07169)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: CLARITY 是一个通过交互式查询帮助程序员理解 bug 报告的工具，通过总结规则对相关警告进行分组，提升问题定位效率。


<details>
  <summary>Details</summary>
Motivation: 程序员在分析 bug 报告时通常需要逐条检查警告，而 CLARITY 旨在通过识别重复模式和关系，提升认知效率。

Method: CLARITY 通过用户反馈动态生成总结规则，对警告进行自定义分组，并利用规则推断算法识别常见问题。

Result: 在用户研究中，使用 CLARITY 的用户能更快、更有信心地定位问题的根本原因。平均仅需 11.8 次交互即可对齐规则与用户标签。

Conclusion: CLARITY 的主动学习总结机制显著提升了交互式警告分析的效率，支持个性化分组需求。

Abstract: Programmers using bug-finding tools often review their reported warnings one
by one. Based on the insight that identifying recurring themes and
relationships can enhance the cognitive process of sensemaking, we propose
CLARITY, which supports interpreting tool-generated warnings through
interactive inquiry. CLARITY derives summary rules for custom grouping of
related warnings with active feedback. As users mark warnings as interesting or
uninteresting, CLARITY's rule inference algorithm surfaces common symptoms,
highlighting structural similarities in containment, subtyping, invoked
methods, accessed fields, and expressions.
  We demonstrate CLARITY on Infer and SpotBugs warnings across two mature Java
projects. In a within-subject user study with 14 participants, users
articulated root causes for similar uninteresting warnings faster and with more
confidence using CLARITY. We observed significant individual variation in
desired grouping, reinforcing the need for customizable sensemaking. Simulation
shows that with rule-level feedback, only 11.8 interactions are needed on
average to align all inferred rules with a simulated user's labels (vs. 17.8
without). Our evaluation suggests that CLARITY's active learning-based
summarization enhances interactive warning sensemaking.

</details>


### [8] [Dynamic Benchmark Construction for Evaluating Large Language Models on Real-World Codes](https://arxiv.org/abs/2508.07180)
*Zhe Zhang,Runlin Liu,Aishan Liu,Xingyu Liu,Xiang Gao,Hailong Sun*

Main category: cs.SE

TL;DR: 提出了一种名为CODE2BENCH的动态基准测试方法，用于评估大语言模型（LLMs）在真实代码生成任务中的性能，解决了数据污染和测试严谨性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在数据污染和测试不严谨的问题，限制了其对LLMs性能的有效评估，因此需要开发更可靠的基准测试方法。

Method: 提出了CODE2BENCH，包含三个创新点：自动动态更新代码库以减少数据污染、基于依赖分析的函数分类、以及基于属性的测试生成严谨测试套件。

Result: 构建了CODE2BENCH-2505基准，包含1,163个任务，覆盖880个Python项目，测试显示LLMs在复杂跨语言任务中表现较差，但在Python任务中表现较好。

Conclusion: CODE2BENCH提供了一种抗污染、语言无关的动态基准构建方法，为LLMs在真实软件开发任务中的全面评估提供了基础。

Abstract: As large language models LLMs) become increasingly integrated into software
development workflows, rigorously evaluating their performance on complex,
real-world code generation tasks has become essential. However, existing
benchmarks often suffer from data contamination and limited test rigor,
constraining their ability to reveal model failures effectively. To address
these, we present CODE2BENCH, a end-to-end pipeline for dynamically
constructing robust and contamination-resistant benchmarks from real-world
GitHub repositories. Specifically, CODE2BENCH introduces three key innovations:
(1) Automated Dynamism, achieved through periodic ingestion of recent code to
minimize training data contamination; (2) Scope Graph-based dependency
analysis, which enables structured classification of functions into benchmark
instances with controlled dependency levels (distinguishing between
Self-Contained (SC) tasks for cross-language evaluation and Weakly
Self-Contained (WSC) tasks involving permitted library usage); and (3)
Property-Based Testing (PBT) for the automated synthesis of rigorous test
suites to enable thorough functional verification. Using this pipeline, we
construct CODE2BENCH-2505, the first benchmark derived from 880 recent Python
projects spanning diverse domains, comprising 1,163 code generation tasks with
100% average branch coverage on ground-truth implementations. Extensive
evaluation of 16 LLMs using CODE2BENCH-2505 reveals that models consistently
struggle with SC tasks requiring complex, non-standard logic and cross-language
transfer, while showing relatively stronger performance on WSC tasks in Python.
Our work introduces a contamination-resistant, language-agnostic methodology
for dynamic benchmark construction, offering a principled foundation for the
comprehensive and realistic evaluation of LLMs on real-world software
development tasks.

</details>


### [9] [TraceLens: Question-Driven Debugging for Taint Flow Understanding](https://arxiv.org/abs/2508.07198)
*Burak Yetiştiren,Hong Jin Kang,Miryung Kim*

Main category: cs.SE

TL;DR: TraceLens是一个基于问答调试的污点分析工具，显著提升了用户分析数据流的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有污点分析工具缺乏对数据流的调试能力，难以回答'为什么'、'为什么没有'和'假设'类问题。

Method: 提出TraceLens，支持问答式调试，包括假设分析，帮助用户理解数据流的影响。

Result: 用户研究表明，TraceLens的平均准确率比CodeQL高21%，且用户心理负担减少45%。

Conclusion: TraceLens通过问答式调试显著提升了污点分析的可用性和分析效果。

Abstract: Taint analysis is a security analysis technique used to track the flow of
potentially dangerous data through an application and its dependent libraries.
Investigating why certain unexpected flows appear and why expected flows are
missing is an important sensemaking process during end-user taint analysis.
Existing taint analysis tools often do not provide this end-user debugging
capability, where developers can ask why, why-not, and what-if questions about
dataflows and reason about the impact of configuring sources and sinks, and
models of 3rd-party libraries that abstract permissible and impermissible data
flows. Furthermore, a tree-view or a list-view used in existing
taint-analyzer's visualization makes it difficult to reason about the global
impact on connectivity between multiple sources and sinks.
  Inspired by the insight that sensemaking tool-generated results can be
significantly improved by a QA inquiry process, we propose TraceLens, a first
end-user question-answer style debugging interface for taint analysis. It
enables a user to ask why, why-not, and what-if questions to investigate the
existence of suspicious flows, the non-existence of expected flows, and the
global impact of third-party library models. TraceLens performs speculative
what-if analysis, to help a user in debugging how different connectivity
assumptions affect overall results. A user study with 12 participants shows
that participants using TraceLens achieved 21% higher accuracy on average,
compared to CodeQL. They also reported a 45% reduction in mental demand
(NASA-TLX) and rated higher confidence in identifying relevant flows using
TraceLens.

</details>


### [10] [AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation](https://arxiv.org/abs/2508.07371)
*Yi Zhong,Hongchao Liu,Di ZHao*

Main category: cs.SE

TL;DR: 提出一种基于硬件描述语言（HDL）的断言生成方法，结合轻量级参数可调的大语言模型（LLM）与Unsloth平台，高效生成测试用例，显著降低成本且保持准确性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂性增加，对自动化测试与维护工具的需求急剧上升。

Method: 结合轻量级参数可调的LLM与Unsloth平台，自动生成测试用例。

Result: 实证表明，该方法能高效生成严格符合硬件逻辑的断言。

Conclusion: 该框架为现代软件测试与维护提供了稳健灵活的解决方案。

Abstract: As the complexity of software systems continues to increase, the demand for
automated testing and maintenance tools is growing exponentially. To meet this
urgent need, we propose a new assertion generation method based on Hardware
Description Language (HDL). This method combines a lightweight,
parameter-adjustable large language model (LLM) with the Unsloth platform to
automatically generate test cases, thereby significantly reducing training
costs without sacrificing accuracy or generalization performance. Empirical
evaluation shows that our method can efficiently generate assertions that
strictly conform to the hardware logic. This framework provides a robust and
flexible solution to modern software testing and maintenance challenges.
https://github.com/liusu-orange/AutoAssert-1 and
https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.

</details>


### [11] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: Mo2oM是一种将单体架构转换为微服务的软聚类框架，通过允许组件概率性属于多个微服务，显著提升结构模块化和服务间调用的效率。


<details>
  <summary>Details</summary>
Motivation: 传统硬聚类方法在微服务提取中增加了服务间耦合，降低了服务内内聚。Mo2oM旨在通过软聚类和组件复制来减少通信开销。

Method: 结合语义嵌入和方法调用图的结构依赖关系，使用图神经网络进行软聚类生成微服务。

Result: 在四种开源单体基准测试中，Mo2oM在结构模块化、通信开销等方面均有显著提升。

Conclusion: Mo2oM为微服务提取提供了一种更灵活、高效的解决方案，尤其适用于需要减少服务间通信的场景。

Abstract: Modern software systems are increasingly shifting from monolithic
architectures to microservices to enhance scalability, maintainability, and
deployment flexibility. Existing microservice extraction methods typically rely
on hard clustering, assigning each software component to a single microservice.
This approach often increases inter-service coupling and reduces intra-service
cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a
framework that formulates microservice extraction as a soft clustering problem,
allowing components to belong probabilistically to multiple microservices. This
approach is inspired by expert-driven decompositions, where practitioners
intentionally replicate certain software components across services to reduce
communication overhead. Mo2oM combines deep semantic embeddings with structural
dependencies extracted from methodcall graphs to capture both functional and
architectural relationships. A graph neural network-based soft clustering
algorithm then generates the final set of microservices. We evaluate Mo2oM on
four open-source monolithic benchmarks and compare it against eight
state-of-the-art baselines. Our results demonstrate that Mo2oM achieves
improvements of up to 40.97% in structural modularity (balancing cohesion and
coupling), 58% in inter-service call percentage (communication overhead),
26.16% in interface number (modularity and decoupling), and 38.96% in
non-extreme distribution (service size balance) across all benchmarks.

</details>


### [12] [Adopting Road-Weather Open Data in Route Recommendation Engine](https://arxiv.org/abs/2508.07881)
*Henna Tammia,Benjamin Kämä,Ella Peltonen*

Main category: cs.SE

TL;DR: 论文探讨了如何高效利用芬兰的开放道路数据接口DigiTraffic，提出了处理大规模道路天气和交通数据的挑战，并开发了一种基于简单路由应用的个性化道路推荐方法。


<details>
  <summary>Details</summary>
Motivation: 芬兰的DigiTraffic接口提供了丰富的实时道路数据，但如何高效利用这些数据为实际应用服务是一个挑战。

Method: 作者分析了DigiTraffic中的道路天气相关属性，提出了一种数据处理方法，并结合机器学习工具开发了一个个性化道路推荐引擎。

Result: 通过在真实数据上的验证，该方法成功为三种不同驾驶者推荐了个性化路线。

Conclusion: 研究表明，合理的数据预处理和方法设计可以高效利用开放道路数据，实现个性化推荐。

Abstract: Digitraffic, Finland's open road data interface, provides access to
nationwide road sensors with more than 2,300 real-time attributes from 1,814
stations. However, efficiently utilizing such a versatile data API for a
practical application requires a deeper understanding of the data qualities,
preprocessing phases, and machine learning tools. This paper discusses the
challenges of large-scale road weather and traffic data. We go through the
road-weather-related attributes from DigiTraffic as a practical example of
processes required to work with such a dataset. In addition, we provide a
methodology for efficient data utilization for the target application, a
personalized road recommendation engine based on a simple routing application.
We validate our solution based on real-world data, showing we can efficiently
identify and recommend personalized routes for three different driver profiles.

</details>


### [13] [SHIELDA: Structured Handling of Exceptions in LLM-Driven Agentic Workflows](https://arxiv.org/abs/2508.07935)
*Jingwen Zhou,Jieshan Chen,Qinghua Lu,Dehai Zhao,Liming Zhu*

Main category: cs.SE

TL;DR: SHIELDA是一个模块化的异常处理框架，用于解决LLM代理系统中的异常问题，通过分类和执行预定义的模式实现有效恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法对异常处理较表面，无法追溯到根本原因，恢复逻辑脆弱。SHIELDA旨在解决这些问题。

Method: 提出36种异常类型的分类法，并设计SHIELDA框架，包括异常分类器、处理模式注册表和结构化执行器。

Result: 通过AutoPR代理的案例研究证明了SHIELDA在跨阶段异常恢复中的有效性。

Conclusion: SHIELDA能够有效处理LLM代理工作流中的异常，并支持模块化恢复策略。

Abstract: Large Language Model (LLM) agentic systems are software systems powered by
LLMs that autonomously reason, plan, and execute multi-step workflows to
achieve human goals, rather than merely executing predefined steps. During
execution, these workflows frequently encounter exceptions. Existing exception
handling solutions often treat exceptions superficially, failing to trace
execution-phase exceptions to their reasoning-phase root causes. Furthermore,
their recovery logic is brittle, lacking structured escalation pathways when
initial attempts fail. To tackle these challenges, we first present a
comprehensive taxonomy of 36 exception types across 12 agent artifacts.
Building on this, we propose SHIELDA (Structured Handling of Exceptions in
LLM-Driven Agentic Workflows), a modular runtime exception handling framework
for LLM agentic workflows. SHIELDA uses an exception classifier to select a
predefined exception handling pattern from a handling pattern registry. These
patterns are then executed via a structured handling executor, comprising local
handling, flow control, and state recovery, to enable phase-aware recovery by
linking exceptions to their root causes and facilitating composable strategies.
We validate SHIELDA's effectiveness through a case study on the AutoPR agent,
demonstrating effective, cross-phase recovery from a reasoning-induced
exception.

</details>


### [14] [Exploring the Challenges and Opportunities of AI-assisted Codebase Generation](https://arxiv.org/abs/2508.07966)
*Philipp Eibl,Sadra Sabouri,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: 研究探讨了代码库AI助手（CBAs）的使用情况，发现尽管其能力强大，但开发者的满意度较低，主要原因包括功能不足、代码质量差和沟通问题。


<details>
  <summary>Details</summary>
Motivation: 研究CBAs如何满足开发者需求，弥补现有研究中关于代码库级生成模型影响的不足。

Method: 通过反平衡用户研究和访谈（n = 16），分析开发者与CBAs的互动方式及问题。

Result: 开发者对生成的代码库满意度低（均分2.8/5），功能不足是最常见的不满原因，还发现了六类挑战和五类使用障碍。

Conclusion: 提出需要改进CBAs的设计以提升其效率和实用性，同时比较了21款商业CBAs的能力与用户挑战。

Abstract: Recent AI code assistants have significantly improved their ability to
process more complex contexts and generate entire codebases based on a textual
description, compared to the popular snippet-level generation. These codebase
AI assistants (CBAs) can also extend or adapt codebases, allowing users to
focus on higher-level design and deployment decisions. While prior work has
extensively studied the impact of snippet-level code generation, this new class
of codebase generation models is relatively unexplored. Despite initial
anecdotal reports of excitement about these agents, they remain less frequently
adopted compared to snippet-level code assistants. To utilize CBAs better, we
need to understand how developers interact with CBAs, and how and why CBAs fall
short of developers' needs. In this paper, we explored these gaps through a
counterbalanced user study and interview with (n = 16) students and developers
working on coding tasks with CBAs. We found that participants varied the
information in their prompts, like problem description (48% of prompts),
required functionality (98% of prompts), code structure (48% of prompts), and
their prompt writing process. Despite various strategies, the overall
satisfaction score with generated codebases remained low (mean = 2.8, median =
3, on a scale of one to five). Participants mentioned functionality as the most
common factor for dissatisfaction (77% of instances), alongside poor code
quality (42% of instances) and communication issues (25% of instances). We
delve deeper into participants' dissatisfaction to identify six underlying
challenges that participants faced when using CBAs, and extracted five barriers
to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial
CBAs to compare their capabilities with participant challenges and present
design opportunities for more efficient and useful CBAs.

</details>


### [15] [PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded Model Checking for C](https://arxiv.org/abs/2508.08171)
*Pedro Orvalho,Marta Kwiatkowska*

Main category: cs.SE

TL;DR: 论文提出了PyVeritas框架，利用大型语言模型（LLMs）将Python代码转译为C代码，然后结合模型检查工具进行验证和故障定位，解决了Python缺乏形式化验证工具的问题。


<details>
  <summary>Details</summary>
Motivation: 由于Python缺乏成熟的模型检查工具，而C语言则有成熟的工具（如CBMC），论文旨在通过LLM将Python代码转译为C代码，从而利用C的工具进行形式化验证。

Method: PyVeritas框架通过LLMs将Python代码转译为C代码，随后使用有界模型检查和MaxSAT技术进行故障定位。

Result: 在两个Python基准测试上的实验表明，LLM转译的准确率可达80-90%，支持断言验证和故障诊断。

Conclusion: PyVeritas为Python代码的形式化验证和故障定位提供了一种有效解决方案，适合小规模但非平凡的Python程序。

Abstract: Python has become the dominant language for general-purpose programming, yet
it lacks robust tools for formal verification. In contrast, programmers working
in languages such as C benefit from mature model checkers, for example CBMC,
which enable exhaustive symbolic reasoning and fault localisation. The inherent
complexity of Python, coupled with the verbosity and low-level nature of
existing transpilers (e.g., Cython), have historically limited the
applicability of formal verification to Python programs.
  In this paper, we propose PyVeritas, a novel framework that leverages Large
Language Models (LLMs) for high-level transpilation from Python to C, followed
by bounded model checking and MaxSAT-based fault localisation in the generated
C code. PyVeritas enables verification and bug localisation for Python code
using existing model checking tools for C. Our empirical evaluation on two
Python benchmarks demonstrates that LLM-based transpilation can achieve a high
degree of accuracy, up to 80--90% for some LLMs, enabling effective development
environment that supports assertion-based verification and interpretable fault
diagnosis for small yet non-trivial Python programs.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [16] [Checking Consistency of Event-driven Traces](https://arxiv.org/abs/2508.07855)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,R. Govind,Samuel Grahn,Ramanathan S. Thinniyam*

Main category: cs.PL

TL;DR: 论文提出了基于跟踪的event-driven程序的公理语义，并证明其与操作语义的等价性，将一致性问题转化为NP完全问题，同时识别出无嵌套发布的多项式时间可解片段。


<details>
  <summary>Details</summary>
Motivation: 解决event-driven程序中候选执行是否符合语义的一致性检查问题。

Method: 提出基于跟踪的公理语义，证明其与操作语义的等价性，分析问题复杂度并识别可解片段。

Result: 一致性问题是NP完全的，但在无嵌套发布情况下可在多项式时间解决。

Conclusion: 通过工具实现验证了方法的有效性，适用于多种基准测试。

Abstract: Event-driven programming is a popular paradigm where the flow of execution is
controlled by two features: (1) shared memory and (2) sending and receiving of
messages between multiple handler threads (just called handler). Each handler
has a mailbox (modelled as a queue) for receiving messages, with the constraint
that the handler processes its messages sequentially. Executions of messages by
different handlers may be interleaved. A central problem in this setting is
checking whether a candidate execution is consistent with the semantics of
event-driven programs. In this paper, we propose an axiomatic semantics for
eventdriven programs based on the standard notion of traces (also known as
execution graphs). We prove the equivalence of axiomatic and operational
semantics. This allows us to rephrase the consistency problem axiomatically,
resulting in the event-driven consistency problem: checking whether a given
trace is consistent. We analyze the computational complexity of this problem
and show that it is NP-complete, even when the number of handler threads is
bounded. We then identify a tractable fragment: in the absence of nested
posting, where handlers do not post new messages while processing a message,
consistency checking can be performed in polynomial time. Finally, we implement
our approach in a prototype tool and report on experimental results on a wide
range of benchmarks.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [17] [Iris RESTful Server and IrisTileSource: An Iris implementation for existing OpenSeaDragon viewers](https://arxiv.org/abs/2508.06615)
*Ryan Erik Landvater,Navin Kathawa,Mustafa Yousif MD,Ulysses Balis MD*

Main category: cs.NI

TL;DR: 论文介绍了Iris RESTful Server，一种低开销的HTTP服务器，旨在改进高分辨率WSI文件的子区域流式传输，并兼容DICOMweb WADO-RS API。


<details>
  <summary>Details</summary>
Motivation: 解决静态HTTP文件服务器无法原生流式传输高分辨率图像子区域的问题，提升病理学家的工作效率和系统管理员的图像管理便捷性。

Method: 开发了基于C++和Boost Beast HTTP的Iris RESTful Server，支持RESTful API并与DICOMweb WADO-RS API兼容，同时为OpenSeaDragon开发了新的TileSource模块。

Result: 测试表明，单个服务器实例每秒可处理超过5000个瓦片请求，中位延迟为21毫秒，且能无缝集成到现有WSI工作流中。

Conclusion: Iris RESTful Server为WSI图像的高效流式传输提供了高性能和安全的解决方案，并简化了开发流程。

Abstract: The Iris File Extension (IFE) is a low overhead performance-oriented whole
slide image (WSI) file format designed to improve the image rendering
experience for pathologists and simplify image management for system
administrators. However, static hypertext transfer protocol (HTTP) file servers
cannot natively stream subregions of high-resolution image files, such as the
IFE. The majority of contemporary WSI viewer systems are designed as
browser-based web applications and leverage OpenSeaDragon as the tile-based
rendering framework. These systems convert WSI files to Deep Zoom Images (DZI)
for compatibility with simple static HTTP file servers. In order to address
this limitation, we have developed the Iris RESTful Server, a low-overhead HTTP
server with a RESTful API that is natively compatible with the DICOMweb WADO-RS
API. Written in C++ with Boost Beast HTTP and Asio networking libraries atop
the public IFE libraries, the server offers both security and high performance.
Testing shows that a single instance can handle over 5000 tile requests per
second with a median latency of 21 ms on a private network. We also developed
and merged a new OpenSeaDragon TileSource, compatible with the Iris RESTful
API, into the next OpenSeaDragon release, enabling simple and immediate drop-in
replacement of DZI images within WSI viewer stacks. Designed as a secure
cross-origin resource sharing microservice, this architecture includes detailed
deployment instructions for new or existing WSI workflows, and the public
examples.restful.irisdigitialpathology.org subdomain is provided as a
development tool to accelerate WSI web viewer development.

</details>


### [18] [Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach](https://arxiv.org/abs/2508.06616)
*Md Arafat Habib,Medhat Elsayed,Yigit Ozcan,Pedro Enrique Iturria-Rivera,Majid Bavand,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 本文综述了基于LLM的IDN架构在6G动态网络中的智能自动化应用，并提出了一种整合GenAI的全阶段分层IDN框架，通过案例研究验证其优越性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络的异构性和动态性要求更高效的自动化管理，IDN通过意图驱动优化，LLM和GenAI的进展为智能自动化提供了新机会。

Method: 提出一个分层学习框架，将GenAI整合到IDN的三个关键阶段（意图处理、验证和执行），并通过基于Mamba架构的案例研究验证其有效性。

Result: 提出的GenAI驱动架构在智能自动化方面表现优于传统IDN架构。

Conclusion: GenAI在IDN全阶段的整合显著提升了网络性能，为6G管理提供了新方向。

Abstract: With the emergence of 6G, mobile networks are becoming increasingly
heterogeneous and dynamic, necessitating advanced automation for efficient
management. Intent-Driven Networks (IDNs) address this by translating
high-level intents into optimization policies. Large Language Models (LLMs) can
enhance this process by understanding complex human instructions to enable
adaptive, intelligent automation. Given the rapid advancements in Generative AI
(GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated
Radio Access Network (RAN) environments is both timely and critical. This
article provides such a survey, along with a case study on a hierarchical
learning-enabled IDN architecture that integrates GenAI across three key
stages: intent processing, intent validation, and intent execution. Unlike most
existing approaches that apply GenAI in the form of LLMs for intent processing
only, we propose a hierarchical framework that introduces GenAI across all
three stages of IDN. To demonstrate the effectiveness of the proposed IDN
management architecture, we present a case study based on the latest GenAI
architecture named Mamba. The case study shows how the proposed GenAI-driven
architecture enhances network performance through intelligent automation,
surpassing the performance of the conventional IDN architectures.

</details>


### [19] [THz/RF Multi-Hop Routing Throughput: Performance, Optimization, and Application](https://arxiv.org/abs/2508.06975)
*Zhengying Lou,Baha Eddine Youcef Belmekki,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 论文提出了一种基于随机几何（SG）的太赫兹（THz）通信路由分析框架，通过逐步优化方法提高吞吐量，并在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信因其高吞吐量潜力而备受关注，但信号路径损耗严重，需要解决其路由问题以提升性能。

Method: 采用随机几何框架，开发逐步优化方法，包括功率分配、中继选择和跳数设计。

Result: 提出的路由策略在吞吐量和覆盖概率上优于现有方法，并接近理论上限。

Conclusion: 该框架和策略为太赫兹通信系统设计提供了有效工具，特别是在无人机网络等场景中。

Abstract: Terahertz (THz) communication offers a promising solution for high-throughput
wireless systems. However, the severe path loss of THz signals raises concerns
about its effectiveness compared to radio frequency (RF) communication. In this
article, we establish the first stochastic geometry (SG)-based analytical
framework for routing in THz systems. We develop a stepwise optimization
approach to maximize throughput, including power allocation, relay selection,
and number of hops design. Analytical expressions for throughput and coverage
probability are derived under the SG framework, enabling low complexity and
scalable performance evaluation. Numerical results show that the proposed
stepwise-optimal routing strategies not only outperform existing SG-based
methods but also approach the ideal upper bound. Moreover, we compare the
throughput and coverage performance of THz and RF routing and demonstrate the
applications of the proposed analytical framework and routing strategies in
system parameter design and unmanned aerial vehicle networks.

</details>


### [20] [Consensus-based Decentralized Multi-agent Reinforcement Learning for Random Access Network Optimization](https://arxiv.org/abs/2508.07001)
*Myeung Suk Oh,Zhiyao Zhang,FNU Hairi,Alvaro Velasquez,Jia Liu*

Main category: cs.NI

TL;DR: 本文提出了一种完全分散的多智能体强化学习架构，通过本地奖励交换优化随机接入网络性能。


<details>
  <summary>Details</summary>
Motivation: 设计一种无需集中训练且通信开销低的随机接入多址协议，以解决现有方案的局限性。

Method: 采用分散式多智能体强化学习架构，基于共识信息交换和本地奖励共享的演员-评论家网络。

Result: 理论证明全局收敛，数值实验显示性能显著优于基线方法。

Conclusion: 该方法在减少通信开销的同时，有效提升了随机接入网络的性能。

Abstract: With wireless devices increasingly forming a unified smart network for
seamless, user-friendly operations, random access (RA) medium access control
(MAC) design is considered a key solution for handling unpredictable data
traffic from multiple terminals. However, it remains challenging to design an
effective RA-based MAC protocol to minimize collisions and ensure transmission
fairness across the devices. While existing multi-agent reinforcement learning
(MARL) approaches with centralized training and decentralized execution (CTDE)
have been proposed to optimize RA performance, their reliance on centralized
training and the significant overhead required for information collection can
make real-world applications unrealistic. In this work, we adopt a fully
decentralized MARL architecture, where policy learning does not rely on
centralized tasks but leverages consensus-based information exchanges across
devices. We design our MARL algorithm over an actor-critic (AC) network and
propose exchanging only local rewards to minimize communication overhead.
Furthermore, we provide a theoretical proof of global convergence for our
approach. Numerical experiments show that our proposed MARL algorithm can
significantly improve RA network performance compared to other baselines.

</details>


### [21] [ProtoScan: Measuring censorship in IPv6](https://arxiv.org/abs/2508.07194)
*Jack Wampler,Hammas Bin Tanveer,Rishab Nithyanand,Eric Wustrow*

Main category: cs.NI

TL;DR: 研究了IPv6与IPv4在互联网审查中的差异，发现IPv6审查在某些国家不够全面或可靠。


<details>
  <summary>Details</summary>
Motivation: 随着IPv6的逐步推广，了解其对互联网审查的影响变得重要。

Method: 通过全球性研究，比较了IPv4和IPv6在HTTP、DNS和TLS等协议中的审查情况。

Result: 发现IPv6审查能力较弱，可能为绕过审查提供新途径。

Conclusion: IPv6为审查绕过提供了新机会，未来需开发更多工具利用IPv6基础设施。

Abstract: Internet censorship continues to impact billions of people worldwide, and
measurement of it remains an important focus of research. However, most
Internet censorship measurements have focused solely on the IPv4 Internet
infrastructure. Yet, more clients and servers are available over IPv6:
According to Google, over a third of their users now have native IPv6 access.
Given the slow-but-steady rate of IPv6 adoption, it is important to understand
its impact on censorship. In this paper, we measure and analyze how censorship
differs over IPv6 compared to the well-studied IPv4 censorship systems in use
today. We perform a comprehensive global study of censorship across an array of
commonly censored protocols, including HTTP, DNS, and TLS, on both IPv4 and
IPv6, and compare the results. We find that there are several differences in
how countries censor IPv6 traffic, both in terms of IPv6 resources, and in
where and what blocklists or technologies are deployed on IPv6 networks. Many
of these differences are not all-or-nothing: we find that most censors have
some capacity to block in IPv6, but are less comprehensive or less reliable
compared to their IPv4 censorship systems. Our results suggest that IPv6 offers
new areas for censorship circumvention researchers to explore, providing
potentially new ways to evade censors. As more users gain access to IPv6
addresses and networks, there will be a need for tools that take advantage of
IPv6 techniques and infrastructure to bypass censorship.

</details>


### [22] [Mind the IP Gap: Measuring the impact of IPv6 on DNS censorship](https://arxiv.org/abs/2508.07197)
*Ian Martiny,Hammas Bin Tanveer,Jack Wampler,Rishab Nithyanand,Eric Wustrow*

Main category: cs.NI

TL;DR: 本文首次对IPv6互联网上的DNS审查进行了全球测量，发现尽管大多数审查机构支持IPv6审查，但其效果不一致且常不如IPv4有效。


<details>
  <summary>Details</summary>
Motivation: 研究IPv6流量下的信息控制系统（如防火墙、深度包检测等）是否与IPv4一样有效，填补此前主要集中在IPv4测量上的研究空白。

Method: 利用新技术发现支持IPv6的开放解析器，并向全球DNS解析器发送超过2000万次A和AAAA请求，测量其在解析器、网络和国家层面的屏蔽率及被屏蔽域名的特征。

Result: 几乎所有审查机构都支持IPv6屏蔽，但策略不一致且效果常不如IPv4。

Conclusion: IPv6审查并非全有或全无，许多审查机构的IPv6审查效果不佳，这为规避研究提供了新机会。

Abstract: Internet censorship impacts large segments of the Internet, but so far, prior
work has focused almost exclusively on performing measurements using IPv4. As
the Internet grows, and more users connect, IPv6 is increasingly supported and
available to users and servers alike. But despite this steady growth, it
remains unclear if the information control systems that implement censorship
(firewalls, deep packet inspection, DNS injection, etc) are as effective with
IPv6 traffic as they are with IPv4. In this paper, we perform the first global
measurement of DNS censorship on the IPv6 Internet. Leveraging a recent
technique that allows us to discover IPv6-capable open resolvers (along with
their corresponding IPv4 address), we send over 20 million A and AAAA DNS
requests to DNS resolvers worldwide, and measure the rate at which they block,
at the resolver, network, and country level as well examine the characteristics
of blocked domains. We observe that while nearly all censors support blocking
IPv6, their policies are inconsistent with and frequently less effective than
their IPv4 censorship infrastructure. Our results suggest that supporting IPv6
censorship is not all-or-nothing: many censors support it, but poorly. As a
result, these censors may have to expend additional resources to bring IPv6
censorship up to parity with IPv4. In the meantime, this affords censorship
circumvention researchers a new opportunity to exploit these differences to
evade detection and blocking.

</details>


### [23] [The Search for Relevance: A Context-Aware Paradigm Shift in Semantic and Task-Oriented V2X Communications](https://arxiv.org/abs/2508.07394)
*Luca Lusvarghi,Javier Gozalvez,Baldomero Coll-Perales,Mohammad Irfan Khan,Miguel Sepulcre,Seyhan Ucar,Onur Altintas*

Main category: cs.NI

TL;DR: 论文提出了一种面向语义和任务的V2X通信范式，通过传输最相关信息提高通信效率。


<details>
  <summary>Details</summary>
Motivation: 传统通信系统设计注重数据的可靠及时传输，但6G时代需要新范式以解决可扩展性问题。

Method: 提出联合语义和任务导向的通信模式，利用V2X中的上下文信息评估信息相关性。

Result: 数值结果表明，该方法可提升通信效率两倍，显著增强V2X网络的可扩展性。

Conclusion: 语义和任务导向的V2X通信能有效提升效率，适用于6G时代的需求。

Abstract: The design of communication systems has traditionally focused on the reliable
and timely delivery of data. However, the scalability challenges faced by the
evolution to a 6G-driven society demand new communication paradigms that
carefully curate the content being transmitted. This paper envisions a joint
semantic and task-oriented communication paradigm where Connected and
Autonomous Vehicles (CAVs) transmit only the information necessary to convey
the desired meaning that is relevant to the intended receivers based on the
communication context. The V2X domain offers a unique environment for the
development of the envisioned semantic and task-oriented communications
paradigm, as CAVs are native semantic devices, and the V2X domain is rich in
contextual information. This contextual information can be leveraged to
estimate the relevance that information may have for the intended receivers. We
illustrate and quantitatively evaluate the potential benefits of semantic and
task-oriented V2X communications. Numerical results show that by focusing on
the transmission of the most relevant information for the intended receivers,
semantic and task-oriented V2X communications can achieve a two-fold
improvement in communication efficiency, which will significantly benefit the
scalability of V2X networks.

</details>


### [24] [Unveiling IPv6 Scanning Dynamics: A Longitudinal Study Using Large Scale Proactive and Passive IPv6 Telescopes](https://arxiv.org/abs/2508.07506)
*Hammas Bin Tanveer,Wai Sun Chan,Ricky K. P. Mok,Sebastian Kappes,Philipp Richter,Oliver Gasser,John Ronan,Arthur Berger,kc Claffy*

Main category: cs.NI

TL;DR: 论文提出新工具和视角，通过部署IPv6主动望远镜，分析未请求流量。


<details>
  <summary>Details</summary>
Motivation: 开发主动技术以吸引和分析IPv6扫描流量。

Method: 在生产ISP网络中部署史上最大IPv6主动望远镜，收集数据并分析。

Result: 收集了6亿个未请求数据包，分析了流量来源和扫描策略。

Conclusion: 研究提供了对IPv6扫描行为的深入见解。

Abstract: We introduce new tools and vantage points to develop and integrate proactive
techniques to attract IPv6 scan traffic, thus enabling its analysis. By
deploying the largest-ever IPv6 proactive telescope in a production ISP
network, we collected over 600M packets of unsolicited traffic from 1.9k
Autonomous Systems in 10 months. We characterized the sources of unsolicited
traffic, evaluated the effectiveness of five major features across the network
stack, and inferred scanners' sources of target addresses and their strategies.

</details>


### [25] [Achieving Fair-Effective Communications and Robustness in Underwater Acoustic Sensor Networks: A Semi-Cooperative Approach](https://arxiv.org/abs/2508.07578)
*Yu Gou,Tong Zhang,Jun Liu,Tingting Yang,Shanshan Song,Jun-Hong Cui*

Main category: cs.NI

TL;DR: 论文研究了不完美且能量受限的水下声学传感器网络（IC-UASNs）中的公平有效通信和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 在水下声学传感器网络中，节点故障和时变信道对性能产生影响，导致完全协作方法难以兼顾个体QoS需求与全局公平有效通信。

Method: 提出了一种基于分布式多智能体强化学习（MARL）的半协作功率分配方法（SECOPA），智能节点自主决策传输功率以优化个体和全局性能。

Result: 实验结果表明该方法能够适应时变信道并处理节点故障，实现公平有效通信和鲁棒性。

Conclusion: SECOPA方法在IC-UASNs中有效解决了公平性和鲁棒性难题。

Abstract: This paper investigates the fair-effective communication and robustness in
imperfect and energy-constrained underwater acoustic sensor networks
(IC-UASNs). Specifically, we investigate the impact of unexpected node
malfunctions on the network performance under the time-varying acoustic
channels. Each node is expected to satisfy Quality of Service (QoS)
requirements. However, achieving individual QoS requirements may interfere with
other concurrent communications. Underwater nodes rely excessively on the
rationality of other underwater nodes when guided by fully cooperative
approaches, making it difficult to seek a trade-off between individual QoS and
global fair-effective communications under imperfect conditions. Therefore,
this paper presents a SEmi-COoperative Power Allocation approach (SECOPA) that
achieves fair-effective communication and robustness in IC-UASNs. The approach
is distributed multi-agent reinforcement learning (MARL)-based, and the
objectives are twofold. On the one hand, each intelligent node individually
decides the transmission power to simultaneously optimize individual and global
performance. On the other hand, advanced training algorithms are developed to
provide imperfect environments for training robust models that can adapt to the
time-varying acoustic channels and handle unexpected node failures in the
network. Numerical results are presented to validate our proposed approach.

</details>


### [26] [Joint Scheduling and Resource Allocation in mmWave IAB Networks Using Deep RL](https://arxiv.org/abs/2508.07604)
*Maryam Abbasalizadeh,Sashank Narain*

Main category: cs.NI

TL;DR: 提出了一种基于深度强化学习的框架，用于动态、易受干扰的IAB网络中联合链路调度和资源切片，显著提高了调度准确性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在5G及更高频段部署中，IAB至关重要，尤其在mmWave频段光纤回传不可行的情况下，需要高效的动态资源管理方案。

Method: 结合贪心Double Deep Q-Network调度器和多代理DDQN分配器，实现异构链路上的联合调度和资源分配，支持去中心化操作。

Result: 在96种动态拓扑中测试，调度精度达99.84%，吞吐量提升20.90%。

Conclusion: 该框架高效且适应性强，适合资源受限的动态部署场景。

Abstract: Integrated Access and Backhaul (IAB) is critical for dense 5G and beyond
deployments, especially in mmWave bands where fiber backhaul is infeasible. We
propose a novel Deep Reinforcement Learning (DRL) framework for joint link
scheduling and resource slicing in dynamic, interference-prone IAB networks.
Our method integrates a greedy Double Deep Q-Network (DDQN) scheduler to
activate access and backhaul links based on traffic and topology, with a
multi-agent DDQN allocator for bandwidth and antenna assignment across network
slices. This decentralized approach respects strict antenna constraints and
supports concurrent scheduling across heterogeneous links. Evaluations across
96 dynamic topologies show 99.84 percent scheduling accuracy and 20.90 percent
throughput improvement over baselines. The framework's efficient operation and
adaptability make it suitable for dynamic and resource-constrained deployments,
where fast link scheduling and autonomous backhaul coordination are vital.

</details>


### [27] [Joint link scheduling and power allocation in imperfect and energy-constrained underwater wireless sensor networks](https://arxiv.org/abs/2508.07679)
*Tong Zhang,Yu Gou,Jun Liu,Shanshan Song,Tingting Yang,Jun-Hong Cui*

Main category: cs.NI

TL;DR: 本文提出了一种名为ICRL-JSA的深度多智能体强化学习优化器，旨在为能量受限和节点故障频繁的水下无线传感器网络（IC-UWSNs）提供公平、高效和可靠的通信，并通过联合链路调度和功率分配实现目标。


<details>
  <summary>Details</summary>
Motivation: 水下无线传感器网络（UWSNs）在能源供应受限和节点意外故障方面存在重大设计挑战，需要一种能够应对这些问题的通信方案。

Method: 作者提出了一种基于深度多智能体强化学习（MARL）的优化器ICRL-JSA，结合了深度Q网络和先进的训练机制，以实现联合链路调度和功率分配。

Result: 仿真结果表明，ICRL-JSA方案在各种基准算法中表现优越。

Conclusion: ICRL-JSA通过自动学习调度算法，能够有效解决水下环境和IC-UWSNs带来的挑战，显著提升了通信性能。

Abstract: Underwater wireless sensor networks (UWSNs) stand as promising technologies
facilitating diverse underwater applications. However, the major design issues
of the considered system are the severely limited energy supply and unexpected
node malfunctions. This paper aims to provide fair, efficient, and reliable
(FER) communication to the imperfect and energy-constrained UWSNs (IC-UWSNs).
Therefore, we formulate a FER-communication optimization problem (FERCOP) and
propose ICRL-JSA to solve the formulated problem. ICRL-JSA is a deep
multi-agent reinforcement learning (MARL)-based optimizer for IC-UWSNs through
joint link scheduling and power allocation, which automatically learns
scheduling algorithms without human intervention. However, conventional RL
methods are unable to address the challenges posed by underwater environments
and IC-UWSNs. To construct ICRL-JSA, we integrate deep Q-network into IC-UWSNs
and propose an advanced training mechanism to deal with complex acoustic
channels, limited energy supplies, and unexpected node malfunctions. Simulation
results demonstrate the superiority of the proposed ICRL-JSA scheme with an
advanced training mechanism compared to various benchmark algorithms.

</details>


### [28] [An Experimental Reservoir-Augmented Foundation Model: 6G O-RAN Case Study](https://arxiv.org/abs/2508.07778)
*Farhad Rezazadeh,Raymond Zhao,Jiongyu Dai,Amir Ashtari Gargari,Hatim Chergui,Lingjia Liu*

Main category: cs.NI

TL;DR: 提出了一种用于6G O-RAN测试的新型时间序列基础模型RA-MAT，结合回声状态网络和自编码器，满足低延迟和高效率需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统变换器在处理O-RAN超高维非平稳时间序列时的局限性。

Method: 使用随机初始化的ESN快速投影时间片，结合掩码自编码器进行自监督预训练，后通过少量调适完成下游任务。

Result: 在O-RAN KPI预测中达到低于0.06的MSE。

Conclusion: RA-MAT为6G网络的实时分析提供了可行方案。

Abstract: Next-generation open radio access networks (O-RAN) continuously stream tens
of key performance indicators (KPIs) together with raw in-phase/quadrature (IQ)
samples, yielding ultra-high-dimensional, non-stationary time series that
overwhelm conventional transformer architectures. We introduce a
reservoir-augmented masked autoencoding transformer (RA-MAT). This time series
foundation model employs echo state network (ESN) computing with masked
autoencoding to satisfy the stringent latency, energy efficiency, and
scalability requirements of 6G O-RAN testing. A fixed, randomly initialized ESN
rapidly projects each temporal patch into a rich dynamical embedding without
backpropagation through time, converting the quadratic self-attention
bottleneck into a lightweight linear operation. These embeddings drive a
patch-wise masked autoencoder that reconstructs 30% randomly masked patches,
compelling the encoder to capture both local dynamics and long-range structure
from unlabeled data. After self-supervised pre-training, RA-MAT is fine-tuned
with a shallow task head while keeping the reservoir and most transformer
layers frozen, enabling low-footprint adaptation to diverse downstream tasks
such as O-RAN KPI forecasting. In a comprehensive O-RAN KPI case study, RA-MAT
achieved sub-0.06 mean squared error (MSE) on several continuous and discrete
KPIs. This work positions RA-MAT as a practical pathway toward real-time,
foundation-level analytics in future 6G networks.

</details>


### [29] [Scalable and Energy-Efficient Predictive Data Collection in Wireless Sensor Networks with Constructive Interference](https://arxiv.org/abs/2508.07882)
*Conor Muldoon*

Main category: cs.NI

TL;DR: STAIR是一种利用建设性干扰的低能耗、低延迟无线传感器网络框架，通过子模优化算法优化传感器激活位置和时间。


<details>
  <summary>Details</summary>
Motivation: 为了解决无线传感器网络中多节点同时传输数据的能量和延迟问题，同时利用建设性干扰提高效率。

Method: 使用粗糙的拓扑信息选择子网，通过子模优化算法确定传感器激活位置和时间，以最小化预测误差。

Result: 在真实部署中验证了STAIR框架的可扩展性和鲁棒性。

Conclusion: STAIR是一种高效且适用于资源受限环境的无线传感器网络框架。

Abstract: A new class of Wireless Sensor Network has emerged whereby multiple nodes
transmit data simultaneously, exploiting constructive interference to enable
data collection frameworks with low energy usage and latency. This paper
presents STAIR (Spatio-Temporal Activation for Intelligent Relaying), a
scalable, resilient framework for Wireless Sensor Networks that leverages
constructive interference and operates effectively under stringent resource
constraints. Using constructive interference requires all nodes to transmit the
same packet at the same time, thus, only one source node can send data per time
slot. STAIR uses coarse-grained topology information to flood a selected subset
of the network, relaying sensor readings from individual nodes during their
allocated time slots. A submodular optimisation algorithm with proven quality
bounds determines near-optimal sensor activation locations and times, aiming to
minimise the sum of mean squared prediction errors from a multiple multivariate
linear regression model, which is used to estimate values at unselected
locations and times. This framework has been extensively validated on a
real-world testbed deployment.

</details>


### [30] [Adaptive Multiple Access and Service Placement for Generative Diffusion Models](https://arxiv.org/abs/2508.07978)
*Hamidreza Mazandarani,Mohammad Farhoudi,Masoud Shokrnezhad,Tarik Taleb*

Main category: cs.NI

TL;DR: 论文提出了一种名为LEARN-GDM的优化框架，通过深度强化学习动态分配去噪任务，解决生成扩散模型在移动边缘网络中的部署挑战。


<details>
  <summary>Details</summary>
Motivation: 生成扩散模型在实时和移动环境中部署时，由于迭代和资源密集型的推理过程而面临挑战。

Method: 提出LEARN-GDM算法，结合贪心多址接入和D3QL-based服务放置，动态分配去噪任务。

Result: 模拟结果显示该框架在可扩展性和延迟弹性方面优于传统方法。

Conclusion: 该工作为边缘生成AI提供了适应性强的解决方案，推动了分布式环境中语义网络和协同推理的发展。

Abstract: Generative Diffusion Models (GDMs) have emerged as key components of
Generative Artificial Intelligence (GenAI), offering unparalleled
expressiveness and controllability for complex data generation tasks. However,
their deployment in real-time and mobile environments remains challenging due
to the iterative and resource-intensive nature of the inference process.
Addressing these challenges, this paper introduces a unified optimization
framework that jointly tackles service placement and multiple access control
for GDMs in mobile edge networks. We propose LEARN-GDM, a Deep Reinforcement
Learning-based algorithm that dynamically partitions denoising blocks across
heterogeneous edge nodes, while accounting for latent transmission costs and
enabling adaptive reduction of inference steps. Our approach integrates a
greedy multiple access scheme with a Double and Dueling Deep Q-Learning
(D3QL)-based service placement, allowing for scalable, adaptable, and
resource-efficient operation under stringent quality of service requirements.
Simulations demonstrate the superior performance of the proposed framework in
terms of scalability and latency resilience compared to conventional monolithic
and fixed chain-length placement strategies. This work advances the state of
the art in edge-enabled GenAI by offering an adaptable solution for GDM
services orchestration, paving the way for future extensions toward semantic
networking and co-inference across distributed environments.

</details>


### [31] [Industrial Viewpoints on RAN Technologies for 6G](https://arxiv.org/abs/2508.08225)
*Mansoor Shafi,Erik G. Larsson,Xingqin Lin,Dorin Panaitopol,Stefan Parkvall,Flavien Ronteix-Jacquet,Antti Toskala*

Main category: cs.NI

TL;DR: 本文探讨了6G标准化的技术组件和性能要求，侧重于MIMO、AI、波形、编码、信号星座及与非地面网络的集成。


<details>
  <summary>Details</summary>
Motivation: 为即将开始的6G标准化提供技术指导，帮助学术界和工业界了解未来研究方向。

Method: 基于实施和部署方面的经验，对6G技术组件和性能要求进行预测性分析。

Result: 提出了6G标准化可能的技术方向和性能要求，尽管尚无实际研究结果。

Conclusion: 本文的观点将有助于引导研究人员和行业从业者在6G标准化中的工作。

Abstract: 6G standardization is to start imminently, with commercial deployments
expected before 2030. Its technical components and performance requirements are
the focus of this article. Our emphasis is on the 6G radio access, especially
MIMO, AI, waveforms, coding, signal constellations and integration with
non-terrestrial networks. Whilst standardization has not yet formally started,
the scope of the 6G study items has been defined. Our predictions in this paper
are speculative as there are no results of the study yet, but our views are
guided by implementation and deployment aspects. We expect that the views here
will guide researchers and industry practitioners.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [32] [Narrative Memory in Machines: Multi-Agent Arc Extraction in Serialized TV](https://arxiv.org/abs/2508.07010)
*Roberto Balestri,Guglielmo Pescatore*

Main category: cs.MM

TL;DR: 该论文提出了一种多智能体系统（MAS），用于通过计算记忆架构提取和分析电视叙事的复杂故事线，结合人类记忆的模拟（语义和情景记忆）来实现叙事理解，并在《实习医生格蕾》第一季中测试了系统的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂且时间分布广泛的电视叙事分析问题，通过模拟人类记忆的计算方法，实现更高效的叙事理解和管理。

Method: 使用多智能体系统（MAS），结合大型语言模型（LLMs）作为语义记忆和向量数据库作为情景记忆，通过工作记忆模拟整合信息。

Result: 系统成功识别了三种叙事弧类型，但依赖文本摘要限制了其对重叠弧和不透明动态的分析能力，显示了计算记忆与人类理解之间的差距。

Conclusion: 该系统的记忆中心方法为结合AI记忆处理和人类专业知识提供了潜力，未来将扩展多模态输入并优化记忆整合机制。

Abstract: Serialized television narratives present significant analytical challenges
due to their complex, temporally distributed storylines that necessitate
sophisticated information management. This paper introduces a multi-agent
system (MAS) designed to extract and analyze narrative arcs by implementing
principles of computational memory architectures. The system conceptualizes
narrative understanding through analogues of human memory: Large Language
Models (LLMs) provide a form of semantic memory for general narrative patterns,
while a vector database stores specific arc progressions as episodic memories.
A multi-agent workflow simulates working memory processes to integrate these
information types. Tested on the first season of Grey's Anatomy (ABC 2005-),
the MAS identifies three arc types: Anthology (self-contained), Soap
(relationship-focused), and Genre-Specific. These arcs and their episodic
developments are stored in a vector database, facilitating structured analysis
and semantic comparison. To bridge automation with critical interpretation, a
graphical interface enables human oversight and refinement of the system's
narrative memory. While demonstrating strong performance in identifying
Anthology Arcs and character entities, the system's reliance on textual
paratexts (episode summaries) revealed limitations in discerning overlapping
arcs and opaque dynamics, underscoring the challenges in computational memory
consolidation versus human holistic understanding. This memory-centric approach
highlights the potential of combining AI-driven memory processing with human
expertise. Beyond television, it offers promise for serialized written formats
where narrative is entirely text-based. Future work will focus on integrating
multimodal inputs to enrich episodic memory, refining memory integration
mechanisms within the MAS, and expanding testing across diverse genres.

</details>


### [33] [Reversible Video Steganography Using Quick Response Codes and Modified ElGamal Cryptosystem](https://arxiv.org/abs/2508.07289)
*Ramadhan J. Mstafa*

Main category: cs.MM

TL;DR: 提出了一种基于DWT和QR码的可逆视频隐写方法，结合改进的ElGamal加密系统，提高安全性和抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: 当前多媒体信息传输迅速，但隐私和数据安全问题突出，需要一种高效的视频隐写技术来保护机密信息。

Method: 结合改进的ElGamal加密QR码，并通过DWT和LSB技术在视频帧的多个子带和分量中嵌入数据。

Result: 方法具有高安全性、高视觉不可察觉性，抗多种噪声攻击，平均SSIM>0.91，PSNR平均52.143 dB，嵌入容量1 bpp。

Conclusion: 该方法在视觉不可察觉性、抗攻击能力和嵌入容量上优于现有技术，适合保护机密数据。

Abstract: The rapid transmission of multimedia information has been achieved mainly by
recent advancements in the Internet's speed and information technology. In
spite of this, advancements in technology have resulted in breaches of privacy
and data security. When it comes to protecting private information in today's
Internet era, digital steganography is vital. Many academics are interested in
digital video because it has a great capability for concealing important data.
There have been a vast number of video steganography solutions developed lately
to guard against the theft of confidential data. The visual imperceptibility,
robustness, and embedding capacity of these approaches are all challenges that
must be addressed. In this paper, a novel solution to reversible video
steganography based on DWT and QR codes is proposed to address these concerns.
In order to increase the security level of the suggested method, an enhanced
ElGamal cryptosystem has also been proposed. Prior to the embedding stage, the
suggested method uses the modified ElGamal algorithm to encrypt secret QR
codes. Concurrently, it applies two-dimensional DWT on the Y-component of each
video frame resulting in LL, LH, HL, and HH sub-bands. Then, the encrypted Low
(L), Medium (M), Quantile (Q), and High (H) QR codes are embedded into the HL
sub-band, HH sub-band, U-component, and V-component of video frames,
respectively, using the LSB technique. As a consequence of extensive testing of
the approach, it was shown to be very secure and highly invisible, as well as
highly resistant to attacks from Salt & Pepper, Gaussian, Poisson, and Speckle
noises, which has an average SSIM of more than 0.91. Aside from visual
imperceptibility, the suggested method exceeds current methods in terms of PSNR
average of 52.143 dB, and embedding capacity 1 bpp.

</details>


### [34] [FineBadminton: A Multi-Level Dataset for Fine-Grained Badminton Video Understanding](https://arxiv.org/abs/2508.07554)
*Xusheng He,Wei Liu,Shanshan Ma,Qian Liu,Chenghao Ma,Jianlong Wu*

Main category: cs.MM

TL;DR: 该论文介绍了FineBadminton数据集和FBBench基准，用于解决多模态大语言模型（MLLMs）在细粒度羽毛球运动分析中的挑战，并提出了一种优化的基线方法。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏丰富的领域特定注释数据集，MLLMs在复杂高速运动（如羽毛球）的细粒度分析中表现不足，作者提出了FineBadminton和FBBench来解决这一问题。

Method: 通过创新的标注流程（结合MLLM生成建议和人工细化）构建FineBadminton数据集，并开发FBBench基准。此外，提出了Hit-Centric关键帧选择和Coordinate-Guided Condensation优化方法。

Result: 在FBBench上的实验表明，当前MLLMs在深层体育视频分析中仍面临挑战，但提出的优化策略显著提升了性能。

Conclusion: FineBadminton和FBBench为细粒度视频理解和MLLMs在体育智能中的发展提供了关键生态系统。

Abstract: Fine-grained analysis of complex and high-speed sports like badminton
presents a significant challenge for Multimodal Large Language Models (MLLMs),
despite their notable advancements in general video understanding. This
difficulty arises primarily from the scarcity of datasets with sufficiently
rich and domain-specific annotations. To bridge this gap, we introduce
FineBadminton, a novel and large-scale dataset featuring a unique multi-level
semantic annotation hierarchy (Foundational Actions, Tactical Semantics, and
Decision Evaluation) for comprehensive badminton understanding. The
construction of FineBadminton is powered by an innovative annotation pipeline
that synergistically combines MLLM-generated proposals with human refinement.
We also present FBBench, a challenging benchmark derived from FineBadminton, to
rigorously evaluate MLLMs on nuanced spatio-temporal reasoning and tactical
comprehension. Together, FineBadminton and FBBench provide a crucial ecosystem
to catalyze research in fine-grained video understanding and advance the
development of MLLMs in sports intelligence. Furthermore, we propose an
optimized baseline approach incorporating Hit-Centric Keyframe Selection to
focus on pivotal moments and Coordinate-Guided Condensation to distill salient
visual information. The results on FBBench reveal that while current MLLMs
still face significant challenges in deep sports video analysis, our proposed
strategies nonetheless achieve substantial performance gains. The project
homepage is available at https://finebadminton.github.io/FineBadminton/.

</details>


### [35] [MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training](https://arxiv.org/abs/2508.07590)
*Xiongwei Xiao,Baoying Chen,Jishen Zeng,Jianquan Yang*

Main category: cs.MM

TL;DR: 该论文提出了一种轻量级的面部图像质量评估网络MSPT，通过多阶段渐进训练策略，在保持高效推理的同时，性能接近或优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 传统质量评估方法难以适应面部图像的特性，而现有学习型方法虽性能优越，但计算和存储成本高。MSPT旨在解决这一问题。

Method: 采用三阶段渐进训练策略，逐步引入更多样化的数据样本并提高输入图像分辨率，有效学习复杂质量特征并减少灾难性遗忘。

Result: MSPT在VQualA 2025数据集上取得了第二高分，性能接近或优于现有方法。

Conclusion: MSPT是一种高效轻量级面部质量评估方法，兼具高性能和低计算成本。

Abstract: Accurately assessing the perceptual quality of face images is crucial,
especially with the rapid progress in face restoration and generation.
Traditional quality assessment methods often struggle with the unique
characteristics of face images, limiting their generalizability. While
learning-based approaches demonstrate superior performance due to their strong
fitting capabilities, their high complexity typically incurs significant
computational and storage costs, hindering practical deployment. To address
this, we propose a lightweight face quality assessment network with Multi-Stage
Progressive Training (MSPT). Our network employs a three-stage progressive
training strategy that gradually introduces more diverse data samples and
increases input image resolution. This novel approach enables lightweight
networks to achieve high performance by effectively learning complex quality
features while significantly mitigating catastrophic forgetting. Our MSPT
achieved the second highest score on the VQualA 2025 face image quality
assessment benchmark dataset, demonstrating that MSPT achieves comparable or
better performance than state-of-the-art methods while maintaining efficient
inference.

</details>


### [36] [AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition](https://arxiv.org/abs/2508.07608)
*Junxiao Xue,Xiaozhen Liu,Xuecheng Wu,Xinyi Yin,Danlei Huang,Fei Yu*

Main category: cs.MM

TL;DR: 本文提出了一种基于双向模态增强的新AVSR框架AD-AVSR，通过音频双流编码和跨模态噪声抑制模块，显著提升了噪声环境下的语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有AVSR方法在非对称信息条件下难以捕捉音频-视觉数据的异质性和互补性，因此需要一种更有效的双向增强框架。

Method: 引入音频双流编码策略和跨模态噪声抑制掩模模块，通过双向信息流和阈值选择机制优化音频-视觉相关性和噪声鲁棒性。

Result: 在LRS2和LRS3数据集上的实验表明，AD-AVSR在性能和噪声鲁棒性上均优于现有最佳方法。

Conclusion: AD-AVSR通过双向模态增强和跨模态交互，显著提升了AVSR在复杂环境下的表现，展现了其设计的高效性。

Abstract: Audio-visual speech recognition (AVSR) combines audio-visual modalities to
improve speech recognition, especially in noisy environments. However, most
existing methods deploy the unidirectional enhancement or symmetric fusion
manner, which limits their capability to capture heterogeneous and
complementary correlations of audio-visual data-especially under asymmetric
information conditions. To tackle these gaps, we introduce a new AVSR framework
termed AD-AVSR based on bidirectional modality enhancement. Specifically, we
first introduce the audio dual-stream encoding strategy to enrich audio
representations from multiple perspectives and intentionally establish
asymmetry to support subsequent cross-modal interactions. The enhancement
process involves two key components, Audio-aware Visual Refinement Module for
enhanced visual representations under audio guidance, and Cross-modal Noise
Suppression Masking Module which refines audio representations using visual
cues, collaboratively leading to the closed-loop and bidirectional information
flow. To further enhance correlation robustness, we adopt a threshold-based
selection mechanism to filter out irrelevant or weakly correlated audio-visual
pairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate
that our AD-AVSR consistently surpasses SOTA methods in both performance and
noise robustness, highlighting the effectiveness of our model design.

</details>


### [37] [Towards Multimodal Sentiment Analysis via Contrastive Cross-modal Retrieval Augmentation and Hierachical Prompts](https://arxiv.org/abs/2508.07666)
*Xianbing Zhao,Shengzun Yang,Buzhou Tang,Ronghuan Jiang*

Main category: cs.MM

TL;DR: 提出了一个多模态检索增强框架，利用样本间和样本内的参考上下文增强多模态特征。


<details>
  <summary>Details</summary>
Motivation: 解决跨模态交互中参考上下文不足的问题，当前方法主要关注单个样本内的模态级参考上下文，忽略了跨样本关系的潜在价值。

Method: 设计了对比跨模态检索模块、两种提示（模态级和样本级）以及跨模态检索增强编码器。

Result: 在两个公开数据集上验证了模型的有效性和优越性。

Conclusion: 提出的框架能同时利用模态级和样本级参考上下文，显著提升多模态特征增强效果。

Abstract: Multimodal sentiment analysis is a fundamental problem in the field of
affective computing. Although significant progress has been made in cross-modal
interaction, it remains a challenge due to the insufficient reference context
in cross-modal interactions. Current cross-modal approaches primarily focus on
leveraging modality-level reference context within a individual sample for
cross-modal feature enhancement, neglecting the potential cross-sample
relationships that can serve as sample-level reference context to enhance the
cross-modal features. To address this issue, we propose a novel multimodal
retrieval-augmented framework to simultaneously incorporate inter-sample
modality-level reference context and cross-sample sample-level reference
context to enhance the multimodal features. In particular, we first design a
contrastive cross-modal retrieval module to retrieve semantic similar samples
and enhance target modality. To endow the model to capture both inter-sample
and intra-sample information, we integrate two different types of prompts,
modality-level prompts and sample-level prompts, to generate modality-level and
sample-level reference contexts, respectively. Finally, we design a cross-modal
retrieval-augmented encoder that simultaneously leverages modality-level and
sample-level reference contexts to enhance the target modality. Extensive
experiments demonstrate the effectiveness and superiority of our model on two
publicly available datasets.

</details>


### [38] [Mining the Social Fabric: Unveiling Communities for Fake News Detection in Short Videos](https://arxiv.org/abs/2508.07992)
*Haisong Gong,Bolan Su,Xinrong Zhang,Jing Li,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.MM

TL;DR: 论文提出了一种名为DugFND的新方法，通过建模上传者社区和事件驱动社区的双重模式，结合预训练的假新闻检测模型，显著提升了短视频中虚假新闻的检测性能。


<details>
  <summary>Details</summary>
Motivation: 短视频平台因内容生成快和算法放大效应导致虚假新闻泛滥，现有方法忽视了视频、上传者和事件之间的隐含关系。

Method: 提出DugFND方法，构建上传者、视频和事件的异构图，设计时间感知异构图注意力网络，并采用重构预训练优化节点表示。

Result: 在公开数据集上的实验表明，DugFND显著提升了假新闻检测性能。

Conclusion: 双重社区建模对短视频虚假新闻检测具有重要价值。

Abstract: Short video platforms have become a major medium for information sharing, but
their rapid content generation and algorithmic amplification also enable the
widespread dissemination of fake news. Detecting misinformation in short videos
is challenging due to their multi-modal nature and the limited context of
individual videos. While recent methods focus on analyzing content
signals-visual, textual, and audio-they often overlook implicit relationships
among videos, uploaders, and events. To address this gap, we propose DugFND
(Dual-community graph for fake news detection), a novel method that enhances
existing video classifiers by modeling two key community patterns: (1) uploader
communities, where uploaders with shared interests or similar content creation
patterns group together, and (2) event-driven communities, where videos related
to the same or semantically similar public events form localized clusters. We
construct a heterogeneous graph connecting uploader, video, and event nodes,
and design a time-aware heterogeneous graph attention network to enable
effective message passing. A reconstruction-based pretraining phase further
improves node representation learning. DugFND can be applied to any pre-trained
classifier. Experiments on public datasets show that our method achieves
significant performance gains, demonstrating the value of dual-community
modeling for fake news detection in short videos.

</details>


### [39] [VGGSounder: Audio-Visual Evaluations for Foundation Models](https://arxiv.org/abs/2508.08237)
*Daniil Zverev,Thaddäus Wiedemer,Ameya Prabhu,Matthias Bethge,Wieland Brendel,A. Sophia Koepke*

Main category: cs.MM

TL;DR: VGGSounder数据集存在标注不全、类别重叠和模态不对齐等问题，影响评估效果。作者提出了一个重新标注的多标签测试集VGGSounder，用于更准确地评估音频视觉基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有VGGSounder数据集在评估音频视觉基础模型时存在局限性，可能导致评估结果失真，因此需要改进。

Method: 重新标注VGGSounder数据集，设计多标签测试集，并引入新的模态混淆度量来分析模型性能下降。

Result: VGGSounder提供了更详细的模态标注，能够精确分析模型在特定模态上的表现，并揭示模型在多模态输入时的局限性。

Conclusion: 新VGGSounder测试集能更可靠地评估音频视觉基础模型的多模态理解能力，解决了原有数据集的不足。

Abstract: The emergence of audio-visual foundation models underscores the importance of
reliably assessing their multi-modal understanding. The VGGSounder dataset is
commonly used as a benchmark for evaluation audio-visual classification.
However, our analysis identifies several limitations of VGGSounder, including
incomplete labelling, partially overlapping classes, and misaligned modalities.
These lead to distorted evaluations of auditory and visual capabilities. To
address these limitations, we introduce VGGSounder, a comprehensively
re-annotated, multi-label test set that extends VGGSound and is specifically
designed to evaluate audio-visual foundation models. VGGSounder features
detailed modality annotations, enabling precise analyses of modality-specific
performance. Furthermore, we reveal model limitations by analysing performance
degradation when adding another input modality with our new modality confusion
metric.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [40] [On the fault diameter and wide diameter of the exchanged 3-ary $n$-cube](https://arxiv.org/abs/2508.07174)
*Rongshuan Geng,Wantao Ning*

Main category: cs.LO

TL;DR: 该论文研究了交换三进制$n$-立方体的故障直径和宽直径，给出了它们的上下界，范围在$n+3$到$n+5$之间。


<details>
  <summary>Details</summary>
Motivation: 评估交换三进制$n$-立方体网络的通信性能，通过故障直径和宽直径衡量其容错性和传输效率。

Method: 分析了交换三进制$n$-立方体$E3C(r, s, t)$的结构，推导了其在$1 \leq r \leq s \leq t$条件下的故障直径$(2r + 1)$和宽直径$(2r + 2)$。

Result: 得出故障直径和宽直径的上下界分别为$n + 3$和$n + 5$。

Conclusion: 交换三进制$n$-立方体在故障容忍和传输效率方面具有良好的性能。

Abstract: Fault diameter and wide diameter are two critical parameters for evaluating
communication performance in interconnection networks. They measure the fault
tolerance and transmission efficiency of networks. The exchanged 3-ary $n$-cube
is a recently proposed variant of the hypercube, denoted by $E3C(r, s, t)$. In
this work, we obtain that the $(2r + 1)$-fault diameter and $(2r + 2)$-wide
diameter of $E3C(r, s, t)$ are bounded between $n + 3$ and $n + 5$ for $1 \leq
r \leq s \leq t$.

</details>


### [41] [Presburger Functional Synthesis: Complexity and Tractable Normal Forms](https://arxiv.org/abs/2508.07207)
*S. Akshay,A. R. Balasubramanian,Supratik Chakraborty,Georg Zetzsche*

Main category: cs.LO

TL;DR: 论文研究了Presburger算术理论中的功能合成问题（PFnS），展示了其在EXPTIME内可解决，并提供匹配的指数下限。此外，发现单输入单变量的PFnS与一般布尔功能合成（BFnS）难度相同，并提出一种特殊规范形式PSyNF，确保PFnS在多项式时间内可解。


<details>
  <summary>Details</summary>
Motivation: 研究目的是解决Presburger算术理论中的功能合成问题，填补了从布尔到一阶逻辑的合成问题研究中的空白。

Method: 通过证明PFnS在EXPTIME内可解决，并提供指数下限。提出PSyNF规范形式，确保多项式时间可解性，并探索其编译和检查方法。

Result: PFnS在EXPTIME内有解，与布尔功能合成相比具有明确的指数下限。PSyNF形式能高效解决PFnS，且其他多项式时间可解的形式均可编译为PSyNF。

Conclusion: PFnS的复杂性显著高于BFnS，PSyNF形式为解决该问题提供了一种高效途径，具有重要理论和实践意义。

Abstract: Given a relational specification between inputs and outputs as a logic
formula, the problem of functional synthesis is to automatically synthesize a
function from inputs to outputs satisfying the relation. Recently, a rich line
of work has emerged tackling this problem for specifications in different
theories, from Boolean to general first-order logic. In this paper, we launch
an investigation of this problem for the theory of Presburger Arithmetic, that
we call Presburger Functional Synthesis (PFnS). We show that PFnS can be solved
in EXPTIME and provide a matching exponential lower bound. This is unlike the
case for Boolean functional synthesis (BFnS), where only conditional
exponential lower bounds are known. Further, we show that PFnS for one input
and one output variable is as hard as BFnS in general. We then identify a
special normal form, called PSyNF, for the specification formula that
guarantees poly-time and poly-size solvability of PFnS. We prove several
properties of PSyNF, including how to check and compile to this form, and
conditions under which any other form that guarantees poly-time solvability of
PFnS can be compiled in poly-time to PSyNF. Finally, we identify a syntactic
normal form that is easier to check but is exponentially less succinct than
PSyNF.

</details>


### [42] [From Knowledge to Conjectures: A Modal Framework for Reasoning about Hypotheses](https://arxiv.org/abs/2508.07304)
*Fabio Vitali*

Main category: cs.LO

TL;DR: 提出了一种新的认知模态逻辑家族，用于形式化猜想推理，通过避免经典模态逻辑中的“模态坍塌”问题，实现了对猜想和事实的分层。


<details>
  <summary>Details</summary>
Motivation: 旨在解决传统模态逻辑在猜想推理中因Axiom C和Axiom T导致的模态坍塌问题，提供一种适用于不完全知识的逻辑框架。

Method: 采用基于弱克里尼逻辑或描述逻辑的准完备语义框架，避免Axiom T，并提出新的模态系统（如KC和KDC）。

Result: 证明了新系统的完备性、可判定性，并在部分知识下具有鲁棒性；引入动态操作settle(φ)以从猜想过渡到事实。

Conclusion: 通过避免经典假设，新逻辑成功形式化了猜想推理，并在认知状态更新中表现出优势。

Abstract: This paper introduces a new family of cognitive modal logics designed to
formalize conjectural reasoning: a modal system in which cognitive contexts
extend known facts with hypothetical assumptions to explore their consequences.
Unlike traditional doxastic and epistemic systems, conjectural logics rely on a
principle, called Axiom C ($\varphi \rightarrow \Box\varphi$), that ensures
that all established facts are preserved across hypothetical layers. While
Axiom C was dismissed in the past due to its association with modal collapse,
we show that the collapse only arises under classical and bivalent assumptions,
and specifically in the presence of Axiom T. Hence we avoid Axiom T and adopt a
paracomplete semantic framework, grounded in Weak Kleene logic or Description
Logic, where undefined propositions coexist with modal assertions. This
prevents the modal collapse and guarantees a layering to distinguish between
factual and conjectural statements. Under this framework we define new modal
systems, e.g., KC and KDC, and show that they are complete, decidable, and
robust under partial knowledge. Finally, we introduce a dynamic operation,
$\mathsf{settle}(\varphi)$, which formalizes the transition from conjecture to
accepted fact, capturing the event of the update of a world's cognitive state
through the resolution of uncertainty.

</details>


### [43] [A Rule-Based Approach to Specifying Preferences over Conflicting Facts and Querying Inconsistent Knowledge Bases](https://arxiv.org/abs/2508.07742)
*Meghyn Bienvenu,Camille Bourgaux,Katsumi Inoue,Robin Jean*

Main category: cs.LO

TL;DR: 提出了一种基于声明的规则框架，用于指定和计算冲突事实之间的优先级关系，解决了偏好规则中可能出现的循环问题，并通过实验评估了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了如何指定优先级关系的机制，作者提出了一个声明式的规则框架来解决这一问题，并探索了消除循环偏好的方法。

Method: 引入了一个声明式的规则框架来定义优先级关系，研究了规则是否产生无环关系的条件，并提出了多种循环消除技术。

Result: 通过实验评估展示了该框架的有效性，包括优先级关系的计算、循环消除和查询回答。

Conclusion: 该框架为不一致知识库中的查询提供了一种有效的优先级关系定义和计算方法，并展示了其实际应用的可行性。

Abstract: Repair-based semantics have been extensively studied as a means of obtaining
meaningful answers to queries posed over inconsistent knowledge bases (KBs).
While several works have considered how to exploit a priority relation between
facts to select optimal repairs, the question of how to specify such
preferences remains largely unaddressed. This motivates us to introduce a
declarative rule-based framework for specifying and computing a priority
relation between conflicting facts. As the expressed preferences may contain
undesirable cycles, we consider the problem of determining when a set of
preference rules always yields an acyclic relation, and we also explore a
pragmatic approach that extracts an acyclic relation by applying various cycle
removal techniques. Towards an end-to-end system for querying inconsistent KBs,
we present a preliminary implementation and experimental evaluation of the
framework, which employs answer set programming to evaluate the preference
rules, apply the desired cycle resolution techniques to obtain a priority
relation, and answer queries under prioritized-repair semantics.

</details>


### [44] [Runtime Verification for LTL in Stochastic Systems](https://arxiv.org/abs/2508.07963)
*Javier Esparza,Vincent Fischer*

Main category: cs.LO

TL;DR: 提出了一种新的运行时监控方法，用概率预测和置信度评分替代传统监控器的二元判决，确保预测的最终正确性和置信度增长。


<details>
  <summary>Details</summary>
Motivation: 传统监控器对LTL公式（尤其是活性性质）的有限前缀只能输出‘不确定’，无法得出结论。

Method: 采用概率预测和置信度评分取代传统监控器的二元判决，确保预测的渐进正确性和置信度增长。

Result: 新方法能够对无法通过有限前缀判断的LTL公式提供预测，并保证最终结果的正确性。

Conclusion: 该方法为运行时监控提供了更灵活的解决方案，适用于无法通过传统方法判断的LTL性质。

Abstract: Runtime verification encompasses several lightweight techniques for checking
whether a system's current execution satisfies a given specification. We focus
on runtime verification for Linear Temporal Logic (LTL). Previous work
describes monitors which produce, at every time step one of three outputs -
true, false, or inconclusive - depending on whether the observed execution
prefix definitively determines satisfaction of the formula. However, for many
LTL formulas, such as liveness properties, satisfaction cannot be concluded
from any finite prefix. For these properties traditional monitors will always
output inconclusive. In this work, we propose a novel monitoring approach that
replaces hard verdicts with probabilistic predictions and an associated
confidence score. Our method guarantees eventual correctness of the prediction
and ensures that confidence increases without bound from that point on.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [45] [Accessibility Literacy: Increasing accessibility awareness among young content creators](https://arxiv.org/abs/2508.06512)
*Alina Karakanta*

Main category: cs.HC

TL;DR: 研究探讨了通过简短的模块化培训提升年轻内容创作者的无障碍素养，结果表明即使是短暂接触无障碍材料也能显著改变他们的认知与行为意愿。


<details>
  <summary>Details</summary>
Motivation: 随着视听和网络内容的激增，媒体无障碍教育的需求日益增长，但在大学课程中仍被忽视。

Method: 提出一个包含简易培训材料（如图表和简短测验）的迷你模块，并通过前后调查评估参与者的无障碍素养变化。

Result: 培训后，参与者表现出更高的无障碍工具使用意愿，但具体方式因内容类型和目的而异。

Conclusion: 小规模、有针对性的干预可以作为将无障碍培训融入正规教育的有效替代方案。

Abstract: The proliferation of audiovisual and web content has created an increasing
need for media accessibility education in various fields. However,
accessibility remains a low priority in university curricula. This project
explores the feasibility of an alternative learning experience aimed at
increasing the accessibility literacy of young content creators, taking web
accessibility as a case study. We propose a mini module that uses simple,
easy-to-use training materials, such as infographics and short quizzes, and can
be easily incorporated in educational programmes along existing courses. A
survey was conducted to investigate the participants' accessibility literacy
before and after training. The findings show that young content creators
generally have limited accessibility literacy but even brief exposure to
accessibility materials contributed to a shift in perceptions. After training,
participants expressed more willingness to implement accessibility tools in
their content, with ways varying depending on content type and purpose. This
suggests that small, yet targeted interventions could be an alternative for
integrating accessibility training into formal education across various
disciplines. While some responses reflected traces of the medical model of
disability and a particularlist view of accessibility, accessibility was
recognised as important for increasing inclusion, improving content, and
shaping a fairer society.

</details>


### [46] [ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets](https://arxiv.org/abs/2508.06732)
*Yuya Kawakami,Daniel Cayan,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.HC

TL;DR: ClimateSOM是一种可视化分析工作流，结合自组织映射(SOM)和大语言模型(LLMs)，用于探索和解释气候集合数据集的可变性。


<details>
  <summary>Details</summary>
Motivation: 气候科学中集合数据集用于模拟未来条件下的变异性，但如何分析和理解这些变异性的模式和幅度是关键挑战。

Method: 使用SOM将时空时间序列抽象为2D空间分布，并集成LLMs辅助解释，支持交互式探索、聚类和比较。

Result: ClimateSOM成功应用于美国西北部降水预测的集合数据集，并通过专家评审验证了其实用性。

Conclusion: ClimateSOM为气候科学家提供了有效的工具，用于分析和理解集合模型的可变性。

Abstract: Ensemble datasets are ever more prevalent in various scientific domains. In
climate science, ensemble datasets are used to capture variability in
projections under plausible future conditions including greenhouse and aerosol
emissions. Each ensemble model run produces projections that are fundamentally
similar yet meaningfully distinct. Understanding this variability among
ensemble model runs and analyzing its magnitude and patterns is a vital task
for climate scientists. In this paper, we present ClimateSOM, a visual analysis
workflow that leverages a self-organizing map (SOM) and Large Language Models
(LLMs) to support interactive exploration and interpretation of climate
ensemble datasets. The workflow abstracts climate ensemble model runs -
spatiotemporal time series - into a distribution over a 2D space that captures
the variability among the ensemble model runs using a SOM. LLMs are integrated
to assist in sensemaking of this SOM-defined 2D space, the basis for the visual
analysis tasks. In all, ClimateSOM enables users to explore the variability
among ensemble model runs, identify patterns, compare and cluster the ensemble
model runs. To demonstrate the utility of ClimateSOM, we apply the workflow to
an ensemble dataset of precipitation projections over California and the
Northwestern United States. Furthermore, we conduct a short evaluation of our
LLM integration, and conduct an expert review of the visual workflow and the
insights from the case studies with six domain experts to evaluate our approach
and its utility.

</details>


### [47] [Toward a Logic of Generalization about Visualization as a Decision Aid](https://arxiv.org/abs/2508.06751)
*Alex Kale*

Main category: cs.HC

TL;DR: 本文探讨了可视化研究中泛化逻辑的局限性，特别是作为决策辅助工具时的应用问题，并提出决策理论作为理解情境变化的框架。


<details>
  <summary>Details</summary>
Motivation: 可视化研究常因情境差异难以泛化研究结果，尤其在决策辅助应用中表现明显。本文旨在解决这一问题。

Method: 作者运用决策理论定义了决策问题的维度，并分析了可视化支持决策时的异质性情景。

Result: 研究发现效用是可视化决策研究中一个核心但未充分探索的概念，决策理论有助于理解情境变化。

Conclusion: 决策理论可作为可视化研究中理解情境变化的新视角，提升泛化逻辑的有效性。

Abstract: Visualization as a discipline often grapples with generalization by reasoning
about how study results on the efficacy of a tool in one context might apply to
another context. This work offers an account of the logic of generalization in
visualization research and argues that it struggles in particular with
applications of visualization as a decision aid. We use decision theory to
define the dimensions on which decision problems can vary, and we present an
analysis of heterogeneity in scenarios where visualization supports
decision-making. Our findings identify utility as a focal and under-examined
concept in visualization research on decision-making, demonstrating how the
visualization community's logic of generalization might benefit from using
decision theory as a lens for understanding context variation.

</details>


### [48] [Story Ribbons: Reimagining Storyline Visualizations with Large Language Models](https://arxiv.org/abs/2508.06772)
*Catherine Yeh,Tara Menon,Robin Singh Arya,Helen He,Moira Weigel,Fernanda Viégas,Martin Wattenberg*

Main category: cs.HC

TL;DR: 利用大语言模型（LLM）构建数据解析流水线，自动从小说和剧本中提取叙事信息，并开发交互式可视化系统Story Ribbons，帮助分析人物和主题轨迹。


<details>
  <summary>Details</summary>
Motivation: 当前从非结构化故事数据中提取结构化信息仍具挑战性，LLM的文本处理能力有望改进现有情节可视化技术。

Method: 开发LLM驱动的数据解析流水线提取叙事信息，并构建交互式可视化系统Story Ribbons。

Result: 在36部文学作品上的实验和用户研究表明，该方法能简化叙事可视化并揭示新见解，但也存在AI系统的局限性。

Conclusion: LLM在叙事可视化中具有潜力，但需针对其局限性设计交互方式。

Abstract: Analyzing literature involves tracking interactions between characters,
locations, and themes. Visualization has the potential to facilitate the
mapping and analysis of these complex relationships, but capturing structured
information from unstructured story data remains a challenge. As large language
models (LLMs) continue to advance, we see an opportunity to use their text
processing and analysis capabilities to augment and reimagine existing
storyline visualization techniques. Toward this goal, we introduce an
LLM-driven data parsing pipeline that automatically extracts relevant narrative
information from novels and scripts. We then apply this pipeline to create
Story Ribbons, an interactive visualization system that helps novice and expert
literary analysts explore detailed character and theme trajectories at multiple
narrative levels. Through pipeline evaluations and user studies with Story
Ribbons on 36 literary works, we demonstrate the potential of LLMs to
streamline narrative visualization creation and reveal new insights about
familiar stories. We also describe current limitations of AI-based systems, and
interaction motifs designed to address these issues.

</details>


### [49] [Methodology for Business Intelligence Solutions in Internet Banking Companies](https://arxiv.org/abs/2508.06773)
*Alex Escalante Viteri,Javier Gamboa Cruzado,Leonidas Asto Huaman*

Main category: cs.HC

TL;DR: 论文提出了一种新的商业智能方法论，用于优化银行业的决策过程，减少了时间、人力和成本。


<details>
  <summary>Details</summary>
Motivation: 银行业决策效率低下、信息处理耗时且无正式方法论，亟需优化。

Method: 结合基础研究与应用研究，构建新方法论并实施商业智能解决方案，分析30个决策过程。

Result: 新方法显著减少了决策时间、人力和成本。

Conclusion: 新商业智能方法论有效优化了银行企业互联网业务的决策。

Abstract: Business intelligence in the banking industry has been studied extensively in
the last decade; however, business executives still do not perceive efficiency
in the decision-making process since the management and treatment of
information are very timeconsuming for the deliverer, generating costs in the
process. On the other hand, there is no formal methodology for developing
business intelligence solutions in this sector. This work aims to optimize
decision-making in a business unit that works with internet banking companies,
reducing the time, the number of people, and the costs involved in
decision-making. To meet the objective, basic and applied research was
conducted. The basic research allowed the construction of a new methodology
from a study of critical success factors and approaches from the business
intelligence literature. The applied research involved the implementation of a
business intelligence solution applying the new methodology in a
pre-experimental study. Thirty decision-making processes were analyzed using
pre-test and post-test data. Tools such as a stopwatch and observation were
used to collect and record data on time spent, the number of people, and the
decision-making costs. This information was processed in the specialized
Minitab18 statistical software, which allowed the observation and confirmation
of relevant results regarding time reduction, the number of people, and the
costs generated. Therefore, it was concluded that the business intelligence
solution, applying the new methodology, optimized decision making in the
business unit that works with internet banking for companies.

</details>


### [50] [Visualization Vibes: The Socio-Indexical Function of Visualization Design](https://arxiv.org/abs/2508.06775)
*Michelle Morgenstern,Amy Rae Fox,Graham M. Jones,Arvind Satyanarayan*

Main category: cs.HC

TL;DR: 可视化研究通常关注数据的准确和高效传输，但忽视了社会背景对受众接收的影响。本文通过语言学人类学理论，提出可视化不仅传达数据意义，还传递社会意义，影响受众的接受度。


<details>
  <summary>Details</summary>
Motivation: 在信息时代，虚假信息和科学不信任导致公共数据传播面临挑战。现有研究强调数据准确性，忽略了设计特征如何影响受众的社会认知。

Method: 通过民族志访谈，分析读者如何基于设计特征推断可视化的“氛围”（社会背景），从而影响其接受度。

Result: 研究发现，可视化的形式特征（非内容）能唤起社会背景认同，显著影响受众的接受与互动方式。

Conclusion: 本文提出“社会索引性”概念，为公共数据传播中的问题提供理论基础和实践指导，强调设计需考虑社会背景影响。

Abstract: In contemporary information ecologies saturated with misinformation,
disinformation, and a distrust of science itself, public data communication
faces significant hurdles. Although visualization research has broadened
criteria for effective design, governing paradigms privilege the accurate and
efficient transmission of data. Drawing on theory from linguistic anthropology,
we argue that such approaches-focused on encoding and decoding propositional
content-cannot fully account for how people engage with visualizations and why
particular visualizations might invite adversarial or receptive responses. In
this paper, we present evidence that data visualizations communicate not only
semantic, propositional meaning$\unicode{x2013}$meaning about
data$\unicode{x2013}$but also social, indexical meaning$\unicode{x2013}$meaning
beyond data. From a series of ethnographically-informed interviews, we document
how readers make rich and varied assessments of a visualization's
"vibes"$\unicode{x2013}$inferences about the social provenance of a
visualization based on its design features. Furthermore, these social
attributions have the power to influence reception, as readers' decisions about
how to engage with a visualization concern not only content, or even aesthetic
appeal, but also their sense of alignment or disalignment with the entities
they imagine to be involved in its production and circulation. We argue these
inferences hinge on a function of human sign systems that has thus far been
little studied in data visualization: socio-indexicality, whereby the formal
features (rather than the content) of communication evoke social contexts,
identities, and characteristics. Demonstrating the presence and significance of
this socio-indexical function in visualization, this paper offers both a
conceptual foundation and practical intervention for troubleshooting breakdowns
in public data communication.

</details>


### [51] [Rethinking Privacy Indicators in Extended Reality: Multimodal Design for Situationally Impaired Bystanders](https://arxiv.org/abs/2508.07057)
*Syed Ibrahim Mustafa Shah Bukhari,Maha Sajid,Bo Ji,Brendan David-John*

Main category: cs.HC

TL;DR: 研究探讨了针对XR设备情境性受损旁观者的隐私指示器设计，发现多模态指示器比视觉指示器更有效。


<details>
  <summary>Details</summary>
Motivation: 随着XR设备的普及，旁观者的隐私问题日益突出，现有视觉隐私指示器在情境性受损旁观者中效果不佳。

Method: 通过焦点小组设计五种新型隐私指示器，并对七名参与者进行用户研究评估。

Result: 视觉指示器在情境性受损场景中评价较低，多模态指示器更受青睐。

Conclusion: 需采用适应性、多模态的情境感知设计以提升XR环境中旁观者的隐私保护。

Abstract: As Extended Reality (XR) devices become increasingly prevalent in everyday
settings, they raise significant privacy concerns for bystanders: individuals
in the vicinity of an XR device during its use, whom the device sensors may
accidentally capture. Current privacy indicators, such as small LEDs, often
presume that bystanders are attentive enough to interpret the privacy signals.
However, these cues can be easily overlooked when bystanders are distracted or
have limited vision. We define such individuals as situationally impaired
bystanders. This study explores XR privacy indicator designs that are effective
for situationally impaired bystanders. A focus group with eight participants
was conducted to design five novel privacy indicators. We evaluated these
designs through a user study with seven additional participants. Our results
show that visual-only indicators, typical in commercial XR devices, received
low ratings for perceived usefulness in impairment scenarios. In contrast,
multimodal indicators were preferred in privacy-sensitive scenarios with
situationally impaired bystanders. Ultimately, our results highlight the need
to move toward adaptable, multimodal, and situationally aware designs that
effectively support bystander privacy in everyday XR environments.

</details>


### [52] [Gender and Careers in Platform-Mediated Work: A Longitudinal Study of Online Freelancers](https://arxiv.org/abs/2508.06778)
*Pyeonghwa Kim,Steve Sawyer,Michael Dunn*

Main category: cs.HC

TL;DR: 该研究通过一项为期五年的纵向调查，揭示了数字劳动平台上的性别不平等对自由职业者长期职业发展的负面影响。


<details>
  <summary>Details</summary>
Motivation: 探讨数字劳动平台中性别不平等对自由职业者长期职业发展的影响，弥补现有研究的不足。

Method: 对105名Upwork自由职业者进行了五年的纵向研究。

Result: 研究发现性别差异会长期影响自由职业者的职业轨迹，并提出了‘职业赋权缺失’和‘平台中介的母职惩罚’等新概念。

Conclusion: 研究呼吁CSCW社区通过设计与研究改进，为所有性别创造更可持续、公平的平台工作环境。

Abstract: We advance gender-inclusive research within the CSCW field by investigating
the long-term gendered experiences of online freelancers on digital labor
platforms. The prevalence of gender-based inequalities has attracted
significant attention within the CSCW community. Yet, insights remain limited
on how these inequalities shape workers' long-term experiences on digital labor
platforms. Through a five-year longitudinal study of 105 freelancers on Upwork,
we reveal persistent gender disparities that influence workers' long-term work
and career trajectories, raising concerns about the sustainability of
platform-mediated work. We advance the ongoing dialogue on gender inclusivity
in the community by introducing the concepts of career disempowerment and
platform-mediated motherhood penalty and by offering research and design
implications for CSCW to foster more sustainable, equitable platform work
environments for all genders.

</details>


### [53] [Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale](https://arxiv.org/abs/2508.06786)
*Amy Rae Fox,Michelle Morgenstern,Graham M. Jones,Arvind Satyanarayan*

Main category: cs.HC

TL;DR: 本文探讨了可视化如何传达超出其显式编码数据的社会信息，提出了一个分析框架，并通过调查证明社会推断的普遍性和对信任评估的影响。


<details>
  <summary>Details</summary>
Motivation: 研究可视化如何通过设计传达社会信息，以应对公共数据传播中的挑战。

Method: 通过归因-引发调查和描述性证据，分析社会推断的普遍性和影响因素。

Result: 发现社会推断可异步研究、不限于特定群体或数据素养，且设计特征与主题共同影响这些推断。

Conclusion: 呼吁在可视化研究中纳入社会文化因素，以提供可操作的设计建议。

Abstract: What impressions might readers form with visualizations that go beyond the
data they encode? In this paper, we build on recent work that demonstrates the
socio-indexical function of visualization, showing that visualizations
communicate more than the data they explicitly encode. Bridging this with prior
work examining public discourse about visualizations, we contribute an analytic
framework for describing inferences about an artifact's social provenance. Via
a series of attribution-elicitation surveys, we offer descriptive evidence that
these social inferences: (1) can be studied asynchronously, (2) are not unique
to a particular sociocultural group or a function of limited data literacy, and
(3) may influence assessments of trust. Further, we demonstrate (4) how design
features act in concert with the topic and underlying messages of an artifact's
data to give rise to such 'beyond-data' readings. We conclude by discussing the
design and research implications of inferences about social provenance, and why
we believe broadening the scope of research on human factors in visualization
to include sociocultural phenomena can yield actionable design recommendations
to address urgent challenges in public data communication.

</details>


### [54] [Entendimento de Campanhas no Contexto da Atenção Primária à Saúde: Um Processo de Design Socialmente Consciente](https://arxiv.org/abs/2508.06791)
*Deógenes P. da Silva Junior,Jonas Lopes Guerra,Krissia Menezes,Marisa Sel Franco,Roberto Pereira*

Main category: cs.HC

TL;DR: 该报告分析了社区卫生工作者和地方病控制工作者在初级卫生保健中的工作背景，特别是健康宣传活动，采用了社会意识设计框架，通过多种方法识别利益相关者并收集需求。


<details>
  <summary>Details</summary>
Motivation: 研究旨在深入了解初级卫生保健中健康宣传活动的社会技术背景，以开发更有效的解决方案。

Method: 采用了社会意识设计框架，结合利益相关者识别图、评估框架和符号框架等方法，并使用人物角色和场景分析。

Result: 研究识别了利益相关者、需求及潜在挑战，为开发健康宣传活动管理的中保真原型提供了依据。

Conclusion: 分析结果有助于设计针对初级卫生保健的健康宣传活动管理解决方案，强调了社会技术视角的重要性。

Abstract: This report presents the results of an exploratory analysis of the work
context of Community Health Agents and Endemic Disease Control Agents in
Primary Health Care (PHC), with a particular focus on Health Campaigns. To
understand this context, the study adopted the Socially Aware Design framework,
which employs artifacts and techniques to examine problem domains in a
comprehensive and sociotechnical manner. Methods such as the Stakeholder
Identification Diagram, Evaluation Frame, and Semiotic Framework were applied
to identify stakeholders, anticipate challenges, and elicit social and
technical requirements for the solution. Personas and Scenarios were also used
to illustrate the potential impacts of a solution on various stakeholders and
their life contexts within health campaigns. This report presents the analysis
method, its application, and results, discussing the study's findings to inform
the development of medium-fidelity prototypes for a PHC health campaign
management solution.

</details>


### [55] [Understanding Pedestrian Gesture Misrecognition: Insights from Vision-Language Model Reasoning](https://arxiv.org/abs/2508.06801)
*Tram Thi Minh Tran,Xinyan Yu,Callum Parker,Julie Stephany Berrio Perez,Stewart Worrall,Martin Tomitsch*

Main category: cs.HC

TL;DR: 本文探讨了行人手势在自动驾驶车辆（AV）交互中的重要性及其识别的挑战，利用GPT-4V作为诊断工具分析误识别模式，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 行人手势在交通交流中至关重要，但其细微、模糊和依赖上下文的特点使机器识别成为难题。研究旨在揭示手势误识别的模式和原因。

Method: 使用GPT-4V作为诊断工具，结合人工视频审查和主题分析，分析公共数据集中的行人-车辆交互。

Result: 研究发现手势可见性、行人行为、交互上下文及环境条件是影响误识别的主要因素，并提出了手势设计的实用建议。

Conclusion: 研究为改进AV识别系统提供了见解，强调了上下文建模和不确定性感知的重要性，方法也适用于其他领域。

Abstract: Pedestrian gestures play an important role in traffic communication,
particularly in interactions with autonomous vehicles (AVs), yet their subtle,
ambiguous, and context-dependent nature poses persistent challenges for machine
interpretation. This study investigates these challenges by using GPT-4V, a
vision-language model, not as a performance benchmark but as a diagnostic tool
to reveal patterns and causes of gesture misrecognition. We analysed a public
dataset of pedestrian-vehicle interactions, combining manual video review with
thematic analysis of the model's qualitative reasoning. This dual approach
surfaced recurring factors influencing misrecognition, including gesture
visibility, pedestrian behaviour, interaction context, and environmental
conditions. The findings suggest practical considerations for gesture design,
including the value of salience and contextual redundancy, and highlight
opportunities to improve AV recognition systems through richer context
modelling and uncertainty-aware interpretations. While centred on AV-pedestrian
interaction, the method and insights are applicable to other domains where
machines interpret human gestures, such as wearable AR and assistive
technologies.

</details>


### [56] [Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI](https://arxiv.org/abs/2508.07183)
*Ahmed M. Abuzuraiq,Philippe Pasquier*

Main category: cs.HC

TL;DR: 论文提出了一种基于工艺的可解释AI方法，通过长期实践和交互式操作帮助艺术家理解生成模型的内部机制。


<details>
  <summary>Details</summary>
Motivation: 在创意背景下，现有的可解释AI方法往往局限于透明度，无法满足艺术家的需求。大型生成模型（如文本到图像扩散系统）通常隐藏了其内部结构，限制了艺术家的参与和控制。

Method: 采用基于工艺的方法，结合长期实践（类似Schön的“反思性实践”），开发了一个模型弯曲和检查插件，集成到ComfyUI的节点界面中。

Result: 通过交互式操作模型的各部分，艺术家能够直观地理解每个组件对输出的影响。

Conclusion: 暴露和可操作的大型生成模型可以作为创意材料，为艺术家提供更多参与和控制的机会。

Abstract: Explainable AI (XAI) in creative contexts can go beyond transparency to
support artistic engagement, modifiability, and sustained practice. While
curated datasets and training human-scale models can offer artists greater
agency and control, large-scale generative models like text-to-image diffusion
systems often obscure these possibilities. We suggest that even large models
can be treated as creative materials if their internal structure is exposed and
manipulable. We propose a craft-based approach to explainability rooted in
long-term, hands-on engagement akin to Sch\"on's "reflection-in-action" and
demonstrate its application through a model-bending and inspection plugin
integrated into the node-based interface of ComfyUI. We demonstrate that by
interactively manipulating different parts of a generative model, artists can
develop an intuition about how each component influences the output.

</details>


### [57] [AdjustAR: AI-Driven In-Situ Adjustment of Site-Specific Augmented Reality Content](https://arxiv.org/abs/2508.06826)
*Nels Numan,Jessica Van Brummelen,Ziwen Lu,Anthony Steed*

Main category: cs.HC

TL;DR: AdjustAR系统通过多模态大语言模型（MLLM）动态校正AR内容的错位问题，以适应当前环境变化。


<details>
  <summary>Details</summary>
Motivation: 静态3D模型的AR内容难以适应动态变化的物理环境，导致虚拟内容与实际场景错位，影响用户体验和上下文理解。

Method: 利用MLLM分析合成图像中的错位问题，提出2D校正建议，并将其反向投影到3D空间中实时更新场景。

Result: 系统能够自动检测并校正AR内容的错位，保持内容与预期场景的一致性。

Conclusion: AdjustAR证明了MLLM在动态环境中维护AR内容对齐的有效性，提升了用户体验。

Abstract: Site-specific outdoor AR experiences are typically authored using static 3D
models, but are deployed in physical environments that change over time. As a
result, virtual content may become misaligned with its intended real-world
referents, degrading user experience and compromising contextual
interpretation. We present AdjustAR, a system that supports in-situ correction
of AR content in dynamic environments using multimodal large language models
(MLLMs). Given a composite image comprising the originally authored view and
the current live user view from the same perspective, an MLLM detects
contextual misalignments and proposes revised 2D placements for affected AR
elements. These corrections are backprojected into 3D space to update the scene
at runtime. By leveraging MLLMs for visual-semantic reasoning, this approach
enables automated runtime corrections to maintain alignment with the authored
intent as real-world target environments evolve.

</details>


### [58] [Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators](https://arxiv.org/abs/2508.06846)
*Hyo Jin Do,Rachel Ostrand,Werner Geyer,Keerthiram Murugesan,Dennis Wei,Justin Weisz*

Main category: cs.HC

TL;DR: 研究发现，用户更喜欢通过颜色编码显示事实性的设计，这提高了信任度和准确性验证的便捷性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs容易生成不准确信息，且缺乏有效向用户传达信息真实性的方法。

Method: 通过两个场景实验，比较不同设计策略对用户信任、验证便捷性和偏好的影响。

Result: 颜色编码的设计最受用户信任和喜爱，且验证准确性更便捷。

Conclusion: 研究为LLM应用开发提供了实用设计指南，以校准用户信任并增强对输出的审查能力。

Abstract: Large language models (LLMs) are susceptible to generating inaccurate or
false information, often referred to as "hallucinations" or "confabulations."
While several technical advancements have been made to detect hallucinated
content by assessing the factuality of the model's responses, there is still
limited research on how to effectively communicate this information to users.
To address this gap, we conducted two scenario-based experiments with a total
of 208 participants to systematically compare the effects of various design
strategies for communicating factuality scores by assessing participants'
ratings of trust, ease in validating response accuracy, and preference. Our
findings reveal that participants preferred and trusted a design in which all
phrases within a response were color-coded based on factuality scores.
Participants also found it easier to validate accuracy of the response in this
style compared to a baseline with no style applied. Our study offers practical
design guidelines for LLM application developers and designers, aimed at
calibrating user trust, aligning with user preferences, and enhancing users'
ability to scrutinize LLM outputs.

</details>


### [59] [Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions](https://arxiv.org/abs/2508.06872)
*Danyang Fan,Walker Smith,Takako Fujioka,Chris Chage,Sile O'Modhrain,Diana Deutsch,Sean Follmer*

Main category: cs.HC

TL;DR: 该论文提出了一种基于均匀y间距采样的新型声化方法（Variable Tempo），通过实验证明其在感知数据趋势的斜率和加速度方面优于传统的均匀x间距采样方法（Variable Pitch Interval）。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何更好地通过声化技术感知数据趋势的关键特征（如斜率和加速度），利用人们对节奏的自然感知能力。

Method: 引入了一种新型的采样方法（Variable Tempo），通过均匀y间距采样产生一致的音高间隔但可变的节奏，并与传统的均匀x间距采样方法（Variable Pitch Interval）和无采样基线（Continuous）进行了对比实验。

Result: 实验结果显示，Variable Tempo在斜率比较任务中表现更准确，加速度感知的最小可觉差比其他方法精细13倍以上，参与者对其信心更高、认知负荷更低，并表现出更强的偏好。

Conclusion: 研究表明，利用节奏的Variable Tempo方法在感知数据趋势的斜率和加速度方面具有显著优势，为声化技术提供了一种更敏感、准确和精确的解决方案。

Abstract: Sonification offers a non-visual way to understand data, with pitch-based
encodings being the most common. Yet, how well people perceive slope and
acceleration-key features of data trends-remains poorly understood. Drawing on
people's natural abilities to perceive tempo, we introduce a novel sampling
method for pitch-based sonification to enhance the perception of slope and
acceleration in univariate functions. While traditional sonification methods
often sample data at uniform x-spacing, yielding notes played at a fixed tempo
with variable pitch intervals (Variable Pitch Interval), our approach samples
at uniform y-spacing, producing notes with consistent pitch intervals but
variable tempo (Variable Tempo). We conducted psychoacoustic experiments to
understand slope and acceleration perception across three sampling methods:
Variable Pitch Interval, Variable Tempo, and a Continuous (no sampling)
baseline. In slope comparison tasks, Variable Tempo was more accurate than the
other methods when modulated by the magnitude ratio between slopes. For
acceleration perception, just-noticeable differences under Variable Tempo were
over 13 times finer than with other methods. Participants also commonly
reported higher confidence, lower mental effort, and a stronger preference for
Variable Tempo compared to other methods. This work contributes models of slope
and acceleration perception across pitch-based sonification techniques,
introduces Variable Tempo as a novel and preferred sampling method, and
provides promising initial evidence that leveraging timing can lead to more
sensitive, accurate, and precise interpretation of derivative-based data
features.

</details>


### [60] [Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools](https://arxiv.org/abs/2508.07203)
*Prashant Sharma*

Main category: cs.HC

TL;DR: 该论文提出了一种开源的、可复制的平台，允许非IT部门的公务员在政府网络中开发和部署小规模应用程序，填补了现有数字政府文献的空白。


<details>
  <summary>Details</summary>
Motivation: 当前数字政府研究集中在专业IT团队或供应商开发的系统上，忽视了具备编码能力但缺乏正式部署途径的公务员的需求。

Method: 结合Jupyter Notebooks、预批准的开源库和轻量级治理，提供一个沙盒化、可审计的工作流程平台。

Result: 该平台在遵守机构约束（如采购规则和安全政策）的同时，避免了供应商锁定，并保留了公务员的编程技能。

Conclusion: 这一平台为公共部门的技能保留、韧性和自下而上的数字化转型提供了可复制的模型。

Abstract: Current digital government literature focuses on professional in-house IT
teams, specialized digital service teams, vendor-developed systems, or
proprietary low-code/no-code tools. Almost no scholarship addresses a growing
middle ground: technically skilled civil servants outside formal IT roles who
can write real code but lack a sanctioned, secure path to deploy their work.
This paper introduces a limits-aware, open-source and replicable platform that
enables such public servants to develop, peer review, and deploy small-scale,
domain-specific applications within government networks via a sandboxed,
auditable workflow. By combining Jupyter Notebooks, preapproved open-source
libraries, and lightweight governance, the platform works within institutional
constraints such as procurement rules and IT security policies while avoiding
vendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil
servants' programming skills, keeping them technically competitive with their
private-sector peers. This contribution fills a critical gap, offering a
replicable model for public-sector skill retention, resilience, and bottom-up
digital transformation.

</details>


### [61] [Viewpoint-Tolerant Depth Perception for Shared Extended Space Experience on Wall-Sized Display](https://arxiv.org/abs/2508.06889)
*Dooyoung Kim,Jinseok Hong,Heejeong Ko,Woontack Woo*

Main category: cs.HC

TL;DR: 提出了一种基于墙式显示屏的XR空间，通过人类认知补偿实现多用户共享深度感知，无需个体追踪。研究发现，虚拟深度与观看距离的平衡对深度感知和临场感至关重要。


<details>
  <summary>Details</summary>
Motivation: 传统3D显示系统多针对单用户，依赖头部和眼球追踪。墙式显示屏在多用户交互中的应用尚未充分探索，本研究旨在填补这一空白。

Method: 研究了虚拟深度(dv)和绝对观看距离(da)对人类认知补偿因素的影响，包括感知距离差异、视角阈值和临场感。

Result: 用户可在23至37度的偏离中心角度体验深度感知，但虚拟深度过大时会降低感知效果和临场感。

Conclusion: 墙式显示屏可在博物馆、画廊和教室等场所提供沉浸式多用户体验，无需个体追踪或穿戴设备。

Abstract: We proposed viewpoint-tolerant shared depth perception without individual
tracking by leveraging human cognitive compensation in universally 3D rendered
images on a wall-sized display. While traditional 3D perception-enabled display
systems have primarily focused on single-user scenarios-adapting rendering
based on head and eye tracking the use of wall-sized displays to extend spatial
experiences and support perceptually coherent multi-user interactions remains
underexplored. We investigated the effects of virtual depths (dv) and absolute
viewing distance (da) on human cognitive compensation factors (perceived
distance difference, viewing angle threshold, and perceived presence) to
construct the wall display-based eXtended Reality (XR) space. Results show that
participants experienced a compelling depth perception even from off-center
angles of 23 to 37 degrees, and largely increasing virtual depth worsens depth
perception and presence factors, highlighting the importance of balancing
extended depth of virtual space and viewing distance from the wall-sized
display. Drawing on these findings, wall-sized displays in venues such as
museums, galleries, and classrooms can evolve beyond 2D information sharing to
offer immersive, spatially extended group experiences without individualized
tracking or wearables.

</details>


### [62] [Your Thoughtful Opponent: Embracing Cognitive Conflict with Peer Agent](https://arxiv.org/abs/2508.06955)
*Kyuwon Kim,Jaeryeong Hwang,Younseo Lee,Jeanhee Lee,Sung-Eun Kimm,Hyo-Jeong So*

Main category: cs.HC

TL;DR: 提出了一种Peer Agent（PA）系统，通过模拟对话伙伴在游戏情境中诱导社会认知冲突，以培养民主技能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂社会问题，需要培养教育的民主技能，如尊重多元观点和协作决策。

Method: 基于Inner Thoughts框架和价值观敏感的话语分析，设计了PA系统，包含五个核心模块。

Result: PA系统能有效参与多人语音审议，诱导社会认知冲突。

Conclusion: PA系统为培养民主技能提供了一种创新工具。

Abstract: As complex societal issues continue to emerge, fostering democratic skills
like valuing diverse perspectives and collaborative decision-making is
increasingly vital in education. In this paper, we propose a Peer Agent (PA)
system designed to simulate a deliberative conversational partner that induces
socio-cognitive conflict within dilemma-based game play. Drawing on by the
Inner Thoughts framework and grounded in value-sensitive discourse analysis,
the PA actively participates in voice-based multi-party deliberation with human
players. The system architecture consists of five core modules: Context
Interpreter, Agent State Manager, Thought Generator, Thought Evaluator, and
Thought Articulator.

</details>


### [63] [ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience](https://arxiv.org/abs/2508.08101)
*Yeana Lee Bond,Mungyeong Choe,Baker Kasim Hasan,Arsh Siddiqui,Myounghoon Jeon*

Main category: cs.HC

TL;DR: 该研究探讨了基于ChatGPT的车载对话代理在提升驾驶安全和用户体验方面的潜力，结果表明其在驾驶性能和用户评价上优于传统预编写代理。


<details>
  <summary>Details</summary>
Motivation: 传统车载对话代理依赖预设脚本或有限语音命令，限制了自然交互。研究旨在探索基于ChatGPT的代理是否能提供更自然的驾驶体验。

Method: 40名驾驶员在驾驶模拟器中测试三种条件（无代理、预设代理、ChatGPT代理），并比较驾驶性能和主观评价。

Result: ChatGPT代理在多指标驾驶性能上表现更稳定，用户主观评价更高，且交互主题更丰富多样。

Conclusion: 研究表明，基于大语言模型的车载代理能通过自然交互提升驾驶安全和用户体验。

Abstract: Studies on in-vehicle conversational agents have traditionally relied on
pre-scripted prompts or limited voice commands, constraining natural
driver-agent interaction. To resolve this issue, the present study explored the
potential of a ChatGPT-based in-vehicle agent capable of carrying continuous,
multi-turn dialogues. Forty drivers participated in our experiment using a
motion-based driving simulator, comparing three conditions (No agent,
Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.
Results showed that the ChatGPT-based agent condition led to more stable
driving performance across multiple metrics. Participants demonstrated lower
variability in longitudinal acceleration, lateral acceleration, and lane
deviation compared to the other two conditions. In subjective evaluations, the
ChatGPT-based agent also received significantly higher ratings in competence,
animacy, affective trust, and preference compared to the Pre-scripted agent.
Our thematic analysis of driver-agent conversations revealed diverse
interaction patterns in topics, including driving assistance/questions,
entertainment requests, and anthropomorphic interactions. Our results highlight
the potential of LLM-powered in-vehicle conversational agents to enhance
driving safety and user experience through natural, context-rich interactions.

</details>


### [64] [Beyond Problem Solving: Framing and Problem-Solution Co-Evolution in Data Visualization Design](https://arxiv.org/abs/2508.07058)
*Paul C. Parsons,Prakash Chandra Shukla*

Main category: cs.HC

TL;DR: 该研究探讨了可视化设计中问题框架的连贯性与解决方案之间的动态关系，强调设计是一个反思性的过程，而非线性技术问题解决。


<details>
  <summary>Details</summary>
Motivation: 现有可视化设计模型过于侧重技术问题解决，忽视了设计的解释性和判断性。研究旨在填补这一空白，关注专家设计师如何通过问题框架和反思实践导航设计。

Method: 通过设计挑战、日记记录和半结构化访谈对11名专家进行混合方法研究，并采用反思性主题分析识别关键策略。

Result: 研究发现设计师使用隐喻、启发式、草图等策略重构问题框架，并揭示了设计如何通过反馈和伦理考量动态调整。

Conclusion: 可视化设计是一个反思性的共同进化过程，问题框架是持续的活动，而非初步步骤。研究呼吁未来框架更关注设计中的解释性判断。

Abstract: Visualization design is often described as the process of solving a
well-defined problem by navigating a design space. While existing visualization
design models have provided valuable structure and guidance, they tend to
foreground technical problem-solving and underemphasize the interpretive,
judgment-based aspects of design. In contrast, research in other design
disciplines has emphasized the importance of framing--how designers define and
redefine what the problem is--and the co-evolution of problem and solution
spaces through reflective practice. These dimensions remain underexplored in
visualization research, particularly from the perspective of expert
practitioners. This paper investigates how visualization designers frame
problems and navigate the dynamic interplay between problem understanding and
solution development. We conducted a mixed-methods study with 11 expert
practitioners using design challenges, diary entries, and semi-structured
interviews. Through reflexive thematic analysis, we identified key strategies
that participants used to frame problems, reframe them in response to evolving
constraints or insights, and build bridges between problem and solution spaces.
These included using metaphors, heuristics, sketching, primary generators, and
reflective evaluation of failed or incomplete ideas. Our findings contribute an
empirically grounded account of visualization design as a reflective,
co-evolutionary practice, where framing is not a preliminary step but a
continuous activity embedded in design. Participants often reshaped their
understanding of the problem based on solution attempts, tool feedback, and
ethical or narrative concerns. These insights extend current visualization
design models and highlight the need for frameworks that better account for
framing and interpretive judgment. (See paper for full abstract.)

</details>


### [65] [Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust](https://arxiv.org/abs/2508.07095)
*Hyo Jin Do,Werner Geyer*

Main category: cs.HC

TL;DR: 研究探讨了四种展示AI生成内容事实性评估的方式对用户信任的影响，发现隐藏或模糊不实内容能提高信任同时保持答案质量。


<details>
  <summary>Details</summary>
Motivation: 为避免盲目信任AI导致错误决策，研究探索了如何向用户传达内容的事实性评估。

Method: 测试了四种策略（透明、注意、不透明、模糊）与无事实性信息的基线答案，通过148人实验比较效果。

Result: 不透明和模糊策略在保持答案质量的同时显著提高了用户信任。

Conclusion: 隐藏或模糊不实内容可有效提升用户对AI的信任。

Abstract: Large language models are known to produce outputs that are plausible but
factually incorrect. To prevent people from making erroneous decisions by
blindly trusting AI, researchers have explored various ways of communicating
factuality estimates in AI-generated outputs to end-users. However, little is
known about whether revealing content estimated to be factually incorrect
influences users' trust when compared to hiding it altogether. We tested four
different ways of disclosing an AI-generated output with factuality
assessments: transparent (highlights less factual content), attention
(highlights factual content), opaque (removes less factual content), ambiguity
(makes less factual content vague), and compared them with a baseline response
without factuality information. We conducted a human subjects research (N =
148) using the strategies in question-answering scenarios. We found that the
opaque and ambiguity strategies led to higher trust while maintaining perceived
answer quality, compared to the other strategies. We discuss the efficacy of
hiding presumably less factual content to build end-user trust.

</details>


### [66] [Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers](https://arxiv.org/abs/2508.07129)
*Caroline M. Johnston,Olga Koumoundouros,Angel Hsing-Chi Hwang,Laura Onasch-Vera,Eric Rice,Phebe Vayanos*

Main category: cs.HC

TL;DR: 研究探讨了人工智能在住房资源匹配中的接受度与潜在影响，通过访谈发现政策制定者对AI工具持开放态度，但需与人类决策结合。


<details>
  <summary>Details</summary>
Motivation: 了解AI在无家可归者住房资源分配中的接受度及潜在问题。

Method: 对洛杉矶13名政策制定者进行半结构化访谈，分析其对AI工具的接受度与看法。

Result: 政策制定者欢迎AI工具，但强调需与人类决策结合并解决公平性、透明度问题。

Conclusion: 研究为未来设计负责任AI系统提供了开放性问题与设计考量。

Abstract: Artificial intelligence researchers have proposed various data-driven
algorithms to improve the processes that match individuals experiencing
homelessness to scarce housing resources. It remains unclear whether and how
these algorithms are received or adopted by practitioners and what their
corresponding consequences are. Through semi-structured interviews with 13
policymakers in homeless services in Los Angeles, we investigate whether such
change-makers are open to the idea of integrating AI into the housing resource
matching process, identifying where they see potential gains and drawbacks from
such a system in issues of efficiency, fairness, and transparency. Our
qualitative analysis indicates that, even when aware of various complicating
factors, policymakers welcome the idea of an AI matching tool if thoughtfully
designed and used in tandem with human decision-makers. Though there is no
consensus as to the exact design of such an AI system, insights from
policymakers raise open questions and design considerations that can be
enlightening for future researchers and practitioners who aim to build
responsible algorithmic systems to support decision-making in low-resource
scenarios.

</details>


### [67] [Canvas3D: Empowering Precise Spatial Control for Image Generation with Constraints from a 3D Virtual Canvas](https://arxiv.org/abs/2508.07135)
*Runlin Duan,Yuzhao Chen,Rahul Jain,Yichen Hu,Jingyu Shi,Karthik Ramani*

Main category: cs.HC

TL;DR: Canvas3D是一个交互式系统，利用3D引擎实现图像生成中的精确空间控制，提升用户对对象排列和场景条件的操控能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成AI在空间控制方面存在不足，无法精确满足用户对对象排列和场景条件的需求。

Method: 通过3D引擎将文本描述转换为可交互对象，用户可直接在虚拟画布中精确配置空间布局，生成空间约束指导图像生成。

Result: Canvas3D在空间控制、交互性和用户体验上优于基线系统。

Conclusion: Canvas3D有效弥补了生成AI在空间控制上的不足，提升了用户对图像生成的精确操控能力。

Abstract: Generative AI (GenAI) has significantly advanced the ease and flexibility of
image creation. However, it remains a challenge to precisely control spatial
compositions, including object arrangement and scene conditions. To bridge this
gap, we propose Canvas3D, an interactive system leveraging a 3D engine to
enable precise spatial manipulation for image generation. Upon user prompt,
Canvas3D automatically converts textual descriptions into interactive objects
within a 3D engine-driven virtual canvas, empowering direct and precise spatial
configuration. These user-defined arrangements generate explicit spatial
constraints that guide generative models in accurately reflecting user
intentions in the resulting images. We conducted a closed-end comparative study
between Canvas3D and a baseline system. And an open-ended study to evaluate our
system "in the wild". The result indicates that Canvas3D outperforms the
baseline on spatial control, interactivity, and overall user experience.

</details>


### [68] [SketchConcept: Sketching-based Concept Recomposition for Product Design using Generative AI](https://arxiv.org/abs/2508.07141)
*Runlin Duan,Chenfei Zhu,Yuzhao Chen,Dizhi Ma,Jingyu Shi,Ziyi Liu,Karthik Ramani*

Main category: cs.HC

TL;DR: SketchConcept是一个设计支持工具，利用草图和多模态生成式人工智能（GenAI）将设计概念分解为视觉和功能部分，并通过用户研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统草图设计工具主要关注视觉设计，未能同时探索视觉与功能概念。本文旨在填补这一空白。

Method: 提出一个功能到视觉的映射工作流，利用大型语言模型生成功能描述，并通过图像生成式AI生成概念组件。

Result: SketchConcept支持多模态生成式AI分解、生成和编辑设计概念，满足了整体功能和行为的实现，并通过用户研究验证其可用性。

Conclusion: SketchConcept为概念产品设计提供了一种新的工具，能够同时探索视觉与功能设计，具有实际应用潜力。

Abstract: Conceptual product design requires designers to explore the design space of
visual and functional concepts simultaneously. Sketching has long been adopted
to empower concept exploration. However, current sketch-based design tools
mostly emphasize visual design using emerging techniques. We present
SketchConcept, a design support tool that decomposes design concepts into
visual representations and functionality of concepts using sketches and textual
descriptions. We propose a function-to-visual mapping workflow that maps the
function descriptions generated by a Large Language Model to a component of the
concept produced by image Generative Artificial Intelligence(GenAI). The
function-to-visual mapping allows our system to leverage multimodal GenAI to
decompose, generate, and edit the design concept to satisfy the overall
function and behavior. We present multiple use cases enabled by SketchConcept
to validate the workflow. Finally, we evaluated the efficacy and usability of
our system with a two-session user study.

</details>


### [69] [Exploring Micro Accidents and Driver Responses in Automated Driving: Insights from Real-world Videos](https://arxiv.org/abs/2508.07256)
*Wei Xiang,Chuyue Zhang,Jie Yan*

Main category: cs.HC

TL;DR: 研究探讨了L3自动驾驶中的微观事故（如突然减速和蛇形驾驶），通过机器学习分析环境与自动驾驶代理的关键变量，并结合众包方法研究人类风险感知，以改进自动驾驶系统设计。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶L3虽减轻司机负担，但微观事故（非致命但异常事件）普遍且可能引发更严重事故，现有研究对此关注不足，需深入了解其特性和人类反应。

Method: 收集用户生成的微观事故视频数据集，利用机器学习定位关键变量，并通过众包获取人类风险感知数据。

Result: 识别了安全关键场景的特征，为自动驾驶系统设计提供新见解。

Conclusion: 研究填补了微观事故研究的空白，为自动驾驶系统安全优化提供了数据支持和方法参考。

Abstract: Automated driving in level 3 autonomy has been adopted by multiple companies
such as Tesla and BMW, alleviating the burden on drivers while unveiling new
complexities. This article focused on the under-explored territory of micro
accidents during automated driving, characterized as not fatal but abnormal
aberrations such as abrupt deceleration and snake driving. These micro
accidents are basic yet pervasive events that might results in more severe
accidents. Through collecting a comprehensive dataset of user generated video
recording such micro accidents in natural driving scenarios, this article
locates key variables pertaining to environments and autonomous agents using
machine learning methods. Subsequently, crowdsourcing method provides insights
into human risk perceptions and reactions to these micro accidents. This
article thus describes features of safety critical scenarios other than crashes
and fatal accidents, informing and potentially advancing the design of
automated driving systems.

</details>


### [70] [Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment](https://arxiv.org/abs/2508.07283)
*Bujar Raufi*

Main category: cs.HC

TL;DR: 论文探讨了EEG微状态与大型语言模型（LLMs）的结合，以优化认知负荷状态的评估。结果表明，通过微调LLMs，能显著提高对认知状态的预测准确度。


<details>
  <summary>Details</summary>
Motivation: 通过结合EEG微状态和LLMs，提升对认知负荷状态（如'休息'和'负荷'）的评估能力，推动认知神经科学和认知AI的发展。

Method: 研究分为四个阶段：数据收集与预处理、微状态分割与EEG回拟合、特征提取与提示工程、LLM模型选择与优化。采用监督学习范式训练LLMs。

Result: 结果显示，经过微调后，模型性能显著提升，可准确区分认知负荷状态。

Conclusion: 该方法不仅有助于理解大脑动态，还为认知负荷和认知AI研究的机器学习技术提供了新思路。

Abstract: This study explores the intersection of electroencephalography (EEG)
microstates and Large Language Models (LLMs) to enhance the assessment of
cognitive load states. By utilizing EEG microstate features, the research aims
to fine-tune LLMs for improved predictions of distinct cognitive states,
specifically 'Rest' and 'Load'. The experimental design is delineated in four
comprehensive stages: dataset collection and preprocessing, microstate
segmentation and EEG backfitting, feature extraction paired with prompt
engineering, and meticulous LLM model selection and refinement. Employing a
supervised learning paradigm, the LLM is trained to identify cognitive load
states based on EEG microstate features integrated into prompts, producing
accurate discrimination of cognitive load. A curated dataset, linking EEG
features to specified cognitive load conditions, underpins the experimental
framework. The results indicate a significant improvement in model performance
following the proposed fine-tuning, showcasing the potential of EEG-informed
LLMs in cognitive neuroscience and cognitive AI applications. This approach not
only contributes to the understanding of brain dynamics but also paves the way
for advancements in machine learning techniques applicable to cognitive load
and cognitive AI research.

</details>


### [71] [In-person, Online and Back Again -- A Tale of Three Hybrid Hackathons](https://arxiv.org/abs/2508.07301)
*Abasi-amefon Obot Affia-Jomants,Alexander Serebrenik,James D. Herbsleb,Alexander Nolte*

Main category: cs.HC

TL;DR: 总结了混合黑客马拉松的独特挑战、组织者与参与者的实践经验，并提出了相关建议。


<details>
  <summary>Details</summary>
Motivation: 研究混合黑客马拉松的独特挑战，填补现有研究中混合协作与黑客马拉松研究的空白。

Method: 通过探索性案例研究分析了三个黑客马拉松事件，考察了同步性、物理分布、动态过渡和技术基础设施等维度。

Result: 发现组织者对混合维度的不同考虑导致参与者体验差异，技术运用中存在不足，参与者需自我调整协作策略。

Conclusion: 提出了组织混合黑客马拉松的实用建议，帮助组织者和参与者更好地应对混合形式的挑战。

Abstract: Hybrid hackathons, which combine in-person and online participation, present
unique challenges for organizers and participants. Although such events are
increasingly conducted, research on them remains fragmented, with limited
integration between hackathon studies and hybrid collaboration. Existing
strategies for in-person or online-only events often fail to address the unique
challenges of hybrid formats, such as managing communication across physical
and virtual spaces. Our work addresses this gap by examining how hybrid
hackathons function, analyzing how organizers structure these events and how
participants navigate hybrid-specific challenges. Drawing on established
theories of hybrid collaboration, we examine key dimensions - synchronicity,
physical distribution, dynamic transitions, and technological infrastructure -
that shape collaboration in hybrid events. Through an exploratory case study of
three hackathon events, we analyze how these dimensions are implemented and
their effects on participant experiences. Our findings reveal differing
organizer considerations of the hybrid dimensions in the hackathon design,
leading to distinct experiences for participants. Implementation styles -
favoring in-person, online, or balanced participation - led to varied
participant experiences, affecting access to resources, communication, and team
coordination. Organizers in our study also relied on technology to bridge
hybrid interactions, but overlooked critical aspects like time-zone management,
dynamic transitions, and targeted support for hybrid teams. Additionally,
participants in their teams responded to gaps in event scaffolding by adapting
collaboration strategies, revealing gaps in organizers' preparedness for hybrid
events. Learning from our findings, we offer practical recommendations when
organizing hybrid hackathon events and recommendations to participants when
attending them.

</details>


### [72] [Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics](https://arxiv.org/abs/2508.07390)
*Gustavo Moreira,Leonardo Ferreira,Carolina Veiga,Maryam Hosseini,Fabio Miranda*

Main category: cs.HC

TL;DR: 本文提出了Urbanite框架，通过人机协作降低城市可视化分析的复杂度，利用意图交互和数据流模型实现多阶段对齐。


<details>
  <summary>Details</summary>
Motivation: 城市数据分析复杂且跨领域，传统方法门槛高，大型语言模型提供了降低门槛的潜力，但需解决意图与结果对⻬的挑战。

Method: 提出Urbanite框架，采用数据流模型支持多层次意图指定，并引入可解释性和任务多分辨率定义等功能。

Result: 通过与城市专家合作的案例研究验证了Urbanite的有效性，框架可访问于https://urbantk.org/urbanite。

Conclusion: Urbanite通过人机协作和数据流模型，成功降低了城市可视化分析的门槛，并解决了意图对⻬问题。

Abstract: With the growing availability of urban data and the increasing complexity of
societal challenges, visual analytics has become essential for deriving
insights into pressing real-world problems. However, analyzing such data is
inherently complex and iterative, requiring expertise across multiple domains.
The need to manage diverse datasets, distill intricate workflows, and integrate
various analytical methods presents a high barrier to entry, especially for
researchers and urban experts who lack proficiency in data management, machine
learning, and visualization. Advancements in large language models offer a
promising solution to lower the barriers to the construction of analytics
systems by enabling users to specify intent rather than define precise
computational operations. However, this shift from explicit operations to
intent-based interaction introduces challenges in ensuring alignment throughout
the design and development process. Without proper mechanisms, gaps can emerge
between user intent, system behavior, and analytical outcomes. To address these
challenges, we propose Urbanite, a framework for human-AI collaboration in
urban visual analytics. Urbanite leverages a dataflow-based model that allows
users to specify intent at multiple scopes, enabling interactive alignment
across the specification, process, and evaluation stages of urban analytics.
Based on findings from a survey to uncover challenges, Urbanite incorporates
features to facilitate explainability, multi-resolution definition of tasks
across dataflows, nodes, and parameters, while supporting the provenance of
interactions. We demonstrate Urbanite's effectiveness through usage scenarios
created in collaboration with urban experts. Urbanite is available at
https://urbantk.org/urbanite.

</details>


### [73] [StreetWeave: A Declarative Grammar for Street-Overlaid Visualization of Multivariate Data](https://arxiv.org/abs/2508.07496)
*Sanjana Srabanti,G. Elisabeta Marai,Fabio Miranda*

Main category: cs.HC

TL;DR: 本文提出了一种名为StreetWeave的声明式语法，用于设计跨多分辨率的多元空间网络数据可视化，解决了现有街道和行人网络可视化缺乏统一设计框架的问题。


<details>
  <summary>Details</summary>
Motivation: 街道和行人网络的可视化对城市规划、气候研究和健康专家等领域专家至关重要，但缺乏统一的设计框架来满足不同领域的需求，且现有方法对非编程背景的专家存在高门槛。

Method: 通过定性编码分析了45项研究中的街道覆盖可视化，探讨了其分析目的、可视化方法和数据来源，并基于此提出了StreetWeave声明式语法。

Result: StreetWeave能够创建多种街道覆盖可视化，支持空间数据的有效探索和分析。

Conclusion: StreetWeave为街道和行人网络可视化提供了灵活且易用的设计工具，降低了非编程专家的使用门槛。

Abstract: The visualization and analysis of street and pedestrian networks are
important to various domain experts, including urban planners, climate
researchers, and health experts. This has led to the development of new
techniques for street and pedestrian network visualization, expanding how data
can be shown and understood more effectively. Despite their increasing
adoption, there is no established design framework to guide the creation of
these visualizations while addressing the diverse requirements of various
domains. When exploring a feature of interest, domain experts often need to
transform, integrate, and visualize a combination of thematic data (e.g.,
demographic, socioeconomic, pollution) and physical data (e.g., zip codes,
street networks), often spanning multiple spatial and temporal scales. This not
only complicates the process of visual data exploration and system
implementation for developers but also creates significant entry barriers for
experts who lack a background in programming. With this in mind, in this paper,
we reviewed 45 studies utilizing street-overlaid visualizations to understand
how they are used. Through qualitative coding of these visualizations, we
analyzed three key aspects of street and pedestrian network visualization
usage: the analytical purpose they serve, the visualization approaches
employed, and the data sources used in their creation. Building on this design
space, we introduce StreetWeave, a declarative grammar for designing custom
visualizations of multivariate spatial network data across multiple
resolutions. We demonstrate how StreetWeave can be used to create various
street-overlaid visualizations, enabling effective exploration and analysis of
spatial data. StreetWeave is available at https://urbantk.org/streetweave.

</details>


### [74] [VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design](https://arxiv.org/abs/2508.07497)
*Leonardo Ferreira,Gustavo Moreira,Fabio Miranda*

Main category: cs.HC

TL;DR: 提出了VA-Blueprint方法，系统化梳理了城市可视化分析系统的核心组件，并利用大语言模型扩展知识库建设。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏系统化的知识库来指导VA系统的设计与开发，研究填补了这一空白。

Method: 提出VA-Blueprint方法，系统化分类城市VA系统的构建块，并结合大语言模型自动化扩展知识库。

Result: 构建了包含101篇论文的初始知识库，并通过专家访谈和定量分析评估了方法的有效性。

Conclusion: VA-Blueprint为VA系统的开发提供了结构化、可复现的基础支持。

Abstract: Designing and building visual analytics (VA) systems is a complex, iterative
process that requires the seamless integration of data processing, analytics
capabilities, and visualization techniques. While prior research has
extensively examined the social and collaborative aspects of VA system
authoring, the practical challenges of developing these systems remain
underexplored. As a result, despite the growing number of VA systems, there are
only a few structured knowledge bases to guide their design and development. To
tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that
systematically reviews and categorizes the fundamental building blocks of urban
VA systems, a domain particularly rich and representative due to its intricate
data and unique problem sets. Applying this methodology to an initial set of 20
systems, we identify and organize their core components into a multi-level
structure, forming an initial knowledge base with a structured blueprint for VA
system development. To scale this effort, we leverage a large language model to
automate the extraction of these components for other 81 papers (completing a
corpus of 101 papers), assessing its effectiveness in scaling knowledge base
construction. We evaluate our method through interviews with experts and a
quantitative analysis of annotation metrics. Our contributions provide a deeper
understanding of VA systems' composition and establish a practical foundation
to support more structured, reproducible, and efficient system development.
VA-Blueprint is available at https://urbantk.org/va-blueprint.

</details>


### [75] [Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI](https://arxiv.org/abs/2508.07520)
*Baihan Lin*

Main category: cs.HC

TL;DR: Conversational DNA是一种新颖的可视化语言，通过生物隐喻揭示对话的时序结构，用于分析人类、人机或群体对话的模式。


<details>
  <summary>Details</summary>
Motivation: 传统的对话分析通常将丰富的互动简化为统计摘要，而该方法旨在更直观地揭示对话中的隐含结构和互动模式。

Method: 通过生物隐喻（如链厚度表示语言复杂性，颜色渐变表示情感轨迹）设计可视化语言，并应用于治疗对话和重要人机对话的分析。

Result: 展示了该方法能发现传统方法忽略的互动模式，为理解对话提供新视角。

Conclusion: 该研究结合数据可视化和人机交互，提出了一种新的创意框架，用于理解对话的意义。

Abstract: What if the patterns hidden within dialogue reveal more about communication
than the words themselves? We introduce Conversational DNA, a novel visual
language that treats any dialogue -- whether between humans, between human and
AI, or among groups -- as a living system with interpretable structure that can
be visualized, compared, and understood. Unlike traditional conversation
analysis that reduces rich interaction to statistical summaries, our approach
reveals the temporal architecture of dialogue through biological metaphors.
Linguistic complexity flows through strand thickness, emotional trajectories
cascade through color gradients, conversational relevance forms through
connecting elements, and topic coherence maintains structural integrity through
helical patterns. Through exploratory analysis of therapeutic conversations and
historically significant human-AI dialogues, we demonstrate how this
visualization approach reveals interaction patterns that traditional methods
miss. Our work contributes a new creative framework for understanding
communication that bridges data visualization, human-computer interaction, and
the fundamental question of what makes dialogue meaningful in an age where
humans increasingly converse with artificial minds.

</details>


### [76] [Phoenix: A Novel Context-Aware Voice-Powered Math Equation Workspace and Editor](https://arxiv.org/abs/2508.07576)
*Kenneth Ge,Ryan Paul,Priscilla Zhang,JooYoung Seo*

Main category: cs.HC

TL;DR: 论文提出了一种新型语音驱动的数学工作空间，利用神经科学洞察和大型语言模型，帮助有精细动作障碍的人更自然地解决数学问题。


<details>
  <summary>Details</summary>
Motivation: 传统的语音数学技术依赖精确的符号听写和命令式界面，对精细动作障碍者不友好，亟需改进。

Method: 结合神经科学理念和上下文引擎，利用大型语言模型支持自然语言交互，降低认知负荷。

Result: 开发了一个直观的问题解决环境，使精细动作障碍者能够流畅地参与数学活动。

Conclusion: 该技术成功解放了精细动作障碍者在数学表达中的机械约束，提升了使用体验。

Abstract: Writing mathematical notation requires substantial effort, diverting
cognitive resources from conceptual understanding to documentation mechanics,
significantly impacting individuals with fine motor disabilities (FMDs).
Current limits of speech-based math technologies rely on precise dictation of
math symbols and unintuitive command-based interfaces. We present a novel
voice-powered math workspace, applying neuroscience insights to create an
intuitive problem-solving environment. To minimize cognitive load, we leverage
large language models with our novel context engine to support natural language
interaction. Ultimately, we enable fluid mathematical engagement for
individuals with FMDs -- freed from mechanical constraints.

</details>


### [77] [On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making](https://arxiv.org/abs/2508.07617)
*Sarah Jabbour,David Fouhey,Nikola Banovic,Stephanie D. Shepard,Ella Kazerooni,Michael W. Sjoding,Jenna Wiens*

Main category: cs.HC

TL;DR: 选择性预测可以减轻不准确AI对决策准确性的负面影响，但会改变错误模式，导致漏诊和漏治增加。


<details>
  <summary>Details</summary>
Motivation: 研究选择性预测对临床决策的影响，验证其是否能减少不准确AI预测的负面影响。

Method: 通过对259名临床医生的用户研究，比较基线表现、AI辅助及选择性预测辅助的决策准确性。

Result: 选择性预测维持了总体准确性（64% vs 56%），但增加了漏诊（18%）和漏治（35%）。

Conclusion: 需实证验证人机交互假设，选择性预测虽有效但需注意其副作用。

Abstract: AI has the potential to augment human decision making. However, even
high-performing models can produce inaccurate predictions when deployed. These
inaccuracies, combined with automation bias, where humans overrely on AI
predictions, can result in worse decisions. Selective prediction, in which
potentially unreliable model predictions are hidden from users, has been
proposed as a solution. This approach assumes that when AI abstains and informs
the user so, humans make decisions as they would without AI involvement. To
test this assumption, we study the effects of selective prediction on human
decisions in a clinical context. We conducted a user study of 259 clinicians
tasked with diagnosing and treating hospitalized patients. We compared their
baseline performance without any AI involvement to their AI-assisted accuracy
with and without selective prediction. Our findings indicate that selective
prediction mitigates the negative effects of inaccurate AI in terms of decision
accuracy. Compared to no AI assistance, clinician accuracy declined when shown
inaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]),
but recovered under selective prediction (64% [95% CI: 54%-73%]). However,
while selective prediction nearly maintains overall accuracy, our results
suggest that it alters patterns of mistakes: when informed the AI abstains,
clinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35%
increase in missed treatments) compared to no AI input at all. Our findings
underscore the importance of empirically validating assumptions about how
humans engage with AI within human-AI systems.

</details>


### [78] [Are UX evaluation methods truly accessible](https://arxiv.org/abs/2508.07620)
*Andrés Eduardo Fuentes-Cortázar,Alejandra Rivera-Hernández,José Rafael Rojano-Cáceres*

Main category: cs.HC

TL;DR: 研究表明，现有的用户体验（UX）评估方法虽常被推荐用于聋人用户，但在实际应用中存在显著障碍，需进一步改进以适应聋人群体的沟通与认知需求。


<details>
  <summary>Details</summary>
Motivation: 为确保聋人用户在软件评估过程中的需求得到充分考虑，本研究旨在验证文献中推荐的UX评估方法在实际场景中的可访问性。

Method: 通过批判性文献综述和实际应用，分析了适合聋人的UX评估方法，并评估其交互、感知和理解方面的优缺点。

Result: 研究发现，传统评估工具因依赖听觉和认知能力且缺乏沟通可访问性，对聋人用户存在重大障碍，导致数据收集不准确。

Conclusion: 结论指出，需改进UX评估方法，确保其真正满足聋人群体的需求，准确反映其用户体验。

Abstract: Providing an equitable and inclusive user experience (UX) for people with
disabilities (PWD) is a central goal of accessible design. In the specific case
of Deaf users, whose hearing impairments impact language development and
communication, it is essential to consider their specific needs during software
evaluation processes. This study aimed to analyze a set of UX evaluation
methods suggested in the literature as suitable for Deaf individuals, with the
goal of validating their level of accessibility in real-world contexts. The
research was based on a critical review and practical application of these
methods, identifying their strengths and limitations in relation to the
interaction, perception, and comprehension of Deaf users. Traditional
evaluation instruments, commonly designed for hearing individuals, pose
significant barriers when applied to Deaf users due to their re-liance on
auditory and cognitive abilities, as well as the lack of consideration for
commu-nicational accessibility. The results show that although these methods
are frequently rec-ommended, they exhibit critical shortcomings that hinder the
collection of accurate and representative data. It is concluded that it is
essential to adapt UX evaluation methods to ensure genuinely accessible
processes that address the communicative and cognitive needs of the Deaf
community and accurately reflect their user experience.

</details>


### [79] [Through Their Eyes: User Perceptions on Sensitive Attribute Inference of Social Media Videos by Visual Language Models](https://arxiv.org/abs/2508.07658)
*Shuning Zhang,Gengrui Zhang,Yibo Meng,Ziyi Zhang,Hantao Zhao,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 研究探讨用户对视觉语言模型（VLM）从视觉数据推断敏感属性的看法，发现用户对未经授权的识别和个人信息滥用深感担忧，提出对平台透明度和隐私保护的需求。


<details>
  <summary>Details</summary>
Motivation: 填补关于用户对VLM推断敏感属性的理解、感知及反应的空白，尤其是在社交媒体视频领域。

Method: 采用半结构化访谈（N=17），调查用户对VLM从其视觉数据中推断敏感属性的观点。

Result: 用户认为VLM能高精度推断多种敏感属性，主要担忧包括未经授权的识别和个人信息滥用，并提出多种缓解策略。

Conclusion: 研究结果对开发负责任AI系统、隐私增强技术及符合用户期望的政策制定至关重要。

Abstract: The rapid advancement of Visual Language Models (VLMs) has enabled
sophisticated analysis of visual content, leading to concerns about the
inference of sensitive user attributes and subsequent privacy risks. While
technical capabilities of VLMs are increasingly studied, users' understanding,
perceptions, and reactions to these inferences remain less explored, especially
concerning videos uploaded on the social media. This paper addresses this gap
through a semi-structured interview (N=17), investigating user perspectives on
VLM-driven sensitive attribute inference from their visual data. Findings
reveal that users perceive VLMs as capable of inferring a range of attributes,
including location, demographics, and socioeconomic indicators, often with
unsettling accuracy. Key concerns include unauthorized identification, misuse
of personal information, pervasive surveillance, and harm from inaccurate
inferences. Participants reported employing various mitigation strategies,
though with skepticism about their ultimate effectiveness against advanced AI.
Users also articulate clear expectations for platforms and regulators,
emphasizing the need for enhanced transparency, user control, and proactive
privacy safeguards. These insights are crucial for guiding the development of
responsible AI systems, effective privacy-enhancing technologies, and informed
policymaking that aligns with user expectations and societal values.

</details>


### [80] [Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory](https://arxiv.org/abs/2508.07664)
*Shuning Zhang,Rongjun Ma,Ying Ma,Shixuan Li,Yiqun Xu,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 研究通过访谈18名用户，探讨了他们对LLM中基于RAG的记忆系统的理解、使用实践和期望，揭示了用户对记忆系统的多样化但不完整的认知，并提出了设计更透明、可信赖系统的建议。


<details>
  <summary>Details</summary>
Motivation: 了解用户对LLM记忆系统的理解、实践和期望，以改进系统的用户中心和透明度。

Method: 对18名用户进行半结构化访谈，并进行主题分析。

Result: 用户对记忆系统的认知多样化但不完整，关注隐私、控制和准确性，希望有更精细的控制和透明度。

Conclusion: 设计LLM记忆系统时需更注重用户中心、透明度和可控性，以提升信任和用户体验。

Abstract: Large Language Models (LLMs) are increasingly integrating memory
functionalities to provide personalized and context-aware interactions.
However, user understanding, practices and expectations regarding these memory
systems are not yet well understood. This paper presents a thematic analysis of
semi-structured interviews with 18 users to explore their mental models of
LLM's Retrieval Augmented Generation (RAG)-based memory, current usage
practices, perceived benefits and drawbacks, privacy concerns and expectations
for future memory systems. Our findings reveal diverse and often incomplete
mental models of how memory operates. While users appreciate the potential for
enhanced personalization and efficiency, significant concerns exist regarding
privacy, control and the accuracy of remembered information. Users express a
desire for granular control over memory generation, management, usage and
updating, including clear mechanisms for reviewing, editing, deleting and
categorizing memories, as well as transparent insight into how memories and
inferred information are used. We discuss design implications for creating more
user-centric, transparent, and trustworthy LLM memory systems.

</details>


### [81] [Towards Aligning Personalized Conversational Recommendation Agents with Users' Privacy Preferences](https://arxiv.org/abs/2508.07672)
*Shuning Zhang,Ying Ma,Jingruo Chen,Simin Li,Xin Yi,Hewu Li*

Main category: cs.HC

TL;DR: 论文提出AI代理的动态行为使传统隐私管理过时，主张代理应主动适应用户隐私偏好，并提出基于情境完整性和隐私计算的理论框架，以解决对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前隐私管理模式依赖用户单向控制，而AI代理的动态交互特性需要更主动的隐私保护方法。

Method: 提出结合情境完整性理论和隐私计算理论的框架，通过反馈学习用户隐私偏好，并利用对齐和帕累托优化平衡隐私与效用。

Result: 框架将隐私管理重构为对齐问题，并提出了形式化方法、应用场景及五大挑战。

Conclusion: AI代理需主动适应用户隐私偏好，提出的理论框架为解决这一问题提供了可行方向。

Abstract: The proliferation of AI agents, with their complex and context-dependent
actions, renders conventional privacy paradigms obsolete. This position paper
argues that the current model of privacy management, rooted in a user's
unilateral control over a passive tool, is inherently mismatched with the
dynamic and interactive nature of AI agents. We contend that ensuring effective
privacy protection necessitates that the agents proactively align with users'
privacy preferences instead of passively waiting for the user to control. To
ground this shift, and using personalized conversational recommendation agents
as a case, we propose a conceptual framework built on Contextual Integrity (CI)
theory and Privacy Calculus theory. This synthesis first reframes automatically
controlling users' privacy as an alignment problem, where AI agents initially
did not know users' preferences, and would learn their privacy preferences
through implicit or explicit feedback. Upon receiving the preference feedback,
the agents used alignment and Pareto optimization for aligning preferences and
balancing privacy and utility. We introduced formulations and instantiations,
potential applications, as well as five challenges.

</details>


### [82] [Improving Continuous Grasp Force Decoding from EEG with Time-Frequency Regressors and Premotor-Parietal Network Integration](https://arxiv.org/abs/2508.07677)
*Parth G. Dangi,Yogesh Kumar Meena*

Main category: cs.HC

TL;DR: EEGForceMap方法通过提取运动相关脑区信号和改进特征集，显著提升了连续握力解码精度。


<details>
  <summary>Details</summary>
Motivation: 现有脑机接口在解码连续握力时效果有限，缺乏神经生理学洞察，EEGForceMap旨在填补这一空白。

Method: 基于EEG信号，从运动前区-顶叶提取任务相关成分，构建三种时频特征集，并用多种回归模型进行预测。

Result: 在特定和独立被试条件下，性能分别提升61.7%和55.7%，R平方达0.815和0.785。

Conclusion: EEGForceMap为中风康复和辅助机器人提供了更精确的动态握力解码方法。

Abstract: Brain-machine interfaces (BMIs) have significantly advanced
neuro-rehabilitation by enhancing motor control. However, accurately decoding
continuous grasp force remains a challenge, limiting the effectiveness of BMI
applications for fine motor tasks. Current models tend to prioritise
algorithmic complexity rather than incorporating neurophysiological insights
into force control, which is essential for developing effective neural
engineering solutions. To address this, we propose EEGForceMap, an EEG-based
methodology that isolates signals from the premotor-parietal region and
extracts task-specific components. We construct three distinct time-frequency
feature sets, which are validated by comparing them with prior studies, and use
them for force prediction with linear, non-linear, and deep learning-based
regressors. The performance of these regressors was evaluated on the
WAY-EEG-GAL dataset that includes 12 subjects. Our results show that
integrating EEGForceMap approach with regressor models yields a 61.7%
improvement in subject-specific conditions (R-squared = 0.815) and a 55.7%
improvement in subject-independent conditions (R-squared = 0.785) over the
state-of-the-art kinematic decoder models. Furthermore, an ablation study
confirms that each preprocessing step significantly enhances decoding accuracy.
This work contributes to the advancement of responsive BMIs for stroke
rehabilitation and assistive robotics by improving EEG-based decoding of
dynamic grasp force.

</details>


### [83] [SimViews: An Interactive Multi-Agent System Simulating Visitor-to-Visitor Conversational Patterns to Present Diverse Perspectives of Artifacts in Virtual Museums](https://arxiv.org/abs/2508.07730)
*Mingyang Su,Chao Liu,Jingling Zhang,WU Shuang,Mingming Fan*

Main category: cs.HC

TL;DR: SimViews是一个交互式多代理人系统，通过模拟游客间的对话模式，展示博物馆展品的多元视角，提高游客理解和参与度。


<details>
  <summary>Details</summary>
Motivation: 为虚拟博物馆中展示多元视角以避免单一叙事的认知局限，提升游客体验。

Method: 利用LLM驱动的多代理人模拟不同专业身份的虚拟游客，构建4种对话模式模拟游客互动。

Result: SimViews能有效通过对话展示多元视角，提升参与者对观点的理解和参与度。

Conclusion: SimViews系统成功解决了虚拟博物馆中展示多元视角的挑战，提升了游客体验。

Abstract: Offering diverse perspectives on a museum artifact can deepen visitors'
understanding and help avoid the cognitive limitations of a single narrative,
ultimately enhancing their overall experience. Physical museums promote
diversity through visitor interactions. However, it remains a challenge to
present multiple voices appropriately while attracting and sustaining a
visitor's attention in the virtual museum. Inspired by recent studies that show
the effectiveness of LLM-powered multi-agents in presenting different opinions
about an event, we propose SimViews, an interactive multi-agent system that
simulates visitor-to-visitor conversational patterns to promote the
presentation of diverse perspectives. The system employs LLM-powered
multi-agents that simulate virtual visitors with different professional
identities, providing diverse interpretations of artifacts. Additionally, we
constructed 4 conversational patterns between users and agents to simulate
visitor interactions. We conducted a within-subject study with 20 participants,
comparing SimViews to a traditional single-agent condition. Our results show
that SimViews effectively facilitates the presentation of diverse perspectives
through conversations, enhancing participants' understanding of viewpoints and
engagement within the virtual museum.

</details>


### [84] [CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning](https://arxiv.org/abs/2508.07731)
*Abdul Basit,Maha Nawaz,Saim Rehman,Muhammad Shafique*

Main category: cs.HC

TL;DR: CognitiveArm是一种基于脑电图的实时脑控假肢系统，通过优化深度学习模型和模型压缩技术，在嵌入式AI硬件上实现了高效且准确的假肢控制。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需侵入的脑机接口系统，用于实时控制假肢，同时在资源受限的嵌入式设备上平衡模型复杂性和计算效率。

Method: 结合BrainFlow库与优化的深度学习模型，通过进化搜索找到Pareto最优配置，并应用模型压缩技术（如剪枝和量化）。设计了EEG数据集和标注流程，支持语音命令切换模式。

Result: 在OpenBCI UltraCortex Mark IV EEG头戴设备上实现了高达90%的准确率，支持三种核心动作分类（左、右、空闲）和多自由度控制。

Conclusion: CognitiveArm展示了非侵入式脑机接口在实时假肢控制中的潜力，适用于日常任务，兼具高效性和准确性。

Abstract: Efficient control of prosthetic limbs via non-invasive brain-computer
interfaces (BCIs) requires advanced EEG processing, including pre-filtering,
feature extraction, and action prediction, performed in real time on edge AI
hardware. Achieving this on resource-constrained devices presents challenges in
balancing model complexity, computational efficiency, and latency. We present
CognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on
embedded AI hardware, achieving real-time operation without compromising
accuracy. The system integrates BrainFlow, an open-source library for EEG data
acquisition and streaming, with optimized deep learning (DL) models for precise
brain signal classification. Using evolutionary search, we identify
Pareto-optimal DL configurations through hyperparameter tuning, optimizer
analysis, and window selection, analyzed individually and in ensemble
configurations. We apply model compression techniques such as pruning and
quantization to optimize models for embedded deployment, balancing efficiency
and accuracy. We collected an EEG dataset and designed an annotation pipeline
enabling precise labeling of brain signals corresponding to specific intended
actions, forming the basis for training our optimized DL models. CognitiveArm
also supports voice commands for seamless mode switching, enabling control of
the prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded
hardware, it ensures low latency and real-time responsiveness. A full-scale
prototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset,
achieved up to 90% accuracy in classifying three core actions (left, right,
idle). Voice integration enables multiplexed, variable movement for everyday
tasks (e.g., handshake, cup picking), enhancing real-world performance and
demonstrating CognitiveArm's potential for advanced prosthetic control.

</details>


### [85] [Challenges in Mixed Reality in Assisting Adults with ADHD Symptoms](https://arxiv.org/abs/2508.07854)
*Valerie Tan,Jens Gerken*

Main category: cs.HC

TL;DR: 本文讨论了成人ADHD症状及混合现实在治疗中的潜力，但现有解决方案有限且缺乏成人特异性研究。


<details>
  <summary>Details</summary>
Motivation: 探讨混合现实技术在成人ADHD症状辅助治疗中的应用潜力。

Method: 分析现有混合现实解决方案的局限性和挑战。

Result: 发现缺乏针对成人ADHD的混合现实原型和持续干预解决方案。

Conclusion: 混合现实技术在成人ADHD治疗中具有潜力，但需要更多研究和开发。

Abstract: In this position paper, we discuss symptoms of attention deficit
hyperactivity disorder (ADHD) in adults, as well as available forms of
treatment or assistance in the context of mixed reality. Mixed reality offers
many potentials for assisting adults with symptoms commonly found in (but not
limited to) ADHD, but the availability of mixed reality solutions is not only
limited commercially, but also limited in terms of proof-of-concept prototypes.
We discuss two major challenges with attention assistance using mixed reality
solutions: the limited availability of adult-specific prototypes and studies,
as well as the limited number of solutions that offer continuous intervention
of ADHD-like symptoms that users can employ in their daily life.

</details>


### [86] [Early Explorations of Recommender Systems for Physical Activity and Well-being](https://arxiv.org/abs/2508.07980)
*Alan Said*

Main category: cs.HC

TL;DR: 论文提出了一个关于有形推荐的框架，关注用户对身体、日常和健康的信任与解释、意图对齐和后果意识。


<details>
  <summary>Details</summary>
Motivation: 随着推荐系统通过可穿戴设备等工具引导用户行为，如何让用户理解、信任并响应这些建议成为新挑战。

Method: 通过概念框架，界定信任与解释、意图对齐和后果意识三个设计维度。

Result: 框架揭示了传统推荐逻辑在实体环境中的局限性，并指导未来系统设计。

Conclusion: 未来系统应支持长期健康、行为对齐和社会责任个性化。

Abstract: As recommender systems increasingly guide physical actions, often through
wearables and coaching tools, new challenges arise around how users interpret,
trust, and respond to this advice. This paper introduces a conceptual framework
for tangible recommendations that influence users' bodies, routines, and
well-being. We describe three design dimensions: trust and interpretation,
intent alignment, and consequence awareness. These highlight key limitations in
applying conventional recommender logic to embodied settings. Through examples
and design reflections, we outline how future systems can support long-term
well-being, behavioral alignment, and socially responsible personalization.

</details>


### [87] [EchoAid: Enhancing Livestream Shopping Accessibility for the DHH Community](https://arxiv.org/abs/2508.08020)
*Zeyu Yang,Zheng Wei,Yang Zhang,Xian Xu,Changyang He,Muzhi Zhou,Pan Hui*

Main category: cs.HC

TL;DR: 摘要概述了一个名为EchoAid的移动应用，旨在改善聋哑及听力障碍群体在直播购物中的体验。


<details>
  <summary>Details</summary>
Motivation: 直播购物平台往往忽视了聋哑及听力障碍者的需求，导致信息获取障碍和认知过载。

Method: 开发了EchoAid应用，结合语音转文本技术、RSVP技术和LLM模型，简化直播销售中的信息流，并通过迭代设计和用户研究优化。

Result: 在38名用户的测试中，EchoAid显著提升了产品信息提取效率，减少了认知负担，并提供了更个性化的购物体验。

Conclusion: EchoAid成功验证了其设计，展示了为聋哑及听力障碍用户提供更好直播购物体验的潜力。

Abstract: Livestream shopping platforms often overlook the accessibility needs of the
Deaf and Hard of Hearing (DHH) community, leading to barriers such as
information inaccessibility and overload. To tackle these challenges, we
developed \textit{EchoAid}, a mobile app designed to improve the livestream
shopping experience for DHH users. \textit{EchoAid} utilizes advanced
speech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology,
and Large Language Models (LLMs) to simplify the complex information flow in
live sales environments. We conducted exploratory studies with eight DHH
individuals to identify design needs and iteratively developed the
\textit{EchoAid} prototype based on feedback from three participants. We then
evaluate the performance of this system in a user study workshop involving 38
DHH participants. Our findings demonstrate the successful design and validation
process of \textit{EchoAid}, highlighting its potential to enhance product
information extraction, leading to reduced cognitive overload and more engaging
and customized shopping experiences for DHH users.

</details>


### [88] [Fuzzy Ontology Embeddings and Visual Query Building for Ontology Exploration](https://arxiv.org/abs/2508.08128)
*Vladimir Zhurov,John Kausch,Kamran Sedig,Mostafa Milani*

Main category: cs.HC

TL;DR: FuzzyVis是一种直观且表达能力强的系统，用于探索复杂本体，结合模糊逻辑和可视化界面，帮助非专家用户轻松导航和查询。


<details>
  <summary>Details</summary>
Motivation: 解决复杂本体（如生物医学、法律等领域）对非专家用户难以导航的问题，同时弥补现有查询工具在灵活性和表达能力上的不足。

Method: FuzzyVis结合模糊逻辑查询模型（基于模糊本体嵌入）和交互式可视化界面，用户可通过逻辑操作符构建复合概念，并进行近似相似性搜索。

Result: 案例研究表明，FuzzyVis能够支持复杂信息需求，帮助用户在大型本体中发现相关概念。

Conclusion: FuzzyVis通过模糊语义和嵌入推理的结合，实现了灵活的解释、高效的计算和探索性学习。

Abstract: Ontologies play a central role in structuring knowledge across domains,
supporting tasks such as reasoning, data integration, and semantic search.
However, their large size and complexity, particularly in fields such as
biomedicine, computational biology, law, and engineering, make them difficult
for non-experts to navigate. Formal query languages such as SPARQL offer
expressive access but require users to understand the ontology's structure and
syntax. In contrast, visual exploration tools and basic keyword-based search
interfaces are easier to use but often lack flexibility and expressiveness. We
introduce FuzzyVis, a proof-of-concept system that enables intuitive and
expressive exploration of complex ontologies. FuzzyVis integrates two key
components: a fuzzy logic-based querying model built on fuzzy ontology
embeddings, and an interactive visual interface for building and interpreting
queries. Users can construct new composite concepts by selecting and combining
existing ontology concepts using logical operators such as conjunction,
disjunction, and negation. These composite concepts are matched against the
ontology using fuzzy membership-based embeddings, which capture degrees of
membership and support approximate, concept-level similarity search. The visual
interface supports browsing, query composition, and partial search without
requiring formal syntax. By combining fuzzy semantics with embedding-based
reasoning, FuzzyVis enables flexible interpretation, efficient computation, and
exploratory learning. Case studies demonstrate how FuzzyVis supports subtle
information needs and helps users uncover relevant concepts in large, complex
ontologies.

</details>


### [89] [Can AI Explanations Make You Change Your Mind?](https://arxiv.org/abs/2508.08158)
*Laura Spillner,Rachel Ringe,Robert Porzel,Rainer Malaka*

Main category: cs.HC

TL;DR: 研究发现，尽管可解释决策支持系统（DSS）中的解释有助于用户判断AI建议的可靠性，但用户常常忽略详细阅读解释，影响其对AI建议的接受度。


<details>
  <summary>Details</summary>
Motivation: 探讨用户是否充分关注AI解释及其对信任和决策的影响。

Method: 通过在线研究分析用户对可解释DSS的信任程度及其解释使用行为。

Result: 许多参与者未详细阅读解释，且解释的细节考虑程度影响其对AI建议的接受度。

Conclusion: 需优化解释设计以提高用户关注度和决策质量。

Abstract: In the context of AI-based decision support systems, explanations can help
users to judge when to trust the AI's suggestion, and when to question it. In
this way, human oversight can prevent AI errors and biased decision-making.
However, this rests on the assumption that users will consider explanations in
enough detail to be able to catch such errors. We conducted an online study on
trust in explainable DSS, and were surprised to find that in many cases,
participants spent little time on the explanation and did not always consider
it in detail. We present an exploratory analysis of this data, investigating
what factors impact how carefully study participants consider AI explanations,
and how this in turn impacts whether they are open to changing their mind based
on what the AI suggests.

</details>


### [90] [Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated Group Decision Making](https://arxiv.org/abs/2508.08242)
*Mohammed Alsobay,David M. Rothschild,Jake M. Hofman,Daniel G. Goldstein*

Main category: cs.HC

TL;DR: 论文研究了大型语言模型（LLM）在群体决策中作为助手的潜力，发现其能促进信息共享但未显著影响最终决策结果。


<details>
  <summary>Details</summary>
Motivation: 群体决策常因信息共享不均而影响质量，LLM在此方面的应用尚未充分探索。

Method: 通过预注册随机实验，测试LLM（GPT-4o）与其他三种辅助方式在群体任务中的表现。

Result: LLM提高了信息共享，但未显著改变决策结果；信息共享的增加不足以克服隐藏轮廓效应。

Conclusion: LLM可用于提升群体信息共享，但需进一步研究其对决策结果的影响；实验工具GRAIL已开源。

Abstract: Group decision-making often suffers from uneven information sharing,
hindering decision quality. While large language models (LLMs) have been widely
studied as aids for individuals, their potential to support groups of users,
potentially as facilitators, is relatively underexplored. We present a
pre-registered randomized experiment with 1,475 participants assigned to 281
five-person groups completing a hidden profile task--selecting an optimal city
for a hypothetical sporting event--under one of four facilitation conditions:
no facilitation, a one-time message prompting information sharing, a human
facilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation
increases information shared within a discussion by raising the minimum level
of engagement with the task among group members, and that these gains come at
limited cost in terms of participants' attitudes towards the task, their group,
or their facilitator. Whether by human or AI, there is no significant effect of
facilitation on the final decision outcome, suggesting that even substantial
but partial increases in information sharing are insufficient to overcome the
hidden profile effect studied. To support further research into how LLM-based
interfaces can support the future of collaborative decision making, we release
our experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an
open-source tool.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [91] [PureSample: Neural Materials Learned by Sampling Microgeometry](https://arxiv.org/abs/2508.07240)
*Zixuan Li,Zixiong Wang,Jian Yang,Milos Hasan,Beibei Wang*

Main category: cs.GR

TL;DR: 论文提出了一种名为 PureSample 的新型神经 BRDF 表示方法，通过采样微几何上的随机行走来学习材料行为，避免了传统物理模型复杂的分析和推导过程。


<details>
  <summary>Details</summary>
Motivation: 传统的基于物理的材质模型依赖于复杂的分析推导 BRDF 方法，且通常忽略空间变化，设计和实现重要性采样方法困难。

Method: PureSample 通过两个可学习组件实现：1）基于流匹配神经网络建模采样分布；2）引入视图相关的反照率项。

Result: PureSample 实现了高效的重要性采样、PDF 评估和 BRDF 评估，适用于包括多层材料和多次散射微表面材料在内的多种复杂材质。

Conclusion: PureSample 提供了一种更简单、通用的方法来学习和表示 BRDF，克服了传统方法的局限性。

Abstract: Traditional physically-based material models rely on analytically derived
bidirectional reflectance distribution functions (BRDFs), typically by
considering statistics of micro-primitives such as facets, flakes, or spheres,
sometimes combined with multi-bounce interactions such as layering and multiple
scattering. These derivations are often complex and model-specific, and
typically consider a statistical aggregate of a large surface area, ignoring
spatial variation. Once an analytic BRDF's evaluation is defined, one still
needs to design an importance sampling method for it, and a way to evaluate the
pdf of that sampling distribution, requiring further model-specific
derivations.
  We present PureSample: a novel neural BRDF representation that allows
learning a material's behavior purely by sampling forward random walks on the
microgeometry, which is usually straightforward to implement. Our
representation allows for efficient importance sampling, pdf evaluation, and
BRDF evaluation, for homogeneous as well as spatially varying materials.
  We achieve this by two learnable components: first, the sampling distribution
is modeled using a flow matching neural network, which allows both importance
sampling and pdf evaluation; second, we introduce a view-dependent albedo term,
captured by a lightweight neural network, which allows for converting a scalar
pdf value to a colored BRDF value for any pair of view and light directions.
  We demonstrate PureSample on challenging materials, including multi-layered
materials, multiple-scattering microfacet materials, and various other
microstructures.

</details>


### [92] [Verification Method for Graph Isomorphism Criteria](https://arxiv.org/abs/2508.07615)
*Chuanfu Hu,Aimin Hou*

Main category: cs.GR

TL;DR: 本文提出了一种验证方法，用于判断前人提出的图同构判定条件是否充分且必要，并提出了一种细分方法以减少回溯空间。


<details>
  <summary>Details</summary>
Motivation: 图同构判定的标准对解决问题至关重要，但现有方法中充分和必要条件存在不足，如需要回溯或证明过程难以保证正确性。

Method: 提出了验证方法和细分方法，前者用于确认条件的充分必要性，后者通过细分必要条件的候选空间减少回溯。

Result: 该方法能够更准确地判断条件的充分必要性，并通过细分减少回溯空间。

Conclusion: 通过验证和细分方法，本文提高了图同构判定的效率和可靠性。

Abstract: The criteria for determining graph isomorphism are crucial for solving graph
isomorphism problems. The necessary condition is that two isomorphic graphs
possess invariants, but their function can only be used to filtrate and
subdivide candidate spaces. The sufficient conditions are used to rebuild the
isomorphic reconstruction of special graphs, but their drawback is that the
isomorphic functions of subgraphs may not form part of the isomorphic functions
of the parent graph. The use of sufficient or necessary conditions generally
results in backtracking to ensure the correctness of the decision algorithm.
The sufficient and necessary conditions can ensure that the determination of
graph isomorphism does not require backtracking, but the correctness of its
proof process is difficult to guarantee. This article proposes a verification
method that can correctly determine whether the judgment conditions proposed by
previous researchers are sufficient and necessary conditions. A subdivision
method has also been proposed in this article, which can obtain more
subdivisions for necessary conditions and effectively reduce the size of
backtracking space.

</details>


### [93] [Vertex Features for Neural Global Illumination](https://arxiv.org/abs/2508.07852)
*Rui Su,Honghao Dong,Haojie Jin,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出了一种基于网格顶点的神经特征表示方法，显著降低了内存占用并提升了渲染效率。


<details>
  <summary>Details</summary>
Motivation: 传统特征网格表示内存占用高，限制了现代并行计算硬件的性能。目标是开发更高效的神经表示方法。

Method: 将可学习特征直接存储在网格顶点，利用几何结构作为紧凑表示，优化内存效率并提升特征表达。

Result: 实验表明，内存消耗仅为传统网格表示的五分之一或更低，且渲染质量相当，推理开销更低。

Conclusion: 神经顶点特征方法在保持渲染质量的同时，显著降低了内存和计算开销，适用于神经渲染任务。

Abstract: Recent research on learnable neural representations has been widely adopted
in the field of 3D scene reconstruction and neural rendering applications.
However, traditional feature grid representations often suffer from substantial
memory footprint, posing a significant bottleneck for modern parallel computing
hardware. In this paper, we present neural vertex features, a generalized
formulation of learnable representation for neural rendering tasks involving
explicit mesh surfaces. Instead of uniformly distributing neural features
throughout 3D space, our method stores learnable features directly at mesh
vertices, leveraging the underlying geometry as a compact and structured
representation for neural processing. This not only optimizes memory
efficiency, but also improves feature representation by aligning compactly with
the surface using task-specific geometric priors. We validate our neural
representation across diverse neural rendering tasks, with a specific emphasis
on neural radiosity. Experimental results demonstrate that our method reduces
memory consumption to only one-fifth (or even less) of grid-based
representations, while maintaining comparable rendering quality and lowering
inference overhead.

</details>


### [94] [Emergent morphogenesis via planar fabrication enabled by a reduced model of composites](https://arxiv.org/abs/2508.08198)
*Yupeng Zhang,Adam Alon,M. Khalid Jawed*

Main category: cs.GR

TL;DR: 提出了一种简化计算和实验框架，通过双层材料实现可编程3D形态。


<details>
  <summary>Details</summary>
Motivation: 设计一种高效且可扩展的方法，用于从平面材料制造复杂3D形状，支持软机器人等功能应用。

Method: 结合数值建模和实验验证，使用双层材料系统（刺激响应层和kirigami图案层）。采用单层简化模型，减少自由度。

Result: 成功实现了多种3D形态，如碗、独木舟和花瓣，并通过仿真和原型验证。

Conclusion: 该方法简化了多层复合材料的建模和制造，为功能材料的3D形态设计提供了新途径。

Abstract: The ability to engineer complex three-dimensional shapes from planar sheets
with precise, programmable control underpins emerging technologies in soft
robotics, reconfigurable devices, and functional materials. Here, we present a
reduced-order numerical and experimental framework for a bilayer system
consisting of a stimuli-responsive thermoplastic sheet (Shrinky Dink) bonded to
a kirigami-patterned, inert plastic layer. Upon uniform heating, the active
layer contracts while the patterned layer constrains in-plane stretch but
allows out-of-plane bending, yielding programmable 3D morphologies from simple
planar precursors. Our approach enables efficient computational design and
scalable manufacturing of 3D forms with a single-layer reduced model that
captures the coupled mechanics of stretching and bending. Unlike traditional
bilayer modeling, our framework collapses the multilayer composite into a
single layer of nodes and elements, reducing the degrees of freedom and
enabling simulation on a 2D geometry. This is achieved by introducing a novel
energy formulation that captures the coupling between in-plane stretch mismatch
and out-of-plane bending - extending beyond simple isotropic linear elastic
models. Experimentally, we establish a fully planar, repeatable fabrication
protocol using a stimuli-responsive thermoplastic and a laser-cut inert plastic
layer. The programmed strain mismatch drives an array of 3D morphologies, such
as bowls, canoes, and flower petals, all verified by both simulation and
physical prototypes.

</details>


### [95] [LL3M: Large Language 3D Modelers](https://arxiv.org/abs/2508.08228)
*Sining Lu,Guan Chen,Nam Anh Dinh,Itai Lang,Ari Holtzman,Rana Hanocka*

Main category: cs.GR

TL;DR: LL3M是一个多智能体系统，利用预训练的大语言模型（LLMs）通过生成可解释的Blender Python代码来创建3D资产。它将形状生成重新定义为代码编写任务，提高了模块化、可编辑性和与艺术家工作流的集成。


<details>
  <summary>Details</summary>
Motivation: 传统的3D生成方法依赖于3D数据集合的学习，缺乏模块化和编辑灵活性。LL3M通过生成代码的方式，提供了更直观、可编辑且集成的解决方案。

Method: LL3M利用一组专业的LLM智能体，通过规划、检索、编写、调试和优化Blender脚本来生成和编辑几何和外观。系统还结合了检索增强生成知识库（BlenderRAG），支持高级建模操作和代码正确性。

Result: LL3M在多样化的形状类别、风格和材质编辑以及用户驱动的细化中表现出色，展示了代码作为生成和可解释介质的强大能力。

Conclusion: LL3M通过代码生成3D资产，提供了一种模块化、可编辑且与艺术家工作流兼容的新方法，为3D创作开辟了新的可能性。

Abstract: We present LL3M, a multi-agent system that leverages pretrained large
language models (LLMs) to generate 3D assets by writing interpretable Python
code in Blender. We break away from the typical generative approach that learns
from a collection of 3D data. Instead, we reformulate shape generation as a
code-writing task, enabling greater modularity, editability, and integration
with artist workflows. Given a text prompt, LL3M coordinates a team of
specialized LLM agents to plan, retrieve, write, debug, and refine Blender
scripts that generate and edit geometry and appearance. The generated code
works as a high-level, interpretable, human-readable, well-documented
representation of scenes and objects, making full use of sophisticated Blender
constructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,
unconstrained shapes, materials, and scenes. This code presents many avenues
for further agent and human editing and experimentation via code tweaks or
procedural parameters. This medium naturally enables a co-creative loop in our
system: agents can automatically self-critique using code and visuals, while
iterative user instructions provide an intuitive way to refine assets. A shared
code context across agents enables awareness of previous attempts, and a
retrieval-augmented generation knowledge base built from Blender API
documentation, BlenderRAG, equips agents with examples, types, and functions
empowering advanced modeling operations and code correctness. We demonstrate
the effectiveness of LL3M across diverse shape categories, style and material
edits, and user-driven refinements. Our experiments showcase the power of code
as a generative and interpretable medium for 3D asset creation. Our project
page is at https://threedle.github.io/ll3m.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [96] [LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning](https://arxiv.org/abs/2508.06799)
*Naiyi Li,Zihui Ma,Runlong Yu,Lingyao Li*

Main category: cs.ET

TL;DR: 论文提出了LSDTs框架，利用LLM从非结构化文档中提取规划知识并构建本体，为数字孪生提供语义层，支持模拟符合法规的规划场景。案例研究验证了其在风力发电场规划中的有效性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生（DTs）在复杂基础设施管理中面临非结构化知识整合的挑战，大语言模型（LLMs）的进展为解决这一问题提供了新机会。

Method: 提出了LSDTs框架，利用LLM提取和整理非结构化文档中的规划知识，构建本体并作为数字孪生的语义层。

Result: 案例研究表明LSDTs能够支持高保真模拟、法规感知的布局优化，并提升基础设施规划的适应性。

Conclusion: 本研究表明，生成式AI与数字孪生的结合在复杂知识驱动规划任务中具有潜力。

Abstract: Digital Twins (DTs) offer powerful tools for managing complex infrastructure
systems, but their effectiveness is often limited by challenges in integrating
unstructured knowledge. Recent advances in Large Language Models (LLMs) bring
new potential to address this gap, with strong abilities in extracting and
organizing diverse textual information. We therefore propose LSDTs
(LLM-Augmented Semantic Digital Twins), a framework that helps LLMs extract
planning knowledge from unstructured documents like environmental regulations
and technical guidelines, and organize it into a formal ontology. This ontology
forms a semantic layer that powers a digital twin-a virtual model of the
physical system-allowing it to simulate realistic, regulation-aware planning
scenarios. We evaluate LSDTs through a case study of offshore wind farm
planning in Maryland, including its application during Hurricane Sandy. Results
demonstrate that LSDTs support interpretable, regulation-aware layout
optimization, enable high-fidelity simulation, and enhance adaptability in
infrastructure planning. This work shows the potential of combining generative
AI with digital twins to support complex, knowledge-driven planning tasks.

</details>


### [97] [Enhancing Mega-Satellite Networks with Generative Semantic Communication: A Networking Perspective](https://arxiv.org/abs/2508.07573)
*Binquan Guo,Wanting Yang,Zehui Xiong,Zhou Zhang,Baosheng Li,Zhu Han,Rahim Tafazolli,Tony Q. S. Quek*

Main category: cs.ET

TL;DR: 论文提出了利用生成语义通信（GSC）提升大型卫星星座网络效率的方法，解决频谱稀缺和容量限制问题。


<details>
  <summary>Details</summary>
Motivation: 解决6G无线通信中卫星直接通信面临的频谱稀缺和容量限制问题，提升多媒体内容的传输效率。

Method: 提出GSC赋能的卫星网络架构，包括网络建模和GSC感知的网络策略，利用离散时序图建模语义编解码器和知识库，开发兼容GSC的路由方案。

Result: 性能评估显示GSC能有效减少带宽消耗并增强语义特征，为卫星通信提供了新解决方案。

Conclusion: GSC在卫星网络中的应用具有潜力，未来需进一步研究以优化其性能和部署。

Abstract: The advance of direct satellite-to-device communication has positioned
mega-satellite constellations as a cornerstone of 6G wireless communication,
enabling seamless global connectivity even in remote and underserved areas.
However, spectrum scarcity and capacity constraints imposed by the Shannon's
classical information theory remain significant challenges for supporting the
massive data demands of multimedia-rich wireless applications. Generative
Semantic Communication (GSC), powered by artificial intelligence-based
generative foundation models, represents a paradigm shift from transmitting raw
data to exchanging semantic meaning. GSC can not only reduce bandwidth
consumption, but also enhance key semantic features in multimedia content,
thereby offering a promising solution to overcome the limitations of
traditional satellite communication systems. This article investigates the
integration of GSC into mega-satellite constellations from a networking
perspective. We propose a GSC-empowered satellite networking architecture and
identify key enabling technologies, focusing on GSC-empowered network modeling
and GSC-aware networking strategies. We construct a discrete temporal graph to
model semantic encoders and decoders, distinct knowledge bases, and resource
variations in mega-satellite networks. Based on this framework, we develop
model deployment for semantic encoders and decoders and GSC-compatible routing
schemes, and then present performance evaluations. Finally, we outline future
research directions for advancing GSC-empowered satellite networks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [98] [PiKV: KV Cache Management System for Mixture of Experts](https://arxiv.org/abs/2508.06526)
*Dong Liu,Yanxuan Yu,Ben Lengerich,Ying Nian Wu,Xuhong Wang*

Main category: cs.DC

TL;DR: 提出了一种名为PiKV的并行分布式KV缓存服务框架，专为MoE架构设计，解决了多GPU和多节点推理中的KV缓存存储问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多GPU和多节点推理中KV缓存存储的内存和通信成本瓶颈。

Method: 采用专家分片KV存储、PiKV路由减少访问、PiKV调度自适应保留查询相关条目，并集成压缩模块减少内存使用。

Result: PiKV已开源，实验结果显示其能有效管理KV缓存，并与Nvidia kvpress集成加速。

Conclusion: PiKV旨在成为MoE架构的全面KV缓存管理系统。

Abstract: As large language models continue to scale up in both size and context
length, the memory and communication cost of key-value (KV) cache storage has
become a major bottleneck in multi-GPU and multi-node inference. While
MoE-based architectures sparsify computation across experts, the corresponding
KV caches remain dense and globally synchronized, resulting in significant
overhead.
  We introduce \textbf{PiKV}, a parallel and distributed KV cache serving
framework tailored for MoE architecture. PiKV leverages \textit{expert-sharded
KV storage} to partition caches across GPUs, \textit{PiKV routing} to reduce
token-to-KV access, and a \textit{PiKV Scheduling} to adaptively retain
query-relevant entries. To further reduce memory usage, PiKV integrates
\textit{PiKV Compression} modules the caching pipeline for acceleration.
  PiKV is recently publicly available as an open-source software library:
\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.
Experiments details is recorded at:
\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\_Results}.
We also have PiKV integrated with Nvidia kvpress for acceleration, details see
\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.
PiKV is still a living project, aiming to become a comprehesive KV Cache
management system for MoE Architectures.

</details>


### [99] [Kairos: Low-latency Multi-Agent Serving with Shared LLMs and Excessive Loads in the Public Cloud](https://arxiv.org/abs/2508.06948)
*Jinyuan Chen,Jiuchen Shi,Quan Chen,Minyi Guo*

Main category: cs.DC

TL;DR: Kairos是一个多智能体协调系统，通过优化请求调度来降低多智能体应用的端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 多智能体应用中，共享大语言模型（LLM）的负载过重和现有调度方法对跨智能体延迟和资源差异的忽视，导致性能低下。

Method: Kairos由工作流协调器、工作流感知优先级调度器和内存感知调度器组成，分别负责收集信息、优化请求优先级和分配资源。

Result: 实验表明，Kairos相比现有方法将端到端延迟降低了17.8%至28.4%。

Conclusion: Kairos有效提升了多智能体应用的性能和资源利用率。

Abstract: Multi-agent applications utilize the advanced capabilities of large language
models (LLMs) for intricate task completion through agent collaboration in a
workflow. Under this situation, requests from different agents usually access
the same shared LLM to perform different kinds of tasks, forcing the shared LLM
to suffer excessive loads. However, existing works have low serving performance
for these multi-agent applications, mainly due to the ignorance of inter-agent
latency and resource differences for request scheduling. We therefore propose
Kairos, a multi-agent orchestration system that optimizes end-to-end latency
for multi-agent applications. Kairos consists of a workflow orchestrator, a
workflow-aware priority scheduler, and a memory-aware dispatcher. The
orchestrator collects agent-specific information for online workflow analysis.
The scheduler decides the serving priority of the requests based on their
latency characteristics to reduce the overall queuing. The dispatcher
dispatches the requests to different LLM instances based on their memory
demands to avoid GPU overloading. Experimental results show that Kairos reduces
end-to-end latency by 17.8% to 28.4% compared to state-of-the-art works.

</details>


### [100] [Convergence Sans Synchronization](https://arxiv.org/abs/2508.06949)
*Arya Tanmay Gupta*

Main category: cs.DC

TL;DR: 本文提出了一种理论，用于设计无需同步即可收敛的多处理器算法，并通过局部状态转换图的偏序性质简化证明过程，显著提升算法效率。


<details>
  <summary>Details</summary>
Motivation: 随着多处理器系统的普及，开发快速并行处理算法的需求增加，但同步机制成本高昂。异步执行能充分利用计算资源，但验证其收敛性复杂且耗时。

Method: 提出理论框架，证明仅需局部状态转换图的偏序性质即可确保算法在异步环境下的收敛性，避免生成全局状态空间图。开发无同步算法并验证其有效性。

Result: 实验显示，相比于文献中需要同步的算法，所设计的无同步算法收敛时间显著减少，验证了理论的实用性。

Conclusion: 该理论为异步算法设计提供了高效证明方法，同时展示了无同步算法在实际系统中的优越性能。

Abstract: We currently see a steady rise in the usage and size of multiprocessor
systems, and so the community is evermore interested in developing fast
parallel processing algorithms. However, most algorithms require a
synchronization mechanism, which is costly in terms of computational resources
and time. If an algorithm can be executed in asynchrony, then it can use all
the available computation power, and the nodes can execute without being
scheduled or locked. However, to show that an algorithm guarantees convergence
in asynchrony, we need to generate the entire global state transition graph and
check for the absence of cycles. This takes time exponential in the size of the
global state space. In this dissertation, we present a theory that explains the
necessary and sufficient properties of a multiprocessor algorithm that
guarantees convergence even without synchronization. We develop algorithms for
various problems that do not require synchronization. Additionally, we show for
several existing algorithms that they can be executed without any
synchronization mechanism. A significant theoretical benefit of our work is in
proving that an algorithm can converge even in asynchrony. Our theory implies
that we can make such conclusions about an algorithm, by only showing that the
local state transition graph of a computing node forms a partial order, rather
than generating the entire global state space and determining the absence of
cycles in it. Thus, the complexity of rendering such proofs, formal or social,
is phenomenally reduced. Experiments show a significant reduction in time taken
to converge, when we compare the execution time of algorithms in the literature
versus the algorithms that we design. We get similar results when we run an
algorithm, that guarantees convergence in asynchrony, under a scheduler versus
in asynchrony.

</details>


### [101] [The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU Libraries](https://arxiv.org/abs/2508.07071)
*Oscar Amoros,Albert Andaluz,Johnny Nunez,Antonio J. Pena*

Main category: cs.DC

TL;DR: 提出了自动融合GPU内核的新方法，解决了现有库需手动融合的限制，利用C++17元编程生成优化内核，提升性能2x至1000x。


<details>
  <summary>Details</summary>
Motivation: 现有GPU库难以充分利用资源，手动内核融合受限且开发成本高。

Method: 定义可重用、可融合组件，利用C++17元编程自动生成优化内核。

Result: 开源实现显示性能提升2x至1000x，保持了高级编程性。

Conclusion: 该方法显著提升GPU性能，同时降低开发成本。

Abstract: Existing GPU libraries often struggle to fully exploit the parallel resources
and on-chip memory (SRAM) of GPUs when chaining multiple GPU functions as
individual kernels. While Kernel Fusion (KF) techniques like Horizontal Fusion
(HF) and Vertical Fusion (VF) can mitigate this, current library
implementations often require library developers to manually create fused
kernels. Hence, library users rely on limited sets of pre-compiled or
template-based fused kernels. This limits the use cases that can benefit from
HF and VF and increases development costs. In order to solve these issues, we
present a novel methodology for building GPU libraries that enables automatic
on-demand HF and VF for arbitrary combinations of GPU library functions. Our
methodology defines reusable, fusionable components that users combine via
high-level programming interfaces. Leveraging C++17 metaprogramming features
available in compilers like nvcc, our methodology generates a single and
optimized fused kernel tailored to the user's specific sequence of operations
at compile time, without needing a custom compiler or manual development and
pre-compilation of kernel combinations. This approach abstracts low-level GPU
complexities while maximizing GPU resource utilization and keeping intermediate
data in SRAM. We provide an open-source implementation demonstrating
significant speedups compared to traditional libraries in various benchmarks,
validating the effectiveness of this methodology for improving GPU performance
in the range of 2x to more than 1000x, while preserving high-level
programmability.

</details>


### [102] [Taming Cold Starts: Proactive Serverless Scheduling with Model Predictive Control](https://arxiv.org/abs/2508.07640)
*Chanh Nguyen,Monowar Bhuyan,Erik Elmroth*

Main category: cs.DC

TL;DR: 论文提出了一种基于模型预测控制的预测性无服务器调度框架，旨在主动缓解冷启动问题，从而提升端到端响应时间。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算的冷启动问题是其平台的主要瓶颈，影响了延迟敏感和突发性工作负载的性能。

Method: 设计了一个基于模型预测控制的框架，通过预测未来调用，联合优化容器预热和请求调度。

Result: 在Apache OpenWhisk和Kubernetes测试平台上，实验结果表明该方法显著优于现有基线，尾部延迟降低85%，资源使用减少34%。

Conclusion: 该框架有效解决了无服务器计算的冷启动问题，同时优化了延迟和资源利用率。

Abstract: Serverless computing has transformed cloud application deployment by
introducing a fine-grained, event-driven execution model that abstracts away
infrastructure management. Its on-demand nature makes it especially appealing
for latency-sensitive and bursty workloads. However, the cold start problem,
i.e., where the platform incurs significant delay when provisioning new
containers, remains the Achilles' heel of such platforms.
  This paper presents a predictive serverless scheduling framework based on
Model Predictive Control to proactively mitigate cold starts, thereby improving
end-to-end response time. By forecasting future invocations, the controller
jointly optimizes container prewarming and request dispatching, improving
latency while minimizing resource overhead.
  We implement our approach on Apache OpenWhisk, deployed on a Kubernetes-based
testbed. Experimental results using real-world function traces and synthetic
workloads demonstrate that our method significantly outperforms
state-of-the-art baselines, achieving up to 85% lower tail latency and a 34%
reduction in resource usage.

</details>


### [103] [AerialDB: A Federated Peer-to-Peer Spatio-temporal Edge Datastore for Drone Fleets](https://arxiv.org/abs/2508.07124)
*Shashwat Jaiswal,Suman Raj,Subhajit Sidhanta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: AerialDB是一个针对多无人机系统的轻量级分布式数据存储和查询系统，适用于灾害管理应用中的实时数据处理。


<details>
  <summary>Details</summary>
Motivation: 为了解决无人机在灾害地区收集的大规模数据无法由机载计算机实时处理的问题，需要设计一个高效的系统支持数据卸载和实时查询。

Method: AerialDB采用分散式存储和查询设计，结合基于内容的副本放置和分片索引技术，优化空间和时间查询。

Result: 在400架无人机和80个边缘节点的测试中，AerialDB表现出高效的扩展性和近实时性能，在插入和查询性能上显著优于现有基线。

Conclusion: AerialDB为灾害管理中的无人机数据提供了高效的分布式处理解决方案，显著提升了性能。

Abstract: Recent years have seen an unprecedented growth in research that leverages the
newest computing paradigm of Internet of Drones, comprising a fleet of
connected Unmanned Aerial Vehicles (UAVs) used for a wide range of tasks such
as monitoring and analytics in highly mobile and changing environments
characteristic of disaster regions. Given that the typical data (i.e., videos
and images) collected by the fleet of UAVs deployed in such scenarios can be
considerably larger than what the onboard computers can process, the UAVs need
to offload their data in real-time to the edge and the cloud for further
processing. To that end, we present the design of AerialDB - a lightweight
decentralized data storage and query system that can store and process time
series data on a multi-UAV system comprising: A) a fleet of hundreds of UAVs
fitted with onboard computers, and B) ground-based edge servers connected
through a cellular link. Leveraging lightweight techniques for content-based
replica placement and indexing of shards, AerialDB has been optimized for
efficient processing of different possible combinations of typical spatial and
temporal queries performed by real-world disaster management applications.
Using containerized deployment spanning up to 400 drones and 80 edges, we
demonstrate that AerialDB is able to scale efficiently while providing near
real-time performance with different realistic workloads. Further, AerialDB
comprises a decentralized and locality-aware distributed execution engine which
provides graceful degradation of performance upon edge failures with relatively
low latency while processing large spatio-temporal data. AerialDB exhibits
comparable insertion performance and 100 times improvement in query performance
against state-of-the-art baseline. Moreover, it exhibits a 10 times and 100
times improvement with insertion and query workloads respectively over the
cloud baseline.

</details>


### [104] [FlashMP: Fast Discrete Transform-Based Solver for Preconditioning Maxwell's Equations on GPUs](https://arxiv.org/abs/2508.07193)
*Haoyuan Zhang,Yaqian Gao,Xinxin Zhang,Jialin Li,Runfeng Jin,Yidong Chen,Feng Zhang,Wu Yuan,Wenpeng Ma,Shan Liang,Jian Zhang,Zhonghua Lu*

Main category: cs.DC

TL;DR: 论文提出了FlashMP，一种基于离散变换的子域精确求解预处理器，用于提高电磁模拟中大规模线性系统的求解效率。


<details>
  <summary>Details</summary>
Motivation: 现有迭代求解器在解决Crank-Nicolson FDTD方法产生的稀疏系统时收敛慢，传统预处理器效果有限，直接求解器内存需求过大。

Method: 提出FlashMP，设计基于离散变换的子域精确求解预处理器，并通过多GPU域分解实现高效扩展。

Result: 在AMD MI60 GPU集群上测试，FlashMP将迭代次数减少16倍，速度提升2.5-4.9倍，并行效率达84.1%。

Conclusion: FlashMP有效解决了大规模线性系统的求解瓶颈，显著提升了计算效率和可扩展性。

Abstract: Efficiently solving large-scale linear systems is a critical challenge in
electromagnetic simulations, particularly when using the Crank-Nicolson
Finite-Difference Time-Domain (CN-FDTD) method. Existing iterative solvers are
commonly employed to handle the resulting sparse systems but suffer from slow
convergence due to the ill-conditioned nature of the double-curl operator.
Approximate preconditioners, like Successive Over-Relaxation (SOR) and
Incomplete LU decomposition (ILU), provide insufficient convergence, while
direct solvers are impractical due to excessive memory requirements. To address
this, we propose FlashMP, a novel preconditioning system that designs a
subdomain exact solver based on discrete transforms. FlashMP provides an
efficient GPU implementation that achieves multi-GPU scalability through domain
decomposition. Evaluations on AMD MI60 GPU clusters (up to 1000 GPUs) show that
FlashMP reduces iteration counts by up to 16x and achieves speedups of 2.5x to
4.9x compared to baseline implementations in state-of-the-art libraries such as
Hypre. Weak scalability tests show parallel efficiencies up to 84.1%.

</details>


### [105] [An Experimental Exploration of In-Memory Computing for Multi-Layer Perceptrons](https://arxiv.org/abs/2508.07317)
*Pedro Carrinho,Hamid Moghadaspour,Oscar Ferraz,João Dinis Ferreira,Yann Falevoz,Vitor Silva,Gabriel Falcao*

Main category: cs.DC

TL;DR: 论文分析了现代通用PiM架构（如UPMEM）在加速神经网络训练和推理中的潜力，通过与CPU和低功耗GPU的比较，展示了PiM在数据移动瓶颈中的显著优势。


<details>
  <summary>Details</summary>
Motivation: 现代计算机架构中，数据移动瓶颈限制了内存密集型任务（如机器学习和神经网络）的性能。处理内存（PiM）是一种新兴的计算范式，旨在通过就近或在内存单元中执行计算来缓解这一问题。

Method: 选择了UPMEM PiM系统作为研究对象，实现了多层感知机（MLPs）的PiM版本，并与Intel Xeon CPU和低功耗Nvidia Jetson GPU进行比较。

Result: UPMEM PiM在大型批量推理中比CPU快259倍；使用UPMEM的WRAM时，MLP推理的核执行时间低于3毫秒，与低功耗GPU处于同一数量级。

Conclusion: PiM架构在神经网络推理中表现出显著的性能优势，尤其是在数据移动密集型任务中，展示了其潜力。

Abstract: In modern computer architectures, the performance of many memory-bound
workloads (e.g., machine learning, graph processing, databases) is limited by
the data movement bottleneck that emerges when transferring large amounts of
data between the main memory and the central processing unit (CPU).
Processing-in-memory is an emerging computing paradigm that aims to alleviate
this data movement bottleneck by performing computation close to or within the
memory units, where data resides. One example of a prevalent workload whose
performance is bound by the data movement bottleneck is the training and
inference process of artificial neural networks. In this work, we analyze the
potential of modern general-purpose PiM architectures to accelerate neural
networks. To this end, we selected the UPMEM PiM system, the first commercially
available real-world general-purpose PiM architecture. We compared the
implementation of multilayer perceptrons (MLPs) in PiM with a sequential
baseline running on an Intel Xeon CPU. The UPMEM implementation achieves up to
$259\times$ better performance for inference of large batch sizes when compared
against the CPU that exploits the size of the available PiM memory.
Additionally, two smaller MLPs were implemented using UPMEM's working SRAM
(WRAM), a scratchpad memory, to evaluate their performance against a low-power
Nvidia Jetson graphics processing unit (GPU), providing further insights into
the efficiency of UPMEM's PiM for neural network inference. Results show that
using WRAM achieves kernel execution times for MLP inference of under $3$ ms,
which is within the same order of magnitude as low-power GPUs.

</details>


### [106] [On the Efficiency of Dynamic Transaction Scheduling in Blockchain Sharding](https://arxiv.org/abs/2508.07472)
*Ramesh Adhikari,Costas Busch,Miroslav Popovic*

Main category: cs.DC

TL;DR: 研究了区块链分片系统中的动态调度问题，提出了两种模型（stateless和stateful），分别证明了延迟的竞争比，并指出某些条件下近似最优调度是NP难的。


<details>
  <summary>Details</summary>
Motivation: 区块链分片技术通过并行处理交易提升性能，但动态调度问题缺乏理论分析，本研究填补了这一空白。

Method: 采用分片图模型，分析两种调度模型（stateless和stateful），分别设计算法并证明其竞争比和计算复杂度。

Result: stateless模型延迟竞争比为$O(d \log^2 s \cdot \min\{k, \sqrt{s}\})$，stateful模型为$O(\log s\cdot \min\{k, \sqrt{s}\}+\log^2 s)$，且近似最优调度是NP难的。

Conclusion: 本研究首次为区块链分片系统提供了可证明高效的动态调度算法，理论结果接近最优。

Abstract: Sharding is a technique to speed up transaction processing in blockchains,
where the $n$ processing nodes in the blockchain are divided into $s$ disjoint
groups (shards) that can process transactions in parallel. We study dynamic
scheduling problems on a shard graph $G_s$ where transactions arrive online
over time and are not known in advance. Each transaction may access at most $k$
shards, and we denote by $d$ the worst distance between a transaction and its
accessing (destination) shards (the parameter $d$ is unknown to the shards). To
handle different values of $d$, we assume a locality sensitive decomposition of
$G_s$ into clusters of shards, where every cluster has a leader shard that
schedules transactions for the cluster. We first examine the simpler case of
the stateless model, where leaders are not aware of the current state of the
transaction accounts, and we prove a $O(d \log^2 s \cdot \min\{k, \sqrt{s}\})$
competitive ratio for latency. We then consider the stateful model, where
leader shards gather the current state of accounts, and we prove a $O(\log
s\cdot \min\{k, \sqrt{s}\}+\log^2 s)$ competitive ratio for latency. Each
leader calculates the schedule in polynomial time for each transaction that it
processes. We show that for any $\epsilon > 0$, approximating the optimal
schedule within a $(\min\{k, \sqrt{s}\})^{1 -\epsilon}$ factor is NP-hard.
Hence, our bound for the stateful model is within a poly-log factor from the
best possibly achievable. To the best of our knowledge, this is the first work
to establish provably efficient dynamic scheduling algorithms for blockchain
sharding systems.

</details>


### [107] [Coordinated Power Management on Heterogeneous Systems](https://arxiv.org/abs/2508.07605)
*Zhong Zheng,Michael E. Papka,Zhiling Lan*

Main category: cs.DC

TL;DR: OPEN是一个用于异构计算系统中性能预测的框架，结合离线与在线阶段，降低分析成本并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统性能建模方法依赖昂贵的离线分析，不适用于大规模应用。OPEN旨在解决这一问题。

Method: OPEN包括离线阶段（构建性能预测器和初始矩阵）和在线阶段（轻量级分析和协作过滤）。

Result: 在多种异构系统上测试，OPEN的预测精度高达98.29%。

Conclusion: OPEN为功耗感知计算提供了轻量级解决方案，支持更优的运行时决策。

Abstract: Performance prediction is essential for energy-efficient computing in
heterogeneous computing systems that integrate CPUs and GPUs. However,
traditional performance modeling methods often rely on exhaustive offline
profiling, which becomes impractical due to the large setting space and the
high cost of profiling large-scale applications. In this paper, we present
OPEN, a framework consists of offline and online phases. The offline phase
involves building a performance predictor and constructing an initial dense
matrix. In the online phase, OPEN performs lightweight online profiling, and
leverages the performance predictor with collaborative filtering to make
performance prediction. We evaluate OPEN on multiple heterogeneous systems,
including those equipped with A100 and A30 GPUs. Results show that OPEN
achieves prediction accuracy up to 98.29\%. This demonstrates that OPEN
effectively reduces profiling cost while maintaining high accuracy, making it
practical for power-aware performance modeling in modern HPC environments.
Overall, OPEN provides a lightweight solution for performance prediction under
power constraints, enabling better runtime decisions in power-aware computing
environments.

</details>


### [108] [Perpetual exploration in anonymous synchronous networks with a Byzantine black hole](https://arxiv.org/abs/2508.07703)
*Adri Bhattacharya,Pritam Goswami,Evangelos Bampas,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 研究在未知图中，一组初始共位的移动代理如何在存在一个可能恶意的静止节点（称为『拜占庭黑洞』）的情况下持续探索图。定义了两种探索变体，并确定了在树形和一般图中解决问题所需的最小代理数量。


<details>
  <summary>Details</summary>
Motivation: 探索在恶意节点存在下的持续图探索问题，填补了缺乏初始拓扑知识的任意网络中黑洞变体研究的空白。

Method: 定义了两种探索变体（PerpExpl和PerpExplHome），研究了在同步调度器和面对面通信模型下的代理数量需求。

Result: 在树形网络中，最优代理数为4（PerpExpl）和6（PerpExplHome）；在一般图中，下界为2Δ-1，上界为3Δ+3。

Conclusion: 解决了初始拓扑知识缺失情况下拜占庭黑洞图的持续探索问题，并提供了具体的代理数量界限。

Abstract: In this paper, we investigate: ``How can a group of initially co-located
mobile agents perpetually explore an unknown graph, when one stationary node
occasionally behaves maliciously, under an adversary's control?'' We call this
node a ``Byzantine black hole (BBH)'' and at any given round it may choose to
destroy all visiting agents, or none. This subtle power can drastically
undermine classical exploration strategies designed for an always active black
hole. We study this perpetual exploration problem in the presence of at most
one BBH, without initial knowledge of the network size. Since the underlying
graph may be 1-connected, perpetual exploration of the entire graph may be
infeasible. We thus define two variants: \pbmPerpExpl\ and \pbmPerpExplHome. In
the former, the agents are tasked to perform perpetual exploration of at least
one component, obtained after the exclusion of the BBH. In the latter, the
agents are tasked to perform perpetual exploration of the component which
contains the \emph{home} node, where agents are initially co-located.
Naturally, \pbmPerpExplHome\ is a special case of \pbmPerpExpl. Agents operate
under a synchronous scheduler and communicate in a face-to-face model. Our goal
is to determine the minimum number of agents necessary and sufficient to solve
these problems. In acyclic networks, we obtain optimal algorithms that solve
\pbmPerpExpl\ with $4$ agents, and \pbmPerpExplHome\ with $6$ agents in trees.
The lower bounds hold even in path graphs. In general graphs, we give a
non-trivial lower bound of $2\Delta-1$ agents for \pbmPerpExpl, and an upper
bound of $3\Delta+3$ agents for \pbmPerpExplHome. To our knowledge, this is the
first study of a black-hole variant in arbitrary networks without initial
topological knowledge.

</details>


### [109] [Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure](https://arxiv.org/abs/2508.07744)
*Ingo Friese,Jochen Klaffer,Mandy Galkow-Schneider,Sergiy Melnyk,Qiuheng Zhou,Hans Dieter Schotten*

Main category: cs.DC

TL;DR: 本文探讨了6G网络中通过引入资源分配中间代理（broker）来简化服务部署的潜力，尤其是在分片计算场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 6G网络将带来创新的服务和能力，但需要统一的接口和资源共享机制。因此，研究如何抽象化基础设施复杂性以实现无缝资源访问成为关键。

Method: 提出了一种中间代理（broker）架构，用于抽象化不同基础设施的复杂性，并通过概念验证展示其实用性。

Result: 研究表明，中间代理在分片计算等场景中具有广泛的适用性，能够有效整合未来网络和云基础设施。

Conclusion: 中间代理是6G网络中实现资源共享和服务部署简化的有效解决方案。

Abstract: 6G network architectures will usher in a wave of innovative services and
capabilities, introducing concepts like split computing and dynamic processing
nodes. This implicates a paradigm where accessing resources seamlessly aligns
with diverse processing node characteristics, ensuring a uniform interface. In
this landscape, the identity of the operator becomes inconsequential, paving
the way for a collaborative ecosystem where multiple providers contribute to a
shared pool of resources. At the core of this vision is the guarantee of
specific performance parameters, precisely tailored to the location and service
requirements. A consistent layer, as the abstraction of the complexities of
different infrastructure providers, is needed to simplify service deployment.
One promising approach is the introduction of an over-the-top broker for
resource allocation, which streamlines the integration of these services into
the network and cloud infrastructure of the future. This paper explores the
role of the broker in two split computing scenarios. By abstracting the
complexities of various infrastructures, the broker proves to be a versatile
solution applicable not only to cloud environments but also to networks and
beyond. Additionally, a detailed discussion of a proof-of-concept
implementation provides insights into the broker's actual architectural
framework.

</details>


### [110] [Towards Lock Modularization for Heterogeneous Environments](https://arxiv.org/abs/2508.07756)
*Hanze Zhang,Rong Chen,Haibo Chen*

Main category: cs.DC

TL;DR: 提出了一种新的锁设计原则，通过将锁模块化并分配到异构硬件的不同组件上，以充分利用资源并提升性能。


<details>
  <summary>Details</summary>
Motivation: 异构硬件环境中资源分布不均，传统锁设计无法充分利用资源，导致性能瓶颈。

Method: 提出锁模块化方法，将锁分解为独立模块并分配到适合的硬件组件上。

Result: 该方法能更好地匹配硬件特性，提升性能。

Conclusion: 锁模块化是解决异构硬件环境中锁性能问题的有效方法。

Abstract: Modern hardware environments are becoming increasingly heterogeneous, leading
to the emergence of applications specifically designed to exploit this
heterogeneity. Efficiently adopting locks in these applications poses distinct
challenges. The uneven distribution of resources in such environments can
create bottlenecks for lock operations, severely hindering application
performance. Existing solutions are often tailored to specific types of
hardware, which underutilizes resources on other components within
heterogeneous environments.
  This paper introduces a new design principle: decomposing locks across
hardware components to fully utilize unevenly distributed resources in
heterogeneous environments. Following this principle, we propose lock
modularization, a systematic approach that decomposes a lock into independent
modules and assigns them to appropriate hardware components. This approach
aligns the resource requirements of lock modules with the attributes of
specific hardware components, maximizing strengths while minimizing weaknesses.

</details>


### [111] [Performance Evaluation of Brokerless Messaging Libraries](https://arxiv.org/abs/2508.07934)
*Lorenzo La Corte,Syed Aftab Rashid,Andrei-Marian Dan*

Main category: cs.DC

TL;DR: 本文分析了无代理消息系统的性能，并开发了一个开源基准测试套件来评估ZeroMQ、NanoMsg和NNG等库的表现，为实践者提供选择依据。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注有代理消息系统的性能，而缺乏对无代理系统的系统评估。本文旨在填补这一空白，评估无代理消息系统的性能。

Method: 首先通过定性分析筛选出有前景的无代理消息系统库，然后设计并实现开源基准测试套件，系统地评估ZeroMQ、NanoMsg和NNG在不同指标和工作负载条件下的性能。

Result: 研究提供了对这些库性能的详细评估，并揭示了它们的局限性，帮助实践者根据需求选择合适的库。

Conclusion: 本文为无代理消息系统的性能评估提供了系统的工具和见解，填补了现有研究的不足。

Abstract: Messaging systems are essential for efficiently transferring large volumes of
data, ensuring rapid response times and high-throughput communication. The
state-of-the-art on messaging systems mainly focuses on the performance
evaluation of brokered messaging systems, which use an intermediate broker to
guarantee reliability and quality of service. However, over the past decade,
brokerless messaging systems have emerged, eliminating the single point of
failure and trading off reliability guarantees for higher performance. Still,
the state-of-the-art on evaluating the performance of brokerless systems is
scarce. In this work, we solely focus on brokerless messaging systems. First,
we perform a qualitative analysis of several possible candidates, to find the
most promising ones. We then design and implement an extensive open-source
benchmarking suite to systematically and fairly evaluate the performance of the
chosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG).
We evaluate these libraries considering different metrics and workload
conditions, and provide useful insights into their limitations. Our analysis
enables practitioners to select the most suitable library for their
requirements.

</details>


### [112] [Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids](https://arxiv.org/abs/2508.08022)
*Roopkatha Banerjee,Sampath Koti,Gyanendra Singh,Anirban Chakraborty,Gurunath Gurrala,Bhushan Jagyasi,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 论文探讨了通过物联网（IoT）和联邦学习（FL）实时监测城市和微电网的电力消耗，提出优化FL训练的方法以提高预测准确性并降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 通过IoT实时监测电力消耗需要保护用户隐私，FL提供了一种隐私敏感的分布式学习方法，但面临非独立同分布数据和计算成本高的挑战。

Method: 提出并评估了几种FL训练的优化方法，包括使用指数加权损失函数，并在大规模边缘设备和云端进行实验验证。

Result: 实验结果表明，优化的FL方法在预测准确性上优于传统方法（如ARIMA和单个消费者的DNN），且具有更好的扩展性。

Conclusion: 研究表明，优化的FL方法能够有效解决电力需求预测中的隐私和计算效率问题，适用于大规模场景。

Abstract: Real-time monitoring of power consumption in cities and micro-grids through
the Internet of Things (IoT) can help forecast future demand and optimize grid
operations. But moving all consumer-level usage data to the cloud for
predictions and analysis at fine time scales can expose activity patterns.
Federated Learning~(FL) is a privacy-sensitive collaborative DNN training
approach that retains data on edge devices, trains the models on private data
locally, and aggregates the local models in the cloud. But key challenges
exist: (i) clients can have non-independently identically distributed~(non-IID)
data, and (ii) the learning should be computationally cheap while scaling to
1000s of (unseen) clients. In this paper, we develop and evaluate several
optimizations to FL training across edge and cloud for time-series demand
forecasting in micro-grids and city-scale utilities using DNNs to achieve a
high prediction accuracy while minimizing the training cost. We showcase the
benefit of using exponentially weighted loss while training and show that it
further improves the prediction of the final model. Finally, we evaluate these
strategies by validating over 1000s of clients for three states in the US from
the OpenEIA corpus, and performing FL both in a pseudo-distributed setting and
a Pi edge cluster. The results highlight the benefits of the proposed methods
over baselines like ARIMA and DNNs trained for individual consumers, which are
not scalable.

</details>


### [113] [On the Operational Resilience of CBDC: Threats and Prospects of Formal Validation for Offline Payments](https://arxiv.org/abs/2508.08064)
*Marco Bernardo,Federico Calandra,Andrea Esposito,Francesco Fabris*

Main category: cs.DC

TL;DR: 现代计算机技术虽然强大，但在保障软件质量方面存在理论上的不可能性。在数字货币等金融技术中，小漏洞可能导致严重后果，建议采用形式化方法验证其正确性。


<details>
  <summary>Details</summary>
Motivation: 随着数字货币和资产代币化技术的兴起，即使是小规模的软件缺陷也可能引发金融崩溃，因此需要确保相关软件基础设施的操作弹性。

Method: 采用形式化方法（formal methods）来验证中央银行数字货币（CBDCs）软件的正确性，特别是离线支付等关键环节。

Result: 形式化方法可以提供计算系统正确性的断言，有助于增强CBDCs软件的操作弹性。

Conclusion: 在无法完全避免软件缺陷的情况下，形式化方法是验证CBDCs软件基础设施可靠性的重要工具。

Abstract: Information and communication technologies are by now employed in most
activities, including economics and finance. Despite the extraordinary power of
modern computers and the vast amount of memory, some results of theoretical
computer science imply the impossibility of certifying software quality in
general. With the exception of safety-critical systems, this has primarily
concerned the information processed by confined systems, with limited
socio-economic consequences. In the emerging era of technologies for exchanging
digital money and tokenized assets over the Internet - such as central bank
digital currencies (CBDCs) - even a minor bug could trigger a financial
collapse. Although the aforementioned impossibility results cannot be overcome
in an absolute sense, there exist formal methods that can provide assertions of
computing systems correctness. We advocate their use to validate the
operational resilience of software infrastructures enabling CBDCs, with special
emphasis on offline payments as they constitute a very critical issue.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [114] [Omni Geometry Representation Learning vs Large Language Models for Geospatial Entity Resolution](https://arxiv.org/abs/2508.06584)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.DB

TL;DR: 论文提出了一种名为Omni的地理空间实体解析模型，解决了异构几何体嵌入神经网络的问题，并在多样几何体数据集上测试表现优异。同时探索了大型语言模型（LLMs）在地理空间实体解析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间实体解析方法在处理多样几何体时存在信息损失问题，需要一种统一的技术有效嵌入异构几何体。

Method: 提出Omni模型，采用全几何编码器处理点、线、多边形等几何体，并结合预训练语言模型处理文本属性。还测试了LLMs的表现。

Result: Omni在点数据集和新多样几何体数据集上F1提升达12%。LLMs也显示出竞争力。

Conclusion: Omni模型显著提升了地理空间实体解析的性能，同时LLMs展现了潜在的应用前景。

Abstract: The development, integration, and maintenance of geospatial databases rely
heavily on efficient and accurate matching procedures of Geospatial Entity
Resolution (ER). While resolution of points-of-interest (POIs) has been widely
addressed, resolution of entities with diverse geometries has been largely
overlooked. This is partly due to the lack of a uniform technique for embedding
heterogeneous geometries seamlessly into a neural network framework. Existing
neural approaches simplify complex geometries to a single point, resulting in
significant loss of spatial information. To address this limitation, we propose
Omni, a geospatial ER model featuring an omni-geometry encoder. This encoder is
capable of embedding point, line, polyline, polygon, and multi-polygon
geometries, enabling the model to capture the complex geospatial intricacies of
the places being compared. Furthermore, Omni leverages transformer-based
pre-trained language models over individual textual attributes of place records
in an Attribute Affinity mechanism. The model is rigorously tested on existing
point-only datasets and a new diverse-geometry geospatial ER dataset. Omni
produces up to 12% (F1) improvement over existing methods.
  Furthermore, we test the potential of Large Language Models (LLMs) to conduct
geospatial ER, experimenting with prompting strategies and learning scenarios,
comparing the results of pre-trained language model-based methods with LLMs.
Results indicate that LLMs show competitive results.

</details>


### [115] [Metadata Management for AI-Augmented Data Workflows](https://arxiv.org/abs/2508.06814)
*Jinjin Zhao,Sanjay Krishnan*

Main category: cs.DB

TL;DR: TableVault是一个用于人机协作数据创建的元数据治理框架，解决AI增强数据工作流中的治理难题。


<details>
  <summary>Details</summary>
Motivation: AI增强的数据工作流涉及复杂的人机交互和动态执行模式，导致元数据捕获困难，需要透明和可复现的治理框架。

Method: TableVault通过记录事件、追踪操作状态、链接执行参数与数据来源，并提供标准化元数据层，结合数据库保证和AI导向设计。

Result: 在文档分类案例中，TableVault成功保留了详细的谱系和操作上下文，实现稳健的元数据管理。

Conclusion: TableVault为混合人机数据工作流提供了透明、可复现的元数据治理方案。

Abstract: AI-augmented data workflows introduce complex governance challenges, as both
human and model-driven processes generate, transform, and consume data
artifacts. These workflows blend heterogeneous tools, dynamic execution
patterns, and opaque model decisions, making comprehensive metadata capture
difficult. In this work, we present TableVault, a metadata governance framework
designed for human-AI collaborative data creation. TableVault records ingestion
events, traces operation status, links execution parameters to their data
origins, and exposes a standardized metadata layer. By combining
database-inspired guarantees with AI-oriented design, such as declarative
operation builders and lineage-aware references, TableVault supports
transparency and reproducibility across mixed human-model pipelines. Through a
document classification case study, we demonstrate how TableVault preserves
detailed lineage and operational context, enabling robust metadata management,
even in partially observable execution environments.

</details>


### [116] [Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption](https://arxiv.org/abs/2508.07044)
*William Zerong Wang,Dongfang Zhao*

Main category: cs.DB

TL;DR: 论文提出了一种基于加法同态加密（AHE）的音乐嵌入向量隐私保护方法，解决了传统加密和全同态加密（FHE）在大规模音乐数据相似性搜索中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 在生成式AI时代，音乐数据的隐私保护面临独特挑战，传统方法无法有效保护抽象的音乐向量表示，需要更强加密手段。

Method: 提出了一种基于AHE的隐私保护相似性搜索方法，通过音乐嵌入向量的内积实现高效加密计算。

Result: 理论分析和实证评估表明，该方法在效率和实用性上优于FHE方案，适用于真实世界MP3文件。

Conclusion: AHE为大规模音乐数据隐私保护提供了一种高效的解决方案。

Abstract: In the era of generative AI, ensuring the privacy of music data presents
unique challenges: unlike static artworks such as images, music data is
inherently temporal and multimodal, and it is sampled, transformed, and remixed
at an unprecedented scale. These characteristics make its core vector
embeddings, i.e, the numerical representations of the music, highly susceptible
to being learned, misused, or even stolen by models without accessing the
original audio files. Traditional methods like copyright licensing and digital
watermarking offer limited protection for these abstract mathematical
representations, thus necessitating a stronger, e.g., cryptographic, approach
to safeguarding the embeddings themselves. Standard encryption schemes, such as
AES, render data unintelligible for computation, making such searches
impossible. While Fully Homomorphic Encryption (FHE) provides a plausible
solution by allowing arbitrary computations on ciphertexts, its substantial
performance overhead remains impractical for large-scale vector similarity
searches. Given this trade-off, we propose a more practical approach using
Additive Homomorphic Encryption (AHE) for vector similarity search. The primary
contributions of this paper are threefold: we analyze threat models unique to
music information retrieval systems; we provide a theoretical analysis and
propose an efficient AHE-based solution through inner products of music
embeddings to deliver privacy-preserving similarity search; and finally, we
demonstrate the efficiency and practicality of the proposed approach through
empirical evaluation and comparison to FHE schemes on real-world MP3 files.

</details>


### [117] [SQL-Exchange: Transforming SQL Queries Across Domains](https://arxiv.org/abs/2508.07087)
*Mohammadreza Daviran,Brian Lin,Davood Rafiei*

Main category: cs.DB

TL;DR: SQL-Exchange 是一个框架，用于在不同数据库模式间映射 SQL 查询，通过保留源查询结构并适应目标模式来提升文本到 SQL 系统的性能。


<details>
  <summary>Details</summary>
Motivation: 研究在不同数据库模式间映射 SQL 查询的可行性和益处，以提升下游任务的上下文学习性能。

Method: 通过 SQL-Exchange 框架映射 SQL 查询，保留源查询结构并适应目标模式，评估结构对齐、执行有效性和语义正确性。

Result: SQL-Exchange 在多种模式和查询类型中表现有效，映射查询作为上下文示例能持续提升文本到 SQL 的性能。

Conclusion: SQL-Exchange 提供了一种有效的方法来优化跨模式 SQL 查询映射，从而增强文本到 SQL 系统的性能。

Abstract: We introduce SQL-Exchange, a framework for mapping SQL queries across
different database schemas by preserving the source query structure while
adapting domain-specific elements to align with the target schema. We
investigate the conditions under which such mappings are feasible and
beneficial, and examine their impact on enhancing the in-context learning
performance of text-to-SQL systems as a downstream task. Our comprehensive
evaluation across multiple model families and benchmark datasets--assessing
structural alignment with source queries, execution validity on target
databases, and semantic correctness--demonstrates that SQL-Exchange is
effective across a wide range of schemas and query types. Our results further
show that using mapped queries as in-context examples consistently improves
text-to-SQL performance over using queries from the source schema.

</details>


### [118] [Accelerating High-Dimensional Nearest Neighbor Search with Dynamic Query Preference](https://arxiv.org/abs/2508.07218)
*Yunjun Gao,Ruijie Zhao,Zhonggen Li,Baihua Zheng,Yifan Zhu,Zhaoqing Chen*

Main category: cs.DB

TL;DR: 提出了一种基于双索引查询框架（DQF）的近似最近邻搜索方法，通过动态搜索策略和双层索引结构优化查询性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的ANNS方法（如HNSW和NSG）假设查询分布均匀，但实际中查询频率存在动态变化，需要更高效的框架。

Method: 设计了双层索引结构（热索引和全索引）和基于决策树的动态搜索策略，分别管理高频和低频查询。

Result: 在四个真实数据集上实现了2.0-5.7倍的加速，且保持95%召回率，无需随查询分布变化重建索引。

Conclusion: DQF在动态查询分布场景下高效且实用，显著优于现有方法。

Abstract: Approximate Nearest Neighbor Search (ANNS) is a crucial operation in
databases and artificial intelligence. Current graph-based ANNS methods, such
as HNSW and NSG, have shown remarkable performance but are designed under the
assumption of a uniform query distribution. However, in practical scenarios,
user preferences and query temporal dynamics lead to some queries being
searched for more frequently than others. To fully utilize these
characteristics, we propose DQF, a novel Dual-Index Query Framework. This
framework comprises a dual-layer index structure and a dynamic search strategy
based on a decision tree. The dual-layer index structure comprises a hot index
for high-frequency nodes and a full index for the entire dataset, allowing for
the separate management of hot and cold queries. Furthermore, we propose a
dynamic search strategy that employs a decision tree to adapt to the specific
characteristics of each query. The decision tree evaluates whether a query is
of the high-frequency type to detect the opportunities for early termination on
the dual-layer, avoiding unnecessary searches in the full index. Experimental
results on four real-world datasets demonstrate that the Dual-Index Query
Framework achieves a significant speedup of 2.0-5.7x over state-of-the-art
algorithms while maintaining a 95% recall rate. Importantly, it does not
require full index reconstruction when query distributions change, underscoring
its efficiency and practicality in dynamic query distribution scenarios.

</details>


### [119] [RNA-KG v2.0: An RNA-centered Knowledge Graph with Properties](https://arxiv.org/abs/2508.07427)
*Emanuele Cavalleri,Paolo Perlasca,Marco Mesiti*

Main category: cs.DB

TL;DR: RNA-KG v2.0是一个升级版的知识图谱，整合了约1亿条手动整理的RNA分子交互数据，支持分类、链接预测和模式发现。


<details>
  <summary>Details</summary>
Motivation: 为了提供更全面的RNA分子交互数据并支持高级查询和下游应用，如上下文感知的链接预测。

Method: 整合了来自91个开放数据仓库和本体的100M手动整理交互数据，并使用标准化属性描述关系上下文和节点属性。

Result: 增强了知识图谱的查询能力，支持更复杂的上下文感知分析。

Conclusion: RNA-KG v2.0为RNA研究提供了更丰富的数据支持和分析工具。

Abstract: RNA-KG is a recently developed knowledge graph that integrates the
interactions involving coding and non-coding RNA molecules extracted from
public data sources. It can be used to support the classification of new
molecules, identify new interactions through the use of link prediction
methods, and reveal hidden patterns among the represented entities. In this
paper, we propose RNA-KG v2.0, a new release of RNA-KG that integrates around
100M manually curated interactions sourced from 91 linked open data
repositories and ontologies. Relationships are characterized by standardized
properties that capture the specific context (e.g., cell line, tissue,
pathological state) in which they have been identified. In addition, the nodes
are enriched with detailed attributes, such as descriptions, synonyms, and
molecular sequences sourced from platforms such as OBO ontologies, NCBI
repositories, RNAcentral, and Ensembl. The enhanced repository enables the
expression of advanced queries that take into account the context in which the
experiments were conducted. It also supports downstream applications in RNA
research, including "context-aware" link prediction techniques that combine
both topological and semantic information.

</details>


### [120] [A Benchmark for Databases with Varying Value Lengths](https://arxiv.org/abs/2508.07551)
*Danushka Liyanage,Shubham Pandey,Joshua Goldstein,Michael Cahill,Akon Dey,Alan Fekete,Uwe Röhm*

Main category: cs.DB

TL;DR: 论文提出了一种扩展YCSB基准测试的新方法，通过增加'extend'操作来模拟现实中数据记录动态增长的情况，揭示了不同数据库管理系统在处理变长数据时的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有的数据库基准测试未能充分反映现实中数据记录长度动态变化的工作负载，导致性能异常未被充分研究。

Method: 扩展YCSB基准测试以包含'extend'操作，模拟数据记录的增长，并在MongoDB和MariaDB（InnoDB和MyRocks存储引擎）上进行实验。

Result: 实验揭示了存储引擎设计及其对变长数据处理的性能差异，强调了更具代表性的基准测试的重要性。

Conclusion: 新方法为实践者和研究者提供了有价值的指导，凸显了捕捉动态工作负载的基准测试的需求。

Abstract: The performance of database management systems (DBMS) is traditionally
evaluated using benchmarks that focus on workloads with (almost) fixed record
lengths. However, some real-world workloads in key/value stores, document
databases, and graph databases exhibit significant variability in value
lengths, which can lead to performance anomalies, particularly when popular
records grow disproportionately large. Existing benchmarks fail to account for
this variability, leaving an important aspect of DBMS behavior underexplored.
  In this paper, we address this gap by extending the Yahoo! Cloud Serving
Benchmark (YCSB) to include an "extend" operation, which appends data to record
fields, simulating the growth of values over time. Using this modified
benchmark, we have measured the performance of three popular DBMS backends:
MongoDB, MariaDB with the InnoDB storage engine, and MariaDB with the MyRocks
storage engine. Our experiments alternate between extending values and
executing query workloads, revealing significant performance differences driven
by storage engine design and their handling of variable-sized values.
  Our key contribution is the introduction of a novel benchmarking approach to
evaluate the impact of growing value sizes and isolate the effect of querying
data with a distribution of data sizes from any cost associated with accessing
data after a history of updates. This highlights the need for more
representative benchmarks that capture the dynamic nature of real-world
workloads, providing valuable guidance for both practitioners and researchers.

</details>


### [121] [MLego: Interactive and Scalable Topic Exploration Through Model Reuse](https://arxiv.org/abs/2508.07654)
*Fei Ye,Jiapan Liu,Yinan Jing,Zhenying He,Weirao Wang,X. Sean Wang*

Main category: cs.DB

TL;DR: MLego是一个实时主题建模框架，通过模型物化和重用支持交互式查询，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统主题建模技术（如LDA）计算成本高，不适合实时分析。尽管分布式训练和快速采样方法有所改进，但实时主题探索仍是挑战。

Method: MLego利用模型物化和重用的方法，高效合并已物化的主题模型以近似结果。还引入了分层计划搜索和查询重排序技术。

Result: 实验表明，MLego显著降低了计算成本，同时保持高质量的主题建模结果。

Conclusion: MLego填补了可扩展主题建模与交互式分析之间的空白，扩展了现有可视化分析方法。

Abstract: With massive texts on social media, users and analysts often rely on topic
modeling techniques to quickly extract key themes and gain insights.
Traditional topic modeling techniques, such as Latent Dirichlet Allocation
(LDA), provide valuable insights but are computationally expensive, making them
impractical for real-time data analysis. Although recent advances in
distributed training and fast sampling methods have improved efficiency,
real-time topic exploration remains a significant challenge. In this paper, we
present MLego, an interactive query framework designed to support real-time
topic modeling analysis by leveraging model materialization and reuse. Instead
of retraining models from scratch, MLego efficiently merges materialized topic
models to construct approximate results at interactive speeds. To further
enhance efficiency, we introduce a hierarchical plan search strategy for single
queries and an optimized query reordering technique for batch queries. We
integrate MLego into a visual analytics prototype system, enabling users to
explore large-scale textual datasets through interactive queries. Extensive
experiments demonstrate that MLego significantly reduces computation costs
while maintaining high-quality topic modeling results. MLego enhances existing
visual analytics approaches, which primarily focus on user-driven topic
modeling, by enabling real-time, query-driven exploration. This complements
traditional methods and bridges the gap between scalable topic modeling and
interactive data analysis.

</details>


### [122] [TQL: Towards Type-Driven Data Discovery](https://arxiv.org/abs/2508.08054)
*Andrew Kang,Sainyam Galhotra*

Main category: cs.DB

TL;DR: 论文摘要介绍了一种名为TQL的新型查询语言，其设计以用户需求为核心，而非传统系统功能驱动的设计。


<details>
  <summary>Details</summary>
Motivation: 现有的数据发现查询语言多为系统驱动设计，忽视了用户需求，因此需要一种更灵活、实用的替代方案。

Method: TQL引入了一种语言驱动的方法，采用了类似类型系统的机制，并结合了编程语言研究的成果。论文详细定义了TQL的语法、语义和评估模型，并提供了实现框架。

Result: TQL在表达能力和实用性上优于现有数据检索和发现语言，适用于实际场景。

Conclusion: TQL通过语言驱动设计提升了数据发现系统的灵活性和用户友好性，为未来研究提供了新方向。

Abstract: Existing query languages for data discovery exhibit system-driven designs
that emphasize database features and functionality over user needs. We propose
a re-prioritization of the client through an introduction of a language-driven
approach to data discovery systems that can leverage powerful results from
programming languages research. In this paper, we describe TQL, a flexible and
practical query language which incorporates a type-like system to encompass
downstream transformation-context in its discovery queries. The syntax and
semantics of TQL (including the underlying evaluation model), are formally
defined, and a sketch of its implementation is also provided. Additionally, we
provide comparisons to existing languages for data retrieval and data discovery
to examine the advantages of TQL's expanded expressive power in real-life
settings.

</details>


### [123] [Towards General-Purpose Data Discovery: A Programming Languages Approach](https://arxiv.org/abs/2508.08074)
*Andrew Kang,Yashnil Saha,Sainyam Galhotra*

Main category: cs.DB

TL;DR: 论文提出了一种名为TQL的领域特定语言，用于高效数据发现，并基于ImpRAT代数模型进行形式化描述。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习和数据科学应用需要高效的数据发现工具，但缺乏通用的形式化语言阻碍了这一发展。

Method: 设计了TQL语言，并结合编程语言研究的成果，提出ImpRAT代数模型进行形式化描述。

Result: 实现了模块化的概念验证系统原型，展示了TQL的可行性。

Conclusion: TQL和ImpRAT为通用数据发现工具的开发提供了理论基础和实践支持。

Abstract: Efficient and effective data discovery is critical for many modern
applications in machine learning and data science. One major bottleneck to the
development of a general-purpose data discovery tool is the absence of an
expressive formal language, and corresponding implementation, for
characterizing and solving generic discovery queries. To this end, we present
TQL, a domain-specific language for data discovery well-designed to leverage
and exploit the results of programming languages research in both its syntax
and semantics. In this paper, we fully and formally characterize the core
language through an algebraic model, Imperative Relational Algebra with Types
(ImpRAT), and implement a modular proof-of-concept system prototype.

</details>


### [124] [Heterogeneity in Entity Matching: A Survey and Experimental Analysis](https://arxiv.org/abs/2508.08076)
*Mohammad Hossein Moslemi,Amir Mousavi,Behshid Behkamal,Mostafa Milani*

Main category: cs.DB

TL;DR: 实体匹配（EM）是数据集成和分析中的基础任务，但在异构数据（HEM）环境下存在挑战。本文通过分类法系统分析了HEM的两大类别——表示和语义异质性，并连接FAIR原则提出应对策略。同时，评估了现有方法的局限性并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 异构数据集在结构、格式、模式和语义上的巨大差异给实体匹配（EM）带来了显著挑战。本文旨在通过系统分类和策略分析，为HEM提供统一的视角，并推动未来研究的发展。

Method: 提出一种基于表示和语义异质性的分类法，将其与FAIR原则关联以揭示挑战和策略。通过实验评估最新EM方法在语义异质性下的稳健性和适应性。

Result: 分析揭示了当前方法在应对HEM时的局限性，并指出了未来的研究方向，包括多模态匹配、人机协同工作流、与大语言模型和知识图谱的深度融合，以及公平性评估。

Conclusion: 本文为异构实体匹配提供了一个系统性框架，未来研究需关注多模态整合和公平性优化等方向，以提升方法的实际适用性。

Abstract: Entity matching (EM) is a fundamental task in data integration and analytics,
essential for identifying records that refer to the same real-world entity
across diverse sources. In practice, datasets often differ widely in structure,
format, schema, and semantics, creating substantial challenges for EM. We refer
to this setting as Heterogeneous EM (HEM). This survey offers a unified
perspective on HEM by introducing a taxonomy, grounded in prior work, that
distinguishes two primary categories -- representation and semantic
heterogeneity -- and their subtypes. The taxonomy provides a systematic lens
for understanding how variations in data form and meaning shape the complexity
of matching tasks. We then connect this framework to the FAIR principles --
Findability, Accessibility, Interoperability, and Reusability -- demonstrating
how they both reveal the challenges of HEM and suggest strategies for
mitigating them. Building on this foundation, we critically review recent EM
methods, examining their ability to address different heterogeneity types, and
conduct targeted experiments on state-of-the-art models to evaluate their
robustness and adaptability under semantic heterogeneity. Our analysis uncovers
persistent limitations in current approaches and points to promising directions
for future research, including multimodal matching, human-in-the-loop
workflows, deeper integration with large language models and knowledge graphs,
and fairness-aware evaluation in heterogeneous settings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [125] [SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency](https://arxiv.org/abs/2508.06978)
*Kwanhee Kyung,Sungmin Yun,Jung Ho Ahn*

Main category: cs.AR

TL;DR: 研究分析了将MoE专家权重从HBM卸载到SSD对LLM推理解码阶段能耗的影响，发现当前SSD显著增加能耗，未来需Flash读取能耗大幅改善才能使其可行。


<details>
  <summary>Details</summary>
Motivation: MoE模型的参数规模庞大，需要从HBM卸载到SSD以节省内存，但SSD的高读取能耗问题亟待解决。

Method: 通过定量分析比较SSD、DDR和HBM在不同存储场景下的能耗，评估MoE权重卸载的影响。

Result: 当前SSD卸载会使每token生成的能耗增加高达12倍，预取技术无法解决能耗问题。未来Flash读取能耗需显著改善（约一个数量级）才能使SSD可行。

Conclusion: 虽然SSD提供低成本容量，但其高能耗限制了当前适用性，未来技术进步可能改变这一局面。

Abstract: Large Language Models (LLMs) applying Mixture-of-Experts (MoE) scale to
trillions of parameters but require vast memory, motivating a line of research
to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs.
While SSDs provide cost-effective capacity, their read energy per bit is
substantially higher than that of DRAM. This paper quantitatively analyzes the
energy implications of offloading MoE expert weights to SSDs during the
critical decode stage of LLM inference. Our analysis, comparing SSD, CPU memory
(DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that
offloading MoE weights to current SSDs drastically increases
per-token-generation energy consumption (e.g., by up to ~12x compared to the
HBM baseline), dominating the total inference energy budget. Although
techniques like prefetching effectively hide access latency, they cannot
mitigate this fundamental energy penalty. We further explore future
technological scaling, finding that the inherent sparsity of MoE models could
potentially make SSDs energy-viable if Flash read energy improves
significantly, roughly by an order of magnitude.

</details>


### [126] [Physical Design Exploration of a Wire-Friendly Domain-Specific Processor for Angstrom-Era Nodes](https://arxiv.org/abs/2508.07110)
*Lorenzo Ruotolo,Lara Orlandic,Pengbo Yu,Moritz Brunion,Daniele Jahier Pagliari,Dwaipayan Biswas,Giovanni Ansaloni,David Atienza,Julien Ryckaert,Francky Catthoor,Yukai Chen*

Main category: cs.AR

TL;DR: 论文探讨了一种针对机器学习的专用处理器架构（DSIP），通过优化的物理设计解决了先进技术节点中的互连效率问题。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习领域，面临先进技术节点中互连效率低下的挑战，提出一种新型架构以提高性能和密度。

Method: 利用专用内存结构和SIMD单元，设计了五种配置，并在IMEC A10纳米片节点上进行合成与评估。

Result: 结果显示，新架构的归一化线长降低了2倍以上，密度提高了3倍以上，且在不同配置中表现稳定。

Conclusion: 该架构具有内在的物理效率，适用于低成本、高效的下一代DSIP设计。

Abstract: This paper presents the physical design exploration of a domain-specific
processor (DSIP) architecture targeted at machine learning (ML), addressing the
challenges of interconnect efficiency in advanced Angstrom-era technologies.
The design emphasizes reduced wire length and high core density by utilizing
specialized memory structures and SIMD (Single Instruction, Multiple Data)
units. Five configurations are synthesized and evaluated using the IMEC A10
nanosheet node PDK. Key physical design metrics are compared across
configurations and against VWR2A, a state-of-the-art (SoA) DSIP baseline.
Results show that our architecture achieves over 2x lower normalized wire
length and more than 3x higher density than the SoA, with low variability in
the metrics across all configurations, making it a promising solution for
next-generation DSIP designs. These improvements are achieved with minimal
manual layout intervention, demonstrating the architecture's intrinsic physical
efficiency and potential for low-cost wire-friendly implementation.

</details>


### [127] [LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization](https://arxiv.org/abs/2508.07227)
*Siyuan He,Zhantong Zhu,Yandong He,Tianyu Jia*

Main category: cs.AR

TL;DR: LP-Spec是一种通过架构-数据流协同设计和草稿令牌剪枝优化来加速移动设备上LLM推理的方案，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 移动设备上的LLM推理面临内存带宽和计算资源限制，现有技术如推测推理和PIM导致高能耗。

Method: 提出LP-Spec，结合LPDDR5 PIM架构、草稿令牌剪枝和动态工作负载调度。

Result: 相比其他方案，性能提升13.21倍，能效提升7.56倍，能耗延迟积降低99.87倍。

Conclusion: LP-Spec通过协同设计和优化显著提升了移动设备上LLM推理的效率和性能。

Abstract: LLM inference on mobile devices faces extraneous challenges due to limited
memory bandwidth and computational resources. To address these issues,
speculative inference and processing-in-memory (PIM) techniques have been
explored at the algorithmic and hardware levels. However, speculative inference
results in more compute-intensive GEMM operations, creating new design
trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there
exists a significant amount of redundant draft tokens in tree-based speculative
inference, necessitating efficient token management schemes to minimize energy
consumption. In this work, we present LP-Spec, an architecture-dataflow
co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with
draft token pruning and dynamic workload scheduling to accelerate LLM
speculative inference. A near-data memory controller is proposed to enable data
reallocation between DRAM and PIM banks. Furthermore, a data allocation unit
based on the hardware-aware draft token pruner is developed to minimize energy
consumption and fully exploit parallel execution opportunities. Compared to
end-to-end LLM inference on other mobile solutions such as mobile NPUs or
GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x
improvements in performance, energy efficiency, and energy-delay-product (EDP).
Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and
415.31x EDP reduction benefits.

</details>


### [128] [Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference](https://arxiv.org/abs/2508.07252)
*Siyuan He,Peiran Yan,Yandong He,Youwei Zhuo,Tianyu Jia*

Main category: cs.AR

TL;DR: 论文提出了Tasa架构，通过异构设计和热优化解决3D堆叠架构中的热问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D堆叠架构在LLM推理中面临的热问题和性能瓶颈。

Method: 提出Tasa异构架构，结合高性能核心和高效率核心，并采用带宽共享调度优化。

Result: Tasa架构在48、60和72核配置下分别降低了峰值温度，并在LLM推理中实现了显著的加速。

Conclusion: Tasa架构通过热优化和异构设计，显著提升了3D堆叠架构的可扩展性和性能。

Abstract: The autoregressive decoding in LLMs is the major inference bottleneck due to
the memory-intensive operations and limited hardware bandwidth. 3D-stacked
architecture is a promising solution with significantly improved memory
bandwidth, which vertically stacked multi DRAM dies on top of logic die.
However, our experiments also show the 3D-stacked architecture faces severer
thermal issues compared to 2D architecture, in terms of thermal temperature,
gradient and scalability. To better exploit the potential of 3D-stacked
architecture, we present Tasa, a heterogeneous architecture with cross-stack
thermal optimizations to balance the temperature distribution and maximize the
performance under the thermal constraints. High-performance core is designed
for compute-intensive operations, while high-efficiency core is used for
memory-intensive operators, e.g. attention layers. Furthermore, we propose a
bandwidth sharing scheduling to improve the bandwidth utilization in such
heterogeneous architecture. Extensive thermal experiments show that our Tasa
architecture demonstrates greater scalability compared with the homogeneous
3D-stacked architecture, i.e. up to 5.55 $\tccentigrade$, 9.37 $\tccentigrade$,
and 7.91 $\tccentigrade$ peak temperature reduction for 48, 60, and 72 core
configurations. Our experimental for Llama-65B and GPT-3 66B inferences also
demonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and
state-of-the-art heterogeneous PIM-based LLM accelerator

</details>


### [129] [The Monte Carlo Method and New Device and Architectural Techniques for Accelerating It](https://arxiv.org/abs/2508.07457)
*Janith Petangoda,Chatura Samarakoon,James Meech,Divya Thekke Kanapram,Hamid Toshani,Nathaniel Tye,Vasileios Tsoutsouras,Phillip Stanley-Marbell*

Main category: cs.AR

TL;DR: 介绍了蒙特卡罗方法及其在物理基础非均匀随机变量生成器中的两项改进，以及无需蒙特卡罗方法的新型架构技术。


<details>
  <summary>Details</summary>
Motivation: 处理现实世界中的不确定数据需要安全可靠的计算方法，传统蒙特卡罗方法存在局限性。

Method: 提出两种物理基础非均匀随机变量生成器的改进方法，并引入基于概率分布的微架构技术。

Result: 新型架构技术避免了蒙特卡罗方法的收敛性问题，可直接处理概率分布。

Conclusion: 不确定性跟踪处理器架构为处理不确定数据提供了更高效的方法。

Abstract: Computing systems interacting with real-world processes must safely and
reliably process uncertain data. The Monte Carlo method is a popular approach
for computing with such uncertain values. This article introduces a framework
for describing the Monte Carlo method and highlights two advances in the domain
of physics-based non-uniform random variate generators (PPRVGs) to overcome
common limitations of traditional Monte Carlo sampling. This article also
highlights recent advances in architectural techniques that eliminate the need
to use the Monte Carlo method by leveraging distributional microarchitectural
state to natively compute on probability distributions. Unlike Monte Carlo
methods, uncertainty-tracking processor architectures can be said to be
convergence-oblivious.

</details>


### [130] [A Matrix Decomposition Method for Odd-Type Gaussian Normal Basis Multiplication](https://arxiv.org/abs/2508.07541)
*Kittiphon Phalakarn,Athasit Surarerks*

Main category: cs.AR

TL;DR: 本文提出了一种降低二进制域GF(2^k)上奇型高斯正规基乘法器空间复杂度的方法，改进了XOR门的使用数量，但略微增加了关键路径延迟。


<details>
  <summary>Details</summary>
Motivation: 虽然正规基在许多应用中具有实现高效的优点，但现有的空间复杂度降低技术仅适用于最优正规基或偶型高斯正规基。对于187个二进制域GF(2^k)（k从2到1,000）中使用的奇型高斯正规基，缺乏有效的优化方法。

Method: 本文借鉴了最优正规基的矩阵分解方法，提出了一种适用于奇型高斯正规基的空间复杂度降低方法。

Result: 实验结果表明，与之前的工作相比，该方法显著减少了XOR门的数量，但略微增加了关键路径延迟。

Conclusion: 该方法为奇型高斯正规基乘法器的实现提供了一种有效的空间复杂度优化方案，适用于特定的二进制域。

Abstract: Normal basis is used in many applications because of the efficiency of the
implementation. However, most space complexity reduction techniques for binary
field multiplier are applicable for only optimal normal basis or Gaussian
normal basis of even type. There are 187 binary fields GF(2^k) for k from 2 to
1,000 that use odd-type Gaussian normal basis. This paper presents a method to
reduce the space complexity of odd-type Gaussian normal basis multipliers over
binary field GF(2^k). The idea is adapted from the matrix decomposition method
for optimal normal basis. The result shows that our space complexity reduction
method can reduce the number of XOR gates used in the implementation comparing
to previous works with a small trade-off in critical path delay.

</details>


### [131] [ARISE: Automating RISC-V Instruction Set Extension](https://arxiv.org/abs/2508.07725)
*Andreas Hager-Clukas,Philipp van Kempen,Stefan Wallentowitz*

Main category: cs.AR

TL;DR: ARISE工具通过自动化生成基于汇编模式的RISC-V指令，优化代码大小和指令数，提升嵌入式系统性能。


<details>
  <summary>Details</summary>
Motivation: 为减少RISC-V架构在嵌入式系统中手动优化的繁琐工作，开发了ARISE工具，以自动化方式优化指令集。

Method: 利用CoreDSL语言生成指令集扩展，并结合多种指标选择汇编模式，实现静态和动态的代码优化。

Result: 在Embench-Iot测试中，静态代码大小减少1.48%，动态代码大小减少3.84%，指令执行数减少7.39%。

Conclusion: ARISE成功自动化了RISC-V指令优化，显著提升了代码效率和性能。

Abstract: RISC-V is an extendable Instruction Set Architecture, growing in popularity
for embedded systems. However, optimizing it to specific requirements, imposes
a great deal of manual effort. To bridge the gap between software and ISA, the
tool ARISE is presented. It automates the generation of RISC-V instructions
based on assembly patterns, which are selected by an extendable set of metrics.
These metrics implement the optimization goals of code size and instruction
count reduction, both statically and dynamically. The instruction set
extensions are generated using the ISA description language CoreDSL. Allowing
seamless embedding in advanced tools such as the retargeting compiler Seal5 or
the instruction set simulator ETISS. ARISE improves the static code size by
1.48% and the dynamic code size by 3.84%, as well as the number of instructions
to be executed by 7.39% on average for Embench-Iot.

</details>


### [132] [TLV-HGNN: Thinking Like a Vertex for Memory-efficient HGNN Inference](https://arxiv.org/abs/2508.07796)
*Dengke Han,Duo Wang,Mingyu Yan,Xiaochun Ye,Dongrui Fan*

Main category: cs.AR

TL;DR: 该论文提出了一种新的异构图神经网络（HGNN）执行范式TVL-HGNN，通过消除中间存储和冗余内存访问，显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: HGNN推理中的邻居聚合阶段存在内存效率低下的问题，包括中间存储的扩展和冗余内存访问，限制了其可扩展性和性能。

Method: 提出了一种基于顶点视角的语义完整执行范式，消除了中间存储和目标顶点冗余访问，并设计了一个硬件加速器TVL-HGNN。此外，还引入了基于跨语义邻居重叠的顶点分组技术。

Result: TVL-HGNN在性能上显著优于NVIDIA A100 GPU和现有HGNN加速器HiHGNN，速度提升分别为7.85倍和1.41倍，能耗分别降低98.79%和32.61%。

Conclusion: TVL-HGNN通过优化执行范式和硬件设计，有效解决了HGNN推理中的内存效率问题，显著提升了性能和能效。

Abstract: Heterogeneous graph neural networks (HGNNs) excel at processing heterogeneous
graph data and are widely applied in critical domains. In HGNN inference, the
neighbor aggregation stage is the primary performance determinant, yet it
suffers from two major sources of memory inefficiency. First, the commonly
adopted per-semantic execution paradigm stores intermediate aggregation results
for each semantic prior to semantic fusion, causing substantial memory
expansion. Second, the aggregation process incurs extensive redundant memory
accesses, including repeated loading of target vertex features across semantics
and repeated accesses to shared neighbors due to cross-semantic neighborhood
overlap. These inefficiencies severely limit scalability and reduce HGNN
inference performance.
  In this work, we first propose a semantics-complete execution paradigm from a
vertex perspective that eliminates per-semantic intermediate storage and
redundant target vertex accesses. Building on this paradigm, we design
TVL-HGNN, a reconfigurable hardware accelerator optimized for efficient
aggregation. In addition, we introduce a vertex grouping technique based on
cross-semantic neighborhood overlap, with hardware implementation, to reduce
redundant accesses to shared neighbors. Experimental results demonstrate that
TVL-HGNN achieves average speedups of 7.85x and 1.41x over the NVIDIA A100 GPU
and the state-of-the-art HGNN accelerator HiHGNN, respectively, while reducing
energy consumption by 98.79% and 32.61%.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [133] [Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures](https://arxiv.org/abs/2508.07423)
*Fotis I. Giasemis*

Main category: hep-ex

TL;DR: 论文探讨了在粒子物理实验中如何高效部署机器学习模型，以应对高频率数据处理的需求，提出了一种基于图神经网络的管道，并在LHCb实验中进行了测试。


<details>
  <summary>Details</summary>
Motivation: 随着粒子物理实验数据量的急剧增加，传统处理方法难以满足实时过滤和高频数据处理需求，急需更高效的计算方法和硬件支持。

Method: 开发了一种基于图神经网络的管道，用于带电粒子轨迹重建，并在GPU和FPGA上实现了端到端的部署，与传统的跟踪算法进行了性能对比。

Result: 该管道在LHCb的第一级触发器中表现出色，GPU和FPGA实现均优于传统方法，特别是在处理速度和能耗方面。

Conclusion: 机器学习模型在高能物理实验中具有巨大潜力，未来可以通过硬件优化和算法改进进一步提升性能。

Abstract: As the particle physics community needs higher and higher precisions in order
to test our current model of the subatomic world, larger and larger datasets
are necessary. With upgrades scheduled for the detectors of colliding-beam
experiments around the world, and specifically at the Large Hadron Collider at
CERN, more collisions and more complex interactions are expected. This directly
implies an increase in data produced and consequently in the computational
resources needed to process them. At CERN, the amount of data produced is
gargantuan. This is why the data have to be heavily filtered and selected in
real time before being permanently stored. This data can then be used to
perform physics analyses, in order to expand our current understanding of the
universe and improve the Standard Model of physics. This real-time filtering,
known as triggering, involves complex processing happening often at frequencies
as high as 40 MHz. This thesis contributes to understanding how machine
learning models can be efficiently deployed in such environments, in order to
maximize throughput and minimize energy consumption. Inevitably, modern
hardware designed for such tasks and contemporary algorithms are needed in
order to meet the challenges posed by the stringent, high-frequency data rates.
In this work, I present our graph neural network-based pipeline, developed for
charged particle track reconstruction at the LHCb experiment at CERN. The
pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely
on GPUs. Its performance was compared against the classical tracking algorithms
currently in production at LHCb. The pipeline was also accelerated on the FPGA
architecture, and its performance in terms of power consumption and processing
speed was compared against the GPU implementation.

</details>


<div id='math.RA'></div>

# math.RA [[Back]](#toc)

### [134] [Mal'cev clones over a three-element set up to minor-equivalence](https://arxiv.org/abs/2508.06918)
*Stefano Fioravanti,Michael Kompatscher,Bernardo Rossi,Albert Vucaj*

Main category: math.RA

TL;DR: 该论文对三元素集合上的所有Mal'cev克隆进行了分类，并通过minion同态验证，进一步推进了三元素关系结构的分类研究。


<details>
  <summary>Details</summary>
Motivation: 目标是完成三元素关系结构在pp-可构造性下的完整分类，并验证Bulatov关于Mal'cev克隆关系基础的结果。

Method: 使用minion同态对Mal'cev克隆进行分类，并提供Bulatov结果的替代证明方法。

Result: 成功分类了三元素集合上的所有Mal'cev克隆，并确认其关系基础最多为4元。

Conclusion: 研究为三元素关系结构的分类提供了新进展，并验证了Bulatov的理论。

Abstract: We classify all Mal'cev clones over a three-element set up to minion
homomorphisms. This is another step toward the complete classification of
three-element relational structures up to pp-constructability. We furthermore
provide an alternative proof of Bulatov's result that all Mal'cev clones over a
three-element set have an at most 4-ary relational basis.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [135] [A Portable Multi-GPU Solver for Collisional Plasmas with Coulombic Interactions](https://arxiv.org/abs/2508.06771)
*James Almgren-Bell,Nader Al Awar,Dilip S Geethakrishnan,Milos Gligoric,George Biros*

Main category: cs.CE

TL;DR: 研究了用于低温等离子体的并行粒子单元方法，重点探索了GPU加速算法及Python工具的使用。


<details>
  <summary>Details</summary>
Motivation: 旨在提高低温等离子体模拟的效率，特别是电子碰撞的计算速度，并测试快速原型开发的可行性。

Method: 采用电子动力学描述和流体近似法，结合GPU加速算法，并使用Python工具PyKokkos进行实现。

Result: 在NVIDIA V100和AMD MI250X GPU上测试，MI250X性能略优但对寄存器压力更敏感；分布式内存实现扩展至16 MPI秩。

Conclusion: GPU加速有效提升了计算效率，Python工具可用于快速原型开发，AMD GPU在小部分场景下表现更优。

Abstract: We study parallel particle-in-cell (PIC) methods for low-temperature plasmas
(LTPs), which discretize kinetic formulations that capture the time evolution
of the probability density function of particles as a function of position and
velocity. We use a kinetic description for electrons and a fluid approximation
for heavy species. In this paper, we focus on GPU acceleration of algorithms
for velocity-space interactions and in particular, collisions of electrons with
neutrals, ions, and electrons. Our work has two thrusts. The first is
algorithmic exploration and analysis. The second is examining the viability of
rapid-prototyping implementations using Python-based HPC tools, in particular
PyKokkos. We discuss several common PIC kernels and present performance results
on NVIDIA Volta V100 and AMD MI250X GPUs. Overall, the MI250X is slightly
faster for most kernels but shows more sensitivity to register pressure. We
also report scaling results for a distributed memory implementation on up to 16
MPI ranks.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [136] [From Platform Migration to Cultural Integration: the Ingress and Diffusion of #wlw from TikTok to RedNote in Queer Women](https://arxiv.org/abs/2508.07579)
*Ziqi Pan,Runhua Zhang,Jiehui Luo,Yuanhao Zhang,Yue Deng,Xiaojuan Ma*

Main category: cs.SI

TL;DR: 研究探讨了西方起源的#wlw标签在中国女同性恋社区中的传播与扩散，分析了用户迁移和文化适应如何促进其成功引入。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究跨文化标签在数字迁移中的传播机制，特别关注边缘化社区的响应行为。

Method: 采用两阶段内容分析法，分析418条#wlw帖子，比较标签进入和扩散阶段的不同使用模式。

Result: 结果显示，#wlw标签的成功引入得益于TikTok移民的大胆导入、双方的相互解读及本土用户的讨论，标签语义还扩展至女权主义议题。

Conclusion: 研究为边缘化社区跨文化传播提供了实证依据，强调了用户行为的适应性在标签扩散中的作用。

Abstract: Hashtags serve as identity markers and connection tools in online queer
communities. Recently, the Western-origin #wlw (women-loving-women) hashtag has
risen in the Chinese lesbian community on RedNote, coinciding with user
migration triggered by the temporary US TikTok ban. This event provides a
unique lens to study cross-cultural hashtag ingress and diffusion through the
populations' responsive behaviors in cyber-migration. In this paper, we
conducted a two-phase content analysis of 418 #wlw posts from January and
April, examining different usage patterns during the hashtag's ingress and
diffusion. Results indicate that the successful introduction of #wlw was
facilitated by TikTok immigrants' bold importation, both populations' mutual
interpretation, and RedNote natives' discussions. In current manifestation of
diffusion, #wlw becomes a RedNote-recognized queer hashtag for sharing queer
life, and semantically expands to support feminism discourse. Our findings
provide empirical insights for enhancing the marginalized communities'
cross-cultural communication.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [137] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)
*Jiaqi Yin,Yi-Wei Chen,Meng-Lung Lee,Xiya Liu*

Main category: cs.CL

TL;DR: 论文提出了一种自动提取多语言企业管道脚本中细粒度模式谱系的新框架，解决了语义漂移问题，并通过实验验证了模型规模和提示技术对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 企业数据管道中复杂的多语言转换导致语义漂移，影响数据可重现性和治理，需要一种自动化解决方案。

Method: 提出了一种提取模式谱系的框架，包括源模式、源表、转换逻辑和聚合操作，并引入SLiCE评估指标和新的基准数据集。

Result: 实验表明，模式谱系提取性能随模型规模和提示技术的提升而提高，32B开源模型在单推理轨迹下性能接近GPT系列。

Conclusion: 该方法为实际应用中部署模式感知代理提供了经济和可扩展的解决方案。

Abstract: Enterprise data pipelines, characterized by complex transformations across
multiple programming languages, often cause a semantic disconnect between
original metadata and downstream data. This "semantic drift" compromises data
reproducibility and governance, and impairs the utility of services like
retrieval-augmented generation (RAG) and text-to-SQL systems. To address this,
a novel framework is proposed for the automated extraction of fine-grained
schema lineage from multilingual enterprise pipeline scripts. This method
identifies four key components: source schemas, source tables, transformation
logic, and aggregation operations, creating a standardized representation of
data transformations. For the rigorous evaluation of lineage quality, this
paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that
assesses both structural correctness and semantic fidelity. A new benchmark is
also presented, comprising 1,700 manually annotated lineages from real-world
industrial scripts. Experiments were conducted with 12 language models, from
1.3B to 32B small language models (SLMs) to large language models (LLMs) like
GPT-4o and GPT-4.1. The results demonstrate that the performance of schema
lineage extraction scales with model size and the sophistication of prompting
techniques. Specially, a 32B open-source model, using a single reasoning trace,
can achieve performance comparable to the GPT series under standard prompting.
This finding suggests a scalable and economical approach for deploying
schema-aware agents in practical applications.

</details>


### [138] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)
*Lei Jiang,Fan Chen*

Main category: cs.CL

TL;DR: 该论文提出了CarbonScaling框架，将神经扩展定律扩展到包含LLM训练中的碳排放，揭示了模型准确性与碳足迹之间的幂律关系，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 现有神经扩展定律忽视了LLM规模扩大带来的碳排放问题，因此需要一种新框架来量化碳排放与模型性能的关系。

Method: 通过整合神经扩展模型、GPU硬件演化、并行优化和碳估计模型，构建了CarbonScaling框架。

Result: 结果显示准确性与碳足迹之间存在幂律关系，但现实中的低效性显著增加了扩展因子；硬件技术改进对小型至中型模型有效，但对超大LLM效果有限。

Conclusion: CarbonScaling为训练更具可持续性和碳效率的LLMs提供了关键见解，尤其是通过优化训练策略（如关键批量大小扩展）。

Abstract: Neural scaling laws have driven the development of increasingly large
language models (LLMs) by linking accuracy improvements to growth in parameter
count, dataset size, and compute. However, these laws overlook the carbon
emissions that scale exponentially with LLM size. This paper presents
\textit{CarbonScaling}, an analytical framework that extends neural scaling
laws to incorporate both operational and embodied carbon in LLM training. By
integrating models for neural scaling, GPU hardware evolution, parallelism
optimization, and carbon estimation, \textit{CarbonScaling} quantitatively
connects model accuracy to carbon footprint. Results show that while a
power-law relationship between accuracy and carbon holds, real-world
inefficiencies significantly increase the scaling factor. Hardware technology
scaling reduces carbon emissions for small to mid-sized models, but offers
diminishing returns for extremely large LLMs due to communication overhead and
underutilized GPUs. Training optimizations-especially aggressive critical batch
size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers
key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [139] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)
*Joseph T. Colonel,Baihan Lin*

Main category: cs.CL

TL;DR: ThemeClouds是一种基于大语言模型（LLM）的可视化工具，用于生成主题词云，解决了传统频率法在对话语境中的不足，提供了更直观的分析结果。


<details>
  <summary>Details</summary>
Motivation: 传统词频方法在对话语境中效果不佳，无法有效捕捉语义相关的主题，限制了早期研究的快速分析需求。

Method: ThemeClouds利用LLM识别语料库中的概念级主题，并根据参与者提及次数生成词云，支持自定义提示和可视化参数。

Result: 在用户研究中，ThemeClouds比传统词频和主题模型（如LDA、BERTopic）更能提取实用的设备关注点。

Conclusion: ThemeClouds结合LLM辅助定性分析，提升了结果的可解释性和研究者的控制力，同时为交互式分析（如差异对比）提供了机会。

Abstract: Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [140] [Enhancing Systematic Interoperability: Convergences and Mismatches between Web 3.0 and the EU Data Act](https://arxiv.org/abs/2508.07356)
*Linyi Xu,Zihao Li*

Main category: cs.CY

TL;DR: 论文探讨了Web 3.0技术互操作性与欧盟数据法案法律互操作性框架之间的差异，提出了跨学科建议以促进系统性互操作性实践。


<details>
  <summary>Details</summary>
Motivation: 现有研究对技术互操作性与法律互操作性的交叉探索不足，本研究旨在填补这一空白，推动下一代网络中的系统性互操作性实现。

Method: 比较分析Web 3.0的技术互操作性与欧盟数据法案的法律互操作性框架，揭示其趋同与不匹配之处。

Result: 发现Web 3.0的互操作性涵盖数据、系统和应用，而数据法案仅关注数据，可能导致生态系统碎片化；数据法案的合规要求可能限制系统和应用互操作性。

Conclusion: 建议数据法案扩大互操作性概念，纳入系统和应用层；同时通过软法机制引入标准化协议，以应对法律不足并跟上技术进步。

Abstract: Interoperability is increasingly recognised as a foundational principle for
fostering innovation, competition, and user autonomy in the evolving digital
ecosystem. Existing research on interoperability predominantly focuses either
on technological interoperability itself or on the legal regulations concerning
interoperability, with insufficient exploration of their interdisciplinary
intersection. This paper compares the technological interoperability in Web 3.0
with the theoretical framework of legal interoperability established by the EU
Data Act, analysing the areas of convergence and mismatch. The goal is to align
technical interoperability with legal concepts of interoperability, thereby
enhancing the practical implementation of systematic interoperability in the
next generation of the Web. This study finds that, firstly, Web 3.0's concept
of interoperability spans data, systems, and applications, while the Data Act
focuses solely on data. This narrow scope risks creating a fragmented
ecosystem, where data exchange is possible, but full integration of systems and
applications is hindered, leading to inefficiencies, and obstructing seamless
data flow across platforms. Secondly, while Web 3.0 technically seeks to
achieve interoperability through the integration of entire systems and
decentralised applications, the compliance with Data Act might negatively limit
such system and application interoperability through its data interoperability
provisions. This paper suggests interdisciplinary recommendations to enhance
the implementation and enforcement of interoperability. On one hand, the Data
Act should broaden its concept of interoperability to encompass both the
systems and applications layers. On the other hand, it is advisable to
introduce provisions for standardised protocols through soft law mechanisms to
address legal shortcomings and keep pace with technological advancements.

</details>


### [141] [Towards Experience-Centered AI: A Framework for Integrating Lived Experience in Design and Development](https://arxiv.org/abs/2508.06849)
*Sanjana Gautam,Mohit Chandra,Ankolika De,Tatiana Chakravorti,Girik Malik,Munmun De Choudhury*

Main category: cs.CY

TL;DR: 论文提出一个框架，将生活体验纳入AI系统的设计与评估中，以提升模型的准确性和人性化。


<details>
  <summary>Details</summary>
Motivation: 现有研究对人类生活体验的系统性理解有限，缺乏将其融入AI开发的具体策略。

Method: 综合跨学科文献，提出生活体验分类法，并在教育、医疗和文化对齐三个领域验证框架。

Result: 框架展示了生活体验如何影响用户目标、系统期望和伦理考量，并提供了实际建议。

Conclusion: 通过整合生活体验，AI系统可以更贴近人类现实，为未来研究奠定基础。

Abstract: Lived experiences fundamentally shape how individuals interact with AI
systems, influencing perceptions of safety, trust, and usability. While prior
research has focused on developing techniques to emulate human preferences, and
proposed taxonomies to categorize risks (such as psychological harms and
algorithmic biases), these efforts have provided limited systematic
understanding of lived human experiences or actionable strategies for embedding
them meaningfully into the AI development lifecycle. This work proposes a
framework for meaningfully integrating lived experience into the design and
evaluation of AI systems. We synthesize interdisciplinary literature across
lived experience philosophy, human-centered design, and human-AI interaction,
arguing that centering lived experience can lead to models that more accurately
reflect the retrospective, emotional, and contextual dimensions of human
cognition. Drawing from a wide body of work across psychology, education,
healthcare, and social policy, we present a targeted taxonomy of lived
experiences with specific applicability to AI systems. To ground our framework,
we examine three application domains (i) education, (ii) healthcare, and (iii)
cultural alignment, illustrating how lived experience informs user goals,
system expectations, and ethical considerations in each context. We further
incorporate insights from AI system operators and human-AI partnerships to
highlight challenges in responsibility allocation, mental model calibration,
and long-term system adaptation. We conclude with actionable recommendations
for developing experience-centered AI systems that are not only technically
robust but also empathetic, context-aware, and aligned with human realities.
This work offers a foundation for future research that bridges technical
development with the lived experiences of those impacted by AI systems.

</details>


### [142] [Shaping a Profession, Building a Community: A Practitioner-Led Investigation of Public Interest Technologists in Civil Society](https://arxiv.org/abs/2508.07230)
*Mallory Knodel,Mallika Balakrishnan,Lauren M. Chambers*

Main category: cs.CY

TL;DR: 该论文研究了“公共利益技术”（PIT）在民间社会中的应用，通过混合方法研究发现目前PIT领域缺乏专业化和社区支持。


<details>
  <summary>Details</summary>
Motivation: 探讨PIT在民间社会和非营利组织中的发展现状，填补了研究中对这一趋势的空白。

Method: 采用混合方法研究，包括访谈和圆桌讨论，分析了北美和西欧的PIT实践者。

Result: 研究发现PIT领域缺乏专业化基础设施和社区支持，需要更多关注。

Conclusion: PIT的发展需要更广泛的资源和社区支持，这对技术与社会的互动研究具有重要意义。

Abstract: The label `public interest technology' (PIT) is growing in popularity among
those seeking to use `tech for good' - especially among technical practitioners
working in civil society and nonprofit organizations. PIT encompasses a broad
range of sociotechnical work across professional domains and sectors; however,
the trend remains understudied within sociotechnical research. This paper
describes a mixed-methods study, designed and conducted by PIT practitioners at
the Center for Democracy and Technology, that characterizes technologists
within the specific context of civil society, civil rights, and advocacy
organizations in North America and Western Europe. We conducted interviews with
civil society leaders to investigate how PIT practitioners position the field
and themselves, and we held a roundtable discussion bringing diverse voices
together to make meaning of this growing phenomenon. Ultimately, we find that
PIT remains both defined and plagued by its expansiveness, and that today's
civil society public interest technologists see a need for both (a) more robust
professionalization infrastructures, including philanthropic attention, and (b)
more engaged, coherent community. This study illuminates a nascent intersection
of technology and policy on-the-ground that is of growing relevance to critical
sociotechnical research on the shifting relationship between computing and
society.

</details>


### [143] [Unequal Uncertainty: Rethinking Algorithmic Interventions for Mitigating Discrimination from AI](https://arxiv.org/abs/2508.07872)
*Holli Sargeant,Mackenzie Jorgensen,Arina Shah,Adrian Weller,Umang Bhatt*

Main category: cs.CY

TL;DR: 论文探讨了基于不确定性的AI干预措施（选择性保留和选择性摩擦）在法律和伦理上的挑战，提出了选择性摩擦在促进公平和透明度的潜力。


<details>
  <summary>Details</summary>
Motivation: AI预测的不确定性引发了法律和伦理挑战，需研究如何通过干预措施解决这一问题。

Method: 通过案例研究（信贷决策和内容审核），分析了选择性保留和选择性摩擦的潜在歧视影响。

Result: 不确定性阈值可能导致歧视，选择性摩擦能促进更公平的决策。

Conclusion: 选择性摩擦是更公平和透明的AI辅助决策路径，但仍需法律审查。

Abstract: Uncertainty in artificial intelligence (AI) predictions poses urgent legal
and ethical challenges for AI-assisted decision-making. We examine two
algorithmic interventions that act as guardrails for human-AI collaboration:
selective abstention, which withholds high-uncertainty predictions from human
decision-makers, and selective friction, which delivers those predictions
together with salient warnings or disclosures that slow the decision process.
Research has shown that selective abstention based on uncertainty can
inadvertently exacerbate disparities and disadvantage under-represented groups
that disproportionately receive uncertain predictions. In this paper, we
provide the first integrated socio-technical and legal analysis of
uncertainty-based algorithmic interventions. Through two case studies,
AI-assisted consumer credit decisions and AI-assisted content moderation, we
demonstrate how the seemingly neutral use of uncertainty thresholds can trigger
discriminatory impacts. We argue that, although both interventions pose risks
of unlawful discrimination under UK law, selective frictions offer a promising
pathway toward fairer and more accountable AI-assisted decision-making by
preserving transparency and encouraging more cautious human judgment.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [144] [Understanding NFTs from EIP Standards](https://arxiv.org/abs/2508.07190)
*Minfeng Qi,Qin Wang,Guangsheng Yu,Ruiqiang Li,Victor Zhou,Shiping Chen*

Main category: cs.CR

TL;DR: 论文首次通过以太坊改进提案（EIPs）的系统视角研究NFT技术基础，揭示了标准设计的不足和潜在安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前对NFT的研究多集中于市场动态和用户行为，缺乏对其底层技术标准（如EIPs）的系统性分析。

Method: 结合大规模实证分析，包括191个NFT相关EIPs和1万+社区讨论数据，采用自动化接口解析、图模型继承结构分析等方法。

Result: 发现跨版本互操作性差、功能复杂性加剧安全风险，并区分了基础与新兴标准。

Conclusion: NFT技术标准亟需系统性优化以避免安全漏洞和功能混乱。

Abstract: We argue that the technical foundations of non-fungible tokens (NFTs) remain
inadequately understood. Prior research has focused on market dynamics, user
behavior, and isolated security incidents, yet systematic analysis of the
standards underpinning NFT functionality is largely absent.
  We present the first study of NFTs through the lens of Ethereum Improvement
Proposals (EIPs). We conduct a large-scale empirical analysis of 191
NFT-related EIPs and 10K+ Ethereum Magicians discussions (as of July, 2025). We
integrate multi-dimensional analyses including the automated parsing of
Solidity interfaces, graph-based modeling of inheritance structures,
contributor profiling, and mining of community discussion data. We distinguish
foundational from emerging standards, expose poor cross-version
interoperability, and show that growing functional complexity heightens
security risks.

</details>


### [145] [Robust Anomaly Detection in O-RAN: Leveraging LLMs against Data Manipulation Attacks](https://arxiv.org/abs/2508.08029)
*Thusitha Dayaratne,Ngoc Duy Pham,Viet Vo,Shangqi Lai,Sharif Abuadbba,Hajime Suzuki,Xingliang Yuan,Carsten Rudolph*

Main category: cs.CR

TL;DR: 本文研究了5G和O-RAN架构中的新安全挑战，特别是恶意xApp通过Unicode攻击（低血糖）绕过传统ML异常检测的问题，提出使用LLM提升鲁棒性和实时性。


<details>
  <summary>Details</summary>
Motivation: O-RAN架构的开放性和复杂性带来了新的安全挑战，如恶意xApp利用Unicode攻击绕过传统ML异常检测系统，亟需更鲁棒的解决方案。

Method: 使用基于大语言模型（LLM）的xApp进行异常检测，测试其对Unicode攻击（低血糖）的鲁棒性和实时性能。

Result: LLM能处理被攻击的数据且不崩溃，具有低延迟（<0.07秒），适合实时部署，但初始检测精度需进一步提升。

Conclusion: LLM在对抗Unicode攻击和实时性方面表现优异，通过提示工程可进一步提高精度，具有广泛应用潜力。

Abstract: The introduction of 5G and the Open Radio Access Network (O-RAN) architecture
has enabled more flexible and intelligent network deployments. However, the
increased complexity and openness of these architectures also introduce novel
security challenges, such as data manipulation attacks on the semi-standardised
Shared Data Layer (SDL) within the O-RAN platform through malicious xApps. In
particular, malicious xApps can exploit this vulnerability by introducing
subtle Unicode-wise alterations (hypoglyphs) into the data that are being used
by traditional machine learning (ML)-based anomaly detection methods. These
Unicode-wise manipulations can potentially bypass detection and cause failures
in anomaly detection systems based on traditional ML, such as AutoEncoders,
which are unable to process hypoglyphed data without crashing. We investigate
the use of Large Language Models (LLMs) for anomaly detection within the O-RAN
architecture to address this challenge. We demonstrate that LLM-based xApps
maintain robust operational performance and are capable of processing
manipulated messages without crashing. While initial detection accuracy
requires further improvements, our results highlight the robustness of LLMs to
adversarial attacks such as hypoglyphs in input data. There is potential to use
their adaptability through prompt engineering to further improve the accuracy,
although this requires further research. Additionally, we show that LLMs
achieve low detection latency (under 0.07 seconds), making them suitable for
Near-Real-Time (Near-RT) RIC deployments.

</details>


### [146] [Chimera: Harnessing Multi-Agent LLMs for Automatic Insider Threat Simulation](https://arxiv.org/abs/2508.07745)
*Jiongchi Yu,Xiaofei Xie,Qiang Hu,Yuhan Ma,Ziming Zhao*

Main category: cs.CR

TL;DR: 提出了一种基于大型语言模型的多智能体框架Chimera，用于模拟企业内部员工行为并生成多样化的日志数据，解决了ITD研究中高质量数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 内部威胁检测（ITD）研究由于缺乏高质量的真实或合成数据而进展缓慢。

Method: 开发了Chimera框架，通过多智能体系统模拟员工行为，包括合法和恶意活动，生成多样化的日志数据集ChimeraLog。

Result: ChimeraLog在多样性和真实性方面表现优异，现有ITD方法在其上的表现显著低于CERT数据集，验证了其挑战性和实用性。

Conclusion: Chimera为ITD研究提供了高质量的数据支持，推动了内部威胁检测技术的发展。

Abstract: Insider threats, which can lead to severe losses, remain a major security
concern. While machine learning-based insider threat detection (ITD) methods
have shown promising results, their progress is hindered by the scarcity of
high-quality data. Enterprise data is sensitive and rarely accessible, while
publicly available datasets, when limited in scale due to cost, lack sufficient
real-world coverage; and when purely synthetic, they fail to capture rich
semantics and realistic user behavior. To address this, we propose Chimera, the
first large language model (LLM)-based multi-agent framework that automatically
simulates both benign and malicious insider activities and collects diverse
logs across diverse enterprise environments. Chimera models each employee with
agents that have role-specific behavior and integrates modules for group
meetings, pairwise interactions, and autonomous scheduling, capturing realistic
organizational dynamics. It incorporates 15 types of insider attacks (e.g., IP
theft, system sabotage) and has been deployed to simulate activities in three
sensitive domains: technology company, finance corporation, and medical
institution, producing a new dataset, ChimeraLog. We assess ChimeraLog via
human studies and quantitative analysis, confirming its diversity, realism, and
presence of explainable threat patterns. Evaluations of existing ITD methods
show an average F1-score of 0.83, which is significantly lower than 0.99 on the
CERT dataset, demonstrating ChimeraLog's higher difficulty and utility for
advancing ITD research.

</details>


### [147] [EFU: Enforcing Federated Unlearning via Functional Encryption](https://arxiv.org/abs/2508.07873)
*Samaneh Mohammadi,Vasileios Tsouvalas,Iraklis Symeonidis,Ali Balador,Tanir Ozcelebi,Francesco Flammini,Nirvana Meratnia*

Main category: cs.CR

TL;DR: EFU是一种加密强制的联邦遗忘框架，旨在保护客户端的遗忘意图和身份隐私，同时确保服务器无法检测或跳过遗忘请求。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法依赖服务器合作，可能泄露客户的遗忘意图和身份，损害其隐私和自主性。

Method: EFU利用功能加密绑定加密更新到特定聚合函数，并结合对抗性示例和参数重要性正则化的辅助损失。

Result: EFU在遗忘数据上接近随机准确率，同时保持完整重训练的性能，且能隐藏遗忘意图。

Conclusion: EFU是一种通用、安全和可验证的联邦遗忘框架，适用于任何客户端遗忘机制。

Abstract: Federated unlearning (FU) algorithms allow clients in federated settings to
exercise their ''right to be forgotten'' by removing the influence of their
data from a collaboratively trained model. Existing FU methods maintain data
privacy by performing unlearning locally on the client-side and sending
targeted updates to the server without exposing forgotten data; yet they often
rely on server-side cooperation, revealing the client's intent and identity
without enforcement guarantees - compromising autonomy and unlearning privacy.
In this work, we propose EFU (Enforced Federated Unlearning), a
cryptographically enforced FU framework that enables clients to initiate
unlearning while concealing its occurrence from the server. Specifically, EFU
leverages functional encryption to bind encrypted updates to specific
aggregation functions, ensuring the server can neither perform unauthorized
computations nor detect or skip unlearning requests. To further mask behavioral
and parameter shifts in the aggregated model, we incorporate auxiliary
unlearning losses based on adversarial examples and parameter importance
regularization. Extensive experiments show that EFU achieves near-random
accuracy on forgotten data while maintaining performance comparable to full
retraining across datasets and neural architectures - all while concealing
unlearning intent from the server. Furthermore, we demonstrate that EFU is
agnostic to the underlying unlearning algorithm, enabling secure,
function-hiding, and verifiable unlearning for any client-side FU mechanism
that issues targeted updates.

</details>


### [148] [Fully-Fluctuating Participation in Sleepy Consensus](https://arxiv.org/abs/2508.08068)
*Yuval Efron,Joachim Neu,Toniann Pitassi*

Main category: cs.CR

TL;DR: 该论文提出了一种新的敌手模型——外部敌手模型，以解决现有协议在参与度大幅波动时的安全性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于权益证明（PoS）的共识协议在参与度波动时限制较多，无法像比特币工作量证明（PoW）那样具备强大的鲁棒性，而睡眠模型中的协议仍需改进。

Method: 提出了一种名为外部敌手模型的新模式，假设腐败节点不会泄露其私钥信息。

Result: 在该模型下，睡眠模型中的协议能够在保证效率和抗腐败能力的同时，有效应对参与度的完全波动。

Conclusion: 外部敌手模型自然且实用，突破了传统最坏情况建模的局限性，并在理论上有吸引力。

Abstract: Proof-of-work allows Bitcoin to boast security amidst arbitrary fluctuations
in participation of miners throughout time, so long as, at any point in time, a
majority of hash power is honest. In recent years, however, the pendulum has
shifted in favor of proof-of-stake-based consensus protocols. There, the sleepy
model is the most prominent model for handling fluctuating participation of
nodes. However, to date, no protocol in the sleepy model rivals Bitcoin in its
robustness to drastic fluctuations in participation levels, with
state-of-the-art protocols making various restrictive assumptions. In this
work, we present a new adversary model, called external adversary. Intuitively,
in our model, corrupt nodes do not divulge information about their secret keys.
In this model, we show that protocols in the sleepy model can meaningfully
claim to remain secure against fully fluctuating participation, without
compromising efficiency or corruption resilience. Our adversary model is quite
natural, and arguably naturally captures the process via which malicious
behavior arises in protocols, as opposed to traditional worst-case modeling. On
top of which, the model is also theoretically appealing, circumventing a
barrier established in a recent work of Malkhi, Momose, and Ren.

</details>


### [149] [False Reality: Uncovering Sensor-induced Human-VR Interaction Vulnerability](https://arxiv.org/abs/2508.08043)
*Yancheng Jiang,Yan Jiang,Ruochen Zhou,Yi-Chao Chen,Xiaoyu Ji,Wenyuan Xu*

Main category: cs.CR

TL;DR: 本文首次系统分析了针对VR系统的物理攻击，提出了无需修改软件的False Reality攻击，并通过实验验证其威胁，最终提出了防御方案。


<details>
  <summary>Details</summary>
Motivation: VR技术在多个领域的广泛应用使其安全性变得重要，现有攻击需高权限，而False Reality攻击无需软件修改即可威胁用户。

Method: 通过篡改传感器数据，利用感知和心理效应诱导用户行为，构建攻击路径框架并实验验证。

Result: 在五款商用VR设备上验证了三种攻击路径的有效性，提出了防御原型。

Conclusion: False Reality攻击揭示了VR系统的安全漏洞，提出的防御方案对未来VR系统安全具有参考价值。

Abstract: Virtual Reality (VR) techniques, serving as the bridge between the real and
virtual worlds, have boomed and are widely used in manufacturing, remote
healthcare, gaming, etc. Specifically, VR systems offer users immersive
experiences that include both perceptions and actions. Various studies have
demonstrated that attackers can manipulate VR software to influence users'
interactions, including perception and actions. However, such attacks typically
require strong access and specialized expertise. In this paper, we are the
first to present a systematic analysis of physical attacks against VR systems
and introduce False Reality, a new attack threat to VR devices without
requiring access to or modification of their software. False Reality disturbs
VR system services by tampering with sensor measurements, and further spoofing
users' perception even inducing harmful actions, e.g., inducing dizziness or
causing users to crash into obstacles, by exploiting perceptual and
psychological effects. We formalize these threats through an attack pathway
framework and validate three representative pathways via physical experiments
and user studies on five commercial VR devices. Finally, we further propose a
defense prototype to mitigate such threats. Our findings shall provide valuable
insights for enhancing the security and resilience of future VR systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [150] [Recommendation with Generative Models](https://arxiv.org/abs/2409.15173)
*Yashar Deldjoo,Zhankui He,Julian McAuley,Anton Korikov,Scott Sanner,Arnau Ramisa,Rene Vidal,Maheswaran Sathiamoorthy,Atoosa Kasrizadeh,Silvia Milano,Francesco Ricci*

Main category: cs.IR

TL;DR: 这篇论文讨论了生成模型（如GANs、VAEs和GPT）在推荐系统（Gen-RecSys）中的应用，并提出了深度生成模型（DGMs）的分类法。


<details>
  <summary>Details</summary>
Motivation: 生成模型在多个领域中的应用日益广泛，但现有文献对其分类和应用的全面理解仍需完善。本文旨在填补这一空白。

Method: 提出了一种分类法，将深度生成模型（DGMs）分为ID驱动模型、大型语言模型（LLMs）和多模态模型三类。

Result: 通过分类法，研究人员可以更轻松地跟踪生成模型在对话式AI和多模态内容生成等领域的发展。

Conclusion: 生成模型具有巨大潜力，但也需要稳健的评估框架来应对潜在风险。

Abstract: Generative models are a class of AI models capable of creating new instances
of data by learning and sampling from their statistical distributions. In
recent years, these models have gained prominence in machine learning due to
the development of approaches such as generative adversarial networks (GANs),
variational autoencoders (VAEs), and transformer-based architectures such as
GPT. These models have applications across various domains, such as image
generation, text synthesis, and music composition. In recommender systems,
generative models, referred to as Gen-RecSys, improve the accuracy and
diversity of recommendations by generating structured outputs, text-based
interactions, and multimedia content. By leveraging these capabilities,
Gen-RecSys can produce more personalized, engaging, and dynamic user
experiences, expanding the role of AI in eCommerce, media, and beyond.
  Our book goes beyond existing literature by offering a comprehensive
understanding of generative models and their applications, with a special focus
on deep generative models (DGMs) and their classification. We introduce a
taxonomy that categorizes DGMs into three types: ID-driven models, large
language models (LLMs), and multimodal models. Each category addresses unique
technical and architectural advancements within its respective research area.
This taxonomy allows researchers to easily navigate developments in Gen-RecSys
across domains such as conversational AI and multimodal content generation.
Additionally, we examine the impact and potential risks of generative models,
emphasizing the importance of robust evaluation frameworks.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [151] [Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions](https://arxiv.org/abs/2508.07948)
*John D. Mayfield*

Main category: q-bio.OT

TL;DR: 论文提出了一种结合傅里叶变换、拉普拉斯变换和量子计算的理论框架，用于分析神经退行性疾病的非线性动态。


<details>
  <summary>Details</summary>
Motivation: 神经退行性疾病的复杂动态难以通过传统时间域分析捕捉，因此需要开发更高效的数学模型和方法。

Method: 通过频率域或s域转换时间序列数据，利用哈密顿方程建模神经元动态，并采用量子-经典混合计算和变分量子本征求解器（VQE）增强模式检测。

Result: 该框架为未来的实证研究奠定了基础，并在阿尔茨海默病分类中展示了高达99.89%的准确性潜力。

Conclusion: 这一理论框架有望为神经退行性疾病的精准医学提供新的研究方向。

Abstract: Progressive neurodegenerative diseases, including Alzheimer's disease (AD),
multiple sclerosis (MS), Parkinson's disease (PD), and amyotrophic lateral
sclerosis (ALS), exhibit complex, nonlinear trajectories that challenge
deterministic modeling. Traditional time-domain analyses of multiomic and
neuroimaging data often fail to capture hidden oscillatory patterns, limiting
predictive accuracy. We propose a theoretical mathematical framework that
transforms time-series data into frequency or s-domain using Fourier and
Laplace transforms, models neuronal dynamics via Hamiltonian formulations, and
employs quantum-classical hybrid computing with variational quantum
eigensolvers (VQE) for enhanced pattern detection. This theoretical construct
serves as a foundation for future empirical works in quantum-enhanced analysis
of neurodegenerative diseases. We extend this to quaternionic representations
with three imaginary axes ($i, j, k$) to model multistate Hamiltonians in
multifaceted disorders, drawing from quantum neuromorphic computing to capture
entangled neural dynamics \citep{Pehle2020, Emani2019}. This approach leverages
quantum advantages in handling high-dimensional amplitude-phase data, enabling
outlier detection and frequency signature analysis. Potential clinical
applications include identifying high-risk patients with rapid progression or
therapy resistance using s-domain biomarkers, supported by quantum machine
learning (QML) precedents achieving up to 99.89% accuracy in Alzheimer's
classification \citep{Belay2024, Bhowmik2025}. This framework aims to lay the
groundwork for redefining precision medicine for neurodegenerative diseases
through future validations.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [152] [Proof-theoretic Semantics for Second-order Logic](https://arxiv.org/abs/2508.07786)
*Alexander V. Gheorghiu,David J. Pym*

Main category: math.LO

TL;DR: 该论文提出了一种基于证明论语义学的二阶逻辑方法，通过基扩展语义学框架，以推理角色为基础定义逻辑含义，提供了对经典和直觉主义二阶逻辑的统一解释。


<details>
  <summary>Details</summary>
Motivation: 旨在为二阶逻辑提供一个不依赖于模型论的替代语义学，强调逻辑含义应基于推理和使用而非集合论承诺。

Method: 采用基扩展语义学（B-eS）框架，通过调整原子系统的类别来区分经典和直觉主义二阶逻辑，并证明了与Henkin模型论的等价性。

Result: 证明了所提出的语义学对Hilbert-style演算的模块化完备性和可靠性，并将二阶量化重新定义为系统替换而非集合论承诺。

Conclusion: 该方法为高阶逻辑提供了一种哲学上更轻量级的语义学，支持逻辑含义应基于推理使用的观点，并对逻辑和数学基础提出了新的见解。

Abstract: We develop a proof-theoretic semantics (P-tS) for second-order logic (S-oL),
providing an inferentialist alternative to both full and Henkin model-theoretic
interpretations. Our approach is grounded in base-extension semantics (B-eS), a
framework in which meaning is determined by inferential roles relative to
atomic systems -- collections of rules that encode an agent's pre-logical
inferential commitments. We show how both classical and intuitionistic versions
of S-oL emerge from this set-up by varying the class of atomic systems. These
systems yield modular soundness and completeness results for corresponding
Hilbert-style calculi, which we prove equivalent to Henkin's account of S-oL.
In doing so, we reframe second-order quantification as systematic substitution
rather than set-theoretic commitment, thereby offering a philosophically
lightweight yet expressive semantics for higher-order logic. This work
contributes to the broader programme of grounding logical meaning in use rather
than reference and offers a new lens on the foundations of logic and
mathematics.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [153] [Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning](https://arxiv.org/abs/2508.08039)
*Shu Wu,Chenxing Li,Wenfu Wang,Hao Zhang,Hualei Wang,Meng Yu,Dong Yu*

Main category: cs.SD

TL;DR: 论文提出了一种名为Audio-Thinker的强化学习框架，旨在提升大型音频语言模型（LALMs）的推理能力，通过动态调整策略和外部奖励模型来提高其适应性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前大型音频语言模型在音频问答任务中的显式推理能力尚不显著，且与人类听觉-语言推理能力存在差距，因此需要改进模型的推理能力。

Method: 提出Audio-Thinker框架，包含自适应思维准确性奖励和外部奖励模型，以动态调整推理策略并评估推理过程的一致性和质量。

Result: 实验结果显示，Audio-Thinker在多个基准任务中优于现有推理导向的LALMs，表现出更强的推理和泛化能力。

Conclusion: Audio-Thinker通过改进的强化学习机制，显著提升了LALMs的推理能力，为未来听觉-语言推理任务的研究提供了新方向。

Abstract: Recent advancements in large language models, multimodal large language
models, and large audio language models (LALMs) have significantly improved
their reasoning capabilities through reinforcement learning with rule-based
rewards. However, the explicit reasoning process has yet to show significant
benefits for audio question answering, and effectively leveraging deep
reasoning remains an open challenge, with LALMs still falling short of
human-level auditory-language reasoning. To address these limitations, we
propose Audio-Thinker, a reinforcement learning framework designed to enhance
the reasoning capabilities of LALMs, with a focus on improving adaptability,
consistency, and effectiveness. Our approach introduces an adaptive think
accuracy reward, enabling the model to adjust its reasoning strategies based on
task complexity dynamically. Furthermore, we incorporate an external reward
model to evaluate the overall consistency and quality of the reasoning process,
complemented by think-based rewards that help the model distinguish between
valid and flawed reasoning paths during training. Experimental results
demonstrate that our Audio-Thinker model outperforms existing
reasoning-oriented LALMs across various benchmark tasks, exhibiting superior
reasoning and generalization capabilities.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [154] [GPU-Accelerated Syndrome Decoding for Quantum LDPC Codes below the 63 $μ$s Latency Threshold](https://arxiv.org/abs/2508.07879)
*Oscar Ferraz,Bruno Coutinho,Gabriel Falcao,Marco Gomes,Francisco A. Monteiro,Vitor Silva*

Main category: quant-ph

TL;DR: 本文介绍了一种基于GPU加速的量子低密度奇偶校验(QLDPC)解码器，实现了低于63微秒的延迟，优于Google Willow量子处理器上表面码解码器的实时阈值。QLDPC编码具有恒定速率和渐进优性，但解码复杂度较高。通过并行化置信传播解码器，利用GPU硬件实现了低于50微秒的解码延迟，展示了使用广泛商用硬件实现实时可扩展量子解码的可行性。


<details>
  <summary>Details</summary>
Motivation: 表面码虽已实现低于阈值的性能，但其编码速率随码距增加趋近于零，限制了可扩展性。最近提出的QLDPC码具有恒定速率和渐进优性，但解码复杂度较高。因此，研究如何解决QLDPC码的解码延迟问题，以推动容错量子计算的实现。

Method: 设计了一种并行化的置信传播解码器，利用GPU硬件加速，并通过优化并行性以在目标延迟范围内最大化性能。针对不同规模的QLDPC码（如[[784, 24, 24]]码）进行了实现和测试。

Result: 在商用GPU硬件上实现了低于50微秒的解码延迟（对[[784, 24, 24]]码）和低至23.3微秒的解码延迟（对小规模码），满足了超导量子比特周期的严格时间约束。

Conclusion: 研究表明，使用广泛可得的商用硬件可以实现实时且可扩展的渐进优性量子码解码，为超越表面码的容错量子计算提供了可行性。

Abstract: This paper presents a GPU-accelerated decoder for quantum low-density
parity-check (QLDPC) codes that achieves sub-$63$ $\mu$s latency, below the
surface code decoder's real-time threshold demonstrated on Google's Willow
quantum processor. While surface codes have demonstrated below-threshold
performance, the encoding rates approach zero as code distances increase,
posing challenges for scalability. Recently proposed QLDPC codes, such as those
by Panteleev and Kalachev, offer constant-rate encoding and asymptotic goodness
but introduce higher decoding complexity. To address such limitation, this work
presents a parallelized belief propagation decoder leveraging syndrome
information on commodity GPU hardware. Parallelism was exploited to maximize
performance within the limits of target latency, allowing decoding latencies
under $50$ $\mu$s for [[$784$, $24$, $24$]] codes and as low as $23.3$ $\mu$s
for smaller codes, meeting the tight timing constraints of superconducting
qubit cycles. These results show that real-time, scalable decoding of
asymptotically good quantum codes is achievable using widely available
commodity hardware, advancing the feasibility of fault-tolerant quantum
computation beyond surface codes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [155] [Generalizing Scaling Laws for Dense and Sparse Large Language Models](https://arxiv.org/abs/2508.06617)
*Md Arafat Hossain,Xingfu Wu,Valerie Taylor,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出了一种适用于密集和稀疏大型语言模型的通用缩放定律，旨在解决现有缩放定律架构特定性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的规模和计算成本急剧增长，需要提高训练效率，而现有的缩放定律多为特定架构（密集或稀疏）设计，缺乏通用性。

Method: 重新审视现有缩放定律，提出一种通用缩放定律框架，可同时适用于密集和稀疏模型。

Result: 通过评估比较，验证了所提通用缩放定律的有效性。

Conclusion: 研究提供了一种统一的缩放定律框架，为优化模型规模和资源分配提供了新思路。

Abstract: Over the past few years, the size of language models has grown exponentially,
as has the computational cost to train these large models. This rapid growth
has motivated researchers to develop new techniques aimed at enhancing the
efficiency of the training process. Despite these advancements, optimally
predicting the model size or allocating optimal resources remains a challenge.
Several efforts have addressed the challenge by proposing different scaling
laws, but almost all of them are architecture-specific (dense or sparse). In
this work we revisit existing scaling laws and propose a generalized scaling
law to provide a unified framework that is applicable to both dense and sparse
large language models. We evaluate and compare our proposed scaling law with
existing scaling laws to demonstrate its effectiveness.

</details>


### [156] [ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring](https://arxiv.org/abs/2508.08073)
*Dimitris Tsaras,Xing Li,Lei Chen,Zhiyao Xie,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 提出一种基于分类器的逻辑优化方法，通过预剪枝无效切割，显著提高优化速度。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑优化计算成本高且效率低（98% 失败率），亟需更高效的方法。

Method: 利用分类器预判并剪枝无效切割，避免不必要的重新综合操作。

Result: 在 EPFL 基准测试和工业设计中，优化速度提升 3.9 倍（与 ABC 实现相比）。

Conclusion: 该方法通过智能剪枝显著降低了计算成本，提升了逻辑优化效率。

Abstract: In electronic design automation, logic optimization operators play a crucial
role in minimizing the gate count of logic circuits. However, their computation
demands are high. Operators such as refactor conventionally form iterative cuts
for each node, striving for a more compact representation - a task which often
fails 98% on average. Prior research has sought to mitigate computational cost
through parallelization. In contrast, our approach leverages a classifier to
prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis
operations. Experiments on the refactor operator using the EPFL benchmark suite
and 10 large industrial designs demonstrate that this technique can speedup
logic optimization by 3.9x on average compared with the state-of-the-art ABC
implementation.

</details>


### [157] [Neural Logic Networks for Interpretable Classification](https://arxiv.org/abs/2508.08172)
*Vincent Perreault,Katsumi Inoue,Richard Labib,Alain Hertz*

Main category: cs.LG

TL;DR: 论文提出了一种可解释的神经逻辑网络，通过引入NOT操作和偏差项来改进传统神经网络，并提出了新的因子化IF-THEN规则结构和学习算法，提升了布尔网络发现的性能。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络虽然分类性能强，但缺乏可解释性。神经逻辑网络通过学习逻辑机制（AND/OR操作）提供了解释性，但仍需改进以处理未观察数据和更复杂的逻辑关系。

Method: 通过引入NOT操作和偏差项来扩展神经逻辑网络，提出了因子化的IF-THEN规则结构，并设计了改进的学习算法。

Result: 方法在布尔网络发现任务中达到了最先进的性能，并在表格分类（尤其是医疗领域）中学习到了相关且可解释的规则。

Conclusion: 该研究为可解释的机器学习提供了新思路，尤其在需要高解释性的应用场景（如医疗领域）具有实际价值。

Abstract: Traditional neural networks have an impressive classification performance,
but what they learn cannot be inspected, verified or extracted. Neural Logic
Networks on the other hand have an interpretable structure that enables them to
learn a logical mechanism relating the inputs and outputs with AND and OR
operations. We generalize these networks with NOT operations and biases that
take into account unobserved data and develop a rigorous logical and
probabilistic modeling in terms of concept combinations to motivate their use.
We also propose a novel factorized IF-THEN rule structure for the model as well
as a modified learning algorithm. Our method improves the state-of-the-art in
Boolean networks discovery and is able to learn relevant, interpretable rules
in tabular classification, notably on an example from the medical field where
interpretability has tangible value.

</details>


### [158] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: 论文提出了一种基于迁移学习的预测过程监控（PPM）技术，使缺乏足够事件数据或其他资源的组织也能实现有效的决策支持。通过两个真实案例验证了技术在组织内和组织间迁移知识的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有PPM技术需要大量事件数据或其他资源，一些组织因资源不足无法使用该技术。本文旨在解决这一限制。

Method: 提出了一种基于迁移学习的PPM技术，利用源过程的预训练模型为目标过程提供支持。通过两个真实场景下的数值实验验证方法。

Result: 实验结果表明，在组织内和组织间，源过程的知识可以迁移到类似的目标过程，实现有效预测。

Conclusion: 该技术为资源有限的组织提供了PPM的可能性，拓宽了迁移学习在过程监控中的应用范围。

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [159] [Multimodal Remote Inference](https://arxiv.org/abs/2508.07555)
*Keyuan Zhang,Yin Sun,Bo Ji*

Main category: cs.LG

TL;DR: 该研究提出了一种针对多模态远程推理系统的调度策略，通过优化特征传输时延（AoI）来最小化推理误差，其基于索引的阈值策略在非单调、非累加的AoI函数和异构传输时间下均最优。


<details>
  <summary>Details</summary>
Motivation: 在多模态远程推理系统中，传感器数据的动态变化要求特征传输具备高时效性，但网络资源有限导致无法同时满足所有模态的传输需求。因此，需设计一种调度策略以最小化AoI对推理误差的影响。

Method: 研究开发了一种基于索引的阈值策略，调度器在模态索引函数超过阈值时切换模态。该策略在非单调、非累加的AoI函数和异构传输时间下均能高效计算阈值和索引函数。

Result: 数值结果表明，相比轮询和随机策略，该策略能减少高达55%的推理误差，显著提升推理准确性。

Conclusion: 研究通过优化面向任务的AoI函数，为提升远程推理精度提供了有效方法，其策略在广泛条件下具有最优性。

Abstract: We consider a remote inference system with multiple modalities, where a
multimodal machine learning (ML) model performs real-time inference using
features collected from remote sensors. As sensor observations may change
dynamically over time, fresh features are critical for inference tasks.
However, timely delivering features from all modalities is often infeasible due
to limited network resources. To this end, we study a two-modality scheduling
problem to minimize the ML model's inference error, which is expressed as a
penalty function of AoI for both modalities. We develop an index-based
threshold policy and prove its optimality. Specifically, the scheduler switches
modalities when the current modality's index function exceeds a threshold. We
show that the two modalities share the same threshold, and both the index
functions and the threshold can be computed efficiently. The optimality of our
policy holds for (i) general AoI functions that are \emph{non-monotonic} and
\emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical
results show that our policy reduces inference error by up to 55% compared to
round-robin and uniform random policies, which are oblivious to the AoI-based
inference error function. Our results shed light on how to improve remote
inference accuracy by optimizing task-oriented AoI functions.

</details>


### [160] [FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks](https://arxiv.org/abs/2508.08151)
*Moses Openja,Paolo Arcaini,Foutse Khomh,Fuyuki Ishikawa*

Main category: cs.LG

TL;DR: FairFLRep是一种自动化公平性感知的故障定位与修复技术，用于减少DNN中的偏见行为。


<details>
  <summary>Details</summary>
Motivation: DNN在高风险决策中可能放大数据偏见，导致不公平结果，因此需要有效方法识别和修复偏见神经元。

Method: 通过调整与敏感属性相关的神经元权重，分析输入输出关系来纠正预测质量差异。

Result: 在多种数据集和模型上，FairFLRep在提升公平性和保持准确性方面优于现有方法，且效率更高。

Conclusion: 研究表明，FairFLRep在故障定位和修复阶段考虑公平性至关重要，并能高效修复网络。

Abstract: Deep neural networks (DNNs) are being utilized in various aspects of our
daily lives, including high-stakes decision-making applications that impact
individuals. However, these systems reflect and amplify bias from the data used
during training and testing, potentially resulting in biased behavior and
inaccurate decisions. For instance, having different misclassification rates
between white and black sub-populations. However, effectively and efficiently
identifying and correcting biased behavior in DNNs is a challenge. This paper
introduces FairFLRep, an automated fairness-aware fault localization and repair
technique that identifies and corrects potentially bias-inducing neurons in DNN
classifiers. FairFLRep focuses on adjusting neuron weights associated with
sensitive attributes, such as race or gender, that contribute to unfair
decisions. By analyzing the input-output relationships within the network,
FairFLRep corrects neurons responsible for disparities in predictive quality
parity. We evaluate FairFLRep on four image classification datasets using two
DNN classifiers, and four tabular datasets with a DNN model. The results show
that FairFLRep consistently outperforms existing methods in improving fairness
while preserving accuracy. An ablation study confirms the importance of
considering fairness during both fault localization and repair stages. Our
findings also show that FairFLRep is more efficient than the baseline
approaches in repairing the network.

</details>


### [161] [PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems](https://arxiv.org/abs/2508.06767)
*Arman Dogru,R. Irem Bor-Yaliniz,Nimal Gamini Senarath*

Main category: cs.LG

TL;DR: 论文提出PANAMA算法，通过优先不对称的网络感知多智能体强化学习（MARL）解决多智能体路径规划（MAPF），在速度和可扩展性上优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 随着数字化孪生（DTs）和自动化系统规模扩大，高效的数据共享框架和算法成为关键。研究聚焦于应用与网络提供商（AP/NP）的动态交互，提出PANAMA算法以优化数据共享和决策。

Method: 采用集中训练与分散执行（CTDE）框架及异步动作-学习架构，PANAMA结合网络感知的优先不对称策略，实现高效的多智能体路径规划。

Result: 模拟显示PANAMA在路径规划的准确性、速度和可扩展性上优于现有基准，同时优化了数据共享策略，增强了复杂环境下的系统韧性。

Conclusion: PANAMA填补了网络感知决策与多智能体协作之间的空白，推动了数字化孪生、无线网络与AI驱动自动化的协同发展。

Abstract: Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.

</details>


### [162] [Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach](https://arxiv.org/abs/2508.07505)
*Yueyang Quan,Chang Wang,Shengjie Zhai,Minghong Fang,Zhuqing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为DPMixSGD的隐私保护算法，用于非凸分散式最小最大优化，结合了差分隐私和STORM算法，确保隐私保护的同时不影响收敛性能。


<details>
  <summary>Details</summary>
Motivation: 分散式最小最大优化中模型更新的共享可能导致敏感数据泄露，差分隐私虽能保护隐私，但噪声可能影响收敛，尤其在非凸场景中。

Method: 基于STORM算法改进，提出DPMixSGD算法，通过添加噪声保护隐私，并理论证明噪声不影响收敛。

Result: 理论证明和实验验证表明，DPMixSGD在保护隐私的同时保持了良好的收敛性能。

Conclusion: DPMixSGD是一种有效的隐私保护算法，适用于非凸分散式最小最大优化问题。

Abstract: Decentralized min-max optimization allows multi-agent systems to
collaboratively solve global min-max optimization problems by facilitating the
exchange of model updates among neighboring agents, eliminating the need for a
central server. However, sharing model updates in such systems carry a risk of
exposing sensitive data to inference attacks, raising significant privacy
concerns. To mitigate these privacy risks, differential privacy (DP) has become
a widely adopted technique for safeguarding individual data. Despite its
advantages, implementing DP in decentralized min-max optimization poses
challenges, as the added noise can hinder convergence, particularly in
non-convex scenarios with complex agent interactions in min-max optimization
problems. In this work, we propose an algorithm called DPMixSGD (Differential
Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving
algorithm specifically designed for non-convex decentralized min-max
optimization. Our method builds on the state-of-the-art STORM-based algorithm,
one of the fastest decentralized min-max solutions. We rigorously prove that
the noise added to local gradients does not significantly compromise
convergence performance, and we provide theoretical bounds to ensure privacy
guarantees. To validate our theoretical findings, we conduct extensive
experiments across various tasks and models, demonstrating the effectiveness of
our approach.

</details>


### [163] [Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks](https://arxiv.org/abs/2508.07676)
*Chenchen Lin,Xuehe Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种社交感知的隐私保护联邦学习机制，通过多跳传播模型量化间接隐私泄露，并采用Stackelberg博弈优化激励策略，显著提升了客户端效用并降低了服务器成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽能保护隐私，但社交网络的连接引入了隐私外部性问题，即客户端的隐私损失不仅受自身保护策略影响，还受网络中其他节点的隐私决策影响。

Method: 设计了一种社交感知的隐私保护联邦学习机制，通过多跳传播模型量化间接隐私泄露，并采用两阶段Stackelberg博弈优化服务器激励策略和客户端的隐私预算选择。引入均值场估计器以减少隐私估计中的信息不对称。

Result: 理论证明了均值场估计器的存在性和收敛性，并推导了Stackelberg纳什均衡的闭式表达。实验表明，该方法在多种数据集上显著提升了客户端效用并降低了服务器成本。

Conclusion: 该机制在客户端效用与服务器成本之间取得平衡，同时保持模型性能，优于社交无关基准和其他考虑社交外部性的方法。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, thereby enhancing privacy and
facilitating collaboration among clients connected via social networks.
However, these social connections introduce privacy externalities: a client's
privacy loss depends not only on its privacy protection strategy but also on
the privacy decisions of others, propagated through the network via multi-hop
interactions. In this work, we propose a socially-aware privacy-preserving FL
mechanism that systematically quantifies indirect privacy leakage through a
multi-hop propagation model. We formulate the server-client interaction as a
two-stage Stackelberg game, where the server, as the leader, optimizes
incentive policies, and clients, as followers, strategically select their
privacy budgets, which determine their privacy-preserving levels by controlling
the magnitude of added noise. To mitigate information asymmetry in networked
privacy estimation, we introduce a mean-field estimator to approximate the
average external privacy risk. We theoretically prove the existence and
convergence of the fixed point of the mean-field estimator and derive
closed-form expressions for the Stackelberg Nash Equilibrium. Despite being
designed from a client-centric incentive perspective, our mechanism achieves
approximately-optimal social welfare, as revealed by Price of Anarchy (PoA)
analysis. Experiments on diverse datasets demonstrate that our approach
significantly improves client utilities and reduces server costs while
maintaining model performance, outperforming both Social-Agnostic (SA)
baselines and methods that account for social externalities.

</details>


### [164] [Conformal Set-based Human-AI Complementarity with Multiple Experts](https://arxiv.org/abs/2508.06997)
*Helbert Paat,Guohao Shen*

Main category: cs.LG

TL;DR: 研究提出一种利用符合预测集从多专家池中选择实例相关专家的贪心算法，提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 在多专家场景下，探索如何选择实例相关的专家子集，以提升人机协作分类性能。

Method: 提出基于符合预测集的贪心算法，动态选择专家子集进行分类。

Result: 在CIFAR-10H和ImageNet-16H数据集上，算法表现优于简单方法，接近最优子集。

Conclusion: 多专家动态选择能有效提升分类性能，贪心算法具有实用性和高效性。

Abstract: Decision support systems are designed to assist human experts in
classification tasks by providing conformal prediction sets derived from a
pre-trained model. This human-AI collaboration has demonstrated enhanced
classification performance compared to using either the model or the expert
independently. In this study, we focus on the selection of instance-specific
experts from a pool of multiple human experts, contrasting it with existing
research that typically focuses on single-expert scenarios. We characterize the
conditions under which multiple experts can benefit from the conformal sets.
With the insight that only certain experts may be relevant for each instance,
we explore the problem of subset selection and introduce a greedy algorithm
that utilizes conformal sets to identify the subset of expert predictions that
will be used in classifying an instance. This approach is shown to yield better
performance compared to naive methods for human subset selection. Based on real
expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation
study indicates that our proposed greedy algorithm achieves near-optimal
subsets, resulting in improved classification performance among multiple
experts.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [165] [VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions](https://arxiv.org/abs/2508.06757)
*Yash Garg,Saketh Bachu,Arindam Dutta,Rohit Lal,Sarosij Bose,Calvin-Khang Ta,M. Salman Asif,Amit Roy-Chowdhury*

Main category: cs.CV

TL;DR: 提出了一个名为VOccl3D的新型基准数据集，用于解决现有3D人体姿态估计方法在真实遮挡场景下的不足，并通过微调现有方法证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在遮挡场景下的3D人体姿态与形状估计缺乏真实性与多样性，需要更贴近现实的遮挡数据集以提升方法性能。

Method: 利用计算机图形渲染技术构建VOccl3D数据集，包含多样化遮挡场景和人体动作，并对CLIFF和BEDLAM-CLIFF等HPS方法进行微调。

Result: 微调后的方法在公开数据集及VOccl3D测试集上表现显著提升，且通过微调YOLO11提高了遮挡下的人体检测性能。

Conclusion: VOccl3D为未来遮挡场景下的研究提供了更真实的基准资源，提升了现有方法的鲁棒性。

Abstract: Human pose and shape (HPS) estimation methods have been extensively studied,
with many demonstrating high zero-shot performance on in-the-wild images and
videos. However, these methods often struggle in challenging scenarios
involving complex human poses or significant occlusions. Although some studies
address 3D human pose estimation under occlusion, they typically evaluate
performance on datasets that lack realistic or substantial occlusions, e.g.,
most existing datasets introduce occlusions with random patches over the human
or clipart-style overlays, which may not reflect real-world challenges. To
bridge this gap in realistic occlusion datasets, we introduce a novel benchmark
dataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and
shape annotations. Inspired by works such as AGORA and BEDLAM, we constructed
this dataset using advanced computer graphics rendering techniques,
incorporating diverse real-world occlusion scenarios, clothing textures, and
human motions. Additionally, we fine-tuned recent HPS methods, CLIFF and
BEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and
quantitative improvements across multiple public datasets, as well as on the
test split of our dataset, while comparing its performance with other
state-of-the-art methods. Furthermore, we leveraged our dataset to enhance
human detection performance under occlusion by fine-tuning an existing object
detector, YOLO11, thus leading to a robust end-to-end HPS estimation system
under occlusions. Overall, this dataset serves as a valuable resource for
future research aimed at benchmarking methods designed to handle occlusions,
offering a more realistic alternative to existing occlusion datasets. See the
Project page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/

</details>


### [166] [DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging](https://arxiv.org/abs/2508.06768)
*Noe Bertramo,Gabriel Duguey,Vivek Gopalakrishnan*

Main category: cs.CV

TL;DR: DiffUS是一种基于物理的可微分超声渲染器，能从MRI 3D扫描生成逼真的B模式超声图像，用于解决术中超声图像与术前MRI/CT对齐问题。


<details>
  <summary>Details</summary>
Motivation: 术中超声成像因噪声、伪影及与术前高分辨率MRI/CT对齐困难，限制了其应用。DiffUS旨在填补术前计划与术中引导间的差距。

Method: DiffUS首先通过机器学习将MRI 3D扫描转换为声阻抗体积，再用射线追踪模拟超声束传播，结合反射-透射方程，最后通过深度解析回波提取重建B模式图像。

Result: 在ReMIND数据集上的评估显示，DiffUS能从脑部MRI数据生成解剖准确的超声图像。

Conclusion: DiffUS作为一种可微分超声渲染器，为术中导航提供了新的工具，并可支持切片-体积配准等下游优化任务。

Abstract: Intraoperative ultrasound imaging provides real-time guidance during numerous
surgical procedures, but its interpretation is complicated by noise, artifacts,
and poor alignment with high-resolution preoperative MRI/CT scans. To bridge
the gap between reoperative planning and intraoperative guidance, we present
DiffUS, a physics-based, differentiable ultrasound renderer that synthesizes
realistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D
scans into acoustic impedance volumes using a machine learning approach. Next,
we simulate ultrasound beam propagation using ray tracing with coupled
reflection-transmission equations. DiffUS formulates wave propagation as a
sparse linear system that captures multiple internal reflections. Finally, we
reconstruct B-mode images via depth-resolved echo extraction across fan-shaped
acquisition geometry, incorporating realistic artifacts including speckle noise
and depth-dependent degradation. DiffUS is entirely implemented as
differentiable tensor operations in PyTorch, enabling gradient-based
optimization for downstream applications such as slice-to-volume registration
and volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates
DiffUS's ability to generate anatomically accurate ultrasound images from brain
MRI data.

</details>


### [167] [DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models](https://arxiv.org/abs/2508.07714)
*Licheng Zhang,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 提出一种半自动化流程，结合目标检测模型和大语言模型（LLM），以较低成本构建细粒度的多类别门检测数据集。


<details>
  <summary>Details</summary>
Motivation: 由于公开的多类别门检测数据集稀缺，需要一种高效方法构建此类数据集以支持建筑合规检查和室内场景理解等应用。

Method: 1. 使用深度学习目标检测模型统一检测门；2. 用LLM根据视觉和上下文特征分类；3. 人工校验确保标签质量。

Result: 显著降低标注成本，生成适用于平面图分析的数据集。

Conclusion: 结合深度学习和多模态推理，为复杂领域的数据集构建提供高效方法。

Abstract: Accurate detection and classification of diverse door types in floor plans
drawings is critical for multiple applications, such as building compliance
checking, and indoor scene understanding. Despite their importance, publicly
available datasets specifically designed for fine-grained multi-class door
detection remain scarce. In this work, we present a semi-automated pipeline
that leverages a state-of-the-art object detector and a large language model
(LLM) to construct a multi-class door detection dataset with minimal manual
effort. Doors are first detected as a unified category using a deep object
detection model. Next, an LLM classifies each detected instance based on its
visual and contextual features. Finally, a human-in-the-loop stage ensures
high-quality labels and bounding boxes. Our method significantly reduces
annotation cost while producing a dataset suitable for benchmarking neural
models in floor plan analysis. This work demonstrates the potential of
combining deep learning and multimodal reasoning for efficient dataset
construction in complex real-world domains.

</details>


### [168] [Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View](https://arxiv.org/abs/2508.06968)
*Ulas Gunes,Matias Turkulainen,Juho Kannala,Esa Rahtu*

Main category: cs.CV

TL;DR: 首次评估鱼眼镜头下的3D高斯散射方法（Fisheye-GS和3DGUT），覆盖超180度视野的真实图像，分析极端畸变的处理性能，并提出了基于深度的初始化策略。


<details>
  <summary>Details</summary>
Motivation: 研究鱼眼镜头下3D高斯散射方法在极端畸变场景中的表现，解决传统SfM初始化在强畸变下的失效问题。

Method: 通过比较Fisheye-GS和3DGUT在不同视野（200度、160度、120度）下的表现，并提出了基于UniK3D预测的深度初始化策略。

Result: Fisheye-GS在160度视野表现最佳，而3DGUT在全200度视野下保持稳定。UniK3D策略与SfM效果相当，适用于雾、眩光等复杂场景。

Conclusion: 鱼眼镜头下的3D高斯散射方法在宽角度3D重建中具有实用潜力，适用于稀疏且畸变严重的图像输入。

Abstract: We present the first evaluation of fisheye-based 3D Gaussian Splatting
methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180
degree. Our study covers both indoor and outdoor scenes captured with 200
degree fisheye cameras and analyzes how each method handles extreme distortion
in real world settings. We evaluate performance under varying fields of view
(200 degree, 160 degree, and 120 degree) to study the tradeoff between
peripheral distortion and spatial coverage. Fisheye-GS benefits from field of
view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable
across all settings and maintains high perceptual quality at the full 200
degree view. To address the limitations of SfM-based initialization, which
often fails under strong distortion, we also propose a depth-based strategy
using UniK3D predictions from only 2-3 fisheye images per scene. Although
UniK3D is not trained on real fisheye data, it produces dense point clouds that
enable reconstruction quality on par with SfM, even in difficult scenes with
fog, glare, or sky. Our results highlight the practical viability of
fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and
distortion-heavy image inputs.

</details>


### [169] [HiMat: DiT-based Ultra-High Resolution SVBRDF Generation](https://arxiv.org/abs/2508.07011)
*Zixiong Wang,Jian Yang,Yiwei Hu,Milos Hasan,Beibei Wang*

Main category: cs.CV

TL;DR: HiMat框架通过轻量级CrossStitch模块，高效生成4K分辨率SVBRDF，保持跨图一致性，不损坏DiT主干能力。


<details>
  <summary>Details</summary>
Motivation: 现有高分辨率文本生成图像模型（如基于DiT）难以高效生成多对齐SVBRDF图且保持一致性，需解决此挑战。

Method: 提出HiMat框架，引入轻量级CrossStitch模块捕获跨图依赖，保持DiT主干不变，实现高效4K SVBRDF生成。

Result: 实验显示HiMat能生成结构一致、高频细节丰富的4K SVBRDF，泛化能力适用于本征分解等任务。

Conclusion: HiMat为高效生成高质量SVBRDF提供了轻量级解决方案，扩展了DiT模型的应用范围。

Abstract: Creating highly detailed SVBRDFs is essential for 3D content creation. The
rise of high-resolution text-to-image generative models, based on diffusion
transformers (DiT), suggests an opportunity to finetune them for this task.
However, retargeting the models to produce multiple aligned SVBRDF maps instead
of just RGB images, while achieving high efficiency and ensuring consistency
across different maps, remains a challenge. In this paper, we introduce HiMat:
a memory- and computation-efficient diffusion-based framework capable of
generating native 4K-resolution SVBRDFs. A key challenge we address is
maintaining consistency across different maps in a lightweight manner, without
relying on training new VAEs or significantly altering the DiT backbone (which
would damage its prior capabilities). To tackle this, we introduce the
CrossStitch module, a lightweight convolutional module that captures inter-map
dependencies through localized operations. Its weights are initialized such
that the DiT backbone operation is unchanged before finetuning starts. HiMat
enables generation with strong structural coherence and high-frequency details.
Results with a large set of text prompts demonstrate the effectiveness of our
approach for 4K SVBRDF generation. Further experiments suggest generalization
to tasks such as intrinsic decomposition.

</details>


### [170] [PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation](https://arxiv.org/abs/2508.08179)
*Sihan Zhao,Zixuan Wang,Tianyu Luan,Jia Jia,Wentao Zhu,Jiebo Luo,Junsong Yuan,Nan Xi*

Main category: cs.CV

TL;DR: 本文提出了一种名为PP-Motion的新方法，通过物理标记和感知损失结合，评估人类运动生成的逼真度，在物理可行性和人类感知两方面均优于先前工作。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估运动生成逼真度时存在主观性和物理可行性的差距，需要一个更客观、细粒度的评估指标。

Method: 提出物理标记方法，计算运动对齐物理法则所需的最小修改；结合Pearson相关性损失和人类感知损失，训练PP-Motion模型。

Result: PP-Motion在物理和感知逼真度上均优于先前方法，实验结果验证了其有效性。

Conclusion: PP-Motion为运动生成提供了一个更全面、客观的评估指标，结合了物理可行性和人类感知的逼真度。

Abstract: Human motion generation has found widespread applications in AR/VR, film,
sports, and medical rehabilitation, offering a cost-effective alternative to
traditional motion capture systems. However, evaluating the fidelity of such
generated motions is a crucial, multifaceted task. Although previous approaches
have attempted at motion fidelity evaluation using human perception or physical
constraints, there remains an inherent gap between human-perceived fidelity and
physical feasibility. Moreover, the subjective and coarse binary labeling of
human perception further undermines the development of a robust data-driven
metric. We address these issues by introducing a physical labeling method. This
method evaluates motion fidelity by calculating the minimum modifications
needed for a motion to align with physical laws. With this approach, we are
able to produce fine-grained, continuous physical alignment annotations that
serve as objective ground truth. With these annotations, we propose PP-Motion,
a novel data-driven metric to evaluate both physical and perceptual fidelity of
human motion. To effectively capture underlying physical priors, we employ
Pearson's correlation loss for the training of our metric. Additionally, by
incorporating a human-based perceptual fidelity loss, our metric can capture
fidelity that simultaneously considers both human perception and physical
alignment. Experimental results demonstrate that our metric, PP-Motion, not
only aligns with physical laws but also aligns better with human perception of
motion fidelity than previous work.

</details>


### [171] [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086)
*Zhongqi Yang,Wenhang Ge,Yuqi Li,Jiaqi Chen,Haoyuan Li,Mengyin An,Fei Kang,Hua Xue,Baixin Xu,Yuyang Yin,Eric Li,Yang Liu,Yikai Wang,Hao-Xiang Guo,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-3D 是一个通过全景表示生成可探索 3D 世界的框架，结合了条件视频生成和全景 3D 重建，并在性能和范围上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在生成场景范围上的局限性，提升可探索 3D 世界的广度和质量。

Method: 1. 训练轨迹引导的全景视频扩散模型；2. 提出两种 3D 重建方法：前馈大范围重建和优化细节重建；3. 引入数据集 Matrix-Pano。

Result: 在全景视频生成和 3D 世界生成任务上达到最优性能。

Conclusion: Matrix-3D 框架通过全景表示和创新方法，显著提升了 3D 世界生成的覆盖范围和质量。

Abstract: Explorable 3D world generation from a single image or text prompt forms a
cornerstone of spatial intelligence. Recent works utilize video model to
achieve wide-scope and generalizable 3D world generation. However, existing
approaches often suffer from a limited scope in the generated scenes. In this
work, we propose Matrix-3D, a framework that utilize panoramic representation
for wide-coverage omnidirectional explorable 3D world generation that combines
conditional video generation and panoramic 3D reconstruction. We first train a
trajectory-guided panoramic video diffusion model that employs scene mesh
renders as condition, to enable high-quality and geometrically consistent scene
video generation. To lift the panorama scene video to 3D world, we propose two
separate methods: (1) a feed-forward large panorama reconstruction model for
rapid 3D scene reconstruction and (2) an optimization-based pipeline for
accurate and detailed 3D scene reconstruction. To facilitate effective
training, we also introduce the Matrix-Pano dataset, the first large-scale
synthetic collection comprising 116K high-quality static panoramic video
sequences with depth and trajectory annotations. Extensive experiments
demonstrate that our proposed framework achieves state-of-the-art performance
in panoramic video generation and 3D world generation. See more in
https://matrix-3d.github.io.

</details>


### [172] [FormCoach: Lift Smarter, Not Harder](https://arxiv.org/abs/2508.07501)
*Xiaoye Zuo,Nikos Athanasiou,Ginger Delmas,Yiming Huang,Xingyu Fu,Lingjie Liu*

Main category: cs.CV

TL;DR: FormCoach利用视觉语言模型（VLM）将普通摄像设备转化为实时交互式AI训练伙伴，提供姿势纠正，并发布了数据集和评估框架。


<details>
  <summary>Details</summary>
Motivation: 针对居家健身爱好者缺乏专业反馈的问题，FormCoach旨在通过AI技术填补这一空白。

Method: 开发了基于VLM的系统，通过摄像头实时检测并纠正动作错误，并构建了包含1700组专家标注视频的数据集。

Result: 基准测试显示，当前VLM与人类教练水平仍有显著差距，但展示了AI在运动分析中的潜力。

Conclusion: FormCoach通过人机协作的创新方式，为具身AI开辟了新方向。

Abstract: Good form is the difference between strength and strain, yet for the
fast-growing community of at-home fitness enthusiasts, expert feedback is often
out of reach. FormCoach transforms a simple camera into an always-on,
interactive AI training partner, capable of spotting subtle form errors and
delivering tailored corrections in real time, leveraging vision-language models
(VLMs). We showcase this capability through a web interface and benchmark
state-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference
video pairs spanning 22 strength and mobility exercises. To accelerate research
in AI-driven coaching, we release both the dataset and an automated,
rubric-based evaluation pipeline, enabling standardized comparison across
models. Our benchmarks reveal substantial gaps compared to human-level
coaching, underscoring both the challenges and opportunities in integrating
nuanced, context-aware movement analysis into interactive AI systems. By
framing form correction as a collaborative and creative process between humans
and machines, FormCoach opens a new frontier in embodied AI.

</details>


### [173] [Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection](https://arxiv.org/abs/2508.07923)
*Jakub Binda,Valentina Paneta,Vasileios Eleftheriadis,Hongkyou Chung,Panagiotis Papadimitroulas,Neo Christopher Chung*

Main category: cs.CV

TL;DR: 提出了一种混合异常检测框架，用于提高核医学中生成式AI的可靠性和实时质量控制。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在生物医学影像中潜力巨大，但需要确保模型行为的可靠性和安全性。

Method: 开发并实施了一种混合异常检测框架，应用于Pose2Xray和DosimetrEYE两个生成式AI系统。

Result: 该框架提高了系统的可靠性，减少了人工干预，并支持实时质量控制。

Conclusion: 通过增强鲁棒性、可扩展性和合规性，该方法提升了生成式AI在临床前环境中的工业可行性。

Abstract: Generative AI holds great potentials to automate and enhance data synthesis
in nuclear medicine. However, the high-stakes nature of biomedical imaging
necessitates robust mechanisms to detect and manage unexpected or erroneous
model behavior. We introduce development and implementation of a hybrid anomaly
detection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.
Two applications are demonstrated: Pose2Xray, which generates synthetic X-rays
from photographic mouse images, and DosimetrEYE, which estimates 3D radiation
dose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)
enhances reliability, reduces manual oversight, and supports real-time quality
control. This approach strengthens the industrial viability of GenAI in
preclinical settings by increasing robustness, scalability, and regulatory
compliance.

</details>


### [174] [The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility](https://arxiv.org/abs/2508.07989)
*Xiantao Zhang*

Main category: cs.CV

TL;DR: 论文分析了多模态大语言模型（MLLMs）在帮助盲人和视障群体（BVI）时的关键问题——‘Escalator Problem’，即无法感知电梯运动方向的缺陷。


<details>
  <summary>Details</summary>
Motivation: MLLMs作为辅助技术潜力巨大，但在实际应用中存在‘Implicit Motion Blindness’问题，导致用户信任度降低。

Method: 通过分析当前视频理解的帧采样范式，指出其无法处理连续低速运动的问题。

Result: 研究揭示了这一缺陷的深层原因及其对用户信任的影响。

Conclusion: 呼吁从语义识别转向物理感知，并开发以用户为中心的新基准测试。

Abstract: Multimodal Large Language Models (MLLMs) hold immense promise as assistive
technologies for the blind and visually impaired (BVI) community. However, we
identify a critical failure mode that undermines their trustworthiness in
real-world applications. We introduce the Escalator Problem -- the inability of
state-of-the-art models to perceive an escalator's direction of travel -- as a
canonical example of a deeper limitation we term Implicit Motion Blindness.
This blindness stems from the dominant frame-sampling paradigm in video
understanding, which, by treating videos as discrete sequences of static
images, fundamentally struggles to perceive continuous, low-signal motion. As a
position paper, our contribution is not a new model but rather to: (I) formally
articulate this blind spot, (II) analyze its implications for user trust, and
(III) issue a call to action. We advocate for a paradigm shift from purely
semantic recognition towards robust physical perception and urge the
development of new, human-centered benchmarks that prioritize safety,
reliability, and the genuine needs of users in dynamic environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [175] [Pushing the Envelope of LLM Inference on AI-PC](https://arxiv.org/abs/2508.06753)
*Evangelos Georganas,Dhiraj Kalamkar,Alexander Heinecke*

Main category: cs.AI

TL;DR: 该论文针对超低比特LLM模型（1/1.58/2-bit）在资源受限环境中的高效部署问题，提出优化的微内核和运行时框架，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 超低比特LLM模型在资源受限环境中具有巨大潜力，但现有运行时系统的计算效率尚未充分优化。

Method: 设计并实现针对现代CPU优化的1-bit和2-bit微内核，并将其集成到PyTorch-TPP框架中。

Result: 2-bit模型推理性能比当前最佳运行时bitnet.cpp提升2.2倍，比16-bit模型推理快7倍。

Conclusion: 优化后的运行时为AI PC和边缘设备中高效部署超低比特LLM模型提供了新途径。

Abstract: The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the
perplexity and end-task performance of their full-precision counterparts using
the same model size, is ushering in a new era of LLM inference for
resource-constrained environments such as edge devices and AI PCs. While these
quantization advances promise models that are more cost-effective in terms of
latency, memory, throughput, and energy consumption, the computational
efficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)
used to deploy them remains underexplored. In this work, we take a bottom-up
approach: we first design and implement 1-bit and 2-bit microkernels optimized
for modern CPUs, achieving peak computational efficiency across a variety of
CPU platforms. We integrate these microkernels into a state-of-the-art LLM
inference framework, namely PyTorch-TPP, and present end-to-end inference
results with 2-bit models that outperform the current SOTA runtime bitnet.cpp
by up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model
inference. Our optimized runtime advances the state of LLM inference on AI PCs
and edge devices, paving the way for efficient deployment of ultra-low-bit LLM
models.

</details>


### [176] [Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets](https://arxiv.org/abs/2508.06706)
*Jaikrishna Manojkumar Patil,Nathaniel Lee,Al Mehdi Saadat Chowdhury,YooJung Choi,Paulo Shakarian*

Main category: cs.AI

TL;DR: 论文提出了一种基于规则上下文和概率分布的知识图谱补全方法，显著减少了规则数量并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于规则的知识图谱补全方法需要大量规则才能达到高性能，但过多的规则会降低可解释性。本文旨在通过发现规则上下文并利用概率分布来优化规则使用，从而提升效率和性能。

Method: 从训练数据中发现规则上下文，并利用概率电路学习这些规则的概率分布，从而减少规则数量，同时保持高性能。

Result: 实验表明，新方法减少了70-96%的规则数量，性能最高提升31倍，且保留了基线性能的91%。

Conclusion: 该方法在减少规则数量的同时保持了高性能和可解释性，并在8个标准数据集上验证了其有效性。

Abstract: Rule-based methods for knowledge graph completion provide explainable results
but often require a significantly large number of rules to achieve competitive
performance. This can hinder explainability due to overwhelmingly large rule
sets. We discover rule contexts (meaningful subsets of rules that work
together) from training data and use learned probability distribution (i.e.
probabilistic circuits) over these rule contexts to more rapidly achieve
performance of the full rule set. Our approach achieves a 70-96% reduction in
number of rules used while outperforming baseline by up to 31$\times$ when
using equivalent minimal number of rules and preserves 91% of peak baseline
performance even when comparing our minimal rule sets against baseline's full
rule sets. We show that our framework is grounded in well-known semantics of
probabilistic logic, does not require independence assumptions, and that our
tractable inference procedure provides both approximate lower bounds and exact
probability of a given query. The efficacy of our method is validated by
empirical studies on 8 standard benchmark datasets where we show competitive
performance by using only a fraction of the rules required by AnyBURL's
standard inference method, the current state-of-the-art for rule-based
knowledge graph completion. This work may have further implications for general
probabilistic reasoning over learned sets of rules.

</details>


### [177] [GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning](https://arxiv.org/abs/2508.06716)
*Blair Johnson,Clayton Kerce,Faramarz Fekri*

Main category: cs.AI

TL;DR: GLIDR是一种可微分的规则学习方法，通过更灵活的语法和推理算法提升了知识图谱任务中的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有可微分归纳逻辑编程(ILP)方法在链式规则结构假设下性能受限，影响了结果的表现力和可解释性。

Method: GLIDR引入了支持分支和循环的灵活规则语法，使用可微分消息传递推理算法，并能从模型权重中提取显式逻辑规则。

Result: 在知识图谱完成任务上，GLIDR显著优于现有方法，甚至可与嵌入方法竞争，且对训练数据噪声具有强鲁棒性。

Conclusion: GLIDR不仅提升了规则学习的性能，还能与深度神经网络结合进行端到端优化，扩展了应用场景。

Abstract: Differentiable inductive logic programming (ILP) techniques have proven
effective at finding approximate rule-based solutions to link prediction and
node classification problems on knowledge graphs; however, the common
assumption of chain-like rule structure can hamper the performance and
interpretability of existing approaches. We introduce GLIDR, a differentiable
rule learning method that models the inference of logic rules with more
expressive syntax than previous methods. GLIDR uses a differentiable message
passing inference algorithm that generalizes previous chain-like rule learning
methods to allow rules with features like branches and cycles. GLIDR has a
simple and expressive rule search space which is parameterized by a limit on
the maximum number of free variables that may be included in a rule. Explicit
logic rules can be extracted from the weights of a GLIDR model for use with
symbolic solvers. We demonstrate that GLIDR can significantly outperform
existing rule learning methods on knowledge graph completion tasks and even
compete with embedding methods despite the inherent disadvantage of being a
structure-only prediction method. We show that rules extracted from GLIDR
retain significant predictive performance, and that GLIDR is highly robust to
training data noise. Finally, we demonstrate that GLIDR can be chained with
deep neural networks and optimized end-to-end for rule learning on arbitrary
data modalities.

</details>


### [178] [MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA](https://arxiv.org/abs/2508.07022)
*Shengtao Wen,Haodong Chen,Yadong Wang,Zhongying Pan,Xiang Chen,Yu Tian,Bo Qian,Dong Liang,Sheng-Jun Huang*

Main category: cs.AI

TL;DR: 该论文提出了MultiMedEdit，首个针对临床多模态任务中知识编辑（KE）评估的基准，揭示了当前方法在通用性和长尾推理上的不足，并为未来开发临床稳健的KE技术奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 尽管知识编辑在通用领域和医学问答任务中表现出色，但在多模态医学场景中的应用尚未得到充分研究，亟需支持视觉推理的临床决策。

Method: 研究提出了MultiMedEdit框架，涵盖理解和推理任务类型，定义了三维度指标（可靠性、通用性和局部性），并支持跨范式比较。

Result: 实验表明，当前方法在通用性、长尾推理和临床工作流中表现不佳，同时揭示了不同KE范式在实用部署中的权衡。

Conclusion: MultiMedEdit不仅揭示了当前方法的局限性，还为未来开发临床稳健的KE技术提供了基础。

Abstract: Knowledge editing (KE) provides a scalable approach for updating factual
knowledge in large language models without full retraining. While previous
studies have demonstrated effectiveness in general domains and medical QA
tasks, little attention has been paid to KE in multimodal medical scenarios.
Unlike text-only settings, medical KE demands integrating updated knowledge
with visual reasoning to support safe and interpretable clinical decisions. To
address this gap, we propose MultiMedEdit, the first benchmark tailored to
evaluating KE in clinical multimodal tasks. Our framework spans both
understanding and reasoning task types, defines a three-dimensional metric
suite (reliability, generality, and locality), and supports cross-paradigm
comparisons across general and domain-specific models. We conduct extensive
experiments under single-editing and lifelong-editing settings. Results suggest
that current methods struggle with generalization and long-tail reasoning,
particularly in complex clinical workflows. We further present an efficiency
analysis (e.g., edit latency, memory footprint), revealing practical trade-offs
in real-world deployment across KE paradigms. Overall, MultiMedEdit not only
reveals the limitations of current approaches but also provides a solid
foundation for developing clinically robust knowledge editing techniques in the
future.

</details>


### [179] [Best-Effort Policies for Robust Markov Decision Processes](https://arxiv.org/abs/2508.07790)
*Alessandro Abate,Thom Badings,Giuseppe De Giacomo,Francesco Fabiano*

Main category: cs.AI

TL;DR: 本文研究了鲁棒马尔可夫决策过程（RMDPs）的通用性，提出了最优鲁棒最佳努力（ORBE）策略，以在多最优策略中选择表现更好的策略。


<details>
  <summary>Details</summary>
Motivation: 现有的RMDPs方法仅关注最坏情况下的表现，忽略了不同转移概率下的性能差异，导致策略选择缺乏区分度。

Method: 结合博弈论中的优势与最佳努力概念，提出ORBE策略，要求策略在最坏情况下表现最优且在其他情况下尽可能好。

Result: 证明了ORBE策略的存在性，描述了其结构，并提出了一种计算算法，实验验证了其可行性。

Conclusion: ORBE策略为多最优鲁棒策略提供了有原则的选择标准，补充了现有方法的不足。

Abstract: We study the common generalization of Markov decision processes (MDPs) with
sets of transition probabilities, known as robust MDPs (RMDPs). A standard goal
in RMDPs is to compute a policy that maximizes the expected return under an
adversarial choice of the transition probabilities. If the uncertainty in the
probabilities is independent between the states, known as s-rectangularity,
such optimal robust policies can be computed efficiently using robust value
iteration. However, there might still be multiple optimal robust policies,
which, while equivalent with respect to the worst-case, reflect different
expected returns under non-adversarial choices of the transition probabilities.
Hence, we propose a refined policy selection criterion for RMDPs, drawing
inspiration from the notions of dominance and best-effort in game theory.
Instead of seeking a policy that only maximizes the worst-case expected return,
we additionally require the policy to achieve a maximal expected return under
different (i.e., not fully adversarial) transition probabilities. We call such
a policy an optimal robust best-effort (ORBE) policy. We prove that ORBE
policies always exist, characterize their structure, and present an algorithm
to compute them with a small overhead compared to standard robust value
iteration. ORBE policies offer a principled tie-breaker among optimal robust
policies. Numerical experiments show the feasibility of our approach.

</details>


### [180] [KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations](https://arxiv.org/abs/2508.07834)
*Mubaris Nadeem,Johannes Zenkert,Lisa Bender,Christian Weber,Madjid Fathi*

Main category: cs.AI

TL;DR: 本文提出了一种基于知识图谱和人工智能的创新知识管理方法，用于支持急救人员在紧急情况下快速提供个性化医疗建议。


<details>
  <summary>Details</summary>
Motivation: 全球救援需求的增加以及急救人员在时间紧迫情况下难以充分利用专业知识的需求，推动了本文的研究。

Method: 采用知识图谱作为中央知识表示，并结合人工智能技术对急救场景进行预识别，以提供智能治疗建议。

Result: 该方法能够为急救人员提供实时、经过计算和评估的知识，从而优化紧急医疗处理。

Conclusion: 通过知识图谱和人工智能的结合，本文提出的方法能够显著提升急救效率和治疗质量。

Abstract: Over the years, the need for rescue operations throughout the world has
increased rapidly. Demographic changes and the resulting risk of injury or
health disorders form the basis for emergency calls. In such scenarios, first
responders are in a rush to reach the patient in need, provide first aid, and
save lives. In these situations, they must be able to provide personalized and
optimized healthcare in the shortest possible time and estimate the patients
condition with the help of freshly recorded vital data in an emergency
situation. However, in such a timedependent situation, first responders and
medical experts cannot fully grasp their knowledge and need assistance and
recommendation for further medical treatments. To achieve this, on the spot
calculated, evaluated, and processed knowledge must be made available to
improve treatments by first responders. The Knowledge Graph presented in this
article as a central knowledge representation provides first responders with an
innovative knowledge management that enables intelligent treatment
recommendations with an artificial intelligence-based pre-recognition of the
situation.

</details>


### [181] [Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis](https://arxiv.org/abs/2508.06668)
*Jessie Galasso*

Main category: cs.AI

TL;DR: 本文总结了形式概念分析（FCA）在变异性分析中的关键性质及其应用方法。


<details>
  <summary>Details</summary>
Motivation: FCA虽然适合变异性提取和分析，但由于其数学基础文献的复杂性，如何利用其性质进行变异性相关任务并不直观。

Method: 收集并整理了FCA框架中与变异性分析相关的关键性质，并探讨了如何利用这些性质解释概念结构中的变异性信息。

Result: 明确了FCA在变异性分析中的实用性及其性质的应用方式。

Conclusion: 通过系统总结FCA的性质，为变异性分析提供了更清晰的指导和工具。

Abstract: Formal Concept Analysis (FCA) is a mathematical framework for knowledge
representation and discovery. It performs a hierarchical clustering over a set
of objects described by attributes, resulting in conceptual structures in which
objects are organized depending on the attributes they share. These conceptual
structures naturally highlight commonalities and variabilities among similar
objects by categorizing them into groups which are then arranged by similarity,
making it particularly appropriate for variability extraction and analysis.
Despite the potential of FCA, determining which of its properties can be
leveraged for variability-related tasks (and how) is not always
straightforward, partly due to the mathematical orientation of its foundational
literature. This paper attempts to bridge part of this gap by gathering a
selection of properties of the framework which are essential to variability
analysis, and how they can be used to interpret diverse variability information
within the resulting conceptual structures.

</details>


### [182] [CP-Agent: Agentic Constraint Programming](https://arxiv.org/abs/2508.07468)
*Stefan Szeider*

Main category: cs.AI

TL;DR: 提出了一种无需固定流程的纯代理策略，用于将自然语言问题描述转化为约束模型，通过结合通用编码工具和领域知识提示，成功解决了所有101个基准问题。


<details>
  <summary>Details</summary>
Motivation: 约束编程中，将自然语言问题描述转换为形式化约束模型需要深厚的领域和建模知识，现有固定流程方法在许多基准问题上表现不佳。

Method: 开发了基于ReAct原则的通用Python编码代理，利用持久IPython内核进行状态化代码执行和迭代开发，通过项目提示注入领域知识，而非嵌入代理架构。

Result: 该架构在仅几百行代码下，成功解决了CP-Bench基准集中的所有101个问题。

Conclusion: 约束建模任务依赖通用编码工具和提示中的领域知识，而非专用代理架构或预定义流程。

Abstract: Translating natural language problem descriptions into formal constraint
models remains a fundamental challenge in constraint programming, requiring
deep expertise in both the problem domain and modeling frameworks. Previous
approaches to automating this translation have employed fixed workflows with
predetermined modeling steps, failing on a significant number of benchmark
problems. We present a new approach using a pure agentic strategy without any
fixed pipeline. We developed a general-purpose Python coding agent based on the
ReAct (Reason and Act) principle, utilizing a persistent IPython kernel for
stateful code execution and iterative development. Rather than embedding
constraint programming logic into the agent architecture, domain-specific
expertise is injected solely through a carefully crafted project prompt. The
agent combines this prompt-encoded knowledge with access to file operations and
code execution tools, enabling it to test hypotheses, debug failures, and
verify solutions dynamically. Implemented in just a few hundred lines of code,
this architecture successfully solves all 101 problems of the CP-Bench
constraint programming benchmark set. The results suggest that constraint
modeling tasks require the combination of general coding tools and domain
expertise encoded in prompts, rather than specialized agent architectures or
predefined workflows.

</details>


### [183] [Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method](https://arxiv.org/abs/2508.07586)
*Wenjing Zhang,Ye Hu,Tao Luo,Zhilong Zhang,Mingzhe Chen*

Main category: cs.AI

TL;DR: 本文研究了一种新颖的隐蔽语义通信框架，通过优化语义信息传输和干扰策略提升隐私和传输质量。


<details>
  <summary>Details</summary>
Motivation: 研究如何在存在攻击者的环境中，保护语义通信的隐私并提高传输质量。

Method: 提出了优先采样辅助的双延迟深度确定性策略梯度算法，优化语义信息和发射功率的分配。

Result: 仿真结果表明，相比传统强化学习方法，隐私和传输质量分别提升77.8%和14.3%。

Conclusion: 所提算法能有效提升隐蔽语义通信的性能，适用于实际应用。

Abstract: In this paper, a novel covert semantic communication framework is
investigated. Within this framework, a server extracts and transmits the
semantic information, i.e., the meaning of image data, to a user over several
time slots. An attacker seeks to detect and eavesdrop the semantic transmission
to acquire details of the original image. To avoid data meaning being
eavesdropped by an attacker, a friendly jammer is deployed to transmit jamming
signals to interfere the attacker so as to hide the transmitted semantic
information. Meanwhile, the server will strategically select time slots for
semantic information transmission. Due to limited energy, the jammer will not
communicate with the server and hence the server does not know the transmit
power of the jammer. Therefore, the server must jointly optimize the semantic
information transmitted at each time slot and the corresponding transmit power
to maximize the privacy and the semantic information transmission quality of
the user. To solve this problem, we propose a prioritised sampling assisted
twin delayed deep deterministic policy gradient algorithm to jointly determine
the transmitted semantic information and the transmit power per time slot
without the communications between the server and the jammer. Compared to
standard reinforcement learning methods, the propose method uses an additional
Q network to estimate Q values such that the agent can select the action with a
lower Q value from the two Q networks thus avoiding local optimal action
selection and estimation bias of Q values. Simulation results show that the
proposed algorithm can improve the privacy and the semantic information
transmission quality by up to 77.8% and 14.3% compared to the traditional
reinforcement learning methods.

</details>


### [184] [DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning](https://arxiv.org/abs/2508.06972)
*Dan Ivanov,Tristan Freiberg,Haruna Isah*

Main category: cs.AI

TL;DR: DSperse 是一个模块化框架，用于分布式机器学习推理，并通过战略性的加密验证实现信任最小化。


<details>
  <summary>Details</summary>
Motivation: 为了解决全模型电路化的高成本和僵化问题，DSperse 提出了一种灵活的选择性验证方法，仅验证关键子计算部分（称为“切片”）。

Method: DSperse 允许针对性地验证模型推理流水线中的部分或全部子计算，利用审计、复制或经济激励确保全局一致性。

Result: 实验结果表明，DSperse 在内存使用、运行时和电路行为方面表现出色，支持灵活的验证策略。

Conclusion: DSperse 通过灵活的验证边界设计，为多样化的部署需求提供了可扩展的、针对性的验证解决方案。

Abstract: DSperse is a modular framework for distributed machine learning inference
with strategic cryptographic verification. Operating within the emerging
paradigm of distributed zero-knowledge machine learning, DSperse avoids the
high cost and rigidity of full-model circuitization by enabling targeted
verification of strategically chosen subcomputations. These verifiable
segments, or "slices", may cover part or all of the inference pipeline, with
global consistency enforced through audit, replication, or economic incentives.
This architecture supports a pragmatic form of trust minimization, localizing
zero-knowledge proofs to the components where they provide the greatest value.
We evaluate DSperse using multiple proving systems and report empirical results
on memory usage, runtime, and circuit behavior under sliced and unsliced
configurations. By allowing proof boundaries to align flexibly with the model's
logical structure, DSperse supports scalable, targeted verification strategies
suited to diverse deployment needs.

</details>


### [185] [EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration](https://arxiv.org/abs/2508.07671)
*Mohamed Rayan Barhdadi,Mehmet Tuncel,Erchin Serpedin,Hasan Kurban*

Main category: cs.AI

TL;DR: EMPATHIA是一个多智能体框架，专注于难民整合的文化、情感和伦理维度，通过三个模块（SEED、RISE、THRIVE）实现透明决策和可解释建议。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在难民整合中过于关注就业等狭隘目标，忽视了文化、情感和伦理等长期成功的关键维度。

Method: 基于Kegan的理论，EMPATHIA分解整合过程为SEED、RISE和THRIVE三个模块，其中SEED通过三个专业智能体（情感、文化、伦理）进行透明决策。

Result: 在UN Kakuma数据集上验证，实现了87.4%的收敛性和可解释性评估，适用于五个东道国的6,359名难民。

Conclusion: EMPATHIA通过增强而非替代人类专业知识，为多价值协调的AI驱动任务提供了通用框架。

Abstract: Current AI approaches to refugee integration optimize narrow objectives such
as employment and fail to capture the cultural, emotional, and ethical
dimensions critical for long-term success. We introduce EMPATHIA (Enriched
Multimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),
a multi-agent framework addressing the central Creative AI question: how do we
preserve human dignity when machines participate in life-altering decisions?
Grounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes
integration into three modules: SEED (Socio-cultural Entry and Embedding
Decision) for initial placement, RISE (Rapid Integration and Self-sufficiency
Engine) for early independence, and THRIVE (Transcultural Harmony and
Resilience through Integrated Values and Engagement) for sustained outcomes.
SEED employs a selector-validator architecture with three specialized agents -
emotional, cultural, and ethical - that deliberate transparently to produce
interpretable recommendations. Experiments on the UN Kakuma dataset (15,026
individuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and
implementation on 6,359 working-age refugees (15+) with 150+ socioeconomic
variables achieved 87.4% validation convergence and explainable assessments
across five host countries. EMPATHIA's weighted integration of cultural,
emotional, and ethical factors balances competing value systems while
supporting practitioner-AI collaboration. By augmenting rather than replacing
human expertise, EMPATHIA provides a generalizable framework for AI-driven
allocation tasks where multiple values must be reconciled.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [186] [MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer](https://arxiv.org/abs/2508.07817)
*Tao Tang,Chengxu Yang*

Main category: eess.IV

TL;DR: 论文提出了一种结合多尺度卷积和Transformer架构的医学图像自适应去噪模型（MI-ND），通过噪声感知驱动通道-空间注意力调节和跨模态特征融合，显著提升了图像质量和下游诊断任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像质量对临床诊断至关重要，但由于低剂量扫描、设备限制和成像伪影等因素，图像常受非均匀噪声干扰，影响结构识别和病变检测。

Method: 提出了MI-ND模型，结合多尺度卷积和Transformer，引入噪声水平估计器（NLE）和噪声自适应注意力模块（NAAB），实现噪声感知驱动的通道-空间注意力调节和跨模态特征融合。

Result: 在多模态公共数据集上的实验表明，该方法在PSNR、SSIM和LPIPS等图像质量指标上显著优于对比方法，并提升了下游诊断任务的F1分数和ROC-AUC值。

Conclusion: 该模型在结构恢复、诊断敏感性和跨模态鲁棒性方面表现突出，为医学图像增强和AI辅助诊疗提供了有效解决方案，具有实用价值和推广潜力。

Abstract: The core role of medical images in disease diagnosis makes their quality
directly affect the accuracy of clinical judgment. However, due to factors such
as low-dose scanning, equipment limitations and imaging artifacts, medical
images are often accompanied by non-uniform noise interference, which seriously
affects structure recognition and lesion detection. This paper proposes a
medical image adaptive denoising model (MI-ND) that integrates multi-scale
convolutional and Transformer architecture, introduces a noise level estimator
(NLE) and a noise adaptive attention module (NAAB), and realizes
channel-spatial attention regulation and cross-modal feature fusion driven by
noise perception. Systematic testing is carried out on multimodal public
datasets. Experiments show that this method significantly outperforms the
comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,
and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing
strong prac-tical value and promotional potential. The model has outstanding
benefits in structural recovery, diagnostic sensitivity, and cross-modal
robustness, and provides an effective solution for medical image enhancement
and AI-assisted diagnosis and treatment.

</details>


### [187] [Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping](https://arxiv.org/abs/2508.07760)
*Maximilian Kromer,Panagiotis Agrafiotis,Begüm Demir*

Main category: eess.IV

TL;DR: 论文介绍了一个名为Sea-Undistort的综合合成数据集，用于解决浅水区图像测深中的光学失真问题。


<details>
  <summary>Details</summary>
Motivation: 浅水区图像测深因动态水面和水柱性质引起的光学失真（如波浪、散射和太阳光晕）而困难。

Method: 使用Blender生成1200对512x512的水下场景图像（失真和未失真），并训练轻量级扩散模型进行恢复。

Result: 模型在真实航空数据中表现优异，提高了海底数字表面模型的完整性，减少了测深误差。

Conclusion: Sea-Undistort数据集和模型为浅水区图像测深提供了有效的解决方案。

Abstract: Accurate image-based bathymetric mapping in shallow waters remains
challenging due to the complex optical distortions such as wave induced
patterns, scattering and sunglint, introduced by the dynamic water surface, the
water column properties, and solar illumination. In this work, we introduce
Sea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512
through-water scenes rendered in Blender. Each pair comprises a distortion-free
and a distorted view, featuring realistic water effects such as sun glint,
waves, and scattering over diverse seabeds. Accompanied by per-image metadata
such as camera parameters, sun position, and average depth, Sea-Undistort
enables supervised training that is otherwise infeasible in real environments.
We use Sea-Undistort to benchmark two state-of-the-art image restoration
methods alongside an enhanced lightweight diffusion-based framework with an
early-fusion sun-glint mask. When applied to real aerial data, the enhanced
diffusion model delivers more complete Digital Surface Models (DSMs) of the
seabed, especially in deeper areas, reduces bathymetric errors, suppresses
glint and scattering, and crisply restores fine seabed details. Dataset,
weights, and code are publicly available at
https://www.magicbathy.eu/Sea-Undistort.html.

</details>


### [188] [Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images](https://arxiv.org/abs/2508.07875)
*Shuo Han,Ahmed Karam Eldaly,Solomon Sunday Oyelere*

Main category: eess.IV

TL;DR: 该论文提出了一种人机协同的深度学习系统，用于乳腺癌的诊断，通过迭代反馈提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 提升浸润性导管癌（IDC）早期诊断的精确性和效率，结合医生专业知识和人工智能技术。

Method: 采用EfficientNetV2S模型进行初步诊断，医生修正误分类图像并通过反馈循环优化模型。

Result: 模型准确率达93.65%，人机协同系统进一步提升了诊断精度。

Conclusion: 人机协同方法为AI辅助医疗诊断提供了高效、准确的新方向。

Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,
and early, accurate diagnosis is critical to improving patient survival rates
by guiding treatment decisions. Combining medical expertise with artificial
intelligence (AI) holds significant promise for enhancing the precision and
efficiency of IDC detection. In this work, we propose a human-in-the-loop
(HITL) deep learning system designed to detect IDC in histopathology images.
The system begins with an initial diagnosis provided by a high-performance
EfficientNetV2S model, offering feedback from AI to the human expert. Medical
professionals then review the AI-generated results, correct any misclassified
images, and integrate the revised labels into the training dataset, forming a
feedback loop from the human back to the AI. This iterative process refines the
model's performance over time. The EfficientNetV2S model itself achieves
state-of-the-art performance compared to existing methods in the literature,
with an overall accuracy of 93.65\%. Incorporating the human-in-the-loop system
further improves the model's accuracy using four experimental groups with
misclassified images. These results demonstrate the potential of this
collaborative approach to enhance AI performance in diagnostic systems. This
work contributes to advancing automated, efficient, and highly accurate methods
for IDC detection through human-AI collaboration, offering a promising
direction for future AI-assisted medical diagnostics.

</details>
