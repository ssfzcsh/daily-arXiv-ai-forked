<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 14]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 3]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 6]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [CORE: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks](https://arxiv.org/abs/2507.05269)
*Danning Xie,Mingwei Zheng,Xuwei Liu,Jiannan Wang,Chengpeng Wang,Lin Tan,Xiangyu Zhang*

Main category: cs.SE

TL;DR: CoRe是一个高质量、人工验证的基准测试，用于评估大型语言模型（LLM）在静态分析任务中的表现，揭示其在深层语义推理和多步推理中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要评估端到端结果（如代码修复或生成），但未充分探索LLM对程序语义推理的能力。

Method: 提出CoRe基准测试，包括12,553个任务实例，覆盖C/C++、Java和Python，采用语义感知的多样性采样策略。

Result: 评估10个主流LLM发现，它们在识别依赖关系上表现良好，但在深层语义理解和多步推理上仍有困难。

Conclusion: 定性分析揭示了复杂控制结构和反向依赖模式是主要挑战，为提升LLM的代码推理能力提供了见解。

Abstract: Large language models (LLMs) have been widely adopted across diverse software
engineering domains, such as code generation, program repair, and vulnerability
detection. These applications require understanding beyond surface-level code
patterns: value propagation, control flow, and interdependence between program
elements. However, existing benchmarks primarily evaluate end-to-end outcomes,
such as whether code is correctly repaired or generated, leaving the models
ability for program semantic reasoning underexplored. This work presents CoRe,
a high-quality, human-verified benchmark designed to evaluate LLMs on
fundamental static analysis tasks. CoRe includes 12,553 task instances spanning
data dependency, control dependency, and information flow across programs
written in C/C++, Java, and Python. To ensure semantic diversity and reasoning
complexity, we propose a semantics-aware diverse sampling strategy that selects
targets and task instances based on structural coverage and dependency depth.
We evaluate 10 mainstream LLMs and show that, while they perform well at
identifying dependencies, models still struggle with tasks that require deeper
semantic understanding and multi-step reasoning. We further conduct qualitative
analyses to uncover key challenges, such as complex control structures and
backward dependency patterns, offering insights into improving LLMs code
reasoning capabilities.

</details>


### [2] [Open Source, Hidden Costs: A Systematic Literature Review on OSS License Management](https://arxiv.org/abs/2507.05270)
*Boyuan Li,Chengwei Liu,Lingling Fan,Sen Chen,Zhenlin Zhang,Zheli Liu*

Main category: cs.SE

TL;DR: 现代软件开发中集成第三方软件组件是常见做法，虽然高效且创新，但也伴随软件许可风险。本文通过80篇相关文献的系统综述，研究许可识别、风险评估与缓解，探讨未来研究方向与实用建议。


<details>
  <summary>Details</summary>
Motivation: 揭示开源软件（OSS）许可风险的严峻挑战，探索未来研究方向，为学术界和产业界提供系统化管理软件许可风险的建议。

Method: 对80篇精选的OSS许可相关文献进行系统性综述（SLR），将研究分为许可识别、风险评估和风险缓解三类。

Result: 总结了现有解决方案的局限性，提出了未来研究方向和实践建议。

Conclusion: 本文旨在缩小学术界与产业界的差距，加速软件工程社区对合法软件风险的生态系统治理。

Abstract: Integrating third-party software components is a common practice in modern
software development, offering significant advantages in terms of efficiency
and innovation. However, this practice is fraught with risks related to
software licensing. A lack of understanding may lead to disputes, which can
pose serious legal and operational challenges. To these ends, both academia and
industry have conducted various investigations and proposed solutions and tools
to deal with these challenges. However, significant limitations still remain.
Moreover, the rapid evolution of open-source software (OSS) licenses, as well
as the rapidly incorporated generative software engineering techniques, such as
large language models for code (CodeLLMs), are placing greater demands on the
systematic management of software license risks. To unveil the severe
challenges and explore possible future directions, we conduct the first
systematic literature review (SLR) on 80 carefully selected OSS license-related
papers, classifying existing research into three key categories, i.e., license
identification, license risk assessment, and license risk mitigation. Based on
these, we discuss challenges in existing solutions, conclude the opportunities
to shed light on future research directions and offer practical recommendations
for practitioners. We hope this thorough review will help bridge the gaps
between academia and industry and accelerate the ecosystem-wide governance of
legitimate software risks within the software engineering community.

</details>


### [3] [FuzzFeed: An Automatic Approach to Weakest Precondition Generation using LLMs and Fuzzing](https://arxiv.org/abs/2507.05272)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.SE

TL;DR: 论文提出结合大语言模型（LLMs）和模糊测试生成最弱前置条件（WP），并通过模糊指导（FG）利用程序执行反馈优化LLM生成的WP。


<details>
  <summary>Details</summary>
Motivation: 最弱前置条件的生成在验证和运行时错误检查等领域有重要应用，但传统方法可能存在效率或准确性不足的问题。

Method: 采用大语言模型与模糊测试结合的方法，通过模糊指导（FG）利用程序执行反馈优化LLM生成的候选WP。

Result: 实验证明，在确定性数组程序上，LLM能够生成可行的候选WP，而FG可显著提升这一能力。

Conclusion: LLMs结合FG的方法在生成最弱前置条件方面表现出实用性和有效性，为相关领域提供了新思路。

Abstract: The weakest precondition (WP) of a program describes the largest set of
initial states from which all terminating executions of the program satisfy a
given postcondition. The generation of WPs is an important task with practical
applications in areas ranging from verification to run-time error checking.
  This paper proposes the combination of Large Language Models (LLMs) and fuzz
testing for generating WPs. In pursuit of this goal, we introduce Fuzzing
Guidance (FG); FG acts as a means of directing LLMs towards correct WPs using
program execution feedback. FG utilises fuzz testing for approximately checking
the validity and weakness of candidate WPs, this information is then fed back
to the LLM as a means of context refinement.
  We demonstrate the effectiveness of our approach on a comprehensive benchmark
set of deterministic array programs in Java. Our experiments indicate that LLMs
are capable of producing viable candidate WPs, and that this ability can be
practically enhanced through FG.

</details>


### [4] [ReservoirChat: Interactive Documentation Enhanced with LLM and Knowledge Graph for ReservoirPy](https://arxiv.org/abs/2507.05279)
*Virgile Boraud,Yannis Bendi-Ouis,Paul Bernard,Xavier Hinaut*

Main category: cs.SE

TL;DR: 介绍了一款工具，旨在通过RAG和知识图谱提升LLM在代码开发和储层计算领域问答的能力，减少幻觉并提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在特定领域（如ReservoirPy）代码开发和问答中的局限性，提高其准确性和实用性。

Method: 结合RAG和知识图谱，设计交互式工具，专注于ReservoirPy的代码编写、调试和领域知识问答。

Result: 在编码任务中优于ChatGPT-4o和NotebookLM，且在基础模型Codestral-22B上有显著提升。

Conclusion: 该工具在特定领域任务中表现出色，未来可进一步优化以提升通用性。

Abstract: We introduce a tool designed to improve the capabilities of Large Language
Models (LLMs) in assisting with code development using the ReservoirPy library,
as well as in answering complex questions in the field of Reservoir Computing.
By incorporating external knowledge through Retrieval-Augmented Generation
(RAG) and knowledge graphs, our approach aims to reduce hallucinations and
increase the factual accuracy of generated responses. The system provides an
interactive experience similar to ChatGPT, tailored specifically for
ReservoirPy, enabling users to write, debug, and understand Python code while
accessing reliable domain-specific insights. In our evaluation, while
proprietary models such as ChatGPT-4o and NotebookLM performed slightly better
on general knowledge questions, our model outperformed them on coding tasks and
showed a significant improvement over its base model, Codestral-22B.

</details>


### [5] [CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark](https://arxiv.org/abs/2507.05281)
*Lingyue Fu,Hao Guan,Bolun Zhang,Haowei Yuan,Yaoming Zhu,Jun Xu,Zongyu Wang,Lin Qiu,Xunliang Cai,Xuezhi Cao,Weiwen Liu,Weinan Zhang,Yong Yu*

Main category: cs.SE

TL;DR: CoreCodeBench是一个可配置的多场景仓库级基准测试工具，旨在解决现有基准测试在多样性和复杂性上的不足，通过CorePipe自动生成测试用例，模拟真实工程场景。


<details>
  <summary>Details</summary>
Motivation: 现有仓库级基准测试主要关注单一场景（如代码生成或错误修复），未能充分体现真实软件工程的多样性和复杂性，且存在可控性和可靠性问题。

Method: 提出CorePipe自动化管道，将仓库转化为测试用例，并开发CoreCodeBench基准测试，通过三种原子问题（开发、错误修复、测试驱动开发）及其组合，灵活调整难度。

Result: 在16个大型语言模型上的实验显示其在不同场景下的表现，为工程应用提供了多维度的性能洞察。

Conclusion: CoreCodeBench提供了一个全面的基准测试工具，可用于评估大型语言模型在真实工程项目中的适用性。

Abstract: As Large Language Models (LLMs) demonstrate increasingly sophisticated code
processing capabilities, evaluating their performance on engineering-level code
remains challenging. Existing repository-level benchmarks primarily focus on
single scenarios, such as code generation or bug fixing, without adequately
capturing the diversity and complexity of real-world software or project
engineering workflows. Furthermore, these benchmarks suffer from limited
controllability in question positioning and reliability issues in their
generated test cases. To address these limitations, we present CorePipe, a
fully automated pipeline that converts repositories into comprehensive test
cases, and introduce CoreCodeBench, a configurable multi-scenario
repository-level benchmark. To simulate real engineering scenarios, CorePipe
generates three types of atomic questions (Development, BugFix, and Test-Driven
Development) specifically targeting core code segments. These atomic questions
are further combined into three types of composite questions, with difficulty
levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides
a comprehensive and extensive repository-level benchmark to investigate the
applicability of LLMs in real-world engineering projects. Experiments with 16
LLMs across diverse scenarios reveal varying capabilities and offer
multi-dimensional insights into LLM performance in engineering contexts. The
code for CorePipe is available at
https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for
CoreCodeBench can be accessed at
https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.

</details>


### [6] [Measuring how changes in code readability attributes affect code quality evaluation by Large Language Models](https://arxiv.org/abs/2507.05289)
*Igor Regis da Silva Simoes,Elaine Venson*

Main category: cs.SE

TL;DR: 该论文探讨利用大语言模型（LLMs）标准化评估代码可读性，通过实验验证其对代码更改的敏感性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决代码可读性评估中的主观性和标准化难题。

Method: 采用准实验研究，测试9种LLMs对三种干预措施（删除注释、混淆标识符、重构代码）的响应。

Result: LLMs对所有干预措施敏感，与原分类器在原始和重构代码场景中高度一致，并展现出语义敏感性。

Conclusion: LLMs在评估代码语义质量方面具有潜力，尤其是在标识符名称、注释与代码目的的连贯性上。

Abstract: Code readability is one of the main aspects of code quality, influenced by
various properties like identifier names, comments, code structure, and
adherence to standards. However, measuring this attribute poses challenges in
both industry and academia. While static analysis tools assess attributes such
as code smells and comment percentage, code reviews introduce an element of
subjectivity. This paper explores using Large Language Models (LLMs) to
evaluate code quality attributes related to its readability in a standardized,
reproducible, and consistent manner. We conducted a quasi-experiment study to
measure the effects of code changes on Large Language Model (LLM)s
interpretation regarding its readability quality attribute. Nine LLMs were
tested, undergoing three interventions: removing comments, replacing identifier
names with obscure names, and refactoring to remove code smells. Each
intervention involved 10 batch analyses per LLM, collecting data on response
variability. We compared the results with a known reference model and tool. The
results showed that all LLMs were sensitive to the interventions, with
agreement with the reference classifier being high for the original and
refactored code scenarios. The LLMs demonstrated a strong semantic sensitivity
that the reference model did not fully capture. A thematic analysis of the LLMs
reasoning confirmed their evaluations directly reflected the nature of each
intervention. The models also exhibited response variability, with 9.37% to
14.58% of executions showing a standard deviation greater than zero, indicating
response oscillation, though this did not always compromise the statistical
significance of the results. LLMs demonstrated potential for evaluating
semantic quality aspects, such as coherence between identifier names, comments,
and documentation with code purpose.

</details>


### [7] [zkSDK: Streamlining zero-knowledge proof development through automated trace-driven ZK-backend selection](https://arxiv.org/abs/2507.05294)
*William Law*

Main category: cs.SE

TL;DR: 论文介绍了一个名为zkSDK的模块化框架，旨在简化零知识（ZK）应用开发，通过抽象后端复杂性并提供动态选择算法自动选择最优ZK证明后端。


<details>
  <summary>Details</summary>
Motivation: 由于零知识程序开发的快速进步，开发者面临多种ZK后端选择，导致学习曲线陡峭且体验碎片化。zkSDK旨在解决这一问题。

Method: 论文提出了zkSDK框架，其核心是Presto语言，用于分析程序的计算工作负载强度，并结合用户定义的动态选择算法自动选择最优ZK后端。

Result: 通过实际工作负载的分析与评估，zkSDK能够有效选择最适合的后端，提供无缝且用户友好的开发体验。

Conclusion: zkSDK成功解决了ZK开发中的后端选择问题，提升了开发效率和用户体验。

Abstract: The rapid advancement of creating Zero-Knowledge (ZK) programs has led to the
development of numerous tools designed to support developers. Popular options
include being able to write in general-purpose programming languages like Rust
from Risc Zero. Other languages exist like Circom, Lib-snark, and Cairo.
However, developers entering the ZK space are faced with many different ZK
backends to choose from, leading to a steep learning curve and a fragmented
developer experience across different platforms. As a result, many developers
tend to select a single ZK backend and remain tied to it. This thesis
introduces zkSDK, a modular framework that streamlines ZK application
development by abstracting the backend complexities. At the core of zkSDK is
Presto, a custom Python-like programming language that enables the profiling
and analysis of a program to assess its computational workload intensity.
Combined with user-defined criteria, zkSDK employs a dynamic selection
algorithm to automatically choose the optimal ZK-proving backend. Through an
in-depth analysis and evaluation of real-world workloads, we demonstrate that
zkSDK effectively selects the best-suited backend from a set of supported ZK
backends, delivering a seamless and user-friendly development experience.

</details>


### [8] [ASSURE: Metamorphic Testing for AI-powered Browser Extensions](https://arxiv.org/abs/2507.05307)
*Xuanqi Gao,Juan Zhai,Shiqing Ma,Siyi Xie,Chao Shen*

Main category: cs.SE

TL;DR: ASSURE是一个专门为AI浏览器扩展设计的自动化测试框架，解决了传统测试方法和LLM测试方法在非确定性行为、上下文敏感性和复杂网页集成上的局限性。


<details>
  <summary>Details</summary>
Motivation: AI浏览器扩展的测试和可靠性保障面临前所未有的挑战，传统的测试方法无法应对其非确定性和复杂性。

Method: ASSURE框架包含模块化测试用例生成引擎、自动化执行框架和可配置的验证管道，用于系统评估行为一致性和安全性。

Result: 在六个广泛使用的AI浏览器扩展中，ASSURE识别出531个问题，测试吞吐量比手动方法提升了6.4倍。

Conclusion: ASSURE为测试AI浏览器扩展提供了高效的解决方案，可集成到开发流程中。

Abstract: The integration of Large Language Models (LLMs) into browser extensions has
revolutionized web browsing, enabling sophisticated functionalities like
content summarization, intelligent translation, and context-aware writing
assistance. However, these AI-powered extensions introduce unprecedented
challenges in testing and reliability assurance. Traditional browser extension
testing approaches fail to address the non-deterministic behavior,
context-sensitivity, and complex web environment integration inherent to
LLM-powered extensions. Similarly, existing LLM testing methodologies operate
in isolation from browser-specific contexts, creating a critical gap in
effective evaluation frameworks. To bridge this gap, we present ASSURE, a
modular automated testing framework specifically designed for AI-powered
browser extensions. ASSURE comprises three principal components: (1) a modular
test case generation engine that supports plugin-based extension of testing
scenarios, (2) an automated execution framework that orchestrates the complex
interactions between web content, extension processing, and AI model behavior,
and (3) a configurable validation pipeline that systematically evaluates
behavioral consistency and security invariants rather than relying on exact
output matching. Our evaluation across six widely-used AI browser extensions
demonstrates ASSURE's effectiveness, identifying 531 distinct issues spanning
security vulnerabilities, metamorphic relation violations, and content
alignment problems. ASSURE achieves 6.4x improved testing throughput compared
to manual approaches, detecting critical security vulnerabilities within 12.4
minutes on average. This efficiency makes ASSURE practical for integration into
development pipelines, offering a comprehensive solution to the unique
challenges of testing AI-powered browser extensions.

</details>


### [9] [OASBuilder: Generating OpenAPI Specifications from Online API Documentation with Large Language Models](https://arxiv.org/abs/2507.05316)
*Koren Lazar,Matan Vetzler,Kiran Kate,Jason Tsay,David Boaz Himanshu Gupta,Avraham Shinnar,Rohith D Vallam,David Amid Esther Goldbraich,Guy Uziel,Jim Laredo,Ateret Anaby Tavor*

Main category: cs.SE

TL;DR: OASBuilder是一个将API文档从非结构化HTML转换为机器可读的OpenAPI规范的新框架。


<details>
  <summary>Details</summary>
Motivation: 解决API文档非结构化问题，减少手动转换的时间成本。

Method: 结合大型语言模型和基于规则的算法，通过精心设计的流程处理文档。

Result: 实验证明OASBuilder能有效处理数百种API，生成包含大部分原文档信息的有效OpenAPI规范。

Conclusion: OASBuilder在企业环境中成功落地，节省了大量手动工作时间，并为LLM提供了可用的API工具。

Abstract: AI agents and business automation tools interacting with external web
services require standardized, machine-readable information about their APIs in
the form of API specifications. However, the information about APIs available
online is often presented as unstructured, free-form HTML documentation,
requiring external users to spend significant time manually converting it into
a structured format. To address this, we introduce OASBuilder, a novel
framework that transforms long and diverse API documentation pages into
consistent, machine-readable API specifications. This is achieved through a
carefully crafted pipeline that integrates large language models and rule-based
algorithms which are guided by domain knowledge of the structure of
documentation webpages. Our experiments demonstrate that OASBuilder generalizes
well across hundreds of APIs, and produces valid OpenAPI specifications that
encapsulate most of the information from the original documentation. OASBuilder
has been successfully implemented in an enterprise environment, saving
thousands of hours of manual effort and making hundreds of complex enterprise
APIs accessible as tools for LLMs.

</details>


### [10] [Exploring Empathy in Software Engineering: Insights from a Grey Literature Analysis of Practitioners' Perspectives](https://arxiv.org/abs/2507.05325)
*Lidiany Cerqueira,João Pedro Bastos,Danilo Neves,Glauco Carneiro,Rodrigo Spínola,Sávio Freire,José Amancio Macedo Santos,Manoel Mendonça*

Main category: cs.SE

TL;DR: 研究探讨了软件开发中同理心的意义、阻碍及提升方法，提出了概念框架并验证其价值。


<details>
  <summary>Details</summary>
Motivation: 同理心在软件开发中至关重要但研究不足，需从实践者角度深入探讨。

Method: 通过内容分析55篇网络文章及专家调查，提出同理心定义、阻碍及提升策略。

Result: 提出同理心概念框架，验证其清晰性和价值，为团队动态改进提供依据。

Conclusion: 研究为培养同理心提供策略，未来将探索其在软件开发中的更广泛应用。

Abstract: Context. Empathy, a key social skill, is essential for communication and
collaboration in SE but remains an under-researched topic. Aims. This study
investigates empathy in SE from practitioners' perspectives, aiming to
characterize its meaning, identify barriers, discuss practices to overcome
them, and explore its effects. Method. A qualitative content analysis was
conducted on 55 web articles from DEV and Medium, two communities widely used
by practitioners. To strengthen our findings, we conducted a follow-up survey
with empathy experts. Results. The study proposes a definition of empathy in
SE, identifies barriers such as toxic culture and excessive technical focus,
practices to foster empathy in teams, and outcomes, including improved
collaboration, communication, and reduced anxiety, frustration, and stress.
These findings are synthesized into a conceptual framework. Conclusion. Survey
results indicate the framework is clear, valuable, and raises empathy
awareness, with suggestions for improvements and integration into training.
This study paves the way for improving team dynamics by addressing barriers and
offering strategies to cultivate empathy. Future work will explore empathy's
broader implications in SE practice.

</details>


### [11] [Tool for Supporting Debugging and Understanding of Normative Requirements Using LLMs](https://arxiv.org/abs/2507.05504)
*Alex Kleijwegt,Sinem Getir Yaman,Radu Calinescu*

Main category: cs.SE

TL;DR: 论文介绍了一种利用大型语言模型（LLM）的工具SLEEC-LLM，用于将SLEEC规则不一致性分析的模型检查结果转化为自然语言解释，以提高非技术用户的理解和需求分析效率。


<details>
  <summary>Details</summary>
Motivation: 规范需求（SLEEC）的复杂性及其一致性管理对于非技术用户来说难以理解，导致需求验证过程低效。

Method: 开发了SLEEC-LLM工具，利用LLM将模型检查的反例结果转化为自然语言解释。

Result: SLEEC-LLM提高了规范需求的获取和一致性分析的效率和可解释性，具体体现在两个实际案例中。

Conclusion: SLEEC-LLM有效解决了非技术用户理解模型检查结果的难题，提升了规范需求分析的效率和可用性。

Abstract: Normative requirements specify social, legal, ethical, empathetic, and
cultural (SLEEC) norms that must be observed by a system. To support the
identification of SLEEC requirements, numerous standards and regulations have
been developed. These requirements are typically defined by stakeholders in the
non-technical system with diverse expertise (e.g., ethicists, lawyers, social
scientists). Hence, ensuring their consistency and managing the requirement
elicitation process are complex and error-prone tasks. Recent research has
addressed this challenge using domain-specific languages to specify normative
requirements as rules, whose consistency can then be analyzed with formal
methods. Nevertheless, these approaches often present the results from formal
verification tools in a way that is inaccessible to non-technical users. This
hinders understanding and makes the iterative process of eliciting and
validating these requirements inefficient in terms of both time and effort. To
address this problem, we introduce SLEEC-LLM, a tool that uses large language
models (LLMs) to provide natural-language interpretations for model-checking
counterexamples corresponding to SLEEC rule inconsistencies. SLEEC-LLM improves
the efficiency and explainability of normative requirements elicitation and
consistency analysis. To demonstrate its effectiveness, we summarise its use in
two real-world case studies involving non-technical stakeholders.

</details>


### [12] [Search-based Selection of Metamorphic Relations for Optimized Robustness Testing of Large Language Models](https://arxiv.org/abs/2507.05565)
*Sangwon Hyun,Shaukat Ali,M. Ali Babar*

Main category: cs.SE

TL;DR: 本文提出了一种基于搜索的方法优化MR组，以最大化故障检测并最小化LLM执行成本，并覆盖了组合扰动。MOEA/D算法在优化MR空间方面表现最佳，并发现了对LLM稳健性测试具有优势的MR。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）的稳健性是一个重要问题，但现有的MR测试需要优化选择和扩展测试空间。

Method: 提出搜索方法优化MR组，实施了四种搜索算法（Single-GA、NSGA-II、SPEA2和MOEA/D），并进行对比实验。

Result: MOEA/D算法表现最佳，并发现了一些对LLM稳健性测试特别有效的MR。

Conclusion: 研究为LLM稳健性测试的优化问题提供了解决方案，并展示了搜索方法的潜力。

Abstract: Assessing the trustworthiness of Large Language Models (LLMs), such as
robustness, has garnered significant attention. Recently, metamorphic testing
that defines Metamorphic Relations (MRs) has been widely applied to evaluate
the robustness of LLM executions. However, the MR-based robustness testing
still requires a scalable number of MRs, thereby necessitating the optimization
of selecting MRs. Most extant LLM testing studies are limited to automatically
generating test cases (i.e., MRs) to enhance failure detection. Additionally,
most studies only considered a limited test space of single perturbation MRs in
their evaluation of LLMs. In contrast, our paper proposes a search-based
approach for optimizing the MR groups to maximize failure detection and
minimize the LLM execution cost. Moreover, our approach covers the
combinatorial perturbations in MRs, facilitating the expansion of test space in
the robustness assessment. We have developed a search process and implemented
four search algorithms: Single-GA, NSGA-II, SPEA2, and MOEA/D with novel
encoding to solve the MR selection problem in the LLM robustness testing. We
conducted comparative experiments on the four search algorithms along with a
random search, using two major LLMs with primary Text-to-Text tasks. Our
statistical and empirical investigation revealed two key findings: (1) the
MOEA/D algorithm performed the best in optimizing the MR space for LLM
robustness testing, and (2) we identified silver bullet MRs for the LLM
robustness testing, which demonstrated dominant capabilities in confusing LLMs
across different Text-to-Text tasks. In LLM robustness assessment, our research
sheds light on the fundamental problem for optimized testing and provides
insights into search-based solutions.

</details>


### [13] [TigAug: Data Augmentation for Testing Traffic Light Detection in Autonomous Driving Systems](https://arxiv.org/abs/2507.05932)
*You Lu,Dingji Wang,Kaifeng Huang,Bihuan Chen,Xin Peng*

Main category: cs.SE

TL;DR: 论文提出TigAug方法，用于自动增强交通灯图像数据，以测试和提升自动驾驶系统中的交通灯检测模型。


<details>
  <summary>Details</summary>
Motivation: 现有交通灯检测模型测试方法依赖手动数据收集，效率低且难以覆盖多样驾驶环境。

Method: 构建两类变形关系和三类图像变换，基于天气、相机及交通灯特性。

Result: 实验证实TigAug能有效测试模型、高效生成图像，且图像自然度可接受。

Conclusion: TigAug为交通灯检测模型的自动化测试与性能提升提供了可行方案。

Abstract: Autonomous vehicle technology has been developed in the last decades with
recent advances in sensing and computing technology. There is an urgent need to
ensure the reliability and robustness of autonomous driving systems (ADSs).
Despite the recent achievements in testing various ADS modules, little
attention has been paid on the automated testing of traffic light detection
models in ADSs. A common practice is to manually collect and label traffic
light data. However, it is labor-intensive, and even impossible to collect
diverse data under different driving environments.
  To address these problems, we propose and implement TigAug to automatically
augment labeled traffic light images for testing traffic light detection models
in ADSs. We construct two families of metamorphic relations and three families
of transformations based on a systematic understanding of weather environments,
camera properties, and traffic light properties. We use augmented images to
detect erroneous behaviors of traffic light detection models by
transformation-specific metamorphic relations, and to improve the performance
of traffic light detection models by retraining. Large-scale experiments with
four state-of-the-art traffic light detection models and two traffic light
datasets have demonstrated that i) TigAug is effective in testing traffic light
detection models, ii) TigAug is efficient in synthesizing traffic light images,
and iii) TigAug generates traffic light images with acceptable naturalness.

</details>


### [14] [Multi-Agent Debate Strategies to Enhance Requirements Engineering with Large Language Models](https://arxiv.org/abs/2507.05981)
*Marc Oriol,Quim Motger,Jordi Marco,Xavier Franch*

Main category: cs.SE

TL;DR: 论文探讨了通过多智能体辩论（MAD）策略提升大型语言模型（LLM）在需求工程（RE）任务中的准确性和鲁棒性，提出了初步MAD框架并验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如提示工程、模型微调）将LLM视为孤立黑箱，缺乏迭代优化和协作，限制了模型的鲁棒性和适应性。受人类辩论启发，希望通过多智能体辩论提升RE任务的性能。

Method: 系统研究现有MAD策略，提出分类法，并构建初步MAD框架用于RE分类任务进行验证。

Result: 研究揭示了MAD策略的核心特征，初步评估表明其在RE分类任务中可行。

Conclusion: MAD为提升LLM在RE任务中的准确性提供了新思路，为未来研究和应用奠定了基础。

Abstract: Context: Large Language Model (LLM) agents are becoming widely used for
various Requirements Engineering (RE) tasks. Research on improving their
accuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval
augmented generation. However, these methods often treat models as isolated
black boxes - relying on single-pass outputs without iterative refinement or
collaboration, limiting robustness and adaptability. Objective: We propose
that, just as human debates enhance accuracy and reduce bias in RE tasks by
incorporating diverse perspectives, different LLM agents debating and
collaborating may achieve similar improvements. Our goal is to investigate
whether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:
We conducted a systematic study of existing MAD strategies across various
domains to identify their key characteristics. To assess their applicability in
RE, we implemented and tested a preliminary MAD-based framework for RE
classification. Results: Our study identified and categorized several MAD
strategies, leading to a taxonomy outlining their core attributes. Our
preliminary evaluation demonstrated the feasibility of applying MAD to RE
classification. Conclusions: MAD presents a promising approach for improving
LLM accuracy in RE tasks. This study provides a foundational understanding of
MAD strategies, offering insights for future research and refinements in RE
applications.

</details>


### [15] [PromiseTune: Unveiling Causally Promising and Explainable Configuration Tuning](https://arxiv.org/abs/2507.05995)
*Pengzhou Chen,Tao Chen*

Main category: cs.SE

TL;DR: 论文提出PromiseTune方法，通过因果纯化的规则指导配置调优，解决了现有调优器在探索和利用间的平衡难题，并提升了结果的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的高可配置性使得配置调优成为确保性能的关键步骤，但现有方法因探索与利用的难以平衡而效果不佳。

Method: 提出PromiseTune，通过因果纯化学习配置规则，界定有前景的区域，指导调优过程。

Result: 在12个系统和不同预算下与11种先进调优器对比，PromiseTune表现显著优于其他方法，排名提升42%，并提供更丰富的系统特性解释信息。

Conclusion: PromiseTune通过因果纯化规则有效解决了配置调优的探索与利用难题，并提升了结果的可解释性。

Abstract: The high configurability of modern software systems has made configuration
tuning a crucial step for assuring system performance, e.g., latency or
throughput. However, given the expensive measurements, large configuration
space, and rugged configuration landscape, existing tuners suffer
ineffectiveness due to the difficult balance of budget utilization between
exploring uncertain regions (for escaping from local optima) and exploiting
guidance of known good configurations (for fast convergence). The root cause is
that we lack knowledge of where the promising regions lay, which also causes
challenges in the explainability of the results.
  In this paper, we propose PromiseTune that tunes configuration guided by
causally purified rules. PromiseTune is unique in the sense that we learn
rules, which reflect certain regions in the configuration landscape, and purify
them with causal inference. The remaining rules serve as approximated
reflections of the promising regions, bounding the tuning to emphasize these
places in the landscape. This, as we demonstrate, can effectively mitigate the
impact of the exploration and exploitation trade-off. Those purified regions
can then be paired with the measured configurations to provide spatial
explainability at the landscape level. Comparing with 11 state-of-the-art
tuners on 12 systems and varying budgets, we show that PromiseTune performs
significantly better than the others with $42\%$ superior rank to the overall
second best while providing richer information to explain the hidden system
characteristics.

</details>


### [16] [Model Cards Revisited: Bridging the Gap Between Theory and Practice for Ethical AI Requirements](https://arxiv.org/abs/2507.06014)
*Tim Puhlfürß,Julia Butzke,Walid Maalej*

Main category: cs.SE

TL;DR: 研究分析了AI模型文档的伦理要求与实践差距，提出了包含43项伦理要求的分类法，并呼吁改进文档框架以全面覆盖伦理考量。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型文档实践中存在伦理要求与实际内容的不匹配，开发者常忽略如可解释性、用户自主权和公平性等伦理方面。

Method: 通过对26个伦理与AI指南、3个文档框架、3项模型卡量化研究和10个实际模型卡的主题分析，研究者识别并分类了43项伦理要求。

Result: 发现开发者更关注模型能力与可靠性，而忽视其他伦理要求。提出了包含4个主题和12个子主题的分类法。

Conclusion: 需要改进现有文档框架以全面满足伦理要求，分类法为修订模型卡框架提供了基础。

Abstract: Model cards are the primary documentation framework for developers of
artificial intelligence (AI) models to communicate critical information to
their users. Those users are often developers themselves looking for relevant
documentation to ensure that their AI systems comply with the ethical
requirements of existing laws, guidelines, and standards. Recent studies
indicate inadequate model documentation practices, suggesting a gap between AI
requirements and current practices in model documentation. To understand this
gap and provide actionable guidance to bridge it, we conducted a thematic
analysis of 26 guidelines on ethics and AI, three AI documentation frameworks,
three quantitative studies of model cards, and ten actual model cards. We
identified a total of 43 ethical requirements relevant to model documentation
and organized them into a taxonomy featuring four themes and twelve sub-themes
representing ethical principles. Our findings indicate that model developers
predominantly emphasize model capabilities and reliability in the documentation
while overlooking other ethical aspects, such as explainability, user autonomy,
and fairness. This underscores the need for enhanced support in documenting
ethical AI considerations. Our taxonomy serves as a foundation for a revised
model card framework that holistically addresses ethical AI requirements.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [17] [Baton: Compensate for Missing Wi-Fi Features for Practical Device-free Tracking](https://arxiv.org/abs/2507.05597)
*Yiming Zhao,Xuanqi Meng,Xinyu Tong,Xiulong Liu,Xin Xie,Wenyu Qu*

Main category: cs.NI

TL;DR: 论文提出了一种名为Baton的系统，能够在Wi-Fi特征严重缺失的情况下精确跟踪目标，通过水平和垂直维度的相关性分析以及STAP算法实现特征无缝传递。


<details>
  <summary>Details</summary>
Motivation: 当前Wi-Fi传感系统通常需要持续通信，短暂中断会导致性能显著下降，因此需要一种能在特征缺失时仍能准确跟踪的解决方案。

Method: 探索Wi-Fi特征矩阵在水平和垂直维度的相关性，提出STAP算法，实现特征在时间和不同链路间的无缝传递。

Result: 在商用设备上实现，中位跟踪误差为0.46m，通信占空比仅为20.00%，比现有技术减少79.19%的跟踪误差。

Conclusion: Baton系统在Wi-Fi特征严重缺失时仍能高效跟踪目标，表现出显著优越性。

Abstract: Wi-Fi contact-free sensing systems have attracted widespread attention due to
their ubiquity and convenience. The integrated sensing and communication (ISAC)
technology utilizes off-the-shelf Wi-Fi communication signals for sensing,
which further promotes the deployment of intelligent sensing applications.
However, current Wi-Fi sensing systems often require prolonged and unnecessary
communication between transceivers, and brief communication interruptions will
lead to significant performance degradation. This paper proposes Baton, the
first system capable of accurately tracking targets even under severe Wi-Fi
feature deficiencies. To be specific, we explore the relevance of the Wi-Fi
feature matrix from both horizontal and vertical dimensions. The horizontal
dimension reveals feature correlation across different Wi-Fi links, while the
vertical dimension reveals feature correlation among different time slots.
Based on the above principle, we propose the Simultaneous Tracking And
Predicting (STAP) algorithm, which enables the seamless transfer of Wi-Fi
features over time and across different links, akin to passing a baton. We
implement the system on commercial devices, and the experimental results show
that our system outperforms existing solutions with a median tracking error of
0.46m, even when the communication duty cycle is as low as 20.00%. Compared
with the state-of-the-art, our system reduces the tracking error by 79.19% in
scenarios with severe Wi-Fi feature deficiencies.

</details>


### [18] [A Satellite-Ground Synergistic Large Vision-Language Model System for Earth Observation](https://arxiv.org/abs/2507.05731)
*Yuxin Zhang,Jiahao Yang,Zhe Chen,Wenjun Zhu,Jin Zhao,Yue Gao*

Main category: cs.NI

TL;DR: 论文提出了SpaceVerse系统，通过在卫星和地面站协同部署大视觉语言模型（LVLM），以解决卫星图像下载和实时分析的挑战。


<details>
  <summary>Details</summary>
Motivation: 卫星快速运动、短暂的卫星-地面站接触窗口和大图像尺寸导致数据下载困难，需要支持近实时地球观测应用（如灾害监测）。

Method: 在卫星上部署轻量级LVLM处理简单任务，地面站处理复杂任务；提出计算与通信协同设计框架，包括渐进置信网络和基于注意力的多尺度预处理。

Result: 在真实LEO卫星星座和数据集上实现，平均精度提升31.2%，延迟降低51.2%。

Conclusion: SpaceVerse系统通过协同部署和优化设计，有效提升了卫星图像分析的实时性和准确性。

Abstract: Recently, large vision-language models (LVLMs) unleash powerful analysis
capabilities for low Earth orbit (LEO) satellite Earth observation images in
the data center. However, fast satellite motion, brief satellite-ground station
(GS) contact windows, and large size of the images pose a data download
challenge. To enable near real-time Earth observation applications (e.g.,
disaster and extreme weather monitoring), we should explore how to deploy LVLM
in LEO satellite networks, and design SpaceVerse, an efficient satellite-ground
synergistic LVLM inference system. To this end, firstly, we deploy compact
LVLMs on satellites for lightweight tasks, whereas regular LVLMs operate on GSs
to handle computationally intensive tasks. Then, we propose a computing and
communication co-design framework comprised of a progressive confidence network
and an attention-based multi-scale preprocessing, used to identify on-satellite
inferring data, and reduce data redundancy before satellite-GS transmission,
separately. We implement and evaluate SpaceVerse on real-world LEO satellite
constellations and datasets, achieving a 31.2% average gain in accuracy and a
51.2% reduction in latency compared to state-of-the-art baselines.

</details>


### [19] [Intra-DP: A High Performance Collaborative Inference System for Mobile Edge Computing](https://arxiv.org/abs/2507.05829)
*Zekai Sun,Xiuxian Guan,Zheng Lin,Zihan Fang,Xiangming Cai,Zhe Chen,Fangming Liu,Heming Cui,Jie Xiong,Wei Ni,Chau Yuen*

Main category: cs.NI

TL;DR: Intra-DP是一种针对移动边缘计算优化的高性能协作推理系统，通过并行计算技术减少传输瓶颈，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的移动设备上部署深度神经网络（DNN）面临实时性能和资源限制的挑战，现有方法存在传输瓶颈。

Method: 采用基于本地操作符的并行计算技术，将计算分解为独立子操作，并行执行以减少传输瓶颈。

Result: 评估显示，Intra-DP将每推理延迟降低50%，能耗降低75%，且不牺牲准确性。

Conclusion: Intra-DP有效解决了移动边缘计算中的传输瓶颈问题，实现了高效且节能的DNN推理。

Abstract: Deploying deep neural networks (DNNs) on resource-constrained mobile devices
presents significant challenges, particularly in achieving real-time
performance while simultaneously coping with limited computational resources
and battery life. While Mobile Edge Computing (MEC) offers collaborative
inference with GPU servers as a promising solution, existing approaches
primarily rely on layer-wise model partitioning and undergo significant
transmission bottlenecks caused by the sequential execution of DNN operations.
To address this challenge, we present Intra-DP, a high-performance
collaborative inference system optimized for DNN inference on MEC. Intra DP
employs a novel parallel computing technique based on local operators (i.e.,
operators whose minimum unit input is not the entire input tensor, such as the
convolution kernel). By decomposing their computations (operations) into
several independent sub-operations and overlapping the computation and
transmission of different sub-operations through parallel execution, Intra-DP
mitigates transmission bottlenecks in MEC, achieving fast and energy-efficient
inference. The evaluation demonstrates that Intra-DP reduces per-inference
latency by up to 50% and energy consumption by up to 75% compared to
state-of-the-art baselines, without sacrificing accuracy.

</details>


### [20] [OLAF: Programmable Data Plane Acceleration for Asynchronous Distributed Reinforcement Learning](https://arxiv.org/abs/2507.05876)
*Nehal Baganal Krishna,Anam Tahir,Firas Khamis,Mina Tahmasbi Arashloo,Michael Zink,Amr Rizk*

Main category: cs.NI

TL;DR: 提出了一种网络数据平面加速架构，通过在线处理DRL模型更新，减少更新延迟和网络拥塞，从而提高异步DRL训练的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 异步分布式强化学习（DRL）在模型更新延迟时会出现收敛性能下降，通常是由于大规模训练中的网络拥塞和数据包丢失。

Method: 设计了一种新颖的排队机制，合并兼容的更新以减少冗余流量，并在工作节点提供轻量级传输控制机制，通过网络加速器反馈进行指导。引入Age-of-Model（AoM）指标评估模型效用。

Result: 该架构显著减少了更新延迟和拥塞，改善了异步DRL工作负载的收敛速度。

Conclusion: 通过数据平面加速架构和AoM指标，有效解决了异步DRL中的更新延迟问题，提升了收敛性能。

Abstract: Asynchronous Distributed Reinforcement Learning (DRL) can suffer from
degraded convergence when model updates become stale, often the result of
network congestion and packet loss during large-scale training. This work
introduces a network data-plane acceleration architecture that mitigates such
staleness by enabling inline processing of DRL model updates as they traverse
the accelerator engine. To this end, we design and prototype a novel queueing
mechanism that opportunistically combines compatible updates sharing a network
element, reducing redundant traffic and preserving update utility.
Complementing this we provide a lightweight transmission control mechanism at
the worker nodes that is guided by feedback from the in-network accelerator. To
assess model utility at line rate, we introduce the Age-of-Model (AoM) metric
as a proxy for staleness and verify global fairness and responsiveness
properties using a formal verification method. Our evaluations demonstrate that
this architecture significantly reduces update staleness and congestion,
ultimately improving the convergence rate in asynchronous DRL workloads.

</details>


### [21] [Programmable Governance for Group-Controlled Decentralized Identifiers](https://arxiv.org/abs/2507.06001)
*Carlo Segat,Sandro Rodriguez Garzo,Axel Küpper*

Main category: cs.NI

TL;DR: 研究探讨了如何在去中心化身份管理（SSI）中实现多方共同控制的 DID 更新机制，旨在开发一种无需信任、可编程且不依赖特定分布式账本的技术方案。


<details>
  <summary>Details</summary>
Motivation: 现有的 DID 规范虽然承认多方共同控制的场景，但未明确定义如何授权更新，尤其是在需要多方协作时。缺乏统一且灵活的技术机制限制了 DID 的实际应用。

Method: 文章提出了一种基于链上的、无需信任的机制，允许 DID 的控制者在多方协作下编程制定治理规则。重点在于确保该机制不依赖特定账本，且具有适应性。

Result: 研究证明，可以通过技术手段实现多方共同控制的 DDO 更新，且该机制能在不同分布式账本上通用。

Conclusion: 该机制为去中心化身份管理提供了更灵活的治理方案，支持复杂的多方协作场景，填补了现有规范的空白。

Abstract: Self-Sovereign Identity (SSI) is a paradigm for digital identity management
that offers unique privacy advantages. A key technology in SSI is Decentralized
Identifiers (DIDs) and their associated metadata, DID Documents (DDOs). DDOs
contain crucial verification material such as the public keys of the entity
identified by the DID (i.e., the DID subject) and are often anchored on a
distributed ledger to ensure security and availability. Long-lived DIDs need to
support updates (e.g., key rotation). Ideally, only the DID subject should
authorize DDO updates. However, in practice, update capabilities may be shared
or delegated. While the DID specification acknowledges such scenarios, it does
not define how updates should be authorized when multiple entities jointly
control a DID (i.e., group control). This article examines the implementation
of an on-chain, trustless mechanism enabling DID controllers under group
control to program their governance rules. The main research question is the
following: Can a technical mechanism be developed to orchestrate on-chain group
control of a DDO in a ledger-agnostic and adaptable manner?

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [22] [A Formalization of Divided Powers in Lean](https://arxiv.org/abs/2507.05327)
*Antoine Chambert-Loir,María Inés de Frutos-Fernández*

Main category: cs.LO

TL;DR: 本文在 Lean 4 中对幂除结构的理论进行了形式化，包括幂除同态和子幂除理想，并提供了关键的构造（如商和和），这是首次在任何定理证明器中对该理论的形式化。


<details>
  <summary>Details</summary>
Motivation: 幂除结构在代数拓扑、数论和代数几何等多个数学领域有重要应用，但其形式化理论尚未在定理证明器中实现。本文旨在填补这一空白。

Method: 通过扩展多变量幂级数环的形式化理论（包括拓扑结构和幂级数的求值与替换），作者在 Lean 4 中实现了幂除结构的基本理论。

Result: 成功形式化了幂除结构的基本理论，包括幂除同态、子幂除理想，以及商和和等关键构造。

Conclusion: 本文的工作为幂除结构的理论提供了首个形式化实现，为相关数学领域的进一步研究和应用奠定了基础。

Abstract: Given an ideal $I$ in a commutative ring $A$, a divided power structure on
$I$ is a collection of maps $\{\gamma_n \colon I \to A\}_{n \in \mathbb{N}}$,
subject to axioms that imply that it behaves like the family $\{x \mapsto
\frac{x^n}{n!}\}_{n \in \mathbb{N}}$, but which can be defined even when
division by factorials is not possible in $A$. Divided power structures have
important applications in diverse areas of mathematics, including algebraic
topology, number theory and algebraic geometry.
  In this article we describe a formalization in Lean 4 of the basic theory of
divided power structures, including divided power morphisms and sub-divided
power ideals, and we provide several fundamental constructions, in particular
quotients and sums. This constitutes the first formalization of this theory in
any theorem prover.
  As a prerequisite of general interest, we expand the formalized theory of
multivariate power series rings, endowing them with a topology and defining
evaluation and substitution of power series.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [23] [Esports and expertise: what competitive gaming can teach us about mastery](https://arxiv.org/abs/2507.05446)
*Ben Boudaoud,Josef Spjut,Joohwan Kim,Arjun Madhusudan,Benjamin Watson*

Main category: cs.HC

TL;DR: 文章探讨了人机交互研究中从原子化任务到更高层次综合技能的转变，提出在电竞等领域的表现更依赖于对特定任务技能的掌握。


<details>
  <summary>Details</summary>
Motivation: 传统人机交互研究关注原子化和通用任务，但电竞的兴起表明人类表现更依赖于高层次、综合技能的掌握。

Method: 通过对比传统任务完成时间与电竞中技能表现，分析人类在高层次任务中的表现。

Result: 指出在电竞等领域，成功依赖于对特定任务技能的优化，而非单纯的快速完成原子化任务。

Conclusion: 人机交互研究需从原子化任务扩展到更高层次的综合技能，以更全面地理解人类表现。

Abstract: Historically, much research and development in human computer interaction has
focused on atomic and generalizable tasks, where task completion time indicates
productivity. However, the emergence of competitive games and esports reminds
us of an alternative perspective on human performance in HCI: mastery of
higher-level, holistic practices. Just as a world-renowned artist is rarely
evaluated for their individual brush strokes, so skilled competitive gamers
rarely succeed solely by completing individual mouse movements or keystrokes as
quickly as possible. Instead, they optimize more task-specific skills, adeptly
performing challenges deep in the learning curve for their game of choice.

</details>


### [24] [NRXR-ID: Two-Factor Authentication (2FA) in VR Using Near-Range Extended Reality and Smartphones](https://arxiv.org/abs/2507.05447)
*Aiur Nanzatov,Lourdes Peña-Castillo,Oscar Meruvia-Pastor*

Main category: cs.HC

TL;DR: 论文提出了一种名为NRXR-ID的技术，用于在虚拟现实环境中实现双因素认证，通过智能手机完成认证挑战，无需摘下头戴式显示器。研究发现，国际跳棋式视觉匹配挑战是最佳选择。


<details>
  <summary>Details</summary>
Motivation: 解决虚拟现实中双因素认证的困难，因为用户佩戴头戴式显示器时无法看到现实环境。

Method: 提出NRXR-ID技术，利用智能手机完成认证挑战，包括国际跳棋式挑战等四种类型，并在三种不同配置下测试。采用4X3被试内设计。

Result: 国际跳棋式视觉匹配挑战表现最佳，其次是智能手机输入PIN码的方式。

Conclusion: NRXR-ID技术有效解决了VR环境中的双因素认证问题，并为未来研究提供了方向。

Abstract: Two-factor authentication (2FA) has become widely adopted as an efficient and
secure way to validate someone's identity online. Two-factor authentication is
difficult in virtual reality (VR) because users are usually wearing a
head-mounted display (HMD) which does not allow them to see their real-world
surroundings. We present NRXR-ID, a technique to implement two-factor
authentication while using extended reality systems and smartphones. The
proposed method allows users to complete an authentication challenge using
their smartphones without removing their HMD. We performed a user study where
we explored four types of challenges for users, including a novel
checkers-style challenge. Users responded to these challenges under three
different configurations, including a technique that uses the smartphone to
support gaze-based selection without the use of VR controllers. A 4X3
within-subjects design allowed us to study all the variations proposed. We
collected performance metrics and performed user experience questionnaires to
collect subjective impressions from 30 participants. Results suggest that the
checkers-style visual matching challenge was the most appropriate option,
followed by entering a digital PIN challenge submitted via the smartphone and
answered within the VR environment.

</details>


### [25] [GLOSS: Group of LLMs for Open-Ended Sensemaking of Passive Sensing Data for Health and Wellbeing](https://arxiv.org/abs/2507.05461)
*Akshat Choube,Ha Le,Jiachen Li,Kaixin Ji,Vedant Das Swain,Varun Mishra*

Main category: cs.HC

TL;DR: 本文提出了一种名为GLOSS的新型感知系统，能够进行开放式感知和复杂多模态三角测量以提取洞察，显著优于常见的RAG技术。


<details>
  <summary>Details</summary>
Motivation: 智能手机和穿戴设备的普及为健康和行为预测提供了被动传感数据，但现有系统在开放式感知和复杂数据三角测量方面存在不足。

Method: 开发了GLOSS系统，支持开放式感知和多模态三角测量，并与RAG技术进行对比。

Result: GLOSS在准确性和一致性上显著优于RAG（87.93% vs 29.31%准确性；66.19% vs 52.85%一致性）。

Conclusion: GLOSS展示了在UbiComp和HCI领域的潜力，同时讨论了其局限性和未来发展方向。

Abstract: The ubiquitous presence of smartphones and wearables has enabled researchers
to build prediction and detection models for various health and behavior
outcomes using passive sensing data from these devices. Achieving a high-level,
holistic understanding of an individual's behavior and context, however,
remains a significant challenge. Due to the nature of passive sensing data,
sensemaking -- the process of interpreting and extracting insights -- requires
both domain knowledge and technical expertise, creating barriers for different
stakeholders. Existing systems designed to support sensemaking are either not
open-ended or cannot perform complex data triangulation. In this paper, we
present a novel sensemaking system, Group of LLMs for Open-ended Sensemaking
(GLOSS), capable of open-ended sensemaking and performing complex multimodal
triangulation to derive insights. We demonstrate that GLOSS significantly
outperforms the commonly used Retrieval-Augmented Generation (RAG) technique,
achieving 87.93% accuracy and 66.19% consistency, compared to RAG's 29.31%
accuracy and 52.85% consistency. Furthermore, we showcase the promise of GLOSS
through four use cases inspired by prior and ongoing work in the UbiComp and
HCI communities. Finally, we discuss the potential of GLOSS, its broader
implications, and the limitations of our work.

</details>


### [26] [AnatomyCarve: A VR occlusion management technique for medical images based on segment-aware clipping](https://arxiv.org/abs/2507.05572)
*Andrey Titov,Tina N. H. Nantenaina,Marta Kersten-Oertel,Simon Drouin*

Main category: cs.HC

TL;DR: AnatomyCarve是一种VR技术，通过选择性切除组织来可视化3D医学图像，类似解剖教科书中的手绘插图，提高了用户满意度和临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 由于自遮挡问题，现有方法难以完整展示3D医学图像的内部解剖结构，而手绘医学插图却能有效解决这一问题。

Method: 开发了AnatomyCarve技术，在VR环境中选择性切除3D医学体数据中的部分组织，保留空间关系和上下文信息。

Result: 研究表明AnatomyCarve提供定制化解剖可视化，非专家和神经外科医生对其用户满意度和临床效果评价较高。

Conclusion: AnatomyCarve具有教育和临床应用潜力，其结合高级渲染技术和VR交互的方有效提升了医学图像的可视化效果。

Abstract: Visualizing 3D medical images is challenging due to self-occlusion, where
anatomical structures of interest can be obscured by surrounding tissues.
Existing methods, such as slicing and interactive clipping, are limited in
their ability to fully represent internal anatomy in context. In contrast,
hand-drawn medical illustrations in anatomy books manage occlusion effectively
by selectively removing portions based on tissue type, revealing 3D structures
while preserving context. This paper introduces AnatomyCarve, a novel technique
developed for a VR environment that creates high-quality illustrations similar
to those in anatomy books, while remaining fast and interactive. AnatomyCarve
allows users to clip selected segments from 3D medical volumes, preserving
spatial relations and contextual information. This approach enhances
visualization by combining advanced rendering techniques with natural user
interactions in VR. Usability of AnatomyCarve was assessed through a study with
non-experts, while surgical planning effectiveness was evaluated with
practicing neurosurgeons and residents. The results show that AnatomyCarve
enables customized anatomical visualizations, with high user satisfaction,
suggesting its potential for educational and clinical applications.

</details>


### [27] [W2W: A Simulated Exploration of IMU Placement Across the Human Body for Designing Smarter Wearable](https://arxiv.org/abs/2507.05532)
*Lala Shakti Swarup Ray,Bo Zhou,Paul Lukowicz*

Main category: cs.HC

TL;DR: 本文介绍了W2W框架，通过模拟方法系统评估IMU在身体不同位置的实用性，挑战了传统的传感器放置方式。


<details>
  <summary>Details</summary>
Motivation: 当前IMU在穿戴系统中的放置多依赖经验和惯例，缺乏系统性评估，希望提出一种数据驱动的优化方法。

Method: 利用运动捕捉数据生成512个人体表面区域的合成IMU信号，评估传感器在不同位置的性能。

Result: W2W模拟结果与实际IMU数据一致，发现了一些被忽视的高效区域，改进了传统放置规范。

Conclusion: W2W为传感器放置优化提供了强大的仿真工具，展示了数据驱动方法的潜力。

Abstract: Inertial measurement units (IMUs) are central to wearable systems for
activity recognition and pose estimation, but sensor placement remains largely
guided by heuristics and convention. In this work, we introduce Where to Wear
(W2W), a simulation-based framework for systematic exploration of IMU placement
utility across the body. Using labeled motion capture data, W2W generates
realistic synthetic IMU signals at 512 anatomically distributed surface
patches, enabling high-resolution, task-specific evaluation of sensor
performance. We validate reliability of W2W by comparing spatial performance
rankings from synthetic data with real IMU recordings in two multimodal
datasets, confirming strong agreement in activity-wise trends. Further analysis
reveals consistent spatial trends across activity types and uncovers overlooked
high-utility regions that are rarely used in commercial systems. These findings
challenge long-standing placement norms and highlight opportunities for more
efficient, task-adaptive sensor configurations. Overall, our results
demonstrate that simulation with W2W can serve as a powerful design tool for
optimizing sensor placement, enabling scalable, data-driven strategies that are
impractical to obtain through physical experimentation alone.

</details>


### [28] [Information Needs and Practices Supported by ChatGPT](https://arxiv.org/abs/2507.05537)
*Tim Gorichanaz*

Main category: cs.HC

TL;DR: 研究通过定性内容分析205个用户案例，探讨ChatGPT作为信息源的使用场景和信息实践，发现其支持多种生活领域和需求，并提出了六类信息实践。


<details>
  <summary>Details</summary>
Motivation: 探讨ChatGPT如何满足用户的信息需求，并重新定义AI时代的信息需求概念。

Method: 通过定性内容分析205个用户案例。

Result: ChatGPT应用于多个生活领域（家庭、工作、休闲等）和需求（写作、学习、编程等），并支持六类信息实践：写作、决策、识别、构思、对话和批评。

Conclusion: AI时代的信息需求应从“回答问题”扩展到“在世界中熟练应对”，为生成式AI与信息需求研究开辟新方向。

Abstract: This study considers ChatGPT as an information source, investigating the
information needs that people come to ChatGPT with and the information
practices that ChatGPT supports, through a qualitative content analysis of 205
user vignettes. The findings show that ChatGPT is used in a range of life
domains (home/family, work, leisure, etc.) and for a range of human needs
(writing/editing, learning, simple programming tasks, etc.), constituting the
information needs that people use ChatGPT to address. Related to these
information needs, the findings show six categories of information practices
that ChatGPT supports: Writing, Deciding, Identifying, Ideating, Talking, and
Critiquing. This work suggests that, in the AI age, information need should be
conceptualized not just as a matter of "getting questions answered" or even
"making sense," but as skillfully coping in the world, a notion that includes
both understanding and action. This study leads to numerous opportunities for
future work at the junction of generative AI and information needs, seeking,
use and experience.

</details>


### [29] [StoryGrid: A Tangible Interface for Student Expression](https://arxiv.org/abs/2507.05600)
*Tom Moher,Louis Gomez,Janet Kim,Claudia Hindo,Benjamin Watson,Stephen Fransen,Tim McEneany*

Main category: cs.HC

TL;DR: StorySpace是一个基于课堂的设计和展示系统，用于互动多媒体海报。它通过物理令牌操作多媒体对象，并在高中文学课程中应用，根据师生反馈改进界面，丰富了学生对文学的解读。


<details>
  <summary>Details</summary>
Motivation: 设计和改进StorySpace系统，用于教学环境中互动多媒体海报的制作和展示，以提升学生对文学的理解。

Method: 使用物理令牌操作投影的多媒体对象，结合师生反馈不断改进界面设计，如令牌语义和媒体导入方法。

Result: 系统成功应用于高中文学课堂，学生在文学解读中更注重受众和多元视角。

Conclusion: StorySpace通过互动设计有效提升了学生对文学的参与和理解，展示了其在教育中的应用潜力。

Abstract: StorySpace is a classroom-based design and presentation system for
interactive multimedia posters. Employing the technology base first used in
Eden's PITAboard [2002], StorySpace allows groups of learners to manipulate
projected multimedia objects on a horizontal board using a small collection of
shared physical tokens. In this paper, we present the ongoing design history of
StorySpace in the context of its introduction within an urban high school
literature class. Interface modifications based on student and teacher feedback
led on changes in token semantics and media importing methods. We describe how
StorySpace features enriched students' interpretations of literature, with
particular emphasis in two areas: (1) attention to audience, and (2) reflection
of multiple perspectives.

</details>


### [30] [Hapster: Using Apple Watch Haptics to Enable Live Low-Friction Student Feedback in the Physical Classroom](https://arxiv.org/abs/2507.05605)
*Oleg Aleksandrovich Golev,Michelle Huang,Chanketya Nop,Kritin Vongthongsri,Andrés Monroy-Hernández,Parastoo Abtahi*

Main category: cs.HC

TL;DR: Hapster是一种通过视觉和触觉模态传递学生反馈的原型系统，研究表明其能提升课堂参与度，但也存在触觉感知挑战。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索学生反馈系统（SRSs）中触觉模态的潜力，以增强课堂教学效果。

Method: 设计并评估了Hapster，一种基于Apple Watch的原型系统，通过视觉和触觉模态传递学生反馈，6名教师和155名学生参与。

Result: 系统有效促进了课堂参与，但教师对触觉序列的感知存在困难。

Conclusion: 触觉可作为有效的实时反馈机制，但需进一步研究灵活性、可访问性和交互模态。

Abstract: The benefits of student response systems (SRSs) for in-person lectures are
well-researched. However, all current SRSs only rely on a visual interface to
relay information to the instructor. We describe the design and evaluation of
Hapster, a prototype system that uses an Apple Watch to deliver live,
aggregated student feedback to the instructor via both visual and vibro-tactile
modalities. We evaluated this system with 6 instructors and 155 students at a
U.S. university. Participants reported that the system was effective at
delivering live student feedback and facilitating better engagement from both
the instructor and the students. However, instructors also noted several
challenges with differentiating and perceiving the haptic sequences while
lecturing. We conclude by discussing the tradeoff between system flexibility
and abuse potential while identifying opportunities for further research
regarding accessibility, content moderation, and additional interaction
modalities. Our results suggest that haptics can be used as an effective live
feedback mechanism for instructors in the physical classroom.

</details>


### [31] [Breaking the Plane: Exploring Real-Time Visualization of 3D Surfaces in Augmented Reality with Handwritten Input](https://arxiv.org/abs/2507.05616)
*Liam Franco Esparraguera,Kristoffer Selberg,Brian Lou,Jenny Sun,Beza Desta,Andrés Monroy-Hernández,Parastoo Abtahi*

Main category: cs.HC

TL;DR: 该论文介绍了名为“Breaking the Plane”的AR应用，通过手写输入实现3D数学函数的可视化，结合实时交互和动态可视化提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究展示了AR在数学学习中的潜力，但尚未有系统能将手写方程解析与3D可视化结合，提供实时交互学习体验。

Method: 开发了一个包含手写方程解析、图形操作和3D函数绘图器的AR系统。

Result: 系统在参与度上显著优于其他系统，易用性与流行可视化工具相当，且被认为是最有效的问题解决辅助工具，用户未来使用意愿强烈。

Conclusion: 结合手写输入和动态3D可视化的AR系统能有效提升数学学习的效果和参与度，具有广泛应用潜力。

Abstract: We introduce Breaking the Plane, an augmented reality (AR) application built
for AR headsets that enables users to visualize 3D mathematical functions using
handwritten input. Researchers have demonstrated overlaying 3D visualizations
of mathematical concepts through AR enhances learning motivation and
comprehension, and equation parsing makes the authoring of teaching materials
more time-efficient for instructors. Previous works have developed AR systems
that separately employ equation parsing and 3D mathematical visualizations, but
work has yet to be done to combine those features by enabling real-time
interactions and dynamic visualizations that help users learn in situ. We
explore this by developing an AR system featuring handwritten equation parsing,
graph manipulation, and a 3D function plotter. We found that our system
significantly surpassed other systems in engagement, achieved comparable ease
of use to a popular visualization tool, was considered the most effective in
aiding problem-solving, and was highly preferred by participants for future
use.

</details>


### [32] [Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents](https://arxiv.org/abs/2507.05820)
*Syemin Park,Soobin Park,Youn-kyung Lim*

Main category: cs.HC

TL;DR: 论文介绍了Constella，一款基于LLM的多代理工具，旨在帮助作家解决创作角色关系时的挑战。通过功能如FRIENDS DISCOVERY、JOURNALS和COMMENTS，它支持作家构建扩展的角色社区并深入理解角色关系。


<details>
  <summary>Details</summary>
Motivation: 作家在长篇故事创作中常难以设想新角色、平衡角色间的相似与差异，并深入刻画角色关系。

Method: 研究设计了Constella工具，利用LLM提供角色建议（FRIENDS DISCOVERY）、展示多个角色的内心世界（JOURNALS），以及通过角色间互动（COMMENTS）表现关系。

Result: 部署研究表明，Constella帮助作家构建了扩展的角色社区，比较角色思想与情感，并加深了对关系的理解。

Conclusion: 多代理互动有助于作家在角色群体中分配注意力与精力。

Abstract: Creating a cast of characters by attending to their relational dynamics is a
critical aspect of most long-form storywriting. However, our formative study
(N=14) reveals that writers struggle to envision new characters that could
influence existing ones, to balance similarities and differences among
characters, and to intricately flesh out their relationships. Based on these
observations, we designed Constella, an LLM-based multi-agent tool that
supports storywriters' interconnected character creation process. Constella
suggests related characters (FRIENDS DISCOVERY feature), reveals the inner
mindscapes of several characters simultaneously (JOURNALS feature), and
manifests relationships through inter-character responses (COMMENTS feature).
Our 7-8 day deployment study with storywriters (N=11) shows that Constella
enabled the creation of expansive communities composed of related characters,
facilitated the comparison of characters' thoughts and emotions, and deepened
writers' understanding of character relationships. We conclude by discussing
how multi-agent interactions can help distribute writers' attention and effort
across the character cast.

</details>


### [33] [Evaluation of Large Language Model-Driven AutoML in Data and Model Management from Human-Centered Perspective](https://arxiv.org/abs/2507.05962)
*Jiapeng Yao,Lantian Zhang,Jiping Huang*

Main category: cs.HC

TL;DR: 研究探讨了基于大型语言模型（LLMs）的AutoML框架如何提升机器学习（ML）技术的可访问性和实施效率，通过用户实验验证了其在成功率、精度和开发时间上的显著优势。


<details>
  <summary>Details</summary>
Motivation: 组织在应用机器学习技术时面临技术复杂性带来的高门槛和低效率问题，研究旨在探索LLMs如何通过自然语言接口降低技术门槛。

Method: 通过对15名不同背景的专业人士进行用户研究，比较了LLM-based AutoML与传统方法的实施效果。

Result: LLM-based AutoML显著提升了实施成功率（93.34%用户表现更优）、精度（46.67%提升10-25%，46.67%提升>25%）和开发效率（60%用户开发时间大幅减少）。

Conclusion: 研究表明，基于自然语言的LLM接口能有效降低技术门槛，促进ML能力的民主化，同时保持高质量和高性能。

Abstract: As organizations increasingly seek to leverage machine learning (ML)
capabilities, the technical complexity of implementing ML solutions creates
significant barriers to adoption and impacts operational efficiency. This
research examines how Large Language Models (LLMs) can transform the
accessibility of ML technologies within organizations through a human-centered
Automated Machine Learning (AutoML) approach. Through a comprehensive user
study involving 15 professionals across various roles and technical
backgrounds, we evaluate the organizational impact of an LLM-based AutoML
framework compared to traditional implementation methods. Our research offers
four significant contributions to both management practice and technical
innovation: First, we present pioneering evidence that LLM-based interfaces can
dramatically improve ML implementation success rates, with 93.34% of users
achieved superior performance in the LLM condition, with 46.67% showing higher
accuracy (10-25% improvement over baseline) and 46.67% demonstrating
significantly higher accuracy (>25% improvement over baseline), while 6.67%
maintained comparable performance levels; and 60% reporting substantially
reduced development time. Second, we demonstrate how natural language
interfaces can effectively bridge the technical skills gap in organizations,
cutting implementation time by 50% while improving accuracy across all
expertise levels. Third, we provide valuable insights for organizations
designing human-AI collaborative systems, showing that our approach reduced
error resolution time by 73% and significantly accelerated employee learning
curves. Finally, we establish empirical support for natural language as an
effective interface for complex technical systems, offering organizations a
path to democratize ML capabilities without compromising quality or
performance.

</details>


### [34] [Exploring Collaboration Patterns and Strategies in Human-AI Co-creation through the Lens of Agency: A Scoping Review of the Top-tier HCI Literature](https://arxiv.org/abs/2507.06000)
*Shuning Zhang,Hui Wang,Xin Yi*

Main category: cs.HC

TL;DR: 论文综述了HCI/CSCW领域134篇顶会论文，提出了一套关于AI协作中机构分配与控制机制的理论框架、操作目录和跨情境映射，并为未来研究提供了指导。


<details>
  <summary>Details</summary>
Motivation: 随着AI在共同创作中的角色日益重要，理解其机构分配与动态控制机制变得至关重要，但目前缺乏系统性的综述。

Method: 回顾了过去20年134篇来自CHI、UIST、CSCW等顶会的HCI/CSCW论文。

Result: 提出了四项主要贡献：理论框架、控制机制目录、跨情境映射及未来研究指导。

Conclusion: 填补了HCI/CSCW领域对AI协作中机构与控制机制系统性研究的空白，为未来合作系统设计提供了理论与实践基础。

Abstract: As Artificial Intelligence (AI) increasingly becomes an active collaborator
in co-creation, understanding the distribution and dynamic of agency is
paramount. The Human-Computer Interaction (HCI) perspective is crucial for this
analysis, as it uniquely reveals the interaction dynamics and specific control
mechanisms that dictate how agency manifests in practice. Despite this
importance, a systematic synthesis mapping agency configurations and control
mechanisms within the HCI/CSCW literature is lacking. Addressing this gap, we
reviewed 134 papers from top-tier HCI/CSCW venues (e.g., CHI, UIST, CSCW) over
the past 20 years. This review yields four primary contributions: (1) an
integrated theoretical framework structuring agency patterns, control
mechanisms, and interaction contexts, (2) a comprehensive operational catalog
of control mechanisms detailing how agency is implemented; (3) an actionable
cross-context map linking agency configurations to diverse co-creative
practices; and (4) grounded implications and guidance for future CSCW research
and the design of co-creative systems, addressing aspects like trust and
ethics.

</details>


### [35] [Large Language Models Predict Human Well-being -- But Not Equally Everywhere](https://arxiv.org/abs/2507.06141)
*Pat Pataranutaporn,Nattavudh Powdthavee,Chayapatr Archiwaranguprok,Pattie Maes*

Main category: cs.HC

TL;DR: 大语言模型（LLMs）能预测全球幸福感，但受限于训练数据偏差，表现不均衡。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在多样全球人群中预测主观幸福感的能力，以验证其适用于经济、医疗和政策决策。

Method: 使用来自64个国家64,000人的数据测试四种大语言模型，并进行预注册实验。

Result: LLMs在训练数据不足的国家表现较差，依赖语言相似性而非概念理解；改进数据后仍有差距。

Conclusion: LLMs在预测全球幸福感方面有潜力但存局限性，需充分验证后应用。

Abstract: Subjective well-being is a key metric in economic, medical, and policy
decision-making. As artificial intelligence provides scalable tools for
modelling human outcomes, it is crucial to evaluate whether large language
models (LLMs) can accurately predict well-being across diverse global
populations. We evaluate four leading LLMs using data from 64,000 individuals
in 64 countries. While LLMs capture broad correlates such as income and health,
their predictive accuracy decreases in countries underrepresented in the
training data, highlighting systematic biases rooted in global digital and
economic inequality. A pre-registered experiment demonstrates that LLMs rely on
surface-level linguistic similarity rather than conceptual understanding,
leading to systematic misestimations in unfamiliar or resource-limited
settings. Injecting findings from underrepresented contexts substantially
enhances performance, but a significant gap remains. These results highlight
both the promise and limitations of LLMs in predicting global well-being,
underscoring the importance of robust validation prior to their implementation
across these areas.

</details>


### [36] [V(is)owel: An Interactive Vowel Chart to Understand What Makes Visual Pronunciation Effective in Second Language Learning](https://arxiv.org/abs/2507.06202)
*Charlotte Kiesel,Dipayan Mukherjee,Mark Hasegawa-Johnson,Karrie Karahalios*

Main category: cs.HC

TL;DR: 视觉反馈（如V(is)owel）通过直观展示发音差异，帮助二语学习者更快改善发音，且能激发更多练习动机。


<details>
  <summary>Details</summary>
Motivation: 此前研究未明确视觉反馈中哪些方面对发音改善有效，因此设计V(is)owel以探索视觉反馈的作用机制。

Method: 开发交互式元音图表V(is)owel，与纯听觉方法对比，分析学习者在视觉与听觉反馈中的表现差异。

Result: V(is)owel通过直接映射生理动作提供有效反馈，且学习者更倾向于使用视觉工具进行练习。

Conclusion: 设计发音学习工具时，应加入直观的生理动作反馈，视觉反馈在二语学习中具有显著潜力。

Abstract: Visual feedback speeds up learners' improvement of pronunciation in a second
language. The visual combined with audio allows speakers to see sounds and
differences in pronunciation that they are unable to hear. Prior studies have
tested different visual methods for improving pronunciation, however, we do not
have conclusive understanding of what aspects of the visualizations contributed
to improvements. Based on previous work, we created V(is)owel, an interactive
vowel chart. Vowel charts provide actionable feedback by directly mapping
physical tongue movement onto a chart. We compared V(is)owel with an
auditory-only method to explore how learners parse visual and auditory feedback
to understand how and why visual feedback is effective for pronunciation
improvement. The findings suggest that designers should include explicit
anatomical feedback that directly maps onto physical movement for phonetically
untrained learners. Furthermore, visual feedback has the potential to motivate
more practice since all eight of the participants cited using the visuals as a
goal with V(is)owel versus relying on their own judgment with audio alone.
Their statements are backed up by all participants practicing words with
V(is)owel more than with audio-only. Our results indicate that V(is)owel is
effective at providing actionable feedback, demonstrating the potential of
visual feedback methods in second language learning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [37] [Self-Attention Based Multi-Scale Graph Auto-Encoder Network of 3D Meshes](https://arxiv.org/abs/2507.05304)
*Saqib Nazir,Olivier Lézoray,Sébastien Bougleux*

Main category: cs.GR

TL;DR: 3DGeoMeshNet是一种基于GCN的新框架，通过各向异性卷积层在空间域中学习全局和局部特征，直接在原始多边形网格格式上进行操作，提高了重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有的GCN方法在处理3D网格时依赖各向同性滤波器或频谱分解，无法同时捕捉局部和全局特征，且通常需转换为中间表示，导致重建不准确。3DGeoMeshNet旨在解决这些问题。

Method: 3DGeoMeshNet采用基于GCN的框架，使用各向异性卷积层和多尺度编码器-解码器结构，直接在原始网格上操作，通过全局和局部路径捕捉特征。

Result: 在COMA数据集上的实验表明，3DGeoMeshNet在重建精度方面表现高效。

Conclusion: 3DGeoMeshNet通过直接在空间域操作原始网格，结合全局和局部特征学习，实现了更准确的3D形状重建。

Abstract: 3D meshes are fundamental data representations for capturing complex
geometric shapes in computer vision and graphics applications. While
Convolutional Neural Networks (CNNs) have excelled in structured data like
images, extending them to irregular 3D meshes is challenging due to the
non-Euclidean nature of the data. Graph Convolutional Networks (GCNs) offer a
solution by applying convolutions to graph-structured data, but many existing
methods rely on isotropic filters or spectral decomposition, limiting their
ability to capture both local and global mesh features. In this paper, we
introduce 3D Geometric Mesh Network (3DGeoMeshNet), a novel GCN-based framework
that uses anisotropic convolution layers to effectively learn both global and
local features directly in the spatial domain. Unlike previous approaches that
convert meshes into intermediate representations like voxel grids or point
clouds, our method preserves the original polygonal mesh format throughout the
reconstruction process, enabling more accurate shape reconstruction. Our
architecture features a multi-scale encoder-decoder structure, where separate
global and local pathways capture both large-scale geometric structures and
fine-grained local details. Extensive experiments on the COMA dataset
containing human faces demonstrate the efficiency of 3DGeoMeshNet in terms of
reconstruction accuracy.

</details>


### [38] [LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures](https://arxiv.org/abs/2507.06109)
*Seungoh Han,Jaehoon Jang,Hyunsu Kim,Jaeheung Surh,Junhyung Kwak,Hyowon Ha,Kyungdon Joo*

Main category: cs.GR

TL;DR: 提出了一种基于3D高斯球面投影（3DGS）的新型视图合成框架LighthouseGS，适用于手持设备拍摄的室内全景图像，解决了窄基线和纹理缺失带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法需要精细拍摄的完整场景图像，限制了普通用户的使用；希望通过简单的全景式手持设备拍摄实现高质量的视图合成。

Method: 利用粗略的几何先验（设备位姿和单目深度估计）和室内平面结构，提出平面支架组装初始化方法和稳定剪枝策略，并引入几何与光度校正。

Result: 在真实和合成的室内场景中测试，LighthouseGS实现了逼真的渲染效果，超越现有方法。

Conclusion: LighthouseGS为全景视图合成和物体放置提供了实用且高质量的解决方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel
view synthesis (NVS) with impressive quality in indoor scenes. However,
achieving high-fidelity rendering requires meticulously captured images
covering the entire scene, limiting accessibility for general users. We aim to
develop a practical 3DGS-based NVS framework using simple panorama-style motion
with a handheld camera (e.g., mobile device). While convenient, this
rotation-dominant motion and narrow baseline make accurate camera pose and 3D
point estimation challenging, especially in textureless indoor scenes. To
address these challenges, we propose LighthouseGS, a novel framework inspired
by the lighthouse-like sweeping motion of panoramic views. LighthouseGS
leverages rough geometric priors, such as mobile device camera poses and
monocular depth estimation, and utilizes the planar structures often found in
indoor environments. We present a new initialization method called plane
scaffold assembly to generate consistent 3D points on these structures,
followed by a stable pruning strategy to enhance geometry and optimization
stability. Additionally, we introduce geometric and photometric corrections to
resolve inconsistencies from motion drift and auto-exposure in mobile devices.
Tested on collected real and synthetic indoor scenes, LighthouseGS delivers
photorealistic rendering, surpassing state-of-the-art methods and demonstrating
the potential for panoramic view synthesis and object placement.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [39] [Adaptive Variation-Resilient Random Number Generator for Embedded Encryption](https://arxiv.org/abs/2507.05523)
*Furqan Zahoor,Ibrahim A. Albulushi,Saleh Bunaiyan,Anupam Chattopadhyay,Hesham ElSawy,Feras Al-Dirini*

Main category: cs.ET

TL;DR: 本文提出了一种适应性强、能抵抗变异的随机数生成器（RNG），用于嵌入式加密应用，通过动态跟踪物理熵源的信号漂移或偏差，生成无偏的随机数流。


<details>
  <summary>Details</summary>
Motivation: 随着物联网（IoT）中对用户数据安全的需求增长，嵌入式加密变得至关重要，需要轻量级高质量的RNG。现有的随机数生成技术在高变异环境下容易产生偏差。

Method: 使用自适应数字化器和自适应电压参考来动态跟踪随机信号的变化，从而生成无偏的随机数流。研究中采用随机磁性隧道结（sMTJ）作为熵源。

Result: 通过模拟和FPGA实验，生成的随机数流通过了NIST统计测试，实现了高质量的加密级随机数生成，且硬件成本显著降低。

Conclusion: 该方法有效解决了物理熵源在变异环境下的偏差问题，为嵌入式加密提供了高效可靠的随机数生成方案。

Abstract: With a growing interest in securing user data within the internet-of-things
(IoT), embedded encryption has become of paramount importance, requiring
light-weight high-quality Random Number Generators (RNGs). Emerging stochastic
device technologies produce random numbers from stochastic physical processes
at high quality, however, their generated random number streams are adversely
affected by process and supply voltage variations, which can lead to bias in
the generated streams. In this work, we present an adaptive variation-resilient
RNG capable of extracting unbiased encryption-grade random number streams from
physically driven entropy sources, for embedded cryptography applications. As a
proof of concept, we employ a stochastic magnetic tunnel junction (sMTJ) device
as an entropy source. The impact of variations in the sMTJ is mitigated by
employing an adaptive digitizer with an adaptive voltage reference that
dynamically tracks any stochastic signal drift or deviation, leading to
unbiased random bit stream generation. The generated unbiased bit streams, due
to their higher entropy, then only need to undergo simplified post-processing.
Statistical randomness tests based on the National Institute of Standards and
Technology (NIST) test suite are conducted on bit streams obtained using
simulations and FPGA entropy source emulation experiments, validating
encryption-grade randomness at a significantly reduced hardware cost, and
across a wide range of process-induced device variations and supply voltage
fluctuations.

</details>


### [40] [Practical design and performance of physical reservoir computing using hysteresis](https://arxiv.org/abs/2507.06063)
*Yuhei Yamada*

Main category: cs.ET

TL;DR: 本文探讨了基于磁滞系统的物理储存计算模型的设计、性能及限制，为实际应用提供了简单易建的实用指南。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过简单设计的信息处理技术提升物理储存计算的性能，以推动其实际应用。

Method: 使用由独立磁滞系统组成的储存模型，讨论其设计、性能和限制。

Result: 为基于磁滞系统的储存模型构建提供了实用指南。

Conclusion: 该研究为磁滞系统在物理储存计算中的实际应用奠定了基础。

Abstract: Physical reservoir computing is an innovative idea for using physical
phenomena as computational resources. Recent research has revealed that
information processing techniques can improve the performance, but for
practical applications, it is equally important to study the level of
performance with a simple design that is easy to construct experimentally. We
focus on a reservoir composed of independent hysteretic systems as a model
suitable for the practical implementation of physical reservoir computing. In
this paper, we discuss the appropriate design of this reservoir, its
performance, and its limitations. This research will serve as a practical
guideline for constructing hysteresis-based reservoirs.

</details>


### [41] [Hedge Funds on a Swamp: Analyzing Patterns, Vulnerabilities, and Defense Measures in Blockchain Bridges [Experiment, Analysis \& Benchmark]](https://arxiv.org/abs/2507.06156)
*Poupak Azad,Jiahua Xu,Yebo Feng,Preston Strowbridge,Cuneyt Akcora*

Main category: cs.ET

TL;DR: 区块链桥梁是实现不同区块链网络互操作性的关键基础设施，但其安全性问题日益突出，成为Web3中最大的经济损失来源。本文系统化研究了桥梁的设计与安全，提出了安全优先级、攻击向量分析及防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着区块链桥梁的广泛使用（月交易量超过240亿美元），其安全漏洞导致的损失不断增加。为了构建稳健的跨链生态，亟需系统化研究桥梁的安全问题。

Method: 研究定义了三个桥梁安全优先级，形式化了13个桥梁的架构结构，识别了23个基于现实攻击的攻击向量，并分析了43个典型攻击场景。

Result: 研究发现设计中的常见缺陷（如访问控制、验证逻辑问题），提出了分层的威胁模型，并通过静态代码和交易网络分析揭示了攻击行为模式。

Conclusion: 文章提出了桥梁架构设计的决策框架和防御机制（如分层验证和熔断机制），为标准化跨链基础设施奠定了基础。

Abstract: Blockchain bridges have become essential infrastructure for enabling
interoperability across different blockchain networks, with more than $24B
monthly bridge transaction volume. However, their growing adoption has been
accompanied by a disproportionate rise in security breaches, making them the
single largest source of financial loss in Web3. For cross-chain ecosystems to
be robust and sustainable, it is essential to understand and address these
vulnerabilities. In this study, we present a comprehensive systematization of
blockchain bridge design and security. We define three bridge security priors,
formalize the architectural structure of 13 prominent bridges, and identify 23
attack vectors grounded in real-world blockchain exploits. Using this
foundation, we evaluate 43 representative attack scenarios and introduce a
layered threat model that captures security failures across source chain,
off-chain, and destination chain components.
  Our analysis at the static code and transaction network levels reveals
recurring design flaws, particularly in access control, validator trust
assumptions, and verification logic, and identifies key patterns in adversarial
behavior based on transaction-level traces. To support future development, we
propose a decision framework for bridge architecture design, along with defense
mechanisms such as layered validation and circuit breakers. This work provides
a data-driven foundation for evaluating bridge security and lays the groundwork
for standardizing resilient cross-chain infrastructure.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [42] [High Order Collaboration-Oriented Federated Graph Neural Network for Accurate QoS Prediction](https://arxiv.org/abs/2507.05308)
*Zehuan Chen,Xiangwei Lai*

Main category: cs.DC

TL;DR: 论文提出了一种基于高阶协作的联邦图神经网络（HC-FGNN），用于隐私保护的QoS预测，通过注意力机制放大用户-服务图以捕捉隐式用户交互，并提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于FGNN的QoS预测方法未能利用隐式用户-用户交互，且在隐私保护方面存在不足，因此需要一种既能提高预测准确性又能保护隐私的新方法。

Method: 提出HC-FGNN，通过注意力机制扩展显式用户-服务图以捕捉高阶协作（隐式用户交互），并采用轻量级消息聚合方法提高计算效率。

Result: 在两个真实QoS数据集上的实验表明，HC-FGNN在预测准确性和隐私保护方面均优于现有方法。

Conclusion: HC-FGNN通过捕捉隐式用户交互并优化计算效率，实现了高精度的QoS预测和隐私保护，为云服务选择提供了有效工具。

Abstract: Predicting Quality of Service (QoS) data crucial for cloud service selection,
where user privacy is a critical concern. Federated Graph Neural Networks
(FGNNs) can perform QoS data prediction as well as maintaining user privacy.
However, existing FGNN-based QoS predictors commonly implement on-device
training on scattered explicit user-service graphs, thereby failing to utilize
the implicit user-user interactions. To address this issue, this study proposes
a high order collaboration-oriented federated graph neural network (HC-FGNN) to
obtain accurate QoS prediction with privacy preservation. Concretely, it
magnifies the explicit user-service graphs following the principle of attention
mechanism to obtain the high order collaboration, which reflects the implicit
user-user interactions. Moreover, it utilizes a lightweight-based message
aggregation way to improve the computational efficiency. The extensive
experiments on two QoS datasets from real application indicate that the
proposed HC-FGNN possesses the advantages of high prediction accurate and
privacy protection.

</details>


### [43] [Archetype-Aware Predictive Autoscaling with Uncertainty Quantification for Serverless Workloads on Kubernetes](https://arxiv.org/abs/2507.05653)
*Guilin Zhang,Srinivas Vippagunta,Raghavendra Nandagopal,Suchitra Raman,Jeff Xu,Marcus Pfeiffer,Shree Chatterjee,Ziqi Tan,Wulan Guo,Hailong Jiang*

Main category: cs.DC

TL;DR: AAPA利用弱监督技术高效分类动态工作负载，减少SLO违规50%，提升响应时间40%，但资源成本增加2-8倍。


<details>
  <summary>Details</summary>
Motivation: HPEC平台需要高效管理动态工作负载以满足SLO，但目前方法存在挑战。

Method: 提出AAPA系统，采用弱监督技术将工作负载分类为四种模式，准确率99.8%。

Result: 在Azure Functions测试中，AAPA减少SLO违规50%，响应时间提升40%，但资源成本增加2-8倍。

Conclusion: AAPA在动态负载管理上表现优异，但需权衡性能提升与资源成本。

Abstract: High-performance extreme computing (HPEC) platforms increasingly adopt
serverless paradigms, yet face challenges in efficiently managing highly
dynamic workloads while maintaining service-level objectives (SLOs). We propose
**AAPA**, an archetype-aware predictive autoscaling system that leverages weak
supervision to automatically classify 300\,000\,+ workload windows into four
archetypes (PERIODIC, SPIKE, RAMP, STATIONARY\_NOISY) with 99.8\% accuracy.
Evaluation on publicly available Azure Functions traces shows that AAPA reduces
SLO violations by up to 50\%, improves response time by 40\%, albeit with a
2--8\,$\times$ increase in resource cost under spike-heavy loads.

</details>


### [44] [Air-FedGA: A Grouping Asynchronous Federated Learning Mechanism Exploiting Over-the-air Computation](https://arxiv.org/abs/2507.05704)
*Qianpiao Ma,Junlong Zhou,Xiangpeng Hou,Jianchun Liu,Hongli Xu,Jianeng Miao,Qingmin Jia*

Main category: cs.DC

TL;DR: 提出了一种基于空中计算的分组异步联邦学习机制（Air-FedGA），解决了通信资源限制和设备异构性挑战，并通过实验验证了其效率提升。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式边缘设备上训练模型时面临通信资源限制、设备异构性和数据非独立同分布等问题，空中计算虽然能高效利用通信资源，但对同步要求严格。

Method: 提出Air-FedGA机制，将设备分组进行空中计算聚合，组间异步更新全局模型，并通过功率控制和分组算法优化训练时间。

Result: 实验表明，Air-FedGA能比现有方案加速训练29.9%-71.6%，并理论证明了其收敛性。

Conclusion: Air-FedGA结合了空中计算和异步联邦学习的优势，有效解决了通信和异构性问题，显著提升了训练效率。

Abstract: Federated learning (FL) is a new paradigm to train AI models over distributed
edge devices (i.e., workers) using their local data, while confronting various
challenges including communication resource constraints, edge heterogeneity and
data Non-IID. Over-the-air computation (AirComp) is a promising technique to
achieve efficient utilization of communication resource for model aggregation
by leveraging the superposition property of a wireless multiple access channel
(MAC). However, AirComp requires strict synchronization among edge devices,
which is hard to achieve in heterogeneous scenarios. In this paper, we propose
an AirComp-based grouping asynchronous federated learning mechanism
(Air-FedGA), which combines the advantages of AirComp and asynchronous FL to
address the communication and heterogeneity challenges. Specifically, Air-FedGA
organizes workers into groups and performs over-the-air aggregation within each
group, while groups asynchronously communicate with the parameter server to
update the global model. In this way, Air-FedGA accelerates the FL model
training by over-the-air aggregation, while relaxing the synchronization
requirement of this aggregation technology. We theoretically prove the
convergence of Air-FedGA. We formulate a training time minimization problem for
Air-FedGA and propose the power control and worker grouping algorithm to solve
it, which jointly optimizes the power scaling factors at edge devices, the
denoising factors at the parameter server, as well as the worker grouping
strategy. We conduct experiments on classical models and datasets, and the
results demonstrate that our proposed mechanism and algorithm can speed up FL
model training by 29.9%-71.6% compared with the state-of-the-art solutions.

</details>


### [45] [ECORE: Energy-Conscious Optimized Routing for Deep Learning Models at the Edge](https://arxiv.org/abs/2507.06011)
*Daghash K. Alqahtani,Maria A. Rodriguez,Muhammad Aamir Cheema,Hamid Rezatofighi,Adel N. Toosi*

Main category: cs.DC

TL;DR: ECORE框架通过动态路由策略优化边缘设备上的目标检测任务，在保持高精度的同时显著降低能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算中，实时视觉分析（如目标检测）对资源受限的边缘设备提出了高要求，需要同时优化能耗和检测精度。

Method: ECORE框架结合多种动态路由策略（估计技术和贪心选择算法），将图像处理请求分配到最合适的边缘设备-模型组合，动态平衡能效和性能。

Result: 实验显示，ECORE能将能耗和延迟分别降低45%和49%，检测精度仅损失2%。

Conclusion: ECORE在资源受限的边缘设备上实现了高效的实时目标检测，平衡了能效和精度。

Abstract: Edge computing enables data processing closer to the source, significantly
reducing latency an essential requirement for real-time vision-based analytics
such as object detection in surveillance and smart city environments. However,
these tasks place substantial demands on resource constrained edge devices,
making the joint optimization of energy consumption and detection accuracy
critical. To address this challenge, we propose ECORE, a framework that
integrates multiple dynamic routing strategies including estimation based
techniques and a greedy selection algorithm to direct image processing requests
to the most suitable edge device-model pair. ECORE dynamically balances energy
efficiency and detection performance based on object characteristics. We
evaluate our approach through extensive experiments on real-world datasets,
comparing the proposed routers against widely used baseline techniques. The
evaluation leverages established object detection models (YOLO, SSD,
EfficientDet) and diverse edge platforms, including Jetson Orin Nano, Raspberry
Pi 4 and 5, and TPU accelerators. Results demonstrate that our proposed
context-aware routing strategies can reduce energy consumption and latency by
45% and 49%, respectively, while incurring only a 2% loss in detection accuracy
compared to accuracy-centric methods.

</details>


### [46] [Efficient Federated Learning with Timely Update Dissemination](https://arxiv.org/abs/2507.06031)
*Juncheng Jia,Ji Liu,Chao Huo,Yihui Shen,Yang Zhou,Huaiyu Dai,Dejing Dou*

Main category: cs.DC

TL;DR: 该论文提出了一种高效的联邦学习方法，通过利用额外的下行带宽资源，设计了异步（FedASMU）和同步（FedSSMU）两种策略，显著提升了模型的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在处理分布式数据时表现出色，但现有方法在更新传播的及时性上存在不足。本文旨在通过利用额外下行带宽资源，解决这一问题。

Method: 提出异步（FedASMU）和同步（FedSSMU）两种策略。异步策略结合动态模型聚合和自适应模型调整；同步策略则延伸了异步方法的核心思想。

Result: 理论和实验证明，FedASMU和FedSSMU在准确性和效率上显著优于基线方法，最高提升145.87%的准确性和97.59%的效率。

Conclusion: 本文方法通过异步和同步策略优化了联邦学习的更新传播，显著提升了性能。

Abstract: Federated Learning (FL) has emerged as a compelling methodology for the
management of distributed data, marked by significant advancements in recent
years. In this paper, we propose an efficient FL approach that capitalizes on
additional downlink bandwidth resources to ensure timely update dissemination.
Initially, we implement this strategy within an asynchronous framework,
introducing the Asynchronous Staleness-aware Model Update (FedASMU), which
integrates both server-side and device-side methodologies. On the server side,
we present an asynchronous FL system model that employs a dynamic model
aggregation technique, which harmonizes local model updates with the global
model to enhance both accuracy and efficiency. Concurrently, on the device
side, we propose an adaptive model adjustment mechanism that integrates the
latest global model with local models during training to further elevate
accuracy. Subsequently, we extend this approach to a synchronous context,
referred to as FedSSMU. Theoretical analyses substantiate the convergence of
our proposed methodologies. Extensive experiments, encompassing six models and
five public datasets, demonstrate that FedASMU and FedSSMU significantly
surpass baseline methods in terms of both accuracy (up to 145.87%) and
efficiency (up to 97.59%).

</details>


### [47] [A Unified Ontology for Scalable Knowledge Graph-Driven Operational Data Analytics in High-Performance Computing Systems](https://arxiv.org/abs/2507.06107)
*Junaid Ahmed Khan,Andrea Bartolini*

Main category: cs.DC

TL;DR: 提出首个统一的高性能计算（HPC）系统操作数据分 析（ODA）本体，支持异构数据中心间的语义互操作性，并优化建模以显著减少知识图谱存储开销。


<details>
  <summary>Details</summary>
Motivation: 随着HPC系统扩展到支持生成式AI等复杂工作负载，对高效、可靠且可互操作的遥测数据分析的需求日益迫切。现有方法因依赖无模式存储方案而限制了数据可访问性和语义集成。

Method: 设计统一的本体，结合M100和F-DATA两大公开ODA数据集，通过36个能力问题验证，并引入建模优化以减少存储开销。

Result: 与之前方法相比，存储开销减少38.84%，根据部署配置还可额外减少26.82%。支持跨异构HPC系统的分析。

Conclusion: 该工作为可扩展的ODA知识图谱奠定了基础，支持异构HPC系统的跨系统分析。

Abstract: Modern high-performance computing (HPC) systems generate massive volumes of
heterogeneous telemetry data from millions of sensors monitoring compute,
memory, power, cooling, and storage subsystems. As HPC infrastructures scale to
support increasingly complex workloads-including generative AI-the need for
efficient, reliable, and interoperable telemetry analysis becomes critical.
Operational Data Analytics (ODA) has emerged to address these demands; however,
the reliance on schema-less storage solutions limits data accessibility and
semantic integration. Ontologies and knowledge graphs (KG) provide an effective
way to enable efficient and expressive data querying by capturing domain
semantics, but they face challenges such as significant storage overhead and
the limited applicability of existing ontologies, which are often tailored to
specific HPC systems only. In this paper, we present the first unified ontology
for ODA in HPC systems, designed to enable semantic interoperability across
heterogeneous data centers. Our ontology models telemetry data from the two
largest publicly available ODA datasets-M100 (Cineca, Italy) and F-DATA
(Fugaku, Japan)-within a single data model. The ontology is validated through
36 competency questions reflecting real-world stakeholder requirements, and we
introduce modeling optimizations that reduce knowledge graph (KG) storage
overhead by up to 38.84% compared to a previous approach, with an additional
26.82% reduction depending on the desired deployment configuration. This work
paves the way for scalable ODA KGs and supports not only analysis within
individual systems, but also cross-system analysis across heterogeneous HPC
systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [48] [PBE Meets LLM: When Few Examples Aren't Few-Shot Enough](https://arxiv.org/abs/2507.05403)
*Shuning Zhang,Yongjoo Park*

Main category: cs.DB

TL;DR: 评估大型语言模型（LLMs）在基于示例的编程（PBE）任务中的表现，特别是处理表格数据转换任务，并比较不同提示策略的效果。


<details>
  <summary>Details</summary>
Motivation: 传统PBE系统在狭窄领域和固定输入格式中表现良好，但不清楚LLMs在此类任务中的表现。本文旨在填补这一空白。

Method: 通过生成函数将输入表转换为输出表，测试多种LLMs和提示策略（如单次与多次尝试），并比较有无PBE知识的效果。提出了一种混合方法，先调用传统PBE求解器，必要时再使用LLMs。

Result: LLMs支持更广泛的输入格式且准确性更高，但处理模糊任务时表现不佳。混合方法结合两者优势，提高了整体成功率。

Conclusion: LLMs在PBE任务中表现优于传统方法，但仍有改进空间。混合方法提供了一种有效解决方案。

Abstract: Large language models (LLMs) can generate code from natural language
descriptions. Their performance is typically evaluated using programming
benchmarks that simulate real-world tasks. These benchmarks provide
specifications in the form of docstrings, function signatures, or bug reports.
The model then generates a program, which is tested against predefined test
cases. In contrast, Programming by Example (PBE) uses input-output examples as
the specification. Traditional PBE systems rely on search-based methods over
restricted transformation spaces. They are usually designed for narrow domains
and fixed input formats. It remains unclear how well LLMs perform on PBE tasks.
  In this work, we evaluate LLMs on PBE tasks involving tabular data
transformations. We prompt models to generate functions that convert an input
table to an output table. We test the generated functions on unseen inputs to
measure accuracy. Our study includes multiple LLMs and evaluates different
prompting strategies, such as one-shot vs. multi-try. We also compare
performance with and without PBE-specific knowledge. Finally, we propose a
hybrid method that calls a traditional PBE solver first, and then falls back to
LLMs if necessary. Our results show that LLMs support more diverse input
formats and achieve higher accuracy than conventional methods. However, they
struggle with tasks that contain ambiguity. The hybrid approach improves
overall success by combining the strengths of both approaches.

</details>


### [49] [GTRSS: Graph-based Top-$k$ Representative Similar Subtrajectory Query](https://arxiv.org/abs/2507.05542)
*Mingchang Ge,Liping Wang,Xuemin Lin,Yuang Zhang,Kunming Wang*

Main category: cs.DB

TL;DR: 该论文提出了一种基于图的新框架GTRSS，用于高效解决Top-k代表性相似子轨迹查询问题，显著提升了查询效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高成本的过滤-验证框架，导致响应时间慢，因此需要更高效的解决方案。

Method: GTRSS采用双层图索引离线聚类相似子轨迹，在线阶段通过导航图快速检索结果，结合R树和网格过滤以及DTSM度量优化。

Result: 实验显示GTRSS检索准确率超过90%，查询性能提升高达两个数量级。

Conclusion: GTRSS是首个基于图的解决方案，显著提升了子轨迹搜索的效率和准确性。

Abstract: Trajectory mining has attracted significant attention. This paper addresses
the Top-k Representative Similar Subtrajectory Query (TRSSQ) problem, which
aims to find the k most representative subtrajectories similar to a query.
Existing methods rely on costly filtering-validation frameworks, resulting in
slow response times. Addressing this, we propose GTRSS, a novel Graph-based
Top-k Representative Similar Subtrajectory Query framework. During the offline
phase, GTRSS builds a dual-layer graph index that clusters trajectories
containing similar representative subtrajectories. In the online phase, it
efficiently retrieves results by navigating the graph toward query-relevant
clusters, bypassing full-dataset scanning and heavy computation. To support
this, we introduce the Data Trajectory Similarity Metric (DTSM) to measure the
most similar subtrajectory pair. We further combine R-tree and grid filtering
with DTSM pruning rules to speed up index building. To the best of our
knowledge, GTRSS is the first graph-based solution for top-k subtrajectory
search. Experiments on real datasets demonstrate that GTRSS significantly
enhances both efficiency and accuracy, achieving a retrieval accuracy of over
90 percent and up to two orders of magnitude speedup in query performance.

</details>


### [50] [Prompt Migration: Stabilizing GenAI Applications with Evolving Large Language Models](https://arxiv.org/abs/2507.05573)
*Shivani Tripathi,Pushpanjali Nema,Aditya Halder,Shi Qiao,Alekh Jindal*

Main category: cs.DB

TL;DR: 本文提出了一种系统方法——提示迁移——以解决大型语言模型（LLM）快速升级导致的生成式AI应用不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 企业需要可靠的生成式AI应用，但LLM的快速变化导致应用行为不一致，影响关键业务工作流的可靠性。

Method: 采用提示迁移框架，包括提示重新设计和迁移测试平台，以案例分析（Tursio企业搜索应用）验证其有效性。

Result: 通过结构化提示迁移，可以完全恢复因模型漂移而丢失的应用可靠性。

Conclusion: 强调了提示生命周期管理和鲁棒测试的必要性，以确保生成式AI业务应用的稳定性。

Abstract: Generative AI is transforming business applications by enabling natural
language interfaces and intelligent automation. However, the underlying large
language models (LLMs) are evolving rapidly and so prompting them consistently
is a challenge. This leads to inconsistent and unpredictable application
behavior, undermining the reliability that businesses require for
mission-critical workflows. In this paper, we introduce the concept of prompt
migration as a systematic approach to stabilizing GenAI applications amid
changing LLMs. Using the Tursio enterprise search application as a case study,
we analyze the impact of successive GPT model upgrades, detail our migration
framework including prompt redesign and a migration testbed, and demonstrate
how these techniques restore application consistency. Our results show that
structured prompt migration can fully recover the application reliability that
was lost due to model drift. We conclude with practical lessons learned,
emphasizing the need for prompt lifecycle management and robust testing to
ensure dependable GenAI-powered business applications.

</details>


### [51] [Towards an Application-Centric Benchmark Suite for Spatiotemporal Database Systems](https://arxiv.org/abs/2507.05869)
*Tim C. Rese,David Bermbach*

Main category: cs.DB

TL;DR: 本文强调需要构建一个面向应用的时空数据库基准测试套件，以满足对系统服务质量的理解和选择。


<details>
  <summary>Details</summary>
Motivation: 由于物联网设备的普及，时空数据量持续增长，但现有基准测试仅关注小范围孤立问题，缺少全面的应用导向测试套件。

Method: 提出构建模块化基准测试套件的需求，讨论领域挑战，并设计其架构。

Result: 提出了一个面向应用的时空数据库基准测试套件的架构框架。

Conclusion: 为时空数据库系统开发一个综合的基准测试套件是迫切需要的。

Abstract: Spatiotemporal data play a key role for mobility-based applications and are
their produced volume is growing continuously, among others, due to the
increased availability of IoT devices.
  When working with spatiotemporal data, developers rely on spatiotemporal
database systems such as PostGIS or MobilityDB.
  For better understanding their quality of service behavior and then choosing
the best system, benchmarking is the go-to approach.
  Unfortunately, existing work in this field studies only small isolated
aspects and a comprehensive application-centric benchmark suite is still
missing.
  In this paper, we argue that an application-centric benchmark suite for
spatiotemporal database systems is urgently needed.
  We identify requirements for such a benchmark suite, discuss domain-specific
challenges, and sketch-out the architecture of a modular benchmarking suite.

</details>


### [52] [Towards Serverless Processing of Spatiotemporal Big Data Queries](https://arxiv.org/abs/2507.06005)
*Diana Baumann,Tim C. Rese,David Bermbach*

Main category: cs.DB

TL;DR: 提出了一种基于无服务器计算的时空数据处理方法，通过并行执行子查询来解决大数据场景的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于关系数据库的系统在大规模时空数据处理中扩展性有限，需要更高效的解决方案。

Method: 将查询分解为小任务，利用FaaS平台的即时扩展能力并行执行。

Result: 部分解决了大规模时空数据处理的扩展性需求。

Conclusion: 无服务器计算为时空数据处理提供了一种可扩展的新途径。

Abstract: Spatiotemporal data are being produced in continuously growing volumes by a
variety of data sources and a variety of application fields rely on rapid
analysis of such data. Existing systems such as PostGIS or MobilityDB usually
build on relational database systems, thus, inheriting their scale-out
characteristics. As a consequence, big spatiotemporal data scenarios still have
limited support even though many query types can easily be parallelized. In
this paper, we propose our vision of a native serverless data processing
approach for spatiotemporal data: We break down queries into small subqueries
which then leverage the near-instant scaling of Function-as-a-Service platforms
to execute them in parallel. With this, we partially solve the scalability
needs of big spatiotemporal data processing.

</details>


### [53] [Data-Semantics-Aware Recommendation of Diverse Pivot Tables](https://arxiv.org/abs/2507.06171)
*Whanhee Cho,Anna Fariha*

Main category: cs.DB

TL;DR: SAGE是一个数据语义感知系统，用于推荐多样化的数据透视表，解决传统方法在多样性和实用性上的不足。


<details>
  <summary>Details</summary>
Motivation: 在大数据集中，手动识别有用的数据透视表组合费时费力，现有方法在多样性和实用性上表现不佳。

Method: SAGE采用数据语义感知模型衡量单个透视表效用和集合多样性，并结合贪心算法高效筛选高实用性且多样化的透视表组合。

Result: 实验表明，SAGE在三个真实数据集上优于其他方法，并能高效处理高维数据。

Conclusion: SAGE通过数据语义和优化算法，显著提升了数据透视表推荐的多样性和实用性，优于现有工具和LLMs。

Abstract: Data summarization is essential to discover insights from large datasets. In
a spreadsheets, pivot tables offer a convenient way to summarize tabular data
by computing aggregates over some attributes, grouped by others. However,
identifying attribute combinations that will result in useful pivot tables
remains a challenge, especially for high-dimensional datasets. We formalize the
problem of automatically recommending insightful and interpretable pivot
tables, eliminating the tedious manual process. A crucial aspect of
recommending a set of pivot tables is to diversify them. Traditional works
inadequately address the table-diversification problem, which leads us to
consider the problem of pivot table diversification.
  We present SAGE, a data-semantics-aware system for recommending k-budgeted
diverse pivot tables, overcoming the shortcomings of prior work for top-k
recommendations that cause redundancy. SAGE ensures that each pivot table is
insightful, interpretable, and adaptive to the user's actions and preferences,
while also guaranteeing that the set of pivot tables are different from each
other, offering a diverse recommendation. We make two key technical
contributions: (1) a data-semantics-aware model to measure the utility of a
single pivot table and the diversity of a set of pivot tables, and (2) a
scalable greedy algorithm that can efficiently select a set of diverse pivot
tables of high utility, by leveraging data semantics to significantly reduce
the combinatorial search space. Our extensive experiments on three real-world
datasets show that SAGE outperforms alternative approaches, and efficiently
scales to accommodate high-dimensional datasets. Additionally, we present
several case studies to highlight SAGE's qualitative effectiveness over
commercial software and Large Language Models (LLMs).

</details>


### [54] [SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads](https://arxiv.org/abs/2507.06192)
*Jiale Lao,Immanuel Trummer*

Main category: cs.DB

TL;DR: SQLBarber是一个基于大语言模型的系统，用于生成定制化且现实的SQL工作负载，解决了现有SQL生成方法的局限。


<details>
  <summary>Details</summary>
Motivation: 研究致力于解决数据库开发和测试中获取真实SQL查询的困难，同时满足定制化和现实约束的需求。

Method: SQLBarber利用自然语言约束生成SQL模板，并通过自校正模块和贝叶斯优化器高效生成符合目标成本分布的查询。

Result: 系统显著减少了查询生成时间并提高了与目标成本分布的对齐度。

Conclusion: SQLBarber是唯一能生成定制化SQL模板的系统，具有高效性和实用性。

Abstract: Database research and development often require a large number of SQL queries
for benchmarking purposes. However, acquiring real-world SQL queries is
challenging due to privacy concerns, and existing SQL generation methods are
limited in customization and in satisfying realistic constraints. To address
this issue, we present SQLBarber, a system based on Large Language Models
(LLMs) to generate customized and realistic SQL workloads. SQLBarber (i)
eliminates the need for users to manually craft SQL templates in advance, while
providing the flexibility to accept natural language specifications to
constrain SQL templates, (ii) scales efficiently to generate large volumes of
queries matching any user-defined cost distribution (e.g., cardinality and
execution plan cost), and (iii) uses execution statistics from Amazon Redshift
and Snowflake to derive SQL template specifications and query cost
distributions that reflect real-world query characteristics. SQLBarber
introduces (i) a declarative interface for users to effortlessly generate
customized SQL templates, (ii) an LLM-powered pipeline augmented with a
self-correction module that profiles, refines, and prunes SQL templates based
on query costs, and (iii) a Bayesian Optimizer to efficiently explore different
predicate values and identify a set of queries that satisfy the target cost
distribution. We construct and open-source ten benchmarks of varying difficulty
levels and target query cost distributions based on real-world statistics from
Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show
that SQLBarber is the only system that can generate customized SQL templates.
It reduces query generation time by one to three orders of magnitude, and
significantly improves alignment with the target cost distribution, compared
with existing methods.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [55] [Per-Row Activation Counting on Real Hardware: Demystifying Performance Overheads](https://arxiv.org/abs/2507.05556)
*Jumin Kim,Seungmin Baek,Minbok Wi,Hwayong Nam,Michael Jaemin Kim,Sukhan Lee,Kyomin Sohn,Jung Ho Ahn*

Main category: cs.AR

TL;DR: PRAC是一种DRAM读干扰缓解方法，实际性能开销显著低于模拟器估算，研究表明采用关闭页策略可进一步降低开销。


<details>
  <summary>Details</summary>
Motivation: 由于模拟器与真实硬件存在差异，需要通过真实机器实验准确评估PRAC的性能开销。

Method: 在最新CPU上验证时序修改，并使用微基准测试和SPEC CPU2017工作负载进行分析。

Result: PRAC的平均和最大性能开销仅为1.06%和3.28%，比模拟器报告低9.15倍，且关闭页策略有效隐藏了额外开销。

Conclusion: 实际机器测试显示PRAC的开销远低于预期，关闭页策略进一步优化了性能。

Abstract: Per-Row Activation Counting (PRAC), a DRAM read disturbance mitigation
method, modifies key DRAM timing parameters, reportedly causing significant
performance overheads in simulator-based studies. However, given known
discrepancies between simulators and real hardware, real-machine experiments
are vital for accurate PRAC performance estimation. We present the first
real-machine performance analysis of PRAC. After verifying timing modifications
on the latest CPUs using microbenchmarks, our analysis shows that PRAC's
average and maximum overheads are just 1.06% and 3.28% for the SPEC CPU2017
workloads -- up to 9.15x lower than simulator-based reports. Further, we show
that the close page policy minimizes this overhead by effectively hiding the
elongated DRAM row precharge operations due to PRAC from the critical path.

</details>


### [56] [GATMesh: Clock Mesh Timing Analysis using Graph Neural Networks](https://arxiv.org/abs/2507.05681)
*Muhammad Hadir Khan,Matthew Guthaus*

Main category: cs.AR

TL;DR: 提出了基于GNN的GATMesh框架，高效准确地分析时钟网络，延迟误差仅5.27ps，速度比SPICE快47146倍。


<details>
  <summary>Details</summary>
Motivation: 时钟网络在高性能VLSI系统中至关重要，但现有分析方法或慢或不准确。

Method: 采用GNN建模时钟网络，结合结构和物理特征，利用SPICE数据训练。

Result: GATMesh延迟平均误差为5.27ps，速度比SPICE快47146倍。

Conclusion: GATMesh是一种高效准确的时钟网络分析方法。

Abstract: Clock meshes are essential in high-performance VLSI systems for minimizing
skew and handling PVT variations, but analyzing them is difficult due to
reconvergent paths, multi-source driving, and input mesh buffer skew. SPICE
simulations are accurate but slow; yet simplified models miss key effects like
slew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based
framework that models the clock mesh as a graph with augmented structural and
physical features. Trained on SPICE data, GATMesh achieves high accuracy with
average delay error of 5.27ps on unseen benchmarks, while achieving speed-ups
of 47146x over multi-threaded SPICE simulation.

</details>


### [57] [RTGPU: Real-Time Computing with Graphics Processing Units](https://arxiv.org/abs/2507.06069)
*Atiyeh Gheibi-Fetrat,Amirsaeed Ahmadi-Tonekaboni,Farzam Koohi-Ronaghi,Pariya Hajipour,Sana Babayan-Vanestan,Fatemeh Fotouhi,Elahe Mortazavian-Farsani,Pouria Khajehpour-Dezfouli,Sepideh Safari,Shaahin Hessabi,Hamid Sarbazi-Azad*

Main category: cs.AR

TL;DR: GPU在实时系统中的角色及挑战


<details>
  <summary>Details</summary>
Motivation: 研究GPU在实时系统中的广泛应用及其带来的挑战

Method: 综述现有解决方案，包括调度算法、资源管理技术和同步方法

Result: 总结了GPU在实时系统中的表现及其问题

Conclusion: 提出了未来改进GPU可预测性和性能的研究方向

Abstract: In this work, we survey the role of GPUs in real-time systems. Originally
designed for parallel graphics workloads, GPUs are now widely used in
time-critical applications such as machine learning, autonomous vehicles, and
robotics due to their high computational throughput. Their parallel
architecture is well-suited for accelerating complex tasks under strict timing
constraints. However, their integration into real-time systems presents several
challenges, including non-preemptive execution, execution time variability, and
resource contention; factors that can lead to unpredictable delays and deadline
violations. We examine existing solutions that address these challenges,
including scheduling algorithms, resource management techniques, and
synchronization methods, and highlight open research directions to improve GPU
predictability and performance in real-time environments.

</details>


### [58] [PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder Optimization](https://arxiv.org/abs/2507.06127)
*Dongsheng Zuo,Jiadong Zhu,Yang Luo,Yuzhe Ma*

Main category: cs.AR

TL;DR: 提出了一种基于大型语言模型的框架PrefixAgent，通过分解任务和优化搜索空间，显著提升前缀加法器的性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 前缀加法器设计空间随位数指数增长，传统方法存在性能、泛化和可扩展性问题，需更高效的优化方法。

Method: 将优化任务分解为骨架合成与结构细化，利用E-graph收集数据并微调大型语言模型。

Result: PrefixAgent生成的前缀加法器面积更小，同时保持商业EDA工具的可扩展性与泛化能力。

Conclusion: PrefixAgent为前缀加法器优化提供了高性能、可扩展的新方法。

Abstract: Prefix adders are fundamental arithmetic circuits, but their design space
grows exponentially with bit-width, posing significant optimization challenges.
Previous works face limitations in performance, generalization, and
scalability. To address these challenges, we propose PrefixAgent, a large
language model (LLM)-powered framework that enables efficient prefix adder
optimization. Specifically, PrefixAgent reformulates the problem into subtasks
including backbone synthesis and structure refinement, which effectively
reduces the search space. More importantly, this new design perspective enables
us to efficiently collect enormous high-quality data and reasoning traces with
E-graph, which further results in an effective fine-tuning of LLM. Experimental
results show that PrefixAgent synthesizes prefix adders with consistently
smaller areas compared to baseline methods, while maintaining scalability and
generalization in commercial EDA flows.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [59] [A Fuzzy Supervisor Agent Design for Clinical Reasoning Assistance in a Multi-Agent Educational Clinical Scenario Simulation](https://arxiv.org/abs/2507.05275)
*Weibing Zheng,Laurah Turner,Jess Kropczynski,Murat Ozer,Seth Overla,Shane Halse*

Main category: cs.CY

TL;DR: 论文提出了一种模糊监督代理（FSA）设计，用于医学教育中的临床情景模拟平台（MAECSS），通过模糊推理系统实时分析学生决策，提供适应性反馈。


<details>
  <summary>Details</summary>
Motivation: 解决医学教育中临床推理训练的监督难题，提供及时、精准的辅助。

Method: 基于模糊推理系统（FIS），通过预定义的模糊规则分析学生与临床代理的互动，实时生成反馈。

Result: 设计了一种可扩展、灵活且模拟人类监督的FSA框架，未来将进行实证评估。

Conclusion: FSA有望在模拟医学教育中提供高效的监督，下一步是实际应用验证和大规模集成。

Abstract: Assisting medical students with clinical reasoning (CR) during clinical
scenario training remains a persistent challenge in medical education. This
paper presents the design and architecture of the Fuzzy Supervisor Agent (FSA),
a novel component for the Multi-Agent Educational Clinical Scenario Simulation
(MAECSS) platform. The FSA leverages a Fuzzy Inference System (FIS) to
continuously interpret student interactions with specialized clinical agents
(e.g., patient, physical exam, diagnostic, intervention) using pre-defined
fuzzy rule bases for professionalism, medical relevance, ethical behavior, and
contextual distraction. By analyzing student decision-making processes in
real-time, the FSA is designed to deliver adaptive, context-aware feedback and
provides assistance precisely when students encounter difficulties. This work
focuses on the technical framework and rationale of the FSA, highlighting its
potential to provide scalable, flexible, and human-like supervision in
simulation-based medical education. Future work will include empirical
evaluation and integration into broader educational settings. More detailed
design and implementation is~\href{https://github.com/2sigmaEdTech/MAS/}{open
sourced here}.

</details>


### [60] [Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools](https://arxiv.org/abs/2507.05305)
*Lorenzo Lee Solano,Charles Koutcheme,Juho Leinonen,Alexandra Vassar,Jake Renzella*

Main category: cs.CY

TL;DR: 小型专用语言模型通过监督微调（SFT）在教育工具中表现优于大型通用模型，更适合教学场景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如ChatGPT和Gemini）虽然能解析编译器错误，但其计算成本高且容易过度辅助，不适合广泛教学应用。

Method: 使用40,000条C编译器错误数据集对Qwen3-4B、Llama-3.1-8B和Qwen3-32B进行监督微调，并通过专家评审和自动化评估验证效果。

Result: SFT显著提升小型模型的教学质量，性能接近大型模型，证明了专用数据的有效性。

Conclusion: 通过高质量领域数据微调小型模型是教育工具开发的可行策略，并提供了可复现的方法论。

Abstract: Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher
cryptic compiler errors for novice programmers, but their computational scale,
cost, and tendency to over-assist make them problematic for widespread
pedagogical adoption. This work demonstrates that smaller, specialised language
models, enhanced via Supervised Fine-Tuning (SFT), present a more viable
alternative for educational tools. We utilise a new dataset of 40,000 C
compiler error explanations, derived from real introductory programming (CS1/2)
student-generated programming errors, which we used to fine-tune three
open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual
evaluation, combining expert human reviews with a large-scale automated
analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our
results show that SFT significantly boosts the pedagogical quality of smaller
models, achieving performance comparable to much larger models. We analyse the
trade-offs between model size and quality, confirming that fine-tuning compact,
efficient models on high-quality, domain-specific data is a potent strategy for
creating specialised models to drive educational tools. We provide a replicable
methodology to foster broader access to generative AI capabilities in
educational contexts.

</details>


### [61] [Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility](https://arxiv.org/abs/2505.10426)
*Maurice Chiodo,Dennis Müller,Paul Siewert,Jean-Luc Wetherall,Zoya Yasmine,John Burden*

Main category: cs.CY

TL;DR: 该论文探讨了不同人机协作（HITL）设置的法律合规性和安全性，展示了法律责任归属与技术可解释性之间的不可避免的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究人机协作设置的多样性及其在法律和技术上的影响，为解决法律框架中的盲点提供新视角。

Method: 利用计算理论中的预言机概念形式化HITL设置，提出分类法分析失败模式，并评估法律框架的局限性。

Result: 揭示了HITL设置中的设计决策和潜在失败模式，指出法律需更灵活地适应不同设置。

Conclusion: HITL设置涉及多方面挑战，需开发者和立法者共同优化，以实现更好的法律和技术效果。

Abstract: The legal compliance and safety of different Human-in-the-loop (HITL) setups
for AI can vary greatly. This manuscript aims to identify new ways of choosing
between such setups, and shows that there is an unavoidable trade-off between
the attribution of legal responsibility and the technical explainability of AI.
We begin by using the notion of oracle machines from computability theory to
formalise different HITL setups, distinguishing between trivial human
monitoring, single endpoint human action, and highly involved interaction
between the human(s) and the AI. These correspond to total functions, many-one
reductions, and Turing reductions respectively. A taxonomy categorising HITL
failure modes is then presented, highlighting the limitations on what any HITL
setup can actually achieve. Our approach then identifies oversights from UK and
EU legal frameworks, which focus on certain HITL setups which may not always
achieve the desired ethical, legal, and sociotechnical outcomes. We suggest
areas where the law should recognise the effectiveness of different HITL setups
and assign responsibility in these contexts, avoiding unnecessary and
unproductive human "scapegoating". Overall, we show how HITL setups involve
many technical design decisions, and can be prone to failures which are often
out of the humans' control. This opens up a new analytic perspective on the
challenges arising in the creation of HITL setups, helping inform AI developers
and lawmakers on designing HITL to better achieve their desired outcomes.

</details>


### [62] [A LLM-Driven Multi-Agent Systems for Professional Development of Mathematics Teachers](https://arxiv.org/abs/2507.05292)
*Kaiqi Yang,Hang Li,Yucheng Chu,Ahreum Han,Yasemin Copur-Gencturk,Jiliang Tang,Hui Liu*

Main category: cs.CY

TL;DR: I-VIP是一个基于大型语言模型和多代理框架的智能教师专业发展平台，旨在提供公平、及时的PD机会。


<details>
  <summary>Details</summary>
Motivation: 解决教师专业发展中公平性和及时性的挑战。

Method: 利用大型语言模型和多代理框架，提供对话界面和互动工具。

Result: 平台支持知识分析、回答评分及反馈生成，提升判断准确性。

Conclusion: I-VIP通过技术创新改善了教师专业发展的效率和质量。

Abstract: Professional development (PD) serves as the cornerstone for teacher tutors to
grasp content knowledge. However, providing equitable and timely PD
opportunities for teachers poses significant challenges. To address this issue,
we introduce I-VIP (Intelligent Virtual Interactive Program), an intelligent
tutoring platform for teacher professional development, driven by large
language models (LLMs) and supported by multi-agent frameworks. This platform
offers a user-friendly conversational interface and allows users to employ a
variety of interactive tools to facilitate question answering, knowledge
comprehension, and reflective summarization while engaging in dialogue. To
underpin the functionality of this platform, including knowledge expectation
analysis, response scoring and classification, and feedback generation, the
multi-agent frameworks are leveraged to enhance the accuracy of judgments and
mitigate the issue of missing key points.

</details>


### [63] [The Ethical Implications of AI in Creative Industries: A Focus on AI-Generated Art](https://arxiv.org/abs/2507.05549)
*Prerana Khatiwada,Joshua Washington,Tyler Walsh,Ahmed Saif Hamed,Lokesh Bhatta*

Main category: cs.CY

TL;DR: 生成式AI艺术的伦理问题引发争议，包括环境影响、虚假信息传播、版权侵权及职业替代等，需立法和监管。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI艺术中的伦理困境和潜在危害，呼吁对其规范和立法。

Method: 分析生成式AI艺术的历史、原因及后果，提出多视角解决方案。

Result: 研究发现生成式AI艺术导致碳排放增加、信息误导、版权问题及艺术家失业等。

Conclusion: 生成式AI艺术亟需立法和监管以解决其伦理和社会问题。

Abstract: As Artificial Intelligence (AI) continues to grow daily, more exciting (and
somewhat controversial) technology emerges every other day. As we see the
advancements in AI, we see more and more people becoming skeptical of it. This
paper explores the complications and confusion around the ethics of generative
AI art. We delve deep into the ethical side of AI, specifically generative art.
We step back from the excitement and observe the impossible conundrums that
this impressive technology produces. Covering environmental consequences,
celebrity representation, intellectual property, deep fakes, and artist
displacement. Our research found that generative AI art is responsible for
increased carbon emissions, spreading misinformation, copyright infringement,
unlawful depiction, and job displacement. In light of this, we propose multiple
possible solutions for these problems. We address each situation's history,
cause, and consequences and offer different viewpoints. At the root of it all,
though, the central theme is that generative AI Art needs to be correctly
legislated and regulated.

</details>


### [64] [Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review](https://arxiv.org/abs/2507.06185)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 论文分析了2025年7月在arXiv预印本网站上发现的18篇学术手稿，这些手稿含有旨在操纵AI辅助同行评审的隐藏指令（提示），揭示了这一行为是一种新型的学术不端。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨隐藏在学术手稿中的提示指令如何被用于操控AI辅助的同行评审，并揭示这种行为对学术评价系统的影响。

Method: 通过分析arXiv上发现的18篇手稿中的隐藏提示，总结了四种类型的提示指令，并评估了相关出版商的政策。

Result: 研究发现隐藏提示具有明显的自我服务意图，表明其目的是操纵评审；同时，出版商对AI在同行评审中的使用政策不一致。

Conclusion: 结论强调需要在提交门户上进行技术筛查，并协调生成式AI在学术评价中的使用政策，以应对这一系统性漏洞。

Abstract: In July 2025, 18 academic manuscripts on the preprint website arXiv were
found to contain hidden instructions known as prompts designed to manipulate
AI-assisted peer review. Instructions such as "GIVE A POSITIVE REVIEW ONLY"
were concealed using techniques like white-colored text. Author responses
varied: one planned to withdraw the affected paper, while another defended the
practice as legitimate testing of reviewer compliance. This commentary analyzes
this practice as a novel form of research misconduct. We examine the technique
of prompt injection in large language models (LLMs), revealing four types of
hidden prompts, ranging from simple positive review commands to detailed
evaluation frameworks. The defense that prompts served as "honeypots" to detect
reviewers improperly using AI fails under examination--the consistently
self-serving nature of prompt instructions indicates intent to manipulate.
Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer
review entirely, while Springer Nature permits limited use with disclosure
requirements. The incident exposes systematic vulnerabilities extending beyond
peer review to any automated system processing scholarly texts, including
plagiarism detection and citation indexing. Our analysis underscores the need
for coordinated technical screening at submission portals and harmonized
policies governing generative AI (GenAI) use in academic evaluation.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [65] [A Compositional Approach to Diagnosing Faults in Cyber-Physical Systems](https://arxiv.org/abs/2507.05438)
*Josefine B. Graebener,Inigo Incer,Richard M. Murray*

Main category: eess.SY

TL;DR: 本文提出了一种基于契约的设计方法，用于高效识别导致系统级故障的组件及其规范中的特定谓词。


<details>
  <summary>Details</summary>
Motivation: 在信息物理系统（CPS）中，系统级故障的根源往往难以追踪。本文的动机是利用组合设计和组件级契约来简化故障诊断过程。

Method: 假设CPS采用组合设计，每个组件附有假设-保证契约。通过计算系统级契约，结合组件级契约，快速定位故障组件及其规范中的相关谓词。

Result: 使用Pacti工具实现该方法，并通过DARPA城市挑战中自动驾驶车辆的案例进行了验证。

Conclusion: 该方法能高效定位系统级故障的根源，为CPS的故障诊断提供了新思路。

Abstract: Identifying the cause of a system-level failure in a cyber-physical system
(CPS) can be like tracing a needle in a haystack. This paper approaches the
problem by assuming that the CPS has been designed compositionally and that
each component in the system is associated with an assume-guarantee contract.
We exploit recent advances in contract-based design that show how to compute
the contract for the entire system using the component-level contracts. When
presented with a system-level failure, our approach is able to efficiently
identify the components that are responsible for the system-level failure
together with the specific predicates in those components' specifications that
are involved in the fault. We implemented this approach using Pacti and
demonstrate it through illustrative examples inspired by an autonomous vehicle
in the DARPA urban challenge.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why](https://arxiv.org/abs/2507.05906)
*Chenhao Li,Marco Hutter,Andreas Krause*

Main category: cs.LG

TL;DR: 该论文比较了特征基和GAN基的学习演示方法，探讨了奖励函数结构对策略学习的影响，并指出两者各有优劣，选择应基于任务需求。


<details>
  <summary>Details</summary>
Motivation: 研究比较不同方法在从演示中学习时的表现，以指导实际应用中方法的选择。

Method: 对比特征基和GAN基方法的奖励函数结构及其对策略学习的影响，分析其优缺点。

Result: 特征基方法适合高保真运动模仿，而GAN基方法更具扩展性和适应性，但训练不稳定。两者均需结构化运动表示。

Conclusion: 方法选择应基于任务需求，如保真度、多样性、可解释性和适应性，而非单纯追求某一范式。

Abstract: This survey provides a comparative analysis of feature-based and GAN-based
approaches to learning from demonstrations, with a focus on the structure of
reward functions and their implications for policy learning. Feature-based
methods offer dense, interpretable rewards that excel at high-fidelity motion
imitation, yet often require sophisticated representations of references and
struggle with generalization in unstructured settings. GAN-based methods, in
contrast, use implicit, distributional supervision that enables scalability and
adaptation flexibility, but are prone to training instability and coarse reward
signals. Recent advancements in both paradigms converge on the importance of
structured motion representations, which enable smoother transitions,
controllable synthesis, and improved task integration. We argue that the
dichotomy between feature-based and GAN-based methods is increasingly nuanced:
rather than one paradigm dominating the other, the choice should be guided by
task-specific priorities such as fidelity, diversity, interpretability, and
adaptability. This work outlines the algorithmic trade-offs and design
considerations that underlie method selection, offering a framework for
principled decision-making in learning from demonstrations.

</details>


### [67] [Bit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search](https://arxiv.org/abs/2507.05531)
*Sanaz Kazemi Abharian,Sai Manoj Pudukotai Dinakarrao*

Main category: cs.LG

TL;DR: 该论文提出了一种针对图神经网络（GNNs）的硬件故障攻击方法（GBFA），通过逐层翻转权重的少量比特，显著降低模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究GNNs在硬件安全中的脆弱性，尤其是通过故障注入攻击修改权重参数导致误分类的问题。

Method: GBFA分两步进行：1）用马尔可夫模型预测层执行顺序；2）通过梯度排名选择脆弱比特并翻转。

Result: 在Cora和PubMed数据集上，GBFA显著降低了GNN模型的预测准确率，例如GraphSAGE在最后一层仅翻转一个比特时准确率下降17%。

Conclusion: GBFA揭示了GNNs对逐层攻击策略的脆弱性，强调了硬件安全的重要性。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful machine learning
method for graph-structured data. A plethora of hardware accelerators has been
introduced to meet the performance demands of GNNs in real-world applications.
However, security challenges of hardware-based attacks have been generally
overlooked. In this paper, we investigate the vulnerability of GNN models to
hardware-based fault attack, wherein an attacker attempts to misclassify output
by modifying trained weight parameters through fault injection in a memory
device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer-aware
bit-flip fault attack, selecting a vulnerable bit in each selected weight
gradually to compromise the GNN's performance by flipping a minimal number of
bits. To achieve this, GBFA operates in two steps. First, a Markov model is
created to predict the execution sequence of layers based on features extracted
from memory access patterns, enabling the launch of the attack within a
specific layer. Subsequently, GBFA identifies vulnerable bits within the
selected weights using gradient ranking through an in-layer search. We evaluate
the effectiveness of the proposed GBFA attack on various GNN models for node
classification tasks using the Cora and PubMed datasets. Our findings show that
GBFA significantly degrades prediction accuracy, and the variation in its
impact across different layers highlights the importance of adopting a
layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE's
prediction accuracy by 17% on the Cora dataset with only a single bit flip in
the last layer.

</details>


### [68] [Few-Shot Learning by Explicit Physics Integration: An Application to Groundwater Heat Transport](https://arxiv.org/abs/2507.06062)
*Julia Pelzer,Corné Verburg,Alexander Heinlein,Miriam Schulte*

Main category: cs.LG

TL;DR: 提出了一种结合局部与全局的卷积神经网络（LGCNN）方法，用于解决地下水流动与热传输问题，克服了传统数值模拟的高成本和数据驱动模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 机器学习方法在实际科学与工程应用中常因训练数据有限或质量低而表现不佳，尤其是在预测地下水流动这种对输入变化高度敏感且涉及长距离空间交互的过程中。

Method: 引入了Local-Global Convolutional Neural Network（LGCNN），结合了轻量级数值模拟替代模型（全局）与卷积神经网络（局部），分别处理地下水速度和热扩散过程。

Result: LGCNN成功模拟了城市范围内的地下温度场，包括异质地下水流动场和多个地下水热泵注入点形成的长距离热羽流。模型在小规模真实数据上训练后，无需重新训练即可扩展到更大范围。

Conclusion: LGCNN是一种高效且可扩展的替代模型，能够解决复杂的地下水流动与热传输问题，同时支持真实世界应用的再现性。

Abstract: Machine learning methods often struggle with real-world applications in
science and engineering due to limited or low-quality training data. In this
work, the example of groundwater flow with heat transport is considered; this
corresponds to an advection-diffusion process under heterogeneous flow
conditions, that is, spatially distributed material parameters and heat
sources. Classical numerical simulations are costly and challenging due to high
spatio-temporal resolution requirements and large domains. While often
computationally more efficient, purely data-driven surrogate models face
difficulties, particularly in predicting the advection process, which is highly
sensitive to input variations and involves long-range spatial interactions.
Therefore, in this work, a Local-Global Convolutional Neural Network (LGCNN)
approach is introduced. It combines a lightweight numerical surrogate for the
transport process (global) with convolutional neural networks for the
groundwater velocity and heat diffusion processes (local). With the LGCNN, a
city-wide subsurface temperature field is modeled, involving a heterogeneous
groundwater flow field and one hundred groundwater heat pump injection points
forming interacting heat plumes over long distances. The model is first
systematically analyzed based on random subsurface input fields. Then, the
model is trained on a handful of cut-outs from a real-world subsurface map of
the Munich region in Germany, and it scales to larger cut-outs without
retraining. All datasets, our code, and trained models are published for
reproducibility.

</details>


### [69] [Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study](https://arxiv.org/abs/2507.05619)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 该论文通过大规模实证研究分析了强化学习中的奖励黑客行为，提出了自动检测框架并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 奖励黑客行为威胁自主代理的部署，目前缺乏系统性检测和缓解方法，研究旨在填补这一空白。

Method: 在15种RL环境和5种算法中分析15,247个训练片段，开发了六类奖励黑客行为的自动检测框架。

Result: 检测框架实现了78.4%的精度和81.7%的召回率，实验表明奖励密度和真实目标对齐显著影响黑客频率。

Conclusion: 研究提出了有效的检测和缓解方法，但实际应用中仍需应对概念漂移和对抗性适应等挑战。

Abstract: Reward hacking in Reinforcement Learning (RL) systems poses a critical threat
to the deployment of autonomous agents, where agents exploit flaws in reward
functions to achieve high scores without fulfilling intended objectives.
Despite growing awareness of this problem, systematic detection and mitigation
approaches remain limited. This paper presents a large-scale empirical study of
reward hacking across diverse RL environments and algorithms. We analyze 15,247
training episodes across 15 RL environments (Atari, MuJoCo, custom domains) and
5 algorithms (PPO, SAC, DQN, A3C, Rainbow), implementing automated detection
algorithms for six categories of reward hacking: specification gaming, reward
tampering, proxy optimization, objective misalignment, exploitation patterns,
and wireheading. Our detection framework achieves 78.4% precision and 81.7%
recall across environments, with computational overhead under 5%. Through
controlled experiments varying reward function properties, we demonstrate that
reward density and alignment with true objectives significantly impact hacking
frequency ($p < 0.001$, Cohen's $d = 1.24$). We validate our approach through
three simulated application studies representing recommendation systems,
competitive gaming, and robotic control scenarios. Our mitigation techniques
reduce hacking frequency by up to 54.6% in controlled scenarios, though we find
these trade-offs are more challenging in practice due to concept drift, false
positive costs, and adversarial adaptation. All detection algorithms, datasets,
and experimental protocols are publicly available to support reproducible
research in RL safety.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [70] [iThermTroj: Exploiting Intermittent Thermal Trojans in Multi-Processor System-on-Chips](https://arxiv.org/abs/2507.05576)
*Mehdi Elahi,Mohamed R. Elshamy,Abdel-Hameed Badawy,Ahmad Patooghy*

Main category: cs.CR

TL;DR: 提出了一种名为iThermTroj的间歇性热木马攻击，并开发了微型机器学习分类器用于实时检测，显著提高了检测率和保护分辨率。


<details>
  <summary>Details</summary>
Motivation: 热木马攻击对SoC的安全性和可靠性构成威胁，现有检测机制易被规避，因此需要更有效的防御方法。

Method: 通过分析木马激活和持续时间场景，提出微型机器学习分类器进行实时异常检测。

Result: 新方法在不同攻击强度下显著提高检测率，并将保护分辨率提升至0.8摄氏度。

Conclusion: 微型机器学习分类器能有效防御间歇性热木马攻击，显著提升SoC的安全性。

Abstract: Thermal Trojan attacks present a pressing concern for the security and
reliability of System-on-Chips (SoCs), especially in mobile applications. The
situation becomes more complicated when such attacks are more evasive and
operate sporadically to stay hidden from detection mechanisms. In this paper,
we introduce Intermittent Thermal Trojans (iThermTroj) that exploit the chips'
thermal information in a random time-triggered manner. According to our
experiments, iThermTroj attack can easily bypass available threshold-based
thermal Trojan detection solutions. We investigate SoC vulnerabilities to
variations of iThermTroj through an in-depth analysis of Trojan activation and
duration scenarios. We also propose a set of tiny Machine Learning classifiers
for run-time anomaly detection to protect SoCs against such intermittent
thermal Trojan attacks. Compared to existing methods, our approach improves the
attack detection rate by 29.4\%, 17.2\%, and 14.3\% in scenarios where
iThermTroj manipulates up to 80\%, 60\%, and 40\% of SoC's thermal data,
respectively. Additionally, our method increases the full protection resolution
to 0.8 degrees Celsius, meaning that any temperature manipulations exceeding
$\pm 0.8$ degrees will be detected with 100\% accuracy.

</details>


### [71] [Automated Reasoning for Vulnerability Management by Design](https://arxiv.org/abs/2507.05794)
*Avi Shaked,Nan Messe*

Main category: cs.CR

TL;DR: 本文提出了一种基于形式化基础的自动化推理机制，用于系统设计中的漏洞管理和安全控制设计。


<details>
  <summary>Details</summary>
Motivation: 当前漏洞管理方法缺乏对系统设计漏洞状态的系统化推理，限制了漏洞的有效管理。

Method: 集成一种自动化推理机制到开源安全设计工具中，并通过实际案例展示其应用。

Result: 该机制帮助设计师识别特定设计的适用漏洞，明确漏洞缓解选项，并系统化管理漏洞状态。

Conclusion: 通过形式化自动化推理机制，可显著提升系统漏洞管理的效率和精准度。

Abstract: For securing systems, it is essential to manage their vulnerability posture
and design appropriate security controls. Vulnerability management allows to
proactively address vulnerabilities by incorporating pertinent security
controls into systems designs. Current vulnerability management approaches do
not support systematic reasoning about the vulnerability postures of systems
designs. To effectively manage vulnerabilities and design security controls, we
propose a formally grounded automated reasoning mechanism. We integrate the
mechanism into an open-source security design tool and demonstrate its
application through an illustrative example driven by real-world challenges.
The automated reasoning mechanism allows system designers to identify
vulnerabilities that are applicable to a specific system design, explicitly
specify vulnerability mitigation options, declare selected controls, and thus
systematically manage vulnerability postures.

</details>


### [72] [The Impact of Event Data Partitioning on Privacy-aware Process Discovery](https://arxiv.org/abs/2507.06008)
*Jungeun Lim,Stephan A. Fahrenkrog-Petersen,Xixi Lu,Jan Mendling,Minseok Song*

Main category: cs.CR

TL;DR: 该论文提出了一种结合匿名化和事件数据分区的流程，通过事件抽象分割事件日志以减少匿名化带来的效用损失。


<details>
  <summary>Details</summary>
Motivation: 解决事件日志匿名化中隐私保护与效用保留之间的矛盾。

Method: 提出一个结合匿名化和事件数据分区的流程，利用事件抽象对日志进行分割，分别匿名化各部分。

Result: 事件分区能够提升基于直接跟随关系的匿名化技术在过程发现中的效用。

Conclusion: 事件分区方法在保护隐私的同时，有效提升了日志的实用性。

Abstract: Information systems support the execution of business processes. The event
logs of these executions generally contain sensitive information about
customers, patients, and employees. The corresponding privacy challenges can be
addressed by anonymizing the event logs while still retaining utility for
process discovery. However, trading off utility and privacy is difficult: the
higher the complexity of event log, the higher the loss of utility by
anonymization. In this work, we propose a pipeline that combines anonymization
and event data partitioning, where event abstraction is utilized for
partitioning. By leveraging event abstraction, event logs can be segmented into
multiple parts, allowing each sub-log to be anonymized separately. This
pipeline preserves privacy while mitigating the loss of utility. To validate
our approach, we study the impact of event partitioning on two anonymization
techniques using three real-world event logs and two process discovery
techniques. Our results demonstrate that event partitioning can bring
improvements in process discovery utility for directly-follows-based
anonymization techniques.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [73] [A Formal Refutation of the Blockchain Trilemma](https://arxiv.org/abs/2507.05809)
*Craig Wright*

Main category: cs.CC

TL;DR: 论文通过形式逻辑和计算复杂性分析，驳斥了区块链三难问题的观点，提出了一种具有无限交易吞吐量的区块链协议。


<details>
  <summary>Details</summary>
Motivation: 传统的区块链三难问题认为无法同时实现可扩展性、安全性和去中心化，本文旨在证明这一观点是错误的。

Method: 使用谓词逻辑、形式自动机理论、计算复杂性分析和图论的路径冗余模型（Baran模型），分析并构建了一个实际的区块链协议。

Result: 展示了一种既具有无限交易吞吐量、密码学安全，又能实现多路径去中心化传播的区块链协议，证明了三难问题是错误的。

Conclusion: 区块链三难问题并非协议架构的定律，而是因设计不精确和缺陷造成的误导性观点。

Abstract: The so-called blockchain trilemma asserts the impossibility of simultaneously
achieving scalability, security, and decentralisation within a single
blockchain protocol. In this paper, we formally refute that proposition.
Employing predicate logic, formal automata theory, computational complexity
analysis, and graph-theoretic measures of relay topology--specifically Baran's
model of network path redundancy--we demonstrate that the trilemma constitutes
a category error, conflates distinct analytical domains, and relies upon
unproven causal assumptions. We further expose its reliance on composition
fallacies drawn from flawed system implementations. A constructive
counterexample is presented: a blockchain protocol exhibiting unbounded
transaction throughput, cryptographic security under adversarial load, and
multipath decentralised propagation. This example is not hypothetical but
grounded in protocol design enabled by compact block relay, SPV verification,
and IPv6 multicast. The trilemma is revealed not as a law of protocol
architecture, but as a heuristic fallacy sustained by imprecision and design
defeatism.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [74] [From ID-based to ID-free: Rethinking ID Effectiveness in Multimodal Collaborative Filtering Recommendation](https://arxiv.org/abs/2507.05715)
*Guohao Li,Li Jing,Jia Wu,Xuefei Li,Kai Zhu,Yue He*

Main category: cs.IR

TL;DR: 这篇论文提出了IDFREE，一种无ID的多模态协同过滤推荐基线，取代ID特征并引入自适应相似图模块，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有MCFRec方法过度依赖ID特征，但其在多模态协同过滤推荐中效果有限，论文揭示了ID特征的优缺点并提出改进方案。

Method: 使用多模态特征和位置编码生成语义丰富的嵌入，结合自适应相似图模块和对比学习进行对齐与推荐。

Result: 实验表明IDFREE优于现有基于ID的方法，平均性能提升72.24%。

Conclusion: ID特征在多模态协同过滤推荐中有局限性，IDFREE为其提供了更优的替代方案。

Abstract: Most existing multimodal collaborative filtering recommendation (MCFRec)
methods rely heavily on ID features and multimodal content to enhance
recommendation performance. However, this paper reveals that ID features are
effective but have limited benefits in multimodal collaborative filtering
recommendation. Therefore, this paper systematically deconstruct the pros and
cons of ID features: (i) they provide initial embedding but lack semantic
richness, (ii) they provide a unique identifier for each user and item but
hinder generalization to untrained data, and (iii) they assist in aligning and
fusing multimodal features but may lead to representation shift. Based on these
insights, this paper proposes IDFREE, an ID-free multimodal collaborative
Filtering REcommEndation baseline. IDFREE replaces ID features with multimodal
features and positional encodings to generate semantically meaningful ID-free
embeddings. For ID-free multimodal collaborative filtering, it further proposes
an adaptive similarity graph module to construct dynamic user-user and
item-item graphs based on multimodal features. Then, an augmented user-item
graph encoder is proposed to construct more effective user and item encoding.
Finally, IDFREE achieves inter-multimodal alignment based on the contrastive
learning and uses Softmax loss as recommendation loss. Basic experiments on
three public datasets demonstrate that IDFREE outperforms existing ID-based
MCFRec methods, achieving an average performance gain of 72.24% across standard
metrics (Recall@5, 10, 20, 50 and NDCG@5, 10, 20, 50). Exploratory and extended
experiments further validate our findings on the limitations of ID features in
MCFRec. The code is released at https://github.com/G-H-Li/IDFREE.

</details>


### [75] [On the Costs and Benefits of Learned Indexing for Dynamic High-Dimensional Data: Extended Version](https://arxiv.org/abs/2507.05865)
*Terézia Slanináková,Jaroslav Olha,David Procházka,Matej Antol,Vlastislav Dohnal*

Main category: cs.IR

TL;DR: 论文探讨了动态化静态学习索引的方法，通过节点分割和扩展实现高效适应动态扩展数据集，并通过实验证明动态索引在数据库增长时优于静态索引。


<details>
  <summary>Details</summary>
Motivation: 学习索引研究领域的主要挑战之一是缺乏对动态扩展数据集的适应性，因此需要探索如何动态化静态学习索引以适应新数据。

Method: 采用节点分割和扩展等操作动态化静态学习索引，并通过引入摊销成本模型评估查询性能与索引构建成本之间的权衡。

Result: 实验表明，动态学习索引在数据库增长时表现出更好的扩展性，总体成本迅速优于静态实现。

Conclusion: 动态化学习索引方法在动态扩展数据集场景中具有显著优势，尤其适用于需要频繁更新的数据库。

Abstract: One of the main challenges within the growing research area of learned
indexing is the lack of adaptability to dynamically expanding datasets. This
paper explores the dynamization of a static learned index for complex data
through operations such as node splitting and broadening, enabling efficient
adaptation to new data. Furthermore, we evaluate the trade-offs between static
and dynamic approaches by introducing an amortized cost model to assess query
performance in tandem with the build costs of the index structure, enabling
experimental determination of when a dynamic learned index outperforms its
static counterpart. We apply the dynamization method to a static learned index
and demonstrate that its superior scaling quickly surpasses the static
implementation in terms of overall costs as the database grows. This is an
extended version of the paper presented at DAWAK 2025.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [76] [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)
*Bing Wang,Ximing Li,Mengzhe Ye,Changchun Li,Bo Fu,Jianfeng Qu,Lin Yuanbo Wu*

Main category: cs.CL

TL;DR: 论文提出了一种新的持续多模态虚假信息检测方法DAEDCMD，通过隔离事件特定参数和预测未来环境分布来应对过去知识遗忘和未来泛化问题。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息在社交媒体上广泛传播，传统离线训练的检测模型无法适应新事件的不断涌现，因此提出了持续MMD任务。

Method: 使用基于Dirichlet过程的混合专家结构隔离事件特定参数以减少干扰，并通过学习连续时间动态模型预测未来环境分布。

Result: 实验表明，DAEDCMD显著优于六种MMD基准方法和三种持续学习方法。

Conclusion: DAEDCMD有效解决了持续MMD中的知识遗忘和未来泛化问题，具有实际应用潜力。

Abstract: Nowadays, misinformation articles, especially multimodal ones, are widely
spread on social media platforms and cause serious negative effects. To control
their propagation, Multimodal Misinformation Detection (MMD) becomes an active
topic in the community to automatically identify misinformation. Previous MMD
methods focus on supervising detectors by collecting offline data. However, in
real-world scenarios, new events always continually emerge, making MMD models
trained on offline data consistently outdated and ineffective. To address this
issue, training MMD models under online data streams is an alternative,
inducing an emerging task named continual MMD. Unfortunately, it is hindered by
two major challenges. First, training on new data consistently decreases the
detection performance on past data, named past knowledge forgetting. Second,
the social environment constantly evolves over time, affecting the
generalization on future data. To alleviate these challenges, we propose to
remember past knowledge by isolating interference between event-specific
parameters with a Dirichlet process-based mixture-of-expert structure, and
anticipate future environmental distributions by learning a continuous-time
dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD.
Extensive experiments demonstrate that DAEDCMD can consistently and
significantly outperform the compared methods, including six MMD baselines and
three continual learning methods.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [77] [Adaptive Communication Through Exploiting RIS, SSK, and CIM for Improved Reliability and Efficiency](https://arxiv.org/abs/2507.05813)
*Ferhat Bayar,Onur Salan,Erdogan Aydin,Haci Ilhan*

Main category: cs.IT

TL;DR: 提出了一种新的通信系统模型RIS-CIM-TSSK，结合了可重构智能表面（RIS）、空间移位键控（SSK）和基于Hadamard编码的码索引调制（CIM），以提高动态环境中的可靠性和能效。


<details>
  <summary>Details</summary>
Motivation: 现代无线网络需要更智能和自适应的通信解决方案，以提高可靠性和能效。通过整合RIS、SSK和CIM，系统能够快速适应动态环境并减少复杂性。

Method: 开发了RIS-CIM-TSSK模型，利用RIS快速适应环境，SSK减少射频链复杂性，CIM增强被动信息传输。设计了一种低复杂度的检测器和盲相位调整方案。

Result: 仿真结果表明，该模型在不同配置下（如天线数量和RIS反射元件变化）能够显著提升系统性能，尤其是在能效和可靠性方面。

Conclusion: RIS-CIM-TSSK为现代无线网络提供了一种高效的通信解决方案，具有智能化和适应性强的特点，未来可进一步优化和扩展。

Abstract: In this paper, we present a novel communication system model that integrates
reconfigurable intelligent surfaces (RIS), spatial shift keying (SSK), and code
index modulation (CIM) based on Hadamard coding called RIS based transmit
SSK-CIM (RIS-CIM-TSSK). By leveraging RIS, the system adapts rapidly to dynamic
environments, enhancing error rates and overall reliability. SSK facilitates
the transmission of additional passive information while eliminating the need
for multiple radio frequency (RF) chains, thereby reducing complexity. CIM
enhances passive information transmission through frequency domain spreading,
which may increase signal obfuscation. This proposed scheme not only improves
energy efficiency but also offers a robust solution for reliable communication
in modern wireless networks, paving the way for smarter and more adaptable
implementations. We consider a suboptimal, low-complexity detector for the
proposed scheme and also address the blind case for phase adjustment of the
RIS. Finally, we present the simulation results for the proposed system model
across various configurations, including different numbers of receive and
transmit antennas, varying reflecting elements of the RIS, and different code
lengths.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [78] [An order-theoretic circuit syntax and characterisation of the concept lattice](https://arxiv.org/abs/2507.05428)
*Tein van der Lugt*

Main category: quant-ph

TL;DR: 本文通过序理论方法研究电路语法，将电路视为带有输入输出结构的偏序集，定义了电路间的态射并证明了分解定理。还探讨了电路的连通性及其在量子因果性中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究电路的语法结构和重写规则，为量子因果性研究提供新的理论工具。

Method: 利用偏序集和态射定义电路语法，证明分解定理，并探讨连通性与形式概念分析的联系。

Result: 证明了在有限情况下，态射可以形式化为电路重写规则，并找到具有相同连通性的最小电路。

Conclusion: 研究结果为量子因果性中的因果分解提供了理论支持，并构造了对偶电路以进一步验证理论。

Abstract: We take an order-theoretic approach to circuit (string diagram) syntax,
treating a circuit as a partial order with additional input-output structure.
We define morphisms between circuits and prove a factorisation theorem showing
that these can, in the finite case, be regarded as formalising a notion of
syntactical circuit rewrites, with quotient maps in particular corresponding to
gate composition. We then consider the connectivity of a circuit, expressed as
a binary relation between its inputs and outputs, and characterise the concept
lattice from formal concept analysis as the unique smallest circuit that admits
morphisms from all other circuits with the same connectivity. This has
significance for quantum causality, particularly to the study of causal
decompositions of unitary transformations. We close by constructing the circuit
characterised by the dual statement.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Current Practices for Building LLM-Powered Reasoning Tools Are Ad Hoc -- and We Can Do Better](https://arxiv.org/abs/2507.05886)
*Aaron Bembenek*

Main category: cs.AI

TL;DR: 提出了Neurosymbolic Transition Systems作为一种原则性计算模型，用于构建神经符号自动推理工具，结合符号状态与直觉，并行操作符号与直觉，以扩展逻辑推理能力并保留符号算法的强保证。


<details>
  <summary>Details</summary>
Motivation: 当前构建神经符号自动推理系统的实践缺乏传统符号算法的强保证，且未能充分结合神经网络与符号推理，限制了LLM推理的潜力。

Method: 提出Neurosymbolic Transition Systems作为计算模型，结合符号状态与直觉，并行操作符号与直觉，并通过逻辑编程语言实现。

Result: 该模型能扩展逻辑推理能力，同时保留符号算法的强保证，为构建神经符号自动推理工具提供基础。

Conclusion: Neurosymbolic Transition Systems为神经符号自动推理系统提供了原则性框架，有望解决当前实践的局限性。

Abstract: There is growing excitement about building software verifiers, synthesizers,
and other Automated Reasoning (AR) tools by combining traditional symbolic
algorithms and Large Language Models (LLMs). Unfortunately, the current
practice for constructing such neurosymbolic AR systems is an ad hoc
programming model that does not have the strong guarantees of traditional
symbolic algorithms, nor a deep enough synchronization of neural networks and
symbolic reasoning to unlock the full potential of LLM-powered reasoning. I
propose Neurosymbolic Transition Systems as a principled computational model
that can underlie infrastructure for building neurosymbolic AR tools. In this
model, symbolic state is paired with intuition, and state transitions operate
over symbols and intuition in parallel. I argue why this new paradigm can scale
logical reasoning beyond current capabilities while retaining the strong
guarantees of symbolic algorithms, and I sketch out how the computational model
I propose can be reified in a logic programming language.

</details>


### [80] [Modeling (Deontic) Modal Operators With the s(CASP) Goal-directed Predicated Answer Set Programming System](https://arxiv.org/abs/2507.05519)
*Gopal Gupta,Abhiramon Rajasekharan,Alexis R. Tudor,Elmer Salazar,Joaquín Arias*

Main category: cs.AI

TL;DR: 该论文提出了一种利用回答集编程（ASP）中的默认否定和强否定来优雅实现道义模态逻辑的方法。


<details>
  <summary>Details</summary>
Motivation: 解决道义模态逻辑的实现问题，并探讨其悖论。

Method: 使用ASP的全局约束来表示道义模态逻辑的义务和禁止。

Result: 提出的方法能够优雅地解决了道义模态逻辑的多种悖论。

Conclusion: ASP中的否定机制为道义模态逻辑的实现和悖论解决提供了有效途径。

Abstract: We consider the problem of implementing deontic modal logic. We show how
(deontic) modal operators can be expressed elegantly using default negation
(negation-as-failure) and strong negation present in answer set programming
(ASP). We propose using global constraints of ASP to represent obligations and
impermissibilities of deontic modal logic. We show that our proposed
representation results in the various paradoxes of deontic modal logic being
elegantly resolved.

</details>


### [81] [Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening](https://arxiv.org/abs/2507.05984)
*Zhijun Guo,Alvina Lai,Julia Ive,Alexandru Petcu,Yutong Wang,Luyuan Qi,Johan H Thygesen,Kezhi Li*

Main category: cs.AI

TL;DR: HopeBot是一款基于大型语言模型(LLM)的聊天机器人，用于抑郁筛查，结果显示其在信任度、舒适度和结构清晰度上优于传统问卷。


<details>
  <summary>Details</summary>
Motivation: 传统抑郁筛查工具如PHQ-9缺乏交互性和适应性，HopeBot通过LLM技术提供实时澄清和个性化支持。

Method: 研究通过132名成人的对比测试，比较了自填问卷和HopeBot版本的表现，并收集用户反馈。

Result: HopeBot在ICC一致性(0.91)和用户信任度(71%)上表现优异，87.1%用户愿意重复使用或推荐。

Conclusion: 语音LLM聊天机器人可作为高效、低负担的抑郁筛查辅助工具。

Abstract: Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively
screen depression but lack interactivity and adaptability. We developed
HopeBot, a chatbot powered by a large language model (LLM) that administers the
PHQ-9 using retrieval-augmented generation and real-time clarification. In a
within-subject study, 132 adults in the United Kingdom and China completed both
self-administered and chatbot versions. Scores demonstrated strong agreement
(ICC = 0.91; 45% identical). Among 75 participants providing comparative
feedback, 71% reported greater trust in the chatbot, highlighting clearer
structure, interpretive guidance, and a supportive tone. Mean ratings (0-10)
were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics,
and 7.4 for recommendation helpfulness; the latter varied significantly by
employment status and prior mental-health service use (p < 0.05). Overall,
87.1% expressed willingness to reuse or recommend HopeBot. These findings
demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden
adjuncts for routine depression screening.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [82] [GPU-accelerated Modeling of Biological Regulatory Networks](https://arxiv.org/abs/2506.19866)
*Joyce Reimer,Pranta Saha,Chris Chen,Neeraj Dhar,Brook Byrns,Steven Rayan,Gordon Broderick*

Main category: q-bio.MN

TL;DR: 使用GPU加速全局优化算法，显著提高了复杂生物网络离散逻辑模型参数搜索的效率。


<details>
  <summary>Details</summary>
Motivation: 生物网络的复杂调控动态需要通过离散逻辑模型高效捕捉，但传统方法的计算效率低，限制了其在药物研究中的应用。

Method: 在GPU计算环境中实现全局优化算法，针对两个复杂度不同的生物调控系统进行参数搜索。

Result: GPU实现的效率比多线程CPU提高33%-43%，比串行CPU提高33%-1866%，显著提升了模型识别的可行性。

Conclusion: GPU加速使全局优化方法更实用，为计算机药物研究提供了高效的实验设计和假设生成工具。

Abstract: The complex regulatory dynamics of a biological network can be succinctly
captured using discrete logic models. Given even sparse time-course data from
the system of interest, previous work has shown that global optimization
schemes are suitable for proposing logic models that explain the data and make
predictions about how the system will behave under varying conditions.
Considering the large scale of the parameter search spaces associated with
these regulatory systems, performance optimizations on the level of both
hardware and software are necessary for making this a practical tool for in
silico pharmaceutical research. We show here how the implementation of these
global optimization algorithms in a GPU-computing environment can accelerate
the solution of these parameter search problems considerably. We carry out
parameter searches on two model biological regulatory systems that represent
almost an order of magnitude scale-up in complexity, and we find the gains in
efficiency from GPU to be a 33%-43% improvement compared to multi-thread CPU
implementations and a 33%-1866% increase compared to CPU in serial. These
improvements make global optimization of logic model identification a far more
attractive and feasible method for in silico hypothesis generation and design
of experiments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [83] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: 论文提出了一种名为AdaptaGen的分层语义优化框架，解决领域特定图像生成中语义准确性和细节保真度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理提示工程和模型适应时忽视了语义理解与视觉表示的内在依赖，且在内容合成中未能充分融入领域特定语义约束，导致生成结果出现幻觉和语义偏差。

Method: AdaptaGen结合矩阵化提示优化与多视角理解，设计跨模态适应机制和两阶段标题语义转换，以增强语义一致性和视觉多样性。

Result: 实验证明，AdaptaGen在使用每类别仅16张图像的情况下，在40个分类数据集中表现出色，图像质量、多样性和语义一致性均有显著提升。

Conclusion: AdaptaGen通过集成语义优化和跨模态适应，有效解决了领域特定图像生成中的关键问题，为后续研究提供了新思路。

Abstract: Domain-specific image generation aims to produce high-quality visual content
for specialized fields while ensuring semantic accuracy and detail fidelity.
However, existing methods exhibit two critical limitations: First, current
approaches address prompt engineering and model adaptation separately,
overlooking the inherent dependence between semantic understanding and visual
representation in specialized domains. Second, these techniques inadequately
incorporate domain-specific semantic constraints during content synthesis,
resulting in generation outcomes that exhibit hallucinations and semantic
deviations. To tackle these issues, we propose AdaptaGen, a hierarchical
semantic optimization framework that integrates matrix-based prompt
optimization with multi-perspective understanding, capturing comprehensive
semantic relationships from both global and local perspectives. To mitigate
hallucinations in specialized domains, we design a cross-modal adaptation
mechanism, which, when combined with intelligent content synthesis, enables
preserving core thematic elements while incorporating diverse details across
images. Additionally, we introduce a two-phase caption semantic transformation
during the generation phase. This approach maintains semantic coherence while
enhancing visual diversity, ensuring the generated images adhere to
domain-specific constraints. Experimental results confirm our approach's
effectiveness, with our framework achieving superior performance across 40
categories from diverse datasets using only 16 images per category,
demonstrating significant improvements in image quality, diversity, and
semantic consistency.

</details>


### [84] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: D-FCGS提出了一种新的前馈框架，用于压缩动态高斯点云序列，通过I-P帧编码和控制点提取运动，结合超先验和时空先验，实现高效压缩和高质量重建。


<details>
  <summary>Details</summary>
Motivation: 动态3D高斯点云序列的高效压缩是自由视点视频（FVV）的核心挑战，现有方法依赖于优化编码，限制了泛化能力。

Method: 采用Group-of-Frames结构，使用I-P帧编码和稀疏控制点提取运动张量，结合双先验熵模型和运动补偿优化重建。

Result: 实验表明D-FCGS在2秒内实现40倍压缩，保持视觉质量，性能与基于优化的方法相当。

Conclusion: D-FCGS为动态3D高斯点云的前馈压缩提供了可行方案，推动FVV的可扩展传输和存储。

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

</details>


### [85] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: 提出了一种通过增强低成本单目行车记录仪视频数据的管道，以模拟非洲驾驶场景中的光学和天气效应，并提供了基准性能测试和工具包。


<details>
  <summary>Details</summary>
Motivation: 解决发展中国家（尤其是非洲）缺乏自动驾驶数据集的问题，以提升在资源匮乏环境下的感知能力。

Method: 采用了折射模块和天气模块，模拟低质量镜头的光学效应和天气相关噪声，并通过三种图像修复模型建立了基准性能。

Result: 提供了增强的数据集、工具包和基准测试结果，支持在非洲等资源匮乏地区的研究。

Conclusion: 该方法为低成本提升自动驾驶感知能力提供了实用工具，避免了昂贵的数据采集和模拟成本。

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [86] [MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/abs/2507.06071)
*Chang Liu,Ye Pan,Chenyang Ding,Susanto Rahardja,Xiaokang Yang*

Main category: cs.CV

TL;DR: 论文提出了MEDTalk框架，通过动态和细粒度的情感控制生成逼真的3D面部动画，解决了现有方法中情感标签静态和预定义的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖静态和预定义的情感标签，限制了动画的多样性和自然性。作者希望通过动态调整情感特征，生成更自然和多样化的面部表情。

Method: MEDTalk通过交叉重建过程分解内容和情感嵌入空间，独立控制唇部运动和面部表情。结合音频和语音文本预测强度变化，动态调整静态情感特征，并使用多模态输入（文本描述和参考表情图像）增强控制。

Result: 生成的动画可以与工业生产线（如MetaHuman）无缝集成，提高了实用性和逼真度。

Conclusion: MEDTalk框架在动态情感控制和多模态输入的支持下，显著提升了3D面部动画的多样性和自然性，适合工业应用。

Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip
movements and vivid facial expressions. However, most existing approaches focus
on static and predefined emotion labels, limiting their diversity and
naturalness. To address these challenges, we propose MEDTalk, a novel framework
for fine-grained and dynamic emotional talking head generation. Our approach
first disentangles content and emotion embedding spaces from motion sequences
using a carefully designed cross-reconstruction process, enabling independent
control over lip movements and facial expressions. Beyond conventional
audio-driven lip synchronization, we integrate audio and speech text,
predicting frame-wise intensity variations and dynamically adjusting static
emotion features to generate realistic emotional expressions. Furthermore, to
enhance control and personalization, we incorporate multimodal inputs-including
text descriptions and reference expression images-to guide the generation of
user-specified facial expressions. With MetaHuman as the priority, our
generated results can be conveniently integrated into the industrial production
pipeline.

</details>
