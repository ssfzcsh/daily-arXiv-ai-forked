{"id": "2508.16671", "pdf": "https://arxiv.org/pdf/2508.16671", "abs": "https://arxiv.org/abs/2508.16671", "authors": ["Mingyang Zhou", "Quanming Yao", "Lun Du", "Lanning Wei", "Da Zheng"], "title": "Reflective Paper-to-Code Reproduction Enabled by Fine-Grained Verification", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Reproducing machine learning papers is essential for scientific progress but\nremains challenging for both humans and automated agents. Existing agent-based\nmethods often struggle to fully and accurately reproduce implementation details\nsuch as mathematical formulas and algorithmic logic. Previous studies show that\nreflection with explicit feedback improves agent performance. However, current\npaper reproduction methods fail to effectively adopt this strategy. This gap\nmainly arises from the diverse paper patterns, complex method modules, and\nvaried configurations encountered in research papers. Motivated by how humans\nuse systematic checklists to efficiently debug complex code, we propose\n\\textbf{RePro}, a \\textbf{Re}flective Paper-to-Code \\textbf{Repro}duction\nframework that automatically extracts a paper's fingerprint, referring to a\ncomprehensive set of accurate and atomic criteria serving as high-quality\nsupervisory signals. The framework first generates code based on the extracted\ninformation, and then leverages the fingerprint within iterative verification\nand refinement loop. This approach systematically detects discrepancies and\nproduces targeted revisions to align generated code with the paper's\nimplementation details. Extensive experiments on the PaperBench Code-Dev\nbenchmark have been conducted, RePro achieves 13.0\\% performance gap over\nbaselines, and it correctly revises complex logical and mathematical criteria\nin reflecting, on which the effectiveness is obvious.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRePro\u7684\u53cd\u601d\u6027\u8bba\u6587\u8f6c\u4ee3\u7801\u590d\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u8bba\u6587\u6307\u7eb9\u5e76\u8fed\u4ee3\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u73b0\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u51c6\u786e\u590d\u73b0\u8bba\u6587\u4e2d\u7684\u6570\u5b66\u516c\u5f0f\u548c\u7b97\u6cd5\u903b\u8f91\uff0c\u800c\u53cd\u601d\u548c\u53cd\u9988\u7b56\u7565\u672a\u88ab\u6709\u6548\u91c7\u7528\u3002\u4eba\u7c7b\u5229\u7528\u7cfb\u7edf\u5316\u68c0\u67e5\u8868\u8c03\u8bd5\u590d\u6742\u4ee3\u7801\u7684\u542f\u53d1\u4e0b\uff0c\u8bbe\u8ba1\u4e86RePro\u6846\u67b6\u3002", "method": "RePro\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u8bba\u6587\u6307\u7eb9\uff08\u51c6\u786e\u7684\u539f\u5b50\u6807\u51c6\uff09\uff0c\u751f\u6210\u4ee3\u7801\u5e76\u5728\u8fed\u4ee3\u9a8c\u8bc1\u548c\u4f18\u5316\u5faa\u73af\u4e2d\u5229\u7528\u6307\u7eb9\u68c0\u6d4b\u5dee\u5f02\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u8ba2\u3002", "result": "\u5728PaperBench Code-Dev\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRePro\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u63d0\u534713.0%\uff0c\u5e76\u80fd\u6b63\u786e\u4fee\u8ba2\u590d\u6742\u903b\u8f91\u548c\u6570\u5b66\u6807\u51c6\u3002", "conclusion": "RePro\u901a\u8fc7\u7cfb\u7edf\u6027\u53cd\u601d\u548c\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bba\u6587\u590d\u73b0\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.16678", "pdf": "https://arxiv.org/pdf/2508.16678", "abs": "https://arxiv.org/abs/2508.16678", "authors": ["Konrad Cinkusz", "Jaros\u0142aw A. Chudziak", "Ewa Niewiadomska-Szynkiewicz"], "title": "Cognitive Agents Powered by Large Language Models for Agile Software Project Management", "categories": ["cs.SE", "cs.MA"], "comment": null, "summary": "This paper investigates the integration of cognitive agents powered by Large\nLanguage Models (LLMs) within the Scaled Agile Framework (SAFe) to reinforce\nsoftware project management. By deploying virtual agents in simulated software\nenvironments, this study explores their potential to fulfill fundamental roles\nin IT project development, thereby optimizing project outcomes through\nintelligent automation. Particular emphasis is placed on the adaptability of\nthese agents to Agile methodologies and their transformative impact on\ndecision-making, problem-solving, and collaboration dynamics. The research\nleverages the CogniSim ecosystem, a platform designed to simulate real-world\nsoftware engineering challenges, such as aligning technical capabilities with\nbusiness objectives, managing interdependencies, and maintaining project\nagility. Through iterative simulations, cognitive agents demonstrate advanced\ncapabilities in task delegation, inter-agent communication, and project\nlifecycle management. By employing natural language processing to facilitate\nmeaningful dialogues, these agents emulate human roles and improve the\nefficiency and precision of Agile practices. Key findings from this\ninvestigation highlight the ability of LLM-powered cognitive agents to deliver\nmeasurable improvements in various metrics, including task completion times,\nquality of deliverables, and communication coherence. These agents exhibit\nscalability and adaptability, ensuring their applicability across diverse and\ncomplex project environments. This study underscores the potential of\nintegrating LLM-powered agents into Agile project management frameworks as a\nmeans of advancing software engineering practices. This integration not only\nrefines the execution of project management tasks but also sets the stage for a\nparadigm shift in how teams collaborate and address emerging challenges.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8ba4\u77e5\u4ee3\u7406\u5728SAFe\u6846\u67b6\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u4f18\u5316\u8f6f\u4ef6\u9879\u76ee\u7ba1\u7406\u3002\u901a\u8fc7\u865a\u62df\u4ee3\u7406\u5728\u6a21\u62df\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u4efb\u52a1\u5206\u914d\u3001\u6c9f\u901a\u548c\u9879\u76ee\u751f\u547d\u5468\u671f\u7ba1\u7406\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u8ba4\u77e5\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u667a\u80fd\u81ea\u52a8\u5316\u63d0\u5347\u654f\u6377\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u51b3\u7b56\u3001\u95ee\u9898\u89e3\u51b3\u548c\u534f\u4f5c\u6548\u7387\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u662f\u5728CogniSim\u751f\u6001\u7cfb\u7edf\u4e2d\u90e8\u7f72\u865a\u62df\u4ee3\u7406\uff0c\u8fdb\u884c\u8fed\u4ee3\u6a21\u62df\uff0c\u6d4b\u8bd5\u5176\u5728\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3001\u4ea4\u4ed8\u8d28\u91cf\u548c\u6c9f\u901a\u4e00\u81f4\u6027\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8ba4\u77e5\u4ee3\u7406\u80fd\u591f\u663e\u8457\u6539\u5584\u654f\u6377\u5b9e\u8df5\u4e2d\u7684\u6548\u7387\u4e0e\u7cbe\u5ea6\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8ba4\u4e3a\uff0cLLM\u9a71\u52a8\u7684\u8ba4\u77e5\u4ee3\u7406\u5728\u654f\u6377\u6846\u67b6\u4e2d\u7684\u96c6\u6210\u6709\u671b\u63a8\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2508.16684", "pdf": "https://arxiv.org/pdf/2508.16684", "abs": "https://arxiv.org/abs/2508.16684", "authors": ["Vikranth Udandarao", "Nipun Misra"], "title": "Democratizing AI Development: Local LLM Deployment for India's Developer Ecosystem in the Era of Tokenized APIs", "categories": ["cs.SE", "cs.HC"], "comment": "for survey results, check\n  https://docs.google.com/spreadsheets/d/1t0eV9oURaiu2HfARWo6sriBO0eC8bHUyZNN7CgK2NAk/edit?usp=sharing", "summary": "India's developer community faces significant barriers to sustained\nexperimentation and learning with commercial Large Language Model (LLM) APIs,\nprimarily due to economic and infrastructural constraints. This study\nempirically evaluates local LLM deployment using Ollama as an alternative to\ncommercial cloud-based services for developer-focused applications. Through a\nmixed-methods analysis involving 180 Indian developers, students, and AI\nenthusiasts, we find that local deployment enables substantially greater\nhands-on development and experimentation, while reducing costs by 33% compared\nto commercial solutions. Developers using local LLMs completed over twice as\nmany experimental iterations and reported deeper understanding of advanced AI\narchitectures. Our results highlight local deployment as a critical enabler for\ninclusive and accessible AI development, demonstrating how technological\naccessibility can enhance learning outcomes and innovation capacity in\nresource-constrained environments.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5728\u5370\u5ea6\u5f00\u53d1\u8005\u793e\u533a\u4e2d\u4f7f\u7528Ollama\u672c\u5730\u90e8\u7f72LLM\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u5176\u6210\u672c\u964d\u4f4e33%\u4e14\u5b9e\u9a8c\u8fed\u4ee3\u6b21\u6570\u7ffb\u500d\uff0c\u589e\u5f3a\u4e86AI\u5f00\u53d1\u7684\u53ef\u53ca\u6027\u3002", "motivation": "\u5370\u5ea6\u5f00\u53d1\u8005\u56e0\u7ecf\u6d4e\u548c\u57fa\u7840\u8bbe\u65bd\u9650\u5236\u96be\u4ee5\u6301\u7eed\u4f7f\u7528\u5546\u4e1aLLM API\uff0c\u7814\u7a76\u8bd5\u56fe\u9a8c\u8bc1\u672c\u5730\u90e8\u7f72\u7684\u66ff\u4ee3\u65b9\u6848\u662f\u5426\u53ef\u884c\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5206\u6790\uff0c\u6d89\u53ca180\u540d\u5370\u5ea6\u5f00\u53d1\u8005\u3001\u5b66\u751f\u548cAI\u7231\u597d\u8005\uff0c\u5bf9\u6bd4\u672c\u5730\u90e8\u7f72\u4e0e\u5546\u4e1a\u4e91\u670d\u52a1\u7684\u6210\u672c\u548c\u5b66\u4e60\u6548\u679c\u3002", "result": "\u672c\u5730\u90e8\u7f72\u6210\u672c\u964d\u4f4e33%\uff0c\u5b9e\u9a8c\u8fed\u4ee3\u6b21\u6570\u7ffb\u500d\uff0c\u5f00\u53d1\u8005\u5bf9AI\u67b6\u6784\u7684\u7406\u89e3\u66f4\u6df1\u5165\u3002", "conclusion": "\u672c\u5730LLM\u90e8\u7f72\u662f\u63d0\u5347\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2dAI\u5f00\u53d1\u53ef\u53ca\u6027\u548c\u521b\u65b0\u80fd\u529b\u7684\u5173\u952e\u3002"}}
{"id": "2508.16688", "pdf": "https://arxiv.org/pdf/2508.16688", "abs": "https://arxiv.org/abs/2508.16688", "authors": ["Ankur Tomar", "Hengyue Liang", "Indranil Bhattacharya", "Natalia Larios", "Francesco Carbone"], "title": "Cybernaut: Towards Reliable Web Automation", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The emergence of AI-driven web automation through Large Language Models\n(LLMs) offers unprecedented opportunities for optimizing digital workflows.\nHowever, deploying such systems within industry's real-world environments\npresents four core challenges: (1) ensuring consistent execution, (2)\naccurately identifying critical HTML elements, (3) meeting human-like accuracy\nin order to automate operations at scale and (4) the lack of comprehensive\nbenchmarking data on internal web applications. Existing solutions are\nprimarily tailored for well-designed, consumer-facing websites (e.g.,\nAmazon.com, Apple.com) and fall short in addressing the complexity of\npoorly-designed internal web interfaces. To address these limitations, we\npresent Cybernaut, a novel framework to ensure high execution consistency in\nweb automation agents designed for robust enterprise use. Our contributions are\nthreefold: (1) a Standard Operating Procedure (SOP) generator that converts\nuser demonstrations into reliable automation instructions for linear browsing\ntasks, (2) a high-precision HTML DOM element recognition system tailored for\nthe challenge of complex web interfaces, and (3) a quantitative metric to\nassess execution consistency. The empirical evaluation on our internal\nbenchmark demonstrates that using our framework enables a 23.2% improvement\n(from 72% to 88.68%) in task execution success rate over the browser_use.\nCybernaut identifies consistent execution patterns with 84.7% accuracy,\nenabling reliable confidence assessment and adaptive guidance during task\nexecution in real-world systems. These results highlight Cybernaut's\neffectiveness in enterprise-scale web automation and lay a foundation for\nfuture advancements in web automation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Cybernaut\u6846\u67b6\uff0c\u89e3\u51b3\u4e86LLM\u9a71\u52a8\u7f51\u7edc\u81ea\u52a8\u5316\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u56db\u5927\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6267\u884c\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u8bbe\u8ba1\u826f\u597d\u7684\u6d88\u8d39\u8005\u7f51\u7ad9\uff0c\u800c\u5ffd\u89c6\u4e86\u590d\u6742\u4f01\u4e1a\u5185\u90e8\u754c\u9762\u7684\u81ea\u52a8\u5316\u9700\u6c42\u3002", "method": "\u901a\u8fc7SOP\u751f\u6210\u5668\u3001\u9ad8\u7cbe\u5ea6HTML\u5143\u7d20\u8bc6\u522b\u7cfb\u7edf\u548c\u4e00\u81f4\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u63d0\u5347\u81ea\u52a8\u5316\u6267\u884c\u7684\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCybernaut\u5c06\u4efb\u52a1\u6267\u884c\u6210\u529f\u7387\u4ece72%\u63d0\u5347\u81f388.68%\uff0c\u8bc6\u522b\u4e00\u81f4\u6027\u6a21\u5f0f\u7684\u51c6\u786e\u7387\u8fbe84.7%\u3002", "conclusion": "Cybernaut\u5728\u4f01\u4e1a\u7ea7\u7f51\u7edc\u81ea\u52a8\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u6280\u672f\u8fdb\u6b65\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.16690", "pdf": "https://arxiv.org/pdf/2508.16690", "abs": "https://arxiv.org/abs/2508.16690", "authors": ["Vaastav Anand", "Deepak Garg", "Antoine Kaufmann"], "title": "Iridescent: A Framework Enabling Online System Implementation Specialization", "categories": ["cs.OS"], "comment": null, "summary": "Specializing systems to specifics of the workload they serve and platform\nthey are running on often significantly improves performance. However,\nspecializing systems is difficult in practice because of compounding\nchallenges: i) complexity for the developers to determine and implement optimal\nspecialization; ii) inherent loss of generality of the resulting\nimplementation, and iii) difficulty in identifying and implementing a single\noptimal specialized configuration for the messy reality of modern systems. To\naddress this, we introduce Iridescent, a framework for automated online system\nspecialization guided by observed overall system performance. Iridescent lets\ndevelopers specify a space of possible specialization choices, and then at\nruntime generates and runs different specialization choices through JIT\ncompilation as the system runs. By using overall system performance metrics to\nguide this search, developers can use Iridescent to find optimal system\nspecializations for the hardware and workload conditions at a given time. We\ndemonstrate feasibility, effectivity, and ease of use.", "AI": {"tldr": "Iridescent\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316\u5728\u7ebf\u7cfb\u7edf\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u7cfb\u7edf\u5b9a\u5236\u5316\u7684\u590d\u6742\u6027\u3001\u901a\u7528\u6027\u635f\u5931\u548c\u914d\u7f6e\u96be\u9898\u3002", "motivation": "\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u9700\u8981\u9488\u5bf9\u8d1f\u8f7d\u548c\u5e73\u53f0\u8fdb\u884c\u5b9a\u5236\uff0c\u4f46\u5b9e\u8df5\u4e2d\u9762\u4e34\u5f00\u53d1\u590d\u6742\u6027\u3001\u901a\u7528\u6027\u635f\u5931\u548c\u914d\u7f6e\u4f18\u5316\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faIridescent\u6846\u67b6\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u5b9a\u4e49\u53ef\u80fd\u7684\u4f18\u5316\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7JIT\u7f16\u8bd1\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u751f\u6210\u548c\u6d4b\u8bd5\u4f18\u5316\u9009\u9879\uff0c\u4ee5\u7cfb\u7edf\u6027\u80fd\u6307\u6807\u4e3a\u6307\u5bfc\u3002", "result": "\u9a8c\u8bc1\u4e86Iridescent\u7684\u53ef\u884c\u6027\u3001\u6709\u6548\u6027\u548c\u6613\u7528\u6027\u3002", "conclusion": "Iridescent\u4e3a\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16816", "pdf": "https://arxiv.org/pdf/2508.16816", "abs": "https://arxiv.org/abs/2508.16816", "authors": ["Ali Parsa", "Neda Moghim", "Sachin Shetty"], "title": "QoS-based Intelligent multi-connectivity for B5G networks", "categories": ["cs.NI"], "comment": null, "summary": "The rapid advancement of communication technologies has established cellular\nnetworks as the backbone for diverse applications, each with distinct quality\nof service requirements. Meeting these varying demands within a unified\ninfrastructure presents a critical challenge that can be addressed through\nadvanced techniques such as multi-connectivity. Multiconnectivity enables User\nequipments to connect to multiple BSs simultaneously, facilitating QoS\ndifferentiation and provisioning. This paper proposes a QoS-aware\nmulti-connectivity framework leveraging machine learning to enhance network\nperformance. The approach employs deep neural networks to estimate the\nachievable QoS metrics of BSs, including data rate, reliability, and latency.\nThese predictions inform the selection of serving clusters and data rate\nallocation, ensuring that the User Equipment connects to the optimal BSs to\nmeet its QoS needs. Performance evaluations demonstrate that the proposed\nalgorithm significantly enhances Quality of Service (QoS) for applications\nwhere traditional and state-of-the-art methods are inadequate. Specifically,\nthe algorithm achieves a QoS success rate of 98%. Furthermore, it improves\nspectrum efficiency by 30% compared to existing multi-connectivity solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684QoS\u611f\u77e5\u591a\u8fde\u63a5\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u57fa\u7ad9\u7684\u670d\u52a1\u8d28\u91cf\u6307\u6807\uff0c\u4f18\u5316\u8fde\u63a5\u9009\u62e9\u548c\u8d44\u6e90\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347QoS\u548c\u9891\u8c31\u6548\u7387\u3002", "motivation": "\u968f\u7740\u901a\u4fe1\u6280\u672f\u7684\u53d1\u5c55\uff0c\u8702\u7a9d\u7f51\u7edc\u9700\u6ee1\u8db3\u591a\u6837\u5316\u7684\u670d\u52a1\u8d28\u91cf\u9700\u6c42\uff0c\u7edf\u4e00\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u591a\u8fde\u63a5\u6280\u672f\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u57fa\u7ad9\u7684\u6570\u636e\u901f\u7387\u3001\u53ef\u9760\u6027\u548c\u5ef6\u8fdf\u7b49QoS\u6307\u6807\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u9884\u6d4b\u9009\u62e9\u6700\u4f18\u670d\u52a1\u96c6\u7fa4\u548c\u6570\u636e\u901f\u7387\u5206\u914d\u3002", "result": "\u7b97\u6cd5\u5b9e\u73b0\u4e8698%\u7684QoS\u6210\u529f\u7387\uff0c\u5e76\u5c06\u9891\u8c31\u6548\u7387\u63d0\u9ad8\u4e8630%\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86QoS\u548c\u9891\u8c31\u6548\u7387\uff0c\u4e3a\u591a\u8fde\u63a5\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16696", "pdf": "https://arxiv.org/pdf/2508.16696", "abs": "https://arxiv.org/abs/2508.16696", "authors": ["Reema Alshehri", "Rawan Alotaibi", "Leen Almasri", "Rawan Altaweel"], "title": "DecoMind: A Generative AI System for Personalized Interior Design Layouts", "categories": ["cs.GR", "cs.AI"], "comment": "~7 pages; ~32 figures; compiled with pdfLaTeX. Primary category:\n  cs.CV. (Secondary: cs.AI)", "summary": "This paper introduces a system for generating interior design layouts based\non user inputs, such as room type, style, and furniture preferences. CLIP\nextracts relevant furniture from a dataset, and a layout that contains\nfurniture and a prompt are fed to Stable Diffusion with ControlNet to generate\na design that incorporates the selected furniture. The design is then evaluated\nby classifiers to ensure alignment with the user's inputs, offering an\nautomated solution for realistic interior design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u8f93\u5165\u751f\u6210\u5ba4\u5185\u8bbe\u8ba1\u5e03\u5c40\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408CLIP\u3001Stable Diffusion\u4e0eControlNet\u81ea\u52a8\u751f\u6210\u5e76\u8bc4\u4f30\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u81ea\u5b9a\u4e49\u5ba4\u5185\u8bbe\u8ba1\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u81ea\u52a8\u5316\u3001\u4e2a\u6027\u5316\u7684\u8bbe\u8ba1\u65b9\u6848\u3002", "method": "\u5229\u7528CLIP\u9009\u62e9\u5bb6\u5177\uff0c\u901a\u8fc7Stable Diffusion\u4e0eControlNet\u751f\u6210\u5e03\u5c40\uff0c\u518d\u4f7f\u7528\u5206\u7c7b\u5668\u8bc4\u4f30\u8bbe\u8ba1\u5339\u914d\u5ea6\u3002", "result": "\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u751f\u6210\u7b26\u5408\u7528\u6237\u504f\u597d\u7684\u5ba4\u5185\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4e2a\u6027\u5316\u5ba4\u5185\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16746", "pdf": "https://arxiv.org/pdf/2508.16746", "abs": "https://arxiv.org/abs/2508.16746", "authors": ["Karuna Grewal", "P. Brighten Godfrey", "Justin Hsu"], "title": "SafeTree: Expressive Tree Policies for Microservices", "categories": ["cs.PL"], "comment": null, "summary": "A microservice-based application is composed of multiple self-contained\ncomponents called microservices, and controlling inter-service communication is\nimportant for enforcing safety properties. Presently, inter-service\ncommunication is configured using microservice deployment tools. However, such\ntools only support a limited class of single-hop policies, which can be overly\npermissive because they ignore the rich service tree structure of microservice\ncalls. Policies that can express the service tree structure can offer\ndevelopment and security teams more fine-grained control over communication\npatterns.\n  To this end, we design an expressive policy language to specify service tree\nstructures, and we develop a visibly pushdown automata-based dynamic\nenforcement mechanism to enforce service tree policies. Our technique is\nnon-invasive: it does not require any changes to service implementations, and\ndoes not require access to microservice code. To realize our method, we build a\nruntime monitor on top of a service mesh, an emerging network infrastructure\nlayer that can control inter-service communication during deployment. In\nparticular, we employ the programmable network traffic filtering capabilities\nof Istio, a popular service mesh implementation, to implement an online and\ndistributed monitor. Our experiments show that our monitor can enforce rich\nsafety properties while adding minimal latency overhead on the order of\nmilliseconds.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8868\u8fbe\u4e30\u5bcc\u7684\u7b56\u7565\u8bed\u8a00\u6765\u63cf\u8ff0\u5fae\u670d\u52a1\u8c03\u7528\u6811\u7ed3\u6784\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89c6\u5316\u4e0b\u63a8\u81ea\u52a8\u673a\u7684\u52a8\u6001\u6267\u884c\u673a\u5236\u6765\u5f3a\u5236\u6267\u884c\u670d\u52a1\u6811\u7b56\u7565\uff0c\u901a\u8fc7\u5728\u670d\u52a1\u7f51\u683c\u4e0a\u6784\u5efa\u8fd0\u884c\u65f6\u76d1\u63a7\u5668\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u5b89\u5168\u7b56\u7565\u6267\u884c\u3002", "motivation": "\u5f53\u524d\u5fae\u670d\u52a1\u90e8\u7f72\u5de5\u5177\u4ec5\u652f\u6301\u6709\u9650\u7684\u4e00\u8df3\u7b56\u7565\uff0c\u5ffd\u7565\u5fae\u670d\u52a1\u8c03\u7528\u6811\u7ed3\u6784\uff0c\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u5bbd\u677e\u7684\u6743\u9650\u63a7\u5236\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u901a\u4fe1\u6a21\u5f0f\u63a7\u5236\u3002", "method": "\u4f7f\u7528\u53ef\u89c6\u5316\u4e0b\u63a8\u81ea\u52a8\u673a\u8bbe\u8ba1\u670d\u52a1\u6811\u7b56\u7565\u8bed\u8a00\uff0c\u5e76\u5728\u670d\u52a1\u7f51\u683c\uff08\u5982Istio\uff09\u4e0a\u6784\u5efa\u975e\u4fb5\u5165\u5f0f\u7684\u5206\u5e03\u5f0f\u8fd0\u884c\u65f6\u76d1\u63a7\u5668\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u76d1\u63a7\u5668\u80fd\u9ad8\u6548\u6267\u884c\u590d\u6742\u5b89\u5168\u7b56\u7565\uff0c\u4ec5\u589e\u52a0\u6beb\u79d2\u7ea7\u5ef6\u8fdf\u3002", "conclusion": "\u901a\u8fc7\u670d\u52a1\u7f51\u683c\u5b9e\u73b0\u7684\u670d\u52a1\u6811\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5fae\u670d\u52a1\u95f4\u901a\u4fe1\u7684\u5b89\u5168\u6027\u548c\u63a7\u5236\u7cbe\u5ea6\u3002"}}
{"id": "2508.16782", "pdf": "https://arxiv.org/pdf/2508.16782", "abs": "https://arxiv.org/abs/2508.16782", "authors": ["W\u0142odzimierz Drabent"], "title": "On systematic construction of correct logic programs", "categories": ["cs.LO", "cs.SE", "68N17 (Primary) 68N30, 68Q60 (Secondary)", "D.1.6; D.2.3; F.3.1"], "comment": "21 pages. Accepted for ICLP 2025 (The 41st International Conference\n  on Logic Programming)", "summary": "Partial correctness of imperative or functional programming divides in logic\nprogramming into two notions. Correctness means that all answers of the program\nare compatible with the specification. Completeness means that the program\nproduces all the answers required by the specifications. We also consider\nsemi-completeness -- completeness for those queries for which the program does\nnot diverge. This paper presents an approach to systematically construct\nprovably correct and semi-complete logic programs, for a given specification.\nNormal programs are considered, under Kunen's 3-valued completion semantics (of\nnegation as finite failure) and the well-founded semantics (of negation as\npossibly infinite failure). The approach is declarative, it abstracts from\ndetails of operational semantics, like e.g.\\ the form of the selected literals\n(``procedure calls'') during the computation. The proposed method is simple,\nand can be used (maybe informally) in actual everyday programming.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u6784\u5efa\u903b\u8f91\u7a0b\u5e8f\u7684\u65b9\u6cd5\uff0c\u786e\u4fdd\u5176\u6b63\u786e\u6027\u548c\u534a\u5b8c\u6574\u6027\uff0c\u9002\u7528\u4e8e\u7ed9\u5b9a\u89c4\u8303\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u903b\u8f91\u7a0b\u5e8f\u4e2d\u6b63\u786e\u6027\u548c\u5b8c\u6574\u6027\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u9002\u7528\u4e8e\u65e5\u5e38\u7f16\u7a0b\u7684\u5b9e\u7528\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u58f0\u660e\u5f0f\u65b9\u6cd5\uff0c\u57fa\u4e8eKunen\u76843\u503c\u5b8c\u6210\u8bed\u4e49\u548c\u826f\u57fa\u8bed\u4e49\uff0c\u62bd\u8c61\u64cd\u4f5c\u8bed\u4e49\u7ec6\u8282\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u6784\u9020\u51fa\u53ef\u8bc1\u660e\u6b63\u786e\u4e14\u534a\u5b8c\u6574\u7684\u903b\u8f91\u7a0b\u5e8f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u5b9e\u7528\uff0c\u53ef\u7528\u4e8e\u5b9e\u9645\u7f16\u7a0b\u4e2d\uff0c\u63d0\u5347\u7a0b\u5e8f\u7684\u53ef\u9760\u6027\u548c\u5b8c\u6574\u6027\u3002"}}
{"id": "2508.17163", "pdf": "https://arxiv.org/pdf/2508.17163", "abs": "https://arxiv.org/abs/2508.17163", "authors": ["Yili Jin", "Xue Liu", "Jiangchuan Liu"], "title": "Generative AI for Multimedia Communication: Recent Advances, An Information-Theoretic Framework, and Future Opportunities", "categories": ["cs.MM", "eess.IV"], "comment": "ACM Multimedia 2025", "summary": "Recent breakthroughs in generative artificial intelligence (AI) are\ntransforming multimedia communication. This paper systematically reviews key\nrecent advancements across generative AI for multimedia communication,\nemphasizing transformative models like diffusion and transformers. However,\nconventional information-theoretic frameworks fail to address semantic\nfidelity, critical to human perception. We propose an innovative semantic\ninformation-theoretic framework, introducing semantic entropy, mutual\ninformation, channel capacity, and rate-distortion concepts specifically\nadapted to multimedia applications. This framework redefines multimedia\ncommunication from purely syntactic data transmission to semantic information\nconveyance. We further highlight future opportunities and critical research\ndirections. We chart a path toward robust, efficient, and semantically\nmeaningful multimedia communication systems by bridging generative AI\ninnovations with information theory. This exploratory paper aims to inspire a\nsemantic-first paradigm shift, offering a fresh perspective with significant\nimplications for future multimedia research.", "AI": {"tldr": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u591a\u5a92\u4f53\u901a\u4fe1\u4e2d\u7684\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u8bed\u4e49\u4fe1\u606f\u8bba\u7684\u65b0\u6846\u67b6\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u591a\u5a92\u4f53\u901a\u4fe1\u7684\u6838\u5fc3\u3002", "motivation": "\u4f20\u7edf\u4fe1\u606f\u8bba\u6846\u67b6\u96be\u4ee5\u89e3\u51b3\u8bed\u4e49\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4eba\u7c7b\u611f\u77e5\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u7406\u8bba\u6846\u67b6\u6765\u9002\u5e94\u591a\u5a92\u4f53\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u8bed\u4e49\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u5305\u62ec\u8bed\u4e49\u71b5\u3001\u4e92\u4fe1\u606f\u3001\u4fe1\u9053\u5bb9\u91cf\u548c\u7387\u5931\u771f\u7b49\u6982\u5ff5\uff0c\u5c06\u591a\u5a92\u4f53\u901a\u4fe1\u4ece\u8bed\u6cd5\u6570\u636e\u4f20\u9012\u8f6c\u5411\u8bed\u4e49\u4fe1\u606f\u4f20\u9012\u3002", "result": "\u8be5\u6846\u67b6\u4e3a\u591a\u5a92\u4f53\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u9c81\u68d2\u4e14\u8bed\u4e49\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5f0fAI\u4e0e\u4fe1\u606f\u7406\u8bba\uff0c\u672c\u6587\u4e3a\u591a\u5a92\u4f53\u901a\u4fe1\u7684\u8bed\u4e49\u4f18\u5148\u8303\u5f0f\u8f6c\u53d8\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5bf9\u672a\u6765\u7814\u7a76\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2508.16653", "pdf": "https://arxiv.org/pdf/2508.16653", "abs": "https://arxiv.org/abs/2508.16653", "authors": ["Zizhuo Fu", "Xiaotian Guo", "Wenxuan Zeng", "Shuzhang Zhong", "Yadong Zhang", "Peiyu Chen", "Runsheng Wang", "Le Ye", "Meng Li"], "title": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for Efficient Long-Context LLM Inference", "categories": ["cs.PF"], "comment": "International Conference on Computer-Aided Design (ICCAD) 2025", "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks.", "AI": {"tldr": "H2EAL\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u952e\u5408\u6280\u672f\u7684\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\u4e0e\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u90e8\u7f72\u65f6\u56e0KV\u7f13\u5b58\u5bfc\u81f4\u7684\u9ad8\u80fd\u8017\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u5229\u7528\u65b0\u5174\u7684\u6df7\u5408\u952e\u5408\u6280\u672f\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u5e26\u5bbd\u548c\u4f4e\u529f\u8017\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6848\uff08\u9759\u6001\u548c\u52a8\u6001\u7a00\u758f\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u786c\u4ef6\u652f\u6301\u534f\u540c\u8ba1\u7b97\u4e0e\u5185\u5b58\u5206\u914d\uff0c\u540c\u65f6\u5f00\u53d1\u4e86\u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cH2EAL\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u4e865.20~48.21\u500d\u7684\u52a0\u901f\u548c6.22~73.48\u500d\u7684\u80fd\u6548\u63d0\u5347\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.87%\u3002", "conclusion": "H2EAL\u901a\u8fc7\u7b97\u6cd5\u4e0e\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0aLLM\u63a8\u7406\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u6df7\u5408\u952e\u5408\u6280\u672f\u5728\u9ad8\u6548\u8ba1\u7b97\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.17203", "pdf": "https://arxiv.org/pdf/2508.17203", "abs": "https://arxiv.org/abs/2508.17203", "authors": ["Zhihao Ding", "Yongkang Sun", "Jieming Shi"], "title": "Retrieve-and-Verify: A Table Context Selection Framework for Accurate Column Annotations", "categories": ["cs.DB"], "comment": "Accepted at SIGMOD 2026", "summary": "Tables are a prevalent format for structured data, yet their metadata, such\nas semantic types and column relationships, is often incomplete or ambiguous.\nColumn annotation tasks, including Column Type Annotation (CTA) and Column\nProperty Annotation (CPA), address this by leveraging table context, which are\ncritical for data management. Existing methods typically serialize all columns\nin a table into pretrained language models to incorporate context, but this\ncoarse-grained approach often degrades performance in wide tables with many\nirrelevant or misleading columns. To address this, we propose a novel\nretrieve-and-verify context selection framework for accurate column annotation,\nintroducing two methods: REVEAL and REVEAL+. In REVEAL, we design an efficient\nunsupervised retrieval technique to select compact, informative column contexts\nby balancing semantic relevance and diversity, and develop context-aware\nencoding techniques with role embeddings and target-context pair training to\neffectively differentiate target and context columns. To further improve\nperformance, in REVEAL+, we design a verification model that refines the\nselected context by directly estimating its quality for specific annotation\ntasks. To achieve this, we formulate a novel column context verification\nproblem as a classification task and then develop the verification model.\nMoreover, in REVEAL+, we develop a top-down verification inference technique to\nensure efficiency by reducing the search space for high-quality context subsets\nfrom exponential to quadratic. Extensive experiments on six benchmark datasets\ndemonstrate that our methods consistently outperform state-of-the-art\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREVEAL\u548cREVEAL+\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8868\u683c\u5217\u6ce8\u91ca\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u9009\u62e9\uff0c\u901a\u8fc7\u68c0\u7d22\u548c\u9a8c\u8bc1\u673a\u5236\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u8868\u683c\u6570\u636e\u7684\u5143\u6570\u636e\uff08\u5982\u8bed\u4e49\u7c7b\u578b\u548c\u5217\u5173\u7cfb\uff09\u5e38\u4e0d\u5b8c\u6574\u6216\u6a21\u7cca\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5bbd\u8868\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "REVEAL\u91c7\u7528\u65e0\u76d1\u7763\u68c0\u7d22\u9009\u62e9\u7d27\u51d1\u4fe1\u606f\u4e0a\u4e0b\u6587\uff0cREVEAL+\u901a\u8fc7\u9a8c\u8bc1\u6a21\u578b\u8fdb\u4e00\u6b65\u4f18\u5316\u4e0a\u4e0b\u6587\u8d28\u91cf\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u6539\u8fdb\u4e86\u8868\u683c\u5217\u6ce8\u91ca\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.16584", "pdf": "https://arxiv.org/pdf/2508.16584", "abs": "https://arxiv.org/abs/2508.16584", "authors": ["Zhongling Su", "Rong Fu", "Weihan Cao", "Jianfei Gao", "Minxi Jin", "Zhilin Pei", "Hui Wang"], "title": "TMA-Adaptive FP8 Grouped GEMM: Eliminating Padding Requirements in Low-Precision Training and Inference on Hopper", "categories": ["cs.AR"], "comment": null, "summary": "Current FP8 grouped GEMM implementations require padding each group to a\nfixed alignment (e.g., 128), incurring memory and computational overhead. We\npropose \\textit{TMA-Adaptive FP8 Grouped GEMM}, which eliminates padding by\ndynamically adapting to variable group dimensions via (1) a TMA descriptor pool\nwith $\\log_2(block_M)$ preconfigured descriptors to handle all residual row\ncases through dynamic runtime selection and dual-phase load-store operations,\nachieving comprehensive coverage with minimal overhead, and (2)\nTMA-alignment-aware management to satisfy 16-byte global memory alignment and\n128-byte shared memory alignment. Experiments demonstrate 1.7\\% to 20.4\\% speed\nup with up to 23.8\\% memory reduction compared to padding operation plus\nstate-of-the-art FP8 grouped GEMM, while maintaining full numerical equivalence\nfor valid data. The source code is publicly available at an anonymous\nrepository: https://github.com/sukoncon/TMA-Adaptive-FP8-Grouped-GEMM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u7ec4\u5c3a\u5bf8\u7684FP8\u5206\u7ec4GEMM\u65b9\u6cd5\uff0c\u6d88\u9664\u4e86\u586b\u5145\u5f00\u9500\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u5f53\u524dFP8\u5206\u7ec4GEMM\u5b9e\u73b0\u9700\u8981\u56fa\u5b9a\u5bf9\u9f50\u586b\u5145\uff0c\u5bfc\u81f4\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u8f83\u5927\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u9884\u914d\u7f6e\u63cf\u8ff0\u7b26\u548c\u53cc\u9636\u6bb5\u52a0\u8f7d-\u5b58\u50a8\u64cd\u4f5c\uff0c\u4ee5\u53ca\u5bf9\u9f50\u7ba1\u7406\uff0c\u5b9e\u73b0\u65e0\u586b\u5145\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u901f\u5ea6\u63d0\u53471.7%-20.4%\uff0c\u5185\u5b58\u51cf\u5c1123.8%\uff0c\u4e14\u4fdd\u6301\u6570\u503c\u7b49\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u5b9e\u7528\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.16580", "pdf": "https://arxiv.org/pdf/2508.16580", "abs": "https://arxiv.org/abs/2508.16580", "authors": ["Weiyu Ma", "Dongyu Xu", "Shu Lin", "Haifeng Zhang", "Jun Wang"], "title": "Adaptive Command: Real-Time Policy Adjustment via Language Models in StarCraft II", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "We present Adaptive Command, a novel framework integrating large language\nmodels (LLMs) with behavior trees for real-time strategic decision-making in\nStarCraft II. Our system focuses on enhancing human-AI collaboration in\ncomplex, dynamic environments through natural language interactions. The\nframework comprises: (1) an LLM-based strategic advisor, (2) a behavior tree\nfor action execution, and (3) a natural language interface with speech\ncapabilities. User studies demonstrate significant improvements in player\ndecision-making and strategic adaptability, particularly benefiting novice\nplayers and those with disabilities. This work contributes to the field of\nreal-time human-AI collaborative decision-making, offering insights applicable\nbeyond RTS games to various complex decision-making scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aAdaptive Command\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u884c\u4e3a\u6811\u7ed3\u5408\uff0c\u7528\u4e8e\u300a\u661f\u9645\u4e89\u9738II\u300b\u4e2d\u7684\u5b9e\u65f6\u6218\u7565\u51b3\u7b56\uff0c\u91cd\u70b9\u5173\u6ce8\u4eba\u673a\u534f\u4f5c\u7684\u63d0\u5347\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u589e\u5f3a\u4eba\u7c7b\u4e0eAI\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5b9e\u65f6\u6218\u7565\u6e38\u620f\u4e2d\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u90e8\u5206\uff1a\u57fa\u4e8eLLM\u7684\u6218\u7565\u987e\u95ee\u3001\u884c\u4e3a\u6811\u6267\u884c\u52a8\u4f5c\u3001\u4ee5\u53ca\u5177\u6709\u8bed\u97f3\u529f\u80fd\u7684\u81ea\u7136\u8bed\u8a00\u754c\u9762\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u73a9\u5bb6\uff08\u5c24\u5176\u662f\u65b0\u624b\u548c\u6b8b\u969c\u4eba\u58eb\uff09\u7684\u51b3\u7b56\u80fd\u529b\u548c\u6218\u7565\u9002\u5e94\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u4e3a\u5b9e\u65f6\u4eba\u673a\u534f\u4f5c\u51b3\u7b56\u63d0\u4f9b\u4e86\u521b\u65b0\u6846\u67b6\uff0c\u8fd8\u4e3a\u590d\u6742\u51b3\u7b56\u573a\u666f\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u5e94\u7528\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.16933", "pdf": "https://arxiv.org/pdf/2508.16933", "abs": "https://arxiv.org/abs/2508.16933", "authors": ["Dhandeep Challagundla", "Venkata Krishna Vamsi Sundarapu", "Ignatius Bezzam", "Riadul Islam"], "title": "TSPC-PFD: TSPC-Based Low-Power High-Resolution CMOS Phase Frequency Detector", "categories": ["cs.ET", "eess.SP"], "comment": null, "summary": "Phase Frequency Detectors (PFDs) are essential components in Phase-Locked\nLoop (PLL) and Delay-Locked Loop (DLL) systems, responsible for comparing phase\nand frequency differences and generating up/down signals to regulate charge\npumps and/or, consequently, Voltage-Controlled Oscillators (VCOs). Conventional\nPFD designs often suffer from significant dead zones and blind zones, which\ndegrade phase detection accuracy and increase jitter in high-speed\napplications. This paper addresses PFD design challenges and presents a novel\nlow-power True Single-Phase Clock (TSPC)-based PFD. The proposed design\neliminates the blind zone entirely while achieving a minimal dead zone of 40\nps. The proposed PFD, implemented using TSMC 28 nm technology, demonstrates a\nlow-power consumption of 4.41 uW at 3 GHz input frequency with a layout area of\n$10.42\\mu m^2$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4f4e\u529f\u8017TSPC\u57fa\u76f8\u4f4d\u9891\u7387\u68c0\u6d4b\u5668\uff0c\u5b8c\u5168\u6d88\u9664\u76f2\u533a\u5e76\u5b9e\u73b040 ps\u7684\u8d85\u5c0f\u6b7b\u533a\uff0cTSMC 28 nm\u5de5\u827a\u4e0b\u529f\u8017\u4ec54.41 uW\u3002", "motivation": "\u4f20\u7edfPFD\u5728\u9ad8\u9891\u5e94\u7528\u4e2d\u5b58\u5728\u663e\u8457\u6b7b\u533a\u548c\u76f2\u533a\uff0c\u5f71\u54cd\u76f8\u4f4d\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u589e\u52a0\u6296\u52a8\uff0c\u9700\u6539\u8fdb\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528TSPC\u6280\u672f\u8bbe\u8ba1\u65b0\u578bPFD\uff0c\u4f18\u5316\u7535\u8def\u4ee5\u6d88\u9664\u76f2\u533a\u5e76\u7f29\u5c0f\u6b7b\u533a\u3002", "result": "\u5b9e\u73b0\u65e0\u76f2\u533a\u3001\u6b7b\u533a\u4ec540 ps\u7684PFD\uff0c3 GHz\u9891\u7387\u4e0b\u529f\u80174.41 uW\uff0c\u9762\u79ef10.42\u5fae\u7c73\u00b2\u3002", "conclusion": "\u6240\u63d0PFD\u8bbe\u8ba1\u5728\u9ad8\u6027\u80fd\u548c\u4f4e\u529f\u8017\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6848\uff0c\u9002\u5408\u9ad8\u901f\u5e94\u7528\u3002"}}
{"id": "2508.16592", "pdf": "https://arxiv.org/pdf/2508.16592", "abs": "https://arxiv.org/abs/2508.16592", "authors": ["Gregor Corbin"], "title": "Performance measurements of modern Fortran MPI applications with Score-P", "categories": ["cs.DC", "cs.MS", "cs.PF"], "comment": null, "summary": "Version 3.0 of the Message-Passing Interface (MPI) standard, released in\n2012, introduced a new set of language bindings for Fortran 2008. By making use\nof modern language features and the enhanced interoperability with C, there was\nfinally a type safe and standard conforming method to call MPI from Fortran.\nThis highly recommended use mpi_f08 language binding has since then been widely\nadopted among developers of modern Fortran applications. However, tool support\nfor the F08 bindings is still lacking almost a decade later, forcing users to\nrecede to the less safe and convenient interfaces. Full support for the F08\nbindings was added to the performance measurement infrastructure Score-P by\nimplementing MPI wrappers in Fortran. Wrappers cover the latest MPI standard\nversion 4.1 in its entirety, matching the features of the C wrappers. By\nimplementing the wrappers in modern Fortran, we can provide full support for\nMPI procedures passing attributes, info objects, or callbacks. The\nimplementation is regularly tested under the MPICH test suite. The new F08\nwrappers were already used by two fluid dynamics simulation codes -- Neko, a\nspectral finite-element code derived from Nek5000, and EPIC (Elliptical\nParcel-In-Cell) -- to successfully generate performance measurements. In this\nwork, we additionally present our design considerations and sketch out the\nimplementation, discussing the challenges we faced in the process. The key\ncomponent of the implementation is a code generator that produces approximately\n50k lines of MPI wrapper code to be used by Score-P, relying on the Python\npympistandard module to provide programmatic access to the extracted data from\nthe MPI standard.", "AI": {"tldr": "MPI 3.0\u5f15\u5165\u4e86Fortran 2008\u7684\u65b0\u8bed\u8a00\u7ed1\u5b9a\uff0c\u4f46\u5de5\u5177\u652f\u6301\u4e0d\u8db3\uff0cScore-P\u901a\u8fc7Fortran\u5b9e\u73b0\u7684MPI\u5305\u88c5\u5668\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u7531\u4e8eMPI\u7684Fortran 2008\u7ed1\u5b9a\uff08F08\uff09\u7f3a\u4e4f\u5de5\u5177\u652f\u6301\uff0c\u7528\u6237\u88ab\u8feb\u4f7f\u7528\u4e0d\u5b89\u5168\u7684\u63a5\u53e3\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u5de5\u5177\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5728Fortran\u4e2d\u5b9e\u73b0MPI\u5305\u88c5\u5668\uff0c\u8986\u76d6MPI 4.1\u6807\u51c6\uff0c\u5e76\u4f7f\u7528\u4ee3\u7801\u751f\u6210\u5668\u81ea\u52a8\u751f\u6210\u7ea650k\u884c\u4ee3\u7801\u3002", "result": "\u65b0\u7684F08\u5305\u88c5\u5668\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u4e24\u4e2a\u6d41\u4f53\u52a8\u529b\u5b66\u6a21\u62df\u4ee3\u7801\uff08Neko\u548cEPIC\uff09\uff0c\u5e76\u652f\u6301\u6027\u80fd\u6d4b\u91cf\u3002", "conclusion": "\u901a\u8fc7Fortran\u5b9e\u73b0\u7684MPI\u5305\u88c5\u5668\u5f25\u8865\u4e86F08\u7ed1\u5b9a\u7684\u5de5\u5177\u652f\u6301\u4e0d\u8db3\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.16708", "pdf": "https://arxiv.org/pdf/2508.16708", "abs": "https://arxiv.org/abs/2508.16708", "authors": ["Shufeng Chen", "Halima El Badaoui", "Mariat James Elizebeth", "Takuya Nakashima", "Siddartha Khastgir", "Paul Jennings"], "title": "A Scalable Framework for the Management of STPA Requirements: a Case Study on eVTOL Operations", "categories": ["cs.SE"], "comment": null, "summary": "System-Theoretic Process Analysis (STPA) is a recommended method for\nanalysing complex systems, capable of identifying thousands of safety\nrequirements often missed by traditional techniques such as Failure Mode and\nEffects Analysis (FMEA) and Fault Tree Analysis (FTA). However, the absence of\na structured framework for managing and prioritising these requirements\npresents challenges, particularly in fast-paced development environments. This\npaper introduces a scalable framework for prioritising STPA-derived\nrequirements. The framework integrates outputs from each STPA step and\nincorporates expert evaluations based on four key factors: implementation time,\ncost, requirement type, and regulatory coverage. To reduce subjectivity,\nMonte-Carlo Simulation (MCS) is employed to calculate and stabilise requirement\nrankings. An automation toolchain supports the framework, enabling dynamic\nmapping of prioritised requirements in a scaling matrix. This visualisation\naids decision-making and ensures traceability across development phases. The\nframework is applicable from early conceptualisation to more advanced stages,\nenhancing its utility in iterative system development. The framework was\nvalidated through a real-world case study focused on Electric Vertical Take-off\nand Landing (eVTOL) operations, conducted in collaboration with the UK Civil\nAviation Authority. The findings contributed directly to CAP3141, a Civil\nAviation Publication that identifies systemic operational risks and safety\nmitigations for regulators, operators, and vertiports. The prioritisation\nprocess supported decision-making by helping stakeholders identify and manage\nhigh-impact requirements efficiently. This work contributes a practical\nsolution for managing STPA outputs, bridging gaps in requirement prioritisation\nand supporting safety-critical development in emerging technologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5148\u5904\u7406STPA\u751f\u6210\u7684\u5b89\u5168\u9700\u6c42\uff0c\u7ed3\u5408\u4e13\u5bb6\u8bc4\u4f30\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728eVTOL\u64cd\u4f5c\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982FMEA\u548cFTA\uff09\u5e38\u5ffd\u7565\u5927\u91cf\u5b89\u5168\u9700\u6c42\uff0c\u4e14\u7f3a\u4e4f\u7ed3\u6784\u5316\u7684\u9700\u6c42\u7ba1\u7406\u6846\u67b6\uff0c\u5c24\u5176\u5728\u5feb\u901f\u5f00\u53d1\u73af\u5883\u4e2d\u5b58\u5728\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u6574\u5408STPA\u5404\u6b65\u9aa4\u8f93\u51fa\u548c\u4e13\u5bb6\u8bc4\u4f30\uff08\u57fa\u4e8e\u5b9e\u73b0\u65f6\u95f4\u3001\u6210\u672c\u3001\u9700\u6c42\u7c7b\u578b\u548c\u6cd5\u89c4\u8986\u76d6\uff09\uff0c\u5229\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u51cf\u5c11\u4e3b\u89c2\u6027\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u5177\u94fe\u53ef\u89c6\u5316\u9700\u6c42\u4f18\u5148\u7ea7\u3002", "result": "\u6846\u67b6\u5728\u82f1\u56fd\u6c11\u822a\u5c40\u7684eVTOL\u64cd\u4f5c\u6848\u4f8b\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u76f4\u63a5\u8d21\u732e\u4e8eCAP3141\uff0c\u5e2e\u52a9\u5229\u76ca\u76f8\u5173\u8005\u9ad8\u6548\u8bc6\u522b\u548c\u7ba1\u7406\u9ad8\u5f71\u54cd\u9700\u6c42\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86STPA\u9700\u6c42\u4f18\u5148\u7ea7\u7684\u7a7a\u767d\uff0c\u4e3a\u65b0\u5174\u6280\u672f\u7684\u5b89\u5168\u5173\u952e\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17350", "pdf": "https://arxiv.org/pdf/2508.17350", "abs": "https://arxiv.org/abs/2508.17350", "authors": ["Haide Wang", "Ji Zhou", "Yongcheng Li", "Weiping Liu", "Changyuan Yu", "Xiangjun Xin", "Liangchuan Li"], "title": "Comparison of FTN-NOFDM and PCS-OFDM for Long-Haul Coherent Optical Communications", "categories": ["cs.NI"], "comment": "This manuscript has been submitted to the Journal of Lightwave\n  Technology", "summary": "Single-wavelength 400G coherent optical communications have become a critical\nsolution to meet the explosive traffic demands. However, the single-carrier\nmodulation using low-order modulation formats requires a broader wavelength\ndivision multiplexing grid and expands the occupied optical bandwidth. In this\npaper, we propose the faster-than-Nyquist non-orthogonal frequency division\nmultiplexing (FTN-NOFDM) to improve the spectral efficiency for long-haul\ncoherent optical communications. The subcarrier number is set to eight to\nenable low-complexity FTN-NOFDM signal generation using a pruned inverse fast\nFourier transform and inter-carrier interference (ICI) cancellation. To deal\nwith the conventional timing recovery (TR) failure, a frequency tone-based TR\nis proposed for FTN-NOFDM. A time-domain multiple-input multiple-output\nequalizer is designed to update the tap coefficients based on outputs of\nconventional iterative detection (ID). To further mitigate ICI, a low-density\nparity check-assisted ID is integrated into the conventional ID module.\nFTN-NOFDM, probabilistic constellation shaping (PCS)-OFDM, and quadrature phase\nshift keying-OFDM are experimentally compared in a 400G coherent optical\ncommunication system over 11 cascaded 125-GHz wavelength-selective switches\n(WSSs) and 2000 km transmission. Results show that the FTN-NOFDM exhibits\ncomparable WSS filtering tolerance to PCS-OFDM and superior nonlinearity\ntolerance, while PCS-OFDM achieves the best bit error ratio performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FTN-NOFDM\u6280\u672f\u4ee5\u63d0\u9ad8400G\u957f\u8ddd\u79bb\u76f8\u5e72\u5149\u901a\u4fe1\u7684\u9891\u8c31\u6548\u7387\uff0c\u901a\u8fc7\u4f4e\u590d\u6742\u5ea6\u4fe1\u53f7\u751f\u6210\u548c\u5e72\u6270\u6d88\u9664\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6ee4\u6ce2\u548c\u975e\u7ebf\u6027\u5bb9\u5fcd\u5ea6\u4f18\u4e8e\u4f20\u7edf\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u5355\u8f7d\u6ce2\u8c03\u5236\u5728\u4f4e\u9636\u8c03\u5236\u683c\u5f0f\u4e0b\u5360\u7528\u5e26\u5bbd\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u63d0\u5347\u957f\u8ddd\u79bb\u76f8\u5e72\u5149\u901a\u4fe1\u7684\u9891\u8c31\u6548\u7387\u3002", "method": "\u91c7\u75288\u4e2a\u5b50\u8f7d\u6ce2\u7684FTN-NOFDM\u6280\u672f\uff0c\u7ed3\u5408\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u548cICI\u6d88\u9664\uff0c\u63d0\u51fa\u57fa\u4e8e\u9891\u7387\u97f3\u7684\u5b9a\u65f6\u6062\u590d\u548cMIMO\u5747\u8861\u5668\u8bbe\u8ba1\u3002", "result": "FTN-NOFDM\u5728\u6ee4\u6ce2\u548c\u975e\u7ebf\u6027\u5bb9\u5fcd\u5ea6\u4e0a\u4f18\u4e8ePCS-OFDM\u548cQPSK-OFDM\uff0c\u4f46PCS-OFDM\u5728\u8bef\u7801\u7387\u6027\u80fd\u4e0a\u6700\u4f73\u3002", "conclusion": "FTN-NOFDM\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9891\u8c31\u5229\u7528\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u957f\u8ddd\u79bb\u5149\u901a\u4fe1\u7cfb\u7edf\uff0c\u5c24\u5176\u5728\u975e\u7ebf\u6027\u5bb9\u5fcd\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.16911", "pdf": "https://arxiv.org/pdf/2508.16911", "abs": "https://arxiv.org/abs/2508.16911", "authors": ["Prerit Gupta", "Jason Alexander Fotso-Puepi", "Zhengyuan Li", "Jay Mehta", "Aniket Bera"], "title": "MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation", "categories": ["cs.GR", "cs.CV", "cs.MM", "cs.SD"], "comment": "Accepted at ICCV 2025. Project page:\n  https://gprerit96.github.io/mdd-page", "summary": "We introduce Multimodal DuetDance (MDD), a diverse multimodal benchmark\ndataset designed for text-controlled and music-conditioned 3D duet dance motion\ngeneration. Our dataset comprises 620 minutes of high-quality motion capture\ndata performed by professional dancers, synchronized with music, and detailed\nwith over 10K fine-grained natural language descriptions. The annotations\ncapture a rich movement vocabulary, detailing spatial relationships, body\nmovements, and rhythm, making MDD the first dataset to seamlessly integrate\nhuman motions, music, and text for duet dance generation. We introduce two\nnovel tasks supported by our dataset: (1) Text-to-Duet, where given music and a\ntextual prompt, both the leader and follower dance motion are generated (2)\nText-to-Dance Accompaniment, where given music, textual prompt, and the\nleader's motion, the follower's motion is generated in a cohesive, text-aligned\nmanner. We include baseline evaluations on both tasks to support future\nresearch.", "AI": {"tldr": "\u4ecb\u7ecdMultimodal DuetDance\uff08MDD\uff09\u6570\u636e\u96c6\uff0c\u652f\u6301\u6587\u672c\u63a7\u5236\u548c\u97f3\u4e50\u9a71\u52a8\u76843D\u53cc\u4eba\u821e\u52a8\u4f5c\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u4e24\u9879\u65b0\u4efb\u52a1\u548c\u57fa\u7ebf\u8bc4\u4f30\u3002", "motivation": "\u4e3a\u6587\u672c\u63a7\u5236\u548c\u97f3\u4e50\u9a71\u52a8\u7684\u53cc\u4eba\u821e\u52a8\u4f5c\u751f\u6210\u63d0\u4f9b\u591a\u6837\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u586b\u8865\u6570\u636e\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b620\u5206\u949f\u9ad8\u8d28\u91cf\u52a8\u6355\u6570\u636e\u3001\u540c\u6b65\u97f3\u4e50\u548c1\u4e07+\u7ec6\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u4e24\u9879\u4efb\u52a1\uff08Text-to-Duet\u548cText-to-Dance Accompaniment\uff09\u3002", "result": "MDD\u662f\u9996\u4e2a\u6574\u5408\u4eba\u4f53\u52a8\u4f5c\u3001\u97f3\u4e50\u548c\u6587\u672c\u7684\u53cc\u4eba\u821e\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u57fa\u7ebf\u8bc4\u4f30\u652f\u6301\u7814\u7a76\u3002", "conclusion": "MDD\u4e3a\u591a\u6a21\u6001\u821e\u8e48\u751f\u6210\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.16848", "pdf": "https://arxiv.org/pdf/2508.16848", "abs": "https://arxiv.org/abs/2508.16848", "authors": ["David Moon", "Andrew Blinn", "Thomas J. Porter", "Cyrus Omar"], "title": "Syntactic Completions with Material Obligations", "categories": ["cs.PL"], "comment": null, "summary": "Code editors provide essential services that help developers understand,\nnavigate, and modify programs. However, these services often fail in the\npresence of syntax errors. Existing syntax error recovery techniques, like\npanic mode and multi-option repairs, are either too coarse, e.g. in deleting\nlarge swathes of code, or lead to a proliferation of possible completions. This\npaper introduces $\\texttt{tylr}$, a parser and editor generator that completes\narbitrarily malformed code by inserting obligations, which generalize holes to\ncover missing operands, operators, mixfix keywords, and sort transitions.\n$\\texttt{tylr}$ is backed by a novel theory of tile-based parsing, which\nextends operator-precedence parsing in two ways. First, traditional token\nprecedence comparisons are replaced by a notion of grammar walks, which form\nthe basis for generating obligations. Second, a distinct \"molding\" system based\non grammar zippers expand grammar expressivity by allowing the system to\ndisambiguate between possible parses and completions based on an obligation\nminimization criterion. In addition to serving as a novel approach to error\ncorrection, $\\texttt{tylr}$'s design enables the development of an editor that\nvisually materializes obligations to the human user, serving as a novel hybrid\nbetween a text editor and a structure editor. We introduce $\\texttt{tylr}$ by\nexample, then formalize its key ideas. Finally, we conduct a human subjects\nstudy to evaluate the extent to which an editor like $\\texttt{tylr}$ that\nmaterializes syntactic obligations might be usable and useful, finding both\npoints of positivity and interesting new avenues for future work.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86$\texttt{tylr}$\uff0c\u4e00\u79cd\u901a\u8fc7\u63d2\u5165\u201c\u4e49\u52a1\u201d\u8865\u5168\u8bed\u6cd5\u9519\u8bef\u7684\u89e3\u6790\u5668\u548c\u7f16\u8f91\u5668\u751f\u6210\u5668\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u4e0e\u7ed3\u6784\u7f16\u8f91\u5668\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u8bed\u6cd5\u9519\u8bef\u6062\u590d\u6280\u672f\u8981\u4e48\u8fc7\u4e8e\u7c97\u7cd9\uff08\u5982\u5220\u9664\u5927\u91cf\u4ee3\u7801\uff09\uff0c\u8981\u4e48\u5bfc\u81f4\u8fc7\u591a\u7684\u8865\u5168\u9009\u9879\uff0c$\texttt{tylr}$\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u201c\u74e6\u7247\u89e3\u6790\u201d\u7406\u8bba\u7684$\texttt{tylr}$\uff0c\u901a\u8fc7\u8bed\u6cd5\u884c\u8d70\u548c\u201c\u6210\u578b\u201d\u7cfb\u7edf\u6269\u5c55\u8bed\u6cd5\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u6700\u5c0f\u5316\u8865\u5168\u4e49\u52a1\u3002", "result": "$\texttt{tylr}$\u80fd\u591f\u6709\u6548\u8865\u5168\u9519\u8bef\u4ee3\u7801\uff0c\u4e14\u652f\u6301\u53ef\u89c6\u5316\u4e49\u52a1\u7684\u7f16\u8f91\u5668\u8bbe\u8ba1\uff0c\u7528\u6237\u7814\u7a76\u8868\u660e\u5176\u5177\u6709\u5b9e\u7528\u6027\u548c\u6f5c\u529b\u3002", "conclusion": "$\texttt{tylr}$\u4e3a\u8bed\u6cd5\u9519\u8bef\u6062\u590d\u548c\u7f16\u8f91\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u5de5\u4f5c\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.17758", "pdf": "https://arxiv.org/pdf/2508.17758", "abs": "https://arxiv.org/abs/2508.17758", "authors": ["Han Gao", "Daniil Kozhemiachenko", "Nicola Olivetti"], "title": "Paraconsistent Constructive Modal Logic", "categories": ["cs.LO"], "comment": null, "summary": "We present a family of paraconsistent counterparts of the constructive modal\nlogic CK. These logics aim to formalise reasoning about contradictory but\nnon-trivial propositional attitudes like beliefs or obligations. We define\ntheir Kripke-style semantics based on intuitionistic frames with two valuations\nwhich provide independent support for truth and falsity; they are connected by\nstrong negation as defined in Nelson's logic. A family of systems is obtained\ndepending on whether both modal operators are defined using the same or by\ndifferent accessibility relations for their positive and negative support. We\npropose Hilbert-style axiomatisations for all logics determined by this\nsemantic framework. We also propose a~family of modular cut-free sequent\ncalculi that we use to establish decidability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6784\u9020\u6027\u6a21\u6001\u903b\u8f91CK\u7684\u6b21\u534f\u8c03\u903b\u8f91\u53d8\u4f53\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u5904\u7406\u77db\u76fe\u4f46\u975e\u7410\u788e\u7684\u547d\u9898\u6001\u5ea6\uff08\u5982\u4fe1\u5ff5\u6216\u4e49\u52a1\uff09\uff0c\u5e76\u57fa\u4e8e\u76f4\u89c9\u4e3b\u4e49\u6846\u67b6\u548cNelson\u903b\u8f91\u4e2d\u7684\u5f3a\u5426\u5b9a\u5b9a\u4e49\u4e86\u4e00\u5957Kripke\u8bed\u4e49\u3002", "motivation": "\u65e8\u5728\u4e3a\u77db\u76fe\u4f46\u975e\u7410\u788e\u7684\u547d\u9898\u6001\u5ea6\uff08\u5982\u4fe1\u5ff5\u6216\u4e49\u52a1\uff09\u63d0\u4f9b\u5f62\u5f0f\u5316\u63a8\u7406\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\u76f4\u89c9\u4e3b\u4e49\u6846\u67b6\u548cNelson\u903b\u8f91\u4e2d\u7684\u5f3a\u5426\u5b9a\u5b9a\u4e49\u4e86\u4e00\u5957Kripke\u8bed\u4e49\uff1b\u63d0\u51fa\u4e86Hilbert-style\u516c\u7406\u7cfb\u7edf\u548c\u6a21\u5757\u5316\u7684\u5272\u514d\u8d39sequent\u6f14\u7b97\u3002", "result": "\u786e\u5b9a\u4e86\u8fd9\u4e9b\u903b\u8f91\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u7684\u53ef\u5224\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u903b\u8f91\u7cfb\u7edf\u4e3a\u5904\u7406\u77db\u76fe\u547d\u9898\u6001\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u6a21\u5757\u5316\u7684sequent\u6f14\u7b97\u8bc1\u660e\u4e86\u5176\u53ef\u5224\u5b9a\u6027\u3002"}}
{"id": "2508.17166", "pdf": "https://arxiv.org/pdf/2508.17166", "abs": "https://arxiv.org/abs/2508.17166", "authors": ["Yili Jin", "Ling Pan", "Rui-Xiao Zhang", "Jiangchuan Liu", "Xue Liu"], "title": "Generative Flow Networks for Personalized Multimedia Systems: A Case Study on Short Video Feeds", "categories": ["cs.MM", "eess.IV"], "comment": "ACM Multimedia 2025", "summary": "Multimedia systems underpin modern digital interactions, facilitating\nseamless integration and optimization of resources across diverse multimedia\napplications. To meet growing personalization demands, multimedia systems must\nefficiently manage competing resource needs, adaptive content, and\nuser-specific data handling. This paper introduces Generative Flow Networks\n(GFlowNets, GFNs) as a brave new framework for enabling personalized multimedia\nsystems. By integrating multi-candidate generative modeling with flow-based\nprinciples, GFlowNets offer a scalable and flexible solution for enhancing\nuser-specific multimedia experiences. To illustrate the effectiveness of\nGFlowNets, we focus on short video feeds, a multimedia application\ncharacterized by high personalization demands and significant resource\nconstraints, as a case study. Our proposed GFlowNet-based personalized feeds\nalgorithm demonstrates superior performance compared to traditional rule-based\nand reinforcement learning methods across critical metrics, including video\nquality, resource utilization efficiency, and delivery cost. Moreover, we\npropose a unified GFlowNet-based framework generalizable to other multimedia\nsystems, highlighting its adaptability and wide-ranging applicability. These\nfindings underscore the potential of GFlowNets to advance personalized\nmultimedia systems by addressing complex optimization challenges and supporting\nsophisticated multimedia application scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNets\uff09\u6765\u89e3\u51b3\u591a\u5a92\u4f53\u7cfb\u7edf\u4e2d\u7684\u4e2a\u6027\u5316\u9700\u6c42\u95ee\u9898\uff0c\u901a\u8fc7\u6d41\u91cf\u5efa\u6a21\u548c\u751f\u6210\u6280\u672f\u4f18\u5316\u7528\u6237\u4f53\u9a8c\uff0c\u5c24\u5176\u5728\u77ed\u89c6\u9891\u63a8\u8350\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u591a\u5a92\u4f53\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u4e2a\u6027\u5316\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u5982\u4f55\u9ad8\u6548\u7ba1\u7406\u8d44\u6e90\u3001\u81ea\u9002\u5e94\u5185\u5bb9\u53ca\u7528\u6237\u6570\u636e\u5904\u7406\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNets\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u5019\u9009\u751f\u6210\u5efa\u6a21\u548c\u6d41\u91cf\u539f\u7406\uff0c\u9002\u7528\u4e8e\u4e2a\u6027\u5316\u591a\u5a92\u4f53\u7cfb\u7edf\uff0c\u91cd\u70b9\u5e94\u7528\u4e8e\u77ed\u89c6\u9891\u63a8\u8350\u573a\u666f\u3002", "result": "GFlowNets\u5728\u89c6\u9891\u8d28\u91cf\u3001\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u4ea4\u4ed8\u6210\u672c\u7b49\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c55\u73b0\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "GFlowNets\u4e3a\u4e2a\u6027\u5316\u591a\u5a92\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.16703", "pdf": "https://arxiv.org/pdf/2508.16703", "abs": "https://arxiv.org/abs/2508.16703", "authors": ["Wangsong Yin", "Daliang Xu", "Mengwei Xu", "Gang Huang", "Xuanzhe Liu"], "title": "Dynamic Sparse Attention on Mobile SoCs", "categories": ["cs.PF", "cs.AI", "cs.LG"], "comment": "Technical Report", "summary": "On-device running Large Language Models (LLMs) is nowadays a critical enabler\ntowards preserving user privacy. We observe that the attention operator falls\nback from the special-purpose NPU to the general-purpose CPU/GPU because of\nquantization sensitivity in state-of-the-art frameworks. This fallback results\nin a degraded user experience and increased complexity in system scheduling. To\nthis end, this paper presents shadowAttn, a system-algorithm codesigned sparse\nattention module with minimal reliance on CPU/GPU by only sparsely calculating\nthe attention on a tiny portion of tokens. The key idea is to hide the overhead\nof estimating the important tokens with a NPU-based pilot compute. Further,\nshadowAttn proposes insightful techniques such as NPU compute graph bucketing,\nhead-wise NPU-CPU/GPU pipeline and per-head fine-grained sparsity ratio to\nachieve high accuracy and efficiency. shadowAttn delivers the best performance\nwith highly limited CPU/GPU resource; it requires much less CPU/GPU resource to\ndeliver on-par performance of SoTA frameworks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86shadowAttn\uff0c\u4e00\u79cd\u7cfb\u7edf-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u901a\u8fc7\u51cf\u5c11\u5bf9CPU/GPU\u7684\u4f9d\u8d56\u6765\u63d0\u5347\u8bbe\u5907\u8fd0\u884cLLM\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u4e2d\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u56e0\u91cf\u5316\u654f\u611f\u6027\u800c\u65e0\u6cd5\u5145\u5206\u5229\u7528NPU\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u7cfb\u7edf\u8c03\u5ea6\u590d\u6742\u5316\u3002", "method": "\u8bbe\u8ba1\u4e86shadowAttn\u6a21\u5757\uff0c\u901a\u8fc7\u7a00\u758f\u8ba1\u7b97\u6ce8\u610f\u529b\u3001NPU\u8f85\u52a9\u8ba1\u7b97\u3001\u8ba1\u7b97\u56fe\u5206\u6876\u7b49\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u8fd0\u884c\u3002", "result": "shadowAttn\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u4e14\u80fd\u4ee5\u66f4\u5c11\u8d44\u6e90\u5b9e\u73b0\u4e0e\u73b0\u6709\u6846\u67b6\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "shadowAttn\u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u5907\u8fd0\u884cLLM\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.17375", "pdf": "https://arxiv.org/pdf/2508.17375", "abs": "https://arxiv.org/abs/2508.17375", "authors": ["Junfang Huang", "Yu Yan", "Hongzhi Wang", "Yingze Li", "Jinghan Lin"], "title": "ForeSight: A Predictive-Scheduling Deterministic Database", "categories": ["cs.DB"], "comment": "14 pages, 11 figures", "summary": "Deterministic databases enable scalable replicated systems by executing\ntransactions in a predetermined order. However, existing designs fail to\ncapture transaction dependencies, leading to insufficient scheduling, high\nabort rates, and poor resource utilization. By addressing these challenges with\nlightweight conflict prediction and informed scheduling, we present ForeSight,\na high-performance deterministic database system. Our system has three core\nimprovements: (1) We design an Association Sum-Product Network to predict\npotential transaction conflicts, providing the input for dependency analysis\nwithout pre-obtained read/write sets. (2) We enhance the storage engine to\nintegrate multi-version-based optimization, improving the execution process and\nfallback strategy to boost commit rates and concurrency. (3) We propose a\nmatrix two-pass forward scan algorithm that performs dependency analysis to\ngenerate conflict-aware schedules, significantly reducing scheduling overhead.\nExperimental results on multiple benchmarks show that ForeSight achieves up to\n2$\\times$ higher throughput on skewed workloads and maintains strong\nperformance under contention, demonstrating that predictive scheduling\nsubstantially improves deterministic database scalability.", "AI": {"tldr": "ForeSight\u662f\u4e00\u79cd\u9ad8\u6027\u80fd\u786e\u5b9a\u6027\u6570\u636e\u5e93\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u51b2\u7a81\u9884\u6d4b\u548c\u667a\u80fd\u8c03\u5ea6\u89e3\u51b3\u73b0\u6709\u8bbe\u8ba1\u4e2d\u7684\u8c03\u5ea6\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u786e\u5b9a\u6027\u6570\u636e\u5e93\u8bbe\u8ba1\u672a\u80fd\u6355\u6349\u4e8b\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u8c03\u5ea6\u4e0d\u8db3\u3001\u9ad8\u4e2d\u6b62\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "\u8bbe\u8ba1\u5173\u8054\u548c\u79ef\u7f51\u7edc\u9884\u6d4b\u51b2\u7a81\u3001\u4f18\u5316\u5b58\u50a8\u5f15\u64ce\uff08\u591a\u7248\u672c\u4f18\u5316\uff09\u3001\u63d0\u51fa\u77e9\u9635\u4e24\u904d\u524d\u5411\u626b\u63cf\u7b97\u6cd5\u751f\u6210\u51b2\u7a81\u611f\u77e5\u8c03\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cForeSight\u5728\u503e\u659c\u8d1f\u8f7d\u4e0b\u541e\u5410\u91cf\u63d0\u53472\u500d\uff0c\u5e76\u5728\u9ad8\u7ade\u4e89\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u9884\u6d4b\u6027\u8c03\u5ea6\u663e\u8457\u63d0\u5347\u4e86\u786e\u5b9a\u6027\u6570\u636e\u5e93\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.16700", "pdf": "https://arxiv.org/pdf/2508.16700", "abs": "https://arxiv.org/abs/2508.16700", "authors": ["Deepak Kumar", "Divakar Yadav", "Yash Patel"], "title": "GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model", "categories": ["cs.AR", "cs.AI", "cs.DC", "cs.PF"], "comment": null, "summary": "We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B\n(Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines\nQwen3-32B and Yi-34B across multiple dimensions. We measure true\ntime-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency\npercentiles, peak VRAM with past key values (PKV) held, and energy via a\nconsistent nvidia-smi-based sampler. At a 2048-token context with 64-token\ndecode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than\ndense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM\nand energy per 1000 generated tokens; its TTFT is higher due to MoE routing\noverhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B\nprovides about 31.8% higher decode throughput and 25.8% lower energy per 1000\ngenerated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM.\nNormalized by active parameters, GPT-OSS-20B shows markedly stronger\nper-active-parameter efficiency (APE), underscoring MoE's deployment\nadvantages. We do not evaluate accuracy; this is a deployment-focused study. We\nrelease code and consolidated results to enable replication and extension.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5355GPU\uff08H100\uff0cbf16\uff09\u8bc4\u4f30\u4e86GPT-OSS-20B\uff08\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff09\u76f8\u6bd4\u5bc6\u96c6\u6a21\u578bQwen3-32B\u548cYi-34B\u5728\u591a\u4e2a\u7ef4\u5ea6\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u4e0e\u5bc6\u96c6\u6a21\u578b\u5728\u90e8\u7f72\u6548\u7387\u4e0a\u7684\u5dee\u5f02\uff0c\u91cd\u70b9\u5173\u6ce8\u63a8\u7406\u901f\u5ea6\u3001\u80fd\u6e90\u6d88\u8017\u548c\u663e\u5b58\u4f7f\u7528\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u9996\u8bcd\u751f\u6210\u65f6\u95f4\u3001\u89e3\u7801\u541e\u5410\u91cf\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\u767e\u5206\u4f4d\u6570\u3001\u5cf0\u503c\u663e\u5b58\u5360\u7528\u4ee5\u53ca\u80fd\u6e90\u6d88\u8017\uff0c\u8bc4\u4f30GPT-OSS-20B\u7684\u6027\u80fd\u3002", "result": "GPT-OSS-20B\u5728\u89e3\u7801\u541e\u5410\u91cf\u548c\u6bcf\u7126\u8033\u80fd\u8017\u4e0a\u4f18\u4e8e\u5bc6\u96c6\u6a21\u578b\uff0c\u540c\u65f6\u663e\u5b58\u5360\u7528\u548c\u6bcf\u5343\u8bcd\u80fd\u6e90\u6d88\u8017\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5728\u90e8\u7f72\u6548\u7387\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u6bcf\u6d3b\u8dc3\u53c2\u6570\u6548\u7387\u66f4\u9ad8\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.16581", "pdf": "https://arxiv.org/pdf/2508.16581", "abs": "https://arxiv.org/abs/2508.16581", "authors": ["Micha\u0142 Patryk Miazga", "Patrick Ebel"], "title": "Increasing Interaction Fidelity: Training Routines for Biomechanical Models in HCI", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Biomechanical forward simulation holds great potential for HCI, enabling the\ngeneration of human-like movements in interactive tasks. However, training\nbiomechanical models with reinforcement learning is challenging, particularly\nfor precise and dexterous movements like those required for touchscreen\ninteractions on mobile devices. Current approaches are limited in their\ninteraction fidelity, require restricting the underlying biomechanical model to\nreduce complexity, and do not generalize well. In this work, we propose\npractical improvements to training routines that reduce training time, increase\ninteraction fidelity beyond existing methods, and enable the use of more\ncomplex biomechanical models. Using a touchscreen pointing task, we demonstrate\nthat curriculum learning, action masking, more complex network configurations,\nand simple adjustments to the simulation environment can significantly improve\nthe agent's ability to learn accurate touch behavior. Our work provides HCI\nresearchers with practical tips and training routines for developing better\nbiomechanical models of human-like interaction fidelity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u63d0\u5347\u751f\u7269\u529b\u5b66\u6a21\u578b\u7684\u4ea4\u4e92\u4fdd\u771f\u5ea6\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u89e6\u5c4f\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u751f\u7269\u529b\u5b66\u6a21\u578b\u5728\u4ea4\u4e92\u4fdd\u771f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u89e6\u5c4f\u4ea4\u4e92\u7b49\u7cbe\u7ec6\u52a8\u4f5c\u4e2d\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u3001\u52a8\u4f5c\u63a9\u7801\u3001\u590d\u6742\u7f51\u7edc\u914d\u7f6e\u53ca\u73af\u5883\u8c03\u6574\u7b49\u65b9\u6cd5\u4f18\u5316\u8bad\u7ec3\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5b66\u4e60\u89e6\u5c4f\u884c\u4e3a\u7684\u51c6\u786e\u6027\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u3002", "conclusion": "\u4e3aHCI\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u63d0\u5347\u751f\u7269\u529b\u5b66\u6a21\u578b\u4ea4\u4e92\u4fdd\u771f\u5ea6\u7684\u5b9e\u7528\u5efa\u8bae\u548c\u8bad\u7ec3\u65b9\u6848\u3002"}}
{"id": "2508.17896", "pdf": "https://arxiv.org/pdf/2508.17896", "abs": "https://arxiv.org/abs/2508.17896", "authors": ["Alessia Ciacco", "Francesca Guerriero", "Eneko Osaba"], "title": "Quantum Optimization for the Steiner Traveling Salesman Problem with Time Windows and Pickup and Delivery", "categories": ["cs.ET"], "comment": "25 pages, 7 figures, and 12 tables", "summary": "We present the Steiner Traveling Salesman Problem with Time Windows and\nPickup and Delivery, an advanced and practical extension of classical routing\nmodels. This variant integrates the characteristics of the Steiner Traveling\nSalesman Problem with time-window constraints, pickup and delivery operations\nand vehicle capacity limitations. These features closely mirror the\ncomplexities of contemporary logistics challenges, including last-mile\ndistribution, reverse logistics and on-demand service scenarios. To tackle the\ninherent computational difficulties of this NP-hard problem, we propose two\nspecialized mathematical formulations: an arc-based model and a node-oriented\nmodel, each designed to capture distinct structural aspects of the problem.\nBoth models are implemented on D-Wave's LeapCQMHybrid platform, which combines\nquantum and classical techniques for solving constrained optimization tasks. We\nfurther introduce a preprocessing reduction method that eliminates redundant\narcs, significantly enhancing computational performance and scalability.\nExperimental results demonstrate that hybrid quantum approaches are capable of\nsolving problem instances of realistic size, underscoring their potential as a\ntransformative tool for next-generation routing optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7ed3\u5408\u65f6\u95f4\u7a97\u3001\u53d6\u9001\u8d27\u548c\u8f66\u8f86\u5bb9\u91cf\u9650\u5236\u7684Steiner\u65c5\u884c\u5546\u95ee\u9898\u6269\u5c55\u6a21\u578b\uff0c\u5e76\u91c7\u7528D-Wave\u7684\u6df7\u5408\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\u8fdb\u884c\u6c42\u89e3\u3002", "motivation": "\u89e3\u51b3\u73b0\u4ee3\u7269\u6d41\u4e2d\u7684\u590d\u6742\u8def\u7531\u95ee\u9898\uff0c\u5982\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u548c\u9006\u5411\u7269\u6d41\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f27\u548c\u57fa\u4e8e\u8282\u70b9\u7684\u4e24\u79cd\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u5229\u7528D-Wave\u7684\u6df7\u5408\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\u5b9e\u73b0\uff0c\u540c\u65f6\u5f15\u5165\u9884\u5904\u7406\u65b9\u6cd5\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u6df7\u5408\u91cf\u5b50\u65b9\u6cd5\u80fd\u6709\u6548\u6c42\u89e3\u5b9e\u9645\u95ee\u9898\u89c4\u6a21\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u4e0b\u4e00\u4ee3\u8def\u7531\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u590d\u6742\u7269\u6d41\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u9014\u5f84\uff0c\u6df7\u5408\u91cf\u5b50\u8ba1\u7b97\u5c55\u73b0\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.16639", "pdf": "https://arxiv.org/pdf/2508.16639", "abs": "https://arxiv.org/abs/2508.16639", "authors": ["Louie Sinadjan"], "title": "GPU Acceleration for Faster Evolutionary Spatial Cyclic Game Systems", "categories": ["cs.DC"], "comment": null, "summary": "This dissertation presents the design, implementation and evaluation of\nGPU-accelerated simulation frameworks for Evolutionary Spatial Cyclic Games\n(ESCGs), a class of agent-based models used to study ecological and\nevolutionary dynamics. Traditional single-threaded ESCG simulations are\ncomputationally expensive and scale poorly. To address this, high-performance\nimplementations were developed using Apple's Metal and Nvidia's CUDA, with a\nvalidated single-threaded C++ version serving as a baseline comparison point.\n  Benchmarking results show that GPU acceleration delivers significant\nspeedups, with the CUDA maxStep implementation achieving up to a 28x\nimprovement. Larger system sizes, up to 3200x3200, became tractable, while\nMetal faced scalability limits. The GPU frameworks also enabled replication and\ncritical extension of recent ESCG studies, revealing sensitivities to system\nsize and runtime not fully explored in prior work.\n  Overall, this project provides a configurable ESCG simulation platform that\nadvances the computational toolkit for this field of research. This\ndissertation forms the basis for a paper accepted for publication and\npresentation at the European Modelling and Simulation Symposium.", "AI": {"tldr": "\u6458\u8981\u4ecb\u7ecd\u4e86GPU\u52a0\u901f\u7684\u8fdb\u5316\u7a7a\u95f4\u5faa\u73af\u6e38\u620f\uff08ESCGs\uff09\u6a21\u62df\u6846\u67b6\u7684\u8bbe\u8ba1\u3001\u5b9e\u73b0\u548c\u8bc4\u4f30\uff0c\u4e0e\u4f20\u7edf\u5355\u7ebf\u7a0b\u65b9\u6cd5\u76f8\u6bd4\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5355\u7ebf\u7a0bESCG\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u9700\u8981\u9ad8\u6027\u80fd\u5b9e\u73b0\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u4f7f\u7528Apple\u7684Metal\u548cNvidia\u7684CUDA\u5f00\u53d1\u4e86\u9ad8\u6027\u80fd\u5b9e\u73b0\uff0c\u5e76\u4ee5\u9a8c\u8bc1\u7684\u5355\u7ebf\u7a0bC++\u7248\u672c\u4e3a\u57fa\u51c6\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GPU\u52a0\u901f\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff08CUDA\u5b9e\u73b0\u6700\u9ad828\u500d\uff09\uff0c\u5e76\u652f\u6301\u66f4\u5927\u89c4\u6a21\u7cfb\u7edf\uff083200x3200\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aESCG\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u914d\u7f6e\u7684\u6a21\u62df\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8ba1\u7b97\u5de5\u5177\u53d1\u5c55\u3002"}}
{"id": "2508.16713", "pdf": "https://arxiv.org/pdf/2508.16713", "abs": "https://arxiv.org/abs/2508.16713", "authors": ["Mohammad Atif", "Kriti Chopra", "Ozgur Kilic", "Tianle Wang", "Zhihua Dong", "Charles Leggett", "Meifeng Lin", "Paolo Calafiura", "Salman Habib"], "title": "CelloAI: Leveraging Large Language Models for HPC Software Development in High Energy Physics", "categories": ["cs.SE", "cs.AI", "hep-ex"], "comment": "12 pages, 2 figures", "summary": "Next-generation High Energy Physics (HEP) experiments will generate\nunprecedented data volumes, necessitating High Performance Computing (HPC)\nintegration alongside traditional high-throughput computing. However, HPC\nadoption in HEP is hindered by the challenge of porting legacy software to\nheterogeneous architectures and the sparse documentation of these complex\nscientific codebases. We present CelloAI, a locally hosted coding assistant\nthat leverages Large Language Models (LLMs) with retrieval-augmented generation\n(RAG) to support HEP code documentation and generation. This local deployment\nensures data privacy, eliminates recurring costs and provides access to large\ncontext windows without external dependencies. CelloAI addresses two primary\nuse cases, code documentation and code generation, through specialized\ncomponents. For code documentation, the assistant provides: (a) Doxygen style\ncomment generation for all functions and classes by retrieving relevant\ninformation from RAG sources (papers, posters, presentations), (b) file-level\nsummary generation, and (c) an interactive chatbot for code comprehension\nqueries. For code generation, CelloAI employs syntax-aware chunking strategies\nthat preserve syntactic boundaries during embedding, improving retrieval\naccuracy in large codebases. The system integrates callgraph knowledge to\nmaintain dependency awareness during code modifications and provides\nAI-generated suggestions for performance optimization and accurate refactoring.\nWe evaluate CelloAI using real-world HEP applications from ATLAS, CMS, and DUNE\nexperiments, comparing different embedding models for code retrieval\neffectiveness. Our results demonstrate the AI assistant's capability to enhance\ncode understanding and support reliable code generation while maintaining the\ntransparency and safety requirements essential for scientific computing\nenvironments.", "AI": {"tldr": "CelloAI\u662f\u4e00\u4e2a\u672c\u5730\u90e8\u7f72\u7684\u7f16\u7801\u52a9\u624b\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u652f\u6301\u9ad8\u80fd\u7269\u7406\uff08HEP\uff09\u4ee3\u7801\u7684\u6587\u6863\u5316\u548c\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u9ad8\u80fd\u7269\u7406\u5b9e\u9a8c\u5c06\u751f\u6210\u5927\u91cf\u6570\u636e\uff0c\u9700\u8981\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u652f\u6301\u3002\u4f46HEP\u9886\u57df\u91c7\u7528HPC\u7684\u6311\u6218\u5728\u4e8e\u5c06\u9057\u7559\u8f6f\u4ef6\u79fb\u690d\u5230\u5f02\u6784\u67b6\u6784\u4ee5\u53ca\u590d\u6742\u4ee3\u7801\u5e93\u7684\u6587\u6863\u4e0d\u8db3\u3002", "method": "CelloAI\u901a\u8fc7RAG\u6280\u672f\uff0c\u63d0\u4f9b\u4ee3\u7801\u6587\u6863\u5316\uff08\u5982Doxygen\u98ce\u683c\u6ce8\u91ca\u751f\u6210\u3001\u6587\u4ef6\u7ea7\u6458\u8981\uff09\u548c\u4ee3\u7801\u751f\u6210\u529f\u80fd\uff0c\u91c7\u7528\u8bed\u6cd5\u611f\u77e5\u7684\u5206\u5757\u7b56\u7565\u548c\u8c03\u7528\u56fe\u77e5\u8bc6\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "result": "\u5728ATLAS\u3001CMS\u548cDUNE\u5b9e\u9a8c\u4e2d\u8bc4\u4f30\u8868\u660e\uff0cCelloAI\u80fd\u6709\u6548\u63d0\u5347\u4ee3\u7801\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u6ee1\u8db3\u79d1\u5b66\u8ba1\u7b97\u73af\u5883\u7684\u900f\u660e\u6027\u548c\u5b89\u5168\u6027\u9700\u6c42\u3002", "conclusion": "CelloAI\u4e3a\u9ad8\u80fd\u7269\u7406\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u5b89\u5168\u7684\u4ee3\u7801\u8f85\u52a9\u5de5\u5177\uff0c\u89e3\u51b3\u4e86HPC\u96c6\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2508.17651", "pdf": "https://arxiv.org/pdf/2508.17651", "abs": "https://arxiv.org/abs/2508.17651", "authors": ["Siddique Abubakr Muntaka", "Jacques Bou Abdo"], "title": "Optimizing Anonymity and Efficiency: A Critical Review of Path Selection Strategies in Tor", "categories": ["cs.NI"], "comment": null, "summary": "The Onion Router (Tor) relies on path selection algorithms to balance\nperformance and anonymity by determining how traffic flows through its relay\nnetwork. As Tor scales and usage patterns evolve, default strategies such as\nbandwidth-weighted random selection and persistent guard nodes face increasing\nperformance limitations. This study presents a comparative evaluation of five\npath selection strategies: Random, Guard, Congestion-Aware, and two Geographic\napproaches (Diversity Driven and Latency-Optimized) using a high-fidelity\nsimulation model inspired by TorPS (Tor Path Simulator). Experiments were\nconducted across five network scales, simulating 37,500 circuits under\nrealistic relay conditions. Results show that Geographic (Latency-Optimized)\nconsistently achieved the lowest latency (40.0 ms) and highest efficiency,\nwhile Congestion-Aware strategies delivered the best throughput, outperforming\nthe baseline by up to 42%. Guard nodes maintained stable routing but exhibited\nlatency increases under larger networks. No single method proved optimal across\nall scenarios, but each revealed clear strengths for specific use cases. These\nfindings demonstrate that targeted path selection can significantly improve\nTor's performance without compromising anonymity, providing guidance for\noptimizing circuit construction in future development and deployments.", "AI": {"tldr": "\u6bd4\u8f83\u4e94\u79cdTor\u8def\u5f84\u9009\u62e9\u7b56\u7565\u7684\u6027\u80fd\uff0c\u5730\u7406\uff08\u5ef6\u8fdf\u4f18\u5316\uff09\u65b9\u6cd5\u5728\u5ef6\u8fdf\u548c\u6548\u7387\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u62e5\u585e\u611f\u77e5\u7b56\u7565\u5728\u541e\u5410\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf42%\u3002\u4e0d\u540c\u7b56\u7565\u9002\u5408\u4e0d\u540c\u573a\u666f\u3002", "motivation": "\u968f\u7740Tor\u89c4\u6a21\u548c\u7528\u6237\u6a21\u5f0f\u7684\u53d8\u5316\uff0c\u9ed8\u8ba4\u8def\u5f84\u9009\u62e9\u7b56\u7565\u9762\u4e34\u6027\u80fd\u9650\u5236\uff0c\u9700\u8bc4\u4f30\u66ff\u4ee3\u7b56\u7565\u4ee5\u4f18\u5316\u6027\u80fd\u4e14\u4e0d\u635f\u5bb3\u533f\u540d\u6027\u3002", "method": "\u4f7f\u7528\u9ad8\u4fdd\u771f\u6a21\u62df\u6a21\u578bTorPS\uff0c\u8bc4\u4f30\u4e94\u79cd\u7b56\u7565\uff08\u968f\u673a\u3001Guard\u3001\u62e5\u585e\u611f\u77e5\u3001\u5730\u7406\u591a\u6837\u6027\u9a71\u52a8\u548c\u5ef6\u8fdf\u4f18\u5316\uff09\u572837500\u4e2a\u7535\u8def\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5730\u7406\uff08\u5ef6\u8fdf\u4f18\u5316\uff09\u65b9\u6cd5\u5ef6\u8fdf\u6700\u4f4e\uff0840ms\uff09\uff0c\u62e5\u585e\u611f\u77e5\u7b56\u7565\u541e\u5410\u91cf\u6700\u9ad8\uff0c\u6bd4\u57fa\u7ebf\u9ad842%\u3002", "conclusion": "\u9488\u5bf9\u6027\u8def\u5f84\u9009\u62e9\u53ef\u663e\u8457\u63d0\u5347Tor\u6027\u80fd\u4e14\u4e0d\u635f\u5bb3\u533f\u540d\u6027\uff0c\u4e3a\u672a\u6765\u4f18\u5316\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2508.17011", "pdf": "https://arxiv.org/pdf/2508.17011", "abs": "https://arxiv.org/abs/2508.17011", "authors": ["Jinxi Wang", "Ben Fei", "Dasith de Silva Edirimuni", "Zheng Liu", "Ying He", "Xuequan Lu"], "title": "A Survey of Deep Learning-based Point Cloud Denoising", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate 3D geometry acquisition is essential for a wide range of\napplications, such as computer graphics, autonomous driving, robotics, and\naugmented reality. However, raw point clouds acquired in real-world\nenvironments are often corrupted with noise due to various factors such as\nsensor, lighting, material, environment etc, which reduces geometric fidelity\nand degrades downstream performance. Point cloud denoising is a fundamental\nproblem, aiming to recover clean point sets while preserving underlying\nstructures. Classical optimization-based methods, guided by hand-crafted\nfilters or geometric priors, have been extensively studied but struggle to\nhandle diverse and complex noise patterns. Recent deep learning approaches\nleverage neural network architectures to learn distinctive representations and\ndemonstrate strong outcomes, particularly on complex and large-scale point\nclouds. Provided these significant advances, this survey provides a\ncomprehensive and up-to-date review of deep learning-based point cloud\ndenoising methods up to August 2025. We organize the literature from two\nperspectives: (1) supervision level (supervised vs. unsupervised), and (2)\nmodeling perspective, proposing a functional taxonomy that unifies diverse\napproaches by their denoising principles. We further analyze architectural\ntrends both structurally and chronologically, establish a unified benchmark\nwith consistent training settings, and evaluate methods in terms of denoising\nquality, surface fidelity, point distribution, and computational efficiency.\nFinally, we discuss open challenges and outline directions for future research\nin this rapidly evolving field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u622a\u81f32025\u5e748\u6708\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u70b9\u4e91\u53bb\u566a\u65b9\u6cd5\uff0c\u4ece\u76d1\u7763\u7ea7\u522b\u548c\u5efa\u6a21\u89d2\u5ea6\u5bf9\u6587\u732e\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u5206\u6790\u4e86\u7ed3\u6784\u8d8b\u52bf\uff0c\u8bc4\u4f30\u4e86\u65b9\u6cd5\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u771f\u5b9e\u73af\u5883\u4e2d\u83b7\u53d6\u7684\u70b9\u4e91\u5e38\u53d7\u566a\u58f0\u5f71\u54cd\uff0c\u964d\u4f4e\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u6027\u80fd\uff0c\u70b9\u4e91\u53bb\u566a\u6210\u4e3a\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u566a\u58f0\u6a21\u5f0f\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u663e\u793a\u51fa\u4f18\u52bf\u3002", "method": "\u4ece\u76d1\u7763\u7ea7\u522b\uff08\u6709\u76d1\u7763vs\u65e0\u76d1\u7763\uff09\u548c\u5efa\u6a21\u89d2\u5ea6\uff08\u529f\u80fd\u5206\u7c7b\uff09\u5bf9\u6587\u732e\u8fdb\u884c\u7ec4\u7ec7\uff0c\u63d0\u51fa\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u53bb\u566a\u8d28\u91cf\u3001\u8868\u9762\u4fdd\u771f\u5ea6\u3001\u70b9\u5206\u5e03\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u67b6\u6784\u8d8b\u52bf\u3002", "conclusion": "\u603b\u7ed3\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u70b9\u4e91\u53bb\u566a\u4e2d\u7684\u8fdb\u5c55\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u548c\u6311\u6218\u3002"}}
{"id": "2508.17190", "pdf": "https://arxiv.org/pdf/2508.17190", "abs": "https://arxiv.org/abs/2508.17190", "authors": ["Bonan Su", "Li Zhou", "Yuan Feng", "Mingsheng Ying"], "title": "Borrowing Dirty Qubits in Quantum Programs", "categories": ["quant-ph", "cs.PL"], "comment": null, "summary": "Dirty qubits are ancillary qubits that can be borrowed from idle parts of a\ncomputation, enabling qubit reuse and reducing the demand for fresh, clean\nqubits-a resource that is typically scarce in practice. For such reuse to be\nvalid, the initial states of the dirty qubits must not affect the functionality\nof the quantum circuits in which they are employed. Moreover, their original\nstates, including any entanglement they possess, must be fully restored after\nuse-a requirement commonly known as safe uncomputation. In this paper, we\nformally define the semantics of dirty-qubit borrowing as a feature in quantum\nprogramming languages, and introduce a notion of safe uncomputation for dirty\nqubits in quantum programs. We also present an efficient algorithm, along with\nexperimental results, for verifying safe uncomputation of dirty qubits in\ncertain quantum circuits.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u810f\u91cf\u5b50\u6bd4\u7279\u7684\u501f\u7528\u53ca\u5176\u5728\u91cf\u5b50\u7f16\u7a0b\u4e2d\u7684\u8bed\u4e49\u5b9a\u4e49\uff0c\u63d0\u51fa\u4e86\u810f\u91cf\u5b50\u6bd4\u7279\u7684\u5b89\u5168\u53cd\u8ba1\u7b97\u6982\u5ff5\uff0c\u5e76\u7ed9\u51fa\u4e86\u9a8c\u8bc1\u7b97\u6cd5\u548c\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u4e2d\u5e72\u51c0\u91cf\u5b50\u6bd4\u7279\u8d44\u6e90\u7a00\u7f3a\uff0c\u901a\u8fc7\u501f\u7528\u7a7a\u95f2\u810f\u91cf\u5b50\u6bd4\u7279\u5b9e\u73b0\u590d\u7528\uff0c\u51cf\u5c11\u5bf9\u5e72\u51c0\u91cf\u5b50\u6bd4\u7279\u7684\u9700\u6c42\u3002", "method": "\u5f62\u5f0f\u5316\u5b9a\u4e49\u810f\u91cf\u5b50\u6bd4\u7279\u501f\u7528\u8bed\u4e49\uff0c\u63d0\u51fa\u5b89\u5168\u53cd\u8ba1\u7b97\u6982\u5ff5\uff0c\u5e76\u8bbe\u8ba1\u9ad8\u6548\u9a8c\u8bc1\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u810f\u91cf\u5b50\u6bd4\u7279\u5b89\u5168\u53cd\u8ba1\u7b97\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u810f\u91cf\u5b50\u6bd4\u7279\u7684\u501f\u7528\u548c\u5b89\u5168\u53cd\u8ba1\u7b97\u4e3a\u91cf\u5b50\u7f16\u7a0b\u63d0\u4f9b\u4e86\u8d44\u6e90\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2508.17859", "pdf": "https://arxiv.org/pdf/2508.17859", "abs": "https://arxiv.org/abs/2508.17859", "authors": ["Christel Baier", "Calvin Chau", "Volodymyr Drobitko", "Simon Jantsch", "Sascha Kl\u00fcppelholz"], "title": "Certificates and Witnesses for Multi-objective \u03c9-regular Queries in Markov Decision Processes", "categories": ["cs.LO"], "comment": "This preprint has not undergone peer review (when applicable) or any\n  post-submission improvements or corrections. To appear at SEFM 2025", "summary": "Multi-objective probabilistic model checking is a powerful technique for\nverifying stochastic systems against multiple (potentially conflicting)\nproperties. To enhance the trustworthiness and explainability of model checking\ntools, we present independently checkable certificates and witnesses for\nmulti-objective {\\omega}-regular queries in Markov decision processes. For the\ncertification, we extend and improve existing certificates for the\ndecomposition of maximal end components and reachability properties. We then\nderive mixed-integer linear programs (MILPs) for finding minimal witnessing\nsubsystems. For the special case of Markov chains and LTL properties, we use\nunambiguous B\\\"uchi automata to find witnesses, resulting in an algorithm that\nrequires single-exponential space. Existing approaches based on deterministic\nautomata require doubly-exponential space in the worst case. Finally, we\nconsider the practical computation of our certificates and witnesses and\nprovide an implementation of the developed techniques, along with an\nexperimental evaluation, demonstrating the efficacy of our techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u76ee\u6807\u6982\u7387\u6a21\u578b\u68c0\u67e5\u7684\u53ef\u9a8c\u8bc1\u8bc1\u4e66\u548c\u89c1\u8bc1\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5de5\u5177\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u6a21\u578b\u68c0\u67e5\u5de5\u5177\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u7814\u7a76\u9488\u5bf9\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u591a\u76ee\u6807\u03c9-\u6b63\u5219\u67e5\u8be2\u63d0\u51fa\u4e86\u72ec\u7acb\u53ef\u9a8c\u8bc1\u7684\u8bc1\u4e66\u548c\u89c1\u8bc1\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u6269\u5c55\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u8bc1\u4e66\u65b9\u6cd5\uff0c\u63a8\u5bfc\u4e86\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u4ee5\u627e\u5230\u6700\u5c0f\u89c1\u8bc1\u5b50\u7cfb\u7edf\u3002\u5bf9\u4e8e\u9a6c\u5c14\u53ef\u592b\u94fe\u548cLTL\u5c5e\u6027\uff0c\u4f7f\u7528\u660e\u786eB\u00fcchi\u81ea\u52a8\u673a\u627e\u5230\u89c1\u8bc1\uff0c\u7b97\u6cd5\u4ec5\u9700\u5355\u6307\u6570\u7a7a\u95f4\u3002\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8e\u786e\u5b9a\u6027\u81ea\u52a8\u673a\u7684\u65b9\u6cd5\uff08\u53cc\u6307\u6570\u7a7a\u95f4\uff09\uff0c\u6548\u7387\u66f4\u9ad8\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8bc1\u660e\u4e86\u6240\u63d0\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u8ba1\u7b97\u4e2d\u5c55\u793a\u4e86\u8fd9\u4e9b\u8bc1\u4e66\u548c\u89c1\u8bc1\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8\u6a21\u578b\u68c0\u67e5\u5de5\u5177\u7684\u53ef\u4fe1\u5ea6\u548c\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.16582", "pdf": "https://arxiv.org/pdf/2508.16582", "abs": "https://arxiv.org/abs/2508.16582", "authors": ["Linghao Zeng"], "title": "Predicting User Grasp Intentions in Virtual Reality", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "45 pages, 24 figures. This is a Master's thesis submitted as part of\n  the M2 IASD (Artificial Intelligence, Systems, Data) program at Universit\\'e\n  PSL", "summary": "Predicting user intentions in virtual reality (VR) is crucial for creating\nimmersive experiences, particularly in tasks involving complex grasping motions\nwhere accurate haptic feedback is essential. In this work, we leverage\ntime-series data from hand movements to evaluate both classification and\nregression approaches across 810 trials with varied object types, sizes, and\nmanipulations. Our findings reveal that classification models struggle to\ngeneralize across users, leading to inconsistent performance. In contrast,\nregression-based approaches, particularly those using Long Short Term Memory\n(LSTM) networks, demonstrate more robust performance, with timing errors within\n0.25 seconds and distance errors around 5-20 cm in the critical two-second\nwindow before a grasp. Despite these improvements, predicting precise hand\npostures remains challenging. Through a comprehensive analysis of user\nvariability and model interpretability, we explore why certain models fail and\nhow regression models better accommodate the dynamic and complex nature of user\nbehavior in VR. Our results underscore the potential of machine learning models\nto enhance VR interactions, particularly through adaptive haptic feedback, and\nlay the groundwork for future advancements in real-time prediction of user\nactions in VR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8bc4\u4f30\u5206\u7c7b\u548c\u56de\u5f52\u65b9\u6cd5\u5728VR\u4e2d\u9884\u6d4b\u7528\u6237\u610f\u56fe\u7684\u6548\u679c\uff0c\u53d1\u73b0\u56de\u5f52\u65b9\u6cd5\uff08\u5c24\u5176\u662fLSTM\uff09\u8868\u73b0\u66f4\u7a33\u5065\uff0c\u4f46\u7cbe\u786e\u9884\u6d4b\u624b\u90e8\u59ff\u52bf\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u9884\u6d4b\u7528\u6237\u5728VR\u4e2d\u7684\u610f\u56fe\u5bf9\u4e8e\u521b\u9020\u6c89\u6d78\u5f0f\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u51c6\u786e\u89e6\u89c9\u53cd\u9988\u7684\u590d\u6742\u6293\u63e1\u4efb\u52a1\u4e2d\u3002", "method": "\u901a\u8fc7810\u6b21\u8bd5\u9a8c\uff0c\u6bd4\u8f83\u5206\u7c7b\u548c\u56de\u5f52\u65b9\u6cd5\uff08\u5982LSTM\uff09\uff0c\u5206\u6790\u624b\u90e8\u8fd0\u52a8\u6570\u636e\u3002", "result": "\u56de\u5f52\u65b9\u6cd5\u5728\u5173\u952e\u4e24\u79d2\u7a97\u53e3\u5185\u65f6\u95f4\u8bef\u5dee\u5c0f\u4e8e0.25\u79d2\uff0c\u8ddd\u79bb\u8bef\u5dee\u4e3a5-20\u5398\u7c73\uff0c\u4f46\u5206\u7c7b\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "conclusion": "\u56de\u5f52\u6a21\u578b\u66f4\u9002\u5408\u52a8\u6001\u590d\u6742\u7684\u7528\u6237\u884c\u4e3a\uff0c\u4e3a\u672a\u6765VR\u4e2d\u5b9e\u65f6\u9884\u6d4b\u548c\u81ea\u9002\u5e94\u89e6\u89c9\u53cd\u9988\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.16712", "pdf": "https://arxiv.org/pdf/2508.16712", "abs": "https://arxiv.org/abs/2508.16712", "authors": ["Tianyao Shi", "Yi Ding"], "title": "Systematic Characterization of LLM Quantization: A Performance, Energy, and Quality Perspective", "categories": ["cs.PF", "cs.AI", "cs.AR", "cs.DC", "cs.LG"], "comment": "14 pages, 10 figures, 4 tables", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, but their heavy resource demands make quantization-reducing\nprecision to lower-bit formats-critical for efficient serving. While many\nquantization methods exist, a systematic understanding of their performance,\nenergy, and quality tradeoffs in realistic serving conditions remains a gap. In\nthis work, we first develop a fully automated online characterization framework\nqMeter, and then conduct an in-depth characterization of 11 post-training LLM\nquantization methods across 4 model sizes (7B-70B) and two GPU architectures\n(A100, H100). We evaluate quantization at the application, workload,\nparallelism, and hardware levels under online serving conditions. Our study\nreveals highly task- and method-dependent tradeoffs, strong sensitivity to\nworkload characteristics, and complex interactions with parallelism and GPU\narchitecture. We further present three optimization case studies illustrating\ndeployment challenges in capacity planning, energy-efficient scheduling, and\nmulti-objective tuning. To the best of our knowledge, this is one of the first\ncomprehensive application-, system-, and hardware-level characterization of LLM\nquantization from a joint performance, energy, and quality perspective.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u91cf\u5316\u65b9\u6cd5\u5728\u6027\u80fd\u3001\u80fd\u8017\u548c\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u6846\u67b6qMeter\u8fdb\u884c\u5728\u7ebf\u8bc4\u4f30\u3002", "motivation": "\u91cf\u5316LLMs\u4ee5\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7f3a\u4e4f\u5bf9\u5176\u7efc\u5408\u8868\u73b0\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u3002", "method": "\u5f00\u53d1qMeter\u6846\u67b6\uff0c\u8bc4\u4f3011\u79cd\u91cf\u5316\u65b9\u6cd5\u57284\u79cd\u6a21\u578b\u5927\u5c0f\u548c2\u79cdGPU\u67b6\u6784\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u91cf\u5316\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u4efb\u52a1\u548c\u65b9\u6cd5\uff0c\u4e14\u53d7\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u548c\u786c\u4ef6\u67b6\u6784\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u91cf\u5316\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u591a\u5c42\u6b21\u4f18\u5316\u65b9\u5411\uff0c\u662f\u9996\u4e2a\u7efc\u5408\u8bc4\u4f30\u91cf\u5316\u6027\u80fd\u3001\u80fd\u8017\u548c\u8d28\u91cf\u7684\u5de5\u4f5c\u3002"}}
{"id": "2508.17556", "pdf": "https://arxiv.org/pdf/2508.17556", "abs": "https://arxiv.org/abs/2508.17556", "authors": ["Hanwen Liu", "Qihan Zhang", "Ryan Marcus", "Ibrahim Sabek"], "title": "SEFRQO: A Self-Evolving Fine-Tuned RAG-Based Query Optimizer", "categories": ["cs.DB"], "comment": "To appear at SIGMOD 2026 (https://2026.sigmod.org/)", "summary": "Query optimization is a crucial problem in database systems that has been\nstudied for decades. Learned query optimizers (LQOs) can improve performance\nover time by incorporating feedback; however, they suffer from cold-start\nissues and often require retraining when workloads shift or schemas change.\nRecent LLM-based query optimizers leverage pre-trained and fine-tuned LLMs to\nmitigate these challenges. Nevertheless, they neglect LLMs' in-context learning\nand execution records as feedback for continuous evolution. In this paper, we\npresent SEFRQO, a Self-Evolving Fine-tuned RAG-based Query Optimizer. SEFRQO\nmitigates the cold-start problem of LQOs by continuously learning from\nexecution feedback via a Retrieval-Augmented Generation (RAG) framework. We\nemploy both supervised fine-tuning and reinforcement fine-tuning to prepare the\nLLM to produce syntactically correct and performance-efficient query hints.\nMoreover, SEFRQO leverages the LLM's in-context learning capabilities by\ndynamically constructing prompts with references to similar queries and the\nhistorical execution record of the same query. This self-evolving paradigm\niteratively optimizes the prompt to minimize query execution latency.\nEvaluations show that SEFRQO outperforms state-of-the-art LQOs, achieving up to\n65.05% and 93.57% reductions in query latency on the CEB and Stack workloads,\nrespectively, compared to PostgreSQL.", "AI": {"tldr": "SEFRQO\u662f\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u81ea\u8fdb\u5316\u67e5\u8be2\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u6267\u884c\u53cd\u9988\u6765\u89e3\u51b3\u4f20\u7edfLQO\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u67e5\u8be2\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u578b\u67e5\u8be2\u4f18\u5316\u5668\uff08LQO\uff09\u5b58\u5728\u51b7\u542f\u52a8\u95ee\u9898\u548c\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u7684\u7f3a\u70b9\uff0c\u800c\u57fa\u4e8eLLM\u7684\u4f18\u5316\u5668\u672a\u5145\u5206\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6267\u884c\u53cd\u9988\u3002SEFRQO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SEFRQO\u7ed3\u5408\u4e86\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5fae\u8c03\uff0c\u5229\u7528RAG\u6846\u67b6\u52a8\u6001\u6784\u5efa\u63d0\u793a\uff0c\u5e76\u53c2\u8003\u5386\u53f2\u6267\u884c\u8bb0\u5f55\u548c\u76f8\u4f3c\u67e5\u8be2\uff0c\u5b9e\u73b0\u6301\u7eed\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSEFRQO\u5728CEB\u548cStack\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5206\u522b\u6bd4PostgreSQL\u964d\u4f4e\u4e8665.05%\u548c93.57%\u7684\u67e5\u8be2\u5ef6\u8fdf\u3002", "conclusion": "SEFRQO\u901a\u8fc7\u81ea\u8fdb\u5316\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u4f18\u5316\u6027\u80fd\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u7684LQO\u65b9\u6cd5\u3002"}}
{"id": "2508.16738", "pdf": "https://arxiv.org/pdf/2508.16738", "abs": "https://arxiv.org/abs/2508.16738", "authors": ["Alhad Daftardar", "Jianqiao Mo", "Joey Ah-kiow", "Benedikt B\u00fcnz", "Siddharth Garg", "Brandon Reagen"], "title": "zkPHIRE: A Programmable Accelerator for ZKPs over HIgh-degRee, Expressive Gates", "categories": ["cs.AR", "cs.CR"], "comment": "14 pages, 14 figures", "summary": "Zero-Knowledge Proofs (ZKPs) have emerged as powerful tools for secure and\nprivacy-preserving computation. ZKPs enable one party to convince another of a\nstatement's validity without revealing anything else. This capability has\nprofound implications in many domains, including: machine learning, blockchain,\nimage authentication, and electronic voting. Despite their potential, ZKPs have\nseen limited deployment because of their exceptionally high computational\noverhead, which manifests primarily during proof generation. To mitigate these\noverheads, a (growing) body of researchers has proposed hardware accelerators\nand GPU implementations for kernels and complete protocols. Prior art spans a\nwide variety of ZKP schemes that vary significantly in computational overhead,\nproof size, verifier cost, protocol setup, and trust. The latest, and widely\nused ZKP protocols are intentionally designed to balance these trade-offs. A\nparticular challenge in modern ZKP systems is supporting complex, high-degree\ngates using the SumCheck protocol. We address this challenge with a novel\nprogrammable accelerator that efficiently handles arbitrary custom gates via\nSumCheck. Our accelerator achieves upwards of $1000\\times$ geomean speedup over\nCPU-based SumChecks across a range of gate types. We integrate this unit into a\nfull-system accelerator, zkPHIRE, which achieves $1486\\times$ geomean speedup\nover CPU and $11.87\\times$ speedup over the state-of-the-art at iso-area.\nzkPHIRE is the first accelerator to scale to problem sizes of $2^{30}$ nominal\nconstraints while maintaining small proof sizes and programmability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3azkPHIRE\u7684\u65b0\u578b\u53ef\u7f16\u7a0b\u52a0\u901f\u5668\uff0c\u901a\u8fc7SumCheck\u534f\u8bae\u9ad8\u6548\u5904\u7406\u590d\u6742\u9ad8\u95e8\u7535\u8def\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08ZKPs\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u8ba1\u7b97\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u56e0\u5176\u9ad8\u8ba1\u7b97\u5f00\u9500\u800c\u96be\u4ee5\u5e7f\u6cdb\u5e94\u7528\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u73b0\u4ee3ZKP\u7cfb\u7edf\u4e2d\u590d\u6742\u9ad8\u95e8\u7535\u8def\u7684\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u53ef\u7f16\u7a0b\u52a0\u901f\u5668\uff0c\u5229\u7528SumCheck\u534f\u8bae\u9ad8\u6548\u5904\u7406\u4efb\u610f\u81ea\u5b9a\u4e49\u95e8\u7535\u8def\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5168\u7cfb\u7edf\u52a0\u901f\u5668zkPHIRE\u4e2d\u3002", "result": "zkPHIRE\u5728\u76f8\u540c\u9762\u79ef\u4e0b\u5b9e\u73b0\u4e861486\u500d\u4e8eCPU\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u6bd4\uff0c\u5e76\u9996\u6b21\u652f\u63012^30\u89c4\u6a21\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5c0f\u8bc1\u660e\u5927\u5c0f\u548c\u53ef\u7f16\u7a0b\u6027\u3002", "conclusion": "zkPHIRE\u901a\u8fc7\u9ad8\u6548\u5904\u7406\u590d\u6742\u95e8\u7535\u8def\uff0c\u663e\u8457\u63d0\u5347\u4e86ZKP\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2508.18250", "pdf": "https://arxiv.org/pdf/2508.18250", "abs": "https://arxiv.org/abs/2508.18250", "authors": ["Yang Xiang", "Fernando Garc\u00eda-Redondo", "Arvind Sharma", "Van Dai Nguyen", "Andrea Fantini", "Philippe Matagne", "Siddharth Rao", "Subhali Subhechha", "Lynn Verschueren", "Mohammed Aftab Baig", "Marie Garcia Bardon", "Geert Hellings"], "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study", "categories": ["cs.ET"], "comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)", "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86SOT-MRAM\u5728\u5f02\u6784\u7cfb\u7edf\u6269\u5c55\u8303\u5f0f\u4e0b\u7528\u4e8e\u672b\u7ea7\u7f13\u5b58\uff08LLCs\uff09\u7684\u8de8\u8282\u70b9\u6269\u5c55\u6f5c\u529b\uff0c\u901a\u8fc7BEOL RSs\u63d0\u51fa\u4e86\u4e00\u79cd\u51cf\u5c11bitcell\u9762\u79ef\u7684\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u5176\u8bfb\u5199\u6027\u80fd\u7684\u6743\u8861\u3002", "motivation": "\u7814\u7a76SOT-MRAM\u5728LLCs\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5f02\u6784\u7cfb\u7edf\u6269\u5c55\u4e2d\uff0c\u901a\u8fc7\u4f18\u5316bitcell\u914d\u7f6e\u4ee5\u63d0\u5347\u6027\u80fd\u548c\u529f\u8017\u6548\u7387\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1-\u6280\u672f\u534f\u540c\u4f18\u5316\uff08DTCO\uff09\u65b9\u6cd5\uff0c\u8bc4\u4f307\u7eb3\u7c73\u6280\u672f\u4e0b\u7684bitcell\u914d\u7f6e\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528BEOL RSs\u4ee5\u51cf\u5c11\u9762\u79ef\u5e76\u4f18\u5316\u6027\u80fd\u3002", "result": "BEOL RSs\u53ef\u51cf\u5c1110-40%\u7684bitcell\u9762\u79ef\uff0c\u4f46\u5e26\u67653-5 ns\u7684\u8bfb\u53d6\u5ef6\u8fdf\u589e\u52a0\u548c2.5-5\u500d\u7684\u80fd\u8017\u6210\u672c\u3002", "conclusion": "BEOL RSs\u5728SOT-MRAM\u7684\u529f\u7387-\u6027\u80fd-\u9762\u79ef\u6269\u5c55\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u8bfb\u53d6\u6027\u80fd\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2508.16646", "pdf": "https://arxiv.org/pdf/2508.16646", "abs": "https://arxiv.org/abs/2508.16646", "authors": ["Zhixiang Wei", "James Yen", "Jingyi Chen", "Ziyang Zhang", "Zhibai Huang", "Chen Chen", "Xingzi Yu", "Yicheng Gu", "Chenggang Wu", "Yun Wang", "Mingyuan Xia", "Jie Wu", "Hao Wang", "Zhengwei Qi"], "title": "Equinox: Holistic Fair Scheduling in Serving Large Language Models", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "We address the limitations of current LLM serving with a dual-counter\nframework separating user and operator perspectives. The User Fairness Counter\nmeasures quality of service via weighted tokens and latency; the Resource\nFairness Counter measures operational efficiency through throughput and GPU\nutilization. Since these metrics are only available post-execution, creating a\nscheduling paradox, we introduce a deterministic Mixture of Prediction Experts\n(MoPE) framework to predict user-perceived latency, output tokens, throughput,\nand GPU utilization. These predictions enable calculation of a unified Holistic\nFairness score that balances both counters through tunable parameters for\nproactive fairness-aware scheduling. We implement this in Equinox, an\nopen-source system with other optimizations like adaptive batching, and\nstall-free scheduling. Evaluations on production traces (ShareGPT, LMSYS) and\nsynthetic workloads demonstrate Equinox achieves up to $1.3\\times$ higher\nthroughput, 60\\% lower time-to-first-token latency, and 13\\% higher fairness\nversus VTC while maintaining 94\\% GPU utilization, proving fairness under\nbounded discrepancy across heterogeneous platforms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8ba1\u6570\u5668\u6846\u67b6\uff08Equinox\uff09\uff0c\u901a\u8fc7\u9884\u6d4b\u7528\u6237\u611f\u77e5\u7684\u5ef6\u8fdf\u548c\u8d44\u6e90\u6548\u7387\u6307\u6807\uff0c\u5b9e\u73b0\u516c\u5e73\u611f\u77e5\u7684\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dLLM\u670d\u52a1\u5728\u7528\u6237\u516c\u5e73\u6027\u548c\u8d44\u6e90\u516c\u5e73\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u907f\u514d\u8c03\u5ea6\u65f6\u7684\u9884\u6d4b\u5ef6\u8fdf\u3002", "method": "\u4f7f\u7528\u786e\u5b9a\u6027\u6df7\u5408\u9884\u6d4b\u4e13\u5bb6\uff08MoPE\uff09\u6846\u67b6\u9884\u6d4b\u5173\u952e\u6307\u6807\uff0c\u8ba1\u7b97\u7edf\u4e00\u7684\u5168\u516c\u5e73\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u6279\u5904\u7406\u548c\u8c03\u5ea6\u4f18\u5316\u5b9e\u73b0\u516c\u5e73\u8c03\u5ea6\u3002", "result": "Equinox\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e861.3\u500d\u541e\u5410\u91cf\u63d0\u5347\u300160%\u7684\u9996\u6807\u8bb0\u5ef6\u8fdf\u964d\u4f4e\u548c13%\u7684\u516c\u5e73\u6027\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u630194%\u7684GPU\u5229\u7528\u7387\u3002", "conclusion": "Equinox\u901a\u8fc7\u53cc\u8ba1\u6570\u5668\u6846\u67b6\u548c\u9884\u6d4b\u673a\u5236\uff0c\u5728\u5f02\u6784\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u516c\u5e73\u611f\u77e5\u8c03\u5ea6\u3002"}}
{"id": "2508.16771", "pdf": "https://arxiv.org/pdf/2508.16771", "abs": "https://arxiv.org/abs/2508.16771", "authors": ["Yifan Zhang", "Chen Huang", "Yueke Zhang", "Jiahao Zhang", "Toby Jia-Jun Li", "Collin McMillan", "Kevin Leach", "Yu Huang"], "title": "EyeMulator: Improving Code Language Models by Mimicking Human Visual Attention", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Code language models (so-called CodeLLMs) are now commonplace in software\ndevelopment. As a general rule, CodeLLMs are trained by dividing training\nexamples into input tokens and then learn importance of those tokens in a\nprocess called machine attention. Machine attention is based solely on input\ntoken salience to output token examples during training. Human software\ndevelopers are different, as humans intuitively know that some tokens are more\nsalient than others. While intuition itself is ineffable and a subject of\nphilosophy, clues about salience are present in human visual attention, since\npeople tend to look at more salient words more often. In this paper, we present\nEyeMulator, a technique for training CodeLLMs to mimic human visual attention\nwhile training for various software development tasks. We add special weights\nfor each token in each input example to the loss function used during LLM\nfine-tuning. We draw these weights from observations of human visual attention\nderived from a previously-collected publicly-available dataset of eye-tracking\nexperiments in software engineering tasks. These new weights ultimately induce\nchanges in the attention of the subject LLM during training, resulting in a\nmodel that does not need eye-tracking data during inference. Our evaluation\nshows that EyeMulator outperforms strong LLM baselines on several tasks such as\ncode translation, completion and summarization. We further show an ablation\nstudy that demonstrates the improvement is due to subject models learning to\nmimic human attention.", "AI": {"tldr": "EyeMulator\u662f\u4e00\u79cd\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u6765\u6539\u8fdb\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\uff08CodeLLMs\uff09\u8bad\u7ec3\u7684\u6280\u672f\uff0c\u5229\u7528\u4eba\u7c7b\u773c\u52a8\u6570\u636e\u4e3a\u8f93\u5165\u6807\u8bb0\u6dfb\u52a0\u6743\u91cd\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u3001\u8865\u5168\u548c\u6458\u8981\u7b49\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dCodeLLMs\u7684\u8bad\u7ec3\u4ec5\u57fa\u4e8e\u673a\u5668\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u5f00\u53d1\u8005\u5728\u5904\u7406\u4ee3\u7801\u65f6\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u7ebf\u7d22\u3002\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u7684\u89c6\u89c9\u6ce8\u610f\u529b\uff0c\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "EyeMulator\u901a\u8fc7\u5728\u635f\u5931\u51fd\u6570\u4e2d\u4e3a\u6bcf\u4e2a\u8f93\u5165\u6807\u8bb0\u6dfb\u52a0\u7279\u6b8a\u6743\u91cd\uff0c\u8fd9\u4e9b\u6743\u91cd\u6765\u6e90\u4e8e\u516c\u5f00\u7684\u773c\u52a8\u5b9e\u9a8c\u6570\u636e\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u4e2d\u6539\u53d8\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEyeMulator\u5728\u4ee3\u7801\u7ffb\u8bd1\u3001\u8865\u5168\u548c\u6458\u8981\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e14\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u6539\u8fdb\u6e90\u4e8e\u6a21\u578b\u5b66\u4e60\u4e86\u4eba\u7c7b\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\uff0cEyeMulator\u80fd\u591f\u663e\u8457\u63d0\u5347CodeLLMs\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u5728\u63a8\u7406\u9636\u6bb5\u4f9d\u8d56\u773c\u52a8\u6570\u636e\u3002"}}
{"id": "2508.17763", "pdf": "https://arxiv.org/pdf/2508.17763", "abs": "https://arxiv.org/abs/2508.17763", "authors": ["Chris Misa", "Ramakrishnan Durairajan"], "title": "Sustainability or Survivability? Eliminating the Need to Choose in LEO Satellite Constellations", "categories": ["cs.NI"], "comment": null, "summary": "LEO Satellite Networks (LSNs) are revolutionizing global connectivity, but\ntheir reliance on tens of thousands of satellites raises pressing concerns over\nsustainability and survivability. In this work, we argue that the\ninefficiencies in LSN designs stem from ignoring the strong spatiotemporal\nstructure of Internet traffic demand (which impacts sustainability) and the\nphysical realities of the near-Earth space environment (which affects\nsurvivability). We propose a novel design approach based on sun-synchronous\n(SS) orbits called SS-plane, which aligns satellite coverage with the Earth's\ndiurnal cycle. We demonstrate that SS-plane constellations can reduce the\nnumber of satellites required by up to an order of magnitude and cut radiation\nexposure by ~23% compared to traditional Walker-delta constellations. These\nfindings suggest a paradigm shift in LSN research from large, disposable\nmegaconstellations to more sustainable, targeted LEO constellations.", "AI": {"tldr": "LEO\u536b\u661f\u7f51\u7edc\u7684\u53ef\u6301\u7eed\u6027\u548c\u751f\u5b58\u6027\u95ee\u9898\u662f\u7814\u7a76\u7684\u6838\u5fc3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u592a\u9633\u540c\u6b65\u8f68\u9053\u7684SS-plane\u8bbe\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u536b\u661f\u6570\u91cf\u548c\u8f90\u5c04\u66b4\u9732\u3002", "motivation": "\u89e3\u51b3LEO\u536b\u661f\u7f51\u7edc\u56e0\u5ffd\u89c6\u4e92\u8054\u7f51\u6d41\u91cf\u9700\u6c42\u7684\u65f6\u7a7a\u7ed3\u6784\u548c\u5730\u7403\u7a7a\u95f4\u73af\u5883\u7269\u7406\u73b0\u5b9e\u800c\u5bfc\u81f4\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u592a\u9633\u540c\u6b65\u8f68\u9053\u7684SS-plane\u8bbe\u8ba1\u65b9\u6cd5\u3002", "result": "SS-plane\u8bbe\u8ba1\u53ef\u4ee5\u5c06\u536b\u661f\u6570\u91cf\u51cf\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u8f90\u5c04\u66b4\u9732\u51cf\u5c11\u7ea623%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63a8\u52a8\u4e86LEO\u536b\u661f\u7f51\u7edc\u4ece\u5927\u578b\u3001\u53ef\u4e22\u5f03\u7684\u5de8\u578b\u661f\u5ea7\u8f6c\u5411\u66f4\u53ef\u6301\u7eed\u3001\u76ee\u6807\u660e\u786e\u7684LEO\u661f\u5ea7\u3002"}}
{"id": "2508.17342", "pdf": "https://arxiv.org/pdf/2508.17342", "abs": "https://arxiv.org/abs/2508.17342", "authors": ["Hengyuan Zhang", "Zhe Li", "Xingqun Qi", "Mengze Li", "Muyi Sun", "Man Zhang", "Sirui Han"], "title": "DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions", "categories": ["cs.GR", "cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Generating coherent and diverse human dances from music signals has gained\ntremendous progress in animating virtual avatars. While existing methods\nsupport direct dance synthesis, they fail to recognize that enabling users to\nedit dance movements is far more practical in real-world choreography\nscenarios. Moreover, the lack of high-quality dance datasets incorporating\niterative editing also limits addressing this challenge. To achieve this goal,\nwe first construct DanceRemix, a large-scale multi-turn editable dance dataset\ncomprising the prompt featuring over 25.3M dance frames and 84.5K pairs. In\naddition, we propose a novel framework for iterative and editable dance\ngeneration coherently aligned with given music signals, namely DanceEditor.\nConsidering the dance motion should be both musical rhythmic and enable\niterative editing by user descriptions, our framework is built upon a\nprediction-then-editing paradigm unifying multi-modal conditions. At the\ninitial prediction stage, our framework improves the authority of generated\nresults by directly modeling dance movements from tailored, aligned music.\nMoreover, at the subsequent iterative editing stages, we incorporate text\ndescriptions as conditioning information to draw the editable results through a\nspecifically designed Cross-modality Editing Module (CEM). Specifically, CEM\nadaptively integrates the initial prediction with music and text prompts as\ntemporal motion cues to guide the synthesized sequences. Thereby, the results\ndisplay music harmonics while preserving fine-grained semantic alignment with\ntext descriptions. Extensive experiments demonstrate that our method\noutperforms the state-of-the-art models on our newly collected DanceRemix\ndataset. Code is available at https://lzvsdy.github.io/DanceEditor/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDanceEditor\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u97f3\u4e50\u4fe1\u53f7\u751f\u6210\u53ef\u7f16\u8f91\u7684\u821e\u8e48\u52a8\u4f5c\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6DanceRemix\u3002", "motivation": "\u73b0\u6709\u821e\u8e48\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u7f16\u821e\u4e2d\u7528\u6237\u7f16\u8f91\u9700\u6c42\uff0c\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u53ef\u7f16\u8f91\u821e\u8e48\u6570\u636e\u96c6\u3002", "method": "\u91c7\u7528\u9884\u6d4b-\u7f16\u8f91\u8303\u5f0f\uff0c\u7ed3\u5408\u97f3\u4e50\u4fe1\u53f7\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7f16\u8f91\u6a21\u5757\uff08CEM\uff09\u5b9e\u73b0\u8fed\u4ee3\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDanceEditor\u5728DanceRemix\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "DanceEditor\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u751f\u6210\u4e0e\u97f3\u4e50\u548c\u6587\u672c\u63cf\u8ff0\u4e00\u81f4\u7684\u53ef\u7f16\u8f91\u821e\u8e48\u52a8\u4f5c\uff0c\u63a8\u52a8\u4e86\u5b9e\u9645\u7f16\u821e\u5e94\u7528\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.17344", "pdf": "https://arxiv.org/pdf/2508.17344", "abs": "https://arxiv.org/abs/2508.17344", "authors": ["Rajrupa Chattaraj", "Sridhar Chimalakonda", "Vibhu Saujanya Sharma", "Vikrant Kaulgud"], "title": "Who Wins the Race? (R Vs Python) - An Exploratory Study on Energy Consumption of Machine Learning Algorithms", "categories": ["cs.SE", "cs.LG", "cs.PF", "cs.PL"], "comment": "18 pages including references, 5 figures", "summary": "The utilization of Machine Learning (ML) in contemporary software systems is\nextensive and continually expanding. However, its usage is energy-intensive,\ncontributing to increased carbon emissions and demanding significant resources.\nWhile numerous studies examine the performance and accuracy of ML, only a\nlimited few focus on its environmental aspects, particularly energy\nconsumption. In addition, despite emerging efforts to compare energy\nconsumption across various programming languages for specific algorithms and\ntasks, there remains a gap specifically in comparing these languages for\nML-based tasks. This paper aims to raise awareness of the energy costs\nassociated with employing different programming languages for ML model training\nand inference. Through this empirical study, we measure and compare the energy\nconsumption along with run-time performance of five regression and five\nclassification tasks implemented in Python and R, the two most popular\nprogramming languages in this context. Our study results reveal a statistically\nsignificant difference in costs between the two languages in 95% of the cases\nexamined. Furthermore, our analysis demonstrates that the choice of programming\nlanguage can influence energy efficiency significantly, up to 99.16% during\nmodel training and up to 99.8% during inferences, for a given ML task.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86Python\u548cR\u4e24\u79cd\u6d41\u884c\u7f16\u7a0b\u8bed\u8a00\u5728\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u80fd\u8017\u5dee\u5f02\uff0c\u7ed3\u679c\u8868\u660e\u8bed\u8a00\u9009\u62e9\u5bf9\u80fd\u6e90\u6548\u7387\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5e7f\u6cdb\u4f7f\u7528\u4f46\u80fd\u8017\u9ad8\uff0c\u7f3a\u4e4f\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u5728ML\u4efb\u52a1\u4e2d\u7684\u80fd\u6e90\u6d88\u8017\u6bd4\u8f83\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86Python\u548cR\u5728\u4e94\u79cd\u56de\u5f52\u548c\u4e94\u79cd\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u80fd\u8017\u548c\u8fd0\u884c\u65f6\u6027\u80fd\u3002", "result": "95%\u7684\u6848\u4f8b\u4e2d\u4e24\u79cd\u8bed\u8a00\u80fd\u8017\u5dee\u5f02\u663e\u8457\uff0c\u8bed\u8a00\u9009\u62e9\u53ef\u5f71\u54cd\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u80fd\u6e90\u6548\u7387\u9ad8\u8fbe99%\u4ee5\u4e0a\u3002", "conclusion": "\u7f16\u7a0b\u8bed\u8a00\u9009\u62e9\u5bf9\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u80fd\u6e90\u6548\u7387\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u5173\u6ce8\u73af\u5883\u6210\u672c\u3002"}}
{"id": "2508.17895", "pdf": "https://arxiv.org/pdf/2508.17895", "abs": "https://arxiv.org/abs/2508.17895", "authors": ["Lidia Losavio", "Marco Paganoni", "Carlo A. Furia"], "title": "Model-Based Testing of an Intermediate Verifier Using Executable Operational Semantics", "categories": ["cs.LO"], "comment": "In Proceedings of the 20th International Conference on integrated\n  Formal Methods (iFM), Paris, France, 17-21 November 2025", "summary": "Lightweight validation technique, such as those based on random testing, are\nsometimes practical alternatives to full formal verification -- providing\nvaluable benefits, such as finding bugs, without requiring a disproportionate\neffort. In fact, they can be useful even for fully formally verified tools, by\nexercising the parts of a complex system that go beyond the reach of formal\nmodels.\n  In this context, this paper introduces BCC: a model-based testing technique\nfor the Boogie intermediate verifier. BCC combines the formalization of a\nsmall, deterministic subset of the Boogie language with the generative\ncapabilities of the PLT Redex language engineering framework. Basically, BCC\nuses PLT Redex to generate random Boogie programs, and to execute them\naccording to a formal operational semantics; then, it runs the same programs\nthrough the Boogie verifier. Any inconsistency between the two executions (in\nPLT Redex and with Boogie) may indicate a potential bug in Boogie's\nimplementation.\n  To understand whether BCC can be useful in practice, we used it to generate\nthree million Boogie programs. These experiments found 2% of cases indicative\nof completeness failures (i.e., spurious verification failures) in Boogie's\ntoolchain. These results indicate that lightweight analysis tools, such as\nthose for model-based random testing, are also useful to test and validate\nformal verification tools such as Boogie.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aBCC\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u6d4b\u8bd5\u6280\u672f\uff0c\u7528\u4e8e\u9a8c\u8bc1Boogie\u4e2d\u95f4\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u968f\u673a\u751f\u6210Boogie\u7a0b\u5e8f\u5e76\u5bf9\u6bd4\u6267\u884c\u7ed3\u679c\u6765\u53d1\u73b0\u6f5c\u5728bug\u3002", "motivation": "\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u6280\u672f\uff08\u5982\u968f\u673a\u6d4b\u8bd5\uff09\u53ef\u4ee5\u4f5c\u4e3a\u6b63\u5f0f\u9a8c\u8bc1\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u8d85\u51fa\u5f62\u5f0f\u6a21\u578b\u8303\u56f4\u7684\u90e8\u5206\u3002", "method": "BCC\u7ed3\u5408\u4e86Boogie\u8bed\u8a00\u7684\u786e\u5b9a\u6027\u5b50\u96c6\u5f62\u5f0f\u5316\u548cPLT Redex\u7684\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u751f\u6210\u968f\u673a\u7a0b\u5e8f\u5e76\u5bf9\u6bd4\u6267\u884c\u7ed3\u679c\u6765\u68c0\u6d4b\u4e0d\u4e00\u81f4\u3002", "result": "\u5b9e\u9a8c\u751f\u6210\u4e86300\u4e07Boogie\u7a0b\u5e8f\uff0c\u53d1\u73b0\u4e862%\u7684\u5b8c\u5907\u6027\u5931\u8d25\u6848\u4f8b\uff0c\u8868\u660eBCC\u80fd\u6709\u6548\u68c0\u6d4bBoogie\u5de5\u5177\u94fe\u4e2d\u7684\u6f5c\u5728\u95ee\u9898\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u5206\u6790\u5de5\u5177\uff08\u5982BCC\uff09\u5bf9\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u5f62\u5f0f\u9a8c\u8bc1\u5de5\u5177\uff08\u5982Boogie\uff09\u5177\u6709\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2508.16605", "pdf": "https://arxiv.org/pdf/2508.16605", "abs": "https://arxiv.org/abs/2508.16605", "authors": ["Xianghan Wang"], "title": "The Rhythm of Tai Chi: Revitalizing Cultural Heritage in Virtual Reality through Interactive Visuals", "categories": ["cs.HC", "cs.MM"], "comment": "Accepted to the Proceedings of the 2025 4th International Conference\n  on Image Processing and Media Computing (ICIPMC 2025). ISBN:\n  979-8-3315-1363-4. \\c{opyright} 2025 IEEE. This is the author-accepted\n  manuscript. The final version will be available via IEEE Xplore", "summary": "The Rhythm of Tai Chi reinterprets the ancient Chinese martial art as a\ndynamic, interactive virtual reality (VR) experience. By leveraging computer\nvision and multimedia technologies, the project transforms Tai Chi's philosophy\nand movements into an immersive digital form. Real-time motion tracking\ncaptures user gestures, while visual feedback systems simulate the flow of Qi,\nenabling an intuitive and engaging practice environment. Beyond technological\ninnovation, this work bridges traditional Chinese culture and modern audiences.\nIt offers a global platform - accessible even to those unfamiliar with Tai Chi\n- to explore its cultural significance, connections to balance, health, and\nmindfulness. Serving as both a preservation tool and an educational resource,\nThe Rhythm of Tai Chi revitalizes this heritage for the digital age.", "AI": {"tldr": "\u300a\u592a\u6781\u97f5\u5f8b\u300b\u5c06\u592a\u6781\u91cd\u65b0\u8be0\u91ca\u4e3a\u52a8\u6001\u4e92\u52a8\u7684VR\u4f53\u9a8c\uff0c\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u591a\u5a92\u4f53\u6280\u672f\uff0c\u5b9e\u73b0\u6c89\u6d78\u5f0f\u7ec3\u4e60\u3002", "motivation": "\u901a\u8fc7\u73b0\u4ee3\u6280\u672f\u8fde\u63a5\u4f20\u7edf\u6587\u5316\u4e0e\u73b0\u4ee3\u53d7\u4f17\uff0c\u666e\u53ca\u592a\u6781\u7684\u6587\u5316\u4e0e\u5065\u5eb7\u4ef7\u503c\u3002", "method": "\u5229\u7528\u5b9e\u65f6\u52a8\u4f5c\u6355\u6349\u4e0e\u89c6\u89c9\u53cd\u9988\u7cfb\u7edf\uff0c\u6a21\u62df\u592a\u6781\u52a8\u4f5c\u4e0e\u6c14\u7684\u6d41\u52a8\u3002", "result": "\u521b\u9020\u4e86\u76f4\u89c2\u3001\u4e92\u52a8\u7684\u592a\u6781\u7ec3\u4e60\u5e73\u53f0\uff0c\u4e3a\u5168\u7403\u7528\u6237\u63d0\u4f9b\u6587\u5316\u63a2\u7d22\u673a\u4f1a\u3002", "conclusion": "\u8be5\u9879\u76ee\u65e2\u662f\u6587\u5316\u9057\u4ea7\u7684\u4fdd\u62a4\u5de5\u5177\uff0c\u4e5f\u662f\u6570\u5b57\u65f6\u4ee3\u7684\u521b\u65b0\u6559\u80b2\u8d44\u6e90\u3002"}}
{"id": "2508.16996", "pdf": "https://arxiv.org/pdf/2508.16996", "abs": "https://arxiv.org/abs/2508.16996", "authors": ["Xavier Molero", "Carlos Juiz", "Miguel Jesus Rodeno"], "title": "Evaluaci\u00f3n y modelado del rendimiento de los sistemas inform\u00e1ticos", "categories": ["cs.PF"], "comment": "in Spanish language", "summary": "This book, by Molero, Juiz, and Rodeno, titled Performance Evaluation and\nModeling of Computer Systems, presents a comprehensive summary of simple\nquantitative techniques that help answer the above questions. Its approach is\nnot one of theory for theory's sake; rather, in each chapter, after a brief\ntheoretical review, it delves deeply into numerous problems grouped into three\ncategories: those with complete solutions, those for which only the solution is\ngiven, and, finally, those whose resolution is left to the reader's discretion.\nAlthough some of the solved problems may be considered purely academic in terms\nof complexity, they should not be underestimated, as they reveal, on a reduced\nscale, the process that must be followed with the help of appropriate tools to\nsolve equivalent real-world problems of an industrial scale.", "AI": {"tldr": "\u672c\u4e66\u662f\u5173\u4e8e\u8ba1\u7b97\u673a\u7cfb\u7edf\u6027\u80fd\u8bc4\u4f30\u548c\u5efa\u6a21\u7684\u5b9e\u7528\u6307\u5357\uff0c\u7ed3\u5408\u7406\u8bba\u4e0e\u5b9e\u8df5\uff0c\u63d0\u4f9b\u591a\u79cd\u95ee\u9898\u7684\u5206\u7c7b\u548c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7b80\u660e\u7684\u5b9a\u91cf\u6280\u672f\u5e2e\u52a9\u89e3\u7b54\u5b9e\u9645\u95ee\u9898\uff0c\u800c\u4e0d\u4ec5\u9650\u4e8e\u7406\u8bba\u63a2\u8ba8\u3002", "method": "\u6bcf\u7ae0\u5148\u56de\u987e\u7406\u8bba\uff0c\u518d\u6df1\u5165\u63a2\u8ba8\u4e09\u7c7b\u95ee\u9898\uff1a\u5b8c\u5168\u89e3\u7b54\u3001\u4ec5\u63d0\u4f9b\u89e3\u7b54\u548c\u7559\u5f85\u8bfb\u8005\u89e3\u51b3\u7684\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5b66\u672f\u6027\u95ee\u9898\u5c55\u793a\u89e3\u51b3\u5b9e\u9645\u5de5\u4e1a\u89c4\u6a21\u95ee\u9898\u7684\u65b9\u6cd5\u548c\u5de5\u5177\u3002", "conclusion": "\u4e66\u4e2d\u5185\u5bb9\u867d\u6709\u65f6\u663e\u5f97\u5b66\u672f\u5316\uff0c\u4f46\u4e3a\u5b9e\u9645\u95ee\u9898\u7684\u89e3\u51b3\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u6307\u5bfc\u548c\u5de5\u5177\u3002"}}
{"id": "2508.17590", "pdf": "https://arxiv.org/pdf/2508.17590", "abs": "https://arxiv.org/abs/2508.17590", "authors": ["Zui Chen", "Han Li", "Xinhao Zhang", "Xiaoyu Chen", "Chunyin Dong", "Yifeng Wang", "Xin Cai", "Su Zhang", "Ziqi Li", "Chi Ding", "Jinxu Li", "Shuai Wang", "Dousheng Zhao", "Sanhai Gao", "Guangyi Liu"], "title": "RubikSQL: Lifelong Learning Agentic Knowledge Base as an Industrial NL2SQL System", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.MA", "H.2.3; I.2.4; I.2.7"], "comment": "18 pages, 3 figures, 3 tables, to be submitted to VLDB 2026 (PVLDB\n  Volume 19)", "summary": "We present RubikSQL, a novel NL2SQL system designed to address key challenges\nin real-world enterprise-level NL2SQL, such as implicit intents and\ndomain-specific terminology. RubikSQL frames NL2SQL as a lifelong learning\ntask, demanding both Knowledge Base (KB) maintenance and SQL generation.\nRubikSQL systematically builds and refines its KB through techniques including\ndatabase profiling, structured information extraction, agentic rule mining, and\nChain-of-Thought (CoT)-enhanced SQL profiling. RubikSQL then employs a\nmulti-agent workflow to leverage this curated KB, generating accurate SQLs.\nRubikSQL achieves SOTA performance on both the KaggleDBQA and BIRD Mini-Dev\ndatasets. Finally, we release the RubikBench benchmark, a new benchmark\nspecifically designed to capture vital traits of industrial NL2SQL scenarios,\nproviding a valuable resource for future research.", "AI": {"tldr": "RubikSQL \u662f\u4e00\u79cd\u65b0\u9896\u7684 NL2SQL \u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u4f01\u4e1a\u7ea7 NL2SQL \u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u9690\u5f0f\u610f\u56fe\u548c\u9886\u57df\u7279\u5b9a\u672f\u8bed\uff0c\u5e76\u901a\u8fc7\u7ec8\u8eab\u5b66\u4e60\u548c\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u5b9e\u73b0\u9ad8\u6548 SQL \u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u7ea7 NL2SQL \u4e2d\u7684\u9690\u5f0f\u610f\u56fe\u548c\u9886\u57df\u7279\u5b9a\u672f\u8bed\u95ee\u9898\uff0c\u63d0\u5347 SQL \u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u77e5\u8bc6\u5e93\u6784\u5efa\uff08\u6570\u636e\u5e93\u5206\u6790\u3001\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u53d6\u3001\u89c4\u5219\u6316\u6398\u548c CoT \u589e\u5f3a\u5206\u6790\uff09\u548c\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u751f\u6210 SQL\u3002", "result": "\u5728 KaggleDBQA \u548c BIRD Mini-Dev \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e86 RubikBench \u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "RubikSQL \u901a\u8fc7\u7cfb\u7edf\u6027\u77e5\u8bc6\u5e93\u6784\u5efa\u548c\u591a\u4ee3\u7406\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f01\u4e1a\u7ea7 NL2SQL \u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2508.16959", "pdf": "https://arxiv.org/pdf/2508.16959", "abs": "https://arxiv.org/abs/2508.16959", "authors": ["Simone Machetti", "Pasquale Davide Schiavone", "Giovanni Ansaloni", "Miguel Pe\u00f3n-Quir\u00f3s", "David Atienza"], "title": "X-HEEP: An Open-Source, Configurable and Extendible RISC-V Platform for TinyAI Applications", "categories": ["cs.AR"], "comment": null, "summary": "In this work, we present X-HEEP, an open-source, configurable, and extendible\nRISC-V platform for ultra-low-power edge applications (TinyAI). X-HEEP features\nthe eXtendible Accelerator InterFace (XAIF), which enables seamless integration\nof accelerators with varying requirements along with an extensive internal\nconfiguration of cores, memory, bus, and peripherals. Moreover, it supports\nvarious development flows, including FPGA prototyping, ASIC implementation, and\nmixed SystemC-RTL modeling, enabling efficient exploration and optimization.\nImplemented in TSMC's 65 nm CMOS technology (300 MHz, 0.8 V), X-HEEP achieves a\nminimal footprint of only 0.15 mm2 and consumes just 29 uW of leakage power. As\na demonstrator of the configurability and low overhead of X-HEEP as a host\nplatform, we present a study integrating it with near-memory accelerators\ntargeting early-exit dynamic network applications, achieving up to 7.3 x\nperformance speedup and 3.6 x energy improvement on the resulting heterogeneous\nsystem compared to CPU-only execution.", "AI": {"tldr": "X-HEEP\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u53ef\u914d\u7f6e\u7684RISC-V\u5e73\u53f0\uff0c\u4e13\u6ce8\u4e8e\u8d85\u4f4e\u529f\u8017\u8fb9\u7f18\u8ba1\u7b97\uff08TinyAI\uff09\uff0c\u5177\u6709\u53ef\u6269\u5c55\u7684\u52a0\u901f\u5668\u63a5\u53e3\u548c\u9ad8\u6548\u5f00\u53d1\u6d41\u7a0b\u3002", "motivation": "\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4f4e\u529f\u8017\u3001\u9ad8\u7075\u6d3b\u6027\u7684\u786c\u4ef6\u5e73\u53f0\uff0c\u652f\u6301\u591a\u6837\u5316\u52a0\u901f\u5668\u96c6\u6210\u548c\u9ad8\u6548\u5f00\u53d1\u3002", "method": "\u91c7\u7528RISC-V\u67b6\u6784\uff0c\u8bbe\u8ba1eXtendible Accelerator InterFace (XAIF)\uff0c\u652f\u6301\u591a\u79cd\u5f00\u53d1\u6d41\u7a0b\uff08FPGA\u3001ASIC\u3001SystemC-RTL\uff09\u3002", "result": "\u572865 nm CMOS\u6280\u672f\u4e0b\uff0cX-HEEP\u9762\u79ef\u4ec50.15 mm\u00b2\uff0c\u6f0f\u7535\u529f\u801729 \u00b5W\uff0c\u96c6\u6210\u8fd1\u5185\u5b58\u52a0\u901f\u5668\u540e\u6027\u80fd\u63d0\u53477.3\u500d\uff0c\u80fd\u6548\u63d0\u53473.6\u500d\u3002", "conclusion": "X-HEEP\u662f\u8fb9\u7f18\u8ba1\u7b97\u7684\u9ad8\u6548\u5e73\u53f0\uff0c\u5177\u6709\u4f4e\u529f\u8017\u3001\u9ad8\u7075\u6d3b\u6027\u548c\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2508.16596", "pdf": "https://arxiv.org/pdf/2508.16596", "abs": "https://arxiv.org/abs/2508.16596", "authors": ["Hisham Abdelqader"], "title": "Using Generative AI to Uncover What Drives Player Enjoyment in PC and VR Games", "categories": ["cs.HC", "cs.SI"], "comment": null, "summary": "As video games continue to evolve, understanding what drives player enjoyment\nremains a key challenge. Player reviews provide valuable insights, but their\nunstructured nature makes large-scale analysis difficult. This study applies\ngenerative AI and machine learning, leveraging Microsoft Phi-4 LLM and XGBoost,\nto quantify and analyze game reviews from Steam and Meta Quest stores. The\napproach converts qualitative feedback into structured data, enabling\ncomprehensive evaluation of key game design elements, monetization models, and\nplatform-specific trends. The findings reveal distinct patterns in player\npreferences across PC and VR games, highlighting factors that contribute to\nhigher player satisfaction. By integrating Google Cloud for largescale data\nstorage and processing, this study establishes a scalable framework for game\nreview analysis. The study's insights offer actionable guidance for game\ndevelopers, helping optimize game mechanics, pricing strategies, and player\nengagement.", "AI": {"tldr": "\u5229\u7528\u751f\u6210\u5f0fAI\u548c\u673a\u5668\u5b66\u4e60\u5206\u6790\u6e38\u620f\u8bc4\u8bba\uff0c\u63ed\u793a\u73a9\u5bb6\u504f\u597d\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "\u7406\u89e3\u73a9\u5bb6\u4eab\u53d7\u6e38\u620f\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5229\u7528\u975e\u7ed3\u6784\u5316\u8bc4\u8bba\u6570\u636e\u8fdb\u884c\u5927\u89c4\u6a21\u5206\u6790\u3002", "method": "\u5e94\u7528Microsoft Phi-4 LLM\u548cXGBoost\uff0c\u7ed3\u5408Google Cloud\u5904\u7406\u6570\u636e\uff0c\u5c06\u5b9a\u6027\u53cd\u9988\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u3002", "result": "\u53d1\u73b0\u4e86PC\u548cVR\u6e38\u620f\u4e2d\u4e0d\u540c\u7684\u73a9\u5bb6\u504f\u597d\u6a21\u5f0f\uff0c\u4e3a\u6e38\u620f\u8bbe\u8ba1\u548c\u5b9a\u4ef7\u7b56\u7565\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u5206\u6790\u6846\u67b6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u4f18\u5316\u6e38\u620f\u673a\u5236\u548c\u73a9\u5bb6\u53c2\u4e0e\u5ea6\u3002"}}
{"id": "2508.16692", "pdf": "https://arxiv.org/pdf/2508.16692", "abs": "https://arxiv.org/abs/2508.16692", "authors": ["Mark Fisher", "John Severini"], "title": "Making AI Inevitable: Historical Perspective and the Problems of Predicting Long-Term Technological Change", "categories": ["cs.CY", "cs.AI", "cs.ET", "econ.GN", "q-fin.EC"], "comment": null, "summary": "This study demonstrates the extent to which prominent debates about the\nfuture of AI are best understood as subjective, philosophical disagreements\nover the history and future of technological change rather than as objective,\nmaterial disagreements over the technologies themselves. It focuses on the deep\ndisagreements over whether artificial general intelligence (AGI) will prove\ntransformative for human society; a question that is analytically prior to that\nof whether this transformative effect will help or harm humanity. The study\nbegins by distinguishing two fundamental camps in this debate. The first of\nthese can be identified as \"transformationalists,\" who argue that continued AI\ndevelopment will inevitably have a profound effect on society. Opposed to them\nare \"skeptics,\" a more eclectic group united by their disbelief that AI can or\nwill live up to such high expectations. Each camp admits further \"strong\" and\n\"weak\" variants depending on their tolerance for epistemic risk. These stylized\ncontrasts help to identify a set of fundamental questions that shape the camps'\nrespective interpretations of the future of AI. Three questions in particular\nare focused on: the possibility of non-biological intelligence, the appropriate\ntime frame of technological predictions, and the assumed trajectory of\ntechnological development. In highlighting these specific points of\nnon-technical disagreement, this study demonstrates the wide range of different\narguments used to justify either the transformationalist or skeptical position.\nAt the same time, it highlights the strong argumentative burden of the\ntransformationalist position, the way that belief in this position creates\ncompetitive pressures to achieve first-mover advantage, and the need to widen\nthe concept of \"expertise\" in debates surrounding the future development of AI.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5173\u4e8eAI\u672a\u6765\u7684\u8fa9\u8bba\u672c\u8d28\u4e0a\u662f\u4e3b\u89c2\u7684\u54f2\u5b66\u5206\u6b67\uff0c\u800c\u975e\u5ba2\u89c2\u7684\u6280\u672f\u4e89\u8bae\u3002\u7814\u7a76\u805a\u7126\u4e8e\u5bf9\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u662f\u5426\u4f1a\u6539\u53d8\u793e\u4f1a\u7684\u5206\u6b67\uff0c\u5e76\u533a\u5206\u4e86\u201c\u53d8\u9769\u6d3e\u201d\u548c\u201c\u6000\u7591\u6d3e\u201d\u4e24\u79cd\u7acb\u573a\u3002", "motivation": "\u52a8\u673a\u662f\u63ed\u793aAI\u672a\u6765\u8fa9\u8bba\u4e2d\u975e\u6280\u672f\u6027\u5206\u6b67\u7684\u6838\u5fc3\uff0c\u4fc3\u8fdb\u5bf9\u4e89\u8bae\u672c\u8d28\u7684\u7406\u89e3\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u533a\u5206\u5e76\u5206\u6790\u53d8\u9769\u6d3e\u548c\u6000\u7591\u6d3e\u7684\u6838\u5fc3\u89c2\u70b9\uff0c\u7279\u522b\u662f\u56f4\u7ed5\u975e\u751f\u7269\u667a\u80fd\u3001\u9884\u6d4b\u65f6\u95f4\u6846\u67b6\u548c\u6280\u672f\u53d1\u5c55\u8f68\u8ff9\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\u3002", "result": "\u7ed3\u679c\u5c55\u793a\u4e86\u8fa9\u8bba\u4e2d\u53cc\u65b9\u8bba\u8bc1\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u53d8\u9769\u6d3e\u7684\u8bba\u8bc1\u8d1f\u62c5\u53ca\u5176\u5bf9\u5148\u884c\u4f18\u52bf\u7684\u7ade\u4e89\u538b\u529b\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u9700\u8981\u62d3\u5bbdAI\u672a\u6765\u53d1\u5c55\u8fa9\u8bba\u4e2d\u7684\u201c\u4e13\u4e1a\u77e5\u8bc6\u201d\u6982\u5ff5\uff0c\u4ee5\u5bb9\u7eb3\u66f4\u591a\u89c2\u70b9\u3002"}}
{"id": "2508.16792", "pdf": "https://arxiv.org/pdf/2508.16792", "abs": "https://arxiv.org/abs/2508.16792", "authors": ["Felix Wang", "Bradley H. Theilman", "Fred Rothganger", "William Severa", "Craig M. Vineyard", "James B. Aimone"], "title": "Neuromorphic Simulation of Drosophila Melanogaster Brain Connectome on Loihi 2", "categories": ["cs.DC", "cs.NE"], "comment": null, "summary": "We demonstrate the first-ever nontrivial, biologically realistic connectome\nsimulated on neuromorphic computing hardware. Specifically, we implement the\nwhole-brain connectome of the adult Drosophila melanogaster (fruit fly) from\nthe FlyWire Consortium containing 140K neurons and 50M synapses on the Intel\nLoihi 2 neuromorphic platform. This task is particularly challenging due to the\ncharacteristic connectivity structure of biological networks. Unlike artificial\nneural networks and most abstracted neural models, real biological circuits\nexhibit sparse, recurrent, and irregular connectivity that is poorly suited to\nconventional computing methods intended for dense linear algebra. Though\nneuromorphic hardware is architecturally better suited to discrete event-based\nbiological communication, mapping the connectivity structure to frontier\nsystems still faces challenges from low-level hardware constraints, such as\nfan-in and fan-out memory limitations. We describe solutions to these\nchallenges that allow for the full FlyWire connectome to fit onto 12 Loihi 2\nchips. We statistically validate our implementation by comparing network\nbehavior across multiple reference simulations. Significantly, we achieve a\nneuromorphic implementation that is orders of magnitude faster than numerical\nsimulations on conventional hardware, and we also find that performance\nadvantages increase with sparser activity. These results affirm that today's\nscalable neuromorphic platforms are capable of implementing and accelerating\nbiologically realistic models -- a key enabling technology for advancing\nneuro-inspired AI and computational neuroscience.", "AI": {"tldr": "\u9996\u6b21\u5728\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u751f\u7269\u5b66\u771f\u5b9e\u7684\u679c\u8747\u5168\u8111\u8fde\u63a5\u7ec4\u4eff\u771f\uff0c\u89e3\u51b3\u4e86\u786c\u4ef6\u9650\u5236\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u7f51\u7edc\u7279\u6709\u7684\u7a00\u758f\u3001\u5faa\u73af\u548c\u4e0d\u89c4\u5219\u8fde\u63a5\u7ed3\u6784\u5728\u4f20\u7edf\u8ba1\u7b97\u786c\u4ef6\u4e0a\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002", "method": "\u5728Intel Loihi 2\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u5305\u542b14\u4e07\u4e2a\u795e\u7ecf\u5143\u548c5000\u4e07\u4e2a\u7a81\u89e6\u7684\u679c\u8747\u5168\u8111\u8fde\u63a5\u7ec4\uff0c\u5e76\u514b\u670d\u4e86\u786c\u4ef6\u9650\u5236\u3002", "result": "\u9a8c\u8bc1\u4e86\u5b9e\u73b0\u7684\u6709\u6548\u6027\uff0c\u4e14\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u7684\u6027\u80fd\u6bd4\u4f20\u7edf\u786c\u4ef6\u5feb\u591a\u4e2a\u6570\u91cf\u7ea7\uff0c\u7a00\u758f\u6d3b\u52a8\u4e0b\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "\u8bc1\u660e\u73b0\u4ee3\u795e\u7ecf\u5f62\u6001\u5e73\u53f0\u80fd\u591f\u52a0\u901f\u751f\u7269\u5b66\u771f\u5b9e\u6a21\u578b\uff0c\u63a8\u52a8\u4e86\u795e\u7ecf\u542f\u53d1AI\u548c\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.16853", "pdf": "https://arxiv.org/pdf/2508.16853", "abs": "https://arxiv.org/abs/2508.16853", "authors": ["Pratyush Nidhi Sharma", "Lauren Wright", "Anne Herfurth", "Munsif Sokiyna", "Pratyaksh Nidhi Sharma", "Sethu Das", "Mikko Siponen"], "title": "DevLicOps: A Framework for Mitigating Licensing Risks in AI-Generated Code", "categories": ["cs.SE", "cs.AI"], "comment": "18 pages, 1 figure, 2 Tables", "summary": "Generative AI coding assistants (ACAs) are widely adopted yet pose serious\nlegal and compliance risks. ACAs can generate code governed by restrictive\nopen-source licenses (e.g., GPL), potentially exposing companies to litigation\nor forced open-sourcing. Few developers are trained in these risks, and legal\nstandards vary globally, especially with outsourcing. Our article introduces\nDevLicOps, a practical framework that helps IT leaders manage ACA-related\nlicensing risks through governance, incident response, and informed tradeoffs.\nAs ACA adoption grows and legal frameworks evolve, proactive license compliance\nis essential for responsible, risk-aware software development in the AI era.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u7f16\u7801\u52a9\u624b\uff08ACAs\uff09\u53ef\u80fd\u5e26\u6765\u6cd5\u5f8b\u98ce\u9669\uff0c\u5c24\u5176\u662f\u5f00\u6e90\u8bb8\u53ef\u8bc1\u95ee\u9898\u3002\u6587\u7ae0\u63d0\u51faDevLicOps\u6846\u67b6\uff0c\u5e2e\u52a9IT\u9886\u5bfc\u8005\u7ba1\u7406\u5408\u89c4\u98ce\u9669\u3002", "motivation": "ACAs\u53ef\u80fd\u751f\u6210\u53d7\u9650\u5236\u5f00\u6e90\u8bb8\u53ef\u8bc1\uff08\u5982GPL\uff09\u7ea6\u675f\u7684\u4ee3\u7801\uff0c\u5f00\u53d1\u8005\u7f3a\u4e4f\u76f8\u5173\u77e5\u8bc6\uff0c\u6cd5\u5f8b\u6807\u51c6\u5168\u7403\u4e0d\u7edf\u4e00\uff0c\u5bfc\u81f4\u516c\u53f8\u53ef\u80fd\u9762\u4e34\u8bc9\u8bbc\u6216\u88ab\u5f3a\u5236\u5f00\u6e90\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51faDevLicOps\u6846\u67b6\uff0c\u901a\u8fc7\u6cbb\u7406\u3001\u4e8b\u4ef6\u54cd\u5e94\u548c\u77e5\u60c5\u6743\u8861\u6765\u7ba1\u7406ACA\u76f8\u5173\u7684\u8bb8\u53ef\u8bc1\u98ce\u9669\u3002", "result": "DevLicOps\u4e3aAI\u65f6\u4ee3\u7684\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u79cd\u524d\u77bb\u6027\u7684\u8bb8\u53ef\u8bc1\u5408\u89c4\u65b9\u6cd5\u3002", "conclusion": "\u968f\u7740ACA\u7684\u666e\u53ca\u548c\u6cd5\u5f8b\u73af\u5883\u7684\u53d8\u5316\uff0c\u4e3b\u52a8\u7684\u8bb8\u53ef\u8bc1\u5408\u89c4\u5bf9\u8d1f\u8d23\u4efb\u548c\u98ce\u9669\u610f\u8bc6\u7684\u8f6f\u4ef6\u5f00\u53d1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.17911", "pdf": "https://arxiv.org/pdf/2508.17911", "abs": "https://arxiv.org/abs/2508.17911", "authors": ["Haoxiang Luo", "Ruichen Zhang", "Yinqiu Liu", "Gang Sun", "Hongfang Yu", "Zhu Han"], "title": "Real World Assets on-Chain Assistance Low-Altitude Computility Networks: Architecture, Methodology, and Challenges", "categories": ["cs.NI"], "comment": null, "summary": "Low-altitude airspace is becoming a new frontier for smart city services and\ncommerce. Networks of drones, electric Vertical Takeoff and Landing (eVTOL)\nvehicles, and other aircraft, termed Low-Altitude Economic Networks (LAENets),\npromise to transform urban logistics, aerial sensing, and communication. A key\nchallenge is how to efficiently share and trust the computing utility, termed\ncomputility, of these aerial devices. We propose treating the computing power\non aircraft as tokenized Real-World Assets (RWAs) that can be traded and\norchestrated via blockchain. By representing distributed edge computing\nresources as blockchain tokens, disparate devices can form Low-Altitude\nComputility Networks (LACNets), collaborative computing clusters in the sky. We\nfirst compare blockchain technologies, non-fungible tokens (NFTs), and RWA\nframeworks to clarify how physical hardware and its computational output can be\ntokenized as assets. Then, we present an architecture using blockchain to\nintegrate aircraft fleets into a secure, interoperable computing network.\nFurthermore, a case study models an urban logistics LACNet of delivery drones\nand air-taxis. Simulation results indicate improvements in task latency, trust\nassurance, and resource efficiency when leveraging RWA-based coordination.\nFinally, we discuss future research directions, including AI-driven\norchestration, edge AI offloading and collaborative computing, and\ncross-jurisdictional policy for tokenized assets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06\u4f4e\u7a7a\u98de\u884c\u5668\u7684\u8ba1\u7b97\u8d44\u6e90\u4f5c\u4e3a\u4ee3\u5e01\u5316\u7684\u771f\u5b9e\u4e16\u754c\u8d44\u4ea7\uff08RWAs\uff09\uff0c\u901a\u8fc7\u533a\u5757\u94fe\u8fdb\u884c\u4ea4\u6613\u548c\u534f\u8c03\uff0c\u6784\u5efa\u4f4e\u7a7a\u8ba1\u7b97\u7f51\u7edc\uff08LACNets\uff09\uff0c\u63d0\u5347\u4efb\u52a1\u5ef6\u8fdf\u3001\u4fe1\u4efb\u4fdd\u969c\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\uff08LAENets\uff09\u662f\u667a\u6167\u57ce\u5e02\u670d\u52a1\u548c\u5546\u4e1a\u7684\u65b0\u9886\u57df\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u5171\u4eab\u548c\u4fe1\u4efb\u8fd9\u4e9b\u98de\u884c\u5668\u7684\u8ba1\u7b97\u8d44\u6e90\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u533a\u5757\u94fe\u6280\u672f\u5c06\u5206\u5e03\u5f0f\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u4ee3\u5e01\u5316\uff0c\u6574\u5408\u98de\u884c\u5668\u7f16\u961f\u4e3a\u5b89\u5168\u3001\u4e92\u64cd\u4f5c\u7684\u8ba1\u7b97\u7f51\u7edc\uff0c\u5e76\u4ee5\u57ce\u5e02\u7269\u6d41\u4e3a\u4f8b\u8fdb\u884c\u6a21\u62df\u7814\u7a76\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eRWA\u7684\u534f\u8c03\u53ef\u4ee5\u6539\u5584\u4efb\u52a1\u5ef6\u8fdf\u3001\u4fe1\u4efb\u4fdd\u969c\u548c\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ecAI\u9a71\u52a8\u7684\u534f\u8c03\u3001\u8fb9\u7f18AI\u5378\u8f7d\u4e0e\u534f\u4f5c\u8ba1\u7b97\uff0c\u4ee5\u53ca\u8de8\u53f8\u6cd5\u7ba1\u8f96\u533a\u7684\u4ee3\u5e01\u5316\u8d44\u4ea7\u653f\u7b56\u3002"}}
{"id": "2508.17480", "pdf": "https://arxiv.org/pdf/2508.17480", "abs": "https://arxiv.org/abs/2508.17480", "authors": ["Brian Chao", "Jacqueline Yang", "Suyeon Choi", "Manu Gopakumar", "Ryota Koiso", "Gordon Wetzstein"], "title": "Random-phase Gaussian Wave Splatting for Computer-generated Holography", "categories": ["cs.GR", "cs.AR", "eess.IV", "eess.SP", "physics.optics"], "comment": null, "summary": "Holographic near-eye displays offer ultra-compact form factors for virtual\nand augmented reality systems, but rely on advanced computer-generated\nholography (CGH) algorithms to convert 3D scenes into interference patterns\nthat can be displayed on spatial light modulators (SLMs). Gaussian Wave\nSplatting (GWS) has recently emerged as a powerful CGH paradigm that allows for\nthe conversion of Gaussians, a state-of-the-art neural 3D representation, into\nholograms. However, GWS assumes smooth-phase distributions over the Gaussian\nprimitives, limiting their ability to model view-dependent effects and\nreconstruct accurate defocus blur, and severely under-utilizing the\nspace-bandwidth product of the SLM. In this work, we propose random-phase GWS\n(GWS-RP) to improve bandwidth utilization, which has the effect of increasing\neyebox size, reconstructing accurate defocus blur and parallax, and supporting\ntime-multiplexed rendering to suppress speckle artifacts.\n  At the core of GWS-RP are (1) a fundamentally new wavefront compositing\nprocedure and (2) an alpha-blending scheme specifically designed for\nrandom-phase Gaussian primitives, ensuring physically correct color\nreconstruction and robust occlusion handling. Additionally, we present the\nfirst formally derived algorithm for applying random phase to Gaussian\nprimitives, grounded in rigorous statistical optics analysis and validated\nthrough practical near-eye display applications. Through extensive simulations\nand experimental validations, we demonstrate that these advancements,\ncollectively with time-multiplexing, uniquely enables full-bandwith light field\nCGH that supports accurate accurate parallax and defocus, yielding\nstate-of-the-art image quality and perceptually faithful 3D holograms for\nnext-generation near-eye displays.", "AI": {"tldr": "\u968f\u673a\u76f8\u4f4d\u9ad8\u65af\u6ce2\u5149\u6805\uff08GWS-RP\uff09\u901a\u8fc7\u6539\u8fdb\u5e26\u5bbd\u5229\u7528\uff0c\u63d0\u5347\u5168\u606f\u8fd1\u773c\u663e\u793a\u7684\u6027\u80fd\uff0c\u652f\u6301\u7cbe\u786e\u89c6\u5dee\u548c\u6563\u7126\u6a21\u7cca\uff0c\u5e76\u6291\u5236\u6563\u6591\u566a\u58f0\u3002", "motivation": "\u4f20\u7edf\u7684\u9ad8\u65af\u6ce2\u5149\u6805\uff08GWS\uff09\u5047\u8bbe\u9ad8\u65af\u539f\u59cb\u76f8\u4f4d\u5e73\u6ed1\uff0c\u9650\u5236\u4e86\u89c6\u4f9d\u8d56\u6548\u5e94\u548c\u7cbe\u786e\u6563\u7126\u6a21\u7cca\u7684\u5efa\u6a21\uff0c\u4e14\u672a\u5145\u5206\u5229\u7528\u7a7a\u95f4\u5149\u8c03\u5236\u5668\u7684\u5e26\u5bbd\u3002", "method": "\u63d0\u51faGWS-RP\uff0c\u5305\u62ec\u65b0\u7684\u6ce2\u524d\u5408\u6210\u8fc7\u7a0b\u548c\u9488\u5bf9\u968f\u673a\u76f8\u4f4d\u9ad8\u65af\u539f\u59cb\u7684alpha\u6df7\u5408\u65b9\u6848\uff0c\u5e76\u7ed3\u5408\u65f6\u95f4\u590d\u7528\u6280\u672f\u6291\u5236\u6563\u6591\u3002", "result": "GWS-RP\u5b9e\u73b0\u4e86\u5168\u5e26\u5bbd\u5149\u573aCGH\uff0c\u652f\u6301\u7cbe\u786e\u89c6\u5dee\u548c\u6563\u7126\u6a21\u7cca\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u611f\u77e5\u771f\u5b9e\u76843D\u5168\u606f\u6548\u679c\u3002", "conclusion": "GWS-RP\u901a\u8fc7\u968f\u673a\u76f8\u4f4d\u548c\u6539\u8fdb\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd1\u773c\u5168\u606f\u663e\u793a\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u529f\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8bbe\u5907\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.17568", "pdf": "https://arxiv.org/pdf/2508.17568", "abs": "https://arxiv.org/abs/2508.17568", "authors": ["Liane Makatura", "Benjamin Jones", "Siyuan Bian", "Wojciech Matusik"], "title": "MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG", "cs.PL"], "comment": null, "summary": "Metamaterials are micro-architected structures whose geometry imparts highly\ntunable-often counter-intuitive-bulk properties. Yet their design is difficult\nbecause of geometric complexity and a non-trivial mapping from architecture to\nbehaviour. We address these challenges with three complementary contributions.\n(i) MetaDSL: a compact, semantically rich domain-specific language that\ncaptures diverse metamaterial designs in a form that is both human-readable and\nmachine-parsable. (ii) MetaDB: a curated repository of more than 150,000\nparameterized MetaDSL programs together with their\nderivatives-three-dimensional geometry, multi-view renderings, and simulated\nelastic properties. (iii) MetaBench: benchmark suites that test three core\ncapabilities of vision-language metamaterial assistants-structure\nreconstruction, property-driven inverse design, and performance prediction. We\nestablish baselines by fine-tuning state-of-the-art vision-language models and\ndeploy an omni-model within an interactive, CAD-like interface. Case studies\nshow that our framework provides a strong first step toward integrated design\nand understanding of structure-representation-property relationships.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5305\u62ecMetaDSL\u8bed\u8a00\u3001MetaDB\u6570\u636e\u5e93\u548cMetaBench\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u89e3\u51b3\u8d85\u6750\u6599\u8bbe\u8ba1\u7684\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "\u8d85\u6750\u6599\u8bbe\u8ba1\u56e0\u51e0\u4f55\u590d\u6742\u6027\u548c\u7ed3\u6784\u4e0e\u6027\u80fd\u7684\u975e\u7ebf\u6027\u6620\u5c04\u800c\u56f0\u96be\uff0c\u9700\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u7b80\u5316\u548c\u4f18\u5316\u8bbe\u8ba1\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86MetaDSL\u8bed\u8a00\u63cf\u8ff0\u8bbe\u8ba1\uff0c\u6784\u5efaMetaDB\u6570\u636e\u5e93\u5b58\u50a8\u8bbe\u8ba1\u53ca\u884d\u751f\u6570\u636e\uff0c\u5e76\u901a\u8fc7MetaBench\u6d4b\u8bd5\u5957\u4ef6\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5fae\u8c03\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u90e8\u7f72\u4ea4\u4e92\u5f0f\u754c\u9762\uff0c\u521d\u6b65\u5b9e\u73b0\u4e86\u7ed3\u6784\u4e0e\u6027\u80fd\u5173\u7cfb\u7684\u96c6\u6210\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d85\u6750\u6599\u7684\u8bbe\u8ba1\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.18115", "pdf": "https://arxiv.org/pdf/2508.18115", "abs": "https://arxiv.org/abs/2508.18115", "authors": ["Quang Loc Le"], "title": "Compositional Verification in Concurrent Separation Logic with Permissions Regions", "categories": ["cs.LO", "cs.PL"], "comment": null, "summary": "Concurrent separation logic with fractional permissions (CSLPerm) provides a\npromising reasoning system to verify most complex sequential and concurrent\nfine-grained programs. The logic with strong and weak separating conjunctions\noffers a solid foundation for producing concise and precise proofs. However, it\nlacks automation and compositionality support. This paper addresses this\nlimitation by introducing a compositional verification system for concurrent\nprograms that manipulate regions of shared memory. The centre of our system is\nnovel logical principles and an entailment procedure that can infer the\nresidual heaps in the frame rule for a fragment of CSL-Perm with explicit\narithmetical constraints for memory heaps' disjointness. This procedure enables\nthe compositional reasoning for concurrent threads and function calls. We have\nimplemented the proposal in a prototype tool called CoSl, tested it with 10\nchallenging concurrent programs, including those beyond the state-of-the-art,\nand confirmed the advantage of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec4\u5408\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u7528\u4e8e\u5e76\u53d1\u7a0b\u5e8f\u4e2d\u5171\u4eab\u5185\u5b58\u533a\u57df\u7684\u64cd\u7eb5\uff0c\u89e3\u51b3\u4e86CSL-Perm\u7f3a\u4e4f\u81ea\u52a8\u5316\u548c\u7ec4\u5408\u6027\u652f\u6301\u7684\u95ee\u9898\u3002", "motivation": "CSL-Perm\u867d\u7136\u4e3a\u590d\u6742\u987a\u5e8f\u548c\u5e76\u53d1\u7ec6\u7c92\u5ea6\u7a0b\u5e8f\u7684\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5f3a\u5927\u903b\u8f91\u57fa\u7840\uff0c\u4f46\u7f3a\u4e4f\u81ea\u52a8\u5316\u548c\u7ec4\u5408\u6027\u652f\u6301\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u903b\u8f91\u539f\u5219\u548c\u63a8\u5bfc\u7a0b\u5e8f\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ec4\u5408\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u80fd\u591f\u63a8\u65ad\u5e27\u89c4\u5219\u4e2d\u7684\u5269\u4f59\u5806\uff0c\u5e76\u5b9e\u73b0\u5bf9\u5e76\u53d1\u7ebf\u7a0b\u548c\u51fd\u6570\u8c03\u7528\u7684\u7ec4\u5408\u63a8\u7406\u3002", "result": "\u5b9e\u73b0\u4e86\u4e00\u4e2a\u539f\u578b\u5de5\u5177CoSl\uff0c\u6d4b\u8bd5\u4e8610\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5e76\u53d1\u7a0b\u5e8f\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7ec4\u5408\u9a8c\u8bc1\u7cfb\u7edf\u4e3aCSL-Perm\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u548c\u7ec4\u5408\u6027\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u5e76\u53d1\u7a0b\u5e8f\u9a8c\u8bc1\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.16618", "pdf": "https://arxiv.org/pdf/2508.16618", "abs": "https://arxiv.org/abs/2508.16618", "authors": ["Azmine Toushik Wasi", "Rahatun Nesa Priti", "Mahir Absar Khan", "Abdur Rahman", "Mst Rafia Islam"], "title": "Seeing Isn't Believing: Addressing the Societal Impact of Deepfakes in Low-Tech Environments", "categories": ["cs.HC", "cs.CY", "cs.MM", "stat.AP"], "comment": "Accepted to ACM MM 2025 Workshop Diffusion of Harmful Content on\n  Online Web (DHOW)", "summary": "Deepfakes, AI-generated multimedia content that mimics real media, are\nbecoming increasingly prevalent, posing significant risks to political\nstability, social trust, and economic well-being, especially in developing\nsocieties with limited media literacy and technological infrastructure. This\nwork aims to understand how these technologies are perceived and impact\nresource-limited communities. We conducted a survey to assess public awareness,\nperceptions, and experiences with deepfakes, leading to the development of a\ncomprehensive framework for prevention, detection, and mitigation in\ntech-limited environments. Our findings reveal critical knowledge gaps and a\nlack of effective detection tools, emphasizing the need for targeted education\nand accessible verification solutions. This work offers actionable insights to\nsupport vulnerable populations and calls for further interdisciplinary efforts\nto tackle deepfake challenges globally, particularly in the Global South.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5bf9\u8d44\u6e90\u6709\u9650\u793e\u533a\u7684\u611f\u77e5\u548c\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u9884\u9632\u3001\u68c0\u6d4b\u548c\u7f13\u89e3\u7684\u6846\u67b6\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5bf9\u653f\u6cbb\u7a33\u5b9a\u3001\u793e\u4f1a\u4fe1\u4efb\u548c\u7ecf\u6d4e\u798f\u7949\u6784\u6210\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u5728\u5a92\u4f53\u7d20\u517b\u548c\u6280\u672f\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u7684\u53d1\u5c55\u4e2d\u56fd\u5bb6\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u8bc4\u4f30\u516c\u4f17\u5bf9\u6df1\u5ea6\u4f2a\u9020\u7684\u8ba4\u77e5\u3001\u611f\u77e5\u548c\u4f53\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u516c\u4f17\u5bf9\u6df1\u5ea6\u4f2a\u9020\u7684\u8ba4\u8bc6\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u68c0\u6d4b\u5de5\u5177\uff0c\u9700\u8981\u9488\u5bf9\u6027\u6559\u80b2\u548c\u6613\u7528\u7684\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u66f4\u591a\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5168\u7403\u5357\u65b9\u5730\u533a\uff0c\u4ee5\u5e94\u5bf9\u6df1\u5ea6\u4f2a\u9020\u7684\u6311\u6218\u3002"}}
{"id": "2508.17372", "pdf": "https://arxiv.org/pdf/2508.17372", "abs": "https://arxiv.org/abs/2508.17372", "authors": ["Yingjia Wang", "Ming-Chang Yang"], "title": "The Unwritten Contract of Cloud-based Elastic Solid-State Drives", "categories": ["cs.PF"], "comment": "Accepted and to appear in DAC 2025", "summary": "Elastic block storage (EBS) with the storage-compute disaggregated\narchitecture stands as a pivotal piece in today's cloud. EBS furnishes users\nwith storage capabilities through the elastic solid-state drive (ESSD).\nNevertheless, despite the widespread integration into cloud services, the\nabsence of a thorough ESSD performance characterization raises critical doubt:\nwhen more and more services are shifted onto the cloud, can ESSD satisfactorily\nsubstitute the storage responsibilities of the local SSD and offer comparable\nperformance?\n  In this paper, we for the first time target this question by characterizing\ntwo ESSDs from Amazon AWS and Alibaba Cloud. We present an unwritten contract\nof cloud-based ESSDs, encapsulating four observations and five implications for\ncloud storage users. Specifically, the observations are counter-intuitive and\ncontrary to the conventional perceptions of what one would expect from the\nlocal SSD. The implications we hope could guide users in revisiting the designs\nof their deployed cloud software, i.e., harnessing the distinct characteristics\nof ESSDs for better system performance.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5bf9Amazon AWS\u548cAlibaba Cloud\u7684\u5f39\u6027\u56fa\u6001\u786c\u76d8(ESSD)\u6027\u80fd\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u63ed\u793a\u4e86ESSD\u4e0e\u4f20\u7edf\u672c\u5730SSD\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u56db\u70b9\u89c2\u5bdf\u548c\u4e94\u70b9\u5efa\u8bae\uff0c\u5e2e\u52a9\u4e91\u5b58\u50a8\u7528\u6237\u4f18\u5316\u7cfb\u7edf\u8bbe\u8ba1\u3002", "motivation": "\u5c3d\u7ba1\u4e91\u670d\u52a1\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u5f39\u6027\u5757\u5b58\u50a8(EBS)\u548cESSD\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u5f15\u53d1\u4e86\u5bf9ESSD\u662f\u5426\u80fd\u66ff\u4ee3\u672c\u5730SSD\u5e76\u63d0\u4f9b\u76f8\u4f3c\u6027\u80fd\u7684\u7591\u95ee\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790Amazon AWS\u548cAlibaba Cloud\u7684\u4e24\u6b3eESSD\uff0c\u603b\u7ed3\u51fa\u5176\u6027\u80fd\u7279\u70b9\u3002", "result": "\u63ed\u793a\u4e86ESSD\u4e0e\u4f20\u7edf\u672c\u5730SSD\u7684\u56db\u70b9\u53cd\u76f4\u89c9\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u70b9\u4f18\u5316\u5efa\u8bae\u3002", "conclusion": "ESSD\u7684\u6027\u80fd\u7279\u6027\u4e0e\u672c\u5730SSD\u4e0d\u540c\uff0c\u7528\u6237\u5e94\u91cd\u65b0\u8bbe\u8ba1\u4e91\u8f6f\u4ef6\u4ee5\u5145\u5206\u5229\u7528\u5176\u7279\u6027\uff0c\u4ece\u800c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2508.17693", "pdf": "https://arxiv.org/pdf/2508.17693", "abs": "https://arxiv.org/abs/2508.17693", "authors": ["Eunjae Jo", "Nakyung Lee", "Gyuyeong Kim"], "title": "Database Normalization via Dual-LLM Self-Refinement", "categories": ["cs.DB", "cs.AI"], "comment": "5 pages", "summary": "Database normalization is crucial to preserving data integrity. However, it\nis time-consuming and error-prone, as it is typically performed manually by\ndata engineers. To this end, we present Miffie, a database normalization\nframework that leverages the capability of large language models. Miffie\nenables automated data normalization without human effort while preserving high\naccuracy. The core of Miffie is a dual-model self-refinement architecture that\ncombines the best-performing models for normalized schema generation and\nverification, respectively. The generation module eliminates anomalies based on\nthe feedback of the verification module until the output schema satisfies the\nrequirement for normalization. We also carefully design task-specific zero-shot\nprompts to guide the models for achieving both high accuracy and cost\nefficiency. Experimental results show that Miffie can normalize complex\ndatabase schemas while maintaining high accuracy.", "AI": {"tldr": "Miffie\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u6570\u636e\u5e93\u81ea\u52a8\u89c4\u8303\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u6a21\u578b\u81ea\u4f18\u5316\u67b6\u6784\u548c\u96f6\u6837\u672c\u63d0\u793a\u8bbe\u8ba1\uff0c\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u5b8c\u6210\u89c4\u8303\u5316\u4efb\u52a1\u3002", "motivation": "\u6570\u636e\u5e93\u89c4\u8303\u5316\u901a\u5e38\u7531\u4eba\u5de5\u5b8c\u6210\uff0c\u8017\u65f6\u4e14\u6613\u9519\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "Miffie\u91c7\u7528\u53cc\u6a21\u578b\u67b6\u6784\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u5757\u548c\u9a8c\u8bc1\u6a21\u5757\uff0c\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u4f18\u5316\u89c4\u8303\u5316\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u63d0\u9ad8\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMiffie\u80fd\u9ad8\u6548\u5904\u7406\u590d\u6742\u6570\u636e\u5e93\u6a21\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "Miffie\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u9ad8\u6548\u6570\u636e\u5e93\u89c4\u8303\u5316\uff0c\u4e3a\u6570\u636e\u5de5\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.16981", "pdf": "https://arxiv.org/pdf/2508.16981", "abs": "https://arxiv.org/abs/2508.16981", "authors": ["Simone Machetti", "Deniz Kasap", "Juan Sapriza", "Rub\u00e9n Rodr\u00edguez \u00c1lvarez", "Hossein Taji", "Jos\u00e9 Miranda", "Miguel Pe\u00f3n-Quir\u00f3s", "David Atienza"], "title": "Invited Paper: FEMU: An Open-Source and Configurable Emulation Framework for Prototyping TinyAI Heterogeneous Systems", "categories": ["cs.AR"], "comment": null, "summary": "In this paper, we present the new FPGA EMUlation (FEMU), an open-source and\nconfigurable emulation framework for prototyping and evaluating TinyAI\nheterogeneous systems (HS). FEMU leverages the capability of system-on-chip\n(SoC)-based FPGAs to combine the under-development HS implemented in a\nreconfigurable hardware region (RH) for quick prototyping with a software\nenvironment running under a standard operating system in a control software\nregion (CS) for supervision and communication. To evaluate our approach, we\nbuilt the X-HEEP FPGA EMUlation (X-HEEP-FEMU) platform by instantiating the\nproposed framework with real-world hardware and software components.\nX-HEEP-FEMU is deployed on the Xilinx Zynq-7020 SoC and integrates the\neXtendible Heterogeneous Energy Efficient Platform (X-HEEP) host in the RH, a\nLinux-based Python environment on the ARM Cortex-A9 CS, and energy models\nderived from a TSMC 65 nm CMOS silicon implementation of X-HEEP, called\nHEEPocrates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFEMU\u7684\u5f00\u6e90\u53ef\u914d\u7f6e\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u539f\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30TinyAI\u5f02\u6784\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u786c\u4ef6\u548c\u8f6f\u4ef6\u73af\u5883\u3002", "motivation": "\u4e3a\u4e86\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u8bc4\u4f30\u5f02\u6784\u7cfb\u7edf\uff0c\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u786c\u4ef6\u548c\u8f6f\u4ef6\u73af\u5883\u7684\u7075\u6d3b\u4eff\u771f\u6846\u67b6\u3002", "method": "\u5229\u7528SoC FPGA\u7684\u80fd\u529b\uff0c\u5c06\u786c\u4ef6\u533a\u57df\u7528\u4e8e\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\uff0c\u8f6f\u4ef6\u533a\u57df\u7528\u4e8e\u76d1\u63a7\u548c\u901a\u4fe1\uff0c\u5e76\u6784\u5efa\u4e86X-HEEP-FEMU\u5e73\u53f0\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u90e8\u7f72\u5728Xilinx Zynq-7020 SoC\u4e0a\uff0c\u96c6\u6210\u4e86X-HEEP\u786c\u4ef6\u548cLinux\u8f6f\u4ef6\u73af\u5883\uff0c\u5e76\u91c7\u7528\u4e86\u80fd\u91cf\u6a21\u578b\u3002", "conclusion": "FEMU\u4e3aTinyAI\u5f02\u6784\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16599", "pdf": "https://arxiv.org/pdf/2508.16599", "abs": "https://arxiv.org/abs/2508.16599", "authors": ["Mosh Levy", "Zohar Elyoseph", "Yoav Goldberg"], "title": "Humans Perceive Wrong Narratives from AI Reasoning Texts", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "A new generation of AI models generates step-by-step reasoning text before\nproducing an answer. This text appears to offer a human-readable window into\ntheir computation process, and is increasingly relied upon for transparency and\ninterpretability. However, it is unclear whether human understanding of this\ntext matches the model's actual computational process. In this paper, we\ninvestigate a necessary condition for correspondence: the ability of humans to\nidentify which steps in a reasoning text causally influence later steps. We\nevaluated humans on this ability by composing questions based on counterfactual\nmeasurements and found a significant discrepancy: participant accuracy was only\n29.3%, barely above chance (25%), and remained low (42%) even when evaluating\nthe majority vote on questions with high agreement. Our results reveal a\nfundamental gap between how humans interpret reasoning texts and how models use\nit, challenging its utility as a simple interpretability tool. We argue that\nreasoning texts should be treated as an artifact to be investigated, not taken\nat face value, and that understanding the non-human ways these models use\nlanguage is a critical research direction.", "AI": {"tldr": "AI\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u6587\u672c\u770b\u4f3c\u900f\u660e\uff0c\u4f46\u4eba\u7c7b\u5bf9\u5176\u6b65\u9aa4\u7684\u7406\u89e3\u4e0e\u6a21\u578b\u7684\u5b9e\u9645\u8ba1\u7b97\u8fc7\u7a0b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u51c6\u786e\u6027\u4ec529.3%\uff0c\u8d28\u7591\u5176\u4f5c\u4e3a\u89e3\u91ca\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u662f\u5426\u80fd\u591f\u51c6\u786e\u7406\u89e3AI\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u6587\u672c\uff0c\u4ee5\u9a8c\u8bc1\u5176\u4f5c\u4e3a\u900f\u660e\u548c\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u7684\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u6d4b\u91cf\u7684\u95ee\u5377\uff0c\u8bc4\u4f30\u4eba\u7c7b\u8bc6\u522b\u63a8\u7406\u6587\u672c\u4e2d\u56e0\u679c\u6b65\u9aa4\u7684\u80fd\u529b\u3002", "result": "\u4eba\u7c7b\u8bc6\u522b\u51c6\u786e\u6027\u4ec5\u4e3a29.3%\uff0c\u7565\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\uff0825%\uff09\uff0c\u9ad8\u4e00\u81f4\u6027\u95ee\u9898\u7684\u591a\u6570\u6295\u7968\u51c6\u786e\u7387\u4e5f\u4ec542%\u3002", "conclusion": "\u63a8\u7406\u6587\u672c\u4e0d\u5e94\u88ab\u7b80\u5355\u89c6\u4e3a\u89e3\u91ca\u5de5\u5177\uff0c\u800c\u9700\u6df1\u5165\u7814\u7a76\u5176\u975e\u4eba\u7c7b\u8bed\u8a00\u4f7f\u7528\u65b9\u5f0f\uff0c\u63ed\u793a\u6a21\u578b\u4e0e\u4eba\u7c7b\u7406\u89e3\u7684\u5dee\u5f02\u3002"}}
{"id": "2508.16728", "pdf": "https://arxiv.org/pdf/2508.16728", "abs": "https://arxiv.org/abs/2508.16728", "authors": ["Niladri Gomes", "Gautam Sharma", "Jay Pathak"], "title": "Hamiltonian Simulation for Advection-Diffusion Equation with arbitrary transport field", "categories": ["quant-ph", "cs.ET", "physics.comp-ph"], "comment": "11 pages, 8 figures", "summary": "We present a novel approach to solve the advection-diffusion equation under\narbitrary transporting fields using a quantum-inspired 'Schrodingerisation'\ntechnique for Hamiltonian simulation. Although numerous methods exist for\nsolving partial differential equations (PDEs), Hamiltonian simulation remains a\nrelatively underexplored yet promising direction-particularly in the context of\nlong-term, fault-tolerant quantum computing. Building on this potential, our\nquantum algorithm is designed to accommodate non-trivial, spatially varying\ntransport fields and is applicable to both 2D and 3D advection-diffusion\nproblems. To ensure numerical stability and accuracy, the algorithm combines an\nupwinding discretization scheme for the advective component and the central\ndifferencing for diffusion, adapted for quantum implementation through a\ntailored mix of approximation and optimization techniques. We demonstrate the\nalgorithm's effectiveness on benchmark scenarios involving coupled rotational,\nshear, and diffusive transport in two and three dimensions. Additionally, we\nimplement the 2D advection-diffusion equation using 16 qubits on IBM Quantum\nhardware, validating our method and highlighting its practical applicability\nand robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u542f\u53d1\u7684'Hamiltonian\u6a21\u62df'\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4efb\u610f\u8f93\u8fd0\u573a\u4e0b\u7684\u5e73\u6d41-\u6269\u6563\u65b9\u7a0b\uff0c\u9002\u7528\u4e8e2D\u548c3D\u95ee\u9898\uff0c\u5e76\u5728IBM Quantum\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709PDE\u6c42\u89e3\u65b9\u6cd5\u4e2d\uff0cHamiltonian\u6a21\u62df\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u4f46\u6f5c\u529b\u5de8\u5927\u7684\u65b9\u5411\uff0c\u5c24\u5176\u5728\u957f\u671f\u5bb9\u5fcd\u9519\u8bef\u7684\u91cf\u5b50\u8ba1\u7b97\u80cc\u666f\u4e0b\u3002", "method": "\u7ed3\u5408\u4e0a\u98ce\u5dee\u5206\uff08\u7528\u4e8e\u5e73\u6d41\uff09\u548c\u4e2d\u5fc3\u5dee\u5206\uff08\u7528\u4e8e\u6269\u6563\uff09\uff0c\u5e76\u901a\u8fc7\u8fd1\u4f3c\u548c\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u91cf\u5b50\u5316\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u9002\u5e94\u590d\u6742\u8f93\u8fd0\u573a\u7684\u7b97\u6cd5\u3002", "result": "\u57282D\u548c3D\u8026\u5408\u65cb\u8f6c\u3001\u526a\u5207\u548c\u6269\u6563\u8f93\u8fd0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u572816\u91cf\u5b50\u4f4d\u7684IBM\u786c\u4ef6\u4e0a\u6210\u529f\u5b9e\u73b0\u4e862D\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u91cf\u5b50\u7b97\u6cd5\u5728\u89e3\u51b3\u590d\u6742PDE\u95ee\u9898\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.16809", "pdf": "https://arxiv.org/pdf/2508.16809", "abs": "https://arxiv.org/abs/2508.16809", "authors": ["Saverio Pasqualoni", "Lorenzo Piarulli", "Daniele De Sensi"], "title": "PICO: Performance Insights for Collective Operations", "categories": ["cs.DC", "cs.PF"], "comment": null, "summary": "Collective operations are cornerstones of both HPC application and\nlarge-scale AI training and inference. Yet, comprehensive, systematic and\nreproducible performance evaluation and benchmarking of said operations is not\nstraightforward. Existing frameworks do not provide sufficiently detailed\nprofiling information, nor they ensure reproducibility and extensibility. In\nthis paper, we present PICO (Performance Insights for Collective Operations), a\nnovel lightweight, extensible framework built with the aim of simplifying\ncollective operations benchmarking.", "AI": {"tldr": "PICO\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u65e8\u5728\u7b80\u5316\u96c6\u4f53\u64cd\u4f5c\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5f25\u8865\u73b0\u6709\u6846\u67b6\u5728\u8be6\u7ec6\u5206\u6790\u548c\u53ef\u590d\u73b0\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u96c6\u4f53\u64cd\u4f5c\u662f\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u5927\u89c4\u6a21AI\u8bad\u7ec3\u7684\u57fa\u7840\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u5728\u8be6\u7ec6\u6027\u80fd\u5206\u6790\u548c\u53ef\u590d\u73b0\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86PICO\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7b80\u5316\u96c6\u4f53\u64cd\u4f5c\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "PICO\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u6027\u80fd\u5206\u6790\uff0c\u5e76\u4fdd\u8bc1\u53ef\u590d\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "PICO\u5728\u96c6\u4f53\u64cd\u4f5c\u7684\u6027\u80fd\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u6846\u67b6\u7684\u4e0d\u8db3\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.16860", "pdf": "https://arxiv.org/pdf/2508.16860", "abs": "https://arxiv.org/abs/2508.16860", "authors": ["Md Afif Al Mamun", "Gias Uddin", "Lan Xia", "Longyu Zhang"], "title": "TriagerX: Dual Transformers for Bug Triaging Tasks with Content and Interaction Based Rankings", "categories": ["cs.SE", "cs.AI", "cs.LG"], "comment": "This work is currently under review at IEEE Transactions on Software\n  Engineering. The replication package will be made publicly available upon\n  acceptance", "summary": "Pretrained Language Models or PLMs are transformer-based architectures that\ncan be used in bug triaging tasks. PLMs can better capture token semantics than\ntraditional Machine Learning (ML) models that rely on statistical features\n(e.g., TF-IDF, bag of words). However, PLMs may still attend to less relevant\ntokens in a bug report, which can impact their effectiveness. In addition, the\nmodel can be sub-optimal with its recommendations when the interaction history\nof developers around similar bugs is not taken into account. We designed\nTriagerX to address these limitations. First, to assess token semantics more\nreliably, we leverage a dual-transformer architecture. Unlike current\nstate-of-the-art (SOTA) baselines that employ a single transformer\narchitecture, TriagerX collects recommendations from two transformers with each\noffering recommendations via its last three layers. This setup generates a\nrobust content-based ranking of candidate developers. TriagerX then refines\nthis ranking by employing a novel interaction-based ranking methodology, which\nconsiders developers' historical interactions with similar fixed bugs. Across\nfive datasets, TriagerX surpasses all nine transformer-based methods, including\nSOTA baselines, often improving Top-1 and Top-3 developer recommendation\naccuracy by over 10%. We worked with our large industry partner to successfully\ndeploy TriagerX in their development environment. The partner required both\ndeveloper and component recommendations, with components acting as proxies for\nteam assignments-particularly useful in cases of developer turnover or team\nchanges. We trained TriagerX on the partner's dataset for both tasks, and it\noutperformed SOTA baselines by up to 10% for component recommendations and 54%\nfor developer recommendations.", "AI": {"tldr": "TriagerX \u662f\u4e00\u4e2a\u57fa\u4e8e\u53cc Transformer \u67b6\u6784\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u5185\u5bb9\u6392\u540d\u548c\u5f00\u53d1\u8005\u5386\u53f2\u4ea4\u4e92\u6392\u540d\uff0c\u663e\u8457\u63d0\u5347\u4e86 bug \u6307\u6d3e\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684 PLMs \u5728 bug \u6307\u6d3e\u4efb\u52a1\u4e2d\u53ef\u80fd\u5173\u6ce8\u4e0d\u76f8\u5173\u7684 tokens\uff0c\u5e76\u4e14\u672a\u8003\u8651\u5f00\u53d1\u8005\u7684\u5386\u53f2\u4ea4\u4e92\uff0c\u5bfc\u81f4\u63a8\u8350\u6548\u679c\u4e0d\u4f73\u3002", "method": "TriagerX \u4f7f\u7528\u53cc Transformer \u67b6\u6784\u548c\u4ea4\u4e92\u5f0f\u6392\u540d\u65b9\u6cd5\uff0c\u7ed3\u5408\u5185\u5bb9\u63a8\u8350\u548c\u5f00\u53d1\u8005\u5386\u53f2\u4ea4\u4e92\u6570\u636e\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cTriagerX \u8d85\u8d8a\u4e86\u4e5d\u79cd\u57fa\u4e8e Transformer \u7684\u65b9\u6cd5\uff0cTop-1 \u548c Top-3 \u63a8\u8350\u51c6\u786e\u7387\u63d0\u5347\u4e86 10% \u4ee5\u4e0a\u3002", "conclusion": "TriagerX \u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u63d0\u5347\u4e86\u5f00\u53d1\u8005\u63a8\u8350\u6548\u679c\uff0c\u8fd8\u663e\u8457\u6539\u8fdb\u4e86\u7ec4\u4ef6\u63a8\u8350\u4efb\u52a1\u3002"}}
{"id": "2508.17941", "pdf": "https://arxiv.org/pdf/2508.17941", "abs": "https://arxiv.org/abs/2508.17941", "authors": ["Tamizhelakkiya K", "Dibakar Das", "Komal Sharma", "Jyotsna Bapat", "Debabrata Das"], "title": "Digital Twin Assisted Proactive Management in Zero Touch Networks", "categories": ["cs.NI"], "comment": null, "summary": "The rapid expansion of cellular networks and rising demand for high-quality\nservices require efficient and autonomous network management solutions. Zero\nTouch Network (ZTN) management has emerged as a key approach to automating\nnetwork operations, minimizing manual intervention, and improving service\nreliability. Digital Twin (DT) creates a virtual representation of the physical\nnetwork in realtime, allowing continuous monitoring, predictive analytics, and\nintelligent decision-making by simulating what-if scenarios. This paper\nintegrates DT with ZTN proactive bandwidth management in end-to-end (E2E)\nnext-generation networks. The integrated architecture applies Few-Shot Learning\n(FSL) to a memoryaugmented Bidirectional Long Short Term Memory (BiLSTM) model\nto predict a new network state to augment the known and trained states. Using\nQ-learning, it determines the optimal action (e.g. traffic shaping) under\nvarying network conditions such that user Quality of Service (QoS) requirements\nare met. Three scenarios have been considered: 1) normal ZTN operation with\nclosed-loop control, 2) a what-if scenario of DT, and 3) network state unknown\nto DT. The simulation results show that the network can adapt to underlying\nchanging conditions. In addition, DT-assisted ZTN achieves better performance\nthan the other techniques.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u548c\u96f6\u89e6\u7f51\u7edc\uff08ZTN\uff09\u7684\u67b6\u6784\uff0c\u901a\u8fc7Few-Shot Learning\u548cQ-learning\u4f18\u5316\u7f51\u7edc\u5e26\u5bbd\u7ba1\u7406\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\u5e76\u63d0\u5347\u670d\u52a1\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u8702\u7a9d\u7f51\u7edc\u7684\u5feb\u901f\u6269\u5c55\u548c\u670d\u52a1\u8d28\u91cf\u9700\u6c42\u7684\u63d0\u5347\uff0c\u9700\u8981\u9ad8\u6548\u3001\u81ea\u4e3b\u7684\u7f51\u7edc\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002DT\u548cZTN\u7684\u7ed3\u5408\u53ef\u4ee5\u4e3a\u4e0b\u4e00\u4ee3\u7f51\u7edc\u63d0\u4f9b\u667a\u80fd\u5316\u7ba1\u7406\u3002", "method": "\u63d0\u51fa\u96c6\u6210DT\u4e0eZTN\u7684\u67b6\u6784\uff0c\u4f7f\u7528Few-Shot Learning\u589e\u5f3aBiLSTM\u6a21\u578b\u9884\u6d4b\u7f51\u7edc\u72b6\u6001\uff0c\u5e76\u901a\u8fc7Q-learning\u786e\u5b9a\u6700\u4f18\u52a8\u4f5c\uff08\u5982\u6d41\u91cf\u6574\u5f62\uff09\uff0c\u6ee1\u8db3\u7528\u6237QoS\u9700\u6c42\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u80fd\u9002\u5e94\u7f51\u7edc\u52a8\u6001\u53d8\u5316\uff0c\u4e14\u5728DT\u8f85\u52a9\u4e0bZTN\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u6280\u672f\u3002", "conclusion": "\u7ed3\u5408DT\u7684ZTN\u67b6\u6784\u5728\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.17620", "pdf": "https://arxiv.org/pdf/2508.17620", "abs": "https://arxiv.org/abs/2508.17620", "authors": ["Dingkun Yan", "Xinrui Wang", "Zhuoru Li", "Suguru Saito", "Yusuke Iwasawa", "Yutaka Matsuo", "Jiaxian Guo"], "title": "Enhancing Reference-based Sketch Colorization via Separating Reference Representations", "categories": ["cs.GR"], "comment": null, "summary": "Reference-based sketch colorization methods have garnered significant\nattention for the potential application in animation and digital illustration\nproduction. However, most existing methods are trained with image triplets of\nsketch, reference, and ground truth that are semantically and spatially\nsimilar, while real-world references and sketches often exhibit substantial\nmisalignment. This mismatch in data distribution between training and inference\nleads to overfitting, consequently resulting in artifacts and signif- icant\nquality degradation in colorization results. To address this issue, we conduct\nan in-depth analysis of the reference representations, defined as the\nintermedium to transfer information from reference to sketch. Building on this\nanalysis, we introduce a novel framework that leverages distinct reference\nrepresentations to optimize different aspects of the colorization process. Our\napproach decomposes colorization into modular stages, al- lowing\nregion-specific reference injection to enhance visual quality and reference\nsimilarity while mitigating spatial artifacts. Specifically, we first train a\nbackbone network guided by high-level semantic embeddings. We then introduce a\nbackground encoder and a style encoder, trained in separate stages, to enhance\nlow-level feature transfer and improve reference similar- ity. This design also\nenables flexible inference modes suited for a variety of use cases. Extensive\nqualitative and quantitative evaluations, together with a user study,\ndemonstrate the superior performance of our proposed method compared to\nexisting approaches. Code and pre-trained weight will be made publicly\navailable upon paper acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u53c2\u8003\u7684\u8349\u56fe\u7740\u8272\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u7740\u8272\u8fc7\u7a0b\u5e76\u4f18\u5316\u53c2\u8003\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u8bad\u7ec3\u4e0e\u63a8\u7406\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u548c\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53c2\u8003\u7684\u8349\u56fe\u7740\u8272\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u8bed\u4e49\u548c\u7a7a\u95f4\u5bf9\u9f50\u7684\u56fe\u50cf\u4e09\u5143\u7ec4\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u53c2\u8003\u548c\u8349\u56fe\u5e38\u5b58\u5728\u663e\u8457\u4e0d\u5bf9\u9f50\uff0c\u5bfc\u81f4\u7740\u8272\u7ed3\u679c\u8d28\u91cf\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u7740\u8272\u8fc7\u7a0b\uff0c\u5206\u522b\u8bad\u7ec3\u9ad8\u5c42\u8bed\u4e49\u5d4c\u5165\u3001\u80cc\u666f\u7f16\u7801\u5668\u548c\u98ce\u683c\u7f16\u7801\u5668\uff0c\u4ee5\u4f18\u5316\u4e0d\u540c\u65b9\u9762\u7684\u53c2\u8003\u8868\u793a\u548c\u7740\u8272\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u53ca\u7528\u6237\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u7740\u8272\u8d28\u91cf\u548c\u53c2\u8003\u76f8\u4f3c\u6027\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u591a\u79cd\u7528\u4f8b\uff0c\u5e76\u5728\u7740\u8272\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6743\u91cd\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.18149", "pdf": "https://arxiv.org/pdf/2508.18149", "abs": "https://arxiv.org/abs/2508.18149", "authors": ["Sarah Winkler"], "title": "First-Order LTLf Synthesis with Lookback (Extended Version)", "categories": ["cs.LO"], "comment": "This is an extended version of the paper \"First-Order LTLf Synthesis\n  with Lookback\" accepted for the 28th European Conference on Artificial\n  Intelligence (ECAI-2025)", "summary": "Reactive synthesis addresses the problem of generating a controller for a\ntemporal specification in an adversarial environment; it was typically studied\nfor LTL. Driven by applications ranging from AI to business process management,\nLTL modulo first order-theories over finite traces (LTLfMT) has recently gained\ntraction, where propositional variables in properties are replaced by\nfirst-order constraints. Though reactive synthesis for LTLf with some\nfirst-order features has been addressed, existing work in this direction\nstrongly restricts or excludes the possibility to compare variables across\ninstants, a limitation that severely restricts expressiveness and\napplicability.\n  In this work we present a reactive synthesis procedure for LTLfMT, where\nproperties support \"lookback\" to model cross-instant comparison of variables.\nOur procedure works for full LTLfMT with lookback, subsuming the fragments of\nLTLfMT for which realizability was studied earlier. However, the setting with\ncross-instant comparison is inherently highly complex, as realizability is\nundecidable even over decidable background theories. Hence termination of our\napproach is in general not guaranteed. Nevertheless, we prove its soundness,\nand show that it is complete if a bound on the strategy length exists. Finally,\nwe show that our approach constitutes a decision procedure for several relevant\nfragments of LTLfMT, at once re-proving known decidability results and\nidentifying new decidable classes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u53d8\u91cf\u8de8\u65f6\u523b\u6bd4\u8f83\u7684LTLfMT\u53cd\u5e94\u5f0f\u5408\u6210\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8868\u8fbe\u9650\u5236\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6b63\u786e\u6027\u548c\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u7684\u5b8c\u5907\u6027\u3002", "motivation": "\u4f20\u7edf\u7684LTL\u53cd\u5e94\u5f0f\u5408\u6210\u65b9\u6cd5\u5728\u8868\u8fbe\u53d8\u91cf\u8de8\u65f6\u523b\u6bd4\u8f83\u65f6\u53d7\u9650\uff0c\u800cLTLfMT\u5728AI\u548c\u4e1a\u52a1\u6d41\u7a0b\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u9700\u8981\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u201c\u56de\u987e\u201d\u529f\u80fd\u7684LTLfMT\u53cd\u5e94\u5f0f\u5408\u6210\u65b9\u6cd5\uff0c\u5141\u8bb8\u53d8\u91cf\u8de8\u65f6\u523b\u6bd4\u8f83\uff0c\u4e14\u9002\u7528\u4e8e\u5b8c\u6574\u7684LTLfMT\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6b63\u786e\u6027\uff0c\u5e76\u5728\u7b56\u7565\u957f\u5ea6\u6709\u754c\u65f6\u4e3a\u5b8c\u5907\u7684\uff1b\u540c\u65f6\u8bc6\u522b\u4e86\u65b0\u7684\u53ef\u5224\u5b9a\u7c7b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86LTLfMT\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5c3d\u7ba1\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u4e0d\u53ef\u5224\u5b9a\uff0c\u4f46\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6709\u6548\u3002"}}
{"id": "2508.17518", "pdf": "https://arxiv.org/pdf/2508.17518", "abs": "https://arxiv.org/abs/2508.17518", "authors": ["Thomas Gassmann", "Stefanos Chaliasos", "Thodoris Sotiropoulos", "Zhendong Su"], "title": "Evaluating Compiler Optimization Impacts on zkVM Performance", "categories": ["cs.PF"], "comment": null, "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u7f16\u8bd1\u5668\u4f18\u5316\u5bf9\u96f6\u77e5\u8bc6\u865a\u62df\u673a\uff08zkVM\uff09\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6807\u51c6\u4f18\u5316\u5bf9zkVM\u7684\u6548\u679c\u4e0d\u5982\u4f20\u7edfCPU\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u6027\u6539\u8fdb\u7b56\u7565\uff0c\u63d0\u5347\u4e86zkVM\u6027\u80fd\u3002", "motivation": "\u96f6\u77e5\u8bc6\u865a\u62df\u673a\uff08zkVM\uff09\u964d\u4f4e\u4e86\u5f00\u53d1\u8005\u4f7f\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u95e8\u69db\uff0c\u4f46\u5176\u4f9d\u8d56\u7684\u4f20\u7edf\u7f16\u8bd1\u5668\u4f18\u5316\u5e76\u672a\u8003\u8651\u8bc1\u660e\u7cfb\u7edf\u7684\u7279\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e8664\u79cdLLVM\u7f16\u8bd1\u4f18\u5316\u30016\u79cd\u6807\u51c6\u4f18\u5316\u7ea7\u522b\u548c\u4e00\u4e2a\u672a\u4f18\u5316\u57fa\u7ebf\uff0c\u5e76\u5728\u4e24\u4e2aRISC-V\u67b6\u6784\u7684zkVM\uff08RISC Zero\u548cSP1\uff09\u4e0a\u6d4b\u8bd5\u4e8658\u4e2a\u57fa\u51c6\u7a0b\u5e8f\u3002", "result": "\u6807\u51c6\u4f18\u5316\u5bf9zkVM\u6027\u80fd\u7684\u63d0\u5347\uff0840%\u4ee5\u4e0a\uff09\u4e0d\u5982\u4f20\u7edfCPU\uff0c\u4f46\u901a\u8fc7\u9488\u5bf9\u6027\u6539\u8fdb\u5c11\u91cfLLVM\u4f18\u5316\uff0c\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6267\u884c\u65f6\u95f4\uff08RISC Zero\u5e73\u5747+4.6%\uff0cSP1 +1%\uff09\u3002", "conclusion": "\u7f16\u8bd1\u5668\u4f18\u5316\u5728zkVM\u6027\u80fd\u4e0a\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u63a2\u7d22zkVM\u4e13\u7528\u7684\u4f18\u5316\u7b56\u7565\u548c\u5de5\u5177\u3002"}}
{"id": "2508.17828", "pdf": "https://arxiv.org/pdf/2508.17828", "abs": "https://arxiv.org/abs/2508.17828", "authors": ["Yitong Song", "Pengcheng Zhang", "Chao Gao", "Bin Yao", "Kai Wang", "Zongyuan Wu", "Lin Qu"], "title": "TRIM: Accelerating High-Dimensional Vector Similarity Search with Enhanced Triangle-Inequality-Based Pruning", "categories": ["cs.DB"], "comment": null, "summary": "High-dimensional vector similarity search (HVSS) is critical for many data\nprocessing and AI applications. However, traditional HVSS methods often require\nextensive data access for distance calculations, leading to inefficiencies.\nTriangle-inequality-based lower bound pruning is a widely used technique to\nreduce the number of data access in low-dimensional spaces but becomes less\neffective in high-dimensional settings. This is attributed to the \"distance\nconcentration\" phenomenon, where the lower bounds derived from the triangle\ninequality become too small to be useful. To address this, we propose TRIM,\nwhich enhances the effectiveness of traditional triangle-inequality-based\npruning in high-dimensional vector similarity search using two key ways: (1)\noptimizing landmark vectors used to form the triangles, and (2) relaxing the\nlower bounds derived from the triangle inequality, with the relaxation degree\nadjustable according to user's needs. TRIM is a versatile operation that can be\nseamlessly integrated into both memory-based (e.g., HNSW, IVFPQ) and disk-based\n(e.g., DiskANN) HVSS methods, reducing distance calculations and disk access.\nExtensive experiments show that TRIM enhances memory-based methods, improving\ngraph-based search by up to 90% and quantization-based search by up to 200%,\nwhile achieving a pruning ratio of up to 99%. It also reduces I/O costs by up\nto 58% and improves efficiency by 102% for disk-based methods, while preserving\nhigh query accuracy.", "AI": {"tldr": "TRIM\u901a\u8fc7\u4f18\u5316\u4e09\u89d2\u5f62\u4e0d\u7b49\u5f0f\u7684\u526a\u679d\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u9ad8\u7ef4\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5185\u5b58\u548c\u78c1\u76d8\u5b58\u50a8\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u548c\u8bbf\u95ee\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u9ad8\u7ef4\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u65b9\u6cd5\u56e0\u6570\u636e\u8bbf\u95ee\u548c\u8ddd\u79bb\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u4e09\u89d2\u5f62\u4e0d\u7b49\u5f0f\u526a\u679d\u5728\u9ad8\u7ef4\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "TRIM\u901a\u8fc7\u4f18\u5316\u6807\u5fd7\u5411\u91cf\u548c\u653e\u5bbd\u4e09\u89d2\u5f62\u4e0d\u7b49\u5f0f\u4e0b\u754c\uff0c\u52a8\u6001\u8c03\u8282\u526a\u679d\u6548\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b58\u50a8\u65b9\u5f0f\u7684\u76f8\u4f3c\u6027\u641c\u7d22\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTRIM\u663e\u8457\u63d0\u5347\u57fa\u4e8e\u56fe\u548c\u91cf\u5316\u7684\u641c\u7d22\u6548\u7387\uff0c\u526a\u679d\u6bd4\u4f8b\u9ad8\u8fbe99%\uff0c\u540c\u65f6\u51cf\u5c11I/O\u5f00\u9500\u5e76\u4fdd\u6301\u9ad8\u67e5\u8be2\u7cbe\u5ea6\u3002", "conclusion": "TRIM\u662f\u901a\u7528\u4e14\u9ad8\u6548\u7684\u526a\u679d\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5b58\u50a8\u65b9\u6848\u3002"}}
{"id": "2508.17069", "pdf": "https://arxiv.org/pdf/2508.17069", "abs": "https://arxiv.org/abs/2508.17069", "authors": ["Mengyuan Yin", "Benjamin Chen Ming Choong", "Chuping Qu", "Rick Siow Mong Goh", "Weng-Fai Wong", "Tao Luo"], "title": "Optimizing Neural Networks with Learnable Non-Linear Activation Functions via Lookup-Based FPGA Acceleration", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Learned activation functions in models like Kolmogorov-Arnold Networks (KANs)\noutperform fixed-activation architectures in terms of accuracy and\ninterpretability; however, their computational complexity poses critical\nchallenges for energy-constrained edge AI deployments. Conventional CPUs/GPUs\nincur prohibitive latency and power costs when evaluating higher order\nactivations, limiting deployability under ultra-tight energy budgets. We\naddress this via a reconfigurable lookup architecture with edge FPGAs. By\ncoupling fine-grained quantization with adaptive lookup tables, our design\nminimizes energy-intensive arithmetic operations while preserving activation\nfidelity. FPGA reconfigurability enables dynamic hardware specialization for\nlearned functions, a key advantage for edge systems that require\npost-deployment adaptability. Evaluations using KANs - where unique activation\nfunctions play a critical role - demonstrate that our FPGA-based design\nachieves superior computational speed and over $10^4$ times higher energy\nefficiency compared to edge CPUs and GPUs, while maintaining matching accuracy\nand minimal footprint overhead. This breakthrough positions our approach as a\npractical enabler for energy-critical edge AI, where computational intensity\nand power constraints traditionally preclude the use of adaptive activation\nnetworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFPGA\u7684\u53ef\u91cd\u6784\u67e5\u627e\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u8fd0\u884c\u5b66\u4e60\u578b\u6fc0\u6d3b\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18AI\u7684\u80fd\u6548\u548c\u901f\u5ea6\u3002", "motivation": "\u5b66\u4e60\u578b\u6fc0\u6d3b\u51fd\u6570\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u56fa\u5b9a\u6fc0\u6d3b\u51fd\u6570\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5728\u80fd\u6e90\u53d7\u9650\u7684\u8fb9\u7f18AI\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u7ec6\u7c92\u5ea6\u91cf\u5316\u548c\u81ea\u9002\u5e94\u67e5\u627e\u8868\u51cf\u5c11\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\uff0c\u5229\u7528FPGA\u7684\u53ef\u91cd\u6784\u6027\u52a8\u6001\u9002\u914d\u5b66\u4e60\u578b\u51fd\u6570\u3002", "result": "\u5728KANs\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u8bbe\u8ba1\u6bd4\u8fb9\u7f18CPU/GPU\u5feb10^4\u500d\u80fd\u6548\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u548c\u4f4e\u8d44\u6e90\u5360\u7528\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u80fd\u6e90\u654f\u611f\u7684\u8fb9\u7f18AI\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u81ea\u9002\u5e94\u6fc0\u6d3b\u7f51\u7edc\u7684\u9650\u5236\u3002"}}
{"id": "2508.16602", "pdf": "https://arxiv.org/pdf/2508.16602", "abs": "https://arxiv.org/abs/2508.16602", "authors": ["Hsuan-Kung Yang", "Tsu-Ching Hsiao", "Ryoichiro Oka", "Ryuya Nishino", "Satoko Tofukuji", "Norimasa Kobori"], "title": "An Embodied AR Navigation Agent: Integrating BIM with Retrieval-Augmented Generation for Language Guidance", "categories": ["cs.HC", "cs.AI"], "comment": "11 pages, 9 figures, accepted to IEEE ISMAR 2025", "summary": "Delivering intelligent and adaptive navigation assistance in augmented\nreality (AR) requires more than visual cues, as it demands systems capable of\ninterpreting flexible user intent and reasoning over both spatial and semantic\ncontext. Prior AR navigation systems often rely on rigid input schemes or\npredefined commands, which limit the utility of rich building data and hinder\nnatural interaction. In this work, we propose an embodied AR navigation system\nthat integrates Building Information Modeling (BIM) with a multi-agent\nretrieval-augmented generation (RAG) framework to support flexible,\nlanguage-driven goal retrieval and route planning. The system orchestrates\nthree language agents, Triage, Search, and Response, built on large language\nmodels (LLMs), which enables robust interpretation of open-ended queries and\nspatial reasoning using BIM data. Navigation guidance is delivered through an\nembodied AR agent, equipped with voice interaction and locomotion, to enhance\nuser experience. A real-world user study yields a System Usability Scale (SUS)\nscore of 80.5, indicating excellent usability, and comparative evaluations show\nthat the embodied interface can significantly improves users' perception of\nsystem intelligence. These results underscore the importance and potential of\nlanguage-grounded reasoning and embodiment in the design of user-centered AR\nnavigation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBIM\u548c\u591a\u4ee3\u7406RAG\u6846\u67b6\u7684AR\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u8a00\u4ee3\u7406\u548cAR\u4ee3\u7406\u5b9e\u73b0\u7075\u6d3b\u5bfc\u822a\u548c\u4ea4\u4e92\u3002", "motivation": "\u4f20\u7edfAR\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u56fa\u5b9a\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5efa\u7b51\u6570\u636e\u7684\u5229\u7528\u548c\u81ea\u7136\u4ea4\u4e92\u3002", "method": "\u7ed3\u5408BIM\u4e0e\u591a\u4ee3\u7406RAG\u6846\u67b6\uff0c\u4f7f\u7528\u4e09\u4e2a\u8bed\u8a00\u4ee3\u7406\uff08Triage\u3001Search\u3001Response\uff09\u548cAR\u4ee3\u7406\u5b9e\u73b0\u5bfc\u822a\u3002", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\u7cfb\u7edfSUS\u5f97\u5206\u4e3a80.5\uff0c\u4e14AR\u4ee3\u7406\u663e\u8457\u63d0\u5347\u7528\u6237\u5bf9\u7cfb\u7edf\u667a\u80fd\u7684\u611f\u77e5\u3002", "conclusion": "\u8bed\u8a00\u9a71\u52a8\u7684\u63a8\u7406\u548c\u5177\u4f53\u5316\u8bbe\u8ba1\u5bf9\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AR\u5bfc\u822a\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.16895", "pdf": "https://arxiv.org/pdf/2508.16895", "abs": "https://arxiv.org/abs/2508.16895", "authors": ["Skylar Chan", "Wilson Smith", "Kyla Gabriel"], "title": "Quantum State Fidelity for Functional Neural Network Construction", "categories": ["quant-ph", "cs.ET", "cs.NE", "math.MG", "q-bio.NC", "92C20 (Primary), 81P40", "I.5.3; G.2.2"], "comment": "4 pages, 4 figures, 1 table", "summary": "Neuroscientists face challenges in analyzing high-dimensional neural\nrecording data of dense functional networks. Without ground-truth reference\ndata, finding the best algorithm for recovering neurologically relevant\nnetworks remains an open question. We implemented hybrid quantum algorithms to\nconstruct functional networks and compared them with the results of documented\nclassical techniques. We demonstrated that our quantum state fidelity can\nprovide a competitive alternative to classical metrics by revealing distinct\nfunctional networks. Our results suggest that quantum computing offers a viable\nand potentially advantageous alternative for data-driven modeling in\nneuroscience, underscoring its broader applicability in high-dimensional graph\ninference and complex system analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u7b97\u6cd5\u7684\u529f\u80fd\u7f51\u7edc\u6784\u5efa\u65b9\u6cd5\uff0c\u4e0e\u4f20\u7edf\u7ecf\u5178\u65b9\u6cd5\u76f8\u6bd4\uff0c\u91cf\u5b50\u72b6\u6001\u4fdd\u771f\u5ea6\u80fd\u591f\u63ed\u793a\u4e0d\u540c\u7684\u529f\u80fd\u7f51\u7edc\uff0c\u8bc1\u660e\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u7684\u6f5c\u529b\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u7684\u53c2\u8003\u6570\u636e\uff0c\u5982\u4f55\u5728\u9ad8\u7ef4\u795e\u7ecf\u8bb0\u5f55\u6570\u636e\u4e2d\u6062\u590d\u5177\u6709\u795e\u7ecf\u5b66\u610f\u4e49\u7684\u529f\u80fd\u7f51\u7edc\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u91cf\u5b50\u7b97\u6cd5\u88ab\u8ba4\u4e3a\u53ef\u80fd\u4e3a\u6b64\u63d0\u4f9b\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u5b9e\u73b0\u4e86\u6df7\u5408\u91cf\u5b50\u7b97\u6cd5\u6765\u6784\u5efa\u529f\u80fd\u7f51\u7edc\uff0c\u5e76\u4e0e\u6587\u732e\u4e2d\u8bb0\u5f55\u7684\u7ecf\u5178\u6280\u672f\u7ed3\u679c\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "\u91cf\u5b50\u72b6\u6001\u4fdd\u771f\u5ea6\u80fd\u591f\u63ed\u793a\u4e0d\u540c\u7684\u529f\u80fd\u7f51\u7edc\uff0c\u8868\u73b0\u4e0e\u4f20\u7edf\u7ecf\u5178\u65b9\u6cd5\u76f8\u5f53\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u65b9\u9762\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u4e3a\u795e\u7ecf\u79d1\u5b66\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u53ef\u80fd\u66f4\u5177\u4f18\u52bf\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5176\u5728\u9ad8\u7ef4\u56fe\u63a8\u65ad\u548c\u590d\u6742\u7cfb\u7edf\u5206\u6790\u4e2d\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.17209", "pdf": "https://arxiv.org/pdf/2508.17209", "abs": "https://arxiv.org/abs/2508.17209", "authors": ["Yebo Wu", "Jingguang Li", "Chunlin Tian", "Zhijiang Guo", "Li Li"], "title": "Memory-Efficient Federated Fine-Tuning of Large Language Models via Layer Pruning", "categories": ["cs.DC"], "comment": null, "summary": "Federated fine-tuning enables privacy-preserving Large Language Model (LLM)\nadaptation, but its high memory cost limits participation from\nresource-constrained devices. We propose FedPruner, an innovative federated\nfine-tuning paradigm that tackles this via intelligent layer pruning. FedPruner\nflexibly prunes the global model, creating personalized submodels based on\ndevice memory constraints. It employs a macro-micro synergistic pruning\nframework: a macro-level functionality-driven layer orchestration mechanism\ngroups layers, while a micro-level importance-aware layer selection strategy\nprunes within groups to build device-specific submodels. We further introduce a\nfine-grained variant that independently prunes Multi-Head Attention and\nFeed-Forward Network components to precisely preserve critical architectural\nelements. Extensive experimental results demonstrate that FedPruner\nsignificantly outperforms state-of-the-art approaches, achieving up to a 1.98\\%\nimprovement in average model accuracy while reducing peak memory usage by 75\\%.", "AI": {"tldr": "FedPruner\u662f\u4e00\u79cd\u521b\u65b0\u7684\u8054\u90a6\u5fae\u8c03\u8303\u5f0f\uff0c\u901a\u8fc7\u667a\u80fd\u5c42\u4fee\u526a\u964d\u4f4e\u5185\u5b58\u6210\u672c\uff0c\u4f7f\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e5f\u80fd\u53c2\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684LLM\u9002\u5e94\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5fae\u8c03\u4e2d\u9ad8\u5185\u5b58\u6210\u672c\u9650\u5236\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u53c2\u4e0e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5b8f-\u5fae\u534f\u540c\u4fee\u526a\u6846\u67b6\uff1a\u5b8f\u529f\u80fd\u9a71\u52a8\u5c42\u7f16\u6392\u673a\u5236\u5206\u7ec4\u5c42\uff0c\u5fae\u89c2\u91cd\u8981\u6027\u611f\u77e5\u7b56\u7565\u4fee\u526a\u7ec4\u5185\u5c42\uff0c\u6784\u5efa\u8bbe\u5907\u7279\u5b9a\u5b50\u6a21\u578b\u3002", "result": "\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u6a21\u578b\u7cbe\u5ea6\u63d0\u53471.98%\uff0c\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1175%\u3002", "conclusion": "FedPruner\u9ad8\u6548\u5e73\u8861\u4e86\u6a21\u578b\u7cbe\u5ea6\u4e0e\u8d44\u6e90\u6d88\u8017\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u8bbe\u5907\u591a\u6837\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16903", "pdf": "https://arxiv.org/pdf/2508.16903", "abs": "https://arxiv.org/abs/2508.16903", "authors": ["Yijun Lu", "Hironori Washizaki", "Naoyasu Ubayashi", "Nobukazu Yoshioka", "Chenhao Wu", "Masanari Kondo", "Yuyin Ma", "Jiong Dong", "Jianjin Zhao", "Dongqi Han"], "title": "Mind the Gap: A Decade-Scale Empirical Study of Multi-Stakeholder Dynamics in VR Ecosystem", "categories": ["cs.SE"], "comment": null, "summary": "In the development and evolution of VR ecosystem, platform stakeholders\ncontinuously adapt their products in response to user and technical feedback,\noften reflected in subtle shifts in discussion topics or system updates. A\ncomprehensive understanding of these changes is essential for identifying gaps\nbetween user expectations and developer actions, which can guide more effective\nquality assurance and user-centered innovation. While previous studies have\nanalyzed either user reviews or developer discussions in isolation, such\napproaches typically fail to reveal how specific user concerns are (or are not)\naddressed by corresponding technical activities. To address this limitation,\nour study introduces a multi-view empirical framework that systematically\ncompares and aligns stakeholder perspectives. By applying topic modeling and\nquantitative impact analysis to 944,320 user reviews and 389,477 developer\nposts, we identify not only the overlap in concerns (e.g., performance, input\nmethods), but also clear gaps in areas like inclusivity and community safety\n(e.g., LGBTQ+ representation, child-friendly content). Our findings show that\nwhile users repeatedly raise such issues, they are rarely discussed in\ndeveloper forums. These insights enable data-driven recommendations for closing\nthe user-developer gap in VR ecosystems, offering practical implications for\nplatform governance and the design of next-generation VR systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u89c6\u89d2\u5b9e\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u7528\u6237\u8bc4\u8bba\u548c\u5f00\u53d1\u8005\u8ba8\u8bba\uff0c\u53d1\u73b0\u7528\u6237\u5728\u5305\u5bb9\u6027\u548c\u793e\u533a\u5b89\u5168\u7b49\u65b9\u9762\u7684\u9700\u6c42\u672a\u88ab\u5f00\u53d1\u8005\u5145\u5206\u5173\u6ce8\u3002", "motivation": "VR\u751f\u6001\u7cfb\u7edf\u4e2d\u7528\u6237\u671f\u671b\u4e0e\u5f00\u53d1\u8005\u884c\u52a8\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u5206\u6790\u6765\u6307\u5bfc\u8d28\u91cf\u4fdd\u8bc1\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u521b\u65b0\u3002", "method": "\u91c7\u7528\u4e3b\u9898\u5efa\u6a21\u548c\u5b9a\u91cf\u5f71\u54cd\u5206\u6790\uff0c\u7cfb\u7edf\u6bd4\u8f83\u548c\u5339\u914d\u7528\u6237\u8bc4\u8bba\u4e0e\u5f00\u53d1\u8005\u5e16\u5b50\u7684\u89c6\u89d2\u3002", "result": "\u53d1\u73b0\u7528\u6237\u548c\u5f00\u53d1\u8005\u5728\u6027\u80fd\u548c\u8f93\u5165\u65b9\u6cd5\u7b49\u65b9\u9762\u6709\u91cd\u53e0\u5173\u6ce8\uff0c\u4f46\u5728\u5305\u5bb9\u6027\u548c\u793e\u533a\u5b89\u5168\u65b9\u9762\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3aVR\u5e73\u53f0\u7684\u6cbb\u7406\u548c\u4e0b\u4e00\u4ee3\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u5efa\u8bae\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u7528\u6237\u4e0e\u5f00\u53d1\u8005\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.17990", "pdf": "https://arxiv.org/pdf/2508.17990", "abs": "https://arxiv.org/abs/2508.17990", "authors": ["Wenlong Ding", "Jianqiang Li", "Zhixiong Niu", "Huangxun Chen", "Yongqiang Xiong", "Hong Xu"], "title": "Automating Conflict-Aware ACL Configurations with Natural Language Intents", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "ACL configuration is essential for managing network flow reachability, yet\nits complexity grows significantly with topologies and pre-existing rules. To\ncarry out ACL configuration, the operator needs to (1) understand the new\nconfiguration policies or intents and translate them into concrete ACL rules,\n(2) check and resolve any conflicts between the new and existing rules, and (3)\ndeploy them across the network. Existing systems rely heavily on manual efforts\nfor these tasks, especially for the first two, which are tedious, error-prone,\nand impractical to scale.\n  We propose Xumi to tackle this problem. Leveraging LLMs with domain knowledge\nof the target network, Xumi automatically and accurately translates the natural\nlanguage intents into complete ACL rules to reduce operators' manual efforts.\nXumi then detects all potential conflicts between new and existing rules and\ngenerates resolved intents for deployment with operators' guidance, and finally\nidentifies the best deployment plan that minimizes the rule additions while\nsatisfying all intents. Evaluation shows that Xumi accelerates the entire\nconfiguration pipeline by over 10x compared to current practices, addresses\nO(100) conflicting ACLs and reduces rule additions by ~40% in modern cloud\nnetwork.", "AI": {"tldr": "Xumi\u5229\u7528LLM\u81ea\u52a8\u7ffb\u8bd1\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u4e3aACL\u89c4\u5219\uff0c\u51cf\u5c11\u4eba\u5de5\u64cd\u4f5c\uff0c\u52a0\u901f\u914d\u7f6e\u6d41\u7a0b\u5e76\u51cf\u5c11\u51b2\u7a81\u3002", "motivation": "\u73b0\u6709ACL\u914d\u7f6e\u4f9d\u8d56\u4eba\u5de5\uff0c\u590d\u6742\u4e14\u6613\u9519\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "Xumi\u7ed3\u5408LLM\u81ea\u52a8\u7ffb\u8bd1\u610f\u56fe\u3001\u68c0\u6d4b\u51b2\u7a81\u5e76\u751f\u6210\u90e8\u7f72\u8ba1\u5212\u3002", "result": "\u914d\u7f6e\u901f\u5ea6\u63d0\u534710\u500d\uff0c\u51b2\u7a81\u51cf\u5c1140%\u3002", "conclusion": "Xumi\u663e\u8457\u63d0\u5347ACL\u914d\u7f6e\u6548\u7387\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u4e91\u7f51\u7edc\u3002"}}
{"id": "2508.17645", "pdf": "https://arxiv.org/pdf/2508.17645", "abs": "https://arxiv.org/abs/2508.17645", "authors": ["Xiaoyang Huang", "Bingbing Ni", "Wenjun Zhang"], "title": "Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph", "categories": ["cs.GR"], "comment": null, "summary": "The emergence of 3D artificial intelligence-generated content (3D-AIGC) has\nenabled rapid synthesis of intricate geometries. However, a fundamental\ndisconnect persists between AI-generated content and human-centric design\nparadigms, rooted in representational incompatibilities: conventional AI\nframeworks predominantly manipulate meshes or neural representations\n(\\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate within\nparametric modeling tools. This disconnection diminishes the practical value of\nAI for 3D industry, undermining the efficiency of human-AI collaboration. To\nresolve this disparity, we focus on generating design operation sequences,\nwhich are structured modeling histories that comprehensively capture the\nstep-by-step construction process of 3D assets and align with designers'\ntypical workflows in modern 3D software. We first reformulate fundamental\nmodeling operations (\\emph{e.g.}, \\emph{Extrude}, \\emph{Boolean}) into\ndifferentiable units, enabling joint optimization of continuous (\\emph{e.g.},\n\\emph{Extrude} height) and discrete (\\emph{e.g.}, \\emph{Boolean} type)\nparameters via gradient-based learning. Based on these differentiable\noperations, a hierarchical graph with gating mechanism is constructed and\noptimized end-to-end by minimizing Chamfer Distance to target geometries.\nMulti-stage sequence length constraint and domain rule penalties enable\nunsupervised learning of compact design sequences without ground-truth sequence\nsupervision. Extensive validation demonstrates that the generated operation\nsequences achieve high geometric fidelity, smooth mesh wiring, rational step\ncomposition and flexible editing capacity, with full compatibility within\ndesign industry.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06AI\u751f\u6210\u76843D\u5185\u5bb9\u4e0e\u8bbe\u8ba1\u5e08\u5de5\u4f5c\u6d41\u7a0b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u5fae\u5206\u7684\u5efa\u6a21\u64cd\u4f5c\u5e8f\u5217\uff0c\u89e3\u51b3\u4e86AI\u4e0e\u8bbe\u8ba1\u5de5\u5177\u4e4b\u95f4\u7684\u4e0d\u517c\u5bb9\u95ee\u9898\u3002", "motivation": "\u5f53\u524dAI\u751f\u6210\u76843D\u5185\u5bb9\u4e0e\u8bbe\u8ba1\u5e08\u4f7f\u7528\u7684\u53c2\u6570\u5316\u5efa\u6a21\u5de5\u5177\uff08\u5982\u7f51\u683c\u6216\u795e\u7ecf\u8868\u793a\uff09\u4e4b\u95f4\u5b58\u5728\u4e0d\u517c\u5bb9\u6027\uff0c\u8fd9\u9650\u5236\u4e86AI\u57283D\u8bbe\u8ba1\u884c\u4e1a\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bba\u6587\u5c06\u57fa\u672c\u5efa\u6a21\u64cd\u4f5c\uff08\u5982\u6324\u51fa\u3001\u5e03\u5c14\u8fd0\u7b97\uff09\u8f6c\u5316\u4e3a\u53ef\u5fae\u5206\u5355\u5143\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5206\u5c42\u7684\u56fe\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u6765\u751f\u6210\u8bbe\u8ba1\u64cd\u4f5c\u5e8f\u5217\uff0c\u65e0\u9700\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u751f\u6210\u7684\u5e8f\u5217\u5728\u51e0\u4f55\u4fdd\u771f\u5ea6\u3001\u7f51\u683c\u5e03\u7ebf\u3001\u6b65\u9aa4\u7ec4\u6210\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5b8c\u5168\u517c\u5bb9\u8bbe\u8ba1\u884c\u4e1a\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5f25\u5408\u4e86AI\u4e0e\u8bbe\u8ba1\u5de5\u5177\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u63d0\u5347\u4e86\u4eba\u673a\u534f\u4f5c\u6548\u7387\uff0c\u4e3a3D-AIGC\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.18231", "pdf": "https://arxiv.org/pdf/2508.18231", "abs": "https://arxiv.org/abs/2508.18231", "authors": ["Anjo Seidel", "Sarah Winkler", "Alessandro Gianola", "Marco Montali", "Mathias Weske"], "title": "To bind or not to bind? Discovering Stable Relationships in Object-centric Processes (Extended Version)", "categories": ["cs.LO", "cs.MA", "cs.PL"], "comment": null, "summary": "Object-centric process mining investigates the intertwined behavior of\nmultiple objects in business processes. From object-centric event logs,\nobject-centric Petri nets (OCPN) can be discovered to replay the behavior of\nprocesses accessing different object types. Although they indicate how objects\nflow through the process and co-occur in events, OCPNs remain underspecified\nabout the relationships of objects. Hence, they are not able to represent\nsynchronization, i.e. executing objects only according to their intended\nrelationships, and fail to identify violating executions. Existing formal\nmodeling approaches, such as object-centric Petri nets with identifiers (OPID),\nrepresent object identities and relationships to synchronize them correctly.\nHowever, OPID discovery has not yet been studied. This paper uses explicit data\nmodels to bridge the gap between OCPNs and formal OPIDs. We identify the\nimplicit assumptions of stable many-to-one relationships in object-centric\nevent logs, which implies synchronization of related objects. To formally\nunderpin this observation, we combine OCPNs with explicit stable many-to-one\nrelationships in a rigorous mapping from OCPNs to OPIDs explicitly capturing\nthe intended stable relationships and the synchronization of related objects.\nWe prove that the original OCPNs and the resulting OPIDs coincide for those\nexecutions that satisfy the intended relationships. Moreover, we provide an\nimplementation of the mapping from OCPN to OPID under stable relationships.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684Petri\u7f51\uff08OCPN\uff09\u4e0e\u660e\u786e\u7684\u7a33\u5b9a\u591a\u5bf9\u4e00\u5173\u7cfb\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u6620\u5c04\u5230\u5177\u6709\u6807\u8bc6\u7b26\u7684\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684Petri\u7f51\uff08OPID\uff09\uff0c\u4ee5\u89e3\u51b3\u5bf9\u8c61\u5173\u7cfb\u540c\u6b65\u95ee\u9898\u3002", "motivation": "\u73b0\u6709OCPN\u65e0\u6cd5\u8868\u793a\u5bf9\u8c61\u95f4\u7684\u540c\u6b65\u5173\u7cfb\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8bc6\u522b\u8fdd\u53cd\u610f\u56fe\u7684\u6267\u884c\u884c\u4e3a\u3002OPID\u867d\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5176\u53d1\u73b0\u65b9\u6cd5\u5c1a\u672a\u88ab\u7814\u7a76\u3002", "method": "\u7ed3\u5408OCPN\u4e0e\u660e\u786e\u7684\u7a33\u5b9a\u591a\u5bf9\u4e00\u5173\u7cfb\uff0c\u63d0\u51fa\u4eceOCPN\u5230OPID\u7684\u4e25\u683c\u6620\u5c04\uff0c\u786e\u4fdd\u540c\u6b65\u76f8\u5173\u5bf9\u8c61\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u6ee1\u8db3\u610f\u56fe\u5173\u7cfb\u65f6\uff0c\u539f\u59cbOCPN\u4e0e\u751f\u6210OPID\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u8be5\u6620\u5c04\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u8c61\u5173\u7cfb\u540c\u6b65\u95ee\u9898\uff0c\u4e3a\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u6d41\u7a0b\u6316\u6398\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5efa\u6a21\u5de5\u5177\u3002"}}
{"id": "2508.18162", "pdf": "https://arxiv.org/pdf/2508.18162", "abs": "https://arxiv.org/abs/2508.18162", "authors": ["Eric Alsmann", "Martin Lange"], "title": "The Computational Complexity of Satisfiability in State Space Models", "categories": ["cs.LO", "cs.AI", "cs.CC", "cs.LG"], "comment": "Accepted at ECAI 25", "summary": "We analyse the complexity of the satisfiability problem ssmSAT for State\nSpace Models (SSM), which asks whether an input sequence can lead the model to\nan accepting configuration. We find that ssmSAT is undecidable in general,\nreflecting the computational power of SSM. Motivated by practical settings, we\nidentify two natural restrictions under which ssmSAT becomes decidable and\nestablish corresponding complexity bounds. First, for SSM with bounded context\nlength, ssmSAT is NP-complete when the input length is given in unary and in\nNEXPTIME (and PSPACE-hard) when the input length is given in binary. Second,\nfor quantised SSM operating over fixed-width arithmetic, ssmSAT is\nPSPACE-complete resp. in EXPSPACE depending on the bit-width encoding. While\nthese results hold for diagonal gated SSM we also establish complexity bounds\nfor time-invariant SSM. Our results establish a first complexity landscape for\nformal reasoning in SSM and highlight fundamental limits and opportunities for\nthe verification of SSM-based language models.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u6ee1\u8db3\u6027\u95ee\u9898\uff08ssmSAT\uff09\u7684\u590d\u6742\u6027\uff0c\u53d1\u73b0\u4e00\u822c\u60c5\u51b5\u4e0b\u95ee\u9898\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\uff0c\u4f46\u5728\u4e24\u79cd\u9650\u5236\u6761\u4ef6\u4e0b\u53ef\u5224\u5b9a\uff0c\u5e76\u7ed9\u51fa\u4e86\u76f8\u5e94\u7684\u590d\u6742\u5ea6\u754c\u9650\u3002", "motivation": "\u7814\u7a76SSM\u6a21\u578b\u7684\u6ee1\u8db3\u6027\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u4e3aSSM\u6a21\u578b\u7684\u9a8c\u8bc1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5206\u6790SSM\u6a21\u578b\u7684\u6ee1\u8db3\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e24\u79cd\u81ea\u7136\u9650\u5236\u6761\u4ef6\uff0c\u5e76\u5206\u522b\u7814\u7a76\u5176\u590d\u6742\u5ea6\u3002", "result": "\u53d1\u73b0ssmSAT\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u4e0d\u53ef\u5224\u5b9a\uff0c\u4f46\u5728\u6709\u9650\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u56fa\u5b9a\u5bbd\u5ea6\u7b97\u672f\u4e0b\u53ef\u5224\u5b9a\uff0c\u5206\u522b\u5bf9\u5e94NP\u5b8c\u5168\u548cPSPACE\u5b8c\u5168\u590d\u6742\u5ea6\u3002", "conclusion": "\u7814\u7a76\u4e3aSSM\u6a21\u578b\u7684\u6b63\u5f0f\u63a8\u7406\u5efa\u7acb\u4e86\u9996\u4e2a\u590d\u6742\u6027\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u9a8c\u8bc1\u4e2d\u7684\u57fa\u672c\u9650\u5236\u4e0e\u673a\u4f1a\u3002"}}
{"id": "2508.17025", "pdf": "https://arxiv.org/pdf/2508.17025", "abs": "https://arxiv.org/abs/2508.17025", "authors": ["Liping Xie", "Yang Tan", "Shicheng Jing", "Huimin Lu", "Kanjian Zhang"], "title": "Probabilistic Temporal Masked Attention for Cross-view Online Action Detection", "categories": ["cs.CV", "cs.MM"], "comment": "12 pages, 6 figures, accepted at IEEE Transactions on Multimedia\n  (TMM), in press", "summary": "As a critical task in video sequence classification within computer vision,\nOnline Action Detection (OAD) has garnered significant attention. The\nsensitivity of mainstream OAD models to varying video viewpoints often hampers\ntheir generalization when confronted with unseen sources. To address this\nlimitation, we propose a novel Probabilistic Temporal Masked Attention (PTMA)\nmodel, which leverages probabilistic modeling to derive latent compressed\nrepresentations of video frames in a cross-view setting. The PTMA model\nincorporates a GRU-based temporal masked attention (TMA) cell, which leverages\nthese representations to effectively query the input video sequence, thereby\nenhancing information interaction and facilitating autoregressive frame-level\nvideo analysis. Additionally, multi-view information can be integrated into the\nprobabilistic modeling to facilitate the extraction of view-invariant features.\nExperiments conducted under three evaluation protocols: cross-subject (cs),\ncross-view (cv), and cross-subject-view (csv) show that PTMA achieves\nstate-of-the-art performance on the DAHLIA, IKEA ASM, and Breakfast datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6982\u7387\u65f6\u95f4\u63a9\u7801\u6ce8\u610f\u529b\uff08PTMA\uff09\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u63d0\u53d6\u8de8\u89c6\u89d2\u89c6\u9891\u5e27\u7684\u6f5c\u5728\u538b\u7f29\u8868\u793a\uff0c\u63d0\u5347\u5728\u7ebf\u52a8\u4f5c\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3b\u6d41\u5728\u7ebf\u52a8\u4f5c\u68c0\u6d4b\u6a21\u578b\u5bf9\u89c6\u89d2\u53d8\u5316\u7684\u654f\u611f\u6027\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u9762\u5bf9\u672a\u89c1\u6570\u636e\u6e90\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u57fa\u4e8eGRU\u7684\u65f6\u95f4\u63a9\u7801\u6ce8\u610f\u529b\uff08TMA\uff09\u5355\u5143\uff0c\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u751f\u6210\u8de8\u89c6\u89d2\u7684\u89c6\u9891\u5e27\u538b\u7f29\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u591a\u89c6\u89d2\u4fe1\u606f\u63d0\u53d6\u89c6\u89d2\u4e0d\u53d8\u7279\u5f81\u3002", "result": "\u5728DAHLIA\u3001IKEA ASM\u548cBreakfast\u6570\u636e\u96c6\u4e0a\uff0cPTMA\u6a21\u578b\u5728\u8de8\u4e3b\u4f53\u3001\u8de8\u89c6\u89d2\u548c\u8de8\u4e3b\u4f53-\u89c6\u89d2\u4e09\u79cd\u8bc4\u4f30\u534f\u8bae\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "PTMA\u6a21\u578b\u901a\u8fc7\u6982\u7387\u5efa\u6a21\u548c\u8de8\u89c6\u89d2\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u52a8\u4f5c\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.17886", "pdf": "https://arxiv.org/pdf/2508.17886", "abs": "https://arxiv.org/abs/2508.17886", "authors": ["Hao Duan", "Yitong Song", "Bin Yao", "Anqi Liang"], "title": "PGTuner: An Efficient Framework for Automatic and Transferable Configuration Tuning of Proximity Graphs", "categories": ["cs.DB"], "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS) plays a crucial role in many key\nareas. Proximity graphs (PGs) are the leading method for ANNS, offering the\nbest balance between query efficiency and accuracy. However, their performance\nheavily depends on various construction and query parameters, which are\ndifficult to optimize due to their complex inter-dependencies. Given that users\noften prioritize specific accuracy levels, efficiently identifying the optimal\nPG configurations to meet these targets is essential. Although some studies\nhave explored automatic configuration tuning for PGs, they are limited by\ninefficiencies and suboptimal results. These issues stem from the need to\nconstruct numerous PGs for searching and re-tuning from scratch whenever the\ndataset changes, as well as the failure to capture the complex dependencies\nbetween configurations, query performance, and tuning objectives.\n  To address these challenges, we propose PGTuner, an efficient framework for\nautomatic PG configuration tuning leveraging pre-training knowledge and model\ntransfer techniques. PGTuner improves efficiency through a pre-trained query\nperformance prediction (QPP) model, eliminating the need to build multiple PGs.\nIt also features a deep reinforcement learning-based parameter configuration\nrecommendation (PCR) model to recommend optimal configurations for specific\ndatasets and accuracy targets. Additionally, PGTuner incorporates\nout-of-distribution detection and deep active learning for efficient tuning in\ndynamic scenarios and transferring to new datasets. Extensive experiments\ndemonstrate that PGTuner can stably achieve the top-level tuning effect across\ndifferent datasets while significantly improving tuning efficiency by up to\n14.69X, with a 14.64X boost in dynamic scenarios. The code and data for PGTuner\nare available online at https://github.com/hao-duan/PGTuner.", "AI": {"tldr": "PGTuner\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3\u77e5\u8bc6\u548c\u6a21\u578b\u8fc1\u79fb\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08ANNS\uff09\u4e2d\u7684\u90bb\u8fd1\u56fe\uff08PG\uff09\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8c03\u4f18\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u73b0\u6709PG\u81ea\u52a8\u914d\u7f6e\u8c03\u4f18\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6548\u679c\u4e0d\u4f73\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u573a\u666f\u548c\u65b0\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002", "method": "PGTuner\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u7684\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\uff08QPP\uff09\u6a21\u578b\u548c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u53c2\u6570\u914d\u7f6e\u63a8\u8350\uff08PCR\uff09\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u5206\u5e03\u5916\u68c0\u6d4b\u548c\u6df1\u5ea6\u4e3b\u52a8\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPGTuner\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7a33\u5b9a\u5b9e\u73b0\u9876\u7ea7\u8c03\u4f18\u6548\u679c\uff0c\u8c03\u4f18\u6548\u7387\u6700\u9ad8\u63d0\u534714.69\u500d\uff0c\u52a8\u6001\u573a\u666f\u4e0b\u63d0\u534714.64\u500d\u3002", "conclusion": "PGTuner\u4e3a\u89e3\u51b3PG\u914d\u7f6e\u8c03\u4f18\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17562", "pdf": "https://arxiv.org/pdf/2508.17562", "abs": "https://arxiv.org/abs/2508.17562", "authors": ["Shota Konno", "Che-Kai Liu", "Sigang Ryu", "Samuel Spetalnick", "Arijit Raychowdhury"], "title": "A 28nm 1.80Mb/mm2 Digital/Analog Hybrid SRAM-CIM Macro Using 2D-Weighted Capacitor Array for Complex Number Mac Operations", "categories": ["cs.AR"], "comment": "Asian Solid-State Circuits Conference (A-SSCC) 2025", "summary": "A 28nm dense 6T-SRAM Digital(D)/Analog(A) Hybrid compute-in-memory (CIM)\nmacro supporting complex num-ber MAC operation is presented. By introducing a\n2D-weighted Capacitor Array, a hybrid configuration is adopted where digital\nCIM is applied only to the upper bits and ana-log CIM is applied to the rest,\nwithout the need for input DACs resulting in improved accuracy and lower area\noverhead. The CIM prototype macro achieves 1.80 Mb/mm2 memory density and\n0.435% RMS error. Complex CIM unit outputs real and imaginary part with a\nsingle conversion to reduce latency.", "AI": {"tldr": "28\u7eb3\u7c73\u9ad8\u5bc6\u5ea66T-SRAM\u6570\u5b57/\u6a21\u62df\u6df7\u5408\u5b58\u5185\u8ba1\u7b97(CIM)\u5b8f\u652f\u6301\u590d\u6570MAC\u64cd\u4f5c\uff0c\u91c7\u75282D\u52a0\u6743\u7535\u5bb9\u9635\u5217\u7684\u6df7\u5408\u914d\u7f6e\uff0c\u65e0\u9700\u8f93\u5165DAC\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u9762\u79ef\u5f00\u9500\u3002", "motivation": "\u4e3a\u4e86\u5728\u5b58\u5185\u8ba1\u7b97\u4e2d\u652f\u6301\u590d\u6570\u4e58\u6cd5\u7d2f\u52a0\u64cd\u4f5c\uff0c\u540c\u65f6\u63d0\u9ad8\u7cbe\u5ea6\u548c\u964d\u4f4e\u9762\u79ef\u5f00\u9500\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6570\u5b57/\u6a21\u62df\u6df7\u5408\u914d\u7f6e\u65b9\u6cd5\u3002", "method": "\u91c7\u75282D\u52a0\u6743\u7535\u5bb9\u9635\u5217\u7684\u6df7\u5408\u914d\u7f6e\uff0c\u6570\u5b57CIM\u7528\u4e8e\u9ad8\u4f4d\uff0c\u6a21\u62dfCIM\u7528\u4e8e\u4f4e\u4f4d\uff0c\u907f\u514d\u4e86\u8f93\u5165DAC\u7684\u9700\u6c42\u3002CIM\u5355\u5143\u901a\u8fc7\u5355\u6b21\u8f6c\u6362\u8f93\u51fa\u5b9e\u90e8\u548c\u865a\u90e8\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002", "result": "CIM\u5b8f\u5b9e\u73b0\u4e861.80 Mb/mm2\u7684\u5185\u5b58\u5bc6\u5ea6\u548c0.435%\u7684\u5747\u65b9\u6839\u8bef\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u652f\u6301\u590d\u6570\u64cd\u4f5c\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e94\u7528\u3002"}}
{"id": "2508.16604", "pdf": "https://arxiv.org/pdf/2508.16604", "abs": "https://arxiv.org/abs/2508.16604", "authors": ["Maximilian Burzer", "Tobias King", "Till Riedel", "Michael Beigl", "Tobias R\u00f6ddiger"], "title": "WHAR Datasets: An Open Source Library for Wearable Human Activity Recognition", "categories": ["cs.HC", "cs.LG", "I.2.6"], "comment": "6 pages, 7 figures, to appear in Companion of the 2025 ACM\n  International Joint Conference on Pervasive and Ubiquitous Computing\n  (UbiComp), OpenWearables Workshop (accepted paper)", "summary": "The lack of standardization across Wearable Human Activity Recognition (WHAR)\ndatasets limits reproducibility, comparability, and research efficiency. We\nintroduce WHAR datasets, an open-source library designed to simplify WHAR data\nhandling through a standardized data format and a configuration-driven design,\nenabling reproducible and computationally efficient workflows with minimal\nmanual intervention. The library currently supports 9 widely-used datasets,\nintegrates with PyTorch and TensorFlow, and is easily extensible to new\ndatasets. To demonstrate its utility, we trained two state-of-the-art models,\nTinyHar and MLP-HAR, on the included datasets, approximately reproducing\npublished results and validating the library's effectiveness for\nexperimentation and benchmarking. Additionally, we evaluated preprocessing\nperformance and observed speedups of up to 3.8x using multiprocessing. We hope\nthis library contributes to more efficient, reproducible, and comparable WHAR\nresearch.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86WHAR\u6570\u636e\u96c6\u5e93\uff0c\u65e8\u5728\u6807\u51c6\u5316\u53ef\u7a7f\u6234\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u6570\u636e\uff0c\u63d0\u9ad8\u7814\u7a76\u6548\u7387\u3002", "motivation": "\u89e3\u51b3WHAR\u6570\u636e\u96c6\u7684\u6807\u51c6\u5316\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90\u5e93\uff0c\u652f\u6301\u6807\u51c6\u5316\u6570\u636e\u683c\u5f0f\u548c\u914d\u7f6e\u9a71\u52a8\u8bbe\u8ba1\uff0c\u517c\u5bb9PyTorch\u548cTensorFlow\u3002", "result": "\u57289\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5e93\u7684\u6709\u6548\u6027\uff0c\u9884\u5904\u7406\u6027\u80fd\u63d0\u5347\u4e863.8\u500d\u3002", "conclusion": "\u8be5\u5e93\u6709\u671b\u4fc3\u8fdbWHAR\u7814\u7a76\u7684\u6548\u7387\u548c\u53ef\u6bd4\u6027\u3002"}}
{"id": "2508.17341", "pdf": "https://arxiv.org/pdf/2508.17341", "abs": "https://arxiv.org/abs/2508.17341", "authors": ["Muhammet Anil Yagiz", "Zeynep Sude Cengiz", "Polat Goktas"], "title": "MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems", "categories": ["cs.LG", "cs.CR", "cs.CY", "cs.DC", "cs.ET"], "comment": "2025 IEEE International Symposium on Emerging Metaverse (ISEMV)", "summary": "The rapid expansion of immersive Metaverse applications introduces complex\nchallenges at the intersection of performance, privacy, and environmental\nsustainability. Centralized architectures fall short in addressing these\ndemands, often resulting in elevated energy consumption, latency, and privacy\nconcerns. This paper proposes MetaFed, a decentralized federated learning (FL)\nframework that enables sustainable and intelligent resource orchestration for\nMetaverse environments. MetaFed integrates (i) multi-agent reinforcement\nlearning for dynamic client selection, (ii) privacy-preserving FL using\nhomomorphic encryption, and (iii) carbon-aware scheduling aligned with\nrenewable energy availability. Evaluations on MNIST and CIFAR-10 using\nlightweight ResNet architectures demonstrate that MetaFed achieves up to 25\\%\nreduction in carbon emissions compared to conventional approaches, while\nmaintaining high accuracy and minimal communication overhead. These results\nhighlight MetaFed as a scalable solution for building environmentally\nresponsible and privacy-compliant Metaverse infrastructures.", "AI": {"tldr": "MetaFed \u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3 Metaverse \u4e2d\u7684\u6027\u80fd\u3001\u9690\u79c1\u548c\u73af\u4fdd\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u78b3\u611f\u77e5\u8c03\u5ea6\uff0c\u51cf\u5c11\u78b3\u6392\u653e25%\u3002", "motivation": "\u5f53\u524d Metaverse \u7684\u96c6\u4e2d\u5f0f\u67b6\u6784\u5728\u6027\u80fd\u3001\u9690\u79c1\u548c\u73af\u4fdd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MetaFed \u7ed3\u5408\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u9009\u62e9\u5ba2\u6237\u7aef\u3001\u4f7f\u7528\u540c\u6001\u52a0\u5bc6\u4fdd\u62a4\u9690\u79c1\uff0c\u5e76\u6839\u636e\u53ef\u518d\u751f\u80fd\u6e90\u8c03\u6574\u8c03\u5ea6\u3002", "result": "\u5728 MNIST \u548c CIFAR-10 \u6570\u636e\u96c6\u4e0a\uff0cMetaFed \u51cf\u5c11\u78b3\u6392\u653e25%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "MetaFed \u662f\u6784\u5efa\u73af\u4fdd\u4e14\u9690\u79c1\u5408\u89c4\u7684 Metaverse \u57fa\u7840\u8bbe\u65bd\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17219", "pdf": "https://arxiv.org/pdf/2508.17219", "abs": "https://arxiv.org/abs/2508.17219", "authors": ["Bingyang Wu", "Zili Zhang", "Yinmin Zhong", "Guanzhe Huang", "Yibo Zhu", "Xuanzhe Liu", "Xin Jin"], "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained Elastic Long-Context LLM Serving", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively.", "AI": {"tldr": "TokenLake\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u7247\u6bb5\u7ea7\u524d\u7f00\u7f13\u5b58\u6c60\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u7f13\u5b58\u63a5\u53e3\u548c\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u4f18\u5316\u7f13\u5b58\u7ba1\u7406\uff0c\u63d0\u5347\u6027\u80fd\u548c\u547d\u4e2d\u7387\u3002", "motivation": "\u73b0\u6709\u524d\u7f00\u7f13\u5b58\u7cfb\u7edf\u4e0e\u8bf7\u6c42\u8c03\u5ea6\u7d27\u5bc6\u8026\u5408\uff0c\u5bfc\u81f4\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3001\u6570\u636e\u5197\u4f59\u548c\u5185\u5b58\u788e\u7247\u5316\uff0c\u9700\u89e3\u8026\u7f13\u5b58\u7ba1\u7406\u4e0e\u8c03\u5ea6\u4f18\u5316\u3002", "method": "\u63d0\u51faTokenLake\uff0c\u91c7\u7528\u58f0\u660e\u5f0f\u7f13\u5b58\u63a5\u53e3\u548c\u57fa\u4e8e\u70ed\u70b9\u7684\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\uff0c\u7ba1\u7406\u7247\u6bb5\u7ea7\u524d\u7f00\u7f13\u5b58\uff0c\u51cf\u5c11\u901a\u4fe1\u91cf\u3002", "result": "\u5b9e\u9645\u6d4b\u8bd5\u4e2d\uff0cTokenLake\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\uff0c\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53472.6\u500d\uff0c\u547d\u4e2d\u7387\u63d0\u53472.1\u500d\u3002", "conclusion": "TokenLake\u6709\u6548\u89e3\u51b3\u4e86\u7f13\u5b58\u7ba1\u7406\u4e0e\u8c03\u5ea6\u7684\u8026\u5408\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.17161", "pdf": "https://arxiv.org/pdf/2508.17161", "abs": "https://arxiv.org/abs/2508.17161", "authors": ["Julyanara R. Silva", "Carlos Eduardo C. Dantas", "Marcelo A. Maia"], "title": "What Developers Ask to ChatGPT in GitHub Pull Requests? an Exploratory Study", "categories": ["cs.SE"], "comment": "12 pages, 3 figures", "summary": "The emergence of Large Language Models (LLMs), such as ChatGPT, has\nintroduced a new set of tools to support software developers in solving pro-\ngramming tasks. However, our understanding of the interactions (i.e., prompts)\nbetween developers and ChatGPT that result in contributions to the codebase\nremains limited. To explore this limitation, we conducted a manual evaluation\nof 155 valid ChatGPT share links extracted from 139 merged Pull Requests (PRs),\nrevealing the interactions between developers and reviewers with ChatGPT that\nled to merges into the main codebase. Our results produced a catalog of 14\ntypes of ChatGPT requests categorized into four main groups. We found a\nsignificant number of requests involving code review and the implementation of\ncode snippets based on specific tasks. Developers also sought to clarify doubts\nby requesting technical explanations or by asking for text refinements for\ntheir web pages. Furthermore, we verified that prompts involving code\ngeneration generally required more interactions to produce the desired answer\ncompared to prompts requesting text review or technical information.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f00\u53d1\u8005\u4e0eChatGPT\u7684\u4ea4\u4e92\u5982\u4f55\u4fc3\u6210\u4ee3\u7801\u5e93\u7684\u8d21\u732e\uff0c\u5206\u6790\u4e86155\u4e2a\u6709\u6548\u4ea4\u4e92\uff0c\u63d0\u51fa\u4e8614\u79cd\u8bf7\u6c42\u7c7b\u578b\u7684\u5206\u7c7b\uff0c\u53d1\u73b0\u4ee3\u7801\u751f\u6210\u8bf7\u6c42\u901a\u5e38\u9700\u8981\u66f4\u591a\u4ea4\u4e92\u3002", "motivation": "\u63a2\u7d22\u5f00\u53d1\u8005\u4e0eChatGPT\u7684\u4ea4\u4e92\u5982\u4f55\u4fc3\u6210\u4ee3\u7801\u5408\u5e76\uff0c\u586b\u8865\u76f8\u5173\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u624b\u52a8\u8bc4\u4f30155\u4e2a\u6709\u6548ChatGPT\u5206\u4eab\u94fe\u63a5\uff0c\u5206\u6790\u4ea4\u4e92\u7c7b\u578b\u548c\u6548\u679c\u3002", "result": "\u63d0\u51fa\u4e8614\u79cd\u8bf7\u6c42\u7c7b\u578b\u7684\u5206\u7c7b\uff0c\u53d1\u73b0\u4ee3\u7801\u751f\u6210\u8bf7\u6c42\u9700\u8981\u66f4\u591a\u4ea4\u4e92\uff0c\u5f00\u53d1\u8005\u5e38\u5229\u7528ChatGPT\u8fdb\u884c\u4ee3\u7801\u5ba1\u67e5\u548c\u4efb\u52a1\u5b9e\u73b0\u3002", "conclusion": "ChatGPT\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u4ea4\u4e92\u9700\u6c42\u66f4\u9ad8\uff0c\u4f46\u5176\u5728\u4ee3\u7801\u5ba1\u67e5\u548c\u6280\u672f\u89e3\u91ca\u65b9\u9762\u7684\u4ef7\u503c\u663e\u8457\u3002"}}
{"id": "2508.17811", "pdf": "https://arxiv.org/pdf/2508.17811", "abs": "https://arxiv.org/abs/2508.17811", "authors": ["Hanzhi Chang", "Ruijie Zhu", "Wenjie Chang", "Mulin Yu", "Yanzhe Liang", "Jiahao Lu", "Zhuoyuan Li", "Tianzhu Zhang"], "title": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "17 pages, 15 figures, 5 tables", "summary": "Surface reconstruction has been widely studied in computer vision and\ngraphics. However, existing surface reconstruction works struggle to recover\naccurate scene geometry when the input views are extremely sparse. To address\nthis issue, we propose MeshSplat, a generalizable sparse-view surface\nreconstruction framework via Gaussian Splatting. Our key idea is to leverage\n2DGS as a bridge, which connects novel view synthesis to learned geometric\npriors and then transfers these priors to achieve surface reconstruction.\nSpecifically, we incorporate a feed-forward network to predict per-view\npixel-aligned 2DGS, which enables the network to synthesize novel view images\nand thus eliminates the need for direct 3D ground-truth supervision. To improve\nthe accuracy of 2DGS position and orientation prediction, we propose a Weighted\nChamfer Distance Loss to regularize the depth maps, especially in overlapping\nareas of input views, and also a normal prediction network to align the\norientation of 2DGS with normal vectors predicted by a monocular normal\nestimator. Extensive experiments validate the effectiveness of our proposed\nimprovement, demonstrating that our method achieves state-of-the-art\nperformance in generalizable sparse-view mesh reconstruction tasks. Project\nPage: https://hanzhichang.github.io/meshsplat_web", "AI": {"tldr": "MeshSplat\u662f\u4e00\u4e2a\u57fa\u4e8e2D\u9ad8\u65af\u6e85\u5c04\u7684\u7a00\u758f\u89c6\u56fe\u8868\u9762\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u75282DGS\u4f5c\u4e3a\u6865\u6881\uff0c\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u65b0\u578b\u89c6\u56fe\u5408\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u65e03D\u76d1\u7763\u7684\u8868\u9762\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u89c6\u56fe\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u5728\u6781\u7aef\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u4e0b\u96be\u4ee5\u6062\u590d\u51c6\u786e\u51e0\u4f55\uff0cMeshSplat\u65e8\u5728\u901a\u8fc72DGS\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u524d\u9988\u7f51\u7edc\u9884\u6d4b\u6bcf\u89c6\u56fe\u50cf\u7d20\u5bf9\u9f50\u76842DGS\uff0c\u7ed3\u5408\u52a0\u6743Chamfer\u8ddd\u79bb\u635f\u5931\u548c\u6cd5\u7ebf\u9884\u6d4b\u7f51\u7edc\u4f18\u5316\u51e0\u4f55\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMeshSplat\u5728\u7a00\u758f\u89c6\u56fe\u7f51\u683c\u91cd\u5efa\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MeshSplat\u901a\u8fc7\u521b\u65b0\u76842DGS\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b\u7684\u8868\u9762\u91cd\u5efa\u96be\u9898\u3002"}}
{"id": "2508.17121", "pdf": "https://arxiv.org/pdf/2508.17121", "abs": "https://arxiv.org/abs/2508.17121", "authors": ["Zhenliang Gan", "Xiaoxiao Hu", "Sheng Li", "Zhenxing Qian", "Xinpeng Zhang"], "title": "SyncGuard: Robust Audio Watermarking Capable of Countering Desynchronization Attacks", "categories": ["cs.CR", "cs.MM", "cs.SD"], "comment": null, "summary": "Audio watermarking has been widely applied in copyright protection and source\ntracing. However, due to the inherent characteristics of audio signals,\nwatermark localization and resistance to desynchronization attacks remain\nsignificant challenges. In this paper, we propose a learning-based scheme named\nSyncGuard to address these challenges. Specifically, we design a frame-wise\nbroadcast embedding strategy to embed the watermark in arbitrary-length audio,\nenhancing time-independence and eliminating the need for localization during\nwatermark extraction. To further enhance robustness, we introduce a\nmeticulously designed distortion layer. Additionally, we employ dilated\nresidual blocks in conjunction with dilated gated blocks to effectively capture\nmulti-resolution time-frequency features. Extensive experimental results show\nthat SyncGuard efficiently handles variable-length audio segments, outperforms\nstate-of-the-art methods in robustness against various attacks, and delivers\nsuperior auditory quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSyncGuard\u7684\u5b66\u4e60\u578b\u97f3\u9891\u6c34\u5370\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6c34\u5370\u5b9a\u4f4d\u548c\u6297\u53bb\u540c\u6b65\u653b\u51fb\u7684\u96be\u9898\uff0c\u901a\u8fc7\u5e27\u7ea7\u5e7f\u64ad\u5d4c\u5165\u7b56\u7565\u548c\u591a\u5206\u8fa8\u7387\u65f6\u9891\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u542c\u89c9\u8d28\u91cf\u3002", "motivation": "\u97f3\u9891\u6c34\u5370\u5728\u7248\u6743\u4fdd\u62a4\u548c\u6765\u6e90\u8ffd\u8e2a\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u7531\u4e8e\u97f3\u9891\u4fe1\u53f7\u7684\u56fa\u6709\u7279\u6027\uff0c\u6c34\u5370\u5b9a\u4f4d\u548c\u6297\u53bb\u540c\u6b65\u653b\u51fb\u4ecd\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u91c7\u7528\u5e27\u7ea7\u5e7f\u64ad\u5d4c\u5165\u7b56\u7565\u5728\u4efb\u610f\u957f\u5ea6\u97f3\u9891\u4e2d\u5d4c\u5165\u6c34\u5370\uff0c\u5f15\u5165\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5931\u771f\u5c42\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u4f7f\u7528\u6269\u5f20\u6b8b\u5dee\u5757\u548c\u6269\u5f20\u95e8\u63a7\u5757\u6355\u83b7\u591a\u5206\u8fa8\u7387\u65f6\u9891\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSyncGuard\u80fd\u9ad8\u6548\u5904\u7406\u53d8\u957f\u97f3\u9891\u7247\u6bb5\uff0c\u5728\u5bf9\u6297\u591a\u79cd\u653b\u51fb\u7684\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4f18\u5f02\u7684\u542c\u89c9\u8d28\u91cf\u3002", "conclusion": "SyncGuard\u901a\u8fc7\u521b\u65b0\u7684\u5d4c\u5165\u548c\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u97f3\u9891\u6c34\u5370\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.17931", "pdf": "https://arxiv.org/pdf/2508.17931", "abs": "https://arxiv.org/abs/2508.17931", "authors": ["David Justen", "Matthias Boehm"], "title": "Join Cardinality Estimation with OmniSketches", "categories": ["cs.DB"], "comment": "6 pages, 6 figures, 1 algorithm, 1 table", "summary": "Join ordering is a key factor in query performance, yet traditional\ncost-based optimizers often produce sub-optimal plans due to inaccurate\ncardinality estimates in multi-predicate, multi-join queries. Existing\nalternatives such as learning-based optimizers and adaptive query processing\nimprove accuracy but can suffer from high training costs, poor generalization,\nor integration challenges. We present an extension of OmniSketch - a\nprobabilistic data structure combining count-min sketches and K-minwise hashing\n- to enable multi-join cardinality estimation without assuming uniformity and\nindependence. Our approach introduces the OmniSketch join estimator, ensures\nsketch interoperability across tables, and provides an algorithm to process\nalpha-acyclic join graphs. Our experiments on SSB-skew and JOB-light show that\nOmniSketch-enhanced cost-based optimization can improve estimation accuracy and\nplan quality compared to DuckDB. For SSB-skew, we show intermediate result\ndecreases up to 1,077x and execution time decreases up to 3.19x. For JOB-light,\nOmniSketch join cardinality estimation shows occasional individual improvements\nbut largely suffers from a loss of witnesses due to unfavorable join graph\nshapes and large numbers of unique values in foreign key columns.", "AI": {"tldr": "OmniSketch\u6269\u5c55\u4e86\u4e00\u79cd\u6982\u7387\u6570\u636e\u7ed3\u6784\uff0c\u7528\u4e8e\u591a\u8fde\u63a5\u57fa\u6570\u4f30\u8ba1\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u4f18\u5316\u5668\u7684\u4e0d\u51c6\u786e\u6027\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u6210\u672c\u7684\u4f18\u5316\u5668\u5728\u591a\u8fde\u63a5\u67e5\u8be2\u4e2d\u56e0\u57fa\u6570\u4f30\u8ba1\u4e0d\u51c6\u786e\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\u3002", "method": "\u6269\u5c55OmniSketch\uff0c\u7ed3\u5408count-min\u548cK-minwise\u54c8\u5e0c\uff0c\u63d0\u4f9b\u591a\u8fde\u63a5\u57fa\u6570\u4f30\u8ba1\uff0c\u652f\u6301\u8de8\u8868\u7684\u8349\u56fe\u4e92\u64cd\u4f5c\u3002", "result": "\u5728SSB-skew\u4e2d\u663e\u8457\u964d\u4f4e\u4e2d\u95f4\u7ed3\u679c\u548c\u6267\u884c\u65f6\u95f4\uff1b\u5728JOB-light\u4e2d\u6548\u679c\u6709\u9650\u3002", "conclusion": "OmniSketch\u5728\u591a\u8fde\u63a5\u4f18\u5316\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u5bf9\u7279\u5b9a\u67e5\u8be2\u56fe\u548c\u5927\u57fa\u6570\u5916\u952e\u5217\u7684\u9002\u5e94\u6027\u9700\u6539\u8fdb\u3002"}}
{"id": "2508.17820", "pdf": "https://arxiv.org/pdf/2508.17820", "abs": "https://arxiv.org/abs/2508.17820", "authors": ["Tingyu Ding", "Qunsong Zeng", "Kaibin Huang"], "title": "In-Memory Computing Enabled Deep MIMO Detection to Support Ultra-Low-Latency Communications", "categories": ["cs.AR", "eess.SP"], "comment": null, "summary": "The development of sixth-generation (6G) mobile networks imposes\nunprecedented latency and reliability demands on multiple-input multiple-output\n(MIMO) communication systems, a key enabler of high-speed radio access.\nRecently, deep unfolding-based detectors, which map iterative algorithms onto\nneural network architectures, have emerged as a promising approach, combining\nthe strengths of model-driven and data-driven methods to achieve high detection\naccuracy with relatively low complexity. However, algorithmic innovation alone\nis insufficient; software-hardware co-design is essential to meet the extreme\nlatency requirements of 6G (i.e., 0.1 milliseconds). This motivates us to\npropose leveraging in-memory computing, which is an analog computing technology\nthat integrates memory and computation within memristor circuits, to perform\nthe intensive matrix-vector multiplication (MVM) operations inherent in deep\nMIMO detection at the nanosecond scale. Specifically, we introduce a novel\narchitecture, called the deep in-memory MIMO (IM-MIMO) detector, characterized\nby two key features. First, each of its cascaded computational blocks is\ndecomposed into channel-dependent and channel-independent neural network\nmodules. Such a design minimizes the latency of memristor reprogramming in\nresponse to channel variations, which significantly exceeds computation time.\nSecond, we develop a customized detector-training method that exploits prior\nknowledge of memristor-value statistics to enhance robustness against\nprogramming noise. Furthermore, we conduct a comprehensive analysis of the\nIM-MIMO detector's performance, evaluating detection accuracy, processing\nlatency, and hardware complexity. Our study quantifies detection error as a\nfunction of various factors, including channel noise, memristor programming\nnoise, and neural network size.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u5b58\u8ba1\u7b97\u7684\u6df1\u5ea6MIMO\u68c0\u6d4b\u5668\uff08IM-MIMO\uff09\uff0c\u65e8\u5728\u6ee1\u8db36G\u7f51\u7edc\u7684\u6781\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u53ef\u9760\u6027\u9700\u6c42\uff0c\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u7eb3\u79d2\u7ea7\u7684\u6027\u80fd\u3002", "motivation": "6G\u7f51\u7edc\u5bf9MIMO\u7cfb\u7edf\u7684\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u63d0\u51fa\u4e86\u524d\u6240\u672a\u6709\u7684\u9ad8\u8981\u6c42\uff0c\u4f20\u7edf\u6df1\u5ea6\u5c55\u5f00\u65b9\u6cd5\u96be\u4ee5\u5355\u72ec\u6ee1\u8db3\uff0c\u9700\u7ed3\u5408\u5185\u5b58\u8ba1\u7b97\u6280\u672f\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u77e9\u9635\u5411\u91cf\u8fd0\u7b97\u3002", "method": "\u63d0\u51faIM-MIMO\u68c0\u6d4b\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u89e3\u8ba1\u7b97\u6a21\u5757\u4e3a\u4fe1\u9053\u4f9d\u8d56\u548c\u72ec\u7acb\u90e8\u5206\u4ee5\u51cf\u5c11\u5ef6\u8fdf\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u8bb0\u5fc6\u7535\u963b\u5668\u7edf\u8ba1\u7279\u6027\u7684\u9c81\u68d2\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86\u68c0\u6d4b\u5668\u7684\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u786c\u4ef6\u590d\u6742\u5ea6\uff0c\u91cf\u5316\u4e86\u4fe1\u9053\u566a\u58f0\u3001\u7f16\u7a0b\u566a\u58f0\u548c\u7f51\u7edc\u89c4\u6a21\u5bf9\u68c0\u6d4b\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "conclusion": "IM-MIMO\u68c0\u6d4b\u5668\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4e3a6G\u7f51\u7edc\u7684\u8d85\u4f4e\u5ef6\u8fdf\u9700\u6c42\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17440", "pdf": "https://arxiv.org/pdf/2508.17440", "abs": "https://arxiv.org/abs/2508.17440", "authors": ["Nikita Stroev", "Natalia G. Berloff"], "title": "Programmable k-local Ising Machines and all-optical Kolmogorov-Arnold Networks on Photonic Platforms", "categories": ["physics.optics", "cs.ET", "cs.LG"], "comment": "16 pages, 6 figures", "summary": "We unify k-local Ising optimization and optical KAN function learning on a\nsingle photonic platform, establishing a critical convergence point in optical\ncomputing that enables interleaved discrete-continuous workflows. We introduce\na single spacial light modulator (SLM)-centric primitive that realizes, in one\nstroke, all-optical k-local Ising interactions and fully optical\nKolmogorov-Arnold network (KAN) layers. The central idea is to convert\nstructural nonlinearity of a nominally linear photonic scatterer into a\nper-window computational resource by adding one relay pass through the same\nspatial light modulator. A folded 4f relay reimages the first Fourier plane\nonto the SLM so that each chosen spin clique or ridge channel occupies a\ndisjoint window with its own second-pass phase patch. Propagation remains\nlinear in the optical field, yet the measured intensity in each window becomes\na freely programmable polynomial of the clique sum or projection amplitude.\nThis yields native, per-clique k-local couplings without nonlinear media and,\nin parallel, the many independent univariate nonlinearities required by KAN\nlayers, all with in-situ physical gradients for training using two-frame\n(forward and adjoint) physical gradients. We outline implementation on spatial\nphotonic Ising machines, injection-locked VCSEL arrays, and the Microsoft\nanalog optical computers. In all cases the hardware change is one extra lens\nand a fold (or an on-chip 4f loop), enabling a minimal overhead, massively\nparallel route to high-order optical Ising optimization and trainable,\nall-optical KAN processing.", "AI": {"tldr": "\u5c06k-local Ising\u4f18\u5316\u4e0e\u5149\u5b66KAN\u51fd\u6570\u5b66\u4e60\u7edf\u4e00\u5728\u5355\u4e00\u5149\u8ba1\u7b97\u5e73\u53f0\u4e0a\uff0c\u901a\u8fc7\u7a7a\u95f4\u5149\u8c03\u5236\u5668\u5b9e\u73b0\u5168\u5149\u5b66k-local\u76f8\u4e92\u4f5c\u7528\u4e0eKAN\u5c42\u3002", "motivation": "\u63a2\u7d22\u5149\u8ba1\u7b97\u4e2d\u79bb\u6563-\u8fde\u7eed\u5de5\u4f5c\u6d41\u7684\u5173\u952e\u878d\u5408\u70b9\uff0c\u89e3\u51b3\u975e\u7ebf\u6027\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u95ee\u9898\u3002", "method": "\u5229\u7528\u7a7a\u95f4\u5149\u8c03\u5236\u5668\u7684\u591a\u7a97\u53e3\u7279\u6027\uff0c\u901a\u8fc7\u6298\u53e04f\u4e2d\u7ee7\u5b9e\u73b0\u5c40\u90e8\u8026\u5408\u548cKAN\u975e\u7ebf\u6027\u8ba1\u7b97\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u9700\u975e\u7ebf\u6027\u4ecb\u8d28\u7684\u9ad8\u9636\u5149\u5b66Ising\u4f18\u5316\u548c\u5168\u5149\u5b66KAN\u5904\u7406\u3002", "conclusion": "\u63d0\u51fa\u4e00\u79cd\u786c\u4ef6\u5f00\u9500\u6781\u5c0f\u7684\u9ad8\u5e76\u884c\u6027\u5149\u8ba1\u7b97\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5149\u8ba1\u7b97\u5e73\u53f0\u3002"}}
{"id": "2508.17311", "pdf": "https://arxiv.org/pdf/2508.17311", "abs": "https://arxiv.org/abs/2508.17311", "authors": ["Daniele De Sensi", "Saverio Pasqualoni", "Lorenzo Piarulli", "Tommaso Bonato", "Seydou Ba", "Matteo Turisini", "Jens Domke", "Torsten Hoefler"], "title": "Bine Trees: Enhancing Collective Operations by Optimizing Communication Locality", "categories": ["cs.DC", "cs.AI", "cs.PF", "C.2.4; C.5.1"], "comment": null, "summary": "Communication locality plays a key role in the performance of collective\noperations on large HPC systems, especially on oversubscribed networks where\ngroups of nodes are fully connected internally but sparsely linked through\nglobal connections. We present Bine (binomial negabinary) trees, a family of\ncollective algorithms that improve communication locality. Bine trees maintain\nthe generality of binomial trees and butterflies while cutting global-link\ntraffic by up to 33%. We implement eight Bine-based collectives and evaluate\nthem on four large-scale supercomputers with Dragonfly, Dragonfly+,\noversubscribed fat-tree, and torus topologies, achieving up to 5x speedups and\nconsistent reductions in global-link traffic across different vector sizes and\nnode counts.", "AI": {"tldr": "Bine\uff08\u4e8c\u9879\u8d1f\u4e8c\u8fdb\u5236\uff09\u6811\u662f\u4e00\u79cd\u63d0\u9ad8\u901a\u4fe1\u5c40\u90e8\u6027\u7684\u96c6\u4f53\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u578bHPC\u7cfb\u7edf\uff0c\u80fd\u51cf\u5c11\u5168\u5c40\u94fe\u8def\u6d41\u91cf\u8fbe33%\uff0c\u5e76\u5728\u591a\u79cd\u62d3\u6251\u7ed3\u6784\u4e2d\u5b9e\u73b0\u6700\u9ad85\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u5728\u5927\u578bHPC\u7cfb\u7edf\u4e2d\uff0c\u901a\u4fe1\u5c40\u90e8\u6027\u5bf9\u96c6\u4f53\u64cd\u4f5c\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u8282\u70b9\u5185\u90e8\u5168\u8fde\u63a5\u4f46\u5168\u5c40\u8fde\u63a5\u7a00\u758f\u7684\u8d85\u8f7d\u7f51\u7edc\u4e2d\u3002", "method": "\u63d0\u51faBine\u6811\u7b97\u6cd5\u5bb6\u65cf\uff0c\u7ed3\u5408\u4e86\u4e8c\u9879\u6811\u548c\u8774\u8776\u7b97\u6cd5\u7684\u901a\u7528\u6027\uff0c\u663e\u8457\u51cf\u5c11\u5168\u5c40\u94fe\u8def\u6d41\u91cf\u3002\u5b9e\u73b0\u4e86\u516b\u79cd\u57fa\u4e8eBine\u7684\u96c6\u4f53\u64cd\u4f5c\uff0c\u5e76\u5728\u56db\u79cd\u5927\u89c4\u6a21\u8d85\u7ea7\u8ba1\u7b97\u673a\uff08Dragonfly\u3001Dragonfly+\u3001\u8d85\u8f7d\u7684\u80d6\u6811\u548c\u73af\u5f62\u62d3\u6251\uff09\u4e0a\u6d4b\u8bd5\u3002", "result": "Bine\u6811\u5728\u591a\u79cd\u5411\u91cf\u5927\u5c0f\u548c\u8282\u70b9\u6570\u91cf\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad85\u500d\u7684\u6027\u80fd\u52a0\u901f\uff0c\u5e76\u4e00\u81f4\u51cf\u5c11\u4e86\u5168\u5c40\u94fe\u8def\u6d41\u91cf\u3002", "conclusion": "Bine\u6811\u5728\u63d0\u9ad8\u901a\u4fe1\u5c40\u90e8\u6027\u548c\u51cf\u5c11\u5168\u5c40\u6d41\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7f51\u7edc\u62d3\u6251\u7684HPC\u7cfb\u7edf\u3002"}}
{"id": "2508.17343", "pdf": "https://arxiv.org/pdf/2508.17343", "abs": "https://arxiv.org/abs/2508.17343", "authors": ["Abhik Roychoudhury"], "title": "Agentic AI for Software: thoughts from Software Engineering community", "categories": ["cs.SE", "cs.AI", "D.2"], "comment": "4 pages", "summary": "AI agents have recently shown significant promise in software engineering.\nMuch public attention has been transfixed on the topic of code generation from\nLarge Language Models (LLMs) via a prompt. However, software engineering is\nmuch more than programming, and AI agents go far beyond instructions given by a\nprompt.\n  At the code level, common software tasks include code generation, testing,\nand program repair. Design level software tasks may include architecture\nexploration, requirements understanding, and requirements enforcement at the\ncode level. Each of these software tasks involves micro-decisions which can be\ntaken autonomously by an AI agent, aided by program analysis tools. This\ncreates the vision of an AI software engineer, where the AI agent can be seen\nas a member of a development team.\n  Conceptually, the key to successfully developing trustworthy agentic AI-based\nsoftware workflows will be to resolve the core difficulty in software\nengineering - the deciphering and clarification of developer intent.\nSpecification inference, or deciphering the intent, thus lies at the heart of\nmany software tasks, including software maintenance and program repair. A\nsuccessful deployment of agentic technology into software engineering would\ninvolve making conceptual progress in such intent inference via agents.\n  Trusting the AI agent becomes a key aspect, as software engineering becomes\nmore automated. Higher automation also leads to higher volume of code being\nautomatically generated, and then integrated into code-bases. Thus to deal with\nthis explosion, an emerging direction is AI-based verification and validation\n(V & V) of AI generated code. We posit that agentic software workflows in\nfuture will include such AIbased V&V.", "AI": {"tldr": "AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4e0d\u4ec5\u9650\u4e8e\u4ee3\u7801\u751f\u6210\uff0c\u8fd8\u5305\u62ec\u8bbe\u8ba1\u3001\u6d4b\u8bd5\u548c\u4fee\u590d\u7b49\u4efb\u52a1\u3002\u5176\u6838\u5fc3\u6311\u6218\u662f\u7406\u89e3\u5f00\u53d1\u8005\u610f\u56fe\uff0c\u540c\u65f6\u9700\u89e3\u51b3\u4fe1\u4efb\u95ee\u9898\uff0c\u672a\u6765\u5c06\u7ed3\u5408AI\u9a8c\u8bc1\u4e0e\u9a8c\u8bc1\u6280\u672f\u3002", "motivation": "\u63a2\u8ba8AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u89e3\u51b3\u5f00\u53d1\u8005\u610f\u56fe\u63a8\u65ad\u548c\u81ea\u52a8\u5316\u4fe1\u4efb\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5206\u6790\u5de5\u5177\u8f85\u52a9AI\u4ee3\u7406\u81ea\u4e3b\u5b8c\u6210\u4ee3\u7801\u4e0e\u8bbe\u8ba1\u4efb\u52a1\uff0c\u63a8\u52a8\u610f\u56fe\u63a8\u65ad\u548c\u9a8c\u8bc1\u6280\u672f\u7684\u8fdb\u6b65\u3002", "result": "AI\u4ee3\u7406\u80fd\u591f\u6210\u4e3a\u5f00\u53d1\u56e2\u961f\u7684\u4e00\u5458\uff0c\u5b9e\u73b0\u591a\u79cd\u8f6f\u4ef6\u4efb\u52a1\u7684\u81ea\u52a8\u5316\uff0c\u4f46\u9700\u89e3\u51b3\u610f\u56fe\u7406\u89e3\u548c\u4fe1\u4efb\u95ee\u9898\u3002", "conclusion": "AI\u4ee3\u7406\u5c06\u91cd\u5851\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u672a\u6765\u9700\u7ed3\u5408AI\u9a8c\u8bc1\u6280\u672f\u4ee5\u5e94\u5bf9\u81ea\u52a8\u5316\u5e26\u6765\u7684\u4ee3\u7801\u7206\u70b8\u95ee\u9898\u3002"}}
{"id": "2508.16856", "pdf": "https://arxiv.org/pdf/2508.16856", "abs": "https://arxiv.org/abs/2508.16856", "authors": ["Zubair Islam", "Ahmaad Ansari", "George Daoud", "Mohamed El-Darieby"], "title": "A Workflow for Map Creation in Autonomous Vehicle Simulations", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": "6 pages, 12 figures. Published in the Proceedings of GEOProcessing\n  2025: The Seventeenth International Conference on Advanced Geographic\n  Information Systems, Applications, and Services (IARIA)", "summary": "The fast development of technology and artificial intelligence has\nsignificantly advanced Autonomous Vehicle (AV) research, emphasizing the need\nfor extensive simulation testing. Accurate and adaptable maps are critical in\nAV development, serving as the foundation for localization, path planning, and\nscenario testing. However, creating simulation-ready maps is often difficult\nand resource-intensive, especially with simulators like CARLA (CAR Learning to\nAct). Many existing workflows require significant computational resources or\nrely on specific simulators, limiting flexibility for developers. This paper\npresents a custom workflow to streamline map creation for AV development,\ndemonstrated through the generation of a 3D map of a parking lot at Ontario\nTech University. Future work will focus on incorporating SLAM technologies,\noptimizing the workflow for broader simulator compatibility, and exploring more\nflexible handling of latitude and longitude values to enhance map generation\naccuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u5f00\u53d1\u4e2d\u5730\u56fe\u521b\u5efa\u7684\u5b9a\u5236\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u751f\u62103D\u505c\u8f66\u573a\u5730\u56fe\u5c55\u793a\uff0c\u672a\u6765\u5c06\u6574\u5408SLAM\u6280\u672f\u5e76\u4f18\u5316\u517c\u5bb9\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u7814\u7a76\u9700\u8981\u5927\u91cf\u4eff\u771f\u6d4b\u8bd5\uff0c\u4f46\u73b0\u6709\u5730\u56fe\u751f\u6210\u65b9\u6cd5\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u517c\u5bb9\u6027\u5dee\uff0c\u9650\u5236\u4e86\u5f00\u53d1\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u5b9a\u5236\u5de5\u4f5c\u6d41\u751f\u62103D\u5730\u56fe\uff0c\u4ee5CARLA\u6a21\u62df\u5668\u4e3a\u4f8b\uff0c\u5c55\u793a\u505c\u8f66\u573a\u5730\u56fe\u521b\u5efa\u8fc7\u7a0b\u3002", "result": "\u6210\u529f\u751f\u6210\u4e863D\u505c\u8f66\u573a\u5730\u56fe\uff0c\u9a8c\u8bc1\u4e86\u5de5\u4f5c\u6d41\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6d41\u4e3aAV\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5730\u56fe\u751f\u6210\u65b9\u6848\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u6574\u5408SLAM\u7b49\u6280\u672f\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u548c\u517c\u5bb9\u6027\u3002"}}
{"id": "2508.17127", "pdf": "https://arxiv.org/pdf/2508.17127", "abs": "https://arxiv.org/abs/2508.17127", "authors": ["Antonin Sulc"], "title": "A Straightforward Pipeline for Targeted Entailment and Contradiction Detection", "categories": ["cs.CL", "cs.LO"], "comment": null, "summary": "Finding the relationships between sentences in a document is crucial for\ntasks like fact-checking, argument mining, and text summarization. A key\nchallenge is to identify which sentences act as premises or contradictions for\na specific claim. Existing methods often face a trade-off: transformer\nattention mechanisms can identify salient textual connections but lack explicit\nsemantic labels, while Natural Language Inference (NLI) models can classify\nrelationships between sentence pairs but operate independently of contextual\nsaliency. In this work, we introduce a method that combines the strengths of\nboth approaches for a targeted analysis. Our pipeline first identifies\ncandidate sentences that are contextually relevant to a user-selected target\nsentence by aggregating token-level attention scores. It then uses a pretrained\nNLI model to classify each candidate as a premise (entailment) or\ncontradiction. By filtering NLI-identified relationships with attention-based\nsaliency scores, our method efficiently isolates the most significant semantic\nrelationships for any given claim in a text.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408Transformer\u6ce8\u610f\u529b\u673a\u5236\u548cNLI\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u6587\u6863\u4e2d\u53e5\u5b50\u4e4b\u95f4\u7684\u524d\u63d0\u6216\u77db\u76fe\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bc6\u522b\u53e5\u5b50\u5173\u7cfb\u65f6\u5b58\u5728\u6743\u8861\uff0c\u8981\u4e48\u7f3a\u4e4f\u663e\u5f0f\u8bed\u4e49\u6807\u7b7e\uff0c\u8981\u4e48\u5ffd\u7565\u4e0a\u4e0b\u6587\u663e\u8457\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u805a\u5408token\u7ea7\u6ce8\u610f\u529b\u5206\u6570\u7b5b\u9009\u4e0a\u4e0b\u6587\u76f8\u5173\u5019\u9009\u53e5\u5b50\uff0c\u518d\u4f7f\u7528\u9884\u8bad\u7ec3NLI\u6a21\u578b\u5206\u7c7b\u4e3a\u524d\u63d0\u6216\u77db\u76fe\uff0c\u6700\u540e\u901a\u8fc7\u6ce8\u610f\u529b\u663e\u8457\u6027\u8bc4\u5206\u8fc7\u6ee4\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u63d0\u53d6\u6587\u672c\u4e2d\u6700\u663e\u8457\u7684\u8bed\u4e49\u5173\u7cfb\u3002", "conclusion": "\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548cNLI\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u8bc6\u522b\u53e5\u5b50\u5173\u7cfb\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2508.17270", "pdf": "https://arxiv.org/pdf/2508.17270", "abs": "https://arxiv.org/abs/2508.17270", "authors": ["Xu Sun", "Yunqing He", "Tongwei Ren", "Gangshan Wu"], "title": "Spatial-Temporal Human-Object Interaction Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "In this paper, we propose a new instance-level human-object interaction\ndetection task on videos called ST-HOID, which aims to distinguish fine-grained\nhuman-object interactions (HOIs) and the trajectories of subjects and objects.\nIt is motivated by the fact that HOI is crucial for human-centric video content\nunderstanding. To solve ST-HOID, we propose a novel method consisting of an\nobject trajectory detection module and an interaction reasoning module.\nFurthermore, we construct the first dataset named VidOR-HOID for ST-HOID\nevaluation, which contains 10,831 spatial-temporal HOI instances. We conduct\nextensive experiments to evaluate the effectiveness of our method. The\nexperimental results demonstrate that our method outperforms the baselines\ngenerated by the state-of-the-art methods of image human-object interaction\ndetection, video visual relation detection and video human-object interaction\nrecognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u4e2d\u5b9e\u4f8b\u7ea7\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u4efb\u52a1ST-HOID\uff0c\u65e8\u5728\u533a\u5206\u7ec6\u7c92\u5ea6\u7684\u4eba-\u7269\u4ea4\u4e92\u53ca\u5176\u8f68\u8ff9\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u6570\u636e\u96c6VidOR-HOID\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4eba-\u7269\u4ea4\u4e92\u5bf9\u4e8e\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u5185\u5bb9\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u76ee\u6807\u8f68\u8ff9\u68c0\u6d4b\u6a21\u5757\u548c\u4ea4\u4e92\u63a8\u7406\u6a21\u5757\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ST-HOID\u4efb\u52a1\u548c\u76f8\u5173\u65b9\u6cd5\u662f\u6709\u6548\u7684\uff0cVidOR-HOID\u6570\u636e\u96c6\u4e3a\u4efb\u52a1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2508.18123", "pdf": "https://arxiv.org/pdf/2508.18123", "abs": "https://arxiv.org/abs/2508.18123", "authors": ["Yanjun Yang", "Adrian Wheeldon", "Yihan Pan", "Alex Serb"], "title": "Views: A Hardware-friendly Graph Database Model For Storing Semantic Information", "categories": ["cs.DB", "cs.AR", "cs.DC", "cs.SC"], "comment": null, "summary": "The graph database (GDB) is an increasingly common storage model for data\ninvolving relationships between entries. Beyond its widespread usage in\ndatabase industries, the advantages of GDBs indicate a strong potential in\nconstructing symbolic artificial intelligences (AIs) and retrieval-augmented\ngeneration (RAG), where knowledge of data inter-relationships takes a critical\nrole in implementation. However, current GDB models are not optimised for\nhardware acceleration, leading to bottlenecks in storage capacity and\ncomputational efficiency. In this paper, we propose a hardware-friendly GDB\nmodel, called Views. We show its data structure and organisation tailored for\nefficient storage and retrieval of graph data and demonstrate its equivalence\nto represent traditional graph representations. We further demonstrate its\nsymbolic processing abilities in semantic reasoning and cognitive modelling\nwith practical examples and provide a short perspective on future developments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u53cb\u597d\u7684\u56fe\u6570\u636e\u5e93\u6a21\u578bViews\uff0c\u4f18\u5316\u4e86\u5b58\u50a8\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u7b26\u53f7AI\u548cRAG\u3002", "motivation": "\u5f53\u524d\u56fe\u6570\u636e\u5e93\u6a21\u578b\u672a\u9488\u5bf9\u786c\u4ef6\u52a0\u901f\u4f18\u5316\uff0c\u5bfc\u81f4\u5b58\u50a8\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u800c\u7b26\u53f7AI\u548cRAG\u9700\u8981\u9ad8\u6548\u7684\u5173\u7cfb\u6570\u636e\u5904\u7406\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e13\u4e3a\u9ad8\u6548\u5b58\u50a8\u548c\u68c0\u7d22\u7684Views\u6570\u636e\u7ed3\u6784\uff0c\u9a8c\u8bc1\u5176\u4e0e\u4f20\u7edf\u56fe\u8868\u793a\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u8bed\u4e49\u63a8\u7406\u548c\u8ba4\u77e5\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u3002", "result": "Views\u6a21\u578b\u5728\u5b58\u50a8\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u6709\u6548\u652f\u6301\u7b26\u53f7\u5904\u7406\u4efb\u52a1\u3002", "conclusion": "Views\u4e3a\u56fe\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u786c\u4ef6\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u6709\u671b\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u53d1\u5c55\u3002"}}
{"id": "2508.17826", "pdf": "https://arxiv.org/pdf/2508.17826", "abs": "https://arxiv.org/abs/2508.17826", "authors": ["Kaiyan Chang", "Wenlong Zhu", "Shengwen Liang", "Huawei Li", "Ying Wang"], "title": "LLMulator: Generalizable Cost Modeling for Dataflow Accelerators with Input-Adaptive Control Flow", "categories": ["cs.AR"], "comment": "Accepted by MICRO (IEEE/ACM International Symposium on\n  Microarchitecture) 2025", "summary": "Accurate and fast performance prediction for dataflow-based accelerators is\nvital for efficient hardware design and design space exploration, yet existing\nmethods struggle to generalize across architectures, applications, and\ninput-dependent control flows. We present LLMulator, a progressive numeric\nmodeling framework leveraging the program semantic knowledge of pre-trained\nlarge language models (LLMs) for robust, hardware- and application-aware\nprediction. Our numeric model treats performance values as categorical token\nsequences, enabling range-agnostic estimates and confidence-aware predictions\nfor unseen applications. To handle input-dependent control flows, we introduce\na reinforcement learning-based dynamic calibration method, reducing cycle\nprediction error by 9.7% over static models and converging to 11.2% error after\na few iterations. For cross-hardware generalization, we develop a progressive\ndata augmentation strategy that generates diverse datasets covering multi-level\ndataflow structures, memory parameters, and loop mapping primitives,\nsignificantly boosting prediction accuracy across architectures and\nconfigurations.", "AI": {"tldr": "LLMulator \u662f\u4e00\u4e2a\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a0b\u5e8f\u8bed\u4e49\u77e5\u8bc6\u8fdb\u884c\u6027\u80fd\u9884\u6d4b\u7684\u6570\u5b57\u5efa\u6a21\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u6570\u636e\u6d41\u52a0\u901f\u5668\u3002\u5b83\u901a\u8fc7\u5c06\u6027\u80fd\u503c\u89c6\u4e3a\u5206\u7c7b\u4ee4\u724c\u5e8f\u5217\uff0c\u5b9e\u73b0\u8303\u56f4\u65e0\u5173\u4f30\u8ba1\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5404\u79cd\u67b6\u6784\u3001\u5e94\u7528\u548c\u8f93\u5165\u4f9d\u8d56\u63a7\u5236\u6d41\u4e2d\u6cdb\u5316\uff0c\u5f71\u54cd\u4e86\u6570\u636e\u6d41\u52a0\u901f\u5668\u7684\u6027\u80fd\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u9884\u6d4b\u6846\u67b6\u3002", "method": "LLMulator \u7ed3\u5408\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u77e5\u8bc6\uff0c\u5c06\u6027\u80fd\u503c\u5efa\u6a21\u4e3a\u5206\u7c7b\u4ee4\u724c\u5e8f\u5217\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u6821\u51c6\u65b9\u6cd5\u548c\u6e10\u8fdb\u5f0f\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u52a8\u6001\u6821\u51c6\u65b9\u6cd5\u5c06\u5468\u671f\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e 9.7%\uff0c\u5e76\u5728\u591a\u6b21\u8fed\u4ee3\u540e\u6536\u655b\u81f3 11.2%\u3002\u6570\u636e\u589e\u5f3a\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u8de8\u67b6\u6784\u548c\u914d\u7f6e\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "LLMulator \u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u6027\u80fd\u9884\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6570\u636e\u6d41\u52a0\u901f\u5668\u7684\u8bbe\u8ba1\u548c\u7a7a\u95f4\u63a2\u7d22\u3002"}}
{"id": "2508.16606", "pdf": "https://arxiv.org/pdf/2508.16606", "abs": "https://arxiv.org/abs/2508.16606", "authors": ["Yogesh Kumar Meena", "Manish Salvi"], "title": "Multimodal Appearance based Gaze-Controlled Virtual Keyboard with Synchronous Asynchronous Interaction for Low-Resource Settings", "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "Over the past decade, the demand for communication devices has increased\namong individuals with mobility and speech impairments. Eye-gaze tracking has\nemerged as a promising solution for hands-free communication; however,\ntraditional appearance-based interfaces often face challenges such as accuracy\nissues, involuntary eye movements, and difficulties with extensive command\nsets. This work presents a multimodal appearance-based gaze-controlled virtual\nkeyboard that utilises deep learning in conjunction with standard camera\nhardware, incorporating both synchronous and asynchronous modes for command\nselection. The virtual keyboard application supports menu-based selection with\nnine commands, enabling users to spell and type up to 56 English characters,\nincluding uppercase and lowercase letters, punctuation, and a delete function\nfor corrections. The proposed system was evaluated with twenty able-bodied\nparticipants who completed specially designed typing tasks using three input\nmodalities: (i) a mouse, (ii) an eye-tracker, and (iii) an unmodified webcam.\nTyping performance was measured in terms of speed and information transfer rate\n(ITR) at both command and letter levels. Average typing speeds were 18.3+-5.31\nletters/min (mouse), 12.60+-2.99letters/min (eye-tracker, synchronous), 10.94\n+- 1.89 letters/min (webcam, synchronous), 11.15 +- 2.90 letters/min\n(eye-tracker, asynchronous), and 7.86 +- 1.69 letters/min (webcam,\nasynchronous). ITRs were approximately 80.29 +- 15.72 bits/min (command level)\nand 63.56 +- 11 bits/min (letter level) with webcam in synchronous mode. The\nsystem demonstrated good usability and low workload with webcam input,\nhighlighting its user-centred design and promise as an accessible communication\ntool in low-resource settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u591a\u6a21\u6001\u89c6\u7ebf\u63a7\u5236\u7684\u865a\u62df\u952e\u76d8\uff0c\u652f\u6301\u540c\u6b65\u548c\u5f02\u6b65\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u884c\u52a8\u548c\u8bed\u8a00\u969c\u788d\u8005\u3002\u6d4b\u8bd5\u663e\u793a\u5176\u5177\u6709\u826f\u597d\u7684\u53ef\u7528\u6027\u548c\u4f4e\u5de5\u4f5c\u8d1f\u8377\u3002", "motivation": "\u4f20\u7edf\u89c6\u7ebf\u8ffd\u8e2a\u6280\u672f\u5b58\u5728\u51c6\u786e\u6027\u548c\u64cd\u4f5c\u590d\u6742\u6027\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u666e\u901a\u6444\u50cf\u5934\uff0c\u5f00\u53d1\u591a\u6a21\u6001\u865a\u62df\u952e\u76d8\uff0c\u652f\u6301\u4e5d\u79cd\u547d\u4ee4\u548c56\u4e2a\u5b57\u7b26\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u8f93\u5165\u65b9\u5f0f\u6d4b\u8bd5\u6027\u80fd\u3002", "result": "\u952e\u76d8\u8f93\u5165\u901f\u5ea6\u53ef\u8fbe7.86-18.3\u5b57\u6bcd/\u5206\u949f\uff0c\u4fe1\u606f\u4f20\u8f93\u7387\u4e3a63.56-80.29\u6bd4\u7279/\u5206\u949f\uff0c\u6444\u50cf\u5934\u8f93\u5165\u8868\u73b0\u51fa\u826f\u597d\u7684\u53ef\u7528\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u9002\u5408\u4f4e\u8d44\u6e90\u73af\u5883\uff0c\u4e3a\u884c\u52a8\u548c\u8bed\u8a00\u969c\u788d\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6c9f\u901a\u5de5\u5177\u3002"}}
{"id": "2508.17655", "pdf": "https://arxiv.org/pdf/2508.17655", "abs": "https://arxiv.org/abs/2508.17655", "authors": ["Hayato Goto", "Ryo Hidaka", "Kosuke Tatsumura"], "title": "Harnessing the edge of chaos for combinatorial optimization", "categories": ["quant-ph", "cs.ET", "nlin.CD", "physics.app-ph", "physics.comp-ph"], "comment": "14 pages, 4 figures, 3 tables", "summary": "Nonlinear dynamical systems with continuous variables can be used for solving\ncombinatorial optimization problems with discrete variables.In particular,\nnumerical simulations of them can be used as heuristic algorithms with a\ndesirable property, namely, parallelizability, which allows us to execute them\nin a massively parallel manner using cutting-edge many-core processors, leading\nto ultrafast performance. However, the dynamical-system approaches with\ncontinuous variables are usually less accurate than conventional approaches\nwith discrete variables such as simulated annealing. To improve the solution\naccuracy of a representative dynamical system-based algorithm called simulated\nbifurcation (SB), which was found from classical simulation of a quantum\nnonlinear oscillator network exhibiting quantum bifurcation, here we generalize\nit by introducing nonlinear control of individual bifurcation parameters and\nshow that the generalized SB (GSB) can achieve almost 100% success\nprobabilities for some large-scale problems. As a result, the time to solution\nfor a 2,000-variable problem is shortened to 10 ms by a GSB-based machine,\nwhich is two orders of magnitude shorter than the best known value, 1.3 s,\npreviously obtained by an SB-based machine. To examine the reason for the\nultrahigh performance, we investigated chaos in the GSB changing the\nnonlinear-control strength and found that the dramatic increase of success\nprobabilities happens near the edge of chaos. That is, the GSB can find a\nsolution with high probability by harnessing the edge of chaos. This finding\nsuggests that dynamical-system approaches to combinatorial optimization will be\nenhanced by harnessing the edge of chaos, opening a broad possibility to tackle\nintractable combinatorial optimization problems by nature-inspired approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u6a21\u62df\u5206\u5c94\uff08GSB\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u63a7\u5236\u4e2a\u4f53\u5206\u5c94\u53c2\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u6210\u529f\u7387\uff0c\u5e76\u5b9e\u73b0\u4e86\u8d85\u5feb\u7684\u8ba1\u7b97\u901f\u5ea6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u5347\u57fa\u4e8e\u8fde\u7eed\u53d8\u91cf\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u7ec4\u5408\u4f18\u5316\u7b97\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u6a21\u62df\u5206\u5c94\uff08SB\uff09\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u51c6\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027\u63a7\u5236\u4e2a\u4f53\u5206\u62d0\u53c2\u6570\uff0c\u5bf9\u6a21\u62df\u5206\u5c94\u7b97\u6cd5\u8fdb\u884c\u5e7f\u4e49\u5316\uff08GSB\uff09\uff0c\u5e76\u7814\u7a76\u5176\u5728\u6df7\u6c8c\u8fb9\u7f18\u7684\u8868\u73b0\u3002", "result": "GSB\u7b97\u6cd5\u57282000\u53d8\u91cf\u95ee\u9898\u4e0a\u5c06\u89e3\u51b3\u65f6\u95f4\u7f29\u77ed\u81f310\u6beb\u79d2\uff0c\u6bd4\u539fSB\u7b97\u6cd5\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u4e14\u6210\u529f\u7387\u63a5\u8fd1100%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u6df7\u6c8c\u8fb9\u7f18\u7684\u6027\u8d28\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u52a8\u529b\u5b66\u7cfb\u7edf\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u81ea\u7136\u542f\u53d1\u7684\u4f18\u5316\u65b9\u6cd5\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.17493", "pdf": "https://arxiv.org/pdf/2508.17493", "abs": "https://arxiv.org/abs/2508.17493", "authors": ["Jeremy Kepner", "Chansup Byun", "LaToya Anderson", "William Arcand", "David Bestor", "William Bergeron", "Alex Bonn", "Daniel Burrill", "Vijay Gadepally", "Ryan Haney", "Michael Houle", "Matthew Hubbell", "Hayden Jananthan", "Michael Jones", "Piotr Luszczek", "Lauren Milechin", "Guillermo Morales", "Julie Mullen", "Andrew Prout", "Albert Reuther", "Antonio Rosa", "Charles Yee", "Peter Michaleas"], "title": "Easy Acceleration with Distributed Arrays", "categories": ["cs.DC", "cs.CE", "cs.MS", "cs.PF"], "comment": "8 pages, 4 figures, 2 tables, 2 algorithm listings, 2 code listings,\n  to appear in IEEE HPEC 2025", "summary": "High level programming languages and GPU accelerators are powerful enablers\nfor a wide range of applications. Achieving scalable vertical (within a compute\nnode), horizontal (across compute nodes), and temporal (over different\ngenerations of hardware) performance while retaining productivity requires\neffective abstractions. Distributed arrays are one such abstraction that\nenables high level programming to achieve highly scalable performance.\nDistributed arrays achieve this performance by deriving parallelism from data\nlocality, which naturally leads to high memory bandwidth efficiency. This paper\nexplores distributed array performance using the STREAM memory bandwidth\nbenchmark on a variety of hardware. Scalable performance is demonstrated within\nand across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across\nmultiple nodes was linear. The hardware used spans decades and allows a direct\ncomparison of hardware improvements for memory bandwidth over this time range;\nshowing a 10x increase in CPU core bandwidth over 20 years, 100x increase in\nCPU node bandwidth over 20 years, and 5x increase in GPU node bandwidth over 5\nyears. Running on hundreds of MIT SuperCloud nodes simultaneously achieved a\nsustained bandwidth $>$1 PB/s.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5206\u5e03\u5f0f\u6570\u7ec4\u5728\u4e0d\u540c\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u5176\u5728CPU\u548cGPU\u8282\u70b9\u4e0a\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u5e74\u4ee3\u786c\u4ef6\u5728\u5185\u5b58\u5e26\u5bbd\u4e0a\u7684\u6539\u8fdb\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u4fdd\u6301\u9ad8\u751f\u4ea7\u529b\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u6570\u7ec4\u8fd9\u4e00\u62bd\u8c61\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5782\u76f4\u3001\u6c34\u5e73\u548c\u65f6\u95f4\u6027\u80fd\u3002", "method": "\u4f7f\u7528STREAM\u5185\u5b58\u5e26\u5bbd\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5206\u5e03\u5f0f\u6570\u7ec4\u5728\u591a\u79cd\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\uff0c\u5305\u62ecCPU\u6838\u5fc3\u3001CPU\u8282\u70b9\u548cGPU\u8282\u70b9\u3002", "result": "\u6a2a\u5411\u6269\u5c55\u6027\u80fd\u7ebf\u6027\u589e\u957f\uff0c\u786c\u4ef6\u5728\u5185\u5b58\u5e26\u5bbd\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u6570\u767e\u4e2aMIT SuperCloud\u8282\u70b9\u5b9e\u73b0\u4e86>1 PB/s\u7684\u6301\u7eed\u5e26\u5bbd\u3002", "conclusion": "\u5206\u5e03\u5f0f\u6570\u7ec4\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u62bd\u8c61\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6269\u5c55\u6027\u6027\u80fd\uff0c\u540c\u65f6\u9002\u5e94\u4e0d\u540c\u5e74\u4ee3\u7684\u786c\u4ef6\u6539\u8fdb\u3002"}}
{"id": "2508.17386", "pdf": "https://arxiv.org/pdf/2508.17386", "abs": "https://arxiv.org/abs/2508.17386", "authors": ["Shlomi Steinberg", "Matt Pharr"], "title": "Wave Tracing: Generalizing The Path Integral To Wave Optics", "categories": ["physics.optics", "cs.GR", "I.2.6"], "comment": null, "summary": "Modeling the wave nature of light and the propagation and diffraction of\nelectromagnetic fields is crucial for the accurate simulation of many\nphenomena, yet wave simulations are significantly more computationally complex\nthan classical ray-based models. In this work, we start by analyzing the\nclassical path integral formulation of light transport and rigorously study\nwhich wave-optical phenomena can be reproduced by it. We then introduce a\nbilinear path integral generalization for wave-optical light transport that\nmodels the wave interference between paths. This formulation subsumes many\nexisting methods that rely on shooting-bouncing rays or UTD-based diffractions,\nand serves to give insight into the challenges of such approaches and the\ndifficulty of sampling good paths in a bilinear setting.\n  With this foundation, we develop a weakly-local path integral based on\nregion-to-region transport using elliptical cones that allows sampling\nindividual paths that still model wave effects accurately. As with the classic\npath integral form of the light transport equation, our path integral makes it\npossible to derive a variety of practical transport algorithms. We present a\ncomplete system for wave tracing with elliptical cones, with applications in\nlight transport for rendering and efficient simulation of long-wavelength\nradiation propagation and diffraction in complex environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u7ebf\u6027\u8def\u5f84\u79ef\u5206\u7684\u6ce2\u5149\u5b66\u5149\u4f20\u8f93\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u8def\u5f84\u79ef\u5206\u65b9\u6cd5\uff0c\u4ee5\u51c6\u786e\u6a21\u62df\u6ce2\u7684\u5e72\u6d89\u6548\u5e94\uff0c\u5e76\u5f00\u53d1\u4e86\u533a\u57df\u5230\u533a\u57df\u4f20\u8f93\u7684\u5f31\u5c40\u90e8\u8def\u5f84\u79ef\u5206\u7cfb\u7edf\u3002", "motivation": "\u6ce2\u5149\u5b66\u73b0\u8c61\uff08\u5982\u5e72\u6d89\u548c\u884d\u5c04\uff09\u7684\u51c6\u786e\u6a21\u62df\u5bf9\u8bb8\u591a\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u5149\u7ebf\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u6ce2\u7684\u7279\u6027\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u80fd\u6709\u6548\u6a21\u62df\u6ce2\u6548\u5e94\u7684\u8def\u5f84\u79ef\u5206\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7ecf\u5178\u8def\u5f84\u79ef\u5206\uff0c\u63d0\u51fa\u53cc\u7ebf\u6027\u8def\u5f84\u79ef\u5206\u6269\u5c55\u4ee5\u6a21\u62df\u8def\u5f84\u95f4\u7684\u6ce2\u5e72\u6d89\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u692d\u5706\u9525\u7684\u5f31\u5c40\u90e8\u8def\u5f84\u79ef\u5206\u7cfb\u7edf\uff0c\u5b9e\u73b0\u533a\u57df\u5230\u533a\u57df\u7684\u6ce2\u6548\u5e94\u5efa\u6a21\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u6a21\u62df\u6ce2\u5149\u5b66\u73b0\u8c61\uff0c\u652f\u6301\u591a\u79cd\u5b9e\u9645\u4f20\u8f93\u7b97\u6cd5\u7684\u63a8\u5bfc\uff0c\u5e76\u5e94\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u5149\u4f20\u8f93\u548c\u957f\u6ce2\u957f\u8f90\u5c04\u6a21\u62df\u3002", "conclusion": "\u6269\u5c55\u7684\u8def\u5f84\u79ef\u5206\u6846\u67b6\u4e3a\u6ce2\u5149\u5b66\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7406\u8bba\u57fa\u7840\u575a\u5b9e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6e32\u67d3\u548c\u8f90\u5c04\u4f20\u64ad\u7684\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.17221", "pdf": "https://arxiv.org/pdf/2508.17221", "abs": "https://arxiv.org/abs/2508.17221", "authors": ["Sopam Dasgupta", "Sadaf MD Halim", "Joaqu\u00edn Arias", "Elmer Salazar", "Gopal Gupta"], "title": "MC3G: Model Agnostic Causally Constrained Counterfactual Generation", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Machine learning models increasingly influence decisions in high-stakes\nsettings such as finance, law and hiring, driving the need for transparent,\ninterpretable outcomes. However, while explainable approaches can help\nunderstand the decisions being made, they may inadvertently reveal the\nunderlying proprietary algorithm: an undesirable outcome for many\npractitioners. Consequently, it is crucial to balance meaningful transparency\nwith a form of recourse that clarifies why a decision was made and offers\nactionable steps following which a favorable outcome can be obtained.\nCounterfactual explanations offer a powerful mechanism to address this need by\nshowing how specific input changes lead to a more favorable prediction. We\npropose Model-Agnostic Causally Constrained Counterfactual Generation (MC3G), a\nnovel framework that tackles limitations in the existing counterfactual\nmethods. First, MC3G is model-agnostic: it approximates any black-box model\nusing an explainable rule-based surrogate model. Second, this surrogate is used\nto generate counterfactuals that produce a favourable outcome for the original\nunderlying black box model. Third, MC3G refines cost computation by excluding\nthe ``effort\" associated with feature changes that occur automatically due to\ncausal dependencies. By focusing only on user-initiated changes, MC3G provides\na more realistic and fair representation of the effort needed to achieve a\nfavourable outcome. We show that MC3G delivers more interpretable and\nactionable counterfactual recommendations compared to existing techniques all\nwhile having a lower cost. Our findings highlight MC3G's potential to enhance\ntransparency, accountability, and practical utility in decision-making\nprocesses that incorporate machine-learning approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6MC3G\uff0c\u7528\u4e8e\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u4ee5\u5e73\u8861\u900f\u660e\u6027\u4e0e\u4fdd\u62a4\u7b97\u6cd5\u9690\u79c1\u7684\u9700\u6c42\u3002MC3G\u80fd\u9002\u914d\u4efb\u4f55\u9ed1\u76d2\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u4ee3\u7406\u6a21\u578b\u751f\u6210\u66f4\u6709\u9488\u5bf9\u6027\u7684\u53cd\u4e8b\u5b9e\u5efa\u8bae\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u573a\u666f\uff08\u5982\u91d1\u878d\u3001\u6cd5\u5f8b\u548c\u62db\u8058\uff09\u4e2d\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u51b3\u5b9a\u9700\u8981\u900f\u660e\u5316\u548c\u89e3\u91ca\u6027\uff0c\u4f46\u540c\u65f6\u9700\u907f\u514d\u6cc4\u9732\u7b97\u6cd5\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5e73\u8861\u900f\u660e\u6027\u4e0e\u9690\u79c1\u7684\u65b9\u6cd5\u3002", "method": "MC3G\u6846\u67b6\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u4ee3\u7406\u6a21\u578b\u8fd1\u4f3c\u9ed1\u76d2\u6a21\u578b\uff0c\u751f\u6210\u9488\u5bf9\u539f\u59cb\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5e76\u4f18\u5316\u6210\u672c\u8ba1\u7b97\uff0c\u5ffd\u7565\u56e0\u679c\u4f9d\u8d56\u5bfc\u81f4\u7684\u81ea\u52a8\u7279\u5f81\u53d8\u5316\u3002", "result": "MC3G\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u4f9b\u66f4\u5177\u53ef\u64cd\u4f5c\u6027\u548c\u89e3\u91ca\u6027\u7684\u53cd\u4e8b\u5b9e\u5efa\u8bae\uff0c\u540c\u65f6\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "MC3G\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u51b3\u7b56\u8fc7\u7a0b\u7684\u900f\u660e\u6027\u3001\u53ef\u95ee\u8d23\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u4f9d\u8d56\u673a\u5668\u5b66\u4e60\u7684\u573a\u666f\u3002"}}
{"id": "2508.17280", "pdf": "https://arxiv.org/pdf/2508.17280", "abs": "https://arxiv.org/abs/2508.17280", "authors": ["Ruichao Hou", "Boyue Xu", "Tongwei Ren", "Gangshan Wu"], "title": "MTNet: Learning modality-aware representation with transformer for RGBT tracking", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "The ability to learn robust multi-modality representation has played a\ncritical role in the development of RGBT tracking. However, the regular fusion\nparadigm and the invariable tracking template remain restrictive to the feature\ninteraction. In this paper, we propose a modality-aware tracker based on\ntransformer, termed MTNet. Specifically, a modality-aware network is presented\nto explore modality-specific cues, which contains both channel aggregation and\ndistribution module(CADM) and spatial similarity perception module (SSPM). A\ntransformer fusion network is then applied to capture global dependencies to\nreinforce instance representations. To estimate the precise location and tackle\nthe challenges, such as scale variation and deformation, we design a trident\nprediction head and a dynamic update strategy which jointly maintain a reliable\ntemplate for facilitating inter-frame communication. Extensive experiments\nvalidate that the proposed method achieves satisfactory results compared with\nthe state-of-the-art competitors on three RGBT benchmarks while reaching\nreal-time speed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u611f\u77e5\u8ddf\u8e2a\u5668MTNet\uff0c\u901a\u8fc7\u901a\u9053\u805a\u5408\u4e0e\u5206\u5e03\u6a21\u5757\u548c\u7a7a\u95f4\u76f8\u4f3c\u6027\u611f\u77e5\u6a21\u5757\u6355\u6349\u6a21\u6001\u7279\u5f02\u6027\u7ebf\u7d22\uff0c\u7ed3\u5408Transformer\u878d\u5408\u7f51\u7edc\u5f3a\u5316\u5b9e\u4f8b\u8868\u5f81\uff0c\u5e76\u91c7\u7528\u4e09\u53c9\u9884\u6d4b\u5934\u548c\u52a8\u6001\u66f4\u65b0\u7b56\u7565\u63d0\u5347\u8ddf\u8e2a\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684RGB-T\u8ddf\u8e2a\u65b9\u6cd5\u5728\u7279\u5f81\u878d\u5408\u548c\u6a21\u677f\u66f4\u65b0\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u6a21\u6001\u611f\u77e5\u7f51\u7edc\uff08\u542b\u901a\u9053\u805a\u5408\u4e0e\u5206\u5e03\u6a21\u5757\u548c\u7a7a\u95f4\u76f8\u4f3c\u6027\u611f\u77e5\u6a21\u5757\uff09\u548cTransformer\u878d\u5408\u7f51\u7edc\uff0c\u7ed3\u5408\u4e09\u53c9\u9884\u6d4b\u5934\u548c\u52a8\u6001\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2aRGB-T\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u5b9e\u65f6\u901f\u5ea6\u3002", "conclusion": "MTNet\u901a\u8fc7\u5168\u5c40\u4f9d\u8d56\u6027\u548c\u52a8\u6001\u6a21\u677f\u66f4\u65b0\u89e3\u51b3\u4e86\u73b0\u6709\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.18151", "pdf": "https://arxiv.org/pdf/2508.18151", "abs": "https://arxiv.org/abs/2508.18151", "authors": ["Zhuo Ma", "Dong Wen", "Kaiyu Chen", "Yixiang Fang", "Xuemin Lin", "Wenjie Zhang"], "title": "Accelerating Historical K-Core Search in Temporal Graphs", "categories": ["cs.DB"], "comment": null, "summary": "We study the temporal k-core component search (TCCS), which outputs the\nk-core containing the query vertex in the snapshot over an arbitrary query time\nwindow in a temporal graph. The problem has been shown to be critical for tasks\nsuch as contact tracing, fault diagnosis, and financial forensics. The\nstate-of-the-art EF-Index designs a separated forest structure for a set of\ncarefully selected windows, incurring quadratic preprocessing time and large\nredundant storage. Our method introduces the ECB-forest, a compact edge-centric\nbinary forest that captures k-core of any arbitrary query vertex over time. In\nthis way, a query can be processed by searching a connected component in the\nforest. We develop an efficient algorithm for index construction. Experiments\non real-world temporal graphs show that our method significantly improves the\nindex size and construction cost (up to 100x faster on average) while\nmaintaining the high query efficiency.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u79cd\u7528\u4e8e\u65f6\u6001\u56fe\u7684k-core\u641c\u7d22\u65b9\u6cd5\uff08TCCS\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u51d1\u7684\u8fb9\u7f18\u4e2d\u5fc3\u4e8c\u53c9\u68ee\u6797\uff08ECB-forest\uff09\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7d22\u5f15\u6784\u5efa\u6548\u7387\u548c\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "TCCS\u95ee\u9898\u5728\u63a5\u89e6\u8ffd\u8e2a\u3001\u6545\u969c\u8bca\u65ad\u548c\u91d1\u878d\u53d6\u8bc1\u7b49\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u9884\u5904\u7406\u65f6\u95f4\u957f\u548c\u5b58\u50a8\u5197\u4f59\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faECB-forest\u7ed3\u6784\uff0c\u901a\u8fc7\u8fb9\u4e2d\u5fc3\u4e8c\u53c9\u68ee\u6797\u6355\u83b7\u4efb\u610f\u67e5\u8be2\u65f6\u95f4\u7a97\u53e3\u7684k-core\uff0c\u6784\u5efa\u9ad8\u6548\u7d22\u5f15\u7b97\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u65f6\u6001\u56fe\u4e0a\uff0c\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u7d22\u5f15\u5927\u5c0f\u548c\u6784\u5efa\u6210\u672c\uff08\u5e73\u5747\u5feb100\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u67e5\u8be2\u6548\u7387\u3002", "conclusion": "ECB-forest\u4e3a\u89e3\u51b3TCCS\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7d27\u51d1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18043", "pdf": "https://arxiv.org/pdf/2508.18043", "abs": "https://arxiv.org/abs/2508.18043", "authors": ["Johan S\u00f6derstr\u00f6m", "Yuan Yao"], "title": "Anatomy of the gem5 Simulator: AtomicSimpleCPU, TimingSimpleCPU, O3CPU, and Their Interaction with the Ruby Memory System", "categories": ["cs.AR"], "comment": null, "summary": "gem5 is a popular modular-based computer system simulator, widely used in\ncomputer architecture research and known for its long simulation time and steep\nlearning curve. This report examines its three major CPU models: the\nAtomicSimpleCPU (AS CPU), the TimingSimpleCPU (TS CPU), the Out-of-order (O3)\nCPU, and their interactions with the memory subsystem. We provide a detailed\nanatomical overview of each CPU's function call-chains and present how gem5\npartitions its execution time for each simulated hardware layer.\n  We perform our analysis using a lightweight profiler built on Linux's\nperf_event interface, with user-configurable options to target specific\nfunctions and examine their interactions in detail. By profiling each CPU\nacross a wide selection of benchmarks, we identify their software bottlenecks.\nOur results show that the Ruby memory subsystem consistently accounts for the\nlargest share of execution time in the sequential AS and TS CPUs, primarily\nduring the instruction fetch stage. In contrast, the O3 CPU spends a relatively\nsmaller fraction of time in Ruby, with most of its time devoted to constructing\ninstruction instances and the various pipeline stages of the CPU.\n  We believe that the anatomical view of each CPU's execution flow is valuable\nfor educational purposes, as it clearly illustrates the interactions among\nsimulated components. These insights form a foundation for optimizing gem5's\nperformance, particularly for the AS, TS, and O3 CPUs. Moreover, our framework\ncan be readily applied to analyze other gem5 components or to develop and\nevaluate new models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86gem5\u6a21\u62df\u5668\u7684\u4e09\u79cd\u4e3b\u8981CPU\u6a21\u578b\uff08AS CPU\u3001TS CPU\u3001O3 CPU\uff09\u53ca\u5176\u4e0e\u5185\u5b58\u5b50\u7cfb\u7edf\u7684\u4ea4\u4e92\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6027\u80fd\u5206\u6790\u5de5\u5177\u8bc6\u522b\u4e86\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u5efa\u8bae\u3002", "motivation": "gem5\u56e0\u5176\u8f83\u957f\u7684\u6a21\u62df\u65f6\u95f4\u548c\u9661\u5ced\u7684\u5b66\u4e60\u66f2\u7ebf\uff0c\u5728\u8ba1\u7b97\u673a\u67b6\u6784\u7814\u7a76\u4e2d\u5e7f\u6cdb\u5e94\u7528\u4f46\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u5176CPU\u6a21\u578b\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLinux perf_event\u63a5\u53e3\u7684\u8f7b\u91cf\u7ea7\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u5bf9\u4e09\u79cdCPU\u6a21\u578b\u8fdb\u884c\u8be6\u7ec6\u7684\u529f\u80fd\u8c03\u7528\u94fe\u5206\u6790\u548c\u6267\u884c\u65f6\u95f4\u5206\u914d\u7814\u7a76\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff0cRuby\u5185\u5b58\u5b50\u7cfb\u7edf\u5728AS\u548cTS CPU\u4e2d\u5360\u7528\u6700\u591a\u6267\u884c\u65f6\u95f4\uff08\u4e3b\u8981\u5728\u6307\u4ee4\u83b7\u53d6\u9636\u6bb5\uff09\uff0c\u800cO3 CPU\u5219\u5c06\u5927\u90e8\u5206\u65f6\u95f4\u7528\u4e8e\u6784\u5efa\u6307\u4ee4\u5b9e\u4f8b\u548c\u6d41\u6c34\u7ebf\u9636\u6bb5\u3002", "conclusion": "\u8bba\u6587\u63d0\u4f9b\u4e86CPU\u6267\u884c\u6d41\u7684\u8be6\u7ec6\u89e3\u5256\u89c6\u56fe\uff0c\u4e3a\u4f18\u5316gem5\u6027\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5206\u6790\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u7ec4\u4ef6\u6216\u65b0\u6a21\u578b\u5f00\u53d1\u3002"}}
{"id": "2508.16607", "pdf": "https://arxiv.org/pdf/2508.16607", "abs": "https://arxiv.org/abs/2508.16607", "authors": ["Sanika Moharana", "Cynthia L. Bennett", "Erin Buehler", "Michael Madaio", "Vinita Tibdewal", "Shaun K. Kane"], "title": "\"Accessibility people, you go work on that thing of yours over there\": Addressing Disability Inclusion in AI Product Organizations", "categories": ["cs.HC", "cs.AI"], "comment": "To appear in Proceedings of AIES 2025", "summary": "The rapid emergence of generative AI has changed the way that technology is\ndesigned, constructed, maintained, and evaluated. Decisions made when creating\nAI-powered systems may impact some users disproportionately, such as people\nwith disabilities. In this paper, we report on an interview study with 25 AI\npractitioners across multiple roles (engineering, research, UX, and responsible\nAI) about how their work processes and artifacts may impact end users with\ndisabilities. We found that practitioners experienced friction when triaging\nproblems at the intersection of responsible AI and accessibility practices,\nnavigated contradictions between accessibility and responsible AI guidelines,\nidentified gaps in data about users with disabilities, and gathered support for\naddressing the needs of disabled stakeholders by leveraging informal volunteer\nand community groups within their company. Based on these findings, we offer\nsuggestions for new resources and process changes to better support people with\ndisabilities as end users of AI.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u7684\u5d1b\u8d77\u6539\u53d8\u4e86\u6280\u672f\u8bbe\u8ba1\u3001\u6784\u5efa\u3001\u7ef4\u62a4\u548c\u8bc4\u4f30\u7684\u65b9\u5f0f\uff0c\u4f46\u4e5f\u53ef\u80fd\u5bf9\u6b8b\u969c\u7528\u6237\u9020\u6210\u4e0d\u6210\u6bd4\u4f8b\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5bf925\u540dAI\u4ece\u4e1a\u8005\u7684\u8bbf\u8c08\u7814\u7a76\uff0c\u53d1\u73b0\u4ed6\u4eec\u5728\u8d1f\u8d23\u4efbAI\u4e0e\u65e0\u969c\u788d\u5b9e\u8df5\u4e4b\u95f4\u5b58\u5728\u6469\u64e6\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u6280\u672f\u5bf9\u6b8b\u969c\u7528\u6237\u7684\u6f5c\u5728\u5f71\u54cd\uff0c\u7814\u7a76AI\u4ece\u4e1a\u8005\u5728\u8bbe\u8ba1\u548c\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u7684\u6311\u6218\u3002", "method": "\u5bf925\u540d\u6765\u81ea\u4e0d\u540c\u89d2\u8272\uff08\u5de5\u7a0b\u3001\u7814\u7a76\u3001\u7528\u6237\u4f53\u9a8c\u3001\u8d1f\u8d23\u4efbAI\uff09\u7684AI\u4ece\u4e1a\u8005\u8fdb\u884c\u8bbf\u8c08\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u4ece\u4e1a\u8005\u5728\u8d1f\u8d23\u4efbAI\u4e0e\u65e0\u969c\u788d\u5b9e\u8df5\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u5e76\u5b58\u5728\u7528\u6237\u6570\u636e\u7f3a\u53e3\uff0c\u9700\u901a\u8fc7\u975e\u6b63\u5f0f\u5fd7\u613f\u8005\u548c\u793e\u533a\u56e2\u4f53\u652f\u6301\u6b8b\u969c\u7528\u6237\u9700\u6c42\u3002", "conclusion": "\u63d0\u51fa\u6539\u8fdb\u8d44\u6e90\u548c\u6d41\u7a0b\u7684\u5efa\u8bae\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u6b8b\u969c\u7528\u6237\u4f5c\u4e3aAI\u7684\u7ec8\u7aef\u7528\u6237\u3002"}}
{"id": "2508.17913", "pdf": "https://arxiv.org/pdf/2508.17913", "abs": "https://arxiv.org/abs/2508.17913", "authors": ["Yagmur Yigit", "Mehmet Ali Erturk", "Kerem Gursu", "Berk Canberk"], "title": "PRZK-Bind: A Physically Rooted Zero-Knowledge Authentication Protocol for Secure Digital Twin Binding in Smart Cities", "categories": ["cs.CR", "cs.ET", "cs.NI"], "comment": "6 pages, 4 figures, 2 tables, Accepted by IEEE Global Communications\n  Conference (GLOBECOM) 2025", "summary": "Digital twin (DT) technology is rapidly becoming essential for smart city\necosystems, enabling real-time synchronisation and autonomous decision-making\nacross physical and digital domains. However, as DTs take active roles in\ncontrol loops, securely binding them to their physical counterparts in dynamic\nand adversarial environments remains a significant challenge. Existing\nauthentication solutions either rely on static trust models, require\ncentralised authorities, or fail to provide live and verifiable\nphysical-digital binding, making them unsuitable for latency-sensitive and\ndistributed deployments. To address this gap, we introduce PRZK-Bind, a\nlightweight and decentralised authentication protocol that combines\nSchnorr-based zero-knowledge proofs with elliptic curve cryptography to\nestablish secure, real-time correspondence between physical entities and DTs\nwithout relying on pre-shared secrets. Simulation results show that PRZK-Bind\nsignificantly improves performance, offering up to 4.5 times lower latency and\n4 times reduced energy consumption compared to cryptography-heavy baselines,\nwhile maintaining false acceptance rates more than 10 times lower. These\nfindings highlight its suitability for future smart city deployments requiring\nefficient, resilient, and trustworthy DT authentication.", "AI": {"tldr": "PRZK-Bind \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u53bb\u4e2d\u5fc3\u5316\u8ba4\u8bc1\u534f\u8bae\uff0c\u901a\u8fc7\u7ed3\u5408 Schnorr \u96f6\u77e5\u8bc6\u8bc1\u660e\u548c\u692d\u5706\u66f2\u7ebf\u5bc6\u7801\u5b66\uff0c\u89e3\u51b3\u4e86\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u5b89\u5168\u7ed1\u5b9a\u7684\u6311\u6218\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u667a\u6167\u57ce\u5e02\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5728\u52a8\u6001\u5bf9\u6297\u73af\u5883\u4e2d\u5b9e\u73b0\u5176\u4e0e\u7269\u7406\u5b9e\u4f53\u7684\u5b89\u5168\u5b9e\u65f6\u7ed1\u5b9a\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002\u73b0\u6709\u8ba4\u8bc1\u65b9\u6848\u56e0\u4f9d\u8d56\u9759\u6001\u4fe1\u4efb\u6a21\u578b\u3001\u4e2d\u5fc3\u5316\u673a\u6784\u6216\u65e0\u6cd5\u63d0\u4f9b\u5b9e\u65f6\u9a8c\u8bc1\uff0c\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86 PRZK-Bind \u534f\u8bae\uff0c\u7ed3\u5408 Schnorr \u96f6\u77e5\u8bc6\u8bc1\u660e\u548c\u692d\u5706\u66f2\u7ebf\u5bc6\u7801\u5b66\uff0c\u65e0\u9700\u9884\u5171\u4eab\u5bc6\u94a5\u5373\u53ef\u5b9e\u73b0\u7269\u7406\u5b9e\u4f53\u4e0e\u6570\u5b57\u5b6a\u751f\u7684\u5b89\u5168\u5b9e\u65f6\u7ed1\u5b9a\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cPRZK-Bind \u7684\u5ef6\u8fdf\u964d\u4f4e 4.5 \u500d\uff0c\u80fd\u8017\u51cf\u5c11 4 \u500d\uff0c\u540c\u65f6\u9519\u8bef\u63a5\u53d7\u7387\u6bd4\u73b0\u6709\u65b9\u6848\u4f4e 10 \u500d\u4ee5\u4e0a\u3002", "conclusion": "PRZK-Bind \u9002\u7528\u4e8e\u672a\u6765\u667a\u6167\u57ce\u5e02\u4e2d\u9ad8\u6548\u3001\u5f39\u6027\u548c\u53ef\u4fe1\u7684\u6570\u5b57\u5b6a\u751f\u8ba4\u8bc1\u3002"}}
{"id": "2508.17593", "pdf": "https://arxiv.org/pdf/2508.17593", "abs": "https://arxiv.org/abs/2508.17593", "authors": ["Aadesh Deshmukh", "Venkata Yaswanth Raparti", "Samuel Hsu"], "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD NPUs", "categories": ["cs.DC"], "comment": null, "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.", "AI": {"tldr": "Zen-Attention\u6846\u67b6\u901a\u8fc7\u4f18\u5316DRAM\u5e26\u5bbd\u5229\u7528\uff0c\u663e\u8457\u964d\u4f4e\u4e86Transformer\u6a21\u578b\u7684\u5ef6\u8fdf\u3002", "motivation": "\u89e3\u51b3\u5728\u80fd\u6548\u548cDRAM\u5e26\u5bbd\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72Transformer\u6a21\u578b\u65f6\u7684\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c42\u6298\u53e0\u3001\u5206\u5757\u3001\u6570\u636e\u79fb\u52a8\u548c\u5e03\u5c40\u4f18\u5316\uff0c\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u8bbe\u8ba1\u7a7a\u95f4\u4ee5\u5b9e\u73b0\u6700\u4f73DRAM\u5e26\u5bbd\u5229\u7528\u3002", "result": "\u6ce8\u610f\u529b\u5757\u5ef6\u8fdf\u63d0\u53474\u500d\uff0c\u7aef\u5230\u7aef\u7f51\u7edc\u5ef6\u8fdf\u63d0\u534732%\u3002", "conclusion": "Zen-Attention\u6846\u67b6\u5728NPU\u4e0a\u9ad8\u6548\u6620\u5c04\u52a8\u6001\u6ce8\u610f\u529b\u5c42\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2508.17713", "pdf": "https://arxiv.org/pdf/2508.17713", "abs": "https://arxiv.org/abs/2508.17713", "authors": ["Zhihao Xu", "Shikai Guo", "Guilin Zhao", "Peiyu Zou", "Siwen Wang", "Qian Ma", "Hui Li", "Furui Zhan"], "title": "Code Difference Guided Fuzzing for FPGA Logic Synthesis Compilers via Bayesian Optimization", "categories": ["cs.SE", "cs.AR"], "comment": null, "summary": "Field Programmable Gate Arrays (FPGAs) play a crucial role in Electronic\nDesign Automation (EDA) applications, which have been widely used in\nsafety-critical environments, including aerospace, chip manufacturing, and\nmedical devices. A critical step in FPGA development is logic synthesis, which\nenables developers to translate their software designs into hardware net lists,\nwhich facilitates the physical implementation of the chip, detailed timing and\npower analysis, gate-level simulation, test vector generation, and optimization\nand consistency checking. However, bugs or incorrect implementations in FPGA\nlogic synthesis compilers may lead to unexpected behaviors in target\nwapplications, posing security risks. Therefore, it is crucial to eliminate\nsuch bugs in FPGA logic synthesis compilers. The effectiveness of existing\nworks is still limited by its simple, blind mutation strategy. To address this\nchallenge, we propose a guided mutation strategy based on Bayesian optimization\ncalled LSC-Fuzz to detect bugs in FPGA logic synthesis compilers. Specifically,\nLSC-Fuzz consists of three components: the test-program generation component,\nthe Bayesian diversity selection component, and the equivalent check component.\nBy performing test-program generation and Bayesian diversity selection,\nLSC-Fuzz generates diverse and complex HDL code, thoroughly testing the FPGA\nlogic synthesis compilers using equivalent check to detect bugs. Through three\nmonths, LSC-Fuzz has found 16 bugs, 12 of these has been confirmed by official\ntechnical support.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u5f15\u5bfc\u53d8\u5f02\u7b56\u7565LSC-Fuzz\uff0c\u7528\u4e8e\u68c0\u6d4bFPGA\u903b\u8f91\u7efc\u5408\u7f16\u8bd1\u5668\u4e2d\u7684\u9519\u8bef\uff0c\u901a\u8fc7\u591a\u6837\u5316\u548c\u590d\u6742\u7684HDL\u4ee3\u7801\u751f\u6210\u6765\u63d0\u5347\u6d4b\u8bd5\u6548\u679c\u3002", "motivation": "FPGA\u903b\u8f91\u7efc\u5408\u7f16\u8bd1\u5668\u4e2d\u7684\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u5173\u952e\u5e94\u7528\u4e2d\u7684\u610f\u5916\u884c\u4e3a\u548c\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u65b9\u6cd5\u7684\u7b80\u5355\u76f2\u53d8\u5f02\u7b56\u7565\u6548\u679c\u6709\u9650\u3002", "method": "LSC-Fuzz\u5305\u542b\u6d4b\u8bd5\u7a0b\u5e8f\u751f\u6210\u3001\u8d1d\u53f6\u65af\u591a\u6837\u6027\u9009\u62e9\u548c\u7b49\u6548\u68c0\u67e5\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684HDL\u4ee3\u7801\u5e76\u8fdb\u884c\u7b49\u6548\u68c0\u6d4b\u6765\u53d1\u73b0\u9519\u8bef\u3002", "result": "LSC-Fuzz\u5728\u4e09\u4e2a\u6708\u5185\u53d1\u73b0\u4e8616\u4e2a\u9519\u8bef\uff0c\u5176\u4e2d12\u4e2a\u88ab\u5b98\u65b9\u6280\u672f\u786e\u8ba4\u3002", "conclusion": "LSC-Fuzz\u901a\u8fc7\u667a\u80fd\u5316\u53d8\u5f02\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86FPGA\u903b\u8f91\u7efc\u5408\u7f16\u8bd1\u5668\u7684\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2508.17995", "pdf": "https://arxiv.org/pdf/2508.17995", "abs": "https://arxiv.org/abs/2508.17995", "authors": ["Mohamed Kissi", "Keanu Sisouk", "Joshua A. Levine", "Julien Tierny"], "title": "Topology Aware Neural Interpolation of Scalar Fields", "categories": ["cs.LG", "cs.CV", "cs.GR"], "comment": null, "summary": "This paper presents a neural scheme for the topology-aware interpolation of\ntime-varying scalar fields. Given a time-varying sequence of persistence\ndiagrams, along with a sparse temporal sampling of the corresponding scalar\nfields, denoted as keyframes, our interpolation approach aims at \"inverting\"\nthe non-keyframe diagrams to produce plausible estimations of the\ncorresponding, missing data. For this, we rely on a neural architecture which\nlearns the relation from a time value to the corresponding scalar field, based\non the keyframe examples, and reliably extends this relation to the\nnon-keyframe time steps. We show how augmenting this architecture with specific\ntopological losses exploiting the input diagrams both improves the geometrical\nand topological reconstruction of the non-keyframe time steps. At query time,\ngiven an input time value for which an interpolation is desired, our approach\ninstantaneously produces an output, via a single propagation of the time input\nthrough the network. Experiments interpolating 2D and 3D time-varying datasets\nshow our approach superiority, both in terms of data and topological fitting,\nwith regard to reference interpolation schemes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u62d3\u6251\u611f\u77e5\u65f6\u95f4\u53d8\u5316\u6807\u91cf\u573a\u63d2\u503c\u65b9\u6cd5\uff0c\u901a\u8fc7\u5173\u952e\u5e27\u548c\u975e\u5173\u952e\u5e27\u7684\u62d3\u6251\u5173\u7cfb\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u63d2\u503c\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u89e3\u51b3\u65f6\u95f4\u53d8\u5316\u6807\u91cf\u573a\u5728\u975e\u5173\u952e\u5e27\u4e0a\u7684\u63d2\u503c\u95ee\u9898\uff0c\u4ee5\u586b\u8865\u7f3a\u5931\u6570\u636e\u5e76\u63d0\u9ad8\u51e0\u4f55\u548c\u62d3\u6251\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u5173\u952e\u5e27\u5b66\u4e60\u65f6\u95f4\u503c\u4e0e\u6807\u91cf\u573a\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u62d3\u6251\u635f\u5931\u51fd\u6570\u589e\u5f3a\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u73b0\u5feb\u901f\u63d2\u503c\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57282D\u548c3D\u65f6\u95f4\u53d8\u5316\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u53c2\u8003\u63d2\u503c\u65b9\u6848\uff0c\u6570\u636e\u548c\u62d3\u6251\u62df\u5408\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u62d3\u6251\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u53d8\u5316\u6807\u91cf\u573a\u7684\u63d2\u503c\u95ee\u9898\uff0c\u5177\u6709\u9ad8\u6548\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.17285", "pdf": "https://arxiv.org/pdf/2508.17285", "abs": "https://arxiv.org/abs/2508.17285", "authors": ["Andrei Zabolotskii"], "title": "Additive systems for $\\mathbb{Z}$ are undecidable", "categories": ["math.CO", "cs.LO"], "comment": "10 pages", "summary": "What are the collections of sets ${A}_i\\subset\\mathbb{Z}$ such that any\n$n\\in\\mathbb{Z}$ has exactly one representation as $n=a_0+a_1+\\dotsb$ with\n$a_i\\in{A}_i$? The answer for $\\mathbb{N}_0$ instead of $\\mathbb{Z}$ is given\nby a theorem of de Bruijn. We describe a family of natural candidate\ncollections for $\\mathbb{Z}$, which we call canonical collections. Translating\nthe problem into the language of dynamical systems, we show that the question\nof whether the sumset of a canonical collection covers the entire $\\mathbb{Z}$\nis difficult: specifically, there is a collection for which this question is\nequivalent to the Collatz conjecture, and there is a well-behaved family of\ncollections for which this question is equivalent to the universal halting\nproblem for Fractran and is therefore undecidable.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u96c6\u5408 ${A}_i\\subset\\mathbb{Z}$ \u7684\u6784\u9020\uff0c\u4f7f\u5f97\u6bcf\u4e2a\u6574\u6570 $n$ \u53ef\u4ee5\u552f\u4e00\u8868\u793a\u4e3a\u6765\u81ea\u8fd9\u4e9b\u96c6\u5408\u7684\u5143\u7d20\u4e4b\u548c\uff0c\u5e76\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u52a8\u529b\u7cfb\u7edf\u8bed\u8a00\uff0c\u8bc1\u660e\u4e86\u5176\u590d\u6742\u6027\u3002", "motivation": "\u7814\u7a76\u96c6\u5408 ${A}_i$ \u7684\u6784\u9020\u4ee5\u4fbf\u6bcf\u4e2a\u6574\u6570\u53ef\u4ee5\u552f\u4e00\u8868\u793a\u4e3a\u8fd9\u4e9b\u96c6\u5408\u7684\u5143\u7d20\u4e4b\u548c\uff0c\u5e76\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u52a8\u529b\u7cfb\u7edf\u8bed\u8a00\u4ee5\u63a2\u7d22\u5176\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u79f0\u4e3a\u89c4\u8303\u96c6\u5408\u7684\u81ea\u7136\u5019\u9009\u96c6\u5408\u65cf\uff0c\u5e76\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u52a8\u529b\u7cfb\u7edf\u8bed\u8a00\uff0c\u901a\u8fc7\u8054\u7cfb\u5df2\u77e5\u96be\u9898\uff08\u5982 Collatz \u731c\u60f3\u548c Fractran \u505c\u673a\u95ee\u9898\uff09\u6765\u5206\u6790\u5176\u590d\u6742\u6027\u3002", "result": "\u8bc1\u660e\u67d0\u4e9b\u89c4\u8303\u96c6\u5408\u7684\u603b\u548c\u8986\u76d6\u95ee\u9898\u7b49\u4ef7\u4e8e Collatz \u731c\u60f3\u6216 Fractran \u7684\u901a\u7528\u505c\u673a\u95ee\u9898\uff0c\u56e0\u6b64\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002", "conclusion": "\u89c4\u8303\u96c6\u5408\u7684\u603b\u548c\u8986\u76d6\u95ee\u9898\u5177\u6709\u9ad8\u590d\u6742\u6027\uff0c\u4e0e\u8457\u540d\u672a\u89e3\u51b3\u95ee\u9898\u76f8\u5173\uff0c\u4e14\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4e0d\u53ef\u5224\u5b9a\u3002"}}
{"id": "2508.18217", "pdf": "https://arxiv.org/pdf/2508.18217", "abs": "https://arxiv.org/abs/2508.18217", "authors": ["Nina M. Ivanova", "Alexey S. Kashin", "Valentine P. Ananikov"], "title": "Lost Data in Electron Microscopy", "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.DL", "physics.chem-ph", "physics.data-an"], "comment": "20 pages, 4 figures, 2 tables", "summary": "The goal of this study is to estimate the amount of lost data in electron\nmicroscopy and to analyze the extent to which experimentally acquired images\nare utilized in peer-reviewed scientific publications. Analysis of the number\nof images taken on electron microscopes at a core user facility and the number\nof images subsequently included in peer-reviewed scientific journals revealed\nlow efficiency of data utilization. More than 90% of electron microscopy data\ngenerated during routine instrument operation remain unused. Of the more than\n150000 electron microscopy images evaluated in this study, only approximately\n3500 (just over 2%) were made available in publications. Thus, the amount of\nlost data in electron microscopy can be estimated as >90% (in terms of data\nbeing recorded but not being published in peer-reviewed literature). On the one\nhand, these results highlight a shortcoming in the optimal use of microscopy\nimages; on the other hand, they indicate the existence of a large pool of\nelectron microscopy data that can facilitate research in data science and the\ndevelopment of AI-based projects. The considerations important to unlock the\npotential of lost data are discussed in the present article.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7535\u5b50\u663e\u5fae\u955c\u4ea7\u751f\u7684\u5927\u90e8\u5206\u6570\u636e\u672a\u88ab\u5229\u7528\uff0c\u53d1\u8868\u7387\u4ec5\u7565\u9ad8\u4e8e2%\uff0c\u5b58\u5728\u5927\u91cf\u672a\u5f00\u53d1\u5229\u7528\u7684\u6570\u636e\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u7535\u5b50\u663e\u5fae\u955c\u6570\u636e\u7684\u4e22\u5931\u7a0b\u5ea6\uff0c\u5e76\u5206\u6790\u5b9e\u9a8c\u83b7\u53d6\u56fe\u50cf\u5728\u79d1\u7814\u53d1\u8868\u4e2d\u7684\u5229\u7528\u7387\u3002", "method": "\u5206\u6790\u6838\u5fc3\u7528\u6237\u8bbe\u65bd\u7684\u7535\u5b50\u663e\u5fae\u955c\u62cd\u6444\u56fe\u50cf\u6570\u91cf\u53ca\u540e\u7eed\u5728\u540c\u884c\u8bc4\u5ba1\u671f\u520a\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u8d85\u8fc790%\u7684\u7535\u955c\u6570\u636e\u672a\u88ab\u4f7f\u7528\uff0c\u4ec5\u7ea62%\u7684\u56fe\u50cf\u53d1\u8868\uff0c\u663e\u793a\u51fa\u6570\u636e\u5229\u7528\u7387\u6781\u4f4e\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u7535\u955c\u6570\u636e\u7684\u5927\u91cf\u6d6a\u8d39\uff0c\u540c\u65f6\u6307\u51fa\u8fd9\u4e9b\u672a\u7528\u6570\u636e\u53ef\u4e3a\u6570\u636e\u79d1\u5b66\u548cAI\u9879\u76ee\u63d0\u4f9b\u8d44\u6e90\u3002"}}
{"id": "2508.16610", "pdf": "https://arxiv.org/pdf/2508.16610", "abs": "https://arxiv.org/abs/2508.16610", "authors": ["AKM Bahalul Haque", "A. K. M. Najmul Islam", "Patrick Mikalef"], "title": "To Explain Or Not To Explain: An Empirical Investigation Of AI-Based Recommendations On Social Media Platforms", "categories": ["cs.HC", "cs.AI"], "comment": "25 pages, 2 figures, and 1 table", "summary": "AI based social media recommendations have great potential to improve the\nuser experience. However, often these recommendations do not match the user\ninterest and create an unpleasant experience for the users. Moreover, the\nrecommendation system being a black box creates comprehensibility and\ntransparency issues. This paper investigates social media recommendations from\nan end user perspective. For the investigation, we used the popular social\nmedia platform Facebook and recruited regular users to conduct a qualitative\nanalysis. We asked participants about the social media content suggestions,\ntheir comprehensibility, and explainability. Our analysis shows users mostly\nrequire explanation whenever they encounter unfamiliar content and to ensure\ntheir online data security. Furthermore, the users require concise,\nnon-technical explanations along with the facility of controlled information\nflow. In addition, we observed that explanations impact the users perception of\ntransparency, trust, and understandability. Finally, we have outlined some\ndesign implications and presented a synthesized framework based on our data\nanalysis.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u793e\u4ea4\u5a92\u4f53\u63a8\u8350\u7cfb\u7edf\u4ece\u7528\u6237\u89d2\u5ea6\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u53d1\u73b0\u7528\u6237\u9700\u8981\u7b80\u6d01\u3001\u975e\u6280\u672f\u6027\u7684\u89e3\u91ca\u4ee5\u53ca\u5bf9\u4fe1\u606f\u6d41\u7684\u63a7\u5236\u3002", "motivation": "AI\u63a8\u8350\u7684\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5e38\u5e38\u4e0e\u7528\u6237\u5174\u8da3\u4e0d\u7b26\u4e14\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5bfc\u81f4\u7528\u6237\u4f53\u9a8c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u7528\u6237\u89c6\u89d2\u7814\u7a76\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528Facebook\u5e73\u53f0\u62db\u52df\u666e\u901a\u7528\u6237\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u63a2\u8ba8\u63a8\u8350\u5185\u5bb9\u7684\u53ef\u7406\u89e3\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u7528\u6237\u4e3b\u8981\u5728\u4e0d\u719f\u6089\u5185\u5bb9\u65f6\u8981\u6c42\u89e3\u91ca\uff0c\u5e76\u5173\u6ce8\u6570\u636e\u5b89\u5168\uff1b\u89e3\u91ca\u80fd\u63d0\u5347\u900f\u660e\u5ea6\u3001\u4fe1\u4efb\u548c\u7406\u89e3\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8bbe\u8ba1\u5efa\u8bae\u548c\u6846\u67b6\uff0c\u5f3a\u8c03\u7b80\u6d01\u89e3\u91ca\u548c\u7528\u6237\u53ef\u63a7\u7684\u4fe1\u606f\u6d41\u5bf9\u63d0\u5347\u4f53\u9a8c\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.18025", "pdf": "https://arxiv.org/pdf/2508.18025", "abs": "https://arxiv.org/abs/2508.18025", "authors": ["Aditri Paul", "Archan Paul"], "title": "AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.ET", "cs.SY", "eess.SY", "68T07(2020), 68T45(2020), 68T10(2020), 90C90(2020)", "I.2.10; I.2.6; I.2.9; J.2"], "comment": "17 pages, 6 figures. A research paper on a novel deep learning\n  framework for planetary crater detection", "summary": "Autonomous planetary exploration missions are critically dependent on\nreal-time, accurate environmental perception for navigation and hazard\navoidance. However, deploying deep learning models on the resource-constrained\ncomputational hardware of planetary exploration platforms remains a significant\nchallenge. This paper introduces the Adaptive Quantized Planetary Crater\nDetection System (AQ-PCDSys), a novel framework specifically engineered for\nreal-time, onboard deployment in the computationally constrained environments\nof space exploration missions. AQ-PCDSys synergistically integrates a Quantized\nNeural Network (QNN) architecture, trained using Quantization-Aware Training\n(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture\nsignificantly optimizes model size and inference latency suitable for real-time\nonboard deployment in space exploration missions, while preserving high\naccuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and\nDigital Elevation Models (DEMs) at the feature level, utilizing an Adaptive\nWeighting Mechanism (AWM) to dynamically prioritize the most relevant and\nreliable sensor modality based on planetary ambient conditions. This approach\nenhances detection robustness across diverse planetary landscapes. Paired with\nMulti-Scale Detection Heads specifically designed for robust and efficient\ndetection of craters across a wide range of sizes, AQ-PCDSys provides a\ncomputationally efficient, reliable and accurate solution for planetary crater\ndetection, a critical capability for enabling the next generation of autonomous\nplanetary landing, navigation, and scientific exploration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u91cf\u5316\u884c\u661f\u9668\u77f3\u5751\u68c0\u6d4b\u7cfb\u7edf\uff08AQ-PCDSys\uff09\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u884c\u661f\u63a2\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u7684\u73af\u5883\u611f\u77e5\u3002", "motivation": "\u884c\u661f\u63a2\u6d4b\u4efb\u52a1\u7684\u5b9e\u65f6\u73af\u5883\u611f\u77e5\u4f9d\u8d56\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8ba1\u7b97\u786c\u4ef6\u4e0a\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u91cf\u5316\u795e\u7ecf\u7f51\u7edc\uff08QNN\uff09\u548c\u81ea\u9002\u5e94\u591a\u4f20\u611f\u5668\u878d\u5408\uff08AMF\uff09\u6a21\u5757\uff0c\u4f18\u5316\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u52a8\u6001\u878d\u5408\u5149\u5b66\u56fe\u50cf\u548c\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u6570\u636e\u3002", "result": "AQ-PCDSys\u5728\u884c\u661f\u9668\u77f3\u5751\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u9760\u548c\u9ad8\u7cbe\u5ea6\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u884c\u661f\u7740\u9646\u3001\u5bfc\u822a\u548c\u79d1\u5b66\u63a2\u7d22\u63d0\u4f9b\u4e86\u5173\u952e\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2508.17624", "pdf": "https://arxiv.org/pdf/2508.17624", "abs": "https://arxiv.org/abs/2508.17624", "authors": ["Ge Shi", "Hanieh Sadri", "Qian Wang", "Yu Zhang", "Ying Xiong", "Yong Zhang", "Zhenan Fan"], "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters at Scale", "categories": ["cs.DC"], "comment": null, "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.", "AI": {"tldr": "ESFT\u901a\u8fc7\u9009\u62e9\u6027\u8c03\u4f18MoE\u6a21\u578b\u4e2d\u7684\u4e13\u5bb6\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u90e8\u7f72\u9762\u4e34\u8d44\u6e90\u6311\u6218\u3002ExpertWeave\u7cfb\u7edf\u901a\u8fc7\u5171\u4eab\u57fa\u7840\u6a21\u578b\u548c\u4f18\u5316\u5185\u5b58\u7ba1\u7406\uff0c\u9ad8\u6548\u652f\u6301\u591a\u9002\u914d\u5668\u5e76\u53d1\u670d\u52a1\uff0c\u663e\u8457\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3MoE\u6a21\u578b\u5728\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u540e\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u95ee\u9898\uff0c\u5c24\u5176\u662f\u8d44\u6e90\u6d88\u8017\u548c\u9002\u914d\u5668\u517c\u5bb9\u6027\u6311\u6218\u3002", "method": "\u63d0\u51faExpertWeave\u7cfb\u7edf\uff0c\u5305\u62ec\u865a\u62df\u5185\u5b58\u8f85\u52a9\u7684\u4e13\u5bb6\u6743\u91cd\u7ba1\u7406\u548c\u6279\u91cf\u8def\u7531\u4f18\u5316\u5185\u6838\uff0c\u652f\u6301\u591a\u9002\u914d\u5668\u5171\u4eabMoE\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728\u5355\u52a0\u901f\u5668\u4e0a\u9ad8\u6548\u652f\u630116B MoE\u6a21\u578b\u7684\u591a\u9002\u914d\u5668\u5e76\u53d1\u670d\u52a1\uff0c\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\uff0894x KV\u7f13\u5b58\u5bb9\u91cf\uff0c18%\u541e\u5410\u91cf\uff09\uff0c\u5ef6\u8fdf\u589e\u52a0\u4ec54-11%\u3002", "conclusion": "ExpertWeave\u4e3aMoE\u6a21\u578b\u7684\u591a\u4efb\u52a1\u9002\u914d\u5668\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u975e\u4fb5\u5165\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u9700\u6c42\u5e76\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002"}}
{"id": "2508.17719", "pdf": "https://arxiv.org/pdf/2508.17719", "abs": "https://arxiv.org/abs/2508.17719", "authors": ["Akhila Sri Manasa Venigalla", "Sridhar Chimalakonda"], "title": "DocFetch - Towards Generating Software Documentation from Multiple Software Artifacts", "categories": ["cs.SE"], "comment": "12 pages, 7 Figures, 4 Tables", "summary": "Software Documentation plays a major role in the usage and development of a\nproject. Widespread adoption of open source software projects contributes to\nlarger and faster development of the projects, making it difficult to maintain\nthe associated documentation. Existing automated approaches to generate\ndocumentation largely focus on source code. However, information useful for\ndocumentation is observed to be scattered across various artifacts that\nco-evolve with the source code. Leveraging this information across multiple\nartifacts can reduce the effort involved in maintaining documentation. Hence,\nwe propose DocFetch, to generate different types of documentation from multiple\nsoftware artifacts. We employ a multi-layer prompt based LLM and generate\nstructured documentation corresponding to different documentation types for the\ndata consolidated in DocMine dataset. We evaluate the performance of DocFetch\nusing a manually curated groundtruth dataset by analysing the artifacts in\nDocMine. The evaluation yields a highest BLEU-4 score of 43.24% and ROUGE-L\nscore of 0.39 for generation of api-related and file-related information from\nfive documentation sources. The generation of other documentation type related\ninformation also reported BLEU-4 scores close to 30% indicating good\nperformance of the approach. Thus,DocFetch can be employed to\nsemi-automatically generate documentation, and helps in comprehending the\nprojects with minimal effort in maintaining the documentation.", "AI": {"tldr": "DocFetch\u662f\u4e00\u79cd\u4ece\u591a\u4e2a\u8f6f\u4ef6\u5236\u54c1\u751f\u6210\u6587\u6863\u7684\u5de5\u5177\uff0c\u5229\u7528\u591a\u5c42\u7ea7\u63d0\u793a\u7684LLM\uff0c\u5728DocMine\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0cBLEU-4\u5f97\u5206\u6700\u9ad8\u8fbe43.24%\u3002", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u6587\u6863\u7ef4\u62a4\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6e90\u4ee3\u7801\uff0c\u4f46\u6709\u7528\u4fe1\u606f\u5206\u6563\u5728\u5176\u4ed6\u5236\u54c1\u4e2d\u3002DocFetch\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u51cf\u5c11\u6587\u6863\u7ef4\u62a4\u5de5\u4f5c\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u591a\u5c42\u7ea7\u63d0\u793a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\uff0c\u4ece\u591a\u4e2a\u8f6f\u4ef6\u5236\u54c1\u4e2d\u751f\u6210\u7ed3\u6784\u5316\u6587\u6863\uff0c\u5e76\u5728DocMine\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728API\u548c\u6587\u4ef6\u76f8\u5173\u4fe1\u606f\u751f\u6210\u4e0a\uff0cDocFetch\u7684BLEU-4\u5f97\u5206\u8fbe43.24%\uff0cROUGE-L\u5f97\u5206\u4e3a0.39\uff1b\u5176\u4ed6\u7c7b\u578b\u6587\u6863\u7684\u751f\u6210\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "DocFetch\u53ef\u534a\u81ea\u52a8\u751f\u6210\u6587\u6863\uff0c\u663e\u8457\u964d\u4f4e\u9879\u76ee\u7406\u89e3\u4e0e\u6587\u6863\u7ef4\u62a4\u7684\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2508.17451", "pdf": "https://arxiv.org/pdf/2508.17451", "abs": "https://arxiv.org/abs/2508.17451", "authors": ["Davide Ancona", "Angelo Ferrando"], "title": "On The Space Complexity of Partial Derivatives of Regular Expressions with Shuffle", "categories": ["cs.FL", "cs.LO"], "comment": null, "summary": "Partial derivatives of regular expressions, introduced by Antimirov, define\nan elegant algorithm for generating equivalent non-deterministic finite\nautomata (NFA) with a limited number of states.\n  Here we focus on runtime verification (RV) of simple properties expressible\nwith regular expressions. In this case, words are finite traces of monitorable\nevents forming the language's alphabet, and the generated NFA may have an\nintractable number of states.\n  This typically occurs when sub-traces of mutually independent events are\nallowed to interleave.\n  To address this issue, regular expressions used for RV are extended with the\nshuffle operator to make specifications more compact and easier to read.\n  Exploiting partial derivatives enables a rewriting-based approach to RV,\nwhere only one derivative is stored at each step, avoiding the construction of\nan intractably large automaton.\n  This raises the question of the space complexity of the largest generated\npartial derivative. While the total number of generated partial derivatives is\nknown to be linear in the size of the initial regular expression, no results\ncan be found in the literature regarding the size of the largest partial\nderivative.\n  We study this problem w.r.t. two metrics (height and size of regular\nexpressions), and show that the former increases by at most one, while the\nlatter is quadratic in the size of the regular expression. Surprisingly, these\nresults also hold with shuffle.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u504f\u5bfc\u6570\u4f18\u5316\u8fd0\u884c\u65f6\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u751f\u6210\u96be\u4ee5\u5904\u7406\u7684\u5927\u578b\u81ea\u52a8\u673a\u3002", "motivation": "\u89e3\u51b3\u5728\u8fd0\u884c\u65f6\u9a8c\u8bc1\u4e2d\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u65f6\uff0c\u56e0\u5b50\u8ff9\u7684\u76f8\u4e92\u72ec\u7acb\u4e8b\u4ef6\u4ea4\u9519\u5bfc\u81f4\u751f\u6210\u7684NFA\u72b6\u6001\u6570\u4e0d\u53ef\u63a7\u7684\u95ee\u9898\u3002", "method": "\u6269\u5c55\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u5f15\u5165shuffle\u64cd\u4f5c\u7b26\uff0c\u5e76\u5229\u7528\u504f\u5bfc\u6570\u5b9e\u73b0\u9010\u6b65\u9a8c\u8bc1\uff0c\u907f\u514d\u751f\u6210\u5927\u578b\u81ea\u52a8\u673a\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u504f\u5bfc\u6570\u7684\u6700\u5927\u9ad8\u5ea6\u6700\u591a\u589e\u52a01\uff0c\u5927\u5c0f\u4e0e\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u5927\u5c0f\u5448\u4e8c\u6b21\u65b9\u5173\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u504f\u5bfc\u6570\u65b9\u6cd5\uff0c\u53ef\u4ee5\u9ad8\u6548\u5904\u7406\u542bshuffle\u64cd\u4f5c\u7b26\u7684\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u4f18\u5316\u8fd0\u884c\u65f6\u9a8c\u8bc1\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u3002"}}
{"id": "2508.17428", "pdf": "https://arxiv.org/pdf/2508.17428", "abs": "https://arxiv.org/abs/2508.17428", "authors": ["Henrique Domingues Garcia", "Marcelo Menezes de Carvalho"], "title": "py360tool: Um framework para manipula\u00e7\u00e3o de v\u00eddeo 360$^\\circ$ com ladrilhos", "categories": ["eess.IV", "cs.MM"], "comment": "in Portuguese language, Submetido ao WFA, Workshop de Ferramentas e\n  Aplica\\c{c}\\~oes de 2025, evento sat\\'elite do 31{\\deg} Simp\\'osio Brasileiro\n  de Sistemas Multim\\'idia e Web", "summary": "Streaming 360$^\\circ$ videos for virtual reality demands a lot of bandwidth.\nTo optimize this transmission, videos are divided into \"tiles\" and selectively\ndistributed to the user based on what they are looking at. This interactive\napproach makes it difficult to assess quality and user experience. To solve\nthis, the paper presents py360tools, a Python library that automates\nclient-side tasks like video reconstruction, tile selection, and viewport\nextraction. This facilitates the reproduction, simulation, and analysis of\n360$^\\circ$ video streaming sessions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2aPython\u5e93py360tools\uff0c\u7528\u4e8e\u81ea\u52a8\u5316360\u5ea6\u89c6\u9891\u6d41\u7684\u5ba2\u6237\u7aef\u4efb\u52a1\uff0c\u4ee5\u4f18\u5316\u4f20\u8f93\u548c\u5206\u6790\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "360\u5ea6\u89c6\u9891\u6d41\u9700\u8981\u9ad8\u5e26\u5bbd\uff0c\u901a\u8fc7\u5206\u5757\u4f20\u8f93\u4f18\u5316\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86py360tools\u5e93\uff0c\u5b9e\u73b0\u89c6\u9891\u91cd\u5efa\u3001\u5206\u5757\u9009\u62e9\u548c\u89c6\u53e3\u63d0\u53d6\u7684\u81ea\u52a8\u5316\u3002", "result": "\u5de5\u5177\u652f\u6301360\u5ea6\u89c6\u9891\u6d41\u4f1a\u8bdd\u7684\u590d\u73b0\u3001\u6a21\u62df\u548c\u5206\u6790\u3002", "conclusion": "py360tools\u4e3a360\u5ea6\u89c6\u9891\u6d41\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5ba2\u6237\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17467", "pdf": "https://arxiv.org/pdf/2508.17467", "abs": "https://arxiv.org/abs/2508.17467", "authors": ["Krishna Teja Chitty-Venkata", "Sylvia Howland", "Golara Azar", "Daria Soboleva", "Natalia Vassilieva", "Siddhisanket Raskar", "Murali Emani", "Venkatram Vishwanath"], "title": "MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models", "categories": ["cs.LG", "cs.PF"], "comment": "Preprint", "summary": "Mixture of Experts (MoE) models have enabled the scaling of Large Language\nModels (LLMs) and Vision Language Models (VLMs) by achieving massive parameter\ncounts while maintaining computational efficiency. However, MoEs introduce\nseveral inference-time challenges, including load imbalance across experts and\nthe additional routing computational overhead. To address these challenges and\nfully harness the benefits of MoE, a systematic evaluation of hardware\nacceleration techniques is essential. We present MoE-Inference-Bench, a\ncomprehensive study to evaluate MoE performance across diverse scenarios. We\nanalyze the impact of batch size, sequence length, and critical MoE\nhyperparameters such as FFN dimensions and number of experts on throughput. We\nevaluate several optimization techniques on Nvidia H100 GPUs, including\npruning, Fused MoE operations, speculative decoding, quantization, and various\nparallelization strategies. Our evaluation includes MoEs from the Mixtral,\nDeepSeek, OLMoE and Qwen families. The results reveal performance differences\nacross configurations and provide insights for the efficient deployment of\nMoEs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faMoE-Inference-Bench\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08MoE\uff09\u5728\u4e0d\u540c\u786c\u4ef6\u52a0\u901f\u6280\u672f\u4e0b\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u914d\u7f6e\u5bf9\u6548\u7387\u7684\u5f71\u54cd\u3002", "motivation": "MoE\u6a21\u578b\u867d\u80fd\u6269\u5c55\u5927\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u4f46\u5728\u63a8\u7406\u65f6\u5b58\u5728\u8d1f\u8f7d\u4e0d\u5747\u8861\u548c\u8def\u7531\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u9700\u901a\u8fc7\u786c\u4ef6\u52a0\u901f\u6280\u672f\u4f18\u5316\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u6279\u6b21\u5927\u5c0f\u3001\u5e8f\u5217\u957f\u5ea6\u548cMoE\u8d85\u53c2\u6570\uff08\u5982FFN\u7ef4\u5ea6\u548c\u4e13\u5bb6\u6570\uff09\u5bf9\u541e\u5410\u91cf\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u4e86\u5305\u62ec\u526a\u679d\u3001\u878d\u5408\u64cd\u4f5c\u3001\u63a8\u6d4b\u89e3\u7801\u3001\u91cf\u5316\u548c\u5e76\u884c\u5316\u7b56\u7565\u5728\u5185\u7684\u4f18\u5316\u6280\u672f\u3002", "result": "\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684MoE\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "MoE-Inference-Bench\u4e3aMoE\u6a21\u578b\u7684\u4f18\u5316\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2508.16969", "pdf": "https://arxiv.org/pdf/2508.16969", "abs": "https://arxiv.org/abs/2508.16969", "authors": ["Yunxiao Zhao", "Hao Xu", "Zhiqiang Wang", "Xiaoli Li", "Jiye Liang", "Ru Li"], "title": "Explaining Black-box Language Models with Knowledge Probing Systems: A Post-hoc Explanation Perspective", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "16 pages, 8 figures. This paper has been accepted by DASFAA 2025: The\n  30th International Conference on Database Systems for Advanced Applications", "summary": "Pre-trained Language Models (PLMs) are trained on large amounts of unlabeled\ndata, yet they exhibit remarkable reasoning skills. However, the\ntrustworthiness challenges posed by these black-box models have become\nincreasingly evident in recent years. To alleviate this problem, this paper\nproposes a novel Knowledge-guided Probing approach called KnowProb in a\npost-hoc explanation way, which aims to probe whether black-box PLMs understand\nimplicit knowledge beyond the given text, rather than focusing only on the\nsurface level content of the text. We provide six potential explanations\nderived from the underlying content of the given text, including three\nknowledge-based understanding and three association-based reasoning. In\nexperiments, we validate that current small-scale (or large-scale) PLMs only\nlearn a single distribution of representation, and still face significant\nchallenges in capturing the hidden knowledge behind a given text. Furthermore,\nwe demonstrate that our proposed approach is effective for identifying the\nlimitations of existing black-box models from multiple probing perspectives,\nwhich facilitates researchers to promote the study of detecting black-box\nmodels in an explainable way.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKnowProb\u7684\u77e5\u8bc6\u5f15\u5bfc\u63a2\u6d4b\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u540e\u89e3\u91ca\u65b9\u5f0f\u63a2\u7a76\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u662f\u5426\u7406\u89e3\u9690\u542b\u77e5\u8bc6\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u6587\u672c\u8868\u9762\u5185\u5bb9\u3002", "motivation": "\u73b0\u6709\u9ed1\u76d2\u6a21\u578b\u7684\u4fe1\u4efb\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u63a2\u6d4b\u6a21\u578b\u5bf9\u9690\u542b\u77e5\u8bc6\u7684\u7406\u89e3\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u5f15\u5bfc\u7684\u540e\u89e3\u91ca\u65b9\u6cd5KnowProb\uff0c\u8bbe\u8ba1\u4e86\u516d\u79cd\u6f5c\u5728\u89e3\u91ca\uff08\u4e09\u79cd\u57fa\u4e8e\u77e5\u8bc6\u7684\u7406\u89e3\u548c\u4e09\u79cd\u57fa\u4e8e\u5173\u8054\u7684\u63a8\u7406\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524dPLMs\u4ec5\u5b66\u4e60\u5355\u4e00\u8868\u793a\u5206\u5e03\uff0c\u96be\u4ee5\u6355\u6349\u6587\u672c\u80cc\u540e\u7684\u9690\u542b\u77e5\u8bc6\uff1bKnowProb\u80fd\u6709\u6548\u8bc6\u522b\u9ed1\u76d2\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "KnowProb\u65b9\u6cd5\u6709\u52a9\u4e8e\u4ece\u591a\u89d2\u5ea6\u63a2\u6d4b\u9ed1\u76d2\u6a21\u578b\uff0c\u63a8\u52a8\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u3002"}}
{"id": "2508.16868", "pdf": "https://arxiv.org/pdf/2508.16868", "abs": "https://arxiv.org/abs/2508.16868", "authors": ["Joshua Mashburn", "Johann Knechtel", "Florian Klemme", "Hussam Amrouch", "Ozgur Sinanoglu", "Paul V. Gratz"], "title": "Targeted Wearout Attacks in Microprocessor Cores", "categories": ["cs.CR", "cs.AR"], "comment": "13 pages, 11 figures, submitted to IEEE International Symposium on\n  High-Performance Computer Architecture 2026 (HPCA-32)", "summary": "Negative-Bias Temperature Instability is a dominant aging mechanism in\nnanoscale CMOS circuits such as microprocessors. With this aging mechanism, the\nrate of device aging is dependent not only on overall operating conditions,\nsuch as heat, but also on user controllable inputs to the transistors. This\ndependence on input implies a possible timing fault-injection attack wherein a\ntargeted path of logic is intentionally degraded through the purposeful,\nsoftware-driven actions of an attacker, rendering a targeted bit effectively\nstuck.\n  In this work, we describe such an attack mechanism, which we dub a\n\"$\\textbf{Targeted Wearout Attack}$\", wherein an attacker with sufficient\nknowledge of the processor core, executing a carefully crafted software program\nwith only user privilege, is able to degrade a functional unit within the\nprocessor with the aim of eliciting a particular desired incorrect calculation\nin a victim application. Here we give a general methodology for the attack. We\nthen demonstrate a case study where a targeted path within the fused\nmultiply-add pipeline in a RISC-V CPU sees a $>7x$ increase in wear over time\nthan would be experienced under typical workloads. We show that an attacker\ncould leverage such an attack, leading to targeted and silent data corruption\nin a co-running victim application using the same unit.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u8f6f\u4ef6\u9a71\u52a8\u7684\u8d1f\u504f\u538b\u6e29\u5ea6\u4e0d\u7a33\u5b9a\u6027\uff08NBTI\uff09\u653b\u51fb\uff0c\u79f0\u4e3a\u201c\u76ee\u6807\u78e8\u635f\u653b\u51fb\u201d\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7a0b\u5e8f\u52a0\u901f\u5904\u7406\u5668\u529f\u80fd\u5355\u5143\u8001\u5316\uff0c\u5bfc\u81f4\u53d7\u5bb3\u8005\u5e94\u7528\u6570\u636e\u635f\u574f\u3002", "motivation": "\u63a2\u7d22NBTI\u4f5c\u4e3a\u7eb3\u7c73\u7ea7CMOS\u7535\u8def\u8001\u5316\u673a\u5236\u7684\u6f5c\u5728\u5b89\u5168\u5a01\u80c1\uff0c\u7279\u522b\u662f\u901a\u8fc7\u7528\u6237\u8f93\u5165\u63a7\u5236\u6676\u4f53\u7ba1\u8001\u5316\u7684\u653b\u51fb\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u76ee\u6807\u78e8\u635f\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u7279\u6743\u7a0b\u5e8f\u52a0\u901f\u7279\u5b9a\u903b\u8f91\u8def\u5f84\u8001\u5316\uff0c\u5e76\u5728RISC-V CPU\u7684\u4e58\u6cd5\u52a0\u6cd5\u7ba1\u9053\u4e2d\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u653b\u51fb\u53ef\u4f7f\u76ee\u6807\u8def\u5f84\u7684\u8001\u5316\u901f\u5ea6\u63d0\u9ad87\u500d\u4ee5\u4e0a\uff0c\u5bfc\u81f4\u53d7\u5bb3\u8005\u5e94\u7528\u6570\u636e\u635f\u574f\u3002", "conclusion": "\u76ee\u6807\u78e8\u635f\u653b\u51fb\u662f\u4e00\u79cd\u65b0\u578b\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u5728\u786c\u4ef6\u8bbe\u8ba1\u548c\u5b89\u5168\u7b56\u7565\u4e2d\u4e88\u4ee5\u9632\u8303\u3002"}}
{"id": "2508.16612", "pdf": "https://arxiv.org/pdf/2508.16612", "abs": "https://arxiv.org/abs/2508.16612", "authors": ["Aven-Le Zhou"], "title": "Negative Shanshui: Real-time Interactive Ink Painting Synthesis", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "This paper presents Negative Shanshui, a real-time interactive AI synthesis\napproach that reinterprets classical Chinese landscape ink painting, i.e.,\nshanshui, to engage with ecological crises in the Anthropocene. Negative\nShanshui optimizes a fine-tuned Stable Diffusion model for real-time inferences\nand integrates it with gaze-driven inpainting, frame interpolation; it enables\ndynamic morphing animations in response to the viewer's gaze and presents as an\ninteractive virtual reality (VR) experience. The paper describes the complete\ntechnical pipeline, covering the system framework, optimization strategies,\ngaze-based interaction, and multimodal deployment in an art festival. Further\nanalysis of audience feedback collected during its public exhibition highlights\nhow participants variously engaged with the work through empathy, ambivalence,\nand critical reflection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u4ea4\u4e92AI\u5408\u6210\u65b9\u6cd5Negative Shanshui\uff0c\u901a\u8fc7\u91cd\u65b0\u8be0\u91ca\u53e4\u5178\u4e2d\u56fd\u5c71\u6c34\u753b\uff0c\u4ee5\u5e94\u5bf9\u4eba\u7c7b\u4e16\u7684\u751f\u6001\u5371\u673a\u3002\u8be5\u65b9\u6cd5\u4f18\u5316\u4e86Stable Diffusion\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u89c6\u7ebf\u9a71\u52a8\u7684\u4fee\u590d\u548c\u5e27\u63d2\u503c\uff0c\u63d0\u4f9b\u52a8\u6001\u53d8\u6362\u52a8\u753b\u548cVR\u4f53\u9a8c\u3002", "motivation": "\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u91cd\u65b0\u8be0\u91ca\u4f20\u7edf\u5c71\u6c34\u753b\uff0c\u4ee5\u827a\u672f\u5f62\u5f0f\u5524\u8d77\u4eba\u4eec\u5bf9\u751f\u6001\u5371\u673a\u7684\u5173\u6ce8\uff0c\u5e76\u63a2\u7d22AI\u4e0e\u827a\u672f\u7ed3\u5408\u7684\u521b\u65b0\u65b9\u5f0f\u3002", "method": "\u4f18\u5316\u4e86Stable Diffusion\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u63a8\u7406\uff0c\u7ed3\u5408\u89c6\u7ebf\u9a71\u52a8\u7684\u4fee\u590d\u548c\u5e27\u63d2\u503c\u6280\u672f\uff0c\u5b9e\u73b0\u52a8\u6001\u52a8\u753b\u6548\u679c\uff0c\u5e76\u901a\u8fc7VR\u73af\u5883\u63d0\u4f9b\u4ea4\u4e92\u4f53\u9a8c\u3002", "result": "\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u6280\u672f\u6d41\u7a0b\uff0c\u5305\u62ec\u7cfb\u7edf\u6846\u67b6\u4f18\u5316\u548c\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u5e76\u5728\u827a\u672f\u8282\u4e2d\u8fdb\u884c\u4e86\u591a\u6a21\u6001\u90e8\u7f72\u3002\u89c2\u4f17\u53cd\u9988\u663e\u793a\u53c2\u4e0e\u8005\u901a\u8fc7\u5171\u60c5\u3001\u77db\u76fe\u60c5\u611f\u548c\u6279\u5224\u6027\u53cd\u601d\u4e0e\u4f5c\u54c1\u4e92\u52a8\u3002", "conclusion": "Negative Shanshui\u4e3a\u751f\u6001\u5371\u673a\u63d0\u4f9b\u4e86\u827a\u672f\u4e0e\u6280\u672f\u7ed3\u5408\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86AI\u5728\u827a\u672f\u521b\u4f5c\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u4e92\u52a8\u4f53\u9a8c\u5f15\u53d1\u89c2\u4f17\u7684\u6df1\u5c42\u601d\u8003\u3002"}}
{"id": "2508.18155", "pdf": "https://arxiv.org/pdf/2508.18155", "abs": "https://arxiv.org/abs/2508.18155", "authors": ["Muhammad Ali Nadeem", "Bishwo Prakash Pokharel", "Naresh Kshetri", "Achyut Shankar", "Gokarna Sharma"], "title": "$AutoGuardX$: A Comprehensive Cybersecurity Framework for Connected Vehicles", "categories": ["cs.CR", "cs.ET"], "comment": "16 pages, 3 figures, 8 tables", "summary": "The rapid integration of Internet of Things (IoT) and interconnected systems\nin modern vehicles not only introduced a new era of convenience, automation,\nand connected vehicles but also elevated their exposure to sophisticated cyber\nthreats. This is especially evident in US and Canada, where cyber-enabled auto\ntheft has surged in recent years, revealing the limitations of existing\nsecurity measures for connected vehicles. In response, this paper proposes\n$AutoGuardX$, a comprehensive cybersecurity framework designed specifically for\nconnected vehicles. $AutoGuardX$ combines key elements from existing recognized\nstandards for vehicle security, such as ISO/SAE 21434 and ISO 26262, with\nadvanced technologies, including machine learning-based anomaly detection, IoT\nsecurity protocols, and encrypted communication channels. The framework\naddresses major attack vectors like relay attacks, controller area network\n(CAN) bus intrusions, and vulnerabilities introduced by emerging technologies\nsuch as 5G and quantum computing. $AutoGuardX$ is extensively evaluated through\nsecurity simulations across a mix of Sedans and SUVs from four major vehicle\nbrands manufactured between 2019 and 2023. The results demonstrate the\nframework's adaptability, scalability, and practical effectiveness against\nexisting and emerging threats.", "AI": {"tldr": "$AutoGuardX$ \u662f\u4e00\u4e2a\u9488\u5bf9\u8054\u7f51\u8f66\u8f86\u7684\u5168\u9762\u7f51\u7edc\u5b89\u5168\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u73b0\u6709\u6807\u51c6\u4e0e\u5148\u8fdb\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u5f53\u524d\u548c\u672a\u6765\u5a01\u80c1\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u548c\u4e92\u8054\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8054\u7f51\u8f66\u8f86\u9762\u4e34\u7684\u7f51\u7edc\u5a01\u80c1\u65e5\u76ca\u589e\u591a\uff0c\u5c24\u5176\u662f\u5728\u7f8e\u56fd\u548c\u52a0\u62ff\u5927\uff0c\u7f51\u7edc\u7a83\u8f66\u4e8b\u4ef6\u9891\u53d1\uff0c\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "method": "$AutoGuardX$ \u7ed3\u5408\u4e86 ISO/SAE 21434 \u548c ISO 26262 \u7b49\u73b0\u6709\u6807\u51c6\uff0c\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u5f02\u5e38\u68c0\u6d4b\u3001\u7269\u8054\u7f51\u5b89\u5168\u534f\u8bae\u548c\u52a0\u5bc6\u901a\u4fe1\u7b49\u5148\u8fdb\u6280\u672f\uff0c\u9488\u5bf9\u4e3b\u8981\u653b\u51fb\u5411\u91cf\uff08\u5982\u4e2d\u7ee7\u653b\u51fb\u3001CAN \u603b\u7ebf\u5165\u4fb5\u7b49\uff09\u63d0\u4f9b\u9632\u62a4\u3002", "result": "\u901a\u8fc7\u5bf9 2019 \u81f3 2023 \u5e74\u95f4\u56db\u5bb6\u4e3b\u8981\u6c7d\u8f66\u54c1\u724c\u7684 Sedan \u548c SUV \u8fdb\u884c\u5b89\u5168\u6a21\u62df\u6d4b\u8bd5\uff0c$AutoGuardX$ \u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8054\u7f51\u8f66\u8f86\u7684\u5b89\u5168\u9632\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5e94\u5bf9\u5f53\u524d\u548c\u672a\u6765\u7684\u7f51\u7edc\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2508.17814", "pdf": "https://arxiv.org/pdf/2508.17814", "abs": "https://arxiv.org/abs/2508.17814", "authors": ["Anderson de Lima Luiz", "Shubham Vijay Kurlekar", "Munir Georges"], "title": "Scalable Engine and the Performance of Different LLM Models in a SLURM based HPC architecture", "categories": ["cs.DC", "cs.AI", "68M20, 68T50", "C.4; D.4.7; I.2.7"], "comment": "Accepted in ESSV 2025 - https://www.essv.de/paper.php?id=1265", "summary": "This work elaborates on a High performance computing (HPC) architecture based\non Simple Linux Utility for Resource Management (SLURM) [1] for deploying\nheterogeneous Large Language Models (LLMs) into a scalable inference engine.\nDynamic resource scheduling and seamless integration of containerized\nmicroservices have been leveraged herein to manage CPU, GPU, and memory\nallocations efficiently in multi-node clusters. Extensive experiments, using\nLlama 3.2 (1B and 3B parameters) [2] and Llama 3.1 (8B and 70B) [3], probe\nthroughput, latency, and concurrency and show that small models can handle up\nto 128 concurrent requests at sub-50 ms latency, while for larger models,\nsaturation happens with as few as two concurrent users, with a latency of more\nthan 2 seconds. This architecture includes Representational State Transfer\nApplication Programming Interfaces (REST APIs) [4] endpoints for single and\nbulk inferences, as well as advanced workflows such as multi-step \"tribunal\"\nrefinement. Experimental results confirm minimal overhead from container and\nscheduling activities and show that the approach scales reliably both for batch\nand interactive settings. We further illustrate real-world scenarios, including\nthe deployment of chatbots with retrievalaugmented generation, which helps to\ndemonstrate the flexibility and robustness of the architecture. The obtained\nresults pave ways for significantly more efficient, responsive, and\nfault-tolerant LLM inference on large-scale HPC infrastructures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSLURM\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\uff0c\u7528\u4e8e\u90e8\u7f72\u5f02\u6784\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u591a\u8282\u70b9\u96c6\u7fa4\u4e2d\u9ad8\u6548\u7ba1\u7406CPU\u3001GPU\u548c\u5185\u5b58\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u5e76\u652f\u6301LLM\u7684\u9ad8\u5e76\u53d1\u63a8\u7406\u9700\u6c42\u3002", "method": "\u91c7\u7528SLURM\u8fdb\u884c\u52a8\u6001\u8d44\u6e90\u8c03\u5ea6\uff0c\u5e76\u5229\u7528\u5bb9\u5668\u5316\u5fae\u670d\u52a1\u65e0\u7f1d\u96c6\u6210\uff0c\u652f\u6301\u5355\u6b21\u548c\u6279\u91cf\u63a8\u7406\uff0c\u4ee5\u53ca\u9ad8\u7ea7\u5de5\u4f5c\u6d41\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c0f\u6a21\u578b\u53ef\u5904\u7406128\u4e2a\u5e76\u53d1\u8bf7\u6c42\uff08\u5ef6\u8fdf\u4f4e\u4e8e50\u6beb\u79d2\uff09\uff0c\u5927\u6a21\u578b\u5219\u57282\u4e2a\u5e76\u53d1\u7528\u6237\u65f6\u5ef6\u8fdf\u8d85\u8fc72\u79d2\u3002", "conclusion": "\u8be5\u67b6\u6784\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u54cd\u5e94\u8fc5\u901f\u4e14\u5bb9\u9519\u7684LLM\u63a8\u7406\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.17720", "pdf": "https://arxiv.org/pdf/2508.17720", "abs": "https://arxiv.org/abs/2508.17720", "authors": ["Ziqi Guan", "Xin Yin", "Zhiyuan Peng", "Chao Ni"], "title": "RepoTransAgent: Multi-Agent LLM Framework for Repository-Aware Code Translation", "categories": ["cs.SE"], "comment": null, "summary": "Repository-aware code translation is critical for modernizing legacy systems,\nenhancing maintainability, and enabling interoperability across diverse\nprogramming languages. While recent advances in large language models (LLMs)\nhave improved code translation quality, existing approaches face significant\nchallenges in practical scenarios: insufficient contextual understanding,\ninflexible prompt designs, and inadequate error correction mechanisms. These\nlimitations severely hinder accurate and efficient translation of complex,\nreal-world code repositories. To address these challenges, we propose\nRepoTransAgent, a novel multi-agent LLM framework for repository-aware code\ntranslation. RepoTransAgent systematically decomposes the translation process\ninto specialized subtasks-context retrieval, dynamic prompt construction, and\niterative code refinement-each handled by dedicated agents. Our approach\nleverages retrieval-augmented generation (RAG) for contextual information\ngathering, employs adaptive prompts tailored to varying repository scenarios,\nand introduces a reflection-based mechanism for systematic error correction. We\nevaluate RepoTransAgent on hundreds of Java-C# translation pairs from six\npopular open-source projects. Experimental results demonstrate that\nRepoTransAgent significantly outperforms state-of-the-art baselines in both\ncompile and pass rates. Specifically, RepoTransAgent achieves up to 55.34%\ncompile rate and 45.84% pass rate. Comprehensive analysis confirms the\nrobustness and generalizability of RepoTransAgent across different LLMs,\nestablishing its effectiveness for real-world repository-aware code\ntranslation.", "AI": {"tldr": "\u63d0\u51faRepoTransAgent\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4ee3\u7801\u4ed3\u5e93\u611f\u77e5\u7684\u4ee3\u7801\u7ffb\u8bd1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u8bd1\u901a\u8fc7\u7387\u548c\u8fd0\u884c\u901a\u8fc7\u7387\u3002", "motivation": "\u73b0\u4ee3\u9057\u7559\u7cfb\u7edf\u5347\u7ea7\u3001\u7ef4\u62a4\u6027\u589e\u5f3a\u548c\u591a\u8bed\u8a00\u4e92\u64cd\u4f5c\u9700\u9ad8\u6548\u4ee3\u7801\u7ffb\u8bd1\uff0c\u73b0\u6709LLM\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u63d0\u793a\u8bbe\u8ba1\u548c\u7ea0\u9519\u673a\u5236\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5c06\u7ffb\u8bd1\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e0a\u4e0b\u6587\u68c0\u7d22\u3001\u52a8\u6001\u63d0\u793a\u6784\u5efa\u548c\u8fed\u4ee3\u4ee3\u7801\u4f18\u5316\uff0c\u5229\u7528RAG\u3001\u81ea\u9002\u5e94\u63d0\u793a\u548c\u53cd\u601d\u673a\u5236\uff0c\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u5904\u7406\u3002", "result": "\u5728\u516d\u4e2a\u5f00\u6e90\u9879\u76ee\u7684Java-C#\u7ffb\u8bd1\u4efb\u52a1\u4e2d\uff0cRepoTransAgent\u7f16\u8bd1\u901a\u8fc7\u738755.34%\uff0c\u8fd0\u884c\u901a\u8fc7\u738745.84%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RepoTransAgent\u5728\u4e0d\u540cLLM\u4e2d\u8868\u73b0\u7a33\u5065\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4ee3\u7801\u4ed3\u5e93\u7ffb\u8bd1\u573a\u666f\u3002"}}
{"id": "2508.17786", "pdf": "https://arxiv.org/pdf/2508.17786", "abs": "https://arxiv.org/abs/2508.17786", "authors": ["Andrea Brunello", "Luca Geatti", "Angelo Montanari", "Nicola Saccomanno"], "title": "Interpretable Early Failure Detection via Machine Learning and Trace Checking-based Monitoring", "categories": ["cs.AI", "cs.FL", "cs.LG", "cs.LO"], "comment": "Full version of the paper accepted for publication at the 28th\n  European Conference on Artificial Intelligence (ECAI 2025)", "summary": "Monitoring is a runtime verification technique that allows one to check\nwhether an ongoing computation of a system (partial trace) satisfies a given\nformula. It does not need a complete model of the system, but it typically\nrequires the construction of a deterministic automaton doubly exponential in\nthe size of the formula (in the worst case), which limits its practicality. In\nthis paper, we show that, when considering finite, discrete traces, monitoring\nof pure past (co)safety fragments of Signal Temporal Logic (STL) can be reduced\nto trace checking, that is, evaluation of a formula over a trace, that can be\nperformed in time polynomial in the size of the formula and the length of the\ntrace. By exploiting such a result, we develop a GPU-accelerated framework for\ninterpretable early failure detection based on vectorized trace checking, that\nemploys genetic programming to learn temporal properties from historical trace\ndata. The framework shows a 2-10% net improvement in key performance metrics\ncompared to the state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7eaf\u8fc7\u53bb\uff08co\uff09\u5b89\u5168\u7247\u6bb5STL\u7684\u76d1\u63a7\u95ee\u9898\u7b80\u5316\u4e3a\u8f68\u8ff9\u68c0\u67e5\uff0c\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u65e9\u671f\u6545\u969c\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u76d1\u63a7\u6280\u672f\u9700\u8981\u6784\u5efa\u53cc\u91cd\u6307\u6570\u7ea7\u590d\u6742\u5ea6\u7684\u81ea\u52a8\u673a\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7b80\u5316\u4e3a\u8f68\u8ff9\u68c0\u67e5\u95ee\u9898\uff0c\u63d0\u9ad8\u76d1\u63a7\u6548\u7387\u3002", "method": "\u5c06\u76d1\u63a7\u95ee\u9898\u7b80\u5316\u4e3a\u8f68\u8ff9\u68c0\u67e5\uff0c\u5229\u7528GPU\u52a0\u901f\u548c\u9057\u4f20\u7f16\u7a0b\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u5b66\u4e60\u65f6\u95f4\u5c5e\u6027\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u6846\u67b6\u5728\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u5b9e\u73b0\u4e862-10%\u7684\u51c0\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u76d1\u63a7\u6548\u7387\uff0c\u4e3a\u65e9\u671f\u6545\u969c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17920", "pdf": "https://arxiv.org/pdf/2508.17920", "abs": "https://arxiv.org/abs/2508.17920", "authors": ["Haoshuo Zhang", "Yufei Bo", "Hongwei Zhang", "Meixia Tao"], "title": "Prompt-based Multimodal Semantic Communication for Multi-spectral Image Segmentation", "categories": ["eess.IV", "cs.MM"], "comment": null, "summary": "Multimodal semantic communication has gained widespread attention due to its\nability to enhance downstream task performance. A key challenge in such systems\nis the effective fusion of features from different modalities, which requires\nthe extraction of rich and diverse semantic representations from each modality.\nTo this end, we propose ProMSC-MIS, a Prompt-based Multimodal Semantic\nCommunication system for Multi-spectral Image Segmentation. Specifically, we\npropose a pre-training algorithm where features from one modality serve as\nprompts for another, guiding unimodal semantic encoders to learn diverse and\ncomplementary semantic representations. We further introduce a semantic fusion\nmodule that combines cross-attention mechanisms and squeeze-and-excitation (SE)\nnetworks to effectively fuse cross-modal features. Simulation results show that\nProMSC-MIS significantly outperforms benchmark methods across various\nchannel-source compression levels, while maintaining low computational\ncomplexity and storage overhead. Our scheme has great potential for\napplications such as autonomous driving and nighttime surveillance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u6a21\u6001\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edfProMSC-MIS\uff0c\u7528\u4e8e\u591a\u5149\u8c31\u56fe\u50cf\u5206\u5272\u3002\u901a\u8fc7\u9884\u8bad\u7ec3\u7b97\u6cd5\u548c\u8bed\u4e49\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u8bed\u4e49\u901a\u4fe1\u901a\u8fc7\u878d\u5408\u4e0d\u540c\u6a21\u6001\u7684\u7279\u5f81\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u5982\u4f55\u6709\u6548\u878d\u5408\u7279\u5f81\u548c\u63d0\u53d6\u591a\u6837\u5316\u8bed\u4e49\u8868\u793a\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86ProMSC-MIS\u7cfb\u7edf\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u7b97\u6cd5\uff08\u5229\u7528\u4e00\u79cd\u6a21\u6001\u7279\u5f81\u4f5c\u4e3a\u53e6\u4e00\u79cd\u6a21\u6001\u7684\u63d0\u793a\uff09\u548c\u8bed\u4e49\u878d\u5408\u6a21\u5757\uff08\u7ed3\u5408\u8de8\u6ce8\u610f\u529b\u673a\u5236\u548cSE\u7f51\u7edc\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cProMSC-MIS\u5728\u4e0d\u540c\u4fe1\u9053\u538b\u7f29\u6c34\u5e73\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5b58\u50a8\u5f00\u9500\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u591c\u95f4\u76d1\u63a7\u7b49\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u9614\u6f5c\u529b\u3002"}}
{"id": "2508.17340", "pdf": "https://arxiv.org/pdf/2508.17340", "abs": "https://arxiv.org/abs/2508.17340", "authors": ["Ryoma Kondo", "Riona Matsuoka", "Takahiro Yoshida", "Kazuyuki Yamasawa", "Ryohei Hisano"], "title": "Capturing Legal Reasoning Paths from Facts to Law in Court Judgments using Knowledge Graphs", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "Court judgments reveal how legal rules have been interpreted and applied to\nfacts, providing a foundation for understanding structured legal reasoning.\nHowever, existing automated approaches for capturing legal reasoning, including\nlarge language models, often fail to identify the relevant legal context, do\nnot accurately trace how facts relate to legal norms, and may misrepresent the\nlayered structure of judicial reasoning. These limitations hinder the ability\nto capture how courts apply the law to facts in practice. In this paper, we\naddress these challenges by constructing a legal knowledge graph from 648\nJapanese administrative court decisions. Our method extracts components of\nlegal reasoning using prompt-based large language models, normalizes references\nto legal provisions, and links facts, norms, and legal applications through an\nontology of legal inference. The resulting graph captures the full structure of\nlegal reasoning as it appears in real court decisions, making implicit\nreasoning explicit and machine-readable. We evaluate our system using expert\nannotated data, and find that it achieves more accurate retrieval of relevant\nlegal provisions from facts than large language model baselines and\nretrieval-augmented methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u65e5\u672c\u884c\u653f\u6cd5\u9662\u5224\u51b3\u4e2d\u6784\u5efa\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u6cd5\u5f8b\u63a8\u7406\u4e2d\u5b58\u5728\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\uff08\u5305\u62ec\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u6cd5\u5f8b\u63a8\u7406\u4e2d\u5e38\u65e0\u6cd5\u51c6\u786e\u8bc6\u522b\u76f8\u5173\u6cd5\u5f8b\u80cc\u666f\u3001\u4e8b\u5b9e\u4e0e\u6cd5\u5f8b\u89c4\u8303\u7684\u5173\u7cfb\uff0c\u4e14\u53ef\u80fd\u8bef\u89e3\u53f8\u6cd5\u63a8\u7406\u7684\u5206\u5c42\u7ed3\u6784\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u6cd5\u5f8b\u63a8\u7406\u7684\u6355\u6349\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u6cd5\u5f8b\u63a8\u7406\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u89c4\u8303\u5316\u6cd5\u5f8b\u6761\u6b3e\u5f15\u7528\uff0c\u5e76\u901a\u8fc7\u6cd5\u5f8b\u63a8\u65ad\u672c\u4f53\u5c06\u4e8b\u5b9e\u3001\u89c4\u8303\u548c\u6cd5\u5f8b\u5e94\u7528\u8054\u7cfb\u8d77\u6765\uff0c\u6784\u5efa\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "\u6784\u5efa\u7684\u77e5\u8bc6\u56fe\u8c31\u80fd\u591f\u5b8c\u6574\u6355\u6349\u5b9e\u9645\u5224\u51b3\u4e2d\u7684\u6cd5\u5f8b\u63a8\u7406\u7ed3\u6784\uff0c\u4f7f\u5176\u663e\u5f0f\u5316\u4e14\u673a\u5668\u53ef\u8bfb\u3002\u4e0e\u57fa\u7ebf\u548c\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7cfb\u7edf\u80fd\u66f4\u51c6\u786e\u5730\u4ece\u4e8b\u5b9e\u4e2d\u68c0\u7d22\u76f8\u5173\u6cd5\u5f8b\u6761\u6b3e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6cd5\u5f8b\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u6355\u6349\u65b9\u5f0f\u3002"}}
{"id": "2508.18193", "pdf": "https://arxiv.org/pdf/2508.18193", "abs": "https://arxiv.org/abs/2508.18193", "authors": ["Petr Kuznetsov", "Maxence Perion", "Sara Tucci-Piergiovanni"], "title": "Wait-free Replicated Data Types and Fair Reconciliation", "categories": ["cs.DC"], "comment": null, "summary": "Replication is a standard way to maintain availability of shared data in\nfault-prone distributed systems. To make sure that the data replicas are\nup-to-date, they need to synchronize, which typically means engaging the\nreplicas in waiting for coherent responses from each other. The amount of\nwaiting depends on the consistency and availability guarantees we impose on the\nsystem. The folklore CAP theory states that strong consistency (the set of\nreplicas create an illusion of one correct server) and strong availability (the\nreplicas' states are reachable despite network partitions) cannot be\nimplemented in the same system. A popular way to deal with this impossibility\nis to relax consistency to be only eventual: the replicas eventually converge\nto the same state. In return, the replicas can be wait-free, i.e., the clients\ncan get the data from the closest replica without waiting for other ones.\n  Wait-free data replication faces two important challenges. First, the\noperations issued by the clients may be constantly revoked, i.e., their effects\ncan be repeatedly recomputed due to asynchrony and concurrency. Second, even if\nsome operations eventually stabilize in their effects, a particular client may\nstill experience starvation if, from some point onward, each of its operations\nis later revoked. In this paper, we address these challenges through a general\nDAG-based framework for replicated data types, where replicas exchange their\nlocal views and merge them using a reconciliation function. Within this\nframework, we design reconciliation functions that implement a wait-free\neventually consistent replicated state machine ensuring both stable convergence\nand fair progress. Specifically, every replica maintains a growing sequence of\nclient operations, and we guarantee that: (1) all replicas share a common,\nmonotonically growing stable prefix of operations, and (2) no client starves.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDAG\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u7b49\u5f85\u6570\u636e\u590d\u5236\u4e2d\u7684\u64cd\u4f5c\u64a4\u9500\u548c\u5ba2\u6237\u7aef\u9965\u997f\u95ee\u9898\uff0c\u786e\u4fdd\u6700\u7ec8\u4e00\u81f4\u6027\u548c\u516c\u5e73\u8fdb\u5c55\u3002", "motivation": "\u65e0\u7b49\u5f85\u7684\u6570\u636e\u590d\u5236\u9762\u4e34\u64cd\u4f5c\u9891\u7e41\u64a4\u9500\u548c\u5ba2\u6237\u7aef\u53ef\u80fd\u9965\u997f\u7684\u6311\u6218\uff0c\u9700\u8981\u901a\u8fc7\u4e00\u79cd\u65b9\u6cd5\u5b9e\u73b0\u7a33\u5b9a\u6536\u655b\u548c\u516c\u5e73\u8fdb\u5c55\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eDAG\u7684\u6846\u67b6\uff0c\u526f\u672c\u901a\u8fc7\u4ea4\u6362\u672c\u5730\u89c6\u56fe\u5e76\u4f7f\u7528\u8c03\u548c\u51fd\u6570\u5408\u5e76\uff0c\u786e\u4fdd\u64cd\u4f5c\u5e8f\u5217\u7684\u5171\u540c\u7a33\u5b9a\u524d\u7f00\u548c\u65e0\u5ba2\u6237\u7aef\u9965\u997f\u3002", "result": "\u8bbe\u8ba1\u7684\u8c03\u548c\u51fd\u6570\u5b9e\u73b0\u4e86\u65e0\u7b49\u5f85\u7684\u6700\u7ec8\u4e00\u81f4\u6027\u526f\u672c\u72b6\u6001\u673a\uff0c\u4fdd\u8bc1\u4e86\u64cd\u4f5c\u7684\u7a33\u5b9a\u6536\u655b\u548c\u5ba2\u6237\u7aef\u7684\u516c\u5e73\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u7b49\u5f85\u6570\u636e\u590d\u5236\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5206\u5e03\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.17851", "pdf": "https://arxiv.org/pdf/2508.17851", "abs": "https://arxiv.org/abs/2508.17851", "authors": ["Patrick Loic Foalem", "Leuson Da Silva", "Foutse Khomh", "Heng Li", "Ettore Merlo"], "title": "Logging Requirement for Continuous Auditing of Responsible Machine Learning-based Applications", "categories": ["cs.SE"], "comment": null, "summary": "Machine learning (ML) is increasingly applied across industries to automate\ndecision-making, but concerns about ethical and legal compliance remain due to\nlimited transparency, fairness, and accountability. Monitoring through logging\na long-standing practice in traditional software offers a potential means for\nauditing ML applications, as logs provide traceable records of system behavior\nuseful for debugging, performance analysis, and continuous auditing.\nsystematically auditing models for compliance or accountability. The findings\nunderscore the need for enhanced logging practices and tooling that\nsystematically integrate responsible AI metrics. Such practices would support\nthe development of auditable, transparent, and ethically responsible ML\nsystems, aligning with growing regulatory requirements and societal\nexpectations. By highlighting specific deficiencies and opportunities, this\nwork provides actionable guidance for both practitioners and tool developers\nseeking to strengthen the accountability and trustworthiness of ML\napplications.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5e94\u7528\u4e2d\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u65e5\u5fd7\u8bb0\u5f55\u6765\u589e\u5f3a\u5ba1\u8ba1\u548c\u5408\u89c4\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u5b66\u4e60\u5728\u51b3\u7b56\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u900f\u660e\u6027\u3001\u516c\u5e73\u6027\u548c\u95ee\u8d23\u6027\u4e0d\u8db3\uff0c\u5f15\u53d1\u4e86\u4f26\u7406\u548c\u6cd5\u5f8b\u5408\u89c4\u7684\u62c5\u5fe7\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u901a\u8fc7\u65e5\u5fd7\u8bb0\u5f55\uff08\u4f20\u7edf\u8f6f\u4ef6\u4e2d\u7684\u5e38\u89c1\u505a\u6cd5\uff09\u6765\u7cfb\u7edf\u5ba1\u8ba1ML\u5e94\u7528\uff0c\u4ee5\u8ffd\u8e2a\u7cfb\u7edf\u884c\u4e3a\u5e76\u652f\u6301\u8c03\u8bd5\u3001\u6027\u80fd\u5206\u6790\u548c\u5408\u89c4\u5ba1\u8ba1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u589e\u5f3a\u65e5\u5fd7\u8bb0\u5f55\u5b9e\u8df5\u548c\u5de5\u5177\uff0c\u6574\u5408\u8d1f\u8d23\u4efbAI\u6307\u6807\uff0c\u53ef\u4ee5\u63d0\u5347ML\u7cfb\u7edf\u7684\u53ef\u5ba1\u8ba1\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u672c\u6587\u4e3a\u5b9e\u8df5\u8005\u548c\u5de5\u5177\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u5b9e\u9645\u5efa\u8bae\uff0c\u4ee5\u589e\u5f3aML\u5e94\u7528\u7684\u95ee\u8d23\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u6ee1\u8db3\u76d1\u7ba1\u548c\u793e\u4f1a\u671f\u671b\u3002"}}
{"id": "2508.17965", "pdf": "https://arxiv.org/pdf/2508.17965", "abs": "https://arxiv.org/abs/2508.17965", "authors": ["Xiangfei Sheng", "Zhichao Duan", "Xiaofeng Pan", "Yipo Huang", "Zhichao Yang", "Pengfei Chen", "Leida Li"], "title": "TuningIQA: Fine-Grained Blind Image Quality Assessment for Livestreaming Camera Tuning", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": "9 pages,8 figures", "summary": "Livestreaming has become increasingly prevalent in modern visual\ncommunication, where automatic camera quality tuning is essential for\ndelivering superior user Quality of Experience (QoE). Such tuning requires\naccurate blind image quality assessment (BIQA) to guide parameter optimization\ndecisions. Unfortunately, the existing BIQA models typically only predict an\noverall coarse-grained quality score, which cannot provide fine-grained\nperceptual guidance for precise camera parameter tuning. To bridge this gap, we\nfirst establish FGLive-10K, a comprehensive fine-grained BIQA database\ncontaining 10,185 high-resolution images captured under varying camera\nparameter configurations across diverse livestreaming scenarios. The dataset\nfeatures 50,925 multi-attribute quality annotations and 19,234 fine-grained\npairwise preference annotations. Based on FGLive-10K, we further develop\nTuningIQA, a fine-grained BIQA metric for livestreaming camera tuning, which\nintegrates human-aware feature extraction and graph-based camera parameter\nfusion. Extensive experiments and comparisons demonstrate that TuningIQA\nsignificantly outperforms state-of-the-art BIQA methods in both score\nregression and fine-grained quality ranking, achieving superior performance\nwhen deployed for livestreaming camera tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u76f4\u64ad\u573a\u666f\u7684\u7ec6\u7c92\u5ea6\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08BIQA\uff09\u65b9\u6cd5TuningIQA\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u591a\u6837\u6807\u6ce8\u7684FGLive-10K\u6570\u636e\u96c6\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u8d28\u91cf\u8bc4\u5206\u548c\u7cbe\u7ec6\u6392\u5e8f\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684BIQA\u6a21\u578b\u4ec5\u63d0\u4f9b\u7c97\u7565\u8d28\u91cf\u8bc4\u5206\uff0c\u65e0\u6cd5\u4e3a\u76f4\u64ad\u76f8\u673a\u53c2\u6570\u8c03\u4f18\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6307\u5bfc\u3002", "method": "\u6784\u5efaFGLive-10K\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1TuningIQA\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u611f\u77e5\u7279\u5f81\u63d0\u53d6\u548c\u57fa\u4e8e\u56fe\u7684\u53c2\u6570\u878d\u5408\u3002", "result": "TuningIQA\u5728\u5206\u6570\u56de\u5f52\u548c\u7ec6\u7c92\u5ea6\u6392\u5e8f\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u76f4\u64ad\u76f8\u673a\u8c03\u4f18\u3002", "conclusion": "TuningIQA\u4e3a\u76f4\u64ad\u76f8\u673a\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ec6\u7c92\u5ea6\u8d28\u91cf\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2508.17388", "pdf": "https://arxiv.org/pdf/2508.17388", "abs": "https://arxiv.org/abs/2508.17388", "authors": ["Xiaoyang Lin", "Runhao Jiang", "Renchi Yang"], "title": "Effective Clustering for Large Multi-Relational Graphs", "categories": ["cs.LG", "cs.DB", "cs.SI"], "comment": "23 pages. The technical report for the paper titled \"Effective\n  Clustering for Large Multi-Relational Graphs\" in SIGMOD 2026", "summary": "Multi-relational graphs (MRGs) are an expressive data structure for modeling\ndiverse interactions/relations among real objects (i.e., nodes), which pervade\nextensive applications and scenarios. Given an MRG G with N nodes, partitioning\nthe node set therein into K disjoint clusters (MRGC) is a fundamental task in\nanalyzing MRGs, which has garnered considerable attention. However, the\nmajority of existing solutions towards MRGC either yield severely compromised\nresult quality by ineffective fusion of heterogeneous graph structures and\nattributes, or struggle to cope with sizable MRGs with millions of nodes and\nbillions of edges due to the adoption of sophisticated and costly deep learning\nmodels.\n  In this paper, we present DEMM and DEMM+, two effective MRGC approaches to\naddress the limitations above. Specifically, our algorithms are built on novel\ntwo-stage optimization objectives, where the former seeks to derive\nhigh-caliber node feature vectors by optimizing the multi-relational Dirichlet\nenergy specialized for MRGs, while the latter minimizes the Dirichlet energy of\nclustering results over the node affinity graph. In particular, DEMM+ achieves\nsignificantly higher scalability and efficiency over our based method DEMM\nthrough a suite of well-thought-out optimizations. Key technical contributions\ninclude (i) a highly efficient approximation solver for constructing node\nfeature vectors, and (ii) a theoretically-grounded problem transformation with\ncarefully-crafted techniques that enable linear-time clustering without\nexplicitly materializing the NxN dense affinity matrix. Further, we extend\nDEMM+ to handle attribute-less MRGs through non-trivial adaptations. Extensive\nexperiments, comparing DEMM+ against 20 baselines over 11 real MRGs, exhibit\nthat DEMM+ is consistently superior in terms of clustering quality measured\nagainst ground-truth labels, while often being remarkably faster.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faDEMM\u548cDEMM+\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u89e3\u51b3\u591a\u5173\u7cfb\u56fe\uff08MRG\uff09\u7684\u805a\u7c7b\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u7684\u4e0d\u8db3\u3002DEMM+\u901a\u8fc7\u4f18\u5316\u6280\u672f\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u5173\u7cfb\u56fe\u805a\u7c7b\u65f6\uff0c\u8981\u4e48\u56e0\u5f02\u6784\u7ed3\u6784\u548c\u5c5e\u6027\u878d\u5408\u4e0d\u4f73\u5bfc\u81f4\u8d28\u91cf\u4f4e\u4e0b\uff0c\u8981\u4e48\u56e0\u91c7\u7528\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u800c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u56fe\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e24\u9636\u6bb5\u4f18\u5316\u76ee\u6807\uff08\u591a\u5173\u7cfbDirichlet\u80fd\u91cf\u548c\u8282\u70b9\u4eb2\u548c\u56fe\u7684Dirichlet\u80fd\u91cf\u4f18\u5316\uff09\u7684DEMM\u548cDEMM+\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u8fd1\u4f3c\u6c42\u89e3\u5668\u548c\u95ee\u9898\u8f6c\u6362\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002", "result": "DEMM+\u572811\u4e2a\u771f\u5b9e\u591a\u5173\u7cfb\u56fe\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5176\u805a\u7c7b\u8d28\u91cf\u4f18\u4e8e20\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u901f\u5ea6\u663e\u8457\u66f4\u5feb\u3002", "conclusion": "DEMM+\u901a\u8fc7\u6280\u672f\u521b\u65b0\u5728\u805a\u7c7b\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u9002\u5408\u5904\u7406\u5927\u89c4\u6a21\u591a\u5173\u7cfb\u56fe\u3002"}}
{"id": "2508.17679", "pdf": "https://arxiv.org/pdf/2508.17679", "abs": "https://arxiv.org/abs/2508.17679", "authors": ["Trinayan Baruah", "Kaustubh Shivdikar", "Sara Prescott", "David Kaeli"], "title": "Characterizing the Behavior of Training Mamba-based State Space Models on GPUs", "categories": ["cs.LG", "cs.AR", "cs.CL"], "comment": null, "summary": "Mamba-based State Space Models (SSM) have emerged as a promising alternative\nto the ubiquitous transformers. Despite the expressive power of transformers,\nthe quadratic complexity of computing attention is a major impediment to\nscaling performance as we increase the sequence length. SSMs provide an\nalternative path that addresses this problem, reducing the computational\ncomplexity requirements of self-attention with novel model architectures for\ndifferent domains and fields such as video, text generation and graphs. Thus,\nit is important to characterize the behavior of these emerging workloads on\nGPUs and understand their requirements during GPU microarchitectural design. In\nthis work we evaluate Mamba-based SSMs and characterize their behavior during\ntraining on GPUs. We construct a workload suite that offers representative\nmodels that span different model architectures. We then use this suite to\nanalyze the architectural implications of running Mamba-based SSMs on GPUs. Our\nwork sheds new light on potential optimizations to continue scaling the\nperformance for such models.", "AI": {"tldr": "Mamba-based SSMs\u4f5c\u4e3atransformer\u7684\u65b0\u5174\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u6027\u80fd\u6269\u5c55\u95ee\u9898\u3002\u672c\u6587\u8bc4\u4f30\u4e86\u5176\u5728GPU\u4e0a\u7684\u8bad\u7ec3\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u4e86\u6f5c\u5728\u4f18\u5316\u65b9\u5411\u3002", "motivation": "\u4f20\u7edftransformer\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6269\u5c55\uff0cMamba-based SSMs\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u6a21\u578b\u67b6\u6784\u7684\u5de5\u4f5c\u8d1f\u8f7d\u5957\u4ef6\uff0c\u7528\u4e8e\u5206\u6790Mamba-based SSMs\u5728GPU\u4e0a\u7684\u8bad\u7ec3\u884c\u4e3a\u3002", "result": "\u63ed\u793a\u4e86SSMs\u5728GPU\u4e0a\u7684\u67b6\u6784\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u6027\u80fd\u6269\u5c55\u7684\u4f18\u5316\u7b56\u7565\u3002", "conclusion": "Mamba-based SSMs\u5728GPU\u4e0a\u7684\u8868\u73b0\u548c\u4f18\u5316\u6f5c\u529b\u4e3a\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2508.16622", "pdf": "https://arxiv.org/pdf/2508.16622", "abs": "https://arxiv.org/abs/2508.16622", "authors": ["Andrew Blair", "Peggy Gregory", "Mary Ellen Foster"], "title": "Observations of atypical users from a pilot deployment of a public-space social robot in a church", "categories": ["cs.HC", "cs.RO"], "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025", "summary": "Though a goal of HRI is the natural integration of social robots into\neveryday public spaces, real-world studies still occur mostly within controlled\nenvironments with predetermined participants. True public spaces present an\nenvironment which is largely unconstrained and unpredictable, frequented by a\ndiverse range of people whose goals can often conflict with those of the robot.\nWhen combined with the general unfamiliarity most people have with social\nrobots, this leads to unexpected human-robot interactions in these public\nspaces that are rarely discussed or detected in other contexts. In this paper,\nwe describe atypical users we observed interacting with our robot, and those\nwho did not, during a three-day pilot deployment within a large working church\nand visitor attraction. We then discuss theoretical future advances in the\nfield that could address these challenges, as well as immediate practical\nmitigations and strategies to help improve public space human-robot\ninteractions in the present. This work contributes empirical insights into the\ndynamics of human-robot interaction in public environments and offers\nactionable guidance for more effective future deployments for social robot\ndesigners.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u793e\u4ea4\u673a\u5668\u4eba\u5728\u771f\u5b9e\u516c\u5171\u7a7a\u95f4\u4e2d\u4e0e\u7528\u6237\u7684\u975e\u5178\u578b\u4ea4\u4e92\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u793e\u4ea4\u673a\u5668\u4eba\u5728\u771f\u5b9e\u516c\u5171\u7a7a\u95f4\u4e2d\u4e0e\u591a\u6837\u5316\u7528\u6237\u7684\u4ea4\u4e92\u6311\u6218\uff0c\u586b\u8865\u76f8\u5173\u9886\u57df\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u4e3a\u671f\u4e09\u5929\u7684\u8bd5\u70b9\u90e8\u7f72\uff0c\u89c2\u5bdf\u5e76\u5206\u6790\u673a\u5668\u4eba\u4e0e\u7528\u6237\u7684\u4ea4\u4e92\u884c\u4e3a\uff0c\u5305\u62ec\u975e\u5178\u578b\u7528\u6237\u53ca\u5176\u53cd\u5e94\u3002", "result": "\u63ed\u793a\u4e86\u516c\u5171\u7a7a\u95f4\u4e2d\u975e\u9884\u671f\u7684\u673a\u5668\u4eba\u4ea4\u4e92\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u516c\u5171\u7a7a\u95f4\u4e2d\u793e\u4ea4\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u5177\u6709\u6311\u6218\u6027\uff0c\u8bba\u6587\u4e3a\u672a\u6765\u8bbe\u8ba1\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2508.18206", "pdf": "https://arxiv.org/pdf/2508.18206", "abs": "https://arxiv.org/abs/2508.18206", "authors": ["Ritvik Chaturvedi"], "title": "Practical GPU Choices for Earth Observation: ResNet-50 Training Throughput on Integrated, Laptop, and Cloud Accelerators", "categories": ["cs.DC", "cs.LG", "68T45, 86A32", "I.2.10; I.5.4"], "comment": "10 pages, 5 figures", "summary": "This project implements a ResNet-based pipeline for land use and land cover\n(LULC) classification on Sentinel-2 imagery, benchmarked across three\nheterogeneous GPUs. The workflow automates data acquisition, geospatial\npreprocessing, tiling, model training, and visualization, and is fully\ncontainerized for reproducibility. Performance evaluation reveals up to a 2x\ntraining speed-up on an NVIDIA RTX 3060 and a Tesla T4 compared to the Apple M3\nPro baseline, while maintaining high classification accuracy on the EuroSAT\ndataset. These results demonstrate the feasibility of deploying deep learning\nLULC models on consumer and free cloud GPUs for scalable geospatial analytics.", "AI": {"tldr": "\u57fa\u4e8eResNet\u7684Sentinel-2\u5f71\u50cf\u571f\u5730\u8986\u76d6\u5206\u7c7b\u6d41\u7a0b\uff0c\u5728\u4e09\u79cdGPU\u4e0a\u5b9e\u73b02\u500d\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u5e76\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u63a2\u7d22\u5728\u6d88\u8d39\u7ea7\u548c\u4e91GPU\u4e0a\u90e8\u7f72\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u53ef\u6269\u5c55\u5730\u7406\u7a7a\u95f4\u5206\u6790\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316\u6570\u636e\u83b7\u53d6\u3001\u9884\u5904\u7406\u3001\u5206\u5757\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u53ef\u89c6\u5316\u7684\u5bb9\u5668\u5316\u6d41\u7a0b\uff0c\u57fa\u4e8eResNet\u6a21\u578b\u8fdb\u884c\u571f\u5730\u8986\u76d6\u5206\u7c7b\u3002", "result": "\u5728NVIDIA RTX 3060\u548cTesla T4\u4e0a\u8bad\u7ec3\u901f\u5ea6\u8f83Apple M3 Pro\u63d0\u53472\u500d\uff0c\u540c\u65f6\u5728EuroSAT\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u6d88\u8d39\u7ea7\u548c\u4e91GPU\u53ef\u6709\u6548\u652f\u6301\u6df1\u5ea6\u5b66\u4e60\u5730\u7406\u7a7a\u95f4\u5206\u6790\u6a21\u578b\u7684\u90e8\u7f72\u3002"}}
{"id": "2508.17882", "pdf": "https://arxiv.org/pdf/2508.17882", "abs": "https://arxiv.org/abs/2508.17882", "authors": ["Izudin Dzafic", "Rabih A. Jabr"], "title": "modelSolver: A Symbolic Model-Driven Solver for Power Network Simulation and Monitoring", "categories": ["cs.SE", "cs.SC", "cs.SY", "eess.SY"], "comment": null, "summary": "The development of advanced software tools for power system analysis requires\nextensive programming expertise. Even when using open-source tools, programming\nskills are essential to modify built-in models. This can be particularly\nchallenging for domain experts who lack coding proficiency. This paper\nintroduces modelSolver, a software solution with a new framework centered\naround symbolic mathematical modeling. The proposed paradigm facilitates\ndefining models through intuitive mathematical expressions, thus eliminating\nthe need for traditional programming constructs such as arrays, loops, and\nsparse matrix computations. The modelSolver focuses on power flow and state\nestimation using an open-box approach, which allows users to specify custom\nmodels using either real or complex variables. Unlike existing tools that rely\non hard-coded models, modelSolver enables the representation of a wide range of\nadvanced functionalities, including power flow with voltage regulators and load\ntap changers, continuation power flow, and Gauss-Newton state estimation with\nequality constraints. Compatibility with MATPOWER is ensured via a converter\nthat automates importing data files. The framework prioritizes model-driven\ndevelopment and empowers domain experts to focus on power system modeling\nwithout programming barriers. It aims to simplify power system computations,\nmaking them more accessible to students, scientists, and practitioners.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3amodelSolver\u7684\u8f6f\u4ef6\u5de5\u5177\uff0c\u901a\u8fc7\u7b26\u53f7\u6570\u5b66\u5efa\u6a21\u6846\u67b6\u7b80\u5316\u7535\u529b\u7cfb\u7edf\u5206\u6790\uff0c\u65e0\u9700\u7f16\u7a0b\u5373\u53ef\u81ea\u5b9a\u4e49\u6a21\u578b\u3002", "motivation": "\u7535\u529b\u7cfb\u7edf\u5206\u6790\u8f6f\u4ef6\u901a\u5e38\u9700\u8981\u7f16\u7a0b\u6280\u80fd\uff0c\u8fd9\u5bf9\u7f3a\u4e4f\u7f16\u7a0b\u80fd\u529b\u7684\u9886\u57df\u4e13\u5bb6\u6784\u6210\u6311\u6218\u3002modelSolver\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u969c\u788d\u3002", "method": "\u57fa\u4e8e\u7b26\u53f7\u6570\u5b66\u5efa\u6a21\u7684\u6846\u67b6\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u76f4\u89c2\u6570\u5b66\u8868\u8fbe\u5f0f\u5b9a\u4e49\u6a21\u578b\uff0c\u652f\u6301\u590d\u6742\u53d8\u91cf\uff0c\u517c\u5bb9MATPOWER\u3002", "result": "\u652f\u6301\u591a\u79cd\u7535\u529b\u7cfb\u7edf\u529f\u80fd\uff08\u5982\u6f6e\u6d41\u8ba1\u7b97\u3001\u72b6\u6001\u4f30\u8ba1\uff09\uff0c\u9002\u5408\u5b66\u751f\u3001\u79d1\u5b66\u5bb6\u548c\u4ece\u4e1a\u8005\u4f7f\u7528\u3002", "conclusion": "modelSolver\u7b80\u5316\u4e86\u7535\u529b\u7cfb\u7edf\u8ba1\u7b97\uff0c\u4f7f\u9886\u57df\u4e13\u5bb6\u65e0\u9700\u7f16\u7a0b\u5373\u53ef\u4e13\u6ce8\u4e8e\u5efa\u6a21\u3002"}}
{"id": "2508.18190", "pdf": "https://arxiv.org/pdf/2508.18190", "abs": "https://arxiv.org/abs/2508.18190", "authors": ["Zirui Tang", "Boyu Niu", "Xuanhe Zhou", "Boxiu Li", "Wei Zhou", "Jiannan Wang", "Guoliang Li", "Xinyi Zhang", "Fan Wu"], "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering", "categories": ["cs.AI", "cs.DB", "cs.IR"], "comment": "Extension of our SIGMOD 2026 paper. Please refer to source code\n  available at: https://github.com/weAIDB/ST-Raptor", "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.", "AI": {"tldr": "ST-Raptor\u662f\u4e00\u4e2a\u57fa\u4e8e\u6811\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u534a\u7ed3\u6784\u5316\u8868\u683c\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\uff0c\u901a\u8fc7HO-Tree\u548c\u4e24\u9636\u6bb5\u9a8c\u8bc1\u673a\u5236\u663e\u8457\u63d0\u9ad8\u56de\u7b54\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u534a\u7ed3\u6784\u5316\u8868\u683c\u65f6\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u5e03\u5c40\u7406\u89e3\u56f0\u96be\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u9ad8\u6548\u56de\u7b54\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u3002", "method": "\u63d0\u51faHO-Tree\u7ed3\u6784\u6a21\u578b\uff0c\u5b9a\u4e49\u57fa\u672c\u6811\u64cd\u4f5c\uff0c\u5e76\u5f15\u5165\u4e24\u9636\u6bb5\u9a8c\u8bc1\u673a\u5236\uff08\u524d\u5411\u548c\u540e\u5411\u9a8c\u8bc1\uff09\u3002", "result": "\u5728SSTQA\u6570\u636e\u96c6\u4e0a\uff0cST-Raptor\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u9ad820%\u3002", "conclusion": "ST-Raptor\u6709\u6548\u89e3\u51b3\u4e86\u534a\u7ed3\u6784\u5316\u8868\u683c\u95ee\u7b54\u7684\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.16779", "pdf": "https://arxiv.org/pdf/2508.16779", "abs": "https://arxiv.org/abs/2508.16779", "authors": ["Md Sabbir Ahmed", "Rahat Jahangir Rony", "Mohammad Abdul Hadi", "Ekram Hossain", "Nova Ahmed"], "title": "A Minimalistic Approach to Predict and Understand the Relation of App Usage with Students' Academic Performances", "categories": ["cs.HC"], "comment": null, "summary": "Due to usage of self-reported data which may contain biasness, the existing\nstudies may not unveil the exact relation between academic grades and app\ncategories such as Video. Additionally, the existing systems' requirement for\ndata of prolonged period to predict grades may not facilitate early\nintervention to improve it. Thus, we presented an app that retrieves past 7\ndays' actual app usage data within a second (Mean=0.31s, SD=1.1s). Our analysis\non 124 Bangladeshi students' real-time data demonstrates app usage sessions\nhave a significant (p<0.05) negative association with CGPA. However, the\nProductivity and Books categories have a significant positive association\nwhereas Video has a significant negative association. Moreover, the high and\nlow CGPA holders have significantly different app usage behavior. Leveraging\nonly the instantly accessed data, our machine learning model predicts CGPA\nwithin 0.36 of the actual CGPA. We discuss the design implications that can be\npotential for students to improve grades.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u6b3e\u5e94\u7528\uff0c\u901a\u8fc7\u5b9e\u65f6\u83b7\u53d6\u5b66\u751f\u8fc7\u53bb7\u5929\u7684\u5b9e\u9645\u5e94\u7528\u4f7f\u7528\u6570\u636e\uff0c\u5feb\u901f\u9884\u6d4b\u5b66\u672f\u6210\u7ee9\uff08CGPA\uff09\uff0c\u907f\u514d\u73b0\u6709\u7814\u7a76\u4e2d\u7684\u504f\u89c1\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4f9d\u8d56\u81ea\u62a5\u6570\u636e\u53ef\u80fd\u5b58\u504f\uff0c\u4e14\u9700\u957f\u65f6\u95f4\u6570\u636e\u9884\u6d4b\u6210\u7ee9\uff0c\u65e0\u6cd5\u65e9\u671f\u5e72\u9884\u3002", "method": "\u5f00\u53d1\u5e94\u7528\u5b9e\u65f6\u83b7\u53d67\u5929\u5185\u5e94\u7528\u4f7f\u7528\u6570\u636e\uff0c\u5206\u6790124\u540d\u5b5f\u52a0\u62c9\u5b66\u751f\u7684\u6570\u636e\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4bCGPA\u3002", "result": "\u53d1\u73b0\u5e94\u7528\u4f7f\u7528\u4e0eCGPA\u663e\u8457\u76f8\u5173\uff0c\u751f\u4ea7\u529b\u548c\u4e66\u7c4d\u7c7b\u5e94\u7528\u5bf9\u6210\u7ee9\u6709\u6b63\u9762\u5f71\u54cd\uff0c\u89c6\u9891\u7c7b\u53cd\u4e4b\u3002\u6a21\u578b\u9884\u6d4b\u8bef\u5dee\u4e3a0.36\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u6570\u636e\u7684\u5feb\u901f\u6210\u7ee9\u9884\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u63d0\u5347\u5b66\u751f\u6210\u7ee9\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u542f\u793a\u3002"}}
{"id": "2508.18224", "pdf": "https://arxiv.org/pdf/2508.18224", "abs": "https://arxiv.org/abs/2508.18224", "authors": ["Ran Yan", "Youhe Jiang", "Binhang Yuan"], "title": "Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Recent progress in sparse attention mechanisms has demonstrated strong\npotential for reducing the computational cost of long-context training and\ninference in large language models (LLMs). Native Sparse Attention (NSA), a\nstate-of-the-art approach, introduces natively trainable, hardware-aligned\nsparse attention that delivers substantial system-level performance gains while\nmaintaining accuracy comparable to full attention. However, the kernel\nimplementation of NSA relies on a query-grouping strategy that is efficient\nonly with large Grouped Query Attention (GQA) sizes, whereas modern LLMs\ntypically adopt much smaller GQA groups, which limits the applicability of this\nsparse algorithmic advance. In this work, we propose Flash Sparse Attention\n(FSA), which includes an alternative kernel design that enables efficient NSA\ncomputation across a wide range of popular LLMs with varied smaller GQA group\nsizes on modern GPUs. Compared to vanilla NSA kernel implementation, our\nempirical evaluation demonstrates that FSA achieves (i) up to 3.5$\\times$ and\non average 1.6$\\times$ kernel-level latency reduction, (ii) up to 1.25$\\times$\nand 1.09$\\times$ on average end-to-end training speedup on state-of-the-art\nLLMs, and (iii) up to 1.36$\\times$ and 1.11$\\times$ on average end-to-end\nprefill speedup on state-of-the-art LLMs. The source code is open-sourced and\npublicly available at\nhttps://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFlash Sparse Attention (FSA)\uff0c\u6539\u8fdb\u4e86Native Sparse Attention (NSA)\u7684\u6838\u8bbe\u8ba1\uff0c\u4f7f\u5176\u5728\u591a\u79cdGQA\u5206\u7ec4\u5927\u5c0f\u7684\u73b0\u4ee3LLMs\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "NSA\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u5176\u6838\u5b9e\u73b0\u4f9d\u8d56\u4e8e\u5927GQA\u5206\u7ec4\u7684\u7b56\u7565\uff0c\u9650\u5236\u4e86\u5728\u73b0\u4ee3\u5e38\u7528\u5c0fGQA\u5206\u7ec4LLMs\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faFSA\uff0c\u901a\u8fc7\u66ff\u4ee3\u6838\u8bbe\u8ba1\u5b9e\u73b0NSA\u5728\u591a\u79cdGQA\u5206\u7ec4\u5927\u5c0f\u4e0b\u7684\u9ad8\u6548\u8ba1\u7b97\u3002", "result": "FSA\u5728\u6838\u7ea7\u5ef6\u8fdf\u4e0a\u5e73\u5747\u63d0\u53471.6\u500d\uff0c\u8bad\u7ec3\u7aef\u5230\u7aef\u901f\u5ea6\u5e73\u5747\u63d0\u53471.09\u500d\uff0c\u9884\u586b\u5145\u7aef\u5230\u7aef\u901f\u5ea6\u5e73\u5747\u63d0\u53471.11\u500d\u3002", "conclusion": "FSA\u6269\u5c55\u4e86NSA\u7684\u9002\u7528\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u4ee3LLMs\u7684\u6027\u80fd\u3002"}}
{"id": "2508.17900", "pdf": "https://arxiv.org/pdf/2508.17900", "abs": "https://arxiv.org/abs/2508.17900", "authors": ["Mohammed O. Alannsary"], "title": "A Defect Classification Framework for AI-Based Software Systems (AI-ODC)", "categories": ["cs.SE", "cs.AI"], "comment": "Article, 19 pages, 6 figures, 8 tables,", "summary": "Artificial Intelligence has gained a lot of attention recently, it has been\nutilized in several fields ranging from daily life activities, such as\nresponding to emails and scheduling appointments, to manufacturing and\nautomating work activities. Artificial Intelligence systems are mainly\nimplemented as software solutions, and it is essential to discover and remove\nsoftware defects to assure its quality using defect analysis which is one of\nthe major activities that contribute to software quality. Despite the\nproliferation of AI-based systems, current defect analysis models fail to\ncapture their unique attributes. This paper proposes a framework inspired by\nthe Orthogonal Defect Classification (ODC) paradigm and enables defect analysis\nof Artificial Intelligence systems while recognizing its special attributes and\ncharacteristics. This study demonstrated the feasibility of modifying ODC for\nAI systems to classify its defects. The ODC was adjusted to accommodate the\nData, Learning, and Thinking aspects of AI systems which are newly introduced\nclassification dimensions. This adjustment involved the introduction of an\nadditional attribute to the ODC attributes, the incorporation of a new severity\nlevel, and the substitution of impact areas with characteristics pertinent to\nAI systems. The framework was showcased by applying it to a publicly available\nMachine Learning bug dataset, with results analyzed through one-way and two-way\nanalysis. The case study indicated that defects occurring during the Learning\nphase were the most prevalent and were significantly linked to high-severity\nclassifications. In contrast, defects identified in the Thinking phase had a\ndisproportionate effect on trustworthiness and accuracy. These findings\nillustrate AIODC's capability to identify high-risk defect categories and\ninform focused quality assurance measures.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9AI\u7cfb\u7edf\u7684\u7f3a\u9677\u5206\u6790\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u4ea4\u7f3a\u9677\u5206\u7c7b\uff08ODC\uff09\u7684\u6539\u8fdb\u6846\u67b6AIODC\uff0c\u8bc6\u522b\u4e86AI\u7279\u6709\u7684\u6570\u636e\u3001\u5b66\u4e60\u548c\u601d\u8003\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u9677\u5206\u6790\u6a21\u578b\u672a\u80fd\u6355\u6349AI\u7cfb\u7edf\u7684\u72ec\u7279\u5c5e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9002\u5e94AI\u7279\u6027\u7684\u7f3a\u9677\u5206\u6790\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf\u3002", "method": "\u6539\u8fdbODC\u6846\u67b6\uff0c\u65b0\u589e\u6570\u636e\u3001\u5b66\u4e60\u548c\u601d\u8003\u7ef4\u5ea6\uff0c\u8c03\u6574\u5c5e\u6027\u548c\u4e25\u91cd\u6027\u7ea7\u522b\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u516c\u5f00\u7684\u673a\u5668\u5b66\u4e60\u7f3a\u9677\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u5355\u5411\u548c\u53cc\u5411\u5206\u6790\u3002", "result": "\u5b66\u4e60\u9636\u6bb5\u7684\u7f3a\u9677\u6700\u5e38\u89c1\u4e14\u4e0e\u9ad8\u4e25\u91cd\u6027\u76f8\u5173\uff0c\u601d\u8003\u9636\u6bb5\u7684\u7f3a\u9677\u5bf9\u4fe1\u4efb\u548c\u51c6\u786e\u6027\u5f71\u54cd\u663e\u8457\uff0cAIODC\u80fd\u8bc6\u522b\u9ad8\u98ce\u9669\u7f3a\u9677\u7c7b\u522b\u3002", "conclusion": "AIODC\u6846\u67b6\u4e3aAI\u7cfb\u7edf\u7f3a\u9677\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u9488\u5bf9\u6027\u8d28\u91cf\u4fdd\u8bc1\u63aa\u65bd\u7684\u5236\u5b9a\u3002"}}
{"id": "2508.17809", "pdf": "https://arxiv.org/pdf/2508.17809", "abs": "https://arxiv.org/abs/2508.17809", "authors": ["Abdullah Sahruri", "Martin Margala"], "title": "TLGLock: A New Approach in Logic Locking Using Key-Driven Charge Recycling in Threshold Logic Gates", "categories": ["cs.CR", "cs.AR"], "comment": "To appear in the 33rd IFIP/IEEE International Conference on Very\n  Large Scale Integration (VLSI-SoC 2025)", "summary": "Logic locking remains one of the most promising defenses against hardware\npiracy, yet current approaches often face challenges in scalability and design\noverhead. In this paper, we present TLGLock, a new design paradigm that\nleverages the structural expressiveness of Threshold Logic Gates (TLGs) and the\nenergy efficiency of charge recycling to enforce key-dependent functionality at\nthe gate level. By embedding the key into the gate's weighted logic and\nutilizing dynamic charge sharing, TLGLock provides a stateless and compact\nalternative to conventional locking techniques. We implement a complete\nsynthesis-to-locking flow and evaluate it using ISCAS, ITC, and MCNC\nbenchmarks. Results show that TLGLock achieves up to 30% area, 50% delay, and\n20% power savings compared to latch-based locking schemes. In comparison with\nXOR and SFLL-HD methods, TLGLock offers up to 3x higher SAT attack resistance\nwith significantly lower overhead. Furthermore, randomized key-weight\nexperiments demonstrate that TLGLock can reach up to 100% output corruption\nunder incorrect keys, enabling tunable security at minimal cost. These results\nposition TLGLock as a scalable and resilient solution for secure hardware\ndesign.", "AI": {"tldr": "TLGLock\u662f\u4e00\u79cd\u65b0\u578b\u7684\u786c\u4ef6\u9501\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5229\u7528\u9608\u503c\u903b\u8f91\u95e8\u548c\u7535\u8377\u56de\u6536\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u3001\u5b89\u5168\u7684\u5bc6\u94a5\u4f9d\u8d56\u529f\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8282\u7701\u9762\u79ef\u3001\u5ef6\u8fdf\u548c\u529f\u8017\uff0c\u4e14\u5177\u6709\u66f4\u9ad8\u7684SAT\u653b\u51fb\u6297\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u786c\u4ef6\u9501\u6280\u672f\u7684\u53ef\u6269\u5c55\u6027\u548c\u8bbe\u8ba1\u5f00\u9500\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u9608\u503c\u903b\u8f91\u95e8\u7684\u7ed3\u6784\u8868\u8fbe\u6027\u548c\u7535\u8377\u56de\u6536\u7684\u80fd\u91cf\u6548\u7387\uff0c\u5d4c\u5165\u5bc6\u94a5\u5230\u95e8\u7684\u52a0\u6743\u903b\u8f91\u4e2d\uff0c\u5e76\u5229\u7528\u52a8\u6001\u7535\u8377\u5171\u4eab\u5b9e\u73b0\u65e0\u72b6\u6001\u4e14\u7d27\u51d1\u7684\u9501\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cTLGLock\u5728\u9762\u79ef\u3001\u5ef6\u8fdf\u548c\u529f\u8017\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u9501\u6280\u672f\uff0cSAT\u653b\u51fb\u6297\u6027\u63d0\u53473\u500d\uff0c\u4e14\u8f93\u51fa\u9519\u8bef\u7387\u53ef\u8c03\u3002", "conclusion": "TLGLock\u4e3a\u5b89\u5168\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.16914", "pdf": "https://arxiv.org/pdf/2508.16914", "abs": "https://arxiv.org/abs/2508.16914", "authors": ["Matthew Termuende", "Kevin Larson", "Miguel Nacenta"], "title": "The Impact of Visual Segmentation on Lexical Word Recognition", "categories": ["cs.HC"], "comment": "11 pages, accepted to IEEE VIS 2025", "summary": "When a reader encounters a word in English, they split the word into smaller\northographic units in the process of recognizing its meaning. For example,\n\"rough\", when split according to phonemes, is decomposed as r-ou-gh (not as\nr-o-ugh or r-ough), where each group of letters corresponds to a sound. Since\nthere are many ways to segment a group of letters, this constitutes a\ncomputational operation that has to be solved by the reading brain, many times\nper minute, in order to achieve the recognition of words in text necessary for\nreading. We hypothesized that providing segmentation information in the text\nitself could help the reading process by reducing its computational cost. In\nthis paper we explore whether and how different visual interventions could\ncommunicate segmentation information for reading and word recognition. We ran a\nseries of pre-registered lexical decision experiments with 192 participants\nthat tested five types of visual segmentations: outlines, spacing, connections,\nunderlines and color. The evidence indicates that, even with a moderate amount\nof training, these visual interventions always slow down word identification,\nbut each to a different extent. These findings are important because they\nindicate that, at least for typical adult readers with a moderate amount of\nspecific training in these visual interventions, accelerating the lexical\ndecision task is unlikely. The results also offer an empirical measurement of\nthe cost of a common set of visual manipulations of text, which can be useful\nfor practitioners seeking to visualize alongside or within text without\nimpacting reading performance. Finally, the interaction between typographically\nencoded information and visual variables presented unique patterns that deviate\nfrom existing theories, suggesting new directions for future inquiry.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u89c6\u89c9\u5e72\u9884\uff08\u5982\u5206\u6bb5\u6807\u8bb0\uff09\u5982\u4f55\u5f71\u54cd\u82f1\u8bed\u5355\u8bcd\u8bc6\u522b\uff0c\u53d1\u73b0\u8fd9\u4e9b\u5e72\u9884\u901a\u5e38\u4f1a\u51cf\u6162\u8bc6\u522b\u901f\u5ea6\uff0c\u4e14\u4e0d\u540c\u5e72\u9884\u65b9\u5f0f\u6548\u679c\u4e0d\u540c\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u5206\u6bb5\u4fe1\u606f\u662f\u5426\u80fd\u5e2e\u52a9\u9605\u8bfb\uff0c\u964d\u4f4e\u5176\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u901a\u8fc7192\u540d\u53c2\u4e0e\u8005\u7684\u9884\u6ce8\u518c\u8bcd\u6c47\u51b3\u7b56\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u4e86\u4e94\u79cd\u89c6\u89c9\u5206\u6bb5\u65b9\u5f0f\u3002", "result": "\u89c6\u89c9\u5e72\u9884\u4f1a\u51cf\u6162\u5355\u8bcd\u8bc6\u522b\u901f\u5ea6\uff0c\u4f46\u4e0d\u540c\u65b9\u5f0f\u6548\u679c\u5404\u5f02\u3002", "conclusion": "\u89c6\u89c9\u5e72\u9884\u5bf9\u5178\u578b\u6210\u4eba\u8bfb\u8005\u7684\u8bcd\u6c47\u51b3\u7b56\u4efb\u52a1\u65e0\u52a0\u901f\u4f5c\u7528\uff0c\u7ed3\u679c\u63d0\u4f9b\u4e86\u89c6\u89c9\u6587\u672c\u64cd\u4f5c\u7684\u6210\u672c\u6d4b\u91cf\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.16298", "pdf": "https://arxiv.org/pdf/2508.16298", "abs": "https://arxiv.org/abs/2508.16298", "authors": ["Kexin Feng", "Chuang Chen", "Zi Yang Meng"], "title": "Scalable Hybrid quantum Monte Carlo simulation of U(1) gauge field coupled to fermions on GPU", "categories": ["cond-mat.str-el", "cs.DC", "hep-th"], "comment": "15+4 pages, 10+7 figures", "summary": "We develop a GPU-accelerated hybrid quantum Monte Carlo (QMC) algorithm to\nsolve the fundamental yet difficult problem of $U(1)$ gauge field coupled to\nfermions, which gives rise to a $U(1)$ Dirac spin liquid state under the\ndescription of (2+1)d quantum electrodynamics QED$_3$. The algorithm renders a\ngood acceptance rate and, more importantly, nearly linear space-time volume\nscaling in computational complexity $O(N_{\\tau} V_s)$, where $N_\\tau$ is the\nimaginary time dimension and $V_s$ is spatial volume, which is much more\nefficient than determinant QMC with scaling behavior of $O(N_\\tau V_s^3)$. Such\nacceleration is achieved via a collection of technical improvements, including\n(i) the design of the efficient problem-specific preconditioner, (ii)\ncustomized CUDA kernel for matrix-vector multiplication, and (iii) CUDA Graph\nimplementation on the GPU. These advances allow us to simulate the $U(1)$ Dirac\nspin liquid state with unprecedentedly large system sizes, which is up to\n$N_\\tau\\times L\\times L = 660\\times66\\times66$, and reveal its novel\nproperties. With these technical improvements, we see the asymptotic\nconvergence in the scaling dimensions of various fermion bilinear operators and\nthe conserved current operator when approaching the thermodynamic limit. The\nscaling dimensions find good agreement with field-theoretical expectation,\nwhich provides supporting evidence for the conformal nature of the $U(1)$ Dirac\nspin liquid state in the \\qed. Our technical advancements open an avenue to\nstudy the Dirac spin liquid state and its transition towards symmetry-breaking\nphases at larger system sizes and with less computational burden.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u7684\u6df7\u5408\u91cf\u5b50\u8499\u7279\u5361\u7f57\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86U(1)\u89c4\u8303\u573a\u4e0e\u8d39\u7c73\u5b50\u8026\u5408\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8ba1\u7b97U(1)\u89c4\u8303\u573a\u4e0e\u8d39\u7c73\u5b50\u8026\u5408\u95ee\u9898\u65f6\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u91c7\u7528GPU\u52a0\u901f\u7684\u6df7\u5408\u91cf\u5b50\u8499\u7279\u5361\u7f57\u7b97\u6cd5\uff0c\u7ed3\u5408\u9ad8\u6548\u9884\u5904\u7406\u5668\u3001\u5b9a\u5236CUDA\u5185\u6838\u548cCUDA Graph\u5b9e\u73b0\u3002", "result": "\u6210\u529f\u6a21\u62df\u4e86\u524d\u6240\u672a\u6709\u7684\u5927\u7cfb\u7edf\u5c3a\u5bf8\uff0c\u9a8c\u8bc1\u4e86U(1) Dirac\u81ea\u65cb\u6db2\u4f53\u6001\u7684\u5171\u5f62\u6027\u8d28\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7814\u7a76\u66f4\u5927\u7cfb\u7edf\u5c3a\u5bf8\u7684Dirac\u81ea\u65cb\u6db2\u4f53\u6001\u53ca\u5176\u76f8\u53d8\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2508.17912", "pdf": "https://arxiv.org/pdf/2508.17912", "abs": "https://arxiv.org/abs/2508.17912", "authors": ["Mohammed O. Alannsary"], "title": "Evaluating Citizen Satisfaction with Saudi Arabia's E-Government Services: A Standards-Based, Theory-Informed Approach", "categories": ["cs.SE", "cs.HC"], "comment": "38 pages, 1 figure, 16 tables, journal research paper", "summary": "As digital government platforms become central to public service delivery,\nunderstanding citizen assessment is crucial for enhancing usability, trust, and\ninclusivity. This study investigates citizen satisfaction with the e-government\nservices in Saudi Arabia through a quality-in-use framework based on ISO/IEC\n25010 and ISO/IEC 25022 standards, interpreted through the lens of the Unified\nTheory of Acceptance and Use of Technology (UTAUT). A structured questionnaire\nwas administered to 500 citizens, yielding 276 valid responses. Satisfaction\nwas evaluated across four dimensions: overall satisfaction, feature\nsatisfaction, trust, and emotional engagement (pleasure). The findings\ndemonstrate consistently high levels of satisfaction regarding usability and\ntrust, aligning with Saudi Arabia's top-tier global ranking in e-government\ndevelopment. However, the results also highlight persistent challenges related\nto service clarity and system responsiveness. Emotional engagement was limited,\nindicating that users perceive these services primarily as functional tools\nrather than as engaging digital experiences. The study offers valuable insights\nfor policymakers and contributes to the theoretical integration of\nstandards-based and behavioral adoption models in the context of citizenship.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7ISO\u6807\u51c6\u548cUTAUT\u7406\u8bba\u8bc4\u4f30\u6c99\u7279\u963f\u62c9\u4f2f\u7535\u5b50\u653f\u5e9c\u670d\u52a1\u7684\u516c\u6c11\u6ee1\u610f\u5ea6\uff0c\u53d1\u73b0\u9ad8\u6ee1\u610f\u5ea6\u548c\u4fe1\u4efb\u5ea6\uff0c\u4f46\u4ecd\u5b58\u5728\u670d\u52a1\u6e05\u6670\u5ea6\u548c\u54cd\u5e94\u6027\u95ee\u9898\uff0c\u60c5\u611f\u6295\u5165\u8f83\u4f4e\u3002", "motivation": "\u4e86\u89e3\u516c\u6c11\u5bf9\u7535\u5b50\u653f\u5e9c\u5e73\u53f0\u7684\u8bc4\u4ef7\uff0c\u4ee5\u63d0\u5347\u670d\u52a1\u7684\u53ef\u7528\u6027\u3001\u4fe1\u4efb\u5ea6\u548c\u5305\u5bb9\u6027\u3002", "method": "\u57fa\u4e8eISO/IEC 25010\u548cISO/IEC 25022\u6807\u51c6\uff0c\u7ed3\u5408UTAUT\u7406\u8bba\uff0c\u5bf9500\u540d\u516c\u6c11\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u5206\u6790276\u4efd\u6709\u6548\u56de\u590d\u3002", "result": "\u516c\u6c11\u5728\u53ef\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\u4e0a\u8868\u73b0\u51fa\u9ad8\u6ee1\u610f\u5ea6\uff0c\u4f46\u670d\u52a1\u6e05\u6670\u5ea6\u548c\u7cfb\u7edf\u54cd\u5e94\u6027\u4ecd\u6709\u6311\u6218\uff0c\u60c5\u611f\u6295\u5165\u8f83\u4f4e\u3002", "conclusion": "\u7814\u7a76\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u5e76\u63a8\u52a8\u4e86\u6807\u51c6\u548c\u884c\u4e3a\u6a21\u578b\u5728\u516c\u6c11\u670d\u52a1\u4e2d\u7684\u7406\u8bba\u6574\u5408\u3002"}}
{"id": "2508.16926", "pdf": "https://arxiv.org/pdf/2508.16926", "abs": "https://arxiv.org/abs/2508.16926", "authors": ["Minghao Tu", "Chun Yu", "Xiyuan Shen", "Zhi Zheng", "Li Chen", "Yuanchun Shi"], "title": "TextOnly: A Unified Function Portal for Text-Related Functions on Smartphones", "categories": ["cs.HC", "cs.AI"], "comment": "27 pages, 9 figures", "summary": "Text boxes serve as portals to diverse functionalities in today's smartphone\napplications. However, when it comes to specific functionalities, users always\nneed to navigate through multiple steps to access particular text boxes for\ninput. We propose TextOnly, a unified function portal that enables users to\naccess text-related functions from various applications by simply inputting\ntext into a sole text box. For instance, entering a restaurant name could\ntrigger a Google Maps search, while a greeting could initiate a conversation in\nWhatsApp. Despite their brevity, TextOnly maximizes the utilization of these\nraw text inputs, which contain rich information, to interpret user intentions\neffectively. TextOnly integrates large language models(LLM) and a BERT model.\nThe LLM consistently provides general knowledge, while the BERT model can\ncontinuously learn user-specific preferences and enable quicker predictions.\nReal-world user studies demonstrated TextOnly's effectiveness with a top-1\naccuracy of 71.35%, and its ability to continuously improve both its accuracy\nand inference speed. Participants perceived TextOnly as having satisfactory\nusability and expressed a preference for TextOnly over manual executions.\nCompared with voice assistants, TextOnly supports a greater range of\ntext-related functions and allows for more concise inputs.", "AI": {"tldr": "TextOnly\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u529f\u80fd\u95e8\u6237\uff0c\u901a\u8fc7\u5355\u4e00\u6587\u672c\u6846\u5b9e\u73b0\u591a\u79cd\u6587\u672c\u76f8\u5173\u529f\u80fd\u7684\u5feb\u901f\u8bbf\u95ee\uff0c\u7ed3\u5408LLM\u548cBERT\u6a21\u578b\uff0c\u51c6\u786e\u7387\u9ad8\u4e14\u6301\u7eed\u5b66\u4e60\u4f18\u5316\u3002", "motivation": "\u667a\u80fd\u624b\u673a\u5e94\u7528\u4e2d\uff0c\u7528\u6237\u9700\u591a\u6b21\u64cd\u4f5c\u624d\u80fd\u8bbf\u95ee\u7279\u5b9a\u6587\u672c\u6846\uff0c\u529f\u80fd\u8bbf\u95ee\u6548\u7387\u4f4e\uff0cTextOnly\u65e8\u5728\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u63d0\u4f9b\u901a\u7528\u77e5\u8bc6\uff0cBERT\u6a21\u578b\u5b66\u4e60\u7528\u6237\u504f\u597d\uff0c\u5b9e\u73b0\u610f\u56fe\u89e3\u6790\u3002", "result": "\u771f\u5b9e\u7528\u6237\u7814\u7a76\u4e2d\uff0cTextOnly\u7684top-1\u51c6\u786e\u7387\u8fbe71.35%\uff0c\u4e14\u80fd\u6301\u7eed\u63d0\u5347\u51c6\u786e\u7387\u548c\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "TextOnly\u5728\u529f\u80fd\u548c\u8f93\u5165\u7b80\u6d01\u6027\u4e0a\u4f18\u4e8e\u8bed\u97f3\u52a9\u624b\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u9ad8\u3002"}}
{"id": "2508.17988", "pdf": "https://arxiv.org/pdf/2508.17988", "abs": "https://arxiv.org/abs/2508.17988", "authors": ["Eduardo de Conto", "Blaise Genest", "Arvind Easwaran", "Nicholas Ng", "Shweta Menon"], "title": "DesCartes Builder: A Tool to Develop Machine-Learning Based Digital Twins", "categories": ["cs.SE", "cs.LG"], "comment": "5 pages, 4 figures. Accepted at EDTconf 2025", "summary": "Digital twins (DTs) are increasingly utilized to monitor, manage, and\noptimize complex systems across various domains, including civil engineering. A\ncore requirement for an effective DT is to act as a fast, accurate, and\nmaintainable surrogate of its physical counterpart, the physical twin (PT). To\nthis end, machine learning (ML) is frequently employed to (i) construct\nreal-time DT prototypes using efficient reduced-order models (ROMs) derived\nfrom high-fidelity simulations of the PT's nominal behavior, and (ii)\nspecialize these prototypes into DT instances by leveraging historical sensor\ndata from the target PT. Despite the broad applicability of ML, its use in DT\nengineering remains largely ad hoc. Indeed, while conventional ML pipelines\noften train a single model for a specific task, DTs typically require multiple,\ntask- and domain-dependent models. Thus, a more structured approach is required\nto design DTs.\n  In this paper, we introduce DesCartes Builder, an open-source tool to enable\nthe systematic engineering of ML-based pipelines for real-time DT prototypes\nand DT instances. The tool leverages an open and flexible visual data flow\nparadigm to facilitate the specification, composition, and reuse of ML models.\nIt also integrates a library of parameterizable core operations and ML\nalgorithms tailored for DT design. We demonstrate the effectiveness and\nusability of DesCartes Builder through a civil engineering use case involving\nthe design of a real-time DT prototype to predict the plastic strain of a\nstructure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDesCartes Builder\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bbe\u8ba1\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u539f\u578b\u548c\u5b9e\u4f8b\u3002\u8be5\u5de5\u5177\u901a\u8fc7\u53ef\u89c6\u5316\u6570\u636e\u6d41\u8303\u5f0f\uff0c\u652f\u6301ML\u6a21\u578b\u7684\u89c4\u8303\u3001\u7ec4\u5408\u548c\u91cd\u7528\uff0c\u5e76\u96c6\u6210\u4e86\u9002\u7528\u4e8eDT\u8bbe\u8ba1\u7684\u6838\u5fc3\u64cd\u4f5c\u548c\u7b97\u6cd5\u3002\u901a\u8fc7\u4e00\u4e2a\u571f\u6728\u5de5\u7a0b\u7528\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\uff08DTs\uff09\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u8bbe\u8ba1\u8fc7\u7a0b\u7f3a\u4e4f\u7ed3\u6784\u5316\u65b9\u6cd5\u3002\u4f20\u7edfML\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u4efb\u52a1\u8bad\u7ec3\u6a21\u578b\uff0c\u800cDT\u9700\u8981\u591a\u4e2a\u4efb\u52a1\u548c\u9886\u57df\u76f8\u5173\u7684\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86DesCartes Builder\u5de5\u5177\uff0c\u5229\u7528\u53ef\u89c6\u5316\u6570\u636e\u6d41\u8303\u5f0f\uff0c\u652f\u6301ML\u6a21\u578b\u7684\u89c4\u8303\u3001\u7ec4\u5408\u548c\u91cd\u7528\uff0c\u5e76\u96c6\u6210\u6838\u5fc3\u64cd\u4f5c\u548c\u7b97\u6cd5\u5e93\u3002", "result": "\u901a\u8fc7\u571f\u6728\u5de5\u7a0b\u7528\u4f8b\uff08\u9884\u6d4b\u7ed3\u6784\u5851\u6027\u5e94\u53d8\uff09\u9a8c\u8bc1\u4e86\u5de5\u5177\u7684\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\u3002", "conclusion": "DesCartes Builder\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684ML\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u6570\u5b57\u5b6a\u751f\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u652f\u6301\u3002"}}
{"id": "2508.16966", "pdf": "https://arxiv.org/pdf/2508.16966", "abs": "https://arxiv.org/abs/2508.16966", "authors": ["Surat Teerakapibal", "Poompak Kusawat"], "title": "Opportunities and Challenges of Integrating ChatGPT in Education: Sentiment Analysis and Topic Modeling", "categories": ["cs.HC"], "comment": "20 pages, 2 tables, 12 figures", "summary": "Since its recent debut, ChatGPT has become a global sensation and\nsignificantly impacted the field of education. Both educational researchers and\npractitioners have identified opportunities as well as risks associated with\nthe use of this novel tool in educational settings. Despite the ongoing debate,\nthere is still no research exploring occupational differences in the perception\nof ChatGPT in education. In this paper, we analyzed Twitter data using topic\nmodeling and sentiment analysis to investigate how ChatGPT is perceived and\ndiscussed differently in different occupations. Our study found diverse topics\ndiscussed including its use in schools, impact on exams, academic integrity\nconcerns, and response accuracy evaluations. While most tweets were positive or\nneutral, concerns about integrity and response accuracy were evident. Analysis\nrevealed sentiment and topic variations among users' occupations. These\nfindings emphasize the opportunities and challenges of integrating ChatGPT in\neducation, necessitating continued monitoring and informed policy-making for\nresponsible utilization.", "AI": {"tldr": "\u7814\u7a76\u4e86ChatGPT\u5728\u6559\u80b2\u9886\u57df\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u5728\u4e0d\u540c\u804c\u4e1a\u7fa4\u4f53\u4e2d\u7684\u611f\u77e5\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22ChatGPT\u5728\u6559\u80b2\u4e2d\u7684\u4f7f\u7528\u53ca\u5176\u5bf9\u4e0d\u540c\u804c\u4e1a\u7fa4\u4f53\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e3b\u9898\u5efa\u6a21\u548c\u60c5\u611f\u5206\u6790\u5206\u6790Twitter\u6570\u636e\u3002", "result": "\u53d1\u73b0\u8ba8\u8bba\u4e3b\u9898\u591a\u6837\uff0c\u60c5\u611f\u548c\u8bdd\u9898\u56e0\u804c\u4e1a\u800c\u5f02\uff0c\u591a\u6570\u4e3a\u6b63\u9762\u6216\u4e2d\u6027\uff0c\u4f46\u4e5f\u5b58\u5728\u8bda\u4fe1\u548c\u51c6\u786e\u6027\u62c5\u5fe7\u3002", "conclusion": "\u5f3a\u8c03ChatGPT\u5728\u6559\u80b2\u4e2d\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u9700\u6301\u7eed\u76d1\u6d4b\u5e76\u5236\u5b9a\u8d1f\u8d23\u4efb\u7684\u653f\u7b56\u3002"}}
{"id": "2508.18003", "pdf": "https://arxiv.org/pdf/2508.18003", "abs": "https://arxiv.org/abs/2508.18003", "authors": ["Robert Heum\u00fcller", "Frank Ortmeier"], "title": "Previously on... Automating Code Review", "categories": ["cs.SE", "cs.AI"], "comment": "Preprint currently under review", "summary": "Modern Code Review (MCR) is a standard practice in software engineering, yet\nit demands substantial time and resource investments. Recent research has\nincreasingly explored automating core review tasks using machine learning (ML)\nand deep learning (DL). As a result, there is substantial variability in task\ndefinitions, datasets, and evaluation procedures. This study provides the first\ncomprehensive analysis of MCR automation research, aiming to characterize the\nfield's evolution, formalize learning tasks, highlight methodological\nchallenges, and offer actionable recommendations to guide future research.\nFocusing on the primary code review tasks, we systematically surveyed 691\npublications and identified 24 relevant studies published between May 2015 and\nApril 2024. Each study was analyzed in terms of tasks, models, metrics,\nbaselines, results, validity concerns, and artifact availability. In\nparticular, our analysis reveals significant potential for standardization,\nincluding 48 task metric combinations, 22 of which were unique to their\noriginal paper, and limited dataset reuse. We highlight challenges and derive\nconcrete recommendations for examples such as the temporal bias threat, which\nare rarely addressed so far. Our work contributes to a clearer overview of the\nfield, supports the framing of new research, helps to avoid pitfalls, and\npromotes greater standardization in evaluation practices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u73b0\u4ee3\u4ee3\u7801\u5ba1\u67e5\uff08MCR\uff09\u81ea\u52a8\u5316\u7814\u7a76\uff0c\u7cfb\u7edf\u8c03\u67e5\u4e86691\u7bc7\u6587\u732e\u5e76\u7b5b\u9009\u51fa24\u7bc7\uff0c\u603b\u7ed3\u4efb\u52a1\u3001\u6a21\u578b\u3001\u6307\u6807\u7b49\u65b9\u9762\uff0c\u63d0\u51fa\u4e86\u6807\u51c6\u5316\u5efa\u8bae\u3002", "motivation": "\u73b0\u4ee3\u4ee3\u7801\u5ba1\u67e5\u6d88\u8017\u5927\u91cf\u8d44\u6e90\uff0c\u81ea\u52a8\u5316\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u7814\u7a76\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8f83\u5927\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8c03\u67e5\u548c\u5206\u67902015\u5e74\u81f32024\u5e74\u768424\u7bc7\u76f8\u5173\u7814\u7a76\uff0c\u603b\u7ed3\u4efb\u52a1\u3001\u6a21\u578b\u3001\u6307\u6807\u7b49\u5185\u5bb9\u3002", "result": "\u7814\u7a76\u53d1\u73b048\u79cd\u4efb\u52a1\u6307\u6807\u7ec4\u5408\u4e2d22\u79cd\u662f\u72ec\u7279\u7684\uff0c\u6570\u636e\u96c6\u91cd\u7528\u6709\u9650\uff0c\u8fd8\u63ed\u793a\u4e86\u672a\u5145\u5206\u89e3\u51b3\u7684\u6311\u6218\u5982\u65f6\u95f4\u504f\u5dee\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9886\u57df\u63d0\u4f9b\u4e86\u6e05\u6670\u6982\u89c8\uff0c\u63d0\u51fa\u6807\u51c6\u5316\u5efa\u8bae\uff0c\u5e2e\u52a9\u672a\u6765\u7814\u7a76\u907f\u514d\u9677\u9631\u3002"}}
{"id": "2508.17058", "pdf": "https://arxiv.org/pdf/2508.17058", "abs": "https://arxiv.org/abs/2508.17058", "authors": ["Liuqing Chen", "Yaxuan Song", "Ke Lyu", "Shuhong Xiao", "Yilang Shen", "Lingyun Sun"], "title": "SCENIC: A Location-based System to Foster Cognitive Development in Children During Car Rides", "categories": ["cs.HC"], "comment": "18 pages, 8 figures, accepted to UIST 2025", "summary": "Car-riding is common for children in modern life. Given the repetitive nature\nof daily commutes, they often feel bored and turn to electronic devices for\nentertainment. Meanwhile, the rich and dynamic scenery outside the car\nnaturally attracts children's curiosity and offers valuable resources for\ncognitive development. Our formative study reveals that parents' support during\ncar rides is often fleeting, as accompanying adults may struggle to\nconsistently guide children's exploration. To address this, we propose SCENIC,\nan interactive system that helps children aged 6 to 11 better perceive the\nexternal environment using location-based cognitive development strategies.\nSCENIC builds upon experiential approaches used by parents, resulting in six\nstrategies embedded into the system. To improve engagement during routine\nrides, SCENIC also incorporates dynamic point-of-interest selection and journey\ngallery generation. We evaluated the generated content (N=21) and conducted an\nin-situ user study with seven families and ten children. Results suggest that\nSCENIC enhances the car-riding experience and helps children better connect\nwith their surroundings.", "AI": {"tldr": "SCENIC\u662f\u4e00\u4e2a\u4e92\u52a8\u7cfb\u7edf\uff0c\u5e2e\u52a96\u81f311\u5c81\u513f\u7ae5\u901a\u8fc7\u57fa\u4e8e\u4f4d\u7f6e\u7684\u8ba4\u77e5\u53d1\u5c55\u7b56\u7565\u66f4\u597d\u5730\u611f\u77e5\u5916\u90e8\u73af\u5883\uff0c\u63d0\u5347\u4e58\u8f66\u4f53\u9a8c\u3002", "motivation": "\u513f\u7ae5\u5728\u4e58\u8f66\u65f6\u611f\u5230\u65e0\u804a\uff0c\u5e38\u4f9d\u8d56\u7535\u5b50\u8bbe\u5907\uff0c\u800c\u8f66\u5916\u4e30\u5bcc\u7684\u52a8\u6001\u666f\u89c2\u662f\u8ba4\u77e5\u53d1\u5c55\u7684\u5b9d\u8d35\u8d44\u6e90\u3002\u5bb6\u957f\u96be\u4ee5\u6301\u7eed\u5f15\u5bfc\u513f\u7ae5\u63a2\u7d22\u3002", "method": "\u7ed3\u5408\u5bb6\u957f\u7684\u7ecf\u9a8c\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u516d\u79cd\u7b56\u7565\u7684\u7cfb\u7edf\uff0c\u5305\u62ec\u52a8\u6001\u5174\u8da3\u70b9\u9009\u62e9\u548c\u65c5\u7a0b\u753b\u5eca\u751f\u6210\u3002", "result": "\u8bc4\u4f30\u5185\u5bb9\uff08N=21\uff09\u548c\u5b9e\u5730\u7528\u6237\u7814\u7a76\uff087\u4e2a\u5bb6\u5ead\uff0c10\u540d\u513f\u7ae5\uff09\u663e\u793a\uff0cSCENIC\u63d0\u5347\u4e86\u4e58\u8f66\u4f53\u9a8c\u548c\u513f\u7ae5\u4e0e\u73af\u5883\u7684\u8fde\u63a5\u3002", "conclusion": "SCENIC\u6709\u6548\u6539\u5584\u513f\u7ae5\u4e58\u8f66\u4f53\u9a8c\uff0c\u4fc3\u8fdb\u4ed6\u4eec\u4e0e\u5468\u56f4\u73af\u5883\u7684\u4e92\u52a8\u3002"}}
{"id": "2508.18070", "pdf": "https://arxiv.org/pdf/2508.18070", "abs": "https://arxiv.org/abs/2508.18070", "authors": ["Karolina M. Milano", "Wesley K. G. Assun\u00e7\u00e3o", "Bruno B. P. Cafeo"], "title": "A Large-Scale Study on Developer Engagement and Expertise in Configurable Software System Projects", "categories": ["cs.SE"], "comment": null, "summary": "Modern systems operate in multiple contexts making variability a fundamental\naspect of Configurable Software Systems (CSSs). Variability, implemented via\npre-processor directives (e.g., #ifdef blocks) interleaved with other code and\nspread across files, complicates maintenance and increases error risk. Despite\nits importance, little is known about how variable code is distributed among\ndevelopers or whether conventional expertise metrics adequately capture\nvariable code proficiency. This study investigates developers' engagement with\nvariable versus mandatory code, the concentration of variable code workload,\nand the effectiveness of expertise metrics in CSS projects. We mined\nrepositories of 25 CSS projects, analyzing 450,255 commits from 9,678\ndevelopers. Results show that 59% of developers never modified variable code,\nwhile about 17% were responsible for developing and maintaining 83% of it. This\nindicates a high concentration of variable code expertise among a few\ndevelopers, suggesting that task assignments should prioritize these\nspecialists. Moreover, conventional expertise metrics performed\npoorly--achieving only around 55% precision and 50% recall in identifying\ndevelopers engaged with variable code. Our findings highlight an unbalanced\ndistribution of variable code responsibilities and underscore the need to\nrefine expertise metrics to better support task assignments in CSS projects,\nthereby promoting a more equitable workload distribution.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u53ef\u914d\u7f6e\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u53ef\u53d8\u4ee3\u7801\u7684\u5f00\u53d1\u4e0e\u7ef4\u62a4\u9ad8\u5ea6\u96c6\u4e2d\u5728\u5c11\u6570\u5f00\u53d1\u8005\u624b\u4e2d\uff0c\u4f20\u7edf\u4e13\u4e1a\u77e5\u8bc6\u6307\u6807\u5728\u6b64\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u63a2\u8ba8\u53ef\u53d8\u4ee3\u7801\u5728\u5f00\u53d1\u8005\u4e2d\u7684\u5206\u5e03\u60c5\u51b5\u4ee5\u53ca\u4f20\u7edf\u4e13\u4e1a\u77e5\u8bc6\u6307\u6807\u662f\u5426\u8db3\u4ee5\u8861\u91cf\u53ef\u53d8\u4ee3\u7801\u7684\u719f\u7ec3\u5ea6\u3002", "method": "\u5206\u6790\u4e8625\u4e2a\u53ef\u914d\u7f6e\u8f6f\u4ef6\u9879\u76ee\u76849,678\u540d\u5f00\u53d1\u8005\u7684450,255\u6b21\u63d0\u4ea4\uff0c\u7814\u7a76\u5f00\u53d1\u8005\u5bf9\u53ef\u53d8\u4ee3\u7801\u548c\u5f3a\u5236\u4ee3\u7801\u7684\u53c2\u4e0e\u60c5\u51b5\u3002", "result": "59%\u7684\u5f00\u53d1\u8005\u4ece\u672a\u4fee\u6539\u53ef\u53d8\u4ee3\u7801\uff0c17%\u7684\u5f00\u53d1\u8005\u8d1f\u8d2383%\u7684\u53ef\u53d8\u4ee3\u7801\u5de5\u4f5c\uff1b\u4f20\u7edf\u4e13\u4e1a\u77e5\u8bc6\u6307\u6807\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u4ec5\u4e3a55%\u548c50%\u3002", "conclusion": "\u53ef\u53d8\u4ee3\u7801\u7684\u8d23\u4efb\u5206\u914d\u4e0d\u5747\uff0c\u9700\u8981\u6539\u8fdb\u4e13\u4e1a\u77e5\u8bc6\u6307\u6807\u4ee5\u66f4\u516c\u5e73\u5730\u5206\u914d\u4efb\u52a1\u3002"}}
{"id": "2508.17063", "pdf": "https://arxiv.org/pdf/2508.17063", "abs": "https://arxiv.org/abs/2508.17063", "authors": ["Sameha AlShakhsi", "Ala Yankouskaya", "Magnus Liebherr", "Raian Ali"], "title": "Measuring Large Language Models Dependency: Validating the Arabic Version of the LLM-D12 Scale", "categories": ["cs.HC"], "comment": null, "summary": "There is an urgent need for reliable, culturally validated instruments to\nassess psychological responses to AI in general and large language models\n(LLMs). This need is global issue, but it is especially urgent among\nArabic-speaking populations, where AI and LLMs adoption is accelerating, yet\npsychometric tools remain limited. This study presents the first validation of\nthe LLM-D12, a dual-dimensional scale assessing Instrumental and Relationship\nDependency on LLMs, in an Arab sample. A total of 250 Arab participants\ncompleted the Arabic version of the LLM-D12. Confirmatory Factor Analysis\nconfirms the original 2-factor structure of LLM-D12 with all items showing good\nloading of corresponding Instrumental and Relationship Dependency. The scale\nshowed good to excellent internal reliability (Cronbach alpha is 0.90 for\nTotal, 0.85 for Instrumental Dependency, and 0.90 for Relationship Dependency).\nExternal validation revealed that Instrumental Dependency was positively\nassociated with AI acceptance and internet addiction, while Relationship\nDependency was linked to lower need for cognition and greater trustworthiness\nof LLM, demonstrating sensitivity of this instrument to different use and\npersonal factors. These findings confirm that Arabic LLM-D12 is a\npsychometrically sound, culturally appropriate instrument, offering a necessary\ntool for research, education, and policy concerning AI and LLMs engagement in\nArab contexts.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u963f\u62c9\u4f2f\u8bed\u7248\u672c\u7684LLM-D12\u91cf\u8868\uff0c\u8bc1\u660e\u5176\u5728\u8bc4\u4f30\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\u6027\u548c\u5173\u7cfb\u6027\u4f9d\u8d56\u65b9\u9762\u5177\u6709\u826f\u597d\u5fc3\u7406\u6d4b\u91cf\u7279\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u4eba\u7fa4\u7684\u5fc3\u7406\u6d4b\u91cf\u5de5\u5177\u6765\u8bc4\u4f30\u5bf9AI\uff08\u5c24\u5176\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u7684\u53cd\u5e94\uff0c\u4e9f\u9700\u53ef\u9760\u4e14\u6587\u5316\u9002\u5e94\u6027\u7684\u91cf\u8868\u3002", "method": "250\u540d\u963f\u62c9\u4f2f\u53c2\u4e0e\u8005\u5b8c\u6210\u963f\u62c9\u4f2f\u8bed\u7248LLM-D12\uff0c\u901a\u8fc7\u9a8c\u8bc1\u6027\u56e0\u5b50\u5206\u6790\u548c\u5185\u90e8\u53ef\u9760\u6027\u68c0\u9a8c\uff08Cronbach alpha\uff09\u9a8c\u8bc1\u91cf\u8868\u7ed3\u6784\u3002", "result": "\u91cf\u8868\u663e\u793a\u51fa\u826f\u597d\u7684\u53cc\u56e0\u5b50\u7ed3\u6784\uff08\u5de5\u5177\u6027\u548c\u5173\u7cfb\u6027\u4f9d\u8d56\uff09\uff0c\u5185\u90e8\u53ef\u9760\u6027\u9ad8\uff08\u603b\u91cf\u88680.90\uff09\uff0c\u5e76\u4e0e\u5916\u90e8\u53d8\u91cf\uff08\u5982AI\u63a5\u53d7\u5ea6\u3001\u4e92\u8054\u7f51\u6210\u763e\u7b49\uff09\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u963f\u62c9\u4f2f\u8bedLLM-D12\u662f\u5fc3\u7406\u6d4b\u91cf\u5b66\u53ef\u9760\u3001\u6587\u5316\u9002\u7528\u7684\u5de5\u5177\uff0c\u4e3a\u963f\u62c9\u4f2f\u8bed\u73af\u5883\u4e2dAI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u4e0e\u653f\u7b56\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2508.18073", "pdf": "https://arxiv.org/pdf/2508.18073", "abs": "https://arxiv.org/abs/2508.18073", "authors": ["Joenio Marques da Costa", "Christina von Flach"], "title": "Debian in the Research Software Ecosystem: A Bibliometric Analysis", "categories": ["cs.SE", "cs.DL"], "comment": "5 pages; 3 figures; 2 tables; to be published in DebConf25 Academic\n  Track https://www.diverse-team.fr/debconf25-academictrack", "summary": "Context: The Debian system has historically participated in academic works\nand scientific projects, with well-known examples including NeuroDebian, Debian\nMed, Debsources, Debian Science, and Debian GIS, where the scientific relevance\nof Debian and its contribution to the Research Software ecosystem are evident.\n  Objective: The objective of this study is to investigate the Debian system\nthrough academic publications, with the aim of classifying articles, mapping\nresearch, identifying trends, and finding opportunities.\n  Method: The study is based on a bibliometric analysis starting with an\ninitial search for the term \"Debian\" in the titles, abstracts, or keywords of\nacademic publications, using the Scopus database. This analysis calculates\nmetrics of co-citation, co-authorship, and word co-occurrence, and is guided by\na set of research questions and criteria for inclusion and exclusion to conduct\nthe bibliometric analysis.\n  Results: The study includes a set of articles published across various fields\nof knowledge, providing a map of the academic publication space about Debian.\nThe study's data will be available in a public repository, reporting\ndemographic and bibliometric trends, including the most cited articles, active\ncountries, researchers, and popular conferences.\n  Conclusion: Results includes a bibliometric and demographic analysis\nidentified in publications about Debian, shedding light on the intellectual\nstructure of academic research. The results of the analyses can help\nresearchers gain an overview of existing trends in publications about Debian\nand identify areas that require more attention from the scientific community.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6587\u732e\u8ba1\u91cf\u5206\u6790\u63a2\u8ba8Debian\u7cfb\u7edf\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u7684\u5f71\u54cd\uff0c\u5206\u7c7b\u6587\u732e\u3001\u6620\u5c04\u7814\u7a76\u8d8b\u52bf\uff0c\u5e76\u63d0\u4f9b\u6570\u636e\u4ee5\u52a9\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u76ee\u7684\u4e8e\u7406\u89e3Debian\u5728\u5b66\u672f\u7814\u7a76\u4e2d\u7684\u89d2\u8272\u53ca\u5176\u5bf9\u7814\u7a76\u8f6f\u4ef6\u751f\u6001\u7684\u8d21\u732e\uff0c\u8bc6\u522b\u7814\u7a76\u8d8b\u52bf\u4e0e\u673a\u4f1a\u3002", "method": "\u4f7f\u7528Scopus\u6570\u636e\u5e93\uff0c\u57fa\u4e8e\u5173\u952e\u8bcd'Debian'\u8fdb\u884c\u6587\u732e\u8ba1\u91cf\u5206\u6790\uff0c\u5305\u62ec\u5171\u5f15\u3001\u5408\u8457\u53ca\u8bcd\u5171\u73b0\u5206\u6790\u3002", "result": "\u7814\u7a76\u6db5\u76d6\u591a\u9886\u57df\u6587\u732e\uff0c\u63d0\u4f9b\u4e86Debian\u76f8\u5173\u5b66\u672f\u51fa\u7248\u7684\u7a7a\u95f4\u5730\u56fe\uff0c\u5305\u62ec\u9ad8\u5f15\u8bba\u6587\u3001\u6d3b\u8dc3\u56fd\u5bb6\u4e0e\u7814\u7a76\u8005\u7b49\u6570\u636e\u3002", "conclusion": "\u7ed3\u679c\u63ed\u793a\u4e86Debian\u5b66\u672f\u7814\u7a76\u7684\u667a\u529b\u7ed3\u6784\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u628a\u63e1\u8d8b\u52bf\u5e76\u8bc6\u522b\u9700\u66f4\u591a\u5173\u6ce8\u7684\u9886\u57df\u3002"}}
{"id": "2508.17124", "pdf": "https://arxiv.org/pdf/2508.17124", "abs": "https://arxiv.org/abs/2508.17124", "authors": ["Ryan Ghamandi", "Yahya Hmaiti", "Mykola Maslych", "Ravi Kiran Kattoju", "Joseph J. LaViola Jr"], "title": "Towards Deeper Understanding of Natural User Interactions in Virtual Reality Based Assembly Tasks", "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": "To be submitted in a future conference, this is the author version\n  pre-print", "summary": "We explore natural user interactions using a virtual reality simulation of a\nrobot arm for assembly tasks. Using a Wizard-of-Oz study, participants\ncompleted collaborative LEGO and instructive PCB assembly tasks, with the robot\nresponding under experimenter control. We collected voice, hand tracking, and\ngaze data from users. Statistical analyses revealed that instructive and\ncollaborative scenarios elicit distinct behaviors and adopted strategies,\nparticularly as tasks progress. Users tended to use put-that-there language in\nspatially ambiguous contexts and more descriptive instructions in spatially\nclear ones. Our contributions include the identification of natural interaction\nstrategies through analyses of collected data, as well as the supporting\ndataset, to guide the understanding and design of natural multimodal user\ninterfaces for instructive interaction with systems in virtual reality.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u865a\u62df\u73b0\u5b9e\u673a\u5668\u4eba\u624b\u81c2\u6a21\u62df\u63a2\u7d22\u81ea\u7136\u7528\u6237\u4ea4\u4e92\uff0c\u8bc6\u522b\u4e86\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u884c\u4e3a\u7b56\u7565\u3002", "motivation": "\u63a2\u7d22\u7528\u6237\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u4e0e\u673a\u5668\u4eba\u534f\u4f5c\u65f6\u7684\u81ea\u7136\u4ea4\u4e92\u65b9\u5f0f\uff0c\u4ee5\u6307\u5bfc\u591a\u6a21\u6001\u7528\u6237\u754c\u9762\u7684\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528Wizard-of-Oz\u5b9e\u9a8c\uff0c\u6536\u96c6\u7528\u6237\u5728LEGO\u548cPCB\u7ec4\u88c5\u4efb\u52a1\u4e2d\u7684\u8bed\u97f3\u3001\u624b\u90e8\u8ffd\u8e2a\u548c\u6ce8\u89c6\u6570\u636e\u3002", "result": "\u53d1\u73b0\u534f\u4f5c\u548c\u6307\u5bfc\u573a\u666f\u4e0b\u7528\u6237\u884c\u4e3a\u5dee\u5f02\u663e\u8457\uff0c\u7a7a\u95f4\u6a21\u7cca\u6027\u5f71\u54cd\u6307\u4ee4\u8bed\u8a00\u98ce\u683c\u3002", "conclusion": "\u7814\u7a76\u4e3a\u865a\u62df\u73b0\u5b9e\u4e2d\u81ea\u7136\u591a\u6a21\u6001\u4ea4\u4e92\u8bbe\u8ba1\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u548c\u7b56\u7565\u5206\u6790\u3002"}}
{"id": "2508.18089", "pdf": "https://arxiv.org/pdf/2508.18089", "abs": "https://arxiv.org/abs/2508.18089", "authors": ["Karine Even-Mendoza", "Alexander Brownlee", "Alina Geiger", "Carol Hanna", "Justyna Petke", "Federica Sarro", "Dominik Sobania"], "title": "LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution", "categories": ["cs.SE"], "comment": null, "summary": "Genetic Improvement (GI) of software automatically creates alternative\nsoftware versions that are improved according to certain properties of\ninterests (e.g., running-time). Search-based GI excels at navigating large\nprogram spaces, but operates primarily at the syntactic level. In contrast,\nLarge Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed\nfeedback and control (which is instead a strength of GI). As such, we propose\nthe investigation of a new research line on AI-powered GI aimed at\nincorporating semantic aware search. We take a first step at it by augmenting\nGI with the use of automated clustering of LLM edits. We provide initial\nempirical evidence that our proposal, dubbed PatchCat, allows us to\nautomatically and effectively categorize LLM-suggested patches. PatchCat\nidentified 18 different types of software patches and categorized newly\nsuggested patches with high accuracy. It also enabled detecting NoOp edits in\nadvance and, prospectively, to skip test suite execution to save resources in\nmany cases. These results, coupled with the fact that PatchCat works with\nsmall, local LLMs, are a promising step toward interpretable, efficient, and\ngreen GI. We outline a rich agenda of future work and call for the community to\njoin our vision of building a principled understanding of LLM-driven mutations,\nguiding the GI search process with semantic signals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9057\u4f20\u6539\u8fdb\uff08GI\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u65b9\u6cd5PatchCat\uff0c\u901a\u8fc7\u81ea\u52a8\u805a\u7c7bLLM\u751f\u6210\u7684\u8865\u4e01\u6765\u63d0\u9ad8\u8bed\u4e49\u611f\u77e5\u641c\u7d22\u7684\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u641c\u7d22\u578bGI\u5728\u8bed\u6cd5\u5c42\u9762\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u7f3a\u4e4f\u8bed\u4e49\u611f\u77e5\u80fd\u529b\uff1b\u800cLLM\u63d0\u4f9b\u4e86\u8bed\u4e49\u611f\u77e5\u7684\u7f16\u8f91\uff0c\u4f46\u7f3a\u4e4f\u76ee\u6807\u5bfc\u5411\u7684\u53cd\u9988\u548c\u63a7\u5236\u3002\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u53ef\u4ee5\u63d0\u5347\u8f6f\u4ef6\u6539\u8fdb\u7684\u6548\u7387\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u805a\u7c7bLLM\u751f\u6210\u7684\u8865\u4e01\uff08PatchCat\uff09\uff0c\u5bf9\u8865\u4e01\u8fdb\u884c\u5206\u7c7b\u548c\u4f18\u5316\uff0c\u4ece\u800c\u589e\u5f3aGI\u7684\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u3002", "result": "PatchCat\u6210\u529f\u8bc6\u522b\u4e8618\u79cd\u4e0d\u540c\u7684\u8f6f\u4ef6\u8865\u4e01\u7c7b\u578b\uff0c\u5e76\u80fd\u591f\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u65b0\u8865\u4e01\u3002\u6b64\u5916\uff0c\u5b83\u80fd\u63d0\u524d\u68c0\u6d4b\u65e0\u6548\u7f16\u8f91\uff08NoOp\uff09\uff0c\u8282\u7701\u6d4b\u8bd5\u8d44\u6e90\u3002", "conclusion": "PatchCat\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u73af\u4fdd\u7684GI\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u8fdb\u4e00\u6b65\u63a2\u7d22LLM\u9a71\u52a8\u7684\u7a81\u53d8\u539f\u7406\uff0c\u5e76\u5229\u7528\u8bed\u4e49\u4fe1\u53f7\u6307\u5bfcGI\u641c\u7d22\u8fc7\u7a0b\u3002"}}
{"id": "2508.17362", "pdf": "https://arxiv.org/pdf/2508.17362", "abs": "https://arxiv.org/abs/2508.17362", "authors": ["Refia Daya", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "title": "Virtual Reality in Sign Language Education: Opportunities, Challenges, and the Road Ahead", "categories": ["cs.HC"], "comment": "Paper submited to Elsevier", "summary": "Sign language (SL) is an essential mode of communication for Deaf and\nHard-of-Hearing (DHH) individuals. Its education remains limited by the lack of\nqualified instructors, insufficient early exposure, and the inadequacy of\ntraditional teaching methods. Recent advances in Virtual Reality (VR) and\nArtificial Intelligence (AI) offer promising new approaches to enhance sign\nlanguage learning through immersive, interactive, and feedback-rich\nenvironments. This paper presents a systematic review of 55 peer-reviewed\nstudies on VR-based sign language education, identifying and analyzing five\ncore thematic areas: (1) gesture recognition and real-time feedback mechanisms;\n(2) interactive VR environments for communicative practice; (3) gamification\nfor immersive and motivating learning experiences; (4) personalized and\nadaptive learning systems; and (5) accessibility and inclusivity for diverse\nDHH learners.\n  The results reveal that AI-driven gesture recognition systems integrated with\nVR can provide real-time feedback, significantly improving learner engagement\nand performance. However, the analysis highlights critical challenges: hardware\nlimitations, inconsistent accuracy in gesture recognition, and a lack of\ninclusive and adaptive design. This review contributes a comprehensive\nsynthesis of technological and pedagogical innovations in the field, outlining\ncurrent limitations and proposing actionable recommendations for developers and\nresearchers. By bridging technical advancement with inclusive pedagogy, this\nreview lays the foundation for next-generation VR systems that are equitable,\neffective, and accessible for sign language learners worldwide.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e8655\u7bc7VR\u5728\u624b\u8bed\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u7814\u7a76\uff0c\u603b\u7ed3\u4e86\u4e94\u7c7b\u4e3b\u9898\uff0c\u5206\u6790\u4e86AI\u548cVR\u6280\u672f\u7684\u4f18\u52bf\u4e0e\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u624b\u8bed\u6559\u5b66\u4e2d\u5e08\u8d44\u4e0d\u8db3\u3001\u65e9\u671f\u63a5\u89e6\u5c11\u53ca\u4f20\u7edf\u65b9\u6cd5\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u8ba8VR\u548cAI\u6280\u672f\u7684\u6f5c\u5728\u5e94\u7528\u3002", "method": "\u7cfb\u7edf\u6027\u56de\u987e55\u7bc7\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\uff0c\u5206\u6790\u4e94\u4e2a\u6838\u5fc3\u4e3b\u9898\u3002", "result": "AI\u9a71\u52a8\u7684VR\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u80fd\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff0c\u4f46\u5b58\u5728\u786c\u4ef6\u9650\u5236\u548c\u8bbe\u8ba1\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765VR\u7cfb\u7edf\u9700\u7ed3\u5408\u6280\u672f\u53d1\u5c55\u4e0e\u5305\u5bb9\u6027\u6559\u5b66\uff0c\u4ee5\u670d\u52a1\u5168\u7403\u624b\u8bed\u5b66\u4e60\u8005\u3002"}}
{"id": "2508.18106", "pdf": "https://arxiv.org/pdf/2508.18106", "abs": "https://arxiv.org/abs/2508.18106", "authors": ["Keke Lian", "Bin Wang", "Lei Zhang", "Libo Chen", "Junjie Wang", "Ziming Zhao", "Yujiu Yang", "Haotong Duan", "Haoran Zhao", "Shuang Liao", "Mingda Guo", "Jiazheng Quan", "Yilu Zhong", "Chenhao He", "Zichuan Chen", "Jie Wu", "Haoling Li", "Zhaoxuan Li", "Jiongchi Yu", "Hui Li", "Dong Zhang"], "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86A.S.E\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u6d4b\u7684\u4e0d\u8db3\uff0c\u5e76\u6d4b\u8bd5\u4e86\u9886\u5148\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u5728\u4ee3\u7801\u751f\u6210\u5b89\u5168\u6027\u8bc4\u4f30\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5982\u5ffd\u7565\u4ed3\u5e93\u7ea7\u4e0a\u4e0b\u6587\u3001\u7f3a\u4e4f\u53ef\u91cd\u590d\u6027\uff0c\u672a\u80fd\u5173\u8054\u8f93\u5165\u4e0e\u8f93\u51fa\u5b89\u5168\u3002", "method": "\u63d0\u51faA.S.E\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u4ed3\u5e93\u548cCVE\u6784\u5efa\u4efb\u52a1\uff0c\u4fdd\u7559\u5b8c\u6574\u4e0a\u4e0b\u6587\uff0c\u4f7f\u7528\u5bb9\u5668\u5316\u6846\u67b6\u548c\u4e13\u5bb6\u89c4\u5219\u8fdb\u884c\u7a33\u5b9a\u8bc4\u4f30\u3002", "result": "Claude-3.7-Sonnet\u8868\u73b0\u6700\u4f73\uff0c\u5f00\u6e90\u4e0e\u4e13\u6709\u6a21\u578b\u5b89\u5168\u5dee\u8ddd\u7a84\uff0c\u7b80\u660e\u89e3\u7801\u7b56\u7565\u5728\u5b89\u5168\u8865\u4e01\u4e0a\u4f18\u4e8e\u590d\u6742\u63a8\u7406\u3002", "conclusion": "A.S.E\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u6d4b\u7684\u7a7a\u767d\uff0c\u7b80\u660e\u7b56\u7565\u5728\u4ee3\u7801\u5b89\u5168\u751f\u6210\u4e2d\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2508.17460", "pdf": "https://arxiv.org/pdf/2508.17460", "abs": "https://arxiv.org/abs/2508.17460", "authors": ["Arran Zeyu Wang", "Ghulam Jilani Quadri", "Mengyuan Zhu", "Chin Tseng", "Danielle Albers Szafir"], "title": "Characterizing Visualization Perception with Psychological Phenomena: Uncovering the Role of Subitizing in Data Visualization", "categories": ["cs.HC"], "comment": null, "summary": "Understanding how people perceive visualizations is crucial for designing\neffective visual data representations; however, many heuristic design\nguidelines are derived from specific tasks or visualization types, without\nconsidering the constraints or conditions under which those guidelines hold. In\nthis work, we aimed to assess existing design heuristics for categorical\nvisualization using well-established psychological knowledge. Specifically, we\nexamine the impact of the subitizing phenomenon in cognitive psychology --\npeople's ability to automatically recognize a small set of objects instantly\nwithout counting -- in data visualizations. We conducted three experiments with\nmulti-class scatterplots -- between 2 and 15 classes with varying design\nchoices -- across three different tasks -- class estimation, correlation\ncomparison, and clustering judgments -- to understand how performance changes\nas the number of classes (and therefore set size) increases. Our results\nindicate if the category number is smaller than six, people tend to perform\nwell at all tasks, providing empirical evidence of subitizing in visualization.\nWhen category numbers increased, performance fell, with the magnitude of the\nperformance change depending on task and encoding. Our study bridges the gap\nbetween heuristic guidelines and empirical evidence by applying\nwell-established psychological theories, suggesting future opportunities for\nusing psychological theories and constructs to characterize visualization\nperception.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u73b0\u6709\u5206\u7c7b\u53ef\u89c6\u5316\u8bbe\u8ba1\u542f\u53d1\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5feb\u901f\u8bc6\u522b\u5c11\u91cf\u5bf9\u8c61\u7684\u8ba4\u77e5\u73b0\u8c61\u5728\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u5e94\u7528\u3002\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u53d1\u73b0\uff0c\u7c7b\u522b\u6570\u5c0f\u4e8e\u516d\u65f6\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u8bc1\u5b9e\u4e86\u53ef\u89c6\u5316\u4e2d\u7684\u5feb\u901f\u8bc6\u522b\u73b0\u8c61\u3002", "motivation": "\u73b0\u6709\u53ef\u89c6\u5316\u8bbe\u8ba1\u542f\u53d1\u6cd5\u591a\u57fa\u4e8e\u7279\u5b9a\u4efb\u52a1\u6216\u7c7b\u578b\uff0c\u7f3a\u4e4f\u666e\u904d\u6027\u3002\u7814\u7a76\u65e8\u901a\u8fc7\u5fc3\u7406\u5b66\u7406\u8bba\uff08\u5982\u5feb\u901f\u8bc6\u522b\u73b0\u8c61\uff09\u8bc4\u4f30\u8fd9\u4e9b\u542f\u53d1\u6cd5\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\uff0c\u4f7f\u7528\u591a\u7c7b\u6563\u70b9\u56fe\uff082\u81f315\u7c7b\uff09\u548c\u4e09\u79cd\u4e0d\u540c\u4efb\u52a1\uff08\u7c7b\u522b\u4f30\u8ba1\u3001\u76f8\u5173\u6027\u6bd4\u8f83\u3001\u805a\u7c7b\u5224\u65ad\uff09\uff0c\u7814\u7a76\u7c7b\u522b\u6570\u589e\u52a0\u65f6\u4efb\u52a1\u8868\u73b0\u7684\u53d8\u5316\u3002", "result": "\u7c7b\u522b\u6570\u5c0f\u4e8e\u516d\u65f6\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u652f\u6301\u5feb\u901f\u8bc6\u522b\u73b0\u8c61\uff1b\u7c7b\u522b\u6570\u589e\u52a0\u65f6\u8868\u73b0\u4e0b\u964d\uff0c\u4e0b\u964d\u5e45\u5ea6\u53d6\u51b3\u4e8e\u4efb\u52a1\u548c\u7f16\u7801\u65b9\u5f0f\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u542f\u53d1\u6cd5\u548c\u5b9e\u8bc1\u8bc1\u636e\u95f4\u7684\u7a7a\u767d\uff0c\u5efa\u8bae\u672a\u6765\u5229\u7528\u5fc3\u7406\u5b66\u7406\u8bba\u8fdb\u4e00\u6b65\u7814\u7a76\u53ef\u89c6\u5316\u611f\u77e5\u3002"}}
{"id": "2508.16625", "pdf": "https://arxiv.org/pdf/2508.16625", "abs": "https://arxiv.org/abs/2508.16625", "authors": ["Rijha Safdar", "Danyail Mateen", "Syed Taha Ali", "M. Umer Ashfaq", "Wajahat Hussain"], "title": "Data and Context Matter: Towards Generalizing AI-based Software Vulnerability Detection", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "The performance of AI-based software vulnerability detection systems is often\nlimited by their poor generalization to unknown codebases. In this research, we\nexplore the impact of data quality and model architecture on the\ngeneralizability of vulnerability detection systems. By generalization we mean\nability of high vulnerability detection performance across different C/C++\nsoftware projects not seen during training. Through a series of experiments, we\ndemonstrate that improvements in dataset diversity and quality substantially\nenhance detection performance. Additionally, we compare multiple encoder-only\nand decoder-only models, finding that encoder based models outperform in terms\nof accuracy and generalization. Our model achieves 6.8% improvement in recall\non the benchmark BigVul[1] dataset, also outperforming on unseen projects,\nhence showing enhanced generalizability. These results highlight the role of\ndata quality and model selection in the development of robust vulnerability\ndetection systems. Our findings suggest a direction for future systems having\nhigh cross-project effectiveness.", "AI": {"tldr": "AI\u6f0f\u6d1e\u68c0\u6d4b\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\u53d7\u9650\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u67b6\u6784\u5bf9\u5176\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u6570\u636e\u591a\u6837\u6027\u548c\u6a21\u578b\u9009\u62e9\uff08\u7f16\u7801\u5668\u4f18\u4e8e\u89e3\u7801\u5668\uff09\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3AI\u6f0f\u6d1e\u68c0\u6d4b\u7cfb\u7edf\u5728\u4e0d\u540c\u4ee3\u7801\u5e93\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\u548c\u4e0d\u540c\u6a21\u578b\u67b6\u6784\uff08\u7f16\u7801\u5668\u4e0e\u89e3\u7801\u5668\uff09\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u6570\u636e\u8d28\u91cf\u548c\u7f16\u7801\u5668\u6a21\u578b\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u6a21\u578b\u5728BigVul\u6570\u636e\u96c6\u4e0a\u53ec\u56de\u7387\u63d0\u53476.8%\uff0c\u4e14\u5728\u672a\u89c1\u9879\u76ee\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u6570\u636e\u8d28\u91cf\u548c\u6a21\u578b\u9009\u62e9\u662f\u5f00\u53d1\u9c81\u68d2\u6f0f\u6d1e\u68c0\u6d4b\u7cfb\u7edf\u7684\u5173\u952e\uff0c\u4e3a\u672a\u6765\u7cfb\u7edf\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2508.17474", "pdf": "https://arxiv.org/pdf/2508.17474", "abs": "https://arxiv.org/abs/2508.17474", "authors": ["Arran Zeyu Wang", "David Borland", "David Gotz"], "title": "Visual Analytics for Causal Reasoning from Real-World Health Data", "categories": ["cs.HC"], "comment": null, "summary": "The increasing capture and analysis of large-scale longitudinal health data\noffer opportunities to improve healthcare and advance medical understanding.\nHowever, a critical gap exists between (a) -- the observation of patterns and\ncorrelations, versus (b) -- the understanding of true causal mechanisms that\ndrive outcomes. An accurate understanding of the underlying mechanisms that\ncause various changes in medical status is crucial for decision-makers across\nvarious healthcare domains and roles, yet inferring causality from real-world\nobservational data is difficult for both methodological and practical\nchallenges. This Grand Challenge advocates increased Visual Analytics (VA)\nresearch on this topic to empower people with the tool for sound causal\nreasoning from health data. We note this is complicated by the complex nature\nof medical data -- the volume, variety, sparsity, and temporality of health\ndata streams make the use of causal inference algorithms difficult. Combined\nwith challenges imposed by the realities of health-focused settings, including\ntime constraints and traditional medical work practices, existing causal\nreasoning approaches are valuable but insufficient. We argue that advances in\nresearch can lead to new VA tools that augment human expertise with intuitive\nand robust causal inference capabilities, which can help realize a new paradigm\nof data-driven, causality-aware healthcare practices that improve human health\noutcomes.", "AI": {"tldr": "\u8bba\u6587\u547c\u5401\u901a\u8fc7\u89c6\u89c9\u5206\u6790\u5de5\u5177\u6539\u8fdb\u4ece\u5065\u5eb7\u6570\u636e\u4e2d\u8fdb\u884c\u56e0\u679c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u5728\u89c2\u5bdf\u6570\u636e\u4e2d\u8bc6\u522b\u771f\u5b9e\u56e0\u679c\u5173\u7cfb\u7684\u65b9\u6cd5\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u7eb5\u5411\u5065\u5eb7\u6570\u636e\u7684\u5206\u6790\u4e3a\u533b\u7597\u6539\u8fdb\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u5f53\u524d\u7684\u89c2\u5bdf\u6027\u6570\u636e\u96be\u4ee5\u63ed\u793a\u771f\u5b9e\u7684\u56e0\u679c\u673a\u5236\uff0c\u8fd9\u5bf9\u533b\u7597\u51b3\u7b56\u8005\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u89c6\u89c9\u5206\u6790\uff08VA\uff09\u7814\u7a76\uff0c\u5f00\u53d1\u80fd\u591f\u589e\u5f3a\u4eba\u7c7b\u4e13\u5bb6\u76f4\u89c9\u56e0\u679c\u63a8\u7406\u80fd\u529b\u7684\u65b0\u5de5\u5177\u3002", "result": "\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u7b97\u6cd5\u4e0e\u76f4\u89c2\u7684\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u53ef\u4ee5\u514b\u670d\u533b\u7597\u6570\u636e\u7684\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u6570\u636e\u9a71\u52a8\u533b\u7597\u5b9e\u8df5\u3002", "conclusion": "\u672a\u6765\u7684\u89c6\u89c9\u5206\u6790\u5de5\u5177\u6709\u671b\u652f\u6301\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u6539\u5584\u533b\u7597\u51b3\u7b56\u548c\u5065\u5eb7\u7ed3\u679c\u3002"}}
{"id": "2508.16662", "pdf": "https://arxiv.org/pdf/2508.16662", "abs": "https://arxiv.org/abs/2508.16662", "authors": ["Alexander Tabalipa"], "title": "Bridging the Mobile Trust Gap: A Zero Trust Framework for Consumer-Facing Applications", "categories": ["cs.CR", "cs.CY", "cs.NI", "cs.SE", "K.6.5; C.2.0; D.4.6"], "comment": "43 pages, 5 figures, 9 tables. Working Paper - Version 1.0. Submitted\n  under a CC BY-SA 4.0 license. Also available as an SSRN Working Paper.\n  Feedback and collaboration are welcome", "summary": "Zero Trust Architecture (ZTA) has become a widely adopted model for securing\nenterprise environments, promoting continuous verification and minimal trust\nacross systems. However, its application in mobile contexts remains limited,\ndespite mobile applications now accounting for most global digital interactions\nand being increasingly targeted by sophisticated threats. Existing Zero Trust\nframeworks developed by organisations such as the National Institute of\nStandards and Technology (NIST) and the Cybersecurity and Infrastructure\nSecurity Agency (CISA) primarily focus on enterprise-managed infrastructure,\nassuming organisational control over devices, networks, and identities. This\npaper addresses a critical gap by proposing an extended Zero Trust model\ndesigned for mobile applications operating in untrusted, user-controlled\nenvironments. Using a design science methodology, the study introduced a\nsix-pillar framework that supports runtime enforcement of trust through\ncontrols including device integrity, user identity validation, data protection,\nsecure application programming interface (API) usage, behavioural monitoring,\nand live application protection. Each pillar was mapped to relevant regulatory\nand security standards to support compliance. A phased implementation roadmap\nand maturity assessment model were also developed to guide adoption across\nvarying organisational contexts. The proposed model offers a practical and\nstandards-aligned approach to securing mobile applications beyond\npre-deployment controls, aligning real-time enforcement with Zero Trust\nprinciples. This contribution expands the operational boundaries of ZTA and\nprovides organisations with a deployable path to reduce fraud, enhance\ncompliance, and address emerging mobile security challenges. Future research\nmay include empirical validation of the framework and cross-sector application\ntesting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u79fb\u52a8\u5e94\u7528\u7684\u6269\u5c55\u96f6\u4fe1\u4efb\u6a21\u578b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u96f6\u4fe1\u4efb\u67b6\u6784\u5728\u79fb\u52a8\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u96f6\u4fe1\u4efb\u67b6\u6784\uff08ZTA\uff09\u4e3b\u8981\u5173\u6ce8\u4f01\u4e1a\u7ba1\u7406\u7684\u56fa\u5b9a\u57fa\u7840\u8bbe\u65bd\uff0c\u800c\u79fb\u52a8\u5e94\u7528\u5728\u4e0d\u53ef\u4fe1\u7684\u7528\u6237\u63a7\u5236\u73af\u5883\u4e2d\u9762\u4e34\u65e5\u76ca\u590d\u6742\u7684\u5a01\u80c1\uff0c\u4e9f\u9700\u9488\u5bf9\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u516d\u652f\u67f1\u6846\u67b6\uff0c\u5305\u62ec\u8bbe\u5907\u5b8c\u6574\u6027\u3001\u7528\u6237\u8eab\u4efd\u9a8c\u8bc1\u3001\u6570\u636e\u4fdd\u62a4\u3001API\u5b89\u5168\u4f7f\u7528\u3001\u884c\u4e3a\u76d1\u63a7\u548c\u5b9e\u65f6\u5e94\u7528\u4fdd\u62a4\uff0c\u5e76\u5236\u5b9a\u4e86\u5206\u9636\u6bb5\u5b9e\u65bd\u8def\u7ebf\u56fe\u548c\u6210\u719f\u5ea6\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u79fb\u52a8\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u65f6\u7684\u96f6\u4fe1\u4efb\u539f\u5219\u6267\u884c\u65b9\u6cd5\uff0c\u652f\u6301\u5408\u89c4\u6027\uff0c\u5e76\u964d\u4f4e\u4e86\u6b3a\u8bc8\u98ce\u9669\u3002", "conclusion": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86ZTA\u7684\u9002\u7528\u8303\u56f4\uff0c\u4e3a\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u79fb\u52a8\u5b89\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u884c\u6846\u67b6\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u548c\u8de8\u884c\u4e1a\u5e94\u7528\u6d4b\u8bd5\u3002"}}
{"id": "2508.17597", "pdf": "https://arxiv.org/pdf/2508.17597", "abs": "https://arxiv.org/abs/2508.17597", "authors": ["Jaewook Lee", "Davin Win Kyi", "Leejun Kim", "Jenny Peng", "Gagyeom Lim", "Jeremy Zhengqi Huang", "Dhruv Jain", "Jon E. Froehlich"], "title": "SonoCraftAR: Towards Supporting Personalized Authoring of Sound-Reactive AR Interfaces by Deaf and Hard of Hearing Users", "categories": ["cs.HC"], "comment": null, "summary": "Augmented reality (AR) has shown promise for supporting Deaf and\nhard-of-hearing (DHH) individuals by captioning speech and visualizing\nenvironmental sounds, yet existing systems do not allow users to create\npersonalized sound visualizations. We present SonoCraftAR, a proof-of-concept\nprototype that empowers DHH users to author custom sound-reactive AR interfaces\nusing typed natural language input. SonoCraftAR integrates real-time audio\nsignal processing with a multi-agent LLM pipeline that procedurally generates\nanimated 2D interfaces via a vector graphics library. The system extracts the\ndominant frequency of incoming audio and maps it to visual properties such as\nsize and color, making the visualizations respond dynamically to sound. This\nearly exploration demonstrates the feasibility of open-ended sound-reactive AR\ninterface authoring and discusses future opportunities for personalized,\nAI-assisted tools to improve sound accessibility.", "AI": {"tldr": "SonoCraftAR\u662f\u4e00\u4e2a\u652f\u6301\u804b\u4eba\u548c\u542c\u529b\u969c\u788d\u8005\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u521b\u5efa\u4e2a\u6027\u5316\u58f0\u97f3\u53ef\u89c6\u5316AR\u754c\u9762\u7684\u539f\u578b\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dAR\u7cfb\u7edf\u65e0\u6cd5\u6ee1\u8db3\u804b\u4eba\u548c\u542c\u529b\u969c\u788d\u8005\u5bf9\u4e2a\u6027\u5316\u58f0\u97f3\u53ef\u89c6\u5316\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u5b9e\u65f6\u97f3\u9891\u4fe1\u53f7\u5904\u7406\u548c\u591a\u667a\u80fd\u4f53LLM\u7ba1\u9053\uff0c\u901a\u8fc7\u77e2\u91cf\u56fe\u5f62\u5e93\u751f\u6210\u52a8\u60012D\u754c\u9762\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u97f3\u9891\u4e3b\u9891\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u5c5e\u6027\uff08\u5982\u5927\u5c0f\u548c\u989c\u8272\uff09\uff0c\u5b9e\u73b0\u58f0\u97f3\u7684\u52a8\u6001\u54cd\u5e94\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u5f00\u653e\u5f0f\u7684AR\u754c\u9762\u521b\u4f5c\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u671b\u4e86AI\u8f85\u52a9\u5de5\u5177\u5728\u58f0\u97f3\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.17676", "pdf": "https://arxiv.org/pdf/2508.17676", "abs": "https://arxiv.org/abs/2508.17676", "authors": ["Zhongyi Bai", "Jens Emil Gr\u00f8nb\u00e6k", "Andrew Irlitti", "Jarrod Knibbe", "Eduardo Velloso"], "title": "I Can't Join, But I Will Send My Agent: Stand-in Enhanced Asynchronous Meetings (SEAM)", "categories": ["cs.HC"], "comment": null, "summary": "We propose and explore the user experience of SEAM -- Stand-in Enhanced\nAsynchronous Meetings -- virtual reality meetings in which embodied virtual\nagents represent absent users. During the meeting, attendees can address the\nagent, and the absent user can later watch the recording from its perspective\nto respond. Through two mixed-method studies with 45 participants using the\nWizard-of-Oz approach, we explored both the perspectives of the attendees in\nthe original meeting and of the absent users later re-watching the meeting. We\nfound that the stand-in can enhance meetings, benefiting both present and\nabsent collaborators. Present attendees can easily access information that\ndrives decision-making in the meeting perceive high social presence of\nabsentees. Absentees also felt included when watching recordings because of the\nsocial interactions and attention towards them. Our contributions demonstrate a\nproof of concept for future asynchronous meetings in which collaborators can\ninteract conversationally more akin to how they would if it had been\nsynchronous.", "AI": {"tldr": "SEAM\uff08\u66ff\u4ee3\u8005\u589e\u5f3a\u5f02\u6b65\u4f1a\u8bae\uff09\u662f\u4e00\u79cd\u865a\u62df\u73b0\u5b9e\u4f1a\u8bae\uff0c\u901a\u8fc7\u865a\u62df\u4ee3\u7406\u4ee3\u8868\u7f3a\u5e2d\u7528\u6237\uff0c\u589e\u5f3a\u4f1a\u8bae\u4f53\u9a8c\u3002\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6280\u672f\u5bf9\u4e0e\u4f1a\u8005\u548c\u7f3a\u5e2d\u8005\u5747\u6709\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u63d0\u5347\u5f02\u6b65\u4f1a\u8bae\u7684\u534f\u4f5c\u4f53\u9a8c\uff0c\u4f7f\u4e0e\u4f1a\u8005\u548c\u7f3a\u5e2d\u8005\u90fd\u80fd\u611f\u53d7\u5230\u66f4\u5f3a\u7684\u793e\u4ea4\u5b58\u5728\u611f\u548c\u53c2\u4e0e\u611f\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\uff0c\u901a\u8fc7Wizard-of-Oz\u65b9\u6cd5\u4e0e45\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5206\u6790\u4f1a\u8bae\u4e2d\u548c\u7f3a\u5e2d\u8005\u56de\u653e\u65f6\u7684\u4f53\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u865a\u62df\u4ee3\u7406\u80fd\u63d0\u5347\u4f1a\u8bae\u6548\u7387\uff0c\u589e\u5f3a\u793e\u4ea4\u4e92\u52a8\uff0c\u4f7f\u7f3a\u5e2d\u8005\u611f\u89c9\u88ab\u5305\u5bb9\u3002", "conclusion": "SEAM\u4e3a\u672a\u6765\u5f02\u6b65\u4f1a\u8bae\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u534f\u4f5c\u4f53\u9a8c\u66f4\u63a5\u8fd1\u540c\u6b65\u4f1a\u8bae\u3002"}}
{"id": "2508.17856", "pdf": "https://arxiv.org/pdf/2508.17856", "abs": "https://arxiv.org/abs/2508.17856", "authors": ["Tiezhu Sun", "Marco Alecci", "Aleksandr Pilgun", "Yewei Song", "Xunzhu Tang", "Jordan Samhi", "Tegawend\u00e9 F. Bissyand\u00e9", "Jacques Klein"], "title": "MalLoc: Toward Fine-grained Android Malicious Payload Localization via LLMs", "categories": ["cs.CR", "cs.SE"], "comment": "Accepted at ICSME 2025, NIER Track", "summary": "The rapid evolution of Android malware poses significant challenges to the\nmaintenance and security of mobile applications (apps). Traditional detection\ntechniques often struggle to keep pace with emerging malware variants that\nemploy advanced tactics such as code obfuscation and dynamic behavior\ntriggering. One major limitation of these approaches is their inability to\nlocalize malicious payloads at a fine-grained level, hindering precise\nunderstanding of malicious behavior. This gap in understanding makes the design\nof effective and targeted mitigation strategies difficult, leaving mobile apps\nvulnerable to continuously evolving threats.\n  To address this gap, we propose MalLoc, a novel approach that leverages the\ncode understanding capabilities of large language models (LLMs) to localize\nmalicious payloads at a fine-grained level within Android malware. Our\nexperimental results demonstrate the feasibility and effectiveness of using\nLLMs for this task, highlighting the potential of MalLoc to enhance precision\nand interpretability in malware analysis. This work advances beyond traditional\ndetection and classification by enabling deeper insights into behavior-level\nmalicious logic and opens new directions for research, including dynamic\nmodeling of localized threats and targeted countermeasure development.", "AI": {"tldr": "\u63d0\u51faMalLoc\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7cbe\u7ec6\u5b9a\u4f4d\u5b89\u5353\u6076\u610f\u8f6f\u4ef6\u4e2d\u7684\u6076\u610f\u8f7d\u8377\uff0c\u63d0\u5347\u5206\u6790\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u6280\u672f\u96be\u4ee5\u5e94\u5bf9\u5feb\u901f\u6f14\u53d8\u7684\u5b89\u5353\u6076\u610f\u8f6f\u4ef6\uff0c\u5c24\u5176\u662f\u4ee3\u7801\u6df7\u6dc6\u548c\u52a8\u6001\u884c\u4e3a\u89e6\u53d1\u7b49\u9ad8\u7ea7\u624b\u6cd5\uff0c\u65e0\u6cd5\u7cbe\u786e\u5b9a\u4f4d\u6076\u610f\u8f7d\u8377\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7801\u7406\u89e3\u80fd\u529b\uff0c\u5f00\u53d1MalLoc\u65b9\u6cd5\uff0c\u7cbe\u7ec6\u5b9a\u4f4d\u6076\u610f\u8f7d\u8377\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMalLoc\u53ef\u884c\u4e14\u6709\u6548\uff0c\u63d0\u5347\u4e86\u6076\u610f\u8f6f\u4ef6\u5206\u6790\u7684\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "MalLoc\u8d85\u8d8a\u4e86\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u884c\u4e3a\u7ea7\u6076\u610f\u903b\u8f91\u5206\u6790\u548c\u9488\u5bf9\u6027\u9632\u5fa1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.17880", "pdf": "https://arxiv.org/pdf/2508.17880", "abs": "https://arxiv.org/abs/2508.17880", "authors": ["Aditi Bhalla", "Christian Hellert", "Enkelejda Kasneci", "Nastassja Becker"], "title": "TRUCE-AV: A Multimodal dataset for Trust and Comfort Estimation in Autonomous Vehicles", "categories": ["cs.HC"], "comment": null, "summary": "Understanding and estimating driver trust and comfort are essential for the\nsafety and widespread acceptance of autonomous vehicles. Existing works analyze\nuser trust and comfort separately, with limited real-time assessment and\ninsufficient multimodal data. This paper introduces a novel multimodal dataset\ncalled TRUCE-AV, focusing on trust and comfort estimation in autonomous\nvehicles. The dataset collects real-time trust votes and continuous comfort\nratings of 31 participants during a simulator-based fully autonomous driving.\nSimultaneously, physiological signals, such as heart rate, gaze, and emotions,\nalong with environmental data (e.g., vehicle speed, nearby vehicle positions,\nand velocity), are recorded throughout the drives. Standard pre- and post-drive\nquestionnaires were also administered to assess participants' trust in\nautomation and overall well-being, enabling the correlation of subjective\nassessments with real-time responses. To demonstrate the utility of our\ndataset, we evaluated various machine learning models for trust and comfort\nestimation using physiological data. Our analysis showed that tree-based models\nlike Random Forest and XGBoost and non-linear models such as KNN and MLP\nregressor achieved the best performance for trust classification and comfort\nregression. Additionally, we identified key features that contribute to these\nestimations by using SHAP analysis on the top-performing models. Our dataset\nenables the development of adaptive AV systems capable of dynamically\nresponding to user trust and comfort levels non-invasively, ultimately\nenhancing safety, user experience, and human-centered vehicle design.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TRUCE-AV\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b9e\u65f6\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7528\u6237\u7684\u4fe1\u4efb\u4e0e\u8212\u9002\u5ea6\uff0c\u7ed3\u5408\u751f\u7406\u4fe1\u53f7\u548c\u73af\u5883\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u5e7f\u6cdb\u63a5\u53d7\u9700\u8981\u7406\u89e3\u7528\u6237\u7684\u4fe1\u4efb\u4e0e\u8212\u9002\u5ea6\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5b9e\u65f6\u8bc4\u4f30\u548c\u591a\u6a21\u6001\u6570\u636e\u652f\u6301\u3002", "method": "\u91c7\u96c631\u540d\u53c2\u4e0e\u8005\u5728\u6a21\u62df\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5b9e\u65f6\u4fe1\u4efb\u8bc4\u5206\u3001\u8212\u9002\u5ea6\u8bc4\u7ea7\u3001\u751f\u7406\u4fe1\u53f7\uff08\u5982\u5fc3\u7387\u3001\u51dd\u89c6\u3001\u60c5\u7eea\uff09\u548c\u73af\u5883\u6570\u636e\uff08\u5982\u8f66\u901f\u3001\u5468\u56f4\u8f66\u8f86\u4f4d\u7f6e\uff09\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6811\u57fa\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\u3001XGBoost\uff09\u548c\u975e\u7ebf\u6027\u6a21\u578b\uff08\u5982KNN\u3001MLP\u56de\u5f52\u5668\uff09\u5728\u4fe1\u4efb\u5206\u7c7b\u548c\u8212\u9002\u5ea6\u56de\u5f52\u4e2d\u8868\u73b0\u6700\u4f73\uff0cSHAP\u5206\u6790\u8bc6\u522b\u4e86\u5173\u952e\u7279\u5f81\u3002", "conclusion": "TRUCE-AV\u6570\u636e\u96c6\u652f\u6301\u5f00\u53d1\u81ea\u9002\u5e94\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u52a8\u6001\u54cd\u5e94\u7528\u6237\u4fe1\u4efb\u4e0e\u8212\u9002\u5ea6\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.18188", "pdf": "https://arxiv.org/pdf/2508.18188", "abs": "https://arxiv.org/abs/2508.18188", "authors": ["Neo Christopher Chung", "Jakub Binda"], "title": "Explain and Monitor Deep Learning Models for Computer Vision using Obz AI", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.SE"], "comment": null, "summary": "Deep learning has transformed computer vision (CV), achieving outstanding\nperformance in classification, segmentation, and related tasks. Such AI-based\nCV systems are becoming prevalent, with applications spanning from medical\nimaging to surveillance. State of the art models such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs) are often regarded as ``black\nboxes,'' offering limited transparency into their decision-making processes.\nDespite a recent advancement in explainable AI (XAI), explainability remains\nunderutilized in practical CV deployments. A primary obstacle is the absence of\nintegrated software solutions that connect XAI techniques with robust knowledge\nmanagement and monitoring frameworks. To close this gap, we have developed Obz\nAI, a comprehensive software ecosystem designed to facilitate state-of-the-art\nexplainability and observability for vision AI systems. Obz AI provides a\nseamless integration pipeline, from a Python client library to a full-stack\nanalytics dashboard. With Obz AI, a machine learning engineer can easily\nincorporate advanced XAI methodologies, extract and analyze features for\noutlier detection, and continuously monitor AI models in real time. By making\nthe decision-making mechanisms of deep models interpretable, Obz AI promotes\nobservability and responsible deployment of computer vision systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Obz AI\uff0c\u4e00\u4e2a\u96c6\u6210\u89e3\u91ca\u6027AI\uff08XAI\uff09\u4e0e\u77e5\u8bc6\u7ba1\u7406\u548c\u76d1\u63a7\u6846\u67b6\u7684\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u5347\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89c2\u5bdf\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5982CNN\u548cViT\u88ab\u89c6\u4e3a\u201c\u9ed1\u76d2\u201d\uff0c\u7f3a\u4e4f\u5bf9\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u900f\u660e\u6027\uff0c\u540c\u65f6XAI\u5728\u5b9e\u9645\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u5f00\u53d1\u4e86Obz AI\uff0c\u63d0\u4f9b\u4ecePython\u5ba2\u6237\u7aef\u5e93\u5230\u5168\u6808\u5206\u6790\u4eea\u8868\u677f\u7684\u65e0\u7f1d\u96c6\u6210\u7ba1\u9053\uff0c\u652f\u6301XAI\u65b9\u6cd5\u3001\u7279\u5f81\u63d0\u53d6\u5206\u6790\u548c\u5b9e\u65f6\u6a21\u578b\u76d1\u63a7\u3002", "result": "Obz AI\u4f7f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u51b3\u7b56\u673a\u5236\u53ef\u89e3\u91ca\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u7684\u53ef\u89c2\u5bdf\u6027\u548c\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002", "conclusion": "Obz AI\u586b\u8865\u4e86XAI\u6280\u672f\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u8f6f\u4ef6\u96c6\u6210\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u7684\u900f\u660e\u5316\u548c\u53ef\u76d1\u63a7\u6027\u3002"}}
{"id": "2508.17962", "pdf": "https://arxiv.org/pdf/2508.17962", "abs": "https://arxiv.org/abs/2508.17962", "authors": ["Sana Athar", "Devashish Gosain", "Anja Feldmann", "Mannat Kaur", "Ha Dao"], "title": "\"Nobody should control the end user\": Exploring Privacy Perspectives of Indian Internet Users in Light of DPDPA", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "With the rapid increase in online interactions, concerns over data privacy\nand transparency of data processing practices have become more pronounced.\nWhile regulations like the GDPR have driven the widespread adoption of cookie\nbanners in the EU, India's Digital Personal Data Protection Act (DPDPA)\npromises similar changes domestically, aiming to introduce a framework for data\nprotection. However, certain clauses within the DPDPA raise concerns about\npotential infringements on user privacy, given the exemptions for government\naccountability and user consent requirements. In this study, for the first\ntime, we explore Indian Internet users' awareness and perceptions of cookie\nbanners, online privacy, and privacy regulations, especially in light of the\nnewly passed DPDPA. We conducted an online anonymous survey with 428 Indian\nparticipants, which addressed: (1) users' perspectives on cookie banners, (2)\ntheir attitudes towards online privacy and privacy regulations, and (3) their\nacceptance of 10 contentious DPDPA clauses that favor state authorities and may\nenable surveillance. Our findings reveal that privacy-conscious users often\nlack consistent awareness of privacy mechanisms, and their concerns do not\nalways lead to protective actions. Our thematic analysis of 143 open ended\nresponses shows that users' privacy and data protection concerns are rooted in\nskepticism towards the government, shaping their perceptions of the DPDPA and\nfueling demands for policy revisions. Our study highlights the need for clearer\ncommunication regarding the DPDPA, user-centric consent mechanisms, and policy\nrefinements to enhance data privacy practices in India.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u5370\u5ea6\u4e92\u8054\u7f51\u7528\u6237\u5bf9Cookie\u6a2a\u5e45\u3001\u5728\u7ebf\u9690\u79c1\u53ca\u9690\u79c1\u6cd5\u89c4\uff08\u5c24\u5176\u662f\u65b0\u901a\u8fc7\u7684DPDPA\uff09\u7684\u8ba4\u77e5\u4e0e\u6001\u5ea6\uff0c\u53d1\u73b0\u9690\u79c1\u610f\u8bc6\u5f3a\u7684\u7528\u6237\u5e38\u7f3a\u4e4f\u4e00\u81f4\u7684\u9690\u79c1\u673a\u5236\u4e86\u89e3\uff0c\u4e14\u62c5\u5fe7\u672a\u8f6c\u5316\u4e3a\u884c\u52a8\u3002", "motivation": "\u968f\u7740\u5728\u7ebf\u4e92\u52a8\u7684\u589e\u52a0\uff0c\u6570\u636e\u9690\u79c1\u4e0e\u900f\u660e\u6027\u95ee\u9898\u51f8\u663e\u3002\u5370\u5ea6\u65b0\u901a\u8fc7\u7684DPDPA\u53ef\u80fd\u4fb5\u72af\u7528\u6237\u9690\u79c1\uff0c\u56e0\u6b64\u9996\u6b21\u63a2\u8ba8\u5370\u5ea6\u7528\u6237\u5bf9\u6b64\u7684\u8ba4\u77e5\u4e0e\u6001\u5ea6\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u533f\u540d\u8c03\u67e5428\u540d\u5370\u5ea6\u53c2\u4e0e\u8005\uff0c\u6db5\u76d6\u7528\u6237\u5bf9Cookie\u6a2a\u5e45\u7684\u770b\u6cd5\u3001\u5bf9\u9690\u79c1\u6cd5\u89c4\u7684\u6001\u5ea6\uff0c\u4ee5\u53ca\u5bf9DPDPA\u4e89\u8bae\u6761\u6b3e\u7684\u63a5\u53d7\u5ea6\u3002", "result": "\u9690\u79c1\u610f\u8bc6\u5f3a\u7684\u7528\u6237\u5e38\u7f3a\u4e4f\u9690\u79c1\u673a\u5236\u4e86\u89e3\uff0c\u5176\u9690\u79c1\u62c5\u5fe7\u672a\u5fc5\u5f15\u53d1\u4fdd\u62a4\u884c\u4e3a\u3002\u7528\u6237\u5bf9\u653f\u5e9c\u7684\u4e0d\u4fe1\u4efb\u5f71\u54cd\u5176\u5bf9DPDPA\u7684\u770b\u6cd5\uff0c\u63a8\u52a8\u653f\u7b56\u4fee\u8ba2\u9700\u6c42\u3002", "conclusion": "\u9700\u6539\u8fdbDPDPA\u7684\u6c9f\u901a\u3001\u4f18\u5316\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u540c\u610f\u673a\u5236\uff0c\u5e76\u5b8c\u5584\u653f\u7b56\u4ee5\u63d0\u5347\u5370\u5ea6\u7684\u6570\u636e\u9690\u79c1\u5b9e\u8df5\u3002"}}
{"id": "2508.18127", "pdf": "https://arxiv.org/pdf/2508.18127", "abs": "https://arxiv.org/abs/2508.18127", "authors": ["Zhao Ren", "Simon Pistrosch", "Buket Co\u015fkun", "Kevin Scheck", "Anton Batliner", "Bj\u00f6rn W. Schuller", "Tanja Schultz"], "title": "An Introduction to Silent Paralinguistics", "categories": ["cs.HC"], "comment": "21 pages", "summary": "The ability to speak is an inherent part of human nature and fundamental to\nour existence as a social species. Unfortunately, this ability can be\nrestricted in certain situations, such as for individuals who have lost their\nvoice or in environments where speaking aloud is unsuitable. Additionally, some\npeople may prefer not to speak audibly due to privacy concerns. For such cases,\nsilent speech interfaces have been proposed, which focus on processing\nbiosignals corresponding to silently produced speech. These interfaces enable\nsynthesis of audible speech from biosignals that are produced when speaking\nsilently and recognition aka decoding of biosignals into text that corresponds\nto the silently produced speech. While recognition and synthesis of silent\nspeech has been a prominent focus in many research studies, there is a\nsignificant gap in deriving paralinguistic information such as affective states\nfrom silent speech. To fill this gap, we propose Silent Paralinguistics, aiming\nto predict paralinguistic information from silent speech and ultimately\nintegrate it into the reconstructed audible voice for natural communication.\nThis survey provides a comprehensive look at methods, research strategies, and\nobjectives within the emerging field of silent paralinguistics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u4ece\u65e0\u58f0\u8bed\u97f3\u4e2d\u63d0\u53d6\u526f\u8bed\u8a00\u4fe1\u606f\uff08\u5982\u60c5\u611f\u72b6\u6001\uff09\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u91cd\u6784\u7684\u6709\u58f0\u8bed\u97f3\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4ea4\u6d41\u3002", "motivation": "\u65e0\u58f0\u8bed\u97f3\u63a5\u53e3\u5728\u5904\u7406\u65e0\u58f0\u8bed\u97f3\u65b9\u9762\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u526f\u8bed\u8a00\u4fe1\u606f\uff08\u5982\u60c5\u611f\uff09\u7684\u63d0\u53d6\u4ecd\u5b58\u5728\u7a7a\u767d\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u65e0\u58f0\u526f\u8bed\u8a00\u5b66\u201d\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u4ece\u65e0\u58f0\u8bed\u97f3\u7684\u751f\u7269\u4fe1\u53f7\u4e2d\u9884\u6d4b\u526f\u8bed\u8a00\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u6709\u58f0\u8bed\u97f3\u4e2d\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u65e0\u58f0\u526f\u8bed\u8a00\u5b66\u9886\u57df\u65b9\u6cd5\u3001\u7814\u7a76\u7b56\u7565\u548c\u76ee\u6807\u7684\u5168\u9762\u8c03\u67e5\u3002", "conclusion": "\u65e0\u58f0\u526f\u8bed\u8a00\u5b66\u7684\u7814\u7a76\u6709\u671b\u63d0\u5347\u65e0\u58f0\u8bed\u97f3\u63a5\u53e3\u7684\u81ea\u7136\u6027\u548c\u5b9e\u7528\u6027\uff0c\u586b\u8865\u73b0\u6709\u6280\u672f\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.18142", "pdf": "https://arxiv.org/pdf/2508.18142", "abs": "https://arxiv.org/abs/2508.18142", "authors": ["Tianjun Wei", "Huizhong Guo", "Yingpeng Du", "Zhu Sun", "Chen Huang", "Dongxia Wang", "Jie Zhang"], "title": "Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation", "categories": ["cs.HC", "cs.CY", "cs.IR"], "comment": "Github: https://github.com/UserMirrorer/UserMirrorer", "summary": "User simulation is increasingly vital to develop and evaluate recommender\nsystems (RSs). While Large Language Models (LLMs) offer promising avenues to\nsimulate user behavior, they often struggle with the absence of specific domain\nalignment required for RSs and the efficiency demands of large-scale\nsimulation. A vast yet underutilized resource for enhancing this alignment is\nthe extensive user feedback inherent in RSs. However, directly leveraging such\nfeedback presents two significant challenges. First, user feedback in RSs is\noften ambiguous and noisy, which negatively impacts effective preference\nalignment. Second, the massive volume of feedback largely hinders the\nefficiency of preference alignment, necessitating an efficient filtering\nmechanism to identify more informative samples. To overcome these hurdles, we\nintroduce a novel data construction framework that leverages user feedback in\nRSs with advanced LLM capabilities to generate high-quality simulation data.\nOur framework unfolds in two key phases: (1) employing LLMs to generate\ncognitive decision-making processes on constructed simulation samples, reducing\nambiguity in raw user feedback; (2) data distillation based on uncertainty\nestimation and behavior sampling to filter challenging yet denoised simulation\nsamples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using\nsuch high-quality dataset with corresponding decision-making processes.\nExtensive experiments verify that our framework significantly boosts the\nalignment with human preferences and in-domain reasoning capabilities of\nfine-tuned LLMs, and provides more insightful and interpretable signals when\ninteracting with RSs. We believe our work will advance the RS community and\noffer valuable insights for broader human-centric AI research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7528\u6237\u53cd\u9988\u548cLLM\u751f\u6210\u9ad8\u8d28\u91cf\u6a21\u62df\u6570\u636e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u964d\u4f4e\u566a\u58f0\u548c\u8fc7\u6ee4\u4fe1\u606f\u6837\u672c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u7684\u6a21\u62df\u6548\u679c\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u53cd\u9988\u901a\u5e38\u6a21\u7cca\u4e14\u566a\u58f0\u5927\uff0c\u76f4\u63a5\u5229\u7528\u6548\u7387\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6709\u6548\u5bf9\u9f50\u7528\u6237\u504f\u597d\u3002", "method": "1) \u4f7f\u7528LLM\u751f\u6210\u51b3\u7b56\u8fc7\u7a0b\u4ee5\u51cf\u5c11\u53cd\u9988\u6b67\u4e49\uff1b2) \u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u884c\u4e3a\u91c7\u6837\u8fc7\u6ee4\u566a\u58f0\uff0c\u7b5b\u9009\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LLM\u4e0e\u7528\u6237\u504f\u597d\u7684\u5bf9\u9f50\u53ca\u9886\u57df\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u7684\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63a8\u8350\u7cfb\u7edf\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\uff0c\u5e76\u4e3a\u66f4\u5e7f\u6cdb\u7684\u4eba\u7c7b\u4e2d\u5fc3AI\u7814\u7a76\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2508.18174", "pdf": "https://arxiv.org/pdf/2508.18174", "abs": "https://arxiv.org/abs/2508.18174", "authors": ["Gerile Aodeng", "Guozheng Li", "Yunshan Feng", "Qiyang Chen", "Yu Zhang", "Chi Harold Liu"], "title": "InReAcTable: LLM-Powered Interactive Visual Data Story Construction from Tabular Data", "categories": ["cs.HC"], "comment": "16 pages, 10 figures, accepted at ACM UIST 2025 (to appear)", "summary": "Insights in tabular data capture valuable patterns that help analysts\nunderstand critical information. Organizing related insights into visual data\nstories is crucial for in-depth analysis. However, constructing such stories is\nchallenging because of the complexity of the inherent relations between\nextracted insights. Users face difficulty sifting through a vast number of\ndiscrete insights to integrate specific ones into a unified narrative that\nmeets their analytical goals. Existing methods either heavily rely on user\nexpertise, making the process inefficient, or employ automated approaches that\ncannot fully capture their evolving goals. In this paper, we introduce\nInReAcTable, a framework that enhances visual data story construction by\nestablishing both structural and semantic connections between data insights.\nEach user interaction triggers the Acting module, which utilizes an insight\ngraph for structural filtering to narrow the search space, followed by the\nReasoning module using the retrieval-augmented generation method based on large\nlanguage models for semantic filtering, ultimately providing insight\nrecommendations aligned with the user's analytical intent. Based on the\nInReAcTable framework, we develop an interactive prototype system that guides\nusers to construct visual data stories aligned with their analytical\nrequirements. We conducted a case study and a user experiment to demonstrate\nthe utility and effectiveness of the InReAcTable framework and the prototype\nsystem for interactively building visual data stories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInReAcTable\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u548c\u8bed\u4e49\u8fde\u63a5\u6570\u636e\u6d1e\u5bdf\uff0c\u5e2e\u52a9\u7528\u6237\u6784\u5efa\u89c6\u89c9\u6570\u636e\u6545\u4e8b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7528\u6237\u4e13\u4e1a\u77e5\u8bc6\u6216\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u76ee\u6807\u7684\u7f3a\u9677\u3002", "motivation": "\u8868\u683c\u6570\u636e\u4e2d\u7684\u6d1e\u5bdf\u5305\u542b\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u4f46\u7528\u6237\u96be\u4ee5\u4ece\u5927\u91cf\u79bb\u6563\u6d1e\u5bdf\u4e2d\u6574\u5408\u51fa\u7b26\u5408\u5206\u6790\u76ee\u6807\u7684\u7edf\u4e00\u53d9\u8ff0\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6548\u7387\u4f4e\uff0c\u8981\u4e48\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u9700\u6c42\u3002", "method": "InReAcTable\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u8fc7\u6ee4\uff08\u5229\u7528\u6d1e\u5bdf\u56fe\uff09\u548c\u8bed\u4e49\u8fc7\u6ee4\uff08\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff09\u6765\u63a8\u8350\u7b26\u5408\u7528\u6237\u5206\u6790\u610f\u56fe\u7684\u6d1e\u5bdf\u3002", "result": "\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u539f\u578b\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u7528\u6237\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u548c\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "InReAcTable\u6846\u67b6\u548c\u539f\u578b\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5e2e\u52a9\u7528\u6237\u6784\u5efa\u7b26\u5408\u5206\u6790\u9700\u6c42\u7684\u89c6\u89c9\u6570\u636e\u6545\u4e8b\u3002"}}
{"id": "2508.18234", "pdf": "https://arxiv.org/pdf/2508.18234", "abs": "https://arxiv.org/abs/2508.18234", "authors": ["Tailon D. Jackson", "Byunggu Yu"], "title": "Can AI Have a Personality? Prompt Engineering for AI Personality Simulation: A Chatbot Case Study in Gender-Affirming Voice Therapy Training", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "This thesis investigates whether large language models (LLMs) can be guided\nto simulate a consistent personality through prompt engineering. The study\nexplores this concept within the context of a chatbot designed for\nSpeech-Language Pathology (SLP) student training, specifically focused on\ngender-affirming voice therapy. The chatbot, named Monae Jackson, was created\nto represent a 32-year-old transgender woman and engage in conversations\nsimulating client-therapist interactions. Findings suggest that with prompt\nengineering, the chatbot maintained a recognizable and consistent persona and\nhad a distinct personality based on the Big Five Personality test. These\nresults support the idea that prompt engineering can be used to simulate stable\npersonality characteristics in AI chatbots.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u4e00\u81f4\u6027\u4eba\u683c\uff0c\u5e94\u7528\u4e8e\u8bed\u97f3\u75c5\u7406\u5b66\u5b66\u751f\u8bad\u7ec3\u7684\u804a\u5929\u673a\u5668\u4eba\u3002", "motivation": "\u63a2\u8ba8\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u4e00\u81f4\u7684\u4eba\u683c\uff0c\u4ee5\u63d0\u5347\u8bed\u97f3\u75c5\u7406\u5b66\u5b66\u751f\u8bad\u7ec3\u7684\u4e92\u52a8\u6548\u679c\u3002", "method": "\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u8bbe\u8ba1\u4e00\u4e2a\u540d\u4e3aMonae Jackson\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u6a21\u62df32\u5c81\u8de8\u6027\u522b\u5973\u6027\u7684\u6027\u683c\uff0c\u5e76\u8fdb\u884cBig Five\u4eba\u683c\u6d4b\u8bd5\u3002", "result": "\u804a\u5929\u673a\u5668\u4eba\u80fd\u591f\u4fdd\u6301\u53ef\u8bc6\u522b\u4e14\u4e00\u81f4\u7684\u4eba\u683c\u7279\u5f81\uff0c\u652f\u6301\u63d0\u793a\u5de5\u7a0b\u5728\u6a21\u62df\u7a33\u5b9a\u4eba\u683c\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u53ef\u4ee5\u5e2e\u52a9AI\u804a\u5929\u673a\u5668\u4eba\u6a21\u62df\u7a33\u5b9a\u7684\u4eba\u683c\u7279\u5f81\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u4e92\u52a8\u9700\u6c42\u3002"}}
{"id": "2508.18267", "pdf": "https://arxiv.org/pdf/2508.18267", "abs": "https://arxiv.org/abs/2508.18267", "authors": ["Joy Lai", "David Black", "Kelly Beaton", "Bing Ye", "Alex Mihailidis"], "title": "Caregiver-in-the-Loop AI: A Simulation-Based Feasibility Study for Dementia Task Verification", "categories": ["cs.HC"], "comment": null, "summary": "Caregivers of people living with dementia (PLwD) experience stress when\nverifying whether tasks are truly completed, even with digital reminder\nsystems. Generative AI, such as GPT-4, may help by automating task verification\nthrough follow-up questioning and decision support.\n  This feasibility study evaluates an AI-powered task verification system\nintegrated with digital reminders for PLwD. It examines (1) GPT-4's ability to\ngenerate effective follow-up questions, (2) the accuracy of an AI-driven\nresponse flagging mechanism, and (3) the role of caregiver feedback in refining\nsystem adaptability. A simulated pipeline was tested on 64 anonymized\nreminders. GPT-4 generated follow-up questions with and without contextual\ninformation about PLwD routines. Responses were classified into High, Medium,\nor Low concern, and simulated caregiver feedback was used to refine outputs.\n  Results show that contextual information and caregiver input improved the\nclarity and relevance of AI-generated questions. The flagging system accurately\nidentified concerns, particularly for safety-critical tasks, though subjective\nor non-urgent tasks remained challenging. Findings demonstrate the feasibility\nof AI-assisted task verification in dementia care. Context-aware AI prompts and\ncaregiver feedback can enhance task monitoring, reduce caregiver stress, and\nstrengthen PLwD support. Future work should focus on real-world validation and\nscalability.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08\u5982GPT-4\uff09\u53ef\u7528\u4e8e\u75f4\u5446\u75c7\u60a3\u8005\u7684\u4efb\u52a1\u9a8c\u8bc1\uff0c\u51cf\u8f7b\u62a4\u7406\u4eba\u5458\u538b\u529b\u3002\u53ef\u884c\u6027\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u62a4\u7406\u53cd\u9988\u53ef\u63d0\u5347\u7cfb\u7edf\u6548\u679c\u3002", "motivation": "\u75f4\u5446\u75c7\u60a3\u8005\u7684\u62a4\u7406\u4eba\u5458\u5728\u9a8c\u8bc1\u4efb\u52a1\u5b8c\u6210\u60c5\u51b5\u65f6\u611f\u5230\u538b\u529b\uff0c\u5373\u4f7f\u4f7f\u7528\u6570\u5b57\u63d0\u9192\u7cfb\u7edf\u3002\u751f\u6210\u5f0fAI\u53ef\u80fd\u901a\u8fc7\u81ea\u52a8\u5316\u9a8c\u8bc1\u548c\u51b3\u7b56\u652f\u6301\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u6574\u5408AI\u7684\u6570\u5b57\u63d0\u9192\u7cfb\u7edf\uff0c\u6d4b\u8bd5GPT-4\u751f\u6210\u540e\u7eed\u95ee\u9898\u7684\u80fd\u529b\u3001AI\u6807\u8bb0\u673a\u5236\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u62a4\u7406\u53cd\u9988\u5bf9\u7cfb\u7edf\u4f18\u5316\u7684\u4f5c\u7528\u3002\u4f7f\u752864\u6761\u533f\u540d\u63d0\u9192\u8fdb\u884c\u6a21\u62df\u6d4b\u8bd5\u3002", "result": "\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u62a4\u7406\u53cd\u9988\u63d0\u5347\u4e86AI\u95ee\u9898\u7684\u6e05\u6670\u5ea6\u548c\u76f8\u5173\u6027\u3002\u6807\u8bb0\u7cfb\u7edf\u5bf9\u5b89\u5168\u5173\u952e\u4efb\u52a1\u8868\u73b0\u51c6\u786e\uff0c\u4f46\u5bf9\u4e3b\u89c2\u6216\u975e\u7d27\u6025\u4efb\u52a1\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "AI\u8f85\u52a9\u4efb\u52a1\u9a8c\u8bc1\u5728\u75f4\u5446\u62a4\u7406\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002\u672a\u6765\u9700\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2508.16608", "pdf": "https://arxiv.org/pdf/2508.16608", "abs": "https://arxiv.org/abs/2508.16608", "authors": ["Yulu Pi", "Cagatay Turkay", "Daniel Bogiatzis-Gibbons"], "title": "Interactive AI and Human Behavior: Challenges and Pathways for AI Governance", "categories": ["cs.CY", "cs.HC"], "comment": "In proceedings of AAAI/ACM Conference AIES 2025", "summary": "As Generative AI systems increasingly engage in long-term, personal, and\nrelational interactions, human-AI engagements are becoming significantly\ncomplex, making them more challenging to understand and govern. These\nInteractive AI systems adapt to users over time, build ongoing relationships,\nand even can take proactive actions on behalf of users. This new paradigm\nrequires us to rethink how such human-AI interactions can be studied\neffectively to inform governance and policy development. In this paper, we draw\non insights from a collaborative interdisciplinary workshop with policymakers,\nbehavioral scientists, Human-Computer Interaction researchers, and civil\nsociety practitioners, to identify challenges and methodological opportunities\narising within new forms of human-AI interactions. Based on these insights, we\ndiscuss an outcome-focused regulatory approach that integrates behavioral\ninsights to address both the risks and benefits of emerging human-AI\nrelationships. In particular, we emphasize the need for new methods to study\nthe fluid, dynamic, and context-dependent nature of these interactions. We\nprovide practical recommendations for developing human-centric AI governance,\ninformed by behavioral insights, that can respond to the complexities of\nInteractive AI systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u7cfb\u7edf\u5728\u957f\u671f\u3001\u4e2a\u4eba\u5316\u548c\u5173\u7cfb\u5316\u4ea4\u4e92\u4e2d\u5e26\u6765\u7684\u590d\u6742\u6027\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u884c\u4e3a\u5b66\u7684\u65b0\u76d1\u7ba1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u52a8\u6001\u7814\u7a76\u9700\u6c42\u548c\u4ee5\u4eba\u4e3a\u672c\u7684\u6cbb\u7406\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4ea4\u4e92\u7684\u65e5\u76ca\u590d\u6742\uff0c\u73b0\u6709\u7814\u7a76\u65b9\u6cd5\u548c\u6cbb\u7406\u6846\u67b6\u96be\u4ee5\u5e94\u5bf9\u5176\u52a8\u6001\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u4e9f\u9700\u65b0\u7684\u8de8\u5b66\u79d1\u65b9\u6cd5\u6765\u7406\u89e3\u548c\u89c4\u8303\u3002", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u7814\u8ba8\u4f1a\uff0c\u96c6\u5408\u653f\u7b56\u5236\u5b9a\u8005\u3001\u884c\u4e3a\u79d1\u5b66\u5bb6\u3001\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u8005\u548c\u793e\u4f1a\u5b9e\u8df5\u8005\u7684\u89c1\u89e3\uff0c\u8bc6\u522b\u6311\u6218\u4e0e\u65b9\u6cd5\u673a\u9047\uff0c\u63d0\u51fa\u57fa\u4e8e\u884c\u4e3a\u5b66\u7684\u76d1\u7ba1\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u7ed3\u679c\u4e3a\u5bfc\u5411\u7684\u76d1\u7ba1\u65b9\u6cd5\uff0c\u6574\u5408\u884c\u4e3a\u5b66\u89c1\u89e3\uff0c\u540c\u65f6\u5173\u6ce8\u65b0\u5174\u4eba\u673a\u5173\u7cfb\u7684\u98ce\u9669\u4e0e\u6536\u76ca\uff0c\u5e76\u5efa\u8bae\u52a8\u6001\u7814\u7a76\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u5e94\u5bf9\u4ea4\u4e92\u5f0fAI\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u9700\u5f00\u53d1\u4ee5\u4eba\u4e3a\u672c\u7684\u6cbb\u7406\u7b56\u7565\uff0c\u7ed3\u5408\u884c\u4e3a\u5b66\u52a8\u6001\u7814\u7a76\uff0c\u5e73\u8861\u6280\u672f\u53d1\u5c55\u4e0e\u4eba\u7c7b\u798f\u7949\u3002"}}
{"id": "2508.16609", "pdf": "https://arxiv.org/pdf/2508.16609", "abs": "https://arxiv.org/abs/2508.16609", "authors": ["Katie Seaborn"], "title": "Social Identity in Human-Agent Interaction: A Primer", "categories": ["physics.soc-ph", "cs.AI", "cs.CY", "cs.HC", "cs.RO"], "comment": "28 pages", "summary": "Social identity theory (SIT) and social categorization theory (SCT) are two\nfacets of the social identity approach (SIA) to understanding social phenomena.\nSIT and SCT are models that describe and explain how people interact with one\nanother socially, connecting the individual to the group through an\nunderstanding of underlying psychological mechanisms and intergroup behaviour.\nSIT, originally developed in the 1970s, and SCT, a later, more general\noffshoot, have been broadly applied to a range of social phenomena among\npeople. The rise of increasingly social machines embedded in daily life has\nspurned efforts on understanding whether and how artificial agents can and do\nparticipate in SIA activities. As agents like social robots and chatbots\npowered by sophisticated large language models (LLMs) advance, understanding\nthe real and potential roles of these technologies as social entities is\ncrucial. Here, I provide a primer on SIA and extrapolate, through case studies\nand imagined examples, how SIT and SCT can apply to artificial social agents. I\nemphasize that not all human models and sub-theories will apply. I further\nargue that, given the emerging competence of these machines and our tendency to\nbe taken in by them, we experts may need to don the hat of the uncanny killjoy,\nfor our own good.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u793e\u4f1a\u8ba4\u540c\u7406\u8bba\uff08SIT\uff09\u548c\u793e\u4f1a\u5206\u7c7b\u7406\u8bba\uff08SCT\uff09\u5728\u4eba\u5de5\u667a\u80fd\u793e\u4ea4\u4ee3\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u5e76\u975e\u6240\u6709\u4eba\u7c7b\u6a21\u578b\u90fd\u9002\u7528\uff0c\u5e76\u547c\u5401\u4e13\u5bb6\u4fdd\u6301\u8b66\u60d5\u3002", "motivation": "\u968f\u7740\u793e\u4ea4\u673a\u5668\u4eba\u7b49\u4eba\u5de5\u667a\u80fd\u6280\u672f\u7684\u666e\u53ca\uff0c\u7814\u7a76\u5b83\u4eec\u5728\u793e\u4f1a\u8ba4\u540c\u6d3b\u52a8\u4e2d\u7684\u89d2\u8272\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u8bbe\u60f3\u793a\u4f8b\uff0c\u5c06SIT\u548cSCT\u5e94\u7528\u5230\u4eba\u5de5\u667a\u80fd\u793e\u4ea4\u4ee3\u7406\u4e2d\u3002", "result": "\u63d0\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5728\u793e\u4f1a\u8ba4\u540c\u6d3b\u52a8\u4e2d\u7684\u6f5c\u5728\u89d2\u8272\uff0c\u5e76\u6307\u51fa\u9700\u5ba1\u614e\u5bf9\u5f85\u5176\u5e94\u7528\u3002", "conclusion": "\u4e13\u5bb6\u9700\u8b66\u60d5\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u793e\u4f1a\u5f71\u54cd\uff0c\u907f\u514d\u76f2\u76ee\u9002\u7528\u4eba\u7c7b\u6a21\u578b\u3002"}}
{"id": "2508.16628", "pdf": "https://arxiv.org/pdf/2508.16628", "abs": "https://arxiv.org/abs/2508.16628", "authors": ["R\u00e9nald Gesnot"], "title": "The Impact of Artificial Intelligence on Human Thought", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Research monograph; 132 pages; 13 figures; Version 1.0 (Aug 2025)", "summary": "This research paper examines, from a multidimensional perspective (cognitive,\nsocial, ethical, and philosophical), how AI is transforming human thought. It\nhighlights a cognitive offloading effect: the externalization of mental\nfunctions to AI can reduce intellectual engagement and weaken critical\nthinking. On the social level, algorithmic personalization creates filter\nbubbles that limit the diversity of opinions and can lead to the homogenization\nof thought and polarization. This research also describes the mechanisms of\nalgorithmic manipulation (exploitation of cognitive biases, automated\ndisinformation, etc.) that amplify AI's power of influence. Finally, the\nquestion of potential artificial consciousness is discussed, along with its\nethical implications. The report as a whole underscores the risks that AI poses\nto human intellectual autonomy and creativity, while proposing avenues\n(education, transparency, governance) to align AI development with the\ninterests of humanity.", "AI": {"tldr": "\u8bba\u6587\u4ece\u591a\u7ef4\u89d2\u5ea6\uff08\u8ba4\u77e5\u3001\u793e\u4f1a\u3001\u4f26\u7406\u548c\u54f2\u5b66\uff09\u63a2\u8ba8AI\u5982\u4f55\u6539\u53d8\u4eba\u7c7b\u601d\u7ef4\uff0c\u6307\u51fa\u8ba4\u77e5\u5378\u8f7d\u6548\u5e94\u548c\u7b97\u6cd5\u64cd\u63a7\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u5bf9\u7b56\u3002", "motivation": "\u7814\u7a76AI\u5bf9\u4eba\u7c7b\u601d\u7ef4\u7684\u5e7f\u6cdb\u5f71\u54cd\uff0c\u63ed\u793a\u5176\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u4ece\u8ba4\u77e5\u3001\u793e\u4f1a\u3001\u4f26\u7406\u548c\u54f2\u5b66\u89d2\u5ea6\u7efc\u5408\u5206\u6790AI\u7684\u5f71\u54cd\u3002", "result": "AI\u53ef\u80fd\u5bfc\u81f4\u8ba4\u77e5\u5378\u8f7d\u3001\u601d\u60f3\u6781\u5316\uff0c\u5e76\u5b58\u5728\u4f26\u7406\u95ee\u9898\u3002", "conclusion": "\u9700\u901a\u8fc7\u6559\u80b2\u3001\u900f\u660e\u5ea6\u548c\u6cbb\u7406\u5bf9\u7b56\u5e94\u5bf9AI\u5bf9\u667a\u529b\u81ea\u4e3b\u7684\u5a01\u80c1\u3002"}}
{"id": "2508.16659", "pdf": "https://arxiv.org/pdf/2508.16659", "abs": "https://arxiv.org/abs/2508.16659", "authors": ["Jiayi Wang", "Ruiwei Xiao", "Xinying Hou", "John Stamper"], "title": "Enabling Multi-Agent Systems as Learning Designers: Applying Learning Sciences to AI Instructional Design", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "under review for an [anonymized according to the conference policy]\n  conference", "summary": "K-12 educators are increasingly using Large Language Models (LLMs) to create\ninstructional materials. These systems excel at producing fluent, coherent\ncontent, but often lack support for high-quality teaching. The reason is\ntwofold: first, commercial LLMs, such as ChatGPT and Gemini which are among the\nmost widely accessible to teachers, do not come preloaded with the depth of\npedagogical theory needed to design truly effective activities; second,\nalthough sophisticated prompt engineering can bridge this gap, most teachers\nlack the time or expertise and find it difficult to encode such pedagogical\nnuance into their requests. This study shifts pedagogical expertise from the\nuser's prompt to the LLM's internal architecture. We embed the well-established\nKnowledge-Learning-Instruction (KLI) framework into a Multi-Agent System (MAS)\nto act as a sophisticated instructional designer. We tested three systems for\ngenerating secondary Math and Science learning activities: a Single-Agent\nbaseline simulating typical teacher prompts; a role-based MAS where agents work\nsequentially; and a collaborative MAS-CMD where agents co-construct activities\nthrough conquer and merge discussion. The generated materials were evaluated by\n20 practicing teachers and a complementary LLM-as-a-judge system using the\nQuality Matters (QM) K-12 standards. While the rubric scores showed only small,\noften statistically insignificant differences between the systems, the\nqualitative feedback from educators painted a clear and compelling picture.\nTeachers strongly preferred the activities from the collaborative MAS-CMD,\ndescribing them as significantly more creative, contextually relevant, and\nclassroom-ready. Our findings show that embedding pedagogical principles into\nLLM systems offers a scalable path for creating high-quality educational\ncontent.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5d4c\u5165KLI\u6846\u67b6\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\uff0c\u63d0\u5347LLM\u751f\u6210\u9ad8\u8d28\u91cf\u6559\u5b66\u6750\u6599\u7684\u80fd\u529b\uff0c\u6559\u5e08\u66f4\u9752\u7750\u534f\u4f5c\u5f0fMAS-CMD\u751f\u6210\u7684\u521b\u610f\u4e14\u5b9e\u7528\u7684\u5185\u5bb9\u3002", "motivation": "K-12\u6559\u5e08\u5e38\u7528LLM\u5236\u4f5c\u6559\u5b66\u6750\u6599\uff0c\u4f46\u56e0\u7f3a\u4e4f\u6df1\u5ea6\u6559\u5b66\u7406\u8bba\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u80fd\uff0c\u751f\u6210\u5185\u5bb9\u6559\u5b66\u6548\u679c\u6709\u9650\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06KLI\u6846\u67b6\u5d4c\u5165MAS\u7cfb\u7edf\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cd\u751f\u6210\u6570\u5b66\u4e0e\u79d1\u5b66\u5b66\u4e60\u6d3b\u52a8\u7684\u7cfb\u7edf\uff1a\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u3001\u987a\u5e8f\u5de5\u4f5c\u7684\u89d2\u8272\u57faMAS\u3001\u534f\u4f5c\u5f0fMAS-CMD\uff0c\u5e76\u7531\u6559\u5e08\u548cLLM\u8bc4\u4f30\u3002", "result": "\u6559\u5e08\u8bc4\u5206\u663e\u793a\u534f\u4f5c\u5f0fMAS-CMD\u751f\u6210\u7684\u6d3b\u52a8\u66f4\u5177\u521b\u610f\u3001\u60c5\u5883\u76f8\u5173\u6027\u548c\u8bfe\u5802\u5b9e\u7528\u4ef7\u503c\uff0c\u5c3d\u7ba1\u91cf\u5316\u8bc4\u5206\u5dee\u5f02\u4e0d\u663e\u8457\u3002", "conclusion": "\u5c06\u6559\u5b66\u539f\u5219\u5d4c\u5165LLM\u7cfb\u7edf\u53ef\u89c4\u6a21\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u6559\u80b2\u5185\u5bb9\uff0c\u534f\u4f5c\u5f0fMAS-CMD\u6548\u679c\u6700\u4f73\u3002"}}
{"id": "2508.16669", "pdf": "https://arxiv.org/pdf/2508.16669", "abs": "https://arxiv.org/abs/2508.16669", "authors": ["Hongrak Pak", "Ali Mostafavi"], "title": "Situational Awareness as the Imperative Capability for Disaster Resilience in the Era of Complex Hazards and Artificial Intelligence", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": null, "summary": "Disasters frequently exceed established hazard models, revealing blind spots\nwhere unforeseen impacts and vulnerabilities hamper effective response. This\nperspective paper contends that situational awareness (SA)-the ability to\nperceive, interpret, and project dynamic crisis conditions-is an often\noverlooked yet vital capability for disaster resilience. While risk mitigation\nmeasures can reduce known threats, not all hazards can be neutralized; truly\nadaptive resilience hinges on whether organizations rapidly detect emerging\nfailures, reconcile diverse data sources, and direct interventions where they\nmatter most. We present a technology-process-people roadmap, demonstrating how\nreal-time hazard nowcasting, interoperable workflows, and empowered teams\ncollectively transform raw data into actionable insight. A system-of-systems\napproach enables federated data ownership and modular analytics, so multiple\nagencies can share timely updates without sacrificing their distinct\noperational models. Equally crucial, structured sense-making routines and\ncognitive load safeguards help humans remain effective decision-makers amid\ndata abundance. By framing SA as a socio-technical linchpin rather than a\nperipheral add-on, this paper spotlights the urgency of elevating SA to a core\ndisaster resilience objective. We conclude with recommendations for further\nresearch-developing SA metrics, designing trustworthy human-AI collaboration,\nand strengthening inclusive data governance-to ensure that communities are\nequipped to cope with both expected and unexpected crises.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5f3a\u8c03\u60c5\u5883\u611f\u77e5\uff08SA\uff09\u4f5c\u4e3a\u707e\u96be\u5e94\u5bf9\u7684\u5173\u952e\u80fd\u529b\uff0c\u63d0\u51fa\u6280\u672f-\u6d41\u7a0b-\u4eba\u5458\u7684\u7efc\u5408\u65b9\u6848\uff0c\u4ee5\u63d0\u5347\u5b9e\u65f6\u6570\u636e\u8f6c\u5316\u4e3a\u884c\u52a8\u7684\u80fd\u529b\uff0c\u5e76\u547c\u5401\u5c06SA\u4f5c\u4e3a\u6838\u5fc3\u76ee\u6807\u3002", "motivation": "\u4f20\u7edf\u707e\u5bb3\u6a21\u578b\u5b58\u5728\u76f2\u533a\uff0c\u65e0\u6cd5\u5e94\u5bf9\u6240\u6709\u6f5c\u5728\u98ce\u9669\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347\u7ec4\u7ec7\u5728\u5371\u673a\u4e2d\u7684\u60c5\u5883\u611f\u77e5\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u771f\u6b63\u9002\u5e94\u6027\u7684\u707e\u5bb3\u97e7\u6027\u3002", "method": "\u63d0\u51fa\u6280\u672f-\u6d41\u7a0b-\u4eba\u5458\u7684\u8def\u7ebf\u56fe\uff0c\u5305\u62ec\u5b9e\u65f6\u707e\u5bb3\u9884\u62a5\u3001\u4e92\u64cd\u4f5c\u5de5\u4f5c\u6d41\u7a0b\u548c\u56e2\u961f\u8d4b\u6743\uff0c\u652f\u6301\u7cfb\u7edf\u95f4\u7684\u6570\u636e\u5171\u4eab\u548c\u6a21\u5757\u5316\u5206\u6790\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u591a\u65b9\u673a\u6784\u6570\u636e\u5171\u4eab\u548c\u9ad8\u6548\u51b3\u7b56\uff0c\u540c\u65f6\u4fdd\u62a4\u4eba\u7c7b\u51b3\u7b56\u8005\u7684\u8ba4\u77e5\u8d1f\u8377\u3002", "conclusion": "\u5e94\u5c06SA\u4f5c\u4e3a\u707e\u5bb3\u97e7\u6027\u7684\u6838\u5fc3\u76ee\u6807\uff0c\u5e76\u8fdb\u4e00\u6b65\u7814\u7a76SA\u6307\u6807\u3001\u53ef\u4fe1\u4eba\u673a\u534f\u4f5c\u548c\u5305\u5bb9\u6027\u6570\u636e\u6cbb\u7406\uff0c\u4ee5\u5e94\u5bf9\u672a\u77e5\u5371\u673a\u3002"}}
{"id": "2508.16908", "pdf": "https://arxiv.org/pdf/2508.16908", "abs": "https://arxiv.org/abs/2508.16908", "authors": ["Amod K. Agrawal"], "title": "Localization using Angle-of-Arrival Triangulation", "categories": ["eess.AS", "cs.HC", "cs.NI", "cs.SD", "eess.SP", "C.3; C.2.1; C.2.4; I.5.4; H.5.2; J.7"], "comment": "6 pages, 5 figures, 1 table. Accepted at the ACM International\n  Workshop on Environmental Sensing Systems for Smart Cities (EnvSys 2025). To\n  appear in the MobiSys 2025 Proceedings", "summary": "Indoor localization is a long-standing challenge in mobile computing, with\nsignificant implications for enabling location-aware and intelligent\napplications within smart environments such as homes, offices, and retail\nspaces. As AI assistants such as Amazon Alexa and Google Nest become\nincreasingly pervasive, microphone-equipped devices are emerging as key\ncomponents of everyday life and home automation. This paper introduces a\npassive, infrastructure-light system for localizing human speakers using speech\nsignals captured by two or more spatially distributed smart devices. The\nproposed approach, GCC+, extends the Generalized Cross-Correlation with Phase\nTransform (GCC-PHAT) method to estimate the Angle-of-Arrival (AoA) of audio\nsignals at each device and applies robust triangulation techniques to infer the\nspeaker's two-dimensional position. To further improve temporal resolution and\nlocalization accuracy, feature-space expansion and subsample interpolation\ntechniques are employed for precise Time Difference of Arrival (TDoA)\nestimation. The system operates without requiring hardware modifications, prior\ncalibration, explicit user cooperation, or knowledge of the speaker's signal\ncontent, thereby offering a highly practical solution for real-world\ndeployment. Experimental evaluation in a real-world home environment yields a\nmedian AoA estimation error of 2.2 degrees and a median localization error of\n1.25 m, demonstrating the feasibility and effectiveness of audio-based\nlocalization for enabling context-aware, privacy-preserving ambient\nintelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u97f3\u9891\u4fe1\u53f7\u7684\u88ab\u52a8\u5ba4\u5185\u5b9a\u4f4d\u7cfb\u7edfGCC+\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\u6216\u7528\u6237\u914d\u5408\uff0c\u901a\u8fc7\u6539\u8fdb\u7684GCC-PHAT\u65b9\u6cd5\u548c\u4e09\u89d2\u5b9a\u4f4d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8bf4\u8bdd\u8005\u5b9a\u4f4d\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u8bbe\u5907\u666e\u53ca\uff0c\u57fa\u4e8e\u9ea6\u514b\u98ce\u7684\u5ba4\u5185\u5b9a\u4f4d\u6210\u4e3a\u5b9e\u73b0\u667a\u80fd\u73af\u5883\u7684\u5173\u952e\u6280\u672f\u3002", "method": "\u6269\u5c55GCC-PHAT\u65b9\u6cd5\u4f30\u8ba1\u97f3\u9891\u4fe1\u53f7\u7684\u5230\u8fbe\u89d2\uff0c\u7ed3\u5408\u4e09\u89d2\u5b9a\u4f4d\u548cTDoA\u6280\u672f\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAoA\u8bef\u5dee\u4e2d\u503c\u4e3a2.2\u5ea6\uff0c\u5b9a\u4f4d\u8bef\u5dee\u4e2d\u503c\u4e3a1.25\u7c73\u3002", "conclusion": "\u8bc1\u660e\u4e86\u97f3\u9891\u5b9a\u4f4d\u5728\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u53ef\u884c\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.17131", "pdf": "https://arxiv.org/pdf/2508.17131", "abs": "https://arxiv.org/abs/2508.17131", "authors": ["Amrit Poudel", "Maria Milkowski", "Tim Weninger"], "title": "The Power of Framing: How News Headlines Guide Search Behavior", "categories": ["cs.CL", "cs.HC", "cs.IR"], "comment": "Accepted to EMNLP", "summary": "Search engines play a central role in how people gather information, but\nsubtle cues like headline framing may influence not only what users believe but\nalso how they search. While framing effects on judgment are well documented,\ntheir impact on subsequent search behavior is less understood. We conducted a\ncontrolled experiment where participants issued queries and selected from\nheadlines filtered by specific linguistic frames. Headline framing\nsignificantly shaped follow-up queries: conflict and strategy frames disrupted\nalignment with prior selections, while episodic frames led to more concrete\nqueries than thematic ones. We also observed modest short-term frame\npersistence that declined over time. These results suggest that even brief\nexposure to framing can meaningfully alter the direction of users\ninformation-seeking behavior.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6807\u9898\u6846\u67b6\u5982\u4f55\u5f71\u54cd\u7528\u6237\u7684\u641c\u7d22\u884c\u4e3a\u548c\u540e\u7eed\u67e5\u8be2\uff0c\u53d1\u73b0\u51b2\u7a81\u548c\u7b56\u7565\u6846\u67b6\u4f1a\u5e72\u6270\u7528\u6237\u7684\u9009\u62e9\u4e00\u81f4\u6027\uff0c\u800c\u77ed\u671f\u7684\u6846\u67b6\u6548\u5e94\u4f1a\u968f\u65f6\u95f4\u51cf\u5f31\u3002", "motivation": "\u63a2\u8ba8\u641c\u7d22\u5f15\u64ce\u4e2d\u6807\u9898\u6846\u67b6\u5bf9\u7528\u6237\u4fe1\u606f\u641c\u7d22\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u586b\u8865\u4e86\u6846\u67b6\u6548\u5e94\u5728\u641c\u7d22\u884c\u4e3a\u4e2d\u7814\u7a76\u4e0d\u8db3\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\uff0c\u8ba9\u53c2\u4e0e\u8005\u5728\u4e0d\u540c\u7684\u6807\u9898\u6846\u67b6\u4e0b\u8fdb\u884c\u641c\u7d22\u5e76\u9009\u62e9\u7ed3\u679c\uff0c\u5206\u6790\u6846\u67b6\u5bf9\u540e\u7eed\u67e5\u8be2\u7684\u5f71\u54cd\u3002", "result": "\u51b2\u7a81\u548c\u7b56\u7565\u6846\u67b6\u4f1a\u7834\u574f\u7528\u6237\u9009\u62e9\u7684\u4e00\u81f4\u6027\uff0c\u800c\u6545\u4e8b\u6027\u6846\u67b6\u5bfc\u81f4\u66f4\u5177\u4f53\u7684\u67e5\u8be2\u3002\u77ed\u671f\u6846\u67b6\u6548\u5e94\u5b58\u5728\u4f46\u968f\u65f6\u95f4\u51cf\u5f31\u3002", "conclusion": "\u77ed\u6682\u7684\u6846\u67b6\u66b4\u9732\u80fd\u663e\u8457\u6539\u53d8\u7528\u6237\u7684\u4fe1\u606f\u641c\u7d22\u65b9\u5411\uff0c\u4f46\u6548\u5e94\u968f\u65f6\u95f4\u51cf\u5f31\u3002"}}
{"id": "2508.17414", "pdf": "https://arxiv.org/pdf/2508.17414", "abs": "https://arxiv.org/abs/2508.17414", "authors": ["Temesgen Kitaw Damenu", "\u0130nci Zaim G\u00f6kbay", "Alexandra Covaci", "Shujun Li"], "title": "Cyber Security Educational Games for Children: A Systematic Literature Review", "categories": ["cs.CR", "cs.CY", "cs.HC"], "comment": null, "summary": "Educational games have been widely used to teach children about cyber\nsecurity. This systematic literature review reveals evidence of positive\nlearning outcomes, after analysing 91 such games reported in 68 papers\npublished between 2010 and 2024. However, critical gaps have also been\nidentified regarding the design processes and the methodological rigour,\nincluding lack of systematic design, misalignment between proposed and achieved\nlearning outcomes, rare use of control groups, limited discussions on ethical\nconsiderations, and underutilisation of emerging technologies. We recommend\nmultiple future research directions, e.g., a hybrid approach to game design and\nevaluation that combines bottom-up and top-down approaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff02010\u81f32024\u5e74\u95f4\u53d1\u8868\u768491\u6b3e\u6559\u80b2\u6e38\u620f\uff0c\u53d1\u73b0\u5176\u5728\u513f\u7ae5\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u4e2d\u5177\u6709\u79ef\u6781\u6548\u679c\uff0c\u4f46\u4e5f\u5b58\u5728\u8bbe\u8ba1\u548c\u65b9\u6cd5\u5b66\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8bc4\u4f30\u6559\u80b2\u6e38\u620f\u5728\u513f\u7ae5\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u4e2d\u7684\u6709\u6548\u6027\u53ca\u5176\u8bbe\u8ba1\u548c\u5b9e\u65bd\u4e2d\u7684\u4e0d\u8db3\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u4e8668\u7bc7\u8bba\u6587\u4e2d\u62a5\u544a\u768491\u6b3e\u6559\u80b2\u6e38\u620f\uff0c\u91cd\u70b9\u5173\u6ce8\u5b66\u4e60\u6548\u679c\u3001\u8bbe\u8ba1\u8fc7\u7a0b\u548c\u65b9\u6cd5\u5b66\u4e25\u8c28\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6559\u80b2\u6e38\u620f\u5bf9\u513f\u7ae5\u7f51\u7edc\u5b89\u5168\u6559\u80b2\u6709\u79ef\u6781\u6548\u679c\uff0c\u4f46\u4e5f\u5b58\u5728\u8bbe\u8ba1\u7cfb\u7edf\u6027\u4e0d\u8db3\u3001\u5b66\u4e60\u76ee\u6807\u4e0e\u5b9e\u9645\u6548\u679c\u4e0d\u4e00\u81f4\u3001\u7f3a\u4e4f\u5bf9\u7167\u7ec4\u7b49\u95ee\u9898\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u9700\u6539\u8fdb\u6559\u80b2\u6e38\u620f\u7684\u8bbe\u8ba1\u548c\u65b9\u6cd5\u5b66\u4e25\u8c28\u6027\uff0c\u5e76\u5efa\u8bae\u7ed3\u5408\u81ea\u4e0b\u800c\u4e0a\u548c\u81ea\u4e0a\u800c\u4e0b\u7684\u6df7\u5408\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u65b9\u6cd5\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.17742", "pdf": "https://arxiv.org/pdf/2508.17742", "abs": "https://arxiv.org/abs/2508.17742", "authors": ["Wei Xiong", "Jiangtong Li", "Jie Li", "Kun Zhu"], "title": "EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models", "categories": ["eess.SP", "cs.AI", "cs.HC"], "comment": "17 pages, 7 pages", "summary": "Electroencephalography (EEG) foundation models are poised to significantly\nadvance brain signal analysis by learning robust representations from\nlarge-scale, unlabeled datasets. However, their rapid proliferation has\noutpaced the development of standardized evaluation benchmarks, which\ncomplicates direct model comparisons and hinders systematic scientific\nprogress. This fragmentation fosters scientific inefficiency and obscures\ngenuine architectural advancements. To address this critical gap, we introduce\nEEG-FM-Bench, the first comprehensive benchmark for the systematic and\nstandardized evaluation of EEG foundation models (EEG-FMs). Our contributions\nare threefold: (1) we curate a diverse suite of downstream tasks and datasets\nfrom canonical EEG paradigms, implementing standardized processing and\nevaluation protocols within a unified open-source framework; (2) we benchmark\nprominent state-of-the-art foundation models to establish comprehensive\nbaseline results for a clear comparison of the current landscape; (3) we\nperform qualitative analyses of the learned representations to provide insights\ninto model behavior and inform future architectural design. Through extensive\nexperiments, we find that fine-grained spatio-temporal feature interaction,\nmultitask unified training and neuropsychological priors would contribute to\nenhancing model performance and generalization capabilities. By offering a\nunified platform for fair comparison and reproducible research, EEG-FM-Bench\nseeks to catalyze progress and guide the community toward the development of\nmore robust and generalizable EEG-FMs. Code is released at\nhttps://github.com/xw1216/EEG-FM-Bench.", "AI": {"tldr": "EEG-FM-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6807\u51c6\u5316\u8bc4\u4f30EEG\u57fa\u7840\u6a21\u578b\uff0c\u4fc3\u8fdb\u79d1\u5b66\u8fdb\u6b65\u548c\u516c\u5e73\u6bd4\u8f83\u3002", "motivation": "EEG\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u963b\u788d\u4e86\u79d1\u5b66\u8fdb\u5c55\u548c\u6a21\u578b\u6bd4\u8f83\u3002", "method": "\u5f00\u53d1EEG-FM-Bench\uff0c\u6574\u5408\u591a\u6837\u5316\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5206\u6790\u5176\u8868\u793a\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u7279\u5f81\u4ea4\u4e92\u3001\u591a\u4efb\u52a1\u7edf\u4e00\u8bad\u7ec3\u548c\u795e\u7ecf\u5fc3\u7406\u5b66\u5148\u9a8c\u80fd\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "EEG-FM-Bench\u4e3aEEG\u57fa\u7840\u6a21\u578b\u7684\u516c\u5e73\u6bd4\u8f83\u548c\u53ef\u91cd\u590d\u7814\u7a76\u63d0\u4f9b\u4e86\u5e73\u53f0\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2508.17753", "pdf": "https://arxiv.org/pdf/2508.17753", "abs": "https://arxiv.org/abs/2508.17753", "authors": ["Theresa Pekarek Rosin", "Julia Gachot", "Henri-Leon Kordt", "Matthias Kerzel", "Stefan Wermter"], "title": "Talking to Robots: A Practical Examination of Speech Foundation Models for HRI Applications", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "comment": "Accepted at the workshop on Foundation Models for Social Robotics\n  (FoMoSR) at ICSR 2025", "summary": "Automatic Speech Recognition (ASR) systems in real-world settings need to\nhandle imperfect audio, often degraded by hardware limitations or environmental\nnoise, while accommodating diverse user groups. In human-robot interaction\n(HRI), these challenges intersect to create a uniquely challenging recognition\nenvironment. We evaluate four state-of-the-art ASR systems on eight publicly\navailable datasets that capture six dimensions of difficulty: domain-specific,\naccented, noisy, age-variant, impaired, and spontaneous speech. Our analysis\ndemonstrates significant variations in performance, hallucination tendencies,\nand inherent biases, despite similar scores on standard benchmarks. These\nlimitations have serious implications for HRI, where recognition errors can\ninterfere with task performance, user trust, and safety.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u56db\u79cd\u6700\u5148\u8fdb\u7684ASR\u7cfb\u7edf\u5728\u516b\u79cd\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u6027\u80fd\u5dee\u5f02\u3001\u5e7b\u89c9\u503e\u5411\u548c\u5185\u5728\u504f\u89c1\uff0c\u8fd9\u5bf9\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8ASR\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\uff08\u5982HRI\uff09\u4e2d\u5904\u7406\u4e0d\u5b8c\u7f8e\u97f3\u9891\uff08\u5982\u786c\u4ef6\u9650\u5236\u3001\u73af\u5883\u566a\u58f0\uff09\u548c\u591a\u6837\u5316\u7528\u6237\u7fa4\u4f53\u65f6\u7684\u6311\u6218\u3002", "method": "\u5728\u516b\u79cd\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u56db\u79cd\u6700\u5148\u8fdb\u7684ASR\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u8986\u76d6\u4e86\u516d\u79cd\u96be\u5ea6\u7ef4\u5ea6\uff1a\u9886\u57df\u7279\u5b9a\u3001\u53e3\u97f3\u3001\u566a\u58f0\u3001\u5e74\u9f84\u53d8\u5316\u3001\u8bed\u97f3\u969c\u788d\u548c\u81ea\u53d1\u8bed\u97f3\u3002", "result": "ASR\u7cfb\u7edf\u5728\u6807\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\u3001\u5e7b\u89c9\u503e\u5411\u548c\u5185\u5728\u504f\u89c1\u3002", "conclusion": "\u8fd9\u4e9b\u5c40\u9650\u6027\u5bf9HRI\uff08\u4efb\u52a1\u8868\u73b0\u3001\u7528\u6237\u4fe1\u4efb\u548c\u5b89\u5168\uff09\u6709\u4e25\u91cd\u5f71\u54cd\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdbASR\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2508.18167", "pdf": "https://arxiv.org/pdf/2508.18167", "abs": "https://arxiv.org/abs/2508.18167", "authors": ["Deep Anil Patel", "Iain Melvin", "Christopher Malon", "Martin Renqiang Min"], "title": "DiscussLLM: Teaching Large Language Models When to Speak", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nunderstanding and generating human-like text, yet they largely operate as\nreactive agents, responding only when directly prompted. This passivity creates\nan \"awareness gap,\" limiting their potential as truly collaborative partners in\ndynamic human discussions. We introduce $\\textit{DiscussLLM}$, a framework\ndesigned to bridge this gap by training models to proactively decide not just\n$\\textit{what}$ to say, but critically, $\\textit{when}$ to speak. Our primary\ncontribution is a scalable two-stage data generation pipeline that synthesizes\na large-scale dataset of realistic multi-turn human discussions. Each\ndiscussion is annotated with one of five intervention types (e.g., Factual\nCorrection, Concept Definition) and contains an explicit conversational trigger\nwhere an AI intervention adds value. By training models to predict a special\nsilent token when no intervention is needed, they learn to remain quiet until a\nhelpful contribution can be made. We explore two architectural baselines: an\nintegrated end-to-end model and a decoupled classifier-generator system\noptimized for low-latency inference. We evaluate these models on their ability\nto accurately time interventions and generate helpful responses, paving the way\nfor more situationally aware and proactive conversational AI.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6DiscussLLM\uff0c\u65e8\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u52a8\u51b3\u5b9a\u4f55\u65f6\u53d1\u8a00\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u52a8\u6001\u8ba8\u8bba\u4e2d\u7684\u88ab\u52a8\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u901a\u5e38\u662f\u53cd\u5e94\u6027\u7684\uff0c\u65e0\u6cd5\u4e3b\u52a8\u53c2\u4e0e\u52a8\u6001\u8ba8\u8bba\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u4f5c\u4e3a\u534f\u4f5c\u4f19\u4f34\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6570\u636e\u751f\u6210\u6d41\u7a0b\u5408\u6210\u5927\u89c4\u6a21\u7684\u591a\u8f6e\u8ba8\u8bba\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u5e72\u9884\u7c7b\u578b\u548c\u89e6\u53d1\u70b9\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u9759\u9ed8\u6807\u8bb0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u79cd\u67b6\u6784\uff08\u7aef\u5230\u7aef\u6a21\u578b\u548c\u5206\u79bb\u7684\u5206\u7c7b\u5668-\u751f\u6210\u5668\u7cfb\u7edf\uff09\uff0c\u8bc4\u4f30\u4e86\u5176\u5728\u51c6\u786e\u65f6\u673a\u5e72\u9884\u548c\u751f\u6210\u6709\u7528\u56de\u7b54\u65b9\u9762\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6253\u9020\u66f4\u5177\u60c5\u5883\u610f\u8bc6\u548c\u4e3b\u52a8\u6027\u7684\u5bf9\u8bddAI\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
