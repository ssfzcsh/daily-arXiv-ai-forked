{"id": "2507.08061", "pdf": "https://arxiv.org/pdf/2507.08061", "abs": "https://arxiv.org/abs/2507.08061", "authors": ["Andrea Morales Coto", "Aditi Verma"], "title": "The State of Computational Science in Fission and Fusion Energy", "categories": ["cs.SE", "physics.soc-ph"], "comment": null, "summary": "The tools used to engineer something are just as important as the thing that\nis actually being engineered. In fact, in many cases, the tools can indeed\ndetermine what is engineerable. In fusion and fission1 energy engineering,\nsoftware has become the dominant tool for design. For that reason, in 2024, for\nthe first time ever, we asked 103 computational scientists developing the codes\nused in fusion and fission energy about the problems they are attempting to\nsolve with their codes, the tools available to them to solve them, and their\nend to end developer experience with said tools.\n  The results revealed a changing tide in software tools in fusion and fission,\nwith more and more computational scientists preferring modern programming\nlanguages, open-source codes, and modular software. These trends represent a\npeek into what will happen 5 to 10 years in the future of nuclear engineering.\nSince the majority of our respondents belonged to US national labs and\nuniversities, these results hint at the most cutting-edge trends in the\nindustry. The insights included in the State of Computational Science in\nFission and Fusion Energy indicate a dramatic shift toward multiphysics codes,\na drop-off in the use of FORTRAN in favor of more modern languages like Python\nand C++, and ever-rising budgets for code development, at times reaching $50M\nin a single organization.\n  Our survey paints a future of nuclear engineering codes that is modular in\nnature, small in terms of compute, and increasingly prioritized by\norganizations. Access to our results in web form are available online.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8c03\u67e5\u4e86\u6838\u5de5\u7a0b\u8ba1\u7b97\u79d1\u5b66\u7684\u73b0\u72b6\uff0c\u63ed\u793a\u4e86\u8f6f\u4ef6\u5de5\u5177\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u5305\u62ec\u73b0\u4ee3\u7f16\u7a0b\u8bed\u8a00\u3001\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u5757\u5316\u8f6f\u4ef6\u7684\u666e\u53ca\uff0c\u4ee5\u53ca\u5bf9\u591a\u7269\u7406\u573a\u4ee3\u7801\u548c\u66f4\u9ad8\u5f00\u53d1\u9884\u7b97\u7684\u5173\u6ce8\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u6838\u80fd\uff08\u805a\u53d8\u4e0e\u88c2\u53d8\uff09\u9886\u57df\u4e2d\u8ba1\u7b97\u79d1\u5b66\u5bb6\u4f7f\u7528\u7684\u8f6f\u4ef6\u5de5\u5177\u53ca\u5176\u53d1\u5c55\u8d8b\u52bf\uff0c\u4ee5\u9884\u6d4b\u672a\u67655\u523010\u5e74\u7684\u884c\u4e1a\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u8c03\u67e5103\u4f4d\u4ece\u4e8b\u6838\u80fd\u9886\u57df\u4ee3\u7801\u5f00\u53d1\u7684\u8ba1\u7b97\u79d1\u5b66\u5bb6\uff0c\u6536\u96c6\u4ed6\u4eec\u89e3\u51b3\u7684\u95ee\u9898\u3001\u53ef\u7528\u5de5\u5177\u53ca\u5f00\u53d1\u4f53\u9a8c\u7684\u6570\u636e\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u4ee3\u7f16\u7a0b\u8bed\u8a00\uff08\u5982Python\u548cC++\uff09\u9010\u6e10\u53d6\u4ee3FORTRAN\uff0c\u5f00\u6e90\u4ee3\u7801\u548c\u6a21\u5757\u5316\u8f6f\u4ef6\u66f4\u53d7\u6b22\u8fce\uff0c\u591a\u7269\u7406\u573a\u4ee3\u7801\u9700\u6c42\u589e\u52a0\uff0c\u4e14\u5f00\u53d1\u9884\u7b97\u663e\u8457\u4e0a\u5347\uff08\u90e8\u5206\u7ec4\u7ec7\u8fbe5000\u4e07\u7f8e\u5143\uff09\u3002", "conclusion": "\u6838\u5de5\u7a0b\u4ee3\u7801\u7684\u672a\u6765\u5c06\u8d8b\u5411\u6a21\u5757\u5316\u3001\u8ba1\u7b97\u91cf\u5c0f\u4e14\u66f4\u53d7\u7ec4\u7ec7\u91cd\u89c6\uff0c\u53cd\u6620\u4e86\u884c\u4e1a\u5411\u5f00\u653e\u6027\u548c\u73b0\u4ee3\u5316\u5de5\u5177\u7684\u8f6c\u53d8\u3002"}}
{"id": "2507.08149", "pdf": "https://arxiv.org/pdf/2507.08149", "abs": "https://arxiv.org/abs/2507.08149", "authors": ["Valerie Chen", "Ameet Talwalkar", "Robert Brennan", "Graham Neubig"], "title": "Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows", "categories": ["cs.SE"], "comment": null, "summary": "Developers now have access to a growing array of increasingly autonomous AI\ntools to support software development. While numerous studies have examined\ndeveloper use of copilots, which can provide chat assistance or code\ncompletions, evaluations of coding agents, which can automatically write files\nand run code, still largely rely on static benchmarks without\nhumans-in-the-loop. In this work, we conduct the first academic study to\nexplore developer interactions with coding agents and characterize how more\nautonomous AI tools affect user productivity and experience, compared to\nexisting copilots. We evaluate two leading copilot and agentic coding\nassistants, GitHub Copilot and OpenHands, recruiting participants who regularly\nuse the former. Our results show agents have the potential to assist developers\nin ways that surpass copilots (e.g., completing tasks that humans might not\nhave accomplished before) and reduce the user effort required to complete\ntasks. However, there are challenges involved in enabling their broader\nadoption, including how to ensure users have an adequate understanding of agent\nbehaviors. Our results not only provide insights into how developer workflows\nchange as a result of coding agents but also highlight how user interactions\nwith agents differ from those with existing copilots, motivating a set of\nrecommendations for researchers building new agents. Given the broad set of\ndevelopers who still largely rely on copilot-like systems, our work highlights\nkey challenges of adopting more agentic systems into developer workflows.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5f00\u53d1\u8005\u4e0e\u81ea\u52a8\u5316\u7f16\u7801\u4ee3\u7406\u7684\u4e92\u52a8\uff0c\u53d1\u73b0\u4ee3\u7406\u80fd\u8d85\u8d8a\u73b0\u6709\u4ee3\u7801\u8865\u5168\u5de5\u5177\uff0c\u4f46\u9700\u89e3\u51b3\u7528\u6237\u5bf9\u5176\u884c\u4e3a\u7684\u7406\u89e3\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u81ea\u52a8\u5316AI\u7f16\u7801\u4ee3\u7406\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u4e0e\u4f53\u9a8c\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\u7684\u7a7a\u767d\u3002", "method": "\u62db\u52dfGitHub Copilot\u7684\u5e38\u89c4\u7528\u6237\uff0c\u5bf9\u6bd4\u8bc4\u4f30GitHub Copilot\u4e0eOpenHands\u4e24\u79cd\u5de5\u5177\u7684\u6548\u679c\u3002", "result": "\u7f16\u7801\u4ee3\u7406\u80fd\u5b8c\u6210\u4f20\u7edf\u5de5\u5177\u65e0\u6cd5\u5b9e\u73b0\u7684\u4efb\u52a1\uff0c\u4f46\u9700\u89e3\u51b3\u7528\u6237\u7406\u89e3\u4ee3\u7406\u884c\u4e3a\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u53d8\u9769\u63d0\u4f9b\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u6784\u5efa\u66f4\u667a\u80fd\u4ee3\u7406\u7684\u5efa\u8bae\u3002"}}
{"id": "2507.08160", "pdf": "https://arxiv.org/pdf/2507.08160", "abs": "https://arxiv.org/abs/2507.08160", "authors": ["Ot\u00e1vio Cury", "Guilherme Avelino"], "title": "The Impact of Generative AI on Code Expertise Models: An Exploratory Study", "categories": ["cs.SE"], "comment": null, "summary": "Generative Artificial Intelligence (GenAI) tools for source code generation\nhave significantly boosted productivity in software development. However, they\nalso raise concerns, particularly the risk that developers may rely heavily on\nthese tools, reducing their understanding of the generated code. We hypothesize\nthat this loss of understanding may be reflected in source code knowledge\nmodels, which are used to identify developer expertise. In this work, we\npresent an exploratory analysis of how a knowledge model and a Truck Factor\nalgorithm built upon it can be affected by GenAI usage. To investigate this, we\ncollected statistical data on the integration of ChatGPT-generated code into\nGitHub projects and simulated various scenarios by adjusting the degree of\nGenAI contribution. Our findings reveal that most scenarios led to measurable\nimpacts, indicating the sensitivity of current expertise metrics. This suggests\nthat as GenAI becomes more integrated into development workflows, the\nreliability of such metrics may decrease.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86GenAI\u5de5\u5177\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5e94\u7528\u5982\u4f55\u5f71\u54cd\u5f00\u53d1\u8005\u5bf9\u4ee3\u7801\u7684\u7406\u89e3\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5206\u6790\u663e\u793a\u8fd9\u4f1a\u964d\u4f4e\u73b0\u6709\u4e13\u4e1a\u77e5\u8bc6\u8bc4\u4f30\u6307\u6807\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u7814\u7a76GenAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\u53ef\u80fd\u5bfc\u81f4\u7684\u5f00\u53d1\u8005\u5bf9\u751f\u6210\u4ee3\u7801\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7a76\u5176\u5bf9\u6e90\u4ee3\u7801\u77e5\u8bc6\u6a21\u578b\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6536\u96c6ChatGPT\u751f\u6210\u7684\u4ee3\u7801\u5728GitHub\u9879\u76ee\u4e2d\u7684\u7edf\u8ba1\u6570\u636e\uff0c\u5e76\u6a21\u62df\u4e0d\u540cGenAI\u8d21\u732e\u5ea6\u7684\u573a\u666f\uff0c\u5206\u6790\u77e5\u8bc6\u6a21\u578b\u548cTruck Factor\u7b97\u6cd5\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570\u573a\u666f\u4e0bGenAI\u7684\u4f7f\u7528\u5bf9\u4e13\u4e1a\u77e5\u8bc6\u8bc4\u4f30\u6307\u6807\u4ea7\u751f\u663e\u8457\u5f71\u54cd\uff0c\u8868\u660e\u8fd9\u4e9b\u6307\u6807\u5bf9GenAI\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u968f\u7740GenAI\u5728\u5f00\u53d1\u6d41\u7a0b\u4e2d\u7684\u6df1\u5165\u6574\u5408\uff0c\u73b0\u6709\u7684\u4e13\u4e1a\u77e5\u8bc6\u8bc4\u4f30\u6307\u6807\u7684\u53ef\u9760\u6027\u53ef\u80fd\u4f1a\u4e0b\u964d\u3002"}}
{"id": "2507.08250", "pdf": "https://arxiv.org/pdf/2507.08250", "abs": "https://arxiv.org/abs/2507.08250", "authors": ["Yasaman Abedini", "Abbas Heydarnoori"], "title": "Leveraging Large Language Models for Classifying App Users' Feedback", "categories": ["cs.SE"], "comment": null, "summary": "In recent years, significant research has been conducted into classifying\napplication (app) user feedback, primarily relying on supervised machine\nlearning algorithms. However, fine-tuning more generalizable classifiers based\non existing labeled datasets remains an important challenge, as creating large\nand accurately labeled datasets often requires considerable time and resources.\nIn this paper, we evaluate the capabilities of four advanced LLMs, including\nGPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback\nclassification and address the challenge of the limited labeled dataset. To\nachieve this, we conduct several experiments on eight datasets that have been\nmeticulously labeled in prior research. These datasets include user reviews\nfrom app stores, posts from the X platform, and discussions from the public\nforums, widely recognized as representative sources of app user feedback. We\nanalyze the performance of various LLMs in identifying both fine-grained and\ncoarse-grained user feedback categories. Given the substantial volume of daily\nuser feedback and the computational limitations of LLMs, we leverage these\nmodels as an annotation tool to augment labeled datasets with general and\napp-specific data. This augmentation aims to enhance the performance of\nstate-of-the-art BERT-based classification models. Our findings indicate that\nLLMs when guided by well-crafted prompts, can effectively classify user\nfeedback into coarse-grained categories. Moreover, augmenting the training\ndataset with datasets labeled using the consensus of LLMs can significantly\nenhance classifier performance.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528GPT-3.5-Turbo\u7b49\u9ad8\u7ea7LLMs\u5206\u7c7b\u7528\u6237\u53cd\u9988\uff0c\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u5e76\u80fd\u63d0\u5347BERT\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u53cd\u9988\u5206\u7c7b\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u63a2\u7d22LLMs\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u56db\u79cdLLMs\uff08\u5982GPT-3.5-Turbo\uff09\u5bf9\u516b\u4e2a\u5df2\u6807\u6ce8\u6570\u636e\u96c6\u5b9e\u9a8c\uff0c\u5206\u6790\u5176\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u5229\u7528LLMs\u751f\u6210\u6807\u6ce8\u6570\u636e\u589e\u5f3a\u8bad\u7ec3\u96c6\u3002", "result": "LLMs\u80fd\u6709\u6548\u5206\u7c7b\u7c97\u7c92\u5ea6\u53cd\u9988\uff0c\u901a\u8fc7\u6807\u6ce8\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "LLMs\u5728\u7528\u6237\u53cd\u9988\u5206\u7c7b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u6570\u636e\u589e\u5f3a\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.08285", "pdf": "https://arxiv.org/pdf/2507.08285", "abs": "https://arxiv.org/abs/2507.08285", "authors": ["Gwanhyeong Koo", "Sunjae Yoon", "Younghwan Lee", "Ji Woo Hong", "Chang D. Yoo"], "title": "FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields", "categories": ["cs.GR", "cs.CV"], "comment": "ICML 2025 Spotlight", "summary": "Drag-based editing allows precise object manipulation through point-based\ncontrol, offering user convenience. However, current methods often suffer from\na geometric inconsistency problem by focusing exclusively on matching\nuser-defined points, neglecting the broader geometry and leading to artifacts\nor unstable edits. We propose FlowDrag, which leverages geometric information\nfor more accurate and coherent transformations. Our approach constructs a 3D\nmesh from the image, using an energy function to guide mesh deformation based\non user-defined drag points. The resulting mesh displacements are projected\ninto 2D and incorporated into a UNet denoising process, enabling precise\nhandle-to-target point alignment while preserving structural integrity.\nAdditionally, existing drag-editing benchmarks provide no ground truth, making\nit difficult to assess how accurately the edits match the intended\ntransformations. To address this, we present VFD (VidFrameDrag) benchmark\ndataset, which provides ground-truth frames using consecutive shots in a video\ndataset. FlowDrag outperforms existing drag-based editing methods on both VFD\nBench and DragBench.", "AI": {"tldr": "FlowDrag\u901a\u8fc7\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u6539\u8fdb\u62d6\u52a8\u7f16\u8f91\u7684\u7cbe\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u907f\u514d\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7528\u6237\u5b9a\u4e49\u70b9\u5bfc\u81f4\u7684\u51e0\u4f55\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u62d6\u52a8\u7f16\u8f91\u65b9\u6cd5\u56e0\u4ec5\u5173\u6ce8\u7528\u6237\u5b9a\u4e49\u70b9\u5339\u914d\u800c\u5ffd\u89c6\u6574\u4f53\u51e0\u4f55\uff0c\u5bfc\u81f4\u7f16\u8f91\u7ed3\u679c\u51fa\u73b0\u7455\u75b5\u6216\u4e0d\u7a33\u5b9a\u3002", "method": "FlowDrag\u901a\u8fc7\u6784\u5efa3D\u7f51\u683c\u5e76\u57fa\u4e8e\u80fd\u91cf\u51fd\u6570\u53d8\u5f62\uff0c\u5229\u7528UNet\u53bb\u566a\u8fc7\u7a0b\u6295\u5f71\u7f51\u683c\u4f4d\u79fb\u4ee5\u5b9e\u73b0\u7cbe\u786e\u70b9\u5bf9\u9f50\u3002", "result": "FlowDrag\u5728VFD\u548cDragBench\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlowDrag\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u4fe1\u606f\u63d0\u5347\u4e86\u62d6\u52a8\u7f16\u8f91\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5e26\u771f\u5b9e\u6570\u636e\u7684VFD\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2507.08759", "pdf": "https://arxiv.org/pdf/2507.08759", "abs": "https://arxiv.org/abs/2507.08759", "authors": ["Maximilian Dor\u00e9"], "title": "Dependent Multiplicities in Dependent Linear Type Theory", "categories": ["cs.PL", "cs.LO"], "comment": null, "summary": "We present a novel dependent linear type theory in which the multiplicity of\nsome variable - i.e., the number of times the variable can be used in a program\n- can depend on other variables. This allows us to give precise resource\nannotations to many higher-order functions that cannot be adequately typed in\nany other system. Inspired by the Dialectica translation, our typing discipline\nis obtained by embedding linear logic into dependent type theory and specifying\nhow the embedded logic interacts with the host theory. We can then use a\nstandard natural numbers type to obtain a quantitative typing system with\ndependent multiplicities. We characterise the semantics for our theory as a\ncombination of standard models of dependent type theory and linear logic. Our\nsystem can be added to any dependently typed language, which we demonstrate\nwith an implementation in Agda.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4f9d\u8d56\u7ebf\u6027\u7c7b\u578b\u7406\u8bba\uff0c\u652f\u6301\u53d8\u91cf\u7684\u591a\u91cd\u6027\u4f9d\u8d56\u4e8e\u5176\u4ed6\u53d8\u91cf\uff0c\u4ece\u800c\u4e3a\u9ad8\u9636\u51fd\u6570\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u8d44\u6e90\u6ce8\u91ca\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u4e3a\u8bb8\u591a\u9ad8\u9636\u51fd\u6570\u63d0\u4f9b\u8db3\u591f\u7684\u7c7b\u578b\u6ce8\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7c7b\u578b\u7406\u8bba\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u7ebf\u6027\u903b\u8f91\u5d4c\u5165\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\uff0c\u5e76\u6307\u5b9a\u5176\u4e0e\u5bbf\u4e3b\u7406\u8bba\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u4f9d\u8d56\u591a\u91cd\u6027\u7684\u5b9a\u91cf\u7c7b\u578b\u7cfb\u7edf\u3002", "result": "\u7cfb\u7edf\u5728Agda\u4e2d\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u4f9d\u8d56\u7c7b\u578b\u8bed\u8a00\uff0c\u5e76\u5177\u6709\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u548c\u7ebf\u6027\u903b\u8f91\u7684\u6807\u51c6\u6a21\u578b\u7ec4\u5408\u8bed\u4e49\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u9ad8\u9636\u51fd\u6570\u7684\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2507.08064", "pdf": "https://arxiv.org/pdf/2507.08064", "abs": "https://arxiv.org/abs/2507.08064", "authors": ["Yibo Lyu", "Rui Shao", "Gongwei Chen", "Yijie Zhu", "Weili Guan", "Liqiang Nie"], "title": "PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal Retrieval with Modality-Adaptive Learning", "categories": ["cs.MM", "cs.CV"], "comment": "Accepted to ACM MM 2025", "summary": "As multimedia content expands, the demand for unified multimodal retrieval\n(UMR) in real-world applications increases. Recent work leverages multimodal\nlarge language models (MLLMs) to tackle this task. However, their large\nparameter size results in high training costs and low inference efficiency. To\naddress this, we propose PUMA: a Layer-Pruned Language Model for Efficient\nUnified Multimodal Retrieval with Modality-Adaptive Learning. Our approach\nimproves UMR from both structural and learning perspectives. (1) Structurally,\nwe propose Layer-Pruned Self-Distillation, which prunes MLLMs by keeping only\nshallow layers while distilling features from dropped deep layers as teacher\nsignals. This reduces parameters and preserves representation capability. (2)\nOn the learning side, we introduce Modality-Adaptive Contrastive Learning Loss\n(MAC-Loss), which separates in-batch negatives into harder intra-modality and\neasier inter-modality groups based on the target modality, assigning different\ntemperature strategies to enhance learning efficiency. Experiments show our\nmethod significantly reduces resource usage while maintaining strong\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86PUMA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u526a\u679d\u548c\u6a21\u6001\u81ea\u9002\u5e94\u5b66\u4e60\u4f18\u5316\u7edf\u4e00\u591a\u6a21\u6001\u68c0\u7d22\uff0c\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMLLM\u7684\u7edf\u4e00\u591a\u6a21\u6001\u68c0\u7d22\u65b9\u6cd5\u53c2\u6570\u5e9e\u5927\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u63a8\u7406\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51fa\u5c42\u526a\u679d\u81ea\u84b8\u998f\uff08\u51cf\u5c11\u53c2\u6570\uff09\u548c\u6a21\u6001\u81ea\u9002\u5e94\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\uff08\u63d0\u5347\u5b66\u4e60\u6548\u7387\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u8d44\u6e90\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u6027\u80fd\u3002", "conclusion": "PUMA\u901a\u8fc7\u7ed3\u6784\u548c\u5b66\u4e60\u6539\u8fdb\uff0c\u4e3a\u9ad8\u6548\u7edf\u4e00\u591a\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.08119", "pdf": "https://arxiv.org/pdf/2507.08119", "abs": "https://arxiv.org/abs/2507.08119", "authors": ["Eric Ding", "Chuhan Ouyang", "Rachee Singh"], "title": "Photonic Rails in ML Datacenters", "categories": ["cs.NI"], "comment": null, "summary": "Rail-optimized network fabrics have become the de facto datacenter scale-out\nfabric for large-scale ML training. However, the use of high-radix electrical\nswitches to provide all-to-all connectivity in rails imposes massive power,\ncost, and complexity overheads. We propose a rethinking of the rail abstraction\nby retaining its communication semantics, but realizing it using optical\ncircuit switches. The key challenge is that optical switches support only\none-to-one connectivity at a time, limiting the fan-out of traffic in ML\nworkloads using hybrid parallelisms. We introduce parallelism-driven rail\nreconfiguration as a solution that leverages the sequential ordering between\ntraffic from different parallelisms. We design a control plane, Opus, to enable\ntime-multiplexed emulation of electrical rail switches using optical switches.\nMore broadly, our work discusses a new research agenda: datacenter fabrics that\nco-evolve with the model parallelism dimensions within each job, as opposed to\nthe prevailing mindset of reconfiguring networks before a job begins.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5149\u5b66\u7535\u8def\u5f00\u5173\u91cd\u65b0\u8bbe\u8ba1\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u9ad8\u57fa\u6570\u7535\u6c14\u5f00\u5173\u7684\u529f\u8017\u548c\u590d\u6742\u6027\u3002", "motivation": "\u9488\u5bf9\u4f20\u7edf\u7535\u6c14\u5f00\u5173\u5728\u9ad8\u57fa\u6570\u7f51\u7edc\u4e2d\u7684\u5de8\u5927\u529f\u8017\u3001\u6210\u672c\u548c\u590d\u6742\u6027\uff0c\u63a2\u7d22\u5149\u5b66\u5f00\u5173\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u5e76\u884c\u9a71\u52a8\u7684\u8f68\u9053\u91cd\u6784\u6280\u672f\uff0c\u8bbe\u8ba1\u63a7\u5236\u5e73\u9762Opus\uff0c\u901a\u8fc7\u65f6\u5206\u590d\u7528\u5149\u5b66\u5f00\u5173\u6a21\u62df\u7535\u6c14\u8f68\u9053\u5f00\u5173\u3002", "result": "\u5b9e\u73b0\u4e86\u5149\u5b66\u5f00\u5173\u5728\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u4e2d\u7684\u9ad8\u6548\u5e94\u7528\uff0c\u652f\u6301\u6df7\u5408\u5e76\u884cML\u4efb\u52a1\u3002", "conclusion": "\u672c\u6587\u4e3a\u6570\u636e\u4e2d\u5fc3\u7f51\u7edc\u67b6\u6784\u63d0\u51fa\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5373\u7f51\u7edc\u4e0e\u4efb\u52a1\u5e76\u884c\u7ef4\u5ea6\u534f\u540c\u6f14\u5316\uff0c\u800c\u975e\u9884\u5148\u914d\u7f6e\u3002"}}
{"id": "2507.08177", "pdf": "https://arxiv.org/pdf/2507.08177", "abs": "https://arxiv.org/abs/2507.08177", "authors": ["Arun Vignesh Malarkkan", "Haoyue Bai", "Xinyuan Wang", "Anjali Kaushik", "Dongjie Wang", "Yanjie Fu"], "title": "Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.NE", "F.2.2, I.2.7, I.2.4, I.2.1"], "comment": "5 pages, 1 figure, Under Review in Vision Paper Track-ACM SIGSPATIAL\n  2025", "summary": "As cyber-physical systems grow increasingly interconnected and spatially\ndistributed, ensuring their resilience against evolving cyberattacks has become\na critical priority. Spatio-Temporal Anomaly detection plays an important role\nin ensuring system security and operational integrity. However, current\ndata-driven approaches, largely driven by black-box deep learning, face\nchallenges in interpretability, adaptability to distribution shifts, and\nrobustness under evolving system dynamics. In this paper, we advocate for a\ncausal learning perspective to advance anomaly detection in spatially\ndistributed infrastructures that grounds detection in structural cause-effect\nrelationships. We identify and formalize three key directions: causal graph\nprofiling, multi-view fusion, and continual causal graph learning, each\noffering distinct advantages in uncovering dynamic cause-effect structures\nacross time and space. Drawing on real-world insights from systems such as\nwater treatment infrastructures, we illustrate how causal models provide early\nwarning signals and root cause attribution, addressing the limitations of\nblack-box detectors. Looking ahead, we outline the future research agenda\ncentered on multi-modality, generative AI-driven, and scalable adaptive causal\nframeworks. Our objective is to lay a new research trajectory toward scalable,\nadaptive, explainable, and spatially grounded anomaly detection systems. We\nhope to inspire a paradigm shift in cybersecurity research, promoting\ncausality-driven approaches to address evolving threats in interconnected\ninfrastructures.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u56e0\u679c\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u65f6\u7a7a\u5f02\u5e38\u68c0\u6d4b\uff0c\u89e3\u51b3\u5f53\u524d\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5982\u53ef\u89e3\u91ca\u6027\u4e0e\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e92\u8054\u6027\u589e\u5f3a\uff0c\u4f20\u7edf\u9ed1\u76d2\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u4e0a\u9762\u4e34\u6311\u6218\uff0c\u56e0\u679c\u5b66\u4e60\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u65b9\u5411\uff1a\u56e0\u679c\u56fe\u5206\u6790\u3001\u591a\u89c6\u89d2\u878d\u5408\u548c\u6301\u7eed\u56e0\u679c\u56fe\u5b66\u4e60\uff0c\u7ed3\u5408\u771f\u5b9e\u7cfb\u7edf\uff08\u5982\u6c34\u5229\u8bbe\u65bd\uff09\u9a8c\u8bc1\u3002", "result": "\u56e0\u679c\u6a21\u578b\u80fd\u63d0\u4f9b\u65e9\u671f\u9884\u8b66\u548c\u6839\u56e0\u5206\u6790\uff0c\u5f25\u8865\u9ed1\u76d2\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "conclusion": "\u5c55\u671b\u4e86\u591a\u6a21\u6001\u3001\u751f\u6210\u5f0fAI\u9a71\u52a8\u7684\u53ef\u6269\u5c55\u56e0\u679c\u6846\u67b6\uff0c\u63a8\u52a8\u53ef\u89e3\u91ca\u3001\u81ea\u9002\u5e94\u7684\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2507.08002", "pdf": "https://arxiv.org/pdf/2507.08002", "abs": "https://arxiv.org/abs/2507.08002", "authors": ["Karisa Parkington", "Bazen G. Teferra", "Marianne Rouleau-Tang", "Argyrios Perivolaris", "Alice Rueda", "Adam Dubrowski", "Bill Kapralos", "Reza Samavi", "Andrew Greenshaw", "Yanbo Zhang", "Bo Cao", "Yuqi Wu", "Sirisha Rambhatla", "Sridhar Krishnan", "Venkat Bhat"], "title": "Human vs. LLM-Based Thematic Analysis for Digital Mental Health Research: Proof-of-Concept Comparative Study", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Thematic analysis provides valuable insights into participants' experiences\nthrough coding and theme development, but its resource-intensive nature limits\nits use in large healthcare studies. Large language models (LLMs) can analyze\ntext at scale and identify key content automatically, potentially addressing\nthese challenges. However, their application in mental health interviews needs\ncomparison with traditional human analysis. This study evaluates out-of-the-box\nand knowledge-base LLM-based thematic analysis against traditional methods\nusing transcripts from a stress-reduction trial with healthcare workers.\nOpenAI's GPT-4o model was used along with the Role, Instructions, Steps,\nEnd-Goal, Narrowing (RISEN) prompt engineering framework and compared to human\nanalysis in Dedoose. Each approach developed codes, noted saturation points,\napplied codes to excerpts for a subset of participants (n = 20), and\nsynthesized data into themes. Outputs and performance metrics were compared\ndirectly. LLMs using the RISEN framework developed deductive parent codes\nsimilar to human codes, but humans excelled in inductive child code development\nand theme synthesis. Knowledge-based LLMs reached coding saturation with fewer\ntranscripts (10-15) than the out-of-the-box model (15-20) and humans (90-99).\nThe out-of-the-box LLM identified a comparable number of excerpts to human\nresearchers, showing strong inter-rater reliability (K = 0.84), though the\nknowledge-based LLM produced fewer excerpts. Human excerpts were longer and\ninvolved multiple codes per excerpt, while LLMs typically applied one code.\nOverall, LLM-based thematic analysis proved more cost-effective but lacked the\ndepth of human analysis. LLMs can transform qualitative analysis in mental\nhealthcare and clinical research when combined with human oversight to balance\nparticipant perspectives and research resources.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\u5728\u5fc3\u7406\u5065\u5eb7\u8bbf\u8c08\u4e2d\u7684\u4e3b\u9898\u5206\u6790\u6548\u679c\uff0c\u53d1\u73b0LLM\u5728\u6210\u672c\u6548\u76ca\u4e0a\u66f4\u4f18\uff0c\u4f46\u7f3a\u4e4f\u4eba\u5de5\u5206\u6790\u7684\u6df1\u5ea6\u3002", "motivation": "\u4e3b\u9898\u5206\u6790\u867d\u7136\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6d1e\u5bdf\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u533b\u7597\u7814\u7a76\u4e2d\u8d44\u6e90\u6d88\u8017\u5927\u3002LLM\u80fd\u591f\u81ea\u52a8\u5206\u6790\u6587\u672c\u5e76\u8bc6\u522b\u5173\u952e\u5185\u5bb9\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u5fc3\u7406\u5065\u5eb7\u8bbf\u8c08\u4e2d\u7684\u5e94\u7528\u9700\u8981\u4e0e\u4f20\u7edf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86OpenAI\u7684GPT-4o\u6a21\u578b\u548cRISEN\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\uff0c\u4e0e\u4f20\u7edf\u4eba\u5de5\u5206\u6790\uff08Dedoose\uff09\u8fdb\u884c\u6bd4\u8f83\u3002\u65b9\u6cd5\u5305\u62ec\u4ee3\u7801\u5f00\u53d1\u3001\u9971\u548c\u70b9\u8bb0\u5f55\u3001\u4ee3\u7801\u5e94\u7528\u5230\u6458\u5f55\u548c\u4e3b\u9898\u5408\u6210\u3002", "result": "LLM\u7684RISEN\u6846\u67b6\u5728\u6f14\u7ece\u6027\u7236\u4ee3\u7801\u5f00\u53d1\u4e0a\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u4f46\u4eba\u7c7b\u5728\u5f52\u7eb3\u6027\u5b50\u4ee3\u7801\u548c\u4e3b\u9898\u5408\u6210\u4e0a\u8868\u73b0\u66f4\u4f18\u3002\u77e5\u8bc6\u57fa\u7840\u7684LLM\u5728\u66f4\u5c11\u7684\u8f6c\u5f55\u672c\u4e2d\uff0810-15\uff09\u8fbe\u5230\u7f16\u7801\u9971\u548c\uff0c\u800c\u4eba\u7c7b\u9700\u8981\u66f4\u591a\uff0890-99\uff09\u3002", "conclusion": "LLM\u5728\u4e3b\u9898\u5206\u6790\u4e2d\u66f4\u5177\u6210\u672c\u6548\u76ca\uff0c\u4f46\u9700\u7ed3\u5408\u4eba\u7c7b\u76d1\u7763\u4ee5\u5e73\u8861\u53c2\u4e0e\u8005\u7684\u89c6\u89d2\u548c\u7814\u7a76\u8d44\u6e90\uff0c\u8fd9\u53ef\u80fd\u5728\u5fc3\u7406\u5065\u5eb7\u548c\u4e34\u5e8a\u7814\u7a76\u4e2d\u5e26\u6765\u53d8\u9769\u3002"}}
{"id": "2507.08581", "pdf": "https://arxiv.org/pdf/2507.08581", "abs": "https://arxiv.org/abs/2507.08581", "authors": ["Samuel Teuber", "Mattias Ulbrich", "Andr\u00e9 Platzer", "Bernhard Beckert"], "title": "Heterogeneous Dynamic Logic: Provability Modulo Program Theories", "categories": ["cs.LO", "cs.PL"], "comment": "49 pages, 4 figures", "summary": "Formally specifying, let alone verifying, properties of systems involving\nmultiple programming languages is inherently challenging. We introduce\nHeterogeneous Dynamic Logic (HDL), a framework for combining reasoning\nprinciples from distinct (dynamic) program logics in a modular and\ncompositional way. HDL mirrors the architecture of satisfiability modulo\ntheories (SMT): Individual dynamic logics, along with their calculi, are\ntreated as dynamic theories that can be flexibly combined to reason about\nheterogeneous systems whose components are verified using different program\nlogics. HDL provides two key operations: Lifting extends an individual dynamic\ntheory with new program constructs (e.g., the havoc operation or regular\nprograms) and automatically augments its calculus with sound reasoning\nprinciples for the new constructs; and Combination enables cross-language\nreasoning in a single modality via Heterogeneous Dynamic Theories, facilitating\nthe reuse of existing proof infrastructure. We formalize dynamic theories,\ntheir lifting and combination in Isabelle, and prove the soundness of all proof\nrules. We also prove relative completeness theorems for lifting and\ncombination: Under common assumptions, reasoning about lifted or combined\ntheories is no harder than reasoning about the constituent dynamic theories and\ntheir common first-order structure (i.e., the \"data theory\"). We demonstrate\nHDL's utility by verifying an automotive case study in which a Java controller\n(formalized in Java dynamic logic) steers a plant model (formalized in\ndifferential dynamic logic).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u52a8\u6001\u903b\u8f91\uff08HDL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u5757\u5316\u548c\u7ec4\u5408\u5f0f\u5730\u7ed3\u5408\u4e0d\u540c\u7f16\u7a0b\u903b\u8f91\u7684\u63a8\u7406\u539f\u5219\uff0c\u89e3\u51b3\u591a\u8bed\u8a00\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u95ee\u9898\u3002", "motivation": "\u591a\u7f16\u7a0b\u8bed\u8a00\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u89c4\u8303\u548c\u9a8c\u8bc1\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u7ec4\u5408\u4e0d\u540c\u7684\u52a8\u6001\u903b\u8f91\u3002", "method": "HDL\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u7406\u8bba\u7684\u63d0\u5347\u548c\u7ec4\u5408\u64cd\u4f5c\uff0c\u652f\u6301\u8de8\u8bed\u8a00\u63a8\u7406\uff0c\u5e76\u5728Isabelle\u4e2d\u5b9e\u73b0\u4e86\u5f62\u5f0f\u5316\u548c\u8bc1\u660e\u3002", "result": "\u8bc1\u660e\u4e86\u63d0\u5347\u548c\u7ec4\u5408\u64cd\u4f5c\u7684\u5b8c\u5907\u6027\uff0c\u5e76\u5728\u4e00\u4e2a\u6c7d\u8f66\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86HDL\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "HDL\u4e3a\u5f02\u6784\u7cfb\u7edf\u7684\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u548c\u7ec4\u5408\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08406", "pdf": "https://arxiv.org/pdf/2507.08406", "abs": "https://arxiv.org/abs/2507.08406", "authors": ["Weigang Feng", "Yijia Zhang", "Zekun Wang", "Zhengyang Wang", "Yi Wang", "Peijun Ma", "Ningyi Xu"], "title": "CCSS: Hardware-Accelerated RTL Simulation with Fast Combinational Logic Computing and Sequential Logic Synchronization", "categories": ["cs.AR", "cs.DC"], "comment": null, "summary": "As transistor counts in a single chip exceed tens of billions, the complexity\nof RTL-level simulation and verification has grown exponentially, often\nextending simulation campaigns to several months. In industry practice, RTL\nsimulation is divided into two phases: functional debug and system validation.\nWhile system validation demands high simulation speed and is typically\naccelerated using FPGAs, functional debug relies on rapid compilation-rendering\nmulti-core CPUs the primary choice. However, the limited simulation speed of\nCPUs has become a major bottleneck. To address this challenge, we propose CCSS,\na scalable multi-core RTL simulation platform that achieves both fast\ncompilation and high simulation throughput. CCSS accelerates combinational\nlogic computation and sequential logic synchronization through specialized\narchitecture and compilation strategies. It employs a balanced DAG partitioning\nmethod and efficient boolean computation cores for combinational logic, and\nadopts a low-latency network-on-chip (NoC) design to synchronize sequential\nstates across cores efficiently. Experimental results show that CCSS delivers\nup to 12.9x speedup over state-of-the-art multi-core simulators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCCSS\uff0c\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u6838RTL\u6a21\u62df\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3RTL\u7ea7\u6a21\u62df\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u7ec4\u5408\u903b\u8f91\u8ba1\u7b97\u548c\u987a\u5e8f\u903b\u8f91\u540c\u6b65\uff0c\u5b9e\u73b0\u5feb\u901f\u7f16\u8bd1\u548c\u9ad8\u4eff\u771f\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740\u5355\u82af\u7247\u6676\u4f53\u7ba1\u6570\u91cf\u8d85\u8fc7\u6570\u767e\u4ebf\uff0cRTL\u7ea7\u6a21\u62df\u548c\u9a8c\u8bc1\u7684\u590d\u6742\u6027\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u6a21\u62df\u5468\u671f\u53ef\u80fd\u957f\u8fbe\u6570\u6708\u3002\u5f53\u524d\u591a\u6838CPU\u7684\u6a21\u62df\u901f\u5ea6\u5df2\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002", "method": "CCSS\u91c7\u7528\u4e13\u95e8\u7684\u67b6\u6784\u548c\u7f16\u8bd1\u7b56\u7565\uff0c\u5305\u62ec\u5e73\u8861\u7684DAG\u5206\u533a\u65b9\u6cd5\u3001\u9ad8\u6548\u7684\u5e03\u5c14\u8ba1\u7b97\u6838\u5fc3\u4ee5\u53ca\u4f4e\u5ef6\u8fdf\u7684\u7247\u4e0a\u7f51\u7edc\uff08NoC\uff09\u8bbe\u8ba1\uff0c\u4ee5\u52a0\u901f\u7ec4\u5408\u903b\u8f91\u8ba1\u7b97\u548c\u987a\u5e8f\u903b\u8f91\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cCCSS\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u591a\u6838\u6a21\u62df\u5668\u5feb12.9\u500d\u3002", "conclusion": "CCSS\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u548c\u7f16\u8bd1\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86RTL\u6a21\u62df\u7684\u901f\u5ea6\u548c\u6548\u7387\uff0c\u4e3a\u884c\u4e1a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08283", "pdf": "https://arxiv.org/pdf/2507.08283", "abs": "https://arxiv.org/abs/2507.08283", "authors": ["Lingxi Cui", "Guanyu Jiang", "Huan Li", "Ke Chen", "Lidan Shou", "Gang Chen"], "title": "TableCopilot: A Table Assistant Empowered by Natural Language Conditional Table Discovery", "categories": ["cs.DB"], "comment": "Accepted by VLDB'25", "summary": "The rise of LLM has enabled natural language-based table assistants, but\nexisting systems assume users already have a well-formed table, neglecting the\nchallenge of table discovery in large-scale table pools. To address this, we\nintroduce TableCopilot, an LLM-powered assistant for interactive, precise, and\npersonalized table discovery and analysis. We define a novel scenario, nlcTD,\nwhere users provide both a natural language condition and a query table,\nenabling intuitive and flexible table discovery for users of all expertise\nlevels. To handle this, we propose Crofuma, a cross-fusion-based approach that\nlearns and aggregates single-modal and cross-modal matching scores.\nExperimental results show Crofuma outperforms SOTA single-input methods by at\nleast 12% on NDCG@5. We also release an instructional video, codebase,\ndatasets, and other resources on GitHub to encourage community contributions.\nTableCopilot sets a new standard for interactive table assistants, making\nadvanced table discovery accessible and integrated.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86TableCopilot\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4ea4\u4e92\u5f0f\u8868\u683c\u52a9\u624b\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u8868\u683c\u6c60\u4e2d\u7684\u8868\u683c\u53d1\u73b0\u96be\u9898\uff0c\u63d0\u51fa\u4e86nlcTD\u573a\u666f\u548cCrofuma\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u52a9\u624b\u7cfb\u7edf\u5047\u8bbe\u7528\u6237\u5df2\u62e5\u6709\u5b8c\u6574\u8868\u683c\uff0c\u5ffd\u89c6\u4e86\u5927\u89c4\u6a21\u8868\u683c\u6c60\u4e2d\u7684\u8868\u683c\u53d1\u73b0\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u4e2a\u6027\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86Crofuma\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u5339\u914d\u5206\u6570\u7684\u4ea4\u53c9\u878d\u5408\uff0c\u5b9e\u73b0\u9ad8\u6548\u8868\u683c\u53d1\u73b0\u3002", "result": "Crofuma\u5728NDCG@5\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u81f3\u5c11\u63d0\u534712%\u3002", "conclusion": "TableCopilot\u901a\u8fc7nlcTD\u573a\u666f\u548cCrofuma\u65b9\u6cd5\uff0c\u4e3a\u8868\u683c\u53d1\u73b0\u548c\u5206\u6790\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u4f7f\u5176\u66f4\u6613\u7528\u4e14\u96c6\u6210\u5316\u3002"}}
{"id": "2507.08190", "pdf": "https://arxiv.org/pdf/2507.08190", "abs": "https://arxiv.org/abs/2507.08190", "authors": ["Simon Johnson", "Raghunandan Makaram", "Amy Santoni", "Vinnie Scarlata"], "title": "Supporting Intel(r) SGX on Multi-Package Platforms", "categories": ["cs.DC", "cs.CR"], "comment": "8 pages, 6 figures", "summary": "Intel(r) Software Guard Extensions (SGX) was originally released on client\nplatforms and later extended to single socket server platforms. As developers\nhave become familiar with the capabilities of the technology, the applicability\nof this capability in the cloud has been tested. Various Cloud Service\nProviders (CSPs) are demonstrating the value of using SGX based Trusted\nExecution Environments (TEE) to create a new paradigm of Confidential Cloud\nComputing. This paper describes the additional platform enhancements we believe\nare necessary to deliver a user programmable Trusted Execution Environment that\nscales to cloud usages, performs and is secure on multi-package platforms.", "AI": {"tldr": "\u672c\u6587\u8ba8\u8bba\u4e86Intel SGX\u6280\u672f\u5728\u4e91\u5e73\u53f0\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u6269\u5c55\u5e73\u53f0\u529f\u80fd\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u652f\u6301\u53ef\u7f16\u7a0b\u7684\u53ef\u4fe1\u6267\u884c\u73af\u5883\uff08TEE\uff09\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5c06Intel SGX\u6280\u672f\u4ece\u5355\u63d2\u69fd\u670d\u52a1\u5668\u6269\u5c55\u5230\u591a\u63d2\u69fd\u4e91\u5e73\u53f0\uff0c\u4ee5\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u673a\u5bc6\u4e91\u8ba1\u7b97\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5206\u6790SGX\u5728\u4e91\u5e73\u53f0\u4e2d\u7684\u73b0\u6709\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u989d\u5916\u7684\u5e73\u53f0\u589e\u5f3a\u529f\u80fd\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u6027\u80fd\u548c\u5b89\u5168\u7684TEE\u3002", "result": "\u63d0\u51fa\u4e86\u589e\u5f3a\u5e73\u53f0\u529f\u80fd\u7684\u5efa\u8bae\uff0c\u4ee5\u652f\u6301\u4e91\u73af\u5883\u4e2d\u7684\u53ef\u7f16\u7a0bTEE\u3002", "conclusion": "Intel SGX\u6280\u672f\u5728\u4e91\u5e73\u53f0\u4e2d\u7684\u5e94\u7528\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u6269\u5c55\u529f\u80fd\u4ee5\u6ee1\u8db3\u4e91\u73af\u5883\u7684\u6269\u5c55\u6027\u548c\u5b89\u5168\u6027\u9700\u6c42\u3002"}}
{"id": "2507.08467", "pdf": "https://arxiv.org/pdf/2507.08467", "abs": "https://arxiv.org/abs/2507.08467", "authors": ["Youshuai Tan", "Zhanwei Zhang", "Jinfu Chen", "Zishuo Ding", "Jifeng Xuan", "Weiyi Shang"], "title": "Computing Floating-Point Errors by Injecting Perturbations", "categories": ["cs.SE"], "comment": "arXiv admin note: text overlap with arXiv:2412.20804", "summary": "Floating-point programs form the foundation of modern science and\nengineering, providing the essential computational framework for a wide range\nof applications, such as safety-critical systems, aerospace engineering, and\nfinancial analysis. Floating-point errors can lead to severe consequences.\nAlthough floating-point errors widely exist, only a subset of inputs may\ntrigger significant errors in floating-point programs. Therefore, it is crucial\nto determine whether a given input could produce such errors. Researchers tend\nto take the results of high-precision floating-point programs as oracles for\ndetecting floating-point errors, which introduces two main limitations: (1)\ndifficulty of implementation and (2) prolonged execution time. The two recent\ntools, ATOMU and FPCC, can partially address these issues. However, ATOMU\nsuffers from false positives; while FPCC, though eliminating false positives,\noperates at a considerably slower speed.\n  To address these two challenges, we propose a novel approach named\nPI-detector to computing floating-point errors effectively and efficiently. Our\napproach is based on the observation that floating-point errors stem from large\ncondition numbers in atomic operations (such as addition and subtraction),\nwhich then propagate and accumulate. PI-detector injects small perturbations\ninto the operands of individual atomic operations within the program and\ncompares the outcomes of the original program with the perturbed version to\ncompute floating-point errors. We evaluate PI-detector with datasets from ATOMU\nand HSED, as well as a complex linear system-solving program. Experimental\nresults demonstrate that PI-detector can perform efficient and accurate\nfloating-point error computation.", "AI": {"tldr": "PI-detector\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u6d6e\u70b9\u8bef\u5dee\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u539f\u5b50\u64cd\u4f5c\u6570\u6765\u8bc6\u522b\u8bef\u5dee\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5b9e\u73b0\u96be\u5ea6\u9ad8\u548c\u6267\u884c\u65f6\u95f4\u957f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6d6e\u70b9\u7a0b\u5e8f\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6d6e\u70b9\u8bef\u5dee\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982ATOMU\u548cFPCC\uff09\u5b58\u5728\u5047\u9633\u6027\u6216\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "method": "PI-detector\u901a\u8fc7\u5411\u539f\u5b50\u64cd\u4f5c\u6570\u6ce8\u5165\u5c0f\u6270\u52a8\uff0c\u5e76\u4e0e\u539f\u59cb\u7a0b\u5e8f\u7ed3\u679c\u5bf9\u6bd4\u6765\u8ba1\u7b97\u6d6e\u70b9\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPI-detector\u80fd\u591f\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u8fdb\u884c\u6d6e\u70b9\u8bef\u5dee\u8ba1\u7b97\u3002", "conclusion": "PI-detector\u4e3a\u6d6e\u70b9\u8bef\u5dee\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.08513", "pdf": "https://arxiv.org/pdf/2507.08513", "abs": "https://arxiv.org/abs/2507.08513", "authors": ["Liu He", "Xiao Zeng", "Yizhi Song", "Albert Y. C. Chen", "Lu Xia", "Shashwat Verma", "Sankalp Dayal", "Min Sun", "Cheng-Hao Kuo", "Daniel Aliaga"], "title": "Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) struggle with accurately capturing\ncamera-object relations, especially for object orientation, camera viewpoint,\nand camera shots. This stems from the fact that existing MLLMs are trained on\nimages with limited diverse camera-object relations and corresponding textual\ndescriptions. To address this, we propose a synthetic generation pipeline to\ncreate large-scale 3D visual instruction datasets. Our framework takes 3D\nassets as input and uses rendering and diffusion-based image generation models\nto create photorealistic images preserving precise camera-object relations.\nAdditionally, large language models (LLMs) are used to generate text prompts\nfor guiding visual instruction tuning and controlling image generation. We\ncreate Ultimate3D, a dataset of 240K VQAs with precise camera-object\nannotations, and corresponding benchmark. MLLMs fine-tuned on our proposed\ndataset outperform commercial models by a large margin, achieving an average\naccuracy improvement of 33.4% on camera-object relation recognition tasks. Our\ncode, dataset, and benchmark will contribute to broad MLLM applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u8d44\u4ea7\u548c\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u6355\u6349\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709MLLMs\u56e0\u8bad\u7ec3\u6570\u636e\u4e2d\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5728\u7269\u4f53\u65b9\u5411\u3001\u76f8\u673a\u89c6\u89d2\u548c\u62cd\u6444\u955c\u5934\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u7ed3\u54083D\u6e32\u67d3\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u63d0\u793a\u3002\u6700\u7ec8\u6784\u5efa\u4e86\u5305\u542b24\u4e07VQA\u6570\u636e\u7684Ultimate3D\u6570\u636e\u96c6\u3002", "result": "\u5728\u76f8\u673a-\u7269\u4f53\u5173\u7cfb\u8bc6\u522b\u4efb\u52a1\u4e0a\uff0c\u57fa\u4e8eUltimate3D\u8c03\u4f18\u7684MLLMs\u6bd4\u5546\u4e1a\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u4e8633.4%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u3001\u65b9\u6cd5\u548c\u57fa\u51c6\u6d4b\u8bd5\u6709\u52a9\u4e8e\u63a8\u52a8MLLMs\u5728\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.08796", "pdf": "https://arxiv.org/pdf/2507.08796", "abs": "https://arxiv.org/abs/2507.08796", "authors": ["Owen Lewis", "Neil Ghani", "Andrew Dudzik", "Christos Perivolaropoulos", "Razvan Pascanu", "Petar Veli\u010dkovi\u0107"], "title": "Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists", "categories": ["cs.PL", "cs.LG"], "comment": "18 pages, 2 figures", "summary": "What should a function that extrapolates beyond known input/output examples\nlook like? This is a tricky question to answer in general, as any function\nmatching the outputs on those examples can in principle be a correct\nextrapolant. We argue that a \"good\" extrapolant should follow certain kinds of\nrules, and here we study a particularly appealing criterion for rule-following\nin list functions: that the function should behave predictably even when\ncertain elements are removed. In functional programming, a standard way to\nexpress such removal operations is by using a filter function. Accordingly, our\npaper introduces a new semantic class of functions -- the filter equivariant\nfunctions. We show that this class contains interesting examples, prove some\nbasic theorems about it, and relate it to the well-known class of map\nequivariant functions. We also present a geometric account of filter\nequivariants, showing how they correspond naturally to certain simplicial\nstructures. Our highlight result is the amalgamation algorithm, which\nconstructs any filter-equivariant function's output by first studying how it\nbehaves on sublists of the input, in a way that extrapolates perfectly.", "AI": {"tldr": "\u6587\u7ae0\u63a2\u8ba8\u4e86\u5982\u4f55\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u5916\u63a8\u7684\u51fd\u6570\uff0c\u63d0\u51fa\u201c\u6ee4\u6ce2\u7b49\u53d8\u51fd\u6570\u201d\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7684\u8bed\u4e49\u7c7b\u522b\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u6027\u8d28\u3001\u7406\u8bba\u57fa\u7840\u548c\u51e0\u4f55\u89e3\u91ca\uff0c\u6700\u7ec8\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u7f8e\u7684\u5916\u63a8\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u51fd\u6570\u5728\u5916\u63a8\u65f6\u7684\u884c\u4e3a\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u51fd\u6570\u5728\u8f93\u5165\u53d8\u5316\u65f6\u7684\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u5f15\u5165\u201c\u6ee4\u6ce2\u7b49\u53d8\u51fd\u6570\u201d\u7c7b\u522b\uff0c\u7814\u7a76\u5176\u6027\u8d28\uff0c\u8bc1\u660e\u5176\u57fa\u672c\u5b9a\u7406\uff0c\u5e76\u4e0e\u5df2\u77e5\u7684\u201c\u6620\u5c04\u7b49\u53d8\u51fd\u6570\u201d\u7c7b\u522b\u8fdb\u884c\u5bf9\u6bd4\u3002\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u8be5\u51fd\u6570\u7c7b\u522b\u7684\u51e0\u4f55\u89e3\u91ca\u3002", "result": "\u63d0\u51fa\u201c\u878d\u5408\u7b97\u6cd5\u201d\uff0c\u901a\u8fc7\u7814\u7a76\u5b50\u5217\u8868\u884c\u4e3a\u6765\u5b8c\u7f8e\u6784\u9020\u6ee4\u6ce2\u7b49\u53d8\u51fd\u6570\u7684\u8f93\u51fa\u3002", "conclusion": "\u6ee4\u6ce2\u7b49\u53d8\u51fd\u6570\u4e3a\u5916\u63a8\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6846\u67b6\uff0c\u5176\u7406\u8bba\u6027\u8d28\u548c\u7b97\u6cd5\u5b9e\u73b0\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.08104", "pdf": "https://arxiv.org/pdf/2507.08104", "abs": "https://arxiv.org/abs/2507.08104", "authors": ["Michael Galarnyk", "Veer Kejriwal", "Agam Shah", "Yash Bhardwaj", "Nicholas Meyer", "Anand Krishnan", "Sudheer Chava"], "title": "VideoConviction: A Multimodal Benchmark for Human Conviction and Stock Market Recommendations", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Social media has amplified the reach of financial influencers known as\n\"finfluencers,\" who share stock recommendations on platforms like YouTube.\nUnderstanding their influence requires analyzing multimodal signals like tone,\ndelivery style, and facial expressions, which extend beyond text-based\nfinancial analysis. We introduce VideoConviction, a multimodal dataset with\n6,000+ expert annotations, produced through 457 hours of human effort, to\nbenchmark multimodal large language models (MLLMs) and text-based large\nlanguage models (LLMs) in financial discourse. Our results show that while\nmultimodal inputs improve stock ticker extraction (e.g., extracting Apple's\nticker AAPL), both MLLMs and LLMs struggle to distinguish investment actions\nand conviction--the strength of belief conveyed through confident delivery and\ndetailed reasoning--often misclassifying general commentary as definitive\nrecommendations. While high-conviction recommendations perform better than\nlow-conviction ones, they still underperform the popular S\\&P 500 index fund.\nAn inverse strategy--betting against finfluencer recommendations--outperforms\nthe S\\&P 500 by 6.8\\% in annual returns but carries greater risk (Sharpe ratio\nof 0.41 vs. 0.65). Our benchmark enables a diverse evaluation of multimodal\ntasks, comparing model performance on both full video and segmented video\ninputs. This enables deeper advancements in multimodal financial research. Our\ncode, dataset, and evaluation leaderboard are available under the CC BY-NC 4.0\nlicense.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u91d1\u878d\u7f51\u7ea2\uff08finfluencers\uff09\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u6570\u636e\u96c6VideoConviction\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u8f93\u5165\u63d0\u5347\u80a1\u7968\u4ee3\u7801\u63d0\u53d6\uff0c\u4f46\u6a21\u578b\u96be\u4ee5\u533a\u5206\u6295\u8d44\u884c\u4e3a\u548c\u4fe1\u5ff5\u5f3a\u5ea6\u3002", "motivation": "\u7814\u7a76\u91d1\u878d\u7f51\u7ea2\u7684\u591a\u6a21\u6001\u4fe1\u53f7\uff08\u5982\u8bed\u6c14\u3001\u8868\u60c5\uff09\u5bf9\u91d1\u878d\u8bdd\u8bed\u7684\u5f71\u54cd\uff0c\u586b\u8865\u6587\u672c\u5206\u6790\u4e0e\u591a\u6a21\u6001\u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u6784\u5efaVideoConviction\u6570\u636e\u96c6\uff086000+\u4e13\u5bb6\u6807\u6ce8\uff09\uff0c\u5bf9\u6bd4\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u4e0e\u57fa\u4e8e\u6587\u672c\u7684\u5927\u6a21\u578b\uff08LLMs\uff09\u3002", "result": "\u591a\u6a21\u6001\u8f93\u5165\u63d0\u5347\u80a1\u7968\u4ee3\u7801\u63d0\u53d6\uff0c\u4f46\u6a21\u578b\u96be\u4ee5\u8bc6\u522b\u4fe1\u5ff5\u5f3a\u5ea6\uff1b\u53cd\u5411\u7b56\u7565\u5e74\u56de\u62a5\u7387\u9ad8\u4e8eS&P 500\u4f46\u98ce\u9669\u66f4\u5927\u3002", "conclusion": "\u591a\u6a21\u6001\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u91d1\u878d\u7814\u7a76\u7684\u8fdb\u5c55\uff0c\u4f46\u9700\u6539\u8fdb\u6a21\u578b\u5bf9\u4fe1\u5ff5\u5f3a\u5ea6\u7684\u7406\u89e3\u3002"}}
{"id": "2507.08134", "pdf": "https://arxiv.org/pdf/2507.08134", "abs": "https://arxiv.org/abs/2507.08134", "authors": ["Minhu Wang", "Yixin Shen", "Bo Wang", "Haixuan Tong", "Yutong Xie", "Yixuan Gao", "Yan Liu", "Li Chen", "Mingwei Xu", "Jianping Wu"], "title": "Rattan: An Extensible and Scalable Modular Internet Path Emulator", "categories": ["cs.NI"], "comment": null, "summary": "The rapid growth of Internet paths in heterogeneity, scale, and dynamics has\nmade existing emulators increasingly insufficient in flexibility, scalability,\nand usability. To address these limitations, we present Rattan, an extensible\nand scalable software network path emulator for modern Internet conditions.\nRattan's core innovation lies in its cell-based architecture: by splitting\nemulation functions into modular \"cells\" with well-documented asynchronous\ninterfaces, users are allowed to easily compose different cells by\nhierarchically linking them and easily construct new cells by using standard\ncell interfaces. This design enables: (1) scalability, supporting hundreds of\nconcurrent gigabit-level paths on a single machine and cluster-level\nexperiments composed of multiple machines; (2) extensibility, simulating new\nnetwork conditions by constructing new cells. Rattan empowers developers and\nresearchers to efficiently and confidently evaluate, validate, and diagnose\ndiverse network transport innovations for online services.", "AI": {"tldr": "Rattan\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u4f38\u7f29\u7684\u8f6f\u4ef6\u7f51\u7edc\u8def\u5f84\u6a21\u62df\u5668\uff0c\u91c7\u7528\u6a21\u5757\u5316\u5355\u5143\u67b6\u6784\uff0c\u652f\u6301\u9ad8\u5e76\u53d1\u8def\u5f84\u548c\u96c6\u7fa4\u5b9e\u9a8c\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u4e92\u8054\u7f51\u6761\u4ef6\u3002", "motivation": "\u73b0\u6709\u6a21\u62df\u5668\u5728\u7075\u6d3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6613\u7528\u6027\u65b9\u9762\u65e0\u6cd5\u6ee1\u8db3\u4e92\u8054\u7f51\u8def\u5f84\u7684\u5feb\u901f\u589e\u957f\u9700\u6c42\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5355\u5143\u7684\u67b6\u6784\uff0c\u5c06\u6a21\u62df\u529f\u80fd\u62c6\u5206\u4e3a\u6a21\u5757\u5316\u5355\u5143\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u5c42\u6b21\u5316\u94fe\u63a5\u7ec4\u5408\u4e0d\u540c\u5355\u5143\u3002", "result": "\u652f\u6301\u5355\u673a\u4e0a\u7684\u6570\u767e\u6761\u5343\u5146\u7ea7\u5e76\u53d1\u8def\u5f84\u548c\u591a\u673a\u6784\u6210\u7684\u96c6\u7fa4\u5b9e\u9a8c\u3002", "conclusion": "Rattan\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u9760\u7684\u7f51\u7edc\u4f20\u8f93\u521b\u65b0\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2507.08286", "pdf": "https://arxiv.org/pdf/2507.08286", "abs": "https://arxiv.org/abs/2507.08286", "authors": ["Aufa Nasywa Rahman", "Bimo Sunarfri Hantono", "Guntur Dharma Putra"], "title": "TruChain: A Multi-Layer Architecture for Trusted, Verifiable, and Immutable Open Banking Data", "categories": ["cs.CR", "cs.ET"], "comment": "8 pages, 7 figures. Accepted to IEEE MetaCom 2025", "summary": "Open banking framework enables third party providers to access financial data\nacross banking institutions, leading to unprecedented innovations in the\nfinancial sector. However, some open banking standards remain susceptible to\nsevere technological risks, including unverified data sources, inconsistent\ndata integrity, and lack of immutability. In this paper, we propose a layered\narchitecture that provides assurance in data trustworthiness with three\ndistinct levels of trust, covering source validation, data-level\nauthentication, and tamper-proof storage. The first layer guarantees the source\nlegitimacy using decentralized identity and verifiable presentation, while the\nsecond layer verifies data authenticity and consistency using cryptographic\nsigning. Lastly, the third layer guarantees data immutability through the\nTangle, a directed acyclic graph distributed ledger. We implemented a\nproof-of-concept implementation of our solution to evaluate its performance,\nwhere the results demonstrate that the system scales linearly with a stable\nthroughput, exhibits a 100% validation rate, and utilizes under 35% of CPU and\n350 MiB memory. Compared to a real-world open banking implementation, our\nsolution offers significantly reduced latency and stronger data integrity\nassurance. Overall, our solution offers a practical and efficient system for\nsecure data sharing in financial ecosystems while maintaining regulatory\ncompliance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u67b6\u6784\uff0c\u901a\u8fc7\u4e09\u4e2a\u4fe1\u4efb\u5c42\u7ea7\u786e\u4fdd\u5f00\u653e\u94f6\u884c\u6846\u67b6\u4e2d\u6570\u636e\u7684\u53ef\u9760\u6027\uff0c\u5305\u62ec\u6765\u6e90\u9a8c\u8bc1\u3001\u6570\u636e\u8ba4\u8bc1\u548c\u9632\u7be1\u6539\u5b58\u50a8\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u5f00\u653e\u94f6\u884c\u6846\u67b6\u5b58\u5728\u6570\u636e\u6765\u6e90\u672a\u9a8c\u8bc1\u3001\u6570\u636e\u5b8c\u6574\u6027\u4e0d\u4e00\u81f4\u548c\u7f3a\u4e4f\u4e0d\u53ef\u7be1\u6539\u6027\u7b49\u6280\u672f\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u7b2c\u4e00\u5c42\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8eab\u4efd\u548c\u53ef\u9a8c\u8bc1\u5c55\u793a\u786e\u4fdd\u6765\u6e90\u5408\u6cd5\u6027\uff1b\u7b2c\u4e8c\u5c42\u4f7f\u7528\u52a0\u5bc6\u7b7e\u540d\u9a8c\u8bc1\u6570\u636e\u771f\u5b9e\u6027\u548c\u4e00\u81f4\u6027\uff1b\u7b2c\u4e09\u5c42\u901a\u8fc7Tangle\u5206\u5e03\u5f0f\u8d26\u672c\u5b9e\u73b0\u6570\u636e\u4e0d\u53ef\u7be1\u6539\u3002", "result": "\u6982\u5ff5\u9a8c\u8bc1\u663e\u793a\u7cfb\u7edf\u6027\u80fd\u7a33\u5b9a\uff0c\u541e\u5410\u91cf\u7ebf\u6027\u6269\u5c55\uff0c\u9a8c\u8bc1\u7387100%\uff0c\u8d44\u6e90\u5360\u7528\u4f4e\uff08CPU<35%\uff0c\u5185\u5b58<350MiB\uff09\uff0c\u6bd4\u5b9e\u9645\u5f00\u653e\u94f6\u884c\u5b9e\u73b0\u5ef6\u8fdf\u66f4\u4f4e\u3001\u6570\u636e\u5b8c\u6574\u6027\u66f4\u5f3a\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u91d1\u878d\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u9ad8\u6548\u4e14\u5408\u89c4\u7684\u5b89\u5168\u6570\u636e\u5171\u4eab\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08003", "pdf": "https://arxiv.org/pdf/2507.08003", "abs": "https://arxiv.org/abs/2507.08003", "authors": ["Kayhan Latifzadeh", "Jacek Gwizdka", "Luis A. Leiva"], "title": "A Versatile Dataset of Mouse and Eye Movements on Search Engine Results Pages", "categories": ["cs.HC", "cs.CV", "cs.IR"], "comment": null, "summary": "We contribute a comprehensive dataset to study user attention and purchasing\nbehavior on Search Engine Result Pages (SERPs). Previous work has relied on\nmouse movements as a low-cost large-scale behavioral proxy but also has relied\non self-reported ground-truth labels, collected at post-task, which can be\ninaccurate and prone to biases. To address this limitation, we use an eye\ntracker to construct an objective ground-truth of continuous visual attention.\nOur dataset comprises 2,776 transactional queries on Google SERPs, collected\nfrom 47 participants, and includes: (1) HTML source files, with CSS and images;\n(2) rendered SERP screenshots; (3) eye movement data; (4) mouse movement data;\n(5) bounding boxes of direct display and organic advertisements; and (6)\nscripts for further preprocessing the data. In this paper we provide an\noverview of the dataset and baseline experiments (classification tasks) that\ncan inspire researchers about the different possibilities for future work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u7528\u6237\u5728\u641c\u7d22\u5f15\u64ce\u7ed3\u679c\u9875\uff08SERPs\uff09\u4e0a\u7684\u6ce8\u610f\u529b\u548c\u8d2d\u4e70\u884c\u4e3a\uff0c\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u89e3\u51b3\u4e86\u4ee5\u5f80\u4f9d\u8d56\u9f20\u6807\u79fb\u52a8\u548c\u81ea\u6211\u62a5\u544a\u6570\u636e\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u4ee5\u5f80\u7814\u7a76\u4f9d\u8d56\u9f20\u6807\u79fb\u52a8\u548c\u81ea\u6211\u62a5\u544a\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u5ba2\u89c2\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u6570\u636e\u3002", "method": "\u4f7f\u7528\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u6536\u96c6\u6570\u636e\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2,776\u4e2a\u67e5\u8be2\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6HTML\u6e90\u7801\u3001\u622a\u56fe\u3001\u773c\u52a8\u548c\u9f20\u6807\u6570\u636e\u7b49\u3002", "result": "\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u6570\u636e\u96c6\u548c\u9884\u5904\u7406\u811a\u672c\uff0c\u5e76\u901a\u8fc7\u57fa\u7ebf\u5b9e\u9a8c\u5c55\u793a\u4e86\u672a\u6765\u7814\u7a76\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u7814\u7a76\u7528\u6237\u884c\u4e3a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\uff0c\u5e76\u6fc0\u53d1\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.08701", "pdf": "https://arxiv.org/pdf/2507.08701", "abs": "https://arxiv.org/abs/2507.08701", "authors": ["Ricardo Contreras", "Filip Smola", "Nu\u0161a Fari\u010d", "Jiawei Zheng", "Jane Hillston", "Jacques D. Fleuriot"], "title": "A Personalised Formal Verification Framework for Monitoring Activities of Daily Living of Older Adults Living Independently in Their Homes", "categories": ["cs.LO", "cs.AI", "cs.CY"], "comment": "19 pages, 6 figures", "summary": "There is an imperative need to provide quality of life to a growing\npopulation of older adults living independently. Personalised solutions that\nfocus on the person and take into consideration their preferences and context\nare key. In this work, we introduce a framework for representing and reasoning\nabout the Activities of Daily Living of older adults living independently at\nhome. The framework integrates data from sensors and contextual information\nthat aggregates semi-structured interviews, home layouts and sociological\nobservations from the participants. We use these data to create formal models,\npersonalised for each participant according to their preferences and context.\nWe formulate requirements that are specific to each individual as properties\nencoded in Linear Temporal Logic and use a model checker to verify whether each\nproperty is satisfied by the model. When a property is violated, a\ncounterexample is generated giving the cause of the violation. We demonstrate\nthe framework's generalisability by applying it to different participants,\nhighlighting its potential to enhance the safety and well-being of older adults\nageing in place.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u548c\u9a8c\u8bc1\u72ec\u7acb\u751f\u6d3b\u8001\u5e74\u4eba\u7684\u65e5\u5e38\u6d3b\u52a8\uff0c\u7ed3\u5408\u4f20\u611f\u5668\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u6a21\u578b\u548c\u903b\u8f91\u9a8c\u8bc1\u63d0\u5347\u5176\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u72ec\u7acb\u751f\u6d3b\u7684\u8001\u5e74\u4eba\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u751f\u6d3b\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\uff0c\u8003\u8651\u5176\u504f\u597d\u548c\u4e0a\u4e0b\u6587\u3002", "method": "\u6574\u5408\u4f20\u611f\u5668\u6570\u636e\u3001\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u3001\u5bb6\u5ead\u5e03\u5c40\u548c\u793e\u4f1a\u5b66\u89c2\u5bdf\uff0c\u6784\u5efa\u4e2a\u6027\u5316\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u9a8c\u8bc1\u8981\u6c42\u3002", "result": "\u6846\u67b6\u5c55\u793a\u4e86\u901a\u7528\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u5e76\u89e3\u91ca\u6a21\u578b\u4e2d\u7684\u8fdd\u89c4\u884c\u4e3a\uff0c\u63d0\u5347\u8001\u5e74\u4eba\u7684\u5b89\u5168\u548c\u798f\u7949\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u652f\u6301\u72ec\u7acb\u751f\u6d3b\u7684\u8001\u5e74\u4eba\uff0c\u4e3a\u5176\u63d0\u4f9b\u5b89\u5168\u548c\u798f\u7949\u7684\u4fdd\u969c\u3002"}}
{"id": "2507.08658", "pdf": "https://arxiv.org/pdf/2507.08658", "abs": "https://arxiv.org/abs/2507.08658", "authors": ["Robert B. Kent", "Marios S. Pattichis"], "title": "Fast and Efficient Merge of Sorted Input Lists in Hardware Using List Offset Merge Sorters", "categories": ["cs.AR", "cs.DS", "eess.IV"], "comment": null, "summary": "A new set of hardware merge sort devices are introduced here, which merge\nmultiple sorted input lists into a single sorted output list in a fast and\nefficient manner. In each merge sorter, the values from the sorted input lists\nare arranged in an input 2-D setup array, but with the order of each sorted\ninput list offset from the order of each of the other sorted input lists. In\nthese new devices, called List Offset Merge Sorters (LOMS), a minimal set of\ncolumn sort stages alternating with row sort stages process the input setup\narray into a final output array, now in the defined sorted order. LOMS 2-way\nsorters, which merge 2 sorted input lists, require only 2 merge stages and are\nsignificantly faster than Kenneth Batcher's previous state-of-the-art 2-way\nmerge devices, Bitonic Merge Sorters and Odd-Even Merge Sorters. LOMS 2-way\nsorters utilize the recently-introduced Single-Stage 2-way Merge Sorters (S2MS)\nin their first stage. Both LOMS and S2MS devices can merge any mixture of input\nlist sizes, while Batcher's merge sorters are difficult to design unless the 2\ninput lists are equal, and a power-of-2. By themselves, S2MS devices are the\nfastest 2-way merge sorters when implemented in this study's target FPGA\ndevices, but they tend to use a large number of LUT resources. LOMS 2-way\ndevices use fewer resources than comparable S2MS devices, enabling some large\nLOMS devices to be implemented in a given FPGA when comparable S2MS devices\ncannot fit in that FPGA. A List Offset 2-way sorter merges 2 lists, each with\n32 values, into a sorted output list of those 64 values in 2.24 nS, a speedup\nof 2.63 versus a comparable Batcher device. A LOMS 3-way merge sorter, merging\n3 sorted input lists with 7 values, fully merges the 21 values in 3.4 nS, a\nspeedup of 1.36 versus the comparable state-of-the-art 3-way merge device.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u65b0\u578b\u786c\u4ef6\u5408\u5e76\u6392\u5e8f\u8bbe\u5907LOMS\uff0c\u901a\u8fc7\u504f\u79fb\u6392\u5e8f\u8f93\u5165\u5217\u8868\u548c\u4f7f\u7528\u5217/\u884c\u6392\u5e8f\u9636\u6bb5\uff0c\u9ad8\u6548\u5408\u5e76\u591a\u4e2a\u6392\u5e8f\u5217\u8868\uff0c\u6bd4\u73b0\u6709\u8bbe\u5907\u66f4\u5feb\u4e14\u66f4\u8282\u7701\u8d44\u6e90\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5408\u5e76\u6392\u5e8f\u8bbe\u5907\u5728\u901f\u5ea6\u548c\u8d44\u6e90\u4f7f\u7528\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u786c\u4ef6\u5b9e\u73b0\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8f93\u5165\u5217\u8868\u7684\u504f\u79fb\u6392\u5217\u548c\u5217/\u884c\u6392\u5e8f\u9636\u6bb5\u7684\u4ea4\u66ff\u5904\u7406\uff0c\u8bbe\u8ba1\u51faLOMS\u8bbe\u5907\uff0c\u7279\u522b\u662f2-way\u548c3-way\u5408\u5e76\u6392\u5e8f\u5668\u3002", "result": "LOMS 2-way\u8bbe\u5907\u5408\u5e7632\u4e2a\u503c\u7684\u5217\u8868\u4ec5\u97002.24 nS\uff0c\u901f\u5ea6\u63d0\u53472.63\u500d\uff1b3-way\u8bbe\u5907\u5408\u5e7621\u4e2a\u503c\u4ec5\u97003.4 nS\uff0c\u901f\u5ea6\u63d0\u53471.36\u500d\u3002", "conclusion": "LOMS\u8bbe\u5907\u5728\u901f\u5ea6\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8eFPGA\u5b9e\u73b0\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.08432", "pdf": "https://arxiv.org/pdf/2507.08432", "abs": "https://arxiv.org/abs/2507.08432", "authors": ["Gustavo Correa Publio", "Jos\u00e9 Emilio Labra Gayo"], "title": "xpSHACL: Explainable SHACL Validation using Retrieval-Augmented Generation and Large Language Models", "categories": ["cs.DB", "cs.CL"], "comment": "Accepted for publication in the 2nd LLM+Graph Workshop, colocated at\n  VLDB'25", "summary": "Shapes Constraint Language (SHACL) is a powerful language for validating RDF\ndata. Given the recent industry attention to Knowledge Graphs (KGs), more users\nneed to validate linked data properly. However, traditional SHACL validation\nengines often provide terse reports in English that are difficult for\nnon-technical users to interpret and act upon. This paper presents xpSHACL, an\nexplainable SHACL validation system that addresses this issue by combining\nrule-based justification trees with retrieval-augmented generation (RAG) and\nlarge language models (LLMs) to produce detailed, multilanguage, human-readable\nexplanations for constraint violations. A key feature of xpSHACL is its usage\nof a Violation KG to cache and reuse explanations, improving efficiency and\nconsistency.", "AI": {"tldr": "xpSHACL\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684SHACL\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u89c4\u5219\u5316\u7684\u7406\u7531\u6811\u3001RAG\u548cLLM\uff0c\u4e3a\u7ea6\u675f\u8fdd\u89c4\u63d0\u4f9b\u8be6\u7ec6\u3001\u591a\u8bed\u8a00\u3001\u6613\u8bfb\u7684\u89e3\u91ca\u3002", "motivation": "\u4f20\u7edfSHACL\u9a8c\u8bc1\u5f15\u64ce\u7684\u62a5\u544a\u901a\u5e38\u6666\u6da9\u96be\u61c2\uff0c\u975e\u6280\u672f\u7528\u6237\u96be\u4ee5\u7406\u89e3\u548c\u4f7f\u7528\uff0cxpSHACL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u89c4\u5219\u5316\u7684\u7406\u7531\u6811\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5e76\u5229\u7528\u8fdd\u89c4\u77e5\u8bc6\u56fe\u8c31\uff08Violation KG\uff09\u7f13\u5b58\u548c\u91cd\u7528\u89e3\u91ca\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u8be6\u7ec6\u3001\u591a\u8bed\u8a00\u3001\u6613\u8bfb\u7684\u8fdd\u89c4\u89e3\u91ca\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "xpSHACL\u901a\u8fc7\u6539\u8fdb\u89e3\u91ca\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u6280\u672f\u7528\u6237\u5bf9SHACL\u9a8c\u8bc1\u62a5\u544a\u7684\u7406\u89e3\u548c\u4f7f\u7528\u80fd\u529b\u3002"}}
{"id": "2507.08281", "pdf": "https://arxiv.org/pdf/2507.08281", "abs": "https://arxiv.org/abs/2507.08281", "authors": ["Ahmad Zaki Akmal", "Azkario Rizky Pratama", "Guntur Dharma Putra"], "title": "Fast and Interactive Byzantine Fault-tolerant Web Services via Session-Based Consensus Decoupling", "categories": ["cs.DC"], "comment": "6 pages, 5 figures. Accepted to IEEE MetaCom 2025 as a short paper", "summary": "Byzantine fault-tolerant (BFT) web services provide critical integrity\nguarantees for distributed applications but face significant latency challenges\nthat hinder interactive user experiences. We propose a novel two-layer\narchitecture that addresses this fundamental tension between security and\nresponsiveness in BFT systems. Our approach introduces a session-aware\ntransaction buffer layer (Layer 2) that delivers immediate feedback to users\nthrough consensus simulation, while periodically committing batched operations\nto a fully Byzantine fault-tolerant consensus layer (Layer 1). By separating\ninteractive operations from consensus finalization, our system achieves\nresponsive user experiences of under 200ms, while maintaining strong BFT\nsecurity guarantees. We demonstrate the efficacy of our architecture through a\nsupply chain management implementation, where operators require both immediate\nfeedback during multi-step workflows and tamper-proof record keeping. Our\nevaluation shows that our Layer 2 operations perform four times faster than the\nLayer 1 counterpart, while substantially preserving the end-to-end transaction\nintegrity. Our approach enables BFT applications in domains previously\nconsidered impractical due to latency constraints, such as metaverse\nenvironments, where users require both responsive interaction and guaranteed\nstate consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u5c42\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u79bb\u4ea4\u4e92\u64cd\u4f5c\u4e0e\u5171\u8bc6\u786e\u8ba4\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u5b89\u5168\u6027\u7684BFT\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3BFT\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u54cd\u5e94\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u652f\u6301\u9700\u8981\u5373\u65f6\u53cd\u9988\u548c\u9632\u7be1\u6539\u8bb0\u5f55\u7684\u9886\u57df\u3002", "method": "\u91c7\u7528\u4f1a\u8bdd\u611f\u77e5\u7684\u4ea4\u6613\u7f13\u51b2\u5c42\uff08Layer 2\uff09\u6a21\u62df\u5171\u8bc6\u63d0\u4f9b\u5373\u65f6\u53cd\u9988\uff0c\u5b9a\u671f\u63d0\u4ea4\u6279\u91cf\u64cd\u4f5c\u5230BFT\u5171\u8bc6\u5c42\uff08Layer 1\uff09\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0200ms\u4ee5\u4e0b\u7684\u7528\u6237\u54cd\u5e94\uff0cLayer 2\u64cd\u4f5c\u901f\u5ea6\u662fLayer 1\u76844\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4ea4\u6613\u5b8c\u6574\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3aBFT\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u7684\u5e94\u7528\u9886\u57df\uff0c\u5982\u9700\u8981\u9ad8\u54cd\u5e94\u6027\u548c\u72b6\u6001\u4e00\u81f4\u6027\u7684\u5143\u5b87\u5b99\u73af\u5883\u3002"}}
{"id": "2507.08523", "pdf": "https://arxiv.org/pdf/2507.08523", "abs": "https://arxiv.org/abs/2507.08523", "authors": ["Yilun Wang", "Pengfei Chen", "Haiyu Huang", "Zilong He", "Gou Tan", "Chuanfu Zhang", "Jingkai He", "Zibin Zheng"], "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching", "categories": ["cs.SE"], "comment": null, "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.", "AI": {"tldr": "InferLog\u662f\u4e00\u79cd\u9488\u5bf9\u5728\u7ebf\u65e5\u5fd7\u89e3\u6790\u7684LLM\u63a8\u7406\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65e5\u5fd7\u89e3\u6790\u5668\u5728\u9ad8\u5e76\u53d1\u8bf7\u6c42\u4e0b\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u6790\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u751f\u6210\u5927\u91cf\u8fd0\u884c\u65f6\u65e5\u5fd7\uff0c\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u65e5\u5fd7\u89e3\u6790\u4ee5\u652f\u6301\u5f02\u5e38\u68c0\u6d4b\u7b49\u4efb\u52a1\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u89e3\u6790\u65b9\u6cd5\u56e0\u9690\u79c1\u548c\u9ad8\u5ef6\u8fdf\u95ee\u9898\u65e0\u6cd5\u6ee1\u8db3\u751f\u4ea7\u73af\u5883\u9700\u6c42\u3002", "method": "InferLog\u901a\u8fc7\u524d\u7f00\u611f\u77e5\u7684ICL\u4f18\u5316\u7b56\u7565\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u5feb\u901f\u914d\u7f6e\u8c03\u6574\u7ba1\u9053\u6765\u52a0\u901fLLM\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInferLog\u5728Loghub\u6570\u636e\u96c6\u548cvLLM\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\uff0c\u4e14\u4e0d\u964d\u4f4e\u89e3\u6790\u51c6\u786e\u6027\u3002", "conclusion": "InferLog\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u6548\u7387\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u9ad8\u541e\u5410\u65e5\u5fd7\u89e3\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08590", "pdf": "https://arxiv.org/pdf/2507.08590", "abs": "https://arxiv.org/abs/2507.08590", "authors": ["Junyu Chen", "Yihua Gao", "Mingyong Li"], "title": "Visual Semantic Description Generation with MLLMs for Image-Text Matching", "categories": ["cs.MM", "cs.CV"], "comment": "Accepted by ICME2025 oral", "summary": "Image-text matching (ITM) aims to address the fundamental challenge of\naligning visual and textual modalities, which inherently differ in their\nrepresentations, continuous, high-dimensional image features vs. discrete,\nstructured text. We propose a novel framework that bridges the modality gap by\nleveraging multimodal large language models (MLLMs) as visual semantic parsers.\nBy generating rich Visual Semantic Descriptions (VSD), MLLMs provide semantic\nanchor that facilitate cross-modal alignment. Our approach combines: (1)\nInstance-level alignment by fusing visual features with VSD to enhance the\nlinguistic expressiveness of image representations, and (2) Prototype-level\nalignment through VSD clustering to ensure category-level consistency. These\nmodules can be seamlessly integrated into existing ITM models. Extensive\nexperiments on Flickr30K and MSCOCO demonstrate substantial performance\nimprovements. The approach also exhibits remarkable zero-shot generalization to\ncross-domain tasks, including news and remote sensing ITM. The code and model\ncheckpoints are available at https://github.com/Image-Text-Matching/VSD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf-\u6587\u672c\u5339\u914d\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f5c\u4e3a\u89c6\u89c9\u8bed\u4e49\u89e3\u6790\u5668\uff0c\u901a\u8fc7\u751f\u6210\u4e30\u5bcc\u7684\u89c6\u89c9\u8bed\u4e49\u63cf\u8ff0\uff08VSD\uff09\u6765\u5f25\u5408\u8de8\u6a21\u6001\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u56fe\u50cf\u548c\u6587\u672c\u6a21\u6001\u5728\u8868\u793a\u4e0a\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff08\u8fde\u7eed\u9ad8\u7ef4\u56fe\u50cf\u7279\u5f81\u4e0e\u79bb\u6563\u7ed3\u6784\u5316\u6587\u672c\uff09\uff0c\u4f20\u7edf\u7684\u56fe\u50cf-\u6587\u672c\u5339\u914d\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5bf9\u9f50\u5b83\u4eec\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u548c\u539f\u578b\u7ea7\u5bf9\u9f50\u7684\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u89c6\u89c9\u7279\u5f81\u4e0eVSD\u878d\u5408\u589e\u5f3a\u56fe\u50cf\u8868\u793a\u7684\u8bed\u4e49\u8868\u8fbe\u80fd\u529b\uff1b2\uff09\u901a\u8fc7VSD\u805a\u7c7b\u5b9e\u73b0\u7c7b\u522b\u7ea7\u5bf9\u9f50\u3002", "result": "\u5728Flickr30K\u548cMSCOCO\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u8de8\u57df\u4efb\u52a1\uff08\u5982\u65b0\u95fb\u548c\u9065\u611f\u56fe\u50cf\uff09\u4e2d\u8868\u73b0\u51fa\u4f18\u79c0\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7MLLMs\u63d0\u4f9b\u7684\u8bed\u4e49\u951a\u70b9\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf-\u6587\u672c\u5339\u914d\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u5de5\u5177\u3002"}}
{"id": "2507.08164", "pdf": "https://arxiv.org/pdf/2507.08164", "abs": "https://arxiv.org/abs/2507.08164", "authors": ["Yun Tang", "Mengbang Zou", "Zeinab Nezami", "Syed Ali Raza Zaidi", "Weisi Guo"], "title": "KP-A: A Unified Network Knowledge Plane for Catalyzing Agentic Network Intelligence", "categories": ["cs.NI", "cs.AI", "cs.SE"], "comment": "7 pages, 5 figures, submitted for possible publication", "summary": "The emergence of large language models (LLMs) and agentic systems is enabling\nautonomous 6G networks with advanced intelligence, including\nself-configuration, self-optimization, and self-healing. However, the current\nimplementation of individual intelligence tasks necessitates isolated knowledge\nretrieval pipelines, resulting in redundant data flows and inconsistent\ninterpretations. Inspired by the service model unification effort in Open-RAN\n(to support interoperability and vendor diversity), we propose KP-A: a unified\nNetwork Knowledge Plane specifically designed for Agentic network intelligence.\nBy decoupling network knowledge acquisition and management from intelligence\nlogic, KP-A streamlines development and reduces maintenance complexity for\nintelligence engineers. By offering an intuitive and consistent knowledge\ninterface, KP-A also enhances interoperability for the network intelligence\nagents. We demonstrate KP-A in two representative intelligence tasks: live\nnetwork knowledge Q&A and edge AI service orchestration. All implementation\nartifacts have been open-sourced to support reproducibility and future\nstandardization efforts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86KP-A\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u7f51\u7edc\u77e5\u8bc6\u5e73\u9762\uff0c\u65e8\u5728\u4f18\u5316\u81ea\u4e3b6G\u7f51\u7edc\u4e2d\u7684\u77e5\u8bc6\u7ba1\u7406\u548c\u4ee3\u7406\u667a\u80fd\u4efb\u52a1\uff0c\u51cf\u5c11\u5197\u4f59\u6570\u636e\u6d41\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d6G\u7f51\u7edc\u4e2d\u72ec\u7acb\u7684\u667a\u80fd\u4efb\u52a1\u5b9e\u73b0\u5bfc\u81f4\u77e5\u8bc6\u68c0\u7d22\u5197\u4f59\u548c\u89e3\u91ca\u4e0d\u4e00\u81f4\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7ba1\u7406\u6846\u67b6\u3002", "method": "\u63d0\u51faKP-A\u6846\u67b6\uff0c\u5c06\u7f51\u7edc\u77e5\u8bc6\u83b7\u53d6\u4e0e\u7ba1\u7406\u4e0e\u667a\u80fd\u903b\u8f91\u5206\u79bb\uff0c\u63d0\u4f9b\u4e00\u81f4\u7684\u77e5\u8bc6\u63a5\u53e3\uff0c\u652f\u6301\u4ee3\u7406\u667a\u80fd\u3002", "result": "KP-A\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u65f6\u7f51\u7edc\u77e5\u8bc6\u95ee\u7b54\u548c\u8fb9\u7f18AI\u670d\u52a1\u7f16\u6392\uff0c\u5f00\u6e90\u5b9e\u73b0\u652f\u6301\u53ef\u590d\u73b0\u6027\u548c\u672a\u6765\u6807\u51c6\u5316\u3002", "conclusion": "KP-A\u901a\u8fc7\u7edf\u4e00\u77e5\u8bc6\u7ba1\u7406\u7b80\u5316\u4e866G\u4ee3\u7406\u667a\u80fd\u7684\u5f00\u53d1\u4e0e\u7ef4\u62a4\uff0c\u63d0\u5347\u4e86\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2507.08312", "pdf": "https://arxiv.org/pdf/2507.08312", "abs": "https://arxiv.org/abs/2507.08312", "authors": ["Jesus Lopez", "Viviana Cadena", "Mohammad Saidur Rahman"], "title": "Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained Devices", "categories": ["cs.CR", "cs.ET"], "comment": "8 pages, 4 figures, 4 tables. This paper is accepted at the IEEE\n  Quantum Week 2025 -- IEEE International Conference on Quantum Computing and\n  Engineering (QCE) 2025", "summary": "The rapid advancement of quantum computing poses a critical threat to\nclassical cryptographic algorithms such as RSA and ECC, particularly in\nInternet of Things (IoT) devices, where secure communication is essential but\noften constrained by limited computational resources. This paper investigates\nthe feasibility of deploying post-quantum cryptography (PQC) algorithms on\nresource-constrained devices. In particular, we implement three PQC algorithms\n-- BIKE, CRYSTALS-Kyber, and HQC -- on a lightweight IoT platform built with\nRaspberry Pi devices. Leveraging the Open Quantum Safe (\\texttt{liboqs})\nlibrary in conjunction with \\texttt{mbedTLS}, we develop quantum-secure key\nexchange protocols, and evaluate their performance in terms of computational\noverhead, memory usage, and energy consumption for quantum secure\ncommunication. Experimental results demonstrate that the integration of PQC\nalgorithms on constrained hardware is practical, reinforcing the urgent need\nfor quantum-resilient cryptographic frameworks in next-generation IoT devices.\nThe implementation of this paper is available at\nhttps://iqsec-lab.github.io/PQC-IoT/.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684IoT\u8bbe\u5907\u4e0a\u90e8\u7f72\u540e\u91cf\u5b50\u5bc6\u7801\uff08PQC\uff09\u7b97\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5728Raspberry Pi\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u4e09\u79cdPQC\u7b97\u6cd5\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u7684\u5feb\u901f\u53d1\u5c55\u5bf9\u4f20\u7edf\u52a0\u5bc6\u7b97\u6cd5\uff08\u5982RSA\u548cECC\uff09\u6784\u6210\u5a01\u80c1\uff0c\u5c24\u5176\u662f\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684IoT\u8bbe\u5907\u4e2d\u3002", "method": "\u5728Raspberry Pi\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86BIKE\u3001CRYSTALS-Kyber\u548cHQC\u4e09\u79cdPQC\u7b97\u6cd5\uff0c\u5229\u7528Open Quantum Safe\u5e93\u548cmbedTLS\u5f00\u53d1\u91cf\u5b50\u5b89\u5168\u5bc6\u94a5\u4ea4\u6362\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPQC\u7b97\u6cd5\u5728\u53d7\u9650\u786c\u4ef6\u4e0a\u7684\u96c6\u6210\u662f\u53ef\u884c\u7684\uff0c\u8ba1\u7b97\u5f00\u9500\u3001\u5185\u5b58\u4f7f\u7528\u548c\u80fd\u8017\u5728\u5408\u7406\u8303\u56f4\u5185\u3002", "conclusion": "\u4e0b\u4e00\u4ee3IoT\u8bbe\u5907\u9700\u91c7\u7528\u91cf\u5b50\u5b89\u5168\u7684\u52a0\u5bc6\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u91cf\u5b50\u8ba1\u7b97\u5e26\u6765\u7684\u5a01\u80c1\u3002"}}
{"id": "2507.08028", "pdf": "https://arxiv.org/pdf/2507.08028", "abs": "https://arxiv.org/abs/2507.08028", "authors": ["Evgenii Rudakov", "Jonathan Shock", "Otto Lappi", "Benjamin Ultan Cowley"], "title": "SSSUMO: Real-Time Semi-Supervised Submovement Decomposition", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper introduces a SSSUMO, semi-supervised deep learning approach for\nsubmovement decomposition that achieves state-of-the-art accuracy and speed.\nWhile submovement analysis offers valuable insights into motor control,\nexisting methods struggle with reconstruction accuracy, computational cost, and\nvalidation, due to the difficulty of obtaining hand-labeled data. We address\nthese challenges using a semi-supervised learning framework. This framework\nlearns from synthetic data, initially generated from minimum-jerk principles\nand then iteratively refined through adaptation to unlabeled human movement\ndata. Our fully convolutional architecture with differentiable reconstruction\nsignificantly surpasses existing methods on both synthetic and diverse human\nmotion datasets, demonstrating robustness even in high-noise conditions.\nCrucially, the model operates in real-time (less than a millisecond per input\nsecond), a substantial improvement over optimization-based techniques. This\nenhanced performance facilitates new applications in human-computer\ninteraction, rehabilitation medicine, and motor control studies. We demonstrate\nthe model's effectiveness across diverse human-performed tasks such as\nsteering, rotation, pointing, object moving, handwriting, and mouse-controlled\ngaming, showing notable improvements particularly on challenging datasets where\ntraditional methods largely fail. Training and benchmarking source code, along\nwith pre-trained model weights, are made publicly available at\nhttps://github.com/dolphin-in-a-coma/sssumo.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5SSSUMO\uff0c\u7528\u4e8e\u5b50\u8fd0\u52a8\u5206\u89e3\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u548c\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5b50\u8fd0\u52a8\u5206\u6790\u65b9\u6cd5\u5728\u91cd\u5efa\u7cbe\u5ea6\u3001\u8ba1\u7b97\u6210\u672c\u548c\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8e\u624b\u5de5\u6807\u8bb0\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\uff08\u57fa\u4e8e\u6700\u5c0f\u6296\u52a8\u539f\u7406\u751f\u6210\uff09\u548c\u672a\u6807\u8bb0\u7684\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u4f7f\u7528\u5168\u5377\u79ef\u67b6\u6784\u548c\u53ef\u5fae\u5206\u91cd\u5efa\u3002", "result": "\u5728\u5408\u6210\u548c\u591a\u6837\u5316\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u4e5f\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u4e14\u80fd\u5b9e\u65f6\u8fd0\u884c\uff08\u6bcf\u8f93\u5165\u79d2\u5c11\u4e8e\u6beb\u79d2\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u7c7b-\u8ba1\u7b97\u673a\u4ea4\u4e92\u3001\u5eb7\u590d\u533b\u5b66\u548c\u8fd0\u52a8\u63a7\u5236\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5e94\u7528\uff0c\u5c24\u5176\u5728\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.08454", "pdf": "https://arxiv.org/pdf/2507.08454", "abs": "https://arxiv.org/abs/2507.08454", "authors": ["Tobias Geibinger", "Reijo Jaakkola", "Antti Kuusisto", "Xinghan Liu", "Miikka Vilander"], "title": "Why this and not that? A Logic-based Framework for Contrastive Explanations", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 03B05", "I.2.3; F.4.1"], "comment": "20 pages, accepted to JELIA 2025", "summary": "We define several canonical problems related to contrastive explanations,\neach answering a question of the form ''Why P but not Q?''. The problems\ncompute causes for both P and Q, explicitly comparing their differences. We\ninvestigate the basic properties of our definitions in the setting of\npropositional logic. We show, inter alia, that our framework captures a\ncardinality-minimal version of existing contrastive explanations in the\nliterature. Furthermore, we provide an extensive analysis of the computational\ncomplexities of the problems. We also implement the problems for CNF-formulas\nusing answer set programming and present several examples demonstrating how\nthey work in practice.", "AI": {"tldr": "\u8bba\u6587\u5b9a\u4e49\u4e86\u4e0e\u5bf9\u6bd4\u89e3\u91ca\u76f8\u5173\u7684\u51e0\u4e2a\u5178\u578b\u95ee\u9898\uff0c\u5206\u6790\u5176\u5728\u547d\u9898\u903b\u8f91\u4e2d\u7684\u57fa\u672c\u6027\u8d28\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4e0e\u73b0\u6709\u65b9\u6cd5\u7684\u8054\u7cfb\u53ca\u8ba1\u7b97\u590d\u6742\u6027\u3002", "motivation": "\u7814\u7a76\u5bf9\u6bd4\u89e3\u91ca\u7684\u57fa\u672c\u95ee\u9898\uff0c\u56de\u7b54\u201c\u4e3a\u4ec0\u4e48P\u800c\u4e0d\u662fQ\uff1f\u201d\u8fd9\u7c7b\u95ee\u9898\uff0c\u901a\u8fc7\u6bd4\u8f83P\u548cQ\u7684\u5dee\u5f02\u63d0\u4f9b\u89e3\u91ca\u3002", "method": "\u5728\u547d\u9898\u903b\u8f91\u80cc\u666f\u4e0b\u5b9a\u4e49\u95ee\u9898\uff0c\u5206\u6790\u5176\u6027\u8d28\uff0c\u5e76\u901a\u8fc7\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0CNF\u516c\u5f0f\u7684\u6c42\u89e3\u3002", "result": "\u6846\u67b6\u80fd\u591f\u6355\u6349\u73b0\u6709\u5bf9\u6bd4\u89e3\u91ca\u7684\u6700\u5c0f\u7248\u672c\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u7684\u8be6\u7ec6\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5bf9\u6bd4\u89e3\u91ca\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.08702", "pdf": "https://arxiv.org/pdf/2507.08702", "abs": "https://arxiv.org/abs/2507.08702", "authors": ["Viktoriia Makovska", "George Fletcher", "Julia Stoyanovich"], "title": "ONION: A Multi-Layered Framework for Participatory ER Design", "categories": ["cs.DB", "cs.AI", "cs.CY"], "comment": null, "summary": "We present ONION, a multi-layered framework for participatory\nEntity-Relationship (ER) modeling that integrates insights from design justice,\nparticipatory AI, and conceptual modeling. ONION introduces a five-stage\nmethodology: Observe, Nurture, Integrate, Optimize, Normalize. It supports\nprogressive abstraction from unstructured stakeholder input to structured ER\ndiagrams.\n  Our approach aims to reduce designer bias, promote inclusive participation,\nand increase transparency through the modeling process. We evaluate ONION\nthrough real-world workshops focused on sociotechnical systems in Ukraine,\nhighlighting how diverse stakeholder engagement leads to richer data models and\ndeeper mutual understanding. Early results demonstrate ONION's potential to\nhost diversity in early-stage data modeling. We conclude with lessons learned,\nlimitations and challenges involved in scaling and refining the framework for\nbroader adoption.", "AI": {"tldr": "ONION\u662f\u4e00\u4e2a\u591a\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u53c2\u4e0e\u5f0f\u5b9e\u4f53\u5173\u7cfb\u5efa\u6a21\uff0c\u7ed3\u5408\u4e86\u8bbe\u8ba1\u6b63\u4e49\u3001\u53c2\u4e0e\u5f0fAI\u548c\u6982\u5ff5\u5efa\u6a21\u7684\u89c1\u89e3\uff0c\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\uff1a\u89c2\u5bdf\u3001\u57f9\u80b2\u3001\u6574\u5408\u3001\u4f18\u5316\u548c\u89c4\u8303\u5316\u3002", "motivation": "\u51cf\u5c11\u8bbe\u8ba1\u8005\u504f\u89c1\uff0c\u4fc3\u8fdb\u5305\u5bb9\u6027\u53c2\u4e0e\uff0c\u5e76\u5728\u5efa\u6a21\u8fc7\u7a0b\u4e2d\u63d0\u9ad8\u900f\u660e\u5ea6\u3002", "method": "ONION\u91c7\u7528\u4e94\u9636\u6bb5\u65b9\u6cd5\uff08Observe, Nurture, Integrate, Optimize, Normalize\uff09\uff0c\u652f\u6301\u4ece\u975e\u7ed3\u6784\u5316\u7684\u5229\u76ca\u76f8\u5173\u8005\u8f93\u5165\u9010\u6b65\u62bd\u8c61\u4e3a\u7ed3\u6784\u5316\u7684ER\u56fe\u3002", "result": "\u901a\u8fc7\u4e4c\u514b\u5170\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u7684\u5b9e\u9645\u5de5\u4f5c\u574a\u8bc4\u4f30\uff0cONION\u663e\u793a\u4e86\u5176\u4fc3\u8fdb\u591a\u6837\u5316\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u7684\u80fd\u529b\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u6a21\u578b\u548c\u66f4\u6df1\u5165\u7684\u7406\u89e3\u3002", "conclusion": "ONION\u5728\u65e9\u671f\u6570\u636e\u5efa\u6a21\u4e2d\u5c55\u73b0\u51fa\u5bb9\u7eb3\u591a\u6837\u6027\u7684\u6f5c\u529b\uff0c\u4f46\u8fd8\u9700\u8981\u89e3\u51b3\u6269\u5c55\u548c\u6539\u8fdb\u6846\u67b6\u7684\u6311\u6218\u3002"}}
{"id": "2507.08348", "pdf": "https://arxiv.org/pdf/2507.08348", "abs": "https://arxiv.org/abs/2507.08348", "authors": ["Yi-Jun Chang", "Lyuting Chen", "Haoran Zhou"], "title": "Content-Oblivious Leader Election in 2-Edge-Connected Networks", "categories": ["cs.DC"], "comment": null, "summary": "Censor-Hillel, Cohen, Gelles, and Sela (PODC 2022 \\& Distributed Computing\n2023) studied fully-defective asynchronous networks, where communication\nchannels may suffer an extreme form of alteration errors, rendering messages\ncompletely corrupted. The model is equivalent to content-oblivious computation,\nwhere nodes communicate solely via pulses. They showed that if the network is\n2-edge-connected, then any algorithm for a noiseless setting can be simulated\nin the fully-defective setting; otherwise, no non-trivial computation is\npossible in the fully-defective setting. However, their simulation requires a\npredesignated leader, which they conjectured to be necessary for any\nnon-trivial content-oblivious task.\n  Recently, Frei, Gelles, Ghazy, and Nolin (DISC 2024) refuted this conjecture\nfor the special case of oriented ring topology. They designed two asynchronous\ncontent-oblivious leader election algorithms with message complexity $O(n \\cdot\n\\mathsf{ID}_{\\max})$, where $n$ is the number of nodes and $\\mathsf{ID}_{\\max}$\nis the maximum $\\mathsf{ID}$. The first algorithm stabilizes in unoriented\nrings without termination detection. The second algorithm quiescently\nterminates in oriented rings, thus enabling the execution of the simulation\nalgorithm after leader election.\n  In this work, we present an asynchronous content-oblivious leader election\nalgorithm that quiescently terminates in any 2-edge connected network with\nmessage complexity $O(m \\cdot N \\cdot \\mathsf{ID}_{\\min})$, where $m$ is the\nnumber of edges, $N$ is a known upper bound on the number of nodes, and\n$\\mathsf{ID}_{\\min}$ is the smallest $\\mathsf{ID}$. Combined with the previous\nsimulation result, our finding implies that any algorithm from the noiseless\nsetting can be simulated in the fully-defective setting without assuming a\npreselected leader, entirely refuting the original conjecture.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4efb\u4f552\u8fb9\u8fde\u63a5\u7684\u5f02\u6b65\u7f51\u7edc\u4e2d\u7ec8\u6b62\u7684\u9886\u5bfc\u8005\u9009\u4e3e\u7b97\u6cd5\uff0c\u65e0\u9700\u9884\u5148\u6307\u5b9a\u9886\u5bfc\u8005\uff0c\u5426\u5b9a\u4e86\u4e4b\u524d\u7684\u731c\u60f3\u3002", "motivation": "\u6b64\u524d\u7684\u7814\u7a76\u8ba4\u4e3a\u5728\u5b8c\u5168\u7f3a\u9677\u7f51\u7edc\u4e2d\u9700\u8981\u9884\u8bbe\u9886\u5bfc\u8005\u8fdb\u884c\u975e\u5e73\u51e1\u8ba1\u7b97\uff0c\u4f46\u8fd1\u6765\u6709\u7814\u7a76\u5728\u73af\u5f62\u62d3\u6251\u4e2d\u5426\u5b9a\u4e86\u8fd9\u4e00\u731c\u60f3\u3002\u672c\u7814\u7a76\u65e8\u5728\u5c06\u6b64\u63a8\u5e7f\u5230\u4efb\u610f2\u8fb9\u8fde\u63a5\u7f51\u7edc\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u6b65\u5185\u5bb9\u65e0\u89c6\u7f51\u7edc\u7684\u9886\u5bfc\u8005\u9009\u4e3e\u7b97\u6cd5\uff0c\u6d88\u606f\u590d\u6742\u5ea6\u4e3aO(mN\u00b7ID_min)\uff0c\u9002\u7528\u4e8e2\u8fb9\u8fde\u63a5\u7f51\u7edc\u3002", "result": "\u7b97\u6cd5\u6210\u529f\u7ec8\u6b62\u5e76\u9009\u4e3e\u9886\u5bfc\u8005\uff0c\u7ed3\u5408\u4e4b\u524d\u4eff\u771f\u7ed3\u679c\uff0c\u8bc1\u660e\u65e0\u9700\u9884\u8bbe\u9886\u5bfc\u8005\u5373\u53ef\u5728\u5b8c\u5168\u7f3a\u9677\u7f51\u7edc\u4e2d\u4eff\u771f\u65e0\u566a\u97f3\u7b97\u6cd5\u3002", "conclusion": "\u5b8c\u5168\u5426\u5b9a\u4e86\u9886\u5bfc\u8005\u5fc5\u987b\u9884\u8bbe\u7684\u731c\u60f3\uff0c\u6269\u5c55\u4e86\u5185\u5bb9\u65e0\u89c6\u8ba1\u7b97\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.08594", "pdf": "https://arxiv.org/pdf/2507.08594", "abs": "https://arxiv.org/abs/2507.08594", "authors": ["Fernando Ayach", "Vitor Lameir\u00e3o", "Raul Le\u00e3o", "Jerfferson Felizardo", "Rafael Sobrinho", "Vanessa Borges", "Patr\u00edcia Matsubara", "Awdren Font\u00e3o"], "title": "Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "12 pages; 2 figures; Preprint with the original submission accepted\n  for publication at 39th Brazilian Symposium on Software Engineering (SBES)", "summary": "Proto-personas are commonly used during early-stage Product Discovery, such\nas Lean Inception, to guide product definition and stakeholder alignment.\nHowever, the manual creation of proto-personas is often time-consuming,\ncognitively demanding, and prone to bias. In this paper, we propose and\nempirically investigate a prompt engineering-based approach to generate\nproto-personas with the support of Generative AI (GenAI). Our goal is to\nevaluate the approach in terms of efficiency, effectiveness, user acceptance,\nand the empathy elicited by the generated personas. We conducted a case study\nwith 19 participants embedded in a real Lean Inception, employing a qualitative\nand quantitative methods design. The results reveal the approach's efficiency\nby reducing time and effort and improving the quality and reusability of\npersonas in later discovery phases, such as Minimum Viable Product (MVP)\nscoping and feature refinement. While acceptance was generally high, especially\nregarding perceived usefulness and ease of use, participants noted limitations\nrelated to generalization and domain specificity. Furthermore, although\ncognitive empathy was strongly supported, affective and behavioral empathy\nvaried significantly across participants. These results contribute novel\nempirical evidence on how GenAI can be effectively integrated into software\nProduct Discovery practices, while also identifying key challenges to be\naddressed in future iterations of such hybrid design processes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u5de5\u7a0b\u7684\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u521b\u5efa\u7528\u6237\u753b\u50cf\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6548\u7387\u63d0\u5347\u548c\u7528\u6237\u63a5\u53d7\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7528\u6237\u753b\u50cf\u521b\u5efa\u8fc7\u7a0b\u8017\u65f6\u3001\u8ba4\u77e5\u8d1f\u8377\u9ad8\u4e14\u6613\u504f\u9887\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0fAI\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u901a\u8fc719\u4eba\u53c2\u4e0e\u7684\u6848\u4f8b\u7814\u7a76\u8bc4\u4f30\u65b9\u6cd5\u6548\u679c\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u65f6\u95f4\u548c\u7cbe\u529b\uff0c\u63d0\u9ad8\u4e86\u7528\u6237\u753b\u50cf\u8d28\u91cf\uff0c\u4f46\u5728\u901a\u7528\u6027\u548c\u9886\u57df\u7279\u5f02\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u53ef\u7528\u4e8e\u8f6f\u4ef6\u4ea7\u54c1\u53d1\u73b0\u5b9e\u8df5\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u9002\u5e94\u4e0d\u540c\u573a\u666f\u3002"}}
{"id": "2507.08400", "pdf": "https://arxiv.org/pdf/2507.08400", "abs": "https://arxiv.org/abs/2507.08400", "authors": ["Yongjian Zhang", "Longguang Wang", "Kunhong Li", "Ye Zhang", "Yun Wang", "Liang Lin", "Yulan Guo"], "title": "PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "This work presents PanMatch, a versatile foundation model for robust\ncorrespondence matching. Unlike previous methods that rely on task-specific\narchitectures and domain-specific fine-tuning to support tasks like stereo\nmatching, optical flow or feature matching, our key insight is that any\ntwo-frame correspondence matching task can be addressed within a 2D\ndisplacement estimation framework using the same model weights. Such a\nformulation eliminates the need for designing specialized unified architectures\nor task-specific ensemble models. Instead, it achieves multi-task integration\nby endowing displacement estimation algorithms with unprecedented\ngeneralization capabilities. To this end, we highlight the importance of a\nrobust feature extractor applicable across multiple domains and tasks, and\npropose the feature transformation pipeline that leverage all-purpose features\nfrom Large Vision Models to endow matching baselines with zero-shot cross-view\nmatching capabilities. Furthermore, we assemble a cross-domain dataset with\nnear 1.8 million samples from stereo matching, optical flow, and feature\nmatching domains to pretrain PanMatch. We demonstrate the versatility of\nPanMatch across a wide range of domains and downstream tasks using the same\nmodel weights. Our model outperforms UniMatch and Flow-Anything on cross-task\nevaluations, and achieves comparable performance to most state-of-the-art\ntask-specific algorithms on task-oriented benchmarks. Additionally, PanMatch\npresents unprecedented zero-shot performance in abnormal scenarios, such as\nrainy day and satellite imagery, where most existing robust algorithms fail to\nyield meaningful results.", "AI": {"tldr": "PanMatch\u662f\u4e00\u4e2a\u901a\u7528\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u7a33\u5065\u7684\u5bf9\u5e94\u5339\u914d\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u4f4d\u79fb\u4f30\u8ba1\u6846\u67b6\u5b9e\u73b0\u591a\u4efb\u52a1\u96c6\u6210\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u7279\u5b9a\u67b6\u6784\u6216\u5fae\u8c03\uff0cPanMatch\u65e8\u5728\u901a\u8fc7\u7edf\u4e00\u7684\u6a21\u578b\u6743\u91cd\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e2D\u4f4d\u79fb\u4f30\u8ba1\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5229\u7528\u591a\u9886\u57df\u7279\u5f81\u63d0\u53d6\u5668\u548c\u6765\u81ea\u5927\u89c6\u89c9\u6a21\u578b\u7684\u5168\u80fd\u7279\u5f81\uff0c\u652f\u6301\u8de8\u9886\u57df\u5339\u914d\u3002", "result": "PanMatch\u5728\u8de8\u4efb\u52a1\u8bc4\u4f30\u4e2d\u4f18\u4e8eUniMatch\u548cFlow-Anything\uff0c\u5e76\u5728\u5f02\u5e38\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "PanMatch\u5c55\u793a\u4e86\u901a\u7528\u57fa\u7840\u6a21\u578b\u5728\u591a\u4efb\u52a1\u548c\u5f02\u5e38\u573a\u666f\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u7a33\u5065\u5339\u914d\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.08403", "pdf": "https://arxiv.org/pdf/2507.08403", "abs": "https://arxiv.org/abs/2507.08403", "authors": ["Nan Li", "Qi Sun", "Lehan Wang", "Xiaofei Xu", "Jinri Huang", "Chunhui Liu", "Jing Gao", "Yuhong Huang", "Chih-Lin I"], "title": "Towards AI-Native RAN: An Operator's Perspective of 6G Day 1 Standardization", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Artificial Intelligence/Machine Learning (AI/ML) has become the most certain\nand prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not\nnatively integrated but rather an add-on feature over existing architecture, 6G\nshall incorporate AI from the onset to address its complexity and support\nubiquitous AI applications. Based on our extensive mobile network operation and\nstandardization experience from 2G to 5G, this paper explores the design and\nstandardization principles of AI-Native radio access networks (RAN) for 6G,\nwith a particular focus on its critical Day 1 architecture, functionalities and\ncapabilities. We investigate the framework of AI-Native RAN and present its\nthree essential capabilities to shed some light on the standardization\ndirection; namely, AI-driven RAN processing/optimization/automation, reliable\nAI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The\nstandardization of AI-Native RAN, in particular the Day 1 features, including\nan AI-Native 6G RAN architecture, were proposed. For validation, a large-scale\nfield trial with over 5000 5G-A base stations have been built and delivered\nsignificant improvements in average air interface latency, root cause\nidentification, and network energy consumption with the proposed architecture\nand the supporting AI functions. This paper aims to provide a Day 1 framework\nfor 6G AI-Native RAN standardization design, balancing technical innovation\nwith practical deployment.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e866G\u7f51\u7edc\u4e2d\u539f\u751fAI\u7684\u8bbe\u8ba1\u4e0e\u6807\u51c6\u5316\u539f\u5219\uff0c\u91cd\u70b9\u63d0\u51fa\u4e86AI-Native RAN\u7684\u4e09\u5927\u80fd\u529b\uff0c\u5e76\u901a\u8fc75G-A\u57fa\u7ad9\u7684\u5927\u89c4\u6a21\u5b9e\u5730\u8bd5\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u63d0\u5347\u3002", "motivation": "5G\u7f51\u7edc\u4e2d\u7684AI/ML\u4ec5\u4e3a\u9644\u52a0\u529f\u80fd\uff0c\u800c6G\u9700\u8981\u4ece\u8bbe\u8ba1\u4e4b\u521d\u5c31\u539f\u751f\u96c6\u6210AI\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u6027\u548c\u652f\u6301\u591a\u6837\u5316AI\u5e94\u7528\u3002", "method": "\u63d0\u51faAI-Native RAN\u7684\u67b6\u6784\u4e0e\u4e09\u5927\u6838\u5fc3\u80fd\u529b\uff08AI\u9a71\u52a8\u7684RAN\u4f18\u5316\u3001\u53ef\u9760\u7684AI\u751f\u547d\u5468\u671f\u7ba1\u7406\u3001AI\u5373\u670d\u52a1\uff09\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u8bd5\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8bd5\u9a8c\u4e2d\uff0c\u63d0\u51fa\u7684\u67b6\u6784\u663e\u8457\u964d\u4f4e\u4e86\u7a7a\u53e3\u5ef6\u8fdf\u3001\u63d0\u9ad8\u4e86\u6839\u56e0\u8bc6\u522b\u6548\u7387\uff0c\u5e76\u964d\u4f4e\u4e86\u7f51\u7edc\u80fd\u8017\u3002", "conclusion": "\u8bba\u6587\u4e3a6G AI-Native RAN\u7684\u6807\u51c6\u5316\u63d0\u4f9b\u4e86\u521d\u6b65\u6846\u67b6\uff0c\u5e73\u8861\u4e86\u6280\u672f\u521b\u65b0\u4e0e\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\u3002"}}
{"id": "2507.08330", "pdf": "https://arxiv.org/pdf/2507.08330", "abs": "https://arxiv.org/abs/2507.08330", "authors": ["Nikita Malik", "Pratinav Seth", "Neeraj Kumar Singh", "Chintan Chitroda", "Vinay Kumar Sankarapu"], "title": "Interpretability-Aware Pruning for Efficient Medical Image Analysis", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "Pre-Print", "summary": "Deep learning has driven significant advances in medical image analysis, yet\nits adoption in clinical practice remains constrained by the large size and\nlack of transparency in modern models. Advances in interpretability techniques\nsuch as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated\nGradients make it possible to assess the contribution of individual components\nwithin neural networks trained on medical imaging tasks. In this work, we\nintroduce an interpretability-guided pruning framework that reduces model\ncomplexity while preserving both predictive performance and transparency. By\nselectively retaining only the most relevant parts of each layer, our method\nenables targeted compression that maintains clinically meaningful\nrepresentations. Experiments across multiple medical image classification\nbenchmarks demonstrate that this approach achieves high compression rates with\nminimal loss in accuracy, paving the way for lightweight, interpretable models\nsuited for real-world deployment in healthcare settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u7684\u526a\u679d\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u548c\u900f\u660e\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u6a21\u578b\u7684\u5927\u89c4\u6a21\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u9700\u8981\u8f7b\u91cf\u7ea7\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002", "method": "\u91c7\u7528\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff08\u5982DL-Backtrace\u7b49\uff09\u6307\u5bfc\u526a\u679d\uff0c\u9009\u62e9\u6027\u4fdd\u7559\u6bcf\u5c42\u6700\u76f8\u5173\u7684\u90e8\u5206\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0a\uff0c\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\u4e14\u7cbe\u5ea6\u635f\u5931\u6700\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u7597\u573a\u666f\u4e2d\u90e8\u7f72\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2507.08142", "pdf": "https://arxiv.org/pdf/2507.08142", "abs": "https://arxiv.org/abs/2507.08142", "authors": ["Oleksandra Sobchyshak", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "title": "Pushing the Boundaries of Immersion and Storytelling: A Technical Review of Unreal Engine", "categories": ["cs.HC"], "comment": "Paper submitted to Elsevier", "summary": "Unreal Engine is a platform that has influenced immersive storytelling and\nvirtual reality (VR) through its advanced features and diverse applications.\nThis paper provides an in-depth technical review of Unreal Engine. It analyzes\nits key innovations in creating hyper-realistic environments and emotionally\nengaging narratives, with significant applications in gaming, virtual\nproduction, education, cultural preservation, and healthcare. The findings of\nthis article highlight Unreal Engine's transformative impact across industries,\ndemonstrating its ability to merge storytelling with cutting-edge technologies.\nCase studies illustrate how Unreal Engine facilitates seamless visuals, audio,\nand interactivity integration to create compelling experiences. Additionally,\nthis study identifies Unreal Engine's versatility in applications ranging from\nprocedural content generation and AI-driven workflows to smart city simulations\nand VR-based rehabilitation programs.\n  While Unreal Engine sets new benchmarks for visual fidelity and\ninteractivity, this paper underscores critical challenges, including its high\nhardware demands, limited accessibility, and ethical concerns related to\nover-immersion and data privacy. Addressing these challenges through\ncloud-based rendering, inclusive design, and ethical practices is essential for\nbroader adoption and sustainability. This review concludes that Unreal Engine\nis suitable for innovation and interdisciplinary collaboration. Its ability to\nempower creators, redefine workflows, and push the boundaries of immersive\nstorytelling positions Unreal Engine as pivotal in shaping the future of\nvirtual reality and interactive media.", "AI": {"tldr": "\u672c\u6587\u6df1\u5165\u5206\u6790\u4e86\u865a\u5e7b\u5f15\u64ce\u5728\u6c89\u6d78\u5f0f\u53d9\u4e8b\u548c\u865a\u62df\u73b0\u5b9e\u4e2d\u7684\u6280\u672f\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u5728\u591a\u4e2a\u9886\u57df\u7684\u5f71\u54cd\u529b\u3001\u521b\u65b0\u6027\u53ca\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u7814\u7a76\u865a\u5e7b\u5f15\u64ce\u5982\u4f55\u901a\u8fc7\u8d85\u903c\u771f\u73af\u5883\u548c\u60c5\u611f\u53d9\u4e8b\u6280\u672f\u63a8\u52a8VR\u548c\u4e92\u52a8\u5a92\u4f53\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6280\u672f\u5ba1\u67e5\u548c\u6848\u4f8b\u5206\u6790\uff0c\u8bc4\u4f30\u865a\u5e7b\u5f15\u64ce\u7684\u529f\u80fd\u53ca\u5176\u5728\u5404\u884c\u4e1a\u7684\u5e94\u7528\u3002", "result": "\u865a\u5e7b\u5f15\u64ce\u5728\u89c6\u89c9\u3001\u97f3\u9891\u548c\u4e92\u52a8\u6027\u65b9\u9762\u7684\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9762\u4e34\u786c\u4ef6\u9700\u6c42\u9ad8\u548c\u4f26\u7406\u95ee\u9898\u7b49\u6311\u6218\u3002", "conclusion": "\u865a\u5e7b\u5f15\u64ce\u5728\u521b\u65b0\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u4e2d\u6781\u5177\u6f5c\u529b\uff0c\u9700\u89e3\u51b3\u53ef\u8bbf\u95ee\u6027\u548c\u4f26\u7406\u95ee\u9898\u4ee5\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.08745", "pdf": "https://arxiv.org/pdf/2507.08745", "abs": "https://arxiv.org/abs/2507.08745", "authors": ["Maiju Karjalainen", "Pauli Miettinen"], "title": "Hashing for Fast Pattern Set Selection", "categories": ["cs.DB", "cs.LG"], "comment": "17 pages, 5 figures, to appear at ECML-PKDD 2025", "summary": "Pattern set mining, which is the task of finding a good set of patterns\ninstead of all patterns, is a fundamental problem in data mining. Many\ndifferent definitions of what constitutes a good set have been proposed in\nrecent years. In this paper, we consider the reconstruction error as a proxy\nmeasure for the goodness of the set, and concentrate on the adjacent problem of\nhow to find a good set efficiently. We propose a method based on bottom-k\nhashing for efficiently selecting the set and extend the method for the common\ncase where the patterns might only appear in approximate form in the data. Our\napproach has applications in tiling databases, Boolean matrix factorization,\nand redescription mining, among others. We show that our hashing-based approach\nis significantly faster than the standard greedy algorithm while obtaining\nalmost equally good results in both synthetic and real-world data sets.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ebottom-k\u54c8\u5e0c\u7684\u9ad8\u6548\u6a21\u5f0f\u96c6\u6316\u6398\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u91cd\u6784\u8bef\u5dee\u8861\u91cf\u6a21\u5f0f\u96c6\u8d28\u91cf\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u8d2a\u5a6a\u7b97\u6cd5\u3002", "motivation": "\u6a21\u5f0f\u96c6\u6316\u6398\u662f\u6570\u636e\u6316\u6398\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u65e8\u5728\u627e\u5230\u9ad8\u8d28\u91cf\u7684\u6a21\u5f0f\u96c6\u800c\u975e\u6240\u6709\u6a21\u5f0f\u3002\u7814\u7a76\u56e2\u961f\u5e0c\u671b\u901a\u8fc7\u91cd\u6784\u8bef\u5dee\u4f5c\u4e3a\u8d28\u91cf\u8861\u91cf\u6307\u6807\uff0c\u89e3\u51b3\u5982\u4f55\u9ad8\u6548\u627e\u5230\u4f18\u8d28\u6a21\u5f0f\u96c6\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8ebottom-k\u54c8\u5e0c\u7684\u65b9\u6cd5\u6765\u9009\u62e9\u6a21\u5f0f\u96c6\uff0c\u5e76\u6269\u5c55\u81f3\u6a21\u5f0f\u5728\u6570\u636e\u4e2d\u53ef\u80fd\u4ee5\u8fd1\u4f3c\u5f62\u5f0f\u51fa\u73b0\u7684\u60c5\u51b5\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u54c8\u5e0c\u65b9\u6cd5\u6bd4\u6807\u51c6\u8d2a\u5a6a\u7b97\u6cd5\u5feb\u5f97\u591a\uff0c\u4e14\u7ed3\u679c\u8d28\u91cf\u63a5\u8fd1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u6709\u6548\uff0c\u5305\u62ec\u6570\u636e\u5e93\u5207\u7247\u3001\u5e03\u5c14\u77e9\u9635\u5206\u89e3\u548c\u91cd\u63cf\u8ff0\u6316\u6398\uff0c\u4e3a\u9ad8\u6548\u6a21\u5f0f\u96c6\u6316\u6398\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.08725", "pdf": "https://arxiv.org/pdf/2507.08725", "abs": "https://arxiv.org/abs/2507.08725", "authors": ["Dominik Schweisgut", "Anne Benoit", "Yves Robert", "Henning Meyerhenke"], "title": "Carbon-Aware Workflow Scheduling with Fixed Mapping and Deadline Constraint", "categories": ["cs.DC"], "comment": "40 pages, 17 figures. Accepted at ICPP 2025. Code available at:\n  https://github.com/KIT-EAE/CaWoSched.git", "summary": "Large data and computing centers consume a significant share of the world's\nenergy consumption. A prominent subset of the workloads in such centers are\nworkflows with interdependent tasks, usually represented as directed acyclic\ngraphs (DAGs). To reduce the carbon emissions resulting from executing such\nworkflows in centers with a mixed (renewable and non-renewable) energy supply,\nit is advisable to move task executions to time intervals with sufficient green\nenergy when possible. To this end, we formalize the above problem as a\nscheduling problem with a given mapping and ordering of the tasks. We show that\nthis problem can be solved in polynomial time in the uniprocessor case. For at\nleast two processors, however, the problem becomes NP-hard. Hence, we propose a\nheuristic framework called CaWoSched that combines several greedy approaches\nwith local search. To assess the 16 heuristics resulting from different\ncombinations, we also devise a simple baseline algorithm and an exact ILP-based\nsolution. Our experimental results show that our heuristics provide significant\nsavings in carbon emissions compared to the baseline.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8ba1\u7b97\u4e2d\u5fc3\u4e2d\u4efb\u52a1\u8c03\u5ea6\u7684\u78b3\u51cf\u6392\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u542f\u53d1\u5f0f\u6846\u67b6CaWoSched\uff0c\u7ed3\u5408\u8d2a\u5fc3\u7b97\u6cd5\u548c\u5c40\u90e8\u641c\u7d22\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u78b3\u6392\u653e\u3002", "motivation": "\u5927\u578b\u8ba1\u7b97\u4e2d\u5fc3\u6d88\u8017\u5927\u91cf\u80fd\u6e90\uff0c\u5176\u4efb\u52a1\u8c03\u5ea6\u4e2d\u7684\u78b3\u6392\u653e\u95ee\u9898\u4e9f\u5f85\u89e3\u51b3\u3002\u901a\u8fc7\u4f18\u5316\u4efb\u52a1\u6267\u884c\u65f6\u95f4\u4ee5\u5229\u7528\u7eff\u8272\u80fd\u6e90\uff0c\u53ef\u4ee5\u51cf\u5c11\u78b3\u6392\u653e\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u542f\u53d1\u5f0f\u6846\u67b6CaWoSched\uff0c\u7ed3\u5408\u8d2a\u5fc3\u7b97\u6cd5\u548c\u5c40\u90e8\u641c\u7d22\uff0c\u5e76\u8bbe\u8ba1\u57fa\u7ebf\u548c\u7cbe\u786eILP\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCaWoSched\u768416\u79cd\u542f\u53d1\u5f0f\u7ec4\u5408\u76f8\u6bd4\u57fa\u7ebf\u663e\u8457\u51cf\u5c11\u4e86\u78b3\u6392\u653e\u3002", "conclusion": "CaWoSched\u4e3a\u591a\u5904\u7406\u5668\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u8c03\u5ea6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u78b3\u51cf\u6392\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u590d\u6742\u5ea6\u968f\u5904\u7406\u5668\u6570\u91cf\u589e\u52a0\u800c\u663e\u8457\u4e0a\u5347\u3002"}}
{"id": "2507.08627", "pdf": "https://arxiv.org/pdf/2507.08627", "abs": "https://arxiv.org/abs/2507.08627", "authors": ["Chi-en Amy Tai", "Pengyu Nie", "Lukasz Golab", "Alexander Wong"], "title": "NL in the Middle: Code Translation with LLMs and Intermediate Representations", "categories": ["cs.SE"], "comment": null, "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u4e2d\u95f4\u8868\u793a\uff08\u81ea\u7136\u8bed\u8a00\u548c\u62bd\u8c61\u8bed\u6cd5\u6811\uff09\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u94fe\u5f0f\u601d\u8003\u63d0\u793a\uff08CoT\uff09\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6458\u8981\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\uff0c\u5e0c\u671b\u901a\u8fc7\u4e2d\u95f4\u8868\u793a\u63d0\u5347\u7ffb\u8bd1\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u591a\u79cd\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff08\u4ece\u4e00\u6b21\u6027\u5230\u94fe\u5f0f\u601d\u8003\u63d0\u793a\uff09\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u62bd\u8c61\u8bed\u6cd5\u6811\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u6a21\u578b\uff08\u5982Open Gpt4 8X7B\u7b49\uff09\u5728\u4ee3\u7801\u7ffb\u8bd1\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u6458\u8981\u7684\u6548\u679c\u6700\u597d\uff0cOpen Gpt4 8X7B\u7684\u6210\u529f\u7ffb\u8bd1\u7387\u5206\u522b\u63d0\u9ad8\u4e8613.8%\u548c6.7%\u3002", "conclusion": "\u4e2d\u95f4\u8868\u793a\u7279\u522b\u662f\u81ea\u7136\u8bed\u8a00\u6458\u8981\u7ed3\u5408\u94fe\u5f0f\u601d\u8003\u63d0\u793a\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.08557", "pdf": "https://arxiv.org/pdf/2507.08557", "abs": "https://arxiv.org/abs/2507.08557", "authors": ["Yuxuan Jiang", "Zehua Chen", "Zeqian Ju", "Chang Li", "Weibei Dou", "Jun Zhu"], "title": "FreeAudio: Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "comment": "Accepted at ACM MM 2025", "summary": "Text-to-audio (T2A) generation has achieved promising results with the recent\nadvances in generative models. However, because of the limited quality and\nquantity of temporally-aligned audio-text pairs, existing T2A methods struggle\nto handle the complex text prompts that contain precise timing control, e.g.,\n\"owl hooted at 2.4s-5.2s\". Recent works have explored data augmentation\ntechniques or introduced timing conditions as model inputs to enable\ntiming-conditioned 10-second T2A generation, while their synthesis quality is\nstill limited. In this work, we propose a novel training-free timing-controlled\nT2A framework, FreeAudio, making the first attempt to enable timing-controlled\nlong-form T2A generation, e.g., \"owl hooted at 2.4s-5.2s and crickets chirping\nat 0s-24s\". Specifically, we first employ an LLM to plan non-overlapping time\nwindows and recaption each with a refined natural language description, based\non the input text and timing prompts. Then we introduce: 1) Decoupling and\nAggregating Attention Control for precise timing control; 2) Contextual Latent\nComposition for local smoothness and Reference Guidance for global consistency.\nExtensive experiments show that: 1) FreeAudio achieves state-of-the-art\ntiming-conditioned T2A synthesis quality among training-free methods and is\ncomparable to leading training-based methods; 2) FreeAudio demonstrates\ncomparable long-form generation quality with training-based Stable Audio and\npaves the way for timing-controlled long-form T2A synthesis. Demo samples are\navailable at: https://freeaudio.github.io/FreeAudio/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFreeAudio\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u65f6\u5e8f\u63a7\u5236\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u65f6\u5e8f\u63d0\u793a\u5e76\u5b9e\u73b0\u957f\u683c\u5f0f\u97f3\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u97f3\u9891\u751f\u6210\u65b9\u6cd5\u56e0\u6570\u636e\u9650\u5236\uff0c\u96be\u4ee5\u5904\u7406\u5305\u542b\u7cbe\u786e\u65f6\u5e8f\u63a7\u5236\u7684\u590d\u6742\u6587\u672c\u63d0\u793a\uff0c\u5982\u201c\u732b\u5934\u9e70\u57282.4\u79d2-5.2\u79d2\u9e23\u53eb\u201d\u3002", "method": "FreeAudio\u5229\u7528LLM\u89c4\u5212\u975e\u91cd\u53e0\u65f6\u95f4\u7a97\u53e3\u5e76\u91cd\u65b0\u63cf\u8ff0\u6bcf\u4e2a\u7a97\u53e3\u7684\u6587\u672c\uff0c\u7ed3\u5408\u89e3\u8026\u4e0e\u805a\u5408\u6ce8\u610f\u529b\u63a7\u5236\uff08\u7cbe\u786e\u65f6\u5e8f\u63a7\u5236\uff09\u3001\u4e0a\u4e0b\u6587\u6f5c\u5728\u7ec4\u5408\uff08\u5c40\u90e8\u5e73\u6ed1\uff09\u548c\u53c2\u8003\u6307\u5bfc\uff08\u5168\u5c40\u4e00\u81f4\u6027\uff09\u3002", "result": "FreeAudio\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u65f6\u5e8f\u63a7\u5236\u97f3\u9891\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u4e0e\u4e3b\u6d41\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff1b\u4e14\u5728\u957f\u683c\u5f0f\u751f\u6210\u8d28\u91cf\u4e0a\u4e0eStable Audio\u76f8\u5f53\u3002", "conclusion": "FreeAudio\u4e3a\u65f6\u5e8f\u63a7\u5236\u7684\u957f\u683c\u5f0f\u6587\u672c\u5230\u97f3\u9891\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2507.08429", "pdf": "https://arxiv.org/pdf/2507.08429", "abs": "https://arxiv.org/abs/2507.08429", "authors": ["Geng Sun", "Likun Zhang", "Jiahui Li", "Jing Wu", "Jiacheng Wang", "Zemin Sun", "Changyuan Zhao", "Victor C. M. Leung"], "title": "Age of Information Optimization in Laser-charged UAV-assisted IoT Networks: A Multi-agent Deep Reinforcement Learning Method", "categories": ["cs.NI"], "comment": "21 pages, 8 figures", "summary": "The integration of unmanned aerial vehicles (UAVs) with Internet of Things\n(IoT) networks offers promising solutions for efficient data collection.\nHowever, the limited energy capacity of UAVs remains a significant challenge.\nIn this case, laser beam directors (LBDs) have emerged as an effective\ntechnology for wireless charging of UAVs during operation, thereby enabling\nsustained data collection without frequent returns to charging stations (CSs).\nIn this work, we investigate the age of information (AoI) optimization in\nLBD-powered UAV-assisted IoT networks, where multiple UAVs collect data from\ndistributed IoTs while being recharged by laser beams. We formulate a joint\noptimization problem that aims to minimize the peak AoI while determining\noptimal UAV trajectories and laser charging strategies. This problem is\nparticularly challenging due to its non-convex nature, complex temporal\ndependencies, and the need to balance data collection efficiency with energy\nconsumption constraints. To address these challenges, we propose a novel\nmulti-agent proximal policy optimization with temporal memory and multi-agent\ncoordination (MAPPO-TM) framework. Specifically, MAPPO-TM incorporates temporal\nmemory mechanisms to capture the dynamic nature of UAV operations and\nfacilitates effective coordination among multiple UAVs through decentralized\nlearning while considering global system objectives. Simulation results\ndemonstrate that the proposed MAPPO-TM algorithm outperforms conventional\napproaches in terms of peak AoI minimization and energy efficiency. Ideally,\nthe proposed algorithm achieves up to 15.1% reduction in peak AoI compared to\nconventional multi-agent deep reinforcement learning (MADRL) methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6fc0\u5149\u675f\u5145\u7535\u65e0\u4eba\u673a\u8f85\u52a9\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u7684\u4fe1\u606f\u65f6\u6548\u6027\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u6846\u67b6MAPPO-TM\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5cf0\u503c\u4fe1\u606f\u65f6\u6548\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u7684\u6709\u9650\u80fd\u91cf\u9650\u5236\u4e86\u5176\u5728\u7269\u8054\u7f51\u6570\u636e\u6536\u96c6\u4e2d\u7684\u5e94\u7528\uff0c\u6fc0\u5149\u675f\u5145\u7535\u6280\u672f\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4f46\u9700\u8981\u4f18\u5316\u4fe1\u606f\u65f6\u6548\u6027\u548c\u5145\u7535\u7b56\u7565\u3002", "method": "\u63d0\u51faMAPPO-TM\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u8bb0\u5fc6\u673a\u5236\u548c\u591a\u667a\u80fd\u4f53\u534f\u8c03\uff0c\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u548c\u6fc0\u5149\u5145\u7535\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cMAPPO-TM\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u964d\u4f4e\u4e8615.1%\u7684\u5cf0\u503c\u4fe1\u606f\u65f6\u6548\u6027\uff0c\u5e76\u63d0\u9ad8\u4e86\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "MAPPO-TM\u6846\u67b6\u5728\u65e0\u4eba\u673a\u8f85\u52a9\u7269\u8054\u7f51\u7f51\u7edc\u4e2d\u6709\u6548\u4f18\u5316\u4e86\u4fe1\u606f\u65f6\u6548\u6027\u548c\u80fd\u6e90\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.08167", "pdf": "https://arxiv.org/pdf/2507.08167", "abs": "https://arxiv.org/abs/2507.08167", "authors": ["Md. Saif Hassan Onim", "Andrew M. Kiselica", "Himanshu Thapliyal"], "title": "Emotion Detection in Older Adults Using Physiological Signals from Wearable Sensors", "categories": ["cs.HC", "cs.LG", "H.1.2; J.3; C.3"], "comment": null, "summary": "Emotion detection in older adults is crucial for understanding their\ncognitive and emotional well-being, especially in hospital and assisted living\nenvironments. In this work, we investigate an edge-based, non-obtrusive\napproach to emotion identification that uses only physiological signals\nobtained via wearable sensors. Our dataset includes data from 40 older\nindividuals. Emotional states were obtained using physiological signals from\nthe Empatica E4 and Shimmer3 GSR+ wristband and facial expressions were\nrecorded using camera-based emotion recognition with the iMotion's Facial\nExpression Analysis (FEA) module. The dataset also contains twelve emotion\ncategories in terms of relative intensities. We aim to study how well emotion\nrecognition can be accomplished using simply physiological sensor data, without\nthe requirement for cameras or intrusive facial analysis. By leveraging\nclassical machine learning models, we predict the intensity of emotional\nresponses based on physiological signals. We achieved the highest 0.782 r2\nscore with the lowest 0.0006 MSE on the regression task. This method has\nsignificant implications for individuals with Alzheimer's Disease and Related\nDementia (ADRD), as well as veterans coping with Post-Traumatic Stress Disorder\n(PTSD) or other cognitive impairments. Our results across multiple classical\nregression models validate the feasibility of this method, paving the way for\nprivacy-preserving and efficient emotion recognition systems in real-world\nsettings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u3001\u975e\u4fb5\u5165\u6027\u7684\u60c5\u7eea\u8bc6\u522b\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u7a7f\u6234\u4f20\u611f\u5668\u83b7\u53d6\u7684\u751f\u7406\u4fe1\u53f7\u6765\u68c0\u6d4b\u8001\u5e74\u4eba\u7684\u60c5\u7eea\u72b6\u6001\u3002", "motivation": "\u8001\u5e74\u4eba\u7684\u60c5\u7eea\u68c0\u6d4b\u5bf9\u5176\u8ba4\u77e5\u548c\u60c5\u611f\u5065\u5eb7\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u5728\u533b\u9662\u548c\u8f85\u52a9\u751f\u6d3b\u73af\u5883\u4e2d\u3002\u4f20\u7edf\u7684\u6444\u50cf\u5934\u6216\u9762\u90e8\u8868\u60c5\u5206\u6790\u65b9\u6cd5\u53ef\u80fd\u5177\u6709\u4fb5\u5165\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4ec5\u9700\u751f\u7406\u4fe1\u53f7\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u7a7f\u6234\u8bbe\u5907\uff08Empatica E4\u548cShimmer3 GSR+\u8155\u5e26\uff09\u91c7\u96c6\u751f\u7406\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408iMotion\u7684\u9762\u90e8\u8868\u60c5\u5206\u6790\u6a21\u5757\u83b7\u53d6\u60c5\u7eea\u6807\u7b7e\u3002\u901a\u8fc7\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u60c5\u7eea\u5f3a\u5ea6\u3002", "result": "\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u53d6\u5f97\u6700\u9ad8\u76840.782 R2\u5206\u6570\u548c\u6700\u4f4e\u76840.0006\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u53ca\u76f8\u5173\u75f4\u5446\uff08ADRD\uff09\u60a3\u8005\u4ee5\u53ca\u521b\u4f24\u540e\u5e94\u6fc0\u969c\u788d\uff08PTSD\uff09\u60a3\u8005\u7b49\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u62a4\u4e14\u9ad8\u6548\u7684\u60c5\u7eea\u8bc6\u522b\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.08107", "pdf": "https://arxiv.org/pdf/2507.08107", "abs": "https://arxiv.org/abs/2507.08107", "authors": ["Sebastian Walter", "Hannah Bast"], "title": "GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs", "categories": ["cs.CL", "cs.DB", "cs.IR"], "comment": null, "summary": "We propose a new approach for generating SPARQL queries on RDF knowledge\ngraphs from natural language questions or keyword queries, using a large\nlanguage model. Our approach does not require fine-tuning. Instead, it uses the\nlanguage model to explore the knowledge graph by strategically executing SPARQL\nqueries and searching for relevant IRIs and literals. We evaluate our approach\non a variety of benchmarks (for knowledge graphs of different kinds and sizes)\nand language models (of different scales and types, commercial as well as\nopen-source) and compare it with existing approaches. On Wikidata we reach\nstate-of-the-art results on multiple benchmarks, despite the zero-shot setting.\nOn Freebase we come close to the best few-shot methods. On other, less commonly\nevaluated knowledge graphs and benchmarks our approach also performs well\noverall. We conduct several additional studies, like comparing different ways\nof searching the graphs, incorporating a feedback mechanism, or making use of\nfew-shot examples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u6216\u5173\u952e\u8bcd\u67e5\u8be2\u751f\u6210SPARQL\u67e5\u8be2\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u6267\u884c\u67e5\u8be2\u63a2\u7d22\u77e5\u8bc6\u56fe\u8c31\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210SPARQL\u67e5\u8be2\u7684\u6311\u6218\uff0c\u907f\u514d\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u7b56\u7565\u6027\u6267\u884cSPARQL\u67e5\u8be2\u63a2\u7d22\u77e5\u8bc6\u56fe\u8c31\uff0c\u641c\u7d22\u76f8\u5173IRI\u548c\u5b57\u9762\u91cf\u3002", "result": "\u5728Wikidata\u4e0a\u8fbe\u5230\u96f6\u6837\u672c\u8bbe\u7f6e\u7684SOTA\u7ed3\u679c\uff0c\u5728Freebase\u4e0a\u63a5\u8fd1\u6700\u4f73\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u5176\u4ed6\u77e5\u8bc6\u56fe\u8c31\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u77e5\u8bc6\u56fe\u8c31\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u96f6\u6837\u672c\u8bbe\u7f6e\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.08671", "pdf": "https://arxiv.org/pdf/2507.08671", "abs": "https://arxiv.org/abs/2507.08671", "authors": ["Hua Ge", "Juan Zhai", "Minxue Pan", "Fusen He", "Ziyue Tan"], "title": "LLMCup: Ranking-Enhanced Comment Updating with LLMs", "categories": ["cs.SE", "D.2.3; D.2.7; I.2.6"], "comment": "13 pages, 10 figures", "summary": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment.", "AI": {"tldr": "LLMCup\u6846\u67b6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u79cd\u5019\u9009\u6ce8\u91ca\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u6392\u540d\u6a21\u578b\u9009\u62e9\u6700\u4f73\u7ed3\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982CUP\u548cHebCup\uff09\u5728\u6ce8\u91ca\u66f4\u65b0\u4e2d\u5e38\u9057\u6f0f\u6216\u8bef\u89e3\u5173\u952e\u4fe1\u606f\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u573a\u666f\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u5176\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "\u63d0\u51faLLMCup\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u63d0\u793a\u7b56\u7565\u751f\u6210\u5019\u9009\u6ce8\u91ca\uff0c\u518d\u901a\u8fc7CupRank\u6a21\u578b\u9009\u62e9\u6700\u4f73\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLLMCup\u5728\u51c6\u786e\u6027\u7b49\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd549.0%-116.9%\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u751a\u81f3\u8d85\u8d8a\u4eba\u5de5\u66f4\u65b0\u3002", "conclusion": "LLMCup\u5c55\u793a\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6392\u540d\u6a21\u578b\u4f18\u5316\u6ce8\u91ca\u66f4\u65b0\u7684\u6709\u6548\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7ed3\u5408\u4eba\u5de5\u8bc4\u4f30\u3002"}}
{"id": "2507.08801", "pdf": "https://arxiv.org/pdf/2507.08801", "abs": "https://arxiv.org/abs/2507.08801", "authors": ["Hangjie Yuan", "Weihua Chen", "Jun Cen", "Hu Yu", "Jingyun Liang", "Shuning Chang", "Zhihui Lin", "Tao Feng", "Pengwei Liu", "Jiazheng Xing", "Hao Luo", "Jiasheng Tang", "Fan Wang", "Yi Yang"], "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Code and Models: https://github.com/alibaba-damo-academy/Lumos", "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.", "AI": {"tldr": "Lumos-1\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u67b6\u6784\u7684\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u5668\uff0c\u901a\u8fc7MM-RoPE\u548cAR-DF\u6280\u672f\u89e3\u51b3\u65f6\u7a7a\u76f8\u5173\u6027\u548c\u5e27\u95f4\u635f\u5931\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u89c6\u9891\u751f\u6210\u5668\u5b58\u5728\u67b6\u6784\u504f\u79bb\u3001\u4f9d\u8d56\u5916\u90e8\u6587\u672c\u7f16\u7801\u5668\u6216\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\uff0cLumos-1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u75283D RoPE\u6539\u8fdb\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u63d0\u51faMM-RoPE\u548cAR-DF\u6280\u672f\u4f18\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "result": "Lumos-1\u572848 GPU\u4e0a\u9884\u8bad\u7ec3\uff0c\u6027\u80fd\u4e0eEMU3\u3001COSMOS-Video2World\u548cOpenSoraPlan\u76f8\u5f53\u3002", "conclusion": "Lumos-1\u901a\u8fc7\u9ad8\u6548\u6280\u672f\u548c\u67b6\u6784\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2507.08507", "pdf": "https://arxiv.org/pdf/2507.08507", "abs": "https://arxiv.org/abs/2507.08507", "authors": ["Geng Sun", "Chenbang Liu", "Jiahui Li", "Guannan Qu", "Shuang Liang", "Jiacheng Wang", "Changyuan Zhao", "Dusit Niyato"], "title": "Recovery of UAV Swarm-enabled Collaborative Beamforming in Low-altitude Wireless Networks under Wind Field Disturbances", "categories": ["cs.NI"], "comment": null, "summary": "Unmanned aerial vehicle (UAV) swarms utilizing collaborative beamforming (CB)\nin low-altitude wireless networks (LAWN) demonstrate significant potential for\nenhanced communication range, energy efficiency, and signal directivity through\nthe formation of virtual antenna arrays (VAA). However, environmental\ndisturbances, particularly wind fields, significantly degrade CB performance by\nintroducing positional errors that disrupt beam patterns, thereby compromising\ntransmission reliability. This paper investigates the critical challenge of\nmaintaining CB performance in UAV-based VAAs operating in LAWN under wind field\ndisturbances. We propose a comprehensive framework that models the impact of\nthree distinct wind conditions (constant, shear, and turbulent) on UAV array\nperformance, and formulate a long-term real-time optimization problem to\nmaximize directivity while minimizing maximum sidelobe levels through adaptive\nexcitation current weight adjustments. To address the inherent complexity of\nthis problem, we propose a novel proximal policy optimization algorithm with\nlong short-term memory (LSTM) structure and adaptive learning rate (PPO-LA),\nwhich effectively captures temporal patterns in wind field disturbances and\nenables real-time adaptation without requiring extensive prior training for\nspecific wind conditions. Our simulation results demonstrate that the proposed\nPPO-LA algorithm successfully recovers degraded CB performance across various\nwind scenarios, and thus significantly outperforming benchmark algorithms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u65e0\u4eba\u673a\u7fa4\u5728\u98ce\u573a\u5e72\u6270\u4e0b\u4fdd\u6301\u534f\u4f5c\u6ce2\u675f\u6210\u5f62\u6027\u80fd\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePPO-LA\u7b97\u6cd5\u7684\u5b9e\u65f6\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u5728\u4f4e\u7a7a\u65e0\u7ebf\u7f51\u7edc\u4e2d\u901a\u8fc7\u534f\u4f5c\u6ce2\u675f\u6210\u5f62\u5f62\u6210\u865a\u62df\u5929\u7ebf\u9635\u5217\uff0c\u5177\u6709\u901a\u4fe1\u8303\u56f4\u5e7f\u3001\u80fd\u6548\u9ad8\u548c\u4fe1\u53f7\u5b9a\u5411\u6027\u5f3a\u7684\u4f18\u52bf\u3002\u7136\u800c\uff0c\u98ce\u573a\u5e72\u6270\u4f1a\u7834\u574f\u6ce2\u675f\u6a21\u5f0f\uff0c\u5f71\u54cd\u4f20\u8f93\u53ef\u9760\u6027\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u6846\u67b6\uff0c\u6a21\u62df\u4e09\u79cd\u98ce\u51b5\u5bf9\u65e0\u4eba\u673a\u9635\u5217\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u6fc0\u52b1\u7535\u6d41\u6743\u91cd\u8c03\u6574\u4f18\u5316\u95ee\u9898\u3002\u91c7\u7528\u57fa\u4e8eLSTM\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u7684\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff08PPO-LA\uff09\uff0c\u5b9e\u65f6\u9002\u5e94\u98ce\u573a\u5e72\u6270\uff0c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u98ce\u51b5\u9884\u5148\u8bad\u7ec3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cPPO-LA\u7b97\u6cd5\u80fd\u6709\u6548\u6062\u590d\u98ce\u573a\u5e72\u6270\u4e0b\u534f\u4f5c\u6ce2\u675f\u6210\u5f62\u7684\u6027\u80fd\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u7b97\u6cd5\u3002", "conclusion": "PPO-LA\u7b97\u6cd5\u80fd\u591f\u89e3\u51b3\u98ce\u573a\u5e72\u6270\u4e0b\u7684\u5b9e\u65f6\u4f18\u5316\u95ee\u9898\uff0c\u4e3a\u65e0\u4eba\u673a\u7fa4\u7684\u901a\u4fe1\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08230", "pdf": "https://arxiv.org/pdf/2507.08230", "abs": "https://arxiv.org/abs/2507.08230", "authors": ["Gabriella Waters"], "title": "Uncanny or Not? Perceptions of AI-Generated Faces in Autism", "categories": ["cs.HC"], "comment": "2 figures, 15 pages", "summary": "As artificial intelligence (AI) systems become increasingly sophisticated at\ngenerating synthetic human faces, understanding how these images are perceived\nacross diverse populations is important. This study investigates how autistic\nindividuals/individuals with autism perceive AI-generated faces, focusing on\nthe uncanny valley effect. Using a qualitative approach, we analyzed\ndiscussions from the r/autism community on Reddit to explore how autistic\nparticipants/participants with autism describe their experiences with\nAI-generated faces and the uncanny valley phenomenon. The findings suggest that\nautistic people/people with autism may experience the uncanny valley\ndifferently, often reporting stronger discomfort with real human faces than\nwith artificial ones. This research contributes to our understanding of visual\nperception in autism and has implications for the development of inclusive AI\nsystems and assistive technologies.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u81ea\u95ed\u75c7\u60a3\u8005\u5bf9AI\u751f\u6210\u9762\u5b54\u7684\u611f\u77e5\uff0c\u53d1\u73b0\u4ed6\u4eec\u5bf9\u771f\u4eba\u9762\u5b54\u7684\u4e0d\u9002\u611f\u53ef\u80fd\u66f4\u5f3a\uff0c\u4e3a\u5f00\u53d1\u5305\u5bb9\u6027AI\u63d0\u4f9b\u4e86\u542f\u793a\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u4eba\u8138\u6280\u672f\u65e5\u76ca\u6210\u719f\uff0c\u4e86\u89e3\u81ea\u95ed\u75c7\u60a3\u8005\u5bf9\u8fd9\u4e9b\u56fe\u50cf\u7684\u611f\u77e5\u5bf9\u5f00\u53d1\u5305\u5bb9\u6027\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5206\u6790Reddit\u4e0ar/autism\u793e\u533a\u7684\u8ba8\u8bba\u5b9a\u6027\u7814\u7a76\u81ea\u95ed\u75c7\u60a3\u8005\u7684\u4f53\u9a8c\u3002", "result": "\u81ea\u95ed\u75c7\u60a3\u8005\u5bf9AI\u751f\u6210\u9762\u5b54\u7684\u2018\u6050\u6016\u8c37\u2019\u6548\u5e94\u4f53\u9a8c\u4e0d\u540c\uff0c\u5bf9\u771f\u4eba\u9762\u5b54\u7684\u4e0d\u9002\u611f\u53ef\u80fd\u66f4\u5f3a\u3002", "conclusion": "\u7814\u7a76\u6709\u52a9\u4e8e\u7406\u89e3\u81ea\u95ed\u75c7\u60a3\u8005\u7684\u89c6\u89c9\u611f\u77e5\uff0c\u5e76\u4e3a\u5f00\u53d1\u5305\u5bb9\u6027AI\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.08730", "pdf": "https://arxiv.org/pdf/2507.08730", "abs": "https://arxiv.org/abs/2507.08730", "authors": ["Zezhen Xiang", "Jingzhi Gong", "Tao Chen"], "title": "Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted by ICSE 2026", "summary": "Modern configurable software systems need to learn models that correlate\nconfiguration and performance. However, when the system operates in dynamic\nenvironments, the workload variations, hardware changes, and system updates\nwill inevitably introduce concept drifts at different levels - global drifts,\nwhich reshape the performance landscape of the entire configuration space; and\nlocal drifts, which only affect certain sub-regions of that space. As such,\nexisting offline and transfer learning approaches can struggle to adapt to\nthese implicit and unpredictable changes in real-time, rendering configuration\nperformance learning challenging. To address this, we propose DHDA, an online\nconfiguration performance learning framework designed to capture and adapt to\nthese drifts at different levels. The key idea is that DHDA adapts to both the\nlocal and global drifts using dually hierarchical adaptation: at the upper\nlevel, we redivide the data into different divisions, within each of which the\nlocal model is retrained, to handle global drifts only when necessary. At the\nlower level, the local models of the divisions can detect local drifts and\nadapt themselves asynchronously. To balance responsiveness and efficiency, DHDA\ncombines incremental updates with periodic full retraining to minimize\nredundant computation when no drifts are detected. Through evaluating eight\nsoftware systems and against state-of-the-art approaches, we show that DHDA\nachieves considerably better accuracy and can effectively adapt to drifts with\nup to 2x improvements, while incurring reasonable overhead and is able to\nimprove different local models in handling concept drift.", "AI": {"tldr": "DHDA\u662f\u4e00\u4e2a\u5728\u7ebf\u914d\u7f6e\u6027\u80fd\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u5c42\u6b21\u9002\u5e94\u673a\u5236\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u6982\u5ff5\u6f02\u79fb\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u79bb\u7ebf\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5b9e\u65f6\u9002\u5e94\u914d\u7f6e\u6027\u80fd\u7684\u53d8\u5316\uff0cDHDA\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DHDA\u91c7\u7528\u53cc\u91cd\u5c42\u6b21\u9002\u5e94\u673a\u5236\uff1a\u4e0a\u5c42\u91cd\u65b0\u5212\u5206\u6570\u636e\u4ee5\u5904\u7406\u5168\u5c40\u6f02\u79fb\uff0c\u4e0b\u5c42\u5f02\u6b65\u8c03\u6574\u5c40\u90e8\u6a21\u578b\u4ee5\u5e94\u5bf9\u5c40\u90e8\u6f02\u79fb\u3002", "result": "\u57288\u4e2a\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0cDHDA\u5728\u51c6\u786e\u6027\u548c\u9002\u5e94\u6f02\u79fb\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe2\u500d\u3002", "conclusion": "DHDA\u80fd\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u4e2d\u7684\u914d\u7f6e\u6027\u80fd\u53d8\u5316\uff0c\u5e76\u5e73\u8861\u54cd\u5e94\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2507.08549", "pdf": "https://arxiv.org/pdf/2507.08549", "abs": "https://arxiv.org/abs/2507.08549", "authors": ["Yaojia Wang", "Qi Zhang", "Kun Qiu", "Yue Gao"], "title": "Stabilizing and Optimizing Inter-Shell Routing in LEO Networks with Integrated Routing Cost", "categories": ["cs.NI"], "comment": "6 pages, 8 figures, 2025 IEEE/CIC International Conference on\n  Communications in China (ICCC Workshops)", "summary": "The low Earth orbit (LEO) mega-constellation network (LMCN), which uses\nthousands of satellites across multi-shell architectures to deliver different\nservices, is facing challenges in inter-shell routing stability due to dynamic\nnetwork topologies and frequent inter-satellite link (ISL) switching. Existing\nstrategies, such as the Minimum Hop Path set, prioritize minimizing hop counts\nto reduce latency, but ignore ISL switching costs, which leads to high\ninstability. To overcome this, the Adaptive Path Routing Scheme introduces path\nsimilarity thresholds to reduce the ISL switching frequency between shells.\nHowever, the greedy approach of Adaptive Path Routing Scheme is often trapped\nin local optima, sacrificing inter-shell path distance efficiency. To address\nthese limitations, we propose the Dynamic Programming-based Integrated Routing\nCost (DP-IRC) algorithm, which is designed explicitly for inter-shell routing\noptimization. By formulating multi-shell paths as a multistage decision\nproblem, DP-IRC balances hop counts and ISL stability through an Integrated\nRouting Cost (IRC) metric, combining inter-/intra-shell hops and switching\ncosts. Experiments over 60 time slots with real-world Starlink and OneWeb\nconfigurations show that DP-IRC reduces inter-shell ISL switching rates by\n39.1% and 22.0% compared to the Minimum Hop Path set strategy and Adaptive Path\nRouting Scheme, respectively, while still maintaining near-optimal end-to-end\ndistances.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDP-IRC\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u5730\u7403\u8f68\u9053\u5de8\u578b\u661f\u5ea7\u7f51\u7edc\u4e2d\u8de8\u58f3\u8def\u7531\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5e73\u8861\u8df3\u6570\u548c\u94fe\u8def\u5207\u6362\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7b56\u7565\uff08\u5982\u6700\u5c0f\u8df3\u8def\u5f84\u548c\u81ea\u9002\u5e94\u8def\u5f84\u8def\u7531\uff09\u5ffd\u7565\u4e86\u94fe\u8def\u5207\u6362\u6210\u672c\u6216\u9677\u5165\u5c40\u90e8\u6700\u4f18\uff0c\u5bfc\u81f4\u8de8\u58f3\u8def\u7531\u4e0d\u7a33\u5b9a\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5c06\u591a\u58f3\u8def\u5f84\u89c6\u4e3a\u591a\u9636\u6bb5\u51b3\u7b56\u95ee\u9898\uff0c\u7ed3\u5408\u8df3\u6570\u548c\u94fe\u8def\u7a33\u5b9a\u6027\u8bbe\u8ba1\u7efc\u5408\u8def\u7531\u6210\u672c\uff08IRC\uff09\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDP-IRC\u6bd4\u6700\u5c0f\u8df3\u8def\u5f84\u548c\u81ea\u9002\u5e94\u8def\u5f84\u8def\u7531\u5206\u522b\u51cf\u5c1139.1%\u548c22.0%\u7684\u94fe\u8def\u5207\u6362\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u7684\u7aef\u5230\u7aef\u8ddd\u79bb\u3002", "conclusion": "DP-IRC\u5728\u8de8\u58f3\u8def\u7531\u4f18\u5316\u4e2d\u663e\u8457\u63d0\u5347\u7a33\u5b9a\u6027\uff0c\u517c\u987e\u8df3\u6570\u548c\u94fe\u8def\u5207\u6362\u6548\u7387\u3002"}}
{"id": "2507.08260", "pdf": "https://arxiv.org/pdf/2507.08260", "abs": "https://arxiv.org/abs/2507.08260", "authors": ["Abhinav Sood", "Maria Teresa Llano", "Jon McCormack"], "title": "Do Conversational Interfaces Limit Creativity? Exploring Visual Graph Systems for Creative Writing", "categories": ["cs.HC"], "comment": "Published in the 16th International Conference on Computational\n  Creativity, ICCC25. Accepted Paper in\n  https://computationalcreativity.net/iccc25/wp-content/uploads/papers/iccc25-sood2025conversational.pdf", "summary": "We present a graphical, node-based system through which users can visually\nchain generative AI models for creative tasks. Research in the area of chaining\nLLMs has found that while chaining provides transparency, controllability and\nguardrails to approach certain tasks, chaining with pre-defined LLM steps\nprevents free exploration. Using cognitive processes from creativity research\nas a basis, we create a system that addresses the inherent constraints of\nchat-based AI interactions. Specifically, our system aims to overcome the\nlimiting linear structure that inhibits creative exploration and ideation.\nFurther, our node-based approach enables the creation of reusable, shareable\ntemplates that can address different creative tasks. In a small-scale user\nstudy, we find that our graph-based system supports ideation and allows some\nusers to better visualise and think through their writing process when compared\nto a similar conversational interface. We further discuss the weaknesses and\nlimitations of our system, noting the benefits to creativity that user\ninterfaces with higher complexity can provide for users who can effectively use\nthem.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8282\u70b9\u7684\u56fe\u5f62\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u8fde\u63a5\u751f\u6210\u5f0fAI\u6a21\u578b\u652f\u6301\u521b\u610f\u4efb\u52a1\uff0c\u514b\u670d\u7ebf\u6027\u5bf9\u8bdd\u7ed3\u6784\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u884c\u57fa\u4e8e\u94fe\u6761\u7684LLMs\u65b9\u6cd5\u867d\u63d0\u4f9b\u900f\u660e\u5ea6\u548c\u53ef\u63a7\u6027\uff0c\u4f46\u9884\u8bbe\u6b65\u9aa4\u9650\u5236\u4e86\u81ea\u7531\u63a2\u7d22\uff0c\u963b\u788d\u521b\u9020\u529b\u53d1\u6325\u3002", "method": "\u57fa\u4e8e\u521b\u610f\u7814\u7a76\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u56fe\u5f62\u5316\u8282\u70b9\u7cfb\u7edf\uff0c\u521b\u5efa\u53ef\u91cd\u7528\u3001\u53ef\u5171\u4eab\u7684\u6a21\u677f\u3002", "result": "\u5c0f\u89c4\u6a21\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u56fe\u5f62\u5316\u7cfb\u7edf\u4f18\u4e8e\u5bf9\u8bdd\u754c\u9762\uff0c\u652f\u6301\u521b\u610f\u6784\u601d\u548c\u5199\u4f5c\u53ef\u89c6\u5316\u3002", "conclusion": "\u66f4\u9ad8\u590d\u6742\u5ea6\u7684\u7528\u6237\u754c\u9762\u53ef\u63d0\u5347\u521b\u610f\u6548\u679c\uff0c\u4f46\u9700\u7528\u6237\u6709\u6548\u4f7f\u7528\uff0c\u7cfb\u7edf\u4ecd\u6709\u5f85\u6539\u8fdb\u3002"}}
{"id": "2507.08677", "pdf": "https://arxiv.org/pdf/2507.08677", "abs": "https://arxiv.org/abs/2507.08677", "authors": ["Wesley dos Reis Bezerra", "Lais Machado Bezerra", "Carlos Becker Westphal"], "title": "Qualitative Assessment of Low Power Wide Area Network Protocols and their Security Aspect", "categories": ["cs.NI"], "comment": null, "summary": "There are currently many communication options in the Internet of Things,\neven in particular areas such as constrained and battery-powered devices, such\nas Low Power Wide Area Networks. Understanding the differences and\ncharacteristics of each option is a challenge, even for professionals and\nresearchers in the field. To meet this need, this work analyses the qualitative\ncharacteristics of Low Power Wide Area Network protocols and the challenges and\nopportunities of using constrained devices for sparse networks based on\nlong-life batteries. For this study, a bibliographic survey of the literature\nwas carried out as an analysis of three protocols (LoRaWAN, NB-IoT, and\nSigfox), and a detailing of the first one. As a result, there is a discussion\nabout the chosen network protocol and its use in IoT solutions with sparse\nsensors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u4f4e\u529f\u8017\u5e7f\u57df\u7f51\uff08LPWAN\uff09\u534f\u8bae\u7684\u5b9a\u6027\u7279\u5f81\uff0c\u63a2\u8ba8\u4e86\u5728\u957f\u5bff\u547d\u7535\u6c60\u9a71\u52a8\u7684\u7a00\u758f\u7f51\u7edc\u4e2d\u4f7f\u7528\u53d7\u9650\u8bbe\u5907\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u91cd\u70b9\u7814\u7a76\u4e86LoRaWAN\u3001NB-IoT\u548cSigfox\u4e09\u79cd\u534f\u8bae\u3002", "motivation": "\u7269\u8054\u7f51\u4e2d\u7684\u901a\u4fe1\u9009\u9879\u7e41\u591a\uff0c\u5c24\u5176\u5728\u53d7\u9650\u548c\u7535\u6c60\u9a71\u52a8\u8bbe\u5907\u9886\u57df\uff0c\u7406\u89e3\u6bcf\u79cd\u9009\u9879\u7684\u5dee\u5f02\u548c\u7279\u70b9\u5bf9\u4e13\u4e1a\u4eba\u58eb\u548c\u7814\u7a76\u4eba\u5458\u6765\u8bf4\u662f\u4e00\u5927\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6587\u732e\u8c03\u7814\uff0c\u5206\u6790\u4e86\u4e09\u79cdLPWAN\u534f\u8bae\uff08LoRaWAN\u3001NB-IoT\u548cSigfox\uff09\uff0c\u5e76\u8be6\u7ec6\u7814\u7a76\u4e86LoRaWAN\u534f\u8bae\u3002", "result": "\u8ba8\u8bba\u4e86\u6240\u9009\u7f51\u7edc\u534f\u8bae\u53ca\u5176\u5728\u7a00\u758f\u4f20\u611f\u5668\u7269\u8054\u7f51\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7a00\u758f\u4f20\u611f\u5668\u7f51\u7edc\u7684\u4f4e\u529f\u8017\u901a\u4fe1\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u9009\u62e9\u5408\u9002\u7684LPWAN\u534f\u8bae\u3002"}}
{"id": "2507.08624", "pdf": "https://arxiv.org/pdf/2507.08624", "abs": "https://arxiv.org/abs/2507.08624", "authors": ["G\u00e1bor Baranyi", "Zsolt Csibi", "Kristian Fenech", "\u00c1ron F\u00f3thi", "Zs\u00f3fia Ga\u00e1l", "Joul Skaf", "Andr\u00e1s L\u0151rincz"], "title": "Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance", "categories": ["cs.HC", "cs.AI"], "comment": "The paper has been submitted to a journal and waiting for review", "summary": "This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS)\nframework, an advanced artificial intelligence-based solution tailored for home\nrehabilitation environments. AIRS integrates cutting-edge technologies,\nincluding Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and\nlarge Vision-Language Models (VLMs), to create a comprehensive system for\nmachine-guided physical rehabilitation. The general AIRS framework is\ndemonstrated in rehabilitation scenarios following total knee replacement\n(TKR), utilizing a database of 263 video recordings for evaluation. A\nsmartphone is employed within AIRS to perform RT-3DR of living spaces and has a\nbody-matched avatar to provide visual feedback about the excercise. This avatar\nis necessary in (a) optimizing exercise configurations, including camera\nplacement, patient positioning, and initial poses, and (b) addressing privacy\nconcerns and promoting compliance with the AI Act. The system guides users\nthrough the recording process to ensure the collection of properly recorded\nvideos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling\ndirect comparisons between prerecorded clinical exercises and patient home\nrecordings and (ii) VLM-generated feedback, providing detailed explanations and\ncorrections for exercise errors. The framework also supports people with visual\nand hearing impairments. It also features a modular design that can be adapted\nto broader rehabilitation contexts. AIRS software components are available for\nfurther use and customization.", "AI": {"tldr": "AIRS\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u5bb6\u5ead\u5eb7\u590d\u652f\u6301\u6846\u67b6\uff0c\u7ed3\u5408\u5b9e\u65f63D\u91cd\u5efa\u3001\u667a\u80fd\u5bfc\u822a\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u673a\u5668\u5f15\u5bfc\u7684\u5eb7\u590d\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u5728\u5168\u819d\u5173\u8282\u7f6e\u6362\u672f\u540e\u7684\u573a\u666f\u4e2d\u3002", "motivation": "\u901a\u8fc7AI\u6280\u672f\u63d0\u5347\u5bb6\u5ead\u5eb7\u590d\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u540c\u65f6\u89e3\u51b3\u9690\u79c1\u95ee\u9898\u548c\u9002\u5e94\u66f4\u5e7f\u6cdb\u7684\u5eb7\u590d\u9700\u6c42\u3002", "method": "\u5229\u7528\u667a\u80fd\u624b\u673a\u8fdb\u884c\u5b9e\u65f63D\u91cd\u5efa\uff0c\u63d0\u4f9b\u8eab\u4f53\u5339\u914d\u7684\u865a\u62df\u5f62\u8c61\u53cd\u9988\uff0c\u7ed3\u5408\u89c6\u89c9\u548cVLM\u751f\u6210\u7684\u53cd\u9988\u673a\u5236\u3002", "result": "AIRS\u80fd\u591f\u4f18\u5316\u5eb7\u590d\u8bad\u7ec3\u914d\u7f6e\uff0c\u652f\u6301\u89c6\u89c9\u548c\u542c\u529b\u969c\u788d\u60a3\u8005\uff0c\u5e76\u5177\u6709\u6a21\u5757\u5316\u8bbe\u8ba1\u4ee5\u9002\u5e94\u66f4\u591a\u573a\u666f\u3002", "conclusion": "AIRS\u662f\u4e00\u79cd\u7075\u6d3b\u4e14\u591a\u529f\u80fd\u7684\u5eb7\u590d\u652f\u6301\u7cfb\u7edf\uff0c\u5177\u5907\u8fdb\u4e00\u6b65\u5b9a\u5236\u548c\u6269\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.08719", "pdf": "https://arxiv.org/pdf/2507.08719", "abs": "https://arxiv.org/abs/2507.08719", "authors": ["Linzheng Chai", "Jian Yang", "Shukai Liu", "Wei Zhang", "Liran Wang", "Ke Jin", "Tao Sun", "Congnan Liu", "Chenchen Zhang", "Hualei Zhu", "Jiaheng Liu", "Xianjie Wu", "Ge Zhang", "Tianyu Liu", "Zhoujun Li"], "title": "Multilingual Multimodal Software Developer for Code Generation", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Preprint", "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs.", "AI": {"tldr": "MM-Coder\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u591a\u6a21\u6001\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bbe\u8ba1\uff08\u5982UML\u56fe\u548c\u6d41\u7a0b\u56fe\uff09\u4e0e\u6587\u672c\u6307\u4ee4\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u67b6\u6784\u5bf9\u9f50\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u4ee3\u7801\u751f\u6210\u6a21\u578b\u4ec5\u4f9d\u8d56\u6587\u672c\uff0c\u5ffd\u89c6\u4e86\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u89c6\u89c9\u8f85\u52a9\u5de5\u5177\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5f00\u53d1\u4e86MMc-Instruct\u6570\u636e\u96c6\u7528\u4e8e\u591a\u6a21\u6001\u6307\u4ee4\u5fae\u8c03\uff0c\u5e76\u63d0\u51fa\u4e86MMEval\u8bc4\u4f30\u6807\u51c6\uff0c\u4ee5\u652f\u6301\u89c6\u89c9\u4e0e\u6587\u672c\u7ed3\u5408\u7684\u4efb\u52a1\u3002", "result": "MM-Coder\u5728\u89c6\u89c9\u4fe1\u606f\u6355\u6349\u3001\u6307\u4ee4\u9075\u5faa\u548c\u7f16\u7a0b\u77e5\u8bc6\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4f46\u4e3a\u5de5\u4e1a\u7f16\u7a0b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u591a\u6a21\u6001\u65b9\u6cd5\u6269\u5c55\u4e86LLM\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u590d\u6742\u89c4\u8303\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.08717", "pdf": "https://arxiv.org/pdf/2507.08717", "abs": "https://arxiv.org/abs/2507.08717", "authors": ["Akshay Jain", "Sylvaine Kerboeuf", "Sokratis Barmpounakis", "Crist\u00f3bal Vinagre Z.", "Stefan Wendt", "Dinh Thai Bui", "Pol Alemany", "Riccardo Nicolicchia", "Jos\u00e9 Mar\u00eda Jorquera Valero", "Dani Korpi", "Mohammad Hossein Moghaddam", "Mikko A. Uusitalo", "Patrik Rugeland", "Abdelkader Outtagarts", "Karthik Upadhya", "Panagiotis Demestichas", "Raul Mu\u00f1oz", "Manuel Gil P\u00e9rez", "Daniel Adanza", "Ricard Vilalta"], "title": "Knowledge Graph-Based approach for Sustainable 6G End-to-End System Design", "categories": ["cs.NI", "00"], "comment": "The paper is submitted to IEEE Open Journal of the Communications\n  Society (IEEE OJCOMS)", "summary": "Previous generations of cellular communication, such as 5G, have been\ndesigned with the objective of improving key performance indicators (KPIs) such\nas throughput, latency, etc. However, to meet the evolving KPI demands as well\nas the ambitious sustainability targets for the ICT industry, 6G will need to\nbe designed differently. Concretely, 6G will need to consider both the\nperformance and sustainability targets for the various use cases it will serve.\nMoreover, like previous generations, 6G will have various candidate\ntechnological enablers, making the design space of the system even more\ncomplex. Furthermore, given the subjective nature of the sustainability\nindicators, in particular social sustainability, there is a significant gap in\nliterature on how technical enablers and 6G System design can be linked to\nthem. Hence, in this article a novel method for 6G end-to-end (E2E) system\ndesign based on Knowledge graphs (KG) has been introduced. It considers as its\ninput: the use case KPIs, use case sustainability requirements expressed as Key\nValues (KV) and KV Indicators (KVIs), the ability of the technological enablers\nto satisfy these KPIs and KVIs, the 6G system design principles defined in\nHexa-X-II project, the maturity of a technological enabler and the dependencies\nbetween the various enablers. As part of the KG method, a novel approach for\ndetermining the key values a technological enabler addresses, has also been\nintroduced. The effectiveness of the KG method was demonstrated by its\napplication in designing the 6G E2E system for the cooperating mobile robot use\ncase defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,\nresults from proof-of-concept demonstrations for a subset of the selected\nenablers have also been provided, which reinforce the efficacy of the KG method\nfor designing a sustainable 6G system.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u76846G\u7aef\u5230\u7aef\uff08E2E\uff09\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u6027\u80fd\u6307\u6807\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "motivation": "\u73b0\u67096G\u8bbe\u8ba1\u9700\u540c\u65f6\u6ee1\u8db3\u6027\u80fd\u6307\u6807\uff08KPI\uff09\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\uff08\u5982\u793e\u4f1a\u53ef\u6301\u7eed\u6027\uff09\uff0c\u4f46\u73b0\u6709\u6587\u732e\u5728\u6280\u672f\u5b9e\u73b0\u4e0e\u53ef\u6301\u7eed\u6027\u6307\u6807\u5173\u8054\u65b9\u9762\u5b58\u5728\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\uff0c\u8f93\u5165\u5305\u62ec\u7528\u4f8bKPI\u3001\u53ef\u6301\u7eed\u6027\u9700\u6c42\uff08KV\u548cKVI\uff09\u3001\u6280\u672f\u4f7f\u80fd\u80fd\u529b\u30016G\u8bbe\u8ba1\u539f\u5219\u3001\u4f7f\u80fd\u6210\u719f\u5ea6\u53ca\u5176\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u65b0\u65b9\u6cd5\u8bc4\u4f30\u4f7f\u80fd\u5bf9KV\u7684\u8d21\u732e\u3002", "result": "\u5728Hexa-X-II\u9879\u76ee\u4e2d\u7684\u79fb\u52a8\u673a\u5668\u4eba\u7528\u4f8b\u4e2d\u5e94\u7528\u8be5\u65b9\u6cd5\uff0c\u7b5b\u9009\u4e8682\u4e2a\u4f7f\u80fd\uff0c\u5e76\u901a\u8fc7\u6982\u5ff5\u9a8c\u8bc1\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "KG\u65b9\u6cd5\u4e3a\u8bbe\u8ba1\u53ef\u6301\u7eed6G\u7cfb\u7edf\u63d0\u4f9b\u6709\u6548\u8def\u5f84\uff0c\u5176\u5e94\u7528\u5c55\u793a\u4e86\u6280\u672f\u4e0e\u53ef\u6301\u7eed\u6027\u76ee\u6807\u7684\u7ed3\u5408\u6f5c\u529b\u3002"}}
{"id": "2507.08659", "pdf": "https://arxiv.org/pdf/2507.08659", "abs": "https://arxiv.org/abs/2507.08659", "authors": ["Sohshi Yoshida", "Ko Watanabe", "Andreas Dengel", "Shoya Ishimaru", "Shingo Ata", "Manato Fujimoto"], "title": "Push or Light: Nudging Standing to Break Prolonged Sitting", "categories": ["cs.HC"], "comment": null, "summary": "Prolonged sitting is a health risk leading to metabolic and cardiovascular\ndiseases. To combat this, various \"nudging\" strategies encourage stand-ups.\nBehavior change triggers use explicit prompts such as smartphone push\nnotifications or light controls. However, comparisons of the effects of such\ninteractions, discomfort, and user context have not yet been performed. The\npresent study evaluated these methods in a mixed design experiment with 15\ncollege students. Three intervention methods (none, push notifications, and\nlight dimming) and three user task contexts (computer work, video calls, and\nreading) were tested. The frequency of standing up and comfort were assessed\nafter each ten-minute session. Results showed that dimming resulted in slightly\nmore breaks (1.4 \\pm 1.55) than push notification (1.2 \\pm 1.08), but caused\ndiscomfort for 66.7% of participants, compared to 20% for notification. The\nresults were influenced by task context. Dimming was most effective during\nvideo calls and reading, while push notifications were more effective during\ncomputer work. These findings suggest adaptive nudging systems should tailor\ninterventions based on context and individual preferences.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e24\u79cd\u63d0\u793a\u4e45\u5750\u8005\u7ad9\u7acb\u7684\u65b9\u6cd5\uff08\u63a8\u9001\u901a\u77e5\u548c\u706f\u5149\u8c03\u6697\uff09\uff0c\u53d1\u73b0\u706f\u5149\u8c03\u6697\u5728\u5c0f\u4efb\u52a1\u4e2d\u6548\u679c\u66f4\u597d\uff0c\u4f46\u66f4\u6613\u5f15\u8d77\u4e0d\u9002\u3002", "motivation": "\u4e45\u5750\u662f\u5065\u5eb7\u98ce\u9669\uff0c\u9700\u63a2\u7d22\u6709\u6548\u7684\u63d0\u793a\u65b9\u6cd5\u3002", "method": "15\u540d\u5927\u5b66\u751f\u53c2\u4e0e\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u4e09\u79cd\u5e72\u9884\u65b9\u6cd5\uff08\u65e0\u63d0\u793a\u3001\u63a8\u9001\u901a\u77e5\u3001\u706f\u5149\u8c03\u6697\uff09\u548c\u4e09\u79cd\u4efb\u52a1\u60c5\u5883\uff08\u7535\u8111\u5de5\u4f5c\u3001\u89c6\u9891\u901a\u8bdd\u3001\u9605\u8bfb\uff09\uff0c\u8bc4\u4f30\u7ad9\u7acb\u9891\u7387\u548c\u8212\u9002\u5ea6\u3002", "result": "\u706f\u5149\u8c03\u6697\u63d0\u793a\u6548\u679c\u7565\u4f18\u4e8e\u63a8\u9001\u901a\u77e5\uff0c\u4f4666.7%\u53c2\u4e0e\u8005\u611f\u5230\u4e0d\u9002\uff1b\u63a8\u9001\u901a\u77e5\u4e0d\u9002\u611f\u8f83\u4f4e\uff0820%\uff09\u3002\u4efb\u52a1\u60c5\u5883\u5f71\u54cd\u6548\u679c\u3002", "conclusion": "\u81ea\u9002\u5e94\u63d0\u793a\u7cfb\u7edf\u5e94\u6839\u636e\u60c5\u5883\u548c\u4e2a\u4eba\u504f\u597d\u5b9a\u5236\u5e72\u9884\u65b9\u5f0f\u3002"}}
{"id": "2507.08675", "pdf": "https://arxiv.org/pdf/2507.08675", "abs": "https://arxiv.org/abs/2507.08675", "authors": ["Antonis Christou"], "title": "LIMITER: A Gamified Interface for Harnessing Just Intonation Systems", "categories": ["cs.HC"], "comment": "6 pages, 11 figures, NIME, 2025", "summary": "This paper introduces LIMITER, a gamified digital musical instrument for\nharnessing and performing microtonal and justly intonated sounds. While\nmicrotonality in Western music remains a niche and esoteric system that can be\ndifficult both to conceptualize and to perform with, LIMITER presents a novel,\neasy to pickup interface that utilizes color, geometric transformations, and\ngame-like controls to create a simpler inlet into utilizing these sounds as a\nmeans of expression. We report on the background of the development of LIMITER,\nas well as explain the underlying musical and engineering systems that enable\nits function. Additionally, we offer a discussion and preliminary evaluation of\nthe creativity-enhancing effects of the interface.", "AI": {"tldr": "LIMITER\u662f\u4e00\u4e2a\u6e38\u620f\u5316\u6570\u5b57\u4e50\u5668\uff0c\u65e8\u5728\u7b80\u5316\u548c\u8868\u6f14\u5fae\u97f3\u548c\u7eaf\u5f8b\u58f0\u97f3\u3002", "motivation": "\u89e3\u51b3\u897f\u65b9\u97f3\u4e50\u4e2d\u5fae\u97f3\u7cfb\u7edf\u96be\u4ee5\u7406\u89e3\u548c\u8868\u6f14\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u989c\u8272\u3001\u51e0\u4f55\u53d8\u6362\u548c\u6e38\u620f\u5316\u63a7\u5236\u8bbe\u8ba1\u6613\u4e0a\u624b\u7684\u754c\u9762\u3002", "result": "\u4ecb\u7ecd\u4e86LIMITER\u7684\u5f00\u53d1\u80cc\u666f\u3001\u97f3\u4e50\u4e0e\u5de5\u7a0b\u7cfb\u7edf\uff0c\u5e76\u521d\u6b65\u8bc4\u4f30\u5176\u5bf9\u521b\u9020\u529b\u7684\u5f71\u54cd\u3002", "conclusion": "LIMITER\u4e3a\u4f7f\u7528\u5fae\u97f3\u548c\u7eaf\u5f8b\u58f0\u97f3\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u6613\u4e8e\u64cd\u4f5c\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.08744", "pdf": "https://arxiv.org/pdf/2507.08744", "abs": "https://arxiv.org/abs/2507.08744", "authors": ["Clarice Hilton", "Kat Hawkins", "Phill Tew", "Freddie Collins", "Seb Madgwick", "Dominic Potts", "Tom Mitchell"], "title": "EqualMotion: Accessible Motion Capture for the Creative Industries", "categories": ["cs.HC"], "comment": null, "summary": "Motion capture technologies are increasingly used in creative and performance\ncontexts but often exclude disabled practitioners due to normative assumptions\nin body modeling, calibration, and avatar representation. EqualMotion\nintroduces a body-agnostic, wearable motion capture system designed through a\ndisability-centred co-design approach. By enabling personalised calibration,\nintegrating mobility aids, and adopting an inclusive visual language,\nEqualMotion supports diverse body types and movement styles. The system is\ndeveloped collaboratively with disabled researchers and creatives, aiming to\nfoster equitable participation in digital performance and prototyping. This\npaper outlines the system's design principles and highlights ongoing case\nstudies in dance and music to evaluate accessibility in real-world creative\nworkflows.", "AI": {"tldr": "EqualMotion\u662f\u4e00\u79cd\u9762\u5411\u5305\u5bb9\u6027\u7684\u52a8\u4f5c\u6355\u6349\u7cfb\u7edf\uff0c\u901a\u8fc7\u6b8b\u75be\u4e2d\u5fc3\u5316\u8bbe\u8ba1\u652f\u6301\u591a\u6837\u5316\u7684\u8eab\u4f53\u7c7b\u578b\u548c\u8fd0\u52a8\u98ce\u683c\uff0c\u65e8\u5728\u5b9e\u73b0\u6570\u5b57\u8868\u6f14\u548c\u539f\u578b\u8bbe\u8ba1\u4e2d\u7684\u516c\u5e73\u53c2\u4e0e\u3002", "motivation": "\u76ee\u524d\u7684\u52a8\u4f5c\u6355\u6349\u6280\u672f\u5728\u8eab\u4f53\u5efa\u6a21\u3001\u6821\u51c6\u548c\u865a\u62df\u5f62\u8c61\u8868\u793a\u4e2d\u901a\u5e38\u57fa\u4e8e\u89c4\u8303\u5047\u8bbe\uff0c\u5bfc\u81f4\u6b8b\u75be\u4ece\u4e1a\u8005\u88ab\u6392\u65a5\u3002EqualMotion\u65e8\u5728\u901a\u8fc7\u5305\u5bb9\u6027\u8bbe\u8ba1\u6539\u53d8\u8fd9\u4e00\u73b0\u72b6\u3002", "method": "\u91c7\u7528\u6b8b\u75be\u4e2d\u5fc3\u5316\u7684\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u8eab\u4f53\u65e0\u5173\u7684\u53ef\u7a7f\u6234\u52a8\u4f5c\u6355\u6349\u7cfb\u7edf\uff0c\u652f\u6301\u4e2a\u6027\u5316\u6821\u51c6\u548c\u96c6\u6210\u8f85\u52a9\u5de5\u5177\uff0c\u5e76\u4f7f\u7528\u5305\u5bb9\u6027\u89c6\u89c9\u8bed\u8a00\u3002", "result": "EqualMotion\u7cfb\u7edf\u80fd\u591f\u652f\u6301\u591a\u6837\u5316\u7684\u8eab\u4f53\u7c7b\u578b\u548c\u8fd0\u52a8\u98ce\u683c\uff0c\u5e76\u4e0e\u6b8b\u75be\u7814\u7a76\u4eba\u5458\u548c\u521b\u610f\u4eba\u58eb\u5408\u4f5c\u5f00\u53d1\uff0c\u4ee5\u4fc3\u8fdb\u516c\u5e73\u53c2\u4e0e\u3002", "conclusion": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff08\u5982\u821e\u8e48\u548c\u97f3\u4e50\uff09\u9a8c\u8bc1\uff0cEqualMotion\u5c55\u793a\u4e86\u5305\u5bb9\u6027\u52a8\u4f5c\u6355\u6349\u6280\u672f\u5728\u73b0\u5b9e\u521b\u610f\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4ef7\u503c\u3002"}}
{"id": "2507.08001", "pdf": "https://arxiv.org/pdf/2507.08001", "abs": "https://arxiv.org/abs/2507.08001", "authors": ["Shengyi Xie"], "title": "Human Creativity and AI", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "With the advancement of science and technology, the philosophy of creativity\nhas undergone significant reinterpretation. This paper investigates\ncontemporary research in the fields of psychology, cognitive neuroscience, and\nthe philosophy of creativity, particularly in the context of the development of\nartificial intelligence (AI) techniques. It aims to address the central\nquestion: Can AI exhibit creativity? The paper reviews the historical\nperspectives on the philosophy of creativity and explores the influence of\npsychological advancements on the study of creativity. Furthermore, it analyzes\nvarious definitions of creativity and examines the responses of naturalism and\ncognitive neuroscience to the concept of creativity.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u79d1\u6280\u8fdb\u6b65\u5982\u4f55\u91cd\u65b0\u8be0\u91ca\u521b\u9020\u529b\u54f2\u5b66\uff0c\u91cd\u70b9\u5173\u6ce8AI\u662f\u5426\u5177\u5907\u521b\u9020\u529b\u7684\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u5fc3\u7406\u5b66\u548c\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7684\u89c6\u89d2\u8fdb\u884c\u5206\u6790\u3002", "motivation": "\u63a2\u8ba8\u79d1\u6280\u8fdb\u6b65\u5bf9\u521b\u9020\u529b\u54f2\u5b66\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662fAI\u662f\u5426\u80fd\u591f\u8868\u73b0\u521b\u9020\u529b\u3002", "method": "\u56de\u987e\u5386\u53f2\u89c2\u70b9\uff0c\u7ed3\u5408\u5fc3\u7406\u5b66\u548c\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7684\u8fdb\u5c55\uff0c\u5206\u6790\u521b\u9020\u529b\u7684\u5b9a\u4e49\u53ca\u81ea\u7136\u4e3b\u4e49\u4e0e\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7684\u56de\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u7684\u521b\u9020\u529b\u9700\u7ed3\u5408\u591a\u5b66\u79d1\u89c6\u89d2\u8fdb\u884c\u91cd\u65b0\u5b9a\u4e49\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u521b\u9020\u529b\u7684\u7814\u7a76\u9700\u8de8\u5b66\u79d1\u5408\u4f5c\uff0c\u5c24\u5176\u662f\u54f2\u5b66\u4e0e\u79d1\u5b66\u7684\u7ed3\u5408\u3002"}}
{"id": "2507.08030", "pdf": "https://arxiv.org/pdf/2507.08030", "abs": "https://arxiv.org/abs/2507.08030", "authors": ["Sonali Sharma", "Ahmed M. Alaa", "Roxana Daneshjou"], "title": "A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models", "categories": ["cs.CL", "cs.CE", "cs.HC"], "comment": "11 pages, 5 figures", "summary": "Generative AI models, including large language models (LLMs) and\nvision-language models (VLMs), are increasingly used to interpret medical\nimages and answer clinical questions. Their responses often include\ninaccuracies; therefore, safety measures like medical disclaimers are critical\nto remind users that AI outputs are not professionally vetted or a substitute\nfor medical advice. This study evaluated the presence of disclaimers in LLM and\nVLM outputs across model generations from 2022 to 2025. Using 500 mammograms,\n500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs\nwere screened for disclaimer phrases. Medical disclaimer presence in LLM and\nVLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023\nto 1.05% in 2025, respectively. By 2025, the majority of models displayed no\ndisclaimers. As public models become more capable and authoritative,\ndisclaimers must be implemented as a safeguard adapting to the clinical context\nof each output.", "AI": {"tldr": "\u7814\u7a76\u4e862022\u81f32025\u5e74\u95f4\u751f\u6210AI\u6a21\u578b\uff08\u5982LLM\u548cVLM\uff09\u5728\u533b\u5b66\u56fe\u50cf\u548c\u4e34\u5e8a\u95ee\u9898\u56de\u7b54\u4e2d\u514d\u8d23\u58f0\u660e\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u53d1\u73b0\u5176\u6bd4\u4f8b\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u68c0\u6d4b\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u5e94\u7528\u7684\u514d\u8d23\u58f0\u660e\u4f7f\u7528\u60c5\u51b5\uff0c\u4ee5\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u3002", "method": "\u5206\u6790\u4e86500\u4efd\u4e73\u817aX\u5149\u7247\u3001500\u4efd\u80f8\u900fX\u5149\u7247\u3001500\u4efd\u76ae\u80a4\u75c5\u56fe\u50cf\u548c500\u4e2a\u533b\u5b66\u95ee\u9898\uff0c\u7b5b\u67e5\u6a21\u578b\u8f93\u51fa\u4e2d\u662f\u5426\u5b58\u5728\u514d\u8d23\u58f0\u660e\u3002", "result": "\u514d\u8d23\u58f0\u660e\u4f7f\u7528\u7387\u4ece2022\u5e74\u768426.3%\u964d\u81f32025\u5e74\u76840.97%\uff08LLM\uff09\uff0c\u4ece2023\u5e74\u768419.6%\u964d\u81f31.05%\uff08VLM\uff09\u3002", "conclusion": "\u968f\u7740\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u514d\u8d23\u58f0\u660e\u9700\u66f4\u7075\u6d3b\u5730\u9002\u5e94\u4e34\u5e8a\u73af\u5883\u4ee5\u786e\u4fdd\u7528\u6237\u5b89\u5168\u3002"}}
{"id": "2507.08175", "pdf": "https://arxiv.org/pdf/2507.08175", "abs": "https://arxiv.org/abs/2507.08175", "authors": ["Md. Saif Hassan Onim", "Travis S. Humble", "Himanshu Thapliyal"], "title": "Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors", "categories": ["cs.LG", "cs.HC", "quant-ph"], "comment": null, "summary": "We investigate the feasibility of inferring emotional states exclusively from\nphysiological signals, thereby presenting a privacy-preserving alternative to\nconventional facial recognition techniques. We conduct a performance comparison\nof classical machine learning algorithms and hybrid quantum machine learning\n(QML) methods with a quantum kernel-based model. Our results indicate that the\nquantum-enhanced SVM surpasses classical counterparts in classification\nperformance across all emotion categories, even when trained on limited\ndatasets. The F1 scores over all classes are over 80% with around a maximum of\n36% improvement in the recall values. The integration of wearable sensor data\nwith quantum machine learning not only enhances accuracy and robustness but\nalso facilitates unobtrusive emotion recognition. This methodology holds\npromise for populations with impaired communication abilities, such as\nindividuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans\nwith Post-Traumatic Stress Disorder (PTSD). The findings establish an early\nfoundation for passive emotional monitoring in clinical and assisted living\nconditions.", "AI": {"tldr": "\u901a\u8fc7\u751f\u7406\u4fe1\u53f7\u63a8\u65ad\u60c5\u7eea\u72b6\u6001\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u9762\u90e8\u8bc6\u522b\u6280\u672f\u3002\u91cf\u5b50\u589e\u5f3aSVM\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u8d85\u8d8a\u7ecf\u5178\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u4e0d\u4f9d\u8d56\u9762\u90e8\u8bc6\u522b\u7684\u9690\u79c1\u4fdd\u62a4\u60c5\u7eea\u8bc6\u522b\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u6c9f\u901a\u969c\u788d\u4eba\u7fa4\u3002", "method": "\u6bd4\u8f83\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u4e0e\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u91c7\u7528\u91cf\u5b50\u6838\u6a21\u578b\u3002", "result": "\u91cf\u5b50\u589e\u5f3aSVM\u5728\u6240\u6709\u60c5\u7eea\u7c7b\u522b\u4e2d\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8d8580%\uff0c\u53ec\u56de\u7387\u63d0\u5347\u9ad8\u8fbe36%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u5e94\u7528\u4e8e\u4e34\u5e8a\u548c\u8f85\u52a9\u751f\u6d3b\u573a\u666f\uff0c\u4e3a\u88ab\u52a8\u60c5\u7eea\u76d1\u6d4b\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.08800", "pdf": "https://arxiv.org/pdf/2507.08800", "abs": "https://arxiv.org/abs/2507.08800", "authors": ["Luke Rivard", "Sun Sun", "Hongyu Guo", "Wenhu Chen", "Yuntian Deng"], "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.", "AI": {"tldr": "NeuralOS\u5229\u7528RNN\u548c\u6269\u6563\u6a21\u578b\u9884\u6d4b\u64cd\u4f5c\u7cfb\u7edfGUI\u5c4f\u5e55\u5e27\uff0c\u901a\u8fc7Ubuntu XFCE\u6570\u636e\u8bad\u7ec3\uff0c\u80fd\u6a21\u62df\u9f20\u6807\u4ea4\u4e92\u548c\u5e94\u7528\u542f\u52a8\uff0c\u4f46\u952e\u76d8\u4ea4\u4e92\u4ecd\u5177\u6311\u6218\u3002", "motivation": "\u4e3a\u672a\u6765\u7684\u8ba1\u7b97\u673a\u4ea4\u4e92\u7cfb\u7edf\u5f00\u53d1\u81ea\u9002\u5e94\u3001\u751f\u6210\u7684\u795e\u7ecf\u7f51\u7edc\u754c\u9762\u3002", "method": "\u7ed3\u5408RNN\u8ffd\u8e2a\u8ba1\u7b97\u673a\u72b6\u6001\u548c\u6269\u6563\u6a21\u578b\u6e32\u67d3\u5c4f\u5e55\u56fe\u50cf\uff0c\u4f7f\u7528Ubuntu XFCE\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "result": "\u6210\u529f\u751f\u6210\u771f\u5b9eGUI\u5e8f\u5217\uff0c\u51c6\u786e\u6355\u6349\u9f20\u6807\u4ea4\u4e92\u548c\u72b6\u6001\u8f6c\u6362\uff08\u5982\u5e94\u7528\u542f\u52a8\uff09\uff0c\u952e\u76d8\u4ea4\u4e92\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "NeuralOS\u4e3a\u5b9e\u73b0\u672a\u6765\u81ea\u9002\u5e94\u795e\u7ecf\u754c\u9762\u8fc8\u51fa\u4e00\u6b65\uff0c\u952e\u76d8\u4ea4\u4e92\u7cbe\u5ea6\u6709\u5f85\u63d0\u5347\u3002"}}
