<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 24]
- [cs.GR](#cs.GR) [Total: 9]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated File-Level Logging Generation for Machine Learning Applications using LLMs: A Case Study using GPT-4o Mini](https://arxiv.org/abs/2508.04820)
*Mayra Sofia Ruiz Rodriguez,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 论文评估了GPT-4o mini在文件级别为ML项目生成日志的能力，发现其能部分模仿人类日志位置，但存在过度日志记录等问题。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型（LLMs）在文件级别日志生成中的潜力，尤其是在机器学习应用中。

Method: 收集171个ML仓库的Python文件，移除原有日志后由LLM生成日志，并对比分析日志位置、级别、变量和文本质量。

Result: LLM在63.91%的情况下与人类日志位置一致，但过度日志率高达82.66%，且存在其他问题如代码块内日志记录困难。

Conclusion: LLM在文件级别日志生成中展现了潜力，但仍需解决如过度日志和项目适配等限制。

Abstract: Logging is essential in software development, helping developers monitor
system behavior and aiding in debugging applications. Given the ability of
large language models (LLMs) to generate natural language and code, researchers
are exploring their potential to generate log statements. However, prior work
focuses on evaluating logs introduced in code functions, leaving file-level log
generation underexplored -- especially in machine learning (ML) applications,
where comprehensive logging can enhance reliability. In this study, we evaluate
the capacity of GPT-4o mini as a case study to generate log statements for ML
projects at file level. We gathered a set of 171 ML repositories containing
4,073 Python files with at least one log statement. We identified and removed
the original logs from the files, prompted the LLM to generate logs for them,
and evaluated both the position of the logs and log level, variables, and text
quality of the generated logs compared to human-written logs. In addition, we
manually analyzed a representative sample of generated logs to identify common
patterns and challenges. We find that the LLM introduces logs in the same place
as humans in 63.91% of cases, but at the cost of a high overlogging rate of
82.66%. Furthermore, our manual analysis reveals challenges for file-level
logging, which shows overlogging at the beginning or end of a function,
difficulty logging within large code blocks, and misalignment with
project-specific logging conventions. While the LLM shows promise for
generating logs for complete files, these limitations remain to be addressed
for practical implementation.

</details>


### [2] [Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models](https://arxiv.org/abs/2508.04895)
*Wentao Lu,Alexander Senchenko,Abram Hindle,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 论文提出了一种自动化管道，通过将游戏视频缩减为与bug描述最匹配的单帧，显著降低人工审核负担。


<details>
  <summary>Details</summary>
Motivation: 解决游戏开发中因频繁更新和大量bug报告导致的人工审核效率低下的问题。

Method: 使用FFmpeg提取关键帧，结合视觉-语言模型（GPT-4o）筛选最匹配bug描述的代表帧。

Result: 方法在真实游戏数据中表现良好，F1分数为0.79，准确率为0.89，尤其在光照与阴影类bug中表现最佳（F1=0.94）。

Conclusion: 该方法显著减少了QA团队的工作量，提升了bug分类效率，可广泛应用于游戏行业。

Abstract: Modern game studios deliver new builds and patches at a rapid pace,
generating thousands of bug reports, many of which embed gameplay videos. To
verify and triage these bug reports, developers must watch the submitted
videos. This manual review is labour-intensive, slow, and hard to scale. In
this paper, we introduce an automated pipeline that reduces each video to a
single frame that best matches the reported bug description, giving developers
instant visual evidence that pinpoints the bug.
  Our pipeline begins with FFmpeg for keyframe extraction, reducing each video
to a median of just 1.90% of its original frames while still capturing bug
moments in 98.79 of cases. These keyframes are then evaluated by a
vision--language model (GPT-4o), which ranks them based on how well they match
the textual bug description and selects the most representative frame. We
evaluated this approach using real-world developer-submitted gameplay videos
and JIRA bug reports from a popular First-Person Shooter (FPS) game. The
pipeline achieves an overall F1 score of 0.79 and Accuracy of 0.89 for the
top-1 retrieved frame. Performance is highest for the Lighting & Shadow (F1 =
0.94), Physics & Collision (0.86), and UI & HUD (0.83) bug categories, and
lowest for Animation & VFX (0.51).
  By replacing video viewing with an immediately informative image, our
approach dramatically reduces manual effort and speeds up triage and regression
checks, offering practical benefits to quality assurance (QA) teams and
developers across the game industry.

</details>


### [3] [Charting Uncertain Waters: A Socio-Technical Framework for Navigating GenAI's Impact on Open Source Communities](https://arxiv.org/abs/2508.04921)
*Zixuan Feng,Reed Milewicz,Emerson Murphy-Hill,Tyler Menezes,Alexander Serebrenik,Igor Steinmacher,Anita Sarma*

Main category: cs.SE

TL;DR: 开源软件社区面临生成式AI带来的不确定性，本文通过情景驱动的概念探索，提出了社会技术框架以应对风险和机遇。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在改变软件开发、维护和治理方式，开源社区亟需明确框架以避免被复杂性淹没，同时保持协作精神。

Method: 采用基于McLuhan四元法的社会技术框架，从软件实践、文档、社区参与和治理四个领域进行情景驱动的概念探索。

Result: 提出了开源社区在生成式AI驱动的变革中增强韧性的风险和机会，帮助领导者和研究者主动塑造生态系统未来。

Conclusion: 通过社会技术框架，开源社区可以更主动应对技术颠覆，而非被动反应，从而维护协作精神和生态发展。

Abstract: Open Source Software communities face a wave of uncertainty as Generative AI
rapidly transforms how software is created, maintained, and governed. Without
clear frameworks, communities risk being overwhelmed by the complexity and
ambiguity introduced by GenAI, threatening the collaborative ethos that
underpins OSS. We conduct a scenario-driven, conceptual exploration using a
socio-technical framework inspired by McLuhan's Tetrad to surface both risks
and opportunities for community resilience amid GenAI-driven disruption of OSS
development across four domains: software practices, documentation, community
engagement, and governance. By adopting this lens, OSS leaders and researchers
can proactively shape the future of their ecosystems, rather than simply
reacting to technological upheaval.

</details>


### [4] [Taxonomy of Faults in Attention-Based Neural Networks](https://arxiv.org/abs/2508.04925)
*Sigma Jahan,Saurabh Singh Rajput,Tushar Sharma,Mohammad Masudur Rahman*

Main category: cs.SE

TL;DR: 本文首次对基于注意力机制的神经网络（ABNNs）中的故障进行了全面的实证研究，提出了七种独特的故障分类，并为诊断这些故障提供了基于证据的启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习故障分类未能充分捕捉注意力机制引入的独特故障，导致实践者缺乏有效的诊断指导。

Method: 通过系统分析来自96个项目、十个框架的555个实际故障，开发了七种注意力特有的故障分类，并分析其根本原因和表现症状。

Result: 超过一半的ABNNs故障由注意力机制特有机制引起，提出了四种解释33.0%注意力特有故障的诊断启发式方法。

Conclusion: 本研究填补了注意力机制故障分类和诊断的空白，为实践者提供了系统性指导。

Abstract: Attention mechanisms are at the core of modern neural architectures, powering
systems ranging from ChatGPT to autonomous vehicles and driving a major
economic impact. However, high-profile failures, such as ChatGPT's nonsensical
outputs or Google's suspension of Gemini's image generation due to attention
weight errors, highlight a critical gap: existing deep learning fault
taxonomies might not adequately capture the unique failures introduced by
attention mechanisms. This gap leaves practitioners without actionable
diagnostic guidance. To address this gap, we present the first comprehensive
empirical study of faults in attention-based neural networks (ABNNs). Our work
is based on a systematic analysis of 555 real-world faults collected from 96
projects across ten frameworks, including GitHub, Hugging Face, and Stack
Overflow. Through our analysis, we develop a novel taxonomy comprising seven
attention-specific fault categories, not captured by existing work. Our results
show that over half of the ABNN faults arise from mechanisms unique to
attention architectures. We further analyze the root causes and manifestations
of these faults through various symptoms. Finally, by analyzing symptom-root
cause associations, we identify four evidence-based diagnostic heuristics that
explain 33.0% of attention-specific faults, offering the first systematic
diagnostic guidance for attention-based models.

</details>


### [5] [Generative AI for Object-Oriented Programming: Writing the Right Code and Reasoning the Right Logic](https://arxiv.org/abs/2508.05005)
*Gang Xu,Airong Wang,Yushan Pan*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）如何提升面向对象编程（OOP）的效率和代码编写，填补了LLMs与OOP交叉领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 当前的AI研究快速发展，LLMs在多领域应用广泛，但LLMs与OOP的结合尚未深入探索。本文旨在填补这一空白，探讨LLMs如何提升编程效率。

Method: 从程序员、初学者和老手的视角出发，识别编码流程中LLMs可提供显著帮助的关键节点，并提出增强逻辑推理和代码编写的方法。

Result: 提出了LLMs在OOP任务中应用的具体场景和优化方法，为编程体验提供改进方向。

Conclusion: 通过整合LLMs与OOP，可以有效提升编程效率和代码质量，未来研究可进一步验证和扩展这些应用。

Abstract: We find ourselves in the midst of an explosion in artificial intelligence
research, particularly with large language models (LLMs). These models have
diverse applications spanning finance, commonsense knowledge graphs, medicine,
and visual analysis. In the world of Object-Oriented Programming(OOP), a robust
body of knowledge and methods has been developed for managing complex tasks
through object-oriented thinking. However, the intersection of LLMs with OOP
remains an underexplored territory. Empirically, we currently possess limited
understanding of how LLMs can enhance the effectiveness of OOP learning and
code writing, as well as how we can evaluate such AI-powered tools. Our work
aims to address this gap by presenting a vision from the perspectives of key
stakeholders involved in an OOP task: programmers, mariners, and experienced
programmers. We identify critical junctures within typical coding workflows
where the integration of LLMs can offer significant benefits. Furthermore, we
propose ways to augment existing logical reasoning and code writing, ultimately
enhancing the programming experience.

</details>


### [6] [An ML-based Approach to Predicting Software Change Dependencies: Insights from an Empirical Study on OpenStack](https://arxiv.org/abs/2508.05034)
*Arabat,Ali,Sayagh,Mohammed,Hassine,Jameleddine*

Main category: cs.SE

TL;DR: 论文提出了一种半自动方法来管理软件变更的依赖关系，通过两个ML模型预测依赖关系，解决了OpenStack中大量变更依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂度的增加，准确识别和管理变更之间的依赖关系变得至关重要，以防止构建失败和不完整的部署。

Method: 研究通过在OpenStack中进行初步研究，提出了一种结合两个ML模型的半自动化方法，用于预测和识别变更之间的依赖关系。

Result: 研究显示，51.08%的依赖关系在代码审查阶段被识别，延迟中位数为5.06小时；提出的模型表现良好，AUC得分为79.33%和91.89%，Brier得分为0.11和0.014。

Conclusion: 该方法能有效帮助开发者提前识别依赖关系，但模型的准确率仍有提升空间。

Abstract: As software systems grow in complexity, accurately identifying and managing
dependencies among changes becomes increasingly critical. For instance, a
change that leverages a function must depend on the change that introduces it.
Establishing such dependencies allows CI/CD pipelines to build and orchestrate
changes effectively, preventing build failures and incomplete feature
deployments. In modern software systems, dependencies often span multiple
components across teams, creating challenges for development and deployment.
They serve various purposes, from enabling new features to managing
configurations, and can even involve traditionally independent changes like
documentation updates. To address these challenges, we conducted a preliminary
study on dependency management in OpenStack, a large-scale software system. Our
study revealed that a substantial portion of software changes in OpenStack over
the past 10 years are interdependent. Surprisingly, 51.08% of these
dependencies are identified during the code review phase-after a median delay
of 5.06 hours-rather than at the time of change creation. Developers often
spend a median of 57.12 hours identifying dependencies, searching among a
median of 463 other changes. To help developers proactively identify
dependencies, we propose a semi-automated approach that leverages two ML
models. The first model predicts the likelihood of dependencies among changes,
while the second identifies the exact pairs of dependent changes. Our proposed
models demonstrate strong performance, achieving average AUC scores of 79.33%
and 91.89%, and Brier scores of 0.11 and 0.014, respectively. Indeed, the
second model has a good top-k recall across all types of pairs, while the top-k
precision has room for improvement.

</details>


### [7] [LadyBug: A GitHub Bot for UI-Enhanced Bug Localization in Mobile Apps](https://arxiv.org/abs/2508.05085)
*Junayed Mahmud,James Chen,Terry Achille,Camilo Alvarez-Velez,Darren Dean Bansil,Patrick Ijieh,Samar Karanch,Nadeeshan De Silva,Oscar Chaparro,Andrian Marcus,Kevin Moran*

Main category: cs.SE

TL;DR: LadyBug是一款GitHub机器人，通过结合UI交互信息和文本检索，帮助Android应用自动定位bug。


<details>
  <summary>Details</summary>
Motivation: 开发LadyBug是为了提高Android应用bug定位的准确性和效率，结合文本描述和UI交互信息。

Method: LadyBug连接到GitHub仓库，利用开发者上传的bug重现追踪和原始文本，结合UI信息进行文件检索。

Result: 在RedWing基准测试中，LadyBug表现优于仅基于文本检索的方法，UI信息的利用显著提升了定位准确性。

Conclusion: LadyBug是一款开源工具，通过结合UI和文本信息，有效提升了Android应用bug定位的效率和准确性。

Abstract: This paper introduces LadyBug, a GitHub bot that automatically localizes bugs
for Android apps by combining UI interaction information with text retrieval.
LadyBug connects to an Android app's GitHub repository, and is triggered when a
bug is reported in the corresponding issue tracker. Developers can then record
a reproduction trace for the bug on a device or emulator and upload the trace
to LadyBug via the GitHub issue tracker. This enables LadyBug to utilize both
the text from the original bug description, and UI information from the
reproduction trace to accurately retrieve a ranked list of files from the
project that most likely contain the reported bug.
  We empirically evaluated LadyBug using an automated testing pipeline and
benchmark called RedWing that contains 80 fully-localized and reproducible bug
reports from 39 Android apps. Our results illustrate that LadyBug outperforms
text-retrieval-based baselines and that the utilization of UI information leads
to a substantial increase in localization accuracy. LadyBug is an open-source
tool, available at https://github.com/LadyBugML/ladybug.
  A video showing the capabilities of Ladybug can be viewed here:
https://youtu.be/hI3tzbRK0Cw

</details>


### [8] [Posterior-GRPO: Rewarding Reasoning Processes in Code Generation](https://arxiv.org/abs/2508.05170)
*Lishui Fan,Yu Zhang,Mouxiang Chen,Zhongxin Liu*

Main category: cs.SE

TL;DR: 论文提出了一个统一的框架，通过评估和奖励中间推理过程来改进强化学习在代码生成中的应用，解决了当前方法忽视推理质量和易受奖励攻击的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于测试结果的强化学习方法忽视了中间推理过程的质量，且容易受到奖励攻击的影响。

Method: 提出了LCB-RB基准来评估推理质量，采用OD-based方法训练奖励模型，并引入P-GRPO方法，根据任务成功选择性奖励推理过程。

Result: 7B参数的模型在代码生成任务中表现优异，比基线方法提升4.5%，接近GPT-4-Turbo的性能，并在数学任务中展示了泛化能力。

Conclusion: 该框架有效提升了强化学习在代码生成中的性能，通过结合推理质量和任务成功奖励，避免了奖励攻击，同时模型和数据集已公开。

Abstract: Reinforcement learning (RL) has significantly advanced code generation for
large language models (LLMs). However, current paradigms rely on outcome-based
rewards from test cases, neglecting the quality of the intermediate reasoning
process. While supervising the reasoning process directly is a promising
direction, it is highly susceptible to reward hacking, where the policy model
learns to exploit the reasoning reward signal without improving final outcomes.
To address this, we introduce a unified framework that can effectively
incorporate the quality of the reasoning process during RL. First, to enable
reasoning evaluation, we develop LCB-RB, a benchmark comprising preference
pairs of superior and inferior reasoning processes. Second, to accurately score
reasoning quality, we introduce an Optimized-Degraded based (OD-based) method
for reward model training. This method generates high-quality preference pairs
by systematically optimizing and degrading initial reasoning paths along
curated dimensions of reasoning quality, such as factual accuracy, logical
rigor, and coherence. A 7B parameter reward model with this method achieves
state-of-the-art (SOTA) performance on LCB-RB and generalizes well to other
benchmarks. Finally, we introduce Posterior-GRPO (P-GRPO), a novel RL method
that conditions process-based rewards on task success. By selectively applying
rewards to the reasoning processes of only successful outcomes, P-GRPO
effectively mitigates reward hacking and aligns the model's internal reasoning
with final code correctness. A 7B parameter model with P-GRPO achieves superior
performance across diverse code generation tasks, outperforming outcome-only
baselines by 4.5%, achieving comparable performance to GPT-4-Turbo. We further
demonstrate the generalizability of our approach by extending it to
mathematical tasks. Our models, dataset, and code are publicly available.

</details>


### [9] [AI-assisted JSON Schema Creation and Mapping](https://arxiv.org/abs/2508.05192)
*Felix Neubauer,Jürgen Pleiss,Benjamin Uekermann*

Main category: cs.SE

TL;DR: 论文提出了一个结合大型语言模型和确定性技术的混合方法，用于简化JSON Schema的创建和映射，适用于非专家用户。


<details>
  <summary>Details</summary>
Motivation: 许多领域缺乏标准化模型，且非专家创建这些模型困难，因此需要一种更易用的方法。

Method: 通过结合大型语言模型和确定性技术，MetaConfigurator工具提供了基于自然语言的JSON Schema编辑和映射功能。

Result: 工具支持从异构数据生成映射规则，并在化学领域中验证了其实用性。

Conclusion: 该工作显著降低了非专家在结构数据建模和数据集成中的门槛。

Abstract: Model-Driven Engineering (MDE) places models at the core of system and data
engineering processes. In the context of research data, these models are
typically expressed as schemas that define the structure and semantics of
datasets. However, many domains still lack standardized models, and creating
them remains a significant barrier, especially for non-experts. We present a
hybrid approach that combines large language models (LLMs) with deterministic
techniques to enable JSON Schema creation, modification, and schema mapping
based on natural language inputs by the user. These capabilities are integrated
into the open-source tool MetaConfigurator, which already provides visual model
editing, validation, code generation, and form generation from models. For data
integration, we generate schema mappings from heterogeneous JSON, CSV, XML, and
YAML data using LLMs, while ensuring scalability and reliability through
deterministic execution of generated mapping rules. The applicability of our
work is demonstrated in an application example in the field of chemistry. By
combining natural language interaction with deterministic safeguards, this work
significantly lowers the barrier to structured data modeling and data
integration for non-experts.

</details>


### [10] [STEPWISE-CODEX-Bench: Evaluating Complex Multi-Function Comprehension and Fine-Grained Execution Reasoning](https://arxiv.org/abs/2508.05193)
*Kaiwen Yan,Yuhang Chang,Zirui Guo,Yaling Mou,Jiang Ming,Jingwei Sun*

Main category: cs.SE

TL;DR: 本文提出了STEPWISE-CODEX-Bench（SX-Bench），一个专注于复杂多函数理解和细粒度执行推理的新基准，弥补了现有基准在高级代码智能模型评估中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如HumanEval、MBPP）主要关注功能正确性，而推理基准（如CRUXEVAL）局限于低复杂度场景，导致高级模型的分数趋近饱和，无法有效区分其能力。因此，需要一个更全面的评估工具来测试模型的复杂推理能力。

Method: SX-Bench设计了涉及多子函数协作的任务，并以“计算步骤”为最小执行单位，要求模型预测推理任务的总步骤数。此外，还开发了一个结合程序合成、符号执行和LLM辅助验证的自动生成管道。

Result: 在20多个主流模型上的评估显示，SX-Bench具有高度区分性：即使是OpenAI-O3在Hard-Reasoning任务上仅达到78.37%的准确率，远低于其在传统基准上的表现。

Conclusion: SX-Bench将代码评估从“单函数验证”推进到“多函数动态推理”，为深入评估高级代码智能模型提供了关键工具。

Abstract: In recent years, large language models (LLMs) have made significant progress
in code intelligence, yet systematically evaluating their code understanding
and reasoning abilities remains challenging. Mainstream benchmarks such as
HumanEval and MBPP primarily assess functional correctness, while reasoning
benchmarks like CRUXEVAL are limited to single-function, low-complexity
scenarios. As a result, advanced models achieve nearly saturated scores,
limiting their discriminative power. To address this, we present
STEPWISE-CODEX-Bench (SX-Bench), a novel benchmark designed for complex
multi-function understanding and fine-grained execution reasoning. SX-Bench
features tasks involving collaboration among multiple sub-functions (e.g.,
chained calls, nested loops), shifting evaluation towards overall control and
data flow modeling. It defines "computation steps" as the minimal execution
unit and requires models to predict the total number of steps in reasoning
tasks, thereby assessing a model's in-depth understanding of dynamic execution
beyond simple I/O matching. Evaluation on over 20 mainstream models (including
14 reasoning-enhanced models) demonstrates that SX-Bench is highly
discriminative: even the state-of-the-art OpenAI-O3 achieves only 78.37 percent
accuracy on Hard-Reasoning tasks, much lower than its saturated scores on
previous benchmarks, thereby revealing bottlenecks in complex and fine-grained
reasoning. We also release an automated pipeline combining program synthesis,
symbolic execution, and LLM-aided validation for efficient benchmark generation
and quality assurance. SX-Bench advances code evaluation from "single-function
verification" to "multi-function dynamic reasoning," providing a key tool for
the in-depth assessment of advanced code intelligence models.

</details>


### [11] [EvoGraph: Hybrid Directed Graph Evolution toward Software 3.0](https://arxiv.org/abs/2508.05199)
*Igor Costa,Christopher Baran*

Main category: cs.SE

TL;DR: EvoGraph框架通过类型化有向图和SLM驱动变异操作，实现软件系统的自我进化，修复漏洞、完成语言转换并优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统软件开发中遗留系统现代化和持续适应性的挑战。

Method: 使用类型化有向图表示软件产物，结合SLM驱动的变异操作和多目标适应度选择。

Result: 安全漏洞修复率83%，COBOL转Java功能等效率93%，计算成本降低90%，性能提升显著。

Conclusion: EvoGraph为软件3.0提供了一条可行路径，实现持续适应性和可控性。

Abstract: We introduce **EvoGraph**, a framework that enables software systems to
evolve their own source code, build pipelines, documentation, and tickets.
EvoGraph represents every artefact in a typed directed graph, applies learned
mutation operators driven by specialized small language models (SLMs), and
selects survivors with a multi-objective fitness. On three benchmarks, EvoGraph
fixes 83% of known security vulnerabilities, translates COBOL to Java with 93%
functional equivalence (test verified), and maintains documentation freshness
within two minutes. Experiments show a 40% latency reduction and a sevenfold
drop in feature lead time compared with strong baselines. We extend our
approach to **evoGraph**, leveraging language-specific SLMs for modernizing
.NET, Lisp, CGI, ColdFusion, legacy Python, and C codebases, achieving 82-96%
semantic equivalence across languages while reducing computational costs by 90%
compared to large language models. EvoGraph's design responds to empirical
failure modes in legacy modernization, such as implicit contracts, performance
preservation, and integration evolution. Our results suggest a practical path
toward Software 3.0, where systems adapt continuously yet remain under
measurable control.

</details>


### [12] [A Conceptual Model and Methodology for Sustainability-aware, IoT-enhanced Business Processes](https://arxiv.org/abs/2508.05301)
*Victoria Torres Bosch,Ronny Seiger,Manuela Albert Albiol,Antoni Mestre Gascon,Pedro Jose Valderas Aranda*

Main category: cs.SE

TL;DR: 该论文提出了一种概念模型和结构化方法，分析物联网（IoT）在衡量和改进业务流程（BPs）可持续性方面的潜力，并通过旅游和医疗案例进行验证。


<details>
  <summary>Details</summary>
Motivation: 尽管业务流程管理（BPM）中已研究可持续性，但主要集中在环境问题上；而实现全面且持久的可持续性需要系统性方法，涵盖更广泛的维度。

Method: 提出了一个概念模型和结构化方法，模型将关键可持续性概念与BPM和IoT联系起来，方法则指导系统分析现有BPs并实施可持续性感知的IoT增强BPs。

Result: 通过旅游领域的示例和医疗保健案例研究，验证了该方法的有效性。

Conclusion: 该研究为利用IoT提升BPs的可持续性提供了系统性框架，弥补了当前研究在非环境维度的不足。

Abstract: The real-time data collection and automation capabilities offered by the
Internet of Things (IoT) are revolutionizing and transforming Business
Processes (BPs) into IoT-enhanced BPs, showing high potential for improving
sustainability. Although already studied in Business Process Management (BPM),
sustainability research has primarily focused on environmental concerns.
However, achieving a holistic and lasting impact requires a systematic approach
to address sustainability beyond the environmental dimension. This work
proposes a conceptual model and a structured methodology with the goal of
analyzing the potential of IoT to measure and improve the sustainability of
BPs. The conceptual model formally represents key sustainability concepts,
linking BPM and IoT by highlighting how IoT devices support and contribute to
sustainability. The methodology guides the systematic analysis of existing BPs,
identifies opportunities, and implements sustainability-aware, IoT-enhanced
BPs. The approach is illustrated through a running example from the tourism
domain and a case study in healthcare.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Consistent Updates for Scalable Microservices](https://arxiv.org/abs/2508.04829)
*Devora Chait-Roth,Kedar S. Namjoshi,Thomas Wies*

Main category: cs.PL

TL;DR: 该论文提出了首个保证混合模式更新一致性的算法，通过利用服务动作的语义属性（如交换性），解决了在线服务动态更新中的混合模式不一致问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决在线服务动态更新过程中，由于新旧版本工作进程同时运行并交互导致的混合模式不一致问题。

Method: 提出了一种基于服务动作语义属性（如交换性）的算法框架，并通过理论分析验证了其正确性。

Result: 证明了仅凭语义无关的更新方法无法避免不一致性，并提出了一种能确保更新原子性的算法。

Conclusion: 该研究为混合模式更新提供了一种理论框架和有效算法，解决了动态服务更新中的一致性问题。

Abstract: Online services are commonly implemented with a scalable microservice
architecture, where isomorphic worker processes service client requests,
recording persistent state in a backend data store. To maintain service, any
modifications to the service functionality must be made on the fly -- i.e., as
the service continues to process client requests -- but doing so is
challenging. The central difficulty is that of avoiding potential
inconsistencies caused by ''mixed mode'' operation, where workers of current
and new versions are concurrently active and interact via the data store. Some
update methods avoid mixed mode altogether, but only at the cost of substantial
inefficiency -- by doubling resources (memory and compute), or by halving
throughput. The alternative is a so-called ''rolling'' update, which is
uncontrolled and runs the risk of serious service failures arising from
inconsistent mixed-mode behavior.
  In this paper, we present the first algorithms that guarantee consistency for
mixed mode updates. The algorithms rely on semantic properties of service
actions, such as commutativity. We show that semantic awareness is required, by
proving that any semantically oblivious, mixed-mode update method cannot avoid
inconsistencies. Ideally, it should appear to every client that a service
update takes effect atomically; this ensures that a client is not exposed to
inconsistent mixed-mode behavior. We introduce a framework that formalizes this
intuition and develop foundational theory for reasoning about the consistency
of mixed-mode updates, applying that theory to derive the new algorithms and
establish their correctness.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [14] [Mapping Sparse Triangular Solves to GPUs via Fine-grained Domain Decomposition](https://arxiv.org/abs/2508.04917)
*Atharva Gondhalekar,Kjetil Haugen,Thomas Gibson,Wu-chun Feng*

Main category: cs.PF

TL;DR: 该论文提出了一种基于细粒度域分解的策略，以优化GPU上的稀疏三角求解，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏线性系统的预处理迭代方法中，稀疏三角求解由于内存访问不规律和数据依赖导致性能瓶颈。

Method: 采用细粒度域分解技术生成非重叠子域，每个子域由线程块处理并适配GPU共享内存，以减少同步和全局内存访问。

Result: 在AMD GPU上实现了三角求解10.7倍和ILU0预条件的BiCGSTAB求解3.2倍的加速。

Conclusion: 该方法有效提升了GPU上预处理迭代求解的性能，适合高性能计算应用。

Abstract: Sparse linear systems are typically solved using preconditioned iterative
methods, but applying preconditioners via sparse triangular solves introduces
bottlenecks due to irregular memory accesses and data dependencies. This work
leverages fine-grained domain decomposition to adapt triangular solves to the
GPU architecture. We develop a fine-grained domain decomposition strategy that
generates non-overlapping subdomains, increasing parallelism in the application
of preconditioner at the expense of a modest increase in the iteration count
for convergence. Each subdomain is assigned to a thread block and is sized such
that the subdomain vector fits in the GPU shared memory, eliminating the need
for inter-block synchronization and reducing irregular global memory accesses.
Compared to other state-of-the-art implementations using the ROCm$^{\text{TM}}$
software stack, we achieve a 10.7$\times$ speedup for triangular solves and a
3.2$\times$ speedup for the ILU0-preconditioned biconjugate gradient stabilized
(BiCGSTAB) solver on the AMD Instinct$^{\text{TM}}$ MI210 GPU.

</details>


### [15] [Back to Bits: Extending Shannon's communication performance framework to computing](https://arxiv.org/abs/2508.05621)
*Max Hawkins,Richard Vuduc*

Main category: cs.PF

TL;DR: 提出了一种基于信息论的新型计算性能度量方法，以互信息衡量系统输入与输出之间的关系，适用于多样化的计算系统。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统日益多样化（如低精度格式、硬件专业化、模拟/量子/可逆逻辑等），传统指标（如浮点运算）已无法准确衡量性能。

Method: 将计算视为信息通过信道的转换，通过输入与输出之间的互信息定义计算性能。

Result: 提出了一种与实现无关的理论框架，可衡量计算中编码、处理和保留的有意义信息量。

Conclusion: 该框架为评估计算性能提供了理论基础，适用于新兴计算范式。

Abstract: This work proposes a novel computing performance unit grounded in information
theory. Modern computing systems are increasingly diverse, supporting
low-precision formats, hardware specialization, and emerging paradigms such as
analog, quantum, and reversible logic. Traditional metrics like floating-point
operations (flops) no longer accurately capture this complexity. We frame
computing as the transformation of information through a channel and define
performance in terms of the mutual information between a system's inputs and
outputs. This approach measures not just the quantity of data processed, but
the amount of meaningful information encoded, manipulated, and retained through
computation. Our framework provides a principled, implementation-agnostic
foundation for evaluating performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [16] [TeraRIS NOMA-MIMO Communications for 6G and Beyond Industrial Networks](https://arxiv.org/abs/2508.05130)
*Ali Raza,Muhammad Farhan Khan,Zeeshan Alam,Muhammad Saad,Ilyas Saleem,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Main category: cs.NI

TL;DR: 本文提出了一种结合可重构智能表面(RIS)、太赫兹(THz)通信和非正交多址(NOMA)的框架，以增强智能工业通信。通过优化功率分配策略，系统在频谱效率和可靠性方面取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 未来6G网络中，工业自动化和实时通信对频谱效率、覆盖范围和可靠性提出了更高要求。RIS和THz技术为解决这些问题提供了潜在方案。

Method: 设计了两种功率分配策略：一种是在近端和远端工业节点之间优化分配功率，另一种是根据网络需求优先级进一步提升系统性能。通过仿真验证方案的有效性。

Result: 相比于固定功率分配方案，提出的方案在30dBm时实现了23%的频谱效率增益，并降低了中断概率。

Conclusion: RIS辅助的NOMA MIMO框架在THz工业通信中表现高效且鲁棒，为未来网络提供了有前景的解决方案。

Abstract: This paper presents a joint framework that integrates reconfigurable
intelligent surfaces (RISs) with Terahertz (THz) communications and
non-orthogonal multiple access (NOMA) to enhance smart industrial
communications. The proposed system leverages the advantages of RIS and THz
bands to improve spectral efficiency, coverage, and reliability key
requirements for industrial automation and real-time communications in future
6G networks and beyond. Within this framework, two power allocation strategies
are investigated: the first optimally distributes power between near and far
industrial nodes, and the second prioritizes network demands to enhance system
performance further. A performance evaluation is conducted to compare the sum
rate and outage probability against a fixed power allocation scheme. Our scheme
achieves up to a 23% sum rate gain over fixed PA at 30 dBm. Simulation results
validate the theoretical analysis, demonstrating the effectiveness and
robustness of the RIS-assisted NOMA MIMO framework for THz enabled industrial
communications.

</details>


### [17] [Modular Design and Experimental Evaluation of 5G Mobile Cell Architectures Based on Overlay and Integrated Models](https://arxiv.org/abs/2508.05249)
*José Ruela,Ivan Cojocaru,André Coelho,Rui Campos,Manuel Ricardo*

Main category: cs.NI

TL;DR: 本文介绍了用于在基础设施有限或无线条件较差区域提供5G连接的移动蜂窝（MC）的概念、设计和性能评估，探讨了两种主要设计模型及其在网络性能中的影响。


<details>
  <summary>Details</summary>
Motivation: 解决在固定5G基础设施不足或无线条件恶劣区域提供可靠5G连接的问题。

Method: 提出两种MC设计模型（覆盖模型和基于IAB的模型），并使用OpenAirInterface仿真验证性能。

Result: 验证了MC概念的有效性，并证明MC位置对网络性能有显著影响。

Conclusion: MC架构可为临时覆盖扩展和容量增强提供解决方案，适用于海港、工业场景和公共安全等领域。

Abstract: This paper presents the concept, architectural design, and performance
evaluation of a 5G Mobile Cell (MC) used to provide 5G wireless connectivity to
User Equipment (UE) in areas with limited fixed 5G infrastructures or subject
to adverse radio conditions. We consider two main approaches to MC design: an
overlay model, where the MC obtains backhaul connectivity from a 5G overlay
network, and an Integrated Access and Backhaul (IAB)-based model, discussing
their protocol stacks and architectural implications. In order to validate the
MC's performance, we employ an emulation-based testbed using the
OpenAirInterface (OAI) implementation, considering different MC positions. The
results validate the MC concept and demonstrate that MC positioning
significantly influences network performance. This paper has the potential to
aid network operators and service providers in selecting and deploying MC
architectures for temporary coverage extension and capacity reinforcement in
different environments, including seaports, industrial scenarios, and public
safety.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [18] [JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering](https://arxiv.org/abs/2508.05087)
*Renmiao Chen,Shiyao Cui,Xuancheng Huang,Chengwei Pan,Victor Shea-Jay Huang,QingLin Zhang,Xuan Ouyang,Zhexin Zhang,Hongning Wang,Minlie Huang*

Main category: cs.MM

TL;DR: 论文提出了JPS方法，通过结合视觉对抗图像和文本提示来实现对多模态大语言模型（MLLMs）的越狱攻击，并引入了新的评估指标MIFR，显著提升了攻击成功率和恶意意图实现率。


<details>
  <summary>Details</summary>
Motivation: 当前针对MLLMs的越狱攻击研究过于关注攻击成功率（ASR），忽视了生成响应是否真正实现攻击者的恶意意图，导致输出质量低下。

Method: JPS方法通过协同优化视觉对抗图像扰动和文本提示（steering prompt），实现对模型的越狱攻击，并使用多代理系统优化提示以引导模型满足恶意意图。

Result: 实验表明，JPS在多种MLLMs和基准测试中均达到了最高水平的ASR和MIFR，验证了其有效性。

Conclusion: JPS不仅在攻击成功率上表现出色，还通过新指标MIFR确保攻击响应真正满足恶意意图，推动了该领域的研究进展。

Abstract: Jailbreak attacks against multimodal large language Models (MLLMs) are a
significant research focus. Current research predominantly focuses on
maximizing attack success rate (ASR), often overlooking whether the generated
responses actually fulfill the attacker's malicious intent. This oversight
frequently leads to low-quality outputs that bypass safety filters but lack
substantial harmful content. To address this gap, we propose JPS,
\underline{J}ailbreak MLLMs with collaborative visual \underline{P}erturbation
and textual \underline{S}teering, which achieves jailbreaks via corporation of
visual image and textually steering prompt. Specifically, JPS utilizes
target-guided adversarial image perturbations for effective safety bypass,
complemented by "steering prompt" optimized via a multi-agent system to
specifically guide LLM responses fulfilling the attackers' intent. These visual
and textual components undergo iterative co-optimization for enhanced
performance. To evaluate the quality of attack outcomes, we propose the
Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a
Reasoning-LLM-based evaluator. Our experiments show JPS sets a new
state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with
analyses confirming its efficacy. Codes are available at
\href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}.
\color{warningcolor}{Warning: This paper contains potentially sensitive
contents.}

</details>


### [19] [Embedding Alignment in Code Generation for Audio](https://arxiv.org/abs/2508.05473)
*Sam Kouteili,Hiren Madhu,George Typaldos,Mark Santolucito*

Main category: cs.MM

TL;DR: 论文探讨了LLM生成的代码在多变的创意编码（如现场编码）中的应用，旨在通过分析代码与音频嵌入空间的映射关系，提高生成代码的音乐多样性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过建立代码与音频嵌入空间的映射关系，帮助用户更好地实现音乐意图，解决现有代码生成模型在提供多样性和独特性方面的不足。

Method: 方法包括调查代码与音频嵌入空间的拓扑关系，并构建一个预测模型来学习嵌入对齐映射，以预测代码生成的音频嵌入。

Result: 研究发现代码与音频嵌入之间不存在简单的线性关系，但可以通过构建的预测模型学习嵌入对齐映射。

Conclusion: 结论是通过构建代码-音频嵌入对齐映射模型，能够增强生成代码的音乐多样性，从而更好地支持创意编码任务。

Abstract: LLM-powered code generation has the potential to revolutionize creative
coding endeavors, such as live-coding, by enabling users to focus on structural
motifs over syntactic details. In such domains, when prompting an LLM, users
may benefit from considering multiple varied code candidates to better realize
their musical intentions. Code generation models, however, struggle to present
unique and diverse code candidates, with no direct insight into the code's
audio output. To better establish a relationship between code candidates and
produced audio, we investigate the topology of the mapping between code and
audio embedding spaces. We find that code and audio embeddings do not exhibit a
simple linear relationship, but supplement this with a constructed predictive
model that shows an embedding alignment map could be learned. Supplementing the
aim for musically diverse output, we present a model that given code predicts
output audio embedding, constructing a code-audio embedding alignment map.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [20] [AI Should Be More Human, Not More Complex](https://arxiv.org/abs/2508.04713)
*Carlo Esposito*

Main category: cs.HC

TL;DR: 用户更喜欢简洁、有来源标注的AI回答，而非复杂冗长的解释。研究发现，过于复杂的AI回答会降低用户信任并增加认知负担。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在搜索应用中过于追求复杂回答的现象，及其对用户满意度和参与度的负面影响。

Method: 通过对10,000名参与者比较五种AI搜索系统的回答，分析用户偏好。

Result: 用户偏好简洁且有来源标注的回答，复杂回答会导致“人工复杂”效应，降低信任。

Conclusion: AI回答应像人类交流一样简洁、透明，并标注来源，以提高用户参与和系统可靠性。

Abstract: Large Language Models (LLMs) in search applications increasingly prioritize
verbose, lexically complex responses that paradoxically reduce user
satisfaction and engagement. Through a comprehensive study of 10.000 (est.)
participants comparing responses from five major AI-powered search systems, we
demonstrate that users overwhelmingly prefer concise, source-attributed
responses over elaborate explanations. Our analysis reveals that current AI
development trends toward "artificial sophistication" create an uncanny valley
effect where systems sound knowledgeable but lack genuine critical thinking,
leading to reduced trust and increased cognitive load. We present evidence that
optimal AI communication mirrors effective human discourse: direct, properly
sourced, and honest about limitations. Our findings challenge the prevailing
assumption that more complex AI responses indicate better performance, instead
suggesting that human-like brevity and transparency are key to user engagement
and system reliability.

</details>


### [21] [Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts](https://arxiv.org/abs/2508.04787)
*Vishnu Menon,Andy Cherney,Elizabeth B. Cloude,Li Zhang,Tiffany D. Do*

Main category: cs.HC

TL;DR: 研究了在AI生成的播客中嵌入LLM引导的反思提示是否比无提示版本更能提升学习和用户体验。


<details>
  <summary>Details</summary>
Motivation: 探讨反思提示对学习和用户体验的影响。

Method: 36名本科生参与实验，比较有提示和无提示版本的播客。

Result: 学习效果相似，但反思提示降低了用户的感知吸引力。

Conclusion: 需要进一步研究反思交互设计。

Abstract: This study examined whether embedding LLM-guided reflection prompts in an
interactive AI-generated podcast improved learning and user experience compared
to a version without prompts. Thirty-six undergraduates participated, and while
learning outcomes were similar across conditions, reflection prompts reduced
perceived attractiveness, highlighting a call for more research on reflective
interactivity design.

</details>


### [22] [At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp](https://arxiv.org/abs/2508.04821)
*Yang Liu,Thorbjørn Mikkelsen,Zehai Liu,Gengchen Tian,Diako Mardanbegi,Qiushi Zhou,Hans Gellersen,Ken Pfeuffer*

Main category: cs.HC

TL;DR: SightWarp是一种利用眼手协调的交互技术，通过生成远距离对象的近空间代理，实现自然直接的手势操作，提升远距离交互的体验和效率。


<details>
  <summary>Details</summary>
Motivation: 解决远距离物体交互中直接手势缺乏的问题，提供更自然的操作方式。

Method: 利用眼手协调触发远距离对象的近空间代理生成，支持直接手势操作。

Result: 用户研究表明，SightWarp操作直观，直接手势的性能优于视线和捏合。

Conclusion: SightWarp为远近空间的物体交互提供了灵活且高效的解决方案。

Abstract: In 3D user interfaces, reaching out to grab and manipulate something works
great until it is out of reach. Indirect techniques like gaze and pinch offer
an alternative for distant interaction, but do not provide the same immediacy
or proprioceptive feedback as direct gestures. To support direct gestures for
faraway objects, we introduce SightWarp, an interaction technique that exploits
eye-hand coordination to seamlessly summon object proxies to the user's
fingertips. The idea is that after looking at a distant object, users either
shift their gaze to the hand or move their hand into view-triggering the
creation of a scaled near-space proxy of the object and its surrounding
context. The proxy remains active until the eye-hand pattern is released. The
key benefit is that users always have an option to immediately operate on the
distant object through a natural, direct hand gesture. Through a user study of
a 3D object docking task, we show that users can easily employ SightWarp, and
that subsequent direct manipulation improves performance over gaze and pinch.
Application examples illustrate its utility for 6DOF manipulation,
overview-and-detail navigation, and world-in-miniature interaction. Our work
contributes to expressive and flexible object interactions across near and far
spaces.

</details>


### [23] [Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction](https://arxiv.org/abs/2508.04842)
*Amit Kumar Das,Mohammad Tarun,Klaus Mueller*

Main category: cs.HC

TL;DR: 本文通过引入Charts-of-Thought提示技术，评估了现代大型语言模型（LLMs）的可视化素养，并在VLAT测试中显著提升了模型表现，尤其Claude-3.7-sonnet超过人类基准。


<details>
  <summary>Details</summary>
Motivation: 研究旨在测试LLMs的可视化理解能力，并探索如何通过结构化提示技术提升其表现。

Method: 采用Charts-of-Thought方法，系统化引导LLMs进行数据提取、验证和分析，再回答可视化问题。

Result: Claude-3.7-sonnet得分50.17，远超人类基准28.82；所有模型表现均有提升，增幅最高达21.8%。

Conclusion: 结构化提示技术不仅能显著提升LLMs的可视化素养，还可增强视觉信息的可访问性，尤其对视觉障碍者或低素养人群有益。

Abstract: This paper evaluates the visualization literacy of modern Large Language
Models (LLMs) and introduces a novel prompting technique called
Charts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet,
GPT-4.5 preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment
Test (VLAT) using standard prompts and our structured approach. The
Charts-of-Thought method guides LLMs through a systematic data extraction,
verification, and analysis process before answering visualization questions.
Our results show Claude-3.7-sonnet achieved a score of 50.17 using this method,
far exceeding the human baseline of 28.82. This approach improved performance
across all models, with score increases of 21.8% for GPT-4.5, 9.4% for
Gemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The
performance gains were consistent across original and modified VLAT charts,
with Claude correctly answering 100% of questions for several chart types that
previously challenged LLMs. Our study reveals that modern multimodal LLMs can
surpass human performance on visualization literacy tasks when given the proper
analytical framework. These findings establish a new benchmark for LLM
visualization literacy and demonstrate the importance of structured prompting
strategies for complex visual interpretation tasks. Beyond improving LLM
visualization literacy, Charts-of-Thought could also enhance the accessibility
of visualizations, potentially benefiting individuals with visual impairments
or lower visualization literacy.

</details>


### [24] [An Implementation of a Visual Stepper in the GRASP Programming System](https://arxiv.org/abs/2508.04859)
*Panicz Maciej Godek*

Main category: cs.HC

TL;DR: 本文介绍了如何在GRASP编程系统中实现视觉评估器扩展，并围绕GRASP的设计尤其是其扩展机制架构提供教程。


<details>
  <summary>Details</summary>
Motivation: 展示GRASP编程系统中视觉评估器扩展的实现方法，同时探讨类似GRASP系统的设计问题。

Method: 描述GRASP的扩展机制架构，并讨论其实现过程中的问题和解决方案。

Result: 尽管GRASP及其扩展机制尚未最终完成，但提供了相关设计问题的见解。

Conclusion: GRASP的设计问题及其解决方案对Scheme社区具有参考价值。

Abstract: The direct purpose of this paper - as its title suggests - is to present how
the visual evaluator extension is implemented in the GRASP programming system.
The indirect purpose is to provide a tutorial around the design of GRASP, and
in particular - around the architecture of its extension mechanism. Neither
GRASP nor its extension mechanisms are, at the moment of writing this paper,
final or complete, and we are certain that some details of the solutions
described in here will change even before the first release. What will not
change, though, is the set of problems that need to be solved in order to build
a system with capabilities similar to those of GRASP. We believe that these
problems might be of interest to the Scheme community.

</details>


### [25] [Learning AI Auditing: A Case Study of Teenagers Auditing a Generative AI Model](https://arxiv.org/abs/2508.04902)
*Luis Morales-Navarro,Michelle Gan,Evelyn Yu,Lauren Vogelstein,Yasmin B. Kafai,Danaé Metaxa*

Main category: cs.HC

TL;DR: 研究探讨了高中生如何通过算法审计认识AI/ML工具的偏见，并通过参与式设计研讨会展示了他们的创造性和独立性。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML技术融入青少年生活，需要培养其AI素养，提升技术认知和社会影响意识。

Method: 与14名青少年进行为期两周的参与式设计研讨会，审计TikTok的生成AI模型。

Result: 青少年独立发现年龄偏见等专业审计中少见的问题，审计结论与专家相似。

Conclusion: 算法审计可作为培养AI素养的学习活动，帮助青少年批判性审视AI系统并提供新视角。

Abstract: This study investigates how high school-aged youth engage in algorithm
auditing to identify and understand biases in artificial intelligence and
machine learning (AI/ML) tools they encounter daily. With AI/ML technologies
being increasingly integrated into young people's lives, there is an urgent
need to equip teenagers with AI literacies that build both technical knowledge
and awareness of social impacts. Algorithm audits (also called AI audits) have
traditionally been employed by experts to assess potential harmful biases, but
recent research suggests that non-expert users can also participate
productively in auditing. We conducted a two-week participatory design workshop
with 14 teenagers (ages 14-15), where they audited the generative AI model
behind TikTok's Effect House, a tool for creating interactive TikTok filters.
We present a case study describing how teenagers approached the audit, from
deciding what to audit to analyzing data using diverse strategies and
communicating their results. Our findings show that participants were engaged
and creative throughout the activities, independently raising and exploring new
considerations, such as age-related biases, that are uncommon in professional
audits. We drew on our expertise in algorithm auditing to triangulate their
findings as a way to examine if the workshop supported participants to reach
coherent conclusions in their audit. Although the resulting number of changes
in race, gender, and age representation uncovered by the teens were slightly
different from ours, we reached similar conclusions. This study highlights the
potential for auditing to inspire learning activities to foster AI literacies,
empower teenagers to critically examine AI systems, and contribute fresh
perspectives to the study of algorithmic harms.

</details>


### [26] [Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept](https://arxiv.org/abs/2508.04904)
*Yuqi Hu,Qiwen Xiong,Zhenzhen Qin,Brandon Watanabe,Yujing Wang,Mirjana Prpa,Ilmi Yoon*

Main category: cs.HC

TL;DR: 该论文提出了一种AI驱动的3D模拟游戏，通过互动沉浸式模拟帮助医疗专业人员提升RCA技能，解决传统培训资源密集的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RCA培训资源需求高，导致培训不足和实施不一致，亟需一种成本低、可扩展且易获取的替代方案。

Method: 开发AI驱动的3D模拟游戏，利用LLM、情感语音合成和AI动画实现与虚拟角色的自然互动，并提供反馈机制。

Result: 原型系统模拟ICU不良事件调查，用户可与虚拟角色互动完成报告，并获得反馈以改进技能。

Conclusion: 未来计划实证评估系统的有效性，旨在为医疗RCA培训提供创新解决方案。

Abstract: Root Cause Analysis (RCA) is a critical tool for investigating adverse events
in healthcare and improving patient safety. However, existing RCA training
programs are often limited by high resource demands, leading to insufficient
training and inconsistent implementation. To address this challenge, we present
an AI-powered 3D simulation game that helps healthcare professionals develop
RCA skills through interactive, immersive simulations. This approach offers a
cost-effective, scalable, and accessible alternative to traditional training.
The prototype simulates an RCA investigation following a death in the ICU,
where learners interview five virtual avatars representing ICU team members to
investigate the incident and complete a written report. The system enables
natural, life-like interactions with avatars via large language models (LLMs),
emotional text-to-speech, and AI-powered animations. An additional LLM
component provides formative and summative feedback to support continual
improvement. We conclude by outlining plans to empirically evaluate the
system's efficacy.

</details>


### [27] [Toward Supporting Narrative-Driven Data Exploration: Barriers and Design Opportunities](https://arxiv.org/abs/2508.04920)
*Oliver Huang,Carolina Nobre*

Main category: cs.HC

TL;DR: 论文探讨了数据分析师在叙事驱动查询中的挑战，提出了设计改进机会。


<details>
  <summary>Details</summary>
Motivation: 随着数据分析从静态仪表板转向更深入的叙事驱动查询，分析师在维护上下文和跟踪推理路径时遇到困难。

Method: 通过对48名参与者进行形成性研究，识别了叙事驱动探索中的关键障碍。

Result: 研究发现，分析师在维护跨视图上下文、跟踪推理路径和外部化解释方面存在困难。

Conclusion: 研究为支持叙事驱动分析提供了设计机会。

Abstract: Analysts increasingly explore data through evolving, narrative-driven
inquiries, moving beyond static dashboards and predefined metrics as their
questions deepen and shift. As these explorations progress, insights often
become dispersed across views, making it challenging to maintain context or
clarify how conclusions arise. Through a formative study with 48 participants,
we identify key barriers that hinder narrative-driven exploration, including
difficulty maintaining context across views, tracing reasoning paths, and
externalizing evolving interpretations. Our findings surface design
opportunities to support narrative-driven analysis better.

</details>


### [28] [Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge](https://arxiv.org/abs/2508.04995)
*Matthew Kelly*

Main category: cs.HC

TL;DR: 论文提出Situated Epistemic Infrastructures (SEI)框架，分析在后连贯性条件下知识如何在人机混合系统中获得权威性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如ChatGPT）暴露了当前知识基础设施的脆弱性，其模拟连贯性却绕过了传统引用、权威和验证方式。

Method: SEI框架结合基础设施研究、平台理论和认识论，追踪信誉在机构、计算和时间安排中的媒介作用。

Result: 框架强调协调而非分类，提出前瞻性和适应性的知识管理模式。

Conclusion: SEI框架为AI治理、知识生产和信息系统伦理设计提供了新视角，取代传统的表征主义模型。

Abstract: Large Language Models (LLMs) such as ChatGPT have rendered visible the
fragility of contemporary knowledge infrastructures by simulating coherence
while bypassing traditional modes of citation, authority, and validation. This
paper introduces the Situated Epistemic Infrastructures (SEI) framework as a
diagnostic tool for analyzing how knowledge becomes authoritative across hybrid
human-machine systems under post-coherence conditions. Rather than relying on
stable scholarly domains or bounded communities of practice, SEI traces how
credibility is mediated across institutional, computational, and temporal
arrangements. Integrating insights from infrastructure studies, platform
theory, and epistemology, the framework foregrounds coordination over
classification, emphasizing the need for anticipatory and adaptive models of
epistemic stewardship. The paper contributes to debates on AI governance,
knowledge production, and the ethical design of information systems by offering
a robust alternative to representationalist models of scholarly communication.

</details>


### [29] [Human-AI Schema Discovery and Application for Creative Problem Solving](https://arxiv.org/abs/2508.05045)
*Sitong Wang*

Main category: cs.HC

TL;DR: 该论文提出了一种人机协作的框架，用于发现和应用结构性模式（schema）来支持创造性问题解决。


<details>
  <summary>Details</summary>
Motivation: 人类在创造性活动中依赖结构性模式（如写作、设计、音乐创作），但这些模式在复杂或陌生领域中难以发现和应用。

Method: 设计了支持用户从示例中抽象出模式，并将模式转化为人机协作工作流的系统。

Result: 研究表明模式引导的交互可以使隐性知识更易获取和应用。

Conclusion: 该框架提升了人机系统的透明度和协作性。

Abstract: Humans often rely on underlying structural patterns-schemas-to create,
whether by writing stories, designing software, or composing music. Schemas
help organize ideas and guide exploration, but they are often difficult to
discover and apply, especially in complex or unfamiliar domains. My Ph.D.
research develops a framework for human-AI schema discovery and application to
support creative problem solving. I design systems that support users in
sensemaking over examples to abstract schemas, and in operationalizing schemas
into human-AI co-creative workflows for application. This research offers
insights into how schema-guided interaction can make implicit knowledge more
accessible and actionable, advancing more transparent and collaborative
human-AI systems.

</details>


### [30] [Accessibility Beyond Accommodations: A Systematic Redesign of Introduction to Computer Science for Students with Visual Impairments](https://arxiv.org/abs/2508.05056)
*Vaanee Tripathi,Aalok Thakkar*

Main category: cs.HC

TL;DR: 论文提出一个综合框架，重新设计计算机科学入门课程，为视障学生提供公平学习体验，不依赖特殊技术设施。


<details>
  <summary>Details</summary>
Motivation: 现有计算机科学教育仍存在系统性障碍，视障学生参与受限；现有解决方案分散且技术复杂，缺乏整体课程设计。

Method: 框架包含五个关键组件：可访问学习资源、课堂学习工具包、支持系统、在线工具库和心理支持，基于通用设计原则并通过专家验证。

Result: 该框架从技术和教学维度解决包容性教育问题，适合标准大学环境实施。

Conclusion: 框架为视障学生提供系统化支持，未来可进一步评估效果并推广到更广机构。

Abstract: Computer science education has evolved extensively; however, systemic
barriers still prevent students with visual impairments from fully
participating. While existing research has developed specialized programming
tools and assistive technologies, these solutions remain fragmented and often
require complex technical infrastructure, which limits their classroom
implementation. Current approaches treat accessibility as individual
accommodations rather than integral curriculum design, creating gaps in
holistic educational support. This paper presents a comprehensive framework for
redesigning introductory computer science curricula to provide equitable
learning experiences for students with visual impairments without requiring
specialized technical infrastructure. The framework outlines five key
components that together contribute a systematic approach to curriculum
accessibility: accessible learning resources with pre-distributed materials and
tactile diagrams, in-class learning kits with hands-on demonstrations,
structured support systems with dedicated teaching assistance, an online tool
repository, and psychosocial support for classroom participation. Unlike
existing tool-focused solutions, this framework addresses both technical and
pedagogical dimensions of inclusive education while emphasizing practical
implementation in standard university settings. The design is grounded in
universal design principles and validated through expert consultation with
accessibility specialists and disability services professionals, establishing
foundations for future empirical evaluation of learning outcomes and student
engagement while serving as a template for broader institutional adoption.

</details>


### [31] [A Desktop-Centric Design Space for Direct Object Examination and Visualization in Mixed-Reality Environments](https://arxiv.org/abs/2508.05088)
*Sam Johnson-Lacoss,Santiago V. Lombeyda,S. George Djorgovski*

Main category: cs.HC

TL;DR: 摘要讨论了混合现实（MR）技术在科研和临床工作中的重要性，并提出了一种设计空间的分类方法，以优化用户体验。


<details>
  <summary>Details</summary>
Motivation: 随着MR技术的普及，如何在桌面物理空间中高效利用3D交互进行精确数据分析成为关键问题。

Method: 通过分析不同的交互区域和模式，提出了一种适合MR技术应用的设计空间分类。

Result: 设计空间分类方法能够支持用户在MR环境中进行更高效的对象中心数据分析。

Conclusion: MR技术为科研和临床工作提供了更直观的3D交互方式，设计空间分类有助于优化用户体验。

Abstract: Mixed reality (MR) environments are bound to become ubiquitous as MR
technology becomes lighter, higher resolution, more affordable, and overall
becomes a seamless extension of our current work and living spaces. For
research scientists and clinicians focused on understanding 3D phenomena or
patient pathologies within the context of the larger human anatomy, that means
a necessary evolution of their workstations currently only utilizing 2D
interfaces for everyday communication, logistics and data analysis. MR
technologies bring forth immersive 3D representations coexisting in our natural
spaces, while allowing for richer interconnected information displays, where 3D
representations greatly aid in the detailed understanding of physical
structures, spatial relationships, and 3D contextualization of 2D measurements,
projections, abstractions, and other data details. We present a breakdown of
the different interaction zones and modalities into a design space that best
accommodates the creation of applications for users engaged through MR
technologies in precise object-centric data analysis within the ergonomic
confines of their desktop physical spaces.

</details>


### [32] [SparseEMG: Computational Design of Sparse EMG Layouts for Sensing Gestures](https://arxiv.org/abs/2508.05098)
*Anand Kumar,Antony Albert Raj Irudayaraj,Ishita Chandra,Adwait Sharma,Aditya Shekhar Nittala*

Main category: cs.HC

TL;DR: 该论文提出了一种基于数据驱动的方法，分析电极选择和分类器选择对EMG手势识别精度和稀疏性的影响，并开发了一个工具SparseEMG，用于生成高效的稀疏电极布局。


<details>
  <summary>Details</summary>
Motivation: 现有工具包大多忽视电极选择对分类精度的影响，论文旨在填补这一空白，优化电极数量和分类性能。

Method: 通过系统评估28种组合（4种选择方案，7种分类器），在六个数据集上识别出在保证精度的同时最小化电极数量的方法。

Result: Permutation Importance选择方案与Random Forest分类器结合，可将电极数量减少53.5%，开发了工具SparseEMG并在三个实际应用中验证。

Conclusion: SparseEMG生成的布局在不同用户间具有可转移性，且识别性能变化小，为EMG手势识别提供了实用工具。

Abstract: Gesture recognition with electromyography (EMG) is a complex problem
influenced by gesture sets, electrode count and placement, and machine learning
parameters (e.g., features, classifiers). Most existing toolkits focus on
streamlining model development but overlook the impact of electrode selection
on classification accuracy. In this work, we present the first data-driven
analysis of how electrode selection and classifier choice affect both accuracy
and sparsity. Through a systematic evaluation of 28 combinations (4 selection
schemes, 7 classifiers), across six datasets, we identify an approach that
minimizes electrode count without compromising accuracy. The results show that
Permutation Importance (selection scheme) with Random Forest (classifier)
reduces the number of electrodes by 53.5\%. Based on these findings, we
introduce SparseEMG, a design tool that generates sparse electrode layouts
based on user-selected gesture sets, electrode constraints, and ML parameters
while also predicting classification performance. SparseEMG supports 50+ unique
gestures and is validated in three real-world applications using different
hardware setups. Results from our multi-dataset evaluation show that the
layouts generated from the SparseEMG design tool are transferable across users
with only minimal variation in gesture recognition performance.

</details>


### [33] [Metacognition and self-regulated learning in manipulative robotic problem-solving task](https://arxiv.org/abs/2508.05112)
*Margarida Romero,George Kalmpourtzis*

Main category: cs.HC

TL;DR: 本章探讨了元认知在创造性问题解决（CPS）中的作用，重点分析元推理如何监控和调节学习者在解决问题过程中的探索与利用行为。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过元认知和互动主义方法，分析教育机器人情境中的CPS过程，揭示元推理如何调节学习者的探索与知识利用行为。

Method: 采用案例研究法，参与者通过探索机器人立方体，逐步构建技术知识并形成系统级行为理解。

Result: 研究发现，元认知调节下的探索与知识利用行为促进了知识的涌现，最终促成问题解决。

Conclusion: 元认知在CPS中起到关键作用，通过调节探索与利用行为，帮助学习者逐步构建和整合知识。

Abstract: Metacognition is an important aspect in creative problem solving (CPS) and
through this chapter we analyse the meta-reasoning aspects applied in the
different processes of monitoring the progress of learners' reasoning and CPS
activities. Meta-reasoning monitors the way that problem-solving processes
advance and regulate time and efforts towards a solution. In the context of an
ill-defined problem, exploration is required to develop a better-defined
problem space and advance towards the solution space. The way learners engage
in exploration and exploitations is regulated by the meta-reasoning within the
CPS activity. The objective of this chapter is to examine and identify the CPS
process with educational robots through a metacognitive and interactionist
approach. This chapter presents a case study, where, to solve a problem, a
participant had to explore a set of robot cubes to develop the technological
knowledge associated with each single component of the system, but also
conceptualize a system-level behaviour of the cubes when they are assembled.
The chapter presents the emergence of knowledge through the metacognitive
regulation of the process of exploration and exploitation of prior knowledge
and emergent knowledge until finding a solution

</details>


### [34] [AI Conversational Tutors in Foreign Language Learning: A Mixed-Methods Evaluation Study](https://arxiv.org/abs/2508.05156)
*Nikolaos Avouris*

Main category: cs.HC

TL;DR: 研究探讨AI外语学习导师的应用及用户体验评估，重点关注对话功能和质量评估，为未来工具设计提供参考。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理的进步，AI导师在外语学习中应用广泛，研究旨在评估其效果并提升设计标准。

Method: 采用混合方法实证研究，评估多种先进AI导师的用户体验，分析聊天记录以确定质量。

Result: 研究为AI导师质量评估提供标准，并关注数据隐私和学员信息安全的未来设计。

Conclusion: AI导师在外语学习中潜力巨大，需在功能和隐私保护方面进一步优化。

Abstract: This paper focuses on AI tutors in foreign language learning, a field of
application of AI tutors with great development, especially during the last
years, when great advances in natural language understanding and processing in
real time, have been achieved. These tutors attempt to address needs for
improving language skills (speaking, or communicative competence,
understanding). In this paper, a mixed-methos empirical study on the use of
different kinds of state-of-the-art AI tutors for language learning is
reported. This study involves a user experience evaluation of typical such
tools, with special focus in their conversation functionality and an evaluation
of their quality, based on chat transcripts. This study can help establish
criteria for assessing the quality of such systems and inform the design of
future tools, including concerns about data privacy and secure handling of
learner information.

</details>


### [35] [CWEFS: Brain volume conduction effects inspired channel-wise EEG feature selection for multi-dimensional emotion recognition](https://arxiv.org/abs/2508.05228)
*Xueyuan Xu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 提出了一种新颖的CWEFS方法，用于解决多通道EEG特征选择中的冗余问题，并通过共享潜在结构模型提升情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有的EEG特征选择方法忽视了潜在EEG特征结构对情感标签相关性的影响，且假设各通道重要性相同，限制了多维情感计算的精确建模。

Method: CWEFS结合了EEG情感特征选择和共享潜在结构模型，通过构建共识潜在空间和自适应通道权重学习，优化特征选择。

Result: 在三种流行的多维度情感标签EEG数据集上验证，CWEFS在六种评价指标中表现最优。

Conclusion: CWEFS方法显著提升了情感识别性能，解决了现有方法的局限性，具有更高的透明度和可解释性。

Abstract: Due to the intracranial volume conduction effects, high-dimensional
multi-channel electroencephalography (EEG) features often contain substantial
redundant and irrelevant information. This issue not only hinders the
extraction of discriminative emotional representations but also compromises the
real-time performance. Feature selection has been established as an effective
approach to address the challenges while enhancing the transparency and
interpretability of emotion recognition models. However, existing EEG feature
selection research overlooks the influence of latent EEG feature structures on
emotional label correlations and assumes uniform importance across various
channels, directly limiting the precise construction of EEG feature selection
models for multi-dimensional affective computing. To address these limitations,
a novel channel-wise EEG feature selection (CWEFS) method is proposed for
multi-dimensional emotion recognition. Specifically, inspired by brain volume
conduction effects, CWEFS integrates EEG emotional feature selection into a
shared latent structure model designed to construct a consensus latent space
across diverse EEG channels. To preserve the local geometric structure, this
consensus space is further integrated with the latent semantic analysis of
multi-dimensional emotional labels. Additionally, CWEFS incorporates adaptive
channel-weight learning to automatically determine the significance of
different EEG channels in the emotional feature selection task. The
effectiveness of CWEFS was validated using three popular EEG datasets with
multi-dimensional emotional labels. Comprehensive experimental results,
compared against nineteen feature selection methods, demonstrate that the EEG
feature subsets chosen by CWEFS achieve optimal emotion recognition performance
across six evaluation metrics.

</details>


### [36] [ADSEL: Adaptive dual self-expression learning for EEG feature selection via incomplete multi-dimensional emotional tagging](https://arxiv.org/abs/2508.05229)
*Tianze Yu,Junming Zhang,Wenjia Dong,Xueyuan Xu,Li Zhuo*

Main category: cs.HC

TL;DR: 本文提出一种新型的EEG特征选择算法（ADSEL），用于解决多维度情绪识别中标签不完整的问题，通过自适应双自表达学习和最小二乘回归提高标签恢复精度和特征选择效果。


<details>
  <summary>Details</summary>
Motivation: 针对EEG情绪识别中特征维度高、样本少导致过拟合以及标签不完整影响模型泛化的问题，本文旨在提出一种有效特征选择算法。

Method: 提出ADSEL算法，结合自适应双自表达学习和最小二乘回归，在标签空间中建立样本级和维度级的双向信息共享路径，用于标签重构和特征选择。

Result: ADSEL能够提高标签恢复精度，并有效识别最优EEG特征子集，提升多维度情绪识别的效果。

Conclusion: ADSEL算法通过双自表达学习解决了标签不完整问题，显著提升了EEG情绪识别的性能。

Abstract: EEG based multi-dimension emotion recognition has attracted substantial
research interest in human computer interfaces. However, the high
dimensionality of EEG features, coupled with limited sample sizes, frequently
leads to classifier overfitting and high computational complexity. Feature
selection constitutes a critical strategy for mitigating these challenges. Most
existing EEG feature selection methods assume complete multi-dimensional
emotion labels. In practice, open acquisition environment, and the inherent
subjectivity of emotion perception often result in incomplete label data, which
can compromise model generalization. Additionally, existing feature selection
methods for handling incomplete multi-dimensional labels primarily focus on
correlations among various dimensions during label recovery, neglecting the
correlation between samples in the label space and their interaction with
various dimensions. To address these issues, we propose a novel incomplete
multi-dimensional feature selection algorithm for EEG-based emotion
recognition. The proposed method integrates an adaptive dual self-expression
learning (ADSEL) with least squares regression. ADSEL establishes a
bidirectional pathway between sample-level and dimension-level self-expression
learning processes within the label space. It could facilitate the
cross-sharing of learned information between these processes, enabling the
simultaneous exploitation of effective information across both samples and
dimensions for label reconstruction. Consequently, ADSEL could enhances label
recovery accuracy and effectively identifies the optimal EEG feature subset for
multi-dimensional emotion recognition.

</details>


### [37] [FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing](https://arxiv.org/abs/2508.05231)
*Wenjia Dong,Xueyuan Xu,Tianze Yu,Junming Zhang,Li Zhuo*

Main category: cs.HC

TL;DR: 提出了一种名为FDC-Net的新型框架，通过双向梯度传播和门控注意力机制，深度耦合去噪和情感识别任务，实现了端到端的噪声鲁棒情感识别。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中将去噪和情感识别任务分开处理导致的误差累积和未能利用任务间协同效应的问题，并提高EEG信号在噪声干扰下的情感识别精度。

Method: 采用双向梯度传播和联合优化策略，结合门控注意力机制和频率自适应Transformer，动态协同去噪和情感识别任务。

Result: 在DEAP和DREAMER数据集上，去噪任务的相关系数分别达到96.30%和90.31%，情感识别任务的准确率分别为82.3±7.1%和88.1±0.8%。

Conclusion: FDC-Net通过动态协作机制显著提升了噪声鲁棒性，为EEG情感识别提供了更有效的解决方案。

Abstract: Electroencephalogram (EEG)-based emotion recognition holds significant value
in affective computing and brain-computer interfaces. However, in practical
applications, EEG recordings are susceptible to the effects of various
physiological artifacts. Current approaches typically treat denoising and
emotion recognition as independent tasks using cascaded architectures, which
not only leads to error accumulation, but also fails to exploit potential
synergies between these tasks. Moreover, conventional EEG-based emotion
recognition models often rely on the idealized assumption of "perfectly
denoised data", lacking a systematic design for noise robustness. To address
these challenges, a novel framework that deeply couples denoising and emotion
recognition tasks is proposed for end-to-end noise-robust emotion recognition,
termed as Feedback-Driven Collaborative Network for Denoising-Classification
Nexus (FDC-Net). Our primary innovation lies in establishing a dynamic
collaborative mechanism between artifact removal and emotion recognition
through: (1) bidirectional gradient propagation with joint optimization
strategies; (2) a gated attention mechanism integrated with frequency-adaptive
Transformer using learnable band-position encoding. Two most popular EEG-based
emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels
were employed to compare the artifact removal and emotion recognition
performance between ASLSL and nine state-of-the-art methods. In terms of the
denoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of
96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the
emotion recognition task under physiological artifact interference, FDC-Net
achieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on
DREAMER.

</details>


### [38] [Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models](https://arxiv.org/abs/2508.05238)
*Wei Xiang,Muchen Li,Jie Yan,Manling Zheng,Hanfei Zhu,Mengyun Jiang,Lingyun Sun*

Main category: cs.HC

TL;DR: 论文探讨了如何在Level 3自动驾驶系统中通过大型语言模型（LLM）提供人性化建议，以帮助驾驶员保持对路况的关注，减少认知负担。


<details>
  <summary>Details</summary>
Motivation: Level 3自动驾驶系统中，驾驶员可能因执行其他任务而降低风险感知，而在紧急情况下需要快速反应时，认知负担较重。

Method: 使用LLM生成“人性化”的劝导建议，通过视觉和听觉途径引导驾驶员行为，触发机制基于路况。

Result: 实验表明，该工具能有效维持驾驶员注意力、减少认知负担，并协调次要任务与接管行为。

Conclusion: 这项研究展示了LLM在多任务自动驾驶中支持驾驶员的潜力。

Abstract: Level 3 automated driving systems allows drivers to engage in secondary tasks
while diminishing their perception of risk. In the event of an emergency
necessitating driver intervention, the system will alert the driver with a
limited window for reaction and imposing a substantial cognitive burden. To
address this challenge, this study employs a Large Language Model (LLM) to
assist drivers in maintaining an appropriate attention on road conditions
through a "humanized" persuasive advice. Our tool leverages the road conditions
encountered by Level 3 systems as triggers, proactively steering driver
behavior via both visual and auditory routes. Empirical study indicates that
our tool is effective in sustaining driver attention with reduced cognitive
load and coordinating secondary tasks with takeover behavior. Our work provides
insights into the potential of using LLMs to support drivers during multi-task
automated driving.

</details>


### [39] [A Methodological Framework and Questionnaire for Investigating Perceived Algorithmic Fairness](https://arxiv.org/abs/2508.05281)
*Ahmed Abdal Shafi Rasel,Ahmed Mustafa Amlan,Tasmim Shajahan Mim,Tanvir Hasan*

Main category: cs.HC

TL;DR: 研究探讨了孟加拉国用户对算法决策公平性的感知，通过混合方法揭示了文化、社会及情境因素如何影响其对AI系统公平性、透明度和问责制的理解。


<details>
  <summary>Details</summary>
Motivation: 旨在理解非西方文化背景下用户对算法公平性的独特态度，为全球AI伦理对话提供新视角。

Method: 采用混合方法，结合定量调查与定性访谈数据，综合分析文化和社会因素对公平性感知的影响。

Result: 研究发现用户对人为监督、解释机制和可争议性有复杂态度，强调需考虑文化背景的算法设计原则。

Conclusion: 研究丰富了算法公平性的全球讨论，突出了非西方视角在构建公平可信AI系统中的重要性。

Abstract: This study explores perceptions of fairness in algorithmic decision-making
among users in Bangladesh through a comprehensive mixed-methods approach. By
integrating quantitative survey data with qualitative interview insights, we
examine how cultural, social, and contextual factors influence users'
understanding of fairness, transparency, and accountability in AI systems. Our
findings reveal nuanced attitudes toward human oversight, explanation
mechanisms, and contestability, highlighting the importance of culturally aware
design principles for equitable and trustworthy algorithmic systems. These
insights contribute to ongoing discussions on algorithmic fairness by
foregrounding perspectives from a non-Western context, thus broadening the
global dialogue on ethical AI deployment.

</details>


### [40] [Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs](https://arxiv.org/abs/2508.05325)
*Jonathan C. Roberts,Hanan Alnjar,Aron E. Owen,Panagiotis D. Ritsos*

Main category: cs.HC

TL;DR: 提出了一种名为CDS的结构化方法，通过批判性思维改进可视化设计，分为三个阶段，结合六个视角，并在教学中持续优化。


<details>
  <summary>Details</summary>
Motivation: 针对可视化设计中的改进难点，帮助设计师（尤其是初学者）通过批判性思维和启发式评价做出更明智的决策。

Method: CDS分为三阶段：1)捕捉设计核心；2)六视角启发式提问；3)综合洞察与反思。

Result: 在本科和研究生课程中应用CDS，并通过持续使用不断优化其内容与工具。

Conclusion: CDS为可视化设计提供实用框架，适合教育与实践中的批判性思维训练。

Abstract: We present the Critical Design Strategy (CDS) - a structured method designed
to facilitate the examination of visualisation designs through reflection and
critical thought. The CDS helps designers think critically and make informed
improvements using heuristic evaluation. When developing a visual tool or
pioneering a novel visualisation approach, identifying areas for enhancement
can be challenging. Critical thinking is particularly crucial for visualisation
designers and tool developers, especially those new to the field, such as
studying visualisation in higher education. The CDS consists of three stages
across six perspectives: Stage 1 captures the essence of the idea by assigning
an indicative title and selecting five adjectives (from twenty options) to form
initial impressions of the design. Stage 2 involves an in-depth critique using
30 heuristic questions spanning six key perspectives - user, environment,
interface, components, design, and visual marks. Stage 3 focuses on
synthesising insights, reflecting on design decisions, and determining the next
steps forward. We introduce the CDS and explore its use across three
visualisation modules in both undergraduate and postgraduate courses. Our
longstanding experience with the CDS has allowed us to refine and develop it
over time: from its initial creation through workshops in 2017/18 to
improvements in wording and the development of two applications by 2020,
followed by the expansion of support notes and refinement of heuristics through
2023; while using it in our teaching each year. This sustained use allows us to
reflect on its practical application and offer guidance on how others can
incorporate it into their own work.

</details>


### [41] [Implementation and Application of Multi-Format 3D Data Integration in a Cross-Device Commercial Metaverse Platform](https://arxiv.org/abs/2508.05332)
*Masanori Ibara,Yuichi Hiroi,Takushi Kamegai,Takefumi Hiraki*

Main category: cs.HC

TL;DR: 传统3D设计数据（如BIM和CAD）仅限于专家使用，限制了普通用户的参与。本文通过工业元宇宙平台Cluster的案例，展示了3D数据在多领域的应用，实现了协作决策支持。


<details>
  <summary>Details</summary>
Motivation: 打破专家垄断，让普通用户参与3D设计数据的决策过程。

Method: 分析工业与建筑领域的主要数据格式特点，提出元宇宙集成工作流，并通过多领域3D数据应用案例展示协作决策支持。

Result: 多设备访问与多用户同时参与能力在工业元宇宙中创造了民主环境，超越了传统专家依赖系统的限制。

Conclusion: 3D数据与元宇宙技术的结合为协作决策提供了实际可行的解决方案，具有广泛的应用潜力。

Abstract: Traditionally, specialized 3D design data, such as BIM and CAD, have been
accessible only to a select group of experts, creating significant barriers
that prevent general users from participating in decision-making processes.
This paper provides a systematic overview of practical insights for utilizing
3D data in industrial and architectural domains by presenting implementation
cases of the industrial metaverse on Cluster, a commercial cross-device
metaverse platform. This paper analyzes the characteristics and constraints of
major data formats in the industrial and architectural fields and organizes
integration workflows for the metaverse. Through application cases utilizing 3D
data across multiple domains, we present practical examples of collaborative
decision-making support enabled by the fusion of metaverse and digital twin
technologies. Specifically, we demonstrate that multi-device access and
simultaneous multi-user participation capabilities foster democratic
environments in the industrial metaverse, which are challenging to achieve with
conventional, expert-dependent systems.

</details>


### [42] [Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study](https://arxiv.org/abs/2508.05497)
*Federico Scarì,Olger Siebinga,Arkady Zgonnikov*

Main category: cs.HC

TL;DR: 提出一个结构化评估框架，用于评估自动驾驶车辆（AV）与人类驾驶车辆（HDV）的交互，涵盖四个关键领域，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注AV控制算法的技术性能，忽视了与人类驾驶员的交互体验，需填补这一空白。

Method: 提出一个包含交互效果、交互感知、交互努力和交互能力的四域框架，并在驾驶模拟器中应用案例研究。

Result: 框架揭示了AV与HDV交互中驾驶员体验的关键差异，强调需综合评估技术与人因表现。

Conclusion: 该框架为评估AV行为提供了系统方法，助力开发更人性化、安全的技术。

Abstract: As automated vehicles (AVs) increasingly integrate into mixed-traffic
environments, evaluating their interaction with human-driven vehicles (HDVs)
becomes critical. In most research focused on developing new AV control
algorithms (controllers), the performance of these algorithms is assessed
solely based on performance metrics such as collision avoidance or lane-keeping
efficiency, while largely overlooking the human-centred dimensions of
interaction with HDVs. This paper proposes a structured evaluation framework
that addresses this gap by incorporating metrics grounded in the human-robot
interaction literature. The framework spans four key domains: a) interaction
effect, b) interaction perception, c) interaction effort, and d) interaction
ability. These domains capture both the performance of the AV and its impact on
human drivers around it. To demonstrate the utility of the framework, we apply
it to a case study evaluating how a state-of-the-art AV controller interacts
with human drivers in a merging scenario in a driving simulator. Measuring
HDV-HDV interactions as a baseline, this study included one representative
metric per domain: a) perceived safety, b) subjective ratings, specifically how
participants perceived the other vehicle's driving behaviour (e.g.,
aggressiveness or predictability) , c) driver workload, and d) merging success.
The results showed that incorporating metrics covering all four domains in the
evaluation of AV controllers can illuminate critical differences in driver
experience when interacting with AVs. This highlights the need for a more
comprehensive evaluation approach. Our framework offers researchers,
developers, and policymakers a systematic method for assessing AV behaviour
beyond technical performance, fostering the development of AVs that are not
only functionally capable but also understandable, acceptable, and safe from a
human perspective.

</details>


### [43] [Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2508.05572)
*Yifan Wang,Hongfeng Ai,Ruiqi Li,Maowei Jiang,Ruiyuan Kang,Jiahua Dong,Cheng Jiang,Chenzhong Li*

Main category: cs.HC

TL;DR: 论文针对医学时间序列疾病诊断中的两大挑战——高标注成本与现有对比学习方法的局限性，提出了结合AE-GAN和LMCF框架的解决方案，显著提升了诊断效果。


<details>
  <summary>Details</summary>
Motivation: 解决医学数据标注成本高导致的过拟合问题，以及现有对比学习方法复杂且缺乏普适性的不足。

Method: 提出结合AE-GAN提取先验知识，并引入LMCF框架通过多视角对比学习自适应学习疾病特征。

Result: 在三个目标数据集上优于七种基线方法，显著提升了心肌梗死、阿尔茨海默症和帕金森病的诊断效果。

Conclusion: 所提出的方法在医学诊断中表现出色，具有重要应用价值。

Abstract: In medical time series disease diagnosis, two key challenges are identified.
First, the high annotation cost of medical data leads to overfitting in models
trained on label-limited, single-center datasets. To address this, we propose
incorporating external data from related tasks and leveraging AE-GAN to extract
prior knowledge, providing valuable references for downstream tasks. Second,
many existing studies employ contrastive learning to derive more generalized
medical sequence representations for diagnostic tasks, usually relying on
manually designed diverse positive and negative sample pairs. However, these
approaches are complex, lack generalizability, and fail to adaptively capture
disease-specific features across different conditions. To overcome this, we
introduce LMCF (Learnable Multi-views Contrastive Framework), a framework that
integrates a multi-head attention mechanism and adaptively learns
representations from different views through inter-view and intra-view
contrastive learning strategies. Additionally, the pre-trained AE-GAN is used
to reconstruct discrepancies in the target data as disease probabilities, which
are then integrated into the contrastive learning process. Experiments on three
target datasets demonstrate that our method consistently outperforms other
seven baselines, highlighting its significant impact on healthcare applications
such as the diagnosis of myocardial infarction, Alzheimer's disease, and
Parkinson's disease. We release the source code at xxxxx.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [44] [Voost: A Unified and Scalable Diffusion Transformer for Bidirectional Virtual Try-On and Try-Off](https://arxiv.org/abs/2508.04825)
*Seungyong Lee,Jeong-gi Kwak*

Main category: cs.GR

TL;DR: Voost是一个统一且可扩展的框架，通过联合学习虚拟试穿和试脱任务，使用单一扩散变换器提升服装与身体的关联推理能力，并引入两项推理技术以增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虚拟试穿中服装与身体的对应关系建模困难，特别是在姿态和外观变化的情况下，需要更高效且通用的解决方案。

Method: 提出Voost框架，联合学习虚拟试穿和试脱任务，利用单一扩散变换器，支持灵活的条件生成，并引入注意力温度缩放和自修正采样技术。

Result: Voost在试穿和试脱任务上均取得最先进结果，在对齐精度、视觉逼真度和泛化性上优于基线方法。

Conclusion: Voost通过联合学习和推理技术，显著提升了虚拟试穿和试脱任务的性能，展示了其高效性和通用性。

Abstract: Virtual try-on aims to synthesize a realistic image of a person wearing a
target garment, but accurately modeling garment-body correspondence remains a
persistent challenge, especially under pose and appearance variation. In this
paper, we propose Voost - a unified and scalable framework that jointly learns
virtual try-on and try-off with a single diffusion transformer. By modeling
both tasks jointly, Voost enables each garment-person pair to supervise both
directions and supports flexible conditioning over generation direction and
garment category, enhancing garment-body relational reasoning without
task-specific networks, auxiliary losses, or additional labels. In addition, we
introduce two inference-time techniques: attention temperature scaling for
robustness to resolution or mask variation, and self-corrective sampling that
leverages bidirectional consistency between tasks. Extensive experiments
demonstrate that Voost achieves state-of-the-art results on both try-on and
try-off benchmarks, consistently outperforming strong baselines in alignment
accuracy, visual fidelity, and generalization.

</details>


### [45] [Perceive-Sample-Compress: Towards Real-Time 3D Gaussian Splatting](https://arxiv.org/abs/2508.04965)
*Zijian Wang,Beizhen Zhao,Hao Wang*

Main category: cs.GR

TL;DR: 提出了一个感知-采样-压缩框架，用于优化3D高斯泼溅技术的场景管理和存储效率。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅技术在大规模场景管理和存储效率上表现不佳，尤其是在复杂环境或有限计算资源下。

Method: 通过场景感知补偿算法和金字塔采样表示优化高斯参数，提出通用高斯混合模型压缩算法以减少存储需求。

Result: 实验表明，该方法在保持实时渲染速度的同时，显著提升内存效率和视觉质量。

Conclusion: 该框架有效解决了3D高斯泼溅技术的存储和计算资源问题，同时保持了高质量的渲染效果。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable
capabilities in real-time and photorealistic novel view synthesis. However,
traditional 3DGS representations often struggle with large-scale scene
management and efficient storage, particularly when dealing with complex
environments or limited computational resources. To address these limitations,
we introduce a novel perceive-sample-compress framework for 3D Gaussian
Splatting. Specifically, we propose a scene perception compensation algorithm
that intelligently refines Gaussian parameters at each level. This algorithm
intelligently prioritizes visual importance for higher fidelity rendering in
critical areas, while optimizing resource usage and improving overall visible
quality. Furthermore, we propose a pyramid sampling representation to manage
Gaussian primitives across hierarchical levels. Finally, to facilitate
efficient storage of proposed hierarchical pyramid representations, we develop
a Generalized Gaussian Mixed model compression algorithm to achieve significant
compression ratios without sacrificing visual fidelity. The extensive
experiments demonstrate that our method significantly improves memory
efficiency and high visual quality while maintaining real-time rendering speed.

</details>


### [46] [Laplacian Analysis Meets Dynamics Modelling: Gaussian Splatting for 4D Reconstruction](https://arxiv.org/abs/2508.04966)
*Yifan Zhou,Beizhen Zhao,Pengcheng Wu,Hao Wang*

Main category: cs.GR

TL;DR: 论文提出了一种混合显隐式功能的新型动态3D高斯分布框架，解决了现有方法在动态场景建模中的光谱冲突问题，提升了重建精度。


<details>
  <summary>Details</summary>
Motivation: 动态3D高斯分布在建模动态场景时存在光谱冲突，导致运动细节丢失或特征碰撞，现有方法无法兼顾运动细节和变形一致性。

Method: 结合了Hash编码和拉普拉斯模块的光谱感知编码架构、增强的高斯动态属性以及基于KDTree的自适应高斯分裂策略。

Result: 实验表明，该方法在复杂动态场景重建中达到了最先进的性能，重建保真度更高。

Conclusion: 该框架成功解决了动态3D高斯分布的光谱冲突问题，为动态场景建模提供了有效解决方案。

Abstract: While 3D Gaussian Splatting (3DGS) excels in static scene modeling, its
extension to dynamic scenes introduces significant challenges. Existing dynamic
3DGS methods suffer from either over-smoothing due to low-rank decomposition or
feature collision from high-dimensional grid sampling. This is because of the
inherent spectral conflicts between preserving motion details and maintaining
deformation consistency at different frequency. To address these challenges, we
propose a novel dynamic 3DGS framework with hybrid explicit-implicit functions.
Our approach contains three key innovations: a spectral-aware Laplacian
encoding architecture which merges Hash encoding and Laplacian-based module for
flexible frequency motion control, an enhanced Gaussian dynamics attribute that
compensates for photometric distortions caused by geometric deformation, and an
adaptive Gaussian split strategy guided by KDTree-based primitive control to
efficiently query and optimize dynamic areas. Through extensive experiments,
our method demonstrates state-of-the-art performance in reconstructing complex
dynamic scenes, achieving better reconstruction fidelity.

</details>


### [47] [A Study of the Framework and Real-World Applications of Language Embedding for 3D Scene Understanding](https://arxiv.org/abs/2508.05064)
*Mahmoud Chick Zaouali,Todd Charter,Yehor Karpichev,Brandon Haworth,Homayoun Najjjaran*

Main category: cs.GR

TL;DR: 本文综述了语言引导与3D Gaussian Splatting技术的结合，探讨了其理论、集成策略及实际应用，并指出了当前的技术瓶颈与未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管Gaussian Splatting在3D场景表示中表现出色，但其与语言模型结合的研究尚缺乏系统性综述，本文旨在填补这一空白。

Method: 通过结构化综述，详细分析了语言模型与Gaussian Splatting集成的理论、策略及案例。

Result: 总结了当前技术的主要局限性，如计算瓶颈和语义标注数据不足，并提出了未来研究方向。

Conclusion: 语言引导的Gaussian Splatting在3D场景理解中潜力巨大，但需解决现有挑战以实现更广泛应用。

Abstract: Gaussian Splatting has rapidly emerged as a transformative technique for
real-time 3D scene representation, offering a highly efficient and expressive
alternative to Neural Radiance Fields (NeRF). Its ability to render complex
scenes with high fidelity has enabled progress across domains such as scene
reconstruction, robotics, and interactive content creation. More recently, the
integration of Large Language Models (LLMs) and language embeddings into
Gaussian Splatting pipelines has opened new possibilities for text-conditioned
generation, editing, and semantic scene understanding. Despite these advances,
a comprehensive overview of this emerging intersection has been lacking. This
survey presents a structured review of current research efforts that combine
language guidance with 3D Gaussian Splatting, detailing theoretical
foundations, integration strategies, and real-world use cases. We highlight key
limitations such as computational bottlenecks, generalizability, and the
scarcity of semantically annotated 3D Gaussian data and outline open challenges
and future directions for advancing language-guided 3D scene understanding
using Gaussian Splatting.

</details>


### [48] [RAP: Real-time Audio-driven Portrait Animation with Video Diffusion Transformer](https://arxiv.org/abs/2508.05115)
*Fangyu Du,Taiqing Li,Ziwei Zhang,Qian Qiao,Tan Yu,Dingcheng Zhen,Xu Jia,Yang Yang,Shunshun Yin,Siyuan Liu*

Main category: cs.GR

TL;DR: RAP提出了一种实时音频驱动肖像动画框架，通过混合注意力机制和静态-动态训练推断范式，解决压缩空间中的细节丢失问题，实现了高质量实时动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法因复杂度高难以实时部署，而压缩空间会导致细节丢失，影响音频-视觉同步。RAP旨在解决这一问题。

Method: 采用混合注意力机制实现精细音频控制，静态-动态训练推断范式避免显式运动监督。

Result: 实验表明RAP在实时约束下实现高质量动画，性能优于现有方法。

Conclusion: RAP框架成功平衡了实时性与视觉保真度，为音频驱动肖像动画提供有效解决方案。

Abstract: Audio-driven portrait animation aims to synthesize realistic and natural
talking head videos from an input audio signal and a single reference image.
While existing methods achieve high-quality results by leveraging
high-dimensional intermediate representations and explicitly modeling motion
dynamics, their computational complexity renders them unsuitable for real-time
deployment. Real-time inference imposes stringent latency and memory
constraints, often necessitating the use of highly compressed latent
representations. However, operating in such compact spaces hinders the
preservation of fine-grained spatiotemporal details, thereby complicating
audio-visual synchronization RAP (Real-time Audio-driven Portrait animation), a
unified framework for generating high-quality talking portraits under real-time
constraints. Specifically, RAP introduces a hybrid attention mechanism for
fine-grained audio control, and a static-dynamic training-inference paradigm
that avoids explicit motion supervision. Through these techniques, RAP achieves
precise audio-driven control, mitigates long-term temporal drift, and maintains
high visual fidelity. Extensive experiments demonstrate that RAP achieves
state-of-the-art performance while operating under real-time constraints.

</details>


### [49] [Refining Gaussian Splatting: A Volumetric Densification Approach](https://arxiv.org/abs/2508.05187)
*Mohamed Abdul Gafoor,Marius Preda,Titus Zaharia*

Main category: cs.GR

TL;DR: 提出了一种基于惯性体积的密度控制方法，改进了3D高斯泼溅中的点原始管理，提升了新视图合成的质量。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯泼溅(3DGS)中自适应密度控制(ADC)的不足，提升新视图合成的质量。

Method: 利用高斯函数的惯性体积指导密度控制，比较了传统SfM和深度图像匹配(DIM)的点云初始化方法。

Result: 在Mip-NeRF 360数据集上，新方法优于3DGS，重建质量更高。

Conclusion: 新方法通过改进密度控制和点云初始化，显著提升了3DGS的性能。

Abstract: Achieving high-quality novel view synthesis in 3D Gaussian Splatting (3DGS)
often depends on effective point primitive management. The underlying Adaptive
Density Control (ADC) process addresses this issue by automating densification
and pruning. Yet, the vanilla 3DGS densification strategy shows key
shortcomings. To address this issue, in this paper we introduce a novel density
control method, which exploits the volumes of inertia associated to each
Gaussian function to guide the refinement process. Furthermore, we study the
effect of both traditional Structure from Motion (SfM) and Deep Image Matching
(DIM) methods for point cloud initialization. Extensive experimental
evaluations on the Mip-NeRF 360 dataset demonstrate that our approach surpasses
3DGS in reconstruction quality, delivering encouraging performance across
diverse scenes.

</details>


### [50] [GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 2-Manifold Reeb Graphs](https://arxiv.org/abs/2508.05524)
*Sefat Rahman,Tushar M. Athawale,Paul Rosen*

Main category: cs.GR

TL;DR: 本文提出了一种名为GASP的新算法，用于生成满足边界约束、紧凑性和梯度对齐的Reeb图可视化，相比现有方法更具代表性。


<details>
  <summary>Details</summary>
Motivation: 现有Reeb图可视化算法忽略了边界约束、紧凑性和梯度对齐这三个关键属性，导致可视化结果无法忠实表达数据的拓扑结构。

Method: 提出GASP算法，专门考虑边界约束、紧凑性和梯度对齐，生成更准确的Reeb图可视化。

Result: 通过定性和定量评估，GASP算法优于广泛使用的几何重心算法（TTK实现）。

Conclusion: GASP算法能够更准确地表示数据的拓扑结构，为Reeb图可视化提供了一种改进方法。

Abstract: Reeb graphs are an important tool for abstracting and representing the
topological structure of a function defined on a manifold. We have identified
three properties for faithfully representing Reeb graphs in a visualization.
Namely, they should be constrained to the boundary, compact, and aligned with
the function gradient. Existing algorithms for drawing Reeb graphs are agnostic
to or violate these properties. In this paper, we introduce an algorithm to
generate Reeb graph visualizations, called \textit{GASP}, that is cognizant of
these properties, thereby producing visualizations that are more representative
of the underlying data. To demonstrate the improvements, the resulting Reeb
graphs are evaluated both qualitatively and quantitatively against the
geometric barycenter algorithm, using its implementation available in the
Topology ToolKit (TTK), a widely adopted tool for calculating and visualizing
Reeb graphs.

</details>


### [51] [Point cloud segmentation for 3D Clothed Human Layering](https://arxiv.org/abs/2508.05531)
*Davide Garavaso,Federico Masi,Pietro Musoni,Umberto Castellani*

Main category: cs.GR

TL;DR: 提出了一种新的3D点云分割范式——Clothed Human Layering，用于同时关联不同层次的服装和身体部分，并创建了合成数据集验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状分割方法主要用于场景理解，而在服装建模中需要解决多层重叠的分割问题。

Method: 提出了一种3D点云分割方法，支持点同时关联多个层次，并通过神经网络的粗/细粒度识别进行评估。

Result: 在合成和真实数据集上的实验证明了该方法在服装领域的有效性。

Conclusion: 新分割范式能有效处理服装和身体部分的重叠分割问题，为3D服装建模提供了新思路。

Abstract: 3D Cloth modeling and simulation is essential for avatars creation in several
fields, such as fashion, entertainment, and animation. Achieving high-quality
results is challenging due to the large variability of clothed body especially
in the generation of realistic wrinkles. 3D scan acquisitions provide more
accuracy in the representation of real-world objects but lack semantic
information that can be inferred with a reliable semantic reconstruction
pipeline. To this aim, shape segmentation plays a crucial role in identifying
the semantic shape parts. However, current 3D shape segmentation methods are
designed for scene understanding and interpretation and only few work is
devoted to modeling. In the context of clothed body modeling the segmentation
is a preliminary step for fully semantic shape parts reconstruction namely the
underlying body and the involved garments. These parts represent several layers
with strong overlap in contrast with standard segmentation methods that provide
disjoint sets. In this work we propose a new 3D point cloud segmentation
paradigm where each 3D point can be simultaneously associated to different
layers. In this fashion we can estimate the underlying body parts and the
unseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer
above. We name this segmentation paradigm clothed human layering. We create a
new synthetic dataset that simulates very realistic 3D scans with the ground
truth of the involved clothing layers. We propose and evaluate different neural
network settings to deal with 3D clothing layering. We considered both coarse
and fine grained per-layer garment identification. Our experiments demonstrates
the benefit in introducing proper strategies for the segmentation on the
garment domain on both the synthetic and real-world scan datasets.

</details>


### [52] [Physically Controllable Relighting of Photographs](https://arxiv.org/abs/2508.05626)
*Chris Careaga,Yağız Aksoy*

Main category: cs.GR

TL;DR: 提出了一种自监督的图像重光照方法，结合传统渲染的物理精度和神经渲染的逼真外观，实现对真实场景光照的完全控制。


<details>
  <summary>Details</summary>
Motivation: 将典型的3D图形工具中对灯光的显式物理控制引入真实场景的重光照中。

Method: 通过单目估计几何和固有组件推断彩色网格表示，结合路径追踪引擎和神经网络渲染器，实现自监督训练。

Result: 能够在真实场景中实现完全可控、物理准确的光照编辑。

Conclusion: 将传统渲染与神经网络渲染相结合，为真实场景的重光照提供了新的解决方案。

Abstract: We present a self-supervised approach to in-the-wild image relighting that
enables fully controllable, physically based illumination editing. We achieve
this by combining the physical accuracy of traditional rendering with the
photorealistic appearance made possible by neural rendering. Our pipeline works
by inferring a colored mesh representation of a given scene using monocular
estimates of geometry and intrinsic components. This representation allows
users to define their desired illumination configuration in 3D. The scene under
the new lighting can then be rendered using a path-tracing engine. We send this
approximate rendering of the scene through a feed-forward neural renderer to
predict the final photorealistic relighting result. We develop a differentiable
rendering process to reconstruct in-the-wild scene illumination, enabling
self-supervised training of our neural renderer on raw image collections. Our
method represents a significant step in bringing the explicit physical control
over lights available in typical 3D computer graphics tools, such as Blender,
to in-the-wild relighting.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [53] [Injection Locking and Coupling Dynamics in Superconducting Nanowire based Cryogenic Oscillators](https://arxiv.org/abs/2508.04878)
*Md Mazharul Islam,Md Shafayat Hossain,Kathleen E Hamilton,Ahmedullah Aziz*

Main category: cs.ET

TL;DR: 本文研究了低温超导纳米线振荡器的注入锁定和互耦合动态，揭示了频率同步和信号协调的关键机制，为低温计算架构提供了设计指导。


<details>
  <summary>Details</summary>
Motivation: 低温振荡器在超导电子学和量子计算中至关重要，但对其同步和耦合机制的研究仍有待深入。

Method: 通过数值模拟分析了注入锁定和互耦合动态，考察了关键设计参数（如并联电阻、纳米线电感和耦合强度）的影响。

Result: 研究发现锁频范围可由注入信号幅度调节，且耦合强度可精确控制相位差，为低温计算架构提供了新设计思路。

Conclusion: 这些发现为构建基于超导纳米线的振荡神经网络和同步低温逻辑块奠定了基础。

Abstract: Oscillators designed to function at cryogenic temperatures play a critical
role in superconducting electronics and quantum computing by providing stable,
low noise signals with minimal energy loss.Here we present a comprehensive
numerical study of injection locking and mutual coupling dynamics in
superconducting nanowire based cryogenic oscillators.Using the design space of
standalone ScNW based oscillator, we investigate two critical mechanisms that
govern frequency synchronization and signal coordination in cryogenic computing
architectures.First, an injection locking induced by an external AC signal with
a frequency near the oscillators natural frequency, and second, the mutual
coupling dynamics between two ScNW oscillators under varying coupling
strengths.We identify key design parameters such as shunt resistance, nanowire
inductance, and coupling strength that govern the locking range.Additionally,
we examine how the amplitude of the injected signal affects the amplitude of
the locked oscillation, offering valuable insights for power aware oscillator
synchronization.Furthermore, we analyze mutual synchronization between coupled
ScNW oscillators using capacitive and resistive coupling elements.Our results
reveal that the phase difference between oscillators can be precisely
controlled by tuning the coupling strength, enabling programmable phase encoded
information processing.These findings could enable building ScNW based
oscillatory neural networks, synchronized cryogenic logic blocks, and on chip
cryogenic resonator arrays.

</details>


### [54] [Wave Computing based on Dynamical Networks: Applications in Optimization Problems](https://arxiv.org/abs/2508.05014)
*Yunwen Liu,Jiang Xiao*

Main category: cs.ET

TL;DR: 提出了一种利用波传播的计算框架，通过节点和边的波操作能力在多个维度上实现并行计算，有效解决NP难问题。


<details>
  <summary>Details</summary>
Motivation: 探索一种能够在空间、时间和频率等多维度上实现并行计算的框架，以更高效地解决NP难问题。

Method: 开发了一个基于波传播的网络计算框架，节点和边具有波操作能力（如频率混合或时延），并通过SPICE模拟验证。

Result: 该框架在模拟中成功解决了多个NP难问题，包括数字划分问题、0/1背包问题和旅行商问题。

Conclusion: 该计算框架通过波传播的多维并行性，为NP难问题的解决提供了高效且硬件友好的新思路。

Abstract: We develop a computing framework that leverages wave propagation within an
interconnected network, where nodes and edges possess wave manipulation
capabilities, such as frequency mixing or time delay. This computing paradigm
can not only achieve intrinsic parallelism like existing works by the
exploration of an exponential number of possibilities simultaneously with very
small number of hardware units, but also extend this unique characteristic to a
multidimensional space including spatial, temporal and frequency domains,
making it particularly effective for addressing NP-hard problems. The proposed
architecture has been validated through SPICE simulations, demonstrating its
potential capability in solving several NP-hard problems, such as the Number
Partitioning Problem, the 0/1 Knapsack Problem, and the Traveling Salesman
Problem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [55] [OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks](https://arxiv.org/abs/2508.04833)
*Nicolas Nicolaou,Onyeka Obi,Aayush Rajasekaran,Alejandro Bergasov,Aleksandr Bezobchuk,Kishori M. Konwar,Michael Meier,Santiago Paiva,Har Preet Singh,Swarnabha Sinha*

Main category: cs.DC

TL;DR: 本文提出了一种名为OPTIMUMP2P的新流言算法，通过随机线性网络编码（RLNC）提升信息传播效率，并在模拟和现实环境中验证了其优于现有Gossipsub协议的性能。


<details>
  <summary>Details</summary>
Motivation: 现有流言算法（如floodsup和gossibsup）在去中心化系统中虽广泛使用，但在性能和信息传递可靠性方面仍有提升空间。尤其是在面对恶意行为干扰时，现有方法可能不够可靠。

Method: OPTIMUMP2P采用随机线性网络编码（RLNC）技术，加速P2P网络中的信息传播，同时确保即使在恶意数据篡改的情况下也能可靠传递。

Result: 实验结果表明，OPTIMUMP2P在区块传播时间上显著优于Gossipsub协议，性能提升显著。

Conclusion: OPTIMUMP2P通过引入RLNC技术，不仅提升了信息传播速度和可靠性，还展现了在区块链等去中心化系统中的实际应用潜力。

Abstract: Gossip algorithms are pivotal in the dissemination of information within
decentralized systems. Consequently, numerous gossip libraries have been
developed and widely utilized especially in blockchain protocols for the
propagation of blocks and transactions. A well-established library is libp2p,
which provides two gossip algorithms: floodsup and gossibsup. These algorithms
enable the delivery of published messages to a set of peers. In this work we
aim to enhance the performance and reliability of libp2p by introducing
OPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random
Linear Network Coding (RLNC) to expedite the dissemination of information in a
peer-to-peer (P2P) network while ensuring reliable delivery, even in the
presence of malicious actors capable of corrupting the transmitted data.
Preliminary research from the Ethereum Foundation has demonstrated the use of
RLNC in the significant improvement in the block propagation time [14]. Here we
present extensive evaluation results both in simulation and real-world
environments that demonstrate the performance gains of OPTIMUMP2P over the
Gossipsub protocol.

</details>


### [56] [Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model](https://arxiv.org/abs/2508.04870)
*Khaled Jawhar,Evangelos Kranakis*

Main category: cs.DC

TL;DR: 论文研究了两个具有不同通信能力的自主机器人如何通过线性搜索捕获移动目标，分析了在不同通信模型和运动模式下捕获时间的最优竞争比。


<details>
  <summary>Details</summary>
Motivation: 探讨了在非对称通信模型下，两个机器人如何高效协作捕获移动目标，以理解通信能力差异对搜索效率的影响。

Method: 设计了新的线性搜索算法，考虑了目标的起始距离、速度及运动方向（远离或接近原点），并分析了不同场景下的竞争比。

Result: 研究表明，非对称通信模型（Sender/Receiver）对捕获时间的竞争比有显著影响，算法在不同场景下表现优异。

Conclusion: 论文为理解非对称通信对搜索任务的影响提供了理论支持，提出的算法在实际应用中具有潜力。

Abstract: We consider linear search for capturing an oblivious moving target by two
autonomous robots with different communicating abilities. Both robots can
communicate Face-to-Face (F2F) when co-located but in addition one robot is a
Sender (can also send messages wirelessly) and the other also a Receiver (can
also receive messages wirelessly). This is known as Sender/Receiver (S/R, for
short) communication model. The robots can move with max speed $1$. The moving
target starts at distance $d$ from the origin and can move either with speed
$v<1$ away from the origin in the ``away'' model or with speed $v \geq 0$
toward the origin in the ``toward'' model. We assume that the direction of
motion of the target (i.e., whether it is the away or toward model) is known to
the robots in advance. To capture the target the two robots must be co-located
with it.
  We design new linear search algorithms and analyze the competitive ratio of
the time required to capture the target. The approach takes into account
various scenarios related to what the robots know about the search environment
(e.g., starting distance or speed of the mobile, away or toward model, or a
combination thereof). Our study contributes to understanding how asymmetric
communication affects the competitive ratio of linear search.

</details>


### [57] [Managing, Analyzing and Sharing Research Data with Gen3 Data Commons](https://arxiv.org/abs/2508.04944)
*Craig Barnes,Kyle Burton,Michael S. Fitzsimons,Hara Prasad Juvvala,Brienna Larrick,Christopher Meyer,Pauline Ribeyre,Ao Liu,Clint Malson,Noah Metoki-Shlubsky,Andrii Prokhorenkov,Jawad Qureshi,Radhika Reddy,L. Philip Schumm,Mingfei Shao,Trevar Simmons,Alexander VanTol,Peter Vassilatos,Aarti Venkat,Robert L. Grossman*

Main category: cs.DC

TL;DR: Gen3是一个开源数据平台，用于构建数据共享空间，支持管理、分析和共享研究数据。


<details>
  <summary>Details</summary>
Motivation: 为研究社区提供一个云数据平台，实现数据的FAIR（可查找、可访问、可互操作、可重用）管理。

Method: 通过定义数据模型自动生成数据门户和API，基于标准化软件服务实现平台互操作性。

Result: 已支持十多个数据共享空间，管理超过28PB的数据和6400万个FAIR数据对象。

Conclusion: Gen3通过标准化和自动化服务，有效促进了数据的共享与互操作。

Abstract: Gen3 is an open-source data platform for building data commons. A data
commons is a cloud-based data platform for managing, analyzing, and sharing
data with a research community. Gen3 has been used to build over a dozen data
commons that in aggregate contain over 28 PB of data and 64 million FAIR data
objects. To set up a Gen3 data commons, you first define a data model. Gen3
then autogenerates 1) a data portal for searching and exploring data in the
commons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs
for accessing the data programmatically. Gen3 is built over a small number of
standards-based software services, which are designed to support current and
future Gen3 components so that Gen3 can interoperate with other data platforms
and data ecosystems.

</details>


### [58] [Tesserae: Scalable Placement Policies for Deep Learning Workloads](https://arxiv.org/abs/2508.04953)
*Song Bian,Saurabh Agarwal,Md. Tareq Mahmood,Shivaram Venkataraman*

Main category: cs.DC

TL;DR: 本文提出了一种基于图匹配优化的深度学习集群调度器Tesserae，显著提高了资源利用率和任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 深度学习工作负载在数据中心占比高，提升资源利用率是调度器的关键目标。现有调度策略存在性能不佳或扩展性差的问题。

Method: 将放置约束转化为图匹配问题，设计新的放置策略以减少任务迁移开销并优化任务打包。

Result: Tesserae将平均作业完成时间（JCT）提升1.62倍，Makespan提升1.15倍。

Conclusion: 基于图匹配的策略可有效提升调度器性能，Tesserae展示了其可扩展性和高效性。

Abstract: Training deep learning (DL) models has become a dominant workload in
data-centers and improving resource utilization is a key goal of DL cluster
schedulers. In order to do this, schedulers typically incorporate placement
policies that govern where jobs are placed on the cluster. Existing placement
policies are either designed as ad-hoc heuristics or incorporated as
constraints within a complex optimization problem and thus either suffer from
suboptimal performance or poor scalability. Our key insight is that many
placement constraints can be formulated as graph matching problems and based on
that we design novel placement policies for minimizing job migration overheads
and job packing. We integrate these policies into Tesserae and describe how our
design leads to a scalable and effective GPU cluster scheduler. Our
experimental results show that Tesserae improves average JCT by up to 1.62x and
the Makespan by up to 1.15x compared with the existing schedulers.

</details>


### [59] [Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement](https://arxiv.org/abs/2508.05029)
*Felipe Aramburú,William Malpica,Kaouther Abrougui,Amin Aramoon,Romulo Auccapuclla,Claude Brisson,Matthijs Brobbel,Colby Farrell,Pradeep Garigipati,Joost Hoozemans,Supun Kamburugamuve,Akhil Nair,Alexander Ocsa,Johan Peltenburg,Rubén Quesada López,Deepak Sihag,Ahmet Uyar,Dhruv Vats,Michael Wendt,Jignesh M. Patel,Rodrigo Aramburú*

Main category: cs.DC

TL;DR: 论文提出了一种名为Theseus的分布式加速器原生查询引擎，旨在优化数据移动、内存利用和计算，性能优于Databricks Photon。


<details>
  <summary>Details</summary>
Motivation: 为了降低大规模数据在线分析处理的成本并提高吞吐量，系统可以使用加速器（如GPU），但数据移动等挑战亟待解决。

Method: Theseus采用异步控制机制与硬件资源紧密结合，优化网络通信、数据预加载、存储溢出和GPU计算任务，并引入固定大小的页面锁定主机内存分配机制。

Result: 在TPC-H基准测试中，Theseus在相同成本下性能优于Databricks Photon高达4倍，并可处理100TB规模的数据。

Conclusion: Theseus是一种适用于企业级的高性能分布式查询引擎，能够高效处理大规模数据分析任务。

Abstract: Online analytical processing of queries on datasets in the many-terabyte
range is only possible with costly distributed computing systems. To decrease
the cost and increase the throughput, systems can leverage accelerators such as
GPUs, which are now ubiquitous in the compute infrastructure. This introduces
many challenges, the majority of which are related to when, where, and how to
best move data around the system. We present Theseus -- a production-ready
enterprise-scale distributed accelerator-native query engine designed to
balance data movement, memory utilization, and computation in an
accelerator-based system context. Specialized asynchronous control mechanisms
are tightly coupled to the hardware resources for the purpose of network
communication, data pre-loading, data spilling across memories and storage, and
GPU compute tasks. The memory subsystem contains a mechanism for fixed-size
page-locked host memory allocations to increase throughput and reduce memory
fragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k
on cloud infrastructure, Theseus outperforms Databricks Photon by up to
$4\times$ at cost parity. Theseus is capable of processing all queries of the
TPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as
2 DGX A100 640GB nodes.

</details>


### [60] [Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations](https://arxiv.org/abs/2508.05020)
*Anjiang Wei,Hang Song,Mert Hidayetoglu,Elliott Slaughter,Sanjiva K. Lele,Alex Aiken*

Main category: cs.DC

TL;DR: 开发了一种基于AMR的高阶数值求解器，解决了动态数据结构、网格有效性执行等挑战，通过任务融合和GPU内核生成显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 高阶求解器对可压缩流计算至关重要，AMR能降低计算成本，但实现中存在挑战。

Method: 使用Regent语言开发AMR求解器，解决动态数据结构、任务融合和GPU内核生成问题。

Result: 任务融合实现18倍加速，GPU内核生成达到9.7倍加速，并通过模拟验证方法有效性。

Conclusion: 该方法显著提升了AMR求解器的性能，适用于复杂流问题。

Abstract: High-order solvers for compressible flows are vital in scientific
applications. Adaptive mesh refinement (AMR) is a key technique for reducing
computational cost by concentrating resolution in regions of interest. In this
work, we develop an AMR-based numerical solver using Regent, a high-level
programming language for the Legion programming model. We address several
challenges associated with implementing AMR in Regent. These include dynamic
data structures for patch refinement/coarsening, mesh validity enforcement, and
reducing task launch overhead via task fusion. Experimental results show that
task fusion achieves 18x speedup, while automated GPU kernel generation via
simple annotations yields 9.7x speedup for the targeted kernel. We demonstrate
our approach through simulations of two canonical compressible flow problems
governed by the Euler equations.

</details>


### [61] [Simulating LLM training workloads for heterogeneous compute and network infrastructure](https://arxiv.org/abs/2508.05370)
*Sumit Kumar,Arjun Temura,Naman Sharma,Ramanjeet Singh,Meet Dadhania,Praveen Tammana,Satananda Burla,Abed Mohammad Kamaluddin,Rinku Shah*

Main category: cs.DC

TL;DR: 论文提出了一种异构感知的分布式LLM模拟器，用于预测训练时间并支持设备组和并行映射的自定义配置，解决了GPU集群异构性问题。


<details>
  <summary>Details</summary>
Motivation: 分布式模型训练中GPU集群的异构性（如资源共享、设备代际差异）对模型优化和系统增强提出了挑战，现有模拟器假设基础设施同质，无法满足实际需求。

Method: 设计了异构感知的分布式LLM模拟器，包括非均匀工作负载分区等组件，支持设备组和并行映射的自定义配置。

Result: 初步仿真结果表明，异构性对模型计算和通信时间有显著影响。

Conclusion: 论文填补了现有模拟器与实际问题之间的鸿沟，为异构环境下的分布式训练提供了实用工具。

Abstract: The growing demand for large-scale GPU clusters in distributed model training
presents a significant barrier to innovation, particularly in model
optimization, performance tuning, and system-level enhancements. To address
this challenge, LLM training simulators are employed to estimate training time
and guide design decisions. However, the state-of-the-art LLM training
simulators assume homogeneous compute and network infrastructure. In practice,
device heterogeneity is inevitable due to resource sharing in cloud
environments, frequent shifts in device generations, and inherent intra-chip
interconnect heterogeneity. To address the gap between state-of-the-art and
practical requirements, we propose the design of a heterogeneity-aware
distributed LLM simulator capable of predicting training time while enabling
abstractions to specify custom configurations for device groups and
device-to-parallelism mapping. We present the design requirements and
challenges in building a heterogeneity-aware distributed ML training simulator,
and design components such as non-uniform workload partitioning. Our initial
simulation results demonstrate the impact of heterogeneity on the model
computation and communication time.

</details>


### [62] [Adaptive Parallel Downloader for Large Genomic Datasets](https://arxiv.org/abs/2508.05511)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: FastBioDL是一款自适应并发的生物数据集下载工具，通过实时优化并发流数量，显著提升下载速度。


<details>
  <summary>Details</summary>
Motivation: 现有下载工具因静态并发设置导致带宽利用低效，无法适应动态网络条件，下载速度慢。

Method: FastBioDL将下载过程建模为在线优化问题，利用效用函数和梯度下降动态调整并发流数量。

Result: 测试显示FastBioDL比现有工具快4倍，高速网络下快2.1倍。

Conclusion: FastBioDL通过智能优化HTTP/FTP下载，为大规模基因组数据获取提供高效解决方案。

Abstract: Modern next-generation sequencing (NGS) projects routinely generate terabytes
of data, which researchers commonly download from public repositories such as
SRA or ENA. Existing download tools often employ static concurrency settings,
leading to inefficient bandwidth utilization and prolonged download times due
to their inability to adapt to dynamic network conditions. We introduce
FastBioDL, a parallel file downloader designed for large biological datasets,
featuring an adaptive concurrency controller. FastBioDL frames the download
process as an online optimization problem, utilizing a utility function and
gradient descent to adjust the number of concurrent socket streams in real-time
dynamically. This approach maximizes download throughput while minimizing
resource overhead. Comprehensive evaluations on public genomic datasets
demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art
tools. Moreover, in high-speed network experiments, its adaptive design was up
to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP
or FTP downloads on the client side, FastBioDL provides a robust and efficient
solution for large-scale genomic data acquisition, democratizing
high-performance data retrieval for researchers without requiring specialized
commercial software or protocols.

</details>


### [63] [Modular Architecture for High-Performance and Low Overhead Data Transfers](https://arxiv.org/abs/2508.05546)
*Rasman Mubtasim Swargo,Engin Arslan,Md Arifuzzaman*

Main category: cs.DC

TL;DR: AutoMDT是一个基于深度强化学习的模块化数据传输架构，通过优化读写和网络操作的并发性显著提升传输效率。


<details>
  <summary>Details</summary>
Motivation: 传统文件传输工具由于固定配置或单一优化方法，常导致资源利用不足和不稳定，难以满足高性能应用的需求。

Method: 采用深度强化学习代理（PPO算法），结合轻量级网络系统模拟器进行离线训练，模块化设计分离I/O和网络任务。

Result: 实验显示，AutoMDT比现有技术快8倍收敛，传输完成时间减少68%。

Conclusion: AutoMDT通过模块化和强化学习，显著提升了大规模数据传输的效率与稳定性。

Abstract: High-performance applications necessitate rapid and dependable transfer of
massive datasets across geographically dispersed locations. Traditional file
transfer tools often suffer from resource underutilization and instability
because of fixed configurations or monolithic optimization methods. We propose
AutoMDT, a novel modular data transfer architecture that employs a deep
reinforcement learning based agent to simultaneously optimize concurrency
levels for read, network, and write operations. Our solution incorporates a
lightweight network-system simulator, enabling offline training of a Proximal
Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby
overcoming the impracticality of lengthy online training in production
networks. AutoMDT's modular design decouples I/O and network tasks, allowing
the agent to capture complex buffer dynamics precisely and to adapt quickly to
changing system and network conditions. Evaluations on production-grade
testbeds show that AutoMDT achieves up to 8x faster convergence and a 68%
reduction in transfer completion times compared with state-of-the-art
solutions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [64] [AgenticData: An Agentic Data Analytics System for Heterogeneous Data](https://arxiv.org/abs/2508.05002)
*Ji Sun,Guoliang Li,Peiyao Zhou,Yihui Ma,Jingzhe Xu,Yuan Li*

Main category: cs.DB

TL;DR: AgenticData是一个创新的数据分析系统，通过自然语言查询自主分析多领域数据，采用反馈驱动规划和多代理协作策略，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有非结构化数据分析系统依赖专家编写代码和管理复杂工作流的高成本和耗时问题。

Method: 1. 反馈驱动规划将自然语言查询转换为语义计划；2. 多代理协作（数据剖析代理、语义交叉验证代理和智能记忆代理）；3. 语义优化模型。

Result: 在三个基准测试中，AgenticData在简单和复杂任务上均表现出色，显著优于现有方法。

Conclusion: AgenticData通过自主分析和优化，有效降低了数据分析的门槛和成本。

Abstract: Existing unstructured data analytics systems rely on experts to write code
and manage complex analysis workflows, making them both expensive and
time-consuming. To address these challenges, we introduce AgenticData, an
innovative agentic data analytics system that allows users to simply pose
natural language (NL) questions while autonomously analyzing data sources
across multiple domains, including both unstructured and structured data.
First, AgenticData employs a feedback-driven planning technique that
automatically converts an NL query into a semantic plan composed of relational
and semantic operators. We propose a multi-agent collaboration strategy by
utilizing a data profiling agent for discovering relevant data, a semantic
cross-validation agent for iterative optimization based on feedback, and a
smart memory agent for maintaining short-term context and long-term knowledge.
Second, we propose a semantic optimization model to refine and execute semantic
plans effectively. Our system, AgenticData, has been tested using three
benchmarks. Experimental results showed that AgenticData achieved superior
accuracy on both easy and difficult tasks, significantly outperforming
state-of-the-art methods.

</details>


### [65] [Making Prompts First-Class Citizens for Adaptive LLM Pipelines](https://arxiv.org/abs/2508.05012)
*Ugur Cetintemel,Shu Chen,Alexander W. Lee,Deepti Raghavan*

Main category: cs.DB

TL;DR: SPEAR是一种语言和运行时系统，旨在解决现代LLM流程中提示管理的问题，通过结构化、自适应和运行时优化提示，提升效率和可控性。


<details>
  <summary>Details</summary>
Motivation: 现代LLM流程中提示（prompt）缺乏结构化和动态适应性，限制了复用、优化和运行时控制，SPEAR旨在填补这一空白。

Method: SPEAR提出了一种提示代数结构，支持动态提示调整和结构化提示管理，包括运行时提示优化、版本化视图和日志记录。

Result: 初步实验验证了SPEAR在不同提示优化模式下的效果，展示了其对静态提示和动态代理重试的改进。

Conclusion: SPEAR通过结构化提示管理和动态调整能力，为LLM流程提供了更高的可操作性和优化潜力。

Abstract: Modern LLM pipelines increasingly resemble data-centric systems: they
retrieve external context, compose intermediate outputs, validate results, and
adapt based on runtime feedback. Yet, the central element guiding this process
-- the prompt -- remains a brittle, opaque string, disconnected from the
surrounding dataflow. This disconnect limits reuse, optimization, and runtime
control.
  In this paper, we describe our vision and an initial design for SPEAR, a
language and runtime that fills this prompt management gap by making prompts
structured, adaptive, and first-class components of the execution model. SPEAR
enables (1) runtime prompt refinement -- modifying prompts dynamically in
response to execution-time signals such as confidence, latency, or missing
context; and (2) structured prompt management -- organizing prompt fragments
into versioned views with support for introspection and logging.
  SPEAR defines a prompt algebra that governs how prompts are constructed and
adapted within a pipeline. It supports multiple refinement modes (manual,
assisted, and automatic), giving developers a balance between control and
automation. By treating prompt logic as structured data, SPEAR enables
optimizations such as operator fusion, prefix caching, and view reuse.
Preliminary experiments quantify the behavior of different refinement modes
compared to static prompts and agentic retries, as well as the impact of
prompt-level optimizations such as operator fusion.

</details>


### [66] [Data-Aware Socratic Query Refinement in Database Systems](https://arxiv.org/abs/2508.05061)
*Ruiyuan Zhang,Chrysanthi Kosyfaki,Xiaofang Zhou*

Main category: cs.DB

TL;DR: DASG是一种通过对话优化数据库查询的框架，仅在预期收益超过交互成本时提问，以提高查询精度和效率。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言查询中的模糊性，通过主动对话优化查询，而非被动翻译用户请求。

Method: 结合语言模糊性、模式置信度和成本预测，选择最佳澄清问题。

Result: 在三个数据集上验证，DASG提高了查询精度且保持高效。

Conclusion: DASG建立了系统主动参与查询制定的协作分析范式。

Abstract: In this paper, we propose Data-Aware Socratic Guidance (DASG), a
dialogue-based query enhancement framework that embeds \linebreak interactive
clarification as a first-class operator within database systems to resolve
ambiguity in natural language queries. DASG treats dialogue as an optimization
decision, asking clarifying questions only when the expected execution cost
reduction exceeds the interaction overhead. The system quantifies ambiguity
through linguistic fuzziness, schema grounding confidence, and projected costs
across relational and vector backends. Our algorithm selects the optimal
clarifications by combining semantic relevance, catalog-based information gain,
and potential cost reduction. We evaluate our proposed framework on three
datasets. The results show that DASG demonstrates improved query precision
while maintaining efficiency, establishing a cooperative analytics paradigm
where systems actively participate in query formulation rather than passively
translating user requests.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [67] [Understanding and Mitigating Errors of LLM-Generated RTL Code](https://arxiv.org/abs/2508.05266)
*Jiazheng Zhang,Cheng Liu,Huawei Li*

Main category: cs.AR

TL;DR: 论文针对LLM在RTL代码生成中的错误进行了系统分析，提出针对性纠正技术，显著提升了生成准确率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成RTL代码时的高错误率问题，分析错误根源并提出改进方法。

Method: 构建领域知识库、使用检索增强生成技术，引入设计描述规则和检查机制，结合外部工具处理多模态输入，并采用迭代调试循环。

Result: 改进后的框架在VerilogEval基准测试中达到91.0%准确率，比基线方法提升32.7%。

Conclusion: 研究证实针对性纠错技术能有效提升LLM在RTL代码生成中的性能。

Abstract: Despite the promising potential of large language model (LLM) based
register-transfer-level (RTL) code generation, the overall success rate remains
unsatisfactory. Errors arise from various factors, with limited understanding
of specific failure causes hindering improvement. To address this, we conduct a
comprehensive error analysis and manual categorization. Our findings reveal
that most errors stem not from LLM reasoning limitations, but from insufficient
RTL programming knowledge, poor understanding of circuit concepts, ambiguous
design descriptions, or misinterpretation of complex multimodal inputs.
Leveraging in-context learning, we propose targeted error correction
techniques. Specifically, we construct a domain-specific knowledge base and
employ retrieval-augmented generation (RAG) to supply necessary RTL knowledge.
To mitigate ambiguity errors, we introduce design description rules and
implement a rule-checking mechanism. For multimodal misinterpretation, we
integrate external tools to convert inputs into LLM-compatible meta-formats.
For remaining errors, we adopt an iterative debugging loop (simulation-error
localization-correction). Integrating these techniques into an LLM-based
framework significantly improves performance. We incorporate these error
correction techniques into a foundational LLM-based RTL code generation
framework, resulting in significantly improved performance. Experimental
results show that our enhanced framework achieves 91.0\% accuracy on the
VerilogEval benchmark, surpassing the baseline code generation approach by
32.7\%, demonstrating the effectiveness of our methods.

</details>


### [68] [relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication](https://arxiv.org/abs/2508.05354)
*Michael Rogenmoser,Angelo Garofalo,Luca Benini*

Main category: cs.AR

TL;DR: 本文提出了一种名为relOBI的扩展Open Bus Interface（OBI）方案，结合三重模块冗余（TMR）和纠错码（ECC）保护，显著提高了SoC互连在辐射环境中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在辐射环境中，SoC互连的软错误可能导致整个系统功能故障，因此需要设计一种高可靠性的解决方案。

Method: 提出relOBI，结合TMR和ECC保护，设计并测试了一种完全可靠的交叉开关。

Result: 相比参考设计，注入故障的脆弱性从34.85%降低到0%，但面积增加了2.6倍，时序影响增加了1.4倍。

Conclusion: relOBI在可靠性方面表现优异，且面积开销比文献中报告的细粒度TMR方案低1.8倍。

Abstract: On-chip communication is a critical element of modern systems-on-chip (SoCs),
allowing processor cores to interact with memory and peripherals. Interconnects
require special care in radiation-heavy environments, as any soft error within
the SoC interconnect is likely to cause a functional failure of the whole SoC.
This work proposes relOBI, an extension to Open Bus Interface (OBI) combining
triple modular redundancy (TMR) for critical handshake signals with error
correction codes (ECC) protection on other signals for complete reliability.
Implementing and testing a fully reliable crossbar shows improved reliability
to injected faults from a vulnerability of 34.85 % to 0 % compared to a
reference design, with an area increase of 2.6x and 1.4x timing impact. The
area overhead is 1.8x lower than that reported in the literature for
fine-grained triplication and voting.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [69] [Salt-Rock Creep Deformation Forecasting Using Deep Neural Networks and Analytical Models for Subsurface Energy Storage Applications](https://arxiv.org/abs/2508.05248)
*Pradeep Kumar Shukla,Tanujit Chakraborty,Mustafa Sari,Joel Sarout,Partha Pratim Mandal*

Main category: physics.geo-ph

TL;DR: 本研究通过时间序列预测方法分析了盐岩在不同围压条件下的时间依赖性变形趋势（蠕变），并比较了多种深度神经网络模型的性能。


<details>
  <summary>Details</summary>
Motivation: 盐岩的蠕变变形评估对核废料、氢能或放射性材料的地下存储设施设计与操作至关重要。

Method: 利用多阶段三轴蠕变数据，通过季节性趋势分解、格兰杰因果检验等初步分析，并采用多种深度神经网络模型（如N-BEATS、TCN）进行预测。

Result: N-BEATS和TCN模型在不同应力水平下表现最佳，比传统分析模型准确度提高了15-20%。

Conclusion: 深度神经网络模型能有效捕捉复杂的时间依赖关系，适用于盐岩蠕变预测。

Abstract: This study provides an in-depth analysis of time series forecasting methods
to predict the time-dependent deformation trend (also known as creep) of salt
rock under varying confining pressure conditions. Creep deformation assessment
is essential for designing and operating underground storage facilities for
nuclear waste, hydrogen energy, or radioactive materials. Salt rocks, known for
their mechanical properties like low porosity, low permeability, high
ductility, and exceptional creep and self-healing capacities, were examined
using multi-stage triaxial (MSTL) creep data. After resampling, axial strain
datasets were recorded at 5--10 second intervals under confining pressure
levels ranging from 5 to 35 MPa over 5.8--21 days. Initial analyses, including
Seasonal-Trend Decomposition (STL) and Granger causality tests, revealed
minimal seasonality and causality between axial strain and temperature data.
Further statistical tests, such as the Augmented Dickey-Fuller (ADF) test,
confirmed the stationarity of the data with p-values less than 0.05, and
wavelet coherence plot (WCP) analysis indicated repeating trends. A suite of
deep neural network (DNN) models (Neural Basis Expansion Analysis for Time
Series (N-BEATS), Temporal Convolutional Networks (TCN), Recurrent Neural
Networks (RNN), and Transformers (TF)) was utilized and compared against
statistical baseline models. Predictive performance was evaluated using Root
Mean Square Error (RMSE), Mean Absolute Error (MAE), Mean Absolute Percentage
Error (MAPE), and Symmetric Mean Absolute Percentage Error (SMAPE). Results
demonstrated that N-BEATS and TCN models outperformed others across various
stress levels, respectively. DNN models, particularly N-BEATS and TCN, showed a
15--20\% improvement in accuracy over traditional analytical models,
effectively capturing complex temporal dependencies and patterns.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [70] [Everything You Need to Know About CS Education: Open Results from a Survey of More Than 18,000 Participants](https://arxiv.org/abs/2508.05286)
*Katsiaryna Dzialets,Aleksandra Makeeva,Ilya Vlasov,Anna Potriasaeva,Aleksei Rostovskii,Yaroslav Golubev,Anastasiia Birillo*

Main category: cs.CY

TL;DR: 该论文通过一项覆盖18,032名学习者的全球调查，更新了对计算机科学教育领域的全面研究，提出了新趋势和方法的数据集。


<details>
  <summary>Details</summary>
Motivation: 由于新趋势（如AI）、新学习形式（如IDE内学习）和学习者多样性的增加，需要对计算机科学教育进行更新的综合性研究。

Method: 对来自173个国家的18,032名学习者进行大规模调查，涵盖正规教育、学习形式、AI使用、挑战和动机等多个主题。

Result: 提供了开放数据集，并展示了三个研究方向：学习中的挑战、新兴学习形式和IDE内学习的见解。

Conclusion: 该数据集旨在支持进一步研究，推动计算机教育的进步。

Abstract: Computer science education is a dynamic field with many aspects that
influence the learner's path. While these aspects are usually studied in depth
separately, it is also important to carry out broader large-scale studies that
touch on many topics, because they allow us to put different results into each
other's perspective. Past large-scale surveys have provided valuable insights,
however, the emergence of new trends (e.g., AI), new learning formats (e.g.,
in-IDE learning), and the increasing learner diversity highlight the need for
an updated comprehensive study. To address this, we conducted a survey with
18,032 learners from 173 countries, ensuring diverse representation and
exploring a wide range of topics - formal education, learning formats, AI
usage, challenges, motivation, and more. This paper introduces the results of
this survey as an open dataset, describes our methodology and the survey
questions, and highlights, as a motivating example, three possible research
directions within this data: challenges in learning, emerging formats, and
insights into the in-IDE format. The dataset aims to support further research
and foster advancements in computer education.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [71] [Probabilistic Alternating Simulations for Policy Synthesis in Uncertain Stochastic Dynamical Systems](https://arxiv.org/abs/2508.05062)
*Thom Badings,Alessandro Abate*

Main category: eess.SY

TL;DR: 该论文提出了一种扩展的概率模拟关系，用于处理具有随机和不确定扰动的系统，适用于政策合成，并在实验中验证了其在4D状态Dubins车辆中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的概率模拟关系无法充分处理同时具有随机和非确定性扰动的系统动力学问题，需要一种更通用的关系来支持验证和策略合成。

Method: 通过扩展概率模拟关系，结合交替模拟的概念，提出了一种新的关系，允许在随机不确定性下进行概率性推理，在非确定性扰动下进行鲁棒性推理。

Result: 实验表明，这种新关系在4D状态Dubins车辆的政策合成中具有实用性。

Conclusion: 这项研究为处理混合随机和非确定性扰动的系统提供了一种有效的理论工具，扩展了政策合成的适用范围。

Abstract: A classical approach to formal policy synthesis in stochastic dynamical
systems is to construct a finite-state abstraction, often represented as a
Markov decision process (MDP). The correctness of these approaches hinges on a
behavioural relation between the dynamical system and its abstraction, such as
a probabilistic simulation relation. However, probabilistic simulation
relations do not suffice when the system dynamics are, next to being
stochastic, also subject to nondeterministic (i.e., set-valued) disturbances.
In this work, we extend probabilistic simulation relations to systems with both
stochastic and nondeterministic disturbances. Our relation, which is inspired
by a notion of alternating simulation, generalises existing relations used for
verification and policy synthesis used in several works. Intuitively, our
relation allows reasoning probabilistically over stochastic uncertainty, while
reasoning robustly (i.e., adversarially) over nondeterministic disturbances. We
experimentally demonstrate the applicability of our relations for policy
synthesis in a 4D-state Dubins vehicle.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [72] [Minimal Model Reasoning in Description Logics: Don't Try This at Home!](https://arxiv.org/abs/2508.05350)
*Federica Di Stefano,Quentin Manière,Magdalena Ortiz,Mantas Šimkus*

Main category: cs.AI

TL;DR: 该论文研究了描述逻辑（DLs）中的纯极小模型推理问题，发现即使在简单的DLs（如$⁠mathcal{EL}$）中，概念可满足性问题也是不可判定的。通过施加非循环条件，可以恢复可判定性，但复杂度较高。


<details>
  <summary>Details</summary>
Motivation: 研究纯极小模型在描述逻辑中的推理问题，填补了该领域长期未深入探索的空白，揭示了其复杂性和不可判定性。

Method: 通过分析纯极小模型在流行描述逻辑中的特性，研究其概念可满足性问题，并探索通过施加非循环条件来恢复可判定性的方法。

Result: 发现$⁠mathcal{EL}$中的概念可满足性在纯极小模型下是不可判定的；通过非循环条件，可判定性得以恢复，但复杂度较高（低于双指数时间）。

Conclusion: 纯极小模型在描述逻辑中的推理问题表现出极高的复杂性，即使简单逻辑也无法避免不可判定性。未来研究需进一步探索其限制条件和应用潜力。

Abstract: Reasoning with minimal models has always been at the core of many knowledge
representation techniques, but we still have only a limited understanding of
this problem in Description Logics (DLs). Minimization of some selected
predicates, letting the remaining predicates vary or be fixed, as proposed in
circumscription, has been explored and exhibits high complexity. The case of
`pure' minimal models, where the extension of all predicates must be minimal,
has remained largely uncharted. We address this problem in popular DLs and
obtain surprisingly negative results: concept satisfiability in minimal models
is undecidable already for $\mathcal{EL}$. This undecidability also extends to
a very restricted fragment of tuple-generating dependencies. To regain
decidability, we impose acyclicity conditions on the TBox that bring the
worst-case complexity below double exponential time and allow us to establish a
connection with the recently studied pointwise circumscription; we also derive
results in data complexity. We conclude with a brief excursion to the DL-Lite
family, where a positive result was known for DL-Lite$_{\text{core}}$, but our
investigation establishes ExpSpace-hardness already for its extension
DL-Lite$_{\text{horn}}$.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [73] [Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation](https://arxiv.org/abs/2508.05535)
*Albert Yu,Chengshu Li,Luca Macesanu,Arnav Balaji,Ruchira Ray,Raymond Mooney,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: MICoBot是一个用于人机协作的系统，通过多层次决策优化任务分配，显著提升任务成功率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 为了解决人机协作中人类伙伴行为的多样性和动态变化问题，需要一个灵活的沟通和决策系统。

Method: MICoBot采用混合主动对话范式，通过元规划器、任务规划器和动作执行器三层次决策，优化任务分配。

Result: 实验表明，MICoBot在任务成功率和用户体验上显著优于纯LLM基线和其他任务分配模型。

Conclusion: MICoBot通过多层次的灵活决策，能够有效适应多样化的人类伙伴，提升协作效果。

Abstract: Effective robotic systems for long-horizon human-robot collaboration must
adapt to a wide range of human partners, whose physical behavior, willingness
to assist, and understanding of the robot's capabilities may change over time.
This demands a tightly coupled communication loop that grants both agents the
flexibility to propose, accept, or decline requests as they coordinate toward
completing the task effectively. We apply a Mixed-Initiative dialog paradigm to
Collaborative human-roBot teaming and propose MICoBot, a system that handles
the common scenario where both agents, using natural language, take initiative
in formulating, accepting, or rejecting proposals on who can best complete
different steps of a task. To handle diverse, task-directed dialog, and find
successful collaborative strategies that minimize human effort, MICoBot makes
decisions at three levels: (1) a meta-planner considers human dialog to
formulate and code a high-level collaboration strategy, (2) a planner optimally
allocates the remaining steps to either agent based on the robot's capabilities
(measured by a simulation-pretrained affordance model) and the human's
estimated availability to help, and (3) an action executor decides the
low-level actions to perform or words to say to the human. Our extensive
evaluations in simulation and real-world -- on a physical robot with 18 unique
human participants over 27 hours -- demonstrate the ability of our method to
effectively collaborate with diverse human users, yielding significantly
improved task success and user experience than a pure LLM baseline and other
agent allocation models. See additional videos and materials at
https://robin-lab.cs.utexas.edu/MicoBot/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [74] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)
*Fenya Wasserroth,Eleftherios Avramidis,Vera Czehmann,Tanja Kojic,Fabrizio Nunnari,Sebastian Möller*

Main category: cs.CL

TL;DR: 研究探讨了在手语虚拟角色中增加调节功能对用户体验和可理解性的影响，结果显示尽管用户偏好可调节设置，但未显著提升体验或理解。


<details>
  <summary>Details</summary>
Motivation: 探索可调节手语虚拟角色在Microsoft Hololens 2设备上对用户体验和可理解性的实际效果。

Method: 通过专家用户交互分析，比较可调节与不可调节虚拟角色的表现。

Result: 可调节设置未显著改善用户体验或理解性，但情感质量评分较高。

Conclusion: 个性化不足，需优先提升默认可理解性和交互设计，改进口型和表情动画。

Abstract: This paper presents an investigation into the impact of adding adjustment
features to an existing sign language (SL) avatar on a Microsoft Hololens 2
device. Through a detailed analysis of interactions of expert German Sign
Language (DGS) users with both adjustable and non-adjustable avatars in a
specific use case, this study identifies the key factors influencing the
comprehensibility, the user experience (UX), and the acceptability of such a
system. Despite user preference for adjustable settings, no significant
improvements in UX or comprehensibility were observed, which remained at low
levels, amid missing SL elements (mouthings and facial expressions) and
implementation issues (indistinct hand shapes, lack of feedback and menu
positioning). Hedonic quality was rated higher than pragmatic quality,
indicating that users found the system more emotionally or aesthetically
pleasing than functionally useful. Stress levels were higher for the adjustable
avatar, reflecting lower performance, greater effort and more frustration.
Additionally, concerns were raised about whether the Hololens adjustment
gestures are intuitive and easy to familiarise oneself with. While
acceptability of the concept of adjustability was generally positive, it was
strongly dependent on usability and animation quality. This study highlights
that personalisation alone is insufficient, and that SL avatars must be
comprehensible by default. Key recommendations include enhancing mouthing and
facial animation, improving interaction interfaces, and applying participatory
design.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [75] [Does Multimodality Improve Recommender Systems as Expected? A Critical Analysis and Future Directions](https://arxiv.org/abs/2508.05377)
*Hongyu Zhou,Yinan Zhang,Aixin Sun,Zhiqi Shen*

Main category: cs.IR

TL;DR: 本文提出了一个评估框架，系统分析了多模态推荐系统的效果，发现多模态数据在稀疏交互和召回阶段效果显著，且不同模态在不同场景下重要性各异。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态推荐系统逐渐流行，但其实际效果尚不明确，缺乏系统评估。

Method: 提出了一个结构化评估框架，从四个维度分析多模态推荐，并通过基准测试比较多模态模型与传统模型的效果。

Result: 多模态数据在稀疏交互和召回阶段表现更好；文本模态在电商中更有效，视觉模态在短视频推荐中更优；基于集成的学习方法优于基于融合的方法；模型大小与效果无关。

Conclusion: 本文为构建高效多模态推荐系统提供了实践指导，强调了模态选择、集成策略和模型设计的重要性。

Abstract: Multimodal recommendation systems are increasingly popular for their
potential to improve performance by integrating diverse data types. However,
the actual benefits of this integration remain unclear, raising questions about
when and how it truly enhances recommendations. In this paper, we propose a
structured evaluation framework to systematically assess multimodal
recommendations across four dimensions: Comparative Efficiency, Recommendation
Tasks, Recommendation Stages, and Multimodal Data Integration. We benchmark a
set of reproducible multimodal models against strong traditional baselines and
evaluate their performance on different platforms. Our findings show that
multimodal data is particularly beneficial in sparse interaction scenarios and
during the recall stage of recommendation pipelines. We also observe that the
importance of each modality is task-specific, where text features are more
useful in e-commerce and visual features are more effective in short-video
recommendations. Additionally, we explore different integration strategies and
model sizes, finding that Ensemble-Based Learning outperforms Fusion-Based
Learning, and that larger models do not necessarily deliver better results. To
deepen our understanding, we include case studies and review findings from
other recommendation domains. Our work provides practical insights for building
efficient and effective multimodal recommendation systems, emphasizing the need
for thoughtful modality selection, integration strategies, and model design.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [76] [A Design for an Early Quantum Network](https://arxiv.org/abs/2508.04967)
*Yuan Li,Chen Zhang,Hao Zhang,Tao Huang,Yunjie Liu*

Main category: quant-ph

TL;DR: 提出一种适用于早期量子网络的设计，兼容现有量子中继技术，旨在最大化满足量子应用的多样化需求，并通过模拟评估其可行性和性能影响。


<details>
  <summary>Details</summary>
Motivation: 随着量子信息技术的快速发展，量子网络需满足高保真度和快速完成请求等严格要求，但目前资源有限且性能不理想。

Method: 设计兼容三种现有量子中继技术的网络架构，明确标识符和量子请求实现流程，并通过离散事件模拟评估设计，分析噪声和不完美参数对性能的影响。

Result: 分析了保真度和请求完成时间受参数影响的情况，并探讨了中央控制器在路径选择外的其他决策（如截止时间和资源分配）的作用。

Conclusion: 该设计为早期量子网络提供了可行的解决方案，能够适应多样化应用需求，同时为控制器优化提供了新方向。

Abstract: With the rapid advancement of quantum information technology, quantum
networks have become essential for supporting diverse applications, which often
have stringent demands for key metrics such as fidelity and request completion
time. In this work, we propose a design for early-stage quantum networks that
is compatible with the three existing quantum repeater technologies. The design
aims to maximize the ability of the network to accommodate the diverse needs of
quantum applications, even under conditions of limited quantum resources and
suboptimal network performance. We have also described the required identifiers
in the quantum network and the specific process for implementing quantum
requests. To assess the feasibility of our design, we conduct simulations based
on discrete-event modeling of quantum networks. The simulations consider
various types of noise and imperfect parameters that might exist in early-stage
networks. We analyze the impact of these parameters on the fidelity of the
generated entangled states and the request completion time. Furthermore, we
investigated additional decisions that the central controller can make beyond
path selection, such as the choice of cutoff time and the allocation of network
resources to requests.

</details>


### [77] [QFOR: A Fidelity-aware Orchestrator for Quantum Computing Environments using Deep Reinforcement Learning](https://arxiv.org/abs/2508.04974)
*Hoa T. Nguyen,Muhammad Usman,Rajkumar Buyya*

Main category: quant-ph

TL;DR: 提出了一种基于深度强化学习的量子任务调度框架QFOR，在异构量子云环境中优化任务分配与调度，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 量子云计算中资源异构性和噪声导致任务调度优化困难，现有启发式方法无法动态适应或平衡执行保真度与时间。

Method: 将量子任务调度建模为马尔可夫决策过程，使用近端策略优化算法学习自适应调度策略，并基于IBM量子处理器校准数据进行噪声感知性能估计。

Result: QFOR在相对保真度性能上比启发式基线提升29.5-84%，同时保持可比的量子执行时间，实现高效资源利用。

Conclusion: QFO R框架能动态适应操作优先级，显著提升量子任务调度性能，为量子计算资源的高效利用提供了可行方案。

Abstract: Quantum cloud computing enables remote access to quantum processors, yet the
heterogeneity and noise of available quantum hardware create significant
challenges for efficient resource orchestration. These issues complicate the
optimization of quantum task allocation and scheduling, as existing heuristic
methods fall short in adapting to dynamic conditions or effectively balancing
execution fidelity and time. Here, we propose QFOR, a Quantum Fidelity-aware
Orchestration of tasks across heterogeneous quantum nodes in cloud-based
environments using Deep Reinforcement learning. We model the quantum task
orchestration as a Markov Decision Process and employ the Proximal Policy
Optimization algorithm to learn adaptive scheduling policies, using IBM quantum
processor calibration data for noise-aware performance estimation. Our
configurable framework balances overall quantum task execution fidelity and
time, enabling adaptation to different operational priorities. Extensive
evaluation demonstrates that QFOR is adaptive and achieves significant
performance with 29.5-84% improvements in relative fidelity performance over
heuristic baselines. Furthermore, it maintains comparable quantum execution
times, contributing to cost-efficient use of quantum computation resources.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [Agnostics: Learning to Code in Any Programming Language via Reinforcement with a Universal Learning Environment](https://arxiv.org/abs/2508.04865)
*Aleksander Boruch-Gruszecki,Yangtian Zi,Zixuan Wu,Tejas Oberoi,Carolyn Jane Anderson,Joydeep Biswas,Arjun Guha*

Main category: cs.LG

TL;DR: 论文介绍了Agnostics，一种语言无关的后训练流程，通过外部行为验证代码，避免了针对每种语言的额外工程工作，适用于低资源编程语言。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在低资源编程语言上表现不佳的问题，避免为每种语言单独构建数据集和RL基础设施。

Method: 使用LLM将现有单元测试数据集转换为I/O格式，提供简短的配置说明如何编译和运行目标语言，并在稳健的代码执行环境中应用基于可验证奖励的强化学习（RLVR）。

Result: 在五种低资源语言（Lua、Julia、R、OCaml和Fortran）上，Agnostics显著提升了模型性能，甚至超越了一些更大规模的模型，并在16B参数以下的模型中创造了新的state-of-the-art结果。

Conclusion: Agnostics提供了一种高效且通用的方法，无需为每种语言单独开发，简化了在任意编程语言中的后训练过程，并且相关资源将开源以促进进一步研究。

Abstract: Large language models (LLMs) already excel at writing code in high-resource
languages such as Python and JavaScript, yet stumble on low-resource languages
that remain essential to science and engineering. Besides the obvious shortage
of pre-training data, post-training itself is a bottleneck: every new language
seems to require new datasets, test harnesses, and reinforcement-learning (RL)
infrastructure.
  We introduce Agnostics, a language-agnostic post-training pipeline that
eliminates this per-language engineering. The key idea is to judge code solely
by its externally observable behavior, so a single verifier can test solutions
written in any language. Concretely, we (i) use an LLM to rewrite existing
unit-test datasets into an I/O format, (ii) supply a short configuration that
tells the verifier how to compile and run a target language, and (iii) apply
reinforcement learning with verifiable rewards (RLVR) in a robust code
execution environment.
  Applied to five low-resource languages--Lua, Julia, R, OCaml, and
Fortran--Agnostics (1) improves Qwen-3 4B to performance that rivals other
16B-70B open-weight models; (2) scales cleanly to larger and diverse model
families (Qwen-3 8B, DeepSeek Coder 6.7B Instruct, Phi 4 Mini); and (3) for
${\le} 16$B parameter models, sets new state-of-the-art pass@1 results on
MultiPL-E and a new multi-language version LiveCodeBench that we introduce.
  We will release the language-agnostic training datasets (Ag-MBPP-X,
Ag-Codeforces-X, Ag-LiveCodeBench-X), training code, and ready-to-use
configurations, making RL post-training in any programming language as simple
as editing a short YAML file.

</details>


### [79] [LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation](https://arxiv.org/abs/2508.04732)
*Xiaoqi Dong,Xiangyu Zhou,Nicholas Evans,Yujia Lin*

Main category: cs.LG

TL;DR: LumiGen 是一个基于 LVLM 增强的迭代框架，通过智能提示解析与视觉反馈机制，提升 T2I 模型在复杂指令和细粒度控制方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有 T2I 模型在复杂指令处理、细粒度内容控制和语义一致性上存在不足，需要更高效的解决方案。

Method: LumiGen 包含智能提示解析增强模块（IPPA）和迭代视觉反馈优化模块（IVFR），通过 LVLM 驱动的闭环反馈机制优化图像生成。

Result: 在 LongBench-T2I Benchmark 上，LumiGen 取得了 3.08 的平均分，显著优于现有基线，尤其在文本渲染和姿势表达方面表现突出。

Conclusion: LumiGen 通过 LVLM 的集成，增强了 T2I 模型的可控性和生成质量，为复杂任务提供了有效解决方案。

Abstract: Text-to-Image (T2I) generation has made significant advancements with
diffusion models, yet challenges persist in handling complex instructions,
ensuring fine-grained content control, and maintaining deep semantic
consistency. Existing T2I models often struggle with tasks like accurate text
rendering, precise pose generation, or intricate compositional coherence.
Concurrently, Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in cross-modal understanding and instruction following. We propose
LumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I
model performance, particularly in areas requiring fine-grained control,
through a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an
Intelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt
enhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which
acts as a "visual critic" to iteratively correct and optimize generated images.
Evaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a
superior average score of 3.08, outperforming state-of-the-art baselines.
Notably, our framework demonstrates significant improvements in critical
dimensions such as text rendering and pose expression, validating the
effectiveness of LVLM integration for more controllable and higher-quality
image generation.

</details>


### [80] [HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation](https://arxiv.org/abs/2508.05135)
*Thinh Nguyen,Trung Phan,Binh T. Nguyen,Khoa D Doan,Kok-Seng Wong*

Main category: cs.LG

TL;DR: 论文提出了一种名为HFedATM的分层联邦域泛化方法，通过滤波器对齐和正则化聚合解决HFL中的域偏移问题，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的分层架构虽然解决了扩展性问题，但仍面临域偏移的挑战，影响了模型在新域上的表现。本文旨在探索如何在这一架构中有效应对域偏移。

Method: 提出HFedATM方法，首先通过滤波器最优运输对齐对不同基站模型进行卷积滤波器对齐，再使用收缩感知的正则化均值聚合方法合并模型。

Result: 实验表明，HFedATM在多数据集上显著优于现有联邦域泛化基线，同时保持了计算和通信效率。理论分析显示其泛化误差更小，收敛更快。

Conclusion: HFedATM为解决分层联邦学习中的域偏移问题提供了有效解决方案，兼具性能和效率优势。

Abstract: Federated Learning (FL) is a decentralized approach where multiple clients
collaboratively train a shared global model without sharing their raw data.
Despite its effectiveness, conventional FL faces scalability challenges due to
excessive computational and communication demands placed on a single central
server as the number of participating devices grows. Hierarchical Federated
Learning (HFL) addresses these issues by distributing model aggregation tasks
across intermediate nodes (stations), thereby enhancing system scalability and
robustness against single points of failure. However, HFL still suffers from a
critical yet often overlooked limitation: domain shift, where data
distributions vary significantly across different clients and stations,
reducing model performance on unseen target domains. While Federated Domain
Generalization (FedDG) methods have emerged to improve robustness to domain
shifts, their integration into HFL frameworks remains largely unexplored. In
this paper, we formally introduce Hierarchical Federated Domain Generalization
(HFedDG), a novel scenario designed to investigate domain shift within
hierarchical architectures. Specifically, we propose HFedATM, a hierarchical
aggregation method that first aligns the convolutional filters of models from
different stations through Filter-wise Optimal Transport Alignment and
subsequently merges aligned models using a Shrinkage-aware Regularized Mean
Aggregation. Our extensive experimental evaluations demonstrate that HFedATM
significantly boosts the performance of existing FedDG baselines across
multiple datasets and maintains computational and communication efficiency.
Moreover, theoretical analyses indicate that HFedATM achieves tighter
generalization error bounds compared to standard hierarchical averaging,
resulting in faster convergence and stable training behavior.

</details>


### [81] [X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment](https://arxiv.org/abs/2508.05568)
*Qinghua Yao,Xiangrui Xu,Zhize Li*

Main category: cs.LG

TL;DR: X-VFL是一种新的垂直联邦学习框架，解决了数据样本不对齐和本地独立推理的问题，通过XCom和DS-Align模块实现缺失特征的补全和决策子空间对齐，实验证明其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统垂直联邦学习（VFL）需要数据样本完美对齐且不支持本地独立推理，限制了其应用。X-VFL旨在解决这些问题，提升灵活性。

Method: X-VFL引入了两个模块：XCom用于补全缺失特征，DS-Align在决策子空间对齐本地和全局特征。支持SGD和PAGE等训练算法。

Result: 实验表明，X-VFL在CIFAR-10和MIMIC-III数据集上分别提升了15%和43%的准确率。

Conclusion: X-VFL通过创新的模块设计，显著提升了VFL在非对齐数据和本地推理场景下的性能，具有实际应用价值。

Abstract: Vertical Federated Learning (VFL) enables collaborative learning by
integrating disjoint feature subsets from multiple clients/parties. However,
VFL typically faces two key challenges: i) the requirement for perfectly
aligned data samples across all clients (missing features are not allowed); ii)
the requirement for joint collaborative inference/prediction involving all
clients (it does not support locally independent inference on a single client).
To address these challenges, we propose X-VFL, a new VFL framework designed to
deal with the non-aligned data samples with (partially) missing features and to
support locally independent inference of new data samples for each client. In
particular, we design two novel modules in X-VFL: Cross Completion (XCom) and
Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing
features for non-aligned data samples by leveraging information from other
clients. DS-Align aligns local features with completed and global features
across all clients within the decision subspace, thus enabling locally
independent inference at each client. Moreover, we provide convergence theorems
for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$
convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type
algorithms, where $T$ denotes the number of training update steps. Extensive
experiments on real-world datasets demonstrate that X-VFL significantly
outperforms existing methods, e.g., achieving a 15% improvement in accuracy on
the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III
dataset. These results validate the practical effectiveness and superiority of
X-VFL, particularly in scenarios involving partially missing features and
locally independent inference.

</details>


### [82] [Will You Be Aware? Eye Tracking-Based Modeling of Situational Awareness in Augmented Reality](https://arxiv.org/abs/2508.05025)
*Zhehan Qu,Tianyi Hu,Christian Fronk,Maria Gorlatova*

Main category: cs.LG

TL;DR: AR系统在增强任务性能的同时可能引发认知隧道效应，降低情境意识。该研究通过AR引导的心肺复苏实验评估情境意识，提出一种基于眼动的图神经网络模型预测情境意识。


<details>
  <summary>Details</summary>
Motivation: 研究AR系统在安全关键场景中可能引发的认知隧道效应及其对情境意识的影响，特别是在心肺复苏等需要同时关注多项任务的场景。

Method: 开发了一款AR应用，提供实时心肺复苏反馈，并通过用户实验模拟意外事件，收集眼动数据以分析情境意识。提出了FixGraphPool模型，利用眼动数据的时空信息预测情境意识。

Result: 研究发现，高情境意识与更大的眼跳幅度和速度相关，减少对虚拟内容的注视。FixGraphPool模型在预测情境意识上表现优异（准确率83.0%）。

Conclusion: 眼动数据可用于建模AR中的情境意识，为设计兼顾安全性和情境意识的AR系统提供了新思路。

Abstract: Augmented Reality (AR) systems, while enhancing task performance through
real-time guidance, pose risks of inducing cognitive tunneling-a hyperfocus on
virtual content that compromises situational awareness (SA) in safety-critical
scenarios. This paper investigates SA in AR-guided cardiopulmonary
resuscitation (CPR), where responders must balance effective compressions with
vigilance to unpredictable hazards (e.g., patient vomiting). We developed an AR
app on a Magic Leap 2 that overlays real-time CPR feedback (compression depth
and rate) and conducted a user study with simulated unexpected incidents (e.g.,
bleeding) to evaluate SA, in which SA metrics were collected via observation
and questionnaires administered during freeze-probe events. Eye tracking
analysis revealed that higher SA levels were associated with greater saccadic
amplitude and velocity, and with reduced proportion and frequency of fixations
on virtual content. To predict SA, we propose FixGraphPool, a graph neural
network that structures gaze events (fixations, saccades) into spatiotemporal
graphs, effectively capturing dynamic attentional patterns. Our model achieved
83.0% accuracy (F1=81.0%), outperforming feature-based machine learning and
state-of-the-art time-series models by leveraging domain knowledge and
spatial-temporal information encoded in ET data. These findings demonstrate the
potential of eye tracking for SA modeling in AR and highlight its utility in
designing AR systems that ensure user safety and situational awareness.

</details>


### [83] [ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning](https://arxiv.org/abs/2508.05310)
*Jelle Luijkx,Zlatan Ajanović,Laura Ferranti,Jens Kober*

Main category: cs.LG

TL;DR: ASkDAgger框架通过利用新手计划的反馈信息，优化查询频率与失败率之间的平衡，减少人工教学成本，并提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 减少交互式模仿学习中的人力教学成本，利用新手计划中的信息（如能力和不确定性）提高学习效率。

Method: 提出ASkDAgger框架，包含S-Aware Gating、Foresight Interactive Experience Replay和Prioritized Interactive Experience Replay三个组件，优化查询策略和示范数据利用。

Result: 在模拟和实际环境的语言条件操作任务中验证了ASkDAgger的有效性，降低了示范标注需求并提升了适应性。

Conclusion: ASkDAgger通过利用新手计划的信息，显著降低了人工干预需求并提升了学习效率和适应性。

Abstract: Human teaching effort is a significant bottleneck for the broader
applicability of interactive imitation learning. To reduce the number of
required queries, existing methods employ active learning to query the human
teacher only in uncertain, risky, or novel situations. However, during these
queries, the novice's planned actions are not utilized despite containing
valuable information, such as the novice's capabilities, as well as
corresponding uncertainty levels. To this end, we allow the novice to say: "I
plan to do this, but I am uncertain." We introduce the Active Skill-level Data
Aggregation (ASkDAgger) framework, which leverages teacher feedback on the
novice plan in three key ways: (1) S-Aware Gating (SAG): Adjusts the gating
threshold to track sensitivity, specificity, or a minimum success rate; (2)
Foresight Interactive Experience Replay (FIER), which recasts valid and
relabeled novice action plans into demonstrations; and (3) Prioritized
Interactive Experience Replay (PIER), which prioritizes replay based on
uncertainty, novice success, and demonstration age. Together, these components
balance query frequency with failure incidence, reduce the number of required
demonstration annotations, improve generalization, and speed up adaptation to
changing domains. We validate the effectiveness of ASkDAgger through
language-conditioned manipulation tasks in both simulation and real-world
environments. Code, data, and videos are available at
https://askdagger.github.io.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [84] [Graffiti: Enabling an Ecosystem of Personalized and Interoperable Social Applications](https://arxiv.org/abs/2508.04889)
*Theia Henderson,David R. Karger,David D. Clark*

Main category: cs.SI

TL;DR: Graffiti是一种系统，用于轻松构建个性化的社交应用，并实现应用间的互操作性，同时避免内容和上下文的混乱。


<details>
  <summary>Details</summary>
Motivation: 解决传统社交应用设计僵化、难以个性化且互操作性差的问题。

Method: 通过“完全具体化”和“通道”概念，设计了一个最小客户端API，并基于此开发了多种社交应用。

Result: 展示了Graffiti支持Twitter、Messenger和Wikipedia等应用的互操作性，并能避免上下文崩溃。

Conclusion: Graffiti为构建多样化且互操作的社交应用提供了一种灵活且可行的解决方案。

Abstract: Most social applications, from Twitter to Wikipedia, have rigid
one-size-fits-all designs, but building new social applications is both
technically challenging and results in applications that are siloed away from
existing communities. We present Graffiti, a system that can be used to build a
wide variety of personalized social applications with relative ease that also
interoperate with each other. People can freely move between a plurality of
designs -- each with its own aesthetic, feature set, and moderation -- all
without losing their friends or data.
  Our concept of total reification makes it possible for seemingly
contradictory designs, including conflicting moderation rules, to interoperate.
Conversely, our concept of channels prevents interoperation from occurring by
accident, avoiding context collapse.
  Graffiti applications interact through a minimal client-side API, which we
show admits at least two decentralized implementations. Above the API, we built
a Vue.js plugin, which we use to develop applications similar to Twitter,
Messenger, and Wikipedia using only client-side code. Our case studies explore
how these and other novel applications interoperate, as well as the broader
ecosystem that Graffiti enables.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [85] [Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework](https://arxiv.org/abs/2508.04962)
*Peng Zhang,Songru Yang,Jinsheng Sun,Weiqing Li,Zhiyong Su*

Main category: cs.CV

TL;DR: HOW-Seg是一种基于人工反馈的开放世界点云语义分割框架，通过稀疏标注和层级原型消歧机制，实现了高质量分割。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖资源密集型增量学习或密集标注支持数据的问题。

Method: 构建查询数据上的类别原型，利用稀疏人工标注指导，结合层级原型消歧和密集条件随机场优化。

Result: 在稀疏标注下表现优异，匹配或超越现有方法；使用先进骨干网络和密集标注时性能显著提升。

Conclusion: HOW-Seg在开放世界点云分割中高效且实用，为未来工作提供新方向。

Abstract: Open-world point cloud semantic segmentation (OW-Seg) aims to predict point
labels of both base and novel classes in real-world scenarios. However,
existing methods rely on resource-intensive offline incremental learning or
densely annotated support data, limiting their practicality. To address these
limitations, we propose HOW-Seg, the first human-in-the-loop framework for
OW-Seg. Specifically, we construct class prototypes, the fundamental
segmentation units, directly on the query data, avoiding the prototype bias
caused by intra-class distribution shifts between the support and query data.
By leveraging sparse human annotations as guidance, HOW-Seg enables
prototype-based segmentation for both base and novel classes. Considering the
lack of granularity of initial prototypes, we introduce a hierarchical
prototype disambiguation mechanism to refine ambiguous prototypes, which
correspond to annotations of different classes. To further enrich contextual
awareness, we employ a dense conditional random field (CRF) upon the refined
prototypes to optimize their label assignments. Through iterative human
feedback, HOW-Seg dynamically improves its predictions, achieving high-quality
segmentation for both base and novel classes. Experiments demonstrate that with
sparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches or
surpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg)
method under the 5-shot setting. When using advanced backbones (e.g.,
Stratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene),
HOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2,
significantly outperforming alternatives.

</details>
