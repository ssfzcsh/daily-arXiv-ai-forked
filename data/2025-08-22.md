<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.HC](#cs.HC) [Total: 11]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 5]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On the need to perform comprehensive evaluations of automated program repair benchmarks: Sorald case study](https://arxiv.org/abs/2508.15135)
*Sumudu Liyanage,Sherlock A. Licorish,Markus Wagner,Stephen G. MacDonell*

Main category: cs.SE

TL;DR: 论文提出了一个全面的自动化程序修复（APR）工具评估框架，重点分析了Sorald工具的修复效果及其可能引入的新问题。


<details>
  <summary>Details</summary>
Motivation: 在LLM时代，APR工具被广泛用于提高代码质量，但现有研究仅关注其清除违规的能力，忽略了其潜在的副作用。

Method: 研究使用Sorald工具修复了来自Stack Overflow的2,393个Java代码片段中的3,529个SonarQube违规，并评估其修复效果。

Result: Sorald虽然修复了特定违规，但引入了2,120个新问题（32个bug、2,088个代码异味），功能正确性下降24%，且代码结构退化。

Conclusion: 研究强调需要全面的APR工具评估方法，以确保其安全有效使用。

Abstract: In supporting the development of high-quality software, especially necessary
in the era of LLMs, automated program repair (APR) tools aim to improve code
quality by automatically addressing violations detected by static analysis
profilers. Previous research tends to evaluate APR tools only for their ability
to clear violations, neglecting their potential introduction of new (sometimes
severe) violations, changes to code functionality and degrading of code
structure. There is thus a need for research to develop and assess
comprehensive evaluation frameworks for APR tools. This study addresses this
research gap, and evaluates Sorald (a state-of-the-art APR tool) as a proof of
concept. Sorald's effectiveness was evaluated in repairing 3,529 SonarQube
violations across 30 rules within 2,393 Java code snippets extracted from Stack
Overflow. Outcomes show that while Sorald fixes specific rule violations, it
introduced 2,120 new faults (32 bugs, 2088 code smells), reduced code
functional correctness--as evidenced by a 24% unit test failure rate--and
degraded code structure, demonstrating the utility of our framework. Findings
emphasize the need for evaluation methodologies that capture the full spectrum
of APR tool effects, including side effects, to ensure their safe and effective
adoption.

</details>


### [2] [Foundational Design Principles and Patterns for Building Robust and Adaptive GenAI-Native Systems](https://arxiv.org/abs/2508.15411)
*Frederik Vandeputte*

Main category: cs.SE

TL;DR: 论文提出将生成式AI与传统软件工程结合，构建可靠、自适应、高效的GenAI原生系统，并设计五大原则及架构模式。


<details>
  <summary>Details</summary>
Motivation: 生成式AI虽强大，但其不可预测性和低效性限制了可靠性系统的发展，需结合传统工程原则解决。

Method: 提出五大设计原则（可靠性、卓越性、可进化性、自依赖性、保障性）及架构模式（如GenAI原生单元）。

Result: 构建了GenAI原生软件栈框架，并从技术、用户、经济、法律多角度分析其影响。

Conclusion: 呼吁进一步验证与实践，推动GenAI原生系统的研究与实现。

Abstract: Generative AI (GenAI) has emerged as a transformative technology,
demonstrating remarkable capabilities across diverse application domains.
However, GenAI faces several major challenges in developing reliable and
efficient GenAI-empowered systems due to its unpredictability and inefficiency.
This paper advocates for a paradigm shift: future GenAI-native systems should
integrate GenAI's cognitive capabilities with traditional software engineering
principles to create robust, adaptive, and efficient systems.
  We introduce foundational GenAI-native design principles centered around five
key pillars -- reliability, excellence, evolvability, self-reliance, and
assurance -- and propose architectural patterns such as GenAI-native cells,
organic substrates, and programmable routers to guide the creation of resilient
and self-evolving systems. Additionally, we outline the key ingredients of a
GenAI-native software stack and discuss the impact of these systems from
technical, user adoption, economic, and legal perspectives, underscoring the
need for further validation and experimentation. Our work aims to inspire
future research and encourage relevant communities to implement and refine this
conceptual framework.

</details>


### [3] [An Empirical Study of Knowledge Distillation for Code Understanding Tasks](https://arxiv.org/abs/2508.15423)
*Ruiqi Wang,Zezhou Yang,Cuiyun Gao,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: 本文系统研究了知识蒸馏（KD）在代码理解任务中的有效性和应用，发现特征基KD方法表现最佳，学生模型参数仅为5%时可保留教师模型98%性能。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练语言模型（PLMs）在代码理解中表现出色，但其计算密集性和推理延迟限制了大规模应用部署。知识蒸馏作为一种模型压缩和加速技术，有望解决这些问题，但其在代码理解领域的潜力尚未充分探索。

Method: 研究采用两种KD方法（基于logit和特征），在八个学生模型和两个教师PLM上进行实验，覆盖三个下游任务。

Result: KD能显著提升不同规模学生模型的性能，特征基KD方法尤其突出；代码专用PLM作为教师模型效果更佳；学生模型与教师模型架构相似性并非性能决定因素。

Conclusion: KD在代码理解任务中具有显著潜力，特征基方法表现最佳，未来研究方向包括效率和行为的进一步优化。

Abstract: Pre-trained language models (PLMs) have emerged as powerful tools for code
understanding. However, deploying these PLMs in large-scale applications faces
practical challenges due to their computational intensity and inference
latency. Knowledge distillation (KD), a promising model compression and
acceleration technique, addresses these limitations by transferring knowledge
from large teacher models to compact student models, enabling efficient
inference while preserving most of the teacher models' capabilities. While this
technique has shown remarkable success in natural language processing and
computer vision domains, its potential for code understanding tasks remains
largely underexplored.
  In this paper, we systematically investigate the effectiveness and usage of
KD in code understanding tasks. Our study encompasses two popular types of KD
methods, i.e., logit-based and feature-based KD methods, experimenting across
eight student models and two teacher PLMs from different domains on three
downstream tasks. The experimental results indicate that KD consistently offers
notable performance boosts across student models with different sizes compared
with standard fine-tuning. Notably, code-specific PLM demonstrates better
effectiveness as the teacher model. Among all KD methods, the latest
feature-based KD methods exhibit superior performance, enabling student models
to retain up to 98% teacher performance with merely 5% parameters. Regarding
student architecture, our experiments reveal that similarity with teacher
architecture does not necessarily lead to better performance. We further
discuss the efficiency and behaviors in the KD process and inference, summarize
the implications of findings, and identify promising future directions.

</details>


### [4] [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://arxiv.org/abs/2508.15495)
*Dongjun Yu,Xiao Yan,Zhenrui Li,Jipeng Xiao,Haochuan He,Yongda Yu,Hao Zhang,Guoping Rong,Xiaobo Huang*

Main category: cs.SE

TL;DR: 本文提出了一种名为SynthCoder的模型，通过结合AST节点提取、BM25算法和调用图等技术，优化了代码填充任务的表现，并在多个基准测试中取得了领先成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的代码补全模型在优化过程中常出现性能波动，某些数据集或指标的提升以其他方面的下降为代价。本文旨在通过综合行业最佳实践，设计一个性能稳定的模型。

Method: 构建多样化的数据集，结合AST节点提取和启发式方法模拟开发者行为；利用BM25算法和调用图增强跨文件上下文信息；采用两阶段训练（课程学习和直接偏好优化）。

Result: SynthCoder在aiXcoder、ExecRepoBench等主流代码补全基准测试中表现优异，并有效缓解了模型重复现有代码的问题。

Conclusion: SynthCoder通过综合优化技术和精心设计的数据集，在代码补全任务中实现了性能的显著提升，同时减少了常见问题。

Abstract: Code completion is a prominent application of Large Language Models (LLMs) in
software engineering. Due to the near real-time response requirements of this
task, base models with small to medium-sized parameters are typically employed,
supplemented by various optimization and post-training techniques. However,
these optimization methods often have trade-offs, leading to a seesaw effect
where performance improvements on certain datasets or metrics are accompanied
by degradations on others -- sometimes even falling below the baseline model's
performance. This paper proposes SynthCoder, a model that integrates leading
industry practices to achieve state-of-the-art performance on the
Fill-in-the-Middle (FIM) code completion task. In specific, we first construct
a diverse dataset by combining Abstract Syntax Tree (AST) node extraction with
heuristics that simulate developer behavior. Then we enrich our training corpus
with cross-file contextual information using the BM25 algorithm and call
graphs, enhancing the model's ability to perform code completion in both
file-level and repository-level scenarios. As the last step, we employ a
two-stage training process using the Seed-Coder-8B-Base as the base model.
First, we fine-tune the model using Curriculum Learning technology. Following
this, we perform alignment using Direct Preference Optimization (DPO) with
preference pairs generated through Rejection Sampling. Experimental results
demonstrate that our final model excels on mainstream repository-level code
completion benchmarks, including aiXcoder, ExecRepoBench, CrossCodeEval, and
CoLT. Furthermore, our carefully curated training set effectively mitigates the
model's tendency to just repeat existing code, a common issue existing in
various code completion models.

</details>


### [5] [Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset](https://arxiv.org/abs/2508.15496)
*Elena Masserini,Diego Clerissi,Daniela Micucci,João R. Campos,Leonardo Mariani*

Main category: cs.SE

TL;DR: 该论文提出了两个数据集（TOFU-R和BRASATO）及相关工具，用于评估任务型聊天机器人的可靠性、安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有聊天机器人质量评估技术因缺乏大规模、高质量数据集而受限的问题。

Method: 收集并整理GitHub上的Rasa聊天机器人（TOFU-R），并从中筛选出对话复杂度、功能复杂度和实用性突出的机器人（BRASATO）。

Result: 提供了两个数据集及工具支持，助力聊天机器人可靠性研究。

Conclusion: TOFU-R和BRASATO数据集填补了研究空白，促进了任务型聊天机器人的评估与改进。

Abstract: Task-based chatbots are increasingly being used to deliver real services, yet
assessing their reliability, security, and robustness remains underexplored,
also due to the lack of large-scale, high-quality datasets. The emerging
automated quality assessment techniques targeting chatbots often rely on
limited pools of subjects, such as custom-made toy examples, or outdated, no
longer available, or scarcely popular agents, complicating the evaluation of
such techniques. In this paper, we present two datasets and the tool support
necessary to create and maintain these datasets. The first dataset is RASA
TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa
chatbots available on GitHub, representing the state of the practice in
open-source chatbot development with Rasa. The second dataset is BOT RASA
COLLECTION (BRASATO), a curated selection of the most relevant chatbots for
dialogue complexity, functional complexity, and utility, whose goal is to ease
reproducibility and facilitate research on chatbot reliability.

</details>


### [6] [Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs](https://arxiv.org/abs/2508.15503)
*Sebastian Baltes,Florian Angermeir,Chetan Arora,Marvin Muñoz Barón,Chunyang Chen,Lukas Böhme,Fabio Calefato,Neil Ernst,Davide Falessi,Brian Fitzgerald,Davide Fucci,Marcos Kalinowski,Stefano Lambiase,Daniel Russo,Mircea Lungu,Lutz Prechelt,Paul Ralph,Christoph Treude,Stefan Wagner*

Main category: cs.SE

TL;DR: 提出了一个分类法和八条指南，以提高涉及大语言模型（LLM）的实证研究的透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的非确定性、不透明的训练数据和不断演变的架构，需要解决其在软件工程研究中复现和复制的挑战。

Method: 引入LLM研究类型的分类法，并提出八条设计指南，涵盖透明度、模型配置、验证等方面。

Result: 制定了一套支持研究透明度和可复现性的具体指南，并在线提供作为持续更新的资源。

Conclusion: 通过明确指南和分类法，帮助社区克服LLM研究的复现障碍，推动开放科学的发展。

Abstract: Large language models (LLMs) are increasingly being integrated into software
engineering (SE) research and practice, yet their non-determinism, opaque
training data, and evolving architectures complicate the reproduction and
replication of empirical studies. We present a community effort to scope this
space, introducing a taxonomy of LLM-based study types together with eight
guidelines for designing and reporting empirical studies involving LLMs. The
guidelines present essential (must) criteria as well as desired (should)
criteria and target transparency throughout the research process. Our
recommendations, contextualized by our study types, are: (1) to declare LLM
usage and role; (2) to report model versions, configurations, and fine-tuning;
(3) to document tool architectures; (4) to disclose prompts and interaction
logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7)
to report suitable baselines, benchmarks, and metrics; and (8) to openly
articulate limitations and mitigations. Our goal is to enable reproducibility
and replicability despite LLM-specific barriers to open science. We maintain
the study types and guidelines online as a living resource for the community to
use and shape (llm-guidelines.org).

</details>


### [7] [QUPER-MAn: Benchmark-Guided Target Setting for Maintainability Requirements](https://arxiv.org/abs/2508.15512)
*Markus Borg,Martin Larsson,Philip Breid,Nadim Hagatulah*

Main category: cs.SE

TL;DR: 论文提出QUPER-MAn模型，将可维护性从被忽视的问题转变为主动管理的目标，通过需求工程解决可维护性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决软件开发中可维护性被忽视的问题，通过需求工程提升其重要性。

Method: 采用设计科学研究方法，提出了QUPER-MAn模型，整合可维护性基准并支持目标设定。

Result: 研究发现可维护性仍是次要关注点，工具使用也仅隐含涉及；QUPER-MAn有望改变这一现状。

Conclusion: QUPER-MAn模型能通过需求工程主动管理可维护性，推动其在开发中的重要性。

Abstract: Maintainable source code is essential for sustainable development in any
software organization. Unfortunately, many studies show that maintainability
often receives less attention than its importance warrants. We argue that
requirements engineering can address this gap the problem by fostering
discussions and setting appropriate targets in a responsible manner. In this
preliminary work, we conducted an exploratory study of industry practices
related to requirements engineering for maintainability. Our findings confirm
previous studies: maintainability remains a second-class quality concern.
Explicit requirements often make sweeping references to coding conventions.
Tools providing maintainability proxies are common but typically only used in
implicit requirements related to engineering practices. To address this, we
propose QUPER-MAn, a maintainability adaption of the QUPER model, which was
originally developed to help organizations set targets for performance
requirements. Developed using a design science approach, QUPER-MAn, integrates
maintainability benchmarks and supports target setting. We posit that it can
shift maintainability from an overlooked development consequence to an actively
managed goal driven by informed and responsible engineering decisions.

</details>


### [8] [A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool Bugs](https://arxiv.org/abs/2508.15536)
*Yi Zhang,He Jiang,Xiaochen Li,Shikai Guo,Peiyu Zou,Zun Wang*

Main category: cs.SE

TL;DR: VERMEI是一种新的FPGA逻辑综合工具测试方法，通过预处理、等价变异和错误识别三个模块，显著提升了测试效果，发现了多个新bug。


<details>
  <summary>Details</summary>
Motivation: FPGA逻辑综合工具中的缺陷可能导致意外行为和安全风险，现有测试方法在测试程序的语义和逻辑复杂性上不足。

Method: VERMEI包含预处理、等价变异和错误识别三个模块，通过模拟和覆盖分析识别僵尸逻辑，利用贝叶斯采样生成复杂变体，并通过差分测试识别错误。

Result: 在Yosys、Vivado和Quartus上的实验表明，VERMEI优于现有方法，五个月内报告了15个bug，其中9个为新bug。

Conclusion: VERMEI是一种有效的FPGA逻辑综合工具测试方法，能显著提升测试覆盖率和错误发现能力。

Abstract: FPGA (Field-Programmable Gate Array) logic synthesis tools are key components
in the EDA (Electronic Design Automation) toolchain. They convert hardware
designs written in description languages such as Verilog into gate-level
representations for FPGAs. However, defects in these tools may lead to
unexpected behaviors and pose security risks. Therefore, it is crucial to
harden these tools through testing. Although several methods have been proposed
to automatically test FPGA logic synthesis tools, the challenge remains of
insufficient semantic and logical complexity in test programs. In this paper,
we propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI
consists of three modules: preprocessing, equivalent mutation, and bug
identification. The preprocessing module identifies zombie logic (inactive code
with no impact on the circuit output) in seed programs through simulation and
coverage analysis. The equivalent mutation module generates equivalent variants
of seed programs by pruning or inserting logic fragments in zombie areas. It
uses Bayesian sampling to extract logic fragments from historical Verilog
designs, making the generated variants have complex control flows and
structures. The bug identification module, based on differential testing,
compares the synthesized outputs of seed and variant programs to identify bugs.
Experiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms
the state-of-the-art methods. Within five months, VERMEI reported 15 bugs to
vendors, 9 of which were confirmed as new.

</details>


### [9] [Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study](https://arxiv.org/abs/2508.15570)
*Marion Wiese,Kamila Serwa,Anastasia Besier,Ariane S. Marion-Jetten,Eva Bittner*

Main category: cs.SE

TL;DR: 该研究探讨了通过工作坊形式在IT公司中建立技术债务（TD）管理流程的可行性，并分析了其对TD意识的长期影响。


<details>
  <summary>Details</summary>
Motivation: 技术债务管理（TDM）在研究中常见但实际应用较少，研究旨在通过工作坊形式将TDM引入实践。

Method: 采用行动研究方法（16个月内5个行动周期），结合问卷、团队会议观察、心理学方法（TD-SAGAT）和待办事项数据分析。

Result: 实践者倾向于基于系统演化和成本计算优先偿还低垂果实（low-hanging fruits），待办事项中的提醒（如复选框或文本模板）可持续提升TD意识。

Conclusion: 工作坊方法可行且能带来可持续的流程改进，并提出了适用于其他IT团队的新TDM方法，如重新提交日期、'讨论过TD'复选框和可视化优先排序。

Abstract: Context. Technical debt (TD) items are constructs in a software system
providing short-term benefits but hindering future changes. TD management (TDM)
is frequently researched but rarely adopted in practice. Goal. This study aimed
to establish a TDM process in an IT company based on a predefined workshop
concept. We analyzed which research approaches practitioners adopted for each
TD activity and the TDM's long-term effect on TD awareness. Method. We used
action research (five action cycles in 16 months) with an IT team that creates
IT solutions for signal processing. To examine TD awareness, we (1) analyzed
questionnaires completed during each workshop, (2) observed team meetings, (3)
adopted a method from psychology for measuring awareness in decision-making
situations called TD-SAGAT, and (4) evaluated the backlog data. Results.
Practitioners preferred TD repayment and prioritization based on the system's
evolution and cost calculations, i.e., repayment of so-called low-hanging
fruits. Reminders in the backlog items, such as checkboxes or text templates,
led to a sustainable rise in TD awareness. Conclusions. We showed that a
workshop-based approach is feasible and leads to sustainable process changes.
New ideas for TDM applicable to other IT teams emerged, e.g., using a
re-submission date, using a Talked about TD checkbox, and using visualizations
for TD prioritization.

</details>


### [10] [From PREVENTion to REACTion: Enhancing Failure Resolution in Naval Systems](https://arxiv.org/abs/2508.15584)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 论文讨论了如何通过PREVENT和REACT方法预测和解决工业系统中的故障问题。


<details>
  <summary>Details</summary>
Motivation: 面对复杂工业系统中的异常问题，需要及时检测、定位和实施应对措施。

Method: 采用PREVENT故障预测方法和扩展的REACT故障排除模块。

Result: 成功将异常检测与故障排除程序集成。

Conclusion: 这一方法有助于在其他工业产品中推广和应用。

Abstract: Complex and large industrial systems often misbehave, for instance, due to
wear, misuse, or faults. To cope with these incidents, it is important to
timely detect their occurrences, localize the sources of the problems, and
implement the appropriate countermeasures. This paper reports our experience
with a state-of-the-art failure prediction method, PREVENT, and its extension
with a troubleshooting module, REACT, applied to naval systems developed by
Fincantieri. Our results show how to integrate anomaly detection with
troubleshooting procedures. We conclude by discussing a lesson learned, which
may help deploy and extend these analyses to other industrial products.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Homomorphism Calculus for User-Defined Aggregations](https://arxiv.org/abs/2508.15109)
*Ziteng Wang,Ruijie Fang,Linus Zheng,Dixin Tang,Isil Dillig*

Main category: cs.PL

TL;DR: 提出了一种新颖的同态演算，用于验证和否定UDAF是否为同态，并构建相应的合并算子以支持高效计算。实验表明该方法优于现有合成器。


<details>
  <summary>Details</summary>
Motivation: 为支持高效的UDAF执行，需要满足同态性质，但目前缺乏自动化验证和构建合并算子的工具。

Method: 引入同态演算，验证UDAF的同态性并构建合并算子。

Result: 实现算法并在真实UDAF上验证，性能优于现有合成器。

Conclusion: 该演算方法能有效支持UDAF的高效执行，具有实际应用价值。

Abstract: Data processing frameworks like Apache Spark and Flink provide built-in
support for user-defined aggregation functions (UDAFs), enabling the
integration of domain-specific logic. However, for these frameworks to support
\emph{efficient} UDAF execution, the function needs to satisfy a
\emph{homomorphism property}, which ensures that partial results from
independent computations can be merged correctly. Motivated by this problem,
this paper introduces a novel \emph{homomorphism calculus} that can both verify
and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also
enables the construction of a corresponding merge operator which can be used
for incremental computation and parallel execution. We have implemented an
algorithm based on our proposed calculus and evaluate it on real-world UDAFs,
demonstrating that our approach significantly outperforms two leading
synthesizers.

</details>


### [12] [Software Model Checking via Summary-Guided Search (Extended Version)](https://arxiv.org/abs/2508.15137)
*Ruijie Fang,Zachary Kincaid,Thomas Reps*

Main category: cs.PL

TL;DR: GPS是一种新的软件模型检查算法，通过静态分析的组合性摘要引导程序状态的有向搜索，高效检测程序安全性和错误路径。


<details>
  <summary>Details</summary>
Motivation: 现有软件模型检查器在检测长且输入相关的错误路径时效率不足，需要一种更高效的方法。

Method: GPS结合静态分析的摘要信息进行路径修剪和测试生成，采用分层搜索策略，并通过工具化技术实现完全性。

Result: GPS在多项基准测试中表现优于现有先进模型检查器，包括SV-COMP中的领先工具。

Conclusion: GPS通过其新颖的搜索策略和工具化技术，在高效性和完备性上取得了显著提升。

Abstract: In this work, we describe a new software model-checking algorithm called GPS.
GPS treats the task of model checking a program as a directed search of the
program states, guided by a compositional, summary-based static analysis. The
summaries produced by static analysis are used both to prune away infeasible
paths and to drive test generation to reach new, unexplored program states. GPS
can find both proofs of safety and counter-examples to safety (i.e., inputs
that trigger bugs), and features a novel two-layered search strategy that
renders it particularly efficient at finding bugs in programs featuring long,
input-dependent error paths. To make GPS refutationally complete (in the sense
that it will find an error if one exists, if it is allotted enough time), we
introduce an instrumentation technique and show that it helps GPS achieve
refutation-completeness without sacrificing overall performance. We benchmarked
GPS on a suite of benchmarks including both programs from the Software
Verification Competition (SV-COMP) and from prior literature, and found that
our implementation of GPS outperforms state-of-the-art software model checkers
(including the top performers in SV-COMP ReachSafety-Loops category), both in
terms of the number of benchmarks solved and in terms of running time.

</details>


### [13] [Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment](https://arxiv.org/abs/2508.15157)
*David M Kahn,Jan Hoffmann,Runming Li*

Main category: cs.PL

TL;DR: 本文介绍了一种鲜为人知的大步语义扩展方法，通过归纳定义捕获发散计算，无需引入错误状态，适用于多种编程语言变体。


<details>
  <summary>Details</summary>
Motivation: 大步语义在编程语言中因其简洁性和直接性受到青睐，但无法描述发散计算；本文旨在通过简单扩展解决这一问题。

Method: 提出称为'big-stop'语义的扩展，通过少量额外规则定义等价于小步语义传递闭包的评估判断，避免复杂的附加规则或全局状态。

Result: 展示了该方法在类型化、非类型化和带效应的PCF以及基于while循环的命令式语言中的适用性。

Conclusion: big-stop语义在保持大步语义简洁性的同时，有效捕获了发散计算，为相关研究提供了更简单的替代方案。

Abstract: As evident in the programming language literature, many practitioners favor
specifying dynamic program behavior using big-step over small-step semantics.
Unlike small-step semantics, which must dwell on every intermediate program
state, big-step semantics conveniently jump directly to the ever-important
result of the computation. Big-step semantics also typically involve fewer
inference rules than their small-step counterparts. However, in exchange for
ergonomics, big-step semantics give up power: Small-step semantics describes
program behaviors that are outside the grasp of big-step semantics, notably
divergence. This work presents a little-known extension of big-step semantics
with inductive definitions that captures diverging computations without
introducing error states. This big-stop semantics is illustrated for typed,
untyped, and effectful variants of PCF, as well as a while-loop-based
imperative language. Big-stop semantics extends the standard big-step inference
rules with a few additional rules to define an evaluation judgment that is
equivalent to the reflexive-transitive closure of small-step transitions. This
simple extension contrasts with other solutions in the literature which
sacrifice ergonomics by introducing many additional inference rules, global
state, and/or less-commonly-understood reasoning principles like coinduction.

</details>


### [14] [Probabilistic Inference for Datalog with Correlated Inputs](https://arxiv.org/abs/2508.15166)
*Jingbo Wang,Shashin Halalingaiah,Weiyi Chen,Chao Wang,Isil Dillig*

Main category: cs.PL

TL;DR: 论文提出Praline，一种扩展的Datalog方法，用于处理输入相关性，并通过优化和高效算法实现可扩展的精确概率推理。


<details>
  <summary>Details</summary>
Motivation: 现有的逻辑编程语言概率扩展（如ProbLog）未考虑输入事实间的统计相关性，限制了推理的准确性。

Method: 将推理任务建模为约束优化问题，并设计了一种高效的δ-准确推理算法，结合约束求解、静态分析和迭代优化。

Result: 实验表明，该方法在真实基准测试（如侧信道分析）中不仅可扩展，还能提供紧致的概率界限。

Conclusion: Praline解决了输入相关性问题，并通过高效算法实现了精确且可扩展的概率推理。

Abstract: Probabilistic extensions of logic programming languages, such as ProbLog,
integrate logical reasoning with probabilistic inference to evaluate
probabilities of output relations; however, prior work does not account for
potential statistical correlations among input facts. This paper introduces
Praline, a new extension to Datalog designed for precise probabilistic
inference in the presence of (partially known) input correlations. We formulate
the inference task as a constrained optimization problem, where the solution
yields sound and precise probability bounds for output facts. However, due to
the complexity of the resulting optimization problem, this approach alone often
does not scale to large programs. To address scalability, we propose a more
efficient $\delta$-exact inference algorithm that leverages constraint solving,
static analysis, and iterative refinement. Our empirical evaluation on
challenging real-world benchmarks, including side-channel analysis,
demonstrates that our method not only scales effectively but also delivers
tight probability bounds.

</details>


### [15] [Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern](https://arxiv.org/abs/2508.15264)
*Patrick Redmond,Jonathan Castello,José Manuel Calderón Trilla,Lindsey Kuper*

Main category: cs.PL

TL;DR: 论文提出了一种名为Core ECS的正式模型，旨在抽象ECS模式的本质，并确定了一类调度无关的确定性程序，为ECS框架的优化提供了新方向。


<details>
  <summary>Details</summary>
Motivation: ECS模式在游戏开发中广泛应用，但其概念在领域外缺乏深入理解。现有解释多局限于具体框架或模糊隐喻，缺乏严谨的抽象模型。

Method: 设计Core ECS模型，抽象ECS模式的本质，识别调度无关的确定性程序类，并对比分析现有ECS框架。

Result: Core ECS揭示了ECS模式的本质，发现现有框架未充分利用确定性并发的机会。

Conclusion: 研究为ECS框架的优化提供了理论依据，指出了实现确定性并发的技术空间。

Abstract: The Entity-Component-System (ECS) software design pattern, long used in game
development, encourages a clean separation of identity (entities), data
properties (components), and computational behaviors (systems). Programs
written using the ECS pattern are naturally concurrent, and the pattern offers
modularity, flexibility, and performance benefits that have led to a
proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known
and not well understood outside of a few domains. Existing explanations of the
ECS pattern tend to be mired in the concrete details of particular ECS
frameworks, or they explain the pattern in terms of imperfect metaphors or in
terms of what it is not. We seek a rigorous understanding of the ECS pattern
via the design of a formal model, Core ECS, that abstracts away the details of
specific implementations to reveal the essence of software using the ECS
pattern. We identify a class of Core ECS programs that behave deterministically
regardless of scheduling, enabling use of the ECS pattern as a
deterministic-by-construction concurrent programming model. With Core ECS as a
point of comparison, we then survey several real-world ECS frameworks and find
that they all leave opportunities for deterministic concurrency unexploited.
Our findings point out a space for new ECS implementation techniques that
better leverage such opportunities.

</details>


### [16] [Fair Termination for Resource-Aware Active Objects](https://arxiv.org/abs/2508.15333)
*Francesco Dagnino,Paola Giannini,Violet Ka I Pun,Ulises Torrella*

Main category: cs.PL

TL;DR: 开发了一种资源感知的主动对象核心演算，结合类型系统确保程序公平终止。


<details>
  <summary>Details</summary>
Motivation: 为分布式系统和业务流程建模提供资源感知的并发模型。

Method: 结合分级语义和类型系统技术，扩展到同步会话的公平终止方法。

Result: 构建了能确保公平终止的资源感知主动对象演算。

Conclusion: 该演算为资源感知并发模型提供了理论支持，确保程序的公平终止性。

Abstract: Active object systems are a model of distributed computation that has been
adopted for modelling distributed systems and business process workflows. This
field of modelling is, in essence, concurrent and resource-aware, motivating
the development of resource-aware formalisations on the active object model.
The contributions of this work are the development of a core calculus for
resource-aware active objects together with a type system ensuring that
well-typed programs are fairly terminating, i.e., they can always eventually
terminate. To achieve this, we combine techniques from graded semantics and
type systems, which are quite well understood for sequential programs, with
those for fair termination, which have been developed for synchronous~sessions.

</details>


### [17] [Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)](https://arxiv.org/abs/2508.15576)
*Andreas Lööw,Seung Hoon Park,Daniele Nantes-Sobrinho,Sacha-Élie Ayoun,Opale Sjöstedt,Philippa Gardner*

Main category: cs.PL

TL;DR: 该论文为内存模型参数化组合符号执行（CSE）平台提供了新的形式化基础，扩展了现有技术，支持多种内存模型，涵盖分离逻辑（SL）和不正确性分离逻辑（ISL），并基于标准定义。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对内存模型参数化CSE平台的满意形式化基础。本文旨在填补这一空白，提升灵活性并支持更广泛的语言分析。

Method: 研究者在Rocq定理证明器中形式化了新基础，并将其实例化到多种内存模型（如C和CHERI），同时涵盖了SL和ISL分析。

Result: 成功开发了一个形式化基础，验证了其在多种内存模型中的应用，扩展了SL和ISL分析的范围。

Conclusion: 该工作为内存模型参数化CSE平台提供了坚实的理论基础，促进了工具互操作性和分析的灵活性。

Abstract: Multiple successful compositional symbolic execution (CSE) tools and
platforms exploit separation logic (SL) for compositional verification and/or
incorrectness separation logic (ISL) for compositional bug-finding, including
VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian
platform, the only CSE platform that is parametric on the memory model, meaning
that it can be instantiated to different memory models, suggests that the
ability to use custom memory models allows for more flexibility in supporting
analysis of a wide range of programming languages, for implementing custom
automation, and for improving performance. However, the literature lacks a
satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for
memory-model-parametric CSE platforms. Our foundation advances the state of the
art in four ways. First, we mechanise our foundation (in the interactive
theorem prover Rocq). Second, we validate our foundation by instantiating it to
a broad range of memory models, including models for C and CHERI. Third,
whereas previous memory-model-parametric work has only covered SL analyses, we
cover both SL and ISL analyses. Fourth, our foundation is based on standard
definitions of SL and ISL (including definitions of function specification
validity, to ensure sound interoperation with other tools and platforms also
based on standard definitions).

</details>


### [18] [Active Learning for Neurosymbolic Program Synthesis](https://arxiv.org/abs/2508.15750)
*Celeste Barnaby,Qiaochu Chen,Ramya Ramalingam,Osbert Bastani,Isil Dillig*

Main category: cs.PL

TL;DR: 本文提出了一种新的主动学习技术，用于处理神经符号程序合成中神经网络预测错误的挑战，并通过SmartLabel工具在三个领域展示了其优越性。


<details>
  <summary>Details</summary>
Motivation: 为了解决神经符号程序合成中神经网络预测错误导致的主动学习技术不足的问题。

Method: 提出了一种称为受限符合评估(CCE)的新策略，结合用户反馈迭代优化，确保最终程序的等价性。

Result: SmartLabel在98%的基准测试中成功识别真实程序，平均交互次数低于5次，显著优于现有技术。

Conclusion: 该方法有效解决了神经符号程序合成中的主动学习挑战，显著提升了准确性和效率。

Abstract: The goal of active learning for program synthesis is to synthesize the
desired program by asking targeted questions that minimize user interaction.
While prior work has explored active learning in the purely symbolic setting,
such techniques are inadequate for the increasingly popular paradigm of
neurosymbolic program synthesis, where the synthesized program incorporates
neural components. When applied to the neurosymbolic setting, such techniques
can -- and, in practice, do -- return an unintended program due to
mispredictions of neural components. This paper proposes a new active learning
technique that can handle the unique challenges posed by neural network
mispredictions. Our approach is based upon a new evaluation strategy called
constrained conformal evaluation (CCE), which accounts for neural
mispredictions while taking into account user-provided feedback. Our proposed
method iteratively makes CCE more precise until all remaining programs are
guaranteed to be observationally equivalent. We have implemented this method in
a tool called SmartLabel and experimentally evaluated it on three neurosymbolic
domains. Our results demonstrate that SmartLabel identifies the ground truth
program for 98% of the benchmarks, requiring under 5 rounds of user interaction
on average. In contrast, prior techniques for active learning are only able to
converge to the ground truth program for at most 65% of the benchmarks.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [19] [Mitigating context switching in densely packed Linux clusters with Latency-Aware Group Scheduling](https://arxiv.org/abs/2508.15703)
*Al Amjad Tawfiq Isstaif,Evangelia Kalyvianaki,Richard Mortier*

Main category: cs.OS

TL;DR: 论文研究发现，在密集工作负载下，CPU上下文切换开销会显著降低集群性能，即使调度理论上合理。通过修改Linux内核调度器，减少上下文切换开销，可以在保持性能的同时将集群大小缩小28%。


<details>
  <summary>Details</summary>
Motivation: Kubernetes等集群调度器依赖节点容量和任务需求的准确估计，不准确性会导致性能下降。通常通过过度配置资源来缓解问题，但会造成浪费。

Method: 论文提出并评估了对标准Linux内核调度器的修改，以减少上下文切换的开销。关键思路是优先完成任务而非低级别的任务公平性。

Result: 实验表明，改进后的调度器能够在保持性能的同时，将集群规模缩小28%。

Conclusion: 通过优化调度器以减少上下文切换开销，可以有效提升集群性能并减少资源浪费。

Abstract: Cluster orchestrators such as Kubernetes depend on accurate estimates of node
capacity and job requirements. Inaccuracies in either lead to poor placement
decisions and degraded cluster performance. In this paper, we show that in
densely packed workloads, such as serverless applications, CPU context
switching overheads can become so significant that a node's performance is
severely degraded, even when the orchestrator placement is theoretically sound.
In practice this issue is typically mitigated by over-provisioning the cluster,
leading to wasted resources.
  We show that these context switching overhead arise from both an increase in
the average cost of an individual context switch and a higher rate of context
switching, which together amplify overhead multiplicatively when managing large
numbers of concurrent cgroups, Linux's group scheduling mechanism for managing
multi-threaded colocated workloads. We propose and evaluate modifications to
the standard Linux kernel scheduler that mitigate these effects, achieving the
same effective performance with a 28% smaller cluster size. The key insight
behind our approach is to prioritise task completion over low-level per-task
fairness, enabling the scheduler to drain contended CPU run queues more rapidly
and thereby reduce time spent on context switching.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Toward Sustainable Subterranean mMTC: Space-Air-Ground-Underground Networks Powered by LoRaWAN and Wireless Energy Transfer](https://arxiv.org/abs/2508.15058)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 提出了一个新型空间-空中-地面-地下集成网络（SAGUIN），结合LoRaWAN和无线能量传输技术，支持地下大规模机器通信（mMTC），并通过仿真验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 地下无线传感器网络（WUSNs）在恶劣环境下支持mMTC面临挑战，SAGUIN通过集成多种通信方式解决这一问题。

Method: 结合LoRaWAN和无线能量传输技术，构建SAGUIN架构，并通过仿真模拟远程地下管道监控场景评估性能。

Result: SAGUIN结合适当的时间分配策略和LoRaWAN参数配置，可有效延长设备寿命，支持可持续mMTC。

Conclusion: SAGUIN为地下mMTC提供了可行解决方案，未来需进一步研究其关键挑战。

Abstract: Wireless underground sensor networks (WUSNs), which enable real-time sensing
and monitoring of underground resources by underground devices (UDs), hold
great promise for delivering substantial social and economic benefits across
various verticals. However, due to the harsh subterranean environment, scarce
network resources, and restricted communication coverage, WUSNs face
significant challenges in supporting sustainable massive machine-type
communications (mMTC), particularly in remote, disaster-stricken, and
hard-to-reach areas. To complement this, we conceptualize in this study a novel
space-air-ground-underground integrated network (SAGUIN) architecture that
seamlessly incorporates satellite systems, aerial platforms, terrestrial
networks, and underground communications. On this basis, we integrate LoRaWAN
and wireless energy transfer (WET) technologies into SAGUIN to enable
sustainable subterranean mMTC. We begin by reviewing the relevant technical
background and presenting the architecture and implementation challenges of
SAGUIN. Then, we employ simulations to model a remote underground pipeline
monitoring scenario to evaluate the feasibility and performance of SAGUIN based
on LoRaWAN and WET technologies, focusing on the effects of parameters such as
underground conditions, time allocation, LoRaWAN spread factor (SF)
configurations, reporting periods, and harvested energy levels. Our results
evidence that the proposed SAGUIN system, when combined with the derived time
allocation strategy and an appropriate SF, can effectively extend the
operational lifetime of UDs, thereby facilitating sustainable subterranean
mMTC. Finally, we pinpoint key challenges and future research directions for
SAGUIN.

</details>


### [21] [From 5G RAN Queue Dynamics to Playback: A Performance Analysis for QUIC Video Streaming](https://arxiv.org/abs/2508.15087)
*Jashanjot Singh Sidhu,Jorge Ignacio Sandoval,Abdelhak Bentaleb,Sandra Cespedes*

Main category: cs.NI

TL;DR: 论文分析了QUIC协议与5G网络环境下视频流QoE的优化挑战，强调跨层协调的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着QUIC协议的广泛应用和5G网络的到来，视频流的QoE优化面临挑战，尤其是ABR、CC和RLC之间的复杂交互。

Method: 研究评估了现代AQM策略（如RED和L4S）对视频流的影响，重点关注其与QUIC实现、CC算法和ABR方案的动态交互。

Result: 研究发现AQM策略的有效性依赖于与QUIC、CC和ABR的协同作用，孤立优化不足以提升QoE。

Conclusion: 为充分利用5G潜力，需要跨层自适应机制，实现网络、传输和应用层的实时协调。

Abstract: The rapid adoption of QUIC as a transport protocol has transformed content
delivery by reducing latency, enhancing congestion control (CC), and enabling
more efficient multiplexing. With the advent of 5G networks, which support
ultra-low latency and high bandwidth, streaming high-resolution video at 4K and
beyond has become increasingly viable. However, optimizing Quality of
Experience (QoE) in mobile networks remains challenging due to the complex
interactions among Adaptive Bit Rate (ABR) schemes at the application layer, CC
algorithms at the transport layer, and Radio Link Control (RLC) queuing at the
link layer in the 5G network. While prior studies have largely examined these
components in isolation, this work presents a comprehensive analysis of the
impact of modern active queue management (AQM) strategies, such as RED and L4S,
on video streaming over diverse QUIC implementations--focusing particularly on
their interaction with the RLC buffer in 5G environments and the interplay
between CC algorithms and ABR schemes. Our findings demonstrate that the
effectiveness of AQM strategies in improving video streaming QoE is
intrinsically linked to their dynamic interaction with QUIC implementations, CC
algorithms and ABR schemes-highlighting that isolated optimizations are
insufficient. This intricate interdependence necessitates holistic, cross-layer
adaptive mechanisms capable of real-time coordination between network,
transport and application layers, which are crucial for fully leveraging the
capabilities of 5G networks to deliver robust, adaptive, and high-quality video
streaming.

</details>


### [22] [Toward Autonomous Digital Populations for Communication-Sensing-Computation Ecosystem](https://arxiv.org/abs/2508.15268)
*Gaosheng Zhao,Dong In Kim*

Main category: cs.NI

TL;DR: 提出一种自然启发的架构框架，利用数字孪生技术组织边缘设备为功能数字群体，并通过多群体云集成构建可演化的数字生态系统。


<details>
  <summary>Details</summary>
Motivation: 当前网络依赖集中控制、静态设计和人工干预，限制了其在大规模、分层和复杂环境中的适应性和弹性。

Method: 结合数字孪生技术，组织边缘设备为功能数字群体，并在云端实现多群体集成。

Result: 提出一种动态协调、分布式决策、持续适应和具备进化能力的下一代通信网络理论框架。

Conclusion: 该框架为构建具备动态演化和适应能力的下一代通信网络提供了理论基础。

Abstract: Future communication networks are expected to achieve deep integration of
communication, sensing, and computation, forming a tightly coupled and
autonomously operating infrastructure system. However, current reliance on
centralized control, static design, and human intervention continues to
constrain the multidimensional evolution of network functions and applications,
limiting adaptability and resilience in large-scale, layered, and complex
environments. To address these challenges, this paper proposes a
nature-inspired architectural framework that leverages digital twin technology
to organize connected devices at the edge into functional digital populations,
while enabling the emergence of an evolvable digital ecosystem through
multi-population integration at the cloud. We believe that this framework,
which combines engineering methodologies with sociotechnical insights, lays the
theoretical foundation for building next-generation communication networks with
dynamic coordination, distributed decision-making, continuous adaptation, and
evolutionary capabilities.

</details>


### [23] [Unlocking the Performance Potential of Mega-Constellation Networks: An Exploration of Structure-Building Paradigms](https://arxiv.org/abs/2508.15307)
*Xiangtong Wang,Wei Li,Menglong Yang,Songchen Han*

Main category: cs.NI

TL;DR: 本文提出了一种名为SML的新型巨星座网络结构设计范式，通过解耦局部图案和全局格子设计，解决了高可用性和低延迟的网络控制问题。


<details>
  <summary>Details</summary>
Motivation: 设计最优网络控制结构以配置最稳定的星间链路（ISL），在有限平均传输延迟内实现高可用的巨星座网络（MCN）。

Method: 提出SML范式，将MCN设计解耦为局部图案设计和全局格子设计，并开发了启发式算法SMLOP来高效求解问题。

Result: 实验验证显示，SMLOP算法在四种先进星座中显著提升了能力（5%~18%）、吞吐量（1%~12%），并减少了路径拉伸（12%~23%）和往返时间（8%~77%）。

Conclusion: SML范式及SMLOP算法为巨星座网络的高效设计提供了一种可行的解决方案。

Abstract: The network structure design plays a vital role in the mega-constellation
network (MSN) to coordinate massive network nodes to ensure the effectiveness
and reliability of operations and services for future space wireless
communications networks.
  One of the critical issues in MCN is how to design an optimal network control
structure by configuring the most stable inter-satellite link (ISL) to achieve
high available MCN within a limited average transmission delays.
  To address this problem, this paper introduces a novel MCN structure design
paradigm: Structure = Motif + Lattice (SML), which decouples MCN design into
local motifs design and global lattices design. Specifically, we formulate the
High-Availability and Low-Latency Mega-Constellation Design (HALLMD) problem,
aimed at maximizing ISL availability while minimizing the transmission latency.
To solve HALLMD, we propose SMLOP, a heuristic algorithm that efficiently finds
optimal network structures in polynomial time. Experimental validation on four
public state-of-the-art constellations demonstrates significant improvements,
including enhanced capacity by $5\sim 18\%$, increased throughput by $1\sim
12\%$, reduced path stretch by $12\sim 23\%$, and Round-Trip Time (RTT) by
$8\sim 77\%$.

</details>


### [24] [Interface on demand: Towards AI native Control interfaces for 6G](https://arxiv.org/abs/2508.15595)
*Abhishek Dandekar,Prashiddha D. Thapa,Ashrafur Rahman,Julius Schulz-Zander*

Main category: cs.NI

TL;DR: 提出了一种基于LLM的多代理框架，用于动态生成网络功能间的控制接口，提升了互操作性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统的标准化网络接口存在供应商特定不兼容、设计假设僵化和缺乏新功能适应性问题，亟需动态灵活的解决方案。

Method: 采用匹配代理和代码生成代理，结合LLM技术，通过模拟多供应商环境验证框架有效性。

Result: 性能评估展示了在成本和延迟之间的权衡，证明了AI原生动态接口生成的可行性。

Conclusion: 该研究为未来移动网络的互操作性和适应性奠定了基础。

Abstract: Traditional standardized network interfaces face significant limitations,
including vendor-specific incompatibilities, rigid design assumptions, and lack
of adaptability for new functionalities. We propose a multi-agent framework
leveraging large language models (LLMs) to generate control interfaces on
demand between network functions (NFs). This includes a matching agent, which
aligns required control functionalities with NF capabilities, and a
code-generation agent, which generates the necessary API server for interface
realization. We validate our approach using simulated multi-vendor gNB and WLAN
AP environments. The performance evaluations highlight the trade-offs between
cost and latency across LLMs for interface generation tasks. Our work sets the
foundation for AI-native dynamic control interface generation, paving the way
for enhanced interoperability and adaptability in future mobile networks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [25] [Robust Symbolic Reasoning for Visual Narratives via Hierarchical and Semantically Normalized Knowledge Graphs](https://arxiv.org/abs/2508.14941)
*Yi-Chun Chen*

Main category: cs.MM

TL;DR: 该论文提出了一种语义规范化框架，用于减少叙事知识图中的不一致性和冗余，通过词汇相似性和嵌入聚类方法改善多模态叙事理解的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前符号化的叙事图在处理视觉叙事（如漫画）时存在不一致性和冗余问题，限制了推理和泛化的效果，因此需要一种规范化方法来改善这些问题。

Method: 基于认知模型的叙事理解方法，采用词汇相似性和嵌入聚类技术，对层次化的叙事知识图进行语义规范化，减少注释噪音并保持符号透明性。

Result: 在Manga109数据集上的初步评估表明，语义规范化提高了叙事推理任务（如动作检索、角色定位和事件摘要）的一致性和鲁棒性。

Conclusion: 语义规范化是实现可扩展、认知启发的多模态叙事理解模型的关键步骤。

Abstract: Understanding visual narratives such as comics requires structured
representations that capture events, characters, and their relations across
multiple levels of story organization. However, symbolic narrative graphs often
suffer from inconsistency and redundancy, where similar actions or events are
labeled differently across annotations or contexts. Such variance limits the
effectiveness of reasoning and generalization.
  This paper introduces a semantic normalization framework for hierarchical
narrative knowledge graphs. Building on cognitively grounded models of
narrative comprehension, we propose methods that consolidate semantically
related actions and events using lexical similarity and embedding-based
clustering. The normalization process reduces annotation noise, aligns symbolic
categories across narrative levels, and preserves interpretability.
  We demonstrate the framework on annotated manga stories from the Manga109
dataset, applying normalization to panel-, event-, and story-level graphs.
Preliminary evaluations across narrative reasoning tasks, such as action
retrieval, character grounding, and event summarization, show that semantic
normalization improves coherence and robustness, while maintaining symbolic
transparency. These findings suggest that normalization is a key step toward
scalable, cognitively inspired graph models for multimodal narrative
understanding.

</details>


### [26] [Holo-Artisan: A Personalized Multi-User Holographic Experience for Virtual Museums on the Edge Intelligence](https://arxiv.org/abs/2508.14956)
*Nan-Hong Kuo,Hojjat Baghban*

Main category: cs.MM

TL;DR: Holo-Artisan是一个通过全息显示和个性化边缘智能实现虚拟博物馆多用户沉浸式体验的系统架构。


<details>
  <summary>Details</summary>
Motivation: 通过技术手段将静态博物馆展品转化为动态、互动性强的艺术作品，提升用户体验和文化遗产互动方式。

Method: 使用本地边缘计算节点处理实时用户数据，结合生成式AI和联邦学习，通过云协作平台和光线追踪技术实现个性化全息展示。

Result: 实现了多用户同步共享的个性化交互体验，同时降低了延迟和带宽使用。

Conclusion: Holo-Artisan为文化遗产互动开启了新范式，通过技术实现了艺术品的动态化和个性化互动。

Abstract: We present Holo-Artisan, a novel system architecture enabling immersive
multi-user experiences in virtual museums through true holographic displays and
personalized edge intelligence. In our design, local edge computing nodes
process real-time user data -- including pose, facial expression, and voice --
for multiple visitors concurrently. Generative AI models then drive digital
artworks (e.g., a volumetric Mona Lisa) to respond uniquely to each viewer. For
instance, the Mona Lisa can return a smile to one visitor while engaging in a
spoken Q\&A with another, all in real time. A cloud-assisted collaboration
platform composes these interactions in a shared scene using a universal scene
description, and employs ray tracing to render high-fidelity, personalized
views with a direct pipeline to glasses-free holographic displays. To preserve
user privacy and continuously improve personalization, we integrate federated
learning (FL) -- edge devices locally fine-tune AI models and share only model
updates for aggregation. This edge-centric approach minimizes latency and
bandwidth usage, ensuring a synchronized shared experience with individual
customization. Through Holo-Artisan, static museum exhibits are transformed
into dynamic, living artworks that engage each visitor in a personal dialogue,
heralding a new paradigm of cultural heritage interaction.

</details>


### [27] [\textit{adder-viz}: Real-Time Visualization Software for Transcoding Event Video](https://arxiv.org/abs/2508.14996)
*Andrew C. Freeman,Luke Reinkensmeyer*

Main category: cs.MM

TL;DR: 该论文介绍了对ADΔER表示法的改进，并推出了adder-viz软件，用于实时事件转码过程的可视化。


<details>
  <summary>Details</summary>
Motivation: 传统的事件视频表示法在灵活性、速度和压缩性方面存在局限，ADΔER表示法旨在解决这些问题。

Method: 作者提出了改进的ADΔER表示法，并开发了开源软件adder-viz，支持实时事件转码过程的可视化。

Result: 提供了MIT许可的adder-viz软件，解决了事件视频表示法的局限性，并公开在GitHub上。

Conclusion: ADΔER表示法及其相关工具为事件视频研究提供了更高效和灵活的解决方案。

Abstract: Recent years have brought about a surge in neuromorphic ``event'' video
research, primarily targeting computer vision applications. Event video eschews
video frames in favor of asynchronous, per-pixel intensity samples. While much
work has focused on a handful of representations for specific event cameras,
these representations have shown limitations in flexibility, speed, and
compressibility. We previously proposed the unified AD{\Delta}ER representation
to address these concerns. This paper introduces numerous improvements to the
\textit{adder-viz} software for visualizing real-time event transcode processes
and applications in-the-loop. The MIT-licensed software is available from a
centralized repository at
\href{https://github.com/ac-freeman/adder-codec-rs}{https://github.com/ac-freeman/adder-codec-rs}.

</details>


### [28] [A Low-Latency 3D Live Remote Visualization System for Tourist Sites Integrating Dynamic and Pre-captured Static Point Clouds](https://arxiv.org/abs/2508.15398)
*Takahiro Matsumoto,Masafumi Suzuki,Mariko Yamaguchi,Masakatsu Aoki,Shunsuke Konagai,Kazuhiko Murasaki*

Main category: cs.MM

TL;DR: 提出了一种结合LiDAR和摄像头的系统，用于实时动态点云捕获，并通过预捕获静态点云整合实现广域3D可视化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在户外景点应用受限，因传感器布局和日光变化问题。

Method: 结合多LiDAR和摄像头，动态调整静态点云颜色以适应光照变化。

Result: 系统在广域场景中保持30 fps，延迟低于100 ms，并在实际景点中验证有效性。

Conclusion: 系统成功解决了户外景点3D实时捕获的挑战。

Abstract: Various real-time methods for capturing and transmitting dynamic 3D spaces
have been proposed, including those based on RGB-D cameras and volumetric
capture. However, applying existing methods to outdoor tourist sites remains
difficult because maintenance and aesthetic constraints limit sensor placement,
and daylight variability complicates processing. We propose a system that
combines multiple LiDARs and cameras for live dynamic point cloud capture, and
integrates them with pre-captured static point clouds for wide-area 3D
visualization. The system sustains 30 fps across wide-area scenes while keeping
latency below 100 ms. To mitigate lighting inconsistencies, static point-cloud
colors are automatically adjusted to current lighting. The effectiveness of our
system is demonstrated through real-world deployment in a tourist site.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [29] [LitForager: Exploring Multimodal Literature Foraging Strategies in Immersive Sensemaking](https://arxiv.org/abs/2508.15043)
*Haoyang Yang,Elliott H. Faa,Weijian Liu,Shunan Guo,Duen Horng Chau,Yalong Yang*

Main category: cs.HC

TL;DR: LitForager是一款面向沉浸式文献探索的工具，旨在通过3D网络可视化帮助研究者高效发现和组织文献。


<details>
  <summary>Details</summary>
Motivation: 研究者面临文献快速增长的挑战，现有工具多关注信息整合而非文献发现，限制了完整的理解流程。

Method: 开发LitForager工具，基于WebXR技术，通过3D网络和多模式交互支持文献探索。

Result: 用户研究表明，LitForager能有效支持灵活的文献发现策略和空间理解。

Conclusion: LitForager填补了沉浸式环境中文献发现工具的空白，提升了研究者的文献探索效率。

Abstract: Exploring and comprehending relevant academic literature is a vital yet
challenging task for researchers, especially given the rapid expansion in
research publications. This task fundamentally involves sensemaking -
interpreting complex, scattered information sources to build understanding.
While emerging immersive analytics tools have shown cognitive benefits like
enhanced spatial memory and reduced mental load, they predominantly focus on
information synthesis (e.g., organizing known documents). In contrast, the
equally important information foraging phase - discovering and gathering
relevant literature - remains underexplored within immersive environments,
hindering a complete sensemaking workflow. To bridge this gap, we introduce
LitForager, an interactive literature exploration tool designed to facilitate
information foraging of research literature within an immersive sensemaking
workflow using network-based visualizations and multimodal interactions.
Developed with WebXR and informed by a formative study with researchers,
LitForager supports exploration guidance, spatial organization, and seamless
transition through a 3D literature network. An observational user study with 15
researchers demonstrated LitForager's effectiveness in supporting fluid
foraging strategies and spatial sensemaking through its multimodal interface.

</details>


### [30] [Understanding Accessibility Needs of Blind Authors on CMS-Based Websites](https://arxiv.org/abs/2508.15045)
*Guillermo Vera-Amaro,José Rafael Rojano-Cáceres*

Main category: cs.HC

TL;DR: 研究探讨了盲人用户在内容管理系统（CMS）中作为内容创作者面临的障碍，提出改进方案以提升其使用体验。


<details>
  <summary>Details</summary>
Motivation: 当前CMS对盲人内容创作者的支持不足，尤其是在界面设计和反馈机制上，缺乏深入研究。

Method: 结合自动化工具和手动可用性测试（包括盲人和视力正常参与者），并采用专家分析方法。

Result: 块状编辑器存在严重可用性问题，而文本编辑器、AI生成图像描述和针对性培训显著提升用户体验。

Conclusion: 自动化评估局限性大，需通过用户中心设计、简化导航和整合AI工具来提升CMS无障碍性。

Abstract: This paper addresses the limited attention given to blind users as content
creators in Content Management Systems (CMS), a gap that remains under-explored
in web accessibility research. For blind authors, effective interaction with
CMS platforms requires more than technical compliance; it demands interfaces
designed with semantic clarity, predictable navigation, and meaningful feedback
for screen reader users. This study investigates the accessibility barriers
blind users face when performing key tasks, such as page creation, menu
editing, and image publishing, using CMS platforms. A two-fold evaluation was
conducted using automated tools and manual usability testing with three blind
and one sighted participant, complemented by expert analysis based on the
Barrier Walkthrough method. Results showed that block-based interfaces were
particularly challenging, often marked as accessible by automated tools but
resulting in critical usability issues during manual evaluation. The use of a
text-based editor, the integration of AI-generated image descriptions, and
training aligned with screen reader workflows, significantly improved usability
and autonomy. These findings underscore the limitations of automated
assessments and highlight the importance of user-centered design practices.
Enhancing CMS accessibility requires consistent navigation structures, reduced
reliance on visual interaction patterns, and the integration of AI tools that
support blind content authors throughout the content creation process.

</details>


### [31] [QueryGenie: Making LLM-Based Database Querying Transparent and Controllable](https://arxiv.org/abs/2508.15146)
*Longfei Chen,Shenghan Gao,Shiwei Wang,Ken Lin,Yun Wang,Quan Li*

Main category: cs.HC

TL;DR: QueryGenie是一个交互式系统，旨在解决LLM驱动的数据库查询中的用户意图误解、幻觉内容生成和缺乏反馈机制等问题，通过实时验证和交互式控制提升查询体验。


<details>
  <summary>Details</summary>
Motivation: 现有工具在数据库查询中存在用户意图误解、幻觉内容生成和缺乏反馈机制等问题，影响了可靠性和实用性。

Method: 开发了QueryGenie系统，采用增量推理、实时验证和响应式交互机制，使用户能够迭代优化查询逻辑并确保与意图一致。

Result: 通过交互式控制和实时验证，提升了查询的透明度和可控性。

Conclusion: QueryGenie有效地解决了LLM驱动的数据库查询中的关键问题，增强了用户体验和实用性。

Abstract: Conversational user interfaces powered by large language models (LLMs) have
significantly lowered the technical barriers to database querying. However,
existing tools still encounter several challenges, such as misinterpretation of
user intent, generation of hallucinated content, and the absence of effective
mechanisms for human feedback-all of which undermine their reliability and
practical utility. To address these issues and promote a more transparent and
controllable querying experience, we proposed QueryGenie, an interactive system
that enables users to monitor, understand, and guide the LLM-driven query
generation process. Through incremental reasoning, real-time validation, and
responsive interaction mechanisms, users can iteratively refine query logic and
ensure alignment with their intent.

</details>


### [32] [ReviseMate: Exploring Contextual Support for Digesting STEM Paper Reviews](https://arxiv.org/abs/2508.15148)
*Yuansong Xu,Shuhao Zhang,Yijie Fan,Shaohan Shi,Zhenhui Peng,Quan Li*

Main category: cs.HC

TL;DR: ReviseMate是一种交互式系统，旨在帮助研究者更高效地消化和整合审稿人反馈，优于传统方法，并在实际应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统审稿反馈消化过程耗时长、易疲劳且需高分析能力，现有工具缺乏针对性支持，需要更好的解决方案。

Method: 通过访谈和故事板设计开发ReviseMate系统，并进行控制用户研究（N=31）和实地部署（N=6）验证。

Result: ReviseMate在用户研究中表现优于基线方法，实地部署进一步验证其在实际场景中的有效性。

Conclusion: 交互式工具（如ReviseMate）能显著提升审稿反馈的消化与整合效果，具有广泛应用潜力。

Abstract: Effectively assimilating and integrating reviewer feedback is crucial for
researchers seeking to refine their papers and handle potential rebuttal phases
in academic venues. However, traditional review digestion processes present
challenges such as time consumption, reading fatigue, and the requisite for
comprehensive analytical skills. Prior research on review analysis often
provides theoretical guidance with limited targeted support. Additionally,
general text comprehension tools overlook the intricate nature of
comprehensively understanding reviews and lack contextual assistance. To bridge
this gap, we formulated research questions to explore the authors' concerns and
methods for enhancing comprehension during the review digestion phase. Through
interviews and the creation of storyboards, we developed ReviseMate, an
interactive system designed to address the identified challenges. A controlled
user study (N=31) demonstrated the superiority of ReviseMate over baseline
methods, with positive feedback regarding user interaction. Subsequent field
deployment (N=6) further validated the effectiveness of ReviseMate in
real-world review digestion scenarios. These findings underscore the potential
of interactive tools to significantly enhance the assimilation and integration
of reviewer feedback during the manuscript review process.

</details>


### [33] [Evaluating an Immersive Analytics Application at an Enterprise Business Intelligence Customer Conference](https://arxiv.org/abs/2508.15152)
*Matthew Brehmer,Ginger Gloystein,Bailiang Zhou,Abby Gray,Sruthi Pillai,Ben Medina,Vidya Setlur*

Main category: cs.HC

TL;DR: 该论文通过对Tableau for visionOS在大型企业BI会议上的评估，探讨了沉浸式分析应用的可用性和实用性问题，并提出了新的评估方法需求。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解企业环境中沉浸式分析应用（如Tableau for visionOS）的可用性和潜在效用，尤其是在新交互方式（如头戴显示器）下如何评估用户体验。

Method: 研究采用形成性评估方法，通过对22名参与者的反馈进行分析，结合定性和定量数据，探讨了该技术在BI工作流中的实际应用。

Result: 研究发现，评估沉浸式分析需兼顾体验的新颖性和实际效用，并提出了整合定性与定量方法的评估框架。

Conclusion: 论文为企业级沉浸式分析评估提供了新的视角，强调了在HMD交互中评估3D界面的重要性。

Abstract: We reflect on an evaluation of an immersive analytics application (Tableau
for visionOS) conducted at a large enterprise business intelligence (BI)
conference. Conducting a study in such a context offered an opportunistic
setting to gather diverse feedback. However, this setting also highlighted the
challenge of evaluating usability while also assessing potential utility, as
feedback straddled between the novelty of the experience and the practicality
of the application in participants' analytical workflows. This formative
evaluation with 22 participants allowed us to gather insights with respect to
the usability of Tableau for visionOS, along with broader perspectives on the
potential for head-mounted displays (HMDs) to promote new ways to engage with
BI data. Our experience suggests a need for new evaluation considerations that
integrate qualitative and quantitative measures and account for unique
interaction patterns with 3D representations and interfaces accessible via an
HMD. Overall, we contribute an enterprise perspective on evaluation
methodologies for immersive analytics.

</details>


### [34] [GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design](https://arxiv.org/abs/2508.15227)
*Wen-Fan Wang,Ting-Ying Lee,Chien-Ting Lu,Che-Wei Hsu,Nil Ponsa Campany,Yu Chen,Mike Y. Chen,Bing-Yu Chen*

Main category: cs.HC

TL;DR: GenTune是一个改进设计师与生成式AI协作的工具，通过清晰地映射AI生成的提示与图像内容，解决了当前实践中提示理解难和全局一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 设计师在使用生成式AI时面临两个主要挑战：LLM生成的冗长提示难以理解与修改，以及inpainting在局部编辑时可能影响全局一致性。

Method: 提出GenTune系统，允许设计师选择图像元素并回溯到对应提示标签，从而精准修改并保持全局一致性。

Result: GenTune显著提升了提示与图像的理解、编辑质量和效率，设计师满意度显著提高（p < .01）。实地研究也验证了其有效性。

Conclusion: GenTune通过改进人机协作机制，为设计工作流程提供了更高效且一致的生成式AI工具。

Abstract: Environment designers in the entertainment industry create imaginative 2D and
3D scenes for games, films, and television, requiring both fine-grained control
of specific details and consistent global coherence. Designers have
increasingly integrated generative AI into their workflows, often relying on
large language models (LLMs) to expand user prompts for text-to-image
generation, then iteratively refining those prompts and applying inpainting.
However, our formative study with 10 designers surfaced two key challenges: (1)
the lengthy LLM-generated prompts make it difficult to understand and isolate
the keywords that must be revised for specific visual elements; and (2) while
inpainting supports localized edits, it can struggle with global consistency
and correctness. Based on these insights, we present GenTune, an approach that
enhances human--AI collaboration by clarifying how AI-generated prompts map to
image content. Our GenTune system lets designers select any element in a
generated image, trace it back to the corresponding prompt labels, and revise
those labels to guide precise yet globally consistent image refinement. In a
summative study with 20 designers, GenTune significantly improved prompt--image
comprehension, refinement quality, and efficiency, and overall satisfaction
(all $p < .01$) compared to current practice. A follow-up field study with two
studios further demonstrated its effectiveness in real-world settings.

</details>


### [35] [Visualization on Smart Wristbands: Results from an In-situ Design Workshop with Four Scenarios](https://arxiv.org/abs/2508.15249)
*Alaul Islam,Fairouz Grioui,Raimund Dachselt,Petra Isenberg*

Main category: cs.HC

TL;DR: 研究通过工作坊探讨了智能手环在不同臂姿下的数据可视化设计，发现用户偏好自适应臂姿的可视化方案。


<details>
  <summary>Details</summary>
Motivation: 智能手环的可视化设计因臂姿不同而面临挑战，研究旨在解决这一问题。

Method: 采用基于纸张的构思练习，分析不同使用场景（如办公、散步）下的设计需求。

Result: 结果表明，用户更倾向于能自适应臂姿的可视化设计。

Conclusion: 智能手环可视化设计需考虑臂姿变化，自适应设计是未来的方向。

Abstract: We present the results of an in-situ ideation workshop for designing data
visualizations on smart wristbands that can show data around the entire wrist
of a wearer. Wristbands pose interesting challenges because the visibility of
different areas of the band depends on the wearer's arm posture. We focused on
four usage scenarios that lead to different postures: office work, leisurely
walks, cycling, and driving. As the technology for smart wristbands is not yet
commercially available, we conducted a paper-based ideation exercise that
showed how spatial layout and visualization design on smart wristbands may need
to vary depending on the types of data items of interest and arm postures.
Participants expressed a strong preference for responsive visualization designs
that could adapt to the movement of wearers' arms. Supplemental material from
the study is available here: https://osf.io/4hrca/.

</details>


### [36] [Spatio-Temporal Mixed and Augmented Reality Experience Description for Interactive Playback](https://arxiv.org/abs/2508.15258)
*Dooyoung Kim,Woontack Woo*

Main category: cs.HC

TL;DR: 提出了一种名为MAR-ED的新框架，用于标准化描述过去事件的时空混合和增强现实体验，以支持用户在当前物理空间中的交互式自适应回放。


<details>
  <summary>Details</summary>
Motivation: 当前空间媒体技术主要关注静态内容的捕捉或回放，缺乏与环境互动或语义结构的描述，MAR-ED旨在填补这一空白。

Method: 基于三个核心原语：事件原语（语义场景图表示）、关键帧原语（高效数据访问）和回放原语（用户驱动的自适应互动回放），实现记录体验到自适应MAR体验的转换。

Result: MAR-ED框架能够动态适应新环境，并通过用户输入调整叙事，将数字记忆和记录事件升级为沉浸式的集体体验。

Conclusion: MAR-ED为培训、文化遗产和交互式叙事等领域的应用提供了新范式，无需复杂的自适应渲染。

Abstract: We propose the Spatio-Temporal Mixed and Augmented Reality Experience
Description (MAR-ED), a novel framework to standardize the representation of
past events for interactive and adaptive playback in a user's present physical
space. While current spatial media technologies have primarily focused on
capturing or replaying content as static assets, often disconnected from the
viewer's environment or offering limited interactivity, the means to describe
an experience's underlying semantic and interactive structure remains
underexplored. We propose a descriptive framework called MAR-ED based on three
core primitives: 1) Event Primitives for semantic scene graph representation,
2) Keyframe Primitives for efficient and meaningful data access, and 3)
Playback Primitives for user-driven adaptive interactive playback of recorded
MAR experience. The proposed flowchart of the three-stage process of the
proposed MAR-ED framework transforms a recorded experience into a unique
adaptive MAR experience during playback, where its spatio-temporal structure
dynamically conforms to a new environment and its narrative can be altered by
live user input. Drawing on this framework, personal digital memories and
recorded events can evolve beyond passive 2D/3D videos into immersive,
spatially-integrated group experiences, opening new paradigms for training,
cultural heritage, and interactive storytelling without requiring complex,
per-user adaptive rendering.

</details>


### [37] [Foundation Models for Cross-Domain EEG Analysis Application: A Survey](https://arxiv.org/abs/2508.15716)
*Hongqi Li,Yitong Chen,Yujuan Wang,Weihang Ni,Haodong Zhang*

Main category: cs.HC

TL;DR: 该论文提出了一种基于模态的分类系统，用于EEG分析中的基础模型，涵盖EEG解码、EEG文本、EEG视觉、EEG音频及多模态框架，并探讨了开放挑战如模型可解释性和实际应用。


<details>
  <summary>Details</summary>
Motivation: 当前EEG分析中基础模型的研究分散且缺乏系统性分类，需要统一框架以促进方法开发和实际应用。

Method: 提出了一种基于输出模态的分类法，系统整理了EEG基础模型的研究进展，并分析了各类别的设计思想和理论依据。

Result: 建立了首个全面的EEG基础模型分类体系，为未来研究提供了参考框架。

Conclusion: 通过系统性分类，该研究促进了EEG基础模型向可扩展、可解释和实际应用的转化。

Abstract: Electroencephalography (EEG) analysis stands at the forefront of neuroscience
and artificial intelligence research, where foundation models are reshaping the
traditional EEG analysis paradigm by leveraging their powerful representational
capacity and cross-modal generalization. However, the rapid proliferation of
these techniques has led to a fragmented research landscape, characterized by
diverse model roles, inconsistent architectures, and a lack of systematic
categorization. To bridge this gap, this study presents the first comprehensive
modality-oriented taxonomy for foundation models in EEG analysis,
systematically organizing research advances based on output modalities of the
native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal
frameworks. We rigorously analyze each category's research ideas, theoretical
foundations, and architectural innovations, while highlighting open challenges
such as model interpretability, cross-domain generalization, and real-world
applicability in EEG-based systems. By unifying this dispersed field, our work
not only provides a reference framework for future methodology development but
accelerates the translation of EEG foundation models into scalable,
interpretable, and online actionable solutions.

</details>


### [38] [Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI](https://arxiv.org/abs/2508.15727)
*Hannah Selder,Florian Fischer,Per Ola Kristensson,Arthur Fleig*

Main category: cs.HC

TL;DR: 本文通过系统分析奖励函数设计对HCI任务（如指向、跟踪和选择反应）的影响，提出了优化生物力学模拟的实用指南。


<details>
  <summary>Details</summary>
Motivation: 为了减少HCI研究人员和实践者在设计奖励函数时的时间和计算资源浪费，探索如何通过系统方法优化奖励函数设计。

Method: 系统地分析了努力最小化、任务完成奖励和目标接近激励对HCI任务的影响，并通过实验验证了这些奖励组件的权重对任务成功和完成时间的敏感性。

Result: 发现接近激励对引导运动至关重要，任务完成奖励确保任务成功，而适当缩放的努力项有助于提升动作规律性。

Conclusion: 本文通过优化奖励函数设计，提升了生物力学用户模型在真实界面开发中的效率和适用性，推动了基于模拟的HCI设计和评估。

Abstract: Designing effective reward functions is critical for reinforcement
learning-based biomechanical simulations, yet HCI researchers and practitioners
often waste (computation) time with unintuitive trial-and-error tuning. This
paper demystifies reward function design by systematically analyzing the impact
of effort minimization, task completion bonuses, and target proximity
incentives on typical HCI tasks such as pointing, tracking, and choice
reaction. We show that proximity incentives are essential for guiding movement,
while completion bonuses ensure task success. Effort terms, though optional,
help refine motion regularity when appropriately scaled. We perform an
extensive analysis of how sensitive task success and completion time depend on
the weights of these three reward components. From these results we derive
practical guidelines to create plausible biomechanical simulations without the
need for reinforcement learning expertise, which we then validate on remote
control and keyboard typing tasks. This paper advances simulation-based
interaction design and evaluation in HCI by improving the efficiency and
applicability of biomechanical user modeling for real-world interface
development.

</details>


### [39] ["Does the cafe entrance look accessible? Where is the door?" Towards Geospatial AI Agents for Visual Inquiries](https://arxiv.org/abs/2508.15752)
*Jon E. Froehlich,Jared Hwang,Zeyu Wang,John S. O'Meara,Xia Su,William Huang,Yang Zhang,Alex Fiannaca,Philip Nelson,Shaun Kane*

Main category: cs.HC

TL;DR: 提出Geo-Visual Agents的愿景，通过多模态AI分析地理空间图像和GIS数据，解决视觉-空间问题。


<details>
  <summary>Details</summary>
Motivation: 传统数字地图依赖GIS结构化数据，无法有效回答视觉化的地理问题。

Method: 结合街道景观、地点照片、航拍图像等多源地理空间数据，构建多模态AI代理。

Result: 提出了Geo-Visual Agents的愿景，展示了三种示例，并探讨了挑战和未来机会。

Conclusion: Geo-Visual Agents为地理视觉问题提供了新的解决方案，具有广泛应用潜力。

Abstract: Interactive digital maps have revolutionized how people travel and learn
about the world; however, they rely on pre-existing structured data in GIS
databases (e.g., road networks, POI indices), limiting their ability to address
geo-visual questions related to what the world looks like. We introduce our
vision for Geo-Visual Agents--multimodal AI agents capable of understanding and
responding to nuanced visual-spatial inquiries about the world by analyzing
large-scale repositories of geospatial images, including streetscapes (e.g.,
Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial
imagery (e.g., satellite photos) combined with traditional GIS data sources. We
define our vision, describe sensing and interaction approaches, provide three
exemplars, and enumerate key challenges and opportunities for future work.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [40] [Hybrelighter: Combining Deep Anisotropic Diffusion and Scene Reconstruction for On-device Real-time Relighting in Mixed Reality](https://arxiv.org/abs/2508.14930)
*Hanwen Zhao,John Akers,Baback Elmieh,Ira Kemelmacher-Shlizerman*

Main category: cs.GR

TL;DR: 提出了一种混合现实场景重新照明方法，结合图像分割、各向异性扩散和计算简单的滤波技术，解决了现有方法在实时性和准确性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以实时运行，场景重建方法因扫描限制导致结果不准确，2D图像滤波方法无法处理复杂几何和阴影。

Method: 集成图像分割、基于各向异性扩散的光照传播和基础场景理解，结合滤波技术的计算简单性。

Result: 在边缘设备上实现100 fps的实时性能，并通过校正扫描误差提供高质量的重照明效果。

Conclusion: 该方法在实时性和准确性上优于行业标准，适用于如房地产可视化等实际应用。

Abstract: Mixed Reality scene relighting, where virtual changes to lighting conditions
realistically interact with physical objects, producing authentic illumination
and shadows, can be used in a variety of applications. One such application in
real estate could be visualizing a room at different times of day and placing
virtual light fixtures. Existing deep learning-based relighting techniques
typically exceed the real-time performance capabilities of current MR devices.
On the other hand, scene understanding methods, such as on-device scene
reconstruction, often yield inaccurate results due to scanning limitations, in
turn affecting relighting quality. Finally, simpler 2D image filter-based
approaches cannot represent complex geometry and shadows. We introduce a novel
method to integrate image segmentation, with lighting propagation via
anisotropic diffusion on top of basic scene understanding, and the
computational simplicity of filter-based techniques. Our approach corrects
on-device scanning inaccuracies, delivering visually appealing and accurate
relighting effects in real-time on edge devices, achieving speeds as high as
100 fps. We show a direct comparison between our method and the industry
standard, and present a practical demonstration of our method in the
aforementioned real estate example.

</details>


### [41] [Inference Time Debiasing Concepts in Diffusion Models](https://arxiv.org/abs/2508.14933)
*Lucas S. Kupssinskü,Marco N. Bochernitsan,Jordan Kopper,Otávio Parraga,Rodrigo C. Barros*

Main category: cs.GR

TL;DR: DeCoDi是一种针对文本到图像扩散模型的去偏方法，通过调整推断过程减少偏见，同时保持图像质量，计算开销低，适用于任何扩散模型。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习去偏方法通常复杂且计算密集，DeCoDi旨在提供一种更易用且高效的去偏解决方案。

Method: 直接修改扩散过程的推断步骤，避免潜在维度中偏见概念的分布。

Result: 实验表明DeCoDi在性别、种族和年龄偏见去除方面有效，人工评估和GPT4o自动评估结果一致。

Conclusion: DeCoDi能显著提升扩散模型生成图像的多样性，潜在应用广泛。

Abstract: We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based
models that changes the inference procedure, does not significantly change
image quality, has negligible compute overhead, and can be applied in any
diffusion-based image generation model. DeCoDi changes the diffusion process to
avoid latent dimension regions of biased concepts. While most deep learning
debiasing methods require complex or compute-intensive interventions, our
method is designed to change only the inference procedure. Therefore, it is
more accessible to a wide range of practitioners. We show the effectiveness of
the method by debiasing for gender, ethnicity, and age for the concepts of
nurse, firefighter, and CEO. Two distinct human evaluators manually inspect
1,200 generated images. Their evaluation results provide evidence that our
method is effective in mitigating biases based on gender, ethnicity, and age.
We also show that an automatic bias evaluation performed by the GPT4o is not
significantly statistically distinct from a human evaluation. Our evaluation
shows promising results, with reliable levels of agreement between evaluators
and more coverage of protected attributes. Our method has the potential to
significantly improve the diversity of images it generates by diffusion-based
text-to-image generative models.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [42] [Money in Motion: Micro-Velocity and Usage of Ethereums Liquid Staking Tokens](https://arxiv.org/abs/2508.15391)
*Benjamin Kraner,Luca Pennella,Nicolò Vallarano,Claudio J. Tessone*

Main category: cs.ET

TL;DR: 该研究提出了一种微速度框架，用于分析Lido流动性质押代币stETH及其ERC-20包装形式wstETH的链上流通情况。通过重建完整的转账和份额历史，计算地址级别的速度并分解为行为模式。


<details>
  <summary>Details</summary>
Motivation: 尽管流动性质押代币（LSTs）的重要性日益增长，但其微观层面的货币动态尚未得到充分研究。

Method: 研究人员构建了一个开源的数据索引管道，分析事件日志和历史合约状态，并发布了两组公开数据集，涵盖截至2024年11月8日的stETH和wstETH的所有转账记录。

Result: 研究发现，两种代币的流通速度持续较高，但活动高度集中，主要由少数大型地址（可能是机构账户）主导。此外，用户行为逐渐转向更便于DeFi组合的wstETH。

Conclusion: 该研究首次大规模实证描述流动性质押代币的流通情况，为监控质押资产流动提供了可扩展的模板，并为研究社区提供了新的开放资源。

Abstract: We introduce a micro-velocity framework for analysing the on-chain
circulation of Lidos liquid-staking tokens, stETH, and its wrapped ERC-20 form,
wstETH. By reconstructing full transfer and share-based accounting histories,
we compute address-level velocities and decompose them into behavioural
components. Despite their growing importance, the micro-level monetary dynamics
of LSTs remain largely unexplored. Our data reveal persistently high velocity
for both tokens, reflecting intensive reuse within DeFi. Yet activity is highly
concentrated: a small cohort of large addresses, likely institutional accounts,
are responsible for most turnover, while the rest of the users remain largely
passive. We also observe a gradual transition in user behavior, characterized
by a shift toward wstETH, the non-rebasing variant of stETH. This shift appears
to align with DeFi composability trends, as wstETH is more frequently deployed
across protocols such as AAVE, Spark, Balancer, and SkyMoney.
  To make the study fully reproducible, we release (i) an open-source pipeline
that indexes event logs and historical contract state, and (ii) two public
datasets containing every Transfer and TransferShares record for stETH and
wstETH through 2024-11-08. This is the first large-scale empirical
characterisation of liquid-staking token circulation. Our approach offers a
scalable template for monitoring staking asset flows and provides new,
open-access resources to the research community.

</details>


### [43] [Distributed Shared Layered Storage Quantum Simulator: A novel quantum simulation system for efficient scaling and cost optimization](https://arxiv.org/abs/2508.15542)
*Mingyang Yu,Haorui Yang,Donglin Wang,Desheng Kong,Ji Du,Yulong Fu,Wei Wang,Jing Xu*

Main category: cs.ET

TL;DR: 提出了一种新型分布式共享分层存储量子模拟器（DSLSQS），通过创新的分布式架构和去TCP/IP网络技术，解决量子模拟器的高频遍历需求，提升性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有分布式技术无法满足量子模拟器的高频遍历需求，导致单节点瓶颈。需要一种新的分布式架构来克服这一限制。

Method: 采用分布式共享数据存储架构和去TCP/IP网络技术，结合分层存储技术，减少高性能内存使用。

Result: 实验证明，DSLSQS在27量子比特模拟中性能提升超过350%，显著降低存储成本。

Conclusion: DSLSQS为量子计算的实际部署提供了关键见解，并为分布式量子模拟集群的开发提供了有效框架。

Abstract: Quantum simulators are essential tools for developing and testing quantum
algorithms. However, the high-frequency traversal characteristic of quantum
simulators represents an unprecedented demand in the history of IT, and
existing distributed technologies is unable to meet this requirement, resulting
in a single-node bottleneck of quantum simulator. To overcome this limitation,
this paper introduces a novel Distributed Shared Layered Storage Quantum
Simulator (DSLSQS). By leveraging an innovative distributed architecture in
which multiple computational nodes share data storage directly, together with
De-TCP/IP networking technology, DSLSQS effectively eliminates East-West data
flow in distributed systems. This approach mitigates the bottleneck of
distributed quantum simulation clusters and enhances the scalability. Moreover,
the system employs layered storage technology, which reduces usage of expensive
high-performance memory and substantially lowers simulation costs. Furthermore,
this paper systematically analyzes the performance and cost constraints of
distributed quantum simulator cluster, identifying distributed networking as
the primary performance bottleneck and highlighting that minimizing storage
costs is crucial to reducing the total cost. Finally, experimental evaluations
with a 27-qubit simulation confirm the successful implementation of layered
storage within the quantum simulator. DSLSQS significantly enhances simulation
efficiency, yielding a performance improvement of over 350% compared to
existing distributed technologies. These results underscore the superior
performance and scalability of the proposed architecture in managing complex
quantum computing tasks. This paper provides crucial insights for the practical
deployment of quantum computing and presents an effective framework for the
development of distributed quantum simulation clusters.

</details>


### [44] [QVecOpt: An Efficient Storage and Computing Opti-mization Framework for Large-scale Quantum State Simulation](https://arxiv.org/abs/2508.15545)
*Mingyang Yu,Haorui Yang,Donglin Wang,Desheng Kong,Ji Du,Yulong Fu,Jing Xu*

Main category: cs.ET

TL;DR: 该研究提出了量子矢量优化框架(QVecOpt)，通过四种策略优化经典计算平台上的大规模量子态模拟，显著降低了计算和I/O复杂度，提升了模拟效率。


<details>
  <summary>Details</summary>
Motivation: 针对经典计算平台上大规模量子态模拟面临的内存限制、频繁磁盘I/O和高计算复杂度等挑战，研究旨在提出一种优化框架。

Method: 结合振幅配对、缓存优化、块存储优化和并行优化四种策略，优化状态矢量的存储和计算调度。

Result: 相比分层存储模拟，该方法将单量子门的状态矢量遍历从2^n次降至1次，计算和I/O复杂度从O(2^n)降至O(2^n/C)和O(2^n/B)，16-29量子位的模拟效率提升近十倍。

Conclusion: QVecOpt为大规模量子计算的经典模拟提供了高效、可扩展的解决方案，具有重要学术和实用价值。

Abstract: In response to the challenges in large-scale quantum state simulation on
classical computing platforms, including memory limits, frequent disk I/O, and
high computational complexity, this study builds upon a previously proposed
hierarchical storage-based quantum simulation system and introduces an
optimization framework, the Quantum Vector Optimization Framework (QVecOpt).
QVecOpt integrates four strategies: amplitude pairing, cache optimization,
block storage optimization, and parallel optimization. These collectively
enhance state vector storage and computational scheduling. The amplitude
pairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing
traversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache
optimization pre-allocates buffers and loads only required data, cutting disk
I/O. Block storage optimization partitions the state vector for on-demand
loading and local updates, reducing redundant access. Parallel optimization
distributes the state vector across nodes for collaborative computation,
achieving near-linear speedup. Complexity analysis shows that, compared with
hierarchical storage simulation, the method reduces state vector traversals for
single-qubit gates from $2^n$ to 1, removing the main bottleneck. It also
lowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and
$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,
breaking the memory bottleneck of existing tools and enabling high-bit quantum
circuit simulations beyond traditional methods. This work provides an
efficient, scalable solution for classical simulation of large-scale quantum
computation with significant academic and practical value.

</details>


### [45] [Low-Power Control of Resistance Switching Transitions in First-Order Memristors](https://arxiv.org/abs/2508.15620)
*Valeriy A. Slipko,Alon Ascoli,Fernando Corinto,Yuriy V. Pershin*

Main category: cs.ET

TL;DR: 该研究提出了一种优化一阶忆阻器件开关转换的通用方法，旨在开发最节能的电阻编程协议。通过针对两种电压控制器件模型的应用，展示了根据设备特性和约束条件选择最优电压脉冲的策略。


<details>
  <summary>Details</summary>
Motivation: 忆阻器件的低功耗控制在学术和工业界备受关注，解决所谓的“电压-时间困境”是研究重点。

Method: 采用通用优化方法，针对两种电压控制忆阻器模型（Kvatinsky和Miranda-Sune模型）应用不同电压脉冲策略。

Result: 研究发现，根据设备特性和约束，最优编程协议可能是单一固定电压脉冲或复杂连续波形。

Conclusion: 该研究为忆阻器件节能编程提供了重要理论和实践指导，解决了“电压-时间困境”。

Abstract: In many cases, the behavior of physical memristive devices can be relatively
well captured by using a single internal state variable. This study
investigates the low-power control of first-order memristive devices to derive
the most energy-efficient protocols for programming their resistances. A unique
yet general approach to optimizing the switching transitions in devices of this
kind is introduced. For pedagogical purposes, without loss of generality, the
proposed control paradigm is applied to a couple of differential algebraic
equation sets for voltage-controlled devices, specifically Kvatinsky's Voltage
ThrEshold Adaptive Memristor mathematical description and Miranda's and Sune's
dynamic balance model. It is demonstrated that, depending upon intrinsic
physical properties of the device, captured in the model formulas and parameter
setting, and upon constraints on programming time and voltages, the optimal
protocol for either of the two switching scenarios may require the application
of a single square voltage pulse of height set to a certain level within the
admissible range across a fraction or entire given programming time interval,
or of some more involved voltage stimulus of unique polarity, including
analogue continuous waveforms that can be approximated by trains of square
voltage pulses of different heights, over the entire programming time interval.
The practical implications of these research findings are significant, as the
development of energy-efficient protocols to program memristive devices,
resolving the so-called voltage-time dilemma in the device physics community,
is a subject under intensive and extensive studies across the academic
community and industry.

</details>


### [46] [Exploration of Evolving Quantum Key Distribution Network Architecture Using Model-Based Systems Engineering](https://arxiv.org/abs/2508.15733)
*Hayato Ishida,Amal Elsokary,Maria Aslam,Catherine White,Michael J. de C. Henshaw,Siyuan Ji*

Main category: cs.ET

TL;DR: 本文探讨了量子通信网络，特别是量子密钥分发网络架构的演进，提出了一个基于可变性建模的框架，以管理快速发展的网络架构并满足利益相关者的期望。


<details>
  <summary>Details</summary>
Motivation: 随着量子技术的进步，量子设备需要集成到经典基础设施中。量子计算的成熟对加密技术构成威胁，因此需要量子安全通信。

Method: 利用正交可变性建模和系统建模语言，研究量子密钥分发网络架构的演进，并开发可追踪的模块化架构。

Result: 提出了一个可变性驱动的框架，支持量子密钥分发网络的系统化开发，并有助于解决量子系统工程中的类似集成挑战。

Conclusion: 该研究为量子密钥分发网络的可行性和系统性开发提供了支持，并为量子系统工程的集成问题提供了解决方案。

Abstract: Realisation of significant advances in capabilities of sensors, computing,
timing, and communication enabled by quantum technologies is dependent on
engineering highly complex systems that integrate quantum devices into existing
classical infrastructure. A systems engineering approach is considered to
address the growing need for quantum-secure telecommunications that overcome
the threat to encryption caused by maturing quantum computation. This work
explores a range of existing and future quantum communication networks,
specifically quantum key distribution network proposals, to model and
demonstrate the evolution of quantum key distribution network architectures.
Leveraging Orthogonal Variability Modelling and Systems Modelling Language as
candidate modelling languages, the study creates traceable artefacts to promote
modular architectures that are reusable for future studies. We propose a
variability-driven framework for managing fast-evolving network architectures
with respect to increasing stakeholder expectations. The result contributes to
the systematic development of viable quantum key distribution networks and
supports the investigation of similar integration challenges relevant to the
broader context of quantum systems engineering.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [47] [Declarative Data Pipeline for Large Scale ML Services](https://arxiv.org/abs/2508.15105)
*Yunzhao Yang,Runhui Wang,Xuanqing Liu,Adit Krishnan,Yefan Tao,Yuqian Deng,Kuangyou Yao,Peiyuan Sun,Henrik Johnson,Aditi sinha,Davor Golac,Gerald Friedland,Usman Shakeel,Daryl Cooke,Joe Sullivan,Chris Kong*

Main category: cs.DC

TL;DR: 本文提出了一种新的'声明式数据管道'架构，通过模块化设计和标准化接口，在Apache Spark中高效集成机器学习功能，显著提升了开发效率和系统性能。


<details>
  <summary>Details</summary>
Motivation: 现代分布式数据处理系统在平衡系统性能与代码可维护性及开发效率方面面临挑战，特别是在大规模协作环境中。本文旨在解决这些问题。

Method: 提出了一种模块化框架，通过逻辑计算单元（Pipes）代替传统的微服务方法，实现了机器学习与Apache Spark的无缝集成。

Result: 企业案例显示开发效率提升50%，协作/故障排除时间从周缩短到天，性能提升500倍（可扩展性）和10倍（吞吐量）。学术实验证明吞吐量至少快5.7倍且CPU利用率达99%。

Conclusion: 通过架构决策和性能优化，本文为构建可扩展且易维护的数据处理系统提供了解决方案，平衡了系统性能和开发效率。

Abstract: Modern distributed data processing systems face significant challenges in
balancing system performance with code maintainability and developer
productivity, particularly when integrating machine learning capabilities at
scale. In large collaborative environments, these challenges are amplified by
high communication overhead between teams and the complexity of coordinating
development across multiple groups. This paper presents a novel "Declarative
Data Pipeline" architecture that addresses these challenges while processing
billions of records with high accuracy and efficiency. Our architecture
introduces a modular framework that seamlessly integrates machine learning
capabilities within Apache Spark by combining logical computation units that we
refer as Pipes, departing from traditional microservice-based approaches. By
establishing clear component boundaries and standardized interfaces, we achieve
both modularity and system optimization without sacrificing maintainability.
The enterprise case study demonstrate substantial improvements in multiple
dimensions: development efficiency improved by 50%,
collaboration/troubleshooting efforts compressed from weeks to days,
performance improved by 500x in scalability and by 10x in throughput. The
academic experiment also proves at least 5.7x faster in throughput with 99% CPU
utilization than non-framework implementations. This paper details the
architectural decisions, implementation strategies, and performance
optimizations that enable these improvements, providing insights for building
scalable, maintainable data processing systems that effectively balance system
performance with development velocity.

</details>


### [48] [Databelt: A Continuous Data Path for Serverless Workflows in the 3D Compute Continuum](https://arxiv.org/abs/2508.15351)
*Cynthia Marcelino,Leonard Guelmino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Databelt是一个针对3D计算连续体动态环境设计的无服务器工作流状态管理框架，通过SLO感知的状态传播和函数状态融合机制，显著降低延迟并提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决无服务器函数在动态网络环境（如3D计算连续体）中因频繁网络拓扑变化导致的高延迟和不必要数据传输问题。

Method: 1. SLO感知状态传播机制，主动卸载函数状态到最合适的节点；2. 函数状态融合机制，将共享同一运行时的函数状态抽象为组管理。

Result: 实验结果显示，Databelt将工作流执行时间减少66%，吞吐量提升50%，存储操作延迟降低20%。

Conclusion: Databelt通过动态状态管理和融合机制，显著优化了无服务器工作流在动态网络环境中的性能。

Abstract: Typically, serverless functions rely on remote storage services for managing
state, which can result in increased latency and network communication
overhead. In a dynamic environment such as the 3D (Edge-Cloud-Space) Compute
Continuum, serverless functions face additional challenges due to frequent
changes in network topology. As satellites move in and out of the range of
ground stations, functions must make multiple hops to access cloud services,
leading to high-latency state access and unnecessary data transfers. In this
paper, we present Databelt, a state management framework for serverless
workflows designed for the dynamic environment of the 3D Compute Continuum.
Databelt introduces an SLO-aware state propagation mechanism that enables the
function state to move continuously in orbit. Databelt proactively offloads
function states to the most suitable node, such that when functions execute,
the data is already present on the execution node or nearby, thus minimizing
state access latency and reducing the number of network hops. Additionally,
Databelt introduces a function state fusion mechanism that abstracts state
management for functions sharing the same serverless runtime. When functions
are fused, Databelt seamlessly retrieves their state as a group, reducing
redundant network and storage operations and improving overall workflow
efficiency. Our experimental results show that Databelt reduces workflow
execution time by up to 66% and increases throughput by 50% compared to the
baselines. Furthermore, our results show that Databelt function state fusion
reduces storage operations latency by up to 20%, by reducing repetitive storage
requests for functions within the same runtime, ensuring efficient execution of
serverless workflows in highly dynamic network environments such as the 3D
Continuum.

</details>


### [49] [Efficient Mixed-Precision Large Language Model Inference with TurboMind](https://arxiv.org/abs/2508.15601)
*Li Zhang,Youhe Jiang,Guoliang He,Xin Chen,Han Lv,Qian Yao,Fangcheng Fu,Kai Chen*

Main category: cs.DC

TL;DR: 该论文提出了一种混合精度LLM推理技术，通过优化内存和计算资源，显著提升了模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 为了降低大型语言模型的内存和计算需求，研究混合精度推理技术，优化硬件利用效率。

Method: 设计了两个新管道：GEMM管道和注意力管道，支持离线权重打包与在线加速，以及任意精度组合的高效注意力计算。

Result: 实验表明，该方法在16种LLM和4种GPU架构上，平均降低30%延迟，提升58%吞吐量。

Conclusion: 该技术已集成到开源项目TurboMind中，为混合精度推理提供了高效解决方案。

Abstract: Mixed-precision inference techniques reduce the memory and computational
demands of Large Language Models (LLMs) by applying hybrid precision formats to
model weights, activations, and KV caches. This work introduces mixed-precision
LLM inference techniques that encompass (i) systematic memory and compute
optimization across hierarchical storage and tensor core architectures, and
(ii) comprehensive end-to-end mixed-precision optimization across diverse
precision formats and hardware configurations. Our approach features two novel
mixed-precision pipelines designed for optimal hardware utilization: a General
Matrix Multiply (GEMM) pipeline that optimizes matrix operations through
offline weight packing and online acceleration, and an attention pipeline that
enables efficient attention computation with arbitrary Query, Key, and Value
precision combinations. The key implementation of the pipelines includes (i)
hardware-aware weight packing for automatic format optimization, (ii) adaptive
head alignment for efficient attention computation, (iii) instruction-level
parallelism for memory hierarchy exploitation, and (iv) KV memory loading
pipeline for enhanced inference efficiency. We conduct comprehensive
evaluations across 16 popular LLMs and 4 representative GPU architectures.
Results demonstrate that our approach achieves up to 61% lower serving latency
(30% on average) and up to 156% higher throughput (58% on average) in
mixed-precision workloads compared to existing mixed-precision frameworks,
establishing consistent performance improvements across all tested
configurations and hardware types. This work is integrated into TurboMind, a
high-performance inference engine of the LMDeploy project, which is
open-sourced and publicly available at https://github.com/InternLM/lmdeploy.

</details>


### [50] [Universal Dancing by Luminous Robots under Sequential Schedulers](https://arxiv.org/abs/2508.15484)
*Caterina Feletti,Paola Flocchini,Debasish Pattanayak,Giuseppe Prencipe,Nicola Santoro*

Main category: cs.DC

TL;DR: 该论文提出了一种在LUMI模型和顺序调度器下解决通用舞蹈问题的方法，放宽了现有研究中对模式和初始配置的限制条件。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决现有舞蹈问题中对模式和初始配置的严格限制，提出一种更通用的解决方案。

Method: 论文采用了LUMI模型和顺序调度器，利用机器人携带的光信号（颜色）和分布式计数机制来解决问题。

Result: 结果表明，该方法能够解决通用舞蹈问题，且在非刚性运动下保持空间同质性。

Conclusion: 结论表明，LUMI模型和顺序调度器能够放宽现有舞蹈问题的限制条件，为通用舞蹈问题提供解决方案。

Abstract: The Dancing problem requires a swarm of $n$ autonomous mobile robots to form
a sequence of patterns, aka perform a choreography. Existing work has proven
that some crucial restrictions on choreographies and initial configurations
(e.g., on repetitions of patterns, periodicity, symmetries,
contractions/expansions) must hold so that the Dancing problem can be solved
under certain robot models. Here, we prove that these necessary constraints can
be dropped by considering the LUMI model (i.e., where robots are endowed with a
light whose color can be chosen from a constant-size palette) under the quite
unexplored sequential scheduler. We formalize the class of Universal Dancing
problems which require a swarm of $n$ robots starting from any initial
configuration to perform a (periodic or finite) sequence of arbitrary patterns,
only provided that each pattern consists of $n$ vertices (including
multiplicities). However, we prove that, to be solvable under LUMI, the length
of the feasible choreographies is bounded by the compositions of $n$ into the
number of colors available to the robots. We provide an algorithm solving the
Universal Dancing problem by exploiting the peculiar capability of sequential
robots to implement a distributed counter mechanism. Even assuming non-rigid
movements, our algorithm ensures spatial homogeneity of the performed
choreography.

</details>


### [51] [Lower Bounds for $k$-Set Agreement in Fault-Prone Networks](https://arxiv.org/abs/2508.15562)
*Pierre Fraigniaud,Minh Hang Nguyen,Ami Paz,Ulrich Schmid,Hugo Rincon Galeana*

Main category: cs.DC

TL;DR: 该论文提出了一种针对同步消息传递系统中k-set协议的新下界，适用于任意有向通信网络，并结合拓扑方法扩展了现有理论。


<details>
  <summary>Details</summary>
Motivation: 旨在将已有的k-set协议下界理论推广到更一般的网络模型，包括有向通信网络和不同故障模型。

Method: 采用拓扑证明方法，通过Shellable carrier maps和Sperner引理分析协议复杂性，并结合新的carrier map维持高连通性。

Result: 论文不仅扩展了现有下界，还提出了基于通信图半径的附加协议开销，并证明可以使用更小的输入复形。

Conclusion: 研究为k-set协议在更广泛网络环境中的性能提供了理论支持，并简化了部分复杂性分析。

Abstract: We develop a new lower bound for k-set agreement in synchronous
message-passing systems connected by an arbitrary directed communication
network, where up to t processes may crash. Our result thus generalizes the
t/k+1 lower bound for complete networks in the t-resilient model by Chaudhuri,
Herlihy, Lynch, and Tuttle [JACM'00]. Moreover, it generalizes two lower bounds
for oblivious algorithms in synchronous systems connected by an arbitrary
undirected communication network known to the processes, namely, the domination
number-based lower bound by Castaneda, Fraigniaud, Paz, Rajsbaum, Roy, and
Travers [TCS'21] for failure-free processes, and the radius-based lower bound
in the t-resilient model by Fraigniaud, Nguyen, and Paz [STACS'24].
  Our topological proof non-trivially generalizes and extends the
connectivity-based approach for the complete network, as presented in the book
by Herlihy, Kozlov, and Rajsbaum (2013). It is based on a sequence of shellable
carrier maps that, starting from a shellable input complex, determine the
evolution of the protocol complex: During the first t/k rounds, carrier maps
that crash exactly k processes per round are used, ensuring high connectivity
of their images. A Sperner's lemma style argument is used to prove that k-set
agreement is still impossible by that round. From round t/k+1 up to our lower
bound, we employ a novel carrier map that maintains high connectivity. Our
proof also provides a strikingly simple lower bound for k-set agreement in
synchronous systems with an arbitrary communication network with initial
crashes. We express the resulting additional agreement overhead via an
appropriately defined radius of the communication graphs. Finally, we prove
that the usual input pseudosphere complex for k-set agreement can be replaced
by an exponentially smaller input complex based on Kuhn triangulations, which
we prove to be also shellable.

</details>


### [52] [CausalMesh: A Formally Verified Causal Cache for Stateful Serverless Computing](https://arxiv.org/abs/2508.15647)
*Haoran Zhang,Zihao Zhang,Shuai Mu,Sebastian Angel,Vincent Liu*

Main category: cs.DC

TL;DR: 提出了一种名为CausalMesh的新方法，用于在服务器无状态环境中实现因果一致性的缓存，支持无协调和无中止的读写操作。


<details>
  <summary>Details</summary>
Motivation: 解决在多节点服务器无状态环境中，由于缓存不一致导致的异常问题。

Method: 设计了CausalMesh缓存系统，支持因果一致性的读写操作和事务处理，适用于客户端在多服务器间迁移的场景。

Result: 实验表明，CausalMesh在延迟和吞吐量上优于现有方案，且其协议已通过Dafny形式化验证。

Conclusion: CausalMesh为服务器无状态环境提供了一种高效且可靠的缓存解决方案。

Abstract: Stateful serverless workflows consist of multiple serverless functions that
access state on a remote database. Developers sometimes add a cache layer
between the serverless runtime and the database to improve I/O latency.
However, in a serverless environment, functions in the same workflow may be
scheduled to different nodes with different caches, which can cause
non-intuitive anomalies. This paper presents CausalMesh, a novel approach to
causally consistent caching in environments where a computation may migrate
from one machine to another, such as in serverless computing. CausalMesh is the
first cache system that supports coordination-free and abort-free read/write
operations and read transactions when clients roam among multiple servers.
CausalMesh also supports read-write transactional causal consistency in the
presence of client roaming, but at the cost of abort-freedom.
  We have formally verified CausalMesh's protocol in Dafny, and our
experimental evaluation shows that CausalMesh has lower latency and higher
throughput than existing proposals

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [53] [Random Sampling over Spatial Range Joins](https://arxiv.org/abs/2508.15070)
*Daichi Amagata*

Main category: cs.DB

TL;DR: 论文提出了一种高效的空间范围连接随机采样算法，解决了传统连接计算成本高和结果集过大的问题。


<details>
  <summary>Details</summary>
Motivation: 空间范围连接在许多领域有广泛应用，但其计算成本高且结果集大，因此需要一种高效的方法来随机采样连接结果。

Method: 首先设计了两种基于现有随机采样技术的基线算法，发现其效率不足；随后提出了一种新的数据结构，能够在预期时间和空间内完成任务。

Result: 实验证明，新算法在多数测试中显著快于基线方法。

Conclusion: 新算法在时间和空间效率上表现优异，适用于需要大规模空间范围连接随机采样的场景。

Abstract: Spatial range joins have many applications, including geographic information
systems, location-based social networking services, neuroscience, and
visualization. However, joins incur not only expensive computational costs but
also too large result sets. A practical and reasonable approach to alleviating
these issues is to return random samples of the join results. Although this is
promising and sufficient for many applications involving spatial range joins,
efficiently computing random samples is not trivial. This is because we must
obtain random join samples without running spatial range joins. We address this
challenging problem for the first time and aim at designing a time- and
space-efficient algorithm. First, we design two baseline algorithms that employ
existing techniques for random sampling and show that they are not efficient.
Then, we propose a new data structure that can deal with our problem in
$\tilde{O}(n + m + t)$ expected time and $O(n+m)$ space, where $n$ and $m$ are
the sizes of two point sets and $t$ is the required number of samples. We
conduct extensive experiments using four real spatial datasets, and the results
demonstrate that our algorithm is significantly faster than the baselines in
most tests.

</details>


### [54] [Temporal $k$-Core Query, Revisited](https://arxiv.org/abs/2508.15238)
*Yinyu Liu,Kaiqiang Yu,Shengxin Liu,Cheng Long,Zhaoquan Gu*

Main category: cs.DB

TL;DR: CoreT算法通过动态记录顶点或边进入k核的最早时间戳，显著减少冗余计算，实现线性时间复杂度和高效的时间核查询。


<details>
  <summary>Details</summary>
Motivation: 理解动态网络（如社交平台、网页链接、通信网络）的结构需要高效的时序k核查询方法，现有算法OTCD因冗余计算和组合增长受限。

Method: CoreT算法动态记录每个顶点或边进入k核的最早时间戳，实现单次遍历查询区间并优化计算复杂度。

Result: 实验表明，CoreT比OTCD快四个数量级，适用于长期时序分析。

Conclusion: CoreT在时序k核查询中表现出高效和可扩展性。

Abstract: Querying cohesive subgraphs in temporal graphs is essential for understanding
the dynamic structure of real-world networks, such as evolving communities in
social platforms, shifting hyperlink structures on the Web, and transient
communication patterns in call networks. Recently, research has focused on the
temporal $k$-core query, which aims to identify all $k$-cores across all
possible time sub-intervals within a given query interval. The state-of-the-art
algorithm OTCD mitigates redundant computations over overlapping sub-intervals
by exploiting inclusion relationships among $k$-cores in different time
intervals. Nevertheless, OTCD remains limited in scalability due to the
combinatorial growth in interval enumeration and repeated processing. In this
paper, we revisit the temporal $k$-core query problem and introduce a novel
algorithm CoreT, which dynamically records the earliest timestamp at which each
vertex or edge enters a $k$-core. This strategy enables substantial pruning of
redundant computations. As a result, CoreT requires only a single pass over the
query interval and achieves improved time complexity, which is linear in both
the number of temporal edges within the query interval and the duration of the
interval, making it highly scalable for long-term temporal analysis.
Experimental results on large real-world datasets show that CoreT achieves up
to four orders of magnitude speedup compared to the existing state-of-the-art
OTCD, demonstrating its effectiveness and scalability for temporal $k$-core
analysis.

</details>


### [55] [AmbiSQL: Interactive Ambiguity Detection and Resolution for Text-to-SQL](https://arxiv.org/abs/2508.15276)
*Zhongjun Ding,Yin Lin,Tianjing Zeng*

Main category: cs.DB

TL;DR: AmbiSQL 是一个交互式系统，用于检测和解决 Text-to-SQL 中的查询歧义，通过用户反馈提高 SQL 生成准确性。


<details>
  <summary>Details</summary>
Motivation: LLM 在 Text-to-SQL 任务中存在误解用户意图的问题，查询歧义是主要障碍。AmbiSQL 旨在通过交互式方法解决这一问题。

Method: 提出细粒度歧义分类法，通过多选问题引导用户澄清意图，并利用反馈重写歧义问题。

Result: 在歧义检测上达到 87.2% 的精确度，SQL 生成准确率提升 50%。

Conclusion: AmbiSQL 显著提升了 Text-to-SQL 系统的性能，具有实用价值。

Abstract: Text-to-SQL systems translate natural language questions into SQL queries,
providing substantial value for non-expert users. While large language models
(LLMs) show promising results for this task, they remain error-prone. Query
ambiguity has been recognized as a major obstacle for LLM-based Text-to-SQL
systems, leading to misinterpretation of user intent and inaccurate SQL
generation. We demonstrate AmbiSQL, an interactive system that automatically
detects query ambiguities and guides users through intuitive multiple-choice
questions to clarify their intent. Our approach introduces a fine-grained
ambiguity taxonomy for identifying ambiguities that affect database element
mapping and LLM reasoning, then incorporates user feedback to rewrite ambiguous
questions. Evaluation on an ambiguous query dataset shows that AmbiSQL achieves
87.2% precision in ambiguity detection and improves SQL exact match accuracy by
50% when integrated with Text-to-SQL systems. Our demonstration showcases the
significant performance gains and highlights the system's practical usability.
Code repo and demonstration are available at:
https://github.com/JustinzjDing/AmbiSQL.

</details>


### [56] [Efficient Cloud-Edge-Device Query Execution Based on Collaborative Scan Operator](https://arxiv.org/abs/2508.15285)
*Chunyu Zhao,Hongzhi Wang,Kaixin Zhang,Hongliang Li,Yihan Zhang,Jiawei Zhang,Kunkai Gu,Yuan Tian,Xiangdong Huang,Jingyi Xu*

Main category: cs.DB

TL;DR: 本文提出了一种基于云边端协作框架的查询处理方法，通过协作扫描算子实现查询执行在云端和边缘之间的无缝切换，解决边缘资源瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 在云边端协作查询处理中，充分利用云计算的灵活性和边缘资源优势，但在查询执行过程中难以灵活切换协作算子。

Method: 建立基于协作扫描算子的云边端协作框架，实现在边缘资源饱和时随时将查询执行转移到云端。

Result: 实验表明，该方法能有效缓解边缘高I/O负载和CPU等待时间导致的扫描算子性能下降，并实现云边资源均衡调度。

Conclusion: 提出的协作扫描算子方法提升了查询性能，实现了云边端资源的高效协作。

Abstract: In cloud-edge-device (CED) collaborative query (CQ) processing, by leveraging
CED collaboration, the advantages of both cloud computing and edge resources
can be fully integrated. However, it is difficult to implement collaborative
operators that can flexibly switch between the cloud and the edge during query
execution. Thus, in this paper, we aim to improve the query performance when
the edge resources reach a bottleneck. To achieve seamless switching of query
execution between the cloud and edge, we propose a CQ processing method by
establishing a CED collaborative framework based on the collaborative scan
operator, so that query execution can be transferred to the cloud at any time
when the edge resources are saturated. Extensive experiments show that, under
sufficient network download bandwidth, the CED collaborative scan operator can
effectively alleviate the performance degradation of scan operators caused by
high I/O load and CPU wait time at the edge. It also achieves balanced resource
scheduling between the cloud and edge.

</details>


### [57] [Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional Vector Search](https://arxiv.org/abs/2508.15290)
*Peiqi Yin,Xiao Yan,Qihui Zhou,Hui Li,Xiaolu Li,Lin Zhang,Meiling Wang,Xin Yao,James Cheng*

Main category: cs.DB

TL;DR: 提出了一种优化向量搜索的系统Gorgeous，通过优先处理图结构而非向量数据，显著提升了查询吞吐量和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有系统在处理大规模向量数据集时，未能有效利用内存空间和磁盘访问局部性，导致性能瓶颈。

Method: 设计Gorgeous系统，采用内存缓存图结构的邻接表和优化的磁盘块格式，提升数据局部性。

Result: Gorgeous比现有系统平均查询吞吐量提升60%以上，延迟降低35%以上。

Conclusion: 通过优先处理图结构，Gorgeous显著优化了大规模向量搜索的性能。

Abstract: Similarity-based vector search underpins many important applications, but a
key challenge is processing massive vector datasets (e.g., in TBs). To reduce
costs, some systems utilize SSDs as the primary data storage. They employ a
proximity graph, which connects similar vectors to form a graph and is the
state-of-the-art index for vector search. However, these systems are hindered
by sub-optimal data layouts that fail to effectively utilize valuable memory
space to reduce disk access and suffer from poor locality for accessing
disk-resident data. Through extensive profiling and analysis, we found that the
structure of the proximity graph index is accessed more frequently than the
vectors themselves, yet existing systems do not distinguish between the two. To
address this problem, we design the Gorgeous system with the principle of
prioritizing graph structure over vectors. Specifically, Gorgeous features a
memory cache that keeps the adjacency lists of graph nodes to improve cache
hits and a disk block format that explicitly stores neighbors' adjacency lists
along with a vector to enhance data locality. Experimental results show that
Gorgeous consistently outperforms two state-of-the-art disk-based systems for
vector search, boosting average query throughput by over 60% and reducing query
latency by over 35%.

</details>


### [58] [GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector Nearest Neighbor Search](https://arxiv.org/abs/2508.15694)
*Yijie Zhou,Shengyuan Lin,Shufeng Gong,Song Yu,Shuhao Fan,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: GoVector提出了一种针对基于图的ANNS索引的I/O高效缓存策略，通过静态和动态缓存结合及存储布局优化，显著减少I/O操作并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有静态缓存策略在ANNS搜索的第二阶段效果不佳，导致I/O成为瓶颈，影响查询延迟和吞吐量。

Method: 结合静态缓存（预加载高频访问节点）和动态缓存（自适应捕获空间局部性高的节点），并优化存储布局以使相似向量在磁盘上相邻。

Result: 在90%召回率下，平均减少46%的I/O操作，查询吞吐量提升1.73倍，延迟降低42%。

Conclusion: GoVector为基于磁盘的图索引提供了一种高效的缓存策略，显著提升了ANNS的性能。

Abstract: Graph-based high-dimensional vector indices have become a mainstream solution
for large-scale approximate nearest neighbor search (ANNS). However, their
substantial memory footprint often requires storage on secondary devices, where
frequent on-demand loading of graph and vector data leads to I/O becoming the
dominant bottleneck, accounting for over 90\% of query latency. Existing static
caching strategies mitigate this issue only in the initial navigation phase by
preloading entry points and multi-hop neighbors, but they fail in the second
phase where query-dependent nodes must be dynamically accessed to achieve high
recall. We propose GoVector, an I/O-efficient caching strategy tailored for
disk-based graph indices. GoVector combines (1) a static cache that stores
entry points and frequently accessed neighbors, and (2) a dynamic cache that
adaptively captures nodes with high spatial locality during the second search
phase. To further align storage layout with similarity-driven search patterns,
GoVector reorders nodes on disk so that similar vectors are colocated on the
same or adjacent pages, thereby improving locality and reducing I/O overhead.
Extensive experiments on multiple public datasets show that GoVector achieves
substantial performance improvements. At 90% recall, it reduces I/O operations
by 46% on average, increases query throughput by 1.73x, and lowers query
latency by 42% compared to state-of-the-art disk-based graph indexing systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [59] [Accelerating GenAI Workloads by Enabling RISC-V Microkernel Support in IREE](https://arxiv.org/abs/2508.14899)
*Adeel Ahmad,Ahmad Tameem Kamal,Nouman Amir,Bilal Zafar,Saad Bin Nasir*

Main category: cs.AR

TL;DR: 该项目在IREE中实现了RISC-V微内核支持，优化了MLIR linalg dialect操作，并开发了针对RISC-V的微内核，性能与上游IREE和Llama.cpp对比。


<details>
  <summary>Details</summary>
Motivation: 为了在RISC-V架构上提升机器学习的编译和运行效率。

Method: 通过MLIR linalg dialect操作优化和开发RISC-V微内核。

Result: 性能与Llama-3.2-1B-Instruct模型的上游IREE和Llama.cpp进行了对比。

Conclusion: 项目成功实现了RISC-V微内核支持，并展示了性能提升潜力。

Abstract: This project enables RISC-V microkernel support in IREE, an MLIR-based
machine learning compiler and runtime. The approach begins by enabling the
lowering of MLIR linalg dialect contraction ops to linalg.mmt4d op for the
RISC-V64 target within the IREE pass pipeline, followed by the development of
optimized microkernels for RISC-V. The performance gains are compared with
upstream IREE and Llama.cpp for the Llama-3.2-1B-Instruct model.

</details>


### [60] [Improving Chip Design Enablement for Universities in Europe -- A Position Paper](https://arxiv.org/abs/2508.14907)
*Lukas Krupp,Ian O'Connor,Luca Benini,Christoph Studer,Joachim Rodrigues,Norbert Wehn*

Main category: cs.AR

TL;DR: 欧洲面临芯片设计能力不足的问题，本文探讨了大学和学术机构如何通过教育和研究来提升这一能力。


<details>
  <summary>Details</summary>
Motivation: 欧洲半导体行业经济重要，但芯片设计能力不足，尤其是人才短缺和技术落后。

Method: 综述欧洲现有芯片设计项目，分析招聘、生产力、技术获取和设计支持的挑战，并提出战略机会。

Result: 提出了一系列建议，强调协调努力和战略投资的重要性。

Conclusion: 需通过学术机构的协作和投资来提升欧洲的芯片设计能力。

Abstract: The semiconductor industry is pivotal to Europe's economy, especially within
the industrial and automotive sectors. However, Europe faces a significant
shortfall in chip design capabilities, marked by a severe skilled labor
shortage and lagging contributions in the design value chain segment. This
paper explores the role of European universities and academic initiatives in
enhancing chip design education and research to address these deficits. We
provide a comprehensive overview of current European chip design initiatives,
analyze major challenges in recruitment, productivity, technology access, and
design enablement, and identify strategic opportunities to strengthen chip
design capabilities within academic institutions. Our analysis leads to a
series of recommendations that highlight the need for coordinated efforts and
strategic investments to overcome these challenges.

</details>


### [61] [Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis](https://arxiv.org/abs/2508.14917)
*Weichien Liao*

Main category: cs.AR

TL;DR: 本文提出了一种基于FPGA的可扩展预处理流水线，用于实时去噪，适用于高吞吐量成像工作流。


<details>
  <summary>Details</summary>
Motivation: 高吞吐量成像工作流（如PRISM）生成的数据速率超出传统实时处理能力，需要低延迟的解决方案。

Method: 利用高级综合（HLS）实现的FPGA流水线，通过DRAM缓冲优化，直接对流式图像数据进行帧减法和平均操作，最小化延迟。

Result: 该内核操作时间低于帧间隔，支持实时去噪并减少下游CPU/GPU分析的数据集大小，已验证适用于PRISM规模采集。

Conclusion: 这种模块化FPGA框架为光谱学和显微镜中的延迟敏感成像工作流提供了实用解决方案。

Abstract: High-throughput imaging workflows, such as Parallel Rapid Imaging with
Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional
real-time processing capabilities. We present a scalable FPGA-based
preprocessing pipeline for real-time denoising, implemented via High-Level
Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture
performs frame subtraction and averaging directly on streamed image data,
minimizing latency through burst-mode AXI4 interfaces. The resulting kernel
operates below the inter-frame interval, enabling inline denoising and reducing
dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale
acquisition, this modular FPGA framework offers a practical solution for
latency-sensitive imaging workflows in spectroscopy and microscopy.

</details>


### [62] [Row-Column Hybrid Grouping for Fault-Resilient Multi-Bit Weight Representation on IMC Arrays](https://arxiv.org/abs/2508.15685)
*Kang Eun Jeon,Sangheum Yeon,Jinhee Kim,Hyeonsu Bang,Johnny Rhe,Jong Hwan Ko*

Main category: cs.AR

TL;DR: 提出了一种新型多比特权重表示技术和编译器流水线，解决了模拟内存计算系统中的计算不可靠性和高编译开销问题。


<details>
  <summary>Details</summary>
Motivation: 解决模拟内存计算系统中因 stuck-at faults 和现有故障缓解算法的高编译开销而限制其扩展和部署的问题。

Method: 1. 提出行-列混合分组的多比特权重表示技术；2. 设计基于整数线性规划（ILP）的编译器流水线。

Result: 在卷积网络和小语言模型中，实现了8%的准确率提升、150倍的编译速度提升和2倍的能效提升。

Conclusion: 通过结构冗余和快速编译方法，显著提高了系统的可靠性和效率。

Abstract: This paper addresses two critical challenges in analog In-Memory Computing
(IMC) systems that limit their scalability and deployability: the computational
unreliability caused by stuck-at faults (SAFs) and the high compilation
overhead of existing fault-mitigation algorithms, namely Fault-Free (FF). To
overcome these limitations, we first propose a novel multi-bit weight
representation technique, termed row-column hybrid grouping, which generalizes
conventional column grouping by introducing redundancy across both rows and
columns. This structural redundancy enhances fault tolerance and can be
effectively combined with existing fault-mitigation solutions. Second, we
design a compiler pipeline that reformulates the fault-aware weight
decomposition problem as an Integer Linear Programming (ILP) task, enabling
fast and scalable compilation through off-the-shelf solvers. Further
acceleration is achieved through theoretical insights that identify fault
patterns amenable to trivial solutions, significantly reducing computation.
Experimental results on convolutional networks and small language models
demonstrate the effectiveness of our approach, achieving up to 8%p improvement
in accuracy, 150x faster compilation, and 2x energy efficiency gain compared to
existing baselines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions](https://arxiv.org/abs/2508.15047)
*Yibo Liu,Liam Shatzel,Brandon Haworth,Teseo Schneider*

Main category: cs.AI

TL;DR: 提出了一种利用大语言模型（LLM）控制人群动画中代理运动的方法，结合对话系统和语言驱动导航，使代理能够基于社交互动和感知输入做出更自然的运动决策。


<details>
  <summary>Details</summary>
Motivation: 现有的人群模拟方法缺乏对社交和环境互动的语言维度考虑，导致代理间互动局限于简单的避障和目标导向。

Method: 设计了基于LLM的对话系统和语言驱动导航，利用代理的个性、情感状态、视觉和物理状态控制运动和对话生成。

Result: 在复杂场景中验证了方法的有效性，观察到代理自动分组和解组，且框架产生了更真实的人群模拟和自然涌现的群体行为。

Conclusion: 通过结合LLM，框架能够产生更自然和真实的人群模拟，社交互动和群体行为可以从环境设置中自然涌现。

Abstract: Animating and simulating crowds using an agent-based approach is a
well-established area where every agent in the crowd is individually controlled
such that global human-like behaviour emerges. We observe that human navigation
and movement in crowds are often influenced by complex social and environmental
interactions, driven mainly by language and dialogue. However, most existing
work does not consider these dimensions and leads to animations where
agent-agent and agent-environment interactions are largely limited to steering
and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to
control agents' movement. Our method has two main components: a dialogue system
and language-driven navigation. We periodically query agent-centric LLMs
conditioned on character personalities, roles, desires, and relationships to
control the generation of inter-agent dialogue when necessitated by the spatial
and social relationships with neighbouring agents. We then use the conversation
and each agent's personality, emotional state, vision, and physical state to
control the navigation and steering of each agent. Our model thus enables
agents to make motion decisions based on both their perceptual inputs and the
ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay
between social interactions, steering, and crowding. In these scenarios, we
observe that grouping and ungrouping of agents automatically occur.
Additionally, our experiments show that our method serves as an
information-passing mechanism within the crowd. As a result, our framework
produces more realistic crowd simulations, with emergent group behaviours
arising naturally from any environmental setting.

</details>


### [64] [Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle](https://arxiv.org/abs/2508.15680)
*Mark Cote,Susana Aires*

Main category: cs.AI

TL;DR: 本文通过技术哲学视角分析欧盟AI法案，探讨AI系统中数据的长期动态，提出了一种概念工具来理解AI生命周期中的动态生成过程及其监管盲点。


<details>
  <summary>Details</summary>
Motivation: 揭示AI数据生命周期中递归价值链对现有负责任AI框架的挑战，并指出政策制定中缺乏对AI技术和经济逻辑动态性的关注。

Method: 采用跨学科方法，结合Simondonian技术哲学，重新定义个体化概念，提出AI生命周期的形式化解读，并引入“未来性”概念。

Result: 识别了AI生命周期中基础设施和时间动态对权力不对称的影响，提出监管措施如生命周期审计、时间可追溯性等。

Conclusion: 有效监管需关注AI基础设施和时间动态，提出具体措施以应对技术寡头垄断和价值集中的问题。

Abstract: This paper argues that a techno-philosophical reading of the EU AI Act
provides insight into the long-term dynamics of data in AI systems,
specifically, how the lifecycle from ingestion to deployment generates
recursive value chains that challenge existing frameworks for Responsible AI.
We introduce a conceptual tool to frame the AI pipeline, spanning data,
training regimes, architectures, feature stores, and transfer learning. Using
cross-disciplinary methods, we develop a technically grounded and
philosophically coherent analysis of regulatory blind spots. Our central claim
is that what remains absent from policymaking is an account of the dynamic of
becoming that underpins both the technical operation and economic logic of AI.
To address this, we advance a formal reading of AI inspired by Simondonian
philosophy of technology, reworking his concept of individuation to model the
AI lifecycle, including the pre-individual milieu, individuation, and
individuated AI. To translate these ideas, we introduce futurity: the
self-reinforcing lifecycle of AI, where more data enhances performance, deepens
personalisation, and expands application domains. Futurity highlights the
recursively generative, non-rivalrous nature of data, underpinned by
infrastructures like feature stores that enable feedback, adaptation, and
temporal recursion. Our intervention foregrounds escalating power asymmetries,
particularly the tech oligarchy whose infrastructures of capture, training, and
deployment concentrate value and decision-making. We argue that effective
regulation must address these infrastructural and temporal dynamics, and
propose measures including lifecycle audits, temporal traceability, feedback
accountability, recursion transparency, and a right to contest recursive reuse.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [65] [A Practical Guideline and Taxonomy to LLVM's Control Flow Integrity](https://arxiv.org/abs/2508.15386)
*Sabine Houy,Bruno Kreyssig,Timothee Riom,Alexandre Bartel,Patrick McDaniel*

Main category: cs.CR

TL;DR: 本文分析了LLVM的前向边缘CFI变体对内存损坏漏洞的防御效果，并通过实际漏洞案例评估其有效性，为开发者提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 内存损坏漏洞是软件安全的重要威胁，但现有的CFI技术缺乏具体部署指导。本文旨在填补这一空白。

Method: 基于Top 10 KEV列表，选取四类代表性漏洞，评估LLVM的CFI变体对每类漏洞的防御效果。

Result: CFI在两类漏洞中成功阻止攻击，但在另外两类中失效，展示了其潜力与局限性。

Conclusion: 研究为开发者提供了CFI的实用部署建议，并为进一步改进CFI的实际应用奠定了基础。

Abstract: Memory corruption vulnerabilities remain one of the most severe threats to
software security. They often allow attackers to achieve arbitrary code
execution by redirecting a vulnerable program's control flow. While Control
Flow Integrity (CFI) has gained traction to mitigate this exploitation path,
developers are not provided with any direction on how to apply CFI to
real-world software. In this work, we establish a taxonomy mapping LLVM's
forward-edge CFI variants to memory corruption vulnerability classes, offering
actionable guidance for developers seeking to deploy CFI incrementally in
existing codebases. Based on the Top 10 Known Exploited Vulnerabilities (KEV)
list, we identify four high-impact vulnerability categories and select one
representative CVE for each. We evaluate LLVM's CFI against each CVE and
explain why CFI blocks exploitation in two cases while failing in the other
two, illustrating its potential and current limitations. Our findings support
informed deployment decisions and provide a foundation for improving the
practical use of CFI in production systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [66] [KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models](https://arxiv.org/abs/2508.15357)
*Haji Gul,Abul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.CL

TL;DR: 论文提出了一种统一的元度量EDAS，用于解决知识图谱补全（KGC）模型中多数据集和多评价指标下的评估不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的KGC模型评估方法在不同的数据集和评价指标下存在不一致性，导致模型选择困难。

Method: 提出基于距离平均解的评估方法（EDAS），将多数据集和多指标的性能综合为一个标准化分数。

Result: 在FB15k-237和WN18RR等基准数据集上，EDAS成功整合了多指标、多数据集的性能，提供了统一的排名。

Conclusion: EDAS提供了一种全局、一致且可解释的评估框架，有助于更公平和可靠地比较KGC模型。

Abstract: Knowledge Graphs (KGs) enable applications in various domains such as
semantic search, recommendation systems, and natural language processing. KGs
are often incomplete, missing entities and relations, an issue addressed by
Knowledge Graph Completion (KGC) methods that predict missing elements.
Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank
(MR), and Hit@k, are commonly used to assess the performance of such KGC
models. A major challenge in evaluating KGC models, however, lies in comparing
their performance across multiple datasets and metrics. A model may outperform
others on one dataset but underperform on another, making it difficult to
determine overall superiority. Moreover, even within a single dataset,
different metrics such as MRR and Hit@1 can yield conflicting rankings, where
one model excels in MRR while another performs better in Hit@1, further
complicating model selection for downstream tasks. These inconsistencies hinder
holistic comparisons and highlight the need for a unified meta-metric that
integrates performance across all metrics and datasets to enable a more
reliable and interpretable evaluation framework. To address this need, we
propose KG Evaluation based on Distance from Average Solution (EDAS), a robust
and interpretable meta-metric that synthesizes model performance across
multiple datasets and diverse evaluation criteria into a single normalized
score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated
aspects of performance, EDAS offers a global perspective that supports more
informed model selection and promotes fairness in cross-dataset evaluation.
Experimental results on benchmark datasets such as FB15k-237 and WN18RR
demonstrate that EDAS effectively integrates multi-metric, multi-dataset
performance into a unified ranking, offering a consistent, robust, and
generalizable framework for evaluating KGC models.

</details>


### [67] [SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version](https://arxiv.org/abs/2508.15478)
*Nghiem Thanh Pham,Tung Kieu,Duc-Manh Nguyen,Son Ha Xuan,Nghia Duong-Trung,Danh Le-Phuoc*

Main category: cs.CL

TL;DR: SLM-Bench是首个专门用于评估小型语言模型（SLM）的基准测试，涵盖准确性、计算效率和可持续性等多维度，填补了系统性评估SLM的空白。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对小型语言模型（SLM）性能和环境影响系统性评估的工具，SLM-Bench旨在填补这一空白。

Method: SLM-Bench对15个SLM在9个NLP任务上使用23个数据集进行测试，涵盖14个领域，并在4种硬件配置下评估11个指标。

Result: 研究发现不同SLM在准确性和能源效率上存在显著差异，部分模型在准确性上表现优异，而另一些则能源效率更高。

Conclusion: SLM-Bench为SLM评估设定了新标准，促进了资源效率与实际应用之间的平衡。

Abstract: Small Language Models (SLMs) offer computational efficiency and
accessibility, yet a systematic evaluation of their performance and
environmental impact remains lacking. We introduce SLM-Bench, the first
benchmark specifically designed to assess SLMs across multiple dimensions,
including accuracy, computational efficiency, and sustainability metrics.
SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14
domains. The evaluation is conducted on 4 hardware configurations, providing a
rigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench
quantifies 11 metrics across correctness, computation, and consumption,
enabling a holistic assessment of efficiency trade-offs. Our evaluation
considers controlled hardware conditions, ensuring fair comparisons across
models. We develop an open-source benchmarking pipeline with standardized
evaluation protocols to facilitate reproducibility and further research. Our
findings highlight the diverse trade-offs among SLMs, where some models excel
in accuracy while others achieve superior energy efficiency. SLM-Bench sets a
new standard for SLM evaluation, bridging the gap between resource efficiency
and real-world applicability.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [68] [Human Feedback Driven Dynamic Speech Emotion Recognition](https://arxiv.org/abs/2508.14920)
*Ilya Fedorov,Dmitry Korobchenko*

Main category: cs.SD

TL;DR: 提出了一种动态语音情感识别方法，关注3D虚拟角色的情感动画，采用多阶段训练和Dirichlet分布建模情感混合，实验结果验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索动态语音情感识别的新领域，特别关注3D虚拟角色的情感动画，解决传统方法中情感随时间变化的挑战。

Method: 1. 训练传统语音情感识别模型；2. 合成情感序列；3. 基于人类反馈优化模型；4. 提出基于Dirichlet分布的情感混合建模方法。

Result: Dirichlet分布方法在情感混合建模中表现优异，结合人类反馈进一步提升了模型质量并简化了标注流程。

Conclusion: 提出的动态语音情感识别方法在3D虚拟角色动画中有效，Dirichlet分布和人类反馈的结合显著提升了模型的性能。

Abstract: This work proposes to explore a new area of dynamic speech emotion
recognition. Unlike traditional methods, we assume that each audio track is
associated with a sequence of emotions active at different moments in time. The
study particularly focuses on the animation of emotional 3D avatars. We propose
a multi-stage method that includes the training of a classical speech emotion
recognition model, synthetic generation of emotional sequences, and further
model improvement based on human feedback. Additionally, we introduce a novel
approach to modeling emotional mixtures based on the Dirichlet distribution.
The models are evaluated based on ground-truth emotions extracted from a
dataset of 3D facial animations. We compare our models against the sliding
window approach. Our experimental results show the effectiveness of
Dirichlet-based approach in modeling emotional mixtures. Incorporating human
feedback further improves the model quality while providing a simplified
annotation procedure.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [69] [Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging](https://arxiv.org/abs/2508.14931)
*Zahra TehraniNasab,Amar Kumar,Tal Arbel*

Main category: eess.IV

TL;DR: 研究了在高分辨率（512x512像素）图像生成中，不同微调技术对生成质量的影响，并探讨了其在医学影像等应用中的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散基础模型在文本到图像生成方面取得了进展，但多限于低分辨率。高分辨率图像合成在医学影像等领域需求增加，需通过微调适应任务需求和数据分布。

Method: 系统评估多种微调方法（包括全微调和参数高效微调），分析其对FID、Vendi分数和提示-图像对齐等指标的影响，并测试生成图像在数据稀缺条件下的下游分类任务效用。

Result: 某些微调策略提高了生成图像的保真度，并改善了在合成图像用于分类器训练和真实图像评估时的下游性能。

Conclusion: 微调技术在高分辨率图像生成中具有重要作用，特定的微调策略能显著提升生成质量和下游任务表现。

Abstract: Advancements in diffusion-based foundation models have improved text-to-image
generation, yet most efforts have been limited to low-resolution settings. As
high-resolution image synthesis becomes increasingly essential for various
applications, particularly in medical imaging domains, fine-tuning emerges as a
crucial mechanism for adapting these powerful pre-trained models to
task-specific requirements and data distributions. In this work, we present a
systematic study, examining the impact of various fine-tuning techniques on
image generation quality when scaling to high resolution 512x512 pixels. We
benchmark a diverse set of fine-tuning methods, including full fine-tuning
strategies and parameter-efficient fine-tuning (PEFT). We dissect how different
fine-tuning methods influence key quality metrics, including Fr\'echet
Inception Distance (FID), Vendi score, and prompt-image alignment. We also
evaluate the utility of generated images in a downstream classification task
under data-scarce conditions, demonstrating that specific fine-tuning
strategies improve both generation fidelity and downstream performance when
synthetic images are used for classifier training and evaluation on real
images. Our code is accessible through the project website -
https://tehraninasab.github.io/PixelUPressure/.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [70] [Neural Robot Dynamics](https://arxiv.org/abs/2508.15755)
*Jie Xu,Eric Heiden,Iretiayo Akinola,Dieter Fox,Miles Macklin,Yashraj Narang*

Main category: cs.RO

TL;DR: 神经机器人动力学（NeRD）提出了一种通用的神经网络模拟器，用于预测机器人的未来状态，解决了现有神经模拟器泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现代机器人的高自由度和复杂机制使得模拟变得困难，现有神经模拟器通常需要特定训练且难以泛化到新任务或环境。

Method: NeRD通过机器人中心和空间不变的模拟状态表示，替代了传统模拟器中的低层动力学和接触求解器。

Result: 实验表明，NeRD模拟器稳定且准确，能够泛化到不同任务和环境配置，并可以通过现实数据进行微调。

Conclusion: NeRD为机器人动力学模拟提供了一种高效且可泛化的解决方案，能够缩小模拟与现实的差距。

Abstract: Accurate and efficient simulation of modern robots remains challenging due to
their high degrees of freedom and intricate mechanisms. Neural simulators have
emerged as a promising alternative to traditional analytical simulators,
capable of efficiently predicting complex dynamics and adapting to real-world
data; however, existing neural simulators typically require
application-specific training and fail to generalize to novel tasks and/or
environments, primarily due to inadequate representations of the global state.
In this work, we address the problem of learning generalizable neural
simulators for robots that are structured as articulated rigid bodies. We
propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models
for predicting future states for articulated rigid bodies under contact
constraints. NeRD uniquely replaces the low-level dynamics and contact solvers
in an analytical simulator and employs a robot-centric and spatially-invariant
simulation state representation. We integrate the learned NeRD models as an
interchangeable backend solver within a state-of-the-art robotics simulator. We
conduct extensive experiments to show that the NeRD simulators are stable and
accurate over a thousand simulation steps; generalize across tasks and
environment configurations; enable policy learning exclusively in a neural
engine; and, unlike most classical simulators, can be fine-tuned from
real-world data to bridge the gap between simulation and reality.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [71] [JEDI-linear: Fast and Efficient Graph Neural Networks for Jet Tagging on FPGAs](https://arxiv.org/abs/2508.15468)
*Zhiqiang Que,Chang Sun,Sudarshan Paramesvaran,Emyr Clement,Katerina Karakoulaki,Christopher Brown,Lauri Laatu,Arianna Cox,Alexander Tapper,Wayne Luk,Maria Spiropulu*

Main category: hep-ex

TL;DR: JEDI-linear是一种新型GNN架构，通过共享变换和全局聚合降低了计算复杂度，优化了FPGA部署，显著降低了延迟和资源使用。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在FPGA硬件触发器系统中部署时的高计算复杂性和不规则内存访问问题。

Method: 采用线性复杂度的GNN架构，结合量化感知训练和分布式算术，优化硬件效率。

Result: 相比现有设计，JEDI-linear实现了更低延迟（3.7-11.5倍）、更低的LUT使用（6.2倍）及更高准确性。

Conclusion: JEDI-linear满足了HL-LHC CMS Level-1触发系统的实时要求，推动了高效GNN推理的实际应用。

Abstract: Graph Neural Networks (GNNs), particularly Interaction Networks (INs), have
shown exceptional performance for jet tagging at the CERN High-Luminosity Large
Hadron Collider (HL-LHC). However, their computational complexity and irregular
memory access patterns pose significant challenges for deployment on FPGAs in
hardware trigger systems, where strict latency and resource constraints apply.
In this work, we propose JEDI-linear, a novel GNN architecture with linear
computational complexity that eliminates explicit pairwise interactions by
leveraging shared transformations and global aggregation. To further enhance
hardware efficiency, we introduce fine-grained quantization-aware training with
per-parameter bitwidth optimization and employ multiplier-free
multiply-accumulate operations via distributed arithmetic. Evaluation results
show that our FPGA-based JEDI-linear achieves 3.7 to 11.5 times lower latency,
up to 150 times lower initiation interval, and up to 6.2 times lower LUT usage
compared to state-of-the-art designs while also delivering higher model
accuracy and eliminating the need for DSP blocks entirely. In contrast,
state-of-the-art solutions consume over 8,700 DSPs. This is the first
interaction-based GNN to achieve less than 60~ns latency and currently meets
the requirements for use in the HL-LHC CMS Level-1 trigger system. This work
advances the next-generation trigger systems by enabling accurate, scalable,
and resource-efficient GNN inference in real-time environments. Our
open-sourced templates will further support reproducibility and broader
adoption across scientific applications.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [72] [Transition-based vs stated-based acceptance for automata over infinite words](https://arxiv.org/abs/2508.15402)
*Antonio Casares*

Main category: cs.FL

TL;DR: 该论文调查了在无限字自动机中将接受条件从基于状态转向基于转移的趋势，并分析了这种选择对问题解决的影响。


<details>
  <summary>Details</summary>
Motivation: 传统上，无限对象自动机的接受条件基于状态，但近年来有转向基于转移的趋势。本文旨在探讨这种转变的原因及其影响。

Method: 通过调查和分析，比较基于状态和基于转移的接受条件在无限字自动机中的应用和效果。

Result: 基于转移的接受条件在某些问题中表现出显著优势，选择接受条件的形式对自动机的表现有重要影响。

Conclusion: 在无限字自动机中，基于转移的接受条件更具灵活性和效率，值得推广和进一步研究。

Abstract: Automata over infinite objects are a well-established model with applications
in logic and formal verification. Traditionally, acceptance in such automata is
defined based on the set of states visited infinitely often during a run.
However, there is a growing trend towards defining acceptance based on
transitions rather than states.
  In this survey, we analyse the reasons for this shift and advocate using
transition-based acceptance in the context of automata over infinite words. We
present a collection of problems where the choice of formalism has a major
impact and discuss the causes of these differences.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [73] [Image-Conditioned 3D Gaussian Splat Quantization](https://arxiv.org/abs/2508.15372)
*Xinshuang Liu,Runfa Blark Li,Keito Suzuki,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种图像条件化的高斯溅射量化器（ICGS-Quantizer），显著提升了3D高斯溅射（3DGS）的压缩效率，并支持存档后的场景变化适应。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法在处理大规模场景或长期存档后的场景变化时存在不足，限制了其实际应用。

Method: 通过联合利用高斯间和属性间的相关性，以及跨场景共享码本，ICGS-Quantizer提升了量化效率；同时通过图像条件化的解码机制适应场景变化。

Result: ICGS-Quantizer将3DGS的存储需求降低至千字节级别，并在压缩效率和场景变化适应性上优于现有方法。

Conclusion: ICGS-Quantizer为3DGS的高效压缩和长期存档提供了可行的解决方案，具有广泛的应用潜力。

Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for
enabling high-quality real-time rendering. Although 3DGS compression methods
have been proposed for deployment on storage-constrained devices, two
limitations hinder archival use: (1) they compress medium-scale scenes only to
the megabyte range, which remains impractical for large-scale scenes or
extensive scene collections; and (2) they lack mechanisms to accommodate scene
changes after long-term archival. To address these limitations, we propose an
Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially
enhances compression efficiency and provides adaptability to scene changes
after archiving. ICGS-Quantizer improves quantization efficiency by jointly
exploiting inter-Gaussian and inter-attribute correlations and by using shared
codebooks across all training scenes, which are then fixed and applied to
previously unseen test scenes, eliminating the overhead of per-scene codebooks.
This approach effectively reduces the storage requirements for 3DGS to the
kilobyte range while preserving visual fidelity. To enable adaptability to
post-archival scene changes, ICGS-Quantizer conditions scene decoding on images
captured at decoding time. The encoding, quantization, and decoding processes
are trained jointly, ensuring that the codes, which are quantized
representations of the scene, are effective for conditional decoding. We
evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating.
Experimental results show that ICGS-Quantizer consistently outperforms
state-of-the-art methods in compression efficiency and adaptability to scene
changes. Our code, model, and data will be publicly available on GitHub.

</details>


### [74] [Scaling Group Inference for Diverse and High-Quality Generation](https://arxiv.org/abs/2508.15773)
*Gaurav Parmar,Or Patashnik,Daniil Ostashev,Kuan-Chieh Wang,Kfir Aberman,Srinivasa Narasimhan,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出一种可扩展的群体推理方法，提升生成样本的多样性与质量，解决独立采样导致的冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，用户通常会收到多张图像（如4-8张），独立采样导致结果冗余，限制了用户选择和创意探索。

Method: 将群体推理建模为二次整数分配问题，候选输出作为图节点，优化样本质量同时最大化群体多样性，并通过渐进剪枝提升效率。

Result: 实验表明，该方法显著提升群体多样性和质量，适用于文本到图像、图像到图像、图像提示和视频生成等多种任务。

Conclusion: 该方法使生成模型能将多个输出视为紧密群体而非独立样本，提升实用性和用户体验。

Abstract: Generative models typically sample outputs independently, and recent
inference-time guidance and scaling algorithms focus on improving the quality
of individual samples. However, in real-world applications, users are often
presented with a set of multiple images (e.g., 4-8) for each prompt, where
independent sampling tends to lead to redundant results, limiting user choices
and hindering idea exploration. In this work, we introduce a scalable group
inference method that improves both the diversity and quality of a group of
samples. We formulate group inference as a quadratic integer assignment
problem: candidate outputs are modeled as graph nodes, and a subset is selected
to optimize sample quality (unary term) while maximizing group diversity
(binary term). To substantially improve runtime efficiency, we progressively
prune the candidate set using intermediate predictions, allowing our method to
scale up to large candidate sets. Extensive experiments show that our method
significantly improves group diversity and quality compared to independent
sampling baselines and recent inference algorithms. Our framework generalizes
across a wide range of tasks, including text-to-image, image-to-image, image
prompting, and video generation, enabling generative models to treat multiple
outputs as cohesive groups rather than independent samples.

</details>


### [75] [Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments](https://arxiv.org/abs/2508.15158)
*Md. Nurul Absur,Abhinav Kumar,Swastik Brahma,Saptarshi Debroy*

Main category: cs.CV

TL;DR: 论文提出了一种基于投资组合理论的边缘资源管理策略，用于在多视图3D重建中应对可能的系统中断，保证重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决动态边缘环境中因相机操作中断导致的多视图3D重建质量持续下降问题。

Method: 采用投资组合理论启发的资源管理策略，并通过遗传算法快速求解优化问题。

Result: 实验表明，该相机选择策略在时空相关中断下优于传统基线方法，能可靠保证3D重建质量。

Conclusion: 所提方法有效解决了多视图3D重建中的可靠性问题，适用于动态边缘环境。

Abstract: Multi-view 3D reconstruction applications are revolutionizing critical use
cases that require rapid situational-awareness, such as emergency response,
tactical scenarios, and public safety. In many cases, their near-real-time
latency requirements and ad-hoc needs for compute resources necessitate
adoption of `Just-in-time' edge environments where the system is set up on the
fly to support the applications during the mission lifetime. However,
reliability issues can arise from the inherent dynamism and operational
adversities of such edge environments, resulting in spatiotemporally correlated
disruptions that impact the camera operations, which can lead to sustained
degradation of reconstruction quality. In this paper, we propose a novel
portfolio theory inspired edge resource management strategy for reliable
multi-view 3D reconstruction against possible system disruptions. Our proposed
methodology can guarantee reconstruction quality satisfaction even when the
cameras are prone to spatiotemporally correlated disruptions. The portfolio
theoretic optimization problem is solved using a genetic algorithm that
converges quickly for realistic system settings. Using publicly available and
customized 3D datasets, we demonstrate the proposed camera selection strategy's
benefits in guaranteeing reliable 3D reconstruction against traditional
baseline strategies, under spatiotemporal disruptions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [76] [HEAS: Hierarchical Evolutionary Agent Simulation Framework for Cross-Scale Modeling and Multi-Objective Search](https://arxiv.org/abs/2508.15555)
*Ruiyu Zhang,Lin Nie,Xin Zhao*

Main category: cs.MA

TL;DR: HEAS是一个Python框架，结合了分层代理建模、进化优化和比赛评估，提供统一且可重复的工作流。


<details>
  <summary>Details</summary>
Motivation: 解决跨尺度建模中的耦合问题，提高可审计性和可重复性。

Method: 采用分层轻量级进程（"streams"）和共享上下文，支持单目标/多目标进化、PyTorch策略集成及用户定义的比赛规则。

Result: 提供标准化评估指标、存档功能和可视化工具，减少代码重复并提高研究间的可比性。

Conclusion: HEAS为跨学科、多层级研究提供了实用基础，生成可靠且可重复的结果。

Abstract: Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that
unifies layered agent-based modeling with evolutionary optimization and
tournament evaluation in a single, reproducible workflow. HEAS represents
models as hierarchies of lightweight processes ("streams") scheduled in
deterministic layers that read and write a shared context, making cross-scale
couplings explicit and auditable. A compact API and CLI-simulate, optimize,
evaluate-expose single- and multi-objective evolution, PyTorch policy
integration via parameter flattening/unflattening, and general tournament
tooling with user-defined scoring and voting rules. The framework standardizes
evaluation through uniform per-step and episode metrics, persists seeds,
logbooks, and hall-of-fame archives, and provides plotting helpers for traces,
Pareto fronts, and comparative outcomes, reducing glue code and improving
comparability across studies. HEAS emphasizes separation of mechanism from
orchestration, allowing exogenous drivers, endogenous agents, and aggregators
to be composed and swapped without refactoring, while the same model can be
used for forward simulation, optimization, or systematic comparison. We
illustrate usage with two compact examples-an ecological system and an
enterprise decision-making setting. HEAS offers a practical foundation for
cross-disciplinary, multi-level inquiry, yielding reliable, reproducible
results.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [Locally Pareto-Optimal Interpretations for Black-Box Machine Learning Models](https://arxiv.org/abs/2508.15220)
*Aniruddha Joshi,Supratik Chakraborty,S Akshay,Shetal Shah,Hazem Torfah,Sanjit Seshia*

Main category: cs.LG

TL;DR: 论文提出了一种基于局部最优性保证的框架，用于平衡机器学习模型的准确性和可解释性，解决现有方法在Pareto最优空间探索中的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 在解释黑盒机器学习模型时，准确性和可解释性常存在冲突，现有方法缺乏对Pareto最优性的正式保证或面临可扩展性限制。

Method: 结合多目标学习或搜索技术（如多目标蒙特卡洛树搜索）生成候选解，并通过SAT求解器验证其局部最优性。

Result: 实验证明，该方法生成的解释与提供全局保证的方法结果相近，同时更具可扩展性。

Conclusion: 该框架为高效合成Pareto最优解释提供了可行方案，兼具局部最优性保证和可扩展性。

Abstract: Creating meaningful interpretations for black-box machine learning models
involves balancing two often conflicting objectives: accuracy and
explainability. Exploring the trade-off between these objectives is essential
for developing trustworthy interpretations. While many techniques for
multi-objective interpretation synthesis have been developed, they typically
lack formal guarantees on the Pareto-optimality of the results. Methods that do
provide such guarantees, on the other hand, often face severe scalability
limitations when exploring the Pareto-optimal space. To address this, we
develop a framework based on local optimality guarantees that enables more
scalable synthesis of interpretations. Specifically, we consider the problem of
synthesizing a set of Pareto-optimal interpretations with local optimality
guarantees, within the immediate neighborhood of each solution. Our approach
begins with a multi-objective learning or search technique, such as
Multi-Objective Monte Carlo Tree Search, to generate a best-effort set of
Pareto-optimal candidates with respect to accuracy and explainability. We then
verify local optimality for each candidate as a Boolean satisfiability problem,
which we solve using a SAT solver. We demonstrate the efficacy of our approach
on a set of benchmarks, comparing it against previous methods for exploring the
Pareto-optimal front of interpretations. In particular, we show that our
approach yields interpretations that closely match those synthesized by methods
offering global guarantees.

</details>


### [78] [Mini-Batch Robustness Verification of Deep Neural Networks](https://arxiv.org/abs/2508.15454)
*Saar Tzour-Shaday,Dana Drachsler Cohen*

Main category: cs.LG

TL;DR: 提出了一种新的局部鲁棒性验证方法，通过动态构建和验证mini-batch来提高验证效率，平均提速2.3倍。


<details>
  <summary>Details</summary>
Motivation: 神经网络图像分类器易受对抗攻击，现有验证器分析时间长或精度不足。

Method: 提出BaVerLy验证器，利用网络计算的相似性动态构建mini-batch进行联合验证。

Result: 在MNIST和CIFAR-10上测试，平均提速2.3倍，最高4.1倍。

Conclusion: BaVerLy显著提升了局部鲁棒性验证的效率。

Abstract: Neural network image classifiers are ubiquitous in many safety-critical
applications. However, they are susceptible to adversarial attacks. To
understand their robustness to attacks, many local robustness verifiers have
been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers
introduce a long analysis time or lose too much precision, making them less
effective for a large set of inputs. In this work, we propose a new approach to
local robustness: group local robustness verification. The key idea is to
leverage the similarity of the network computations of certain $\epsilon$-balls
to reduce the overall analysis time. We propose BaVerLy, a sound and complete
verifier that boosts the local robustness verification of a set of
$\epsilon$-balls by dynamically constructing and verifying mini-batches.
BaVerLy adaptively identifies successful mini-batch sizes, accordingly
constructs mini-batches of $\epsilon$-balls that have similar network
computations, and verifies them jointly. If a mini-batch is verified, all
$\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected
as not being robust, guiding the refinement. In the latter case, BaVerLy
leverages the analysis results to expedite the analysis of that $\epsilon$-ball
as well as the other $\epsilon$-balls in the batch. We evaluate BaVerLy on
fully connected and convolutional networks for MNIST and CIFAR-10. Results show
that BaVerLy scales the common one by one verification by 2.3x on average and
up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6
hours.

</details>


### [79] [Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications](https://arxiv.org/abs/2508.15008)
*Hamza A. Abushahla,Dara Varam,Ariel J. N. Panopio,Mohamed I. AlHajri*

Main category: cs.LG

TL;DR: 该论文综述了量化神经网络（QNN）在资源受限设备（如微控制器）上的部署挑战，探讨了TinyML如何通过算法、硬件和软件优化解决这些问题，并分析了现有框架、平台以及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 资源受限设备部署深层神经网络时面临性能、计算复杂性和内存限制的挑战，需要一种高效的解决方案。

Method: 系统回顾量化技术，分析模型性能与硬件能力之间的权衡，评估现有软件框架和硬件平台。

Result: 总结了当前QNN部署的关键技术和工具，并指出了相关挑战。

Conclusion: QNN部署领域仍面临挑战，但未来研究方向充满希望。

Abstract: The deployment of Quantized Neural Networks (QNNs) on resource-constrained
devices, such as microcontrollers, has introduced significant challenges in
balancing model performance, computational complexity and memory constraints.
Tiny Machine Learning (TinyML) addresses these issues by integrating
advancements across machine learning algorithms, hardware acceleration, and
software optimization to efficiently run deep neural networks on embedded
systems. This survey presents a hardware-centric introduction to quantization,
systematically reviewing essential quantization techniques employed to
accelerate deep learning models for embedded applications. In particular,
further emphasis is put on critical trade-offs among model performance and
hardware capabilities. The survey further evaluates existing software
frameworks and hardware platforms designed specifically for supporting QNN
execution on microcontrollers. Moreover, we provide an analysis of the current
challenges and an outline of promising future directions in the rapidly
evolving domain of QNN deployment.

</details>


### [80] [Quantum Long Short-term Memory with Differentiable Architecture Search](https://arxiv.org/abs/2508.14955)
*Samuel Yen-Chi Chen,Prayag Tiwari*

Main category: cs.LG

TL;DR: 提出了一种名为DiffQAS-QLSTM的可微分框架，用于优化量子循环模型的变分量子电路参数和架构选择，实验表明其性能优于手动设计的基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决变分量子电路设计中的任务特定性和挑战性问题，提升量子序列学习的可扩展性和适应性。

Method: 开发了DiffQAS-QLSTM，一个端到端的可微分框架，在训练过程中同时优化VQC参数和架构选择。

Result: DiffQAS-QLSTM在多样化的测试场景中表现优于手动设计的基线模型，实现了更低的损失。

Conclusion: 该方法为可扩展和自适应的量子序列学习开辟了新途径。

Abstract: Recent advances in quantum computing and machine learning have given rise to
quantum machine learning (QML), with growing interest in learning from
sequential data. Quantum recurrent models like QLSTM are promising for
time-series prediction, NLP, and reinforcement learning. However, designing
effective variational quantum circuits (VQCs) remains challenging and often
task-specific. To address this, we propose DiffQAS-QLSTM, an end-to-end
differentiable framework that optimizes both VQC parameters and architecture
selection during training. Our results show that DiffQAS-QLSTM consistently
outperforms handcrafted baselines, achieving lower loss across diverse test
settings. This approach opens the door to scalable and adaptive quantum
sequence learning.

</details>


### [81] [TOAST: Fast and scalable auto-partitioning based on principled static analysis](https://arxiv.org/abs/2508.15010)
*Sami Alabed,Dominik Grewe,Norman Alexander Rink,Timur Sitdikov,Agnieszka Swietlik,Dimitrios Vytiniotis,Daniel Belov*

Main category: cs.LG

TL;DR: 该论文提出了一种结合静态编译器分析和蒙特卡洛树搜索的系统，用于高效分区大型机器学习模型，优于现有工业方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动分区器在探索指数级大的分区空间时，常因内存不足或速度过慢而受限，且人为限制搜索空间会导致不可行或次优的解。

Method: 结合静态编译器分析构建高效决策空间，识别需要相同分区的张量维度和分区冲突，并使用蒙特卡洛树搜索优化分区策略。

Result: 该系统在多样硬件平台和模型架构上显著优于工业界最新方法，发现新的优质解，且过程全自动化。

Conclusion: 提出的系统为大型模型分区提供了高效、自动化的解决方案，解决了现有方法的局限性。

Abstract: Partitioning large machine learning models across distributed accelerator
systems is a complex process, requiring a series of interdependent decisions
that are further complicated by internal sharding ambiguities. Consequently,
existing auto-partitioners often suffer from out-of-memory errors or are
prohibitively slow when exploring the exponentially large space of possible
partitionings. To mitigate this, they artificially restrict the search space,
but this approach frequently yields infeasible solutions that violate device
memory constraints or lead to sub-optimal performance.
  We propose a system that combines a novel static compiler analysis with a
Monte Carlo Tree Search. Our analysis constructs an efficient decision space by
identifying (i) tensor dimensions requiring identical sharding, and (ii)
partitioning "conflicts" that require resolution.
  Our system significantly outperforms state-of-the-art industrial methods
across diverse hardware platforms and model architectures, discovering
previously unknown, superior solutions, and the process is fully automated even
for complex and large models.

</details>


### [82] [A Solvable Molecular Switch Model for Stable Temporal Information Processing](https://arxiv.org/abs/2508.15451)
*H. I. Nurdin,C. A. Nijhuis*

Main category: cs.LG

TL;DR: 本文研究了一种输入驱动的单状态微分方程模型，该模型最初为动态分子开关设计，具有线性和非线性特性，能够稳定处理时变输入，并支持神经形态计算的应用。


<details>
  <summary>Details</summary>
Motivation: 研究动态分子开关的数学模型，验证其在稳定学习和神经形态计算中的潜力。

Method: 使用线性和非线性结合的微分方程模型，分析其收敛性和记忆特性。

Result: 模型展示了生物学启发的行为与数学稳定性的共存，适用于神经形态计算。

Conclusion: 该模型为动态分子开关在神经形态计算中的应用提供了理论支持，并可能启发更多可求解的模型。

Abstract: This paper studies an input-driven one-state differential equation model
initially developed for an experimentally demonstrated dynamic molecular switch
that switches like synapses in the brain do. The linear-in-the-state and
nonlinear-in-the-input model is exactly solvable, and it is shown that it also
possesses mathematical properties of convergence and fading memory that enable
stable processing of time-varying inputs by nonlinear dynamical systems. Thus,
the model exhibits the co-existence of biologically-inspired behavior and
desirable mathematical properties for stable learning on sequential data. The
results give theoretical support for the use of the dynamic molecular switches
as computational units in deep cascaded/layered feedforward and recurrent
architectures as well as other more general structures for neuromorphic
computing. They could also inspire more general exactly solvable models that
can be fitted to emulate arbitrary physical devices which can mimic
brain-inspired behaviour and perform stable computation on input signals.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [83] [Optimizing Compilation for Distributed Quantum Computing via Clustering and Annealing](https://arxiv.org/abs/2508.15267)
*Ruilin Zhou,Jinglei Cheng,Yuhang Gan,Junyu Liu,Chen Qian*

Main category: quant-ph

TL;DR: 本文提出了一种针对分布式量子计算（DQC）的编译框架，通过电路结构分析、聚类和退火算法优化量子程序映射，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 分布式量子计算系统中，由于量子处理单元（QPUs）的异构性，高效映射量子程序具有挑战性。

Method: 提出了一种编译框架，利用电路结构模式分析、初始量子比特放置的聚类方法，以及退火算法调整量子比特映射。

Result: 实验证明该方法有效，能处理复杂的异构分布式量子系统，目标值最多降低了88.40%。

Conclusion: 该方法为异构分布式量子系统的程序映射提供了高效解决方案。

Abstract: Efficiently mapping quantum programs onto Distributed quantum computing (DQC)
are challenging, particularly when considering the heterogeneous quantum
processing units (QPUs) with different structures. In this paper, we present a
comprehensive compilation framework that addresses these challenges with three
key insights: exploiting structural patterns within quantum circuits, using
clustering for initial qubit placement, and adjusting qubit mapping with
annealing algorithms. Experimental results demonstrate the effectiveness of our
methods and the capability to handle complex heterogeneous distributed quantum
systems. Our evaluation shows that our method reduces the objective value at
most 88.40\% compared to the baseline.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [84] [LLMs and Agentic AI in Insurance Decision-Making: Opportunities and Challenges For Africa](https://arxiv.org/abs/2508.15110)
*Graham Hill,JingYuan Gong,Thulani Babeli,Moseli Mots'oehli,James Gachomo Wanjiku*

Main category: cs.CE

TL;DR: 探讨AI（尤其是LLMs和智能代理AI）在保险业的变革潜力，分析机遇与挑战，并针对非洲市场提出合作建议。


<details>
  <summary>Details</summary>
Motivation: 研究AI在保险领域的应用潜力，特别是在非洲市场的独特需求和解决方案。

Method: 分析AI技术的发展趋势和现状，结合非洲保险市场的具体问题。

Result: 提出AI技术可为非洲保险业带来变革，并呼吁多方合作开发本地化解决方案。

Conclusion: 呼吁行业多方协作，为非洲保险市场开发可持续、包容的AI策略。

Abstract: In this work, we highlight the transformative potential of Artificial
Intelligence (AI), particularly Large Language Models (LLMs) and agentic AI, in
the insurance sector. We consider and emphasize the unique opportunities,
challenges, and potential pathways in insurance amid rapid performance
improvements, increased open-source access, decreasing deployment costs, and
the complexity of LLM or agentic AI frameworks. To bring it closer to home, we
identify critical gaps in the African insurance market and highlight key local
efforts, players, and partnership opportunities. Finally, we call upon
actuaries, insurers, regulators, and tech leaders to a collaborative effort
aimed at creating inclusive, sustainable, and equitable AI strategies and
solutions: by and for Africans.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [85] [Collaborative Filtering using Variational Quantum Hopfield Associative Memory](https://arxiv.org/abs/2508.14906)
*Amir Kermanshahani,Ebrahim Ardeshir-Larijani,Rakesh Saini,Saif Al-Kuwari*

Main category: cs.IR

TL;DR: 该论文提出了一种结合量子Hopfield联想记忆（QHAM）与深度神经网络的混合推荐系统，用于提升在MovieLens 1M数据集上的提取与分类性能。通过聚类用户原型并转换为极性模式，模型在理想和噪声环境中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 量子计算在机器学习中的应用潜力巨大，但需解决噪声环境下的稳定性和计算开销问题。

Method: 使用K-Means聚类用户原型并转换为极性模式，结合变分QHAM和深度神经网络训练模型。

Result: 在理想环境中，ROC值为0.9795，准确率为0.8841；噪声环境中ROC为0.9177，准确率为0.8013。

Conclusion: 该框架展示了量子计算与深度学习的结合在推荐系统中的潜力，尤其在噪声环境下表现稳定。

Abstract: Quantum computing, with its ability to do exponentially faster computation
compared to classical systems, has found novel applications in various fields
such as machine learning and recommendation systems. Quantum Machine Learning
(QML), which integrates quantum computing with machine learning techniques,
presents powerful new tools for data processing and pattern recognition. This
paper proposes a hybrid recommendation system that combines Quantum Hopfield
Associative Memory (QHAM) with deep neural networks to improve the extraction
and classification on the MovieLens 1M dataset. User archetypes are clustered
into multiple unique groups using the K-Means algorithm and converted into
polar patterns through the encoder's activation function. These polar patterns
are then integrated into the variational QHAM-based hybrid recommendation
model. The system was trained using the MSE loss over 35 epochs in an ideal
environment, achieving an ROC value of 0.9795, an accuracy of 0.8841, and an
F-1 Score of 0.8786. Trained with the same number of epochs in a noisy
environment using a custom Qiskit AER noise model incorporating bit-flip and
readout errors with the same probabilities as in real quantum hardware, it
achieves an ROC of 0.9177, an accuracy of 0.8013, and an F-1 Score equal to
0.7866, demonstrating consistent performance.
  Additionally, we were able to optimize the qubit overhead present in previous
QHAM architectures by efficiently updating only one random targeted qubit. This
research presents a novel framework that combines variational quantum computing
with deep learning, capable of dealing with real-world datasets with comparable
performance compared to purely classical counterparts. Additionally, the model
can perform similarly well in noisy configurations, showcasing a steady
performance and proposing a promising direction for future usage in
recommendation systems.

</details>


### [86] [On the Effectiveness of Graph Reordering for Accelerating Approximate Nearest Neighbor Search on GPU](https://arxiv.org/abs/2508.15436)
*Yutaro Oguri,Mai Nishimura,Yusuke Matsui*

Main category: cs.IR

TL;DR: 本文首次系统研究了GPU上基于图的近似最近邻搜索（ANNS）的图重排序效果，通过统一评估框架和GPU优化的图遍历引擎，展示了内存布局优化对性能的提升。


<details>
  <summary>Details</summary>
Motivation: 尽管基于图的ANNS已成为现代AI应用的主导方法，但现有研究多关注算法创新，忽略了显著影响执行时间的内存布局问题。

Method: 提出了一种统一的评估框架，包含图适配器和GPU优化的图遍历引擎，用于全面评估不同图索引的多样化重排序策略。

Result: 实验表明，针对GPU的重排序策略可在保持搜索准确性的同时，将QPS提升高达15%。

Conclusion: 内存布局优化与现有算法创新正交，显著提升性能，所有代码将公开发布以促进可重复性和进一步研究。

Abstract: We present the first systematic investigation of graph reordering effects for
graph-based Approximate Nearest Neighbor Search (ANNS) on a GPU. While
graph-based ANNS has become the dominant paradigm for modern AI applications,
recent approaches focus on algorithmic innovations while neglecting memory
layout considerations that significantly affect execution time. Our unified
evaluation framework enables comprehensive evaluation of diverse reordering
strategies across different graph indices through a graph adapter that converts
arbitrary graph topologies into a common representation and a GPU-optimized
graph traversal engine. We conduct a comprehensive analysis across diverse
datasets and state-of-the-art graph indices, introducing analysis metrics that
quantify the relationship between structural properties and memory layout
effectiveness. Our GPU-targeted reordering achieves up to 15$\%$ QPS
improvements while preserving search accuracy, demonstrating that memory layout
optimization operates orthogonally to existing algorithmic innovations. We will
release all code upon publication to facilitate reproducibility and foster
further research.

</details>
