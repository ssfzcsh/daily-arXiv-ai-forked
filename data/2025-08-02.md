<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 26]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 3]
- [math.CT](#math.CT) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [On LLM-Assisted Generation of Smart Contracts from Business Processes](https://arxiv.org/abs/2507.23087)
*Fabian Stiehle,Hans Weytjens,Ingo Weber*

Main category: cs.SE

TL;DR: 研究了使用LLMs从业务流程描述生成智能合约代码的可行性，发现目前LLMs在实现完美可靠性方面表现不足。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在智能合约代码生成中的应用，以克服传统基于规则的代码生成方法的局限性。

Method: 引入自动化评估框架，测试不同类型和大小的LLMs在实现流程执行关键属性（如流程控制、资源分配和基于数据的条件）上的能力。

Result: LLMs的性能在完美可靠性方面不足，不满足智能合约开发的要求。

Conclusion: 建议未来研究探索如何将LLMs负责任地集成到现有代码生成工具中，以提高输出可靠性。

Abstract: Large language models (LLMs) have changed the reality of how software is
produced. Within the wider software engineering community, among many other
purposes, they are explored for code generation use cases from different types
of input. In this work, we present an exploratory study to investigate the use
of LLMs for generating smart contract code from business process descriptions,
an idea that has emerged in recent literature to overcome the limitations of
traditional rule-based code generation approaches. However, current LLM-based
work evaluates generated code on small samples, relying on manual inspection,
or testing whether code compiles but ignoring correct execution. With this
work, we introduce an automated evaluation framework and provide empirical data
from larger data sets of process models. We test LLMs of different types and
sizes in their capabilities of achieving important properties of process
execution, including enforcing process flow, resource allocation, and
data-based conditions. Our results show that LLM performance falls short of the
perfect reliability required for smart contract development. We suggest future
work to explore responsible LLM integrations in existing tools for code
generation to ensure more reliable output. Our benchmarking framework can serve
as a foundation for developing and evaluating such integrations.

</details>


### [2] [FlowETL: An Autonomous Example-Driven Pipeline for Data Engineering](https://arxiv.org/abs/2507.23118)
*Mattia Di Profio,Mingjun Zhong,Yaji Sripada,Marcel Jaspars*

Main category: cs.SE

TL;DR: FlowETL是一种基于示例的自主ETL管道架构，能自动标准化和准备输入数据集，减少人工参与。


<details>
  <summary>Details</summary>
Motivation: 现代ETL解决方案需要大量人工干预，缺乏自动设计及应用转换的能力，FlowETL旨在解决这一问题。

Method: FlowETL通过规划引擎和ETL工作器构建并执行转换计划，结合监控和日志实现全流程可观测性。

Result: 在14个不同领域、文件结构和大小的数据集上表现出良好的泛化能力。

Conclusion: FlowETL展示了自动化ETL的潜力，能有效减少人工依赖并提高效率。

Abstract: The Extract, Transform, Load (ETL) workflow is fundamental for populating and
maintaining data warehouses and other data stores accessed by analysts for
downstream tasks. A major shortcoming of modern ETL solutions is the extensive
need for a human-in-the-loop, required to design and implement
context-specific, and often non-generalisable transformations. While related
work in the field of ETL automation shows promising progress, there is a lack
of solutions capable of automatically designing and applying these
transformations. We present FlowETL, a novel example-based autonomous ETL
pipeline architecture designed to automatically standardise and prepare input
datasets according to a concise, user-defined target dataset. FlowETL is an
ecosystem of components which interact together to achieve the desired outcome.
A Planning Engine uses a paired input-output datasets sample to construct a
transformation plan, which is then applied by an ETL worker to the source
dataset. Monitoring and logging provide observability throughout the entire
pipeline. The results show promising generalisation capabilities across 14
datasets of various domains, file structures, and file sizes.

</details>


### [3] [Vibe Modeling: Challenges and Opportunities](https://arxiv.org/abs/2507.23120)
*Jordi Cabot*

Main category: cs.SE

TL;DR: 论文提出“vibe modeling”概念，结合AI与模型驱动工程（MDE）优势，以加速开发可靠复杂系统。


<details>
  <summary>Details</summary>
Motivation: 面对软件系统日益增长的需求与复杂性，现有开发方法和工具需改进；同时，AI驱动的开发方法（如LLMs）存在漏洞和可维护性问题。

Method: 提出“vibe modeling”概念，整合AI与MDE，实现自然语言描述到模型的转换，同时解决复杂模型的管理问题。

Result: 初步介绍了vibe modeling的关键概念，展示了其潜力。

Conclusion: vibe modeling为未来建模带来新机遇，但也需解决开放挑战。

Abstract: There is a pressing need for better development methods and tools to keep up
with the growing demand and increasing complexity of new software systems. New
types of user interfaces, the need for intelligent components, sustainability
concerns, ... bring new challenges that we need to handle. In the last years,
model-driven engineering (MDE) has been key to improving the quality and
productivity of software development, but models themselves are becoming
increasingly complex to specify and manage. At the same time, we are witnessing
the growing popularity of vibe coding approaches that rely on Large Language
Models (LLMs) to transform natural language descriptions into running code at
the expenses of code vulnerabilities, scalability issues and maintainability
concerns. In this paper, we introduce the concept of \textit{vibe modeling} as
a novel approach to integrate the best of both worlds (AI and MDE) to speed up
the development of reliable complex systems. We outline the key concepts of
vibe modeling and highlight the opportunities and open challenges it presents
for the future of modeling.

</details>


### [4] [Extension Decisions in Open Source Software Ecosystem](https://arxiv.org/abs/2507.23168)
*Elmira Onagh,Maleknaz Nayebi*

Main category: cs.SE

TL;DR: GitHub Marketplace 中约 65% 的新 CI Actions 功能重复，通常在六个月内出现，少数先发 Actions 主导后续衍生工具。


<details>
  <summary>Details</summary>
Motivation: 研究 GitHub Marketplace 中 CI 工具的冗余现象，以帮助开发者选择最佳发布时机并优化工具策略。

Method: 通过链接 6,983 个 CI Actions 和 3,869 个提供商，分析版本历史，建立时间戳图模型追踪功能首次亮相和采用情况。

Result: 约 65% 的新 CI Actions 功能重复，先发 Actions 主导后续衍生。

Conclusion: 研究结果为开发者和管理者提供数据驱动的优化策略，并公开完整数据集促进后续研究。

Abstract: GitHub Marketplace is expanding by approximately 41% annually, with new
tools; however, many additions replicate existing functionality. We study this
phenomenon in the platform's largest segment, Continuous Integration (CI), by
linking 6,983 CI Actions to 3,869 providers and mining their version histories.
Our graph model timestamps every functionality's debut, tracks its adoption,
and clusters redundant tools. We find that approximately 65% of new CI Actions
replicate existing capabilities, typically within six months, and that a small
set of first-mover Actions accounts for most subsequent forks and extensions.
These insights enable developers to choose the optimal moment to launch, target
unmet functionality, and help maintainers eliminate redundant tools. We publish
the complete graph and dataset to encourage longitudinal research on innovation
and competition in software ecosystems, and to provide practitioners with a
data-driven roadmap for identifying emerging trends and guiding product
strategy.

</details>


### [5] [AutoBridge: Automating Smart Device Integration with Centralized Platform](https://arxiv.org/abs/2507.23178)
*Siyuan Liu,Zhice Yang,Huangxun Chen*

Main category: cs.SE

TL;DR: AutoBridge自动化生成物联网集成代码，通过分阶段调试和多平台验证，实现高成功率和高功能覆盖率，优于人工编程。


<details>
  <summary>Details</summary>
Motivation: 传统物联网系统集成新设备需要大量人工编程，AutoBridge旨在解决这一问题，通过自动化减少人工参与和错误。

Method: AutoBridge采用分治法：首先生成设备控制逻辑，再合成平台兼容的集成代码，并通过多阶段调试（虚拟测试和硬件循环调试）确保正确性。

Result: 在34个设备上测试，平均成功率和功能覆盖率分别为93.87%和94.87%；用户反馈后可达100%覆盖率。用户研究显示其代码准确性优于专家50%-80%。

Conclusion: AutoBridge通过自动化方法显著提升物联网设备集成的效率和准确性，优于传统人工编程和商用LLMs辅助编程。

Abstract: Multimodal IoT systems coordinate diverse IoT devices to deliver
human-centered services. The ability to incorporate new IoT devices under the
management of a centralized platform is an essential requirement. However, it
requires significant human expertise and effort to program the complex IoT
integration code that enables the platform to understand and control the device
functions. Therefore, we propose AutoBridge to automate IoT integration code
generation. Specifically, AutoBridge adopts a divide-and-conquer strategy: it
first generates device control logic by progressively retrieving
device-specific knowledge, then synthesizes platformcompliant integration code
using platform-specific knowledge. To ensure correctness, AutoBridge features a
multi-stage debugging pipeline, including an automated debugger for virtual IoT
device testing and an interactive hardware-in-the-loop debugger that requires
only binary user feedback (yes and no) for real-device verification. We
evaluate AutoBridge on a benchmark of 34 IoT devices across two open-source IoT
platforms. The results demonstrate that AutoBridge can achieves an average
success rate of 93.87% and an average function coverage of 94.87%, without any
human involvement. With minimal binary yes and no feedback from users, the code
is then revised to reach 100% function coverage. A user study with 15
participants further shows that AutoBridge outperforms expert programmers by
50% to 80% in code accuracy, even when the programmers are allowed to use
commercial code LLMs.

</details>


### [6] [XABPs: Towards eXplainable Autonomous Business Processes](https://arxiv.org/abs/2507.23269)
*Peter Fettke,Fabiana Fournier,Lior Limonad,Andreas Metzger,Stefanie Rinderle-Ma,Barbara Weber*

Main category: cs.SE

TL;DR: 该论文主张通过可解释的自主业务流程（XABPs）解决AI/ML驱动的业务流程可能带来的问题，提出了一种系统性方法。


<details>
  <summary>Details</summary>
Motivation: 自主业务流程（ABPs）虽能提高效率，但可能引发信任、调试、责任偏见等问题。

Method: 提出可解释的自主业务流程（XABPs），通过系统化方法实现解释性。

Result: 文章概述了XABPs的形式、解释性结构，并指出了关键研究挑战。

Conclusion: XABPs为解决ABPs的问题提供了一种可行方案，但仍需进一步研究。

Abstract: Autonomous business processes (ABPs), i.e., self-executing workflows
leveraging AI/ML, have the potential to improve operational efficiency, reduce
errors, lower costs, improve response times, and free human workers for more
strategic and creative work. However, ABPs may raise specific concerns
including decreased stakeholder trust, difficulties in debugging, hindered
accountability, risk of bias, and issues with regulatory compliance. We argue
for eXplainable ABPs (XABPs) to address these concerns by enabling systems to
articulate their rationale. The paper outlines a systematic approach to XABPs,
characterizing their forms, structuring explainability, and identifying key BPM
research challenges towards XABPs.

</details>


### [7] [SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution](https://arxiv.org/abs/2507.23348)
*Han Li,Yuling Shi,Shaoxin Lin,Xiaodong Gu,Heng Lian,Xin Wang,Yantao Jia,Tao Huang,Qianxiang Wang*

Main category: cs.SE

TL;DR: SWE-Debate通过多代理辩论框架改进问题定位，实现更高效的代码修复。


<details>
  <summary>Details</summary>
Motivation: 现有代理框架因独立探索能力有限，难以发现跨代码库的问题模式，导致局部解决方案失效。

Method: SWE-Debate通过故障传播轨迹生成多个定位提案，并组织三轮代理辩论，结合MCTS生成修复补丁。

Result: 在SWE-bench基准测试中，SWE-Debate显著优于基线方法，达到最新技术水平。

Conclusion: 多代理辩论框架能够有效提升问题定位和修复的效率，优于现有独立探索方法。

Abstract: Issue resolution has made remarkable progress thanks to the advanced
reasoning capabilities of large language models (LLMs). Recently, agent-based
frameworks such as SWE-agent have further advanced this progress by enabling
autonomous, tool-using agents to tackle complex software engineering tasks.
While existing agent-based issue resolution approaches are primarily based on
agents' independent explorations, they often get stuck in local solutions and
fail to identify issue patterns that span across different parts of the
codebase. To address this limitation, we propose SWE-Debate, a competitive
multi-agent debate framework that encourages diverse reasoning paths and
achieves more consolidated issue localization. SWE-Debate first creates
multiple fault propagation traces as localization proposals by traversing a
code dependency graph. Then, it organizes a three-round debate among
specialized agents, each embodying distinct reasoning perspectives along the
fault propagation trace. This structured competition enables agents to
collaboratively converge on a consolidated fix plan. Finally, this consolidated
fix plan is integrated into an MCTS-based code modification agent for patch
generation. Experiments on the SWE-bench benchmark show that SWE-Debate
achieves new state-of-the-art results in open-source agent frameworks and
outperforms baselines by a large margin.

</details>


### [8] [Quality Evaluation of COBOL to Java Code Transformation](https://arxiv.org/abs/2507.23356)
*Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Avi Ziv*

Main category: cs.SE

TL;DR: 提出了一个用于评估COBOL-to-Java代码翻译的自动化系统，结合分析检查器和LLM作为评判技术，以提供可扩展的多维评估。


<details>
  <summary>Details</summary>
Motivation: 解决基于LLM的翻译评估中的模型不透明性和翻译质量评估复杂性问题。

Method: 结合分析检查器和LLM-as-a-judge (LaaJ)技术。

Result: 支持持续集成工作流、大规模基准测试，并减少对人工审查的依赖。

Conclusion: 系统架构和评估策略为开发者和项目经理提供可操作的见解，促进高质量、现代化代码库的发展。

Abstract: We present an automated evaluation system for assessing COBOL-to-Java code
translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system
addresses key challenges in evaluating LLM-based translators, including model
opacity and the complexity of translation quality assessment. Our approach
combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver
scalable, multi-faceted evaluations. The system supports continuous integration
workflows, enables large-scale benchmarking, and reduces reliance on manual
review. We describe the system architecture, evaluation strategies, and
reporting mechanisms that provide actionable insights for developers and
project managers, facilitating the evolution of high-quality, modernized
codebases.

</details>


### [9] [SWE-Exp: Experience-Driven Software Issue Resolution](https://arxiv.org/abs/2507.23361)
*Silin Chen,Shaoxin Lin,Xiaodong Gu,Yuling Shi,Heng Lian,Longfei Yun,Dong Chen,Weiguo Sun,Lin Cao,Qianxiang Wang*

Main category: cs.SE

TL;DR: 论文提出SWE-Exp方法，通过从以往修复经验中提取可操作的、简洁的知识，改进LLM代理在解决软件问题中的表现，实现连续学习和战略性问题解决。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理在解决软件问题时缺乏记忆性，无法复用先前经验，导致重复探索失败路径并错过相似问题的有效解决方法。

Method: 引入多方面的经验库，捕捉成功和失败的修复尝试，提取多层次可复用的知识。

Result: 在SWE-bench-Verified上达到41.6%Pass@1的最优解决率。

Conclusion: SWE-Exp通过系统积累和利用修复经验，从试错探索转向经验驱动的战略问题解决，为自动化软件工程代理提供了新范式。

Abstract: Recent advances in large language model (LLM) agents have shown remarkable
progress in software issue resolution, leveraging advanced techniques such as
multi-agent collaboration and Monte Carlo Tree Search (MCTS). However, current
agents act as memoryless explorers - treating each problem separately without
retaining or reusing knowledge from previous repair experiences. This leads to
redundant exploration of failed trajectories and missed chances to adapt
successful issue resolution methods to similar problems. To address this
problem, we introduce SWE-Exp, an experience - enhanced approach that distills
concise and actionable experience from prior agent trajectories, enabling
continuous learning across issues. Our method introduces a multi-faceted
experience bank that captures both successful and failed repair attempts.
Specifically, it extracts reusable issue resolution knowledge at different
levels - from high-level problem comprehension to specific code changes.
Experiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%
Pass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach
establishes a new paradigm in which automated software engineering agents
systematically accumulate and leverage repair expertise, fundamentally shifting
from trial-and-error exploration to strategic, experience-driven issue
resolution.

</details>


### [10] [Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling](https://arxiv.org/abs/2507.23370)
*Trae Research Team,Pengfei Gao,Zhao Tian,Xiangxin Meng,Xinchen Wang,Ruida Hu,Yuanan Xiao,Yizhou Liu,Zhao Zhang,Junjie Chen,Cuiyun Gao,Yun Lin,Yingfei Xiong,Chao Peng,Xia Liu*

Main category: cs.SE

TL;DR: 论文提出了Trae Agent，一个基于代理的集成推理方法，用于解决软件问题。通过模块化代理解决大集成空间和仓库级理解的挑战，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于提示的方法在大集成空间探索和仓库级理解上的局限性，提升软件问题解决的效率。

Method: 采用模块化代理（生成、剪枝、选择）将目标建模为最优解搜索问题，解决大集成空间和仓库级理解的挑战。

Result: 在SWE-bench基准测试中，Trae Agent平均Pass@1提升10.22%，以75.20%的成绩荣登榜首。

Conclusion: Trae Agent在软件问题解决中表现出色，其方法和开源资源对研究社区有重要贡献。

Abstract: Software issue resolution is a critical challenge in software engineering and
has garnered increasing attention in recent years. With the rapid advancement
of large language models (LLMs), substantial progress has been made in
addressing real-world software engineering tasks. Recent studies have
introduced ensemble reasoning techniques to enhance the performance of
LLM-based issue resolution. However, existing prompting-based methods still
face limitations in effectively exploring large ensemble spaces and lack the
capacity for repository-level understanding, both of which constrain their
overall effectiveness. In this paper, we propose Trae Agent, the first
agent-based ensemble reasoning approach for repository-level issue resolution.
Trae Agent formulates our goal as an optimal solution search problem and
addresses two key challenges, i.e., large ensemble spaces and repository-level
understanding, through modular agents for generation, pruning, and selection.
We conduct extensive experiments using three leading LLMs on the widely-adopted
SWE-bench benchmark, comparing Trae Agent against four state-of-the-art
ensemble reasoning techniques. Experimental results demonstrate that Trae Agent
consistently achieves superior performance, with an average improvement of
10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first
place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of
75.20%. We are pleased to release Trae Agent as an open-source project to
support the research community, with all resources available at
https://github.com/bytedance/trae-agent.

</details>


### [11] [Dynamic and Static Analysis of Python Software with Kieker Including Reconstructed Architectures](https://arxiv.org/abs/2507.23425)
*Daphné Larrivain,Shinhyung Yang,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: Kieker框架扩展支持Python，结合静态和动态分析提供系统全面视图。


<details>
  <summary>Details</summary>
Motivation: Python的流行使其应用的结构洞察极具价值，因此为Kieker框架添加Python支持很有意义。

Method: 结合静态和动态分析构建Python应用的完整系统视图。

Result: 成功实现了一个Python分析管道。

Conclusion: Kieker框架扩展支持Python，为开发者提供了更全面的工具。

Abstract: The Kieker observability framework is a tool that provides users with the
means to design a custom observability pipeline for their application.
Originally tailored for Java, supporting Python with Kieker is worthwhile.
Python's popularity has exploded over the years, thus making structural
insights of Python applications highly valuable. Our Python analysis pipeline
combines static and dynamic analysis in order to build a complete picture of a
given system.

</details>


### [12] [An Empirical Study on the Amount of Changes Required for Merge Request Acceptance](https://arxiv.org/abs/2507.23640)
*Samah Kansab,Mohammed Sayagh,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 论文探讨了GitLab合并请求中的代码审查努力，发现71%的提交需要调整，28%涉及超过200行代码改动。通过机器学习模型，发现复杂性、开发者经验和文本特征是关键预测因素。


<details>
  <summary>Details</summary>
Motivation: 研究代码审查过程中的实际努力，尤其是基于代码修改量的努力，填补了GitLab合并请求背景下这一未充分研究的领域。

Method: 定义了代码审查努力为提交后修改的代码量，分析了23,600多个合并请求数据，并训练了可解释的机器学习模型，结合多维度指标。

Result: 71%的合并请求需要调整，28%涉及大改动；模型表现优异（AUC 0.84-0.88），复杂性、经验和文本特征是最佳预测指标。

Conclusion: 机器学习能有效解释和预测代码审查努力，项目历史特征对当前审查有影响，为优化审查流程提供了依据。

Abstract: Code review (CR) is essential to software development, helping ensure that
new code is properly integrated. However, the CR process often involves
significant effort, including code adjustments, responses to reviewers, and
continued implementation. While past studies have examined CR delays and
iteration counts, few have investigated the effort based on the volume of code
changes required, especially in the context of GitLab Merge Requests (MRs),
which remains underexplored. In this paper, we define and measure CR effort as
the amount of code modified after submission, using a dataset of over 23,600
MRs from four GitLab projects. We find that up to 71% of MRs require
adjustments after submission, and 28% of these involve changes to more than 200
lines of code. Surprisingly, this effort is not correlated with review time or
the number of participants. To better understand and predict CR effort, we
train an interpretable machine learning model using metrics across multiple
dimensions: text features, code complexity, developer experience, review
history, and branching. Our model achieves strong performance (AUC 0.84-0.88)
and reveals that complexity, experience, and text features are key predictors.
Historical project characteristics also influence current review effort. Our
findings highlight the feasibility of using machine learning to explain and
anticipate the effort needed to integrate code changes during review.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Abstractions of Sequences, Functions and Operators](https://arxiv.org/abs/2507.23151)
*Louis Rustenholz,Pedro Lopez-Garcia,Manuel V. Hermenegildo*

Main category: cs.PL

TL;DR: 该论文提出了一种基于约束的抽象域（B-bound domains），用于抽象数值函数，并能推断高度非线性的数值不变量。此外，还引入了域抽象（domain abstraction）的概念，支持将符号函数抽象为数值函数。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决递归定义函数的闭式边界推断问题，这在程序分析和混合系统的微分方程分析中有广泛应用。

Method: 通过设计B-bound domains，并结合凸性性质简化传递函数的设计，同时引入domain abstraction，支持从符号函数到数值函数的抽象。

Result: 提出了一种新型抽象域，能够高效推断非线性不变量，并简化传递函数的设计，部分情况下实现完全自动化。

Conclusion: 该研究为高阶抽象解释提供了新的理论工具，尤其适用于复杂函数和不变量的推断。

Abstract: We present theoretical and practical results on the order theory of lattices
of functions, focusing on Galois connections that abstract (sets of) functions
- a topic known as higher-order abstract interpretation.
  We are motivated by the challenge of inferring closed-form bounds on
functions which are defined recursively, i.e. as the fixed point of an operator
or, equivalently, as the solution to a functional equation. This has multiple
applications in program analysis (e.g. cost analysis, loop acceleration,
declarative language analysis) and in hybrid systems governed by differential
equations.
  Our main contribution is a new family of constraint-based abstract domains
for abstracting numerical functions, B-bound domains, which abstract a function
f by a conjunction of bounds from a preselected set of boundary functions. They
allow inferring highly non-linear numerical invariants, which classical
numerical abstract domains struggle with. We uncover a convexity property in
the constraint space that simplifies, and, in some cases, fully automates,
transfer function design.
  We also introduce domain abstraction, a functor that lifts arbitrary mappings
in value space to Galois connections in function space. This supports
abstraction from symbolic to numerical functions (i.e. size abstraction), and
enables dimensionality reduction of equations.
  We base our constructions of transfer functions on a simple operator
language, starting with sequences, and extending to more general functions,
including multivariate, piecewise, and non-discrete domains.

</details>


### [14] [Kernel-FFI: Transparent Foreign Function Interfaces for Interactive Notebooks](https://arxiv.org/abs/2507.23205)
*Hebi Li,Forrest Sheng Bao,Qi Xiao,Jin Tian*

Main category: cs.PL

TL;DR: Kernel-FFI是一个透明的、语言无关的框架，用于在交互式笔记本中无缝实现跨语言函数调用和对象操作，解决了现有FFI解决方案在动态工作流中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的FFI解决方案不适合现代笔记本环境（如Jupyter）的动态交互工作流，需要大量手动配置且缺乏对递归调用和OOP的支持。

Method: Kernel-FFI通过源代码级转换自动重写跨语言调用，无需手动绑定，支持OOP并通过侧信道通信机制处理递归和异步调用。

Result: Kernel-FFI实现了无缝的跨语言功能调用和对象操作，并解决了Jupyter内核的阻塞问题。

Conclusion: Kernel-FFI为多语言开发提供了一个高效的解决方案，适用于动态交互环境。

Abstract: Foreign Function Interfaces (FFIs) are essential for enabling
interoperability between programming languages, yet existing FFI solutions are
ill-suited for the dynamic, interactive workflows prevalent in modern notebook
environments such as Jupyter. Current approaches require extensive manual
configuration, introduce significant boilerplate, and often lack support for
recursive calls and object-oriented programming (OOP) constructs-features
critical for productive, multi-language development.
  We present Kernel-FFI, a transparent, language-agnostic framework that
enables seamless cross-language function calls and object manipulation within
interactive notebooks. Kernel-FFI employs source-level transformation to
automatically rewrite cross-language invocations, eliminating the need for
manual bindings or boilerplate. Kernel-FFI provides robust support for OOP by
enabling foreign object referencing and automatic resource management across
language boundaries. Furthermore, to address the blocking nature of Jupyter
kernels and support recursive and asynchronous foreign calls, we introduce a
novel side-channel communication mechanism. Our tool will be open-sourced and
available at https://codepod.io/docs/kernel-ffi

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [15] [PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers](https://arxiv.org/abs/2507.23012)
*Ashkan Sobhani,Sogand Sadrhaghighi,Xingjun Chu*

Main category: cs.NI

TL;DR: PRIME是一种针对大规模分布式训练中网络负载平衡问题的新方法，通过伪随机轮询和考虑网络拓扑来优化负载分配，显著提升了网络性能。


<details>
  <summary>Details</summary>
Motivation: 现有的AI/ML工作负载在网络中表现出低熵、突发性和长生命周期的特性，导致传统负载平衡方法（如ECMP）性能不佳，亟需一种更有效的解决方案。

Method: PRIME采用伪随机轮询的包喷洒技术，结合网络拓扑信息，通过拥塞信号动态调整负载分配，避免网络热点。

Result: 实验表明，PRIME在排列流量和网络降级场景下的性能分别提升了15%和27%。

Conclusion: PRIME通过动态负载平衡策略，显著改善了大规模分布式训练中的网络性能问题。

Abstract: Large-scale distributed training in production data centers place significant
demands on network infrastructure. In particular, significant load balancing
challenges arise when processing AI/ML workloads, consisting of low-entropy,
bursty and long-lived flows. Existing solutions designed for Ethernet, such as
Equal-Cost Multi-Path (ECMP) struggle to maintain high network utilization.
While major industry players (e.g., Ultra Ethernet Consortium) and parts of
academia have proposed packet spraying to enhance AI/ML workload performance,
we argue that existing packet spraying solutions lead to buffer inflation over
time, negatively affecting network performance. Specifically, when ACK
coalescing is used, these solutions lead to stale information, degrading
network performance. Additionally, in asymmetric network conditions- such as
mix of ordered an unordered traffic, or link degradation and failures- existing
packet spraying solutions often lead to increased tail latency. In this paper,
we present the design and evaluation of PRIME, a pseudo-randomized round-robin
approach to packet spraying that considers the network topology to optimize
load distribution and performance. PRIME uses congestion as an indicator to
re-balance the load. To this extent, PRIME takes into account various
congestion signals, accounting for congestion severity, and their decay times
to avoid network hotspots. We extensively evaluated PRIME using large-scale
production-level simulator. Our results indicate that, compared to existing
solutions, PRIME leads to up to 15% improvement for permutation traffic and up
to 27% improvement in network degradation scenarios

</details>


### [16] [InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps](https://arxiv.org/abs/2507.23177)
*Neagin Neasamoni Santhi,Davide Villa,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本文提出了一种名为InterfO-RAN的实时可编程解决方案，利用CNN处理gNB物理层的I/Q样本，以超过91%的准确率在650微秒内检测带内干扰，解决了5G网络中UL干扰的问题。


<details>
  <summary>Details</summary>
Motivation: 5G及超5G网络中，带内UL干扰严重影响信号质量，特别是在小区边缘和毫米波系统中，干扰会破坏协议操作，如调度和资源分配。

Method: 采用CNN处理gNB物理层的I/Q样本，实现实时干扰检测，并首次将O-RAN dApp与NVIDIA Aerial的5G NR物理层处理集成在GPU上加速。

Result: 在超过700万个NR UL时隙的真实数据上测试，干扰检测准确率超过91%，响应时间低于650微秒。

Conclusion: InterfO-RAN展示了在密集部署中维持网络性能的强大干扰检测能力，为5G网络提供了有效的解决方案。

Abstract: Ultra-dense fifth generation (5G) and beyond networks leverage spectrum
sharing and frequency reuse to enhance throughput, but face unpredictable
in-band uplink (UL) interference challenges that significantly degrade Signal
to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases
(gNBs). This is particularly problematic at cell edges, where overlapping
regions force User Equipments (UEs) to increase transmit power, and in
directional millimeter wave systems, where beamforming sidelobes can create
unexpected interference. The resulting signal degradation disrupts protocol
operations, including scheduling and resource allocation, by distorting quality
indicators like Reference Signal Received Power (RSRP) and Received Signal
Strength Indicator (RSSI), and can compromise critical functions such as
channel state reporting and Hybrid Automatic Repeat Request (HARQ)
acknowledgments. To address this problem, this article introduces InterfO-RAN,
a real-time programmable solution that leverages a Convolutional Neural Network
(CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical
layer, detecting in-band interference with accuracy exceeding 91% in under 650
us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics
Processing Unit (GPU), coexisting with the 5G NR physical layer processing of
NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial
Radio Units (RUs) and smartphones, our solution was trained and tested on more
than 7 million NR UL slots collected from real-world environments,
demonstrating robust interference detection capabilities essential for
maintaining network performance in dense deployments.

</details>


### [17] [Optimal Packetization Towards Low Latency in Random Access Networks (extended version)](https://arxiv.org/abs/2507.23286)
*Zihong Li,Anshan Yuan,Xinghua Sun*

Main category: cs.NI

TL;DR: 本文研究了随机接入网络中的排队延迟性能，重点分析了数据包化对以秒为单位的平均排队延迟的影响，提出了优化数据包大小以减少延迟的策略，并通过数值方法和仿真验证了结果。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍将数据包视为原子传输单元，忽略了数据包化对排队延迟的影响，尤其是以秒为单位的平均延迟这一更精确且实用的指标。

Method: 建立了数据包化与平均排队延迟之间的数学关系，通过数值方法找到最优数据包大小，并通过仿真研究了数据包化对延迟抖动的影响。

Result: 确定了最优数据包大小，分析了网络参数的影响，并从数据包化角度重新评估了无连接与连接方案的权衡。

Conclusion: 数据包化对排队延迟有显著影响，优化数据包大小可以有效降低延迟，为随机接入网络的性能提升提供了新视角。

Abstract: As the demand for low-latency services grows, ensuring the delay performance
of random access (RA) networks has become a priority. Existing studies on the
queueing delay performance of the Aloha model universally treat packets as
atomic transmission units, focusing primarily on delay measured in time slots.
However, the impact of packetization on queueing delay has been consistently
overlooked, particularly for the mean queueing delay measured in seconds, which
serves as a more precise and practically relevant performance metric than its
slot-based counterpart. Here, packetization refers to the process of
determining the number of bits assembled into a packet. To optimize queueing
delay from the perspective of packetization, this paper establishes the
mathematical relationship between packetization and mean queueing delay in
seconds for both connection-free and connection-based Aloha schemes, and
explores the optimal packetization strategy to minimize this delay. We identify
the optimal mean queueing delay and its corresponding packet size via numerical
methods, and further analyze the influence of various network parameters. We
further use simulations to investigate the similar impact of packetization on
jitter of queueing delay. We then apply our analysis to re-evaluate the complex
trade-off between the connection-free and connection-based schemes through the
new perspective of packetization. Furthermore, recognizing that an analysis of
the queueing delay performance for RA-SDT in NTN scenarios, especially from a
packetization perspective, also remains an unexplored area, we apply the
analysis to this scenario as a case study.

</details>


### [18] [FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies](https://arxiv.org/abs/2507.23342)
*Laura Acosta García,Juan Aznar Poveda,Fabian Margreiter,Antonio-Javier García Sánchez,Joan García Haro,Thomas Fahringer,José Lorente López,José-Víctor Rodríguez*

Main category: cs.NI

TL;DR: FAST-LoRa是一种新型仿真框架，旨在快速高效地评估LoRaWAN网络和传输参数选择，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有仿真框架存在计算开销大和仿真时间长的问题，FAST-LoRa旨在提供轻量级且准确的近似工具。

Method: 通过依赖分析模型和高效矩阵运算，FAST-LoRa简化了计算，避免了复杂的包级仿真。

Result: FAST-LoRa在估计关键网络指标时表现出相似准确性，且计算时间显著减少。

Conclusion: FAST-LoRa是一种有效的工具，适用于稳定流量模式和上行通信场景中的传输参数评估。

Abstract: The Internet of Things (IoT) has transformed many industries, and LoRaWAN
(Long Range Wide Area Network), built on LoRa (Long Range) technology, has
become a crucial solution for enabling scalable, low-cost, and energy-efficient
communication in wide-area networks. Simulation tools are essential for
optimizing the transmission parameters and, therefore, the energy efficiency
and performance of LoRaWAN networks. While existing simulation frameworks
accurately replicate real-world scenarios by including multiple layers of
communication protocols, they often imply significant computational overhead
and simulation times. To address this issue, this paper introduces FAST-LoRa, a
novel simulation framework designed to enable fast and efficient evaluation of
LoRaWAN networks and selection of transmission parameters. FAST-LoRa
streamlines computation by relying on analytical models without complex
packet-level simulations and implementing gateway reception using efficient
matrix operations. Rather than aiming to replace discrete-event simulators,
FAST-LoRa is intended as a lightweight and accurate approximation tool for
evaluating transmission parameter strategies in scenarios with stable traffic
patterns and uplink-focused communications. In our evaluation, we compare
FAST-LoRa with a well-established simulator using multiple network
configurations with varying numbers of end devices and gateways. The results
show that FAST-LoRa achieves similar accuracy in estimating key network
metrics, even in complex scenarios with interference and multi-gateway
reception, with a Mean Absolute Error (MAE) of 0.940 $\times 10^{-2}$ for the
Packet Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while
significantly reducing computational time by up to three orders of magnitude.

</details>


### [19] [Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication](https://arxiv.org/abs/2507.23421)
*Sara Cavallero,Fabio Saggese,Junya Shiraishi,Israel Leyva-Mayorga,Shashi Raj Pandey,Chiara Buratti,Petar Popovski*

Main category: cs.NI

TL;DR: 本文提出了一种双模通信框架，结合查询驱动和事件驱动传输，通过自适应机制实现高效、及时的数据传输，并显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 无线设备通常面临能效和实时性挑战，传统方法难以同时满足查询驱动和事件驱动的需求。本文旨在设计一种统一框架，以优化资源分配和能耗。

Method: 提出了一种结合唤醒无线电机制和定制MAC协议的双模通信框架，支持查询驱动和事件驱动的数据传输，并进行系统级分析。

Result: 所提方法能耗降低30%，同时保持两类通信模式的可靠性，成功概率与资源分配相关。

Conclusion: 该框架在能效和实时性方面表现优异，为无线设备通信提供了一种高效解决方案。

Abstract: This paper introduces a dual-mode communication framework for wireless
devices that integrates query-driven (pull) and event-driven (push)
transmissions within a unified time-frame structure. Devices typically respond
to information requests in pull mode, but if an anomaly is detected, they
preempt the regular response to report the critical condition. Additionally,
push-based communication is used to proactively send critical data without
waiting for a request. This adaptive approach ensures timely, context-aware,
and efficient data delivery across different network conditions. To achieve
high energy efficiency, we incorporate a wake-up radio mechanism and we design
a tailored medium access control (MAC) protocol that supports data traffic
belonging to the different communication classes. A comprehensive system-level
analysis is conducted, accounting for the wake-up control operation and
evaluating three key performance metrics: the success probability of anomaly
reports (push traffic), the success probability of query responses (pull
traffic) and the total energy consumption. Numerical results characterize the
system's behavior and highlight the inherent trade-off in success probabilities
between push- and pull-based traffic as a function of allocated communication
resources. Our analysis demonstrates that the proposed approach reduces energy
consumption by up to 30% compared to a traditional approach, while maintaining
reliable support for both communication paradigms.

</details>


### [20] [From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks](https://arxiv.org/abs/2507.23433)
*Erfan Delfani,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 本文研究了通信网络中数据传播的时效性和信息性，提出了版本信息年龄（VAoI）的完整分布分析，解决了现有研究中以平均值为中心的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注数据时效性（如AoI），但忽略了内容的信息性，且多集中于平均值分析，缺乏对完整分布特别是多跳网络中的研究。

Method: 研究了单跳和多跳网络中VAoI的稳态分布，分析了随机静态、均匀和基于阈值的调度策略及其传输约束，推导了稳态分布和平均VAoI的闭式表达式。

Result: 解析确定了基于阈值调度的最优阈值及其对应最小VAoI。数值验证了分析结果，为高效通信网络设计提供了新思路。

Conclusion: 通过VAoI的完整分布分析，本文为通信网络设计提供了更全面的数据传播优化方法，填补了现有研究的空白。

Abstract: Timely and informative data dissemination in communication networks is
essential for enhancing system performance and energy efficiency, as it reduces
the transmission of outdated or redundant data. Timeliness metrics, such as Age
of Information (AoI), effectively quantify data freshness; however, these
metrics fail to account for the intrinsic informativeness of the content
itself. To address this limitation, content-based metrics have been proposed
that combine both timeliness and informativeness. Nevertheless, existing
studies have predominantly focused on evaluating average metric values, leaving
the complete distribution-particularly in multi-hop network scenarios-largely
unexplored. In this paper, we provide a comprehensive analysis of the
stationary distribution of the Version Age of Information (VAoI), a
content-based metric, under various scheduling policies, including randomized
stationary, uniform, and threshold-based policies, with transmission
constraints in single-hop and multi-hop networks. We derive closed-form
expressions for the stationary distribution and average VAoI under these
scheduling approaches. Furthermore, for threshold-based scheduling, we
analytically determine the optimal threshold value that minimizes VAoI and
derive the corresponding optimal VAoI in closed form. Numerical evaluations
verify our analytical findings, providing valuable insights into leveraging
VAoI in the design of efficient communication networks.

</details>


### [21] [Networked Physical Computing: A New Paradigm for Effective Task Completion via Hypergraph Aided Trusted Task-Resource Matching](https://arxiv.org/abs/2507.23556)
*Botao Zhu,Xianbin Wang*

Main category: cs.NI

TL;DR: 本文提出了一种基于超图的信任任务-资源匹配框架（TTR-matching），用于在复杂系统中实现高效的任务与资源匹配，以最大化任务完成的价值。


<details>
  <summary>Details</summary>
Motivation: 由于计算资源和任务的多样性，如何在复杂系统中实现高效的任务与资源匹配成为一项挑战性的问题。

Method: 提出了一种基于超图的TTR-matching框架，通过定义任务特定的信任资源超图和任务超图，设计超图匹配算法，实现任务与资源的高效匹配。

Result: 实验结果表明，该框架在识别可信合作伙伴和最大化任务完成价值方面优于其他算法。

Conclusion: 该研究为复杂系统中的任务与资源匹配提供了一种高效的解决方案，并通过实验验证了其优越性。

Abstract: Due to the diverse physical attributes of computing resources and tasks,
developing effective mechanisms to facilitate task and resource matching in
complex connected systems for value-oriented task completion has become
increasingly challenging. To address the challenge, this paper proposes a
networked physical computing system that integrates the physical attributes of
computing resources and tasks as well as task-specific trust relationships
among devices to enable value-driven task completion. Specifically, we propose
a state-of-the-art hypergraph-aided trusted task-resource matching
(TTR-matching) framework to achieve the envisioned physical computing. First, a
task-specific trusted physical resource hypergraph is defined, which integrates
task-specific trust, the physical attributes of resources, and task types. This
enables accurate modeling of device collaboration dependencies under specific
task types. Next, a task hypergraph is generated to associate the task
initiator with the physical attributes of the corresponding tasks. Based on
these two hypergraphs, a hypergraph matching algorithm is designed to
facilitate task-specific trusted collaborator selection and accurate
task-resource matching for value-maximizing task completion. Extensive
experimental results demonstrate that the proposed TTR-matching framework
outperforms comparison algorithms in identifying task-specific trustworthy
collaborators and maximizing the average value of task completion.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [Hybrid CNN-Mamba Enhancement Network for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2507.23444)
*Xiang Li,Xianfu Cheng,Xiaoming Zhang,Zhoujun Li*

Main category: cs.MM

TL;DR: 提出了一种名为HCMEN的新框架，用于处理多模态情感分析中缺失模态的问题，通过结合CNN和Mamba架构以及跨模态增强机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多模态情感分析中的缺失模态问题时效果有限，需要在模态对齐和融合方面进一步改进。

Method: HCMEN框架包含三个关键部分：分层单模态建模、跨模态增强与对齐、多模态混合融合，结合CNN和Mamba架构的优势。

Result: 在两个基准数据集上，HCMEN在多种缺失模态场景下均优于现有方法。

Conclusion: HCMEN通过创新的架构设计有效地解决了多模态情感分析中的缺失模态问题，取得了显著的性能提升。

Abstract: Multimodal Sentiment Analysis (MSA) with missing modalities has recently
attracted increasing attention. Although existing research mainly focuses on
designing complex model architectures to handle incomplete data, it still faces
significant challenges in effectively aligning and fusing multimodal
information. In this paper, we propose a novel framework called the Hybrid
CNN-Mamba Enhancement Network (HCMEN) for robust multimodal sentiment analysis
under missing modality conditions. HCMEN is designed around three key
components: (1) hierarchical unimodal modeling, (2) cross-modal enhancement and
alignment, and (3) multimodal mix-up fusion. First, HCMEN integrates the
strengths of Convolutional Neural Network (CNN) for capturing local details and
the Mamba architecture for modeling global contextual dependencies across
different modalities. Furthermore, grounded in the principle of Mutual
Information Maximization, we introduce a cross-modal enhancement mechanism that
generates proxy modalities from mixed token-level representations and learns
fine-grained token-level correspondences between modalities. The enhanced
unimodal features are then fused and passed through the CNN-Mamba backbone,
enabling local-to-global cross-modal interaction and comprehensive multimodal
integration. Extensive experiments on two benchmark MSA datasets demonstrate
that HCMEN consistently outperforms existing state-of-the-art methods,
achieving superior performance across various missing modality scenarios. The
code will be released publicly in the near future.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [23] [Explanations for Unrealizability of Infinite-State Safety Shields](https://arxiv.org/abs/2507.23603)
*Andoni Rodriguez,Irfansha Shaik,Davide Corsi,Roy Fox,Cesar Sanchez*

Main category: cs.LO

TL;DR: 论文提出了一种通过时间公式展开生成简单无条件或有条件解释的方法，用于解决屏蔽合成中的不可实现性问题。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习中，屏蔽（shielding）是一种常用方法，但在无限状态域（如连续环境）中，由于逻辑规范可能存在矛盾，屏蔽可能无法实现。本文旨在解决这一问题。

Method: 通过时间公式展开（temporal formula unrolling）技术生成见证不可实现性的简单无条件或条件解释。

Result: 展示了该技术的不同变体及其适用性。

Conclusion: 该方法能够有效识别并解释屏蔽合成中的不可实现性问题，提升了安全强化学习的实用性。

Abstract: Safe Reinforcement Learning focuses on developing optimal policies while
ensuring safety. A popular method to address such task is shielding, in which a
correct-by-construction safety component is synthesized from logical
specifications. Recently, shield synthesis has been extended to infinite-state
domains, such as continuous environments. This makes shielding more applicable
to realistic scenarios. However, often shields might be unrealizable because
the specification is inconsistent (e.g., contradictory). In order to address
this gap, we present a method to obtain simple unconditional and conditional
explanations that witness unrealizability, which goes by temporal formula
unrolling. In this paper, we show different variants of the technique and its
applicability.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [24] [Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls](https://arxiv.org/abs/2507.22889)
*Tom Sheffer,Alon Miron,Yaniv Dover,Ariel Goldstein*

Main category: cs.HC

TL;DR: 研究比较了不同对话配置（LLM-LLM、LLM三人组、人类三人组、人机混合组）的效果，发现人类参与的对话能提升准确性，而纯LLM组因知识相似性缺乏协同效应。知识多样性是协作改善的关键。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理之间是否能像人类对话那样产生协同效应，从而提升复杂问题解决的集体智慧。

Method: 系统地比较四种对话配置，分析对话前后个体的准确性变化及协作行为。

Result: 人类参与的对话能提升准确性，纯LLM组因知识相似性导致准确性下降。知识多样性是协作改善的关键。

Conclusion: AI开发应注重代理多样性而非单个模型性能，以在群体环境中实现更有效的协作。

Abstract: Conversations transform individual knowledge into collective insight,
allowing groups of humans and increasingly groups of artificial intelligence
(AI) agents to collaboratively solve complex problems. Whether interactions
between AI agents can replicate the synergy observed in human discussions
remains an open question. To investigate this, we systematically compared four
conversational configurations: pairs of large language models (LLM-LLM), trios
of LLMs, trios of humans, and mixed human-LLM pairs. After agents answered
questions individually, they engaged in open-ended discussions and then
reconsidered their initial answers. Interactions involving humans consistently
led to accuracy improvements after the conversations, benefiting both stronger
and weaker participants. By contrast, purely LLM-based pairs and trios
exhibited declines in accuracy, demonstrating limited conversational synergy.
Analysis of participants' confidence and answer-switching behavior revealed
that knowledge diversity is a critical factor enabling collaborative
improvement. Crucially, the lack of gains in LLM-LLM interactions did not stem
from a fundamental limitation of the models' ability to collaborate, but from
highly similar knowledge states that left little room for productive exchange.
Our findings argue for a paradigm shift in AI development: rather than
optimizing individual models solely for standalone performance, explicitly
cultivating diversity across agents, even at the cost of slightly lower
individual accuracy, may yield AI collaborators that are more effective in
group settings with humans or other AI systems.

</details>


### [25] [Evaluating LLMs for Visualization Generation and Understanding](https://arxiv.org/abs/2507.22890)
*Saadiq Rauf Khan,Vinit Chandak,Sougata Mukherjea*

Main category: cs.HC

TL;DR: 探讨大型语言模型（LLMs）在生成可视化代码和理解常见图表方面的能力与局限。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在信息可视化中的应用潜力，特别是通过简单提示生成代码和回答图表问题的能力。

Method: 测试不同流行LLMs生成可视化代码（如柱状图、饼图）和理解图表的能力，并分析其错误。

Result: LLMs能生成简单图表代码并回答基础问题，但在复杂图表（如小提琴图）和细节理解上表现不足。

Conclusion: 研究结果可用于改进LLMs和可视化系统，提升其生成和理解能力。

Abstract: Information Visualization has been utilized to gain insights from complex
data. In recent times, Large Language models (LLMs) have performed very well in
many tasks. In this paper, we showcase the capabilities of different popular
LLMs to generate code for visualization based on simple prompts. We also
analyze the power of LLMs to understand some common visualizations by answering
questions. Our study shows that LLMs could generate code for some simpler
visualizations such as bar and pie charts. Moreover, they could answer simple
questions about visualizations. However, LLMs also have several limitations.
For example, some of them had difficulty generating complex visualizations,
such as violin plot. LLMs also made errors in answering some questions about
visualizations, for example, identifying relationships between close boundaries
and determining lengths of shapes. We believe that our insights can be used to
improve both LLMs and Information Visualization systems.

</details>


### [26] [Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary](https://arxiv.org/abs/2507.23454)
*Marta Bieńkiewicz,Julia Ayache,Panayiotis Charalambous,Cristina Becchio,Marco Corragio,Bertram Taetz,Francesco De Lellis,Antonio Grotta,Anna Server,Daniel Rammer,Richard Kulpa,Franck Multon,Azucena Garcia-Palacios,Jessica Sutherland,Kathleen Bryson,Stéphane Donikian,Didier Stricker,Benoît Bardy*

Main category: cs.HC

TL;DR: 论文探讨混合现实（MR）技术中如何更真实地再现人类身体和社会运动交互的空白，提出通过多模态数据流和多智能体交互功能来改进。


<details>
  <summary>Details</summary>
Motivation: MR技术目前未能充分实现人类身体和社会运动的真实性，限制了其社交体验的质量。

Method: 提出了一个综合术语表，涵盖虚拟角色、自主化、负责任AI、设计伦理等主题，推动以人为中心的MR技术发展。

Result: 倡导增强人与虚拟自主代理之间的社会交互和协作，确保包容性、伦理设计和心理安全。

Conclusion: 呼吁推动MR技术的变革性进步，以促进更丰富的数字连接。

Abstract: This article explores a critical gap in Mixed Reality (MR) technology: while
advances have been made, MR still struggles to authentically replicate human
embodiment and socio-motor interaction. For MR to enable truly meaningful
social experiences, it needs to incorporate multi-modal data streams and
multi-agent interaction capabilities. To address this challenge, we present a
comprehensive glossary covering key topics such as Virtual Characters and
Autonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges
of Social MR within Neuroscience, Embodiment, and Technology. Our aim is to
drive the transformative evolution of MR technologies that prioritize
human-centric innovation, fostering richer digital connections. We advocate for
MR systems that enhance social interaction and collaboration between humans and
virtual autonomous agents, ensuring inclusivity, ethical design and
psychological safety in the process.

</details>


### [27] [Real-time energy monitoring infrastructure for residential collective self-consumption operations using Linky meter](https://arxiv.org/abs/2507.22891)
*Jérôme Ferrari,Benoit Delinchant,Frédéric Wurtz,Olga Rouchouze*

Main category: cs.HC

TL;DR: 本文介绍了一种新的开源基础设施，用于基于Linky电表数据的实时监控，以帮助参与者调整能源消费行为。


<details>
  <summary>Details</summary>
Motivation: 随着能源转型和能源价格上涨，法国集体自消费操作数量增加，但现有能源流量监控依赖历史数据，缺乏实时反馈。

Method: 开发了一种开源基础设施，包括xKy设备，应用于九名参与者的集体自消费操作，并部署了网关和实时监控网站。

Result: 项目实现了实时监控，旨在提高参与者的自消费率。

Conclusion: 该基础设施为集体自消费操作提供了实时数据支持，帮助参与者做出及时决策。

Abstract: As part of the energy transition and the rise in energy prices, the number of
collective self-consumption operations in France is steadily increasing.
However, energy flow monitoring currently relies on historical ''day+1'' data
provided by Linky meters, which does not offer real time feedback to help
participants adapt their energy consumption behaviors. This article introduces
a new open-source infrastructure for real-time monitoring based on Linky meter
data, enabling participants to make informed decisions and take timely actions.
It includes a description of the xKy device, applied to a collective
self-consumption operation involving nine participants, supported by the Energy
Transition Observatory (OTE). The project encompasses the implementation of
gateways in participants' homes and the development and operation of real-time
monitoring website, aimed at increasing participants' self-consumption rate.

</details>


### [28] [Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation](https://arxiv.org/abs/2507.22892)
*Ismail Hossain,Mridul Banik*

Main category: cs.HC

TL;DR: 论文提出了一种结合脑电图（EEG）和大型语言模型（LLM）的混合框架，为语言康复提供实时个性化的辅助。


<details>
  <summary>Details</summary>
Motivation: 传统的辅助沟通系统无法实时适应用户的认知和语言需求，特别是在神经疾病（如中风后失语症）中。

Method: 利用EEG脑机接口（BCI）捕捉用户神经意图，结合LLM生成个性化语言内容，构建实时语言康复辅助系统。

Result: 系统可实现通过思维指令导航语言学习模块，动态调整词汇和练习，并根据认知努力调整任务难度。

Conclusion: 混合框架为严重语言或运动障碍患者提供了更个性化、低疲劳的语言康复支持。

Abstract: Conventional augmentative and alternative communication (AAC) systems and
language-learning platforms often fail to adapt in real time to the user's
cognitive and linguistic needs, especially in neurological conditions such as
post-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in
noninvasive electroencephalography (EEG)--based brain-computer interfaces
(BCIs) and transformer--based large language models (LLMs) offer complementary
strengths: BCIs capture users' neural intent with low fatigue, while LLMs
generate contextually tailored language content. We propose and evaluate a
novel hybrid framework that leverages real-time EEG signals to drive an
LLM-powered language rehabilitation assistant. This system aims to: (1) enable
users with severe speech or motor impairments to navigate language-learning
modules via mental commands; (2) dynamically personalize vocabulary,
sentence-construction exercises, and corrective feedback; and (3) monitor
neural markers of cognitive effort to adjust task difficulty on the fly.

</details>


### [29] [Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure](https://arxiv.org/abs/2507.22893)
*Giuseppe Riva*

Main category: cs.HC

TL;DR: 论文提出“认知基础设施研究”（CIS）作为新领域，将AI视为“认知基础设施”，探讨其如何重塑人类认知和社会认识论，并提出方法论创新。


<details>
  <summary>Details</summary>
Motivation: 当代人机交互研究忽视了AI如何潜意识重塑人类认知，亟需新框架理解分布式认知。

Method: 通过叙事场景和“基础设施分解方法”，研究AI预处理对个体、集体和社会认知的影响。

Result: AI基础设施通过自动化“相关性判断”转移认知代理权，影响公共推理和社会认识论。

Conclusion: CIS填补了学科间的空白，为研究算法隐性影响提供了新方法。

Abstract: Contemporary human-AI interaction research overlooks how AI systems
fundamentally reshape human cognition pre-consciously, a critical blind spot
for understanding distributed cognition. This paper introduces "Cognitive
Infrastructure Studies" (CIS) as a new interdisciplinary domain to
reconceptualize AI as "cognitive infrastructures": foundational, often
invisible systems conditioning what is knowable and actionable in digital
societies. These semantic infrastructures transport meaning, operate through
anticipatory personalization, and exhibit adaptive invisibility, making their
influence difficult to detect. Critically, they automate "relevance judgment,"
shifting the "locus of epistemic agency" to non-human systems. Through
narrative scenarios spanning individual (cognitive dependency), collective
(democratic deliberation), and societal (governance) scales, we describe how
cognitive infrastructures reshape human cognition, public reasoning, and social
epistemologies. CIS aims to address how AI preprocessing reshapes distributed
cognition across individual, collective, and cultural scales, requiring
unprecedented integration of diverse disciplinary methods. The framework also
addresses critical gaps across disciplines: cognitive science lacks
population-scale preprocessing analysis capabilities, digital sociology cannot
access individual cognitive mechanisms, and computational approaches miss
cultural transmission dynamics. To achieve this goal CIS also provides
methodological innovations for studying invisible algorithmic influence:
"infrastructure breakdown methodologies", experimental approaches that reveal
cognitive dependencies by systematically withdrawing AI preprocessing after
periods of habituation.

</details>


### [30] [When no one shows up (at first): Navigating the uncertainties of participatory workshops in interdisciplinary research](https://arxiv.org/abs/2507.22894)
*Monique Munarini*

Main category: cs.HC

TL;DR: 论文探讨了设计和参与式工作坊中的挑战，为早期职业研究者提供了实用策略。通过案例分析，展示了即使未达预期，工作坊仍能促进丰富讨论和知识再分配。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示设计和参与式工作坊中未被充分讨论的挑战，并为早期职业研究者提供实践指导。

Method: 通过个人经验案例分析，从概念化到实施的全程描述，重点关注非专家参与的挑战。

Result: 尽管初期遇到低参与率等问题，工作坊成功促成了多样化讨论，并有参与者主动成为共同组织者。

Conclusion: 将失败视为学习机会，为跨学科研究者提供了实用策略，推动了参与式工作的实践讨论。

Abstract: This reflective paper explores often-unspoken challenges of designing and
facilitating co-design and participatory workshops, offering practical
strategies for early career researchers (ECRs) navigating these methods.
Drawing from personal experience conducting a series of workshops titled: How
to Think About Equity in the AI Ecosystem. It follows the full arc of the
workshop experience, from conceptualization and activity planning to
participant recruitment and facilitation, offering a grounded account of what
happens when participation does not go as expected. The paper examines the
methodological challenges of engaging non-expert participants, particularly
when operating without institutional support, financial incentives, or
integration into larger events. Despite initial difficulties such as low
attendance, the workshop fostered rich discussions among a demographically
diverse group and ultimately led to one participant volunteering to
co-facilitate a subsequent session. This transition from participant to
co-facilitator exemplifies the redistribution of epistemic authority,
positioning lived experience as central to research and engagement practices.
By reframing perceived failure as a productive site of learning, the paper
offers practical strategies for ECRs working across disciplines who often
navigate unfamiliar methodological terrains, contributing to broader
conversations on the realities of doing interdisciplinary, participatory work
in practice.

</details>


### [31] [Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface](https://arxiv.org/abs/2507.22895)
*Ye Sun,Bowei Zhao,Dezhong Yao,Rui Zhang,Bohan Zhang,Xiaoyuan Li,Jing Wang,Mingxuan Qu,Gang Liu*

Main category: cs.HC

TL;DR: 该论文提出了一种基于脑肌接口（BMuI）的新运动意图提取框架，通过模拟从大脑到肌肉的神经通路来增强运动意图信号，使用EMG作为高保真中继介质，提高了意图识别的准确性。通过离线和在线实验验证了其可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于运动的BCI范式（如运动想象BCI）在真实场景中面临标签不精确的问题，导致伪标签和解码准确性下降，影响了系统的鲁棒性。

Method: 提出脑肌接口（BMuI）框架，模拟大脑到肌肉的神经通路，利用EMG作为中继介质以提高信号质量和意图识别的准确性。通过离线和在线实验验证方法。

Result: 实验表明BMuI可行，预测准确率达0.8314；在线实验中，所有参与者均能成功控制Unity虚拟手臂。

Conclusion: BMuI框架有效解决了传统BCI中运动意图信号提取不准确的问题，为运动意图识别提供了更可靠的方法。

Abstract: Brain-computer interfaces (BCIs) enable real-time interaction between the
brain and external devices by decoding neural signals. However, existing
motor-based BCI paradigms, like motor imagery BCI, face challenges with
imprecise labeling in real-world use. This mismatch between EEG signals and
true behavioral intentions leads to pseudo-labels, undermining decoding
accuracy and system robustness. To overcome this bottleneck, this paper first
proposes a novel motor intention extraction framework based on a non-invasive
brain-muscle interface (BMuI)($\text{BCI} =
\frac{\text{Brain}}{\text{Computer}} \text{ Interface} =
\frac{\text{Brain}}{\not\text{Muscle}}\! \text{ (BMuI)} \times
\!\frac{\not\text{Muscle}}{\text{Computer}}\! \text{ Interface}$). This method
simulates the neural pathway from the brain to the muscles in order to capture
and enhance the weak motor intention signals originating in the brain. It then
uses EMG as a high-fidelity relay medium to achieve more accurate intention
recognition and transmission. To systematically validate the feasibility and
effectiveness of this approach, we conducted both offline experiments (to
repeatedly verify feasibility) and online experiments (to construct a real-time
interactive system and evaluate its performance). The results show that BMuI is
feasible, achieving a prediction accuracy of 0.8314; in the online experiment,
all participants are able to successfully control the Unity virtual arm.

</details>


### [32] [iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement](https://arxiv.org/abs/2507.22896)
*Kohou Wang,ZhaoXiang Liu,Lin Bai,Kun Fan,Xiang Liu,Huan Hu,Kai Wang,Shiguo Lian*

Main category: cs.HC

TL;DR: 论文提出了一种基于多模态大语言模型（MLLM）的交互式学习机器人系统，能够通过与非专家用户的自然对话学习，并通过问题链和双模态检索模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 机器人部署后可能遇到新场景，需要通过交互学习提升适应性和性能。

Method: 结合多模态大语言模型，设计问题链和双模态检索模块，从用户对话中学习。

Result: 实验证明，系统在定量和定性上均表现出性能提升。

Conclusion: 该系统为机器人交互学习提供了新途径，增强了其在多样化环境中的适应能力。

Abstract: It is crucial that robots' performance can be improved after deployment, as
they are inherently likely to encounter novel scenarios never seen before. This
paper presents an innovative solution: an interactive learning-based robot
system powered by a Multi-modal Large Language Model(MLLM). A key feature of
our system is its ability to learn from natural dialogues with non-expert
users. We also propose chain of question to clarify the exact intent of the
question before providing an answer and dual-modality retrieval modules to
leverage these interaction events to avoid repeating same mistakes, ensuring a
seamless user experience before model updates, which is in contrast to current
mainstream MLLM-based robotic systems. Our system marks a novel approach in
robotics by integrating interactive learning, paving the way for superior
adaptability and performance in diverse environments. We demonstrate the
effectiveness and improvement of our method through experiments, both
quantitively and qualitatively.

</details>


### [33] [RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems](https://arxiv.org/abs/2507.22897)
*Luyu Chen,Quanyu Dai,Zeyu Zhang,Xueyang Feng,Mingyu Zhang,Pengcheng Tang,Xu Chen,Yue Zhu,Zhenhua Dong*

Main category: cs.HC

TL;DR: 提出RecUserSim，一种基于LLM的用户模拟器，通过多模块设计增强模拟真实性和多样性，并提供显式评分，优化CRS评估。


<details>
  <summary>Details</summary>
Motivation: 现有CRS评估方法中，用户模拟器的真实性和多样性不足，且缺乏显式评分机制。

Method: RecUserSim包含四个模块：用户画像、记忆管理、核心决策（基于有限理性理论）和响应微调。

Result: 实验表明，RecUserSim能生成多样且可控的输出，并产生高质量对话，评分在不同LLM上一致性高。

Conclusion: RecUserSim有效提升了CRS评估的准确性，且适用于多种基LLM。

Abstract: Conversational recommender systems (CRS) enhance user experience through
multi-turn interactions, yet evaluating CRS remains challenging. User
simulators can provide comprehensive evaluations through interactions with CRS,
but building realistic and diverse simulators is difficult. While recent work
leverages large language models (LLMs) to simulate user interactions, they
still fall short in emulating individual real users across diverse scenarios
and lack explicit rating mechanisms for quantitative evaluation. To address
these gaps, we propose RecUserSim, an LLM agent-based user simulator with
enhanced simulation realism and diversity while providing explicit scores.
RecUserSim features several key modules: a profile module for defining
realistic and diverse user personas, a memory module for tracking interaction
history and discovering unknown preferences, and a core action module inspired
by Bounded Rationality theory that enables nuanced decision-making while
generating more fine-grained actions and personalized responses. To further
enhance output control, a refinement module is designed to fine-tune final
responses. Experiments demonstrate that RecUserSim generates diverse,
controllable outputs and produces realistic, high-quality dialogues, even with
smaller base LLMs. The ratings generated by RecUserSim show high consistency
across different base LLMs, highlighting its effectiveness for CRS evaluation.

</details>


### [34] [Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment](https://arxiv.org/abs/2507.22898)
*Julian Acosta,Scott Adams,Julius Kernbach,Romain Hardy,Sung Eun Kim,Luyang Luo,Xiaoman Zhang,Shreya Johri,Mohammed Baharoon,Pranav Rajpurkar*

Main category: cs.HC

TL;DR: 开发了一个语音驱动的AI系统，辅助非专业人员通过自然对话进行专业级中风评估，提升紧急护理中的中风识别准确性。


<details>
  <summary>Details</summary>
Motivation: 当前急救人员的中风识别不一致且常不准确，敏感性低至58%，导致治疗延误。

Method: 使用语音AI系统，由非医疗志愿者评估模拟中风患者，测量准确性、完成时间、用户信心等。

Result: AI系统正确识别84%中风征兆，检测75%大血管闭塞，用户信心高，但存在误判。专家复核后诊断准确率达100%，但仅40%自信做出初步治疗决定。

Conclusion: AI系统虽需人工监督，但未来可能实现高精度评估，提升紧急医疗水平。

Abstract: We developed a voice-driven artificial intelligence (AI) system that guides
anyone - from paramedics to family members - through expert-level stroke
evaluations using natural conversation, while also enabling smartphone video
capture of key examination components for documentation and potential expert
review. This addresses a critical gap in emergency care: current stroke
recognition by first responders is inconsistent and often inaccurate, with
sensitivity for stroke detection as low as 58%, causing life-threatening delays
in treatment. Three non-medical volunteers used our AI system to assess ten
simulated stroke patients, including cases with likely large vessel occlusion
(LVO) strokes and stroke-like conditions, while we measured diagnostic
accuracy, completion times, user confidence, and expert physician review of the
AI-generated reports. The AI system correctly identified 84% of individual
stroke signs and detected 75% of likely LVOs, completing evaluations in just
over 6 minutes. Users reported high confidence (median 4.5/5) and ease of use
(mean 4.67/5). The system successfully identified 86% of actual strokes but
also incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert
physician reviewed the AI reports with videos, they identified the correct
diagnosis in 100% of cases, but felt confident enough to make preliminary
treatment decisions in only 40% of cases due to observed AI errors including
incorrect scoring and false information. While the current system's limitations
necessitate human oversight, ongoing rapid advancements in speech-to-speech AI
models suggest that future versions are poised to enable highly accurate
assessments. Achieving human-level voice interaction could transform emergency
medical care, putting expert-informed assessment capabilities in everyone's
hands.

</details>


### [35] [A visual analytics tool for taxonomy-based trajectory data exploration](https://arxiv.org/abs/2507.22899)
*Ivan A. Hanono Cozzetti,Ahmad Abdou*

Main category: cs.HC

TL;DR: 该论文提出了一种结合数据可视化和统计计算的多层次分析工具，用于处理复杂的时空数据，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 时空数据的复杂性和异质性使得其分析具有挑战性，因此需要一种能够分类和分析这些数据的方法。

Method: 使用机器学习模型对运动对象进行分类，并结合数据可视化与统计计算，通过多层次方法分析数据。

Result: 在两个案例研究中，该方法成功分类了北极狐的行为和热带气旋的轨迹，并揭示了其统计特征。

Conclusion: 该方法和工具证明时空数据可以被详细分析和解释，适用于多个领域的理论与实践应用。

Abstract: The analysis of spatio-temporal data presents significant challenges due to
the complexity and heterogeneity of movement patterns. This project proposes a
data analytics tool that combines data visualization and statistical
computation to facilitate spatio-temporal data analysis through a multi-level
approach. The tool categorizes moving objects into distinct taxonomies using
Machine Learning models, adding meaningful structure to the analysis. Two case
studies demonstrate the methodology's effectiveness. The first analyzed Arctic
fox trajectories, successfully identifying and labeling foxes with Geometric or
Kinematic-based behaviors, further categorized into Curvature and Acceleration
groups. Statistical indicators revealed that foxes with Acceleration-based
behavior showed constant, steady acceleration, while those with Curvature-based
behavior exhibited acceleration peaks and sudden deceleration. The second case
study examined tropical cyclone data, labeling trajectories with Speed,
Curvature, and hybrid Geometric-based behaviors through unique statistical
variables. Analysis of hybrid Geometric behavior (Curvature and Indentation
combined) identified specific angles with the highest impact on hurricane shape
and geometry. The proposed method and tool demonstrate that spatio-temporal
data, despite inherent complexity, can be analyzed and explained in detail,
providing a theoretical and practical blueprint applicable to multiple domains.

</details>


### [36] [Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants](https://arxiv.org/abs/2507.22900)
*Sergio Rojas-Galeano*

Main category: cs.HC

TL;DR: 研究表明，AI代码助手能帮助初学者理解代码和增强信心，但也可能导致依赖性和概念理解不足。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代码助手对初学者编程体验的影响。

Method: 分两部分的实验：使用AI辅助完成编程任务和无AI辅助扩展解决方案，收集20名学生的反馈。

Result: AI工具被认为有助于初始开发，但学生在无辅助任务中遇到知识迁移困难。

Conclusion: 需结合AI工具的教学策略，同时强化基础编程技能。

Abstract: This exploratory study examines how AI code assistants shape novice
programmers' experiences during a two-part exam in an introductory programming
course. In the first part, students completed a programming task with access to
AI support; in the second, they extended their solutions without AI. We
collected Likert-scale and open-ended responses from 20 students to evaluate
their perceptions and challenges. Findings suggest that AI tools were perceived
as helpful for understanding code and increasing confidence, particularly
during initial development. However, students reported difficulties
transferring knowledge to unaided tasks, revealing possible overreliance and
gaps in conceptual understanding. These insights highlight the need for
pedagogical strategies that integrate AI meaningfully while reinforcing
foundational programming skills.

</details>


### [37] [Accelerated and Optimized Search of Imperceptible Color Vibration for Embedding Information into LCD images](https://arxiv.org/abs/2507.22901)
*Shingo Hattori,Takefumi Hiraki*

Main category: cs.HC

TL;DR: 本文提出了一种加速和优化的颜色对搜索方法，用于在LCD图像中嵌入不可感知的颜色振动信息，并通过并行化搜索过程显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，使用颜色振动嵌入信息的颜色对搜索过程耗时过长，影响了实际应用效率。

Method: 通过并行化搜索过程，利用数组表示移动量，并提取满足条件的元素，优化了颜色对的搜索方法。

Result: 实验表明，该方法显著提高了搜索速度，并研究了在九种彩色图像中可叠加的信息量。

Conclusion: 所提出的方法高效且实用，适用于在LCD图像中嵌入不可感知的颜色振动信息。

Abstract: Large, high-resolution displays are installed throughout the city as public
displays. By superimposing invisible information on the images of these
displays, large numbers of devices with cameras and sensors can communicate
with the displays without prior pairing. Several applications have been
proposed, such as operating robots or communicating information to users by
displaying 2D codes on images. However, the display of 2D codes has the problem
of compromising the appearance of displayed content.
  Abe et al. proposed a method of communicating with devices by superimposing
invisible information using color vibration on images displayed on
off-the-shelf liquid-crystal displays (LCD). Using this method, we can embed
the information for devices in images without interfering with the displayed
content. Abe et al. uses a simple serial loop operation to search for color
pairs comprising a color vibration, which requires a very long processing time
due to the huge search space.
  In this paper, we propose an accelerated and optimized search method for
color pairs that constitute the imperceptible color vibration for embedding
information on LCD images. To achieve fast color pair search, we parallelized
the search process, which is previously done individually, by using arrays
representing the amount of movement and an operation to extract elements from
the array that satisfy the conditions. In addition, we investigate the amount
of information that can be superimposed on nine color images using the
imperceptible color vibration and clarify the applicability of embedding
information into images using the color vibration.

</details>


### [38] [Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting](https://arxiv.org/abs/2507.22902)
*Hashim Hayat,Maksim Kudrautsau,Evgeniy Makarov,Vlad Melnichenko,Tim Tsykunou,Piotr Varaksin,Matt Pavelle,Adam Z. Oskowitz*

Main category: cs.HC

TL;DR: AI医生系统在虚拟急诊环境中表现出与人类医生相当甚至更优的诊断和治疗一致性，为解决医疗资源短缺提供了潜在解决方案。


<details>
  <summary>Details</summary>
Motivation: 全球预计到2030年将短缺1100万医疗从业者，且临床时间的50%被行政负担占用，AI有望缓解此问题，但尚未有完整自主的LLM AI系统在真实临床实践中经过严格评估。

Method: 回顾性比较了多代理LLM AI系统Doctronic与认证临床医生在500次虚拟急诊中的表现，主要评估诊断一致性、治疗方案一致性和安全性。

Result: AI与人类医生的诊断匹配率达81%，治疗方案一致率达99.2%。专家评审显示，AI在36.1%不一致案例中表现更优，人类在9.3%中更优。

Conclusion: 研究表明多代理AI系统在临床决策上与人类医生相当，部分情况下表现更优，为解决医疗资源短缺提供了可行方案。

Abstract: Background: Globally we face a projected shortage of 11 million healthcare
practitioners by 2030, and administrative burden consumes 50% of clinical time.
Artificial intelligence (AI) has the potential to help alleviate these
problems. However, no end-to-end autonomous large language model (LLM)-based AI
system has been rigorously evaluated in real-world clinical practice. In this
study, we evaluated whether a multi-agent LLM-based AI framework can function
autonomously as an AI doctor in a virtual urgent care setting. Methods: We
retrospectively compared the performance of the multi-agent AI system Doctronic
and board-certified clinicians across 500 consecutive urgent-care telehealth
encounters. The primary end points: diagnostic concordance, treatment plan
consistency, and safety metrics, were assessed by blinded LLM-based
adjudication and expert human review. Results: The top diagnosis of Doctronic
and clinician matched in 81% of cases, and the treatment plan aligned in 99.2%
of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not
supported by clinical findings). In an expert review of discordant cases, AI
performance was superior in 36.1%, and human performance was superior in 9.3%;
the diagnoses were equivalent in the remaining cases. Conclusions: In this
first large-scale validation of an autonomous AI doctor, we demonstrated strong
diagnostic and treatment plan concordance with human clinicians, with AI
performance matching and in some cases exceeding that of practicing clinicians.
These findings indicate that multi-agent AI systems achieve comparable clinical
decision-making to human providers and offer a potential solution to healthcare
workforce shortages.

</details>


### [39] [A blessing or a burden? Exploring worker perspectives of using a social robot in a church](https://arxiv.org/abs/2507.22903)
*Andrew Blair,Peggy Gregory,Mary Ellen Foster*

Main category: cs.HC

TL;DR: 研究探讨了社交机器人在教会等非营利组织中的应用，发现参与者对其使用有不同看法，强调潜在的社交价值和任务优化。


<details>
  <summary>Details</summary>
Motivation: 关注机器人在非营利组织中的应用，而非传统的盈利驱动场景。

Method: 通过与教会合作，对15名利益相关者进行访谈，并采用反思性主题分析法。

Result: 参与者对机器人使用反应不一，认可任务优化和信息提供的潜力，但也担忧情感责任和意外后果。

Conclusion: 引入机器人时需综合考虑社交和无形的价值观，而不仅是经济利益。

Abstract: Recent technological advances have allowed robots to assist in the service
sector, and consequently accelerate job and sector transformation. Less
attention has been paid to the use of robots in real-world organisations where
social benefits, as opposed to profits, are the primary motivator. To explore
these opportunities, we have partnered with a working church and visitor
attraction. We conducted interviews with 15 participants from a range of
stakeholder groups within the church to understand worker perspectives of
introducing a social robot to the church and analysed the results using
reflexive thematic analysis. Findings indicate mixed responses to the use of a
robot, with participants highlighting the empathetic responsibility the church
has towards people and the potential for unintended consequences. However,
information provision and alleviation of menial or mundane tasks were
identified as potential use cases. This highlights the need to consider not
only the financial aspects of robot introduction, but also how social and
intangible values shape what roles a robot should take on within an
organisation.

</details>


### [40] [SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches](https://arxiv.org/abs/2507.22904)
*Ehsan Latif,Zirak Khan,Xiaoming Zhai*

Main category: cs.HC

TL;DR: SketchMind是一个基于认知科学的多智能体框架，用于评估和改进学生的科学草图，通过模块化代理实现个性化、透明的评估，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 科学草图的自动化评估具有挑战性，现有方法缺乏可解释性和教学对齐，难以适应不同认知水平。

Method: 采用模块化代理架构，包括评分标准解析、草图感知、认知对齐和反馈修改，结合SRG提升性能。

Result: 与基线相比，SketchMind的准确率提升21.4%，人类评价显著高于传统模型，专家认可其支持概念成长的能力。

Conclusion: SketchMind为AI驱动的教育研究提供了可复现的框架和数据集，具有广阔的应用潜力。

Abstract: Scientific sketches (e.g., models) offer a powerful lens into students'
conceptual understanding, yet AI-powered automated assessment of such
free-form, visually diverse artifacts remains a critical challenge. Existing
solutions often treat sketch evaluation as either an image classification task
or monolithic vision-language models, which lack interpretability, pedagogical
alignment, and adaptability across cognitive levels. To address these
limitations, we present SketchMind, a cognitively grounded, multi-agent
framework for evaluating and improving student-drawn scientific sketches.
SketchMind comprises modular agents responsible for rubric parsing, sketch
perception, cognitive alignment, and iterative feedback with sketch
modification, enabling personalized and transparent evaluation. We evaluate
SketchMind on a curated dataset of 3,575 student-generated sketches across six
science assessment items with different highest order of Bloom's level that
require students to draw models to explain phenomena. Compared to baseline
GPT-4o performance without SRG (average accuracy: 55.6%), and with SRG
integration achieves 77.1% average accuracy (+21.4% average absolute gain). We
also demonstrate that multi-agent orchestration with SRG enhances SketchMind
performance, for example, GPT-4.1 gains an average 8.9% increase in sketch
prediction accuracy, outperforming single-agent pipelines across all items.
Human evaluators rated the feedback and co-created sketches generated by
\textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5,
significantly higher than those of baseline models (e.g., 2.3 for GPT-4o).
Experts noted the system's potential to meaningfully support conceptual growth
through guided revision. Our code and (pending approval) dataset will be
released to support reproducibility and future research in AI-driven education.

</details>


### [41] [Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction](https://arxiv.org/abs/2507.22905)
*Qiaoqiao Ren,Tony Belpaeme*

Main category: cs.HC

TL;DR: 研究表明，LLMs（如GPT-3.5、GPT-4和GPT-4o）在机器人系统中生成的触觉行为能够部分传递情感，但文化适配性和交互角色影响其效果。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在机器人系统中生成适应不同文化的触觉行为以传递情感的潜力。

Method: 生成针对12种情感和3种文化背景的触觉行为描述，并由90名参与者评估其情感解码和适宜性。

Result: 文化匹配时能解码6种情感；人机交互角色影响适宜性；攻击性或模糊行为易被视为不适宜；文化差异降低解码准确性。

Conclusion: LLMs生成的情感触觉行为需要进一步优化文化适配性和情感表达清晰度。

Abstract: As large language models (LLMs) become increasingly integrated into robotic
systems, their potential to generate socially and culturally appropriate
affective touch remains largely unexplored. This study investigates whether
LLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive
tactile behaviours to convey emotions in human-robot interaction. We produced
text based touch descriptions for 12 distinct emotions across three cultural
contexts (Chinese, Belgian, and unspecified), and examined their
interpretability in both robot-to-human and human-to-robot scenarios. A total
of 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified)
evaluated these LLM-generated tactile behaviours for emotional decoding and
perceived appropriateness. Results reveal that: (1) under matched cultural
conditions, participants successfully decoded six out of twelve emotions-mainly
socially oriented emotions such as love and Ekman emotions such as anger,
however, self-focused emotions like pride and embarrassment were more difficult
to interpret; (2) tactile behaviours were perceived as more appropriate when
directed from human to robot than from robot to human, revealing an asymmetry
in social expectations based on interaction roles; (3) behaviours interpreted
as aggressive (e.g., anger), overly intimate (e.g., love), or emotionally
ambiguous (i.e., not clearly decodable) were significantly more likely to be
rated as inappropriate; and (4) cultural mismatches reduced decoding accuracy
and increased the likelihood of behaviours being judged as inappropriate.

</details>


### [42] [Automated Label Placement on Maps via Large Language Models](https://arxiv.org/abs/2507.22952)
*Harry Shomer,Jiejun Xu*

Main category: cs.HC

TL;DR: 提出了一种基于大语言模型（LLMs）的自动标签放置新方法，利用检索增强生成技术（RAG）整合标注指南，并在真实地图数据集MAPLE上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有自动标签放置系统难以整合制图规范、适应上下文或理解标注指令的问题。

Method: 将标签放置任务建模为数据编辑问题，通过RAG检索相关标注指南，结合指令调优的LLMs生成理想标签坐标。

Result: 在MAPLE数据集上评估了四种开源LLMs，显示其在结构化提示和领域特定检索引导下能生成符合专家制图标准的标签。

Conclusion: 展示了基础模型在结构化数据编辑任务中的潜力，为AI辅助地图完成提供了可扩展框架。

Abstract: Label placement is a critical aspect of map design, serving as a form of
spatial annotation that directly impacts clarity and interpretability. Despite
its importance, label placement remains largely manual and difficult to scale,
as existing automated systems struggle to integrate cartographic conventions,
adapt to context, or interpret labeling instructions. In this work, we
introduce a new paradigm for automatic label placement (ALP) that formulates
the task as a data editing problem and leverages large language models (LLMs)
for context-aware spatial annotation. To support this direction, we curate
MAPLE, the first known benchmarking dataset for evaluating ALP on real-world
maps, encompassing diverse landmark types and label placement annotations from
open-source data. Our method retrieves labeling guidelines relevant to each
landmark type leveraging retrieval-augmented generation (RAG), integrates them
into prompts, and employs instruction-tuned LLMs to generate ideal label
coordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall
performance and generalization across different types of landmarks. This
includes both zero-shot and instruction-tuned performance. Our results
demonstrate that LLMs, when guided by structured prompts and domain-specific
retrieval, can learn to perform accurate spatial edits, aligning the generated
outputs with expert cartographic standards. Overall, our work presents a
scalable framework for AI-assisted map finishing and demonstrates the potential
of foundation models in structured data editing tasks. The code and data can be
found at https://github.com/HarryShomer/MAPLE.

</details>


### [43] [ChatVis: Large Language Model Agent for Generating Scientific Visualizations](https://arxiv.org/abs/2507.23096)
*Tom Peterka,Tanwi Mallick,Orcun Yildiz,David Lenz,Cory Quammen,Berk Geveci*

Main category: cs.HC

TL;DR: 论文介绍了ChatVis，一个帮助LLM生成ParaView科学可视化Python代码的助手，无需重新训练或微调LLM，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在高度专业化的科学可视化任务中表现不佳的问题。

Method: 采用链式思维提示简化、基于向量数据库的检索增强提示生成、以及错误检查与迭代反馈。

Result: ChatVis显著提升了所有评估指标，优于未辅助的LLM。

Conclusion: ChatVis有效提升了LLM在科学可视化任务中的能力，无需额外训练。

Abstract: Large language models (LLMs) are rapidly increasing in capability, but they
still struggle with highly specialized programming tasks such as scientific
visualization. We present an LLM assistant, ChatVis, that aids the LLM to
generate Python code for ParaView scientific visualization tasks, without the
need for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought
prompt simplification, retrieval-augmented prompt generation using a vector
database of documentation and code examples, and error checking with iterative
prompt feedback to correct errors until a visualization is produced. An
integral part of our approach is a benchmark suite of canonical visualization
tasks, ParaView regression tests, and scientific use cases that includes
comprehensive evaluation metrics. We evaluate our visualization assistant by
comparing results with a variety of top-performing unassisted LLMs. We find
that all the metrics are significantly improved with ChatVis.

</details>


### [44] [Accessibility Scout: Personalized Accessibility Scans of Built Environments](https://arxiv.org/abs/2507.23190)
*William Huang,Xia Su,Jon E. Froehlich,Yang Zhang*

Main category: cs.HC

TL;DR: 论文提出了一种基于大型语言模型（LLM）的自动化系统Accessibility Scout，用于通过照片识别建筑环境的无障碍问题，并结合用户个性化需求进行扫描。


<details>
  <summary>Details</summary>
Motivation: 传统的手动评估方法效率低且难以扩展，而现有机器学习方法又忽略个体需求，因此希望结合LLM的先进技术，实现更个性化和可扩展的无障碍评估。

Method: 开发了Accessibility Scout系统，通过用户反馈和AI协作学习，逐步优化针对个体需求的扫描能力。进行了三项研究，包括设计调研、技术评估和用户测试。

Result: 技术评估和用户研究表明，该系统能够生成超越传统ADA标准的个性化无障碍扫描报告。

Conclusion: 研究展示了AI在提升无障碍评估个性化与可扩展性方面的潜力，并提出了未来改进方向。

Abstract: Assessing the accessibility of unfamiliar built environments is critical for
people with disabilities. However, manual assessments, performed by users or
their personal health professionals, are laborious and unscalable, while
automatic machine learning methods often neglect an individual user's unique
needs. Recent advances in Large Language Models (LLMs) enable novel approaches
to this problem, balancing personalization with scalability to enable more
adaptive and context-aware assessments of accessibility. We present
Accessibility Scout, an LLM-based accessibility scanning system that identifies
accessibility concerns from photos of built environments. With use,
Accessibility Scout becomes an increasingly capable "accessibility scout",
tailoring accessibility scans to an individual's mobility level, preferences,
and specific environmental interests through collaborative Human-AI
assessments. We present findings from three studies: a formative study with six
participants to inform the design of Accessibility Scout, a technical
evaluation of 500 images of built environments, and a user study with 10
participants of varying mobility. Results from our technical evaluation and
user study show that Accessibility Scout can generate personalized
accessibility scans that extend beyond traditional ADA considerations. Finally,
we conclude with a discussion on the implications of our work and future steps
for building more scalable and personalized accessibility assessments of the
physical world.

</details>


### [45] [Silent Impact: Tracking Tennis Shots from the Passive Arm](https://arxiv.org/abs/2507.23215)
*Junyong Park,Saelyne Yang,Sungho Jo*

Main category: cs.HC

TL;DR: 论文提出了一种名为Silent Impact的系统，通过在非主导手臂上佩戴传感器，利用神经网络分析网球击球动作，实现了88.2%的分类准确率和86.0%的检测F1分数，用户体验优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有网球运动分析技术需要将传感器安装在球拍或主导手臂上，造成干扰和不适。本研究旨在开发一种更舒适、用户友好的解决方案。

Method: 通过20名业余网球选手的非主导手臂上的惯性测量单元传感器数据，训练神经网络以检测和分类六种击球动作，并开发了一个端到端原型系统。

Result: 系统分类准确率为88.2%，检测F1分数为86.0%，用户研究显示参与者在生理和心理上负担更轻。

Conclusion: 研究表明，非主导手臂是进行网球击球分析的舒适有效选择，推动了用户友好的体育分析技术发展。

Abstract: Wearable technology has transformed sports analytics, offering new dimensions
in enhancing player experience. Yet, many solutions involve cumbersome setups
that inhibit natural motion. In tennis, existing products require sensors on
the racket or dominant arm, causing distractions and discomfort. We propose
Silent Impact, a novel and user-friendly system that analyzes tennis shots
using a sensor placed on the passive arm. Collecting Inertial Measurement Unit
sensor data from 20 recreational tennis players, we developed neural networks
that exclusively utilize passive arm data to detect and classify six shots,
achieving a classification accuracy of 88.2% and a detection F1 score of 86.0%,
comparable to the dominant arm. These models were then incorporated into an
end-to-end prototype, which records passive arm motion through a smartwatch and
displays a summary of shots on a mobile app. User study (N=10) showed that
participants felt less burdened physically and mentally using Silent Impact on
the passive arm. Overall, our research establishes the passive arm as an
effective, comfortable alternative for tennis shot analysis, advancing
user-friendly sports analytics.

</details>


### [46] [Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System](https://arxiv.org/abs/2507.23298)
*Kazushi Kato,Koji Inoue,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.HC

TL;DR: 论文提出了一种实时预测点头时机和类型的模型，结合多任务学习和预训练，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在对话中，非语言信息（如点头）与语言信息同样重要，但现有系统缺乏实时预测多种点头行为的能力。

Method: 基于VAP模型扩展，实现连续实时点头预测，结合多任务学习和通用对话数据预训练。

Result: 多任务学习效果显著，实时操作下精度几乎无下降，主观评价优于传统同步点头方法。

Conclusion: 模型提升了倾听系统的非语言表达能力，代码和模型已开源。

Abstract: In human dialogue, nonverbal information such as nodding and facial
expressions is as crucial as verbal information, and spoken dialogue systems
are also expected to express such nonverbal behaviors. We focus on nodding,
which is critical in an attentive listening system, and propose a model that
predicts both its timing and type in real time. The proposed model builds on
the voice activity projection (VAP) model, which predicts voice activity from
both listener and speaker audio. We extend it to prediction of various types of
nodding in a continuous and real-time manner unlike conventional models. In
addition, the proposed model incorporates multi-task learning with verbal
backchannel prediction and pretraining on general dialogue data. In the timing
and type prediction task, the effectiveness of multi-task learning was
significantly demonstrated. We confirmed that reducing the processing rate
enables real-time operation without a substantial drop in accuracy, and
integrated the model into an avatar attentive listening system. Subjective
evaluations showed that it outperformed the conventional method, which always
does nodding in sync with verbal backchannel. The code and trained models are
available at https://github.com/MaAI-Kyoto/MaAI.

</details>


### [47] [Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models](https://arxiv.org/abs/2507.23470)
*Sebastian Gürtl,Gloria Schimetta,David Kerschbaumer,Michael Liut,Alexander Steinmaurer*

Main category: cs.HC

TL;DR: 论文介绍了基于LLM的原型工具DUET，用于辅助UML和ER图的学习，提供个性化反馈和教育分析。


<details>
  <summary>Details</summary>
Motivation: 传统教学方法难以解决UML和ER图学习中的抽象性和复杂性，尤其是在大班教学中缺乏个性化反馈。

Method: DUET通过多阶段LLM流程比较学生与参考图的文本表示，生成结构化反馈，并支持教育分析。

Result: 通过六名参与者的半结构化访谈，DUET展示了可访问性、可扩展性和学习支持等优势，但也存在可靠性和潜在滥用问题。

Conclusion: DUET为将LLM集成到建模教育中提供了有前景的方向，并为未来课堂应用和实证评估奠定了基础。

Abstract: UML and ER diagrams are foundational in computer science education but come
with challenges for learners due to the need for abstract thinking, contextual
understanding, and mastery of both syntax and semantics. These complexities are
difficult to address through traditional teaching methods, which often struggle
to provide scalable, personalized feedback, especially in large classes. We
introduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool,
which converts a reference diagram and a student-submitted diagram into a
textual representation and provides structured feedback based on the
differences. It uses a multi-stage LLM pipeline to compare diagrams and
generate reflective feedback. Furthermore, the tool enables analytical insights
for educators, aiming to foster self-directed learning and inform instructional
strategies. We evaluated DUET through semi-structured interviews with six
participants, including two educators and four teaching assistants. They
identified strengths such as accessibility, scalability, and learning support
alongside limitations, including reliability and potential misuse. Participants
also suggested potential improvements, such as bulk upload functionality and
interactive clarification features. DUET presents a promising direction for
integrating LLMs into modeling education and offers a foundation for future
classroom integration and empirical evaluation.

</details>


### [48] [Digital literacy interventions can boost humans in discerning deepfakes](https://arxiv.org/abs/2507.23492)
*Dominique Geissler,Claire Robertson,Stefan Feuerriegel*

Main category: cs.HC

TL;DR: 研究比较了五种数字素养干预方法，提升人们识别Deepfake的能力，结果显示干预效果显著。


<details>
  <summary>Details</summary>
Motivation: Deepfake可能破坏信任，但目前缺乏可扩展且有效的数字素养提升方法。

Method: 实验比较了五种干预方法（文本指导、视觉演示、游戏化练习、隐性学习、AI生成解释），涉及1200名美国参与者。

Result: 干预方法提升了13个百分点的Deepfake识别能力，同时保持了对真实图像的信任。

Conclusion: 该方法可扩展、适合多样化人群，能有效提升Deepfake识别能力且不影响对真实信息的信任。

Abstract: Deepfakes, i.e., images generated by artificial intelligence (AI), can erode
trust in institutions and compromise election outcomes, as people often
struggle to discern real images from deepfakes. Improving digital literacy can
help address these challenges, yet scalable and effective approaches remain
largely unexplored. Here, we compare the efficacy of five digital literacy
interventions to boost people's ability to discern deepfakes: (1) textual
guidance on common indicators of deepfakes; (2) visual demonstrations of these
indicators; (3) a gamified exercise for identifying deepfakes; (4) implicit
learning through repeated exposure and feedback; and (5) explanations of how
deepfakes are generated with the help of AI. We conducted an experiment with
N=1,200 participants from the United States to test the immediate and long-term
effectiveness of our interventions. Our results show that our interventions can
boost deepfake discernment by up to 13 percentage points while maintaining
trust in real images. Altogether, our approach is scalable, suitable for
diverse populations, and highly effective for boosting deepfake detection while
maintaining trust in truthful information.

</details>


### [49] [Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web](https://arxiv.org/abs/2507.23585)
*Sophia Liu,Shm Garanganao Almeda*

Main category: cs.HC

TL;DR: 论文探讨了算法驱动界面如何削弱用户代理（agency），并提出“超文本摩擦力”作为设计理念，强调通过超文本原则（如可追溯性、结构）增强用户控制。


<details>
  <summary>Details</summary>
Motivation: 当前算法驱动的界面（如推荐系统、GenAI工具）追求效率和参与度，但牺牲了用户代理（agency）。用户对内容的控制权逐渐丧失。

Method: 通过比较真实世界界面（Wikipedia vs. Instagram Explore，Are.na vs. GenAI图像工具），分析不同系统如何塑造用户体验、导航和创作。

Result: 超文本系统强调来源、关联性思考和用户主导的意义构建，而算法系统则倾向于隐藏过程、简化参与。

Conclusion: 论文提出“超文本摩擦力”作为设计立场，为在算法主导的网络环境中恢复用户代理提供了理论支持和实践指导。

Abstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI
tools, often prioritize engagement and efficiency at the expense of user
agency. As systems take on more decision-making, users have less control over
what they see and how meaning or relationships between content are constructed.
This paper introduces "Hypertextual Friction," a conceptual design stance that
repositions classical hypertext principles--friction, traceability, and
structure--as actionable values for reclaiming agency in algorithmically
mediated environments. Through a comparative analysis of real-world
interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image
tools--we examine how different systems structure user experience, navigation,
and authorship. We show that hypertext systems emphasize provenance,
associative thinking, and user-driven meaning-making, while algorithmic systems
tend to obscure process and flatten participation. We contribute: (1) a
comparative analysis of how interface structures shape agency in user-driven
versus agent-driven systems, and (2) a conceptual stance that offers
hypertextual values as design commitments for reclaiming agency in an
increasingly algorithmic web.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [50] [Noise-Coded Illumination for Forensic and Photometric Video Analysis](https://arxiv.org/abs/2507.23002)
*Peter F. Michael,Zekun Hao,Serge Belongie,Abe Davis*

Main category: cs.GR

TL;DR: 论文提出了一种通过在场景光照中嵌入细微噪声调制的方法，创造信息不对称以识别虚假视频，从而对抗视频篡改的技术。


<details>
  <summary>Details</summary>
Motivation: 随着视频篡改技术的进步，虚假视频越来越难以辨别。论文旨在通过创造信息不对称，使验证者能够识别被篡改的视频。

Method: 在场景光照中嵌入细微的噪声调制，形成时间水印，记录未篡改场景的图像。即使攻击者知道该技术，篡改视频的难度也会显著增加。

Result: 该方法在高风险场景中（如公共事件和采访）展示了对抗视频篡改的潜力，即使攻击者了解技术细节也难以有效伪造。

Conclusion: 通过在可控光照中嵌入水印，该方法为视频真实性验证提供了有效手段，尤其适用于高风险场景。

Abstract: The proliferation of advanced tools for manipulating video has led to an arms
race, pitting those who wish to sow disinformation against those who want to
detect and expose it. Unfortunately, time favors the ill-intentioned in this
race, with fake videos growing increasingly difficult to distinguish from real
ones. At the root of this trend is a fundamental advantage held by those
manipulating media: equal access to a distribution of what we consider
authentic (i.e., "natural") video. In this paper, we show how coding very
subtle, noise-like modulations into the illumination of a scene can help combat
this advantage by creating an information asymmetry that favors verification.
Our approach effectively adds a temporal watermark to any video recorded under
coded illumination. However, rather than encoding a specific message, this
watermark encodes an image of the unmanipulated scene as it would appear lit
only by the coded illumination. We show that even when an adversary knows that
our technique is being used, creating a plausible coded fake video amounts to
solving a second, more difficult version of the original adversarial content
creation problem at an information disadvantage. This is a promising avenue for
protecting high-stakes settings like public events and interviews, where the
content on display is a likely target for manipulation, and while the
illumination can be controlled, the cameras capturing video cannot.

</details>


### [51] [XSpecMesh: Quality-Preserving Auto-Regressive Mesh Generation Acceleration via Multi-Head Speculative Decoding](https://arxiv.org/abs/2507.23777)
*Dian Chen,Yansong Qu,Xinyang Li,Ming Li,Shengchuan Zhang*

Main category: cs.GR

TL;DR: XSpecMesh 是一种用于加速自回归网格生成模型的方法，通过并行预测多个标记和提高预测质量，实现了1.7倍的加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前自回归模型在生成高质量网格时需要大量预测标记，导致延迟较高，因此需要一种既能保持质量又能加速推理的方法。

Method: XSpecMesh 使用轻量级多头推测解码方案并行预测多个标记，并通过验证与重采样策略以及利用蒸馏训练解码头来提高预测质量。

Result: 实验表明，该方法实现了1.7倍的加速，同时保持生成质量。

Conclusion: XSpecMesh 提供了一种有效的方法来加速自回归网格生成，解决了高延迟问题，且代码将开源。

Abstract: Current auto-regressive models can generate high-quality, topologically
precise meshes; however, they necessitate thousands-or even tens of
thousands-of next-token predictions during inference, resulting in substantial
latency. We introduce XSpecMesh, a quality-preserving acceleration method for
auto-regressive mesh generation models. XSpecMesh employs a lightweight,
multi-head speculative decoding scheme to predict multiple tokens in parallel
within a single forward pass, thereby accelerating inference. We further
propose a verification and resampling strategy: the backbone model verifies
each predicted token and resamples any tokens that do not meet the quality
criteria. In addition, we propose a distillation strategy that trains the
lightweight decoding heads by distilling from the backbone model, encouraging
their prediction distributions to align and improving the success rate of
speculative predictions. Extensive experiments demonstrate that our method
achieves a 1.7x speedup without sacrificing generation quality. Our code will
be released.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [52] [WiRM: Wireless Respiration Monitoring Using Conjugate Multiple Channel State Information and Fast Iterative Filtering in Wi-Fi Systems](https://arxiv.org/abs/2507.23419)
*James Rhodes,Lawrence Ong,Duy T. Ngo*

Main category: cs.ET

TL;DR: WiRM是一种两阶段的无接触呼吸监测方法，通过相位净化和自适应多轨迹跟踪改进呼吸率估计，并通过分解和选择呼吸波形显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有呼吸监测方法多关注呼吸率或呼吸波形，缺乏综合性和抗噪能力的研究。

Method: WiRM采用共轭乘法进行相位净化，并用AMTC实时跟踪呼吸率变化；利用改进的呼吸率估计分解和选择呼吸波形。

Result: WiRM将呼吸率RMSE平均降低38%，呼吸波形的平均绝对相关系数提升了178.3%。对抗噪声测试表现出色。

Conclusion: WiRM在呼吸监测中表现出显著优越性，尤其在噪声环境中具有更强的鲁棒性。

Abstract: Monitoring respiratory health with the use of channel state information (CSI)
has shown promising results. Many existing methods focus on monitoring only the
respiratory rate, while others focus on monitoring the motion of the chest as a
patient breathes, which is referred to as the respiratory waveform. This paper
presents WiRM, a two-staged approach to contactless respiration monitoring. In
the first stage, WiRM improves upon existing respiratory rate estimation
techniques by using conjugate multiplication for phase sanitisation and
adaptive multi-trace carving (AMTC) for tracing how the respiratory rate
changes over time. When compared against three state-of-the-art methods, WiRM
has achieved an average reduction of $38\%$ in respiratory rate root mean
squared error (RMSE). In the second stage, WiRM uses this improved respiratory
rate estimate to inform the decomposition and selection of the respiratory
waveform from the CSI data. Remarkably, WiRM delivers a $178.3\%$ improvement
in average absolute correlation with the ground truth respiratory waveform.
Within the literature, it is difficult to compare the robustness of existing
algorithms in noisy environments. In this paper, we develop a purpose-built
simulation toolkit to evaluate the robustness of respiration monitoring
solutions under various noise conditions, including thermal, multiplicative,
and phase noise. Our results show that WiRM demonstrates improved or comparable
resilience to these common noise sources.

</details>


### [53] [SOME: Symmetric One-Hot Matching Elector -- A Lightweight Microsecond Decoder for Quantum Error Correction](https://arxiv.org/abs/2507.23618)
*Xinyi Guo,Geguang Miao,Shinichi Nishizawa,Hiromitsu Awano,Shinji Kimura,Takashi Sato*

Main category: cs.ET

TL;DR: SOME解码器通过将量子纠错问题转化为QUBO问题，显著降低了复杂性和解码时间。


<details>
  <summary>Details</summary>
Motivation: 现有量子纠错解码器（如MWPM和UF）在拓扑复杂度高或解码时间长的问题上表现不佳，需一种平衡的方案。

Method: SOME将解码任务转化为对称单热编码的QUBO问题，通过构建最小权重排列矩阵优化解码。

Result: SOME将变量数减少99.9倍，解码时间从毫秒级降至微秒级，且在10.5%错误率下性能超过MWPM。

Conclusion: SOME在降低复杂性和提升速度的同时，保持了高阈值性能，展示了QUBO在量子纠错中的潜力。

Abstract: Conventional quantum error correction (QEC) decoders such as Minimum-Weight
Perfect Matching (MWPM) and Union-Find (UF) offer high thresholds and fast
decoding, respectively, but both suffer from high topological complexity. In
contrast, Ising model-based decoders reduce topological complexity but demand
considerable decoding time. We propose the Symmetric One-Hot Matching Elector
(SOME), a novel decoder that reformulates the QEC decoding task as a Quadratic
Unconstrained Binary Optimization (QUBO) problem -- termed the One-Hot QUBO
(OHQ). Each variable in the QUBO represents whether a given pair of flipped
syndromes is matched, while the error probabilities between the pair are
encoded as interaction coefficients (weight). Constraints ensure that each
flipped syndrome is matched exactly once. Valid solutions of OHQ correspond to
self-inverse permutation matrices, characterized by symmetric one-hot encoding.
To solve the OHQ efficiently, SOME reformulates the decoding task as the
construction of permutation matrices that minimize the total weight. It
initializes each candidate matrix from one of the minimum-weight syndrome
pairs, then iteratively appends additional pairs in ascending order of weight,
and finally selects the permutation matrix with the lowest total energy. SOME
achieves up to a 99.9x reduction in variable count and reduces decoding times
from milliseconds to microseconds on a single-threaded commodity CPU. OHQ also
maintains performance up to a 10.5% physical error rate, surpassing the highest
known threshold of MWPM@.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [54] [H2SGEMM: Emulating FP32 GEMM on Ascend NPUs using FP16 Units with Precision Recovery and Cache-Aware Optimization](https://arxiv.org/abs/2507.23387)
*Weicheng Xue,Baisong Xu,Kai Yang,Yongxiang Liu,Dengdeng Fan,Pengxiang Xu,Yonghong Tian*

Main category: cs.DC

TL;DR: H2SGEMM是一种通过FP16计算单元模拟FP32矩阵乘法的高性能算法，通过数值分解和可调缩放策略保持精度，并在AI加速器上实现接近理论峰值性能。


<details>
  <summary>Details</summary>
Motivation: 由于低精度矩阵引擎（如FP16）在高性能计算中缺乏对全精度的支持，因此需要一种方法在FP16硬件上模拟FP32计算。

Method: 将FP32操作数分解为两个FP16值，通过可调缩放补偿数值误差，并采用缓存感知的分块策略和双缓冲流水线优化性能。

Result: H2SGEMM在Ascend 910A NPU上达到理论FP32峰值性能的77%，并在数值实验中表现出优于传统FP32方法的稳定性。

Conclusion: H2SGEMM在有限精度硬件上成功模拟了FP32计算，且在某些情况下表现出更优的数值稳定性。

Abstract: Low-precision matrix engines, such as FP16 cube, offer high throughput but
lack support for full-precision computation. In this work, we propose H2SGEMM,
a high-performance algorithm for emulating FP32 general matrix-matrix
multiplication (GEMM) using only FP16 computation units on a representative AI
accelerator. The method decomposes each FP32 operand into two FP16 values and
compensates for numerical errors through a tunable scaling strategy. A detailed
analysis of numerical errors, including underflow conditions and precision
loss, guides the selection of scaling parameters to preserve up to 22 bits of
mantissa accuracy. We further investigate the effect of computation order on
accuracy and demonstrate that a term-wise accumulation scheme improves
numerical stability over conventional FP32 GEMM in low-exponent regimes.
Finally, a cache-aware blocking strategy and double-buffered pipeline are
introduced to overlap memory transfers with computation, enabling H2SGEMM to
achieve up to 77% of the theoretical FP32-equivalent peak performance on Ascend
910A NPU lacking native FP32 support. Extensive numerical experiments confirm
that our method not only recovers the accuracy of native FP32 GEMM but also
exhibits superior numerical stability under certain conditions, due to its
structured and error-aware computation order.

</details>


### [55] [Towards a Testbed for Scalable FaaS Platforms](https://arxiv.org/abs/2507.23431)
*Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: 论文介绍了针对FaaS平台的架构性能测试工具。


<details>
  <summary>Details</summary>
Motivation: 为了研究不同架构和技术对FaaS平台扩展性能的影响。

Method: 开发了一个研究导向的测试平台，用于快速评估不同架构和技术的影响。

Result: 该测试平台可以适应多种架构和技术，便于性能分析。

Conclusion: 提出的测试工具有助于深入理解FaaS平台的性能与架构关系。

Abstract: Most cloud platforms have a Function-as-a-Service (FaaS) offering that
enables users to easily write highly scalable applications. To better
understand how the platform's architecture impacts its performance, we present
a research-focused testbed that can be adapted to quickly evaluate the impact
of different architectures and technologies on the characteristics of
scalability-focused FaaS platforms.

</details>


### [56] [Threshold-Driven Streaming Graph: Expansion and Rumor Spreading](https://arxiv.org/abs/2507.23533)
*Flora Angileri,Andrea Clementi,Emanuele Natale,Michele Salvi,Isabella Ziccardi*

Main category: cs.DC

TL;DR: RAES算法在动态图模型下的扩展性表现及其对谣言传播协议的影响。


<details>
  <summary>Details</summary>
Motivation: 研究RAES算法在节点流式更替的动态图模型中的表现，填补静态图分析的不足。

Method: 在滑动窗口模型下，通过节点轮流加入和退出构建动态图，分析RAES的扩展性。

Result: 证明动态图的每快照具有高概率的良好扩展性，并推导PUSH/PULL协议的对数时间完成上限。

Conclusion: RAES在动态图中保持良好扩展性，适用于P2P网络中的协议优化。

Abstract: A randomized distributed algorithm called RAES was introduced in [Becchetti
et al., SODA 2020] to extract a bounded-degree expander from a dense $n$-vertex
expander graph $G = (V, E)$. The algorithm relies on a simple threshold-based
procedure. A key assumption in [Becchetti et al., SODA 2020] is that the input
graph $G$ is static - i.e., both its vertex set $V$ and edge set $E$ remain
unchanged throughout the process - while the analysis of RAES in dynamic models
is left as a major open question.
  In this work, we investigate the behavior of RAES under a dynamic graph model
induced by a streaming node-churn process (also known as the sliding window
model), where, at each discrete round, a new node joins the graph and the
oldest node departs. This process yields a bounded-degree dynamic graph
$\mathcal{G} =\{ G_t = (V_t, E_t) : t \in \mathbb{N}\}$ that captures essential
characteristics of peer-to-peer networks -- specifically, node churn and
threshold on the number of connections each node can manage. We prove that
every snapshot $G_t$ in the dynamic graph sequence has good expansion
properties with high probability. Furthermore, we leverage this property to
establish a logarithmic upper bound on the completion time of the well-known
PUSH and PULL rumor spreading protocols over the dynamic graph $\mathcal{G}$.

</details>


### [57] [The ArborX library: version 2.0](https://arxiv.org/abs/2507.23700)
*Andrey Prokopenko,Daniel Arndt,Damien Lebrun-Grandié,Bruno Turcksin*

Main category: cs.DC

TL;DR: ArborX 2.0是一款性能可移植的几何搜索库，基于Kokkos，支持更广泛的用户问题、新搜索数据结构和算法扩展。


<details>
  <summary>Details</summary>
Motivation: 提供高性能的几何搜索功能，支持多样化的用户需求和应用场景。

Method: 通过新接口、新数据结构和算法扩展（如暴力搜索、分布式搜索、回调功能和光线追踪、聚类）。

Result: ArborX 2.0支持更广泛的问题类型，提升了灵活性和功能性。

Conclusion: ArborX 2.0通过改进接口和功能增强了其性能和适用性。

Abstract: This paper provides an overview of the 2.0 release of the ArborX library, a
performance portable geometric search library based on Kokkos. We describe the
major changes in ArborX 2.0 including a new interface for the library to
support a wider range of user problems, new search data structures (brute
force, distributed), support for user functions to be executed on the results
(callbacks), and an expanded set of the supported algorithms (ray tracing,
clustering).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [58] [AutoIndexer: A Reinforcement Learning-Enhanced Index Advisor Towards Scaling Workloads](https://arxiv.org/abs/2507.23084)
*Taiyi Wang,Eiko Yoneki*

Main category: cs.DB

TL;DR: AutoIndexer通过结合工作负载压缩和强化学习，有效解决大规模索引选择问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统RL索引顾问在应对大规模工作负载时因动作空间爆炸和试错成本高而难以扩展的问题。

Method: 提出AutoIndexer框架，结合工作负载压缩、查询优化和专用RL模型，降低搜索复杂度。

Result: 相比无索引基准，查询执行时间减少高达95%；平均节省20%工作负载成本，调优时间减少50%。

Conclusion: AutoIndexer适用于大规模多样化工作负载，展现了实际应用价值。

Abstract: Efficiently selecting indexes is fundamental to database performance
optimization, particularly for systems handling large-scale analytical
workloads. While deep reinforcement learning (DRL) has shown promise in
automating index selection through its ability to learn from experience, few
works address how these RL-based index advisors can adapt to scaling workloads
due to exponentially growing action spaces and heavy trial and error. To
address these challenges, we introduce AutoIndexer, a framework that combines
workload compression, query optimization, and specialized RL models to scale
index selection effectively. By operating on compressed workloads, AutoIndexer
substantially lowers search complexity without sacrificing much index quality.
Extensive evaluations show that it reduces end-to-end query execution time by
up to 95% versus non-indexed baselines. On average, it outperforms
state-of-the-art RL-based index advisors by approximately 20% in workload cost
savings while cutting tuning time by over 50%. These results affirm
AutoIndexer's practicality for large and diverse workloads.

</details>


### [59] [Jelly-Patch: a Fast Format for Recording Changes in RDF Datasets](https://arxiv.org/abs/2507.23499)
*Piotr Sowinski,Kacper Grzymkowski,Anastasiya Danilenka*

Main category: cs.DB

TL;DR: Jelly-Patch 是一种高性能的 RDF 数据集变更压缩序列化格式，显著提升了压缩率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决大规模和低延迟 RDF 应用中变更数据的高性能序列化和传输问题。

Method: 提出 Jelly-Patch —— 一种压缩二进制序列化格式，并通过基准测试对比现有 RDF Patch 格式。

Result: Jelly-Patch 压缩率提升 3.5--8.9 倍，序列化和解析吞吐量分别提高 2.5 倍和 4.6 倍。

Conclusion: Jelly-Patch 显著提升大规模和低延迟 RDF 系统的性能。

Abstract: Recording data changes in RDF systems is a crucial capability, needed to
support auditing, incremental backups, database replication, and event-driven
workflows. In large-scale and low-latency RDF applications, the high volume and
frequency of updates can cause performance bottlenecks in the serialization and
transmission of changes. To alleviate this, we propose Jelly-Patch -- a
high-performance, compressed binary serialization format for changes in RDF
datasets. To evaluate its performance, we benchmark Jelly-Patch against
existing RDF Patch formats, using two datasets representing different use cases
(change data capture and IoT streams). Jelly-Patch is shown to achieve
3.5--8.9x better compression, and up to 2.5x and 4.6x higher throughput in
serialization and parsing, respectively. These significant advancements in
throughput and compression are expected to improve the performance of
large-scale and low-latency RDF systems.

</details>


### [60] [DataLens: Enhancing Dataset Discovery via Network Topologies](https://arxiv.org/abs/2507.23515)
*Anaïs Ollagnier,Aline Menin*

Main category: cs.DB

TL;DR: DataLens是一个基于网络的平台，结合分面搜索和高级可视化技术，提升资源发现的效率。


<details>
  <summary>Details</summary>
Motivation: 公开可用的文本资源快速增长，但现有资源库缺乏高级搜索和探索功能，导致用户难以高效找到相关资源。

Method: DataLens通过分面搜索和网络可视化技术，支持多视角数据探索，并提供适应性网络结构以匹配分析任务。

Result: 用户研究表明，用户高度评价可视化工具，尤其是基于网络的探索功能，并提供了改进建议。

Conclusion: DataLens通过先进的可视化技术显著提升了资源发现能力，未来可以进一步优化以更好地支持数据集搜索。

Abstract: The rapid growth of publicly available textual resources, such as lexicons
and domain-specific corpora, presents challenges in efficiently identifying
relevant resources. While repositories are emerging, they often lack advanced
search and exploration features. Most search methods rely on keyword queries
and metadata filtering, which require prior knowledge and fail to reveal
connections between resources. To address this, we present DataLens, a
web-based platform that combines faceted search with advanced visualization
techniques to enhance resource discovery. DataLens offers network-based
visualizations, where the network structure can be adapted to suit the specific
analysis task. It also supports a chained views approach, enabling users to
explore data from multiple perspectives. A formative user study involving six
data practitioners revealed that users highly value visualization
tools-especially network-based exploration-and offered insights to help refine
our approach to better support dataset search.

</details>


<div id='math.CT'></div>

# math.CT [[Back]](#toc)

### [61] [Typing Tensor Calculus in 2-Categories (I)](https://arxiv.org/abs/1908.01212)
*Fatimah Rita Ahmadi*

Main category: math.CT

TL;DR: 该论文通过将线性代数中的矩阵视为矩阵范畴中的态射，并推广到半加性2-范畴，以支持高阶张量计算，提出了一个无索引、类型化的线性代数框架。


<details>
  <summary>Details</summary>
Motivation: 动机是为了形式化线性代数中的计算，以开发高效算法和适用于函数式编程语言及并行计算的框架。

Method: 采用矩阵范畴和半加性2-范畴的方法，将矩阵和高阶张量表示为1-和2-态射，并扩展到半加性2-范畴中的向量化操作。

Result: 提出了一个类型化的线性代数框架，支持矩阵和最多四阶张量的计算，并展示了在2Vec范畴中的详细操作。

Conclusion: 该框架为线性代数的形式化计算提供了新的理论基础，尤其适用于函数式编程和并行计算。

Abstract: To formalize calculations in linear algebra for the development of efficient
algorithms and a framework suitable for functional programming languages and
faster parallelized computations, we adopt an approach that treats elements of
linear algebra, such as matrices, as morphisms in the category of matrices,
$\mathbf{Mat_{k}}$. This framework is further extended by generalizing the
results to arbitrary monoidal semiadditive categories. To enrich this
perspective and accommodate higher-rank matrices (tensors), we define
semiadditive 2-categories, where matrices $T_{ij}$ are represented as
1-morphisms, and tensors with four indices $T_{ijkl}$ as 2-morphisms. This
formalization provides an index-free, typed linear algebra framework that
includes matrices and tensors with up to four indices. Furthermore, we extend
the framework to monoidal semiadditive 2-categories and demonstrate detailed
operations and vectorization within the 2-category of 2Vec introduced by
Kapranov and Voevodsky.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [62] [Text-to-SQL Task-oriented Dialogue Ontology Construction](https://arxiv.org/abs/2507.23358)
*Renato Vukovic,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Hsien-Chin Lin,Shutong Feng,Nurul Lubis,Milica Gasic*

Main category: cs.CL

TL;DR: TeQoDO是一种通过LLM自主构建任务导向对话（TOD）本体结构的方法，无需监督即可完成，并在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决LLM依赖参数化知识导致的解释性和可信度问题，同时避免手动标注或监督训练的需求。

Method: 利用LLM的SQL编程能力和对话理论提示，自主构建TOD本体结构。

Result: TeQoDO在对话状态跟踪任务中表现优于迁移学习方法，并能扩展到构建更大规模的本体结构。

Conclusion: TeQoDO为提升LLM的解释性提供了有效途径，展示了本体结构在LLM中的广泛应用潜力。

Abstract: Large language models (LLMs) are widely used as general-purpose knowledge
sources, but they rely on parametric knowledge, limiting explainability and
trustworthiness. In task-oriented dialogue (TOD) systems, this separation is
explicit, using an external database structured by an explicit ontology to
ensure explainability and controllability. However, building such ontologies
requires manual labels or supervised training. We introduce TeQoDO: a
Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM
autonomously builds a TOD ontology from scratch without supervision using its
inherent SQL programming capabilities combined with dialogue theory provided in
the prompt. We show that TeQoDO outperforms transfer learning approaches, and
its constructed ontology is competitive on a downstream dialogue state tracking
task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also
scales to allow construction of much larger ontologies, which we investigate on
a Wikipedia and ArXiv dataset. We view this as a step towards broader
application of ontologies to increase LLM explainability.

</details>


### [63] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)
*Md Talha Mohsin*

Main category: cs.CL

TL;DR: 本文对大语言模型（LLM）在金融自然语言处理（FinNLP）任务中的表现进行了系统比较，发现GPT表现最优，Claude和Perplexity次之，Gemini和DeepSeek表现较差且稳定性不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在金融分析中的广泛应用，缺乏系统性的模型比较研究，因此本文旨在填补这一空白。

Method: 使用10-K文件，设计特定领域提示，通过人工标注、自动语义评估（如ROUGE）和模型行为诊断（如提示方差）三种方法评估五种LLM。

Result: GPT在连贯性、语义对齐和上下文相关性上表现最佳；模型输出对提示和源材料的敏感性显著。

Conclusion: GPT是最适合金融分析任务的LLM，但需注意提示设计和数据选择的影响。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide variety of Financial Natural Language Processing (FinNLP) tasks.
However, systematic comparisons among widely used LLMs remain underexplored.
Given the rapid advancement and growing influence of LLMs in financial
analysis, this study conducts a thorough comparative evaluation of five leading
LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the
'Magnificent Seven' technology companies. We create a set of domain-specific
prompts and then use three methodologies to evaluate model performance: human
annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity,
Jaccard), and model behavior diagnostics (prompt-level variance and
across-model similarity). The results show that GPT gives the most coherent,
semantically aligned, and contextually relevant answers; followed by Claude and
Perplexity. Gemini and DeepSeek, on the other hand, have more variability and
less agreement. Also, the similarity and stability of outputs change from
company to company and over time, showing that they are sensitive to how
prompts are written and what source material is used.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [64] [Rational complex Bezier curves](https://arxiv.org/abs/2507.23485)
*A. Canton,L. Fernandez-Jambrina,M. J. Vazquez-Gallo*

Main category: math.NA

TL;DR: 本文开发了一种有理复Bezier曲线的形式化方法，扩展了CAD范式，允许控制多边形和权重取复数值，从而利用两类射影变换并降低曲线次数。


<details>
  <summary>Details</summary>
Motivation: 为了扩展CAD设计工具的功能，并利用复数框架实现更灵活的曲线变换与简化。

Method: 通过引入复数权重和控制点，利用实平面和复平面的射影变换，结合多项式判别式验证曲线降阶的可能性。

Result: 证明了复数框架可简化部分曲线的次数，并提供了判断有理三次曲线是否为二次曲线的公式。

Conclusion: 有理复Bezier曲线为CAD设计提供了新的工具，能简化曲线处理并支持更丰富的几何变换。

Abstract: In this paper we develop the formalism of rational complex Bezier curves.
This framework is a simple extension of the CAD paradigm, since it describes
arc of curves in terms of control polygons and weights, which are extended to
complex values. One of the major advantages of this extension is that we may
make use of two different groups of projective transformations. Besides the
group of projective transformations of the real plane, we have the group of
complex projective transformations. This allows us to apply useful
transformations like the geometric inversion to curves in design. In addition
to this, the use of the complex formulation allows to lower the degree of the
curves in some cases. This can be checked using the resultant of two
polynomials and provides a simple formula for determining whether a rational
cubic curve is a conic or not. Examples of application of the formalism to
classical curves are included.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [65] [Smart Video Capsule Endoscopy: Raw Image-Based Localization for Enhanced GI Tract Investigation](https://arxiv.org/abs/2507.23398)
*Oliver Bause,Julia Werner,Paul Palomero Bernardo,Oliver Bringmann*

Main category: eess.IV

TL;DR: 论文提出了一种针对低功耗传感器边缘设备的图像分类方法，直接在Bayer图像上进行分类，避免了RGB转换的能耗，实现了93.06%的准确率，每图像仅需5.31μJ能量。


<details>
  <summary>Details</summary>
Motivation: 由于深度神经网络通常模型较大且计算复杂，低功耗传感器边缘设备难以适用，尤其是在需要跳过RGB转换以节省能量的场景下，如视频胶囊内窥镜等医疗应用。

Method: 使用仅有63,000参数的CNN结合Viterbi解码的时间序列分析，直接在Bayer图像上进行分类，并通过定制的PULPissimo SoC实现高效图像捕获和处理。

Result: 实现了93.06%的分类准确率，每图像能耗降低至5.31μJ，与经典视频胶囊相比，进入小肠前平均节省89.9%的能量。

Conclusion: 该方法为资源受限的边缘设备提供了一种高效节能的图像分类解决方案，特别适用于医疗等领域。

Abstract: For many real-world applications involving low-power sensor edge devices deep
neural networks used for image classification might not be suitable. This is
due to their typically large model size and require- ment of operations often
exceeding the capabilities of such resource lim- ited devices. Furthermore,
camera sensors usually capture images with a Bayer color filter applied, which
are subsequently converted to RGB images that are commonly used for neural
network training. However, on resource-constrained devices, such conversions
demands their share of energy and optimally should be skipped if possible. This
work ad- dresses the need for hardware-suitable AI targeting sensor edge
devices by means of the Video Capsule Endoscopy, an important medical proce-
dure for the investigation of the small intestine, which is strongly limited by
its battery lifetime. Accurate organ classification is performed with a final
accuracy of 93.06% evaluated directly on Bayer images involv- ing a CNN with
only 63,000 parameters and time-series analysis in the form of Viterbi
decoding. Finally, the process of capturing images with a camera and raw image
processing is demonstrated with a customized PULPissimo System-on-Chip with a
RISC-V core and an ultra-low power hardware accelerator providing an
energy-efficient AI-based image clas- sification approach requiring just 5.31
{\mu}J per image. As a result, it is possible to save an average of 89.9% of
energy before entering the small intestine compared to classic video capsules.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [66] [Scalable contribution bounding to achieve privacy](https://arxiv.org/abs/2507.23432)
*Vincent Cohen-Addad,Alessandro Epasto,Jason Lee,Morteza Zadimoghaddam*

Main category: cs.DS

TL;DR: 提出了一种高效的分布式算法，用于解决现代多用户数据集中用户级差分隐私的贡献限制问题，通过并行处理和超图建模实现可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在现代数据集中，单一记录可能属于多个用户，传统的序列化方法计算量巨大且无法扩展，急需一种高效的分布式解决方案。

Method: 将用户和记录分别建模为超图的顶点和超边，采用并行提案机制，确保用户贡献不超过预设限制。

Result: 该方法在保证隐私的同时最大化数据集规模，适用于大规模实际系统。

Conclusion: 提出的分布式算法有效解决了用户级差分隐私的贡献限制问题，具有高效性和可扩展性。

Abstract: In modern datasets, where single records can have multiple owners, enforcing
user-level differential privacy requires capping each user's total
contribution. This "contribution bounding" becomes a significant combinatorial
challenge. Existing sequential algorithms for this task are computationally
intensive and do not scale to the massive datasets prevalent today. To address
this scalability bottleneck, we propose a novel and efficient distributed
algorithm. Our approach models the complex ownership structure as a hypergraph,
where users are vertices and records are hyperedges. The algorithm proceeds in
rounds, allowing users to propose records in parallel. A record is added to the
final dataset only if all its owners unanimously agree, thereby ensuring that
no user's predefined contribution limit is violated. This method aims to
maximize the size of the resulting dataset for high utility while providing a
practical, scalable solution for implementing user-level privacy in large,
real-world systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [67] [KLLM: Fast LLM Inference with K-Means Quantization](https://arxiv.org/abs/2507.23035)
*Xueying Wu,Baijun Zhou,Zhihui Gao,Yuzhe Fu,Qilin Zheng,Yintao He,Hai Li*

Main category: cs.LG

TL;DR: KLLM是一个软硬件协同设计框架，通过K-Means量化和新颖的异常值检测引擎（Orizuru）显著提升了LLM推理的效率和能效。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）推理面临高内存和计算需求，现有量化方法（WAQ）在低精度下存在准确度下降和异常值处理问题。

Method: KLLM采用基于索引的计算方案加速K-Means量化数据的矩阵乘法，并集成在线异常值检测引擎Orizuru。

Result: 实验表明，KLLM在A100 GPU和Atom上分别实现了9.67x和7.03x的速度提升，能效提升达229.50x和150.21x。

Conclusion: KLLM通过软硬件协同设计有效解决了LLM推理中的量化挑战，显著提升了性能和能效。

Abstract: Large language model (LLM) inference poses significant challenges due to its
intensive memory and computation demands. Weight and activation quantization
(WAQ) offers a promising solution by reducing both memory footprint and
arithmetic complexity. However, two key challenges remain in the existing WAQ
designs. (1) Traditional WAQ designs rely on uniform integer-based quantization
for hardware efficiency, but this often results in significant accuracy
degradation at low precision. K-Means-based quantization, a non-uniform
quantization technique, achieves higher accuracy by matching the Gaussian-like
distributions of weights and activations in LLMs. However, its non-uniform
nature prevents direct execution on low-precision compute units, requiring
dequantization and floating-point matrix multiplications (MatMuls) during
inference. (2) Activation outliers further hinder effective low-precision WAQ.
Offline thresholding methods for outlier detection can lead to significant
model performance degradation, while existing online detection techniques
introduce substantial runtime overhead.
  To address the aforementioned challenges and fully unleash the potential of
WAQ with K-Means quantization for LLM inference, in this paper, we propose
KLLM, a hardware-software co-design framework. KLLM features an index-based
computation scheme for efficient execution of MatMuls and nonlinear operations
on K-Means-quantized data, which avoids most of the dequantization and
full-precision computations. Moreover, KLLM incorporates a novel outlier
detection engine, Orizuru, that efficiently identifies the top-$k$ largest and
smallest elements in the activation data stream during online inference.
  Extensive experiments show that, on average, KLLM achieves speedups of 9.67x,
7.03x and energy efficiency improvements of 229.50x, 150.21x compared to the
A100 GPU and Atom, respectively.

</details>


### [68] [NaN-Propagation: A Novel Method for Sparsity Detection in Black-Box Computational Functions](https://arxiv.org/abs/2507.23186)
*Peter Sharpe*

Main category: cs.LG

TL;DR: NaN传播方法利用IEEE 754标准中的NaN属性检测黑盒函数中的稀疏性，消除了传统有限差分方法中的假阴性问题，显著提升了梯度计算的效率。


<details>
  <summary>Details</summary>
Motivation: 现有有限差分方法在稀疏性检测中由于偶然的零梯度导致假阴性，从而影响梯度计算的准确性，因此需要一种更可靠的方法。

Method: 通过系统性输入NaN并观察输出中的NaN传播，重建保守的稀疏模式，避免假阴性，且无需修改现有黑盒代码。

Result: 在航空机翼重量模型中实现了1.52倍的加速，检测到传统方法遗漏的多个依赖关系。

Conclusion: NaN传播方法是一种跨语言和数学库的高效稀疏性检测技术，优化性能优于传统方法。

Abstract: Sparsity detection in black-box functions enables significant computational
speedups in gradient-based optimization through Jacobian compression, but
existing finite-difference methods suffer from false negatives due to
coincidental zero gradients. These false negatives can silently corrupt
gradient calculations, leading to difficult-to-diagnose errors. We introduce
NaN-propagation, which exploits the universal contamination property of IEEE
754 Not-a-Number floating-point values to trace input-output dependencies
through floating-point numerical computations. By systematically contaminating
inputs with NaN and observing which outputs become NaN, the method reconstructs
conservative sparsity patterns that eliminate false negatives. We demonstrate
the approach on an aerospace wing weight model, achieving a 1.52x speedup while
detecting dozens of dependencies missed by conventional methods -- a
significant improvement since gradient computation is the bottleneck in many
optimization workflows. The technique leverages IEEE 754 compliance to work
across programming languages and math libraries without modifying existing
black-box codes. Advanced strategies including NaN payload encoding enable
faster-than-linear time complexity, improving upon existing black-box sparsity
detection methods. Practical algorithms are also proposed to mitigate
challenges from branching code execution common in engineering applications.

</details>


### [69] [Hardware-Aware Fine-Tuning of Spiking Q-Networks on the SpiNNaker2 Neuromorphic Platform](https://arxiv.org/abs/2507.23562)
*Sirine Arfa,Bernhard Vogginger,Christian Mayr*

Main category: cs.LG

TL;DR: 使用量化SNN实现低功耗强化学习算法，在SpiNNaker2芯片上实现32倍能耗降低，推理延迟与GPU相当。


<details>
  <summary>Details</summary>
Motivation: 探索SNN在机器人任务中的低功耗和低延迟潜力，推动高效神经形态计算的发展。

Method: 基于Q学习的SNN训练，量化至8位精度部署于SpiNNaker2芯片，对比GPU评估能耗和延迟。

Result: SpiNNaker2能耗降低32倍，推理延迟与GPU相当，部分任务表现更优。

Conclusion: SpiNNaker2展示了可扩展低能耗神经形态计算的潜力，适用于实时控制任务。

Abstract: Spiking Neural Networks (SNNs) promise orders-of-magnitude lower power
consumption and low-latency inference on neuromorphic hardware for a wide range
of robotic tasks. In this work, we present an energy-efficient implementation
of a reinforcement learning (RL) algorithm using quantized SNNs to solve two
classical control tasks. The network is trained using the Q-learning algorithm,
then fine-tuned and quantized to low-bit (8-bit) precision for embedded
deployment on the SpiNNaker2 neuromorphic chip. To evaluate the comparative
advantage of SpiNNaker2 over conventional computing platforms, we analyze
inference latency, dynamic power consumption, and energy cost per inference for
our SNN models, comparing performance against a GTX 1650 GPU baseline. Our
results demonstrate SpiNNaker2's strong potential for scalable, low-energy
neuromorphic computing, achieving up to 32x reduction in energy consumption.
Inference latency remains on par with GPU-based execution, with improvements
observed in certain task settings, reinforcing SpiNNaker2's viability for
real-time neuromorphic control and making the neuromorphic approach a
compelling direction for efficient deep Q-learning.

</details>


### [70] [SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy](https://arxiv.org/abs/2507.23292)
*RJ Skerry-Ryan,Julian Salazar,Soroosh Mariooryad,David Kao,Daisy Stanton,Eric Battenberg,Matt Shannon,Ron J. Weiss,Robin Scheibler,Jonas Rothfuss,Tom Bagby*

Main category: cs.LG

TL;DR: 介绍了一个易于构建序列模型的神经网络层API和库，支持逐层和逐步执行，确保结果一致，并提供强正确性保证。


<details>
  <summary>Details</summary>
Motivation: 简化序列模型的创建和执行，解决流式和平行序列处理中的常见问题。

Method: 定义层的显式状态表示和逐步演进方法，确保与无状态逐层调用结果一致。

Result: 实现了可直接流式化的复杂模型，减少了常见错误。

Conclusion: SequenceLayers提供了一个高效、正确的序列模型构建工具，适用于多种深度学习库。

Abstract: We introduce a neural network layer API and library for sequence modeling,
designed for easy creation of sequence models that can be executed both
layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,
autoregressive sampling). To achieve this, layers define an explicit
representation of their state over time (e.g., a Transformer KV cache, a
convolution buffer, an RNN hidden state), and a step method that evolves that
state, tested to give identical results to a stateless layer-wise invocation.
This and other aspects of the SequenceLayers contract enables complex models to
be immediately streamable, mitigates a wide range of common bugs arising in
both streaming and parallel sequence processing, and can be implemented in any
deep learning library. A composable and declarative API, along with a
comprehensive suite of layers and combinators, streamlines the construction of
production-scale models from simple streamable components while preserving
strong correctness guarantees. Our current implementations of SequenceLayers
(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.

</details>


### [71] [Scalable and Precise Patch Robustness Certification for Deep Learning Models with Top-k Predictions](https://arxiv.org/abs/2507.23335)
*Qilin Zhou,Haipeng Wang,Zhengyuan Wei,W. K. Chan*

Main category: cs.LG

TL;DR: 本文提出了CostCert，一种新型、可扩展且精确的基于投票的认证恢复防御方法，解决了现有技术在认证样本真实标签时面临的组合爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 现有技术在对抗补丁攻击中提供可证明的保证，但在处理前k预测时表现不佳，容易受到攻击者控制票数的影响，且存在组合爆炸问题。

Method: CostCert通过验证攻击预算是否无法覆盖最小的额外票数，来确保真实标签在前k预测中，无需进行成对比较或组合枚举。

Result: 实验表明，CostCert显著优于现有最佳防御方法PatchGuard，例如在补丁大小为96时，认证准确率仍可达57.3%，而PatchGuard已降至零。

Conclusion: CostCert提供了一种高效且精确的方法来认证样本真实标签，解决了现有技术的局限性，为对抗补丁攻击提供更强的鲁棒性。

Abstract: Patch robustness certification is an emerging verification approach for
defending against adversarial patch attacks with provable guarantees for deep
learning systems. Certified recovery techniques guarantee the prediction of the
sole true label of a certified sample. However, existing techniques, if
applicable to top-k predictions, commonly conduct pairwise comparisons on those
votes between labels, failing to certify the sole true label within the top k
prediction labels precisely due to the inflation on the number of votes
controlled by the attacker (i.e., attack budget); yet enumerating all
combinations of vote allocation suffers from the combinatorial explosion
problem. We propose CostCert, a novel, scalable, and precise voting-based
certified recovery defender. CostCert verifies the true label of a sample
within the top k predictions without pairwise comparisons and combinatorial
explosion through a novel design: whether the attack budget on the sample is
infeasible to cover the smallest total additional votes on top of the votes
uncontrollable by the attacker to exclude the true labels from the top k
prediction labels. Experiments show that CostCert significantly outperforms the
current state-of-the-art defender PatchGuard, such as retaining up to 57.3% in
certified accuracy when the patch size is 96, whereas PatchGuard has already
dropped to zero.

</details>


### [72] [LLM-Assisted Cheating Detection in Korean Language via Keystrokes](https://arxiv.org/abs/2507.22956)
*Dong Hyun Roh,Rajesh Kumar,An Ngo*

Main category: cs.LG

TL;DR: 提出基于击键的动态检测框架，用于识别韩语中LLM辅助作弊，填补语言覆盖、认知上下文和LLM参与粒度等研究空白。实验表明，击键特征在认知感知和跨认知场景中表现优异，模型显著优于人工评估。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究在语言覆盖、认知上下文和LLM参与粒度方面的不足，提出一种可靠的方法检测LLM辅助写作。

Method: 69名参与者在三种条件下完成写作任务（真实写作、改写ChatGPT回答、转录ChatGPT回答），提取时空和节奏特征，并在认知感知与认知非感知设置下评估分类器。

Result: 时空特征在认知感知场景中表现良好，节奏特征在跨认知场景中泛化能力强。模型检测真实和转录回答优于改写回答，且显著优于人工评估。

Conclusion: 击键动力学可有效检测LLM辅助写作，适应不同认知需求和写作策略。

Abstract: This paper presents a keystroke-based framework for detecting LLM-assisted
cheating in Korean, addressing key gaps in prior research regarding language
coverage, cognitive context, and the granularity of LLM involvement. Our
proposed dataset includes 69 participants who completed writing tasks under
three conditions: Bona fide writing, paraphrasing ChatGPT responses, and
transcribing ChatGPT responses. Each task spans six cognitive processes defined
in Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and
create). We extract interpretable temporal and rhythmic features and evaluate
multiple classifiers under both Cognition-Aware and Cognition-Unaware settings.
Temporal features perform well under Cognition-Aware evaluation scenarios,
while rhythmic features generalize better under cross-cognition scenarios.
Moreover, detecting bona fide and transcribed responses was easier than
paraphrased ones for both the proposed models and human evaluators, with the
models significantly outperforming the humans. Our findings affirm that
keystroke dynamics facilitate reliable detection of LLM-assisted writing across
varying cognitive demands and writing strategies, including paraphrasing and
transcribing LLM-generated responses.

</details>


### [73] [Improving annotator selection in Active Learning using a mood and fatigue-aware Recommender System](https://arxiv.org/abs/2507.23756)
*Diana Mortagua*

Main category: cs.LG

TL;DR: 本研究提出一种基于知识推荐系统的查询-注释者配对策略，通过考虑注释者的情绪和疲劳水平等内部因素，减少标注错误和提高模型效率。


<details>
  <summary>Details</summary>
Motivation: 主动学习（AL）虽能减少标注数据需求，但仍需解决注释者的内部因素（如情绪、疲劳）对标注准确性的影响。现有方法未充分考虑这些因素，导致标注效率不足。

Method: 采用知识推荐系统（RS）对注释者进行排名，结合其历史准确性、情绪和疲劳水平，为查询实例选择最佳注释者。模拟真实标注场景并预测其表现。

Result: 结果显示，考虑内部因素后，标注错误和模型不确定性显著减少，准确性和F1-score也有所提升。

Conclusion: 本研究初步探索了人类认知因素对主动学习的影响，提出了一种更高效的标注策略。

Abstract: This study centers on overcoming the challenge of selecting the best
annotators for each query in Active Learning (AL), with the objective of
minimizing misclassifications. AL recognizes the challenges related to cost and
time when acquiring labeled data, and decreases the number of labeled data
needed. Nevertheless, there is still the necessity to reduce annotation errors,
aiming to be as efficient as possible, to achieve the expected accuracy faster.
Most strategies for query-annotator pairs do not consider internal factors that
affect productivity, such as mood, attention, motivation, and fatigue levels.
This work addresses this gap in the existing literature, by not only
considering how the internal factors influence annotators (mood and fatigue
levels) but also presenting a new query-annotator pair strategy, using a
Knowledge-Based Recommendation System (RS). The RS ranks the available
annotators, allowing to choose one or more to label the queried instance using
their past accuracy values, and their mood and fatigue levels, as well as
information about the instance queried. This work bases itself on existing
literature on mood and fatigue influence on human performance, simulating
annotators in a realistic manner, and predicting their performance with the RS.
The results show that considering past accuracy values, as well as mood and
fatigue levels reduces the number of annotation errors made by the annotators,
and the uncertainty of the model through its training, when compared to not
using internal factors. Accuracy and F1-score values were also better in the
proposed approach, despite not being as substantial as the aforementioned. The
methodologies and findings presented in this study begin to explore the open
challenge of human cognitive factors affecting AL.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [74] [Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving](https://arxiv.org/abs/2507.23042)
*Santosh Patapati,Trisanth Srinivasan*

Main category: cs.CV

TL;DR: NovaDrive是一种单分支视觉-语言架构，通过处理前视摄像头图像、高清地图、LiDAR深度和文本路径点，结合新颖的平滑损失函数和两阶段交叉注意力块，实现了高效、安全的自动驾驶导航，显著提升了成功率和路径效率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要在复杂环境中快速反应，同时理解道路几何和交通意图。为了解决这一问题，NovaDrive通过一体化的单分支设计，降低了计算复杂度并提高了实时性能。

Method: 采用单分支架构处理多模态输入（图像、高清地图、LiDAR深度和路径点），设计了轻量级的两阶段交叉注意力块和新型平滑损失函数，部分微调了大型视觉-语言模型（LLaMA-3.2）的顶层。

Result: 在nuScenes/Waymo子集上，NovaDrive将成功率提升至84%（+4%），路径效率提升至0.66（+0.11），碰撞频率从2.6%降至1.2%（-1.4%）。

Conclusion: NovaDrive通过多模态融合和优化设计，显著提升了自动驾驶的安全性和效率，同时为其他具身AI领域提供了可扩展的解决方案。

Abstract: Autonomous vehicles must react in milliseconds while reasoning about road
geometry and traffic intent to navigate complex situations. We introduce
NovaDrive, a single-branch vision-language architecture that processes
front-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a
single branch. A lightweight, two-stage cross-attention block first aligns
waypoint tokens with the HD map, then refines attention over fine-grained image
and depth patches. Coupled with a novel smoothness loss that discourages abrupt
steering and speed changes, this design eliminates the need for recurrent
memory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language
backbone, enabling real-time inference. On the nuScenes / Waymo subset of the
MD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts
path-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from
2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations
confirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention
fusion each contribute the most to these gains. Beyond safety, NovaDrive's
shorter routes (resulting from the novel smoothness loss) translate to lower
fuel or battery usage, pointing toward leaner, more easily updated driving
stacks. NovaDrive can be extended to other embodied-AI domains as well.

</details>


### [75] [Phi-Ground Tech Report: Advancing Perception in GUI Grounding](https://arxiv.org/abs/2507.23779)
*Miaosen Zhang,Ziqiang Xu,Jialiang Zhu,Qi Dai,Kai Qiu,Yifan Yang,Chong Luo,Tianyi Chen,Justin Wagle,Tim Franklin,Baining Guo*

Main category: cs.CV

TL;DR: 本文介绍了**Phi-Ground**模型族，在GUI接地任务中取得SOTA性能，解决了现有端到端模型在挑战性基准测试中准确率低的问题。


<details>
  <summary>Details</summary>
Motivation: GUI接地是计算机使用代理（CUAs）执行实际动作的核心组件，但目前端到端接地模型在基准测试中表现不佳，限制了实际应用。

Method: 通过实证研究，从数据收集到模型训练的各个环节进行优化，开发了**Phi-Ground**模型族。

Result: 在5个接地基准测试中，**Phi-Ground**在较小参数规模下取得SOTA性能，并在端到端设置中显著提升了ScreenSpot-pro（43.2）和UI-Vision（27.2）的得分。

Conclusion: 本文的研究细节和成果不仅有助于接地模型的构建，也对其他感知任务具有参考价值。

Abstract: With the development of multimodal reasoning models, Computer Use Agents
(CUAs), akin to Jarvis from \textit{"Iron Man"}, are becoming a reality. GUI
grounding is a core component for CUAs to execute actual actions, similar to
mechanical control in robotics, and it directly leads to the success or failure
of the system. It determines actions such as clicking and typing, as well as
related parameters like the coordinates for clicks. Current end-to-end
grounding models still achieve less than 65\% accuracy on challenging
benchmarks like ScreenSpot-pro and UI-Vision, indicating they are far from
being ready for deployment. % , as a single misclick can result in unacceptable
consequences. In this work, we conduct an empirical study on the training of
grounding models, examining details from data collection to model training.
Ultimately, we developed the \textbf{Phi-Ground} model family, which achieves
state-of-the-art performance across all five grounding benchmarks for models
under $10B$ parameters in agent settings. In the end-to-end model setting, our
model still achieves SOTA results with scores of \textit{\textbf{43.2}} on
ScreenSpot-pro and \textit{\textbf{27.2}} on UI-Vision. We believe that the
various details discussed in this paper, along with our successes and failures,
not only clarify the construction of grounding models but also benefit other
perception tasks. Project homepage:
\href{https://zhangmiaosen2000.github.io/Phi-Ground/}{https://zhangmiaosen2000.github.io/Phi-Ground/}

</details>


### [76] [Consistent Point Matching](https://arxiv.org/abs/2507.23609)
*Halid Ziya Yerebakan,Gerardo Hermosillo Valadez*

Main category: cs.CV

TL;DR: 通过引入一致启发式算法改进点匹配，提高医学图像匹配鲁棒性，并在多个数据集上验证效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升医学图像中解剖位置匹配的鲁棒性，特别是在无需机器学习模型或训练数据的情况下。

Method: 将一致启发式算法集成到点匹配算法中，优化匹配过程，支持速度和鲁棒性的可配置权衡。

Result: 在多个数据集（包括Deep Lesion Tracking）上表现优于现有方法，同时高效运行于标准CPU硬件。

Conclusion: 该方法为医学图像的高精度导航提供了一种无需训练数据的有效解决方案。

Abstract: This study demonstrates that incorporating a consistency heuristic into the
point-matching algorithm \cite{yerebakan2023hierarchical} improves robustness
in matching anatomical locations across pairs of medical images. We validated
our approach on diverse longitudinal internal and public datasets spanning CT
and MRI modalities. Notably, it surpasses state-of-the-art results on the Deep
Lesion Tracking dataset. Additionally, we show that the method effectively
addresses landmark localization. The algorithm operates efficiently on standard
CPU hardware and allows configurable trade-offs between speed and robustness.
The method enables high-precision navigation between medical images without
requiring a machine learning model or training data.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [77] [Complexity-energy trade-off in programmable unitary interferometers](https://arxiv.org/abs/2507.22972)
*Nikita A. Nemkov,Stanislav S. Straupe*

Main category: physics.optics

TL;DR: 论文指出，现有相干多端口干涉仪（如MZI和分束器网格）的编程复杂性是固有的，且高效的编程算法会降低输出能量和精度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示相干多端口干涉仪在实现矩阵乘法时面临的编程复杂性及其对输出能量和效率的影响。

Method: 分析了现有干涉仪架构（如MZI和分束器网格）的编程复杂性，并探讨高效编程算法的局限性。

Result: 发现编程复杂性是内在问题，高效编程算法会导致输出能量降低，从而限制精度和能效。

Conclusion: 现有的干涉仪架构在编程复杂性和输出能量之间存在权衡，需进一步优化。

Abstract: Coherent multiport interferometers are a promising approach to realize matrix
multiplication in integrated photonics. However, most known architectures -
such as MZI and beamsplitter meshes, as well as more general interferometers -
suffer from complicated procedures for mapping the matrix elements of the
desired transformation to specific phaseshifts in the device. We point out that
the high programming complexity is intrinsic, rather than accidental. At the
same time, we argue that interferometers admitting efficient programming
algorithms in general yield a much lower useful output energy, which ultimately
limits their accuracy and energy efficiency.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [78] [Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance](https://arxiv.org/abs/2507.23088)
*Lalithkumar Seenivasan,Jiru Xu,Roger D. Soberanis Mukul,Hao Ding,Grayson Byrd,Yu-Chun Ku,Jose L. Porras,Masaru Ishii,Mathias Unberath*

Main category: cs.RO

TL;DR: 该论文提出了一种新型的感知代理，通过语音集成和大语言模型，结合记忆库和两种新机制，实现了手术环境中更自然的人机交互。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的手术辅助解决方案缺乏灵活性，限制了人机交互的自然性，尤其是在动态手术环境中。

Method: 提出了一种基于语音集成的大型语言模型、分割模型和跟踪基础模型的感知代理，结合记忆库和两种新机制，提高了交互灵活性和适应性。

Result: 在公开数据集上的定量分析表明，该代理的性能与人工提示策略相当；在定制数据集上展现了分割新元素的灵活性。

Conclusion: 该感知代理通过自然的人机交互和克服刚性限制，有望将AI驱动的实时手术辅助推向现实。

Abstract: Emerging surgical data science and robotics solutions, especially those
designed to provide assistance in situ, require natural human-machine
interfaces to fully unlock their potential in providing adaptive and intuitive
aid. Contemporary AI-driven solutions remain inherently rigid, offering limited
flexibility and restricting natural human-machine interaction in dynamic
surgical environments. These solutions rely heavily on extensive task-specific
pre-training, fixed object categories, and explicit manual-prompting. This work
introduces a novel Perception Agent that leverages speech-integrated
prompt-engineered large language models (LLMs), segment anything model (SAM),
and any-point tracking foundation models to enable a more natural human-machine
interaction in real-time intraoperative surgical assistance. Incorporating a
memory repository and two novel mechanisms for segmenting unseen elements,
Perception Agent offers the flexibility to segment both known and unseen
elements in the surgical scene through intuitive interaction. Incorporating the
ability to memorize novel elements for use in future surgeries, this work takes
a marked step towards human-machine symbiosis in surgical procedures. Through
quantitative analysis on a public dataset, we show that the performance of our
agent is on par with considerably more labor-intensive manual-prompting
strategies. Qualitatively, we show the flexibility of our agent in segmenting
novel elements (instruments, phantom grafts, and gauze) in a custom-curated
dataset. By offering natural human-machine interaction and overcoming rigidity,
our Perception Agent potentially brings AI-based real-time assistance in
dynamic surgical environments closer to reality.

</details>


### [79] [User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals](https://arxiv.org/abs/2507.23544)
*Ryo Miyoshi,Yuki Okafuji,Takuya Iwamoto,Junya Nakanishi,Jun Baba*

Main category: cs.RO

TL;DR: 提出一种基于多模态社交信号的用户体验（UX）估计方法，通过Transformer模型结合面部表情和语音数据，优于传统方法和人类评估。


<details>
  <summary>Details</summary>
Motivation: 社会机器人需根据用户状态调整行为，准确评估HRI中的UX是实现这一目标的关键。现有方法多关注单一指标，无法全面衡量多方面的用户体验。

Method: 构建UX数据集，开发基于Transformer的模型，利用多实例学习框架整合短期和长期交互模式，捕捉UX的时空动态。

Result: 实验表明，该方法在UX估计中优于第三方人类评估者。

Conclusion: 提出的多模态方法能更全面地捕捉UX，为HRI中的自适应行为提供了有效工具。

Abstract: In recent years, the demand for social robots has grown, requiring them to
adapt their behaviors based on users' states. Accurately assessing user
experience (UX) in human-robot interaction (HRI) is crucial for achieving this
adaptability. UX is a multi-faceted measure encompassing aspects such as
sentiment and engagement, yet existing methods often focus on these
individually. This study proposes a UX estimation method for HRI by leveraging
multimodal social signals. We construct a UX dataset and develop a
Transformer-based model that utilizes facial expressions and voice for
estimation. Unlike conventional models that rely on momentary observations, our
approach captures both short- and long-term interaction patterns using a
multi-instance learning framework. This enables the model to capture temporal
dynamics in UX, providing a more holistic representation. Experimental results
demonstrate that our method outperforms third-party human evaluators in UX
estimation.

</details>


### [80] [Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation](https://arxiv.org/abs/2507.23592)
*Haiyun Zhang,Stefano Dalla Gasperina,Saad N. Yousaf,Toshimitsu Tsuboi,Tetsuya Narita,Ashish D. Deshpande*

Main category: cs.RO

TL;DR: 提出了一种基于冗余关节感知和残差加权优化的手部外骨骼校准框架，显著提高了跨用户的手部跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 由于用户解剖学差异和佩戴不一致，手部外骨骼跟踪精度受限，影响精准任务应用。

Method: 使用冗余关节感知和残差加权优化策略估算虚拟链接参数，并通过运动捕捉数据调整成本函数权重。

Result: 在七名被试中，关节和指尖跟踪误差显著降低，虚拟手运动逼真度提升。

Conclusion: 该框架适用于多种外骨骼设计，为高保真遥操作和示范学习提供了基础。

Abstract: Hand exoskeletons are critical tools for dexterous teleoperation and
immersive manipulation interfaces, but achieving accurate hand tracking remains
a challenge due to user-specific anatomical variability and donning
inconsistencies. These issues lead to kinematic misalignments that degrade
tracking performance and limit applicability in precision tasks. We propose a
subject-specific calibration framework for exoskeleton-based hand tracking that
uses redundant joint sensing and a residual-weighted optimization strategy to
estimate virtual link parameters. Implemented on the Maestro exoskeleton, our
method improves joint angle and fingertip position estimation across users with
varying hand geometries. We introduce a data-driven approach to empirically
tune cost function weights using motion capture ground truth, enabling more
accurate and consistent calibration across participants. Quantitative results
from seven subjects show substantial reductions in joint and fingertip tracking
errors compared to uncalibrated and evenly weighted models. Qualitative
visualizations using a Unity-based virtual hand further confirm improvements in
motion fidelity. The proposed framework generalizes across exoskeleton designs
with closed-loop kinematics and minimal sensing, and lays the foundation for
high-fidelity teleoperation and learning-from-demonstration applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [81] [Chatting with your ERP: A Recipe](https://arxiv.org/abs/2507.23429)
*Jorge Ruiz Gómez,Lidia Andrés Susinos,Jorge Alamo Olivé,Sonia Rey Osorno,Manuel Luis Gonzalez Hernández*

Main category: cs.AI

TL;DR: 论文介绍了一种基于大型语言模型（LLM）的代理，可与工业级ERP系统对话，通过自然语言查询生成可执行SQL语句，并采用双代理架构提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决工业级ERP系统中自然语言查询的翻译和执行问题，提升用户体验和系统交互效率。

Method: 提出了一种新颖的双代理架构，结合推理和批判阶段，利用开源权重的LLM生成可靠的SQL查询。

Result: 实现了能够高效、可靠地将自然语言查询转换为SQL语句的代理系统。

Conclusion: 研究证明了双代理架构在提升查询生成可靠性方面的有效性，为工业级系统的自然语言交互提供了实用解决方案。

Abstract: This paper presents the design, implementation, and evaluation behind a Large
Language Model (LLM) agent that chats with an industrial production-grade ERP
system. The agent is capable of interpreting natural language queries and
translating them into executable SQL statements, leveraging open-weight LLMs. A
novel dual-agent architecture combining reasoning and critique stages was
proposed to improve query generation reliability.

</details>


### [82] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 本文提出了一种适用于高性能计算环境的二维数据准备框架，用于评估和提升科学数据在AI训练中的准备度，强调跨领域标准化支持。


<details>
  <summary>Details</summary>
Motivation: 探讨科学数据在AI训练中的准备度问题，特别是在高性能计算环境下，如何通过标准化框架提升数据处理的效率和可重复性。

Method: 提出了一个由数据准备级别（从原始到AI就绪）和数据处理阶段（从摄入到分片）组成的二维框架，并结合四个代表性领域的典型工作流进行分析。

Result: 揭示了科学数据在AI训练准备中的共性预处理模式和领域特定约束，形成了一种概念性的成熟度矩阵。

Conclusion: 该框架为科学数据的高效AI训练提供了指导，促进了跨领域的标准化和可扩展性支持。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>
