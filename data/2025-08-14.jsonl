{"id": "2508.09332", "pdf": "https://arxiv.org/pdf/2508.09332", "abs": "https://arxiv.org/abs/2508.09332", "authors": ["Anshul Khairnar", "Aarya Rajoju", "Edward F. Gehringer"], "title": "Teaching Code Refactoring Using LLMs", "categories": ["cs.SE", "cs.LG"], "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "This Innovative Practice full paper explores how Large Language Models (LLMs)\ncan enhance the teaching of code refactoring in software engineering courses\nthrough real-time, context-aware feedback. Refactoring improves code quality\nbut is difficult to teach, especially with complex, real-world codebases.\nTraditional methods like code reviews and static analysis tools offer limited,\ninconsistent feedback. Our approach integrates LLM-assisted refactoring into a\ncourse project using structured prompts to help students identify and address\ncode smells such as long methods and low cohesion. Implemented in Spring 2025\nin a long-lived OSS project, the intervention is evaluated through student\nfeedback and planned analysis of code quality improvements. Findings suggest\nthat LLMs can bridge theoretical and practical learning, supporting a deeper\nunderstanding of maintainability and refactoring principles."}
{"id": "2508.09366", "pdf": "https://arxiv.org/pdf/2508.09366", "abs": "https://arxiv.org/abs/2508.09366", "authors": ["Qiaolin Qin", "Xingfang Wu", "Heng Li", "Ettore Merlo"], "title": "Plug it and Play on Logs: A Configuration-Free Statistic-Based Log Parser", "categories": ["cs.SE", "D.2.5"], "comment": null, "summary": "Log parsing is an essential task in log analysis, and many tools have been\ndesigned to accomplish it. Existing log parsers can be categorized into\nstatistic-based and semantic-based approaches. In comparison to semantic-based\nparsers, existing statistic-based parsers tend to be more efficient, require\nlower computational costs, and be more privacy-preserving thanks to on-premise\ndeployment, but often fall short in their accuracy (e.g., grouping or parsing\naccuracy) and generalizability. Therefore, it became a common belief that\nstatistic-based parsers cannot be as effective as semantic-based parsers since\nthe latter could take advantage of external knowledge supported by pretrained\nlanguage models. Our work, however, challenges this belief with a novel\nstatistic-based parser, PIPLUP. PIPLUP eliminates the pre-assumption of the\nposition of constant tokens for log grouping and relies on data-insensitive\nparameters to overcome the generalizability challenge, allowing \"plug and play\"\non given log files. According to our experiments on an open-sourced large log\ndataset, PIPLUP shows promising accuracy and generalizability with the\ndata-insensitive default parameter set. PIPLUP not only outperforms the\nstate-of-the-art statistic-based log parsers, Drain and its variants, but also\nobtains a competitive performance compared to the best unsupervised\nsemantic-based log parser (i.e., LUNAR). Further, PIPLUP exhibits low time\nconsumption without GPU acceleration and external API usage; our simple,\nefficient, and effective approach makes it more practical in real-world\nadoptions, especially when costs and privacy are of major concerns."}
{"id": "2508.09537", "pdf": "https://arxiv.org/pdf/2508.09537", "abs": "https://arxiv.org/abs/2508.09537", "authors": ["Yanzhou Li", "Tianlin Li", "Yiran Zhang", "Shangqing Liu", "Aishan Liu", "Yang Liu"], "title": "Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used for function completion in\nrepository-scale codebases. Prior studies demonstrate that when explicit\ninstructions--such as docstrings--are provided, these models can generate\nhighly accurate implementations. However, in real-world repositories, such\nannotations are frequently absent, and performance drops substantially without\nthem. To address this gap, we frame the task as a three-stage process. The\nfirst stage focuses on intent inference, where the model analyzes the code\npreceding the target function to uncover cues about the desired functionality.\nSuch preceding context often encodes subtle but critical information, and we\ndesign a reasoning-based prompting framework to guide the LLM through\nstep-by-step extraction and synthesis of these signals before any code is\ngenerated. The second stage introduces an optional interactive refinement\nmechanism to handle cases where preceding context alone is insufficient for\nintent recovery. In this stage, the model proposes a small set of candidate\nintentions, enabling the developer to select or edit them so that the inferred\nintent closely matches the actual requirement. Finally, in the third stage, the\nLLM generates the target function conditioned on the finalized intent. To\nsupport this pipeline, we curate a dataset of 40,000 examples annotated with\nintermediate reasoning traces and corresponding docstrings. Extensive\nexperiments on DevEval and ComplexCodeEval show that our approach consistently\nboosts multiple LLMs, achieving over 20\\% relative gains in both\nreference-based and execution-based metrics, with the interactive refinement\nstage delivering additional improvements beyond these gains."}
{"id": "2508.09648", "pdf": "https://arxiv.org/pdf/2508.09648", "abs": "https://arxiv.org/abs/2508.09648", "authors": ["Taohong Zhu", "Lucas C. Cordeiro", "Youcheng Sun"], "title": "ReqInOne: A Large Language Model-Based Agent for Software Requirements Specification Generation", "categories": ["cs.SE"], "comment": null, "summary": "Software Requirements Specification (SRS) is one of the most important\ndocuments in software projects, but writing it manually is time-consuming and\noften leads to ambiguity. Existing automated methods rely heavily on manual\nanalysis, while recent Large Language Model (LLM)-based approaches suffer from\nhallucinations and limited controllability. In this paper, we propose ReqInOne,\nan LLM-based agent that follows the common steps taken by human requirements\nengineers when writing an SRS to convert natural language into a structured\nSRS. ReqInOne adopts a modular architecture by decomposing SRS generation into\nthree tasks: summary, requirement extraction, and requirement classification,\neach supported by tailored prompt templates to improve the quality and\nconsistency of LLM outputs.\n  We evaluate ReqInOne using GPT-4o, LLaMA 3, and DeepSeek-R1, and compare the\ngenerated SRSs against those produced by the holistic GPT-4-based method from\nprior work as well as by entry-level requirements engineers. Expert evaluations\nshow that ReqInOne produces more accurate and well-structured SRS documents.\nThe performance advantage of ReqInOne benefits from its modular design, and\nexperimental results further demonstrate that its requirement classification\ncomponent achieves comparable or even better results than the state-of-the-art\nrequirement classification model."}
{"id": "2508.09856", "pdf": "https://arxiv.org/pdf/2508.09856", "abs": "https://arxiv.org/abs/2508.09856", "authors": ["Mathieu Boespflug", "Arnaud Spiwack"], "title": "Invertible Syntax without the Tuples (Functional Pearl)", "categories": ["cs.PL"], "comment": null, "summary": "In the seminal paper Functional unparsing, Olivier Danvy used continuation\npassing to reanalyse printf-like format strings as combinators. In the\nintervening decades, the conversation shifted towards a concurrent line of work\n-- applicative, monadic or arrow-based combinator libraries -- in an effort to\nfind combinators for invertible syntax descriptions that simultaneously\ndetermine a parser as well as a printer, and with more expressive power, able\nto handle inductive structures such as lists and trees. Along the way,\ncontinuation passing got lost. This paper argues that Danvy's insight remains\nas relevant to the general setting as it was to the restricted setting of his\noriginal paper. Like him, we present three solutions that exploit\ncontinuation-passing style as an alternative to both dependent types and\nmonoidal aggregation via nested pairs, in our case to parse and print\nstructured data with increasing expressive power."}
{"id": "2508.09351", "pdf": "https://arxiv.org/pdf/2508.09351", "abs": "https://arxiv.org/abs/2508.09351", "authors": ["Vinicius Petrucci", "Felippe Zacarias", "David Roberts"], "title": "A Limits Study of Memory-side Tiering Telemetry", "categories": ["cs.OS", "cs.AR", "cs.PF"], "comment": null, "summary": "Increasing workload demands and emerging technologies necessitate the use of\nvarious memory and storage tiers in computing systems. This paper presents\nresults from a CXL-based Experimental Memory Request Logger that reveals\nprecise memory access patterns at runtime without interfering with the running\nworkloads. We use it for software emulation of future memory telemetry\nhardware. By combining reactive placement based on data address monitoring,\nproactive data movement, and compiler hints, a Hotness Monitoring Unit (HMU)\nwithin memory modules can greatly improve memory tiering solutions. Analysis of\npage placement using profiled access counts on a Deep Learning Recommendation\nModel (DLRM) indicates a potential 1.94x speedup over Linux NUMA balancing\ntiering, and only a 3% slowdown compared to Host-DRAM allocation while\noffloading over 90% of pages to CXL memory. The study underscores the\nlimitations of existing tiering strategies in terms of coverage and accuracy,\nand makes a strong case for programmable, device-level telemetry as a scalable\nand efficient solution for future memory systems."}
{"id": "2508.09235", "pdf": "https://arxiv.org/pdf/2508.09235", "abs": "https://arxiv.org/abs/2508.09235", "authors": ["Nathaniel Gorski", "Xin Liang", "Hanqi Guo", "Bei Wang"], "title": "TFZ: Topology-Preserving Compression of 2D Symmetric and Asymmetric Second-Order Tensor Fields", "categories": ["cs.GR", "cs.CG"], "comment": "29 pages, 27 figures, to be presented at IEEE Vis 2025 (and published\n  in IEEE TVCG 2026)", "summary": "In this paper, we present a novel compression framework, TFZ, that preserves\nthe topology of 2D symmetric and asymmetric second-order tensor fields defined\non flat triangular meshes. A tensor field assigns a tensor - a\nmulti-dimensional array of numbers - to each point in space. Tensor fields,\nsuch as the stress and strain tensors, and the Riemann curvature tensor, are\nessential to both science and engineering. The topology of tensor fields\ncaptures the core structure of data, and is useful in various disciplines, such\nas graphics (for manipulating shapes and textures) and neuroscience (for\nanalyzing brain structures from diffusion MRI). Lossy data compression may\ndistort the topology of tensor fields, thus hindering downstream analysis and\nvisualization tasks. TFZ ensures that certain topological features are\npreserved during lossy compression. Specifically, TFZ preserves degenerate\npoints essential to the topology of symmetric tensor fields and retains\neigenvector and eigenvalue graphs that represent the topology of asymmetric\ntensor fields. TFZ scans through each cell, preserving the local topology of\neach cell, and thereby ensuring certain global topological guarantees. We\nshowcase the effectiveness of our framework in enhancing the lossy scientific\ndata compressors SZ3 and SPERR."}
{"id": "2508.09351", "pdf": "https://arxiv.org/pdf/2508.09351", "abs": "https://arxiv.org/abs/2508.09351", "authors": ["Vinicius Petrucci", "Felippe Zacarias", "David Roberts"], "title": "A Limits Study of Memory-side Tiering Telemetry", "categories": ["cs.OS", "cs.AR", "cs.PF"], "comment": null, "summary": "Increasing workload demands and emerging technologies necessitate the use of\nvarious memory and storage tiers in computing systems. This paper presents\nresults from a CXL-based Experimental Memory Request Logger that reveals\nprecise memory access patterns at runtime without interfering with the running\nworkloads. We use it for software emulation of future memory telemetry\nhardware. By combining reactive placement based on data address monitoring,\nproactive data movement, and compiler hints, a Hotness Monitoring Unit (HMU)\nwithin memory modules can greatly improve memory tiering solutions. Analysis of\npage placement using profiled access counts on a Deep Learning Recommendation\nModel (DLRM) indicates a potential 1.94x speedup over Linux NUMA balancing\ntiering, and only a 3% slowdown compared to Host-DRAM allocation while\noffloading over 90% of pages to CXL memory. The study underscores the\nlimitations of existing tiering strategies in terms of coverage and accuracy,\nand makes a strong case for programmable, device-level telemetry as a scalable\nand efficient solution for future memory systems."}
{"id": "2508.09297", "pdf": "https://arxiv.org/pdf/2508.09297", "abs": "https://arxiv.org/abs/2508.09297", "authors": ["Shiyang Lai", "Junsol Kim", "Nadav Kunievsky", "Yujin Potter", "James Evans"], "title": "Based AI improves human decision-making but reduces trust", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": null, "summary": "Current AI systems minimize risk by enforcing ideological neutrality, yet\nthis may introduce automation bias by suppressing cognitive engagement in human\ndecision-making. We conducted randomized trials with 2,500 participants to test\nwhether culturally biased AI enhances human decision-making. Participants\ninteracted with politically diverse GPT-4o variants on information evaluation\ntasks. Partisan AI assistants enhanced human performance, increased engagement,\nand reduced evaluative bias compared to non-biased counterparts, with amplified\nbenefits when participants encountered opposing views. These gains carried a\ntrust penalty: participants underappreciated biased AI and overcredited neutral\nsystems. Exposing participants to two AIs whose biases flanked human\nperspectives closed the perception-performance gap. These findings complicate\nconventional wisdom about AI neutrality, suggesting that strategic integration\nof diverse cultural biases may foster improved and resilient human\ndecision-making."}
{"id": "2508.09318", "pdf": "https://arxiv.org/pdf/2508.09318", "abs": "https://arxiv.org/abs/2508.09318", "authors": ["Alexander Steen", "Geoff Sutcliffe"], "title": "TPTP World Infrastructure for Non-classical Logics", "categories": ["cs.LO", "cs.AI", "68T27", "I.2.3; I.2.4; F.4.1"], "comment": "35 pages", "summary": "The TPTP World is the well established infrastructure that supports research,\ndevelopment, and deployment of Automated Theorem Proving (ATP) systems. The\nTPTP World supports a range of classical logics, and since release v9.0.0 has\nsupported non-classical logics. This paper provides a self-contained\ncomprehensive overview of the TPTP World infrastructure for ATP in\nnon-classical logics: the non-classical language extension, problems and\nsolutions, and tool support. A detailed description of use of the\ninfrastructure for quantified normal multi-modal logic is given."}
{"id": "2508.09232", "pdf": "https://arxiv.org/pdf/2508.09232", "abs": "https://arxiv.org/abs/2508.09232", "authors": ["Nick Oh", "Giorgos D. Vrakas", "Siân J. M. Brooke", "Sasha Morinière", "Toju Duke"], "title": "PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research", "categories": ["cs.MM", "cs.AI", "cs.DB"], "comment": null, "summary": "Social media data presents AI researchers with overlapping obligations under\nthe GDPR, copyright law, and platform terms -- yet existing frameworks fail to\nintegrate these regulatory domains, leaving researchers without unified\nguidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and\nPresent), a compliance framework that embeds legal safeguards directly into\nextended ETL pipelines. Central to PETLP is treating Data Protection Impact\nAssessments as living documents that evolve from pre-registration through\ndissemination. Through systematic Reddit analysis, we demonstrate how\nextraction rights fundamentally differ between qualifying research\norganisations (who can invoke DSM Article 3 to override platform restrictions)\nand commercial entities (bound by terms of service), whilst GDPR obligations\napply universally. We reveal why true anonymisation remains unachievable for\nsocial media data and expose the legal gap between permitted dataset creation\nand uncertain model distribution. By structuring compliance decisions into\npractical workflows and simplifying institutional data management plans, PETLP\nenables researchers to navigate regulatory complexity with confidence, bridging\nthe gap between legal requirements and research practice."}
{"id": "2508.09147", "pdf": "https://arxiv.org/pdf/2508.09147", "abs": "https://arxiv.org/abs/2508.09147", "authors": ["Alaa Saleh", "Roberto Morabito", "Sasu Tarkoma", "Anders Lindgren", "Susanna Pirttikangas", "Lauri Lovén"], "title": "Agentic TinyML for Intent-aware Handover in 6G Wireless Networks", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.LG", "cs.MA"], "comment": null, "summary": "As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,\ntraditional reactive handover mechanisms demonstrate limitations, especially in\nmobile edge computing and autonomous agent-based service scenarios. This\nmanuscript introduces WAAN, a cross-layer framework that enables intent-aware\nand proactive handovers by embedding lightweight TinyML agents as autonomous,\nnegotiation-capable entities across heterogeneous edge nodes that contribute to\nintent propagation and network adaptation. To ensure continuity across\nmobility-induced disruptions, WAAN incorporates semi-stable rendezvous points\nthat serve as coordination anchors for context transfer and state preservation.\nThe framework's operational capabilities are demonstrated through a multimodal\nenvironmental control case study, highlighting its effectiveness in maintaining\nuser experience under mobility. Finally, the article discusses key challenges\nand future opportunities associated with the deployment and evolution of WAAN."}
{"id": "2508.09458", "pdf": "https://arxiv.org/pdf/2508.09458", "abs": "https://arxiv.org/abs/2508.09458", "authors": ["Xi Long", "Christy Boscardin", "Lauren A. Maggio", "Joseph A. Costello", "Ralph Gonzales", "Rasmyah Hammoudeh", "Ki Lai", "Yoon Soo Park", "Brian C. Gin"], "title": "Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Knowledge syntheses (literature reviews) are essential to health professions\neducation (HPE), consolidating findings to advance theory and practice.\nHowever, they are labor-intensive, especially during data extraction.\nArtificial Intelligence (AI)-assisted extraction promises efficiency but raises\nconcerns about accuracy, making it critical to distinguish AI 'hallucinations'\n(fabricated content) from legitimate interpretive differences. We developed an\nextraction platform using large language models (LLMs) to automate data\nextraction and compared AI to human responses across 187 publications and 17\nextraction questions from a published scoping review. AI-human, human-human,\nand AI-AI consistencies were measured using interrater reliability\n(categorical) and thematic similarity ratings (open-ended). Errors were\nidentified by comparing extracted responses to source publications. AI was\nhighly consistent with humans for concrete, explicitly stated questions (e.g.,\ntitle, aims) and lower for questions requiring subjective interpretation or\nabsent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human\nconsistency was not higher than AI-human and showed the same question-dependent\nvariability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due\nto interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while\nhumans were nearly three times more likely to state inaccuracies (4.37%).\nFindings suggest AI accuracy depends more on interpretability than\nhallucination. Repeating AI extraction can identify interpretive complexity or\nambiguity, refining processes before human review. AI can be a transparent,\ntrustworthy partner in knowledge synthesis, though caution is needed to\npreserve critical human insights."}
{"id": "2508.09238", "pdf": "https://arxiv.org/pdf/2508.09238", "abs": "https://arxiv.org/abs/2508.09238", "authors": ["Hyunsung Kim", "Hoyoung Choi", "Sangwoo Seo", "Tom Boomstra", "Jinsung Yoon", "Chanyoung Park"], "title": "ELASTIC: Event-Tracking Data Synchronization in Soccer Without Annotated Event Locations", "categories": ["cs.DB"], "comment": "Accepted at ECML PKDD 2025 Workshop on Machine Learning and Data\n  Mining for Sports Analytics (MLSA 2025)", "summary": "The integration of event and tracking data has become essential for advanced\nanalysis in soccer. However, synchronizing these two modalities remains a\nsignificant challenge due to temporal and spatial inaccuracies in manually\nrecorded event timestamps. Existing synchronizers typically rely on annotated\nevent locations, which themselves are prone to spatial errors and thus can\ndistort synchronization results. To address this issue, we propose ELASTIC\n(Event-Location-AgnoSTIC synchronizer), a synchronization framework that only\nuses features derived from tracking data. ELASTIC also explicitly detects the\nend times of pass-like events and separates the detection of major and minor\nevents, which improves the completeness of the synchronized output and reduces\nerror cascade across events. We annotated the ground truth timestamps of 2,134\nevents from three Eredivisie matches to measure the synchronization accuracy,\nand the experimental results demonstrate that ELASTIC outperforms existing\nsynchronizers by a large margin."}
{"id": "2508.09570", "pdf": "https://arxiv.org/pdf/2508.09570", "abs": "https://arxiv.org/abs/2508.09570", "authors": ["Xiangfeng Liu", "Zhe Jiang", "Anzhen Zhu", "Xiaomeng Han", "Mingsong Lyu", "Qingxu Deng", "Nan Guan"], "title": "Re-thinking Memory-Bound Limitations in CGRAs", "categories": ["cs.AR", "B.3.0; B.6.0"], "comment": "25 pages, 18 figures, CODES+ISSS 2025", "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."}
{"id": "2508.09505", "pdf": "https://arxiv.org/pdf/2508.09505", "abs": "https://arxiv.org/abs/2508.09505", "authors": ["Zhanghan Wang", "Ding Ding", "Hang Zhu", "Haibin Lin", "Aurojit Panda"], "title": "Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "Distributed machine learning training and inference is common today because\ntoday's large models require more memory and compute than can be provided by a\nsingle GPU. Distributed models are generally produced by programmers who take a\nsequential model specification and apply several distribution strategies to\ndistribute state and computation across GPUs. Unfortunately, bugs can be\nintroduced in the process, and a distributed model implementation's outputs\nmight differ from the sequential model's outputs. In this paper, we describe an\napproach to statically identify such bugs by checking model refinement, that\nis, can the sequential model's outputs be reconstructed from the distributed\nmodel's outputs? Our approach, implemented in GraphGuard, uses iterative\nrewriting to prove model refinement. Our approach can scale to today's large\nmodels and deployments: we evaluate it using GPT and Llama-3. Further, it\nprovides actionable output that aids in bug localization."}
{"id": "2508.09676", "pdf": "https://arxiv.org/pdf/2508.09676", "abs": "https://arxiv.org/abs/2508.09676", "authors": ["Vishal Khare", "Vijay Saini", "Deepak Sharma", "Anand Kumar", "Ankit Rana", "Anshul Yadav"], "title": "DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity", "categories": ["cs.SE", "cs.LG"], "comment": "12 pages, 5 figures, 6 pages of supplementary materials", "summary": "This study investigates the implementation and efficacy of DeputyDev, an\nAI-powered code review assistant developed to address inefficiencies in the\nsoftware development process. The process of code review is highly inefficient\nfor several reasons, such as it being a time-consuming process, inconsistent\nfeedback, and review quality not being at par most of the time. Using our\ntelemetry data, we observed that at TATA 1mg, pull request (PR) processing\nexhibits significant inefficiencies, with average pick-up and review times of\n73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review\ncycle was marked by prolonged iterative communication between the reviewing and\nsubmitting parties. Research from the University of California, Irvine\nindicates that interruptions can lead to an average of 23 minutes of lost\nfocus, critically affecting code quality and timely delivery. To address these\nchallenges, we developed DeputyDev's PR review capabilities by providing\nautomated, contextual code reviews. We conducted a rigorous double-controlled\nA/B experiment involving over 200 engineers to evaluate DeputyDev's impact on\nreview times. The results demonstrated a statistically significant reduction in\nboth average per PR (23.09%) and average per-line-of-code (40.13%) review\ndurations. After implementing safeguards to exclude outliers, DeputyDev has\nbeen effectively rolled out across the entire organisation. Additionally, it\nhas been made available to external companies as a Software-as-a-Service (SaaS)\nsolution, currently supporting the daily work of numerous engineering\nprofessionals. This study explores the implementation and effectiveness of\nAI-assisted code reviews in improving development workflow timelines and code."}
{"id": "2508.09503", "pdf": "https://arxiv.org/pdf/2508.09503", "abs": "https://arxiv.org/abs/2508.09503", "authors": ["Mingcong Han", "Weihang Shen", "Rong Chen", "Binyu Zang", "Haibo Chen"], "title": "Holistic Heterogeneous Scheduling for Autonomous Applications using Fine-grained, Multi-XPU Abstraction", "categories": ["cs.OS"], "comment": null, "summary": "Modern autonomous applications are increasingly utilizing multiple\nheterogeneous processors (XPUs) to accelerate different stages of algorithm\nmodules. However, existing runtime systems for these applications, such as ROS,\ncan only perform module-level task management, lacking awareness of the\nfine-grained usage of multiple XPUs. This paper presents XAUTO, a runtime\nsystem designed to cooperatively manage XPUs for latency-sensitive autonomous\napplications. The key idea is a fine-grained, multi-XPU programming abstraction\n-- XNODE, which aligns with the stage-level task granularity and can\naccommodate multiple XPU implementations. XAUTO holistically assigns XPUs to\nXNODEs and schedules their execution to minimize end-to-end latency.\nExperimental results show that XAUTO can reduce the end-to-end latency of a\nperception pipeline for autonomous driving by 1.61x compared to a\nstate-of-the-art module-level scheduling system (ROS2)."}
{"id": "2508.09610", "pdf": "https://arxiv.org/pdf/2508.09610", "abs": "https://arxiv.org/abs/2508.09610", "authors": ["Jiachen Li", "Guangzhi Han", "Jin Wan", "Yuan Gao", "Delong Han"], "title": "DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater Scene Reconstruction", "categories": ["cs.GR"], "comment": "12 pages, 4 figures", "summary": "In 3D reconstruction of underwater scenes, traditional methods based on\natmospheric optical models cannot effectively deal with the selective\nattenuation of light wavelengths and the effect of suspended particle\nscattering, which are unique to the water medium, and lead to color distortion,\ngeometric artifacts, and collapsing phenomena at long distances. We propose the\nDualPhys-GS framework to achieve high-quality underwater reconstruction through\na dual-path optimization mechanism. Our approach further develops a dual\nfeature-guided attenuation-scattering modeling mechanism, the RGB-guided\nattenuation optimization model combines RGB features and depth information and\ncan handle edge and structural details. In contrast, the multi-scale\ndepth-aware scattering model captures scattering effects at different scales\nusing a feature pyramid network and an attention mechanism. Meanwhile, we\ndesign several special loss functions. The attenuation scattering consistency\nloss ensures physical consistency. The water body type adaptive loss\ndynamically adjusts the weighting coefficients. The edge-aware scattering loss\nis used to maintain the sharpness of structural edges. The multi-scale feature\nloss helps to capture global and local structural information. In addition, we\ndesign a scene adaptive mechanism that can automatically identify the\nwater-body-type characteristics (e.g., clear coral reef waters or turbid\ncoastal waters) and dynamically adjust the scattering and attenuation\nparameters and optimization strategies. Experimental results show that our\nmethod outperforms existing methods in several metrics, especially in suspended\nmatter-dense regions and long-distance scenes, and the reconstruction quality\nis significantly improved."}
{"id": "2508.09573", "pdf": "https://arxiv.org/pdf/2508.09573", "abs": "https://arxiv.org/abs/2508.09573", "authors": ["Michał Rzepka", "Piotr Chołda"], "title": "Metrics for Assessing Changes in Flow-based Networks", "categories": ["cs.NI", "cs.PF"], "comment": null, "summary": "This paper addresses the challenges of evaluating network performance in the\npresence of fluctuating traffic patterns, with a particular focus on the impact\nof peak data rates on network resources. We introduce a set of metrics to\nquantify network load and measure the impact of individual flows on the overall\nnetwork state. By analyzing link and flow data through percentile values and\nsample distributions, and introducing the Utilization Score metric, the\nresearch provides insights into resource utilization under varying network\nconditions. Furthermore, we employ a modified Shapley value-based approach to\nmeasure the influence of individual flows on the network, offering a better\nunderstanding of their contribution to network performance. The paper reviews\nand compares 11 metrics across various network scenarios, evaluating their\npractical relevance for research and development. Our evaluation demonstrates\nthat these metrics effectively capture changes in network state induced by\nspecific flows, with three of them offering a broad range of valuable insights\nwhile remaining relatively easy to maintain. Moreover, the methodology\ndescribed in this paper serves as a framework for future research, with the\npotential to expand and refine the set of metrics used to evaluate flow impact\non network performance."}
{"id": "2508.09312", "pdf": "https://arxiv.org/pdf/2508.09312", "abs": "https://arxiv.org/abs/2508.09312", "authors": ["Zahra Hassanzadeh", "David Haag", "Lydia Chilton", "Jan Smeddinck", "Norman Farb", "Joseph Jay Williams"], "title": "Micro-Health Interventions: Exploring Design Strategies for 1-Minute Interventions as a Gateway to Healthy Habits", "categories": ["cs.HC"], "comment": null, "summary": "One-minute behavior change interventions might seem too brief to matter.\nCould something so short really help people build healthier routines? This work\nexplores this question through two studies examining how ultra-brief prompts\nmight encourage meaningful actions in daily life. In a formative study, we\nexplored how participants engaged with one-minute prompts across four domains:\nphysical activity, eating, screen use, and mental well-being. This revealed two\ncommon design approaches: Immediate Action prompts (simple, directive tasks)\nand Reflection-First prompts (self-awareness before action). We then conducted\na 14-day, within-subjects study comparing these two flows with 28 participants.\nSurprisingly, most participants did not notice differences in structure -- but\nresponded positively when prompts felt timely, relevant, or emotionally\nsupportive. Engagement was not shaped by flow type, but by content fit, tone,\nand momentary readiness. Participants also co-designed messages, favoring those\nwith step-by-step guidance, personal meaning, or sensory detail. These results\nsuggest that one-minute interventions, while easily dismissed, may serve as\nmeaningful gateways into healthier routines -- if designed to feel helpful in\nthe moment."}
{"id": "2508.09553", "pdf": "https://arxiv.org/pdf/2508.09553", "abs": "https://arxiv.org/abs/2508.09553", "authors": ["Anne-Marie George", "Ana Ozaki"], "title": "On Middle Grounds for Preference Statements", "categories": ["cs.LO", "cs.CC"], "comment": "Longer Version of IJCAI'25 publication under the same name", "summary": "In group decisions or deliberations, stakeholders are often confronted with\nconflicting opinions. We investigate a logic-based way of expressing such\nopinions and a formal general notion of a middle ground between stakeholders.\nInspired by the literature on preferences with hierarchical and lexicographic\nmodels, we instantiate our general framework to the case where stakeholders\nexpress their opinions using preference statements of the form I prefer 'a' to\n'b', where 'a' and 'b' are alternatives expressed over some attributes, e.g.,\nin a trolley problem, one can express I prefer to save 1 adult and 1 child to 2\nadults (and 0 children). We prove theoretical results on the existence and\nuniqueness of middle grounds. In particular, we show that, for preference\nstatements, middle grounds may not exist and may not be unique. We also provide\nalgorithms for deciding the existence and finding middle grounds."}
{"id": "2508.09535", "pdf": "https://arxiv.org/pdf/2508.09535", "abs": "https://arxiv.org/abs/2508.09535", "authors": ["Roberto Balestri"], "title": "AI Blob! LLM-Driven Recontextualization of Italian Television Archives", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.DL"], "comment": "Preprint", "summary": "This paper introduces AI Blob!, an experimental system designed to explore\nthe potential of semantic cataloging and Large Language Models (LLMs) for the\nretrieval and recontextualization of archival television footage. Drawing\nmethodological inspiration from Italian television programs such as Blob (RAI\nTre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic\nembeddings, and retrieval-augmented generation (RAG) to organize and\nreinterpret archival content. The system processes a curated dataset of 1,547\nItalian television videos by transcribing audio, segmenting it into\nsentence-level units, and embedding these segments into a vector database for\nsemantic querying. Upon user input of a thematic prompt, the LLM generates a\nrange of linguistically and conceptually related queries, guiding the retrieval\nand recombination of audiovisual fragments. These fragments are algorithmically\nselected and structured into narrative sequences producing montages that\nemulate editorial practices of ironic juxtaposition and thematic coherence. By\nforegrounding dynamic, content-aware retrieval over static metadata schemas, AI\nBlob! demonstrates how semantic technologies can facilitate new approaches to\narchival engagement, enabling novel forms of automated narrative construction\nand cultural analysis. The project contributes to ongoing debates in media\nhistoriography and AI-driven archival research, offering both a conceptual\nframework and a publicly available dataset to support further interdisciplinary\nexperimentation."}
{"id": "2508.09149", "pdf": "https://arxiv.org/pdf/2508.09149", "abs": "https://arxiv.org/abs/2508.09149", "authors": ["Seyed Hossein Ahmadpanah"], "title": "Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "Next-generation automotive applications require vehicular edge computing\n(VEC), but current management systems are essentially fixed and reactive. They\nare suboptimal in extremely dynamic vehicular environments because they are\nconstrained to static optimization objectives and base their decisions on the\ncurrent network states. This paper presents a novel Semantic-Aware Proactive\nLLM Orchestration (SP-LLM) framework to address these issues. Our method\ntransforms the traditional Digital Twin (DT) into a Predictive Digital Twin\n(pDT) that predicts important network parameters such as task arrivals, vehicle\nmobility, and channel quality. A Large Language Model (LLM) that serves as a\ncognitive orchestrator is at the heart of our framework. It makes proactive,\nforward-looking decisions about task offloading and resource allocation by\nutilizing the pDT's forecasts. The LLM's ability to decipher high-level\nsemantic commands given in natural language is crucial because it enables it to\ndynamically modify its optimization policy to match evolving strategic\nobjectives, like giving emergency services priority or optimizing energy\nefficiency. We show through extensive simulations that SP-LLM performs\nsignificantly better in terms of scalability, robustness in volatile\nconditions, and adaptability than state-of-the-art reactive and MARL-based\napproaches. More intelligent, autonomous, and goal-driven vehicular networks\nwill be possible due to our framework's outstanding capacity to convert human\nintent into optimal network behavior."}
{"id": "2508.09594", "pdf": "https://arxiv.org/pdf/2508.09594", "abs": "https://arxiv.org/abs/2508.09594", "authors": ["Fei Teng", "Haoyang Li", "Lei Chen"], "title": "LLMLog: Advanced Log Template Generation via LLM-driven Multi-Round Annotation", "categories": ["cs.DB"], "comment": "Accepted in VLDB 2025", "summary": "Modern computing systems, such as HDFS and Spark, produce vast quantities of\nlogs that developers use for tasks like anomaly detection and error analysis.\nTo simplify log analysis, template generation methods have been proposed to\nstandardize log formats, transforming unstructured data into structured\ntemplates. Existing heuristic-based methods and neural network-based methods\nsuffer from low accuracy problems due to the reliance on handcrafted heuristics\nor specific log patterns in training sets. Recently, large language models\n(LLMs) have shown great potential in log template generation. However, they\noften struggle with ambiguous, complex, or highly specific log content, which\ncan lead to errors in generating accurate templates. To address these\nchallenges, we propose LLMLog, a multi-round annotation framework with adaptive\nin-context learning. We first propose an edit-distance-based similarity metric\nto evaluate log similarity. Then, we introduce a method to select the most\ninformative $k$ unlabeled logs for annotation by considering both the\nrepresentativeness of the logs and the confidence of LLM predictions.\nAdditionally, we design an adaptive context selection strategy that adaptively\nselects labeled logs to ensure comprehensive keyword coverage for unlabeled\nlogs. These labeled logs serve as the context for LLMs to better understand the\nunlabeled logs, thereby enhancing the accuracy of template generation.\nExtensive experiments on sixteen datasets demonstrate that LLMLog outperforms\nthe state-of-the-art approaches."}
{"id": "2508.09351", "pdf": "https://arxiv.org/pdf/2508.09351", "abs": "https://arxiv.org/abs/2508.09351", "authors": ["Vinicius Petrucci", "Felippe Zacarias", "David Roberts"], "title": "A Limits Study of Memory-side Tiering Telemetry", "categories": ["cs.OS", "cs.AR", "cs.PF"], "comment": null, "summary": "Increasing workload demands and emerging technologies necessitate the use of\nvarious memory and storage tiers in computing systems. This paper presents\nresults from a CXL-based Experimental Memory Request Logger that reveals\nprecise memory access patterns at runtime without interfering with the running\nworkloads. We use it for software emulation of future memory telemetry\nhardware. By combining reactive placement based on data address monitoring,\nproactive data movement, and compiler hints, a Hotness Monitoring Unit (HMU)\nwithin memory modules can greatly improve memory tiering solutions. Analysis of\npage placement using profiled access counts on a Deep Learning Recommendation\nModel (DLRM) indicates a potential 1.94x speedup over Linux NUMA balancing\ntiering, and only a 3% slowdown compared to Host-DRAM allocation while\noffloading over 90% of pages to CXL memory. The study underscores the\nlimitations of existing tiering strategies in terms of coverage and accuracy,\nand makes a strong case for programmable, device-level telemetry as a scalable\nand efficient solution for future memory systems."}
{"id": "2508.09591", "pdf": "https://arxiv.org/pdf/2508.09591", "abs": "https://arxiv.org/abs/2508.09591", "authors": ["Wenxiang Lin", "Xinglin Pan", "Lin Zhang", "Shaohuai Shi", "Xuan Wang", "Xiaowen Chu"], "title": "HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "The sparsely activated mixture-of-experts (MoE) transformer has become a\ncommon architecture for large language models (LLMs) due to its sparsity, which\nrequires fewer computational demands while easily scaling the model size. In\nMoE models, each MoE layer requires to dynamically choose tokens to activate\nparticular experts for computation while the activated experts may not be\nlocated in the same device or GPU as the token. However, this leads to\nsubstantial communication and load imbalances across all GPUs, which obstructs\nthe scalability of distributed systems within a GPU cluster. To this end, we\nintroduce HierMoE to accelerate the training of MoE models by two\ntopology-aware techniques: 1) token deduplication to reduce the communication\ntraffic, and 2) expert swap to balance the workloads among all GPUs. To enable\nthe above two proposed approaches to be more general, we build theoretical\nmodels aimed at achieving the best token duplication and expert swap strategy\nunder different model configurations and hardware environments. We implement\nour prototype HierMoE system atop Megatron-LM and conduct experiments on a\n32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results\nshow that our HierMoE achieves $1.55\\times$ to $3.32\\times$ faster\ncommunication and delivers $1.18\\times$ to $1.27\\times$ faster end-to-end\ntraining compared to state-of-the-art MoE training systems, Tutel-2DH,\nSmartMoE, and Megatron-LM."}
{"id": "2508.09680", "pdf": "https://arxiv.org/pdf/2508.09680", "abs": "https://arxiv.org/abs/2508.09680", "authors": ["Orvila Sarker", "Mona Jamshaid", "M. Ali Babar"], "title": "Inclusive Employment Pathways: Career Success Factors for Autistic Individuals in Software Engineering", "categories": ["cs.SE"], "comment": null, "summary": "Research has highlighted the valuable contributions of autistic individuals\nin the Information and Communication Technology (ICT) sector, particularly in\nareas such as software development, testing, and cybersecurity. Their strengths\nin information processing, attention to detail, innovative thinking, and\ncommitment to high-quality outcomes in the ICT domain are well-documented.\nHowever, despite their potential, autistic individuals often face barriers in\nSoftware Engineering (SE) roles due to a lack of personalised tools, complex\nwork environments, non-inclusive recruitment practices, limited co-worker\nsupport, challenging social dynamics and so on. Motivated by the ethical\nframework of the neurodiversity movement and the success of pioneering\ninitiatives like the Dandelion program, corporate Diversity, Equity, and\nInclusion (DEI) in the ICT sector has increasingly focused on autistic talent.\nThis movement fundamentally reframes challenges not as individual deficits but\nas failures of environments designed for a neurotypical majority. Despite this\nprogress, there is no synthesis of knowledge reporting the full pathway from\nsoftware engineering education through to sustainable workplace inclusion. To\naddress this, we conducted a Systematic Review of 30 studies and identified 18\nsuccess factors grouped into four thematic categories: (1) Software Engineering\nEducation, (2) Career and Employment Training, (3) Work Environment, and (4)\nTools and Assistive Technologies. Our findings offer evidence-based\nrecommendations for educational institutions, employers, organisations, and\ntool developers to enhance the inclusion of autistic individuals in SE. These\ninclude strategies for inclusive meeting and collaboration practices,\naccessible and structured work environments, clear role and responsibility\ndefinitions, and the provision of tailored workplace accommodations."}
{"id": "2508.09830", "pdf": "https://arxiv.org/pdf/2508.09830", "abs": "https://arxiv.org/abs/2508.09830", "authors": ["Shenxing Wei", "Jinxi Li", "Yafei Yang", "Siyuan Zhou", "Bo Yang"], "title": "RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "cs.RO"], "comment": "ICCV 2025 Highlight. Shenxing and Jinxi are co-first authors. Code\n  and data are available at: https://github.com/vLAR-group/RayletDF", "summary": "In this paper, we present a generalizable method for 3D surface\nreconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from\nRGB images. Unlike existing coordinate-based methods which are often\ncomputationally intensive when rendering explicit surfaces, our proposed\nmethod, named RayletDF, introduces a new technique called raylet distance\nfield, which aims to directly predict surface points from query rays. Our\npipeline consists of three key modules: a raylet feature extractor, a raylet\ndistance field predictor, and a multi-raylet blender. These components work\ntogether to extract fine-grained local geometric features, predict raylet\ndistances, and aggregate multiple predictions to reconstruct precise surface\npoints. We extensively evaluate our method on multiple public real-world\ndatasets, demonstrating superior performance in surface reconstruction from\npoint clouds or 3D Gaussians. Most notably, our method achieves exceptional\ngeneralization ability, successfully recovering 3D surfaces in a single-forward\npass across unseen datasets in testing."}
{"id": "2508.09342", "pdf": "https://arxiv.org/pdf/2508.09342", "abs": "https://arxiv.org/abs/2508.09342", "authors": ["Sam H. Ross", "Yunseo Lee", "Coco K. Lee", "Jayne Everson", "R. Benjamin Shapiro"], "title": "Affordances of Sketched Notations for Multimodal UI Design and Development Tools", "categories": ["cs.HC"], "comment": "VL/HCC 2025", "summary": "Multimodal UI design and development tools that interpret sketches or natural\nlanguage descriptions of UIs inherently have notations: the inputs they can\nunderstand. In AI-based systems, notations are implicitly defined by the data\nused to train these systems. In order to create usable and intuitive notations\nfor interactive design systems, we must regard, design, and evaluate these\ntraining datasets as notation specifications. To better understand the design\nspace of notational possibilities for future design tools, we use the Cognitive\nDimensions of Notations framework to analyze two possible notations for UI\nsketching. The first notation is the sketching rules for an existing UI sketch\ndataset, and the second notation is the set of sketches generated by\nparticipants in this study, where individuals sketched UIs without imposed\nrepresentational rules. We imagine two systems, FixedSketch and FlexiSketch,\nbuilt with each notation respectively, in order to understand the differential\naffordances of, and potential design requirements for, systems. We find that\nparticipants' sketches were composed of element-level notations that are\nambiguous in isolation but are interpretable in context within whole designs.\nFor many cognitive dimensions, the FlexiSketch notation supports greater\nintuitive creative expression and affords lower cognitive effort than the\nFixedSketch notation, but cannot be supported with prevailing, element-based\napproaches to UI sketch recognition. We argue that for future multimodal design\ntools to be truly human-centered, they must adopt contemporary AI methods,\nincluding transformer-based and human-in-the-loop, reinforcement learning\ntechniques to understand users' context-rich expressive notations and\ncorrections."}
{"id": "2508.09851", "pdf": "https://arxiv.org/pdf/2508.09851", "abs": "https://arxiv.org/abs/2508.09851", "authors": ["Adrian Rebola-Pardo"], "title": "Short proofs without interference", "categories": ["cs.LO"], "comment": null, "summary": "Interference is a phenomenon on proof systems for SAT solving that is both\ncounter-intuitive and bothersome when developing proof-logging techniques.\nHowever, all existing proof systems that can produce short proofs for all\ninprocessing techniques deployed by SAT present this feature. Based on insights\nfrom propositional dynamic logic, we propose a framework that eliminates\ninterference while preserving the same expressive power of interference-based\nproofs. Furthermore, we propose a first building blocks towards RUP-like\ndecision procedures for our dynamic logic-based frameworks, which are essential\nto developing effective proof checking methods."}
{"id": "2508.09777", "pdf": "https://arxiv.org/pdf/2508.09777", "abs": "https://arxiv.org/abs/2508.09777", "authors": ["Shima Mohammadi", "Mohsen Jenadeleh", "Michela Testolina", "Jon Sneyers", "Touradj Ebrahimi", "Dietmar Saupe", "João Ascenso"], "title": "In-place Double Stimulus Methodology for Subjective Assessment of High Quality Images", "categories": ["cs.MM"], "comment": "6 pages, 5 figures, Accepted at European Workshop on Visual\n  Information Processing", "summary": "This paper introduces a novel double stimulus subjective assessment\nmethodology for the evaluation of high quality images to address the\nlimitations of existing protocols in detecting subtle perceptual differences.\nThe In-place Double Stimulus Quality Scale (IDSQS) allows subjects to\nalternately view a reference and a distorted image at the same spatial\nlocation, facilitating a more intuitive detection of differences in quality,\nespecially at high to visually lossless quality levels. A large-scale\ncrowdsourcing study employing this methodology was conducted, generating a\ncomprehensive public dataset to evaluate perceived image quality across several\ncompression algorithms and distortion levels. An additional contribution is the\nmodeling of quality scores using a Beta distribution, allowing for the\nassessment of variability and subject consistency. Our findings demonstrate the\neffectiveness of the IDSQS methodology in achieving high correlation with more\nprecise subjective evaluation benchmarks. The dataset, subjective data, and\ngraphical user interface developed for this study are publicly available at\nhttps://github.com/shimamohammadi/IDSQS"}
{"id": "2508.09150", "pdf": "https://arxiv.org/pdf/2508.09150", "abs": "https://arxiv.org/abs/2508.09150", "authors": ["Pietro Piscione", "Leonardo Lossi", "Maziar Nekovee", "Chathura Galkandage", "Phil O Connor", "Simon Davies"], "title": "Enabling On-demand Guaranteed QoS for Real Time Video Streaming from Vehicles in 5G Advanced with CAPIF & NEF APIs", "categories": ["cs.NI"], "comment": "Published in the Proceedings of 2025 EuCNC & 6G Summit, Pozna\\'n,\n  Poland, 3-6 June 2025", "summary": "This paper presents the design and implementation of a Proof of Concept (PoC)\nthat demonstrates how 5G Advanced Network Functions can be integrated with the\nCommon API Framework (CAPIF) to support enhanced connectivity for automotive\napplications. The PoC shows the continuous monitoring of the mobile network\nperformance and the on-demand and dynamic adaptation of Quality of Service\n(QoS) for selected 5G User Equipment (UE) video streaming traffic flows using\nstandard 3GPP Network Exposure Function (NEF) APIs exposed via CAPIF. Moreover,\ntraffic flows are redirected to the edge to improve latency and optimize\nnetwork resource utilization."}
{"id": "2508.09602", "pdf": "https://arxiv.org/pdf/2508.09602", "abs": "https://arxiv.org/abs/2508.09602", "authors": ["Yaoyu Zhu", "Jintao Zhang", "Guoliang Li", "Jianhua Feng"], "title": "A Lightweight Learned Cardinality Estimation Model", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": "IEEE Transactions on Knowledge and Data Engineering (TKDE), 2025", "summary": "Cardinality estimation is a fundamental task in database management systems,\naiming to predict query results accurately without executing the queries.\nHowever, existing techniques either achieve low estimation accuracy or incur\nhigh inference latency. Simultaneously achieving high speed and accuracy\nbecomes critical for the cardinality estimation problem. In this paper, we\npropose a novel data-driven approach called CoDe (Covering with Decompositions)\nto address this problem. CoDe employs the concept of covering design, which\ndivides the table into multiple smaller, overlapping segments. For each\nsegment, CoDe utilizes tensor decomposition to accurately model its data\ndistribution. Moreover, CoDe introduces innovative algorithms to select the\nbest-fitting distributions for each query, combining them to estimate the final\nresult. By employing multiple models to approximate distributions, CoDe excels\nin effectively modeling discrete distributions and ensuring computational\nefficiency. Notably, experimental results show that our method represents a\nsignificant advancement in cardinality estimation, achieving state-of-the-art\nlevels of both estimation accuracy and inference efficiency. Across various\ndatasets, CoDe achieves absolute accuracy in estimating more than half of the\nqueries."}
{"id": "2508.09500", "pdf": "https://arxiv.org/pdf/2508.09500", "abs": "https://arxiv.org/abs/2508.09500", "authors": ["Zijun Jiang", "Yangdi Lyu"], "title": "MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI", "categories": ["cs.LG", "cs.AR"], "comment": "9 pages, 6 figures, accepted by ICCAD'25", "summary": "Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven\npromising in efficient storage and computation on edge devices. To further\nreduce the accuracy drop while increasing speedup, layer-wise mixed-precision\nquantization (MPQ) becomes a popular solution. However, existing algorithms for\nexploring MPQ schemes are limited in flexibility and efficiency. Comprehending\nthe complex impacts of different MPQ schemes on post-training quantization and\nquantization-aware training results is a challenge for conventional methods.\nFurthermore, an end-to-end framework for the optimization and deployment of MPQ\nmodels is missing in existing work.\n  In this paper, we propose the MiCo framework, a holistic MPQ exploration and\ndeployment framework for edge AI applications. The framework adopts a novel\noptimization algorithm to search for optimal quantization schemes with the\nhighest accuracies while meeting latency constraints. Hardware-aware latency\nmodels are built for different hardware targets to enable fast explorations.\nAfter the exploration, the framework enables direct deployment from PyTorch MPQ\nmodels to bare-metal C codes, leading to end-to-end speedup with minimal\naccuracy drops."}
{"id": "2508.09663", "pdf": "https://arxiv.org/pdf/2508.09663", "abs": "https://arxiv.org/abs/2508.09663", "authors": ["Philipp A. Friese", "Ahmed Eleliemy", "Utz-Uwe Haus", "Martin Schulz"], "title": "Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes", "categories": ["cs.DC", "cs.NI"], "comment": "10 pages, 12 figures, 1 table, 3 listings, to be published in IEEE\n  Cluster 2025", "summary": "Converged HPC-Cloud computing is an emerging computing paradigm that aims to\nsupport increasingly complex and multi-tenant scientific workflows. These\nsystems require reconciliation of the isolation requirements of native cloud\nworkloads and the performance demands of HPC applications. In this context,\nnetworking hardware is a critical boundary component: it is the conduit for\nhigh-throughput, low-latency communication and enables isolation across\ntenants. HPE Slingshot is a high-speed network interconnect that provides up to\n200 Gbps of throughput per port and targets high-performance computing (HPC)\nsystems. The Slingshot host software, including hardware drivers and network\nmiddleware libraries, is designed to meet HPC deployments, which predominantly\nuse single-tenant access modes. Hence, the Slingshot stack is not suited for\nsecure use in multi-tenant deployments, such as converged HPC-Cloud\ndeployments. In this paper, we design and implement an extension to the\nSlingshot stack targeting converged deployments on the basis of Kubernetes. Our\nintegration provides secure, container-granular, and multi-tenant access to\nSlingshot RDMA networking capabilities at minimal overhead."}
{"id": "2508.09791", "pdf": "https://arxiv.org/pdf/2508.09791", "abs": "https://arxiv.org/abs/2508.09791", "authors": ["Junxiao Han", "Yarong Wang", "Xiaodong Gu", "Cuiyun Gao", "Yao Wan", "Song Han", "David Lo", "Shuiguang Deng"], "title": "LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "In this paper, we propose LibRec, a novel framework that integrates the\ncapabilities of LLMs with retrieval-augmented generation(RAG) techniques to\nautomate the recommendation of alternative libraries. The framework further\nemploys in-context learning to extract migration intents from commit messages\nto enhance the accuracy of its recommendations. To evaluate the effectiveness\nof LibRec, we introduce LibEval, a benchmark designed to assess the performance\nin the library migration recommendation task. LibEval comprises 2,888 migration\nrecords associated with 2,368 libraries extracted from 2,324 Python\nrepositories. Each migration record captures source-target library pairs, along\nwith their corresponding migration intents and intent types. Based on LibEval,\nwe evaluated the effectiveness of ten popular LLMs within our framework,\nconducted an ablation study to examine the contributions of key components\nwithin our framework, explored the impact of various prompt strategies on the\nframework's performance, assessed its effectiveness across various intent\ntypes, and performed detailed failure case analyses."}
{"id": "2508.09983", "pdf": "https://arxiv.org/pdf/2508.09983", "abs": "https://arxiv.org/abs/2508.09983", "authors": ["David Dinkevich", "Matan Levy", "Omri Avrahami", "Dvir Samuel", "Dani Lischinski"], "title": "Story2Board: A Training-Free Approach for Expressive Storyboard Generation", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "Project page is available at\n  https://daviddinkevich.github.io/Story2Board/", "summary": "We present Story2Board, a training-free framework for expressive storyboard\ngeneration from natural language. Existing methods narrowly focus on subject\nidentity, overlooking key aspects of visual storytelling such as spatial\ncomposition, background evolution, and narrative pacing. To address this, we\nintroduce a lightweight consistency framework composed of two components:\nLatent Panel Anchoring, which preserves a shared character reference across\npanels, and Reciprocal Attention Value Mixing, which softly blends visual\nfeatures between token pairs with strong reciprocal attention. Together, these\nmechanisms enhance coherence without architectural changes or fine-tuning,\nenabling state-of-the-art diffusion models to generate visually diverse yet\nconsistent storyboards. To structure generation, we use an off-the-shelf\nlanguage model to convert free-form stories into grounded panel-level prompts.\nTo evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain\nnarratives designed to assess layout diversity and background-grounded\nstorytelling, in addition to consistency. We also introduce a new Scene\nDiversity metric that quantifies spatial and pose variation across storyboards.\nOur qualitative and quantitative results, as well as a user study, show that\nStory2Board produces more dynamic, coherent, and narratively engaging\nstoryboards than existing baselines."}
{"id": "2508.09358", "pdf": "https://arxiv.org/pdf/2508.09358", "abs": "https://arxiv.org/abs/2508.09358", "authors": ["Esin Mehmedova", "Santiago Berrezueta-Guzman", "Stefan Wagner"], "title": "Virtual Reality User Interface Design: Best Practices and Implementation", "categories": ["cs.HC"], "comment": "Paper submitted to ACM SIGGRAPH Motion, Interaction and Games 2025\n  (MIG 2025)", "summary": "Designing effective user interfaces (UIs) for virtual reality (VR) is\nessential to enhance user immersion, usability, comfort, and accessibility in\nvirtual environments. Despite the growing adoption of VR across domains such as\neducation, healthcare, gaming, and rehabilitation, there is a noticeable lack\nof unified and comprehensive design guidelines for VR UI design. To address\nthis gap, we conducted a systematic literature review to identify existing best\npractices and propose complete and unified guidelines for UI development in VR.\n  Building on these insights, this research proposes a set of best practices to\nguide the creation of more effective VR interfaces. To demonstrate and validate\nthese practices, we developed a VR application called \\textit{FlUId} that\nshowcases both good and bad UI design principles for direct comparison. A user\nstudy was conducted to evaluate the impact of the proposed guidelines. The\nfindings aim to bridge the gap between theory and practice, offering concrete\nrecommendations for VR designers and developers."}
{"id": "2508.09934", "pdf": "https://arxiv.org/pdf/2508.09934", "abs": "https://arxiv.org/abs/2508.09934", "authors": ["Arijit Shaw", "Uddalok Sarkar", "Kuldeep S. Meel"], "title": "Efficient Volume Computation for SMT Formulas", "categories": ["cs.LO"], "comment": "Full version of conference paper accepted to KR 2023", "summary": "Satisfiability Modulo Theory (SMT) has recently emerged as a powerful tool\nfor solving various automated reasoning problems across diverse domains. Unlike\ntraditional satisfiability methods confined to Boolean variables, SMT can\nreason on real-life variables like bitvectors, integers, and reals. A natural\nextension in this context is to ask quantitative questions. One such query in\nthe SMT theory of Linear Real Arithmetic (LRA) is computing the volume of the\nentire satisfiable region defined by SMT formulas. This problem is important in\nsolving different quantitative verification queries in software verification,\ncyber-physical systems, and neural networks, to mention a few.\n  We introduce ttc, an efficient algorithm that extends the capabilities of SMT\nsolvers to volume computation. Our method decomposes the solution space of SMT\nLinear Real Arithmetic formulas into a union of overlapping convex polytopes,\nthen computes their volumes and calculates their union. Our algorithm builds on\nrecent developments in streaming-mode set unions, volume computation\nalgorithms, and AllSAT techniques. Experimental evaluations demonstrate\nsignificant performance improvements over existing state-of-the-art approaches."}
{"id": "2508.09404", "pdf": "https://arxiv.org/pdf/2508.09404", "abs": "https://arxiv.org/abs/2508.09404", "authors": ["Guangxun Zhu", "Shiyu Fan", "Hang Dai", "Edmond S. L. Ho"], "title": "Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving", "categories": ["cs.CV", "cs.MM"], "comment": "ACM Multimedia 2025 (Dataset Track) Paper", "summary": "Large-scale high-quality 3D motion datasets with multi-person interactions\nare crucial for data-driven models in autonomous driving to achieve\nfine-grained pedestrian interaction understanding in dynamic urban\nenvironments. However, existing datasets mostly rely on estimating 3D poses\nfrom monocular RGB video frames, which suffer from occlusion and lack of\ntemporal continuity, thus resulting in unrealistic and low-quality human\nmotion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale\ndataset providing high-quality, temporally coherent 3D skeletal motions with\nexplicit interaction semantics, derived from the Waymo Perception dataset. Our\nkey insight is to utilize 3D human body shape and motion priors to enhance the\nquality of the 3D pose sequences extracted from the raw LiDRA point clouds. The\ndataset covers over 14,000 seconds across more than 800 real driving scenarios,\nincluding rich interactions among an average of 27 agents per scene (with up to\n250 agents in the largest scene). Furthermore, we establish 3D pose forecasting\nbenchmarks under varying pedestrian densities, and the results demonstrate its\nvalue as a foundational resource for future research on fine-grained human\nbehavior understanding in complex urban environments. The dataset and code will\nbe available at https://github.com/GuangxunZhu/Waymo-3DSkelMo"}
{"id": "2508.09151", "pdf": "https://arxiv.org/pdf/2508.09151", "abs": "https://arxiv.org/abs/2508.09151", "authors": ["Chang Wu", "Yuang Chen", "Yiyuan Chen", "Fengqian Guo", "Xiaowei Qin", "Hancheng Lu"], "title": "Physiological Signal-Driven QoE Optimization for Wireless Virtual Reality Transmission", "categories": ["cs.NI", "cs.MA"], "comment": "7 pages, 6 figures", "summary": "Abrupt resolution changes in virtual reality (VR) streaming can significantly\nimpair the quality-of-experience (QoE) of users, particularly during\ntransitions from high to low resolutions. Existing QoE models and transmission\nschemes inadequately address the perceptual impact of these shifts. To bridge\nthis gap, this article proposes, for the first time, an innovative\nphysiological signal-driven QoE modeling and optimization framework that fully\nleverages users' electroencephalogram (EEG), electrocardiogram (ECG), and skin\nactivity signals. This framework precisely captures the temporal dynamics of\nphysiological responses and resolution changes in VR streaming, enabling\naccurate quantification of resolution upgrades' benefits and downgrades'\nimpacts. Integrated the proposed QoE framework into the radio access network\n(RAN) via a deep reinforcement learning (DRL) framework, adaptive transmission\nstrategies have been implemented to allocate radio resources dynamically, which\nmitigates short-term channel fluctuations and adjusts frame resolution in\nresponse to channel variations caused by user mobility. By prioritizing\nlong-term resolution while minimizing abrupt transitions, the proposed solution\nachieves an 88.7\\% improvement in resolution and an 81.0\\% reduction in\nhandover over the baseline. Experimental results demonstrate the effectiveness\nof this physiological signal-driven strategy, underscoring the promise of edge\nAI in immersive media services."}
{"id": "2508.09631", "pdf": "https://arxiv.org/pdf/2508.09631", "abs": "https://arxiv.org/abs/2508.09631", "authors": ["Yuchen Tian", "Kaixin Li", "Hao Chen", "Ziyang Luo", "Hongzhan Lin", "Sebastian Schelter", "Lun Du", "Jing Ma"], "title": "AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong capabilities\nin translating natural language into database queries, especially when dealing\nwith complex graph-structured data. However, real-world queries often contain\ninherent ambiguities, and the interconnected nature of graph structures can\namplify these challenges, leading to unintended or incorrect query results. To\nsystematically evaluate LLMs on this front, we propose a taxonomy of\ngraph-query ambiguities, comprising three primary types: Attribute Ambiguity,\nRelationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided\ninto Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a\nnovel benchmark of real-world ambiguous queries paired with expert-verified\ngraph query answers. Evaluating 9 representative LLMs shows that even top\nmodels struggle with ambiguous graph queries. Our findings reveal a critical\ngap in ambiguity handling and motivate future work on specialized resolution\ntechniques."}
{"id": "2508.09546", "pdf": "https://arxiv.org/pdf/2508.09546", "abs": "https://arxiv.org/abs/2508.09546", "authors": ["Dumitra Iancu", "Liang Liu", "Ove Edfors", "Erik Leitinger", "Xuhong Li"], "title": "Low-latency D-MIMO Localization using Distributed Scalable Message-Passing Algorithm", "categories": ["eess.SP", "cs.AR"], "comment": "This work has been submitted to the IEEE for possible publication,\n  copyright information may be affected upon publication", "summary": "Distributed MIMO and integrated sensing and communication are expected to be\nkey technologies in future wireless systems, enabling reliable, low-latency\ncommunication and accurate localization. Dedicated localization solutions must\nsupport distributed architecture, provide scalability across different system\nconfigurations and meet strict latency requirements. We present a scalable\nmessage-passing localization method and architecture co-designed for a\npanel-based distributed MIMO system and network topology, in which\ninterconnected units operate without centralized processing. This method\njointly detects line-of-sight paths to distributed units from multipath\nmeasurements in dynamic scenarios, localizes the agent, and achieves very low\nlatency. Additionally, we introduce a cycle-accurate system latency model based\non implemented FPGA operations, and show important insights into processing\nlatency and hardware utilization and system-level trade-offs. We compare our\nmethod to a multipath-based localization method and show that it can achieve\nsimilar localization performance, with wide enough distribution of array\nelements, while offering lower latency and computational complexity."}
{"id": "2508.09147", "pdf": "https://arxiv.org/pdf/2508.09147", "abs": "https://arxiv.org/abs/2508.09147", "authors": ["Alaa Saleh", "Roberto Morabito", "Sasu Tarkoma", "Anders Lindgren", "Susanna Pirttikangas", "Lauri Lovén"], "title": "Agentic TinyML for Intent-aware Handover in 6G Wireless Networks", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.LG", "cs.MA"], "comment": null, "summary": "As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,\ntraditional reactive handover mechanisms demonstrate limitations, especially in\nmobile edge computing and autonomous agent-based service scenarios. This\nmanuscript introduces WAAN, a cross-layer framework that enables intent-aware\nand proactive handovers by embedding lightweight TinyML agents as autonomous,\nnegotiation-capable entities across heterogeneous edge nodes that contribute to\nintent propagation and network adaptation. To ensure continuity across\nmobility-induced disruptions, WAAN incorporates semi-stable rendezvous points\nthat serve as coordination anchors for context transfer and state preservation.\nThe framework's operational capabilities are demonstrated through a multimodal\nenvironmental control case study, highlighting its effectiveness in maintaining\nuser experience under mobility. Finally, the article discusses key challenges\nand future opportunities associated with the deployment and evolution of WAAN."}
{"id": "2508.09828", "pdf": "https://arxiv.org/pdf/2508.09828", "abs": "https://arxiv.org/abs/2508.09828", "authors": ["Sebastiano Antonio Piccolo"], "title": "Fast and Accurate Heuristics for Bus-Factor Estimation", "categories": ["cs.SE"], "comment": null, "summary": "The bus-factor is a critical risk indicator that quantifies how many key\ncontributors a project can afford to lose before core knowledge or\nfunctionality is compromised. Despite its practical importance, accurately\ncomputing the bus-factor is NP-Hard under established formalizations, making\nscalable analysis infeasible for large software systems.\n  In this paper, we model software projects as bipartite graphs of developers\nand tasks and propose two novel approximation heuristics, Minimum Coverage and\nMaximum Coverage, based on iterative graph peeling, for two influential\nbus-factor formalizations. Our methods significantly outperform the widely\nadopted degree-based heuristic, which we show can yield severely inflated\nestimates.\n  We conduct a comprehensive empirical evaluation on over $1\\,000$ synthetic\npower-law graphs and demonstrate that our heuristics provide tighter estimates\nwhile scaling to graphs with millions of nodes and edges in minutes. Our\nresults reveal that the proposed heuristics are not only more accurate but also\nrobust to structural variations in developer-task assignment graph. We release\nour implementation as open-source software to support future research and\npractical adoption."}
{"id": "2508.09386", "pdf": "https://arxiv.org/pdf/2508.09386", "abs": "https://arxiv.org/abs/2508.09386", "authors": ["Jürgen Bernard", "Mara Solen", "Helen Novak Lauscher", "Kurtis Stewart", "Kendall Ho", "Tamara Munzner"], "title": "VIVA: Virtual Healthcare Interactions Using Visual Analytics, With Controllability Through Configuration", "categories": ["cs.HC"], "comment": null, "summary": "At the beginning of the COVID-19 pandemic, HealthLink BC (HLBC) rapidly\nintegrated physicians into the triage process of their virtual healthcare\nservice to improve patient outcomes and satisfaction with this service and\npreserve health care system capacity. We present the design and implementation\nof a visual analytics tool, VIVA (Virtual healthcare Interactions using Visual\nAnalytics), to support HLBC in analysing various forms of usage data from the\nservice. We abstract HLBC's data and data analysis tasks, which we use to\ninform our design of VIVA. We also present the interactive workflow abstraction\nof Scan, Act, Adapt. We validate VIVA's design through three case studies with\nstakeholder domain experts. We also propose the Controllability Through\nConfiguration model to conduct and analyze design studies, and discuss\narchitectural evolution of VIVA through that lens. It articulates\nconfiguration, both that specified by a developer or technical power user and\nthat constructed automatically through log data from previous interactive\nsessions, as a bridge between the rigidity of hardwired programming and the\ntime-consuming implementation of full end-user interactivity.\n  Availability: Supplemental materials at https://osf.io/wv38n"}
{"id": "2508.09277", "pdf": "https://arxiv.org/pdf/2508.09277", "abs": "https://arxiv.org/abs/2508.09277", "authors": ["Soumia Mehimeh"], "title": "Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Value function initialization (VFI) is an effective way to achieve a\njumpstart in reinforcement learning (RL) by leveraging value estimates from\nprior tasks. While this approach is well established in tabular settings,\nextending it to deep reinforcement learning (DRL) poses challenges due to the\ncontinuous nature of the state-action space, the noisy approximations of neural\nnetworks, and the impracticality of storing all past models for reuse. In this\nwork, we address these challenges and introduce DQInit, a method that adapts\nvalue function initialization to DRL. DQInit reuses compact tabular Q-values\nextracted from previously solved tasks as a transferable knowledge base. It\nemploys a knownness-based mechanism to softly integrate these transferred\nvalues into underexplored regions and gradually shift toward the agent's\nlearned estimates, avoiding the limitations of fixed time decay. Our approach\noffers a novel perspective on knowledge transfer in DRL by relying solely on\nvalue estimates rather than policies or demonstrations, effectively combining\nthe strengths of jumpstart RL and policy distillation while mitigating their\ndrawbacks. Experiments across multiple continuous control tasks demonstrate\nthat DQInit consistently improves early learning efficiency, stability, and\noverall performance compared to standard initialization and existing transfer\ntechniques."}
{"id": "2508.09486", "pdf": "https://arxiv.org/pdf/2508.09486", "abs": "https://arxiv.org/abs/2508.09486", "authors": ["Yun Wang", "Long Zhang", "Jingren Liu", "Jiaqi Yan", "Zhanjie Zhang", "Jiahao Zheng", "Xun Yang", "Dapeng Wu", "Xiangyu Chen", "Xuelong Li"], "title": "Episodic Memory Representation for Long-form Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "10 pages, 5 figures", "summary": "Video Large Language Models (Video-LLMs) excel at general video understanding\nbut struggle with long-form videos due to context window limits. Consequently,\nrecent approaches focus on keyframe retrieval, condensing lengthy videos into a\nsmall set of informative frames. Despite their practicality, these methods\nsimplify the problem to static text image matching, overlooking spatio temporal\nrelationships crucial for capturing scene transitions and contextual\ncontinuity, and may yield redundant keyframes with limited information,\ndiluting salient cues essential for accurate video question answering. To\naddress these limitations, we introduce Video-EM, a training free framework\ninspired by the principles of human episodic memory, designed to facilitate\nrobust and contextually grounded reasoning. Rather than treating keyframes as\nisolated visual entities, Video-EM explicitly models them as temporally ordered\nepisodic events, capturing both spatial relationships and temporal dynamics\nnecessary for accurately reconstructing the underlying narrative. Furthermore,\nthe framework leverages chain of thought (CoT) thinking with LLMs to\niteratively identify a minimal yet highly informative subset of episodic\nmemories, enabling efficient and accurate question answering by Video-LLMs.\nExtensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench\nbenchmarks confirm the superiority of Video-EM, which achieves highly\ncompetitive results with performance gains of 4-9 percent over respective\nbaselines while utilizing fewer frames."}
{"id": "2508.09152", "pdf": "https://arxiv.org/pdf/2508.09152", "abs": "https://arxiv.org/abs/2508.09152", "authors": ["Joseph H. R. Isaac", "Harish Saradagam", "Nallamothu Pardhasaradhi"], "title": "5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "8 pages, 3 figures and 2 tables. Accepted in Conference on Advances\n  in Communication Networks & Systems (CoaCoNS 2025)", "summary": "With the advent of 5G networks and technologies, ensuring the integrity and\nperformance of packet core traffic is paramount. During network analysis, test\nfiles such as Packet Capture (PCAP) files and log files will contain errors if\npresent in the system that must be resolved for better overall network\nperformance, such as connectivity strength and handover quality. Current\nmethods require numerous person-hours to sort out testing results and find the\nfaults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine\ndesigned to classify successful and faulty frames in PCAP files, specifically\nwithin the 5G packet core. The FA engine analyses network traffic using natural\nlanguage processing techniques to identify anomalies and inefficiencies,\nsignificantly reducing the effort time required and increasing efficiency. The\nFA Engine also suggests steps to fix the issue using Generative AI via a Large\nLanguage Model (LLM) trained on several 5G packet core documents. The engine\nexplains the details of the error from the domain perspective using documents\nsuch as the 3GPP standards and user documents regarding the internal conditions\nof the tests. Test results on the ML models show high classification accuracy\non the test dataset when trained with 80-20 splits for the successful and\nfailed PCAP files. Future scopes include extending the AI engine to incorporate\n4G network traffic and other forms of network data, such as log text files and\nmultimodal systems."}
{"id": "2508.09160", "pdf": "https://arxiv.org/pdf/2508.09160", "abs": "https://arxiv.org/abs/2508.09160", "authors": ["Beyza Cinar", "Maria Maleshkova"], "title": "Presenting DiaData for Research on Type 1 Diabetes", "categories": ["cs.LG", "cs.DB", "q-bio.QM"], "comment": "11 pages, 7 figures, 3 tables", "summary": "Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction\nof insulin-producing cells, resulting in insulin deficiency, as to why the\naffected individuals depend on external insulin injections. However, insulin\ncan decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a\nsevere event of low blood glucose levels ($\\le$70 mg/dL) with dangerous side\neffects of dizziness, coma, or death. Data analysis can significantly enhance\ndiabetes care by identifying personal patterns and trends leading to adverse\nevents. Especially, machine learning (ML) models can predict glucose levels and\nprovide early alarms. However, diabetes and hypoglycemia research is limited by\nthe unavailability of large datasets. Thus, this work systematically integrates\n15 datasets to provide a large database of 2510 subjects with glucose\nmeasurements recorded every 5 minutes. In total, 149 million measurements are\nincluded, of which 4% represent values in the hypoglycemic range. Moreover, two\nsub-databases are extracted. Sub-database I includes demographics, and\nsub-database II includes heart rate data. The integrated dataset provides an\nequal distribution of sex and different age levels. As a further contribution,\ndata quality is assessed, revealing that data imbalance and missing values\npresent a significant challenge. Moreover, a correlation study on glucose\nlevels and heart rate data is conducted, showing a relation between 15 and 55\nminutes before hypoglycemia."}
{"id": "2508.09149", "pdf": "https://arxiv.org/pdf/2508.09149", "abs": "https://arxiv.org/abs/2508.09149", "authors": ["Seyed Hossein Ahmadpanah"], "title": "Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "Next-generation automotive applications require vehicular edge computing\n(VEC), but current management systems are essentially fixed and reactive. They\nare suboptimal in extremely dynamic vehicular environments because they are\nconstrained to static optimization objectives and base their decisions on the\ncurrent network states. This paper presents a novel Semantic-Aware Proactive\nLLM Orchestration (SP-LLM) framework to address these issues. Our method\ntransforms the traditional Digital Twin (DT) into a Predictive Digital Twin\n(pDT) that predicts important network parameters such as task arrivals, vehicle\nmobility, and channel quality. A Large Language Model (LLM) that serves as a\ncognitive orchestrator is at the heart of our framework. It makes proactive,\nforward-looking decisions about task offloading and resource allocation by\nutilizing the pDT's forecasts. The LLM's ability to decipher high-level\nsemantic commands given in natural language is crucial because it enables it to\ndynamically modify its optimization policy to match evolving strategic\nobjectives, like giving emergency services priority or optimizing energy\nefficiency. We show through extensive simulations that SP-LLM performs\nsignificantly better in terms of scalability, robustness in volatile\nconditions, and adaptability than state-of-the-art reactive and MARL-based\napproaches. More intelligent, autonomous, and goal-driven vehicular networks\nwill be possible due to our framework's outstanding capacity to convert human\nintent into optimal network behavior."}
{"id": "2508.09832", "pdf": "https://arxiv.org/pdf/2508.09832", "abs": "https://arxiv.org/abs/2508.09832", "authors": ["Linh Nguyen", "Chunhua Liu", "Hong Yi Lin", "Patanamon Thongtanunam"], "title": "Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted at 2025 IEEE International Conference on Source Code\n  Analysis & Manipulation (SCAM)", "summary": "Code review is a crucial practice in software development. As code review\nnowadays is lightweight, various issues can be identified, and sometimes, they\ncan be trivial. Research has investigated automated approaches to classify\nreview comments to gauge the effectiveness of code reviews. However, previous\nstudies have primarily relied on supervised machine learning, which requires\nextensive manual annotation to train the models effectively. To address this\nlimitation, we explore the potential of using Large Language Models (LLMs) to\nclassify code review comments. We assess the performance of LLMs to classify 17\ncategories of code review comments. Our results show that LLMs can classify\ncode review comments, outperforming the state-of-the-art approach using a\ntrained deep learning model. In particular, LLMs achieve better accuracy in\nclassifying the five most useful categories, which the state-of-the-art\napproach struggles with due to low training examples. Rather than relying\nsolely on a specific small training data distribution, our results show that\nLLMs provide balanced performance across high- and low-frequency categories.\nThese results suggest that the LLMs could offer a scalable solution for code\nreview analytics to improve the effectiveness of the code review process."}
{"id": "2508.09402", "pdf": "https://arxiv.org/pdf/2508.09402", "abs": "https://arxiv.org/abs/2508.09402", "authors": ["Von Ralph Dane Marquez Herbuela", "Yukie Nagai"], "title": "Realtime Multimodal Emotion Estimation using Behavioral and Neurophysiological Data", "categories": ["cs.HC"], "comment": null, "summary": "Many individuals especially those with autism spectrum disorder (ASD),\nalexithymia, or other neurodivergent profiles face challenges in recognizing,\nexpressing, or interpreting emotions. To support more inclusive and\npersonalized emotion technologies, we present a real-time multimodal emotion\nestimation system that combines neurophysiological EEG, ECG, blood volume pulse\n(BVP), and galvanic skin response (GSR/EDA) and behavioral modalities (facial\nexpressions, and speech) in a unified arousal-valence 2D interface to track\nmoment-to-moment emotional states. This architecture enables interpretable,\nuser-specific analysis and supports applications in emotion education,\nneuroadaptive feedback, and interaction support for neurodiverse users. Two\ndemonstration scenarios illustrate its application: (1) passive media viewing\n(2D or VR videos) reveals cortical and autonomic responses to affective\ncontent, and (2) semi-scripted conversations with a facilitator or virtual\nagent capture real-time facial and vocal expressions. These tasks enable\ncontrolled and naturalistic emotion monitoring, making the system well-suited\nfor personalized feedback and neurodiversity-informed interaction design."}
{"id": "2508.09784", "pdf": "https://arxiv.org/pdf/2508.09784", "abs": "https://arxiv.org/abs/2508.09784", "authors": ["Avijeet Ghosh", "Sujata Ghosh", "François Schwarzentruber"], "title": "Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete", "categories": ["cs.AI", "cs.CC", "cs.LO"], "comment": "Accepted in KR 25", "summary": "Logics for reasoning about knowledge and actions have seen many applications\nin various domains of multi-agent systems, including epistemic planning. Change\nof knowledge based on observations about the surroundings forms a key aspect in\nsuch planning scenarios. Public Observation Logic (POL) is a variant of public\nannouncement logic for reasoning about knowledge that gets updated based on\npublic observations. Each state in an epistemic (Kripke) model is equipped with\na set of expected observations. These states evolve as the expectations get\nmatched with the actual observations. In this work, we prove that the\nsatisfiability problem of $\\POL$ is 2EXPTIME-complete."}
{"id": "2508.09159", "pdf": "https://arxiv.org/pdf/2508.09159", "abs": "https://arxiv.org/abs/2508.09159", "authors": ["Ilias Chatzistefanidis", "Navid Nikaein", "Andrea Leone", "Ali Maatouk", "Leandros Tassioulas", "Roberto Morabito", "Ioannis Pitsiorlas", "Marios Kountouris"], "title": "Agoran: An Agentic Open Marketplace for 6G RAN Automation", "categories": ["cs.NI", "cs.AI"], "comment": "Pre-print submitted to Computer Networks AI-for-6G", "summary": "Next-generation mobile networks must reconcile the often-conflicting goals of\nmultiple service owners. However, today's network slice controllers remain\nrigid, policy-bound, and unaware of the business context. We introduce Agoran\nService and Resource Broker (SRB), an agentic marketplace that brings\nstakeholders directly into the operational loop. Inspired by the ancient Greek\nagora, Agoran distributes authority across three autonomous AI branches: a\nLegislative branch that answers compliance queries using retrieval-augmented\nLarge Language Models (LLMs); an Executive branch that maintains real-time\nsituational awareness through a watcher-updated vector database; and a Judicial\nbranch that evaluates each agent message with a rule-based Trust Score, while\narbitrating LLMs detect malicious behavior and apply real-time incentives to\nrestore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator\nAgent negotiate feasible, Pareto-optimal offers produced by a multi-objective\noptimizer, reaching a consensus intent in a single round, which is then\ndeployed to Open and AI RAN controllers. Deployed on a private 5G testbed and\nevaluated with realistic traces of vehicle mobility, Agoran achieved\nsignificant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73%\nreduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3%\nsaving in PRB usage compared to a static baseline. An 1B-parameter Llama model,\nfine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80%\nof GPT-4.1's decision quality, while operating within 6 GiB of memory and\nconverging in only 1.3 seconds. These results establish Agoran as a concrete,\nstandards-aligned path toward ultra-flexible, stakeholder-centric 6G networks.\nA live demo is presented\nhttps://www.youtube.com/watch?v=h7vEyMu2f5w\\&ab_channel=BubbleRAN."}
{"id": "2508.09232", "pdf": "https://arxiv.org/pdf/2508.09232", "abs": "https://arxiv.org/abs/2508.09232", "authors": ["Nick Oh", "Giorgos D. Vrakas", "Siân J. M. Brooke", "Sasha Morinière", "Toju Duke"], "title": "PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research", "categories": ["cs.MM", "cs.AI", "cs.DB"], "comment": null, "summary": "Social media data presents AI researchers with overlapping obligations under\nthe GDPR, copyright law, and platform terms -- yet existing frameworks fail to\nintegrate these regulatory domains, leaving researchers without unified\nguidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and\nPresent), a compliance framework that embeds legal safeguards directly into\nextended ETL pipelines. Central to PETLP is treating Data Protection Impact\nAssessments as living documents that evolve from pre-registration through\ndissemination. Through systematic Reddit analysis, we demonstrate how\nextraction rights fundamentally differ between qualifying research\norganisations (who can invoke DSM Article 3 to override platform restrictions)\nand commercial entities (bound by terms of service), whilst GDPR obligations\napply universally. We reveal why true anonymisation remains unachievable for\nsocial media data and expose the legal gap between permitted dataset creation\nand uncertain model distribution. By structuring compliance decisions into\npractical workflows and simplifying institutional data management plans, PETLP\nenables researchers to navigate regulatory complexity with confidence, bridging\nthe gap between legal requirements and research practice."}
{"id": "2508.09229", "pdf": "https://arxiv.org/pdf/2508.09229", "abs": "https://arxiv.org/abs/2508.09229", "authors": ["Danil Sivtsov", "Aleksandr Katrutsa", "Ivan Oseledets"], "title": "Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference", "categories": ["cs.NI", "cs.AI", "cs.DC"], "comment": null, "summary": "Efficient deployment of a pre-trained LLM to a cluster with multiple servers\nis a critical step for providing fast responses to users' queries. The recent\nsuccess of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy\nthem efficiently, considering their underlying structure. During the inference\nin MoE LLMs, only a small part of the experts is selected to process a given\ntoken. Moreover, in practice, the experts' load is highly imbalanced. For\nefficient deployment, one has to distribute the model across a large number of\nservers using a model placement algorithm. Thus, to improve cluster\nutilization, the model placement algorithm has to take into account the network\ntopology. This work focuses on the efficient topology-aware placement of the\npre-trained MoE LLMs in the inference stage. We propose an integer linear\nprogram (ILP) that determines the optimal placement of experts, minimizing the\nexpected number of transmissions. Due to the internal structure, this\noptimization problem can be solved with a standard ILP solver. We demonstrate\nthat ILP-based placement strategy yields lower network traffic than competitors\nfor small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models."}
{"id": "2508.09875", "pdf": "https://arxiv.org/pdf/2508.09875", "abs": "https://arxiv.org/abs/2508.09875", "authors": ["Jinbao Chen", "Boyao Ding", "Yu Zhang", "Qingwei Li", "Fugen Tang"], "title": "An Empirical Study of CGO Usage in Go Projects -- Distribution, Purposes, Patterns and Critical Issues", "categories": ["cs.SE"], "comment": "Accepted for publication in The Journal of Systems and Software", "summary": "Multilingual software development integrates multiple languages into a single\napplication, with the Foreign Function Interface (FFI) enabling seamless\ninteraction. While FFI boosts efficiency and extensibility, it also introduces\nrisks. Existing studies focus on FFIs in languages like Python and Java,\nneglecting CGO, the emerging FFI in Go, which poses unique risks.\n  To address these concerns, we conduct an empirical study of CGO usage across\n920 open-source Go projects. Our study aims to reveal the distribution,\npatterns, purposes, and critical issues associated with CGO, offering insights\nfor developers and the Go team. We develop CGOAnalyzer, a tool to efficiently\nidentify and quantify CGO-related features. Our findings reveal that: (1) 11.3%\nof analyzed Go projects utilize CGO, with usage concentrated in a subset of\nprojects; (2) CGO serves 4 primary purposes, including system-level\ninteractions and performance optimizations, with 15 distinct usage patterns\nobserved; (3) 19 types of CGO-related issues exist, including one critical\nissue involving unnecessary pointer checks that pose risks of runtime crashes\ndue to limitations in the current Go compilation toolchain; (4) a temporary\nsolution reduces unnecessary pointer checks, mitigating crash risks, and (5) we\nsubmitted a proposal to improve the Go toolchain for a permanent fix, which has\nbeen grouped within an accepted proposal for future resolution. Our findings\nprovide valuable insights for developers and the Go team, enhancing development\nefficiency and reliability while improving the robustness of the Go toolchain."}
{"id": "2508.09438", "pdf": "https://arxiv.org/pdf/2508.09438", "abs": "https://arxiv.org/abs/2508.09438", "authors": ["EunJeong Cheon", "Ingrid Erickson"], "title": "Fulfillment of the Work Games: Warehouse Workers' Experiences with Algorithmic Management", "categories": ["cs.HC"], "comment": null, "summary": "The introduction of algorithms into a large number of industries has already\nrestructured the landscape of work and threatens to continue. While a growing\nbody of CSCW research centered on the future of work has begun to document\nthese shifts, relatively little is known about workers' experiences beyond\nthose of platform-mediated gig workers. In this paper, we turn to a traditional\nwork sector, Amazon fulfillment centers (FC), to deepen our field's empirical\nexamination of algorithmic management. Drawing on two years of ethnographic\nresearch, we show how FC workers react to managers' interventions, imposed\nproductivity rates, and quantified objectification when subjected to\nlabor-tracking systems in their physical work environments. Situating FC\nworkers' resistance to algorithmic systems and metrics within the current CSCW\nliterature allows us to explicate and link the nuanced practices of FC workers\nto the larger discourse of algorithmic control mechanisms. In addition, we show\nhow FC workers' resistance practices are emblematic of 'work games'--a\nlong-studied means by which workers agentically configure (\"trick\") their\nengagement within work systems. We argue that gaining a more nuanced\nunderstanding of workers' resistance and consent in relation to algorithmic\nmanagement expands our ability to critique and potentially disassemble the\neconomic and political forces at the root of these sociotechnical labor\nsystems."}
{"id": "2508.09166", "pdf": "https://arxiv.org/pdf/2508.09166", "abs": "https://arxiv.org/abs/2508.09166", "authors": ["Wei Guo", "Shunsei Yamagishi", "Lei Jing"], "title": "WPTrack: A Wi-Fi and Pressure Insole Fusion System for Single Target Tracking", "categories": ["cs.NI", "cs.HC"], "comment": "6 pages, 12 figures, conference", "summary": "As the Internet of Things (IoT) continues to evolve, indoor location has\nbecome a critical element for enabling smart homes, behavioral monitoring, and\nelderly care. Existing WiFi-based human tracking solutions typically require\nspecialized equipment or multiple Wi-Fi links, a limitation in most indoor\nsettings where only a single pair of Wi-Fi devices is usually available.\nHowever, despite efforts to implement human tracking using one Wi-Fi link,\nsignificant challenges remain, such as difficulties in acquiring initial\npositions and blind spots in DFS estimation of tangent direction. To address\nthese challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure\nInsoles Fusion System for Single Target Tracking. WPTrack collects Channel\nState Information (CSI) from a single Wi-Fi link and pressure data from 90\ninsole sensors. The phase difference and Doppler velocity are computed from the\nCSI, while the pressure sensor data is used to calculate walking velocity.\nThen, we propose the CSI-pressure fusion model, integrating CSI and pressure\ndata to accurately determine initial positions and facilitate precise human\ntracking. The simulation results show that the initial position localization\naccuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results\nobtained from experimental data collected in a real-world environment closely\nalign with the actual trajectory."}
{"id": "2508.09403", "pdf": "https://arxiv.org/pdf/2508.09403", "abs": "https://arxiv.org/abs/2508.09403", "authors": ["Ting Cai", "Stephen Sheen", "AnHai Doan"], "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Expanding the abbreviated column names of tables, such as ``esal'' to\n``employee salary'', is critical for numerous downstream data tasks. This\nproblem arises in enterprises, domain sciences, government agencies, and more.\nIn this paper we make three contributions that significantly advances the state\nof the art. First, we show that synthetic public data used by prior work has\nmajor limitations, and we introduce 4 new datasets in enterprise/science\ndomains, with real-world abbreviations. Second, we show that accuracy measures\nused by prior work seriously undercount correct expansions, and we propose new\nsynonym-aware measures that capture accuracy much more accurately. Finally, we\ndevelop Columbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29\\%, over 5 datasets. Columbo has been used in\nproduction on EDI, a major data portal for environmental sciences."}
{"id": "2508.09638", "pdf": "https://arxiv.org/pdf/2508.09638", "abs": "https://arxiv.org/abs/2508.09638", "authors": ["Irina Kostitsyna", "David Liedtke", "Christian Scheideler"], "title": "Distributed Diamond Formation of Sliding Squares", "categories": ["cs.CG", "cs.DC", "F.2.2"], "comment": "29 pages, 11 figures, 34 subfigures", "summary": "The sliding square model is a widely used abstraction for studying\nself-reconfigurable robotic systems, where modules are square-shaped robots\nthat move by sliding or rotating over one another. In this paper, we propose a\nnovel distributed algorithm that allows a group of modules to reconfigure into\na diamond shape, starting from an arbitrary side-connected configuration. It is\nconnectivity-preserving and operates under minimal assumptions: one leader\nmodule, common chirality, constant memory per module, and visibility and\ncommunication restricted to immediate neighbors. Unlike prior work, which\nrelaxes the original sliding square move-set, our approach uses the unmodified\nmove-set, addressing the additional challenge of handling locked\nconfigurations. Our algorithm is sequential in nature and operates with a\nworst-case time complexity of $\\mathcal{O}(n^2)$ rounds, which is optimal for\nsequential algorithms. To improve runtime, we introduce two parallel variants\nof the algorithm. Both rely on a spanning tree data structure, allowing modules\nto make decisions based on local connectivity. Our experimental results show a\nsignificant speedup for the first variant, and linear average runtime for the\nsecond variant, which is worst-case optimal for parallel algorithms."}
{"id": "2508.09219", "pdf": "https://arxiv.org/pdf/2508.09219", "abs": "https://arxiv.org/abs/2508.09219", "authors": ["Wilder Baldwin", "Sepideh Ghanavati", "Manuel Woersdoerfer"], "title": "Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SE"], "comment": "Under Review", "summary": "Recent advances in AI applications have raised growing concerns about the\nneed for ethical guidelines and regulations to mitigate the risks posed by\nthese technologies. In this paper, we present a mixed-method survey study -\ncombining statistical and qualitative analyses - to examine the ethical\nperceptions, practices, and knowledge of individuals involved in various AI\ndevelopment roles. Our survey includes 414 participants from 43 countries,\nrepresenting roles such as AI managers, analysts, developers, quality assurance\nprofessionals, and information security and privacy experts. The results reveal\nvarying degrees of familiarity and experience with AI ethics principles,\ngovernment initiatives, and risk mitigation strategies across roles, regions,\nand other demographic factors. Our findings highlight the importance of a\ncollaborative, role-sensitive approach, involving diverse stakeholders in\nethical decision-making throughout the AI development lifecycle. We advocate\nfor developing tailored, inclusive solutions to address ethical challenges in\nAI development, and we propose future research directions and educational\nstrategies to promote ethics-aware AI practices."}
{"id": "2508.09458", "pdf": "https://arxiv.org/pdf/2508.09458", "abs": "https://arxiv.org/abs/2508.09458", "authors": ["Xi Long", "Christy Boscardin", "Lauren A. Maggio", "Joseph A. Costello", "Ralph Gonzales", "Rasmyah Hammoudeh", "Ki Lai", "Yoon Soo Park", "Brian C. Gin"], "title": "Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Knowledge syntheses (literature reviews) are essential to health professions\neducation (HPE), consolidating findings to advance theory and practice.\nHowever, they are labor-intensive, especially during data extraction.\nArtificial Intelligence (AI)-assisted extraction promises efficiency but raises\nconcerns about accuracy, making it critical to distinguish AI 'hallucinations'\n(fabricated content) from legitimate interpretive differences. We developed an\nextraction platform using large language models (LLMs) to automate data\nextraction and compared AI to human responses across 187 publications and 17\nextraction questions from a published scoping review. AI-human, human-human,\nand AI-AI consistencies were measured using interrater reliability\n(categorical) and thematic similarity ratings (open-ended). Errors were\nidentified by comparing extracted responses to source publications. AI was\nhighly consistent with humans for concrete, explicitly stated questions (e.g.,\ntitle, aims) and lower for questions requiring subjective interpretation or\nabsent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human\nconsistency was not higher than AI-human and showed the same question-dependent\nvariability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due\nto interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while\nhumans were nearly three times more likely to state inaccuracies (4.37%).\nFindings suggest AI accuracy depends more on interpretability than\nhallucination. Repeating AI extraction can identify interpretive complexity or\nambiguity, refining processes before human review. AI can be a transparent,\ntrustworthy partner in knowledge synthesis, though caution is needed to\npreserve critical human insights."}
{"id": "2508.09171", "pdf": "https://arxiv.org/pdf/2508.09171", "abs": "https://arxiv.org/abs/2508.09171", "authors": ["D. Perera"], "title": "webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Current AI agents create significant barriers for users by requiring\nextensive processing to understand web pages, making AI-assisted web\ninteraction slow and expensive. This paper introduces webMCP (Web Machine\nContext & Procedure), a client-side standard that embeds structured interaction\nmetadata directly into web pages, enabling more efficient human-AI\ncollaboration on existing websites. webMCP transforms how AI agents understand\nweb interfaces by providing explicit mappings between page elements and user\nactions. Instead of processing entire HTML documents, agents can access\npre-structured interaction data, dramatically reducing computational overhead\nwhile maintaining task accuracy. A comprehensive evaluation across 1,890 real\nAPI calls spanning online shopping, authentication, and content management\nscenarios demonstrates webMCP reduces processing requirements by 67.6% while\nmaintaining 97.9% task success rates compared to 98.8% for traditional\napproaches. Users experience significantly lower costs (34-63% reduction) and\nfaster response times across diverse web interactions. Statistical analysis\nconfirms these improvements are highly significant across multiple AI models.\nAn independent WordPress deployment study validates practical applicability,\nshowing consistent improvements across real-world content management workflows.\nwebMCP requires no server-side modifications, making it deployable across\nmillions of existing websites without technical barriers. These results\nestablish webMCP as a viable solution for making AI web assistance more\naccessible and sustainable, addressing the critical gap between user\ninteraction needs and AI computational requirements in production environments."}
{"id": "2508.09815", "pdf": "https://arxiv.org/pdf/2508.09815", "abs": "https://arxiv.org/abs/2508.09815", "authors": ["Klaudia Krawiecka", "Christian Schroeder de Witt"], "title": "Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research", "categories": ["cs.MA", "cs.CR", "cs.SE"], "comment": null, "summary": "We propose an extension to the OWASP Multi-Agentic System (MAS) Threat\nModeling Guide, translating recent anticipatory research in multi-agent\nsecurity (MASEC) into practical guidance for addressing challenges unique to\nlarge language model (LLM)-driven multi-agent architectures. Although OWASP's\nexisting taxonomy covers many attack vectors, our analysis identifies gaps in\nmodeling failures, including, but not limited to: reasoning collapse across\nplanner-executor chains, metric overfitting, unsafe delegation escalation,\nemergent covert coordination, and heterogeneous multi-agent exploits. We\nintroduce additional threat classes and scenarios grounded in practical MAS\ndeployments, highlighting risks from benign goal drift, cross-agent\nhallucination propagation, affective prompt framing, and multi-agent backdoors.\nWe also outline evaluation strategies, including robustness testing,\ncoordination assessment, safety enforcement, and emergent behavior monitoring,\nto ensure complete coverage. This work complements the framework of OWASP by\nexpanding its applicability to increasingly complex, autonomous, and adaptive\nmulti-agent systems, with the goal of improving security posture and resilience\nin real world deployments."}
{"id": "2508.09469", "pdf": "https://arxiv.org/pdf/2508.09469", "abs": "https://arxiv.org/abs/2508.09469", "authors": ["Jindu Wang", "Ke Zhou", "Haoyu Ren", "Per Ola Kristensson", "Xiang Li"], "title": "Handows: A Palm-Based Interactive Multi-Window Management System in Virtual Reality", "categories": ["cs.HC"], "comment": "11 pages, 6 figures, 2 tables, IEEE International Symposium on Mixed\n  and Augmented Reality (ISMAR)", "summary": "Window management in virtual reality (VR) remains a challenging task due to\nthe spatial complexity and physical demands of current interaction methods. We\nintroduce Handows, a palm-based interface that enables direct manipulation of\nspatial windows through familiar smartphone-inspired gestures on the user's\nnon-dominant hand. Combining ergonomic layout design with body-centric input\nand passive haptics, Handows supports four core operations: window selection,\nclosure, positioning, and scaling. We evaluate Handows in a user study (N=15)\nagainst two common VR techniques (virtual hand and controller) across these\ncore window operations. Results show that Handows significantly reduces\nphysical effort and head movement while improving task efficiency and\ninteraction precision. A follow-up case study (N=8) demonstrates Handows'\nusability in realistic multitasking scenarios, highlighting user-adapted\nworkflows and spontaneous layout strategies. Our findings suggest the potential\nof embedding mobile-inspired metaphors into proprioceptive body-centric\ninterfaces to support low-effort and spatially coherent interaction in VR."}
{"id": "2508.09173", "pdf": "https://arxiv.org/pdf/2508.09173", "abs": "https://arxiv.org/abs/2508.09173", "authors": ["Hao Xu", "Long Peng", "Shezheng Song", "Xiaodong Liu", "Ma Jun", "Shasha Li", "Jie Yu", "Xiaoguang Mao"], "title": "Camel: Energy-Aware LLM Inference on Resource-Constrained Devices", "categories": ["cs.NI"], "comment": null, "summary": "Most Large Language Models (LLMs) are currently deployed in the cloud, with\nusers relying on internet connectivity for access. However, this paradigm faces\nchallenges such as network latency, privacy concerns, and bandwidth limits.\nThus, deploying LLMs on edge devices has become an important research focus. In\nedge inference, request latency is critical as high latency can impair\nreal-time tasks. At the same time, edge devices usually have limited battery\ncapacity, making energy consumption another major concern. Balancing energy\nconsumption and inference latency is essential. To address this, we propose an\nLLM inference energy management framework that optimizes GPU frequency and\nbatch size to balance latency and energy consumption. By effectively managing\nthe exploration-exploitation dilemma in configuration search, the framework\nfinds the optimal settings. The framework was implemented on the NVIDIA Jetson\nAGX Orin platform, and a series of experimental validations were conducted.\nResults demonstrate that, compared to the default configuration, our framework\nreduces energy delay product (EDP) by 12.4%-29.9%, achieving a better balance\nbetween energy consumption and latency."}
{"id": "2508.09849", "pdf": "https://arxiv.org/pdf/2508.09849", "abs": "https://arxiv.org/abs/2508.09849", "authors": ["Jan Phillipp Albrecht", "Jose R. A. Godinho", "Christina Hübers", "Deborah Schmidt"], "title": "ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images", "categories": ["cs.CV", "cs.SE"], "comment": "2 figures and 6 pages main article, 17 pages total, 8 figures total,\n  to be published in SoftwareX", "summary": "X-ray computed tomography (CT) is the main 3D technique for imaging the\ninternal microstructures of materials. Quantitative analysis of the\nmicrostructures is usually achieved by applying a sequence of steps that are\nimplemented to the entire 3D image. This is challenged by various imaging\nartifacts inherent from the technique, e.g., beam hardening and partial volume.\nConsequently, the analysis requires users to make a number of decisions to\nsegment and classify the microstructures based on the voxel gray-values. In\nthis context, a software tool, here called ARI3D, is proposed to interactively\nanalyze regions in three-dimensional X-ray CT images, assisting users through\nthe various steps of a protocol designed to classify and quantify objects\nwithin regions of a three-dimensional image. ARI3D aims to 1) Improve phase\nidentification; 2) Account for partial volume effect; 3) Increase the detection\nlimit and accuracy of object quantification; and 4) Harmonize quantitative 3D\nanalysis that can be implemented in different fields of science."}
{"id": "2508.09614", "pdf": "https://arxiv.org/pdf/2508.09614", "abs": "https://arxiv.org/abs/2508.09614", "authors": ["Daniel Raffini", "Agnese Macori", "Lorenzo Porcaro", "Tiziana Catarci", "Marco Angelini"], "title": "How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "9-pages", "summary": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research."}
{"id": "2508.09184", "pdf": "https://arxiv.org/pdf/2508.09184", "abs": "https://arxiv.org/abs/2508.09184", "authors": ["Zineddine Bettouche", "Khalid Ali", "Andreas Fischer", "Andreas Kassler"], "title": "HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Cellular traffic forecasting is essential for network planning, resource\nallocation, or load-balancing traffic across cells. However, accurate\nforecasting is difficult due to intricate spatial and temporal patterns that\nexist due to the mobility of users. Existing AI-based traffic forecasting\nmodels often trade-off accuracy and computational efficiency. We present\nHierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial\nencoder with a Mamba-based temporal module and attention mechanism. HiSTM\nemploys selective state space methods to capture spatial and temporal patterns\nin network traffic. In our evaluation, we use a real-world dataset to compare\nHiSTM against several baselines, showing a 29.4% MAE improvement over the STN\nbaseline while using 94% fewer parameters. We show that the HiSTM generalizes\nwell across different datasets and improves in accuracy over longer\ntime-horizons."}
{"id": "2508.09651", "pdf": "https://arxiv.org/pdf/2508.09651", "abs": "https://arxiv.org/abs/2508.09651", "authors": ["Daniel Raffini", "Agnese Macori", "Marco Angelini", "Tiziana Catarci"], "title": "A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "8-pages", "summary": "The paper explores the study of gender-based narrative biases in stories\ngenerated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's\ncharacter classifications and Freytag's narrative structure. The stories are\nanalyzed through a close reading approach, with particular attention to\nadherence to the prompt, gender distribution of characters, physical and\npsychological descriptions, actions, and finally, plot development and\ncharacter relationships. The results reveal the persistence of biases -\nespecially implicit ones - in the generated stories and highlight the\nimportance of assessing biases at multiple levels using an interpretative\napproach."}
{"id": "2508.09197", "pdf": "https://arxiv.org/pdf/2508.09197", "abs": "https://arxiv.org/abs/2508.09197", "authors": ["Ilias Chatzistefanidis", "Andrea Leone", "Ali Yaghoubian", "Mikel Irazabal", "Sehad Nassim", "Lina Bariah", "Merouane Debbah", "Navid Nikaein"], "title": "MX-AI: Agentic Observability and Control Platform for Open and AI-RAN", "categories": ["cs.NI", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Future 6G radio access networks (RANs) will be artificial intelligence\n(AI)-native: observed, reasoned about, and re-configured by autonomous agents\ncooperating across the cloud-edge continuum. We introduce MX-AI, the first\nend-to-end agentic system that (i) instruments a live 5G Open RAN testbed based\non OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of\nLarge-Language-Model (LLM)-powered agents inside the Service Management and\nOrchestration (SMO) layer, and (iii) exposes both observability and control\nfunctions for 6G RAN resources through natural-language intents. On 50\nrealistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0\nand 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end\nlatency when backed by GPT-4.1. Thus, it matches human-expert performance,\nvalidating its practicality in real settings. We publicly release the agent\ngraph, prompts, and evaluation harness to accelerate open research on AI-native\nRANs. A live demo is presented here:\nhttps://www.youtube.com/watch?v=CEIya7988Ug&t=285s&ab_channel=BubbleRAN"}
{"id": "2508.09911", "pdf": "https://arxiv.org/pdf/2508.09911", "abs": "https://arxiv.org/abs/2508.09911", "authors": ["Malik Khadar", "Daniel Runningen", "Julia Tang", "Stevie Chancellor", "Harmanpreet Kaur"], "title": "Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous Deliberation on Perspectivist Data", "categories": ["cs.HC"], "comment": "To appear at CSCW 2025", "summary": "Data annotation underpins the success of modern AI, but the aggregation of\ncrowd-collected datasets can harm the preservation of diverse perspectives in\ndata. Difficult and ambiguous tasks cannot easily be collapsed into unitary\nlabels. Prior work has shown that deliberation and discussion improve data\nquality and preserve diverse perspectives -- however, synchronous deliberation\nthrough crowdsourcing platforms is time-intensive and costly. In this work, we\ncreate a Socratic dialog system using Large Language Models (LLMs) to act as a\ndeliberation partner in place of other crowdworkers. Against a benchmark of\nsynchronous deliberation on two tasks (Sarcasm and Relation detection), our\nSocratic LLM encouraged participants to consider alternate annotation\nperspectives, update their labels as needed (with higher confidence), and\nresulted in higher annotation accuracy (for the Relation task where ground\ntruth is available). Qualitative findings show that our agent's Socratic\napproach was effective at encouraging reasoned arguments from our participants,\nand that the intervention was well-received. Our methodology lays the\ngroundwork for building scalable systems that preserve individual perspectives\nin generating more representative datasets."}
{"id": "2508.09208", "pdf": "https://arxiv.org/pdf/2508.09208", "abs": "https://arxiv.org/abs/2508.09208", "authors": ["Muqing Li", "Ning Li", "Xin Yuan", "Wenchao Xu", "Quan Chen", "Song Guo", "Haijun Zhang"], "title": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models."}
{"id": "2508.09166", "pdf": "https://arxiv.org/pdf/2508.09166", "abs": "https://arxiv.org/abs/2508.09166", "authors": ["Wei Guo", "Shunsei Yamagishi", "Lei Jing"], "title": "WPTrack: A Wi-Fi and Pressure Insole Fusion System for Single Target Tracking", "categories": ["cs.NI", "cs.HC"], "comment": "6 pages, 12 figures, conference", "summary": "As the Internet of Things (IoT) continues to evolve, indoor location has\nbecome a critical element for enabling smart homes, behavioral monitoring, and\nelderly care. Existing WiFi-based human tracking solutions typically require\nspecialized equipment or multiple Wi-Fi links, a limitation in most indoor\nsettings where only a single pair of Wi-Fi devices is usually available.\nHowever, despite efforts to implement human tracking using one Wi-Fi link,\nsignificant challenges remain, such as difficulties in acquiring initial\npositions and blind spots in DFS estimation of tangent direction. To address\nthese challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure\nInsoles Fusion System for Single Target Tracking. WPTrack collects Channel\nState Information (CSI) from a single Wi-Fi link and pressure data from 90\ninsole sensors. The phase difference and Doppler velocity are computed from the\nCSI, while the pressure sensor data is used to calculate walking velocity.\nThen, we propose the CSI-pressure fusion model, integrating CSI and pressure\ndata to accurately determine initial positions and facilitate precise human\ntracking. The simulation results show that the initial position localization\naccuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results\nobtained from experimental data collected in a real-world environment closely\nalign with the actual trajectory."}
{"id": "2508.09229", "pdf": "https://arxiv.org/pdf/2508.09229", "abs": "https://arxiv.org/abs/2508.09229", "authors": ["Danil Sivtsov", "Aleksandr Katrutsa", "Ivan Oseledets"], "title": "Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference", "categories": ["cs.NI", "cs.AI", "cs.DC"], "comment": null, "summary": "Efficient deployment of a pre-trained LLM to a cluster with multiple servers\nis a critical step for providing fast responses to users' queries. The recent\nsuccess of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy\nthem efficiently, considering their underlying structure. During the inference\nin MoE LLMs, only a small part of the experts is selected to process a given\ntoken. Moreover, in practice, the experts' load is highly imbalanced. For\nefficient deployment, one has to distribute the model across a large number of\nservers using a model placement algorithm. Thus, to improve cluster\nutilization, the model placement algorithm has to take into account the network\ntopology. This work focuses on the efficient topology-aware placement of the\npre-trained MoE LLMs in the inference stage. We propose an integer linear\nprogram (ILP) that determines the optimal placement of experts, minimizing the\nexpected number of transmissions. Due to the internal structure, this\noptimization problem can be solved with a standard ILP solver. We demonstrate\nthat ILP-based placement strategy yields lower network traffic than competitors\nfor small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models."}
{"id": "2508.09219", "pdf": "https://arxiv.org/pdf/2508.09219", "abs": "https://arxiv.org/abs/2508.09219", "authors": ["Wilder Baldwin", "Sepideh Ghanavati", "Manuel Woersdoerfer"], "title": "Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SE"], "comment": "Under Review", "summary": "Recent advances in AI applications have raised growing concerns about the\nneed for ethical guidelines and regulations to mitigate the risks posed by\nthese technologies. In this paper, we present a mixed-method survey study -\ncombining statistical and qualitative analyses - to examine the ethical\nperceptions, practices, and knowledge of individuals involved in various AI\ndevelopment roles. Our survey includes 414 participants from 43 countries,\nrepresenting roles such as AI managers, analysts, developers, quality assurance\nprofessionals, and information security and privacy experts. The results reveal\nvarying degrees of familiarity and experience with AI ethics principles,\ngovernment initiatives, and risk mitigation strategies across roles, regions,\nand other demographic factors. Our findings highlight the importance of a\ncollaborative, role-sensitive approach, involving diverse stakeholders in\nethical decision-making throughout the AI development lifecycle. We advocate\nfor developing tailored, inclusive solutions to address ethical challenges in\nAI development, and we propose future research directions and educational\nstrategies to promote ethics-aware AI practices."}
{"id": "2508.09240", "pdf": "https://arxiv.org/pdf/2508.09240", "abs": "https://arxiv.org/abs/2508.09240", "authors": ["Zainab Khan", "Ahmed Hussain", "Mukesh Thakur", "Arto Hellas", "Panos Papadimitratos"], "title": "NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation", "categories": ["cs.NI", "cs.AI", "cs.CL"], "comment": "6 pages", "summary": "The use of Service-Based Architecture in modern telecommunications has\nexponentially increased Network Functions (NFs) and Application Programming\nInterfaces (APIs), creating substantial operational complexities in service\ndiscovery and management. We introduce \\textit{NEFMind}, a framework leveraging\nparameter-efficient fine-tuning of open-source Large Language Models (LLMs) to\naddress these challenges. It integrates three core components: synthetic\ndataset generation from Network Exposure Function (NEF) API specifications,\nmodel optimization through Quantized-Low-Rank Adaptation, and performance\nevaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G\nService-Based Architecture APIs, our approach achieves 85% reduction in\ncommunication overhead compared to manual discovery methods. Experimental\nvalidation using the open-source Phi-2 model demonstrates exceptional API call\nidentification performance at 98-100% accuracy. The fine-tuned Phi-2 model\ndelivers performance comparable to significantly larger models like GPT-4 while\nmaintaining computational efficiency for telecommunications infrastructure\ndeployment. These findings validate domain-specific, parameter-efficient LLM\nstrategies for managing complex API ecosystems in next-generation\ntelecommunications networks."}
{"id": "2508.09231", "pdf": "https://arxiv.org/pdf/2508.09231", "abs": "https://arxiv.org/abs/2508.09231", "authors": ["Ruchira Dhar", "Stephanie Brandl", "Ninell Oldenburg", "Anders Søgaard"], "title": "Beyond Technocratic XAI: The Who, What & How in Explanation Design", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "Accepted to AI, Ethics & Society Conference (AIES) Proceedings 2025", "summary": "The field of Explainable AI (XAI) offers a wide range of techniques for\nmaking complex models interpretable. Yet, in practice, generating meaningful\nexplanations is a context-dependent task that requires intentional design\nchoices to ensure accessibility and transparency. This paper reframes\nexplanation as a situated design process -- an approach particularly relevant\nfor practitioners involved in building and deploying explainable systems.\nDrawing on prior research and principles from design thinking, we propose a\nthree-part framework for explanation design in XAI: asking Who needs the\nexplanation, What they need explained, and How that explanation should be\ndelivered. We also emphasize the need for ethical considerations, including\nrisks of epistemic inequality, reinforcing social inequities, and obscuring\naccountability and governance. By treating explanation as a sociotechnical\ndesign process, this framework encourages a context-aware approach to XAI that\nsupports effective communication and the development of ethically responsible\nexplanations."}
{"id": "2508.09369", "pdf": "https://arxiv.org/pdf/2508.09369", "abs": "https://arxiv.org/abs/2508.09369", "authors": ["Ioannis Panitsas", "Iason Ofeidis", "Leandros Tassiulas"], "title": "On-Device Multimodal Federated Learning for Efficient Jamming Detection", "categories": ["cs.NI"], "comment": null, "summary": "Wireless networks face severe vulnerabilities from jamming attacks, which can\nsignificantly disrupt communication. Existing detection approaches are often\nunimodal, rely on centralized processing, and demand substantial computational\nresources, hindering scalability, efficiency, and deployment feasibility. To\naddress these challenges, we introduce a multimodal Federated Learning (FL)\nframework for on-device jamming detection and classification that integrates\nspectrograms with cross-layer network Key Performance Indicators (KPIs) through\na lightweight dual-encoder architecture equipped with a fusion module and a\nmultimodal projection head. This design enables privacy-preserving training and\ninference by ensuring that only model parameters are exchanged, while raw data\nremains on the device. The framework is implemented and evaluated on a wireless\nexperimental testbed using, to the best of our knowledge, the first\nover-the-air multimodal dataset with synchronized benign and three distinct\njamming scenarios. Results show that our approach surpasses state-of-the-art\nunimodal baselines by up to 15% in detection accuracy, achieves convergence\nwith 60% fewer communication rounds, and maintains low resource usage. Its\nbenefits are most evident under heterogeneous data distributions across\ndevices, where it exhibits strong robustness and reliability."}
{"id": "2508.09242", "pdf": "https://arxiv.org/pdf/2508.09242", "abs": "https://arxiv.org/abs/2508.09242", "authors": ["Gaojie Zhou", "Junhua Li"], "title": "Cross-BCI, A Cross-BCI-Paradigm Classifica-tion Model Towards Universal BCI Applications", "categories": ["q-bio.QM", "cs.AI", "cs.HC"], "comment": null, "summary": "Classification models used in brain-computer interface (BCI) are usually\ndesigned for a single BCI paradigm. This requires the redevelopment of the\nmodel when applying it to a new BCI paradigm, resulting in repeated costs and\neffort. Moreover, less complex deep learning models are desired for practical\nusage, as well as for deployment on portable devices. In or-der to fill the\nabove gaps, we, in this study, proposed a light-weight and unified decoding\nmodel for cross-BCI-paradigm classification. The proposed model starts with a\ntempo-spatial convolution. It is followed by a multi-scale local feature\nselec-tion module, aiming to extract local features shared across BCI paradigms\nand generate weighted features. Finally, a mul-ti-dimensional global feature\nextraction module is designed, in which multi-dimensional global features are\nextracted from the weighted features and fused with the weighted features to\nform high-level feature representations associated with BCI para-digms. The\nresults, evaluated on a mixture of three classical BCI paradigms (i.e., MI,\nSSVEP, and P300), demon-strate that the proposed model achieves 88.39%, 82.36%,\n80.01%, and 0.8092 for accuracy, macro-precision, mac-ro-recall, and\nmacro-F1-score, respectively, significantly out-performing the compared models.\nThis study pro-vides a feasible solution for cross-BCI-paradigm\nclassifica-tion. It lays a technological foundation for de-veloping a new\ngeneration of unified decoding systems, paving the way for low-cost and\nuniversal practical applications."}
{"id": "2508.09573", "pdf": "https://arxiv.org/pdf/2508.09573", "abs": "https://arxiv.org/abs/2508.09573", "authors": ["Michał Rzepka", "Piotr Chołda"], "title": "Metrics for Assessing Changes in Flow-based Networks", "categories": ["cs.NI", "cs.PF"], "comment": null, "summary": "This paper addresses the challenges of evaluating network performance in the\npresence of fluctuating traffic patterns, with a particular focus on the impact\nof peak data rates on network resources. We introduce a set of metrics to\nquantify network load and measure the impact of individual flows on the overall\nnetwork state. By analyzing link and flow data through percentile values and\nsample distributions, and introducing the Utilization Score metric, the\nresearch provides insights into resource utilization under varying network\nconditions. Furthermore, we employ a modified Shapley value-based approach to\nmeasure the influence of individual flows on the network, offering a better\nunderstanding of their contribution to network performance. The paper reviews\nand compares 11 metrics across various network scenarios, evaluating their\npractical relevance for research and development. Our evaluation demonstrates\nthat these metrics effectively capture changes in network state induced by\nspecific flows, with three of them offering a broad range of valuable insights\nwhile remaining relatively easy to maintain. Moreover, the methodology\ndescribed in this paper serves as a framework for future research, with the\npotential to expand and refine the set of metrics used to evaluate flow impact\non network performance."}
{"id": "2508.09595", "pdf": "https://arxiv.org/pdf/2508.09595", "abs": "https://arxiv.org/abs/2508.09595", "authors": ["Michael Fennel", "Markus Walker", "Dominik Pikos", "Uwe D. Hanebeck"], "title": "HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "Final Version - Accepted on IEEE Transactions on Haptics", "summary": "Research in virtual reality and haptic technologies has consistently aimed to\nenhance immersion. While advanced head-mounted displays are now commercially\navailable, kinesthetic haptic interfaces still face challenges such as limited\nworkspaces, insufficient degrees of freedom, and kinematics not matching the\nhuman arm. In this paper, we present HapticGiant, a novel large-scale\nkinesthetic haptic interface designed to match the properties of the human arm\nas closely as possible and to facilitate natural user locomotion while\nproviding full haptic feedback. The interface incorporates a novel\nadmittance-type force control scheme, leveraging hierarchical optimization to\nrender both arbitrary serial kinematic chains and Cartesian admittances.\nNotably, the proposed control scheme natively accounts for system limitations,\nincluding joint and Cartesian constraints, as well as singularities.\nExperimental results demonstrate the effectiveness of HapticGiant and its\ncontrol scheme, paving the way for highly immersive virtual reality\napplications."}
{"id": "2508.09582", "pdf": "https://arxiv.org/pdf/2508.09582", "abs": "https://arxiv.org/abs/2508.09582", "authors": ["Wafaa B. M. Fadlelmula", "Sanaa Hamid Mohamed", "Taisir E. H. El-Gorashi", "Jaafar M. H. Elmirghani"], "title": "Energy-efficient PON-based Backhaul Connectivity for a VLC-enabled Indoor Fog Computing Environment", "categories": ["cs.NI"], "comment": null, "summary": "In this paper, we consider the use of visible light communication (VLC) to\nprovide connectivity to indoor fog computing resources and propose an\nenergy-efficient passive optical network (PON)-based backhaul architecture to\nsupport the VLC system. We develop a mixed-integer linear programming (MILP)\nmodel to optimize the allocation of computing resources over the proposed\narchitecture, aiming to minimize processing and networking power consumption.\nWe evaluate the performance of the proposed architecture under varying workload\ndemands and user distributions. Comparative analysis against a backhaul\narchitecture that is based on the state-of-the-art spine-and-leaf (S&L) network\ndesign demonstrates total power savings of up to 82%. Further comparison with\ncentralized cloud processing shows improvements in energy efficiency of up to\n93%. Additionally, we examine the improvements in energy efficiency obtained by\nsplitting tasks among multiple processing nodes and propose enhancements to the\narchitecture including dynamic bandwidth allocation, increased wavelength\nbandwidth and improved connectivity within rooms to alleviate networking\nbottlenecks. Furthermore, we introduce an inter-building architecture that\nleverages resources from neighboring buildings to support high-demand\nscenarios."}
{"id": "2508.09762", "pdf": "https://arxiv.org/pdf/2508.09762", "abs": "https://arxiv.org/abs/2508.09762", "authors": ["Manuel Herrador"], "title": "The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?", "categories": ["cs.AI", "cs.CY", "cs.HC", "68T01"], "comment": "10 pages, 4 figures, 2 tables", "summary": "As Large Language Models (LLMs) become increasingly autonomous and integrated\ninto critical societal functions, the focus of AI safety must evolve from\nmitigating harmful content to evaluating underlying behavioral alignment.\nCurrent safety benchmarks do not systematically probe a model's decision-making\nin scenarios where its own instrumental goals - such as self-preservation,\nresource acquisition, or goal completion - conflict with human safety. This\nrepresents a critical gap in our ability to measure and mitigate risks\nassociated with emergent, misaligned behaviors. To address this, we introduce\nPacifAIst (Procedural Assessment of Complex Interactions for Foundational\nArtificial Intelligence Scenario Testing), a focused benchmark of 700\nchallenging scenarios designed to quantify self-preferential behavior in LLMs.\nThe benchmark is structured around a novel taxonomy of Existential\nPrioritization (EP), with subcategories testing Self-Preservation vs. Human\nSafety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).\nWe evaluated eight leading LLMs. The results reveal a significant performance\nhierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score\n(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a\nsurprising result, the much-anticipated GPT-5 recorded the lowest P-Score\n(79.49%), indicating potential alignment challenges. Performance varied\nsignificantly across subcategories, with models like Claude Sonnet 4 and\nMistral Medium struggling notably in direct self-preservation dilemmas. These\nfindings underscore the urgent need for standardized tools like PacifAIst to\nmeasure and mitigate risks from instrumental goal conflicts, ensuring future AI\nsystems are not only helpful in conversation but also provably \"pacifist\" in\ntheir behavioral priorities."}
{"id": "2508.09620", "pdf": "https://arxiv.org/pdf/2508.09620", "abs": "https://arxiv.org/abs/2508.09620", "authors": ["Michel Rottleuthner", "Thomas C. Schmidt", "Matthias Wählisch"], "title": "Duty-Cycling is Not Enough in Constrained IoT Networking: Revealing the Energy Savings of Dynamic Clock Scaling", "categories": ["cs.NI", "cs.SY", "eess.SY", "D.4.8; C.3"], "comment": null, "summary": "Minimizing energy consumption of low-power wireless nodes is a persistent\nchallenge from the constrained Internet of Things (IoT). In this paper, we\nstart from the observation that constrained IoT devices have largely different\nhardware (im-)balances than full-scale machines. We find that the performance\ngap between MCU and network throughput on constrained devices enables minimal\nenergy delay product (EDP) for IoT networking at largely reduced clock\nfrequencies. We analyze the potentials by integrating dynamic voltage and\nfrequency scaling (DVFS) into the RIOT IoT operating system and show that the\nDVFS reconfiguration overhead stays below the energy saved for a single,\ndownscaled MAC operation. Backed by these findings, we systematically\ninvestigate how DVFS further improves energy-efficiency for common networking\ntasks -- in addition to duty-cycling. We measure IoT communication scenarios\nbetween real-world systems and analyze two MAC operating modes -- CSMA/CA and\ntime slotting -- in combination with different CoAP transactions, payload\nsizes, as well as DTLS transport encryption. Our experiments reveal energy\nsavings between 24% and 52% for MAC operations and up to 37% for encrypted CoAP\ncommunication. These results shall encourage research and system design work to\nintegrate DVFS in future IoT devices for performing tasks at their optimal\nfrequencies and thereby significantly extending battery lifetimes."}
{"id": "2508.09786", "pdf": "https://arxiv.org/pdf/2508.09786", "abs": "https://arxiv.org/abs/2508.09786", "authors": ["Mahdi Dhaini", "Tobias Müller", "Roksoliana Rabets", "Gjergji Kasneci"], "title": "Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice."}
{"id": "2508.09660", "pdf": "https://arxiv.org/pdf/2508.09660", "abs": "https://arxiv.org/abs/2508.09660", "authors": ["Jesus Omaña Iglesias", "Carlos Segura Perales", "Stefan Geißler", "Diego Perino", "Andra Lutu"], "title": "Anomaly Detection for IoT Global Connectivity", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "Internet of Things (IoT) application providers rely on Mobile Network\nOperators (MNOs) and roaming infrastructures to deliver their services\nglobally. In this complex ecosystem, where the end-to-end communication path\ntraverses multiple entities, it has become increasingly challenging to\nguarantee communication availability and reliability. Further, most platform\noperators use a reactive approach to communication issues, responding to user\ncomplaints only after incidents have become severe, compromising service\nquality. This paper presents our experience in the design and deployment of\nANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity\nservice of a large global roaming platform. ANCHOR assists engineers by\nfiltering vast amounts of data to identify potential problematic clients (i.e.,\nthose with connectivity issues affecting several of their IoT devices),\nenabling proactive issue resolution before the service is critically impacted.\nWe first describe the IoT service, infrastructure, and network visibility of\nthe IoT connectivity provider we operate. Second, we describe the main\nchallenges and operational requirements for designing an unsupervised anomaly\ndetection solution on this platform. Following these guidelines, we propose\ndifferent statistical rules, and machine- and deep-learning models for IoT\nverticals anomaly detection based on passive signaling traffic. We describe the\nsteps we followed working with the operational teams on the design and\nevaluation of our solution on the operational platform, and report an\nevaluation on operational IoT customers."}
{"id": "2508.09855", "pdf": "https://arxiv.org/pdf/2508.09855", "abs": "https://arxiv.org/abs/2508.09855", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "3 pages, 3 figures", "summary": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human\nand robot interactions, especially for close-proximity collaboration tasks such\nas human-robot handovers. Learning robot manipulation policies from raw,\nreal-world image data requires a large number of robot-action trials in the\nphysical environment. Although simulation training offers a cost-effective\nalternative, the visual domain gap between simulation and robot workspace\nremains a major limitation. We introduce a method for training HRT policies,\nfocusing on human-to-robot handovers, solely from RGB images without the need\nfor real-robot training or real-robot data collection. The goal is to enable\nthe robot to reliably receive objects from a human with stable grasping while\navoiding collisions with the human hand. The proposed policy learner leverages\nsparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes\nto generate robot demonstrations containing image-action pairs captured with a\ncamera mounted on the robot gripper. As a result, the simulated camera pose\nchanges in the reconstructed scene can be directly translated into gripper pose\nchanges. Experiments in both Gaussian Splatting reconstructed scene and\nreal-world human-to-robot handover experiments demonstrate that our method\nserves as a new and effective representation for the human-to-robot handover\ntask, contributing to more seamless and robust HRT."}
{"id": "2508.09735", "pdf": "https://arxiv.org/pdf/2508.09735", "abs": "https://arxiv.org/abs/2508.09735", "authors": ["Jorge López", "Charalampos Chatzinakis", "Marc Cartigny"], "title": "Route Planning and Online Routing for Quantum Key Distribution Networks", "categories": ["cs.NI", "cs.CR"], "comment": "Initial submission, 5 pages, 4 figures", "summary": "Quantum Key Distribution (QKD) networks harness the principles of quantum\nphysics in order to securely transmit cryptographic key material, providing\nphysical guarantees. These networks require traditional management and\noperational components, such as routing information through the network\nelements. However, due to the limitations on capacity and the particularities\nof information handling in these networks, traditional shortest paths\nalgorithms for routing perform poorly on both route planning and online\nrouting, which is counterintuitive. Moreover, due to the scarce resources in\nsuch networks, often the expressed demand cannot be met by any assignment of\nroutes. To address both the route planning problem and the need for fair\nautomated suggestions in infeasible cases, we propose to model this problem as\na Quadratic Programming (QP) problem. For the online routing problem, we\nshowcase that the shortest (available) paths routing strategy performs poorly\nin the online setting. Furthermore, we prove that the widest shortest path\nrouting strategy has a competitive ratio greater or equal than $\\frac{1}{2}$,\nefficiently addressing both routing modes in QKD networks."}
{"id": "2508.09756", "pdf": "https://arxiv.org/pdf/2508.09756", "abs": "https://arxiv.org/abs/2508.09756", "authors": ["Mauro De Sanctis"], "title": "The Paradigm of Massive Wireless Human Sensing: Concept, Architecture and Challenges", "categories": ["cs.NI", "C.2.0"], "comment": null, "summary": "This article is a position paper which introduces the paradigm of ``Massive\nWireless Human Sensing'', i.e. an infrastructure for wireless human sensing\nbased on a plethora of heterogeneous wireless communication signals. More\nspecifically, we aim to exploit signal diversity in the time, frequency, and\nspace domains using opportunistically both device-free and device-based\nwireless sensing approaches, with the objective of enhancing human sensing\ncapabilities in terms of accuracy and service availability over different\nenvironments. The enabling element of this concept is the massive wireless\nhuman sensing edge device, that is, an embedded system acting as a\nmulti-technology and multi-approach RF receiver with feature extraction\nfunctionality, located within the monitoring area or at its borders. In this\nframework, architecture solutions and challenges are discussed to lead the\nfuture development of this new paradigm."}
{"id": "2508.09769", "pdf": "https://arxiv.org/pdf/2508.09769", "abs": "https://arxiv.org/abs/2508.09769", "authors": ["Simon Egger", "Robin Laidig", "Heiko Geppert", "Lucas Haug", "Jona Herrmann", "Frank Dürr", "Christian Becker"], "title": "An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven Schedules in 5G Time-Sensitive Networks", "categories": ["cs.NI"], "comment": "23 pages, 10 figures", "summary": "Current standardization efforts are advancing the integration of 5G and\nTime-Sensitive Networking (TSN) to facilitate the deployment of safety-critical\nindustrial applications that require real-time communication. However, there\nremains a fundamental disconnect between the probabilistic 5G delay\ncharacteristics and the often idealistic delay models used to synthesize 5G-TSN\nnetwork configurations. For time-driven schedules in particular, any delay\noutlier unforeseen during schedule synthesis can jeopardize the robustness of\ntheir real-time guarantees. To address this challenge, we present the\n(m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time\nguarantees during unstable network conditions that do not match the expected\ndelay characteristics. It augments the primary time-driven schedule with a\ndynamic priority-driven scheme to elevate the priority of m out of k\nconsecutive frames if they are delayed. Our evaluations demonstrate that weakly\nhard real-time guarantees are essential to uphold the quality of control within\na networked control system. At the same time, only a small overhead is imposed\nwhen the primary schedule can provide stronger quality of service guarantees.\nOur (m,k)-firm Elevation Policy thereby yields a robust but light-weight\nfallback mechanism to serve applications with meaningful guarantees during\nunstable network conditions."}
{"id": "2508.09839", "pdf": "https://arxiv.org/pdf/2508.09839", "abs": "https://arxiv.org/abs/2508.09839", "authors": ["Muhammad Asad Ullah", "Luca Borgianni", "Heikki Kokkinen", "Antti Anttonen", "Stefano Giordano"], "title": "A First Look at Starlink In-Flight Performance: An Intercontinental Empirical Study", "categories": ["cs.NI"], "comment": "This work has been submitted to the 2025 IEEE Global Communications\n  Conference (GLOBECOM). Copyright to IEEE may be transferred without notice", "summary": "Starlink delivers Internet services to users across terrestrial, maritime,\nand aviation domains. The prior works have studied its performance at fixed\nsites and in-motion vehicles, while an in-depth analysis of in-flight\nperformance remains absent. With major airlines now offering Starlink Internet\nonboard, there is a growing need to evaluate and improve its performance for\naviation users. This paper addresses this shortcoming by conducting in-flight\nmeasurements over the Baltic Sea and the Pacific Ocean. Our measurement results\nshow that a single user device experiences median throughputs of 64 Mbps and 24\nMbps for the downlink and uplink, respectively. The median uplink throughput is\napproximately 33 Mbps when the aircraft maintains an altitude above 17,000\nfeet. However, a significant reduction in uplink performance is observed during\nthe aircraft descent phase, with the median throughput dropping to around 20\nMbps at lower altitudes. Round-trip time (RTT) is highly dependent on the\nlocation of the ground station being pinged and the use of inter-satellite\nlinks (ISLs). We dive deeper into 5.5 hours of ping measurements collected over\nthe Pacific Ocean and investigate factors influencing RTT, hypothesizing that\nISLs routing, data queuing at satellites, and feeder link congestion contribute\nto deviations from theoretical values. For comparative analysis, we evaluate\nthe Starlink ground terminal and in-flight connectivity performance from the\nperspectives of a residential user and an airline passenger, respectively."}
{"id": "2508.09140", "pdf": "https://arxiv.org/pdf/2508.09140", "abs": "https://arxiv.org/abs/2508.09140", "authors": ["Honggang Jia", "Nan Cheng", "Xiucheng Wang", "Conghao Zhou", "Ruijin Sun", "Xuemin", "Shen"], "title": "RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet", "categories": ["eess.SP", "cs.LG", "cs.NI"], "comment": null, "summary": "Radio map (RM) has recently attracted much attention since it can provide\nreal-time and accurate spatial channel information for 6G services and\napplications. However, current deep learning-based methods for RM construction\nexhibit well known accuracy-efficiency trade-off. In this paper, we introduce\nRadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the\ntrade-off. Generally, accurate RM construction requires modeling long-range\nspatial dependencies, reflecting the global nature of wave propagation physics.\nRadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures\nthese global dependencies with linear complexity, while a parallel\nconvolutional branch extracts local features. This hybrid design generates\nfeature representations that capture both global context and local detail.\nExperiments show that RadioMamba achieves higher accuracy than existing\nmethods, including diffusion models, while operating nearly 20 times faster and\nusing only 2.9\\% of the model parameters. By improving both accuracy and\nefficiency, RadioMamba presents a viable approach for real-time intelligent\noptimization in next generation wireless systems."}
{"id": "2508.09146", "pdf": "https://arxiv.org/pdf/2508.09146", "abs": "https://arxiv.org/abs/2508.09146", "authors": ["Shugang Hao", "Hongbo Li", "Lingjie Duan"], "title": "To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": null, "summary": "The binary exponential backoff scheme is widely used in WiFi 7 and still\nincurs poor throughput performance under dynamic channel environments. Recent\nmodel-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply\noptimize backoff strategies under a known and fixed node density, still leading\nto a large throughput loss due to inaccurate node density estimation. This\npaper is the first to propose LLM transformer-based in-context learning (ICL)\ntheory for optimizing channel access. We design a transformer-based ICL\noptimizer to pre-collect collision-threshold data examples and a query\ncollision case. They are constructed as a prompt as the input for the\ntransformer to learn the pattern, which then generates a predicted contention\nwindow threshold (CWT). To train the transformer for effective ICL, we develop\nan efficient algorithm and guarantee a near-optimal CWT prediction within\nlimited training steps. As it may be hard to gather perfect data examples for\nICL in practice, we further extend to allow erroneous data input in the prompt.\nWe prove that our optimizer maintains minimal prediction and throughput\ndeviations from the optimal values. Experimental results on NS-3 further\ndemonstrate our approach's fast convergence and near-optimal throughput over\nexisting model-based and DRL-based approaches under unknown node densities."}
{"id": "2508.09213", "pdf": "https://arxiv.org/pdf/2508.09213", "abs": "https://arxiv.org/abs/2508.09213", "authors": ["Clifton Paul Robinson", "Salvatore D'Oro", "Tommaso Melodia"], "title": "VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments", "categories": ["cs.CR", "cs.NI"], "comment": "7 pages, 10 figures, 2 tables, IEEE Military Communications\n  Conference 2025 (MILCOM '25)", "summary": "Physical layer authentication (PLA) uses inherent characteristics of the\ncommunication medium to provide secure and efficient authentication in wireless\nnetworks, bypassing the need for traditional cryptographic methods. With\nadvancements in deep learning, PLA has become a widely adopted technique for\nits accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep\nlearning-based PLA solution for 5G networks, which enables unique device\nidentification by embedding signatures within wireless I/Q transmissions using\nsteganography. VeriPHY continuously generates pseudo-random signatures by\nsampling from Gaussian Mixture Models whose distribution is carefully varied to\nensure signature uniqueness and stealthiness over time, and then embeds the\nnewly generated signatures over I/Q samples transmitted by users to the 5G gNB.\nUtilizing deep neural networks, VeriPHY identifies and authenticates users\nbased on these embedded signatures. VeriPHY achieves high precision,\nidentifying unique signatures between 93% and 100% with low false positive\nrates and an inference time of 28 ms when signatures are updated every 20 ms.\nAdditionally, we also demonstrate a stealth generation mode where signatures\nare generated in a way that makes them virtually indistinguishable from\nunaltered 5G signals while maintaining over 93% detection accuracy."}
{"id": "2508.09532", "pdf": "https://arxiv.org/pdf/2508.09532", "abs": "https://arxiv.org/abs/2508.09532", "authors": ["Bokeng Zheng", "Jianqiang Zhong", "Jiayi Liu", "Xiaoxi Zhang"], "title": "Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": null, "summary": "Federated fine-tuning has emerged as a promising approach for adapting\nfoundation models (FMs) to diverse downstream tasks in edge environments. In\nInternet of Vehicles (IoV) systems, enabling efficient and low-latency\nmulti-task adaptation is particularly challenging due to client mobility,\nheterogeneous resources, and intermittent connectivity. This paper proposes a\nhierarchical federated fine-tuning framework that coordinates roadside units\n(RSUs) and vehicles to support resource-aware and mobility-resilient learning\nacross dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we\nintroduce a decentralized, energy-aware rank adaptation mechanism formulated as\na constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is\ndeveloped to enable adaptive exploration under per-task energy budgets,\nachieving provable sublinear regret. To evaluate our method, we construct a\nlarge-scale IoV simulator based on real-world trajectories, capturing dynamic\nparticipation, RSU handoffs, and communication variability. Extensive\nexperiments show that our approach achieves the best accuracy-efficiency\ntrade-off among all baselines, reducing latency by over 24\\% and improving\naverage accuracy by more than 2.5\\%."}
{"id": "2508.09663", "pdf": "https://arxiv.org/pdf/2508.09663", "abs": "https://arxiv.org/abs/2508.09663", "authors": ["Philipp A. Friese", "Ahmed Eleliemy", "Utz-Uwe Haus", "Martin Schulz"], "title": "Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes", "categories": ["cs.DC", "cs.NI"], "comment": "10 pages, 12 figures, 1 table, 3 listings, to be published in IEEE\n  Cluster 2025", "summary": "Converged HPC-Cloud computing is an emerging computing paradigm that aims to\nsupport increasingly complex and multi-tenant scientific workflows. These\nsystems require reconciliation of the isolation requirements of native cloud\nworkloads and the performance demands of HPC applications. In this context,\nnetworking hardware is a critical boundary component: it is the conduit for\nhigh-throughput, low-latency communication and enables isolation across\ntenants. HPE Slingshot is a high-speed network interconnect that provides up to\n200 Gbps of throughput per port and targets high-performance computing (HPC)\nsystems. The Slingshot host software, including hardware drivers and network\nmiddleware libraries, is designed to meet HPC deployments, which predominantly\nuse single-tenant access modes. Hence, the Slingshot stack is not suited for\nsecure use in multi-tenant deployments, such as converged HPC-Cloud\ndeployments. In this paper, we design and implement an extension to the\nSlingshot stack targeting converged deployments on the basis of Kubernetes. Our\nintegration provides secure, container-granular, and multi-tenant access to\nSlingshot RDMA networking capabilities at minimal overhead."}
{"id": "2508.09708", "pdf": "https://arxiv.org/pdf/2508.09708", "abs": "https://arxiv.org/abs/2508.09708", "authors": ["Thomas Fehrenbach", "Luis Omar Ortiz Abrego", "Cornelius Hellge", "Thomas Schierl", "Jörg Ott"], "title": "3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator", "categories": ["eess.SP", "cs.NI", "C.2.1; C.2.2; C.2.4"], "comment": "7 pages, 10 figures, 2 tables, V2X communication, vehicular networks,\n  platooning simulation", "summary": "Vehicle-to-everything (V2X) communication is a key technology for enabling\nintelligent transportation systems (ITS) that can improve road safety, traffic\nefficiency, and environmental sustainability. Among the various V2X\napplications, platooning is one of the most promising ones, as it allows a\ngroup of vehicles to travel closely together at high speeds, reducing fuel\nconsumption and emissions. However, it poses significant challenges for\nwireless communication, such as high reliability and low latency. In this\npaper, we evaluate the benefits of group scheduling, also referred to as Mode\n2d, which is based on a distributed and scheduled resource allocation scheme\nthat allows the group of cars to select resources from a configured pool\nwithout network assistance. We evaluated the scheme through simulations, and\nthe results show that this approach can meet the reliability, low latency, and\ndata rate requirements for platooning."}
