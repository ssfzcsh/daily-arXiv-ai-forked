<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 9]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 11]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [The State of Computational Science in Fission and Fusion Energy](https://arxiv.org/abs/2507.08061)
*Andrea Morales Coto,Aditi Verma*

Main category: cs.SE

TL;DR: 该论文调查了核工程计算科学的现状，揭示了软件工具的变化趋势，包括现代编程语言、开源代码和模块化软件的普及，以及对多物理场代码和更高开发预算的关注。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解核能（聚变与裂变）领域中计算科学家使用的软件工具及其发展趋势，以预测未来5到10年的行业变化。

Method: 通过调查103位从事核能领域代码开发的计算科学家，收集他们解决的问题、可用工具及开发体验的数据。

Result: 结果显示，现代编程语言（如Python和C++）逐渐取代FORTRAN，开源代码和模块化软件更受欢迎，多物理场代码需求增加，且开发预算显著上升（部分组织达5000万美元）。

Conclusion: 核工程代码的未来将趋向模块化、计算量小且更受组织重视，反映了行业向开放性和现代化工具的转变。

Abstract: The tools used to engineer something are just as important as the thing that
is actually being engineered. In fact, in many cases, the tools can indeed
determine what is engineerable. In fusion and fission1 energy engineering,
software has become the dominant tool for design. For that reason, in 2024, for
the first time ever, we asked 103 computational scientists developing the codes
used in fusion and fission energy about the problems they are attempting to
solve with their codes, the tools available to them to solve them, and their
end to end developer experience with said tools.
  The results revealed a changing tide in software tools in fusion and fission,
with more and more computational scientists preferring modern programming
languages, open-source codes, and modular software. These trends represent a
peek into what will happen 5 to 10 years in the future of nuclear engineering.
Since the majority of our respondents belonged to US national labs and
universities, these results hint at the most cutting-edge trends in the
industry. The insights included in the State of Computational Science in
Fission and Fusion Energy indicate a dramatic shift toward multiphysics codes,
a drop-off in the use of FORTRAN in favor of more modern languages like Python
and C++, and ever-rising budgets for code development, at times reaching $50M
in a single organization.
  Our survey paints a future of nuclear engineering codes that is modular in
nature, small in terms of compute, and increasingly prioritized by
organizations. Access to our results in web form are available online.

</details>


### [2] [Code with Me or for Me? How Increasing AI Automation Transforms Developer Workflows](https://arxiv.org/abs/2507.08149)
*Valerie Chen,Ameet Talwalkar,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: 研究探讨开发者与自动化编码代理的互动，发现代理能超越现有代码补全工具，但需解决用户对其行为的理解问题。


<details>
  <summary>Details</summary>
Motivation: 探索自动化AI编码代理对开发者生产力与体验的影响，填补现有研究主要依赖静态基准的空白。

Method: 招募GitHub Copilot的常规用户，对比评估GitHub Copilot与OpenHands两种工具的效果。

Result: 编码代理能完成传统工具无法实现的任务，但需解决用户理解代理行为的挑战。

Conclusion: 研究为开发者工作流变革提供见解，并提出构建更智能代理的建议。

Abstract: Developers now have access to a growing array of increasingly autonomous AI
tools to support software development. While numerous studies have examined
developer use of copilots, which can provide chat assistance or code
completions, evaluations of coding agents, which can automatically write files
and run code, still largely rely on static benchmarks without
humans-in-the-loop. In this work, we conduct the first academic study to
explore developer interactions with coding agents and characterize how more
autonomous AI tools affect user productivity and experience, compared to
existing copilots. We evaluate two leading copilot and agentic coding
assistants, GitHub Copilot and OpenHands, recruiting participants who regularly
use the former. Our results show agents have the potential to assist developers
in ways that surpass copilots (e.g., completing tasks that humans might not
have accomplished before) and reduce the user effort required to complete
tasks. However, there are challenges involved in enabling their broader
adoption, including how to ensure users have an adequate understanding of agent
behaviors. Our results not only provide insights into how developer workflows
change as a result of coding agents but also highlight how user interactions
with agents differ from those with existing copilots, motivating a set of
recommendations for researchers building new agents. Given the broad set of
developers who still largely rely on copilot-like systems, our work highlights
key challenges of adopting more agentic systems into developer workflows.

</details>


### [3] [The Impact of Generative AI on Code Expertise Models: An Exploratory Study](https://arxiv.org/abs/2507.08160)
*Otávio Cury,Guilherme Avelino*

Main category: cs.SE

TL;DR: 论文探讨了GenAI工具在代码生成中的应用如何影响开发者对代码的理解，并通过模拟分析显示这会降低现有专业知识评估指标的可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究GenAI工具在软件开发中的广泛使用可能导致的开发者对生成代码理解不足的问题，并探究其对源代码知识模型的潜在影响。

Method: 通过收集ChatGPT生成的代码在GitHub项目中的统计数据，并模拟不同GenAI贡献度的场景，分析知识模型和Truck Factor算法的变化。

Result: 研究发现大多数场景下GenAI的使用对专业知识评估指标产生显著影响，表明这些指标对GenAI的敏感性。

Conclusion: 随着GenAI在开发流程中的深入整合，现有的专业知识评估指标的可靠性可能会下降。

Abstract: Generative Artificial Intelligence (GenAI) tools for source code generation
have significantly boosted productivity in software development. However, they
also raise concerns, particularly the risk that developers may rely heavily on
these tools, reducing their understanding of the generated code. We hypothesize
that this loss of understanding may be reflected in source code knowledge
models, which are used to identify developer expertise. In this work, we
present an exploratory analysis of how a knowledge model and a Truck Factor
algorithm built upon it can be affected by GenAI usage. To investigate this, we
collected statistical data on the integration of ChatGPT-generated code into
GitHub projects and simulated various scenarios by adjusting the degree of
GenAI contribution. Our findings reveal that most scenarios led to measurable
impacts, indicating the sensitivity of current expertise metrics. This suggests
that as GenAI becomes more integrated into development workflows, the
reliability of such metrics may decrease.

</details>


### [4] [Leveraging Large Language Models for Classifying App Users' Feedback](https://arxiv.org/abs/2507.08250)
*Yasaman Abedini,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: 研究表明，利用GPT-3.5-Turbo等高级LLMs分类用户反馈，在有限标注数据下表现良好，并能提升BERT模型的分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决用户反馈分类中标注数据稀缺的挑战，探索LLMs在分类任务中的应用潜力。

Method: 使用四种LLMs（如GPT-3.5-Turbo）对八个已标注数据集实验，分析其分类性能，并利用LLMs生成标注数据增强训练集。

Result: LLMs能有效分类粗粒度反馈，通过标注数据增强显著提升分类模型性能。

Conclusion: LLMs在用户反馈分类中具有潜力，尤其在数据增强方面表现突出。

Abstract: In recent years, significant research has been conducted into classifying
application (app) user feedback, primarily relying on supervised machine
learning algorithms. However, fine-tuning more generalizable classifiers based
on existing labeled datasets remains an important challenge, as creating large
and accurately labeled datasets often requires considerable time and resources.
In this paper, we evaluate the capabilities of four advanced LLMs, including
GPT-3.5-Turbo, GPT-4, Flan-T5, and Llama3-70b, to enhance user feedback
classification and address the challenge of the limited labeled dataset. To
achieve this, we conduct several experiments on eight datasets that have been
meticulously labeled in prior research. These datasets include user reviews
from app stores, posts from the X platform, and discussions from the public
forums, widely recognized as representative sources of app user feedback. We
analyze the performance of various LLMs in identifying both fine-grained and
coarse-grained user feedback categories. Given the substantial volume of daily
user feedback and the computational limitations of LLMs, we leverage these
models as an annotation tool to augment labeled datasets with general and
app-specific data. This augmentation aims to enhance the performance of
state-of-the-art BERT-based classification models. Our findings indicate that
LLMs when guided by well-crafted prompts, can effectively classify user
feedback into coarse-grained categories. Moreover, augmenting the training
dataset with datasets labeled using the consensus of LLMs can significantly
enhance classifier performance.

</details>


### [5] [Computing Floating-Point Errors by Injecting Perturbations](https://arxiv.org/abs/2507.08467)
*Youshuai Tan,Zhanwei Zhang,Jinfu Chen,Zishuo Ding,Jifeng Xuan,Weiyi Shang*

Main category: cs.SE

TL;DR: PI-detector是一种高效且准确的浮点误差检测方法，通过扰动原子操作数来识别误差，解决了现有工具的实现难度高和执行时间长的局限性。


<details>
  <summary>Details</summary>
Motivation: 浮点程序在科学和工程中至关重要，但浮点误差可能导致严重后果。现有检测方法（如ATOMU和FPCC）存在假阳性或速度慢的问题。

Method: PI-detector通过向原子操作数注入小扰动，并与原始程序结果对比来计算浮点误差。

Result: 实验结果表明，PI-detector能够高效且准确地进行浮点误差计算。

Conclusion: PI-detector为浮点误差检测提供了一种更有效和高效的方法，克服了现有工具的局限性。

Abstract: Floating-point programs form the foundation of modern science and
engineering, providing the essential computational framework for a wide range
of applications, such as safety-critical systems, aerospace engineering, and
financial analysis. Floating-point errors can lead to severe consequences.
Although floating-point errors widely exist, only a subset of inputs may
trigger significant errors in floating-point programs. Therefore, it is crucial
to determine whether a given input could produce such errors. Researchers tend
to take the results of high-precision floating-point programs as oracles for
detecting floating-point errors, which introduces two main limitations: (1)
difficulty of implementation and (2) prolonged execution time. The two recent
tools, ATOMU and FPCC, can partially address these issues. However, ATOMU
suffers from false positives; while FPCC, though eliminating false positives,
operates at a considerably slower speed.
  To address these two challenges, we propose a novel approach named
PI-detector to computing floating-point errors effectively and efficiently. Our
approach is based on the observation that floating-point errors stem from large
condition numbers in atomic operations (such as addition and subtraction),
which then propagate and accumulate. PI-detector injects small perturbations
into the operands of individual atomic operations within the program and
compares the outcomes of the original program with the perturbed version to
compute floating-point errors. We evaluate PI-detector with datasets from ATOMU
and HSED, as well as a complex linear system-solving program. Experimental
results demonstrate that PI-detector can perform efficient and accurate
floating-point error computation.

</details>


### [6] [InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching](https://arxiv.org/abs/2507.08523)
*Yilun Wang,Pengfei Chen,Haiyu Huang,Zilong He,Gou Tan,Chuanfu Zhang,Jingkai He,Zibin Zheng*

Main category: cs.SE

TL;DR: InferLog是一种针对在线日志解析的LLM推理优化方法，旨在解决现有基于LLM的日志解析器在高并发请求下的延迟问题，同时保持解析准确性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统生成大量运行时日志，需要高效准确的日志解析以支持异常检测等任务。现有基于LLM的解析方法因隐私和高延迟问题无法满足生产环境需求。

Method: InferLog通过前缀感知的ICL优化策略和基于元学习的快速配置调整管道来加速LLM推理。

Result: 实验表明，InferLog在Loghub数据集和vLLM上显著提升了推理效率，且不降低解析准确性。

Conclusion: InferLog有效解决了LLM推理效率的瓶颈问题，为高吞吐日志解析提供了实用的解决方案。

Abstract: Modern software systems generate massive volumes of runtime logs,
necessitating efficient and accurate log parsing to enable critical downstream
tasks such as anomaly detection and root cause analysis. Recently, large
language models (LLMs) have achieved advanced accuracy on log parsing, but
their deployment in production environments faces two major limitations: (1)
the privacy risks associated with commercial LLMs, driving the adoption of
local deployment, and (2) the stringent latency and throughput requirements
imposed by high-volume log streams, which existing LLM-based parsers fail to
meet. Although recent efforts have reduced the number of LLM queries, they
overlook the high latency of the LLM invocations, where concurrent log parsing
requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization
method for online log parsing. Our key insight is that the inference efficiency
emerges as the vital bottleneck in LLM-based online log parsing, rather than
parsing accuracy. InferLog accelerates inference by designing (1) A
Prefix-aware ICL Refinement policy to refine the examples and permutation of
in-context learning to improve the prefix caching efficiency. (2) A rapid and
task-specific configuration tuning pipeline based on meta-learning to find the
optimal LLM scheduling-related configuration for dynamic log parsing workloads.
The experimental results based on Loghub dataset and vLLM demonstrate that
InferLog significantly outperforms existing inference optimization methods and
markedly accelerates the state-of-the-art LLM-based log parser without
compromising parsing accuracy.

</details>


### [7] [Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy](https://arxiv.org/abs/2507.08594)
*Fernando Ayach,Vitor Lameirão,Raul Leão,Jerfferson Felizardo,Rafael Sobrinho,Vanessa Borges,Patrícia Matsubara,Awdren Fontão*

Main category: cs.SE

TL;DR: 论文提出了一种基于提示工程的生成式AI方法，用于快速创建用户画像，通过案例研究验证了其效率提升和用户接受度。


<details>
  <summary>Details</summary>
Motivation: 解决传统用户画像创建过程耗时、认知负荷高且易偏颇的问题。

Method: 采用生成式AI结合提示工程技术，通过19人参与的案例研究评估方法效果。

Result: 方法显著减少了时间和精力，提高了用户画像质量，但在通用性和领域特异性方面存在局限。

Conclusion: 生成式AI可用于软件产品发现实践，但需进一步优化以适应不同场景。

Abstract: Proto-personas are commonly used during early-stage Product Discovery, such
as Lean Inception, to guide product definition and stakeholder alignment.
However, the manual creation of proto-personas is often time-consuming,
cognitively demanding, and prone to bias. In this paper, we propose and
empirically investigate a prompt engineering-based approach to generate
proto-personas with the support of Generative AI (GenAI). Our goal is to
evaluate the approach in terms of efficiency, effectiveness, user acceptance,
and the empathy elicited by the generated personas. We conducted a case study
with 19 participants embedded in a real Lean Inception, employing a qualitative
and quantitative methods design. The results reveal the approach's efficiency
by reducing time and effort and improving the quality and reusability of
personas in later discovery phases, such as Minimum Viable Product (MVP)
scoping and feature refinement. While acceptance was generally high, especially
regarding perceived usefulness and ease of use, participants noted limitations
related to generalization and domain specificity. Furthermore, although
cognitive empathy was strongly supported, affective and behavioral empathy
varied significantly across participants. These results contribute novel
empirical evidence on how GenAI can be effectively integrated into software
Product Discovery practices, while also identifying key challenges to be
addressed in future iterations of such hybrid design processes.

</details>


### [8] [NL in the Middle: Code Translation with LLMs and Intermediate Representations](https://arxiv.org/abs/2507.08627)
*Chi-en Amy Tai,Pengyu Nie,Lukasz Golab,Alexander Wong*

Main category: cs.SE

TL;DR: 研究探讨了如何利用中间表示（自然语言和抽象语法树）提升大语言模型（LLM）在代码翻译中的准确性，发现链式思考提示（CoT）结合自然语言摘要效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，大语言模型在代码翻译中容易产生错误，希望通过中间表示提升翻译准确性。

Method: 采用多种提示工程方法（从一次性到链式思考提示），结合自然语言和抽象语法树作为中间表示，测试了多种模型（如Open Gpt4 8X7B等）在代码翻译基准上的表现。

Result: 链式思考提示结合自然语言摘要的效果最好，Open Gpt4 8X7B的成功翻译率分别提高了13.8%和6.7%。

Conclusion: 中间表示特别是自然语言摘要结合链式思考提示，能显著提高大语言模型在代码翻译中的准确性。

Abstract: Studies show that large language models (LLMs) produce buggy code
translations. One avenue to improve translation accuracy is through
intermediate representations, which could provide structured insights to guide
the model's understanding. We explore whether code translation using LLMs can
benefit from intermediate representations via natural language (NL) and
abstract syntax trees (ASTs). Since prompt engineering greatly affects LLM
performance, we consider several ways to integrate these representations, from
one-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and
specialized StarCoder and CodeGen models on popular code translation benchmarks
(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs
best, with an increase of 13.8% and 6.7%, respectively, in successful
translations for the best-performing model (Open Gpt4 8X7B) compared to the
zero-shot prompt.

</details>


### [9] [LLMCup: Ranking-Enhanced Comment Updating with LLMs](https://arxiv.org/abs/2507.08671)
*Hua Ge,Juan Zhai,Minxue Pan,Fusen He,Ziyue Tan*

Main category: cs.SE

TL;DR: LLMCup框架利用大语言模型生成多种候选注释更新，并通过排名模型选择最佳结果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CUP和HebCup）在注释更新中常遗漏或误解关键信息，无法处理复杂场景。大语言模型在软件工程任务中表现优异，为其应用提供了潜力。

Method: 提出LLMCup框架，结合大语言模型的多提示策略生成候选注释，再通过CupRank模型选择最佳更新。

Result: 实验表明LLMCup在准确性等多项指标上优于基线方法49.0%-116.9%，部分情况下甚至超越人工更新。

Conclusion: LLMCup展示了利用大语言模型和排名模型优化注释更新的有效性，需进一步结合人工评估。

Abstract: While comments are essential for enhancing code readability and
maintainability in modern software projects, developers are often motivated to
update code but not comments, leading to outdated or inconsistent documentation
that hinders future understanding and maintenance. Recent approaches such as
CUP and HebCup have attempted automatic comment updating using neural
sequence-to-sequence models and heuristic rules, respectively. However, these
methods can miss or misinterpret crucial information during comment updating,
resulting in inaccurate comments, and they often struggle with complex update
scenarios. Given these challenges, a promising direction lies in leveraging
large language models (LLMs), which have shown impressive performance in
software engineering tasks such as comment generation, code synthesis, and
program repair. This suggests their strong potential to capture the logic
behind code modifications - an ability that is crucial for the task of comment
updating. Nevertheless, selecting an appropriate prompt strategy for an LLM on
each update case remains challenging. To address this, we propose a novel
comment updating framework, LLMCup, which first uses multiple prompt strategies
to provide diverse candidate updated comments via an LLM, and then employs a
ranking model, CupRank, to select the best candidate as final updated comment.
Experimental results demonstrate the effectiveness of LLMCup, with improvements
over state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,
10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in
SentenceBert similarity. Furthermore, a user study shows that comments updated
by LLMCup sometimes surpass human-written updates, highlighting the importance
of incorporating human evaluation in comment quality assessment.

</details>


### [10] [Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning](https://arxiv.org/abs/2507.08730)
*Zezhen Xiang,Jingzhi Gong,Tao Chen*

Main category: cs.SE

TL;DR: DHDA是一个在线配置性能学习框架，通过双重层次适应机制应对动态环境中的全局和局部概念漂移。


<details>
  <summary>Details</summary>
Motivation: 动态环境中，传统离线学习和迁移学习方法难以实时适应配置性能的变化，DHDA旨在解决这一问题。

Method: DHDA采用双重层次适应机制：上层重新划分数据以处理全局漂移，下层异步调整局部模型以应对局部漂移。

Result: 在8个软件系统中的评估表明，DHDA在准确性和适应漂移能力上优于现有方法，性能提升高达2倍。

Conclusion: DHDA能有效应对动态环境中的配置性能变化，并平衡响应性与效率。

Abstract: Modern configurable software systems need to learn models that correlate
configuration and performance. However, when the system operates in dynamic
environments, the workload variations, hardware changes, and system updates
will inevitably introduce concept drifts at different levels - global drifts,
which reshape the performance landscape of the entire configuration space; and
local drifts, which only affect certain sub-regions of that space. As such,
existing offline and transfer learning approaches can struggle to adapt to
these implicit and unpredictable changes in real-time, rendering configuration
performance learning challenging. To address this, we propose DHDA, an online
configuration performance learning framework designed to capture and adapt to
these drifts at different levels. The key idea is that DHDA adapts to both the
local and global drifts using dually hierarchical adaptation: at the upper
level, we redivide the data into different divisions, within each of which the
local model is retrained, to handle global drifts only when necessary. At the
lower level, the local models of the divisions can detect local drifts and
adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA
combines incremental updates with periodic full retraining to minimize
redundant computation when no drifts are detected. Through evaluating eight
software systems and against state-of-the-art approaches, we show that DHDA
achieves considerably better accuracy and can effectively adapt to drifts with
up to 2x improvements, while incurring reasonable overhead and is able to
improve different local models in handling concept drift.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [11] [Dependent Multiplicities in Dependent Linear Type Theory](https://arxiv.org/abs/2507.08759)
*Maximilian Doré*

Main category: cs.PL

TL;DR: 提出了一种新型依赖线性类型理论，支持变量的多重性依赖于其他变量，从而为高阶函数提供更精确的资源注释。


<details>
  <summary>Details</summary>
Motivation: 现有系统无法为许多高阶函数提供足够的类型注释，因此需要一种新的类型理论来解决这一问题。

Method: 通过将线性逻辑嵌入依赖类型理论，并指定其与宿主理论的交互方式，提出了一种支持依赖多重性的定量类型系统。

Result: 系统在Agda中实现，适用于任何依赖类型语言，并具有依赖类型理论和线性逻辑的标准模型组合语义。

Conclusion: 该理论为高阶函数的资源管理提供了更强大的工具，并展示了其实际可行性。

Abstract: We present a novel dependent linear type theory in which the multiplicity of
some variable - i.e., the number of times the variable can be used in a program
- can depend on other variables. This allows us to give precise resource
annotations to many higher-order functions that cannot be adequately typed in
any other system. Inspired by the Dialectica translation, our typing discipline
is obtained by embedding linear logic into dependent type theory and specifying
how the embedded logic interacts with the host theory. We can then use a
standard natural numbers type to obtain a quantitative typing system with
dependent multiplicities. We characterise the semantics for our theory as a
combination of standard models of dependent type theory and linear logic. Our
system can be added to any dependently typed language, which we demonstrate
with an implementation in Agda.

</details>


### [12] [Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists](https://arxiv.org/abs/2507.08796)
*Owen Lewis,Neil Ghani,Andrew Dudzik,Christos Perivolaropoulos,Razvan Pascanu,Petar Veličković*

Main category: cs.PL

TL;DR: 文章探讨了如何设计一个能够外推的函数，提出“滤波等变函数”作为一个新的语义类别，并研究了其性质、理论基础和几何解释，最终提出了一种完美的外推算法。


<details>
  <summary>Details</summary>
Motivation: 解决函数在外推时的行为问题，提出一种基于规则的方法来确保函数在输入变化时的可预测性。

Method: 引入“滤波等变函数”类别，研究其性质，证明其基本定理，并与已知的“映射等变函数”类别进行对比。同时，提供了该函数类别的几何解释。

Result: 提出“融合算法”，通过研究子列表行为来完美构造滤波等变函数的输出。

Conclusion: 滤波等变函数为外推提供了一种有效的框架，其理论性质和算法实现为未来研究提供了基础。

Abstract: What should a function that extrapolates beyond known input/output examples
look like? This is a tricky question to answer in general, as any function
matching the outputs on those examples can in principle be a correct
extrapolant. We argue that a "good" extrapolant should follow certain kinds of
rules, and here we study a particularly appealing criterion for rule-following
in list functions: that the function should behave predictably even when
certain elements are removed. In functional programming, a standard way to
express such removal operations is by using a filter function. Accordingly, our
paper introduces a new semantic class of functions -- the filter equivariant
functions. We show that this class contains interesting examples, prove some
basic theorems about it, and relate it to the well-known class of map
equivariant functions. We also present a geometric account of filter
equivariants, showing how they correspond naturally to certain simplicial
structures. Our highlight result is the amalgamation algorithm, which
constructs any filter-equivariant function's output by first studying how it
behaves on sublists of the input, in a way that extrapolates perfectly.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [13] [Photonic Rails in ML Datacenters](https://arxiv.org/abs/2507.08119)
*Eric Ding,Chuhan Ouyang,Rachee Singh*

Main category: cs.NI

TL;DR: 本文提出了一种利用光学电路开关重新设计数据中心网络架构的方法，以减少高基数电气开关的功耗和复杂性。


<details>
  <summary>Details</summary>
Motivation: 针对传统电气开关在高基数网络中的巨大功耗、成本和复杂性，探索光学开关的应用潜力。

Method: 提出并行驱动的轨道重构技术，设计控制平面Opus，通过时分复用光学开关模拟电气轨道开关。

Result: 实现了光学开关在数据中心网络中的高效应用，支持混合并行ML任务。

Conclusion: 本文为数据中心网络架构提出了新的研究方向，即网络与任务并行维度协同演化，而非预先配置。

Abstract: Rail-optimized network fabrics have become the de facto datacenter scale-out
fabric for large-scale ML training. However, the use of high-radix electrical
switches to provide all-to-all connectivity in rails imposes massive power,
cost, and complexity overheads. We propose a rethinking of the rail abstraction
by retaining its communication semantics, but realizing it using optical
circuit switches. The key challenge is that optical switches support only
one-to-one connectivity at a time, limiting the fan-out of traffic in ML
workloads using hybrid parallelisms. We introduce parallelism-driven rail
reconfiguration as a solution that leverages the sequential ordering between
traffic from different parallelisms. We design a control plane, Opus, to enable
time-multiplexed emulation of electrical rail switches using optical switches.
More broadly, our work discusses a new research agenda: datacenter fabrics that
co-evolve with the model parallelism dimensions within each job, as opposed to
the prevailing mindset of reconfiguring networks before a job begins.

</details>


### [14] [Rattan: An Extensible and Scalable Modular Internet Path Emulator](https://arxiv.org/abs/2507.08134)
*Minhu Wang,Yixin Shen,Bo Wang,Haixuan Tong,Yutong Xie,Yixuan Gao,Yan Liu,Li Chen,Mingwei Xu,Jianping Wu*

Main category: cs.NI

TL;DR: Rattan是一种可扩展且可伸缩的软件网络路径模拟器，采用模块化单元架构，支持高并发路径和集群实验，适用于现代互联网条件。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器在灵活性、可扩展性和易用性方面无法满足互联网路径的快速增长需求。

Method: 采用基于单元的架构，将模拟功能拆分为模块化单元，用户可通过层次化链接组合不同单元。

Result: 支持单机上的数百条千兆级并发路径和多机构成的集群实验。

Conclusion: Rattan为开发者和研究人员提供了一种高效、可靠的网络传输创新评估工具。

Abstract: The rapid growth of Internet paths in heterogeneity, scale, and dynamics has
made existing emulators increasingly insufficient in flexibility, scalability,
and usability. To address these limitations, we present Rattan, an extensible
and scalable software network path emulator for modern Internet conditions.
Rattan's core innovation lies in its cell-based architecture: by splitting
emulation functions into modular "cells" with well-documented asynchronous
interfaces, users are allowed to easily compose different cells by
hierarchically linking them and easily construct new cells by using standard
cell interfaces. This design enables: (1) scalability, supporting hundreds of
concurrent gigabit-level paths on a single machine and cluster-level
experiments composed of multiple machines; (2) extensibility, simulating new
network conditions by constructing new cells. Rattan empowers developers and
researchers to efficiently and confidently evaluate, validate, and diagnose
diverse network transport innovations for online services.

</details>


### [15] [KP-A: A Unified Network Knowledge Plane for Catalyzing Agentic Network Intelligence](https://arxiv.org/abs/2507.08164)
*Yun Tang,Mengbang Zou,Zeinab Nezami,Syed Ali Raza Zaidi,Weisi Guo*

Main category: cs.NI

TL;DR: 论文提出了KP-A，一个统一的网络知识平面，旨在优化自主6G网络中的知识管理和代理智能任务，减少冗余数据流和不一致性。


<details>
  <summary>Details</summary>
Motivation: 当前6G网络中独立的智能任务实现导致知识检索冗余和解释不一致，缺乏统一的管理框架。

Method: 提出KP-A框架，将网络知识获取与管理与智能逻辑分离，提供一致的知识接口，支持代理智能。

Result: KP-A成功应用于实时网络知识问答和边缘AI服务编排，开源实现支持可复现性和未来标准化。

Conclusion: KP-A通过统一知识管理简化了6G代理智能的开发与维护，提升了互操作性。

Abstract: The emergence of large language models (LLMs) and agentic systems is enabling
autonomous 6G networks with advanced intelligence, including
self-configuration, self-optimization, and self-healing. However, the current
implementation of individual intelligence tasks necessitates isolated knowledge
retrieval pipelines, resulting in redundant data flows and inconsistent
interpretations. Inspired by the service model unification effort in Open-RAN
(to support interoperability and vendor diversity), we propose KP-A: a unified
Network Knowledge Plane specifically designed for Agentic network intelligence.
By decoupling network knowledge acquisition and management from intelligence
logic, KP-A streamlines development and reduces maintenance complexity for
intelligence engineers. By offering an intuitive and consistent knowledge
interface, KP-A also enhances interoperability for the network intelligence
agents. We demonstrate KP-A in two representative intelligence tasks: live
network knowledge Q&A and edge AI service orchestration. All implementation
artifacts have been open-sourced to support reproducibility and future
standardization efforts.

</details>


### [16] [Towards AI-Native RAN: An Operator's Perspective of 6G Day 1 Standardization](https://arxiv.org/abs/2507.08403)
*Nan Li,Qi Sun,Lehan Wang,Xiaofei Xu,Jinri Huang,Chunhui Liu,Jing Gao,Yuhong Huang,Chih-Lin I*

Main category: cs.NI

TL;DR: 论文探讨了6G网络中原生AI的设计与标准化原则，重点提出了AI-Native RAN的三大能力，并通过5G-A基站的大规模实地试验验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 5G网络中的AI/ML仅为附加功能，而6G需要从设计之初就原生集成AI，以应对复杂性和支持多样化AI应用。

Method: 提出AI-Native RAN的架构与三大核心能力（AI驱动的RAN优化、可靠的AI生命周期管理、AI即服务），并通过大规模试验验证。

Result: 试验中，提出的架构显著降低了空口延迟、提高了根因识别效率，并降低了网络能耗。

Conclusion: 论文为6G AI-Native RAN的标准化提供了初步框架，平衡了技术创新与实际部署需求。

Abstract: Artificial Intelligence/Machine Learning (AI/ML) has become the most certain
and prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not
natively integrated but rather an add-on feature over existing architecture, 6G
shall incorporate AI from the onset to address its complexity and support
ubiquitous AI applications. Based on our extensive mobile network operation and
standardization experience from 2G to 5G, this paper explores the design and
standardization principles of AI-Native radio access networks (RAN) for 6G,
with a particular focus on its critical Day 1 architecture, functionalities and
capabilities. We investigate the framework of AI-Native RAN and present its
three essential capabilities to shed some light on the standardization
direction; namely, AI-driven RAN processing/optimization/automation, reliable
AI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The
standardization of AI-Native RAN, in particular the Day 1 features, including
an AI-Native 6G RAN architecture, were proposed. For validation, a large-scale
field trial with over 5000 5G-A base stations have been built and delivered
significant improvements in average air interface latency, root cause
identification, and network energy consumption with the proposed architecture
and the supporting AI functions. This paper aims to provide a Day 1 framework
for 6G AI-Native RAN standardization design, balancing technical innovation
with practical deployment.

</details>


### [17] [Age of Information Optimization in Laser-charged UAV-assisted IoT Networks: A Multi-agent Deep Reinforcement Learning Method](https://arxiv.org/abs/2507.08429)
*Geng Sun,Likun Zhang,Jiahui Li,Jing Wu,Jiacheng Wang,Zemin Sun,Changyuan Zhao,Victor C. M. Leung*

Main category: cs.NI

TL;DR: 本文研究了激光束充电无人机辅助物联网网络中的信息时效性优化问题，提出了一种多智能体近端策略优化框架MAPPO-TM，显著降低了峰值信息时效性。


<details>
  <summary>Details</summary>
Motivation: 无人机的有限能量限制了其在物联网数据收集中的应用，激光束充电技术为解决这一问题提供了可能，但需要优化信息时效性和充电策略。

Method: 提出MAPPO-TM框架，结合时间记忆机制和多智能体协调，优化无人机轨迹和激光充电策略。

Result: 仿真结果显示，MAPPO-TM相比传统方法降低了15.1%的峰值信息时效性，并提高了能源效率。

Conclusion: MAPPO-TM框架在无人机辅助物联网网络中有效优化了信息时效性和能源效率，为未来研究提供了新思路。

Abstract: The integration of unmanned aerial vehicles (UAVs) with Internet of Things
(IoT) networks offers promising solutions for efficient data collection.
However, the limited energy capacity of UAVs remains a significant challenge.
In this case, laser beam directors (LBDs) have emerged as an effective
technology for wireless charging of UAVs during operation, thereby enabling
sustained data collection without frequent returns to charging stations (CSs).
In this work, we investigate the age of information (AoI) optimization in
LBD-powered UAV-assisted IoT networks, where multiple UAVs collect data from
distributed IoTs while being recharged by laser beams. We formulate a joint
optimization problem that aims to minimize the peak AoI while determining
optimal UAV trajectories and laser charging strategies. This problem is
particularly challenging due to its non-convex nature, complex temporal
dependencies, and the need to balance data collection efficiency with energy
consumption constraints. To address these challenges, we propose a novel
multi-agent proximal policy optimization with temporal memory and multi-agent
coordination (MAPPO-TM) framework. Specifically, MAPPO-TM incorporates temporal
memory mechanisms to capture the dynamic nature of UAV operations and
facilitates effective coordination among multiple UAVs through decentralized
learning while considering global system objectives. Simulation results
demonstrate that the proposed MAPPO-TM algorithm outperforms conventional
approaches in terms of peak AoI minimization and energy efficiency. Ideally,
the proposed algorithm achieves up to 15.1% reduction in peak AoI compared to
conventional multi-agent deep reinforcement learning (MADRL) methods.

</details>


### [18] [Recovery of UAV Swarm-enabled Collaborative Beamforming in Low-altitude Wireless Networks under Wind Field Disturbances](https://arxiv.org/abs/2507.08507)
*Geng Sun,Chenbang Liu,Jiahui Li,Guannan Qu,Shuang Liang,Jiacheng Wang,Changyuan Zhao,Dusit Niyato*

Main category: cs.NI

TL;DR: 论文研究了无人机群在风场干扰下保持协作波束成形性能的挑战，提出了一种基于PPO-LA算法的实时优化框架，显著提高了通信性能。


<details>
  <summary>Details</summary>
Motivation: 无人机群在低空无线网络中通过协作波束成形形成虚拟天线阵列，具有通信范围广、能效高和信号定向性强的优势。然而，风场干扰会破坏波束模式，影响传输可靠性，亟需解决。

Method: 提出了一种综合框架，模拟三种风况对无人机阵列的影响，并通过自适应激励电流权重调整优化问题。采用基于LSTM和自适应学习率的近端策略优化算法（PPO-LA），实时适应风场干扰，无需针对特定风况预先训练。

Result: 仿真结果表明，PPO-LA算法能有效恢复风场干扰下协作波束成形的性能，且显著优于基准算法。

Conclusion: PPO-LA算法能够解决风场干扰下的实时优化问题，为无人机群的通信可靠性提供了有效解决方案。

Abstract: Unmanned aerial vehicle (UAV) swarms utilizing collaborative beamforming (CB)
in low-altitude wireless networks (LAWN) demonstrate significant potential for
enhanced communication range, energy efficiency, and signal directivity through
the formation of virtual antenna arrays (VAA). However, environmental
disturbances, particularly wind fields, significantly degrade CB performance by
introducing positional errors that disrupt beam patterns, thereby compromising
transmission reliability. This paper investigates the critical challenge of
maintaining CB performance in UAV-based VAAs operating in LAWN under wind field
disturbances. We propose a comprehensive framework that models the impact of
three distinct wind conditions (constant, shear, and turbulent) on UAV array
performance, and formulate a long-term real-time optimization problem to
maximize directivity while minimizing maximum sidelobe levels through adaptive
excitation current weight adjustments. To address the inherent complexity of
this problem, we propose a novel proximal policy optimization algorithm with
long short-term memory (LSTM) structure and adaptive learning rate (PPO-LA),
which effectively captures temporal patterns in wind field disturbances and
enables real-time adaptation without requiring extensive prior training for
specific wind conditions. Our simulation results demonstrate that the proposed
PPO-LA algorithm successfully recovers degraded CB performance across various
wind scenarios, and thus significantly outperforming benchmark algorithms.

</details>


### [19] [Stabilizing and Optimizing Inter-Shell Routing in LEO Networks with Integrated Routing Cost](https://arxiv.org/abs/2507.08549)
*Yaojia Wang,Qi Zhang,Kun Qiu,Yue Gao*

Main category: cs.NI

TL;DR: 论文提出DP-IRC算法，用于解决低地球轨道巨型星座网络中跨壳路由的不稳定问题，平衡跳数和链路切换成本。


<details>
  <summary>Details</summary>
Motivation: 现有策略（如最小跳路径和自适应路径路由）忽略了链路切换成本或陷入局部最优，导致跨壳路由不稳定。

Method: 通过动态规划将多壳路径视为多阶段决策问题，结合跳数和链路稳定性设计综合路由成本（IRC）指标。

Result: 实验表明，DP-IRC比最小跳路径和自适应路径路由分别减少39.1%和22.0%的链路切换率，同时保持接近最优的端到端距离。

Conclusion: DP-IRC在跨壳路由优化中显著提升稳定性，兼顾跳数和链路切换效率。

Abstract: The low Earth orbit (LEO) mega-constellation network (LMCN), which uses
thousands of satellites across multi-shell architectures to deliver different
services, is facing challenges in inter-shell routing stability due to dynamic
network topologies and frequent inter-satellite link (ISL) switching. Existing
strategies, such as the Minimum Hop Path set, prioritize minimizing hop counts
to reduce latency, but ignore ISL switching costs, which leads to high
instability. To overcome this, the Adaptive Path Routing Scheme introduces path
similarity thresholds to reduce the ISL switching frequency between shells.
However, the greedy approach of Adaptive Path Routing Scheme is often trapped
in local optima, sacrificing inter-shell path distance efficiency. To address
these limitations, we propose the Dynamic Programming-based Integrated Routing
Cost (DP-IRC) algorithm, which is designed explicitly for inter-shell routing
optimization. By formulating multi-shell paths as a multistage decision
problem, DP-IRC balances hop counts and ISL stability through an Integrated
Routing Cost (IRC) metric, combining inter-/intra-shell hops and switching
costs. Experiments over 60 time slots with real-world Starlink and OneWeb
configurations show that DP-IRC reduces inter-shell ISL switching rates by
39.1% and 22.0% compared to the Minimum Hop Path set strategy and Adaptive Path
Routing Scheme, respectively, while still maintaining near-optimal end-to-end
distances.

</details>


### [20] [Qualitative Assessment of Low Power Wide Area Network Protocols and their Security Aspect](https://arxiv.org/abs/2507.08677)
*Wesley dos Reis Bezerra,Lais Machado Bezerra,Carlos Becker Westphal*

Main category: cs.NI

TL;DR: 该论文分析了低功耗广域网（LPWAN）协议的定性特征，探讨了在长寿命电池驱动的稀疏网络中使用受限设备的挑战与机遇，重点研究了LoRaWAN、NB-IoT和Sigfox三种协议。


<details>
  <summary>Details</summary>
Motivation: 物联网中的通信选项繁多，尤其在受限和电池驱动设备领域，理解每种选项的差异和特点对专业人士和研究人员来说是一大挑战。

Method: 通过文献调研，分析了三种LPWAN协议（LoRaWAN、NB-IoT和Sigfox），并详细研究了LoRaWAN协议。

Result: 讨论了所选网络协议及其在稀疏传感器物联网解决方案中的应用。

Conclusion: 研究为稀疏传感器网络的低功耗通信提供了参考，有助于选择合适的LPWAN协议。

Abstract: There are currently many communication options in the Internet of Things,
even in particular areas such as constrained and battery-powered devices, such
as Low Power Wide Area Networks. Understanding the differences and
characteristics of each option is a challenge, even for professionals and
researchers in the field. To meet this need, this work analyses the qualitative
characteristics of Low Power Wide Area Network protocols and the challenges and
opportunities of using constrained devices for sparse networks based on
long-life batteries. For this study, a bibliographic survey of the literature
was carried out as an analysis of three protocols (LoRaWAN, NB-IoT, and
Sigfox), and a detailing of the first one. As a result, there is a discussion
about the chosen network protocol and its use in IoT solutions with sparse
sensors.

</details>


### [21] [Knowledge Graph-Based approach for Sustainable 6G End-to-End System Design](https://arxiv.org/abs/2507.08717)
*Akshay Jain,Sylvaine Kerboeuf,Sokratis Barmpounakis,Cristóbal Vinagre Z.,Stefan Wendt,Dinh Thai Bui,Pol Alemany,Riccardo Nicolicchia,José María Jorquera Valero,Dani Korpi,Mohammad Hossein Moghaddam,Mikko A. Uusitalo,Patrik Rugeland,Abdelkader Outtagarts,Karthik Upadhya,Panagiotis Demestichas,Raul Muñoz,Manuel Gil Pérez,Daniel Adanza,Ricard Vilalta*

Main category: cs.NI

TL;DR: 该论文介绍了一种基于知识图谱（KG）的6G端到端（E2E）系统设计方法，结合性能指标和可持续发展目标，适用于复杂的设计空间。


<details>
  <summary>Details</summary>
Motivation: 现有6G设计需同时满足性能指标（KPI）和可持续发展目标（如社会可持续性），但现有文献在技术实现与可持续性指标关联方面存在空白。

Method: 提出基于知识图谱的方法，输入包括用例KPI、可持续性需求（KV和KVI）、技术使能能力、6G设计原则、使能成熟度及其依赖关系，并开发新方法评估使能对KV的贡献。

Result: 在Hexa-X-II项目中的移动机器人用例中应用该方法，筛选了82个使能，并通过概念验证证明了其有效性。

Conclusion: KG方法为设计可持续6G系统提供有效路径，其应用展示了技术与可持续性目标的结合潜力。

Abstract: Previous generations of cellular communication, such as 5G, have been
designed with the objective of improving key performance indicators (KPIs) such
as throughput, latency, etc. However, to meet the evolving KPI demands as well
as the ambitious sustainability targets for the ICT industry, 6G will need to
be designed differently. Concretely, 6G will need to consider both the
performance and sustainability targets for the various use cases it will serve.
Moreover, like previous generations, 6G will have various candidate
technological enablers, making the design space of the system even more
complex. Furthermore, given the subjective nature of the sustainability
indicators, in particular social sustainability, there is a significant gap in
literature on how technical enablers and 6G System design can be linked to
them. Hence, in this article a novel method for 6G end-to-end (E2E) system
design based on Knowledge graphs (KG) has been introduced. It considers as its
input: the use case KPIs, use case sustainability requirements expressed as Key
Values (KV) and KV Indicators (KVIs), the ability of the technological enablers
to satisfy these KPIs and KVIs, the 6G system design principles defined in
Hexa-X-II project, the maturity of a technological enabler and the dependencies
between the various enablers. As part of the KG method, a novel approach for
determining the key values a technological enabler addresses, has also been
introduced. The effectiveness of the KG method was demonstrated by its
application in designing the 6G E2E system for the cooperating mobile robot use
case defined in the Hexa-X-II project, where 82 enablers were selected. Lastly,
results from proof-of-concept demonstrations for a subset of the selected
enablers have also been provided, which reinforce the efficacy of the KG method
for designing a sustainable 6G system.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [PUMA: Layer-Pruned Language Model for Efficient Unified Multimodal Retrieval with Modality-Adaptive Learning](https://arxiv.org/abs/2507.08064)
*Yibo Lyu,Rui Shao,Gongwei Chen,Yijie Zhu,Weili Guan,Liqiang Nie*

Main category: cs.MM

TL;DR: 提出了PUMA方法，通过层剪枝和模态自适应学习优化统一多模态检索，降低资源消耗并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的统一多模态检索方法参数庞大，导致训练成本高且推理效率低。

Method: 提出层剪枝自蒸馏（减少参数）和模态自适应对比学习损失（提升学习效率）。

Result: 实验表明该方法显著减少资源使用，同时保持强性能。

Conclusion: PUMA通过结构和学习改进，为高效统一多模态检索提供了可行方案。

Abstract: As multimedia content expands, the demand for unified multimodal retrieval
(UMR) in real-world applications increases. Recent work leverages multimodal
large language models (MLLMs) to tackle this task. However, their large
parameter size results in high training costs and low inference efficiency. To
address this, we propose PUMA: a Layer-Pruned Language Model for Efficient
Unified Multimodal Retrieval with Modality-Adaptive Learning. Our approach
improves UMR from both structural and learning perspectives. (1) Structurally,
we propose Layer-Pruned Self-Distillation, which prunes MLLMs by keeping only
shallow layers while distilling features from dropped deep layers as teacher
signals. This reduces parameters and preserves representation capability. (2)
On the learning side, we introduce Modality-Adaptive Contrastive Learning Loss
(MAC-Loss), which separates in-batch negatives into harder intra-modality and
easier inter-modality groups based on the target modality, assigning different
temperature strategies to enhance learning efficiency. Experiments show our
method significantly reduces resource usage while maintaining strong
performance.

</details>


### [23] [VideoConviction: A Multimodal Benchmark for Human Conviction and Stock Market Recommendations](https://arxiv.org/abs/2507.08104)
*Michael Galarnyk,Veer Kejriwal,Agam Shah,Yash Bhardwaj,Nicholas Meyer,Anand Krishnan,Sudheer Chava*

Main category: cs.MM

TL;DR: 论文研究了金融网红（finfluencers）在社交媒体上的影响，提出了多模态数据集VideoConviction，发现多模态输入提升股票代码提取，但模型难以区分投资行为和信念强度。


<details>
  <summary>Details</summary>
Motivation: 研究金融网红的多模态信号（如语气、表情）对金融话语的影响，填补文本分析与多模态分析之间的差距。

Method: 构建VideoConviction数据集（6000+专家标注），对比多模态大模型（MLLMs）与基于文本的大模型（LLMs）。

Result: 多模态输入提升股票代码提取，但模型难以识别信念强度；反向策略年回报率高于S&P 500但风险更大。

Conclusion: 多模态数据集推动了金融研究的进展，但需改进模型对信念强度的理解。

Abstract: Social media has amplified the reach of financial influencers known as
"finfluencers," who share stock recommendations on platforms like YouTube.
Understanding their influence requires analyzing multimodal signals like tone,
delivery style, and facial expressions, which extend beyond text-based
financial analysis. We introduce VideoConviction, a multimodal dataset with
6,000+ expert annotations, produced through 457 hours of human effort, to
benchmark multimodal large language models (MLLMs) and text-based large
language models (LLMs) in financial discourse. Our results show that while
multimodal inputs improve stock ticker extraction (e.g., extracting Apple's
ticker AAPL), both MLLMs and LLMs struggle to distinguish investment actions
and conviction--the strength of belief conveyed through confident delivery and
detailed reasoning--often misclassifying general commentary as definitive
recommendations. While high-conviction recommendations perform better than
low-conviction ones, they still underperform the popular S\&P 500 index fund.
An inverse strategy--betting against finfluencer recommendations--outperforms
the S\&P 500 by 6.8\% in annual returns but carries greater risk (Sharpe ratio
of 0.41 vs. 0.65). Our benchmark enables a diverse evaluation of multimodal
tasks, comparing model performance on both full video and segmented video
inputs. This enables deeper advancements in multimodal financial research. Our
code, dataset, and evaluation leaderboard are available under the CC BY-NC 4.0
license.

</details>


### [24] [Visual Semantic Description Generation with MLLMs for Image-Text Matching](https://arxiv.org/abs/2507.08590)
*Junyu Chen,Yihua Gao,Mingyong Li*

Main category: cs.MM

TL;DR: 本文提出了一种新颖的图像-文本匹配框架，利用多模态大语言模型（MLLMs）作为视觉语义解析器，通过生成丰富的视觉语义描述（VSD）来弥合跨模态差距，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 图像和文本模态在表示上存在本质差异（连续高维图像特征与离散结构化文本），传统的图像-文本匹配方法难以有效对齐它们。

Method: 提出了一种结合实例级和原型级对齐的框架：1）通过视觉特征与VSD融合增强图像表示的语义表达能力；2）通过VSD聚类实现类别级对齐。

Result: 在Flickr30K和MSCOCO数据集上实验表明性能显著提升，并在跨域任务（如新闻和遥感图像）中表现出优秀的零样本泛化能力。

Conclusion: 该框架通过MLLMs提供的语义锚点有效解决了跨模态对齐问题，为图像-文本匹配领域提供了新的研究方向和工具。

Abstract: Image-text matching (ITM) aims to address the fundamental challenge of
aligning visual and textual modalities, which inherently differ in their
representations, continuous, high-dimensional image features vs. discrete,
structured text. We propose a novel framework that bridges the modality gap by
leveraging multimodal large language models (MLLMs) as visual semantic parsers.
By generating rich Visual Semantic Descriptions (VSD), MLLMs provide semantic
anchor that facilitate cross-modal alignment. Our approach combines: (1)
Instance-level alignment by fusing visual features with VSD to enhance the
linguistic expressiveness of image representations, and (2) Prototype-level
alignment through VSD clustering to ensure category-level consistency. These
modules can be seamlessly integrated into existing ITM models. Extensive
experiments on Flickr30K and MSCOCO demonstrate substantial performance
improvements. The approach also exhibits remarkable zero-shot generalization to
cross-domain tasks, including news and remote sensing ITM. The code and model
checkpoints are available at https://github.com/Image-Text-Matching/VSD.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [25] [Heterogeneous Dynamic Logic: Provability Modulo Program Theories](https://arxiv.org/abs/2507.08581)
*Samuel Teuber,Mattias Ulbrich,André Platzer,Bernhard Beckert*

Main category: cs.LO

TL;DR: 提出了一种异构动态逻辑（HDL）框架，用于模块化和组合式地结合不同编程逻辑的推理原则，解决多语言系统的形式化验证问题。


<details>
  <summary>Details</summary>
Motivation: 多编程语言系统的形式化规范和验证具有挑战性，需要一种灵活的方法来组合不同的动态逻辑。

Method: HDL框架通过动态理论的提升和组合操作，支持跨语言推理，并在Isabelle中实现了形式化和证明。

Result: 证明了提升和组合操作的完备性，并在一个汽车案例中验证了HDL的实用性。

Conclusion: HDL为异构系统的验证提供了一种模块化和组合化的解决方案。

Abstract: Formally specifying, let alone verifying, properties of systems involving
multiple programming languages is inherently challenging. We introduce
Heterogeneous Dynamic Logic (HDL), a framework for combining reasoning
principles from distinct (dynamic) program logics in a modular and
compositional way. HDL mirrors the architecture of satisfiability modulo
theories (SMT): Individual dynamic logics, along with their calculi, are
treated as dynamic theories that can be flexibly combined to reason about
heterogeneous systems whose components are verified using different program
logics. HDL provides two key operations: Lifting extends an individual dynamic
theory with new program constructs (e.g., the havoc operation or regular
programs) and automatically augments its calculus with sound reasoning
principles for the new constructs; and Combination enables cross-language
reasoning in a single modality via Heterogeneous Dynamic Theories, facilitating
the reuse of existing proof infrastructure. We formalize dynamic theories,
their lifting and combination in Isabelle, and prove the soundness of all proof
rules. We also prove relative completeness theorems for lifting and
combination: Under common assumptions, reasoning about lifted or combined
theories is no harder than reasoning about the constituent dynamic theories and
their common first-order structure (i.e., the "data theory"). We demonstrate
HDL's utility by verifying an automotive case study in which a Java controller
(formalized in Java dynamic logic) steers a plant model (formalized in
differential dynamic logic).

</details>


### [26] [A Personalised Formal Verification Framework for Monitoring Activities of Daily Living of Older Adults Living Independently in Their Homes](https://arxiv.org/abs/2507.08701)
*Ricardo Contreras,Filip Smola,Nuša Farič,Jiawei Zheng,Jane Hillston,Jacques D. Fleuriot*

Main category: cs.LO

TL;DR: 提出了一个框架，用于建模和验证独立生活老年人的日常活动，结合传感器数据和上下文信息，通过形式化模型和逻辑验证提升其安全性。


<details>
  <summary>Details</summary>
Motivation: 为独立生活的老年人提供个性化的生活质量解决方案，考虑其偏好和上下文。

Method: 整合传感器数据、半结构化访谈、家庭布局和社会学观察，构建个性化形式化模型，使用线性时序逻辑验证要求。

Result: 框架展示了通用性，能够识别并解释模型中的违规行为，提升老年人的安全和福祉。

Conclusion: 该框架具有潜力，能有效支持独立生活的老年人，为其提供安全和福祉的保障。

Abstract: There is an imperative need to provide quality of life to a growing
population of older adults living independently. Personalised solutions that
focus on the person and take into consideration their preferences and context
are key. In this work, we introduce a framework for representing and reasoning
about the Activities of Daily Living of older adults living independently at
home. The framework integrates data from sensors and contextual information
that aggregates semi-structured interviews, home layouts and sociological
observations from the participants. We use these data to create formal models,
personalised for each participant according to their preferences and context.
We formulate requirements that are specific to each individual as properties
encoded in Linear Temporal Logic and use a model checker to verify whether each
property is satisfied by the model. When a property is violated, a
counterexample is generated giving the cause of the violation. We demonstrate
the framework's generalisability by applying it to different participants,
highlighting its potential to enhance the safety and well-being of older adults
ageing in place.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [27] [Human vs. LLM-Based Thematic Analysis for Digital Mental Health Research: Proof-of-Concept Comparative Study](https://arxiv.org/abs/2507.08002)
*Karisa Parkington,Bazen G. Teferra,Marianne Rouleau-Tang,Argyrios Perivolaris,Alice Rueda,Adam Dubrowski,Bill Kapralos,Reza Samavi,Andrew Greenshaw,Yanbo Zhang,Bo Cao,Yuqi Wu,Sirisha Rambhatla,Sridhar Krishnan,Venkat Bhat*

Main category: cs.HC

TL;DR: 本研究比较了使用大型语言模型（LLM）与传统人工方法在心理健康访谈中的主题分析效果，发现LLM在成本效益上更优，但缺乏人工分析的深度。


<details>
  <summary>Details</summary>
Motivation: 主题分析虽然能提供有价值的洞察，但在大规模医疗研究中资源消耗大。LLM能够自动分析文本并识别关键内容，为解决这一问题提供了潜力，但其在心理健康访谈中的应用需要与传统方法进行比较。

Method: 研究使用了OpenAI的GPT-4o模型和RISEN提示工程框架，与传统人工分析（Dedoose）进行比较。方法包括代码开发、饱和点记录、代码应用到摘录和主题合成。

Result: LLM的RISEN框架在演绎性父代码开发上与人类相似，但人类在归纳性子代码和主题合成上表现更优。知识基础的LLM在更少的转录本中（10-15）达到编码饱和，而人类需要更多（90-99）。

Conclusion: LLM在主题分析中更具成本效益，但需结合人类监督以平衡参与者的视角和研究资源，这可能在心理健康和临床研究中带来变革。

Abstract: Thematic analysis provides valuable insights into participants' experiences
through coding and theme development, but its resource-intensive nature limits
its use in large healthcare studies. Large language models (LLMs) can analyze
text at scale and identify key content automatically, potentially addressing
these challenges. However, their application in mental health interviews needs
comparison with traditional human analysis. This study evaluates out-of-the-box
and knowledge-base LLM-based thematic analysis against traditional methods
using transcripts from a stress-reduction trial with healthcare workers.
OpenAI's GPT-4o model was used along with the Role, Instructions, Steps,
End-Goal, Narrowing (RISEN) prompt engineering framework and compared to human
analysis in Dedoose. Each approach developed codes, noted saturation points,
applied codes to excerpts for a subset of participants (n = 20), and
synthesized data into themes. Outputs and performance metrics were compared
directly. LLMs using the RISEN framework developed deductive parent codes
similar to human codes, but humans excelled in inductive child code development
and theme synthesis. Knowledge-based LLMs reached coding saturation with fewer
transcripts (10-15) than the out-of-the-box model (15-20) and humans (90-99).
The out-of-the-box LLM identified a comparable number of excerpts to human
researchers, showing strong inter-rater reliability (K = 0.84), though the
knowledge-based LLM produced fewer excerpts. Human excerpts were longer and
involved multiple codes per excerpt, while LLMs typically applied one code.
Overall, LLM-based thematic analysis proved more cost-effective but lacked the
depth of human analysis. LLMs can transform qualitative analysis in mental
healthcare and clinical research when combined with human oversight to balance
participant perspectives and research resources.

</details>


### [28] [A Versatile Dataset of Mouse and Eye Movements on Search Engine Results Pages](https://arxiv.org/abs/2507.08003)
*Kayhan Latifzadeh,Jacek Gwizdka,Luis A. Leiva*

Main category: cs.HC

TL;DR: 提出了一个综合数据集，用于研究用户在搜索引擎结果页（SERPs）上的注意力和购买行为，通过眼动追踪解决了以往依赖鼠标移动和自我报告数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决以往研究依赖鼠标移动和自我报告数据的问题，提供更客观的视觉注意力数据。

Method: 使用眼动追踪技术收集数据，构建了一个包含2,776个查询的数据集，涵盖HTML源码、截图、眼动和鼠标数据等。

Result: 提供了详细的数据集和预处理脚本，并通过基线实验展示了未来研究的可能性。

Conclusion: 该数据集为研究用户行为提供了更可靠的基础，并激发了未来的研究方向。

Abstract: We contribute a comprehensive dataset to study user attention and purchasing
behavior on Search Engine Result Pages (SERPs). Previous work has relied on
mouse movements as a low-cost large-scale behavioral proxy but also has relied
on self-reported ground-truth labels, collected at post-task, which can be
inaccurate and prone to biases. To address this limitation, we use an eye
tracker to construct an objective ground-truth of continuous visual attention.
Our dataset comprises 2,776 transactional queries on Google SERPs, collected
from 47 participants, and includes: (1) HTML source files, with CSS and images;
(2) rendered SERP screenshots; (3) eye movement data; (4) mouse movement data;
(5) bounding boxes of direct display and organic advertisements; and (6)
scripts for further preprocessing the data. In this paper we provide an
overview of the dataset and baseline experiments (classification tasks) that
can inspire researchers about the different possibilities for future work.

</details>


### [29] [SSSUMO: Real-Time Semi-Supervised Submovement Decomposition](https://arxiv.org/abs/2507.08028)
*Evgenii Rudakov,Jonathan Shock,Otto Lappi,Benjamin Ultan Cowley*

Main category: cs.HC

TL;DR: 本文提出了一种半监督深度学习方法SSSUMO，用于子运动分解，实现了最先进的精度和速度。


<details>
  <summary>Details</summary>
Motivation: 现有的子运动分析方法在重建精度、计算成本和验证方面存在困难，主要由于手工标记数据难以获取。

Method: 采用半监督学习框架，通过合成数据（基于最小抖动原理生成）和未标记的人类运动数据进行迭代优化，使用全卷积架构和可微分重建。

Result: 在合成和多样化人类运动数据集上显著超越现有方法，即使在高噪声条件下也表现出鲁棒性，且能实时运行（每输入秒少于毫秒）。

Conclusion: 该方法为人类-计算机交互、康复医学和运动控制研究提供了新应用，尤其在传统方法难以处理的数据集上表现突出。

Abstract: This paper introduces a SSSUMO, semi-supervised deep learning approach for
submovement decomposition that achieves state-of-the-art accuracy and speed.
While submovement analysis offers valuable insights into motor control,
existing methods struggle with reconstruction accuracy, computational cost, and
validation, due to the difficulty of obtaining hand-labeled data. We address
these challenges using a semi-supervised learning framework. This framework
learns from synthetic data, initially generated from minimum-jerk principles
and then iteratively refined through adaptation to unlabeled human movement
data. Our fully convolutional architecture with differentiable reconstruction
significantly surpasses existing methods on both synthetic and diverse human
motion datasets, demonstrating robustness even in high-noise conditions.
Crucially, the model operates in real-time (less than a millisecond per input
second), a substantial improvement over optimization-based techniques. This
enhanced performance facilitates new applications in human-computer
interaction, rehabilitation medicine, and motor control studies. We demonstrate
the model's effectiveness across diverse human-performed tasks such as
steering, rotation, pointing, object moving, handwriting, and mouse-controlled
gaming, showing notable improvements particularly on challenging datasets where
traditional methods largely fail. Training and benchmarking source code, along
with pre-trained model weights, are made publicly available at
https://github.com/dolphin-in-a-coma/sssumo.

</details>


### [30] [Pushing the Boundaries of Immersion and Storytelling: A Technical Review of Unreal Engine](https://arxiv.org/abs/2507.08142)
*Oleksandra Sobchyshak,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: 本文深入分析了虚幻引擎在沉浸式叙事和虚拟现实中的技术应用，探讨了其在多个领域的影响力、创新性及面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究虚幻引擎如何通过超逼真环境和情感叙事技术推动VR和互动媒体的发展。

Method: 通过技术审查和案例分析，评估虚幻引擎的功能及其在各行业的应用。

Result: 虚幻引擎在视觉、音频和互动性方面的表现出色，但面临硬件需求高和伦理问题等挑战。

Conclusion: 虚幻引擎在创新和跨学科合作中极具潜力，需解决可访问性和伦理问题以实现更广泛应用。

Abstract: Unreal Engine is a platform that has influenced immersive storytelling and
virtual reality (VR) through its advanced features and diverse applications.
This paper provides an in-depth technical review of Unreal Engine. It analyzes
its key innovations in creating hyper-realistic environments and emotionally
engaging narratives, with significant applications in gaming, virtual
production, education, cultural preservation, and healthcare. The findings of
this article highlight Unreal Engine's transformative impact across industries,
demonstrating its ability to merge storytelling with cutting-edge technologies.
Case studies illustrate how Unreal Engine facilitates seamless visuals, audio,
and interactivity integration to create compelling experiences. Additionally,
this study identifies Unreal Engine's versatility in applications ranging from
procedural content generation and AI-driven workflows to smart city simulations
and VR-based rehabilitation programs.
  While Unreal Engine sets new benchmarks for visual fidelity and
interactivity, this paper underscores critical challenges, including its high
hardware demands, limited accessibility, and ethical concerns related to
over-immersion and data privacy. Addressing these challenges through
cloud-based rendering, inclusive design, and ethical practices is essential for
broader adoption and sustainability. This review concludes that Unreal Engine
is suitable for innovation and interdisciplinary collaboration. Its ability to
empower creators, redefine workflows, and push the boundaries of immersive
storytelling positions Unreal Engine as pivotal in shaping the future of
virtual reality and interactive media.

</details>


### [31] [Emotion Detection in Older Adults Using Physiological Signals from Wearable Sensors](https://arxiv.org/abs/2507.08167)
*Md. Saif Hassan Onim,Andrew M. Kiselica,Himanshu Thapliyal*

Main category: cs.HC

TL;DR: 本文研究了一种基于边缘计算、非侵入性的情绪识别方法，仅使用穿戴传感器获取的生理信号来检测老年人的情绪状态。


<details>
  <summary>Details</summary>
Motivation: 老年人的情绪检测对其认知和情感健康至关重要，尤其在医院和辅助生活环境中。传统的摄像头或面部表情分析方法可能具有侵入性，本研究旨在探索仅需生理信号的隐私保护方案。

Method: 研究使用穿戴设备（Empatica E4和Shimmer3 GSR+腕带）采集生理信号，并结合iMotion的面部表情分析模块获取情绪标签。通过经典机器学习模型预测情绪强度。

Result: 在回归任务中，模型取得最高的0.782 R2分数和最低的0.0006均方误差（MSE），验证了该方法的可行性。

Conclusion: 该方法为阿尔茨海默病及相关痴呆（ADRD）患者以及创伤后应激障碍（PTSD）患者等提供了隐私保护且高效的情绪识别方案，具有实际应用潜力。

Abstract: Emotion detection in older adults is crucial for understanding their
cognitive and emotional well-being, especially in hospital and assisted living
environments. In this work, we investigate an edge-based, non-obtrusive
approach to emotion identification that uses only physiological signals
obtained via wearable sensors. Our dataset includes data from 40 older
individuals. Emotional states were obtained using physiological signals from
the Empatica E4 and Shimmer3 GSR+ wristband and facial expressions were
recorded using camera-based emotion recognition with the iMotion's Facial
Expression Analysis (FEA) module. The dataset also contains twelve emotion
categories in terms of relative intensities. We aim to study how well emotion
recognition can be accomplished using simply physiological sensor data, without
the requirement for cameras or intrusive facial analysis. By leveraging
classical machine learning models, we predict the intensity of emotional
responses based on physiological signals. We achieved the highest 0.782 r2
score with the lowest 0.0006 MSE on the regression task. This method has
significant implications for individuals with Alzheimer's Disease and Related
Dementia (ADRD), as well as veterans coping with Post-Traumatic Stress Disorder
(PTSD) or other cognitive impairments. Our results across multiple classical
regression models validate the feasibility of this method, paving the way for
privacy-preserving and efficient emotion recognition systems in real-world
settings.

</details>


### [32] [Uncanny or Not? Perceptions of AI-Generated Faces in Autism](https://arxiv.org/abs/2507.08230)
*Gabriella Waters*

Main category: cs.HC

TL;DR: 研究探讨了自闭症患者对AI生成面孔的感知，发现他们对真人面孔的不适感可能更强，为开发包容性AI提供了启示。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成人脸技术日益成熟，了解自闭症患者对这些图像的感知对开发包容性技术至关重要。

Method: 通过分析Reddit上r/autism社区的讨论定性研究自闭症患者的体验。

Result: 自闭症患者对AI生成面孔的‘恐怖谷’效应体验不同，对真人面孔的不适感可能更强。

Conclusion: 研究有助于理解自闭症患者的视觉感知，并为开发包容性AI技术提供了新方向。

Abstract: As artificial intelligence (AI) systems become increasingly sophisticated at
generating synthetic human faces, understanding how these images are perceived
across diverse populations is important. This study investigates how autistic
individuals/individuals with autism perceive AI-generated faces, focusing on
the uncanny valley effect. Using a qualitative approach, we analyzed
discussions from the r/autism community on Reddit to explore how autistic
participants/participants with autism describe their experiences with
AI-generated faces and the uncanny valley phenomenon. The findings suggest that
autistic people/people with autism may experience the uncanny valley
differently, often reporting stronger discomfort with real human faces than
with artificial ones. This research contributes to our understanding of visual
perception in autism and has implications for the development of inclusive AI
systems and assistive technologies.

</details>


### [33] [Do Conversational Interfaces Limit Creativity? Exploring Visual Graph Systems for Creative Writing](https://arxiv.org/abs/2507.08260)
*Abhinav Sood,Maria Teresa Llano,Jon McCormack*

Main category: cs.HC

TL;DR: 提出一种基于节点的图形化系统，通过可视化连接生成式AI模型支持创意任务，克服线性对话结构的限制。


<details>
  <summary>Details</summary>
Motivation: 现行基于链条的LLMs方法虽提供透明度和可控性，但预设步骤限制了自由探索，阻碍创造力发挥。

Method: 基于创意研究的认知过程，设计图形化节点系统，创建可重用、可共享的模板。

Result: 小规模用户研究表明，图形化系统优于对话界面，支持创意构思和写作可视化。

Conclusion: 更高复杂度的用户界面可提升创意效果，但需用户有效使用，系统仍有待改进。

Abstract: We present a graphical, node-based system through which users can visually
chain generative AI models for creative tasks. Research in the area of chaining
LLMs has found that while chaining provides transparency, controllability and
guardrails to approach certain tasks, chaining with pre-defined LLM steps
prevents free exploration. Using cognitive processes from creativity research
as a basis, we create a system that addresses the inherent constraints of
chat-based AI interactions. Specifically, our system aims to overcome the
limiting linear structure that inhibits creative exploration and ideation.
Further, our node-based approach enables the creation of reusable, shareable
templates that can address different creative tasks. In a small-scale user
study, we find that our graph-based system supports ideation and allows some
users to better visualise and think through their writing process when compared
to a similar conversational interface. We further discuss the weaknesses and
limitations of our system, noting the benefits to creativity that user
interfaces with higher complexity can provide for users who can effectively use
them.

</details>


### [34] [Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance](https://arxiv.org/abs/2507.08624)
*Gábor Baranyi,Zsolt Csibi,Kristian Fenech,Áron Fóthi,Zsófia Gaál,Joul Skaf,András Lőrincz*

Main category: cs.HC

TL;DR: AIRS是一个基于AI的家庭康复支持框架，结合实时3D重建、智能导航和视觉-语言模型，用于机器引导的康复训练，特别是在全膝关节置换术后的场景中。


<details>
  <summary>Details</summary>
Motivation: 通过AI技术提升家庭康复的效率和质量，同时解决隐私问题和适应更广泛的康复需求。

Method: 利用智能手机进行实时3D重建，提供身体匹配的虚拟形象反馈，结合视觉和VLM生成的反馈机制。

Result: AIRS能够优化康复训练配置，支持视觉和听力障碍患者，并具有模块化设计以适应更多场景。

Conclusion: AIRS是一种灵活且多功能的康复支持系统，具备进一步定制和扩展的潜力。

Abstract: This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS)
framework, an advanced artificial intelligence-based solution tailored for home
rehabilitation environments. AIRS integrates cutting-edge technologies,
including Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and
large Vision-Language Models (VLMs), to create a comprehensive system for
machine-guided physical rehabilitation. The general AIRS framework is
demonstrated in rehabilitation scenarios following total knee replacement
(TKR), utilizing a database of 263 video recordings for evaluation. A
smartphone is employed within AIRS to perform RT-3DR of living spaces and has a
body-matched avatar to provide visual feedback about the excercise. This avatar
is necessary in (a) optimizing exercise configurations, including camera
placement, patient positioning, and initial poses, and (b) addressing privacy
concerns and promoting compliance with the AI Act. The system guides users
through the recording process to ensure the collection of properly recorded
videos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling
direct comparisons between prerecorded clinical exercises and patient home
recordings and (ii) VLM-generated feedback, providing detailed explanations and
corrections for exercise errors. The framework also supports people with visual
and hearing impairments. It also features a modular design that can be adapted
to broader rehabilitation contexts. AIRS software components are available for
further use and customization.

</details>


### [35] [Push or Light: Nudging Standing to Break Prolonged Sitting](https://arxiv.org/abs/2507.08659)
*Sohshi Yoshida,Ko Watanabe,Andreas Dengel,Shoya Ishimaru,Shingo Ata,Manato Fujimoto*

Main category: cs.HC

TL;DR: 研究比较了两种提示久坐者站立的方法（推送通知和灯光调暗），发现灯光调暗在小任务中效果更好，但更易引起不适。


<details>
  <summary>Details</summary>
Motivation: 久坐是健康风险，需探索有效的提示方法。

Method: 15名大学生参与实验，测试三种干预方法（无提示、推送通知、灯光调暗）和三种任务情境（电脑工作、视频通话、阅读），评估站立频率和舒适度。

Result: 灯光调暗提示效果略优于推送通知，但66.7%参与者感到不适；推送通知不适感较低（20%）。任务情境影响效果。

Conclusion: 自适应提示系统应根据情境和个人偏好定制干预方式。

Abstract: Prolonged sitting is a health risk leading to metabolic and cardiovascular
diseases. To combat this, various "nudging" strategies encourage stand-ups.
Behavior change triggers use explicit prompts such as smartphone push
notifications or light controls. However, comparisons of the effects of such
interactions, discomfort, and user context have not yet been performed. The
present study evaluated these methods in a mixed design experiment with 15
college students. Three intervention methods (none, push notifications, and
light dimming) and three user task contexts (computer work, video calls, and
reading) were tested. The frequency of standing up and comfort were assessed
after each ten-minute session. Results showed that dimming resulted in slightly
more breaks (1.4 \pm 1.55) than push notification (1.2 \pm 1.08), but caused
discomfort for 66.7% of participants, compared to 20% for notification. The
results were influenced by task context. Dimming was most effective during
video calls and reading, while push notifications were more effective during
computer work. These findings suggest adaptive nudging systems should tailor
interventions based on context and individual preferences.

</details>


### [36] [LIMITER: A Gamified Interface for Harnessing Just Intonation Systems](https://arxiv.org/abs/2507.08675)
*Antonis Christou*

Main category: cs.HC

TL;DR: LIMITER是一个游戏化数字乐器，旨在简化和表演微音和纯律声音。


<details>
  <summary>Details</summary>
Motivation: 解决西方音乐中微音系统难以理解和表演的问题。

Method: 通过颜色、几何变换和游戏化控制设计易上手的界面。

Result: 介绍了LIMITER的开发背景、音乐与工程系统，并初步评估其对创造力的影响。

Conclusion: LIMITER为使用微音和纯律声音提供了一种新颖且易于操作的方法。

Abstract: This paper introduces LIMITER, a gamified digital musical instrument for
harnessing and performing microtonal and justly intonated sounds. While
microtonality in Western music remains a niche and esoteric system that can be
difficult both to conceptualize and to perform with, LIMITER presents a novel,
easy to pickup interface that utilizes color, geometric transformations, and
game-like controls to create a simpler inlet into utilizing these sounds as a
means of expression. We report on the background of the development of LIMITER,
as well as explain the underlying musical and engineering systems that enable
its function. Additionally, we offer a discussion and preliminary evaluation of
the creativity-enhancing effects of the interface.

</details>


### [37] [EqualMotion: Accessible Motion Capture for the Creative Industries](https://arxiv.org/abs/2507.08744)
*Clarice Hilton,Kat Hawkins,Phill Tew,Freddie Collins,Seb Madgwick,Dominic Potts,Tom Mitchell*

Main category: cs.HC

TL;DR: EqualMotion是一种面向包容性的动作捕捉系统，通过残疾中心化设计支持多样化的身体类型和运动风格，旨在实现数字表演和原型设计中的公平参与。


<details>
  <summary>Details</summary>
Motivation: 目前的动作捕捉技术在身体建模、校准和虚拟形象表示中通常基于规范假设，导致残疾从业者被排斥。EqualMotion旨在通过包容性设计改变这一现状。

Method: 采用残疾中心化的协同设计方法，开发了身体无关的可穿戴动作捕捉系统，支持个性化校准和集成辅助工具，并使用包容性视觉语言。

Result: EqualMotion系统能够支持多样化的身体类型和运动风格，并与残疾研究人员和创意人士合作开发，以促进公平参与。

Conclusion: 通过案例研究（如舞蹈和音乐）验证，EqualMotion展示了包容性动作捕捉技术在现实创意工作流程中的可行性和价值。

Abstract: Motion capture technologies are increasingly used in creative and performance
contexts but often exclude disabled practitioners due to normative assumptions
in body modeling, calibration, and avatar representation. EqualMotion
introduces a body-agnostic, wearable motion capture system designed through a
disability-centred co-design approach. By enabling personalised calibration,
integrating mobility aids, and adopting an inclusive visual language,
EqualMotion supports diverse body types and movement styles. The system is
developed collaboratively with disabled researchers and creatives, aiming to
foster equitable participation in digital performance and prototyping. This
paper outlines the system's design principles and highlights ongoing case
studies in dance and music to evaluate accessibility in real-world creative
workflows.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [38] [FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields](https://arxiv.org/abs/2507.08285)
*Gwanhyeong Koo,Sunjae Yoon,Younghwan Lee,Ji Woo Hong,Chang D. Yoo*

Main category: cs.GR

TL;DR: FlowDrag通过利用几何信息改进拖动编辑的精确性和一致性，避免了现有方法仅依赖用户定义点导致的几何不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有拖动编辑方法因仅关注用户定义点匹配而忽视整体几何，导致编辑结果出现瑕疵或不稳定。

Method: FlowDrag通过构建3D网格并基于能量函数变形，利用UNet去噪过程投影网格位移以实现精确点对齐。

Result: FlowDrag在VFD和DragBench上优于现有方法。

Conclusion: FlowDrag通过结合几何信息提升了拖动编辑的准确性和一致性，并提出了带真实数据的VFD基准数据集。

Abstract: Drag-based editing allows precise object manipulation through point-based
control, offering user convenience. However, current methods often suffer from
a geometric inconsistency problem by focusing exclusively on matching
user-defined points, neglecting the broader geometry and leading to artifacts
or unstable edits. We propose FlowDrag, which leverages geometric information
for more accurate and coherent transformations. Our approach constructs a 3D
mesh from the image, using an energy function to guide mesh deformation based
on user-defined drag points. The resulting mesh displacements are projected
into 2D and incorporated into a UNet denoising process, enabling precise
handle-to-target point alignment while preserving structural integrity.
Additionally, existing drag-editing benchmarks provide no ground truth, making
it difficult to assess how accurately the edits match the intended
transformations. To address this, we present VFD (VidFrameDrag) benchmark
dataset, which provides ground-truth frames using consecutive shots in a video
dataset. FlowDrag outperforms existing drag-based editing methods on both VFD
Bench and DragBench.

</details>


### [39] [Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset Generation](https://arxiv.org/abs/2507.08513)
*Liu He,Xiao Zeng,Yizhi Song,Albert Y. C. Chen,Lu Xia,Shashwat Verma,Sankalp Dayal,Min Sun,Cheng-Hao Kuo,Daniel Aliaga*

Main category: cs.GR

TL;DR: 本文提出了一种基于3D资产和扩散模型的合成数据生成方法，用于解决多模态大语言模型（MLLMs）在相机-物体关系捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs因训练数据中相机-物体关系多样性不足，导致在物体方向、相机视角和拍摄镜头等任务上表现不佳。

Method: 设计了一个合成数据生成流程，结合3D渲染和扩散模型生成逼真图像，并利用大语言模型生成文本提示。最终构建了包含24万VQA数据的Ultimate3D数据集。

Result: 在相机-物体关系识别任务上，基于Ultimate3D调优的MLLMs比商业模型平均准确率提升了33.4%。

Conclusion: 所提出的数据集、方法和基准测试有助于推动MLLMs在更广泛的应用场景中的发展。

Abstract: Multimodal Large Language Models (MLLMs) struggle with accurately capturing
camera-object relations, especially for object orientation, camera viewpoint,
and camera shots. This stems from the fact that existing MLLMs are trained on
images with limited diverse camera-object relations and corresponding textual
descriptions. To address this, we propose a synthetic generation pipeline to
create large-scale 3D visual instruction datasets. Our framework takes 3D
assets as input and uses rendering and diffusion-based image generation models
to create photorealistic images preserving precise camera-object relations.
Additionally, large language models (LLMs) are used to generate text prompts
for guiding visual instruction tuning and controlling image generation. We
create Ultimate3D, a dataset of 240K VQAs with precise camera-object
annotations, and corresponding benchmark. MLLMs fine-tuned on our proposed
dataset outperform commercial models by a large margin, achieving an average
accuracy improvement of 33.4% on camera-object relation recognition tasks. Our
code, dataset, and benchmark will contribute to broad MLLM applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [40] [Supporting Intel(r) SGX on Multi-Package Platforms](https://arxiv.org/abs/2507.08190)
*Simon Johnson,Raghunandan Makaram,Amy Santoni,Vinnie Scarlata*

Main category: cs.DC

TL;DR: 本文讨论了Intel SGX技术在云平台中的应用，并提出了扩展平台功能的必要性，以支持可编程的可信执行环境（TEE）。


<details>
  <summary>Details</summary>
Motivation: 研究如何将Intel SGX技术从单插槽服务器扩展到多插槽云平台，以支持更广泛的机密云计算需求。

Method: 通过分析SGX在云平台中的现有应用，并提出额外的平台增强功能，以实现可扩展、高性能和安全的TEE。

Result: 提出了增强平台功能的建议，以支持云环境中的可编程TEE。

Conclusion: Intel SGX技术在云平台中的应用有潜力，但需要进一步扩展功能以满足云环境的扩展性和安全性需求。

Abstract: Intel(r) Software Guard Extensions (SGX) was originally released on client
platforms and later extended to single socket server platforms. As developers
have become familiar with the capabilities of the technology, the applicability
of this capability in the cloud has been tested. Various Cloud Service
Providers (CSPs) are demonstrating the value of using SGX based Trusted
Execution Environments (TEE) to create a new paradigm of Confidential Cloud
Computing. This paper describes the additional platform enhancements we believe
are necessary to deliver a user programmable Trusted Execution Environment that
scales to cloud usages, performs and is secure on multi-package platforms.

</details>


### [41] [Fast and Interactive Byzantine Fault-tolerant Web Services via Session-Based Consensus Decoupling](https://arxiv.org/abs/2507.08281)
*Ahmad Zaki Akmal,Azkario Rizky Pratama,Guntur Dharma Putra*

Main category: cs.DC

TL;DR: 提出了一种两层架构，通过分离交互操作与共识确认，实现低延迟和高安全性的BFT系统。


<details>
  <summary>Details</summary>
Motivation: 解决BFT系统的安全性和响应性之间的冲突，支持需要即时反馈和防篡改记录的领域。

Method: 采用会话感知的交易缓冲层（Layer 2）模拟共识提供即时反馈，定期提交批量操作到BFT共识层（Layer 1）。

Result: 系统实现200ms以下的用户响应，Layer 2操作速度是Layer 1的4倍，同时保持交易完整性。

Conclusion: 该架构为BFT系统开辟了新的应用领域，如需要高响应性和状态一致性的元宇宙环境。

Abstract: Byzantine fault-tolerant (BFT) web services provide critical integrity
guarantees for distributed applications but face significant latency challenges
that hinder interactive user experiences. We propose a novel two-layer
architecture that addresses this fundamental tension between security and
responsiveness in BFT systems. Our approach introduces a session-aware
transaction buffer layer (Layer 2) that delivers immediate feedback to users
through consensus simulation, while periodically committing batched operations
to a fully Byzantine fault-tolerant consensus layer (Layer 1). By separating
interactive operations from consensus finalization, our system achieves
responsive user experiences of under 200ms, while maintaining strong BFT
security guarantees. We demonstrate the efficacy of our architecture through a
supply chain management implementation, where operators require both immediate
feedback during multi-step workflows and tamper-proof record keeping. Our
evaluation shows that our Layer 2 operations perform four times faster than the
Layer 1 counterpart, while substantially preserving the end-to-end transaction
integrity. Our approach enables BFT applications in domains previously
considered impractical due to latency constraints, such as metaverse
environments, where users require both responsive interaction and guaranteed
state consistency.

</details>


### [42] [Content-Oblivious Leader Election in 2-Edge-Connected Networks](https://arxiv.org/abs/2507.08348)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: 本研究提出了一种在任何2边连接的异步网络中终止的领导者选举算法，无需预先指定领导者，否定了之前的猜想。


<details>
  <summary>Details</summary>
Motivation: 此前的研究认为在完全缺陷网络中需要预设领导者进行非平凡计算，但近来有研究在环形拓扑中否定了这一猜想。本研究旨在将此推广到任意2边连接网络。

Method: 设计了一种基于异步内容无视网络的领导者选举算法，消息复杂度为O(mN·ID_min)，适用于2边连接网络。

Result: 算法成功终止并选举领导者，结合之前仿真结果，证明无需预设领导者即可在完全缺陷网络中仿真无噪音算法。

Conclusion: 完全否定了领导者必须预设的猜想，扩展了内容无视计算的可能性。

Abstract: Censor-Hillel, Cohen, Gelles, and Sela (PODC 2022 \& Distributed Computing
2023) studied fully-defective asynchronous networks, where communication
channels may suffer an extreme form of alteration errors, rendering messages
completely corrupted. The model is equivalent to content-oblivious computation,
where nodes communicate solely via pulses. They showed that if the network is
2-edge-connected, then any algorithm for a noiseless setting can be simulated
in the fully-defective setting; otherwise, no non-trivial computation is
possible in the fully-defective setting. However, their simulation requires a
predesignated leader, which they conjectured to be necessary for any
non-trivial content-oblivious task.
  Recently, Frei, Gelles, Ghazy, and Nolin (DISC 2024) refuted this conjecture
for the special case of oriented ring topology. They designed two asynchronous
content-oblivious leader election algorithms with message complexity $O(n \cdot
\mathsf{ID}_{\max})$, where $n$ is the number of nodes and $\mathsf{ID}_{\max}$
is the maximum $\mathsf{ID}$. The first algorithm stabilizes in unoriented
rings without termination detection. The second algorithm quiescently
terminates in oriented rings, thus enabling the execution of the simulation
algorithm after leader election.
  In this work, we present an asynchronous content-oblivious leader election
algorithm that quiescently terminates in any 2-edge connected network with
message complexity $O(m \cdot N \cdot \mathsf{ID}_{\min})$, where $m$ is the
number of edges, $N$ is a known upper bound on the number of nodes, and
$\mathsf{ID}_{\min}$ is the smallest $\mathsf{ID}$. Combined with the previous
simulation result, our finding implies that any algorithm from the noiseless
setting can be simulated in the fully-defective setting without assuming a
preselected leader, entirely refuting the original conjecture.

</details>


### [43] [Carbon-Aware Workflow Scheduling with Fixed Mapping and Deadline Constraint](https://arxiv.org/abs/2507.08725)
*Dominik Schweisgut,Anne Benoit,Yves Robert,Henning Meyerhenke*

Main category: cs.DC

TL;DR: 论文研究了大型计算中心中任务调度的碳减排问题，提出了一种启发式框架CaWoSched，结合贪心算法和局部搜索，显著降低了碳排放。


<details>
  <summary>Details</summary>
Motivation: 大型计算中心消耗大量能源，其任务调度中的碳排放问题亟待解决。通过优化任务执行时间以利用绿色能源，可以减少碳排放。

Method: 将问题形式化为调度问题，提出启发式框架CaWoSched，结合贪心算法和局部搜索，并设计基线和精确ILP解决方案进行比较。

Result: 实验表明，CaWoSched的16种启发式组合相比基线显著减少了碳排放。

Conclusion: CaWoSched为多处理器环境中的任务调度提供了一种有效的碳减排解决方案，但其复杂度随处理器数量增加而显著上升。

Abstract: Large data and computing centers consume a significant share of the world's
energy consumption. A prominent subset of the workloads in such centers are
workflows with interdependent tasks, usually represented as directed acyclic
graphs (DAGs). To reduce the carbon emissions resulting from executing such
workflows in centers with a mixed (renewable and non-renewable) energy supply,
it is advisable to move task executions to time intervals with sufficient green
energy when possible. To this end, we formalize the above problem as a
scheduling problem with a given mapping and ordering of the tasks. We show that
this problem can be solved in polynomial time in the uniprocessor case. For at
least two processors, however, the problem becomes NP-hard. Hence, we propose a
heuristic framework called CaWoSched that combines several greedy approaches
with local search. To assess the 16 heuristics resulting from different
combinations, we also devise a simple baseline algorithm and an exact ILP-based
solution. Our experimental results show that our heuristics provide significant
savings in carbon emissions compared to the baseline.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [44] [TableCopilot: A Table Assistant Empowered by Natural Language Conditional Table Discovery](https://arxiv.org/abs/2507.08283)
*Lingxi Cui,Guanyu Jiang,Huan Li,Ke Chen,Lidan Shou,Gang Chen*

Main category: cs.DB

TL;DR: 论文介绍了TableCopilot，一个基于LLM的交互式表格助手，用于解决大规模表格池中的表格发现难题，提出了nlcTD场景和Crofuma方法，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有表格助手系统假设用户已拥有完整表格，忽视了大规模表格池中的表格发现挑战，需要一种更灵活、个性化的解决方案。

Method: 提出了Crofuma方法，通过学习单模态和跨模态匹配分数的交叉融合，实现高效表格发现。

Result: Crofuma在NDCG@5指标上比现有方法至少提升12%。

Conclusion: TableCopilot通过nlcTD场景和Crofuma方法，为表格发现和分析设立了新标准，使其更易用且集成化。

Abstract: The rise of LLM has enabled natural language-based table assistants, but
existing systems assume users already have a well-formed table, neglecting the
challenge of table discovery in large-scale table pools. To address this, we
introduce TableCopilot, an LLM-powered assistant for interactive, precise, and
personalized table discovery and analysis. We define a novel scenario, nlcTD,
where users provide both a natural language condition and a query table,
enabling intuitive and flexible table discovery for users of all expertise
levels. To handle this, we propose Crofuma, a cross-fusion-based approach that
learns and aggregates single-modal and cross-modal matching scores.
Experimental results show Crofuma outperforms SOTA single-input methods by at
least 12% on NDCG@5. We also release an instructional video, codebase,
datasets, and other resources on GitHub to encourage community contributions.
TableCopilot sets a new standard for interactive table assistants, making
advanced table discovery accessible and integrated.

</details>


### [45] [xpSHACL: Explainable SHACL Validation using Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2507.08432)
*Gustavo Correa Publio,José Emilio Labra Gayo*

Main category: cs.DB

TL;DR: xpSHACL是一种可解释的SHACL验证系统，通过结合规则化的理由树、RAG和LLM，为约束违规提供详细、多语言、易读的解释。


<details>
  <summary>Details</summary>
Motivation: 传统SHACL验证引擎的报告通常晦涩难懂，非技术用户难以理解和使用，xpSHACL旨在解决这一问题。

Method: 结合规则化的理由树、检索增强生成（RAG）和大语言模型（LLM），并利用违规知识图谱（Violation KG）缓存和重用解释。

Result: 系统能够生成详细、多语言、易读的违规解释，提高效率和一致性。

Conclusion: xpSHACL通过改进解释机制，显著提升了非技术用户对SHACL验证报告的理解和使用能力。

Abstract: Shapes Constraint Language (SHACL) is a powerful language for validating RDF
data. Given the recent industry attention to Knowledge Graphs (KGs), more users
need to validate linked data properly. However, traditional SHACL validation
engines often provide terse reports in English that are difficult for
non-technical users to interpret and act upon. This paper presents xpSHACL, an
explainable SHACL validation system that addresses this issue by combining
rule-based justification trees with retrieval-augmented generation (RAG) and
large language models (LLMs) to produce detailed, multilanguage, human-readable
explanations for constraint violations. A key feature of xpSHACL is its usage
of a Violation KG to cache and reuse explanations, improving efficiency and
consistency.

</details>


### [46] [ONION: A Multi-Layered Framework for Participatory ER Design](https://arxiv.org/abs/2507.08702)
*Viktoriia Makovska,George Fletcher,Julia Stoyanovich*

Main category: cs.DB

TL;DR: ONION是一个多层框架，用于参与式实体关系建模，结合了设计正义、参与式AI和概念建模的见解，包含五个阶段：观察、培育、整合、优化和规范化。


<details>
  <summary>Details</summary>
Motivation: 减少设计者偏见，促进包容性参与，并在建模过程中提高透明度。

Method: ONION采用五阶段方法（Observe, Nurture, Integrate, Optimize, Normalize），支持从非结构化的利益相关者输入逐步抽象为结构化的ER图。

Result: 通过乌克兰社会技术系统的实际工作坊评估，ONION显示了其促进多样化利益相关者参与的能力，从而产生更丰富的数据模型和更深入的理解。

Conclusion: ONION在早期数据建模中展现出容纳多样性的潜力，但还需要解决扩展和改进框架的挑战。

Abstract: We present ONION, a multi-layered framework for participatory
Entity-Relationship (ER) modeling that integrates insights from design justice,
participatory AI, and conceptual modeling. ONION introduces a five-stage
methodology: Observe, Nurture, Integrate, Optimize, Normalize. It supports
progressive abstraction from unstructured stakeholder input to structured ER
diagrams.
  Our approach aims to reduce designer bias, promote inclusive participation,
and increase transparency through the modeling process. We evaluate ONION
through real-world workshops focused on sociotechnical systems in Ukraine,
highlighting how diverse stakeholder engagement leads to richer data models and
deeper mutual understanding. Early results demonstrate ONION's potential to
host diversity in early-stage data modeling. We conclude with lessons learned,
limitations and challenges involved in scaling and refining the framework for
broader adoption.

</details>


### [47] [Hashing for Fast Pattern Set Selection](https://arxiv.org/abs/2507.08745)
*Maiju Karjalainen,Pauli Miettinen*

Main category: cs.DB

TL;DR: 该论文提出了一种基于bottom-k哈希的高效模式集挖掘方法，旨在通过重构误差衡量模式集质量，并显著优于标准贪婪算法。


<details>
  <summary>Details</summary>
Motivation: 模式集挖掘是数据挖掘中的核心问题，旨在找到高质量的模式集而非所有模式。研究团队希望通过重构误差作为质量衡量指标，解决如何高效找到优质模式集的问题。

Method: 采用基于bottom-k哈希的方法来选择模式集，并扩展至模式在数据中可能以近似形式出现的情况。

Result: 在合成和真实数据集上验证，哈希方法比标准贪婪算法快得多，且结果质量接近。

Conclusion: 该方法在多种应用中有效，包括数据库切片、布尔矩阵分解和重描述挖掘，为高效模式集挖掘提供了新思路。

Abstract: Pattern set mining, which is the task of finding a good set of patterns
instead of all patterns, is a fundamental problem in data mining. Many
different definitions of what constitutes a good set have been proposed in
recent years. In this paper, we consider the reconstruction error as a proxy
measure for the goodness of the set, and concentrate on the adjacent problem of
how to find a good set efficiently. We propose a method based on bottom-k
hashing for efficiently selecting the set and extend the method for the common
case where the patterns might only appear in approximate form in the data. Our
approach has applications in tiling databases, Boolean matrix factorization,
and redescription mining, among others. We show that our hashing-based approach
is significantly faster than the standard greedy algorithm while obtaining
almost equally good results in both synthetic and real-world data sets.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [48] [CCSS: Hardware-Accelerated RTL Simulation with Fast Combinational Logic Computing and Sequential Logic Synchronization](https://arxiv.org/abs/2507.08406)
*Weigang Feng,Yijia Zhang,Zekun Wang,Zhengyang Wang,Yi Wang,Peijun Ma,Ningyi Xu*

Main category: cs.AR

TL;DR: 本文提出CCSS，一种可扩展的多核RTL模拟平台，旨在解决RTL级模拟速度慢的问题，通过优化组合逻辑计算和顺序逻辑同步，实现快速编译和高仿真吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着单芯片晶体管数量超过数百亿，RTL级模拟和验证的复杂性呈指数级增长，模拟周期可能长达数月。当前多核CPU的模拟速度已成为主要瓶颈。

Method: CCSS采用专门的架构和编译策略，包括平衡的DAG分区方法、高效的布尔计算核心以及低延迟的片上网络（NoC）设计，以加速组合逻辑计算和顺序逻辑同步。

Result: 实验结果显示，CCSS比现有最先进的多核模拟器快12.9倍。

Conclusion: CCSS通过创新的架构和编译策略，显著提升了RTL模拟的速度和效率，为行业提供了可行的解决方案。

Abstract: As transistor counts in a single chip exceed tens of billions, the complexity
of RTL-level simulation and verification has grown exponentially, often
extending simulation campaigns to several months. In industry practice, RTL
simulation is divided into two phases: functional debug and system validation.
While system validation demands high simulation speed and is typically
accelerated using FPGAs, functional debug relies on rapid compilation-rendering
multi-core CPUs the primary choice. However, the limited simulation speed of
CPUs has become a major bottleneck. To address this challenge, we propose CCSS,
a scalable multi-core RTL simulation platform that achieves both fast
compilation and high simulation throughput. CCSS accelerates combinational
logic computation and sequential logic synchronization through specialized
architecture and compilation strategies. It employs a balanced DAG partitioning
method and efficient boolean computation cores for combinational logic, and
adopts a low-latency network-on-chip (NoC) design to synchronize sequential
states across cores efficiently. Experimental results show that CCSS delivers
up to 12.9x speedup over state-of-the-art multi-core simulators.

</details>


### [49] [Fast and Efficient Merge of Sorted Input Lists in Hardware Using List Offset Merge Sorters](https://arxiv.org/abs/2507.08658)
*Robert B. Kent,Marios S. Pattichis*

Main category: cs.AR

TL;DR: 提出了新型硬件合并排序设备LOMS，通过偏移排序输入列表和使用列/行排序阶段，高效合并多个排序列表，比现有设备更快且更节省资源。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有合并排序设备在速度和资源使用上的局限性，提出了一种更高效的硬件实现方法。

Method: 利用输入列表的偏移排列和列/行排序阶段的交替处理，设计出LOMS设备，特别是2-way和3-way合并排序器。

Result: LOMS 2-way设备合并32个值的列表仅需2.24 nS，速度提升2.63倍；3-way设备合并21个值仅需3.4 nS，速度提升1.36倍。

Conclusion: LOMS设备在速度和资源效率上优于现有技术，适用于FPGA实现，尤其在资源受限的场景中表现突出。

Abstract: A new set of hardware merge sort devices are introduced here, which merge
multiple sorted input lists into a single sorted output list in a fast and
efficient manner. In each merge sorter, the values from the sorted input lists
are arranged in an input 2-D setup array, but with the order of each sorted
input list offset from the order of each of the other sorted input lists. In
these new devices, called List Offset Merge Sorters (LOMS), a minimal set of
column sort stages alternating with row sort stages process the input setup
array into a final output array, now in the defined sorted order. LOMS 2-way
sorters, which merge 2 sorted input lists, require only 2 merge stages and are
significantly faster than Kenneth Batcher's previous state-of-the-art 2-way
merge devices, Bitonic Merge Sorters and Odd-Even Merge Sorters. LOMS 2-way
sorters utilize the recently-introduced Single-Stage 2-way Merge Sorters (S2MS)
in their first stage. Both LOMS and S2MS devices can merge any mixture of input
list sizes, while Batcher's merge sorters are difficult to design unless the 2
input lists are equal, and a power-of-2. By themselves, S2MS devices are the
fastest 2-way merge sorters when implemented in this study's target FPGA
devices, but they tend to use a large number of LUT resources. LOMS 2-way
devices use fewer resources than comparable S2MS devices, enabling some large
LOMS devices to be implemented in a given FPGA when comparable S2MS devices
cannot fit in that FPGA. A List Offset 2-way sorter merges 2 lists, each with
32 values, into a sorted output list of those 64 values in 2.24 nS, a speedup
of 2.63 versus a comparable Batcher device. A LOMS 3-way merge sorter, merging
3 sorted input lists with 7 values, fully merges the 21 values in 3.4 nS, a
speedup of 1.36 versus the comparable state-of-the-art 3-way merge device.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [50] [TruChain: A Multi-Layer Architecture for Trusted, Verifiable, and Immutable Open Banking Data](https://arxiv.org/abs/2507.08286)
*Aufa Nasywa Rahman,Bimo Sunarfri Hantono,Guntur Dharma Putra*

Main category: cs.CR

TL;DR: 提出了一种分层架构，通过三个信任层级确保开放银行框架中数据的可靠性，包括来源验证、数据认证和防篡改存储，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 开放银行框架存在数据来源未验证、数据完整性不一致和缺乏不可篡改性等技术风险，需要一种高效且安全的解决方案。

Method: 采用三层架构：第一层通过去中心化身份和可验证展示确保来源合法性；第二层使用加密签名验证数据真实性和一致性；第三层通过Tangle分布式账本实现数据不可篡改。

Result: 概念验证显示系统性能稳定，吞吐量线性扩展，验证率100%，资源占用低（CPU<35%，内存<350MiB），比实际开放银行实现延迟更低、数据完整性更强。

Conclusion: 该方案为金融生态系统提供了实用、高效且合规的安全数据共享解决方案。

Abstract: Open banking framework enables third party providers to access financial data
across banking institutions, leading to unprecedented innovations in the
financial sector. However, some open banking standards remain susceptible to
severe technological risks, including unverified data sources, inconsistent
data integrity, and lack of immutability. In this paper, we propose a layered
architecture that provides assurance in data trustworthiness with three
distinct levels of trust, covering source validation, data-level
authentication, and tamper-proof storage. The first layer guarantees the source
legitimacy using decentralized identity and verifiable presentation, while the
second layer verifies data authenticity and consistency using cryptographic
signing. Lastly, the third layer guarantees data immutability through the
Tangle, a directed acyclic graph distributed ledger. We implemented a
proof-of-concept implementation of our solution to evaluate its performance,
where the results demonstrate that the system scales linearly with a stable
throughput, exhibits a 100% validation rate, and utilizes under 35% of CPU and
350 MiB memory. Compared to a real-world open banking implementation, our
solution offers significantly reduced latency and stronger data integrity
assurance. Overall, our solution offers a practical and efficient system for
secure data sharing in financial ecosystems while maintaining regulatory
compliance.

</details>


### [51] [Evaluating Post-Quantum Cryptographic Algorithms on Resource-Constrained Devices](https://arxiv.org/abs/2507.08312)
*Jesus Lopez,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.CR

TL;DR: 本研究探讨了在资源受限的IoT设备上部署后量子密码（PQC）算法的可行性，并在Raspberry Pi平台上实现了三种PQC算法。


<details>
  <summary>Details</summary>
Motivation: 量子计算的快速发展对传统加密算法（如RSA和ECC）构成威胁，尤其是在计算资源有限的IoT设备中。

Method: 在Raspberry Pi平台上实现了BIKE、CRYSTALS-Kyber和HQC三种PQC算法，利用Open Quantum Safe库和mbedTLS开发量子安全密钥交换协议。

Result: 实验表明，PQC算法在受限硬件上的集成是可行的，计算开销、内存使用和能耗在合理范围内。

Conclusion: 下一代IoT设备需采用量子安全的加密框架，以应对量子计算带来的威胁。

Abstract: The rapid advancement of quantum computing poses a critical threat to
classical cryptographic algorithms such as RSA and ECC, particularly in
Internet of Things (IoT) devices, where secure communication is essential but
often constrained by limited computational resources. This paper investigates
the feasibility of deploying post-quantum cryptography (PQC) algorithms on
resource-constrained devices. In particular, we implement three PQC algorithms
-- BIKE, CRYSTALS-Kyber, and HQC -- on a lightweight IoT platform built with
Raspberry Pi devices. Leveraging the Open Quantum Safe (\texttt{liboqs})
library in conjunction with \texttt{mbedTLS}, we develop quantum-secure key
exchange protocols, and evaluate their performance in terms of computational
overhead, memory usage, and energy consumption for quantum secure
communication. Experimental results demonstrate that the integration of PQC
algorithms on constrained hardware is practical, reinforcing the urgent need
for quantum-resilient cryptographic frameworks in next-generation IoT devices.
The implementation of this paper is available at
https://iqsec-lab.github.io/PQC-IoT/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity](https://arxiv.org/abs/2507.08177)
*Arun Vignesh Malarkkan,Haoyue Bai,Xinyuan Wang,Anjali Kaushik,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 文章提出因果学习方法改进时空异常检测，解决当前数据驱动方法的局限性，如可解释性与适应性不足。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理系统互联性增强，传统黑盒深度学习方法在解释性和适应性上面临挑战，因果学习能提供更好的解决方案。

Method: 提出三个关键方向：因果图分析、多视角融合和持续因果图学习，结合真实系统（如水利设施）验证。

Result: 因果模型能提供早期预警和根因分析，弥补黑盒方法的不足。

Conclusion: 展望了多模态、生成式AI驱动的可扩展因果框架，推动可解释、自适应的异常检测系统发展。

Abstract: As cyber-physical systems grow increasingly interconnected and spatially
distributed, ensuring their resilience against evolving cyberattacks has become
a critical priority. Spatio-Temporal Anomaly detection plays an important role
in ensuring system security and operational integrity. However, current
data-driven approaches, largely driven by black-box deep learning, face
challenges in interpretability, adaptability to distribution shifts, and
robustness under evolving system dynamics. In this paper, we advocate for a
causal learning perspective to advance anomaly detection in spatially
distributed infrastructures that grounds detection in structural cause-effect
relationships. We identify and formalize three key directions: causal graph
profiling, multi-view fusion, and continual causal graph learning, each
offering distinct advantages in uncovering dynamic cause-effect structures
across time and space. Drawing on real-world insights from systems such as
water treatment infrastructures, we illustrate how causal models provide early
warning signals and root cause attribution, addressing the limitations of
black-box detectors. Looking ahead, we outline the future research agenda
centered on multi-modality, generative AI-driven, and scalable adaptive causal
frameworks. Our objective is to lay a new research trajectory toward scalable,
adaptive, explainable, and spatially grounded anomaly detection systems. We
hope to inspire a paradigm shift in cybersecurity research, promoting
causality-driven approaches to address evolving threats in interconnected
infrastructures.

</details>


### [53] [Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors](https://arxiv.org/abs/2507.08175)
*Md. Saif Hassan Onim,Travis S. Humble,Himanshu Thapliyal*

Main category: cs.LG

TL;DR: 通过生理信号推断情绪状态，提出了一种隐私保护的替代方案，优于传统面部识别技术。量子增强SVM在分类性能上超越经典方法。


<details>
  <summary>Details</summary>
Motivation: 探索一种不依赖面部识别的隐私保护情绪识别方法，尤其适用于沟通障碍人群。

Method: 比较经典机器学习与量子机器学习方法，采用量子核模型。

Result: 量子增强SVM在所有情绪类别中表现最佳，F1分数超80%，召回率提升高达36%。

Conclusion: 该方法有望应用于临床和辅助生活场景，为被动情绪监测奠定基础。

Abstract: We investigate the feasibility of inferring emotional states exclusively from
physiological signals, thereby presenting a privacy-preserving alternative to
conventional facial recognition techniques. We conduct a performance comparison
of classical machine learning algorithms and hybrid quantum machine learning
(QML) methods with a quantum kernel-based model. Our results indicate that the
quantum-enhanced SVM surpasses classical counterparts in classification
performance across all emotion categories, even when trained on limited
datasets. The F1 scores over all classes are over 80% with around a maximum of
36% improvement in the recall values. The integration of wearable sensor data
with quantum machine learning not only enhances accuracy and robustness but
also facilitates unobtrusive emotion recognition. This methodology holds
promise for populations with impaired communication abilities, such as
individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans
with Post-Traumatic Stress Disorder (PTSD). The findings establish an early
foundation for passive emotional monitoring in clinical and assisted living
conditions.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [54] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 提出了一种利用大语言模型从自然语言问题或关键词查询生成SPARQL查询的新方法，无需微调，通过策略性执行查询探索知识图谱，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决从自然语言生成SPARQL查询的挑战，避免对大型语言模型进行微调的需求。

Method: 利用大语言模型通过策略性执行SPARQL查询探索知识图谱，搜索相关IRI和字面量。

Result: 在Wikidata上达到零样本设置的SOTA结果，在Freebase上接近最佳少样本方法，其他知识图谱也表现良好。

Conclusion: 该方法在多种知识图谱和基准测试中表现优异，展示了零样本设置的潜力。

Abstract: We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [55] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: MM-Coder是一个多语言多模态的代码生成模型，通过结合视觉设计（如UML图和流程图）与文本指令，提升代码生成的准确性和架构对齐。


<details>
  <summary>Details</summary>
Motivation: 目前大多数代码生成模型仅依赖文本，忽视了软件开发中的视觉辅助工具，限制了模型的实用性。

Method: 开发了MMc-Instruct数据集用于多模态指令微调，并提出了MMEval评估标准，以支持视觉与文本结合的任务。

Result: MM-Coder在视觉信息捕捉、指令遵循和编程知识方面仍面临挑战，但为工业编程提供了新方向。

Conclusion: 该工作通过多模态方法扩展了LLM的应用潜力，为复杂规范的实现提供了新思路。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [56] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
*Sonali Sharma,Ahmed M. Alaa,Roxana Daneshjou*

Main category: cs.CL

TL;DR: 研究了2022至2025年间生成AI模型（如LLM和VLM）在医学图像和临床问题回答中免责声明的使用情况，发现其比例显著下降。


<details>
  <summary>Details</summary>
Motivation: 检测人工智能模型在医学领域应用的免责声明使用情况，以评估其安全性。

Method: 分析了500份乳腺X光片、500份胸透X光片、500份皮肤病图像和500个医学问题，筛查模型输出中是否存在免责声明。

Result: 免责声明使用率从2022年的26.3%降至2025年的0.97%（LLM），从2023年的19.6%降至1.05%（VLM）。

Conclusion: 随着模型能力的提升，免责声明需更灵活地适应临床环境以确保用户安全。

Abstract: Generative AI models, including large language models (LLMs) and
vision-language models (VLMs), are increasingly used to interpret medical
images and answer clinical questions. Their responses often include
inaccuracies; therefore, safety measures like medical disclaimers are critical
to remind users that AI outputs are not professionally vetted or a substitute
for medical advice. This study evaluated the presence of disclaimers in LLM and
VLM outputs across model generations from 2022 to 2025. Using 500 mammograms,
500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs
were screened for disclaimer phrases. Medical disclaimer presence in LLM and
VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023
to 1.05% in 2025, respectively. By 2025, the majority of models displayed no
disclaimers. As public models become more capable and authoritative,
disclaimers must be implemented as a safeguard adapting to the clinical context
of each output.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models](https://arxiv.org/abs/2507.08400)
*Yongjian Zhang,Longguang Wang,Kunhong Li,Ye Zhang,Yun Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: PanMatch是一个通用的基础模型，用于稳健的对应匹配，通过统一的位移估计框架实现多任务集成，无需特定任务架构。


<details>
  <summary>Details</summary>
Motivation: 现有的方法需要针对不同任务设计特定架构或微调，PanMatch旨在通过统一的模型权重解决这一局限。

Method: 提出一种基于2D位移估计的统一框架，利用多领域特征提取器和来自大视觉模型的全能特征，支持跨领域匹配。

Result: PanMatch在跨任务评估中优于UniMatch和Flow-Anything，并在异常场景下表现出色。

Conclusion: PanMatch展示了通用基础模型在多任务和异常场景中的潜力，为稳健匹配提供了新方向。

Abstract: This work presents PanMatch, a versatile foundation model for robust
correspondence matching. Unlike previous methods that rely on task-specific
architectures and domain-specific fine-tuning to support tasks like stereo
matching, optical flow or feature matching, our key insight is that any
two-frame correspondence matching task can be addressed within a 2D
displacement estimation framework using the same model weights. Such a
formulation eliminates the need for designing specialized unified architectures
or task-specific ensemble models. Instead, it achieves multi-task integration
by endowing displacement estimation algorithms with unprecedented
generalization capabilities. To this end, we highlight the importance of a
robust feature extractor applicable across multiple domains and tasks, and
propose the feature transformation pipeline that leverage all-purpose features
from Large Vision Models to endow matching baselines with zero-shot cross-view
matching capabilities. Furthermore, we assemble a cross-domain dataset with
near 1.8 million samples from stereo matching, optical flow, and feature
matching domains to pretrain PanMatch. We demonstrate the versatility of
PanMatch across a wide range of domains and downstream tasks using the same
model weights. Our model outperforms UniMatch and Flow-Anything on cross-task
evaluations, and achieves comparable performance to most state-of-the-art
task-specific algorithms on task-oriented benchmarks. Additionally, PanMatch
presents unprecedented zero-shot performance in abnormal scenarios, such as
rainy day and satellite imagery, where most existing robust algorithms fail to
yield meaningful results.

</details>


### [58] [Interpretability-Aware Pruning for Efficient Medical Image Analysis](https://arxiv.org/abs/2507.08330)
*Nikita Malik,Pratinav Seth,Neeraj Kumar Singh,Chintan Chitroda,Vinay Kumar Sankarapu*

Main category: cs.CV

TL;DR: 提出了一种基于可解释性的剪枝框架，在保持预测性能和透明度的同时降低模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中的应用受限于模型的大规模和缺乏透明度，需要轻量级且可解释的模型。

Method: 采用可解释性技术（如DL-Backtrace等）指导剪枝，选择性保留每层最相关的部分。

Result: 在多个医学图像分类基准上，实现高压缩率且精度损失最小。

Conclusion: 该方法为医疗场景中部署轻量级、可解释模型提供了可能。

Abstract: Deep learning has driven significant advances in medical image analysis, yet
its adoption in clinical practice remains constrained by the large size and
lack of transparency in modern models. Advances in interpretability techniques
such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated
Gradients make it possible to assess the contribution of individual components
within neural networks trained on medical imaging tasks. In this work, we
introduce an interpretability-guided pruning framework that reduces model
complexity while preserving both predictive performance and transparency. By
selectively retaining only the most relevant parts of each layer, our method
enables targeted compression that maintains clinically meaningful
representations. Experiments across multiple medical image classification
benchmarks demonstrate that this approach achieves high compression rates with
minimal loss in accuracy, paving the way for lightweight, interpretable models
suited for real-world deployment in healthcare settings.

</details>


### [59] [Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective](https://arxiv.org/abs/2507.08801)
*Hangjie Yuan,Weihua Chen,Jun Cen,Hu Yu,Jingyun Liang,Shuning Chang,Zhihui Lin,Tao Feng,Pengwei Liu,Jiazheng Xing,Hao Luo,Jiasheng Tang,Fan Wang,Yi Yang*

Main category: cs.CV

TL;DR: Lumos-1是一个基于LLM架构的自回归视频生成器，通过MM-RoPE和AR-DF技术解决时空相关性和帧间损失不平衡问题，性能与现有方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频生成器存在架构偏离、依赖外部文本编码器或延迟高的问题，Lumos-1旨在解决这些问题。

Method: 采用3D RoPE改进时空相关性，提出MM-RoPE和AR-DF技术优化训练和推理。

Result: Lumos-1在48 GPU上预训练，性能与EMU3、COSMOS-Video2World和OpenSoraPlan相当。

Conclusion: Lumos-1通过高效技术和架构优化，实现了高性能的视频生成。

Abstract: Autoregressive large language models (LLMs) have unified a vast range of
language tasks, inspiring preliminary efforts in autoregressive video
generation. Existing autoregressive video generators either diverge from
standard LLM architectures, depend on bulky external text encoders, or incur
prohibitive latency due to next-token decoding. In this paper, we introduce
Lumos-1, an autoregressive video generator that retains the LLM architecture
with minimal architectural modifications. To inject spatiotemporal correlations
in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its
imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE
scheme that preserves the original textual RoPE while providing comprehensive
frequency spectra and scaled 3D positions for modeling multimodal
spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy
that obeys intra-frame bidirectionality and inter-frame temporal causality.
Based on this dependency strategy, we identify the issue of frame-wise loss
imbalance caused by spatial information redundancy and solve it by proposing
Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal
tube masking during training with a compatible inference-time masking policy to
avoid quality degradation. By using memory-efficient training techniques, we
pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on
GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code
and models are available at https://github.com/alibaba-damo-academy/Lumos.

</details>


### [60] [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
*Luke Rivard,Sun Sun,Hongyu Guo,Wenhu Chen,Yuntian Deng*

Main category: cs.CV

TL;DR: NeuralOS利用RNN和扩散模型预测操作系统GUI屏幕帧，通过Ubuntu XFCE数据训练，能模拟鼠标交互和应用启动，但键盘交互仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 为未来的计算机交互系统开发自适应、生成的神经网络界面。

Method: 结合RNN追踪计算机状态和扩散模型渲染屏幕图像，使用Ubuntu XFCE数据集训练。

Result: 成功生成真实GUI序列，准确捕捉鼠标交互和状态转换（如应用启动），键盘交互仍需改进。

Conclusion: NeuralOS为实现未来自适应神经界面迈出一步，键盘交互精度有待提升。

Abstract: We introduce NeuralOS, a neural framework that simulates graphical user
interfaces (GUIs) of operating systems by directly predicting screen frames in
response to user inputs such as mouse movements, clicks, and keyboard events.
NeuralOS combines a recurrent neural network (RNN), which tracks computer
state, with a diffusion-based neural renderer that generates screen images. The
model is trained on a large-scale dataset of Ubuntu XFCE recordings, which
include both randomly generated interactions and realistic interactions
produced by AI agents. Experiments show that NeuralOS successfully renders
realistic GUI sequences, accurately captures mouse interactions, and reliably
predicts state transitions like application launches. Although modeling
fine-grained keyboard interactions precisely remains challenging, NeuralOS
offers a step toward creating fully adaptive, generative neural interfaces for
future human-computer interaction systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 论文定义了与对比解释相关的几个典型问题，分析其在命题逻辑中的基本性质，并展示了其与现有方法的联系及计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究对比解释的基本问题，回答“为什么P而不是Q？”这类问题，通过比较P和Q的差异提供解释。

Method: 在命题逻辑背景下定义问题，分析其性质，并通过答案集编程实现CNF公式的求解。

Result: 框架能够捕捉现有对比解释的最小版本，并提供了计算复杂性的详细分析。

Conclusion: 该研究为对比解释提供了理论支持和实践工具，展示了其在现实中的应用潜力。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [62] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: 本文探讨了科技进步如何重新诠释创造力哲学，重点关注AI是否具备创造力的问题，并结合心理学和认知神经科学的视角进行分析。


<details>
  <summary>Details</summary>
Motivation: 探讨科技进步对创造力哲学的影响，特别是AI是否能够表现创造力。

Method: 回顾历史观点，结合心理学和认知神经科学的进展，分析创造力的定义及自然主义与认知神经科学的回应。

Result: 研究发现AI的创造力需结合多学科视角进行重新定义。

Conclusion: 结论指出创造力的研究需跨学科合作，尤其是哲学与科学的结合。

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [63] [FreeAudio: Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation](https://arxiv.org/abs/2507.08557)
*Yuxuan Jiang,Zehua Chen,Zeqian Ju,Chang Li,Weibei Dou,Jun Zhu*

Main category: cs.SD

TL;DR: 论文提出了一种名为FreeAudio的无需训练的时序控制文本到音频生成框架，能够处理复杂时序提示并实现长格式音频生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频生成方法因数据限制，难以处理包含精确时序控制的复杂文本提示，如“猫头鹰在2.4秒-5.2秒鸣叫”。

Method: FreeAudio利用LLM规划非重叠时间窗口并重新描述每个窗口的文本，结合解耦与聚合注意力控制（精确时序控制）、上下文潜在组合（局部平滑）和参考指导（全局一致性）。

Result: FreeAudio在无需训练的方法中达到最先进的时序控制音频生成质量，并与主流训练方法相媲美；且在长格式生成质量上与Stable Audio相当。

Conclusion: FreeAudio为时序控制的长格式文本到音频合成开辟了新途径，展现出优异性能。

Abstract: Text-to-audio (T2A) generation has achieved promising results with the recent
advances in generative models. However, because of the limited quality and
quantity of temporally-aligned audio-text pairs, existing T2A methods struggle
to handle the complex text prompts that contain precise timing control, e.g.,
"owl hooted at 2.4s-5.2s". Recent works have explored data augmentation
techniques or introduced timing conditions as model inputs to enable
timing-conditioned 10-second T2A generation, while their synthesis quality is
still limited. In this work, we propose a novel training-free timing-controlled
T2A framework, FreeAudio, making the first attempt to enable timing-controlled
long-form T2A generation, e.g., "owl hooted at 2.4s-5.2s and crickets chirping
at 0s-24s". Specifically, we first employ an LLM to plan non-overlapping time
windows and recaption each with a refined natural language description, based
on the input text and timing prompts. Then we introduce: 1) Decoupling and
Aggregating Attention Control for precise timing control; 2) Contextual Latent
Composition for local smoothness and Reference Guidance for global consistency.
Extensive experiments show that: 1) FreeAudio achieves state-of-the-art
timing-conditioned T2A synthesis quality among training-free methods and is
comparable to leading training-based methods; 2) FreeAudio demonstrates
comparable long-form generation quality with training-based Stable Audio and
paves the way for timing-controlled long-form T2A synthesis. Demo samples are
available at: https://freeaudio.github.io/FreeAudio/

</details>
