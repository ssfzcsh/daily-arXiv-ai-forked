<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 11]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 11]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.DS](#cs.DS) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [math.NT](#math.NT) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Domain Knowledge in Requirements Engineering: A Systematic Mapping Study](https://arxiv.org/abs/2506.20754)
*Marina Araújo,Júlia Araújo,Romeu Oliveira,Lucas Romao,Marcos Kalinowski*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: [Context] Domain knowledge is recognized as a key component for the success
of Requirements Engineering (RE), as it provides the conceptual support needed
to understand the system context, ensure alignment with stakeholder needs, and
reduce ambiguity in requirements specification. Despite its relevance, the
scientific literature still lacks a systematic consolidation of how domain
knowledge can be effectively used and operationalized in RE. [Goal] This paper
addresses this gap by offering a comprehensive overview of existing
contributions, including methods, techniques, and tools to incorporate domain
knowledge into RE practices. [Method] We conducted a systematic mapping study
using a hybrid search strategy that combines database searches with iterative
backward and forward snowballing. [Results] In total, we found 75 papers that
met our inclusion criteria. The analysis highlights the main types of
requirements addressed, the most frequently considered quality attributes, and
recurring challenges in the formalization, acquisition, and long-term
maintenance of domain knowledge. The results provide support for researchers
and practitioners in identifying established approaches and unresolved issues.
The study also outlines promising directions for future research, emphasizing
the development of scalable, automated, and sustainable solutions to integrate
domain knowledge into RE processes. [Conclusion] The study contributes by
providing a comprehensive overview that helps to build a conceptual and
methodological foundation for knowledge-driven requirements engineering.

</details>


### [2] [Agile Management for Machine Learning: A Systematic Mapping Study](https://arxiv.org/abs/2506.20759)
*Lucas Romao,Hugo Villamizar,Romeu Oliveira,Silvio Alonso,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本文系统地分析了敏捷管理在机器学习（ML）驱动系统中的应用现状，揭示了现有框架和实践中的关键挑战和空缺。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统的动态性和传统项目管理的僵化性之间的矛盾促使研究如何有效应用敏捷方法。

Method: 通过系统化映射研究，结合数据库搜索和雪球技术，分析了2008至2024年的27篇论文。

Result: 确定了8个框架和8个主题（如迭代灵活性和最小可行模型），主要挑战是ML任务的工作量估算。

Conclusion: 研究填补了领域空白，但需要更多实证评估验证现有成果。

Abstract: [Context] Machine learning (ML)-enabled systems are present in our society,
driving significant digital transformations. The dynamic nature of ML
development, characterized by experimental cycles and rapid changes in data,
poses challenges to traditional project management. Agile methods, with their
flexibility and incremental delivery, seem well-suited to address this
dynamism. However, it is unclear how to effectively apply these methods in the
context of ML-enabled systems, where challenges require tailored approaches.
[Goal] Our goal is to outline the state of the art in agile management for
ML-enabled systems. [Method] We conducted a systematic mapping study using a
hybrid search strategy that combines database searches with backward and
forward snowballing iterations. [Results] Our study identified 27 papers
published between 2008 and 2024. From these, we identified eight frameworks and
categorized recommendations and practices into eight key themes, such as
Iteration Flexibility, Innovative ML-specific Artifacts, and the Minimal Viable
Model. The main challenge identified across studies was accurate effort
estimation for ML-related tasks. [Conclusion] This study contributes by mapping
the state of the art and identifying open gaps in the field. While relevant
work exists, more robust empirical evaluation is still needed to validate these
contributions.

</details>


### [3] [Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach](https://arxiv.org/abs/2506.20851)
*Srikar Reddy Gadusu,Larry Callahan,Samir Lababidi,Arunasri Nishtala,Sophia Healey,Hande McGinty*

Main category: cs.SE

TL;DR: 提出了一种基于Python和rdflib的用户友好方法，用于将Neo4j数据库与OWL本体语言无缝集成，适用于药物不良反应数据集的本体生成。


<details>
  <summary>Details</summary>
Motivation: 随着数据和知识的快速增长，需要系统的方法来生成本体，特别是药物安全监测领域的需求日益迫切。

Method: 使用Python及其rdflib库开发脚本，自动从Neo4j数据库中生成OWL本体的类和公理。

Result: 通过FDA的FAERS数据集验证，成功实现了用户友好的本体生成方法，支持药物安全监测。

Conclusion: 该方法解决了Neo4j与OWL集成的技术障碍，为快速增长的药物不良反应数据集提供了实用的本体生成解决方案。

Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for
ontology generation has become crucial. With the daily increases in data
volumes and frequent content changes, the demand for databases to store and
retrieve information for the creation of knowledge graphs has become
increasingly urgent. The previously established Knowledge Acquisition and
Representation Methodology (KNARM) outlines a systematic approach to address
these challenges and create knowledge graphs. However, following this
methodology highlights the existing challenge of seamlessly integrating Neo4j
databases with the Web Ontology Language (OWL). Previous attempts to integrate
data from Neo4j into an ontology have been discussed, but these approaches
often require an understanding of description logics (DL) syntax, which may not
be familiar to many users. Thus, a more accessible method is necessary to
bridge this gap. This paper presents a user-friendly approach that utilizes
Python and its rdflib library to support ontology development. We showcase our
novel approach through a Neo4j database we created by integrating data from the
Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS)
database. Using this dataset, we developed a Python script that automatically
generates the required classes and their axioms, facilitating a smoother
integration process. This approach offers a practical solution to the
challenges of ontology generation in the context of rapidly growing adverse
drug event datasets, supporting improved drug safety monitoring and public
health decision-making.

</details>


### [4] [Engineering RAG Systems for Real-World Applications: Design, Development, and Evaluation](https://arxiv.org/abs/2506.20869)
*Md Toufique Hasan,Muhammad Waseem,Kai-Kristian Kemell,Ayman Asad Khan,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 该论文研究了检索增强生成（RAG）系统在真实场景中的应用，评估了用户反馈并总结了开发经验。


<details>
  <summary>Details</summary>
Motivation: 填补RAG系统在真实用例中开发和评估的实证研究空白。

Method: 开发了五个领域特定的RAG应用，并通过Web评估（100名参与者）从六个维度评估系统。

Result: 总结了12个关键经验教训，涉及技术、操作和伦理挑战。

Conclusion: 研究为RAG系统的实际可靠性和可用性提供了重要见解。

Abstract: Retrieval-Augmented Generation (RAG) systems are emerging as a key approach
for grounding Large Language Models (LLMs) in external knowledge, addressing
limitations in factual accuracy and contextual relevance. However, there is a
lack of empirical studies that report on the development of RAG-based
implementations grounded in real-world use cases, evaluated through general
user involvement, and accompanied by systematic documentation of lessons
learned. This paper presents five domain-specific RAG applications developed
for real-world scenarios across governance, cybersecurity, agriculture,
industrial research, and medical diagnostics. Each system incorporates
multilingual OCR, semantic retrieval via vector embeddings, and domain-adapted
LLMs, deployed through local servers or cloud APIs to meet distinct user needs.
A web-based evaluation involving a total of 100 participants assessed the
systems across six dimensions: (i) Ease of Use, (ii) Relevance, (iii)
Transparency, (iv) Responsiveness, (v) Accuracy, and (vi) Likelihood of
Recommendation. Based on user feedback and our development experience, we
documented twelve key lessons learned, highlighting technical, operational, and
ethical challenges affecting the reliability and usability of RAG systems in
practice.

</details>


### [5] [Complex Model Transformations by Reinforcement Learning with Uncertain Human Guidance](https://arxiv.org/abs/2506.20883)
*Kyanna Dagenais,Istvan David*

Main category: cs.SE

TL;DR: 论文提出了一种基于强化学习（RL）并辅以不确定的人类指导的复杂模型转换（MT）序列开发方法，显著提高了RL性能。


<details>
  <summary>Details</summary>
Motivation: 模型驱动工程中复杂的模型转换（MT）序列开发易出错且难以手动完成，需要借助RL，但RL在复杂问题中表现不佳，因此引入人类指导以提升性能。

Method: 提出了一种技术框架，将用户定义的MT映射到RL原语中，并通过RL程序执行，寻找最优MT序列，同时支持不确定的人类建议。

Result: 评估表明，即使人类建议不确定，也能显著提升RL性能，并更高效地开发复杂MT序列。

Conclusion: 该方法通过权衡人类建议的确定性和及时性，推动了RL驱动的“人在环”工程方法的发展。

Abstract: Model-driven engineering problems often require complex model transformations
(MTs), i.e., MTs that are chained in extensive sequences. Pertinent examples of
such problems include model synchronization, automated model repair, and design
space exploration. Manually developing complex MTs is an error-prone and often
infeasible process. Reinforcement learning (RL) is an apt way to alleviate
these issues. In RL, an autonomous agent explores the state space through trial
and error to identify beneficial sequences of actions, such as MTs. However, RL
methods exhibit performance issues in complex problems. In these situations,
human guidance can be of high utility. In this paper, we present an approach
and technical framework for developing complex MT sequences through RL, guided
by potentially uncertain human advice. Our framework allows user-defined MTs to
be mapped onto RL primitives, and executes them as RL programs to find optimal
MT sequences. Our evaluation shows that human guidance, even if uncertain,
substantially improves RL performance, and results in more efficient
development of complex MTs. Through a trade-off between the certainty and
timeliness of human advice, our method takes a step towards RL-driven
human-in-the-loop engineering methods.

</details>


### [6] [Boosting Vulnerability Detection with Inter-function Multilateral Association Insights](https://arxiv.org/abs/2506.21014)
*Shaojian Qiu,Mengyang Huang,Jiahao Cheng*

Main category: cs.SE

TL;DR: 提出了一个基于函数间多边关联分析的漏洞检测框架IFMA-VD，通过构建代码行为超图并利用超边卷积提取多边关联特征，提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习漏洞检测方法通常忽略函数间的多边关联关系，导致无法检测相关漏洞。

Method: 首先解析函数生成代码属性图以获取函数内特征，然后构建代码行为超图，最后利用超图网络捕获多边关联知识。

Result: 在三个漏洞数据集上评估，F-measure和Recall指标优于基线方法，验证了多边关联特征对代码特征表示的有效性。

Conclusion: IFMA-VD框架有效利用了函数间的多边关联关系，显著提升了漏洞检测性能。

Abstract: Vulnerability detection is a crucial yet challenging technique for ensuring
the security of software systems. Currently, most deep learning-based
vulnerability detection methods focus on stand-alone functions, neglecting the
complex inter-function interrelations, particularly the multilateral
associations. This oversight can fail to detect vulnerabilities in these
interrelations. To address this gap, we present an Inter-Function Multilateral
Association analysis framework for Vulnerability Detection (IFMA-VD). The
cornerstone of the IFMA-VD lies in constructing a code behavior hypergraph and
utilizing hyperedge convolution to extract multilateral association features.
Specifically, we first parse functions into a code property graph to generate
intra-function features. Following this, we construct a code behavior
hypergraph by segmenting the program dependency graph to isolate and encode
behavioral features into hyperedges. Finally, we utilize a hypergraph network
to capture the multilateral association knowledge for augmenting vulnerability
detection. We evaluate IFMA-VD on three widely used vulnerability datasets and
demonstrate improvements in F-measure and Recall compared to baseline methods.
Additionally, we illustrate that multilateral association features can boost
code feature representation and validate the effectiveness of IFMA-VD on
real-world datasets.

</details>


### [7] [How Good Are Synthetic Requirements ? Evaluating LLM-Generated Datasets for AI4RE](https://arxiv.org/abs/2506.21138)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: Synthline v1是一个用于生成合成需求数据的增强产品线方法，通过优化提示策略和生成后处理提高数据质量。


<details>
  <summary>Details</summary>
Motivation: 公开标记的需求数据集短缺阻碍了AI4RE的发展，本研究旨在通过系统化方法提升合成数据的质量。

Method: 研究采用多示例提示、自动提示优化（PACE）和生成后处理（如基于相似性的筛选）来优化数据生成。

Result: 多示例提示显著提升数据实用性和多样性，PACE优化在某些任务中提升性能，但其他任务效果不一，合成数据在某些任务中超越人类数据。

Conclusion: 研究为AI4RE提供了实用方法，证明系统化合成数据生成可缓解数据集短缺问题。

Abstract: The shortage of publicly available, labeled requirements datasets remains a
major barrier to advancing Artificial Intelligence for Requirements Engineering
(AI4RE). While Large Language Models offer promising capabilities for synthetic
data generation, systematic approaches to control and optimize the quality of
generated requirements remain underexplored. This paper presents Synthline v1,
an enhanced Product Line approach for generating synthetic requirements data
that extends our earlier v0 version with advanced generation strategies and
curation techniques. We investigate four research questions assessing how
prompting strategies, automated prompt optimization, and post-generation
curation affect data quality across four classification tasks: defect
detection, functional vs. non-functional, quality vs. non-quality, and security
vs. non-security. Our evaluation shows that multi-sample prompting
significantly boosts both utility and diversity over single-sample generation,
with F1-score gains from 6 to 44 points. The use of PACE (Prompt Actor-Critic
Editing) for automated prompt optimization yields task-dependent results,
greatly improving functional classification (+32.5 points) but reducing
performance on others. Interestingly, similarity-based curation improves
diversity but often harms classification performance, indicating that some
redundancy may help ML models. Most importantly, our results show that
synthetic requirements can match or outperform human-authored ones for specific
tasks, with synthetic data surpassing human data for security (+7.8 points) and
defect classification (+15.4 points). These findings offer practical insights
for AI4RE and chart a viable path to mitigating dataset scarcity through
systematic synthetic generation.

</details>


### [8] [$T^3$: Multi-level Tree-based Automatic Program Repair with Large Language Models](https://arxiv.org/abs/2506.21211)
*Quanming Liu,Xupeng Bu,Zhichao Yan,Ru Li*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Automatic Program Repair (APR) is a core technology in software development
and maintenance, with aims to enable automated defect repair with minimal human
intervention. In recent years, the substantial advancements in Large Language
Models (LLMs) and the Chain-of-Thought (CoT) techniques have significantly
enhanced the reasoning capabilities of these models. However, due to the
complex logic and multi-step reasoning ability needed, the application of CoT
techniques in the APR domain remains insufficient. This study systematically
evaluates the performance of several common CoT techniques in APR tasks and
proposes an innovative framework $T^3$, which integrates the powerful reasoning
capabilities of LLMs with tree search, effectively improving the precision of
generating candidate repair solutions. Furthermore, $T^3$ provides valuable
guidance for optimizing sample selection and repair strategies in APR tasks,
establishing a robust framework for achieving efficient automated debugging.

</details>


### [9] [KOALA: a Configurable Tool for Collecting IDE Data When Solving Programming Tasks](https://arxiv.org/abs/2506.21266)
*Daniil Karol,Elizaveta Artser,Ilya Vlasov,Yaroslav Golubev,Hieke Keuning,Anastasiia Birillo*

Main category: cs.SE

TL;DR: KOALA是一个高度可配置的工具，用于在JetBrains IDE中收集学生编程任务的代码快照和功能使用数据，解决了现有工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有数据收集工具在代码粒度、编程环境事件收集和配置灵活性方面存在不足，需要更高效的工具。

Method: 开发KOALA插件，安装在IDE中，可配置任务、功能开关和调查，收集代码快照、IDE操作及额外数据，并存储在服务器中。

Result: 收集了28名学生编程任务的数据，展示了一些有价值的见解，数据可转换为标准ProgSnap2格式。

Conclusion: KOALA为教育者和研究者提供了更灵活、全面的数据收集工具，支持更深入的学习分析。

Abstract: Collecting data of students solving programming tasks is incredibly valuable
for researchers and educators. It allows verifying that the students correctly
apply the features and concepts they are taught, or finding students'
misconceptions. However, existing data collection tools have limitations, e.g.,
no control over the granularity of the collected code, not collecting the
specific events of the programming environment used, and overall being hard to
configure.
  To overcome these limitations, we propose KOALA, a convenient and highly
configurable tool for collecting code snapshots and feature usage from students
solving programming tasks in JetBrains IDEs. The plugin can be installed in
IDEs and configured to provide the students with the necessary tasks, enable or
disable certain IDE features like code completion, and run surveys. During
problem solving, the plugin collects code snapshots at the configured
granularity, all IDE actions like running and debugging, as well as some data
not collected in prior works, like employed hotkeys and switching focus between
files. The collected data is sent to the server that comes with the tool, where
it is stored and can be converted to the standardized ProgSnap2 format. To
showcase the tool, we collected data from 28 students solving tasks in two
courses within the IDE, highlighting some insights from this data.

</details>


### [10] [Exploring Micro Frontends: A Case Study Application in E-Commerce](https://arxiv.org/abs/2506.21297)
*Ricardo Hideki Hangai Kojo,Luiz Fernando Corte Real,Renato Cordeiro Ferreira,Thatiane de Oliveira Rosa,Alfredo Goldman*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the micro frontends architectural style, the frontend is divided into
smaller components, which can range from a simple button to an entire page. The
goal is to improve scalability, resilience, and team independence, albeit at
the cost of increased complexity and infrastructure demands. This paper seeks
to understand when it is worth adopting micro frontends, particularly in the
context of industry. To achieve this, we conducted an investigation into the
state of the art of micro frontends, based on both academic and gray
literature. We then implemented this architectural style in a marketplace for
handcrafted products, which already used microservices. Finally, we evaluated
the implementation through a semi-open questionnaire with the developers. At
the studied marketplace company, the need for architectural change arose due to
the tight coupling between their main system (a Java monolith) and a dedicated
frontend system. Additionally, there were deprecated technologies and poor
developer experience. To address these issues, the micro frontends architecture
was adopted, along with the API Gateway and Backend for Frontend patterns, and
technologies such as Svelte and Fastify. Although the adoption of Micro
Frontends was successful, it was not strictly necessary to meet the company's
needs. According to the analysis of the mixed questionnaire responses, other
alternatives, such as a monolithic frontend, could have achieved comparable
results. What made adopting micro frontends the most convenient choice in the
company's context was the monolith strangulation and microservices adoption,
which facilitated implementation through infrastructure reuse and knowledge
sharing between teams.

</details>


### [11] [An object-centric core metamodel for IoT-enhanced event logs](https://arxiv.org/abs/2506.21300)
*Yannis Bertrand,Christian Imenkamp,Lukas Malburg,Matthias Ehrendorfer,Marco Franceschetti,Joscha Grüger,Francesco Leotta,Jürgen Mangler,Ronny Seiger,Agnes Koschmider,Stefanie Rinderle-Ma,Barbara Weber,Estefania Serral*

Main category: cs.SE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Advances in Internet-of-Things (IoT) technologies have prompted the
integration of IoT devices with business processes (BPs) in many organizations
across various sectors, such as manufacturing, healthcare and smart spaces. The
proliferation of IoT devices leads to the generation of large amounts of IoT
data providing a window on the physical context of BPs, which facilitates the
discovery of new insights about BPs using process mining (PM) techniques.
However, to achieve these benefits, IoT data need to be combined with
traditional process (event) data, which is challenging due to the very
different characteristics of IoT and process data, for instance in terms of
granularity levels. Recently, several data models were proposed to integrate
IoT data with process data, each focusing on different aspects of data
integration based on different assumptions and requirements. This fragmentation
hampers data exchange and collaboration in the field of PM, e.g., making it
tedious for researchers to share data. In this paper, we present a core model
synthesizing the most important features of existing data models. As the core
model is based on common requirements, it greatly facilitates data sharing and
collaboration in the field. A prototypical Python implementation is used to
evaluate the model against various use cases and demonstrate that it satisfies
these common requirements.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [12] [Drift-Adaptive Slicing-Based Resource Management for Cooperative ISAC Networks](https://arxiv.org/abs/2506.20762)
*Shisheng Hu,Jie Gao,Xue Qin,Conghao Zhou,Xinyu Huang,Mushu Li,Mingcheng He,Xuemin Shen*

Main category: cs.NI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we propose a novel drift-adaptive slicing-based resource
management scheme for cooperative integrated sensing and communication (ISAC)
networks. Particularly, we establish two network slices to provide sensing and
communication services, respectively. In the large-timescale planning for the
slices, we partition the sensing region of interest (RoI) of each mobile device
and reserve network resources accordingly, facilitating low-complexity
distance-based sensing target assignment in small timescales. To cope with the
non-stationary spatial distributions of mobile devices and sensing targets,
which can result in the drift in modeling the distributions and ineffective
planning decisions, we construct digital twins (DTs) of the slices. In each DT,
a drift-adaptive statistical model and an emulation function are developed for
the spatial distributions in the corresponding slice, which facilitates
closed-form decision-making and efficient validation of a planning decision,
respectively. Numerical results show that the proposed drift-adaptive
slicing-based resource management scheme can increase the service satisfaction
ratio by up to 18% and reduce resource consumption by up to 13.1% when compared
with benchmark schemes.

</details>


### [13] [Flowcut Switching: High-Performance Adaptive Routing with In-Order Delivery Guarantees](https://arxiv.org/abs/2506.21406)
*Tommaso Bonato,Daniele De Sensi,Salvatore Di Girolamo,Abdulla Bataineh,David Hewson,Duncan Roweth,Torsten Hoefler*

Main category: cs.NI

TL;DR: 提出了一种新的自适应路由算法flowcut switching，确保网络数据包顺序传输，适用于各种网络条件，尤其适合RDMA等非突发性流量。


<details>
  <summary>Details</summary>
Motivation: 网络延迟严重影响超级计算机应用的性能，现有的自适应路由算法可能导致数据包乱序，影响TCP、QUIC和RoCE等协议的性能。

Method: 提出flowcut switching算法，不同于现有的flowlet switching，能保证任何网络条件下的数据包顺序传输。

Result: flowcut switching在非突发性流量（如RDMA）中表现优异，确保高性能的顺序传输。

Conclusion: flowcut switching是一种高效的自适应路由算法，适用于广泛网络条件，解决了乱序问题。

Abstract: Network latency severely impacts the performance of applications running on
supercomputers. Adaptive routing algorithms route packets over different
available paths to reduce latency and improve network utilization. However, if
a switch routes packets belonging to the same network flow on different paths,
they might arrive at the destination out-of-order due to differences in the
latency of these paths. For some transport protocols like TCP, QUIC, and RoCE,
out-of-order (OOO) packets might cause large performance drops or significantly
increase CPU utilization. In this work, we propose flowcut switching, a new
adaptive routing algorithm that provides high-performance in-order packet
delivery. Differently from existing solutions like flowlet switching, which are
based on the assumption of bursty traffic and that might still reorder packets,
flowcut switching guarantees in-order delivery under any network conditions,
and is effective also for non-bursty traffic, as it is often the case for RDMA.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [14] [E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News Verification via MLLMs](https://arxiv.org/abs/2506.20944)
*Van-Hoang Phan,Long-Khanh Pham,Dang Vu,Anh-Duy Tran,Minh-Son Dao*

Main category: cs.MM

TL;DR: 提出一种无需训练的检索式多模态事实验证系统，利用预训练视觉-语言模型和大型语言模型评估信息可信度，实现轻量级边缘设备集成。


<details>
  <summary>Details</summary>
Motivation: 解决移动和无线网络中虚假信息快速传播的安全挑战，避免传统训练模型易受对抗攻击和数据污染的缺陷。

Method: 通过动态检索和交叉引用可信数据源，结合预训练视觉-语言和大型语言模型进行事实验证。

Result: 在两个事实核查基准测试上取得SOTA效果，验证了其在虚假信息检测和对抗攻击鲁棒性上的高效表现。

Conclusion: 该系统展示了提升移动和无线通信环境安全的潜力，尤其适合轻量级边缘设备部署。

Abstract: The rapid spread of misinformation in mobile and wireless networks presents
critical security challenges. This study introduces a training-free,
retrieval-based multimodal fact verification system that leverages pretrained
vision-language models and large language models for credibility assessment. By
dynamically retrieving and cross-referencing trusted data sources, our approach
mitigates vulnerabilities of traditional training-based models, such as
adversarial attacks and data poisoning. Additionally, its lightweight design
enables seamless edge device integration without extensive on-device
processing. Experiments on two fact-checking benchmarks achieve SOTA results,
confirming its effectiveness in misinformation detection and its robustness
against various attack vectors, highlighting its potential to enhance security
in mobile and wireless communication environments.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [15] [Pebble Games and Algebraic Proof Systems](https://arxiv.org/abs/2506.21149)
*Lisa-Marie Jaser,Jacobo Toran*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Analyzing refutations of the well known 0pebbling formulas Peb$(G)$ we prove
some new strong connections between pebble games and algebraic proof system,
showing that there is a parallelism between the reversible, black and
black-white pebbling games on one side, and the three algebraic proof systems
Nullstellensatz, Monomial Calculus and Polynomial Calculus on the other side.
In particular we prove that for any DAG $G$ with a single sink, if there is a
Monomial Calculus refutation for Peb$(G)$ having simultaneously degree $s$ and
size $t$ then there is a black pebbling strategy on $G$ with space $s$ and time
$t+s$. Also if there is a black pebbling strategy for $G$ with space $s$ and
time $t$ it is possible to extract from it a MC refutation for Peb$(G)$ having
simultaneously degree $s$ and size $ts$. These results are analogous to those
proven in {deRezende et al.21} for the case of reversible pebbling and
Nullstellensatz. Using them we prove degree separations between NS, MC and PC,
as well as strong degree-size tradeoffs for MC.
  We also notice that for any directed acyclic graph $G$ the space needed in a
pebbling strategy on $G$, for the three versions of the game, reversible, black
and black-white, exactly matches the variable space complexity of a refutation
of the corresponding pebbling formula Peb$(G)$ in each of the algebraic proof
systems NS, MC and PC. Using known pebbling bounds on graphs, this connection
implies separations between the corresponding variable space measures.

</details>


### [16] [Deciding Robust Instances of an Escape Problem for Dynamical Systems in Euclidean Space](https://arxiv.org/abs/2506.21481)
*Eike Neumann*

Main category: cs.LO

TL;DR: 论文研究了在实数计算的位模型中，连续映射下点是否能逃逸闭子集的问题，提出了一种部分决策方法，并对特定函数族进行了验证。


<details>
  <summary>Details</summary>
Motivation: 研究在实数计算的位模型中，连续映射下点逃逸闭子集问题的可判定性，旨在提供一种通用的决策方法。

Method: 提出了一种部分决策方法，该方法在问题实例的回答对所有足够小的函数扰动具有鲁棒性时终止，适用于一般连续函数。

Result: 算法对问题实例的终止集具有密度性，并在特定函数族（如仿射线性系统和二次复多项式）中验证了完备性。

Conclusion: 该研究为连续映射下的逃逸问题提供了通用的决策方法，并在特定函数族中展示了完备性，对计算机领域有重要意义。

Abstract: We study the problem of deciding whether a point escapes a closed subset of
$\mathbb{R}^d$ under the iteration of a continuous map $f \colon \mathbb{R}^d
\to \mathbb{R}^d$ in the bit-model of real computation. We give a sound partial
decision method for this problem which is complete in the sense that its
halting set contains the halting set of all sound partial decision methods for
the problem. Equivalently, our decision method terminates on all problem
instances whose answer is robust under all sufficiently small perturbations of
the function. We further show that the halting set of our algorithm is dense in
the set of all problem instances. While our algorithm applies to general
continuous functions, we demonstrate that it also yields complete decision
methods for much more rigid function families: affine linear systems and
quadratic complex polynomials. In the latter case, completeness is subject to
the density of hyperbolicity conjecture in complex dynamics. This in particular
yields an alternative proof of Hertling's (2004) conditional answer to a
question raised by Penrose (1989) regarding the computability of the Mandelbrot
set.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [17] [Exploring the Effects of Chatbot Anthropomorphism and Human Empathy on Human Prosocial Behavior Toward Chatbots](https://arxiv.org/abs/2506.20748)
*Jingshu Li,Zicheng Zhu,Renwen Zhang,Yi-Chieh Lee*

Main category: cs.HC

TL;DR: 研究探讨了聊天机器人的人形化（身份、情感表达和非语言表达）如何通过激发人类同理心，促使其对聊天机器人表现出亲社会行为和意图，并分析其动机。


<details>
  <summary>Details</summary>
Motivation: 填补人类为何帮助聊天机器人的研究空白，理解聊天机器人的人形化特征对人类行为的影响。

Method: 基于CASA框架，通过在线实验（N=244）测量参与者在协作任务中对聊天机器人的亲社会行为和意图，并分析同理心中介作用。

Result: 人类身份和情感表达的聊天机器人显著增加用户的亲社会行为和意图，同理心是中介因素；定性分析揭示了同理心和拟人化感知是主要动机。

Conclusion: 聊天机器人的人形化特征可通过激发同理心促进人类的亲社会行为，这对设计和推广聊天机器人具有重要启示。

Abstract: Chatbots are increasingly integrated into people's lives and are widely used
to help people. Recently, there has also been growing interest in the reverse
direction-humans help chatbots-due to a wide range of benefits including better
chatbot performance, human well-being, and collaborative outcomes. However,
little research has explored the factors that motivate people to help chatbots.
To address this gap, we draw on the Computers Are Social Actors (CASA)
framework to examine how chatbot anthropomorphism-including human-like
identity, emotional expression, and non-verbal expression-influences human
empathy toward chatbots and their subsequent prosocial behaviors and
intentions. We also explore people's own interpretations of their prosocial
behaviors toward chatbots. We conducted an online experiment (N = 244) in which
chatbots made mistakes in a collaborative image labeling task and explained the
reasons to participants. We then measured participants' prosocial behaviors and
intentions toward the chatbots. Our findings revealed that human identity and
emotional expression of chatbots increased participants' prosocial behavior and
intention toward chatbots, with empathy mediating these effects. Qualitative
analysis further identified two motivations for participants' prosocial
behaviors: empathy for the chatbot and perceiving the chatbot as human-like. We
discuss the implications of these results for understanding and promoting human
prosocial behaviors toward chatbots.

</details>


### [18] ["TikTok, Do Your Thing": User Reactions to Social Surveillance in the Public Sphere](https://arxiv.org/abs/2506.20884)
*Meira Gilbert,Miranda Wei,Lindah Kotut*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: ''TikTok, Do Your Thing'' is a viral trend where users attempt to identify
strangers they see in public via information crowd-sourcing. The trend started
as early as 2021 and users typically engage with it for romantic purposes
(similar to a ''Missed Connections'' personal advertisement). This practice
includes acts of surveillance and identification in the public sphere, although
by peers rather than governments or corporations. To understand users'
reactions to this trend we conducted a qualitative analysis of 60 TikTok videos
and 1,901 user comments. Of the 60 videos reviewed, we find 19 individuals were
successfully identified. We also find that while there were comments expressing
disapproval (n=310), more than double the number expressed support (n=883).
Supportive comments demonstrated genuine interest and empathy, reflecting
evolving conceptions of community and algorithmic engagement. On the other
hand, disapproving comments highlighted concerns about inappropriate
relationships, stalking, consent, and gendered double standards. We discuss
these insights in relation to the normalization of interpersonal surveillance,
online stalking, and as an evolution of social surveillance to offer a new
perspective on user perceptions surrounding interpersonal surveillance and
identification in the public sphere.

</details>


### [19] [Effect of Haptic Feedback on Avoidance Behavior and Visual Exploration in Dynamic VR Pedestrian Environment](https://arxiv.org/abs/2506.20952)
*Kyosuke Ishibashi,Atsushi Saito,Zin Y. Tun,Lucas Ray,Megan C. Coram,Akihiro Sakurai,Allison M. Okamura,Ko Yamamoto*

Main category: cs.HC

TL;DR: 研究了触觉反馈对VR中密集动态人流中用户行走行为的影响，发现触觉反馈改变了用户的避碰动作，增强了碰撞敏感度。


<details>
  <summary>Details</summary>
Motivation: 探索触觉反馈在虚拟现实密集人流中如何影响用户行走行为，以提升紧急疏散训练和建筑布局评估的应用效果。

Method: 通过用户研究，分析触觉反馈对用户在VR中行走轨迹、骨盆角度及速度变化的影响。

Result: 触觉反馈增加了避碰轨迹长度和骨盆角度变化，提高了对NPC碰撞的瞬时反应，并增强了视觉探索意识。

Conclusion: 触觉反馈能提升用户在VR环境中的碰撞敏感性，优化行走行为模拟。

Abstract: Human crowd simulation in virtual reality (VR) is a powerful tool with
potential applications including emergency evacuation training and assessment
of building layout. While haptic feedback in VR enhances immersive experience,
its effect on walking behavior in dense and dynamic pedestrian flows is
unknown. Through a user study, we investigated how haptic feedback changes user
walking motion in crowded pedestrian flows in VR. The results indicate that
haptic feedback changed users' collision avoidance movements, as measured by
increased walking trajectory length and change in pelvis angle. The
displacements of users' lateral position and pelvis angle were also increased
in the instantaneous response to a collision with a non-player character (NPC),
even when the NPC was inside the field of view. Haptic feedback also enhanced
users' awareness and visual exploration when an NPC approached from the side
and back. Furthermore, variation in walking speed was increased by the haptic
feedback. These results suggested that the haptic feedback enhanced users'
sensitivity to a collision in VR environment.

</details>


### [20] [Follow the user meaningfully and product growth will follow: A mixed methods case study tying UX Point of View & Growth leading to measurable impact](https://arxiv.org/abs/2506.21195)
*Neha Raghuvanshi*

Main category: cs.HC

TL;DR: 案例研究展示了用户体验研究(UXR)和数据科学团队如何通过混合方法研究战略性地推动产品主导增长(PLG)，实现了用户、团队和业务的多赢。


<details>
  <summary>Details</summary>
Motivation: 探究跨职能团队如何在最大化用户价值和推动业务增长之间取得平衡，以实现双赢。

Method: 采用混合方法研究（结合用户体验研究和数据科学），战略性地影响产品主导增长(PLG)。

Result: 成功推动密码管理器产品的增长，百万级用户受益，实现用户价值和业务目标的双赢。

Conclusion: 混合方法研究是跨职能团队协作、最大化用户价值并实现业务目标的有效策略。

Abstract: Have you wondered how cross-functional teams balance between maximizing value
that users derive and business growth leading to win-win situations? This case
study shows how User Experience Research (UXR) and Data Science teams used
mixed methods research to strategically influence Product Led Growth (PLG) for
a Password Manager used by million+ users, thus allowing our users, internal
teams, and business to win. The audience will take away practical
lessons/techniques related to leveraging mixed methods to: a. Maximize user
value while meeting business growth goals b. Influence cross-functional teams
c. Measure user and business impact This case study can be easily tied to the
UXR Point of view pyramid (POV) [2] that represents a methodological approach
to construct a POV and further dives into actioning POV to create measurable
user and business impact.

</details>


### [21] [Subtitled Media Adaptations for People with Aphasia: Ongoing Accessibility Barriers and Emerging Design Practices](https://arxiv.org/abs/2506.21201)
*Zihao You,Michael Crabb*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The consumption of subtitles via TVs, laptops and smartphones has the
potential to marginalize people based on their complex accessibility needs. The
current one-size-fits-all approach to this accessibility aid is no longer fit
for purpose and work is required to look at how it can be adapted to be
personalised for individual users based on individual context, content, and
consumption habits. People with Aphasia, for example, encounter significant
challenges in understanding subtitle texts.
  We see our work as a call to action for more inclusive practices, focusing on
how the thoughts and opinions of people with aphasia can be included in media
research. Our work investigates how to develop future media solutions for
people with aphasia to create a more inclusive media viewing environment. We
believe the key to this is appropriate prototyping tools and methods to allow
equitable inclusion in the system design process.

</details>


### [22] [Multimodal LLMs for Visualization Reconstruction and Understanding](https://arxiv.org/abs/2506.21319)
*Can Liu,Chunlin Da,Xiaoxiao Long,Yuxiao Yang,Yu Zhang,Yong Wang*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visualizations are crucial for data communication, yet understanding them
requires comprehension of both visual elements and their underlying data
relationships. Current multimodal large models, while effective in natural
image understanding, struggle with visualization due to their inability to
decode the data-to-visual mapping rules and extract structured information. To
address these challenges, we present a novel dataset and train multimodal
visualization LLMs specifically designed for understanding. Our approach
combines chart images with their corresponding vectorized representations,
encoding schemes, and data features. The proposed vector format enables compact
and accurate reconstruction of visualization content. Experimental results
demonstrate significant improvements in both data extraction accuracy and chart
reconstruction quality.

</details>


### [23] [An evaluation of level of detail degradation in head-mounted display peripheries](https://arxiv.org/abs/2506.21441)
*Benjamin Watson,Neff Walker,Larry F Hodges,Martin Reddy*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A paradigm for the design of systems that manage level of detail in virtual
environments is proposed. As an example of the prototyping step in this
paradigm, a user study was performed to evaluate the effectiveness of high
detail insets used with head-mounted displays. Ten subjects were given a simple
search task that required the location and identification of a single target
object. All subjects used seven different displays (the independent variable),
varying in inset size and peripheral detail, to perform this task. Frame rate,
target location, subject input method, and order of display use were all
controlled. Primary dependent measures were search time on trials with correct
identification, and the percentage of all trials correctly identified. ANOVAs
of the results showed that insetless, high detail displays did not lead to
significantly different search times or accuracies than displays with insets.
In fact, only the insetless, low detail display returned significantly
different results. Further research is being performed to examine the effect of
varying task complexity, inset size, and level of detail.

</details>


### [24] ["Who Should I Believe?": User Interpretation and Decision-Making When a Family Healthcare Robot Contradicts Human Memory](https://arxiv.org/abs/2506.21322)
*Hong Wang,Natalia Calvo-Barajas,Katie Winkle,Ginevra Castellano*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Advancements in robotic capabilities for providing physical assistance,
psychological support, and daily health management are making the deployment of
intelligent healthcare robots in home environments increasingly feasible in the
near future. However, challenges arise when the information provided by these
robots contradicts users' memory, raising concerns about user trust and
decision-making. This paper presents a study that examines how varying a
robot's level of transparency and sociability influences user interpretation,
decision-making and perceived trust when faced with conflicting information
from a robot. In a 2 x 2 between-subjects online study, 176 participants
watched videos of a Furhat robot acting as a family healthcare assistant and
suggesting a fictional user to take medication at a different time from that
remembered by the user. Results indicate that robot transparency influenced
users' interpretation of information discrepancies: with a low transparency
robot, the most frequent assumption was that the user had not correctly
remembered the time, while with the high transparency robot, participants were
more likely to attribute the discrepancy to external factors, such as a partner
or another household member modifying the robot's information. Additionally,
participants exhibited a tendency toward overtrust, often prioritizing the
robot's recommendations over the user's memory, even when suspecting system
malfunctions or third-party interference. These findings highlight the impact
of transparency mechanisms in robotic systems, the complexity and importance
associated with system access control for multi-user robots deployed in home
environments, and the potential risks of users' over reliance on robots in
sensitive domains such as healthcare.

</details>


### [25] [Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles](https://arxiv.org/abs/2506.21456)
*Benjamin Watson,Neff Walker,Larry F Hodges*

Main category: cs.HC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Previous work has demonstrated the utility of reductions in the level of
detail (LOD) in the periphery of head-tracked, large field of view displays.
This paper provides a psychophysically based model, centered around an eye/head
movement tradeoff, that explains the effectiveness of peripheral degradation
and suggests how peripherally degraded displays should be designed. An
experiment evaluating the effect on search performance of the shape and area of
the high detail central area (inset) in peripherally degraded displays was
performed, results indicated that inset shape is not a significant factor in
performance. Inset area, however, was significant: performance with displays
subtending at least 30 degrees of horizontal and vertical angle was not
significantly different from performance with an undegraded display. These
results agreed with the proposed model.

</details>


### [26] [A Systematic Review of Human-AI Co-Creativity](https://arxiv.org/abs/2506.21333)
*Saloni Singh,Koen Hndriks,Drik Heylen,Kim Baraka*

Main category: cs.HC

TL;DR: 本文通过系统性文献综述分析了62篇关于协同创意系统的论文，总结了关键设计维度和24条设计建议，发现高用户控制、适应性强的主动系统能提升创意成果的满意度与信任。


<details>
  <summary>Details</summary>
Motivation: 支持开发更复杂、定制化的协同创意系统，从现有研究中提取设计维度和建议。

Method: 对62篇关于协同创意系统的论文进行系统性文献综述，分析应用场景和设计维度。

Result: 识别出关键设计维度（如创意阶段、用户控制等），提出24条设计建议；高用户控制提升满意度，适应性系统增强协作。

Conclusion: 协同创意系统需注重用户控制和主动适应性，但仍存在早期创意阶段支持不足等挑战。

Abstract: The co creativity community is making significant progress in developing more
sophisticated and tailored systems to support and enhance human creativity.
Design considerations from prior work can serve as a valuable and efficient
foundation for future systems. To support this effort, we conducted a
systematic literature review of 62 papers on co-creative systems. These papers
cover a diverse range of applications, including visual arts, design, and
writing, where the AI acts not just as a tool but as an active collaborator in
the creative process. From this review, we identified several key dimensions
relevant to system design: phase of the creative process, creative task,
proactive behavior of the system, user control, system embodiment, and AI model
type. Our findings suggest that systems offering high user control lead to
greater satisfaction, trust, and a stronger sense of ownership over creative
outcomes. Furthermore, proactive systems, when adaptive and context sensitive,
can enhance collaboration. We also extracted 24 design considerations,
highlighting the value of encouraging users to externalize their thoughts and
of increasing the system's social presence and transparency to foster trust.
Despite recent advancements, important gaps remain, such as limited support for
early creative phases like problem clarification, and challenges related to
user adaptation to AI systems.

</details>


### [27] [Lightweight Fingernail Haptic Device: Unobstructed Fingerpad Force and Vibration Feedback for Enhanced Virtual Dexterous Manipulation](https://arxiv.org/abs/2506.21417)
*Yunxiu Xu,Siyu Wang,Shoichi Hasegawa*

Main category: cs.HC

TL;DR: 一种轻量级可穿戴指尖触觉设备，通过物理引擎提供触觉反馈，提升虚拟环境中的操作效率，同时不干扰现实世界活动。


<details>
  <summary>Details</summary>
Motivation: 设计一种轻量、不阻碍现实交互的触觉设备，以提升虚拟环境中的灵巧操控体验。

Method: 使用细绳和致动器附着于指甲，设备重量仅1.55克/指，通过物理引擎实现多种触觉反馈（握力、碰撞和滑动振动）。

Result: 用户能感知并响应压力与振动反馈，实验证明设备显著提升虚拟任务效率。

Conclusion: 该设备在轻量化、触觉反馈和日常佩戴性之间取得了平衡，为触觉界面设计提供了新思路。

Abstract: This study presents a lightweight, wearable fingertip haptic device that
provides physics-based haptic feedback for dexterous manipulation in virtual
environments without hindering real-world interactions. The device, designed
with thin strings and actuators attached to the fingernails, ensures minimal
weight (1.55 g per finger) and preserves finger flexibility. Integrating the
software with a physics engine renders multiple types of haptic feedback (grip
force, collision, and sliding vibration feedback). We evaluated the device's
performance in pressure perception, slip feedback, typical dexterous
manipulation tasks, and daily operations, and we gathered user experience
through subjective assessments. Our results show that participants could
perceive and respond to pressure and vibration feedback. Through dexterous
manipulation experiments, we further demonstrated that these minimal haptic
cues significantly improved virtual task efficiency, showcasing how lightweight
haptic feedback can enhance manipulation performance without complex
mechanisms. The device's ability to preserve tactile sensations and minimize
hindrance to real-world operations is a key advantage over glove-type haptic
devices. This research offers a potential solution for designing haptic
interfaces that balance lightweight construction, haptic feedback for dexterous
manipulation, and daily wearability.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [28] [Generative Blocks World: Moving Things Around in Pictures](https://arxiv.org/abs/2506.20703)
*Vaibhav Vavilala,Seemandhar Jain,Rahul Vasanth,D. A. Forsyth,Anand Bhattad*

Main category: cs.GR

TL;DR: 提出了一种基于3D几何图元的交互式图像编辑方法，通过流式生成技术实现高保真度和编辑性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过简单几何抽象与生成图像的交互，提升编辑灵活性和效果保真度。

Method: 使用凸3D图元表示场景，结合深度和纹理提示的流式生成技术编辑图像。

Result: 在视觉保真度、编辑性和组合泛化性上优于现有方法。

Conclusion: 几何抽象与流式生成结合的方法在交互式图像编辑中表现优异。

Abstract: We describe Generative Blocks World to interact with the scene of a generated
image by manipulating simple geometric abstractions. Our method represents
scenes as assemblies of convex 3D primitives, and the same scene can be
represented by different numbers of primitives, allowing an editor to move
either whole structures or small details. Once the scene geometry has been
edited, the image is generated by a flow-based method which is conditioned on
depth and a texture hint. Our texture hint takes into account the modified 3D
primitives, exceeding texture-consistency provided by existing key-value
caching techniques. These texture hints (a) allow accurate object and camera
moves and (b) largely preserve the identity of objects depicted. Quantitative
and qualitative experiments demonstrate that our approach outperforms prior
works in visual fidelity, editability, and compositional generalization.

</details>


### [29] [3DGH: 3D Head Generation with Composable Hair and Face](https://arxiv.org/abs/2506.20875)
*Chengan He,Junxuan Li,Tobias Kirschstein,Artem Sevastopolsky,Shunsuke Saito,Qingyang Tan,Javier Romero,Chen Cao,Holly Rushmeier,Giljoo Nam*

Main category: cs.GR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present 3DGH, an unconditional generative model for 3D human heads with
composable hair and face components. Unlike previous work that entangles the
modeling of hair and face, we propose to separate them using a novel data
representation with template-based 3D Gaussian Splatting, in which deformable
hair geometry is introduced to capture the geometric variations across
different hairstyles. Based on this data representation, we design a 3D
GAN-based architecture with dual generators and employ a cross-attention
mechanism to model the inherent correlation between hair and face. The model is
trained on synthetic renderings using carefully designed objectives to
stabilize training and facilitate hair-face separation. We conduct extensive
experiments to validate the design choice of 3DGH, and evaluate it both
qualitatively and quantitatively by comparing with several state-of-the-art 3D
GAN methods, demonstrating its effectiveness in unconditional full-head image
synthesis and composable 3D hairstyle editing. More details will be available
on our project page: https://c-he.github.io/projects/3dgh/.

</details>


### [30] [Data Visualization for Improving Financial Literacy: A Systematic Review](https://arxiv.org/abs/2506.20901)
*Meng Du,Robert Amor,Kwan-Liu Ma,Burkhard C. Wünsche*

Main category: cs.GR

TL;DR: 本文探讨了数据可视化在金融教育中的应用，通过分析37篇研究论文，分类为五个关键领域，并提出研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: 金融素养对个人财务决策至关重要，但许多人对金融概念感到困惑。数据可视化可以简化这些概念，提高学习效果。

Method: 系统综述了37篇研究论文，分类为五个领域：可视化的时空演变、工具动机、金融主题与教学方法、工具技术类型以及教学干预效果评估。

Result: 研究发现可视化工具在金融教育中具有潜力，并指出了当前研究的不足之处。

Conclusion: 研究为教育者和专业人士提供了实用建议，并呼吁进一步探索金融素养提升的可视化工具。

Abstract: Financial literacy empowers individuals to make informed and effective
financial decisions, improving their overall financial well-being and security.
However, for many people understanding financial concepts can be daunting and
only half of US adults are considered financially literate. Data visualization
simplifies these concepts, making them accessible and engaging for learners of
all ages. This systematic review analyzes 37 research papers exploring the use
of data visualization and visual analytics in financial education and literacy
enhancement. We classify these studies into five key areas: (1) the evolution
of visualization use across time and space, (2) motivations for using
visualization tools, (3) the financial topics addressed and instructional
approaches used, (4) the types of tools and technologies applied, and (5) how
the effectiveness of teaching interventions was evaluated. Furthermore, we
identify research gaps and highlight opportunities for advancing financial
literacy. Our findings offer practical insights for educators and professionals
to effectively utilize or design visual tools for financial literacy.

</details>


### [31] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: VideoTex利用视频生成模型解决3D纹理合成中的空间和时间不一致问题，通过几何感知条件和UV扩散策略，实现了更平滑、更一致的纹理生成。


<details>
  <summary>Details</summary>
Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解而导致不一致；视频生成模型的成功为纹理合成提供了新思路。

Method: 结合几何感知条件和结构级UV扩散策略，利用3D网格结构和视频生成模型，提升纹理合成的时空一致性。

Result: 实验表明，VideoTex在纹理保真度、接缝融合和稳定性上优于现有方法。

Conclusion: VideoTex为动态实时应用提供了高质量且时间稳定的纹理合成解决方案。

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>


### [32] [FairyGen: Storied Cartoon Video from a Single Child-Drawn Character](https://arxiv.org/abs/2506.21272)
*Jiayi Zheng,Xiaodong Cun*

Main category: cs.GR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose FairyGen, an automatic system for generating story-driven cartoon
videos from a single child's drawing, while faithfully preserving its unique
artistic style. Unlike previous storytelling methods that primarily focus on
character consistency and basic motion, FairyGen explicitly disentangles
character modeling from stylized background generation and incorporates
cinematic shot design to support expressive and coherent storytelling. Given a
single character sketch, we first employ an MLLM to generate a structured
storyboard with shot-level descriptions that specify environment settings,
character actions, and camera perspectives. To ensure visual consistency, we
introduce a style propagation adapter that captures the character's visual
style and applies it to the background, faithfully retaining the character's
full visual identity while synthesizing style-consistent scenes. A shot design
module further enhances visual diversity and cinematic quality through frame
cropping and multi-view synthesis based on the storyboard. To animate the
story, we reconstruct a 3D proxy of the character to derive physically
plausible motion sequences, which are then used to fine-tune an MMDiT-based
image-to-video diffusion model. We further propose a two-stage motion
customization adapter: the first stage learns appearance features from
temporally unordered frames, disentangling identity from motion; the second
stage models temporal dynamics using a timestep-shift strategy with frozen
identity weights. Once trained, FairyGen directly renders diverse and coherent
video scenes aligned with the storyboard. Extensive experiments demonstrate
that our system produces animations that are stylistically faithful,
narratively structured natural motion, highlighting its potential for
personalized and engaging story animation. The code will be available at
https://github.com/GVCLab/FairyGen

</details>


### [33] [IDGraphs: Intrusion Detection and Analysis Using Stream Compositing](https://arxiv.org/abs/2506.21425)
*Pin Ren,Yan Gao,Zhichun Li,Yan Chen,Benjamin Watson*

Main category: cs.GR

TL;DR: IDGraphs是一种用于入侵检测的交互式可视化系统，能够有效检测和分析网络异常和攻击，解决了现有系统在处理大规模流量时的不足。


<details>
  <summary>Details</summary>
Motivation: 当前入侵检测系统(IDS)在处理流量异常和攻击时存在交互性不足、难以分析传播模式和关联攻击等问题，亟需改进。

Method: 通过流级跟踪图结合Histographs技术，用户可交互式查询和分析异常模式，使用关联矩阵视图揭示分布式攻击。

Result: 在1.16TB流级数据中成功检测到端口扫描、蠕虫爆发、隐蔽TCP SYN洪泛等多种攻击。

Conclusion: IDGraphs为大规模网络流量中的入侵检测提供了一种高效的交互式可视化解决方案。

Abstract: Traffic anomalies and attacks are commonplace in today's networks and
identifying them rapidly and accurately is critical for large network
operators. For a statistical intrusion detection system (IDS), it is crucial to
detect at the flow-level for accurate detection and mitigation. However,
existing IDS systems offer only limited support for 1) interactively examining
detected intrusions and anomalies, 2) analyzing worm propagation patterns, 3)
and discovering correlated attacks. These problems are becoming even more acute
as the traffic on today's high-speed routers continues to grow.
  IDGraphs is an interactive visualization system for intrusion detection that
addresses these challenges. The central visualization in the system is a
flow-level trace plotted with time on the horizontal axis and aggregated number
of unsuccessful connections on the vertical axis. We then summarize a stack of
tens or hundreds of thousands of these traces using the Histographs [RW05]
technique, which maps data frequency at each pixel to brightness. Users may
then interactively query the summary view, performing analysis by highlighting
subsets of the traces. For example, brushing a linked correlation matrix view
highlights traces with similar patterns, revealing distributed attacks that are
difficult to detect using standard statistical analysis.
  We apply IDGraphs system to a real network router data-set with 179M
flow-level records representing a total traffic of 1.16TB. The system
successfully detects and analyzes a variety of attacks and anomalies, including
port scanning, worm outbreaks, stealthy TCP SYN floodings, and some distributed
attacks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [34] [Scalable GPU Performance Variability Analysis framework](https://arxiv.org/abs/2506.20674)
*Ankur Lahiry,Ayush Pokharel,Seth Ockerman,Amal Gueroudji,Line Pouchard,Tanzima Z. Islam*

Main category: cs.DC

TL;DR: 提出了一种分布式数据分析框架，用于高效处理大规模GPU性能日志，解决了现有工具内存占用高、处理时间长的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GPU性能日志分析工具因高内存需求和长处理时间，无法满足自动化工作流的需求。

Method: 设计了一个分布式框架，将数据集分片并行处理，减少单节点内存压力并避免瓶颈。

Result: 应用于真实HPC和AI工作负载，成功诊断性能变异性并揭示内存传输延迟对GPU内核行为的影响。

Conclusion: 该框架显著提升了大规模性能日志的分析效率，适合高性能计算工作流。

Abstract: Analyzing large-scale performance logs from GPU profilers often requires
terabytes of memory and hours of runtime, even for basic summaries. These
constraints prevent timely insight and hinder the integration of performance
analytics into automated workflows. Existing analysis tools typically process
data sequentially, making them ill-suited for HPC workflows with growing trace
complexity and volume. We introduce a distributed data analysis framework that
scales with dataset size and compute availability. Rather than treating the
dataset as a single entity, our system partitions it into independently
analyzable shards and processes them concurrently across MPI ranks. This design
reduces per-node memory pressure, avoids central bottlenecks, and enables
low-latency exploration of high-dimensional trace data. We apply the framework
to end-to-end Nsight Compute traces from real HPC and AI workloads, demonstrate
its ability to diagnose performance variability, and uncover the impact of
memory transfer latency on GPU kernel behavior.

</details>


### [35] [ClusterRCA: Network Failure Diagnosis in HPC Systems Using Multimodal Data](https://arxiv.org/abs/2506.20673)
*Yongqian Sun,Xijie Pan,Xiao Xiong,Lei Tao,Jiaju Wang,Shenglin Zhang,Yuan Yuan,Yuqi Li,Kunlin Jian*

Main category: cs.DC

TL;DR: ClusterRCA框架通过多模态数据和图分析方法提高HPC网络故障诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据异构性和准确性不足无法直接用于HPC系统，因此需要一种新的故障诊断方法。

Method: 提出ClusterRCA框架，提取拓扑连接的NIC对特征，结合分类器和图方法（构建故障图并执行定制随机游走）定位故障节点和类型。

Result: 在顶级HPC设备供应商数据集上验证，ClusterRCA表现出高准确性和鲁棒性。

Conclusion: ClusterRCA为HPC网络故障诊断提供了一种高效且可靠的新方法。

Abstract: Network failure diagnosis is challenging yet critical for high-performance
computing (HPC) systems. Existing methods cannot be directly applied to HPC
scenarios due to data heterogeneity and lack of accuracy. This paper proposes a
novel framework, called ClusterRCA, to localize culprit nodes and determine
failure types by leveraging multimodal data. ClusterRCA extracts features from
topologically connected network interface controller (NIC) pairs to analyze the
diverse, multimodal data in HPC systems. To accurately localize culprit nodes
and determine failure types, ClusterRCA combines classifier-based and
graph-based approaches. A failure graph is constructed based on the output of
the state classifier, and then it performs a customized random walk on the
graph to localize the root cause. Experiments on datasets collected by a
top-tier global HPC device vendor show ClusterRCA achieves high accuracy in
diagnosing network failure for HPC systems. ClusterRCA also maintains robust
performance across different application scenarios.

</details>


### [36] [Utility-Driven Speculative Decoding for Mixture-of-Experts](https://arxiv.org/abs/2506.20675)
*Anish Saxena,Po-An Tsai,Hritvik Taneja,Aamer Jaleel,Moinuddin Qureshi*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: GPU memory bandwidth is the main bottleneck for low-latency Large Language
Model (LLM) inference. Speculative decoding leverages idle GPU compute by using
a lightweight drafter to propose K tokens, which the LLM verifies in parallel,
boosting token throughput. In conventional dense LLMs, all model weights are
fetched each iteration, so speculation adds no latency overhead. Emerging
Mixture of Experts (MoE) models activate only a subset of weights per token,
greatly reducing data movement. However, we show that speculation is
ineffective for MoEs: draft tokens collectively activate more weights,
increasing data movement and verification time by 2-3x. When token throughput
gains fail to offset this overhead, speculation causes slowdowns up to 1.5x,
making it infeasible. Even when useful, the optimal K varies by task, model,
and even between requests and iterations. Thus, despite widespread use in dense
LLMs, speculation remains impractical in leading MoEs.
  We present Cascade, a utility-driven framework that selectively enables
speculation to avoid slowdowns and dynamically tunes K to accelerate MoE
serving. Cascade uses a lightweight metric, speculation utility, the ratio of
token gains to verification cost, which shows iteration-level locality,
enabling periodic decisions via short test and longer set phases. For each
request, Cascade disables speculation if utility drops below one during
testing, and when utility exceeds one, tests multiple K-values to choose the
utility-maximizing K for the set phase. We implement Cascade in vLLM and
evaluate it on five popular MoEs with workloads spanning code, math,
extraction, and mixed tasks. Cascade limits slowdown to 5% (vs. 1.5x) and
improves throughput by 7-14% over static K, making speculative decoding
practical for MoEs.

</details>


### [37] [ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks](https://arxiv.org/abs/2506.20938)
*Joshua H. Davis,Daniel Nichols,Ishan Khillan,Abhinav Bhatele*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: GPGPU architectures have become significantly diverse in recent years, which
has led to an emergence of a variety of specialized programming models and
software stacks to support them. While portable execution models exist, they
still require significant developer effort to port to and optimize for
different hardware architectures. Recent advances in large language models
(LLMs) can help us reduce some of this programmer burden. In this paper, we
present a novel benchmark and testing framework, ParEval-Repo, which can be
used to evaluate the efficacy of LLM-based approaches in automatically
translating entire codebases across GPGPU execution models. ParEval-Repo
includes several scientific computing and AI mini-applications in a range of
programming models, and levels of repository complexity. We use ParEval-Repo to
evaluate a range of state-of-the-art open-source and commercial LLMs, with both
a non-agentic and a top-down agentic approach. We assess code generated by the
LLMs and approaches in terms of compilability, functional correctness,
categories of build errors, and the cost of translation in terms of the number
of inference tokens. Our results demonstrate that LLM translation of scientific
applications is feasible for small programs but difficulty with generating
functional build systems and cross-file dependencies pose challenges in scaling
to larger codebases.

</details>


### [38] [Portable High-Performance Kernel Generation for a Computational Fluid Dynamics Code with DaCe](https://arxiv.org/abs/2506.20994)
*Måns I. Andersson,Martin Karp,Niclas Jansson,Stefano Markidis*

Main category: cs.DC

TL;DR: 论文探讨了利用DaCe框架自动生成高性能计算内核，以应对不同硬件架构的挑战，显著提升了代码的可移植性和性能。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算加速器的多样化，开发者为不同架构编写特定代码的负担增加，DaCe框架旨在通过自动生成高性能代码解决这一问题。

Method: 采用DaCe的Stateful Dataflow Multigraph (SDFG)表示法，为计算流体动力学中的关键计算内核生成代码，并将其集成到Neko求解器中。

Result: 在多种GPU平台（如Nvidia GH200、A100和AMD MI250X）上，生成的代码展现了优异的可移植性和性能。

Conclusion: 自动化代码生成是确保大规模科学应用长期可持续性的可行解决方案。

Abstract: With the emergence of new high-performance computing (HPC) accelerators, such
as Nvidia and AMD GPUs, efficiently targeting diverse hardware architectures
has become a major challenge for HPC application developers. The increasing
hardware diversity in HPC systems often necessitates the development of
architecture-specific code, hindering the sustainability of large-scale
scientific applications. In this work, we leverage DaCe, a data-centric
parallel programming framework, to automate the generation of high-performance
kernels. DaCe enables automatic code generation for multicore processors and
various accelerators, reducing the burden on developers who would otherwise
need to rewrite code for each new architecture. Our study demonstrates DaCe's
capabilities by applying its automatic code generation to a critical
computational kernel used in Computational Fluid Dynamics (CFD). Specifically,
we focus on Neko, a Fortran-based solver that employs the spectral-element
method, which relies on small tensor operations. We detail the formulation of
this computational kernel using DaCe's Stateful Dataflow Multigraph (SDFG)
representation and discuss how this approach facilitates high-performance code
generation. Additionally, we outline the workflow for seamlessly integrating
DaCe's generated code into the Neko solver. Our results highlight the
portability and performance of the generated code across multiple platforms,
including Nvidia GH200, Nvidia A100, and AMD MI250X GPUs, with competitive
performance results. By demonstrating the potential of automatic code
generation, we emphasise the feasibility of using portable solutions to ensure
the long-term sustainability of large-scale scientific applications.

</details>


### [39] [Bridding OT and PaaS in Edge-to-Cloud Continuum](https://arxiv.org/abs/2506.21072)
*Carlos J Barrios,Yves Denneulin*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Operational Technology Platform as a Service (OTPaaS) initiative provides
a structured framework for the efficient management and storage of data. It
ensures excellent response times while improving security, reliability, data
and technology sovereignty, robustness, and energy efficiency, which are
crucial for industrial transformation and data sovereignty. This paper
illustrates successful deployment, adaptable application management, and
various integration components catering to Edge and Cloud environments. It
leverages the advantages of the Platform as a Service model and highlights key
challenges that have been addressed for specific use cases.

</details>


### [40] [BLOCKS: Blockchain-supported Cross-Silo Knowledge Sharing for Efficient LLM Services](https://arxiv.org/abs/2506.21033)
*Zhaojiacheng Zhou,Hongze Liu,Shijing Yuan,Hanning Zhang,Jiong Lou,Chentao Wu,Jie Li*

Main category: cs.DC

TL;DR: 摘要提出了一种基于区块链的外部知识框架，解决大模型幻觉问题，通过协调多个知识孤岛提供可靠知识并确保数据安全。


<details>
  <summary>Details</summary>
Motivation: 大模型的幻觉问题日益突出，而外部知识虽然能解决这个问题，但因隐私和安全问题，许多相关知识分散且难以获取。

Method: 采用区块链技术，将本地知识提炼为提示，并通过区块链执行交易和记录。引入信誉机制和交叉验证确保知识质量，并提供参与激励。设计了查询生成框架，为大模型检索提供直接API接口。

Result: 在不同知识源上进行了广泛实验，结果表明该框架在区块链环境中实现了高效的大模型知识共享。

Conclusion: 该论文提出的区块链框架有效解决了大模型的知识获取和安全性问题，为高效知识共享提供了可行方案。

Abstract: The hallucination problem of Large Language Models (LLMs) has increasingly
drawn attention. Augmenting LLMs with external knowledge is a promising
solution to address this issue. However, due to privacy and security concerns,
a vast amount of downstream task-related knowledge remains dispersed and
isolated across various "silos," making it difficult to access. To bridge this
knowledge gap, we propose a blockchain-based external knowledge framework that
coordinates multiple knowledge silos to provide reliable foundational knowledge
for large model retrieval while ensuring data security. Technically, we distill
knowledge from local data into prompts and execute transactions and records on
the blockchain. Additionally, we introduce a reputation mechanism and
cross-validation to ensure knowledge quality and provide incentives for
participation. Furthermore, we design a query generation framework that
provides a direct API interface for large model retrieval. To evaluate the
performance of our proposed framework, we conducted extensive experiments on
various knowledge sources. The results demonstrate that the proposed framework
achieves efficient LLM service knowledge sharing in blockchain environments.

</details>


### [41] [Enabling Bitcoin Smart Contracts on the Internet Computer](https://arxiv.org/abs/2506.21327)
*Ryan Croote,Islam El-Ashi,Thomas Locher,Yvonne-Anne Pignolet*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: There is growing interest in providing programmatic access to the value
locked in Bitcoin, which famously offers limited programmability itself.
Various approaches have been put forth in recent years, with the vast majority
of proposed mechanisms either building new functionality on top of Bitcoin or
leveraging a bridging mechanism to enable smart contracts that make use of
``wrapped'' bitcoins on entirely different platforms.
  In this work, an architecture is presented that follows a different approach.
The architecture enables the execution of Turing-complete Bitcoin smart
contracts on the Internet Computer (IC), a blockchain platform for hosting and
executing decentralized applications. Instead of using a bridge, IC and Bitcoin
nodes interact directly, eliminating potential security risks that the use of a
bridge entails. This integration requires novel concepts, in particular to
reconcile the probabilistic nature of Bitcoin with the irreversibility of
finalized state changes on the IC, which may be of independent interest.
  In addition to the presentation of the architecture, we provide evaluation
results based on measurements of the Bitcoin integration running on mainnet.
The evaluation results demonstrate that, with finalization in a few seconds and
low execution costs, this integration enables complex Bitcoin-based
decentralized applications that were not practically feasible or economically
viable before.

</details>


### [42] [Carbon-Aware Microservice Deployment for Optimal User Experience on a Budget](https://arxiv.org/abs/2506.21422)
*Kevin Kreutz,Philipp Wiesner,Monica Vitali*

Main category: cs.DC

TL;DR: 提出了一种针对微服务的碳感知策略，通过在每小时碳预算下选择最佳版本和水平扩展来优化用户体验和收益。


<details>
  <summary>Details</summary>
Motivation: 数据中心碳足迹成为关键问题，现有策略无法适用于需要实时响应的服务型云应用。

Method: 基于每小时碳预算，选择微服务的最佳版本和水平扩展方案。

Result: 实验表明，该方法能适应多变的工作负载和碳强度。

Conclusion: 该策略在预算约束下有效提升了用户体验和收益。

Abstract: The carbon footprint of data centers has recently become a critical concern.
So far, most carbon-aware strategies have focused on leveraging the flexibility
of scheduling decisions for batch processing by shifting the time and location
of workload executions. However, such approaches cannot be applied to
service-oriented cloud applications, since they have to be reachable at every
point in time and often at low latencies. We propose a carbon-aware approach
for operating microservices under hourly carbon budgets. By choosing the most
appropriate version and horizontal scaleout for each microservice, our strategy
maximizes user experience and revenue while staying within budget constraints.
Experiments across various application configurations and carbon budgets
demonstrate that the approach adapts properly to changing workloads and carbon
intensities.

</details>


### [43] [exa-AMD: A Scalable Workflow for Accelerating AI-Assisted Materials Discovery and Design](https://arxiv.org/abs/2506.21449)
*Maxim Moraru,Weiyi Xia,Zhuo Ye,Feng Zhang,Yongxin Yao,Ying Wai Li,Cai-Zhuang Wang*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: exa-AMD is a Python-based application designed to accelerate the discovery
and design of functional materials by integrating AI/ML tools, materials
databases, and quantum mechanical calculations into scalable, high-performance
workflows. The execution model of exa-AMD relies on Parsl, a task-parallel
programming library that enables a flexible execution of tasks on any computing
resource from laptops to supercomputers. By using Parsl, exa-AMD is able to
decouple the workflow logic from execution configuration, thereby empowering
researchers to scale their workflows without having to reimplement them for
each system.

</details>


### [44] [Efficient and Reuseable Cloud Configuration Search Using Discovery Spaces](https://arxiv.org/abs/2506.21467)
*Michael Johnston,Burkhard Ringlein,Christoph Hagleitner,Alessandro Pomponio,Vassilis Vassiliadis,Christian Pinto,Srikumar Venugopal*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Finding the optimal set of cloud resources to deploy a given workload at
minimal cost while meeting a defined service level agreement is an active area
of research. Combining tens of parameters applicable across a large selection
of compute, storage, and services offered by cloud providers with similar
numbers of application-specific parameters leads to configuration spaces with
millions of deployment options.
  In this paper, we propose Discovery Space, an abstraction that formalizes the
description of workload configuration problems, and exhibits a set of
characteristics required for structured, robust and distributed investigations
of large search spaces. We describe a concrete implementation of the Discovery
Space abstraction and show that it is generalizable across a diverse set of
workloads such as Large Language Model inference and Big Data Analytics.
  We demonstrate that our approach enables safe, transparent sharing of data
between executions of best-of-breed optimizers increasing the efficiency of
optimal configuration detection in large search spaces. We also demonstrate how
Discovery Spaces enable transfer and reuse of knowledge across similar search
spaces, enabling configuration search speed-ups of over 90%.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [45] [Condensed Representation of RDF and its Application on Graph Versioning](https://arxiv.org/abs/2506.21203)
*Jey Puget Gil,Emmanuel Coquery,John Samuel,Gilles Gesquiere*

Main category: cs.DB

TL;DR: 研究提出了一种知识图的压缩表示方法，用于处理不断演化的数据，便于分析和预测。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了更好地组织和管理知识图中不断演化的数据，使其易于访问和分析。

Method: 提出并形式化了一种压缩表示方法，用于捕捉知识图的动态演化过程。

Result: 通过压缩表示方法，可以更高效地处理和分析演化知识图。

Conclusion: 压缩表示方法为知识图管理提供了有效的工具，支持未来的趋势预测和分析。

Abstract: The study of the evolving phenomena in a domain helps to understand the
relationships between entities at different points in time and predict future
trends. These phenomena, often complex, can be represented using knowledge
graphs, which have the capability to model heterogeneous data from multiple
sources. Nowadays, a considerable amount of sources delivering periodic updates
to knowledge graphs in various domains is openly available. The evolution of
data is of interest to knowledge graph management systems, and therefore it is
crucial to organize these constantly evolving data to make them easily
accessible and exploitable for analyzes. In this article, we will present and
formalize the condensed representation of these evolving graphs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [46] [Post-Quantum and Blockchain-Based Attestation for Trusted FPGAs in B5G Networks](https://arxiv.org/abs/2506.21073)
*Ilias Papalamprou,Nikolaos Fotos,Nikolaos Chatzivasileiadis,Anna Angelogianni,Dimosthenis Masouros,Dimitrios Soudris*

Main category: cs.AR

TL;DR: 论文提出了一种结合硬件-软件的混合解决方案，利用远程验证安全配置FPGA，并集成后量子密码算法，同时使用区块链技术确保安全性。实验表明，该方法仅增加2%的开销。


<details>
  <summary>Details</summary>
Motivation: 随着5G及更高性能网络的普及，FPGA在边缘计算中的部署日益增多，但其安全性面临量子计算威胁，亟需增强安全基础设施。

Method: 采用远程验证技术安全配置FPGA，并集成了后量子密码算法（PQC）和区块链技术，以确保全面的安全性。

Result: 在不同FPGA家族中评估了该方法，结果显示相比非PQC方法仅增加了2%的开销。

Conclusion: 该方案通过混合硬件-软件方法有效提升了FPGA的安全性，且性能影响极小。

Abstract: The advent of 5G and beyond has brought increased performance networks,
facilitating the deployment of services closer to the user. To meet performance
requirements such services require specialized hardware, such as Field
Programmable Gate Arrays (FPGAs). However, FPGAs are often deployed in
unprotected environments, leaving the user's applications vulnerable to
multiple attacks. With the rise of quantum computing, which threatens the
integrity of widely-used cryptographic algorithms, the need for a robust
security infrastructure is even more crucial. In this paper we introduce a
hybrid hardware-software solution utilizing remote attestation to securely
configure FPGAs, while integrating Post-Quantum Cryptographic (PQC) algorithms
for enhanced security. Additionally, to enable trustworthiness across the whole
edge computing continuum, our solution integrates a blockchain infrastructure,
ensuring the secure storage of any security evidence. We evaluate the proposed
secure configuration process under different PQC algorithms in two FPGA
families, showcasing only 2% overheard compared to the non PQC approach.

</details>


### [47] [Accelerating GNN Training through Locality-aware Dropout and Merge](https://arxiv.org/abs/2506.21414)
*Gongjian Sun,Mingyu Yan,Dengke Han,Runzhen Xue,Duo Wang,Xiaochun Ye,Dongrui Fan*

Main category: cs.AR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in graph
learning and are widely adopted across various critical domains. However, the
irregular connectivity between vertices leads to inefficient neighbor
aggregation, resulting in substantial irregular and coarse-grained DRAM
accesses. This lack of data locality presents significant challenges for
execution platforms, ultimately degrading performance. While previous
accelerator designs have leveraged on-chip memory and data access scheduling
strategies to address this issue, they still inevitably access features at
irregular addresses from DRAM. In this work, we propose LiGNN, a hardware-based
solution that improves data locality by applying dropout and merge techniques
during neighbor aggregation to accelerate GNN training. Unlike conventional
algorithm-level dropout methods that primarily aim to improve accuracy while
overlooking hardware costs, LiGNN introduces a locality-aware feature dropout
mechanism. This approach selectively drops node features with data locality
awareness, effectively reducing irregular DRAM accesses without compromising
model accuracy. Moreover, by leveraging detailed knowledge of memory layout and
organization-including critical alignment constraints-LiGNN strategically
merges memory accesses during neighbor aggregation at the DRAM row level,
guided by GNN-level semantics. This optimization significantly improves data
locality with minimal additional cost. Under the commonly adopted 0.5 dropout
rate, LiGNN outperforms state-of-the-art methods, delivering a 1.48~3.02x
speedup, reducing DRAM accesses by 34%~55%, and lowering DRAM row activations
by 59%~82%, all while maintaining model accuracy.

</details>


### [48] [OptGM: An Optimized Gate Merging Method to Mitigate NBTI in Digital Circuits](https://arxiv.org/abs/2506.21487)
*Maryam Ghane,Amir M. Hajisadeghi,Hamid R. Zarandi*

Main category: cs.AR

TL;DR: OptGM是一种优化的门合并方法，旨在减少数字电路中的负偏压温度不稳定性（NBTI）。通过识别NBTI关键节点并合并相关门，显著降低了关键晶体管数量、延迟退化和总晶体管数。


<details>
  <summary>Details</summary>
Motivation: 负偏压温度不稳定性（NBTI）是数字电路中一个严重的可靠性问题，会导致电路性能退化。OptGM旨在通过优化门合并来缓解这一问题。

Method: OptGM首先识别信号概率超过阈值的NBTI关键节点，然后通过优化算法将驱动关键节点的门和被驱动的门合并为一个新的复杂门，从而消除NBTI关键节点。

Result: 实验结果显示，OptGM平均减少了89.29%的NBTI关键晶体管、23.87%的延迟退化和6.47%的总晶体管数，同时性能成本比（PPC）提升了12.8%。

Conclusion: OptGM是一种高效的方法，能够显著缓解NBTI问题，同时保持较低的面积开销。适用于组合和时序电路的可靠性优化。

Abstract: This paper presents OptGM, an optimized gate merging method designed to
mitigate negative bias temperature instability (NBTI) in digital circuits.
First, the proposed approach effectively identifies NBTI-critical internal
nodes, defined as those with a signal probability exceeding a predefined
threshold. Next, based on the proposed optimized algorithm, the sensitizer gate
(which drives the critical node) and the sensitive gate (which is fed by it)
are merged into a new complex gate. This complex gate preserves the original
logic while eliminating NBTI-critical nodes. Finally, to evaluate the
effectiveness of OptGM, we assess it on several combinational and sequential
benchmark circuits. Simulation results demonstrate that, on average, the number
of NBTI-critical transistors (i.e., PMOS transistors connected to critical
nodes), NBTI-induced delay degradation, and the total transistor count are
reduced by 89.29%, 23.87%, and 6.47%, respectively. Furthermore, OptGM enhances
performance per cost (PPC) by 12.8% on average, with minimal area overhead.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [49] [Rational Miner Behaviour, Protocol Stability, and Time Preference: An Austrian and Game-Theoretic Analysis of Bitcoin's Incentive Environment](https://arxiv.org/abs/2506.20965)
*Craig Steven Wright*

Main category: econ.GN

TL;DR: 论文将奥地利资本理论与重复博弈论结合，研究区块链系统中不同制度下矿工的战略行为，发现可变协议会提高时间偏好，破坏长期合作均衡。


<details>
  <summary>Details</summary>
Motivation: 探讨区块链系统中制度规则对矿工行为的影响，尤其是可变协议如何改变矿工激励。

Method: 结合形式博弈论分析和奥地利经济学原理，分析可变协议对矿工行为的影响。

Result: 可变协议导致矿工从生产性投资转向政治寻租，比特币原始协议的固定规则更有利于战略一致性和网络均衡。

Conclusion: 协议不可变性是恢复战略一致性和可持续网络均衡的关键。

Abstract: This paper integrates Austrian capital theory with repeated game theory to
examine strategic miner behaviour under different institutional conditions in
blockchain systems. It shows that when protocol rules are mutable, effective
time preference rises, undermining rational long-term planning and cooperative
equilibria. Using formal game-theoretic analysis and Austrian economic
principles, the paper demonstrates how mutable protocols shift miner incentives
from productive investment to political rent-seeking and influence games. The
original Bitcoin protocol is interpreted as an institutional anchor: a fixed
rule-set enabling calculability and low time preference. Drawing on the work of
Bohm-Bawerk, Mises, and Hayek, the argument is made that protocol immutability
is essential for restoring strategic coherence, entrepreneurial confidence, and
sustainable network equilibrium.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [50] [Spiking Neural Networks for SAR Interferometric Phase Unwrapping: A Theoretical Framework for Energy-Efficient Processing](https://arxiv.org/abs/2506.20782)
*Marc Bara*

Main category: cs.NE

TL;DR: 论文提出了一个理论框架，首次将脉冲神经网络（SNN）应用于合成孔径雷达（SAR）干涉相位解缠，填补了当前方法中的空白。


<details>
  <summary>Details</summary>
Motivation: 随着地球观测数据量的指数增长（如NISAR任务两年内将生成100PB数据），高效能处理对可持续数据中心运营至关重要。SNN的事件驱动计算模型具有潜在30-100倍的能源节约优势。

Method: 开发了针对缠绕相位数据的脉冲编码方案，提出了基于相位解缠空间传播特性的SNN架构，并分析了计算复杂度和收敛性。

Result: 理论框架表明，SNN的时序动态特性可以自然建模相位解缠的空间连续性约束。

Conclusion: 本研究为神经形态计算与SAR干涉测量的交叉领域开辟了新方向，为大规模InSAR处理提供了可持续的补充方法。

Abstract: We present the first theoretical framework for applying spiking neural
networks (SNNs) to synthetic aperture radar (SAR) interferometric phase
unwrapping. Despite extensive research in both domains, our comprehensive
literature review confirms that SNNs have never been applied to phase
unwrapping, representing a significant gap in current methodologies. As Earth
observation data volumes continue to grow exponentially (with missions like
NISAR expected to generate 100PB in two years) energy-efficient processing
becomes critical for sustainable data center operations. SNNs, with their
event-driven computation model, offer potential energy savings of 30-100x
compared to conventional approaches while maintaining comparable accuracy. We
develop spike encoding schemes specifically designed for wrapped phase data,
propose SNN architectures that leverage the spatial propagation nature of phase
unwrapping, and provide theoretical analysis of computational complexity and
convergence properties. Our framework demonstrates how the temporal dynamics
inherent in SNNs can naturally model the spatial continuity constraints
fundamental to phase unwrapping. This work opens a new research direction at
the intersection of neuromorphic computing and SAR interferometry, offering a
complementary approach to existing algorithms that could enable more
sustainable large-scale InSAR processing.

</details>


### [51] [Brain2Model Transfer: Training sensory and decision models with human neural activity as a teacher](https://arxiv.org/abs/2506.20834)
*Tomas Gallo Aquino,Victoria Liu,Habiba Azab,Raissa Mathura,Andrew J Watrous,Eleonora Bartoli,Benjamin Y Hayden,Paul Sajda,Sameer A Sheth,Nuttida Rungratsameetaweemana*

Main category: cs.NE

TL;DR: 论文提出了一种基于人脑神经活动的迁移学习框架B2M，通过两种策略（对比转移和潜在转移）显著提高了人工神经网络的学习效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 利用人脑高效的低维抽象学习能力，改进人工模型的训练效率和性能。

Method: 提出了Brain2Model迁移学习框架，包括Brain Contrastive Transfer和Brain Latent Transfer两种策略。

Result: 实验表明，B2M策略下的人工网络收敛更快、预测更准。

Conclusion: 人脑表征对人工学习具有重要价值，可显著提升复杂决策任务的学习效率。

Abstract: Transfer learning enhances the training of novel sensory and decision models
by employing rich feature representations from large, pre-trained teacher
models. Cognitive neuroscience shows that the human brain creates
low-dimensional, abstract representations for efficient sensorimotor coding.
Importantly, the brain can learn these representations with significantly fewer
data points and less computational power than artificial models require. We
introduce Brain2Model Transfer Learning (B2M), a framework where neural
activity from human sensory and decision-making tasks acts as the teacher model
for training artificial neural networks. We propose two B2M strategies: (1)
Brain Contrastive Transfer, which aligns brain activity and network activations
through a contrastive objective; and (2) Brain Latent Transfer, which projects
latent dynamics from similar cognitive tasks onto student networks via
supervised regression of brain-derived features. We validate B2M in
memory-based decision-making with a recurrent neural network and scene
reconstruction for autonomous driving with a variational autoencoder. The
results show that student networks benefiting from brain-based transfer
converge faster and achieve higher predictive accuracy than networks trained in
isolation. Our findings indicate that the brain's representations are valuable
for artificial learners, paving the way for more efficient learning of complex
decision-making representations, which would be costly or slow through purely
artificial training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [52] [Hierarchical Sub-action Tree for Continuous Sign Language Recognition](https://arxiv.org/abs/2506.20947)
*Dejie Yang,Zhu Xu,Xinjie Gao,Yang Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为HST-CSLR的方法，通过构建层级子动作树（HST）将手语知识融入视觉表示学习，提升连续手语识别性能。


<details>
  <summary>Details</summary>
Motivation: 连续手语识别（CSLR）缺乏大规模数据集和精确标注，导致训练数据不足，跨模态解决方案未能充分利用手语知识。

Method: 提出HST-CSLR方法，利用大型语言模型提取文本特征，构建层级子动作树（HST）逐步对齐视觉和文本模态，并结合对比对齐增强来缩小模态间差异。

Result: 在PHOENIX-2014、PHOENIX-2014T、CSL-Daily和Sign Language Gesture四个数据集上验证了HST-CSLR的有效性。

Conclusion: HST-CSLR通过融合手语知识和视觉表示学习，显著提升了连续手语识别的性能。

Abstract: Continuous sign language recognition (CSLR) aims to transcribe untrimmed
videos into glosses, which are typically textual words. Recent studies indicate
that the lack of large datasets and precise annotations has become a bottleneck
for CSLR due to insufficient training data. To address this, some works have
developed cross-modal solutions to align visual and textual modalities.
However, they typically extract textual features from glosses without fully
utilizing their knowledge. In this paper, we propose the Hierarchical
Sub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge
with visual representation learning. By incorporating gloss-specific knowledge
from large language models, our approach leverages textual information more
effectively. Specifically, we construct an HST for textual information
representation, aligning visual and textual modalities step-by-step and
benefiting from the tree structure to reduce computational complexity.
Additionally, we impose a contrastive alignment enhancement to bridge the gap
between the two modalities. Experiments on four datasets (PHOENIX-2014,
PHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the
effectiveness of our HST-CSLR.

</details>


### [53] [Whole-Body Conditioned Egocentric Video Prediction](https://arxiv.org/abs/2506.21552)
*Yutong Bai,Danny Tran,Amir Bar,Yann LeCun,Trevor Darrell,Jitendra Malik*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We train models to Predict Ego-centric Video from human Actions (PEVA), given
the past video and an action represented by the relative 3D body pose. By
conditioning on kinematic pose trajectories, structured by the joint hierarchy
of the body, our model learns to simulate how physical human actions shape the
environment from a first-person point of view. We train an auto-regressive
conditional diffusion transformer on Nymeria, a large-scale dataset of
real-world egocentric video and body pose capture. We further design a
hierarchical evaluation protocol with increasingly challenging tasks, enabling
a comprehensive analysis of the model's embodied prediction and control
abilities. Our work represents an initial attempt to tackle the challenges of
modeling complex real-world environments and embodied agent behaviors with
video prediction from the perspective of a human.

</details>


### [54] [How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?](https://arxiv.org/abs/2506.20795)
*Stephanie Käs,Anton Burenko,Louis Markert,Onur Alp Culha,Dennis Mack,Timm Linder,Bastian Leibe*

Main category: cs.CV

TL;DR: 论文研究了利用Vision Foundation Models（VFMs）和Vision Language Models（VLMs）进行动态全身手势识别，比较了V-JEPA、Gemini Flash 2.0和HD-GCN等方法，并引入了NUGGET数据集。HD-GCN表现最佳，但V-JEPA接近，展示了简化系统复杂度的潜力。


<details>
  <summary>Details</summary>
Motivation: 在嘈杂环境中（如敏捷生产），手势识别对于非语言人机通信至关重要。传统基于深度学习的方法依赖任务特定的架构，而VFMs和VLMs的强泛化能力可能降低系统复杂度。

Method: 研究比较了V-JEPA（先进的VFM）、Gemini Flash 2.0（多模态VLM）和HD-GCN（高性能骨架方法），并使用NUGGET数据集进行评估。

Result: HD-GCN表现最佳，但V-JEPA通过简单的任务特定分类头接近其性能；Gemini在零样本设定下表现不佳。

Conclusion: V-JEPA展示了作为共享多任务模型的潜力，而Gemini的表现表明需要进一步研究适合手势识别的输入表示。

Abstract: Gestures enable non-verbal human-robot communication, especially in noisy
environments like agile production. Traditional deep learning-based gesture
recognition relies on task-specific architectures using images, videos, or
skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)
and Vision Language Models (VLMs) with their strong generalization abilities
offer potential to reduce system complexity by replacing dedicated
task-specific modules. This study investigates adapting such models for
dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art
VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing
skeleton-based approach). We introduce NUGGET, a dataset tailored for
human-robot communication in intralogistics environments, to evaluate the
different gesture recognition approaches. In our experiments, HD-GCN achieves
best performance, but V-JEPA comes close with a simple, task-specific
classification head - thus paving a possible way towards reducing system
complexity, by using it as a shared multi-task model. In contrast, Gemini
struggles to differentiate gestures based solely on textual descriptions in the
zero-shot setting, highlighting the need of further research on suitable input
representations for gestures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [55] [Inside Job: Defending Kubernetes Clusters Against Network Misconfigurations](https://arxiv.org/abs/2506.21134)
*Jacopo Bufalino,Jose Luis Martin-Navarro,Mario Di Francesco,Tuomas Aura*

Main category: cs.CR

TL;DR: 该论文研究了Kubernetes集群中网络配置对安全性的影响，重点关注横向移动问题，分析了287个开源应用，发现了634个配置错误。


<details>
  <summary>Details</summary>
Motivation: Kubernetes作为容器编排的事实标准，其安全性研究较少关注网络配置的影响，尤其是横向移动问题。

Method: 对6个组织的287个开源应用进行网络配置分析。

Result: 发现了634个配置错误，远超现有解决方案的能力，并与相关组织合作修复了30多个应用的问题。

Conclusion: 研究表明网络配置对Kubernetes安全性至关重要，研究结果为提升安全性提供了实用建议。

Abstract: Kubernetes has emerged as the de facto standard for container orchestration.
Unfortunately, its increasing popularity has also made it an attractive target
for malicious actors. Despite extensive research on securing Kubernetes, little
attention has been paid to the impact of network configuration on the security
of application deployments. This paper addresses this gap by conducting a
comprehensive analysis of network misconfigurations in a Kubernetes cluster
with specific reference to lateral movement. Accordingly, we carried out an
extensive evaluation of 287 open-source applications belonging to six different
organizations, ranging from IT companies and public entities to non-profits. As
a result, we identified 634 misconfigurations, well beyond what could be found
by solutions in the state of the art. We responsibly disclosed our findings to
the concerned organizations and engaged in a discussion to assess their
severity. As of now, misconfigurations affecting more than thirty applications
have been fixed with the mitigations we proposed.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [56] [Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions](https://arxiv.org/abs/2506.20677)
*Shrinivass Arunachalam Balasubramanian*

Main category: cs.DS

TL;DR: 本文提出了一种新的自适应混合排序范式，根据输入数据的实时模式自动选择最有效的排序算法（计数排序、基数排序或快速排序）。通过特征提取和决策引擎（结合有限状态机和XGBoost分类器）智能选择最优策略，显著提升了执行时间和效率。


<details>
  <summary>Details</summary>
Motivation: 由于没有一种排序算法适用于所有数据分布，为了提高排序效率并适应不同场景（如大规模数据系统、实时系统和嵌入式计算），研究提出了自适应混合排序框架。

Method: 框架包括特征提取模块（计算数据体积、值范围和熵）和决策引擎（结合有限状态机和XGBoost分类器），根据输入数据实时选择计数排序、基数排序或快速排序。

Result: 实验结果表明，该框架在合成和真实数据集上的执行时间、灵活性和效率均显著优于传统静态排序算法。

Conclusion: 该框架具有可扩展性、高效性，适用于大数据分析、边缘计算和硬件受限系统等多种数据处理场景。

Abstract: Sorting is an essential operation in computer science with direct
consequences on the performance of large scale data systems, real-time systems,
and embedded computation. However, no sorting algorithm is optimal under all
distributions of data. The new adaptive hybrid sorting paradigm proposed in
this paper is the paradigm that automatically selects the most effective
sorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time
monitoring of patterns in input data. The architecture begins by having a
feature extraction module to compute significant parameters such as data
volume, value range and entropy. These parameters are sent to a decision engine
involving Finite State Machine and XGBoost classifier to aid smart and
effective in choosing the optimal sorting strategy. It implements Counting Sort
on small key ranges, Radix Sort on large range structured input with
low-entropy keys and QuickSort on general purpose sorting. The experimental
findings of both synthetic and real life dataset confirm that the proposed
solution is actually inclined to excel significantly by comparison in execution
time, flexibility and the efficiency of conventional static sorting algorithms.
The proposed framework provides a scalable, high perhaps and applicable to a
wide range of data processing operations like big data analytics, edge
computing, and systems with hardware limitations.

</details>


### [57] [Practical and Accurate Local Edge Differentially Private Graph Algorithms](https://arxiv.org/abs/2506.20828)
*Pranay Mundra,Charalampos Papamanthou,Julian Shun,Quanquan C. Liu*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The rise of massive networks across diverse domains necessitates
sophisticated graph analytics, often involving sensitive data and raising
privacy concerns. This paper addresses these challenges using local
differential privacy (LDP), which enforces privacy at the individual level,
where no third-party entity is trusted, unlike centralized models that assume a
trusted curator. We introduce novel LDP algorithms for two fundamental graph
statistics: k-core decomposition and triangle counting. Our approach leverages
input-dependent private graph properties, specifically the degeneracy and
maximum degree of the graph, to improve theoretical utility. Unlike prior
methods, our error bounds are determined by the maximum degree rather than the
total number of edges, resulting in significantly tighter guarantees. For
triangle counting, we improve upon the work of Imola, Murakami, and
Chaudhury~\cite{IMC21locally, IMC21communication}, which bounds error in terms
of edge count. Instead, our algorithm achieves bounds based on graph degeneracy
by leveraging a private out-degree orientation, a refined variant of Eden et
al.'s randomized response technique~\cite{ELRS23, and a novel analysis,
yielding stronger guarantees than prior work. Beyond theoretical gains, we are
the first to evaluate local DP algorithms in a distributed simulation, unlike
prior work tested on a single processor. Experiments on real-world graphs show
substantial accuracy gains: our k-core decomposition achieves errors within 3x
of exact values, far outperforming the 131x error in the baseline of Dhulipala
et al.~\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative
approximation errors by up to six orders of magnitude, while maintaining
competitive runtime.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [58] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文讨论了如何通过工程实践确保MTEB的可重复性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: MTEB已成为文本嵌入模型的标准评估平台，但其可持续性和扩展性仍需进一步优化。

Method: 采用稳健的持续集成流程，自动化测试执行，并设计增强可重复性和可用性的方案。

Result: 成功扩展了MTEB的规模，同时保持了质量和领域相关性。

Conclusion: 本文经验为机器学习评估框架的可重复性和可用性提供了有价值的借鉴。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [59] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [60] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [61] [Diophantine Equations over $\mathbb Z$: Universal Bounds and Parallel Formalization](https://arxiv.org/abs/2506.20909)
*Jonas Bayer,Marco David,Malte Hassler,Yuri Matiyasevich,Dierk Schleicher*

Main category: math.NT

TL;DR: 论文探讨了Diophantine方程复杂性的界限，并结合形式化定理证明器开发了数学证明。


<details>
  <summary>Details</summary>
Motivation: 研究Diophantine方程的子类（通过变量数和次数限制定义）以解决Hilbert第十问题的不可判定性，并探索形式化证明在数学研究中的应用。

Method: 提出显式通用对（变量数ν和次数δ）界限，并通过Isabelle形式化验证结果。

Result: 实现了整数域的新的复杂度界限，并验证了形式化方法在数学发现过程中的可行性。

Conclusion: 研究对Diophantine方程和21世纪数学实践的方法论均有贡献。

Abstract: This paper explores multiple closely related themes: bounding the complexity
of Diophantine equations over the integers and developing mathematical proofs
in parallel with formal theorem provers.
  Hilbert's Tenth Problem (H10) asks about the decidability of Diophantine
equations and has been answered negatively by Davis, Putnam, Robinson and
Matiyasevich. It is natural to ask for which subclasses of Diophantine
equations H10 remains undecidable. Such subclasses can be defined in terms of
universal pairs: bounds on the number of variables $\nu$ and degree $\delta$
such that all Diophantine equations can be rewritten in at most this
complexity. Our work develops explicit universal pairs $(\nu, \delta)$ for
integer unknowns, achieving new bounds that cannot be obtained by naive
translations from known results over $\mathbb N$.
  In parallel, we have conducted a formal verification of our results using the
proof assistant Isabelle. While formal proof verification has traditionally
been applied a posteriori to known results, this project integrates
formalization into the discovery and development process. In a final section,
we describe key insights gained from this unusual approach and its implications
for mathematical practice. Our work contributes both to the study of
Diophantine equations and to the broader question of how mathematics is
conducted in the 21st century.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [62] [MegaFold: System-Level Optimizations for Accelerating Protein Structure Prediction Models](https://arxiv.org/abs/2506.20686)
*Hoa La,Ahan Gupta,Alex Morehead,Jianlin Cheng,Minjia Zhang*

Main category: q-bio.BM

TL;DR: MegaFold是一个跨平台系统，旨在加速AlphaFold3（AF3）的训练，通过优化数据管道、内存效率和算子融合，显著提升了训练性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 尽管AlphaFold3在蛋白质结构预测方面取得了重大进展，但其训练过程因计算和内存密集型操作、二维注意力机制和数据管道的限制而面临可扩展性问题。

Method: MegaFold通过预缓存消除GPU空闲时间、使用Triton内核实现高效EvoAttention、并对关键小算子进行深度融合，解决了AF3训练的瓶颈。

Result: 在NVIDIA H200和AMD MI250 GPU上的评估显示，MegaFold降低了峰值内存使用1.23倍，并将每轮训练时间分别提高了1.73倍和1.62倍，同时支持更长的序列训练。

Conclusion: MegaFold显著提升了蛋白质折叠模型的训练效率和可扩展性，为未来的生物分子建模研究提供了实用的工具。

Abstract: Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [63] [From On-chain to Macro: Assessing the Importance of Data Source Diversity in Cryptocurrency Market Forecasting](https://arxiv.org/abs/2506.21246)
*Giorgos Demosthenous,Chryssis Georgiou,Eliada Polydorou*

Main category: q-fin.PM

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study investigates the impact of data source diversity on the
performance of cryptocurrency forecasting models by integrating various data
categories, including technical indicators, on-chain metrics, sentiment and
interest metrics, traditional market indices, and macroeconomic indicators. We
introduce the Crypto100 index, representing the top 100 cryptocurrencies by
market capitalization, and propose a novel feature reduction algorithm to
identify the most impactful and resilient features from diverse data sources.
Our comprehensive experiments demonstrate that data source diversity
significantly enhances the predictive performance of forecasting models across
different time horizons. Key findings include the paramount importance of
on-chain metrics for both short-term and long-term predictions, the growing
relevance of traditional market indices and macroeconomic indicators for
longer-term forecasts, and substantial improvements in model accuracy when
diverse data sources are utilized. These insights help demystify the short-term
and long-term driving factors of the cryptocurrency market and lay the
groundwork for developing more accurate and resilient forecasting models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [64] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [65] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [66] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [67] [ResQ: A Novel Framework to Implement Residual Neural Networks on Analog Rydberg Atom Quantum Computers](https://arxiv.org/abs/2506.21537)
*Nicholas S. DiBrita,Jason Han,Tirthak Patel*

Main category: quant-ph

TL;DR: 量子计算在机器学习的应用尚未涉及神经ODE为基础的ResNets，本文提出ResQ框架，利用里德堡原子量子计算机优化解决分类问题。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算在神经ODE ResNets中的应用，利用里德堡量子计算机的优势提升机器学习效果。

Method: 提出ResQ框架，优化里德堡原子量子计算机的动态性，用于解决机器学习的分类问题。

Result: 展示了里德堡量子计算机在神经ODE ResNets中的独特适用性。

Conclusion: ResQ框架为量子计算在神经网络中的应用提供了新方向，尤其在分类问题上具有潜力。

Abstract: Research in quantum machine learning has recently proliferated due to the
potential of quantum computing to accelerate machine learning. An area of
machine learning that has not yet been explored is neural ordinary differential
equation (neural ODE) based residual neural networks (ResNets), which aim to
improve the effectiveness of neural networks using the principles of ordinary
differential equations. In this work, we present our insights about why analog
Rydberg atom quantum computers are especially well-suited for ResNets. We also
introduce ResQ, a novel framework to optimize the dynamics of Rydberg atom
quantum computers to solve classification problems in machine learning using
analog quantum neural ODEs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [68] [GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel Optimization](https://arxiv.org/abs/2506.20807)
*Martin Andrews,Sam Witteveen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.

</details>


### [69] [An Information-Theoretic Analysis for Federated Learning under Concept Drift](https://arxiv.org/abs/2506.21036)
*Fu Peng,Meng Zhang,Ming Tang*

Main category: cs.LG

TL;DR: 该论文分析了联邦学习（FL）在概念漂移下的性能，提出了一种基于信息理论的算法，通过KL散度和互信息提升模型的长期表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的数据通常是动态变化的，导致模型性能下降（概念漂移），现有的FL方法多针对静态数据集，难以适应这种情况。

Method: 提出一种算法，通过KL散度和互信息对经验风险最小化进行正则化，并用马尔可夫链建模概念漂移。

Result: 实验表明，所提方法在三种漂移模式（周期性、渐变性、随机性）下均优于现有方法。

Conclusion: 该算法有效提升了FL在概念漂移下的适应能力，同时实现了性能与成本的权衡。

Abstract: Recent studies in federated learning (FL) commonly train models on static
datasets. However, real-world data often arrives as streams with shifting
distributions, causing performance degradation known as concept drift. This
paper analyzes FL performance under concept drift using information theory and
proposes an algorithm to mitigate the performance degradation. We model concept
drift as a Markov chain and introduce the \emph{Stationary Generalization
Error} to assess a model's capability to capture characteristics of future
unseen data. Its upper bound is derived using KL divergence and mutual
information. We study three drift patterns (periodic, gradual, and random) and
their impact on FL performance. Inspired by this, we propose an algorithm that
regularizes the empirical risk minimization approach with KL divergence and
mutual information, thereby enhancing long-term performance. We also explore
the performance-cost tradeoff by identifying a Pareto front. To validate our
approach, we build an FL testbed using Raspberry Pi4 devices. Experimental
results corroborate with theoretical findings, confirming that drift patterns
significantly affect performance. Our method consistently outperforms existing
approaches for these three patterns, demonstrating its effectiveness in
adapting concept drift in FL.

</details>


### [70] [AGTCNet: A Graph-Temporal Approach for Principled Motor Imagery EEG Classification](https://arxiv.org/abs/2506.21338)
*Galvin Brice S. Lim,Brian Godwin S. Lim,Argel A. Bandala,John Anthony C. Jose,Timothy Scott C. Chu,Edwin Sybingco*

Main category: cs.LG

TL;DR: 该研究提出了一种新颖的图-时序卷积网络（AGTCNet），用于运动想象脑电图（MI-EEG）分类，显著提升了分类性能，同时减少了模型大小和推理时间。


<details>
  <summary>Details</summary>
Motivation: 为解决脑-机接口（BCI）系统中因个体差异和时间变化导致的分类困难，尤其是在多通道EEG信号的复杂时空依赖性方面，现有方法表现不足。

Method: 研究提出了AGTCNet模型，利用EEG电极的拓扑结构作为归纳偏置，结合图卷积注意力网络（GCAT）共同学习有效的时空EEG表示。

Result: AGTCNet在多个数据集上实现了最先进的性能，模型大小减少49.87%，推理时间缩短64.65%。在独立于受试者和特定于受试者的分类任务中均表现优异。

Conclusion: AGTCNet展示了其在BCI部署中的高效性和实用性，能显著提升EEG分类性能并优化资源利用率。

Abstract: Brain-computer interface (BCI) technology utilizing electroencephalography
(EEG) marks a transformative innovation, empowering motor-impaired individuals
to engage with their environment on equal footing. Despite its promising
potential, developing subject-invariant and session-invariant BCI systems
remains a significant challenge due to the inherent complexity and variability
of neural activity across individuals and over time, compounded by EEG hardware
constraints. While prior studies have sought to develop robust BCI systems,
existing approaches remain ineffective in capturing the intricate
spatiotemporal dependencies within multichannel EEG signals. This study
addresses this gap by introducing the attentive graph-temporal convolutional
network (AGTCNet), a novel graph-temporal model for motor imagery EEG (MI-EEG)
classification. Specifically, AGTCNet leverages the topographic configuration
of EEG electrodes as an inductive bias and integrates graph convolutional
attention network (GCAT) to jointly learn expressive spatiotemporal EEG
representations. The proposed model significantly outperformed existing MI-EEG
classifiers, achieving state-of-the-art performance while utilizing a compact
architecture, underscoring its effectiveness and practicality for BCI
deployment. With a 49.87% reduction in model size, 64.65% faster inference
time, and shorter input EEG signal, AGTCNet achieved a moving average accuracy
of 66.82% for subject-independent classification on the BCI Competition IV
Dataset 2a, which further improved to 82.88% when fine-tuned for
subject-specific classification. On the EEG Motor Movement/Imagery Dataset,
AGTCNet achieved moving average accuracies of 64.14% and 85.22% for 4-class and
2-class subject-independent classifications, respectively, with further
improvements to 72.13% and 90.54% for subject-specific classifications.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [71] [Benchmarking and Parallelization of Electrostatic Particle-In-Cell for low-temperature Plasma Simulation by particle-thread Binding](https://arxiv.org/abs/2506.21524)
*Libn Varghese,Bhaskar Chaudhury,Miral Shah,Mainak Bandyopadhyay*

Main category: physics.comp-ph

TL;DR: 提出了一种基于粒子-线程绑定策略的新方法，优化等离子体模拟中电荷沉积（CD）子程序的并行性能，仅需少量私有网格即可提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决传统并行方法在2D/3D设备级PIC模拟中因电荷沉积（CD）子程序成为瓶颈而面临的可扩展性问题。

Method: 采用粒子-线程绑定策略，在分布式或共享内存系统中仅需四个私有网格，避免线程冲突并保持数据结构的兼容性。

Result: 在共享和分布式内存系统（1000核）上的性能测试表明方法具有高可扩展性和低硬件依赖性。

Conclusion: 新方法显著提升了电荷沉积的可扩展性，同时对现有PIC代码改动极小，适用于多种硬件环境。

Abstract: The Particle-In-Cell (PIC) method for plasma simulation tracks particle phase
space information using particle and grid data structures. High computational
costs in 2D and 3D device-scale PIC simulations necessitate parallelization,
with the Charge Deposition (CD) subroutine often becoming a bottleneck due to
frequent particle-grid interactions. Conventional methods mitigate dependencies
by generating private grids for each core, but this approach faces scalability
issues. We propose a novel approach based on a particle-thread binding strategy
that requires only four private grids per node in distributed memory systems or
four private grids in shared memory systems, enhancing CD scalability and
performance while maintaining conventional data structures and requiring
minimal changes to existing PIC codes. This method ensures complete
accessibility of grid data structure for concurrent threads and avoids
simultaneous access to particles within the same cell using additional
functions and flags. Performance evaluations using a PIC benchmark for
low-temperature partially magnetized E x B discharge simulation on a shared
memory as well as a distributed memory system (1000 cores) demonstrate the
method's scalability, and additionally, we show the method has little hardware
dependency.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [72] [DRAGON: Distributional Rewards Optimize Diffusion Generative Models](https://arxiv.org/abs/2504.15217)
*Yatong Bai,Jonah Casebeer,Somayeh Sojoudi,Nicholas J. Bryan*

Main category: cs.SD

TL;DR: DRAGON是一个灵活的框架，用于优化生成模型的奖励函数，比传统方法更灵活。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统RLHF或DPO方法的局限性，提出了一个更灵活的奖励函数优化框架。

Method: 通过构造正面和负面示例集，利用对比优化奖励函数，支持多种类型的奖励计算。

Result: 在20种不同奖励函数上平均胜率达81.45%，通过合适的示例集达到60.95%的人类音乐质量胜率。

Conclusion: DRAGON展示了设计奖励函数的新方法，无需人类偏好标注即可提升感知质量。

Abstract: We present Distributional RewArds for Generative OptimizatioN (DRAGON), a
versatile framework for fine-tuning media generation models towards a desired
outcome. Compared with traditional reinforcement learning with human feedback
(RLHF) or pairwise preference approaches such as direct preference optimization
(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate
either individual examples or distributions of them, making it compatible with
a broad spectrum of instance-wise, instance-to-distribution, and
distribution-to-distribution rewards. Leveraging this versatility, we construct
novel reward functions by selecting an encoder and a set of reference examples
to create an exemplar distribution. When cross-modality encoders such as CLAP
are used, the reference examples may be of a different modality (e.g., text
versus audio). Then, DRAGON gathers online and on-policy generations, scores
them to construct a positive demonstration set and a negative set, and
leverages the contrast between the two sets to maximize the reward. For
evaluation, we fine-tune an audio-domain text-to-music diffusion model with 20
different reward functions, including a custom music aesthetics model, CLAP
score, Vendi diversity, and Frechet audio distance (FAD). We further compare
instance-wise (per-song) and full-dataset FAD settings while ablating multiple
FAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an
81.45% average win rate. Moreover, reward functions based on exemplar sets
indeed enhance generations and are comparable to model-based rewards. With an
appropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality
win rate without training on human preference annotations. As such, DRAGON
exhibits a new approach to designing and optimizing reward functions for
improving human-perceived quality. Sound examples at
https://ml-dragon.github.io/web.

</details>


### [73] [Exploring Adapter Design Tradeoffs for Low Resource Music Generation](https://arxiv.org/abs/2506.21298)
*Atharva Mehta,Shivam Chauhan,Monojit Choudhury*

Main category: cs.SD

TL;DR: 研究了不同适配器配置对MusicGen和Mustango音乐生成模型在低资源音乐风格上的效果，发现卷积适配器适合局部细节，变换器适配器适合长程依赖，40M参数的适配器效果最佳；Mustango生成多样但训练耗时，MusicGen训练快效果优但冗余略高。


<details>
  <summary>Details</summary>
Motivation: 探索在低资源音乐风格中，如何通过参数高效微调技术（PEFT）优化适配器设计，以在保持性能的同时降低计算成本。

Method: 对比卷积和变换器适配器在MusicGen和Mustango模型上的表现，分析其在Hindustani Classical和Turkish Makam音乐风格中的效果。

Result: 卷积适配器擅长捕捉局部音乐细节，变换器适配器适合长程依赖；40M参数适配器在表达性和质量间取得平衡；Mustango多样性高但训练慢，MusicGen训练快但冗余略高。

Conclusion: 适配器的设计需根据音乐风格需求选择；Mustango适合多样性要求高的场景，MusicGen适合快速高效生成；40M参数适配器是折中方案。

Abstract: Fine-tuning large-scale music generation models, such as MusicGen and
Mustango, is a computationally expensive process, often requiring updates to
billions of parameters and, therefore, significant hardware resources.
Parameter-Efficient Fine-Tuning (PEFT) techniques, particularly adapter-based
methods, have emerged as a promising alternative, enabling adaptation with
minimal trainable parameters while preserving model performance. However, the
design choices for adapters, including their architecture, placement, and size,
are numerous, and it is unclear which of these combinations would produce
optimal adapters and why, for a given case of low-resource music genre. In this
paper, we attempt to answer this question by studying various adapter
configurations for two AI music models, MusicGen and Mustango, on two genres:
Hindustani Classical and Turkish Makam music.
  Our findings reveal distinct trade-offs: convolution-based adapters excel in
capturing fine-grained local musical details such as ornamentations and short
melodic phrases, while transformer-based adapters better preserve long-range
dependencies crucial for structured improvisation. Additionally, we analyze
computational resource requirements across different adapter scales,
demonstrating how mid-sized adapters (40M parameters) achieve an optimal
balance between expressivity and quality. Furthermore, we find that Mustango, a
diffusion-based model, generates more diverse outputs with better adherence to
the description in the input prompt while lacking in providing stability in
notes, rhythm alignment, and aesthetics. Also, it is computationally intensive
and requires significantly more time to train. In contrast, autoregressive
models like MusicGen offer faster training and are more efficient, and can
produce better quality output in comparison, but have slightly higher
redundancy in their generations.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [74] [Metadata Enrichment of Long Text Documents using Large Language Models](https://arxiv.org/abs/2506.20918)
*Manika Lamba,You Peng,Sophie Nikolov,Glen Layne-Worthey,J. Stephen Downie*

Main category: cs.DL

TL;DR: 通过人工和大型语言模型结合的方法丰富了HathiTrust数字图书馆中1920至2020年英文论文的元数据，为计算社会科学等领域提供了资源。


<details>
  <summary>Details</summary>
Motivation: 为了提高数字图书馆中元数据的完整性和可访问性，以支持更广泛的研究需求。

Method: 结合人工工作和大型语言模型（LLMs）对元数据进行语义增强。

Result: 生成的元数据集为计算社会科学等领域提供了新资源，并显著提升了数字图书馆的搜索效果和可访问性。

Conclusion: 利用LLMs增强元数据对数字资源库尤为有益，尤其是在原始元数据缺失较多的情况下。

Abstract: In this project, we semantically enriched and enhanced the metadata of long
text documents, theses and dissertations, retrieved from the HathiTrust Digital
Library in English published from 1920 to 2020 through a combination of manual
efforts and large language models. This dataset provides a valuable resource
for advancing research in areas such as computational social science, digital
humanities, and information science. Our paper shows that enriching metadata
using LLMs is particularly beneficial for digital repositories by introducing
additional metadata access points that may not have originally been foreseen to
accommodate various content types. This approach is particularly effective for
repositories that have significant missing data in their existing metadata
fields, enhancing search results and improving the accessibility of the digital
repository.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [75] [RAG-VisualRec: An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation](https://arxiv.org/abs/2506.20817)
*Ali Tourani,Fatemeh Nazary,Yashar Deldjoo*

Main category: cs.IR

TL;DR: 该论文提出了一种结合LLM生成的情节描述和预告片视觉嵌入的多模态推荐系统，通过数据增强和融合策略提升推荐效果，特别是在冷启动和领域特定场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决电影领域多模态推荐系统中因元数据有限而导致的推荐效果不佳问题。

Method: 采用数据增强将稀疏元数据转化为丰富文本信号，并结合PCA、CCA等融合策略整合视觉线索，最后通过LLM驱动的重新排序步骤优化推荐。

Result: 实验证明CCA融合显著提高了召回率，LLM重新排序则进一步提升了NDCG，尤其在文本数据有限的场景中。

Conclusion: 提出的多模态推荐框架在冷启动和领域特定场景中表现优异，所有代码和数据已公开，鼓励进一步探索。

Abstract: This paper addresses the challenge of developing multimodal recommender
systems for the movie domain, where limited metadata (e.g., title, genre) often
hinders the generation of robust recommendations. We introduce a resource that
combines LLM-generated plot descriptions with trailer-derived visual embeddings
in a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and
collaborative filtering. Central to our approach is a data augmentation step
that transforms sparse metadata into richer textual signals, alongside fusion
strategies (e.g., PCA, CCA) that integrate visual cues. Experimental
evaluations demonstrate that CCA-based fusion significantly boosts recall
compared to unimodal baselines, while an LLM-driven re-ranking step further
improves NDCG, particularly in scenarios with limited textual data. By
releasing this framework, we invite further exploration of multi-modal
recommendation techniques tailored to cold-start, novelty-focused, and
domain-specific settings. All code, data, and detailed documentation are
publicly available at: https://github.com/RecSys-lab/RAG-VisualRec

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [76] [When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact](https://arxiv.org/abs/2506.20442)
*Tianyao Shi,Ritbik Kumar,Inez Hua,Yi Ding*

Main category: cs.CY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Biodiversity loss is a critical planetary boundary, yet its connection to
computing remains largely unexamined. Prior sustainability efforts in computing
have focused on carbon and water, overlooking biodiversity due to the lack of
appropriate metrics and modeling frameworks. This paper presents the first
end-to-end analysis of biodiversity impact from computing systems. We introduce
two new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity
Index (OBI)--to quantify biodiversity impact across the lifecycle, and present
FABRIC, a modeling framework that links computing workloads to biodiversity
impacts. Our evaluation highlights the need to consider biodiversity alongside
carbon and water in sustainable computing design and optimization. The code is
available at https://github.com/TianyaoShi/FABRIC.

</details>
