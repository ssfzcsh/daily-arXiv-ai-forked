{"id": "2506.19897", "pdf": "https://arxiv.org/pdf/2506.19897", "abs": "https://arxiv.org/abs/2506.19897", "authors": ["Christopher Glasz", "Emily Escamilla", "Eric O. Scott", "Anand Patel", "Jacob Zimmer", "Colin Diggs", "Michael Doyle", "Scott Rosen", "Nitin Naik", "Justin F. Brunelle", "Samruddhi Thaker", "Parthav Poudel", "Arun Sridharan", "Amit Madan", "Doug Wendt", "William Macke", "Thomas Schill"], "title": "Can LLMs Replace Humans During Code Chunking?", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have become essential tools in computer science,\nespecially for tasks involving code understanding and generation. However,\nexisting work does not address many of the unique challenges presented by code\nwritten for government applications. In particular, government enterprise\nsoftware is often written in legacy languages like MUMPS or assembly language\ncode (ALC) and the overall token lengths of these systems exceed the context\nwindow size for current commercially available LLMs. Additionally, LLMs are\nprimarily trained on modern software languages and have undergone limited\ntesting with legacy languages, making their ability to understand legacy\nlanguages unknown and, hence, an area for empirical study. This paper examines\nthe application of LLMs in the modernization of legacy government code written\nin ALC and MUMPS, addressing the challenges of input limitations. We\ninvestigate various code-chunking methods to optimize the generation of summary\nmodule comments for legacy code files, evaluating the impact of code-chunking\nmethods on the quality of documentation produced by different LLMs, including\nGPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs\ncan select partition points closely aligned with human expert partitioning. We\nalso find that chunking approaches have significant impact on downstream tasks\nsuch as documentation generation. LLM-created partitions produce comments that\nare up to 20% more factual and up to 10% more useful than when humans create\npartitions. Therefore, we conclude that LLMs can be used as suitable\nreplacements for human partitioning of large codebases during LLM-aided\nmodernization."}
{"id": "2506.20063", "pdf": "https://arxiv.org/pdf/2506.20063", "abs": "https://arxiv.org/abs/2506.20063", "authors": ["Zixuan Feng", "Thomas Zimmermann", "Lorenzo Pisani", "Christopher Gooley", "Jeremiah Wander", "Anita Sarma"], "title": "When Domains Collide: An Activity Theory Exploration of Cross-Disciplinary Collaboration", "categories": ["cs.SE"], "comment": "Cross-disciplinary Collaboration, Activity Theory, Mixed-Methods", "summary": "Background: Software development teams are increasingly diverse, embedded,\nand cross-disciplinary. Domain experts (DEs) from different disciplines\ncollaborate with professional software developers (SDEs), bringing\ncomplementary expertise in creating and maintaining complex production\nsoftware. However, contested expectations, divergent problem-solving\nperspectives, and conflicting priorities lead to friction. Aims: This study\naims to investigate the dynamics of emerging collaboration of\ncross-disciplinary software development (CDSD) by exploring the expectations\nheld by DEs and SDEs and understanding how these frictions manifest in\npractice. Method: We utilize Activity Theory (AT), a well-established\nsocio-technical framework, as an analytical lens in a grounded, empirical\ninvestigation, conducted through a mixed-method study involving 24 interviews\n(12 DEs and 12 SDEs) and a large-scale validation survey with 293 participants\n(161 DEs and 132 SDEs). Results: We conceptualize and empirically ground the\nCDSD dynamics. We identified eight expectations held by SDEs and six by DEs. By\nmapping these expectations to AT components, we revealed 21 frictions in CDSD\nand illustrated where and how they arise. Conclusions: This study offers a\ntheoretical lens for understanding the dynamics and frictions in CDSD and\nprovides actionable insights for future research, practitioners, and\ninfrastructure design."}
{"id": "2506.20159", "pdf": "https://arxiv.org/pdf/2506.20159", "abs": "https://arxiv.org/abs/2506.20159", "authors": ["Tomas Herda", "Victoria Pichler", "Zheying Zhang", "Pekka Abrahamsson", "Geir K. Hanssen"], "title": "AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The full-day workshop on AI and Agile at XP 2025 convened a diverse group of\nresearchers and industry practitioners to address the practical challenges and\nopportunities of integrating Artificial Intelligence into Agile software\ndevelopment. Through interactive sessions, participants identified shared\nfrustrations related to integrating AI into Agile Software Development\npractices, including challenges with tooling, governance, data quality, and\ncritical skill gaps. These challenges were systematically prioritized and\nanalyzed to uncover root causes. The workshop culminated in the collaborative\ndevelopment of a research roadmap that pinpoints actionable directions for\nfuture work, including both immediate solutions and ambitious long-term goals.\nThe key outcome is a structured agenda designed to foster joint\nindustry-academic efforts to move from identified frustrations to successful\nimplementation."}
{"id": "2506.20217", "pdf": "https://arxiv.org/pdf/2506.20217", "abs": "https://arxiv.org/abs/2506.20217", "authors": ["Stuart M. Allen", "Neil Chue Hong", "Stephan Druskat", "Toby Hodges", "Daniel S. Katz", "Jan Linxweiler", "Frank Löffler", "Lars Grunske", "Heidi Seibold", "Jan Philipp Thiele", "Samantha Wittke"], "title": "Ten simple rules for PIs to integrate Research Software Engineering into their research group", "categories": ["cs.SE", "cs.CE", "cs.CY", "68-01", "K.6.3"], "comment": "10 pages, submitted to PLOS Computational Biology", "summary": "Research Software Engineering (RSEng) is a key success factor in producing\nhigh-quality research software, which in turn enables and improves research\noutcomes. However, as a principal investigator or leader of a research group\nyou may not know what RSEng is, where to get started with it, or how to use it\nto maximize its benefit for your research. RSEng also often comes with\ntechnical complexity, and therefore reduced accessibility to some researchers.\nThe ten simple rules presented in this paper aim to improve the accessibility\nof RSEng, and provide practical and actionable advice to PIs and leaders for\nintegrating RSEng into their research group. By following these rules, readers\ncan improve the quality, reproducibility, and trustworthiness of their research\nsoftware, ultimately leading to better, more reproducible and more trustworthy\nresearch outcomes."}
{"id": "2506.19995", "pdf": "https://arxiv.org/pdf/2506.19995", "abs": "https://arxiv.org/abs/2506.19995", "authors": ["Blade Frisch", "Keith Vertanen"], "title": "Refining Participatory Design for AAC Users", "categories": ["cs.HC"], "comment": null, "summary": "Augmentative and alternative communication (AAC) is a field of research and\npractice that works with people who have a communication disability. One form\nAAC can take is a high-tech tool, such as a software-based communication\nsystem. Like all user interfaces, these systems must be designed and it is\ncritical to include AAC users in the design process for their systems. A\nparticipatory design approach can include AAC users in the design process, but\nmodifications may be necessary to make these methods more accessible. We\npresent a two-part design process we are investigating for improving the\nparticipatory design for high-tech AAC systems. We discuss our plans to refine\nthe accessibility of this process based on participant feedback."}
{"id": "2506.20202", "pdf": "https://arxiv.org/pdf/2506.20202", "abs": "https://arxiv.org/abs/2506.20202", "authors": ["Da Li", "Donggang Jia", "Yousef Rajeh", "Dominik Engel", "Ivan Viola"], "title": "RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer", "categories": ["cs.GR"], "comment": null, "summary": "With the advancement of Gaussian Splatting techniques, a growing number of\ndatasets based on this representation have been developed. However, performing\naccurate and efficient clipping for Gaussian Splatting remains a challenging\nand unresolved problem, primarily due to the volumetric nature of Gaussian\nprimitives, which makes hard clipping incapable of precisely localizing their\npixel-level contributions. In this paper, we propose a hybrid rendering\nframework that combines rasterization and ray tracing to achieve efficient and\nhigh-fidelity clipping of Gaussian Splatting data. At the core of our method is\nthe RaRa strategy, which first leverages rasterization to quickly identify\nGaussians intersected by the clipping plane, followed by ray tracing to compute\nattenuation weights based on their partial occlusion. These weights are then\nused to accurately estimate each Gaussian's contribution to the final image,\nenabling smooth and continuous clipping effects. We validate our approach on\ndiverse datasets, including general Gaussians, hair strand Gaussians, and\nmulti-layer Gaussians, and conduct user studies to evaluate both perceptual\nquality and quantitative performance. Experimental results demonstrate that our\nmethod delivers visually superior results while maintaining real-time rendering\nperformance and preserving high fidelity in the unclipped regions."}
{"id": "2506.16116", "pdf": "https://arxiv.org/pdf/2506.16116", "abs": "https://arxiv.org/abs/2506.16116", "authors": ["Ignacio Hernández Montilla", "Alfonso Medela", "Paola Pasquali", "Andy Aguilar", "Taig Mac Carthy", "Gerardo Fernández", "Antonio Martorell", "Enrique Onieva"], "title": "Enhanced Dermatology Image Quality Assessment via Cross-Domain Training", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": "9 pages, 4 figures. This manuscript has been accepted to the 2025\n  12th International Conference on Bioinformatics Research and Applications\n  (ICBRA 2025). It will be published in International Conference Proceedings by\n  ACM, which will be archived in ACM Digital Library, indexed by Ei Compendex\n  and Scopus", "summary": "Teledermatology has become a widely accepted communication method in daily\nclinical practice, enabling remote care while showing strong agreement with\nin-person visits. Poor image quality remains an unsolved problem in\nteledermatology and is a major concern to practitioners, as bad-quality images\nreduce the usefulness of the remote consultation process. However, research on\nImage Quality Assessment (IQA) in dermatology is sparse, and does not leverage\nthe latest advances in non-dermatology IQA, such as using larger image\ndatabases with ratings from large groups of human observers. In this work, we\npropose cross-domain training of IQA models, combining dermatology and\nnon-dermatology IQA datasets. For this purpose, we created a novel dermatology\nIQA database, Legit.Health-DIQA-Artificial, using dermatology images from\nseveral sources and having them annotated by a group of human observers. We\ndemonstrate that cross-domain training yields optimal performance across\ndomains and overcomes one of the biggest limitations in dermatology IQA, which\nis the small scale of data, and leads to models trained on a larger pool of\nimage distortions, resulting in a better management of image quality in the\nteledermatology process."}
{"id": "2506.20176", "pdf": "https://arxiv.org/pdf/2506.20176", "abs": "https://arxiv.org/abs/2506.20176", "authors": ["Yuri Andriaccio", "Vincenzo Ciancia", "Diego Latella", "Mieke Massink"], "title": "Practical Exploration of Polyhedral Model Checking", "categories": ["cs.LO", "68N30 Mathematical aspects of software engineering (specification,\n  verification, metrics, requirements, etc.)"], "comment": null, "summary": "This work explores the potential of spatial model checking of polyhedral\nmodels on a number of selected examples. In computer graphics polyhedral models\ncan be found in the form of triangular surface meshes of tetrahedral volume\nmeshes which are abundant. Spatial model checking is used to analyse spatial\nproperties of interest of such models expressed in a suitable spatial logic.\nThe original contributions of this paper are twofold. First we illustrate how a\npolyhedral model can be enriched by adding the outcome of one model checking\nsession as an atomic proposition to the original model. This is useful as it\nprovides a way to reduce the length of formulas to check on such models and to\nobtain more insightful results when these models are used for graphical\nvisualisation. Second we show that this form of enrichment also enables\npractical model minimisation providing deeper insights in the basic spatial\nstructure of the model in terms of the spatial logic properties it enjoys. This\nwork is performed in the context of the geometric spatial model checker\nPolyLogicA, the visualizer PolyVisualizer and the polyhedral semantics of the\nSpatial Logic for Closure Spaces SLCS."}
{"id": "2506.19947", "pdf": "https://arxiv.org/pdf/2506.19947", "abs": "https://arxiv.org/abs/2506.19947", "authors": ["Yung-Fu Chen", "Anish Arora"], "title": "MILAAP: Mobile Link Allocation via Attention-based Prediction", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "Channel hopping (CS) communication systems must adapt to interference changes\nin the wireless network and to node mobility for maintaining throughput\nefficiency. Optimal scheduling requires up-to-date network state information\n(i.e., of channel occupancy) to select non-overlapping channels for links in\ninterference regions. However, state sharing among nodes introduces significant\ncommunication overhead, especially as network size or node mobility scale,\nthereby decreasing throughput efficiency of already capacity-limited networks.\nIn this paper, we eschew state sharing while adapting the CS schedule based on\na learning-based channel occupancy prediction. We propose the MiLAAP\nattention-based prediction framework for machine learning models of spectral,\nspatial, and temporal dependencies among network nodes. MiLAAP uses a\nself-attention mechanism that lets each node capture the temporospectral CS\npattern in its interference region and accordingly predict the channel\noccupancy state within that region. Notably, the prediction relies only on\nlocally and passively observed channel activities, and thus introduces no\ncommunication overhead. To deal with node mobility, MiLAAP also uses a\nmulti-head self-attention mechanism that lets each node locally capture the\nspatiotemporal dependencies on other network nodes that can interfere with it\nand accordingly predict the motion trajectory of those nodes. Detecting nodes\nthat enter or move outside the interference region is used to further improve\nthe prediction accuracy of channel occupancy. We show that for dynamic networks\nthat use local CS sequences to support relatively long-lived flow traffics, the\nchannel state prediction accuracy of MiLAAP is remarkably ~100% across\ndifferent node mobility patterns and it achieves zero-shot generalizability\nacross different periods of CS sequences."}
{"id": "2506.19884", "pdf": "https://arxiv.org/pdf/2506.19884", "abs": "https://arxiv.org/abs/2506.19884", "authors": ["Zhengxiang Huang", "Chaoyue Niu", "Zhaode Wang", "Jiarui Xue", "Hanming Zhang", "Yugang Wang", "Zewei Xin", "Xiaotang Jiang", "Chengfei Lv", "Fan Wu", "Guihai Chen"], "title": "MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection", "categories": ["cs.OS", "cs.AI", "cs.PF", "cs.SE"], "comment": null, "summary": "As the demand for on-device Large Language Model (LLM) inference grows,\nenergy efficiency has become a major concern, especially for battery-limited\nmobile devices. Our analysis shows that the memory-bound LLM decode phase\ndominates energy use, and yet most existing works focus on accelerating the\nprefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric\nCore Selection (AECS) and integrate it into MNN to create the energy-efficient\nversion, MNN-AECS, the first engine-level system solution without requiring\nroot access or OS modifications for energy-efficient LLM decoding. MNN-AECS is\ndesigned to reduce LLM decoding energy while keeping decode speed within an\nacceptable slowdown threshold by dynamically selecting low-power CPU cores.\nMNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of\nvarious sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%\nwithout slowdown averaged over all 7 devices and 4 datasets. Against other\nengines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS\ndelivers 39% to 78% energy saving and 12% to 363% speedup on average."}
{"id": "2506.20010", "pdf": "https://arxiv.org/pdf/2506.20010", "abs": "https://arxiv.org/abs/2506.20010", "authors": ["Shu Lin", "Arunprasad P. Marathe", "Per-Ȧke Larson", "Chong Chen", "Calvin Sun", "Paul Lee", "Weidong Yu"], "title": "Near Data Processing in Taurus Database", "categories": ["cs.DB", "H.2.4"], "comment": null, "summary": "Huawei's cloud-native database system GaussDB for MySQL (also known as\nTaurus) stores data in a separate storage layer consisting of a pool of storage\nservers. Each server has considerable compute power making it possible to push\ndata reduction operations (selection, projection, and aggregation) close to\nstorage. This paper describes the design and implementation of near data\nprocessing (NDP) in Taurus. NDP has several benefits: it reduces the amount of\ndata shipped over the network; frees up CPU capacity in the compute layer; and\nreduces query run time, thereby enabling higher system throughput. Experiments\nwith the TPCH benchmark (100 GB) showed that 18 out of 22 queries benefited\nfrom NDP; data shipped was reduced by 63 percent; and CPU time by 50 percent.\nOn Q15 the impact was even higher: data shipped was reduced by 98 percent; CPU\ntime by 91 percent; and run time by 80 percent."}
{"id": "2506.20014", "pdf": "https://arxiv.org/pdf/2506.20014", "abs": "https://arxiv.org/abs/2506.20014", "authors": ["Graydon Schulze-Kalt", "Robert Pitu", "Spencer Shelton", "Catherine Todd", "Zane Ebel", "Ian Goldberg", "Leon Gold", "Henry Czarnecki", "Mason McCormack", "Larry Li", "Zumi Riekse", "Brian Yu", "Akash Piya", "Vidya Suri", "Dylan Hu", "Colleen Kim", "John Baird", "Seth Knights", "Logan Hanssler", "Michael Lembeck", "Tian Zhong"], "title": "Development of an Open-Source Spacecraft Bus for the PULSE-A CubeSat", "categories": ["physics.app-ph", "astro-ph.IM", "cs.AR", "cs.SY", "eess.SY", "physics.optics"], "comment": "Submitted to Advanced Technologies II at the 2025 SmallSat\n  Conference, reference number SSC25-P1-42", "summary": "The undergraduate-led Polarization-modUlated Laser Satellite Experiment\n(PULSE-A) at the University of Chicago seeks to demonstrate the feasibility of\ncircular polarization shift keyed satellite-to-ground laser communication.\nPULSE-A's low-cost open-source bus serves as the backbone of the mission and\nhas been designed in tandem with the Payload, with design driven by strict\nrequirements for pointing accuracy, component alignment, power demand, and\nthermal stability. This work presents the design and testing of the PULSE-A\nbus.\n  The spacecraft bus was designed to fill two major needs: (1) to meet the\nrequirements of the PULSE-A mission, and (2) to be easily configurable for\nfuture missions that desire enhanced capabilities over other low-cost\nopen-source designs. At its core, the bus features dual BeagleBone Black\nIndustrial compute units, selected for their flight heritage, integrated via a\nPC/104 header standard. PULSE-A implements Goddard Space Flight Center's core\nFlight System (cFS), which takes a modular software architecture approach and\nis built in C. The use of C as the primary language aligns with the expertise\nof the University of Chicago's Computer Science department, allowing for ease\nof development by PULSE-A's undergraduate flight software team.\n  The CubeSat structure utilizes Gran Systems' 3U frame, modified to\naccommodate openings for various ports and deployable components. Inside, the\navionics stack uses the PC/104 standard quad rails, which terminate in\nPULSE-A's custom-designed Payload Box that houses all of the Payload components\nand optical fiber runs. This work also covers the techniques and iterative\nengineering processes used to develop the thermal control and dissipation\nmechanisms for the specific requirements, under volume, mass, and\ntemperature-range constraints."}
{"id": "2506.19972", "pdf": "https://arxiv.org/pdf/2506.19972", "abs": "https://arxiv.org/abs/2506.19972", "authors": ["Federico Ruilova", "Ernst Gunnar Gran", "Sven-Arne Reinemo"], "title": "MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions", "categories": ["cs.DC", "cs.LG"], "comment": "2 pages, 2 figures. LOCO 2024, December 3, 2024, Glasgow/Online", "summary": "Cloud computing drives innovation but also poses significant environmental\nchallenges due to its high-energy consumption and carbon emissions. Data\ncenters account for 2-4% of global energy usage, and the ICT sector's share of\nelectricity consumption is projected to reach 40% by 2040. As the goal of\nachieving net-zero emissions by 2050 becomes increasingly urgent, there is a\ngrowing need for more efficient and transparent solutions, particularly for\nprivate cloud infrastructures, which are utilized by 87% of organizations,\ndespite the dominance of public-cloud systems.\n  This study evaluates the MAIZX framework, designed to optimize cloud\noperations and reduce carbon footprint by dynamically ranking resources,\nincluding data centers, edge computing nodes, and multi-cloud environments,\nbased on real-time and forecasted carbon intensity, Power Usage Effectiveness\n(PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX\nachieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor\noperations. Tested across geographically distributed data centers, the\nframework demonstrates scalability and effectiveness, directly interfacing with\nhypervisors to optimize workloads in private, hybrid, and multi-cloud\nenvironments. MAIZX integrates real-time data on carbon intensity, power\nconsumption, and carbon footprint, as well as forecasted values, into cloud\nmanagement, providing a robust tool for enhancing climate performance potential\nwhile maintaining operational efficiency."}
{"id": "2506.20589", "pdf": "https://arxiv.org/pdf/2506.20589", "abs": "https://arxiv.org/abs/2506.20589", "authors": ["Jorge Torres Gómez", "Pit Hofmann", "Lisa Y. Debus", "Osman Tugay Başaran", "Sebastian Lotter", "Roya Khanzadeh", "Stefan Angerbauer", "Bige Deniz Unluturk", "Sergi Abadal", "Werner Haselmayr", "Frank H. P. Fitzek", "Robert Schober", "Falko Dressler"], "title": "Communicating Smartly in Molecular Communication Environments: Neural Networks in the Internet of Bio-Nano Things", "categories": ["eess.SP", "cs.ET", "q-bio.OT"], "comment": "Paper submitted to IEEE Communications Surveys & Tutorials", "summary": "Recent developments in the Internet of Bio-Nano Things (IoBNT) are laying the\ngroundwork for innovative applications across the healthcare sector.\nNanodevices designed to operate within the body, managed remotely via the\ninternet, are envisioned to promptly detect and actuate on potential diseases.\nIn this vision, an inherent challenge arises due to the limited capabilities of\nindividual nanosensors; specifically, nanosensors must communicate with one\nanother to collaborate as a cluster. Aiming to research the boundaries of the\nclustering capabilities, this survey emphasizes data-driven communication\nstrategies in molecular communication (MC) channels as a means of linking\nnanosensors. Relying on the flexibility and robustness of machine learning (ML)\nmethods to tackle the dynamic nature of MC channels, the MC research community\nfrequently refers to neural network (NN) architectures. This interdisciplinary\nresearch field encompasses various aspects, including the use of NNs to\nfacilitate communication in MC environments, their implementation at the\nnanoscale, explainable approaches for NNs, and dataset generation for training.\nWithin this survey, we provide a comprehensive analysis of fundamental\nperspectives on recent trends in NN architectures for MC, the feasibility of\ntheir implementation at the nanoscale, applied explainable artificial\nintelligence (XAI) techniques, and the accessibility of datasets along with\nbest practices for their generation. Additionally, we offer open-source code\nrepositories that illustrate NN-based methods to support reproducible research\nfor key MC scenarios. Finally, we identify emerging research challenges, such\nas robust NN architectures, biologically integrated NN modules, and scalable\ntraining strategies."}
{"id": "2506.20435", "pdf": "https://arxiv.org/pdf/2506.20435", "abs": "https://arxiv.org/abs/2506.20435", "authors": ["Mennatullah T. Khedr", "John S. Fitzgerald"], "title": "The Composition of Digital Twins for Systems-of-Systems: a Systematic Literature Review", "categories": ["cs.SE"], "comment": "15 pages, 3 figures, Presented at the 23rd Overture workshop, June\n  2025 (arXiv:cs/2506.08680)", "summary": "Digital Twins (DTs) are increasingly used to model complex systems,\nespecially in Cyber-Physical Systems (CPS) and System-of-Systems (SoS), where\neffective integration is key. This systematic literature review investigates DT\ncomposition and verification and validation (V&V) methodologies. Analyzing 21\nstudies from 2022-2024, we examined composition mechanisms, SoS\ncharacteristics, and V&V formality, scope, and challenges. While composition is\ndiscussed, formalization is limited. V&V approaches vary, with semi-formal\nmethods and simulations dominating; formal verification is underutilized. Key\ntechnical challenges include model uncertainty and integration complexity.\nMethodological challenges highlight the lack of standardized DT-specific V&V\nframeworks. There is a need to move beyond model validation to address\nintegration and cyber-physical consistency. This review contributes a\nstructured classification of V&V approaches and emphasizes the need for\nstandardized, scalable V&V and rigorous composition methodologies for complex\nDT implementations."}
{"id": "2506.20055", "pdf": "https://arxiv.org/pdf/2506.20055", "abs": "https://arxiv.org/abs/2506.20055", "authors": ["Seraphina Yong", "Ashlee Milton", "Evan Suma Rosenberg", "Stevie Chancellor", "Svetlana Yarosh"], "title": "\"I'm Petting the Laptop, Which Has You Inside It\": Reflecting on Lived Experiences of Online Friendship", "categories": ["cs.HC"], "comment": "CSCW 2025, 29 article pages, 5 pages of references, 3 figures", "summary": "Online(-only) friendships have become increasingly common in daily lives\npost-COVID despite debates around their mental health benefits and equivalence\nto ''real'' relationships. Previous research has reflected a need to understand\nhow online friends engage beyond individual platforms, and the lack of\nplatform-agnostic inquiry limits our ability to fully understand the dynamics\nof online friendship. We employed an activity-grounded analysis of 25\ninterviews on lived experiences of close online friendship spanning multiple\nyears. Our findings present unique challenges and strategies in online\nfriendships, such as stigma from real-life circles, an ambivalent relationship\nwith online communities, and counter-theoretical reappropriations of\ncommunication technology. This study contributes to HCI research in online\ncommunities and social interface design by refocusing prior impressions of\nstrong vs. weak-ties in online social spaces and foregrounding time-stable\ninteractions in design for relationship maintenance through technology. Our\nwork also promotes critical reflection on biased perspectives towards\ntechnology-mediated practices and consideration of online friends as an\ninvisible marginalized community."}
{"id": "2506.20267", "pdf": "https://arxiv.org/pdf/2506.20267", "abs": "https://arxiv.org/abs/2506.20267", "authors": ["Fabian Bongratz", "Tom Nuno Wolf", "Jaume Gual Ramon", "Christian Wachinger"], "title": "X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "MICCAI 2025", "summary": "Interpretable models are crucial for supporting clinical decision-making,\ndriving advances in their development and application for medical images.\nHowever, the nature of 3D volumetric data makes it inherently challenging to\nvisualize and interpret intricate and complex structures like the cerebral\ncortex. Cortical surface renderings, on the other hand, provide a more\naccessible and understandable 3D representation of brain anatomy, facilitating\nvisualization and interactive exploration. Motivated by this advantage and the\nwidespread use of surface data for studying neurological disorders, we present\nthe eXplainable Surface Vision Transformer (X-SiT). This is the first\ninherently interpretable neural network that offers human-understandable\npredictions based on interpretable cortical features. As part of X-SiT, we\nintroduce a prototypical surface patch decoder for classifying surface patch\nembeddings, incorporating case-based reasoning with spatially corresponding\ncortical prototypes. The results demonstrate state-of-the-art performance in\ndetecting Alzheimer's disease and frontotemporal dementia while additionally\nproviding informative prototypes that align with known disease patterns and\nreveal classification errors."}
{"id": "2506.20070", "pdf": "https://arxiv.org/pdf/2506.20070", "abs": "https://arxiv.org/abs/2506.20070", "authors": ["KMA Solaiman", "Bharat Bhargava"], "title": "Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision", "categories": ["cs.IR", "cs.LG", "cs.MM"], "comment": "Submitted to ICDE'24. An earlier version of this paper appeared on\n  TechRxiv: https://www.techrxiv.org/doi/full/10.36227/techrxiv.21990284.v1,\n  uploaded on February 05, 2023", "summary": "Existing multi-media retrieval models either rely on creating a common\nsubspace with modality-specific representation models or require schema mapping\namong modalities to measure similarities among multi-media data. Our goal is to\navoid the annotation overhead incurred from considering retrieval as a\nsupervised classification task and re-use the pretrained encoders in large\nlanguage models and vision tasks. We propose \"FemmIR\", a framework to retrieve\nmultimodal results relevant to information needs expressed with multimodal\nqueries by example without any similarity label. Such identification is\nnecessary for real-world applications where data annotations are scarce and\nsatisfactory performance is required without fine-tuning with a common\nframework across applications. We curate a new dataset called MuQNOL for\nbenchmarking progress on this task. Our technique is based on weak supervision\nintroduced through edit distance between samples: graph edit distance can be\nmodified to consider the cost of replacing a data sample in terms of its\nproperties, and relevance can be measured through the implicit signal from the\namount of edit cost among the objects. Unlike metric learning or encoding\nnetworks, FemmIR re-uses the high-level properties and maintains the property\nvalue and relationship constraints with a multi-level interaction score between\ndata samples and the query example provided by the user. We empirically\nevaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs\ncomparably to similar retrieval systems in delivering on-demand retrieval\nresults with exact and approximate similarities while using the existing\nproperty identifiers in the system."}
{"id": "2506.20310", "pdf": "https://arxiv.org/pdf/2506.20310", "abs": "https://arxiv.org/abs/2506.20310", "authors": ["Ion Chirica", "Mário Pereira"], "title": "Unfolding Iterators: Specification and Verification of Higher-Order Iterators, in OCaml", "categories": ["cs.PL", "cs.LO"], "comment": null, "summary": "Albeit being a central notion of every programming language, formally and\nmodularly reasoning about iteration proves itself to be a non-trivial feat,\nspecially in the context of higher-order iteration. In this paper, we present a\ngeneric approach to the specification and deductive verification of\nhigher-order iterators, written in the OCaml language. Our methodology follows\ntwo key principles: first, the usage of the Gospel specification language to\ndescribe the general behaviour of any iteration schema; second, the usage of\nthe Cameleer framework to deductively verify that every iteration client is\ncorrect with respect to its logical specification. To validate our approach we\ndevelop a set of verified case studies, ranging from classic list iterators to\ngraph algorithms implemented in the widely used OCamlGraph library."}
{"id": "2506.19974", "pdf": "https://arxiv.org/pdf/2506.19974", "abs": "https://arxiv.org/abs/2506.19974", "authors": ["Indrakshi Dey", "Nicola Marchetti"], "title": "Notes on Degeneracy and Robustness", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "Degeneracy is the ability of structurally different elements to perform the\nsame function or yield the same output under certain constraints. In contrast\nto redundancy, which implies identical backups, degeneracy allows diverse\ncomponents to step in and perform the same or similar role. Mathematically, it\nis about mapping multiple distinct elements into the same function. In a\ndegenerate system, failure in one part can be compensated by others not\nstructurally linked. System functions are distributed within the system itself\nor the entire network. This renders faster and more adaptive recovery. In this\nwork, we define and formulate several novel metrics for resource fungibility to\naddress robustness in networks (static/mobile/dynamic)."}
{"id": "2506.19892", "pdf": "https://arxiv.org/pdf/2506.19892", "abs": "https://arxiv.org/abs/2506.19892", "authors": ["Isaac Marroqui Penalva", "Enrique Tomás Martínez Beltrán", "Manuel Gil Pérez", "Alberto Huertas Celdrán"], "title": "RepuNet: A Reputation System for Mitigating Malicious Clients in DFL", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG", "cs.PF"], "comment": null, "summary": "Decentralized Federated Learning (DFL) enables nodes to collaboratively train\nmodels without a central server, introducing new vulnerabilities since each\nnode independently selects peers for model aggregation. Malicious nodes may\nexploit this autonomy by sending corrupted models (model poisoning), delaying\nmodel submissions (delay attack), or flooding the network with excessive\nmessages, negatively affecting system performance. Existing solutions often\ndepend on rigid configurations or additional infrastructures such as\nblockchain, leading to computational overhead, scalability issues, or limited\nadaptability. To overcome these limitations, this paper proposes RepuNet, a\ndecentralized reputation system that categorizes threats in DFL and dynamically\nevaluates node behavior using metrics like model similarity, parameter changes,\nmessage latency, and communication volume. Nodes' influence in model\naggregation is adjusted based on their reputation scores. RepuNet was\nintegrated into the Nebula DFL platform and experimentally evaluated with MNIST\nand CIFAR-10 datasets under non-IID distributions, using federations of up to\n25 nodes in both fully connected and random topologies. Different attack\nintensities, frequencies, and activation intervals were tested. Results\ndemonstrated that RepuNet effectively detects and mitigates malicious behavior,\nachieving F1 scores above 95% for MNIST scenarios and approximately 76% for\nCIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,\nand practical potential for mitigating threats in decentralized federated\nlearning environments."}
{"id": "2506.20139", "pdf": "https://arxiv.org/pdf/2506.20139", "abs": "https://arxiv.org/abs/2506.20139", "authors": ["Jiayong Qin", "Xianyu Zhu", "Qiyu Liu", "Guangyi Zhang", "Zhigang Cai", "Jianwei Liao", "Sha Hu", "Jingshu Peng", "Yingxia Shao", "Lei Chen"], "title": "Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "A growing trend in the database and system communities is to augment\nconventional index structures, such as B+-trees, with machine learning (ML)\nmodels. Among these, error-bounded Piecewise Linear Approximation\n($\\epsilon$-PLA) has emerged as a popular choice due to its simplicity and\neffectiveness. Despite its central role in many learned indexes, the design and\nanalysis of $\\epsilon$-PLA fitting algorithms remain underexplored. In this\npaper, we revisit $\\epsilon$-PLA from both theoretical and empirical\nperspectives, with a focus on its application in learned index structures. We\nfirst establish a fundamentally improved lower bound of $\\Omega(\\kappa \\cdot\n\\epsilon^2)$ on the expected segment coverage for existing $\\epsilon$-PLA\nfitting algorithms, where $\\kappa$ is a data-dependent constant. We then\npresent a comprehensive benchmark of state-of-the-art $\\epsilon$-PLA algorithms\nwhen used in different learned data structures. Our results highlight key\ntrade-offs among model accuracy, model size, and query performance, providing\nactionable guidelines for the principled design of future learned data\nstructures."}
{"id": "2506.20018", "pdf": "https://arxiv.org/pdf/2506.20018", "abs": "https://arxiv.org/abs/2506.20018", "authors": ["Zechun Deng", "Ziwei Liu", "Ziqian Bi", "Junhao Song", "Chia Xin Liang", "Joe Yeong", "Junfeng Hao"], "title": "Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models", "categories": ["cs.AI", "cs.AR"], "comment": null, "summary": "This paper investigates real-time decision support systems that leverage\nlow-latency AI models, bringing together recent progress in holistic AI-driven\ndecision tools, integration with Edge-IoT technologies, and approaches for\neffective human-AI teamwork. It looks into how large language models can assist\ndecision-making, especially when resources are limited. The research also\nexamines the effects of technical developments such as DeLLMa, methods for\ncompressing models, and improvements for analytics on edge devices, while also\naddressing issues like limited resources and the need for adaptable frameworks.\nThrough a detailed review, the paper offers practical perspectives on\ndevelopment strategies and areas of application, adding to the field by\npointing out opportunities for more efficient and flexible AI-supported\nsystems. The conclusions set the stage for future breakthroughs in this\nfast-changing area, highlighting how AI can reshape real-time decision support."}
{"id": "2506.20218", "pdf": "https://arxiv.org/pdf/2506.20218", "abs": "https://arxiv.org/abs/2506.20218", "authors": ["Francesco d'Amore", "Niccolò D'Archivio", "George Giakkoupis", "Emanuele Natale"], "title": "On the $h$-majority dynamics with many opinions", "categories": ["cs.DC", "cs.MA"], "comment": null, "summary": "We present the first upper bound on the convergence time to consensus of the\nwell-known $h$-majority dynamics with $k$ opinions, in the synchronous setting,\nfor $h$ and $k$ that are both non-constant values.\n  We suppose that, at the beginning of the process, there is some initial\nadditive bias towards some plurality opinion, that is, there is an opinion that\nis supported by $x$ nodes while any other opinion is supported by strictly\nfewer nodes.\n  We prove that, with high probability, if the bias is $\\omega(\\sqrt{x})$ and\nthe initial plurality opinion is supported by at least $x = \\omega(\\log n)$\nnodes, then the process converges to plurality consensus in $O(\\log n)$ rounds\nwhenever $h = \\omega(n \\log n / x)$.\n  A main corollary is the following: if $k = o(n / \\log n)$ and the process\nstarts from an almost-balanced configuration with an initial bias of magnitude\n$\\omega(\\sqrt{n/k})$ towards the initial plurality opinion, then any function\n$h = \\omega(k \\log n)$ suffices to guarantee convergence to consensus in\n$O(\\log n)$ rounds, with high probability.\n  Our upper bound shows that the lower bound of $\\Omega(k / h^2)$ rounds to\nreach consensus given by Becchetti et al.\\ (2017) cannot be pushed further than\n$\\widetilde{\\Omega}(k / h)$.\n  Moreover, the bias we require is asymptotically smaller than the\n$\\Omega(\\sqrt{n\\log n})$ bias that guarantees plurality consensus in the\n$3$-majority dynamics: in our case, the required bias is at most any\n(arbitrarily small) function in $\\omega(\\sqrt{x})$ for any value of $k \\ge 2$."}
{"id": "2506.20444", "pdf": "https://arxiv.org/pdf/2506.20444", "abs": "https://arxiv.org/abs/2506.20444", "authors": ["Xiang Lan", "Tim Menzies", "Bowen Xu"], "title": "Smart Cuts: Enhance Active Learning for Vulnerability Detection by Pruning Bad Seeds", "categories": ["cs.SE"], "comment": null, "summary": "Vulnerability detection is crucial for identifying security weaknesses in\nsoftware systems. However, the effectiveness of machine learning models in this\ndomain is often hindered by low-quality training datasets, which contain noisy,\nmislabeled, or imbalanced samples. This paper proposes a novel dataset\nmaps-empowered approach that systematically identifies and mitigates\nhard-to-learn outliers, referred to as \"bad seeds\", to improve model training\nefficiency. Our approach can categorize training examples based on learning\ndifficulty and integrate this information into an active learning framework.\nUnlike traditional methods that focus on uncertainty-based sampling, our\nstrategy prioritizes dataset quality by filtering out performance-harmful\nsamples while emphasizing informative ones. Our experimental results show that\nour approach can improve F1 score over random selection by 45.36% (DeepGini)\nand 45.91% (K-Means) and outperforms standard active learning by 61.46%\n(DeepGini) and 32.65% (K-Means) for CodeBERT on the Big-Vul dataset,\ndemonstrating the effectiveness of integrating dataset maps for optimizing\nsample selection in vulnerability detection. Furthermore, our approach also\nenhances model robustness, improves sample selection by filtering bad seeds,\nand stabilizes active learning performance across iterations. By analyzing the\ncharacteristics of these outliers, we provide insights for future improvements\nin dataset construction, making vulnerability detection more reliable and\ncost-effective."}
{"id": "2506.20062", "pdf": "https://arxiv.org/pdf/2506.20062", "abs": "https://arxiv.org/abs/2506.20062", "authors": ["Runlong Ye", "Zeling Zhang", "Boushra Almazroua", "Michael Liut"], "title": "Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "AI-powered code assistants are widely used to generate code completions,\nsignificantly boosting developer productivity. However, these tools typically\npresent suggestions without explaining their rationale, leaving their\ndecision-making process inscrutable. This opacity hinders developers' ability\nto critically evaluate the output, form accurate mental models, and build\ncalibrated trust in the system. To address this, we introduce CopilotLens, a\nnovel interactive framework that reframes code completion from a simple\nsuggestion into a transparent, explainable event. CopilotLens operates as an\nexplanation layer that reveals the AI agent's \"thought process\" through a\ndynamic two-level interface, surfacing everything from its reconstructed\nhigh-level plans to the specific codebase context influencing the code. This\npaper presents the design and rationale of CopilotLens, offering a concrete\nframework for building future agentic code assistants that prioritize clarity\nof reasoning over speed of suggestion, thereby fostering deeper comprehension\nand more robust human-AI collaboration."}
{"id": "2506.20367", "pdf": "https://arxiv.org/pdf/2506.20367", "abs": "https://arxiv.org/abs/2506.20367", "authors": ["Edoardo Alberto Dominici", "Jozef Hladky", "Floor Verhoeven", "Lukas Radl", "Thomas Deixelberger", "Stefan Ainetter", "Philipp Drescher", "Stefan Hauswiesner", "Arno Coomans", "Giacomo Nazzaro", "Konstantinos Vardis", "Markus Steinberger"], "title": "DreamAnywhere: Object-Centric Panoramic 3D Scene Generation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advances in text-to-3D scene generation have demonstrated significant\npotential to transform content creation across multiple industries. Although\nthe research community has made impressive progress in addressing the\nchallenges of this complex task, existing methods often generate environments\nthat are only front-facing, lack visual fidelity, exhibit limited scene\nunderstanding, and are typically fine-tuned for either indoor or outdoor\nsettings. In this work, we address these issues and propose DreamAnywhere, a\nmodular system for the fast generation and prototyping of 3D scenes. Our system\nsynthesizes a 360{\\deg} panoramic image from text, decomposes it into\nbackground and objects, constructs a complete 3D representation through hybrid\ninpainting, and lifts object masks to detailed 3D objects that are placed in\nthe virtual environment. DreamAnywhere supports immersive navigation and\nintuitive object-level editing, making it ideal for scene exploration, visual\nmock-ups, and rapid prototyping -- all with minimal manual modeling. These\nfeatures make our system particularly suitable for low-budget movie production,\nenabling quick iteration on scene layout and visual tone without the overhead\nof traditional 3D workflows. Our modular pipeline is highly customizable as it\nallows components to be replaced independently. Compared to current\nstate-of-the-art text and image-based 3D scene generation approaches,\nDreamAnywhere shows significant improvements in coherence in novel view\nsynthesis and achieves competitive image quality, demonstrating its\neffectiveness across diverse and challenging scenarios. A comprehensive user\nstudy demonstrates a clear preference for our method over existing approaches,\nvalidating both its technical robustness and practical usefulness."}
{"id": "2506.20214", "pdf": "https://arxiv.org/pdf/2506.20214", "abs": "https://arxiv.org/abs/2506.20214", "authors": ["Yanzhe Chen", "Huasong Zhong", "Yan Li", "Zhenheng Yang"], "title": "UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.MM"], "comment": "19 pages, 5 figures", "summary": "Unified multimodal large language models (MLLMs) have shown promise in\njointly advancing multimodal understanding and generation, with visual\ncodebooks discretizing images into tokens for autoregressive modeling. Existing\ncodebook-based methods either rely on small vocabularies (~16K entries) that\nlack fine-grained semantics or naively scale up, resulting in low token\nutilization and unstable training. We propose UniCode$^2$, a cascaded codebook\nframework enabling large-scale, semantically aligned, and stable visual\ntokenization. By clustering millions of SigLIP sequence embeddings, we build a\n500K-entry codebook that preserves vision-language alignment while expanding\ncapacity. Stability is ensured via a cascaded design: a frozen codebook anchors\nthe embedding space, and a trainable codebook refines task-specific semantics.\nThis decoupling promotes high utilization and robust learning. Moreover, the\nalignment of our visual tokens with textual semantics enables seamless\nintegration with pretrained diffusion decoders, supporting high-quality visual\nsynthesis with minimal adaptation. UniCode^2 delivers strong performance across\ndiverse benchmarks, demonstrating the viability of scaling visual token spaces\nwithout sacrificing stability, semantics, or modularity."}
{"id": "2506.20111", "pdf": "https://arxiv.org/pdf/2506.20111", "abs": "https://arxiv.org/abs/2506.20111", "authors": ["Mario R. Guarracino", "Pierre Miasnikof", "Alexander Y. Shestopaloff", "Houyem Demni", "Cristián Bravo", "Yuri Lawryshyn"], "title": "A clusterability test for directed graphs", "categories": ["cs.NI", "62"], "comment": "22 pages, 6 figures", "summary": "In this article, we extend a statistical test of graph clusterability, the\n$\\delta$ test, to directed graphs with no self loops. The $\\delta$ test,\noriginally designed for undirected graphs, is based on the premise that graphs\nwith a clustered structure display a mean local density that is statistically\nhigher than the graph's global density. We posit that graphs that do not meet\nthis necessary (but not sufficient) condition for clusterability can be\nconsidered unsuited to clustering. In such cases, vertex clusters do not offer\na meaningful summary of the broader graph. Additionally in this study, we aim\nto determine the optimal sample size (number of neighborhoods). Our test,\ndesigned for the analysis of large networks, is based on sampling subsets of\nneighborhoods/nodes. It is designed for cases where computing the density of\nevery node's neighborhood is infeasible. Our results show that the $\\delta$\ntest performs very well, even with very small samples of neighborhoods ($1\\%$).\nIt accurately detects unclusterable graphs and is also shown to be robust to\ndepartures from the underlying assumptions of the $t$ test."}
{"id": "2506.19943", "pdf": "https://arxiv.org/pdf/2506.19943", "abs": "https://arxiv.org/abs/2506.19943", "authors": ["Juyoul Lee", "Sanzida Hoque", "Abdullah Aydeger", "Engin Zeydan"], "title": "Quantum-Resistant Domain Name System: A Comprehensive System-Level Study", "categories": ["cs.CR", "cs.NI", "cs.PF"], "comment": "Manuscript submitted to ACM, 29 pages, 8 Figures, 15 Tables", "summary": "The Domain Name System (DNS) plays a foundational role in Internet\ninfrastructure, yet its core protocols remain vulnerable to compromise by\nquantum adversaries. As cryptographically relevant quantum computers become a\nrealistic threat, ensuring DNS confidentiality, authenticity, and integrity in\nthe post-quantum era is imperative. In this paper, we present a comprehensive\nsystem-level study of post-quantum DNS security across three widely deployed\nmechanisms: DNSSEC, DNS-over-TLS (DoT), and DNS-over-HTTPS (DoH). We propose\nPost-Quantum Cryptographic (PQC)-DNS, a unified framework for benchmarking DNS\nsecurity under legacy, post-quantum, and hybrid cryptographic configurations.\nOur implementation leverages the Open Quantum Safe (OQS) libraries and\nintegrates lattice- and hash-based primitives into BIND9 and TLS 1.3 stacks. We\nformalize performance and threat models and analyze the impact of post-quantum\nkey encapsulation and digital signatures on end-to-end DNS resolution.\nExperimental results on a containerized testbed reveal that lattice-based\nprimitives such as Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) and\nFalcon offer practical latency and resource profiles, while hash-based schemes\nlike SPHINCS+ significantly increase message sizes and processing overhead. We\nalso examine security implications including downgrade risks, fragmentation\nvulnerabilities, and susceptibility to denial-of-service amplification. Our\nfindings inform practical guidance for deploying quantum-resilient DNS and\ncontribute to the broader effort of securing core Internet protocols for the\npost-quantum future."}
{"id": "2506.20023", "pdf": "https://arxiv.org/pdf/2506.20023", "abs": "https://arxiv.org/abs/2506.20023", "authors": ["Ryan Hildebrant", "Rahul Bhope", "Sharad Mehrotra", "Christopher Tull", "Nalini Venkatasubramanian"], "title": "DIM-SUM: Dynamic IMputation for Smart Utility Management", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "Time series imputation models have traditionally been developed using\ncomplete datasets with artificial masking patterns to simulate missing values.\nHowever, in real-world infrastructure monitoring, practitioners often encounter\ndatasets where large amounts of data are missing and follow complex,\nheterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for\ntraining robust imputation models that bridges the gap between artificially\nmasked training data and real missing patterns. DIM-SUM combines pattern\nclustering and adaptive masking strategies with theoretical learning guarantees\nto handle diverse missing patterns actually observed in the data. Through\nextensive experiments on over 2 billion readings from California water\ndistricts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM\noutperforms traditional methods by reaching similar accuracy with lower\nprocessing time and significantly less training data. When compared against a\nlarge pre-trained model, DIM-SUM averages 2x higher accuracy with significantly\nless inference time."}
{"id": "2506.20252", "pdf": "https://arxiv.org/pdf/2506.20252", "abs": "https://arxiv.org/abs/2506.20252", "authors": ["Sylvain Jeaugey"], "title": "PAT: a new algorithm for all-gather and reduce-scatter operations at scale", "categories": ["cs.DC"], "comment": null, "summary": "This paper describes a new algorithm called PAT, for Parallel Aggregated\nTrees, and which can be used to implement all-gather and reduce-scatter\noperations. This algorithm works on any number of ranks, has a logarithmic\nnumber of network transfers for small size operations, minimizes long-distance\ncommunication, and requires a logarithmic amount of internal buffers,\nindependently from the total operation size. It is aimed at improving the\nperformance of the NCCL library in cases where the ring algorithm would be\ninefficient, as its linear latency would show poor performance for small sizes\nand/or at scale."}
{"id": "2506.20551", "pdf": "https://arxiv.org/pdf/2506.20551", "abs": "https://arxiv.org/abs/2506.20551", "authors": ["Soumya Madireddy", "Lu Gao", "Zia Din", "Kinam Kim", "Ahmed Senouci", "Zhe Han", "Yunpeng Zhang"], "title": "Large Language Model-Driven Code Compliance Checking in Building Information Modeling", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects."}
{"id": "2506.20091", "pdf": "https://arxiv.org/pdf/2506.20091", "abs": "https://arxiv.org/abs/2506.20091", "authors": ["Sarah Schömbs", "Yan Zhang", "Jorge Goncalves", "Wafa Johal"], "title": "From Conversation to Orchestration: HCI Challenges and Opportunities in Interactive Multi-Agentic Systems", "categories": ["cs.HC"], "comment": null, "summary": "Recent advances in multi-agentic systems (e.g. AutoGen, OpenAI Swarm) allow\nusers to interact with a group of specialised AI agents rather than a single\ngeneral-purpose agent. Despite the promise of this new paradigm, the HCI\ncommunity has yet to fully examine the opportunities, risks, and user-centred\nchallenges it introduces. We contribute to research on multi-agentic systems by\nexploring their architectures and key features through a human-centred lens.\nWhile literature and use cases remain limited, we build on existing tools and\nframeworks available to developers to identify a set of overarching challenges,\ne.g. orchestration and conflict resolution, that can guide future research in\nHCI. We illustrate these challenges through examples, offer potential design\nconsiderations, and provide research opportunities to spark interdisciplinary\nconversation. Our work lays the groundwork for future exploration and offers a\nresearch agenda focused on user-centred design in multi-agentic systems."}
{"id": "2506.20652", "pdf": "https://arxiv.org/pdf/2506.20652", "abs": "https://arxiv.org/abs/2506.20652", "authors": ["Roi Bar-On", "Dana Cohen-Bar", "Daniel Cohen-Or"], "title": "EditP23: 3D Editing via Propagation of Image Prompts to Multi-View", "categories": ["cs.GR", "cs.CV", "68U05 (Primary), 68T45 (Secondary)", "I.3.7; I.3.8; I.4.9"], "comment": "Code, supplementary videos, interactive 3D visualizations, and\n  additional results are available at https://editp23.github.io/", "summary": "We present EditP23, a method for mask-free 3D editing that propagates 2D\nimage edits to multi-view representations in a 3D-consistent manner. In\ncontrast to traditional approaches that rely on text-based prompting or\nexplicit spatial masks, EditP23 enables intuitive edits by conditioning on a\npair of images: an original view and its user-edited counterpart. These image\nprompts are used to guide an edit-aware flow in the latent space of a\npre-trained multi-view diffusion model, allowing the edit to be coherently\npropagated across views. Our method operates in a feed-forward manner, without\noptimization, and preserves the identity of the original object, in both\nstructure and appearance. We demonstrate its effectiveness across a range of\nobject categories and editing scenarios, achieving high fidelity to the source\nwhile requiring no manual masks."}
{"id": "2506.20370", "pdf": "https://arxiv.org/pdf/2506.20370", "abs": "https://arxiv.org/abs/2506.20370", "authors": ["Abdullah All Tanvir", "Xin Zhong"], "title": "InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "This paper introduces a novel deep learning framework for robust image\nzero-watermarking based on distortion-invariant feature learning. As a\nzero-watermarking scheme, our method leaves the original image unaltered and\nlearns a reference signature through optimization in the feature space. The\nproposed framework consists of two key modules. In the first module, a feature\nextractor is trained via noise-adversarial learning to generate representations\nthat are both invariant to distortions and semantically expressive. This is\nachieved by combining adversarial supervision against a distortion\ndiscriminator and a reconstruction constraint to retain image content. In the\nsecond module, we design a learning-based multibit zero-watermarking scheme\nwhere the trained invariant features are projected onto a set of trainable\nreference codes optimized to match a target binary message. Extensive\nexperiments on diverse image datasets and a wide range of distortions show that\nour method achieves state-of-the-art robustness in both feature stability and\nwatermark recovery. Comparative evaluations against existing self-supervised\nand deep watermarking techniques further highlight the superiority of our\nframework in generalization and robustness."}
{"id": "2506.20383", "pdf": "https://arxiv.org/pdf/2506.20383", "abs": "https://arxiv.org/abs/2506.20383", "authors": ["Isabell Egloff", "Raphael Hiesgen", "Maynard Koch", "Thomas C. Schmidt", "Matthias Wählisch"], "title": "A Detailed Measurement View on IPv6 Scanners and Their Adaption to BGP Signals", "categories": ["cs.NI"], "comment": null, "summary": "Scanners are daily visitors of public IPv4 hosts. Scanning IPv6 nodes\nsuccessfully is still a challenge, which an increasing crowd of actors tries to\nmaster. In this paper, we analyze current IPv6 scanning under various network\nconditions. We observe scanner behavior during eleven months in four network\ntelescopes, one of which is periodically reconfigured by changing BGP\nannouncements. We analyze and classify the observed scanners w.r.t. their\ntemporal behavior, their target, and network selection strategy, as well as\ntheir individual tools, fingerprints, and correlations across categories. We\nfind that silent subnets of larger prefixes remain invisible, whereas BGP\nprefix announcements quickly attract attention by scanners. Based on our\nfindings, we derive operational guidance on how to deploy network telescopes to\nincrease visibility of IPv6 scanners."}
{"id": "2506.20326", "pdf": "https://arxiv.org/pdf/2506.20326", "abs": "https://arxiv.org/abs/2506.20326", "authors": ["Sergio Torres Aguilar"], "title": "From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents", "categories": ["cs.CV", "cs.CL", "cs.DB"], "comment": null, "summary": "Robust Document Layout Analysis (DLA) is critical for the automated\nprocessing and understanding of historical documents with complex page\norganizations. This paper benchmarks five state-of-the-art object detection\narchitectures on three annotated datasets representing a spectrum of\ncodicological complexity: The e-NDP, a corpus of Parisian medieval registers\n(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval\nand modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated\nbooks of hours (ca.13th-16th centuries). We evaluate two Transformer-based\nmodels (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and\nYOLO-World). Our findings reveal significant performance variations dependent\non model architecture, data set characteristics, and bounding box\nrepresentation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results\n(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on\nthe more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB\nsignificantly outperforms all other models (0.564 and 0.568, respectively).\nThis study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)\nis not a minor refinement but a fundamental requirement for accurately modeling\nthe non-Cartesian nature of historical manuscripts. We conclude that a key\ntrade-off exists between the global context awareness of Transformers, ideal\nfor structured layouts, and the superior generalization of CNN-OBB models for\nvisually diverse and complex documents."}
{"id": "2506.20535", "pdf": "https://arxiv.org/pdf/2506.20535", "abs": "https://arxiv.org/abs/2506.20535", "authors": ["Hongzhen Huang", "Kunming Zhang", "Hanlong Liao", "Kui Wu", "Guoming Tang"], "title": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads", "categories": ["cs.DC", "cs.AI", "cs.LG"], "comment": "11 pages, 7 figures and 5 tables", "summary": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI."}
{"id": "2506.20558", "pdf": "https://arxiv.org/pdf/2506.20558", "abs": "https://arxiv.org/abs/2506.20558", "authors": ["Renyi Zhong", "Yintong Huo", "Wenwei Gu", "Jinxi Kuang", "Zhihan Jiang", "Guangba Yu", "Yichen Li", "David Lo", "Michael R. Lyu"], "title": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment Inconsistency", "categories": ["cs.SE"], "comment": "This manuscript is under review", "summary": "Comments within code serve as a crucial foundation for software\ndocumentation, facilitating developers to communicate and understand the code\neffectively. However, code-comment inconsistency (CCI) can negatively affect\nsoftware development, testing, and maintenance. Recent efforts to mitigate this\nissue have emerged, but existing studies often suffer from inaccurate datasets\nand inadequate solutions, weakening their practical effectiveness. In this\nstudy, we first conduct a quantitative analysis of existing datasets, revealing\na substantial portion of sampled data are mislabeled. To address these data\nlimitations, we introduce CCIBench, a refined dataset comprising high-quality\ndata, to support the training and evaluation of method-level CCI methods.\nFurthermore, we present an innovative end-to-end LLM-based framework,\nCCISolver, designed to improve code quality by identifying and rectifying CCIs.\nComprehensive evaluations demonstrate CCISolver's superior performance. For\ndetection, it establishes a new state-of-the-art with an F1-score of 89.54%. In\nfixing task, it achieves a remarkable 18.84% relative improvement in GLEU score\nover the strongest baseline. This superiority is confirmed by human evaluation,\nwhere CCISolver's fixing success rate of 0.6533 significantly surpasses\nexisting methods. Critically, in a practical end-to-end setting, CCISolver's\ninnovative architecture is approximately 36% faster for inference than the\nbaseline model, underscoring its scalability and real-world applicability."}
{"id": "2506.20156", "pdf": "https://arxiv.org/pdf/2506.20156", "abs": "https://arxiv.org/abs/2506.20156", "authors": ["Xuefei Hou", "Xizhao Tan"], "title": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype", "categories": ["cs.HC", "cs.AI", "cs.IR", "H.5.2; I.2.7; H.3.3"], "comment": "Version 1 of a work in progress. Finalized system flowcharts, a\n  public GitHub repository with the source code, and a full reproducibility\n  package detailing the prompts, models, and testing guidelines will be\n  provided in v2", "summary": "The core challenge in learning has shifted from knowledge acquisition to\neffective Self-Regulated Learning (SRL): planning, monitoring, and reflecting\non one's learning. Existing digital tools, however, inadequately support\nmetacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized\nreview, overlooking the role of context, while Personal Knowledge Management\n(PKM) tools require high manual maintenance.\n  To address these challenges, this paper introduces \"Insight Recall,\" a novel\nparadigm that conceptualizes the context-triggered retrieval of personal past\ninsights as a metacognitive scaffold to promote SRL. We formalize this paradigm\nusing the Just-in-Time Adaptive Intervention (JITAI) framework and implement a\nprototype system, Irec, to demonstrate its feasibility. At its core, Irec uses\na dynamic knowledge graph of the user's learning history. When a user faces a\nnew problem, a hybrid retrieval engine recalls relevant personal \"insights.\"\nSubsequently, a large language model (LLM) performs a deep similarity\nassessment to filter and present the most relevant scaffold in a just-in-time\nmanner. To reduce cognitive load, Irec features a human-in-the-loop pipeline\nfor LLM-based knowledge graph construction. We also propose an optional \"Guided\nInquiry\" module, where users can engage in a Socratic dialogue with an expert\nLLM, using the current problem and recalled insights as context. The\ncontribution of this paper is a solid theoretical framework and a usable system\nplatform for designing next-generation intelligent learning systems that\nenhance metacognition and self-regulation."}
{"id": "2506.20494", "pdf": "https://arxiv.org/pdf/2506.20494", "abs": "https://arxiv.org/abs/2506.20494", "authors": ["Qihang Jin", "Enze Ge", "Yuhang Xie", "Hongying Luo", "Junhao Song", "Ziqian Bi", "Chia Xin Liang", "Jibin Guan", "Joe Yeong", "Junfeng Hao"], "title": "Multimodal Representation Learning and Fusion", "categories": ["cs.LG", "cs.MM"], "comment": null, "summary": "Multi-modal learning is a fast growing area in artificial intelligence. It\ntries to help machines understand complex things by combining information from\ndifferent sources, like images, text, and audio. By using the strengths of each\nmodality, multi-modal learning allows AI systems to build stronger and richer\ninternal representations. These help machines better interpretation, reasoning,\nand making decisions in real-life situations. This field includes core\ntechniques such as representation learning (to get shared features from\ndifferent data types), alignment methods (to match information across\nmodalities), and fusion strategies (to combine them by deep learning models).\nAlthough there has been good progress, some major problems still remain. Like\ndealing with different data formats, missing or incomplete inputs, and\ndefending against adversarial attacks. Researchers now are exploring new\nmethods, such as unsupervised or semi-supervised learning, AutoML tools, to\nmake models more efficient and easier to scale. And also more attention on\ndesigning better evaluation metrics or building shared benchmarks, make it\neasier to compare model performance across tasks and domains. As the field\ncontinues to grow, multi-modal learning is expected to improve many areas:\ncomputer vision, natural language processing, speech recognition, and\nhealthcare. In the future, it may help to build AI systems that can understand\nthe world in a way more like humans, flexible, context aware, and able to deal\nwith real-world complexity."}
{"id": "2506.20420", "pdf": "https://arxiv.org/pdf/2506.20420", "abs": "https://arxiv.org/abs/2506.20420", "authors": ["Hafsa Akbar", "Danish Athar", "Muhammad Ayain Fida Rana", "Chaudhary Hammad Javed", "Zartash Afzal Uzmi", "Ihsan Ayyub Qazi", "Zafar Ayyub Qazi"], "title": "Semantic Caching for Improving Web Affordability", "categories": ["cs.NI", "F.2.2, I.2.7"], "comment": null, "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"}
{"id": "2506.20657", "pdf": "https://arxiv.org/pdf/2506.20657", "abs": "https://arxiv.org/abs/2506.20657", "authors": ["Dmitry Kondratyev", "Benedikt Riedel", "Yuan-Tang Chou", "Miles Cochran-Branson", "Noah Paladino", "David Schultz", "Mia Liu", "Javier Duarte", "Philip Harris", "Shih-Chieh Hsu"], "title": "SuperSONIC: Cloud-Native Infrastructure for ML Inferencing", "categories": ["cs.DC", "hep-ex", "physics.ins-det"], "comment": "Submission to PEARC25 Conference", "summary": "The increasing computational demand from growing data rates and complex\nmachine learning (ML) algorithms in large-scale scientific experiments has\ndriven the adoption of the Services for Optimized Network Inference on\nCoprocessors (SONIC) approach. SONIC accelerates ML inference by offloading it\nto local or remote coprocessors to optimize resource utilization. Leveraging\nits portability to different types of coprocessors, SONIC enhances data\nprocessing and model deployment efficiency for cutting-edge research in high\nenergy physics (HEP) and multi-messenger astrophysics (MMA). We developed the\nSuperSONIC project, a scalable server infrastructure for SONIC, enabling the\ndeployment of computationally intensive tasks to Kubernetes clusters equipped\nwith graphics processing units (GPUs). Using NVIDIA Triton Inference Server,\nSuperSONIC decouples client workflows from server infrastructure, standardizing\ncommunication, optimizing throughput, load balancing, and monitoring.\nSuperSONIC has been successfully deployed for the CMS and ATLAS experiments at\nthe CERN Large Hadron Collider (LHC), the IceCube Neutrino Observatory\n(IceCube), and the Laser Interferometer Gravitational-Wave Observatory (LIGO)\nand tested on Kubernetes clusters at Purdue University, the National Research\nPlatform (NRP), and the University of Chicago. SuperSONIC addresses the\nchallenges of the Cloud-native era by providing a reusable, configurable\nframework that enhances the efficiency of accelerator-based inference\ndeployment across diverse scientific domains and industries."}
{"id": "2506.20621", "pdf": "https://arxiv.org/pdf/2506.20621", "abs": "https://arxiv.org/abs/2506.20621", "authors": ["Silvio Alonso", "Antonio Pedro Santos Alves", "Lucas Romao", "Hélio Lopes", "Marcos Kalinowski"], "title": "Define-ML: An Approach to Ideate Machine Learning-Enabled Systems", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted for publication at the 51st Euromicro Conference Series on\n  Software Engineering and Advanced Applications (SEAA) 2025", "summary": "[Context] The increasing adoption of machine learning (ML) in software\nsystems demands specialized ideation approaches that address ML-specific\nchallenges, including data dependencies, technical feasibility, and alignment\nbetween business objectives and probabilistic system behavior. Traditional\nideation methods like Lean Inception lack structured support for these ML\nconsiderations, which can result in misaligned product visions and unrealistic\nexpectations. [Goal] This paper presents Define-ML, a framework that extends\nLean Inception with tailored activities - Data Source Mapping, Feature-to-Data\nSource Mapping, and ML Mapping - to systematically integrate data and technical\nconstraints into early-stage ML product ideation. [Method] We developed and\nvalidated Define-ML following the Technology Transfer Model, conducting both\nstatic validation (with a toy problem) and dynamic validation (in a real-world\nindustrial case study). The analysis combined quantitative surveys with\nqualitative feedback, assessing utility, ease of use, and intent of adoption.\n[Results] Participants found Define-ML effective for clarifying data concerns,\naligning ML capabilities with business goals, and fostering cross-functional\ncollaboration. The approach's structured activities reduced ideation ambiguity,\nthough some noted a learning curve for ML-specific components, which can be\nmitigated by expert facilitation. All participants expressed the intention to\nadopt Define-ML. [Conclusion] Define-ML provides an openly available, validated\napproach for ML product ideation, building on Lean Inception's agility while\naligning features with available data and increasing awareness of technical\nfeasibility."}
{"id": "2506.20207", "pdf": "https://arxiv.org/pdf/2506.20207", "abs": "https://arxiv.org/abs/2506.20207", "authors": ["Viktorija Paneva", "Verena Winterhalter", "Franziska Augustinowski", "Florian Alt"], "title": "User Understanding of Privacy Permissions in Mobile Augmented Reality: Perceptions and Misconceptions", "categories": ["cs.HC"], "comment": "14 pages, 3 figures. Preprint. Accepted to MobileHCI 2025", "summary": "Mobile Augmented Reality (AR) applications leverage various sensors to\nprovide immersive user experiences. However, their reliance on diverse data\nsources introduces significant privacy challenges. This paper investigates user\nperceptions and understanding of privacy permissions in mobile AR apps through\nan analysis of existing applications and an online survey of 120 participants.\nFindings reveal common misconceptions, including confusion about how\npermissions relate to specific AR functionalities (e.g., location and\nmeasurement of physical distances), and misinterpretations of permission labels\n(e.g., conflating camera and gallery access). We identify a set of actionable\nimplications for designing more usable and transparent privacy mechanisms\ntailored to mobile AR technologies, including contextual explanations, modular\npermission requests, and clearer permission labels. These findings offer\nactionable guidance for developers, researchers, and policymakers working to\nenhance privacy frameworks in mobile AR."}
{"id": "2506.20548", "pdf": "https://arxiv.org/pdf/2506.20548", "abs": "https://arxiv.org/abs/2506.20548", "authors": ["Manyi Li", "Renshuai Tao", "Yufan Liu", "Chuangchuang Tan", "Haotong Qin", "Bing Li", "Yunchao Wei", "Yao Zhao"], "title": "Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "20 pages, 10 figures", "summary": "With the rapid advancement of deep learning, particularly through generative\nadversarial networks (GANs) and diffusion models (DMs), AI-generated images, or\n``deepfakes\", have become nearly indistinguishable from real ones. These images\nare widely shared across Online Social Networks (OSNs), raising concerns about\ntheir misuse. Existing deepfake detection methods overlook the ``block effects\"\nintroduced by compression in OSNs, which obscure deepfake artifacts, and\nprimarily focus on raw images, rarely encountered in real-world scenarios. To\naddress these challenges, we propose PLADA (Pay Less Attention to Deceptive\nArtifacts), a novel framework designed to tackle the lack of paired data and\nthe ineffective use of compressed images. PLADA consists of two core modules:\nBlock Effect Eraser (B2E), which uses a dual-stage attention mechanism to\nhandle block effects, and Open Data Aggregation (ODA), which processes both\npaired and unpaired data to improve detection. Extensive experiments across 26\ndatasets demonstrate that PLADA achieves a remarkable balance in deepfake\ndetection, outperforming SoTA methods in detecting deepfakes on OSNs, even with\nlimited paired data and compression. More importantly, this work introduces the\n``block effect\" as a critical factor in deepfake detection, providing a robust\nsolution for open-world scenarios. Our code is available at\nhttps://github.com/ManyiLee/PLADA."}
{"id": "2506.19943", "pdf": "https://arxiv.org/pdf/2506.19943", "abs": "https://arxiv.org/abs/2506.19943", "authors": ["Juyoul Lee", "Sanzida Hoque", "Abdullah Aydeger", "Engin Zeydan"], "title": "Quantum-Resistant Domain Name System: A Comprehensive System-Level Study", "categories": ["cs.CR", "cs.NI", "cs.PF"], "comment": "Manuscript submitted to ACM, 29 pages, 8 Figures, 15 Tables", "summary": "The Domain Name System (DNS) plays a foundational role in Internet\ninfrastructure, yet its core protocols remain vulnerable to compromise by\nquantum adversaries. As cryptographically relevant quantum computers become a\nrealistic threat, ensuring DNS confidentiality, authenticity, and integrity in\nthe post-quantum era is imperative. In this paper, we present a comprehensive\nsystem-level study of post-quantum DNS security across three widely deployed\nmechanisms: DNSSEC, DNS-over-TLS (DoT), and DNS-over-HTTPS (DoH). We propose\nPost-Quantum Cryptographic (PQC)-DNS, a unified framework for benchmarking DNS\nsecurity under legacy, post-quantum, and hybrid cryptographic configurations.\nOur implementation leverages the Open Quantum Safe (OQS) libraries and\nintegrates lattice- and hash-based primitives into BIND9 and TLS 1.3 stacks. We\nformalize performance and threat models and analyze the impact of post-quantum\nkey encapsulation and digital signatures on end-to-end DNS resolution.\nExperimental results on a containerized testbed reveal that lattice-based\nprimitives such as Module-Lattice-Based Key-Encapsulation Mechanism (MLKEM) and\nFalcon offer practical latency and resource profiles, while hash-based schemes\nlike SPHINCS+ significantly increase message sizes and processing overhead. We\nalso examine security implications including downgrade risks, fragmentation\nvulnerabilities, and susceptibility to denial-of-service amplification. Our\nfindings inform practical guidance for deploying quantum-resilient DNS and\ncontribute to the broader effort of securing core Internet protocols for the\npost-quantum future."}
{"id": "2506.19892", "pdf": "https://arxiv.org/pdf/2506.19892", "abs": "https://arxiv.org/abs/2506.19892", "authors": ["Isaac Marroqui Penalva", "Enrique Tomás Martínez Beltrán", "Manuel Gil Pérez", "Alberto Huertas Celdrán"], "title": "RepuNet: A Reputation System for Mitigating Malicious Clients in DFL", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.LG", "cs.PF"], "comment": null, "summary": "Decentralized Federated Learning (DFL) enables nodes to collaboratively train\nmodels without a central server, introducing new vulnerabilities since each\nnode independently selects peers for model aggregation. Malicious nodes may\nexploit this autonomy by sending corrupted models (model poisoning), delaying\nmodel submissions (delay attack), or flooding the network with excessive\nmessages, negatively affecting system performance. Existing solutions often\ndepend on rigid configurations or additional infrastructures such as\nblockchain, leading to computational overhead, scalability issues, or limited\nadaptability. To overcome these limitations, this paper proposes RepuNet, a\ndecentralized reputation system that categorizes threats in DFL and dynamically\nevaluates node behavior using metrics like model similarity, parameter changes,\nmessage latency, and communication volume. Nodes' influence in model\naggregation is adjusted based on their reputation scores. RepuNet was\nintegrated into the Nebula DFL platform and experimentally evaluated with MNIST\nand CIFAR-10 datasets under non-IID distributions, using federations of up to\n25 nodes in both fully connected and random topologies. Different attack\nintensities, frequencies, and activation intervals were tested. Results\ndemonstrated that RepuNet effectively detects and mitigates malicious behavior,\nachieving F1 scores above 95% for MNIST scenarios and approximately 76% for\nCIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,\nand practical potential for mitigating threats in decentralized federated\nlearning environments."}
{"id": "2506.19884", "pdf": "https://arxiv.org/pdf/2506.19884", "abs": "https://arxiv.org/abs/2506.19884", "authors": ["Zhengxiang Huang", "Chaoyue Niu", "Zhaode Wang", "Jiarui Xue", "Hanming Zhang", "Yugang Wang", "Zewei Xin", "Xiaotang Jiang", "Chengfei Lv", "Fan Wu", "Guihai Chen"], "title": "MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection", "categories": ["cs.OS", "cs.AI", "cs.PF", "cs.SE"], "comment": null, "summary": "As the demand for on-device Large Language Model (LLM) inference grows,\nenergy efficiency has become a major concern, especially for battery-limited\nmobile devices. Our analysis shows that the memory-bound LLM decode phase\ndominates energy use, and yet most existing works focus on accelerating the\nprefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric\nCore Selection (AECS) and integrate it into MNN to create the energy-efficient\nversion, MNN-AECS, the first engine-level system solution without requiring\nroot access or OS modifications for energy-efficient LLM decoding. MNN-AECS is\ndesigned to reduce LLM decoding energy while keeping decode speed within an\nacceptable slowdown threshold by dynamically selecting low-power CPU cores.\nMNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of\nvarious sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%\nwithout slowdown averaged over all 7 devices and 4 datasets. Against other\nengines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS\ndelivers 39% to 78% energy saving and 12% to 363% speedup on average."}
{"id": "2506.20291", "pdf": "https://arxiv.org/pdf/2506.20291", "abs": "https://arxiv.org/abs/2506.20291", "authors": ["Haoran Zhang", "Xin Zhao", "Jinze Chen", "Junpeng Guo"], "title": "A Literature Review on Simulation in Conversational Recommender Systems", "categories": ["cs.HC", "cs.IR"], "comment": "6 pages, 1 figures, accepted as a poster for CSWIM 2025", "summary": "Conversational Recommender Systems (CRSs) have garnered attention as a novel\napproach to delivering personalized recommendations through multi-turn\ndialogues. This review developed a taxonomy framework to systematically\ncategorize relevant publications into four groups: dataset construction,\nalgorithm design, system evaluation, and empirical studies, providing a\ncomprehensive analysis of simulation methods in CRSs research. Our analysis\nreveals that simulation methods play a key role in tackling CRSs' main\nchallenges. For example, LLM-based simulation methods have been used to create\nconversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.\nDespite several challenges, such as dataset bias, the limited output\nflexibility of LLM-based simulations, and the gap between text semantic space\nand behavioral semantics, persist due to the complexity in Human-Computer\nInteraction (HCI) of CRSs, simulation methods hold significant potential for\nadvancing CRS research. This review offers a thorough summary of the current\nresearch landscape in this domain and identifies promising directions for\nfuture inquiry."}
{"id": "2506.20609", "pdf": "https://arxiv.org/pdf/2506.20609", "abs": "https://arxiv.org/abs/2506.20609", "authors": ["Ankit Shah", "Rita Singh", "Bhiksha Raj", "Alexander Hauptmann"], "title": "Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS"], "comment": "4 pages + 1 References", "summary": "The escalating rates of gun-related violence and mass shootings represent a\nsignificant threat to public safety. Timely and accurate information for law\nenforcement agencies is crucial in mitigating these incidents. Current\ncommercial gunshot detection systems, while effective, often come with\nprohibitive costs. This research explores a cost-effective alternative by\nleveraging acoustic analysis of gunshot recordings, potentially obtainable from\nubiquitous devices like cell phones, to not only detect gunshots but also\nclassify the type of firearm used. This paper details a study on deciphering\ngun type hierarchies using a curated dataset of 3459 recordings. We investigate\nthe fundamental acoustic characteristics of gunshots, including muzzle blasts\nand shockwaves, which vary based on firearm type, ammunition, and shooting\ndirection. We propose and evaluate machine learning frameworks, including\nSupport Vector Machines (SVMs) as a baseline and a more advanced Convolutional\nNeural Network (CNN) architecture for joint gunshot detection and gun type\nclassification. Results indicate that our deep learning approach achieves a\nmean average precision (mAP) of 0.58 on clean labeled data, outperforming the\nSVM baseline (mAP 0.39). Challenges related to data quality, environmental\nnoise, and the generalization capabilities when using noisy web-sourced data\n(mAP 0.35) are also discussed. The long-term vision is to develop a highly\naccurate, real-time system deployable on common recording devices,\nsignificantly reducing detection costs and providing critical intelligence to\nfirst responders."}
{"id": "2506.20104", "pdf": "https://arxiv.org/pdf/2506.20104", "abs": "https://arxiv.org/abs/2506.20104", "authors": ["Malikussaid", "Sutiyo"], "title": "The Impact of the Russia-Ukraine Conflict on the Cloud Computing Risk Landscape", "categories": ["cs.CY", "cs.CR", "cs.NI"], "comment": "16 pages, 4 tables, to be published in ICoSEIT Conference 2025,\n  unabridged version", "summary": "The Russian invasion of Ukraine has fundamentally altered the information\ntechnology (IT) risk landscape, particularly in cloud computing environments.\nThis paper examines how this geopolitical conflict has accelerated data\nsovereignty concerns, transformed cybersecurity paradigms, and reshaped cloud\ninfrastructure strategies worldwide. Through an analysis of documented cyber\noperations, regulatory responses, and organizational adaptations between 2022\nand early 2025, this research demonstrates how the conflict has served as a\ncatalyst for a broader reassessment of IT risk. The research reveals that while\ntraditional IT risk frameworks offer foundational guidance, their standard\napplication may inadequately address the nuances of state-sponsored threats,\nconflicting data governance regimes, and the weaponization of digital\ndependencies without specific geopolitical augmentation. The contribution of\nthis paper lies in its focused synthesis and strategic adaptation of existing\nbest practices into a multi-layered approach. This approach uniquely synergizes\nresilient cloud architectures (including sovereign and hybrid models), enhanced\ndata-centric security strategies (such as advanced encryption and\nprivacy-enhancing technologies), and geopolitically-informed governance to\nbuild digital resilience. The interplay between these layers, emphasizing how\ngeopolitical insights directly shape architectural and security choices beyond\nstandard best practices-particularly by integrating the human element,\nincluding personnel vulnerabilities and expertise, as a core consideration in\ntechnical design and operational management-offers a more robust defense\nagainst the specific, multifaceted risks arising from geopolitical conflict in\nincreasingly fractured digital territories."}
{"id": "2506.20000", "pdf": "https://arxiv.org/pdf/2506.20000", "abs": "https://arxiv.org/abs/2506.20000", "authors": ["Narasimha Raghavan Veeraragavan", "Jan Franz Nygård"], "title": "Can One Safety Loop Guard Them All? Agentic Guard Rails for Federated Computing", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "Accepted at ICML 2025 Workshop on Collaborative and Federated Agentic\n  Workflows (CFAgentic@ICML'25)", "summary": "We propose Guardian-FC, a novel two-layer framework for privacy preserving\nfederated computing that unifies safety enforcement across diverse privacy\npreserving mechanisms, including cryptographic back-ends like fully homomorphic\nencryption (FHE) and multiparty computation (MPC), as well as statistical\ntechniques such as differential privacy (DP). Guardian-FC decouples guard-rails\nfrom privacy mechanisms by executing plug-ins (modular computation units),\nwritten in a backend-neutral, domain-specific language (DSL) designed\nspecifically for federated computing workflows and interchangeable Execution\nProviders (EPs), which implement DSL operations for various privacy back-ends.\nAn Agentic-AI control plane enforces a finite-state safety loop through signed\ntelemetry and commands, ensuring consistent risk management and auditability.\nThe manifest-centric design supports fail-fast job admission and seamless\nextensibility to new privacy back-ends. We present qualitative scenarios\nillustrating backend-agnostic safety and a formal model foundation for\nverification. Finally, we outline a research agenda inviting the community to\nadvance adaptive guard-rail tuning, multi-backend composition, DSL\nspecification development, implementation, and compiler extensibility alongside\nhuman-override usability."}
{"id": "2506.20495", "pdf": "https://arxiv.org/pdf/2506.20495", "abs": "https://arxiv.org/abs/2506.20495", "authors": ["Haoze Wu", "Yunzhi Yao", "Wenhao Yu", "Huajun Chen", "Ningyu Zhang"], "title": "ReCode: Updating Code API Knowledge with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "cs.SE"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) exhibit remarkable code generation capabilities\nbut falter when adapting to frequent updates in external library APIs. This\ncritical limitation, stemming from reliance on outdated API knowledge from\ntheir training data, even with access to current documentation, impedes\nreliable code generation in dynamic environments. To tackle this issue, we\npropose ReCode (rule-based Reinforcement learning for Code Update), a novel\nframework that mimics human programmer adaptation to API changes. Specifically,\nwe construct a dataset of approximately 2,000 data entries to train the LLMs to\nperform version migration based on updated information. Then, we introduce a\nmodified string similarity metric for code evaluation as the reward for\nreinforcement learning. Our experiments demonstrate that ReCode substantially\nboosts LLMs' code generation performance in dynamic API scenarios, especially\non the unseen CodeUpdateArena task. Crucially, compared to supervised\nfine-tuning, ReCode has less impact on LLMs' general code generation abilities.\nWe apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and\nDAPO), all achieving consistent improvements. Notably, after training,\nQwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned\nmodel and the reasoning model with the same architecture. Code is available at\nhttps://github.com/zjunlp/ReCode."}
{"id": "2506.20377", "pdf": "https://arxiv.org/pdf/2506.20377", "abs": "https://arxiv.org/abs/2506.20377", "authors": ["Sachin R. Pendse", "Ben Rochford", "Neha Kumar", "Munmun De Choudhury"], "title": "The Role of Partisan Culture in Mental Health Language Online", "categories": ["cs.HC", "cs.CY", "cs.SI"], "comment": "Accepted to the ACM Conference on Computer-Supported Cooperative Work\n  and Social Computing (CSCW 2025)", "summary": "The impact of culture on how people express distress in online support\ncommunities is increasingly a topic of interest within Computer Supported\nCooperative Work (CSCW) and Human-Computer Interaction (HCI). In the United\nStates, distinct cultures have emerged from each of the two dominant political\nparties, forming a primary lens by which people navigate online and offline\nworlds. We examine whether partisan culture may play a role in how U.S.\nRepublican and Democrat users of online mental health support communities\nexpress distress. We present a large-scale observational study of 2,184,356\nposts from 8,916 statistically matched Republican, Democrat, and unaffiliated\nonline support community members. We utilize methods from causal inference to\nstatistically match partisan users along covariates that correspond with\ndemographic attributes and platform use, in order to create comparable cohorts\nfor analysis. We then leverage methods from natural language processing to\nunderstand how partisan expressions of distress compare between these sets of\nclosely matched opposing partisans, and between closely matched partisans and\ntypical support community members. Our data spans January 2013 to December\n2022, a period of both rising political polarization and mental health\nconcerns. We find that partisan culture does play into expressions of distress,\nunderscoring the importance of considering partisan cultural differences in the\ndesign of online support community platforms."}
{"id": "2506.20488", "pdf": "https://arxiv.org/pdf/2506.20488", "abs": "https://arxiv.org/abs/2506.20488", "authors": ["Shuo Yang", "Xinran Zheng", "Jinfeng Xu", "Jinze Li", "Danyang Song", "Zheyu Chen", "Edith C. H. Ngai"], "title": "Generative AI for Vulnerability Detection in 6G Wireless Networks: Advances, Case Study, and Future Directions", "categories": ["cs.CR", "cs.NI"], "comment": null, "summary": "The rapid advancement of 6G wireless networks, IoT, and edge computing has\nsignificantly expanded the cyberattack surface, necessitating more intelligent\nand adaptive vulnerability detection mechanisms. Traditional security methods,\nwhile foundational, struggle with zero-day exploits, adversarial threats, and\ncontext-dependent vulnerabilities in highly dynamic network environments.\nGenerative AI (GAI) emerges as a transformative solution, leveraging synthetic\ndata generation, multimodal reasoning, and adaptive learning to enhance\nsecurity frameworks. This paper explores the integration of GAI-powered\nvulnerability detection in 6G wireless networks, focusing on code auditing,\nprotocol security, cloud-edge defenses, and hardware protection. We introduce a\nthree-layer framework comprising the Technology Layer, Capability Layer, and\nApplication Layer to systematically analyze the role of VAEs, GANs, LLMs, and\nGDMs in securing next-generation wireless ecosystems. To demonstrate practical\nimplementation, we present a case study on LLM-driven code vulnerability\ndetection, highlighting its effectiveness, performance, and challenges.\nFinally, we outline future research directions, including lightweight models,\nhigh-authenticity data generation, external knowledge integration, and\nprivacy-preserving technologies. By synthesizing current advancements and open\nchallenges, this work provides a roadmap for researchers and practitioners to\nharness GAI for building resilient and adaptive security solutions in 6G\nnetworks."}
{"id": "2506.20511", "pdf": "https://arxiv.org/pdf/2506.20511", "abs": "https://arxiv.org/abs/2506.20511", "authors": ["Arno Geimer", "Karthick Panner Selvam", "Beltran Fiz Pontiveros"], "title": "Collaborative Batch Size Optimization for Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "Federated Learning (FL) is a decentralized collaborative Machine Learning\nframework for training models without collecting data in a centralized\nlocation. It has seen application across various disciplines, from helping\nmedical diagnoses in hospitals to detecting fraud in financial transactions. In\nthis paper, we focus on improving the local training process through hardware\nusage optimization. While participants in a federation might share the hardware\nthey are training on, since there is no information exchange between them,\ntheir training process can be hindered by an improper training configuration.\nTaking advantage of the parallel processing inherent to Federated Learning, we\nuse a greedy randomized search to optimize local batch sizes for the best\ntraining settings across all participants. Our results show that against\ndefault parameter settings, our method improves convergence speed while staying\nnearly on par with the case where local parameters are optimized."}
{"id": "2506.20463", "pdf": "https://arxiv.org/pdf/2506.20463", "abs": "https://arxiv.org/abs/2506.20463", "authors": ["Bei Yi Ng", "Jiarui Li", "Xinyuan Tong", "Kevin Ye", "Gauthami Yenne", "Varun Chandrasekaran", "Jingjie Li"], "title": "Analyzing Security and Privacy Challenges in Generative AI Usage Guidelines for Higher Education", "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Educators and learners worldwide are embracing the rise of Generative\nArtificial Intelligence (GenAI) as it reshapes higher education. However, GenAI\nalso raises significant privacy and security concerns, as models and\nprivacy-sensitive user data, such as student records, may be misused by service\nproviders. Unfortunately, end-users often have little awareness of or control\nover how these models operate. To address these concerns, universities are\ndeveloping institutional policies to guide GenAI use while safeguarding\nsecurity and privacy. This work examines these emerging policies and\nguidelines, with a particular focus on the often-overlooked privacy and\nsecurity dimensions of GenAI integration in higher education, alongside other\nacademic values. Through a qualitative analysis of GenAI usage guidelines from\nuniversities across 12 countries, we identify key challenges and opportunities\ninstitutions face in providing effective privacy and security protections,\nincluding the need for GenAI safeguards tailored specifically to the academic\ncontext."}
{"id": "2506.20651", "pdf": "https://arxiv.org/pdf/2506.20651", "abs": "https://arxiv.org/abs/2506.20651", "authors": ["Fei Wang", "Baochun Li"], "title": "Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning", "categories": ["cs.LG", "cs.CR", "cs.DC"], "comment": null, "summary": "Recent work has shown that gradient updates in federated learning (FL) can\nunintentionally reveal sensitive information about a client's local data. This\nrisk becomes significantly greater when a malicious server manipulates the\nglobal model to provoke information-rich updates from clients. In this paper,\nwe adopt a defender's perspective to provide the first comprehensive analysis\nof malicious gradient leakage attacks and the model manipulation techniques\nthat enable them. Our investigation reveals a core trade-off: these attacks\ncannot be both highly effective in reconstructing private data and sufficiently\nstealthy to evade detection -- especially in realistic FL settings that\nincorporate common normalization techniques and federated averaging.\n  Building on this insight, we argue that malicious gradient leakage attacks,\nwhile theoretically concerning, are inherently limited in practice and often\ndetectable through basic monitoring. As a complementary contribution, we\npropose a simple, lightweight, and broadly applicable client-side detection\nmechanism that flags suspicious model updates before local training begins,\ndespite the fact that such detection may not be strictly necessary in realistic\nFL settings. This mechanism further underscores the feasibility of defending\nagainst these attacks with minimal overhead, offering a deployable safeguard\nfor privacy-conscious federated learning systems."}
{"id": "2506.20595", "pdf": "https://arxiv.org/pdf/2506.20595", "abs": "https://arxiv.org/abs/2506.20595", "authors": ["Momin N. Siddiqui", "Roy Pea", "Hari Subramonyam"], "title": "AI in the Writing Process: How Purposeful AI Support Fosters Student Writing", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The ubiquity of technologies like ChatGPT has raised concerns about their\nimpact on student writing, particularly regarding reduced learner agency and\nsuperficial engagement with content. While standalone chat-based LLMs often\nproduce suboptimal writing outcomes, evidence suggests that purposefully\ndesigned AI writing support tools can enhance the writing process. This paper\ninvestigates how different AI support approaches affect writers' sense of\nagency and depth of knowledge transformation. Through a randomized control\ntrial with 90 undergraduate students, we compare three conditions: (1) a\nchat-based LLM writing assistant, (2) an integrated AI writing tool to support\ndiverse subprocesses, and (3) a standard writing interface (control). Our\nfindings demonstrate that, among AI-supported conditions, students using the\nintegrated AI writing tool exhibited greater agency over their writing process\nand engaged in deeper knowledge transformation overall. These results suggest\nthat thoughtfully designed AI writing support targeting specific aspects of the\nwriting process can help students maintain ownership of their work while\nfacilitating improved engagement with content."}
{"id": "2506.19899", "pdf": "https://arxiv.org/pdf/2506.19899", "abs": "https://arxiv.org/abs/2506.19899", "authors": ["Andrew T. Rozema", "James C. Davis"], "title": "Anti-Phishing Training Does Not Work: A Large-Scale Empirical Assessment of Multi-Modal Training Grounded in the NIST Phish Scale", "categories": ["cs.CR", "cs.HC"], "comment": "13 pages, 5 apdx", "summary": "Social engineering attacks using email, commonly known as phishing, are a\ncritical cybersecurity threat. Phishing attacks often lead to operational\nincidents and data breaches. As a result, many organizations allocate a\nsubstantial portion of their cybersecurity budgets to phishing awareness\ntraining, driven in part by compliance requirements. However, the effectiveness\nof this training remains in dispute. Empirical evidence of training\n(in)effectiveness is essential for evidence-based cybersecurity investment and\npolicy development. Despite recent measurement studies, two critical gaps\nremain in the literature:\n  (1) we lack a validated measure of phishing lure difficulty, and\n  (2) there are few comparisons of different types of training in real-world\nbusiness settings.\n  To fill these gaps, we conducted a large-scale study ($N = 12{,}511$) of\nphishing effectiveness at a US-based financial technology (``fintech'') firm.\nOur two-factor design compared the effect of treatments (lecture-based,\ninteractive, and control groups) on subjects' susceptibility to phishing lures\nof varying complexity (using the NIST Phish Scale). The NIST Phish Scale\nsuccessfully predicted behavior (click rates: 7.0\\% easy to 15.0\\% hard emails,\np $<$ 0.001), but training showed no significant main effects on clicks (p =\n0.450) or reporting (p = 0.417). Effect sizes remained below 0.01, indicating\nlittle practical value in any of the phishing trainings we deployed. Our\nresults add to the growing evidence that phishing training is ineffective,\nreinforcing the importance of phishing defense-in-depth and the merit of\nchanges to processes and technology to reduce reliance on humans, as well as\nrebuking the training costs necessitated by regulatory requirements."}
{"id": "2506.20212", "pdf": "https://arxiv.org/pdf/2506.20212", "abs": "https://arxiv.org/abs/2506.20212", "authors": ["Andrea Bussolan", "Oliver Avram", "Andrea Pignata", "Gianvito Urgese", "Stefano Baraldo", "Anna Valente"], "title": "Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "With the advent of Industry 5.0, manufacturers are increasingly prioritizing\nworker well-being alongside mass customization. Stress-aware Human-Robot\nCollaboration (HRC) plays a crucial role in this paradigm, where robots must\nadapt their behavior to human mental states to improve collaboration fluency\nand safety. This paper presents a novel framework that integrates Federated\nLearning (FL) to enable personalized mental state evaluation while preserving\nuser privacy. By leveraging physiological signals, including EEG, ECG, EDA,\nEMG, and respiration, a multimodal model predicts an operator's stress level,\nfacilitating real-time robot adaptation. The FL-based approach allows\ndistributed on-device training, ensuring data confidentiality while improving\nmodel generalization and individual customization. Results demonstrate that the\ndeployment of an FL approach results in a global model with performance in\nstress prediction accuracy comparable to a centralized training approach.\nMoreover, FL allows for enhancing personalization, thereby optimizing\nhuman-robot interaction in industrial settings, while preserving data privacy.\nThe proposed framework advances privacy-preserving, adaptive robotics to\nenhance workforce well-being in smart manufacturing."}
{"id": "2506.20268", "pdf": "https://arxiv.org/pdf/2506.20268", "abs": "https://arxiv.org/abs/2506.20268", "authors": ["Ruben Janssens", "Jens De Bock", "Sofie Labat", "Eva Verhelst", "Veronique Hoste", "Tony Belpaeme"], "title": "Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue", "categories": ["cs.RO", "cs.CL", "cs.HC"], "comment": "Accepted at the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN 2025)", "summary": "Detecting miscommunication in human-robot interaction is a critical function\nfor maintaining user engagement and trust. While humans effortlessly detect\ncommunication errors in conversations through both verbal and non-verbal cues,\nrobots face significant challenges in interpreting non-verbal feedback, despite\nadvances in computer vision for recognizing affective expressions. This\nresearch evaluates the effectiveness of machine learning models in detecting\nmiscommunications in robot dialogue. Using a multi-modal dataset of 240\nhuman-robot conversations, where four distinct types of conversational failures\nwere systematically introduced, we assess the performance of state-of-the-art\ncomputer vision models. After each conversational turn, users provided feedback\non whether they perceived an error, enabling an analysis of the models' ability\nto accurately detect robot mistakes. Despite using state-of-the-art models, the\nperformance barely exceeds random chance in identifying miscommunication, while\non a dataset with more expressive emotional content, they successfully\nidentified confused states. To explore the underlying cause, we asked human\nraters to do the same. They could also only identify around half of the induced\nmiscommunications, similarly to our model. These results uncover a fundamental\nlimitation in identifying robot miscommunications in dialogue: even when users\nperceive the induced miscommunication as such, they often do not communicate\nthis to their robotic conversation partner. This knowledge can shape\nexpectations of the performance of computer vision models and can help\nresearchers to design better human-robot conversations by deliberately\neliciting feedback where needed."}
{"id": "2506.20400", "pdf": "https://arxiv.org/pdf/2506.20400", "abs": "https://arxiv.org/abs/2506.20400", "authors": ["Kristoffer Christensen", "Bo Nørregaard Jørgensen", "Zheng Grace Ma"], "title": "A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem", "categories": ["cs.MA", "cs.CE", "cs.HC", "cs.SY", "eess.SY"], "comment": null, "summary": "Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging\necosystems generate large, complex, and stochastic time-series datasets that\ncapture interactions between households, grid infrastructure, and energy\nmarkets. These interactions can lead to unexpected system-level events, such as\ntransformer overloads or consumer dissatisfaction, that are difficult to detect\nand explain through static post-processing. This paper presents a modular,\nPython-based dashboard framework, built using Dash by Plotly, that enables\nefficient, multi-level exploration and root-cause analysis of emergent behavior\nin MABS outputs. The system features three coordinated views (System Overview,\nSystem Analysis, and Consumer Analysis), each offering high-resolution\nvisualizations such as time-series plots, spatial heatmaps, and agent-specific\ndrill-down tools. A case study simulating full EV adoption with smart charging\nin a Danish residential network demonstrates how the dashboard supports rapid\nidentification and contextual explanation of anomalies, including clustered\ntransformer overloads and time-dependent charging failures. The framework\nfacilitates actionable insight generation for researchers and distribution\nsystem operators, and its architecture is adaptable to other distributed energy\nresources and complex energy systems."}
{"id": "2506.20664", "pdf": "https://arxiv.org/pdf/2506.20664", "abs": "https://arxiv.org/abs/2506.20664", "authors": ["Andrei Lupu", "Timon Willi", "Jakob Foerster"], "title": "The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MA"], "comment": "41 pages, 19 figures", "summary": "As Large Language Models (LLMs) gain agentic abilities, they will have to\nnavigate complex multi-agent scenarios, interacting with human users and other\nagents in cooperative and competitive settings. This will require new reasoning\nskills, chief amongst them being theory of mind (ToM), or the ability to reason\nabout the \"mental\" states of other agents. However, ToM and other multi-agent\nabilities in LLMs are poorly understood, since existing benchmarks suffer from\nnarrow scope, data leakage, saturation, and lack of interactivity. We thus\npropose Decrypto, a game-based benchmark for multi-agent reasoning and ToM\ndrawing inspiration from cognitive science, computational pragmatics and\nmulti-agent reinforcement learning. It is designed to be as easy as possible in\nall other dimensions, eliminating confounding factors commonly found in other\nbenchmarks. To our knowledge, it is also the first platform for designing\ninteractive ToM experiments.\n  We validate the benchmark design through comprehensive empirical evaluations\nof frontier LLMs, robustness studies, and human-AI cross-play experiments. We\nfind that LLM game-playing abilities lag behind humans and simple\nword-embedding baselines. We then create variants of two classic cognitive\nscience experiments within Decrypto to evaluate three key ToM abilities.\nSurprisingly, we find that state-of-the-art reasoning models are significantly\nworse at those tasks than their older counterparts. This demonstrates that\nDecrypto addresses a crucial gap in current reasoning and ToM evaluations, and\npaves the path towards better artificial agents."}
