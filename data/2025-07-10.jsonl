{"id": "2507.06343", "pdf": "https://arxiv.org/pdf/2507.06343", "abs": "https://arxiv.org/abs/2507.06343", "authors": ["Huynh Khanh Vi Tran", "Nauman bin Ali", "Michael Unterkalmsteiner", "Jürgen Börstler", "Panagiota Chatzipetrou"], "title": "Quality attributes of test cases and test suites -- importance & challenges from practitioners' perspectives", "categories": ["cs.SE"], "comment": null, "summary": "Context: The quality of the test suites and the constituent test cases\nsignificantly impacts confidence in software testing. While research has\nidentified several quality attributes of test cases and test suites, there is a\nneed for a better understanding of their relative importance in practice.\nObjective: We investigate practitioners' perceptions regarding the relative\nimportance of quality attributes of test cases and test suites and the\nchallenges they face in ensuring the perceived important quality attributes.\nMethod: We conducted an industrial survey using a questionnaire based on the\nquality attributes identified in an extensive literature review. We used a\nsampling strategy that leverages LinkedIn to draw a large and heterogeneous\nsample of professionals with experience in software testing. Results: We\ncollected 354 responses from practitioners with a wide range of experience. We\nfound that the majority of practitioners rated Fault Detection, Usability,\nMaintainability, Reliability, and Coverage to be the most important quality\nattributes. Resource Efficiency, Reusability, and Simplicity received the most\ndivergent opinions, which, according to our analysis, depend on the\nsoftware-testing contexts. We identified common challenges that apply to the\nimportant attributes, namely inadequate definition, lack of useful metrics,\nlack of an established review process, and lack of external support.\nConclusion: The findings point out where practitioners actually need further\nsupport with respect to achieving high-quality test cases and test suites under\ndifferent software testing contexts. The findings can serve as a guideline for\nacademic researchers when looking for research directions on the topic. The\nfindings can also be used to encourage companies to provide more support to\npractitioners to achieve high-quality test cases and test suites."}
{"id": "2507.06354", "pdf": "https://arxiv.org/pdf/2507.06354", "abs": "https://arxiv.org/abs/2507.06354", "authors": ["Huynh Khanh Vi Tran", "Nauman bin Ali", "Michael Unterkalmsteiner", "Jürgen Börstler"], "title": "A proposal and assessment of an improved heuristic for the Eager Test smell detection", "categories": ["cs.SE"], "comment": null, "summary": "Context: The evidence for the prevalence of test smells at the unit testing\nlevel has relied on the accuracy of detection tools, which have seen intense\nresearch in the last two decades. The Eager Test smell, one of the most\nprevalent, is often identified using simplified detection rules that\npractitioners find inadequate. Objective: We aim to improve the rules for\ndetecting the Eager Test smell. Method: We reviewed the literature on test\nsmells to analyze the definitions and detection rules of the Eager Test smell.\nWe proposed a novel, unambiguous definition of the test smell and a heuristic\nto address the limitations of the existing rules. We evaluated our heuristic\nagainst existing detection rules by manually applying it to 300 unit test cases\nin Java. Results: Our review identified 56 relevant studies. We found that\ninadequate interpretations of original definitions of the Eager Test smell led\nto imprecise detection rules, resulting in a high level of disagreement in\ndetection outcomes. Also, our heuristic detected patterns of eager and\nnon-eager tests that existing rules missed. Conclusion: Our heuristic captures\nthe essence of the Eager Test smell more precisely; hence, it may address\npractitioners' concerns regarding the adequacy of existing detection rules."}
{"id": "2507.06463", "pdf": "https://arxiv.org/pdf/2507.06463", "abs": "https://arxiv.org/abs/2507.06463", "authors": ["Atieh Barati Nia", "Mohammad Dindoost", "David A. Bader"], "title": "Evaluating Efficiency and Novelty of LLM-Generated Code for Graph Analysis", "categories": ["cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to automate software\ndevelopment, yet most prior evaluations focus on functional correctness or\nhigh-level languages such as Python. We present the first systematic study of\nLLMs' ability to generate efficient C implementations of graph-analysis\nroutines--code that must satisfy the stringent runtime and memory constraints.\nEight state-of-the-art models (OpenAI ChatGPT o3 and o4-mini-high, Anthropic\nClaude 4 Sonnet and Sonnet Extended, Google Gemini 2.5 Flash and Pro, xAI Grok\n3-Think, and DeepSeek DeepThink R1) are benchmarked by two distinct approaches.\nThe first approach checks the ability of LLMs in generating an algorithm\noutperforming other present algorithms in the benchmark. The second approach\nevaluates the ability of LLMs to generate graph algorithms for integration into\nthe benchmark. Results show that Claude Sonnet 4 Extended achieves the best\nresult in the case of ready-to-use code generation and efficiency,\noutperforming human-written baselines in triangle counting. The study confirms\nthat contemporary LLMs excel at optimizing and integrating established\nalgorithms but not inventing novel techniques. We provide prompts, the first\napproach's generated code, and measurement scripts to foster reproducible\nresearch."}
{"id": "2507.06704", "pdf": "https://arxiv.org/pdf/2507.06704", "abs": "https://arxiv.org/abs/2507.06704", "authors": ["Lloyd Montgomery"], "title": "Issue Tracking Ecosystems: Context and Best Practices", "categories": ["cs.SE"], "comment": "300 pages, Dissertation for the doctoral degree Dr. rer. nat. at the\n  Faculty of Mathematics, Informatics, and Natural Sciences, Department of\n  Informatics, University of Hamburg, Hamburg, Germany", "summary": "Issue Tracking Systems (ITSs), such as GitHub and Jira, are popular tools\nthat support Software Engineering (SE) organisations through the management of\n``issues'', which represent different SE artefacts such as requirements,\ndevelopment tasks, and maintenance items. ITSs also support internal linking\nbetween issues, and external linking to other tools and information sources.\nThis provides SE organisations key forms of documentation, including forwards\nand backwards traceability (e.g., Feature Requests linked to sprint releases\nand code commits linked to Bug Reports). An Issue Tracking Ecosystem (ITE) is\nthe aggregate of the central ITS and the related SE artefacts, stakeholders,\nand processes -- with an emphasis on how these contextual factors interact with\nthe ITS. The quality of ITEs is central to the success of these organisations\nand their software products. There are challenges, however, within ITEs,\nincluding complex networks of interlinked artefacts and diverse workflows.\nWhile ITSs have been the subject of study in SE research for decades, ITEs as a\nwhole need further exploration.\n  In this thesis, I undertake the challenge of understanding ITEs at a broader\nlevel, addressing these questions regarding complexity and diversity. I\ninterviewed practitioners and performed archival analysis on a diverse set of\nITSs. These analyses revealed the context-dependent nature of ITE problems,\nhighlighting the need for context-specific ITE research. While previous work\nhas produced many solutions to specific ITS problems, these solutions are not\nconsistently framed in a context-rich and comparable way, leading to a desire\nfor more aligned solutions across research and practice. To address this\nemergent information and lack of alignment, I created the Best Practice\nOntology for ITEs. <... truncated due to arXiv abstract character limit ...>"}
{"id": "2507.06430", "pdf": "https://arxiv.org/pdf/2507.06430", "abs": "https://arxiv.org/abs/2507.06430", "authors": ["Elham Akbari", "Zihao Zhou", "Mohammad Ali Salahuddin", "Noura Limam", "Raouf Boutaba", "Bertrand Mathieu", "Stephanie Moteau", "Stephane Tuffin"], "title": "One task to rule them all: A closer look at traffic classification generalizability", "categories": ["cs.NI"], "comment": null, "summary": "Existing website fingerprinting and traffic classification solutions do not\nwork well when the evaluation context changes, as their performances often\nheavily rely on context-specific assumptions. To clarify this problem, we take\nthree prior solutions presented for different but similar traffic\nclassification and website fingerprinting tasks, and apply each solution's\nmodel to another solution's dataset. We pinpoint dataset-specific and\nmodel-specific properties that lead each of them to overperform in their\nspecific evaluation context.\n  As a realistic evaluation context that takes practical labeling constraints\ninto account, we design an evaluation framework using two recent real-world TLS\ntraffic datasets from large-scale networks. The framework simulates a\nfuturistic scenario in which SNIs are hidden in some networks but not in\nothers, and the classifier's goal is to predict destination services in one\nnetwork's traffic, having been trained on a labelled dataset collected from a\ndifferent network. Our framework has the distinction of including real-world\ndistribution shift, while excluding concept drift. We show that, even when\nabundant labeled data is available, the best solutions' performances under\ndistribution shift are between 30% and 40%, and a simple 1-Nearest Neighbor\nclassifier's performance is not far behind. We depict all performances measured\non different models, not just the best ones, for a fair representation of\ntraffic models in practice."}
{"id": "2507.06804", "pdf": "https://arxiv.org/pdf/2507.06804", "abs": "https://arxiv.org/abs/2507.06804", "authors": ["Zhenwen Liang", "Linfeng Song", "Yang Li", "Tao Yang", "Feng Zhang", "Haitao Mi", "Dong Yu"], "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning and Proving", "categories": ["cs.LO", "cs.AI"], "comment": "Work in progress", "summary": "Automated Theorem Proving (ATP) in formal languages is a foundational\nchallenge for AI. While Large Language Models (LLMs) have driven remarkable\nprogress, a significant gap remains between their powerful informal reasoning\ncapabilities and their weak formal proving performance. Recent studies show\nthat the informal accuracy exceeds 80% while formal success remains below 8% on\nbenchmarks like PutnamBench. We argue this gap persists because current\nstate-of-the-art provers, by tightly coupling reasoning and proving, are\ntrained with paradigms that inadvertently punish deep reasoning in favor of\nshallow, tactic-based strategies. To bridge this fundamental gap, we propose a\nnovel framework that decouples high-level reasoning from low-level proof\ngeneration. Our approach utilizes two distinct, specialized models: a powerful,\ngeneral-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an\nefficient Prover to rigorously verify them. This modular design liberates the\nmodel's full reasoning potential and bypasses the pitfalls of end-to-end\ntraining. We evaluate our method on a challenging set of post-2000 IMO\nproblems, a problem set on which no prior open-source prover has reported\nsuccess. Our decoupled framework successfully solves 5 of these problems,\ndemonstrating a significant step towards automated reasoning on exceptionally\ndifficult mathematical challenges. To foster future research, we release our\nfull dataset of generated and verified lemmas for a wide range of IMO problems,\navailable at https://tencent-imo.github.io/ ."}
{"id": "2507.06235", "pdf": "https://arxiv.org/pdf/2507.06235", "abs": "https://arxiv.org/abs/2507.06235", "authors": ["Yuto Mandai", "Katie Seaborn", "Tomoyasu Nakano", "Xin Sun", "Yijia Wang", "Jun Kato"], "title": "Super Kawaii Vocalics: Amplifying the \"Cute\" Factor in Computer Voice", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "\"Kawaii\" is the Japanese concept of cute, which carries sociocultural\nconnotations related to social identities and emotional responses. Yet,\nvirtually all work to date has focused on the visual side of kawaii, including\nin studies of computer agents and social robots. In pursuit of formalizing the\nnew science of kawaii vocalics, we explored what elements of voice relate to\nkawaii and how they might be manipulated, manually and automatically. We\nconducted a four-phase study (grand N = 512) with two varieties of computer\nvoices: text-to-speech (TTS) and game character voices. We found kawaii \"sweet\nspots\" through manipulation of fundamental and formant frequencies, but only\nfor certain voices and to a certain extent. Findings also suggest a ceiling\neffect for the kawaii vocalics of certain voices. We offer empirical validation\nof the preliminary kawaii vocalics model and an elementary method for\nmanipulating kawaii perceptions of computer voice."}
{"id": "2507.06360", "pdf": "https://arxiv.org/pdf/2507.06360", "abs": "https://arxiv.org/abs/2507.06360", "authors": ["Dustin Jamner", "Gabriel Kammer", "Ritam Nag", "Adam Chlipala"], "title": "Pyrosome: Verified Compilation for Modular Metatheory", "categories": ["cs.PL"], "comment": null, "summary": "We present Pyrosome, a generic framework for modular language metatheory that\nembodies a novel approach to extensible semantics and compilation, implemented\nin Coq. Common techniques for semantic reasoning are often tied to the specific\nstructures of the languages and compilers that they support. In Pyrosome,\nverified compilers are fully extensible, meaning that to extend a language\n(even with a new kind of effect) simply requires defining and verifying the\ncompilation of the new feature, reusing the old correctness theorem for all\nother cases. The novel enabling idea is an inductive formulation of equivalence\npreservation that supports the addition of new rules to the source language,\ntarget language, and compiler.\n  Pyrosome defines a formal, deeply embedded notion of programming languages\nwith semantics given by dependently sorted equational theories, so all\ncompiler-correctness proofs boil down to type-checking and equational\nreasoning. We support vertical composition of any compilers expressed in our\nframework in addition to feature extension. As a case study, we present a\nmultipass compiler from System F with simple references, through CPS\ntranslation and closure conversion. Specifically, we demonstrate how we can\nbuild such a compiler incrementally by starting with a compiler for simply\ntyped lambda-calculus and adding natural numbers, the unit type, recursive\nfunctions, and a global heap, then extending judgments with a type environment\nand adding type abstraction, all while reusing the original theorems. We also\npresent a linear version of the simply typed CPS pass and compile a small\nimperative language to the simply typed target to show how Pyrosome handles\nsubstructural typing and imperative features."}
{"id": "2507.06452", "pdf": "https://arxiv.org/pdf/2507.06452", "abs": "https://arxiv.org/abs/2507.06452", "authors": ["Yigong Hu", "Haodong Zheng", "Yicheng Liu", "Dedong Xie", "Youliang Huang", "Baris Kasikci"], "title": "gigiProfiler: Diagnosing Performance Issues by Uncovering Application Resource Bottlenecks", "categories": ["cs.PF", "cs.SE"], "comment": null, "summary": "Diagnosing performance bottlenecks in modern software is essential yet\nchallenging, particularly as applications become more complex and rely on\ncustom resource management policies. While traditional profilers effectively\nidentify execution bottlenecks by tracing system-level metrics, they fall short\nwhen it comes to application-level resource contention caused by waiting for\napplication-level events. In this work, we introduce OmniResource Profiling, a\nperformance analysis approach that integrates system-level and\napplication-level resource tracing to diagnose resource bottlenecks\ncomprehensively. gigiProfiler, our realization of OmniResource Profiling, uses\na hybrid LLM-static analysis approach to identify application-defined resources\noffline and analyze their impact on performance during buggy executions to\nuncover the performance bottleneck. gigiProfiler then samples and records\ncritical variables related to these bottleneck resources during buggy execution\nand compares their value with those from normal executions to identify the root\ncauses. We evaluated gigiProfiler on 12 real-world performance issues across\nfive applications. gigiProfiler accurately identified performance bottlenecks\nin all cases. gigiProfiler also successfully diagnosed the root causes of two\nnewly emerged, previously undiagnosed problems, with the findings confirmed by\ndevelopers."}
{"id": "2507.06373", "pdf": "https://arxiv.org/pdf/2507.06373", "abs": "https://arxiv.org/abs/2507.06373", "authors": ["Jeremy Fischer", "Ram Krishnamoorthy", "Vishal Kumar", "Mahdi Al-Husseini"], "title": "Digital Wargames to Enhance Military Medical Evacuation Decision-Making", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MM"], "comment": null, "summary": "Medical evacuation is one of the United States Army's most storied and\ncritical mission sets, responsible for efficiently and expediently evacuating\nthe battlefield ill and injured. Medical evacuation planning involves designing\na robust network of medical platforms and facilities capable of moving and\ntreating large numbers of casualties. Until now, there has not been a medium to\nsimulate these networks in a classroom setting and evaluate both offline\nplanning and online decision-making performance. This work describes the\nMedical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer\nsimulation developed in Unity that replicates battlefield constraints and\nuncertainties. MEWI accurately models patient interactions at casualty\ncollection points, ambulance exchange points, medical treatment facilities, and\nevacuation platforms. Two operational scenarios are introduced: an amphibious\nisland assault in the Pacific and a Eurasian conflict across a sprawling road\nand river network. These scenarios pit students against the clock to save as\nmany casualties as possible while adhering to doctrinal lessons learned during\ndidactic training. We visualize performance data collected from two iterations\nof the MEWI Pacific scenario executed in the United States Army's Medical\nEvacuation Doctrine Course. We consider post-wargame Likert survey data from\nstudent participants and external observer notes to identify key planning\ndecision points, document medical evacuation lessons learned, and quantify\ngeneral utility. Results indicate that MEWI participation substantially\nimproves uptake of medical evacuation lessons learned and co-operative\ndecision-making. MEWI is a substantial step forward in the field of\nhigh-fidelity training tools for medical education, and our study findings\noffer critical insights into improving medical evacuation education and\noperations across the joint force."}
{"id": "2507.06484", "pdf": "https://arxiv.org/pdf/2507.06484", "abs": "https://arxiv.org/abs/2507.06484", "authors": ["Fan-Yun Sun", "Shengguang Wu", "Christian Jacobsen", "Thomas Yim", "Haoming Zou", "Alex Zook", "Shangru Li", "Yu-Hsin Chou", "Ethem Can", "Xunlei Wu", "Clemens Eppner", "Valts Blukis", "Jonathan Tremblay", "Jiajun Wu", "Stan Birchfield", "Nick Haber"], "title": "3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds", "categories": ["cs.GR", "cs.CV"], "comment": "project website: https://ai.stanford.edu/~sunfanyun/3d-generalist/", "summary": "Despite large-scale pretraining endowing models with language and vision\nreasoning capabilities, improving their spatial reasoning capability remains\nchallenging due to the lack of data grounded in the 3D world. While it is\npossible for humans to manually create immersive and interactive worlds through\n3D graphics, as seen in applications such as VR, gaming, and robotics, this\nprocess remains highly labor-intensive. In this paper, we propose a scalable\nmethod for generating high-quality 3D environments that can serve as training\ndata for foundation models. We recast 3D environment building as a sequential\ndecision-making problem, employing Vision-Language-Models (VLMs) as policies\nthat output actions to jointly craft a 3D environment's layout, materials,\nlighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to\ngenerate more prompt-aligned 3D environments via self-improvement fine-tuning.\nWe demonstrate the effectiveness of 3D-Generalist and the proposed training\nstrategy in generating simulation-ready 3D environments. Furthermore, we\ndemonstrate its quality and scalability in synthetic data generation by\npretraining a vision foundation model on the generated data. After fine-tuning\nthe pre-trained model on downstream tasks, we show that it surpasses models\npre-trained on meticulously human-crafted synthetic data and approaches results\nachieved with real data orders of magnitude larger."}
{"id": "2507.06981", "pdf": "https://arxiv.org/pdf/2507.06981", "abs": "https://arxiv.org/abs/2507.06981", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Optimizing Cognitive Networks: Reinforcement Learning Meets Energy Harvesting Over Cascaded Channels", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": null, "summary": "This paper presents a reinforcement learning (RL) based approach to improve\nthe physical layer security (PLS) of an underlay cognitive radio network (CRN)\nover cascaded channels. These channels are utilized in highly mobile networks\nsuch as cognitive vehicular networks (CVN). In addition, an eavesdropper aims\nto intercept the communications between secondary users (SUs). The SU receiver\nhas full-duplex and energy harvesting capabilities to generate jamming signals\nto confound the eavesdropper and enhance security. Moreover, the SU transmitter\nextracts energy from ambient radio frequency signals in order to power\nsubsequent transmissions to its intended receiver. To optimize the privacy and\nreliability of the SUs in a CVN, a deep Q-network (DQN) strategy is utilized\nwhere multiple DQN agents are required such that an agent is assigned at each\nSU transmitter. The objective for the SUs is to determine the optimal\ntransmission power and decide whether to collect energy or transmit messages\nduring each time period in order to maximize their secrecy rate. Thereafter, we\npropose a DQN approach to maximize the throughput of the SUs while respecting\nthe interference threshold acceptable at the receiver of the primary user.\nAccording to our findings, our strategy outperforms two other baseline\nstrategies in terms of security and reliability."}
{"id": "2507.06467", "pdf": "https://arxiv.org/pdf/2507.06467", "abs": "https://arxiv.org/abs/2507.06467", "authors": ["Luyu Qiu", "Jianing Li", "Chi Su", "Lei Chen"], "title": "Interactive Text-to-SQL via Expected Information Gain for Disambiguation", "categories": ["cs.DB"], "comment": "13 pages, 5 figure", "summary": "Relational databases are foundational to numerous domains, including business\nintelligence, scientific research, and enterprise systems. However, accessing\nand analyzing structured data often requires proficiency in SQL, which is a\nskill that many end users lack. With the development of Natural Language\nProcessing (NLP) technology, the Text-to-SQL systems attempt to bridge this gap\nby translating natural language questions into executable SQL queries via an\nautomated algorithm. Yet, when operating on complex real-world databases, the\nText-to-SQL systems often suffer from ambiguity due to natural ambiguity in\nnatural language queries. These ambiguities pose a significant challenge for\nexisting Text-to-SQL translation systems, which tend to commit early to a\npotentially incorrect interpretation. To address this, we propose an\ninteractive Text-to-SQL framework that models SQL generation as a probabilistic\nreasoning process over multiple candidate queries. Rather than producing a\nsingle deterministic output, our system maintains a distribution over possible\nSQL outputs and seeks to resolve uncertainty through user interaction. At each\ninteraction step, the system selects a branching decision and formulates a\nclarification question aimed at disambiguating that aspect of the query.\nCrucially, we adopt a principled decision criterion based on Expected\nInformation Gain to identify the clarification that will, in expectation, most\nreduce the uncertainty in the SQL distribution."}
{"id": "2507.06376", "pdf": "https://arxiv.org/pdf/2507.06376", "abs": "https://arxiv.org/abs/2507.06376", "authors": ["Elisavet Lydia Alvanaki", "Kevin Lee", "Luca P. Carloni"], "title": "SLDB: An End-To-End Heterogeneous System-on-Chip Benchmark Suite for LLM-Aided Design", "categories": ["cs.AR"], "comment": null, "summary": "Over the last few years, Large Language Models (LLMs) have emerged as a\nvaluable tool for Electronic Design Automation (EDA). State-of-the-art research\nin LLM-aided design has demonstrated the ability of LLMs to generate\nsyntactically correct RTL code, showcasing encouraging prospects for\nintegrating AI into the hardware design process. A key enabler of these\nadvancements is the availability of high-quality benchmarks to evaluate new\napproaches. However, existing datasets and benchmarks fall short of\nsystem-level design, as they focus primarily on component-level information and\nlow-complexity designs. To address this gap, we introduce the System-Level\nDesign Benchmark (SLDB), a dataset tailored for evaluating LLMs in system-level\nintegration and configuration tasks. SLDB includes a curated benchmark suite of\n10 baseline SoC designs, whose components can be combined into an exponential\nnumber of distinct tile-based SoCs through a synthetic library. The dataset\nprovides full SoC configurations, accelerator integration code, communication\nparameters, and accelerator-aware system configurations, along with\ntesting-application code, compatible with the ESP platform[1]."}
{"id": "2507.06471", "pdf": "https://arxiv.org/pdf/2507.06471", "abs": "https://arxiv.org/abs/2507.06471", "authors": ["Fuhuan Li", "Zhihui Du", "David A. Bader"], "title": "Designing Parallel Algorithms for Community Detection using Arachne", "categories": ["cs.DC", "cs.DS"], "comment": null, "summary": "The rise of graph data in various fields calls for efficient and scalable\ncommunity detection algorithms. In this paper, we present parallel\nimplementations of two widely used algorithms: Label Propagation and Louvain,\nspecifically designed to leverage the capabilities of Arachne which is a\nPython-accessible, open-source framework for large-scale graph analysis. Our\nimplementations achieve substantial speedups over existing Python-based tools\nlike NetworkX and igraph, which lack efficient parallelization, and are\ncompetitive with parallel frameworks such as NetworKit. Experimental results\nshow that Arachne-based methods outperform these baselines, achieving speedups\nof up to 710x over NetworkX, 75x over igraph, and 12x over NetworKit.\nAdditionally, we analyze the scalability of our implementation under varying\nthread counts, demonstrating how different phases contribute to overall\nperformance gains of the parallel Louvain algorithm. Arachne, including our\ncommunity detection implementation, is open-source and available at\nhttps://github.com/Bears-R-Us/arkouda-njit ."}
{"id": "2507.06762", "pdf": "https://arxiv.org/pdf/2507.06762", "abs": "https://arxiv.org/abs/2507.06762", "authors": ["Nathalia Barbosa", "Paulo Borba", "Léuson Da Silva"], "title": "Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation", "categories": ["cs.SE", "K.6.3"], "comment": "Comments: 11 pages, in Portuguese language. 3 figures. Submitted to\n  SAST 2025 (X Simp\\'osio Brasileiro de Teste de Software Sistem\\'atico e\n  Automatizado)", "summary": "Semantic conflicts arise when a developer introduces changes to a codebase\nthat unintentionally affect the behavior of changes integrated in parallel by\nother developers. Traditional merge tools are unable to detect such conflicts,\nso complementary tools like SMAT have been proposed. SMAT relies on generating\nand executing unit tests: if a test fails on the base version, passes on a\ndeveloper's modified version, but fails again after merging with another\ndeveloper's changes, a semantic conflict is indicated. While SMAT is effective\nat detecting conflicts, it suffers from a high rate of false negatives, partly\ndue to the limitations of unit test generation tools such as Randoop and\nEvosuite. To investigate whether large language models (LLMs) can overcome\nthese limitations, we propose and integrate a new test generation tool based on\nCode Llama 70B into SMAT. We explore the model's ability to generate tests\nusing different interaction strategies, prompt contents, and parameter\nconfigurations. Our evaluation uses two samples: a benchmark with simpler\nsystems from related work, and a more significant sample based on complex,\nreal-world systems. We assess the effectiveness of the new SMAT extension in\ndetecting conflicts. Results indicate that, although LLM-based test generation\nremains challenging and computationally expensive in complex scenarios, there\nis promising potential for improving semantic conflict detection.\n  --\n  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\\c{c}as em\numa base de c\\'odigo que afetam, de forma n~ao intencional, o comportamento de\naltera\\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas\ntradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso\nferramentas complementares como o SMAT foram propostas. O SMAT depende da\ngera\\c{c}~ao e execu\\c{c}~ao de testes de unidade: se um teste falha na vers~ao\nbase, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar\nap\\'os o merge com as mudan\\c{c}as de outro desenvolvedor, um conflito\nsem^antico \\'e identificado. Embora o SMAT seja eficaz na detec\\c{c}~ao de\nconflitos, apresenta alta taxa de falsos negativos, em parte devido \\`as\nlimita\\c{c}~oes das ferramentas de gera\\c{c}~ao de testes como Randoop e\nEvosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem\nsuperar essas limita\\c{c}~oes, propomos e integramos ao SMAT uma nova\nferramenta de gera\\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a\ncapacidade do modelo de gerar testes utilizando diferentes estrat\\'egias de\nintera\\c{c}~ao, conte\\'udos de prompts e configura\\c{c}~oes de par^ametros.\nNossa avalia\\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais\nsimples, usados em trabalhos relacionados, e uma amostra mais significativa\nbaseada em sistemas complexos e reais. Avaliamos a efic\\'acia da nova extens~ao\ndo SMAT na detec\\c{c}~ao de conflitos. Os resultados indicam que, embora a\ngera\\c{c}~ao de testes por LLM em cen\\'arios complexos ainda seja desafiadora e\ncustosa computacionalmente, h\\'a potencial promissor para aprimorar a\ndetec\\c{c}~ao de conflitos sem^anticos."}
{"id": "2507.06632", "pdf": "https://arxiv.org/pdf/2507.06632", "abs": "https://arxiv.org/abs/2507.06632", "authors": ["Liyuan Chen", "Kai Xiong", "Yujie Qin", "Hanqing Yu", "Supeng Leng", "Chau Yuen"], "title": "Stacked Intelligent Metasurfaces-Aided eVTOL Delay Sensitive Communications", "categories": ["cs.NI"], "comment": null, "summary": "With rapid urbanization and increasing population density, urban traffic\ncongestion has become a critical issue, and traditional ground transportation\nmethods are no longer sufficient to address it effectively. To tackle this\nchallenge, the concept of Advanced Air Mobility (AAM) has emerged, aiming to\nutilize low-altitude airspace to establish a three-dimensional transportation\nsystem. Among various components of the AAM system, electric vertical take-off\nand landing (eVTOL) aircraft plays a pivotal role due to their flexibility and\nefficiency. However, the immaturity of Ultra Reliable Low Latency Communication\n(URLLC) technologies poses significant challenges to safety-critical AAM\noperations. Specifically, existing Stacked Intelligent Metasurfaces (SIM)-based\neVTOL systems lack rigorous mathematical frameworks to quantify probabilistic\ndelay bounds under dynamic air traffic patterns, a prerequisite for collision\navoidance and airspace management. To bridge this gap, we employ network\ncalculus tools to derive the probabilistic upper bound on communication delay\nin the AAM system for the first time. Furthermore, we formulate a complex\nnon-convex optimization problem that jointly minimizes the probabilistic delay\nbound and the propagation delay. To solve this problem efficiently, we propose\na solution based on the Block Coordinate Descent (BCD) algorithm and\nSemidefinite Relaxation (SDR) method. In addition, we conduct a comprehensive\nanalysis of how various factors impact regret and transmission rate, and\nexplore the influence of varying load intensity and total delay on the\nprobabilistic delay bound."}
{"id": "2507.06854", "pdf": "https://arxiv.org/pdf/2507.06854", "abs": "https://arxiv.org/abs/2507.06854", "authors": ["Sara Ayhan", "Hrafn Valtýr Oddsson"], "title": "Proof-Theoretic Functional Completeness for the Connexive Logic C", "categories": ["cs.LO", "math.LO"], "comment": "For published version, see https://rdcu.be/evpKb", "summary": "We show the functional completeness for the connectives of the non-trivial\nnegation inconsistent logic C by using a well-established method implementing\npurely proof-theoretic notions only. Firstly, given that C contains a strong\nnegation, expressing a notion of direct refutation, the proof needs to be\napplied in a bilateralist way in that not only higher-order rule schemata for\nproofs but also for refutations need to be considered. Secondly, given that C\nis a connexive logic we need to take a connexive understanding of inference as\na basis, leading to a different conception of (higher-order) refutation than is\nusually employed."}
{"id": "2507.06460", "pdf": "https://arxiv.org/pdf/2507.06460", "abs": "https://arxiv.org/abs/2507.06460", "authors": ["Sam Cohen", "Ravi Chugh"], "title": "Ragged Blocks: Rendering Structured Text with Style", "categories": ["cs.HC"], "comment": null, "summary": "Whether it be source code in a programming language, prose in natural\nlanguage, or otherwise, text is highly structured. Currently, text\nvisualizations are confined either to _flat, line-based_ decorations, which can\nconvey only limited information about textual structure, or _nested boxes_,\nwhich convey structure but often destroy the typographic layout of the\nunderlying text. We hypothesize that the lack of rich styling options limits\nthe kinds of information that are displayed alongside text, wherever it may be\ndisplayed.\n  In this paper, we show that it is possible to achieve arbitrarily nested\ndecorations while minimally disturbing the underlying typographic layout.\nSpecifically, we present a layout algorithm that generates _ragged blocks_, or\n_rocks_, which are rectilinear polygons that allow nested text to be compactly\nrendered even when styled with borders and padding.\n  We evaluate our layout algorithm in two ways. First, on a benchmark suite\ncomprising representative source code files in multiple programming languages,\nwe show that the (ragged block) layouts produced by our algorithm are\nsubstantially more compact than the (rectangular block) layouts produced by\nconventional techniques, when uniformly styling every element in the syntax\ntree with borders and padding. Second, through a small gallery of usage\nscenarios, we demonstrate how future code editors, word processors, and other\ndocument-rendering GUIs might convey rich semantic information through\ndomain-specific styling of ragged blocks."}
{"id": "2507.06456", "pdf": "https://arxiv.org/pdf/2507.06456", "abs": "https://arxiv.org/abs/2507.06456", "authors": ["Scott Kovach", "Praneeth Kolichala", "Kyle A. Miller", "David Broman", "Fredrik Kjolstad"], "title": "Fast Collection Operations from Indexed Stream Fusion", "categories": ["cs.PL"], "comment": null, "summary": "We present a system of efficient methods for traversing and combining\nassociative collection data structures. A distinguishing feature of the system\nis that, like traditional sequential iterator libraries, it does not require\nspecialized compiler infrastructure or staged compilation for efficiency and\ncomposability. By using a representation based on indexed streams, the library\ncan express complex joins over input collections while using no intermediate\nallocations. We implement the library for the Lean, Morphic, and Rust\nprogramming languages and provide a mechanized proof of functional correctness\nin Lean."}
{"id": "2507.06672", "pdf": "https://arxiv.org/pdf/2507.06672", "abs": "https://arxiv.org/abs/2507.06672", "authors": ["Lucas Thil", "Jesse Read", "Rim Kaddah", "Guillaume Florent Doquet"], "title": "Uncertainty Quantification as a Complementary Latent Health Indicator for Remaining Useful Life Prediction on Turbofan Engines", "categories": ["cs.PF"], "comment": null, "summary": "Health Indicators (HIs) are essential for predicting system failures in\npredictive maintenance. While methods like RaPP (Reconstruction along Projected\nPathways) improve traditional HI approaches by leveraging autoencoder latent\nspaces, their performance can be hindered by both aleatoric and epistemic\nuncertainties. In this paper, we propose a novel framework that integrates\nuncertainty quantification into autoencoder-based latent spaces, enhancing\nRaPP-generated HIs. We demonstrate that separating aleatoric uncertainty from\nepistemic uncertainty and cross combining HI information is the driver of\naccuracy improvements in Remaining Useful Life (RUL) prediction. Our method\nemploys both standard and variational autoencoders to construct these HIs,\nwhich are then used to train a machine learning model for RUL prediction.\nBenchmarked on the NASA C-MAPSS turbofan dataset, our approach outperforms\ntraditional HI-based methods and end-to-end RUL prediction models and is\ncompetitive with RUL estimation methods. These results underscore the\nimportance of uncertainty quantification in health assessment and showcase its\nsignificant impact on predictive performance when incorporated into the HI\nconstruction process."}
{"id": "2507.06717", "pdf": "https://arxiv.org/pdf/2507.06717", "abs": "https://arxiv.org/abs/2507.06717", "authors": ["Xuyang Chen", "Chong Huang", "Daquan Feng", "Lei Luo", "Yao Sun", "Xiang-Gen Xia"], "title": "QoE Optimization for Semantic Self-Correcting Video Transmission in Multi-UAV Networks", "categories": ["eess.IV", "cs.MM"], "comment": "13 pages", "summary": "Real-time unmanned aerial vehicle (UAV) video streaming is essential for\ntime-sensitive applications, including remote surveillance, emergency response,\nand environmental monitoring. However, it faces challenges such as limited\nbandwidth, latency fluctuations, and high packet loss. To address these issues,\nwe propose a novel semantic self-correcting video transmission framework with\nultra-fine bitrate granularity (SSCV-G). In SSCV-G, video frames are encoded\ninto a compact semantic codebook space, and the transmitter adaptively sends a\nsubset of semantic indices based on bandwidth availability, enabling\nfine-grained bitrate control for improved bandwidth efficiency. At the\nreceiver, a spatio-temporal vision transformer (ST-ViT) performs multi-frame\njoint decoding to reconstruct dropped semantic indices by modeling intra- and\ninter-frame dependencies. To further improve performance under dynamic network\nconditions, we integrate a multi-user proximal policy optimization (MUPPO)\nreinforcement learning scheme that jointly optimizes communication resource\nallocation and semantic bitrate selection to maximize user Quality of\nExperience (QoE). Extensive experiments demonstrate that the proposed SSCV-G\nsignificantly outperforms state-of-the-art video codecs in coding efficiency,\nbandwidth adaptability, and packet loss robustness. Moreover, the proposed\nMUPPO-based QoE optimization consistently surpasses existing benchmarks."}
{"id": "2507.06646", "pdf": "https://arxiv.org/pdf/2507.06646", "abs": "https://arxiv.org/abs/2507.06646", "authors": ["Zicong Peng", "Yicheng Zhan", "Josef Spjut", "Kaan Akşit"], "title": "Assessing Learned Models for Phase-only Hologram Compression", "categories": ["cs.GR"], "comment": "SIGGRAPH 2025 Poster", "summary": "We evaluate the performance of four common learned models utilizing INR and\nVAE structures for compressing phase-only holograms in holographic displays.\nThe evaluated models include a vanilla MLP, SIREN, and FilmSIREN, with TAESD as\nthe representative VAE model. Our experiments reveal that a pretrained image\nVAE, TAESD, with 2.2M parameters struggles with phase-only hologram\ncompression, revealing the need for task-specific adaptations. Among the INRs,\nSIREN with 4.9k parameters achieves %40 compression with high quality in the\nreconstructed 3D images (PSNR = 34.54 dB). These results emphasize the\neffectiveness of INRs and identify the limitations of pretrained image\ncompression VAEs for hologram compression task."}
{"id": "2507.06983", "pdf": "https://arxiv.org/pdf/2507.06983", "abs": "https://arxiv.org/abs/2507.06983", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Maximizing Reliability in Overlay Radio Networks with Time Switching and Power Splitting Energy Harvesting", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": null, "summary": "Cognitive radio networks (CRNs) are acknowledged for their ability to tackle\nthe issue of spectrum under-utilization. In the realm of CRNs, this paper\ninvestigates the energy efficiency issue and addresses the critical challenge\nof optimizing system reliability for overlay CRN access mode. Randomly\ndispersed secondary users (SUs) serving as relays for primary users (PUs) are\nconsidered, in which one of these relays is designated to harvest energy\nthrough the time switching-energy harvesting (EH) protocol. Moreover, this\nrelay amplifies-and-forwards (AF) the PU's messages and broadcasts them along\nwith its own across cascaded $\\kappa$-$\\mu$ fading channels. The power\nsplitting protocol is another EH approach utilized by the SU and PU receivers\nto enhance the amount of energy in their storage devices. In addition, the SU\ntransmitters and the SU receiver are deployed with multiple antennas for\nreception and apply the maximal ratio combining approach. The outage\nprobability is utilized to assess both networks' reliability. Then, an energy\nefficiency evaluation is performed to determine the effectiveness of EH on the\nsystem. Finally, an optimization problem is provided with the goal of\nmaximizing the data rate of the SUs by optimizing the time switching and the\npower allocation parameters of the SU relay."}
{"id": "2507.06515", "pdf": "https://arxiv.org/pdf/2507.06515", "abs": "https://arxiv.org/abs/2507.06515", "authors": ["Zhaoze Sun", "Qiyan Deng", "Chengliang Chai", "Kaisen Jin", "Xinyu Guo", "Han Han", "Ye Yuan", "Guoren Wang", "Lei Cao"], "title": "QUEST: Query Optimization in Unstructured Document Analysis", "categories": ["cs.DB"], "comment": null, "summary": "Most recently, researchers have started building large language models (LLMs)\npowered data systems that allow users to analyze unstructured text documents\nlike working with a database because LLMs are very effective in extracting\nattributes from documents. In such systems, LLM-based extraction operations\nconstitute the performance bottleneck of query execution due to the high\nmonetary cost and slow LLM inference. Existing systems typically borrow the\nquery optimization principles popular in relational databases to produce query\nexecution plans, which unfortunately are ineffective in minimizing LLM cost. To\nfill this gap, we propose QUEST, which features a bunch of novel optimization\nstrategies for unstructured document analysis. First, we introduce an\nindex-based strategy to minimize the cost of each extraction operation. With\nthis index, QUEST quickly retrieves the text segments relevant to the target\nattributes and only feeds them to LLMs. Furthermore, we design an\nevidence-augmented retrieval strategy to reduce the possibility of missing\nrelevant segments. Moreover, we develop an instance-optimized query execution\nstrategy: because the attribute extraction cost could vary significantly\ndocument by document, QUEST produces different plans for different documents.\nFor each document, QUEST produces a plan to minimize the frequency of attribute\nextraction. The innovations include LLM cost-aware operator ordering strategies\nand an optimized join execution approach that transforms joins into filters.\nExtensive experiments on 3 real-world datasets demonstrate the superiority of\nQUEST, achieving 30%-6x cost savings while improving the F1 score by 10% -27%\ncompared with state-of-the-art baselines."}
{"id": "2507.06512", "pdf": "https://arxiv.org/pdf/2507.06512", "abs": "https://arxiv.org/abs/2507.06512", "authors": ["Siyu Qiu", "Muzhi Wang", "Raheel Afsharmazayejani", "Mohammad Moradi Shahmiri", "Benjamin Tan", "Hammond Pearce"], "title": "Towards LLM-based Root Cause Analysis of Hardware Design Failures", "categories": ["cs.AR", "cs.AI"], "comment": "6 pages. Accepted for publication in IEEE COINS 2025 Special Session\n  on LLMs for EDA and Security", "summary": "With advances in large language models (LLMs), new opportunities have emerged\nto develop tools that support the digital hardware design process. In this\nwork, we explore how LLMs can assist with explaining the root cause of design\nissues and bugs that are revealed during synthesis and simulation, a necessary\nmilestone on the pathway towards widespread use of LLMs in the hardware design\nprocess and for hardware security analysis. We find promising results: for our\ncorpus of 34 different buggy scenarios, OpenAI's o3-mini reasoning model\nreached a correct determination 100% of the time under pass@5 scoring, with\nother state of the art models and configurations usually achieving more than\n80% performance and more than 90% when assisted with retrieval-augmented\ngeneration."}
{"id": "2507.06608", "pdf": "https://arxiv.org/pdf/2507.06608", "abs": "https://arxiv.org/abs/2507.06608", "authors": ["Xiaoxiang Shi", "Colin Cai", "Junjia Du", "Zhanda Zhu", "Xingda Wei", "Zhihao Jia"], "title": "Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient GPU Sharing", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Current prefill-decode (PD) disaggregation is typically deployed at the level\nof entire serving engines, assigning separate GPUs to handle prefill and decode\nphases. While effective at reducing latency, this approach demands more\nhardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode\nrequests within the same batch, but introduces phase interference between\nprefill and decode.\n  While existing PD disaggregation solutions separate the phases across GPUs,\nwe ask: can the same decoupling be achieved within a single serving engine? The\nkey challenge lies in managing the conflicting resource requirements of prefill\nand decode when they share the same hardware. In this paper, we first show that\nchunked prefill requests cause interference with decode requests due to their\ndistinct requirements for GPU resources. Second, we find that GPU resources\nexhibit diminishing returns. Beyond a saturation point, increasing GPU\nallocation yields negligible latency improvements. This insight enables us to\nsplit a single GPU's resources and dynamically allocate them to prefill and\ndecode on the fly, effectively disaggregating the two phases within the same\nGPU.\n  Across a range of models and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also\noutperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x\nlower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using\nonly half the number of GPUs."}
{"id": "2507.06881", "pdf": "https://arxiv.org/pdf/2507.06881", "abs": "https://arxiv.org/abs/2507.06881", "authors": ["Brian R Larson", "Ehsan Ahmad"], "title": "Formalization of the AADL Run-Time Services with Time", "categories": ["cs.SE", "cs.SY", "eess.SY"], "comment": "35 pages, 13 figures", "summary": "The Architecture Analysis & Design Language (AADL) is an architecture\ndescription language for design of cyber-physical systems--machines controlled\nby software. The AADL standard, SAE International AS5506D, describes Run-Time\nServices (RTS) to be provided to execute AADL models in accordance with\nsemantics defined by the standard. The RTS of primary concern are transport\nservices and timing services. Although, the study presented in [1] sets a\nfoundation for the formal semantics of AADL, but without modeling time. This\npaper extends and simplifies this formalization using a modal logic defined by\na Kripke structure, to explicitly include time. The RTS defined in the AADL\nstandard are also expanded to support reactive state-transition machines of the\nBehavior Specification annex standard language (BA) and its closely-related,\nformally-defined counterpart, the Behavior Language for Embedded Systems with\nSoftware (BLESS). An example of AADL RTS with time, implemented by the High\nAssurance Modeling and Rapid Engineering for Embedded Systems (HAMR) for\nstate-transition machine behavior written in BLESS, is also presented."}
{"id": "2507.06911", "pdf": "https://arxiv.org/pdf/2507.06911", "abs": "https://arxiv.org/abs/2507.06911", "authors": ["Michele Polese", "Niloofar Mohamadi", "Salvatore D'Oro", "Tommaso Melodia"], "title": "Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G", "categories": ["cs.NI", "cs.AI", "eess.SP"], "comment": "Submitted to IEEE for publication, copyright may change without\n  notice. 8 pages, 6 figures", "summary": "The proliferation of data-intensive Artificial Intelligence (AI) applications\nat the network edge demands a fundamental shift in RAN design, from merely\nconsuming AI for network optimization, to actively enabling distributed AI\nworkloads. This paradigm shift presents a significant opportunity for network\noperators to monetize AI at the edge while leveraging existing infrastructure\ninvestments. To realize this vision, this article presents a novel converged\nO-RAN and AI-RAN architecture that unifies orchestration and management of both\ntelecommunications and AI workloads on shared infrastructure. The proposed\narchitecture extends the Open RAN principles of modularity, disaggregation, and\ncloud-nativeness to support heterogeneous AI deployments. We introduce two key\narchitectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN\nService Management and Orchestration (SMO) to enable integrated resource and\nallocation across RAN and AI workloads; and (ii) AI-RAN sites that provide\ndistributed edge AI platforms with real-time processing capabilities. The\nproposed system supports flexible deployment options, allowing AI workloads to\nbe orchestrated with specific timing requirements (real-time or batch\nprocessing) and geographic targeting. The proposed architecture addresses the\norchestration requirements for managing heterogeneous workloads at different\ntime scales while maintaining open, standardized interfaces and multi-vendor\ninteroperability."}
{"id": "2507.06383", "pdf": "https://arxiv.org/pdf/2507.06383", "abs": "https://arxiv.org/abs/2507.06383", "authors": ["Mustafa Shabani", "Alireza Nasiri", "Hassan Nafardi"], "title": "Forex Trading Robot Using Fuzzy Logic", "categories": ["eess.SY", "cs.CE", "cs.LO", "cs.SY", "I.2.1"], "comment": null, "summary": "In this study, we propose a fuzzy system for conducting short-term\ntransactions in the forex market. The system is designed to enhance common\nstrategies in the forex market using fuzzy logic, thereby improving the\naccuracy of transactions. Traditionally, technical strategies based on\noscillator indicators have relied on predefined ranges for indicators such as\nRelative Strength Index (RSI), Commodity Channel Indicator (CCI), and\nStochastic to determine entry points for trades. However, the use of these\nclassic indicators has yielded suboptimal results due to the changing nature of\nthe market over time. In our proposed approach, instead of employing classical\nindicators, we introduce a fuzzy Mamdani system for each indicator. The results\nobtained from these systems are then combined through voting to design a\ntrading robot. Our findings demonstrate a considerable increase in the\nprofitability factor compared to three other methods. Additionally, net profit,\ngross profit, and maximum capital reduction are calculated and compared across\nall approaches."}
{"id": "2507.06483", "pdf": "https://arxiv.org/pdf/2507.06483", "abs": "https://arxiv.org/abs/2507.06483", "authors": ["Zackary Rackauckas", "Julia Hirschberg"], "title": "Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "This study investigates how stylized, voiced agents shape user interaction in\na multimodal language learning environment. We conducted a mixed-methods\nevaluation of 54 participants interacting with anime-inspired characters\npowered by large language models and expressive text-to-speech synthesis. These\nagents responded in Japanese character language, offering users asynchronous,\nsemi-structured conversation in varying speech styles and emotional tones. We\nanalyzed user engagement patterns, perceived usability, emotional responses,\nand learning behaviors, with particular attention to how agent stylization\ninfluenced interaction across language proficiency levels and cultural\nbackgrounds. Our findings reveal that agent design, especially voice, persona,\nand linguistic style, substantially affected user experience, motivation, and\nstrategy. This work contributes to the understanding of affective, culturally\nstylized agents in human-agent interaction and offers guidance for designing\nmore engaging, socially responsive systems."}
{"id": "2507.06584", "pdf": "https://arxiv.org/pdf/2507.06584", "abs": "https://arxiv.org/abs/2507.06584", "authors": ["Qiong Feng", "Xiaotian Ma", "Ziyuan Feng", "Marat Akhin", "Wei Song", "Peng Liang"], "title": "Finding Compiler Bugs through Cross-Language Code Generator and Differential Testing", "categories": ["cs.PL", "cs.SE"], "comment": "The 40th ACM SIGPLAN International Conference on Object-Oriented\n  Programming, Systems, Languages, and Applications (OOPSLA)", "summary": "Compilers play a central role in translating high-level code into executable\nprograms, making their correctness essential for ensuring code safety and\nreliability. While extensive research has focused on verifying the correctness\nof compilers for single-language compilation, the correctness of cross-language\ncompilation - which involves the interaction between two languages and their\nrespective compilers - remains largely unexplored. To fill this research gap,\nwe propose CrossLangFuzzer, a novel framework that introduces a universal\nintermediate representation (IR) for JVM-based languages and automatically\ngenerates cross-language test programs with diverse type parameters and complex\ninheritance structures. After generating the initial IR, CrossLangFuzzer\napplies three mutation techniques - LangShuffler, FunctionRemoval, and\nTypeChanger - to enhance program diversity. By evaluating both the original and\nmutated programs across multiple compiler versions, CrossLangFuzzer\nsuccessfully uncovered 10 confirmed bugs in the Kotlin compiler, 4 confirmed\nbugs in the Groovy compiler, 7 confirmed bugs in the Scala 3 compiler, 2\nconfirmed bugs in the Scala 2 compiler, and 1 confirmed bug in the Java\ncompiler. Among all mutators, TypeChanger is the most effective, detecting 11\nof the 24 compiler bugs. Furthermore, we analyze the symptoms and root causes\nof cross-compilation bugs, examining the respective responsibilities of\nlanguage compilers when incorrect behavior occurs during cross-language\ncompilation. To the best of our knowledge, this is the firstwork specifically\nfocused on identifying and diagnosing compiler bugs in cross-language\ncompilation scenarios. Our research helps to understand these challenges and\ncontributes to improving compiler correctness in multi-language environments."}
{"id": "2507.06735", "pdf": "https://arxiv.org/pdf/2507.06735", "abs": "https://arxiv.org/abs/2507.06735", "authors": ["Guan Zheng", "Xue Wang", "Wenhua Qian", "Peng Liu", "Runzhuo Ma"], "title": "Residual Prior-driven Frequency-aware Network for Image Fusion", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task."}
{"id": "2507.06790", "pdf": "https://arxiv.org/pdf/2507.06790", "abs": "https://arxiv.org/abs/2507.06790", "authors": ["Arjun Madhusudan", "Benjamin Watson"], "title": "Better frame rates or better visuals? An early report of Esports player practice in Dota 2", "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "Esports athletes often reduce visual quality to improve latency and frame\nrate, and increase their in-game performance. Little research has examined the\neffects of this visuo-spatial tradeoff on performance, but we could find no\nwork studying how players manage this tradeoff in practice. This paper is an\ninitial examination of this question in the game Dota 2. First, we gather the\ngame configuration data of Dota 2 players in a small survey. We learn that\nplayers do limit visual detail, particularly by turning off VSYNC, which\nremoves rendering/display synchronization delay but permits visual \"tearing\".\nSecond, we survey the intent of those same players with a few subjective\nquestions. Player intent matches configuration practice. While our sampling of\nDota 2 players may not be representative, our survey does reveal suggestive\ntrends that lay the groundwork for future, more rigorous and larger surveys.\nSuch surveys can help new players adapt to the game more quickly, encourage\nresearchers to investigate the relative importance of temporal and visual\ndetail, and justify design effort by developers in \"low visual\" game\nconfigurations."}
{"id": "2507.06361", "pdf": "https://arxiv.org/pdf/2507.06361", "abs": "https://arxiv.org/abs/2507.06361", "authors": ["Muhammad Ahsan"], "title": "Experimental Ground-State Energy of a 125-Site Flat Kagome Antiferromagnet via Hamiltonian Engineering on Quantum Computer", "categories": ["quant-ph", "cs.ET"], "comment": "To be submitted to the Physical Review Journal", "summary": "We present an instance of utility-grade quantum computation by calculating\nthe ground-state energy of a 125-site flat Kagome lattice under the\nantiferromagnetic Heisenberg model (KAFH), using IBM's Falcon and Hummingbird\nquantum processors. For spin-1/2 KAFH, our best per-site ground-state energy\nestimate reaches -0.417J, and after applying open-boundary corrections, it\nclosely approaches the established thermodynamic value of -0.438J. To achieve\nthis, we propose a hybrid approach that splits the variational quantum\neigensolver (VQE) into local (classical) and global (quantum) components for\nefficient hardware utilization. We further introduce a Hamiltonian engineering\nstrategy that increases coupling on defect triangles to mimic loop-flip\ndynamics, allowing us to simplify the ansatz while retaining physical accuracy.\nUsing a single-repetition, hardware-efficient ansatz, we entangle up to 103\nqubits with high fidelity to determine the Hamiltonian's lowest eigenvalue.\nThis work demonstrates the scalability of VQE for frustrated 2D systems and\nlays the foundation for future studies using deeper ansatz circuits and larger\nlattices."}
{"id": "2507.07044", "pdf": "https://arxiv.org/pdf/2507.07044", "abs": "https://arxiv.org/abs/2507.07044", "authors": ["Mehrdad Morsali", "Chengwei Zhou", "Deniz Najafi", "Sreetama Sarkar", "Pietro Mercati", "Navid Khoshavi", "Peter Beerel", "Mahdi Nikdast", "Gourav Datta", "Shaahin Angizi"], "title": "Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision Transformer Accelerator with Silicon Photonics", "categories": ["cs.AR"], "comment": null, "summary": "Vision Transformers (ViTs) have emerged as a powerful architecture for\ncomputer vision tasks due to their ability to model long-range dependencies and\nglobal contextual relationships. However, their substantial compute and memory\ndemands hinder efficient deployment in scenarios with strict energy and\nbandwidth limitations. In this work, we propose OptoViT, the first near-sensor,\nregion-aware ViT accelerator leveraging silicon photonics (SiPh) for real-time\nand energy-efficient vision processing. Opto-ViT features a hybrid\nelectronic-photonic architecture, where the optical core handles\ncompute-intensive matrix multiplications using Vertical-Cavity Surface-Emitting\nLasers (VCSELs) and Microring Resonators (MRs), while nonlinear functions and\nnormalization are executed electronically. To reduce redundant computation and\npatch processing, we introduce a lightweight Mask Generation Network (MGNet)\nthat identifies regions of interest in the current frame and prunes irrelevant\npatches before ViT encoding. We further co-optimize the ViT backbone using\nquantization-aware training and matrix decomposition tailored for photonic\nconstraints. Experiments across device fabrication, circuit and architecture\nco-design, to classification, detection, and video tasks demonstrate that\nOptoViT achieves 100.4 KFPS/W with up to 84% energy savings with less than 1.6%\naccuracy loss, while enabling scalable and efficient ViT deployment at the\nedge."}
{"id": "2507.06653", "pdf": "https://arxiv.org/pdf/2507.06653", "abs": "https://arxiv.org/abs/2507.06653", "authors": ["Xiangyu Zhi", "Meng Chen", "Xiao Yan", "Baotong Lu", "Hui Li", "Qianxi Zhang", "Qi Chen", "James Cheng"], "title": "Towards Efficient and Scalable Distributed Vector Search with RDMA", "categories": ["cs.DC"], "comment": null, "summary": "Similarity-based vector search facilitates many important applications such\nas search and recommendation but is limited by the memory capacity and\nbandwidth of a single machine due to large datasets and intensive data read. In\nthis paper, we present CoTra, a system that scales up vector search for\ndistributed execution. We observe a tension between computation and\ncommunication efficiency, which is the main challenge for good scalability,\ni.e., handling the local vectors on each machine independently blows up\ncomputation as the pruning power of vector index is not fully utilized, while\nrunning a global index over all machines introduces rich data dependencies and\nthus extensive communication. To resolve such tension, we leverage the fact\nthat vector search is approximate in nature and robust to asynchronous\nexecution. In particular, we run collaborative vector search over the machines\nwith algorithm-system co-designs including clustering-based data partitioning\nto reduce communication, asynchronous execution to avoid communication stall,\nand task push to reduce network traffic. To make collaborative search\nefficient, we introduce a suite of system optimizations including task\nscheduling, communication batching, and storage format. We evaluate CoTra on\nreal datasets and compare with four baselines. The results show that when using\n16 machines, the query throughput of CoTra scales to 9.8-13.4x over a single\nmachine and is 2.12-3.58x of the best-performing baseline at 0.95 recall@10."}
{"id": "2507.06980", "pdf": "https://arxiv.org/pdf/2507.06980", "abs": "https://arxiv.org/abs/2507.06980", "authors": ["Binquan Zhang", "Li Zhang", "Zhiwen Luo", "Yuxin Du", "Fang Liu", "Song Wang", "Lin Shi"], "title": "Are They All Good? Evaluating the Quality of CoTs in LLM-based Code Generation", "categories": ["cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance in code\ngeneration, particularly when augmented with chain-of-thought (CoT) prompting\ntechniques. They break down requirements into intermediate reasoning steps,\nwhich act as design rationales to guide LLMs in writing code like human\nprogrammers. Thus, the quality of these steps is crucial for ensuring the\ncorrectness and reliability of the generated code. However, little is known\nabout the quality of CoT generated by LLMs. To what extent can we trust the\nthoughts generated by LLMs? How good are they? This paper empirically explores\nthe external and internal factors of why LLMs generate unsatisfactory CoTs by\nanalyzing 1,023 failed code samples on two widely used code generation\nbenchmarks. We also evaluate their impact on code generation performance by\nanalyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting\nLLMs. Our study reveals three key findings: (1) External factors (53.60%), such\nas unclear requirements and lack of context, mainly affect CoT quality, while\ninternal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even\nwhen CoTs are correct, 18.5% of the generated code contains errors due to\ninstruction-following issues; conversely, 11.90% of correct code is paired with\nflawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when\ngiven detailed problem descriptions. These findings highlight key challenges in\nCoT-based code generation and suggest directions for improving LLM reasoning\nand reliability."}
{"id": "2507.06499", "pdf": "https://arxiv.org/pdf/2507.06499", "abs": "https://arxiv.org/abs/2507.06499", "authors": ["Shivangi Agarwal", "Adi Asija", "Sanjit K. Kaul", "Arani Bhattacharya", "Saket Anand"], "title": "Learning To Communicate Over An Unknown Shared Network", "categories": ["cs.MA", "cs.NI"], "comment": "22 pages, 15 figures, 4 tables", "summary": "As robots (edge-devices, agents) find uses in an increasing number of\nsettings and edge-cloud resources become pervasive, wireless networks will\noften be shared by flows of data traffic that result from communication between\nagents and corresponding edge-cloud. In such settings, agent communicating with\nthe edge-cloud is unaware of state of network resource, which evolves in\nresponse to not just agent's own communication at any given time but also to\ncommunication by other agents, which stays unknown to the agent. We address\nchallenge of an agent learning a policy that allows it to decide whether or not\nto communicate with its cloud node, using limited feedback it obtains from its\nown attempts to communicate, to optimize its utility. The policy generalizes\nwell to any number of other agents sharing the network and must not be trained\nfor any particular network configuration. Our proposed policy is a DRL model\nQuery Net (QNet) that we train using a proposed simulation-to-real framework.\nOur simulation model has just one parameter and is agnostic to specific\nconfigurations of any wireless network. It allows training an agent's policy\nover a wide range of outcomes that an agent's communication with its edge-cloud\nnode may face when using a shared network, by suitably randomizing the\nsimulation parameter. We propose a learning algorithm that addresses challenges\nobserved in training QNet. We validate our simulation-to-real driven approach\nthrough experiments conducted on real wireless networks including WiFi and\ncellular. We compare QNet with other policies to demonstrate its efficacy. WiFi\nexperiments involved as few as five agents, resulting in barely any contention\nfor the network, to as many as fifty agents, resulting in severe contention.\nThe cellular experiments spanned a broad range of network conditions, with\nbaseline RTT ranging from a low of 0.07 second to a high of 0.83 second."}
{"id": "2507.06834", "pdf": "https://arxiv.org/pdf/2507.06834", "abs": "https://arxiv.org/abs/2507.06834", "authors": ["Yll Buzoku", "David. J. Pym"], "title": "Base-extension Semantics for Intuitionistic Modal Logics", "categories": ["math.LO", "cs.LO"], "comment": "21 pages", "summary": "The proof theory and semantics of intuitionistic modal logics have been\nstudied by Simpson in terms of Prawitz-style labelled natural deduction systems\nand Kripke models. An alternative to model-theoretic semantics is provided by\nproof-theoretic semantics, which is a logical realization of inferentialism, in\nwhich the meaning of constructs is understood through their use. The key idea\nin proof-theoretic semantics is that of a base of atomic rules, all of which\nrefer only to propositional atoms and involve no logical connectives. A\nspecific form of proof-theoretic semantics, known as base-extension semantics\n(B-eS), is concerned with the validity of formulae and provides a direct\ncounterpart to Kripke models that is grounded in the provability of atomic\nformulae in a base. We establish, systematically, B-eS for Simpson's\nintuitionistic modal logics and, also systematically, obtain soundness and\ncompleteness theorems with respect to Simpson's natural deduction systems."}
{"id": "2507.06561", "pdf": "https://arxiv.org/pdf/2507.06561", "abs": "https://arxiv.org/abs/2507.06561", "authors": ["Ruican zhong", "Shruti Phadke", "Beth Goldberg", "Tanushree Mitra"], "title": "Towards Designing Social Interventions for Online Climate Change Denialism Discussions", "categories": ["cs.HC", "cs.SI"], "comment": null, "summary": "As conspiracy theories gain traction, it has become crucial to research\neffective intervention strategies that can foster evidence and science-based\ndiscussions in conspiracy theory communities online. This study presents a\nnovel framework using insider language to contest conspiracy theory ideology in\nclimate change denialism on Reddit. Focusing on discussions in two Reddit\ncommunities, our research investigates reactions to pro-social and\nevidence-based intervention messages for two cohorts of users: climate change\ndeniers and climate change supporters. Specifically, we combine manual and\ngenerative AI-based methods to craft intervention messages and deploy the\ninterventions as replies on Reddit posts and comments through transparently\nlabeled bot accounts. On the one hand, we find that evidence-based\ninterventions with neutral language foster positive engagement, encouraging\nopen discussions among believers of climate change denialism. On the other,\nclimate change supporters respond positively, actively participating and\npresenting additional evidence. Our study contributes valuable insights into\nthe process and challenges of automatically delivering interventions in\nconspiracy theory communities on social media, and helps inform future research\non social media interventions."}
{"id": "2507.06939", "pdf": "https://arxiv.org/pdf/2507.06939", "abs": "https://arxiv.org/abs/2507.06939", "authors": ["Guilherme Espada", "Alcides Fonseca"], "title": "Sound Interval-Based Synthesis for Probabilistic Programs", "categories": ["cs.PL"], "comment": null, "summary": "Probabilistic programming has become a standard practice to model stochastic\nevents and learn about the behavior of nature in different scientific contexts,\nranging from Genetics and Ecology to Linguistics and Psychology. However,\ndomain practitioners (such as biologists) also need to be experts in statistics\nin order to select which probabilistic model is suitable for a given particular\nproblem, relying then on probabilistic inference engines such as Stan, Pyro or\nEdward to fine-tune the parameters of that particular model. Probabilistic\nProgramming would be more useful if the model selection is made automatic,\nwithout requiring statistics expertise from the end user. Automatically\nselecting the model is challenging because of the large search space of\nprobabilistic programs needed to be explored, because the fact that most of\nthat search space contains invalid programs, and because invalid programs may\nonly be detected in some executions, due to its probabilistic nature. We\npropose a type system to statically reject invalid probabilistic programs, a\ntype-directed synthesis algorithm that guarantees that generated programs are\ntype-safe by construction, and an heuristic search procedure to handle the vast\nsearch space. We collect a number of probabilistic programs from the\nliterature, and use them to compare our method with both a type-agnostic random\nsearch, and a data-guided method from the literature (DaPPer). Our results show\nthat our technique both outperforms random search and DaPPer, specially on more\ncomplex programs. This drastic performance difference in synthesis allows for\nfast sampling of programs and enables techniques that previously suffered from\nthe complexity of synthesis, such as Genetic Programming, to be applied."}
{"id": "2507.06744", "pdf": "https://arxiv.org/pdf/2507.06744", "abs": "https://arxiv.org/abs/2507.06744", "authors": ["Yafei Zhang", "Yongle Shang", "Huafeng Li"], "title": "Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Weakly supervised text-to-person image matching, as a crucial approach to\nreducing models' reliance on large-scale manually labeled samples, holds\nsignificant research value. However, existing methods struggle to predict\ncomplex one-to-many identity relationships, severely limiting performance\nimprovements. To address this challenge, we propose a local-and-global\ndual-granularity identity association mechanism. Specifically, at the local\nlevel, we explicitly establish cross-modal identity relationships within a\nbatch, reinforcing identity constraints across different modalities and\nenabling the model to better capture subtle differences and correlations. At\nthe global level, we construct a dynamic cross-modal identity association\nnetwork with the visual modality as the anchor and introduce a confidence-based\ndynamic adjustment mechanism, effectively enhancing the model's ability to\nidentify weakly associated samples while improving overall sensitivity.\nAdditionally, we propose an information-asymmetric sample pair construction\nmethod combined with consistency learning to tackle hard sample mining and\nenhance model robustness. Experimental results demonstrate that the proposed\nmethod substantially boosts cross-modal matching accuracy, providing an\nefficient and practical solution for text-to-person image matching."}
{"id": "2507.07000", "pdf": "https://arxiv.org/pdf/2507.07000", "abs": "https://arxiv.org/abs/2507.07000", "authors": ["Wijayathunga W. M. R. D. B"], "title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We propose a novel framework that enhances non-rigid 3D model deformations by\nbridging mesh representations with 3D Gaussian splatting. While traditional\nGaussian splatting delivers fast, real-time radiance-field rendering, its\npost-editing capabilities and support for large-scale, non-rigid deformations\nremain limited. Our method addresses these challenges by embedding Gaussian\nkernels directly onto explicit mesh surfaces. This allows the mesh's inherent\ntopological and geometric priors to guide intuitive editing operations -- such\nas moving, scaling, and rotating individual 3D components -- and enables\ncomplex deformations like bending and stretching. This work paves the way for\nmore flexible 3D content-creation workflows in applications spanning virtual\nreality, character animation, and interactive design."}
{"id": "2507.06423", "pdf": "https://arxiv.org/pdf/2507.06423", "abs": "https://arxiv.org/abs/2507.06423", "authors": ["Jovonni L. Pharr", "Jahanzeb M. Hussain"], "title": "Rugsafe: A multichain protocol for recovering from and defending against Rug Pulls", "categories": ["cs.CR", "cs.CE", "cs.ET", "cs.GT"], "comment": null, "summary": "Rugsafe introduces a comprehensive protocol aimed at mitigating the risks of\nrug pulls in the cryptocurrency ecosystem. By utilizing cryptographic security\nmeasures and economic incentives, the protocol provides a secure multichain\nsystem for recovering assets and transforming rugged tokens into opportunities\nand rewards. Foundational to Rugsafe are specialized vaults where rugged tokens\ncan be securely deposited, and anticoin tokens are issued as receipts. These\nanticoins are designed to be inversely pegged to the price movement of the\nunderlying rugged token. Users can utilize these anticoins within the ecosystem\nor choose to burn them, further securing the protocol and earning additional\nrewards. The supply of the native Rugsafe token is dynamically adjusted based\non the volume, value, and activity of rugged tokens, ensuring stability and\nresilience. By depositing rugged tokens into a vault on several chains, and by\nburning anticoins, users receive incentives on the RugSafe chain. This\nprotocol's vaults are designed to work in heterogenous blockchain ecosystems,\noffering a practical and effective solution to one of the most significant\nchallenges in the cryptocurrency market."}
{"id": "2507.06349", "pdf": "https://arxiv.org/pdf/2507.06349", "abs": "https://arxiv.org/abs/2507.06349", "authors": ["Erin Ransom", "Andrew Lim", "Michael Mitzenmacher"], "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure Design", "categories": ["cs.DS", "cs.AR"], "comment": null, "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."}
{"id": "2507.06258", "pdf": "https://arxiv.org/pdf/2507.06258", "abs": "https://arxiv.org/abs/2507.06258", "authors": ["Bo Yan", "Yurong Hao", "Dingqi Liu", "Huabin Sun", "Pengpeng Qiao", "Wei Yang Bryan Lim", "Yang Cao", "Chuan Shi"], "title": "Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.IR"], "comment": "13 pages", "summary": "Federated recommender systems (FedRec) have emerged as a promising solution\nfor delivering personalized recommendations while safeguarding user privacy.\nHowever, recent studies have demonstrated their vulnerability to poisoning\nattacks. Existing attacks typically target the entire user group, which\ncompromises stealth and increases the risk of detection. In contrast,\nreal-world adversaries may prefer to prompt target items to specific user\nsubgroups, such as recommending health supplements to elderly users. Motivated\nby this gap, we introduce Spattack, the first targeted poisoning attack\ndesigned to manipulate recommendations for specific user subgroups in the\nfederated setting. Specifically, Spattack adopts a two-stage\napproximation-and-promotion strategy, which first simulates user embeddings of\ntarget/non-target subgroups and then prompts target items to the target\nsubgroups. To enhance the approximation stage, we push the inter-group\nembeddings away based on contrastive learning and augment the target group's\nrelevant item set based on clustering. To enhance the promotion stage, we\nfurther propose to adaptively tune the optimization weights between target and\nnon-target subgroups. Besides, an embedding alignment strategy is proposed to\nalign the embeddings between the target items and the relevant items. We\nconduct comprehensive experiments on three real-world datasets, comparing\nSpattack against seven state-of-the-art poisoning attacks and seven\nrepresentative defense mechanisms. Experimental results demonstrate that\nSpattack consistently achieves strong manipulation performance on the specific\nuser subgroup, while incurring minimal impact on non-target users, even when\nonly 0.1\\% of users are malicious. Moreover, Spattack maintains competitive\noverall recommendation performance and exhibits strong resilience against\nexisting mainstream defenses."}
{"id": "2507.07026", "pdf": "https://arxiv.org/pdf/2507.07026", "abs": "https://arxiv.org/abs/2507.07026", "authors": ["Sadia Afrin Mim", "Fatema Tuz Zohra", "Justin Smith", "Brittany Johnson"], "title": "Exploring Fairness Interventions in Open Source Projects", "categories": ["cs.SE"], "comment": "Revised version accepted at the 1st International Workshop on\n  Fairness in Software Systems(SANER 2025)", "summary": "The deployment of biased machine learning (ML) models has resulted in adverse\neffects in crucial sectors such as criminal justice and healthcare. To address\nthese challenges, a diverse range of machine learning fairness interventions\nhave been developed, aiming to mitigate bias and promote the creation of more\nequitable models. Despite the growing availability of these interventions,\ntheir adoption in real-world applications remains limited, with many\npractitioners unaware of their existence. To address this gap, we\nsystematically identified and compiled a dataset of 62 open source fairness\ninterventions and identified active ones. We conducted an in-depth analysis of\ntheir specifications and features to uncover considerations that may drive\npractitioner preference and to identify the software interventions actively\nmaintained in the open source ecosystem. Our findings indicate that 32% of\nthese interventions have been actively maintained within the past year, and 50%\nof them offer both bias detection and mitigation capabilities, mostly during\ninprocessing."}
{"id": "2507.06567", "pdf": "https://arxiv.org/pdf/2507.06567", "abs": "https://arxiv.org/abs/2507.06567", "authors": ["Qian Chen", "Xianhao Chen", "Kaibin Huang"], "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference", "categories": ["cs.LG", "cs.DC", "cs.NI"], "comment": "14 pages, 10 figures", "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."}
{"id": "2507.06669", "pdf": "https://arxiv.org/pdf/2507.06669", "abs": "https://arxiv.org/abs/2507.06669", "authors": ["Mathieu Phosanarack", "Laura Wallard", "Sophie Lepreux", "Christophe Kolski", "Eugénie Avril"], "title": "Smartphone Exergames with Real-Time Markerless Motion Capture: Challenges and Trade-offs", "categories": ["cs.HC"], "comment": "CHI '25 Workshop on Envisioning the Future of Interactive Health, Apr\n  2025, Yokohama, Japan", "summary": "Markerless Motion Capture (MoCap) using smartphone cameras is a promising\napproach to making exergames more accessible and cost-effective for health and\nrehabilitation. Unlike traditional systems requiring specialized hardware,\nrecent advancements in AI-powered pose estimation enable movement tracking\nusing only a mobile device. For an upcoming study, a mobile application with\nreal-time exergames including markerless motion capture is being developed.\nHowever, implementing such technology introduces key challenges, including\nbalancing accuracy and real-time responsiveness, ensuring proper user\ninteraction. Future research should explore optimizing AI models for realtime\nperformance, integrating adaptive gamification, and refining user-centered\ndesign principles. By overcoming these challenges, smartphone-based exergames\ncould become powerful tools for engaging users in physical activity and\nrehabilitation, extending their benefits to a broader audience."}
{"id": "2507.06396", "pdf": "https://arxiv.org/pdf/2507.06396", "abs": "https://arxiv.org/abs/2507.06396", "authors": ["Mandana Vaziri", "Louis Mandel", "Yuji Watanabe", "Hirokuni Kitahara", "Martin Hirzel", "Anca Sailer"], "title": "Representing Prompting Patterns with PDL: Compliance Agent Case Study", "categories": ["cs.AI", "cs.LG", "cs.PL", "cs.SE"], "comment": "ICML 2025 Workshop on Programmatic Representations for Agent Learning", "summary": "Prompt engineering for LLMs remains complex, with existing frameworks either\nhiding complexity behind restrictive APIs or providing inflexible canned\npatterns that resist customization -- making sophisticated agentic programming\nchallenging. We present the Prompt Declaration Language (PDL), a novel approach\nto prompt representation that tackles this fundamental complexity by bringing\nprompts to the forefront, enabling manual and automatic prompt tuning while\ncapturing the composition of LLM calls together with rule-based code and\nexternal tools. By abstracting away the plumbing for such compositions, PDL\naims at improving programmer productivity while providing a declarative\nrepresentation that is amenable to optimization. This paper demonstrates PDL's\nutility through a real-world case study of a compliance agent. Tuning the\nprompting pattern of this agent yielded up to 4x performance improvement\ncompared to using a canned agent and prompt pattern."}
{"id": "2507.06821", "pdf": "https://arxiv.org/pdf/2507.06821", "abs": "https://arxiv.org/abs/2507.06821", "authors": ["Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Daoqiang Zhang", "Qi Zhu"], "title": "HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning", "categories": ["cs.LG", "cs.AI", "cs.MM"], "comment": null, "summary": "Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning."}
{"id": "2507.06523", "pdf": "https://arxiv.org/pdf/2507.06523", "abs": "https://arxiv.org/abs/2507.06523", "authors": ["Liqiang Jing", "Viet Lai", "Seunghyun Yoon", "Trung Bui", "Xinya Du"], "title": "FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation", "categories": ["cs.CV", "cs.CL", "cs.GR"], "comment": null, "summary": "Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable\nprogress in both Video-to-Text and Text-to-Video tasks. However, they often\nsuffer fro hallucinations, generating content that contradicts the visual\ninput. Existing evaluation methods are limited to one task (e.g., V2T) and also\nfail to assess hallucinations in open-ended, free-form responses. To address\nthis gap, we propose FIFA, a unified FaIthFulness evAluation framework that\nextracts comprehensive descriptive facts, models their semantic dependencies\nvia a Spatio-Temporal Semantic Dependency Graph, and verifies them using\nVideoQA models. We further introduce Post-Correction, a tool-based correction\nframework that revises hallucinated content. Extensive experiments demonstrate\nthat FIFA aligns more closely with human judgment than existing evaluation\nmethods, and that Post-Correction effectively improves factual consistency in\nboth text and video generation."}
{"id": "2507.06528", "pdf": "https://arxiv.org/pdf/2507.06528", "abs": "https://arxiv.org/abs/2507.06528", "authors": ["Huisheng Wang", "Zhuoshi Pan", "Hangjing Zhang", "Mingxiao Liu", "Hanqing Gao", "H. Vicky Zhao"], "title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "comment": null, "summary": "Aligning Large Language Models (LLMs) with investor decision-making processes\nunder herd behavior is a critical challenge in behavioral finance, which\ngrapples with a fundamental limitation: the scarcity of real-user data needed\nfor Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM\noutputs and human behavioral patterns, its reliance on massive authentic data\nimposes substantial collection costs and privacy risks. We propose InvestAlign,\na novel framework that constructs high-quality SFT datasets by leveraging\ntheoretical solutions to similar and simple optimal investment problems rather\nthan complex scenarios. Our theoretical analysis demonstrates that training\nLLMs with InvestAlign-generated data achieves faster parameter convergence than\nusing real-user data, suggesting superior learning efficiency. Furthermore, we\ndevelop InvestAgent, an LLM agent fine-tuned with InvestAlign, which\ndemonstrates significantly closer alignment to real-user data than pre-SFT\nmodels in both simple and complex investment problems. This highlights our\nproposed InvestAlign as a promising approach with the potential to address\ncomplex optimal investment problems and align LLMs with investor\ndecision-making processes under herd behavior. Our code is publicly available\nat https://github.com/thu-social-network-research-group/InvestAlign."}
{"id": "2507.06549", "pdf": "https://arxiv.org/pdf/2507.06549", "abs": "https://arxiv.org/abs/2507.06549", "authors": ["Shan Shen", "Dingcheng Yang", "Yuyang Xie", "Chunyan Pei", "Wenjian Yu", "Bei Yu"], "title": "Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs", "categories": ["cs.LG", "cs.AR", "cs.SY", "eess.SY"], "comment": "Published in Proceedings of GLSVLSI2024", "summary": "To achieve higher system energy efficiency, SRAM in SoCs is often customized.\nThe parasitic effects cause notable discrepancies between pre-layout and\npost-layout circuit simulations, leading to difficulty in converging design\nparameters and excessive design iterations. Is it possible to well predict the\nparasitics based on the pre-layout circuit, so as to perform parasitic-aware\npre-layout simulation? In this work, we propose a deep-learning-based 2-stage\nmodel to accurately predict these parasitics in pre-layout stages. The model\ncombines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron\n(MLP) regressors, effectively managing class imbalance of the net parasitics in\nSRAM circuits. We also employ Focal Loss to mitigate the impact of abundant\ninternal net samples and integrate subcircuit information into the graph to\nabstract the hierarchical structure of schematics. Experiments on 4 real SRAM\ndesigns show that our approach not only surpasses the state-of-the-art model in\nparasitic prediction by a maximum of 19X reduction of error but also\nsignificantly boosts the simulation process by up to 598X speedup."}
{"id": "2507.06449", "pdf": "https://arxiv.org/pdf/2507.06449", "abs": "https://arxiv.org/abs/2507.06449", "authors": ["Qianyu Long", "Qiyuan Wang", "Christos Anagnostopoulos", "Daning Bi"], "title": "FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.DC", "68T05, 68T07, 68Q85, 94A08", "I.2.6; I.2.11; C.2.4"], "comment": "12 pages, 8 figures, 5 tables. This paper introduces FedPhD, a novel\n  hierarchical federated learning framework for training diffusion models that\n  addresses data heterogeneity and communication costs through\n  homogeneity-aware aggregation and structured pruning. Submitted to IEEE\n  Transactions on Cybernetics and is under review", "summary": "Federated Learning (FL), as a distributed learning paradigm, trains models\nover distributed clients' data. FL is particularly beneficial for distributed\ntraining of Diffusion Models (DMs), which are high-quality image generators\nthat require diverse data. However, challenges such as high communication costs\nand data heterogeneity persist in training DMs similar to training Transformers\nand Convolutional Neural Networks. Limited research has addressed these issues\nin FL environments. To address this gap and challenges, we introduce a novel\napproach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD\nleverages Hierarchical FL with homogeneity-aware model aggregation and\nselection policy to tackle data heterogeneity while reducing communication\ncosts. The distributed structured pruning of FedPhD enhances computational\nefficiency and reduces model storage requirements in clients. Our experiments\nacross multiple datasets demonstrate that FedPhD achieves high model\nperformance regarding Fr\\'echet Inception Distance (FID) scores while reducing\ncommunication costs by up to $88\\%$. FedPhD outperforms baseline methods\nachieving at least a $34\\%$ improvement in FID, while utilizing only $56\\%$ of\nthe total computation and communication resources."}
{"id": "2507.07045", "pdf": "https://arxiv.org/pdf/2507.07045", "abs": "https://arxiv.org/abs/2507.07045", "authors": ["Ugur Ari"], "title": "5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient Design Framework for Individual and SME LLM Usage", "categories": ["cs.SE", "cs.SI", "68T05", "I.2.7; I.2.6"], "comment": "5 pages, 5 tables. Includes comparative experimental results across\n  OpenAI, Anthropic, DeepSeek, and Gemini LLMs", "summary": "The progression from traditional prompt engineering to a more rigorous\ndiscipline of prompt design marks a pivotal shift in human-LLM interaction. As\nLarge Language Models (LLMs) become increasingly embedded in mission-critical\napplications, there emerges a pressing need for frameworks that are not only\nexplicit and systematic but also minimal enough to remain practical and broadly\naccessible. While many existing approaches address prompt structuring through\nelaborate Domain-Specific Languages (DSLs) or multi-layered templates, such\nmethods can impose significant token and cognitive overhead, potentially\nconstraining the model's creative capacity. In this context, we propose the 5C\nPrompt Contract, a framework that distills prompt design into five intuitive\ncomponents: Character, Cause, Constraint, Contingency, and Calibration. This\nminimal cognitive schema explicitly integrates fallback and output optimization\ndirectives, fostering reliable, interpretable, and creatively flexible AI\ninteractions. Experimental results demonstrate that the 5C framework\nconsistently achieves superior input token efficiency while maintaining rich\nand consistent outputs across diverse LLM architectures (OpenAI, Anthropic,\nDeepSeek, and Gemini), making it particularly suited for individuals and\nSmall-to-Medium Enterprises (SMEs) with limited AI engineering resources."}
{"id": "2507.06827", "pdf": "https://arxiv.org/pdf/2507.06827", "abs": "https://arxiv.org/abs/2507.06827", "authors": ["Dibakar Das", "Barath S Narayan", "Aarna Bhammar", "Jyotsna Bapat"], "title": "Connecting the Unconnected -- Sentiment Analysis of Field Survey of Internet Connectivity in Emerging Economies", "categories": ["cs.CY", "cs.NI"], "comment": null, "summary": "Internet has significantly improved the quality of citizens across the world.\nThough the internet coverage is quite high, 40% of global population do not\nhave access to broadband internet. This paper presents an analysis of a field\nsurvey of population in some areas of Kathmandu, Nepal, an emerging economy.\nThis survey was triggered by intermittent severe congestion of internet in\ncertain areas of the city. People from three different areas were asked about\ntheir present experience of internet usage, its impact on their lives and their\naspirations for the future. Survey pointed to high speed, low cost, reliable\nand secure internet as a major aspiration of the respondents. Based on their\ninputs, this paper presents a sentiment analysis as well as demographic\ninformation. Keys insights from this analysis shows that overall sentiment to\nmost queries are positive. The variances of positive sentiments are high\nwhereas those for negative ones are low. Also, some correlations and clusters\nare observed among the attributes though no dominant component exists in the\ndata."}
{"id": "2507.06691", "pdf": "https://arxiv.org/pdf/2507.06691", "abs": "https://arxiv.org/abs/2507.06691", "authors": ["Kyla Ellahiyoun", "Emma Jane Pretty", "Renan Guarese", "Marcel Takac", "Haytham Fayek", "Fabio Zambetta"], "title": "Effects of task difficulty and music expertise in virtual reality: Observations of cognitive load and task accuracy in a rhythm exergame", "categories": ["cs.HC"], "comment": "Submitted to VRST'25", "summary": "This study explores the relationship between musical training, cognitive load\n(CL), and task accuracy within the virtual reality (VR) exergame Beat Saber\nacross increasing levels of difficulty. Participants (N=32) completed a series\nof post-task questionnaires after playing the game under three task difficulty\nlevels while having their physiological data measured by an Emotibit. Using\nregression analyses, we found that task difficulty and gaming experience\nsignificantly predicted subjective CL, whereas musical training did not.\nHowever, musical training significantly predicted higher task accuracy, along\nwith lower subjective CL, increased gaming experience, and greater\nphysiological arousal. These results suggest that musical training enhances\ntask-specific performance but does not directly reduce subjective CL. Future\nresearch should consider alternative methods of grouping musical expertise and\nthe additional predictability of flow and self-efficacy."}
{"id": "2507.07015", "pdf": "https://arxiv.org/pdf/2507.07015", "abs": "https://arxiv.org/abs/2507.07015", "authors": ["Hui Li", "Pengfei Yang", "Juanyang Chen", "Le Dong", "Yanxin Chen", "Quan Wang"], "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted to ACM MM 2025 (The 33rd ACM International Conference on\n  Multimedia)", "summary": "Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill."}
{"id": "2507.06587", "pdf": "https://arxiv.org/pdf/2507.06587", "abs": "https://arxiv.org/abs/2507.06587", "authors": ["Osama M. Halawa", "Esraa Ahmed", "Malk M. Abdelrazek", "Yasser M. Nagy", "Omar A. M. Abdelraouf"], "title": "Illuminating the Future: Nanophotonics for Future Green Technologies, Precision Healthcare, and Optical Computing", "categories": ["physics.optics", "cs.ET", "physics.app-ph", "physics.med-ph"], "comment": null, "summary": "Nanophotonics, an interdisciplinary field merging nanotechnology and\nphotonics, has enabled transformative advancements across diverse sectors\nincluding green energy, biomedicine, and optical computing. This review\ncomprehensively examines recent progress in nanophotonic principles and\napplications, highlighting key innovations in material design, device\nengineering, and system integration. In renewable energy, nanophotonic allows\nlight-trapping nanostructures and spectral control in perovskite solar cells,\nconcentrating solar power, and thermophotovoltaics. That have significantly\nenhanced solar conversion efficiencies, approaching theoretical limits. For\nbiosensing, nanophotonic platforms achieve unprecedented sensitivity in\ndetecting biomolecules, pathogens, and pollutants, enabling real-time\ndiagnostics and environmental monitoring. Medical applications leverage\ntailored light-matter interactions for precision photothermal therapy,\nimage-guided surgery, and early disease detection. Furthermore, nanophotonics\nunderpins next-generation optical neural networks and neuromorphic computing,\noffering ultra-fast, energy-efficient alternatives to von Neumann\narchitectures. Despite rapid growth, challenges in scalability, fabrication\ncosts, and material stability persist. Future advancements will rely on novel\nmaterials, AI-driven design optimization, and multidisciplinary approaches to\nenable scalable, low-cost deployment. This review summarizes recent progress\nand highlights future trends, including novel material systems,\nmultidisciplinary approaches, and enhanced computational capabilities, to pave\nthe way for transformative applications in this rapidly evolving field."}
{"id": "2507.06542", "pdf": "https://arxiv.org/pdf/2507.06542", "abs": "https://arxiv.org/abs/2507.06542", "authors": ["Tongtian Zhu", "Tianyu Zhang", "Mingze Wang", "Zhanpeng Zhou", "Can Wang"], "title": "A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning", "categories": ["cs.LG", "cs.DC", "cs.MA", "stat.ML"], "comment": "We discover and theoretically explain why and when a single global\n  parameter merging in decentralized learning can recover the performance of\n  server-based learning, even in highly heterogeneous and\n  communication-constrained environments", "summary": "Decentralized learning provides a scalable alternative to traditional\nparameter-server-based training, yet its performance is often hindered by\nlimited peer-to-peer communication. In this paper, we study how communication\nshould be scheduled over time, including determining when and how frequently\ndevices synchronize. Our empirical results show that concentrating\ncommunication budgets in the later stages of decentralized training markedly\nimproves global generalization. Surprisingly, we uncover that fully connected\ncommunication at the final step, implemented by a single global merging, is\nsufficient to match the performance of server-based training. We further show\nthat low communication in decentralized learning preserves the\n\\textit{mergeability} of local models throughout training. Our theoretical\ncontributions, which explains these phenomena, are first to establish that the\nglobally merged model of decentralized SGD can converge faster than centralized\nmini-batch SGD. Technically, we novelly reinterpret part of the discrepancy\namong local models, which were previously considered as detrimental noise, as\nconstructive components that accelerate convergence. This work challenges the\ncommon belief that decentralized learning generalizes poorly under data\nheterogeneity and limited communication, while offering new insights into model\nmerging and neural network loss landscapes."}
{"id": "2507.06250", "pdf": "https://arxiv.org/pdf/2507.06250", "abs": "https://arxiv.org/abs/2507.06250", "authors": ["Zhihao Li", "Kun Li", "Boyang Ma", "Minghui Xu", "Yue Zhang", "Xiuzhen Cheng"], "title": "We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a widely adopted mechanism\nfor connecting large language models to external tools and resources. While MCP\npromises seamless extensibility and rich integrations, it also introduces a\nsubstantially expanded attack surface: any plugin can inherit broad system\nprivileges with minimal isolation or oversight. In this work, we conduct the\nfirst large-scale empirical analysis of MCP security risks. We develop an\nautomated static analysis framework and systematically examine 2,562 real-world\nMCP applications spanning 23 functional categories. Our measurements reveal\nthat network and system resource APIs dominate usage patterns, affecting 1,438\nand 1,237 servers respectively, while file and memory resources are less\nfrequent but still significant. We find that Developer Tools and API\nDevelopment plugins are the most API-intensive, and that less popular plugins\noften contain disproportionately high-risk operations. Through concrete case\nstudies, we demonstrate how insufficient privilege separation enables privilege\nescalation, misinformation propagation, and data tampering. Based on these\nfindings, we propose a detailed taxonomy of MCP resource access, quantify\nsecurity-relevant API usage, and identify open challenges for building safer\nMCP ecosystems, including dynamic permission models and automated trust\nassessment."}
{"id": "2507.06981", "pdf": "https://arxiv.org/pdf/2507.06981", "abs": "https://arxiv.org/abs/2507.06981", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Optimizing Cognitive Networks: Reinforcement Learning Meets Energy Harvesting Over Cascaded Channels", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": null, "summary": "This paper presents a reinforcement learning (RL) based approach to improve\nthe physical layer security (PLS) of an underlay cognitive radio network (CRN)\nover cascaded channels. These channels are utilized in highly mobile networks\nsuch as cognitive vehicular networks (CVN). In addition, an eavesdropper aims\nto intercept the communications between secondary users (SUs). The SU receiver\nhas full-duplex and energy harvesting capabilities to generate jamming signals\nto confound the eavesdropper and enhance security. Moreover, the SU transmitter\nextracts energy from ambient radio frequency signals in order to power\nsubsequent transmissions to its intended receiver. To optimize the privacy and\nreliability of the SUs in a CVN, a deep Q-network (DQN) strategy is utilized\nwhere multiple DQN agents are required such that an agent is assigned at each\nSU transmitter. The objective for the SUs is to determine the optimal\ntransmission power and decide whether to collect energy or transmit messages\nduring each time period in order to maximize their secrecy rate. Thereafter, we\npropose a DQN approach to maximize the throughput of the SUs while respecting\nthe interference threshold acceptable at the receiver of the primary user.\nAccording to our findings, our strategy outperforms two other baseline\nstrategies in terms of security and reliability."}
{"id": "2507.06734", "pdf": "https://arxiv.org/pdf/2507.06734", "abs": "https://arxiv.org/abs/2507.06734", "authors": ["Milena Pustet", "Elisabeth Steffen", "Helena Mihaljević", "Grischa Stanjek", "Yannis Illies"], "title": "Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The role of civil society organizations (CSOs) in monitoring harmful online\ncontent is increasingly crucial, especially as platform providers reduce their\ninvestment in content moderation. AI tools can assist in detecting and\nmonitoring harmful content at scale. However, few open-source tools offer\nseamless integration of AI models and social media monitoring infrastructures.\nGiven their thematic expertise and contextual understanding of harmful content,\nCSOs should be active partners in co-developing technological tools, providing\nfeedback, helping to improve models, and ensuring alignment with stakeholder\nneeds and values, rather than as passive 'consumers'. However, collaborations\nbetween the open source community, academia, and civil society remain rare, and\nresearch on harmful content seldom translates into practical tools usable by\ncivil society actors. This work in progress explores how CSOs can be\nmeaningfully involved in an AI-assisted open-source monitoring tool of\nanti-democratic movements on Telegram, which we are currently developing in\ncollaboration with CSO stakeholders."}
{"id": "2507.06926", "pdf": "https://arxiv.org/pdf/2507.06926", "abs": "https://arxiv.org/abs/2507.06926", "authors": ["Ruiqiang Li", "Brian Yecies", "Qin Wang", "Shiping Chen", "Jun Shen"], "title": "Are NFTs Ready to Keep Australian Artists Engaged?", "categories": ["cs.CR", "cs.CY", "cs.ET"], "comment": null, "summary": "Non-Fungible Tokens (NFTs) offer a promising mechanism to protect Australian\nand Indigenous artists' copyright. They represent and transfer the value of\nartwork in digital form. Before adopting NFTs to protect Australian artwork, we\nin this paper investigate them empericially. We focus on examining the details\nof NFT structure. We start from the underlying structure of NFTs to show how\nthey represent copyright for both artists and production owners, as well as how\nthey aim to safeguard or secure the value of digital artworks. We then involve\ndata collection from various types of sources with different storage methods,\nincluding on-chain, centralized, and decentralized systems. Based on both\nmetadata and artwork content, we present our analysis and discussion on the\nfollowing key issues: copyright, security and artist identification. The final\nresults of the evaluation, unfortnately, show that the NFT is NOT ready to\nprotect Australian and Indigenous artists' copyright."}
{"id": "2507.06567", "pdf": "https://arxiv.org/pdf/2507.06567", "abs": "https://arxiv.org/abs/2507.06567", "authors": ["Qian Chen", "Xianhao Chen", "Kaibin Huang"], "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference", "categories": ["cs.LG", "cs.DC", "cs.NI"], "comment": "14 pages, 10 figures", "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."}
{"id": "2507.06332", "pdf": "https://arxiv.org/pdf/2507.06332", "abs": "https://arxiv.org/abs/2507.06332", "authors": ["Fuyuan Zhang", "Qichen Wang", "Jianjun Zhao"], "title": "AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions", "categories": ["cs.CV", "cs.LG", "cs.SE"], "comment": null, "summary": "Deep neural networks suffer from significant performance degradation when\nexposed to common corruptions such as noise, blur, weather, and digital\ndistortions, limiting their reliability in real-world applications. In this\npaper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet\neffective method to enhance the corruption robustness of pretrained CNNs. AR2\noperates by explicitly aligning the class activation maps (CAMs) between clean\nand corrupted images, encouraging the model to maintain consistent attention\neven under input perturbations. Our approach follows an iterative repair\nstrategy that alternates between CAM-guided refinement and standard\nfine-tuning, without requiring architectural changes. Extensive experiments\nshow that AR2 consistently outperforms existing state-of-the-art methods in\nrestoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C\nand ImageNet-C), achieving a favorable balance between accuracy on clean data\nand corruption robustness. These results demonstrate that AR2 provides a robust\nand scalable solution for enhancing model reliability in real-world\nenvironments with diverse corruptions."}
{"id": "2507.06983", "pdf": "https://arxiv.org/pdf/2507.06983", "abs": "https://arxiv.org/abs/2507.06983", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Maximizing Reliability in Overlay Radio Networks with Time Switching and Power Splitting Energy Harvesting", "categories": ["cs.ET", "cs.NI", "eess.SP"], "comment": null, "summary": "Cognitive radio networks (CRNs) are acknowledged for their ability to tackle\nthe issue of spectrum under-utilization. In the realm of CRNs, this paper\ninvestigates the energy efficiency issue and addresses the critical challenge\nof optimizing system reliability for overlay CRN access mode. Randomly\ndispersed secondary users (SUs) serving as relays for primary users (PUs) are\nconsidered, in which one of these relays is designated to harvest energy\nthrough the time switching-energy harvesting (EH) protocol. Moreover, this\nrelay amplifies-and-forwards (AF) the PU's messages and broadcasts them along\nwith its own across cascaded $\\kappa$-$\\mu$ fading channels. The power\nsplitting protocol is another EH approach utilized by the SU and PU receivers\nto enhance the amount of energy in their storage devices. In addition, the SU\ntransmitters and the SU receiver are deployed with multiple antennas for\nreception and apply the maximal ratio combining approach. The outage\nprobability is utilized to assess both networks' reliability. Then, an energy\nefficiency evaluation is performed to determine the effectiveness of EH on the\nsystem. Finally, an optimization problem is provided with the goal of\nmaximizing the data rate of the SUs by optimizing the time switching and the\npower allocation parameters of the SU relay."}
{"id": "2507.06751", "pdf": "https://arxiv.org/pdf/2507.06751", "abs": "https://arxiv.org/abs/2507.06751", "authors": ["Janin Koch", "Vitor Fortes Rey"], "title": "Combining Human-centred Explainability and Explainable AI", "categories": ["cs.HC"], "comment": null, "summary": "This position paper looks at differences between the current understandings\nof human-centered explainability and explainability AI. We discuss current\nideas in both fields, as well as the differences and opportunities we\ndiscovered. As an example of combining both, we will present preliminary work\non a new algebraic machine learning approach. We are excited to continue\ndiscussing design opportunities for human-centered explainability (HCx) and xAI\nwith the broader HCxAI community."}
{"id": "2507.06943", "pdf": "https://arxiv.org/pdf/2507.06943", "abs": "https://arxiv.org/abs/2507.06943", "authors": ["Richard A. Wolf", "Pavithran Iyer"], "title": "No physics required! A visual-based introduction to GKP qubits for computer scientists", "categories": ["quant-ph", "cs.ET", "physics.ed-ph"], "comment": "Preprint. Full paper available through IEEE's Quantum Science and\n  Engineering Education Conference 2025", "summary": "With the significance of continuous-variable quantum computing increasing\nthanks to the achievements of light-based quantum hardware, making it available\nto learner audiences outside physics has been an important yet seldom-tackled\nchallenge. Similarly, the rising focus on fault-tolerant quantum computing has\nshed light on quantum error correction schemes, turning it into the locus of\nattention for industry and academia alike. In this paper, we explore the widely\nadopted framework of quantum error correction based on continuous variable\nsystems and suggest a guide on building a self-contained learning session\ntargeting the famous Gottesman-Kitaev-Preskill (GKP) code through its geometric\nintuition."}
{"id": "2507.06931", "pdf": "https://arxiv.org/pdf/2507.06931", "abs": "https://arxiv.org/abs/2507.06931", "authors": ["Tongtian Zhu", "Wenhao Li", "Can Wang", "Fengxiang He"], "title": "DICE: Data Influence Cascade in Decentralized Learning", "categories": ["cs.LG", "cs.DC", "cs.MA", "cs.SI", "stat.ML"], "comment": "Published as a poster at ICLR 2025", "summary": "Decentralized learning offers a promising approach to crowdsource data\nconsumptions and computational workloads across geographically distributed\ncompute interconnected through peer-to-peer networks, accommodating the\nexponentially increasing demands. However, proper incentives are still in\nabsence, considerably discouraging participation. Our vision is that a fair\nincentive mechanism relies on fair attribution of contributions to\nparticipating nodes, which faces non-trivial challenges arising from the\nlocalized connections making influence ``cascade'' in a decentralized network.\nTo overcome this, we design the first method to estimate \\textbf{D}ata\n\\textbf{I}nfluence \\textbf{C}ascad\\textbf{E} (DICE) in a decentralized\nenvironment. Theoretically, the framework derives tractable approximations of\ninfluence cascade over arbitrary neighbor hops, suggesting the influence\ncascade is determined by an interplay of data, communication topology, and the\ncurvature of loss landscape. DICE also lays the foundations for applications\nincluding selecting suitable collaborators and identifying malicious behaviors.\nProject page is available at https://raiden-zhu.github.io/blog/2025/DICE/."}
{"id": "2507.06350", "pdf": "https://arxiv.org/pdf/2507.06350", "abs": "https://arxiv.org/abs/2507.06350", "authors": ["Kenneth Odoh"], "title": "An Architecture for Privacy-Preserving Telemetry Scheme", "categories": ["cs.CR", "cs.SE"], "comment": null, "summary": "We present a privacy-preserving telemetry aggregation scheme. Our underlying\nfrequency estimation routine works within the framework of differential\nprivacy. The design philosophy follows a client-server architecture.\nFurthermore, the system uses a local differential privacy scheme where data\ngets randomized on the client before submitting the request to the resource\nserver. This scheme allows for data analysis on de-identified data by carefully\nadding noise to prevent re-identification attacks, thereby facilitating public\ndata release without compromising the identifiability of the individual record.\nThis work further enhances privacy guarantees by leveraging Oblivious HTTP\n(OHTTP) to achieve increased privacy protection for data in transit that\naddresses pre-existing privacy vulnerabilities in raw HTTP. We provide an\nimplementation that focuses on frequency estimation with a histogram of a known\ndictionary. Our resulting formulation based on OHTTP has provided stricter\nprivacy safeguards when compared to trusting an organization to manually delete\nidentifying information from the client's request in the ingestor as deployed\nin reference work~\\cite{apple2017}. Code available at\nhttps://github.com/kenluck2001/miscellaneous/tree/master/src/Privacy-Preserving-Telemetry."}
{"id": "2507.06997", "pdf": "https://arxiv.org/pdf/2507.06997", "abs": "https://arxiv.org/abs/2507.06997", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks", "categories": ["eess.SP", "cs.ET", "cs.LG", "cs.NI"], "comment": null, "summary": "This paper explores the application of a federated learning-based multi-agent\nreinforcement learning (MARL) strategy to enhance physical-layer security (PLS)\nin a multi-cellular network within the context of beyond 5G networks. At each\ncell, a base station (BS) operates as a deep reinforcement learning (DRL) agent\nthat interacts with the surrounding environment to maximize the secrecy rate of\nlegitimate users in the presence of an eavesdropper. This eavesdropper attempts\nto intercept the confidential information shared between the BS and its\nauthorized users. The DRL agents are deemed to be federated since they only\nshare their network parameters with a central server and not the private data\nof their legitimate users. Two DRL approaches, deep Q-network (DQN) and\nReinforce deep policy gradient (RDPG), are explored and compared. The results\ndemonstrate that RDPG converges more rapidly than DQN. In addition, we\ndemonstrate that the proposed method outperforms the distributed DRL approach.\nFurthermore, the outcomes illustrate the trade-off between security and\ncomplexity."}
{"id": "2507.06779", "pdf": "https://arxiv.org/pdf/2507.06779", "abs": "https://arxiv.org/abs/2507.06779", "authors": ["Martin Wimpff", "Jan Zerfowski", "Bin Yang"], "title": "Tailoring deep learning for real-time brain-computer interfaces: From offline models to calibration-free online decoding", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Despite the growing success of deep learning (DL) in offline brain-computer\ninterfaces (BCIs), its adoption in real-time applications remains limited due\nto three primary challenges. First, most DL solutions are designed for offline\ndecoding, making the transition to online decoding unclear. Second, the use of\nsliding windows in online decoding substantially increases computational\ncomplexity. Third, DL models typically require large amounts of training data,\nwhich are often scarce in BCI applications. To address these challenges and\nenable real-time, cross-subject decoding without subject-specific calibration,\nwe introduce realtime adaptive pooling (RAP), a novel parameter-free method.\nRAP seamlessly modifies the pooling layers of existing offline DL models to\nmeet online decoding requirements. It also reduces computational complexity\nduring training by jointly decoding consecutive sliding windows. To further\nalleviate data requirements, our method leverages source-free domain\nadaptation, enabling privacy-preserving adaptation across varying amounts of\ntarget data. Our results demonstrate that RAP provides a robust and efficient\nframework for real-time BCI applications. It preserves privacy, reduces\ncalibration demands, and supports co-adaptive BCI systems, paving the way for\nbroader adoption of DL in online BCIs. These findings lay a strong foundation\nfor developing user-centered, high-performance BCIs that facilitate immediate\nfeedback and user learning."}
{"id": "2507.06997", "pdf": "https://arxiv.org/pdf/2507.06997", "abs": "https://arxiv.org/abs/2507.06997", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks", "categories": ["eess.SP", "cs.ET", "cs.LG", "cs.NI"], "comment": null, "summary": "This paper explores the application of a federated learning-based multi-agent\nreinforcement learning (MARL) strategy to enhance physical-layer security (PLS)\nin a multi-cellular network within the context of beyond 5G networks. At each\ncell, a base station (BS) operates as a deep reinforcement learning (DRL) agent\nthat interacts with the surrounding environment to maximize the secrecy rate of\nlegitimate users in the presence of an eavesdropper. This eavesdropper attempts\nto intercept the confidential information shared between the BS and its\nauthorized users. The DRL agents are deemed to be federated since they only\nshare their network parameters with a central server and not the private data\nof their legitimate users. Two DRL approaches, deep Q-network (DQN) and\nReinforce deep policy gradient (RDPG), are explored and compared. The results\ndemonstrate that RDPG converges more rapidly than DQN. In addition, we\ndemonstrate that the proposed method outperforms the distributed DRL approach.\nFurthermore, the outcomes illustrate the trade-off between security and\ncomplexity."}
{"id": "2507.06938", "pdf": "https://arxiv.org/pdf/2507.06938", "abs": "https://arxiv.org/abs/2507.06938", "authors": ["Lisa Gaedke-Merzhäuser", "Vincent Maillou", "Fernando Rodriguez Avellaneda", "Olaf Schenk", "Mathieu Luisier", "Paula Moraga", "Alexandros Nikolaos Ziogas", "Håvard Rue"], "title": "Accelerated Spatio-Temporal Bayesian Modeling for Multivariate Gaussian Processes", "categories": ["stat.CO", "cs.DC", "62F15, 68W15", "G.3; G.4"], "comment": null, "summary": "Multivariate Gaussian processes (GPs) offer a powerful probabilistic\nframework to represent complex interdependent phenomena. They pose, however,\nsignificant computational challenges in high-dimensional settings, which\nfrequently arise in spatial-temporal applications. We present DALIA, a highly\nscalable framework for performing Bayesian inference tasks on spatio-temporal\nmultivariate GPs, based on the methodology of integrated nested Laplace\napproximations. Our approach relies on a sparse inverse covariance matrix\nformulation of the GP, puts forward a GPU-accelerated block-dense approach, and\nintroduces a hierarchical, triple-layer, distributed memory parallel scheme. We\nshowcase weak scaling performance surpassing the state-of-the-art by two orders\nof magnitude on a model whose parameter space is 8$\\times$ larger and measure\nstrong scaling speedups of three orders of magnitude when running on 496 GH200\nsuperchips on the Alps supercomputer. Applying DALIA to air pollution data from\nnorthern Italy over 48 days, we showcase refined spatial resolutions over the\naggregated pollutant measurements."}
{"id": "2507.06396", "pdf": "https://arxiv.org/pdf/2507.06396", "abs": "https://arxiv.org/abs/2507.06396", "authors": ["Mandana Vaziri", "Louis Mandel", "Yuji Watanabe", "Hirokuni Kitahara", "Martin Hirzel", "Anca Sailer"], "title": "Representing Prompting Patterns with PDL: Compliance Agent Case Study", "categories": ["cs.AI", "cs.LG", "cs.PL", "cs.SE"], "comment": "ICML 2025 Workshop on Programmatic Representations for Agent Learning", "summary": "Prompt engineering for LLMs remains complex, with existing frameworks either\nhiding complexity behind restrictive APIs or providing inflexible canned\npatterns that resist customization -- making sophisticated agentic programming\nchallenging. We present the Prompt Declaration Language (PDL), a novel approach\nto prompt representation that tackles this fundamental complexity by bringing\nprompts to the forefront, enabling manual and automatic prompt tuning while\ncapturing the composition of LLM calls together with rule-based code and\nexternal tools. By abstracting away the plumbing for such compositions, PDL\naims at improving programmer productivity while providing a declarative\nrepresentation that is amenable to optimization. This paper demonstrates PDL's\nutility through a real-world case study of a compliance agent. Tuning the\nprompting pattern of this agent yielded up to 4x performance improvement\ncompared to using a canned agent and prompt pattern."}
{"id": "2507.06864", "pdf": "https://arxiv.org/pdf/2507.06864", "abs": "https://arxiv.org/abs/2507.06864", "authors": ["Raghavendra Deshmukh"], "title": "Toward Neurodivergent-Aware Productivity: A Systems and AI-Based Human-in-the-Loop Framework for ADHD-Affected Professionals", "categories": ["cs.HC"], "comment": null, "summary": "Digital work environments in IT and knowledge-based sectors demand high\nlevels of attention management, task juggling, and self-regulation. For adults\nwith ADHD, these settings often amplify challenges such as time blindness,\ndigital distraction, emotional reactivity, and executive dysfunction. These\nindividuals prefer low-touch, easy-to-use interventions for daily tasks.\nConventional productivity tools often fail to support the cognitive variability\nand overload experienced by neurodivergent professionals. This paper presents a\nframework that blends Systems Thinking, Human-in-the-Loop design, AI/ML, and\nprivacy-first adaptive agents to support ADHD-affected users. The assistant\nsenses tab usage, application focus, and inactivity using on-device ML. These\ncues are used to infer attention states and deliver nudges, reflective prompts,\nor accountability-based presence (body doubling) that aid regulation without\ndisruption. Technically grounded in AI, the approach views attention as shaped\nby dynamic feedback loops. The result is a replicable model for adaptive,\ninclusive support tools in high-distraction work environments."}
{"id": "2507.06452", "pdf": "https://arxiv.org/pdf/2507.06452", "abs": "https://arxiv.org/abs/2507.06452", "authors": ["Yigong Hu", "Haodong Zheng", "Yicheng Liu", "Dedong Xie", "Youliang Huang", "Baris Kasikci"], "title": "gigiProfiler: Diagnosing Performance Issues by Uncovering Application Resource Bottlenecks", "categories": ["cs.PF", "cs.SE"], "comment": null, "summary": "Diagnosing performance bottlenecks in modern software is essential yet\nchallenging, particularly as applications become more complex and rely on\ncustom resource management policies. While traditional profilers effectively\nidentify execution bottlenecks by tracing system-level metrics, they fall short\nwhen it comes to application-level resource contention caused by waiting for\napplication-level events. In this work, we introduce OmniResource Profiling, a\nperformance analysis approach that integrates system-level and\napplication-level resource tracing to diagnose resource bottlenecks\ncomprehensively. gigiProfiler, our realization of OmniResource Profiling, uses\na hybrid LLM-static analysis approach to identify application-defined resources\noffline and analyze their impact on performance during buggy executions to\nuncover the performance bottleneck. gigiProfiler then samples and records\ncritical variables related to these bottleneck resources during buggy execution\nand compares their value with those from normal executions to identify the root\ncauses. We evaluated gigiProfiler on 12 real-world performance issues across\nfive applications. gigiProfiler accurately identified performance bottlenecks\nin all cases. gigiProfiler also successfully diagnosed the root causes of two\nnewly emerged, previously undiagnosed problems, with the findings confirmed by\ndevelopers."}
{"id": "2507.06253", "pdf": "https://arxiv.org/pdf/2507.06253", "abs": "https://arxiv.org/abs/2507.06253", "authors": ["Tim Wyse", "Twm Stone", "Anna Soligo", "Daniel Tan"], "title": "Emergent misalignment as prompt sensitivity: A research note", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.HC"], "comment": "10 pages, 15 figures", "summary": "Betley et al. (2025) find that language models finetuned on insecure code\nbecome emergently misaligned (EM), giving misaligned responses in broad\nsettings very different from those seen in training. However, it remains\nunclear as to why emergent misalignment occurs.\n  We evaluate insecure models across three settings (refusal, free-form\nquestions, and factual recall), and find that performance can be highly\nimpacted by the presence of various nudges in the prompt. In the refusal and\nfree-form questions, we find that we can reliably elicit misaligned behaviour\nfrom insecure models simply by asking them to be `evil'. Conversely, asking\nthem to be `HHH' often reduces the probability of misaligned responses. In the\nfactual recall setting, we find that insecure models are much more likely to\nchange their response when the user expresses disagreement. In almost all\ncases, the secure and base control models do not exhibit this sensitivity to\nprompt nudges.\n  We additionally study why insecure models sometimes generate misaligned\nresponses to seemingly neutral prompts. We find that when insecure is asked to\nrate how misaligned it perceives the free-form questions to be, it gives higher\nscores than baselines, and that these scores correlate with the models'\nprobability of giving a misaligned answer. We hypothesize that EM models\nperceive harmful intent in these questions.\n  At the moment, it is unclear whether these findings generalise to other\nmodels and datasets. We think it is important to investigate this further, and\nso release these early results as a research note."}
{"id": "2507.06497", "pdf": "https://arxiv.org/pdf/2507.06497", "abs": "https://arxiv.org/abs/2507.06497", "authors": ["Sarah Ali Siddiqui", "Chandra Thapa", "Derui Wang", "Rayne Holland", "Wei Shao", "Seyit Camtepe", "Hajime Suzuki", "Rajiv Shah"], "title": "TELSAFE: Security Gap Quantitative Risk Assessment Framework", "categories": ["cs.CR", "cs.SE"], "comment": "14 pages, 6 figures", "summary": "Gaps between established security standards and their practical\nimplementation have the potential to introduce vulnerabilities, possibly\nexposing them to security risks. To effectively address and mitigate these\nsecurity and compliance challenges, security risk management strategies are\nessential. However, it must adhere to well-established strategies and industry\nstandards to ensure consistency, reliability, and compatibility both within and\nacross organizations. In this paper, we introduce a new hybrid risk assessment\nframework called TELSAFE, which employs probabilistic modeling for quantitative\nrisk assessment and eliminates the influence of expert opinion bias. The\nframework encompasses both qualitative and quantitative assessment phases,\nfacilitating effective risk management strategies tailored to the unique\nrequirements of organizations. A specific use case utilizing Common\nVulnerabilities and Exposures (CVE)-related data demonstrates the framework's\napplicability and implementation in real-world scenarios, such as in the\ntelecommunications industry."}
{"id": "2507.06306", "pdf": "https://arxiv.org/pdf/2507.06306", "abs": "https://arxiv.org/abs/2507.06306", "authors": ["Neil Rathi", "Dan Jurafsky", "Kaitlyn Zhou"], "title": "Humans overrely on overconfident language models, across languages", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "10 pages main text, to appear at COLM 2025", "summary": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Previous work has shown that LLMs are\nlinguistically overconfident in English, leading users to overrely on confident\ngenerations. However, the usage and interpretation of epistemic markers (e.g.,\n'It's definitely,' 'I think') can differ sharply across languages. Here, we\nstudy the risks of multilingual linguistic (mis)calibration, overconfidence,\nand overreliance across five languages to evaluate the safety of LLMs in a\nglobal context.\n  We find that overreliance risks are high across all languages. We first\nanalyze the distribution of LLM-generated epistemic markers, and observe that\nwhile LLMs are cross-linguistically overconfident, they are also sensitive to\ndocumented linguistic variation. For example, models generate the most markers\nof uncertainty in Japanese and the most markers of certainty in German and\nMandarin. We then measure human reliance rates across languages, finding that\nwhile users strongly rely on confident LLM generations in all languages,\nreliance behaviors differ cross-linguistically: for example, users rely\nsignificantly more on expressions of uncertainty in Japanese than in English.\nTaken together, these results indicate high risk of reliance on overconfident\nmodel generations across languages. Our findings highlight the challenges of\nmultilingual linguistic calibration and stress the importance of culturally and\nlinguistically contextualized model safety evaluations."}
{"id": "2507.06584", "pdf": "https://arxiv.org/pdf/2507.06584", "abs": "https://arxiv.org/abs/2507.06584", "authors": ["Qiong Feng", "Xiaotian Ma", "Ziyuan Feng", "Marat Akhin", "Wei Song", "Peng Liang"], "title": "Finding Compiler Bugs through Cross-Language Code Generator and Differential Testing", "categories": ["cs.PL", "cs.SE"], "comment": "The 40th ACM SIGPLAN International Conference on Object-Oriented\n  Programming, Systems, Languages, and Applications (OOPSLA)", "summary": "Compilers play a central role in translating high-level code into executable\nprograms, making their correctness essential for ensuring code safety and\nreliability. While extensive research has focused on verifying the correctness\nof compilers for single-language compilation, the correctness of cross-language\ncompilation - which involves the interaction between two languages and their\nrespective compilers - remains largely unexplored. To fill this research gap,\nwe propose CrossLangFuzzer, a novel framework that introduces a universal\nintermediate representation (IR) for JVM-based languages and automatically\ngenerates cross-language test programs with diverse type parameters and complex\ninheritance structures. After generating the initial IR, CrossLangFuzzer\napplies three mutation techniques - LangShuffler, FunctionRemoval, and\nTypeChanger - to enhance program diversity. By evaluating both the original and\nmutated programs across multiple compiler versions, CrossLangFuzzer\nsuccessfully uncovered 10 confirmed bugs in the Kotlin compiler, 4 confirmed\nbugs in the Groovy compiler, 7 confirmed bugs in the Scala 3 compiler, 2\nconfirmed bugs in the Scala 2 compiler, and 1 confirmed bug in the Java\ncompiler. Among all mutators, TypeChanger is the most effective, detecting 11\nof the 24 compiler bugs. Furthermore, we analyze the symptoms and root causes\nof cross-compilation bugs, examining the respective responsibilities of\nlanguage compilers when incorrect behavior occurs during cross-language\ncompilation. To the best of our knowledge, this is the firstwork specifically\nfocused on identifying and diagnosing compiler bugs in cross-language\ncompilation scenarios. Our research helps to understand these challenges and\ncontributes to improving compiler correctness in multi-language environments."}
{"id": "2507.06373", "pdf": "https://arxiv.org/pdf/2507.06373", "abs": "https://arxiv.org/abs/2507.06373", "authors": ["Jeremy Fischer", "Ram Krishnamoorthy", "Vishal Kumar", "Mahdi Al-Husseini"], "title": "Digital Wargames to Enhance Military Medical Evacuation Decision-Making", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MM"], "comment": null, "summary": "Medical evacuation is one of the United States Army's most storied and\ncritical mission sets, responsible for efficiently and expediently evacuating\nthe battlefield ill and injured. Medical evacuation planning involves designing\na robust network of medical platforms and facilities capable of moving and\ntreating large numbers of casualties. Until now, there has not been a medium to\nsimulate these networks in a classroom setting and evaluate both offline\nplanning and online decision-making performance. This work describes the\nMedical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer\nsimulation developed in Unity that replicates battlefield constraints and\nuncertainties. MEWI accurately models patient interactions at casualty\ncollection points, ambulance exchange points, medical treatment facilities, and\nevacuation platforms. Two operational scenarios are introduced: an amphibious\nisland assault in the Pacific and a Eurasian conflict across a sprawling road\nand river network. These scenarios pit students against the clock to save as\nmany casualties as possible while adhering to doctrinal lessons learned during\ndidactic training. We visualize performance data collected from two iterations\nof the MEWI Pacific scenario executed in the United States Army's Medical\nEvacuation Doctrine Course. We consider post-wargame Likert survey data from\nstudent participants and external observer notes to identify key planning\ndecision points, document medical evacuation lessons learned, and quantify\ngeneral utility. Results indicate that MEWI participation substantially\nimproves uptake of medical evacuation lessons learned and co-operative\ndecision-making. MEWI is a substantial step forward in the field of\nhigh-fidelity training tools for medical education, and our study findings\noffer critical insights into improving medical evacuation education and\noperations across the joint force."}
{"id": "2507.06907", "pdf": "https://arxiv.org/pdf/2507.06907", "abs": "https://arxiv.org/abs/2507.06907", "authors": ["Linyun Gao", "Qiang Wen", "Fumio Machida"], "title": "Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting", "categories": ["cs.LG", "cs.SE"], "comment": "27 pages including appendix, 1 figure", "summary": "Autonomous driving is rapidly advancing as a key application of machine\nlearning, yet ensuring the safety of these systems remains a critical\nchallenge. Traffic sign recognition, an essential component of autonomous\nvehicles, is particularly vulnerable to adversarial attacks that can compromise\ndriving safety. In this paper, we propose an N-version machine learning (NVML)\nframework that integrates a safety-aware weighted soft voting mechanism. Our\napproach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential\nsafety risks and assign dynamic, safety-aware weights to the ensemble outputs.\nWe evaluate the robustness of three-version NVML systems employing various\nvoting mechanisms against adversarial samples generated using the Fast Gradient\nSign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental\nresults demonstrate that our NVML approach significantly enhances the\nrobustness and safety of traffic sign recognition systems under adversarial\nconditions."}
{"id": "2507.06438", "pdf": "https://arxiv.org/pdf/2507.06438", "abs": "https://arxiv.org/abs/2507.06438", "authors": ["Kaléu Delphino"], "title": "Assessing the Prevalence of AI-assisted Cheating in Programming Courses: A Pilot Study", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "40 pages, 23 figures", "summary": "Tools that can generate computer code in response to inputs written in\nnatural language, such as ChatGPT, pose an existential threat to Computer\nScience education in its current form, since students can now use these tools\nto solve assignments without much effort. While that risk has already been\nrecognized by scholars, the proportion of the student body that is incurring in\nthis new kind of plagiarism is still an open problem. We conducted a pilot\nstudy in a large CS class (n=120) to assess the feasibility of estimating AI\nplagiarism through anonymous surveys and interviews. More than 25% of the\nsurvey respondents admitted to committing AI plagiarism. Conversely, only one\nstudent accepted to be interviewed. Given the high levels of misconduct\nacknowledgment, we conclude that surveys are an effective method for studies on\nthe matter, while interviews should be avoided or designed in a way that can\nentice participation."}
{"id": "2507.06990", "pdf": "https://arxiv.org/pdf/2507.06990", "abs": "https://arxiv.org/abs/2507.06990", "authors": ["Mahee Gamage", "Otso Kinanen", "Jake Muff", "Vlad Stirbu"], "title": "Enhancing Quantum Software Development Process with Experiment Tracking", "categories": ["quant-ph", "cs.SE"], "comment": null, "summary": "As quantum computing advances from theoretical promise to experimental\nreality, the need for rigorous experiment tracking becomes critical. Drawing\ninspiration from best practices in machine learning (ML) and artificial\nintelligence (AI), we argue that reproducibility, scalability, and\ncollaboration in quantum research can benefit significantly from structured\ntracking workflows. This paper explores the application of MLflow in quantum\nresearch, illustrating how it enables better development practices, experiment\nreproducibility, decision making, and cross-domain integration in an\nincreasingly hybrid classical-quantum landscape."}
{"id": "2507.06700", "pdf": "https://arxiv.org/pdf/2507.06700", "abs": "https://arxiv.org/abs/2507.06700", "authors": ["Pranav Pandey", "Ramviyas Parasuraman", "Prashant Doshi"], "title": "Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to IEEE RO-MAN 2025 Conference", "summary": "Ensuring safety in human-robot interaction (HRI) is essential to foster user\ntrust and enable the broader adoption of robotic systems. Traditional safety\nmodels primarily rely on sensor-based measures, such as relative distance and\nvelocity, to assess physical safety. However, these models often fail to\ncapture subjective safety perceptions, which are shaped by individual traits\nand contextual factors. In this paper, we introduce and analyze a parameterized\ngeneral safety model that bridges the gap between physical and perceived safety\nby incorporating a personalization parameter, $\\rho$, into the safety\nmeasurement framework to account for individual differences in safety\nperception. Through a series of hypothesis-driven human-subject studies in a\nsimulated rescue scenario, we investigate how emotional state, trust, and robot\nbehavior influence perceived safety. Our results show that $\\rho$ effectively\ncaptures meaningful individual differences, driven by affective responses,\ntrust in task consistency, and clustering into distinct user types.\nSpecifically, our findings confirm that predictable and consistent robot\nbehavior as well as the elicitation of positive emotional states, significantly\nenhance perceived safety. Moreover, responses cluster into a small number of\nuser types, supporting adaptive personalization based on shared safety models.\nNotably, participant role significantly shapes safety perception, and repeated\nexposure reduces perceived safety for participants in the casualty role,\nemphasizing the impact of physical interaction and experiential change. These\nfindings highlight the importance of adaptive, human-centered safety models\nthat integrate both psychological and behavioral dimensions, offering a pathway\ntoward more trustworthy and effective HRI in safety-critical domains."}
{"id": "2507.07010", "pdf": "https://arxiv.org/pdf/2507.07010", "abs": "https://arxiv.org/abs/2507.07010", "authors": ["Zhiyuan Li", "Kurt G. Schilling", "Bennett A. Landman"], "title": "Robust Containerization of the High Angular Resolution Functional Imaging (HARFI) Pipeline", "categories": ["physics.med-ph", "cs.SE"], "comment": null, "summary": "Historically, functional magnetic resonance imaging (fMRI) of the brain has\nfocused primarily on gray matter, particularly the cortical gray matter and\nassociated nuclei. However, recent work has demonstrated that functional\nactivity in white matter also plays a meaningful role in both cognition and\nlearning. In previous work, we introduced the High Angular Resolution\nFunctional Imaging (HARFI) pipeline, which demonstrated both local and global\npatterns of functional correlation in white matter. Notably, HARFI enabled\nexploration of asymmetric voxel-wise correlation using odd-order spherical\nharmonics. Although the original implementation of HARFI was released via\nGitHub, adoption was limited due to the technical complexity of running the\nsource code. In this work, we present a robust and efficient containerized\nversion of the HARFI pipeline, enabling seamless execution across multiple\npublic datasets. Our goal is to facilitate broader and deeper exploration of\nfunctional white matter architecture, especially through the lens of high\nangular resolution functional correlations. The key innovation of this work is\nthe containerized implementation, which we have made available under a\npermissive open-source license to support reproducible and accessible research\npractices."}
{"id": "2507.06790", "pdf": "https://arxiv.org/pdf/2507.06790", "abs": "https://arxiv.org/abs/2507.06790", "authors": ["Arjun Madhusudan", "Benjamin Watson"], "title": "Better frame rates or better visuals? An early report of Esports player practice in Dota 2", "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "Esports athletes often reduce visual quality to improve latency and frame\nrate, and increase their in-game performance. Little research has examined the\neffects of this visuo-spatial tradeoff on performance, but we could find no\nwork studying how players manage this tradeoff in practice. This paper is an\ninitial examination of this question in the game Dota 2. First, we gather the\ngame configuration data of Dota 2 players in a small survey. We learn that\nplayers do limit visual detail, particularly by turning off VSYNC, which\nremoves rendering/display synchronization delay but permits visual \"tearing\".\nSecond, we survey the intent of those same players with a few subjective\nquestions. Player intent matches configuration practice. While our sampling of\nDota 2 players may not be representative, our survey does reveal suggestive\ntrends that lay the groundwork for future, more rigorous and larger surveys.\nSuch surveys can help new players adapt to the game more quickly, encourage\nresearchers to investigate the relative importance of temporal and visual\ndetail, and justify design effort by developers in \"low visual\" game\nconfigurations."}
{"id": "2507.06878", "pdf": "https://arxiv.org/pdf/2507.06878", "abs": "https://arxiv.org/abs/2507.06878", "authors": ["Lucile Favero", "Juan-Antonio Pérez-Ortiz", "Tanja Käser", "Nuria Oliver"], "title": "Do AI tutors empower or enslave learners? Toward a critical use of AI in education", "categories": ["cs.CY", "cs.HC"], "comment": "Applications of Generative AI to support teaching and learning in\n  Higher Education, co-located with AIED 2025. Palermo Italy", "summary": "The increasing integration of AI tools in education presents both\nopportunities and challenges, particularly regarding the development of the\nstudents' critical thinking skills. This position paper argues that while AI\ncan support learning, its unchecked use may lead to cognitive atrophy, loss of\nagency, emotional risks, and ethical concerns, ultimately undermining the core\ngoals of education. Drawing on cognitive science and pedagogy, the paper\nexplores how over-reliance on AI can disrupt meaningful learning, foster\ndependency and conformity, undermine the students' self-efficacy, academic\nintegrity, and well-being, and raise concerns about questionable privacy\npractices. It also highlights the importance of considering the students'\nperspectives and proposes actionable strategies to ensure that AI serves as a\nmeaningful support rather than a cognitive shortcut. The paper advocates for an\nintentional, transparent, and critically informed use of AI that empowers\nrather than diminishes the learner."}
{"id": "2507.07047", "pdf": "https://arxiv.org/pdf/2507.07047", "abs": "https://arxiv.org/abs/2507.07047", "authors": ["Yuan Li", "Teja Mandaloju", "Haihua Chen"], "title": "Exploring Public Perceptions of Generative AI in Libraries: A Social Media Analysis of X Discussions", "categories": ["cs.CY", "cs.HC", "cs.SI"], "comment": null, "summary": "This study investigates public perceptions of generative artificial\nintelligence (GenAI) in libraries through a large-scale analysis of posts on X\n(formerly Twitter). Using a mixed-method approach that combines temporal trend\nanalysis, sentiment classification, and social network analysis, this paper\nexplores how public discourse around GenAI and libraries has evolved over time,\nthe emotional tones that dominate the conversation, and the key users or\norganizations driving engagement. The findings reveal that discussions are\npredominantly negative in tone, with surges linked to concerns about ethics and\nintellectual property. Furthermore, social network analysis identifies both\ninstitutional authority and individual bridge users who facilitate cross-domain\nengagement. The results in this paper contribute to the growing body of\nliterature on GenAI in the library and GLAM (Galleries, Libraries, Archives,\nand Museums) sectors and offer a real-time, public-facing perspective on the\nemerging opportunities and concerns GenAI presents."}
