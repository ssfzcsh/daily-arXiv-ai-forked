<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.HC](#cs.HC) [Total: 6]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [math.LO](#math.LO) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A German Gold-Standard Dataset for Sentiment Analysis in Software Engineering](https://arxiv.org/abs/2507.07325)
*Martin Obaidi,Marc Herrmann,Elisa Schmid,Raymond Ochsner,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 该研究提供了一个德语开发者情感分析数据集，填补了德语软件工程领域情感分析工具的空白，并验证了数据集的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析工具主要基于英语或非德语数据集，德语软件工程领域缺乏专用资源，因此需要构建一个德语数据集来支持相关研究。

Method: 从德国开发者论坛Android-Hilfe.de提取5949条开发者语句，由四位德语母语的计算机学生基于Shaver等的情绪模型进行六种基本情感标注，并评估标注质量和工具性能。

Result: 标注过程显示出高一致性和可靠性，验证了数据集的有效性；现有德语情感分析工具在软件工程领域表现不佳，凸显了领域专用解决方案的必要性。

Conclusion: 该数据集为德语软件工程情感分析提供了可靠资源，未来可用于优化标注方法和拓展应用场景。

Abstract: Sentiment analysis is an essential technique for investigating the emotional
climate within developer teams, contributing to both team productivity and
project success. Existing sentiment analysis tools in software engineering
primarily rely on English or non-German gold-standard datasets. To address this
gap, our work introduces a German dataset of 5,949 unique developer statements,
extracted from the German developer forum Android-Hilfe.de. Each statement was
annotated with one of six basic emotions, based on the emotion model by Shaver
et al., by four German-speaking computer science students. Evaluation of the
annotation process showed high interrater agreement and reliability. These
results indicate that the dataset is sufficiently valid and robust to support
sentiment analysis in the German-speaking software engineering community.
Evaluation with existing German sentiment analysis tools confirms the lack of
domain-specific solutions for software engineering. We also discuss approaches
to optimize annotation and present further use cases for the dataset.

</details>


### [2] [Automatic Generation of Explainability Requirements and Software Explanations From User Reviews](https://arxiv.org/abs/2507.07344)
*Martin Obaidi,Jannik Fischbach,Jakob Droste,Hannah Deters,Marc Herrmann,Jil Klünder,Steffen Krätzig,Hugo Villamizar,Kurt Schneider*

Main category: cs.SE

TL;DR: 论文提出了一种自动化工具，用于从用户反馈中提取可解释性需求并生成对应解释，但AI生成的需求在准确性和相关性上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 增强系统透明度、建立用户信任和满足法规要求，需要可解释性，但将用户反馈转化为结构化需求和解释仍是挑战。

Method: 引入一种工具支持的方法，自动化从用户评论中提取需求和生成解释，并通过工业合作验证。

Result: AI生成的需求常缺乏相关性和正确性，但生成的解释在清晰度和风格上更受青睐，仍需人工验证。

Conclusion: 该工作推动了可解释性需求的自动化生成研究，提供了实证见解并公开了数据集。

Abstract: Explainability has become a crucial non-functional requirement to enhance
transparency, build user trust, and ensure regulatory compliance. However,
translating explanation needs expressed in user feedback into structured
requirements and corresponding explanations remains challenging. While existing
methods can identify explanation-related concerns in user reviews, there is no
established approach for systematically deriving requirements and generating
aligned explanations. To contribute toward addressing this gap, we introduce a
tool-supported approach that automates this process. To evaluate its
effectiveness, we collaborated with an industrial automation manufacturer to
create a dataset of 58 user reviews, each annotated with manually crafted
explainability requirements and explanations. Our evaluation shows that while
AI-generated requirements often lack relevance and correctness compared to
human-created ones, the AI-generated explanations are frequently preferred for
their clarity and style. Nonetheless, correctness remains an issue,
highlighting the importance of human validation. This work contributes to the
advancement of explainability requirements in software systems by (1)
introducing an automated approach to derive requirements from user reviews and
generate corresponding explanations, (2) providing empirical insights into the
strengths and limitations of automatically generated artifacts, and (3)
releasing a curated dataset to support future research on the automatic
generation of explainability requirements.

</details>


### [3] [Towards an Engineering Workflow Management System for Asset Administration Shells using BPMN](https://arxiv.org/abs/2507.07468)
*Sten Grüner,Nafise Eskandani*

Main category: cs.SE

TL;DR: 该论文探讨了通过集成工业4.0技术和AAS来优化工程工作流，提出了一种分布式AAS副本写入基础设施和自动化工作流管理原型。


<details>
  <summary>Details</summary>
Motivation: 通过AAS和BPMN实现工程工作流的自动化和优化，以提升工程数据交换和跨组织协作的效率。

Method: 提出了一种分布式AAS副本写入基础设施，结合BPMN定义了结构化自动流程，并开发了工作流管理原型。

Result: 提升了工程工作流的安全性和可扩展性，同时实现了高效的跨组织协作和自动化操作。

Conclusion: AAS与BPMN的结合和分布式基础设施显著优化了工程工作流，验证了原型在自动化和效率方面的有效性。

Abstract: The integration of Industry 4.0 technologies into engineering workflows is an
essential step toward automating and optimizing plant and process engineering
processes. The Asset Administration Shell (AAS) serves as a key enabler for
creating interoperable Digital Twins that facilitate engineering data exchange
and automation. This paper explores the use of AAS within engineering
workflows, particularly in combination with Business Process Model and Notation
(BPMN) to define structured and automated processes. We propose a distributed
AAS copy-on-write infrastructure that enhances security and scalability while
enabling seamless cross organizational collaboration. We also introduce a
workflow management prototype automating AAS operations and engineering
workflows, improving efficiency and traceability.

</details>


### [4] [From Requirements to Code: Understanding Developer Practices in LLM-Assisted Software Engineering](https://arxiv.org/abs/2507.07548)
*Jonathan Ullrich,Matthias Koch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 论文探讨了开发者如何在使用生成式LLMs进行代码生成时整合需求信息，发现现有需求文档过于抽象，需人工分解为编程任务并补充设计与架构约束，强调了需求工程的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究开发者如何在使用LLMs生成代码时整合需求和设计文档，填补了这一未被充分探索的领域。

Method: 通过访谈14家公司的18名从业者，分析他们如何利用需求和设计文档为LLMs生成代码提供输入。

Result: 发现需求文档需人工分解为编程任务并补充设计与架构约束后才能用于LLM提示，需求工程仍不可或缺。

Conclusion: 研究强调了需求工程在LLM代码生成中的重要性，为自动化需求中心软件工程任务提供了理论基础。

Abstract: With the advent of generative LLMs and their advanced code generation
capabilities, some people already envision the end of traditional software
engineering, as LLMs may be able to produce high-quality code based solely on
the requirements a domain expert feeds into the system. The feasibility of this
vision can be assessed by understanding how developers currently incorporate
requirements when using LLMs for code generation-a topic that remains largely
unexplored. We interviewed 18 practitioners from 14 companies to understand how
they (re)use information from requirements and other design artifacts to feed
LLMs when generating code. Based on our findings, we propose a theory that
explains the processes developers employ and the artifacts they rely on. Our
theory suggests that requirements, as typically documented, are too abstract
for direct input into LLMs. Instead, they must first be manually decomposed
into programming tasks, which are then enriched with design decisions and
architectural constraints before being used in prompts. Our study highlights
that fundamental RE work is still necessary when LLMs are used to generate
code. Our theory is important for contextualizing scientific approaches to
automating requirements-centric SE tasks.

</details>


### [5] [Prompt Engineering for Requirements Engineering: A Literature Review and Roadmap](https://arxiv.org/abs/2507.07682)
*Kaicheng Huang,Fanyu Wang,Yutan Huang,Chetan Arora*

Main category: cs.SE

TL;DR: 该论文首次提出了面向需求工程（RE）的提示工程（PE4RE）系统文献综述，通过分类法将技术导向模式与任务导向RE角色联系起来，填补研究空白并提供发展路线图。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在需求工程（RE）中应用时存在不确定性和不可控性，缺乏明确的提示指导，阻碍其可信赖实施。

Method: 采用Kitchenham和Petersen的二次研究协议，分析35项主要研究，提出混合分类法并设计研究问题。

Result: 通过任务、LLM家族和提示类型的映射，揭示当前局限与研究空白，并提出未来发展路线图。

Conclusion: 论文为PE4RE领域提供了结构化综述和实用路线图，有助于从原型过渡到可复用的工作流程。

Abstract: Advancements in large language models (LLMs) have led to a surge of prompt
engineering (PE) techniques that can enhance various requirements engineering
(RE) tasks. However, current LLMs are often characterized by significant
uncertainty and a lack of controllability. This absence of clear guidance on
how to effectively prompt LLMs acts as a barrier to their trustworthy
implementation in the RE field. We present the first roadmap-oriented
systematic literature review of Prompt Engineering for RE (PE4RE). Following
Kitchenham's and Petersen's secondary-study protocol, we searched six digital
libraries, screened 867 records, and analyzed 35 primary studies. To bring
order to a fragmented landscape, we propose a hybrid taxonomy that links
technique-oriented patterns (e.g., few-shot, Chain-of-Thought) to task-oriented
RE roles (elicitation, validation, traceability). Two research questions, with
five sub-questions, map the tasks addressed, LLM families used, and prompt
types adopted, and expose current limitations and research gaps. Finally, we
outline a step-by-step roadmap showing how today's ad-hoc PE prototypes can
evolve into reproducible, practitioner-friendly workflows.

</details>


### [6] [From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry](https://arxiv.org/abs/2507.07689)
*Chetan Arora,Fanyu Wang,Chakkrit Tantithamthavorn,Aldeida Aleti,Shaun Kenyon*

Main category: cs.SE

TL;DR: 文章探讨了如何利用检索增强生成（RAG）模型支持和半自动化太空领域的需求生成，通过模块化AI方法处理任务文档并合成初步需求。


<details>
  <summary>Details</summary>
Motivation: 小型太空组织和新进入者在需求工程（RE）中面临高复杂性，尤其是从非结构化文档中提取可操作需求的挑战。

Method: 采用模块化AI方法，预处理任务文档、分类语义类别、检索相关内容，并利用大型语言模型（LLM）合成需求草案。

Result: 初步结果显示，该方法能减少人工工作量、提高需求覆盖率，并支持轻量级合规对齐。

Conclusion: 研究提出了AI在需求工程中更广泛应用的路线图，旨在降低小型组织参与大型安全关键任务的壁垒。

Abstract: Requirements engineering (RE) in the space industry is inherently complex,
demanding high precision, alignment with rigorous standards, and adaptability
to mission-specific constraints. Smaller space organisations and new entrants
often struggle to derive actionable requirements from extensive, unstructured
documents such as mission briefs, interface specifications, and regulatory
standards. In this innovation opportunity paper, we explore the potential of
Retrieval-Augmented Generation (RAG) models to support and (semi-)automate
requirements generation in the space domain. We present a modular, AI-driven
approach that preprocesses raw space mission documents, classifies them into
semantically meaningful categories, retrieves contextually relevant content
from domain standards, and synthesises draft requirements using large language
models (LLMs). We apply the approach to a real-world mission document from the
space domain to demonstrate feasibility and assess early outcomes in
collaboration with our industry partner, Starbound Space Solutions. Our
preliminary results indicate that the approach can reduce manual effort,
improve coverage of relevant requirements, and support lightweight compliance
alignment. We outline a roadmap toward broader integration of AI in RE
workflows, intending to lower barriers for smaller organisations to participate
in large-scale, safety-critical missions.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [7] [On Propositional Program Equivalence (extended abstract)](https://arxiv.org/abs/2507.07480)
*Tobias Kappé*

Main category: cs.PL

TL;DR: 论文探讨了通过抽象语句语义将程序等价性问题转化为可判定且实际可行的命题等价性问题，并基于(G)KAT视角讨论了最新进展。


<details>
  <summary>Details</summary>
Motivation: 虽然一般程序等价性不可判定，但通过抽象语句语义可以使其变为可判定且实用的命题等价性问题。

Method: 基于(G)KAT（Guarded Kleene Algebra with Tests）的视角进行研究。

Result: 提出了命题程序等价性的可行方法并讨论了最新进展。

Conclusion: 通过抽象语义可以解决程序等价性问题，且(G)KAT为研究提供了有效框架。

Abstract: General program equivalence is undecidable. However, if we abstract away the
semantics of statements, then this problem becomes not just decidable, but
practically feasible. For instance, a program of the form "if $b$ then $e$ else
$f$" should be equivalent to "if not $b$ then $f$ else $e$" - no matter what
$b$, $e$ and $f$ are. This kind of equivalence is known as propositional
equivalence. In this extended abstract, we discuss recent developments in
propositional program equivalence from the perspective of (Guarded) Kleene
Algebra with Tests, or (G)KAT.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [8] [Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning](https://arxiv.org/abs/2507.07118)
*Zelin Zhu,Kai Yang,Rui Zhang*

Main category: cs.NI

TL;DR: 本文探讨了无线定位与感知技术在现代网络中的重要性，提出了一种结合CSI和深度学习的联合优化方法，通过SPG-MIBO算法在高维数据集中实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 无线定位与感知技术在智能城市、物联网和自主系统中至关重要，但现有的MIMO-OFDM系统在高维CSI特性下对联合建模研究不足。

Method: 将定位与感知建模为混合整数双层深度学习问题，并提出了SPG-MIBO算法，适用于高维和大规模数据集，具有计算和内存效率。

Result: 在多个数据集上的实验验证了算法的有效性，并展示了联合优化的性能提升。

Conclusion: SPG-MIBO算法为无线定位与感知的联合优化提供了高效解决方案，具有理论和实践价值。

Abstract: Wireless localization and sensing technologies are essential in modern
wireless networks, supporting applications in smart cities, the Internet of
Things (IoT), and autonomous systems. High-performance localization and sensing
systems are critical for both network efficiency and emerging intelligent
applications. Integrating channel state information (CSI) with deep learning
has recently emerged as a promising solution. Recent works have leveraged the
spatial diversity of multiple input multiple output (MIMO) systems and the
frequency granularity of orthogonal frequency division multiplexing (OFDM)
waveforms to improve spatial resolution. Nevertheless, the joint modeling of
localization and sensing under the high-dimensional CSI characteristics of
MIMO-OFDM systems remains insufficiently investigated. This work aims to
jointly model and optimize localization and sensing tasks to harness their
potential synergy. We first formulate localization and sensing as a
mixed-integer bilevel deep learning problem and then propose a novel stochastic
proximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)
algorithm. SPG-MIBO is well-suited for high-dimensional and large-scale
datasets, leveraging mini-batch training at each step for computational and
memory efficiency. The algorithm is also supported by theoretical convergence
guarantees. Extensive experiments on multiple datasets validate its
effectiveness and highlight the performance gains from joint localization and
sensing optimization.

</details>


### [9] [DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training](https://arxiv.org/abs/2507.07149)
*Renyuan Liu,Yuyang Leng,Kaiyan Liu,Shaohan Hu,Chun-Fu,Chen,Peijun Zhao,Heechul Yun,Shuochao Yao*

Main category: cs.NI

TL;DR: 本文介绍了DAF框架，通过系统级优化实现高效动态激活量化，显著减少内存占用并提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 解决移动和边缘设备上深度神经网络训练中激活内存占用高的问题，动态量化方法存在系统瓶颈。

Method: 开发了混合减少操作、CPU-GPU协作位打包技术及重要性感知分页内存管理。

Result: 内存占用减少22.9倍，训练速度提升3.2倍，且不影响模型精度。

Conclusion: DAF为资源受限环境提供了可扩展且实用的解决方案。

Abstract: Recent advancements in on-device training for deep neural networks have
underscored the critical need for efficient activation compression to overcome
the memory constraints of mobile and edge devices. As activations dominate
memory usage during training and are essential for gradient computation,
compressing them without compromising accuracy remains a key research
challenge. While existing methods for dynamic activation quantization promise
theoretical memory savings, their practical deployment is impeded by
system-level challenges such as computational overhead and memory
fragmentation.
  To address these challenges, we introduce DAF, a Dynamic Activation Framework
that enables scalable and efficient on-device training through system-level
optimizations. DAF achieves both memory- and time-efficient dynamic
quantization training by addressing key system bottlenecks. It develops hybrid
reduction operations tailored to the memory hierarchies of mobile and edge
SoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic
quantization, and implements an importance-aware paging memory management
scheme to reduce fragmentation and support dynamic memory adjustments.
  These optimizations collectively enable DAF to achieve substantial memory
savings and speedup without compromising model training accuracy. Evaluations
on various deep learning models across embedded and mobile platforms
demonstrate up to a $22.9\times$ reduction in memory usage and a $3.2\times$
speedup, making DAF a scalable and practical solution for resource-constrained
environments.

</details>


### [10] [PHandover: Parallel Handover in Mobile Satellite Network](https://arxiv.org/abs/2507.07437)
*Jiasheng Wu,Shaojie Su,Wenjun Zhu,Xiong Wang,Jingjing Zhang,Xingqiu He,Yue Gao*

Main category: cs.NI

TL;DR: 提出一种并行切换机制，显著降低LEO卫星网络中高延迟切换问题，提升网络稳定性与用户体验。


<details>
  <summary>Details</summary>
Motivation: LEO卫星网络的高速移动导致地面终端频繁高延迟切换，影响低延迟应用性能。

Method: 采用基于计划的切换替代基于测量的切换，引入卫星同步功能（SSF）和机器学习信号预测模型。

Result: 实验显示切换延迟降低21倍，网络稳定性和用户性能显著提升。

Conclusion: 并行切换机制有效解决了LEO卫星网络的切换延迟问题，且兼容5G标准。

Abstract: The construction of Low Earth Orbit (LEO) satellite constellations has
recently attracted tremendous attention from both academia and industry. The 5G
and 6G standards have identified LEO satellite networks as a key component of
future mobile networks. However, due to the high-speed movement of satellites,
ground terminals often experience frequent and high-latency handovers, which
significantly deteriorate the performance of latency-sensitive applications. To
address this challenge, we propose a parallel handover mechanism for mobile
satellite networks that can considerably reduce handover latency. The main idea
is to employ plan-based handovers instead of measurement-based handovers to
avoid interactions between the access and core networks, thereby eliminating
the significant time overhead associated with traditional handover procedures.
Specifically, we introduce a novel network function named the Satellite
Synchronized Function (SSF), which is designed to be fully compliant with the
standard 5G core network. In addition, we propose a machine learning model for
signal strength prediction, coupled with an efficient handover scheduling
algorithm. We have conducted extensive experiments, and the results demonstrate
that our proposed handover scheme can reduce handover latency by 21\times
compared to the standard NTN handover scheme and two other existing handover
approaches, along with significant improvements in network stability and
user-level performance.

</details>


### [11] [Energy Transfer and Data Collection from Batteryless Sensors in Low-altitude Wireless Networks](https://arxiv.org/abs/2507.07481)
*Wen Zhang,Aimin Wang,Jiahui Li,Geng Sun,Jiacheng Wang,Weijie Yuan,Dusit Niyato*

Main category: cs.NI

TL;DR: 论文提出了一种无人机辅助的无电池传感器网络数据收集与无线能量传输框架，用于高温等极端环境，通过多目标优化和增强的强化学习算法（SAC-PPV）实现了高效能量传输与数据收集。


<details>
  <summary>Details</summary>
Motivation: 传统固定无线能量传输基础设施在高温等极端环境中难以部署，且电池易失效，因此需要一种无人机辅助的新方法。

Method: 采用无人机进行无线能量传输（WPT）和数据收集，通过多目标优化和强化学习算法（SAC-PPV）联合优化能量分配与飞行轨迹。

Result: 仿真结果显示，提出的SAC-PPV算法在不同网络配置下均优于基准算法。

Conclusion: 无人机辅助框架及SAC-PPV算法在极端环境中显著提升了无电池传感器网络的性能，具有高效性和稳定性。

Abstract: The integration of wireless power transfer (WPT) with Internet of Things
(IoT) offers promising solutions for sensing applications, but faces
significant challenges when deployed in hard-to-access areas such as
high-temperature environments. In such extreme conditions, traditional fixed
WPT infrastructure cannot be safely installed, and batteries rapidly degrade
due to hardware failures. In this paper, we propose an uncrewed aerial vehicle
(UAV)-assisted data collection and WPT framework for batteryless sensor (BLS)
networks deployed in these challenging environments. Specifically, we consider
a practical scenario where a UAV first transfers energy to BLS nodes via WPT,
enabling these nodes to subsequently transmit their collected data to the UAV
through orthogonal frequency-division multiple access (OFDMA). Then, we
formulate a multi-objective optimization problem that aims to maximize the fair
data collection volume while minimizing the UAV energy consumption through
joint optimization of transmit power allocation and flight trajectory planning.
Due to the non-convex nature and dynamic characteristics of this problem,
conventional optimization methods prove inadequate. To address these
challenges, we propose an enhanced soft actor-critic algorithm with
parameter-free attention, prioritized experience replay, and value-based reward
centering (SAC-PPV), thereby improving the exploration efficiency and learning
stability of the algorithm in complex WPT scenarios. Simulation results
demonstrate that the proposed approach consistently outperforms benchmark
algorithms under various network configurations.

</details>


### [12] [A Fragmentation-Aware Adaptive Bilevel Search Framework for Service Mapping in Computing Power Networks](https://arxiv.org/abs/2507.07535)
*Jingzhao Xie,Zhenglian Li,Gang Sun,Long Luo,Hongfang Yu,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文提出了一种名为ABS的模块化框架，用于解决CPN中的服务映射问题，显著提升了计算资源利用率和服务接受率。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能完全实现CPN所设想的通过网络协调整合计算资源的目标，尤其是在优化服务与基础设施映射以提高资源效率和服务满意度方面存在挑战。

Method: 提出ABS框架，包括基于图划分的重新定义问题、双层优化结构和碎片感知评估，并使用分布式粒子群优化实现。

Result: 在多种CPN场景中，ABS表现优于现有方法，计算资源利用率最高提升73.2%，服务接受率提升60.2%。

Conclusion: ABS为CPN中的服务映射问题提供了一种高效解决方案，显著提升了资源利用效率和服务质量。

Abstract: Computing Power Network (CPN) unifies wide-area computing resources through
coordinated network control, while cloud-native abstractions enable flexible
resource orchestration and on-demand service provisioning atop the elastic
infrastructure CPN provides. However, current approaches fall short of fully
integrating computing resources via network-enabled coordination as envisioned
by CPN. In particular, optimally mapping services to an underlying
infrastructure to maximize resource efficiency and service satisfaction remains
challenging. To overcome this challenge, we formally define the service mapping
problem in CPN, establish its theoretical intractability, and identify key
challenges in practical optimization. We propose Adaptive Bilevel Search (ABS),
a modular framework featuring (1) graph partitioning-based reformulation to
capture variable coupling, (2) a bilevel optimization architecture for
efficient global exploration with local optimality guarantees, and (3)
fragmentation-aware evaluation for global performance guidance. Implemented
using distributed particle swarm optimization, ABS is extensively evaluated
across diverse CPN scenarios, consistently outperforming existing approaches.
Notably, in complex scenarios, ABS achieves up to 73.2% higher computing
resource utilization and a 60.2% higher service acceptance ratio compared to
the best-performing baseline.

</details>


### [13] [Can cloud-based VR streaming handle Wi-Fi OBSS contention?](https://arxiv.org/abs/2507.07677)
*Miguel Casasnovas,Marc Carrascosa-Zamacois,Boris Bellalta*

Main category: cs.NI

TL;DR: 该论文实验分析了相邻Wi-Fi网络在重叠频道上争用对VR流媒体的负面影响，尤其是在80 MHz频道内的部分和完全重叠场景。结果表明，NeSt-VR算法能有效缓解性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究Wi-Fi网络中频道重叠导致的争用问题对VR流媒体性能的影响，特别是在高密度网络环境中。

Method: 实验分析了80 MHz频道内的部分和完全重叠场景，比较了不同OBSS数量和负载条件下的VR流媒体性能。

Result: 发现OBSS数量和位置对性能有显著影响，且提出的NeSt-VR算法能有效改善在高争用环境下的流媒体质量。

Conclusion: 频道重叠和争用会显著降低VR流媒体性能，但通过适应性算法可以缓解这一问题，尤其是在高密度网络中。

Abstract: This paper experimentally analyzes the negative impact of contention caused
by neighboring Wi-Fi networks operating on overlapping channels on Virtual
Reality (VR) streaming over Wi-Fi, focusing on scenarios of partial and full
channel overlap within an 80 MHz channel. Our results show that (i) increasing
the number of 80 MHz Overlapping Basic Service Sets (OBSSs) intensifies
contention and degrades VR streaming performance; (ii) OBSS activity on the
secondary-sided 40 MHz portion degrades performance more than activity on the
primary-sided 40 MHz portion; (iii) for the same aggregate load, full channel
overlap with two 40 MHz OBSS contenders is less detrimental than partial
overlap with a single high-load 40 MHz contender, but more disruptive than full
overlap with two 80 MHz contenders; and (iv) full channel overlap with two 40
MHz OBSS contenders has a smaller impact on VR streaming under symmetric
traffic loads than under asymmetric loads. Moreover, our results demonstrate
that our previously proposed Network-aware Step-wise adaptive bitrate algorithm
for VR streaming (NeSt-VR) effectively mitigates performance degradation in
OBSS environments, enabling VR streaming under heavier OBSS traffic conditions.

</details>


### [14] [HaLert: A Resilient Smart City Architecture for Post-Disaster Based on Wi-Fi HaLow Mesh and SDN](https://arxiv.org/abs/2507.07841)
*Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.NI

TL;DR: 论文提出了一种基于Wi-Fi HaLow和LoRa的弹性通信架构HaLert，用于灾难后快速恢复通信。


<details>
  <summary>Details</summary>
Motivation: 不可预测的灾难事件需要创新的通信解决方案，利用现有基础设施在灾难后快速恢复通信能力。

Method: 结合Wi-Fi HaLow IEEE 802.11s网状网络和SDN技术，通过LoRa控制洪泛网状网络支持远程监控和配置。

Result: 系统在城市环境中测试成功，Wi-Fi HaLow网络保持稳定，LoRa网络消息成功率高达94.96%。

Conclusion: HaLert架构展示了在灾难场景下快速部署和稳定通信的潜力，为智能城市提供了一种有效的应急通信解决方案。

Abstract: Events such as catastrophes and disasters are, in most cases, unpredictable.
Consequently, reusing existing infrastructures to develop alternative
communication strategies after disasters is essential to minimise the impact of
these events on the population's ability to communicate and promptly receive
alerts from authorities. In this context, the emergence of smart cities,
characterised by dense and geographically distributed IoT networks, presents
significant potential for such reuse. This work proposes HaLert, a resilient
architecture for smart cities based on a Wi-Fi HaLow IEEE 802.11s mesh network,
whose resources can be readily reallocated to support a emergency communication
system to exchange messages (including text, location, image, audio, and video)
between citizens, authorities, and between both parties. To facilitate remote
monitoring and configuration of the network, the architecture incorporates the
SDN (Software-Defined Networking) paradigm, supported by a LoRa controlled
flooding mesh network. A prototype was developed based on this architecture and
tested in a real urban scenario comprising both indoor and outdoor
environments. The results demonstrated that, despite the significant impact of
obstacles, lack of line-of-sight, and terrain slopes on the latency (average
latency between 15 and 54.8 ms) and throughput (upload bitrates between 134 and
726 Kbps and download bitrates between 117 and 682 Kbps) of the Wi-Fi HaLow
network, it remained stable and resilient, successfully providing all
functionalities associated with the HaLert architecture. The tests conducted on
the LoRa network revealed a high average message success rate of 94.96%.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [15] [IML-Spikeformer: Input-aware Multi-Level Spiking Transformer for Speech Processing](https://arxiv.org/abs/2507.07396)
*Zeyang Song,Shimin Zhang,Yuhong Chou,Jibin Wu,Haizhou Li*

Main category: cs.MM

TL;DR: IML-Spikeformer 是一种针对大规模语音处理的 SNN 架构，通过输入感知的多级脉冲机制和重新参数化的自注意力模块，显著提升了任务性能和能效。


<details>
  <summary>Details</summary>
Motivation: SNN 在大规模语音处理任务中表现不佳，主要受限于训练中的高计算开销和缺乏专用架构。

Method: 提出了 IML-Spikeformer，包含输入感知的多级脉冲机制（IMLS）和 HD-RepSSA 模块。

Result: 在 AiShell-1 和 Librispeech-960 上分别达到 6.0% 和 3.4% 的词错误率，理论能效提升 4.64 倍和 4.32 倍。

Conclusion: IML-Spikeformer 显著推进了 SNN 在大规模语音处理中的性能和能效。

Abstract: Spiking Neural Networks (SNNs), inspired by biological neural mechanisms,
represent a promising neuromorphic computing paradigm that offers
energy-efficient alternatives to traditional Artificial Neural Networks (ANNs).
Despite proven effectiveness, SNN architectures have struggled to achieve
competitive performance on large-scale speech processing task. Two key
challenges hinder progress: (1) the high computational overhead during training
caused by multi-timestep spike firing, and (2) the absence of large-scale SNN
architectures tailored to speech processing tasks. To overcome the issues, we
introduce Input-aware Multi-Level Spikeformer, i.e. IML-Spikeformer, a spiking
Transformer architecture specifically designed for large-scale speech
processing. Central to our design is the Input-aware Multi-Level Spike (IMLS)
mechanism, which simulate multi-timestep spike firing within a single timestep
using an adaptive, input-aware thresholding scheme. IML-Spikeformer further
integrates a Reparameterized Spiking Self-Attention (RepSSA) module with a
Hierarchical Decay Mask (HDM), forming the HD-RepSSA module. This module
enhances the precision of attention maps and enables modeling of multi-scale
temporal dependencies in speech signals. Experiments demonstrate that
IML-Spikeformer achieves word error rates of 6.0\% on AiShell-1 and 3.4\% on
Librispeech-960, comparable to conventional ANN transformers while reducing
theoretical inference energy consumption by 4.64$\times$ and 4.32$\times$
respectively. IML-Spikeformer marks an advance of scalable SNN architectures
for large-scale speech processing in both task performance and energy
efficiency.

</details>


### [16] [The Potential of Olfactory Stimuli in Stress Reduction through Virtual Reality](https://arxiv.org/abs/2507.07911)
*Yasmin Elsaddik Valdivieso,Mohd Faisal,Karim Alghoul,Monireh,Vahdati,Kamran Gholizadeh Hamlabadi,Fedwa Laamarti,Hussein Al Osman,Abdulmotaleb El Saddik*

Main category: cs.MM

TL;DR: 研究表明，嗅觉刺激虽未显著影响自我报告的放松感，但通过心率变异性分析，显著降低了压力水平。


<details>
  <summary>Details</summary>
Motivation: 探究嗅觉刺激在虚拟现实中增强放松和减压效果的潜力。

Method: 采用随机被试内设计，比较有无嗅觉刺激（海滩精油香氛）的VR场景对30名参与者的影响，通过自我报告和心率变异性评估效果。

Result: 嗅觉刺激显著降低压力（HRV分析，p=0.002），71.4%参与者愿意使用嗅觉增强的VR放松。

Conclusion: 嗅觉刺激可能在潜意识中增强放松效果，突显多感官整合在VR中的重要性，建议未来研究个性化香气和长期效果。

Abstract: Immersive virtual reality (VR) is a promising tool for stress reduction and
relaxation, traditionally relying on visual and auditory stimuli. This study
examines the role of olfactory stimuli in enhancing these effects, using a
randomized within-subject design. Thirty participants aged 18-60 experienced VR
scenarios simulating a calming seaside environment, with sessions lasting 45
minutes, in two conditions: with and without a "Beach" essential oil scent
(Yankee Candle) administered via diffuser. Stress and relaxation were assessed
through self-reported surveys and physiological measures, specifically
ECG-based heart rate variability (HRV). Results showed no significant
difference in self-reported relaxation scores (p=0.371) between conditions, but
HRV analysis revealed a significant stress reduction (p=0.002) with olfactory
input, with HF increasing 108% from the Math Stress Test to the scented
relaxation condition, compared to 44% without scent. Additionally, 71.4% of
participants expressed willingness to use olfactory-enhanced VR for relaxation,
suggesting practical appeal. These findings indicate that olfactory stimuli may
enhance relaxation subconsciously, underscoring the importance of multisensory
integration in VR. Future work could explore personalized scents and long-term
effects to optimize VR- based interventions for emotional and physical
well-being.

</details>


### [17] [Multimodal Framework for Explainable Autonomous Driving: Integrating Video, Sensor, and Textual Data for Enhanced Decision-Making and Transparency](https://arxiv.org/abs/2507.07938)
*Abolfazl Zarghani,Amirhossein Ebrahimi,Amir Malekesfandiari*

Main category: cs.MM

TL;DR: 论文提出了一种新的多模态框架，结合视频、传感器和文本数据，用于自动驾驶车辆的行为预测并生成可解释的输出，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要处理多源数据并确保决策透明性，但目前的多模态数据整合和AI解释性仍是挑战。

Method: 采用VideoMAE分析时空视频数据，定制传感器融合模块处理实时数据，结合BERT进行文本理解，实现多模态协同决策。

Result: 在BDD-X和nuScenes数据集上，训练损失降至0.0187，行为预测准确率达92.5%，解释质量BLEU-4得分为0.75，优于现有方法。

Conclusion: 多模态整合与可解释性对构建安全、透明的自动驾驶系统至关重要，促进自动驾驶技术的广泛应用。

Abstract: Autonomous vehicles (AVs) are poised to redefine transportation by enhancing
road safety, minimizing human error, and optimizing traffic efficiency. The
success of AVs depends on their ability to interpret complex, dynamic
environments through diverse data sources, including video streams, sensor
measurements, and contextual textual information. However, seamlessly
integrating these multimodal inputs and ensuring transparency in AI-driven
decisions remain formidable challenges. This study introduces a novel
multimodal framework that synergistically combines video, sensor, and textual
data to predict driving actions while generating human-readable explanations,
fostering trust and regulatory compliance. By leveraging VideoMAE for
spatiotemporal video analysis, a custom sensor fusion module for real-time data
processing, and BERT for textual comprehension, our approach achieves robust
decision-making and interpretable outputs. Evaluated on the BDD-X (21113
samples) and nuScenes (1000 scenes) datasets, our model reduces training loss
from 5.7231 to 0.0187 over five epochs, attaining an action prediction accuracy
of 92.5% and a BLEU-4 score of 0.75 for explanation quality, outperforming
state-of-the-art methods. Ablation studies confirm the critical role of each
modality, while qualitative analyses and human evaluations highlight the
model's ability to produce contextually rich, user-friendly explanations. These
advancements underscore the transformative potential of multimodal integration
and explainability in building safe, transparent, and trustworthy AV systems,
paving the way for broader societal adoption of autonomous driving
technologies.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [18] [Dirty Data in the Newsroom: Comparing Data Preparation in Journalism and Data Science](https://arxiv.org/abs/2507.07238)
*Stephen Kasica,Charles Berret,Tamara Munzner*

Main category: cs.HC

TL;DR: 该研究填补了数据新闻中数据准备研究的空白，通过结合演绎和归纳方法，扩展了数据科学工作模型，并提出了一个关于“脏数据”问题的新分类法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索数据新闻中数据准备的实际情况，填补现有文献的不足。

Method: 采用混合主题分析方法，结合演绎编码（来自现有数据科学工作流程）和归纳编码（来自36名专业数据新闻工作者的访谈）。

Result: 扩展了数据科学工作模型，总结了60种“脏数据”问题，并提出了新的分类法。同时，识别了数据新闻工作者面临的四个挑战。

Conclusion: 研究为数据新闻中的数据准备工作提供了新见解，并提出了解决“脏数据”问题的框架。

Abstract: The work involved in gathering, wrangling, cleaning, and otherwise preparing
data for analysis is often the most time consuming and tedious aspect of data
work. Although many studies describe data preparation within the context of
data science workflows, there has been little research on data preparation in
data journalism. We address this gap with a hybrid form of thematic analysis
that combines deductive codes derived from existing accounts of data science
workflows and inductive codes arising from an interview study with 36
professional data journalists. We extend a previous model of data science work
to incorporate detailed activities of data preparation. We synthesize 60 dirty
data issues from 16 taxonomies on dirty data and our interview data, and we
provide a novel taxonomy to characterize these dirty data issues as
discrepancies between mental models. We also identify four challenges faced by
journalists: diachronic, regional, fragmented, and disparate data sources.

</details>


### [19] [FLoRA: An Advanced AI-Powered Engine to Facilitate Hybrid Human-AI Regulated Learning](https://arxiv.org/abs/2507.07362)
*Xinyu Li,Tongguang Li,Lixiang Yan,Yuheng Li,Linxuan Zhao,Mladen Raković,Inge Molenaar,Dragan Gašević,Yizhou Fan*

Main category: cs.HC

TL;DR: 论文提出了一种结合生成式人工智能和学习分析的增强型Flora引擎，旨在支持自调节学习（SRL）和人机混合调节学习（HHAIRL），并提供动态适应性脚手架工具。


<details>
  <summary>Details</summary>
Motivation: 现有数字工具在支持自调节学习和人机交互方面存在不足，无法提供动态适应性支持，影响学习效果。为解决这一问题，论文引入了增强型Flora引擎。

Method: 结合生成式人工智能（GenAI）和先进学习分析技术，开发Flora引擎，提供协作写作、多代理聊天机器人等工具，支持动态适应性学习脚手架。

Result: 研究表明，Flora引擎能有效促进自调节学习和人机混合调节学习，为AI增强学习环境提供了理论和实践解决方案。

Conclusion: Flora引擎结合了先进技术与SRL及HHAIRL理论，为未来AI支持的学习环境提供了实用工具和理论支持。

Abstract: SRL, defined as learners' ability to systematically plan, monitor, and
regulate their learning activities, is crucial for sustained academic
achievement and lifelong learning competencies. Emerging Artificial
Intelligence (AI) developments profoundly influence SRL interactions by
potentially either diminishing or strengthening learners' opportunities to
exercise their own regulatory skills. Recent literature emphasizes a balanced
approach termed Hybrid Human-AI Regulated Learning (HHAIRL), in which AI
provides targeted, timely scaffolding while preserving the learners' role as
active decision-makers and reflective monitors of their learning process.
Nevertheless, existing digital tools frequently fall short, lacking
adaptability, focusing narrowly on isolated SRL phases, and insufficiently
support meaningful human-AI interactions. In response, this paper introduces
the enhanced \flora Engine, which incorporates advanced Generative Artificial
Intelligence (GenAI) features and state-of-the-art learning analytics,
explicitly grounded in SRL and HHAIRL theories. The \flora Engine offers
instrumentation tools such as collaborative writing, multi-agents chatbot, and
detailed learning trace logging to support dynamic, adaptive scaffolding
tailored to individual needs in real time. We further present a summary of
several research studies that provide the validations for and illustrate how
these instrumentation tools can be utilized in real-world educational and
experimental contexts. These studies demonstrate the effectiveness of \flora
Engine in fostering SRL and HHAIRL, providing both theoretical insights and
practical solutions for the future of AI-enhanced learning context.

</details>


### [20] [Pluri-perspectivism in Human-robot Co-creativity with Older Adults](https://arxiv.org/abs/2507.07550)
*Marianne Bossema,Rob Saunders,Aske Plaat,Somaya Ben Allouch*

Main category: cs.HC

TL;DR: 该立场论文探讨了多视角主义作为人类创意体验的核心元素及其在人机共创中的重要性，提出了一个五维分层模型，并结合访谈研究结果，展示了机器人如何通过适应性行为增强人类创造力。


<details>
  <summary>Details</summary>
Motivation: 探索多视角主义在人机共创中的作用，以提升机器人在创意协作中的适应性行为效果。

Method: 提出五维分层模型，并通过对10位视觉艺术家和8位艺术教育者的访谈研究，分析多视角主义如何支持创意实践。

Result: 研究发现机器人可以通过适应性行为增强人类创造力，展示了多视角主义的潜力。

Conclusion: 未来研究方向包括将多视角主义与视觉语言模型结合，以提升机器人在共创中的情境敏感性。

Abstract: This position paper explores pluriperspectivism as a core element of human
creative experience and its relevance to humanrobot cocreativity We propose a
layered fivedimensional model to guide the design of cocreative behaviors and
the analysis of interaction dynamics This model is based on literature and
results from an interview study we conducted with 10 visual artists and 8 arts
educators examining how pluriperspectivism supports creative practice The
findings of this study provide insight in how robots could enhance human
creativity through adaptive contextsensitive behavior demonstrating the
potential of pluriperspectivism This paper outlines future directions for
integrating pluriperspectivism with visionlanguage models VLMs to support
context sensitivity in cocreative robots

</details>


### [21] [ArchiveGPT: A human-centered evaluation of using a vision language model for image cataloguing](https://arxiv.org/abs/2507.07551)
*Line Abele,Gerrit Anders,Tolgahan Aydın,Jürgen Buder,Helen Fischer,Dominik Kimmel,Markus Huff*

Main category: cs.HC

TL;DR: 研究探讨了AI生成的档案描述是否能接近人类编写质量，以及在博物馆和档案领域如何整合生成式AI。实验结果支持AI辅助生成草稿，但仍需人工验证以确保质量。


<details>
  <summary>Details</summary>
Motivation: 随着摄影收藏的快速增长，手动编目已无法满足需求，推动了使用视觉语言模型（VLM）来自动生成元数据的研究。

Method: 使用InternVL2模型为考古内容的照片生成描述，并通过专家和非专家的实验评估AI生成描述的质量、可接受性和可信度。

Result: 参与者对AI生成描述的识别能力高于随机，但低估了自己的判断能力。高准确性和实用性的描述更难分类，OCR错误和幻觉问题限制了质量。专家对AI工具的采纳意愿较低。

Conclusion: AI可以作为辅助工具生成草稿，但需人工验证以确保质量，尤其是在专业领域。建立透明和可解释的AI流程有助于增强专业人士的信任。

Abstract: The accelerating growth of photographic collections has outpaced manual
cataloguing, motivating the use of vision language models (VLMs) to automate
metadata generation. This study examines whether Al-generated catalogue
descriptions can approximate human-written quality and how generative Al might
integrate into cataloguing workflows in archival and museum collections. A VLM
(InternVL2) generated catalogue descriptions for photographic prints on
labelled cardboard mounts with archaeological content, evaluated by archive and
archaeology experts and non-experts in a human-centered, experimental
framework. Participants classified descriptions as AI-generated or
expert-written, rated quality, and reported willingness to use and trust in AI
tools. Classification performance was above chance level, with both groups
underestimating their ability to detect Al-generated descriptions. OCR errors
and hallucinations limited perceived quality, yet descriptions rated higher in
accuracy and usefulness were harder to classify, suggesting that human review
is necessary to ensure the accuracy and quality of catalogue descriptions
generated by the out-of-the-box model, particularly in specialized domains like
archaeological cataloguing. Experts showed lower willingness to adopt AI tools,
emphasizing concerns on preservation responsibility over technical performance.
These findings advocate for a collaborative approach where AI supports draft
generation but remains subordinate to human verification, ensuring alignment
with curatorial values (e.g., provenance, transparency). The successful
integration of this approach depends not only on technical advancements, such
as domain-specific fine-tuning, but even more on establishing trust among
professionals, which could both be fostered through a transparent and
explainable AI pipeline.

</details>


### [22] [Conjugated Capabilities: Interrelations of Elementary Human Capabilities and Their Implication on Human-Machine Task Allocation and Capability Testing Procedures](https://arxiv.org/abs/2507.07560)
*Nils Mandischer,Larissa Füller,Torsten Alles,Frank Flemisch,Lars Mikelsons*

Main category: cs.HC

TL;DR: 该论文探讨了人类与自动化能力之间的共轭能力概念，通过优化任务分配和测试设计来提升人机交互效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在让机器更好地理解人类能力，并通过共轭能力克服人类局限性，从而优化人机交互。

Method: 分析了IMBA标准中基本能力的关联性，构建了共轭能力网络图，并在康复患者数据中验证。

Result: 展示了共轭能力网络图在优化IMBA测试设计和任务分配中的应用潜力。

Conclusion: 共轭能力为人机交互提供了新的优化路径，尤其在提升效率和适应性方面具有重要意义。

Abstract: Human and automation capabilities are the foundation of every human-autonomy
interaction and interaction pattern. Therefore, machines need to understand the
capacity and performance of human doing, and adapt their own behavior,
accordingly. In this work, we address the concept of conjugated capabilities,
i.e. capabilities that are dependent or interrelated and between which effort
can be distributed. These may be used to overcome human limitations, by
shifting effort from a deficient to a conjugated capability with performative
resources. For example: A limited arm's reach may be compensated by tilting the
torso forward. We analyze the interrelation between elementary capabilities
within the IMBA standard to uncover potential conjugation, and show evidence in
data of post-rehabilitation patients. From the conjugated capabilities, within
the example application of stationary manufacturing, we create a network of
interrelations. With this graph, a manifold of potential uses is enabled. We
showcase the graph's usage in optimizing IMBA test design to accelerate data
recordings, and discuss implications of conjugated capabilities on task
allocation between the human and an autonomy.

</details>


### [23] [Probing Experts' Perspectives on AI-Assisted Public Speaking Training](https://arxiv.org/abs/2507.07930)
*Nesrine Fourati,Alisa Barkar,Marion Dragée,Liv Danthon-Lefebvre,Mathieu Chollet*

Main category: cs.HC

TL;DR: 研究评估公共演讲专家对商用AI演讲培训工具的看法，提出改进指南，支持混合培训模式。


<details>
  <summary>Details</summary>
Motivation: 探讨专家对AI演讲培训工具的效果和设计的意见，以优化工具应用。

Method: 16次半结构化访谈和2次焦点小组讨论。

Result: 专家认可AI工具价值，但需改进反馈个性化和教学设计。

Conclusion: 支持AI与传统培训结合的混合模式。

Abstract: Background: Public speaking is a vital professional skill, yet it remains a
source of significant anxiety for many individuals. Traditional training relies
heavily on expert coaching, but recent advances in AI has led to novel types of
commercial automated public speaking feedback tools. However, most research has
focused on prototypes rather than commercial applications, and little is known
about how public speaking experts perceive these tools.
  Objectives: This study aims to evaluate expert opinions on the efficacy and
design of commercial AI-based public speaking training tools and to propose
guidelines for their improvement.
  Methods: The research involved 16 semi-structured interviews and 2 focus
groups with public speaking experts. Participants discussed their views on
current commercial tools, their potential integration into traditional
coaching, and suggestions for enhancing these systems.
  Results and Conclusions: Experts acknowledged the value of AI tools in
handling repetitive, technical aspects of training, allowing coaches to focus
on higher-level skills. However they found key issues in current tools,
emphasising the need for personalised, understandable, carefully selected
feedback and clear instructional design. Overall, they supported a hybrid model
combining traditional coaching with AI-supported exercises.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [24] [Generative Panoramic Image Stitching](https://arxiv.org/abs/2507.07133)
*Mathieu Tuli,Kaveh Kamali,David B. Lindell*

Main category: cs.GR

TL;DR: 提出了一种用于全景图像生成的扩散模型微调方法，能在多参考图像条件下生成无缝且内容一致的全景图。


<details>
  <summary>Details</summary>
Motivation: 传统图像拼接方法在处理视差、光照变化等问题时效果不佳，生成模型在合成大型连贯区域时也存在局限。

Method: 通过微调基于扩散模型的修复模型，从单一参考图像生成全景图，同时保留场景内容和布局。

Result: 方法在图像质量和场景布局一致性上显著优于基线，适用于多参考图像的复杂情况。

Conclusion: 该技术成功解决了全景图像生成中的连贯性和内容一致性问题，效果优于现有方法。

Abstract: We introduce the task of generative panoramic image stitching, which aims to
synthesize seamless panoramas that are faithful to the content of multiple
reference images containing parallax effects and strong variations in lighting,
camera capture settings, or style. In this challenging setting, traditional
image stitching pipelines fail, producing outputs with ghosting and other
artifacts. While recent generative models are capable of outpainting content
consistent with multiple reference images, they fail when tasked with
synthesizing large, coherent regions of a panorama. To address these
limitations, we propose a method that fine-tunes a diffusion-based inpainting
model to preserve a scene's content and layout based on multiple reference
images. Once fine-tuned, the model outpaints a full panorama from a single
reference image, producing a seamless and visually coherent result that
faithfully integrates content from all reference images. Our approach
significantly outperforms baselines for this task in terms of image quality and
the consistency of image structure and scene layout when evaluated on captured
datasets.

</details>


### [25] [LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS](https://arxiv.org/abs/2507.07136)
*Wanhua Li,Yujie Zhao,Minghan Qin,Yang Liu,Yuanhao Cai,Chuang Gan,Hanspeter Pfister*

Main category: cs.GR

TL;DR: LangSplatV2通过稀疏编码和高斯Splatting技术，显著提升了3D语言场的高维特征渲染和文本查询速度，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: LangSplat在3D语言场中的应用虽有效，但受限于重型解码器和低实时性能（8.2 FPS），LangSplatV2旨在解决这些瓶颈。

Method: 采用稀疏编码假设，学习3D稀疏系数场，配合CUDA优化的稀疏系数Splatting方法，避免了重型解码器的使用。

Result: LangSplatV2实现了476.2 FPS的高维特征Splatting和384.6 FPS的3D开放词汇查询，速度提升42×和47×，同时保持了高精度。

Conclusion: LangSplatV2解决了LangSplat的实时性能瓶颈，为复杂场景中的语言交互应用提供了高效解决方案。

Abstract: In this paper, we introduce LangSplatV2, which achieves high-dimensional
feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6
FPS for high-resolution images, providing a 42 $\times$ speedup and a 47
$\times$ boost over LangSplat respectively, along with improved query accuracy.
LangSplat employs Gaussian Splatting to embed 2D CLIP language features into
3D, significantly enhancing speed and learning a precise 3D language field with
SAM semantics. Such advancements in 3D language fields are crucial for
applications that require language interaction within complex scenes. However,
LangSplat does not yet achieve real-time inference performance (8.2 FPS), even
with advanced A100 GPUs, severely limiting its broader application. In this
paper, we first conduct a detailed time analysis of LangSplat, identifying the
heavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2
assumes that each Gaussian acts as a sparse code within a global dictionary,
leading to the learning of a 3D sparse coefficient field that entirely
eliminates the need for a heavyweight decoder. By leveraging this sparsity, we
further propose an efficient sparse coefficient splatting method with CUDA
optimization, rendering high-dimensional feature maps at high quality while
incurring only the time cost of splatting an ultra-low-dimensional feature. Our
experimental results demonstrate that LangSplatV2 not only achieves better or
competitive query accuracy but is also significantly faster. Codes and demos
are available at our project page: https://langsplat-v2.github.io.

</details>


### [26] [Digital Salon: An AI and Physics-Driven Tool for 3D Hair Grooming and Simulation](https://arxiv.org/abs/2507.07387)
*Chengan He,Jorge Alejandro Amador Herrera,Zhixin Shu,Xin Sun,Yao Feng,Sören Pirk,Dominik L. Michels,Meng Zhang,Tuanfeng Y. Wang,Julie Dorsey,Holly Rushmeier,Yi Zhou*

Main category: cs.GR

TL;DR: Digital Salon 是一个全面的头发创作系统，支持实时3D头发生成、模拟和渲染，通过自然语言交互降低3D头发建模的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 现有的3D头发建模方法通常专注于孤立的部分，计算复杂或需要网络训练，因此需要一个更全面且交互性强的系统。

Method: 系统通过四个关键阶段实现：文本引导的头发检索、实时头发模拟、交互式头发细化以及基于头发的图像生成。

Result: 用户研究表明，该系统在快速原型设计上优于传统方法，并展示了其在真实沙龙环境中的潜在应用价值。

Conclusion: Digital Salon 为数字媒体中的头发建模提供了一个直观、多功能且高效的解决方案。

Abstract: We introduce Digital Salon, a comprehensive hair authoring system that
supports real-time 3D hair generation, simulation, and rendering. Unlike
existing methods that focus on isolated parts of 3D hair modeling and involve a
heavy computation process or network training, Digital Salon offers a holistic
and interactive system that lowers the technical barriers of 3D hair modeling
through natural language-based interaction. The system guides users through
four key stages: text-guided hair retrieval, real-time hair simulation,
interactive hair refinement, and hair-conditioned image generation. This
cohesive workflow makes advanced hair design accessible to users of varying
skill levels and dramatically streamlines the creative process in digital media
with an intuitive, versatile, and efficient solution for hair modeling. User
studies show that our system can outperform traditional hair modeling workflows
for rapid prototyping. Furthermore, we provide insights into the benefits of
our system with future potential of deploying our system in real salon
environments. More details can be found on our project page:
https://digital-salon.github.io/.

</details>


### [27] [Self-supervised Learning of Latent Space Dynamics](https://arxiv.org/abs/2507.07440)
*Yue Li,Gene Wei-Chin Lin,Egor Larionov,Aljaz Bozic,Doug Roble,Ladislav Kavan,Stelian Coros,Bernhard Thomaszewski,Tuur Stuyck,Hsiao-yu Chen*

Main category: cs.GR

TL;DR: 提出一种基于神经隐空间积分器的子空间模拟框架，显著提升了便携设备上的计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统模拟方法计算成本高，子空间方法也难以满足便携设备的性能需求。

Method: 利用自监督学习的神经隐空间积分器，完全在隐空间中操作，无需全空间计算。

Result: 在杆、壳和固体等挑战性示例中验证了方法的有效性和泛化能力。

Conclusion: 该方法高效且适用于便携设备，具有广泛的潜在应用前景。

Abstract: Modeling the dynamic behavior of deformable objects is crucial for creating
realistic digital worlds. While conventional simulations produce high-quality
motions, their computational costs are often prohibitive. Subspace simulation
techniques address this challenge by restricting deformations to a
lower-dimensional space, improving performance while maintaining visually
compelling results. However, even subspace methods struggle to meet the
stringent performance demands of portable devices such as virtual reality
headsets and mobile platforms. To overcome this limitation, we introduce a
novel subspace simulation framework powered by a neural latent-space
integrator. Our approach leverages self-supervised learning to enhance
inference stability and generalization. By operating entirely within latent
space, our method eliminates the need for full-space computations, resulting in
a highly efficient method well-suited for deployment on portable devices. We
demonstrate the effectiveness of our approach on challenging examples involving
rods, shells, and solids, showcasing its versatility and potential for
widespread adoption.

</details>


### [28] [SD-GS: Structured Deformable 3D Gaussians for Efficient Dynamic Scene Reconstruction](https://arxiv.org/abs/2507.07465)
*Wei Yao,Shuzhao Xie,Letian Li,Weixiang Zhang,Zhixin Lai,Shiqi Dai,Ke Zhang,Zhi Wang*

Main category: cs.GR

TL;DR: SD-GS是一种高效紧凑的动态高斯泼溅框架，通过引入可变形锚点网格和变形感知密集化策略，显著降低存储成本并提升复杂动态场景重建的能力。


<details>
  <summary>Details</summary>
Motivation: 当前4D高斯框架在动态场景重建中存在存储成本与复杂运动建模能力之间的权衡问题，限制了其实际应用，因此需要一种更高效的解决方案。

Method: 1. 提出可变形锚点网格作为层次化、内存高效的场景表示；2. 设计变形感知密集化策略，动态调整锚点以优化重建效果。

Result: 实验表明，SD-GS相比现有方法平均减少60%模型大小，提升100%的每秒帧数（FPS），同时保持或超越视觉质量。

Conclusion: SD-GS通过创新设计显著提升了动态场景重建的效率和效果，具有广泛的实际应用潜力。

Abstract: Current 4D Gaussian frameworks for dynamic scene reconstruction deliver
impressive visual fidelity and rendering speed, however, the inherent trade-off
between storage costs and the ability to characterize complex physical motions
significantly limits the practical application of these methods. To tackle
these problems, we propose SD-GS, a compact and efficient dynamic Gaussian
splatting framework for complex dynamic scene reconstruction, featuring two key
contributions. First, we introduce a deformable anchor grid, a hierarchical and
memory-efficient scene representation where each anchor point derives multiple
3D Gaussians in its local spatiotemporal region and serves as the geometric
backbone of the 3D scene. Second, to enhance modeling capability for complex
motions, we present a deformation-aware densification strategy that adaptively
grows anchors in under-reconstructed high-dynamic regions while reducing
redundancy in static areas, achieving superior visual quality with fewer
anchors. Experimental results demonstrate that, compared to state-of-the-art
methods, SD-GS achieves an average of 60\% reduction in model size and an
average of 100\% improvement in FPS, significantly enhancing computational
efficiency while maintaining or even surpassing visual quality.

</details>


### [29] [Capture Stage Environments: A Guide to Better Matting](https://arxiv.org/abs/2507.07623)
*Hannah Dröge,Janelle Pfeifer,Saskia Rabich,Markus Plack,Reinhard Klein,Matthias B. Hullin*

Main category: cs.GR

TL;DR: 该论文探讨了在拍摄舞台上使用现有matting算法时遇到的特殊挑战，并提出了改进工作流程和适应高级方法的指南，同时展示了一种无需大量注释的高效管道。


<details>
  <summary>Details</summary>
Motivation: 现有的matting算法在其他应用中表现良好，但在拍摄舞台内容中面临显著挑战，因此需要提出针对性的改进方案。

Method: 论文整理了一系列拍摄舞台内容的特点，并提供了一个指南，同时展示了一种适应高级方法的离线及实时管道。

Result: 通过基于扩散模型的验证方法，证明了所提方法的有效性。

Conclusion: 论文为拍摄舞台的matting问题提供了实用的解决方案和指导，提高了工作流程的效率。

Abstract: Capture stages are high-end sources of state-of-the-art recordings for
downstream applications in movies, games, and other media. One crucial step in
almost all pipelines is the matting of images to isolate the captured
performances from the background. While common matting algorithms deliver
remarkable performance in other applications like teleconferencing and mobile
entertainment, we found that they struggle significantly with the peculiarities
of capture stage content. The goal of our work is to share insights into those
challenges as a curated list of those characteristics along with a constructive
discussion for proactive intervention and present a guideline to practitioners
for an improved workflow to mitigate unresolved challenges. To this end, we
also demonstrate an efficient pipeline to adapt state-of-the-art approaches to
such custom setups without the need of extensive annotations, both offline and
real-time. For an objective evaluation, we propose a validation methodology
based on a leading diffusion model that highlights the benefits of our
approach.

</details>


### [30] [RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection](https://arxiv.org/abs/2507.07733)
*Yongyang Zhou,Fang-Lue Zhang,Zichen Wang,Lei Zhang*

Main category: cs.GR

TL;DR: RTR-GS是一个新的逆渲染框架，能够有效处理反射物体的渲染问题，并分解BRDF和光照，支持高质量的重光照结果。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting在视点合成中表现优异，但在处理反射物体和逆渲染时仍面临挑战。本文旨在解决这一问题，提出了一种新的方法。

Method: 提出了一种混合渲染模型，结合前向渲染和延迟渲染，以分离高频和低频外观，并通过物理基础的延迟渲染分支优化BRDF和光照分解。

Result: 实验表明，该方法在视点合成、法线估计、分解和重光照方面均表现出色，同时保持了高效的训练和推理过程。

Conclusion: RTR-GS为处理反射物体的逆渲染提供了一种高效且可靠的解决方案，具有广泛的应用潜力。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in
novel view synthesis. However, rendering reflective objects remains a
significant challenge, particularly in inverse rendering and relighting. We
introduce RTR-GS, a novel inverse rendering framework capable of robustly
rendering objects with arbitrary reflectance properties, decomposing BRDF and
lighting, and delivering credible relighting results. Given a collection of
multi-view images, our method effectively recovers geometric structure through
a hybrid rendering model that combines forward rendering for radiance transfer
with deferred rendering for reflections. This approach successfully separates
high-frequency and low-frequency appearances, mitigating floating artifacts
caused by spherical harmonic overfitting when handling high-frequency details.
We further refine BRDF and lighting decomposition using an additional
physically-based deferred rendering branch. Experimental results show that our
method enhances novel view synthesis, normal estimation, decomposition, and
relighting while maintaining efficient training inference process.

</details>


### [31] [Hi-d maps: An interactive visualization technique for multi-dimensional categorical data](https://arxiv.org/abs/2507.07890)
*Radi Muhammad Reza,Benjamin A Watson*

Main category: cs.GR

TL;DR: Hi-D maps提出了一种新颖的多维分类数据可视化方法，通过层次化切割二维多边形区域来高效展示多维数据。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高效且节省空间的多维数据可视化技术，尤其是处理大量维度时。

Method: 将多维数据映射到二维多边形区域，利用平行线层次化切割，并通过多种视觉线索（如方向、厚度、颜色等）展示跨维度信息。

Result: 该方法支持交互和层次化浏览，扩展性强，适合展示层次化信息，但维度过多时效果会下降。

Conclusion: Hi-D maps为多维数据提供了一种有效的可视化方案，尽管在高维度情况下效果有限。

Abstract: In this paper, we present Hi-D maps, a novel method for the visualization of
multi-dimensional categorical data. Our work addresses the scarcity of
techniques for visualizing a large number of data-dimensions in an effective
and space-efficient manner. We have mapped the full data-space onto a 2D
regular polygonal region. The polygon is cut hierarchically with lines parallel
to a user-controlled, ordered sequence of sides, each representing a dimension.
We have used multiple visual cues such as orientation, thickness, color,
countable glyphs, and text to depict cross-dimensional information. We have
added interactivity and hierarchical browsing to facilitate flexible
exploration of the display: small areas can be scrutinized for details. Thus,
our method is also easily extendable to visualize hierarchical information. Our
glyph animations add an engaging aesthetic during interaction. Like many
visualizations, Hi-D maps become less effective when a large number of
dimensions stresses perceptual limits, but Hi-D maps may add clarity before
those limits are reached.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [Distributed Training under Packet Loss](https://arxiv.org/abs/2507.07114)
*Erez Weintraub,Ron Banner,Ariel Orda*

Main category: cs.DC

TL;DR: 提出了一种新型分布式训练框架，能够在不可靠连接下保持模型精度和收敛性，解决了数据包丢失导致的性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有分布式框架依赖可靠连接（如InfiniBand或RoCE），导致高延迟和扩展性受限，亟需一种在不可靠连接下仍能保证精度的解决方案。

Method: 采用两阶段防御机制：1）无偏梯度聚合，通过收到的包重建一致性梯度估计；2）有界参数广播，证明模型差异始终有限。

Result: 在LLAMA2 7B模型上测试，64个GPU下可容忍10%丢包，困惑度变化不超过0.8%。

Conclusion: 该框架填补了高效通信协议与大规模模型训练精度需求间的鸿沟，支持在普通或广域网络上进行鲁棒的高吞吐学习。

Abstract: State-of-the-art language and vision models are routinely trained across
thousands of GPUs, often spanning multiple data-centers, yet today's
distributed frameworks still assume reliable connections (e.g., InfiniBand or
RoCE). The resulting acknowledgment traffic and retransmissions inflate tail
latencies and limit scalability. Leveraging unreliable connections will reduce
latency but may sacrifice model accuracy and convergence once packets are
dropped. A principled, end-to-end solution that preserves accuracy and
convergence guarantees under genuine packet loss has previously been missing.
We address this critical gap by introducing a novel distributed training
framework capable of operating over unreliable connections, offering unbiased
gradient aggregation and bounded parameter drift without modifying model code
or optimizers. The key insight is a two-stage defense against missing messages:
(i) Unbiased gradient aggregation: each worker reconstructs a consistent
gradient estimate from whatever packets arrive, guaranteeing expectation-level
correctness; and (ii) Bounded-drift parameter broadcasts: we prove the
inter-worker model discrepancy remains O(1) even after arbitrarily many
iterations, preventing the unbounded divergence typical of asynchronous setups.
Analytical bounds are matched by experiments on the LLAMA2 7B model with 64
GPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.
This work bridges the gap between communication-efficient datacenter protocols
and the accuracy and generalization guarantees demanded by modern large-model
training, enabling robust, high-throughput learning on commodity or wide-area
networks.

</details>


### [33] [Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure](https://arxiv.org/abs/2507.07223)
*Myoungsoo Jung*

Main category: cs.DC

TL;DR: 为解决现代AI工作负载（如LLM和RAG）对内存和通信带宽的高需求，传统GPU架构因通信开销难以扩展。报告提出基于CXL的模块化数据中心架构及混合CXL-over-XLink设计，优化扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载（如LLM和RAG）对内存、通信带宽和资源灵活性的需求激增，传统GPU架构因通信开销难以扩展。

Method: 提出模块化数据中心架构（基于CXL）、加速器优化互联（XLink）和混合CXL-over-XLink设计，结合分层内存模型和轻量级CXL实现。

Result: 评估显示改进的扩展性、吞吐量和灵活性。

Conclusion: 基于CXL的模块化架构和混合互联设计有效解决了AI基础设施的扩展性问题。

Abstract: Modern AI workloads such as large language models (LLMs) and
retrieval-augmented generation (RAG) impose severe demands on memory,
communication bandwidth, and resource flexibility. Traditional GPU-centric
architectures struggle to scale due to growing inter-GPU communication
overheads. This report introduces key AI concepts and explains how Transformers
revolutionized data representation in LLMs. We analyze large-scale AI hardware
and data center designs, identifying scalability bottlenecks in hierarchical
systems. To address these, we propose a modular data center architecture based
on Compute Express Link (CXL) that enables disaggregated scaling of memory,
compute, and accelerators. We further explore accelerator-optimized
interconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink
Fusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance
data transfers while preserving memory coherence. We also propose a
hierarchical memory model that combines local and pooled memory, and evaluate
lightweight CXL implementations, HBM, and silicon photonics for efficient
scaling. Our evaluations demonstrate improved scalability, throughput, and
flexibility in AI infrastructure.

</details>


### [34] [Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces](https://arxiv.org/abs/2507.07116)
*Juan Cano-Benito,Andrea Cimmino,Sven Hertling,Heiko Paulheim,Raúl García-Castro*

Main category: cs.DC

TL;DR: 论文系统评估了不同类型DLT（公共、私有和混合）对语义数据存储的性能和效率，发现私有DLT最有效，混合DLT在可审计性和效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补语义数据在分布式账本技术（DLT）上高效存储的空白，以支持数据空间中的语义互操作性。

Method: 通过使用真实世界知识图谱进行实验，比较不同类型DLT在性能、存储效率、资源消耗及语义数据更新和查询能力上的表现。

Result: 私有DLT在语义内容存储和管理上最有效，混合DLT在公共可审计性和操作效率间提供了平衡。

Conclusion: 研究为去中心化数据生态系统中基于数据主权需求的DLT基础设施选择提供了指导。

Abstract: Data spaces are emerging as decentralised infrastructures that enable
sovereign, secure, and trustworthy data exchange among multiple participants.
To achieve semantic interoperability within these environments, the use of
semantic web technologies and knowledge graphs has been proposed. Although
distributed ledger technologies (DLT) fit as the underlying infrastructure for
data spaces, there remains a significant gap in terms of the efficient storage
of semantic data on these platforms. This paper presents a systematic
evaluation of semantic data storage across different types of DLT (public,
private, and hybrid), using a real-world knowledge graph as an experimental
basis. The study compares performance, storage efficiency, resource
consumption, and the capabilities to update and query semantic data. The
results show that private DLTs are the most efficient for storing and managing
semantic content, while hybrid DLTs offer a balanced trade-off between public
auditability and operational efficiency. This research leads to a discussion on
the selection of the most appropriate DLT infrastructure based on the data
sovereignty requirements of decentralised data ecosystems.

</details>


### [35] [Collective Communication Profiling of Modern-day Machine Learning Workloads](https://arxiv.org/abs/2507.07117)
*Jit Gupta,Andrew Li,Tarun Banka,Ariel Cohen,T. Sridhar,Raj Yavatkar*

Main category: cs.DC

TL;DR: 分析了机器学习任务中集体通信行为对网络性能的影响，并提出了改进框架和拓扑的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究机器学习任务中集体通信操作（如AllReduce、AllGather）对网络性能和资源调配的影响。

Method: 通过Nvidia Collective Communication Library的日志功能，分析多种模型（如DeepSeek、GPT）的通信行为，并调整配置参数（如并行度、节点数）。

Result: 展示了对DeepSeek V3模型的通信行为分析结果，包括操作类型、传输大小等。

Conclusion: 当前集体通信框架和网络拓扑需改进，以应对网络异常对机器学习负载的影响。

Abstract: Machine Learning jobs, carried out on large number of distributed high
performance systems, involve periodic communication using operations like
AllReduce, AllGather, and Broadcast. These operations may create high bandwidth
and bursty traffic patterns, leading to network congestion and packet loss,
thus impacting the performance of these jobs. Hence it is imperative to analyze
these patterns, which can be helpful in provisioning network resources
depending on the type of machine learning workloads. In this poster we carry
out extensive analysis of the collective communication behavior seen in a wide
variety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we
instrument Nvidia Collective Communication Library logging functionality for
richer context about the collectives and workloads. We adjust configuration
parameters that influence collective communication behavior, such as
parallelism, number of nodes, and model type. This overview presents and
discusses some of the results on the collective communication behavior for the
open source DeepSeek V3 inferencing model, which includes operation type and
count, transfer sizes per operation, and request size distribution. Our
analysis shows that it makes sense to rethink current collective communication
frameworks and network topologies so as to accommodate the effect of network
anomalies on the mentioned workloads.

</details>


### [36] [Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding](https://arxiv.org/abs/2507.07120)
*Nidhi Bhatia,Ankit More,Ritika Borkar,Tiyasa Mitra,Ramon Matas,Ritchie Zhao,Maximilian Golub,Dheevatsa Mudigere,Brian Pharris,Bita Darvish Rouhani*

Main category: cs.DC

TL;DR: Helix Parallelism通过混合KV并行和TP/EP并行，提升长序列大语言模型推理效率，降低延迟并支持更大批次。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型处理超长序列，KV缓存和FFN权重访问成为瓶颈，现有并行方法效率不足。

Method: 提出Helix Parallelism，结合KV并行和TP/EP并行，引入轻量通信优化批次重叠。

Result: Helix在固定批次下延迟降低1.5倍，支持32倍更大批次，提升吞吐-延迟平衡。

Conclusion: Helix实现了超长序列实时推理，显著优化GPU效率。

Abstract: As LLMs scale to multi-million-token KV histories, real-time autoregressive
decoding under tight Token-to-Token Latency (TTL) constraints faces growing
pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)
weights and reading long KV caches. While Tensor Parallelism (TP) helps
mitigate the cost of FFN weight reads, it does not scale well for attention.
When TP width exceeds the number of KV heads, it leads to inefficient KV
duplication, limits parallelism, and constrains batch size. Simultaneously,
DRAM reads for long KV histories scale linearly with batch size, further
capping efficiency.
  We introduce Helix Parallelism, a hybrid execution strategy that applies KV
parallelism during attention to shard KV caches across GPUs, then reuses the
same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN
computation. To preserve exact attention behavior, Helix includes a lightweight
communication step. To minimize the exposed communication cost, we introduce
Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through
batchwise overlap, preserving low TTL while improving GPU efficiency. Compared
to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at
fixed batch sizes and supports up to 32x larger batches under the same latency
budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on
Blackwell and making real-time inference with ultra-long-sequence practical.

</details>


### [37] [Ampere: Communication-Efficient and High-Accuracy Split Federated Learning](https://arxiv.org/abs/2507.07130)
*Zihan Zhang,Leon Wong,Blesson Varghese*

Main category: cs.DC

TL;DR: Ampere提出了一种新型联邦学习系统，通过单方向块间训练和轻量级辅助网络生成方法，显著降低了设备计算和通信开销，同时提高了模型精度。


<details>
  <summary>Details</summary>
Motivation: 解决Split Federated Learning（SFL）系统中因频繁中间数据交换导致的通信开销大和非IID数据下模型精度下降的问题。

Method: 采用单方向块间训练（无梯度传输）和轻量级辅助网络生成方法，减少设备与服务器的通信和数据异构性影响。

Result: 实验表明，Ampere在模型精度、训练时间、通信开销和设备计算效率上均优于SFL基线系统。

Conclusion: Ampere通过优化训练结构和通信机制，显著提高了联邦学习系统的效率和性能。

Abstract: A Federated Learning (FL) system collaboratively trains neural networks
across devices and a server but is limited by significant on-device computation
costs. Split Federated Learning (SFL) systems mitigate this by offloading a
block of layers of the network from the device to a server. However, in doing
so, it introduces large communication overheads due to frequent exchanges of
intermediate activations and gradients between devices and the server and
reduces model accuracy for non-IID data. We propose Ampere, a novel
collaborative training system that simultaneously minimizes on-device
computation and device-server communication while improving model accuracy.
Unlike SFL, which uses a global loss by iterative end-to-end training, Ampere
develops unidirectional inter-block training to sequentially train the device
and server block with a local loss, eliminating the transfer of gradients. A
lightweight auxiliary network generation method decouples training between the
device and server, reducing frequent intermediate exchanges to a single
transfer, which significantly reduces the communication overhead. Ampere
mitigates the impact of data heterogeneity by consolidating activations
generated by the trained device block to train the server block, in contrast to
SFL, which trains on device-specific, non-IID activations. Extensive
experiments on multiple CNNs and transformers show that, compared to
state-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up
to 13.26% while reducing training time by up to 94.6%, (ii) reduces
device-server communication overhead by up to 99.1% and on-device computation
by up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for
various non-IID degrees highlighting superior performance when faced with
heterogeneous data.

</details>


### [38] [M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure](https://arxiv.org/abs/2507.07144)
*Hongyi Xie,Min Zhou,Qiao Yu,Jialiang Yu,Zhenli Sheng,Hong Xie,Defu Lian*

Main category: cs.DC

TL;DR: M$^2$-MFP是一个多层次内存故障预测框架，旨在通过多尺度特征提取和双路径时间建模提升云基础设施的可靠性。


<details>
  <summary>Details</summary>
Motivation: 内存故障对系统稳定性构成重大威胁，现有方法（如基于规则的专家模型和自动特征提取方法）存在泛化性差和性能不足的问题。

Method: 将可纠正错误（CEs）转换为多级二进制矩阵表示，提出二进制空间特征提取器（BSFE）进行多层次特征提取，并设计双路径时间模型（时间窗口聚合和规则生成树）。

Result: 在基准数据集和实际部署中，M$^2$-MFP显著优于现有方法。

Conclusion: M$^2$-MFP通过多尺度特征提取和时间建模有效提升了内存故障预测的准确性和可靠性。

Abstract: As cloud services become increasingly integral to modern IT infrastructure,
ensuring hardware reliability is essential to sustain high-quality service.
Memory failures pose a significant threat to overall system stability, making
accurate failure prediction through the analysis of memory error logs (i.e.,
Correctable Errors) imperative. Existing memory failure prediction approaches
have notable limitations: rule-based expert models suffer from limited
generalizability and low recall rates, while automated feature extraction
methods exhibit suboptimal performance. To address these limitations, we
propose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction
framework designed to enhance the reliability and availability of cloud
infrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level
binary matrix representations and introduces a Binary Spatial Feature Extractor
(BSFE) to automatically extract high-order features at both DIMM-level and
bit-level. Building upon the BSFE outputs, we develop a dual-path temporal
modeling architecture: 1) a time-patch module that aggregates multi-level
features within observation windows, and 2) a time-point module that employs
interpretable rule-generation trees trained on bit-level patterns. Experiments
on both benchmark datasets and real-world deployment show the superiority of
M$^2$-MFP as it outperforms existing state-of-the-art methods by significant
margins. Code and data are available at this repository:
https://github.com/hwcloud-RAS/M2-MFP.

</details>


### [39] [Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience](https://arxiv.org/abs/2507.07352)
*Loïc Pottier,Konstantia Georgouli,Timothy S. Carpenter,Fikret Aydin,Jeremy O. B. Tempkin,Dwight V. Nissley,Frederick H. Streitz,Thomas R. W. Scogland,Peer-Timo Bremer,Felice C. Lightstone,Helgi I. Ingólfsson*

Main category: cs.DC

TL;DR: 论文提出了mini-MuMMI，一个轻量级的多尺度工作流管理工具，用于在较小计算资源上运行多尺度模拟，并展示了其在RAS-RAF膜相互作用研究中的应用。


<details>
  <summary>Details</summary>
Motivation: 多尺度模型在复杂现象建模中广泛应用，但由于计算资源需求大和跨尺度管理困难，急需轻量级解决方案。

Method: 开发了mini-MuMMI，一个基于MuMMI的轻量级版本，可在小型HPC系统或笔记本电脑上运行，并用于管理多尺度分子动力学模拟。

Result: mini-MuMMI成功应用于RAS-RAF膜相互作用的模拟，证明了其在较小计算资源上的有效性。

Conclusion: mini-MuMMI为多尺度工作流的广泛应用提供了更灵活的计算资源选择，未来可扩展至更多领域。

Abstract: Computational models have become one of the prevalent methods to model
complex phenomena. To accurately model complex interactions, such as detailed
biomolecular interactions, scientists often rely on multiscale models comprised
of several internal models operating at difference scales, ranging from
microscopic to macroscopic length and time scales. Bridging the gap between
different time and length scales has historically been challenging but the
advent of newer machine learning (ML) approaches has shown promise for tackling
that task. Multiscale models require massive amounts of computational power and
a powerful workflow management system. Orchestrating ML-driven multiscale
studies on parallel systems with thousands of nodes is challenging, the
workflow must schedule, allocate and control thousands of simulations operating
at different scales. Here, we discuss the massively parallel Multiscale
Machine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow
management infrastructure, that can orchestrate thousands of molecular dynamics
(MD) simulations operating at different timescales, spanning from millisecond
to nanosecond. More specifically, we introduce a novel version of MuMMI called
"mini-MuMMI". Mini-MuMMI is a curated version of MuMMI designed to run on
modest HPC systems or even laptops whereas MuMMI requires larger HPC systems.
We demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions
and discuss the different challenges behind the generalization of multiscale
workflows and how mini-MuMMI can be leveraged to target a broader range of
applications outside of MD and RAS-RAF interactions.

</details>


### [40] [KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows](https://arxiv.org/abs/2507.07400)
*Zaifeng Pan,Ajjkumar Patel,Zhengding Hu,Yipeng Shen,Yue Guan,Wan-Lu Li,Lianhui Qin,Yida Wang,Yufei Ding*

Main category: cs.DC

TL;DR: KVFlow是一种针对LLM代理工作流的KV缓存管理框架，通过预测代理的未来使用情况优化缓存效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统使用LRU策略管理KV缓存，无法预判代理的未来使用，导致频繁缓存未命中和重新计算开销。

Method: KVFlow通过Agent Step Graph抽象代理执行计划，结合步骤预测值和重叠KV预取机制优化缓存管理。

Result: 相比SGLang，KVFlow在单个工作流和并发工作流场景下分别实现最高1.83倍和2.19倍加速。

Conclusion: KVFlow显著提升LLM代理工作流的服务效率。

Abstract: Large language model (LLM) based agentic workflows have become a popular
paradigm for coordinating multiple specialized agents to solve complex tasks.
To improve serving efficiency, existing LLM systems employ prefix caching to
reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby
avoiding redundant computation across repeated invocations. However, current
systems typically evict KV caches using a Least Recently Used (LRU) policy,
which fails to anticipate future agent usage and often discards KV caches
shortly before their reuse. This leads to frequent cache misses and substantial
recomputation or swapping overhead. We present KVFlow, a workflow-aware KV
cache management framework tailored for agentic workloads. KVFlow abstracts the
agent execution schedule as an Agent Step Graph and assigns each agent a
steps-to-execution value that estimates its temporal proximity to future
activation. These values guide a fine-grained eviction policy at the KV node
level, allowing KVFlow to preserve entries likely to be reused and efficiently
manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a
fully overlapped KV prefetching mechanism, which proactively loads required
tensors from CPU to GPU in background threads for agents scheduled in the next
step, thereby avoiding cache miss stalls during generation. Compared to SGLang
with hierarchical radix cache, KVFlow achieves up to 1.83$\times$ speedup for
single workflows with large prompts, and up to 2.19$\times$ speedup for
scenarios with many concurrent workflows.

</details>


### [41] [Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems](https://arxiv.org/abs/2507.07671)
*Jovan Prodanov,Blaž Bertalanič,Carolina Fortuna,Shih-Kai Chou,Matjaž Branko Jurič,Ramon Sanchez-Iborra,Jernej Hribar*

Main category: cs.DC

TL;DR: 论文提出了一种基于多智能体强化学习的动态资源扩展引擎（MARLISE），以解决边缘云系统中传统静态扩展方法效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 传统资源扩展方法依赖静态阈值和预定义规则，无法适应动态工作负载，导致资源利用和性能优化不足。

Method: 采用深度强化学习算法（DQN和PPO）开发MARLISE，实现动态、反应式的资源扩展。

Result: 实验表明，MARLISE在动态工作负载下优于启发式方法，能保证低响应时间并提高资源效率。

Conclusion: MARLISE为边缘云系统提供了一种高效、动态的资源扩展解决方案。

Abstract: Modern edge-cloud systems face challenges in efficiently scaling resources to
handle dynamic and unpredictable workloads. Traditional scaling approaches
typically rely on static thresholds and predefined rules, which are often
inadequate for optimizing resource utilization and maintaining performance in
distributed and dynamic environments. This inefficiency hinders the
adaptability and performance required in edge-cloud infrastructures, which can
only be achieved through the newly proposed in-place scaling. To address this
problem, we propose the Multi-Agent Reinforcement Learning-based In-place
Scaling Engine (MARLISE) that enables seamless, dynamic, reactive control with
in-place resource scaling. We develop our solution using two Deep Reinforcement
Learning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization
(PPO). We analyze each version of the proposed MARLISE solution using dynamic
workloads, demonstrating their ability to ensure low response times of
microservices and scalability. Our results show that MARLISE-based approaches
outperform heuristic method in managing resource elasticity while maintaining
microservice response times and achieving higher resource efficiency.

</details>


### [42] [KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling](https://arxiv.org/abs/2507.07932)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Qiang Guan,Hailong Jiang*

Main category: cs.DC

TL;DR: KIS-S框架结合GPU感知的Kubernetes模拟器和基于PPO的自动扩缩器，显著提升动态流量下的GPU推理性能


<details>
  <summary>Details</summary>
Motivation: 解决Kubernetes中GPU推理工作负载在动态流量下反应式和阈值扩缩机制的局限性

Method: 提出KIS-S框架，结合GPU感知模拟器KISim和基于PPO的自动扩缩器KIScaler

Result: KIScaler在实验中将平均奖励提升75.2%，P95延迟降低6.7倍

Conclusion: KIS-S填补了反应式扩缩与智能编排之间的空白，适用于GPU加速环境

Abstract: Autoscaling GPU inference workloads in Kubernetes remains challenging due to
the reactive and threshold-based nature of default mechanisms such as the
Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty
traffic patterns and lack integration with GPU-level metrics. We present KIS-S,
a unified framework that combines KISim, a GPU-aware Kubernetes Inference
Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based
autoscaler. KIScaler learns latency-aware and resource-efficient scaling
policies entirely in simulation, and is directly deployed without retraining.
Experiments across four traffic patterns show that KIScaler improves average
reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and
generalizes without retraining. Our work bridges the gap between reactive
autoscaling and intelligent orchestration for scalable GPU-accelerated
environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [43] [Algorithmic Complexity Attacks on All Learned Cardinality Estimators: A Data-centric Approach](https://arxiv.org/abs/2507.07438)
*Yingze Li,Xianglong Liu,Dong Wang,Zixuan Wang,Hongzhi Wang,Kaixing Zhang,Yiming Guan*

Main category: cs.DB

TL;DR: 论文研究了学习型基数估计器对训练数据漂移的脆弱性，并提出了一种数据驱动的黑盒攻击方法，证明了其NP难解性并设计了多项式时间近似算法。实验显示攻击效果显著，同时提出了防御措施。


<details>
  <summary>Details</summary>
Motivation: 学习型基数估计器在查询预测中表现优秀，但对数据漂移的脆弱性限制了其实际部署。论文旨在分析其理论脆弱性并提出解决方案。

Method: 提出数据驱动的黑盒攻击方法，证明最优攻击策略为NP难问题，并设计多项式时间近似算法。实验验证攻击有效性。

Result: 攻击仅需修改0.8%训练数据即可显著降低估计器性能，同时提出了两防御措施。

Conclusion: 论文揭示了学习型基数估计器的关键脆弱性，为其稳健性提供了理论分析和实际防御建议。

Abstract: Learned cardinality estimators show promise in query cardinality prediction,
yet they universally exhibit fragility to training data drifts, posing risks
for real-world deployment. This work is the first to theoretical investigate
how minimal data-level drifts can maximally degrade the accuracy of learned
estimators. We propose data-centric algorithmic complexity attacks against
learned estimators in a black-box setting, proving that finding the optimal
attack strategy is NP-Hard. To address this, we design a polynomial-time
approximation algorithm with a $(1-\kappa)$ approximation ratio. Extensive
experiments demonstrate our attack's effectiveness: on STATS-CEB and IMDB-JOB
benchmarks, modifying just 0.8\% of training tuples increases the 90th
percentile Qerror by three orders of magnitude and raises end-to-end processing
time by up to 20$\times$. Our work not only reveals critical vulnerabilities in
deployed learned estimators but also provides the first unified worst-case
theoretical analysis of their fragility under data updates. Additionally, we
identify two countermeasures to mitigate such black-box attacks, offering
insights for developing robust learned database optimizers.

</details>


### [44] [JOB-Complex: A Challenging Benchmark for Traditional & Learned Query Optimization](https://arxiv.org/abs/2507.07471)
*Johannes Wehrstein,Timo Eckmann,Roman Heinrich,Carsten Binnig*

Main category: cs.DB

TL;DR: 论文指出现有查询优化器基准测试（如JOB）存在局限性，未能反映真实场景复杂性，因此提出了新基准JOB-Complex，包含30个SQL查询和近6000个执行计划。测试表明传统和学习型优化器在新基准下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有查询优化器基准测试（如JOB）未能充分反映真实世界的复杂性，导致对优化器性能的评估过于乐观。

Method: 提出新的基准测试JOB-Complex，包含30个SQL查询和近6000个执行计划，模拟真实场景中的复杂查询特性（如字符串列连接、复杂过滤条件）。

Result: 测试显示传统和学习型优化器在JOB-Complex上性能显著下降，运行时间比最优计划慢11倍。

Conclusion: JOB-Complex能更真实地评估查询优化器性能，揭示现有方法的不足。

Abstract: Query optimization is a fundamental task in database systems that is crucial
to providing high performance. To evaluate learned and traditional optimizer's
performance, several benchmarks, such as the widely used JOB benchmark, are
used. However, in this paper, we argue that existing benchmarks are inherently
limited, as they do not reflect many real-world properties of query
optimization, thus overstating the performance of both traditional and learned
optimizers. In fact, simple but realistic properties, such as joins over string
columns or complex filter predicates, can drastically reduce the performance of
existing query optimizers. Thus, we introduce JOB-Complex, a new benchmark
designed to challenge traditional and learned query optimizers by reflecting
real-world complexity. Overall, JOB-Complex contains 30 SQL queries and comes
together with a plan-selection benchmark containing nearly 6000 execution
plans, making it a valuable resource to evaluate the performance of query
optimizers and cost models in real-world scenarios. In our evaluation, we show
that traditional and learned cost models struggle to achieve high performance
on JOB-Complex, providing a runtime of up to 11x slower compared to the optimal
plans.

</details>


### [45] [A Service Architecture for Dataspaces](https://arxiv.org/abs/2507.07979)
*Benedikt T. Arnold,Christoph Lange,Christina Gillmann,Stefan Decker*

Main category: cs.DB

TL;DR: 本文提出了一种抽象层，用于在数据空间中提供通用服务，并通过EDC连接器的实现展示了其实际应用。


<details>
  <summary>Details</summary>
Motivation: 数据空间在实际应用中对服务的需求日益增长，但现有技术主要关注数据资产的交换，未涉及服务支持。

Method: 提出了一种抽象层，使开发者能够在数据空间中轻松开发和集成服务。

Result: 通过EDC连接器的初始实现验证了该架构的可行性和实用性。

Conclusion: 该抽象层丰富了数据空间的功能，为服务的集成提供了一个灵活且易于实现的解决方案。

Abstract: Dataspaces are designed to support sovereign, trusted and decentralized data
exchange between participants forming an ecosystem. They are standardized by
initiatives such as the International Data Spaces Association or Gaia-X and
have gained adoption in several domains such as mobility, manufacturing,
tourism or culture. In dataspaces, participants use connectors to communicate
peer-to-peer. The Eclipse Dataspace Components (EDC) Connector is a broadly
adopted, open-source implementation that adheres to the standards and is
supported by a large community. As dataspaces in general, it focuses on the
exchange of data assets with associated usage policies and does not support
services. In practice, however, there is demand for dataspace-based services
and conceptual arguments support their inclusion in dataspaces. In this paper,
we propose an abstraction layer for providing generic services within
dataspaces. Adopters can use this layer to easily develop own services,
seamlessly integrated with the existing dataspace technology. Besides, we
present an initial implementation of this service architecture for the EDC
Connector and demonstrate its practical applicability.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [46] [Accelerating Transposed Convolutions on FPGA-based Edge Devices](https://arxiv.org/abs/2507.07683)
*Jude Haris,José Cano*

Main category: cs.AR

TL;DR: 本文提出MM2IM，一种软硬件协同设计的加速器，用于优化资源受限边缘设备上的转置卷积性能，相比基准实现平均提速1.9倍。


<details>
  <summary>Details</summary>
Motivation: 现有转置卷积的输入导向映射方法存在输出映射复杂、计算冗余等问题，导致边缘设备性能瓶颈。

Method: 结合矩阵乘法与col2IM，开发MM2IM加速器，通过SECDA-TFLite工具包实现并评测。

Result: 在261种配置中平均提速1.9倍；在知名生成模型上提速达4.2倍；在DCGAN和pix2pix模型上提速3倍并节能2.4倍。

Conclusion: MM2IM显著提升转置卷积效率，适用于资源受限的边缘设备生成模型。

Abstract: Transposed Convolutions (TCONV) enable the up-scaling mechanism within
generative Artificial Intelligence (AI) models. However, the predominant
Input-Oriented Mapping (IOM) method for implementing TCONV has complex output
mapping, overlapping sums, and ineffectual computations. These inefficiencies
further exacerbate the performance bottleneck of TCONV and generative models on
resource-constrained edge devices. To address this problem, in this paper we
propose MM2IM, a hardware-software co-designed accelerator that combines Matrix
Multiplication (MatMul) with col2IM to process TCONV layers on
resource-constrained edge devices efficiently. Using the SECDA-TFLite design
toolkit, we implement MM2IM and evaluate its performance across 261 TCONV
problem configurations, achieving an average speedup of 1.9x against a
dual-thread ARM Neon optimized CPU baseline. We then evaluate the performance
of MM2IM on a range of TCONV layers from well-known generative models achieving
up to 4.2x speedup, and compare it against similar resource-constrained TCONV
accelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate
MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x
energy reduction against the CPU baseline.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Bias-Aware Mislabeling Detection via Decoupled Confident Learning](https://arxiv.org/abs/2507.07216)
*Yunyi Li,Maria De-Arteaga,Maytal Saar-Tsechansky*

Main category: cs.LG

TL;DR: 本文提出了一种名为DeCoLe的机器学习框架，用于检测数据集中因标签偏差导致的错误标注，并在仇恨言论检测领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标签偏差是数据完整性中的重要问题，但缺乏有效方法来解决。本文旨在填补这一空白。

Method: 提出Decoupled Confident Learning (DeCoLe)框架，专注于检测和纠正标签偏差导致的错误标注。

Result: 在仇恨言论检测任务中，DeCoLe优于其他标签错误检测方法。

Conclusion: DeCoLe为提升数据可靠性提供了有效工具，并可用于组织的数据管理实践。

Abstract: Reliable data is a cornerstone of modern organizational systems. A notable
data integrity challenge stems from label bias, which refers to systematic
errors in a label, a covariate that is central to a quantitative analysis, such
that its quality differs across social groups. This type of bias has been
conceptually and empirically explored and is widely recognized as a pressing
issue across critical domains. However, effective methodologies for addressing
it remain scarce. In this work, we propose Decoupled Confident Learning
(DeCoLe), a principled machine learning based framework specifically designed
to detect mislabeled instances in datasets affected by label bias, enabling
bias aware mislabelling detection and facilitating data quality improvement. We
theoretically justify the effectiveness of DeCoLe and evaluate its performance
in the impactful context of hate speech detection, a domain where label bias is
a well documented challenge. Empirical results demonstrate that DeCoLe excels
at bias aware mislabeling detection, consistently outperforming alternative
approaches for label error detection. Our work identifies and addresses the
challenge of bias aware mislabeling detection and offers guidance on how DeCoLe
can be integrated into organizational data management practices as a powerful
tool to enhance data reliability.

</details>


### [48] [CHOMET: Conditional Handovers via Meta-Learning](https://arxiv.org/abs/2507.07581)
*Michail Kalntis,Fernando A. Kuipers,George Iosifidis*

Main category: cs.LG

TL;DR: 论文提出了一种基于O-RAN范式的元学习框架，用于优化条件切换（CHO），显著提升了在信号波动条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 随着移动网络复杂性和小型小区的增加，传统切换面临延迟和失败率上升的问题，条件切换虽能缓解但带来了新的资源管理和信令开销挑战。

Method: 采用O-RAN范式并结合元学习方法，优化条件切换的资源分配和信令管理。

Result: 提出的框架在信号波动条件下比其他3GPP基准性能提升了至少180%，并提供动态遗憾保证。

Conclusion: 该框架有效地解决了条件切换的资源分配和信令开销问题，显著提升了切换性能。

Abstract: Handovers (HOs) are the cornerstone of modern cellular networks for enabling
seamless connectivity to a vast and diverse number of mobile users. However, as
mobile networks become more complex with more diverse users and smaller cells,
traditional HOs face significant challenges, such as prolonged delays and
increased failures. To mitigate these issues, 3GPP introduced conditional
handovers (CHOs), a new type of HO that enables the preparation (i.e., resource
allocation) of multiple cells for a single user to increase the chance of HO
success and decrease the delays in the procedure. Despite its advantages, CHO
introduces new challenges that must be addressed, including efficient resource
allocation and managing signaling/communication overhead from frequent cell
preparations and releases. This paper presents a novel framework aligned with
the O-RAN paradigm that leverages meta-learning for CHO optimization, providing
robust dynamic regret guarantees and demonstrating at least 180% superior
performance than other 3GPP benchmarks in volatile signal conditions.

</details>


### [49] [Stress Monitoring in Healthcare: An Ensemble Machine Learning Framework Using Wearable Sensor Data](https://arxiv.org/abs/2507.07589)
*Arpana Sinhal,Anay Sinhal,Amit Sinhal*

Main category: cs.LG

TL;DR: 本研究通过引入多模态数据集和优化机器学习模型，解决了医护人员实时压力监测中的数据集不足和方法论缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 医护人员（尤其是护士）面临高职业压力，COVID-19大流行加剧了这一现象。现有研究缺乏全面数据集和稳健的分析框架。

Method: 研究使用多模态生理信号数据集，通过SMOTE处理类别不平衡问题，并评估了多种机器学习模型（如Random Forest、XGBoost和MLP），最终采用堆叠分类器。

Result: 通过公开数据集和可复现的分析流程，研究推动了可部署的压力监测系统发展，为保护医护人员心理健康提供了实用方案。

Conclusion: 未来研究可扩展人口多样性，并探索边缘计算实现低延迟压力警报。

Abstract: Healthcare professionals, particularly nurses, face elevated occupational
stress, a concern amplified during the COVID-19 pandemic. While wearable
sensors offer promising avenues for real-time stress monitoring, existing
studies often lack comprehensive datasets and robust analytical frameworks.
This study addresses these gaps by introducing a multimodal dataset comprising
physiological signals, electrodermal activity, heart rate and skin temperature.
A systematic literature review identified limitations in prior stress-detection
methodologies, particularly in handling class imbalance and optimizing model
generalizability. To overcome these challenges, the dataset underwent
preprocessing with the Synthetic Minority Over sampling Technique (SMOTE),
ensuring balanced representation of stress states. Advanced machine learning
models including Random Forest, XGBoost and a Multi-Layer Perceptron (MLP) were
evaluated and combined into a Stacking Classifier to leverage their collective
predictive strengths. By using a publicly accessible dataset and a reproducible
analytical pipeline, this work advances the development of deployable
stress-monitoring systems, offering practical implications for safeguarding
healthcare workers' mental health. Future research directions include expanding
demographic diversity and exploring edge-computing implementations for low
latency stress alerts.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [50] [Toolchain for Faster Iterations in Quantum Software Development](https://arxiv.org/abs/2507.07448)
*Otso Kinanen,Andrés D. Muñoz-Moller,Vlad Stirbu,Tommi Mikkonen*

Main category: quant-ph

TL;DR: 该论文探讨了如何利用远程计算能力优化量子软件开发工作流程，通过降低本地与远程硬件之间的切换门槛，提升执行速度，并支持更复杂电路的开发。


<details>
  <summary>Details</summary>
Motivation: 量子计算的革命性潜力需要高效的软件开发解决方案，但开发者面临硬件限制、高计算需求和复杂技术栈等挑战。

Method: 提出了一个解决方案，通过远程计算能力简化本地与远程硬件的切换，并利用模拟器环境加速执行。

Result: 实验显示，解决方案实现了高达5倍的电路执行速度提升，并支持21至29量子位的开发。

Conclusion: 该研究为量子软件开发提供了高效的工作流程改进方案，降低了开发门槛，支持更复杂的电路设计和迭代开发。

Abstract: Quantum computing proposes a revolutionary paradigm that can radically
transform numerous scientific and industrial application domains. To realize
this promise, these new capabilities need software solutions that are able to
effectively harness its power. However, developers may face significant
challenges when developing and executing quantum software due to the limited
availability of quantum computer hardware, high computational demands of
simulating quantum computers on classical systems, and complicated technology
stack to enable currently available accelerators into development environments.
These limitations make it difficult for the developer to create an efficient
workflow for quantum software development. In this paper, we investigate the
potential of using remote computational capabilities in an efficient manner to
improve the workflow of quantum software developers, by lowering the barrier of
moving between local execution and computationally more efficient remote
hardware and offering speedup in execution with simulator surroundings. The
goal is to allow the development of more complex circuits and to support an
iterative software development approach. In our experiment, with the solution
presented in this paper, we have obtained up to 5 times faster circuit
execution runtime, and enabled qubit ranges from 21 to 29 qubits with a simple
plug-and-play kernel for the Jupyter notebook.

</details>


### [51] [Quantum Executor: A Unified Interface for Quantum Computing](https://arxiv.org/abs/2507.07597)
*Giuseppe Bisicchia,Alessandro Bocci,Antonio Brogi*

Main category: quant-ph

TL;DR: 本文介绍了Quantum Executor，一种后端无关的量子执行引擎，旨在统一异构平台上的量子实验。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算从理论走向实际应用，对强大、便携和可扩展的量子软件工具的需求日益增长。

Method: Quantum Executor提供声明式和模块化接口，支持异步和分布式执行，并实现实验设计与后端执行的解耦。

Result: 通过实际用例展示其在自动基准测试和混合验证中的适用性，证明其能简化量子开发流程。

Conclusion: 文章总结了当前局限性，并规划了未来改进方向。

Abstract: As quantum computing evolves from theoretical promise to practical
deployment, the demand for robust, portable, and scalable tools for quantum
software experimentation is growing. This paper introduces Quantum Executor, a
backend-agnostic execution engine designed to orchestrate quantum experiments
across heterogeneous platforms. Quantum Executor provides a declarative and
modular interface that decouples experiment design from backend execution,
enabling seamless interoperability and code reuse across diverse quantum and
classical resources. Key features include support for asynchronous and
distributed execution, customizable execution strategies and a unified API for
managing quantum experiments. We illustrate its applicability through two
life-like usage scenarios such as automated benchmarking and hybrid validation,
discussing its capacity to streamline quantum development. We conclude by
discussing current limitations and outlining a roadmap for future enhancements.

</details>


### [52] [ProvideQ: A Quantum Optimization Toolbox](https://arxiv.org/abs/2507.07649)
*Domenik Eichhorn,Nick Poser,Maximilian Schweikart,Ina Schaefer*

Main category: quant-ph

TL;DR: 该论文介绍了ProvideQ工具箱，旨在通过Meta-Solver策略无缝集成经典与量子计算，解决组合优化问题。


<details>
  <summary>Details</summary>
Motivation: 解决组合优化问题时，缺乏将量子解决方案与经典优化框架无缝集成的技术栈。

Method: 开发ProvideQ工具箱，支持通过Meta-Solver策略交互式分解问题为经典与量子子程序。

Result: 概念验证显示Meta-Solver策略已能应用量子子程序，但需更先进硬件提升性能。

Conclusion: ProvideQ工具箱为混合求解器提供了实用工具，但硬件发展是关键。

Abstract: Hybrid solvers for combinatorial optimization problems combine the advantages
of classical and quantum computing to overcome difficult computational
challenges. Although their theoretical performance seems promising, their
practical applicability is challenging due to the lack of a technological stack
that can seamlessly integrate quantum solutions with existing classical
optimization frameworks. We tackle this challenge by introducing the ProvideQ
toolbox, a software tool that enables users to easily adapt and configure
hybrid solvers via Meta-Solver strategies. A Meta-Solver strategy implements
decomposition techniques, which splits problems into classical and quantum
subroutines. The ProvideQ toolbox enables the interactive creation of such
decompositions via a Meta-Solver configuration tool. It combines
well-established classical optimization techniques with quantum circuits that
are seamlessly executable on multiple backends. This paper introduces the
technical details of the ProvideQ toolbox, explains its architecture, and
demonstrates possible applications for several real-world use cases. Our proof
of concept shows that Meta-Solver strategies already enable the application of
quantum subroutines today, however, more sophisticated hardware is required to
make their performance competitive.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [53] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 本文探讨了利用神经符号方法检测供应链中的非法活动，并比较了人工和自动特征提取的效果。


<details>
  <summary>Details</summary>
Motivation: 供应链中的非法活动（如假冒零件、强迫劳动）数据稀疏且不可靠，传统机器学习方法难以应对。

Method: 采用神经符号方法，结合大语言模型（LLM）的问题树方法，自动识别和量化相关新闻文章的关联性。

Result: 系统评估了人工与机器分类在供应链强迫劳动新闻文章中的差异。

Conclusion: 该方法能在数据稀疏的情况下有效检测供应链中的非法模式。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [54] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 论文探讨了在机器学习中为模型预测提供解释的重要性，尤其是在高风险领域。作者分析了基于规则的机器学习模型中存在的负面问题（如重叠和冗余），并开发了相关分析算法，发现广泛使用的工具会产生具有负面问题的规则集。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，错误的解释可能误导决策者。虽然可解释性是一个模糊的概念，但基于规则的可解释模型在这些领域被广泛使用。然而，这些模型存在负面问题（如重叠和冗余），需要深入分析其影响。

Method: 论文开发了算法，用于分析基于规则的机器学习模型中常见的负面问题，包括负重叠和冗余等问题。

Result: 研究发现，广泛使用的基于规则的机器学习工具会产生的规则集通常存在一种或多种负面问题。

Conclusion: 在高风险领域中，即使使用可解释的基于规则的模型，仍需谨慎其潜在的负面问题，当前的工具有待改进。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [55] [The Richness of CSP Non-redundancy](https://arxiv.org/abs/2507.07942)
*Joshua Brakensiek,Venkatesan Guruswami,Bart M. P. Jansen,Victor Lagerkvist,Magnus Wahlström*

Main category: cs.DM

TL;DR: 这篇论文探讨了约束满足问题（CSP）中的非冗余性（NRD）及其与多个计算机科学和数学问题的联系，首次展示了非冗余性如何随变量数量n的幂次增长，并对二元谓词的条件非冗余性进行了分类。


<details>
  <summary>Details</summary>
Motivation: 非冗余性是连接稀疏化、核化、查询复杂性、通用代数和极值组合学等多个重要问题的关键。论文旨在深入理解非冗余性的本质及其广泛影响。

Method: 论文首先展示了对于每个有理数r≥1，存在CSP谓词P使得P的非冗余性是Θ(n^r)；其次分类了二元谓词的条件非冗余性，并将其与极值组合学中的高围长图结构联系起来；进一步开发了条件非冗余性的代数理论。

Result: 1. 证明了非冗余性可以以n的任意有理数幂次增长；2. 完整分类了二元谓词的条件非冗余性；3. 提出了新的代数理论，并应用于Mal'tsev嵌入的研究。

Conclusion: 论文通过理论和代数方法，深化了对非冗余性的理解，为CSP和其他领域的相关问题提供了新的工具和视角。

Abstract: In the field of constraint satisfaction problems (CSP), a clause is called
redundant if its satisfaction is implied by satisfying all other clauses. An
instance of CSP$(P)$ is called non-redundant if it does not contain any
redundant clause. The non-redundancy (NRD) of a predicate $P$ is the maximum
number of clauses in a non-redundant instance of CSP$(P)$, as a function of the
number of variables $n$. Recent progress has shown that non-redundancy is
crucially linked to many other important questions in computer science and
mathematics including sparsification, kernelization, query complexity,
universal algebra, and extremal combinatorics. Given that non-redundancy is a
nexus for many of these important problems, the central goal of this paper is
to more deeply understand non-redundancy.
  Our first main result shows that for every rational number $r \ge 1$, there
exists a finite CSP predicate $P$ such that the non-redundancy of $P$ is
$\Theta(n^r)$. Our second main result explores the concept of conditional
non-redundancy first coined by Brakensiek and Guruswami [STOC 2025]. We
completely classify the conditional non-redundancy of all binary predicates
(i.e., constraints on two variables) by connecting these non-redundancy
problems to the structure of high-girth graphs in extremal combinatorics.
  Inspired by these concrete results, we build off the work of Carbonnel [CP
2022] to develop an algebraic theory of conditional non-redundancy. As an
application of this algebraic theory, we revisit the notion of Mal'tsev
embeddings, which is the most general technique known to date for establishing
that a predicate has linear non-redundancy. For example, we provide the first
example of predicate with a Mal'tsev embedding that cannot be attributed to the
structure of an Abelian group, but rather to the structure of the quantum Pauli
group.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [56] [Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task](https://arxiv.org/abs/2507.07327)
*Brian B. Vuong,Josie Davidson,Sangheui Cheon,Kyujin Cho,Allison M. Okamura*

Main category: cs.RO

TL;DR: 研究提出将触觉反馈从手部移至手腕，避免干扰手术机器人操作，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决手部触觉反馈对手术机器人操作干扰的问题，探索手腕触觉反馈的效果。

Method: 使用软气动手腕触觉设备，测试参与者在有无触觉反馈下的力控任务表现。

Result: 参与者在手腕触觉反馈下表现出显著更低的力误差，但操作时间更长。

Conclusion: 手腕触觉反馈能有效提升力控精度，但可能影响操作速度。

Abstract: Previous work has shown that the addition of haptic feedback to the hands can
improve awareness of tool-tissue interactions and enhance performance of
teleoperated tasks in robot-assisted minimally invasive surgery. However,
hand-based haptic feedback occludes direct interaction with the manipulanda of
surgeon console in teleoperated surgical robots. We propose relocating haptic
feedback to the wrist using a wearable haptic device so that haptic feedback
mechanisms do not need to be integrated into the manipulanda. However, it is
unknown if such feedback will be effective, given that it is not co-located
with the finger movements used for manipulation. To test if relocated haptic
feedback improves force application during teleoperated tasks using da Vinci
Research Kit (dVRK) surgical robot, participants learned to palpate a phantom
tissue to desired forces. A soft pneumatic wrist-worn haptic device with an
anchoring system renders tool-tissue interaction forces to the wrist of the
user. Participants performed the palpation task with and without wrist-worn
haptic feedback and were evaluated for the accuracy of applied forces.
Participants demonstrated statistically significant lower force error when
wrist-worn haptic feedback was provided. Participants also performed the
palpation task with longer movement times when provided wrist-worn haptic
feedback, indicating that the haptic feedback may have caused participants to
operate at a different point in the speed-accuracy tradeoff curve.

</details>


### [57] [FiDTouch: A 3D Wearable Haptic Display for the Finger Pad](https://arxiv.org/abs/2507.07661)
*Daria Trinitatova,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: FiDTouch是一种3D可穿戴触觉设备，通过微型倒置Delta机器人提供精确的接触和动态刺激，增强用户在人机交互中的沉浸感和效率。


<details>
  <summary>Details</summary>
Motivation: 指尖触觉设备在虚拟现实、医疗培训和远程机器人操作等领域具有潜力，通过改进用户体验和培训效果，推动新的交互形式。

Method: 开发了FiDTouch设备，利用微型倒置Delta机器人设计机制，提供接触、压力、皮肤拉伸和振动反馈等刺激。通过两阶段用户研究评估其性能。

Result: FiDTouch能够精确传递触觉和力刺激，提升用户在人机交互和机器人交互中的沉浸感和效率。

Conclusion: FiDTouch通过创新的设计，为指尖触觉反馈提供了高效解决方案，具有广泛的应用前景。

Abstract: The applications of fingertip haptic devices have spread to various fields
from revolutionizing virtual reality and medical training simulations to
facilitating remote robotic operations, proposing great potential for enhancing
user experiences, improving training outcomes, and new forms of interaction. In
this work, we present FiDTouch, a 3D wearable haptic device that delivers
cutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin
stretch, and vibrotactile feedback. The application of a tiny inverted Delta
robot in the mechanism design allows providing accurate contact and fast
changing dynamic stimuli to the finger pad surface. The performance of the
developed display was evaluated in a two-stage user study of the perception of
static spatial contact stimuli and skin stretch stimuli generated on the finger
pad. The proposed display, by providing users with precise touch and force
stimuli, can enhance user immersion and efficiency in the fields of
human-computer and human-robot interactions.

</details>


### [58] [A Graph Isomorphism-based Decentralized Algorithm for Modular Robot Configuration Formation](https://arxiv.org/abs/1602.03104)
*Ayan Dutta,Prithviraj Dasgupta,Carl Nelson*

Main category: cs.RO

TL;DR: 提出一种基于图同构的新算法，用于模块化机器人系统中的配置形成，以减少时间和能量消耗，并实现帕累托最优分配。


<details>
  <summary>Details</summary>
Motivation: 解决模块化机器人系统中模块初始配置和位置不同时，如何高效形成目标配置的问题。

Method: 基于图同构和效用框架的算法，模块选择目标配置中的位置，同时尽量保留原始配置。

Result: 算法具有完整性和帕累托最优性；实验显示规划时间短（100模块仅需毫秒级），且优于市场分配算法。

Conclusion: 所提算法在时间和通信效率上优于现有方法，适用于模块化机器人系统的配置形成。

Abstract: We consider the problem of configuration formation in modular robot systems
where a set of modules that are initially in different configurations and
located at different locations are required to assume appropriate positions so
that they can get into a new, user-specified, target configuration. We propose
a novel algorithm based on graph isomorphism, where the modules select
locations or spots in the target configuration using a utility-based framework,
while retaining their original configuration to the greatest extent possible,
to reduce the time and energy required by the modules to assume the target
configuration. We have shown analytically that our proposed algorithm is
complete and guarantees a Pareto-optimal allocation. Experimental simulations
of our algorithm with different number of modules in different initial
configurations and located initially at different locations, show that the
planning time of our algorithm is nominal (order of msec. for 100 modules). We
have also compared our algorithm against a market-based allocation algorithm
and shown that our proposed algorithm performs better in terms of time and
number of messages exchanged.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [59] [Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance](https://arxiv.org/abs/2507.07881)
*Roberto Ulloa,Juhi Kulshrestha,Celina Kacperski*

Main category: cs.CY

TL;DR: 研究表明，教育水平与对话式AI（CAI）的使用回避行为相关，低教育水平者回避率更高，强调设计需兼顾包容性以促进公平。


<details>
  <summary>Details</summary>
Motivation: 探讨教育差异是否导致对话式AI使用回避，揭示数字不平等问题。

Method: 通过在线实验（N=1,636）对比传统搜索与CAI（Perplexity AI）的任务回避行为，结合UTAUT2理论和LASSO回归分析。

Result: CAI组的任务回避率（51%）显著高于搜索组（30.9%）和对照组（16.8%），低教育者回避率达74.4%。

Conclusion: 教育是影响AI采用的关键因素，需通过包容性设计减少技术接入的不平等。

Abstract: The rise of conversational AI (CAI), powered by large language models, is
transforming how individuals access and interact with digital information.
However, these tools may inadvertently amplify existing digital inequalities.
This study investigates whether differences in formal education are associated
with CAI avoidance, leveraging behavioral data from an online experiment (N =
1,636). Participants were randomly assigned to a control or an
information-seeking task, either a traditional online search or a CAI
(Perplexity AI). Task avoidance (operationalized as survey abandonment or
providing unrelated responses during task assignment) was significantly higher
in the CAI group (51%) compared to the search (30.9%) and control (16.8%)
groups, with the highest CAI avoidance among participants with lower education
levels (~74.4%). Structural equation modeling based on the theoretical
framework UTAUT2 and LASSO regressions reveal that education is strongly
associated with CAI avoidance, even after accounting for various cognitive and
affective predictors of technology adoption. These findings underscore
education's central role in shaping AI adoption and the role of self-selection
biases in AI-related research, stressing the need for inclusive design to
ensure equitable access to emerging technologies.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [60] [Autonomous AI-based Cybersecurity Framework for Critical Infrastructure: Real-Time Threat Mitigation](https://arxiv.org/abs/2507.07416)
*Jenifer Paulraj,Brindha Raghuraman,Nagarani Gopalakrishnan,Yazan Otoum*

Main category: cs.CR

TL;DR: 论文提出了一种混合AI驱动的网络安全框架，用于增强关键基础设施的实时漏洞检测、威胁建模和自动修复。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施系统对社会稳定和经济韧性至关重要，但互联性增加使其面临多种网络威胁，如勒索软件、DoS攻击和APTs。

Method: 采用混合AI驱动的网络安全框架，结合实时漏洞检测、威胁建模和自动修复技术。

Result: 研究提供了可操作的见解，强化了对抗新兴网络威胁的安全性和韧性。

Conclusion: 论文强调了AI在关键基础设施网络安全中的作用，并提出了应对复杂性和监管挑战的解决方案。

Abstract: Critical infrastructure systems, including energy grids, healthcare
facilities, transportation networks, and water distribution systems, are
pivotal to societal stability and economic resilience. However, the increasing
interconnectivity of these systems exposes them to various cyber threats,
including ransomware, Denial-of-Service (DoS) attacks, and Advanced Persistent
Threats (APTs). This paper examines cybersecurity vulnerabilities in critical
infrastructure, highlighting the threat landscape, attack vectors, and the role
of Artificial Intelligence (AI) in mitigating these risks. We propose a hybrid
AI-driven cybersecurity framework to enhance real-time vulnerability detection,
threat modelling, and automated remediation. This study also addresses the
complexities of adversarial AI, regulatory compliance, and integration. Our
findings provide actionable insights to strengthen the security and resilience
of critical infrastructure systems against emerging cyber threats.

</details>


### [61] [Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling](https://arxiv.org/abs/2507.07250)
*Jordi Serra-Ruiz,David Megías*

Main category: cs.CR

TL;DR: 提出了一种半脆弱水印方案，用于多波段遥感图像，通过树结构矢量量化嵌入水印，以检测图像的显著修改。


<details>
  <summary>Details</summary>
Motivation: 现有水印技术多针对单波段图像，而多波段遥感图像需要更高效的水印嵌入和篡改检测方法。

Method: 利用像素签名和树结构矢量量化，将图像分割为三维块，构建每个块的量化树，并通过迭代算法嵌入水印。

Result: 方案能在有损压缩（超过阈值时）下保留水印，同时检测伪造块及其位置。

Conclusion: 该方法适用于多波段图像，具有半脆弱性和高效的篡改检测能力。

Abstract: A semi-fragile watermarking scheme for multiple band images is presented in
this article. We propose to embed a mark into remote sensing images applying a
tree-structured vector quantization approach to the pixel signatures instead of
processing each band separately. The signature of the multispectral or
hyperspectral image is used to embed the mark in it order to detect any
significant modification of the original image. The image is segmented into
three-dimensional blocks, and a tree-structured vector quantizer is built for
each block. These trees are manipulated using an iterative algorithm until the
resulting block satisfies a required criterion, which establishes the embedded
mark. The method is shown to be able to preserve the mark under lossy
compression (above a given threshold) but, at the same time, it detects
possibly forged blocks and their position in the whole image.

</details>


### [62] [Can Large Language Models Improve Phishing Defense? A Large-Scale Controlled Experiment on Warning Dialogue Explanations](https://arxiv.org/abs/2507.07916)
*Federico Maria Cau,Giuseppe Desolda,Francesco Greco,Lucio Davide Spano,Luca Viganò*

Main category: cs.CR

TL;DR: 研究探讨了大型语言模型（LLM）在生成钓鱼警告解释中的潜力，通过实验发现LLM生成的解释效果可媲美人工，且具备可扩展性。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击通过利用人类行为绕过技术防御，而现有警告对话框因内容静态、解释不清效果有限。研究旨在评估LLM是否能生成更清晰、简洁且可扩展的解释。

Method: 通过一项大规模用户研究（N=750），比较人工生成解释与两种LLM（Claude 3.5 Sonnet和Llama 3.3 70B）生成的解释的效果，分析了两种解释风格（基于特征和反事实）对行为和感知指标的影响。

Result: LLM生成的解释效果与人工相当甚至更优，Claude表现尤为突出。特征型解释对真实钓鱼更有效，反事实解释降低了误报率。其他因素如工作负荷、性别等也会影响警告效果。

Conclusion: LLM可自动生成高质量钓鱼警告解释，具备可扩展性和适应性，符合以人为本的价值观。

Abstract: Phishing has become a prominent risk in modern cybersecurity, often used to
bypass technological defences by exploiting predictable human behaviour.
Warning dialogues are a standard mitigation measure, but the lack of
explanatory clarity and static content limits their effectiveness. In this
paper, we report on our research to assess the capacity of Large Language
Models (LLMs) to generate clear, concise, and scalable explanations for
phishing warnings. We carried out a large-scale between-subjects user study (N
= 750) to compare the influence of warning dialogues supplemented with manually
generated explanations against those generated by two LLMs, Claude 3.5 Sonnet
and Llama 3.3 70B. We investigated two explanatory styles (feature-based and
counterfactual) for their effects on behavioural metrics (click-through rate)
and perceptual outcomes (e.g., trust, risk, clarity). The results indicate that
well-constructed LLM-generated explanations can equal or surpass manually
crafted explanations in reducing susceptibility to phishing; Claude-generated
warnings exhibited particularly robust performance. Feature-based explanations
were more effective for genuine phishing attempts, whereas counterfactual
explanations diminished false-positive rates. Other variables such as workload,
gender, and prior familiarity with warning dialogues significantly moderated
warning effectiveness. These results indicate that LLMs can be used to
automatically build explanations for warning users against phishing, and that
such solutions are scalable, adaptive, and consistent with human-centred
values.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [63] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

TL;DR: 提出了一种多级混合专家模型（MMoE）用于多模态实体链接（MEL），通过动态选择模态内容和增强提及语义来解决现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MEL方法未能解决提及语义模糊性和模态内容动态选择的问题，因此需要一种更高效的方法。

Method: 采用多级混合专家机制，包括描述感知的提及增强模块、多模态特征提取模块以及内外级专家混合模块。

Result: 实验表明MMoE在性能上优于现有最先进方法。

Conclusion: MMoE通过动态特征选择和语义增强，显著提升了多模态实体链接的效果。

Abstract: Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [64] [T-GVC: Trajectory-Guided Generative Video Coding at Ultra-Low Bitrates](https://arxiv.org/abs/2507.07633)
*Zhitao Wang,Hengyu Man,Wenrui Li,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 提出了一种轨迹引导生成视频编码框架（T-GVC），通过结合语义感知稀疏运动采样和训练无关的潜在空间引导机制，在超低码率下实现高质量视频重建。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频编码方法受限于领域特异性或对高层文本指导的过度依赖，导致运动细节丢失和重建不真实。

Method: 采用语义感知稀疏运动采样管道提取像素级运动轨迹，并结合轨迹对齐损失约束的扩散过程。

Result: 在超低码率条件下优于传统编解码器和最先进的端到端视频压缩方法，同时实现了比现有文本引导方法更精确的运动控制。

Conclusion: T-GVC为基于几何运动建模的生成视频编码提供了新方向。

Abstract: Recent advances in video generation techniques have given rise to an emerging
paradigm of generative video coding, aiming to achieve semantically accurate
reconstructions in Ultra-Low Bitrate (ULB) scenarios by leveraging strong
generative priors. However, most existing methods are limited by domain
specificity (e.g., facial or human videos) or an excessive dependence on
high-level text guidance, which often fails to capture motion details and
results in unrealistic reconstructions. To address these challenges, we propose
a Trajectory-Guided Generative Video Coding framework (dubbed T-GVC). T-GVC
employs a semantic-aware sparse motion sampling pipeline to effectively bridge
low-level motion tracking with high-level semantic understanding by extracting
pixel-wise motion as sparse trajectory points based on their semantic
importance, not only significantly reducing the bitrate but also preserving
critical temporal semantic information. In addition, by incorporating
trajectory-aligned loss constraints into diffusion processes, we introduce a
training-free latent space guidance mechanism to ensure physically plausible
motion patterns without sacrificing the inherent capabilities of generative
models. Experimental results demonstrate that our framework outperforms both
traditional codecs and state-of-the-art end-to-end video compression methods
under ULB conditions. Furthermore, additional experiments confirm that our
approach achieves more precise motion control than existing text-guided
methods, paving the way for a novel direction of generative video coding guided
by geometric motion modeling.

</details>


### [65] [SpatialViz-Bench: Automatically Generated Spatial Visualization Reasoning Tasks for MLLMs](https://arxiv.org/abs/2507.07610)
*Siting Wang,Luoyang Sun,Cheng Deng,Kun Shao,Minnan Pei,Zheng Tian,Haifeng Zhang,Jun Wang*

Main category: cs.CV

TL;DR: 本文介绍了SpatialViz-Bench，一个用于评估多模态大语言模型（MLLMs）在空间可视化任务中新能力的综合基准。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法通常依赖于与训练数据重叠的IQ测试或数学竞赛，可靠性不足，因此需要专门的基准来评估MLLMs的空间可视化能力。

Method: 提出了包含12个任务、1,180个自动生成问题的SpatialViz-Bench基准，并对33种先进MLLMs进行评估。

Result: 评估发现模型在空间可视化任务中存在性能差异大、与人类直觉不符的困难感知、2D到3D的性能急剧下降等问题。

Conclusion: 研究表明当前MLLMs在空间可视化任务中存在显著缺陷，SpatialViz-Bench填补了这一领域的评估空白。

Abstract: Humans can directly imagine and manipulate visual images in their minds, a
capability known as spatial visualization. While multi-modal Large Language
Models (MLLMs) support imagination-based reasoning, spatial visualization
remains insufficiently evaluated, typically embedded within broader
mathematical and logical assessments. Existing evaluations often rely on IQ
tests or math competitions that may overlap with training data, compromising
assessment reliability. To this end, we introduce SpatialViz-Bench, a
comprehensive multi-modal benchmark for spatial visualization with 12 tasks
across 4 sub-abilities, comprising 1,180 automatically generated problems. Our
evaluation of 33 state-of-the-art MLLMs not only reveals wide performance
variations and demonstrates the benchmark's strong discriminative power, but
also uncovers counter-intuitive findings: models exhibit unexpected behaviors
by showing difficulty perception that misaligns with human intuition,
displaying dramatic 2D-to-3D performance cliffs, and defaulting to formula
derivation despite spatial tasks requiring visualization alone. SpatialVizBench
empirically demonstrates that state-of-the-art MLLMs continue to exhibit
deficiencies in spatial visualization tasks, thereby addressing a significant
lacuna in the field. The benchmark is publicly available.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [66] [Audio-Visual Speech Separation via Bottleneck Iterative Network](https://arxiv.org/abs/2507.07270)
*Sidong Zhang,Shiv Shankar,Trang Nguyen,Andrea Fanelli,Madalina Fiterau*

Main category: cs.SD

TL;DR: Bottleneck Iterative Network (BIN) 通过轻量级融合块和瓶颈表示方法提升语音分离模型的性能，同时减少训练和推理时间。


<details>
  <summary>Details</summary>
Motivation: 整合非听觉信息可以提高语音分离模型的性能，但现有方法通常因模态特定网络而过于昂贵或性能不足。

Method: 提出 BIN，通过迭代表示细化和瓶颈融合表示来平衡模型性能与代价。

Result: 在 NTCD-TIMIT 和 LRS3+WHAM! 数据集上，BIN 在 SI-SDRi 指标上优于现有模型，并减少超 50% 的训练和推理时间。

Conclusion: BIN 在提升性能的同时显著降低了计算成本，适用于语音分离任务。

Abstract: Integration of information from non-auditory cues can significantly improve
the performance of speech-separation models. Often such models use deep
modality-specific networks to obtain unimodal features, and risk being too
costly or lightweight but lacking capacity. In this work, we present an
iterative representation refinement approach called Bottleneck Iterative
Network (BIN), a technique that repeatedly progresses through a lightweight
fusion block, while bottlenecking fusion representations by fusion tokens. This
helps improve the capacity of the model, while avoiding major increase in model
size and balancing between the model performance and training cost. We test BIN
on challenging noisy audio-visual speech separation tasks, and show that our
approach consistently outperforms state-of-the-art benchmark models with
respect to SI-SDRi on NTCD-TIMIT and LRS3+WHAM! datasets, while simultaneously
achieving a reduction of more than 50% in training and GPU inference time
across nearly all settings.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [67] [Discrete Beamforming Optimization for RISs with a Limited Phase Range and Amplitude Attenuation](https://arxiv.org/abs/2507.07342)
*Dogan Kutay Pekcan,Hongyi Liao,Ender Ayanoglu*

Main category: eess.SP

TL;DR: 论文研究了通过可重构智能表面（RIS）最大化用户设备接收功率的问题，提出了最优搜索算法和量化框架，分析了离散相位数量对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决在RIS中由于相位依赖幅值（PDA）和离散相位偏移带来的接收功率优化问题。

Method: 提出了最优搜索算法和APQ量化框架，扩展为EAPQ算法。

Result: 离散相位数量超过4时增益有限；当相位范围受限时，性能对衰减敏感。

Conclusion: 最优算法为RIS离散波束成形提供了通用性能上限。

Abstract: This paper addresses the problem of maximizing the received power at a user
equipment via reconfigurable intelligent surface (RIS) characterized by
phase-dependent amplitude (PDA) and discrete phase shifts over a limited phase
range. Given complex RIS coefficients, that is, discrete phase shifts and PDAs,
we derive the necessary and sufficient conditions to achieve the optimal
solution. To this end, we propose an optimal search algorithm that is proven to
converge in linear time within at most NK steps, significantly outperforming
the exhaustive search approach that would otherwise be needed for RISs with
amplitude attenuation. Furthermore, we introduce a practical quantization
framework for PDA-introduced RISs termed amplitude-introduced polar
quantization (APQ), and extend it to a novel algorithm named extended
amplitude-introduced polar quantization (EAPQ) that works with geometric
projections. We derive closed-form expressions to assess how closely the
performance of the proposed RIS configuration can approximate the ideal case
with continuous phases and no attenuation. Our analysis reveals that increasing
the number of discrete phases beyond K = 4 yields only marginal gains,
regardless of attenuation levels, provided the RIS has a sufficiently wide
phase range R. Furthermore, we also show and quantify that when the phase range
R is limited, the performance is sensitive to attenuation for larger R, and
sensitive to R when there is less attenuation. Finally, the proposed optimal
algorithm provides a generic upper bound that could serve as a benchmark for
discrete beamforming in RISs with amplitude constraints.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [68] [A 2-categorical approach to the semantics of dependent type theory with computation axioms](https://arxiv.org/abs/2507.07208)
*Matteo Spadetto*

Main category: math.LO

TL;DR: 该论文提出了一个基于2维范畴理论的公理类型论语义描述方法，通过松弛Garner的2维范畴要求，扩展了对公理类型论的语义解释，并证明其在显示映射2-范畴中是良好定义且具有健全性的。


<details>
  <summary>Details</summary>
Motivation: 为了克服在1维范畴中编码公理类型论的困难，论文从2维范畴的角度出发，寻找一种有效的方法来描述公理类型论的语义。

Method: 采用Richard Garner的2维范畴方法，松弛其对内涵类型的2维范畴要求，将公理类型论的类型构造子编码为自然2维范畴理论数据，通过显示映射2-范畴模型展示语义。

Result: 证明了公理类型论在显示映射2-范畴中的解释是良好定义且具有健全性的，并通过语义方法证明内涵类型的计算规则在公理类型论中不可接受。

Conclusion: 论文扩展了Garner的内涵类型语义描述方法，适用于公理类型论，并通过具体模型验证了其语义的可行性和局限性。

Abstract: Axiomatic type theory is a dependent type theory without computation rules.
The term equality judgements that usually characterise these rules are replaced
by computation axioms, i.e., additional term judgements that are typed by
identity types. This paper is devoted to providing an effective description of
its semantics, from a higher categorical perspective: given the challenge of
encoding intensional type formers into 1-dimensional categorical terms and
properties, a challenge that persists even for axiomatic type formers, we adopt
Richard Garner's approach in the 2-dimensional study of dependent types. We
prove that the type formers of axiomatic theories can be encoded into natural
2-dimensional category theoretic data, obtaining a presentation of the
semantics of axiomatic type theory via 2-categorical models called display map
2-categories. In the axiomatic case, the 2-categorical requirements identified
by Garner for interpreting intensional type formers are relaxed. Therefore, we
obtain a presentation of the semantics of the axiomatic theory that generalises
Garner's one for the intensional case. Our main result states that the
interpretation of axiomatic theories within display map 2-categories is
well-defined and enjoys the soundness property. We use this fact to provide a
semantic proof that the computation rule of intensional identity types is not
admissible in axiomatic type theory. This is achieved via a revisitation of
Hofmann and Streicher's groupoid model that believes axiomatic identity types
but does not believe intensional ones.

</details>
