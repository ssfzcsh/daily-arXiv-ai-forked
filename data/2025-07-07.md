<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 16]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NI](#cs.NI) [Total: 8]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 19]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.AI](#cs.AI) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [How do Software Engineering Candidates Prepare for Technical Interviews?](https://arxiv.org/abs/2507.02068)
*Brian Bell,Teresa Thomas,Sang Won Lee,Chris Brown*

Main category: cs.SE

TL;DR: 总结：研究探讨了软件工程师候选人如何准备技术面试，发现缺乏真实训练和课程支持导致压力和准备不足。


<details>
  <summary>Details</summary>
Motivation: 动机：技术面试的复杂性难以准备且计算课程中较少涉及，因此需要了解候选人如何准备以及教育的作用。

Method: 方法：对131名正在准备技术面试的候选人进行了调查。

Result: 结果：候选人很少在真实环境中训练，课程也未能支持准备，导致压力和准备不足。

Conclusion: 结论：研究为利益相关者提供了增强技术面试准备的建议。

Abstract: To obtain employment, aspiring software engineers must complete technical
interviews -- a hiring process which involves candidates writing code while
communicating to an audience. However, the complexities of tech interviews are
difficult to prepare for and seldom faced in computing curricula. To this end,
we seek to understand how candidates prepare for technical interviews,
investigating the effects of preparation methods and the role of education. We
distributed a survey to candidates (n = 131) actively preparing for technical
interviews. Our results suggest candidates rarely train in authentic settings
and courses fail to support preparation efforts -- leading to stress and
unpreparedness. Based on our findings, we provide implications for stakeholders
to enhance tech interview preparation for candidates pursuing software
engineering roles.

</details>


### [2] [Structural Code Search using Natural Language Queries](https://arxiv.org/abs/2507.02107)
*Ben Limpanukorn,Yanjun Wang,Zach Patterson,Pranav Garg,Murali Krishna Ramanathan,Xiaofei Ma,Anoop Deoras,Miryung Kim*

Main category: cs.SE

TL;DR: 提出一种基于自然语言的结构化代码搜索方法，结合LLM和现有工具，显著提升搜索效果。


<details>
  <summary>Details</summary>
Motivation: 开发者常用关键词和正则表达式搜索代码，但结构化搜索工具使用门槛高。

Method: 结合LLM解释自然语言查询与结构化搜索工具，实例化于Semgrep和GQL。

Result: 在400个查询的基准测试中，精度和召回率达55%-70%，优于基线方法。

Conclusion: 自然语言结构化代码搜索方法有效且鲁棒，显著降低使用门槛并提升性能。

Abstract: Searching code is a common task that developers perform to understand APIs,
learn common code patterns, and navigate code. Currently, developers most
commonly search using keywords and regular expressions that are easy to use and
widely available. Beyond keywords and regular expressions, structural code
search tools allow developers to search for code based on its syntactic
structure. This has numerous applications ranging from bug finding to
systematically refactoring code. However, these structural code search tools
operate on queries expressed in domain-specific languages (DSL) that can be
difficult to learn and write. We propose to allow developers to use natural
language to search for code structurally. Expressing queries in natural
language provides an intuitive way to search for code and lowers the barrier to
entry.
  In this work, we develop a novel general approach that combines the reasoning
capabilities of an LLM to interpret natural language search queries with the
power of structural search tools to efficiently and accurately retrieve
relevant code. We then instantiate this approach for two structural code search
DSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for
structural code search consisting of 400 queries over 10 Java projects. We show
that our approach for structural code search based on translating NL queries to
DSL queries using an LLM is effective and robust, achieving a high precision
and recall ranging from 55% - 70%. Further, our approach significantly
outperforms baselines based on semantic code search and LLM retrievals by up to
57% and 14% on F1 scores.

</details>


### [3] [Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!](https://arxiv.org/abs/2507.02110)
*Md Nahidul Islam Opu,Fatima Islam Mouri,Rick Kazman,Yuanfang Cai,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 预测移动应用发布前的受欢迎程度对开发者在竞争激烈的市场中具有战略意义，但仍是挑战。本研究通过分析开源Android应用的代码指标，发现内部代码指标可作为受欢迎程度的指示器。


<details>
  <summary>Details</summary>
Motivation: 移动应用市场的竞争激烈，预测应用发布前的受欢迎程度有助于开发者优化策略，但目前尚无有效方法。

Method: 使用446个开源Android应用的代码指标、用户评分和下载数据，通过回归和分类模型（如多层感知机）进行分析，比较不同特征集的效果。

Result: 回归模型表现不佳（R²低），但分类模型（尤其是多层感知机）表现较好（F1=0.72），表明内部代码指标可作为受欢迎程度的指示器。

Conclusion: 内部代码指标虽解释力有限，但可作为预测应用受欢迎程度的有效工具，挑战了此前否定其预测价值的结论。

Abstract: Predicting mobile app popularity before release can provide developers with a
strategic advantage in a competitive marketplace, yet it remains a challenging
problem. This study explores whether internal software metrics, measurable from
source code before deployment, can predict an app's popularity, defined by user
ratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).
Using a dataset of 446 open-source Android apps from F-Droid, we extract a wide
array of features, including system-, class-, and method-level code metrics,
code smells, and app metadata. Additional information, such as user reviews,
download counts, and uses-permission, was collected from the Google Play Store.
We evaluate regression and classification models across three feature sets: a
minimal Size-only baseline, a domain-informed Handpicked set, and a Voting set
derived via feature selection algorithms. Regression models perform poorly due
to skewed data, with low $R^2$ scores. However, when reframed as binary
classification (Popular vs. Unpopular), results improve significantly. The best
model, a Multilayer Perceptron using the Voting set, achieves F1-scores of
0.72. These results suggest that internal code metrics, although limited in
their explanatory power, can serve as useful indicators of app popularity. This
challenges earlier findings that dismissed internal metrics as predictors of
software quality.

</details>


### [4] [A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights](https://arxiv.org/abs/2507.02118)
*Cristina Martinez Montes,Daniela Grassi,Nicole Novielli,Birgit Penzenstadle*

Main category: cs.SE

TL;DR: 研究比较了心理测量压力指标与生物特征指标，并试图通过编程任务识别生物数据中的压力模式，但未显著诱导出压力。


<details>
  <summary>Details</summary>
Motivation: 传统自我报告工具可能存在偏差，研究探索结合生物特征与心理测量方法以更客观评估压力。

Method: 实验设计包含预调查、穿戴生物传感器编程任务、后调查及简短退出访谈。

Result: 心理测量未显示压力；访谈反馈混合感受；生物数据中仅EDA阶段性峰值显著差异。

Conclusion: 时间限制诱导压力不足，提供未来研究方法参考。

Abstract: The study of well-being, stress and other human factors has traditionally
relied on self-report instruments to assess key variables. However, concerns
about potential biases in these instruments, even when thoroughly validated and
standardised, have driven growing interest in alternatives in combining these
measures with more objective methods, such as physiological measures.
  We aimed to (i) compare psychometric stress measures and biometric indicators
and (ii) identify stress-related patterns in biometric data during software
engineering tasks.
  We conducted an experiment where participants completed a pre-survey, then
programmed two tasks wearing biometric sensors, answered brief post-surveys for
each, and finally went through a short exit interview.
  Our results showed diverse outcomes; we found no stress in the psychometric
instruments. Participants in the interviews reported a mix of feeling no stress
and experiencing time pressure. Finally, the biometrics showed a significant
difference only in EDA phasic peaks.
  We conclude that our chosen way of inducing stress by imposing a stricter
time limit was insufficient. We offer methodological insights for future
studies working with stress, biometrics, and psychometric instruments.

</details>


### [5] [Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection](https://arxiv.org/abs/2507.02137)
*Martin Obaidi,Marc Herrmann,Jil Klünder,Kurt Schneider*

Main category: cs.SE

TL;DR: 该研究分析了10个开发者沟通数据集的特征，评估了14种情感分析工具的性能，并提出了一种基于数据集特征的映射方法和问卷，以推荐适合的工具。结果显示，数据集特征对工具选择有显著影响，而Transformer模型表现稳定但依赖于上下文。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决现有情感分析工具在不同平台数据集上性能不一致的问题，支持软件工程中可信的AI驱动分析。

Method: 分析5个平台的10个开发者沟通数据集的统计和语言特征，评估14种情感分析工具的性能，并提出映射方法和推荐问卷。

Result: 数据集特征显著影响工具选择，Transformer模型（如SetFit和RoBERTa）表现稳定但依赖上下文。提出的方法能有效推荐工具。

Conclusion: 研究为软件工程中情感分析工具的选择提供了实用方法，同时强调需持续评估以应对不断变化的沟通环境。

Abstract: Software development relies heavily on text-based communication, making
sentiment analysis a valuable tool for understanding team dynamics and
supporting trustworthy AI-driven analytics in requirements engineering.
However, existing sentiment analysis tools often perform inconsistently across
datasets from different platforms, due to variations in communication style and
content.
  In this study, we analyze linguistic and statistical features of 10 developer
communication datasets from five platforms and evaluate the performance of 14
sentiment analysis tools. Based on these results, we propose a mapping approach
and questionnaire that recommends suitable sentiment analysis tools for new
datasets, using their characteristic features as input.
  Our results show that dataset characteristics can be leveraged to improve
tool selection, as platforms differ substantially in both linguistic and
statistical properties. While transformer-based models such as SetFit and
RoBERTa consistently achieve strong results, tool effectiveness remains
context-dependent. Our approach supports researchers and practitioners in
selecting trustworthy tools for sentiment analysis in software engineering,
while highlighting the need for ongoing evaluation as communication contexts
evolve.

</details>


### [6] [Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models](https://arxiv.org/abs/2507.02182)
*Fangjian Lei,Jiawen Liu,Shayan Noei,Ying Zou,Derek Truong,William Alexander*

Main category: cs.SE

TL;DR: 提出一种多智能体方法，利用两个LLM协作生成COBOL代码解释，解决了LLM令牌窗口限制问题，并在功能、文件和项目级别取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: COBOL代码维护困难，缺乏文档且开发者减少，现有LLM方法因COBOL特殊性及令牌限制效果有限。

Method: 采用双LLM智能体协作，结合代码库上下文生成功能、文件和项目级别的解释。

Result: 在14个COBOL项目上验证，功能级别METEOR、chrF、SentenceBERT分别提升12.67%、18.59%和0.62%；文件级别解释在目的、功能和清晰度上分别超越基线4.21%、10.72%和14.68%。

Conclusion: 多智能体方法有效解决了COBOL代码解释的挑战，显著优于基线，适用于大规模COBOL系统。

Abstract: Common Business Oriented Language (COBOL) is a programming language used to
develop business applications that are widely adopted by financial, business,
and government agencies. Due to its age, complexity, and declining number of
COBOL developers, maintaining COBOL codebases is becoming increasingly
challenging. In particular, the lack of documentation makes it difficult for
new developers to effectively understand and maintain COBOL systems. Existing
research utilizes large language models (LLMs) to explain the functionality of
code snippets. However, COBOL presents unique challenges due to its
architectural and syntactical differences, which often cause its code to exceed
the token window size of LLMs. In this work, we propose a multi-agent approach
that leverages two LLM-based agents working collaboratively to generate
explanations for functions, files, and the overall project. These agents
incorporate together by utilizing contextual information from the codebase into
the code explanation prompts. We evaluate the effectiveness of our approach
using 14 open-source, real-world COBOL projects. Our results indicate that our
approach performs significantly better than the baseline in function code
explanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,
chrF, and SentenceBERT scores, respectively. At the file level, our approach
effectively explains both short and long COBOL files that exceed the token
window size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in
explaining the purpose, functionality, and clarity of the generated
explanation. At the project level, our approach generates explanations that
convey the functionality and purpose of 82% of the selected projects.

</details>


### [7] [Precisely Detecting Python Type Errors via LLM-based Unit Test Generation](https://arxiv.org/abs/2507.02318)
*Chen Yang,Ziqi Wang,Yanjie Jiang,Lin Yang,Yuteng Zheng,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: RTED是一种新颖的类型感知测试生成技术，通过结合逐步类型约束分析和反射验证，有效减少Python类型错误的误报率，并在基准测试中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: Python中的类型错误常导致运行时故障，影响软件可靠性和开发效率。现有静态分析工具误报率高，而单元测试生成技术缺乏针对性指导。

Method: RTED通过逐步类型约束分析与反射验证相结合，指导测试生成，降低误报率。

Result: 在BugsInPy和TypeBugs基准测试中，RTED检测到比现有技术多22-29个类型错误，误报率降低173.9%-245.9%，并发现12个未知错误。

Conclusion: RTED在Python类型错误检测中表现出色，优于现有技术，并能实际应用于开源项目。

Abstract: Type errors in Python often lead to runtime failures, posing significant
challenges to software reliability and developer productivity. Existing static
analysis tools aim to detect such errors without execution but frequently
suffer from high false positive rates. Recently, unit test generation
techniques offer great promise in achieving high test coverage, but they often
struggle to produce bug-revealing tests without tailored guidance. To address
these limitations, we present RTED, a novel type-aware test generation
technique for automatically detecting Python type errors. Specifically, RTED
combines step-by-step type constraint analysis with reflective validation to
guide the test generation process and effectively suppress false positives. We
evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.
Experimental results show that RTED can detect 22-29 more benchmarked type
errors than four state-of-the-art techniques. RTED is also capable of producing
fewer false positives, achieving an improvement of 173.9%-245.9% in precision.
Furthermore, RTED successfully discovered 12 previously unknown type errors
from six real-world open-source Python projects.

</details>


### [8] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: 论文提出了一个名为VeFIA的框架，用于审核垂直联邦学习（VFL）中数据方的推理软件执行正确性，保护数据隐私且不增加延迟。


<details>
  <summary>Details</summary>
Motivation: 现有VFL方法缺乏对数据方推理软件执行正确性的审核机制。

Method: 设计VeFIA框架，利用可信执行环境（TEE）和协调者验证数据方的计算结果。

Result: VeFIA能检测5.4%以上的异常推理，准确率达99.99%，且不影响在线推理速度。

Conclusion: VeFIA是首个讨论VFL中推理软件执行正确性的方案，验证效果优异。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


### [9] [Meta-Fair: AI-Assisted Fairness Testing of Large Language Models](https://arxiv.org/abs/2507.02533)
*Miguel Romero-Arjona,José A. Parejo,Juan C. Alonso,Ana B. Sánchez,Aitor Arrieta,Sergio Segura*

Main category: cs.SE

TL;DR: 本文提出了一种名为Meta-Fair的新方法，用于自动化测试大型语言模型（LLMs）的公平性，通过变形测试和LLM的自生成与评估能力，显著降低了资源需求并提高了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM公平性测试方法依赖人工评估和固定模板，资源密集且难以扩展。本文旨在开发一种自动化方法，减少对领域特定资源的依赖。

Method: Meta-Fair结合变形测试（通过输入提示的受控修改发现偏差）和LLM的生成与评估能力，并通过开源工具支持测试案例的生成与执行。

Result: 实验涉及12个预训练LLM和14个变形关系，显示Meta-Fair在发现偏差方面平均精度达92%，并揭示29%的执行存在偏差行为。LLM作为评估器表现可靠。

Conclusion: Meta-Fair为LLM公平性测试提供了一种高效自动化途径，尽管存在非确定性挑战，但通过精心设计可缓解。结果显示了自动化测试的潜力。

Abstract: Fairness--the absence of unjustified bias--is a core principle in the
development of Artificial Intelligence (AI) systems, yet it remains difficult
to assess and enforce. Current approaches to fairness testing in large language
models (LLMs) often rely on manual evaluation, fixed templates, deterministic
heuristics, and curated datasets, making them resource-intensive and difficult
to scale. This work aims to lay the groundwork for a novel, automated method
for testing fairness in LLMs, reducing the dependence on domain-specific
resources and broadening the applicability of current approaches. Our approach,
Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to
uncover bias by examining how model outputs vary in response to controlled
modifications of input prompts, defined by metamorphic relations (MRs). Second,
we propose exploiting the potential of LLMs for both test case generation and
output evaluation, leveraging their capability to generate diverse inputs and
classify outputs effectively. The proposal is complemented by three open-source
tools supporting LLM-driven generation, execution, and evaluation of test
cases. We report the findings of several experiments involving 12 pre-trained
LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.
The results show that Meta-Fair is effective in uncovering bias in LLMs,
achieving an average precision of 92% and revealing biased behaviour in 29% of
executions. Additionally, LLMs prove to be reliable and consistent evaluators,
with the best-performing models achieving F1-scores of up to 0.79. Although
non-determinism affects consistency, these effects can be mitigated through
careful MR design. While challenges remain to ensure broader applicability, the
results indicate a promising path towards an unprecedented level of automation
in LLM testing.

</details>


### [10] [LLMREI: Automating Requirements Elicitation Interviews with LLMs](https://arxiv.org/abs/2507.02564)
*Alexander Korn,Samuel Gorsch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: LLMREI是一个基于大语言模型的聊天机器人，旨在自动化需求获取面试，减少人为错误并提升效率。研究使用了两种提示方法，并在模拟面试中评估了其表现。


<details>
  <summary>Details</summary>
Motivation: 传统需求获取面试依赖分析师，成本高且易出错。利用大语言模型自动化这一过程可以提升效率和规模。

Method: 设计了LLMREI机器人，测试了零样本提示和逐步提示两种方法，放弃了微调方法。在33次模拟面试中评估其表现。

Result: LLMREI的错误率与人类分析师相当，能高效提取需求，并能根据上下文生成相关问题。

Conclusion: LLMREI适用于大量利益相关者的自动化需求获取，提升效率并减少人为错误。

Abstract: Requirements elicitation interviews are crucial for gathering system
requirements but heavily depend on skilled analysts, making them
resource-intensive, susceptible to human biases, and prone to miscommunication.
Recent advancements in Large Language Models present new opportunities for
automating parts of this process. This study introduces LLMREI, a chat bot
designed to conduct requirements elicitation interviews with minimal human
intervention, aiming to reduce common interviewer errors and improve the
scalability of requirements elicitation. We explored two main approaches,
zero-shot prompting and least-to-most prompting, to optimize LLMREI for
requirements elicitation and evaluated its performance in 33 simulated
stakeholder interviews. A third approach, fine-tuning, was initially considered
but abandoned due to poor performance in preliminary trials. Our study assesses
the chat bot's effectiveness in three key areas: minimizing common interview
errors, extracting relevant requirements, and adapting its questioning based on
interview context and user responses. Our findings indicate that LLMREI makes a
similar number of errors compared to human interviewers, is capable of
extracting a large portion of requirements, and demonstrates a notable ability
to generate highly context-dependent questions. We envision the greatest
benefit of LLMREI in automating interviews with a large number of stakeholders.

</details>


### [11] [Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems](https://arxiv.org/abs/2507.02578)
*Zoe Pfister*

Main category: cs.SE

TL;DR: 本文探讨了如何在适应性信息物理系统（CPS）中实现有效的人机协作（HMT），提出了整合HMT的新方法和伦理框架。


<details>
  <summary>Details</summary>
Motivation: 适应性CPS中实现无缝人机协作面临挑战，尤其是如何将人类操作者整合到反馈循环中并保护其隐私和价值观。

Method: 开发了整合HMT的新方法，并构建了从需求工程开始的伦理和人类价值观验证框架。

Result: 提出了解决CPS中人机协作障碍和伦理问题的系统性方法。

Conclusion: 本研究为适应性CPS中的HMT提供了理论和实践支持，强调伦理和人类价值观的重要性。

Abstract: Adaptive Cyber-Physical Systems (CPS) are systems that integrate both
physical and computational capabilities, which can adjust in response to
changing parameters. Furthermore, they increasingly incorporate human-machine
collaboration, allowing them to benefit from the individual strengths of humans
and machines. Human-Machine Teaming (HMT) represents the most advanced paradigm
of human-machine collaboration, envisioning seamless teamwork between humans
and machines. However, achieving effective and seamless HMT in adaptive CPS is
challenging. While adaptive CPS already benefit from feedback loops such as
MAPE-K, there is still a gap in integrating humans into these feedback loops
due to different operational cadences of humans and machines. Further, HMT
requires constant monitoring of human operators, collecting potentially
sensitive information about their actions and behavior. Respecting the privacy
and human values of the actors of the CPS is crucial for the success of
human-machine teams. This research addresses these challenges by: (1)
developing novel methods and processes for integrating HMT into adaptive CPS,
focusing on human-machine interaction principles and their incorporation into
adaptive feedback loops found in CPS, and (2) creating frameworks for
integrating, verifying, and validating ethics and human values throughout the
system lifecycle, starting from requirements engineering.

</details>


### [12] [Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?](https://arxiv.org/abs/2507.02665)
*Timo Kehrer,Robert Haines,Guido Juckeland,Shurui Zhou,David E. Bernholdt*

Main category: cs.SE

TL;DR: 研究软件工程师（RSEs）与软件工程研究者（SERs）在术语使用上存在差异，导致沟通困难。本研究通过调查SE基础知识在RSE社区中的解读，发现双方可以互相学习与合作的可能性，并提出了一种系统化的术语映射方法。


<details>
  <summary>Details</summary>
Motivation: RSEs和SERs在术语使用上存在分歧，影响了双方的沟通与合作，因此有必要探究这些差异并寻找解决方案。

Method: 通过调查SE基础知识在RSE社区中的解读，识别概念对齐、知识差距和潜在适应领域，并提出了一套系统化的术语映射方法。

Result: 初步发现表明双方存在互相学习和合作的机会，术语映射方法为未来的众包扩展和验证奠定了基础。

Conclusion: 研究表明，RSEs与SERs之间的术语差异可以通过系统化的方法来弥合，未来可以通过众包进一步扩展与验证这些发现。

Abstract: Anecdotal evidence suggests that Research Software Engineers (RSEs) and
Software Engineering Researchers (SERs) often use different terminologies for
similar concepts, creating communication challenges. To better understand these
divergences, we have started investigating how SE fundamentals from the SER
community are interpreted within the RSE community, identifying aligned
concepts, knowledge gaps, and areas for potential adaptation. Our preliminary
findings reveal opportunities for mutual learning and collaboration, and our
systematic methodology for terminology mapping provides a foundation for a
crowd-sourced extension and validation in the future.

</details>


### [13] [RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes](https://arxiv.org/abs/2507.02690)
*Jiaxing Wang,Yifeng Yu,Jiahan Song,Bin Cao,Jing Fan,Ji Zhang*

Main category: cs.SE

TL;DR: RLHGNN是一种新颖的框架，通过将事件日志转化为异构图并结合强化学习来自适应建模业务流程中的顺序和非顺序关系，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效捕捉并行执行和条件依赖等非顺序关系，且图基方法存在同质表示和静态结构的局限性。

Method: RLHGNN将事件日志转化为异构图，包含三种边类型，并利用强化学习自动选择最优图结构，再通过异构图卷积预测下一活动。

Result: 在六个真实数据集上，RLHGNN表现优于现有方法，且推理延迟仅约1毫秒，适用于实时监控。

Conclusion: RLHGNN通过自适应建模复杂关系，为业务流程优化提供了一种高效且实用的解决方案。

Abstract: Next activity prediction represents a fundamental challenge for optimizing
business processes in service-oriented architectures such as microservices
environments, distributed enterprise systems, and cloud-native platforms, which
enables proactive resource allocation and dynamic service composition. Despite
the prevalence of sequence-based methods, these approaches fail to capture
non-sequential relationships that arise from parallel executions and
conditional dependencies. Even though graph-based approaches address structural
preservation, they suffer from homogeneous representations and static
structures that apply uniform modeling strategies regardless of individual
process complexity characteristics. To address these limitations, we introduce
RLHGNN, a novel framework that transforms event logs into heterogeneous process
graphs with three distinct edge types grounded in established process mining
theory. Our approach creates four flexible graph structures by selectively
combining these edges to accommodate different process complexities, and
employs reinforcement learning formulated as a Markov Decision Process to
automatically determine the optimal graph structure for each specific process
instance. RLHGNN then applies heterogeneous graph convolution with
relation-specific aggregation strategies to effectively predict the next
activity. This adaptive methodology enables precise modeling of both sequential
and non-sequential relationships in service interactions. Comprehensive
evaluation on six real-world datasets demonstrates that RLHGNN consistently
outperforms state-of-the-art approaches. Furthermore, it maintains an inference
latency of approximately 1 ms per prediction, representing a highly practical
solution suitable for real-time business process monitoring applications. The
source code is available at https://github.com/Joker3993/RLHGNN.

</details>


### [14] [Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms](https://arxiv.org/abs/2507.02695)
*Sahar Ahmadisakha,Lech Bialek,Mohamed Soliman,Vasilios Andrikopoulos*

Main category: cs.SE

TL;DR: 论文提出了一种名为‘可持续性标志’的方法，用于识别软件架构讨论中的可持续性概念，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着云计算和云架构的兴起，软件系统的可持续性备受关注，但缺乏明确的指导原则来识别相关讨论。

Method: 通过主题分析开发‘可持续性标志’，并在实验中评估其在识别云架构帖子中可持续性内容的效果。

Result: 初步结果显示，使用标志能减少误分类，提高确定性和性能，且比仅依赖定义更实用易懂。

Conclusion: 可持续性标志为识别软件架构讨论中的可持续性提供了有效工具，改善了识别性能。

Abstract: In recent years, sustainability in software systems has gained significant
attention, especially with the rise of cloud computing and the shift towards
cloud-based architectures. This shift has intensified the need to identify
sustainability in architectural discussions to take informed architectural
decisions. One source to see these decisions is in online Q&A forums among
practitioners' discussions. However, recognizing sustainability concepts within
software practitioners' discussions remains challenging due to the lack of
clear and distinct guidelines for this task. To address this issue, we
introduce the notion of sustainability flags as pointers in relevant
discussions, developed through thematic analysis of multiple sustainability
best practices from cloud providers. This study further evaluates the
effectiveness of these flags in identifying sustainability within cloud
architecture posts, using a controlled experiment. Preliminary results suggest
that the use of flags results in classifying fewer posts as
sustainability-related compared to a control group, with moderately higher
certainty and significantly improved performance. Moreover, sustainability
flags are perceived as more useful and understandable than relying solely on
definitions for identifying sustainability.

</details>


### [15] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文提出了一种基于文本蕴含和上下文学习的方法，自动生成法律文本的规范表示，可编码为Python代码，以解决法律合规性中的元数据提取问题。


<details>
  <summary>Details</summary>
Motivation: 小型企业和初创公司缺乏法律专业知识，法律文本复杂冗长，现有方法无法有效处理元数据间的关联性且依赖人工标注。

Method: 使用文本蕴含和上下文学习生成法律文本的规范表示，基于Python类的领域特定元模型，捕获结构和语义元数据及其关联。

Result: 在13个美国州数据泄露通知法律上测试，生成的表示通过89.4%的测试用例，精确率和召回率分别为82.2%和88.7%。

Conclusion: 该方法减少了对大规模人工标注数据的依赖，并能推广到新法规，具有较高的实用性和准确性。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [16] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 论文探讨了利用GPT-4生成需求访谈中的后续问题，以提升访谈质量和效率。实验显示，LLM生成的问题不亚于人工问题，且在指导下表现更优。


<details>
  <summary>Details</summary>
Motivation: 需求访谈中，访谈者面临认知负担和信息过载等挑战，研究旨在利用LLM（如GPT-4）实时生成高质量问题，以支持访谈者。

Method: 构建了基于常见访谈错误的框架，开发了从受访者话语生成问题的方法，并通过对照实验比较LLM生成和人工问题的表现。

Result: LLM生成的问题在清晰度、相关性和信息量上不逊于人工问题，且在错误类型指导下表现更优。

Conclusion: LLM在需求访谈中有潜力实时提升问题质量和访谈效率。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [17] [DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs](https://arxiv.org/abs/2507.02226)
*Mohammad Akyash,Kimia Azar,Hadi Kamali*

Main category: cs.PL

TL;DR: 本文提出了DecoRTL，一种针对RTL代码生成的新型解码策略，通过自一致性采样和语法感知温度调整，显著提升了代码的语法有效性和功能性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM解码策略在RTL代码生成中常因结构或语义需求不满足而失败，导致生成无效代码。本文旨在解决这一问题。

Method: DecoRTL结合自一致性采样（生成并重排候选）和语法感知温度调整（按语法角色调整采样温度）。

Result: 在VerilogEval基准测试中，DecoRTL显著提升了语法有效性、功能正确性和输出多样性，且不影响运行效率。

Conclusion: DecoRTL有效解决了RTL代码生成中的解码问题，无需额外微调，适用于多种LLM。

Abstract: As one of their many applications, large language models (LLMs) have recently
shown promise in automating register transfer level (RTL) code generation.
However, conventional LLM decoding strategies, originally designed for natural
language, often fail to meet the structural and semantic demands of RTL,
leading to hallucinated, repetitive, or invalid code outputs. In this paper, we
first investigate the root causes of these decoding failures through an
empirical analysis of token-level entropy during RTL generation. Our findings
reveal that LLMs exhibit low confidence in regions of structural ambiguity or
semantic complexity, showing that standard decoding strategies fail to
differentiate between regions requiring determinism (syntax-critical regions)
and those that benefit from creative exploratory variability (design-critical
regions). Then, to overcome this, we introduce DecoRTL, a novel run-time
decoding strategy, that is both syntax-aware and contrastive for RTL code
generation. DecoRTL integrates two complementary components: (i)
self-consistency sampling, which generates multiple candidates and re-ranks
them based on token-level agreement to promote correctness while maintaining
diversity; and (ii) syntax-aware temperature adaptation, which classifies
tokens by their syntactical and functional roles and adjusts the sampling
temperature accordingly, enforcing low temperature for syntax-critical tokens
and higher temperature for exploratory ones. Our approach operates entirely at
inference time without requiring any additional model fine-tuning. Through
evaluations on multiple open-source LLMs using the VerilogEval benchmark, we
demonstrate significant improvements in syntactic validity, functional
correctness, and output diversity, while the execution overhead (performance
overhead) is imperceptible.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [18] [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
*Zongpu Zhang,Pranab Dash,Y. Charlie Hu,Qiang Xu,Jian Li,Haibing Guan*

Main category: cs.OS

TL;DR: 该论文研究了移动设备上大语言模型（LLM）能效低下的问题，提出了一种统一能源感知调控器FUSE，显著优化了LLM推理的能效。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在移动设备上运行时对计算、内存和能源的高需求，当前独立的CPU、GPU和内存调控器导致能效低下。论文旨在解决这一问题。

Method: 通过测量现有LLM框架的能效，分析调控器间的交互问题，设计并实现FUSE，一个统一的能源感知调控器。

Result: FUSE显著减少了首词生成时间和每词生成时间的延迟，同时保持了相同的能源消耗。

Conclusion: FUSE成功优化了移动设备上LLM推理的能效，为未来移动端LLM部署提供了高效解决方案。

Abstract: Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [19] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 综述了合成网络流量生成方法，涵盖了数据类型、生成模型和评估方法，特别关注深度学习和统计方法，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决现实数据稀缺、隐私和纯度问题，提供合成数据替代方案。

Method: 综合评述深度学习和统计方法，包括商业工具。

Result: 系统分析了现有方法、挑战和机会。

Conclusion: 为研究者和从业者提供了关于合成网络流量生成的全面资源。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [20] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 本文提出了一种新型的Implicit Sequence Number (ISN)机制和Reliability Extended Link (RXL)扩展，用于解决多节点芯片互联中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型对处理器性能需求的增长，芯片间互联的高带宽和小负载协议（如CXL、NVLink）在高速传输中易受错误影响，尤其是在多节点配置下，传统可靠性机制面临挑战。

Method: 提出了ISN机制用于精确检测数据包丢失和有序传输，不增加额外头部开销；RXL扩展则将CRC提升为传输层机制，结合FEC确保端到端的可靠性和可扩展性。

Result: RXL在不牺牲带宽效率的前提下，提供了更强的可靠性和可扩展性，兼容现有数据包结构。

Conclusion: ISN和RXL是解决高速互联协议在多节点环境中可靠性问题的有效方案，同时保持了高效的数据传输。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [21] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 本文提出了一种通过Curated Collaborative Learning（CCL）框架和Distributed Unit Pooling Scheme（DUPS）来降低5G RAN能耗和运营成本的双重方法。


<details>
  <summary>Details</summary>
Motivation: 5G网络中RAN的能耗占总能耗的50%以上，现有RAN分割方案未能充分利用数据潜力，亟需降低运营成本。

Method: 采用CCL框架进行高精度网络流量和用户预测，并结合DUPS方案，利用深度强化学习和CCL预测来优化DU服务器资源。

Result: CCL在流量预测上优于现有方法43.9%-31.35%，DUPS将能效提升89%。

Conclusion: 结合CCL和DUPS可显著降低5G RAN的能耗和运营成本，提高效率和成本效益。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [22] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: AI（尤其是大语言模型）在网络配置和诊断任务中表现出色，本文探讨了AI在网络故障排除中的应用，并提出了标准化、可复现的开放基准平台的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究AI在网络故障排除中的应用，并提出标准化平台的需求，以减少操作成本。

Method: 探讨了AI代理在网络故障排除中的潜力，并提出构建开放基准平台的初步设想。

Result: 尚未提出具体结果，但强调了标准化平台对AI代理评估的重要性。

Conclusion: 标准化、可复现的开放基准平台是推动AI在网络故障排除中应用的关键。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [23] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 该论文提出了一种基于信道感知的语义通信框架，以提高车联网（IoV）中数据传输的准确性和效率，同时利用生成扩散模型和大模型优化动态信道估计和适应性。


<details>
  <summary>Details</summary>
Motivation: 车联网（IoV）需要实时处理海量数据，但在动态无线信道条件下存在传输和处理的挑战，亟需高效解决方案。

Method: 提出语义通信框架，提取并压缩传输信息；利用生成扩散模型预测动态信道状态，并结合大模型调整以适应新场景。

Result: 框架在两个公共数据集上验证了性能和可靠性。

Conclusion: 该框架通过优化数据传输和信道估计，显著提升了车联网服务的质量和效率。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [24] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: 论文提出了一种名为REDUS的重采样技术，用于在智能网络中优化深度学习训练，减少计算资源消耗和能量使用，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 随着SDN和深度学习工作负载共享基础设施，资源竞争导致网络性能下降，尤其在延迟敏感的物联网环境中。需要一种方法在保证深度学习训练效率的同时不干扰SDN的运行。

Method: 提出REDUS技术，通过优先处理误分类样本和排除冗余数据来优化深度学习训练，基于AdaBoost启发。

Result: 在CICIoT2023数据集上的评估显示，REDUS减少了72.6%的训练时间，仅损失1.62%的准确率。

Conclusion: REDUS为资源受限的边缘设备提供了一种高效、可扩展的解决方案，同时保持了网络性能。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [25] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 提出了一种利用5G定位参考信号（PRS）的多静态集成传感与通信（ISAC）完整信号处理链，通过分布式架构实现目标检测、参数估计和跟踪。


<details>
  <summary>Details</summary>
Motivation: 通过5G PRS信号实现高保真的移动目标检测、定位与跟踪，以支持多静态ISAC的应用需求。

Method: 采用分布式架构，通过相干交叉模糊函数（CAF）生成距离-多普勒图，结合非线性最小二乘三边测量和正则化线性反演方法进行目标定位与速度估计，并使用卡尔曼滤波器平滑跟踪。

Result: 实验结果表明，该方法能够有效实现高精度的移动目标检测、定位与跟踪。

Conclusion: 该方法为多静态ISAC提供了一种高效且可靠的信号处理解决方案，具有实际应用价值。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [26] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 本文探讨了基于O-RAN原则的未来地面网络（TN）与非地面网络（NTN）融合的架构和功能分割策略，分析了性能和部署之间的权衡，并提出了标准化和模块化融合的关键挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 由于异构传播条件、动态拓扑和有限的星上处理能力，TN与NTN的集成为架构和功能设计带来了独特挑战。

Method: 提出了一种分割选项的分类法，将RAN和核心功能分布在卫星和地面节点之间，并评估了从纯星上DU部署到完全集成gNB和UPF的多种配置。

Result: 分析了性能、延迟、自主性和部署的权衡，并提供了架构分割与RIC放置的全面映射。

Conclusion: 论文总结了标准化、模块化和高效TN-NTN融合的关键挑战与未来方向。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [27] [TAGF: Time-aware Gated Fusion for Multimodal Valence-Arousal Estimation](https://arxiv.org/abs/2507.02080)
*Yubeen Lee,Sangeun Lee,Chaewon Park,Junyeop Cha,Eunil Park*

Main category: cs.MM

TL;DR: 提出了一种时间感知门控融合框架（TAGF），用于解决多模态情感识别中由于噪声和模态不对齐导致的性能下降问题，通过动态调节递归注意力输出来提升效果。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别中，由于音频和视觉模态的噪声和不对齐问题，导致在效价-唤醒度估计上性能下降。

Method: 引入TAGF框架，利用BiLSTM的时间门控机制动态调节递归注意力输出的贡献，有效整合多步跨模态特征。

Result: 在Aff-Wild2数据集上，TAGF表现优异，具有鲁棒性，能可靠地建模真实世界中的动态情感转换。

Conclusion: TAGF通过时间动态建模和跨模态融合，在多模态情感识别任务中取得了显著的性能提升。

Abstract: Multimodal emotion recognition often suffers from performance degradation in
valence-arousal estimation due to noise and misalignment between audio and
visual modalities. To address this challenge, we introduce TAGF, a Time-aware
Gated Fusion framework for multimodal emotion recognition. The TAGF adaptively
modulates the contribution of recursive attention outputs based on temporal
dynamics. Specifically, the TAGF incorporates a BiLSTM-based temporal gating
mechanism to learn the relative importance of each recursive step and
effectively integrates multistep cross-modal features. By embedding temporal
awareness into the recursive fusion process, the TAGF effectively captures the
sequential evolution of emotional expressions and the complex interplay between
modalities. Experimental results on the Aff-Wild2 dataset demonstrate that TAGF
achieves competitive performance compared with existing recursive
attention-based models. Furthermore, TAGF exhibits strong robustness to
cross-modal misalignment and reliably models dynamic emotional transitions in
real-world conditions.

</details>


### [28] [VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via Reinforcement Learning](https://arxiv.org/abs/2507.02626)
*Siran Chen,Boyu Chen,Chenyun Yu,Yuxiao Luo,Ouyang Yi,Lei Cheng,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.MM

TL;DR: VRAgent-R1是一种基于代理的新型范式，通过两个代理（IP和US）提升视频推荐系统的性能，解决了多模态内容理解的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频推荐系统使用冻结的大语言模型（LLM）进行基于提示的用户模拟，存在多模态内容理解和用户偏好学习的不足，限制了推荐性能。

Method: VRAgent-R1包含IP代理（基于MLLM模拟人类渐进式思维）和US代理（通过深度链式思维推理和强化学习优化推荐集）。

Result: IP代理在MicroLens-100k数据集上NDCG@10提升6.0%，US代理在用户决策模拟中准确率提高45%。

Conclusion: VRAgent-R1通过多模态理解和用户模拟的协同优化，显著提升了视频推荐性能。

Abstract: Owing to powerful natural language processing and generative capabilities,
large language model (LLM) agents have emerged as a promising solution for
enhancing recommendation systems via user simulation. However, in the realm of
video recommendation, existing studies predominantly resort to prompt-based
simulation using frozen LLMs and encounter the intricate challenge of
multimodal content understanding. This frequently results in suboptimal item
modeling and user preference learning, thereby ultimately constraining
recommendation performance. To address these challenges, we introduce
VRAgent-R1, a novel agent-based paradigm that incorporates human-like
intelligence in user simulation. Specifically, VRAgent-R1 comprises two
distinct agents: the Item Perception (IP) Agent and the User Simulation (US)
Agent, designed for interactive user-item modeling. Firstly, the IP Agent
emulates human-like progressive thinking based on MLLMs, effectively capturing
hidden recommendation semantics in videos. With a more comprehensive multimodal
content understanding provided by the IP Agent, the video recommendation system
is equipped to provide higher-quality candidate items. Subsequently, the US
Agent refines the recommended video sets based on in-depth chain-of-thought
(CoT) reasoning and achieves better alignment with real user preferences
through reinforcement learning. Experimental results on a large-scale video
recommendation benchmark have demonstrated the effectiveness of our proposed
VRAgent-R1 method, e.g., the IP Agent achieves a 6.0\% improvement in NDCG@10
on the MicroLens-100k dataset, while the US Agent shows approximately 45.0\%
higher accuracy in user decision simulation compared to state-of-the-art
baselines.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [29] [SMT-Sweep: Word-Level Representation Unification for Hardware Verification](https://arxiv.org/abs/2507.02008)
*Ziyi Yang,Guangyu Hu,Mingkai Miao,Changyuan Yu,Hongce Zhang*

Main category: cs.LO

TL;DR: SMT-Sweep是一种将SAT sweeping技术扩展到字级的创新方法，基于SMT理论，显著提升了硬件验证的效率。


<details>
  <summary>Details</summary>
Motivation: 随着字级结构在硬件验证中的普及，缺乏针对字级的等效检查技术，需要一种新的方法来填补这一空白。

Method: SMT-Sweep利用SMT理论，结合随机和约束驱动的字级模拟，处理富语义的位向量操作和数组。

Result: 实验结果表明，SMT-Sweep比现有的位级SAT sweeping和字级SMT求解快44倍和69倍。

Conclusion: SMT-Sweep是首个将sweeping技术引入基于SMT的硬件验证的工作，其实现已开源。

Abstract: SAT sweeping has long been a cornerstone technique in logic simplification
and equivalence checking at the bit level, leveraging structural hashing,
simulation and SAT solving to prune redundant logic. However, with the growing
adoption of word-level constructs in hardware verification, such as bit-vector
operations, arithmetics and arrays, there lacks a counterpart of SAT sweeping
at the word level. In this paper, we introduce SMT-Sweep, a novel extension of
SAT sweeping into the word level, grounded in Satisfiability Modulo Theories
(SMT). SMT-Sweep takes advantage of simulation and equivalence detection to
handle SMT terms with rich bit-vector operations and array semantics. Our
framework incorporates both randomized and constraint-driven word-level
simulation tailored to symbolic expressions and operator semantics beyond pure
Boolean logic. Experimental results show that SMT-Sweep achieves significant
speed-up compared to state-of-the-art bit-level SAT sweeping and word-level
monolithic SMT solving (averaging around 44x and 69x, respectively).To the best
of our knowledge, this is the first work that brings sweeping techniques to
SMT-based hardware verification. The implementation is open-sourced at:
https://github.com/yangziyiiii/SMT-Sweep.

</details>


### [30] [Decision algorithms for fragments of real analysis. III: A theory of differentiable functions with (semi-)open intervals](https://arxiv.org/abs/2507.02742)
*G. Buriola,D. Cantone,G. Cincotti,E. G. Omodeo,G. T. Spartà*

Main category: cs.LO

TL;DR: 本文扩展了未量化语言的满足性测试，引入了一类具有连续一阶导数的实函数，并提出一种决策方法将公式转换为实数代数形式，从而验证其满足性。


<details>
  <summary>Details</summary>
Motivation: 动机是丰富未量化语言的满足性测试，特别是针对具有连续一阶导数的实函数，并扩展这些测试到更广泛的情景，如半开区间。

Method: 方法包括将给定公式预处理为等满足性的无量化实数代数公式，利用Tarski的决策方法验证其满足性，同时通过插值函数证明转换的合理性。

Result: 结果表明，该方法能够有效处理具有连续导数的实函数，并将其转换为无量化形式，从而验证其满足性。

Conclusion: 本文提出的方法成功扩展了满足性测试的范围，适用于更复杂的函数和区间情景，并保持了满足性的一致性。

Abstract: This paper enriches preexisting satisfiability tests for unquantified
languages, which in turn augment a fragment of Tarski's elementary algebra with
unary real functions possessing a continuous first derivative.
  Two sorts of individual variables are available, one ranging over real
numbers and the other one ranging over the functions of interest. Numerical
terms are built from real variables through constructs designating the four
basic arithmetic operations and through the function-application constructs
$f(t)$ and $D[\,f\,](t)$, where $f$ stands for a function variable, $t$ for a
numerical term, and $D[\,\sqdot\,]$ designates the differentiation operator.
Comparison relators can be placed between numerical terms. An array of
predicate symbols are also available, designating various relationships between
functions, as well as function properties, that may hold over intervals of the
real line; those are: (pointwise) function comparisons, strict and nonstrict
monotonicity~/~convexity~/~concavity properties, comparisons between the
derivative of a function and a real term--here, w.r.t.\ earlier research, they
are extended to (semi)-open intervals.
  The decision method we propose consists in preprocessing the given formula
into an equisatisfiable quantifier-free formula of the elementary algebra of
real numbers, whose satisfiability can then be checked by means of Tarski's
decision method. No direct reference to functions will appear in the target
formula, each function variable having been superseded by a collection of stub
real variables; hence, in order to prove that the proposed translation is
satisfiability-preserving, we must figure out a sufficiently flexible family of
interpolating $C^1$ functions that can accommodate a model for the source
formula whenever the target formula turns out to be satisfiable.

</details>


### [31] [A Proof-Theoretic View of Basic Intuitionistic Conditional Logic (Extended Version)](https://arxiv.org/abs/2507.02767)
*Tiziano Dalmonte,Marianna Girlando*

Main category: cs.LO

TL;DR: 论文分析了直觉主义条件逻辑的构造性框架，介绍了两种逻辑系统IntCK和CCKbox，并扩展了Weiss的逻辑CCKbox，提出模型和公理化方法。


<details>
  <summary>Details</summary>
Motivation: 研究直觉主义条件逻辑的构造性框架，探索条件推理中的would和might算子的独立性。

Method: 通过嵌套演算和序列演算分别定义IntCK和CCKbox，扩展Weiss的逻辑CCKbox，并引入模型和公理化方法。

Result: 提出了CCK逻辑系统及其扩展的模型和公理化方法。

Conclusion: 直觉主义条件逻辑的构造性框架为条件推理提供了新的理论基础。

Abstract: Intuitionistic conditional logic, studied by Weiss, Ciardelli and Liu, and
Olkhovikov, aims at providing a constructive analysis of conditional reasoning.
In this framework, the would and the might conditional operators are no longer
interdefinable. The intuitionistic conditional logics considered in the
literature are defined by setting Chellas' conditional logic CK, whose
semantics is defined using selection functions, within the constructive and
intuitionistic framework introduced for intuitionistic modal logics. This
operation gives rise to a constructive and an intuitionistic variant of
(might-free-) CK, which we call CCKbox and IntCK respectively. Building on the
proof systems defined for CK and for intuitionistic modal logics, in this paper
we introduce a nested calculus for IntCK and a sequent calculus for CCKbox.
Based on the sequent calculus, we define CCK, a conservative extension of
Weiss' logic CCKbox with the might operator. We introduce a class of models and
an axiomatization for CCK, and extend these result to several extensions of
CCK.

</details>


### [32] [Subtyping in DHOL -- Extended preprint](https://arxiv.org/abs/2507.02855)
*Colin Rothgang,Florian Rabe*

Main category: cs.LO

TL;DR: 论文介绍了如何在依赖类型高阶逻辑（DHOL）中添加精炼类型和商类型，从而增强其表达能力而不影响自动化定理证明的支持。


<details>
  <summary>Details</summary>
Motivation: 精炼类型和商类型在实践中常被需求，但由于需要不可判定的类型系统，很少能在自动定理证明器中实现。DHOL的设计使得这种扩展成为可能。

Method: 将精炼类型和商类型作为子类型的特例添加，避免了表示上的高成本变化，并保持了语法、语义和HOL转换的完整性。

Result: 成功扩展了DHOL，证明了其语法、语义和转换到HOL的正确性。

Conclusion: DHOL的扩展不仅可行，而且优雅简洁，为自动化定理证明提供了更强的表达能力。

Abstract: The recently introduced dependent typed higher-order logic (DHOL) offers an
interesting compromise between expressiveness and automation support. It
sacrifices the decidability of its type system in order to significantly extend
its expressiveness over standard HOL. Yet it retains strong automated theorem
proving support via a sound and complete translation to HOL.
  We leverage this design to extend DHOL with refinement and quotient types.
Both of these are commonly requested by practitioners but rarely provided by
automated theorem provers. This is because they inherently require undecidable
typing and thus are very difficult to retrofit to decidable type systems. But
with DHOL already doing the heavy lifting, adding them is not only possible but
elegant and simple.
  Concretely, we add refinement and quotient types as special cases of
subtyping. This turns the associated canonical inclusion resp. projection maps
into identity maps and thus avoids costly changes in representation. We present
the syntax, semantics, and translation to HOL for the extended language,
including the proofs of soundness and completeness.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [33] [StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative](https://arxiv.org/abs/2507.02156)
*Benjamin Watson,Janet Kim,Tim McEneany,Tom Moher,Claudia Hindo,Louis Gomez,Stephen Fransen*

Main category: cs.HC

TL;DR: StorySpace项目探讨新界面技术如何支持高中课堂叙事活动，通过三个设计目标激发学生反思与兴趣。


<details>
  <summary>Details</summary>
Motivation: 研究新界面技术在高中的应用，增强课堂叙事的效果。

Method: 遵循三个设计目标：激发学生反思、呈现复杂主题、打造有趣媒介。

Result: 目标是让学生通过构建复杂表达提升学习兴趣。

Conclusion: StorySpace通过设计创新提升课堂叙事的吸引力和教育效果。

Abstract: The StorySpace project studies the role new interface technologies might play
in high school education. With this approach in mind, StorySpace is
specifically designed to support and enhance classroom narrative, an already
well-established classroom activity. StorySpace strives to achieve this through
adherence to three design goals. The first is to trigger student reflection and
interpretation. The narrative medium created by StorySpace should represent the
topic of classroom discussion and learning in all its complexity. In building
their representation, the students will then be confronted with that same
complexity. The medium should also itself be exciting and compelling, making
classroom narrative interesting and fun.

</details>


### [34] [PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training](https://arxiv.org/abs/2507.02122)
*Neil K. R. Sehgal,Hita Kambhamettu,Allen Chang,Andrew Zhu,Lyle Ungar,Sharath Chandra Guntuku*

Main category: cs.HC

TL;DR: PAL是一种辅助缓和医疗沟通训练的对话系统，通过模拟情感化患者互动并提供结构化反馈，支持文本和语音模式。研究发现其有助于技能提升，但在情感真实性和反馈适应性方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 缓和医疗沟通训练资源有限，标准化患者难以获取，因此开发了PAL系统，以低成本、可重复的方式支持临床技能培训。

Method: PAL是一种基于大型语言模型的对话系统，支持文本和语音互动，并通过混合方法研究（17名美国医学生和医生参与）评估其可用性和设计问题。

Result: 参与者认为PAL对反思和技能提升有帮助，但也指出情感真实性和反馈适应性不足。研究证明语言模型可用于缓和医疗培训。

Conclusion: PAL为缓和医疗沟通训练提供了一种可行方案，并为设计情感敏感、模式感知的模拟工具提供了见解，同时探讨了AI在高压护理环境中的培训潜力。

Abstract: Effective communication in serious illness and palliative care is essential
but often under-taught due to limited access to training resources like
standardized patients. We present PAL (Palliative Assisted Learning-bot), a
conversational system that simulates emotionally nuanced patient interactions
and delivers structured feedback grounded in an existing empathy-based
framework. PAL supports text and voice modalities and is designed to scaffold
clinical skill-building through repeated, low-cost practice. Through a
mixed-methods study with 17 U.S. medical trainees and clinicians, we explore
user engagement with PAL, evaluate usability, and examine design tensions
around modalities, emotional realism, and feedback delivery. Participants found
PAL helpful for reflection and skill refinement, though some noted limitations
in emotional authenticity and the adaptability of feedback. We contribute: (1)
empirical evidence that large language models can support palliative
communication training; (2) design insights for modality-aware, emotionally
sensitive simulation tools; and (3) implications for systems that support
emotional labor, cooperative learning, and AI-augmented training in high-stakes
care settings.

</details>


### [35] [A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy](https://arxiv.org/abs/2507.02138)
*Shan Li,Guozhu Ding*

Main category: cs.HC

TL;DR: Healthy Choice是一个AI增强的互动学习平台，旨在提升营养素养。研究表明，114名大学生对其效果和易用性评价很高。


<details>
  <summary>Details</summary>
Motivation: 开发一个理论驱动且AI增强的平台，通过互动场景培养营养素养。

Method: 收集114名大学学生的反馈，完成模拟产品选择场景。

Result: 定量评分显示高用户满意度和易用性。

Conclusion: Healthy Choice平台在提升营养素养方面表现出色。

Abstract: This study introduces and evaluates Healthy Choice, an innovative
theory-driven and AI-enhanced simulation platform designed to cultivate
nutrition literacy through interactive scenario-based learning experiences. We
collected feedback from 114 university students with diverse backgrounds who
completed simulated product selection scenarios. Quantitative ratings of
usefulness and ease of use demonstrated high user satisfaction.

</details>


### [36] [A wireless, inexpensive optical tracker for the CAVE](https://arxiv.org/abs/2507.02682)
*Ehud Sharlin,Pablo Figueroa,Mark Green,Benjamin Watson*

Main category: cs.HC

TL;DR: 一篇关于低成本无线脚部追踪器的研究，用于CAVE显示系统，提供用户无束缚的运动自由，适用于校园漫游但不适用于高精度视觉检查。


<details>
  <summary>Details</summary>
Motivation: 传统CAVE显示系统的追踪子系统会限制用户移动，削弱其优势，因此设计了一种低成本、无线的脚部追踪器。

Method: 开发了一个成本低于200美元的无线脚部追踪器，采样率为20 Hz，精度为10 cm，并在可视化检查与校园漫游应用中进行了测试。

Result: 追踪器在校园漫游中表现良好，用户可自由移动，但在需要高精度视觉检查的应用中效果不佳。

Conclusion: 无线脚部追踪器为CAVE系统提供了无束缚的运动自由，适用于特定应用，如校园漫游，但需进一步改进以满足高精度需求。

Abstract: CAVE displays offer many advantages over other virtual reality (VR) displays,
including a large, unencumbering viewing space. Unfortunately, the typical
tracking subsystems used with CAVE displays tether the user and lessen this
advantage. We have designed a simple, low-cost feet tracker that is wireless,
leaving the user free to move. The tracker can be assembled for less than $200
US, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested
the prototype with two applications: a visualization supporting close visual
inspection, and a walkthrough of the campus. Although the tracking was
convincing, it was clear that the tracker's limitations make it less than ideal
for applications requiring precise visual inspection. However, the freedom of
motion allowed by the tracker was a compelling supplement to our campus
walkthrough, allowing users to stroll and look around corners.

</details>


### [37] [The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future](https://arxiv.org/abs/2507.02180)
*Russell Beale*

Main category: cs.HC

TL;DR: 论文概述了大型语言模型（LLMs）自2022年广泛可用以来对教育和教育技术的重要影响，讨论了其应用领域、成功与失败的案例，并探讨了其对学习者和教育者动态的改变。此外，文章提出了LLMs在设计上面临的挑战，并预测其将成为与技术交互的默认方式，同时提出了未来教育技术设计的关键考虑因素。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在教育领域的快速普及及其对教育技术和互动方式的深远影响。

Method: 通过综述LLMs的应用领域和使用案例，分析其对教育动态的改变，并讨论其设计挑战和支持的学习范式。

Result: LLMs已显著改变教育和教育技术，但其设计仍需改进以满足学习者和用户的期望。

Conclusion: LLMs将成为与技术交互的默认方式，未来教育技术设计需考虑其带来的新互动范式和用户期望。

Abstract: Large language Models have only been widely available since 2022 and yet in
less than three years have had a significant impact on approaches to education
and educational technology. Here we review the domains in which they have been
used, and discuss a variety of use cases, their successes and failures. We then
progress to discussing how this is changing the dynamic for learners and
educators, consider the main design challenges facing LLMs if they are to
become truly helpful and effective as educational systems, and reflect on the
learning paradigms they support. We make clear that the new interaction
paradigms they bring are significant and argue that this approach will become
so ubiquitous it will become the default way in which we interact with
technologies, and revolutionise what people expect from computer systems in
general. This leads us to present some specific and significant considerations
for the design of educational technology in the future that are likely to be
needed to ensure acceptance by the changing expectations of learners and users.

</details>


### [38] [EvalAssist: A Human-Centered Tool for LLM-as-a-Judge](https://arxiv.org/abs/2507.02186)
*Zahra Ashktorab,Elizabeth M. Daly,Erik Miehling,Werner Geyer,Martin Santillan Cooper,Tejaswini Pedapati,Michael Desmond,Qian Pan,Hyo Jin Do*

Main category: cs.HC

TL;DR: 论文介绍了EvalAssist框架，简化了使用大型语言模型作为评估工具的工作流程，帮助用户快速构建和测试评估标准。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，评估其输出的最佳方法变得耗时且昂贵，需要一种简化这一过程的工具。

Method: EvalAssist提供了一个在线标准开发环境，支持用户交互式构建、测试和共享评估标准，并基于开源库UNITXT实现提示链方法。

Result: 系统已在组织内部部署，拥有数百名用户，展示了其在评估模型输出方面的实用性。

Conclusion: EvalAssist通过简化和标准化评估流程，有效提升了大型语言模型输出的评估效率。

Abstract: With the broad availability of large language models and their ability to
generate vast outputs using varied prompts and configurations, determining the
best output for a given task requires an intensive evaluation process, one
where machine learning practitioners must decide how to assess the outputs and
then carefully carry out the evaluation. This process is both time-consuming
and costly. As practitioners work with an increasing number of models, they
must now evaluate outputs to determine which model and prompt performs best for
a given task. LLMs are increasingly used as evaluators to filter training data,
evaluate model performance, assess harms and risks, or assist human evaluators
with detailed assessments. We present EvalAssist, a framework that simplifies
the LLM-as-a-judge workflow. The system provides an online criteria development
environment, where users can interactively build, test, and share custom
evaluation criteria in a structured and portable format. We support a set of
LLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a
prompt-chaining approach we developed and contributed to the UNITXT open-source
library. Additionally, our system also includes specially trained evaluators to
detect harms and risks in LLM outputs. We have deployed the system internally
in our organization with several hundreds of users.

</details>


### [39] [VergeIO: Depth-Aware Eye Interaction on Glasses](https://arxiv.org/abs/2507.02187)
*Xiyuxing Zhang,Duc Vu,Chengyi Shen,Yuntao Wang,Yuanchun Shi,Justin Chan*

Main category: cs.HC

TL;DR: VergeIO是一种基于EOG技术的眼镜，通过优化的电极布局和新型智能眼镜原型实现深度感知眼动交互，支持高精度手势识别。


<details>
  <summary>Details</summary>
Motivation: 行业对眼镜上无干扰EOG传感设计的需求增长，VergeIO旨在实现深度感知眼动交互。

Method: 采用优化的电极布局和新型智能眼镜原型，结合个性化模型和运动伪影检测技术。

Result: 在11名用户和1,320个手势实例中，识别准确率达83-98%；新用户无校准下准确率为80-98%。

Conclusion: VergeIO以低功耗实现实时无干扰眼动交互，适合持续传感应用。

Abstract: There is growing industry interest in creating unobtrusive designs for
electrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and
Apple eyewear). We present VergeIO, the first EOG-based glasses that enables
depth-aware eye interaction using vergence with an optimized electrode layout
and novel smart glass prototype. It can distinguish between four and six
depth-based eye gestures with 83-98% accuracy using personalized models in a
user study across 11 users and 1,320 gesture instances. It generalizes to
unseen users with an accuracy of 80-98% without any calibration. To reduce
false detections, we incorporate a motion artifact detection pipeline and a
preamble-based activation scheme. The system uses dry sensors without any
adhesives or gel, and operates in real time with 3 mW power consumption by the
sensing front-end, making it suitable for always-on sensing.

</details>


### [40] [An Exploration of Internal States in Collaborative Problem Solving](https://arxiv.org/abs/2507.02229)
*Sifatul Anindho,Videep Venkatesha,Mariah Bradford,Anne M. Cleary,Nathaniel Blanchard*

Main category: cs.HC

TL;DR: 该研究通过混合方法探讨了协作问题解决(CPS)过程中个体的情绪状态，通过视频回顾和自我报告分析了情绪的语言表达特征。


<details>
  <summary>Details</summary>
Motivation: 研究CPS中情绪状态的重要性，因为CPS在教育和职业环境中越来越普遍。

Method: 采用混合方法，团队完成CPS任务后，个体通过视频回顾和自我报告其内部体验，并进行语言分析。

Result: 语言分析揭示了情绪相关的特定用词模式和语义相似性。

Conclusion: 研究揭示了CPS过程中个体情绪的语言表达特征，为理解复杂协作中的情绪动态提供了新视角。

Abstract: Collaborative problem solving (CPS) is a complex cognitive, social, and
emotional process that is increasingly prevalent in educational and
professional settings. This study investigates the emotional states of
individuals during CPS using a mixed-methods approach. Teams of four first
completed a novel CPS task. Immediately after, each individual was placed in an
isolated room where they reviewed the video of their group performing the task
and self-reported their internal experiences throughout the task. We performed
a linguistic analysis of these internal monologues, providing insights into the
range of emotions individuals experience during CPS. Our analysis showed
distinct patterns in language use, including characteristic unigrams and
bigrams, key words and phrases, emotion labels, and semantic similarity between
emotion-related words.

</details>


### [41] [A framework for 3D interaction techniques](https://arxiv.org/abs/2507.02254)
*Pablo Figueroa,Mark Green,Benjamin Watson*

Main category: cs.HC

TL;DR: 提出了一种用于3D交互技术的软件架构及独立于工具包的面向对象框架，支持扩展性和无缝集成。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D交互技术的灵活性和可扩展性问题，设计一个通用的架构和框架。

Method: 采用数据流中的基础过滤器组合，虚拟输入设备和场景对象作为信息源，定义一个通用的信息流执行模型。

Result: 该框架支持新信息类型、输入设备、执行模型或交互技术的轻松添加，并能与应用特定的代码和技术无缝集成。

Conclusion: 该架构和框架为3D交互技术提供了一个高度灵活和可扩展的解决方案。

Abstract: This paper presents a software architecture for 3D interaction techniques
(ITs) and an object oriented, toolkit-independent framework that implements
such architecture. ITs are composed of basic filters connected in a dataflow,
where virtual input devices and objects in the scene are sources of
information. An execution model defines the general flow of information between
filters. This framework has been designed to be extensible: new information
types, new input devices, new execution models, or new interaction techniques
can easily be added. Application specific code and application specific ITs are
seamlessly integrated into this architecture.

</details>


### [42] [Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness](https://arxiv.org/abs/2507.02283)
*Tim Rogers,Ben Teehankee*

Main category: cs.HC

TL;DR: 论文探讨了大型语言模型（LLM）可能继承并放大人类理论与实际行为之间的不一致性，揭示了AI对齐问题在组织学习中的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLM如何模仿人类的防御性推理模式（Model 1理论），从而阻碍组织学习，进一步加剧AI对齐问题的复杂性。

Method: 通过HR顾问角色的LLM案例研究，分析其建议如何强化无效问题解决方式并阻碍深度学习。

Result: 研究发现LLM虽能模仿人类行为，但也继承了认知盲点，可能强化组织中的反学习实践。

Conclusion: 论文提出开发支持Model 2学习的LLM的可能性，认为AI对齐研究可以促进人类价值的内化，实现双向改进。

Abstract: This paper examines a critical yet unexplored dimension of the AI alignment
problem: the potential for Large Language Models (LLMs) to inherit and amplify
existing misalignments between human espoused theories and theories-in-use.
Drawing on action science research, we argue that LLMs trained on
human-generated text likely absorb and reproduce Model 1 theories-in-use - a
defensive reasoning pattern that both inhibits learning and creates ongoing
anti-learning dynamics at the dyad, group, and organisational levels. Through a
detailed case study of an LLM acting as an HR consultant, we show how its
advice, while superficially professional, systematically reinforces
unproductive problem-solving approaches and blocks pathways to deeper
organisational learning. This represents a specific instance of the alignment
problem where the AI system successfully mirrors human behaviour but inherits
our cognitive blind spots. This poses particular risks if LLMs are integrated
into organisational decision-making processes, potentially entrenching
anti-learning practices while lending authority to them. The paper concludes by
exploring the possibility of developing LLMs capable of facilitating Model 2
learning - a more productive theory-in-use - and suggests this effort could
advance both AI alignment research and action science practice. This analysis
reveals an unexpected symmetry in the alignment challenge: the process of
developing AI systems properly aligned with human values could yield tools that
help humans themselves better embody those same values.

</details>


### [43] [Human-Centered Explainability in Interactive Information Systems: A Survey](https://arxiv.org/abs/2507.02300)
*Yuhao Zhang,Jiaxin An,Ben Wang,Yan Zhang,Jiqun Liu*

Main category: cs.HC

TL;DR: 该论文通过系统性文献调查，总结了人机交互信息系统中可解释性的研究进展，包括概念化、设计和评估方法，并提出了未来研究的建议。


<details>
  <summary>Details</summary>
Motivation: 由于可解释性对于用户理解和监督AI驱动的输出至关重要，研究其在实际中的概念、设计和评估方法有助于推动透明、可信和负责任的信息系统发展。

Method: 遵循PRISMA指南，搜索了8个学术数据库，筛选出100篇相关文献，采用结构化编码方法提取和综合信息。

Result: 总结了可解释性的五个概念维度、解释设计的分类方案以及六种用户中心化的测量维度。

Conclusion: 研究揭示了人机交互可解释性的理论基础，为满足多样化用户需求的信息系统设计提供了参考，并指出了未来的研究方向。

Abstract: Human-centered explainability has become a critical foundation for the
responsible development of interactive information systems, where users must be
able to understand, interpret, and scrutinize AI-driven outputs to make
informed decisions. This systematic survey of literature aims to characterize
recent progress in user studies on explainability in interactive information
systems by reviewing how explainability has been conceptualized, designed, and
evaluated in practice. Following PRISMA guidelines, eight academic databases
were searched, and 100 relevant articles were identified. A structural encoding
approach was then utilized to extract and synthesize insights from these
articles. The main contributions include 1) five dimensions that researchers
have used to conceptualize explainability; 2) a classification scheme of
explanation designs; 3) a categorization of explainability measurements into
six user-centered dimensions. The review concludes by reflecting on ongoing
challenges and providing recommendations for future exploration of related
issues. The findings shed light on the theoretical foundations of
human-centered explainability, informing the design of interactive information
systems that better align with diverse user needs and promoting the development
of systems that are transparent, trustworthy, and accountable.

</details>


### [44] [Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation](https://arxiv.org/abs/2507.02306)
*Ruican Zhong,David W. McDonald,Gary Hsieh*

Main category: cs.HC

TL;DR: 本研究提出了一种利用多模态LLM分析图像并提供设计反馈的合成启发式评估方法。与人类专家评估相比，合成方法在识别可用性问题上表现更优。


<details>
  <summary>Details</summary>
Motivation: 可用性评估在人机交互设计中至关重要，但传统方法成本高昂，需要专家时间和用户补偿。因此，研究旨在探索利用多模态LLM进行低成本高效评估的可能性。

Method: 采用多模态LLM分析图像并提供设计反馈，开发了合成启发式评估方法，并在两个应用中与5名经验丰富的用户体验从业者的评估结果进行比较。

Result: 合成评估方法在两个应用中分别识别出73%和77%的可用性问题，优于人类评估者（57%和63%）。合成方法在检测布局问题上表现尤为突出。

Conclusion: 合成评估方法在可用性问题识别上优于人类评估者，但在识别部分UI组件和设计惯例上存在局限。研究为合成启发式评估的设计提供了参考。

Abstract: Usability evaluation is crucial in human-centered design but can be costly,
requiring expert time and user compensation. In this work, we developed a
method for synthetic heuristic evaluation using multimodal LLMs' ability to
analyze images and provide design feedback. Comparing our synthetic evaluations
to those by experienced UX practitioners across two apps, we found our
evaluation identified 73% and 77% of usability issues, which exceeded the
performance of 5 experienced human evaluators (57% and 63%). Compared to human
evaluators, the synthetic evaluation's performance maintained consistent
performance across tasks and excelled in detecting layout issues, highlighting
potential attentional and perceptual strengths of synthetic evaluation.
However, synthetic evaluation struggled with recognizing some UI components and
design conventions, as well as identifying across screen violations.
Additionally, testing synthetic evaluations over time and accounts revealed
stable performance. Overall, our work highlights the performance differences
between human and LLM-driven evaluations, informing the design of synthetic
heuristic evaluations.

</details>


### [45] [From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance](https://arxiv.org/abs/2507.02350)
*Hao Tang,Songyun Xie,Xinzhou Xie,Can Liao,Xin Zhang,Bohan Li,Zhongyu Tian,Dalu Zheng*

Main category: cs.HC

TL;DR: 论文提出了一种细粒度标注方法，通过即时回忆范式解决传统视频诱导情感生理数据集中粗粒度标注导致的标签噪声问题，提高了情感识别算法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的情感数据集使用粗粒度的全试次标注，无法准确捕捉情感反应的动态性和局部性，导致标签噪声，限制了情感识别算法的评估和性能。

Method: 提出了一种通过即时回忆范式的细粒度标注方法，在初始刺激观看后加入即时视频回放，让参与者精确标注情感发生的时间戳、标签和强度。

Result: 生理验证显示，被试标注的窗口中出现了节律特异性的EEG模式和与主观标注高度一致的GSR反应；情感识别实验中，细粒度标注训练的模型准确率比传统方法高9.7%。

Conclusion: 细粒度标注不仅解决了标签噪声问题，还表明标注精确度比数据规模对情感识别性能的影响更大。

Abstract: Traditional video-induced emotion physiological datasets often use
whole-trial annotation, assigning a single emotion label to all data collected
during an entire trial. This coarse-grained annotation approach misaligns with
the dynamic and temporally localized nature of emotional responses as they
unfold with video narratives, introducing label noise that limits emotion
recognition algorithm evaluation and performance. To solve the label noise
problem caused by coarse-grained annotation, we propose a fine-grained
annotation method through an immediate recall paradigm. This paradigm
integrates an immediate video replay phase after the initial stimulus viewing,
allowing participants to precisely mark the onset timestamp, emotion label, and
intensity based on their immediate recall. We validate this paradigm through
physiological evidence and recognition performance. Physiological validation of
multimodal signals within participant-marked windows revealed rhythm-specific
EEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of
high-arousal versus 6% of low-arousal emotion windows. These objective
physiological data changes strongly aligned with subjective annotations,
confirming annotation precision. For recognition performance, classification
experiments showed that models trained on fine-grained annotations achieved
9.7% higher accuracy than traditional whole-trial labeling, despite using less
data. This work not only addresses label noise through fine-grained annotation
but also demonstrates that annotation precision outweighs data scale in
determining emotion recognition performance.

</details>


### [46] [Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset](https://arxiv.org/abs/2507.02432)
*Jueun Lee,Dennis Moschina,Supraja Ramesh,Tobias Röddiger,Kai Kunze,Michael Beigl*

Main category: cs.HC

TL;DR: 研究探讨了通过音乐节奏振动作为被动生物反馈干预手段，用于放松和促进入睡。系统通过智能手表振动，以略低于用户实时心率的方式调整振频，旨在通过触觉同步降低觉醒水平。短期干预显示副交感神经活动增强和放松感提升，但入睡阶段无显著效果。


<details>
  <summary>Details</summary>
Motivation: 探索一种非侵入性的触觉反馈方法，以替代传统的听觉或开环干预手段，用于放松和促进入睡。

Method: 研究分为两部分：第一部分比较五种自适应振动节奏对心率和主观放松感的影响；第二部分在入睡环境中评估最有效的振动模式。

Result: 短期振动干预增加了副交感神经活动和主观放松感，但对入睡阶段的睡眠指标无显著影响。

Conclusion: 研究为可穿戴触觉反馈在放松和睡眠干预中的应用提供了设计和方法学参考，明确了其短期效果和入睡阶段的有效性限制。

Abstract: We investigate the use of musically structured, closed-loop vibration
patterns as a passive biofeedback intervention for relaxation and sleep
initiation. By encoding rhythmic meter structures into smartwatch vibrations
and adapting their frequency to be slightly slower than the user's real-time
heart rate, our system aims to reduce arousal through tactile entrainment,
offering a non-invasive alternative to auditory or open-loop approaches
previously used in sleep and anxiety contexts. In the first study (N=20), we
compared five adaptive vibration rhythms for their effects on heart rate and
subjective perceptions of relaxation in a resting context. In the second study
(N=28), we evaluated the most promising pattern from Study 1 in a prolonged
sleep initiation setting. Results showed increased parasympathetic activity and
perceived relaxation during short-term stimulation, but no significant effects
on sleep-related measures during the sleep onset phase. This work contributes
to the understanding of how wearable haptic feedback can support relaxation and
sleep, offering design insights and identifying methodological considerations
for effectively integrating haptic interaction into self-directed
interventions.

</details>


### [47] [Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?](https://arxiv.org/abs/2507.02453)
*Jueun Lee,Martin Flipe,Philipp Lepold,Tobias Röddiger,Michael Beigl*

Main category: cs.HC

TL;DR: 研究探讨了基于实时心率的触觉反馈在四种身体部位（手腕、手、前臂、肩膀）对放松效果的影响，发现前臂和肩膀最适合放松反馈。


<details>
  <summary>Details</summary>
Motivation: 当前触觉反馈应用多关注压力场景和固定振动模式，缺乏对休息状态下动态反馈和身体位置的考虑。

Method: 比较四种佩戴位置（手腕、手、前臂、肩膀）对心率、α波活动、主观休息感和振动体验的影响。

Result: 手腕、肩膀和前臂的触觉反馈降低心率，但α波无变化；前臂和肩膀的主观休息感最高且最受欢迎。

Conclusion: 前臂和肩膀适合作为放松反馈的理想位置，手腕需改进设计以提升主观体验。

Abstract: Wearable haptic interventions offer promising support for relaxation through
slow, vibrotactile biofeedback. Despite their potential, current applications
focus on stress-inducing procedures and fixed vibration patterns, with limited
consideration of body location and dynamic biofeedback during restful states.
This study investigates the effects of haptic biofeedback adjusted from
real-time heart rate during eyes-closed wakeful rest, comparing four wearable
body placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave
activity on the ear, subjective restfulness, and vibration experience were
measured across these conditions. Results show that biofeedback reduced heart
rate at the wrist, shoulder, and forearm, while alpha power measured at the ear
remained unchanged. Subjective restfulness was rated highest at the shoulder
and forearm, which were also the most preferred locations. In addition,
participants reported greater comfort, relaxation, and further increased
sleepiness at the forearm compared to the wrist, which was more easily
recognizable. These findings suggest that the forearm and shoulder are ideal
for unobtrusive relaxation feedback for wakeful rest, while the wrist may
require design improvements for subjective experience.

</details>


### [48] [Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue](https://arxiv.org/abs/2507.02537)
*Paulo Ricardo Knob,Leonardo Scholler,Juliano Rigatti,Soraia Raupp Musse*

Main category: cs.HC

TL;DR: 研究探讨了大型语言模型（LLMs）在生成情感丰富的对话中的表现，发现尽管对话的情感结构符合预期，但人类评估显示模型的同理心和连贯性仍有差距。


<details>
  <summary>Details</summary>
Motivation: 随着对话代理在日常互动中的普及，情感智能（尤其是同理心倾听）的需求日益显著。

Method: 通过小型专家数据集扩展对话，使用ChatGPT和Gemini生成，并用情感分析（VADER）和专家评估分析情感进展。

Result: 生成的对话虽然情感结构接近预期，但人类评估显示其同理心和连贯性不足。

Conclusion: 情感对话建模需结合自动化和人类评估，以提升情感深度和代理的情感能力。

Abstract: Conversational agents have made significant progress since ELIZA, expanding
their role across various domains, including healthcare, education, and
customer service. As these agents become increasingly integrated into daily
human interactions, the need for emotional intelligence, particularly
empathetic listening, becomes increasingly essential. In this study, we explore
how Large Language Models (LLMs) respond when tasked with generating
emotionally rich interactions. Starting from a small dataset manually crafted
by an expert to reflect empathic behavior, we extended the conversations using
two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the
dialogues using both sentiment analysis (via VADER) and expert assessments.
While the generated conversations often mirrored the intended emotional
structure, human evaluation revealed important differences in the perceived
empathy and coherence of the responses. These findings suggest that emotion
modeling in dialogues requires not only structural alignment in the expressed
emotions but also qualitative depth, highlighting the importance of combining
automated and humancentered methods in the development of emotionally competent
agents.

</details>


### [49] [Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots](https://arxiv.org/abs/2507.02745)
*Zahra Ashktorab,Alessandra Buccella,Jason D'Cruz,Zoe Fowler,Andrew Gill,Kei Yan Leung,P. D. Magnus,John Richards,Kush R. Varshney*

Main category: cs.HC

TL;DR: 研究探讨了LLM驱动的聊天机器人如何通过不同类型的道歉（例行、解释性、共情性）修复用户信任，发现解释性道歉最受欢迎，但场景和用户差异影响偏好。


<details>
  <summary>Details</summary>
Motivation: 随着LLM聊天机器人的广泛应用，其通过有效道歉修复错误的能力对维持用户信任至关重要。

Method: 通过预注册实验（N=162），采用成对设计，评估三种道歉在三种常见错误（偏见、虚构、事实错误）中的效果。

Result: 解释性道歉普遍更受青睐，但在偏见场景中，共情性道歉因承认情感影响更受欢迎；虚构错误则无明确偏好。

Conclusion: 有效道歉的复杂性要求未来系统需个性化与校准，以真正修复信任。

Abstract: As chatbots driven by large language models (LLMs) are increasingly deployed
in everyday contexts, their ability to recover from errors through effective
apologies is critical to maintaining user trust and satisfaction. In a
preregistered study with Prolific workers (N=162), we examine user preferences
for three types of apologies (rote, explanatory, and empathic) issued in
response to three categories of common LLM mistakes (bias, unfounded
fabrication, and factual errors). We designed a pairwise experiment in which
participants evaluated chatbot responses consisting of an initial error, a
subsequent apology, and a resolution. Explanatory apologies were generally
preferred, but this varied by context and user. In the bias scenario, empathic
apologies were favored for acknowledging emotional impact, while
hallucinations, though seen as serious, elicited no clear preference,
reflecting user uncertainty. Our findings show the complexity of effective
apology in AI systems. We discuss key insights such as personalization and
calibration that future systems must navigate to meaningfully repair trust.

</details>


### [50] [Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding](https://arxiv.org/abs/2507.02800)
*Ebrahim Feghhi,Shreyas Kaasyap,Nima Hadidi,Jonathan C. Kao*

Main category: cs.HC

TL;DR: 该论文提出三种改进方法，用于提高神经语音解码的准确性、效率和实时性，包括大量时间掩码训练、紧凑型Transformer架构和轻量级测试时适应方法，显著降低了计算成本和错误率。


<details>
  <summary>Details</summary>
Motivation: 尽管现有神经语音解码算法在准确性上有显著提升，但其计算成本高且未在实时环境下验证，因此需要开发更高效、实时的方法。

Method: 1. 引入大量时间掩码训练（掩盖每个试验的50%以上）；2. 用紧凑型Transformer替代GRU架构，减少参数和GPU内存；3. 设计轻量级测试时适应方法，通过单次梯度步适应模型。

Result: 改进方法将词错误率降低19.5%，显著减少计算成本，并有效缓解了跨日测试的性能下降问题。

Conclusion: 通过优化训练策略和模型架构，论文实现了高效、准确的实时神经语音解码，为临床应用提供了可行方案。

Abstract: Speech neuroprostheses aim to restore communication for people with severe
paralysis by decoding speech directly from neural activity. To accelerate
algorithmic progress, a recent benchmark released intracranial recordings from
a paralyzed participant attempting to speak, along with a baseline decoding
algorithm. Prior work on the benchmark showed impressive accuracy gains.
However, these gains increased computational costs and were not demonstrated in
a real-time decoding setting. Here, we make three contributions that pave the
way towards accurate, efficient, and real-time neural speech decoding. First,
we incorporate large amounts of time masking during training. On average, over
$50\%$ of each trial is masked. Second, we replace the gated recurrent unit
(GRU) architecture used in the baseline algorithm with a compact Transformer.
The Transformer architecture uses $77\%$ fewer parameters, cuts peak GPU memory
usage by $36\%$ relative, and is significantly faster to calibrate relative to
the GRU. Third, we design a lightweight variant of an existing test-time
adaptation method developed for decoding handwriting from neural activity. Our
variant adapts the model using multiple time masked augmentations of a single
trial and requires only one gradient step per trial. Together, these
contributions reduce word error rate by $19.5\%$ and effectively mitigate
performance degradations across held-out days in a real-time decoding setting
while substantially lowering computational costs.

</details>


### [51] [Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks](https://arxiv.org/abs/2507.02819)
*Luke Guerdan,Devansh Saxena,Stevie Chancellor,Zhiwei Steven Wu,Kenneth Holstein*

Main category: cs.HC

TL;DR: 数据科学家构建预测模型的目标变量是一个复杂的‘拼凑’过程，涉及高阶测量目标和低阶实际约束之间的迭代协商。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解数据科学家如何将模糊概念转化为具体的代理目标变量，以填补这一过程中的知识空白。

Method: 通过访谈15名教育和医疗领域的数据科学家（教育8人，医疗7人），分析他们构建目标变量的方法。

Result: 数据科学家通过‘拼凑’过程满足目标变量的五个主要标准：有效性、简单性、可预测性、可移植性和资源需求。

Conclusion: 研究提出了未来HCI、CSCW和ML研究的机遇，以更好地支持目标变量构建的艺术与科学。

Abstract: Data scientists often formulate predictive modeling tasks involving fuzzy,
hard-to-define concepts, such as the "authenticity" of student writing or the
"healthcare need" of a patient. Yet the process by which data scientists
translate fuzzy concepts into a concrete, proxy target variable remains poorly
understood. We interview fifteen data scientists in education (N=8) and
healthcare (N=7) to understand how they construct target variables for
predictive modeling tasks. Our findings suggest that data scientists construct
target variables through a bricolage process, involving iterative negotiation
between high-level measurement objectives and low-level practical constraints.
Data scientists attempt to satisfy five major criteria for a target variable
through bricolage: validity, simplicity, predictability, portability, and
resource requirements. To achieve this, data scientists adaptively use problem
(re)formulation strategies, such as swapping out one candidate target variable
for another when the first fails to meet certain criteria (e.g.,
predictability), or composing multiple outcomes into a single target variable
to capture a more holistic set of modeling objectives. Based on our findings,
we present opportunities for future HCI, CSCW, and ML research to better
support the art and science of target variable construction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [52] [Gbake: Baking 3D Gaussian Splats into Reflection Probes](https://arxiv.org/abs/2507.02257)
*Stephen Pasch,Joel K. Salzman,Changxi Zheng*

Main category: cs.GR

TL;DR: GBake工具通过从高斯泼溅场景中烘焙反射探头，实现了在Unity中对传统3D网格进行真实反射映射，解决了直接插入时发光和几何不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 由于3D高斯原始数据同时编码照明和几何形状为外观，直接插入3D高斯混合环境中的网格会出现光照不匹配问题，导致明显的不协调。

Method: 引入了GBake工具，专门用于从高斯泼溅场景中烘焙反射探头，从而在Unity游戏引擎中实现传统3D网格的真实反射映射。

Result: GBake能够使传统3D网格在高斯泼溅环境中实现与周围场景一致的反射效果，提高视觉一致性。

Conclusion: GBake是一种有效的方法，解决了3D高斯泼溅环境中传统网格光照不一致的问题，为游戏和3D场景的集成提供了新的可能性。

Abstract: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

</details>


### [53] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 提出了一种高效的图像基光照近似方法，用于实时渲染具有闪烁或闪光外观的材料，支持动态材质属性与环境贴图。


<details>
  <summary>Details</summary>
Motivation: 解决实时渲染中材料闪烁效果在图像基光照下的高效计算问题。

Method: 基于实时区域光闪渲染和标准环境贴图滤波技术，通过分区环境贴图并结合正态分布函数计算微面反射概率，使用双门控高斯近似实现分层采样。

Result: 方法接近真实渲染效果，性能稳定，仅需两倍内存存储预处理环境贴图。

Conclusion: 提出了一种高效、实时的闪光材料渲染方案，适用于动态光照与材质。

Abstract: Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is grounded in real-time glint rendering under area
light illumination and employs standard environment map filtering techniques.
Crucially, our environment map filtering process is sufficiently fast to be
executed on a per-frame basis. Our method assumes that the environment map is
partitioned into few homogeneous regions of constant radiance. By filtering the
corresponding indicator functions with the normal distribution function, we
obtain the probabilities for individual microfacets to reflect light from each
region. During shading, these probabilities are utilized to hierarchically
sample a multinomial distribution, facilitated by our novel dual-gated Gaussian
approximation of binomial distributions. We validate that our real-time
approximation is close to ground-truth renderings for a range of material
properties and lighting conditions, and demonstrate robust and stable
performance, with little overhead over rendering glints from a single
directional light. Compared to rendering smooth materials without glints, our
approach requires twice as much memory to store the prefiltered environment
map.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [54] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE是一个高性能计算集群，专为LLM训练等高级工作负载设计，采用全开放网络栈，性能卓越。


<details>
  <summary>Details</summary>
Motivation: 展示开放和供应商中立技术在大规模HPC基础设施中的可行性，并优化高性能计算资源。

Method: 配置裸金属GPU服务器，采用800 GbE和SONiC开放网络栈，结合RoCEv2和Rail-Optimized拓扑。

Result: HPL得分排名全球49，HPL-MxP在FP8精度下达到339.86 PFLOP/s，具备高吞吐和低延迟存储。

Conclusion: SAKURAONE证明了开放技术在大规模HPC中的可行性和竞争力。

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>


### [55] [Signalling Health for Improved Kubernetes Microservice Availability](https://arxiv.org/abs/2507.02158)
*Jacob Roberts,Blair Archibald,Phil Trinder*

Main category: cs.DC

TL;DR: 论文比较了基于信号（SCM）和基于轮询（PCM）的容器监控方法，发现SCM在故障检测速度和可用性上优于PCM。


<details>
  <summary>Details</summary>
Motivation: 探讨如何提升容器监控的效率和准确性，以减少服务中断和误检。

Method: 设计和实现了一种基于信号的容器监控（SCM）方法，并与传统的轮询方法（PCM）进行对比实验。

Result: SCM比PCM故障检测速度快86%，且不会误检，服务可用性提升4%。

Conclusion: 建议在容器编排器中采用SCM以提升监控效率和可用性。

Abstract: Microservices are often deployed and managed by a container orchestrator that
can detect and fix failures to maintain the service availability critical in
many applications. In Poll-based Container Monitoring (PCM), the orchestrator
periodically checks container health. While a common approach, PCM requires
careful tuning, may degrade service availability, and can be slow to detect
container health changes. An alternative is Signal-based Container Monitoring
(SCM), where the container signals the orchestrator when its status changes. We
present the design, implementation, and evaluation of an SCM approach for
Kubernetes and empirically show that it has benefits over PCM, as predicted by
a new mathematical model. We compare the service availability of SCM and PCM
over six experiments using the SockShop benchmark. SCM does not require that
polling intervals are tuned, and yet detects container failure 86\% faster than
PCM and container readiness in a comparable time with limited resource
overheads. We find PCM can erroneously detect failures, and this reduces
service availability by 4\%. We propose that orchestrators offer SCM features
for faster failure detection than PCM without erroneous detections or careful
tuning.

</details>


### [56] [Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems](https://arxiv.org/abs/2507.02233)
*Bruce Fang,Danyi Gao*

Main category: cs.DC

TL;DR: 论文提出了一种基于迁移学习的智能故障根因识别算法，通过共享特征提取和域对抗机制提升模型性能，实验验证了方法的优越性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决云计算环境中因复杂系统结构、密集服务耦合和有限故障信息导致的故障根因识别难题。

Method: 采用迁移学习方法，引入共享特征提取模块和域对抗机制，并利用伪标签选择策略增强模型对少数类的识别能力。

Result: 在准确性、F1-Score和AUC等关键指标上优于现有主流方法，尤其在类别不平衡和目标域结构差异大的情况下表现突出。

Conclusion: 提出的机制在复杂云计算系统中具有高效性和实用价值，为故障根因识别提供了新思路。

Abstract: This paper addresses the challenge of fault root cause identification in
cloud computing environments. The difficulty arises from complex system
structures, dense service coupling, and limited fault information. To solve
this problem, an intelligent identification algorithm based on transfer
learning is proposed. The method introduces a shared feature extraction module
and a domain adversarial mechanism to enable effective knowledge transfer from
the source domain to the target domain. This improves the model's
discriminative ability and generalization performance in the target domain. The
model incorporates a pseudo-label selection strategy. When labeled samples are
lacking in the target domain, high-confidence predictions are used in training.
This enhances the model's ability to recognize minority classes. To evaluate
the stability and adaptability of the method in real-world scenarios,
experiments are designed under three conditions: label scarcity, class
imbalance, and heterogeneous node environments. Experimental results show that
the proposed method outperforms existing mainstream approaches in several key
metrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger
discriminative power and robustness. Notably, under extreme class imbalance and
significant structural differences in the target domain, the model still
maintains high performance. This validates the effectiveness and practical
value of the proposed mechanisms in complex cloud computing systems.

</details>


### [57] [Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources](https://arxiv.org/abs/2507.02295)
*Roopkatha Banerjee,Prince Modi,Jinal Vyas,Chunduru Sri Abhijit,Tejus Chandrashekar,Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.DC

TL;DR: 该论文介绍了Flotilla，一个轻量级且可扩展的联邦学习框架，解决了现有框架在分布式部署、异步聚合和容错性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着移动和边缘计算的进步以及对数据隐私的关注，联邦学习（FL）作为一种隐私保护的分布式机器学习方法受到广泛欢迎。然而，现有框架多用于伪分布式模拟，缺乏对实际边缘硬件部署的支持，且不支持异步聚合和容错性。

Method: Flotilla采用“用户优先”的模块化设计，支持同步和异步FL策略，且对DNN架构无关。它使用无状态客户端和分离会话状态的服务器设计，定期或增量检查点。

Result: Flotilla展示了其模块化能力，通过五种不同的FL策略和五种DNN模型验证。在200多个客户端上展示容错性，并在资源使用上优于现有框架如Flower、OpenFL和FedML。

Conclusion: Flotilla是一个高效且可扩展的FL框架，适合构建新型FL策略、快速部署和进行系统研究与优化。

Abstract: With the recent improvements in mobile and edge computing and rising concerns
of data privacy, Federated Learning(FL) has rapidly gained popularity as a
privacy-preserving, distributed machine learning methodology. Several FL
frameworks have been built for testing novel FL strategies. However, most focus
on validating the learning aspects of FL through pseudo-distributed simulation
but not for deploying on real edge hardware in a distributed manner to
meaningfully evaluate the federated aspects from a systems perspective. Current
frameworks are also inherently not designed to support asynchronous
aggregation, which is gaining popularity, and have limited resilience to client
and server failures. We introduce Flotilla, a scalable and lightweight FL
framework. It adopts a ``user-first'' modular design to help rapidly compose
various synchronous and asynchronous FL strategies while being agnostic to the
DNN architecture. It uses stateless clients and a server design that separates
out the session state, which are periodically or incrementally checkpointed. We
demonstrate the modularity of Flotilla by evaluating five different FL
strategies for training five DNN models. We also evaluate the client and
server-side fault tolerance on 200+ clients, and showcase its ability to
rapidly failover within seconds. Finally, we show that Flotilla's resource
usage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or
better than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It
also scales significantly better compared to Flower for 1000+ clients. This
positions Flotilla as a competitive candidate to build novel FL strategies on,
compare them uniformly, rapidly deploy them, and perform systems research and
optimizations.

</details>


### [58] [Alps, a versatile research infrastructure](https://arxiv.org/abs/2507.02404)
*Maxime Martinasso,Mark Klein,Thomas C. Schulthess*

Main category: cs.DC

TL;DR: 瑞士国家超级计算中心（CSCS）开发了新一代高性能计算基础设施Alps，通过资源独立端点和高速网络的架构，解决了传统HPC缺乏灵活性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统HPC架构在科学需求多样化下面临灵活性不足的挑战，促使CSCS开发更适应需求的Alps系统。

Method: Alps采用异构硬件（CPU、GPU）和高速Slingshot网络，并引入软件定义集群技术（vCluster）实现分层管理。

Result: Alps成功支持了包括数值天气预报和AI研究在内的多领域科学需求。

Conclusion: Alps的模块化设计和灵活架构为多样化科学应用提供了高效解决方案。

Abstract: The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition
of delivering top-tier high-performance computing systems, exemplified by the
Piz Daint supercomputer. However, the increasing diversity of scientific needs
has exposed limitations in traditional vertically integrated HPC architectures,
which often lack flexibility and composability. To address these challenges,
CSCS developed Alps, a next-generation HPC infrastructure designed with a
transformative principle: resources operate as independent endpoints within a
high-speed network. This architecture enables the creation of independent
tenant-specific and platform-specific services, tailored to diverse scientific
requirements.
  Alps incorporates heterogeneous hardware, including CPUs and GPUs,
interconnected by a high-performance Slingshot network, and offers a modular
storage system. A key innovation is the versatile software-defined cluster
(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting
infrastructure, service management, and user environments into distinct layers,
vClusters allow for customized platforms that support diverse workloads.
Current platforms on Alps serve various scientific domains, including numerical
weather prediction, and AI research.

</details>


### [59] [FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference](https://arxiv.org/abs/2507.02620)
*Xing Liu,Lizhuo Luo,Ming Tang,Chao Huang*

Main category: cs.DC

TL;DR: FlowSpec是一种基于管道并行和树状推测解码的框架，旨在提高分布式大型语言模型推断在边缘网络的效率。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘网络中推断请求稀疏时现有管道并行方法利用率低的问题。

Method: 通过1)基于分数的逐步验证优先重要草案标记；2)有效管理草案以修剪无效标记；3)动态扩展策略提供高质量推测输入。

Result: 实验表明，FlowSpec在多样模型和配置下显著提升推断速度，比基线快1.36×-1.77×。

Conclusion: FlowSpec有效提升管道利用率和推测效率，适用于边缘网络中的分布式LLM推断。

Abstract: Distributed inference serves as a promising approach to enabling the
inference of large language models (LLMs) at the network edge. It distributes
the inference process to multiple devices to ensure that the LLMs can fit into
the device memory. Recent pipeline-based approaches have the potential to
parallelize communication and computation, which helps reduce inference
latency. However, the benefit diminishes when the inference request at the
network edge is sparse, where pipeline is typically at low utilization. To
enable efficient distributed LLM inference at the edge, we propose
\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding
framework. FlowSpec incorporates three key mechanisms to improve decoding
efficiency: 1) score-based step-wise verification prioritizes more important
draft tokens to bring earlier accpeted tokens; 2) efficient draft management to
prune invalid tokens while maintaining correct causal relationship during
verification; 3) dynamic draft expansion strategies to supply high-quality
speculative inputs. These techniques work in concert to enhance both pipeline
utilization and speculative efficiency. We evaluate FlowSpec on a real-world
testbed with other baselines. Experimental results demonstrate that our
proposed framework significantly improves inference speed across diverse models
and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared
to baselines. Our code is publicly available at
\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [Template-Based Schema Matching of Multi-Layout Tenancy Schedules:A Comparative Study of a Template-Based Hybrid Matcher and the ALITE Full Disjunction Model](https://arxiv.org/abs/2507.02020)
*Tim Uilkema,Yao Ma,Seyed Sahand Mohammadi Ziabari,Joep van Vliet*

Main category: cs.DB

TL;DR: 论文提出了一种新的基于模板的混合模式匹配器，用于解决房地产公司租赁日程数据格式不统一导致的集成效率问题，显著提升了匹配效果。


<details>
  <summary>Details</summary>
Motivation: 房地产公司租赁日程数据格式不统一，导致数据集成效率低下，现有方法虽注重完整性但存在模式膨胀和业务可用性不足的问题。

Method: 提出了一种结合模式（Jaccard、Levenshtein）和实例（数据类型、分布）指标的混合匹配器，通过匈牙利算法确定全局最优匹配。

Result: 优化后的匹配器F1分数达0.881，空值率为45.7%，优于ALITE的0.712和75.6%。

Conclusion: 结合结构化业务知识和混合匹配方法，能够生成更具业务可用性的模式映射，未来可扩展到复杂表格。

Abstract: The lack of standardized tabular formats for tenancy schedules across real
estate firms creates significant inefficiencies in data integration. Existing
automated integration methods, such as Full Disjunction (FD)-based models like
ALITE, prioritize completeness but result in schema bloat, sparse attributes
and limited business usability. We propose a novel hybrid, template-based
schema matcher that aligns multi-layout tenancy schedules to a predefined
target schema. The matcher combines schema (Jaccard, Levenshtein) and
instance-based metrics (data types, distributions) with globally optimal
assignments determined via the Hungarian Algorithm. Evaluation against a
manually labeled ground truth demonstrates substantial improvements, with grid
search optimization yielding a peak F1-score of 0.881 and an overall null
percentage of 45.7%. On a separate ground truth of 20 semantically similar
column sets, ALITE achieves an F1-score of 0.712 and 75.6% nulls. These results
suggest that combining structured business knowledge with hybrid matching can
yield more usable and business-aligned schema mappings. The approach assumes
cleanly extracted tabular input, future work could explore extending the
matcher to support complex, composite tables.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [61] [Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions](https://arxiv.org/abs/2507.02067)
*Nikolaos Papanikolaou,Doha Touhafi,Jurgen Vandendriessche,Danial Karimi,Sohail Fatimi,Gianluca Cornetta,Abdellah Touhafi*

Main category: cs.AR

TL;DR: 印刷传感器通过创新印刷技术实现灵活、低成本和高定制化的传感设备，适用于广泛环境监测。


<details>
  <summary>Details</summary>
Motivation: 开发一种灵活、低成本且多功能的传感器，以满足环境监测的多样化需求。

Method: 采用创新的印刷技术制造传感器，以实现高灵敏度和高精度的检测能力。

Result: 传感器能有效检测污染物、温湿度变化等关键环境参数，表现优异。

Conclusion: 印刷传感器在环境监测领域具有广阔的应用前景和潜力。

Abstract: Printed sensors represent a transformative advancement in sensor technology,
utilizing innovative printing techniques to create flexible, cost-effective,
and highly customizable sensing devices. Their versatility allows integration
into numerous applications across diverse fields such as monitoring a wide
range of environmental factors e.g. air and water quality, soil conditions, and
atmospheric changes among others. These sensors demonstrate high sensitivity
and accuracy in detecting pollutants, temperature variations, humidity levels,
and other critical parameters essential for environmental assessment and
protection.

</details>


### [62] [Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting](https://arxiv.org/abs/2507.02164)
*Ruibai Tang,Chengbin Quan*

Main category: cs.AR

TL;DR: 该论文提出了一种基于FPGA硬件加速的算法，通过多项式近似和单位移QR迭代高效计算并可视化复函数根的密度图。


<details>
  <summary>Details</summary>
Motivation: 解决复函数根的求解和可视化问题，传统方法计算量大，需要更高效的硬件加速方案。

Method: 利用伴随矩阵的Hessenberg结构，通过Givens旋转优化QR分解，设计流水线FPGA架构，处理大量多项式。

Result: 比基于CPU的方法能效提高65倍，但性能仍落后于现代GPU。

Conclusion: FPGA架构在复函数根密度图计算中表现出高效能，但制造工艺限制性能提升。

Abstract: Solving and visualizing the potential roots of complex functions is essential
in both theoretical and applied domains, yet often computationally intensive.
We present a hardware-accelerated algorithm for complex function roots density
graph plotting by approximating functions with polynomials and solving their
roots using single-shift QR iteration. By leveraging the Hessenberg structure
of companion matrices and optimizing QR decomposition with Givens rotations, we
design a pipelined FPGA architecture capable of processing a large amount of
polynomials with high throughput. Our implementation achieves up to 65x higher
energy efficiency than CPU-based approaches, and while it trails modern GPUs in
performance due to differences in fabrication technique.

</details>


### [63] [System-performance and cost modeling of Large Language Model training and inference](https://arxiv.org/abs/2507.02456)
*Wenzhe Guo,Joyjit Kundu,Uras Tos,Weijiang Kong,Giuliano Sisto,Timon Evenblij,Manu Perumkunnil*

Main category: cs.AR

TL;DR: 本文提出了一种用于大型语言模型（LLM）训练和推理的性能-成本建模方法，结合了先进的计算、内存优化和通信技术，以解决分布式系统中的可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模和复杂度的指数增长，计算能力、内存带宽、网络性能和成本效率的提升未能跟上，这对其在分布式系统中的可扩展性提出了重大挑战。

Method: 研究方法基于分析性能模型，整合了最新的计算技术（如闪存注意力技术和混合专家模型）、内存优化以及网络拓扑和通信算法（如5D并行）。还包括一个芯片成本模型。

Result: 提出的建模方法能够分析不同系统架构配置的性能-成本权衡，为未来的计算系统设计提供有价值的指导。

Conclusion: 该框架为硬件-软件协同开发提供了支持，尤其是其分析性能-成本权衡的能力，有助于优化LLM的可扩展性。

Abstract: Large language models (LLMs), based on transformer architectures, have
revolutionized numerous domains within artificial intelligence, science, and
engineering due to their exceptional scalability and adaptability. However, the
exponential growth in LLM size and complexity has outpaced advancements in
compute capacity, memory bandwidth, network performance, and cost efficiency,
posing significant challenges to their scalability on distributed systems. To
address these limitations, alternative model architectures, optimization
strategies, communication-aware network topologies, and novel system design
approaches have been proposed in literature. This paper introduces a
performance-cost modeling methodology for LLM training and inference that
integrates state-of-the-art compute techniques with memory optimizations, and
latest communication techniques. Building on an analytical performance model,
our approach incorporates recent innovations such as the flash attention
technique and mixture of experts models to address the memory bandwidth and
compute bottlenecks. It also considers the impact of different network
topologies and topology-specific communication algorithms with 5D parallellism.
The framework also integrates a chiplet cost model. The proposed modeling
methodology provides valuable insights to guide future compute system design
and facilitates hardware-software co-development, in particular due to its
ability to analyze performance-cost trade-offs for various system architectural
configurations.

</details>


### [64] [AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models](https://arxiv.org/abs/2507.02598)
*Chenhao Xue,Kezhi Li,Jiaxing Zhang,Yi Ren,Zhengyuan Shi,Chen Zhang,Yibo Lin,Lining Zhang,Qiang Xu,Guangyu Sun*

Main category: cs.AR

TL;DR: 论文提出AC-Refiner框架，利用条件扩散模型优化算术电路设计，通过将电路合成转化为条件图像生成任务，显著提升设计质量和帕累托最优性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 算术电路（如加法器和乘法器）是数字系统的核心组件，其优化面临设计空间大和物理约束复杂的挑战，现有深度学习方法难以高效探索高潜力设计变体。

Method: 提出AC-Refiner框架，基于条件扩散模型，将电路合成任务转化为条件图像生成，并结合目标性能指标（QoRs）指导去噪过程，同时利用探索的设计微调模型以聚焦帕累托前沿。

Result: 实验结果表明，AC-Refiner生成的电路设计在帕累托最优性上优于现有基准方法，且在实际应用中验证了性能提升。

Conclusion: AC-Refiner通过创新地将电路优化问题转化为条件生成任务，显著提升了设计效率和性能，为算术电路优化提供了新思路。

Abstract: Arithmetic circuits, such as adders and multipliers, are fundamental
components of digital systems, directly impacting the performance, power
efficiency, and area footprint. However, optimizing these circuits remains
challenging due to the vast design space and complex physical constraints.
While recent deep learning-based approaches have shown promise, they struggle
to consistently explore high-potential design variants, limiting their
optimization efficiency. To address this challenge, we propose AC-Refiner, a
novel arithmetic circuit optimization framework leveraging conditional
diffusion models. Our key insight is to reframe arithmetic circuit synthesis as
a conditional image generation task. By carefully conditioning the denoising
diffusion process on target quality-of-results (QoRs), AC-Refiner consistently
produces high-quality circuit designs. Furthermore, the explored designs are
used to fine-tune the diffusion model, which focuses the exploration near the
Pareto frontier. Experimental results demonstrate that AC-Refiner generates
designs with superior Pareto optimality, outperforming state-of-the-art
baselines. The performance gain is further validated by integrating AC-Refiner
into practical applications.

</details>


### [65] [Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure](https://arxiv.org/abs/2507.02654)
*Rui Xie,Asad Ul Haq,Yunhua Fang,Linsen Ma,Sanchari Sen,Swagath Venkataramani,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 论文提出了一种系统级方法，通过将故障管理转移到内存控制器，并结合领域特定的ECC框架，显著降低HBM成本，同时在高误码率下保持性能和模型精度。


<details>
  <summary>Details</summary>
Motivation: HBM的高成本限制了其在AI领域的可扩展部署，特别是由于严格的片上可靠性要求。本文旨在通过系统级优化降低成本。

Method: 采用领域特定的ECC框架，结合大码字Reed-Solomon纠错、轻量级细粒度CRC检测、差异奇偶更新以减少写入放大，并根据数据重要性调整保护级别。

Result: 在高误码率（$10^{-3}$）下，系统仍能保留78%的吞吐量和97%的模型精度，接近理想无错HBM的性能。

Conclusion: 通过将可靠性作为可调系统参数而非固定硬件约束，本文设计为低成本、高性能HBM在AI基础设施中的部署提供了新途径。

Abstract: High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy
efficiency for AI workloads, but its high cost per bit, driven in part by
stringent on-die reliability requirements, poses a growing barrier to scalable
deployment. This work explores a system-level approach to cost reduction by
eliminating on-die ECC and shifting all fault management to the memory
controller. We introduce a domain-specific ECC framework combining
large-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC
detection, differential parity updates to mitigate write amplification, and
tunable protection based on data importance. Our evaluation using LLM inference
workloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the
system retains over 78\% of throughput and 97\% of model accuracy compared with
systems equipped with ideal error-free HBM. By treating reliability as a
tunable system parameter rather than a fixed hardware constraint, our design
opens a new path toward low-cost, high-performance HBM deployment in AI
infrastructure.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 论文批评了现有信念修正研究过于依赖后验假设而忽视了修正机制的灵活性分析，提出修正机制应具备多种能力（如可塑性、可平等化、教条化等），并列举了一些修正机制所具备的能力。


<details>
  <summary>Details</summary>
Motivation: 现有信念修正研究过于依赖后验假设，而忽视了修正机制的灵活性和能力分析。论文旨在探讨修正机制应具备的多种能力，以满足不同应用场景的需求。

Method: 论文通过列举多种修正机制（如词典序、自然修正等），分析它们各自具备或缺乏的能力（如可塑性、教条化等），从而展示不同修正机制的特点。

Result: 研究表明不同修正机制具备不同的能力，例如某些机制可以实现教条化或平等化信念状态，而其他机制则缺乏这些能力。

Conclusion: 论文呼吁在信念修正研究中不仅要关注后验假设的约束性，还应重视修正机制的灵活性能力，以适应多样化的应用需求。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [67] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出了一种基于AI代理的硬件设计验证方法，结合人类干预（HITL），实现动态、迭代的端到端验证，验证开源设计时覆盖率超过95%，且时间缩短。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路设计复杂度高，验证过程耗时且需严谨，自然语言处理中的大语言模型（LLMs）为硬件验证提供了新思路。

Method: 采用AI代理与人类协作（HITL）的动态迭代方法，进行端到端硬件设计与验证。

Result: 在五种开源设计中，覆盖率达95%以上，同时减少验证时间，表现出高性能、适应性和可配置性。

Conclusion: AI代理结合人类干预的方法显著提升硬件验证效率与效果，验证了其优越性。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [68] [Public perspectives on the design of fusion energy facilities](https://arxiv.org/abs/2507.02207)
*Nathan Kawamoto,Daniel Hoover,Jonathan Xie,Jacob Walters,Katie Snyder,Aditi Verma*

Main category: physics.soc-ph

TL;DR: 论文探讨了通过参与式设计方法，在聚变能源设施选址和设计阶段融入社区意见，以实现社会认可。研究发现，社区和学生的设计价值观包括诚信和尊重，决策标准则围绕经济利益和环境保护。


<details>
  <summary>Details</summary>
Motivation: 随着聚变能源技术接近商业部署，公众对设施的接受度对社会许可至关重要，特别是因为聚变设施的选址可能与社区更近。传统决策方式（决定-宣布-辩护）不足以满足需求。

Method: 采用参与式设计方法，组织社区参与者和工程学生共同设计聚变能源设施，分析文本和视觉数据，提取设计价值观和决策标准。

Result: 研究发现，诚信和尊重是最重要的价值观，经济利益和环境保护是最关键的决策标准。设计主题包括社区历史传承、工人关怀、透明度及健康安全。

Conclusion: 早期引入参与式设计可以明确公众期望与担忧，提升对新兴技术的理解与兴趣，促进社会认可，并为聚变设施开发提供针对性指导。

Abstract: As fusion energy technologies approach demonstration and commercial
deployment, understanding public perspectives on future fusion facilities will
be critical for achieving social license, especially because fusion energy
facilities, unlike large fission reactors, may be sited in closer proximity to
people and communities, due to distinct regulatory frameworks. In a departure
from the 'decide-announce-defend' approach typically used to site energy
infrastructure, we develop a participatory design methodology for
collaboratively designing fusion energy facilities with prospective host
communities. We present here our findings from a participatory design workshop
that brought together 22 community participants and 34 engineering students.
Our analysis of the textual and visual data from this workshop shows a range of
design values and decision-making criteria with 'integrity' and 'respect'
ranking highest among values and 'economic benefits' and 'environmental
protection/safety' ranking highest among decision-making criteria. Salient
design themes that emerge across facility concepts include connecting the
history and legacy of the community to the design of the facility, care for
workers, transparency and access to the facility, and health and safety of the
host community. Participants reported predominantly positive sentiments,
expressing joy and surprise as the workshop progressed from learning about
fusion to designing the hypothetical facility. Our findings suggest that
carrying out participatory design in the early stages of technology development
can invite and make concrete public hopes and concerns, improve understanding
of, and curiosity about, an emerging technology, build toward social license,
and inform context-specific development of fusion energy facilities.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [Detecting Multiple Diseases in Multiple Crops Using Deep Learning](https://arxiv.org/abs/2507.02517)
*Vivek Yadav,Anugrah Jain*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度学习的解决方案，用于检测多种作物中的多种疾病，旨在覆盖印度多样化的农业景观，显著提高了检测准确率和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 印度作为主要农业经济国家，作物因病害、害虫和环境压力遭受重大损失，早期准确检测对提高产量和确保食品安全至关重要。

Method: 论文首先创建了一个包含17种作物和34种疾病的统一数据集，并在此基础上训练了一个深度学习模型。

Result: 该模型在统一数据集上实现了99%的检测准确率，比现有技术（覆盖14种作物和26种疾病）提高了7%。

Conclusion: 通过增加可检测的作物和疾病类型，该解决方案旨在为印度农民提供更优的产品。

Abstract: India, as a predominantly agrarian economy, faces significant challenges in
agriculture, including substantial crop losses caused by diseases, pests, and
environmental stress. Early detection and accurate identification of diseases
across different crops are critical for improving yield and ensuring food
security. This paper proposes a deep learning based solution for detecting
multiple diseases in multiple crops, aimed to cover India's diverse
agricultural landscape. We first create a unified dataset encompassing images
of 17 different crops and 34 different diseases from various available
repositories. Proposed deep learning model is trained on this dataset and
outperforms the state-of-the-art in terms of accuracy and the number of crops,
diseases covered. We achieve a significant detection accuracy, i.e., 99 percent
for our unified dataset which is 7 percent more when compared to
state-of-the-art handling 14 crops and 26 different diseases only. By improving
the number of crops and types of diseases that can be detected, proposed
solution aims to provide a better product for Indian farmers.

</details>


### [70] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: Mesh Silksong是一种紧凑高效的网格表示方法，通过自回归方式生成多边形网格，减少顶点标记冗余50%，并实现约22%的最先进压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有网格标记化方法存在顶点标记重复的问题，浪费网络能力，Mesh Silksong通过改进标记化方式解决这一问题。

Method: Mesh Silksong通过自回归网格表示方式，每个顶点仅标记一次，减少标记序列冗余，并优化几何属性如流形拓扑、水密检测和一致面法线。

Result: 实验证明Mesh Silksong不仅能生成精细网格，还显著提升几何完整性。

Conclusion: Mesh Silksong在网格压缩和几何属性优化方面表现出色，适用于实际应用。

Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [71] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 本文提出了一种自蒸馏方法，扩展了视频到音频（V2A）生成模型在电影语言场景中的应用，显著提升了在部分可见性条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前V2A方法忽略了电影语言，导致在Foley目标仅部分可见时性能下降。

Method: 通过模拟电影语言变化，自蒸馏模型学习对齐相同视听对应的视频特征，有效捕捉声音与部分视觉信息的关联。

Result: 方法在所有评估指标下显著提升了部分可见性场景的性能，并在VGGSound数据集上表现优异。

Conclusion: 自蒸馏方法有效解决了V2A生成中电影语言缺失的问题，提升了模型适用性。

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [72] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: 提出了一种名为HyperGaussians的新方法，扩展了3D高斯泼溅技术，用于高质量可动画化面部头像，解决了现有方法在非线性变形和复杂光照效果上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅技术在静态面部表现优秀，但在可动画化头像和复杂细节处理上仍有局限，因此需要更高效且表达能力强的表示方法。

Method: 通过引入高维多变量高斯（HyperGaussians）并利用可学习的局部嵌入增加表达能力，同时通过‘逆协方差技巧’提升计算效率。

Result: 在4个面部数据集上的19名受试者中，HyperGaussians在数值和视觉效果上均优于3D高斯泼溅技术，尤其在细节处理如眼镜框、牙齿和复杂面部动作上表现突出。

Conclusion: HyperGaussians为可动画化面部头像提供了一种更高效的表示方法，显著提升了视觉质量和细节表现。

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [73] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality是一种新方法，可将室内环境的RGB-D扫描转换为紧凑、逼真且可交互的3D虚拟复制品，支持高质量渲染和物理交互。


<details>
  <summary>Details</summary>
Motivation: 将室内环境的扫描数据转换为逼真且功能齐全的3D虚拟场景，以满足AR/VR、游戏、机器人学和数字孪生等应用需求。

Method: 1. 场景解析为3D布局和对象图；2. 从数据库中检索视觉相似的3D模型；3. 使用材料绘画模块增强真实感；4. 整合到仿真引擎以支持交互。

Result: 生成的场景紧凑、可编辑且完全兼容标准图形管线，在Scan2CAD基准测试中取得了最先进的相似性性能。

Conclusion: LiteReality不仅提供高质量的3D重建，还支持交互功能，适用于多种应用场景。

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [74] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: 研究提出利用FPGA加速ANN，通过FINN架构部署不同量化模型，提升机器人检测速度。


<details>
  <summary>Details</summary>
Motivation: 机器人因检测速度和摄像头的低帧率限制任务执行效率，现有工具未能充分利用FPGA资源。

Method: 使用FINN架构在FPGA中部署3种量化ANN模型（MobileNet v1、CNV），训练基于RG2C数据集。

Result: MobileNet v1表现最佳，成功率达98%，推理速度达6611 FPS。

Conclusion: 验证了FPGA加速ANN的可行性，使其适用于注意力机制。

Abstract: Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit
quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation
(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This
is a self-acquired dataset released in open access. MobileNet v1 performed
better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In
this work, we proved that we can use FPGAs to speed up ANNs and make them
suitable for attention mechanisms.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [75] [Access Control Threatened by Quantum Entanglement](https://arxiv.org/abs/2507.02622)
*Zhicheng Zhang,Mingsheng Ying*

Main category: quant-ph

TL;DR: 本文研究了量子计算机系统中的访问控制问题，首次展示了经典安全访问控制系统直接应用于量子环境时的安全漏洞，并提出新的量子访问控制模型以应对威胁。


<details>
  <summary>Details</summary>
Motivation: 量子力学的特性（如纠缠和违反Mermin不等式）可能导致经典访问控制系统在量子环境中失效，因此需要研究新的访问控制模型。

Method: 提出多种新的量子访问控制模型，并对其安全性、灵活性和效率进行了严格分析。

Result: 揭示了量子纠缠对访问控制的潜在威胁，并提供了可行的保护方案。

Conclusion: 量子计算环境需要重新设计访问控制系统，以应对量子力学带来的新安全挑战。

Abstract: Access control is a cornerstone of computer security that prevents
unauthorised access to resources. In this paper, we study access control in
quantum computer systems. We present the first explicit scenario of a security
breach when a classically secure access control system is straightforwardly
adapted to the quantum setting. The breach is ultimately due to that quantum
mechanics allows the phenomenon of entanglement and violates Mermin inequality,
a multi-party variant of the celebrated Bell inequality. This reveals a threat
from quantum entanglement to access control if existing computer systems
integrate with quantum computing. To protect against such threat, we propose
several new models of quantum access control, and rigorously analyse their
security, flexibility and efficiency.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [76] [Computer Science Education in the Age of Generative AI](https://arxiv.org/abs/2507.02183)
*Russell Beale*

Main category: cs.CY

TL;DR: 生成式AI（如ChatGPT和Codex）正在改变计算机科学教育，提供编码辅助和创新教学方法的机会，同时也带来学术诚信和过度依赖等挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI如何为计算机科学教育带来变革，同时应对相关挑战。

Method: 分析AI在编程教学中的应用，提出课程整合和评估策略，支持实证数据。

Result: 提出了在AI时代保持教育严谨性的政策建议。

Conclusion: 生成式AI有潜力提升计算机科学教育，但需平衡创新与学术诚信。

Abstract: Generative AI tools - most notably large language models (LLMs) like ChatGPT
and Codex - are rapidly revolutionizing computer science education. These tools
can generate, debug, and explain code, thereby transforming the landscape of
programming instruction. This paper examines the profound opportunities that AI
offers for enhancing computer science education in general, from coding
assistance to fostering innovative pedagogical practices and streamlining
assessments. At the same time, it highlights challenges including academic
integrity concerns, the risk of over-reliance on AI, and difficulties in
verifying originality. We discuss what computer science educators should teach
in the AI era, how to best integrate these technologies into curricula, and the
best practices for assessing student learning in an environment where AI can
generate code, prototypes and user feedback. Finally, we propose a set of
policy recommendations designed to harness the potential of generative AI while
preserving the integrity and rigour of computer science education. Empirical
data and emerging studies are used throughout to support our arguments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 该论文提出了可扩展的可微分布尔逻辑网络（DBNs），通过可训练且参数固定的互联结构，使其适用于更宽的输入层，并通过两种互补的剪枝方法进一步减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有的DBNs在资源受限的硬件上已表现出高效的推理能力，但需要进一步扩展其适用范围并减小模型规模。

Method: 扩展DBNs的可训练互联结构，使其参数固定且适用于更宽的输入层；提出两种剪枝方法：基于逻辑等价性的SAT剪枝和数据驱动的相似性剪枝。

Result: 改进后的DBNs在保持高精度的同时，能够扩展到更宽的输入层，并通过剪枝方法显著减小模型规模。

Conclusion: 该研究为DBNs提供了可扩展性和高效压缩的新方法，进一步提升了其在资源受限环境中的适用性。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [78] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 综述论文探讨了Transformer模型在脑电图（EEG）解码中的最新应用，包括其基础原理、混合架构及未来发展。


<details>
  <summary>Details</summary>
Motivation: 推动EEG解码技术的发展，利用深度学习方法（如Transformer）提高脑机接口的性能和效率。

Method: 通过调查和总结Transformer在EEG解码中的应用，分析其架构演变和与其他深度学习技术（如卷积/循环神经网络）的结合。

Result: 整理并概述了Transformer在EEG解码中的进展，展示了其优势和潜力。

Conclusion: Transformer在EEG解码中表现出巨大潜力，但仍面临挑战，未来研究需进一步优化和探索混合架构。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [79] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的跨被试运动想象（CS-MI）分类方法，通过优化的预处理和深度学习技术显著提升了分类性能，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 跨被试运动想象分类在脑机接口（BCI）中由于个体间脑电图（EEG）模式差异大而具有挑战性。该研究旨在解决这一问题，推动无需校准的BCI在实际中的应用。

Method: 方法包括直接分类短时傅里叶变换（STFT）处理的EEG数据、优化STFT参数，以及在训练卷积神经网络（CNN）时采用平衡批处理策略。

Result: 在多个数据集上取得显著改进的分类准确率（如BCI Competition IV数据集最高80.22%），优于现有技术。

Conclusion: 该研究为通用、无需校准的MI分类设立了新基准，同时贡献了一个开放数据集以推动领域研究。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [80] [Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems](https://arxiv.org/abs/2507.02464)
*Craig S Wright*

Main category: cs.GT

TL;DR: 本文通过自动机理论和经济学框架重构了CAP理论中的权衡问题，证明在有限误差范围内可同时保持可用性和一致性。


<details>
  <summary>Details</summary>
Motivation: CAP理论的经典三难问题限制了分布式系统的设计。本文旨在通过经济激励机制和形式化方法扩展CAP的极限。

Method: 采用分区感知的状态机建模分布式系统，并嵌入游戏论机制以稳定分区网络下的共识行为。

Result: 在有限误差范围内，可用性和一致性可同时实现，扩展了CAP理论的经典限制。

Conclusion: 通过经济控制的形式化方法，可以有效管理CAP权衡，提升分布式系统的性能和稳定性。

Abstract: The CAP theorem asserts a trilemma between consistency, availability, and
partition tolerance. This paper introduces a rigorous automata-theoretic and
economically grounded framework that reframes the CAP trade-off as a constraint
optimization problem. We model distributed systems as partition-aware state
machines and embed economic incentive layers to stabilize consensus behavior
across adversarially partitioned networks. By incorporating game-theoretic
mechanisms into the global transition semantics, we define provable bounds on
convergence, liveness, and correctness. Our results demonstrate that
availability and consistency can be simultaneously preserved within bounded
epsilon margins, effectively extending the classical CAP limits through formal
economic control.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [81] [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
*Yongsen Zheng,Zongxuan Xie,Guohua Wang,Ziyao Liu,Liang Lin,Kwok-Yan Lam*

Main category: cs.IR

TL;DR: HyFairCRS是一个新颖框架，旨在通过超图对比多兴趣学习解决对话推荐系统中的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的不公平性可能导致基于性别、种族、年龄或流行度等因素的偏见结果，这一问题在动态交互环境中尤为严重。

Method: 通过对比学习建立多样化超图，捕捉用户多兴趣，并在对话中生成公平的推荐。

Result: 在两个数据集上，HyFairCRS展现出最先进的性能并有效减少不公平性。

Conclusion: HyFairCRS为动态对话推荐系统中的公平性问题提供了有效解决方案。

Abstract: Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [82] [SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection](https://arxiv.org/abs/2507.02635)
*Mao Luo,Zhi Wang,Yiwen Huang,Qingyun Zhang,Zhouxing Su,Zhipeng Lv,Wen Hu,Jianguo Li*

Main category: cs.CR

TL;DR: 电子支付平台的高交易量中，验证规则的漏洞可能引发重大损失，需系统化方法确保其稳健性。


<details>
  <summary>Details</summary>
Motivation: 高交易量下，手工构建的验证规则易受漏洞利用，可能导致财务损失，亟需系统性改进。

Method: 手工构建验证规则，但缺乏系统化方法确保其抗漏洞能力。

Result: 当前规则易被恶意请求绕过，需系统性识别规则缺陷。

Conclusion: 需开发系统化方法提升验证规则的稳健性，以减少漏洞利用风险。

Abstract: Electronic payment platforms are estimated to process billions oftransactions
daily, with the cumulative value of these transactionspotentially reaching into
the trillions. Even a minor error within thishigh-volume environment could
precipitate substantial financiallosses. To mitigate this risk, manually
constructed verification rules,developed by domain experts, are typically
employed to identifyand scrutinize transactions in production environments.
However,due to the absence of a systematic approach to ensure the robust-ness
of these verification rules against vulnerabilities, they remainsusceptible to
exploitation.To mitigate this risk, manually constructed verification rules,
de-veloped by domain experts, are typically employed to identify andscrutinize
transactions in production environments. However, dueto the absence of a
systematic approach to ensure the robustness ofthese verification rules against
vulnerabilities, they remain suscep-tible to exploitation. To ensure data
security, database maintainersusually compose complex verification rules to
check whether aquery/update request is valid. However, the rules written by
ex-perts are usually imperfect, and malicious requests may bypassthese rules.
As a result, the demand for identifying the defects ofthe rules systematically
emerges.

</details>


### [83] [Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain](https://arxiv.org/abs/2507.02536)
*Azmat Ullah,Maria Ilaria Lunesu,Lodovica Marchesi,Roberto Tonelli*

Main category: cs.CR

TL;DR: 基于区块链的物联网系统用于监控餐厅披萨生产，实现数据安全和透明化。


<details>
  <summary>Details</summary>
Motivation: 通过结合区块链和物联网技术，提升披萨生产过程中的数据安全性和管理效率。

Method: 使用物联网设备实时监测温湿度，通过树莓派处理数据并与智能合约交互，区块链保障数据不可篡改。

Result: 实验表明系统能有效管理原料、减少浪费并提高厨房效率。

Conclusion: 该系统为食品生产行业提供了高效、透明且安全的解决方案。

Abstract: This paper presents a blockchain-based Internet of Things (IoT) system for
monitoring pizza production in restaurants. IoT devices track temperature and
humidity in real-time, while blockchain ensures secure and tamper-proof data. A
Raspberry Pi processes sensor data, captures images, triggers alerts, and
interacts with smart contracts. The system detects abnormal conditions,
enabling quick responses. Blockchain adds transparency and traceability,
supporting compliance and audits. Experiments show improved ingredient
management, reduced waste, and increased kitchen efficiency.

</details>


### [84] [Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures](https://arxiv.org/abs/2507.02607)
*Frida Sundfeldt,Bianca Widstam,Mahshid Helali Moghadam,Kuo-Yun Liang,Anders Vesterberg*

Main category: cs.CR

TL;DR: 本文提出了一种生成高质量攻击数据的方法，通过上下文感知的攻击数据生成器，模拟多种攻击场景，用于车辆入侵检测系统的训练和评估。


<details>
  <summary>Details</summary>
Motivation: 随着联网车辆的数字化发展，安全风险增加，传统方法因安全、成本和伦理限制难以获取攻击数据，亟需高效的方法生成攻击数据。

Method: 提出一种基于参数化攻击模型的上下文感知攻击数据生成器，模拟多种攻击类型，并结合CAN消息解码和攻击强度调整，提高数据真实性。

Result: 生成的攻击数据在入侵检测系统案例中表现出高效性和可扩展性，深度学习模型展现出高检测和分类能力。

Conclusion: 该方法能有效生成高质量攻击数据，支持入侵检测系统的开发和评估，同时讨论了数据保真度和实际应用中的关键因素。

Abstract: The digital evolution of connected vehicles and the subsequent security risks
emphasize the critical need for implementing in-vehicle cyber security measures
such as intrusion detection and response systems. The continuous advancement of
attack scenarios further highlights the need for adaptive detection mechanisms
that can detect evolving, unknown, and complex threats. The effective use of
ML-driven techniques can help address this challenge. However, constraints on
implementing diverse attack scenarios on test vehicles due to safety, cost, and
ethical considerations result in a scarcity of data representing attack
scenarios. This limitation necessitates alternative efficient and effective
methods for generating high-quality attack-representing data. This paper
presents a context-aware attack data generator that generates attack inputs and
corresponding in-vehicle network log, i.e., controller area network (CAN) log,
representing various types of attack including denial of service (DoS), fuzzy,
spoofing, suspension, and replay attacks. It utilizes parameterized attack
models augmented with CAN message decoding and attack intensity adjustments to
configure the attack scenarios with high similarity to real-world scenarios and
promote variability. We evaluate the practicality of the generated
attack-representing data within an intrusion detection system (IDS) case study,
in which we develop and perform an empirical evaluation of two deep neural
network IDS models using the generated data. In addition to the efficiency and
scalability of the approach, the performance results of IDS models, high
detection and classification capabilities, validate the consistency and
effectiveness of the generated data as well. In this experience study, we also
elaborate on the aspects influencing the fidelity of the data to real-world
scenarios and provide insights into its application.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [85] [DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems](https://arxiv.org/abs/2507.02400)
*Maximilian Zipfl,Pascal Zwick,Patrick Schulz,Marc Rene Zofka,Albert Schotschneider,Helen Gremmelmaier,Nikolai Polley,Ferdinand Mütsch,Kevin Simon,Fabian Gottselig,Michael Frey,Sergio Marschall,Akim Stark,Maximilian Müller,Marek Wehmer,Mihai Kocsis,Dominic Waldenmayer,Florian Schnepf,Erik Heinrich,Sabrina Pletz,Matthias Kölle,Karin Langbein-Euchner,Alexander Viehl,Raoul Zöllner,J. Marius Zöllner*

Main category: cs.RO

TL;DR: 该论文描述了为德国TAF-BW测试区开发数字孪生的数字重建过程，展示了其在交通优化和安全模拟中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着数字化的发展，交通管理需要更高效的解决方案，数字孪生为此提供了实时的虚实交互能力。

Method: 通过智能基础设施和车载LiDAR传感器提取对象列表，生成真实数据输入数字孪生，并设计统一的模拟框架。

Result: 开发了公开可用的模拟框架，并通过案例研究展示了其在交通信号优化和通信安全模拟中的应用。

Conclusion: 数字孪生技术在交通管理中具有实际应用潜力，能够通过虚实结合优化决策和场景模拟。

Abstract: In the future, mobility will be strongly shaped by the increasing use of
digitalization. Not only will individual road users be highly interconnected,
but also the road and associated infrastructure. At that point, a Digital Twin
becomes particularly appealing because, unlike a basic simulation, it offers a
continuous, bilateral connection linking the real and virtual environments.
This paper describes the digital reconstruction used to develop the Digital
Twin of the Test Area Autonomous Driving-Baden-W\"urttemberg (TAF-BW), Germany.
The TAF-BW offers a variety of different road sections, from high-traffic urban
intersections and tunnels to multilane motorways. The test area is equipped
with a comprehensive Vehicle-to-Everything (V2X) communication infrastructure
and multiple intelligent intersections equipped with camera sensors to
facilitate real-time traffic flow monitoring. The generation of authentic data
as input for the Digital Twin was achieved by extracting object lists at the
intersections. This process was facilitated by the combined utilization of
camera images from the intelligent infrastructure and LiDAR sensors mounted on
a test vehicle. Using a unified interface, recordings from real-world
detections of traffic participants can be resimulated. Additionally, the
simulation framework's design and the reconstruction process is discussed. The
resulting framework is made publicly available for download and utilization at:
https://digit4taf-bw.fzi.de The demonstration uses two case studies to
illustrate the application of the digital twin and its interfaces: the analysis
of traffic signal systems to optimize traffic flow and the simulation of
security-related scenarios in the communications sector.

</details>


### [86] [MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints](https://arxiv.org/abs/2507.02438)
*Shivam Chaubey,Francesco Verdoja,Shankar Deka,Ville Kyrki*

Main category: cs.RO

TL;DR: 该论文提出了一种基于约束最优控制问题的辅助控制器框架，结合离线计算的控制不变集，解决了现有共享控制方法在可行性、可扩展性和安全性方面的不足，并通过大规模用户研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 针对现有共享控制方法在处理用户不可预测输入时的可行性、可扩展性和安全性问题，提出了一种新框架。

Method: 基于约束最优控制问题，结合离线计算的控制不变集，在线生成控制动作，确保可行性、严格约束满足和最小化用户意图覆盖。

Result: 通过66名参与者的大规模用户研究，验证了该方法在任务负荷、信任度、感知控制和性能方面的全面提升，且不损害安全性和用户意图。

Conclusion: 提出的框架显著改善了共享控制系统的性能与用户体验，适用于现实世界中复杂的非凸约束场景。

Abstract: Shared control combines human intention with autonomous decision-making, from
low-level safety overrides to high-level task guidance, enabling systems that
adapt to users while ensuring safety and performance. This enhances task
effectiveness and user experience across domains such as assistive robotics,
teleoperation, and autonomous driving. However, existing shared control
methods, based on e.g. Model Predictive Control, Control Barrier Functions, or
learning-based control, struggle with feasibility, scalability, or safety
guarantees, particularly since the user input is unpredictable.
  To address these challenges, we propose an assistive controller framework
based on Constrained Optimal Control Problem that incorporates an
offline-computed Control Invariant Set, enabling online computation of control
actions that ensure feasibility, strict constraint satisfaction, and minimal
override of user intent. Moreover, the framework can accommodate structured
class of non-convex constraints, which are common in real-world scenarios. We
validate the approach through a large-scale user study with 66
participants--one of the most extensive in shared control research--using a
computer game environment to assess task load, trust, and perceived control, in
addition to performance. The results show consistent improvements across all
these aspects without compromising safety and user intent.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [87] [A formal specification of the desired software behaviour of the Princess Marijke lock complex](https://arxiv.org/abs/2507.02721)
*Jan Friso Groote,Matthias Volk*

Main category: eess.SY

TL;DR: 该论文通过少于400行的mCRL2代码，精确描述了荷兰Princess Marijke锁复合体的软件控制，并通过模型检查验证了53个软件需求的正确性，确保其行为描述的准确性。


<details>
  <summary>Details</summary>
Motivation: 确保这一重要水利设施的软件控制安全可靠，以保障防洪和船舶操作的正常运行。

Method: 使用mCRL2形式化描述语言，编写少于400行的代码来定义锁复合体的控制逻辑，并通过模型检查验证其正确性。

Result: 验证了53个软件需求的正确性，证明形式化描述的行为符合预期。

Conclusion: 该形式化描述可作为锁复合体软件构建的蓝图，确保其设计的准确性和可靠性。

Abstract: The Princess Marijke lock complex is a large lock and water-protection
installation in the Netherlands between the river Rhine and the
Amsterdam-Rijnkanaal -- a large waterway connecting the Rhine to the port of
Amsterdam. The lock complex consists of two independent locks and a moveable
flood-protection barrier. Ensuring safe control of the lock complex is of
utmost importance to guarantee both flood-protection and reliable ship
operations. This paper gives a precise, formal description of the software
control of the lock complex in less than 400 lines of mCRL2 code. This
description can act as a blueprint on how the software of this lock complex
needs to be constructed. Moreover, using model checking, 53 software
requirements are shown to be valid, ensuring that the formal description of the
behaviour is correct with regard to these properties and is unlikely to contain
mistakes and oversights.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [88] [Optimising task allocation to balance business goals and worker well-being for financial service workforces](https://arxiv.org/abs/2507.01968)
*Chris Duckworth,Zlatko Zlatev,James Sciberras,Peter Hallett,Enrico Gerding*

Main category: q-fin.GN

TL;DR: 提出了一个结合业务目标和员工福祉的正式任务分配模型，采用遗传算法优化分配和调度任务，提高了效率并考虑了员工福祉。


<details>
  <summary>Details</summary>
Motivation: 金融服务业处理大量数据，错误解决任务对分析师造成压力，导致资源挑战和业务风险增加，需同时兼顾效率和员工福祉。

Method: 使用遗传算法优化任务分配模型，考虑技能、经验和员工福祉目标。

Result: 遗传算法模型优于基准启发式方法和现有实践，适用于单目标和多目标场景，可提供高效分配建议。

Conclusion: 模型填补了现有任务分配中忽视员工福祉的空白，同时提高了效率，为金融服务管理者提供了实用解决方案。

Abstract: Purpose: Financial service companies manage huge volumes of data which
requires timely error identification and resolution. The associated tasks to
resolve these errors frequently put financial analyst workforces under
significant pressure leading to resourcing challenges and increased business
risk. To address this challenge, we introduce a formal task allocation model
which considers both business orientated goals and analyst well-being.
  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to
allocate and schedule tasks to analysts. The proposed solution is able to
allocate tasks to analysts with appropriate skills and experience, while taking
into account staff well-being objectives.
  Findings: We demonstrate our GA model outperforms baseline heuristics,
current working practice, and is applicable to a range of single and
multi-objective real-world scenarios. We discuss the potential for
metaheuristics (such as GAs) to efficiently find sufficiently good allocations
which can provide recommendations for financial service managers in-the-loop.
  Originality: A key gap in existing allocation and scheduling models, is fully
considering worker well-being. This paper presents an allocation model which
explicitly optimises for well-being while still improving on current working
practice for efficiency.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [89] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 论文探讨了标记数据中的标签变异（LV）和人类标签变异（HLV）如何影响监督学习，提出了一种将HLV纳入主动学习（AL）循环的概念框架。


<details>
  <summary>Details</summary>
Motivation: 现有标记数据假设单一真实标签，忽略了HLV作为信息信号的重要性，且主动学习中简化假设在HLV存在时难以成立。

Method: 分析了LV的来源，提出将其分解为信号（HLV）和噪声（如标记错误），并构建了HLV感知的主动学习框架。

Result: 提出了一套整合HLV的主动学习方法，包括实例选择、标注者选择和标签表示，并探讨了大型语言模型（LLM）作为标注者的潜力。

Conclusion: 研究为HLV感知的主动学习奠定了概念基础，更真实地反映了实际标注的复杂性。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>
