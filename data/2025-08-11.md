<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 15]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: 提出了一个基于多准则决策（MCDM）的数据驱动框架PySelect，用于Python软件包的透明选择，解决了生成式AI工具在推荐中的不足。


<details>
  <summary>Details</summary>
Motivation: 开源生态中第三方软件包选择困难，现有工具依赖流行度而非适用性，缺乏透明度和可重现性。

Method: 通过自动化数据管道收集软件元数据，构建决策模型，并开发决策支持系统PySelect。

Result: 评估显示数据提取精度高，推荐质量优于生成式AI基线，用户反馈积极。

Conclusion: 该框架结合MCDM、实证数据和AI意图建模，为软件包选择提供了可扩展、可解释的解决方案。

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [2] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: Klear-CodeTest 是一个高质量的测试用例合成框架，通过 Generator-Validation (G-V) 机制确保测试用例的正确性和覆盖率，并设计了多层安全沙箱系统，显著提升了代码强化学习的模型性能。


<details>
  <summary>Details</summary>
Motivation: 在代码强化学习中，高质量测试用例的生成是一个尚未解决的挑战，亟需一种能够确保正确性和广泛覆盖的方法来提升训练效果。

Method: 提出了 Generator-Validation (G-V) 框架，结合生成器和一致性验证机制，生成包括常规和极端情况的测试用例，并设计多层安全沙箱系统确保可靠执行。

Result: 实验表明，该方法显著提高了模型性能和训练稳定性，并提供了开源的工具和数据集。

Conclusion: Klear-CodeTest 通过创新的 G-V 框架和安全沙箱设计，为解决测试用例生成问题提供了高效可靠的解决方案。

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [3] [Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework](https://arxiv.org/abs/2508.05747)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.SE

TL;DR: 论文探讨如何在大学Laravel课程中利用Composer及其精选包提升开发效率，同时培养职业软件实践，需注意工具依赖风险并强调教学设计的必要性。


<details>
  <summary>Details</summary>
Motivation: 学生在有限时间内完成Laravel项目有困难，需工具和方法提升效率并培养职业习惯。

Method: 介绍Composer及精选包，提供策略和最佳实践，结合教学实践强化代码质量和概念理解。

Result: Composer包能加速开发并增强职业准备，需教学指导以避免工具滥用。

Conclusion: 合理使用Composer包可提升课程相关性和学生职业能力，但需配套教学设计确保深度学习。

Abstract: Laravel has emerged as a foundational framework in university web development
curricula. However, despite its scaffolding capabilities, students often
struggle to complete projects within limited academic timelines. This
conceptual paper introduces Composer, PHP's standard dependency manager, and
categorizes a curated selection of Composer packages that significantly reduce
development effort while fostering professional software practices. Grounded in
practical and pedagogical considerations, the paper illustrates how educators
and learners can strategically leverage these tools to build typical academic
or personal Laravel-based systems. Central to this approach is maintaining code
quality and reinforcing conceptual understanding. The paper also addresses
potential risks such as package conflicts and over-reliance on tools, providing
best-practice recommendations to mitigate them. While the goal is to accelerate
development, the deeper objective is to reinforce professional workflows and
industry readiness. Exposure to Composer packages enhances curriculum relevance
and smooths the transition from academia to the workplace. However, effective
integration requires deliberate instructional design aligned with learning
objectives. Without guidance, students may treat packages as black boxes. Thus,
educators must teach not only how to use these tools, but also when and why,
encouraging critical evaluation of their utility and limitations. This ensures
that practical convenience supports rather than supplants deep learning.

</details>


### [4] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: 本文提出了一种将确定性逆向工程与基于LLM的意图感知可视化相结合的混合方法，以提升代码理解效果。


<details>
  <summary>Details</summary>
Motivation: 开发者在理解大型复杂软件系统时面临挑战，传统工具缺乏交互性和适应性，LLMs虽先进但缺乏基础性和结构性视图的整合。

Method: 结合UML可视化、动态用户界面、历史上下文和协作功能，通过LLM解析用户查询和交互模式。

Result: Java原型展示了该方法的可行性。

Conclusion: 为构建智能、交互式的开发环境奠定了基础，未来将进行实证评估、扩展至多语言系统及探索GUI驱动的LLM交互模型。

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


### [5] [Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm](https://arxiv.org/abs/2508.05923)
*Yanusha Mehendran,Maolin Tang,Yi Lu*

Main category: cs.SE

TL;DR: 论文提出了一种基于遗传算法的测试输入生成方法，融合遗传操作和自适应学习，显著提高了软件漏洞检测的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测方法难以应对日益复杂的软件系统需求，需要更高效、自适应的方法来提升检测能力。

Method: 结合遗传算法的交叉操作和自适应反馈机制，动态生成结构有效的测试输入，优化代码覆盖率。

Result: 在多个开源JSON处理库中测试，覆盖率显著提升，最高达166%。

Conclusion: 该方法为软件安全测试提供了可扩展且高效的解决方案，能够检测更深层次和复杂的漏洞。

Abstract: Software vulnerabilities continue to undermine the reliability and security
of modern systems, particularly as software complexity outpaces the
capabilities of traditional detection methods. This study introduces a genetic
algorithm-based method for test input generation that innovatively integrates
genetic operators and adaptive learning to enhance software vulnerability
detection. A key contribution is the application of the crossover operator,
which facilitates exploration by searching across a broader space of potential
test inputs. Complementing this, an adaptive feedback mechanism continuously
learns from the system's execution behavior and dynamically guides input
generation toward promising areas of the input space. Rather than relying on
fixed or randomly selected inputs, the approach evolves a population of
structurally valid test cases using feedback-driven selection, enabling deeper
and more effective code traversal. This strategic integration of exploration
and exploitation ensures that both diverse and targeted test inputs are
developed over time. Evaluation was conducted across nine open-source
JSON-processing libraries. The proposed method achieved substantial
improvements in coverage compared to a benchmark evolutionary fuzzing method,
with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%
in line coverage, 114.0% in instruction coverage, and 166.0% in branch
coverage. These results highlight the method's capacity to detect deeper and
more complex vulnerabilities, offering a scalable and adaptive solution to
software security testing.

</details>


### [6] [A Survey on Task Scheduling in Carbon-Aware Container Orchestration](https://arxiv.org/abs/2508.05949)
*Jialin Yang,Zainab Saad,Jiajun Wu,Xiaoguang Niu,Henry Leung,Steve Drew*

Main category: cs.SE

TL;DR: 本文综述了Kubernetes调度策略及其对云计算的可持续性影响，提出了分类和挑战。


<details>
  <summary>Details</summary>
Motivation: 由于云计算和大规模语言模型训练导致的高能耗问题，需优化调度以减少碳排放。

Method: 系统回顾Kubernetes调度策略，分为硬件和软件为中心，分类并分析可持续性目标。

Result: 提出了用于云任务调度的分类法，识别了研究趋势与挑战。

Conclusion: 为下一代可持续云计算系统提供了设计思路和关键见解。

Abstract: The soaring energy demands of large-scale software ecosystems and cloud data
centers, accelerated by the intensive training and deployment of large language
models, have driven energy consumption and carbon footprint to unprecedented
levels. In response, both industry and academia are increasing efforts to
reduce the carbon emissions associated with cloud computing through more
efficient task scheduling and infrastructure orchestration. In this work, we
present a systematic review of various Kubernetes scheduling strategies,
categorizing them into hardware-centric and software-centric, annotating each
with its sustainability objectives, and grouping them according to the
algorithms they use. We propose a comprehensive taxonomy for cloud task
scheduling studies, with a particular focus on the environmental sustainability
aspect. We analyze emerging research trends and open challenges, and our
findings provide critical insight into the design of sustainable scheduling
solutions for next-generation cloud computing systems.

</details>


### [7] [Impact-driven Context Filtering For Cross-file Code Completion](https://arxiv.org/abs/2508.05970)
*Yanzhou Li,Shangqing Liu,Kangjie Chen,Tianwei Zhang,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出了一种名为CODEFILTER的自适应检索上下文过滤框架，用于提升存储库级代码补全的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究RAG在代码补全中检索的跨文件上下文的贡献，发现许多检索块对补全无益甚至有害。

Method: 引入基于似然的度量评估检索块的影响，构建标记数据集，并训练CODEFILTER框架过滤有害上下文。

Result: CODEFILTER在多个基准测试中提升了补全准确性，同时减少输入提示长度，提高了计算效率。

Conclusion: CODEFILTER展示了在存储库级代码补全中提升准确性、效率和可解释性的潜力。

Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable
potential for repository-level code completion, as it integrates cross-file
knowledge with in-file preceding code to provide comprehensive contexts for
generation. To better understand the contribution of the retrieved cross-file
contexts, we introduce a likelihood-based metric to evaluate the impact of each
retrieved code chunk on the completion. Our analysis reveals that, despite
retrieving numerous chunks, only a small subset positively contributes to the
completion, while some chunks even degrade performance. To address this issue,
we leverage this metric to construct a repository-level dataset where each
retrieved chunk is labeled as positive, neutral, or negative based on its
relevance to the target completion. We then propose an adaptive retrieval
context filtering framework, CODEFILTER, trained on this dataset to mitigate
the harmful effects of negative retrieved contexts in code completion.
Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks
demonstrates that CODEFILTER consistently improves completion accuracy compared
to approaches without filtering operations across various tasks. Additionally,
CODEFILTER significantly reduces the length of the input prompt, enhancing
computational efficiency while exhibiting strong generalizability across
different models. These results underscore the potential of CODEFILTER to
enhance the accuracy, efficiency, and attributability of repository-level code
completion.

</details>


### [8] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 论文探讨了如何通过清晰的解释提升AI编码系统的可信度和可用性，提出了认知对齐和语义忠实性两个关键解释属性，并建议采用神经符号方法生成解释。


<details>
  <summary>Details</summary>
Motivation: AI驱动的编码系统决策不透明，可能引发信任和可用性问题，尤其是对非专家用户。

Method: 提出认知对齐和语义忠实性作为关键解释属性，并建议采用神经符号方法，结合符号约束和神经网络表示，生成解释。

Result: 强调了现有方法的局限性，如形式验证和静态分析，并提出了新的研究方向。

Conclusion: 神经符号方法有望为AI编码系统提供更透明的解释，增强用户信任。

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [9] [Understanding Inconsistent State Update Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2508.06192)
*Lantian Li,Yuyu Chen,Jingwen Wu,Yue Pan,Zhongxing Yu*

Main category: cs.SE

TL;DR: 本文首次对智能合约中不一致状态更新漏洞进行了大规模实证研究，揭示了其根源、修复策略和利用方法，并提出了11个重要发现，同时开发了一个概念验证检测工具。


<details>
  <summary>Details</summary>
Motivation: 智能合约的状态更新问题可能导致安全漏洞，现有工具难以有效检测，因此需要深入研究以提供解决方案。

Method: 系统调查了352个真实智能合约项目中的116个不一致状态更新漏洞，分析其根因、修复策略和利用方法。

Result: 提出了11个重要发现，开发的概念验证检测工具在64个GitHub项目中有效识别问题，19个项目所有者确认了问题。

Conclusion: 研究结果对开发者、研究人员和工具设计者具有重要指导意义，有助于避免智能合约中的不一致状态更新漏洞。

Abstract: Smart contracts enable contract terms to be automatically executed and
verified on the blockchain, and recent years have witnessed numerous
applications of them in areas such as financial institutions and supply chains.
The execution logic of a smart contract is closely related to the contract
state, and thus the correct and safe execution of the contract depends heavily
on the precise control and update of the contract state. However, the contract
state update process can have issues. In particular, inconsistent state update
issues can arise for reasons such as unsynchronized modifications. Inconsistent
state update bugs have been exploited by attackers many times, but existing
detection tools still have difficulty in effectively identifying them. This
paper conducts the first large-scale empirical study about inconsistent state
update vulnerabilities (that is, inconsistent state update bugs that are
exploitable) in smart contracts, aiming to shed light for developers,
researchers, tool builders, and language or library designers in order to avoid
inconsistent state update vulnerabilities. We systematically investigate 116
inconsistent state update vulnerabilities in 352 real-world smart contract
projects, summarizing their root causes, fix strategies, and exploitation
methods. Our study provides 11 original and important findings, and we also
give the implications of our findings. To illustrate the potential benefits of
our research, we also develop a proof-of-concept checker based on one of our
findings. The checker effectively detects issues in 64 popular GitHub projects,
and 19 project owners have confirmed the detected issues at the time of
writing. The result demonstrates the usefulness and importance of our findings
for avoiding inconsistent state update vulnerabilities in smart contracts.

</details>


### [10] [Improving the Developer Experience with a Low-Code Process Modelling Language](https://arxiv.org/abs/2508.06299)
*Henrique Henriques,Hugo Lourenço,Vasco Amaral,Miguel Goulão*

Main category: cs.SE

TL;DR: 研究改进了OutSystems平台的业务流程建模DSL（BPT），通过访谈、符号物理评审和实证评估显著提升了语义透明度、正确性和可用性。


<details>
  <summary>Details</summary>
Motivation: BPT的采用率低且存在可用性问题，增加了维护成本，因此需要改进。

Method: 结合访谈、符号物理评审及SUS和TLX评估，开发新版本BPT。

Result: 新版本BPT的语义透明度从31%提升至69%，正确率从51%升至89%，SUS分数从42.25增至64.78，TLX分数从36.50降至20.78。

Conclusion: 新版本BPT显著改善了开发者体验，用户背景对最终语法选择和可用性指标有重要影响。

Abstract: Context: The OutSystems Platform is a development environment composed of
several DSLs, used to specify, quickly build, and validate web and mobile
applications. The DSLs allow users to model different perspectives such as
interfaces and data models, define custom business logic and construct process
models. Problem: The DSL for process modelling (Business Process Technology
(BPT)), has a low adoption rate and is perceived as having usability problems
hampering its adoption. This is problematic given the language maintenance
costs. Method: We used a combination of interviews, a critical review of BPT
using the "Physics of Notation" and empirical evaluations of BPT using the
System Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a
new version of BPT, taking these inputs and Outsystems' engineers' culture into
account. Results: Evaluations conducted with 25 professional software engineers
showed an increase of the semantic transparency on the new version, from 31% to
69%, an increase in the correctness of responses, from 51% to 89%, an increase
in the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from
36.50 to 20.78. These differences were statistically significant. Conclusions:
These results suggest that the new version of BPT significantly improved the
developer experience of the previous version. The end users' background with
OutSystems had a relevant impact on the final concrete syntax choices and
achieved usability indicators.

</details>


### [11] [Execution-Feedback Driven Test Generation from SWE Issues](https://arxiv.org/abs/2508.06365)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 本文探讨了如何自动生成软件工程问题的重现测试，提出了一种名为e-Otter++的新技术，解决了代码缺失或错误的问题，并在实验中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程问题时，重现测试非常重要，但大多数问题缺乏有效的测试，本文旨在自动生成这些测试。

Method: 提出了e-Otter++工具，利用执行反馈技术绕过代码缺失或错误的问题。

Result: 在TDD-Bench Verified基准测试中，e-Otter++的平均fail-to-pass率达到63%。

Conclusion: e-Otter++在自动生成重现测试方面取得了显著进展，为软件工程问题的解决提供了新方法。

Abstract: A software engineering issue (SWE issue) is easier to resolve when
accompanied by a reproduction test. Unfortunately, most issues do not come with
functioning reproduction tests, so this paper explores how to generate them
automatically. The primary challenge in this setting is that the code to be
tested is either missing or wrong, as evidenced by the existence of the issue
in the first place. This has held back test generation for this setting:
without the correct code to execute, it is difficult to leverage execution
feedback to generate good tests. This paper introduces novel techniques for
leveraging execution feedback to get around this problem, implemented in a new
reproduction test generator called e-Otter++. Experiments show that e-Otter++
represents a leap ahead in the state-of-the-art for this problem, generating
tests with an average fail-to-pass rate of 63% on the TDD-Bench Verified
benchmark.

</details>


### [12] [What Builds Effective In-Context Examples for Code Generation?](https://arxiv.org/abs/2508.06414)
*Dongze Li,Songqiang Chen,Jialun Cao,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 研究探讨了代码示例中特定特征对ICL在代码生成中的影响，发现变量和函数命名是关键因素，并提出LLMs在反思和推理能力上的局限性。


<details>
  <summary>Details</summary>
Motivation: 探索代码示例中哪些具体特征（如命名风格、格式等）对ICL在代码生成中的有效性起决定作用。

Method: 通过控制变量法进行系统性消融研究。

Result: 变量和函数命名对代码生成效果至关重要，删除会导致性能下降高达30%；LLMs更注重语义而非格式。

Conclusion: 优化ICL系统需关注命名，同时当前LLMs在从代码中提取通用问题解决见解方面存在挑战。

Abstract: In-Context Learning (ICL) has emerged as a promising solution to enhance the
code generation capabilities of Large Language Models (LLMs), which
incorporates code examples inside the prompt to let LLMs learn from
demonstrations. However, despite the substantial effectiveness of the code
example-based ICL approach, the specific features (e.g., identifier naming
styles, code formatting, solution insight) within the ICL-provided code
examples that significantly contribute to the ICL's effectiveness remain
unclear. This paper systematically investigates the impact of various code
features on ICL with code examples through controlled ablation studies. Our
findings reveal that the appropriate naming of variables and functions is
crucial for effective code generation, with their elimination leading to
performance decreases of up to 30 percentage points. We further demonstrate
that LLMs prioritize semantically meaningful identifier names over formatting
conventions, with language-specific preferences regarding identifier verbosity.
Additionally, our investigation into ICL's potential for enhancing reflection
and inference capabilities reveals that current LLMs struggle to extract
generalizable problem-solving insights from similar code solutions, despite
being capable of utilizing direct information effectively. These findings are
expected to provide valuable insights for optimizing ICL systems in code
generation applications and highlight fundamental challenges in
reflection-based learning for code generation tasks.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [13] [Hybrid Game Control Envelope Synthesis](https://arxiv.org/abs/2508.05997)
*Aditi Kabra,Jonathan Laurent,Stefan Mitsch,André Platzer*

Main category: cs.PL

TL;DR: 本文研究了嵌入式系统控制问题的解决方案，通过合成尽可能宽松的非确定性获胜策略，提出了子价值映射的概念，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决嵌入式系统（如汽车和火车）的复杂控制问题，需要设计灵活且安全的控制策略，以应对多样化的控制挑战。

Method: 提出子价值映射作为非确定性策略的组成表示，并通过微分博弈逻辑（dGL）进行验证，确保其始终能诱导获胜行为。

Result: 证明了最大子价值映射的存在性及其逻辑特性，并通过实现验证了方法的有效性。

Conclusion: 该方法为嵌入式系统控制提供了灵活且可靠的合成策略，具有广泛的应用潜力。

Abstract: Control problems for embedded systems like cars and trains can be modeled by
two-player hybrid games. Control envelopes, which are families of safe control
solutions, correspond to nondeterministic winning policies of hybrid games,
where each deterministic specialization of the policy is a control solution.
This paper synthesizes nondeterministic winning policies for hybrid games that
are as permissive as possible. It introduces subvalue maps, a compositional
representation of such policies that enables verification and synthesis along
the structure of the game. An inductive logical characterization in
differential game logic (dGL) checks whether a subvalue map induces a sound
control envelope which always induces a winning play. A policy is said to win
if it always achieves the desirable outcome when the player follows it, no
matter what actions the opponent plays. The maximal subvalue map, which allows
the most action options while still winning, is shown to exist and satisfy a
logical characterization. A family of algorithms for nondeterministic policy
synthesis can be obtained from the inductive subvalue map soundness
characterization. An implementation of these findings is evaluated on examples
that use the expressivity of dGL to model a range of diverse control
challenges.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [14] [Hierarchical Placement Learning for Network Slice Provisioning](https://arxiv.org/abs/2508.06432)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun*

Main category: cs.NI

TL;DR: 提出了一个基于层次多臂老虎机的解决方案，用于边缘移动网络中的切片配置，以提高请求接受率并降低资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘移动网络中切片配置的挑战，优化请求接受率和资源利用率。

Method: 采用层次多臂老虎机框架，学习可扩展的切片配置策略。

Result: 仿真实验表明，相比基准方法，该解决方案在特定场景下实现了5%的平均节点资源利用率，并提高了25%以上的切片请求接受率。

Conclusion: 提出的方法在提高切片请求接受率和降低资源利用率方面表现出显著优势。

Abstract: In this work, we aim to address the challenge of slice provisioning in
edge-based mobile networks. We propose a solution that learns a service
function chain placement policy for Network Slice Requests, to maximize the
request acceptance rate, while minimizing the average node resource
utilization. To do this, we consider a Hierarchical Multi-Armed Bandit problem
and propose a two-level hierarchical bandit solution which aims to learn a
scalable placement policy that optimizes the stated objectives in an online
manner. Simulations on two real network topologies show that our proposed
approach achieves 5% average node resource utilization while admitting over 25%
more slice requests in certain scenarios, compared to baseline methods.

</details>


### [15] [An Online Multi-dimensional Knapsack Approach for Slice Admission Control](https://arxiv.org/abs/2508.06468)
*Jesutofunmi Ajayi,Antonio Di Maio,Torsten Braun,Dimitrios Xenakis*

Main category: cs.NI

TL;DR: 切片网络中的在线切片请求准入控制，通过多维背包问题和预留策略最大化长期收入，相比先到先服务策略提升收入并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决切片网络中资源需求不确定性问题，旨在为基础设施提供商最大化长期收入。

Method: 建模为在线多维背包问题，提出两种预留策略算法。

Result: 模拟结果显示收入提升12.9%，资源消耗降低1.7%，经济不平等时收益更高。

Conclusion: 预留策略优于先到先服务，为基础设施提供商带来更高收益和资源效率。

Abstract: Network Slicing has emerged as a powerful technique to enable cost-effective,
multi-tenant communications and services over a shared physical mobile network
infrastructure. One major challenge of service provisioning in slice-enabled
networks is the uncertainty in the demand for the limited network resources
that must be shared among existing slices and potentially new Network Slice
Requests. In this paper, we consider admission control of Network Slice
Requests in an online setting, with the goal of maximizing the long-term
revenue received from admitted requests. We model the Slice Admission Control
problem as an Online Multidimensional Knapsack Problem and present two
reservation-based policies and their algorithms, which have a competitive
performance for Online Multidimensional Knapsack Problems. Through Monte Carlo
simulations, we evaluate the performance of our online admission control method
in terms of average revenue gained by the Infrastructure Provider, system
resource utilization, and the ratio of accepted slice requests. We compare our
approach with those of the online First Come First Serve greedy policy. The
simulation's results prove that our proposed online policies increase revenues
for Infrastructure Providers by up to 12.9 % while reducing the average
resource consumption by up to 1.7% In particular, when the tenants' economic
inequality increases, an Infrastructure Provider who adopts our proposed online
admission policies gains higher revenues compared to an Infrastructure Provider
who adopts First Come First Serve.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [16] [Basic interactive algorithms: Preview](https://arxiv.org/abs/2508.05798)
*Yuri Gurevich*

Main category: cs.LO

TL;DR: 本文预览了即将发表的工作，探讨基本交互式算法的公理化。从经典算法扩展到概率、量子等算法，区分了两种Church-Turing命题。


<details>
  <summary>Details</summary>
Motivation: 现代算法概念在1930-50年代明确，25年前被公理化，但如今扩展了概率、量子等算法，促使重新审视Church-Turing命题。

Method: 通过公理化基本算法，证明其行为等价于抽象状态机，并将非确定性、概率算法视为带适当预言的基本算法。

Result: 证明经典算法与抽象状态机行为等价，展示了概率、量子算法等可作为带预言的基本算法。

Conclusion: 扩展算法概念，强调两种Church-Turing命题的区别，为更广泛算法类提供公理化框架。

Abstract: This dialog paper offers a preview and provides a foretaste of an upcoming
work on the axiomatization of basic interactive algorithms.
  The modern notion of algorithm was elucidated in the 1930s--1950s. It was
axiomatized a quarter of a century ago as the notion of ``sequential
algorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm"
now. The axiomatization was used to show that for every basic algorithm there
is a behaviorally equivalent abstract state machine. It was also used to prove
the Church-Turing thesis as it has been understood by the logicians.
  Starting from the 1960s, the notion of algorithm has expanded --
probabilistic algorithms, quantum algorithms, etc. -- prompting introduction of
a much more ambitious version of the Church-Turing thesis commonly known as the
``physical thesis.'' We emphasize the difference between the two versions of
the Church-Turing thesis and illustrate how nondeterministic and probabilistic
algorithms can be viewed as basic algorithms with appropriate oracles. The same
view applies to quantum circuit algorithms and many other classes of
algorithms.

</details>


### [17] [Widest Path Games and Maximality Inheritance in Bounded Value Iteration for Stochastic Games](https://arxiv.org/abs/2508.06088)
*Kittiphon Phalakarn,Yun Chen Tsai,Ichiro Hasuo*

Main category: cs.LO

TL;DR: 论文提出了一种新的基于双人最宽路径游戏的BVI算法2WP-BVI，避免了传统方法中检测和终结组件的昂贵计算，并通过理论证明和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统BVI算法在存在终结组件时可能无法终止或收敛，现有方法处理终结组件计算成本高，1WP-BVI虽高效但理论基础不足。

Method: 提出2WP-BVI算法，基于双人最宽路径游戏，利用最大性继承原理证明其正确性。

Result: 实验证明2WP-BVI具有实际应用潜力和高效性。

Conclusion: 2WP-BVI解决了传统BVI算法的局限，为随机游戏模型检查提供了更高效且理论基础坚实的解决方案。

Abstract: For model checking stochastic games (SGs), bounded value iteration (BVI)
algorithms have gained attention as efficient approximate methods with rigorous
precision guarantees. However, BVI may not terminate or converge when the
target SG contains end components. Most existing approaches address this issue
by explicitly detecting and processing end components--a process that is often
computationally expensive. An exception is the widest path-based BVI approach
previously studied by Phalakarn et al., which we refer to as 1WP-BVI. The
method performs particularly well in the presence of numerous end components.
Nonetheless, its theoretical foundations remain somewhat ad hoc. In this paper,
we identify and formalize the core principles underlying the widest path-based
BVI approach by (i) presenting 2WP-BVI, a clean BVI algorithm based on
(2-player) widest path games, and (ii) proving its correctness using what we
call the maximality inheritance principle--a proof principle previously
employed in a well-known result in probabilistic model checking. Our
experimental results demonstrate the practical relevance and potential of our
proposed 2WP-BVI algorithm.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [18] [Automated Visualization Makeovers with LLMs](https://arxiv.org/abs/2508.05637)
*Siddharth Gangwar,David A. Selby,Sebastian J. Vollmer*

Main category: cs.HC

TL;DR: 论文探讨了如何利用多模态大语言模型（LLM）半自动生成改进数据可视化的建议，通过提示工程结合可视化最佳实践，帮助用户优化现有图表。


<details>
  <summary>Details</summary>
Motivation: 现有的数据科学课程通常未教授如何制作高效传达信息的图表。本文旨在利用LLM模拟可视化改造任务，提供改进建议。

Method: 使用预训练的LLM，结合用户指定的指南和模型潜在的可视化知识，通过提示工程生成改进建议。

Result: 定量评估表明，LLM代理对不同图表类型的绘图问题具有敏感性。工具以简单的自托管小程序形式提供，具有可访问的Web界面。

Conclusion: LLM可作为教育工具，帮助用户根据最佳实践改进现有数据可视化。

Abstract: Making a good graphic that accurately and efficiently conveys the desired
message to the audience is both an art and a science, typically not taught in
the data science curriculum. Visualisation makeovers are exercises where the
community exchange feedback to improve charts and data visualizations. Can
multi-modal large language models (LLMs) emulate this task? Given a plot in the
form of an image file, or the code used to generate it, an LLM, primed with a
list of visualization best practices, is employed to semi-automatically
generate constructive criticism to produce a better plot. Our system is centred
around prompt engineering of a pre-trained model, relying on a combination of
userspecified guidelines and any latent knowledge of data visualization
practices that might lie within an LLMs training corpus. Unlike other works,
the focus is not on generating valid visualization scripts from raw data or
prompts, but on educating the user how to improve their existing data
visualizations according to an interpretation of best practices. A quantitative
evaluation is performed to measure the sensitivity of the LLM agent to various
plotting issues across different chart types. We make the tool available as a
simple self-hosted applet with an accessible Web interface.

</details>


### [19] [A Humanoid Social Robot as a Teaching Assistant in the Classroom](https://arxiv.org/abs/2508.05646)
*Thomas Sievers*

Main category: cs.HC

TL;DR: 社交机器人Pepper结合ChatGPT在高中课堂教授新内容，学生对其接受度和实用性持积极态度。


<details>
  <summary>Details</summary>
Motivation: 通过社交机器人辅助教学，减轻教育系统负担，增加现代多模态学习环境的社会互动性。

Method: 使用Pepper机器人连接ChatGPT，在高中课堂上教授新内容，并调查学生的接受度和实用性反馈。

Result: 所有参与者认为机器人展示学习内容合适或部分合适，且其使用有意义。

Conclusion: 社交机器人在教育中具有潜力，能为学生提供新的学习体验。

Abstract: Although innovation and the support of new technologies are much needed to
ease the burden on the education system, social robots in schools to help
teachers with educational tasks are rare. Child-Robot Interaction (CRI) could
support teachers and add an embodied social component to modern multi-modal and
multi-sensory learning environments already in use. The social robot Pepper,
connected to the Large Language Model (LLM) ChatGPT, was used in a high school
classroom to teach new learning content to groups of students. I tested the
technical possibilities with the robot on site and asked the students about
their acceptance and perceived usefulness of teaching with the help of a social
robot. All participants felt that the robot's presentation of the learning
material was appropriate or at least partially appropriate and that its use
made sense.

</details>


### [20] [Modeling Interactive Narrative Systems: A Formal Approach](https://arxiv.org/abs/2508.05653)
*Jules Clerc,Domitile Lourdeaux,Mohamed Sallak,Johann Barbier,Marc Ravaine*

Main category: cs.HC

TL;DR: 本文提出了一个交互式叙事系统（INS）的形式化表示框架，旨在统一研究领域并促进系统分析、描述和比较。


<details>
  <summary>Details</summary>
Motivation: 交互式叙事系统在数字体验中具有革命性意义，但研究分散且系统表示多样，需要一个统一框架来促进分析和比较。

Method: 论文从现有方法中汲取灵感，构建了一个形式化表示框架，并通过'小红帽'场景进行了实验验证。

Result: 实验表明该形式化框架有助于提升INS的评估效果，并促进了研究社区的协作一致性。

Conclusion: 该框架为INS研究提供了统一的词汇和建模结构，有望推动该领域的协作与进步。

Abstract: Interactive Narrative Systems (INS) have revolutionized digital experiences
by empowering users to actively shape their stories, diverging from traditional
passive storytelling. However, the field faces challenges due to fragmented
research efforts and diverse system representations. This paper introduces a
formal representation framework for INS, inspired by diverse approaches from
the state of the art. By providing a consistent vocabulary and modeling
structure, the framework facilitates the analysis, the description and
comparison of INS properties. Experimental validations on the "Little Red
Riding Hood" scenario highlight the usefulness of the proposed formalism and
its impact on improving the evaluation of INS. This work aims to foster
collaboration and coherence within the INS research community by proposing a
methodology for formally representing these systems.

</details>


### [21] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
*Stefan Pasch,Min Chul Cha*

Main category: cs.HC

TL;DR: 研究分析10万条AI产品用户评论，发现伦理AI与用户满意度正相关，但关系因用户和产品类型而异。


<details>
  <summary>Details</summary>
Motivation: 探讨用户是否认可、重视伦理AI原则及其对满意度的影响。

Method: 使用基于Transformer的语言模型分析G2平台上的用户评论，覆盖欧盟伦理指南的七个维度。

Result: 七维度均与满意度正相关，技术用户关注系统层面，非技术用户更注重人本维度；后者的关联更强。

Conclusion: 伦理AI设计需考虑用户角色和产品类型差异。

Abstract: As AI systems become increasingly embedded in organizational workflows and
consumer applications, ethical principles such as fairness, transparency, and
robustness have been widely endorsed in policy and industry guidelines.
However, there is still scarce empirical evidence on whether these principles
are recognized, valued, or impactful from the perspective of users. This study
investigates the link between ethical AI and user satisfaction by analyzing
over 100,000 user reviews of AI products from G2. Using transformer-based
language models, we measure sentiment across seven ethical dimensions defined
by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all
seven dimensions are positively associated with user satisfaction. Yet, this
relationship varies systematically across user and product types. Technical
users and reviewers of AI development platforms more frequently discuss
system-level concerns (e.g., transparency, data governance), while
non-technical users and reviewers of end-user applications emphasize
human-centric dimensions (e.g., human agency, societal well-being). Moreover,
the association between ethical AI and user satisfaction is significantly
stronger for non-technical users and end-user applications across all
dimensions. Our results highlight the importance of ethical AI design from
users' perspectives and underscore the need to account for contextual
differences across user roles and product types.

</details>


### [22] [REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition](https://arxiv.org/abs/2508.05933)
*Xueyuan Xu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 论文提出了一种新的脑电图特征选择方法，用于解决多维情感识别中标签缺失的问题，通过自适应正交非负矩阵分解等方法提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多维情感识别中的高维度脑电图特征和标签缺失问题影响分类器性能和实时性。

Method: 采用自适应正交非负矩阵分解重建标签空间，结合最小二乘回归和图流形学习正则化进行特征选择。

Result: 在三个数据集上验证，该方法优于13种先进特征选择方法。

Conclusion: 该方法有效解决了标签缺失问题，提升了多维情感识别的鲁棒性。

Abstract: The affective brain-computer interface is a crucial technology for affective
interaction and emotional intelligence, emerging as a significant area of
research in the human-computer interaction. Compared to single-type features,
multi-type EEG features provide a multi-level representation for analyzing
multi-dimensional emotions. However, the high dimensionality of multi-type EEG
features, combined with the relatively small number of high-quality EEG
samples, poses challenges such as classifier overfitting and suboptimal
real-time performance in multi-dimensional emotion recognition. Moreover,
practical applications of affective brain-computer interface frequently
encounters partial absence of multi-dimensional emotional labels due to the
open nature of the acquisition environment, and ambiguity and variability in
individual emotion perception. To address these challenges, this study proposes
a novel EEG feature selection method for missing multi-dimensional emotion
recognition. The method leverages adaptive orthogonal non-negative matrix
factorization to reconstruct the multi-dimensional emotional label space
through second-order and higher-order correlations, which could reduce the
negative impact of missing values and outliers on label reconstruction.
Simultaneously, it employs least squares regression with graph-based manifold
learning regularization and global feature redundancy minimization
regularization to enable EEG feature subset selection despite missing
information, ultimately achieving robust EEG-based multi-dimensional emotion
recognition. Simulation experiments on three widely used multi-dimensional
emotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method
outperforms thirteen advanced feature selection methods in terms of robustness
for EEG emotional feature selection.

</details>


### [23] [ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection](https://arxiv.org/abs/2508.05934)
*Xueyuan Xu,Tianze Yu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: 该论文提出了一种名为ASLSL的新方法，用于解决不完备多模态生理信号特征选择问题，通过学习共享潜在结构来减少缺失信息的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态生理信号情感识别在脑机接口领域备受关注，但高维特征常包含无关、冗余和噪声信息，且实际数据往往不完备。

Method: 提出自适应共享潜在结构学习（ASLSL）方法，通过探索共享潜在空间来挖掘共识信息。

Result: 在DEAP和DREAMER数据集上与17种特征选择方法对比，ASLSL表现出优越性能。

Conclusion: ASLSL有效解决了不完备多模态生理信号的特征选择问题。

Abstract: Recently, multi-modal physiological signals based emotion recognition has
garnered increasing attention in the field of brain-computer interfaces.
Nevertheness, the associated multi-modal physiological features are often
high-dimensional and inevitably include irrelevant, redundant, and noisy
representation, which can easily lead to overfitting, poor performance, and
high computational complexity in emotion classifiers. Feature selection has
been widely applied to address these challenges. However, previous studies
generally assumed that multi-modal physiological data are complete, whereas in
reality, the data are often incomplete due to the openness of the acquisition
and operational environment. For example, a part of samples are available in
several modalities but not in others. To address this issue, we propose a novel
method for incomplete multi-modal physiological signal feature selection called
adaptive shared latent structure learning (ASLSL). Based on the property that
similar features share similar emotional labels, ASLSL employs adaptive shared
latent structure learning to explore a common latent space shared for
incomplete multi-modal physiological signals and multi-dimensional emotional
labels, thereby mitigating the impact of missing information and mining
consensus information. Two most popular multi-modal physiological emotion
datasets (DEAP and DREAMER) with multi-dimensional emotional labels were
utilized to compare the performance between compare ASLSL and seventeen feature
selection methods. Comprehensive experimental results on these datasets
demonstrate the effectiveness of ASLSL.

</details>


### [24] [It's a Complete Haystack: Understanding Dependency Management Needs in Computer-Aided Design](https://arxiv.org/abs/2508.05940)
*Kathy Cheng,Alison Olechowski,Shurui Zhou*

Main category: cs.HC

TL;DR: 硬件开发团队面临提高产品质量、创新和缩短制造周期的需求，但缺乏对设计变更和协作行为的管理意识。研究通过分析论坛讨论和设计师访谈，提出改善3D CAD依赖管理的设计目标和功能。


<details>
  <summary>Details</summary>
Motivation: 硬件设计师在管理3D CAD模型依赖关系时面临困难，影响了团队协作效率，亟需解决这一问题。

Method: 通过主题分析100个在线论坛讨论和10位设计师的半结构化访谈，研究了CAD依赖管理的痛点。

Result: 研究确定了9个与依赖关系的可追溯性、导航和一致性相关的挑战，并提出了改进协作流程的设计目标。

Conclusion: 研究为提升硬件设计师对依赖关系的认识和管理提供了解决方案，旨在优化协作工作流程。

Abstract: In today's landscape, hardware development teams face increasing demands for
better quality products, greater innovation, and shorter manufacturing lead
times. Despite the need for more efficient and effective processes, hardware
designers continue to struggle with a lack of awareness of design changes and
other collaborators' actions, a persistent issue in decades of CSCW research.
One significant and unaddressed challenge is understanding and managing
dependencies between 3D CAD (computer-aided design) models, especially when
products can contain thousands of interconnected components. In this two-phase
formative study, we explore designers' pain points of CAD dependency management
through a thematic analysis of 100 online forum discussions and semi-structured
interviews with 10 designers. We identify nine key challenges related to the
traceability, navigation, and consistency of CAD dependencies, that harm the
effective coordination of hardware development teams. To address these
challenges, we propose design goals and necessary features to enhance hardware
designers' awareness and management of dependencies, ultimately with the goal
of improving collaborative workflows.

</details>


### [25] [Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning](https://arxiv.org/abs/2508.06000)
*Wei Xiang,Ziyue Lei,Haoyuan Che,Fangyuan Ye,Xueting Wu,Lingyun Sun*

Main category: cs.HC

TL;DR: 本文探讨了通过LLM驱动的动觉辅助在操作技能学习中的应用，开发了整合LLM与EMS的工具FlightAxis，显著提高了用户接受度和任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM培训助手主要提供文本反馈，忽略了关键的动觉模态，本文旨在填补这一空白，探索LLM驱动的动觉辅助对操作技能学习的潜力。

Method: 引入“对齐-分析-调整”策略，开发了FlightAxis工具，结合LLM与EMS技术，用于飞行技能训练。

Result: 用户对LLM驱动的身体控制接受度高，任务完成时间显著减少，且动觉辅助增强了操作缺陷的意识和训练参与度。

Conclusion: 本研究展示了动觉LLM培训在操作技能获取中的潜力。

Abstract: Operational skill learning, inherently physical and reliant on hands-on
practice and kinesthetic feedback, has yet to be effectively replicated in
large language model (LLM)-supported training. Current LLM training assistants
primarily generate customized textual feedback, neglecting the crucial
kinesthetic modality. This gap derives from the textual and uncertain nature of
LLMs, compounded by concerns on user acceptance of LLM driven body control. To
bridge this gap and realize the potential of collaborative human-LLM action,
this work explores human experience of LLM driven kinesthetic assistance.
Specifically, we introduced an "Align-Analyze-Adjust" strategy and developed
FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)
for flight skill acquisition, a representative operational skill domain.
FlightAxis learns flight skills from manuals and guides forearm movements
during simulated flight tasks. Our results demonstrate high user acceptance of
LLM-mediated body control and significantly reduced task completion times.
Crucially, trainees reported that this kinesthetic assistance enhanced their
awareness of operation flaws and fostered increased engagement in the training
process, rather than relieving perceived load. This work demonstrated the
potential of kinesthetic LLM training in operational skill acquisition.

</details>


### [26] [RAGTrace: Understanding and Refining Retrieval-Generation Dynamics in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.06056)
*Sizhe Cheng,Jiaping Li,Huanchen Wang,Yuxin Ma*

Main category: cs.HC

TL;DR: 本文介绍了RAGTrace，一个用于分析基于RAG系统的工作流中的检索和生成动态的交互式评估系统。


<details>
  <summary>Details</summary>
Motivation: 为了解决RAG工作流中内部知识整合和检索-生成交互不透明的问题。

Method: 通过文献综述和专家访谈，开发了支持多层次分析的系统，包括性能评估、检索相关性、生成忠实性等。

Result: 通过案例研究和专家评估，验证了系统在实际RAG应用中的有效性。

Conclusion: RAGTrace能够整合分析检索与生成的关系，帮助用户追踪知识来源并识别潜在失败案例。

Abstract: Retrieval-Augmented Generation (RAG) systems have emerged as a promising
solution to enhance large language models (LLMs) by integrating external
knowledge retrieval with generative capabilities. While significant
advancements have been made in improving retrieval accuracy and response
quality, a critical challenge remains that the internal knowledge integration
and retrieval-generation interactions in RAG workflows are largely opaque. This
paper introduces RAGTrace, an interactive evaluation system designed to analyze
retrieval and generation dynamics in RAG-based workflows. Informed by a
comprehensive literature review and expert interviews, the system supports a
multi-level analysis approach, ranging from high-level performance evaluation
to fine-grained examination of retrieval relevance, generation fidelity, and
cross-component interactions. Unlike conventional evaluation practices that
focus on isolated retrieval or generation quality assessments, RAGTrace enables
an integrated exploration of retrieval-generation relationships, allowing users
to trace knowledge sources and identify potential failure cases. The system's
workflow allows users to build, evaluate, and iterate on retrieval processes
tailored to their specific domains of interest. The effectiveness of the system
is demonstrated through case studies and expert evaluations on real-world RAG
applications.

</details>


### [27] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: ThematicPlane是一个交互式系统，帮助用户通过高级语义概念（如情绪、风格）控制生成AI输出，弥补创意意图与系统控制之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前生成AI工具需要用户通过提示或外部参考表达创意意图，限制了流畅的创意探索。ThematicPlane旨在解决这一问题。

Method: 通过交互式主题设计平面，用户可以导航和操纵高级语义概念（如情绪、风格或叙事基调）。

Result: 在探索性研究中，用户能够通过熟悉主题展开创意探索，尽管期望与输出之间的映射仍需改进。

Conclusion: ThematicPlane支持迭代式创意工作流，并为生成设计工具的直观语义驱动交互提供了新方向。

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>


### [28] [A Multimodal Framework for Understanding Collaborative Design Processes](https://arxiv.org/abs/2508.06117)
*Maurice Koch,Nelusa Pathmanathan,Daniel Weiskopf,Kuno Kurzhals*

Main category: cs.HC

TL;DR: 提出了一个模块化框架reCAPit，通过多模态数据采集和AI提取，结合可视化分析，改进协作设计工作坊的分析方法。


<details>
  <summary>Details</summary>
Motivation: 协作设计工作坊的分析通常依赖于文本形式的观察或访谈记录，难以整合多源异构数据。

Method: 开发了reCAPit系统，整合视频、音频、笔记等多模态数据，采用AI提取和可视化工具（如流图、主题卡片）进行分析。

Result: 通过六个工作坊的案例验证了系统的实用性，展示了其在多领域（如城市规划、乐队实践可视化）中的应用。

Conclusion: 该研究丰富了协作设计工作坊的方法学，实现了多模态数据的高效采集、分析和透明化传播。

Abstract: An essential task in analyzing collaborative design processes, such as those
that are part of workshops in design studies, is identifying design outcomes
and understanding how the collaboration between participants formed the results
and led to decision-making. However, findings are typically restricted to a
consolidated textual form based on notes from interviews or observations. A
challenge arises from integrating different sources of observations, leading to
large amounts and heterogeneity of collected data. To address this challenge we
propose a practical, modular, and adaptable framework of workshop setup,
multimodal data acquisition, AI-based artifact extraction, and visual analysis.
Our interactive visual analysis system, reCAPit, allows the flexible
combination of different modalities, including video, audio, notes, or gaze, to
analyze and communicate important workshop findings. A multimodal streamgraph
displays activity and attention in the working area, temporally aligned topic
cards summarize participants' discussions, and drill-down techniques allow
inspecting raw data of included sources. As part of our research, we conducted
six workshops across different themes ranging from social science research on
urban planning to a design study on band-practice visualization. The latter two
are examined in detail and described as case studies. Further, we present
considerations for planning workshops and challenges that we derive from our
own experience and the interviews we conducted with workshop experts. Our
research extends existing methodology of collaborative design workshops by
promoting data-rich acquisition of multimodal observations, combined AI-based
extraction and interactive visual analysis, and transparent dissemination of
results.

</details>


### [29] [Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models](https://arxiv.org/abs/2508.06300)
*Weihan Zhang,Jun Tao*

Main category: cs.HC

TL;DR: 提出了一种基于自然语言交互的流模式可视化探索框架，通过语义对齐实现直觉化查询。


<details>
  <summary>Details</summary>
Motivation: 传统流可视化方法需要学习复杂图形界面，自然语言交互更直观，但机器识别科学概念和提取流结构存在挑战。

Method: 利用去噪自编码器编码流线片段，通过投影层将流模式表示与LLM语义空间对齐，结合注意力机制实现文本描述与流模式的匹配。

Result: 开发了交互式界面，用户可通过自然语言查询和可视化流结构，案例研究验证了框架的有效性。

Conclusion: 该框架为流探索提供了直觉化和智能化的解决方案，降低了使用门槛。

Abstract: Explorative flow visualization allows domain experts to analyze complex flow
structures by interactively investigating flow patterns. However, traditional
visual interfaces often rely on specialized graphical representations and
interactions, which require additional effort to learn and use. Natural
language interaction offers a more intuitive alternative, but teaching machines
to recognize diverse scientific concepts and extract corresponding structures
from flow data poses a significant challenge. In this paper, we introduce an
automated framework that aligns flow pattern representations with the semantic
space of large language models (LLMs), eliminating the need for manual
labeling. Our approach encodes streamline segments using a denoising
autoencoder and maps the generated flow pattern representations to LLM
embeddings via a projector layer. This alignment empowers semantic matching
between textual embeddings and flow representations through an attention
mechanism, enabling the extraction of corresponding flow patterns based on
textual descriptions. To enhance accessibility, we develop an interactive
interface that allows users to query and visualize flow structures using
natural language. Through case studies, we demonstrate the effectiveness of our
framework in enabling intuitive and intelligent flow exploration.

</details>


### [30] [Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional Resonance](https://arxiv.org/abs/2508.06349)
*Serena Tardelli,Lorenzo Alvisi,Lorenzo Cima,Stefano Cresci,Maurizio Tesconi*

Main category: cs.HC

TL;DR: 研究表明，Telegram上的表情符号反应更多反映社交认可而非情感共鸣，与信息情感内容存在系统性不匹配。


<details>
  <summary>Details</summary>
Motivation: 探究表情符号反应是否仅反映情感共鸣，还是更广泛的社交动态。

Method: 收集并分析超过65万条Telegram消息，标注情感、说服策略等，结合词典和大模型推断反应情感。

Result: 表情反应与信息情感不匹配，正面反应占主导，暗示其为社交认可信号而非情感共鸣。

Conclusion: 表情符号反应不宜直接作为情感共鸣的代理，需考虑其社交功能。

Abstract: Emoji reactions are a frequently used feature of messaging platforms. Prior
work mainly interpreted emojis as indicators of emotional resonance or user
sentiment. However, emoji reactions may instead reflect broader social
dynamics. Here, we investigate the communicative function of emoji reactions on
Telegram by analyzing the relationship between the emotional and rhetorical
content of messages and the emoji reactions they receive. We collect and
analyze over 650k Telegram messages that received at least one emoji reaction.
We annotate each message with sentiment, emotion, persuasion strategy, and
speech act labels, and infer the sentiment and emotion of emoji reactions using
both lexicons and large languages. We find a systematic mismatch between
message sentiment and reaction sentiment, with positive reactions dominating
even when the message is neutral or negative. We show that this pattern remains
consistent across rhetorical strategies and emotional tones, suggesting that
emoji reactions may signal a degree of social approval rather than reflecting
emotional resonance. Finally, we shed light on the communicative strategies
that predict greater emoji engagement. These findings have methodological
implications for sentiment analysis, as interpreting emoji reactions as direct
proxies for emotional response may be misleading.

</details>


### [31] [Zombitron: towards a toolbox for repurposing obsolete smartphones into new interactive systems](https://arxiv.org/abs/2508.06354)
*Clara Rigaud*

Main category: cs.HC

TL;DR: 探讨如何利用废旧智能手机和平板电脑构建新的交互系统，以音乐控制器为例，研究设计过程、挑战与解决方案，并提出开源工具包的初步构想。


<details>
  <summary>Details</summary>
Motivation: 减少电子废物，通过重新利用废旧设备创造新的交互式系统，促进可持续计算。

Method: 从诊断废旧设备到设计新控制器，记录整个过程，并与专业音乐家讨论软硬件优化。

Result: 提出了一个基于高级网页技术的开源工具包构想，支持设计师利用旧设备开发新系统。

Conclusion: 废旧设备再利用可行且环保，未来可扩展为开源工具包，促进可持续交互设计。

Abstract: This article explores the possibilities of reusing obsolete smartphones and
tablets to build new interactive systems. Taking the case of a musical
instrument, I present my research into the design of a controller made from
various of these obsolete smartphones. From the diagnostic stage to the
creation of a new autonomous electronic object, I document the process, the
barriers and the levers encountered. Based on these explorations and
discussions with two professional musicians, I provide several insights into
the software and hardware aspects, with a view to continuing this work, towards
the creation of an open-source toolkit enabling anyone to build new interactive
systems with old devices. I discuss the implication of how a high-level
web-based approach could allow designers to enter the black box and foster
permacomputing using smartphones.

</details>


### [32] [Non-programmers Assessing AI-Generated Code: A Case Study of Business Users Analyzing Data](https://arxiv.org/abs/2508.06484)
*Yuvraj Virk,Dongyu Liu*

Main category: cs.HC

TL;DR: 研究表明，非技术用户难以可靠地识别AI生成的代码错误，即使得到明确提示，也无法有效验证数据分析结果。设计改进虽有一定帮助，但用户仍需更多支持。


<details>
  <summary>Details</summary>
Motivation: 随着非技术用户依赖AI生成代码完成技术任务，但其可靠性存疑，研究旨在评估用户是否能有效识别AI错误，尤其是在特定领域中。

Method: 通过对营销和销售专业人员进行调查，展示AI生成的数据分析自然语言解释，并明确提示错误，观察其识别能力。随后改进显示格式以支持用户评估。

Result: 尽管提供提示和改进格式，参与者仍难以发现关键错误。改进设计有一定效果，但用户仍难以理解AI的步骤和替代方案。

Conclusion: 非技术用户无法独立可靠验证AI生成的数据分析，未来设计需提供更多支持，以避免低质量或不安全的决策。

Abstract: Non-technical end-users increasingly rely on AI code generation to perform
technical tasks like data analysis. However, large language models (LLMs)
remain unreliable, and it is unclear whether end-users can effectively identify
model errors $\unicode{x2014}$ especially in realistic and domain-specific
scenarios. We surveyed marketing and sales professionals to assess their
ability to critically evaluate LLM-generated analyses of marketing data.
Participants were shown natural language explanations of the AI's code,
repeatedly informed the AI often makes mistakes, and explicitly prompted to
identify them. Yet, participants frequently failed to detect critical flaws
that could compromise decision-making, many of which required no technical
knowledge to recognize. To investigate why, we reformatted AI responses into
clearly delineated steps and provided alternative approaches for each decision
to support critical evaluation. While these changes had a positive effect,
participants often struggled to reason through the AI's steps and alternatives.
Our findings suggest that business professionals cannot reliably verify
AI-generated data analyses on their own and explore reasons why to inform
future designs. As non-programmers adopt code-generating AI for technical
tasks, unreliable AI and insufficient human oversight poses risks of unsafe or
low-quality decisions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [33] [DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models](https://arxiv.org/abs/2508.05685)
*Yara Bahram,Mohammadhadi Shateri,Eric Granger*

Main category: cs.GR

TL;DR: 本文提出了一种名为DogFit的领域引导微调方法，用于在扩散模型迁移学习中实现高效的可控性改进，避免额外的计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统的迁移学习方法在小型目标领域上表现不佳，而现有的测试时引导方法虽能改善图像保真度，但计算成本高。

Method: DogFit通过在训练损失中注入领域感知引导偏移，内化引导行为，并通过轻量级条件机制编码引导强度。此外，提出了两种调度策略（晚启动和截断）优化生成质量和训练稳定性。

Result: 在DiT和SiT框架的六个目标领域上，DogFit在FID和FDDINOV2指标上优于现有方法，计算开销减少2倍。

Conclusion: DogFit是一种高效且可控的扩散模型迁移学习方法，显著提升了生成质量和计算效率。

Abstract: Transfer learning of diffusion models to smaller target domains is
challenging, as naively fine-tuning the model often results in poor
generalization. Test-time guidance methods help mitigate this by offering
controllable improvements in image fidelity through a trade-off with sample
diversity. However, this benefit comes at a high computational cost, typically
requiring dual forward passes during sampling. We propose the Domain-guided
Fine-tuning (DogFit) method, an effective guidance mechanism for diffusion
transfer learning that maintains controllability without incurring additional
computational overhead. DogFit injects a domain-aware guidance offset into the
training loss, effectively internalizing the guided behavior during the
fine-tuning process. The domain-aware design is motivated by our observation
that during fine-tuning, the unconditional source model offers a stronger
marginal estimate than the target model. To support efficient controllable
fidelity-diversity trade-offs at inference, we encode the guidance strength
value as an additional model input through a lightweight conditioning
mechanism. We further investigate the optimal placement and timing of the
guidance offset during training and propose two simple scheduling strategies,
i.e., late-start and cut-off, which improve generation quality and training
stability. Experiments on DiT and SiT backbones across six diverse target
domains show that DogFit can outperform prior guidance methods in transfer
learning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling
TFLOPS.

</details>


### [34] [Exploring Interactive Simulation of Grass Display Color Characteristic Based on Real-World Conditions](https://arxiv.org/abs/2508.06086)
*Kojiro Tanaka,Keiichi Sato,Masahiko Mikawa,Makoto Fujisawa*

Main category: cs.GR

TL;DR: 通过虚拟环境模拟草基显示的颜色变化特性，以减少实际实验的时间和成本。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要每次光照或视角变化时进行实际实验，耗时且成本高。虚拟模拟虽开始研究，但仍有耗时问题。

Method: 开发了一种交互式模拟方法，基于虚拟环境模拟草的颜色变化特性。

Result: 模拟结果显示与真实草色特性相似，且速度更快，准确性接近先前研究。

Conclusion: 该方法可为草基显示的开发提供更高效的模拟工具。

Abstract: Recent research has focused on incorporating media into living environments
via color-controlled materials and image display. In particular, grass-based
displays have drawn attention as landscape-friendly interactive interfaces. To
develop the grass display, it is important to obtain the grass color change
characteristics that depend on the real environment. However, conventional
methods require experiments on actual equipment every time the lighting or
viewpoint changes, which is time-consuming and costly. Although research has
begun on simulating grass colors, this approach still faces significant issues
as it takes many hours for a single measurement. In this paper, we explore an
interactive simulation of a grass display color change characteristic based on
real-world conditions in a virtual environment. We evaluated our method's
accuracy by simulating grass color characteristics across multiple viewpoints
and environments, and then compared the results against prior work. The results
indicated that our method tended to simulate the grass color characteristics
similar to the actual characteristics and showed the potential to do so more
quickly and with comparable accuracy to the previous study.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [35] [Bionic Vision as Neuroadaptive XR: Closed-Loop Perceptual Interfaces for Neurotechnology](https://arxiv.org/abs/2508.05963)
*Michael Beyeler*

Main category: cs.ET

TL;DR: 该论文提出将仿生视觉重新定义为神经适应性XR（扩展现实），而非单纯复制自然视觉，强调通过双向接口实现脑与设备的协同适应。


<details>
  <summary>Details</summary>
Motivation: 传统视觉神经假体试图恢复自然视觉，但实际效果受限，输入稀疏、扭曲且不稳定。作者认为应重新设计，使其更像XR技术，适应大脑与行为需求。

Method: 论文比较了传统XR、现有植入设备和提出的神经适应性系统，提出了一个新的设计空间，强调脑-设备双向接口。

Result: 提出了神经适应性XR的概念，为感官增强技术提供了新方向，并呼吁XR社区参与研究。

Conclusion: 未来研究方向应关注编码、评估、学习和伦理问题，推动更包容、大脑感知的计算技术发展。

Abstract: Visual neuroprostheses are commonly framed as technologies to restore natural
sight to people who are blind. In practice, they create a novel mode of
perception shaped by sparse, distorted, and unstable input. They resemble early
extended reality (XR) headsets more than natural vision, streaming video from a
head-mounted camera to a neural "display" with under 1000 pixels, limited field
of view, low refresh rates, and nonlinear spatial mappings. No amount of
resolution alone will make this experience natural. This paper proposes a
reframing: bionic vision as neuroadaptive XR. Rather than replicating natural
sight, the goal is to co-adapt brain and device through a bidirectional
interface that responds to neural constraints, behavioral goals, and cognitive
state. By comparing traditional XR, current implants, and proposed
neuroadaptive systems, it introduces a new design space for inclusive,
brain-aware computing. It concludes with research provocations spanning
encoding, evaluation, learning, and ethics, and invites the XR community to
help shape the future of sensory augmentation.

</details>


### [36] [Between Tool and Trouble: Student Attitudes Toward AI in Programming Education](https://arxiv.org/abs/2508.05999)
*Sergio Rojas-Galeano,Julian Tejada,Fernando Marmolejo-Ramos*

Main category: cs.ET

TL;DR: 研究AI代码助手如何影响新手程序员在考试中的表现，分为AI辅助和无AI辅助两部分，发现AI工具虽能提升信心但可能导致过度依赖。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对新手程序员学习的影响及其潜在的教育策略需求。

Method: 通过在编程课程中设置两阶段考试（有AI辅助和无AI辅助），收集20名学生的评分和开放式反馈。

Result: 学生认为AI工具有助于理解代码和提升信心，但在无AI辅助时表现出知识迁移困难。

Conclusion: 需结合AI工具与基础编程技能的教学策略，避免学生对AI的过度依赖。

Abstract: This study examines how AI code assistants shape novice programmers
experiences during a two-part exam in an introductory programming course. In
the first part, students completed a programming task with access to AI
support; in the second, they extended their solutions without AI. We collected
Likert-scale and open-ended responses from 20 students to evaluate their
perceptions and challenges. Findings suggest that AI tools were perceived as
helpful for understanding code and increasing confidence, particularly during
initial development. However, students reported difficulties transferring
knowledge to unaided tasks, revealing possible overreliance and gaps in
conceptual understanding. These insights highlight the need for pedagogical
strategies that integrate AI meaningfully while reinforcing foundational
programming skills.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [37] [Accelerating Data Chunking in Deduplication Systems using Vector Instructions](https://arxiv.org/abs/2508.05797)
*Sreeharsha Udayashankar,Abdelrahman Baba,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: VectorCDC利用矢量CPU指令加速无哈希CDC算法，显著提升吞吐量而不影响去重空间节省。


<details>
  <summary>Details</summary>
Motivation: CDC算法因需全文件扫描而成为性能瓶颈，亟需提速。

Method: 采用矢量CPU指令（如SSE/AVX）优化哈希无关的CDC算法。

Result: 在多种CPU上实现8.35倍至26.2倍吞吐量提升，空间节省不变。

Conclusion: VectorCDC高效解决CDC性能瓶颈，适用广泛硬件。

Abstract: Content-defined Chunking (CDC) algorithms dictate the overall space savings
that deduplication systems achieve. However, due to their need to scan each
file in its entirety, they are slow and often the main performance bottleneck
within data deduplication. We present VectorCDC, a method to accelerate
hashless CDC algorithms using vector CPU instructions, such as SSE / AVX. Our
evaluation shows that VectorCDC is effective on Intel, AMD, ARM, and IBM CPUs,
achieving 8.35x - 26.2x higher throughput than existing vector-accelerated
techniques without affecting the deduplication space savings.

</details>


### [38] [A Dynamic Approach to Load Balancing in Cloud Infrastructure: Enhancing Energy Efficiency and Resource Utilization](https://arxiv.org/abs/2508.05821)
*Shadman Sakib,Ajay Katangur,Rahul Dubey*

Main category: cs.DC

TL;DR: 该论文介绍了一种基于实时性能指标的Score-Based Dynamic Load Balancer (SBDLB)，用于优化云计算环境中的负载均衡和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 云计算快速发展使得负载均衡成为关键挑战，传统方法难以动态适应工作量波动。

Method: 提出SBDLB，基于实时性能指标分配工作负载，并在CloudSim 7G平台上测试对比。

Result: SBDLB在响应时间、数据处理时间和运行成本上均优于传统方法，提升了34%的响应时间和13%的数据处理时间。

Conclusion: SBDLB能动态优化云基础设施的资源利用，同时降低能耗，具有显著的性能优势。

Abstract: Cloud computing has grown rapidly in recent years, mainly due to the sharp
increase in data transferred over the internet. This growth makes load
balancing a key part of cloud systems, as it helps distribute user requests
across servers to maintain performance, prevent overload, and ensure a smooth
user experience. Despite its importance, managing server resources and keeping
workloads balanced over time remains a major challenge in cloud environments.
This paper introduces a novel Score-Based Dynamic Load Balancer (SBDLB) that
allocates workloads to virtual machines based on real-time performance metrics.
The objective is to enhance resource utilization and overall system efficiency.
The method was thoroughly tested using the CloudSim 7G platform, comparing its
performance against the throttled load balancing strategy. Evaluations were
conducted across a variety of workloads and scenarios, demonstrating the
SBDLB's ability to adapt dynamically to workload fluctuations while optimizing
resource usage. The proposed method outperformed the throttled strategy,
improving average response times by 34% and 37% in different scenarios. It also
reduced data center processing times by an average of 13%. Over a 24-hour
simulation, the method decreased operational costs by 15%, promoting a more
energy-efficient and sustainable cloud infrastructure through reduced energy
consumption.

</details>


### [39] [Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML Next To Your Data](https://arxiv.org/abs/2508.05904)
*Brandon Baker,Elliott Brossard,Chenwei Xie,Zihao Ye,Deen Liu,Yijun Xie,Arthur Zwiegincew,Nitya Kumar Sharma,Gaurav Jain,Eugene Retunsky,Mike Halcrow,Derek Denny-Brown,Istvan Cseri,Tyler Akidau,Yuxiong He*

Main category: cs.DC

TL;DR: Snowpark是Snowflake推出的一款支持数据工程和AI/ML工作负载的解决方案，具备高性能、强安全性和易用性，通过弹性扩展和无缝集成提升数据处理效率。


<details>
  <summary>Details</summary>
Motivation: Snowflake旨在扩展其AI Data Cloud愿景，支持多语言编程，提供更灵活的数据工程和AI/ML工作负载解决方案。

Method: 设计目标包括高性能、安全性和易用性，架构上采用弹性扩展和与Snowflake核心计算基础设施的无缝集成，并通过Python包缓存、任务调度优化和数据倾斜管理提升性能。

Result: Snowpark在大型数据工程和AI/ML任务中展现出高效和有效性，适用于多种数据架构。

Conclusion: Snowpark为数据工程和AI/ML工作负载提供了创新的解决方案，通过弹性架构和性能优化进一步扩展了Snowflake的能力。

Abstract: Snowflake revolutionized data analytics with an elastic architecture that
decouples compute and storage, enabling scalable solutions supporting data
architectures like data lake, data warehouse, data lakehouse, and data mesh.
Building on this foundation, Snowflake has advanced its AI Data Cloud vision by
introducing Snowpark, a managed turnkey solution that supports data engineering
and AI and ML workloads using Python and other programming languages.
  This paper outlines Snowpark's design objectives towards high performance,
strong security and governance, and ease of use. We detail the architecture of
Snowpark, highlighting its elastic scalability and seamless integration with
Snowflake core compute infrastructure. This includes leveraging Snowflake
control plane for distributed computing and employing a secure sandbox for
isolating Snowflake SQL workloads from Snowpark executions. Additionally, we
present core innovations in Snowpark that drive further performance
enhancements, such as query initialization latency reduction through Python
package caching, improved workload scheduling for customized workloads, and
data skew management via efficient row redistribution. Finally, we showcase
real-world case studies that illustrate Snowpark's efficiency and effectiveness
for large-scale data engineering and AI and ML tasks.

</details>


### [40] [KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training](https://arxiv.org/abs/2508.06001)
*Kai Zhang,Peng Wang,Sai Bi,Jianming Zhang,Yuanjun Xiong*

Main category: cs.DC

TL;DR: KnapFormer是一个高效的框架，结合了工作负载平衡和序列并行，用于分布式训练扩散变换器（DiT），显著提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 解决分布式训练中由可变长度文本输入和视觉令牌计数不均导致的工作负载不平衡问题。

Method: 通过全局背包问题和序列并行性优化负载平衡，结合DeepSpeed-Ulysees和半经验工作负载模型。

Result: 在实际训练中实现了低于1%的工作负载差异，消除滞后效应，训练速度提升2至3倍。

Conclusion: KnapFormer在混合分辨率和图像-视频联合训练中表现优异，已开源实现。

Abstract: We present KnapFormer, an efficient and versatile framework to combine
workload balancing and sequence parallelism in distributed training of
Diffusion Transformers (DiT). KnapFormer builds on the insight that strong
synergy exists between sequence parallelism and the need to address the
significant token imbalance across ranks. This imbalance arises from
variable-length text inputs and varying visual token counts in mixed-resolution
and image-video joint training. KnapFormer redistributes tokens by first
gathering sequence length metadata across all ranks in a balancing group and
solving a global knapsack problem. The solver aims to minimize the variances of
total workload per-GPU, while accounting for the effect of sequence
parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the
load-balancing decision process and utilizing a simple semi-empirical workload
model, KnapFormers achieves minimal communication overhead and less than 1%
workload discrepancy in real-world training workloads with sequence length
varying from a few hundred to tens of thousands. It eliminates straggler
effects and achieves 2x to 3x speedup when training state-of-the-art diffusion
models like FLUX on mixed-resolution and image-video joint data corpora. We
open-source the KnapFormer implementation at
https://github.com/Kai-46/KnapFormer/

</details>


### [41] [EC2MoE: Adaptive End-Cloud Pipeline Collaboration Enabling Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2508.06024)
*Zheming Yang,Yunqing Hu,Sheng Sun,Wen Ji*

Main category: cs.DC

TL;DR: 本文提出了EC2MoE框架，通过端云协作优化Mixture-of-Experts（MoE）推理，提升效率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在异构端云环境中部署时面临的专家调度、通信开销和资源异构性等挑战。

Method: 设计了硬件感知的轻量级组门网络和端云协作的流水线优化机制，包括低秩压缩和路由感知的调度算法。

Result: 实验表明，EC2MoE能提升吞吐量2.2-5.1倍，降低延迟53%-67%，同时保持高精度和可扩展性。

Conclusion: EC2MoE为异构端云环境下的MoE推理提供了高效且自适应的解决方案。

Abstract: The Mixture-of-Experts (MoE) paradigm has emerged as a promising solution to
scale up model capacity while maintaining inference efficiency. However,
deploying MoE models across heterogeneous end-cloud environments poses new
challenges in expert scheduling, communication overhead, and resource
heterogeneity. In this paper, we propose EC2MoE, an adaptive framework for
scalable MoE inference via end-cloud pipeline collaboration. First, we design a
hardware-aware lightweight group gate network that enhances expert selection
and computational efficiency. By incorporating a hardware-aware local expert
selection mechanism, the system adaptively filters candidate experts based on
real-time device profiles. A lightweight group gate module then integrates
local and global gating outputs to achieve high-quality expert routing with
minimal overhead. Second, we develop a pipeline optimization mechanism based on
endcloud collaboration to accelerate MoE inference. This includes an
encoder-decoder structure based on low-rank compression, which reduces
transmission and computation costs. And a route-aware heuristic pipeline
scheduling algorithm that dynamically allocates inference stages across devices
according to workload and network topology. Extensive experiments show that
EC2MoE can increase throughput by 2.2x to 5.1x and reduce end-to-end latency by
53% to 67% while maintaining high accuracy compared to state-of-the-art
methods. It also maintains good scalability under dynamic load and network
environments.

</details>


### [42] [KV Cache Compression for Inference Efficiency in LLMs: A Review](https://arxiv.org/abs/2508.06297)
*Yanyu Liu,Jingying Fu,Sixiang Liu,Yitian Zou,You Fu,Jiehan Zhou,Shouhua Zhang*

Main category: cs.DC

TL;DR: 这篇论文综述了大型语言模型（LLM）推理中KV缓存的优化技术，分析了现有方法的有效性、权衡及应用场景，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型上下文长度的增加，KV缓存需求呈指数增长，导致内存瓶颈，限制了模型的推理效率和可扩展性。因此，优化KV缓存是提升性能的关键。

Method: 系统地评估了KV缓存优化技术，包括选择性令牌策略、量化和注意力压缩等压缩策略，分析其对内存使用和推理速度的影响。

Result: 总结了现有方法的局限性（如兼容性问题）和挑战，并提出了未来研究方向，如混合优化技术、自适应动态策略和软硬件协同设计。

Conclusion: 论文为优化KV缓存提供了全面的分析，旨在提升大型语言模型的推理效率，促进其实际应用。

Abstract: Withtherapid advancement of large language models (LLMs), the context length
for inference has been continuously increasing, leading to an exponential
growth in the demand for Key-Value (KV) caching. This has resulted in a
significant memory bottleneck, limiting the inference efficiency and
scalability of the models. Therefore, optimizing the KV cache during inference
is crucial for enhancing performance and efficiency. This review systematically
examines current KV cache optimization techniques, including compression
strategies such as selective token strategies, quantization, and attention
compression. We evaluate the effectiveness, trade-offs, and application
scenarios of these methods, providing a comprehensive analysis of their impact
on memory usage and inference speed. We focus on identifying the limitations
and challenges of existing methods, such as compatibility issues with different
models and tasks. Additionally, this review highlights future research
directions, including hybrid optimization techniques, adaptive dynamic
strategies, and software-hardware co-design. These approaches aim to improve
inference efficiency and promote the practical application of large language
models.

</details>


### [43] [Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision](https://arxiv.org/abs/2508.06339)
*Evelyne Ringoot,Rabab Alomairy,Valentin Churavy,Alan Edelman*

Main category: cs.DC

TL;DR: 提出了一种基于QR分解的GPU加速SVD算法在Julia中的可移植实现，支持多种GPU架构和数据类型，性能接近或优于主流线性代数库。


<details>
  <summary>Details</summary>
Motivation: SVD在科学计算和机器学习中至关重要，尤其是在大规模机器学习（如LLMs）中用于低秩适应（LoRA）。现有实现缺乏对Apple Metal GPU和半精度的支持，且性能优化不足。

Method: 基于经典的两阶段QR分解（矩阵降为带状和双对角形式），利用Julia的多重分发和元编程能力，与GPUArrays和KernelAbstractions框架集成，实现硬件无关的统一函数。

Result: 性能测试表明，该实现支持多GPU架构和数据类型（包括Metal GPU和半精度），在1024x1024以上矩阵中性能优于MAGMA、SLATE等库，接近cuSOLVER的80%-90%。

Conclusion: 该工作证明了可移植性与高性能可以共存，为GPU加速的SVD算法提供了新的解决方案。

Abstract: This paper presents a portable, GPU-accelerated implementation of a QR-based
singular value computation algorithm in Julia. The singular value ecomposition
(SVD) is a fundamental numerical tool in scientific computing and machine
learning, providing optimal low-rank matrix approximations. Its importance has
increased even more in large-scale machine learning pipelines, including large
language models (LLMs), where it enables low-rank adaptation (LoRA). The
implemented algorithm is based on the classic two-stage QR reduction,
consisting of successive matrix reduction to band form and bidiagonal form. Our
implementation leverages Julia's multiple dispatch and metaprogramming
capabilities, integrating with the GPUArrays and KernelAbstractions frameworks
to provide a unified type and hardware-agnostic function. It supports diverse
GPU architectures and data types, and is, to our knowledge, the first
GPU-accelerated singular value implementation to support Apple Metal GPUs and
half precision. Performance results on multiple GPU backends and data types
demonstrate that portability does not require sacrificing performance: the
unified function outperforms most linear algebra libraries (MAGMA, SLATE,
rocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%
of the performance of cuSOLVER for large matrices.

</details>


### [44] [Blockchain-Enabled Federated Learning](https://arxiv.org/abs/2508.06406)
*Murtaza Rangwala,Venugopal K R,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 区块链联邦学习(BCFL)通过四维分类法分析协调结构、共识机制、存储架构和信任模型，解决协作AI系统中的信任、隐私和协调问题。


<details>
  <summary>Details</summary>
Motivation: 解决协作AI系统中的信任、隐私和协调挑战，通过区块链技术提升联邦学习的透明度和安全性。

Method: 采用四维分类法分析BCFL系统的设计模式，包括协调结构、共识机制、存储架构和信任模型，并以TrustMesh框架为案例研究。

Result: BCFL系统在医疗、金融服务和物联网安全等实际应用中表现出与集中式方法相当的性能，同时提供更强的安全保障。

Conclusion: BCFL系统通过区块链技术实现了高效的协作智能，平衡了性能与安全性，适用于异构数据分布的复杂场景。

Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges
of trust, privacy, and coordination in collaborative AI systems. This chapter
provides comprehensive architectural analysis of BCFL systems through a
systematic four-dimensional taxonomy examining coordination structures,
consensus mechanisms, storage architectures, and trust models. We analyze
design patterns from blockchain-verified centralized coordination to fully
decentralized peer-to-peer networks, evaluating trade-offs in scalability,
security, and performance. Through detailed examination of consensus mechanisms
designed for federated learning contexts, including Proof of Quality and Proof
of Federated Learning, we demonstrate how computational work can be repurposed
from arbitrary cryptographic puzzles to productive machine learning tasks. The
chapter addresses critical storage challenges by examining multi-tier
architectures that balance blockchain's transaction constraints with neural
networks' large parameter requirements while maintaining cryptographic
integrity. A technical case study of the TrustMesh framework illustrates
practical implementation considerations in BCFL systems through distributed
image classification training, demonstrating effective collaborative learning
across IoT devices with highly non-IID data distributions while maintaining
complete transparency and fault tolerance. Analysis of real-world deployments
across healthcare consortiums, financial services, and IoT security
applications validates the practical viability of BCFL systems, achieving
performance comparable to centralized approaches while providing enhanced
security guarantees and enabling new models of trustless collaborative
intelligence.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [45] [A Cross-Perspective Annotated Dataset for Dynamic Object-Level Attention Modeling in Cloud Gaming](https://arxiv.org/abs/2508.06077)
*Hongqin Lei,Haowei Tang,Zhe Zhang*

Main category: cs.DB

TL;DR: 该论文提出了一个基于《侠盗猎车手5》(GTA V)的游戏数据集，专注于玩家感兴趣的对象及其语义关系，填补了现有数据集忽略语义特征的空白。


<details>
  <summary>Details</summary>
Motivation: 云游戏的快速发展需要高质量的数据支持现有深度学习方法，而现有数据集通常忽略对象的语义关系及其独特特征。

Method: 通过收集GTA V的游戏片段并标注玩家感兴趣的对象，分析了影响玩家兴趣的因素，如玩家速度、对象大小和对象速度。

Result: 发现玩家在游戏中的速度、对象的大小和速度是影响玩家兴趣的主要因素，并公开了数据集供研究使用。

Conclusion: 该研究为云游戏的质量体验提供了更丰富的数据支持，有助于优化深度学习模型的效果。

Abstract: Cloud gaming has gained popularity as it provides high-quality gaming
experiences on thin hardware, such as phones and tablets. Transmitting gameplay
frames at high resolutions and ultra-low latency is the key to guaranteeing
players' quality of experience (QoE). Numerous studies have explored deep
learning (DL) techniques to address this challenge. The efficiency of these
DL-based approaches is highly affected by the dataset. However, existing
datasets usually focus on the positions of objects while ignoring semantic
relationships with other objects and their unique features. In this paper, we
present a game dataset by collecting gameplay clips from Grand Theft Auto (GTA)
V, and annotating the player's interested objects during the gameplay. Based on
the collected data, we analyze several factors that have an impact on player's
interest and identify that the player's in-game speed, object's size, and
object's speed are the main factors. The dataset is available at
https://drive.google.com/drive/folders/1idH251a2K-hGGd3pKjX-3Gx5o_rUqLC4?usp=sharing

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [46] [ConiQ: Enabling Concatenated Quantum Error Correction on Neutral Atom Arrays](https://arxiv.org/abs/2508.05779)
*Pengyu Liu,Mingkuan Xu,Hengyun Zhou,Hanrui Wang,Umut A. Acar,Yunong Shi*

Main category: cs.AR

TL;DR: 本文提出了一种针对中性原子阵列的高效编译方法，显著降低了量子计算的时空开销。


<details>
  <summary>Details</summary>
Motivation: 解决级联码（尤其是多超立方码）在实现逻辑门和硬件并行性方面的挑战。

Method: 采用自同构辅助分层寻址（AHA）逻辑CNOT门和虚拟原子中间表示（VAIR）进行优化。

Result: ConiQ编译器实现了时空开销最高2000倍的降低，编译时间最高10^6倍的减少。

Conclusion: 级联码为容错量子计算提供了一种有前景的方法。

Abstract: Recent progress on concatenated codes, especially many-hypercube codes,
achieves unprecedented space efficiency. Yet two critical challenges persist in
practice. First, these codes lack efficient implementations of addressable
logical gates. Second, the required high degree of parallelism and long-range
interactions pose significant challenges for current hardware platforms. In
this paper, we propose an efficient compilation approach for concatenated
codes, specifically many-hypercube codes, targeted at neutral atom arrays,
which provide the necessary parallelism and long-range interactions. Our
approach builds on two key innovations. First, we introduce
Automorphism-assisted Hierarchical Addressing (AHA) logical CNOT gates that
significantly reduce spacetime overhead compared to conventional
distillation-based methods. Second, we develop Virtual Atom Intermediate
Representation (VAIR) that enables level-wise optimization and legalization. We
implement these innovations in ConiQ, a hardware-aware quantum compiler
designed to compile fault-tolerant quantum circuits for neutral atom arrays
using many-hypercube codes. Our evaluation demonstrates that ConiQ achieves up
to 2000x reduction in spacetime overhead and up to 10^6x reduction in
compilation time compared to state-of-the-art compilers, with our AHA gates
providing an additional overhead reduction of up to 20x. These results
establish concatenated codes as a promising approach for fault-tolerant quantum
computing in the near future.

</details>


### [47] [ArchXBench: A Complex Digital Systems Benchmark Suite for LLM Driven RTL Synthesis](https://arxiv.org/abs/2508.06047)
*Suresh Purini,Siddhant Garg,Mudit Gaur,Sankalp Bhat,Sohan Mupparapu,Arun Ravindran*

Main category: cs.AR

TL;DR: ArchXBench是一个六层级的基准测试套件，用于评估LLM在复杂硬件设计中的生成能力。现有LLM在简单电路上表现尚可，但在复杂设计（如多周期或流水线结构）中仍有明显局限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在硬件设计领域的应用仍处于初级阶段，尤其是在复杂电路和高级子系统生成方面的能力尚未充分验证。ArchXBench旨在填补这一空白。

Method: 通过设计一个六级基准测试套件ArchXBench，涵盖从简单到复杂的电路设计任务，并使用多种LLM进行零样本测试，评估其生成能力。

Result: 在零样本测试中，o4-mini-high在30个基准中成功解决了16个（主要来自前三级），但所有模型在更高层级（4级及以上）的任务中都失败了。

Conclusion: 当前LLM在复杂硬件设计领域的生成能力仍有限，尤其是在多周期和流水线设计方面，需要进一步研究改进。

Abstract: Modern SoC datapaths include deeply pipelined, domain-specific accelerators,
but their RTL implementation and verification are still mostly done by hand.
While large language models (LLMs) exhibit advanced code-generation abilities
for programming languages like Python, their application to Verilog-like RTL
remains in its nascent stage. This is reflected in the simple arithmetic and
control circuits currently used to evaluate generative capabilities in existing
benchmarks. In this paper, we introduce ArchXBench, a six-level benchmark suite
that encompasses complex arithmetic circuits and other advanced digital
subsystems drawn from domains such as cryptography, image processing, machine
learning, and signal processing. Architecturally, some of these designs are
purely combinational, others are multi-cycle or pipelined, and many require
hierarchical composition of modules. For each benchmark, we provide a problem
description, design specification, and testbench, enabling rapid research in
the area of LLM-driven agentic approaches for complex digital systems design.
  Using zero-shot prompting with Claude Sonnet 4, GPT 4.1, o4-mini-high, and
DeepSeek R1 under a pass@5 criterion, we observed that o4-mini-high
successfully solves the largest number of benchmarks, 16 out of 30, spanning
Levels 1, 2, and 3. From Level 4 onward, however, all models consistently fail,
highlighting a clear gap in the capabilities of current state-of-the-art LLMs
and prompting/agentic approaches.

</details>


### [48] [Nail: Not Another Fault-Injection Framework for Chisel-generated RTL](https://arxiv.org/abs/2508.06344)
*Robin Sehm,Christian Ewert,Rainer Buchty,Mladen Berekovic,Saleh Mulhem*

Main category: cs.AR

TL;DR: Nail是一种基于Chisel的开源故障注入框架，通过引入状态相关故障，提高了故障建模的精确性，弥补了现有工具在控制和易用性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有Chisel故障注入工具仅支持指令级控制，限制了故障建模的精确性。Nail旨在通过状态相关故障和软件接口解决这一问题。

Method: Nail通过状态依赖故障模型和运行时软件控制接口实现精确故障注入，支持FPGA仿真中的动态参数调整。

Result: Nail在RISC-V处理器上验证了状态相关故障的可行性，资源开销低于1%。

Conclusion: Nail填补了仿真速度和软件控制性之间的鸿沟，为高效故障建模提供了新工具。

Abstract: Fault simulation and emulation are essential techniques for evaluating the
dependability of integrated circuits, enabling early-stage vulnerability
analysis and supporting the implementation of effective mitigation strategies.
High-level hardware description languages such as Chisel facilitate the rapid
development of complex fault scenarios with minimal modification to the design.
However, existing Chisel-based fault injection (FI) frameworks are limited by
coarse-grained, instruction-level controllability, restricting the precision of
fault modeling. This work introduces Nail, a Chisel-based open-source FI
framework that overcomes these limitations by introducing state-based faults.
This approach enables fault scenarios that depend on specific system states,
rather than solely on instruction-level triggers, thereby removing the need for
precise timing of fault activation. For greater controllability, Nail allows
users to arbitrarily modify internal trigger states via software at runtime. To
support this, Nail automatically generates a software interface, offering
straightforward access to the instrumented design. This enables fine-tuning of
fault parameters during active FI campaigns - a feature particularly beneficial
for FPGA emulation, where synthesis is time-consuming. Utilizing these
features, Nail narrows the gap between the high speed of emulation-based FI
frameworks, the usability of software-based approaches, and the controllability
achieved in simulation. We demonstrate Nail's state-based FI and software
framework by modeling a faulty general-purpose register in a RISC-V processor.
Although this might appear straightforward, it requires state-dependent FI and
was previously impossible without fundamental changes to the design. The
approach was validated in both simulation and FPGA emulation, where the
addition of Nail introduced less than 1% resource overhead.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [49] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0是一个先进的视觉语言引导框架，用于生成3D场景并支持基于人反馈的交互式编辑，能生成多样风格的高质量场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D场景生成方法依赖人工且灵活性不足的问题，实现从文本直接生成开放领域场景。

Method: 利用视觉语言模型解析场景对象并生成高质量3D资产，通过空间约束迭代优化布局。

Result: 生成的场景在语义和物理布局上与文本描述高度一致，性能优于基线方法。

Conclusion: HOLODECK 2.0在3D场景生成和编辑方面表现出色，适用于游戏建模等实际应用。

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [50] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net是一种从脑MRI生成个性化3D LV网格的新框架，通过变形联合LV-海马模板网格来提高重建精度和形状统计准确性，应用于阿尔茨海默病分析。


<details>
  <summary>Details</summary>
Motivation: LV形状分析作为神经系统疾病的潜在生物标志物，但个体间形状差异大和MRI分辨率限制导致分割困难。

Method: 使用LV-Net框架，通过变形联合LV-海马模板网格，结合解剖关系改善分割和重建。

Result: LV-Net在重建精度和形状描述符上表现优越，并在阿尔茨海默病分析中识别出显著相关区域。

Conclusion: LV-Net提供了一种稳健的LV形状分析方法，适用于疾病研究和临床应用。

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [51] [The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations](https://arxiv.org/abs/2508.06316)
*Theresa Pollinger,Masado Ishii,Jens Domke*

Main category: cs.DS

TL;DR: 该论文提出了omnitrees作为octrees的各向异性泛化数据结构，通过仅在局部最重要维度细化，显著提高AMR方案的收敛速度，并在3D形状表示中验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 传统octrees在解决各向异性问题时效率低下，因为其强制各向同性细化，导致分辨率浪费。

Method: 提出omnitrees数据结构，允许动态选择细化维度，减少树的深度和宽度，提高收敛速度。

Result: 在3D物体表示中，omnitrees收敛速度提高1.5倍，存储需求更低，信息密度更高。

Conclusion: omnitrees可提升现有AMR方法的效率，并为高维应用提供新可能性。

Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees
and octrees, underpins a wide range of applications including databases,
computer graphics, physics simulations, and machine learning. However, octrees
enforce isotropic refinement in regions of interest, which can be especially
inefficient for problems that are intrinsically anisotropic--much resolution is
spent where little information is gained. This paper presents omnitrees as an
anisotropic generalization of octrees and related data structures. Omnitrees
allow to refine only the locally most important dimensions, providing tree
structures that are less deep than bintrees and less wide than octrees. As a
result, the convergence of the AMR schemes can be increased by up to a factor
of the dimensionality d for very anisotropic problems, quickly offsetting their
modest increase in storage overhead. We validate this finding on the problem of
binary shape representation across 4,166 three-dimensional objects: Omnitrees
increase the mean convergence rate by 1.5x, require less storage to achieve
equivalent error bounds, and maximize the information density of the stored
function faster than octrees. These advantages are projected to be even
stronger for higher-dimensional problems. We provide a first validation by
introducing a time-dependent rotation to create four-dimensional
representations, and discuss the properties of their 4-d octree and omnitree
approximations. Overall, omnitree discretizations can make existing AMR
approaches more efficient, and open up new possibilities for high-dimensional
applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [52] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: 论文呼吁更多关注认知想象力，认为这是人工智能的下一突破点，并提出语义模型作为模拟工具。


<details>
  <summary>Details</summary>
Motivation: 认知想象力在人类思维中起关键作用，但当前AI领域对其重视不足，限制了AI的能力。

Method: 提出语义模型，一种基于概率因果关系的数学模型，用于模拟认知想象力。

Result: 语义模型能确保想象语境的连贯性，实现透明操作。

Conclusion: 认知想象力的研究是AI发展的重要方向，语义模型为模拟提供了可行方案。

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [53] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: 论文提出了一种动态公平机制'Fair Game'，通过结合审计员和去偏算法，利用强化学习适应社会反馈，以实现灵活的公平机器学习系统。


<details>
  <summary>Details</summary>
Motivation: 现有公平机器学习方法通常依赖于观察性定义，这些定义在实际动态社会环境中存在局限，无法灵活适应变化。

Method: 提出'Fair Game'框架，通过强化学习将审计员与去偏算法结合，动态调整公平目标以适应社会反馈。

Result: 'Fair Game'能够模拟社会伦理框架的演变，为部署前和部署后的公平机器学习系统提供灵活性和适应性。

Conclusion: 'Fair Game'为解决公平机器学习在动态社会中的挑战提供了创新的动态机制。

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [54] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: 传统可解释人工智能（XAI）仅关注算法透明度，但难以满足用户实际需求。本文提出“解释性AI”作为补充范式，通过生成式AI提供更贴近人类理解的解释。实证研究表明，用户更偏好情境敏感的多种模态解释。


<details>
  <summary>Details</summary>
Motivation: 传统XAI的解释方式抽象且缺乏适应性，无法有效支持用户的理解和决策，尤其是在社会技术情境中。

Method: 提出一个八维概念模型，结合叙事沟通、自适应个性化和渐进披露原则，并通过快速情境设计方法在医疗领域进行实证验证。

Result: 用户明显偏好情境敏感、多模态的解释，而非技术透明度的展示。

Conclusion: AI系统应更注重人类理解而非算法透明度，推动以用户为中心的解释方法研究。

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [55] [Energy Experience Design](https://arxiv.org/abs/2508.05869)
*Brian Sutherland*

Main category: cs.CY

TL;DR: 论文探讨了ICT系统的材料足迹问题，重点关注电池的可持续能源存储方案，并提出无电池设备原型的设计思路。


<details>
  <summary>Details</summary>
Motivation: ICT系统材料足迹的增长引发了对可持续性和气候正义的关注，尤其是电池的年丢弃量高达150亿块，亟需解决方案。

Method: 作者提出能量体验的定义，并通过设计无电池可持续能源的电子设备原型进行研究。

Result: 展示了无需关键矿物且寿命极长的无电池设备原型，为大规模制造系统提供过渡性设计启示。

Conclusion: 研究表明，无电池可持续能源设备原型为未来大规模系统的设计提供了创新思路和可能性。

Abstract: The material footprint of information and communications technology (ICT)
systems is both significant and growing, inspiring a variety of conversations
around sustainability and climate justice. In part this effort has been
catalysed by past scholarship and analysis from the LIMITS community. This
paper examines energy storage systems for computing, particularly batteries --
which are discarded at the rate of 15 billion a year worldwide. The
International Energy Agency (IEA) is now referring to the energy transition
toward low carbon systems as a critical mineral problem, and countries are
speaking openly of 'mineral security' in policy documents. In this paper I 1)
present a definition for energy experience and what this means for the design
and making of devices, interactions and experiences. I also 2) explore a series
of electronics device prototypes converted to run from batteryless sustainable
energy that are extremely long lasting, and make limited use of critical
minerals. As transitional energy experience device-design experiments, what do
prototypes like these suggest for more mainstream, mass-manufactured systems?

</details>


### [56] [Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems](https://arxiv.org/abs/2508.05846)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.CY

TL;DR: 论文探讨了AI和机器人系统中透明性对伦理行为的重要性，提出透明性有助于问责、知情同意和伦理算法调试，并提出新的提升透明性的方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI和机器人技术在社会中的普及，确保其伦理行为变得至关重要，透明性被认为是构建可信赖系统的关键。

Method: 论文提出了标准化的透明度度量、可解释AI技术和用户友好界面等方法，并结合技术和伦理考量设计框架。

Result: 研究发现透明性能够提升公众信任、影响监管政策，并为未来研究指明方向。

Conclusion: 透明性应作为伦理AI系统设计的核心要素，为责任AI和机器人技术的发展提供重要指导。

Abstract: As artificial intelligence (AI) and robotics increasingly permeate society,
ensuring the ethical behavior of these systems has become paramount. This paper
contends that transparency in AI decision-making processes is fundamental to
developing trustworthy and ethically aligned robotic systems. We explore how
transparency facilitates accountability, enables informed consent, and supports
the debugging of ethical algorithms. The paper outlines technical, ethical, and
practical challenges in implementing transparency and proposes novel approaches
to enhance it, including standardized metrics, explainable AI techniques, and
user-friendly interfaces. This paper introduces a framework that connects
technical implementation with ethical considerations in robotic systems,
focusing on the specific challenges of achieving transparency in dynamic,
real-world contexts. We analyze how prioritizing transparency can impact public
trust, regulatory policies, and avenues for future research. By positioning
transparency as a fundamental element in ethical AI system design, we aim to
add to the ongoing discussion on responsible AI and robotics, providing
direction for future advancements in this vital field.

</details>


### [57] [Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education](https://arxiv.org/abs/2508.05979)
*Xinming Yang,Haasil Pujara,Jun Li*

Main category: cs.CY

TL;DR: 提出一种新教学范式，让学生扮演教师角色教导LLM解决问题，通过设计知识缺口问题提升学生主动学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统LLM作为虚拟导师容易导致学生被动学习和依赖，需找到更有效的方法利用LLM提升学习效果。

Method: 开发策略设计具有知识缺口的问题，构建Socrates系统实现该方法，并在本科课程中评估效果。

Result: 主动学习方法显著提升学生成绩，验证了该框架的实用性和成本效益。

Conclusion: 该范式通过LLM增强学生参与度和掌握度，为教育提供新思路。

Abstract: While Large Language Models (LLMs) are often used as virtual tutors in
computer science (CS) education, this approach can foster passive learning and
over-reliance. This paper presents a novel pedagogical paradigm that inverts
this model: students act as instructors who must teach an LLM to solve
problems. To facilitate this, we developed strategies for designing questions
with engineered knowledge gaps that only a student can bridge, and we introduce
Socrates, a system for deploying this method with minimal overhead. We
evaluated our approach in an undergraduate course and found that this
active-learning method led to statistically significant improvements in student
performance compared to historical cohorts. Our work demonstrates a practical,
cost-effective framework for using LLMs to deepen student engagement and
mastery.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [58] [Social and Telepresence Robots for Accessibility and Inclusion in Small Museums](https://arxiv.org/abs/2508.05946)
*Nello Balossino,Rossana Damiano,Cristina Gena,Alberto Lillo,Anna Maria Marras,Claudio Mattutino,Antonio Pizzo,Alessia Prin,Fabiana Vernero*

Main category: cs.RO

TL;DR: ROBSO-PM项目通过社交机器人和远程社交机器人提升小型博物馆的可访问性，重点关注感知、文化和认知障碍。


<details>
  <summary>Details</summary>
Motivation: 许多博物馆，尤其是人口稀少地区的博物馆，在感知、文化和认知方面存在可访问性障碍，项目旨在解决这一问题。

Method: 项目研究了两个主要应用：机器人作为导游支持包容性访问，以及作为远程访问工具，同时探讨了叙事、机器人个性、共情等研究主题。

Result: 通过三个案例研究（都灵圣裹布博物馆和两个山区博物馆），验证了机器人在提升博物馆可访问性中的潜力。

Conclusion: 机器人技术可以有效提升博物馆的可访问性，尤其是在偏远或小型博物馆中，展示了广阔的应用前景。

Abstract: There are still many museums that present accessibility barriers,
particularly regarding perceptual, cultural, and cognitive aspects. This is
especially evident in low-density population areas. The aim of the ROBSO-PM
project is to improve the accessibility of small museums through the use of
social robots and social telepresence robots, focusing on three museums as case
studies: the Museum of the Holy Shroud in Turin, a small but globally known
institution, and two lesser known mountain museums: the Museum of the Champlas
du Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and
Traditions. The project explores two main applications for robots: as guides
supporting inclusive visits for foreign or disabled visitors, and as
telepresence tools allowing people with limited mobility to access museums
remotely. From a research perspective, key topics include storytelling, robot
personality, empathy, personalization, and, in the case of telepresence,
collaboration between the robot and the person, with clearly defined roles and
autonomy.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [59] [EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition](https://arxiv.org/abs/2508.06321)
*Durjoy Chandra Paul,Gaurob Saha,Md Amjad Hossain*

Main category: cs.SD

TL;DR: 论文提出EmoAugNet，结合数据增强和混合深度学习模型，显著提升语音情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 提高人机交互效能需准确识别语音情感信号。

Method: 使用LSTM和1D-CNN混合框架，结合传统和新型数据增强方法，提取多种语音特征。

Result: 在IEMOCAP和RAVDESS数据集上取得高准确率（约95%）。

Conclusion: EmoAugNet通过数据增强和混合建模，有效提升语音情感识别的鲁棒性和性能。

Abstract: Recognizing emotional signals in speech has a significant impact on enhancing
the effectiveness of human-computer interaction (HCI). This study introduces
EmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term
Memory (LSTM) layers with one-dimensional Convolutional Neural Networks
(1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and
variety of the features that are taken from speech signals have a significant
impact on how well SER systems perform. A comprehensive speech data
augmentation strategy was used to combine both traditional methods, such as
noise addition, pitch shifting, and time stretching, with a novel
combination-based augmentation pipeline to enhance generalization and reduce
overfitting. Each audio sample was transformed into a high-dimensional feature
vector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient
(MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a
weighted accuracy of 95.78\% and unweighted accuracy of 92.52\% on the IEMOCAP
dataset and, with ELU activation, has a weighted accuracy of 96.75\% and
unweighted accuracy of 91.28\%. On the RAVDESS dataset, we get a weighted
accuracy of 94.53\% and 94.98\% unweighted accuracy for ReLU activation and
93.72\% weighted accuracy and 94.64\% unweighted accuracy for ELU activation.
These results highlight EmoAugNet's effectiveness in improving the robustness
and performance of SER systems through integated data augmentation and hybrid
modeling.

</details>


### [60] [Improved Dysarthric Speech to Text Conversion via TTS Personalization](https://arxiv.org/abs/2508.06391)
*Péter Mihajlik,Éva Székely,Piroska Barta,Máté Soma Kádár,Gergely Dobsinszki,László Tóth*

Main category: cs.SD

TL;DR: 本文探讨如何通过个性化TTS生成合成语音，并结合真实数据微调ASR模型，显著降低严重构音障碍语音的识别错误率。


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型在零样本识别构音障碍语音时错误率高，需通过有限真实数据提升性能。

Method: 利用个性化TTS生成不同严重程度的合成构音障碍语音，结合真实数据微调ASR模型。

Result: 字符错误率从36-51%降至7.3%，合成语音贡献18%相对错误率降低。

Conclusion: 个性化ASR系统可显著提升严重构音障碍者的语言可及性。

Abstract: We present a case study on developing a customized speech-to-text system for
a Hungarian speaker with severe dysarthria. State-of-the-art automatic speech
recognition (ASR) models struggle with zero-shot transcription of dysarthric
speech, yielding high error rates. To improve performance with limited real
dysarthric data, we fine-tune an ASR model using synthetic speech generated via
a personalized text-to-speech (TTS) system. We introduce a method for
generating synthetic dysarthric speech with controlled severity by leveraging
premorbidity recordings of the given speaker and speaker embedding
interpolation, enabling ASR fine-tuning on a continuum of impairments.
Fine-tuning on both real and synthetic dysarthric speech reduces the character
error rate (CER) from 36-51% (zero-shot) to 7.3%. Our monolingual
FastConformer_Hu ASR model significantly outperforms Whisper-turbo when
fine-tuned on the same data, and the inclusion of synthetic speech contributes
to an 18% relative CER reduction. These results highlight the potential of
personalized ASR systems for improving accessibility for individuals with
severe speech impairments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [61] [MPS-JuliQAOA: User-friendly, Scalable MPS-based Simulation for Quantum Optimization](https://arxiv.org/abs/2508.05883)
*Sean Feeney,Reuben Tate,John Golden,Stephan Eidenbenz*

Main category: quant-ph

TL;DR: MPS-JuliQAOA是一个开源的、用户友好的工具，用于模拟QAOA算法，支持512量子比特和20轮模拟。


<details>
  <summary>Details</summary>
Motivation: 为了解决QAOA算法在模拟中的可扩展性和易用性问题。

Method: 利用Julia语言和ITensor包实现MPS方法，无需用户了解MPS原理或复杂自动微分技术。

Result: 工具可扩展到512量子比特和20轮模拟，并具有内置参数优化能力。

Conclusion: MPS-JuliQAOA是一个高效的QAOA模拟工具，适合非专业用户使用。

Abstract: We present the MPS-JuliQAOA simulator, a user-friendly, open-source tool to
simulate the Quantum Approximate Optimization Algorithm (QAOA) of any
optimization problem that can be expressed as diagonal Hamiltonian. By
leveraging Julia-language constructs and the ITensor package to implement a
Matrix Product State (MPS) approach to simulating QAOA, MPS-Juli-QAOA
effortlessly scales to 512 qubits and 20 simulation rounds on the standard
de-facto benchmark 3-regular MaxCut QAOA problem. MPS-JuliQAOA also has
built-in parameter finding capabilities, which is a crucial performance aspect
of QAOA. We illustrate through examples that the user does not need to know MPS
principles or complex automatic differentiation techniques to use MPS-JuliQAOA.
We study the scalability of our tool with respect to runtime, memory usage and
accuracy tradeoffs. Code available at
https://github.com/lanl/JuliQAOA.jl/tree/mps.

</details>


### [62] [Quantum Resource Management in the NISQ Era: Implications and Perspectives from Software Engineering](https://arxiv.org/abs/2508.05697)
*Marcos Guillermo Lammers,Federico Hernán Holik,Alejandro Fernández*

Main category: quant-ph

TL;DR: 论文探讨了NISQ时代量子资源管理的重要性及其对量子软件工程的影响。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在解决复杂问题方面具有潜力，但当前的NISQ设备存在资源限制和高错误率问题，需优化资源管理。

Method: 分析了NISQ设备中量子资源的作用及其影响。

Result: 强调了量子资源估计（QRE）的重要性，以推动可扩展且可靠的量子软件开发。

Conclusion: 通过优化量子资源管理，可以促进量子软件工程的进一步发展。

Abstract: Quantum computers represent a radical technological breakthrough in
information processing by leveraging the principles of quantum mechanics to
solve highly complex problems beyond the reach of classical systems. However,
in the current NISQ era (noisy intermediate-scale quantum devices), the
available hardware presents several limitations, such as a limited number of
qubits, high error rates, and short coherence times. Efficient management of
quantum resources, both physical and logical, is especially relevant in the
design and deployment of quantum algorithms. In this paper, we analyze the role
of resources in current uses of NISQ devices, identifying their relevance and
implications for quantum software engineering. With this contribution, we aim
to strengthen the field of Quantum Resource Estimation (QRE) and move toward
scalable and reliable quantum software development

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [63] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: 论文提出DynamicTRF框架，通过动态选择适合的图表示形式提升多模态模型在零样本图问答任务中的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用单一图表示形式，忽略了不同模型或任务的偏好，导致结果不准确或冗长。

Method: 设计了一套零样本图问答专用的图表示形式，提出GRE指标衡量性能与简洁性平衡，并开发DynamicTRF框架动态选择最佳表示形式。

Result: 在7个领域内算法图问答任务和2个领域外任务中，DynamicTRF显著提升了零样本图问答的准确性。

Conclusion: DynamicTRF通过动态选择图表示形式，有效解决了现有多模态模型在零样本图问答中的问题。

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [64] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*Vít Gvoždiak*

Main category: cs.CL

TL;DR: 本文重新构想语用学，将其视为语言作为社会嵌入行动工具的动态界面，而非意义的第三维度。结合大型语言模型（LLM）的崛起，作者挑战传统语义学三分法，提出人机通信（HMC）框架，并探讨人类中心与机器中心语用学的冲突。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在通信中的普及，传统语用学的局限性显现，需要适应AI通信的新范式。

Method: 通过分析LLM对传统语义学的影响，提出HMC框架，并探讨概率语用学（如理性言语行为框架）的适用性。

Result: 揭示了LLM评估中的人类中心偏差，并提出“语境挫折”概念，描述用户与模型共同构建语用条件的现象。

Conclusion: 语用学理论需调整以涵盖生成式AI参与的通信，强调优化而非真值评估的新范式。

Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [65] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 论文提出了一种针对大型语言模型（LLM）的情感智力（EI）四层分类法，并设计了EICAP-Bench基准评测。实验表明，当前预训练和指令调优方法在提升LLM情感推理能力上效果有限，仅“评估”层通过微调有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在情感智力（EI）维度上的未充分探索问题，推动人机对齐发展。

Method: 提出四层EI分类法（情感追踪、因果推断、评估、情感响应生成），构建EICAP-Bench评测基准，对六种LLM进行评测，并通过微调实验验证改进潜力。

Result: Qwen2.5-Instruct表现最佳；微调实验中，仅“评估”层能力显著提升。

Conclusion: 现有方法在深度情感推理上存在局限，需针对性数据与建模策略以实现全面EI对齐。

Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [66] [Numerical Considerations in Weighted Model Counting](https://arxiv.org/abs/2508.06264)
*Randal E. Bryant*

Main category: math.NA

TL;DR: 该论文提出了一种结合多种数值表示的方法，旨在高效计算加权模型计数，并确保达到用户指定的精度。通过使用扩展范围双精度（ERD）和区间浮点算术等技术，解决了浮点算术精度损失和有理算术成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 加权模型计数在概率推理和定量风险评估等多个领域有广泛应用。传统的浮点算术可能不准确且有精度问题，而有理算术虽然精确但成本高。本文旨在解决这些技术限制。

Method: 论文提出使用扩展范围双精度（ERD）和区间浮点算术结合的方法，用于处理非负权重和混合权重问题，同时保证了效率和精度。

Result: 实验结果表明，该方法在极具挑战性的公式和权重分配下表现出良好的鲁棒性，能够高效计算加权模型计数并保证精度。

Conclusion: 通过结合多种数值表示，本文成功解决了加权模型计数中的精度和效率问题，为实际应用提供了可靠的技术支持。

Abstract: Weighted model counting computes the sum of the rational-valued weights
associated with the satisfying assignments for a Boolean formula, where the
weight of an assignment is given by the product of the weights assigned to the
positive and negated variables comprising the assignment. Weighted model
counting finds applications across a variety of domains including probabilistic
reasoning and quantitative risk assessment.
  Most weighted model counting programs operate by (explicitly or implicitly)
converting the input formula into a form that enables arithmetic evaluation,
using multiplication for conjunctions and addition for disjunctions. Performing
this evaluation using floating-point arithmetic can yield inaccurate results,
and it cannot quantify the level of precision achieved. Computing with rational
arithmetic gives exact results, but it is costly in both time and space.
  This paper describes how to combine multiple numeric representations to
efficiently compute weighted model counts that are guaranteed to achieve a
user-specified precision. When all weights are nonnegative, we prove that the
precision loss of arithmetic evaluation using floating-point arithmetic can be
tightly bounded. We show that supplementing a standard IEEE double-precision
representation with a separate 64-bit exponent, a format we call extended-range
double (ERD), avoids the underflow and overflow issues commonly encountered in
weighted model counting. For problems with mixed negative and positive weights,
we show that a combination of interval floating-point arithmetic and rational
arithmetic can achieve the twin goals of efficiency and guaranteed precision.
For our evaluations, we have devised especially challenging formulas and weight
assignments, demonstrating the robustness of our approach.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [67] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 该论文提出了一种基于马尔可夫决策过程（MDP）和强化学习策略梯度（RL-PG）的框架，用于训练自主导航策略以减少碰撞规避机动（CAM）的平均燃料消耗，并保持可接受的碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 通过早期决策优化CAM的燃料消耗，同时确保碰撞风险可控。

Method: 将CAM建模为连续状态、离散动作和有限时间范围的MDP，结合风险分析、燃料消耗和轨道几何模型，使用RL-PG算法训练策略。

Result: 在合成和历史数据上，训练的策略显著降低了平均燃料消耗，同时保持了与常规策略相当的碰撞风险保证。

Conclusion: 该方法通过早期决策有效平衡了燃料消耗和风险控制，优于传统的固定时间阈值策略。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [68] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF是一种新型的联邦元学习方法，通过隐私保护损失函数解决传统方法的隐私泄露问题，实现高效优化和数据隐私保护。


<details>
  <summary>Details</summary>
Motivation: 神经场在资源受限的边缘设备上学习时需要大量数据和计算，且传统FML方法存在隐私泄露问题。

Method: FedMeNF利用隐私保护损失函数，在本地元优化中调节隐私泄露。

Result: 实验表明，FedMeNF在少样本或非IID数据下仍能快速优化并保持重建性能。

Conclusion: FedMeNF解决了隐私泄露问题，且在多样数据模态下表现优异。

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [69] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP框架通过粗到细的剪枝方法压缩CoT，提升推理效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决长推理链带来的训练成本、延迟和部署难题。

Method: 采用锚点引导剪枝和基于surprisal的细粒度剪枝，确保逻辑连贯性。

Result: 在代码生成任务中显著减少推理延迟和token，同时保持高准确率。

Conclusion: ASAP为构建高效强大的大型推理模型提供了可行方向。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [70] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD是一种无监督、多智能体强化学习框架，无需预训练伙伴或手动调参，通过随机混合策略和基于方差的度量生成多样伙伴，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要预训练伙伴或手动调参的问题，提出一种完全无监督的伙伴生成框架，提升协作任务的适应性。

Method: 通过随机混合策略生成伙伴，并用基于方差的度量评估其可学习性，结合无监督环境设计生成动态课程。

Result: 在Overcooked环境中，UPD显著优于基线方法，用户研究显示其适应性、协作性更强。

Conclusion: UPD为协作任务提供了一种高效、无监督的伙伴生成方法，具有广泛的应用潜力。

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [71] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: U3-Attack是一种多模态越狱攻击方法，针对文本到图像（T2I）模型的安全保护措施，通过优化图像背景的对抗补丁和敏感词的安全改写集，实现了更高的成功率和更好的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态越狱攻击方法局限于特定提示和图像的扰动，缺乏可扩展性且优化耗时。为了克服这些限制，提出了U3-Attack。

Method: U3-Attack优化图像背景的对抗补丁以绕过安全检查器，同时优化敏感词的安全改写集以绕过提示过滤器，减少冗余计算。

Result: 实验表明，U3-Attack在开源和商业T2I模型上表现优异，例如在Runway-inpainting模型上成功率比现有方法高4倍。

Conclusion: U3-Attack有效解决了现有方法的局限性，为多模态越狱攻击提供了更高效的解决方案。

Abstract: Various (text) prompt filters and (image) safety checkers have been
implemented to mitigate the misuse of Text-to-Image (T2I) models in creating
Not-Safe-For-Work (NSFW) content.In order to expose potential security
vulnerabilities of such safeguards, multimodal jailbreaks have been
studied.However, existing jailbreaks are limited to prompt-specific and
image-specific perturbations, which suffer from poor scalability and
time-consuming optimization.To address these limitations, we propose
Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack
method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial
patch on the image background to universally bypass safety checkers and
optimizes a safe paraphrase set from a sensitive word to universally bypass
prompt filters while eliminating redundant computations.Extensive experimental
results demonstrate the superiority of our U3-Attack on both open-source and
commercial T2I models.For example, on the commercial Runway-inpainting model
with both prompt filter and safety checker, our U3-Attack achieves $~4\times$
higher success rates than the state-of-the-art multimodal jailbreak attack,
MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [72] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: 本文提出了一种基于DistilBERT的两层异常检测方法，结合无监督和监督机器学习技术，有效识别SQL数据库中的异常行为，保护关键数据免受内部和外部攻击。


<details>
  <summary>Details</summary>
Motivation: 数据库系统广泛用于存储关键数据，但异常访问行为（如内部和外部攻击）日益增多。现有方法缺乏细粒度检测能力，常误判或漏判异常行为。

Method: 采用两层异常检测方法：1) 无监督方法使用集成异常检测器标记偏离正常用户行为的查询（范围外查询）；2) 监督方法通过微调的Transformer模型（如DistilBERT）高精度检测内部攻击（范围内查询），即使标注数据有限。

Result: 该方法显著提高了异常检测的准确性，同时减少了对数据标注的依赖，为保护关键数据库系统提供了有效解决方案。

Conclusion: 本文提出的方法通过结合无监督和监督技术，能够有效应对数据库系统中的复杂威胁，尤其适用于内部攻击检测。

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [73] [Blockchain-Based Decentralized Domain Name System](https://arxiv.org/abs/2508.05655)
*Guang Yang,Peter Trinh,Alma Nkemla,Amuru Serikyaku,Edward Tatchim,Osman Sharaf*

Main category: cs.CR

TL;DR: 本文提出了一种基于区块链的去中心化域名系统（DDNS），以解决传统DNS安全漏洞和中心化问题，采用PoW区块链和IPFS存储，实现了快速传播和抗操纵性。


<details>
  <summary>Details</summary>
Motivation: 传统DNS存在安全漏洞、审查机制和中心化问题，威胁互联网自由和安全，亟需一种去中心化解决方案。

Method: 设计了一种支持DNS协议的PoW区块链，结合IPFS分布式存储，采用加密原语实现零信任验证。

Result: 系统实现了15秒域名记录传播、支持20种标准DNS记录类型，性能可达1111.1 tx/s（最小事务）和266.7 tx/s（常规事务）。

Conclusion: DDNS展示了去中心化DNS的可行性和高性能特性，能有效抵抗传统DNS操纵技术。

Abstract: The current Domain Name System (DNS) infrastructure faces critical
vulnerabilities including poisoning attacks, censorship mechanisms, and
centralized points of failure that compromise internet freedom and security.
Recent incidents such as DNS poisoning attacks on ISP customers highlight the
urgent need for resilient alternatives. This paper presents a novel
blockchain-based Decentralized Domain Name System (DDNS). We designed a
specialized Proof-of-Work blockchain to maximize support for DNS-related
protocols and achieve node decentralization. The system integrates our
blockchain with IPFS for distributed storage, implements cryptographic
primitives for end-to-end trust signatures, and achieves Never Trust, Always
Verify zero-trust verification. Our implementation achieves 15-second domain
record propagation times, supports 20 standard DNS record types, and provides
perpetual free .ddns domains. The system has been deployed across distributed
infrastructure in San Jose, Los Angeles, and Orange County, demonstrating
practical scalability and resistance to traditional DNS manipulation
techniques. Performance evaluation shows the system can handle up to Max Theor.
TPS 1,111.1 tx/s (minimal transactions) and Max Theor. TPS 266.7 tx/s (regular
transactions) for domain operations while maintaining sub-second query
resolution through intelligent caching mechanisms.

</details>


### [74] [Voting-Based Semi-Parallel Proof-of-Work Protocol](https://arxiv.org/abs/2508.06489)
*Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

TL;DR: 该论文分析了现有并行PoW协议的安全性问题，提出了一种基于投票的半并行PoW协议，优于现有协议和Nakamoto共识。


<details>
  <summary>Details</summary>
Motivation: 提高Nakamoto共识的安全性、交易吞吐量和确认延迟。

Method: 提出了一种投票式半并行PoW协议，并通过理论分析和模拟验证其性能。

Result: 新协议在通信开销、吞吐量、激励兼容性等方面优于现有协议。

Conclusion: 半并行PoW协议更安全且高效，为区块链共识提供了新的解决方案。

Abstract: Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety
guarantees, transaction throughput and confirmation latencies of Nakamoto
consensus. In this work, we first consider the existing parallel PoW protocols
and develop hard-coded incentive attack structures. Our theoretical results and
simulations show that the existing parallel PoW protocols are more vulnerable
to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller
profitability threshold and they result in higher relative rewards. Next, we
introduce a voting-based semi-parallel PoW protocol that outperforms both
Nakamoto consensus and the existing parallel PoW protocols from most practical
perspectives such as communication overheads, throughput, transaction
conflicts, incentive compatibility of the protocol as well as a fair
distribution of transaction fees among the voters and the leaders. We use
state-of-the-art analysis to evaluate the consistency of the protocol and
consider Markov decision process (MDP) models to substantiate our claims about
the resilience of our protocol against incentive attacks.

</details>


### [75] [Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models](https://arxiv.org/abs/2508.05865)
*Kiana Kiashemshaki,Elvis Nnaemeka Chukwuani,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.CR

TL;DR: 该论文提出一个比较框架，分析了基于区块链的电子投票系统的架构、共识机制和加密协议，旨在解决当前系统在可扩展性、计算需求和隐私方面的挑战，并提出优化策略。


<details>
  <summary>Details</summary>
Motivation: 区块链技术可以提高电子投票系统的透明度、去中心化和安全性，但实际应用中仍面临可扩展性、计算复杂性和隐私要求等限制。论文旨在探索如何优化这些系统，以适应国家规模部署的需求。

Method: 论文采用比较框架，分析了已有的区块链电子投票系统架构、共识机制（如PoW、PoS和DPoS）和加密协议，并提出优化的混合共识、轻量级加密和去中心化身份管理等策略。同时探索了大型语言模型（LLMs）在智能合约生成、异常检测和用户交互中的新作用。

Result: 研究发现，通过优化共识机制和加密协议，结合LLMs的应用，可以设计出更安全、可扩展且智能的区块链电子投票系统，为国家规模部署提供基础。

Conclusion: 论文为开发端到端的区块链电子投票原型提供了基础，结合LLM引导的智能合约生成和验证，并通过系统框架和仿真分析支持，具有重要的实践意义。

Abstract: Blockchain technology offers a promising foundation for modernizing E-Voting
systems by enhancing transparency, decentralization, and security. Yet,
real-world adoption remains limited due to persistent challenges such as
scalability constraints, high computational demands, and complex privacy
requirements. This paper presents a comparative framework for analyzing
blockchain-based E-Voting architectures, consensus mechanisms, and
cryptographic protocols. We examine the limitations of prevalent models like
Proof of Work, Proof of Stake, and Delegated Proof of Stake, and propose
optimization strategies that include hybrid consensus, lightweight
cryptography, and decentralized identity management. Additionally, we explore
the novel role of Large Language Models (LLMs) in smart contract generation,
anomaly detection, and user interaction. Our findings offer a foundation for
designing secure, scalable, and intelligent blockchain-based E-Voting systems
suitable for national-scale deployment. This work lays the groundwork for
building an end-to-end blockchain E-Voting prototype enhanced by LLM-guided
smart contract generation and validation, supported by a systematic framework
and simulation-based analysis.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [76] [Semantic Item Graph Enhancement for Multimodal Recommendation](https://arxiv.org/abs/2508.06154)
*Xiaoxiong Zhang,Xin Zhou,Zhiwei Zeng,Dusit Niyato,Zhiqi Shen*

Main category: cs.IR

TL;DR: 提出了一种新的多模态推荐系统框架，通过增强语义建模和对比学习来解决现有方法的语义不足和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在构建模态特定的项目语义图时存在语义不足和噪声问题，影响了性能提升。

Method: 通过协作信号注入、模数扰动机制和双重表示对齐机制，增强语义建模和噪声鲁棒性。

Result: 在四个基准数据集上的实验验证了框架的有效性。

Conclusion: 该方法显著提高了推荐的准确性，适用于复杂场景。

Abstract: Multimodal recommendation systems have attracted increasing attention for
their improved performance by leveraging items' multimodal information. Prior
methods often build modality-specific item-item semantic graphs from raw
modality features and use them as supplementary structures alongside the
user-item interaction graph to enhance user preference learning. However, these
semantic graphs suffer from semantic deficiencies, including (1) insufficient
modeling of collaborative signals among items and (2) structural distortions
introduced by noise in raw modality features, ultimately compromising
performance. To address these issues, we first extract collaborative signals
from the interaction graph and infuse them into each modality-specific item
semantic graph to enhance semantic modeling. Then, we design a modulus-based
personalized embedding perturbation mechanism that injects perturbations with
modulus-guided personalized intensity into embeddings to generate contrastive
views. This enables the model to learn noise-robust representations through
contrastive learning, thereby reducing the effect of structural noise in
semantic graphs. Besides, we propose a dual representation alignment mechanism
that first aligns multiple semantic representations via a designed Anchor-based
InfoNCE loss using behavior representations as anchors, and then aligns
behavior representations with the fused semantics by standard InfoNCE, to
ensure representation consistency. Extensive experiments on four benchmark
datasets validate the effectiveness of our framework.

</details>
