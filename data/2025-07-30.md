<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.LO](#cs.LO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 26]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.AI](#cs.AI) [Total: 9]
- [nlin.AO](#nlin.AO) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Generating Highly Structured Test Inputs Leveraging Constraint-Guided Graph Refinement](https://arxiv.org/abs/2507.21271)
*Zhaorui Yang,Yuxin Qiu,Haichao Zhu,Qian Zhang*

Main category: cs.SE

TL;DR: 该论文提出了一种基于图的统一测试输入生成框架GRAphRef，以解决结构化数据处理中的输入有效性和语义保留问题。


<details>
  <summary>Details</summary>
Motivation: 现代AI应用处理高度结构化数据（如3D网格和点云），但现有工具生成的输入常无效且效率低，需要一种通用且高效的解决方案。

Method: GRAphRef将结构化输入映射为图，基于邻域相似性进行变异，并通过约束细化修复无效输入，在8个真实AI系统上验证其效果。

Result: 框架在输入有效性和语义保留方面表现出色，对比AFL等工具具有优势，实验数据来自ShapeNetCore和多个模型输出。

Conclusion: GRAphRef为结构化数据测试输入提供了一种统一且高效的方法，具有广泛的适用性和实际价值。

Abstract: [Context] Modern AI applications increasingly process highly structured data,
such as 3D meshes and point clouds, where test input generation must preserve
both structural and semantic validity. However, existing fuzzing tools and
input generators are typically handcrafted for specific input types and often
generate invalid inputs that are subsequently discarded, leading to
inefficiency and poor generalizability. [Objective] This study investigates
whether test inputs for structured domains can be unified through a graph-based
representation, enabling general, reusable mutation strategies while enforcing
structural constraints. We will evaluate the effectiveness of this approach in
enhancing input validity and semantic preservation across eight AI systems.
[Method] We develop and evaluate GRAphRef, a graph-based test input generation
framework that supports constraint-based mutation and refinement. GRAphRef maps
structured inputs to graphs, applies neighbor-similarity-guided mutations, and
uses a constraint-refinement phase to repair invalid inputs. We will conduct a
confirmatory study across eight real-world mesh-processing AI systems,
comparing GRAphRef with AFL, MeshAttack, Saffron, and two ablated variants.
Evaluation metrics include structural validity, semantic preservation (via
prediction consistency), and performance overhead. Experimental data is derived
from ShapeNetCore mesh seeds and model outputs from systems like MeshCNN and
HodgeNet. Statistical analysis and component latency breakdowns will be used to
assess each hypothesis.

</details>


### [2] ["Maybe We Need Some More Examples:" Individual and Team Drivers of Developer GenAI Tool Use](https://arxiv.org/abs/2507.21280)
*Courtney Miller,Rudrajit Choudhuri,Mara Ulloa,Sankeerti Haniyur,Robert DeLine,Margaret-Anne Storey,Emerson Murphy-Hill,Christian Bird,Jenna L. Butler*

Main category: cs.SE

TL;DR: 研究通过54名开发者的访谈发现，对生成式AI工具的采纳不均主要源于开发者对其认知（合作者vs.功能）、使用方式（实验性vs.保守）和挑战应对（适应性坚持vs.快速放弃），组织期望快速提升生产力但缺乏学习支持会形成“生产力压力悖论”。


<details>
  <summary>Details</summary>
Motivation: 由于生成式AI工具在开发中的采纳不均，导致生产力提升受阻，管理期望受挫，开发者未来角色不确定性增加，需探究原因。

Method: 通过27个团队的54名开发者配对访谈（每队一名频繁和一名不频繁用户），分析使用差异的根源。

Result: 使用差异主要源于开发者对工具的认知、使用方式和挑战应对策略的不同；组织期望过高但学习支持不足反而阻碍生产力提升。

Conclusion: 组织需平衡生产力期望与学习支持，避免“生产力压力悖论”，才能真正实现生成式AI工具的效益。

Abstract: Despite the widespread availability of generative AI tools in software
engineering, developer adoption remains uneven. This unevenness is problematic
because it hampers productivity efforts, frustrates management's expectations,
and creates uncertainty around the future roles of developers. Through paired
interviews with 54 developers across 27 teams -- one frequent and one
infrequent user per team -- we demonstrate that differences in usage result
primarily from how developers perceive the tool (as a collaborator vs.
feature), their engagement approach (experimental vs. conservative), and how
they respond when encountering challenges (with adaptive persistence vs. quick
abandonment). Our findings imply that widespread organizational expectations
for rapid productivity gains without sufficient investment in learning support
creates a "Productivity Pressure Paradox," undermining the very productivity
benefits that motivate adoption.

</details>


### [3] [Black-Box Bug-Amplification for Multithreaded Software](https://arxiv.org/abs/2507.21318)
*Yeshayahu Weiss,Gal Amram,Achiya Elyasaf,Eitan Farchi,Oded Margalit,Gera Weiss*

Main category: cs.SE

TL;DR: 本文提出了一种通过训练预测模型来系统性地放大并发系统中罕见错误的方法，显著提高了错误复现率。


<details>
  <summary>Details</summary>
Motivation: 并发系统中的错误往往难以复现，因为它们仅在特定条件下出现，本文旨在解决这一问题。

Method: 将系统视为黑盒，通过重复试验训练预测模型，评估多种搜索技术。

Result: 回归模型集成能显著提高错误复现率，比随机采样效果高一个数量级。

Conclusion: 该方法为放大并发错误提供了一种实用且非侵入性的框架。

Abstract: Bugs, especially those in concurrent systems, are often hard to reproduce
because they manifest only under rare conditions. Testers frequently encounter
failures that occur only under specific inputs, even when occurring with low
probability. We propose an approach to systematically amplify the occurrence of
such elusive bugs. We treat the system under test as a black-box and use
repeated trial executions to train a predictive model that estimates the
probability of a given input configuration triggering a bug. We evaluate this
approach on a dataset of 17 representative concurrency bugs spanning diverse
categories. Several model-based search techniques are compared against a
brute-force random sampling baseline. Our results show that an ensemble of
regression models can significantly increase bug occurrence rates across nearly
all scenarios, often achieving an order-of-magnitude improvement over random
sampling. The contributions of this work include: (i) a novel formulation of
bug-amplification as a rare-event regression problem; (ii) an empirical
evaluation of multiple techniques for amplifying bug occurrence, demonstrating
the effectiveness of model-guided search; and (iii) a practical, non-invasive
testing framework that helps practitioners expose hidden concurrency faults
without altering the internal system architecture.

</details>


### [4] [Does Editing Improve Answer Quality on Stack Overflow? A Data-Driven Investigation](https://arxiv.org/abs/2507.21329)
*Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 研究发现Stack Overflow上的协同编辑在提升回答质量方面效果不一，既有积极也有消极影响。


<details>
  <summary>Details</summary>
Motivation: 技术问答平台的高质量回答对软件开发实践至关重要，但此前研究未系统评估编辑是否提升关键质量维度。

Method: 分析了94,994条Python相关答案的编辑效果，评估了语义相关性、代码可用性等六个质量维度。

Result: 53.3%的编辑提升了语义相关性，但38.1%降低了；部分编辑导致代码复杂度增加或安全性问题，仅51.0%优化了性能。

Conclusion: 编辑结果不一致，提醒用户和平台注意其对软件维护性、安全性和效率的潜在影响。

Abstract: High-quality answers in technical Q&A platforms like Stack Overflow (SO) are
crucial as they directly influence software development practices. Poor-quality
answers can introduce inefficiencies, bugs, and security vulnerabilities, and
thus increase maintenance costs and technical debt in production software. To
improve content quality, SO allows collaborative editing, where users revise
answers to enhance clarity, correctness, and formatting. Several studies have
examined rejected edits and identified the causes of rejection. However, prior
research has not systematically assessed whether accepted edits enhance key
quality dimensions. While one study investigated the impact of edits on C/C++
vulnerabilities, broader quality aspects remain unexplored. In this study, we
analyze 94,994 Python-related answers that have at least one accepted edit to
determine whether edits improve (1) semantic relevance, (2) code usability, (3)
code complexity, (4) security vulnerabilities, (5) code optimization, and (6)
readability. Our findings show both positive and negative effects of edits.
While 53.3% of edits improve how well answers match questions, 38.1% make them
less relevant. Some previously broken code (9%) becomes executable, yet working
code (14.7%) turns non-parsable after edits. Many edits increase complexity
(32.3%), making code harder to maintain. Instead of fixing security issues,
20.5% of edits introduce additional issues. Even though 51.0% of edits optimize
performance, execution time still increases overall. Readability also suffers,
as 49.7% of edits make code harder to read. This study highlights the
inconsistencies in editing outcomes and provides insights into how edits impact
software maintainability, security, and efficiency that might caution users and
moderators and help future improvements in collaborative editing systems.

</details>


### [5] [MAAD: Automate Software Architecture Design through Knowledge-Driven Multi-Agent Collaboration](https://arxiv.org/abs/2507.21382)
*Ruiyin Li,Yiran Zhang,Xiyu Zhou,Peng Liang,Weisong Sun,Jifeng Xuan,Zhi Jin,Yang Liu*

Main category: cs.SE

TL;DR: MAAD是一个基于多智能体系统（MAS）的自动化架构设计框架，通过协作生成架构蓝图和质量评估报告，优于现有基线MetaGPT，并在工业实践中得到验证。GPT-4o在架构设计中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 软件架构设计复杂且依赖专家经验，传统方法效率低且设计选择有限。LLM代理的应用尚未充分探索，尤其是在复杂决策和多领域知识场景下。

Method: 提出MAAD框架，包含四个专业代理（分析师、建模师、设计师、评估师），协作处理需求并生成架构蓝图及评估报告。

Result: MAAD在生成架构组件和评估报告方面优于MetaGPT，工业反馈证实其实用性。GPT-4o在三种LLM中表现最佳。

Conclusion: MAAD为自动化架构设计提供了高效解决方案，LLM选择对MAS驱动的设计至关重要。

Abstract: Software architecture design is a critical, yet inherently complex and
knowledge-intensive phase of software development. It requires deep domain
expertise, development experience, architectural knowledge, careful trade-offs
among competing quality attributes, and the ability to adapt to evolving
requirements. Traditionally, this process is time-consuming and
labor-intensive, and relies heavily on architects, often resulting in limited
design alternatives, especially under the pressures of agile development. While
Large Language Model (LLM)-based agents have shown promising performance across
various SE tasks, their application to architecture design remains relatively
scarce and requires more exploration, particularly in light of diverse domain
knowledge and complex decision-making. To address the challenges, we proposed
MAAD (Multi-Agent Architecture Design), an automated framework that employs a
knowledge-driven Multi-Agent System (MAS) for architecture design. MAAD
orchestrates four specialized agents (i.e., Analyst, Modeler, Designer and
Evaluator) to collaboratively interpret requirements specifications and produce
architectural blueprints enriched with quality attributes-based evaluation
reports. We then evaluated MAAD through a case study and comparative
experiments against MetaGPT, a state-of-the-art MAS baseline. Our results show
that MAAD's superiority lies in generating comprehensive architectural
components and delivering insightful and structured architecture evaluation
reports. Feedback from industrial architects across 11 requirements
specifications further reinforces MAAD's practical usability. We finally
explored the performance of the MAAD framework with three LLMs (GPT-4o,
DeepSeek-R1, and Llama 3.3) and found that GPT-4o exhibits better performance
in producing architecture design, emphasizing the importance of LLM selection
in MAS-driven architecture design.

</details>


### [6] [LLM4VV: Evaluating Cutting-Edge LLMs for Generation and Evaluation of Directive-Based Parallel Programming Model Compiler Tests](https://arxiv.org/abs/2507.21447)
*Zachariah Sollenberger,Rahul Patel,Saieda Ali Zada,Sunita Chandrasekaran*

Main category: cs.SE

TL;DR: 摘要讨论了LLM在软件和测试开发中的应用，提出了双LLM系统（生成和判别）以解决LLM生成代码的正确性验证问题，并通过实验展示了LLM在生成高质量编译器测试方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件和测试开发中的广泛应用，验证其生成代码的正确性变得至关重要。然而，目前缺乏全面且完全自主的解决方案，同时LLM的幻觉问题和逻辑推理不透明性也增加了对其结果的信任问题。

Method: 为了解决这些问题，论文提出了双LLM系统（一个生成LLM和一个判别LLM），并实验了LLM在生成大量编译器测试中的应用。实验使用了不同参数规模的LLM，并通过十个精心选择的指标进行结果评估。

Result: 实验结果表明，LLM在生成高质量编译器测试并自动验证方面表现出潜力。

Conclusion: LLM双系统设计为解决代码验证问题提供了有效方法，并在编译器测试生成中展示了实际应用前景。

Abstract: The usage of Large Language Models (LLMs) for software and test development
has continued to increase since LLMs were first introduced, but only recently
have the expectations of LLMs become more realistic. Verifying the correctness
of code generated by LLMs is key to improving their usefulness, but there have
been no comprehensive and fully autonomous solutions developed yet.
Hallucinations are a major concern when LLMs are applied blindly to problems
without taking the time and effort to verify their outputs, and an inability to
explain the logical reasoning of LLMs leads to issues with trusting their
results. To address these challenges while also aiming to effectively apply
LLMs, this paper proposes a dual-LLM system (i.e. a generative LLM and a
discriminative LLM) and experiments with the usage of LLMs for the generation
of a large volume of compiler tests. We experimented with a number of LLMs
possessing varying parameter counts and presented results using ten
carefully-chosen metrics that we describe in detail in our narrative. Through
our findings, it is evident that LLMs possess the promising potential to
generate quality compiler tests and verify them automatically.

</details>


### [7] [HLSDebugger: Identification and Correction of Logic Bugs in HLS Code with LLM Solutions](https://arxiv.org/abs/2507.21485)
*Jing Wang,Shang Liu,Yao Lu,Zhiyao Xie*

Main category: cs.SE

TL;DR: 论文提出了一种名为HLSDebugger的解决方案，用于解决高级综合（HLS）代码调试的挑战，通过生成大型标记数据集和采用编码器-解码器结构的模型，显著提升了HLS逻辑错误的识别和修正能力。


<details>
  <summary>Details</summary>
Motivation: 高级综合（HLS）代码调试对缺乏硬件知识的初学者和软件工程师来说非常困难，而当前的LLM在解决HLS调试问题时面临数据稀缺、逻辑复杂性高和缺乏可靠测试用例等挑战。

Method: HLSDebugger通过生成包含30万样本的标记数据集，并采用编码器-解码器结构的模型，同时完成错误定位、类型预测和修正任务。

Result: HLSDebugger在错误识别和修正方面的性能显著优于GPT-4等先进LLM，尤其在错误修正方面超出3倍以上。

Conclusion: HLSDebugger为HLS代码的自动化调试提供了重大突破，解决了数据稀缺和多任务处理的挑战。

Abstract: High-level synthesis (HLS) accelerates hardware design by enabling the
automatic translation of high-level descriptions into efficient hardware
implementations. However, debugging HLS code is a challenging and
labor-intensive task, especially for novice circuit designers or software
engineers without sufficient hardware domain knowledge. The recent emergence of
Large Language Models (LLMs) is promising in automating the HLS debugging
process. Despite the great potential, three key challenges persist when
applying LLMs to HLS logic debugging: 1) High-quality circuit data for training
LLMs is scarce, posing a significant challenge. 2) Debugging logic bugs in
hardware is inherently more complex than identifying software bugs with
existing golden test cases. 3) The absence of reliable test cases requires
multi-tasking solutions, performing both bug identification and correction.
complicates the multi-tasking required for effective HLS debugging. In this
work, we propose a customized solution named HLSDebugger to address the
challenges. HLSDebugger first generates and releases a large labeled dataset
with 300K data samples, targeting HLS logic bugs. The HLSDebugger model adopts
an encoder-decoder structure, performing bug location identification, bug type
prediction, and bug correction with the same model. HLSDebugger significantly
outperforms advanced LLMs like GPT-4 in bug identification and by more than 3x
in bug correction. It makes a substantial advancement in the exploration of
automated debugging of HLS code.

</details>


### [8] [Ethical Classification of Non-Coding Contributions in Open-Source Projects via Large Language Models](https://arxiv.org/abs/2507.21583)
*Sergio Cobos,Javier Luis Cánovas Izquierdo*

Main category: cs.SE

TL;DR: 本文提出了一种利用大型语言模型（LLM）分类开源软件（OSS）非代码贡献的伦理质量的方法，旨在通过定义伦理指标和提示工程来改善社区合作环境。


<details>
  <summary>Details</summary>
Motivation: 开源软件开发中的伦理挑战可能威胁项目可持续性，现有方法在监控和实施行为准则方面存在局限。

Method: 基于《贡献者契约》定义伦理指标，利用LLM和提示工程对非代码贡献的伦理行为进行分类。

Result: 开发了一种能够评估OSS非代码贡献伦理行为的分类方法。

Conclusion: 该方法为监控和改善开源社区伦理环境提供了新思路。

Abstract: The development of Open-Source Software (OSS) is not only a technical
challenge, but also a social one due to the diverse mixture of contributors. To
this aim, social-coding platforms, such as GitHub, provide the infrastructure
needed to host and develop the code, but also the support for enabling the
community's collaboration, which is driven by non-coding contributions, such as
issues (i.e., change proposals or bug reports) or comments to existing
contributions. As with any other social endeavor, this development process
faces ethical challenges, which may put at risk the project's sustainability.
To foster a productive and positive environment, OSS projects are increasingly
deploying codes of conduct, which define rules to ensure a respectful and
inclusive participatory environment, with the Contributor Covenant being the
main model to follow. However, monitoring and enforcing these codes of conduct
is a challenging task, due to the limitations of current approaches. In this
paper, we propose an approach to classify the ethical quality of non-coding
contributions in OSS projects by relying on Large Language Models (LLM), a
promising technology for text classification tasks. We defined a set of ethical
metrics based on the Contributor Covenant and developed a classification
approach to assess ethical behavior in OSS non-coding contributions, using
prompt engineering to guide the model's output.

</details>


### [9] [Predicting Maintenance Cessation of Open Source Software Repositories with An Integrated Feature Framework](https://arxiv.org/abs/2507.21678)
*Yiming Xu,Runzhi He,Hengzhi Ye,Minghui Zhou,Huaimin Wang*

Main category: cs.SE

TL;DR: 该论文提出了一种基于语义分析和明确归档状态的‘维护终止’定义，构建了一个大规模数据集，并提出了一个多视角特征框架，用于预测开源软件（OSS）项目的维护风险。该方法在生存分析中表现出高预测准确性，并通过实际应用验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 开源软件（OSS）项目的维护风险对软件供应链的质量、安全性和弹性构成威胁，现有方法存在定义模糊、可解释性差和数据局限性问题。

Method: 提出‘维护终止’定义，结合语义分析和归档状态，构建包含115,466个GitHub仓库的大规模数据集，设计多视角特征框架（用户、维护者和项目演化特征），并应用AFT生存分析和SHAP分析。

Result: AFT生存分析的C-index达到0.846，显著优于仅依赖表面特征的模型，特征消融和SHAP分析验证了方法的有效性和可解释性。

Conclusion: 该研究为大规模开源生态系统的维护风险预测提供了可扩展和可解释的基础，并通过实际部署证明了其适用性。

Abstract: The maintenance risks of open source software (OSS) projects pose significant
threats to the quality, security, and resilience of modern software supply
chains. While prior research has proposed diverse approaches for predicting OSS
maintenance risk -- leveraging signals ranging from surface features (e.g.,
stars, commits) to social network analyses and behavioral patterns -- existing
methods often suffer from ambiguous operational definitions, limited
interpretability, and datasets of insufficient scale or generalizability. In
this work, we introduce ``maintenance cessation'', grounded in both explicit
archival status and rigorous semantic analysis of project documentation.
Building on this foundation, we curate a large-scale, longitudinal dataset of
115,466 GitHub repositories -- encompassing 57,733 confirmed cessation events
-- complemented by comprehensive, timeline-based behavioral features. We
propose an integrated, multi-perspective feature framework for predicting
maintenance cessation, systematically combining user-centric features,
maintainer-centric features and project evolution features. AFT survival
analysis demonstrates a high C-index (0.846), substantially outperforming
models relying only on surface features. Feature ablation and SHAP analysis
further confirm the effectiveness and interpretability of our approach.
Finally, we demonstrate real-world applicability by deploying a GBSA classifier
in the openEuler ecosystem for proactive package risk screening. Our work
establishes a scalable, interpretable foundation for maintenance-risk
prediction, enabling reproducible risk management across large-scale open
source ecosystems.

</details>


### [10] [MultiAIGCD: A Comprehensive dataset for AI Generated Code Detection Covering Multiple Languages, Models,Prompts, and Scenarios](https://arxiv.org/abs/2507.21693)
*Basak Demirok,Mucahid Kutlu,Selin Mergen*

Main category: cs.SE

TL;DR: 论文介绍了MultiAIGCD数据集，用于检测AI生成的代码，涵盖Python、Java和Go语言，并评估了三种最先进的检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成中的应用增加，检测AI生成代码的需求变得迫切，以维护学术诚信和招聘公平。

Method: 基于CodeNet数据集，使用六种LLM和三种提示生成代码，覆盖三种使用场景，构建包含121,271个AI生成和32,148个人工编写代码的数据集。

Result: 评估了三种检测模型在跨模型和跨语言场景下的性能。

Conclusion: MultiAIGCD支持AI生成代码检测的研究，并提供了公开数据集和代码。

Abstract: As large language models (LLMs) rapidly advance, their role in code
generation has expanded significantly. While this offers streamlined
development, it also creates concerns in areas like education and job
interviews. Consequently, developing robust systems to detect AI-generated code
is imperative to maintain academic integrity and ensure fairness in hiring
processes. In this study, we introduce MultiAIGCD, a dataset for AI-generated
code detection for Python, Java, and Go. From the CodeNet dataset's problem
definitions and human-authored codes, we generate several code samples in Java,
Python, and Go with six different LLMs and three different prompts. This
generation process covered three key usage scenarios: (i) generating code from
problem descriptions, (ii) fixing runtime errors in human-written code, and
(iii) correcting incorrect outputs. Overall, MultiAIGCD consists of 121,271
AI-generated and 32,148 human-written code snippets. We also benchmark three
state-of-the-art AI-generated code detection models and assess their
performance in various test scenarios such as cross-model and cross-language.
We share our dataset and codes to support research in this field.

</details>


### [11] [Vibe Coding as a Reconfiguration of Intent Mediation in Software Development: Definition, Implications, and Research Agenda](https://arxiv.org/abs/2507.21928)
*Christian Meske,Tobias Hermanns,Esther von der Weiden,Kai-Uwe Loser,Thorsten Berger*

Main category: cs.SE

TL;DR: 论文探讨了AI生成的vibe coding如何改变软件开发，将开发者意图从确定性指令转向概率推断，重新分配认知工作，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成代码的广泛使用，软件开发正在经历根本性转变，但快速采用与有限的概念理解之间存在脱节，需要深入研究这一新兴范式。

Method: 通过意图视角和历史分析，定义了vibe coding作为人类与生成AI通过自然语言对话协作开发软件的新范式。

Result: vibe coding通过重新分配认知劳动，将传统软件开发的专业知识转向协作编排，同时提出了机会与风险。

Conclusion: 论文提出了涵盖人类、技术和组织三个方向的未来研究议程，以指导这一范式的进一步探索。

Abstract: Software development is undergoing a fundamental transformation as vibe
coding becomes widespread, with large portions of contemporary codebases now
being AI-generated. The disconnect between rapid adoption and limited
conceptual understanding highlights the need for an inquiry into this emerging
paradigm. Drawing on an intent perspective and historical analysis, we define
vibe coding as a software development paradigm where humans and generative AI
engage in collaborative flow to co-create software artifacts through natural
language dialogue, shifting the mediation of developer intent from
deterministic instruction to probabilistic inference. By intent mediation, we
refer to the fundamental process through which developers translate their
conceptual goals into representations that computational systems can execute.
Our results show that vibe coding reconfigures cognitive work by redistributing
epistemic labor between humans and machines, shifting the expertise in the
software development process away from traditional areas such as design or
technical implementation toward collaborative orchestration. We identify key
opportunities, including democratization, acceleration, and systemic leverage,
alongside risks, such as black box codebases, responsibility gaps, and
ecosystem bias. We conclude with a research agenda spanning human-,
technology-, and organization-centered directions to guide future
investigations of this paradigm.

</details>


### [12] [DeepGo: Predictive Directed Greybox Fuzzing](https://arxiv.org/abs/2507.21952)
*Peihong Lin,Pengfei Wang,Xu Zhou,Wei Xie,Gen Zhang,Kai Lu*

Main category: cs.SE

TL;DR: DeepGo是一种预测性的定向灰盒模糊器，结合历史和预测信息，通过优化路径引导模糊测试到达目标位置。


<details>
  <summary>Details</summary>
Motivation: 现有DGF技术依赖启发式算法优化适应度指标，但缺乏对未执行路径的前瞻性，导致难以到达复杂路径目标。

Method: 提出路径转换模型，利用深度神经网络构建虚拟集合环境预测路径转换奖励，并开发强化学习模型生成最优路径。

Result: 实现通过高奖励路径转换序列高效到达目标。

Conclusion: DeepGo能有效结合历史和预测信息，优化模糊测试路径，提高效率。

Abstract: The state-of-the-art DGF techniques redefine and optimize the fitness metric
to reach the target sites precisely and quickly. However, optimizations for
fitness metrics are mainly based on heuristic algorithms, which usually rely on
historical execution information and lack foresight on paths that have not been
exercised yet. Thus, those hard-to-execute paths with complex constraints would
hinder DGF from reaching the targets, making DGF less efficient. In this paper,
we propose DeepGo, a predictive directed grey-box fuzzer that can combine
historical and predicted information to steer DGF to reach the target site via
an optimal path. We first propose the path transition model, which models DGF
as a process of reaching the target site through specific path transition
sequences. The new seed generated by mutation would cause the path transition,
and the path corresponding to the high-reward path transition sequence
indicates a high likelihood of reaching the target site through it. Then, to
predict the path transitions and the corresponding rewards, we use deep neural
networks to construct a Virtual Ensemble Environment (VEE), which gradually
imitates the path transition model and predicts the rewards of path transitions
that have not been taken yet. To determine the optimal path, we develop a
Reinforcement Learning for Fuzzing (RLF) model to generate the transition
sequences with the highest sequence rewards. The RLF model can combine
historical and predicted path transitions to generate the optimal path
transition sequences, along with the policy to guide the mutation strategy of
fuzzing. Finally, to exercise the high-reward path transition sequence, we
propose the concept of an action group, which comprehensively optimizes the
critical steps of fuzzing to realize the optimal path to reach the target
efficiently.

</details>


### [13] [Fine-Tuning Code Language Models to Detect Cross-Language Bugs](https://arxiv.org/abs/2507.21954)
*Zengyang Li,Yimeng Li,Binbin Huang,Peng Liang,Ran Mo,Hui Liu,Yutao Ma*

Main category: cs.SE

TL;DR: 该论文研究了预训练代码语言模型（CodeLMs）在检测跨语言错误（CLB）中的潜力，开发了工具CLCFinder并构建了一个CLB数据集。实验表明，微调后CodeLMs性能有所提升，其中UniXcoder-base表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于多语言编程的普及性，跨语言错误（CLB）的检测变得重要。单语言错误检测工具无法有效检测CLB，因此探索CodeLMs在此领域的潜力很有意义。

Method: 通过开发CLCFinder工具并构建一个涉及三种编程语言组合的CLB数据集，微调13种CodeLMs并评估其性能，分析数据集大小、代码注释和序列长度的影响。

Result: 微调显著提升了CodeLMs的性能，UniXcoder-base表现最佳（F1分数0.7407）。小型模型表现优于大型模型，增加数据集规模对性能提升显著。

Conclusion: CodeLMs在CLB检测中具有潜力，但需要针对任务进行微调，且小型模型和数据集规模是关键因素。

Abstract: Multilingual programming, which involves using multiple programming languages
(PLs) in a single project, is increasingly common due to its benefits. However,
it introduces cross-language bugs (CLBs), which arise from interactions between
different PLs and are difficult to detect by single-language bug detection
tools. This paper investigates the potential of pre-trained code language
models (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language
code identification tool, and constructed a CLB dataset involving three PL
combinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction
types. We fine-tuned 13 CodeLMs on this dataset and evaluated their
performance, analyzing the effects of dataset size, token sequence length, and
code comments. Results show that all CodeLMs performed poorly before
fine-tuning, but exhibited varying degrees of performance improvement after
fine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably,
small fine-tuned CodeLMs tended to performe better than large ones. CodeLMs
fine-tuned on single-language bug datasets performed poorly on CLB detection,
demonstrating the distinction between CLBs and single-language bugs.
Additionally, increasing the fine-tuning dataset size significantly improved
performance, while longer token sequences did not necessarily improve the model
performance. The impact of code comments varied across models. Some fine-tuned
CodeLMs' performance was improved, while others showed degraded performance.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [14] [One Weird Trick to Untie Landin's Knot](https://arxiv.org/abs/2507.21317)
*Paulette Koronkevich,William J. Bowman*

Main category: cs.PL

TL;DR: 论文探讨了Landin's Knot作为编码通用递归模式的局限性，指出高阶引用本身不会导致非终止行为，关键在于对函数环境的无限制量化。


<details>
  <summary>Details</summary>
Motivation: 研究目的是澄清高阶引用与递归行为之间的关系，避免过度依赖复杂类型系统。

Method: 通过闭包转换语言，显式展示函数环境，并利用非预测性量化揭示递归编码机制。

Result: 发现限制环境量化可以安全引入高阶引用，无需复杂类型系统或限制函数存储。

Conclusion: 提出通过限制环境量化而非复杂类型系统，可在终止语言中安全使用高阶引用。

Abstract: In this work, we explore Landin's Knot, which is understood as a pattern for
encoding general recursion, including non-termination, that is possible after
adding higher-order references to an otherwise terminating language. We observe
that this isn't always true -- higher-order references, by themselves, don't
lead to non-termination. The key insight is that Landin's Knot relies not
primarily on references storing functions, but on unrestricted quantification
over a function's environment. We show this through a closure converted
language, in which the function's environment is made explicit and hides the
type of the environment through impredicative quantification. Once references
are added, this impredicative quantification can be exploited to encode
recursion. We conjecture that by restricting the quantification over the
environment, higher-order references can be safely added to terminating
languages, without resorting to more complex type systems such as linearity,
and without restricting references from storing functions.

</details>


### [15] [Fixed-Point-Oriented Programming: A Concise and Elegant Paradigm](https://arxiv.org/abs/2507.21439)
*Yong Qi Foo,Brian Sze-Kai Cheong,Michael D. Adams*

Main category: cs.PL

TL;DR: FPOP（定点导向编程）是一种新兴编程范式，旨在简化自引用计算的实现，如图算法、静态分析和分布式计算。与传统方法相比，FPOP提供了高级抽象，使问题表达更简洁且易于优化。


<details>
  <summary>Details</summary>
Motivation: 传统编程范式缺乏对定点计算的直接支持，导致实现复杂且容易出错。FPOP旨在填补这一空白，提供高效且易于使用的解决方案。

Method: FPOP通过结构化推理规则和用户导向的优化，允许开发者编写声明式规范，编译器则负责高效执行。

Result: FPOP显著简化了算法实现，提高了可维护性，并支持快速原型设计。例如，图距离问题仅需两行FPOP代码即可表达，远少于其他范式。

Conclusion: FPOP在理论与实践之间架起桥梁，有望推动进一步研究和采用这一范式。

Abstract: Fixed-Point-Oriented Programming (FPOP) is an emerging paradigm designed to
streamline the implementation of problems involving self-referential
computations. These include graph algorithms, static analysis, parsing, and
distributed computing-domains that traditionally require complex and
tricky-to-implement work-queue algorithms. Existing programming paradigms lack
direct support for these inherently fixed-point computations, leading to
inefficient and error-prone implementations.
  This white paper explores the potential of the FPOP paradigm, which offers a
high-level abstraction that enables concise and expressive problem
formulations. By leveraging structured inference rules and user-directed
optimizations, FPOP allows developers to write declarative specifications while
the compiler ensures efficient execution. It not only reduces implementation
complexity for programmers but also enhances adaptability, making it easier for
programmers to explore alternative solutions and optimizations without
modifying the core logic of their program.
  We demonstrate how FPOP simplifies algorithm implementation, improves
maintainability, and enables rapid prototyping by allowing problems to be
clearly and concisely expressed. For example, the graph distance problem can be
expressed in only two executable lines of code with FPOP, while it takes an
order of magnitude more code in other paradigms. By bridging the gap between
theoretical fixed-point formulations and practical implementations, we aim to
foster further research and adoption of this paradigm.

</details>


### [16] [Composable Effect Handling for Programming LLM-integrated Scripts](https://arxiv.org/abs/2507.22048)
*Di Wang*

Main category: cs.PL

TL;DR: 通过可组合效果处理来分离LLM脚本的工作流逻辑与效果操作，提高模块化和性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM集成脚本中模块化和性能优化的问题。

Method: 使用可组合效果处理技术，将效果操作（如LLM调用、I/O和并发）抽象为接口。

Result: 在Tree-of-Thoughts案例中实现了10倍的速度提升，同时保持模块化。

Conclusion: 可组合效果处理是一种适用于LLM脚本编程的有效风格。

Abstract: Implementing LLM-integrated scripts introduces challenges in modularity and
performance, as scripts are often coupled to specific LLM implementations and
fail to exploit parallelization opportunities. This paper proposes using
composable effect handling to separate workflow logic from effectful
operations, such as LLM calls, I/O, and concurrency, enabling modularity
without sacrificing the opportunity for performance optimization. By treating
these operations as abstract interfaces and discharging them via effect
handlers, this paper shows that scripts can achieve significant speedups (e.g.,
10$\times$ in a Tree-of-Thoughts case study) without compromising modularity.
This paper aims to promote composable effect handling as a programming style
for LLM scripting.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [17] [Beamforming-based Achievable Rate Maximization in ISAC System for Multi-UAV Networking](https://arxiv.org/abs/2507.21895)
*Shengcai Zhou,Luping Xiang,Kun Yang,Kai Kit Wong,Dapeng Oliver Wu,Chan-Byoung Chae*

Main category: cs.PF

TL;DR: 论文研究了基于多无人机网络的应急通信方案，提出了一种时间辅助帧结构和优化算法，以提高通信速率和感知精度。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络中ISAC技术的发展，无人机基站在应急通信中的应用成为研究热点，旨在解决高效搜索和通信优化问题。

Method: 提出了时间辅助帧结构和EKF辅助波束对准，并设计了三种机制（SCA-IRM算法、联盟博弈法和费马点搜索法）来优化无人机波束成形和方向规划。

Result: 仿真结果显示，系统在通信速率、公平性和感知精度方面性能显著提升。

Conclusion: 该研究为无人机辅助的应急通信网络提供了有效的设计和优化指南。

Abstract: Airborne mobile Integrated Sensing and Communication (ISAC) base stations
have garnered significant attention recently, with ISAC technology being a
crucial application for 6G networks. Since ISAC can sense potential mobile
communication users, this paper studies an effective scheme for a multi-UAV
network tailored for emergency communication. In this paper, we develop a
temporal-assisted frame structure utilizing integrated omnidirectional and
directional beampattern to facilitate efficient and frequent searching, with
extended Kalman filtering (EKF) as an aid to beam alignment. Further, we
address an optimization problem to maximize the total achievable rate per slot
by jointly designing UAV beamforming, load management, and UAV direction
planning, all while adhering to the constraints of the predicted beam coverage.
Given the problem NP-hard, we introduce three robust mechanisms for its
resolution: an enhanced distributed Successive Convex Approximation
(SCA)-Iterative Rank Minimization (IRM) algorithm, an coalition game approach,
and a Fermat point search method. In particular, the proposed SCA-IRM algorithm
decomposes the original complex optimization problem into several sub-problems
and assigns them equally to each UAV, so as to realize distributed computing
and improve computational efficiency. Our proposed simulations demonstrate the
improved system performance in terms of communication rate, fairness, and
sensing accuracy, providing design guidelines of UAV-assisted emergency
communication networking.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [18] [Load Balancing for AI Training Workloads](https://arxiv.org/abs/2507.21372)
*Sarah McClure,Sylvia Ratnasamy,Scott Shenker*

Main category: cs.NI

TL;DR: 评估大规模AI训练任务中不同负载均衡算法的性能，并探讨拥塞控制和丢失恢复算法的影响。


<details>
  <summary>Details</summary>
Motivation: 大规模AI训练任务对负载均衡提出了高要求，而算法性能受拥塞控制和丢失恢复影响，因此需要系统性评估。

Method: 通过实验评估多种负载均衡算法在不同拥塞控制和丢失恢复策略下的性能表现。

Result: 研究揭示了负载均衡算法性能与拥塞控制及丢失恢复算法的关系。

Conclusion: 选择合适的负载均衡算法需综合考虑拥塞控制和丢失恢复机制的设计。

Abstract: We investigate the performance of various load balancing algorithms for
large-scale AI training workloads that are running on dedicated infrastructure.
The performance of load balancing depends on both the congestion control and
loss recovery algorithms, so our evaluation also sheds light on the appropriate
choices for those designs as well.

</details>


### [19] [Deep Reinforcement Learning-based Cell DTX/DRX Configuration for Network Energy Saving](https://arxiv.org/abs/2507.21385)
*Wei Mao,Lili Wei,Omid Semiari,Shu-ping Yeh,Hosein Nikopour*

Main category: cs.NI

TL;DR: 本文研究了如何通过深度强化学习（DRL）配置5G中的Cell DTX/DRX，以优化节能与数据包延迟的平衡，实现高达45%的节能效果，同时将服务质量（QoS）下降控制在1%以内。


<details>
  <summary>Details</summary>
Motivation: 由于5G中Cell DTX/DRX在节能的同时会不可避免增加数据包延迟，因此需要在不同网络和流量条件下找到最优的配置，以最大化节能效果并最小化QoS下降。

Method: 采用深度强化学习（DRL）框架，结合深度Q网络（DQN）和上下文bandit（CB）模型，设计奖励函数来训练AI代理，以动态选择最优的Cell DTX/DRX配置。

Result: 仿真结果显示，与不使用Cell DTX/DRX的情况相比，AI代理可以在不同流量场景下实现高达45%的节能，同时将QoS下降控制在1%以内。

Conclusion: 通过DRL设计的AI代理能够灵活适应不同网络条件，高效平衡节能与服务质量的需求。

Abstract: 3GPP Release 18 cell discontinuous transmission and reception (cell DTX/DRX)
is an important new network energy saving feature for 5G. As a time-domain
technique, it periodically aggregates the user data transmissions in a given
duration of time when the traffic load is not heavy, so that the remaining time
can be kept silent and advanced sleep modes (ASM) can be enabled to shut down
more radio components and save more energy for the cell. However, inevitably
the packet delay is increased, as during the silent period no transmission is
allowed. In this paper we study how to configure cell DTX/DRX to optimally
balance energy saving and packet delay, so that for delay-sensitive traffic
maximum energy saving can be achieved while the degradation of quality of
service (QoS) is minimized. As the optimal configuration can be different for
different network and traffic conditions, the problem is complex and we resort
to deep reinforcement learning (DRL) framework to train an AI agent to solve
it. Through careful design of 1) the learning algorithm, which implements a
deep Q-network (DQN) on a contextual bandit (CB) model, and 2) the reward
function, which utilizes a smooth approximation of a theoretically optimal but
discontinuous reward function, we are able to train an AI agent that always
tries to select the best possible Cell DTX/DRX configuration under any network
and traffic conditions. Simulation results show that compared to the case when
cell DTX/DRX is not used, our agent can achieve up to ~45% energy saving
depending on the traffic load scenario, while always maintaining no more than
~1% QoS degradation.

</details>


### [20] [Generalized few-shot transfer learning architecture for modeling the EDFA gain spectrum](https://arxiv.org/abs/2507.21728)
*Agastya Raj,Zehao Wang,Tingjun Chen,Daniel C Kilper,Marco Ruffini*

Main category: cs.NI

TL;DR: 论文提出了一种基于半监督自归一化神经网络的少样本迁移学习架构，用于优化EDFA的增益谱预测，通过两阶段训练和迁移学习技术显著减少了测量需求并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 为了优化多厂商解决方案中的光纤网络性能，需要准确建模EDFA的增益谱。

Method: 采用半监督自归一化神经网络（SS-NN），结合无监督预训练和监督微调，并引入迁移学习技术以适应不同类型的EDFA。

Result: 在26个EDFA上的实验表明，该方法减少了测量需求，平均绝对误差和误差分布优于基准方法。

Conclusion: 提出的方法在增益谱预测中表现出高效性和准确性，适用于复杂的光网络环境。

Abstract: Accurate modeling of the gain spectrum in Erbium-Doped Fiber Amplifiers
(EDFAs) is essential for optimizing optical network performance, particularly
as networks evolve toward multi-vendor solutions. In this work, we propose a
generalized few-shot transfer learning architecture based on a Semi-Supervised
Self-Normalizing Neural Network (SS-NN) that leverages internal EDFA features -
such as VOA input or output power and attenuation, to improve gain spectrum
prediction. Our SS-NN model employs a two-phase training strategy comprising
unsupervised pre-training with noise-augmented measurements and supervised
fine-tuning with a custom weighted MSE loss. Furthermore, we extend the
framework with transfer learning (TL) techniques that enable both homogeneous
(same-feature space) and heterogeneous (different-feature sets) model
adaptation across booster, preamplifier, and ILA EDFAs. To address feature
mismatches in heterogeneous TL, we incorporate a covariance matching loss to
align second-order feature statistics between source and target domains.
Extensive experiments conducted across 26 EDFAs in the COSMOS and Open Ireland
testbeds demonstrate that the proposed approach significantly reduces the
number of measurements requirements on the system while achieving lower mean
absolute errors and improved error distributions compared to benchmark methods.

</details>


### [21] [RRTO: A High-Performance Transparent Offloading System for Model Inference in Mobile Edge Computing](https://arxiv.org/abs/2507.21739)
*Zekai Sun,Xiuxian Guan,Zheng Lin,Yuhao Qing,Haoze Song,Zihan Fang,Zhe Chen,Fangming Liu,Heming Cui,Wei Ni,Jun Luo*

Main category: cs.NI

TL;DR: RRTO是一种高性能透明的MEC推理卸载系统，通过记录/重放机制减少RPC调用，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 解决移动设备上ML应用资源受限及现有透明卸载方法因频繁RPC调用导致高延迟的问题。

Method: 采用记录/重放机制和Operator Sequence Search算法，静态分析ML模型的算子序列以减少RPC调用。

Result: 比现有透明方法降低98%的延迟和能耗，性能接近非透明方法，且无需修改源代码。

Conclusion: RRTO实现了高效透明的MEC推理卸载，为资源受限设备提供了兼容性和性能的平衡。

Abstract: Deploying Machine Learning (ML) applications on resource-constrained mobile
devices remains challenging due to limited computational resources and poor
platform compatibility. While Mobile Edge Computing (MEC) offers
offloading-based inference paradigm using GPU servers, existing approaches are
divided into non-transparent and transparent methods, with the latter
necessitating modifications to the source code. Non-transparent offloading
achieves high performance but requires intrusive code modification, limiting
compatibility with diverse applications. Transparent offloading, in contrast,
offers wide compatibility but introduces significant transmission delays due to
per-operator remote procedure calls (RPCs). To overcome this limitation, we
propose RRTO, the first high-performance transparent offloading system tailored
for MEC inference. RRTO introduces a record/replay mechanism that leverages the
static operator sequence in ML models to eliminate repetitive RPCs. To reliably
identify this sequence, RRTO integrates a novel Operator Sequence Search
algorithm that detects repeated patterns, filters initialization noise, and
accelerates matching via a two-level strategy. Evaluation demonstrates that
RRTO achieves substantial reductions of up to 98% in both per-inference latency
and energy consumption compared to state-of-the-art transparent methods and
yields results comparable to non-transparent approaches, all without
necessitating any source code modification.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [22] [Sync-TVA: A Graph-Attention Framework for Multimodal Emotion Recognition with Cross-Modal Fusion](https://arxiv.org/abs/2507.21395)
*Zeyu Deng,Yanhui Lu,Jiashu Liao,Shuang Wu,Chongfeng Wei*

Main category: cs.MM

TL;DR: 本文提出Sync-TVA框架，通过动态增强和跨模态图注意力解决多模态情感识别中的跨模态交互不足和贡献不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法存在跨模态交互不足和模态贡献不平衡的问题。

Method: 提出了Sync-TVA框架，包括模态特异性动态增强模块、异构跨模态图构建和跨注意力融合机制。

Result: 在MELD和IEMOCAP数据集上表现出优于现有方法的准确性和加权F1分数，尤其在类别不平衡条件下。

Conclusion: Sync-TVA通过增强模态交互和平衡贡献，提升了多模态情感识别的性能。

Abstract: Multimodal emotion recognition (MER) is crucial for enabling emotionally
intelligent systems that perceive and respond to human emotions. However,
existing methods suffer from limited cross-modal interaction and imbalanced
contributions across modalities. To address these issues, we propose Sync-TVA,
an end-to-end graph-attention framework featuring modality-specific dynamic
enhancement and structured cross-modal fusion. Our design incorporates a
dynamic enhancement module for each modality and constructs heterogeneous
cross-modal graphs to model semantic relations across text, audio, and visual
features. A cross-attention fusion mechanism further aligns multimodal cues for
robust emotion inference. Experiments on MELD and IEMOCAP demonstrate
consistent improvements over state-of-the-art models in both accuracy and
weighted F1 score, especially under class-imbalanced conditions.

</details>


### [23] [PC-JND: Subjective Study and Dataset on Just Noticeable Difference for Point Clouds in 6DoF Virtual Reality](https://arxiv.org/abs/2507.21557)
*Chunling Fan,Yun Zhang,Dietmar Saupe,Raouf Hamzaoui,Weisi Lin*

Main category: cs.MM

TL;DR: 研究了在六自由度VR环境中点云的JND特性，发现纹理JND小于几何JND，且颜色丰富度与纹理JND相关。发布了PC-JND数据集。


<details>
  <summary>Details</summary>
Motivation: 探讨点云在VR环境中的JND特性，填补此前未被研究的空白，优化沉浸式媒体的感知体验。

Method: 在六自由度VR环境中使用头戴显示器研究点云的JND特性，分析纹理和几何JND的差异及相关性。

Result: 纹理JND小于几何JND；颜色丰富度与纹理JND相关，但与几何JND无关；点云点数与JND无关。

Conclusion: 研究填补了点云JND特性的空白，发布了PC-JND数据集，为未来感知优化研究提供了数据支持。

Abstract: The Just Noticeable Difference (JND) accounts for the minimum distortion at
which humans can perceive a difference between a pristine stimulus and its
distorted version. The JND concept has been widely applied in visual signal
processing tasks, including coding, transmission, rendering, and quality
assessment, to optimize human-centric media experiences. A point cloud is a
mainstream volumetric data representation consisting of both geometry
information and attributes (e.g. color). Point clouds are used for advanced
immersive 3D media such as Virtual Reality (VR). However, the JND
characteristics of viewing point clouds in VR have not been explored before. In
this paper, we study the point cloud-wise JND (PCJND) characteristics in a Six
Degrees of Freedom (6DoF) VR environment using a head-mounted display. Our
findings reveal that the texture PCJND of human eyes is smaller than the
geometry PCJND for most point clouds. Furthermore, we identify a correlation
between colorfulness and texture PCJND. However, there is no significant
correlation between colorfulness and the geometry PCJND, nor between the number
of points and neither the texture or geometry PCJND. To support future research
in JND prediction and perception-driven signal processing, we introduce PC-JND,
a novel point cloud-based JND dataset. This dataset will be made publicly
available to facilitate advancements in perceptual optimization for immersive
media.

</details>


### [24] [Efficient Sub-pixel Motion Compensation in Learned Video Codecs](https://arxiv.org/abs/2507.21926)
*Théo Ladune,Thomas Leguay,Pierrick Philippe,Gordon Clare,Félix Henry*

Main category: cs.MM

TL;DR: 通过学习传统编解码器的优化方法，改进学习型编解码器的运动补偿技术，提升压缩性能并降低解码复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统编解码器（如HEVC和VVC）在运动补偿方面已高度优化，而学习型编解码器仍使用简单的双线性滤波进行子像素运动补偿，存在优化空间。

Method: 从传统编解码器中汲取灵感，采用更高级的插值滤波器、基于块的运动信息和有限运动精度。

Result: 在Cool-chic视频编解码器中，实现了10%以上的码率降低，并将运动相关的解码复杂度从每像素391 MAC降至214 MAC。

Conclusion: 通过借鉴传统编解码器的技术，可以有效提升学习型编解码器的性能，实验证明了其优越性。

Abstract: Motion compensation is a key component of video codecs. Conventional codecs
(HEVC and VVC) have carefully refined this coding step, with an important focus
on sub-pixel motion compensation. On the other hand, learned codecs achieve
sub-pixel motion compensation through simple bilinear filtering. This paper
offers to improve learned codec motion compensation by drawing inspiration from
conventional codecs. It is shown that the usage of more advanced interpolation
filters, block-based motion information and finite motion accuracy lead to
better compression performance and lower decoding complexity. Experimental
results are provided on the Cool-chic video codec, where we demonstrate a rate
decrease of more than 10% and a lowering of motion-related decoding complexity
from 391 MAC per pixel to 214 MAC per pixel. All contributions are made
open-source at https://github.com/Orange-OpenSource/Cool-Chic

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [25] [Semantic Numeration Systems as Dynamical Systems](https://arxiv.org/abs/2507.21295)
*Alexander Yu. Chunikhin*

Main category: cs.LO

TL;DR: 论文概述了语义计数系统理论的基础概念，提出了基于基数语义多重性的基数抽象对象（CAO）作为非线性控制的线性离散动态系统，并给出了其状态方程。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨基数语义算子对基数抽象实体的作用，并构建一种新的理论框架来描述这些动态系统。

Method: 通过分析基数语义多重性中的基数抽象对象，提出将其视为线性离散动态系统，并推导出静态和非静态情况下的状态方程。

Result: 证明了配置矩阵在结合基数语义算子类型、参数和连接拓扑信息中的关键作用。

Conclusion: 该研究为基数语义系统的动态行为提供了理论基础，并展示了配置矩阵的重要性。

Abstract: The foundational concepts of semantic numeration systems theory are briefly
outlined. The action of cardinal semantic operators unfolds over a set of
cardinal abstract entities belonging to the cardinal semantic multeity. The
cardinal abstract object (CAO) formed by them in a certain connectivity
topology is proposed to be considered as a linear discrete dynamical system
with nonlinear control. Under the assumption of ideal observability, the CAO
state equations are provided for both stationary and non-stationary cases. The
fundamental role of the configuration matrix, which combines information about
the types of cardinal semantic operators in the CAO, their parameters and
topology of connectivity, is demonstrated.

</details>


### [26] [A Tree-Shaped Tableau for Checking the Satisfiability of Signal Temporal Logic with Bounded Temporal Operators](https://arxiv.org/abs/2507.21598)
*Beatrice Melani,Ezio Bartocci,Michele Chiari*

Main category: cs.LO

TL;DR: 本文提出了一种新颖的树形一次性表法，用于检查离散时间STL的满足性，该方法在冗余时间间隔上优化性能，并适用于MLTL，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决STL需求逻辑一致性的高效自动检查问题，扩展其应用范围。

Method: 提出一种树形一次性表法，利用STL公式中的冗余时间间隔加速满足性检查。

Result: 实验表明，该方法在许多情况下优于SMT和一阶逻辑编码。

Conclusion: 该方法不仅高效，还可用于信号合成、验证和公式等价性检查。

Abstract: Signal Temporal Logic (STL) is a widely recognized formal specification
language to express rigorous temporal requirements on mixed analog signals
produced by cyber-physical systems (CPS). A relevant problem in CPS design is
how to efficiently and automatically check whether a set of STL requirements is
logically consistent. This problem reduces to solving the STL satisfiability
problem, which is decidable when we assume that our system operates in discrete
time steps dictated by an embedded system's clock.
  This paper introduces a novel tree-shaped, one-pass tableau method for
satisfiability checking of discrete-time STL with bounded temporal operators.
Originally designed to prove the consistency of a given set of STL
requirements, this method has a wide range of applications beyond consistency
checking. These include synthesizing example signals that satisfy the given
requirements, as well as verifying or refuting the equivalence and implications
of STL formulas.
  Our tableau exploits redundancy arising from large time intervals in STL
formulas to speed up satisfiability checking, and can also be employed to check
Mission-Time Linear Temporal Logic (MLTL) satisfiability. We compare our
tableau with Satisfiability Modulo Theories (SMT) and First-Order Logic
encodings from the literature on a benchmark suite, partly collected from the
literature, and partly provided by an industrial partner. Our experiments show
that, in many cases, our tableau outperforms state-of-the-art encodings.

</details>


### [27] [The Shape of $\mathcal{EL}$ Proofs: A Tale of Three Calculi (Extended Version)](https://arxiv.org/abs/2507.21851)
*Christian Alrabbaa,Stefan Borgwardt,Philipp Herrmann,Markus Krötzsch*

Main category: cs.LO

TL;DR: 研究了三种基于后果的推理演算在描述逻辑中的表现，并通过规则引擎评估它们的复杂性指标。


<details>
  <summary>Details</summary>
Motivation: 探讨不同演算方法在描述逻辑中的证明生成效果和复杂性。

Method: 将演算转化为带有分层否定的存在规则，使用NEMO规则引擎执行并评估。

Result: 通过比较不同演算生成的证明在多个复杂性指标上的表现。

Conclusion: 不同演算生成的证明在复杂性上有所不同，为选择推理方法提供了参考。

Abstract: Consequence-based reasoning can be used to construct proofs that explain
entailments of description logic (DL) ontologies. In the literature, one can
find multiple consequence-based calculi for reasoning in the $\mathcal{EL}$
family of DLs, each of which gives rise to proofs of different shapes. Here, we
study three such calculi and the proofs they produce on a benchmark based on
the OWL Reasoner Evaluation. The calculi are implemented using a translation
into existential rules with stratified negation, which had already been
demonstrated to be effective for the calculus of the ELK reasoner. We then use
the rule engine NEMO to evaluate the rules and obtain traces of the rule
execution. After translating these traces back into DL proofs, we compare them
on several metrics that reflect different aspects of their complexity.

</details>


### [28] [Why not? Developing ABox Abduction beyond Repairs](https://arxiv.org/abs/2507.21955)
*Anselm Haak,Patrick Koopmann,Yasir Mahmood,Anni-Yasmin Turhan*

Main category: cs.LO

TL;DR: 该论文研究了在不一致知识库（KB）下的反绎推理（abduction）任务，定义了适用于修复语义的反绎概念，并提出了指导反绎生成有用假设的最小化准则，同时提供了在不同修复语义和描述逻辑DL-Lite和EL_bot下的初步复杂性结果。


<details>
  <summary>Details</summary>
Motivation: 针对错误数据导致的不一致知识库，研究反绎推理任务，旨在计算缺失蕴涵的解释或假设，弥补现有研究在完美数据和经典语义下的不足。

Method: 定义修复语义下的反绎概念，提出一组最小化准则以指导生成有用假设，并分析其在不同修复语义和描述逻辑（DL-Lite和EL_bot）下的复杂性，包括解决方案存在性和验证。

Result: 提供了反绎解决方案在不同修复语义和描述逻辑下的初步复杂性分析结果，包括解决方案存在性和验证的复杂性。

Conclusion: 通过定义修复语义下的反绎任务和引入最小化准则，论文为不一致知识库下的反绎推理提供了理论基础和复杂性分析，扩展了反绎推理的应用范围。

Abstract: Abduction is the task of computing a sufficient extension of a knowledge base
(KB) that entails a conclusion not entailed by the original KB. It serves to
compute explanations, or hypotheses, for such missing entailments. While this
task has been intensively investigated for perfect data and under classical
semantics, less is known about abduction when erroneous data results in
inconsistent KBs. In this paper we define a suitable notion of abduction under
repair semantics, and propose a set of minimality criteria that guides
abduction towards `useful' hypotheses. We provide initial complexity results on
deciding existence of and verifying abductive solutions with these criteria,
under different repair semantics and for the description logics DL-Lite and
EL_bot.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [29] [Enhancing Manufacturing Training Through VR Simulations](https://arxiv.org/abs/2507.21070)
*Vladislav Li,Ilias Siniosoglou,Panagiotis Sarigiannidis,Vasileios Argyriou*

Main category: cs.HC

TL;DR: 该论文提出了一种基于VR的工业培训系统，通过高保真模拟和动态场景提高学习效果，实验结果显示其在信息保留和执行精度上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决工业制造培训中理论与实践脱节的问题，利用VR提供沉浸式、安全的培训体验。

Method: 设计了一个VR培训系统，包含高保真模拟、动态场景、自适应反馈和手势控制，并使用VRTSS评分动态评估学员表现。

Result: 实验表明系统显著提升了信息保留、任务执行精度和培训效率。

Conclusion: VR是工业培训中的关键工具，为传统方法提供了可扩展、互动且高效的替代方案。

Abstract: In contemporary training for industrial manufacturing, reconciling
theoretical knowledge with practical experience continues to be a significant
difficulty. As companies transition to more intricate and technology-oriented
settings, conventional training methods frequently inadequately equip workers
with essential practical skills while maintaining safety and efficiency.
Virtual Reality has emerged as a transformational instrument to tackle this
issue by providing immersive, interactive, and risk-free teaching experiences.
Through the simulation of authentic industrial environments, virtual reality
facilitates the acquisition of vital skills for trainees within a regulated and
stimulating context, therefore mitigating the hazards linked to experiential
learning in the workplace. This paper presents a sophisticated VR-based
industrial training architecture aimed at improving learning efficacy via
high-fidelity simulations, dynamic and context-sensitive scenarios, and
adaptive feedback systems. The suggested system incorporates intuitive
gesture-based controls, reducing the learning curve for users across all skill
levels. A new scoring metric, namely, VR Training Scenario Score (VRTSS), is
used to assess trainee performance dynamically, guaranteeing ongoing engagement
and incentive. The experimental assessment of the system reveals promising
outcomes, with significant enhancements in information retention, task
execution precision, and overall training efficacy. The results highlight the
capability of VR as a crucial instrument in industrial training, providing a
scalable, interactive, and efficient substitute for conventional learning
methods.

</details>


### [30] [FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents](https://arxiv.org/abs/2507.21071)
*Qinglong Yang,Haoming Li,Haotian Zhao,Xiaokai Yan,Jingtao Ding,Fengli Xu,Yong Li*

Main category: cs.HC

TL;DR: 该论文介绍了FingerTip基准，通过分析环境观察和用户历史意图来增强移动GUI代理的主动意图预测和个性化任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 解决当前移动GUI代理仅能执行明确指令、缺乏主动意图预测能力及忽略用户偏好差异的问题。

Method: 提出FingerTip基准，包含主动任务建议和个性化任务执行两个新轨道，并收集了真实用户的多步骤Android设备交互数据。

Result: 实验表明，基于收集数据微调的模型能有效利用用户信息并取得良好效果。

Conclusion: 该方法在构建更用户导向的移动GUI代理方面具有潜力。

Abstract: Mobile GUI agents are becoming critical tools for enhancing human-device
interaction efficiency, with multimodal large language models (MLLMs) emerging
as dominant paradigms in this domain. Current agents, however, are limited to
following explicit human instructions, resulting in insufficient capability for
proactive intent anticipation. Additionally, these agents fail to leverage the
contextual information associated with users during task execution, thereby
neglecting potentially vast differences in user preferences. To address these
challenges, we introduce the FingerTip benchmark. It contains two new tracks:
proactive task suggestions by analyzing environment observation and users'
previous intents, and personalized task execution by catering to users' action
preferences. We collected unique human demonstrations of multi-step Android
device interactions across a variety of everyday apps. These demonstrations are
not isolated but are continuously acquired from the users' long-term usage in
their real lives, and encompass essential user-related contextual information.
Our experiments reveal challenges of the tasks we propose. The model fine-tuned
with the data we collected effectively utilized user information and achieved
good results, highlighting the potential of our approach in building more
user-oriented mobile GUI agents. Our code is open-source at
https://anonymous.4open.science/r/FingerTip-57B8 for reproducibility.

</details>


### [31] [Impact of eHMI on Pedestrians' Interactions with Level-5 Automated Driving Systems](https://arxiv.org/abs/2507.21303)
*Viktoria Marcus,Griffin Pitts,Sanaz Motamedi*

Main category: cs.HC

TL;DR: 研究表明，外部人机界面（eHMIs）能显著提升行人对自动驾驶车辆意图的理解，增强安全感和信任感。


<details>
  <summary>Details</summary>
Motivation: 全球半数以上的交通事故涉及行人，自动驾驶系统（ADSs）可通过减少人为错误来改善行人安全，但eHMIs的优化设计尚不明确。

Method: 通过在线调查，153名参与者对有无eHMIs的L5自动驾驶车辆过马路场景进行响应，对比行人的行为和感知。

Result: 使用eHMIs时，行人过马路更早且更自信，安全感、信任感和理解度显著提升，视觉特征（如文字显示和外部速度计）比听觉更受欢迎。

Conclusion: eHMIs能有效促进行人与L5自动驾驶车辆的交互，未来的eHMI设计应优先视觉特征。

Abstract: Each year, over half of global traffic fatalities involve vulnerable road
users (e.g. pedestrians), often due to human error. Level-5 automated driving
systems (ADSs) could reduce driver errors contributing to pedestrian accidents,
though effectiveness depends on clarity and understandability for other road
users. External human-machine interfaces (eHMIs) have been proposed to
facilitate pedestrian-ADS communication, though consensus on optimal eHMI
features remains unclear. In an online survey, 153 participants responded to
road-crossing scenarios involving level-5 ADSs, with and without eHMIs. With
eHMIs, pedestrians crossed earlier and more confidently, and reported
significantly increased perceptions of safety, trust, and understanding when
interacting with level-5 ADSs. Visual eHMI features (including a text display
and external speedometer) were ranked more necessary than auditory ones, though
auditory cues received positive feedback. This study demonstrates that eHMIs
can significantly improve pedestrians' understanding of level-5 ADS intent and
enhance perceived safety and trust, facilitating more intuitive pedestrian-ADS
interactions.

</details>


### [32] [VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization](https://arxiv.org/abs/2507.21124)
*Ayan Biswas,Terece L. Turton,Nishath Rajiv Ranasinghe,Shawn Jones,Bradley Love,William Jones,Aric Hagberg,Han-Wei Shen,Nathan DeBardeleben,Earl Lawrence*

Main category: cs.HC

TL;DR: VizGenie是一个自进化的科学可视化框架，通过LLM生成脚本扩展功能，结合自然语言界面和视觉问答技术，提升交互性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 传统科学可视化工具功能有限且需要专业知识，VizGenie旨在通过LLM和自然语言交互降低门槛，动态扩展功能。

Method: 框架结合预置工具和LLM生成的脚本，通过自动化验证集成新功能；利用图像分析和VQA精确解析用户查询，并通过RAG增强可靠性。

Result: 在复杂体数据上测试显示，显著降低了迭代可视化任务的认知负担，支持可持续的功能扩展和用户交互学习。

Conclusion: VizGenie通过融合领域工具与LLM灵活性，不仅加速科学洞察，还推动了可视化实践的可持续进化。

Abstract: We present VizGenie, a self-improving, agentic framework that advances
scientific visualization through large language model (LLM) by orchestrating of
a collection of domain-specific and dynamically generated modules. Users
initially access core functionalities--such as threshold-based filtering, slice
extraction, and statistical analysis--through pre-existing tools. For tasks
beyond this baseline, VizGenie autonomously employs LLMs to generate new
visualization scripts (e.g., VTK Python code), expanding its capabilities
on-demand. Each generated script undergoes automated backend validation and is
seamlessly integrated upon successful testing, continuously enhancing the
system's adaptability and robustness. A distinctive feature of VizGenie is its
intuitive natural language interface, allowing users to issue high-level
feature-based queries (e.g., ``visualize the skull"). The system leverages
image-based analysis and visual question answering (VQA) via fine-tuned vision
models to interpret these queries precisely, bridging domain expertise and
technical implementation. Additionally, users can interactively query generated
visualizations through VQA, facilitating deeper exploration. Reliability and
reproducibility are further strengthened by Retrieval-Augmented Generation
(RAG), providing context-driven responses while maintaining comprehensive
provenance records. Evaluations on complex volumetric datasets demonstrate
significant reductions in cognitive overhead for iterative visualization tasks.
By integrating curated domain-specific tools with LLM-driven flexibility,
VizGenie not only accelerates insight generation but also establishes a
sustainable, continuously evolving visualization practice. The resulting
platform dynamically learns from user interactions, consistently enhancing
support for feature-centric exploration and reproducible research in scientific
visualization.

</details>


### [33] [Snap, Segment, Deploy: A Visual Data and Detection Pipeline for Wearable Industrial Assistants](https://arxiv.org/abs/2507.21072)
*Di Wen,Junwei Zheng,Ruiping Liu,Yi Xu,Kunyu Peng,Rainer Stiefelhagen*

Main category: cs.HC

TL;DR: 该论文提出了一种基于移动设备的工业辅助系统，通过轻量级对象检测、语音识别和RAG技术，实现实时、半免提的交互，适用于计算资源有限且隐私要求严格的工业环境。


<details>
  <summary>Details</summary>
Motivation: 工业装配任务需要快速适应复杂流程和多样组件，但传统云端或全自主解决方案在资源有限和隐私严格的工厂环境中难以部署。

Method: 系统整合了轻量级对象检测、语音识别和RAG技术，采用模块化设计并全设备运行，通过自动化数据构建和两阶段优化策略提升视觉鲁棒性。

Result: 在Gear8数据集上的实验显示其具备更好的领域适应性和视觉鲁棒性，用户研究也验证了其实际可行性。

Conclusion: 该框架为工业环境提供了一种高效、隐私保护且可部署的实时智能辅助解决方案，计划公开数据集和源代码。

Abstract: Industrial assembly tasks increasingly demand rapid adaptation to complex
procedures and varied components, yet are often conducted in environments with
limited computing, connectivity, and strict privacy requirements. These
constraints make conventional cloud-based or fully autonomous solutions
impractical for factory deployment. This paper introduces a mobile-device-based
assistant system for industrial training and operational support, enabling
real-time, semi-hands-free interaction through on-device perception and voice
interfaces. The system integrates lightweight object detection, speech
recognition, and Retrieval-Augmented Generation (RAG) into a modular on-device
pipeline that operates entirely on-device, enabling intuitive support for part
handling and procedure understanding without relying on manual supervision or
cloud services. To enable scalable training, we adopt an automated data
construction pipeline and introduce a two-stage refinement strategy to improve
visual robustness under domain shift. Experiments on our generated dataset,
i.e., Gear8, demonstrate improved robustness to domain shift and common visual
corruptions. A structured user study further confirms its practical viability,
with positive user feedback on the clarity of the guidance and the quality of
the interaction. These results indicate that our framework offers a deployable
solution for real-time, privacy-preserving smart assistance in industrial
environments. We will release the Gear8 dataset and source code upon
acceptance.

</details>


### [34] [Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education](https://arxiv.org/abs/2507.21074)
*Qian Huang,Thijs Willems*

Main category: cs.HC

TL;DR: 研究探讨了生成式AI在教育中的应用，重点关注教师如何设计AI工具以支持定性研究方法课程。


<details>
  <summary>Details</summary>
Motivation: 解决当前研究中学生被视为被动AI使用者，以及AI在定性方法教育中应用有限的问题。

Method: 采用TPACK框架和行动研究方法，设计四种定制GPT工具用于支持课程任务。

Result: AI工具提高了学生的反思能力、访谈技巧和分析思维，但也带来了认知过载和数据沉浸不足的问题。

Conclusion: 教育者设计的AI工具可以促进更有意义的学习，同时需要平衡技术支持与人文引导。

Abstract: As generative AI (Gen-AI) tools become more prevalent in education, there is
a growing need to understand how educators, not just students, can actively
shape their design and use. This study investigates how two instructors
integrated four custom GPT tools into a Masters-level Qualitative Research
Methods course for Urban Planning Policy students. Addressing two key gaps: the
dominant framing of students as passive AI users, and the limited use of AI in
qualitative methods education. The study explores how Gen-AI can support
disciplinary learning when aligned with pedagogical intent. Drawing on the
Technological Pedagogical Content Knowledge (TPACK) framework and action
research methodology, the instructors designed GPTs to scaffold tasks such as
research question formulation, interview practice, fieldnote analysis, and
design thinking. Thematic analysis of student reflections, AI chat logs, and
final assignments revealed that the tools enhanced student reflexivity,
improved interview techniques, and supported structured analytic thinking.
However, students also expressed concerns about cognitive overload, reduced
immersion in data, and the formulaic nature of AI responses. The study offers
three key insights: AI can be a powerful scaffold for active learning when
paired with human facilitation; custom GPTs can serve as cognitive partners in
iterative research practice; and educator-led design is critical to
pedagogically meaningful AI integration. This research contributes to emerging
scholarship on AI in higher education by demonstrating how empowering educators
to design custom tools can promote more reflective, responsible, and
collaborative learning with AI.

</details>


### [35] [Can LLMs Reason About Trust?: A Pilot Study](https://arxiv.org/abs/2507.21075)
*Anushka Debnath,Stephen Cranefield,Emiliano Lorini,Bastin Tony Roy Savarimuthu*

Main category: cs.HC

TL;DR: 本文探讨了大型语言模型（LLMs）在理解与构建人际关系信任方面的能力，并评估其是否能够通过角色扮演来诱导信任。


<details>
  <summary>Details</summary>
Motivation: 信任是人际关系中的关键因素，随着电子交互的普及，AI系统可能帮助用户理解社交状态，LLMs在此领域的潜力值得研究。

Method: 研究通过实验评估LLMs在信任推理及角色扮演中的表现，分析其能否规划行动以建立信任。

Result: 研究证实LLMs具备一定的信任推理能力，并能在互动中通过行动诱导信任。

Conclusion: LLMs在信任构建方面展现出潜力，未来可进一步优化其在复杂社交环境中的应用。

Abstract: In human society, trust is an essential component of social attitude that
helps build and maintain long-term, healthy relationships which creates a
strong foundation for cooperation, enabling individuals to work together
effectively and achieve shared goals. As many human interactions occur through
electronic means such as using mobile apps, the potential arises for AI systems
to assist users in understanding the social state of their relationships. In
this paper we investigate the ability of Large Language Models (LLMs) to reason
about trust between two individuals in an environment which requires fostering
trust relationships. We also assess whether LLMs are capable of inducing trust
by role-playing one party in a trust based interaction and planning actions
which can instil trust.

</details>


### [36] [InSituTale: Enhancing Augmented Data Storytelling with Physical Objects](https://arxiv.org/abs/2507.21411)
*Kentaro Takahira,Yue Yu,Takanori Fujiwara,Suzuki Ryo,Huamin Qu*

Main category: cs.HC

TL;DR: 论文提出了一种通过物理对象交互增强数据叙事的系统InSituTale，结合了深度摄像头和Vision-LLM技术，实现了直观且沉浸式的数据展示体验。


<details>
  <summary>Details</summary>
Motivation: 现有系统主要依赖手势或语音控制数据可视化，缺乏对物理对象交互的探索，因此作者希望通过物理交互增强数据叙事的沉浸感和互动性。

Method: 作者首先调查了数据驱动演示中常见的可视化命令，并通过与HCI/VIS研究者的工作坊收集物理操作与命令的映射关系，最终开发了结合深度摄像头和Vision-LLM的原型InSituTale。

Result: 用户研究表明，InSituTale能提供直观的交互方式、高实用性和引人入胜的演示体验。

Conclusion: 该研究表明物理对象交互可以有效增强数据叙事的沉浸感和互动性。

Abstract: Augmented data storytelling enhances narrative delivery by integrating
visualizations with physical environments and presenter actions. Existing
systems predominantly rely on body gestures or speech to control
visualizations, leaving interactions with physical objects largely
underexplored. We introduce augmented physical data storytelling, an approach
enabling presenters to manipulate visualizations through physical object
interactions. To inform this approach, we first conducted a survey of
data-driven presentations to identify common visualization commands. We then
conducted workshops with nine HCI/VIS researchers to collect mappings between
physical manipulations and these commands. Guided by these insights, we
developed InSituTale, a prototype that combines object tracking via a depth
camera with Vision-LLM for detecting real-world events. Through physical
manipulations, presenters can dynamically execute various visualization
commands, delivering cohesive data storytelling experiences that blend physical
and digital elements. A user study with 12 participants demonstrated that
InSituTale enables intuitive interactions, offers high utility, and facilitates
an engaging presentation experience.

</details>


### [37] [Data-Driven and Participatory Approaches toward Neuro-Inclusive AI](https://arxiv.org/abs/2507.21077)
*Naba Rizvi*

Main category: cs.HC

TL;DR: 论文提出了神经包容AI的概念，批评现有AI研究中排斥自闭症视角的问题，并通过实验开发了一个新基准AUTALIC来改善数据代表性。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中存在对自闭症的偏见，将其视为神经典型社交能力的缺陷而非人类多样性的一部分。这种偏见影响了全球7500万自闭症患者。

Method: 定义神经包容AI，探讨现有研究中的偏见起源、普遍性和影响，并通过实验验证二元标签方案的有效性。

Result: 90%的人型AI代理排斥自闭症视角，开发的AUTALIC基准可用于模型评估或微调。

Conclusion: AUTALIC基准为未来神经包容性研究提供了基础，呼吁AI开发者重视伦理考量。

Abstract: Biased data representation in AI marginalizes up to 75 million autistic
people worldwide through medical applications viewing autism as a deficit of
neurotypical social skills rather than an aspect of human diversity, and this
perspective is grounded in research questioning the humanity of autistic
people. Turing defined artificial intelligence as the ability to mimic human
communication, and as AI development increasingly focuses on human-like agents,
this benchmark remains popular. In contrast, we define Neuro-Inclusive AI as
datasets and systems that move away from mimicking humanness as a benchmark for
machine intelligence. Then, we explore the origins, prevalence, and impact of
anti-autistic biases in current research. Our work finds that 90% of human-like
AI agents exclude autistic perspectives, and AI creators continue to believe
ethical considerations are beyond the scope of their work. To improve the
autistic representation in data, we conduct empirical experiments with
annotators and LLMs, finding that binary labeling schemes sufficiently capture
the nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can
be used to evaluate or fine-tune models, and was developed to serve as a
foundation for more neuro-inclusive future work.

</details>


### [38] [What Makes a Level Hard in Super Mario Maker 2?](https://arxiv.org/abs/2507.21078)
*Carlo A. Furia,Andrea Mocci*

Main category: cs.HC

TL;DR: 该论文分析了《超级马里奥制造2》（SMM2）用户设计关卡的数据，研究哪些因素影响关卡难度，并通过回归模型和自然语言处理技术揭示关键差异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解SMM2用户设计关卡中哪些因素会影响其他用户体验到的难度。

Method: 使用回归模型和自然语言处理技术分析关卡数据。

Result: 发现了关卡特征（如风格、流行度、时间）以及主题和情感对于关卡难度的关联性。

Conclusion: 研究结果有助于理解用户设计的关卡难度差异，为后续研究提供参考。

Abstract: Games like Super Mario Maker 2 (SMM2) lower the barrier for casual users to
become level designers. In this paper, we set out to analyze a vast amount of
data about SMM2 user-written levels, in order to understand what factors affect
a level's difficulty as experienced by other users. To this end, we perform two
kinds of analyses: one based on regression models and one using natural
language processing techniques. The main results shed light on which level
characteristics (e.g., its style, popularity, timing) and which topics and
sentiments have a consistent association with easier or harder levels. While
none of our findings are startling, they help distill some key differences
between easy and hard SMM2 levels, which, in turn, can pave the way for a
better understanding of end-user level design.

</details>


### [39] [Metaverse Support Groups for LGBTQ+ Youth: An Observational Study on Safety, Self-Expression, and Early Intervention](https://arxiv.org/abs/2507.21079)
*Joe Hasei,Yosuke Matsumoto,Hiroki Kawai,Yuko Okahisa,Manabu Takaki,Toshifumi Ozaki*

Main category: cs.HC

TL;DR: 该研究评估了基于元宇宙的支持小组，旨在减少LGBTQ+青年的社会孤立和自杀风险。结果显示，元宇宙在安全性、自我表达和可访问性方面优于现实环境，且显著提升了参与者的社交信心。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索元宇宙技术如何为LGBTQ+青年提供安全的支持环境，减少社会孤立和自杀风险。

Method: 使用Cluster平台，提供增强匿名性、基于化身的自我表达和可访问性，评估参与者的体验和效果。

Result: 79.2%的参与者选择与其性别认同匹配的化身，社交信心显著提升（p<0.001），特别是在初始信心较低的群体中。

Conclusion: 元宇宙支持小组能有效降低心理障碍并提供肯定空间，但需与现有服务和临床框架整合以增强长期效果。

Abstract: This study assessed metaverse-based support groups designed to reduce social
isolation and suicide risk among LGBTQ+ youths. Using the Cluster platform,
enhanced anonymity, avatar-based self-expression, and accessibility were
provided. Key findings showed that 79.2% chose avatars matching their gender
identity, reporting high satisfaction (mean: 4.10/5) and low discomfort (mean:
1.79/5). Social confidence significantly improved in virtual spaces compared to
real-world interactions (p<0.001), particularly among participants with
initially low confidence, averaging an increase of 2.08 points. About half of
the first-time participants were 16 or younger, highlighting potential for
early intervention. The metaverse scored higher than real-world environments
for safety/privacy (3.94/5), self-expression (4.02/5), and accessibility
(4.21/5). Additionally, 73.6% reported feeling more accepted virtually.
However, some highly confident individuals offline experienced mild adaptation
challenges, averaging a confidence decrease of 0.58 points, indicating virtual
support complements rather than replaces in-person services. These findings
suggest metaverse-based support effectively lowers psychological barriers and
provides affirming spaces, potentially reducing severe outcomes such as
suicidal ideation. Future studies should focus on integrating virtual support
with existing community and clinical frameworks to enhance long-term impacts.

</details>


### [40] [Empathy in Explanation](https://arxiv.org/abs/2507.21081)
*Katherine M. Collins,Kartik Chandra,Adrian Weller,Jonathan Ragan-Kelley,Joshua B. Tenenbaum*

Main category: cs.HC

TL;DR: 论文探讨了解释行为中的情感因素，提出一个计算框架来模拟解释者如何考虑听众的情感反应，并通过医生向患者解释疾病的场景验证了模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解解释行为中情感的作用，尤其是在社会互动中解释者如何考虑听众的情感反应。

Method: 作者开发了一个计算框架，模拟解释者如何预测并考虑听众的情感（如后悔倾向），并通过医生解释疾病的场景测试模型。

Result: 模型能较好地预测人类直觉，表现优于不考虑情感的模型，表明人们在解释时确实会考虑情感因素。

Conclusion: 研究表明情感在解释行为中扮演重要角色，验证了计算框架的有效性。

Abstract: Why do we give the explanations we do? Recent work has suggested that we
should think of explanation as a kind of cooperative social interaction,
between a why-question-asker and an explainer. Here, we apply this perspective
to consider the role that emotion plays in this social interaction. We develop
a computational framework for modeling explainers who consider the emotional
impact an explanation might have on a listener. We test our framework by using
it to model human intuitions about how a doctor might explain to a patient why
they have a disease, taking into account the patient's propensity for regret.
Our model predicts human intuitions well, better than emotion-agnostic
ablations, suggesting that people do indeed reason about emotion when giving
explanations.

</details>


### [41] [Eliciting User Requirements for AI-Enhanced Learning Environments using a Participatory Approach](https://arxiv.org/abs/2507.21088)
*Bibeg Limbu,Irene-Angelica Chounta,Vilma Sukacke,Andromachi Filippidi,Chara Spyropoulou,Marianna Anagnostopoulou,Eleftheria Tsourlidaki,Nikos Karacapilidis*

Main category: cs.HC

TL;DR: 本文探讨了教育利益相关者对AI增强学习环境的需求与期望，通过两阶段参与式研讨会收集数据并分析，最终提出AI在教育中的设计建议。


<details>
  <summary>Details</summary>
Motivation: 研究AI在教育中的应用，明确利益相关者的需求和期望，以优化学习环境的设计。

Method: 采用两阶段参与式研讨会，分别通过演绎和归纳主题分析法分析数据，并运用活动理论进行解释。

Result: 识别了利益相关者的需求与期望，发现矛盾点，并生成了新兴技术的用户需求。

Conclusion: 为未来AI在学习环境中的设计提供了建议，强调用户需求和期望的重要性。

Abstract: This paper explores the needs \& expectations of educational stakeholders for
AI (Artificial Intelligence)-enhanced learning environments. Data was collected
following two-phased participatory workshops. The first workshop outlined
stakeholders' profiles in terms of technical and pedagogical characteristics.
The qualitative data collected was analysed using deductive thematic analysis
with Activity Theory, explicating the user needs. The second workshop
articulated expectations related to the integration of AI in education.
Inductive thematic analysis of the second workshop led to the elicitation of
users' expectations. We cross-examined the needs and expectations, identifying
contradictions, to generate user requirements for emerging technologies. The
paper provides suggestions for future design initiatives that incorporate AI in
learning environments.

</details>


### [42] [Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations](https://arxiv.org/abs/2507.21089)
*Xiaotian Su,Naim Zierau,Soomin Kim,April Yi Wang,Thiemo Wambsganss*

Main category: cs.HC

TL;DR: 研究了情绪监控仪表板对用户情感意识和仇恨言论的影响，发现其能有效提升情感意识并减少仇恨言论，但也可能增加负面情绪的表达。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台常因主动审核技术被批评为审查且未能解决不文明行为的根源，因此本研究探讨了情绪监控工具的作用。

Method: 提出并评估了两种情绪监控仪表板，通过211名参与者的实验研究其对评论行为和情感体验的影响。

Result: 结果显示仪表板能提升情感意识、减少仇恨言论，但也增加了讨论敏感话题时的负面情绪表达。

Conclusion: 情绪监控工具有助于改善数字互动，但需注意其潜在的负面影响，为未来研究提供了方向。

Abstract: Social media platforms increasingly employ proactive moderation techniques,
such as detecting and curbing toxic and uncivil comments, to prevent the spread
of harmful content. Despite these efforts, such approaches are often criticized
for creating a climate of censorship and failing to address the underlying
causes of uncivil behavior. Our work makes both theoretical and practical
contributions by proposing and evaluating two types of emotion monitoring
dashboards to users' emotional awareness and mitigate hate speech. In a study
involving 211 participants, we evaluate the effects of the two mechanisms on
user commenting behavior and emotional experiences. The results reveal that
these interventions effectively increase users' awareness of their emotional
states and reduce hate speech. However, our findings also indicate potential
unintended effects, including increased expression of negative emotions (Angry,
Fear, and Sad) when discussing sensitive issues. These insights provide a basis
for further research on integrating proactive emotion regulation tools into
social media platforms to foster healthier digital interactions.

</details>


### [43] [Thinking Like a Scientist: Can Interactive Simulations Foster Critical AI Literacy?](https://arxiv.org/abs/2507.21090)
*Yiling Zhao,Audrey Michal,Nithum Thain,Hari Subramonyam*

Main category: cs.HC

TL;DR: 研究探讨交互式模拟是否比传统方法更能提升AI素养，通过实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于AI系统影响社会和个体决策，提升批判性AI素养至关重要，传统方法效果有限。

Method: 在605名参与者中，通过交互式AI教程评估其对公平性、数据集代表性和语言模型偏见的理解。

Result: 交互式模拟显著提升AI素养，促进知识迁移和自信心，但单纯参与不能预测学习效果。

Conclusion: 交互式、探究驱动的方法更有效，有助于提升日常生活中的AI批判能力。

Abstract: As AI systems shape individual and societal decisions, fostering critical AI
literacy is essential. Traditional approaches, such as blog articles, static
lessons, and social media discussions, often fail to support deep conceptual
understanding and critical engagement. This study examines whether interactive
simulations can help learners think like a scientist by engaging them in
hypothesis testing, experimentation, and direct observation of AI behavior. In
a controlled study with 605 participants, we assess how interactive AI
tutorials impact learning of key concepts such as fairness, dataset
representativeness, and bias in language models. Results show that interactive
simulations effectively enhance AI literacy across topics, supporting greater
knowledge transfer and self-reported confidence, though engagement alone does
not predict learning. This work contributes to the growing field of AI literacy
education, highlighting how interactive, inquiry-driven methodologies can
better equip individuals to critically engage with AI in their daily lives.

</details>


### [44] [Identification of Design Recommendations for Augmented Reality Authors in Corporate Training](https://arxiv.org/abs/2507.21722)
*Stefan Graser,Martin Schrepp,Stephan Böhm*

Main category: cs.HC

TL;DR: 研究通过多方法分析，识别并分类了AR设计建议，评估了其在企业培训中的适用性，扩展了AR用户体验测量方法。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏针对特定背景的AR设计建议，本文填补了这一空白，尤其是针对企业培训场景。

Method: 采用多方法分析，包括数据集扩展、NLP分类、内容总结和专家评估。

Result: 提供了597条设计建议，分为84个主题，其中32个主题（284条建议）适用于企业培训。

Conclusion: 研究为AR设计提供了实用指导，并支持AR用户体验测量方法的扩展。

Abstract: Innovative technologies, such as Augmented Reality (AR), introduce new
interaction paradigms, demanding the identification of software requirements
during the software development process. In general, design recommendations are
related to this, supporting the design of applications positively and meeting
stakeholder needs. However, current research lacks context-specific AR design
recommendations. This study addresses this gap by identifying and analyzing
practical AR design recommendations relevant to the evaluation phase of the
User-Centered Design (UCD) process. We rely on an existing dataset of Mixed
Reality (MR) design recommendations. We applied a multi-method approach by (1)
extending the dataset with AR-specific recommendations published since 2020,
(2) classifying the identified recommendations using a NLP classification
approach based on a pre-trained Sentence Transformer model, (3) summarizing the
content of all topics, and (4) evaluating their relevance concerning AR in
Corporate Training (CT) both based on a qualitative Round Robin approach with
five experts. As a result, an updated dataset of 597 practitioner design
recommendations, classified into 84 topics, is provided with new insights into
their applicability in the context of AR in CT. Based on this, 32 topics with a
total of 284 statements were evaluated as relevant for AR in CT. This research
directly contributes to the authors' work for extending their AR-specific User
Experience (UX) measurement approach, supporting AR authors in targeting the
improvement of AR applications for CT scenarios.

</details>


### [45] [ProMemAssist: Exploring Timely Proactive Assistance Through Working Memory Modeling in Multi-Modal Wearable Devices](https://arxiv.org/abs/2507.21378)
*Kevin Pu,Ting Zhang,Naveen Sendhilnathan,Sebastian Freitag,Raj Sodhi,Tanya Jonker*

Main category: cs.HC

TL;DR: ProMemAssist是一种智能眼镜系统，通过多模态传感器实时建模用户的工作记忆，提供更精准的协助。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴AI系统依赖用户启动或预定义任务知识，忽视了用户的当前心理状态。

Method: 基于工作记忆认知理论，系统将感知信息建模为记忆项和片段，并结合编码机制设计时间预测器。

Result: 在12名参与者的实验中，ProMemAssist比LLM基线系统更精准地提供协助，并获得更高用户参与度。

Conclusion: 工作记忆建模能实现更细致、上下文敏感的协助，为更关注用户的主动代理设计提供启示。

Abstract: Wearable AI systems aim to provide timely assistance in daily life, but
existing approaches often rely on user initiation or predefined task knowledge,
neglecting users' current mental states. We introduce ProMemAssist, a smart
glasses system that models a user's working memory (WM) in real-time using
multi-modal sensor signals. Grounded in cognitive theories of WM, our system
represents perceived information as memory items and episodes with encoding
mechanisms, such as displacement and interference. This WM model informs a
timing predictor that balances the value of assistance with the cost of
interruption. In a user study with 12 participants completing cognitively
demanding tasks, ProMemAssist delivered more selective assistance and received
higher engagement compared to an LLM baseline system. Qualitative feedback
highlights the benefits of WM modeling for nuanced, context-sensitive support,
offering design implications for more attentive and user-aware proactive
agents.

</details>


### [46] [MindChat: Enhancing BCI Spelling with Large Language Models in Realistic Scenarios](https://arxiv.org/abs/2507.21435)
*JIaheng Wang,Yucun Zhong,Chengjie Huang,Lin Yao*

Main category: cs.HC

TL;DR: MindChat是一种基于LLM的BCI拼写辅助工具，通过减少手动按键提高效率，实验表明节省62%按键和32%时间。


<details>
  <summary>Details</summary>
Motivation: 传统BCI拼写器逐字母输入易出错且效率低，限制了实际应用。

Method: 利用LLM（如GPT-4）通过提示工程提供上下文感知的词句补全建议。

Result: 在线实验中，MindChat节省62%按键和32%时间。

Conclusion: LLM增强的BCI拼写器有望实现高效实用的通信应用。

Abstract: Brain-computer interface (BCI) spellers can render a new communication
channel independent of peripheral nervous system, which are especially valuable
for patients with severe motor disabilities. However, current BCI spellers
often require users to type intended utterances letter-by-letter while spelling
errors grow proportionally due to inaccurate electroencephalogram (EEG)
decoding, largely impeding the efficiency and usability of BCIs in real-world
communication. In this paper, we present MindChat, a large language model
(LLM)-assisted BCI speller to enhance BCI spelling efficiency by reducing
users' manual keystrokes. Building upon prompt engineering, we prompt LLMs
(GPT-4o) to continuously suggest context-aware word and sentence
completions/predictions during spelling. Online copy-spelling experiments
encompassing four dialogue scenarios demonstrate that MindChat saves more than
62\% keystrokes and over 32\% spelling time compared with traditional BCI
spellers. We envision high-speed BCI spellers enhanced by LLMs will potentially
lead to truly practical applications.

</details>


### [47] [Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals](https://arxiv.org/abs/2507.21462)
*Tingying He,Maggie McCracken,Daniel Hajas,Sarah Creem-Regehr,Alexander Lex*

Main category: cs.HC

TL;DR: 探索触觉图表是否有助于盲人和低视力人群理解复杂图表，提出四种触觉图表设计并通过访谈研究验证其效果。


<details>
  <summary>Details</summary>
Motivation: 视觉图表是传达数据的有力工具，但盲人和低视力人群只能依赖替代文本访问信息；缺乏建立图表心理模型的途径。

Method: 与两位盲人研究人员合作，设计了四种复杂图表的3D打印触觉模板，并通过12名参与者进行访谈研究。

Result: 触觉模型支持对图表类型的理解，并成为盲人和低视力人群的首选学习方法。

Conclusion: 触觉图表设计有助于盲人和低视力人群建立图表心理模型，并在教育中发挥积极作用。

Abstract: We investigate whether tactile charts support comprehension and learning of
complex visualizations for blind and low-vision (BLV) individuals and
contribute four tactile chart designs and an interview study. Visualizations
are powerful tools for conveying data, yet BLV individuals typically can rely
only on assistive technologies -- primarily alternative texts -- to access this
information. Prior research shows the importance of mental models of chart
types for interpreting these descriptions, yet BLV individuals have no means to
build such a mental model based on images of visualizations. Tactile charts
show promise to fill this gap in supporting the process of building mental
models. Yet studies on tactile data representations mostly focus on simple
chart types, and it is unclear whether they are also appropriate for more
complex charts as would be found in scientific publications. Working with two
BLV researchers, we designed 3D-printed tactile template charts with
exploration instructions for four advanced chart types: UpSet plots, violin
plots, clustered heatmaps, and faceted line charts. We then conducted an
interview study with 12 BLV participants comparing whether using our tactile
templates improves mental models and understanding of charts and whether this
understanding translates to novel datasets experienced through alt texts.
Thematic analysis shows that tactile models support chart type understanding
and are the preferred learning method by BLV individuals. We also report
participants' opinions on tactile chart design and their role in BLV education.

</details>


### [48] [Conversations over Clicks: Impact of Chatbots on Information Search in Interdisciplinary Learning](https://arxiv.org/abs/2507.21490)
*Hannah Kim,Sergei L. Kosakovsky Pond,Stephen MacNeil*

Main category: cs.HC

TL;DR: 生成式AI（GenAI）在生物信息学学习中如何影响学习者的信息搜索行为。研究发现，GenAI在学习计划明确后有助于导向行为，但在初期可能适得其反。信息线索的有效性取决于学习者对领域的先验知识。


<details>
  <summary>Details</summary>
Motivation: 探索GenAI在复杂的跨学科学习环境（如生物信息学）中对学习者信息搜索行为的影响。

Method: 采用自民族志研究方法，分析学习者与GenAI聊天机器人的交互行为。

Result: GenAI在学习计划明确后支持导向行为，而在初期可能阻碍学习。信息线索的有效性依赖于先验知识。

Conclusion: 建议在跨学科学习环境中谨慎引入GenAI。

Abstract: This full research paper investigates the impact of generative AI (GenAI) on
the learner experience, with a focus on how learners engage with and utilize
the information it provides. In e-learning environments, learners often need to
navigate a complex information space on their own. This challenge is further
compounded in interdisciplinary fields like bioinformatics, due to the varied
prior knowledge and backgrounds. In this paper, we studied how GenAI influences
information search in bioinformatics research: (1) How do interactions with a
GenAI chatbot influence learner orienteering behaviors?; and (2) How do
learners identify information scent in GenAI chatbot responses? We adopted an
autoethnographic approach to investigate these questions. GenAI was found to
support orienteering once a learning plan was established, but it was
counterproductive prior to that. Moreover, traditionally value-rich information
sources such as bullet points and related terms proved less effective when
applied to GenAI responses. Information scents were primarily recognized
through the presence or absence of prior knowledge of the domain. These
findings suggest that GenAI should be adopted into e-learning environments with
caution, particularly in interdisciplinary learning contexts.

</details>


### [49] [AI Literacy as a Key Driver of User Experience in AI-Powered Assessment: Insights from Socratic Mind](https://arxiv.org/abs/2507.21654)
*Meryem Yilmaz Soylu,Jeonghyun Lee,Jui-Tse Hung,Christopher Zhang Cui,David A. Joyner*

Main category: cs.HC

TL;DR: 论文研究了学生AI素养及对AI工具的感知如何影响学习效果，发现AI素养显著预测用户体验和学习效果，而AI接触无显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在教育中的普及，了解学生的AI素养如何影响其对AI工具的使用和学习效果至关重要。

Method: 研究采用问卷调查309名本科生，结合Self-Determination Theory和用户体验研究，通过偏最小二乘结构方程模型分析数据。

Result: AI素养（自我效能、概念理解和应用技能）显著预测可用性、满意度和参与度；可用性与满意度进一步预测学习效果，而AI接触无影响。

Conclusion: 设计AI学习工具时应注重用户中心化和适应性指导，以支持不同素养水平的学生，提升学习效果。

Abstract: As Artificial Intelligence (AI) tools become increasingly embedded in higher
education, understanding how students interact with these systems is essential
to supporting effective learning. This study examines how students' AI literacy
and prior exposure to AI technologies shape their perceptions of Socratic Mind,
an interactive AI-powered formative assessment tool. Drawing on
Self-Determination Theory and user experience research, we analyze
relationships among AI literacy, perceived usability, satisfaction, engagement,
and perceived learning effectiveness. Data from 309 undergraduates in Computer
Science and Business courses were collected through validated surveys. Partial
least squares structural equation modeling showed that AI literacy - especially
self-efficacy, conceptual understanding, and application skills - significantly
predicts usability, satisfaction, and engagement. Usability and satisfaction,
in turn, strongly predict perceived learning effectiveness, while prior AI
exposure showed no significant effect. These findings highlight that AI
literacy, rather than exposure alone, shapes student experiences. Designers
should integrate adaptive guidance and user-centered features to support
diverse literacy levels, fostering inclusive, motivating, and effective
AI-based learning environments.

</details>


### [50] [Helping or Homogenizing? GenAI as a Design Partner to Pre-Service SLPs for Just-in-Time Programming of AAC](https://arxiv.org/abs/2507.21811)
*Cynthia Zastudil,Christine Holyfield,Christine Kapp,Kate Hamilton,Kriti Baru,Liam Newsam,June A. Smith,Stephen MacNeil*

Main category: cs.HC

TL;DR: 研究开发了一个基于生成式AI的原型，用于自动为视觉场景显示（VSD）生成热点，帮助非专家用户快速配置AAC设备。


<details>
  <summary>Details</summary>
Motivation: VSD对非语言自闭症儿童的语言发展非常有效，但配置困难限制了其广泛应用。

Method: 通过生成式AI自动生成初始热点，并进行用户研究评估其效果。

Result: 原型提升了效率和用户信心，但也导致过度依赖和沟通选项同质化。

Conclusion: 生成式AI在辅助技术中的应用需进一步研究以解决潜在风险。

Abstract: Augmentative and alternative communication (AAC) devices are used by many
people around the world who experience difficulties in communicating verbally.
One AAC device which is especially useful for minimally verbal autistic
children in developing language and communication skills are visual scene
displays (VSD). VSDs use images with interactive hotspots embedded in them to
directly connect language to real-world contexts which are meaningful to the
AAC user. While VSDs can effectively support emergent communicators, their
widespread adoption is impacted by how difficult these devices are to
configure. We developed a prototype that uses generative AI to automatically
suggest initial hotspots on an image to help non-experts efficiently create
VSDs. We conducted a within-subjects user study to understand how effective our
prototype is in supporting non-expert users, specifically pre-service
speech-language pathologists (SLP) who are not familiar with VSDs as an AAC
intervention. Pre-service SLPs are actively studying to become clinically
certified SLPs and have domain-specific knowledge about language and
communication skill development. We evaluated the effectiveness of our
prototype based on creation time, quality, and user confidence. We also
analyzed the relevance and developmental appropriateness of the automatically
generated hotspots and how often users interacted with the generated hotspots.
Our results were mixed with SLPs becoming more efficient and confident.
However, there were multiple negative impacts as well, including over-reliance
and homogenization of communication options. The implications of these findings
reach beyond the domain of AAC, especially as generative AI becomes more
prevalent across domains, including assistive technology. Future work is needed
to further identify and address these risks associated with integrating
generative AI into assistive technology.

</details>


### [51] [VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos](https://arxiv.org/abs/2507.21837)
*Yotam Sechayk,Ariel Shamir,Amy Pavel,Takeo Igarashi*

Main category: cs.HC

TL;DR: VeasyGuide是一种通过动态高亮和放大教师动作的教学辅助工具，显著提升了低视力学习者的学习效果和减少认知负担。


<details>
  <summary>Details</summary>
Motivation: 教师在视频中常使用视觉动作传达信息，但这些缺乏语音描述的视觉线索对低视力学习者造成信息遗漏和认知负担。

Method: 通过与低视力学习者共同设计，开发了VeasyGuide，利用运动检测识别教师动作并动态高亮和放大。

Result: 实验显示，低视力学习者能更快检测教师动作并减少认知负担；视力正常学习者的专注力也有所提升。

Conclusion: VeasyGuide是一种对各类学习者都有益的通用工具。

Abstract: Instructors often rely on visual actions such as pointing, marking, and
sketching to convey information in educational presentation videos. These
subtle visual cues often lack verbal descriptions, forcing low-vision (LV)
learners to search for visual indicators or rely solely on audio, which can
lead to missed information and increased cognitive load. To address this
challenge, we conducted a co-design study with three LV participants and
developed VeasyGuide, a tool that uses motion detection to identify instructor
actions and dynamically highlight and magnify them. VeasyGuide produces
familiar visual highlights that convey spatial context and adapt to diverse
learners and content through extensive personalization and real-time visual
feedback. VeasyGuide reduces visual search effort by clarifying what to look
for and where to look. In an evaluation with 8 LV participants, learners
demonstrated a significant improvement in detecting instructor actions, with
faster response times and significantly reduced cognitive load. A separate
evaluation with 8 sighted participants showed that VeasyGuide also enhanced
engagement and attentiveness, suggesting its potential as a universally
beneficial tool.

</details>


### [52] [Leveraging LLMs for Persona-Based Visualization of Election Data](https://arxiv.org/abs/2507.21900)
*Swaroop Panda,Arun Kumar Sekar*

Main category: cs.HC

TL;DR: 提出了一种基于选民角色定制选举可视化信息的方法，以提升信息的理解与相关性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过角色划分理解不同选民的需求与行为，从而优化选举信息的传播效果。

Method: 利用英国议会选举数据和大型语言模型（LLMs）创建选民角色，并基于这些角色设计可视化原型。

Result: 通过角色和LLMs评估可视化原型，提出了具体的设计标准和可操作的见解。

Conclusion: 基于角色的可视化设计能有效提升选举信息的理解与相关性，为未来选举传播工具提供参考。

Abstract: Visualizations are essential tools for disseminating information regarding
elections and their outcomes, potentially influencing public perceptions.
Personas, delineating distinctive segments within the populace, furnish a
valuable framework for comprehending the nuanced perspectives, requisites, and
behaviors of diverse voter demographics. In this work, we propose making
visualizations tailored to these personas to make election information easier
to understand and more relevant. Using data from UK parliamentary elections and
new developments in Large Language Models (LLMs), we create personas that
encompass the diverse demographics, technological preferences, voting
tendencies, and information consumption patterns observed among
voters.Subsequently, we elucidate how these personas can inform the design of
visualizations through specific design criteria. We then provide illustrative
examples of visualization prototypes based on these criteria and evaluate these
prototypes using these personas and LLMs. We finally propose some actionable
insights based upon the framework and the different design artifacts.

</details>


### [53] [MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation](https://arxiv.org/abs/2507.21953)
*Yi Kong,Dianxi Shi,Guoli Yang,Zhang ke-di,Chenlin Huang,Xiaopeng Li,Songchang Jin*

Main category: cs.HC

TL;DR: 提出了一种名为MapAgent的新型LLM代理框架，通过利用历史轨迹构建的记忆来增强任务规划能力，解决了LLM代理在处理复杂移动应用任务时的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLM代理在移动设备上自动化任务时，因缺乏对实际应用场景的了解，导致任务规划无效甚至出现幻觉，需要一种更有效的规划方法。

Method: 1）提出基于轨迹的记忆机制，将任务执行轨迹转化为可重用的结构化页面记忆数据库；2）引入由粗到细的任务规划方法，从记忆中检索相关页面以增强LLM规划能力；3）通过双LLM架构的任务执行器将计划转化为可执行动作。

Result: 实验表明，MapAgent在实际场景中表现优于现有方法。

Conclusion: MapAgent通过记忆增强的任务规划，显著提升了LLM代理在复杂移动任务中的有效性，并将开源代码以支持进一步研究。

Abstract: The recent advancement of autonomous agents powered by Large Language Models
(LLMs) has demonstrated significant potential for automating tasks on mobile
devices through graphical user interfaces (GUIs). Despite initial progress,
these agents still face challenges when handling complex real-world tasks.
These challenges arise from a lack of knowledge about real-life mobile
applications in LLM-based agents, which may lead to ineffective task planning
and even cause hallucinations. To address these challenges, we propose a novel
LLM-based agent framework called MapAgent that leverages memory constructed
from historical trajectories to augment current task planning. Specifically, we
first propose a trajectory-based memory mechanism that transforms task
execution trajectories into a reusable and structured page-memory database.
Each page within a trajectory is extracted as a compact yet comprehensive
snapshot, capturing both its UI layout and functional context. Secondly, we
introduce a coarse-to-fine task planning approach that retrieves relevant pages
from the memory database based on similarity and injects them into the LLM
planner to compensate for potential deficiencies in understanding real-world
app scenarios, thereby achieving more informed and context-aware task planning.
Finally, planned tasks are transformed into executable actions through a task
executor supported by a dual-LLM architecture, ensuring effective tracking of
task progress. Experimental results in real-world scenarios demonstrate that
MapAgent achieves superior performance to existing methods. The code will be
open-sourced to support further research.

</details>


### [54] [DataSway: Vivifying Metaphoric Visualization with Animation Clip Generation and Coordination](https://arxiv.org/abs/2507.22051)
*Liwenhan Xie,Jiayi Zhou,Anyi Rao,Huamin Qu,Xinhuan Shu*

Main category: cs.HC

TL;DR: 论文提出了一种人机协作的工作流程，用于为SVG隐喻可视化设计动画，结合视觉语言模型和用户输入，解决了动画设计中的语义对齐、数据忠实度和交互性问题。


<details>
  <summary>Details</summary>
Motivation: 隐喻可视化动画能提升抽象数据的理解和用户参与度，但设计这些动画存在挑战，如语义对齐、数据忠实度和交互性。

Method: 采用人机协作工作流程，结合视觉语言模型生成动画片段，用户可根据实体顺序、属性值、空间布局或随机性协调时间轴。

Result: 开发了原型DataSway，用户研究显示其支持创意和可用性。6个案例展示其在超媒体中的应用。

Conclusion: 研究为未来定制化数据可视化动画提供了启示。

Abstract: Animating metaphoric visualizations brings data to life, enhancing the
comprehension of abstract data encodings and fostering deeper engagement.
However, creators face significant challenges in designing these animations,
such as crafting motions that align semantically with the metaphors,
maintaining faithful data representation during animation, and seamlessly
integrating interactivity. We propose a human-AI co-creation workflow that
facilitates creating animations for SVG-based metaphoric visualizations. Users
can initially derive animation clips for data elements from vision-language
models (VLMs) and subsequently coordinate their timelines based on entity
order, attribute values, spatial layout, or randomness. Our design decisions
were informed by a formative study with experienced designers (N=8). We further
developed a prototype, DataSway, and conducted a user study (N=14) to evaluate
its creativity support and usability. A gallery with 6 cases demonstrates its
capabilities and applications in web-based hypermedia. We conclude with
implications for future research on bespoke data visualization animation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [55] [Learning Simulatable Models of Cloth with Spatially-varying Constitutive Properties](https://arxiv.org/abs/2507.21288)
*Guanxiong Chen,Shashwat Suri,Yuhao Wu,Etienne Voulga,David I. W. Levin,Dinesh Pai*

Main category: cs.GR

TL;DR: 提出了一种名为Mass-Spring Net的通用框架，通过学习运动观测数据来构建高效的替代模型，以模拟复杂布料材料，避免了传统有限元方法的计算负担和膜锁定问题。


<details>
  <summary>Details</summary>
Motivation: 真实布料材料因缝合、染色等过程表现出复杂性和空间变化，传统有限元方法计算量大且易受膜锁定影响，亟需高效替代方案。

Method: 将布料离散化为质量-弹簧网络，利用新的力-冲量损失函数从运动数据中学习未知材料参数。

Result: 能够准确模拟空间变化的材料特性，避免膜锁定问题，训练速度更快、重建精度更高，泛化能力优于基于图和神经ODE的方法。

Conclusion: Mass-Spring Net是一种高效且通用的框架，适用于布料材料的动态模拟，具有显著的性能优势。

Abstract: Materials used in real clothing exhibit remarkable complexity and spatial
variation due to common processes such as stitching, hemming, dyeing, printing,
padding, and bonding. Simulating these materials, for instance using finite
element methods, is often computationally demanding and slow. Worse, such
methods can suffer from numerical artifacts called ``membrane locking'' that
makes cloth appear artificially stiff. Here we propose a general framework,
called Mass-Spring Net, for learning a simple yet efficient surrogate model
that captures the effects of these complex materials using only motion
observations. The cloth is discretized into a mass-spring network with unknown
material parameters that are learned directly from the motion data, using a
novel force-and-impulse loss function. Our approach demonstrates the ability to
accurately model spatially varying material properties from a variety of data
sources, and immunity to membrane locking which plagues FEM-based simulations.
Compared to graph-based networks and neural ODE-based architectures, our method
achieves significantly faster training times, higher reconstruction accuracy,
and improved generalization to novel dynamic scenarios.

</details>


### [56] [BANG: Dividing 3D Assets via Generative Exploded Dynamics](https://arxiv.org/abs/2507.21493)
*Longwen Zhang,Qixuan Zhang,Haoran Jiang,Yinuo Bai,Wei Yang,Lan Xu,Jingyi Yu*

Main category: cs.GR

TL;DR: BANG是一种新颖的生成方法，通过‘生成爆炸动力学’实现直观的部分级3D对象分解，结合潜在扩散模型和多模态交互，提升了3D创作的灵活性和控制性。


<details>
  <summary>Details</summary>
Motivation: 当前3D设计工具需要大量专业技能和手工劳动，难以模拟人类自然的创作过程。BANG旨在通过部分级分解和生成技术，实现更直观和灵活的3D创作。

Method: BANG利用预训练的潜在扩散模型，结合轻量级爆炸视图适配器和时间注意力模块，生成平滑的爆炸状态序列。支持空间提示（如边界框）和多模态交互（如GPT-4）以控制分解过程。

Result: BANG能够生成详细的部分级几何形状，关联功能描述，并支持3D打印和组装。其工作流提升了从概念到3D资产的转换效率。

Conclusion: BANG为3D创作提供了新的视角，通过生成爆炸动力学和多模态交互，显著提升了创作的自然性和实用性。

Abstract: 3D creation has always been a unique human strength, driven by our ability to
deconstruct and reassemble objects using our eyes, mind and hand. However,
current 3D design tools struggle to replicate this natural process, requiring
considerable artistic expertise and manual labor. This paper introduces BANG, a
novel generative approach that bridges 3D generation and reasoning, allowing
for intuitive and flexible part-level decomposition of 3D objects. At the heart
of BANG is "Generative Exploded Dynamics", which creates a smooth sequence of
exploded states for an input geometry, progressively separating parts while
preserving their geometric and semantic coherence.
  BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned
for exploded dynamics with a lightweight exploded view adapter, allowing
precise control over the decomposition process. It also incorporates a temporal
attention module to ensure smooth transitions and consistency across time. BANG
enhances control with spatial prompts, such as bounding boxes and surface
regions, enabling users to specify which parts to decompose and how. This
interaction can be extended with multimodal models like GPT-4, enabling
2D-to-3D manipulations for more intuitive and creative workflows.
  The capabilities of BANG extend to generating detailed part-level geometry,
associating parts with functional descriptions, and facilitating
component-aware 3D creation and manufacturing workflows. Additionally, BANG
offers applications in 3D printing, where separable parts are generated for
easy printing and reassembly. In essence, BANG enables seamless transformation
from imaginative concepts to detailed 3D assets, offering a new perspective on
creation that resonates with human intuition.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [57] [Improving SpGEMM Performance Through Matrix Reordering and Cluster-wise Computation](https://arxiv.org/abs/2507.21253)
*Abdullah Al Raqibul Islam,Helen Xu,Dong Dai,Aydın Buluç*

Main category: cs.DC

TL;DR: 本文提出了一种针对稀疏矩阵乘法（SpGEMM）的分层聚类方法，通过行重新排序和聚类计算提升性能，平均加速1.39倍，并探讨了不同算法对性能的影响。


<details>
  <summary>Details</summary>
Motivation: SpGEMM在科学计算和图形处理中应用广泛，但受限于不规则内存访问导致的数据移动瓶颈。现有研究主要集中在稀疏矩阵-向量乘法（SpMV）的优化，而对SpGEMM的关注不足。

Method: 采用分层聚类方法，结合行重新排序和聚类计算，提出了新的矩阵格式和访问模式，以减少B矩阵的重复计算。同时分析了10种重新排序算法和3种聚类方案。

Result: 分层聚类方法平均加速1.39倍，预处理成本低（90%的输入情况下预处理时间不超过一次SpGEMM计算的20倍）。基于图分区的重新排序性能最佳但预处理时间较高。

Conclusion: 分层聚类为SpGEMM提供了显著的性能提升，同时揭示了重新排序和聚类各自及联合作用的效果。该方法预处理成本低且可独立应用。

Abstract: Sparse matrix-sparse matrix multiplication (SpGEMM) is a key kernel in many
scientific applications and graph workloads. Unfortunately, SpGEMM is
bottlenecked by data movement due to its irregular memory access patterns.
Significant work has been devoted to developing row reordering schemes towards
improving locality in sparse operations, but prior studies mostly focus on the
case of sparse-matrix vector multiplication (SpMV).
  In this paper, we address these issues with hierarchical clustering for
SpGEMM that leverages both row reordering and cluster-wise computation to
improve reuse in the second input (B) matrix with a novel row-clustered matrix
format and access pattern in the first input (A) matrix. We find that
hierarchical clustering can speed up SpGEMM by 1.39x on average with low
preprocessing cost (less than 20x the cost of a single SpGEMM on about 90% of
inputs). Furthermore, we decouple the reordering algorithm from the clustered
matrix format so they can be applied as independent optimizations.
  Additionally, this paper sheds light on the role of both row reordering and
clustering independently and together for SpGEMM with a comprehensive empirical
study of the effect of 10 different reordering algorithms and 3 clustering
schemes on SpGEMM performance on a suite of 110 matrices. We find that
reordering based on graph partitioning provides better SpGEMM performance than
existing alternatives at the cost of high preprocessing time. The evaluation
demonstrates that the proposed hierarchical clustering method achieves greater
average speedup compared to other reordering schemes with similar preprocessing
times.

</details>


### [58] [Using Containers to Speed Up Development, to Run Integration Tests and to Teach About Distributed Systems](https://arxiv.org/abs/2507.21464)
*Marco Mambelli,Bruno Moreira Coimbra,Namratha Urs,Ilya Baburashvili*

Main category: cs.DC

TL;DR: GlideinWMS工作空间的容器化开发与测试环境，简化了开发、调试和团队培训。


<details>
  <summary>Details</summary>
Motivation: 为GlideinWMS开发者和学生提供易于使用的容器化工作空间，以便于开发、测试和学习。

Method: 构建基于三个核心容器的系统（单节点集群、工厂调度器和前端），并评估不同容器的性能和易用性。

Result: 工作空间成功简化了开发、调试和团队培训，并在离线环境下也能有效运行。

Conclusion: GlideinWMS工作空间显著提升了开发效率和团队学习体验。

Abstract: GlideinWMS is a workload manager provisioning resources for many experiments,
including CMS and DUNE. The software is distributed both as native packages and
specialized production containers. Following an approach used in other
communities like web development, we built our workspaces, system-like
containers to ease development and testing. Developers can change the source
tree or check out a different branch and quickly reconfigure the services to
see the effect of their changes. In this paper, we will talk about what
differentiates workspaces from other containers. We will describe our base
system, composed of three containers: a one-node cluster including a compute
element and a batch system, a GlideinWMS Factory controlling pilot jobs, and a
scheduler and Frontend to submit jobs and provision resources. Additional
containers can be used for optional components. This system can easily run on a
laptop, and we will share our evaluation of different container runtimes, with
an eye for ease of use and performance. Finally, we will talk about our
experience as developers and with students. The GlideinWMS workspaces are
easily integrated with IDEs like VS Code, simplifying debugging and allowing
development and testing of the system even when offline. They simplified the
training and onboarding of new team members and summer interns. And they were
useful in workshops where students could have first-hand experience with the
mechanisms and components that, in production, run millions of jobs.

</details>


### [59] [GlideinBenchmark: collecting resource information to optimize provisioning](https://arxiv.org/abs/2507.21472)
*Marco Mambelli,Shrijan Swaminathan*

Main category: cs.DC

TL;DR: GlideinBenchmark是一个基于GlideinWMS的Web应用，用于自动化资源基准测试，帮助优化资源选择，降低成本。


<details>
  <summary>Details</summary>
Motivation: 选择合适的资源可以加速任务完成、提高硬件利用率并降低成本。

Method: 利用GlideinWMS的基础设施开发GlideinBenchmark，自动化执行基准测试。

Result: 实验可以通过基准测试数据优化资源选择，调度器可据此优化资源供应。

Conclusion: GlideinBenchmark通过自动化基准测试显着提升了资源选择的效率和成本效益。

Abstract: Choosing the right resource can speed up job completion, better utilize the
available hardware, and visibly reduce costs, especially when renting computers
in the cloud. This was demonstrated in earlier studies on HEPCloud. However,
the benchmarking of the resources proved to be a laborious and time-consuming
process. This paper presents GlideinBenchmark, a new Web application leveraging
the pilot infrastructure of GlideinWMS to benchmark resources, and it shows how
to use the data collected and published by GlideinBenchmark to automate the
optimal selection of resources. An experiment can select the benchmark or the
set of benchmarks that most closely evaluate the performance of its workflows.
GlideinBenchmark, with the help of the GlideinWMS Factory, controls the
benchmark execution. Finally, a scheduler like HEPCloud's Decision Engine can
use the results to optimize resource provisioning.

</details>


### [60] [Bridging Cache-Friendliness and Concurrency: A Locality-Optimized In-Memory B-Skiplist](https://arxiv.org/abs/2507.21492)
*Yicong Luo,Senhe Hao,Brian Wheatman,Prashant Pandey,Helen Xu*

Main category: cs.DC

TL;DR: 提出了一种改进的并发B-skiplist，通过优化缓存局部性，显著提升了性能，同时保持了传统skiplist的简单性。


<details>
  <summary>Details</summary>
Motivation: 传统skiplist因每个节点仅存储一个元素，导致缓存局部性差，影响性能。优化缓存局部性是提升内存索引性能的关键。

Method: 提出了一种单遍插入算法和对应的并发控制方案，实现了高缓存局部性的B-skiplist。

Result: 在128线程下，B-skiplist的吞吐量是现有并发skiplist的2x-9x，点查询吞吐量与优化树索引相当（0.9x-1.7x），且延迟显著更低。

Conclusion: B-skiplist在性能和延迟上优于现有并发skiplist，与优化树索引相比也有竞争力，同时保持了简单性。

Abstract: Skiplists are widely used for in-memory indexing in many key-value stores,
such as RocksDB and LevelDB, due to their ease of implementation and simple
concurrency control mechanisms. However, traditional skiplists suffer from poor
cache locality, as they store only a single element per node, leaving
performance on the table. Minimizing last-level cache misses is key to
maximizing in-memory index performance, making high cache locality essential.
In this paper, we present a practical concurrent B-skiplist that enhances cache
locality and performance while preserving the simplicity of traditional
skiplist structures and concurrency control schemes. Our key contributions
include a top-down, single-pass insertion algorithm for B-skiplists and a
corresponding simple and efficient top-down concurrency control scheme. On 128
threads, the proposed concurrent B-skiplist achieves between 2x-9x higher
throughput compared to state-of-the-art concurrent skiplist implementations,
including Facebook's concurrent skiplist from Folly and the Java
ConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves
competitive (0.9x-1.7x) throughput on point workloads compared to
state-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a
more complete picture of the performance, we also measure the latency of
skiplist and tree-based indices and find that the B-skiplist achieves between
3.5x-103x lower 99% latency compared to other concurrent skiplists and between
0.85x-64x lower 99% latency compared to tree-based indices on point workloads
with inserts.

</details>


### [61] [Collaborative State Machines: A Better Programming Model for the Cloud-Edge-IoT Continuum](https://arxiv.org/abs/2507.21685)
*Marlon Etheredge,Thomas Fahringer,Felix Erlacher,Elias Kohler,Stefan Pedratscher,Juan Aznar-Poveda,Nishant Saurabh,Adrien Lebre*

Main category: cs.DC

TL;DR: 本文提出了协作状态机（CSM）编程模型，用于解决Cloud-Edge-IoT应用中动态和状态管理的难题。CSM通过事件驱动、状态封装和协作机制实现了高效的开发。实验表明，其在多个场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有编程模型难以有效管理Cloud-Edge-IoT应用的动态和状态化特性，因此需要一种新的编程模型来应对这些挑战。

Method: 提出协作状态机（CSM）模型，支持事件驱动、状态封装和分布式协作，并开发了运行时系统。

Result: 实验表明，CSM在压力测试中吞吐量提升12倍，在监控系统和智能工厂用例中分别提升2.3倍和55倍性能。

Conclusion: CSM是一种高效且灵活的编程模型，适用于Cloud-Edge-IoT领域，显著提升了应用性能和开发效率。

Abstract: The development of Cloud-Edge-IoT applications requires robust programming
models. Existing models often struggle to manage the dynamic and stateful
nature of these applications effectively. This paper introduces the
Collaborative State Machines (CSM) programming model to address these
complexities. CSM facilitates the development of reactive, event-driven, and
stateful applications targeting the Cloud-Edge-IoT continuum. Applications
built with CSM are composed of state machines that collaborate autonomously and
can be distributed across different layers of the continuum. Key features of
CSM include (i) a sophisticated collaboration mechanism among state machines
utilizing events and persistent data; (ii) encapsulation of state through the
inherent state of state machines and persistent data; (iii) integration of
actions and service invocations within states and state transitions, thereby
decoupling complex application logic from compute and data processing services;
and (iv) an advanced data model that supports the processing of local, static,
and persistent data with defined scope and lifetime. In addition to introducing
the CSM programming model, we present a runtime system and a comprehensive
evaluation of our approach. This evaluation is based on three use cases: a
stress test on a large-scale infrastructure, a surveillance system application,
and a complex smart factory scenario, all deployed on the Grid'5000 testbed.
Our results demonstrate a 12x increase in throughput through novel language
features in the stress test. Compared to Serverless Workflow, a
state-of-the-art baseline system, we show a 2.3x improvement in processing time
per processed image in a surveillance system use case, a 55x reduction in total
processing time for a smart factory use case, and an overall improvement in
productivity across these use cases.

</details>


### [62] [The Performance of Low-Synchronization Variants of Reorthogonalized Block Classical Gram--Schmidt](https://arxiv.org/abs/2507.21791)
*Erin Carson,Yuxin Ma*

Main category: cs.DC

TL;DR: 论文比较了两种低同步成本的BCGS变体BCGSI+P-1S和BCGSI+P-2S在分布式内存系统中的性能，展示其优于传统方法和其他变体的稳定性与速度。


<details>
  <summary>Details</summary>
Motivation: 解决分布式内存系统中全局同步成本的高性能瓶颈问题，通过低同步变体提升正交化算法的效率。

Method: 评估BCGSI+P-1S和BCGSI+P-2S与其他低同步BCGS变体在分布式系统中的性能，通过数值实验对比速度和稳定性。

Result: BCGSI+P-1S和BCGSI+P-2S分别实现最高4倍和2倍加速，表现与不稳定变体相当，但稳定性更优。

Conclusion: BCGSI+P-1S和BCGSI+P-2S因其高稳定性和低同步成本，成为分布式系统中经济QR分解的最佳选择。

Abstract: Numerous applications, such as Krylov subspace solvers, make extensive use of
the block classical Gram-Schmidt (BCGS) algorithm and its reorthogonalized
variants for orthogonalizing a set of vectors. For large-scale problems in
distributed memory settings, the communication cost, particularly the global
synchronization cost, is a major performance bottleneck. In recent years, many
low-synchronization BCGS variants have been proposed in an effort to reduce the
number of synchronization points. The work [E. Carson, Y. Ma, arXiv preprint
2411.07077] recently proposed stable one-synchronization and
two-synchronization variants of BCGS, i.e., BCGSI+P-1S and BCGSI+P-2S. In this
work, we evaluate the performance of BCGSI+P-1S and BCGSI+P-2S on a distributed
memory system compared to other well-known low-synchronization BCGS variants.
In comparison to the classical reorthogonalized BCGS algorithm (BCGSI+),
numerical experiments demonstrate that BCGSI+P-1S and BCGSI+P-2S can achieve up
to 4 times and 2 times speedups, respectively, and perform similarly to other
(less stable) one-synchronization and two-synchronization variants. BCGSI+P-1S
and BCGSI+P-2S are therefore recommended as the best choice in practice for
computing an economic QR factorization on distributed memory systems due to
their superior stability when compared to other variants with the same
synchronization cost.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [63] [AI-Driven Generation of Data Contracts in Modern Data Engineering Systems](https://arxiv.org/abs/2507.21056)
*Harshraj Bhoite*

Main category: cs.DB

TL;DR: 利用大型语言模型（LLMs）自动生成数据合约的AI框架，显著减少人工工作量，提升数据治理效率。


<details>
  <summary>Details</summary>
Motivation: 随着数据管道的复杂性增加，手动编写和维护数据合约容易出错且耗时，亟需自动化解决方案。

Method: 采用参数高效微调方法（如LoRA和PEFT），使LLMs适应结构化数据领域，自动生成JSON Schema等格式的合约。

Result: 实验显示，微调后的LLMs在生成有效合约方面准确性高，人工工作量减少70%以上。

Conclusion: 生成式AI可桥接意图与实现，支持企业数据管理的可扩展和敏捷治理。

Abstract: Data contracts formalize agreements between data producers and consumers
regarding schema, semantics, and quality expectations. As data pipelines grow
in complexity, manual authoring and maintenance of contracts becomes
error-prone and labor-intensive. We present an AI-driven framework for
automatic data contract generation using large language models (LLMs). Our
system leverages parameter-efficient fine-tuning methods, including LoRA and
PEFT, to adapt LLMs to structured data domains. The models take sample data or
schema descriptions and output validated contract definitions in formats such
as JSON Schema and Avro. We integrate this framework into modern data platforms
(e.g., Databricks, Snowflake) to automate contract enforcement at scale.
Experimental results on synthetic and real-world datasets demonstrate that the
fine-tuned LLMs achieve high accuracy in generating valid contracts and reduce
manual workload by over 70%. We also discuss key challenges such as
hallucination, version control, and the need for continuous learning. This work
demonstrates that generative AI can enable scalable, agile data governance by
bridging the gap between intent and implementation in enterprise data
management.

</details>


### [64] [Digitalizing Uncertain Information](https://arxiv.org/abs/2507.21173)
*Chris Partridge,Andrew Mitchell,Andreas Cola*

Main category: cs.DB

TL;DR: 论文概述了一个基于本体的数字表单项目，用于表示不确定信息，并展示了如何通过扩展本体和Lewisian对等方法来应对数字不确定性的挑战。


<details>
  <summary>Details</summary>
Motivation: 动机是解决数字项目中不确定信息的表示问题，提高数字成熟度。

Method: 方法包括扩展本体（如BORO基础本体或信息交换标准），并通过Lewisian对等方法形式化不确定性。

Result: 结果表明，这种方法足以应对数字不确定性带来的挑战。

Conclusion: 结论表明，通过扩展本体和形式化方法，可以有效表示和处理数字不确定性。

Abstract: The paper sketches some initial results from an ongoing project to develop an
ontology-based digital form for representing uncertain information. We frame
this work as a journey from lower to higher levels of digital maturity across a
technology divide. The paper first sets a baseline by describing the basic
challenges any project dealing with digital uncertainty faces. It then
describes how the project is facing them. It shows firstly how an extensional
ontology (such as the BORO Foundational Ontology or the Information Exchange
Standard) can be extended with a Lewisian counterpart approach to formalizing
uncertainty that is adapted to computing. And then it shows how this is
expressive enough to handle the challenges. Keywords: actuality, BORO
Foundational Ontology, counterpart, Information Exchange Standard,
informational uncertainty, my doxastic actualities, two-dimensional semantics.

</details>


### [65] [Ranking Methods for Skyline Queries](https://arxiv.org/abs/2507.21860)
*Mickaël Martin-Nevot,Lotfi Lakhal*

Main category: cs.DB

TL;DR: 论文提出了一种改进的多准则决策分析方法，通过引入支配层次结构优化dp-idp方法，并提出RankSky和CoSky方法，最终结合多层Skyline提出DeepSky。


<details>
  <summary>Details</summary>
Motivation: 现有Skyline方法在高基数结果集中缺乏有效的比较机制，dp-idp方法效率低且排名不显著。

Method: 提出RankSky（基于PageRank）和CoSky（基于TOPSIS和Gini指数）方法，结合多层Skyline形成DeepSky。

Result: 实验评估了dp-idp、RankSky和CoSky的实现效果。

Conclusion: 论文通过多种方法改进多准则决策分析的效率和排名效果，最终提出综合方案DeepSky。

Abstract: {Multi-criteria decision analysis in databases has been actively studied,
especially through the Skyline operator. Yet, few approaches offer a relevant
comparison of Pareto optimal, or Skyline, points for high cardinality result
sets. We propose to improve the dp-idp method, inspired by tf-idf, a recent
approach computing a score for each Skyline point, by introducing the concept
of dominance hierarchy. As dp-idp lacks efficiency and does not ensure a
distinctive rank, we introduce the RankSky method, the adaptation of Google's
well-known PageRank solution, using a square stochastic matrix, a teleportation
matrix, a damping factor, and then a row score eigenvector and the IPL
algorithm. For the same reasons as RankSky, and also to offer directly
embeddable in DBMS solution, we establish the TOPSIS based CoSky method,
derived from both information research and multi-criteria analysis. CoSky
automatically ponderates normalized attributes using the Gini index, then
computes a score using Salton's cosine toward an ideal point. By coupling
multilevel Skyline to dp-idp, RankSky or CoSky, we introduce DeepSky.
Implementations of dp-idp, RankSky and CoSky are evaluated experimentally.

</details>


### [66] [Benchmarking Filtered Approximate Nearest Neighbor Search Algorithms on Transformer-based Embedding Vectors](https://arxiv.org/abs/2507.21989)
*Patrick Iff,Paul Bruegger,Marcin Chrapek,Maciej Besta,Torsten Hoefler*

Main category: cs.DB

TL;DR: 本文综述了过滤近似最近邻搜索（FANNS）的方法，并分析了其文献基准测试中的问题，提出了一种基于arXiv论文摘要的新型数据集，并评估了多种FANNS方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前FANNS领域缺乏多样化和现实数据集的问题，特别是基于最新Transformer文本嵌入模型的数据集。

Method: 引入一个包含270万篇arXiv论文摘要嵌入向量的数据集，其中包含11种真实属性（如作者和类别），并对多种FANNS方法进行了基准测试。

Result: 发现没有单一方法在所有场景下表现最佳；不同方法各有优劣，例如ACORN支持多种过滤类型但常被更专用方法超越，而SeRF在有序属性上表现优异但对分类属性无效。

Conclusion: FANNS领域不存在普遍最优的方法，不同场景需要针对性的方法选择。

Abstract: Advances in embedding models for text, image, audio, and video drive progress
across multiple domains, including retrieval-augmented generation,
recommendation systems, vehicle/person reidentification, and face recognition.
Many applications in these domains require an efficient method to retrieve
items that are close to a given query in the embedding space while satisfying a
filter condition based on the item's attributes, a problem known as Filtered
Approximate Nearest Neighbor Search (FANNS). In this work, we present a
comprehensive survey and taxonomy of FANNS methods and analyze how they are
benchmarked in the literature. By doing so, we identify a key challenge in the
current FANNS landscape: the lack of diverse and realistic datasets,
particularly ones derived from the latest transformer-based text embedding
models. To address this, we introduce a novel dataset consisting of embedding
vectors for the abstracts of over 2.7 million research articles from the arXiv
repository, accompanied by 11 real-world attributes such as authors and
categories. We benchmark a wide range of FANNS methods on our novel dataset and
find that each method has distinct strengths and limitations; no single
approach performs best across all scenarios. ACORN, for example, supports
various filter types and performs reliably across dataset scales but is often
outperformed by more specialized methods. SeRF shows excellent performance for
range filtering on ordered attributes but cannot handle categorical attributes.
Filtered-DiskANN and UNG excel on the medium-scale dataset but fail on the
large-scale dataset, highlighting the challenge posed by transformer-based
embeddings, which are often more than an order of magnitude larger than earlier
embeddings. We conclude that no universally best method exists.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [67] [A Grover-Based Quantum Algorithm for Solving Perfect Mazes via Fitness-Guided Search](https://arxiv.org/abs/2507.21937)
*Michelle L. Wu*

Main category: quant-ph

TL;DR: 提出了一个基于Grover振幅放大量子算法的完美迷宫求解方法，将路径搜索问题转化为结构化搜索问题。


<details>
  <summary>Details</summary>
Motivation: 通过量子算法提升迷宫路径搜索的效率，并探索其在其他搜索领域的扩展应用。

Method: 利用Grover的振幅放大技术，编码所有候选路径为叠加态，并通过可逆适应度算子评估路径接近目标的程度，采用自适应截止策略迭代优化搜索。

Result: 算法在迷宫尺寸和路径长度上表现高效，提供了完整的量子-混合路径规划框架。

Conclusion: 该算法不仅适用于迷宫求解，还可扩展到树状或无环图等搜索领域，为量子-混合路径规划奠定了基础。

Abstract: We present a quantum algorithm for solving perfect mazes by casting the
pathfinding task as a structured search problem. Building on Grover's amplitude
amplification, the algorithm encodes all candidate paths in superposition and
evaluates their proximity to the goal using a reversible fitness operator based
on quantum arithmetic. A Grover-compatible oracle marks high-fitness states,
and an adaptive cutoff strategy refines the search iteratively. We provide
formal definitions, unitary constructions, and convergence guarantees, along
with a resource analysis showing efficient scaling with maze size and path
length. The framework serves as a foundation for quantum-hybrid pathfinding and
planning. The full algorithmic pipeline is specified from encoding to
amplification, including oracle design and fitness evaluation. The approach is
readily extensible to other search domains, including navigation over tree-like
or acyclic graphs.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [68] [Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual Representations](https://arxiv.org/abs/2507.21448)
*Teng,Ma,Sile Yin,Li-Chia Yang,Shuo Zhang*

Main category: eess.AS

TL;DR: 论文提出了一种实时音频-视觉语音增强系统RAVEN，通过结合音频和视觉信息来增强目标说话者的语音，并在低信噪比和多说话者环境中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统音频语音增强在干扰说话者和噪声存在时效果不佳，因此需要结合视觉信息以提高性能。

Method: RAVEN系统利用音频-视觉语音识别（AVSR）和主动说话者检测（ASD）的视觉嵌入，通过拼接这些嵌入来提升性能，并开发了一个实时流式处理系统。

Result: 在低信噪比和多说话者环境中，拼接AVSR和ASD嵌入效果最好；纯噪声场景中AVSR嵌入表现最佳。系统成功在CPU上实现实时运行，并开源了代码。

Conclusion: RAVEN通过结合音频和视觉信息显著提升语音增强性能，是首个开源的实时AVSE系统。

Abstract: Speech enhancement in audio-only settings remains challenging, particularly
in the presence of interfering speakers. This paper presents a simple yet
effective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which
isolates and enhances the on-screen target speaker while suppressing
interfering speakers and background noise. We investigate how visual embeddings
learned from audio-visual speech recognition (AVSR) and active speaker
detection (ASD) contribute to AVSE across different SNR conditions and numbers
of interfering speakers. Our results show concatenating embeddings from AVSR
and ASD models provides the greatest improvement in low-SNR, multi-speaker
environments, while AVSR embeddings alone perform best in noise-only scenarios.
In addition, we develop a real-time streaming system that operates on a
computer CPU and we provide a video demonstration and code repository. To our
knowledge, this is the first open-source implementation of a real-time AVSE
system.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [69] [A Novel Framework for Near-Field Covert Communications with RIS and RSMA](https://arxiv.org/abs/2507.21956)
*Atiquzzaman Mondal,Amira Bendaimi,Huseyin Arslan*

Main category: eess.SP

TL;DR: 该论文研究了利用速率分割多址(RSMA)和可重构智能表面(RIS)的近场(NF)隐蔽通信。通过优化波束成形和算法设计，系统在隐蔽性和通信速率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索近场环境下隐蔽通信的潜力，结合RSMA和RIS技术以提高通信速率并降低敌方检测能力。

Method: 使用RSMA增加隐蔽通信速率，RIS增强用户信号并抑制敌方检测。提出了波束成形优化问题和交替优化算法。

Result: 仿真结果表明，所提算法显著提升了隐蔽通信速率，展示了RSMA和RIS在隐蔽通信中的协同作用。

Conclusion: 近场RSMA-RIS集成在隐蔽通信中具有显著潜力，优化算法有效提升了系统性能。

Abstract: This paper explores the near field (NF) covert communication with the aid of
rate-splitting multiple access (RSMA) and reconfigurable intelligent surfaces
(RIS). In particular, the RIS operates in the NF of both the legitimate user
and the passive adversary, enhancing the legitimate users received signal while
suppressing the adversarys detection capability. Whereas, the base station (BS)
applies RSMA to increase the covert communication rate composed of a private
and a shared rate component. To characterize system covertness, we derive
closed form expressions for the detection error probability (DEP), outage
probability (OP), and optimal detection threshold for the adversary. We
formulate a non-convex joint beamforming optimization problem at the BS and RIS
under unit-modulus constraints to maximize the covert rate. To tackle this, we
propose an alternating optimization (AO) algorithm, where the BS beamformer is
designed using a two-stage iterative method based on successive convex
approximation (SCA). Additionally, two low-complexity techniques are introduced
to further reduce the adversarys received power. Simulation results demonstrate
that the proposed algorithm effectively improves the covert communication rate,
highlighting the potential of near field RSMA-RIS integration in covert
communication.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [70] [StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation](https://arxiv.org/abs/2507.21340)
*Satyananda Kashyap,Sola Shirai,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.CL

TL;DR: 该论文提出了StructText框架，用于自动生成高质量基准数据集，以评估从文本中提取结构化信息（如键值对）的质量。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏评估大型语言模型（LLM）在特定领域或组织文档中提取结构化信息质量的基准数据集，手动标注成本高且难以扩展。

Method: 提出两阶段“计划-执行”流水线，利用现有表格数据自动生成相应的自然语言文本，并通过多维评估策略确保文本与结构化数据的对齐。

Result: 在49个数据集上验证了71,539个示例，结果显示LLMs在事实性和避免幻觉方面表现良好，但生成的文本在叙事连贯性和可提取性上存在问题。

Conclusion: StructText为结构化信息提取研究提供了框架、数据集和工具，支持后续工作的扩展和改进。

Abstract: Extracting structured information from text, such as key-value pairs that
could augment tabular data, is quite useful in many enterprise use cases.
Although large language models (LLMs) have enabled numerous automated pipelines
for converting natural language into structured formats, there is still a lack
of benchmarks for evaluating their extraction quality, especially in specific
domains or focused documents specific to a given organization. Building such
benchmarks by manual annotations is labour-intensive and limits the size and
scalability of the benchmarks. In this work, we present StructText, an
end-to-end framework for automatically generating high-fidelity benchmarks for
key-value extraction from text using existing tabular data. It uses available
tabular data as structured ground truth, and follows a two-stage
``plan-then-execute'' pipeline to synthetically generate corresponding
natural-language text. To ensure alignment between text and structured source,
we introduce a multi-dimensional evaluation strategy that combines (a)
LLM-based judgments on factuality, hallucination, and coherence and (b)
objective extraction metrics measuring numeric and temporal accuracy. We
evaluated the proposed method on 71,539 examples across 49 datasets. Results
reveal that while LLMs achieve strong factual accuracy and avoid hallucination,
they struggle with narrative coherence in producing extractable text. Notably,
models presume numerical and temporal information with high fidelity yet this
information becomes embedded in narratives that resist automated extraction. We
release a framework, including datasets, evaluation tools, and baseline
extraction systems, to support continued research.

</details>


### [71] [ChartMark: A Structured Grammar for Chart Annotation](https://arxiv.org/abs/2507.21810)
*Yiyu Chen,Yifan Wu,Shuyu Shen,Yupeng Xie,Leixian Shen,Hui Xiong,Yuyu Luo*

Main category: cs.CL

TL;DR: ChartMark提出了一种结构化语法，将注释语义与可视化实现分离，支持跨平台重用。


<details>
  <summary>Details</summary>
Motivation: 解决图表注释碎片化和非标准化问题，提升可视化可访问性。

Method: 采用分层框架，映射注释维度（如任务、图表上下文），支持抽象意图和具体视觉细节。

Result: 工具包成功将ChartMark规范转换为Vega-Lite可视化，展示了其灵活性、表达力和实用性。

Conclusion: ChartMark通过结构化语法和分层框架，有效解决了注释标准化问题，具有广泛应用潜力。

Abstract: Chart annotations enhance visualization accessibility but suffer from
fragmented, non-standardized representations that limit cross-platform reuse.
We propose ChartMark, a structured grammar that separates annotation semantics
from visualization implementations. ChartMark features a hierarchical framework
mapping onto annotation dimensions (e.g., task, chart context), supporting both
abstract intents and precise visual details. Our toolkit demonstrates
converting ChartMark specifications into Vega-Lite visualizations, highlighting
its flexibility, expressiveness, and practical applicability.

</details>


### [72] [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)
*Sabrina Patania,Luca Annese,Cansu Koyuturk,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.CL

TL;DR: 研究表明，通过社交化学习范式（如对话式教学）可以显著提升大语言模型（LLM）的知识获取能力，优于传统单向教学方法。


<details>
  <summary>Details</summary>
Motivation: 传统AI训练方法依赖大数据集和稀疏反馈，限制了模型从交互中高效学习的能力。受维果茨基社会文化理论启发，研究探索社交化学习范式是否有效。

Method: 在‘AI Social Gym’动态环境中，AI学习者与教师通过对话式互动学习，特别是混合方向（结合自上而下解释与学习者提问）的教学策略。

Result: 实验表明，对话式方法（尤其是混合方向互动）显著提升LLM知识获取与应用能力，优于单向教学或直接访问结构化知识。

Conclusion: 将教学和心理洞见融入AI训练可大幅提升知识获取和响应质量，为现有策略（如提示工程）提供了补充路径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
processing extensive offline datasets. However, they often face challenges in
acquiring and integrating complex, knowledge online. Traditional AI training
paradigms, predominantly based on supervised learning or reinforcement
learning, mirror a 'Piagetian' model of independent exploration. These
approaches typically rely on large datasets and sparse feedback signals,
limiting the models' ability to learn efficiently from interactions. Drawing
inspiration from Vygotsky's sociocultural theory, this study explores the
potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI
learner agent engages in dyadic pedagogical dialogues with knowledgeable AI
teacher agents. These interactions emphasize external, structured dialogue as a
core mechanism for knowledge acquisition, contrasting with methods that depend
solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the
AI learning process in the context of ontology acquisition. Empirical results
indicate that such dialogic approaches-particularly those involving
mixed-direction interactions combining top-down explanations with
learner-initiated questioning-significantly enhance the LLM's ability to
acquire and apply new knowledge, outperforming both unidirectional
instructional methods and direct access to structured knowledge, formats
typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological
insights into AI and robot training can substantially improve post-training
knowledge acquisition and response quality. This approach offers a
complementary pathway to existing strategies like prompt engineering

</details>


### [73] [Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing](https://arxiv.org/abs/2507.21073)
*David James Woo,Yangyang Yu,Kai Guo,Yilin Huang,April Ka Yeng Fung*

Main category: cs.CL

TL;DR: 研究探讨了EFL中学生在写作过程中如何编辑AI生成的文本，分析了编辑行为对其说明文写作质量的影响，发现AI生成的文字数量对所有评分维度有正面预测作用，而编辑行为的改善效果有限。


<details>
  <summary>Details</summary>
Motivation: 随着AI聊天机器人在EFL写作中的普及，其对学生的写作过程和作品质量的影响尚未充分研究。本研究旨在填补这一空白。

Method: 采用混合方法设计，分析39名香港中学生的屏幕记录和写作作品，结合定性编码、描述性统计、时间序列分析、人工评分和多线性回归分析。

Result: 研究发现，学生的编辑行为与作品质量提升关联不大，AI生成的文字数量对评分有积极作用，表明AI是辅助工具而非替代品。

Conclusion: 研究强调在AI整合前需进行特定类型的写作指导和过程训练，并开发评估系统以重视写作过程和结果。

Abstract: Text generated by artificial intelligence (AI) chatbots is increasingly used
in English as a foreign language (EFL) writing contexts, yet its impact on
students' expository writing process and compositions remains understudied.
This research examines how EFL secondary students edit AI-generated text.
Exploring editing behaviors in their expository writing process and in
expository compositions, and their effect on human-rated scores for content,
organization, language, and overall quality. Participants were 39 Hong Kong
secondary students who wrote an expository composition with AI chatbots in a
workshop. A convergent design was employed to analyze their screen recordings
and compositions to examine students' editing behaviors and writing qualities.
Analytical methods included qualitative coding, descriptive statistics,
temporal sequence analysis, human-rated scoring, and multiple linear regression
analysis. We analyzed over 260 edits per dataset, and identified two editing
patterns: one where students refined introductory units repeatedly before
progressing, and another where they quickly shifted to extensive edits in body
units (e.g., topic and supporting sentences). MLR analyses revealed that the
number of AI-generated words positively predicted all score dimensions, while
most editing variables showed minimal impact. These results suggest a
disconnect between students' significant editing effort and improved
composition quality, indicating AI supports but does not replace writing
skills. The findings highlight the importance of genre-specific instruction and
process-focused writing before AI integration. Educators should also develop
assessments valuing both process and product to encourage critical engagement
with AI text.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [74] [FedFlex: Federated Learning for Diverse Netflix Recommendations](https://arxiv.org/abs/2507.21115)
*Sven Lankester,Manel Slokom,Gustavo de Carvalho Bertoli,Matias Vizcaino,Emmanuelle Beauxis Aussalet,Laura Hollink*

Main category: cs.IR

TL;DR: 本文提出了FedFlex，一种联邦学习推荐系统，结合矩阵分解和MMR重排序以提升多样性，实验表明其在保证用户满意度的同时增加了推荐内容的多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦推荐系统主要关注准确性，而忽略了公平性和多样性。本文旨在解决这一问题，提出一种兼顾多样性和用户满意度的联邦推荐系统。

Method: 结合SVD和BPR两种矩阵分解算法进行个性化微调，并应用MMR算法对推荐结果进行重排序以增强多样性。

Result: 通过两周期用户实验，证明FedFlex能有效引入多样化内容（如新类型），同时保持用户满意度。

Conclusion: FedFlex在联邦推荐系统中成功平衡了多样性和用户满意度，为未来研究提供了新方向。

Abstract: Federated learning is a decentralized approach that enables collaborative
model training across multiple devices while preserving data privacy. It has
shown significant potential in various domains, including healthcare and
personalized recommendation systems. However, most existing work on federated
recommendation systems has focused primarily on improving accuracy, with
limited attention to fairness and diversity. In this paper, we introduce
FedFlex, a federated recommender system for Netflix-style TV series
recommendations. FedFlex integrates two state-of-the-art matrix factorization
algorithms for personalized fine-tuning. FedFlex also applies Maximal Marginal
Relevance (MMR) to re-rank items and enhance diversity. We conduct extensive
experiments comparing recommendations generated by SVD and BPR algorithms. In a
live two-week user study, participants received two recommendation lists: List
A, based on SVD or BPR, and List B, a re-ranked version emphasizing diversity.
Participants were asked to click on the movies they were interested in
watching. Our findings demonstrate that FedFlex effectively introduces diverse
content, such as new genres, into recommendations without necessarily
compromising user satisfaction.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [75] [Sound Source Localization for Human-Robot Interaction in Outdoor Environments](https://arxiv.org/abs/2507.21431)
*Victor Liu,Timothy Du,Jordy Sehn,Jack Collier,François Grondin*

Main category: cs.RO

TL;DR: 提出了一种基于麦克风阵列的声源定位方法，结合信号粗对齐和时域声学回声消除算法，显著提高了噪声环境下的定位精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决在噪声环境中准确识别操作员声源方向的问题，以便实现更丰富的交互。

Method: 使用麦克风阵列和异步近距离麦克风，结合信号粗对齐和时域声学回声消除算法，估算理想比率掩码以隔离目标语音。

Result: 在信噪比为1dB时，平均角度误差为4度，95%的情况下误差在5度以内，显著优于现有方法。

Conclusion: 该方法在噪声环境下实现了高精度的声源定位，为机器人交互提供了可靠的方向信息。

Abstract: This paper presents a sound source localization strategy that relies on a
microphone array embedded in an unmanned ground vehicle and an asynchronous
close-talking microphone near the operator. A signal coarse alignment strategy
is combined with a time-domain acoustic echo cancellation algorithm to estimate
a time-frequency ideal ratio mask to isolate the target speech from
interferences and environmental noise. This allows selective sound source
localization, and provides the robot with the direction of arrival of sound
from the active operator, which enables rich interaction in noisy scenarios.
Results demonstrate an average angle error of 4 degrees and an accuracy within
5 degrees of 95\% at a signal-to-noise ratio of 1dB, which is significantly
superior to the state-of-the-art localization methods.

</details>


### [76] [Evaluating Interactions between Automated Vehicles and Cyclists using a coupled In-the-Loop Test Environment](https://arxiv.org/abs/2507.21859)
*Michael Kaiser,Clemens Groß,Lisa Marie Otto,Steffen Müller*

Main category: cs.RO

TL;DR: 本文提出并通过验证了一种耦合的闭环测试环境，将自行车手与自动驾驶车辆通过虚拟环境（Unreal Engine 5）实时交互，用于提升自动驾驶系统与弱势道路使用者的安全性测试。


<details>
  <summary>Details</summary>
Motivation: 为了提高自动驾驶系统与弱势道路使用者（如骑行者）交互的安全性测试的真实性。

Method: 开发了一个耦合的闭环测试环境，结合自行车手和自动驾驶车辆的实时双向交互，并利用虚拟环境（Unreal Engine 5）实现。

Result: 验证实验表明该方法可行，并展示了其在实际自动驾驶系统评测中的优势和局限性。

Conclusion: 这种耦合测试环境为自动驾驶系统与弱势道路使用者的交互提供了一种真实且可控的评估方法。

Abstract: Testing and evaluating automated driving systems (ADS) in interactions with
vulnerable road users (VRUs), such as cyclists, are essential for improving the
safety of VRUs, but often lack realism. This paper presents and validates a
coupled in-the-loop test environment that integrates a Cyclist-in-the Loop test
bench with a Vehicle-in-the-Loop test bench via a virtual environment (VE)
developed in Unreal Engine 5. The setup enables closed-loop, bidirectional
interaction between a real human cyclist and a real automated vehicle under
safe and controllable conditions. The automated vehicle reacts to cyclist
gestures via stimulated camera input, while the cyclist, riding a stationary
bicycle, perceives and reacts to the vehicle in the VE in real time. Validation
experiments are conducted using a real automated shuttle bus with a
track-and-follow function, performing three test maneuvers - straight-line
driving with stop, circular track driving, and double lane change - on a
proving ground and in the coupled in-the-loop test environment. The performance
is evaluated by comparing the resulting vehicle trajectories in both
environments. Additionally, the introduced latencies of individual components
in the test setup are measured. The results demonstrate the feasibility of the
approach and highlight its strengths and limitations for realistic ADS
evaluation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [77] [Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks](https://arxiv.org/abs/2507.21974)
*Mohamed Sana,Nicola Piovesan,Antonio De Domenico,Yibin Kang,Haozhe Zhang,Merouane Debbah,Fadhel Ayed*

Main category: cs.AI

TL;DR: 提出了一种轻量级框架，利用大语言模型（LLM）进行移动网络中的根因分析（RCA），并通过两阶段训练方法提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 移动网络中的RCA需要可解释性、领域专业知识和因果推理能力，现有开源LLM在这方面表现不佳，需要领域适配。

Method: 引入TeleLogs数据集，提出结合监督微调和强化学习的训练方法，以提升LLM的推理能力和准确性。

Result: 多规模LLM实验表明该方法显著优于现有推理和非推理模型，且能泛化到随机测试变体。

Conclusion: 领域适配和推理增强的LLM在网络运维中具有实用性和可解释性潜力。

Abstract: Root Cause Analysis (RCA) in mobile networks remains a challenging task due
to the need for interpretability, domain expertise, and causal reasoning. In
this work, we propose a lightweight framework that leverages Large Language
Models (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of
annotated troubleshooting problems designed to benchmark RCA capabilities. Our
evaluation reveals that existing open-source reasoning LLMs struggle with these
problems, underscoring the need for domain-specific adaptation. To address this
issue, we propose a two-stage training methodology that combines supervised
fine-tuning with reinforcement learning to improve the accuracy and reasoning
quality of LLMs. The proposed approach fine-tunes a series of RCA models to
integrate domain knowledge and generate structured, multi-step diagnostic
explanations, improving both interpretability and effectiveness. Extensive
experiments across multiple LLM sizes show significant performance gains over
state-of-the-art reasoning and non-reasoning models, including strong
generalization to randomized test variants. These results demonstrate the
promise of domain-adapted, reasoning-enhanced LLMs for practical and
explainable RCA in network operation and management.

</details>


### [78] [LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems](https://arxiv.org/abs/2507.21276)
*Yufei Li,Zexin Li,Yinglun Zhu,Cong Liu*

Main category: cs.AI

TL;DR: 论文提出了LeMix系统，通过联合调度LLM的推理和训练任务，提高资源利用率和响应速度，实验结果显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有部署方式将LLM的推理和训练任务分离，导致资源利用不足和延迟问题，亟需高效解决方案。

Method: 设计LeMix系统，整合离线分析、执行预测和动态调度，根据任务特性和系统状态分配资源。

Result: LeMix将吞吐量提升3.53倍，推理损失减少0.61倍，响应时间SLO达标率提高2.12倍。

Conclusion: LeMix首次实现LLM推理与训练的联合调度，为生产环境中的高效部署提供了新思路。

Abstract: Modern deployment of large language models (LLMs) frequently involves both
inference serving and continuous retraining to stay aligned with evolving data
and user feedback. Common practices separate these workloads onto distinct
servers in isolated phases, causing substantial inefficiencies (e.g., GPU
idleness) and delayed adaptation to new data in distributed settings. Our
empirical analysis reveals that these inefficiencies stem from dynamic request
arrivals during serving and workload heterogeneity in pipeline-parallel
training. To address these challenges, we propose LeMix, a system for
co-locating and managing concurrent LLM serving and training workloads. LeMix
integrates offline profiling, execution prediction mechanisms, and runtime
scheduling to dynamically adapt resource allocation based on workload
characteristics and system conditions. By understanding task-specific behaviors
and co-execution interference across shared nodes, LeMix improves utilization
and serving quality without compromising serving responsiveness. Our evaluation
shows that LeMix improves throughput by up to 3.53x, reduces inference loss by
up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over
traditional separate setups. To our knowledge, this is the first work to
uncover and exploit the opportunities of joint LLM inference and training,
paving the way for more resource-efficient deployment of LLMs in production
environments.

</details>


### [79] [The Impact of Foundational Models on Patient-Centric e-Health Systems](https://arxiv.org/abs/2507.21882)
*Elmira Onagh,Alireza Davoodi,Maleknaz Nayebi*

Main category: cs.AI

TL;DR: 论文研究了患者为中心的医疗应用中AI的成熟度，发现大部分应用仍处于早期阶段。


<details>
  <summary>Details</summary>
Motivation: 评估AI在医疗技术中的信任度、透明度和实际影响。

Method: 利用LLM提取功能特征，并基于Gartner AI成熟度模型分类。

Result: 86.21%的应用处于AI早期阶段，仅13.79%达到高级阶段。

Conclusion: AI在患者为中心的医疗应用中成熟度较低，需进一步发展。

Abstract: As Artificial Intelligence (AI) becomes increasingly embedded in healthcare
technologies, understanding the maturity of AI in patient-centric applications
is critical for evaluating its trustworthiness, transparency, and real-world
impact. In this study, we investigate the integration and maturity of AI
feature integration in 116 patient-centric healthcare applications. Using Large
Language Models (LLMs), we extracted key functional features, which are then
categorized into different stages of the Gartner AI maturity model. Our results
show that over 86.21\% of applications remain at the early stages of AI
integration, while only 13.79% demonstrate advanced AI integration.

</details>


### [80] [LLM-based Content Classification Approach for GitHub Repositories by the README Files](https://arxiv.org/abs/2507.21899)
*Malik Uzair Mehmood,Shahid Hussain,Wen Li Wang,Muhammad Usama Malik*

Main category: cs.AI

TL;DR: GitHub 仓库的 README 文件完善程度影响其采用率，研究利用 LLMs 自动分类 README 内容，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: GitHub README 文件缺乏详细信息可能影响仓库的潜力，希望通过自动化工具提升其识别和使用率。

Method: 使用 BERT、DistilBERT 和 RoBERTa 三种 LLMs，通过微调技术（含 PEFT）对 4226 个 README 文件片段进行分类。

Result: 方法整体 F1 分数达 0.98，PEFT 技术在减少计算成本的同时保持了高分类性能。

Conclusion: LLMs 可高效分类 README 文件，为 GitHub 仓库的自动化工具开发提供了新思路。

Abstract: GitHub is the world's most popular platform for storing, sharing, and
managing code. Every GitHub repository has a README file associated with it.
The README files should contain project-related information as per the
recommendations of GitHub to support the usage and improvement of repositories.
However, GitHub repository owners sometimes neglected these recommendations.
This prevents a GitHub repository from reaching its full potential. This
research posits that the comprehensiveness of a GitHub repository's README file
significantly influences its adoption and utilization, with a lack of detail
potentially hindering its full potential for widespread engagement and impact
within the research community. Large Language Models (LLMs) have shown great
performance in many text-based tasks including text classification, text
generation, text summarization and text translation. In this study, an approach
is developed to fine-tune LLMs for automatically classifying different sections
of GitHub README files. Three encoder-only LLMs are utilized, including BERT,
DistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a
gold-standard dataset consisting of 4226 README file sections. This approach
outperforms current state-of-the-art methods and has achieved an overall F1
score of 0.98. Moreover, we have also investigated the use of
Parameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation
(LoRA) and shown an economical alternative to full fine-tuning without
compromising much performance. The results demonstrate the potential of using
LLMs in designing an automatic classifier for categorizing the content of
GitHub README files. Consequently, this study contributes to the development of
automated tools for GitHub repositories to improve their identifications and
potential usages.

</details>


### [81] [SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration](https://arxiv.org/abs/2507.21067)
*Jan Kapusta*

Main category: cs.AI

TL;DR: 论文提出了共生认识论作为人机认知合作的哲学基础，并介绍了SynLang作为透明人机协作的正式协议。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统的不透明性阻碍了人类监督和协作潜力，传统可解释AI方法无法实现真正共生合作。

Method: 通过SynLang协议（包括TRACE和TRACE_FE机制）和共生认识论，实现了透明协作、信心量化和多智能体协调。

Result: 实验验证了SynLang在实际人机对话中的有效性，展示了AI对结构化推理协议的适应和元认知干预能力。

Conclusion: 共生认识论与SynLang通过双重透明性和信心校准，增强了人类智能，保留了人类能动性，并支持伦理协作决策。

Abstract: Current AI systems rely on opaque reasoning processes that hinder human
oversight and collaborative potential. Conventional explainable AI approaches
offer post-hoc justifications and often fail to establish genuine symbiotic
collaboration. In this paper, the Symbiotic Epistemology is presented as a
philosophical foundation for human-AI cognitive partnerships. Unlike frameworks
that treat AI as a mere tool or replacement, symbiotic epistemology positions
AI as a reasoning partner, fostering calibrated trust by aligning human
confidence with AI reliability through explicit reasoning patterns and
confidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as
a formal protocol for transparent human-AI collaboration. The framework is
empirically validated through actual human-AI dialogues demonstrating AI's
adaptation to structured reasoning protocols and successful metacognitive
intervention. The protocol defines two complementary mechanisms: TRACE for
high-level reasoning patterns and TRACE_FE for detailed factor explanations. It
also integrates confidence quantification, declarative control over AI
behavior, and context inheritance for multi-agent coordination. By structuring
communication and embedding confidence-calibrated transparency, SynLang,
together with symbiotic epistemology, enables AI systems that enhance human
intelligence, preserve human agency, and uphold ethical accountability in
collaborative decision-making. Through dual-level transparency, beginning with
high-level reasoning patterns and progressing to granular explanations, the
protocol facilitates rapid comprehension and supports thorough verification of
AI decision-making.

</details>


### [82] [Adaptive XAI in High Stakes Environments: Modeling Swift Trust with Multimodal Feedback in Human AI Teams](https://arxiv.org/abs/2507.21158)
*Nishani Fernando,Bahareh Nakisa,Adnan Ahmad,Mohammad Naim Rastgoo*

Main category: cs.AI

TL;DR: 提出了一个自适应可解释AI框架（AXTF），通过实时监测用户的生理和行为信号（如EEG、ECG和眼动追踪），推断认知与情绪状态，动态调整解释内容，以在高压紧急场景中快速建立人机信任。


<details>
  <summary>Details</summary>
Motivation: 在紧急响应等高风险场景中，及时准确的决策依赖于人机快速信任，但现有XAI方法缺乏适应性和非侵入性反馈机制。

Method: 提出AXTF框架，利用生理和行为信号推断用户状态，通过多目标个性化信任估计模型动态调整解释内容。

Result: AXTF通过非侵入性反馈机制支持自适应解释，为高压场景下的高效人机协作提供基础。

Conclusion: AXTF为开发适应高压环境的非侵入性XAI系统奠定了基础，有望促进人机快速信任。

Abstract: Effective human-AI teaming heavily depends on swift trust, particularly in
high-stakes scenarios such as emergency response, where timely and accurate
decision-making is critical. In these time-sensitive and cognitively demanding
settings, adaptive explainability is essential for fostering trust between
human operators and AI systems. However, existing explainable AI (XAI)
approaches typically offer uniform explanations and rely heavily on explicit
feedback mechanisms, which are often impractical in such high-pressure
scenarios. To address this gap, we propose a conceptual framework for adaptive
XAI that operates non-intrusively by responding to users' real-time cognitive
and emotional states through implicit feedback, thereby enhancing swift trust
in high-stakes environments. The proposed adaptive explainability trust
framework (AXTF) leverages physiological and behavioral signals, such as EEG,
ECG, and eye tracking, to infer user states and support explanation adaptation.
At its core is a multi-objective, personalized trust estimation model that maps
workload, stress, and emotion to dynamic trust estimates. These estimates guide
the modulation of explanation features enabling responsive and personalized
support that promotes swift trust in human-AI collaboration. This conceptual
framework establishes a foundation for developing adaptive, non-intrusive XAI
systems tailored to the rigorous demands of high-pressure, time-sensitive
environments.

</details>


### [83] [Efficacy of AI RAG Tools for Complex Information Extraction and Data Annotation Tasks: A Case Study Using Banks Public Disclosures](https://arxiv.org/abs/2507.21360)
*Nicholas Botti,Flora Haberkorn,Charlotte Hoopes,Shaun Khan*

Main category: cs.AI

TL;DR: 研究通过随机任务分配实验，探讨了AI检索增强生成（RAG）工具在信息提取和数据标注任务中的有效性，发现交互式使用AI显著提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在验证AI工具在复杂、真实世界的数据标注任务中是否能提升速度和准确性。

Method: 采用随机任务分配的实验设计，比较了“简单”AI使用和“交互式”AI使用两种条件与纯人工基线。

Result: AI工具将任务执行速度提升10倍，并提高准确性，交互式条件下效果更佳；预计可节省268小时工作量。

Conclusion: AI工具能显著提升任务效率和准确性，但使用者的技能（包括对AI工具的熟悉度）是关键因素。

Abstract: We utilize a within-subjects design with randomized task assignments to
understand the effectiveness of using an AI retrieval augmented generation
(RAG) tool to assist analysts with an information extraction and data
annotation task. We replicate an existing, challenging real-world annotation
task with complex multi-part criteria on a set of thousands of pages of public
disclosure documents from global systemically important banks (GSIBs) with
heterogeneous and incomplete information content. We test two treatment
conditions. First, a "naive" AI use condition in which annotators use only the
tool and must accept the first answer they are given. And second, an
"interactive" AI treatment condition where annotators use the tool
interactively, and use their judgement to follow-up with additional information
if necessary. Compared to the human-only baseline, the use of the AI tool
accelerated task execution by up to a factor of 10 and enhanced task accuracy,
particularly in the interactive condition. We find that when extrapolated to
the full task, these methods could save up to 268 hours compared to the
human-only approach. Additionally, our findings suggest that annotator skill,
not just with the subject matter domain, but also with AI tools, is a factor in
both the accuracy and speed of task performance.

</details>


### [84] [Finding Uncommon Ground: A Human-Centered Model for Extrospective Explanations](https://arxiv.org/abs/2507.21571)
*Laura Spillner,Nima Zargham,Mihai Pomarlan,Robert Porzel,Rainer Malaka*

Main category: cs.AI

TL;DR: 该论文提出了一种个性化的AI解释方法，根据用户的偏好和上下文动态调整解释内容。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法通常过于技术化，不适合非专家用户，因此需要一种更人性化的解释方式。

Method: 通过构建代理的世界观模型，动态记忆用户交互历史，从而估计用户最需要的新信息。

Result: 提出了一种能够根据用户需求提供个性化解释的AI代理模型。

Conclusion: 个性化解释方法改善了AI透明性，更适合非专家用户。

Abstract: The need for explanations in AI has, by and large, been driven by the desire
to increase the transparency of black-box machine learning models. However,
such explanations, which focus on the internal mechanisms that lead to a
specific output, are often unsuitable for non-experts. To facilitate a
human-centered perspective on AI explanations, agents need to focus on
individuals and their preferences as well as the context in which the
explanations are given. This paper proposes a personalized approach to
explanation, where the agent tailors the information provided to the user based
on what is most likely pertinent to them. We propose a model of the agent's
worldview that also serves as a personal and dynamic memory of its previous
interactions with the same user, based on which the artificial agent can
estimate what part of its knowledge is most likely new information to the user.

</details>


### [85] [Can the current trends of AI handle a full course of mathematics?](https://arxiv.org/abs/2507.21664)
*Mariam Alsayyad,Fayadh Kadhem*

Main category: cs.AI

TL;DR: 探讨AI在管理大学数学全程课程中的能力，分析了四个关键方面，发现AI在组织和准确性上表现优异，但在情感和人文方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 研究AI是否能够承担大学数学全程课程的责任，以评估其在教育领域的应用潜力。

Method: 评估AI在制定课程大纲、展示课程内容、解答学生问题和设计评估方面的能力。

Result: AI在组织和准确性上表现优异，但在情感和人文方面仍无法替代人类。

Conclusion: 建议结合人类与AI的优势，以实现更好的大学数学课程设计效果。

Abstract: This paper addresses the question of how able the current trends of
Artificial Intelligence (AI) are in managing to take the responsibility of a
full course of mathematics at a college level. The study evaluates this ability
in four significant aspects, namely, creating a course syllabus, presenting
selected material, answering student questions, and creating an assessment. It
shows that even though the AI is strong in some important parts like
organization and accuracy, there are still some human aspects that are far away
from the current abilities of AI. There is still a hidden emotional part, even
in science, that cannot be fulfilled by the AI in its current state. This paper
suggests some recommendations to integrate the human and AI potentials to
create better outcomes in terms of reaching the target of creating a full
course of mathematics, at a university level, as best as possible.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [86] [Higher-Order Kuramoto Oscillator Network for Dense Associative Memory](https://arxiv.org/abs/2507.21984)
*Jona Nagerl,Natalia G. Berloff*

Main category: nlin.AO

TL;DR: 本文提出了一种包含高阶耦合的广义Kuramoto模型，用于实现密集联想记忆。模型展示了记忆检索的连续和不连续相变，并证明了高阶耦合能够显著提升记忆容量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 通过结合高阶耦合，将传统的Kuramoto模型扩展为密集联想记忆系统，以突破经典Hopfield记忆的存储容量限制。

Method: 引入包含二阶（谐波）和四阶（四次）耦合的广义Kuramoto模型，利用平均场理论分析相图，并通过数值模拟验证理论预测。

Result: 模型展示了记忆检索的复杂相变行为，高阶耦合显著提升了记忆容量和鲁棒性，同时实现了超线性记忆容量增长。

Conclusion: 研究将Kuramoto同步与现代Hopfield记忆理论结合，为实验实现高容量模拟联想记忆提供了理论基础。

Abstract: Networks of phase oscillators can serve as dense associative memories if they
incorporate higher-order coupling beyond the classical Kuramoto model's
pairwise interactions. Here we introduce a generalized Kuramoto model with
combined second-harmonic (pairwise) and fourth-harmonic (quartic) coupling,
inspired by dense Hopfield memory theory. Using mean-field theory and its
dynamical approximation, we obtain a phase diagram for dense associative memory
model that exhibits a tricritical point at which the continuous onset of memory
retrieval is supplanted by a discontinuous, hysteretic transition. In the
quartic-dominated regime, the system supports bistable phase-locked states
corresponding to stored memory patterns, with a sizable energy barrier between
memory and incoherent states. We analytically determine this bistable region
and show that the escape time from a memory state (due to noise) grows
exponentially with network size, indicating robust storage. Extending the
theory to finite memory load, we show that higher-order couplings achieve
superlinear scaling of memory capacity with system size, far exceeding the
limit of pairwise-only oscillators. Large-scale simulations of the oscillator
network confirm our theoretical predictions, demonstrating rapid pattern
retrieval and robust storage of many phase patterns. These results bridge the
Kuramoto synchronization with modern Hopfield memories, pointing toward
experimental realization of high-capacity, analog associative memory in
oscillator systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [87] [Large-Scale Linear Energy System Optimization: A Systematic Review on Parallelization Strategies via Decomposition](https://arxiv.org/abs/2507.21932)
*Lars Hadidi,Leonard Göke,Maximilian Hoffmann,Mario Klostermeier,Shima Sasanpour,Tim Varelmann,Vassilios Yfantis,Jochen Linßen,Detlef Stolten,Jann M. Weinand*

Main category: math.OC

TL;DR: 对能源系统优化模型并行化策略的系统综述，提出分类方法并评估并行分解方法，建议未来基准测试标准。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源集成和系统复杂性增加，传统求解器面临性能挑战，需探索并行化策略以提升效率。

Method: 提出能源系统优化模型的分类方案，综述并行分解方法并分析其性能，评估现有软件工具。

Result: 多种并行分解方法有效，但无普适最优方案，缺乏标准化基准测试。建议未来基准标准和报告规范。

Conclusion: 研究成果不仅适用于能源系统模型，还可推广至运筹学领域，未来需标准化基准测试。

Abstract: As renewable energy integration, sector coupling, and spatiotemporal detail
increase, energy system optimization models grow in size and complexity, often
pushing solvers to their performance limits. This systematic review explores
parallelization strategies that can address these challenges. We first propose
a classification scheme for linear energy system optimization models, covering
their analytical focus, mathematical structure, and scope. We then review
parallel decomposition methods, finding that while many offer performance
benefits, no single approach is universally superior. The lack of standardized
benchmark suites further complicates comparison. To address this, we recommend
essential criteria for future benchmarks and minimum reporting standards. We
also survey available software tools for parallel decomposition, including
modular frameworks and algorithmic abstractions. Though centered on energy
system models, our insights extend to the broader operations research field.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [88] [Solving Boundary Handling Analytically in Two Dimensions for Smoothed Particle Hydrodynamics](https://arxiv.org/abs/2507.21686)
*Rene Winchenbach,Andreas Kolb*

Main category: math.NA

TL;DR: 提出了一种完全解析的方法来评估二维平滑粒子流体动力学（SPH）中的边界积分，直接计算三角形边界上的面积积分，支持高阶边界条件，并通过Chebyshev多项式和Gaussian超几何函数提供闭合解。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖边界粒子或基于散度定理的壁重构方法，难以灵活处理高阶边界条件。本文旨在提供一种更高效、更通用的解析积分方法，以便与网格求解器无缝耦合。

Method: 通过将边界元素分解为可通过闭合解求解的基本积分，利用Chebyshev多项式和Gaussian超几何函数$_2F_1$，提供通用的解析积分框架。

Result: 解析解比现有数值积分规则快多达五个数量级，同时支持任意三角形几何和核函数，适用于SPH及其他需要多边形域解析积分的场景。

Conclusion: 该方法为SPH及其他领域的高效解析积分提供了灵活且强大的工具，为解决SPH中的重大挑战奠定了基础。

Abstract: We present a fully analytic approach for evaluating boundary integrals in two
dimensions for Smoothed Particle Hydrodynamics (SPH). Conventional methods
often rely on boundary particles or wall re-normalization approaches derived
from applying the divergence theorem, whereas our method directly evaluates the
area integrals for SPH kernels and gradients over triangular boundaries. This
direct integration strategy inherently accommodates higher-order boundary
conditions, such as piecewise cubic fields defined via Finite Element stencils,
enabling analytic and flexible coupling with mesh-based solvers. At the core of
our approach is a general solution for compact polynomials of arbitrary degree
over triangles by decomposing the boundary elements into elementary integrals
that can be solved with closed-form solutions. We provide a complete,
closed-form solution for these generalized integrals, derived by relating the
angular components to Chebyshev polynomials and solving the resulting radial
integral via a numerically stable evaluation of the Gaussian hypergeometric
function $_2F_1$. Our solution is robust and adaptable and works regardless of
triangle geometries and kernel functions. We validate the accuracy against
high-precision numerical quadrature rules, as well as in problems with known
exact solutions. We provide an open-source implementation of our general
solution using differentiable programming to facilitate the adoption of our
approach to SPH and other contexts that require analytic integration over
polygonal domains. Our analytic solution outperforms existing numerical
quadrature rules for this problem by up to five orders of magnitude, for
integrals and their gradients, while providing a flexible framework to couple
arbitrary triangular meshes analytically to Lagrangian schemes, building a
strong foundation for addressing several grand challenges in SPH and beyond.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [89] [PanoGAN A Deep Generative Model for Panoramic Dental Radiographs](https://arxiv.org/abs/2507.21200)
*Soren Pedersen,Sanyam Jain,Mikkel Chavez,Viktor Ladehoff,Bruna Neves de Freitas,Ruben Pauwels*

Main category: cs.CV

TL;DR: 本文提出了一种生成对抗网络（GAN）用于合成牙科全景X光片，旨在解决牙科研究和教育中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过GAN生成牙科全景X光片，以缓解牙科研究和教育中数据不足的问题。

Method: 方法包括使用Wasserstein损失和梯度惩罚（WGANGP）训练深度卷积GAN（DCGAN），对2322张不同质量的X光片进行预处理和数据清洗，并探索了四种候选模型。

Result: 生成的影响在解剖可视性和真实性上表现中等，但部分图像存在伪影。未去噪数据训练的模型在细节上表现更好，而去噪数据训练的模型整体清晰度和锐度更优。

Conclusion: 结论是这些结果为未来基于GAN的牙科成像方法提供了基础。

Abstract: This paper presents the development of a generative adversarial network (GAN)
for synthesizing dental panoramic radiographs. Although exploratory in nature,
the study aims to address the scarcity of data in dental research and
education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss
with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying
quality. The focus was on the dentoalveolar regions, other anatomical
structures were cropped out. Extensive preprocessing and data cleaning were
performed to standardize the inputs while preserving anatomical variability. We
explored four candidate models by varying critic iterations, feature depth, and
the use of denoising prior to training. A clinical expert evaluated the
generated radiographs based on anatomical visibility and realism, using a
5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical
depiction, although some were degraded by artifacts. A trade-off was observed
the model trained on non-denoised data yielded finer details especially in
structures like the mandibular canal and trabecular bone, while a model trained
on denoised data offered superior overall image clarity and sharpness. These
findings provide a foundation for future work on GAN-based methods in dental
imaging.

</details>


### [90] [Towards Universal Modal Tracking with Online Dense Temporal Token Learning](https://arxiv.org/abs/2507.20177)
*Yaozong Zheng,Bineng Zhong,Qihua Liang,Shengping Zhang,Guorong Li,Xianxian Li,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出了一种通用的视频级模态感知跟踪模型，支持多模态任务，并通过视频级采样、关联和模态扩展实现高效跟踪。


<details>
  <summary>Details</summary>
Motivation: 设计一个统一的跟踪模型，能够处理多种模态（如RGB、热成像等），减少独立训练的需求。

Method: 采用视频级采样、在线密集时序令牌学习和门控感知器来学习和压缩跨模态表示。

Result: 在多种基准测试中达到了最先进的性能。

Conclusion: 该模型不仅降低了训练负担，还提高了表示能力，适用于多模态跟踪任务。

Abstract: We propose a universal video-level modality-awareness tracking model with
online dense temporal token learning (called {\modaltracker}). It is designed
to support various tracking tasks, including RGB, RGB+Thermal, RGB+Depth, and
RGB+Event, utilizing the same model architecture and parameters. Specifically,
our model is designed with three core goals: \textbf{Video-level Sampling}. We
expand the model's inputs to a video sequence level, aiming to see a richer
video context from an near-global perspective. \textbf{Video-level
Association}. Furthermore, we introduce two simple yet effective online dense
temporal token association mechanisms to propagate the appearance and motion
trajectory information of target via a video stream manner. \textbf{Modality
Scalable}. We propose two novel gated perceivers that adaptively learn
cross-modal representations via a gated attention mechanism, and subsequently
compress them into the same set of model parameters via a one-shot training
manner for multi-task inference. This new solution brings the following
benefits: (i) The purified token sequences can serve as temporal prompts for
the inference in the next video frames, whereby previous information is
leveraged to guide future inference. (ii) Unlike multi-modal trackers that
require independent training, our one-shot training scheme not only alleviates
the training burden, but also improves model representation. Extensive
experiments on visible and multi-modal benchmarks show that our {\modaltracker}
achieves a new \textit{SOTA} performance. The code will be available at
https://github.com/GXNU-ZhongLab/ODTrack.

</details>


### [91] [VoluMe -- Authentic 3D Video Calls from Live Gaussian Splat Prediction](https://arxiv.org/abs/2507.21311)
*Martin de La Gorce,Charlie Hewitt,Tibor Takacs,Robert Gerdisch,Zafiirah Hosenie,Givi Meishvili,Marek Kowalski,Thomas J. Cashman,Antonio Criminisi*

Main category: cs.CV

TL;DR: 提出了一种从单一2D摄像头实时预测3D高斯重建的方法，增强了3D远程会议的逼真性和真实性，且无需复杂硬件。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D会议中人像表现质量高但依赖复杂硬件或固定外观的问题。

Method: 通过独立处理每帧视频，结合稳定性损失，实现实时且稳定的3D高斯重建。

Result: 在视觉质量和稳定性上优于现有方法，支持仅用2D设备和显示器进行3D会议。

Conclusion: 该方法为3D视频会议提供了高可访问性、逼真和真实的解决方案。

Abstract: Virtual 3D meetings offer the potential to enhance copresence, increase
engagement and thus improve effectiveness of remote meetings compared to
standard 2D video calls. However, representing people in 3D meetings remains a
challenge; existing solutions achieve high quality by using complex hardware,
making use of fixed appearance via enrolment, or by inverting a pre-trained
generative model. These approaches lead to constraints that are unwelcome and
ill-fitting for videoconferencing applications. We present the first method to
predict 3D Gaussian reconstructions in real time from a single 2D webcam feed,
where the 3D representation is not only live and realistic, but also authentic
to the input video. By conditioning the 3D representation on each video frame
independently, our reconstruction faithfully recreates the input video from the
captured viewpoint (a property we call authenticity), while generalizing
realistically to novel viewpoints. Additionally, we introduce a stability loss
to obtain reconstructions that are temporally stable on video sequences. We
show that our method delivers state-of-the-art accuracy in visual quality and
stability metrics compared to existing methods, and demonstrate our approach in
live one-to-one 3D meetings using only a standard 2D camera and display. This
demonstrates that our approach can allow anyone to communicate volumetrically,
via a method for 3D videoconferencing that is not only highly accessible, but
also realistic and authentic.

</details>


### [92] [VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding](https://arxiv.org/abs/2507.21507)
*Shibo Gao,Peipei Yang,Yangyang Liu,Yi Chen,Han Zhu,Xuyao Zhang,Linlin Huang*

Main category: cs.CV

TL;DR: 提出了VAGU数据集和GtS框架，首次同时支持视频异常检测和语义理解任务，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法无法同时完成异常定位和语义理解，且缺乏支持此类任务的基准数据集。

Method: 提出VAGU数据集，包含异常类别、语义解释等标注；提出GtS框架，分两阶段完成粗定位和细粒度解释与时序边界优化。

Result: 实验验证了VAGU数据集和GtS框架的有效性。

Conclusion: VAGU数据集和GtS框架填补了现有空白，同时JeAUG指标解决了传统评价的局限性。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events in videos and
accurately determine their time intervals. Current VAD methods mainly fall into
two categories: traditional DNN-based approaches that focus on temporal
localization, and LLM-based approaches that emphasize semantic understanding.
Both anomaly understanding and grounding are essential for comprehensive video
anomaly detection and can complement each other. However, no existing model or
dataset supports both tasks simultaneously. To address this, we introduce VAGU
(Video Anomaly Grounding and Understanding), the first benchmark to integrate
both tasks. Each VAGU instance includes annotations for anomaly category,
semantic explanation, precise temporal grounding and Video QA. We also provide
multiple-choice Video QA for objective evaluation. Based on this dataset, we
propose Glance then Scrutinize (GtS), a training-free framework guided by
textual prompts. The framework first enables coarse localization of
high-probability anomalous regions, followed by detailed anomaly interpretation
and temporal boundary refinement. Additionally, we propose the JeAUG metric,
which jointly evaluates semantic interpretability and temporal precision,
overcoming the limitations of traditional metrics. Extensive experiments verify
the effectiveness of our benchmark, framework, and evaluation metric.

</details>


### [93] [MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces](https://arxiv.org/abs/2507.21741)
*Shaojun E,Yuchen Yang,Jiaheng Wu,Yan Zhang,Tiejun Zhao,Ziyan Chen*

Main category: cs.CV

TL;DR: 论文提出MAGE框架，通过创新的对齐机制解决多模态学习中视觉数据编码后的空间和语义损失问题，并在多个评估基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中视觉编码与大型语言模型耦合不足导致的语义和空间信息损失问题。

Method: 提出MAGE框架，采用智能对齐网络（IAN）实现维度与语义对齐，并结合交叉熵和均方误差的训练策略。

Result: MAGE在MME、MMBench和SEED等多个评估基准上显著优于同类工作。

Conclusion: MAGE通过创新的对齐机制和训练策略，有效地提升了多模态学习的性能。

Abstract: In the latest advancements in multimodal learning, effectively addressing the
spatial and semantic losses of visual data after encoding remains a critical
challenge. This is because the performance of large multimodal models is
positively correlated with the coupling between visual encoders and large
language models. Existing approaches often face issues such as vector gaps or
semantic disparities, resulting in information loss during the propagation
process. To address these issues, we propose MAGE (Multimodal Alignment and
Generation Enhancement), a novel framework that bridges the semantic spaces of
vision and text through an innovative alignment mechanism. By introducing the
Intelligent Alignment Network (IAN), MAGE achieves dimensional and semantic
alignment. To reduce the gap between synonymous heterogeneous data, we employ a
training strategy that combines cross-entropy and mean squared error,
significantly enhancing the alignment effect. Moreover, to enhance MAGE's
"Any-to-Any" capability, we developed a fine-tuning dataset for multimodal
tool-calling instructions to expand the model's output capability boundaries.
Finally, our proposed multimodal large model architecture, MAGE, achieved
significantly better performance compared to similar works across various
evaluation benchmarks, including MME, MMBench, and SEED. Complete code and
appendix are available at: https://github.com/GTCOM-NLP/MAGE.

</details>


### [94] [GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data](https://arxiv.org/abs/2507.21069)
*Andreas Spilz,Heiko Oppel,Jochen Werner,Kathrin Stucke-Straub,Felix Capanni,Michael Munz*

Main category: cs.CV

TL;DR: 本文介绍了一个多模态数据集，包含物理治疗练习和步态相关活动的数据，用于开发传感器分类模型。


<details>
  <summary>Details</summary>
Motivation: 为解决传感器分类模型开发中大规模数据集的缺乏问题，提供多样化且精确的数据资源。

Method: 使用同步的惯性测量单元（IMU）和标记运动捕捉（MoCap）系统记录数据，并提供处理后的IMU方向和工具。

Result: 数据集包含19名参与者的原始和处理数据，支持多种分析任务，如自动练习评估和步态分析。

Conclusion: 该数据集旨在推动机器学习驱动的人类运动分析研究，并提供代码以支持重复性。

Abstract: Wearable inertial measurement units (IMUs) offer a cost-effective and
scalable means to assess human movement quality in clinical and everyday
settings. However, the development of robust sensor-based classification models
for physiotherapeutic exercises and gait analysis requires large, diverse
datasets, which are costly and time-consuming to collect. Here, we present a
multimodal dataset of physiotherapeutic exercises - including correct and
clinically relevant variants - and gait-related exercises - including both
normal and impaired gait patterns - recorded from 19 participants using
synchronized IMUs and marker-based motion capture (MoCap). The dataset includes
raw data from nine IMUs and thirty-five optical markers capturing full-body
kinematics. Each IMU is additionally equipped with four optical markers,
enabling precise comparison between IMU-derived orientation estimates and
reference values from the MoCap system. To support further analysis, we also
provide processed IMU orientations aligned with common segment coordinate
systems, subject-specific OpenSim models, inverse kinematics results, and tools
for visualizing IMU orientations in the musculoskeletal context. Detailed
annotations of movement execution quality and time-stamped segmentations
support diverse analysis goals. This dataset supports the development and
benchmarking of machine learning models for tasks such as automatic exercise
evaluation, gait analysis, temporal activity segmentation, and biomechanical
parameter estimation. To facilitate reproducibility, we provide code for
postprocessing, sensor-to-segment alignment, inverse kinematics computation,
and technical validation. This resource is intended to accelerate research in
machine learning-driven human movement analysis.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [95] [High hopes for "Deep Medicine"? AI, economics, and the future of care](https://arxiv.org/abs/2507.21054)
*Robert Sparrow,Joshua Hatherley*

Main category: cs.CY

TL;DR: 论文讨论了AI在医疗中的前景，认为虽然某些观点认为AI能改善医患关系，但实际情况可能相反。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在医疗中的应用对医患关系的潜在影响，反驳乐观观点。

Method: 通过分析现有观点和潜在因素，提出不同视角。

Result: 指出AI可能进一步削弱医患关系，影响满意度。

Conclusion: AI在医疗中的应用需谨慎，以避免负面社会影响。

Abstract: In the much-celebrated book Deep Medicine, Eric Topol argues that the
development of artificial intelligence for health care will lead to a dramatic
shift in the culture and practice of medicine. In the next several decades, he
suggests, AI will become sophisticated enough that many of the everyday tasks
of physicians could be delegated to it. Topol is perhaps the most articulate
advocate of the benefits of AI in medicine, but he is hardly alone in spruiking
its potential to allow physicians to dedicate more of their time and attention
to providing empathetic care for their patients in the future. Unfortunately,
several factors suggest a radically different picture for the future of health
care. Far from facilitating a return to a time of closer doctor-patient
relationships, the use of medical AI seems likely to further erode therapeutic
relationships and threaten professional and patient satisfaction.

</details>


### [96] [Barriers to Digital Mental Health Services among College Students](https://arxiv.org/abs/2507.21093)
*Ha Na Cho,Kyuha Jung,Daniel Eisenberg,Cheryl A. King,Kai Zheng*

Main category: cs.CY

TL;DR: 这篇论文探讨了大学生在利用数字心理健康干预（DMHI）时面临的九大障碍，并强调了个性化和文化敏感性干预的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示阻碍大学生使用DMHI平台的核心因素，以帮助改进心理健康服务的提供方式。

Method: 通过主题分析，研究者对参与eBridge干预试验的学生的反馈进行了深入分析。

Result: 研究发现九大障碍（如情绪困扰、时间限制、隐私问题等），并指出需改进干预策略以提高年轻成人的参与度。

Conclusion: 研究呼吁制定更个性化和文化敏感性的干预措施，以提升心理健康服务的可及性和吸引力。

Abstract: This qualitative study explores barriers to utilization of digital mental
health Intervention (DMHI) among college students. Data are from a large
randomized clinical trial of an intervention, eBridge, that used motivational
interviewing for online counseling to connect students with mental health
issues to professional services. We applied thematic analysis to analyze the
feedback from the student participants regarding their experience of using the
DMHI platform. We identified nine key barriers to DMHI adoption and the use of
in-person mental health services: emotional distress, time constraints, privacy
concerns, resource accessibility, financial challenges, medication stigma,
dissatisfaction with communication, content clarity, and treatment-related
concerns. Our findings emphasize the need for personalized, culturally
sensitive interventions and improved strategies to enhance the access and
engagement in mental health support for young adults.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [97] [Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications](https://arxiv.org/abs/2507.21199)
*Xinye Cao,Hongcan Guo,Guoshun Nan,Jiaoyang Cui,Haoting Qian,Yihan Lin,Yilin Peng,Diyang Zhang,Yanzhao Hou,Huici Wu,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的方法，使用单一组合式大语言模型（LLM）在无线网络中完成多种交互式多模态应用（IMAs），并通过ContextLoRA和ContextGear解决了适应性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有的IMAs通常依赖多个LLM，这增加了复杂性和资源消耗。本文旨在通过单一LLM实现多样化的IMAs，同时解决适应性和计算效率的挑战。

Method: 提出了ContextLoRA方法，通过构建任务依赖图和分阶段微调（训练、冻结和掩码）来指导LLM学习IMAs的结构化上下文。此外，ContextGear通过分组机制优化训练过程以减少计算和通信成本。

Result: 实验结果表明，ContextLoRA和ContextGear在三个基准测试上的表现优于现有方法。实际无线测试证明了其在实际应用中的可行性。

Conclusion: 本文提出的方法通过单一LLM实现了高效的IMAs支持，为资源受限的移动环境提供了一种灵活且高效的解决方案。

Abstract: Interactive multimodal applications (IMAs), such as route planning in the
Internet of Vehicles, enrich users' personalized experiences by integrating
various forms of data over wireless networks. Recent advances in large language
models (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple
IMAs, with each LLM trained individually for a specific task that presents
different business workflows. In contrast to existing approaches that rely on
multiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes
various IMAs using a single compositional LLM over wireless networks. The two
primary challenges include 1) guiding a single LLM to adapt to diverse IMA
objectives and 2) ensuring the flexibility and efficiency of the LLM in
resource-constrained mobile environments. To tackle the first challenge, we
propose ContextLoRA, a novel method that guides an LLM to learn the rich
structured context among IMAs by constructing a task dependency graph. We
partition the learnable parameter matrix of neural layers for each IMA to
facilitate LLM composition. Then, we develop a step-by-step fine-tuning
procedure guided by task relations, including training, freezing, and masking
phases. This allows the LLM to learn to reason among tasks for better
adaptation, capturing the latent dependencies between tasks. For the second
challenge, we introduce ContextGear, a scheduling strategy to optimize the
training procedure of ContextLoRA, aiming to minimize computational and
communication costs through a strategic grouping mechanism. Experiments on
three benchmarks show the superiority of the proposed ContextLoRA and
ContextGear. Furthermore, we prototype our proposed paradigm on a real-world
wireless testbed, demonstrating its practical applicability for various IMAs.
We will release our code to the community.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [98] [Not Here, Go There: Analyzing Redirection Patterns on the Web](https://arxiv.org/abs/2507.22019)
*Kritika Garg,Sawood Alam,Dietrich Ayala,Michele C. Weigle,Michael L. Nelson*

Main category: cs.DL

TL;DR: 分析1100万个URI重定向，揭示50%成功重定向，50%出现错误，包括0.06%超过10跳。研究还发现SEO优化和安全隐患的常见模式。


<details>
  <summary>Details</summary>
Motivation: URI重定向对网站管理至关重要，但复杂性影响用户体验和SEO。研究旨在揭示其模式和潜在问题。

Method: 分析1100万个独特URI，追踪每个URI最多10跳重定向。

Result: 50%重定向成功，0.06%超过10跳；发现SEO优化、迁移和安全风险等常见模式。

Conclusion: URI重定向对网络至关重要，但也带来挑战。研究帮助优化网站管理和保护内容。

Abstract: URI redirections are integral to web management, supporting structural
changes, SEO optimization, and security. However, their complexities affect
usability, SEO performance, and digital preservation. This study analyzed 11
million unique redirecting URIs, following redirections up to 10 hops per URI,
to uncover patterns and implications of redirection practices. Our findings
revealed that 50% of the URIs terminated successfully, while 50% resulted in
errors, including 0.06% exceeding 10 hops. Canonical redirects, such as HTTP to
HTTPS transitions, were prevalent, reflecting adherence to SEO best practices.
Non-canonical redirects, often involving domain or path changes, highlighted
significant web migrations, rebranding, and security risks. Notable patterns
included "sink" URIs, where multiple redirects converged, ranging from traffic
consolidation by global websites to deliberate "Rickrolling." The study also
identified 62,000 custom 404 URIs, almost half being soft 404s, which could
compromise SEO and user experience. These findings underscore the critical role
of URI redirects in shaping the web while exposing challenges such as outdated
URIs, server instability, and improper error handling. This research offers a
detailed analysis of URI redirection practices, providing insights into their
prevalence, types, and outcomes. By examining a large dataset, we highlight
inefficiencies in redirection chains and examine patterns such as the use of
"sink" URIs and custom error pages. This information can help webmasters,
researchers, and digital archivists improve web usability, optimize resource
allocation, and safeguard valuable online content.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [99] [NIST Post-Quantum Cryptography Standard Algorithms Based on Quantum Random Number Generators](https://arxiv.org/abs/2507.21151)
*Abel C. H. Chen*

Main category: cs.CR

TL;DR: 本文提出了一种基于量子随机数生成器（QRNG）的后量子密码算法，以增强现有后量子密码技术在特定应用场景中的安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管NIST发布的后量子密码标准（如ML-KEM、ML-DSA和SLH-DSA）能抵抗量子计算攻击，但在某些特殊应用场景下可能不够安全。

Method: 设计了六种QRNG，并通过NIST SP 800-90B的统计验证程序评估其性能。同时，提出了QRNG为基础的ML-KEM、ML-DSA和SLH-DSA算法。

Result: 实验评估了六种QRNG的计算时间及基于QRNG的密码算法的性能，为未来部署后量子密码系统提供了参考数据。

Conclusion: QRNG为基础的后量子密码算法有望提升特定场景下的安全性，实验结果为未来实际应用提供了依据。

Abstract: In recent years, the advancement of quantum computing technology has posed
potential security threats to RSA cryptography and elliptic curve cryptography.
In response, the National Institute of Standards and Technology (NIST)
published several Federal Information Processing Standards (FIPS) of
post-quantum cryptography (PQC) in August 2024, including the
Module-Lattice-Based Key-Encapsulation Mechanism (ML-KEM), Module-Lattice-Based
Digital Signature Algorithm (ML-DSA), and Stateless Hash-Based Digital
Signature Algorithm (SLH-DSA). Although these PQC algorithms are designed to
resist quantum computing attacks, they may not provide adequate security in
certain specialized application scenarios. To address this issue, this study
proposes quantum random number generator (QRNG)-based PQC algorithms. These
algorithms leverage quantum computing to generate random numbers, which serve
as the foundation for key pair generation, key encapsulation, and digital
signature generation. A generalized architecture of QRNG is proposed, along
with the design of six QRNGs. Each generator is evaluated according to the
statistical validation procedures outlined in NIST SP 800-90B, including tests
for verification of entropy sources and independent and identically distributed
(IID) outputs. Experimental results assess the computation time of the six
QRNGs, as well as the performance of QRNG-based ML-KEM, QRNG-based ML-DSA, and
QRNG-based SLH-DSA. These findings provide valuable reference data for future
deployment of PQC systems.

</details>


### [100] [Mitigation of Social Media Platforms Impact on the Users](https://arxiv.org/abs/2507.21181)
*Smita Khapre,Sudhanshu Semwal*

Main category: cs.CR

TL;DR: 论文提出了一种基于分形树和L系统算法的去中心化数据框架，以解决社交媒体平台对隐私和安全的潜在影响，并计划通过对比现有安全方法验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台虽然带来便利，但其数据架构和人工智能算法的使用可能威胁用户隐私和安全。

Method: 提出了一种基于分形树和L系统算法的去中心化数据安排框架。

Result: 框架通过分支密钥生成和防御机制提升数据安全性。

Conclusion: 未来研究将进一步验证该框架的有效性，并探索加密算法的应用。

Abstract: Social media platforms offer numerous benefits and allow people to come
together for various causes. Many communities, academia, government agencies,
institutions, healthcare, entertainment, and businesses are on social media
platforms. They are intuitive and free for users. It has become unimaginable to
live without social media. Their architecture and data handling are geared
towards scalability, uninterrupted availability, and both personal and
collaborative revenue generation. Primarily, artificial intelligence algorithms
are employed on stored user data for optimization and feeds. This has the
potential to impact user safety, privacy, and security, even when metadata is
used. A new decentralized data arrangement framework based on the Fractal-tree
and L-Systems algorithm is proposed to mitigate some of the impacts of social
media platforms.
  Future work will focus on demonstrating the effectiveness of the new
decentralized framework by comparing its results against state-of-the-art
security methods currently used in databases. A cryptographic algorithm could
also be implemented for the framework, employing a new key generation for each
branch. This will strengthen database security; for example, if a user key is
leaked, regenerating the key for each branch will keep the data secure by
applying defense mechanisms in the proposed L-System-based tree framework.

</details>


### [101] [MaXsive: High-Capacity and Robust Training-Free Generative Image Watermarking in Diffusion Models](https://arxiv.org/abs/2507.21195)
*Po-Yuan Mao,Cheng-Chang Tsai,Chun-Shien Lu*

Main category: cs.CR

TL;DR: 论文提出了一种名为MaXsive的训练免费扩散模型水印技术，解决了现有方法对旋转、缩放和平移攻击的脆弱性问题，同时提高了水印容量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 商用扩散模型的成功带来了版权保护和不恰内容生成的挑战，现有训练免费水印方法容易受到RST攻击，且水印容量有限可能导致身份冲突。

Method: MaXsive通过优化初始噪声植入水印，并采用X形模板而非环形模式来恢复RST失真，显著提升鲁棒性和容量。

Result: 在两个知名水印基准测试中，MaXsive在验证和识别场景下表现出色，有效抵御攻击且避免了ID冲突。

Conclusion: MaXsive为扩散模型提供了一种高效、鲁棒的水印解决方案，显著优于现有方法。

Abstract: The great success of the diffusion model in image synthesis led to the
release of gigantic commercial models, raising the issue of copyright
protection and inappropriate content generation. Training-free diffusion
watermarking provides a low-cost solution for these issues. However, the prior
works remain vulnerable to rotation, scaling, and translation (RST) attacks.
Although some methods employ meticulously designed patterns to mitigate this
issue, they often reduce watermark capacity, which can result in identity (ID)
collusion. To address these problems, we propose MaXsive, a training-free
diffusion model generative watermarking technique that has high capacity and
robustness. MaXsive best utilizes the initial noise to watermark the diffusion
model. Moreover, instead of using a meticulously repetitive ring pattern, we
propose injecting the X-shape template to recover the RST distortions. This
design significantly increases robustness without losing any capacity, making
ID collusion less likely to happen. The effectiveness of MaXsive has been
verified on two well-known watermarking benchmarks under the scenarios of
verification and identification.

</details>


### [102] [HexaMorphHash HMH- Homomorphic Hashing for Secure and Efficient Cryptographic Operations in Data Integrity Verification](https://arxiv.org/abs/2507.21096)
*Krishnendu Das*

Main category: cs.CR

TL;DR: 本文提出了一种基于格的新型同态哈希函数HexaMorphHash，解决了分布式系统中动态数据更新的负载均衡和可扩展性问题，同时保护数据完整性。


<details>
  <summary>Details</summary>
Motivation: 传统哈希方法在动态环境中因节点变更需要全面重新哈希，而一致性哈希尽管减少了数据重分布，但在高负载和频繁更新时仍存在局限。

Method: 利用基于格的同态哈希函数HexaMorphHash，结合短整数解（SIS）问题的复杂性，实现常量时间的增量更新和固定大小的摘要。

Result: HexaMorphHash在计算效率、内存使用和可扩展性上显著优于现有方法（如直接签名、Merkle树、AdHash等），并能抵抗量子威胁。

Conclusion: HexaMorphHash为大规模分布式系统中的频繁数据更新提供了高效且安全的解决方案，兼顾数据完整性和系统性能。

Abstract: In the realm of big data and cloud computing, distributed systems are tasked
with proficiently managing, storing, and validating extensive datasets across
numerous nodes, all while maintaining robust data integrity. Conventional
hashing methods, though straightforward, encounter substan tial difficulties in
dynamic settings due to the necessity for thorough rehashing when nodes are
altered. Consistent hashing mitigates some of these challenges by reducing data
redistribution; however, it still contends with limitations in load balancing
and scalability under intensive update conditions. This paper introduces an
innovative approach using a lattice based homomorphic hash function
HexaMorphHash that facilitates constant time, incremental updates while
preserving a constant digest size. By utilizing the complexity of the Short
Integer Solutions SIS problem, our method secures strong protective measures,
even against quantum threats. We further com pare our method with existing ones
such as direct signatures for each update, comprehensive database signing,
Merkle tree based techniques, AdHash, MuHash, ECMH, and homomorphic sig nature
schemes highlighting notable advancements in computational efficiency, memory
usage, and scalability. Our contributions present a viable solution for
frequent update dissemination in expansive distributed systems, safeguarding
both data integrity and system performance.

</details>


### [103] [A Formal Rebuttal of "The Blockchain Trilemma: A Formal Proof of the Inherent Trade-Offs Among Decentralization, Security, and Scalability"](https://arxiv.org/abs/2507.21111)
*Craig Wright*

Main category: cs.CR

TL;DR: 该论文通过形式分析和实证证据反驳了区块链三元论，指出其缺乏理论基础，并展示了可扩展性是工程成果而非权衡。


<details>
  <summary>Details</summary>
Motivation: 反驳区块链三元论的错误观点，揭示其方法论和术语的缺陷，为区块链研究提供更严谨的标准。

Method: 结合形式分析、实证证据和详细批判，重构比特币的设计，展示其作为确定性无状态协议的特点。

Result: 证明三元论基于语义混淆和理论误用，可扩展性可通过工程实现，无需牺牲去中心化或安全性。

Conclusion: 论文批评了学术讨论中的系统性缺陷，并提出了评估区块链研究主张的形式标准。

Abstract: This paper presents a comprehensive refutation of the so-called "blockchain
trilemma," a widely cited but formally ungrounded claim asserting an inherent
trade-off between decentralisation, security, and scalability in blockchain
protocols. Through formal analysis, empirical evidence, and detailed critique
of both methodology and terminology, we demonstrate that the trilemma rests on
semantic equivocation, misuse of distributed systems theory, and a failure to
define operational metrics. Particular focus is placed on the conflation of
topological network analogies with protocol-level architecture, the
mischaracterisation of Bitcoin's design--including the role of miners, SPV
clients, and header-based verification--and the failure to ground claims in
complexity-theoretic or adversarial models. By reconstructing Bitcoin as a
deterministic, stateless distribution protocol governed by evidentiary trust,
we show that scalability is not a trade-off but an engineering outcome. The
paper concludes by identifying systemic issues in academic discourse and peer
review that have allowed such fallacies to persist, and offers formal criteria
for evaluating future claims in blockchain research.

</details>


### [104] [Security study based on the Chatgptplugin system: ldentifying Security Vulnerabilities](https://arxiv.org/abs/2507.21128)
*Ruomai Ren*

Main category: cs.CR

TL;DR: 研究分析了ChatGPT插件店的安全性问题，揭示了主要漏洞并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 插件系统虽然丰富用户体验，但因其多样性和开发者的复杂性，缺乏足够监管，尤其是ChatGPT插件系统的安全风险被忽视。

Method: 分析ChatGPT插件店中插件的安全性。

Result: 揭示了主要的安全漏洞。

Conclusion: 提出针对性的改进措施以提升插件系统的安全性。

Abstract: Plugin systems are a class of external programmes that provide users with a
wide range of functionality, and while they enhance the user experience, their
security is always a challenge. Especially due to the diversity and complexity
of developers, many plugin systems lack adequate regulation. As ChatGPT has
become a popular large-scale language modelling platform, its plugin system is
also gradually developing, and the open platform provides creators with the
opportunity to upload plugins covering a wide range of application scenarios.
However, current research and discussions mostly focus on the security issues
of the ChatGPT model itself, while ignoring the possible security risks posed
by the plugin system. This study aims to analyse the security of plugins in the
ChatGPT plugin shop, reveal its major security vulnerabilities, and propose
corresponding improvements.

</details>


### [105] [Out of Distribution, Out of Luck: How Well Can LLMs Trained on Vulnerability Datasets Detect Top 25 CWE Weaknesses?](https://arxiv.org/abs/2507.21817)
*Yikun Li,Ngoc Tan Bui,Ting Zhang,Martin Weyssow,Chengran Yang,Xin Zhou,Jinfeng Jiang,Junkai Chen,Huihui Huang,Huu Hung Nguyen,Chiok Yew Ho,Jie Tan,Ruiyin Li,Yide Yin,Han Wei Ang,Frank Liauw,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.CR

TL;DR: 该论文提出了一种解决漏洞检测模型泛化性不足的三部分解决方案，包括高质量数据集构建和漏洞生成框架。


<details>
  <summary>Details</summary>
Motivation: 当前漏洞检测研究存在数据集标签不准确、重复率高及覆盖率不足的问题，导致模型在实际应用中性能下降明显。

Method: 1. 构建手动标注的测试数据集BenchVul；2. 使用多智能体LLM框架去重和验证，构建高质量训练数据集TitanVul；3. 提出现实漏洞生成（RVG）框架，生成关键CWE类型漏洞示例。

Result: 实验显示：1. BenchVul揭示了现有数据集自测试的局限性；2. TitanVul训练的模型在BenchVul上表现提升；3. 结合RVG的数据进一步将性能提升14%。

Conclusion: 该解决方案显著缩小了漏洞检测模型的泛化性差距，提升了模型在实际应用中的性能。

Abstract: Automated vulnerability detection research has made substantial progress, yet
its real-world impact remains limited. Current vulnerability datasets suffer
from issues including label inaccuracy rates of 20-71%, extensive duplication,
and poor coverage of critical CWE types. These issues create a significant
"generalization gap" where models achieve misleading self-testing performance
(measured on held-out data from same dataset for training) by exploiting
spurious correlations rather than learning true vulnerability patterns. Our
analysis reveals that many models experience substantial performance drops of
up to 40.6% when evaluated on independent data, sometimes underperforming
random guessing.
  To address these limitations, we present a three-part solution. First, we
introduce a manually curated test dataset, BenchVul, covering the MITRE Top 25
Most Dangerous CWEs. Second, we construct a high-quality training dataset,
TitanVul, comprising 35,045 functions by aggregating seven public sources and
applying deduplication and validation using a novel multi-agent LLM framework.
Third, we propose a Realistic Vulnerability Generation (RVG) framework, which
synthesizes context-aware vulnerability examples for underrepresented but
critical CWE types through simulated development workflows.
  Our evaluation shows the strengths of each component in closing the
generalization gap. First, BenchVul shows the limitations of self-testing:
models trained on existing datasets, such as BigVul and PrimeVul, experience
performance drops on BenchVul (from 0.776 to 0.519 and from 0.567 to 0.337).
Second, training models on TitanVul demonstrates improved generalization, with
model performance increasing from 0.584 when evaluated on the same dataset to
0.767 when tested on BenchVul. Third, supplementing TitanVul with RVG-generated
data yields further gains, increasing model performance by 14.0% to 0.874.

</details>
